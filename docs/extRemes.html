<!DOCTYPE html><html><head><title>Help for package extRemes</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {extRemes}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#atdf'>
<p>Auto-Tail Dependence Function</p></a></li>
<li><a href='#BayesFactor'>
<p>Estimate Bayes Factor</p></a></li>
<li><a href='#blockmaxxer'>
<p>Find Block Maxima</p></a></li>
<li><a href='#CarcasonneHeat'>
<p>European Climate Assessment and Dataset</p></a></li>
<li><a href='#ci.fevd'>
<p>Confidence Intervals</p>
</p></a></li>
<li><a href='#ci.rl.ns.fevd.bayesian'>
<p>Confidence/Credible Intervals for Effective Return Levels</p></a></li>
<li><a href='#damage'><p> Hurricane Damage Data</p></a></li>
<li><a href='#datagrabber.declustered'>
<p>Get Original Data from an R Object</p></a></li>
<li><a href='#decluster'>
<p>Decluster Data Above a Threshold</p></a></li>
<li><a href='#Denmint'><p> Denver Minimum Temperature</p></a></li>
<li><a href='#Denversp'><p> Denver July hourly precipitation amount.</p></a></li>
<li><a href='#devd'>
<p>Extreme Value Distributions</p></a></li>
<li><a href='#distill.fevd'>
<p>Distill Parameter Information</p>
</p></a></li>
<li><a href='#erlevd'>
<p>Effective Return Levels</p></a></li>
<li><a href='#extremalindex'>
<p>Extemal Index</p></a></li>
<li><a href='#extRemes internal'>
<p>extRemes Internal and Secondary Functions</p></a></li>
<li><a href='#extRemes-package'><p>extRemes &ndash; Weather and Climate Applications of Extreme Value Analysis (EVA)</p></a></li>
<li><a href='#FCwx'>
<p>Fort Collins, Colorado Weather Data</p></a></li>
<li><a href='#fevd'>
<p>Fit An Extreme Value Distribution (EVD) to Data</p></a></li>
<li><a href='#findAllMCMCpars'>
<p>Manipulate MCMC Output from fevd Objects</p></a></li>
<li><a href='#findpars'>
<p>Get EVD Parameters</p></a></li>
<li><a href='#Flood'><p> United States Total Economic Damage Resulting from Floods</p></a></li>
<li><a href='#Fort'><p> Daily precipitation amounts in Fort Collins, Colorado.</p></a></li>
<li><a href='#fpois'>
<p>Fit Homogeneous Poisson to Data and Test Equality of Mean and Variance</p>
</p></a></li>
<li><a href='#ftcanmax'><p> Annual Maximum Precipitation: Fort Collins, Colorado</p></a></li>
<li><a href='#HEAT'><p> Summer Maximum and Minimum Temperature: Phoenix, Arizona</p></a></li>
<li><a href='#hwmi'><p>Heat Wave Magnitude Index</p></a></li>
<li><a href='#hwmid'><p>Heat Wave Magnitude Index</p></a></li>
<li><a href='#is.fixedfevd'>
<p>Stationary Fitted Model Check</p></a></li>
<li><a href='#levd'>
<p>Extreme Value Likelihood</p></a></li>
<li><a href='#lr.test'>
<p>Likelihood-Ratio Test</p></a></li>
<li><a href='#make.qcov'>
<p>Covariate Matrix for Non-Stationary EVD Projections</p></a></li>
<li><a href='#mrlplot'>
<p>Mean Residual Life Plot</p></a></li>
<li><a href='#Ozone4H'><p> Ground-Level Ozone Order Statistics.</p></a></li>
<li><a href='#parcov.fevd'>
<p>EVD Parameter Covariance</p></a></li>
<li><a href='#Peak'><p> Salt River Peak Stream Flow</p></a></li>
<li><a href='#pextRemes'>
<p>Probabilities and Random Draws from Fitted EVDs</p></a></li>
<li><a href='#PORTw'><p> Annual Maximum and Minimum Temperature</p></a></li>
<li><a href='#postmode'>
<p>Posterior Mode from an MCMC Sample</p></a></li>
<li><a href='#Potomac'><p> Potomac River Peak Stream Flow Data.</p></a></li>
<li><a href='#profliker'>
<p>Profile Likelihood Function</p></a></li>
<li><a href='#qqnorm'>
<p>Normal qq-plot with 95 Percent Simultaneous Confidence Bands</p></a></li>
<li><a href='#qqplot'>
<p>qq-plot Between Two Vectors of Data with 95 Percent Confidence Bands</p></a></li>
<li><a href='#return.level'>
<p>Return Level Estimates</p></a></li>
<li><a href='#revtrans.evd'>
<p>Reverse Transformation</p></a></li>
<li><a href='#rlevd'>
<p>Return Levels for Extreme Value Distributions</p></a></li>
<li><a href='#Rsum'><p> Hurricane Frequency Dataset.</p></a></li>
<li><a href='#SantaAna'>
<p>Santa Ana Winds Data</p></a></li>
<li><a href='#shiftplot'><p> Shift Plot Between Two Sets of Data</p></a></li>
<li><a href='#strip'>
<p>Strip Fitted EVD Object of Everything but the Parameter Estimates</p></a></li>
<li><a href='#taildep'>
<p>Tail Dependence</p></a></li>
<li><a href='#taildep.test'>
<p>Tail Dependence Test</p></a></li>
<li><a href='#threshrange.plot'>
<p>Threshold Selection Through Fitting Models to a Range of Thresholds</p></a></li>
<li><a href='#Tphap'><p> Daily Maximum and Minimum Temperature in Phoenix, Arizona.</p></a></li>
<li><a href='#trans'>
<p>Transform Data</p></a></li>
<li><a href='#xbooter'>
<p>Additional Bootstrap Functions for Univariate EVA</p></a></li>
<li><a href='#xtibber'>
<p>Test-Inversion Bootstrap for Extreme-Value Analysis</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>2.1-4</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-02-01</td>
</tr>
<tr>
<td>Title:</td>
<td>Extreme Value Analysis</td>
</tr>
<tr>
<td>Author:</td>
<td>Eric Gilleland</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Eric Gilleland &lt;ericg@ucar.edu&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10.0), Lmoments, distillery (&ge; 1.0-4)</td>
</tr>
<tr>
<td>Imports:</td>
<td>graphics, stats, methods</td>
</tr>
<tr>
<td>Suggests:</td>
<td>fields</td>
</tr>
<tr>
<td>Description:</td>
<td>General functions for performing extreme value analysis.  In particular, allows for inclusion of covariates into the parameters of the extreme-value distributions, as well as estimation through MLE, L-moments, generalized (penalized) MLE (GMLE), as well as Bayes.  Inference methods include parametric normal approximation, profile-likelihood, Bayes, and bootstrapping.  Some bivariate functionality and dependence checking (e.g., auto-tail dependence function plot, extremal index estimation) is also included.  For a tutorial, see Gilleland and Katz (2016) &lt;<a href="https://doi.org/10.18637%2Fjss.v072.i08">doi:10.18637/jss.v072.i08</a>&gt; and for bootstrapping, please see Gilleland (2020) &lt;<a href="https://doi.org/10.1175%2FJTECH-D-20-0070.1">doi:10.1175/JTECH-D-20-0070.1</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://staff.ral.ucar.edu/ericg/extRemes/">https://staff.ral.ucar.edu/ericg/extRemes/</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-02-01 21:14:53 UTC; gille</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-02-02 15:20:07 UTC</td>
</tr>
</table>
<hr>
<h2 id='atdf'>
Auto-Tail Dependence Function
</h2><span id='topic+atdf'></span><span id='topic+plot.atdf'></span>

<h3>Description</h3>

<p>Computes (and by default plots) estimates of the auto-tail dependence function(s) (atdf) based on
either chi (rho) or chibar (rhobar), or both.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>atdf(x, u, lag.max = NULL, type = c("all", "rho", "rhobar"), plot = TRUE,
    na.action = na.fail, ...)

## S3 method for class 'atdf'
plot(x, type = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="atdf_+3A_x">x</code></td>
<td>
<p>For <code>atdf</code>: a univariate time series object or a numeric vector.  For the <code>plot</code> method function, a list object of class &ldquo;atdf&rdquo;.</p>
</td></tr>
<tr><td><code id="atdf_+3A_u">u</code></td>
<td>
<p>numeric between 0 and 1 (non-inclusive) determining the level F^(-1)(u) over which to compute the atdf.  Typically, this should be close to 1, but low enough to incorporate enough data.</p>
</td></tr>
<tr><td><code id="atdf_+3A_lag.max">lag.max</code></td>
<td>
<p>The maximum lag for which to compute the atdf.  Default is 10*log10(n), where n is the length of the data.  Will be automatically limited to one less than the total number of observations in the series.</p>
</td></tr>
<tr><td><code id="atdf_+3A_type">type</code></td>
<td>
<p>character string stating which type of atdf to calculate/plot (rho, rhobar or both).  If NULL the <code>plot</code> method function will take the type to be whatever was passed to the call to <code>atdf</code>.  If &ldquo;all&rdquo;, then a 2 by 1 panel of two plots are graphed.</p>
</td></tr>
<tr><td><code id="atdf_+3A_plot">plot</code></td>
<td>
<p>logical, should the plot be made or not?  If TRUE, output is returned invisibly.  If FALSE, output is returned normally.</p>
</td></tr>
<tr><td><code id="atdf_+3A_na.action">na.action</code></td>
<td>
<p>function to be called to handle missing values.</p>
</td></tr>
<tr><td><code id="atdf_+3A_...">...</code></td>
<td>

<p>Further arguments to be passed to the <code>plot</code> method function or to <code>plot</code>.  Note that if <code>main</code>, <code>xlab</code> or <code>ylab</code> are used with type &ldquo;all&rdquo;, then the labels/title will be applied to both plots, which is probably not desirable.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The tail dependence functions are those described in, e.g., Reiss and Thomas (2007) Eq (2.60) for &quot;chi&quot; and Eq (13.25) &quot;chibar&quot;, and estimated by Eq (2.62) and Eq (13.28), resp.  See also, Sibuya (1960) and Coles (2001) sec. 8.4, as well as other texts on EVT such as Beirlant et al. (2004) sec. 9.4.1 and 10.3.4 and de Haan and Ferreira (2006).
</p>
<p>Specifically, for two series X and Y with associated df's F and G, chi, a function of u, is defined as
</p>
<p>chi(u) = Pr[Y &gt; G^(-1)(u) | X &gt; F^(-1)(u)] = Pr[V &gt; u | U &gt; u],
</p>
<p>where (U,V) = (F(X),G(Y))&ndash;i.e., the copula.  Define chi = limit as u goes to 1 of chi(u).
</p>
<p>The coefficient of tail dependence, chibar(u) was introduced by Coles et al. (1999), and is given by
</p>
<p>chibar(u) = 2*log(Pr[U &gt; u])/log(Pr[U &gt; u, V &gt; u]) - 1.
</p>
<p>Define chibar = limit as u goes to 1 of chibar(u).
</p>
<p>The auto-tail dependence function using chi(u) and/or chibar(u) employs X against itself at different lags.
</p>
<p>The associated estimators for the auto-tail dependence functions employed by these functions are based on the above two coefficients of tail dependence, and are given by Reiss and Thomas (2007) Eq (2.65) and (13.28) for a lag h as
</p>
<p>rho.hat(u, h) = sum( min(x_i, x_(i+h) ) &gt; sort(x)[floor(n*u)])/(n*(1-u))   [based on chi]
</p>
<p>and
</p>
<p>rhobar.hat(u, h) = 2*log(1 - u)/log(sum(min(x_i,x_(i+h)) &gt; sort(x)[floor(n*u)])/(n - h)) - 1.
</p>
<p>Some properties of the above dependence coefficients, chi(u), chi, and chibar(u) and chibar, are that 0 &lt;= chi(u), chi &lt;= 1, where if X and Y are stochastically independent, then chi(u) = 1 - u, and chibar = 0.  If X = Y (perfectly dependent), then chi(u) = chi = 1.  For chibar(u) and chibar, we have that -1 &lt;= chibar(u), chibar &lt;= 1.  If U = V, then chibar = 1.  If chi = 0, then chibar &lt; 1 (tail independence with chibar determining the degree of dependence).
</p>


<h3>Value</h3>

<p>A list object of class &ldquo;atdf&rdquo; is returned with components: 
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>The function calling string.</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>character naming the type of atdf computed.</p>
</td></tr>
<tr><td><code>series</code></td>
<td>
<p>character string naming the series used.</p>
</td></tr>
<tr><td><code>lag</code></td>
<td>
<p>numeric vector giving the lags used.</p>
</td></tr>
<tr><td><code>atdf</code></td>
<td>
<p>numeric vector or if type is &ldquo;all&rdquo;, two-column matrix giving the estimated auto-tail dependence function values.</p>
</td></tr>
</table>
<p>The plot method functoin does not return anything.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Beirlant, J., Goegebeur, Y., Teugels, J. and Segers, J. (2004) <em>Statistics of Extremes: Theory and Applications</em>.  Chichester, West Sussex, England, UK: Wiley, ISBN 9780471976479, 522pp.
</p>
<p>Coles, S.  (2001) <em>An introduction to statistical modeling of extreme values</em>, London, U.K.: Springer-Verlag, 208 pp.
</p>
<p>Coles, S., Heffernan, J. E., and Tawn, J. A. (1999) Dependence measures for extreme value analyses.  <em>Extremes</em>, <b>2</b>, 339&ndash;365.
</p>
<p>de Haan, L. and Ferreira, A. (2006) <em>Extreme Value Theory: An Introduction</em>.  New York, NY, USA: Springer, 288pp.
</p>
<p>Reiss, R.-D. and Thomas, M. (2007) <em>Statistical Analysis of Extreme Values: with applications to insurance, finance, hydrology and other fields</em>. Birkhauser, 530pp., 3rd edition.
</p>
<p>Sibuya, M. (1960) Bivariate extreme statistics.  <em>Ann. Inst. Math. Statist.</em>, <b>11</b>, 195&ndash;210.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+acf">acf</a></code>, <code><a href="stats.html#topic+pacf">pacf</a></code>, <code><a href="#topic+taildep">taildep</a></code>, <code><a href="#topic+taildep.test">taildep.test</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>z &lt;- arima.sim(n = 63, list(ar = c(0.8897, -0.4858), ma = c(-0.2279, 0.2488)),
               sd = sqrt(0.1796))
hold &lt;- atdf(z, 0.8, plot=FALSE)
par(mfrow=c(2,2))
acf(z, xlab="")
pacf(z, xlab="")
plot(hold, type="chi")
plot(hold, type="chibar")

y &lt;- cbind(z[2:63], z[1:62])
y &lt;- apply(y, 1, max)
hold2 &lt;- atdf(y, 0.8, plot=FALSE)
par(mfrow=c(2,2))
acf(y, xlab="")
pacf(y, xlab="")
plot(hold2, type="chi")
plot(hold2, type="chibar")

## Not run: 
data(Fort)
atdf(Fort[,5], 0.9)

data(Tphap)
atdf(Tphap$MaxT, 0.8)

data(PORTw)
atdf(PORTw$TMX1, u=0.9)
atdf(PORTw$TMX1, u=0.8)

## End(Not run)
</code></pre>

<hr>
<h2 id='BayesFactor'>
Estimate Bayes Factor
</h2><span id='topic+BayesFactor'></span>

<h3>Description</h3>

<p>Estimate Bayes factor between two models for two &ldquo;fevd&rdquo; objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BayesFactor(m1, m2, burn.in = 499, FUN = "postmode", 
    method = c("laplace", "harmonic"), verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BayesFactor_+3A_m1">m1</code>, <code id="BayesFactor_+3A_m2">m2</code></td>
<td>

<p>objects of class &ldquo;fevd&rdquo; giving the two models to be compared.
</p>
</td></tr>
<tr><td><code id="BayesFactor_+3A_burn.in">burn.in</code></td>
<td>

<p>numeric how many of the first several iterations from the MCMC sample to throw away before estimating the Bayes factor.
</p>
</td></tr>
<tr><td><code id="BayesFactor_+3A_fun">FUN</code></td>
<td>

<p>function to be used to determine the estimated parameter values from the MCMC sample.  With the exception of the default (posterior mode), the function should operate on a matrix and return a vector of length equal to the number of parameters.  If &ldquo;mean&rdquo; is given, then <code>colMeans</code> is actually used.
</p>
</td></tr>
<tr><td><code id="BayesFactor_+3A_method">method</code></td>
<td>

<p>Estimation method to be used.
</p>
</td></tr>
<tr><td><code id="BayesFactor_+3A_verbose">verbose</code></td>
<td>

<p>logical, should progress information be printed to the screen (no longer necessary).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Better options for estimating the Bayes factor from an MCMC sample are planned for the future.  The current options are perhaps the two most common, but do suffer from major drawbacks.  See Kass and Raftery (1995) for a review.
</p>


<h3>Value</h3>

<p>A list object of class &ldquo;htest&rdquo; is returned with components:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>The estimated Bayes factor.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>character string naming which estimation method was used.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>character vector naming the models being compared.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Kass, R. E. and Raftery, A. E. (1995) Bayes factors.  <em>J American Statistical Association</em>, <b>90</b> (430), 773&ndash;795.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fevd">fevd</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(PORTw)
fB &lt;- fevd(TMX1, PORTw, method = "Bayesian", iter = 500)
fB2 &lt;- fevd(TMX1, PORTw, location.fun = ~AOindex,
    method = "Bayesian", iter = 500)

BayesFactor(fB, fB2, burn.in = 100, method = "harmonic")

</code></pre>

<hr>
<h2 id='blockmaxxer'>
Find Block Maxima
</h2><span id='topic+blockmaxxer'></span><span id='topic+blockmaxxer.data.frame'></span><span id='topic+blockmaxxer.fevd'></span><span id='topic+blockmaxxer.matrix'></span><span id='topic+blockmaxxer.vector'></span>

<h3>Description</h3>

<p>Find the block maximum of a data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>blockmaxxer(x, ...)

## S3 method for class 'data.frame'
blockmaxxer(x, ..., which = 1, blocks = NULL,
    blen = NULL, span = NULL)

## S3 method for class 'fevd'
blockmaxxer(x, ...)

## S3 method for class 'matrix'
blockmaxxer(x, ..., which = 1, blocks = NULL,
    blen = NULL, span = NULL)

## S3 method for class 'vector'
blockmaxxer(x, ..., blocks = NULL, blen = NULL,
    span = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="blockmaxxer_+3A_x">x</code></td>
<td>

<p>An object of class &ldquo;fevd&rdquo; where the fit is for a PP model, a numeric vector, matrix or data frame.
</p>
</td></tr>
<tr><td><code id="blockmaxxer_+3A_...">...</code></td>
<td>

<p>optional arguments to <code>max</code>.
</p>
</td></tr>
<tr><td><code id="blockmaxxer_+3A_which">which</code></td>
<td>

<p>number or name of a column indicating for which column to take the block maxima.  Note, this does not take componentwise maxima (as in the multivariate setting).  Instead, it takes the maxima for a single column and returns a vector, data frame or matrix of the block maxima for that column along with the entire row where that maxima occurred.
</p>
</td></tr>
<tr><td><code id="blockmaxxer_+3A_blocks">blocks</code></td>
<td>

<p>numeric (integer or factor) vector indicating the blocks over which to take the maxima.  Must be non-NULL if <code>blen</code> and <code>span</code> are NULL.
</p>
</td></tr>
<tr><td><code id="blockmaxxer_+3A_blen">blen</code></td>
<td>

<p>(optional) may be used instead of the <code>blocks</code> argument, and <code>span</code> must be non-NULL.  This determines the length of the blocks to be created.  Note, the last block may be smaller or larger than <code>blen</code>.  Ignored if <code>blocks</code> is not NULL.
</p>
</td></tr>
<tr><td><code id="blockmaxxer_+3A_span">span</code></td>
<td>

<p>(optional) must be specified if <code>blen</code> is non-NULL and <code>blocks</code> is NULL.  This is the number of blocks over which to take the maxima, and the returned value will be either a vector of length equal to <code>span</code> or a matrix or data frame with <code>span</code> rows.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector of length equal to the number of blocks (vector method) or a matrix or data frame with number of rows equal to the number of blocks (matrix and data frame methods).
</p>
<p>The <code>fevd</code> method is for finding the block maxima of the data passed to a PP model fit and the blocks are determined by the <code>npy</code> and <code>span</code> components of the fitted object.  If the <code>fevd</code> object is not a PP model, the function will error out.  This is useful for utilizing the PP model in the GEV with approximate annual maxima.  Any covariate values that occur contiguous with the maxima are returned as well.
</p>
<p>The <code>aggregate</code> function is used with <code>max</code> in order to take the maxima from each block.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fevd">fevd</a></code>, <code><a href="base.html#topic+max">max</a></code>, <code><a href="stats.html#topic+aggregate">aggregate</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Fort)

bmFort &lt;- blockmaxxer(Fort, blocks = Fort$year, which="Prec")

plot(Fort$year, Fort$Prec, xlab = "Year", ylab = "Precipitation (inches)",
    cex = 1.25, cex.lab = 1.25,
    col = "darkblue", bg = "lightblue", pch = 21)

points(bmFort$year, bmFort$Prec, col="darkred", cex=1.5)

</code></pre>

<hr>
<h2 id='CarcasonneHeat'>
European Climate Assessment and Dataset
</h2><span id='topic+CarcasonneHeat'></span>

<h3>Description</h3>

<p>Blended temperature multiplied by ten (deg Celsius) series of station STAID: 766 in Carcasonne, France.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("CarcasonneHeat")</code></pre>


<h3>Format</h3>

<p>The format is:
int [1:4, 1:12054] 104888 19800101 96 0 104888 19800102 57 0 104888 19800103 ...
</p>


<h3>Details</h3>

<p>European Climate Assessment and Dataset blended temperature (deg Celsius) series of station STAID: 766 in Carcasonne, France.  Blended and updated with sources: 104888 907635.  See Klein Tank et al. (2002) for more information.
</p>
<p>This index was developed by Simone Russo at the European Commission, Joint Research Centre (JRC).
Reports, articles, papers, scientific and non-scientific works of any form, including tables, maps, or any other kind of output, in printed or electronic form, based in whole or in part on the data supplied, must reference to Russo et al. (2014).
</p>


<h3>Author(s)</h3>

<p>Simone Russo &lt;simone.russo@jrc.ec.europa.eu&gt;</p>


<h3>Source</h3>

<p>We acknowledge the data providers in the ECA&amp;D project.
</p>
<p>Klein Tank, A.M.G. and Coauthors, 2002. Daily dataset of 20th-century surface air
temperature and precipitation series for the European Climate Assessment. Int. J. of Climatol.,
22, 1441-1453.
</p>
<p>Data and metadata available at
<a href="https://www.ecad.eu:443/">https://www.ecad.eu:443/</a>
</p>


<h3>References</h3>

<p>Russo, S. and Coauthors, 2014. Magnitude of extreme heat waves in present climate and their projection in a warming world. <em>J. Geophys. Res.</em>, doi:10.1002/2014JD022098.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(CarcasonneHeat)
str(CarcasonneHeat)

# see help file for hwmi for an example using these data.
</code></pre>

<hr>
<h2 id='ci.fevd'>
Confidence Intervals
</h2><span id='topic+ci.fevd'></span><span id='topic+ci.fevd.bayesian'></span><span id='topic+ci.fevd.lmoments'></span><span id='topic+ci.fevd.mle'></span>

<h3>Description</h3>

<p>Confidence intervals for parameters and return levels using fevd objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fevd'
ci(x, alpha = 0.05, type = c("return.level", "parameter"), 
    return.period = 100, which.par, R = 502, ...)

## S3 method for class 'fevd.bayesian'
ci(x, alpha = 0.05, type = c("return.level", "parameter"),
    return.period = 100, which.par = 1, FUN = "mean", burn.in = 499, tscale = FALSE,
    ...)

## S3 method for class 'fevd.lmoments'
ci(x, alpha = 0.05, type = c("return.level", "parameter"),
    return.period = 100, which.par, R = 502, tscale = FALSE,
    return.samples = FALSE, ...)

## S3 method for class 'fevd.mle'
ci(x, alpha = 0.05, type = c("return.level", "parameter"),
    return.period = 100, which.par, R = 502, method = c("normal",
        "boot", "proflik"), xrange = NULL, nint = 20, verbose = FALSE,
    tscale = FALSE, return.samples = FALSE, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ci.fevd_+3A_x">x</code></td>
<td>

<p>list object returned by <code>fevd</code>.
</p>
</td></tr>
<tr><td><code id="ci.fevd_+3A_alpha">alpha</code></td>
<td>

<p>numeric between 0 and 1 giving the desired significance level (i.e., the (1 - <code>alpha</code>) * 100 percent confidence level; so that the default <code>alpha</code> = 0.05 corresponds to a 95 percent confidence level).
</p>
</td></tr>
<tr><td><code id="ci.fevd_+3A_type">type</code></td>
<td>
<p>character specifying if confidence intervals (CIs) are desired for return level(s) (default) or one or more parameter.</p>
</td></tr>
<tr><td><code id="ci.fevd_+3A_return.period">return.period</code></td>
<td>
<p>numeric vector giving the return period(s) for which it is desired to calculate the corresponding return levels.</p>
</td></tr>
<tr><td><code id="ci.fevd_+3A_...">...</code></td>
<td>
<p> optional arguments to the <code>profliker</code> function.  For example, if it is desired to see the plot (recommended), use <code>verbose</code> = TRUE.  </p>
</td></tr>
<tr><td><code id="ci.fevd_+3A_which.par">which.par</code></td>
<td>
<p>numeric giving the index (indices) for which parameter(s) to calculate CIs.  Default is to do all of them.</p>
</td></tr>
<tr><td><code id="ci.fevd_+3A_fun">FUN</code></td>
<td>
<p>character string naming a function to use to estimate the parameters from the MCMC sample.  The function is applied to each column of the <code>results</code> component of the returned <code>fevd</code> object.</p>
</td></tr>
<tr><td><code id="ci.fevd_+3A_burn.in">burn.in</code></td>
<td>
<p>The first <code>burn.in</code> values are thrown out before calculating anything from the MCMC sample.</p>
</td></tr>
<tr><td><code id="ci.fevd_+3A_r">R</code></td>
<td>
<p>the number of bootstrap iterations to do.</p>
</td></tr>
<tr><td><code id="ci.fevd_+3A_method">method</code></td>
<td>
<p> character naming which method for obtaining CIs should be used.  Default (&ldquo;normal&rdquo;) uses a normal approximation, and in the case of return levels (or transformed scale) applies the delta method using the parameter covariance matrix.  Option &ldquo;boot&rdquo; employs a parametric bootstrap that simulates data from the fitted model, and then fits the EVD to each simulated data set to obtain a sample of parameters or return levels.  Currently, only the percentile method of calculating the CIs from the sample is available.  Finally, &ldquo;proflik&rdquo; uses function <code>profliker</code> to calculate the profile-likelihood function for the parameter(s) of interest, and tries to find the upcross level between this function and the appropriate chi-square critical value (see details).
</p>
</td></tr>
<tr><td><code id="ci.fevd_+3A_tscale">tscale</code></td>
<td>
<p>For the GP df, the scale parameter is a function of the shape parameter and the threshold.  When plotting the parameters, for example, against thresholds to find a good threshold for fitting the GP df, it is imperative to transform the scale parameter to one that is independent of the threshold.  In particular, <code>tscale</code> = scale - shape * threshold.</p>
</td></tr>
<tr><td><code id="ci.fevd_+3A_xrange">xrange</code>, <code id="ci.fevd_+3A_nint">nint</code></td>
<td>
<p>arguments to <code>profliker</code> function.</p>
</td></tr>
<tr><td><code id="ci.fevd_+3A_return.samples">return.samples</code></td>
<td>
<p>logical; should the bootstrap samples be returned?  If so, CIs will not be calculated and only the sample of parameters (return levels) are returned.</p>
</td></tr>
<tr><td><code id="ci.fevd_+3A_verbose">verbose</code></td>
<td>

<p>logical; should progress information be printed to the screen?  For profile likelihood method (<code>method</code> = &ldquo;proflik&rdquo;), if TRUE, the profile-likelihood will also be plotted along with a horizontal line through the chi-square critical value.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Confidence Intervals (<code>ci</code>):
</p>
<p><code>ci</code>: The <code>ci</code> method function will take output from <code>fevd</code> and calculate confidence intervals (or credible intervals in the case of Bayesian estimation) in an appropriate manner based on the estimation method.  There is no need for the user to call <code>ci.fevd</code>, <code>ci.fevd.lmoments</code>, <code>ci.fevd.bayesian</code> or <code>ci.fevd.mle</code>; simply use <code>ci</code> and it will access the correct functions.
</p>
<p>Currently, for L-moments, the only method available in this software is to apply a parameteric bootstrap, which is also available for the MLE/GMLE methods.  A parametric bootstrap is performed via the following steps.
</p>
<p>1. Simulate a sample of size n = lenght of the original data from the fitted model.
</p>
<p>2. Fit the EVD to the simulated sample and store the resulting parameter estimates (and perhaps any combination of them, such as return levels).
</p>
<p>3. Repeat steps 1 and 2 many times (to be precise, <code>R</code> times) to obtain a sample from the population df of the parameters (or combinations thereof).
</p>
<p>4. From the sample resulting form the above steps, calculate confidence intervals.  In the present code, the only option is to do this by taking the alpha/2 and 1 - alpha/2 quantiles of the sample (i.e., the percentile method).  However, if one uses <code>return.samples</code> = TRUE, then the sample is returned instead of confidence intervals allowing one to apply some other method if they so desire.
</p>
<p>As far as guidance on how large <code>R</code> should be, it is a trial and error decision.  Usually, one wants the smallest value (to make it as fast as possible) that still yields accurate results.  Generally, this means doing it once with a relatively low number (say <code>R</code> = 100), and then doing it again with a higher number, say <code>R</code> = 250.  If the results are very different, then do it again with an even higher number.  Keep doing this until the results do not change drastically.
</p>
<p>For MLE/GMLE, the normal approximation (perhaps using the delta method, e.g., for return levels) is used if <code>method</code> = &ldquo;normal&rdquo;.  If <code>method</code> = &ldquo;boot&rdquo;, then parametric bootstrap CIs are found.  Finally, if <code>method</code> = &ldquo;profliker&rdquo;, then bounds based on the profile likelihood method are found (see below for more details).
</p>
<p>For Bayesian estimation, the alpha/2 and 1 - alpha/2 percentiles of the resulting MCMC sample (after removing the first <code>burn.in</code> values) are used.  If return levels are desired, then they are first calculated for each MCMC iteration, and the same procedure is applied.  Note that the MCMC samples are availabel in the <code>fevd</code> output for this method, so any other procedure for finding CIs can be done by the savvy user.
</p>
<p>Finding CIs based on the profile-likelihood method:
</p>
<p>The profile likelihood method is often the best method for finding accurate CIs for the shape parameter and for return levels associated with long return periods (where their distribution functions are generally skewed so that, e.g., the normal approximation is not a good approximation).  The profile likelihood for a parameter is obtained by maximizing the likelihood over the other parameters of the model for each of a range (<code>xrange</code>) of values.  An approximation confidence region can be obtained using the deviance function D = 2 * (l(theta.hat) - l_p(theta)), where l(theta.hat) is the likelihood for the original model evaluated at their estimates and l_p(theta) is the likelihood of the parameter of interest (optimized over the remaining parameters), which approximately follows a chi-square df with degrees of freedom equal ot the number of parameters in the model less the one of interest.  The confidence region is then given by
</p>
<p>C_alpha = the set of theta_1 s.t. D &lt;= q,
</p>
<p>where q is the 1 - alpha quantile of the chi-square df with degrees of freedom equal to 1 and theta_1 is the parameter of interest.  If we let m represent the maximum value of the profile likelihood (i.e., m = max(l_p(theta))), then consider a horizontal line through m - q.  All values of theta_1 that yield a profile likelihood value above this horizontal line are within the confidence region, C_alpha (i.e., the range of these values represents the (1 - alpha) * 100 percent CI for the parameter of interest).  For combinations of parameters, such as return levels, the same technique is applied by transforming the parameters in the likelihood to reflect the desired combination.
</p>
<p>To use the profile-likelihood approach, it is necessary to choose an <code>xrange</code> argument that covers the entire confidence interval and beyond (at least a little), and the <code>nint</code> argument may be important here too (this argument gives the number of points to try in fitting a spline function to the profile likelihood, and smaller values curiously tend to be better, but not too small!  Smaller values are also more efficient).  Further, one should really look at the plot of the profile-likelihood to make sure that this is the case, and that resulting CIs are accurately estimated (perhaps using the <code>locator</code> function to be sure).  Nevertheless, an attempt is made to find the limits automatically.  To look at the plot along with the horizontal line, m - q, and vertical lines through the MLE (thin black dashed) and the CIs (thick dashed blue), use the <code>verbose</code> = TRUE argument in the call to <code>ci</code>.  This is not an explicit argument, but available nonetheless (see examples below).
</p>
<p>See any text on EVA/EVT for more details (e.g., Coles 2001; Beirlant et al 2004; de Haan and Ferreira 2006).
</p>


<h3>Value</h3>

<p>Either a numeric vector of length 3 (if only one parameter/return level is used) or a matrix.  In either case, they will have class &ldquo;ci&rdquo;.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Beirlant, J., Goegebeur, Y., Teugels, J. and Segers, J. (2004). <em>Statistics of Extremes: Theory and Applications</em>.  Chichester, West Sussex, England, UK: Wiley, ISBN 9780471976479, 522pp.
</p>
<p>Coles, S.  (2001). <em>An introduction to statistical modeling of extreme values</em>, London: Springer-Verlag.
</p>
<p>de Haan, L. and Ferreira, A. (2006). <em>Extreme Value Theory: An Introduction</em>.  New York, NY, USA: Springer, 288pp.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fevd">fevd</a></code>, <code><a href="#topic+ci.rl.ns.fevd.bayesian">ci.rl.ns.fevd.bayesian</a></code>, <code><a href="distillery.html#topic+ci">ci</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Fort)

fit &lt;- fevd(Prec, Fort, threshold = 2, type = "GP",
    units = "inches", verbose = TRUE)

ci(fit, type = "parameter")

## Not run: 
ci(fit, type = "return.level", method = "proflik",
    xrange = c(3.5,7.75), verbose = TRUE)
# Can check using locator(2).

ci(fit, method = "boot")


## End(Not run)

</code></pre>

<hr>
<h2 id='ci.rl.ns.fevd.bayesian'>
Confidence/Credible Intervals for Effective Return Levels
</h2><span id='topic+ci.rl.ns.fevd.bayesian'></span><span id='topic+ci.rl.ns.fevd.mle'></span>

<h3>Description</h3>

<p>Calculates credible intervals based on the upper and lower alpha/2 quantiles of the MCMC sample for effective return levels from a non-stationary EVD fit using Bayesian estimation, or find normal approximation confidence intervals if estimation method is MLE.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rl.ns.fevd.bayesian'
ci(x, alpha = 0.05, return.period = 100, FUN = "mean",
                 burn.in = 499, ..., qcov = NULL, qcov.base = NULL,
                 verbose = FALSE)

## S3 method for class 'rl.ns.fevd.mle'
ci(x, alpha = 0.05, return.period = 100, method =
                 c("normal"), verbose = FALSE, qcov = NULL, qcov.base =
                 NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ci.rl.ns.fevd.bayesian_+3A_x">x</code></td>
<td>

<p>An object of class &ldquo;fevd&rdquo;.
</p>
</td></tr>
<tr><td><code id="ci.rl.ns.fevd.bayesian_+3A_alpha">alpha</code></td>
<td>

<p>Confidence level (numeric).
</p>
</td></tr>
<tr><td><code id="ci.rl.ns.fevd.bayesian_+3A_return.period">return.period</code></td>
<td>

<p>numeric giving the desired return period.  Must have length one!
</p>
</td></tr>
<tr><td><code id="ci.rl.ns.fevd.bayesian_+3A_fun">FUN</code></td>
<td>

<p>character string naming the function to use to calculate the estimated return levels from the posterior sample (default takes the posterior mean).
</p>
</td></tr>
<tr><td><code id="ci.rl.ns.fevd.bayesian_+3A_burn.in">burn.in</code></td>
<td>

<p>The first <code>burn.in</code> iterations will be removed from the posterior sample before calculating anything.
</p>
</td></tr>
<tr><td><code id="ci.rl.ns.fevd.bayesian_+3A_method">method</code></td>
<td>
<p>Currently only &ldquo;normal&rdquo; method is implemented.</p>
</td></tr>
<tr><td><code id="ci.rl.ns.fevd.bayesian_+3A_verbose">verbose</code></td>
<td>
<p>logical, should progress information be printed to the screen?  Currently not used by the MLE method.</p>
</td></tr>
<tr><td><code id="ci.rl.ns.fevd.bayesian_+3A_...">...</code></td>
<td>

<p>Not used.
</p>
</td></tr>
<tr><td><code id="ci.rl.ns.fevd.bayesian_+3A_qcov">qcov</code>, <code id="ci.rl.ns.fevd.bayesian_+3A_qcov.base">qcov.base</code></td>
<td>

<p>Matrix giving specific covariate values.  <code>qcov.base</code> is used if difference betwen effective return levels for two (or more) sets of covariates is desired, where it is rl(qcov) - rl(qcov.base).  See <code>make.qcov</code> for more details.  If not supplied, effective return levels are calculated for all of the original covariate values used for the fit.  If <code>qcov.base</code> is not NULL but <code>qcov</code> is NULL, then <code>qcov</code> takes on the values of <code>qcov.base</code> and <code>qcov.base</code> is set to NULL, and a warning message is produced.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Return levels are calculated for all coavariates supplied by <code>qcov</code> (and, if desired, <code>qcov.base</code>) for all values of the posterior sample (less the <code>burn.in</code>), or for all values of the original covariates used for the fit (if <code>qcov</code> and <code>qcov.base</code> are NULL).  The estimates aree taken from the sample according to <code>FUN</code> and credible intervals are returned according to <code>alpha</code>.
</p>


<h3>Value</h3>

<p>A three-column matrix is returned with the estimated effective return levels in the middle and lower and upper to the left and right.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>See Also</h3>

<p><code><a href="#topic+make.qcov">make.qcov</a></code>, <code><a href="#topic+fevd">fevd</a></code>, <code><a href="#topic+ci.fevd">ci.fevd</a></code>, <code><a href="#topic+return.level">return.level</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Fort)
fit &lt;- fevd(Prec, threshold = 2, data = Fort,
    location.fun = ~cos(2 * pi * day /365.25),
    type = "PP", verbose = TRUE)

v &lt;- make.qcov(fit, vals=list(mu1 = c(cos(2 * pi * 1 /365.25),
    cos(2 * pi * 120 /365.25), cos(2 * pi * 360 /365.25))))

ci(fit, return.period = 100, qcov = v)

## Not run: 
fit &lt;- fevd(Prec, threshold = 2, data = Fort,
    location.fun = ~cos(2 * day /365.25),
    type = "PP", method = "Bayesian", verbose = TRUE)

ci(fit, return.period = 100, qcov = v)

## End(Not run)
</code></pre>

<hr>
<h2 id='damage'> Hurricane Damage Data</h2><span id='topic+damage'></span>

<h3>Description</h3>

<p>Estimated economic damage (billions USD) caused by hurricanes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(damage)</code></pre>


<h3>Format</h3>

<p>A data frame with 144 observations on the following 3 variables.
</p>

<dl>
<dt>obs</dt><dd><p>a numeric vector that simply gives the line numbers.</p>
</dd>
<dt>Year</dt><dd><p>a numeric vector giving the years in which the specific hurricane occurred.</p>
</dd>
<dt>Dam</dt><dd><p>a numeric vector giving the total estimated economic damage in billions of U.S. dollars.</p>
</dd>
</dl>



<h3>Details</h3>

<p>More information on these data can be found in Pielke and Landsea (1998) or Katz (2002).
</p>


<h3>References</h3>

<p>Katz, R. W. (2002) Stochastic modeling of hurricane damage. <em>Journal of Applied Meteorology</em>, <b>41</b>, 754&ndash;762.
</p>
<p>Pielke, R. A. Jr. and Landsea, C. W. (1998) Normalized hurricane damages in the United States: 1925-95. <em>Weather and Forecasting</em>, <b>13</b>, (3), 621&ndash;631.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(damage)
plot( damage[,1], damage[,3], xlab="", ylab="Economic Damage", type="l", lwd=2)

# Fig. 3 of Katz (2002).
plot( damage[,"Year"], log( damage[,"Dam"]), xlab="Year", ylab="ln(Damage)", ylim=c(-10,5))

# Fig. 4 of Katz (2002).
qqnorm( log( damage[,"Dam"]), ylim=c(-10,5))
</code></pre>

<hr>
<h2 id='datagrabber.declustered'>
Get Original Data from an R Object
</h2><span id='topic+datagrabber.declustered'></span><span id='topic+datagrabber.extremalindex'></span><span id='topic+datagrabber.fevd'></span>

<h3>Description</h3>

<p>Get the original data set used to obtain the resulting R object for which a method function exists.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'declustered'
datagrabber(x, ...)

## S3 method for class 'extremalindex'
datagrabber(x, ...)

## S3 method for class 'fevd'
datagrabber(x, response = TRUE,
    cov.data = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="datagrabber.declustered_+3A_x">x</code></td>
<td>

<p>An R object that has a method function for <code>datagrabber</code>.
</p>
</td></tr>
<tr><td><code id="datagrabber.declustered_+3A_response">response</code>, <code id="datagrabber.declustered_+3A_cov.data">cov.data</code></td>
<td>
<p>logical; should the response data be returned?  Should the covariate data be returned?</p>
</td></tr>
<tr><td><code id="datagrabber.declustered_+3A_...">...</code></td>
<td>

<p>optional arguments to <code>get</code>.  This may eventually become deprecated as scoping gets mixed up, and is currently not actually used.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Accesses the original data set from a fitted <code>fevd</code> object or from declustered data (objects of class &ldquo;declustered&rdquo;) or from <code>extremalindex</code>.
</p>


<h3>Value</h3>

<p>The original pertinent data in whatever form it takes.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>See Also</h3>

<p><code><a href="distillery.html#topic+datagrabber">datagrabber</a></code>, <code><a href="#topic+extremalindex">extremalindex</a></code>, <code><a href="#topic+decluster">decluster</a></code>, <code><a href="#topic+fevd">fevd</a></code>, <code><a href="base.html#topic+get">get</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rnorm(100, mean=40, sd=20)
y &lt;- apply(cbind(y[1:99], y[2:100]), 1, max)
bl &lt;- rep(1:3, each=33)

ydc &lt;- decluster(y, quantile(y, probs=c(0.95)), r=1, blocks=bl)

yorig &lt;- datagrabber(ydc)
all(y - yorig == 0)


</code></pre>

<hr>
<h2 id='decluster'>
Decluster Data Above a Threshold
</h2><span id='topic+decluster'></span><span id='topic+decluster.data.frame'></span><span id='topic+decluster.default'></span><span id='topic+decluster.intervals'></span><span id='topic+decluster.runs'></span><span id='topic+plot.declustered'></span><span id='topic+print.declustered'></span>

<h3>Description</h3>

<p>Decluster data above a given threshold to try to make them independent.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>decluster(x, threshold, ...)

## S3 method for class 'data.frame'
decluster(x, threshold, ..., which.cols, method = c("runs", "intervals"), 
    clusterfun = "max")

## Default S3 method:
decluster(x, threshold, ..., method = c("runs", "intervals"),
    clusterfun = "max")

## S3 method for class 'intervals'
decluster(x, threshold, ..., clusterfun = "max", groups = NULL, replace.with, 
    na.action = na.fail)

## S3 method for class 'runs'
decluster(x, threshold, ..., data, r = 1, clusterfun = "max", groups = NULL, 
    replace.with, na.action = na.fail)

## S3 method for class 'declustered'
plot(x, which.plot = c("scatter", "atdf"), qu = 0.85, xlab = NULL, 
    ylab = NULL, main = NULL, col = "gray", ...)

## S3 method for class 'declustered'
print(x, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="decluster_+3A_x">x</code></td>
<td>

<p>An R data set to be declustered.  Can be a data frame or a numeric vector.  If a data frame, then <code>which.cols</code> must be specified.
</p>
<p><code>plot</code> and <code>print</code>: an object returned by <code>decluster</code>.
</p>
</td></tr>
<tr><td><code id="decluster_+3A_data">data</code></td>
<td>
<p>A data frame containing the data.  </p>
</td></tr>
<tr><td><code id="decluster_+3A_threshold">threshold</code></td>
<td>

<p>numeric of length one or the size of the data over which (non-inclusive) data are to be declustered.
</p>
</td></tr>
<tr><td><code id="decluster_+3A_qu">qu</code></td>
<td>
<p>quantile for <code>u</code> argument in the call to <code>atdf</code>.</p>
</td></tr>
<tr><td><code id="decluster_+3A_which.cols">which.cols</code></td>
<td>
<p>numeric of length one or two.  The first component tells which column is the one to decluster, and the second component tells which, if any, column is to serve as groups.</p>
</td></tr>
<tr><td><code id="decluster_+3A_which.plot">which.plot</code></td>
<td>
<p>character string naming the type of plot to make.</p>
</td></tr>
<tr><td><code id="decluster_+3A_method">method</code></td>
<td>
<p>character string naming the declustering method to employ.</p>
</td></tr>
<tr><td><code id="decluster_+3A_clusterfun">clusterfun</code></td>
<td>
<p>character string naming a function to be applied to the clusters (the returned value is used).  Typically, for extreme value analysis (EVA), this will be the cluster maximum (default), but other options are ok as long as they return a single number.</p>
</td></tr>
<tr><td><code id="decluster_+3A_groups">groups</code></td>
<td>
<p>numeric of length <code>x</code> giving natural groupings that should be considered as separate clusters.  For example, suppose data cover only summer months across several years.  It would probably not make sense to decluster the data across years (i.e., a new cluster should be defined if they occur in different years).</p>
</td></tr>
<tr><td><code id="decluster_+3A_r">r</code></td>
<td>
<p>integer run length stating how many threshold deficits should be used to define a new cluster.</p>
</td></tr>
<tr><td><code id="decluster_+3A_replace.with">replace.with</code></td>
<td>
<p>number, NaN, Inf, -Inf, or NA.  What should the remaining values in the cluster be replaced with?  The default replaces them with <code>threshold</code>, which for most EVA purposes is ideal.</p>
</td></tr>
<tr><td><code id="decluster_+3A_na.action">na.action</code></td>
<td>
<p>function to be called to handle missing values.</p>
</td></tr>
<tr><td><code id="decluster_+3A_xlab">xlab</code>, <code id="decluster_+3A_ylab">ylab</code>, <code id="decluster_+3A_main">main</code>, <code id="decluster_+3A_col">col</code></td>
<td>
<p>optioal arguments to the <code>plot</code> function.  If not used, then reasonable default values are used.</p>
</td></tr>
<tr><td><code id="decluster_+3A_...">...</code></td>
<td>

<p>optional arguments to <code>decluster.runs</code> or <code>clusterfun</code>.
</p>
<p><code>plot</code>: optional arguments to <code>plot</code>.
</p>
<p>Not used by <code>print</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Runs declustering (see Coles, 2001 sec. 5.3.2): Extremes separated by fewer than <code>r</code> non-extremes belong to the same cluster.
</p>
<p>Intervals declustering (Ferro and Segers, 2003): Extremes separated by fewer than <code>r</code> non-extremes belong to the same cluster, where <code>r</code> is the nc-th largest interexceedance time and nc, the number of clusters, is estimated from the extremal index, theta, and the times between extremes. Setting theta = 1 causes each extreme to form a separate cluster.
</p>
<p>The print statement will report the resulting extremal index estimate based on either the runs or intervals estimate depending on the <code>method</code> argument as well as the number of clusters and run length.  For runs declustering, the run length is the same as the argument given by the user, and for intervals method, it is an estimated run length for the resulting declustered data.  Note that if the declustered data are independent, the extremal index should be close to one (if not equal to 1).
</p>


<h3>Value</h3>

<p>A numeric vector of class &ldquo;declustered&rdquo; is returned with various attributes including:
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>the function call.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>character string giving the name of the data.</p>
</td></tr>
<tr><td><code>decluster.function</code></td>
<td>
<p>value of <code>clusterfun</code> argument.  This is a function.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>character string naming the method.  Same as input argument.</p>
</td></tr>
<tr><td><code>threshold</code></td>
<td>
<p>threshold used for declustering.</p>
</td></tr>
<tr><td><code>groups</code></td>
<td>
<p>character string naming the data used for the groups when applicable.</p>
</td></tr>
<tr><td><code>run.length</code></td>
<td>
<p>the run length used (or estimated if &ldquo;intervals&rdquo; method employed).</p>
</td></tr>
<tr><td><code>na.action</code></td>
<td>
<p>function used to handle missing values.  Same as input argument.</p>
</td></tr>
<tr><td><code>clusters</code></td>
<td>
<p>muneric giving the clusters of threshold exceedances.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Coles, S.  (2001) <em>An introduction to statistical modeling of extreme values</em>, London, U.K.: Springer-Verlag, 208 pp.
</p>
<p>Ferro, C. A. T. and Segers, J. (2003). Inference for clusters of extreme values. <em>Journal of the Royal Statistical Society B</em>, <b>65</b>, 545&ndash;556.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+extremalindex">extremalindex</a></code>, <code><a href="distillery.html#topic+datagrabber">datagrabber</a></code>, <code><a href="#topic+fevd">fevd</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rnorm(100, mean=40, sd=20)
y &lt;- apply(cbind(y[1:99], y[2:100]), 1, max)
bl &lt;- rep(1:3, each=33)

ydc &lt;- decluster(y, quantile(y, probs=c(0.75)), r=1, groups=bl)
ydc

plot(ydc)

## Not run: 
look &lt;- decluster(-Tphap$MinT, threshold=-73)
look
plot(look)

# The code cannot currently grab data of the type of above.
# Better:
y &lt;- -Tphap$MinT
look &lt;- decluster(y, threshold=-73)
look
plot(look)

# Even better.  Use a non-constant threshold.
u &lt;- -70 - 7 *(Tphap$Year - 48)/42
look &lt;- decluster(y, threshold=u)
look
plot(look)

# Better still: account for the fact that there are huge
# gaps in data from one year to another.
bl &lt;- Tphap$Year - 47
look &lt;- decluster(y, threshold=u, groups=bl)
look
plot(look)


# Now try the above with intervals declustering and compare 
look2 &lt;- decluster(y, threshold=u, method="intervals", groups=bl)
look2
dev.new()
plot(look2)
# Looks about the same,
# but note that the run length is estimated to be 5.
# Same resulting number of clusters, however.
# May result in different estimate of the extremal
# index.


#
fit &lt;- fevd(look, threshold=u, type="GP", time.units="62/year")
fit
plot(fit)

# cf.
fit2 &lt;- fevd(-MinT~1, Tphap, threshold=u, type="GP", time.units="62/year")
fit2
dev.new()
plot(fit2)

#
fit &lt;- fevd(look, threshold=u, type="PP", time.units="62/year")
fit
plot(fit)

# cf.
fit2 &lt;- fevd(-MinT~1, Tphap, threshold=u, type="PP", time.units="62/year")
fit2
dev.new()
plot(fit2)



## End(Not run)
</code></pre>

<hr>
<h2 id='Denmint'> Denver Minimum Temperature</h2><span id='topic+Denmint'></span>

<h3>Description</h3>

<p>Daily minimum temperature (degrees centigrade) for Denver, Colorado from 1949 through 1999.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Denmint)</code></pre>


<h3>Format</h3>

<p>A data frame with 18564 observations on the following 5 variables.
</p>

<dl>
<dt>Time</dt><dd><p>a numeric vector indicating the line number (time from first entry to the last).</p>
</dd>
<dt>Year</dt><dd><p>a numeric vector giving the year.</p>
</dd>
<dt>Mon</dt><dd><p>a numeric vector giving the month of each year.</p>
</dd>
<dt>Day</dt><dd><p>a numeric vector giving the day of the month.</p>
</dd>
<dt>Min</dt><dd><p>a numeric vector giving the minimum temperature in degrees Fahrenheit.</p>
</dd>
</dl>



<h3>Source</h3>

<p>Originally, the data came from the Colorado Climate Center at Colorado State University.  The Colorado state climatologist office no longer provides these data without charge. They can be obtained from the NOAA/NCDC web site, but there are slight differences (i.e., some missing values for temperature).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Denmint)
plot( Denmint[,3], Denmint[,5], xlab="", xaxt="n", ylab="Minimum Temperature (deg. F)")
axis(1,at=1:12,labels=c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"))
</code></pre>

<hr>
<h2 id='Denversp'> Denver July hourly precipitation amount.</h2><span id='topic+Denversp'></span>

<h3>Description</h3>

<p>Hourly precipitation (mm) for Denver, Colorado in the month of July from 1949 to 1990.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Denversp)</code></pre>


<h3>Format</h3>

<p>A data frame with 31247 observations on the following 4 variables.
</p>

<dl>
<dt>Year</dt><dd><p>a numeric vector giving the number of years from 1900.</p>
</dd>
<dt>Day</dt><dd><p>a numeric vector giving the day of the month.</p>
</dd>
<dt>Hour</dt><dd><p>a numeric vector giving the hour of the day (1 to 24).</p>
</dd>
<dt>Prec</dt><dd><p>a numeric vector giving the precipitation amount (mm).</p>
</dd>
</dl>



<h3>Details</h3>

<p>These observations are part of an hourly precipitation dataset for the United States that has been critically
assessed by Collander et al. (1993).  The Denver hourly precipitation dataset is examined further by Katz and
Parlange (1995).  Summer precipitation in this region near the eastern edge of the Rocky Mountains is predominantly
of local convective origin (Katz and Parlange (1005)).
</p>


<h3>Source</h3>

<p>Katz, R. W. and Parlange, M. B. (1995) Generalizations of chain-dependent processes: Application to hourly precipitation, <em>Water Resources Research</em> <b>31</b>, (5), 1331&ndash;1341.
</p>


<h3>References</h3>

<p>Collander, R. S., Tollerud, E. I., Li, L., and Viront-Lazar, A. (1993) Hourly precipitation data and station histories: A research assessment, in <em>Preprints, Eighth Symposium on Meteorological Observations and Instrumentation, American Meteorological Society</em>, Boston, 153&ndash;158.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Denversp)
plot( Denversp[,1], Denversp[,4], xlab="", ylab="Hourly precipitation (mm)", xaxt="n")
axis(1,at=c(50,60,70,80,90),labels=c("1950","1960","1970","1980","1990"))
</code></pre>

<hr>
<h2 id='devd'>
Extreme Value Distributions
</h2><span id='topic+devd'></span><span id='topic+pevd'></span><span id='topic+qevd'></span><span id='topic+revd'></span>

<h3>Description</h3>

<p>Density, distribution function (df), quantile function and random generation for the generalized extreme value and generalized Pareto distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>devd(x, loc = 0, scale = 1, shape = 0, threshold = 0, log = FALSE,
    type = c("GEV", "GP"))

pevd(q, loc = 0, scale = 1, shape = 0, threshold = 0, lambda = 1, 
    npy, type = c("GEV", "GP", "PP", "Gumbel", "Frechet", "Weibull", 
        "Exponential", "Beta", "Pareto"), lower.tail = TRUE, log.p = FALSE)

qevd(p, loc = 0, scale = 1, shape = 0, threshold = 0,
    type = c("GEV", "GP", "PP", "Gumbel", "Frechet", "Weibull", "Exponential", "Beta",
    "Pareto"), lower.tail = TRUE)

revd(n, loc = 0, scale = 1, shape = 0, threshold = 0,
    type = c("GEV", "GP"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="devd_+3A_x">x</code>, <code id="devd_+3A_q">q</code></td>
<td>
<p>numeric vector of quantiles.</p>
</td></tr>
<tr><td><code id="devd_+3A_p">p</code></td>
<td>
<p>numeric vector of probabilities.  Must be between 0 and 1 (non-inclusive).</p>
</td></tr>
<tr><td><code id="devd_+3A_n">n</code></td>
<td>
<p>number of observations to draw.</p>
</td></tr>
<tr><td><code id="devd_+3A_npy">npy</code></td>
<td>
<p>Number of points per period (period is usually year).  Currently not used.</p>
</td></tr>
<tr><td><code id="devd_+3A_lambda">lambda</code></td>
<td>
<p>Event frequency base rate.  Currently not used.</p>
</td></tr>
<tr><td><code id="devd_+3A_loc">loc</code>, <code id="devd_+3A_scale">scale</code>, <code id="devd_+3A_shape">shape</code></td>
<td>
<p>location, scale and shape parameters.  Each may be a vector of same length as <code>x</code> (<code>devd</code> or length <code>n</code> for <code>revd</code>.  Must be length 1 for <code>pevd</code> and <code>qevd</code>.</p>
</td></tr>
<tr><td><code id="devd_+3A_threshold">threshold</code></td>
<td>
<p>numeric giving the threshold for the GP df.  May be  a vector of same length as <code>x</code> (<code>devd</code> or length <code>n</code> for <code>revd</code>.  Must be length 1 for <code>pevd</code> and <code>qevd</code>.</p>
</td></tr>
<tr><td><code id="devd_+3A_log">log</code>, <code id="devd_+3A_log.p">log.p</code></td>
<td>
<p>logical; if TRUE, probabilites p are given as log(p).</p>
</td></tr>
<tr><td><code id="devd_+3A_lower.tail">lower.tail</code></td>
<td>
<p>logical; if TRUE (default), probabilities are P[X &lt;= x] otherwise, P[X &gt; x].</p>
</td></tr>
<tr><td><code id="devd_+3A_type">type</code></td>
<td>
<p>character; one of &quot;GEV&quot; or &quot;GP&quot; describing whether to use the GEV or GP.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The extreme value distributions (EVD's) are generalized extreme value (GEV) or generalized Pareto (GP); if type is &ldquo;PP&rdquo;, then <code>pevd</code> changes it to &ldquo;GEV&rdquo;.  The point process characterization is an equivalent form, but is not handled here.  The GEV df is given by
</p>
<p>Pr(X &lt;= x) = G(x) = exp[-(1 + shape * (x - location)/scale)^(-1/shape)]
</p>
<p>for 1 + shape*(x - location) &gt; 0 and scale &gt; 0.  It the shape parameter is zero, then the df is defined by continuity and simplies to
</p>
<p>G(x) = exp(-exp((x - location)/scale)).
</p>
<p>The GEV df is often called a family of df's because it encompasses the three types of EVD's: Gumbel (shape = 0, light tail), Frechet (shape &gt; 0, heavy tail) and the reverse Weibull (shape &lt; 0, bounded upper tail at location - scale/shape).  It was first found by R. von Mises (1936) and also independently noted later by meteorologist A. F. Jenkins (1955).  It enjoys theretical support for modeling maxima taken over large blocks of a series of data.
</p>
<p>The generalized Pareo df is given by (Pickands, 1975)
</p>
<p>Pr(X &lt;= x) = F(x) = 1 - [1 + shape * (x - threshold)/scale]^(-1/shape)
</p>
<p>where 1 + shape * (x - threshold)/scale &gt; 0, scale &gt; 0, and x &gt; threshold.  If shape = 0, then the GP df is defined by continuity and becomes
</p>
<p>F(x) = 1 - exp(-(x - threshold)/scale).
</p>
<p>There is an approximate relationship between the GEV and GP df's where the GP df is approximately the tail df for the GEV df.  In particular, the scale parameter of the GP is a function of the threshold (denote it scale.u), and is equivalent to scale + shape*(threshold - location) where scale, shape and location are parameters from the &ldquo;equivalent&rdquo; GEV df.  Similar to the GEV df, the shape parameter determines the tail behavior, where shape = 0 gives rise to the exponential df (light tail), shape &gt; 0 the Pareto df (heavy tail) and shape &lt; 0 the Beta df (bounded upper tail at location - scale.u/shape).  Theoretical justification supports the use of the GP df family for modeling excesses over a high threshold (i.e., y = x - threshold).  It is assumed here that <code>x</code>, <code>q</code> describe x (not y = x - threshold).  Similarly, the random draws are y + threshold.
</p>
<p>See Coles (2001) and Reiss and Thomas (2007) for a very accessible text on extreme value analysis and for more theoretical texts, see for example, Beirlant et al. (2004), de Haan and Ferreira (2006), as well as Reiss and Thomas (2007).
</p>


<h3>Value</h3>

<p>'devd' gives the density function, 'pevd' gives the distribution function, 'qevd' gives the quantile function, and 'revd' generates random deviates for the GEV or GP df depending on the type argument.
</p>


<h3>Note</h3>

<p>There is a similarity between the location parameter of the GEV df and the threshold for the GP df.  For clarity, two separate arguments are emplyed here to distinguish the two instead of, for example, just using the location parameter to describe both.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Beirlant, J., Goegebeur, Y., Teugels, J. and Segers, J. (2004) <em>Statistics of Extremes: Theory and Applications</em>.  Chichester, West Sussex, England, UK: Wiley, ISBN 9780471976479, 522pp.
</p>
<p>Coles, S.  (2001) <em>An introduction to statistical modeling of extreme values</em>, London, U.K.: Springer-Verlag, 208 pp.
</p>
<p>de Haan, L. and Ferreira, A. (2006) <em>Extreme Value Theory: An Introduction</em>.  New York, NY, USA: Springer, 288pp.
</p>
<p>Jenkinson, A. F. (1955) The frequency distribution of the annual maximum (or minimum) of meteorological elements. <em>Quart. J. R.  Met. Soc.</em>, <b>81</b>, 158&ndash;171.
</p>
<p>Pickands, J. (1975) Statistical inference using extreme order statistics.  <em>Annals of Statistics</em>, <b>3</b>, 119&ndash;131.
</p>
<p>Reiss, R.-D. and Thomas, M. (2007) <em>Statistical Analysis of Extreme Values: with applications to insurance, finance, hydrology and other fields</em>. Birkhauser, 530pp., 3rd edition.
</p>
<p>von Mises, R. (1936) La distribution de la plus grande de n valeurs, <em>Rev. Math. Union Interbalcanique</em> <b>1</b>, 141&ndash;160.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fevd">fevd</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## GEV df (Frechet type)
devd(2:4, 1, 0.5, 0.8) # pdf
pevd(2:4, 1, 0.5, 0.8) # cdf 
qevd(seq(1e-8,1-1e-8,,20), 1, 0.5, 0.8) # quantiles
revd(10, 1, 0.5, 0.8) # random draws

## GP df
devd(2:4, scale=0.5, shape=0.8, threshold=1, type="GP")
pevd(2:4, scale=0.5, shape=0.8, threshold=1, type="GP")
qevd(seq(1e-8,1-1e-8,,20), scale=0.5, shape=0.8, threshold=1, type="GP")
revd(10, scale=0.5, shape=0.8, threshold=1, type="GP")

## Not run: 
# The fickleness of extremes.
z1 &lt;- revd(100, 1, 0.5, 0.8)
hist(z1, breaks="FD", freq=FALSE, xlab="GEV distributed random variables", col="darkblue")
lines(seq(0,max(z1),,200), devd(seq(0,max(z1),,200), 1, 0.5, 0.8), lwd=1.5, col="yellow")
lines(seq(0,max(z1),,200), devd(seq(0,max(z1),,200), 1, 0.5, 0.8), lwd=1.5, lty=2) 

z2 &lt;- revd(100, 1, 0.5, 0.8)
qqplot(z1, z2)

z3 &lt;- revd(100, scale=0.5, shape=0.8, threshold=1, type="GP")
# Or, equivalently
z4 &lt;- revd(100, 5, 0.5, 0.8, 1, type="GP") # the "5" is ignored.
qqplot(z3, z4)

# Just for fun.
qqplot(z1, z3)

# Compare
par(mfrow=c(2,2))
plot(density(z1), xlim=c(0,100), ylim=c(0,1))
plot(density(z2), xlim=c(0,100), ylim=c(0,1))
plot(density(z3), xlim=c(0,100), ylim=c(0,1))
plot(density(z4), xlim=c(0,100), ylim=c(0,1))

# Three types
x &lt;- seq(0,10,,200)
par(mfrow=c(1,2))
plot(x, devd(x, 1, 1, -0.5), type="l", col="blue", lwd=1.5,
    ylab="GEV df")
# Note upper bound at 1 - 1/(-0.5) = 3 in above plot.

lines(x, devd(x, 1, 1, 0), col="lightblue", lwd=1.5)
lines(x, devd(x, 1, 1, 0.5), col="darkblue", lwd=1.5)
legend("topright", legend=c("(reverse) Weibull", "Gumbel", "Frechet"),
    col=c("blue", "lightblue", "darkblue"), bty="n", lty=1, lwd=1.5)

plot(x, devd(x, 1, 1, -0.5, 1, type="GP"), type="l", col="blue", lwd=1.5,
    ylab="GP df")
lines(x, devd(x, 1, 1, 0, 1, type="GP"), col="lightblue", lwd=1.5)
lines(x, devd(x, 1, 1, 0.5, 1, type="GP"), col="darkblue", lwd=1.5)
legend("topright", legend=c("Beta", "Exponential", "Pareto"), 
    col=c("blue", "lightblue", "darkblue"), bty="n", lty=1, lwd=1.5)

# Emphasize the tail differences more by using different scale parameters.
par(mfrow=c(1,2))
plot(x, devd(x, 1, 0.5, -0.5), type="l", col="blue", lwd=1.5,
    ylab="GEV df")
lines(x, devd(x, 1, 1, 0), col="lightblue", lwd=1.5)
lines(x, devd(x, 1, 2, 0.5), col="darkblue", lwd=1.5)
legend("topright", legend=c("(reverse) Weibull", "Gumbel", "Frechet"), 
    col=c("blue", "lightblue", "darkblue"), bty="n", lty=1, lwd=1.5)

plot(x, devd(x, 1, 0.5, -0.5, 1, type="GP"), type="l", col="blue", lwd=1.5,
    ylab="GP df")
lines(x, devd(x, 1, 1, 0, 1, type="GP"), col="lightblue", lwd=1.5)
lines(x, devd(x, 1, 2, 0.5, 1, type="GP"), col="darkblue", lwd=1.5)
legend("topright", legend=c("Beta", "Exponential", "Pareto"),
    col=c("blue", "lightblue", "darkblue"), bty="n", lty=1, lwd=1.5)


## End(Not run)
</code></pre>

<hr>
<h2 id='distill.fevd'>
Distill Parameter Information
</h2><span id='topic+distill.fevd'></span><span id='topic+distill.fevd.bayesian'></span><span id='topic+distill.fevd.lmoments'></span><span id='topic+distill.fevd.mle'></span>

<h3>Description</h3>

<p>Distill parameter information (and possibly other pertinent inforamtion) from fevd objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fevd'
distill(x, ...)

## S3 method for class 'fevd.bayesian'
distill(x, cov = TRUE, FUN = "mean", burn.in = 499, ...)

## S3 method for class 'fevd.lmoments'
distill(x, ...)

## S3 method for class 'fevd.mle'
distill(x, cov = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="distill.fevd_+3A_x">x</code></td>
<td>

<p>list object returned by <code>fevd</code>.
</p>
</td></tr>
<tr><td><code id="distill.fevd_+3A_...">...</code></td>
<td>

<p>Not used.
</p>
</td></tr>
<tr><td><code id="distill.fevd_+3A_cov">cov</code></td>
<td>
<p>logical; should the parameter covariance be returned with the parameters (if TRUE, they are returned as a vector concatenated to the end of the returned value).</p>
</td></tr>
<tr><td><code id="distill.fevd_+3A_fun">FUN</code></td>
<td>
<p>character string naming a function to use to estimate the parameters from the MCMC sample.  The function is applied to each column of the <code>results</code> component of the returned <code>fevd</code> object.</p>
</td></tr>
<tr><td><code id="distill.fevd_+3A_burn.in">burn.in</code></td>
<td>
<p>The first <code>burn.in</code> values are thrown out before calculating anything from the MCMC sample.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Obtaining just the basic information from the fits:
</p>
<p><code>distill</code>: The <code>distill</code> method function works on <code>fevd</code> output to obtain only pertinent information and output it in a very user-friendly format (i.e., a single vector).  Mostly, this simply means returning the parameter estimates, but for some methods, more information (e.g., the optimized negative log-likelihood value and parameter covariances) can also be returned.  In the case of the parameter covariances (returned if <code>cov</code> = TRUE), if np is the number of parameters in the model, the covariance matrix can be obtained by peeling off the last np^2 values of the vector, call it v, and using v &lt;- matrix(v, np, np).
</p>
<p>As with <code>ci</code>, only <code>distill</code> need be called by the user.  The appropriate choice of the other functions is automatically determined from the <code>fevd</code> fitted object.
</p>


<h3>Value</h3>

<p>numeric vector giving the parameter values, and if estimation method is MLE/GMLE, then the negative log-likelihood.  If the estimation method is MLE/GMLE or Bayesian, then the parameter covariance values (collapsed with <code>c</code>) are concatenated to the end as well.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fevd">fevd</a></code>, <code><a href="#topic+ci.fevd">ci.fevd</a></code>, <code><a href="distillery.html#topic+distill">distill</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Fort)

fit &lt;- fevd(Prec, Fort, threshold=0.395, type="PP", units="inches", verbose=TRUE)
fit

distill(fit)

</code></pre>

<hr>
<h2 id='erlevd'>
Effective Return Levels
</h2><span id='topic+erlevd'></span>

<h3>Description</h3>

<p>Find the so-called effective return levels for non-stationary extreme value distributions (EVDs).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>erlevd(x, period = 100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="erlevd_+3A_x">x</code></td>
<td>

<p>A list object of class &ldquo;fevd&rdquo;.
</p>
</td></tr>
<tr><td><code id="erlevd_+3A_period">period</code></td>
<td>

<p>number stating for what return period the effective return levels should be calculated.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Return levels are the same as the quantiles for the GEV df.  For the GP
df, they are very similar to the quantiles, but with the event frequency
taken into consideration.  Effective return levels are the return levels
obtained for given parameter/threshold values of a non-stationary model.
For example, suppose the df for data are modeled as a GEV(location(t) =
mu0 + mu1 * t, scale, shape), where &lsquo;t&rsquo; is time.  Then for any
specific given time, &lsquo;t&rsquo;, return levels can be found.  This is
done for each value of the covariate(s) used to fit the model to the
data.  See, for example, Gilleland and Katz (2011) for more details. 
</p>
<p>This function is called by the <code>plot</code> method function for &ldquo;fevd&rdquo; objects when the models are non-stationary.
</p>


<h3>Value</h3>

<p>A vector of length equal to the length of the data used to obtain the
fit. When <code>x</code> is from a PP fit with blocks, a vector of length
equal to the number of blocks.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Gilleland, E. and Katz, R. W. (2011). New software to analyze how extremes change over time. <em>Eos</em>, 11 January, <b>92</b>, (2), 13&ndash;14.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fevd">fevd</a></code>, <code><a href="#topic+rlevd">rlevd</a></code>, <code><a href="#topic+rextRemes">rextRemes</a></code>, <code><a href="#topic+pextRemes">pextRemes</a></code>, <code><a href="#topic+plot.fevd">plot.fevd</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(PORTw)

fit &lt;- fevd(TMX1, PORTw, location.fun=~AOindex, units="deg C")
fit
tmp &lt;- erlevd(fit, period=20)

## Not run: 
# Currently, the ci function does not work for effective
# return levels.  There were coding issues encountered.
# But, could try:
#
z &lt;- rextRemes(fit, n=500)
dim(z)
# 500 randomly drawn samples from the
# fitted model.  Each row is a sample
# of data from the fitted model of the
# same length as the data.  Each column
# is a separate sample.

sam &lt;- numeric(0)
for( i in 1:500) {
    cat(i, " ")
    dat &lt;- data.frame(z=z[,i], AOindex=PORTw$AOindex)
    res &lt;- fevd(z, dat, location.fun=~AOindex)
    sam &lt;- cbind(sam, c(erlevd(res)))
}
cat("\n")

dim(sam)

a &lt;- 0.05
res &lt;- apply(sam, 1, quantile, probs=c(a/2, 1 - a/2))
nm &lt;- rownames(res)

res &lt;- cbind(res[1,], tmp, res[2,])
colnames(res) &lt;- c(nm[1], "Estimated 20-year eff. ret. level", nm[2])
res


## End(Not run)
</code></pre>

<hr>
<h2 id='extremalindex'>
Extemal Index
</h2><span id='topic+extremalindex'></span><span id='topic+ci.extremalindex'></span><span id='topic+print.extremalindex'></span>

<h3>Description</h3>

<p>Estimate the extremal index.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extremalindex(x, threshold, method = c("intervals", "runs"), run.length = 1,
    na.action = na.fail, ...)

## S3 method for class 'extremalindex'
ci(x, alpha = 0.05, R = 502, return.samples = FALSE, ...)

## S3 method for class 'extremalindex'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extremalindex_+3A_x">x</code></td>
<td>

<p>A data vector.
</p>
<p><code>ci</code> and <code>print</code>: output from <code>extremalindex</code>.
</p>
</td></tr>
<tr><td><code id="extremalindex_+3A_threshold">threshold</code></td>
<td>

<p>numeric of length one or the length of <code>x</code> giving the value above which (non-inclusive) the extremal index should be calculated.
</p>
</td></tr>
<tr><td><code id="extremalindex_+3A_method">method</code></td>
<td>

<p>character string stating which method should be used to estimate the extremal index.
</p>
</td></tr>
<tr><td><code id="extremalindex_+3A_run.length">run.length</code></td>
<td>

<p>For runs declustering only, an integer giving the number of threshold deficits to be considered as starting a new cluster.
</p>
</td></tr>
<tr><td><code id="extremalindex_+3A_na.action">na.action</code></td>
<td>

<p>function to handle missing values.
</p>
</td></tr>
<tr><td><code id="extremalindex_+3A_alpha">alpha</code></td>
<td>
<p>number between zero and one giving the (1 - alpha) * 100 percent confidence level.  For example, alpha = 0.05 corresponds to 95 percent confidence; alpha is the significance level (or probability of type I errors) for hypothesis tests based on the CIs.</p>
</td></tr>
<tr><td><code id="extremalindex_+3A_r">R</code></td>
<td>
<p>Number of replicate samples to use in the bootstrap procedure.</p>
</td></tr>
<tr><td><code id="extremalindex_+3A_return.samples">return.samples</code></td>
<td>
<p>logical; if TRUE, the bootstrap replicate samples will be returned instead of CIs.  This is useful, for example, if one wishes to find CIs using a better method than the one used here (percentile method).</p>
</td></tr>
<tr><td><code id="extremalindex_+3A_...">...</code></td>
<td>

<p>optional arguments to <code>decluster</code>.  Not used by <code>ci</code> or <code>print</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The extremal index is a useful indicator of how much clustering of exceedances of a threshold occurs in the limit of the distribution.  For independent data, theta = 1, (though the converse is does not hold) and if theta &lt; 1, then there is some dependency (clustering) in the limit.
</p>
<p>There are many possible estimators of the extremal index.  The ones used here are runs declustering (e.g., Coles, 2001 sec. 5.3.2) and the intervals estimator described in Ferro and Segers (2003).  It is unbiased in the mean and can be used to estimate the number of clusters, which is also done by this function.
</p>


<h3>Value</h3>

<p>A numeric vector of length three and class &ldquo;extremalindex&rdquo; is returned giving the estimated extremal index, the number of clusters and the run length.  Also has attributes including:
</p>
<table>
<tr><td><code>cluster</code></td>
<td>
<p>the resulting clusters.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>Same as argument above.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>character vector giving the name of the data used, and possibly the data frame or matrix and column name, if applicable. </p>
</td></tr>
<tr><td><code>data.call</code></td>
<td>
<p>character string giving the actual argument passed in for x.  May be the same as data.name.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the function call.</p>
</td></tr>
<tr><td><code>na.action</code></td>
<td>
<p>function used for handling missing values.  Same as argument above.</p>
</td></tr>
<tr><td><code>threshold</code></td>
<td>
<p>the threshold used.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Coles, S.  (2001) <em>An introduction to statistical modeling of extreme values</em>, London, U.K.: Springer-Verlag, 208 pp.
</p>
<p>Ferro, C. A. T. and Segers, J. (2003). Inference for clusters of extreme values. <em>Journal of the Royal Statistical Society B</em>, <b>65</b>, 545&ndash;556.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+decluster">decluster</a></code>, <code><a href="#topic+fevd">fevd</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Fort)

extremalindex(Fort$Prec, 0.395, method="runs", run.length=9, blocks=Fort$year)

## Not run: 
tmp &lt;- extremalindex(Fort$Prec, 0.395, method="runs", run.length=9, blocks=Fort$year)
tmp
ci(tmp)

tmp &lt;- extremalindex(Fort$Prec, 0.395, method="intervals", run.length=9, blocks=Fort$year)
tmp
ci(tmp)


## End(Not run)
</code></pre>

<hr>
<h2 id='extRemes+20internal'>
extRemes Internal and Secondary Functions
</h2><span id='topic+strip.fevd.mle'></span><span id='topic+strip.fevd.lmoments'></span><span id='topic+strip.fevd.bayesian'></span><span id='topic+consecDayMax'></span><span id='topic+cut366thDay'></span><span id='topic+densplot.evd'></span><span id='topic+eeplot'></span><span id='topic+plot.ee'></span><span id='topic+fevdPriorDefault'></span><span id='topic+fevdProposalDefault'></span><span id='topic+grlevd'></span><span id='topic+grlevdTracer'></span><span id='topic+histplot.evd'></span><span id='topic+hwmiFun'></span><span id='topic+hwmidFun'></span><span id='topic+hwyear'></span><span id='topic+initializer'></span><span id='topic+initializer.lmoments'></span><span id='topic+initializer.moms'></span><span id='topic+initializer.mle'></span><span id='topic+mvThreshold'></span><span id='topic+oevd'></span><span id='topic+oevdgen'></span><span id='topic+oevd.profpar'></span><span id='topic+percentilefun'></span><span id='topic+plot.xtibbed'></span><span id='topic+plot.grlevdTracer'></span><span id='topic+print.xtibbed'></span><span id='topic+probprob.plot.evd'></span><span id='topic+quantilefun'></span><span id='topic+quantquant.plot.evd'></span><span id='topic+quantquant2.plot.evd'></span><span id='topic+rl.fevd'></span><span id='topic+rlgrad.fevd'></span><span id='topic+rlplot.evd'></span><span id='topic+rlvar'></span><span id='topic+setup.design'></span><span id='topic+setup.design.default'></span><span id='topic+setup.design.fevd'></span><span id='topic+shapePriorBeta'></span><span id='topic+shapePriorBetaOld'></span><span id='topic+tf'></span><span id='topic+tformRankFrechet'></span><span id='topic+will.accept'></span><span id='topic+xbooterBM'></span><span id='topic+xbooterPOT'></span>

<h3>Description</h3>

<p>Listed below are supporting functions for the major methods in extRemes.
</p>

<hr>
<h2 id='extRemes-package'>extRemes &ndash; Weather and Climate Applications of Extreme Value Analysis (EVA)</h2><span id='topic+extRemes-package'></span><span id='topic+extRemes'></span>

<h3>Description</h3>

<p><span class="pkg">extRemes</span> is a suite of functions for carrying out analyses on the extreme values of a process of interest; be they block maxima over long blocks or excesses over a high threshold.
</p>
<p>Versions &gt;= 2.0-0 of this package differ considerably from the original package (versions &lt;= 1.65), which was largely a package of graphical user interfaces (GUIs) mostly calling functions from the <span class="pkg">ismev</span> package; a companion software package to Coles (2001).  The former GUI windows of <span class="pkg">extRemes</span> (&lt;= 1.65) now run the command-line functions of <span class="pkg">extRemes</span> (&gt;= 2.0) and have been moved to a new package called <span class="pkg">in2extRemes</span>.
</p>
<p>For assistance using <span class="pkg">extRemes</span> (&gt;= 2.0-0), please see the tutorial at:
</p>
<p><a href="https://doi.org/10.18637/jss.v072.i08">doi:10.18637/jss.v072.i08</a> 
</p>
<p>Extreme Value Statistics:
</p>
<p>Extreme value statistics are used primarily to quantify the stochastic behavior of a process at unusually large (or small) values. Particularly, such analyses usually require estimation of the probability of events that are more extreme than any previously observed. Many fields have begun to use extreme value theory and some have been using it for a very long time including meteorology, hydrology, finance and ocean wave modeling to name just a few.  See Gilleland and Katz (2011) for a brief introduction to the capabilities of <span class="pkg">extRemes</span>.
</p>
<p>Example Datasets:
</p>
<p>There are several example datasets included with this toolkit.  In each case, it is possible to load these datasets into R using the <code>data</code> function.  Each data set has its own help file, which can be accessed by <code>help([name of dataset])</code>.  Data included with <span class="pkg">extRemes</span> are:
</p>
<p>Denmint &ndash; Denver daily minimum temperature.
</p>
<p>Flood.dat &ndash; U.S. Flood damage (in terms of monetary loss) ('dat' file used as example of reading in common data
using the extRemes dialog).
</p>
<p>ftcanmax &ndash; Annual maximum precipitation amounts at one rain gauge in Fort Collins, Colorado.
</p>
<p>HEAT &ndash; Summer maximum (and minimum) temperature at Phoenix Sky Harbor airport.
</p>
<p>Ozone4H.dat &ndash; Ground-level ozone order statistics from 1997 from 513 monitoring stations in the eastern United States.
</p>
<p>PORTw &ndash; Maximum and minimum temperature data (and some covariates) for Port Jervis, New York.
</p>
<p>Rsum &ndash; Frequency of Hurricanes.
</p>
<p>SEPTsp &ndash; Maximum and minimum temperature data (and some covariates) for Sept-Iles, Quebec.
</p>
<p>damage &ndash; Hurricane monetary damage.
</p>
<p>Denversp &ndash; Denver precipitation.
</p>
<p>FCwx &ndash; data frame giving daily weather data for Fort Collins, Colorado, U.S.A. from 1900 to 1999.
</p>
<p>Flood &ndash; R source version of the above mentioned 'Flood.dat' dataset.
</p>
<p>Fort &ndash; Precipitation amounts at one rain gauge in Fort Collins, Colorado.
</p>
<p>Peak &ndash; Salt River peak stream flow.
</p>
<p>Potomac &ndash; Potomac River peak stream flow.
</p>
<p>Tphap &ndash; Daily maximum and minimum temperatures at Phoenix Sky Harbor Airport.
</p>
<p>Primary functions available in <span class="pkg">extRemes</span> include:
</p>
<p><code>fevd</code>: Fitting extreme value distribution functions (EVDs: GEV, Gumbel, GP, Exponential, PP) to data (block maxima or threshold excesses).
</p>
<p><code>ci</code>: Method function for finding confidence intervals for EVD parameters and return levels.
</p>
<p><code>taildep</code>: Estimate chi and/or chibar; statistics that inform about tail dependence between two variables.
</p>
<p><code>atdf</code>: Auto-tail dependence function and plot.  Helps to inform about possible dependence in the extremes of a process.  Note that a process that is highly correlated may or may not be dependent in the extremes.
</p>
<p><code>decluster</code>: Decluster threshold exceedance in a data set to yield a new related process that is more closely independent in the extremes.  Includes two methods for declustering both of which are based on runs declustering.
</p>
<p><code>extremalindex</code>: Estimate the extremal index, a measure of dependence in the extremes.  Two methods are available, one based on runs declustering and the other is the intervals estiamte of Ferro and Segers (2003).
</p>
<p><code>devd, pevd, qevd, revd</code>: Functions for finding the density, cumulative probability distribution (cdf), quantiles and make random draws from EVDs.
</p>
<p><code>pextRemes, rextRemes, return.level</code>: Functions for finding the cdf, make random draws from, and find return levels for fitted EVDs.
</p>
<p>To see how to cite <span class="pkg">extRemes</span> in publications or elsewhere, use <code>citation("extRemes")</code>.
</p>


<h3>Acknowledgements</h3>

<p>Funding for <span class="pkg">extRemes</span> was originally provided by the Weather and Climate Impacts Assessment Science (WCIAS) Program at the National Center for Atmospheric Research (NCAR) in Boulder, Colorado. WCIAS was funded by the National Science Foundation (NSF).  Curent funding is provided by the  Regional  Climate  Uncertainty  Program  (RCUP),  an  NSF-supported  program  at NCAR.  NCAR is operated by the nonprofit University Corporation for Atmospheric Research (UCAR) under the sponsorship of the NSF. Any opinions, findings, conclusions, or recommendations expressed in this publication/software package are those of the author(s) and do not necessarily reflect the views of the NSF.
</p>


<h3>References</h3>

<p>Coles, S.  (2001) <em>An introduction to statistical modeling of extreme values</em>, London, U.K.: Springer-Verlag, 208 pp.
</p>
<p>Ferro, C. A. T. and Segers, J. (2003). Inference for clusters of extreme values. <em>Journal of the Royal Statistical Society B</em>, <b>65</b>, 545&ndash;556.
</p>
<p>Gilleland, E. and Katz, R. W. (2011). New software to analyze how extremes change over time. <em>Eos</em>, 11 January, <b>92</b>, (2), 13&ndash;14, <a href="https://doi.org/10.18637/jss.v072.i08">doi:10.18637/jss.v072.i08</a>.
</p>

<hr>
<h2 id='FCwx'>
Fort Collins, Colorado Weather Data
</h2><span id='topic+FCwx'></span>

<h3>Description</h3>

<p>Weather data from Fort Collins, Colorado, U.S.A. from 1900 to 1999.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(FCwx)</code></pre>


<h3>Format</h3>

<p>The format is:
chr &quot;FCwx&quot;
</p>


<h3>Details</h3>

<p>Data frame with components:
</p>
<p>Year: integer years from 1900 to 1999,
</p>
<p>Mn: integer months from 1 to 12,
</p>
<p>Dy: integer days of the month (i.e., from 1 to 28, 29, 30 or 31 depending on the month/year),
</p>
<p>MxT: integer valued daily maximum temperature (degrees Fahrenheit),
</p>
<p>MnT: integer valued daily minimum temperature (degrees Fahrenheit),
</p>
<p>Prec: numeric giving the daily accumulated precipitation (inches),
</p>
<p>Snow: numeric daily accumulated snow amount,
</p>
<p>SnCv: numeric daily snow cover amount
</p>


<h3>Source</h3>

<p>Originally from the Colorado Climate Center at Colorado State University.  The Colorado state climatologist office no longer provides this data without charge. The data can be obtained from the NOAA/NCDC web site, but there are slight differences (i.e., some missing values for temperature).
</p>


<h3>References</h3>

<p>Katz, R. W., Parlange, M. B. and Naveau, P.  (2002) Statistics of extremes in hydrology. <em>Advances in Water Resources</em>, <b>25</b>, 1287&ndash;1304.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(FCwx)
str(FCwx)
plot(FCwx$Mn, FCwx$Prec)
plot(1:36524, FCwx$MxT, type="l")
</code></pre>

<hr>
<h2 id='fevd'>
Fit An Extreme Value Distribution (EVD) to Data
</h2><span id='topic+fevd'></span><span id='topic+plot.fevd'></span><span id='topic+plot.fevd.bayesian'></span><span id='topic+plot.fevd.lmoments'></span><span id='topic+plot.fevd.mle'></span><span id='topic+print.fevd'></span><span id='topic+summary.fevd'></span><span id='topic+summary.fevd.bayesian'></span><span id='topic+summary.fevd.lmoments'></span><span id='topic+summary.fevd.mle'></span>

<h3>Description</h3>

<p>Fit a univariate extreme value distribution functions (e.g., GEV, GP, PP, Gumbel, or Exponential) to data; possibly with covariates in the parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fevd(x, data, threshold = NULL, threshold.fun = ~1, location.fun = ~1,
    scale.fun = ~1, shape.fun = ~1, use.phi = FALSE,
    type = c("GEV", "GP", "PP", "Gumbel", "Exponential"),
    method = c("MLE", "GMLE", "Bayesian", "Lmoments"), initial = NULL,
    span, units = NULL, time.units = "days", period.basis = "year",
    na.action = na.fail, optim.args = NULL, priorFun = NULL,
    priorParams = NULL, proposalFun = NULL, proposalParams = NULL,
    iter = 9999, weights = 1, blocks = NULL, verbose = FALSE)

## S3 method for class 'fevd'
plot(x, type = c("primary", "probprob", "qq", "qq2",
    "Zplot", "hist", "density", "rl", "trace"),
    rperiods = c(2, 5, 10, 20, 50, 80, 100, 120, 200, 250, 300, 500, 800),
    a = 0, hist.args = NULL, density.args = NULL, d = NULL, ...)

## S3 method for class 'fevd.bayesian'
plot(x, type = c("primary", "probprob", "qq", "qq2",
    "Zplot", "hist", "density", "rl", "trace"),
    rperiods = c(2, 5, 10, 20, 50, 80, 100, 120, 200, 250, 300, 500, 800),
    a = 0, hist.args = NULL, density.args = NULL, burn.in = 499, d = NULL, ...)

## S3 method for class 'fevd.lmoments'
plot(x, type = c("primary", "probprob", "qq", "qq2",
    "Zplot", "hist", "density", "rl", "trace"),
    rperiods = c(2, 5, 10, 20, 50, 80, 100, 120, 200, 250, 300, 500, 800),
    a = 0, hist.args = NULL, density.args = NULL, d = NULL, ...)

## S3 method for class 'fevd.mle'
plot(x, type = c("primary", "probprob", "qq", "qq2",
    "Zplot", "hist", "density", "rl", "trace"),
    rperiods = c(2, 5, 10, 20, 50, 80, 100, 120, 200, 250, 300, 500, 800),
    a = 0, hist.args = NULL, density.args = NULL, period = "year",
    prange = NULL, d = NULL, ...)

## S3 method for class 'fevd'
print(x, ...)

## S3 method for class 'fevd'
summary(object, ...)

## S3 method for class 'fevd.bayesian'
summary(object, FUN = "mean", burn.in = 499, ...)

## S3 method for class 'fevd.lmoments'
summary(object, ...)

## S3 method for class 'fevd.mle'
summary(object, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fevd_+3A_x">x</code></td>
<td>
<p><code>fevd</code>: <code>x</code> can be a numeric vector, the name of a column of <code>data</code> or a formula giving the data to which the EVD is to be fit.  In the case of the latter two, the <code>data</code> argument must be specified, and must have appropriately named columns.
</p>
<p><code>plot</code> and <code>print</code> method functions: any list object returned by <code>fevd</code>.
</p>
</td></tr>
<tr><td><code id="fevd_+3A_object">object</code></td>
<td>
<p>A list object of class &ldquo;fevd&rdquo; as returned by <code>fevd</code>.</p>
</td></tr>
<tr><td><code id="fevd_+3A_data">data</code></td>
<td>

<p>A data frame object with named columns giving the data to be fit, as well as any data necessary for modeling non-stationarity through the threshold and/or any of the parameters.
</p>
</td></tr>
<tr><td><code id="fevd_+3A_threshold">threshold</code></td>
<td>

<p>numeric (single or vector).  If fitting a peak over threshold (POT) model (i.e., <code>type</code> = &ldquo;PP&rdquo;, &ldquo;GP&rdquo;, &ldquo;Exponential&rdquo;) this is the threshold over which (non-inclusive) data (or excesses) are used to estimate the parameters of the distribution function.  If the length is greater than 1, then the length must be equal to either the length of <code>x</code> (or number of rows of <code>data</code>) or to the number of unique arguments in <code>threshold.fun</code>.
</p>
</td></tr>
<tr><td><code id="fevd_+3A_threshold.fun">threshold.fun</code></td>
<td>

<p>formula describing a model for the thresholds using columns from <code>data</code>.  Any valid formula will work.  <code>data</code> must be supplied if this argument is anything other than ~ 1.  Not for use with <code>method</code> &ldquo;Lmoments&rdquo;.
</p>
</td></tr>
<tr><td><code id="fevd_+3A_location.fun">location.fun</code>, <code id="fevd_+3A_scale.fun">scale.fun</code>, <code id="fevd_+3A_shape.fun">shape.fun</code></td>
<td>

<p>formula describing a model for each parameter using columns from <code>data</code>.  <code>data</code> must be supplied if any of these arguments are anything other than ~ 1.
</p>
</td></tr>
<tr><td><code id="fevd_+3A_use.phi">use.phi</code></td>
<td>

<p>logical; should the log of the scale parameter be used in the numerical optimization (for <code>method</code> &ldquo;MLE&rdquo;, &ldquo;GMLE&rdquo; and &ldquo;Bayesian&rdquo; only)?  For the ML and GML estimation, this may make things more stable for some data.
</p>
</td></tr>
<tr><td><code id="fevd_+3A_type">type</code></td>
<td>

<p><code>fevd</code>: character stating which EVD to fit.  Default is to fit the generalized extreme value (GEV) distribution function (df).
</p>
<p><code>plot</code> method function: character describing which plot(s) is (are) desired.  Default is &ldquo;primary&rdquo;, which makes a 2 by 2 panel of plots including the QQ plot of the data quantiles against the fitted model quantiles (<code>type</code> &ldquo;qq&rdquo;), a QQ plot (&ldquo;qq2&rdquo;) of quantiles from model-simulated data against the data, a density plot of the data along with the model fitted density (<code>type</code> &ldquo;density&rdquo;) and a return level plot (<code>type</code> &ldquo;rl&rdquo;). In the case of a stationary (fixed) model, the return level plot will show return levels calculated for return periods given by <code>return.period</code>, along with associated CIs (calculated using default <code>method</code> arguments depending on the estimation method used in the fit.  For non-stationary models, the data are plotted as a line along with associated effective return levels for return periods of 2, 20 and 100 years (unless <code>return.period</code> is specified by the user to other values.  Other possible values for <code>type</code> include &ldquo;hist&rdquo;, which is similar to &ldquo;density&rdquo;, but shows the histogram for the data and &ldquo;trace&rdquo;, which is not used for L-moment fits.  In the case of MLE/GMLE, the trace yields a panel of plots that show the negative log-likelihood and gradient negative log-likelihood (note that the MLE gradient is currently used even for GMLE) for each of the estimated parameter(s); allowing one parameter to vary according to <code>prange</code>, while the others remain fixed at their estimated values.  In the case of Bayesian estimation, the &ldquo;trace&rdquo; option creates a panel of plots showing the posterior df and MCMC trace for each parameter.
</p>
</td></tr>
<tr><td><code id="fevd_+3A_method">method</code></td>
<td>

<p><code>fevd</code>: character naming which type of estimation method to use.  Default is to use maximum likelihood estimation (MLE).
</p>
</td></tr>
<tr><td><code id="fevd_+3A_initial">initial</code></td>
<td>

<p>A list object with any named parameter component giving the initial value estimates for starting the numerical optimization (MLE/GMLE) or the MCMC iterations (Bayesian).  In the case of MLE/GMLE, it is best to obtain a good intial guess, and in the Bayesian case, it is perhaps better to choose poor initial estimates.  If NULL (default), then L-moments estimates and estimates based on Gumbel moments will be calculated, and whichever yields the lowest negative log-likelihood is used.  In the case of <code>type</code> &ldquo;PP&rdquo;, an additional MLE/GMLE estimate is made for the generalized Pareto (GP) df, and parameters are converted to those of the Poisson Process (PP) model.  Again, the initial estimates yielding the lowest negative log-likelihoo value are used for the initial guess.
</p>
</td></tr>
<tr><td><code id="fevd_+3A_span">span</code></td>
<td>

<p>single numeric giving the number of years (or other desired temporal unit) in the data set.  Only used for POT models, and only important in the estimation for the PP model, but important for subsequent estimates of return levels for any POT model.  If missing, it will be calculated using information from <code>time.units</code>.
</p>
</td></tr>
<tr><td><code id="fevd_+3A_units">units</code></td>
<td>

<p>(optional) character giving the units of the data, which if given may be used subsequently (e.g., on plot axis labels, etc.).
</p>
</td></tr>
<tr><td><code id="fevd_+3A_time.units">time.units</code></td>
<td>

<p>character string that must be one of &ldquo;hours&rdquo;, &ldquo;minutes&rdquo;, &ldquo;seconds&rdquo;, &ldquo;days&rdquo;, &ldquo;months&rdquo;, &ldquo;years&rdquo;, &ldquo;m/hour&rdquo;, &ldquo;m/minute&rdquo;, &ldquo;m/second&rdquo;, &ldquo;m/day&rdquo;, &ldquo;m/month&rdquo;, or &ldquo;m/year&rdquo;; where m is a number.  If <code>span</code> is missing, then this argument is used in determining the value of <code>span</code>.  It is also returned with the output and used subsequently for plot labelling, etc.
</p>
</td></tr>
<tr><td><code id="fevd_+3A_period.basis">period.basis</code></td>
<td>

<p>character string giving the units for the period.  Used only for plot labelling and naming output vectors from some of the method functions (e.g., for establishing what the period represents for the return period).
</p>
</td></tr>
<tr><td><code id="fevd_+3A_rperiods">rperiods</code></td>
<td>
<p>numeric vector giving the return period(s) for which it is desired to calculate the corresponding return levels.</p>
</td></tr>
<tr><td><code id="fevd_+3A_period">period</code></td>
<td>
<p>character string naming the units for the return period.</p>
</td></tr>
<tr><td><code id="fevd_+3A_burn.in">burn.in</code></td>
<td>
<p>The first <code>burn.in</code> values are thrown out before calculating anything from the MCMC sample.</p>
</td></tr>
<tr><td><code id="fevd_+3A_a">a</code></td>
<td>
<p>when plotting empirical probabilies and such, the function <code>ppoints</code> is called, which has this argument <code>a</code>.</p>
</td></tr>
<tr><td><code id="fevd_+3A_d">d</code></td>
<td>
<p>numeric determining how to scale the rate parameter for the point process.  If NULL, the function will attempt to scale based on the values of <code>period.basis</code> and <code>time.units</code>, the first of which must be &ldquo;year&rdquo; and the second of which must be one of &ldquo;days&rdquo;, &ldquo;months&rdquo;, &ldquo;years&rdquo;, &ldquo;hours&rdquo;, &ldquo;minutes&rdquo; or &ldquo;seconds&rdquo;.  If none of these are the case, then <code>d</code> should be specified, otherwise, it is not necessary.</p>
</td></tr>
<tr><td><code id="fevd_+3A_density.args">density.args</code>, <code id="fevd_+3A_hist.args">hist.args</code></td>
<td>
<p>named list object containing arguments to the <code>density</code> and <code>hist</code> functions, respectively.</p>
</td></tr>
<tr><td><code id="fevd_+3A_na.action">na.action</code></td>
<td>

<p>function to be called to handle missing values.  Generally, this should remain at the default (na.fail), and the user should take care to impute missing values in an appropriate manner as it may have serious consequences on the results.
</p>
</td></tr>
<tr><td><code id="fevd_+3A_optim.args">optim.args</code></td>
<td>

<p>A list with named components matching exactly any arguments that the user wishes to specify to <code>optim</code>, which is used only for MLE and GMLE methods.  By default, the &ldquo;BFGS&rdquo; method is used along with <code>grlevd</code> for the gradient argument.  Generally, the <code>grlevd</code> function is used for the <code>gr</code> option unless the user specifies otherwise, or the optimization method does not take gradient information.
</p>
</td></tr>
<tr><td><code id="fevd_+3A_priorfun">priorFun</code></td>
<td>

<p>character naming a prior df to use for methods GMLE and Bayesian.  The default for GMLE (not including Gumbel or Exponential types) is to use the one suggested by Martins and Stedinger (2000, 2001) on the shape parameter; a beta df on -0.5 to 0.5 with parameters <code>p</code> and <code>q</code>.  Must take <code>x</code> as its first argument for <code>method</code> &ldquo;GMLE&rdquo;.  Optional arguments for the default function are <code>p</code> and <code>q</code> (see details section).
</p>
<p>The default for Bayesian estimation is to use normal distribution functions.  For Bayesian estimation, this function must take <code>theta</code> as its first argument.
</p>
<p>Note: if this argument is not NULL and <code>method</code> is set to &ldquo;MLE&rdquo;, it will be changed to &ldquo;GMLE&rdquo;.
</p>
</td></tr>
<tr><td><code id="fevd_+3A_priorparams">priorParams</code></td>
<td>

<p>named list containing any prior df parameters (where the list names are the same as the function argument names).  Default for GMLE (assuming the default function is used) is to use <code>q</code> = 6 and <code>p</code> = 9.  Note that in the Martins and Stedinger (2000, 2001) papers, they use a different EVD parametrization than is used here such that a positive shape parameter gives the upper bounded distribution instead of the heavy-tail one (as emloyed here).  To be consistent with these papers, <code>p</code> and <code>q</code> are reversed inside the code so that they have the same interpretation as in the papers.
</p>
<p>Default for Bayesian estimation is to use ML estimates for the means of each parameter (may be changed using <code>m</code>, which must be a vector of same length as the number of parameters to be estimated (i.e., if using the default prior df)) and a standard deviation of 10 for all other parameters (again, if using the default prior df, may be changed using <code>v</code>, which must be a vector of length equal to the number of parameters).
</p>
</td></tr>
<tr><td><code id="fevd_+3A_proposalfun">proposalFun</code></td>
<td>

<p>For Bayesian estimation only, this is a character naming a function used to generate proposal parameters at each iteration of the MCMC.  If NULL (default), a random walk chain is used whereby if theta.i is the current value of the parameter, the proposed new parameter theta.star is given by theta.i + z, where z is drawn at random from a normal df.
</p>
</td></tr>
<tr><td><code id="fevd_+3A_proposalparams">proposalParams</code></td>
<td>

<p>A named list object describing any optional arguments to the <code>proposalFun</code> function.  All functions must take argument <code>p</code>, which must be a vector of the parameters, and <code>ind</code>, which is used to identify which parameter is to be proposed.  The default <code>proposalFun</code> function takes additional arguments <code>mean</code> and <code>sd</code>, which must be vectors of length equal to the number of parameters in the model (default is to use zero for the mean of z for every parameter and 0.1 for its standard deviation).
</p>
</td></tr>
<tr><td><code id="fevd_+3A_iter">iter</code></td>
<td>

<p>Used only for Bayesian estimation, this is the number of MCMC iterations to do.
</p>
</td></tr>
<tr><td><code id="fevd_+3A_weights">weights</code></td>
<td>
<p>numeric of length 1 or n giving weights to be applied
in the likelihood calculations (e.g., if there are data points to
be weighted more/less heavily than others).</p>
</td></tr>
<tr><td><code id="fevd_+3A_blocks">blocks</code></td>
<td>
<p>An optional list containing information required to fit point process models in a computationally-efficient manner by using only the exceedances and not the observations below the threshold(s). See details for further information. 
</p>
</td></tr>
<tr><td><code id="fevd_+3A_fun">FUN</code></td>
<td>
<p>character string naming a function to use to estimate the parameters from the MCMC sample.  The function is applied to each column of the <code>results</code> component of the returned <code>fevd</code> object.</p>
</td></tr>
<tr><td><code id="fevd_+3A_verbose">verbose</code></td>
<td>

<p>logical; should progress information be printed to the screen?  If TRUE, for MLE/GMLE, the argument <code>trace</code> will be set to 6 in the call to <code>optim</code>.
</p>
</td></tr>
<tr><td><code id="fevd_+3A_prange">prange</code></td>
<td>
<p>matrix whose columns are numeric vectors of length two for each parameter in the model giving the parameter range over which trace plots should be made.  Default is to use either +/- 2 * std. err. of the parameter (first choice) or, if the standard error cannot be calculated, then +/- 2 * log2(abs(parameter)).  Typically, these values seem to work very well for these plots.</p>
</td></tr>
<tr><td><code id="fevd_+3A_...">...</code></td>
<td>
<p>Not used by most functions here.  Optional arguments to <code>plot</code> for the various <code>plot</code> method functions.
</p>
<p>In the case of the <code>summary</code> method functions, the logical argument <code>silent</code> may be passed to suppress (if TRUE) printing any information to the screen.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See text books on extreme value analysis (EVA) for more on univariate EVA (e.g., Coles, 2001 and Reiss and Thomas, 2007 give fairly accessible introductions to the topic for most audiences; and Beirlant et al., 2004, de Haan and Ferreira, 2006, as well as Reiss and Thomas, 2007 give more complete theoretical treatments).  The extreme value distributions (EVDs) have theoretical support for analyzing extreme values of a process.  In particular, the generalized extreme value (GEV) df is appropriate for modeling block maxima (for large blocks, such as annual maxima), the generalized Pareto (GP) df models threshold excesses (i.e., x - u | x &gt; u and u a high threshold).
</p>
<p>The GEV df is given by
</p>
<p>Pr(X &lt;= x) = G(x) = exp[-(1 + shape*(x - location)/scale)^(-1/shape)]
</p>
<p>for 1 + shape*(x - location) &gt; 0 and scale &gt; 0.  If the shape parameter is zero, then the df is defined by continuity and simplies to
</p>
<p>G(x) = exp(-exp((x - location)/scale)).
</p>
<p>The GEV df is often called a family of distribution functions because it encompasses the three types of EVDs: Gumbel (shape = 0, light tail), Frechet (shape &gt; 0, heavy tail) and the reverse Weibull (shape &lt; 0, bounded upper tail at location - scale/shape).  It was first found by R. von Mises (1936) and also independently noted later by meteorologist A. F. Jenkins (1955).  It enjoys theretical support for modeling maxima taken over large blocks of a series of data.
</p>
<p>The generalized Pareo df is given by (Pickands, 1975)
</p>
<p>Pr(X &lt;= x) = F(x) = 1 - [1 + shape*(x - threshold)/scale]^(-1/shape)
</p>
<p>where 1 + shape*(x - threshold)/scale &gt; 0, scale &gt; 0, and x &gt; threshold.  If shape = 0, then the GP df is defined by continuity and becomes
</p>
<p>F(x) = 1 - exp(-(x - threshold)/scale).
</p>
<p>There is an approximate relationship between the GEV and GP distribution functions where the GP df is approximately the tail df for the GEV df.  In particular, the scale parameter of the GP is a function of the threshold (denote it scale.u), and is equivalent to scale + shape*(threshold - location) where scale, shape and location are parameters from the &ldquo;equivalent&rdquo; GEV df.  Similar to the GEV df, the shape parameter determines the tail behavior, where shape = 0 gives rise to the exponential df (light tail), shape &gt; 0 the Pareto df (heavy tail) and shape &lt; 0 the Beta df (bounded upper tail at location - scale.u/shape).  Theoretical justification supports the use of the GP df family for modeling excesses over a high threshold (i.e., y = x - threshold).  It is assumed here that <code>x</code>, <code>q</code> describe x (not y = x - threshold).  Similarly, the random draws are y + threshold.
</p>
<p>If interest is in minima or deficits under a low threshold, all of the above applies to the negative of the data (e.g., - max(-X_1,...,-X_n) = min(X_1, ..., X_n)) and <code>fevd</code> can be used so long as the user first negates the data, and subsequently realizes that the return levels (and location parameter) given will be the negative of the desired return levels (and location parameter), etc.
</p>
<p>The study of extremes often involves a paucity of data, and for small sample sizes, L-moments may give better estimates than competing methods, but penalized MLE (cf. Coles and Dixon, 1999; Martins and Stedinger, 2000; 2001) may give better estimates than the L-moments for such samples.  Martins and Stedinger (2000; 2001) use the terminology generalized MLE, which is also used here.
</p>
<p>Non-stationary models:
</p>
<p>The current code does not allow for non-stationary models with L-moments estimation.
</p>
<p>For MLE/GMLE (see El Adlouni et al 2007 for using GMLE in fitting models whose parameters vary) and Bayesian estimation, linear models for the parameters may be fit using formulas, in which case the <code>data</code> argument must be supplied.  Specifically, the models allowed for a set of covariates, y, are:
</p>
<p>location(y) = mu0 + mu1 * f1(y) + mu2 * f2(y) + ...
</p>
<p>scale(y) = sig0 + sig1 * g1(y) + sig2 * g2(y) + ...
</p>
<p>log(scale(y)) = phi(y) = phi0 + phi1 * g1(y) + phi2 * g2(y) + ...
</p>
<p>shape(y) = xi0 + xi1 * h1(y) + xi2 * h2(y) + ...
</p>
<p>For non-stationary fitting it is recommended that the covariates within the generalized linear models are (at least approximately) centered and scaled (see examples below).  It is generally ill-advised to include covariates in the shape parameter, but there are situations where it makes sense.
</p>
<p>Non-stationary modeling is accomplished with <code>fevd</code> by using formulas via the arguments: <code>threshold.fun</code>, <code>location.fun</code>, <code>scale.fun</code> and <code>shape.fun</code>.  See examples to see how to do this.
</p>
<p>Initial Value Estimates:
</p>
<p>In the case of MLE/GMLE, it can be very important to get good initial estimates (e.g., see the examples below).  <code>fevd</code> attempts to find such estimates, but it is also possible for the user to supply their own initial estimates as a list object using the <code>initial</code> argument, whereby the components of the list are named according to which parameter(s) they are associated with.  In particular, if the model is non-stationary, with covariates in the location (e.g., mu(t) = mu0 + mu1 * t), then <code>initial</code> may have a component named &ldquo;location&rdquo; that may contain either a single number (in which case, by default, the initial value for mu1 will be zero) or a vector of length two giving initial values for mu0 and mu1.
</p>
<p>For Bayesian estimation, it is good practice to try several starting values at different points to make sure the initial values do not affect the outcome.  However, if initial values are not passed in, the MLEs are used (which probably is not a good thing to do, but is more likely to yield good results).
</p>
<p>For MLE/GMLE, two (in the case of PP, three) initial estimates are calculated along with their associated likelihood values.  The initial estimates that yield the highest likelihood are used.  These methods are:
</p>
<p>1.  L-moment estimates.
</p>
<p>2.  Let m = mean(xdat) and s = sqrt(6 * var(xdat)) / pi.  Then, initial values assigend for the lcoation parameter when either <code>initial</code> is NULL or the location component of <code>initial</code> is NULL, are m - 0.57722 * s.  When <code>initial</code> or the scale component of <code>initial</code> is NULL, the initial value for the scale parameter is taken to be s, and when <code>initial</code> or its shape component is NULL, the initial value for the shape parameter is taken to be 1e-8 (because these initial estimates are moment-based estimates for the Gumbel df, so the initial value is taken to be near zero).
</p>
<p>3.  In the case of PP, which is often the most difficult model to fit, MLEs are obtained for a GP model, and the resulting parameter estimates are converted to those of the approximately equivalent PP model.
</p>
<p>In the case of a non-stationary model, if the default initial estimates are used, then the intercept term for each parameter is given the initial estimate, and all other parameters are set to zero initially.  The exception is in the case of PP model fitting where the MLE from the GP fits are used, in which case, these parameter estimates may be non-zero.
</p>
<p>The generalized MLE (GMLE) method:
</p>
<p>This method places a penalty (or prior df) on the shape parameter to help ensure a better fit.  The procedure is nearly identical to MLE, except the likelihood, L, is multiplied by the prior df, p(shape); and because the negative log-likelihood is used, the effect is that of subtracting this term.  Currently, there is no supplied function by this package to calculate the gradient for the GMLE case, so in particular, the trace plot is not the trace of the actual negative log-likelihood (or gradient thereof) used in the estimation.
</p>
<p>Bayesian Estimation:
</p>
<p>It is possible to give your own prior and proposal distribution functions using the appropriate arguments listed above in the arguments section.  At each iteration of the chain, the parameters are updated one at a time in random order.  The default method uses a random walk chain for the proposal and normal distributions for the parameters.
</p>
<p>Plotting output:
</p>
<p><code>plot</code>: The <code>plot</code> method function will take information from the <code>fevd</code> output and make any of various useful plots.  The default, regardless of estimation method, is to produce a 2 by 2 panel of plots giving some common diagnostic plots.  Possible types (determined by the <code>type</code> argument) include:
</p>
<p>1. &ldquo;primary&rdquo; (default): yields the 2 by 2 panel of plots given by 3, 4, 6 and 7 below.
</p>
<p>2. &ldquo;probprob&rdquo;: Model probabilities against empirical probabilities (obtained from the <code>ppoints</code> function).  A good fit should yield a straight one-to-one line of points.  In the case of a non-stationary model, the data are first transformed to either the Gumbel (block maxima models) or exponential (POT models) scale, and plotted against probabilities from these standardized distribution functions.  In the case of a PP model, the parameters are first converted to those of the approximately equivalent GP df, and are plotted against the empirical data threshold excesses probabilities.
</p>
<p>3. &ldquo;qq&rdquo;: Empirical quantiles against model quantiles.  Again, a good fit will yield a straight one-to-one line of points.  Generally, the qq-plot is preferred to the probability plot in 1 above.  As in 2, for the non-stationary case, data are first transformed and plotted against quantiles from the standardized distributions.  Also as in 2 above, in the case of the PP model, parameters are converted to those of the GP df and quantiles are from threshold excesses of the data.
</p>
<p>4. &ldquo;qq2&rdquo;: Similar to 3, first data are simulated from the fitted model, and then the qq-plot between them (using the function <code>qqplot</code> from this self-same package) is made between them, which also yields confidence bands.  Note that for a good fitting model, this should again yield a straight one-to-one line of points, but generally, it will not be as &ldquo;well-behaved&rdquo; as the plot in 3.  The one-to-one line and a regression line fitting the quantiles is also shown.  In the case of a non-stationary model, simulations are obtained by simulating from an appropriate standardized EVD, re-ordered to follow the same ordering as the data to which the model was fit, and then back transformed using the covariates from <code>data</code> and the parameter estimates to put the simulated sample back on the original scale of the data.  The PP model is handled analogously as in 2 and 3 above.
</p>
<p>5. and 6. &ldquo;Zplot&rdquo;: These are for PP model fits only and are based on Smith and Shively (1995).  The Z plot is a diagnostic for determining whether or not the random variable, Zk, defined as the (possibly non-homogeneous) Poisson intensity parameter(s) integrated from exceedance time k - 1 to exceedance time k (beginning the series with k = 1) is independent exponentially distributed with mean 1.
</p>
<p>For the Z plot, it is necessary to scale the Poisson intensity parameter appropriately.  For example, if the data are given on a daily time scale with an annual period basis, then this parameter should be divided by, for example, 365.25.  From the fitted <code>fevd</code> object, the function will try to account for the correct scaling based on the two components &ldquo;period.basis&rdquo; and &ldquo;time.units&rdquo;.  The former currently must be &ldquo;year&rdquo; and the latter must be one of &ldquo;days&rdquo;, &ldquo;months&rdquo;, &ldquo;years&rdquo;, &ldquo;hours&rdquo;, &ldquo;minutes&rdquo; or &ldquo;seconds&rdquo;.  If none of these are valid for your specific data (e.g., if an annual basis is not desired), then use the <code>d</code> argument to explicitly specify the correct scaling.
</p>
<p>7.  &ldquo;hist&rdquo;: A histogram of the data is made, and the model density is shown with a blue dashed line.  In the case of non-stationary models, the data are first transformed to an appropriate standardized EVD scale, and the model density line is for the self-same standardized EVD.  Currently, this does not work for non-stationary POT models.
</p>
<p>8. &ldquo;density&rdquo;: Same as 5, but the kernel density (using function <code>density</code>) for the data is plotted instead of the histogram. In the case of the PP model, block maxima of the data are calculated and the density of these block maxima are compared to the PP in terms of the equivalent GEV df.  If the model is non-stationary GEV, then the transformed data (to a stationary Gumbel df) are used.  If the model is a non-stationary POT model, then currently this option is not available.
</p>
<p>9. &ldquo;rl&rdquo;: Return level plot.  This is done on the log-scale for the abscissa in order that the type of EVD can be discerned from the shape (i.e., heavy tail distributions are concave, light tailed distributions are straight lines, and bounded upper-tailed distributions are convex, asymptoting at the upper bound).  95 percent CIs are also shown (gray dashed lines).  In the case of non-stationary models, the data are plotted as a line, and the &ldquo;effective&rdquo; return levels (by default the 2-period (i.e., the median), 20-period and 100-period are used; period is usually annual) are also shown (see, e.g., Gilleland and Katz, 2011).  In the case of the PP model, the equivalent GEV df (stationary model) is assumed and data points are block maxima, where the blocks are determined from information passed in the call to <code>fevd</code>.  In particular, the <code>span</code> argument (which, if not passed by the user, will have been determined by <code>fevd</code> using <code>time.units</code> along with the number of points per year (which is estimated from <code>time.units</code>) are used to find the blocks over which the maxima are taken.  For the non-stationary case, the equivalent GP df is assumed and parameters are converted.  This helps facilitate a more meaningful plot, e.g., in the presence of a non-constant threshold, but otherwise constant parameters.
</p>
<p>10. &ldquo;trace&rdquo;: In each of cases (b) and (c) below, a 2 by the number of parameters panel of plots are created.
</p>
<p>(a) L-moments: Not available for the L-moments estimation.
</p>
<p>(b) For MLE/GMLE, the likelihood traces are shown for each parameter of the model, whereby all but one parameter is held fixed at the MLE/GMLE values, and the negative log-likelihood is graphed for varying values of the parameter of interest.  Note that this differs greatly from the profile likelihood (see, e.g., <code>profliker</code>) where the likelihood is maximized over the remaining parameters.  The gradient negative log-likelihoods are also shown for each parameter.  These plots may be useful in diagnosing any fitting problems that might arise in practice.  For ease of interpretation, the gradients are shown directly below the likleihoods for each parameter.
</p>
<p>(c) For Bayesian estimation, the usual trace plots are shown with a gray vertical dashed line showing where the <code>burn.in</code> value lies; and a gray dashed horizontal line through the posterior mean.  However, the posterior densities are also displayed for each parameter directly above the usual trace plots.  It is not currently planned to allow for adding the prior dnsities to the posterior density graphs, as this can be easily implemented by the user, but is more difficult to do generally.
</p>
<p>As with <code>ci</code> and <code>distill</code>, only <code>plot</code> need be called by the user.  The appropriate choice of the other functions is automatically determined from the <code>fevd</code> fitted object.
</p>
<p>Note that when <code>blocks</code> are provided to <code>fevd</code>, certain plots
that require the full set of observations (including non-exceedances)
cannot be produced.
</p>
<p>Summaries and Printing:
</p>
<p><code>summary</code> and <code>print</code> method functions are available, and give different information depending on the estimation method used.  However, in each case, the parameter estimates are printed to the screen.  <code>summary</code> returns some useful output (similar to distill, but in a list format).  The <code>print</code> method function need not be called as one can simply type the name of the <code>fevd</code> fitted object and return to execute the command (see examples below).  The deviance information criterion (DIC) is calculated for the Bayesian estimation method as DIC = D(mean(theta)) + 2 * pd, where pd = mean(D(theta)) - D(mean(theta)), and D(theta) = -2 * log-likelihood evaluated at the parameter values given by theta.  The means are taken over the posterior MCMC sample.  The default estimation method for the parameter values from the MCMC sample is to take the mean of the sample (after removing the first burn.in samples).  A good alternative is to set the <code>FUN</code> argument to &ldquo;postmode&rdquo; in order to obtain the posterior mode of the sample.
</p>
<p>Using Blocks to Reduce Computation in PP Fitting:
</p>
<p>If <code>blocks</code> is supplied, the user should
provide only the exceedances and not all of the data values. For
stationary models, the list should contain a component called
<code>nBlocks</code> indicating the number of observations within a block, where
blocks are defined in a manner analogous to that used in GEV
models. For nonstationary models, the list may contain one or more of
several components. For nonstationary models with covariates, the list
should contain a <code>data</code> component analogous to the <code>data</code> argument,
providing values for the blocks. If the threshold varies, the list
should contain a <code>threshold</code> component that is analogous to the
<code>threshold</code> argument. If some of the observations within any block are
missing (assuming missing at random or missing completely at random),
the list should contain a <code>proportionMissing</code> component that is a
vector with one value per block indicating the proportion of
observations missing for the block. To weight the blocks, the list can
contain a <code>weights</code> component with one value per block. Warning: to
properly analyze nonstationary models, any covariates, thresholds, and
weights must be constant within each block.
</p>


<h3>Value</h3>

<p><code>fevd</code>: A list object of class &ldquo;fevd&rdquo; is returned with components:
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>the function call.  Used as a default for titles in plots and output printed to the screen (e.g., by <code>summary</code>).</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>character vector giving the name of arguments: <code>x</code> and <code>data</code>.  This is used by the <code>datagrabber</code> method function, as well as for some plot labeling.</p>
</td></tr>
<tr><td><code>data.pointer</code>, <code>cov.pointer</code>, <code>cov.data</code>, <code>x</code>, <code>x.fun</code></td>
<td>
<p>Not all of these are included, and which ones are depend on how the data are passed to <code>fevd</code>.  These may be character strings, vectors or data frames depending again on the original function call.  They are used by <code>datagrabber</code> in order to obtain the original data.  If x is a column of data, then x.fun is a formula specifying which column.  Also, if x is a formula, then x.fun is this self-same formula.</p>
</td></tr>
<tr><td><code>in.data</code></td>
<td>
<p>logical; is the argument <code>x</code> a column of the argument <code>data</code> (TRUE) or not (FALSE)?</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>character string naming which estimation method ws used for the fit.  Same as <code>method</code> argument.</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>character string naming which EVD was fit to the data.  Same as <code>type</code> argument.</p>
</td></tr>
<tr><td><code>period.basis</code></td>
<td>
<p>character string naming the period for return periods.  This is used in plot labeling and naming, e.g., output from <code>ci</code>.  Same as the argument passed in.</p>
</td></tr>
<tr><td><code>units</code></td>
<td>
<p>Not present if not passed in by the user.  character string naming the data units.  Used for plot labeling.</p>
</td></tr>
<tr><td><code>par.models</code></td>
<td>
<p>A list object giving the values of the threshold and parameter function arguments, as well as a logical stating whether the log of the scale parameter is used or not.  This is present even if it does not make sense (i.e., for L-moments estimation) because it is used by the <code>is.fixedfevd</code> function for determining whether or not a model is stationary or not.</p>
</td></tr>
<tr><td><code>const.loc</code>, <code>const.scale</code>, <code>const.shape</code></td>
<td>
<p>logicals stating whether the named parameter is constant (TRUE) or not (FALSE).  Currently, not used, but could be useful.  Still present even for L-moments estimation even though it does not really make sense in this context (will always be true).</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>the length of the data to which the model is fit.</p>
</td></tr>
<tr><td><code>span</code>, <code>npy</code></td>
<td>
<p>For POT models only, this is the estimated number of periods (usually years) and number of points per period, both estimated using time.units</p>
</td></tr>
<tr><td><code>na.action</code></td>
<td>
<p>character naming the function used for handling missing values.</p>
</td></tr>
<tr><td><code>results</code></td>
<td>
<p>If L-moments are used, then this is a simple vector of length equal to the number of parameters of the model.  For MLE/GMLE, this is a list object giving the output from <code>optim</code> (in particular, the par component is a vector giving the parameter estimates).  For Bayesian estimation, this is a matrix whose rows give the results for each iteration of the MCMC simulations.  The columns give the parameters, as well as an additional last column that gives the number of parameters that were updated at each iteration.</p>
</td></tr>
<tr><td><code>priorFun</code>, <code>priorParams</code></td>
<td>
<p>These are only present for GMLE and Bayesian estimation methods.  They give the prior df and optional arguments used in the estimation.</p>
</td></tr>
<tr><td><code>proposalFun</code>, <code>proposalParams</code></td>
<td>
<p>These are only present for Bayesian estimation, and they give the proposal function used along with optional arguments.</p>
</td></tr>
<tr><td><code>chain.info</code></td>
<td>
<p>If estimation method is &ldquo;Bayesian&rdquo;, then this component is a matrix whose first several columns give 0 or 1 for each parameter at each iteration, where 0 indicates that the parameter was not updated and 1 that it was.  The first row is NA for these columns.  the last two columns give the likelihood and prior values for the current parameter values.</p>
</td></tr>
<tr><td><code>chain.info</code></td>
<td>
<p>matrix whose first n columns give a one or zero depending on whether the parameter was updated or not, resp.  The last two columns give the log-likelihood and prior values associated with the parameters of that sample.</p>
</td></tr>
<tr><td><code>blocks</code></td>
<td>
<p>Present only if <code>blocks</code> is supplied as an
argument. Will contain the input information and computed
block-level information for use in post-processing the model object,
in particular block-wise design matrices.</p>
</td></tr>
</table>
<p><code>print</code>: Does not return anything.  Information is printed to the screen.
</p>
<p><code>summary</code>: Depending on the estimation method, either a numeric vector giving the parameter estimates (&ldquo;Lmoments&rdquo;) or a list object (all other estimation methods) is returned invisibly, and if <code>silent</code> is FALSE (default), then summary information is printed to the screen.  List components may include:
</p>
<table>
<tr><td><code>par</code>, <code>se.theta</code></td>
<td>
<p>numeric vectors giving the parameter and standard error estimates, resp.</p>
</td></tr>
<tr><td><code>cov.theta</code></td>
<td>
<p>matrix giving the parameter covariances.</p>
</td></tr>
<tr><td><code>nllh</code></td>
<td>
<p>number giving the value of the negative log-likelihood.</p>
</td></tr>
<tr><td><code>AIC</code>, <code>BIC</code>, <code>DIC</code></td>
<td>
<p>numbers giving the Akaike Information Criterion (AIC, Akaike, 1974), the Bayesian Information Criterion (BIC, Schwarz, 1978), and/or the Deviance Information Criterion, resp.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Akaike, H. (1974).  A new look at the statistical model identification.  <em>IEEE Transactions on Automatic Control</em>, <b>19</b>, 716&ndash;723.
</p>
<p>Beirlant, J., Goegebeur, Y., Teugels, J. and Segers, J. (2004). <em>Statistics of Extremes: Theory and Applications</em>.  Chichester, West Sussex, England, UK: Wiley, ISBN 9780471976479, 522pp.
</p>
<p>Coles, S. G.  (2001). <em>An introduction to statistical modeling of extreme values</em>, London: Springer-Verlag.
</p>
<p>Coles, S. G. and Dixon, M. J. (1999).  Likelihood-based inference for extreme value models.  <em>Extremes</em>, <b>2</b> (1), 5&ndash;23.
</p>
<p>El Adlouni, S., Ouarda, T. B. M. J., Zhang, X., Roy, R, and Bobee, B. (2007).  Generalized maximum likelihood estimators for the nonstationary generalized extreme value model.  <em>Water Resources Research</em>, <b>43</b>, W03410, doi: 10.1029/2005WR004545, 13 pp.
</p>
<p>Gilleland, E. and Katz, R. W. (2011). New software to analyze how extremes change over time. <em>Eos</em>, 11 January, <b>92</b>, (2), 13&ndash;14.
</p>
<p>de Haan, L. and Ferreira, A. (2006). <em>Extreme Value Theory: An Introduction</em>.  New York, NY, USA: Springer, 288pp.
</p>
<p>Jenkinson, A. F. (1955). The frequency distribution of the annual maximum (or minimum) of meteorological elements. <em>Quart. J. R.  Met. Soc.</em>, <b>81</b>, 158&ndash;171.
</p>
<p>Martins, E. S. and Stedinger, J. R. (2000).  Generalized maximum likelihood extreme value quantile estimators for hydrologic data.  <em>Water Resources Research</em>, <b>36</b> (3), 737&ndash;744.
</p>
<p>Martins, E. S. and Stedinger, J. R. (2001).  Generalized maximum likelihood Pareto-Poisson estimators for partial duration series.  <em>Water Resources Research</em>, <b>37</b> (10), 2551&ndash;2557.
</p>
<p>Pickands, J. (1975). Statistical inference using extreme order statistics.  <em>Annals of Statistics</em>, <b>3</b>, 119&ndash;131.
</p>
<p>Reiss, R.-D. and Thomas, M. (2007). <em>Statistical Analysis of Extreme Values: with applications to insurance, finance, hydrology and other fields</em>. Birkhauser, 530pp., 3rd edition.
</p>
<p>Schwarz, G. E. (1978).  Estimating the dimension of a model.  <em>Annals of Statistics</em>, <b>6</b>, 461&ndash;464.
</p>
<p>Smith, R. L. and Shively, T. S. (1995).  A point process approach to modeling trends in tropospheric ozone.  <em>Atmospheric Environment</em>, <b>29</b>, 3489&ndash;3499.
</p>
<p>von Mises, R. (1936). La distribution de la plus grande de n valeurs, <em>Rev. Math. Union Interbalcanique</em> <b>1</b>, 141&ndash;160.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ci.fevd">ci.fevd</a></code> for obtaining parameter and return level confidence intervals.
</p>
<p><code>distill.fevd</code> for stripping out a vector of parameter estimates and perhaps other pertinent information from an fevd object.
</p>
<p>For functions to find the density, probability df, quantiles and simulate data from, an EV df, see:
<code><a href="#topic+devd">devd</a></code>, <code><a href="#topic+pevd">pevd</a></code>, <code><a href="#topic+qevd">qevd</a></code>, <code><a href="#topic+revd">revd</a></code>
</p>
<p>For functions to find the probability df and simulate random data from a fitted model from <code>fevd</code>, see:
<code><a href="#topic+pextRemes">pextRemes</a></code>, <code><a href="#topic+rextRemes">rextRemes</a></code>
</p>
<p>For functions to determine if the extreme data are independent or not, see:
<code><a href="#topic+extremalindex">extremalindex</a></code>, <code><a href="#topic+atdf">atdf</a></code>
</p>
<p>For functions to help choose a threshold, see: <code><a href="#topic+threshrange.plot">threshrange.plot</a></code>, <code><a href="#topic+mrlplot">mrlplot</a></code>
</p>
<p>To decluster stationary dependent extremes, see: <code><a href="#topic+decluster">decluster</a></code>
</p>
<p>For more on formulas in R, see: <code><a href="stats.html#topic+formula">formula</a></code>
</p>
<p>To grab the parameters of a fitted <code>fevd</code> model, see: <code><a href="#topic+findpars">findpars</a></code>
</p>
<p>To calculate the parameter covariance, see:
<code><a href="stats.html#topic+optimHess">optimHess</a></code>, <code><a href="#topic+parcov.fevd">parcov.fevd</a></code>
</p>
<p>To see more about the <span class="pkg">extRemes</span> method functions described here, see: <code><a href="distillery.html#topic+ci">ci</a></code> and <code><a href="distillery.html#topic+distill">distill</a></code>
</p>
<p>To calculate effective return levels and CI's for MLE and Bayesian estimation of non-stationary models, see <code><a href="#topic+ci.rl.ns.fevd.bayesian">ci.rl.ns.fevd.bayesian</a></code>, <code><a href="#topic+ci.rl.ns.fevd.mle">ci.rl.ns.fevd.mle</a></code> and <code><a href="#topic+return.level">return.level</a></code>
</p>
<p>To obtain the original data set from a fitted <code>fevd</code> object, use: <code><a href="distillery.html#topic+datagrabber">datagrabber</a></code>
</p>
<p>To calculate the profile likelihood, see: <code><a href="#topic+profliker">profliker</a></code>
</p>
<p>To test the statistical significance of nested models with additional parameters, see: <code><a href="#topic+lr.test">lr.test</a></code>
</p>
<p>To find effective return levels for non-stationary models, see: <code><a href="#topic+erlevd">erlevd</a></code>
</p>
<p>To determine if an <code>fevd</code> object is stationary or not, use: <code><a href="#topic+is.fixedfevd">is.fixedfevd</a></code> and <code><a href="#topic+check.constant">check.constant</a></code>
</p>
<p>For more about the plots created for <code>fevd</code> fitted objects, see:
<code><a href="stats.html#topic+ppoints">ppoints</a></code>, <code><a href="stats.html#topic+density">density</a></code>, <code><a href="graphics.html#topic+hist">hist</a></code>, <code><a href="#topic+qqplot">qqplot</a></code>
</p>
<p>For general numerical optimization in R, see: <code><a href="stats.html#topic+optim">optim</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
z &lt;- revd(100, loc=20, scale=0.5, shape=-0.2)
fit &lt;- fevd(z)
fit
plot(fit)
plot(fit, "trace")

## Not run: 
## Fitting the GEV to block maxima.

# Port Jervis, New York winter maximum and minimum
# temperatures (degrees centigrade).
data(PORTw)

# Gumbel
fit0 &lt;- fevd(TMX1, PORTw, type="Gumbel", units="deg C")
fit0
plot(fit0)
plot(fit0, "trace")
return.level(fit0)

# GEV
fit1 &lt;- fevd(TMX1, PORTw, units="deg C")
fit1
plot(fit1)
plot(fit1, "trace")
return.level(fit1)
return.level(fit1, do.ci=TRUE)
ci(fit1, return.period=c(2,20,100)) # Same as above.

lr.test(fit0, fit1)
ci(fit1, type="parameter")
par(mfrow=c(1,1))
ci(fit1, type="parameter", which.par=3, xrange=c(-0.4,0.01),
    nint=100, method="proflik", verbose=TRUE)

# 100-year return level
ci(fit1, method="proflik", xrange=c(22,28), verbose=TRUE)

plot(fit1, "probprob")
plot(fit1, "qq")
plot(fit1, "hist")
plot(fit1, "hist", ylim=c(0,0.25))

# Non-stationary model.
# Location as a function of AO index.

fit2 &lt;- fevd(TMX1, PORTw, location.fun=~AOindex, units="deg C")
fit2
plot(fit2)
plot(fit2, "trace")
# warnings are not critical here.
# Sometimes the nllh or gradients
# are not defined.

return.level(fit2)

v &lt;- make.qcov(fit2, vals=list(mu1=c(-1, 1)))
return.level(fit2, return.period=c(2, 20, 100), qcov=v)

# Note that first row is for AOindex = -1 and second
# row is for AOindex = 1.

lr.test(fit1, fit2)
# Also compare AIC and BIC

look1 &lt;- summary(fit1, silent=TRUE)
look1 &lt;- c(look1$AIC, look1$BIC)

look2 &lt;- summary(fit2, silent=TRUE)
look2 &lt;- c(look2$AIC, look2$BIC)

# Lower AIC/BIC is better.
names(look1) &lt;- names(look2) &lt;- c("AIC", "BIC")
look1
look2

par(mfrow=c(1,1))
plot(fit2, "rl")


## Fitting the GP df to threshold excesses.

# Hurricane damage data.

data(damage)

ny &lt;- tabulate(damage$Year)
# Looks like only, at most, 5 obs per year.

ny &lt;- mean(ny[ny &gt; 0])
fit0 &lt;- fevd(Dam, damage, threshold=6, type="Exponential", time.units="2.05/year")
fit0
plot(fit0)
plot(fit0, "trace") # ignore the warning.

fit1 &lt;- fevd(Dam, damage, threshold=6, type="GP", time.units="2.05/year")
fit1
plot(fit1) # ignore the warning.
plot(fit1, "trace")

return.level(fit1)

# lr.test(fit0, fit1)

# Fort Collins, CO precipitation

data(Fort)

## GP df

fit &lt;- fevd(Prec, Fort, threshold=0.395, type="GP", units="inches", verbose=TRUE)
fit
plot(fit)
plot(fit, "trace")

ci(fit, type="parameter")
par(mfrow=c(1,1))
ci(fit, type="return.level", method="proflik", xrange=c(4,7.5), verbose=TRUE)
# Can check using locator(2).

ci(fit, type="parameter", which.par=2, method="proflik", xrange=c(0.1, 0.3),
    verbose=TRUE) 
# Can check using locator(2).


## PP model.

fit &lt;- fevd(Prec, Fort, threshold=0.395, type="PP", units="inches", verbose=TRUE)
fit
plot(fit)
plot(fit, "trace")
ci(fit, type="parameter")

# Same thing, but just to try a different optimization method.
# And, for fun, a different way of entering the data set.
fit &lt;- fevd(Fort$Prec, threshold=0.395, type="PP",
    optim.args=list(method="Nelder-Mead"), units="inches", verbose=TRUE)
fit
plot(fit)
plot(fit, "trace")
ci(fit, type="parameter")

## PP model with blocks argument for computational efficiency # CJP2

system.time(fit &lt;- fevd(Prec, Fort, threshold=0.395, type="PP", units="inches", verbose=TRUE))

FortSub = Fort[Fort$Prec &gt; 0.395, ]
system.time(fit.blocks &lt;- fevd(Prec, FortSub, threshold=0.395,
type="PP", units="inches", blocks = list(nBlocks = 100), verbose=TRUE))
fit.blocks
plot(fit.blocks)
plot(fit.blocks, "trace")
ci(fit.blocks, type="parameter")

#
# Phoenix data
#
# Summer only with 62 days per year.

data(Tphap)

plot(MinT~ Year, data=Tphap)

# GP df
fit &lt;- fevd(-MinT ~1, Tphap, threshold=-73, type="GP", units="deg F",
    time.units="62/year", verbose=TRUE)

fit
plot(fit)
plot(fit, "trace")

# PP
fit &lt;- fevd(-MinT ~1, Tphap, threshold=-73, type="PP", units="deg F", time.units="62/year",
    use.phi=TRUE, optim.args=list(method="BFGS", gr=NULL), verbose=TRUE)
fit
plot(fit)
plot(fit, "trace")


# Non-stationary models

fit &lt;- fevd(Prec, Fort, threshold=0.395,
    scale.fun=~sin(2 * pi * (year - 1900)/365.25) + cos(2 * pi * (year - 1900)/365.25),
    type="GP", use.phi=TRUE, verbose=TRUE)
fit
plot(fit)
plot(fit, "trace")
ci(fit, type="parameter")

# Non-constant threshold.

# GP
fit &lt;- fevd(Prec, Fort, threshold=0.475, threshold.fun=~I(-0.15 * cos(2 * pi * month / 12)),
    type="GP", verbose=TRUE)
fit
plot(fit)
par(mfrow=c(1,1))
plot(fit, "rl", xlim=c(0, 365))

# PP

fit &lt;- fevd(Prec, Fort, threshold=0.475, threshold.fun=~I(-0.15 * cos(2 * pi * month / 12)),
    type="PP", verbose=TRUE)
fit
plot(fit)

## Bayesian PP with blocks for computational efficiency
## Note that 1999 iterations may not be sufficient; used here to
## minimize time spent fitting.
# CJP2
## CJP2: Eric, CRAN won't like this being run as part of the examples
## as it takes a long time; we'll probably want to wrap this in a \dontrun{}

set.seed(0)
system.time(fit &lt;- fevd(Prec, Fort, threshold=0.395,
    scale.fun=~sin(2 * pi * (year - 1900)/365.25) + cos(2 * pi * (year - 1900)/365.25),
    type="PP", method="Bayesian", iter=1999, use.phi=TRUE, verbose=TRUE))
fit
ci(fit, type="parameter")

set.seed(0)
FortSub &lt;- Fort[Fort$Prec &gt; 0.395, ]
system.time(fit2 &lt;- fevd(Prec, FortSub, threshold=0.395,
    scale.fun=~sin(2 * pi * (year - 1900)/365.25) + cos(2 * pi * (year -
1900)/365.25), type="PP", blocks = list(nBlocks= 100, data =
data.frame(year = 1900:1999)), use.phi=TRUE, method = "Bayesian",
iter=1999, verbose=TRUE))
# an order of magnitude faster

fit2
ci(fit2, type="parameter")

data(ftcanmax)

fit &lt;- fevd(Prec, ftcanmax, units="inches")
fit

plot(fit)
par(mfrow=c(1,1))
plot(fit, "probprob")
plot(fit, "hist")
plot(fit, "hist", ylim=c(0,0.01))
plot(fit, "density", ylim=c(0,0.01))
plot(fit, "trace")

distill(fit)
distill(fit, cov=FALSE)

fit2 &lt;- fevd(Prec, ftcanmax, location.fun=~Year)
fit2

plot(fit2)
##
# plot(fit2, "trace") # Gives warnings because of some NaNs produced
                      # (nothing to worry about).

lr.test(fit, fit2)

ci(fit)
ci(fit, type="parameter")

fit0 &lt;- fevd(Prec, ftcanmax, type="Gumbel")
fit0

plot(fit0)
lr.test(fit0, fit)
plot(fit0, "trace")

ci(fit, return.period=c(2, 20, 100))
ci(fit, type="return.level", method="proflik", return.period=20, verbose=TRUE)

ci(fit, type="parameter", method="proflik", which.par=3, xrange=c(-0.1,0.5), verbose=TRUE)

# L-moments
fitLM &lt;- fevd(Prec, ftcanmax, method="Lmoments", units="inches")
fitLM # less info.
plot(fitLM)
# above is slightly slower because of the parametric bootstrap
# for finding CIs in return levels.
par(mfrow=c(1,1))
plot(fitLM, "density", ylim=c(0,0.01))

# GP model.
# CJP2 : fixed so have 744/year (31 days *24 hours/day)

data(Denversp)

fitGP &lt;- fevd(Prec, Denversp, threshold=0.5, type="GP", units="mm",
    time.units="744/year", verbose=TRUE)

fitGP
plot(fitGP)
plot(fitGP, "trace")
# you can see the difficulty in getting good numerics here.
# the warnings are not a coding problem, but challenges in 
# the likelihood for the data.

# PP model.
fitPP &lt;- fevd(Prec, Denversp, threshold=0.5, type="PP", units="mm",
    time.units="744/year", verbose=TRUE)

fitPP
plot(fitPP)
plot(fitPP, "trace")

fitPP &lt;- fevd(Prec, Denversp, threshold=0.5, type="PP", optim.args=list(method="Nelder-Mead"),
    time.units="744/year", units="mm", verbose=TRUE)
fitPP
plot(fitPP) # Much better.
plot(fitPP, "trace")
# Better than above, but can see the difficulty!
# Can see the importance of good starting values!

# Try out for small samples
# Using one of the data example from Martins and Stedinger (2000)
z &lt;- c( -0.3955, -0.3948, -0.3913, -0.3161, -0.1657, 0.3129, 0.3386, 0.5979,
    1.4713, 1.8779, 1.9742, 2.0540, 2.6206, 4.9880, 10.3371 )

tmpML &lt;- fevd( z ) # Usual MLE.

# Find 0.999 quantile for the MLE fit.
# "True" 0.999 quantile is around 11.79
p &lt;- tmpML$results$par
qevd( 0.999, loc = p[ 1 ], scale = p[ 2 ], shape = p[ 3 ] )

tmpLM &lt;- fevd(z, method="Lmoments")
p &lt;- tmpLM$results
qevd( 0.999, loc = p[ 1 ], scale = p[ 2 ], shape = p[ 3 ] )

tmpGML &lt;- fevd(z, method="GMLE")
p &lt;- tmpGML$results$par
qevd( 0.999, loc = p[ 1 ], scale = p[ 2 ], shape = p[ 3 ] )

plot(tmpLM)
dev.new()
plot(tmpGML)

# Bayesian
fitB &lt;- fevd(Prec, ftcanmax, method="Bayesian", verbose=TRUE)
fitB
plot(fitB)
plot(fitB, "trace")

# Above looks good for scale and shape, but location does not appear to have found its way.
fitB &lt;- fevd(Prec, ftcanmax, method="Bayesian", priorParams=list(v=c(1, 10, 10)), verbose=TRUE)
fitB
plot(fitB)
plot(fitB, "trace")

# Better, but what if we start with poor initial values?
fitB &lt;- fevd(Prec, ftcanmax, method="Bayesian", priorParams=list(v=c(0.1, 10, 0.1)),
    initial=list(location=0, scale=0.1, shape=-0.5)), verbose=TRUE)
fitB
plot(fitB)
plot(fitB, "trace")

##
## Non-constant threshold.
##
data(Tphap)

# Negative of minimum temperatures.
plot(-Tphap$MinT)

fitGP2 &lt;- fevd(-MinT ~1, Tphap, threshold=c(-70,-7), threshold.fun=~I((Year - 48)/42), type="GP",
    time.units="62/year", verbose=TRUE)
fitGP2
plot(fitGP2)
plot(fitGP2, "trace")
par(mfrow=c(1,1))
plot(fitGP2, "hist")
plot(fitGP2, "rl")

ci(fitGP2, type="parameter")

##
## Non-stationary models.
##

data(PORTw)

# GEV
fitPORTstdmax &lt;- fevd(PORTw$TMX1, PORTw, scale.fun=~STDTMAX, use.phi=TRUE)
plot(fitPORTstdmax)
plot(fitPORTstdmax, "trace")
# One can see how finding the optimum value numerically can be tricky!

# Bayesian
fitPORTstdmaxB &lt;- fevd(PORTw$TMX1, PORTw, scale.fun=~STDTMAX, use.phi=TRUE,
    method="Bayesian", verbose=TRUE)
fitPORTstdmaxB
plot(fitPORTstdmaxB)
plot(fitPORTstdmaxB, "trace")

# Let us go crazy.
fitCrazy &lt;- fevd(PORTw$TMX1, PORTw, location.fun=~AOindex + STDTMAX, scale.fun=~STDTMAX,
    shape.fun=~STDMIN, use.phi=TRUE)
fitCrazy
plot(fitCrazy)
plot(fitCrazy, "trace")
# With so many parameters, you may need to stretch the device
# using your mouse in order to see them well.

ci(fitCrazy, type="parameter", which=2) # Hmmm.  NA NA.  Not good.
ci(fitCrazy, type="parameter", which=2, method="proflik", verbose=TRUE)
# Above not quite good enough (try to get better bounds).

ci(fitCrazy, type="parameter", which=2, method="proflik", xrange=c(0, 2), verbose=TRUE)
# Much better.


##
## Center and scale covariate.
##
data(Fort)

fitGPcross &lt;- fevd(Prec, Fort, threshold=0.395,
    scale.fun=~cos(day/365.25) + sin(day/365.25) + I((year - 1900)/99),
    type="GP", use.phi=TRUE, units="inches")

fitGPcross
plot(fitGPcross) # looks good!

# Get a closer look at the effective return levels.
par(mfrow=c(1,1))
plot(fitGPcross, "rl", xlim=c(10000,12000))

lr.test(fitGPfc, fitGPcross)


## End(Not run)

</code></pre>

<hr>
<h2 id='findAllMCMCpars'>
Manipulate MCMC Output from fevd Objects
</h2><span id='topic+findAllMCMCpars'></span>

<h3>Description</h3>

<p>Manipulates the MCMC sample from an &ldquo;fevd&rdquo; object to be in a unified format that can be used in other function calls.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>findAllMCMCpars(x, burn.in = 499, qcov = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="findAllMCMCpars_+3A_x">x</code></td>
<td>

<p>Object of class &ldquo;fevd&rdquo; with component <code>method</code> = &ldquo;Bayesian&rdquo;.
</p>
</td></tr>
<tr><td><code id="findAllMCMCpars_+3A_burn.in">burn.in</code></td>
<td>

<p>Burn in period.
</p>
</td></tr>
<tr><td><code id="findAllMCMCpars_+3A_qcov">qcov</code></td>
<td>

<p>Matrix giving specific covariate values.  See 'make.qcov' for more details.  If not suplied, original covariates are used.
</p>
</td></tr>
<tr><td><code id="findAllMCMCpars_+3A_...">...</code></td>
<td>

<p>Not used.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function was first constructed for use by <code>postmode</code>, but might be useful in other areas as well.  It evaluates any parameters that vary according to covariates at the values supplied by <code>qcov</code> or else at the covariate values used to obtain the original fit (default).  If a model does not contain one or more parameters (e.g., the GP does not have a location component), then a column with these values (set to zero) are returned.  That is, a matrix with columns corresponding to location, scale, shape and threshold are returned regardless of the model fit so that subsequent calls to functions like <code>fevd</code> can be made more easily.
</p>
<p>This function is intended more as an internal function, but may still be useful to end users.
</p>
<p>This function is very similar to <code>findpars</code>, but is only for MCMC samples and returns the entire MCMC sample of parameters.  Also, returns a matrix instead of a list.
</p>


<h3>Value</h3>

<p>A matrix is returned whose rows correspond to the MCMC samples (less burn in), and whose columns are &ldquo;location&rdquo; (if no location parameter is in the model, this column is still given with all values identical to zero), &ldquo;scale&rdquo;, &ldquo;shape&rdquo; and &ldquo;threshold&rdquo;.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fevd">fevd</a></code>, <code><a href="#topic+findpars">findpars</a></code>, <code><a href="#topic+postmode">postmode</a></code>
</p>

<hr>
<h2 id='findpars'>
Get EVD Parameters
</h2><span id='topic+findpars'></span><span id='topic+findpars.fevd'></span><span id='topic+findpars.fevd.bayesian'></span><span id='topic+findpars.fevd.lmoments'></span><span id='topic+findpars.fevd.mle'></span>

<h3>Description</h3>

<p>Obtain the parameters from an fevd object.  This function differs greatly from distill.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>findpars(x, ...)

## S3 method for class 'fevd'
findpars(x, ...)

## S3 method for class 'fevd.bayesian'
findpars(x, burn.in = 499, FUN = "mean",
    use.blocks = FALSE, ..., qcov = NULL)

## S3 method for class 'fevd.lmoments'
findpars(x, ...)

## S3 method for class 'fevd.mle'
findpars(x, use.blocks = FALSE, ..., qcov = NULL)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="findpars_+3A_x">x</code></td>
<td>

<p>A list object of class &ldquo;fevd&rdquo; as returned by <code>fevd</code>.
</p>
</td></tr>
<tr><td><code id="findpars_+3A_burn.in">burn.in</code></td>
<td>
<p>number giving the burn in value.  The first 1:burn.in will not be used in obtaining parmaeter estiamtes. </p>
</td></tr>
<tr><td><code id="findpars_+3A_fun">FUN</code></td>
<td>
<p>character string naming a function, or a function, to use
to find the parameter estimates from the MCMC sample.  Default is to
take the posterior mean (after burn in).</p>
</td></tr>
<tr><td><code id="findpars_+3A_use.blocks">use.blocks</code></td>
<td>
<p>logical: If <code>TRUE</code> and <code>x</code> was fit with
<code>blocks</code> provided, returns parameters for each block</p>
</td></tr>
<tr><td><code id="findpars_+3A_...">...</code></td>
<td>

<p>Not used.
</p>
</td></tr>
<tr><td><code id="findpars_+3A_qcov">qcov</code></td>
<td>
<p>numeric matrix with rows the same length as <code>q</code> and columns equal to the number of parameters (+ 1 for the threshold, if a POT model).  This gives any covariate values for a nonstationary model.  If NULL, and model is non-stationary, only the intercept terms for modeled parameters are used, and if a non-constant threshold, only the first threshold value is used.  Not used if model is stationary. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function finds the EVD parameters for each value of the covariates in a non-stationary model.  In the case of a stationary model, it will return vectors of length equal to the length of the data that simply repeat the parameter(s) value(s).
</p>
<p>Note that this differs greatly from <code>distill</code>, which simply returns a vector of the length of the number of parameters in the model.  This function returns a named list containing the EVD parameter values possibly for each value of the covariates used to fit the model.  For example, if a GEV(location(t), scale, shape) is fit with location(t) = mu0 + mu1 * t, say, then the &ldquo;location&rdquo; component of the returned list will have a vector of mu0 + mu1 * t for each value of t used in the model fit.
</p>


<h3>Value</h3>

<p>A list object is returned with components
</p>
<table>
<tr><td><code>location</code>, <code>scale</code>, <code>shape</code></td>
<td>
<p>vector of parameter values (or NULL if the parameter is not in the model).  For stationary models, or for parameters that are fixed in the otherwise non-stationary model, the vectors will repeat the parameter value.  The length of the vectors equals the length of the data used to fit the models.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fevd">fevd</a></code>, <code><a href="distillery.html#topic+distill">distill</a></code>, <code><a href="#topic+parcov.fevd">parcov.fevd</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>z &lt;- revd(100, loc=20, scale=0.5, shape=-0.2)
fit &lt;- fevd(z)
fit

findpars(fit)

## Not run: 
data(PORTw)
fit &lt;- fevd(TMX1, PORTw, location.fun=~AOindex, units="deg C")
fit

findpars(fit)


## End(Not run)
</code></pre>

<hr>
<h2 id='Flood'> United States Total Economic Damage Resulting from Floods</h2><span id='topic+Flood'></span>

<h3>Description</h3>

<p>United States total economic damage (in billions of U.S. dollars) caused by floods by hydrologic year from
1932-1997.  See Pielke and Downton (2000) for more information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Flood)</code></pre>


<h3>Format</h3>

<p>A data frame with 66 observations on the following 5 variables.
</p>

<dl>
<dt>OBS</dt><dd><p>a numeric vector giving the line number.</p>
</dd>
<dt>HYEAR</dt><dd><p>a numeric vector giving the hydrologic year.</p>
</dd>
<dt>USDMG</dt><dd><p>a numeric vector giving total economic damage (in billions of U.S. dollars) caused by floods.</p>
</dd>
<dt>DMGPC</dt><dd><p>a numeric vector giving damage per capita.</p>
</dd>
<dt>LOSSPW</dt><dd><p>a numeric vector giving damage per unit wealth.</p>
</dd>
</dl>



<h3>Details</h3>

<p>From Pielke and Downton (2000):
</p>
<p>The National Weather Service (NWS) maintains a national flood damage record from 1903 to the present, and state level
data from 1983 to the present.  The reported losses are for &quot;significant flood events&quot; and include only direct economic
damage that results from flooding caused by ranfall and/or snowmelt.  The annual losses are based on &quot;hydrologic years&quot;
from October through September.  Flood damage per capita is computed by dividing the inflation-adjusted losses for each
hydrological year by the estimated population on 1 July of that year (www.census.gov).  Flood damage per million dollars
of national wealth uses the net stock of fixed reproducible tangible wealth in millions of current dollars (see Pielke
and Downton (2000) for more details; see also Katz et al. (2002) for analysis).
</p>


<h3>Source</h3>

<p>NWS web site: <a href="https://www.nws.noaa.gov/">https://www.nws.noaa.gov/</a>
</p>


<h3>References</h3>

 
<p>Katz, R. W., Parlange, M. B. and Naveau, P. (2002) Statistics of extremes in hydrology, <em>Advances in Water Resources</em>, <b>25</b>, 1287&ndash;1304.
</p>
<p>Pielke, R. A. Jr. and Downton, M. W. (2000) Precipitation and damaging floods: trends in the United States, 1932-97, <em>Journal of Climate</em>, <b>13</b>, (20), 3625&ndash;3637.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Flood)
plot( Flood[,2], Flood[,3], type="l", lwd=2, xlab="hydrologic year",
    ylab="Total economic damage (billions of U.S. dollars)")
</code></pre>

<hr>
<h2 id='Fort'> Daily precipitation amounts in Fort Collins, Colorado.</h2><span id='topic+Fort'></span>

<h3>Description</h3>

<p>Daily precipitation amounts (inches) from a single rain gauge in Fort Collins, Colorado.  See
Katz et al. (2002) Sec. 2.3.1 for more information and analyses.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Fort)</code></pre>


<h3>Format</h3>

<p>A data frame with dimension 36524 by 5.  Columns are: &quot;obs&quot;, &quot;tobs&quot;, &quot;month&quot;, &quot;day&quot;, &quot;year&quot; and &quot;Prec&quot;; where
&quot;Prec&quot; is the daily precipitation amount (inches).
</p>


<h3>Source</h3>

<p>Originally from the Colorado Climate Center at Colorado State University.  The Colorado state climatologist office no longer provides this data without charge. The data can be obtained from the NOAA/NCDC web site, but there are slight differences (i.e., some missing values for temperature).
</p>


<h3>References</h3>

<p>Katz, R. W., Parlange, M. B. and Naveau, P. (2002) Statistics of extremes in hydrology. <em>Advances in Water Resources</em>, <b>25</b>, 1287&ndash;1304.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Fort)
str(Fort)
plot(Fort[,"month"], Fort[,"Prec"], xlab="month", ylab="daily precipitation (inches)")
</code></pre>

<hr>
<h2 id='fpois'>
Fit Homogeneous Poisson to Data and Test Equality of Mean and Variance
</h2><span id='topic+fpois'></span><span id='topic+fpois.default'></span><span id='topic+fpois.data.frame'></span><span id='topic+fpois.matrix'></span><span id='topic+fpois.list'></span>

<h3>Description</h3>

<p>Fit a homogeneous Poisson to data and test whether or not the mean and variance are equal.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fpois(x, na.action = na.fail, ...)

## Default S3 method:
fpois(x, na.action = na.fail, ...)

## S3 method for class 'data.frame'
fpois(x, na.action = na.fail, ..., which.col = 1)

## S3 method for class 'matrix'
fpois(x, na.action = na.fail, ..., which.col = 1)

## S3 method for class 'list'
fpois(x, na.action = na.fail, ..., which.component = 1)


</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fpois_+3A_x">x</code></td>
<td>

<p>numeric, matrix, data frame or list object containing the data to which the Poisson is to be fit.
</p>
</td></tr>
<tr><td><code id="fpois_+3A_na.action">na.action</code></td>
<td>

<p>function to be called to handle missing values.
</p>
</td></tr>
<tr><td><code id="fpois_+3A_...">...</code></td>
<td>

<p>Not used.
</p>
</td></tr>
<tr><td><code id="fpois_+3A_which.col">which.col</code>, <code id="fpois_+3A_which.component">which.component</code></td>
<td>
<p>column or component (list) number containing the data to which the Poisson is to be fit.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The probability function for the (homogeneous) Poisson distribution is given by:
</p>
<p>Pr( N = k ) = exp(-lambda) * lambda^k / k!
</p>
<p>for k = 0, 1, 2, ...
</p>
<p>The rate parameter, lambda, is both the mean and the variance of the Poisson distribution.
To test the adequacy of the Poisson fit, therefore, it makes sense to test whether or not
the mean equals the variance.  R. A. Fisher showed that under the assumption that X_1, ...,
X_n follow a Poisson distribution, the statistic given by:
</p>
<p>D = (n - 1) * var(X_1) / mean(X_1)
</p>
<p>follows a Chi-square distribution with n - 1 degrees of freedom.  Therefore, the p-value
for the one-sided alternative (greater) is obtained by finding the probability of being greater
than D based on a Chi-square distribution with n - 1 degrees of freedom.
</p>


<h3>Value</h3>

<p>A list of class &ldquo;htest&rdquo; is returned with components:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>The value of the dispersion D</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>named numeric vector giving the estimated mean, variance, and degrees of freedom.</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>character string with the value &ldquo;greater&rdquo; indicating the one-sided alternative hypothesis.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value for the test.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>character string stating the name of the test.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>character naming the data used by the test (if a vector is applied).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+glm">glm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Rsum)
fpois(Rsum$Ct)

## Not run: 

# Because 'Rsum' is a data frame object,
# the above can also be carried out as:

fpois(Rsum, which.col = 3)

# Or:

fpois(Rsum, which.col = "Ct")

##
## For a non-homogeneous fit, use glm.
##
## For example, to fit the non-homogeneous Poisson model
## Enso as a covariate:
##

fit &lt;- glm(Ct~EN, data = Rsum, family = poisson())
summary(fit)

## End(Not run)

</code></pre>

<hr>
<h2 id='ftcanmax'> Annual Maximum Precipitation: Fort Collins, Colorado</h2><span id='topic+ftcanmax'></span>

<h3>Description</h3>

<p>Annual maximum precipitation (inches) for one rain gauge in Fort Collins, Colorado from 1900 through 1999.
See Katz et al. (2002) Sec. 2.3.1 for more information and analyses.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(ftcanmax)</code></pre>


<h3>Format</h3>

<p>A data frame with 100 observations on the following 2 variables.
</p>

<dl>
<dt>Year</dt><dd><p>a numeric vector giving the Year.</p>
</dd>
<dt>Prec</dt><dd><p>a numeric vector giving the annual maximum precipitation amount in inches.</p>
</dd>
</dl>



<h3>Source</h3>

<p>Originally from the Colorado Climate Center at Colorado State University.  The Colorado state climatologist office no longer provides this data without charge. The data can be obtained from the NOAA/NCDC web site, but there are slight differences (i.e., some missing values for temperature).  The annual maximum precipitation data is taken directly from the daily precipitation data available in this package under the name &ldquo;Fort&rdquo;.
</p>


<h3>References</h3>

<p>Katz, R. W., Parlange, M. B. and Naveau, P. (2002) Statistics of extremes in hydrology. <em>Advances in Water Resources</em>, <b>25</b>, 1287&ndash;1304.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ftcanmax)
str(ftcanmax)
plot(ftcanmax, type="l", lwd=2)

</code></pre>

<hr>
<h2 id='HEAT'> Summer Maximum and Minimum Temperature: Phoenix, Arizona</h2><span id='topic+HEAT'></span>

<h3>Description</h3>

<p>Summer maximum and minimum temperature (degrees Fahrenheit) for July through August 1948 through 1990 at
Sky Harbor airport in Phoenix, Arizona.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(HEAT)</code></pre>


<h3>Format</h3>

<p>A data frame with 43 observations on the following 3 variables.
</p>

<dl>
<dt>Year</dt><dd><p>a numeric vector giving the number of years since 1900.</p>
</dd>
<dt>Tmax</dt><dd><p>a numeric vector giving the Summer maximum temperatures in degrees Fahrenheit.</p>
</dd>
<dt>Tmin</dt><dd><p>a numeric vector giving the Summer minimum temperatures in degrees Fahrenheit.</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data is Summer maximum and minimum temperature for the months of July through August from 1948 through 1990.
</p>


<h3>Source</h3>

<p>U.S. National Weather Service Forecast office at the Phoenix Sky Harbor Airport.
</p>


<h3>References</h3>

<p>Balling, R. C., Jr., Skindlov, J. A. and Phillips, D. H. (1990) The impact of increasing summer mean temperatures on extreme maximum and minimum temperatures in Phoenix, Arizona. <em>Journal of Climate</em>, <b>3</b>, 1491&ndash;1494.
</p>
<p>Tarleton, L. F. and Katz, R. W. (1995) Statistical explanation for trends in extreme summer temperatures at Phoenix, A.Z. <em>Journal of Climate</em>, <b>8</b>, (6), 1704&ndash;1708.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(HEAT)
str(HEAT)
plot(HEAT)
</code></pre>

<hr>
<h2 id='hwmi'>Heat Wave Magnitude Index</h2><span id='topic+hwmi'></span>

<h3>Description</h3>

<p>This function computes the Heat Wave Magnitude Index and the associated duration and starting date of a heat wave event.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hwmi(yTref, Tref, yTemp, Temp)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hwmi_+3A_ytref">yTref</code></td>
<td>
<p>a single numeric value giving the starting year of Tref</p>
</td></tr>
<tr><td><code id="hwmi_+3A_tref">Tref</code></td>
<td>
<p>a numeric vector of daily maximum temperatures for a 32-year reference period used to calculate threshold and empirical cumulative distribution function (ecdf).</p>
</td></tr>
<tr><td><code id="hwmi_+3A_ytemp">yTemp</code></td>
<td>
<p>a single numeric value giving the starting year of Temp</p>
</td></tr>
<tr><td><code id="hwmi_+3A_temp">Temp</code></td>
<td>
<p>a numeric vector of daily maximum temperature for at least one year (with length not shorter than 365 days)  or n-years (each with length not shorter than 365 days) containing the data to which the HWMI is to be calculated.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function takes a daily maximum temperature time series as input and computes the climate index HWMI (Heat Wave Magnitude Index).  The Heat Wave Magnitude Index is defined as the maximum magnitude of the heat waves in a year. A &ldquo;heat wave&rdquo; is defined as a sequence of 3 or more days in which the daily maximum temperature is above the 90th percentile of daily maximum temperature for a 31-day running window surrounding this day during the baseline period (e.g., 1981-2010).
</p>
<p>Note that the argument <code>Tref</code> must have daily maximum temperatures for a 32-year period (e.g., using the baseline period of 1981 to 2010, <code>Tref</code> must be for 1980 through 2011).  The first and the 32nd years are needed to calculate the daily threshold over the first and the last year of the baseline period (1981-2010).
</p>
<p>If HWMI is calculated in the Southern Hemisphere, then, in order not to split a Heat Wave event into two, the year should start on the 1st of July and end on the 30th of June of the following year.
</p>
<p>In other words:
</p>
<p>1. Tref will be from the 1st of July 1980 up to the 30th of June 2012
</p>
<p>2. Similarly for Temp, each 365 days in a year must be taken between the 1st of July and the 30th of June.
</p>


<h3>Value</h3>

<p>A list with the following components: 
hwmi: a numeric vector containing the hwmi value for each year and the associated duration and starting day (number between 1 and 365) for each heat wave event in a year.
thr: a numeric vector containing 365 temperature values representing the daily threshold for the reference period (1981-2010 or any other 30 years period).
pdfx: the &quot;n&quot; (n was fixed to 512) coordinates of the points (sum of three daily maximum temperatures) where the density is estimated.
pdfy: the estimated density values.  These will be non-negative, but can be zero.
</p>


<h3>Author(s)</h3>

<p>Simone Russo &lt;simone.russo@jrc.ec.europa.eu&gt;</p>


<h3>References</h3>

<p>Russo, S. and Coauthors, 2014. Magnitude of extreme heat waves in present climate and their projection in a warming world. <em>J. Geophys. Res.</em>, doi:10.1002/2014JD022098.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("CarcasonneHeat")

tiid &lt;- CarcasonneHeat[2,]

jan1980 &lt;- which(tiid == 19800101)
jan2003 &lt;- which(tiid == 20030101)
dec2003 &lt;- which(tiid == 20031231)
dec2011 &lt;- which(tiid == 20111231)

Temp &lt;- CarcasonneHeat[3, jan2003:dec2003] / 10
Tref &lt;- CarcasonneHeat[3, jan1980:dec2011] / 10

##hwmi calculation
hwmiFr2003 &lt;- hwmi(1980, Tref, 2003, Temp)

#### Heat Wave occurred in Carcassonne, France, 2003

plot(c(150:270), Temp[150:270], xlim = c(150, 270),
    ylim = c((min(hwmiFr2003$thr[150:270]) - 
        sd(hwmiFr2003$thr[150:270])), max(Temp[150:270])),
    xlab = "days", ylab = "temperature", col = 8)

par(new = TRUE)
plot(c(150:270), hwmiFr2003$thr[150:270], type = "l",
    xlim = c(150,270),
    ylim = c((min(hwmiFr2003$thr[150:270]) - sd(hwmiFr2003$thr[150:270])),
    max(Temp)), xlab = "", ylab = "", col = 1, lwd = 2)

par(new = TRUE)
plot(c(hwmiFr2003$hwmi[1,3]:(hwmiFr2003$hwmi[1,3] + hwmiFr2003$hwmi[1,2]-1)),
    Temp[hwmiFr2003$hwmi[1,3]:(hwmiFr2003$hwmi[1,3] + hwmiFr2003$hwmi[1,2] - 1)],
    xlim = c(150,270),
    ylim = c((min(hwmiFr2003$thr[150:270]) - sd(hwmiFr2003$thr[150:270])),
        max(Temp[150:270])),
    xlab = "", ylab = "", col = 4, type = "b",
    main = "Carcassonne, France, 2003", lwd = 2)

text(175, 42, "hwmi = 3.68", col = 4, font = 2)
text(175, 41, "Duration = 12 days", col = 4, font = 2)
text(175, 40, "Starting day = 214 (02.Aug.2003)", col = 4, font = 2)


</code></pre>

<hr>
<h2 id='hwmid'>Heat Wave Magnitude Index</h2><span id='topic+hwmid'></span>

<h3>Description</h3>

<p>This function computes the Heat Wave Magnitude Index and the associated duration and starting date of a heat wave event.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hwmid(yTref, Tref, yTemp, Temp)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hwmid_+3A_ytref">yTref</code></td>
<td>
<p>a single numeric value giving the starting year of Tref</p>
</td></tr>
<tr><td><code id="hwmid_+3A_tref">Tref</code></td>
<td>
<p>a numeric vector of daily maximum temperatures for a 32-year reference period used to calculate threshold, T30ymax and T30ymin as defined below.</p>
</td></tr>
<tr><td><code id="hwmid_+3A_ytemp">yTemp</code></td>
<td>
<p>a single numeric value giving the starting year of Temp</p>
</td></tr>
<tr><td><code id="hwmid_+3A_temp">Temp</code></td>
<td>
<p>a numeric vector of daily maximum (minimum or mean) temperature for at least one year (with length not shorter than 365 days)  or n-years (each with length not shorter than 365 days) containing the data to which the HWMId is to be calculated.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function takes a daily temperature time series as input and computes the climate index HWMId (Heat Wave Magnitude Index daily).  The Heat Wave Magnitude Index daily is defined as the maximum magnitude of the heat waves in a year. A &ldquo;heat wave&rdquo; is defined as a sequence of 3 or more days in which the daily maximum temperature is above the 90th percentile of daily maximum temperature for a 31-day running window surrounding this day during the baseline period (e.g., 1981-2010).
</p>
<p>Note that the argument <code>Tref</code> must have daily temperatures for a 32-year period (e.g., using the baseline period of 1981 to 2010, <code>Tref</code> must be for 1980 through 2011).  The first and the 32nd years are needed to calculate the daily threshold over the first and the last year of the baseline period (1981-2010).
</p>


<h3>Value</h3>

<p>A list with the following components: 
hwmid: a numeric vector containing the hwmid value for each year and the associated duration and starting day (number between 1 and 365) for each heat wave event in a year.
thr: a numeric vector containing 365 temperature values representing the daily threshold for the reference period (1981-2010 or any other 30 years period).
T30y75p: a single numeric value giving the 75th percentile value of the time series calculated from Tref and composed of 30-year annual maximum temperatures of the baseline period.
T30y25p: a single numeric value giving the 25th percentile value of the time series calculated from Tref and composed of 30-year annual maximum temperatures of the baseline period.
</p>


<h3>Author(s)</h3>

<p>Simone Russo &lt;simone.russo@jrc.ec.europa.eu, simone.russo@isprambiente.it&gt;</p>


<h3>References</h3>

<p>Russo, S., J. Sillmann, E. Fischer, 2015. Top ten European heatwaves since 1950 and their occurrence in the coming decades. <em>Environmental Research Letters</em>, <b>10</b>, 124003, doi:10.1088/1748-9326/10/12/124003.
</p>
<p>Russo, S. and Coauthors, 2014. Magnitude of extreme heat waves in present climate and their projection in a warming world. <em>J. Geophys. Res.</em>, doi:10.1002/2014JD022098.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("CarcasonneHeat")

tiid &lt;- CarcasonneHeat[2,]

jan1980 &lt;- which(tiid == 19800101)
jan2003 &lt;- which(tiid == 20030101)
dec2003 &lt;- which(tiid == 20031231)
dec2011 &lt;- which(tiid == 20111231)

Temp &lt;- CarcasonneHeat[3, jan2003:dec2003] / 10
Tref &lt;- CarcasonneHeat[3, jan1980:dec2011] / 10


##hwmid calculation
hwmidFr2003 &lt;- hwmid(1980, Tref, 2003, Temp)
hwmiFr2003 &lt;- hwmi(1980, Tref, 2003, Temp)

T30y25p &lt;- hwmidFr2003$T30y25p
T30y75p &lt;- hwmidFr2003$T30y75p
range30y &lt;- (T30y75p - T30y25p)

#daymag&lt;-(Temp[214:225]-hwmidFr2003$T30ymin)/(hwmidFr2003$T30ymax-hwmidFr2003$T30ymin)
#### Heat Wave occurred in Carcassonne, France, 2003

split.screen( rbind( c(0, 1, 0.6, 1), c(0, 0.5, 0, 0.6), c(0.5, 1, 0, 0.6) ) )
screen(1)
par( mar = c(2, 2, 2, 0) )
plot( c(1:365), Temp[1:365], xlim = c(190, 240), ylim = c(25, 50),
    xlab = "", ylab = "", cex.axis = 1.1, col = 8, font.axis = 2)

par( new = TRUE )
plot( c(150:270), hwmiFr2003$thr[150:270], type = "l",xlim = c(190, 240),
    ylim = c(25, 50), xlab = "", ylab = "", col = 1, lwd = 2, axes = FALSE)

par(new = TRUE)


plot(c(214:216), Temp[214:216], xlim = c(190, 240), ylim = c(25, 50),
    xlab = "", ylab = "", col = 4, type = "b", lwd = 2, axes = FALSE,
    pch = "a", cex = 1.2)

par( new = TRUE)
plot(c(217:219), Temp[217:219], xlim = c(190, 240), ylim = c(25,50),
    xlab = "", ylab = "", col = 4, type = "b", lwd = 2, axes = FALSE,
    pch = "b", cex = 1.2)

par(new=TRUE)
plot(c(220:222), Temp[220:222], xlim = c(190, 240), ylim = c(25, 50),
    xlab = "", ylab = "", col = 4, type = "b", lwd = 2, axes = FALSE,
    pch = "c", cex = 1.2)

par(new=TRUE)
plot(c(223:225), Temp[223:225], xlim = c(190, 240), ylim = c(25, 50),
    xlab = "", ylab = "", col = 4, type = "b", lwd = 2, axes = FALSE,
    pch = "d", cex = 1.2)


par(new=TRUE)
plot(c(214:216), (Temp[214:216]+5), xlim = c(190, 240), ylim = c(25, 50),
    xlab = "", ylab = "", col = 3, type = "b", lwd = 2, axes = FALSE,
    pch = "a", cex = 1.2)

par(new=TRUE)
plot(c(217:219), (Temp[217:219]+5), xlim = c(190, 240), ylim = c(25, 50),
    xlab = "", ylab = "", col = 3, type = "b", lwd = 2, axes = FALSE,
    pch = "b", cex = 1.2)

par(new=TRUE)
plot(c(220:222), (Temp[220:222]+5), xlim = c(190, 240), ylim = c(25, 50),
    xlab = "", ylab = "", col = 3, type = "b", lwd = 2, axes = FALSE,
    pch = "c", cex = 1.2)

par(new=TRUE)
plot(c(223:225), (Temp[223:225]+5), xlim = c(190, 240), ylim = c(25, 50),
    xlab = "", ylab = "", col = 3, type = "b", lwd = 2, axes = FALSE,
    pch = "d", cex = 1.2)

text(200, 50, "HW2", col = 3, font = 2)
text(200, 48, "HWMI=4", col = 3, font = 2)
text(200, 46, "HWMId = 41.9",col = 3, font = 2)

text(200, 42, "HW1", col = 4, font = 2)
text(200, 40, "HWMI = 3.68", col = 4, font = 2)
text(200, 38, "HWMId=18.6",col = 4, font = 2)


box()
</code></pre>

<hr>
<h2 id='is.fixedfevd'>
Stationary Fitted Model Check
</h2><span id='topic+is.fixedfevd'></span><span id='topic+check.constant'></span>

<h3>Description</h3>

<p>Test if a fitted <code>fevd</code> object is stationary or not.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.fixedfevd(x)

check.constant(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.fixedfevd_+3A_x">x</code></td>
<td>

<p>A list object of class &ldquo;fevd&rdquo; as returned by <code>fevd</code>.
</p>
<p>For <code>check.constant</code>, this may be a formula or vector.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is mostly intended as an internal function, but it may be useful generally.
</p>
<p><code>check.constant</code> determines if a formula is given simply by ~ 1.  It is used by <code>is.fixedfevd</code>.
</p>


<h3>Value</h3>

<p>logical of length one stating whether the fitted model is stationary (TRUE) or not (FALSE).
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fevd">fevd</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>z &lt;- revd(100, loc=20, scale=0.5, shape=-0.2)
fit &lt;- fevd(z)
fit

is.fixedfevd(fit)

</code></pre>

<hr>
<h2 id='levd'>
Extreme Value Likelihood
</h2><span id='topic+levd'></span>

<h3>Description</h3>

<p>Find the EVD parameter likelihood given data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>levd(x, threshold, location, scale, shape,
    type = c("GEV", "GP", "PP", "Gumbel", "Weibull", "Frechet",
    "Exponential", "Beta", "Pareto"), log = TRUE, negative = TRUE,
    span, npy = 365.25, infval = Inf, weights = 1, blocks = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="levd_+3A_x">x</code></td>
<td>

<p>A numeric vector of data of length n.
</p>
</td></tr>
<tr><td><code id="levd_+3A_threshold">threshold</code></td>
<td>

<p>number or numeric vector of length n giving the desired threshold, if applicable.
</p>
</td></tr>
<tr><td><code id="levd_+3A_location">location</code></td>
<td>

<p>number or numeric vector of length n giving the location parameter value(s), if applicable.
</p>
</td></tr>
<tr><td><code id="levd_+3A_scale">scale</code></td>
<td>

<p>number or numeric vector of length n giving the scale parameter value(s).
</p>
</td></tr>
<tr><td><code id="levd_+3A_shape">shape</code></td>
<td>

<p>number or numeric vector of length n giving the shape parameter value(s), if applicable.
</p>
</td></tr>
<tr><td><code id="levd_+3A_type">type</code></td>
<td>

<p>character string naming the particular EVD for which to compute the likelihood.
</p>
</td></tr>
<tr><td><code id="levd_+3A_log">log</code>, <code id="levd_+3A_negative">negative</code></td>
<td>

<p>logicals; should the negative log-likelihood (default) be returned (both TRUE) or the likelihood (both FALSE)?  It is possible to return other possibilities such as the negative likelihood (log = FALSE, negative = TRUE) or the log-likelihood (log = TRUE, negative = FALSE).
</p>
</td></tr>
<tr><td><code id="levd_+3A_span">span</code></td>
<td>

<p>number stating how many periods (usually years) the data cover (applicable only for PP models).  Currently not used.
</p>
</td></tr>
<tr><td><code id="levd_+3A_npy">npy</code></td>
<td>

<p>number of points per period (period is usually years).
</p>
</td></tr>
<tr><td><code id="levd_+3A_infval">infval</code></td>
<td>

<p>Value to return if the likelihood is infinite.  If negative is FALSE, the negative of this value will be returned.  The default is to return <code>Inf</code>, but noe that for optimization routines, this would result in an error and stop the process.  Therefore, it can be advantageous to use a very large value instead.
</p>
</td></tr>
<tr><td><code id="levd_+3A_weights">weights</code></td>
<td>
<p>numeric of length 1 or n giving weights to be applied in the likelihood calculation (e.g., if some data points are to be weighted more/less heavily than others).</p>
</td></tr>
<tr><td><code id="levd_+3A_blocks">blocks</code></td>
<td>
<p>An optional list containing information required to evaluate the
likelihood of point process models in a computationally-efficient
manner by using only the exceedances and not the observations below
the threshold(s). See details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is called by a wrapper function within <code>fevd</code> and other functions.  It is generally an internal function, but may be useful for some users.
</p>
<p>The negative log-likelihood for the generalized extreme value (GEV) df, possibly with parameters that are functions of covariates, yi) is given by:
</p>
<p>sum(log(scale(yi))) + sum(z^(-1/shape(yi)) + sum(log(z) * (1/shape(yi) + 1)),
</p>
<p>where z = (x - location(yi))/scale(yi), x are the data.  For the Frechet and Weibull cases, the shape parameter is forced to have the correct sign, so it does not matter if the user chooses positive or negative shape.  In the case of shape = 0, defined by continuity (Gumbel case), the negative log-likelihood simplifies to:
</p>
<p>sum(log(scale(yi))) + sum(z) + sum(exp(-z)),
</p>
<p>where z is as above.
</p>
<p>The negative log-likelihood for the GP df is given by:
</p>
<p>sum(log(scale(yi))) + sum( log(z) * (1/shape(yi) + 1)),
</p>
<p>where z = 1 + shape(yi) * t, where t = (x[x &gt; threshold] - threshold(yi))/scale(yi).  Similar to the GEV df, the Beta and Pareto cases are forced to have the correct sign for the shape parameter.  In the case of shape = 0, defined by continuity (Exponential case), the negative log-likelihood simplifies to:
</p>
<p>sum(log(scale(yi))) + z,
</p>
<p>where z is as above in the GP negative log-likelihood.
</p>
<p>See Coles (2001) for more details.
</p>
<p>Using Blocks to Reduce Computation in PP Fitting:
</p>
<p>When <code>blocks</code> is supplied, the user should provide only the
exceedances and not all of the data values. The list should contain a
component called <code>nBlocks</code> indicating the number of observations
within a block, where blocks are defined in a manner analogous to that
used in GEV models. The list should also contain components named
<code>threshold</code>, <code>location</code>, <code>scale</code>, <code>shape</code>, and <code>weights</code> corresponding
to the arguments of the same name supplied to <code>levd</code>, but with values
on a per block basis. If some of the observations within any block are
missing (assuming missing at random or missing completely at random),
the list should contain a <code>proportionMissing</code> component that is a
vector with one value per block indicating the proportion of
observations missing for the block. Scalar values are allowed when a
component is stationary. Warning: to properly analyze nonstationary
models, the components must be constant within each block.
</p>


<h3>Value</h3>

<p>A single number giving the likelihood value (or negative log-likelihood or log-likelihood or negative likelihood depending on the value of the log and negative arguments).
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Coles, S.  (2001) <em>An introduction to statistical modeling of extreme values</em>, London, U.K.: Springer-Verlag, 208 pp.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fevd">fevd</a></code>, <code><a href="#topic+profliker">profliker</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ftcanmax)

levd(ftcanmax$Prec, location=134.66520, scale=53.28089, shape=0.17363)


</code></pre>

<hr>
<h2 id='lr.test'>
Likelihood-Ratio Test
</h2><span id='topic+lr.test'></span>

<h3>Description</h3>

<p>Conduct the likelihood-ratio test for two nested extreme value distribution models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lr.test(x, y, alpha = 0.05, df = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lr.test_+3A_x">x</code>, <code id="lr.test_+3A_y">y</code></td>
<td>

<p>Each can be either an object of class &ldquo;fevd&rdquo; (provided the fit method is MLE or GMLE) or a single numeric giving the negative log-likelihod value for each model.  <code>x</code> should be the model with fewer parameters, but if both <code>x</code> and <code>y</code> are &ldquo;fevd&rdquo; objects, then the order does not matter (it will be determined from which model has more parameters).
</p>
</td></tr>
<tr><td><code id="lr.test_+3A_alpha">alpha</code></td>
<td>

<p>single numeric between 0 and 1 giving the significance level for the test.
</p>
</td></tr>
<tr><td><code id="lr.test_+3A_df">df</code></td>
<td>

<p>single numeric giving the degrees of freedom.  If both <code>x</code> and <code>y</code> are &ldquo;fevd&rdquo; objects, then the degrees of freedom will be calculated, and this argument ignored.  Otherwise, if either or both of <code>x</code> and <code>y</code> are single numerics, then it must be provided or the test may be invalid.
</p>
</td></tr>
<tr><td><code id="lr.test_+3A_...">...</code></td>
<td>

<p>Not used.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When it is desired to incorporate covariates into an extreme value analysis, one method is to incorporate them into the parameters of the extreme value distributions themselves in a regression-like manner (cf. Coles, 2001 ch 6; Reiss and Thomas, 2007 ch 15).  In order to justify whether or not inclusion of the covariates into the model is significant or not is to apply the likelihood-ratio test (of course, the test is more general than that, cf. Coles (2001) p 35).
</p>
<p>The test is only valid for comparing nested models.  That is, the parameters of one model must be a subset of the parameters of the second model.
</p>
<p>Suppose the base model, m0, is nested within the model m1.  Let <code>x</code> be the negative log-likelihood for m0 and <code>y</code> for m1.  Then the likelihood-ratio statistic (or deviance statistic) is given by (Coles, 2001, p 35; Reiss and Thomas, 2007, p 118):
</p>
<p>D = -2*(<code>y</code> - <code>x</code>).
</p>
<p>Letting c.alpha be the (1 - alpha) quantile of the chi-square distribution with degrees of freedom equal to the difference in the number of model parameters, the null hypothesis that D = 0 is rejected if D &gt; c.alpha (i.e., in favor of model m1).
</p>


<h3>Value</h3>

<p>A list object of class &ldquo;htest&rdquo; is returned with components:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>The test statistic value (referred to as D above).</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>numeric vector giving the chi-square critical value (c.alpha described above), the significance leve (alpha) and the degrees of freedom.</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>character string stating &ldquo;greater&rdquo; indicating that the alternative decision is determined if the statistic is greater than c.alpha.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>numeric giving the p-value for the test.  If the p-value is smaller than alpha, then the decision is to reject the null hypothesis in favor of the model with more parameters.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>character string saying &ldquo;Likelihood-ratio Test&rdquo;.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>character vector of length two giving the names of the datasets used for the test (if &ldquo;fevd&rdquo; objects are passed) or the negative log-likelihood values if numbers are passed, or the names of x and y.  Although the names may differ, the models should have been fit to the same data set.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Coles, S.  (2001) <em>An introduction to statistical modeling of extreme values</em>, London, U.K.: Springer-Verlag, 208 pp.
</p>
<p>Reiss, R.-D. and Thomas, M. (2007) <em>Statistical Analysis of Extreme Values: with applications to insurance, finance, hydrology and other fields</em>. Birkhauser, 530pp., 3rd edition. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fevd">fevd</a></code>, <code><a href="#topic+taildep.test">taildep.test</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(PORTw)
fit0 &lt;- fevd(PORTw$TMX1, type="Gumbel") 
fit1 &lt;- fevd(PORTw$TMX1)
fit2 &lt;- fevd(TMX1, PORTw, scale.fun=~STDTMAX)
lr.test(fit0, fit1)
lr.test(fit1, fit2)

</code></pre>

<hr>
<h2 id='make.qcov'>
Covariate Matrix for Non-Stationary EVD Projections
</h2><span id='topic+make.qcov'></span><span id='topic+is.qcov'></span>

<h3>Description</h3>

<p>Create a matrix for use with <code>pextRemes</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make.qcov(x, vals, nr = 1, ...)

is.qcov(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make.qcov_+3A_x">x</code></td>
<td>

<p><code>make.qcov</code>: A list object of class &ldquo;fevd&rdquo; as output from <code>fevd</code>.
</p>
<p><code>is.qcov</code>: Any R object.
</p>
</td></tr>
<tr><td><code id="make.qcov_+3A_vals">vals</code></td>
<td>

<p>Either a named list whose names match the fitted model parameter names, or may be &ldquo;threshold&rdquo;, a matrix or a numeric vector of length equal to the size of the resulting matrix.
</p>
</td></tr>
<tr><td><code id="make.qcov_+3A_nr">nr</code></td>
<td>

<p>The number of rows desired in the resulting matrix.  Only if <code>vals</code> is a vector.  If <code>vals</code> argument is not a vector, the code will either fail or the argument will be ignored.
</p>
</td></tr>
<tr><td><code id="make.qcov_+3A_...">...</code></td>
<td>
<p>optional arguments to <code>matrix</code> (e.g., byrow=TRUE, depending on the order for <code>vals</code>, if a vector).  Only used if <code>vals</code> is a vector.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Simply sets up a matrix of parameter coefficients to be used by <code>pextRemes</code>.  In particular, all parameters/thresholds that are constant (i.e., do not depend on covariate values) should have columns of all ones.  Paramters/threshold that vary in a non-stationary model may have whatever values are of interest.
</p>
<p><code>is.qcov</code> performs some very simple tests to determine if an object is a proper <code>qcov</code> matrix or not.  It is possible to have a matrix that is not a proper <code>qcov</code> matrix, but the returned value is TRUE.  It is also possible to have a valid <code>qcov</code> object that id not appropriate for a particular model.  Mostly this is an internal function.
</p>


<h3>Value</h3>

<p>An nr by np + 1 matrix is returned, where np is the number of parameters in the model.  The last column is always &ldquo;threshold&rdquo; even if the model does not take a threshold (e.g., the GEV df), in which case the last column may be all NA, 0, or some other value depending on the vals argument.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pextRemes">pextRemes</a></code>, <code><a href="#topic+fevd">fevd</a></code>, <code><a href="#topic+erlevd">erlevd</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(PORTw)
fit &lt;- fevd(TMX1, PORTw, location.fun=~AOindex, units="deg C")
fit

v &lt;- cbind(rep(1,4), c(1, -1, 1, -1), rep(1,4), rep(1,4))
v &lt;- make.qcov(fit, vals=v, nr=4)
v

# cf.
v &lt;- make.qcov(fit, vals=list(mu1=c(1, -1, 1, -1)))
v

# Or
v &lt;- make.qcov(fit, vals=c(rep(1,4), c(1, -1, 1, -1), rep(1,8), rep(0,4)), nr=4)
v

</code></pre>

<hr>
<h2 id='mrlplot'>
Mean Residual Life Plot
</h2><span id='topic+mrlplot'></span>

<h3>Description</h3>

<p>An empirical mean residual life plot, including confidence intervals, is produced. The mean residual life plot aids the selection of a threshold for the GPD or point process models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mrlplot(x, nint = 100, alpha = 0.05, na.action = na.fail, xlab = "Threshold", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mrlplot_+3A_x">x</code></td>
<td>

<p>numeric vector of data.
</p>
</td></tr>
<tr><td><code id="mrlplot_+3A_nint">nint</code></td>
<td>

<p>Number of thresholds to use.
</p>
</td></tr>
<tr><td><code id="mrlplot_+3A_alpha">alpha</code></td>
<td>

<p>number giving the 1 - <code>alpha</code> confidence levels to use.
</p>
</td></tr>
<tr><td><code id="mrlplot_+3A_na.action">na.action</code></td>
<td>

<p>function to be called to handle missing values.
</p>
</td></tr>
<tr><td><code id="mrlplot_+3A_xlab">xlab</code></td>
<td>

<p>character string giving the abscissa label to use.
</p>
</td></tr> 
<tr><td><code id="mrlplot_+3A_...">...</code></td>
<td>
<p>optional arguments to <code>plot</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The mean excesses are found for each value of a range of thresholds that cover the range of the data (less one at the high end).  CIs are also shown based on the normal df for the mean excesses.  The goal is to find the lowest threshold such that the graph is linear with increasing thresholds, within uncertainty.
</p>
<p>See Coles (2001) sec. 4.3.1 for more information.
</p>


<h3>Value</h3>

<p>A matrix with the mean excess values and their confidence bounds is returned invisibly.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Coles, S.  (2001). <em>An introduction to statistical modeling of extreme values</em>, London, United Kingdom: Springer-Verlag, 208 pp.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+threshrange.plot">threshrange.plot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Fort)
mrlplot(Fort$Prec)
</code></pre>

<hr>
<h2 id='Ozone4H'> Ground-Level Ozone Order Statistics.</h2><span id='topic+Ozone4H'></span>

<h3>Description</h3>

<p>Ground-level ozone order statistics from 1997 at 513 monitoring stations in the eastern United States.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Ozone4H)</code></pre>


<h3>Format</h3>

<p>A data frame with 513 observations on the following 5 variables.
</p>

<dl>
<dt>station</dt><dd><p>a numeric vector identifying the station (or line) number.</p>
</dd>
<dt>r1</dt><dd><p>a numeric vector giving the maximum ozone reading (ppb) for 1997.</p>
</dd>
<dt>r2</dt><dd><p>a numeric vector giving the second-highest ozone reading (ppb) for 1997.</p>
</dd>
<dt>r3</dt><dd><p>a numeric vector giving the third-highest ozone reading (ppb) for 1997.</p>
</dd>
<dt>r4</dt><dd><p>a numeric vector giving the fourth-highest ozone reading (ppb) for 1997.</p>
</dd>
</dl>



<h3>Details</h3>

<p>Ground level ozone readings in parts per billion (ppb) are recorded hourly at ozone monitoring stations throughout the country during the &quot;ozone season&quot; (roughly April to October).  These data are taken from a dataset giving daily maximum 8-hour average ozone for 5 ozone seasons (including 1997).  The new U.S.  Environmental Protection Agency (EPA) National Ambient Air Quality Standard (NAAQS) for ground-level ozone is based on a three-year average of fourth-highest daily 8-hour maximum ozone readings. 
</p>
<p>For more analysis on the original data regarding the U.S. EPA NAAQS for ground-level ozone, see Fuentes (2003), Gilleland and Nychka (2005) and Gilleland et al. (2006).  These data are in the form required by the <code>rlarg.fit</code> function of Stuart Coles available in the R package <span class="pkg">ismev</span>; see Coles (2001) for more on the r-th largest order statistic model and the function <code>rlarg.fit</code>.
</p>


<h3>Source</h3>

<p>Data was originally provided by the U.S. EPA
</p>


<h3>References</h3>

<p>Coles, S. (2001) <em>An Introduction to Statistical Modeling of Extreme Values</em>. London, U.K.: Springer-Verlag, 208pp.
</p>
<p>Fuentes, M.  (2003) Statistical assessment of geographic areas of compliance with air quality.  <em>Journal of Geophysical Research</em>, <b>108</b>, (D24).
</p>
<p>Gilleland, E. and Nychka, D. (2005) Statistical Models for Monitoring and Regulating Ground-level Ozone. <em>Environmetrics</em>, <b>16</b>, 535&ndash;546.
</p>
<p>Gilleland, E., Nychka, D., and Schneider, U. (2006) Spatial models for the distribution of extremes.  In <em>Applications of Computational Statistics in the Environmental Sciences: Hierarchical Bayes and MCMC Methods</em>, Edited by J.S. Clark &amp; A.  Gelfand. Oxford University Press.  170&ndash;183, ISBN 0-19-8569671.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Ozone4H)
str(Ozone4H)
plot(Ozone4H)
</code></pre>

<hr>
<h2 id='parcov.fevd'>
EVD Parameter Covariance
</h2><span id='topic+parcov.fevd'></span>

<h3>Description</h3>

<p>Try to calculate the parameter covariance for an extreme value distribution (EVD) fitted using MLE.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>parcov.fevd(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="parcov.fevd_+3A_x">x</code></td>
<td>

<p>A list object of class &ldquo;fevd&rdquo; as returned by <code>fevd</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Makes possibly two calls to <code>optimHess</code> in an effort to find the parameter covariance matrix for fitted EVDs where MLE is used.  The first attempt uses the actual gradient of the negative log-likelihood.  If this fails, or the Hessian matrix cannot be inverted, or there are any negative values along the diagonal in the inverted Hessian, then a second attempt is made using finite differences.  See Coles (2001) sec. 2.6.4 for more details.
</p>


<h3>Value</h3>

<p>An np by np matrix is returned where np is the number of parameters in the model.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Coles, S.  (2001) <em>An introduction to statistical modeling of extreme values</em>, London, U.K.: Springer-Verlag, 208 pp.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fevd">fevd</a></code>, <code><a href="#topic+summary.fevd">summary.fevd</a></code>, <code><a href="#topic+print.fevd">print.fevd</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>z &lt;- revd(100, loc=20, scale=0.5, shape=-0.2)
fit &lt;- fevd(z)
fit

parcov.fevd(fit)
</code></pre>

<hr>
<h2 id='Peak'> Salt River Peak Stream Flow</h2><span id='topic+Peak'></span>

<h3>Description</h3>

<p>Peak stream flow data from 1924 through 1999 for the Salt River near Roosevelt, Arizona.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Peak)</code></pre>


<h3>Format</h3>

<p>A data frame with 75 observations on the following 2 variables.
</p>

<dl>
<dt>Year</dt><dd><p>a numeric vector giving the year.</p>
</dd>
<dt>Flow</dt><dd><p>a numeric vector giving the peak stream flow (cfs).</p>
</dd>
<dt>Winter</dt><dd><p>a numeric vector giving the Winter seasonal mean Darwin pressure (mb&ndash;1000).</p>
</dd>
<dt>Spring</dt><dd><p>a numeric vector giving the Spring seasonal mean Darwin pressure (mb&ndash;1000).</p>
</dd>
<dt>Summer</dt><dd><p>a numeric vector giving the Summer seasonal mean Darwin pressure (mb&ndash;1000).</p>
</dd>
<dt>Fall</dt><dd><p>a numeric vector giving the Fall seasonal mean Darwin pressure (mb&ndash;1000) (see Katz et al. (2002)
Sec. 5.2.2).</p>
</dd>
</dl>



<h3>Details</h3>

<p>Peak stream flow in cfs (1 cfs=0.028317 $m^3/s$) data for water years (October through September)
from 1924 through 1999 for the Salt River near Roosevelt, Arizona.  Data for 1986 are missing.
Also includes seasonal mean Darwin pressures (mb&ndash;1000).
</p>
<p>Several analyses have been performed on streamflow at this location (see, e.g., Anderson and
Meerschaert (1998), Dettinger and Diaz (2000); and, for extreme stream flow, Katz et al. (2002) Sec. 5.2.2).
</p>


<h3>Source</h3>

<p>U.S. Geological Survey (<a href="http://water.usgs.gov/nwis/peak">http://water.usgs.gov/nwis/peak</a>) for Salt River peak flows.
NOAA Climate Prediction Center (<a href="http://www.cpc.ncep.noaa.gov/data/indices">http://www.cpc.ncep.noaa.gov/data/indices</a>) for seasonal mean
Darwin pressures.
</p>


<h3>References</h3>

<p>Anderson, P. L. and Meerschaert, M. M.  (1998) Modeling river flows with heavy tails. <em>Water Resour Res</em>, <b>34</b>, (9), 2271&ndash;2280.
</p>
<p>Dettinger, M. D. and Diaz, H. F. (2000) Global characteristics of stream flow seasonality and variability. <em>Journal of Hydrometeorology</em>, <b>1</b>, 289&ndash;310.
</p>
<p>Katz, R. W., Parlange, M. B. and Naveau, P. (2002) Statistics of extremes in hydrology. <em>Advances in Water Resources</em>, <b>25</b>, 1287&ndash;1304.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Peak)
str(Peak)
# Fig. 9 of Katz et al. (2002) Sec. 5.2.2.
plot(Peak[,"Year"], Peak[,"Flow"]/1000, type="l", yaxt="n",
    xlab="Water year (Oct-Sept)", ylab="Annual peak flow (thousand cfs)")
axis(2,at=c(0,40,80,120),labels=c("0","40","80","120"))
</code></pre>

<hr>
<h2 id='pextRemes'>
Probabilities and Random Draws from Fitted EVDs
</h2><span id='topic+pextRemes'></span><span id='topic+pextRemes.fevd'></span><span id='topic+pextRemes.fevd.bayesian'></span><span id='topic+pextRemes.fevd.lmoments'></span><span id='topic+pextRemes.fevd.mle'></span><span id='topic+rextRemes'></span><span id='topic+rextRemes.fevd'></span><span id='topic+rextRemes.fevd.bayesian'></span><span id='topic+rextRemes.fevd.lmoments'></span><span id='topic+rextRemes.fevd.mle'></span>

<h3>Description</h3>

<p>Calculate probabilities from fitted extreme value distributions (EVDs) or draw random samples from them.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pextRemes(x, q, lower.tail = TRUE, ...)

rextRemes(x, n, ...)

## S3 method for class 'fevd'
pextRemes(x, q, lower.tail = TRUE, ..., qcov = NULL)

## S3 method for class 'fevd.bayesian'
pextRemes(x, q, lower.tail = TRUE, ...,
    qcov = NULL, burn.in = 499, FUN = "mean")

## S3 method for class 'fevd.lmoments'
pextRemes(x, q, lower.tail = TRUE, ...)

## S3 method for class 'fevd.mle'
pextRemes(x, q, lower.tail = TRUE, ..., qcov = NULL)

## S3 method for class 'fevd'
rextRemes(x, n, ...)

## S3 method for class 'fevd.bayesian'
rextRemes(x, n, ..., burn.in = 499, FUN = "mean",
    qcov = NULL)

## S3 method for class 'fevd.lmoments'
rextRemes(x, n, ...)

## S3 method for class 'fevd.mle'
rextRemes(x, n, ..., qcov = NULL)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pextRemes_+3A_x">x</code></td>
<td>

<p>A list object of class &ldquo;fevd&rdquo; as returned by <code>fevd</code>.
</p>
</td></tr>
<tr><td><code id="pextRemes_+3A_q">q</code></td>
<td>

<p>Vector of quantiles.
</p>
</td></tr>
<tr><td><code id="pextRemes_+3A_n">n</code></td>
<td>
<p>number of random draws to take from the model.</p>
</td></tr>
<tr><td><code id="pextRemes_+3A_qcov">qcov</code></td>
<td>

<p>numeric matrix with rows the same length as <code>q</code> and columns equal to the number of parameters (+ 1 for the threshold, if a POT model).  This gives any covariate values for a nonstationary model.  If NULL, and model is non-stationary, only the intercept terms for modeled parameters are used, and if a non-constant threshold, only the first threshold value is used.  Not used if model is stationary.
</p>
</td></tr>
<tr><td><code id="pextRemes_+3A_lower.tail">lower.tail</code></td>
<td>

<p>logical; if TRUE (default), probabilities are P[X &lt;= x] otherwise, P[X &gt; x].
</p>
</td></tr>
<tr><td><code id="pextRemes_+3A_burn.in">burn.in</code></td>
<td>
<p>the burn in period.</p>
</td></tr>
<tr><td><code id="pextRemes_+3A_fun">FUN</code></td>
<td>
<p>cahracter string naming a function, or a function, to be used to find the parameter estimates from the posterior df.  Default is the posterior mean.</p>
</td></tr>
<tr><td><code id="pextRemes_+3A_...">...</code></td>
<td>

<p>Not used.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions are essentially wrapper functions for the low-level functions <code>pevd</code> and <code>revd</code>.  The idea is that they take parameter values from a fitted model in order to calculate probabilities or draw random samples.  In the case of non-stationary models, for probabilities, covariate values should be given.  If not, the intercept terms (or first threshold value) are used only; and a warning is given.  In the case of <code>rextRemes</code> for non-stationary values, <code>n</code> samples of length equal to the length of the data set to which the model was fit are generated and returned as a matrix.  In this case, the random draws represent random draws using the current covariate values.
</p>
<p>The extreme value distributions (EVD's) are generalized extreme value (GEV) or generalized Pareto (GP).  The point process characterization is an equivalent form, but is not handled here; parameters are converted to those of the (approx.) equivalent GP df.  The GEV df is given by
</p>
<p>Pr(X &lt;= x) = G(x) = exp[-(1 + shape*(x - location)/scale)^(-1/shape)]
</p>
<p>for 1 + shape*(x - location) &gt; 0 and scale &gt; 0.  It the shape parameter is zero, then the df is defined by continuity and simplies to
</p>
<p>G(x) = exp(-exp((x - location)/scale)).
</p>
<p>The GEV df is often called a family of df's because it encompasses the three types of EVD's: Gumbel (shape = 0, light tail), Frechet (shape &gt; 0, heavy tail) and the reverse Weibull (shape &lt; 0, bounded upper tail at location - scale/shape).  It was first found by R. von Mises (1936) and also independently noted later by meteorologist A. F. Jenkins (1955).  It enjoys theretical support for modeling maxima taken over large blocks of a series of data.
</p>
<p>The generalized Pareo df is given by (Pickands, 1975)
</p>
<p>Pr(X &lt;= x) = F(x) = 1 - [1 + shape*(x - threshold)/scale]^(-1/shape)
</p>
<p>where 1 + shape*(x - threshold)/scale &gt; 0, scale &gt; 0, and x &gt; threshold.  If shape = 0, then the GP df is defined by continuity and becomes
</p>
<p>F(x) = 1 - exp(-(x - threshold)/scale).
</p>
<p>There is an approximate relationship between the GEV and GP df's where the GP df is approximately the tail df for the GEV df.  In particular, the scale parameter of the GP is a function of the threshold (denote it scale.u), and is equivalent to scale + shape*(threshold - location) where scale, shape and location are parameters from the &ldquo;equivalent&rdquo; GE Vdf.  Similar to the GEV df, the shape parameter determines the tail behavior, where shape = 0 gives rise to the exponential df (light tail), shape &gt; 0 the Pareto df (heavy tail) and shape &lt; 0 the Beta df (bounded upper tail at location - scale.u/shape).  Theoretical justification supports the use of the GP df family for modeling excesses over a high threshold (i.e., y = x - threshold).  It is assumed here that <code>x</code>, <code>q</code> describe x (not y = x - threshold).  Similarly, the random draws are y + threshold.
</p>
<p>See Coles (2001) and Reiss and Thomas (2007) for a very accessible text on extreme value analysis and for more theoretical texts, see for example, Beirlant et al. (2004), de Haan and Ferreira (2006), as well as Reiss and Thomas (2007).
</p>


<h3>Value</h3>

<p>A numeric vector of probabilites or random sample is returned.  In the case of non-stationary models, a matrix of random samples is returned by <code>rextRemes</code>.
</p>


<h3>Warning </h3>

<p>In the case of non-stationary models, the code in its current state is somewhat less than ideal.  It requires great care on the part of the user.  In particular, the <code>qcov</code> argument becomes critical.  Parameters that are fixed in the model can be changed if <code>qcov</code> is not correctly used.  Any parameter that is fixed at a given value (including the intercept terms) should have all ones in their columns.  Presently, nothing in the code will force this requirement to be upheld.  Using <code>make.qcov</code> will help, as it has some checks to ensure constant-valued parameters have all ones in their columns.
</p>


<h3>Note</h3>

<p>It is recommended to use <code>make.qcov</code> when creating a <code>qcov</code> matrix.</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Beirlant, J., Goegebeur, Y., Teugels, J. and Segers, J. (2004) <em>Statistics of Extremes: Theory and Applications</em>.  Chichester, West Sussex, England, UK: Wiley, ISBN 9780471976479, 522pp.
</p>
<p>Coles, S.  (2001) <em>An introduction to statistical modeling of extreme values</em>, London, U.K.: Springer-Verlag, 208 pp.
</p>
<p>de Haan, L. and Ferreira, A. (2006) <em>Extreme Value Theory: An Introduction</em>.  New York, NY, USA: Springer, 288pp.
</p>
<p>Jenkinson, A. F. (1955) The frequency distribution of the annual maximum (or minimum) of meteorological elements. <em>Quart. J. R.  Met. Soc.</em>, <b>81</b>, 158&ndash;171.
</p>
<p>Pickands, J. (1975) Statistical inference using extreme order statistics.  <em>Annals of Statistics</em>, <b>3</b>, 119&ndash;131.
</p>
<p>Reiss, R.-D. and Thomas, M. (2007) <em>Statistical Analysis of Extreme Values: with applications to insurance, finance, hydrology and other fields</em>. Birkhauser, 530pp., 3rd edition.
</p>
<p>von Mises, R. (1936) La distribution de la plus grande de n valeurs, <em>Rev. Math. Union Interbalcanique</em> <b>1</b>, 141&ndash;160.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pevd">pevd</a></code>, <code><a href="#topic+revd">revd</a></code>, <code><a href="#topic+fevd">fevd</a></code>, <code><a href="#topic+make.qcov">make.qcov</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>z &lt;- revd(100, loc=20, scale=0.5, shape=-0.2)
fit &lt;- fevd(z)
fit

pextRemes(fit, q=quantile(z, probs=c(0.85, 0.95, 0.99)), lower.tail=FALSE)

z2 &lt;- rextRemes(fit, n=1000)
qqplot(z, z2)

## Not run: 
data(PORTw)
fit &lt;- fevd(TMX1, PORTw, units="deg C")
fit

pextRemes(fit, q=c(17, 20, 25, 30), lower.tail=FALSE)
# Note that fit has a bounded upper tail at:
# location - scale/shape ~
# 15.1406132 + (2.9724952/0.2171486) = 28.82937
#
# which is why P[X &gt; 30] = 0.  Note also that 25
# is less than the upper bound, but larger than
# the maximum observed value.

z &lt;- rextRemes(fit, n=50)
qqplot(z, PORTw$TMX1, xlab="Simulated Data Quantiles",
    ylab="Data Quantiles (PORTw TMX1)")

# Not a great fit because data follow a non-stationary
# distribution.
fit &lt;- fevd(TMX1, PORTw, location.fun=~AOindex, units="deg C")
fit

pextRemes(fit, q=c(17, 20, 25, 30), lower.tail=FALSE)
# Gives a warning because we did not give covariate values.

v &lt;- make.qcov(fit, vals=list(mu1=c(1, -1, 1, -1)))
v
# find probabilities for high positive AOindex vs
# low negative AOindex.  A column for the unnecessary
# threshold is added, but is not used.

pextRemes(fit, q=c(17, 17, 30, 30), qcov=v, lower.tail=FALSE)

z &lt;- rextRemes(fit, n=50)
dim(z)
qqplot(z[,1], PORTw$TMX1, xlab="Simulated Data Quantiles",
    ylab="Data Quantiles (PORTw TMX1)")

qqplot(z[,28], PORTw$TMX1, xlab="Simulated Data Quantiles",
    ylab="Data Quantiles (PORTw TMX1)")
# etc.

##
## GP model with non-constant threshold.
##
fit &lt;- fevd(-MinT ~1, Tphap, threshold=c(-70,-7),
    threshold.fun=~I((Year - 48)/42), type="GP",
    time.units="62/year", verbose=TRUE)
fit

summary(fit$threshold)
v &lt;- make.qcov(fit, vals=c(rep(1,8), c(-77, -73.5, -71.67, -70)), nr=4)
v

# upper bounded df at: u - scale/shape = 
c(-77, -73.5, -71.67, -70) + 2.9500992/0.1636367
# -58.97165 -55.47165 -53.64165 -51.97165
summary(-Tphap$MinT)
pextRemes(fit, q=rep(-58, 4), qcov=v, lower.tail=FALSE)



## End(Not run)
</code></pre>

<hr>
<h2 id='PORTw'> Annual Maximum and Minimum Temperature</h2><span id='topic+PORTw'></span><span id='topic+SEPTsp'></span>

<h3>Description</h3>

<p>Annual maximum and minimum Winter temperature (degrees centigrade) with a covariate for the
North Atlantic Oscillation index from 1927 through 1995.  Data is for Winter for Port Jervis,
New York (PORTw) and Spring for Sept-Iles, Quebec (SEPTsp).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(PORTw)</code></pre>


<h3>Format</h3>

<p>A data frame with 68 observations on the following 16 variables.
</p>

<dl>
<dt>Year</dt><dd><p>a numeric vector giving the year.</p>
</dd>
<dt>MTMAX</dt><dd><p>a numeric vector giving the mean winter maximum temperatures (degrees centigrade).</p>
</dd>
<dt>MTMIN</dt><dd><p>a numeric vector giving the mean winter minimum temperatures (degrees centigrade).</p>
</dd>
<dt>STDTMAX</dt><dd><p>a numeric vector giving the standard deviations of maximum winter temperatures (degrees centigrade).</p>
</dd>
<dt>STDMIN</dt><dd><p>a numeric vector giving the standard deviations of minimum winter temperatures (degrees centigrade).</p>
</dd>
<dt>TMX1</dt><dd><p>a numeric vector giving the maximum winter temperature (degrees centigrade).</p>
</dd>
<dt>TMN0</dt><dd><p>a numeric vector giving the minimum winter temperature (degrees centigrade).</p>
</dd>
<dt>MDTR</dt><dd><p>a numeric vector giving the mean winter diurnal temperature range (degrees centigrade).</p>
</dd>
<dt>AOindex</dt><dd><p>a numeric vector giving the Atlantic Oscillation index (see Thompson and Wallace (1998)).</p>
</dd>
</dl>



<h3>Details</h3>

<p>See Wettstein and Mearns (2002) for a much more detailed explanation of the above variables.
</p>


<h3>Source</h3>

<p>See Wettstein and Mearns (2002).
</p>


<h3>References</h3>

<p>Thompson, D. W. J. and Wallace, J. M. (1998) The Arctic Oscillation signature in the wintertime geopotential height and temperature fields. <em>Geophys. Res. Lett.</em>, <b>25</b>, 1297&ndash;1300.
</p>
<p>Wettstein, J. J. and Mearns, L. O. (2002) The influence of the North Atlantic-Arctic Oscillation on mean, variance and extremes of temperature in the northeastern United States and Canada. <em>Journal of Climate</em>, <b>15</b>, 3586&ndash;3600.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(PORTw)
str(PORTw)
par( mfrow=c(2,1))
plot(PORTw[,"TMX1"], type="l", lwd=2, xlab="", xaxt="n", ylab="Maximum Temperature (C)")
plot(PORTw[,"TMN0"], type="l", lwd=2, xlab="", xaxt="n", ylab="Minimum Temperature (C)")
</code></pre>

<hr>
<h2 id='postmode'>
Posterior Mode from an MCMC Sample
</h2><span id='topic+postmode'></span><span id='topic+postmode.fevd'></span>

<h3>Description</h3>

<p>Calculate the posterior mode from an MCMC sample for &ldquo;fevd&rdquo; objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>postmode(x, burn.in = 499, verbose = FALSE, ...)

## S3 method for class 'fevd'
postmode(x, burn.in = 499, verbose = FALSE, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="postmode_+3A_x">x</code></td>
<td>

<p>An object of class &ldquo;fevd&rdquo; where component <code>method</code> = &ldquo;Bayesian&rdquo;.</p>
</td></tr>
<tr><td><code id="postmode_+3A_burn.in">burn.in</code></td>
<td>

<p>The furst burn.in samples from the posterior distribution will be removed before calculation.
</p>
</td></tr>
<tr><td><code id="postmode_+3A_verbose">verbose</code></td>
<td>

<p>logical, should progress information be printed to the screen.
</p>
</td></tr>
<tr><td><code id="postmode_+3A_...">...</code></td>
<td>

<p>Not used.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The log-likelihood and (log) prior is calculated for every sample from the chain, and added together, giving <code>h</code>.  The parameters from the sample that yield the maximum value for <code>h</code> are returned.  If more than one set of parameters yield a maximum, their average is returned.
</p>


<h3>Value</h3>

<p>A named numeric vector is returned giving the paramter values.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fevd">fevd</a></code>, <code><a href="#topic+findpars">findpars</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ftcanmax)

fit &lt;- fevd(Prec, ftcanmax, method="Bayesian", iter = 1000, verbose=TRUE)

postmode(fit)

</code></pre>

<hr>
<h2 id='Potomac'> Potomac River Peak Stream Flow Data.</h2><span id='topic+Potomac'></span>

<h3>Description</h3>

<p>Potomac River peak stream flow (cfs) data for water years (Oct-Sep) 1895 through 2000
at Point Rocks, Maryland.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Potomac)</code></pre>


<h3>Format</h3>

<p>A data frame with 106 observations on the following 2 variables.
</p>

<dl>
<dt>Year</dt><dd><p>a numeric vector giving the water year (Oct-Sep).</p>
</dd>
<dt>Flow</dt><dd><p>a numeric vector the peak stream flow (cfs; 1 cfs = 0.028317 cubic meters per second).</p>
</dd>
</dl>



<h3>Details</h3>

<p>Potomac River peak stream flow (cfs) data for water years (Oct-Sep) 1895 through 2000 at Point Rocks, Maryland.
</p>
<p>These data (up to 1986) have been analyzed by Smith (1987) and this entire dataset by Katz et al. (2002) Sec. 2.3.2.
</p>


<h3>Source</h3>

<p>U.S. Geological Survey (<a href="http://water.usgs.gov/nwis/peak">http://water.usgs.gov/nwis/peak</a>).
</p>


<h3>References</h3>

<p>Katz, R. W., Parlange, M. B. and Naveau, P. (2002) Statistics of extremes in hydrology. <em>Advances in Water Resources</em>, <b>25</b>, 1287&ndash;1304. 
</p>
<p>Smith, J. A. (1987) Regional flood frequency analysis using extreme order statistics of the annual peak record. <em>Water Resour Res</em>, <b>23</b>, 1657&ndash;1666.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Potomac)
str(Potomac)
# Fig. 3 of Katz et al. (2002) Sec. 2.3.2.
plot(Potomac[,"Year"], Potomac[,"Flow"]/1000, yaxt="n", ylim=c(0,500), type="l", lwd=1.5,
    xlab="Water Year (Oct-Sept)", ylab="Annual peak flow (thousand cfs)")
axis(2,at=seq(0,500,100),labels=as.character(seq(0,500,100)))
</code></pre>

<hr>
<h2 id='profliker'>
Profile Likelihood Function
</h2><span id='topic+profliker'></span>

<h3>Description</h3>

<p>Find the profile likelihood for a range of values for an extreme value df (EVD).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>profliker(object, type = c("return.level", "parameter"), xrange = NULL,
    return.period = 100, which.par = 1, nint = 20, plot = TRUE, gr = NULL,
    method = "BFGS", lower = -Inf, upper = Inf, control = list(), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="profliker_+3A_object">object</code></td>
<td>

<p>A list object of class &ldquo;fevd&rdquo; as returned by <code>fevd</code>.
</p>
</td></tr>
<tr><td><code id="profliker_+3A_type">type</code></td>
<td>

<p>character string stating whether the parameter of interest is a regular parameter or a return level.
</p>
</td></tr>
<tr><td><code id="profliker_+3A_xrange">xrange</code></td>
<td>

<p>numeric vector of length two giving the range of values of the parameter over which to calculate the profile likelihood.
</p>
</td></tr>
<tr><td><code id="profliker_+3A_return.period">return.period</code></td>
<td>

<p>If a return level is of interest, this number gives its associated return period.
</p>
</td></tr>
<tr><td><code id="profliker_+3A_which.par">which.par</code></td>
<td>

<p>If a parameter is of interest, this number tells for which component of the parameter vector to do the profile likelihood.
</p>
</td></tr>
<tr><td><code id="profliker_+3A_nint">nint</code></td>
<td>

<p>The profile likelihood is calculated for a sequence of <code>nint</code> values covering <code>xrange</code>.
</p>
</td></tr>
<tr><td><code id="profliker_+3A_plot">plot</code></td>
<td>

<p>logical; should a plot of the likelihood be made?  Note that this is controlled by the <code>verbose</code> argument in the <code>ci</code> method function for MLE <code>fevd</code> objects when &ldquo;proflik&rdquo; is chosen as the method for finding confidence intervals.  It is usually a good idea to plot the profile likelihood to see if the confidence intervals are really found or not.
</p>
</td></tr>
<tr><td><code id="profliker_+3A_gr">gr</code>, <code id="profliker_+3A_method">method</code>, <code id="profliker_+3A_lower">lower</code>, <code id="profliker_+3A_upper">upper</code>, <code id="profliker_+3A_control">control</code></td>
<td>

<p>optional arguments to <code>optim</code>.
</p>
</td></tr>
<tr><td><code id="profliker_+3A_...">...</code></td>
<td>

<p>optional arguments to <code>plot</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See the help file for <code>ci.fevd.mle</code> for more details on this approach.
</p>


<h3>Value</h3>

<p>A numeric vector is returned invisibly.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ci.fevd.mle">ci.fevd.mle</a></code>, <code><a href="#topic+fevd">fevd</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>z &lt;- revd(100, loc=20, scale=0.5, shape=-0.2)
fit &lt;- fevd(z)
fit

profliker(fit, type="parameter", which.par=3)

profliker(fit, type="parameter",
    xrange=c(-0.35, -0.2), which.par=3)


</code></pre>

<hr>
<h2 id='qqnorm'>
Normal qq-plot with 95 Percent Simultaneous Confidence Bands
</h2><span id='topic+qqnorm'></span>

<h3>Description</h3>

<p>Calculates a normal qq-plot for a vector of data along with 95 percent simultaneous confidence bands.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>qqnorm(y, pch = 20, xlab = "Standard Normal Quantiles", ylab = "Sample Quantiles",
    make.plot = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="qqnorm_+3A_y">y</code></td>
<td>
<p> numeric vector of data.</p>
</td></tr>
<tr><td><code id="qqnorm_+3A_pch">pch</code></td>
<td>
<p> plot symbol to use.</p>
</td></tr>
<tr><td><code id="qqnorm_+3A_xlab">xlab</code></td>
<td>
<p> Character string giving abscissa label.</p>
</td></tr>
<tr><td><code id="qqnorm_+3A_ylab">ylab</code></td>
<td>
<p> Character string giving ordinate axis label.</p>
</td></tr>
<tr><td><code id="qqnorm_+3A_make.plot">make.plot</code></td>
<td>
<p>logical, should the plot be created (TRUE) or not (FALSE)?</p>
</td></tr>
<tr><td><code id="qqnorm_+3A_...">...</code></td>
<td>
<p> optional arguments to the plot function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Confidence intervals are calculated using +/- k, where
</p>
<p>k = 0.895 / (sqrt(n) * (1- 0.01 / sqrt(n) + 0.85/n))
</p>
<p>Gives a 95 percent asymptotic band based on the Kolmogorov-Smirnov statistic (Doksum and Sievers, 1976).
</p>


<h3>Value</h3>

<p>A data frame object is returned invisibly with components:
</p>
<table>
<tr><td><code>x</code>, <code>y</code></td>
<td>
<p>the data and standard normal quantiles, resp.</p>
</td></tr>
<tr><td><code>lower</code>, <code>upper</code></td>
<td>
<p>lower and upper 95 percent confidence bands.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Peter Guttorp, peter &ldquo;at&rdquo; stat.washington.edu, modified by Eric Gilleland
</p>


<h3>References</h3>

<p>Doksum, K. A. and G. L. Sievers, 1976.  Plotting with confidence: graphical comparisons of two populations.  Biometrika, 63 (3), 421&ndash;434.
</p>


<h3>See Also</h3>

 <p><code><a href="stats.html#topic+qnorm">qnorm</a></code>, <code><a href="#topic+qqplot">qqplot</a></code>, <code><a href="#topic+shiftplot">shiftplot</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>z &lt;- rexp(100)
qqnorm( z)

y &lt;- rnorm( 100)
qqnorm( y)
obj &lt;- qqnorm(y, make.plot=FALSE)
str(obj)

data( ftcanmax)
qqnorm( ftcanmax[,"Prec"])
</code></pre>

<hr>
<h2 id='qqplot'>
qq-plot Between Two Vectors of Data with 95 Percent Confidence Bands
</h2><span id='topic+qqplot'></span><span id='topic+plot.qqplot'></span><span id='topic+summary.qqplot'></span>

<h3>Description</h3>

<p>QQ-plot between two data vectors with 95 percent confidence bands based on the Kolmogorov-Smirnov statistic (Doksum and Sievers, 1976).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>qqplot(x, y, pch = 20, xlab = "x Quantiles", ylab = "y Quantiles", regress = TRUE,
    make.plot = TRUE, ...)

## S3 method for class 'qqplot'
plot(x, ...)

## S3 method for class 'qqplot'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="qqplot_+3A_x">x</code></td>
<td>
 <p><code>qqplot</code>: numeric vector of length 'm' giving one data set.
</p>
<p><code>plot</code> method function: list object of class &ldquo;qqplot&rdquo; returned by <code>qqplot</code>.</p>
</td></tr>
<tr><td><code id="qqplot_+3A_object">object</code></td>
<td>
<p>list object of class &ldquo;qqplot&rdquo; returned by <code>qqplot</code>.</p>
</td></tr>
<tr><td><code id="qqplot_+3A_y">y</code></td>
<td>
<p> numeric vector of length 'n' giving the other data set.</p>
</td></tr>
<tr><td><code id="qqplot_+3A_pch">pch</code></td>
<td>
<p> Plot character.</p>
</td></tr>
<tr><td><code id="qqplot_+3A_xlab">xlab</code></td>
<td>
<p> Character string giving the label for the abscissa axis.</p>
</td></tr>
<tr><td><code id="qqplot_+3A_ylab">ylab</code></td>
<td>
<p> Character string giving the label for the ordinate axis.</p>
</td></tr>
<tr><td><code id="qqplot_+3A_regress">regress</code></td>
<td>
<p>logical, should a regression line be fit to the quantiles?</p>
</td></tr>
<tr><td><code id="qqplot_+3A_make.plot">make.plot</code></td>
<td>
<p>logical, should the plot be created (TRUE) or not (FALSE)?</p>
</td></tr>
<tr><td><code id="qqplot_+3A_...">...</code></td>
<td>
<p> Other optional arguments to the plot function.  Not used by <code>summary</code> method function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Plots the sorted (missing-values removed) 'x' values against the sorted, and interpolated (via the approxfun function from package stats), 'y' values.  Confidence bands are about the sorted and interpolated 'y' values using +/- K/sqrt(M), where
</p>
<p>K = 1.36
</p>
<p>and
</p>
<p>M = m*n / (m+n).
</p>
<p>The <code>plot</code> method function does exactly the same thing as <code>qqplot</code> except that it does not need to do any calculations.
</p>
<p>The <code>summary</code> method function merely displays the original call to the function unless a regression line was fit between the quantiles, in which case summary information is displayed for the regression (i.e., the <code>summary</code> method function for <code>lm</code> is run on the &ldquo;lm&rdquo; object).
</p>


<h3>Value</h3>

<p>An object of class &ldquo;qqplot&rdquo; is invisibly returned by each function (in the case of the method functions, the object entered is simply returned invisibly).  This is a list object with components:
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>calling string</p>
</td></tr>
<tr><td><code>names</code></td>
<td>
<p>list object with components x and y giving the object names for the objects passed into x and y, resp.</p>
</td></tr>
<tr><td><code>regression</code></td>
<td>
<p>If regress was TRUE, then this is the fitted regression object as returned by lm.  Otherwise, this component is not included.</p>
</td></tr>
<tr><td><code>qdata</code></td>
<td>
<p>data frame with components: x and y giving the quantiles for x and y, resp., and lower and upper giving the lower and upper 95 percent confidence bands, resp.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Peter Guttorp, peter &ldquo;at&rdquo; stat.washington.edu
</p>


<h3>References</h3>

<p>Doksum, K.A. and G.L. Sievers, 1976.  Plotting with confidence: graphical comparisons of two populations.  Biometrika, 63 (3), 421&ndash;434.
</p>


<h3>See Also</h3>

 <p><code><a href="stats.html#topic+approxfun">approxfun</a></code>, <code><a href="#topic+qqnorm">qqnorm</a></code>, <code><a href="#topic+shiftplot">shiftplot</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>z &lt;- rnorm(100)
y &lt;- rexp(100)
qqplot( z, y)
qqplot( y, z)

data( ftcanmax)
qqplot( ftcanmax[,"Prec"], z)
obj &lt;- qqplot( ftcanmax[,"Prec"], y, make.plot=FALSE)
plot(obj)
summary(obj)

</code></pre>

<hr>
<h2 id='return.level'>
Return Level Estimates
</h2><span id='topic+return.level'></span><span id='topic+return.level.fevd'></span><span id='topic+return.level.fevd.bayesian'></span><span id='topic+return.level.fevd.lmoments'></span><span id='topic+return.level.fevd.mle'></span><span id='topic+return.level.ns.fevd.bayesian'></span><span id='topic+return.level.ns.fevd.mle'></span><span id='topic+print.return.level'></span>

<h3>Description</h3>

<p>Return level estimates from fitted fevd model objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>return.level(x, return.period = c(2, 20, 100), ...)

## S3 method for class 'fevd'
return.level(x, return.period = c(2, 20, 100), ...)

## S3 method for class 'fevd.bayesian'
return.level(x, return.period = c(2, 20, 100), ..., do.ci = FALSE,
                 burn.in = 499, FUN = "mean", qcov = NULL, qcov.base =
                 NULL)

## S3 method for class 'fevd.lmoments'
return.level(x, return.period = c(2, 20, 100), ...,
    do.ci = FALSE)

## S3 method for class 'fevd.mle'
return.level(x, return.period = c(2, 20, 100), ...,
    do.ci = FALSE, qcov = NULL, qcov.base = NULL)

## S3 method for class 'ns.fevd.bayesian'
return.level(x, return.period = 100, ...,
    burn.in = 499, FUN = "mean", do.ci = FALSE, verbose = FALSE,
    qcov = NULL, qcov.base = NULL)

## S3 method for class 'ns.fevd.mle'
return.level(x, return.period = c(2, 20, 100), ...,
    alpha = 0.05, method = c("normal"), do.ci = FALSE, verbose = FALSE,
    qcov = NULL, qcov.base = NULL)

## S3 method for class 'return.level'
print(x, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="return.level_+3A_x">x</code></td>
<td>

<p>A list object of class &ldquo;fevd&rdquo; as returned by <code>fevd</code>.  In the case of the <code>print</code> method function, an object returned by <code>return.level</code>.
</p>
</td></tr>
<tr><td><code id="return.level_+3A_return.period">return.period</code></td>
<td>

<p>numeric vector of desired return periods.  For <code>return.level.ns.fevd.mle</code>, this must have length one.
</p>
</td></tr>
<tr><td><code id="return.level_+3A_qcov">qcov</code></td>
<td>
<p>numeric matrix with rows the same length as <code>q</code> and columns equal to the number of parameters (+ 1 for the threshold, if a POT model).  This gives any covariate values for a nonstationary model.  If NULL, and model is non-stationary, only the intercept terms for modeled parameters are used, and if a non-constant threshold, only the first threshold value is used.  Not used if model is stationary. </p>
</td></tr>
<tr><td><code id="return.level_+3A_qcov.base">qcov.base</code></td>
<td>
<p>numeric matrix analogous to <code>qcov</code>.  When provided, the function returns the difference in return levels between the level for the covariates in <code>qcov</code> and the level for covariates in <code>qcov.base</code>.</p>
</td></tr>
<tr><td><code id="return.level_+3A_do.ci">do.ci</code></td>
<td>
<p>logical; should CIs be returned as well?</p>
</td></tr>
<tr><td><code id="return.level_+3A_burn.in">burn.in</code></td>
<td>
<p>number giving the burn in value.  The first 1:burn.in will not be used in obtaining parmaeter estimates. </p>
</td></tr>
<tr><td><code id="return.level_+3A_fun">FUN</code></td>
<td>
<p>character string naming a function, or a function, to use to find the parameter estimates from the MCMC sample.  Default is to take the posterior mean (after burn in).</p>
</td></tr>
<tr><td><code id="return.level_+3A_alpha">alpha</code></td>
<td>
<p>The (1 - alpha) * 100 percent confidence level for confidence intervals of return levels in non-stationary models.</p>
</td></tr>
<tr><td><code id="return.level_+3A_method">method</code></td>
<td>
<p>character string naming which CI method to employ.</p>
</td></tr>
<tr><td><code id="return.level_+3A_verbose">verbose</code></td>
<td>
<p>logical, should progress information be printed to the screen?</p>
</td></tr>
<tr><td><code id="return.level_+3A_...">...</code></td>
<td>

<p>For the stationary case only, any optional arguments to the <code>ci</code> function.  Not used by the <code>print</code> method function.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The extreme value distributions (EVD's) are generalized extreme value (GEV) or generalized Pareto (GP).  The point process characterization is an equivalent form, but is not handled here.  The GEV df is given by
</p>
<p>Pr(X &lt;= x) = G(x) = exp[-(1 + shape*(x - location)/scale)^(-1/shape)]
</p>
<p>for 1 + shape*(x - location) &gt; 0 and scale &gt; 0.  It the shape parameter is zero, then the df is defined by continuity and simplies to
</p>
<p>G(x) = exp(-exp((x - location)/scale)).
</p>
<p>The GEV df is often called a family of df's because it encompasses the three types of EVD's: Gumbel (shape = 0, light tail), Frechet (shape &gt; 0, heavy tail) and the reverse Weibull (shape &lt; 0, bounded upper tail at location - scale/shape).  It was first found by R. von Mises (1936) and also independently noted later by meteorologist A. F. Jenkins (1955).  It enjoys theretical support for modeling maxima taken over large blocks of a series of data.
</p>
<p>The generalized Pareo df is given by (Pickands, 1975)
</p>
<p>Pr(X &lt;= x) = F(x) = 1 - [1 + shape*(x - threshold)/scale]^(-1/shape)
</p>
<p>where 1 + shape*(x - threshold)/scale &gt; 0, scale &gt; 0, and x &gt; threshold.  If shape = 0, then the GP df is defined by continuity and becomes
</p>
<p>F(x) = 1 - exp(-(x - threshold)/scale).
</p>
<p>There is an approximate relationship between the GEV and GP df's where the GP df is approximately the tail df for the GEV df.  In particular, the scale parameter of the GP is a function of the threshold (denote it scale.u), and is equivalent to scale + shape*(threshold - location) where scale, shape and location are parameters from the &ldquo;equivalent&rdquo; GE Vdf.  Similar to the GEV df, the shape parameter determines the tail behavior, where shape = 0 gives rise to the exponential df (light tail), shape &gt; 0 the Pareto df (heavy tail) and shape &lt; 0 the Beta df (bounded upper tail at location - scale.u/shape).  Theoretical justification supports the use of the GP df family for modeling excesses over a high threshold (i.e., y = x - threshold).  It is assumed here that <code>x</code>, <code>q</code> describe x (not y = x - threshold).  Similarly, the random draws are y + threshold.
</p>
<p>See Coles (2001) and Reiss and Thomas (2007) for a very accessible text on extreme value analysis and for more theoretical texts, see for example, Beirlant et al. (2004), de Haan and Ferreira (2006), as well as Reiss and Thomas (2007).
</p>
<p>Return levels are essentially the same as quantiles.  In the case of the
GEV family, they are the same.  In the case of the GP df, they are very
similar, but the exceedance rate is taken into consideration.  For
non-stationary modeling, effective return levels are calculated for each
value of the covariate(s) used in the model fit (see, e.g., Gilleland
and Katz, 2011).
</p>
<p><code>return.level.ns.fevd.mle</code> allows one to estimate the difference in
return levels for a non-stationary model, based on subtracting the
return levels for <code>qcov.base</code> from those for <code>qcov</code>, in which
case the outputted values and CIs pertain to differences in return levels.
</p>


<h3>Value</h3>

<p>If do.ci is FALSE, an object of class &ldquo;return.level&rdquo; is returned, which is either a numeric vector (stationary models) of length equal to the <code>return.period</code> argument giving the return levels, or a matrix of dimension equal to either n by np or q by np where n is the length of the data used to fit the model and np are the number of return periods, and q is the number of rows of qcov, if supplied.  The returned value also includes useful attributes describing how the return levels came to be estimated.  In particular, the list of attributes include:
</p>
<table>
<tr><td><code>return.period</code></td>
<td>
<p>the return periods associated with the estimated return levels.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>same as the data.name component of the fevd object.</p>
</td></tr>
<tr><td><code>fit.call</code>, <code>call</code></td>
<td>
<p>the original call for the fitted object and the call to this function, resp.</p>
</td></tr>
<tr><td><code>fit.type</code></td>
<td>
<p>character string naming which type of EVD was fit to the data, and subsequently used to estimate the return levels.</p>
</td></tr>
<tr><td><code>data.assumption</code></td>
<td>
<p>character string stating whether the model is stationary or non-stationary.</p>
</td></tr>
<tr><td><code>period</code></td>
<td>
<p>character string stating what the units (period.basis from the fevd object) of the period are.</p>
</td></tr>
<tr><td><code>units</code></td>
<td>
<p>character string giving the data units, if available.</p>
</td></tr>
<tr><td><code>qcov</code></td>
<td>
<p>name of the qcov matrix used to obtain the effective return levels.</p>
</td></tr>
<tr><td><code>qcov.base</code></td>
<td>
<p>when provided as input, the name of the qcov.base matrix used to obtain the
difference in effective return levels.</p>
</td></tr>
</table>
<p>If do.ci is TRUE, then an object returned by the appropriate ci function is returned (stationary case only).
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Beirlant, J., Goegebeur, Y., Teugels, J. and Segers, J. (2004) <em>Statistics of Extremes: Theory and Applications</em>.  Chichester, West Sussex, England, UK: Wiley, ISBN 9780471976479, 522pp.
</p>
<p>Coles, S.  (2001) <em>An introduction to statistical modeling of extreme values</em>, London, U.K.: Springer-Verlag, 208 pp.
</p>
<p>Gilleland, E. and Katz, R. W. (2011). New software to analyze how extremes change over time. <em>Eos</em>, 11 January, <b>92</b>, (2), 13&ndash;14.
</p>
<p>de Haan, L. and Ferreira, A. (2006) <em>Extreme Value Theory: An Introduction</em>.  New York, NY, USA: Springer, 288pp.
</p>
<p>Jenkinson, A. F. (1955) The frequency distribution of the annual maximum (or minimum) of meteorological elements. <em>Quart. J. R.  Met. Soc.</em>, <b>81</b>, 158&ndash;171.
</p>
<p>Pickands, J. (1975) Statistical inference using extreme order statistics.  <em>Annals of Statistics</em>, <b>3</b>, 119&ndash;131.
</p>
<p>Reiss, R.-D. and Thomas, M. (2007) <em>Statistical Analysis of Extreme Values: with applications to insurance, finance, hydrology and other fields</em>. Birkhauser, 530pp., 3rd edition.
</p>
<p>von Mises, R. (1936) La distribution de la plus grande de n valeurs, <em>Rev. Math. Union Interbalcanique</em> <b>1</b>, 141&ndash;160.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pextRemes">pextRemes</a></code>, <code><a href="#topic+fevd">fevd</a></code>, <code><a href="#topic+rlevd">rlevd</a></code>, <code><a href="#topic+ci.rl.ns.fevd.bayesian">ci.rl.ns.fevd.bayesian</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>z &lt;- revd(100, loc=20, scale=0.5, shape=-0.2)
fit &lt;- fevd(z)
fit

return.level(fit)

fitLM &lt;- fevd(z, method="Lmoments")
fitLM
return.level(fitLM)

## Not run: 
fitB &lt;- fevd(z, method="Bayesian", verbose=TRUE)
fitB

return.level(fitB)


## End(Not run)
</code></pre>

<hr>
<h2 id='revtrans.evd'>
Reverse Transformation
</h2><span id='topic+revtrans.evd'></span>

<h3>Description</h3>

<p>Reverse transform standardized data to follow a non-standardized extreme value distribution (EVD).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>revtrans.evd(z, threshold = NULL, location = NULL, scale, shape = NULL,
    type = c("GEV", "GP", "PP", "Gumbel", "Weibull", "Frechet",
    "Exponential", "Beta", "Pareto"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="revtrans.evd_+3A_z">z</code></td>
<td>

<p>numeric vector of data of length n following a standardized EVD.
</p>
</td></tr>
<tr><td><code id="revtrans.evd_+3A_threshold">threshold</code></td>
<td>

<p>number or numeric vector of length n giving the threshold, if applicable.
</p>
</td></tr>
<tr><td><code id="revtrans.evd_+3A_location">location</code></td>
<td>

<p>number or numeric vector of length n giving the location parameter(s), if applicable.
</p>
</td></tr>
<tr><td><code id="revtrans.evd_+3A_scale">scale</code></td>
<td>

<p>number or or numeric vector of length n giving the scale parameter(s).
</p>
</td></tr>
<tr><td><code id="revtrans.evd_+3A_shape">shape</code></td>
<td>

<p>number or numeric vector of length n giving the shape parameter(s), if applicable.
</p>
</td></tr>
<tr><td><code id="revtrans.evd_+3A_type">type</code></td>
<td>

<p>character string naming to what EVD should the data be reverse-transformed.  Default is GEV df.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For standardized EVD data (e.g., via <code>trans</code>), this function performs the reverse transformation back to the original scale.
</p>


<h3>Value</h3>

<p>numeric vector of length n.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>See Also</h3>

<p><code><a href="#topic+trans">trans</a></code>, <code><a href="#topic+trans.fevd">trans.fevd</a></code>, <code><a href="#topic+fevd">fevd</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(PORTw)

fit &lt;- fevd(TMX1, PORTw, location.fun=~AOindex, units="deg C")
fit

z &lt;- trans(fit)

fevd(z)

p &lt;- findpars(fit)

y &lt;- revtrans.evd(z=z, location=p$location, scale=2.6809613,
    shape=-0.1812824)

fevd(y)

qqplot(y, PORTw$TMX1)


</code></pre>

<hr>
<h2 id='rlevd'>
Return Levels for Extreme Value Distributions
</h2><span id='topic+rlevd'></span>

<h3>Description</h3>

<p>Calculate return levels for extreme value distributions (EVDs).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rlevd(period, loc = 0, scale = 1, shape = 0, threshold = 0,
    type = c("GEV", "GP", "PP", "Gumbel", "Frechet", "Weibull",
    "Exponential", "Beta", "Pareto"),
    npy = 365.25, rate = 0.01)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rlevd_+3A_period">period</code></td>
<td>

<p>numeric vector giving the desired return periods.
</p>
</td></tr>
<tr><td><code id="rlevd_+3A_loc">loc</code>, <code id="rlevd_+3A_scale">scale</code>, <code id="rlevd_+3A_shape">shape</code></td>
<td>

<p>single numbers giving the parameter values.
</p>
</td></tr>
<tr><td><code id="rlevd_+3A_threshold">threshold</code></td>
<td>

<p>number giving the threshold, if applicable.
</p>
</td></tr>
<tr><td><code id="rlevd_+3A_type">type</code></td>
<td>

<p>character string naming which EVD to calculate return levels from.  If <code>type</code> is &ldquo;PP&rdquo;, then it is converted to &ldquo;GEV&rdquo;.
</p>
</td></tr>
<tr><td><code id="rlevd_+3A_npy">npy</code></td>
<td>

<p>number stating how many values per year.
</p>
</td></tr>
<tr><td><code id="rlevd_+3A_rate">rate</code></td>
<td>

<p>The rate of exceedance.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The extreme value distributions (EVD's) are generalized extreme value (GEV) or generalized Pareto (GP).  The point process characterization is an equivalent form, but is not handled here.  The GEV df is given by
</p>
<p>Pr(X &lt;= x) = G(x) = exp[-(1 + shape*(x - location)/scale)^(-1/shape)]
</p>
<p>for 1 + shape*(x - location) &gt; 0 and scale &gt; 0.  It the shape parameter is zero, then the df is defined by continuity and simplies to
</p>
<p>G(x) = exp(-exp((x - location)/scale)).
</p>
<p>The GEV df is often called a family of df's because it encompasses the three types of EVD's: Gumbel (shape = 0, light tail), Frechet (shape &gt; 0, heavy tail) and the reverse Weibull (shape &lt; 0, bounded upper tail at location - scale/shape).  It was first found by R. von Mises (1936) and also independently noted later by meteorologist A. F. Jenkins (1955).  It enjoys theretical support for modeling maxima taken over large blocks of a series of data.
</p>
<p>The generalized Pareo df is given by (Pickands, 1975)
</p>
<p>Pr(X &lt;= x) = F(x) = 1 - [1 + shape*(x - threshold)/scale]^(-1/shape)
</p>
<p>where 1 + shape*(x - threshold)/scale &gt; 0, scale &gt; 0, and x &gt; threshold.  If shape = 0, then the GP df is defined by continuity and becomes
</p>
<p>F(x) = 1 - exp(-(x - threshold)/scale).
</p>
<p>There is an approximate relationship between the GEV and GP df's where the GP df is approximately the tail df for the GEV df.  In particular, the scale parameter of the GP is a function of the threshold (denote it scale.u), and is equivalent to scale + shape*(threshold - location) where scale, shape and location are parameters from the &ldquo;equivalent&rdquo; GE Vdf.  Similar to the GEV df, the shape parameter determines the tail behavior, where shape = 0 gives rise to the exponential df (light tail), shape &gt; 0 the Pareto df (heavy tail) and shape &lt; 0 the Beta df (bounded upper tail at location - scale.u/shape).  Theoretical justification supports the use of the GP df family for modeling excesses over a high threshold (i.e., y = x - threshold).  It is assumed here that <code>x</code>, <code>q</code> describe x (not y = x - threshold).  Similarly, the random draws are y + threshold.
</p>
<p>See Coles (2001) and Reiss and Thomas (2007) for a very accessible text on extreme value analysis and for more theoretical texts, see for example, Beirlant et al. (2004), de Haan and Ferreira (2006), as well as Reiss and Thomas (2007).
</p>
<p>Return levels are essentially the same as quantiles.  In the case of the GEV family, they are the same.  In the case of the GP df, they are very similar, but the exceedance rate is taken into consideration.
</p>


<h3>Value</h3>

<p>named numeric vector of same length as <code>period</code> giving the calculated return levels for each return period.
</p>


<h3>Note</h3>

<p>Currently, this function does not handle the PP type.  Return levels for this case can be handled in several different ways.  For example, they could be calculated from the equivalent GEV df or equivalent GP df.  In any case, one needs first to determine how to handle the frequency component.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Beirlant, J., Goegebeur, Y., Teugels, J. and Segers, J. (2004) <em>Statistics of Extremes: Theory and Applications</em>.  Chichester, West Sussex, England, UK: Wiley, ISBN 9780471976479, 522pp.
</p>
<p>Coles, S.  (2001) <em>An introduction to statistical modeling of extreme values</em>, London, U.K.: Springer-Verlag, 208 pp.
</p>
<p>de Haan, L. and Ferreira, A. (2006) <em>Extreme Value Theory: An Introduction</em>.  New York, NY, USA: Springer, 288pp.
</p>
<p>Jenkinson, A. F. (1955) The frequency distribution of the annual maximum (or minimum) of meteorological elements. <em>Quart. J. R.  Met. Soc.</em>, <b>81</b>, 158&ndash;171.
</p>
<p>Pickands, J. (1975) Statistical inference using extreme order statistics.  <em>Annals of Statistics</em>, <b>3</b>, 119&ndash;131.
</p>
<p>Reiss, R.-D. and Thomas, M. (2007) <em>Statistical Analysis of Extreme Values: with applications to insurance, finance, hydrology and other fields</em>. Birkhauser, 530pp., 3rd edition.
</p>
<p>von Mises, R. (1936) La distribution de la plus grande de n valeurs, <em>Rev. Math. Union Interbalcanique</em> <b>1</b>, 141&ndash;160.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+devd">devd</a></code>, <code><a href="#topic+return.level">return.level</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>rlevd(c(2, 20, 100), loc=10, scale=2, shape=0.5)

rlevd(c(2, 20, 100), scale=2, shape=0.5, type="GP")

</code></pre>

<hr>
<h2 id='Rsum'> Hurricane Frequency Dataset.</h2><span id='topic+Rsum'></span>

<h3>Description</h3>

<p>This dataset gives the number of hurricanes per year (from 1925 to 1995) as well as the ENSO state and total monetary damage.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Rsum)</code></pre>


<h3>Format</h3>

<p>A data frame with 71 observations on the following 4 variables.
</p>

<dl>
<dt>Year</dt><dd><p>a numeric vector giving the year.</p>
</dd>
<dt>EN</dt><dd><p>a numeric vector giving the ENSO state (-1 for La Ninha, 1 for El Ninho and 0 otherwise).</p>
</dd>
<dt>Ct</dt><dd><p>a numeric vector giving the number of hurricanes for the corresponding year.</p>
</dd>
<dt>TDam</dt><dd><p>a numeric vector giving the total monetary damage (millions of U.S. dollars).</p>
</dd>
</dl>



<h3>Details</h3>

<p>More information on these data can be found in Pielke and Landsea (1998) or Katz (2002).
</p>


<h3>References</h3>

<p>Katz, R. W. (2002) Stochastic modeling of hurricane damage. <em>Journal of Applied Meteorology</em>, <b>41</b>, 754&ndash;762.
</p>
<p>Pielke, R. A. and Landsea, C. W. (1998) Normalized hurricane damages in the United States: 1925-95. <em>Weather and Forecasting</em>, <b>13</b>, (3), 621&ndash;631.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Rsum)
str(Rsum)
plot(Rsum)

# Reproduce Fig. 1 of Katz (2002).
plot(	Rsum[,"Year"], Rsum[,"TDam"]/1000, type="h", xlab="Year",
	ylab="Total damage (billion U.S. dollars)",
	ylim=c(0,80), lwd=2)

# Reproduce Fig. 2 of Katz (2002).
plot(Rsum[,"Year"],Rsum[,"Ct"],type="h", xlab="Year",
    ylab="Number of Hurricanes", ylim=c(0,5), lwd=2)

</code></pre>

<hr>
<h2 id='SantaAna'>
Santa Ana Winds Data
</h2><span id='topic+SantaAna'></span>

<h3>Description</h3>

<p>Meteorological data pertaining to Santa Ana winds.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("SantaAna")</code></pre>


<h3>Format</h3>

<p>The format is:
chr &quot;SantaAna&quot;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># data(SantaAna)
## maybe str(SantaAna) ; plot(SantaAna) ...
</code></pre>

<hr>
<h2 id='shiftplot'> Shift Plot Between Two Sets of Data</h2><span id='topic+shiftplot'></span>

<h3>Description</h3>

<p>A shift plot is a plot of the quantiles of a data set y minus those of another data set x against those of x.  Includes 95 percent simultaneous confidence bands.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shiftplot(x, y, pch = 20, xlab = "x Quantiles", ylab = "y Quantiles", main = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="shiftplot_+3A_x">x</code></td>
<td>
<p> numeric vector of length m.</p>
</td></tr>
<tr><td><code id="shiftplot_+3A_y">y</code></td>
<td>
<p> numeric vector of length n.</p>
</td></tr>
<tr><td><code id="shiftplot_+3A_pch">pch</code></td>
<td>
<p> Plotting character.</p>
</td></tr>
<tr><td><code id="shiftplot_+3A_xlab">xlab</code></td>
<td>
<p> Character string giving abscissa axis label.</p>
</td></tr>
<tr><td><code id="shiftplot_+3A_ylab">ylab</code></td>
<td>
<p> Character string giving ordinate axis label.</p>
</td></tr>
<tr><td><code id="shiftplot_+3A_main">main</code></td>
<td>
<p> Character string giving plot title.</p>
</td></tr>
<tr><td><code id="shiftplot_+3A_...">...</code></td>
<td>
<p> Other optional arguments to plot function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The shift plot is a graph of y_q - x_q vs. x_q, where y_q and x_q denote the quantiles of x and y, resp.  95 percent simultaneous confidence bands are calculated per Doksum and Sievers (1976).  The primary usage of this plot is where x is a control group and y is an experimental method; or something similar.  For example, x might represent observations, and y might represent climate model output; or some such.
</p>


<h3>Value</h3>

<p>No value is returned, but a plot is created.
</p>


<h3>Author(s)</h3>

<p>Peter Guttorp
</p>


<h3>References</h3>

<p>Doksum, K. A. and Sievers, G. L. (1976)  Plotting with confidence: graphical comparisons of two populations.  <em>Biometrika</em>, <b>63</b>, (3), 421&ndash;434.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+qqplot">qqplot</a></code>, <code><a href="#topic+qqnorm">qqnorm</a></code>, <code><a href="stats.html#topic+approxfun">approxfun</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>z &lt;- rnorm( 100)
y &lt;- rexp(30)
shiftplot( z, y)

data( ftcanmax)
shiftplot( y, ftcanmax[,"Prec"])
</code></pre>

<hr>
<h2 id='strip'>
Strip Fitted EVD Object of Everything but the Parameter Estimates
</h2><span id='topic+strip'></span><span id='topic+strip.fevd'></span>

<h3>Description</h3>

<p>Take any fevd object, regardless of estimation method, and return only a vector of the estimated parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>strip(x, use.names = TRUE, ...)
## S3 method for class 'fevd'
strip(x, use.names = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="strip_+3A_x">x</code></td>
<td>

<p>An object of class &ldquo;fevd&rdquo;.
</p>
</td></tr>
<tr><td><code id="strip_+3A_use.names">use.names</code></td>
<td>

<p>logical stating whether or not to keep the names attribute
</p>
</td></tr>
<tr><td><code id="strip_+3A_...">...</code></td>
<td>

<p>For the Bayesian method, if an alternative function to taking the mean or posterior mode of the MCMC samples is used, then optional arguments may be passed.  Otherwise, not used.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is very similar to <code>distill</code>, but returns less information.  
</p>


<h3>Value</h3>

<p>numeric vector with the parameter estimates.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>See Also</h3>

<p><code><a href="#topic+distill.fevd">distill.fevd</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
z &lt;- revd(100, loc=20, scale=0.5, shape=-0.2)
fit &lt;- fevd(z)
fit

strip( fit )
strip( fit, use.names = FALSE )

# Compare with ...
distill( fit )
distill( fit, cov = FALSE )

## Not run: 
data( "Fort" )
fit &lt;- fevd(Prec, Fort, threshold=0.395,
    scale.fun=~sin(2 * pi * (year - 1900)/365.25) + 
        cos(2 * pi * (year - 1900)/365.25),
    type="PP", method="Bayesian", iter=1999, use.phi=TRUE, verbose=TRUE)

fit

strip( fit )
strip( fit, burn.in = 700 )
strip( fit, FUN = "postmode" )


## End(Not run)
</code></pre>

<hr>
<h2 id='taildep'>
Tail Dependence
</h2><span id='topic+taildep'></span>

<h3>Description</h3>

<p>Function to calculate the estimated tail dependence parameters chi and chibar.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>taildep(x, y, u, type = c("all", "chi", "chibar"), na.rm = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="taildep_+3A_x">x</code>, <code id="taildep_+3A_y">y</code></td>
<td>

<p>numeric vectors of same length.  <code>x</code> may be a two-column matrix or data frame, in which case each column is assumed to be the two vectors of interest (both dependence estimates are symmetric so that it does not matter which is which).
</p>
</td></tr>
<tr><td><code id="taildep_+3A_u">u</code></td>
<td>

<p>single numeric between 0 and 1 (non-inclusive) giving the probability threshold overwhich to compute the dependence measures (should be close to 1, but low enough to include enough data.
</p>
</td></tr>
<tr><td><code id="taildep_+3A_type">type</code></td>
<td>

<p>character string determining which dependence parameter to estimate (chi or chibar).  Default estimates both.
</p>
</td></tr>
<tr><td><code id="taildep_+3A_na.rm">na.rm</code></td>
<td>

<p>logical, should missing values be removed?
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The tail dependence parameters are those described in, e.g., Reiss and Thomas (2007) Eq (2.60) for &quot;chi&quot; and Eq (13.25) &quot;chibar&quot;, and estimated by Eq (2.62) and Eq (13.28), resp.  See also, Sibuya (1960) and Coles (2001) sec. 8.4, as well as other texts on EVT such as Beirlant et al. (2004) sec. 9.4.1 and 10.3.4 and de Haan and Ferreira (2006).
</p>
<p>Specifically, for two series X and Y with associated df's F and G, chi, a function of u, is defined as
</p>
<p>chi(u) = Pr[Y &gt; G^(-1)(u) | X &gt; F^(-1)(u)] = Pr[V &gt; u | U &gt; u],
</p>
<p>where (U,V) = (F(X),G(Y))&ndash;i.e., the copula.  Define chi = limit as u goes to 1 of chi(u).
</p>
<p>The coefficient of tail dependence, chibar(u) was introduced by Coles et al. (1999), and is given by
</p>
<p>chibar(u) = 2*log(Pr[U &gt; u])/log(Pr[U &gt; u, V &gt; u]) - 1.
</p>
<p>Define chibar = limit as u goes to 1 of chibar(u).
</p>
<p>The associated estimators for the tail dependence parameters employed by these functions are based on the above two coefficients of tail dependence, and are given by Reiss and Thomas (2007) Eq (2.62) and (13.25) as
</p>
<p>chi.hat(x, y; u) = sum(x_i &gt;  sort(x)[floor(n*u)] and y_i &gt; sort(y)[floor(n*u)])/(n*(1-u))   [based on chi]
</p>
<p>and
</p>
<p>chibar.hat(x, y; u) = 2*log(1 - u)/log(mean(x_i &gt; sort(x)[floor(n*u)] and y_i &gt; sort(y)[floor(n*u)])) - 1.
</p>
<p>Some properties of the above dependence coefficients, chi(u), chi, and chibar(u) and chibar, are that 0 &lt;= chi(u), chi &lt;= 1, where if X and Y are stochastically independent, then chi(u) = 1 - u, and chibar = 0.  If X = Y (perfectly dependent), then chi(u) = chi = 1.  For chibar(u) and chibar, we have that -1 &lt;= chibar(u), chibar &lt;= 1.  If U = V, then chibar = 1.  If chi = 0, then chibar &lt; 1 (tail independence with chibar determining the degree of dependence).
</p>


<h3>Value</h3>

<p>numeric vector of length 1 or 2 depending on the type argument giving the estimated tail dependence parameters.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Beirlant, J., Goegebeur, Y., Teugels, J. and Segers, J. (2004) <em>Statistics of Extremes: Theory and Applications</em>.  Chichester, West Sussex, England, UK: Wiley, ISBN 9780471976479, 522pp.
</p>
<p>Coles, S.  (2001) <em>An introduction to statistical modeling of extreme values</em>, London: Springer-Verlag.
</p>
<p>Coles, S., Heffernan, J. E., and Tawn, J. A. (1999) Dependence measures for extreme value analyses.  <em>Extremes</em>, <b>2</b>, 339&ndash;365.
</p>
<p>de Haan, L. and Ferreira, A. (2006) <em>Extreme Value Theory: An Introduction</em>.  New York, NY, USA: Springer, 288pp.
</p>
<p>Reiss, R.-D. and Thomas, M. (2007) <em>Statistical Analysis of Extreme Values: with applications to insurance, finance, hydrology and other fields</em>. Birkhauser, 530pp., 3rd edition.
</p>
<p>Sibuya, M. (1960) Bivariate extreme statistics.  <em>Ann. Inst. Math. Statist.</em>, <b>11</b>, 195&ndash;210.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+atdf">atdf</a></code>, <code><a href="#topic+taildep.test">taildep.test</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##
## Example where a r.v. is completely dependent in
## terms of the variables, but completely tail
## independent (see Reiss and Thomas p. 75).
z &lt;- runif(100, -1, 0)
w &lt;- -1*(1 + z)
taildep(z,w,u=0.8)

## Not run: 
data(FCwx)
taildep(FCwx$MxT, FCwx$MnT, 0.8)
taildep(FCwx$MxT, FCwx$Prec, 0.8)

## End(Not run)
</code></pre>

<hr>
<h2 id='taildep.test'>
Tail Dependence Test
</h2><span id='topic+taildep.test'></span><span id='topic+relative.rank'></span>

<h3>Description</h3>

<p>Testing tail dependence against tail independence.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>taildep.test(x, y, cthresh = -0.5, trans = "relative.rank", na.action = na.fail, ...)

relative.rank(x, div = "n", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="taildep.test_+3A_x">x</code>, <code id="taildep.test_+3A_y">y</code></td>
<td>

<p>numeric vectors of same length.  For <code>taildep.test</code>, <code>x</code> may be a two-column matrix or data frame, in which case each column is assumed to be the two vectors of interest.
</p>
</td></tr>
<tr><td><code id="taildep.test_+3A_cthresh">cthresh</code></td>
<td>

<p>single numeric between -1 and 0 (non-inclusive) over which the transformed and shifted x + y variable is tested (see Details).
</p>
</td></tr>
<tr><td><code id="taildep.test_+3A_trans">trans</code></td>
<td>

<p>character string naming a function to transform the x and y variables so that they are in the lower left quadrant (see Details).  If variables are already transformed as such (or it is not necessary), then use &ldquo;identity&rdquo;.
</p>
</td></tr>
<tr><td><code id="taildep.test_+3A_div">div</code></td>
<td>
<p>character one of &ldquo;n&rdquo; or &ldquo;n+1&rdquo; stating whether to divide the ranks by n or n + 1 so that the reslting transformations are in [0,1] or (0,1), resp.</p>
</td></tr>
<tr><td><code id="taildep.test_+3A_na.action">na.action</code></td>
<td>
<p>function to be called to handle missing values.</p>
</td></tr>
<tr><td><code id="taildep.test_+3A_...">...</code></td>
<td>

<p>optional arguments to the <code>trans</code> function.  In the case of <code>relative.rank</code> these are optional arguments to the function <code>rank</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is the tail dependence test described in Reiss and Thomas (2007) section 13.3.  It is, unusually, a test whose null hypothesis is that the two random variables, X and Y, are dependent.  So, for example, if a significance level alpha = 0.01 test is desired, then the null huypothesis (dependence) is rejected for values of the statistic with p-values less than 0.01.
</p>
<p>To do the test, the variables must first be transformed to the left lower quadrant.  Following Reiss and Thomas (2007), the default is to transform the data by means of the sample distribution functions (df's), u = Fhat_n(x) and v = Fhat_n(y) (i.e., using the function <code>relative.rank</code>).  This yields random variables between 0 and 1, and subsequently they are shifted to be between -1 and 0 (this is done by <code>taildep.test</code> so should not be done by the <code>trans</code> function).
</p>
<p>Ultimately, the test statistic is given by
</p>
<p>-(sum(log(c.tilde) + m)/sqrt(m)),
</p>
<p>where c.tilde = (u + v)*1(u+v &gt; c)/c, for c a threshold (i.e., <code>cthresh</code>).  The statistic is assumed to be N(0,1), and the p-value is calculated accordingly.
</p>
<p>The test is somewhat sensitive to the choice of threshold, <code>cthresh</code>, and it is probably a good idea to try several values (approaching zero from the left).  Ideally, the threshold should yield about 10 - 15 percent excesses.
</p>


<h3>Value</h3>

<p>A list object of class &ldquo;htest&rdquo; is returned with components:
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>the calling string</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>character vector giving the names of the data sets employed (if x is a matrix, then the second component will be &ldquo; &rdquo;.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>character string, which will always be &ldquo;Reiss-Thomas (13.35)&rdquo;.</p>
</td></tr>
<tr><td><code>transformation</code></td>
<td>
<p>same as trans argument.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>named vector giving the value of the threshold and any arguments passed to the trans function (perhaps this is not a good idea, and may be changed eventually).</p>
</td></tr>
<tr><td><code>c.full</code></td>
<td>
<p>value of the vector u + v after having been transformed and shifted to be between -1 and 0.  This is so that the user can adjust the threshold so that 10 - 15 percent of the values exceed it.</p>
</td></tr>
<tr><td><code>statistic</code></td>
<td>
<p>numeric giving the value of the test statistic.</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>character string stating &ldquo;greater&rdquo;.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>numeric between 0 and 1 giving the p-value for the test.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Reiss, R.-D. and Thomas, M. (2007) <em>Statistical Analysis of Extreme Values: with applications to insurance, finance, hydrology and other fields</em>. Birkhauser, 530pp., 3rd edition.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+taildep">taildep</a></code>, <code><a href="#topic+atdf">atdf</a></code>, <code><a href="#topic+lr.test">lr.test</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- arima.sim(n = 63, list(ar = c(0.8897, -0.4858), ma = c(-0.2279, 0.2488)),
               sd = sqrt(0.1796))

y &lt;- x + rnorm(63)

taildep.test(x, y)

# Recall that null hypothesis is tail dependence!

## Not run: 
data(PORTw)
taildep.test(PORTw$TMX1, PORTw$TMN0, cthresh=-0.3)

data(FCwx)
taildep.test(FCwx$MxT, FCwx$Prec, cthresh=-0.4)

# Run the example (13.3.6) in Reiss and Thomas (2007)
# using the 'wavesurge' dataset from package 'ismev'.
data(wavesurge)
cth &lt;- seq(-0.46,-0.35,0.01)
tab13.1 &lt;- matrix(NA, 2, 12)
colnames(tab13.1) &lt;- as.character(cth)
for(i in 1:12) {
    tmp &lt;- taildep.test(wavesurge, cthresh=cth[i], ties.method="max")
    tab13.1[1,i] &lt;- tmp$parameter["m"]
    tab13.1[2,i] &lt;- tmp$p.value
} # end of for 'i' loop.

rownames(tab13.1) &lt;- c("m", "p-value")
tab13.1

## End(Not run)

</code></pre>

<hr>
<h2 id='threshrange.plot'>
Threshold Selection Through Fitting Models to a Range of Thresholds
</h2><span id='topic+threshrange.plot'></span>

<h3>Description</h3>

<p>Find an appropriate threshold for GP or PP models by fitting them to a sequence of thresholds in order to find the lowest threshold that yields roughly the same parameter estiamtes as any higher threshold.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>threshrange.plot(x, r, type = c("GP", "PP", "Exponential"), nint = 10, alpha = 0.05,
    na.action = na.fail, set.panels = TRUE, verbose = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="threshrange.plot_+3A_x">x</code></td>
<td>

<p>numeric vector of data.
</p>
</td></tr>
<tr><td><code id="threshrange.plot_+3A_r">r</code></td>
<td>

<p>numeric vector of length two giving the range of thresholds.
</p>
</td></tr>
<tr><td><code id="threshrange.plot_+3A_type">type</code></td>
<td>

<p>character string stating which model to fit.
</p>
</td></tr>
<tr><td><code id="threshrange.plot_+3A_nint">nint</code></td>
<td>

<p>number of thresholds to use.
</p>
</td></tr>
<tr><td><code id="threshrange.plot_+3A_alpha">alpha</code></td>
<td>

<p>number between zero and one stating which 1 - <code>alpha</code> confidence level to use for the confidence limits.
</p>
</td></tr>
<tr><td><code id="threshrange.plot_+3A_na.action">na.action</code></td>
<td>

<p>function to be called to handle missing values.
</p>
</td></tr>
<tr><td><code id="threshrange.plot_+3A_set.panels">set.panels</code></td>
<td>

<p>logical; should the function set the panels on the device (TRUE) or not (FALSE).
</p>
</td></tr>
<tr><td><code id="threshrange.plot_+3A_verbose">verbose</code></td>
<td>

<p>logical; should progress information be printed to the screen?
</p>
</td></tr>
<tr><td><code id="threshrange.plot_+3A_...">...</code></td>
<td>

<p>optional arguments to <code>fevd</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Several GP or PP (or exponential) models are fit to <code>x</code> according to a range of <code>nint</code> thresholds given by <code>r</code>.  The estimated parameters are plotted against these thresholds along with their associated (1 - <code>alpha</code>) * 100 percent CIs.
</p>
<p>Choice of threshold is a compromise between low variance (lower thresholds yield more data with which to fit the models) and bias (higher thresholds yield estimates that are less biased because model assumptions require very high thresholds, and it can happen that lower data values may be more abundant causing the model to be biased toward the wrong values) in the parameter estimates.  An appropriate threshold should yield the same parameter estimates (within uncertainty) as would be fit for any model fit to higher thresholds.  Therefore, the idea is to find the lowest possible threshold whereby a higher threshold would give the same results within uncertainty bounds.
</p>
<p>See Coles (2001) sec. 4.3.4 and 4.4 for more information.
</p>
<p>Note that the default uses maximum likelihood estimation.  While it is possible to use other methods, it is not recommended because of efficiency problems.
</p>


<h3>Value</h3>

<p>A matrix of parameter values and CI bounds for each threshold value is returned invisibly.  A plot is created.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Coles, S.  (2001). <em>An introduction to statistical modeling of extreme values</em>, London, United Kingdom: Springer-Verlag, 208 pp.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fevd">fevd</a></code>, <code><a href="#topic+mrlplot">mrlplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Fort)
threshrange.plot(Fort$Prec, r = c(1, 2.25), nint=5)

## Not run: 
threshrange.plot(Fort$Prec, r=c(0.01,1), nint=30, verbose=TRUE)

threshrange.plot(Fort$Prec, r=c(0.2,0.8), type="PP", nint=15,
    verbose=TRUE)

threshrange.plot(Fort$Prec, r=c(0.2,0.8), type="PP", nint=15,
    optim.args=list(method="Nelder-Mead"), verbose=TRUE)


## End(Not run)
</code></pre>

<hr>
<h2 id='Tphap'> Daily Maximum and Minimum Temperature in Phoenix, Arizona.</h2><span id='topic+Tphap'></span>

<h3>Description</h3>

<p>Daily maximum and minimum temperature (degrees Fahrenheit) for July through August 1948 through 1990 at
Sky Harbor airport in Phoenix, Arizona.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Tphap)</code></pre>


<h3>Format</h3>

<p>A data frame with 43 observations on the following 3 variables.
</p>

<dl>
<dt>Year</dt><dd><p>a numeric vector giving the number of years since 1900.</p>
</dd>
<dt>Month</dt><dd><p>a numeric vector giving the month.</p>
</dd>
<dt>Day</dt><dd><p>a numeric vector giving the day of the month.</p>
</dd>
<dt>MaxT</dt><dd><p>a numeric vector giving the daily maximum temperatures in degrees Fahrenheit.</p>
</dd>
<dt>MinT</dt><dd><p>a numeric vector giving the daily minimum temperatures in degrees Fahrenheit.</p>
</dd>
</dl>



<h3>Details</h3>

<p>Data are daily maximum and minimum temperature for the summer months of July through August from 1948 through 1990.
</p>


<h3>Source</h3>

<p>U.S. National Weather Service Forecast office at the Phoenix Sky Harbor Airport.
</p>


<h3>References</h3>

<p>Balling, R. C., Jr., Skindlov, J. A. and Phillips, D. H. (1990) The impact of increasing summer mean temperatures on extreme maximum and minimum temperatures in Phoenix, Arizona. <em>Journal of Climate</em>, <b>3</b>, 1491&ndash;1494.
</p>
<p>Tarleton, L. F. and Katz, R. W. (1995) Statistical explanation for trends in extreme summer temperatures at Phoenix, A.Z. <em>Journal of Climate</em>, <b>8</b>, (6), 1704&ndash;1708.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Tphap)
str(Tphap)

par( mfrow=c(2,1))
hist( Tphap[,"MaxT"], main="", xlab="Max Temp", xlim=c(60,120), freq=FALSE, breaks="FD", col="red")
hist( Tphap[,"MinT"], main="", xlab="Min Temp", xlim=c(60,120), freq=FALSE, breaks="FD", col="blue")
</code></pre>

<hr>
<h2 id='trans'>
Transform Data
</h2><span id='topic+trans'></span><span id='topic+trans.fevd'></span>

<h3>Description</h3>

<p>Method function to transform a data set.  In the case of <code>fevd</code> objects, the transformation is to a standardized Gumbel or exponential scale.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trans(object, ...)

## S3 method for class 'fevd'
trans(object, ..., burn.in = 499, return.all = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="trans_+3A_object">object</code></td>
<td>

<p>An R object with a <code>trans</code> method.  In the case of &ldquo;fevd&rdquo; objects, output from <code>fevd</code>.
</p>
</td></tr>
<tr><td><code id="trans_+3A_burn.in">burn.in</code></td>
<td>
<p>number giving the burn in value.  The first 1:burn.in will not be used in obtaining parmaeter estiamtes. </p>
</td></tr>
<tr><td><code id="trans_+3A_return.all">return.all</code></td>
<td>
<p>logical, only for POT models, but primarily for use with the Point Process model.  Should only the threshold exceedances be returned?</p>
</td></tr>
<tr><td><code id="trans_+3A_...">...</code></td>
<td>

<p>Not used.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Many important situations occur in extreme value analysis (EVA) where it is useful or necessary to transform data to a standardized scale.  For example, when investigating multivariate or conditional EVA much of the theory revolves around first transfroming the data to a unit scale.  Further, for non-stationary models, it can be useful to transform the data to a df that does not depend on the covariates.
</p>
<p>The present function transforms data taken from &ldquo;fevd&rdquo; class objects and transforms them to either a standard Gumbel (GEV, Gumbel case) or standard exponential (GP, PP, exponential case) df.  In the first case, if the data are Gumbel distributed (really, if a gumbel fit was performed) the transformation is:
</p>
<p>z = (x - location(yi))/scale(yi),
</p>
<p>where yi represent possible covariate terms and z is distributed according to a Gumbel(0, 1) df.  If the data are GEV distributed, then the transformation is:
</p>
<p>z = - log(1 + (shape(yi)/scale(yi) * (x - location(yi)))^(-1/shape(yi))),
</p>
<p>and again z is distributed Gumbel(0, 1).
</p>
<p>In the case of exponentially distributed data, the transformation is:
</p>
<p>z = (x - threshold(yi))/scale(yi)
</p>
<p>and z is distributed according to an exponential(1) df.
</p>
<p>For GP distributed data, the transformation is:
</p>
<p>z = -log((1 + (shape(yi)/scale(yi) * (x - threshold(yi))))^(-1/shape(yi))
</p>
<p>where again z follows an exponential(1) df.
</p>
<p>For PP models, the transformation is:
</p>
<p>z = (1 + shape(yi)/scale(yi) * (x - threshold(yi)))^(-1/shape(yi))
</p>
<p>and z is distributed exponential(1).
</p>
<p>See Coles (2001) sec. 2.3.2 for more details.
</p>


<h3>Value</h3>

<p>numeric vector of transformed data.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Coles, S.  (2001) <em>An introduction to statistical modeling of extreme values</em>, London, U.K.: Springer-Verlag, 208 pp.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+revtrans.evd">revtrans.evd</a></code>, <code><a href="#topic+fevd">fevd</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(PORTw)

fit &lt;- fevd(TMX1, PORTw, location.fun=~AOindex, units="deg C")
fit

z &lt;- trans(fit)

fevd(z)

</code></pre>

<hr>
<h2 id='xbooter'>
Additional Bootstrap Functions for Univariate EVA
</h2><span id='topic+xbooter'></span>

<h3>Description</h3>

<p>Additonal bootstrap capabilities for extreme-value analysis for fevd objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xbooter(x, B, rsize, block.length = 1,
	return.period = c(10, 20, 50, 100, 200, 500),
	qcov = NULL, qcov.base = NULL, shuffle = NULL,
	replace = TRUE, verbose = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="xbooter_+3A_x">x</code></td>
<td>

<p>list object of class &ldquo;fevd&rdquo;
</p>
</td></tr>
<tr><td><code id="xbooter_+3A_b">B</code>, <code id="xbooter_+3A_rsize">rsize</code>, <code id="xbooter_+3A_block.length">block.length</code>, <code id="xbooter_+3A_shuffle">shuffle</code>, <code id="xbooter_+3A_replace">replace</code></td>
<td>
<p> See the help file for booter from the distillery package.  </p>
</td></tr>
<tr><td><code id="xbooter_+3A_return.period">return.period</code></td>
<td>

<p>numeric value for the desired return period for which CIs are desired.
</p>
</td></tr>
<tr><td><code id="xbooter_+3A_qcov">qcov</code></td>
<td>
<p>numeric matrix with rows the same length as <code>q</code> and columns equal to the number of parameters (+ 1 for the threshold, if a POT model).  This gives any covariate values for a nonstationary model.  If NULL, and model is non-stationary, only the intercept terms for modeled parameters are used, and if a non-constant threshold, only the first threshold value is used.  Not used if model is stationary. </p>
</td></tr>
<tr><td><code id="xbooter_+3A_qcov.base">qcov.base</code></td>
<td>
<p>numeric matrix analogous to <code>qcov</code>.  When provided, the function returns the difference in return levels between the level for the covariates in <code>qcov</code> and the level for covariates in <code>qcov.base</code>.</p>
</td></tr>
<tr><td><code id="xbooter_+3A_verbose">verbose</code></td>
<td>

<p>logical if TRUE progress information is printed to the screen.
</p>
</td></tr>
<tr><td><code id="xbooter_+3A_...">...</code></td>
<td>

<p>Additonal optional arguments to the <code>booter</code> function.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>ci</code> method function will perform parametric bootstrapping for &ldquo;fevd&rdquo; objects, but this function is a wrapper to <code>booter</code>, which allows for greater flexibility with &ldquo;fevd&rdquo; objects.  Gives CIs for the EVD parameters and return levels.
</p>


<h3>Value</h3>

<p>Object of class &ldquo;booted&rdquo; is returned.  See the help file for <code>booter</code> for more information.
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Gilleland, E. (2020) Bootstrap methods for statistical inference. Part I: Comparative forecast verification for continuous variables. <em>Journal of Atmospheric and Oceanic Technology</em>, <b>37</b> (11), 2117 - 2134, doi: 10.1175/JTECH-D-20-0069.1.
</p>
<p>Gilleland, E. (2020) Bootstrap methods for statistical inference. Part II: Extreme-value analysis. <em>Journal of Atmospheric and Oceanic Technology</em>, <b>37</b> (11), 2135 - 2144, doi: 10.1175/JTECH-D-20-0070.1.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fevd">fevd</a></code>, <code><a href="distillery.html#topic+booter">booter</a></code>, <code><a href="#topic+xtibber">xtibber</a></code>, <code><a href="#topic+ci.fevd">ci.fevd</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed( 409 )
z &lt;- apply( matrix( rnorm( 100 * 1000 ), 1000, 100 ), 2, max )
fit &lt;- fevd( z )

# In order to keep the code fast for CRAN compiling, 
# a low value for B is used here, but should use a larger
# value in general.
bfit &lt;- xbooter( fit, B = 50, verbose = TRUE )
ci( bfit, type = "perc" )
</code></pre>

<hr>
<h2 id='xtibber'>
Test-Inversion Bootstrap for Extreme-Value Analysis
</h2><span id='topic+xtibber'></span>

<h3>Description</h3>

<p>Test-inversion bootstrap (TIB) for fevd class objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xtibber(x, type = c("return.level", "parameter"), which.one,
	tib.method = c("interp", "rm"), nuisance = "shape", B,
	test.pars, rsize, block.length = 1, shuffle = NULL,
	replace = TRUE, alpha = 0.05, qcov = NULL,
	qcov.base = NULL, stud = FALSE, step.size, tol = 1e-04,
	max.iter = 1000, keep.iters = TRUE, verbose = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="xtibber_+3A_x">x</code></td>
<td>

<p>List object of class &ldquo;fevd&rdquo;.
</p>
</td></tr>
<tr><td><code id="xtibber_+3A_type">type</code></td>
<td>

<p>character string stating whether to calculate TIB intervals for a return level or a parameter as this funciton will only calculate an interval for a single parameter/return level at a time.
</p>
</td></tr>
<tr><td><code id="xtibber_+3A_which.one">which.one</code></td>
<td>

<p>number or character stating which return level or which parameter to find CIs for.
</p>
</td></tr>
<tr><td><code id="xtibber_+3A_tib.method">tib.method</code></td>
<td>

<p>character stating whether to estimate the TIB interval by interpolating from a series of pre-determined values of the nuisance parameter or to use the Robbins-Monroe (RM) method.  See the help file for <code>tibber</code> from the distillery package for more information.
</p>
</td></tr>
<tr><td><code id="xtibber_+3A_nuisance">nuisance</code></td>
<td>

<p>character naming the nuisance parameter.
</p>
</td></tr>
<tr><td><code id="xtibber_+3A_b">B</code>, <code id="xtibber_+3A_rsize">rsize</code>, <code id="xtibber_+3A_block.length">block.length</code>, <code id="xtibber_+3A_shuffle">shuffle</code>, <code id="xtibber_+3A_replace">replace</code></td>
<td>

<p>See the help file for <code>booter</code> from the distillery package for more information on these arguments.
</p>
</td></tr>
<tr><td><code id="xtibber_+3A_test.pars">test.pars</code></td>
<td>

<p>numeric vector giving the sequence of nuisance parameter values for the interpolation method, or a numeric vector of length two giving the starting values for the RM method.
</p>
</td></tr>
<tr><td><code id="xtibber_+3A_alpha">alpha</code></td>
<td>

<p>numeric between zero and one giving the desired confidence level.
</p>
</td></tr>
<tr><td><code id="xtibber_+3A_qcov">qcov</code></td>
<td>
<p>numeric matrix with rows the same length as <code>q</code> and columns equal to the number of parameters (+ 1 for the threshold, if a POT model).  This gives any covariate values for a nonstationary model.  If NULL, and model is non-stationary, only the intercept terms for modeled parameters are used, and if a non-constant threshold, only the first threshold value is used.  Not used if model is stationary. </p>
</td></tr>
<tr><td><code id="xtibber_+3A_qcov.base">qcov.base</code></td>
<td>
<p>numeric matrix analogous to <code>qcov</code>.  When provided, the function returns the difference in return levels between the level for the covariates in <code>qcov</code> and the level for covariates in <code>qcov.base</code>.</p>
</td></tr>
<tr><td><code id="xtibber_+3A_stud">stud</code></td>
<td>

<p>logical if TRUE will calculate Studentized intervals (generally not profitable with the TIB method).
</p>
</td></tr>
<tr><td><code id="xtibber_+3A_step.size">step.size</code></td>
<td>

<p>Used with the RM method only.  Numeric giving the size of increments to use in the root-finding algorithm.
</p>
</td></tr>
<tr><td><code id="xtibber_+3A_tol">tol</code></td>
<td>

<p>Used with the RM method only.  Numeric stating how close to the desired level of confidence is satisfactory.
</p>
</td></tr>
<tr><td><code id="xtibber_+3A_max.iter">max.iter</code></td>
<td>

<p>numeric giving the maximum number of iterations for the root-finding algorithm before giving up.
</p>
</td></tr>
<tr><td><code id="xtibber_+3A_keep.iters">keep.iters</code></td>
<td>

<p>logical, should all of the values in the root-finding search be kept?  Needed if a plot will be made.
</p>
</td></tr>
<tr><td><code id="xtibber_+3A_verbose">verbose</code></td>
<td>

<p>logical, if TRUE will print progress information to the screen.
</p>
</td></tr>
<tr><td><code id="xtibber_+3A_...">...</code></td>
<td>

<p>optional arguments to <code>nlminb</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function provides a wrapper to the <code>tibber</code> function from distillery for &ldquo;fevd&rdquo; objects.
</p>


<h3>Value</h3>

<p>See the help file for tibber for more information on the value
</p>


<h3>Author(s)</h3>

<p>Eric Gilleland
</p>


<h3>References</h3>

<p>Gilleland, E. (2020) Bootstrap methods for statistical inference. Part I: Comparative forecast verification for continuous variables. <em>Journal of Atmospheric and Oceanic Technology</em>, <b>37</b> (11), 2117 - 2134, doi: 10.1175/JTECH-D-20-0069.1.
</p>
<p>Gilleland, E. (2020) Bootstrap methods for statistical inference. Part II: Extreme-value analysis. <em>Journal of Atmospheric and Oceanic Technology</em>, <b>37</b> (11), 2135 - 2144, doi: 10.1175/JTECH-D-20-0070.1.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fevd">fevd</a></code>, <code><a href="distillery.html#topic+tibber">tibber</a></code>, <code><a href="distillery.html#topic+booter">booter</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data("ftcanmax")
fit &lt;- fevd( Prec, data = ftcanmax )

tbfit &lt;- xtibber( fit, which.one = 100, B = 500,
		 test.pars = seq(-0.01,0.2,,100), verbose = TRUE )

tbfit

plot( tbfit )


## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
