<!DOCTYPE html><html><head><title>Help for package emplik</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {emplik}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#BJnoint'><p>The Buckley-James censored regression estimator</p></a></li>
<li><a href='#bjtest'><p>Test the Buckley-James estimator by Empirical Likelihood</p></a></li>
<li><a href='#bjtest1d'><p>Test the Buckley-James estimator by Empirical Likelihood, 1-dim only</p></a></li>
<li><a href='#bjtestII'><p>Alternative test of the Buckley-James estimator by Empirical Likelihood</p></a></li>
<li><a href='#el.cen.EM'><p>Empirical likelihood ratio for mean</p>
with right, left or doubly censored data, by EM algorithm</a></li>
<li><a href='#el.cen.EM2'><p>Empirical likelihood ratio test for a vector of means</p>
with right, left or doubly censored data, by EM algorithm</a></li>
<li><a href='#el.cen.kmc1d'><p>Empirical likelihood ratio for 1 mean constraint</p>
with right censored data</a></li>
<li><a href='#el.cen.test'><p>Empirical likelihood ratio for mean</p>
with right censored data, by QP.</a></li>
<li><a href='#el.ltrc.EM'><p>Empirical likelihood ratio for mean</p>
with left truncated and right censored data, by EM algorithm</a></li>
<li><a href='#el.test'><p>Empirical likelihood ratio test for the means, uncensored data</p></a></li>
<li><a href='#el.test.wt'><p>Weighted Empirical Likelihood ratio for mean, uncensored data</p></a></li>
<li><a href='#el.test.wt2'><p>Weighted Empirical Likelihood ratio for mean(s), uncensored data</p></a></li>
<li><a href='#el.trun.test'><p>Empirical likelihood ratio for mean with left truncated data</p></a></li>
<li><a href='#emplik-internal'><p>Internal emplik functions</p></a></li>
<li><a href='#emplik-package'>
<p>Empirical likelihood for mean functional/hazard functional with possibly censored data.</p></a></li>
<li><a href='#emplikH.disc'><p>Empirical likelihood ratio for discrete hazard</p>
with right censored, left truncated data</a></li>
<li><a href='#emplikH.disc2'><p>Two sample empirical likelihood ratio for discrete hazards</p>
with right censored, left truncated data, one parameter.</a></li>
<li><a href='#emplikH1.test'><p>Empirical likelihood for hazard with right censored,</p>
left truncated data</a></li>
<li><a href='#emplikH1B'><p>Return binomial empirical likelihood ratio for the given lambda, with right censored data</p></a></li>
<li><a href='#emplikH1P'><p>Return Poisson Empirical likelihood ratio for the given lambda, with right censored data</p></a></li>
<li><a href='#emplikH2.test'><p>Empirical likelihood for weighted hazard with</p>
right censored, left truncated data</a></li>
<li><a href='#emplikH2B'><p>Return binomial empirical likelihood ratio for the given lambda, with 2-sample right censored data</p></a></li>
<li><a href='#emplikH2P'><p>Return Poisson Empirical likelihood ratio for the given lambda, with 2-sample right censored data</p></a></li>
<li><a href='#emplikHs.disc2'><p>Two sample empirical likelihood ratio for discrete hazards</p>
with right censored, left truncated data. Many constraints.</a></li>
<li><a href='#emplikHs.test2'><p>Two sample empirical likelihood ratio test for hazards</p>
with right censored, left truncated data. Many constraints.</a></li>
<li><a href='#findLnew'><p>Find the Wilks Confidence Interval Lower Bound from the Given (empirical) Likelihood Ratio Function</p></a></li>
<li><a href='#findUL'><p>Find the Wilks Confidence Interval from the Given (empirical) Likelihood Ratio Function</p></a></li>
<li><a href='#findUL2'><p>Find the Wilks Confidence Interval from the Given (empirical) Likelihood Ratio Function</p></a></li>
<li><a href='#findULold'><p>Find the Wilks Confidence Interval from the Given (empirical) Likelihood Ratio Function</p></a></li>
<li><a href='#findUnew'><p>Find the Wilks Confidence Interval Upper Bound from the Given (empirical) Likelihood Ratio Function</p></a></li>
<li><a href='#myeloma'><p>Multiple Myeloma Data</p></a></li>
<li><a href='#RankRegTest'><p>Test the AFT model Rank Regression estimator by Empirical Likelihood</p></a></li>
<li><a href='#RankRegTestH'><p>Test the AFT model, Rank Regression estimator by (Hazard)Empirical Likelihood</p></a></li>
<li><a href='#ROCnp'><p>Test the ROC curve by Empirical Likelihood</p></a></li>
<li><a href='#ROCnp2'><p>Test the ROC curve by Empirical Likelihood</p></a></li>
<li><a href='#smallcell'><p>Smallcell Lung Cancer Data</p></a></li>
<li><a href='#WRegEst'><p>Compute the casewise weighted regression estimator for AFT model</p></a></li>
<li><a href='#WRegTest'><p>Test the case weighted regression estimator by Empirical Likelihood</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Maintainer:</td>
<td>Mai Zhou &lt;maizhou@gmail.com&gt;</td>
</tr>
<tr>
<td>Version:</td>
<td>1.3-1</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>quantreg, stats</td>
</tr>
<tr>
<td>Suggests:</td>
<td>KMsurv, boot, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Author:</td>
<td>Mai Zhou. (Art Owen for el.test(). Yifan Yang for some C code.)</td>
</tr>
<tr>
<td>Description:</td>
<td>Empirical likelihood ratio tests for means/quantiles/hazards
 	from possibly censored and/or truncated data. Now does regression too.
	This version contains some C code.</td>
</tr>
<tr>
<td>Title:</td>
<td>Empirical Likelihood Ratio for Censored/Truncated Data</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="http://www.ms.uky.edu/~mai/EmpLik.html">http://www.ms.uky.edu/~mai/EmpLik.html</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-09-07 16:28:44 UTC; maizh</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-09-07 17:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='BJnoint'>The Buckley-James censored regression estimator</h2><span id='topic+BJnoint'></span>

<h3>Description</h3>

<p>Compute the Buckley-James estimator in the regression model 
</p>
<p style="text-align: center;"><code class="reqn"> y_i = \beta x_i + \epsilon_i </code>
</p>
 
<p>with right censored <code class="reqn">y_i</code>. Iteration method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BJnoint(x, y, delta, beta0 = NA, maxiter=30, error = 0.00001)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BJnoint_+3A_x">x</code></td>
<td>
<p>a matrix or vector containing the covariate, one row
per observation.</p>
</td></tr>
<tr><td><code id="BJnoint_+3A_y">y</code></td>
<td>
<p>a numeric vector of length N, censored responses. </p>
</td></tr>
<tr><td><code id="BJnoint_+3A_delta">delta</code></td>
<td>
<p>a vector of length N, delta=0/1 for censored/uncensored.</p>
</td></tr>
<tr><td><code id="BJnoint_+3A_beta0">beta0</code></td>
<td>
<p>an optional vector for starting value of iteration.</p>
</td></tr>
<tr><td><code id="BJnoint_+3A_maxiter">maxiter</code></td>
<td>
<p>an optional integer to control iterations.</p>
</td></tr>
<tr><td><code id="BJnoint_+3A_error">error</code></td>
<td>
<p>an optional positive value to control iterations.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function compute the Buckley-James estimator 
when your model do not have an intercept term.
Of course, if you include a column of 1's in the x matrix, 
it is also OK with this function and it
is equivalent to having an intercept term.
If your model do have an intercept term, then you could also (probably should) use the function
<code>bj( )</code> in the <code>Design</code> library. It should be more refined 
than <code>BJnoint</code> in the stopping rule for the iterations.
</p>
<p>This function is included here mainly to produce the estimator value
that may provide some useful information with the function <code>bjtest( )</code>.
For example you may want to test a beta value near the
Buckley-James estimator. 
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>beta</code></td>
<td>
<p>the Buckley-James estimator.</p>
</td></tr>
<tr><td><code>iteration</code></td>
<td>
<p>number of iterations performed.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Mai Zhou. </p>


<h3>References</h3>

<p>Buckley, J. and James, I. (1979).  Linear regression with censored data.
<em>Biometrika</em>, <b>66</b> 429-36.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix(c(rnorm(50,mean=1), rnorm(50,mean=2)), ncol=2,nrow=50)
## Suppose now we wish to test Ho: 2mu(1)-mu(2)=0, then
y &lt;- 2*x[,1]-x[,2]
xx &lt;- c(28,-44,29,30,26,27,22,23,33,16,24,29,24,40,21,31,34,-2,25,19)
</code></pre>

<hr>
<h2 id='bjtest'>Test the Buckley-James estimator by Empirical Likelihood</h2><span id='topic+bjtest'></span>

<h3>Description</h3>

<p>Use the empirical likelihood ratio and Wilks theorem to test if the
regression coefficient is equal to beta.
</p>
<p>The log empirical likelihood been maximized is
</p>
<p style="text-align: center;"><code class="reqn"> \sum_{d=1} \log \Delta F(e_i) + \sum_{d=0} \log [1-F(e_i)];</code>
</p>

<p>where <code class="reqn">e_i</code> are the residuals.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bjtest(y, d, x, beta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bjtest_+3A_y">y</code></td>
<td>
<p>a vector of length N, containing the censored responses.</p>
</td></tr>
<tr><td><code id="bjtest_+3A_d">d</code></td>
<td>
<p>a vector (length N) of either 1's or 0's. 
d=1 means y is uncensored;
d=0 means y is right censored. </p>
</td></tr>
<tr><td><code id="bjtest_+3A_x">x</code></td>
<td>
<p>a matrix of size N by q. </p>
</td></tr>
<tr><td><code id="bjtest_+3A_beta">beta</code></td>
<td>
<p>a vector of length q. The value of the regression 
coefficient to be tested in the model 
<code class="reqn">y_i = \beta x_i  + \epsilon_i</code> </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The above likelihood should be understood as the likelihood of the 
error term, so in the regression model the error epsilon should be iid.
</p>
<p>This version can handle the model where beta is a vector (of length q).
</p>
<p>The estimation equations used when maximize the 
empirical likelihood is 
</p>
<p style="text-align: center;"><code class="reqn"> 0 = \sum d_i \Delta F(e_i) (x \cdot m[,i])/(n w_i) </code>
</p>

<p>which was described in detail in the reference below.
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>"-2LLR"</code></td>
<td>
<p>the -2 loglikelihood ratio; have approximate chisq 
distribution under <code class="reqn">H_o</code>.</p>
</td></tr>
<tr><td><code>logel2</code></td>
<td>
<p>the log empirical likelihood, under estimating equation.</p>
</td></tr>
<tr><td><code>logel</code></td>
<td>
<p>the log empirical likelihood of the Kaplan-Meier of e's.</p>
</td></tr>
<tr><td><code>prob</code></td>
<td>
<p>the probabilities that max the empirical likelihood 
under estimating equation.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Mai Zhou. </p>


<h3>References</h3>

<p>Buckley, J. and James, I. (1979). Linear regression with censored data.
<em>Biometrika</em>,  <b>66</b> 429-36.
</p>
<p>Zhou, M. and Li, G. (2008). Empirical likelihood analysis of the Buckley-James estimator. <em>Journal of Multivariate Analysis</em> <b>99</b>, 649-664. 
</p>
<p>Zhou, M. (2016) Empirical Likelihood Method in Survival Analysis. CRC Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>xx &lt;- c(28,-44,29,30,26,27,22,23,33,16,24,29,24,40,21,31,34,-2,25,19)
</code></pre>

<hr>
<h2 id='bjtest1d'>Test the Buckley-James estimator by Empirical Likelihood, 1-dim only</h2><span id='topic+bjtest1d'></span>

<h3>Description</h3>

<p>Use the empirical likelihood ratio and Wilks theorem to test if the
regression coefficient is equal to beta. For 1-dim beta only.
</p>
<p>The log empirical likelihood been maximized is
</p>
<p style="text-align: center;"><code class="reqn"> \sum_{d=1} \log \Delta F(e_i) + \sum_{d=0} \log [1-F(e_i)] .</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>bjtest1d(y, d, x, beta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bjtest1d_+3A_y">y</code></td>
<td>
<p>a vector of length N, containing the censored responses.</p>
</td></tr>
<tr><td><code id="bjtest1d_+3A_d">d</code></td>
<td>
<p>a vector of either 1's or 0's. d=1 means y is uncensored.
d=0 means y is right censored. </p>
</td></tr>
<tr><td><code id="bjtest1d_+3A_x">x</code></td>
<td>
<p>a vector of length N, covariate.</p>
</td></tr>
<tr><td><code id="bjtest1d_+3A_beta">beta</code></td>
<td>
<p>a number. the regression coefficient to 
be tested in the model y = x beta + epsilon </p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the above likelihood, <code class="reqn"> e_i = y_i - x * beta </code> is the residuals.
</p>
<p>Similar to <code>bjtest( )</code>, but only for 1-dim beta.
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>"-2LLR"</code></td>
<td>
<p>the -2 loglikelihood ratio; have approximate chi square 
distribution under <code class="reqn">H_o</code>.</p>
</td></tr>
<tr><td><code>logel2</code></td>
<td>
<p>the log empirical likelihood, under estimating equation.</p>
</td></tr>
<tr><td><code>logel</code></td>
<td>
<p>the log empirical likelihood of the Kaplan-Meier of e's.</p>
</td></tr>
<tr><td><code>prob</code></td>
<td>
<p>the probabilities that max the empirical likelihood 
under estimating equation constraint.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Mai Zhou.</p>


<h3>References</h3>

<p>Buckley, J. and James, I. (1979). Linear regression with censored data.
Biometrika,  <b>66</b> 429-36.
</p>
<p>Owen, A. (1990). Empirical likelihood ratio confidence regions. 
Ann. Statist. <b>18</b> 90-120.
</p>
<p>Zhou, M. and Li, G. (2008). Empirical likelihood analysis 
of the Buckley-James estimator. Journal of Multivariate Analysis.
649-664.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>xx &lt;- c(28,-44,29,30,26,27,22,23,33,16,24,29,24,40,21,31,34,-2,25,19)
</code></pre>

<hr>
<h2 id='bjtestII'>Alternative test of the Buckley-James estimator by Empirical Likelihood</h2><span id='topic+bjtestII'></span>

<h3>Description</h3>

<p>Use the empirical likelihood ratio (alternative form) and Wilks theorem to test if the
regression coefficient is equal to beta, based on the estimating equations.
</p>
<p>The log empirical likelihood been maximized is
</p>
<p style="text-align: center;"><code class="reqn"> \sum_{j=1}^n \log p_j ; ~  \sum p_j =1 </code>
</p>

<p>where the probability <code class="reqn">p_j</code> is for the jth martingale differences of the estimating equations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bjtestII(y, d, x, beta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bjtestII_+3A_y">y</code></td>
<td>
<p>a vector of length N, containing the censored responses.</p>
</td></tr>
<tr><td><code id="bjtestII_+3A_d">d</code></td>
<td>
<p>a vector of length N. Either 1's or 0's. 
d=1 means y is uncensored;
d=0 means y is right censored. </p>
</td></tr>
<tr><td><code id="bjtestII_+3A_x">x</code></td>
<td>
<p>a matrix of size N by q. </p>
</td></tr>
<tr><td><code id="bjtestII_+3A_beta">beta</code></td>
<td>
<p>a vector of length q. The value of the regression 
coefficient to be tested in the model 
<code class="reqn">Y_i = \beta x_i  + \epsilon_i</code> </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The above likelihood should be understood as the likelihood of the martingale difference terms. For the definition of the Buckley-James martingale or estimating equation, please see the (2015) book
in the reference list.
</p>
<p>The estimation equations used when maximize the 
empirical likelihood is 
</p>
<p style="text-align: center;"><code class="reqn"> 0 = \sum d_i \Delta F(e_i) (x \cdot m[,i])/(n w_i) </code>
</p>

<p>where <code class="reqn">e_i</code> is the residuals, other details are described in the reference book of 2015 below.
</p>
<p>The final test is carried out by <code>el.test</code>. So the output is similar to the output of <code>el.test</code>.
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>"-2LLR"</code></td>
<td>
<p>the -2 loglikelihood ratio; have approximate chisq 
distribution under <code class="reqn">H_o</code>.</p>
</td></tr>
<tr><td><code>logel2</code></td>
<td>
<p>the log empirical likelihood, under estimating equation.</p>
</td></tr>
<tr><td><code>logel</code></td>
<td>
<p>the log empirical likelihood of the Kaplan-Meier of e's.</p>
</td></tr>
<tr><td><code>prob</code></td>
<td>
<p>the probabilities that max the empirical likelihood 
under estimating equation.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Mai Zhou. </p>


<h3>References</h3>

<p>Zhou, M. (2016) Empirical Likelihood Methods in Survival
Analysis. CRC Press.
</p>
<p>Buckley, J. and James, I. (1979). Linear regression with censored data.
Biometrika,  <b>66</b> 429-36.
</p>
<p>Zhou, M. and Li, G. (2008). Empirical likelihood analysis 
of the Buckley-James estimator.
Journal of Multivariate Analysis, <b>99</b>,  649&ndash;664.
</p>
<p>Zhu, H. (2007) Smoothed Empirical Likelihood for Quantiles
and Some Variations/Extension of Empirical Likelihood for Buckley-James Estimator,
Ph.D. dissertation, University of Kentucky. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(myeloma)
bjtestII(y=myeloma[,1], d=myeloma[,2], x=cbind(1, myeloma[,3]), beta=c(37, -3.4))
</code></pre>

<hr>
<h2 id='el.cen.EM'>Empirical likelihood ratio for mean 
with right, left or doubly censored data, by EM algorithm</h2><span id='topic+el.cen.EM'></span>

<h3>Description</h3>

<p>This program uses EM algorithm to compute the maximized 
(wrt <code class="reqn">p_i</code>) empirical
log likelihood function for right, left or doubly censored data with 
the MEAN constraint:
</p>
<p style="text-align: center;"><code class="reqn"> \sum_{d_i=1}  p_i f(x_i)  = \int f(t) dF(t) = \mu . </code>
</p>

<p>Where <code class="reqn">p_i = \Delta F(x_i)</code> is a probability,
<code class="reqn">d_i</code> is the censoring indicator, 1(uncensored), 0(right censored),
2(left censored). 
It also returns those <code class="reqn">p_i</code>. 
</p>
<p>The empirical log likelihood been maximized is
</p>
<p style="text-align: center;"><code class="reqn"> \sum_{d_i=1} \log \Delta F(x_i) + \sum_{d_i=0} \log [1-F(x_i)] 
    + \sum_{d_i=2}  \log F(x_i) . </code>
</p>
 


<h3>Usage</h3>

<pre><code class='language-R'>el.cen.EM(x,d,wt=rep(1,length(d)),fun=function(t){t},mu,maxit=50,error=1e-9,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="el.cen.EM_+3A_x">x</code></td>
<td>
<p>a vector containing the observed survival times.</p>
</td></tr>
<tr><td><code id="el.cen.EM_+3A_d">d</code></td>
<td>
<p>a vector containing the censoring indicators, 
1-uncensored; 0-right censored; 2-left censored.</p>
</td></tr>
<tr><td><code id="el.cen.EM_+3A_wt">wt</code></td>
<td>
<p>a weight vector (case weight). positive. same length as d</p>
</td></tr>
<tr><td><code id="el.cen.EM_+3A_fun">fun</code></td>
<td>
<p>a left continuous (weight) function used to calculate
the mean as in <code class="reqn">H_0</code>.
<code>fun(t)</code> must be able to take a vector input <code>t</code>.
Default to the identity function <code class="reqn">f(t)=t</code>.</p>
</td></tr> 
<tr><td><code id="el.cen.EM_+3A_mu">mu</code></td>
<td>
<p>a real number used in the constraint, the mean value of <code class="reqn">f(X)</code>.</p>
</td></tr>
<tr><td><code id="el.cen.EM_+3A_maxit">maxit</code></td>
<td>
<p>an optional integer, used to control maximum number of
iterations. </p>
</td></tr>
<tr><td><code id="el.cen.EM_+3A_error">error</code></td>
<td>
<p>an optional positive real number specifying the tolerance of
iteration error. This is the bound of the
<code class="reqn">L_1</code> norm of the difference of two successive weights.</p>
</td></tr>
<tr><td><code id="el.cen.EM_+3A_...">...</code></td>
<td>
<p>additional arguments, if any, to pass to <code>fun</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This implementation is all in R and have several for-loops in it. 
A faster version would use C to do the for-loop part.
But this version seems faster enough and is easier to port to Splus.
</p>
<p>We return the log likelihood all the time. Sometimes, (for right censored
and no censor case) we also return the -2 log likelihood ratio.
In other cases, you have to plot a curve with many values of the 
parameter, mu, to
find out where is the place the log likelihood becomes maximum.
And from there you can get -2 log likelihood ratio between
the maximum location and your current parameter in Ho.
</p>
<p>In order to get a proper distribution as NPMLE, we automatically
change the <code class="reqn">d</code> for the largest observation to 1
(even if it is right censored), similar for the left censored, 
smallest observation.
<code class="reqn">\mu</code> is a given constant. 
When the given constants <code class="reqn">\mu</code> is too far
away from the NPMLE, there will be no distribution
satisfy the constraint.
In this case the computation will stop.
The -2 Log empirical likelihood ratio
should be infinite. 
</p>
<p>The constant <code>mu</code> must be inside 
<code class="reqn">( \min f(x_i) , \max f(x_i) ) </code>
for the computation to continue. 
It is always true that the NPMLE values are feasible. So when the
computation stops, try move the <code>mu</code> closer
to the NPMLE &mdash; 
</p>
<p style="text-align: center;"><code class="reqn"> \sum_{d_i=1} p_i^0 f(x_i) </code>
</p>
 
<p><code class="reqn">p_i^0</code> taken to be the jumps of the NPMLE of CDF. 
Or use a different <code>fun</code>. 
</p>
<p>Difference to the function <code>el.cen.EM2</code>: here duplicate (input) observations 
are collapsed (with weight 2, 3, ... etc.) but those
will stay separate by default in the <code>el.cen.EM2</code>. This will lead to
a different <code>loglik</code> value. But the <code>-2LLR</code> value should be same
in either version. 
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>loglik</code></td>
<td>
<p>the maximized empirical log likelihood under the constraint. This may be different from the result of el.cen.EM2
because here the tied observations are collapes into 1 with weight. (while el.cen.EM2 do not). 
However, the -2LLR should be the same.</p>
</td></tr>
<tr><td><code>times</code></td>
<td>
<p>locations of CDF that have positive mass. tied obs. are collapesd</p>
</td></tr>
<tr><td><code>prob</code></td>
<td>
<p>the jump size of CDF at those locations.</p>
</td></tr>
<tr><td><code>"-2LLR"</code></td>
<td>
<p>If available, it is Minus two times the 
Empirical Log Likelihood Ratio.
Should be approximately chi-square distributed under Ho.</p>
</td></tr>
<tr><td><code>Pval</code></td>
<td>
<p>The P-value of the test, using chi-square approximation.</p>
</td></tr>
<tr><td><code>lam</code></td>
<td>
<p>The Lagrange multiplier. Added 5/2007.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Mai Zhou </p>


<h3>References</h3>

<p>Zhou, M. (2005). Empirical likelihood ratio with arbitrary censored/truncated data by EM algorithm. 
<em>Journal of Computational and Graphical Statistics</em>, 643-656.
</p>
<p>Murphy, S. and van der Vaart (1997)
Semiparametric likelihood ratio inference.
<em>Ann. Statist.</em> <b> 25</b>, 1471-1509.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## example with tied observations
x &lt;- c(1, 1.5, 2, 3, 4, 5, 6, 5, 4, 1, 2, 4.5)
d &lt;- c(1,   1, 0, 1, 0, 1, 1, 1, 1, 0, 0,   1)
el.cen.EM(x,d,mu=3.5)
## we should get "-2LLR" = 1.2466....
myfun5 &lt;- function(x, theta, eps) {
u &lt;- (x-theta)*sqrt(5)/eps 
INDE &lt;- (u &lt; sqrt(5)) &amp; (u &gt; -sqrt(5)) 
u[u &gt;= sqrt(5)] &lt;- 0 
u[u &lt;= -sqrt(5)] &lt;- 1 
y &lt;- 0.5 - (u - (u)^3/15)*3/(4*sqrt(5)) 
u[ INDE ] &lt;- y[ INDE ] 
return(u)
}
el.cen.EM(x, d, fun=myfun5, mu=0.5, theta=3.5, eps=0.1)
## example of using wt in the input. Since the x-vector contain
## two 5 (both d=1), and two 2(both d=0), we can also do
xx &lt;- c(1, 1.5, 2, 3, 4, 5, 6, 4, 1, 4.5)
dd &lt;- c(1,   1, 0, 1, 0, 1, 1, 1, 0,   1)
wt &lt;- c(1,   1, 2, 1, 1, 2, 1, 1, 1,   1)
el.cen.EM(x=xx, d=dd, wt=wt, mu=3.5)
## this should be the same as the first example.
</code></pre>

<hr>
<h2 id='el.cen.EM2'>Empirical likelihood ratio test for a vector of means 
with right, left or doubly censored data, by EM algorithm</h2><span id='topic+el.cen.EM2'></span>

<h3>Description</h3>

<p>This function is similar to <code>el.cen.EM()</code>, but for multiple constraints.
In the input there is a vector of observations 
<code class="reqn">x = (x_1, \cdots , x_n)</code> and a 
function <code>fun</code>. The function <code>fun</code> should return the 
(n by k) matrix
</p>
<p style="text-align: center;"><code class="reqn">
          ( f_1(x), f_2(x), \cdots, f_k (x) ) . 
</code>
</p>

<p>Also, the ordering of the observations, when consider censoring or 
redistributing-to-the-right, 
is according to the value of <code>x</code>, not <code>fun(x)</code>. 
So the probability distribution is for values <code>x</code>.
This program uses EM algorithm to maximize 
(wrt <code class="reqn">p_i</code>) empirical
log likelihood function for right, left or doubly censored data with 
the MEAN constraint:
</p>
<p style="text-align: center;"><code class="reqn"> j = 1,2, \cdots ,k ~~~~ 
   \sum_{d_i=1} p_i f_j(x_i) = \int f_j(t) dF(t) = \mu_j ~. </code>
</p>

<p>Where <code class="reqn">p_i = \Delta F(x_i)</code> is a probability,
<code class="reqn">d_i</code> is the censoring indicator, 1(uncensored), 0(right censored),
2(left censored). 
It also returns those <code class="reqn">p_i</code>. 
The log likelihood function is defined as
</p>
<p style="text-align: center;"><code class="reqn"> \sum_{d_i=1} \log \Delta F(x_i)  + \sum_{d_i=2} \log F(x_i) 
     + \sum_{d_i=0} \log [ 1-F(x_i)] ~.</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>el.cen.EM2(x,d,xc=1:length(x),fun,mu,maxit=50,error=1e-9,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="el.cen.EM2_+3A_x">x</code></td>
<td>
<p>a vector containing the observed survival times.</p>
</td></tr>
<tr><td><code id="el.cen.EM2_+3A_d">d</code></td>
<td>
<p>a vector containing the censoring indicators, 
1-uncensored; 0-right censored; 2-left censored.</p>
</td></tr>
<tr><td><code id="el.cen.EM2_+3A_xc">xc</code></td>
<td>
<p>an optional vector of collapsing control values. 
If xc[i] xc[j] have different values then 
(x[i], d[i]), (x[j], d[j]) will not merge into one 
observation with weight two, even 
if they are identical. Default is not to merge.</p>
</td></tr>
<tr><td><code id="el.cen.EM2_+3A_fun">fun</code></td>
<td>
<p>a left continuous (weight) function that returns a matrix. 
The columns (=k) of the matrix is used to calculate
the means and will be tested in <code class="reqn">H_0</code>.
<code>fun(t)</code> must be able to take a vector input <code>t</code>.</p>
</td></tr> 
<tr><td><code id="el.cen.EM2_+3A_mu">mu</code></td>
<td>
<p>a vector of length k. Used in the constraint, 
as the mean of <code class="reqn">f(X)</code>.</p>
</td></tr>
<tr><td><code id="el.cen.EM2_+3A_maxit">maxit</code></td>
<td>
<p>an optional integer, used to control maximum number of
iterations. </p>
</td></tr>
<tr><td><code id="el.cen.EM2_+3A_error">error</code></td>
<td>
<p>an optional positive real number specifying the tolerance of
iteration error. This is the bound of the
<code class="reqn">L_1</code> norm of the difference of two successive weights.</p>
</td></tr>
<tr><td><code id="el.cen.EM2_+3A_...">...</code></td>
<td>
<p>additional inputs to pass to <code>fun()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This implementation is all in R and have several for-loops in it. 
A faster version would use C to do the for-loop part.
(but this version is easier to port to Splus, and seems faster enough). 
</p>
<p>We return the log likelihood all the time. Sometimes, (for right censored
and no censor case) we also return the -2 log likelihood ratio.
In other cases, you have to plot a curve with many values of the 
parameter, mu, to
find out where the log likelihood becomes maximum.
And from there you can get -2 log likelihood ratio between
the maximum location and your current parameter in Ho.
</p>
<p>In order to get a proper distribution as NPMLE, we automatically
change the <code class="reqn">d</code> for the largest observation to 1
(even if it is right censored), similar for the left censored, 
smallest observation.
<code class="reqn">\mu</code> is a given constant vector. 
When the given constants <code class="reqn">\mu</code> is too far
away from the NPMLE, there will be no distribution
satisfy the constraint.
In this case the computation will stop.
The -2 Log empirical likelihood ratio
should be infinite. 
</p>
<p>The constant vector <code>mu</code> must be inside 
<code class="reqn">( \min f(x_i) , \max f(x_i) ) </code>
for the computation to continue. 
It is always true that the NPMLE values are feasible. So when the
computation stops, try move the <code>mu</code> closer
to the NPMLE &mdash; 
</p>
<p style="text-align: center;"><code class="reqn"> \hat \mu _j = \sum_{d_i=1} p_i^0 f_j(x_i) </code>
</p>
 
<p>where <code class="reqn">p_i^0</code> taken to be the jumps of the NPMLE of CDF. 
Or use a different <code>fun</code>. 
</p>
<p>Difference to the function <code>el.cen.EM</code>: due to the introduction of
input <code>xc</code> here in this function, the output <code>loglik</code> may be different
compared to the function <code>el.cen.EM</code>
due to not collapsing of duplicated input survival values.
The <code>-2LLR</code> should be the same from both functions.
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>loglik</code></td>
<td>
<p>the maximized empirical log likelihood under the constraints.</p>
</td></tr>
<tr><td><code>times</code></td>
<td>
<p>locations of CDF that have positive mass.</p>
</td></tr>
<tr><td><code>prob</code></td>
<td>
<p>the jump size of CDF at those locations.</p>
</td></tr>
<tr><td><code>"-2LLR"</code></td>
<td>
<p>If available, it is Minus two times the 
Empirical Log Likelihood Ratio.
Should be approx. chi-square distributed under Ho.</p>
</td></tr>
<tr><td><code>Pval</code></td>
<td>
<p>If available, the P-value of the test, 
using chi-square approximation.</p>
</td></tr>
<tr><td><code>lam</code></td>
<td>
<p>the Lagrange multiplier in the final EM step. (the M-step)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Mai Zhou </p>


<h3>References</h3>

<p>Zhou, M. (2005). Empirical likelihood ratio with arbitrary censored/truncated data by EM algorithm. 
<em>Journal of Computational and Graphical Statistics</em>, 643-656.
</p>
<p>Zhou, M. (2002). 
Computing censored empirical likelihood ratio 
by EM algorithm. 
<em>Tech Report, Univ. of Kentucky, Dept of Statistics</em>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## censored regression with one right censored observation.
## we check the estimation equation, with the MLE inside myfun7. 
y &lt;- c(3, 5.3, 6.4, 9.1, 14.1, 15.4, 18.1, 15.3, 14, 5.8, 7.3, 14.4)
x &lt;- c(1, 1.5, 2,   3,   4,    5,    6,    5,    4,  1,   2,   4.5)
d &lt;- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0)
### first we estimate beta, the MLE
lm.wfit(x=cbind(rep(1,12),x), y=y, w=WKM(x=y, d=d)$jump[rank(y)])$coef
## you should get 1.392885 and 2.845658
## then define myfun7 with the MLE value
myfun7 &lt;- function(y, xmat) {
temp1 &lt;- y - ( 1.392885 +  2.845658 * xmat)
return( cbind( temp1, xmat*temp1) )
}
## now test 
el.cen.EM2(y,d, fun=myfun7, mu=c(0,0), xmat=x)
## we should get, Pval = 1 , as the MLE should.
## for other values of (a, b) inside myfun7, you get other Pval
##
rqfun1 &lt;- function(y, xmat, beta, tau = 0.5) {
temp1 &lt;- tau - (1-myfun55(y-beta*xmat))
return(xmat * temp1)
}
myfun55 &lt;- function(x, eps=0.001){
u &lt;- x*sqrt(5)/eps
INDE &lt;- (u &lt; sqrt(5)) &amp; (u &gt; -sqrt(5))
u[u &gt;= sqrt(5)] &lt;- 0
u[u &lt;= -sqrt(5)] &lt;- 1
y &lt;- 0.5 - (u - (u)^3/15)*3/(4*sqrt(5))
u[ INDE ] &lt;- y[ INDE ]
return(u)
}
## myfun55 is a smoothed indicator fn. 
## eps should be between (1/sqrt(n), 1/n^0.75) [Chen and Hall]
el.cen.EM2(x=y,d=d,xc=1:12,fun=rqfun1,mu=0,xmat=x,beta=3.08,tau=0.44769875)
## default tau=0.5 
el.cen.EM2(x=y,d=d,xc=1:12,fun=rqfun1,mu=0,xmat=x,beta=3.0799107404)
###################################################
### next 2 examples are testing the mean/median residual time
###################################################
mygfun &lt;- function(s, age, muage) {as.numeric(s &gt;= age)*(s-(age+muage))}
mygfun2 &lt;- function(s, age, Mdage) 
          {as.numeric(s &lt;= (age+Mdage)) - 0.5*as.numeric(s &lt;= age)}
## Not run: 
library(survival) 
time &lt;- cancer$time
status &lt;- cancer$status-1
###for mean residual time 
el.cen.EM2(x=time, d=status, fun=mygfun, mu=0, age=365.25, muage=234)$Pval
el.cen.EM2(x=time, d=status, fun=mygfun, mu=0, age=365.25, muage=323)$Pval
### for median resudual time
el.cen.EM2(x=time, d=status, fun=mygfun2, mu=0.5, age=365.25, Mdage=184)$Pval
el.cen.EM2(x=time, d=status, fun=mygfun2, mu=0.5, age=365.25, Mdage=321)$Pval

## End(Not run)
## Not run: 
#### For right censor only data (Kaplan-Meier) we can use this function to get a faster computation
#### by calling the kmc 0.2-2 package.
el.cen.R &lt;- function (x, d, xc = 1:length(x), fun, mu, error = 1e-09, ...)
{
xvec &lt;- as.vector(x)
d &lt;- as.vector(d)
mu &lt;- as.vector(mu)
xc &lt;- as.vector(xc)
n &lt;- length(d)
if (length(xvec) != n)
stop("length of d and x must agree")
if (length(xc) != n)
stop("length of xc and d must agree")
if (n &lt;= 2 * length(mu) + 1)
stop("Need more observations")
if (any((d != 0) &amp; (d != 1) ))
stop("d must be 0(right-censored) or 1(uncensored)")
if (!is.numeric(xvec))
stop("x must be numeric")
if (!is.numeric(mu))
stop("mu must be numeric")

funx &lt;- as.matrix(fun(xvec, ...))
pp &lt;- ncol(funx)
if (length(mu) != pp)
stop("length of mu and ncol of fun(x) must agree")
temp &lt;- Wdataclean5(z = xvec, d, zc = xc, xmat = funx)
x &lt;- temp$value
d &lt;- temp$dd
w &lt;- temp$weight
funx &lt;- temp$xxmat
d[length(d)] &lt;- 1
xd1 &lt;- x[d == 1]
if (length(xd1) &lt;= 1)
stop("need more distinct uncensored obs.")
funxd1 &lt;- funx[d == 1, ]
xd0 &lt;- x[d == 0]
wd1 &lt;- w[d == 1]
wd0 &lt;- w[d == 0]
m &lt;- length(xd0)

pnew &lt;- NA
num &lt;- NA
if (m &gt; 0) {
gfun &lt;- function(x) { return( fun(x, ...) - mu ) }
temp &lt;- kmc.solve(x=x, d=d, g=list(gfun))
logel &lt;- temp$loglik.h0
logel00 &lt;- temp$loglik.null
lam &lt;- - temp$lambda
}
if (m == 0) {
num &lt;- 0
temp6 &lt;- el.test.wt2(x = funxd1, wt = wd1, mu)
pnew &lt;- temp6$prob
lam &lt;- temp6$lambda
logel &lt;- sum(wd1 * log(pnew))
logel00 &lt;- sum(wd1 * log(wd1/sum(wd1)))
}
tval &lt;- 2 * (logel00 - logel)
list(loglik = logel, times = xd1, prob = pnew, lam = lam,
iters = num, `-2LLR` = tval, Pval = 1 - pchisq(tval,
df = length(mu)))
}


## End(Not run)
</code></pre>

<hr>
<h2 id='el.cen.kmc1d'>Empirical likelihood ratio for 1 mean constraint 
with right censored data</h2><span id='topic+el.cen.kmc1d'></span>

<h3>Description</h3>

<p>This program uses a fast recursive formula to compute the maximized 
(wrt <code class="reqn">p_i</code>) empirical
log likelihood ratio for right censored data with 
one MEAN constraint:
</p>
<p style="text-align: center;"><code class="reqn"> \sum_{d_i=1}  p_i f(x_i)  = \int f(t) dF(t) = \mu . </code>
</p>

<p>Where <code class="reqn">p_i = \Delta F(x_i)</code> is a probability,
<code class="reqn">d_i</code> is the censoring indicator, 1(uncensored), 0(right censored). 
It also returns those <code class="reqn">p_i</code>. 
</p>
<p>The empirical log likelihood been maximized is
</p>
<p style="text-align: center;"><code class="reqn"> \sum_{d_i=1} \log \Delta F(x_i) + \sum_{d_i=0} \log [1-F(x_i)] . </code>
</p>
 


<h3>Usage</h3>

<pre><code class='language-R'>el.cen.kmc1d(x, d, fun, mu, tol = .Machine$double.eps^0.5, step=0.001, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="el.cen.kmc1d_+3A_x">x</code></td>
<td>
<p>a vector containing the observed survival times.</p>
</td></tr>
<tr><td><code id="el.cen.kmc1d_+3A_d">d</code></td>
<td>
<p>a vector containing the censoring indicators, 
1-uncensored; 0-right censored.</p>
</td></tr>
<tr><td><code id="el.cen.kmc1d_+3A_fun">fun</code></td>
<td>
<p>a left continuous (weight) function used to calculate
the mean as in <code class="reqn">H_0</code>.
<code>fun(t)</code> must be able to take a vector input <code>t</code>.
</p>
</td></tr> 
<tr><td><code id="el.cen.kmc1d_+3A_mu">mu</code></td>
<td>
<p>a real number used in the constraint, the mean value of <code class="reqn">f(X)</code>.</p>
</td></tr>
<tr><td><code id="el.cen.kmc1d_+3A_tol">tol</code></td>
<td>
<p>a small positive number, for the uniroot error tol.</p>
</td></tr>
<tr><td><code id="el.cen.kmc1d_+3A_step">step</code></td>
<td>
<p>a small positive number, for use in the uniroot function (as interval) to find lambda root. Sometimes 
uniroot will find the wrong root or no root, resulting a negative &quot;-2LLR&quot; or NA. Change the step to a different 
value often can fix this (but not always). Another sign of wrong root is that
the sum of probabilities not sum to one, or has negative probability values.</p>
</td></tr>
<tr><td><code id="el.cen.kmc1d_+3A_...">...</code></td>
<td>
<p>additional arguments, if any, to pass to fun.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is similar to the function in package <code>kmc</code>, but much simpler, i.e.
all implemented in R and only for one mean.
This implementation have two for-loops in R. 
A faster version would use C to do the for-loop part.
But this version seems fast enough and is easier to port to Splus.
</p>
<p>We return the log likelihood all the time. Sometimes, (for right censored case) 
we also return the -2 log likelihood ratio.
In other cases, you have to plot a curve with many values of the 
parameter, mu, to
find out where is the place the log likelihood becomes maximum.
And from there you can get -2 log likelihood ratio between
the maximum location and your current parameter in Ho.
</p>
<p>The input <code>step</code> is used in uniroot function to find a root of lambda. Sometimes
a step value may lead to no root or result in a wrong root. You may try several values 
for the step to see. If the probabilities returned do not sum to one, then the lambda root is a wrong root.
We want the root closest to zero.
</p>
<p>In order to get a proper distribution as NPMLE, we automatically
change the <code class="reqn">d</code> for the largest observation to 1
(even if it is right censored).
<code class="reqn">\mu</code> is a given constant. 
When the given constants <code class="reqn">\mu</code> is too far
away from the NPMLE, there will be no distribution
satisfy the constraint.
In this case the computation will stop or return something ridiculas, (as negative -2LLR).
The -2 Log empirical likelihood ratio
may be +infinite. 
</p>
<p>The constant <code>mu</code> must be inside 
<code class="reqn">( \min f(x_i) , \max f(x_i) ) </code> (with uncensored <code class="reqn">x_i</code>)
for the computation to continue. 
It is always true that the NPMLE values are feasible. So when the
computation stops, try move the <code>mu</code> closer
to the NPMLE &mdash; 
</p>
<p style="text-align: center;"><code class="reqn"> \sum_{d_i=1} p_i^0 f(x_i) </code>
</p>
 
<p><code class="reqn">p_i^0</code> taken to be the jumps of the NPMLE of CDF. 
Or use a different <code>fun</code>. 
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>loglik</code></td>
<td>
<p>the maximized empirical log likelihood under the constraint. Note, here the tied observations are
not collapsed into one obs. with weight 2 (as in el.cen.EM), so the value may differ from those
that do collapse the tied obs. In any case, the -2LLR should not differ (whether collaps or not).</p>
</td></tr>
<tr><td><code>times</code></td>
<td>
<p>locations of CDF that have positive mass.</p>
</td></tr>
<tr><td><code>prob</code></td>
<td>
<p>the jump size of CDF at those locations.</p>
</td></tr>
<tr><td><code>"-2LLR"</code></td>
<td>
<p>If available, it is minus two times the 
empirical Log Likelihood Ratio.
Should be approximately chi-square distributed under Ho. If you got NA or negative value, then something is wrong,
most likely the uniroot has found the wrong root. Suggest: use el.cen.EM2() which uses EM algorithm. It is more stable but slower. </p>
</td></tr>
<tr><td><code>Pval</code></td>
<td>
<p>The P-value of the test, using chi-square approximation.</p>
</td></tr>
<tr><td><code>lam</code></td>
<td>
<p>The Lagrange multiplier.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Mai Zhou </p>


<h3>References</h3>

<p>Zhou, M. and Yang, Y. (2015). A recursive formula for the Kaplan-Meier estimator with mean constraints 
and its application to empirical likelihood.
<em>Computational Statistics</em>  Vol. 30, Issue 4 pp. 1097-1109.
</p>
<p>Zhou, M. (2005). Empirical likelihood ratio with arbitrary censored/truncated data by EM algorithm. 
<em>Journal of Computational and Graphical Statistics</em>, 14(3), 643-656.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c(1, 1.5, 2, 3, 4.2, 5, 6.1, 5.3, 4.5, 0.9, 2.1, 4.3)
d &lt;- c(1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1)
ff &lt;- function(x) {
    x - 3.7
}
el.cen.kmc1d(x=x, d=d, fun=ff, mu=0)
#######################################
## example with tied observations
x &lt;- c(1, 1.5, 2, 3, 4, 5, 6, 5, 4, 1, 2, 4.5)
d &lt;- c(1,   1, 0, 1, 0, 1, 1, 1, 1, 0, 0,   1)
el.cen.EM(x,d,mu=3.5)
## we should get "-2LLR" = 1.2466....
myfun5 &lt;- function(x, theta, eps) {
u &lt;- (x-theta)*sqrt(5)/eps 
INDE &lt;- (u &lt; sqrt(5)) &amp; (u &gt; -sqrt(5)) 
u[u &gt;= sqrt(5)] &lt;- 0 
u[u &lt;= -sqrt(5)] &lt;- 1 
y &lt;- 0.5 - (u - (u)^3/15)*3/(4*sqrt(5)) 
u[ INDE ] &lt;- y[ INDE ] 
return(u)
}
el.cen.EM(x, d, fun=myfun5, mu=0.5, theta=3.5, eps=0.1)
## example of using wt in the input. Since the x-vector contain
## two 5 (both d=1), and two 2(both d=0), we can also do
xx &lt;- c(1, 1.5, 2, 3, 4, 5, 6, 4, 1, 4.5)
dd &lt;- c(1,   1, 0, 1, 0, 1, 1, 1, 0,   1)
wt &lt;- c(1,   1, 2, 1, 1, 2, 1, 1, 1,   1)
el.cen.EM(x=xx, d=dd, wt=wt, mu=3.5)
## this should be the same as the first example.
</code></pre>

<hr>
<h2 id='el.cen.test'>Empirical likelihood ratio for mean
with right censored data, by QP.</h2><span id='topic+el.cen.test'></span>

<h3>Description</h3>

<p>This program computes the maximized (wrt <code class="reqn">p_i</code>) empirical
log likelihood function for right censored data with 
the MEAN constraint:
</p>
<p style="text-align: center;"><code class="reqn"> \sum_i [ d_i p_i g(x_i) ] = \int g(t) dF(t) = \mu  </code>
</p>
 
<p>where <code class="reqn">p_i = \Delta F(x_i)</code> is a probability,
<code class="reqn">d_i</code> is the censoring indicator. 
The <code class="reqn">d</code> for the largest observation is always taken to be 1.
It then computes the -2 log 
empirical likelihood ratio which should be approximately chi-square
distributed if the constraint is true.
Here <code class="reqn">F(t)</code> is the (unknown) CDF; 
<code class="reqn">g(t)</code> can be any given left
continuous function in <code class="reqn">t</code>.
<code class="reqn">\mu</code> is a given constant. 
The data must contain some right censored observations.
If there is no censoring or the only censoring is the largest 
observation, the code will stop and we should use 
<code>el.test( )</code> which is for uncensored data.  
</p>
<p>The log empirical likelihood been maximized is
</p>
<p style="text-align: center;"><code class="reqn"> \sum_{d_i=1} \log \Delta F(x_i) + \sum_{d_i=0} \log [ 1-F(x_i) ].</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>el.cen.test(x,d,fun=function(x){x},mu,error=1e-8,maxit=15)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="el.cen.test_+3A_x">x</code></td>
<td>
<p>a vector containing the observed survival times.</p>
</td></tr>
<tr><td><code id="el.cen.test_+3A_d">d</code></td>
<td>
<p>a vector containing the censoring indicators, 
1-uncensor; 0-censor.</p>
</td></tr>
<tr><td><code id="el.cen.test_+3A_fun">fun</code></td>
<td>
<p>a left continuous (weight) function used to calculate
the mean as in <code class="reqn">H_0</code>. 
<code>fun(t)</code> must be able to take a vector input <code>t</code>. 
Default to the identity function <code class="reqn">f(t)=t</code>.</p>
</td></tr>
<tr><td><code id="el.cen.test_+3A_mu">mu</code></td>
<td>
<p>a real number used in the constraint, sum to this value.</p>
</td></tr>
<tr><td><code id="el.cen.test_+3A_error">error</code></td>
<td>
<p>an optional positive real number specifying the tolerance of
iteration error in the QP. This is the bound of the
<code class="reqn">L_1</code> norm of the difference of two successive weights.</p>
</td></tr>
<tr><td><code id="el.cen.test_+3A_maxit">maxit</code></td>
<td>
<p>an optional integer, used to control maximum number of
iterations. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>When the given constants <code class="reqn">\mu</code> is too far
away from the NPMLE, there will be no distribution
satisfy the constraint.
In this case the computation will stop.
The -2 Log empirical likelihood ratio
should be infinite. 
</p>
<p>The constant <code>mu</code> must be inside 
<code class="reqn">( \min f(x_i) , \max f(x_i) ) </code>
for the computation to continue. 
It is always true that the NPMLE values are feasible. So when the
computation cannot continue, try move the <code>mu</code> closer
to the NPMLE, or use a different <code>fun</code>. 
</p>
<p>This function depends on Wdataclean2(), WKM() and solve3.QP() 
</p>
<p>This function uses sequential Quadratic Programming to find the
maximum. Unlike other functions in this package,
it can be slow for larger sample sizes.
It took about one minute for a sample of size 2000 with 20% censoring
on a 1GHz, 256MB PC, about 19 seconds on a 3 GHz 512MB PC.
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>"-2LLR"</code></td>
<td>
<p>The -2Log Likelihood ratio.</p>
</td></tr>
<tr><td><code>xtimes</code></td>
<td>
<p>the location of the CDF jumps.</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>the jump size of CDF at those locations.</p>
</td></tr>
<tr><td><code>Pval</code></td>
<td>
<p>P-value</p>
</td></tr>
<tr><td><code>error</code></td>
<td>
<p>the <code class="reqn">L_1</code> norm between the last two <code>wts</code>.</p>
</td></tr>
<tr><td><code>iteration</code></td>
<td>
<p>number of iterations carried out</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Mai Zhou, Kun Chen </p>


<h3>References</h3>

<p>Pan, X. and Zhou, M. (1999). Using 1-parameter sub-family 
of distributions in empirical likelihood ratio with 
censored data.
<em>J. Statist. Plann. Inference</em>. <b>75</b>, 379-392.
</p>
<p>Chen, K. and Zhou, M. (2000). 
Computing censored empirical likelihood ratio 
using Quadratic Programming. 
<em>Tech Report, Univ. of Kentucky, Dept of Statistics</em>
</p>
<p>Zhou, M. and Chen, K. (2007). Computation of the empirical
likelihood ratio from censored data.
<em>Journal of Statistical Computing and Simulation</em>, 
<b>77</b>, 1033-1042. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>el.cen.test(rexp(100), c(rep(0,25),rep(1,75)), mu=1.5)
## second example with tied observations
x &lt;- c(1, 1.5, 2, 3, 4, 5, 6, 5, 4, 1, 2, 4.5)
d &lt;- c(1,   1, 0, 1, 0, 1, 1, 1, 1, 0, 0,   1)
el.cen.test(x,d,mu=3.5)
# we should get  "-2LLR" = 1.246634  etc. 
</code></pre>

<hr>
<h2 id='el.ltrc.EM'>Empirical likelihood ratio for mean 
with left truncated and right censored data, by EM algorithm</h2><span id='topic+el.ltrc.EM'></span>

<h3>Description</h3>

<p>This program uses EM algorithm to compute the maximized 
(wrt <code class="reqn">p_i</code>) empirical
log likelihood function for left truncated and right censored data with 
the MEAN constraint:
</p>
<p style="text-align: center;"><code class="reqn"> \sum_{d_i=1}  p_i f(x_i)  = \int f(t) dF(t) = \mu ~. </code>
</p>

<p>Where <code class="reqn">p_i = \Delta F(x_i)</code> is a probability,
<code class="reqn">d_i</code> is the censoring indicator, 1(uncensored), 0(right censored). 
The <code class="reqn">d</code> for the largest observation <code class="reqn">x</code>, is always (automatically)
changed to 1.  <code class="reqn">\mu</code> is a given constant. 
This function also returns those <code class="reqn">p_i</code>. 
</p>
<p>The log empirical likelihood function been maximized is
</p>
<p style="text-align: center;"><code class="reqn">\sum_{d_i=1} \log \frac{ \Delta F(x_i)}{1-F(y_i)}  + 
    \sum_{d_i=0} \log \frac{1-F(x_i)}{1-F(y_i)}.</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>el.ltrc.EM(y,x,d,fun=function(t){t},mu,maxit=30,error=1e-9)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="el.ltrc.EM_+3A_y">y</code></td>
<td>
<p>an optional vector containing the observed left truncation times.</p>
</td></tr>
<tr><td><code id="el.ltrc.EM_+3A_x">x</code></td>
<td>
<p>a vector containing the censored survival times.</p>
</td></tr>
<tr><td><code id="el.ltrc.EM_+3A_d">d</code></td>
<td>
<p>a vector containing the censoring indicators, 
1-uncensored; 0-right censored.</p>
</td></tr>
<tr><td><code id="el.ltrc.EM_+3A_fun">fun</code></td>
<td>
<p>a continuous (weight) function used to calculate
the mean as in <code class="reqn">H_0</code>.
<code>fun(t)</code> must be able to take a vector input <code>t</code>.
Default to the identity function <code class="reqn">f(t)=t</code>.</p>
</td></tr>
<tr><td><code id="el.ltrc.EM_+3A_mu">mu</code></td>
<td>
<p>a real number used in the constraint, mean value of <code class="reqn">f(X)</code>.</p>
</td></tr>
<tr><td><code id="el.ltrc.EM_+3A_error">error</code></td>
<td>
<p>an optional positive real number specifying the tolerance of
iteration error. This is the bound of the
<code class="reqn">L_1</code> norm of the difference of two successive weights.</p>
</td></tr>
<tr><td><code id="el.ltrc.EM_+3A_maxit">maxit</code></td>
<td>
<p>an optional integer, used to control maximum number of
iterations. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>We return the -2 log likelihood ratio, and the constrained
NPMLE of CDF.
The un-constrained NPMLE should be WJT or Lynden-Bell estimator.
</p>
<p>When the given constants <code class="reqn">\mu</code> is too far
away from the NPMLE, there will be no distribution
satisfy the constraint.
In this case the computation will stop.
The -2 Log empirical likelihood ratio
should be infinite. 
</p>
<p>The constant <code>mu</code> must be inside 
<code class="reqn">( \min f(x_i) , \max f(x_i) ) </code>
for the computation to continue. 
It is always true that the NPMLE values are feasible. So when the
computation stops, try move the <code>mu</code> closer
to the NPMLE &mdash; 
</p>
<p style="text-align: center;"><code class="reqn"> \sum_{d_i=1} p_i^0 f(x_i) </code>
</p>
 
<p><code class="reqn">p_i^0</code> taken to be the jumps of the NPMLE of CDF. 
Or use a different <code>fun</code>. 
</p>
<p>This implementation is all in R and have several for-loops in it. 
A faster version would use C to do the for-loop part.
(but this version is easier to port to Splus, and seems faster enough).
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>times</code></td>
<td>
<p>locations of CDF that have positive mass.</p>
</td></tr>
<tr><td><code>prob</code></td>
<td>
<p>the probability of the constrained NPMLE of 
CDF at those locations.</p>
</td></tr>
<tr><td><code>"-2LLR"</code></td>
<td>
<p>It is Minus two times the 
Empirical Log Likelihood Ratio.
Should be approximate chi-square distributed under Ho.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Mai Zhou </p>


<h3>References</h3>

<p>Zhou, M. (2002). 
Computing censored and truncated empirical likelihood ratio 
by EM algorithm. 
<em>Tech Report, Univ. of Kentucky, Dept of Statistics</em>
</p>
<p>Tsai, W. Y., Jewell, N. P., and Wang, M. C. (1987). A note on
product-limit estimator under right censoring and left
truncation. <em>Biometrika</em>, <b>74</b>, 883-886.
</p>
<p>Turnbbull, B. (1976). The empirical distribution 
function with arbitrarily grouped, censored and 
truncated data. JRSS B, 290-295.
</p>
<p>Zhou, M. (2005). Empirical likelihood ratio with arbitrarily censored/truncated data by EM algorithm. 
<em>Journal of Computational and Graphical Statistics</em> 
<b>14</b>, 643-656.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## example with tied observations
y &lt;- c(0, 0, 0.5, 0, 1, 2, 2, 0, 0, 0, 0, 0 )
x &lt;- c(1, 1.5, 2, 3, 4, 5, 6, 5, 4, 1, 2, 4.5)
d &lt;- c(1,   1, 0, 1, 0, 1, 1, 1, 1, 0, 0,   1)
el.ltrc.EM(y,x,d,mu=3.5)
ypsy &lt;- c(51, 58, 55, 28, 25, 48, 47, 25, 31, 30, 33, 43, 45, 35, 36)
xpsy &lt;- c(52, 59, 57, 50, 57, 59, 61, 61, 62, 67, 68, 69, 69, 65, 76)
dpsy &lt;- c(1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1 )
el.ltrc.EM(ypsy,xpsy,dpsy,mu=64)
</code></pre>

<hr>
<h2 id='el.test'>Empirical likelihood ratio test for the means, uncensored data</h2><span id='topic+el.test'></span>

<h3>Description</h3>

<p>Compute the empirical likelihood ratio with the
mean vector fixed at mu.
</p>
<p>The log empirical likelihood been maximized is
</p>
<p style="text-align: center;"><code class="reqn"> \sum_{i=1}^n \log \Delta F(x_i).</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>el.test(x, mu, lam, maxit=25, gradtol=1e-7, 
                 svdtol = 1e-9, itertrace=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="el.test_+3A_x">x</code></td>
<td>
<p>a matrix or vector containing the data, one row
per observation.</p>
</td></tr>
<tr><td><code id="el.test_+3A_mu">mu</code></td>
<td>
<p>a numeric vector (of length <code> = ncol(x)</code>)
to be tested as the mean vector of <code>x</code>
above, as <code class="reqn">H_0</code>.</p>
</td></tr>
<tr><td><code id="el.test_+3A_lam">lam</code></td>
<td>
<p>an optional vector of length <code> = length(mu)</code>,
the starting value of Lagrange
multipliers, will use <code class="reqn">0</code> if missing.</p>
</td></tr>
<tr><td><code id="el.test_+3A_maxit">maxit</code></td>
<td>
<p>an optional integer to control iteration when solve
constrained maximization.</p>
</td></tr>
<tr><td><code id="el.test_+3A_gradtol">gradtol</code></td>
<td>
<p>an optional real value for convergence test.</p>
</td></tr>
<tr><td><code id="el.test_+3A_svdtol">svdtol</code></td>
<td>
<p>an optional real value to detect singularity while
solve equations.</p>
</td></tr>
<tr><td><code id="el.test_+3A_itertrace">itertrace</code></td>
<td>
<p>a logical value. If the iteration history 
needs to be printed out.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>mu</code> is in the interior of the convex hull of the
observations <code>x</code>, then <code>wts</code> should sum to <code>n</code>. 
If <code>mu</code> is outside
the convex hull then <code>wts</code> should sum to nearly zero, and 
<code>-2LLR</code>
will be a large positive number.  It should be infinity,
but for inferential purposes a very large number is
essentially equivalent.  If mu is on the boundary of the convex
hull then <code>wts</code> should sum to nearly k where k is the number of
observations within that face of the convex hull which contains mu.
</p>
<p>When <code>mu</code> is interior to the convex hull, it is typical for
the algorithm to converge quadratically to the solution, perhaps
after a few iterations of searching to get near the solution.
When <code>mu</code> is outside or near the boundary of the convex hull, then
the solution involves a <code>lambda</code> of infinite norm.  The algorithm
tends to nearly double <code>lambda</code> at each iteration and the gradient
size then decreases roughly by half at each iteration.
</p>
<p>The goal in writing the algorithm was to have it &ldquo;fail gracefully&quot;
when <code>mu</code> is not inside the convex hull.  The user can 
either leave <code>-2LLR</code> &ldquo;large and positive&quot; or can replace
it by infinity when the weights do not sum to nearly n.
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>-2LLR</code></td>
<td>
<p>the -2 loglikelihood ratio; approximate chisq distribution
under <code class="reqn">H_o</code>.</p>
</td></tr>
<tr><td><code>Pval</code></td>
<td>
<p>the observed P-value by chi-square approximation.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>the final value of Lagrange multiplier.</p>
</td></tr>
<tr><td><code>grad</code></td>
<td>
<p>the gradient at the maximum.</p>
</td></tr>
<tr><td><code>hess</code></td>
<td>
<p>the Hessian matrix.</p>
</td></tr>
<tr><td><code>wts</code></td>
<td>
<p>weights on the observations</p>
</td></tr>
<tr><td><code>nits</code></td>
<td>
<p>number of iteration performed</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Original Splus code by Art Owen. Adapted to R by Mai Zhou. </p>


<h3>References</h3>

<p>Owen, A. (1990). Empirical likelihood ratio confidence regions. 
<em>Ann. Statist.</em> <b>18</b>, 90-120.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix(c(rnorm(50,mean=1), rnorm(50,mean=2)), ncol=2,nrow=50)
el.test(x, mu=c(1,2))
## Suppose now we wish to test Ho: 2mu(1)-mu(2)=0, then
y &lt;- 2*x[,1]-x[,2]
el.test(y, mu=0)
xx &lt;- c(28,-44,29,30,26,27,22,23,33,16,24,29,24,40,21,31,34,-2,25,19)
el.test(xx, mu=15)  #### -2LLR = 1.805702
</code></pre>

<hr>
<h2 id='el.test.wt'>Weighted Empirical Likelihood ratio for mean, uncensored data</h2><span id='topic+el.test.wt'></span>

<h3>Description</h3>

<p>This program is similar to <code>el.test( )</code> except
it takes weights, and is for one dimensional mu.
</p>
<p>The mean constraint considered is:
</p>
<p style="text-align: center;"><code class="reqn"> \sum_{i=1}^n  p_i x_i  = \mu . </code>
</p>

<p>where <code class="reqn">p_i = \Delta F(x_i)</code> is a probability.
Plus the probability constraint: <code class="reqn"> \sum p_i =1</code>.
</p>
<p>The weighted log empirical likelihood been maximized is
</p>
<p style="text-align: center;"><code class="reqn"> \sum_{i=1}^n w_i \log p_i. </code>
</p>
 


<h3>Usage</h3>

<pre><code class='language-R'>el.test.wt(x, wt, mu, usingC=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="el.test.wt_+3A_x">x</code></td>
<td>
<p>a vector containing the observations.</p>
</td></tr>
<tr><td><code id="el.test.wt_+3A_wt">wt</code></td>
<td>
<p>a vector containing the weights.</p>
</td></tr>
<tr><td><code id="el.test.wt_+3A_mu">mu</code></td>
<td>
<p>a real number used in the constraint, weighted
mean value of <code class="reqn">f(X)</code>.</p>
</td></tr>
<tr><td><code id="el.test.wt_+3A_usingc">usingC</code></td>
<td>
<p>TRUE: use C function, which may be benifit when sample size is large; FALSE: use pure R function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function used to be an internal function. 
It becomes external because others may find it useful
elsewhere.
</p>
<p>The constant <code>mu</code> must be inside 
<code class="reqn">( \min x_i , \max x_i ) </code>
for the computation to continue. 
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>the observations.</p>
</td></tr>
<tr><td><code>wt</code></td>
<td>
<p>the vector of weights.</p>
</td></tr>
<tr><td><code>prob</code></td>
<td>
<p>The probabilities that maximized the weighted 
empirical likelihood under mean constraint.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Mai Zhou, Y.F. Yang for C part. </p>


<h3>References</h3>

<p>Owen, A. (1990). Empirical likelihood ratio 
confidence regions. 
<em>Ann. Statist.</em> <b>18</b>, 90-120.
</p>
<p>Zhou, M. (2002). 
Computing censored empirical likelihood ratio 
by EM algorithm. 
<em>Tech Report, Univ. of Kentucky, Dept of Statistics</em>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## example with tied observations
x &lt;- c(1, 1.5, 2, 3, 4, 5, 6, 5, 4, 1, 2, 4.5)
d &lt;- c(1,   1, 0, 1, 0, 1, 1, 1, 1, 0, 0,   1)
el.cen.EM(x,d,mu=3.5)
## we should get "-2LLR" = 1.2466....
myfun5 &lt;- function(x, theta, eps) {
u &lt;- (x-theta)*sqrt(5)/eps 
INDE &lt;- (u &lt; sqrt(5)) &amp; (u &gt; -sqrt(5)) 
u[u &gt;= sqrt(5)] &lt;- 0 
u[u &lt;= -sqrt(5)] &lt;- 1 
y &lt;- 0.5 - (u - (u)^3/15)*3/(4*sqrt(5)) 
u[ INDE ] &lt;- y[ INDE ] 
return(u)
}
el.cen.EM(x, d, fun=myfun5, mu=0.5, theta=3.5, eps=0.1)
</code></pre>

<hr>
<h2 id='el.test.wt2'>Weighted Empirical Likelihood ratio for mean(s), uncensored data</h2><span id='topic+el.test.wt2'></span>

<h3>Description</h3>

<p>This program is similar to <code>el.test( )</code> except it takes weights.
</p>
<p>The mean constraints are:
</p>
<p style="text-align: center;"><code class="reqn"> \sum_{i=1}^n  p_i x_i  = \mu . </code>
</p>

<p>Where <code class="reqn">p_i = \Delta F(x_i)</code> is a probability.
Plus the probability constraint: <code class="reqn"> \sum p_i =1</code>.
</p>
<p>The weighted log empirical likelihood been maximized is
</p>
<p style="text-align: center;"><code class="reqn"> \sum_{i=1}^n w_i \log p_i. </code>
</p>
 


<h3>Usage</h3>

<pre><code class='language-R'>el.test.wt2(x, wt, mu, maxit = 25, gradtol = 1e-07, Hessian = FALSE, 
    svdtol = 1e-09, itertrace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="el.test.wt2_+3A_x">x</code></td>
<td>
<p>a matrix (of size nxp) or vector containing the observations.</p>
</td></tr>
<tr><td><code id="el.test.wt2_+3A_wt">wt</code></td>
<td>
<p>a vector of length n, containing the weights. If weights are 
all 1, this is very simila to el.test. wt have to be positive.</p>
</td></tr>
<tr><td><code id="el.test.wt2_+3A_mu">mu</code></td>
<td>
<p>a vector of length p, used in the constraint. weighted
mean value of <code class="reqn">f(X)</code>.</p>
</td></tr>
<tr><td><code id="el.test.wt2_+3A_maxit">maxit</code></td>
<td>
<p>an integer, the maximum number of iteration.</p>
</td></tr>
<tr><td><code id="el.test.wt2_+3A_gradtol">gradtol</code></td>
<td>
<p>a positive real number, the tolerance for a solution</p>
</td></tr>
<tr><td><code id="el.test.wt2_+3A_hessian">Hessian</code></td>
<td>
<p>logical. if the Hessian needs to be computed?</p>
</td></tr>
<tr><td><code id="el.test.wt2_+3A_svdtol">svdtol</code></td>
<td>
<p> tolerance in perform SVD of the Hessian matrix. </p>
</td></tr>
<tr><td><code id="el.test.wt2_+3A_itertrace">itertrace</code></td>
<td>
<p>TRUE/FALSE, if the intermediate steps needs to be printed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function used to be an internal function. 
It becomes external because others may find it useful.
</p>
<p>It is similar to the function <code>el.test( )</code> with the
following differences: 
</p>
<p>(1) The output lambda in el.test.wts, when divided by n
(the sample size or sum of all the weights) should be equal to the
output lambda in el.test.
</p>
<p>(2) The Newton step of iteration in el.test.wts is different from
those in el.test. (even when all the weights are one).
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>lambda</code></td>
<td>
<p>the Lagrange multiplier. Solution.</p>
</td></tr>
<tr><td><code>wt</code></td>
<td>
<p>the vector of weights.</p>
</td></tr>
<tr><td><code>grad</code></td>
<td>
<p>The gradian at the final solution.</p>
</td></tr>
<tr><td><code>nits</code></td>
<td>
<p>number of iterations performed. </p>
</td></tr>
<tr><td><code>prob</code></td>
<td>
<p>The probabilities that maximized the weighted 
empirical likelihood under mean constraint.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Mai Zhou </p>


<h3>References</h3>

<p>Owen, A. (1990). Empirical likelihood ratio confidence regions. 
<em>Ann. Statist.</em> <b>18</b>, 90-120.
</p>
<p>Zhou, M. (2005). Empirical likelihood ratio with arbitrary censored/truncated data by EM algorithm. 
<em>Journal of Computational and Graphical Statistics</em>, 
<b>14</b>, 643-656. 
</p>
<p>Zhou, M. (2002). 
Computing censored empirical likelihood ratio 
by EM algorithm. 
<em>Tech Report, Univ. of Kentucky, Dept of Statistics</em>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## example with tied observations
x &lt;- c(1, 1.5, 2, 3, 4, 5, 6, 5, 4, 1, 2, 4.5)
d &lt;- c(1,   1, 0, 1, 0, 1, 1, 1, 1, 0, 0,   1)
el.cen.EM(x,d,mu=3.5)
## we should get "-2LLR" = 1.2466....
myfun5 &lt;- function(x, theta, eps) {
u &lt;- (x-theta)*sqrt(5)/eps 
INDE &lt;- (u &lt; sqrt(5)) &amp; (u &gt; -sqrt(5)) 
u[u &gt;= sqrt(5)] &lt;- 0 
u[u &lt;= -sqrt(5)] &lt;- 1 
y &lt;- 0.5 - (u - (u)^3/15)*3/(4*sqrt(5)) 
u[ INDE ] &lt;- y[ INDE ] 
return(u)
}
el.cen.EM(x, d, fun=myfun5, mu=0.5, theta=3.5, eps=0.1)
</code></pre>

<hr>
<h2 id='el.trun.test'>Empirical likelihood ratio for mean with left truncated data</h2><span id='topic+el.trun.test'></span>

<h3>Description</h3>

<p>This program uses EM algorithm to compute the maximized
(wrt <code class="reqn">p_i</code>) empirical
log likelihood function for left truncated data with 
the MEAN constraint:
</p>
<p style="text-align: center;"><code class="reqn"> \sum  p_i f(x_i)  = \int f(t) dF(t) = \mu ~. </code>
</p>

<p>Where <code class="reqn">p_i = \Delta F(x_i)</code> is a probability.
<code class="reqn">\mu</code> is a given constant. 
It also returns those <code class="reqn">p_i</code> and the <code class="reqn">p_i</code> without
constraint, the Lynden-Bell estimator.
</p>
<p>The log likelihood been maximized is
</p>
<p style="text-align: center;"><code class="reqn"> \sum_{i=1}^n \log \frac{\Delta F(x_i)}{1-F(y_i)} .</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>el.trun.test(y,x,fun=function(t){t},mu,maxit=20,error=1e-9)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="el.trun.test_+3A_y">y</code></td>
<td>
<p>a vector containing the left truncation times.</p>
</td></tr>
<tr><td><code id="el.trun.test_+3A_x">x</code></td>
<td>
<p>a vector containing the survival times. truncation means x&gt;y.</p>
</td></tr>
<tr><td><code id="el.trun.test_+3A_fun">fun</code></td>
<td>
<p>a continuous (weight) function used to calculate
the mean as in <code class="reqn">H_0</code>.
<code>fun(t)</code> must be able to take a vector input <code>t</code>.
Default to the identity function <code class="reqn">f(t)=t</code>.</p>
</td></tr>
<tr><td><code id="el.trun.test_+3A_mu">mu</code></td>
<td>
<p>a real number used in the constraint, mean value of <code class="reqn">f(X)</code>.</p>
</td></tr>
<tr><td><code id="el.trun.test_+3A_error">error</code></td>
<td>
<p>an optional positive real number specifying the tolerance of
iteration error. This is the bound of the
<code class="reqn">L_1</code> norm of the difference of two successive weights.</p>
</td></tr>
<tr><td><code id="el.trun.test_+3A_maxit">maxit</code></td>
<td>
<p>an optional integer, used to control maximum number of
iterations. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>This implementation is all in R and have several for-loops in it. 
A faster version would use C to do the for-loop part. But it seems
faster enough and is easier to port to Splus.
</p>
<p>When the given constants <code class="reqn">\mu</code> is too far
away from the NPMLE, there will be no distribution
satisfy the constraint.
In this case the computation will stop.
The -2 Log empirical likelihood ratio
should be infinite. 
</p>
<p>The constant <code>mu</code> must be inside 
<code class="reqn">( \min f(x_i) , \max f(x_i) ) </code>
for the computation to continue. 
It is always true that the NPMLE values are feasible. So when the
computation stops, try move the <code>mu</code> closer
to the NPMLE &mdash; 
</p>
<p style="text-align: center;"><code class="reqn"> \sum_{d_i=1} p_i^0 f(x_i) </code>
</p>
 
<p><code class="reqn">p_i^0</code> taken to be the jumps of the NPMLE of CDF. 
Or use a different <code>fun</code>. 
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>"-2LLR"</code></td>
<td>
<p>the maximized empirical log likelihood ratio
under the constraint.</p>
</td></tr>
<tr><td><code>NPMLE</code></td>
<td>
<p>jumps of NPMLE of CDF at ordered x.</p>
</td></tr>
<tr><td><code>NPMLEmu</code></td>
<td>
<p>same jumps but for constrained NPMLE.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Mai Zhou </p>


<h3>References</h3>

<p>Zhou, M. (2005). Empirical likelihood ratio with arbitrary censored/truncated data by EM algorithm. 
<em>Journal of Computational and Graphical Statistics</em>, 
<b>14</b>, 643-656.
</p>
<p>Li, G. (1995).
Nonparametric likelihood ratio estimation 
of probabilities for truncated data. 
<em>JASA</em> <b>90</b>, 997-1003.
</p>
<p>Turnbull (1976).
The empirical distribution function with arbitrarily grouped, censored
and truncated data. <em>JRSS</em> B <b>38</b>, 290-295.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## example with tied observations
vet &lt;- c(30, 384, 4, 54, 13, 123, 97, 153, 59, 117, 16, 151, 22, 56, 21, 18,
         139, 20, 31, 52, 287, 18, 51, 122, 27, 54, 7, 63, 392, 10)
vetstart &lt;- c(0,60,0,0,0,33,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
el.trun.test(vetstart, vet, mu=80, maxit=15)
</code></pre>

<hr>
<h2 id='emplik-internal'>Internal emplik functions</h2><span id='topic+Wdataclean2'></span><span id='topic+Wdataclean3'></span><span id='topic+Wdataclean5'></span><span id='topic+kmc.clean'></span><span id='topic+DnR'></span><span id='topic+logelr'></span><span id='topic+logwelr'></span><span id='topic+gradf'></span><span id='topic+llog'></span><span id='topic+llogp'></span><span id='topic+llogpp'></span><span id='topic+solve3.QP'></span><span id='topic+WKM'></span><span id='topic+WCY'></span><span id='topic+el.test.wt3'></span><span id='topic+LTRC'></span><span id='topic+iter'></span><span id='topic+MMRtime'></span><span id='topic+redistF'></span><span id='topic+gradf2'></span><span id='topic+gradf3'></span><span id='topic+cumsumsurv'></span><span id='topic+omega.lambda'></span><span id='topic+emplikHs.test22'></span>

<h3>Description</h3>

<p> Internal emplik functions </p>


<h3>Usage</h3>

<pre><code class='language-R'>logelr(x,mu,lam)
logwelr(x,mu,wt,lam)
gradf(z,wt,lam)
llog(z, eps)
llogp(z, eps)
llogpp(z, eps)
Wdataclean2(z,d,wt=rep(1,length(z)))
Wdataclean3(z,d,zc=rep(1, length(z)),wt=rep(1,length(z)))
Wdataclean5(z,d,zc=rep(1, length(z)),wt=rep(1,length(z)),xmat)
kmc.clean(Xtime, delta)
DnR(x,d,w,y=rep(-Inf,length(x)))
solve3.QP(D, d, A, b, meq, factorized=FALSE)
WKM(x,d,zc=rep(1,length(d)),w=rep(1,length(d)))
WCY(x,d,zc=rep(1,length(d)),wt=rep(1,length(d)),maxit=25,error=1e-09)
LTRC(x,d,w=rep(1, length(d)),y=rep(-Inf, length(x)))
el.test.wt3(x,wt,mu,maxit,gradtol,Hessian,svdtol,itertrace)
iter(x, y, delta, beta)
MMRtime(x, d, age)
redistF(y, d, Fdist)
gradf2(lam, funt1, evt1, rsk1, funt2, evt2, rsk2, K, n)
gradf3(lam, funt1, evt1, rsk1, funt2, evt2, rsk2, K, n)
cumsumsurv(x)
omega.lambda(lambda, delta, gt)
emplikHs.test22(x1, d1, y1 = -Inf, x2, d2, y2 = -Inf, 
                theta, fun1, fun2, maxit = 25, 
                tola = 1e-07, itertrace = FALSE, ...)
</code></pre>


<h3>Details</h3>

<p>These are not intended to be called by the user. 
</p>
<p><code>Wdataclean2</code> and
<code>DnR</code> are used by the functions <code>emplikH1.test</code>, 
<code>emplikH2.test</code> and <code>emplikdisc.test</code>.  It is also used by 
<code>LTRC</code>.  
</p>
<p><code>DnR</code> take the survival data as input, compute the number at risk 
and number of observed events, and output only those time points where
there is some events. Notice the input must be ordered! We should use
Wdataclean2/3/5( ) first and then use the output there as the input of DnR.
</p>
<p><code>Wdataclean2</code> will sort the data and collapse those that are true ties,
and the number of tied value is in the weights.
</p>
<p><code>logelr</code>, <code>llog</code>, <code>llogp</code> and <code>llogpp</code> 
are used by function <code>el.test</code>. They are from Owen.
</p>
<p><code>Wdataclean2</code>, <code>WKM</code> and <code>solve3.QP</code> are
used by function <code>el.cen.test</code>. 
WKM() is the weighted Kaplan-Meier.
</p>
<p><code>WCY</code> is the weighted Chang and Yang self-consistent estimator
for doubly censored data.
</p>
<p><code>Wdataclean2</code> is used by <code>el.cen.EM</code>.
</p>
<p><code>LTRC</code> is for Left Truncated and Right Censored data.
</p>
<p><code>gradf3</code> is used by the function emplikHs.test2. (and stored there)
</p>
<p><code>el.test.wt3</code> are similar to <code>el.test.wt</code> and <code>el.test.wt2</code>, 
but can take vector mean as constraint. <code>llog</code>, <code>llogp</code> and
<code>llogpp</code> are used by both <code>el.test.wt3</code> and
<code>el.test.wt2</code>. In addition <code>logwelr</code>
is used by <code>el.test.wt3</code>.
</p>
<p><code>MMRtime</code> estimate the Mean and Median residual life time at age
with right censored data.
</p>
<p><code>iter</code> is for perform one iteration of EM in the Buckley-James
censored regression estimation.
<code>redistF</code> is for redistribution of probability, according to Fdist.
Used in BJtest() and BJtestII().
</p>
<p><code>cumsumsurv</code> is similar to rev(cumsum(rev( ))) but faster.
<code>omega.lambda</code> is a recursive function, either R or C (future), to compute jumps of KaplaMeier given Lagrange multiplier lambda.
<code>emplikHs.test22</code> similar to emplikHs.test2; and is used by RankRegTestH() only. 
</p>

<hr>
<h2 id='emplik-package'>
Empirical likelihood for mean functional/hazard functional with possibly censored data. 
</h2><span id='topic+emplik-package'></span><span id='topic+emplik'></span>

<h3>Description</h3>

<p>Empirical likelihood ratio tests and confidence intervals for means/hazards
from possibly censored and/or truncated data. 
Now does regression too. The package contains some C code.
</p>


<h3>Details</h3>

<p>For non-censored data and mean parameters, use <code>el.test( )</code>.
</p>
<p>For censored data and mean parameters, use <code>el.cen.EM2( )</code>.
</p>
<p>For censored data and hazard parameters, use <code>emplikH1.test( )</code> [Poisson type likelihood];
use <code>emplikH.disc( )</code> [binomial type likelihood].
</p>
<p>For constructing confidence intervals, use <code>findUL( )</code>.


</p>


<h3>Author(s)</h3>

<p>Mai Zhou (el.test is adapted from Owen's splus code. Yifan Yang for some C code.)
</p>
<p>Maintainer: Mai Zhou &lt;mai@ms.uky.edu&gt; &lt;maizhou@gmail.com&gt;
</p>


<h3>References</h3>

<p>Zhou, M. (2016).
Empirical Likelihood Method in Survival Analysis.
(Chapman and Hall/CRC Biostatistics Series)
CRC press 2016
</p>


<h3>See Also</h3>

<p>Another R package kmc for possible faster results when testing of mean with right censored data.</p>

<hr>
<h2 id='emplikH.disc'>Empirical likelihood ratio for discrete hazard
with right censored, left truncated data</h2><span id='topic+emplikH.disc'></span>

<h3>Description</h3>

<p>Use empirical likelihood ratio and Wilks theorem to test 
the null hypothesis that
</p>
<p style="text-align: center;"><code class="reqn"> 
\sum_i[f(x_i, \theta) \log(1- dH(x_i))] = K  
</code>
</p>

<p>where <code class="reqn">H(t)</code> is the (unknown) discrete cumulative
hazard function; <code class="reqn">f(t,\theta)</code> can be any predictable
function of <code class="reqn">t</code>. 
<code class="reqn">\theta</code> is the parameter of the function 
and <code>K</code> is a given constant. 
The data can be right censored and left truncated.
</p>
<p>When the given constants <code class="reqn">\theta</code> and/or <code>K</code> are too far
away from the NPMLE, there will be no hazard function satisfy this 
constraint and the minus 2Log empirical likelihood ratio
will be infinite. In this case the computation will stop.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>emplikH.disc(x, d, y= -Inf, K, fun, tola=.Machine$double.eps^.25, theta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="emplikH.disc_+3A_x">x</code></td>
<td>
<p>a vector, the observed survival times.</p>
</td></tr>
<tr><td><code id="emplikH.disc_+3A_d">d</code></td>
<td>
<p>a vector, the censoring indicators, 1-uncensor; 0-censor.</p>
</td></tr>
<tr><td><code id="emplikH.disc_+3A_y">y</code></td>
<td>
<p>optional vector, the left truncation times.</p>
</td></tr>
<tr><td><code id="emplikH.disc_+3A_k">K</code></td>
<td>
<p>a real number used in the constraint, sum to this value.</p>
</td></tr>
<tr><td><code id="emplikH.disc_+3A_fun">fun</code></td>
<td>
<p>a left continuous (weight) function used to calculate
the weighted discrete hazard in <code class="reqn">H_0</code>. 
<code>fun(x, theta)</code> must be able to take a
vector input <code>x</code>, and a parameter <code>theta</code>. </p>
</td></tr>
<tr><td><code id="emplikH.disc_+3A_tola">tola</code></td>
<td>
<p>an optional positive real number specifying the tolerance of
iteration error in solve the non-linear equation needed in constrained 
maximization.</p>
</td></tr>
<tr><td><code id="emplikH.disc_+3A_theta">theta</code></td>
<td>
<p>a given real number used as the parameter of the 
function <code class="reqn">f</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The log likelihood been maximized is the &lsquo;binomial&rsquo; empirical likelihood:
</p>
<p style="text-align: center;"><code class="reqn"> \sum D_i \log w_i  + (R_i-D_i) \log [1-w_i] </code>
</p>

<p>where <code class="reqn">w_i = \Delta H(t_i)</code> is the jump 
of the cumulative hazard function, <code class="reqn">D_i</code> is the number of failures
observed at <code class="reqn">t_i</code>, <code class="reqn">R_i</code> is the number of subjects at risk at
time <code class="reqn">t_i</code>.
</p>
<p>For discrete distributions, the jump size of the cumulative hazard at
the last jump is always 1. We have to exclude this jump from the 
summation since <code class="reqn"> \log( 1- dH(\cdot))</code> do not make sense.
</p>
<p>The constants <code>theta</code> and <code>K</code> must be inside the so called
feasible region for the computation to continue. This is similar to the
requirement that in testing the value of the mean, the value must be
inside the convex hull of the observations.
It is always true that the NPMLE values are feasible. So when the
computation stops, try move the <code>theta</code> and <code>K</code> closer
to the NPMLE.  When the computation stops, the -2LLR should have value
infinite.
</p>
<p>In case you do not need the <code>theta</code> in the definition of the
function <code class="reqn">f</code>, you still need to formally define your <code>fun</code> function
with a <code>theta</code> input, just to match the arguments.
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>times</code></td>
<td>
<p>the location of the hazard jumps.</p>
</td></tr>
<tr><td><code>wts</code></td>
<td>
<p>the jump size of hazard function at those locations.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>the final value of the Lagrange multiplier.</p>
</td></tr>
<tr><td><code>"-2LLR"</code></td>
<td>
<p>The discrete -2Log Likelihood ratio.</p>
</td></tr>
<tr><td><code>Pval</code></td>
<td>
<p>P-value</p>
</td></tr>
<tr><td><code>niters</code></td>
<td>
<p>number of iterations used</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Mai Zhou </p>


<h3>References</h3>

<p>Fang, H. (2000). Binomial Empirical Likelihood Ratio Method in
Survival Analysis. Ph.D. Thesis, Univ. of Kentucky, Dept
of Statistics.
</p>
<p>Zhou and Fang (2001). 
&ldquo;Empirical likelihood ratio for 2 sample problem 
for censored data&rdquo;. 
<em>Tech Report, Univ. of Kentucky, Dept of Statistics</em>
</p>
<p>Zhou, M. and Fang, H. (2006). 
A comparison of Poisson and binomial empirical likelihood.
<em>Tech Report, Univ. of Kentucky, Dept of Statistics</em>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>fun4 &lt;- function(x, theta) { as.numeric(x &lt;= theta) }
x &lt;- c(1, 2, 3, 4, 5, 6, 5, 4, 3, 4, 1, 2.4, 4.5)
d &lt;- c(1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1)
# test if -H(4) = -0.7 
emplikH.disc(x=x,d=d,K=-0.7,fun=fun4,theta=4)
# we should get "-2LLR" 0.1446316  etc....
y &lt;- c(-2,-2, -2, 1.5, -1)
emplikH.disc(x=x,d=d,y=y,K=-0.7,fun=fun4,theta=4)
</code></pre>

<hr>
<h2 id='emplikH.disc2'>Two sample empirical likelihood ratio for discrete hazards
with right censored, left truncated data, one parameter.</h2><span id='topic+emplikH.disc2'></span>

<h3>Description</h3>

<p>Use empirical likelihood ratio and Wilks theorem to test 
the null hypothesis that
</p>
<p style="text-align: center;"><code class="reqn"> 
\int{f_1(t) I_{[dH_1 &lt;1]} \log(1-dH_1(t))} -
\int{f_2(t) I_{[dH_2 &lt;1]} \log(1-dH_2(t))} = \theta 
</code>
</p>

<p>where <code class="reqn">H_*(t)</code> is the (unknown) discrete cumulative
hazard function; <code class="reqn">f_*(t)</code> can be any predictable  
functions of <code class="reqn">t</code>. 
<code class="reqn">\theta</code> is the parameter. The given value of <code class="reqn">\theta</code>
in these computation is the value to be tested.
The data can be right censored and left truncated.
</p>
<p>When the given constants <code class="reqn">\theta</code> is too far
away from the NPMLE, there will be no hazard function satisfy this 
constraint and the -2 Log empirical likelihood ratio
will be infinite. In this case the computation will stop.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>emplikH.disc2(x1, d1, y1= -Inf, x2, d2, y2 = -Inf,
              theta, fun1, fun2, tola = 1e-6, maxi, mini)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="emplikH.disc2_+3A_x1">x1</code></td>
<td>
<p>a vector, the observed survival times, sample 1.</p>
</td></tr>
<tr><td><code id="emplikH.disc2_+3A_d1">d1</code></td>
<td>
<p>a vector, the censoring indicators, 1-uncensor; 0-censor.</p>
</td></tr>
<tr><td><code id="emplikH.disc2_+3A_y1">y1</code></td>
<td>
<p>optional vector, the left truncation times.</p>
</td></tr>
<tr><td><code id="emplikH.disc2_+3A_x2">x2</code></td>
<td>
<p>a vector, the observed survival times, sample 2.</p>
</td></tr>
<tr><td><code id="emplikH.disc2_+3A_d2">d2</code></td>
<td>
<p>a vector, the censoring indicators, 1-uncensor; 0-censor.</p>
</td></tr>
<tr><td><code id="emplikH.disc2_+3A_y2">y2</code></td>
<td>
<p>optional vector, the left truncation times.</p>
</td></tr>
<tr><td><code id="emplikH.disc2_+3A_fun1">fun1</code></td>
<td>
<p>a predictable function used to calculate
the weighted discrete hazard in <code class="reqn">H_0</code>. 
<code>fun1(x)</code> must be able to take a vector input <code>x</code>.</p>
</td></tr>
<tr><td><code id="emplikH.disc2_+3A_fun2">fun2</code></td>
<td>
<p> similar to fun1, but for sample 2.</p>
</td></tr>
<tr><td><code id="emplikH.disc2_+3A_tola">tola</code></td>
<td>
<p>an optional positive real number, the tolerance of
iteration error in solve the non-linear equation needed in constrained 
maximization.</p>
</td></tr>
<tr><td><code id="emplikH.disc2_+3A_theta">theta</code></td>
<td>
<p>a given real number. for Ho constraint. </p>
</td></tr>
<tr><td><code id="emplikH.disc2_+3A_maxi">maxi</code></td>
<td>
<p> upper bound for lambda, usually positive. </p>
</td></tr>
<tr><td><code id="emplikH.disc2_+3A_mini">mini</code></td>
<td>
<p> lower bound for lambda, usually negative. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The log likelihood been maximized is the &lsquo;binomial&rsquo; empirical likelihood:
</p>
<p style="text-align: center;"><code class="reqn"> 
\sum D_{1i} \log w_i  + (R_{1i}-D_{1i}) \log [1-w_i] + 
\sum D_{2j} \log v_j  + (R_{2j}-D_{2j}) \log [1-v_j] 
</code>
</p>

<p>where <code class="reqn">w_i = \Delta H_1(t_i)</code> is the jump 
of the cumulative hazard function at <code class="reqn">t_i</code>, 
<code class="reqn">D_{1i}</code> is the number of failures
observed at <code class="reqn">t_i</code>, <code class="reqn">R_{1i}</code> is 
the number of subjects at risk at
time <code class="reqn">t_i</code>.
</p>
<p>For discrete distributions, the jump size of the cumulative hazard at
the last jump is always 1. We have to exclude this jump from the 
summation in the constraint calculation
since <code class="reqn"> \log( 1- dH(\cdot))</code> do not make sense.
</p>
<p>The constants <code>theta</code> must be inside the so called
feasible region for the computation to continue. This is similar to the
requirement that in ELR testing the value of the mean, the value must be
inside the convex hull of the observations.
It is always true that the NPMLE values are feasible. So when the
computation stops, try move the <code>theta</code> closer
to the NPMLE.  When the computation stops, the -2LLR should have value
infinite.
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>times</code></td>
<td>
<p>the location of the hazard jumps.</p>
</td></tr>
<tr><td><code>wts</code></td>
<td>
<p>the jump size of hazard function at those locations.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>the final value of the Lagrange multiplier.</p>
</td></tr>
<tr><td><code>"-2LLR"</code></td>
<td>
<p>The -2Log Likelihood ratio.</p>
</td></tr>
<tr><td><code>Pval</code></td>
<td>
<p>P-value</p>
</td></tr>
<tr><td><code>niters</code></td>
<td>
<p>number of iterations used</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Mai Zhou </p>


<h3>References</h3>

<p>Zhou and Fang (2001). 
&ldquo;Empirical likelihood ratio for 2 sample problems for censored data&rdquo;. 
<em>Tech Report, Univ. of Kentucky, Dept of Statistics</em>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(require("boot", quietly = TRUE)) {
####library(boot)
data(channing)
ymale &lt;- channing[1:97,2]
dmale &lt;- channing[1:97,5]
xmale &lt;- channing[1:97,3]
yfemale &lt;- channing[98:462,2]
dfemale &lt;- channing[98:462,5]
xfemale &lt;- channing[98:462,3]
fun1 &lt;- function(x) { as.numeric(x &lt;= 960) }
emplikH.disc2(x1=xfemale, d1=dfemale, y1=yfemale, 
 x2=xmale, d2=dmale, y2=ymale, theta=0.2, fun1=fun1, fun2=fun1, maxi=4, mini=-10)
######################################################
### You should get "-2LLR" = 1.511239 and a lot more other outputs.
########################################################
emplikH.disc2(x1=xfemale, d1=dfemale, y1=yfemale, 
 x2=xmale, d2=dmale, y2=ymale, theta=0.25, fun1=fun1, fun2=fun1, maxi=4, mini=-5)
########################################################
### This time you get "-2LLR" = 1.150098 etc. etc.
##############################################################
}
</code></pre>

<hr>
<h2 id='emplikH1.test'>Empirical likelihood for hazard with right censored, 
left truncated data</h2><span id='topic+emplikH1.test'></span>

<h3>Description</h3>

<p>Use empirical likelihood ratio and Wilks theorem to test 
the null hypothesis that 
</p>
<p style="text-align: center;"><code class="reqn">\int f(t) dH(t) = \theta </code>
</p>

<p>with right censored, left truncated data. 
Where <code class="reqn">H(t)</code> is the unknown
cumulative hazard
function; <code class="reqn">f(t)</code> can be any given 
function and
<code class="reqn">\theta</code> a given constant. In fact, <code class="reqn">f(t)</code> can even be data
dependent, just have to be &lsquo;predictable&rsquo;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>emplikH1.test(x, d, y= -Inf, theta, fun, tola=.Machine$double.eps^.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="emplikH1.test_+3A_x">x</code></td>
<td>
<p>a vector of the censored survival times.</p>
</td></tr>
<tr><td><code id="emplikH1.test_+3A_d">d</code></td>
<td>
<p>a vector of the censoring indicators, 1-uncensor; 0-censor.</p>
</td></tr>
<tr><td><code id="emplikH1.test_+3A_y">y</code></td>
<td>
<p>a vector of the observed left truncation times.</p>
</td></tr>
<tr><td><code id="emplikH1.test_+3A_theta">theta</code></td>
<td>
<p>a real number used in the <code class="reqn">H_0</code> to set the hazard 
to this value.</p>
</td></tr>
<tr><td><code id="emplikH1.test_+3A_fun">fun</code></td>
<td>
<p>a left continuous (weight) function used to calculate 
the weighted hazard in <code class="reqn"> H_0</code>. <code>fun</code> must be able 
to take a vector input. See example below.</p>
</td></tr>
<tr><td><code id="emplikH1.test_+3A_tola">tola</code></td>
<td>
<p>an optional positive real number specifying the tolerance of
iteration error in solve the non-linear equation needed in constrained
maximization.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is designed for the case where the 
true distributions are all continuous.
So there should be no tie in the data.
</p>
<p>The log empirical likelihood used here is the &lsquo;Poisson&rsquo; version empirical likelihood:
</p>
<p style="text-align: center;"><code class="reqn">  
\sum_{i=1}^n \delta_i \log (dH(x_i))  - [ H(x_i) - H(y_i) ]  ~.
</code>
</p>

<p>If there are ties in the data that are resulted from rounding,
you may break the tie by adding a different tiny number to the tied
observation(s). If those are true ties 
(thus the true distribution is discrete)
we recommend use <code>emplikdisc.test()</code>. 
</p>
<p>The constant <code>theta</code> must be inside the so called
feasible region for the computation to continue. This is similar to the
requirement that in testing the value of the mean, the value must be
inside the convex hull of the observations.
It is always true that the NPMLE values are feasible. So when the
computation complains that there is no hazard function satisfy
the constraint, you should try to move the <code>theta</code> value closer
to the NPMLE.  When the computation stops prematurely, 
the -2LLR should have value infinite.
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>times</code></td>
<td>
<p>the location of the hazard jumps.</p>
</td></tr>
<tr><td><code>wts</code></td>
<td>
<p>the jump size of hazard function at those locations.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>the Lagrange multiplier.</p>
</td></tr>
<tr><td><code>"-2LLR"</code></td>
<td>
<p>the -2Log Likelihood ratio.</p>
</td></tr>
<tr><td><code>Pval</code></td>
<td>
<p>P-value</p>
</td></tr>
<tr><td><code>niters</code></td>
<td>
<p>number of iterations used</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Mai Zhou </p>


<h3>References</h3>

<p>Pan, X. and Zhou, M. (2002),
&ldquo;Empirical likelihood in terms of hazard for censored data&rdquo;. 
<em>Journal of Multivariate Analysis</em> <b>80</b>, 166-188.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>fun &lt;- function(x) { as.numeric(x &lt;= 6.5) }
emplikH1.test( x=c(1,2,3,4,5), d=c(1,1,0,1,1), theta=2, fun=fun) 
fun2 &lt;- function(x) {exp(-x)}  
emplikH1.test( x=c(1,2,3,4,5), d=c(1,1,0,1,1), theta=0.2, fun=fun2) 
</code></pre>

<hr>
<h2 id='emplikH1B'>Return binomial empirical likelihood ratio for the given lambda, with right censored data</h2><span id='topic+emplikH1B'></span>

<h3>Description</h3>

<p>Compute the binomial empirical likelihood ratio for the given tilt parameter lambda. 
Most useful for construct Wilks confidence intervals.
The null hypothesis or constraint is defined by the parameter <code class="reqn">\theta</code>, where 
</p>
<p style="text-align: center;"><code class="reqn">\int fung(t) d log(1-H(t)) = \theta </code>
</p>
<p>.
</p>
<p>Where <code class="reqn">H(t)</code> is the unknown
cumulative hazard function; <code class="reqn">fung(t)</code> can be any given function. 
In the future, the function <code class="reqn">fung</code> may replaced by the vector of <code class="reqn">fung(x)</code>, 
since this is more flexible.
</p>
<p>Input data can be right censored. If no censoring, set <code>d=rep(1, length(x))</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>emplikH1B(lambda, x, d, fung, CIforTheta=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="emplikH1B_+3A_lambda">lambda</code></td>
<td>
<p>a scalar. Can be positive or negative. The amount of tiling.</p>
</td></tr>
<tr><td><code id="emplikH1B_+3A_x">x</code></td>
<td>
<p>a vector of the censored survival times.</p>
</td></tr>
<tr><td><code id="emplikH1B_+3A_d">d</code></td>
<td>
<p>a vector of the censoring indicators, 1-uncensor; 0-right censor.</p>
</td></tr>
<tr><td><code id="emplikH1B_+3A_fung">fung</code></td>
<td>
<p>a left continuous (weight) function used to calculate 
the weighted hazard in the parameter <code class="reqn">\theta</code>. <code>fung</code> must be able 
to take a vector input. See example below.</p>
</td></tr>
<tr><td><code id="emplikH1B_+3A_cifortheta">CIforTheta</code></td>
<td>
<p>an optional logical value. Default to FALSE. If set to TRUE,
will return the integrated hazard value for the given lambda.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is used to calculate lambda confidence interval (Wilks type) for <code class="reqn">\theta</code>.
</p>
<p>This function is designed for the case where the 
true distribution should be discrete. Ties in the data are OK.
</p>
<p>The log empirical likelihood used here is the &lsquo;binomial&rsquo; version empirical likelihood:
</p>
<p style="text-align: center;"><code class="reqn">  
\sum_{i=1}^n \delta_i \log (dH(x_i))  + (R_i - \delta_i)\log [1- dH(x_i) ]  .
</code>
</p>



<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>times</code></td>
<td>
<p>the location of the hazard jumps.</p>
</td></tr>
<tr><td><code>jumps</code></td>
<td>
<p>the jump size of hazard function at those locations.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>the input lambda.</p>
</td></tr>
<tr><td><code>"-2LLR"</code></td>
<td>
<p>the -2Log Likelihood ratio.</p>
</td></tr>
<tr><td><code>IntHaz</code></td>
<td>
<p>The theta defined above, for the given lambda.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Mai Zhou </p>


<h3>References</h3>

<p>Pan, X. and Zhou, M. (2002),
&ldquo;Empirical likelihood in terms of hazard for censored data&rdquo;. 
<em>Journal of Multivariate Analysis</em> <b>80</b>, 166-188.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## fun &lt;- function(x) { as.numeric(x &lt;= 6.5) }
## emplikH1.test( x=c(1,2,3,4,5), d=c(1,1,0,1,1), theta=2, fun=fun) 
## fun2 &lt;- function(x) {exp(-x)}  
## emplikH1.test( x=c(1,2,3,4,5), d=c(1,1,0,1,1), theta=0.2, fun=fun2) 
</code></pre>

<hr>
<h2 id='emplikH1P'>Return Poisson Empirical likelihood ratio for the given lambda, with right censored data</h2><span id='topic+emplikH1P'></span>

<h3>Description</h3>

<p>Compute the Poisson empirical likelihood ratio for the given tilt parameter lambda. 
Most useful for the construction of Wilks confidence intervals.
The null hypothesis or constraint is defined by the parameter <code class="reqn">\theta</code>, where 
</p>
<p style="text-align: center;"><code class="reqn">\int fung(t) dH(t) = \theta </code>
</p>
<p>.
</p>
<p>Where <code class="reqn">H(t)</code> is the unknown
cumulative hazard
function; <code class="reqn">fung(t)</code> can be any given function. 
</p>
<p>In the future, the function <code class="reqn">fung</code> may replaced by the vector of <code class="reqn">fung(x)</code>, 
since this is more flexible.
</p>
<p>Input data can be right censored. If no censoring, set <code>d=rep(1, length(x))</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>emplikH1P(lambda, x, d, fung, CIforTheta=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="emplikH1P_+3A_lambda">lambda</code></td>
<td>
<p>a scalar. Can be positive or negative. The amount of tiling.</p>
</td></tr>
<tr><td><code id="emplikH1P_+3A_x">x</code></td>
<td>
<p>a vector of the censored survival times.</p>
</td></tr>
<tr><td><code id="emplikH1P_+3A_d">d</code></td>
<td>
<p>a vector of the censoring indicators, 1-uncensor; 0-right censor.</p>
</td></tr>
<tr><td><code id="emplikH1P_+3A_fung">fung</code></td>
<td>
<p>a left continuous (weight) function used to calculate 
the weighted hazard in the parameter <code class="reqn">\theta</code>. <code>fung</code> must be able 
to take a vector input. See example below.</p>
</td></tr>
<tr><td><code id="emplikH1P_+3A_cifortheta">CIforTheta</code></td>
<td>
<p>an optional logical value. Default to FALSE. If set to TRUE,
will return the integrated hazard value for the given lambda.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is for calculate lambda confidence intervals for <code class="reqn">\theta</code>.
</p>
<p>This function is designed for the case where the 
true distribution should be continuous.
So there should be no tie in the data.
</p>
<p>The log empirical likelihood used here is the &lsquo;Poisson&rsquo; version empirical likelihood:
</p>
<p style="text-align: center;"><code class="reqn">  
\sum_{i=1}^n \delta_i \log (dH(x_i))  - [ H(x_i) ]  ~.
</code>
</p>

<p>If there are ties in the data that are resulted from rounding,
you may want to break the tie by adding a different tiny number to the tied
observation(s). For example: 2, 2, 2, change to 2.00001, 2.00002, 2.00003.
If those are true ties 
(thus the true distribution must be discrete)
we recommend to use <code>emplikH1B</code> instead. 
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>times</code></td>
<td>
<p>the location of the hazard jumps.</p>
</td></tr>
<tr><td><code>wts</code></td>
<td>
<p>the jump size of hazard function at those locations.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>the Lagrange multiplier.</p>
</td></tr>
<tr><td><code>"-2LLR"</code></td>
<td>
<p>the -2Log Empirical Likelihood ratio, Poisson version.</p>
</td></tr>
<tr><td><code>MeanHaz</code></td>
<td>
<p>The theta defined above, the hazard integral, if CIforTheta =TRUE.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Mai Zhou </p>


<h3>References</h3>

<p>Pan, X. and Zhou, M. (2002),
&ldquo;Empirical likelihood in terms of hazard for censored data&rdquo;. 
<em>Journal of Multivariate Analysis</em> <b>80</b>, 166-188.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## fun &lt;- function(x) { as.numeric(x &lt;= 6.5) }
## emplikH1.test( x=c(1,2,3,4,5), d=c(1,1,0,1,1), theta=2, fun=fun) 
## fun2 &lt;- function(x) {exp(-x)}  
## emplikH1.test( x=c(1,2,3,4,5), d=c(1,1,0,1,1), theta=0.2, fun=fun2) 
</code></pre>

<hr>
<h2 id='emplikH2.test'>Empirical likelihood for weighted hazard with 
right censored, left truncated data</h2><span id='topic+emplikH2.test'></span>

<h3>Description</h3>

<p>Use empirical likelihood ratio and Wilks theorem to test 
the null hypothesis that 
</p>
<p style="text-align: center;"><code class="reqn"> \int f(t, ... ) dH(t) = K </code>
</p>

<p>with right censored, left truncated data, where <code class="reqn"> H(t) </code> is the (unknown)
cumulative hazard function;
<code class="reqn"> f(t, ... )</code> can be any given left continuous function in <code class="reqn">t</code>;
(of course the integral must be finite). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>emplikH2.test(x, d, y= -Inf, K, fun, tola=.Machine$double.eps^.5,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="emplikH2.test_+3A_x">x</code></td>
<td>
<p>a vector containing the censored survival times.</p>
</td></tr>
<tr><td><code id="emplikH2.test_+3A_d">d</code></td>
<td>
<p>a vector of the censoring indicators, 1-uncensor; 0-censor.</p>
</td></tr>
<tr><td><code id="emplikH2.test_+3A_y">y</code></td>
<td>
<p>a vector containing the left truncation times. If left as default
value, -Inf, it means no truncation. </p>
</td></tr>
<tr><td><code id="emplikH2.test_+3A_k">K</code></td>
<td>
<p>a real number used in the constraint, i.e. to set the
weighted integral of hazard to this value.</p>
</td></tr>
<tr><td><code id="emplikH2.test_+3A_fun">fun</code></td>
<td>
<p>a left continuous (in <code>t</code>) weight function
used to calculate
the weighted hazard in <code class="reqn">H_0</code>. <code>fun(t, ... )</code> must be
able to take a vector input <code>t</code>.</p>
</td></tr>
<tr><td><code id="emplikH2.test_+3A_tola">tola</code></td>
<td>
<p>an optional positive real number specifying the tolerance of
iteration error in solve the non-linear equation needed in constrained
maximization.</p>
</td></tr>
<tr><td><code id="emplikH2.test_+3A_...">...</code></td>
<td>
<p>additional parameter(s), if any, passing along to <code>fun</code>. 
This allows an implicit function of <code>fun</code>.  </p>
</td></tr>
</table>


<h3>Details</h3>

<p>This version works for implicit function <code> f(t, ...)</code>.
</p>
<p>This function is designed for continuous distributions.
Thus we do not expect tie in the observation <code>x</code>. If you believe
the true underlying distribution is continuous but the
sample observations have tie due to rounding, then you might want
to add a small number to the observations to break tie.
</p>
<p>The likelihood used here is the &lsquo;Poisson&rsquo; version of the empirical likelihood
</p>
<p style="text-align: center;"><code class="reqn">
\prod_{i=1}^n ( dH(x_i) )^{\delta_i} \exp [-H(x_i)+H(y_i)] . 
</code>
</p>

<p>For discrete distributions we recommend
use <code>emplikdisc.test()</code>.
</p>
<p>Please note here the largest observed time is NOT automatically defined to be
uncensored. In the el.cen.EM( ), it is (to make F a proper distribution always).
</p>
<p>The constant <code>K</code> must be inside the so called
feasible region for the computation to continue. This is similar to the
requirement that when testing the value of the mean, the value must be
inside the convex hull of the observations for the computation to continue.
It is always true that the NPMLE value is feasible. So when the
computation cannot continue, that means there is no hazard function 
dominated by the Nelson-Aalen estimator satisfy 
the constraint. You may try to move the <code>theta</code> and <code>K</code> closer
to the NPMLE.  When the computation cannot continue, 
the -2LLR should have value
infinite.
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>times</code></td>
<td>
<p>the location of the hazard jumps.</p>
</td></tr>
<tr><td><code>wts</code></td>
<td>
<p>the jump size of hazard function at those locations.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>the Lagrange multiplier.</p>
</td></tr>
<tr><td><code>"-2LLR"</code></td>
<td>
<p>the -2Log Likelihood ratio.</p>
</td></tr>
<tr><td><code>Pval</code></td>
<td>
<p>P-value</p>
</td></tr>
<tr><td><code>niters</code></td>
<td>
<p>number of iterations used</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Mai Zhou </p>


<h3>References</h3>

<p>Pan, XR and Zhou, M. (2002),
&ldquo;Empirical likelihood in terms of cumulative
hazard for censored data&rdquo;. 
<em>Journal of Multivariate Analysis</em> <b>80</b>, 166-188.
</p>


<h3>See Also</h3>

<p> emplikHs.test2 </p>


<h3>Examples</h3>

<pre><code class='language-R'>z1&lt;-c(1,2,3,4,5)
d1&lt;-c(1,1,0,1,1)
fun4 &lt;- function(x, theta) { as.numeric(x &lt;= theta) }
emplikH2.test(x=z1,d=d1, K=0.5, fun=fun4, theta=3.5)
#Next, test if H(3.5) = log(2) .
emplikH2.test(x=z1,d=d1, K=log(2), fun=fun4, theta=3.5)
#Next, try one sample log rank test
indi &lt;- function(x,y){ as.numeric(x &gt;= y) }
fun3 &lt;- function(t,z){rowsum(outer(z,t,FUN="indi"),group=rep(1,length(z)))} 
emplikH2.test(x=z1, d=d1, K=sum(0.25*z1), fun=fun3, z=z1) 
##this is testing if the data is from an exp(0.25) population.
</code></pre>

<hr>
<h2 id='emplikH2B'>Return binomial empirical likelihood ratio for the given lambda, with 2-sample right censored data</h2><span id='topic+emplikH2B'></span>

<h3>Description</h3>

<p>Compute the binomial empirical likelihood ratio for the given tilt parameter lambda. 
Most useful for construct Wilks confidence intervals.
The null hypothesis or constraint is defined by the parameter <code class="reqn">\theta</code>, where 
</p>
<p style="text-align: center;"><code class="reqn">
\int fun1(t) d \log(1-H_1(t)) - \int fun2(t) d \log (1-H_2(t)) = \theta </code>
</p>
<p>.
</p>
<p>If the lambda=0, you get the Nelson-Aalen (NPMLE) and output -2LLR =0. Otherwise the lambda is not scaled
(as in one sample case). Since there are two sample sizes. It can be confusing which sample size to use for scale.
So the lambda here is larger than those in one sample by a sclae of (either?) sample size.
</p>
<p>Where <code class="reqn">H_1(t)</code> and <code class="reqn">H_2(t)</code> are the unknown
cumulative hazard function for sample 1/2; <code class="reqn">fun1(t)</code> and <code class="reqn">fun2(t)</code> can be any given function. 
It can even be random, just need to be predictable.
In the future, the input function <code class="reqn">fun</code> may replaced by the vector of <code class="reqn">fun(x)</code>, 
since this is more flexible.
</p>
<p>Input data can be right censored. If no censoring, set <code>d1=rep(1, length(x1))</code>, and/or <code>d2=rep(1, length(x2))</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>emplikH2B(lambda, x1, d1, x2, d2, fun1, fun2, CIforTheta=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="emplikH2B_+3A_lambda">lambda</code></td>
<td>
<p>a scalar. Can be positive or negative. The amount of tiling.</p>
</td></tr>
<tr><td><code id="emplikH2B_+3A_x1">x1</code></td>
<td>
<p>a vector of the censored survival times. sample 1</p>
</td></tr>
<tr><td><code id="emplikH2B_+3A_d1">d1</code></td>
<td>
<p>a vector of the censoring indicators, 1-uncensor; 0-right censor.</p>
</td></tr>
<tr><td><code id="emplikH2B_+3A_x2">x2</code></td>
<td>
<p>a vector of the censored survival times. sample 2</p>
</td></tr>
<tr><td><code id="emplikH2B_+3A_d2">d2</code></td>
<td>
<p>a vector of the censoring indicators, 1-uncensor; 0-right censor.</p>
</td></tr>
<tr><td><code id="emplikH2B_+3A_fun1">fun1</code></td>
<td>
<p>a left continuous (weight) function used to calculate 
the weighted hazard in the parameter <code class="reqn">\theta</code>. <code>fun1</code> must be able 
to take a vector input. See example below.</p>
</td></tr>
<tr><td><code id="emplikH2B_+3A_fun2">fun2</code></td>
<td>
<p>Ditto</p>
</td></tr>
<tr><td><code id="emplikH2B_+3A_cifortheta">CIforTheta</code></td>
<td>
<p>an optional logical value. Default to FALSE. If set to TRUE,
will return the integrated hazard value for the given lambda.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is used to calculate lambda confidence interval (Wilks type) for <code class="reqn">\theta</code>.
</p>
<p>This function is designed for the case where the 
true distribution should be discrete. Ties in the data are OK.
</p>
<p>The log empirical likelihood used here is the &lsquo;binomial&rsquo; version empirical likelihood:
</p>
<p style="text-align: center;"><code class="reqn">  
log EL1 = \sum_{i=1}^n \delta_i \log (dH(x_i))  + (R_i - \delta_i)\log [1- dH(x_i) ]  ~,
</code>
</p>

<p>(similarly defined for sample 2) and the overall log EL = log EL1 + log EL2.
</p>


<h3>Value</h3>

<p>A list with the following components: 
</p>
<table>
<tr><td><code>"-2LLR"</code></td>
<td>
<p>the -2Log Empirical Likelihood ratio, binomial version.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>the input lambda. The tilt. The Lagrange multiplier.</p>
</td></tr>
<tr><td><code>times1</code></td>
<td>
<p>the location of the hazard jumps. sample 1.</p>
</td></tr> 
<tr><td><code>times2</code></td>
<td>
<p>the location of the hazard jumps. sample 2.</p>
</td></tr>
<tr><td><code>wts1</code></td>
<td>
<p>the jump size of hazard function at those locations.</p>
</td></tr>
<tr><td><code>wts2</code></td>
<td>
<p>the jump size of hazard function at those locations.</p>
</td></tr>
<tr><td><code>HazDiff2</code></td>
<td>
<p>Difference of two hazard integrals. theta defined above.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Mai Zhou </p>


<h3>References</h3>

<p>Pan, X. and Zhou, M. (2002),
&ldquo;Empirical likelihood in terms of hazard for censored data&rdquo;. 
<em>Journal of Multivariate Analysis</em> <b>80</b>, 166-188.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## fun &lt;- function(x) { as.numeric(x &lt;= 6.5) }
## emplikH1.test( x=c(1,2,3,4,5), d=c(1,1,0,1,1), theta=2, fun=fun) 
## fun2 &lt;- function(x) {exp(-x)}  
## emplikH1.test( x=c(1,2,3,4,5), d=c(1,1,0,1,1), theta=0.2, fun=fun2) 
</code></pre>

<hr>
<h2 id='emplikH2P'>Return Poisson Empirical likelihood ratio for the given lambda, with 2-sample right censored data</h2><span id='topic+emplikH2P'></span>

<h3>Description</h3>

<p>Compute the Poisson empirical likelihood ratio for the given tilt parameter lambda.
Most useful when construct Wilks confidence intervals. 
The null hypothesis or constraint is defined by the parameter <code class="reqn">\theta</code>, where 
</p>
<p style="text-align: center;"><code class="reqn">\int fun1(t) dH_1(t) - \int fun2(t) dH_2(t) = \theta </code>
</p>
<p>.
</p>
<p>If the lambda value set to zero, you get the Nelson-Aalen (NPMLE) and output -2LLR =0.
For other values of lambda, this is not scaled (like in one sample) since there are two samples and it can be
confusing which sample size to use. So here, the lambda is larger (with a scale of sample size) than those in 
one sample: <code>emplikH1P</code>.
</p>
<p>Where <code class="reqn">H_1(t)</code> is the unknown
cumulative hazard
function of sample one; <code class="reqn">H_2(t)</code> is the cumulative function of sample two; 
<code class="reqn">fun1(t)</code> can be any given function. 
In the future, the function <code class="reqn">fun1</code> may replaced by the vector of <code class="reqn">fun(x)</code>, 
since this is more flexible. Same for <code class="reqn">fun2</code>.
</p>
<p>Input data can be right censored. If no censoring, set <code>d1=rep(1, length(x1))</code>, and/or <code>d2=rep(1, length(x2))</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>emplikH2P(lambda, x1, d1, x2, d2, fun1, fun2, CIforTheta=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="emplikH2P_+3A_lambda">lambda</code></td>
<td>
<p>a scalar. Can be positive or negative. The amount of tiling.</p>
</td></tr>
<tr><td><code id="emplikH2P_+3A_x1">x1</code></td>
<td>
<p>a vector of the censored survival times. sample one.</p>
</td></tr>
<tr><td><code id="emplikH2P_+3A_d1">d1</code></td>
<td>
<p>a vector of the censoring indicators, 1-uncensor; 0-right censor.</p>
</td></tr>
<tr><td><code id="emplikH2P_+3A_x2">x2</code></td>
<td>
<p>a vector of the censored survival times. sample two.</p>
</td></tr>
<tr><td><code id="emplikH2P_+3A_d2">d2</code></td>
<td>
<p>a vector of the censoring indicators, 1-uncensor; 0-right censor.</p>
</td></tr>
<tr><td><code id="emplikH2P_+3A_fun1">fun1</code></td>
<td>
<p>a left continuous (weight) function used to calculate 
the weighted hazard in the parameter <code class="reqn">\theta</code>. <code>fun1</code> must be able 
to take a vector input. See example below.</p>
</td></tr>
<tr><td><code id="emplikH2P_+3A_fun2">fun2</code></td>
<td>
<p>Ditto.</p>
</td></tr>
<tr><td><code id="emplikH2P_+3A_cifortheta">CIforTheta</code></td>
<td>
<p>an optional logical value. Default to FALSE. If set to TRUE,
will return the integrated hazard value for the given lambda.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is for calculate lambda confidence intervals for <code class="reqn">\theta</code>.
</p>
<p>This function is designed for the case where the 
true distribution should be continuous.
So there should be no tie in the data.
</p>
<p>The log empirical likelihood used here is the &lsquo;Poisson&rsquo; version empirical likelihood:
</p>
<p style="text-align: center;"><code class="reqn">  
EL1 = \sum_{i=1}^n \delta_i \log (dH_1(x_i))  - [ H_1(x_i) ]  ~,
</code>
</p>

<p>(similarly defined for sample 2) and the final EL is the sum of EL1 and EL2.
</p>
<p>If there are ties in the data that are resulted from rounding,
you may break the tie by adding a different tiny number to the tied
observation(s). For example: 2, 2, 2, change to 2.00001, 2.00002, 2.00003.
If those are true ties 
(thus the true distribution must be discrete)
we recommend use <code>emplikH2B</code>. 
</p>


<h3>Value</h3>

<p>A list with the following components: 
</p>
<table>
<tr><td><code>"-2LLR"</code></td>
<td>
<p>the -2Log Empirical Likelihood ratio, Poisson version.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>The tilt parameter used. It is also the Lagrange multiplier.</p>
</td></tr>
<tr><td><code>"-2LLR(sample1)"</code></td>
<td>
<p>the -2Log EL ratio, sample 1, Poisson version. &quot;-2LLR&quot; = -2LLR(sample1) + -2LLR(sample2)</p>
</td></tr>
<tr><td><code>HazDiff</code></td>
<td>
<p>Average hazard for the constrained hazard integral, if CIforTheta =TRUE.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Mai Zhou </p>


<h3>References</h3>

<p>Pan, X. and Zhou, M. (2002),
&ldquo;Empirical likelihood in terms of hazard for censored data&rdquo;. 
<em>Journal of Multivariate Analysis</em> <b>80</b>, 166-188.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## fun &lt;- function(x) { as.numeric(x &lt;= 6.5) }
## emplikH1.test( x=c(1,2,3,4,5), d=c(1,1,0,1,1), theta=2, fun=fun) 
## fun2 &lt;- function(x) {exp(-x)}  
## emplikH1.test( x=c(1,2,3,4,5), d=c(1,1,0,1,1), theta=0.2, fun=fun2) 
</code></pre>

<hr>
<h2 id='emplikHs.disc2'>Two sample empirical likelihood ratio for discrete hazards
with right censored, left truncated data. Many constraints.</h2><span id='topic+emplikHs.disc2'></span>

<h3>Description</h3>

<p>Use empirical likelihood ratio and Wilks theorem to test 
the null hypothesis that
</p>
<p style="text-align: center;"><code class="reqn"> 
\int{f_1(t) I_{[dH_1 &lt;1]} \log(1-dH_1(t))} -
\int{f_2(t) I_{[dH_2 &lt;1]} \log(1-dH_2(t))} = \theta 
</code>
</p>

<p>where <code class="reqn">H_*(t)</code> are the (unknown) discrete cumulative
hazard functions; <code class="reqn">f_*(t)</code> can be any predictable  
functions of <code class="reqn">t</code>. 
<code class="reqn">\theta</code> is a vector of parameters (dim=q &gt;= 1). 
The given value of <code class="reqn">\theta</code>
in these computation are the value to be tested.
The data can be right censored and left truncated.
</p>
<p>When the given constants <code class="reqn">\theta</code> is too far
away from the NPMLE, there will be no hazard function satisfy this 
constraint and the -2 Log empirical likelihood ratio
will be infinite. In this case the computation will stop.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>emplikHs.disc2(x1, d1, y1= -Inf, x2, d2, y2 = -Inf,
          theta, fun1, fun2, maxit=25,tola = 1e-6, itertrace =FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="emplikHs.disc2_+3A_x1">x1</code></td>
<td>
<p>a vector, the observed survival times, sample 1.</p>
</td></tr>
<tr><td><code id="emplikHs.disc2_+3A_d1">d1</code></td>
<td>
<p>a vector, the censoring indicators, 1-uncensor; 0-censor.</p>
</td></tr>
<tr><td><code id="emplikHs.disc2_+3A_y1">y1</code></td>
<td>
<p>optional vector, the left truncation times.</p>
</td></tr>
<tr><td><code id="emplikHs.disc2_+3A_x2">x2</code></td>
<td>
<p>a vector, the observed survival times, sample 2.</p>
</td></tr>
<tr><td><code id="emplikHs.disc2_+3A_d2">d2</code></td>
<td>
<p>a vector, the censoring indicators, 1-uncensor; 0-censor.</p>
</td></tr>
<tr><td><code id="emplikHs.disc2_+3A_y2">y2</code></td>
<td>
<p>optional vector, the left truncation times.</p>
</td></tr>
<tr><td><code id="emplikHs.disc2_+3A_fun1">fun1</code></td>
<td>
<p>a predictable function used to calculate
the weighted discrete hazard in <code class="reqn">H_0</code>. 
<code>fun1(x)</code> must be able to take a vector input (length n)
<code>x</code>, and output a matrix of n x q.</p>
</td></tr>
<tr><td><code id="emplikHs.disc2_+3A_fun2">fun2</code></td>
<td>
<p> Ditto.</p>
</td></tr>
<tr><td><code id="emplikHs.disc2_+3A_tola">tola</code></td>
<td>
<p>an optional positive real number, the tolerance of
iteration error in solve the non-linear equation needed in constrained 
maximization.</p>
</td></tr>
<tr><td><code id="emplikHs.disc2_+3A_theta">theta</code></td>
<td>
<p>a given vector of length q. for Ho constraint. </p>
</td></tr>
<tr><td><code id="emplikHs.disc2_+3A_maxit">maxit</code></td>
<td>
<p>integer, maximum number of iteration. </p>
</td></tr>
<tr><td><code id="emplikHs.disc2_+3A_itertrace">itertrace</code></td>
<td>
<p> Logocal, lower bound for lambda </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The log empirical likelihood been maximized is the &lsquo;binomial empirical likelihood&rsquo;:
</p>
<p style="text-align: center;"><code class="reqn"> 
\sum D_{1i} \log w_i  + (R_{1i}-D_{1i}) \log [1-w_i] + 
\sum D_{2j} \log v_j  + (R_{2j}-D_{2j}) \log [1-v_j] 
</code>
</p>

<p>where <code class="reqn">w_i = \Delta H_1(t_i)</code> is the jump 
of the cumulative hazard function at <code class="reqn">t_i</code>, 
<code class="reqn">D_{1i}</code> is the number of failures
observed at <code class="reqn">t_i</code>, and <code class="reqn">R_{1i}</code> is 
the number of subjects at risk at
time <code class="reqn">t_i</code> (for sample one). Similar for sample two.
</p>
<p>For discrete distributions, the jump size of the cumulative hazard at
the last jump is always 1. We have to exclude this jump from the 
summation in the constraint calculation
since <code class="reqn"> \log( 1- dH(\cdot))</code> do not make sense.
In the likelihood, this term contribute a zero (0*Inf).
</p>
<p>This function can handle multiple constraints. So dim( <code>theta</code>) = q.
The constants <code>theta</code> must be inside the so called
feasible region for the computation to continue. This is similar to the
requirement that in testing the value of the mean, the value must be
inside the convex hull of the observations.
It is always true that the NPMLE values are feasible. So when the
computation stops, try move the <code>theta</code> closer
to the NPMLE.  When the computation stops, the -2LLR should have value
infinite.
</p>
<p>This code can also be used to compute one sample problems.
You need to artificially supply data for sample two
(with minimal sample size (2q+2)), and supply a function 
<code>fun2</code> that ALWAYS returns zero (zero vector or zero matrix).
In the output, read the -2LLR(sample1).
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>times1</code></td>
<td>
<p>the location of the hazard jumps in sample 1.</p>
</td></tr>
<tr><td><code>times2</code></td>
<td>
<p>the location of the hazard jumps in sample 2.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>the final value of the Lagrange multiplier.</p>
</td></tr>
<tr><td><code>"-2LLR"</code></td>
<td>
<p>The -2Log Likelihood ratio.</p>
</td></tr>
<tr><td><code>"-2LLR(sample1)"</code></td>
<td>
<p>The -2Log Likelihood ratio for sample 1 only.</p>
</td></tr>
<tr><td><code>niters</code></td>
<td>
<p>number of iterations used</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Mai Zhou </p>


<h3>References</h3>

<p>Zhou and Fang (2001). 
&ldquo;Empirical likelihood ratio for 2 sample problems for censored data&rdquo;. 
<em>Tech Report, Univ. of Kentucky, Dept of Statistics</em>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(require("boot", quietly = TRUE)) {
####library(boot)
data(channing)
ymale &lt;- channing[1:97,2]
dmale &lt;- channing[1:97,5]
xmale &lt;- channing[1:97,3]
yfemale &lt;- channing[98:462,2]
dfemale &lt;- channing[98:462,5]
xfemale &lt;- channing[98:462,3]
fun1 &lt;- function(x) { as.numeric(x &lt;= 960) }
########################################################
emplikHs.disc2(x1=xfemale, d1=dfemale, y1=yfemale, 
 x2=xmale, d2=dmale, y2=ymale, theta=0.25, fun1=fun1, fun2=fun1)
########################################################
### This time you get "-2LLR" = 1.150098 etc. etc.
##############################################################
fun2 &lt;- function(x){ cbind(as.numeric(x &lt;= 960), as.numeric(x &lt;= 860))}
############ fun2 has matrix output ###############
emplikHs.disc2(x1=xfemale, d1=dfemale, y1=yfemale, 
 x2=xmale, d2=dmale, y2=ymale, theta=c(0.25,0), fun1=fun2, fun2=fun2)
################# you get "-2LLR" = 1.554386, etc ###########
}
</code></pre>

<hr>
<h2 id='emplikHs.test2'>Two sample empirical likelihood ratio test for hazards
with right censored, left truncated data. Many constraints.</h2><span id='topic+emplikHs.test2'></span>

<h3>Description</h3>

<p>Use empirical likelihood ratio and Wilks theorem to test 
the null hypothesis that
</p>
<p style="text-align: center;"><code class="reqn"> 
\int{f_1(t) dH_1(t)} -
\int{f_2(t) dH_2(t)} = \theta 
</code>
</p>

<p>where <code class="reqn">H_*(t)</code> is the (unknown) cumulative
hazard functions; <code class="reqn">f_*(t)</code> can be any predictable  
functions of <code class="reqn">t</code>. 
<code class="reqn">\theta</code> is a vector of parameters (dim=q). 
The given value of <code class="reqn">\theta</code>
in these computation are the value to be tested.
The data can be right censored and left truncated.
</p>
<p>When the given constants <code class="reqn">\theta</code> is too far
away from the NPMLE, there will be no hazard function satisfy this 
constraint and the -2 Log empirical likelihood ratio
will be infinite. In this case the computation will stop.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>emplikHs.test2(x1, d1, y1= -Inf, x2, d2, y2 = -Inf,
          theta, fun1, fun2, maxit=25,tola = 1e-7,itertrace =FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="emplikHs.test2_+3A_x1">x1</code></td>
<td>
<p>a vector of length n1, the observed survival times, sample 1.</p>
</td></tr>
<tr><td><code id="emplikHs.test2_+3A_d1">d1</code></td>
<td>
<p>a vector, the censoring indicators, 1-uncensor; 0-censor.</p>
</td></tr>
<tr><td><code id="emplikHs.test2_+3A_y1">y1</code></td>
<td>
<p>optional vector, the left truncation times.</p>
</td></tr>
<tr><td><code id="emplikHs.test2_+3A_x2">x2</code></td>
<td>
<p>a vector of length n2, the observed survival times, sample 2.</p>
</td></tr>
<tr><td><code id="emplikHs.test2_+3A_d2">d2</code></td>
<td>
<p>a vector, the censoring indicators, 1-uncensor; 0-censor.</p>
</td></tr>
<tr><td><code id="emplikHs.test2_+3A_y2">y2</code></td>
<td>
<p>optional vector, the left truncation times.</p>
</td></tr>
<tr><td><code id="emplikHs.test2_+3A_fun1">fun1</code></td>
<td>
<p>a predictable function used to calculate
the weighted discrete hazard to form the null hypothesis <code class="reqn">H_0</code>. 
<code>fun1(x)</code> must be able to take a vector input (length n1)
<code>x</code>, and output a matrix of n1 x q. 
When q=1, the output can also be a vector.</p>
</td></tr>
<tr><td><code id="emplikHs.test2_+3A_fun2">fun2</code></td>
<td>
<p> Ditto. but for length n2</p>
</td></tr>
<tr><td><code id="emplikHs.test2_+3A_tola">tola</code></td>
<td>
<p>an optional positive real number, the tolerance of
iteration error in solve the non-linear equation needed in constrained 
maximization.</p>
</td></tr>
<tr><td><code id="emplikHs.test2_+3A_theta">theta</code></td>
<td>
<p>a given vector of length q. for Ho constraint. </p>
</td></tr>
<tr><td><code id="emplikHs.test2_+3A_maxit">maxit</code></td>
<td>
<p>integer, maximum number of Newton-Raphson type iterations. </p>
</td></tr>
<tr><td><code id="emplikHs.test2_+3A_itertrace">itertrace</code></td>
<td>
<p>Logocal, 
if the results of each iteration needs to be printed. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The log likelihood been maximized is the Poisson likelihood:
</p>
<p style="text-align: center;"><code class="reqn"> 
\sum D_{1i} \log w_i  - \sum R_{1i} w_i + 
\sum D_{2j} \log v_j  - \sum R_{2j} v_j 
</code>
</p>

<p>where <code class="reqn">w_i = \Delta H_1(t_i)</code> is the jump 
of the cumulative hazard function at <code class="reqn">t_i</code>
(for first sample), 
<code class="reqn">D_{1i}</code> is the number of failures
observed at <code class="reqn">t_i</code>, <code class="reqn">R_{1i}</code> is 
the number of subjects at risk at
time <code class="reqn">t_i</code>. Dido for sample two.
</p>
<p>For (proper)
discrete distributions, the jump size of the cumulative hazard at
the last jump is always 1. So, in the likelihood ratio, it cancels.
But the last jump of size 1 still matter when computing the constraint.
</p>
<p>The constants <code>theta</code> must be inside the so called
feasible region for the computation to continue. This is similar to the
requirement that in testing the value of the mean, the value must be
inside the convex hull of the observations.
It is always true that the NPMLE values are feasible. So when the
computation stops, try move the <code>theta</code> closer
to the NPMLE, which we print out first thing in this function,
even when other later computations do not go. 
When the computation stops, the -2LLR should have value
infinite.
</p>
<p>This function uses the llog etc. function, so sometimes it may produce
different result from the one sample result. which use the regular log function.
The advantage is that we avoid the possible log(0) situation.
</p>
<p>You can also use this function for one sample problems. 
You need to artificially supply data for sample two of minimal size
(like size 2q+2), and specify a fun2() that ALWAYS return 0's
(zero vector, with length=n2 vector length, 
or zero matrix,  with dim n2 x q as the input).
Then, look for -2LLR(sample1) in the output.
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>"-2LLR"</code></td>
<td>
<p>The -2Log empirical Likelihood ratio.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>the final value of the Lagrange multiplier.</p>
</td></tr>
<tr><td><code>"-2LLR(sample1)"</code></td>
<td>
<p>The -2Log empirical likelihood ratio for sample 
one only. Useful in one sample problems.</p>
</td></tr>
<tr><td><code>"Llog(sample1)"</code></td>
<td>
<p>The numerator only of the above &quot;-2LLR(sample1)&quot;, without -2.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Mai Zhou </p>


<h3>References</h3>

<p>Zhou and Fang (2001). 
&ldquo;Empirical likelihood ratio for 2 sample problems for censored data&rdquo;. 
<em>Tech Report, Univ. of Kentucky, Dept of Statistics</em>
</p>


<h3>See Also</h3>

<p> emplikH2.test </p>


<h3>Examples</h3>

<pre><code class='language-R'>if(require("boot", quietly = TRUE)) {
####library(boot)
data(channing)
ymale &lt;- channing[1:97,2]
dmale &lt;- channing[1:97,5]
xmale &lt;- channing[1:97,3]
yfemale &lt;- channing[98:462,2]
dfemale &lt;- channing[98:462,5]
xfemale &lt;- channing[98:462,3]
fun1 &lt;- function(x) { as.numeric(x &lt;= 960) }
########################################################
fun2 &lt;- function(x){ cbind(as.numeric(x &lt;= 960), as.numeric(x &lt;= 860))}
############ fun2 has matrix output ###############
emplikHs.test2(x1=xfemale, d1=dfemale, y1=yfemale, 
 x2=xmale, d2=dmale, y2=ymale, theta=c(0,0), fun1=fun2, fun2=fun2)
}
#############################################
###################### Second example:
if(require("KMsurv", quietly = TRUE)) {
####library(KMsurv)
data(kidney)
### these functions counts the risk set size, so delta=1 always ###
temp1 &lt;- Wdataclean3(z=kidney$time[kidney[,3]==1], d=rep(1,43) )
temp2 &lt;- DnR(x=temp1$value, d=temp1$dd, w=temp1$weight)
TIME &lt;- temp2$times
RISK &lt;- temp2$n.risk
fR1 &lt;- approxfun(x=TIME, y=RISK, method="constant", yright=0, rule=2, f=1)
temp1 &lt;- Wdataclean3(z=kidney$time[kidney[,3]==2], d=rep(1,76) )
temp2 &lt;- DnR(x=temp1$value, d=temp1$dd, w=temp1$weight)
TIME &lt;- temp2$times
RISK &lt;- temp2$n.risk
fR2 &lt;- approxfun(x=TIME, y=RISK, method="constant", yright=0, rule=2, f=1)

### the weight function for two sample Gehan-Wilcoxon type test ###
fun &lt;- function(t){ fR1(t)*fR2(t)/((76*43)*sqrt(119/(76*43)) )}
### Here comes the test: ###
emplikHs.test2(x1=kidney[kidney[,3]==1,1],d1=kidney[kidney[,3]==1,2],
   x2=kidney[kidney[,3]==2,1],d2=kidney[kidney[,3]==2,2],
   theta=0, fun1= fun, fun2=fun)
### The results should include this ###
#$"-2LLR"
#[1] 0.002473070
#
#$lambda
#[1] -0.1713749
#######################################
######### the weight function for log-rank test #####
funlogrank &lt;- function(t){sqrt(119/(76*43))*fR1(t)*fR2(t)/(fR1(t)+fR2(t))}
##### Now the log-rank test ###
emplikHs.test2(x1=kidney[kidney[,3]==1,1],d1=kidney[kidney[,3]==1,2],
  x2=kidney[kidney[,3]==2,1],d2=kidney[kidney[,3]==2,2],
  theta=0, fun1=funlogrank, fun2=funlogrank)
##### The result of log rank test should include this ###
#
#$"-2LLR"
#[1] 2.655808
#
#$lambda
#[1] 3.568833
#######################################################
###### the weight function for both type test ####
funBOTH &lt;- function(t) {
       cbind(sqrt(119/(76*43))*fR1(t)*fR2(t)/(fR1(t)+fR2(t)), 
                 fR1(t)*fR2(t)/((76*43)*sqrt(119/(76*43)))) }
#### The test that combine both tests ###
emplikHs.test2(x1=kidney[kidney$type==1,1],d1=kidney[kidney$type==1,2],
    x2=kidney[kidney$type==2,1],d2=kidney[kidney$type==2,2],
    theta=c(0,0), fun1=funBOTH, fun2=funBOTH)
#### the result should include this ###
#
#$"-2LLR"
#[1] 13.25476
#
#$lambda
#[1]  14.80228 -21.86733
##########################################
}
</code></pre>

<hr>
<h2 id='findLnew'>Find the Wilks Confidence Interval Lower Bound from the Given (empirical) Likelihood Ratio Function</h2><span id='topic+findLnew'></span>

<h3>Description</h3>

<p>This function is similar to <code>findUL2</code> but here the seeking of lower and upper bound are
separate (the other half is <code>findUnew</code>).
</p>
<p>The reason for this is: sometime we need to supply the <code>fun</code>
with different nuisance parameter(s) values when seeking Lower or Upper bound.
For example <code>fun</code> returns the -2LLR for a given parameter of interest, 
but there are additional nuisance parameter need to be profiled out, and we 
need to give a range of the nuisance parameter to be max/min over. This range can be 
very different for parameter near Upper bound vs near Lower bound. In the <code>findUL2</code>,
you have to supply a range really wide that (hopefully) works for both Upper and Lower
bound. Here, with separate <code>findLnew</code> and <code>findUnew</code> we can tailor the range
for one end of the confidence interval.
</p>
<p>Those nuisance parameter(s)
are supplied via the ... input of this function.
</p>
<p>Another improvement is that we used the &quot;extendInt&quot; option of the <code>uniroot</code>.
So now we can and did used a smaller default step input, compare to <code>findUL2</code>.
</p>
<p>This program uses uniroot( ) to find (only) the lower (Wilks) confidence
limit based on the -2 log likelihood ratio, which the required 
input <code>fun</code> is supposed to supply.
</p>
<p>Basically, starting from <code>MLE</code>, we search on lower
direction, by <code>step</code> away
from <code>MLE</code>, until we find values that have -2LLR = level.
(the value of -2LLR at MLE is supposed to be zero.)
</p>
<p>At curruent implimentation, only handles one dimesional parameter, i.e. only
confidence intervals, not confidence regions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>findLnew(step=0.003, initStep=0, fun, MLE, level=3.84146, tol=.Machine$double.eps^0.5,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="findLnew_+3A_step">step</code></td>
<td>
<p>a positive number. The starting step size of the search. Reasonable value should be about 1/5 of the SD of MLE. </p>
</td></tr>
<tr><td><code id="findLnew_+3A_initstep">initStep</code></td>
<td>
<p>a nonnegative number. The first step size of the search. Sometimes, you may want to put a larger innitStep to speed the search.</p>
</td></tr>
<tr><td><code id="findLnew_+3A_fun">fun</code></td>
<td>
<p>a function that returns a list. One of the item in the list should be &quot;-2LLR&quot;, which is the -2 log (empirical) likelihood ratio. 
The first input of <code>fun</code> must be the parameter for which we are seeking the confidence interval. (The MLE or NPMLE of this parameter should be supplied as in the input MLE). The rest of the input to <code>fun</code> are typically the data.
If the first input of <code>fun</code> is set to MLE, then the returned -2LLR should be 0.</p>
</td></tr> 
<tr><td><code id="findLnew_+3A_mle">MLE</code></td>
<td>
<p>The MLE of the parameter. No need to be exact, as long as it is inside the confidence interval.</p>
</td></tr>
<tr><td><code id="findLnew_+3A_level">level</code></td>
<td>
<p>an optional positive number, controls the confidence level. Default to 3.84146 = chisq(0.95, df=1). 
Change to 2.70=chisq(0.90, df=1) to get a 90% confidence interval. </p>
</td></tr>
<tr><td><code id="findLnew_+3A_tol">tol</code></td>
<td>
<p>Error bound of the final result.</p>
</td></tr>
<tr><td><code id="findLnew_+3A_...">...</code></td>
<td>
<p>additional arguments, if any, to pass to <code>fun</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Basically we repeatedly testing the value of the parameter, until we find those
which the -2 log likelihood value is equal to 3.84146 (or other level, if set differently).
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>Low</code></td>
<td>
<p>the lower limit of the confidence interval.</p>
</td></tr>
<tr><td><code>FstepL</code></td>
<td>
<p>the final step size when search lower limit. An indication of the precision.</p>
</td></tr>
<tr><td><code>Lvalue</code></td>
<td>
<p>The -2LLR value of the final <code>Low</code> value. Should be approximately equal to level. If larger than level, 
than the confidence interval limit <code>Low</code> is wrong.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Mai Zhou </p>


<h3>References</h3>

<p>Zhou, M. (2016). Empirical Likelihood Method in Survival Analysis. CRC Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## example with tied observations. Kaplan-Meier mean=4.0659.
## For more examples see vignettes.
x &lt;- c(1, 1.5, 2, 3, 4, 5, 6, 5, 4, 1, 2, 4.5)
d &lt;- c(1,   1, 0, 1, 0, 1, 1, 1, 1, 0, 0,  1)
myfun6 &lt;- function(theta, x, d) {
el.cen.EM2(x, d, fun=function(t){t}, mu=theta)
}
findLnew(step=0.1, fun=myfun6, MLE=4.0659, x=x, d=d)
</code></pre>

<hr>
<h2 id='findUL'>Find the Wilks Confidence Interval from the Given (empirical) Likelihood Ratio Function</h2><span id='topic+findUL'></span>

<h3>Description</h3>

<p>This program uses uniroot( ) to find the upper and lower (Wilks) confidence
limits based on the -2 log likelihood ratio, which the required input <code>fun</code> is supposed to supply.
</p>
<p>Basically, starting from <code>MLE</code>, we search on both 
directions, by <code>step</code> away
from <code>MLE</code>, until we find values that have -2LLR = level.
(the value of -2LLR at MLE is supposed to be zero.)
</p>
<p>At curruent implimentation, only handles one dimesional parameter, i.e. only
confidence intervals, not confidence regions.
</p>
<p>For examples of using this function to find confidence interval,
see the pdf vignettes file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>findUL (step = 0.01, initStep =0, fun, MLE, level = 3.84146, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="findUL_+3A_step">step</code></td>
<td>
<p>a positive number. The starting step size of the search. Reasonable value should be about 1/5 of the SD of MLE. </p>
</td></tr>
<tr><td><code id="findUL_+3A_initstep">initStep</code></td>
<td>
<p>a nonnegative number. The first step size of the search. Sometimes, you may want to put a larger innitStep to speed the search.</p>
</td></tr>
<tr><td><code id="findUL_+3A_fun">fun</code></td>
<td>
<p>a function that returns a list. One of the item in the list should be &quot;-2LLR&quot;, which is the -2 log (empirical) likelihood ratio. 
The first input of <code>fun</code> must be the parameter for which we are seeking the confidence interval. (The MLE or NPMLE of this parameter should be supplied as in the input MLE). The rest of the input to <code>fun</code> are typically the data.
If the first input of <code>fun</code> is set to MLE, then the returned -2LLR should be 0.</p>
</td></tr> 
<tr><td><code id="findUL_+3A_mle">MLE</code></td>
<td>
<p>The MLE of the parameter. No need to be exact, as long as it is inside the confidence interval.</p>
</td></tr>
<tr><td><code id="findUL_+3A_level">level</code></td>
<td>
<p>an optional positive number, controls the confidence level. Default to 3.84 = chisq(0.95, df=1). Change to 2.70=chisq(0.90, df=1) to get a 90% confidence interval. </p>
</td></tr>
<tr><td><code id="findUL_+3A_...">...</code></td>
<td>
<p>additional arguments, if any, to pass to <code>fun</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Basically we repeatedly testing the value of the parameter, until we find those
which the -2 log likelihood value is equal to 3.84 (or other level, if set differently).
</p>
<p>If there is no value exactly equal to 3.84, it is better to use findULold( ), where
we stop at the value which result
a -2 log likelihood just below 3.84. (as in the discrete case, like quantiles.)
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>Low</code></td>
<td>
<p>the lower limit of the confidence interval.</p>
</td></tr>
<tr><td><code>Up</code></td>
<td>
<p>the upper limit of the confidence interval.</p>
</td></tr>
<tr><td><code>FstepL</code></td>
<td>
<p>the final step size when search lower limit. An indication of the precision.</p>
</td></tr>
<tr><td><code>FstepU</code></td>
<td>
<p>Ditto. An indication of the precision of the upper limit.</p>
</td></tr>
<tr><td><code>Lvalue</code></td>
<td>
<p>The -2LLR value of the final <code>Low</code> value. Should be approximately equal to level. If larger than level, than the confidence interval limit <code>Low</code> is wrong.</p>
</td></tr>
<tr><td><code>Uvalue</code></td>
<td>
<p>Ditto. Should be approximately equa to level.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Mai Zhou </p>


<h3>References</h3>

<p>Zhou, M. (2016). Empirical Likelihood Method in Survival Analysis. CRC Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## example with tied observations. Kaplan-Meier mean=4.0659.
## For more examples see vignettes.
x &lt;- c(1, 1.5, 2, 3, 4, 5, 6, 5, 4, 1, 2, 4.5)
d &lt;- c(1,   1, 0, 1, 0, 1, 1, 1, 1, 0, 0,  1)
myfun6 &lt;- function(theta, x, d) {
el.cen.EM2(x, d, fun=function(t){t}, mu=theta)
}
findUL(step=0.2, fun=myfun6, MLE=4.0659, x=x, d=d)
</code></pre>

<hr>
<h2 id='findUL2'>Find the Wilks Confidence Interval from the Given (empirical) Likelihood Ratio Function</h2><span id='topic+findUL2'></span>

<h3>Description</h3>

<p>This program uses simple search and uniroot( ) to find the upper and lower (Wilks) confidence
limits based on the -2 log likelihood ratio, which the required input <code>fun</code> is supposed to supply.
</p>
<p>This function is faster than <code>findUL( )</code>.
</p>
<p>Basically, starting from <code>MLE</code>, we search on both 
directions, by <code>step</code> away
from <code>MLE</code>, until we find values that have -2LLR = level.
(the value of -2LLR at MLE is supposed to be zero.)
</p>
<p>At curruent implimentation, only handles one dimesional parameter, i.e. only
confidence intervals, not confidence regions.
</p>
<p>For examples of using this function to find confidence interval,
see the pdf vignettes file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>findUL2(step=0.01, initStep=0, fun, MLE, level=3.84146, tol=.Machine$double.eps^0.5, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="findUL2_+3A_step">step</code></td>
<td>
<p>a positive number. The starting step size of the search. Reasonable value should be about 1/5 of the SD of MLE. </p>
</td></tr>
<tr><td><code id="findUL2_+3A_initstep">initStep</code></td>
<td>
<p>a nonnegative number. The first step size of the search. Sometimes, you may want to put a larger innitStep to speed the search.</p>
</td></tr>
<tr><td><code id="findUL2_+3A_fun">fun</code></td>
<td>
<p>a function that returns a list. One of the item in the list should be &quot;-2LLR&quot;, which is the -2 log (empirical) likelihood ratio. 
The first input of <code>fun</code> must be the parameter for which we are seeking the confidence interval. (The MLE or NPMLE of this parameter should be supplied as in the input MLE). The rest of the input to <code>fun</code> are typically the data.
If the first input of <code>fun</code> is set to MLE, then the returned -2LLR should be 0.</p>
</td></tr> 
<tr><td><code id="findUL2_+3A_mle">MLE</code></td>
<td>
<p>The MLE of the parameter. No need to be exact, as long as it is inside the confidence interval.</p>
</td></tr>
<tr><td><code id="findUL2_+3A_level">level</code></td>
<td>
<p>an optional positive number, controls the confidence level. Default to 3.84 = chisq(0.95, df=1). Change to 2.70=chisq(0.90, df=1) to get a 90% confidence interval. </p>
</td></tr>
<tr><td><code id="findUL2_+3A_tol">tol</code></td>
<td>
<p>tolerance to pass to uniroot( ). Default to .Machine$double.eps^0.5 </p>
</td></tr>
<tr><td><code id="findUL2_+3A_...">...</code></td>
<td>
<p>additional arguments, if any, to pass to <code>fun</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Basically we repeatedly testing the value of the parameter, until we find those
which the -2 log likelihood value is equal to 3.84 (or other level, if set differently).
</p>
<p>If there is no value exactly equal to 3.84, we stop at the value which result
a -2 log likelihood just below 3.84. (as in the discrete case, like quantiles.)
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>Low</code></td>
<td>
<p>the lower limit of the confidence interval.</p>
</td></tr>
<tr><td><code>Up</code></td>
<td>
<p>the upper limit of the confidence interval.</p>
</td></tr>
<tr><td><code>FstepL</code></td>
<td>
<p>the final step size when search lower limit. An indication of the precision.</p>
</td></tr>
<tr><td><code>FstepU</code></td>
<td>
<p>Ditto. An indication of the precision of the upper limit.</p>
</td></tr>
<tr><td><code>Lvalue</code></td>
<td>
<p>The -2LLR value of the final <code>Low</code> value. Should be approximately equal to level. If larger than level, than the confidence interval limit <code>Low</code> is wrong.</p>
</td></tr>
<tr><td><code>Uvalue</code></td>
<td>
<p>Ditto. Should be approximately equa to level.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Mai Zhou </p>


<h3>References</h3>

<p>Zhou, M. (2016). Empirical Likelihood Method in Survival Analysis. CRC Press.
</p>
<p>Zhou, M. (2002). 
Computing censored empirical likelihood ratio 
by EM algorithm. 
<em>JCGS</em>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## example with tied observations. Kaplan-Meier mean=4.0659.
## For more examples see vignettes.
x &lt;- c(1, 1.5, 2, 3, 4, 5, 6, 5, 4, 1, 2, 4.5)
d &lt;- c(1,   1, 0, 1, 0, 1, 1, 1, 1, 0, 0,  1)
myfun6 &lt;- function(theta, x, d) {
el.cen.EM2(x, d, fun=function(t){t}, mu=theta)
}
findUL2(step=0.2, fun=myfun6, MLE=4.0659, x=x, d=d)
</code></pre>

<hr>
<h2 id='findULold'>Find the Wilks Confidence Interval from the Given (empirical) Likelihood Ratio Function</h2><span id='topic+findULold'></span>

<h3>Description</h3>

<p>This program uses simple search to find the upper and lower (Wilks) confidence
limits based on the -2 log likelihood ratio, which the required input <code>fun</code> is supposed to supply.
</p>
<p>Basically, starting from <code>MLE</code>, we search on both 
directions, by <code>step</code> away
from <code>MLE</code>, until we find values that have -2LLR = level.
(the value of -2LLR at MLE is supposed to be zero.)
</p>
<p>At curruent implimentation, only handles one dimesional parameter, i.e. only
confidence intervals, not confidence regions.
</p>
<p>For examples of using this function to find confidence interval,
see the pdf vignettes file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>findULold (step = 0.01, initStep =0, fun, MLE, level = 3.84146, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="findULold_+3A_step">step</code></td>
<td>
<p>a positive number. The starting step size of the search. Reasonable value should be about 1/5 of the SD of MLE. </p>
</td></tr>
<tr><td><code id="findULold_+3A_initstep">initStep</code></td>
<td>
<p>a nonnegative number. The first step size of the search. Sometimes, you may want to put a larger innitStep to speed the search.</p>
</td></tr>
<tr><td><code id="findULold_+3A_fun">fun</code></td>
<td>
<p>a function that returns a list. One of the item in the list should be &quot;-2LLR&quot;, which is the -2 log (empirical) likelihood ratio. 
The first input of <code>fun</code> must be the parameter for which we are seeking the confidence interval. (The MLE or NPMLE of this parameter should be supplied as in the input MLE). The rest of the input to <code>fun</code> are typically the data.
If the first input of <code>fun</code> is set to MLE, then the returned -2LLR should be 0.</p>
</td></tr> 
<tr><td><code id="findULold_+3A_mle">MLE</code></td>
<td>
<p>The MLE of the parameter. No need to be exact, as long as it is inside the confidence interval.</p>
</td></tr>
<tr><td><code id="findULold_+3A_level">level</code></td>
<td>
<p>an optional positive number, controls the confidence level. Default to 3.84146 = chisq(0.95, df=1). Change to 2.70=chisq(0.90, df=1) to get a 90% confidence interval. </p>
</td></tr>
<tr><td><code id="findULold_+3A_...">...</code></td>
<td>
<p>additional arguments, if any, to pass to <code>fun</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Basically we repeatedly testing the value of the parameter, until we find those
which the -2 log likelihood value is equal to 3.84146 (or other level, if set differently).
</p>
<p>If there is no value exactly equal to 3.84146, we stop at the value which result
a -2 log likelihood just below 3.84146. (as in the discrete case, like quantiles.)
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>Low</code></td>
<td>
<p>the lower limit of the confidence interval.</p>
</td></tr>
<tr><td><code>Up</code></td>
<td>
<p>the upper limit of the confidence interval.</p>
</td></tr>
<tr><td><code>FstepL</code></td>
<td>
<p>the final step size when search lower limit. An indication of the precision.</p>
</td></tr>
<tr><td><code>FstepU</code></td>
<td>
<p>Ditto. An indication of the precision of the upper limit.</p>
</td></tr>
<tr><td><code>Lvalue</code></td>
<td>
<p>The -2LLR value of the final <code>Low</code> value. Should be approximately equal to level. If larger than level, than the confidence interval limit <code>Low</code> is wrong.</p>
</td></tr>
<tr><td><code>Uvalue</code></td>
<td>
<p>Ditto. Should be approximately equa to level.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Mai Zhou </p>


<h3>References</h3>

<p>Zhou, M. (2016). Empirical Likelihood Method in Survival Analysis. CRC Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## example with tied observations. Kaplan-Meier mean=4.0659.
## For more examples see vignettes.
x &lt;- c(1, 1.5, 2, 3, 4, 5, 6, 5, 4, 1, 2, 4.5)
d &lt;- c(1,   1, 0, 1, 0, 1, 1, 1, 1, 0, 0,  1)
myfun6 &lt;- function(theta, x, d) {
el.cen.EM2(x, d, fun=function(t){t}, mu=theta)
}
## findULold(step=0.2, fun=myfun6, MLE=4.0659, level = qchisq(0.9, df=1) , x=x, d=d)
</code></pre>

<hr>
<h2 id='findUnew'>Find the Wilks Confidence Interval Upper Bound from the Given (empirical) Likelihood Ratio Function</h2><span id='topic+findUnew'></span>

<h3>Description</h3>

<p>This function is similar to <code>findUL2</code> but here the seeking of lower and upper bound are
separate (the other half is <code>findLnew</code>).
</p>
<p>See the help file of <code>findLnew</code>.
Since sometimes the likelihood ratio is a Profile likelihood ratio and
we need to supply the <code>fun</code>
with different nuisance parameter(s) value(s) when seeking Lower or Upper bound. 
Those nuisance parameter(s)
are supplied via the ... input.
</p>
<p>Another improvement is that we used the &quot;extendInt&quot; option of the <code>uniroot</code>.
So now we can and did used a smaller default step input, 0.003, compare to <code>findUL2</code>.
</p>
<p>This program uses <code>uniroot( )</code> to find (only) the upper (Wilks) confidence
limit based on the -2 log likelihood ratio, which the required 
input <code>fun</code> is supposed to supply.
</p>
<p>Basically, starting from <code>MLE</code>, we search on upper/higher
direction, by <code>step</code> away
from <code>MLE</code>, until we find values that have -2LLR = level.
(the value of -2LLR at MLE is supposed to be zero.)
</p>
<p>At curruent implimentation, only handles one dimesional parameter, i.e. only
confidence intervals, not confidence regions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>findUnew(step=0.003, initStep=0, fun, MLE, level=3.84146, tol=.Machine$double.eps^0.5,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="findUnew_+3A_step">step</code></td>
<td>
<p>a positive number. The starting step size of the search. Reasonable value should be about 1/5 of the SD of MLE. </p>
</td></tr>
<tr><td><code id="findUnew_+3A_initstep">initStep</code></td>
<td>
<p>a nonnegative number. The first step size of the search. Sometimes, you may want to put a larger innitStep to speed the search.</p>
</td></tr>
<tr><td><code id="findUnew_+3A_fun">fun</code></td>
<td>
<p>a function that returns a list. One of the item in the list should be &quot;-2LLR&quot;, which is the -2 log (empirical) likelihood ratio. 
The first input of <code>fun</code> must be the parameter for which we are seeking the confidence interval. (The MLE or NPMLE of this parameter should be supplied as in the input MLE). The rest of the input to <code>fun</code> are typically the data.
If the first input of <code>fun</code> is set to MLE, then the returned -2LLR should be 0.</p>
</td></tr> 
<tr><td><code id="findUnew_+3A_mle">MLE</code></td>
<td>
<p>The MLE of the parameter. No need to be exact, as long as it is inside the confidence interval.</p>
</td></tr>
<tr><td><code id="findUnew_+3A_level">level</code></td>
<td>
<p>an optional positive number, controls the confidence level. Default to 3.84146 = chisq(0.95, df=1). 
Change to 2.70=chisq(0.90, df=1) to get a 90% confidence interval. </p>
</td></tr>
<tr><td><code id="findUnew_+3A_tol">tol</code></td>
<td>
<p>Error bound of the final result.</p>
</td></tr>
<tr><td><code id="findUnew_+3A_...">...</code></td>
<td>
<p>additional arguments, if any, to pass to <code>fun</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Basically we repeatedly testing the value of the parameter, until we find those
which the -2 log likelihood value is equal to 3.84146 (or other level, if set differently).
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>Up</code></td>
<td>
<p>the lower limit of the confidence interval.</p>
</td></tr>
<tr><td><code>FstepU</code></td>
<td>
<p>the final step size when search lower limit. An indication of the precision/error size.</p>
</td></tr>
<tr><td><code>Uvalue</code></td>
<td>
<p>The -2LLR value of the final <code>Up</code> value. Should be approximately equal to level. If larger than level, 
than the confidence interval limit <code>Up</code> is wrong.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Mai Zhou </p>


<h3>References</h3>

<p>Zhou, M. (2016). Empirical Likelihood Method in Survival Analysis. CRC Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## example with tied observations. Kaplan-Meier mean=4.0659.
## For more examples see vignettes.
x &lt;- c(1, 1.5, 2, 3, 4, 5, 6, 5, 4, 1, 2, 4.5)
d &lt;- c(1,   1, 0, 1, 0, 1, 1, 1, 1, 0, 0,  1)
myfun6 &lt;- function(theta, x, d) {
el.cen.EM2(x, d, fun=function(t){t}, mu=theta)
}
findUnew(step=0.1, fun=myfun6, MLE=4.0659, x=x, d=d)
</code></pre>

<hr>
<h2 id='myeloma'>Multiple Myeloma Data</h2><span id='topic+myeloma'></span>

<h3>Description</h3>

<p>Krall, Uthoff, and Harley (1975) analyzed data from a study
on multiple myeloma in which researchers treated 65 patients with
alkylating agents.  
</p>
<p>Of those patients, 48 died during the study
and 17 survived.  In the data set MYELOMA, the variable TIME
represents the survival time in months from diagnosis. The variable
VSTATUS consists of two values, 0 and 1, indicating whether the
patient was alive or dead, respectively, at the of end the study.
If the value of VSTATUS is 0, the corresponding value of TIME is
censored.  
</p>
<p>The variables thought to be related to survival are
LOGBUN (log BUN at diagnosis), HGB (hemoglobin at diagnosis),
PLATELET (platelets at diagnosis: 0=abnormal, 1=normal), 
AGE (age at diagnosis in years), LOGWBC (log WBC at diagnosis), FRAC
(fractures at diagnosis: 0=none, 1=present), LOGPBM (log percentage
of plasma cells in bone marrow), PROTEIN (proteinuria at 
diagnosis), and SCALC (serum calcium at diagnosis).
</p>
<p>Data are from 
<code>http://ftp.sas.com/techsup/download/sample/samp_lib/</code>
</p>
<p><code>statsampExamples_of_Coxs_Model.html</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(myeloma)</code></pre>


<h3>Format</h3>

<p>A data frame containing 65 observations on 11 variables:
</p>

<table>
<tr>
 <td style="text-align: right;">
        [,1] </td><td style="text-align: left;"> "time"</td>
</tr>
<tr>
 <td style="text-align: right;">
        [,2] </td><td style="text-align: left;"> "vstatus"</td>
</tr>
<tr>
 <td style="text-align: right;">
        [,3] </td><td style="text-align: left;"> "logBUN"</td>
</tr>
<tr>
 <td style="text-align: right;">
        [,4] </td><td style="text-align: left;"> "HGB"</td>
</tr>
<tr>
 <td style="text-align: right;">
        [,5] </td><td style="text-align: left;"> "platelet"</td>
</tr>
<tr>
 <td style="text-align: right;">
        [,6] </td><td style="text-align: left;"> "age"</td>
</tr>
<tr>
 <td style="text-align: right;">
        [,7] </td><td style="text-align: left;"> "logWBC"</td>
</tr>
<tr>
 <td style="text-align: right;">
        [,8] </td><td style="text-align: left;"> "FRAC"</td>
</tr>
<tr>
 <td style="text-align: right;">
        [,9] </td><td style="text-align: left;"> "logPBM"</td>
</tr>
<tr>
 <td style="text-align: right;">
        [,10] </td><td style="text-align: left;"> "protein"</td>
</tr>
<tr>
 <td style="text-align: right;">
        [,11] </td><td style="text-align: left;"> "SCALC"</td>
</tr>

</table>



<h3>References</h3>

<p>Krall, J.M., Uthoff, V.A., and Harley, J. B. (1975). 
A Step-up Procedure for Selecting Variables Associated with Survival. 
Biometrics, 31, 49-57.
</p>

<hr>
<h2 id='RankRegTest'>Test the AFT model Rank Regression estimator by Empirical Likelihood</h2><span id='topic+RankRegTest'></span>

<h3>Description</h3>

<p>Use the empirical likelihood ratio and Wilks theorem to test if the
regression coefficient is equal to beta, based on the rank estimator
for the AFT model.
</p>
<p>The log empirical likelihood been maximized is
</p>
<p style="text-align: center;"><code class="reqn"> \sum_{d=1} \log \Delta F(e_i) + \sum_{d=0} \log [1-F(e_i)];</code>
</p>

<p>where <code class="reqn">e_i</code> are the residuals.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RankRegTest(y, d, x, beta, type="Gehan")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RankRegTest_+3A_y">y</code></td>
<td>
<p>a vector of length N, containing the censored responses.</p>
</td></tr>
<tr><td><code id="RankRegTest_+3A_d">d</code></td>
<td>
<p>a vector (length N) of either 1's or 0's. 
d=1 means y is uncensored;
d=0 means y is right censored. </p>
</td></tr>
<tr><td><code id="RankRegTest_+3A_x">x</code></td>
<td>
<p>a matrix of size N by q. </p>
</td></tr>
<tr><td><code id="RankRegTest_+3A_beta">beta</code></td>
<td>
<p>a vector of length q. the value of the regression 
coefficient to be tested in the model 
<code class="reqn">y_i = \beta x_i  + \epsilon_i</code> </p>
</td></tr></table>
<p>.
</p>
<table>
<tr><td><code id="RankRegTest_+3A_type">type</code></td>
<td>
<p>default to Gehan type. 
The other option is Logrank type.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The estimator of beta can be obtained by function 
<code>rankaft( )</code>
in the package <code>rankreg</code>. But here you may test other values of 
beta. If you test the beta value that is obtained from the <code>rankaft( )</code>,
then the -2LLR should be 0 and the p-value should be 1.
</p>
<p>The above likelihood should be understood as the likelihood of the 
error term, so in the regression model the error <code class="reqn">e_i</code> should be iid.
</p>
<p>The estimation equation used when maximize the 
empirical likelihood is 
</p>
<p style="text-align: center;"><code class="reqn"> 0 = \sum_i \phi (e_i)  d_i \Delta F(e_i) (x_i - \bar x_i  )/(n w_i) </code>
</p>

<p>which was described in detail in the references below.
</p>
<p>See also the function <code>RankRegTestH</code>, which is based on the hazard likelihood.
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>"-2LLR"</code></td>
<td>
<p>the -2 loglikelihood ratio; should have approximate chisq 
distribution under <code class="reqn">H_o</code>.</p>
</td></tr>
<tr><td><code>logel2</code></td>
<td>
<p>the log empirical likelihood, under estimating equation.</p>
</td></tr>
<tr><td><code>logel</code></td>
<td>
<p>the log empirical likelihood of the Kaplan-Meier of e's.</p>
</td></tr>
<tr><td><code>prob</code></td>
<td>
<p>the probabilities that max the empirical likelihood 
under rank estimating equation constraint.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Mai Zhou. </p>


<h3>References</h3>

<p>Kalbfleisch, J. and Prentice, R. (2002)
The Statistical Analysis of Failure Time Data. 2nd Ed.
Wiley, New York.  (Chapter 7)
</p>
<p>Jin, Z., Lin, D.Y., Wei, L. J. and Ying, Z. (2003).
Rank-based inference for the accelerated failure time model.
Biometrika, <b>90</b>, 341-53.
</p>
<p>Zhou, M. (2005). Empirical likelihood analysis of the rank
estimator for the censored accelerated failure time model. 
Biometrika, <b>92</b>, 492-98.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(myeloma)
RankRegTest(y=myeloma[,1], d=myeloma[,2], x=myeloma[,3], beta= -15.50147)
# you should get "-2LLR" = 9.050426e-05 (practically zero)
# The beta value, -15.50147, was obtained by rankaft() from the rankreg package.
</code></pre>

<hr>
<h2 id='RankRegTestH'>Test the AFT model, Rank Regression estimator by (Hazard)Empirical Likelihood</h2><span id='topic+RankRegTestH'></span>

<h3>Description</h3>

<p>Use the empirical likelihood ratio and Wilks theorem to test if the
regression coefficient is equal to beta, based on the rank estimator/estimating equation
of the AFT model.
</p>
<p>The log empirical likelihood been maximized is the hazard empirical likelihood.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RankRegTestH(y, d, x, beta, type="Gehan")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RankRegTestH_+3A_y">y</code></td>
<td>
<p>a vector of length N, containing the censored responses.</p>
</td></tr>
<tr><td><code id="RankRegTestH_+3A_d">d</code></td>
<td>
<p>a vector (length N) of either 1's or 0's. 
d=1 means y is uncensored;
d=0 means y is right censored. </p>
</td></tr>
<tr><td><code id="RankRegTestH_+3A_x">x</code></td>
<td>
<p>a matrix of size N by q. </p>
</td></tr>
<tr><td><code id="RankRegTestH_+3A_beta">beta</code></td>
<td>
<p>a vector of length q. the value of the regression 
coefficient to be tested in the model 
<code class="reqn">y_i = \beta x_i  + \epsilon_i</code> </p>
</td></tr></table>
<p>.
</p>
<table>
<tr><td><code id="RankRegTestH_+3A_type">type</code></td>
<td>
<p>default to Gehan type. 
The other option is Logrank type.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The estimator of beta can be obtained by function 
<code>rankaft( )</code>
in the package <code>rankreg</code>. But here you may test other values of 
beta. If you test the beta value that is obtained from the <code>rankaft( )</code>,
then the -2LLR should be 0 and the p-value should be 1.
</p>
<p>The above likelihood should be understood as the likelihood of the 
error term, so in the regression model the error <code class="reqn">e_i</code> should be iid.
</p>
<p>The estimating equation used when maximize the 
empirical likelihood is 
</p>
<p style="text-align: center;"><code class="reqn"> 0 = \sum_i R(e_i) \phi (e_i)  d_i \Delta A(e_i) (x_i - \bar x_i  ) </code>
</p>

<p>where all notation was described in detail in the references below.
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>"-2LLR"</code></td>
<td>
<p>the -2 loglikelihood ratio; should have approximate chisq 
distribution under <code class="reqn">H_0</code>.</p>
</td></tr>
<tr><td><code>logel2</code></td>
<td>
<p>the log empirical likelihood, under estimating equation.</p>
</td></tr>
<tr><td><code>logel</code></td>
<td>
<p>the log empirical likelihood of the Kaplan-Meier of e's.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Mai Zhou </p>


<h3>References</h3>

<p>Zhou, M. (2016) Empirical Likelihood Methods in Survival
Analysis. CRC Press.
</p>
<p>Kalbfleisch, J. and Prentice, R. (2002)
The Statistical Analysis of Failure Time Data. 2nd Ed.
Wiley, New York.  (Chapter 7)
</p>
<p>Jin, Z., Lin, D.Y., Wei, L. J. and Ying, Z. (2003).
Rank-based inference for the accelerated failure time model.
Biometrika, <b>90</b>, 341-53.
</p>
<p>Zhou, M. (2005). Empirical likelihood analysis of the rank
estimator for the censored accelerated failure time
model. Biometrika, <b>92</b>, 492&ndash;498.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(myeloma)
RankRegTestH(y=myeloma[,1], d=myeloma[,2], x=myeloma[,3], beta= -15.50147)
# you should get "-2LLR" = 9.050426e-05 (practically zero)
# The beta value, -15.50147, was obtained by rankaft() from 
# the rankreg package.
</code></pre>

<hr>
<h2 id='ROCnp'>Test the ROC curve by Empirical Likelihood</h2><span id='topic+ROCnp'></span>

<h3>Description</h3>

<p>Use empirical likelihood ratio to test the
hypothesis Ho: (1-b0)th quantile of sample 1 = (1-t0)th quantile 
of sample 2.
This is the same as testing Ho: R(t0)= b0, where R(.) is the ROC curve.
</p>
<p>The log empirical likelihood been maximized is
</p>
<p style="text-align: center;"><code class="reqn"> \sum_{d1=1} \log \Delta F_1(t1_i) + \sum_{d1=0} \log [1-F_1(t1_i)] 
     + \sum_{d2=1} \log \Delta F_2(t2_j) + \sum_{d2=0} \log [1-F_2(t2_j)] .</code>
</p>

<p>This empirical likelihood ratio has a chi square limit under Ho.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ROCnp(t1, d1, t2, d2, b0, t0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ROCnp_+3A_t1">t1</code></td>
<td>
<p>a vector of length n. Observed times, may be right censored.</p>
</td></tr>
<tr><td><code id="ROCnp_+3A_d1">d1</code></td>
<td>
<p>a vector of length n, censoring status.
d=1 means t is uncensored; d=0 means t is right censored. </p>
</td></tr>
<tr><td><code id="ROCnp_+3A_t2">t2</code></td>
<td>
<p>a vector of length m. Observed times, may be right censored.</p>
</td></tr>
<tr><td><code id="ROCnp_+3A_d2">d2</code></td>
<td>
<p>a vector of length m, censoring status.</p>
</td></tr>
<tr><td><code id="ROCnp_+3A_b0">b0</code></td>
<td>
<p>a scalar between 0 and 1. </p>
</td></tr>
<tr><td><code id="ROCnp_+3A_t0">t0</code></td>
<td>
<p>a scalar, betwenn 0 and 1. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Basically, we first test (1-b0)th quantile of sample 1 = c
and also test (1-t0)th quantile of sample 2 = c. 
This way we obtain two log likelihood ratios. 
</p>
<p>Then we minimize the sum of the
two log likelihood ratio over c.
</p>
<p>See the tech report below for details on a similar setting.
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>"-2LLR"</code></td>
<td>
<p>the -2 loglikelihood ratio; have approximate chisq 
distribution under <code class="reqn">H_o</code>.</p>
</td></tr>
<tr><td><code>cstar</code></td>
<td>
<p>the estimated common quantile.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Mai Zhou. </p>


<h3>References</h3>

<p>Zhou, M. and Liang, H (2008). 
Empirical Likelihood for Hybrid Two Sample Problem with
Censored Data. <em>Univ. Kentucky Tech. Report.</em>
</p>
<p>Su, H., Zhou, M. and Liang, H. (2011). Semi-parametric
hybrid empirical likelihood inference for two-sample comparison with censored data. <em>Lifetime Data Analysis</em>,
<b>17</b>, 533-551. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#### An example of testing the equality of two medians. No censoring.
ROCnp(t1=rexp(100), d1=rep(1,100), t2=rexp(120), d2=rep(1,120), b0=0.5, t0=0.5)
##########################################################################
#### Next, an example of finding 90 percent confidence interval of R(0.5)
####  Note: We are finding confidence interval for R(0.5). So we are testing  
####  R(0.5)= 0.35, 0.36, 0.37, 0.38, etc. try to find values so that 
####  testing R(0.5) = L , U  has p-value of 0.10,  then [L,  U] is the 90 percent
####  confidence interval for R(0.5).
#set.seed(123)
#t1 &lt;- rexp(200)
#t2 &lt;- rexp(200)
#ROCnp( t1=t1, d1=rep(1, 200), t2=t2, d2=rep(1, 200), b0=0.5, t0=0.5)$"-2LLR"
#### since the -2LLR value is less than  2.705543 = qchisq(0.9, df=1),  so the 
#### confidence interval contains 0.5.
#gridpoints &lt;- 35:65/100
#ELvalues &lt;- gridpoints
#for( i in 1:31 ) ELvalues[i] &lt;- ROCnp(t1=t1, d1=rep(1, 200), 
#                 t2=t2, d2=rep(1, 200), b0=gridpoints[i], t0=0.5)$"-2LLR"
#myfun1 &lt;- approxfun(x=gridpoints, y=ELvalues)
#uniroot( f= function(x){myfun1(x)-2.705543}, interval= c(0.35, 0.5) )
#uniroot( f= function(x){myfun1(x)-2.705543}, interval= c(0.5, 0.65) )
#### So, taking the two roots, we see the 90 percent confidence interval for R(0.5) 
#### in this case is [0.4478081,  0.5889425].
</code></pre>

<hr>
<h2 id='ROCnp2'>Test the ROC curve by Empirical Likelihood</h2><span id='topic+ROCnp2'></span>

<h3>Description</h3>

<p>Use empirical likelihood ratio to test the
hypothesis Ho: (1-b0)th quantile of sample 1 = (1-t0)th quantile 
of sample 2.
This is the same as testing Ho: R(t0)= b0, where R(.) is the ROC curve.
</p>
<p>The log empirical likelihood been maximized is
</p>
<p style="text-align: center;"><code class="reqn"> \sum_{d1=1} \log \Delta F_1(t1_i) + \sum_{d1=0} \log [1-F_1(t1_i)] 
     + \sum_{d2=1} \log \Delta F_2(t2_j) + \sum_{d2=0} \log [1-F_2(t2_j)] .</code>
</p>

<p>This empirical likelihood ratio has a chi square limit under Ho.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ROCnp2(t1, d1, t2, d2, b0, t0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ROCnp2_+3A_t1">t1</code></td>
<td>
<p>a vector of length n. Observed times, sample 1. may be right censored.</p>
</td></tr>
<tr><td><code id="ROCnp2_+3A_d1">d1</code></td>
<td>
<p>a vector of length n, censoring status.
d=1 means t is uncensored; d=0 means t is right censored. </p>
</td></tr>
<tr><td><code id="ROCnp2_+3A_t2">t2</code></td>
<td>
<p>a vector of length m. Observed times, sample 2. may be right censored.</p>
</td></tr>
<tr><td><code id="ROCnp2_+3A_d2">d2</code></td>
<td>
<p>a vector of length m, censoring status.</p>
</td></tr>
<tr><td><code id="ROCnp2_+3A_b0">b0</code></td>
<td>
<p>a scalar, between 0 and 1. </p>
</td></tr>
<tr><td><code id="ROCnp2_+3A_t0">t0</code></td>
<td>
<p>a scalar, betwenn 0 and 1. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>First, we test (1-b0)th quantile of sample 1 = c
and also test (1-t0)th quantile of sample 2 = c. 
This way we obtain two log likelihood ratios. 
</p>
<p>Then we minimize the sum of the
two log likelihood ratios over c.
</p>
<p>This version use an exhaust search for the minimum (over c). 
Since the objective (log lik) are piecewise constants, the
optimum( ) function in R do not work well.
See the tech report below for details on a similar setting.
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>"-2LLR"</code></td>
<td>
<p>the -2 loglikelihood ratio; have approximate chisq 
distribution under <code class="reqn">H_o</code>.</p>
</td></tr>
<tr><td><code>cstar</code></td>
<td>
<p>the estimated common quantile.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Mai Zhou</p>


<h3>References</h3>

<p>Su, Haiyan; Zhou, Mai and Liang, Hua (2011). 
Semi-parametric Hybrid Empirical Likelihood Inference for  Two Sample Comparison with Censored Data.  
<em>Lifetime Data Analysis</em>, <b>17</b>, 533-551.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#### An example of testing the equality of two medians. 
#### No censoring. 
# ROCnp2(t1=rexp(100), d1=rep(1,100), t2=rexp(120), 
#                              d2=rep(1,120), b0=0.5, t0=0.5)
###############################################################
#### This example do not work on the Solaris Sparc machine. 
#### But works fine on other platforms.
###########
#### Next, an example of finding 90 percent confidence 
#### interval of R(0.5)
####  Note: We are finding confidence interval for R(0.5). 
#### So we are testing  
####  R(0.5)= 0.35, 0.36, 0.37, 0.38, etc. try to find 
#### values so that testing R(0.5) = L , U has p-value 
#### of 0.10, then [L,  U] is the 90 percent
####  confidence interval for R(0.5).
#set.seed(123)
#t1 &lt;- rexp(200)
#t2 &lt;- rexp(200)
#ROCnp( t1=t1, d1=rep(1, 200), t2=t2, d2=rep(1, 200), 
#                                b0=0.5, t0=0.5)$"-2LLR"
#### since the -2LLR value is less than  
#### 2.705543 = qchisq(0.9, df=1),  so the 
#### confidence interval contains 0.5.
#gridpoints &lt;- 35:65/100
#ELvalues &lt;- gridpoints
#for(i in 1:31) ELvalues[i] &lt;- ROCnp2(t1=t1, d1=rep(1, 200), 
#      t2=t2, d2=rep(1, 200), b0=gridpoints[i], t0=0.5)$"-2LLR"
#myfun1 &lt;- approxfun(x=gridpoints, y=ELvalues)
#uniroot(f=function(x){myfun1(x)-2.705543}, 
#                                interval= c(0.35, 0.5) )
#uniroot(f= function(x){myfun1(x)-2.705543}, 
#                                interval= c(0.5, 0.65) )
#### So, taking the two roots, we see the 90 percent 
#### confidence interval for R(0.5) in this 
#### case is [0.4457862,  0.5907723].
###############################################
</code></pre>

<hr>
<h2 id='smallcell'>Smallcell Lung Cancer Data</h2><span id='topic+smallcell'></span>

<h3>Description</h3>

<p>There are 121 observations on 4 variables.
Arm is the indication of two treatments. (either C + E or E + C).
Entry is the age of the patient at entry.
Survival is the survival time and 
indicator is the censoring indicator (right censoring).
For more details please see the reference below.
</p>
<p>Data are from 
Ying, Z., Jung, SH, and Wei, LJ (1995). 
Median regression analysis with censored data.
Journal of the American Statistical Association,
90, 178-184.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(smallcell)</code></pre>


<h3>Format</h3>

<p>A data frame containing 121 observations on 4 variables:
</p>

<table>
<tr>
 <td style="text-align: right;">
        [,1] </td><td style="text-align: left;"> "arm"</td>
</tr>
<tr>
 <td style="text-align: right;">
        [,2] </td><td style="text-align: left;"> "entry"</td>
</tr>
<tr>
 <td style="text-align: right;">
        [,3] </td><td style="text-align: left;"> "survival"</td>
</tr>
<tr>
 <td style="text-align: right;">
        [,4] </td><td style="text-align: left;"> "indicator"</td>
</tr>

</table>



<h3>References</h3>

<p>Ying, Z., Jung, SH, and Wei, LJ
(1995). Median regression analysis with censored data.
<em>Journal of the American Statistical Association</em>, 90, 178-184.
</p>
<p>Maksymiuk, A. W., Jett, J. R., Earle, J. D., Su, J. Q., Diegert, F. A., Mailliard,
J. A., Kardinal, C. G., Krook, J. E., Veeder, M. H., Wiesenfeld, M.,
Tschetter, L. K., and Levitt, R. (1994). Sequencing and Schedule Effects
of Cisplatin Plus Etoposide in Small Cell Lung Cancer Results of a North
Central Cancer Treatment Group Randomized Clinical Trial. 
<em>Journal of Clinical Oncology</em>, 12, 70-76.
</p>

<hr>
<h2 id='WRegEst'>Compute the casewise weighted regression estimator for AFT model</h2><span id='topic+WRegEst'></span>

<h3>Description</h3>

<p>For the AFT model, this function computes the case weighted estimator of
beta. Either the least squares estimator or the regression quantile estimator.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>WRegEst(x, y, delta, LS=TRUE, tau=0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="WRegEst_+3A_x">x</code></td>
<td>
<p>a matrix of size N by q. </p>
</td></tr>
<tr><td><code id="WRegEst_+3A_y">y</code></td>
<td>
<p>a vector of length N, containing the censored responses. Usually 
the log of the original observed failure times. </p>
</td></tr>
<tr><td><code id="WRegEst_+3A_delta">delta</code></td>
<td>
<p>a vector (length N) of either 1's or 0's. 
d=1 means y is uncensored;
d=0 means y is right censored. </p>
</td></tr>
<tr><td><code id="WRegEst_+3A_ls">LS</code></td>
<td>
<p>a logical value. If TRUE then the function will 
return the least squares estimator. If FALSE then the
function will return the quantile regression estimator,
with the quantile level specified by tau. </p>
</td></tr></table>
<p>.
</p>
<table>
<tr><td><code id="WRegEst_+3A_tau">tau</code></td>
<td>
<p>a scalar, between 0 and 1. The quantile to be used in
quantile regression. If tau=0.5 then it is the median regression. If LS=TRUE, then it is ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Due to the readily available minimizer, we only provide least squares
and quantile regression here. However, in the companion testing function
<code>WRegTest </code> the user can supply a self defined psi function,
corresponding to the general M-estimation in the regression modeling.
(since there is no minimization needed).
</p>
<p>The estimator is the minimizer of
</p>
<p style="text-align: center;"><code class="reqn"> \sum_{i=1}^n w_i \rho (Y_i - X_i b) </code>
</p>

<p>Assuming a correlation model
</p>
<p style="text-align: center;"><code class="reqn"> Y_i = X_i \beta + \sigma(X_i) \epsilon_i </code>
</p>
<p>,
where <code class="reqn">\rho( )</code> is either the square or the absolute value function.
</p>


<h3>Value</h3>

<p>The estimator <code class="reqn"> \hat \beta</code>.
</p>


<h3>Author(s)</h3>

<p> Mai Zhou. </p>


<h3>References</h3>

<p>Zhou, M.; Bathke, A. and Kim, M. (2012). 
Empirical likelihood analysis of the
Heteroscastic Accelerated Failure Time model. 
<em>Statistica Sinica</em>, <b>22</b>, 295-316.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(smallcell)
WRegEst(x=cbind(1,smallcell[,1],smallcell[,2]), 
        y=smallcell[,3], delta=smallcell[,4]) 
####################################################
#### you should get         x1         x2         x3
####                 -59.22126 -488.41306   16.03259
####################################################
WRegEst(x=cbind(1,smallcell[,1],smallcell[,2]), 
        y=log10(smallcell[,3]), delta=smallcell[,4], LS=FALSE) 
########################################################
#### you should get      
####     [1]     2.603342985  -0.263000044  0.003836832
########################################################
</code></pre>

<hr>
<h2 id='WRegTest'>Test the case weighted regression estimator by Empirical Likelihood</h2><span id='topic+WRegTest'></span>

<h3>Description</h3>

<p>Use the empirical likelihood ratio and Wilks theorem to test if the
regression coefficient is equal to <code>beta0</code>, 
by the case weighted estimation method.
</p>
<p>The log empirical likelihood been maximized is
</p>
<p style="text-align: center;"><code class="reqn"> \sum_{d=1} \log \Delta F(y_i) + \sum_{d=0} \log [1-F(y_i)].</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>WRegTest(x, y, delta, beta0, psifun=function(t){t})
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="WRegTest_+3A_x">x</code></td>
<td>
<p>a matrix of size N by q. Random design matrix. </p>
</td></tr>
<tr><td><code id="WRegTest_+3A_y">y</code></td>
<td>
<p>a vector of length N, containing the censored responses.</p>
</td></tr>
<tr><td><code id="WRegTest_+3A_delta">delta</code></td>
<td>
<p>a vector (length N) of either 1's or 0's. 
delta=1 means y is uncensored;
delta=0 means y is right censored. </p>
</td></tr>
<tr><td><code id="WRegTest_+3A_beta0">beta0</code></td>
<td>
<p>a vector of length q. The value of the regression 
coefficient to be tested in the linear model </p>
</td></tr></table>
<p>.
</p>
<table>
<tr><td><code id="WRegTest_+3A_psifun">psifun</code></td>
<td>
<p>the estimating function. The definition of it determines
the type of estimator under testing. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The above likelihood should be understood as the likelihood of the 
censored responses <code>y</code> and <code>delta</code>. 
</p>
<p>This version can handle the model where beta is a vector (of length q).
</p>
<p>The estimation equations used when maximize the 
empirical likelihood is 
</p>
<p style="text-align: center;"><code class="reqn"> 0 = \sum \delta_i \Delta F(Y_i) X_i \psi( Y_i - X_i \beta0 ) </code>
</p>

<p>which was described in detail in the reference below.
</p>
<p>For median regression (Least Absolute Deviation) estimator, you should
define the 
<code>psifun</code> as <code class="reqn">+1, -1</code> or <code class="reqn">0</code> when <code class="reqn">t</code> is <code class="reqn">&gt;0, &lt;0 </code>
or <code class="reqn"> =0</code>.
</p>
<p>For ordinary least squares estimator, <code>psifun</code> should be the identity function psifun &lt;- function(t)t.
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>"-2LLR"</code></td>
<td>
<p>the -2 log likelihood ratio; have approximate chisq 
distribution under <code class="reqn">H_0</code>.</p>
</td></tr>
<tr><td><code>P-val</code></td>
<td>
<p>the p-value using the chi-square approximation.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Mai Zhou. </p>


<h3>References</h3>

<p>Zhou, M.; Kim, M. and Bathke, A. (2012). 
Empirical likelihood analysis of the case weighted estimator in 
heteroscastic AFT model.  
<em>Statistica Sinica</em>, <b>22</b>, 295-316.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>xx &lt;- c(28,-44,29,30,26,27,22,23,33,16,24,29,24,40,21,31,34,-2,25,19)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
