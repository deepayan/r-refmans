<!DOCTYPE html><html><head><title>Help for package rbenchmark</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {rbenchmark}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#benchmark'><p>a simple routine for benchmarking R code</p></a></li>
<li><a href='#rbenchmark'><p>rbenchmark provides a simple routine for benchmarking R code.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Benchmarking routine for R</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2012-08-30</td>
</tr>
<tr>
<td>Author:</td>
<td>Wacek Kusnierczyk [aut, cre],
  Dirk Eddelbuettel [ctb],
  Berend Hasselman [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Wacek Kusnierczyk &lt;waku@idi.ntnu.no&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>rbenchmark is inspired by the Perl module Benchmark, and
        is intended to facilitate benchmarking of arbitrary R code. The
        library consists of just one function, benchmark, which is a
        simple wrapper around system.time.  Given a specification of
        the benchmarking process (counts of replications, evaluation
        environment) and an arbitrary number of expressions, benchmark
        evaluates each of the expressions in the specified environment,
        replicating the evaluation as many times as specified, and
        returning the results conveniently wrapped into a data frame.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="http://rbenchmark.googlecode.com">http://rbenchmark.googlecode.com</a></td>
</tr>
<tr>
<td>Packaged:</td>
<td>2012-08-30 10:04:59 UTC; waku</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2012-08-30 12:26:04</td>
</tr>
</table>
<hr>
<h2 id='benchmark'>a simple routine for benchmarking R code</h2><span id='topic+benchmark'></span>

<h3>Description</h3>

<p><code>benchmark</code> is a simple wrapper around <code>system.time</code>.
</p>
<p>Given a specification of the benchmarking process (counts of replications, evaluation environment) and an arbitrary number of expressions, <code>benchmark</code> evaluates each of the expressions in the specified environment, replicating the evaluation as many times as specified, and returning the results conveniently wrapped into a data frame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>benchmark(
   ..., 
   columns = c(
      "test", "replications", "elapsed", "relative", "user.self", "sys.self", 
      "user.child", "sys.child"), 
   order = "test", 
   replications = 100, 
   environment = parent.frame(),
   relative = "elapsed")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="benchmark_+3A_...">...</code></td>
<td>
<p>captures any number of unevaluated expressions passed to benchmark as named or unnamed arguments. </p>
</td></tr>
<tr><td><code id="benchmark_+3A_columns">columns</code></td>
<td>
<p>a character or integer vector specifying which columns should be included in the returned data frame (see below).</p>
</td></tr>
<tr><td><code id="benchmark_+3A_order">order</code></td>
<td>
<p>a character or integer vector specifying which columns should be used to sort the output data frame. Any of the columns that can be specified for <code>columns</code> (see above) can be used, even if it is not included in <code>columns</code> and will not appear in the output data frame.  If <code>order=NULL</code>, the benchmarks will appear in the order of the replication counts and expressions provided in the call to <code>benchmark</code>, without sorting.</p>
</td></tr>
<tr><td><code id="benchmark_+3A_replications">replications</code></td>
<td>
<p>a numeric vector specifying how many times an expression should be evaluated when the runtime is measured. If <code>replications</code> consists of more than one value, each expression will be benchmarked multiple times, once for each value in replications. </p>
</td></tr>
<tr><td><code id="benchmark_+3A_environment">environment</code></td>
<td>
<p>the environment in which the expressions will be evaluated.</p>
</td></tr>
<tr><td><code id="benchmark_+3A_relative">relative</code></td>
<td>
<p>the name or index of the column whose values will be used to compute relative timings (see below). If <code>relative</code> is not given, it defaults to <code>'elapsed'</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The parameters <code>columns</code>, <code>order</code>, <code>replications</code>, and <code>environment</code> are optional and have the following default values:
</p>

<ul>
<li><p> columns = c('test', 'replications', 'elapsed', 'relative', 'user.self', 'sys.self', 'user.child', 'sys.child')
</p>
<p>By default, the returned data frame will contain all columns generated internally in benchmark. These named columns will contain the following data:
</p>

<ul>
<li> <p><code>test</code>: a character string naming each individual benchmark. If the corresponding expression was passed to benchmark in a named argument, the name will be used; otherwise, the expression itself converted to a character string will be used.
</p>
</li>
<li> <p><code>replications</code>: a numeric vector specifying the number of replications used within each individual benchmark. 
</p>
</li>
<li> <p><code>elapsed</code>, <code>user.self</code>, <code>sys.self</code>, <code>user.child</code>, and <code>sys.child</code> are columns containing values reported by system.time; see Sec. 7.1 Operating system access in The R language definition, or see <a href="base.html#topic+system.time">system.time</a>.
</p>
</li>
<li> <p><code>relative</code>: a column containing benchmark values relative to the shortest benchmark value.  The benchmark values used in this computation are taken from the column specified with the <code>relative</code> argument.
</p>
</li></ul>
 
</li>
<li><p> order = 'test'
</p>
<p>By default, the data frame is sorted by the column test (the labels of the expressions or the expressions themselves; see above). 
</p>
</li>
<li><p> replications = 100
</p>
<p>By default, each expression will be benchmarked once, and will be evaluated 100 times within the benchmark. 
</p>
</li>
<li><p> environment = parent.frame()
</p>
<p>By default, all expressions will be evaluated in the environment in which the call to benchmark is made. 
</p>
</li>
<li><p> relative = 'elapsed'
</p>
<p>By default, relative timings are given based on values from the column 'elapsed'.
</p>
</li></ul>
 


<h3>Value</h3>

<p>The value returned from a call to <code>benchmark</code> is a data frame with rows corresponding to individual benchmarks, and columns as specified above.
</p>
<p>An individual benchmark corresponds to a unique combination (see below) of an expression from <code>... and</code> a replication count from <code>replications</code>; if there are n expressions in <code>...</code> and m replication counts in <code>replication</code>, the returned data frame will consist of n*m rows, each corresponding to an individual, independent (see below) benchmark.
</p>
<p>If either <code>...</code> or <code>replications</code> contain duplicates, the returned data frame will contain multiple benchmarks for the involved expression-replication combinations. Note that such multiple benchmarks for a particular expression-replication pair will, in general, have different timing results, since they will be evaluated independently (unless the expressions perform side effects that can influence each other's performance). </p>


<h3>Note</h3>

<p>Not all expressions, if passed as unnamed arguments, will be cast to character strings as you might expect:
</p>
<pre>
   benchmark({x = 5; 1:x^x})
   # the benchmark will be named '{'
</pre>
<p>benchmark performs no smart argument-parameter matching. Any named argument whose name is not exactly 'replications', 'environment', 'columns', or 'order' will be treated as an expression to be benchmarked: 
</p>
<pre>
   benchmark(1:10^5, repl=1000) 
   # there will be a benchmark named 'repl'
</pre>
<p>See &lt;http://code.google.com/p/rbenchmark&gt; for more details.
</p>


<h3>Author(s)</h3>

<p>Wacek Kusnierczyk &lt;mailto:waku@idi.ntnu.no&gt;</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(rbenchmark)

# Example 1
# Benchmarking the allocation of one 10^6-element numeric vector,
# by default replicated 100 times
benchmark(1:10^6)

# simple test functions used in subsequent examples
random.array = function(rows, cols, dist=rnorm) 
                  array(dist(rows*cols), c(rows, cols))
random.replicate = function(rows, cols, dist=rnorm)
                      replicate(cols, dist(rows))

# Example 2
# Benchmarking an expression multiple times with the same replication count,
# output with selected columns only
benchmark(replications=rep(100, 3),
          random.array(100, 100),
          random.array(100, 100),
          columns=c('test', 'elapsed', 'replications'))

# Example 3
# Benchmarking two named expressions with three different replication
# counts, output sorted by test name and replication count,
# with additional column added after the benchmark
within(benchmark(rep=random.replicate(100, 100),
                 arr=random.array(100, 100),
                 replications=10^(1:3),
                 columns=c('test', 'replications', 'elapsed'),
                 order=c('test', 'replications')),
       { average = elapsed/replications })

# Example 4
# Benchmarking a list of arbitrary predefined expressions
tests = list(rep=expression(random.replicate(100, 100)), 
             arr=expression(random.array(100, 100)))
do.call(benchmark,
        c(tests, list(replications=100,
                      columns=c('test', 'elapsed', 'replications'),
                      order='elapsed')))

</code></pre>

<hr>
<h2 id='rbenchmark'>rbenchmark provides a simple routine for benchmarking R code.</h2><span id='topic+rbenchmark'></span>

<h3>Description</h3>

<p><code>rbenchmark</code> is inspired by the Perl module Benchmark, and is intended to facilitate benchmarking of arbitrary R code.
</p>
<p>The library consists of just one function, benchmark, which is a simple wrapper around system.time.
</p>
<p>Given a specification of the benchmarking process (counts of replications, evaluation environment) and an arbitrary number of expressions, benchmark evaluates each of the expressions in the specified environment, replicating the evaluation as many times as specified, and returning the results conveniently wrapped into a data frame. 
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> benchmark</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.0.0</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2012-08-30</td>
</tr>
<tr>
 <td style="text-align: left;">
License: GPL-2
LazyLoad: </td><td style="text-align: left;"> yes</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Wacek Kusnierczyk
</p>
<p>Maintainer: Wacek Kusnierczyk &lt;waku@idi.ntnu.no&gt;
</p>
<p>Contributors: Dirk Eddelbuettel &lt;edd@debian.org&gt;, Berend Hasselman &lt;bhh@xs4all.nl&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(rbenchmark)

# Example 1
# Benchmarking the allocation of one 10^6-element numeric vector,
# by default replicated 100 times
benchmark(1:10^6)

# simple test functions used in subsequent examples
random.array = function(rows, cols, dist=rnorm) 
                  array(dist(rows*cols), c(rows, cols))
random.replicate = function(rows, cols, dist=rnorm)
                      replicate(cols, dist(rows))

# Example 2
# Benchmarking an expression multiple times with the same replication count,
# output with selected columns only
benchmark(replications=rep(100, 3),
          random.array(100, 100),
          random.array(100, 100),
          columns=c('test', 'elapsed', 'replications'))

# Example 3
# Benchmarking two named expressions with three different replication
# counts, output sorted by test name and replication count,
# with additional column added after the benchmark
within(benchmark(rep=random.replicate(100, 100),
                 arr=random.array(100, 100),
                 replications=10^(1:3),
                 columns=c('test', 'replications', 'elapsed'),
                 order=c('test', 'replications')),
       { average = elapsed/replications })

# Example 4
# Benchmarking a list of arbitrary predefined expressions
tests = list(rep=expression(random.replicate(100, 100)), 
             arr=expression(random.array(100, 100)))
do.call(benchmark,
        c(tests, list(replications=100,
                      columns=c('test', 'elapsed', 'replications'),
                      order='elapsed')))

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
