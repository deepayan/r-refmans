<!DOCTYPE html><html><head><title>Help for package cmstatr</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {cmstatr}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#cmstatr-package'><p>cmstatr: Statistical Methods for Composite Material Data</p></a></li>
<li><a href='#ad_ksample'><p>Anderson&ndash;Darling K-Sample Test</p></a></li>
<li><a href='#anderson_darling'><p>Anderson&ndash;Darling test for goodness of fit</p></a></li>
<li><a href='#augment.mnr'><p>Augment data with information from an <code>mnr</code> object</p></a></li>
<li><a href='#basis'><p>Calculate basis values</p></a></li>
<li><a href='#calc_cv_star'><p>Calculate the modified CV from the CV</p></a></li>
<li><a href='#carbon.fabric'><p>Sample data for a generic carbon fabric</p></a></li>
<li><a href='#cv'><p>Calculate the coefficient of variation</p></a></li>
<li><a href='#equiv_change_mean'><p>Equivalency based on change in mean value</p></a></li>
<li><a href='#equiv_mean_extremum'><p>Test for decrease in mean or minimum individual</p></a></li>
<li><a href='#glance.adk'><p>Glance at a <code>adk</code> (Anderson&ndash;Darling k-Sample) object</p></a></li>
<li><a href='#glance.anderson_darling'><p>Glance at an <code>anderson_darling</code> object</p></a></li>
<li><a href='#glance.basis'><p>Glance at a basis object</p></a></li>
<li><a href='#glance.equiv_change_mean'><p>Glance at a <code>equiv_change_mean</code> object</p></a></li>
<li><a href='#glance.equiv_mean_extremum'><p>Glance at an <code>equiv_mean_extremum</code> object</p></a></li>
<li><a href='#glance.levene'><p>Glance at a <code>levene</code> object</p></a></li>
<li><a href='#glance.mnr'><p>Glance at a <code>mnr</code> (maximum normed residual) object</p></a></li>
<li><a href='#hk_ext'><p>Calculate values related to Extended Hanson&ndash;Koopmans tolerance bounds</p></a></li>
<li><a href='#k_equiv'><p>k-factors for determining acceptance based on sample mean and an extremum</p></a></li>
<li><a href='#k_factor_normal'><p>Calculate k factor for basis values (<code class="reqn">kB</code>, <code class="reqn">kA</code>) with normal</p>
distribution</a></li>
<li><a href='#levene_test'><p>Levene's Test for Equality of Variance</p></a></li>
<li><a href='#maximum_normed_residual'><p>Detect outliers using the maximum normed residual method</p></a></li>
<li><a href='#nested_data_plot'><p>Create a plot of nested sources of variation</p></a></li>
<li><a href='#nonpara_binomial_rank'><p>Rank for distribution-free tolerance bound</p></a></li>
<li><a href='#normalize_group_mean'><p>Normalize values to group means</p></a></li>
<li><a href='#normalize_ply_thickness'><p>Normalizes strength values to ply thickness</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#stat_esf'><p>Empirical Survival Function</p></a></li>
<li><a href='#stat_normal_surv_func'><p>Normal Survival Function</p></a></li>
<li><a href='#transform_mod_cv'><p>Transforms data according to the modified CV rule</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Statistical Methods for Composite Material Data</td>
</tr>
<tr>
<td>Version:</td>
<td>0.9.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-03-13</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.3)</td>
</tr>
<tr>
<td>Description:</td>
<td>An implementation of the statistical methods commonly
  used for advanced composite materials in aerospace applications.
  This package focuses on calculating basis values (lower tolerance
  bounds) for material strength properties, as well as performing the
  associated diagnostic tests. This package provides functions for
  calculating basis values assuming several different distributions,
  as well as providing functions for non-parametric methods of computing
  basis values. Functions are also provided for testing the hypothesis
  that there is no difference between strength and modulus data from an
  alternate sample and that from a "qualification" or "baseline" sample.
  For a discussion of these statistical methods and their use, see the
  Composite Materials Handbook, Volume 1 (2012, ISBN: 978-0-7680-7811-4).
  Additional details about this package are available in the paper by
  Kloppenborg (2020, &lt;<a href="https://doi.org/10.21105%2Fjoss.02265">doi:10.21105/joss.02265</a>&gt;).</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://www.cmstatr.net/">https://www.cmstatr.net/</a>, <a href="https://github.com/cmstatr/cmstatr">https://github.com/cmstatr/cmstatr</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/cmstatr/cmstatr/issues">https://github.com/cmstatr/cmstatr/issues</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/AGPL-3">AGPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Imports:</td>
<td>dplyr, generics, ggplot2, kSamples, MASS, purrr, rlang, stats,
tibble</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, lintr, rmarkdown, spelling, testthat, tidyr, vdiffr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>Config/testthat/parallel:</td>
<td>true</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-14 13:30:55 UTC; stefan</td>
</tr>
<tr>
<td>Author:</td>
<td>Stefan Kloppenborg
    <a href="https://orcid.org/0000-0002-1908-5214"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cre],
  Billy Cheng [ctb],
  Ally Fraser [ctb],
  Jeffrey Borlik [ctb],
  Comtek Advanced Structures, Ltd. [fnd]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Stefan Kloppenborg &lt;stefan@kloppenborg.ca&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-14 14:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='cmstatr-package'>cmstatr: Statistical Methods for Composite Material Data</h2><span id='topic+cmstatr'></span><span id='topic+cmstatr-package'></span>

<h3>Description</h3>

<p>To learn more about <code>cmstatr</code>, start with the vignettes:
<code>browseVignettes(package = "cmstatr")</code>
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Stefan Kloppenborg <a href="mailto:stefan@kloppenborg.ca">stefan@kloppenborg.ca</a> (<a href="https://orcid.org/0000-0002-1908-5214">ORCID</a>)
</p>
<p>Other contributors:
</p>

<ul>
<li><p> Billy Cheng <a href="mailto:bcheng@comtekadvanced.com">bcheng@comtekadvanced.com</a> [contributor]
</p>
</li>
<li><p> Ally Fraser <a href="mailto:ally.fraser25@gmail.com">ally.fraser25@gmail.com</a> [contributor]
</p>
</li>
<li><p> Jeffrey Borlik [contributor]
</p>
</li>
<li><p> Comtek Advanced Structures, Ltd. [funder]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://www.cmstatr.net/">https://www.cmstatr.net/</a>
</p>
</li>
<li> <p><a href="https://github.com/cmstatr/cmstatr">https://github.com/cmstatr/cmstatr</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/cmstatr/cmstatr/issues">https://github.com/cmstatr/cmstatr/issues</a>
</p>
</li></ul>


<hr>
<h2 id='ad_ksample'>Anderson&ndash;Darling K-Sample Test</h2><span id='topic+ad_ksample'></span>

<h3>Description</h3>

<p>This function performs an Anderson&ndash;Darling k-sample test. This is used to
determine if several samples (groups) share a common (unspecified)
distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ad_ksample(data = NULL, x, groups, alpha = 0.025)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ad_ksample_+3A_data">data</code></td>
<td>
<p>a data.frame</p>
</td></tr>
<tr><td><code id="ad_ksample_+3A_x">x</code></td>
<td>
<p>the variable in the data.frame on which to perform the
Anderson&ndash;Darling k-Sample test (usually strength)</p>
</td></tr>
<tr><td><code id="ad_ksample_+3A_groups">groups</code></td>
<td>
<p>a variable in the data.frame that defines the groups</p>
</td></tr>
<tr><td><code id="ad_ksample_+3A_alpha">alpha</code></td>
<td>
<p>the significance level (default 0.025)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is a wrapper for the <a href="kSamples.html#topic+ad.test">ad.test</a> function from
the package <code>kSamples</code>. The method &quot;exact&quot; is specified in the call to
<code>ad.test</code>. Refer to that package's documentation for details.
</p>
<p>There is a minor difference in the formulation of the Anderson&ndash;Darling
k-Sample test in CMH-17-1G, compared with that in the Scholz and
Stephens (1987). This difference affects the test statistic and the
critical value in the same proportion, and therefore the conclusion of
the test is unaffected. When
comparing the test statistic generated by this function to that generated
by software that uses the CMH-17-1G formulation (such as ASAP, CMH17-STATS,
etc.), the test statistic reported by this function will be greater by
a factor of <code class="reqn">(k - 1)</code>, with a corresponding change in the critical
value.
</p>
<p>For more information about the difference between this function and
the formulation in CMH-17-1G, see the vignette on the subject, which
can be accessed by running <code>vignette("adktest")</code>
</p>


<h3>Value</h3>

<p>Returns an object of class <code>adk</code>. This object has the following fields:
</p>

<ul>
<li> <p><code>call</code> the expression used to call this function
</p>
</li>
<li> <p><code>data</code> the original data used to compute the ADK
</p>
</li>
<li> <p><code>groups</code> a vector of the groups used in the computation
</p>
</li>
<li> <p><code>alpha</code> the value of alpha specified
</p>
</li>
<li> <p><code>n</code> the total number of observations
</p>
</li>
<li> <p><code>k</code> the number of groups
</p>
</li>
<li> <p><code>sigma</code> the computed standard deviation of the test statistic
</p>
</li>
<li> <p><code>ad</code> the value of the Anderson&ndash;Darling k-Sample test statistic
</p>
</li>
<li> <p><code>p</code> the computed p-value
</p>
</li>
<li> <p><code>reject_same_dist</code> a boolean value indicating whether the null
hypothesis that all samples come from the same distribution is rejected
</p>
</li>
<li> <p><code>raw</code> the original results returned from
<a href="kSamples.html#topic+ad.test">ad.test</a>
</p>
</li></ul>



<h3>References</h3>

<p>F. W. Scholz and M. Stephens, “K-Sample Anderson&ndash;Darling Tests,” Journal
of the American Statistical Association, vol. 82, no. 399. pp. 918–924,
Sep-1987.
</p>
<p>“Composite Materials Handbook, Volume 1. Polymer Matrix Composites
Guideline for Characterization of Structural Materials,” SAE International,
CMH-17-1G, Mar. 2012.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)

carbon.fabric %&gt;%
  filter(test == "WT") %&gt;%
  filter(condition == "RTD") %&gt;%
  ad_ksample(strength, batch)
##
## Call:
## ad_ksample(data = ., x = strength, groups = batch)
##
## N = 18          k = 3
## ADK = 0.912     p-value = 0.95989
## Conclusion: Samples come from the same distribution ( alpha = 0.025 )

</code></pre>

<hr>
<h2 id='anderson_darling'>Anderson&ndash;Darling test for goodness of fit</h2><span id='topic+anderson_darling'></span><span id='topic+anderson_darling_normal'></span><span id='topic+anderson_darling_lognormal'></span><span id='topic+anderson_darling_weibull'></span>

<h3>Description</h3>

<p>Calculates the Anderson&ndash;Darling test statistic for a sample given
a particular distribution, and determines whether to reject the
hypothesis that a sample is drawn from that distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>anderson_darling_normal(data = NULL, x, alpha = 0.05)

anderson_darling_lognormal(data = NULL, x, alpha = 0.05)

anderson_darling_weibull(data = NULL, x, alpha = 0.05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="anderson_darling_+3A_data">data</code></td>
<td>
<p>a data.frame-like object (optional)</p>
</td></tr>
<tr><td><code id="anderson_darling_+3A_x">x</code></td>
<td>
<p>a numeric vector or a variable in the data.frame</p>
</td></tr>
<tr><td><code id="anderson_darling_+3A_alpha">alpha</code></td>
<td>
<p>the required significance level of the test.
Defaults to 0.05.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Anderson&ndash;Darling test statistic is calculated for the distribution
given by the user.
</p>
<p>The observed significance level (OSL), or p-value, is calculated assuming
that the parameters
of the distribution are unknown; these parameters are estimate from the
data.
</p>
<p>The function <code>anderson_darling_normal</code> computes the Anderson&ndash;Darling
test statistic given a normal distribution with mean and standard deviation
equal to the sample mean and standard deviation.
</p>
<p>The function <code>anderson_darling_lognormal</code> is the same as
<code>anderson_darling_normal</code> except that the data is log transformed
first.
</p>
<p>The function <code>anderson_darling_weibull</code> computes the Anderson&ndash;Darling
test statistic given a Weibull distribution with shape and scale parameters
estimated from the data using a maximum likelihood estimate.
</p>
<p>The test statistic, <code>A</code>, is modified to account for
the fact that the parameters of the population are not known,
but are instead estimated from the sample. This modification is
a function of the sample size only, and is different for each
distribution (normal/lognormal or Weibull). Several such modifications
have been proposed. This function uses the modification published in
Stephens (1974), Lawless (1982) and CMH-17-1G. Some other implementations
of the Anderson-Darling test, such as the implementation in the
<code>nortest</code> package, use other modifications, such as the one
published in D'Agostino and Stephens (1986). As such, the p-value
reported by this function may differ from the p-value reported
by implementations of the Anderson&ndash;Darling test that use
different modifiers. Only the unmodified
test statistic is reported in the result of this function, but
the modified test statistic is used to compute the OSL (p-value).
</p>
<p>This function uses the formulae for observed significance
level (OSL) published in CMH-17-1G. These formulae depend on the particular
distribution used.
</p>
<p>The results of this function have been validated against
published values in Lawless (1982).
</p>


<h3>Value</h3>

<p>an object of class <code>anderson_darling</code>. This object has the following
fields.
</p>

<ul>
<li> <p><code>call</code> the expression used to call this function
</p>
</li>
<li> <p><code>dist</code> the distribution used
</p>
</li>
<li> <p><code>data</code> a copy of the data analyzed
</p>
</li>
<li> <p><code>n</code> the number of observations in the sample
</p>
</li>
<li> <p><code>A</code> the Anderson&ndash;Darling test statistic
</p>
</li>
<li> <p><code>osl</code> the observed significance level (p-value),
assuming the
parameters of the distribution are estimated from the data
</p>
</li>
<li> <p><code>alpha</code> the required significance level for the test.
This value is given by the user.
</p>
</li>
<li> <p><code>reject_distribution</code> a logical value indicating whether
the hypothesis that the data is drawn from the specified distribution
should be rejected
</p>
</li></ul>



<h3>References</h3>

<p>J. F. Lawless, <em>Statistical models and methods for lifetime data</em>.
New York: Wiley, 1982.
</p>
<p>&quot;Composite Materials Handbook, Volume 1. Polymer Matrix
Composites Guideline for Characterization of Structural
Materials,&quot; SAE International, CMH-17-1G, Mar. 2012.
</p>
<p>M. A. Stephens, “EDF Statistics for Goodness of Fit and Some
Comparisons,”
Journal of the American Statistical Association, vol. 69, no. 347.
pp. 730–737, 1974.
</p>
<p>R. D’Agostino and M. Stephens, Goodness-of-Fit Techniques.
New York: Marcel Dekker, 1986.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)

carbon.fabric %&gt;%
  filter(test == "FC") %&gt;%
  filter(condition == "RTD") %&gt;%
  anderson_darling_normal(strength)
## Call:
## anderson_darling_normal(data = ., x = strength)
##
## Distribution:  Normal ( n = 18 )
## Test statistic:  A = 0.9224776
## OSL (p-value):  0.01212193  (assuming unknown parameters)
## Conclusion: Sample is not drawn from a Normal distribution (alpha = 0.05)

</code></pre>

<hr>
<h2 id='augment.mnr'>Augment data with information from an <code>mnr</code> object</h2><span id='topic+augment.mnr'></span>

<h3>Description</h3>

<p>Augment accepts an <code>mnr</code> object (returned from the function
<code><a href="#topic+maximum_normed_residual">maximum_normed_residual()</a></code>) and a dataset and adds the column
<code>.outlier</code> to the dataset. The column <code>.outlier</code> is a logical
vector indicating whether each observation is an outlier.
</p>
<p>When passing data into <code>augment</code> using the <code>data</code> argument,
the data must be exactly the data that was passed to
<code>maximum_normed_residual</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mnr'
augment(x, data = x$data, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="augment.mnr_+3A_x">x</code></td>
<td>
<p>an <code>mnr</code> object created by
<code><a href="#topic+maximum_normed_residual">maximum_normed_residual()</a></code></p>
</td></tr>
<tr><td><code id="augment.mnr_+3A_data">data</code></td>
<td>
<p>a <code>data.frame</code> or
<code><a href="tibble.html#topic+tibble">tibble::tibble()</a></code>
containing the original data that was passed to
<code>maximum_normed_residual</code></p>
</td></tr>
<tr><td><code id="augment.mnr_+3A_...">...</code></td>
<td>
<p>Additional arguments. Not used. Included only to match generic
signature.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>When <code>data</code> is supplied, <code>augment</code> returns <code>data</code>, but with
one column appended. When <code>data</code> is not supplied, <code>augment</code>
returns a new <code><a href="tibble.html#topic+tibble">tibble::tibble()</a></code> with the column
<code>values</code> containing the original values used by
<code>maximum_normed_residaul</code> plus one additional column. The additional
column is:
</p>

<ul>
<li> <p><code>.outler</code> a logical value indicating whether the observation
is an outlier
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+maximum_normed_residual">maximum_normed_residual()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- data.frame(strength = c(80, 98, 96, 97, 98, 120))
m &lt;- maximum_normed_residual(data, strength)

# augment can be called with the original data
augment(m, data)

##   strength .outlier
## 1       80    FALSE
## 2       98    FALSE
## 3       96    FALSE
## 4       97    FALSE
## 5       98    FALSE
## 6      120    FALSE

# or augment can be called without the orignal data and it will be
# reconstructed
augment(m)

## # A tibble: 6 x 2
##   values .outlier
##    &lt;dbl&gt; &lt;lgl&gt;
## 1     80 FALSE
## 2     98 FALSE
## 3     96 FALSE
## 4     97 FALSE
## 5     98 FALSE
## 6    120 FALSE

</code></pre>

<hr>
<h2 id='basis'>Calculate basis values</h2><span id='topic+basis'></span><span id='topic+basis_normal'></span><span id='topic+basis_lognormal'></span><span id='topic+basis_weibull'></span><span id='topic+basis_pooled_cv'></span><span id='topic+basis_pooled_sd'></span><span id='topic+basis_hk_ext'></span><span id='topic+basis_nonpara_large_sample'></span><span id='topic+basis_anova'></span>

<h3>Description</h3>

<p>Calculate the basis value for a given data set. There are various functions
to calculate the basis values for different distributions.
The basis value is the lower one-sided tolerance bound of a certain
proportion of the population. For more information on tolerance bounds,
see Meeker, et. al. (2017).
For B-Basis, set the content of tolerance bound to <code class="reqn">p=0.90</code> and
the confidence level to <code class="reqn">conf=0.95</code>; for A-Basis, set <code class="reqn">p=0.99</code> and
<code class="reqn">conf=0.95</code>. While other tolerance bound
contents and confidence levels may be computed, they are infrequently
needed in practice.
</p>
<p>These functions also perform some automated diagnostic
tests of the data prior to calculating the basis values. These diagnostic
tests can be overridden if needed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>basis_normal(
  data = NULL,
  x,
  batch = NULL,
  p = 0.9,
  conf = 0.95,
  override = c()
)

basis_lognormal(
  data = NULL,
  x,
  batch = NULL,
  p = 0.9,
  conf = 0.95,
  override = c()
)

basis_weibull(
  data = NULL,
  x,
  batch = NULL,
  p = 0.9,
  conf = 0.95,
  override = c()
)

basis_pooled_cv(
  data = NULL,
  x,
  groups,
  batch = NULL,
  p = 0.9,
  conf = 0.95,
  modcv = FALSE,
  override = c()
)

basis_pooled_sd(
  data = NULL,
  x,
  groups,
  batch = NULL,
  p = 0.9,
  conf = 0.95,
  modcv = FALSE,
  override = c()
)

basis_hk_ext(
  data = NULL,
  x,
  batch = NULL,
  p = 0.9,
  conf = 0.95,
  method = c("optimum-order", "woodward-frawley"),
  override = c()
)

basis_nonpara_large_sample(
  data = NULL,
  x,
  batch = NULL,
  p = 0.9,
  conf = 0.95,
  override = c()
)

basis_anova(data = NULL, x, groups, p = 0.9, conf = 0.95, override = c())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="basis_+3A_data">data</code></td>
<td>
<p>a data.frame</p>
</td></tr>
<tr><td><code id="basis_+3A_x">x</code></td>
<td>
<p>the variable in the data.frame for which to find the basis value</p>
</td></tr>
<tr><td><code id="basis_+3A_batch">batch</code></td>
<td>
<p>the variable in the data.frame that contains the batches.</p>
</td></tr>
<tr><td><code id="basis_+3A_p">p</code></td>
<td>
<p>the content of the tolerance bound. Should be 0.90 for B-Basis
and 0.99 for A-Basis</p>
</td></tr>
<tr><td><code id="basis_+3A_conf">conf</code></td>
<td>
<p>confidence level Should be 0.95 for both A- and B-Basis</p>
</td></tr>
<tr><td><code id="basis_+3A_override">override</code></td>
<td>
<p>a list of names of diagnostic tests to override,
if desired. Specifying &quot;all&quot; will override all diagnostic
tests applicable to the current method.</p>
</td></tr>
<tr><td><code id="basis_+3A_groups">groups</code></td>
<td>
<p>the variable in the data.frame representing the groups</p>
</td></tr>
<tr><td><code id="basis_+3A_modcv">modcv</code></td>
<td>
<p>a logical value indicating whether the modified CV approach
should be used. Only applicable to pooling methods.</p>
</td></tr>
<tr><td><code id="basis_+3A_method">method</code></td>
<td>
<p>the method for Hanson&ndash;Koopmans nonparametric basis values.
should be &quot;optimum-order&quot; for B-Basis and &quot;woodward-frawley&quot;
for A-Basis.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>data</code> is an optional argument. If <code>data</code> is given, it should
be a
<code>data.frame</code> (or similar object). When <code>data</code> is specified, the
value of <code>x</code> is expected to be a variable within <code>data</code>. If
<code>data</code> is not specified, <code>x</code> must be a vector.
</p>
<p>When <code>modcv=TRUE</code> is set, which is only applicable to the
pooling methods,
the data is first modified according to the modified coefficient
of variation (CV)
rules. This modified data is then used when both calculating the
basis values and
also when performing the diagnostic tests. The modified CV approach
is a way of
adding extra variance to datasets with unexpectedly low variance.
</p>
<p><code>basis_normal</code> calculate the basis value by subtracting <code class="reqn">k</code> times
the standard deviation from the mean. <code class="reqn">k</code> is given by
the function <code><a href="#topic+k_factor_normal">k_factor_normal()</a></code>. The equations in
Krishnamoorthy and Mathew (2008) are used.
<code>basis_normal</code> also
performs a diagnostic test for outliers (using
<code><a href="#topic+maximum_normed_residual">maximum_normed_residual()</a></code>)
and a diagnostic test for normality (using
<code><a href="#topic+anderson_darling_normal">anderson_darling_normal()</a></code>).
If the argument <code>batch</code> is given, this function also performs
a diagnostic test for outliers within
each batch (using <code><a href="#topic+maximum_normed_residual">maximum_normed_residual()</a></code>)
and a diagnostic test for between batch variability (using
<code><a href="#topic+ad_ksample">ad_ksample()</a></code>). The argument <code>batch</code> is only used
for these diagnostic tests.
</p>
<p><code>basis_lognormal</code> calculates the basis value in the same way
that <code>basis_normal</code> does, except that the natural logarithm of the
data is taken.
</p>
<p><code>basis_lognormal</code> function also performs
a diagnostic test for outliers (using
<code><a href="#topic+maximum_normed_residual">maximum_normed_residual()</a></code>)
and a diagnostic test for normality (using
<code><a href="#topic+anderson_darling_lognormal">anderson_darling_lognormal()</a></code>).
If the argument <code>batch</code> is given, this function also performs
a diagnostic test for outliers within
each batch (using <code><a href="#topic+maximum_normed_residual">maximum_normed_residual()</a></code>)
and a diagnostic test for between batch variability (using
<code><a href="#topic+ad_ksample">ad_ksample()</a></code>). The argument <code>batch</code> is only used
for these diagnostic tests.
</p>
<p><code>basis_weibull</code> calculates the basis value for data distributed
according to a Weibull distribution. The confidence level for the
content requested is calculated using the conditional method, as
described in Lawless (1982) Section 4.1.2b. This has good agreement
with tables published in CMH-17-1G. Results differ between this function
and STAT17 by approximately 0.5\
</p>
<p><code>basis_weibull</code> function also performs
a diagnostic test for outliers (using
<code><a href="#topic+maximum_normed_residual">maximum_normed_residual()</a></code>)
and a diagnostic test for normality (using
<code><a href="#topic+anderson_darling_weibull">anderson_darling_weibull()</a></code>).
If the argument <code>batch</code> is given, this function also performs
a diagnostic test for outliers within
each batch (using <code><a href="#topic+maximum_normed_residual">maximum_normed_residual()</a></code>)
and a diagnostic test for between batch variability (using
<code><a href="#topic+ad_ksample">ad_ksample()</a></code>). The argument <code>batch</code> is only used
for these diagnostic tests.
</p>
<p><code>basis_hk_ext</code> calculates the basis value using the Extended
Hanson&ndash;Koopmans method, as described in CMH-17-1G and Vangel (1994).
For nonparametric distributions, this function should be used for samples
up to n=28 for B-Basis and up to <code class="reqn">n=299</code> for A-Basis.
This method uses a pair of order statistics to determine the basis value.
CMH-17-1G suggests that for A-Basis, the first and last order statistic
is used: this is called the &quot;woodward-frawley&quot; method in this package,
after the paper in which this approach is described (as referenced
by Vangel (1994)). For B-Basis, another approach is used whereby the
first and <code>j-th</code> order statistic are used to calculate the basis value.
In this approach, the <code>j-th</code> order statistic is selected to minimize
the difference between the tolerance limit (assuming that the order
statistics are equal to the expected values from a standard normal
distribution) and the population quantile for a standard normal
distribution. This approach is described in Vangel (1994). This second
method (for use when calculating B-Basis values) is called
&quot;optimum-order&quot; in this package.
The results of <code>basis_hk_ext</code> have been
verified against example results from the program STAT-17. Agreement is
typically well within 0.2%.
</p>
<p>Note that the implementation of <code>hk_ext_z_j_opt</code> changed after <code>cmstatr</code>
version 0.8.0. This function is used internally by <code>basis_hk_ext</code>
when <code>method = "optimum-order"</code>. This implementation change may mean
that basis values computed using this method may change slightly
after version 0.8.0. However, both implementations seem to be equally
valid. See the included vignette
for a discussion of the differences between the implementation before
and after version 0.8.0, as well as the factors given in CMH-17-1G.
To access this vignette, run: <code>vignette("hk_ext", package = "cmstatr")</code>
</p>
<p><code>basis_hk_ext</code> also performs
a diagnostic test for outliers (using
<code><a href="#topic+maximum_normed_residual">maximum_normed_residual()</a></code>)
and performs a pair of tests that the sample size and method selected
follow the guidance described above.
If the argument <code>batch</code> is given, this function also performs
a diagnostic test for outliers within
each batch (using <code><a href="#topic+maximum_normed_residual">maximum_normed_residual()</a></code>)
and a diagnostic test for between batch variability (using
<code><a href="#topic+ad_ksample">ad_ksample()</a></code>). The argument <code>batch</code> is only used
for these diagnostic tests.
</p>
<p><code>basis_nonpara_large_sample</code> calculates the basis value
using the large sample method described in CMH-17-1G. This method uses
a sum of binomials to determine the rank of the ordered statistic
corresponding with the desired tolerance limit (basis value). Results
of this function have been verified against results of the STAT-17
program.
</p>
<p><code>basis_nonpara_large_sample</code> also performs
a diagnostic test for outliers (using
<code><a href="#topic+maximum_normed_residual">maximum_normed_residual()</a></code>)
and performs a test that the sample size is sufficiently large.
If the argument <code>batch</code> is given, this function also performs
a diagnostic test for outliers within
each batch (using <code><a href="#topic+maximum_normed_residual">maximum_normed_residual()</a></code>)
and a diagnostic test for between batch variability (using
<code><a href="#topic+ad_ksample">ad_ksample()</a></code>). The argument <code>batch</code> is only used
for these diagnostic tests.
</p>
<p><code>basis_anova</code> calculates basis values using the ANOVA method.
<code>x</code> specifies the data (normally strength) and <code>groups</code>
indicates the group corresponding to each observation. This method is
described in CMH-17-1G, but when the ratio of between-batch mean
square to the within-batch mean square is less than or equal
to one, the tolerance factor is calculated based on pooling the data
from all groups. This approach is recommended by Vangel (1992)
and by Krishnamoorthy and Mathew (2008), and is also implemented
by the software CMH17-STATS and STAT-17.
This function automatically performs a diagnostic
test for outliers within each group
(using <code><a href="#topic+maximum_normed_residual">maximum_normed_residual()</a></code>) and a test for between
group variability (using <code><a href="#topic+ad_ksample">ad_ksample()</a></code>) as well as checking
that the data contains at least 5 groups.
This function has been verified against the results of the STAT-17 program.
</p>
<p><code>basis_pooled_sd</code> calculates basis values by pooling the data from
several groups together. <code>x</code> specifies the data (normally strength)
and <code>group</code> indicates the group corresponding to each observation.
This method is described in CMH-17-1G and matches the pooling method
implemented in ASAP 2008.
</p>
<p><code>basis_pooled_cv</code> calculates basis values by pooling the data from
several groups together. <code>x</code> specifies the data (normally strength)
and <code>group</code> indicates the group corresponding to each observation.
This method is described in CMH-17-1G.
</p>
<p><code>basis_pooled_sd</code> and <code>basis_pooled_cv</code> both automatically
perform a number of diagnostic tests. Using
<code><a href="#topic+maximum_normed_residual">maximum_normed_residual()</a></code>, they check that there are no
outliers within each group and batch (provided that <code>batch</code> is
specified). They check the between batch variability using
<code><a href="#topic+ad_ksample">ad_ksample()</a></code>. They check that there are no outliers within
each group (pooling all batches) using
<code><a href="#topic+maximum_normed_residual">maximum_normed_residual()</a></code>. They check for the normality
of the pooled data using <code><a href="#topic+anderson_darling_normal">anderson_darling_normal()</a></code>.
<code>basis_pooled_sd</code> checks for equality of variance of all
data using <code><a href="#topic+levene_test">levene_test()</a></code> and <code>basis_pooled_cv</code>
checks for equality of variances of all data after transforming it
using <code><a href="#topic+normalize_group_mean">normalize_group_mean()</a></code>
using <code><a href="#topic+levene_test">levene_test()</a></code>.
</p>
<p>The object returned by these functions includes the named vector
<code>diagnostic_results</code>. This contains all of the diagnostic tests
performed. The name of each element of the vector corresponds with the
name of the diagnostic test. The contents of each element will be
&quot;P&quot; if the diagnostic test passed, &quot;F&quot; if the diagnostic test failed,
&quot;O&quot; if the diagnostic test was overridden and <code>NA</code> if the
diagnostic test was skipped (typically because an optional
argument was not supplied).
</p>
<p>The following list summarizes the diagnostic tests automatically
performed by each function.
</p>

<ul>
<li> <p><code>basis_normal</code>
</p>

<ul>
<li> <p><code>outliers_within_batch</code>
</p>
</li>
<li> <p><code>between_batch_variability</code>
</p>
</li>
<li> <p><code>outliers</code>
</p>
</li>
<li> <p><code>anderson_darling_normal</code>
</p>
</li></ul>

</li>
<li> <p><code>basis_lognormal</code>
</p>

<ul>
<li> <p><code>outliers_within_batch</code>
</p>
</li>
<li> <p><code>between_batch_variability</code>
</p>
</li>
<li> <p><code>outliers</code>
</p>
</li>
<li> <p><code>anderson_darling_lognormal</code>
</p>
</li></ul>

</li>
<li> <p><code>basis_weibull</code>
</p>

<ul>
<li> <p><code>outliers_within_batch</code>
</p>
</li>
<li> <p><code>between_batch_variability</code>
</p>
</li>
<li> <p><code>outliers</code>
</p>
</li>
<li> <p><code>anderson_darling_weibull</code>
</p>
</li></ul>

</li>
<li> <p><code>basis_pooled_cv</code>
</p>

<ul>
<li> <p><code>outliers_within_batch</code>
</p>
</li>
<li> <p><code>between_group_variability</code>
</p>
</li>
<li> <p><code>outliers_within_group</code>
</p>
</li>
<li> <p><code>pooled_data_normal</code>
</p>
</li>
<li> <p><code>normalized_variance_equal</code>
</p>
</li></ul>

</li>
<li> <p><code>basis_pooled_sd</code>
</p>

<ul>
<li> <p><code>outliers_within_batch</code>
</p>
</li>
<li> <p><code>between_group_variability</code>
</p>
</li>
<li> <p><code>outliers_within_group</code>
</p>
</li>
<li> <p><code>pooled_data_normal</code>
</p>
</li>
<li> <p><code>pooled_variance_equal</code>
</p>
</li></ul>

</li>
<li> <p><code>basis_hk_ext</code>
</p>

<ul>
<li> <p><code>outliers_within_batch</code>
</p>
</li>
<li> <p><code>between_batch_variability</code>
</p>
</li>
<li> <p><code>outliers</code>
</p>
</li>
<li> <p><code>sample_size</code>
</p>
</li></ul>

</li>
<li> <p><code>basis_nonpara_large_sample</code>
</p>

<ul>
<li> <p><code>outliers_within_batch</code>
</p>
</li>
<li> <p><code>between_batch_variability</code>
</p>
</li>
<li> <p><code>outliers</code>
</p>
</li>
<li> <p><code>sample_size</code>
</p>
</li></ul>

</li>
<li> <p><code>basis_anova</code>
</p>

<ul>
<li> <p><code>outliers_within_group</code>
</p>
</li>
<li> <p><code>equality_of_variance</code>
</p>
</li>
<li> <p><code>number_of_groups</code>
</p>
</li></ul>

</li></ul>



<h3>Value</h3>

<p>an object of class <code>basis</code>
This object has the following fields:
</p>

<ul>
<li> <p><code>call</code> the expression used to call this function
</p>
</li>
<li> <p><code>distribution</code> the distribution used (normal, etc.)
</p>
</li>
<li> <p><code>p</code> the value of <code class="reqn">p</code> supplied
</p>
</li>
<li> <p><code>conf</code> the value of <code class="reqn">conf</code> supplied
</p>
</li>
<li> <p><code>modcv</code> a logical value indicating whether the modified
CV approach was used. Only applicable to pooling methods.
</p>
</li>
<li> <p><code>data</code> a copy of the data used in the calculation
</p>
</li>
<li> <p><code>groups</code> a copy of the groups variable.
Only used for pooling and ANOVA methods.
</p>
</li>
<li> <p><code>batch</code> a copy of the batch data used for diagnostic tests
</p>
</li>
<li> <p><code>modcv_transformed_data</code> the data after the modified CV transformation
</p>
</li>
<li> <p><code>override</code> a vector of the names of diagnostic tests that
were overridden. <code>NULL</code> if none were overridden
</p>
</li>
<li> <p><code>diagnostic_results</code> a named character vector containing the
results of all the diagnostic tests. See the Details section for
additional information
</p>
</li>
<li> <p><code>diagnostic_failures</code> a vector containing any diagnostic tests
that produced failures
</p>
</li>
<li> <p><code>n</code> the number of observations
</p>
</li>
<li> <p><code>r</code> the number of groups, if a pooling method was used.
Otherwise it is NULL.
</p>
</li>
<li> <p><code>basis</code> the basis value computed. This is a number
except when pooling methods are used, in which case it is a data.frame.
</p>
</li></ul>



<h3>References</h3>

<p>J. F. Lawless, Statistical Models and Methods for Lifetime Data.
New York: John Wiley &amp; Sons, 1982.
</p>
<p>“Composite Materials Handbook, Volume 1. Polymer Matrix Composites
Guideline for Characterization of Structural Materials,” SAE International,
CMH-17-1G, Mar. 2012.
</p>
<p>M. Vangel, “One-Sided Nonparametric Tolerance Limits,”
Communications in Statistics - Simulation and Computation,
vol. 23, no. 4. pp. 1137–1154, 1994.
</p>
<p>K. Krishnamoorthy and T. Mathew, Statistical Tolerance Regions: Theory,
Applications, and Computation. Hoboken: John Wiley &amp; Sons, 2008.
</p>
<p>W. Meeker, G. Hahn, and L. Escobar, Statistical Intervals: A Guide
for Practitioners and Researchers, Second Edition.
Hoboken: John Wiley &amp; Sons, 2017.
</p>
<p>M. Vangel, “New Methods for One-Sided Tolerance Limits for a One-Way
Balanced Random-Effects ANOVA Model,” Technometrics, vol. 34, no. 2.
Taylor &amp; Francis, pp. 176–185, 1992.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hk_ext_z_j_opt">hk_ext_z_j_opt()</a></code>
</p>
<p><code><a href="#topic+k_factor_normal">k_factor_normal()</a></code>
</p>
<p><code><a href="#topic+transform_mod_cv">transform_mod_cv()</a></code>
</p>
<p><code><a href="#topic+maximum_normed_residual">maximum_normed_residual()</a></code>
</p>
<p><code><a href="#topic+anderson_darling_normal">anderson_darling_normal()</a></code>
</p>
<p><code><a href="#topic+anderson_darling_lognormal">anderson_darling_lognormal()</a></code>
</p>
<p><code><a href="#topic+anderson_darling_weibull">anderson_darling_weibull()</a></code>
</p>
<p><code><a href="#topic+ad_ksample">ad_ksample()</a></code>
</p>
<p><code><a href="#topic+normalize_group_mean">normalize_group_mean()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)

# A single-point basis value can be calculated as follows
# in this example, three failed diagnostic tests are
# overridden.

carbon.fabric %&gt;%
  filter(test == "FC") %&gt;%
  filter(condition == "RTD") %&gt;%
  basis_normal(strength, batch,
               override = c("outliers",
                            "outliers_within_batch",
                            "anderson_darling_normal"))

## Call:
## basis_normal(data = ., x = strength, batch = batch,
##     override = c("outliers", "outliers_within_batch",
##    "anderson_darling_normal"))
##
## Distribution:  Normal 	( n = 18 )
## The following diagnostic tests were overridden:
##     `outliers`,
##     `outliers_within_batch`,
##     `anderson_darling_normal`
## B-Basis:   ( p = 0.9 , conf = 0.95 )
## 76.94656

# A set of pooled basis values can also be calculated
# using the pooled standard deviation method, as follows.
# In this example, one failed diagnostic test is overridden.
carbon.fabric %&gt;%
  filter(test == "WT") %&gt;%
  basis_pooled_sd(strength, condition, batch,
                  override = c("outliers_within_batch"))

## Call:
## basis_pooled_sd(data = ., x = strength, groups = condition,
##                 batch = batch, override = c("outliers_within_batch"))
##
## Distribution:  Normal - Pooled Standard Deviation 	( n = 54, r = 3 )
## The following diagnostic tests were overridden:
##     `outliers_within_batch`
## B-Basis:   ( p = 0.9 , conf = 0.95 )
## CTD  127.6914
## ETW  125.0698
## RTD  132.1457

</code></pre>

<hr>
<h2 id='calc_cv_star'>Calculate the modified CV from the CV</h2><span id='topic+calc_cv_star'></span>

<h3>Description</h3>

<p>This function calculates the modified coefficient of variation (CV)
based on a (unmodified) CV.
The modified CV is calculated based on the rules in CMH-17-1G. Those
rules are:
</p>

<ul>
<li><p> For CV &lt; 4\%, CV* = 6\%
</p>
</li>
<li><p> For 4\% &lt;= CV &lt; 8\%, CV* = CV / 2 + 4\%
</p>
</li>
<li><p> For CV &gt; 8\%, CV* = CV
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>calc_cv_star(cv)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calc_cv_star_+3A_cv">cv</code></td>
<td>
<p>The CV to modify</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The value of the modified CV
</p>


<h3>References</h3>

<p>&quot;Composite Materials Handbook, Volume 1. Polymer Matrix Composites
Guideline for Characterization of Structural Materials,&quot;
SAE International, CMH-17-1G, Mar. 2012.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cv">cv()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># The modified CV for values of CV smaller than 4% is 6%
calc_cv_star(0.01)
## [1] 0.06

# The modified CV for values of CV larger than 8% is unchanged
calc_cv_star(0.09)
## [1] 0.09

</code></pre>

<hr>
<h2 id='carbon.fabric'>Sample data for a generic carbon fabric</h2><span id='topic+carbon.fabric'></span><span id='topic+carbon.fabric.2'></span>

<h3>Description</h3>

<p>Datasets containing sample data that is typical of a generic carbon
fabric prepreg. This data is used in several examples within the
<code>cmstatr</code> package. This data is fictional and should
only be used for learning how to use this package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>carbon.fabric

carbon.fabric.2
</code></pre>


<h3>Format</h3>

<p>An object of class <code>data.frame</code> with 216 rows and 5 columns.
</p>
<p>An object of class <code>data.frame</code> with 177 rows and 9 columns.
</p>

<hr>
<h2 id='cv'>Calculate the coefficient of variation</h2><span id='topic+cv'></span>

<h3>Description</h3>

<p>The coefficient of variation (CV) is the ratio of the standard
deviation to the mean of a sample. This function takes a vector
of data and calculates the CV.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv(x, na.rm = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv_+3A_x">x</code></td>
<td>
<p>a vector</p>
</td></tr>
<tr><td><code id="cv_+3A_na.rm">na.rm</code></td>
<td>
<p>logical. Should missing values be removed?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The calculated CV
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(15)  # make this example reproducible
x &lt;- rnorm(100, mean = 100, sd = 5)
cv(x)
## [1] 0.04944505

# the cv function can also be used within a call to dplyr::summarise
library(dplyr)
carbon.fabric %&gt;%
filter(test == "WT") %&gt;%
  group_by(condition) %&gt;%
  summarise(mean = mean(strength), cv = cv(strength))

## # A tibble: 3 x 3
##   condition  mean     cv
##   &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;
## 1 CTD        137. 0.0417
## 2 ETW        135. 0.0310
## 3 RTD        142. 0.0451


</code></pre>

<hr>
<h2 id='equiv_change_mean'>Equivalency based on change in mean value</h2><span id='topic+equiv_change_mean'></span>

<h3>Description</h3>

<p>Checks for change in the mean value between a qualification data set and
a sample. This is normally used to check for properties such as modulus.
This function is a wrapper for a two-sample t&ndash;test.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>equiv_change_mean(
  df_qual = NULL,
  data_qual = NULL,
  n_qual = NULL,
  mean_qual = NULL,
  sd_qual = NULL,
  data_sample = NULL,
  n_sample = NULL,
  mean_sample = NULL,
  sd_sample = NULL,
  alpha,
  modcv = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="equiv_change_mean_+3A_df_qual">df_qual</code></td>
<td>
<p>(optional) a data.frame containing the qualification data.
Defaults to NULL.</p>
</td></tr>
<tr><td><code id="equiv_change_mean_+3A_data_qual">data_qual</code></td>
<td>
<p>(optional) a vector of observations from the
&quot;qualification&quot; data to which equivalency is being tested. Or the column of
<code>df_qual</code> that contains this data. Defaults to NULL</p>
</td></tr>
<tr><td><code id="equiv_change_mean_+3A_n_qual">n_qual</code></td>
<td>
<p>the number of observations in the qualification data to which
the sample is being compared for equivalency</p>
</td></tr>
<tr><td><code id="equiv_change_mean_+3A_mean_qual">mean_qual</code></td>
<td>
<p>the mean from the qualification data to which the sample
is being compared for equivalency</p>
</td></tr>
<tr><td><code id="equiv_change_mean_+3A_sd_qual">sd_qual</code></td>
<td>
<p>the standard deviation from the qualification data to which
the sample is being compared for equivalency</p>
</td></tr>
<tr><td><code id="equiv_change_mean_+3A_data_sample">data_sample</code></td>
<td>
<p>a vector of observations from the sample being compared
for equivalency</p>
</td></tr>
<tr><td><code id="equiv_change_mean_+3A_n_sample">n_sample</code></td>
<td>
<p>the number of observations in the sample being compared for
equivalency</p>
</td></tr>
<tr><td><code id="equiv_change_mean_+3A_mean_sample">mean_sample</code></td>
<td>
<p>the mean of the sample being compared for equivalency</p>
</td></tr>
<tr><td><code id="equiv_change_mean_+3A_sd_sample">sd_sample</code></td>
<td>
<p>the standard deviation of the sample being compared for
equivalency</p>
</td></tr>
<tr><td><code id="equiv_change_mean_+3A_alpha">alpha</code></td>
<td>
<p>the acceptable probability of a Type I error</p>
</td></tr>
<tr><td><code id="equiv_change_mean_+3A_modcv">modcv</code></td>
<td>
<p>a logical value indicating whether the modified CV approach
should be used. Defaults to <code>FALSE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are several optional arguments to this function. Either (but not both)
<code>data_sample</code> or all of <code>n_sample</code>, <code>mean_sample</code> and
<code>sd_sample</code> must be supplied. And, either (but not both)
<code>data_qual</code>
(and also <code>df_qual</code> if <code>data_qual</code> is a column name and not a
vector) or all of <code>n_qual</code>, <code>mean_qual</code> and <code>sd_qual</code> must
be supplied. If these requirements are violated, warning(s) or error(s) will
be issued.
</p>
<p>This function uses a two-sample t-test to determine if there is a difference
in the mean value of the qualification data and the sample. A pooled
standard deviation is used in the t-test. The procedure is per CMH-17-1G.
</p>
<p>If <code>modcv</code> is TRUE, the standard deviation used to calculate the
thresholds will be replaced with a standard deviation calculated
using the Modified Coefficient of Variation (CV) approach.
The Modified CV approach is a way of adding extra variance to the
qualification data in the case that the qualification data has less
variance than expected, which sometimes occurs when qualification testing
is performed in a short period of time.
Using the Modified CV approach, the standard deviation is calculated by
multiplying <code>CV_star * mean_qual</code> where <code>mean_qual</code> is either the
value supplied or the value calculated by <code>mean(data_qual)</code> and
<code class="reqn">CV*</code> is determined using <code><a href="#topic+calc_cv_star">calc_cv_star()</a></code>.
</p>
<p>Note that the modified CV option should only be used if that data passes the
Anderson&ndash;Darling test.
</p>


<h3>Value</h3>


<ul>
<li> <p><code>call</code> the expression used to call this function
</p>
</li>
<li> <p><code>alpha</code> the value of alpha passed to this function
</p>
</li>
<li> <p><code>n_sample</code> the number of observations in the sample for which
equivalency is being checked. This is either the value <code>n_sample</code>
passed to this function or the length of the vector <code>data_sample</code>.
</p>
</li>
<li> <p><code>mean_sample</code> the mean of the observations in the sample for
which equivalency is being checked. This is either the value
<code>mean_sample</code> passed to this function or the mean of the vector
<code>data-sample</code>.
</p>
</li>
<li> <p><code>sd_sample</code> the standard deviation of the observations in the
sample for which equivalency is being checked. This is either the value
<code>mean_sample</code> passed to this function or the standard deviation of
the vector <code>data-sample</code>.
</p>
</li>
<li> <p><code>n_qual</code> the number of observations in the qualification data
to which the sample is being compared for equivalency. This is either
the value <code>n_qual</code> passed to this function or the length of the
vector <code>data_qual</code>.
</p>
</li>
<li> <p><code>mean_qual</code> the mean of the qualification data to which the
sample is being compared for equivalency. This is either the value
<code>mean_qual</code> passed to this function or the mean of the vector
<code>data_qual</code>.
</p>
</li>
<li> <p><code>sd_qual</code> the standard deviation of the qualification data to
which the sample is being compared for equivalency. This is either the
value <code>mean_qual</code> passed to this function or the standard deviation
of the vector <code>data_qual</code>.
</p>
</li>
<li> <p><code>modcv</code> logical value indicating whether the equivalency
calculations were performed using the modified CV approach
</p>
</li>
<li> <p><code>sp</code> the value of the pooled standard deviation. If
<code>modecv = TRUE</code>, this pooled standard deviation includes the
modification to the qualification CV.
</p>
</li>
<li> <p><code>t0</code> the test statistic
</p>
</li>
<li> <p><code>t_req</code> the t-value for <code class="reqn">\alpha / 2</code> and
<code class="reqn">df = n1 + n2 -2</code>
</p>
</li>
<li> <p><code>threshold</code> a vector with two elements corresponding to the
minimum and maximum values of the sample mean that would result in a
pass
</p>
</li>
<li> <p><code>result</code> a character vector of either &quot;PASS&quot; or &quot;FAIL&quot;
indicating the result of the test for change in mean
</p>
</li></ul>



<h3>References</h3>

<p>“Composite Materials Handbook, Volume 1. Polymer Matrix Composites
Guideline for Characterization of Structural Materials,” SAE International,
CMH-17-1G, Mar. 2012.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+calc_cv_star">calc_cv_star()</a></code>
</p>
<p><code><a href="stats.html#topic+t.test">stats::t.test()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>equiv_change_mean(alpha = 0.05, n_sample = 9, mean_sample = 9.02,
                  sd_sample = 0.15785, n_qual = 28, mean_qual = 9.24,
                  sd_qual = 0.162, modcv = TRUE)

## Call:
## equiv_change_mean(n_qual = 28, mean_qual = 9.24, sd_qual = 0.162,
##                   n_sample = 9, mean_sample = 9.02, sd_sample = 0.15785,
##                   alpha = 0.05,modcv = TRUE)
##
## For alpha = 0.05
## Modified CV used
##                   Qualification        Sample
##           Number        28               9
##             Mean       9.24             9.02
##               SD      0.162           0.15785
##           Result               PASS
##    Passing Range       8.856695 to 9.623305

</code></pre>

<hr>
<h2 id='equiv_mean_extremum'>Test for decrease in mean or minimum individual</h2><span id='topic+equiv_mean_extremum'></span>

<h3>Description</h3>

<p>This test is used when determining if a new process or
manufacturing location produces material properties that are
&quot;equivalent&quot; to an existing dataset, and hence the existing
basis values are applicable to the new dataset. This test is also
sometimes used for determining if a new batch of material is acceptable.
This function determines thresholds based on both minimum
individual and mean, and optionally evaluates a sample against those
thresholds. The joint distribution between the sample mean
and sample minimum is used to generate these thresholds.
When there is no true difference between the existing (&quot;qualification&quot;)
and the new population from which the sample is obtained, there is a
probability of <code class="reqn">\alpha</code> of falsely concluding that there is a
difference in mean or variance. It is assumed that both the original
and new populations are normally distributed.
According to Vangel (2002), this test provides improved power compared
with a test of mean and standard deviation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>equiv_mean_extremum(
  df_qual = NULL,
  data_qual = NULL,
  mean_qual = NULL,
  sd_qual = NULL,
  data_sample = NULL,
  n_sample = NULL,
  alpha,
  modcv = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="equiv_mean_extremum_+3A_df_qual">df_qual</code></td>
<td>
<p>(optional) a data.frame containing the qualification data.
Defaults to NULL.</p>
</td></tr>
<tr><td><code id="equiv_mean_extremum_+3A_data_qual">data_qual</code></td>
<td>
<p>(optional) a vector of observations from the
&quot;qualification&quot; data to which equivalency is being tested. Or the column of
<code>df_qual</code> that contains this data. Defaults to NULL</p>
</td></tr>
<tr><td><code id="equiv_mean_extremum_+3A_mean_qual">mean_qual</code></td>
<td>
<p>(optional) the mean from the &quot;qualification&quot; data to which
equivalency is being tested. Defaults to NULL</p>
</td></tr>
<tr><td><code id="equiv_mean_extremum_+3A_sd_qual">sd_qual</code></td>
<td>
<p>(optional) the standard deviation from the &quot;qualification&quot;
data to which equivalency is being tested. Defaults to NULL</p>
</td></tr>
<tr><td><code id="equiv_mean_extremum_+3A_data_sample">data_sample</code></td>
<td>
<p>(optional) a vector of observations from the sample for
which equivalency is being tested. Defaults to NULL</p>
</td></tr>
<tr><td><code id="equiv_mean_extremum_+3A_n_sample">n_sample</code></td>
<td>
<p>(optional) the number of observations in the sample for
which equivalency will be tested. Defaults to NULL</p>
</td></tr>
<tr><td><code id="equiv_mean_extremum_+3A_alpha">alpha</code></td>
<td>
<p>the acceptable probability of a type I error</p>
</td></tr>
<tr><td><code id="equiv_mean_extremum_+3A_modcv">modcv</code></td>
<td>
<p>(optional) a boolean value indicating whether a modified CV
should be used. Defaults to FALSE, in which case the standard deviation
supplied (or calculated from <code>data_qual</code>) will be used directly.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is used to
determine acceptance limits for a sample mean and sample minimum.
These acceptance limits are often used to set acceptance limits for
material strength for each lot of material, or each new manufacturing
site. When a sample meets the criteria that its mean and its minimum are
both greater than these limits, then one may accept the lot of material
or the new manufacturing site.
</p>
<p>This procedure is used to ensure that the strength of material processed
at a second site, or made with a new batch of material are not degraded
relative to the data originally used to determine basis values for the
material. For more information about the use of this procedure, see
CMH-17-1G or PS-ACE 100-2002-006.
</p>
<p>There are several optional arguments to this function. However, you can't
omit all of the optional arguments. You must supply either
<code>data_sample</code> or <code>n_sample</code>, but not both. You must also supply
either <code>data_qual</code> (and <code>df_qual</code> if <code>data_qual</code> is a
variable name and not a vector) or both <code>mean_qual</code> and <code>sd_qual</code>,
but if you supply <code>data_qual</code> (and possibly <code>df_qual</code>) you should
not supply either <code>mean_qual</code> or <code>sd_qual</code> (and visa-versa). This
function will issue a warning or error if you violate any of these rules.
</p>
<p>If <code>modcv</code> is TRUE, the standard deviation used to calculate the
thresholds will be replaced with a standard deviation calculated
using the Modified Coefficient of Variation (CV) approach.
The Modified CV approach is a way of adding extra variance to the
qualification data in the case that the qualification data has less
variance than expected, which sometimes occurs when qualification testing
is performed in a short period of time.
Using the Modified CV approach, the standard deviation is calculated by
multiplying <code>CV_star * mean_qual</code> where <code>mean_qual</code> is either the
value supplied or the value calculated by <code>mean(data_qual)</code> and
<code class="reqn">CV*</code> is the value computed by <code><a href="#topic+calc_cv_star">calc_cv_star()</a></code>.
</p>


<h3>Value</h3>

<p>Returns an object of class <code>equiv_mean_extremum</code>. This object is a list
with the following named elements:
</p>

<ul>
<li> <p><code>call</code> the expression used to call this function
</p>
</li>
<li> <p><code>alpha</code> the value of alpha passed to this function
</p>
</li>
<li> <p><code>n_sample</code> the number of observations in the sample for which
equivalency is being checked. This is either the value <code>n_sample</code>
passed to this function or the length of the vector <code>data_sample</code>.
</p>
</li>
<li> <p><code>k1</code> the factor used to calculate the minimum individual
threshold. The minimum individual threshold is calculated as
<code class="reqn">W_{min} = qual\,mean - k_1 \cdot qual\,sd</code>
</p>
</li>
<li> <p><code>k2</code> the factor used to calculate the threshold for mean. The
threshold for mean is calculated as
<code class="reqn">W_{mean} = qual\,mean - k_2 \cdot qual\,sd</code>
</p>
</li>
<li> <p><code>modcv</code> logical value indicating whether the acceptance
thresholds are calculated using the modified CV approach
</p>
</li>
<li> <p><code>cv</code> the coefficient of variation of the qualification data.
This value is not modified, even if <code>modcv=TRUE</code>
</p>
</li>
<li> <p><code>cv_star</code> The modified coefficient of variation. If
<code>modcv=FALSE</code>, this will be <code>NULL</code>
</p>
</li>
<li> <p><code>threshold_min_indiv</code> The calculated threshold value for
minimum individual
</p>
</li>
<li> <p><code>threshold_mean</code> The calculated threshold value for mean
</p>
</li>
<li> <p><code>result_min_indiv</code> a character vector of either &quot;PASS&quot; or
&quot;FAIL&quot; indicating whether the data from <code>data_sample</code> passes the
test for minimum individual. If <code>data_sample</code> was not supplied,
this value will be <code>NULL</code>
</p>
</li>
<li> <p><code>result_mean</code> a character vector of either &quot;PASS&quot; or
&quot;FAIL&quot; indicating whether the data from <code>data_sample</code> passes the
test for mean. If <code>data_sample</code> was not supplied, this value will
be  <code>NULL</code>
</p>
</li>
<li> <p><code>min_sample</code> The minimum value from the vector
<code>data_sample</code>. if <code>data_sample</code> was not supplied, this will
have a value of <code>NULL</code>
</p>
</li>
<li> <p><code>mean_sample</code> The mean value from the vector
<code>data_sample</code>. If <code>data_sample</code> was not supplied, this will
have a value of <code>NULL</code>
</p>
</li></ul>



<h3>References</h3>

<p>M. G. Vangel. Lot Acceptance and Compliance Testing Using the Sample Mean
and an Extremum, Technometrics, vol. 44, no. 3. pp. 242–249. 2002.
</p>
<p>“Composite Materials Handbook, Volume 1. Polymer Matrix Composites
Guideline for Characterization of Structural Materials,” SAE International,
CMH-17-1G, Mar. 2012.
</p>
<p>Federal Aviation Administration, “Material Qualification and Equivalency
for Polymer Matrix Composite Material Systems,” PS-ACE 100-2002-006,
Sep. 2003.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+k_equiv">k_equiv()</a></code>
</p>
<p><code><a href="#topic+calc_cv_star">calc_cv_star()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>equiv_mean_extremum(alpha = 0.01, n_sample = 6,
                    mean_qual = 100, sd_qual = 5.5, modcv = TRUE)
##
## Call:
## equiv_mean_extremum(mean_qual = 100, sd_qual = 5.5, n_sample = 6,
##     alpha = 0.01, modcv = TRUE)
##
## Modified CV used: CV* = 0.0675 ( CV = 0.055 )
##
## For alpha = 0.01 and n = 6
## ( k1 = 3.128346 and k2 = 1.044342 )
##                   Min Individual   Sample Mean
##      Thresholds:    78.88367        92.95069

</code></pre>

<hr>
<h2 id='glance.adk'>Glance at a <code>adk</code> (Anderson&ndash;Darling k-Sample) object</h2><span id='topic+glance.adk'></span>

<h3>Description</h3>

<p>Glance accepts an object of type <code>adk</code> and returns a
<code><a href="tibble.html#topic+tibble">tibble::tibble()</a></code> with
one row of summaries.
</p>
<p>Glance does not do any calculations: it just gathers the results in a
tibble.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'adk'
glance(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="glance.adk_+3A_x">x</code></td>
<td>
<p>an <code>adk</code> object</p>
</td></tr>
<tr><td><code id="glance.adk_+3A_...">...</code></td>
<td>
<p>Additional arguments. Not used. Included only to match generic
signature.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A one-row <code><a href="tibble.html#topic+tibble">tibble::tibble()</a></code> with the following
columns:
</p>

<ul>
<li> <p><code>alpha</code> the significance level for the test
</p>
</li>
<li> <p><code>n</code> the sample size for the test
</p>
</li>
<li> <p><code>k</code> the number of samples
</p>
</li>
<li> <p><code>sigma</code> the computed standard deviation of the test statistic
</p>
</li>
<li> <p><code>ad</code> the test statistic
</p>
</li>
<li> <p><code>p</code> the p-value of the test
</p>
</li>
<li> <p><code>reject_same_dist</code> whether the test concludes that the samples
are drawn from different populations
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+ad_ksample">ad_ksample()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c(rnorm(20, 100, 5), rnorm(20, 105, 6))
k &lt;- c(rep(1, 20), rep(2, 20))
a &lt;- ad_ksample(x = x, groups = k)
glance(a)

## A tibble: 1 x 7
##   alpha     n     k sigma    ad       p reject_same_dist
##   &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;lgl&gt;
## 1 0.025    40     2 0.727  4.37 0.00487 TRUE

</code></pre>

<hr>
<h2 id='glance.anderson_darling'>Glance at an <code>anderson_darling</code> object</h2><span id='topic+glance.anderson_darling'></span>

<h3>Description</h3>

<p>Glance accepts an object of type <code>anderson_darling</code> and
returns a <code><a href="tibble.html#topic+tibble">tibble::tibble()</a></code> with
one row of summaries.
</p>
<p>Glance does not do any calculations: it just gathers the results in a
tibble.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'anderson_darling'
glance(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="glance.anderson_darling_+3A_x">x</code></td>
<td>
<p>an <code>anderson_darling</code> object</p>
</td></tr>
<tr><td><code id="glance.anderson_darling_+3A_...">...</code></td>
<td>
<p>Additional arguments. Not used. Included only to match generic
signature.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A one-row <code><a href="tibble.html#topic+tibble">tibble::tibble()</a></code> with the following
columns:
</p>

<ul>
<li> <p><code>dist</code> the distribution used
</p>
</li>
<li> <p><code>n</code> the number of observations in the sample
</p>
</li>
<li> <p><code>A</code> the Anderson&ndash;Darling test statistic
</p>
</li>
<li> <p><code>osl</code> the observed significance level (p-value),
assuming the
parameters of the distribution are estimated from the data
</p>
</li>
<li> <p><code>alpha</code> the required significance level for the test.
This value is given by the user.
</p>
</li>
<li> <p><code>reject_distribution</code> a logical value indicating whether
the hypothesis that the data is drawn from the specified distribution
should be rejected
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+anderson_darling">anderson_darling()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rnorm(100, 100, 4)
ad &lt;- anderson_darling_weibull(x = x)
glance(ad)

## # A tibble: 1 x 6
##   dist        n     A        osl alpha reject_distribution
##   &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;
## 1 Weibull   100  2.62 0.00000207  0.05 TRUE

</code></pre>

<hr>
<h2 id='glance.basis'>Glance at a basis object</h2><span id='topic+glance.basis'></span>

<h3>Description</h3>

<p>Glance accepts an object of type basis and returns a
<code><a href="tibble.html#topic+tibble">tibble::tibble()</a></code> with
one row of summaries for each basis value.
</p>
<p>Glance does not do any calculations: it just gathers the results in a
tibble.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'basis'
glance(x, include_diagnostics = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="glance.basis_+3A_x">x</code></td>
<td>
<p>a basis object</p>
</td></tr>
<tr><td><code id="glance.basis_+3A_include_diagnostics">include_diagnostics</code></td>
<td>
<p>a logical value indicating whether to include
columns for diagnostic tests. Default FALSE.</p>
</td></tr>
<tr><td><code id="glance.basis_+3A_...">...</code></td>
<td>
<p>Additional arguments. Not used. Included only to match generic
signature.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For the pooled basis methods (<code>basis_pooled_cv</code> and
<code>basis_pooled_sd</code>), the <code><a href="tibble.html#topic+tibble">tibble::tibble()</a></code>
returned by <code>glance</code> will have one row for each group included in
the pooling. For all other basis methods, the resulting <code>tibble</code>
will have a single row.
</p>
<p>If <code>include_diagnostics=TRUE</code>, there will be additional columns
corresponding with the diagnostic tests performed. These column(s) will
be of type character and will contain a &quot;P&quot; if the diagnostic test
passed, a &quot;F&quot; if the diagnostic test failed, an &quot;O&quot; if the diagnostic
test was overridden or <code>NA</code> if the test was not run (typically
because an optional argument was not passed to the function that
computed the basis value).
</p>


<h3>Value</h3>

<p>A <code><a href="tibble.html#topic+tibble">tibble::tibble()</a></code> with the following
columns:
</p>

<ul>
<li> <p><code>p</code> the the content of the tolerance bound. Normally 0.90 or 0.99
</p>
</li>
<li> <p><code>conf</code> the confidence level. Normally 0.95
</p>
</li>
<li> <p><code>distribution</code> a string representing the distribution assumed
when calculating the basis value
</p>
</li>
<li> <p><code>modcv</code> a logical value indicating whether the modified
CV approach was used. Only applicable to pooling methods.
</p>
</li>
<li> <p><code>n</code> the sample size
</p>
</li>
<li> <p><code>r</code> the number of groups used in the calculation. This will
be <code>NA</code> for single-point basis values
</p>
</li>
<li> <p><code>basis</code> the basis value
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+basis">basis()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(10)
x &lt;- rnorm(20, 100, 5)
b &lt;- basis_normal(x = x)
glance(b)

## # A tibble: 1 x 7
##       p  conf distribution modcv     n r     basis
##   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;        &lt;lgl&gt; &lt;int&gt; &lt;lgl&gt; &lt;dbl&gt;
## 1   0.9  0.95 Normal       FALSE    20 NA     92.0


glance(b, include_diagnostics = TRUE)

## # A tibble: 1 x 11
##        p  conf distribution modcv     n r     basis outliers_within…
##    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;        &lt;lgl&gt; &lt;int&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;chr&gt;
##  1   0.9  0.95 Normal       FALSE    20 NA     92.0 NA
## # … with 3 more variables: between_batch_variability &lt;chr&gt;,
## #   outliers &lt;chr&gt;, anderson_darling_normal &lt;chr&gt;

</code></pre>

<hr>
<h2 id='glance.equiv_change_mean'>Glance at a <code>equiv_change_mean</code> object</h2><span id='topic+glance.equiv_change_mean'></span>

<h3>Description</h3>

<p>Glance accepts an object of type <code>equiv_change_mean</code>
and returns a <code><a href="tibble.html#topic+tibble">tibble::tibble()</a></code> with
one row of summaries.
</p>
<p>Glance does not do any calculations: it just gathers the results in a
tibble.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'equiv_change_mean'
glance(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="glance.equiv_change_mean_+3A_x">x</code></td>
<td>
<p>a <code>equiv_change_mean</code> object returned from
<code><a href="#topic+equiv_change_mean">equiv_change_mean()</a></code></p>
</td></tr>
<tr><td><code id="glance.equiv_change_mean_+3A_...">...</code></td>
<td>
<p>Additional arguments. Not used. Included only to match generic
signature.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A one-row <code><a href="tibble.html#topic+tibble">tibble::tibble()</a></code> with the following
columns:
</p>

<ul>
<li> <p><code>alpha</code> the value of alpha passed to this function
</p>
</li>
<li> <p><code>n_sample</code> the number of observations in the sample for which
equivalency is being checked. This is either the value <code>n_sample</code>
passed to this function or the length of the vector <code>data_sample</code>.
</p>
</li>
<li> <p><code>mean_sample</code> the mean of the observations in the sample for
which equivalency is being checked. This is either the value
<code>mean_sample</code> passed to this function or the mean of the vector
<code>data-sample</code>.
</p>
</li>
<li> <p><code>sd_sample</code> the standard deviation of the observations in the
sample for which equivalency is being checked. This is either the value
<code>mean_sample</code> passed to this function or the standard deviation of
the vector <code>data-sample</code>.
</p>
</li>
<li> <p><code>n_qual</code> the number of observations in the qualification data
to which the sample is being compared for equivalency. This is either
the value <code>n_qual</code> passed to this function or the length of the
vector <code>data_qual</code>.
</p>
</li>
<li> <p><code>mean_qual</code> the mean of the qualification data to which the
sample is being compared for equivalency. This is either the value
<code>mean_qual</code> passed to this function or the mean of the vector
<code>data_qual</code>.
</p>
</li>
<li> <p><code>sd_qual</code> the standard deviation of the qualification data to
which the sample is being compared for equivalency. This is either the
value <code>mean_qual</code> passed to this function or the standard deviation
of the vector <code>data_qual</code>.
</p>
</li>
<li> <p><code>modcv</code> logical value indicating whether the equivalency
calculations were performed using the modified CV approach
</p>
</li>
<li> <p><code>sp</code> the value of the pooled standard deviation. If
<code>modecv = TRUE</code>, this pooled standard deviation includes the
modification to the qualification CV.
</p>
</li>
<li> <p><code>t0</code> the test statistic
</p>
</li>
<li> <p><code>t_req</code> the t-value for <code class="reqn">\alpha / 2</code> and
<code class="reqn">df = n1 + n2 -2</code>
</p>
</li>
<li> <p><code>threshold_min</code> the minimum value of the sample mean that would
result in a pass
</p>
</li>
<li> <p><code>threshold_max</code> the maximum value of the sample mean that would
result in a pass
</p>
</li>
<li> <p><code>result</code> a character vector of either &quot;PASS&quot; or &quot;FAIL&quot;
indicating the result of the test for change in mean
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+equiv_change_mean">equiv_change_mean()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x0 &lt;- rnorm(30, 100, 4)
x1 &lt;- rnorm(5, 91, 7)
eq &lt;- equiv_change_mean(data_qual = x0, data_sample = x1, alpha = 0.01)
glance(eq)

## # A tibble: 1 x 14
##   alpha n_sample mean_sample sd_sample n_qual mean_qual sd_qual modcv
##   &lt;dbl&gt;    &lt;int&gt;       &lt;dbl&gt;     &lt;dbl&gt;  &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;lgl&gt;
## 1  0.01        5        85.8      9.93     30      100.    3.90 FALSE
## # ... with 6 more variables: sp &lt;dbl&gt;, t0 &lt;dbl&gt;, t_req &lt;dbl&gt;,
## #   threshold_min &lt;dbl&gt;, threshold_max &lt;dbl&gt;, result &lt;chr&gt;

</code></pre>

<hr>
<h2 id='glance.equiv_mean_extremum'>Glance at an <code>equiv_mean_extremum</code> object</h2><span id='topic+glance.equiv_mean_extremum'></span>

<h3>Description</h3>

<p>Glance accepts an object of type <code>equiv_mean_extremum</code> and returns a
<code><a href="tibble.html#topic+tibble">tibble::tibble()</a></code> with
one row of summaries.
</p>
<p>Glance does not do any calculations: it just gathers the results in a
tibble.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'equiv_mean_extremum'
glance(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="glance.equiv_mean_extremum_+3A_x">x</code></td>
<td>
<p>an equiv_mean_extremum object returned from
<code><a href="#topic+equiv_mean_extremum">equiv_mean_extremum()</a></code></p>
</td></tr>
<tr><td><code id="glance.equiv_mean_extremum_+3A_...">...</code></td>
<td>
<p>Additional arguments. Not used. Included only to match generic
signature.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A one-row <code><a href="tibble.html#topic+tibble">tibble::tibble()</a></code> with the following
columns:
</p>

<ul>
<li> <p><code>alpha</code> the value of alpha passed to this function
</p>
</li>
<li> <p><code>n_sample</code> the number of observations in the sample for which
equivalency is being checked. This is either the value <code>n_sample</code>
passed to this function or the length of the vector <code>data_sample</code>.
</p>
</li>
<li> <p><code>modcv</code> logical value indicating whether the acceptance
thresholds are calculated using the modified CV approach
</p>
</li>
<li> <p><code>threshold_min_indiv</code> The calculated threshold value for
minimum individual
</p>
</li>
<li> <p><code>threshold_mean</code> The calculated threshold value for mean
</p>
</li>
<li> <p><code>result_min_indiv</code> a character vector of either &quot;PASS&quot; or
&quot;FAIL&quot; indicating whether the data from <code>data_sample</code> passes the
test for minimum individual. If <code>data_sample</code> was not supplied,
this value will be <code>NULL</code>
</p>
</li>
<li> <p><code>result_mean</code> a character vector of either &quot;PASS&quot; or
&quot;FAIL&quot; indicating whether the data from <code>data_sample</code> passes the
test for mean. If <code>data_sample</code> was not supplied, this value will
be  <code>NULL</code>
</p>
</li>
<li> <p><code>min_sample</code> The minimum value from the vector
<code>data_sample</code>. if <code>data_sample</code> was not supplied, this will
have a value of <code>NULL</code>
</p>
</li>
<li> <p><code>mean_sample</code> The mean value from the vector
<code>data_sample</code>. If <code>data_sample</code> was not supplied, this will
have a value of <code>NULL</code>
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+equiv_mean_extremum">equiv_mean_extremum()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x0 &lt;- rnorm(30, 100, 4)
x1 &lt;- rnorm(5, 91, 7)
eq &lt;- equiv_mean_extremum(data_qual = x0, data_sample = x1, alpha = 0.01)
glance(eq)

## # A tibble: 1 x 9
##   alpha n_sample modcv threshold_min_indiv threshold_mean
##   &lt;dbl&gt;    &lt;int&gt; &lt;lgl&gt;               &lt;dbl&gt;          &lt;dbl&gt;
## 1  0.01        5 FALSE                86.2           94.9
## # ... with 4 more variables: result_min_indiv &lt;chr&gt;, result_mean &lt;chr&gt;,
## #   min_sample &lt;dbl&gt;, mean_sample &lt;dbl&gt;

</code></pre>

<hr>
<h2 id='glance.levene'>Glance at a <code>levene</code> object</h2><span id='topic+glance.levene'></span>

<h3>Description</h3>

<p>Glance accepts an object of type <code>levene</code> and returns a
<code><a href="tibble.html#topic+tibble">tibble::tibble()</a></code> with
one row of summaries.
</p>
<p>Glance does not do any calculations: it just gathers the results in a
tibble.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'levene'
glance(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="glance.levene_+3A_x">x</code></td>
<td>
<p>a <code>levene</code> object returned from <code><a href="#topic+levene_test">levene_test()</a></code></p>
</td></tr>
<tr><td><code id="glance.levene_+3A_...">...</code></td>
<td>
<p>Additional arguments. Not used. Included only to match generic
signature.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A one-row <code><a href="tibble.html#topic+tibble">tibble::tibble()</a></code> with the following
columns:
</p>

<ul>
<li> <p><code>alpha</code> the value of alpha specified
</p>
</li>
<li> <p><code>modcv</code> a logical value indicating whether the modified
CV approach was used.
</p>
</li>
<li> <p><code>n</code> the total number of observations
</p>
</li>
<li> <p><code>k</code> the number of groups
</p>
</li>
<li> <p><code>f</code> the value of the F test statistic
</p>
</li>
<li> <p><code>p</code> the computed p-value
</p>
</li>
<li> <p><code>reject_equal_variance</code> a boolean value indicating whether the
null hypothesis that all samples have the same variance is rejected
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+levene_test">levene_test()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>df &lt;- data.frame(
  groups = c(rep("A", 5), rep("B", 6)),
  strength = c(rnorm(5, 100, 6), rnorm(6, 105, 7))
)
levene_result &lt;- levene_test(df, strength, groups)
glance(levene_result)

## # A tibble: 1 x 7
##   alpha modcv     n     k      f     p reject_equal_variance
##   &lt;dbl&gt; &lt;lgl&gt; &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;
## 1  0.05 FALSE    11     2 0.0191 0.893 FALSE

</code></pre>

<hr>
<h2 id='glance.mnr'>Glance at a <code>mnr</code> (maximum normed residual) object</h2><span id='topic+glance.mnr'></span>

<h3>Description</h3>

<p>Glance accepts an object of type <code>mnr</code> and returns a
<code><a href="tibble.html#topic+tibble">tibble::tibble()</a></code> with
one row of summaries.
</p>
<p>Glance does not do any calculations: it just gathers the results in a
tibble.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mnr'
glance(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="glance.mnr_+3A_x">x</code></td>
<td>
<p>An <code>mnr</code> object</p>
</td></tr>
<tr><td><code id="glance.mnr_+3A_...">...</code></td>
<td>
<p>Additional arguments. Not used. Included only to match generic
signature.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A one-row <code><a href="tibble.html#topic+tibble">tibble::tibble()</a></code> with the following
columns:
</p>

<ul>
<li> <p><code>mnr</code> the computed MNR test statistic
</p>
</li>
<li> <p><code>alpha</code> the value of alpha used for the test
</p>
</li>
<li> <p><code>crit</code> the critical value given the sample size and the
significance level
</p>
</li>
<li> <p><code>n_outliers</code> the number of outliers found
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+maximum_normed_residual">maximum_normed_residual()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c(rnorm(20, 100, 5), 10)
m &lt;- maximum_normed_residual(x = x)
glance(m)

## # A tibble: 1 x 4
##     mnr alpha  crit n_outliers
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;
## 1  4.23  0.05  2.73          1

</code></pre>

<hr>
<h2 id='hk_ext'>Calculate values related to Extended Hanson&ndash;Koopmans tolerance bounds</h2><span id='topic+hk_ext'></span><span id='topic+hk_ext_z'></span><span id='topic+hk_ext_z_j_opt'></span>

<h3>Description</h3>

<p>Calculates values related to Extended Hanson&ndash;Koopmans tolerance bounds
as described by Vangel (1994).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hk_ext_z(n, i, j, p, conf)

hk_ext_z_j_opt(n, p, conf)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hk_ext_+3A_n">n</code></td>
<td>
<p>the sample size</p>
</td></tr>
<tr><td><code id="hk_ext_+3A_i">i</code></td>
<td>
<p>the first order statistic (1 &lt;= i &lt; j)</p>
</td></tr>
<tr><td><code id="hk_ext_+3A_j">j</code></td>
<td>
<p>the second order statistic (i &lt; j &lt;= n)</p>
</td></tr>
<tr><td><code id="hk_ext_+3A_p">p</code></td>
<td>
<p>the content of the tolerance bound (normally 0.90 or 0.99)</p>
</td></tr>
<tr><td><code id="hk_ext_+3A_conf">conf</code></td>
<td>
<p>the confidence level (normally 0.95)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Hanson (1964) presents a nonparametric method for determining
tolerance bounds based on consecutive order statistics.
Vangel (1994) extends this method using non-consecutive order statistics.
</p>
<p>The extended Hanson&ndash;Koopmans method calculates a tolerance bound
(basis value) based on two order statistics and a weighting value
<code>z</code>. The value of <code>z</code> is based on the sample size, which
order statistics are selected, the desired content of the tolerance
bond and the desired confidence level.
</p>
<p>The function <code>hk_ext_z</code> calculates the weighting variable <code>z</code>
based on selected order statistics <code>i</code> and <code>j</code>. Based on this
value <code>z</code>, the tolerance bound can be calculated as:
</p>
<p style="text-align: center;"><code class="reqn">S = z X_{(i)} + (1 - z) X_{(j)}</code>
</p>

<p>Where <code class="reqn">X_{(i)}</code> and <code class="reqn">X_{(j)}</code> are the <code>i-th</code>
and <code>j-th</code> ordered observation.
</p>
<p>The function <code>hk_ext_z_j_opt</code> determines the value of <code>j</code> and
the corresponding value of <code>z</code>, assuming <code>i=1</code>. The value
of <code>j</code> is selected such that the computed tolerance limit is
nearest to the desired population quantile for a standard normal
distribution when the order statistics are equal to the expected
value of the order statistics for the standard normal distribution.
</p>


<h3>Value</h3>

<p>For <code>hk_ext_z</code>, the return value is a numeric value representing
the parameter z (denoted as k in CMH-17-1G).
</p>
<p>For <code>hk_ext_z_j_opt</code>, the return value is named list containing
<code>z</code> and <code>k</code>. The former is the value of z, as defined by
Vangel (1994), and the latter is the corresponding order statistic.
</p>


<h3>References</h3>

<p>M. Vangel, “One-Sided Nonparametric Tolerance Limits,”
Communications in Statistics - Simulation and Computation,
vol. 23, no. 4. pp. 1137–1154, 1994.
</p>
<p>D. L. Hanson and L. H. Koopmans,
“Tolerance Limits for the Class of Distributions with Increasing
Hazard Rates,” The Annals of Mathematical Statistics,
vol. 35, no. 4. pp. 1561–1570, 1964.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+basis_hk_ext">basis_hk_ext()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># The factors from Table 1 of Vangel (1994) can be recreated
# using the hk_ext_z function. For the sample size n=21,
# the median is the 11th ordered observation. The factor
# required for calculating the tolerance bound with a content
# of 0.9 and a confidence level of 0.95 based on the median
# and first ordered observation can be calculated as follows.
hk_ext_z(n = 21, i = 1, j = 11, p = 0.9, conf = 0.95)

## [1] 1.204806

# The hk_ext_z_j_opt function can be used to refine this value
# of z by finding an optimum value of j, rather than simply
# using the median. Here, we find that the optimal observation
# to use is the 10th, not the 11th (which is the median).
hk_ext_z_j_opt(n = 21, p = 0.9, conf = 0.95)

## $z
## [1] 1.217717
##
## $j
## [1] 10

</code></pre>

<hr>
<h2 id='k_equiv'>k-factors for determining acceptance based on sample mean and an extremum</h2><span id='topic+k_equiv'></span>

<h3>Description</h3>

<p>k-factors for determining acceptance based on sample mean and an extremum
</p>


<h3>Usage</h3>

<pre><code class='language-R'>k_equiv(alpha, n)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="k_equiv_+3A_alpha">alpha</code></td>
<td>
<p>the acceptable probability of a type I error</p>
</td></tr>
<tr><td><code id="k_equiv_+3A_n">n</code></td>
<td>
<p>the number of observations in the sample to test</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The k-factors returned by this function are used for determining
whether to accept a new dataset.
</p>
<p>This function is used as part of the procedure for
determining acceptance limits for a sample mean and sample minimum.
These acceptance limits are often used to set acceptance limits for
material strength for each lot of material, or each new manufacturing
site. When a sample meets the criteria that its mean and its minimum are
both greater than these limits, then one may accept the lot of material
or the new manufacturing site.
</p>
<p>This procedure is used to ensure that the strength of material processed
at a second site, or made with a new batch of material are not degraded
relative to the data originally used to determine basis values for the
material. For more information about the use of this procedure, see
CMH-17-1G or PS-ACE 100-2002-006.
</p>
<p>According to Vangel (2002), the use of mean and extremum for this purpose
is more powerful than the use of mean and standard deviation.
</p>
<p>The results of this function match those published by Vangel within
0.05\
by Vangel are identical to those published in CMH-17-1G.
</p>
<p>This function uses numerical integration and numerical optimization to
find values of the factors <code class="reqn">k_1</code> and <code class="reqn">k_2</code> based on Vangel's
saddle point approximation.
</p>
<p>The value <code class="reqn">n</code> refers to the number of observations in the sample
being compared with the original population (the qualification sample is
usually assumed to be equal to the population statistics).
</p>
<p>The value of <code class="reqn">alpha</code> is the acceptable probability of a type I error.
Normally, this is set to 0.05 for material or process equivalency and 0.01
when setting lot acceptance limits. Though, in principle, this parameter
can be set to any number between 0 and 1. This function, however, has only
been validated in the range of <code class="reqn">1e-5 \le alpha \le 0.5</code>.
</p>


<h3>Value</h3>

<p>a vector with elements c(k1, k2). k1 is for testing the sample
extremum. k2 is for testing the sample mean
</p>


<h3>References</h3>

<p>M. G. Vangel. Lot Acceptance and Compliance Testing Using the Sample Mean
and an Extremum, Technometrics, vol. 44, no. 3. pp. 242–249. 2002.
</p>
<p>“Composite Materials Handbook, Volume 1. Polymer Matrix Composites
Guideline for Characterization of Structural Materials,” SAE International,
CMH-17-1G, Mar. 2012.
</p>
<p>Federal Aviation Administration, “Material Qualification and Equivalency
for Polymer Matrix Composite Material Systems,” PS-ACE 100-2002-006,
Sep. 2003.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+equiv_mean_extremum">equiv_mean_extremum()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>qual_mean &lt;- 100
qual_sd &lt;- 3.5
k &lt;- k_equiv(0.01, 5)
print("Minimum Individual Acceptance Limit:")
print(qual_mean - qual_sd * k[1])
print("Minimum Average Acceptance Limit:")
print(qual_mean - qual_sd * k[2])

## [1] "Minimum Individual Acceptance Limit:"
## [1] 89.24981
## [1] "Minimum Average Acceptance Limit:"
## [1] 96.00123

</code></pre>

<hr>
<h2 id='k_factor_normal'>Calculate k factor for basis values (<code class="reqn">kB</code>, <code class="reqn">kA</code>) with normal
distribution</h2><span id='topic+k_factor_normal'></span>

<h3>Description</h3>

<p>The factors returned by this function are used when calculating basis
values (one-sided confidence bounds) when the data are normally
distributed. The basis value will
be equal to <code class="reqn">\bar{x} - k s</code>,
where <code class="reqn">\bar{x}</code> is the sample mean,
<code class="reqn">s</code> is the sample standard deviation and <code class="reqn">k</code> is the result
of this function.
This function is internally used by <code><a href="#topic+basis_normal">basis_normal()</a></code> when
computing basis values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>k_factor_normal(n, p = 0.9, conf = 0.95)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="k_factor_normal_+3A_n">n</code></td>
<td>
<p>the number of observations (i.e. coupons)</p>
</td></tr>
<tr><td><code id="k_factor_normal_+3A_p">p</code></td>
<td>
<p>the desired content of the tolerance bound.
Should be 0.90 for B-Basis and 0.99 for A-Basis</p>
</td></tr>
<tr><td><code id="k_factor_normal_+3A_conf">conf</code></td>
<td>
<p>confidence level. Should be 0.95 for both A- and B-Basis</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates the k factors used when determining A- and
B-Basis values for normally distributed data. To get <code class="reqn">kB</code>, set
the content of the tolerance bound to <code>p = 0.90</code> and
the confidence level to <code>conf = 0.95</code>. To get <code class="reqn">kA</code>, set
<code>p = 0.99</code> and <code>conf = 0.95</code>. While other tolerance bound
contents and confidence levels may be computed, they are infrequently
needed in practice.
</p>
<p>The k-factor is calculated using equation 2.2.3 of
Krishnamoorthy and Mathew (2008).
</p>
<p>This function has been validated against the <code class="reqn">kB</code> tables in
CMH-17-1G for each value of <code class="reqn">n</code> from <code class="reqn">n = 2</code> to <code class="reqn">n = 95</code>.
It has been validated against the <code class="reqn">kA</code> tables in CMH-17-1G for each
value of <code class="reqn">n</code> from <code class="reqn">n = 2</code> to <code class="reqn">n = 75</code>. Larger values of <code class="reqn">n</code>
also match the tables in CMH-17-1G, but R
emits warnings that &quot;full precision may not have been achieved.&quot; When
validating the results of this function against the tables in CMH-17-1G,
the maximum allowable difference between the two is 0.002. The tables in
CMH-17-1G give values to three decimal places.
</p>
<p>For more information about tolerance bounds in general, see
Meeker, et. al. (2017).
</p>


<h3>Value</h3>

<p>the calculated factor
</p>


<h3>References</h3>

<p>K. Krishnamoorthy and T. Mathew, Statistical Tolerance Regions: Theory,
Applications, and Computation. Hoboken: John Wiley &amp; Sons, 2008.
</p>
<p>W. Meeker, G. Hahn, and L. Escobar, Statistical Intervals: A Guide
for Practitioners and Researchers, Second Edition.
Hoboken: John Wiley &amp; Sons, 2017.
</p>
<p>“Composite Materials Handbook, Volume 1. Polymer Matrix Composites
Guideline for Characterization of Structural Materials,” SAE International,
CMH-17-1G, Mar. 2012.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+basis_normal">basis_normal()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>kb &lt;- k_factor_normal(n = 10, p = 0.9, conf = 0.95)
print(kb)

## [1] 2.35464

# This can be used to caclulate the B-Basis if
# the sample mean and sample standard deviation
# is known, and data is assumed to be normally
# distributed

sample_mean &lt;- 90
sample_sd &lt;- 5.2
print("B-Basis:")
print(sample_mean - sample_sd * kb)

## [1] B-Basis:
## [1] 77.75587

</code></pre>

<hr>
<h2 id='levene_test'>Levene's Test for Equality of Variance</h2><span id='topic+levene_test'></span>

<h3>Description</h3>

<p>This function performs the Levene's test for equality of variance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>levene_test(data = NULL, x, groups, alpha = 0.05, modcv = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="levene_test_+3A_data">data</code></td>
<td>
<p>a data.frame</p>
</td></tr>
<tr><td><code id="levene_test_+3A_x">x</code></td>
<td>
<p>the variable in the data.frame or a vector on which to perform the
Levene's test (usually strength)</p>
</td></tr>
<tr><td><code id="levene_test_+3A_groups">groups</code></td>
<td>
<p>a variable in the data.frame that defines the groups</p>
</td></tr>
<tr><td><code id="levene_test_+3A_alpha">alpha</code></td>
<td>
<p>the significance level (default 0.05)</p>
</td></tr>
<tr><td><code id="levene_test_+3A_modcv">modcv</code></td>
<td>
<p>a logical value indicating whether the modified CV approach
should be used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function performs the Levene's test for equality of variance. The
data is transformed as follows:
</p>
<p style="text-align: center;"><code class="reqn">w_{ij} = \left| x_{ij} - m_i \right|</code>
</p>

<p>Where <code class="reqn">m_i</code> is median of the <code class="reqn">ith</code> group. An F-Test is then
performed on the transformed data.
</p>
<p>When <code>modcv=TRUE</code>, the data from each group is first transformed
according to the modified coefficient of variation (CV) rules before
performing Levene's test.
</p>


<h3>Value</h3>

<p>Returns an object of class <code>adk</code>. This object has the following fields:
</p>

<ul>
<li> <p><code>call</code> the expression used to call this function
</p>
</li>
<li> <p><code>data</code> the original data supplied by the user
</p>
</li>
<li> <p><code>groups</code> a vector of the groups used in the computation
</p>
</li>
<li> <p><code>alpha</code> the value of alpha specified
</p>
</li>
<li> <p><code>modcv</code> a logical value indicating whether the modified
CV approach was used.
</p>
</li>
<li> <p><code>n</code> the total number of observations
</p>
</li>
<li> <p><code>k</code> the number of groups
</p>
</li>
<li> <p><code>f</code> the value of the F test statistic
</p>
</li>
<li> <p><code>p</code> the computed p-value
</p>
</li>
<li> <p><code>reject_equal_variance</code> a boolean value indicating whether the
null hypothesis that all samples have the same variance is rejected
</p>
</li>
<li> <p><code>modcv_transformed_data</code> the data after the modified CV transformation
</p>
</li></ul>



<h3>References</h3>

<p>“Composite Materials Handbook, Volume 1. Polymer Matrix Composites
Guideline for Characterization of Structural Materials,” SAE International,
CMH-17-1G, Mar. 2012.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+calc_cv_star">calc_cv_star()</a></code>
</p>
<p><code><a href="#topic+transform_mod_cv">transform_mod_cv()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)

carbon.fabric.2 %&gt;%
  filter(test == "FC") %&gt;%
  levene_test(strength, condition)
##
## Call:
## levene_test(data = ., x = strength, groups = condition)
##
## n = 91          k = 5
## F = 3.883818    p-value = 0.00600518
## Conclusion: Samples have unequal variance ( alpha = 0.05 )

</code></pre>

<hr>
<h2 id='maximum_normed_residual'>Detect outliers using the maximum normed residual method</h2><span id='topic+maximum_normed_residual'></span>

<h3>Description</h3>

<p>This function detects outliers using the maximum normed residual
method described in CMH-17-1G. This method identifies a value
as an outlier if the absolute difference between the value and
the sample mean divided by the sample standard deviation
exceeds a critical value.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>maximum_normed_residual(data = NULL, x, alpha = 0.05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="maximum_normed_residual_+3A_data">data</code></td>
<td>
<p>a data.frame</p>
</td></tr>
<tr><td><code id="maximum_normed_residual_+3A_x">x</code></td>
<td>
<p>the variable in the data.frame for which to find the MNR
or a vector if <code>data=NULL</code></p>
</td></tr>
<tr><td><code id="maximum_normed_residual_+3A_alpha">alpha</code></td>
<td>
<p>the significance level for the test. Defaults to 0.05</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>data</code> is an optional argument. If <code>data</code> is given, it
should be a
<code>data.frame</code> (or similar object). When <code>data</code> is specified, the
value of <code>x</code> is expected to be a variable within <code>data</code>. If
<code>data</code> is not specified, <code>x</code> must be a vector.
</p>
<p>The maximum normed residual test is a test for outliers. The test statistic
is given in CMH-17-1G. Outliers are identified in the returned object.
</p>
<p>The maximum normed residual test statistic is defined as:
</p>
<p style="text-align: center;"><code class="reqn">MNR = max \frac{\left| x_i - \bar{x} \right|}{s} </code>
</p>

<p>When the value of the MNR test statistic exceeds the critical value
defined in Section 8.3.3.1 of CMH-17-1G, the corresponding value
is identified as an outlier. It is then removed from the sample, and
the test statistic is computed again and compared with the critical
value corresponding with the new sample. This process is repeated until
no values are identified as outliers.
</p>


<h3>Value</h3>

<p>an object of class <code>mnr</code>
This object has the following fields:
</p>

<ul>
<li> <p><code>call</code> the expression used to call this function
</p>
</li>
<li> <p><code>data</code> the original data used to compute the MNR
</p>
</li>
<li> <p><code>alpha</code> the value of alpha given by the user
</p>
</li>
<li> <p><code>mnr</code> the computed MNR test statistic
</p>
</li>
<li> <p><code>crit</code> the critical value given the sample size and the
significance level
</p>
</li>
<li> <p><code>outliers</code> a data.frame containing the <code>index</code> and
<code>value</code> of each of the identified outliers
</p>
</li>
<li> <p><code>n_outliers</code> the number of outliers found
</p>
</li></ul>



<h3>References</h3>

<p>“Composite Materials Handbook, Volume 1. Polymer Matrix Composites
Guideline for Characterization of Structural Materials,” SAE International,
CMH-17-1G, Mar. 2012.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)

carbon.fabric.2 %&gt;%
  filter(test=="FC" &amp; condition=="ETW2" &amp; batch=="A") %&gt;%
  maximum_normed_residual(strength)

## Call:
## maximum_normed_residual(data = ., x = strength)
##
## MNR =  1.958797  ( critical value = 1.887145 )
##
## Outliers ( alpha = 0.05 ):
##   Index  Value
##       6  44.26

carbon.fabric.2 %&gt;%
  filter(test=="FC" &amp; condition=="ETW2" &amp; batch=="B") %&gt;%
  maximum_normed_residual(strength)

## Call:
## maximum_normed_residual(data = ., x = strength)
##
## MNR =  1.469517  ( critical value = 1.887145 )
##
## No outliers detected ( alpha = 0.05 )

</code></pre>

<hr>
<h2 id='nested_data_plot'>Create a plot of nested sources of variation</h2><span id='topic+nested_data_plot'></span>

<h3>Description</h3>

<p>Creates a plot showing the breakdown of variation within a sample. This
function uses <a href="ggplot2.html#topic+ggplot2">ggplot2</a> internally.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nested_data_plot(
  dat,
  x,
  groups = c(),
  stat = "mean",
  ...,
  y_gap = 1,
  divider_color = "grey50",
  point_args = list(),
  dline_args = list(),
  vline_args = list(),
  hline_args = list(),
  label_args = list(),
  connector_args = list()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nested_data_plot_+3A_dat">dat</code></td>
<td>
<p>a <code>data.frame</code> or similar object</p>
</td></tr>
<tr><td><code id="nested_data_plot_+3A_x">x</code></td>
<td>
<p>the variable within <code>dat</code> to plot. Most often this would be a
strength or modulus variable.</p>
</td></tr>
<tr><td><code id="nested_data_plot_+3A_groups">groups</code></td>
<td>
<p>a vector of variables to group the data by</p>
</td></tr>
<tr><td><code id="nested_data_plot_+3A_stat">stat</code></td>
<td>
<p>a function for computing the central location for each group.
This is normally &quot;mean&quot; but could be &quot;median&quot; or another
function.</p>
</td></tr>
<tr><td><code id="nested_data_plot_+3A_...">...</code></td>
<td>
<p>extra options. See Details.</p>
</td></tr>
<tr><td><code id="nested_data_plot_+3A_y_gap">y_gap</code></td>
<td>
<p>the vertical gap between grouping variables</p>
</td></tr>
<tr><td><code id="nested_data_plot_+3A_divider_color">divider_color</code></td>
<td>
<p>the color of the lines between grouping variables.
Or <code>NULL</code> to omit these lines.</p>
</td></tr>
<tr><td><code id="nested_data_plot_+3A_point_args">point_args</code></td>
<td>
<p>arguments to pass to <a href="ggplot2.html#topic+geom_point">ggplot2::geom_point</a> when plotting
individual data points.</p>
</td></tr>
<tr><td><code id="nested_data_plot_+3A_dline_args">dline_args</code></td>
<td>
<p>arguments to pass to <a href="ggplot2.html#topic+geom_segment">ggplot2::geom_segment</a> when plotting
the horizontal lines between data points.</p>
</td></tr>
<tr><td><code id="nested_data_plot_+3A_vline_args">vline_args</code></td>
<td>
<p>arguments to pass to <a href="ggplot2.html#topic+geom_segment">ggplot2::geom_segment</a> when plotting
vertical lines</p>
</td></tr>
<tr><td><code id="nested_data_plot_+3A_hline_args">hline_args</code></td>
<td>
<p>arguments to pass to <a href="ggplot2.html#topic+geom_segment">ggplot2::geom_segment</a> when plotting
horizontal lines connecting levels in groups</p>
</td></tr>
<tr><td><code id="nested_data_plot_+3A_label_args">label_args</code></td>
<td>
<p>arguments to pass to <a href="ggplot2.html#topic+geom_text">ggplot2::geom_label</a> when plotting
labels</p>
</td></tr>
<tr><td><code id="nested_data_plot_+3A_connector_args">connector_args</code></td>
<td>
<p>arguments to pass to <a href="ggplot2.html#topic+geom_point">ggplot2::geom_point</a> when
plotting the connection between the vertical lines
and the horizontal lines connecting levels in groups</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Extra options can be included to control aesthetic options. The following
options are supported. Any (or all) can be set to a single variable
in the data set.
</p>

<ul>
<li> <p><code>color</code>: Controls the color of the data points.
</p>
</li>
<li> <p><code>fill</code>: Controls the fill color of the labels. When a particular label
is associated with data points with more than one level of the supplied
variable, the fill is omitted.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)
carbon.fabric.2 %&gt;%
  filter(test == "WT" &amp; condition == "RTD") %&gt;%
  nested_data_plot(strength,
                   groups = c(batch, panel))

# Labels can be filled too
carbon.fabric.2 %&gt;%
  filter(test == "WT" &amp; condition == "RTD") %&gt;%
  nested_data_plot(strength,
                   groups = c(batch, panel),
                   fill = batch)

</code></pre>

<hr>
<h2 id='nonpara_binomial_rank'>Rank for distribution-free tolerance bound</h2><span id='topic+nonpara_binomial_rank'></span>

<h3>Description</h3>

<p>Calculates the rank order for finding distribution-free tolerance
bounds for large samples. This function should only be used for
computing B-Basis for samples larger than 28 or A-Basis for samples
larger than 298. This function is used by
<code><a href="#topic+basis_nonpara_large_sample">basis_nonpara_large_sample()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nonpara_binomial_rank(n, p, conf)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nonpara_binomial_rank_+3A_n">n</code></td>
<td>
<p>the sample size</p>
</td></tr>
<tr><td><code id="nonpara_binomial_rank_+3A_p">p</code></td>
<td>
<p>the desired content for the tolerance bound</p>
</td></tr>
<tr><td><code id="nonpara_binomial_rank_+3A_conf">conf</code></td>
<td>
<p>the confidence level for the desired tolerance bound</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function uses the sum of binomial terms to determine the rank
of the ordered statistic that corresponds with the desired tolerance
limit. This approach does not assume any particular distribution. This
approach is described by Guenther (1969) and by CMH-17-1G.
</p>
<p>The results of this function have been verified against the tables in
CMH-17-1G and agreement was found for all sample sizes published in
CMH-17-1G for both A- and B-Basis, as well as the sample sizes
<code>n+1</code> and <code>n-1</code>, where
<code>n</code> is the sample size published in CMH-17-1G.
</p>
<p>The tables in CMH-17-1G purportedly list the smallest sample sizes
for which a particular rank can be used. That is, for a sample size
one less than the <code>n</code> published in the table, the next lowest rank
would be used. In some cases, the results of this function disagree by a
rank of one for sample sizes one less than the <code>n</code> published in the
table. This indicates a disagreement in that sample size at which
the rank should change. This is likely due to numerical
differences in this function and the procedure used to generate the tables.
However, the disagreement is limited to sample 6500 for A-Basis; no
discrepancies have been identified for B-Basis. Since these sample sizes are
uncommon for composite materials
testing, and the difference between subsequent order statistics will be
very small for samples this large, this difference will have no practical
effect on computed tolerance bounds.
</p>


<h3>Value</h3>

<p>The rank corresponding with the desired tolerance bound
</p>


<h3>References</h3>

<p>W. Guenther, “Determination of Sample Size for Distribution-Free
Tolerance Limits,” Jan. 1969.
Available online: <a href="https://www.duo.uio.no/handle/10852/48686">https://www.duo.uio.no/handle/10852/48686</a>
</p>
<p>“Composite Materials Handbook, Volume 1. Polymer Matrix Composites
Guideline for Characterization of Structural Materials,” SAE International,
CMH-17-1G, Mar. 2012.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+basis_nonpara_large_sample">basis_nonpara_large_sample()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>nonpara_binomial_rank(n = 1693, p = 0.99, conf = 0.95)
## [1] 11

# The above example indicates that for a sample of 1693 observations,
# the A-Basis is best approximated as the 11th ordered observation.
# In the example below, the same ordered observation would also be used
# for a sample of size 1702.

nonpara_binomial_rank(n = 1702, p = 0.99, conf = 0.95)
## [1] 11

</code></pre>

<hr>
<h2 id='normalize_group_mean'>Normalize values to group means</h2><span id='topic+normalize_group_mean'></span>

<h3>Description</h3>

<p>This function computes the mean of each group, then divides each
observation by its corresponding group mean. This is commonly done
when pooling data across environments.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>normalize_group_mean(x, group)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="normalize_group_mean_+3A_x">x</code></td>
<td>
<p>the variable containing the data to normalized</p>
</td></tr>
<tr><td><code id="normalize_group_mean_+3A_group">group</code></td>
<td>
<p>the variable containing the groups</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes the mean for each group, then divides each value by the mean for
the corresponding group.
</p>


<h3>Value</h3>

<p>Returns a vector of normalized values
</p>


<h3>References</h3>

<p>“Composite Materials Handbook, Volume 1. Polymer Matrix Composites
Guideline for Characterization of Structural Materials,” SAE International,
CMH-17-1G, Mar. 2012.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)
carbon.fabric.2 %&gt;%
filter(test == "WT") %&gt;%
  select(condition, strength) %&gt;%
  mutate(condition_norm = normalize_group_mean(strength, condition)) %&gt;%
  head(10)

##    condition strength condition_norm
## 1        CTD  142.817      1.0542187
## 2        CTD  135.901      1.0031675
## 3        CTD  132.511      0.9781438
## 4        CTD  135.586      1.0008423
## 5        CTD  125.145      0.9237709
## 6        CTD  135.203      0.9980151
## 7        CTD  128.547      0.9488832
## 8        CTD  127.709      0.9426974
## 9        CTD  127.074      0.9380101
## 10       CTD  126.879      0.9365706

</code></pre>

<hr>
<h2 id='normalize_ply_thickness'>Normalizes strength values to ply thickness</h2><span id='topic+normalize_ply_thickness'></span>

<h3>Description</h3>

<p>This function takes a vector of strength values and a
vector of measured thicknesses, and a nominal thickness
and returns the normalized strength.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>normalize_ply_thickness(strength, measured_thk, nom_thk)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="normalize_ply_thickness_+3A_strength">strength</code></td>
<td>
<p>the strength to be normalized. Either a vector or a numeric</p>
</td></tr>
<tr><td><code id="normalize_ply_thickness_+3A_measured_thk">measured_thk</code></td>
<td>
<p>the measured thickness of the samples. Must be the same
length as strength</p>
</td></tr>
<tr><td><code id="normalize_ply_thickness_+3A_nom_thk">nom_thk</code></td>
<td>
<p>the nominal thickness. Must be a single numeric value.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It is often necessary to normalize strength values so that variation in
specimen thickness does not unnecessarily increase variation in strength.
See CMH-17-1G, or other references, for information about the cases where
normalization is appropriate.
</p>
<p>Either cured ply thickness or laminate thickness may be used for
<code>measured_thk</code> and <code>nom_thk</code>, as long as the same decision
made for both values.
</p>
<p>The formula applied is:
</p>
<p style="text-align: center;"><code class="reqn">normalized\,value = test\,value \frac{t_{measured}}{t_{nominal}}</code>
</p>

<p>If you need to normalize based on fiber volume fraction (or another method),
you will first need to calculate the nominal cured ply thickness (or laminate
thickness). Those calculations are outside the scope of this documentation.
</p>


<h3>Value</h3>

<p>The normalized strength values
</p>


<h3>References</h3>

<p>“Composite Materials Handbook, Volume 1. Polymer Matrix Composites
Guideline for Characterization of Structural Materials,” SAE International,
CMH-17-1G, Mar. 2012.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)

carbon.fabric.2 %&gt;%
select(thickness, strength) %&gt;%
  mutate(normalized_strength = normalize_ply_thickness(strength,
                                                       thickness,
                                                       0.105)) %&gt;%
  head(10)

##    thickness strength normalized_strength
## 1      0.112  142.817            152.3381
## 2      0.113  135.901            146.2554
## 3      0.113  132.511            142.6071
## 4      0.112  135.586            144.6251
## 5      0.113  125.145            134.6799
## 6      0.113  135.203            145.5042
## 7      0.113  128.547            138.3411
## 8      0.113  127.709            137.4392
## 9      0.113  127.074            136.7558
## 10     0.114  126.879            137.7543


</code></pre>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic+augment'></span><span id='topic+tidy'></span><span id='topic+glance'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>generics</dt><dd><p><code><a href="generics.html#topic+augment">augment</a></code>, <code><a href="generics.html#topic+glance">glance</a></code>, <code><a href="generics.html#topic+tidy">tidy</a></code></p>
</dd>
</dl>


<h3>See Also</h3>

<p><code><a href="generics.html#topic+augment">generics::augment()</a></code>
</p>
<p><code><a href="generics.html#topic+tidy">generics::tidy()</a></code>
</p>
<p><code><a href="generics.html#topic+glance">generics::glance()</a></code>
</p>

<hr>
<h2 id='stat_esf'>Empirical Survival Function</h2><span id='topic+stat_esf'></span>

<h3>Description</h3>

<p>The empirical survival function (ESF) provides a visualization of a
distribution. This is closely related to the empirical cumulative
distribution function (ECDF). The empirical survival function is
simply ESF = 1 - ECDF.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stat_esf(
  mapping = NULL,
  data = NULL,
  geom = "point",
  position = "identity",
  show.legend = NA,
  inherit.aes = TRUE,
  n = NULL,
  pad = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stat_esf_+3A_mapping">mapping</code></td>
<td>
<p>Set of aesthetic mappings created by <code>aes()</code>.</p>
</td></tr>
<tr><td><code id="stat_esf_+3A_data">data</code></td>
<td>
<p>The data to be displayed in this layer. This has the
same usage as a <code>ggplot2</code> <code>stat</code> function.</p>
</td></tr>
<tr><td><code id="stat_esf_+3A_geom">geom</code></td>
<td>
<p>The geometric object to use to display the data.</p>
</td></tr>
<tr><td><code id="stat_esf_+3A_position">position</code></td>
<td>
<p>Position argument</p>
</td></tr>
<tr><td><code id="stat_esf_+3A_show.legend">show.legend</code></td>
<td>
<p>Should this layer be included in the legends?</p>
</td></tr>
<tr><td><code id="stat_esf_+3A_inherit.aes">inherit.aes</code></td>
<td>
<p>If <code>FALSE</code>, overrides the default aesthetic,
rather than combining with them.</p>
</td></tr>
<tr><td><code id="stat_esf_+3A_n">n</code></td>
<td>
<p>If <code>NULL</code>, do not interpolated. Otherwise, the
number of points to interpolate.</p>
</td></tr>
<tr><td><code id="stat_esf_+3A_pad">pad</code></td>
<td>
<p>If <code>TRUE</code>, pad the ESF with additional points
<code style="white-space: pre;">&#8288;(-Inf, 0)&#8288;</code> and <code style="white-space: pre;">&#8288;(0, Inf)&#8288;</code>.</p>
</td></tr>
<tr><td><code id="stat_esf_+3A_...">...</code></td>
<td>
<p>Other arguments to pass on to <code>layer</code>.</p>
</td></tr>
</table>

<hr>
<h2 id='stat_normal_surv_func'>Normal Survival Function</h2><span id='topic+stat_normal_surv_func'></span>

<h3>Description</h3>

<p>The Normal survival function provides a visualization of a
distribution. A normal curve is fit based on the mean and standard
deviation of the data, and the survival function of this normal
curve is plotted. The survival function is simply one minus the
CDF.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stat_normal_surv_func(
  mapping = NULL,
  data = NULL,
  geom = "smooth",
  position = "identity",
  show.legend = NA,
  inherit.aes = TRUE,
  n = 100,
  pad = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stat_normal_surv_func_+3A_mapping">mapping</code></td>
<td>
<p>Set of aesthetic mappings created by <code>aes()</code>.</p>
</td></tr>
<tr><td><code id="stat_normal_surv_func_+3A_data">data</code></td>
<td>
<p>The data to be displayed in this layer. This has the
same usage as a <code>ggplot2</code> <code>stat</code> function.</p>
</td></tr>
<tr><td><code id="stat_normal_surv_func_+3A_geom">geom</code></td>
<td>
<p>The geometric object to use to display the data.</p>
</td></tr>
<tr><td><code id="stat_normal_surv_func_+3A_position">position</code></td>
<td>
<p>Position argument</p>
</td></tr>
<tr><td><code id="stat_normal_surv_func_+3A_show.legend">show.legend</code></td>
<td>
<p>Should this layer be included in the legends?</p>
</td></tr>
<tr><td><code id="stat_normal_surv_func_+3A_inherit.aes">inherit.aes</code></td>
<td>
<p>If <code>FALSE</code>, overrides the default aesthetic,
rather than combining with them.</p>
</td></tr>
<tr><td><code id="stat_normal_surv_func_+3A_n">n</code></td>
<td>
<p>If <code>NULL</code>, do not interpolated. Otherwise, the
number of points to interpolate.</p>
</td></tr>
<tr><td><code id="stat_normal_surv_func_+3A_pad">pad</code></td>
<td>
<p>If <code>TRUE</code>, pad the ESF with additional points
<code style="white-space: pre;">&#8288;(-Inf, 0)&#8288;</code> and <code style="white-space: pre;">&#8288;(0, Inf)&#8288;</code>.</p>
</td></tr>
<tr><td><code id="stat_normal_surv_func_+3A_...">...</code></td>
<td>
<p>Other arguments to pass on to <code>layer</code>.</p>
</td></tr>
</table>

<hr>
<h2 id='transform_mod_cv'>Transforms data according to the modified CV rule</h2><span id='topic+transform_mod_cv'></span><span id='topic+transform_mod_cv_ad'></span>

<h3>Description</h3>

<p>Transforms data according to the modified coefficient of variation (CV)
rule. This is used to add additional variance to datasets with
unexpectedly low variance, which is sometimes encountered during
testing of new materials over short periods of time.
</p>
<p>Two versions of this transformation are implemented. The first version,
<code>transform_mod_cv()</code>, transforms the data in a single group (with
no other structure) according to the modified CV rules.
</p>
<p>The second
version, <code>transform_mod_cv_ad()</code>, transforms data that is structured
according to both condition and batch, as is commonly done for
the Anderson&ndash;Darling k-Sample and Anderson-Darling tests when pooling
across environments.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>transform_mod_cv_ad(x, condition, batch)

transform_mod_cv(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="transform_mod_cv_+3A_x">x</code></td>
<td>
<p>a vector of data to transform</p>
</td></tr>
<tr><td><code id="transform_mod_cv_+3A_condition">condition</code></td>
<td>
<p>a vector indicating the condition to which each
observation belongs</p>
</td></tr>
<tr><td><code id="transform_mod_cv_+3A_batch">batch</code></td>
<td>
<p>a vector indicating the batch to which each observation
belongs</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The modified CV transformation takes the general form:
</p>
<p style="text-align: center;"><code class="reqn">\frac{S_i^*}{S_i} (x_{ij} - \bar{x_i}) + \bar{x_i}</code>
</p>

<p>Where <code class="reqn">S_i^*</code> is the modified standard deviation
(mod CV times mean) for
the <code class="reqn">ith</code> group; <code class="reqn">S_i</code> is the standard deviation
for the <code class="reqn">ith</code> group, <code class="reqn">\bar{x_i}</code> is
the group mean and <code class="reqn">x_{ij}</code> is the observation.
</p>
<p><code>transform_mod_cv()</code> takes a vector
containing the observations and transforms the data.
The equation above is used, and all observations
are considered to be from the same group.
</p>
<p><code>transform_mod_cv_ad()</code> takes a vector containing the observations
plus a vector containing the corresponding conditions and a vector
containing the batches. This function first calculates the modified
CV value from the data from each condition (independently). Then,
within each condition, the transformation
above is applied to produce the transformed data <code class="reqn">x'</code>.
This transformed data is further transformed using the following
equation.
</p>
<p style="text-align: center;"><code class="reqn">x_{ij}'' = C (x'_{ij} - \bar{x_i}) + \bar{x_i}</code>
</p>

<p>Where:
</p>
<p style="text-align: center;"><code class="reqn">C = \sqrt{\frac{SSE^*}{SSE'}}</code>
</p>

<p style="text-align: center;"><code class="reqn">SSE^* = (n-1) (CV^* \bar{x})^2 - \sum(n_i(\bar{x_i}-\bar{x})^2)</code>
</p>

<p style="text-align: center;"><code class="reqn">SSE' = \sum(x'_{ij} - \bar{x_i})^2</code>
</p>



<h3>Value</h3>

<p>A vector of transformed data
</p>


<h3>See Also</h3>

<p><code><a href="#topic+calc_cv_star">calc_cv_star()</a></code>
</p>
<p><code><a href="#topic+cv">cv()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Transform data according to the modified CV transformation
# and report the original and modified CV for each condition

library(dplyr)
carbon.fabric %&gt;%
filter(test == "FT") %&gt;%
  group_by(condition) %&gt;%
  mutate(trans_strength = transform_mod_cv(strength)) %&gt;%
  head(10)

## # A tibble: 10 x 6
## # Groups:   condition [1]
##    id         test  condition batch strength trans_strength
##    &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;    &lt;dbl&gt;          &lt;dbl&gt;
##  1 FT-RTD-1-1 FT    RTD           1     126.           126.
##  2 FT-RTD-1-2 FT    RTD           1     139.           141.
##  3 FT-RTD-1-3 FT    RTD           1     116.           115.
##  4 FT-RTD-1-4 FT    RTD           1     132.           133.
##  5 FT-RTD-1-5 FT    RTD           1     129.           129.
##  6 FT-RTD-1-6 FT    RTD           1     130.           130.
##  7 FT-RTD-2-1 FT    RTD           2     131.           131.
##  8 FT-RTD-2-2 FT    RTD           2     124.           124.
##  9 FT-RTD-2-3 FT    RTD           2     125.           125.
## 10 FT-RTD-2-4 FT    RTD           2     120.           119.

# The CV of this transformed data can be computed to verify
# that the resulting CV follows the rules for modified CV

carbon.fabric %&gt;%
  filter(test == "FT") %&gt;%
  group_by(condition) %&gt;%
  mutate(trans_strength = transform_mod_cv(strength)) %&gt;%
  summarize(cv = sd(strength) / mean(strength),
            mod_cv = sd(trans_strength) / mean(trans_strength))

## # A tibble: 3 x 3
##   condition     cv mod_cv
##   &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;
## 1 CTD       0.0423 0.0612
## 2 ETW       0.0369 0.0600
## 3 RTD       0.0621 0.0711

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
