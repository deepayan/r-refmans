<!DOCTYPE html><html><head><title>Help for package interpret</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {interpret}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ebm_classify'><p>Build an EBM classification model</p></a></li>
<li><a href='#ebm_predict_proba'><p>ebm_predict_proba</p></a></li>
<li><a href='#ebm_show'><p>ebm_show</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Fit Interpretable Machine Learning Models</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.33</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-01-26</td>
</tr>
<tr>
<td>Description:</td>
<td>Package for training interpretable machine learning models. Historically, the most interpretable machine learning models were not very accurate, and the most accurate models were not very interpretable. Microsoft Research has developed an algorithm called the Explainable Boosting Machine (EBM) which has both high accuracy and interpretable characteristics. EBM uses machine learning techniques like bagging and boosting to breathe new life into traditional GAMs (Generalized Additive Models). This makes them as accurate as random forests and gradient boosted trees, and also enhances their intelligibility and editability. Details on the EBM algorithm can be found in the paper by Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad (2015, &lt;<a href="https://doi.org/10.1145%2F2783258.2788613">doi:10.1145/2783258.2788613</a>&gt;).</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/interpretml/interpret">https://github.com/interpretml/interpret</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/interpretml/interpret/issues">https://github.com/interpretml/interpret/issues</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0.0)</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>C++17</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-01-27 22:52:07 UTC; admins</td>
</tr>
<tr>
<td>Author:</td>
<td>Samuel Jenkins [aut],
  Harsha Nori [aut],
  Paul Koch [aut],
  Rich Caruana [aut, cre],
  Microsoft Corporation [cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Rich Caruana &lt;interpretml@outlook.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-01-27 23:20:08 UTC</td>
</tr>
</table>
<hr>
<h2 id='ebm_classify'>Build an EBM classification model</h2><span id='topic+ebm_classify'></span>

<h3>Description</h3>

<p>Builds a classification model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ebm_classify(
  X, 
  y, 
  max_bins = 255,
  outer_bags = 16, 
  inner_bags = 0,
  learning_rate = 0.01, 
  validation_size = 0.15, 
  early_stopping_rounds = 50, 
  early_stopping_tolerance = 1e-4,
  max_rounds = 5000, 
  min_samples_leaf = 2,
  max_leaves = 3,
  random_state = 42
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ebm_classify_+3A_x">X</code></td>
<td>
<p>features</p>
</td></tr>
<tr><td><code id="ebm_classify_+3A_y">y</code></td>
<td>
<p>targets</p>
</td></tr>
<tr><td><code id="ebm_classify_+3A_max_bins">max_bins</code></td>
<td>
<p>number of bins to create</p>
</td></tr>
<tr><td><code id="ebm_classify_+3A_outer_bags">outer_bags</code></td>
<td>
<p>number of outer bags</p>
</td></tr>
<tr><td><code id="ebm_classify_+3A_inner_bags">inner_bags</code></td>
<td>
<p>number of inner bags</p>
</td></tr>
<tr><td><code id="ebm_classify_+3A_learning_rate">learning_rate</code></td>
<td>
<p>learning rate</p>
</td></tr>
<tr><td><code id="ebm_classify_+3A_validation_size">validation_size</code></td>
<td>
<p>amount of data to use for validation</p>
</td></tr>
<tr><td><code id="ebm_classify_+3A_early_stopping_rounds">early_stopping_rounds</code></td>
<td>
<p>how many rounds without improvement before we quit</p>
</td></tr>
<tr><td><code id="ebm_classify_+3A_early_stopping_tolerance">early_stopping_tolerance</code></td>
<td>
<p>how much does the round need to improve by to be considered as an advancement</p>
</td></tr>
<tr><td><code id="ebm_classify_+3A_max_rounds">max_rounds</code></td>
<td>
<p>number of boosting rounds</p>
</td></tr>
<tr><td><code id="ebm_classify_+3A_min_samples_leaf">min_samples_leaf</code></td>
<td>
<p>number of samples required for a split</p>
</td></tr>
<tr><td><code id="ebm_classify_+3A_max_leaves">max_leaves</code></td>
<td>
<p>how many leaves allowed</p>
</td></tr>
<tr><td><code id="ebm_classify_+3A_random_state">random_state</code></td>
<td>
<p>random seed</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an EBM model
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(mtcars)
  X &lt;- subset(mtcars, select = -c(vs))
  y &lt;- mtcars$vs

  set.seed(42)
  data_sample &lt;- sample(length(y), length(y) * 0.8)

  X_train &lt;- X[data_sample, ]
  y_train &lt;- y[data_sample]
  X_test &lt;- X[-data_sample, ]
  y_test &lt;- y[-data_sample]

  ebm &lt;- ebm_classify(X_train, y_train)
</code></pre>

<hr>
<h2 id='ebm_predict_proba'>ebm_predict_proba</h2><span id='topic+ebm_predict_proba'></span>

<h3>Description</h3>

<p>Predicts probabilities using an EBM model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ebm_predict_proba(
  model, 
  X
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ebm_predict_proba_+3A_model">model</code></td>
<td>
<p>the model</p>
</td></tr>
<tr><td><code id="ebm_predict_proba_+3A_x">X</code></td>
<td>
<p>features</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns the probabilities predicted
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(mtcars)
  X &lt;- subset(mtcars, select = -c(vs))
  y &lt;- mtcars$vs

  set.seed(42)
  data_sample &lt;- sample(length(y), length(y) * 0.8)

  X_train &lt;- X[data_sample, ]
  y_train &lt;- y[data_sample]
  X_test &lt;- X[-data_sample, ]
  y_test &lt;- y[-data_sample]

  ebm &lt;- ebm_classify(X_train, y_train)
  proba_test &lt;- ebm_predict_proba(ebm, X_test)
</code></pre>

<hr>
<h2 id='ebm_show'>ebm_show</h2><span id='topic+ebm_show'></span>

<h3>Description</h3>

<p>Shows the GAM plot for a single feature
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ebm_show(
  model, 
  name
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ebm_show_+3A_model">model</code></td>
<td>
<p>the model</p>
</td></tr>
<tr><td><code id="ebm_show_+3A_name">name</code></td>
<td>
<p>the name of the feature to plot</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(mtcars)
  X &lt;- subset(mtcars, select = -c(vs))
  y &lt;- mtcars$vs

  set.seed(42)
  data_sample &lt;- sample(length(y), length(y) * 0.8)

  X_train &lt;- X[data_sample, ]
  y_train &lt;- y[data_sample]
  X_test &lt;- X[-data_sample, ]
  y_test &lt;- y[-data_sample]

  ebm &lt;- ebm_classify(X_train, y_train)
  ebm_show(ebm, "mpg")
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
