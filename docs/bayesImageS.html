<!DOCTYPE html><html><head><title>Help for package bayesImageS</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {bayesImageS}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bayesImageS'><p>Package bayesImageS</p></a></li>
<li><a href='#exactPotts'><p>Calculate the distribution of the Potts model using a brute force algorithm.</p></a></li>
<li><a href='#getBlocks'><p>Get Blocks of a Graph</p></a></li>
<li><a href='#getEdges'><p>Get Edges of a Graph</p></a></li>
<li><a href='#getNeighbors'><p>Get Neighbours of All Vertices of a Graph</p></a></li>
<li><a href='#gibbsGMM'><p>Fit a mixture of Gaussians to the observed data.</p></a></li>
<li><a href='#gibbsNorm'><p>Fit a univariate normal (Gaussian) distribution to the observed data.</p></a></li>
<li><a href='#gibbsPotts'><p>Fit a hidden Potts model to the observed data, using a fixed value of beta.</p></a></li>
<li><a href='#initSedki'><p>Initialize the ABC algorithm using the method of Sedki et al. (2013)</p></a></li>
<li><a href='#mcmcPotts'><p>Fit the hidden Potts model using a Markov chain Monte Carlo algorithm.</p></a></li>
<li><a href='#mcmcPottsNoData'><p>Simulate pixel labels using chequerboard Gibbs sampling.</p></a></li>
<li><a href='#res'><p>Simulation from the Potts model using single-site Gibbs updates.</p></a></li>
<li><a href='#res2'><p>Simulation from the Potts model using single-site Gibbs updates.</p></a></li>
<li><a href='#res3'><p>Simulation from the Potts model using single-site Gibbs updates.</p></a></li>
<li><a href='#res4'><p>Simulation from the Potts model using single-site Gibbs updates.</p></a></li>
<li><a href='#res5'><p>Simulation from the Potts model using single-site Gibbs updates.</p></a></li>
<li><a href='#smcPotts'><p>Fit the hidden Potts model using approximate Bayesian computation with sequential Monte Carlo (ABC-SMC).</p></a></li>
<li><a href='#sufficientStat'><p>Calculate the sufficient statistic of the Potts model for the given labels.</p></a></li>
<li><a href='#swNoData'><p>Simulate pixel labels using the Swendsen-Wang algorithm.</p></a></li>
<li><a href='#synth'><p>Simulation from the Potts model using Swendsen-Wang.</p></a></li>
<li><a href='#testResample'><p>Test the residual resampling algorithm.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Bayesian Methods for Image Segmentation using a Potts Model</td>
</tr>
<tr>
<td>Version:</td>
<td>0.6-1</td>
</tr>
<tr>
<td>Date:</td>
<td>2021-04-10</td>
</tr>
<tr>
<td>Description:</td>
<td>Various algorithms for segmentation of 2D and 3D images, such
    as computed tomography and satellite remote sensing. This package implements
    Bayesian image analysis using the hidden Potts model with external field
    prior of Moores et al. (2015) &lt;<a href="https://doi.org/10.1016%2Fj.csda.2014.12.001">doi:10.1016/j.csda.2014.12.001</a>&gt;.
    Latent labels are sampled using chequerboard updating or Swendsen-Wang.
    Algorithms for the smoothing parameter include pseudolikelihood, path sampling,
    the exchange algorithm, approximate Bayesian computation (ABC-MCMC and ABC-SMC),
    and the parametric functional approximate Bayesian (PFAB) algorithm. Refer to
    &lt;<a href="https://doi.org/10.1007%2F978-3-030-42553-1_6">doi:10.1007/978-3-030-42553-1_6</a>&gt; for an overview and also to &lt;<a href="https://doi.org/10.1007%2Fs11222-014-9525-6">doi:10.1007/s11222-014-9525-6</a>&gt;
    and &lt;<a href="https://doi.org/10.1214%2F18-BA1130">doi:10.1214/18-BA1130</a>&gt; for further details of specific algorithms.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> | file LICENSE [expanded from: GPL (&ge; 2) | file LICENSE]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://bitbucket.org/Azeari/bayesimages">https://bitbucket.org/Azeari/bayesimages</a>,
<a href="https://mooresm.github.io/bayesImageS/">https://mooresm.github.io/bayesImageS/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://bitbucket.org/Azeari/bayesimages/issues">https://bitbucket.org/Azeari/bayesimages/issues</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.10.6)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>Suggests:</td>
<td>mcmcse, coda, PottsUtils, rstan, knitr, rmarkdown, lattice</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-04-11 05:59:53 UTC; matt</td>
</tr>
<tr>
<td>Author:</td>
<td>Matt Moores <a href="https://orcid.org/0000-0003-4531-3572"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre],
  Dai Feng [ctb],
  Kerrie Mengersen <a href="https://orcid.org/0000-0001-8625-9168"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, ths]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Matt Moores &lt;mmoores@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-04-11 15:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='bayesImageS'>Package bayesImageS</h2><span id='topic+bayesImageS'></span>

<h3>Description</h3>

<p>Bayesian methods for segmentation of 2D and 3D images, such as computed
tomography and satellite remote sensing. This package implements image
analysis using the hidden Potts model with external field prior. Latent labels
are sampled using chequerboard updating or Swendsen-Wang. Algorithms for the 
smoothing parameter include pseudolikelihood, path sampling, the exchange
algorithm, and approximate Bayesian computation (ABC-MCMC and ABC-SMC).
</p>


<h3>Author(s)</h3>

<p>M. T. Moores and K. Mengersen
with additional code contributed by D. Feng
</p>
<p>Maintainer: Matt Moores &lt;mmoores@uow.edu.au&gt;
</p>


<h3>References</h3>

<p>Moores, M. T.; Nicholls, G. K.; Pettitt, A. N. &amp; Mengersen, K. (2020) &quot;Scalable Bayesian inference for the inverse temperature of a hidden Potts model&quot; <em>Bayesian Analysis</em> <b>15</b>(1), 1&ndash;17, DOI: doi: <a href="https://doi.org/10.1214/18-BA1130">10.1214/18-BA1130</a>
</p>
<p>Moores, M. T.; Drovandi, C. C.; Mengersen, K. &amp; Robert, C. P. (2015) &quot;Pre-processing for approximate Bayesian computation in image analysis&quot; <em>Statistics &amp; Computing</em> <b>25</b>(1), 23&ndash;33, DOI: doi: <a href="https://doi.org/10.1007/s11222-014-9525-6">10.1007/s11222-014-9525-6</a>
</p>
<p>Moores, M. T.; Hargrave, C. E.; Deegan, T.; Poulsen, M.; Harden, F. &amp; Mengersen, K. (2015) &quot;An external field prior for the hidden Potts model, with application to cone-beam computed tomography&quot; <em>Computational Statistics &amp; Data Analysis</em> <b>86</b>, 27&ndash;41, DOI: doi: <a href="https://doi.org/10.1016/j.csda.2014.12.001">10.1016/j.csda.2014.12.001</a>
</p>
<p>Feng, D. (2008) &quot;Bayesian Hidden Markov Normal Mixture Models with Application to MRI Tissue Classification&quot; <em>Ph. D. Dissertation, The University of Iowa</em>
</p>


<h3>See Also</h3>

<p><code>vignette(package="bayesImageS")</code>
</p>

<hr>
<h2 id='exactPotts'>Calculate the distribution of the Potts model using a brute force algorithm.</h2><span id='topic+exactPotts'></span>

<h3>Description</h3>

<p><b>Warning</b>: this algorithm is O<code class="reqn">(k^n)</code> and therefore will not scale for
<code class="reqn">k^n &gt; 2^{31} - 1</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>exactPotts(neighbors, blocks, k, beta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="exactPotts_+3A_neighbors">neighbors</code></td>
<td>
<p>A matrix of all neighbours in the lattice, one row per pixel.</p>
</td></tr>
<tr><td><code id="exactPotts_+3A_blocks">blocks</code></td>
<td>
<p>A list of pixel indices, dividing the lattice into independent blocks.</p>
</td></tr>
<tr><td><code id="exactPotts_+3A_k">k</code></td>
<td>
<p>The number of unique labels.</p>
</td></tr>
<tr><td><code id="exactPotts_+3A_beta">beta</code></td>
<td>
<p>The inverse temperature parameter of the Potts model.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following elements:
</p>

<dl>
<dt><code>expectation</code></dt><dd><p>The exact mean of the sufficient statistic.</p>
</dd>
<dt><code>variance</code></dt><dd><p>The exact variance of the sufficient statistic.</p>
</dd>
<dt><code>exp_PL</code></dt><dd><p>Pseudo-likelihood (PL) approximation of the expectation of S(z).</p>
</dd>
<dt><code>var_PL</code></dt><dd><p>PL approx. of the variance of the sufficient statistic.</p>
</dd>
</dl>


<hr>
<h2 id='getBlocks'>Get Blocks of a Graph</h2><span id='topic+getBlocks'></span>

<h3>Description</h3>

<p>Obtain blocks of vertices of a 1D, 2D, or 3D graph, in order to use
the conditional independence to speed up the simulation (chequerboard idea).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getBlocks(mask, nblock)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getBlocks_+3A_mask">mask</code></td>
<td>
<p>a vector, matrix, or 3D array specifying vertices of a graph. Vertices of value 1 are within the graph and 0 are not.</p>
</td></tr>
<tr><td><code id="getBlocks_+3A_nblock">nblock</code></td>
<td>
<p>a scalar specifying the number of blocks. For a 2D graph <code>nblock</code> could be either 2 or 4, and for a 3D graph <code>nblock</code> could be either 2 or 8.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The vertices within each block are mutually independent given the vertices in other blocks. Some blocks could be empty.
</p>


<h3>Value</h3>

<p>A list with the number of components equal to <code>nblock</code>. Each component consists of vertices within the same block.
</p>


<h3>References</h3>

<p>Wilkinson, D. J. (2005)
&quot;Parallel Bayesian Computation&quot;
<cite>Handbook of Parallel Computing and Statistics</cite>, pp. 481-512
<em>Marcel Dekker/CRC Press</em>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  #Example 1: split a line into 2 blocks
  getBlocks(mask=c(1,1,1,1,0,0,1,1,0), nblock=2)
  
  #Example 2: split a 4*4 2D graph into 4 blocks in order
  #           to use the chequerboard idea for a neighbourhood structure
  #           corresponding to the second-order Markov random field.
  getBlocks(mask=matrix(1, nrow=4, ncol=4), nblock=4)
  
  #Example 3: split a 3*3*3 3D graph into 8 blocks
  #           in order to use the chequerboard idea for a neighbourhood
  #           structure based on the 18 neighbors definition, where the
  #           neighbors of a vertex comprise its available
  #           adjacencies sharing the same edges or faces.
  mask &lt;- array(1, dim=rep(3,3))
  getBlocks(mask, nblock=8)
</code></pre>

<hr>
<h2 id='getEdges'>Get Edges of a Graph</h2><span id='topic+getEdges'></span>

<h3>Description</h3>

<p>Obtain edges of a 1D, 2D, or 3D graph based on the
neighbourhood structure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getEdges(mask, neiStruc)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getEdges_+3A_mask">mask</code></td>
<td>
<p>a vector, matrix, or 3D array specifying vertices of a graph. Vertices of value 1 are within the graph and 0 are not.</p>
</td></tr>
<tr><td><code id="getEdges_+3A_neistruc">neiStruc</code></td>
<td>
<p>a scalar, vector of four components, or <code class="reqn">3\times4</code>    matrix corresponding to 1D, 2D, or 3D graphs. It specifies the neighbourhood structure. See <code>getNeighbors</code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There could be more than one way to define the same 3D neighbourhood
structure for a graph (see Example 4 for illustration).
</p>


<h3>Value</h3>

<p>A matrix of two columns with one edge per row.  The edges connecting
vertices and their corresponding first neighbours are listed first, and
then those corresponding to the second neighbours, and so on and so
forth.  The order of neighbours is the same as in <code>getNeighbors</code>.
</p>


<h3>References</h3>

<p>Winkler, G. (2003)
&quot;Image Analysis, Random Fields and Markov Chain Monte Carlo Methods: A Mathematical Introduction&quot; (2nd ed.)
<em>Springer-Verlag</em>
</p>
<p>Feng, D. (2008)
&quot;Bayesian Hidden Markov Normal Mixture Models with Application to MRI Tissue Classification&quot;
<em>Ph. D. Dissertation, The University of Iowa</em>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  #Example 1: get all edges of a 1D graph. 
  mask &lt;- c(0,0,rep(1,4),0,1,1,0,0)
  getEdges(mask, neiStruc=2)
  
  #Example 2: get all edges of a 2D graph based on neighbourhood structure
  #           corresponding to the first-order Markov random field.
  mask &lt;- matrix(1 ,nrow=2, ncol=3)
  getEdges(mask, neiStruc=c(2,2,0,0))
  
  #Example 3: get all edges of a 2D graph based on neighbourhood structure
  #           corresponding to the second-order Markov random field.
  mask &lt;- matrix(1 ,nrow=3, ncol=3)
  getEdges(mask, neiStruc=c(2,2,2,2))
  
  #Example 4: get all edges of a 3D graph based on 6 neighbours structure
  #           where the neighbours of a vertex comprise its available
  #           N,S,E,W, upper and lower adjacencies. To achieve it, there
  #           are several ways, including the two below.
  mask &lt;- array(1, dim=rep(3,3))
  n61 &lt;- matrix(c(2,2,0,0,
                  0,2,0,0,
                  0,0,0,0), nrow=3, byrow=TRUE)
  n62 &lt;- matrix(c(2,0,0,0,
                  0,2,0,0,
                  2,0,0,0), nrow=3, byrow=TRUE)
  e1 &lt;- getEdges(mask, neiStruc=n61)
  e2 &lt;- getEdges(mask, neiStruc=n62)
  e1 &lt;- e1[order(e1[,1], e1[,2]),]
  e2 &lt;- e2[order(e2[,1], e2[,2]),]
  all(e1==e2)
  
  #Example 5: get all edges of a 3D graph based on 18 neighbours structure
  #           where the neighbours of a vertex comprise its available
  #           adjacencies sharing the same edges or faces.
  #           To achieve it, there are several ways, including the one below.
  
  n18 &lt;- matrix(c(2,2,2,2,
                  0,2,2,2,
                  0,0,2,2), nrow=3, byrow=TRUE)  
  mask &lt;- array(1, dim=rep(3,3))
  getEdges(mask, neiStruc=n18)
</code></pre>

<hr>
<h2 id='getNeighbors'>Get Neighbours of All Vertices of a Graph</h2><span id='topic+getNeighbors'></span>

<h3>Description</h3>

<p>Obtain neighbours of vertices of a 1D, 2D, or 3D graph.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getNeighbors(mask, neiStruc)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getNeighbors_+3A_mask">mask</code></td>
<td>
<p>a vector, matrix, or 3D array specifying vertices within a graph. Vertices of value 1 are within the graph and 0 are not.</p>
</td></tr>
<tr><td><code id="getNeighbors_+3A_neistruc">neiStruc</code></td>
<td>
<p>a scalar, vector of four components, or <code class="reqn">3\times4</code> matrix
corresponding to 1D, 2D, or 3D graphs. It gives the definition of
neighbours of a graph.
All components of <code>neiStruc</code> should be positive (<code class="reqn">\ge 0</code>)
even numbers.
For 1D graphs, <code>neiStruc</code> gives
the number of neighbours of each vertex.
For 2D graphs, <code>neiStruc</code>[1] specifies
the number of neighbours on vertical direction, <code>neiStruc</code>[2]
horizontal direction, <code>neiStruc</code>[3] north-west (NW) to south-east (SE)
diagonal direction, and <code>neiStruc</code>[4] south-west (SW) to
north-east (NE) diagonal direction. For 3D
graphs, the first row of <code>neiStruc</code> specifies the number of neighbours on
vertical direction, horizontal direction and two diagonal directions from
the 1-2 perspective, the second row the 1-3 perspective, and the
third row the 2-3 perspective. The index to
perspectives is represented with the leftmost subscript of the
array being the smallest.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There could be more than one way to define the same 3D neighbourhood
structure for a graph (see Example 3 for illustration).
</p>


<h3>Value</h3>

<p>A matrix with each row giving the neighbours of a vertex.
The number of the rows is equal to the number of 
vertices within the graph and the number or columns
is the number of neighbours of each vertex.
</p>
<p>For a 1D graph, if each vertex has two neighbours,
The first column are the neighbours on the left-hand side of
corresponding vertices and the second column the right-hand side.
For the vertices on boundaries, missing neighbours are represented by
the number of vertices within a graph plus 1.
When <code>neiStruc</code> is bigger than 2, The first two columns
are the same as when <code>neiStruc</code> is equal to 2; the third column
are the neighbours on the left-hand side of the vertices on the first column;
the forth column are the neighbours on the right-hand side of the vertices
on the second column, and so on and so forth. And again for the
vertices on boundaries, their missing neighbours are represented by
the number of vertices within a graph plus 1.
</p>
<p>For a 2D graph, the index to vertices is column-wised. For each
vertex, the order of neighbours are as follows. First are those
on the vertical direction, second the horizontal
direction, third the NW to SE diagonal
direction, and forth the SW to NE diagonal
direction. For each direction, the neighbours of every vertex
are arranged in the same way as in a 1D graph.
</p>
<p>For a 3D graph, the index to vertices is that
the leftmost subscript of the array moves the fastest.  For each
vertex, the neighbours from the 1-2 perspective
appear first and then the 1-3 perspective and finally
the 2-3 perspective. For each perspective, the neighbours are arranged
in the same way as in a 2D graph.
</p>


<h3>References</h3>

<p>Winkler, G. (2003)
&quot;Image Analysis, Random Fields and Markov Chain Monte Carlo Methods: A Mathematical Introduction&quot; (2nd ed.)
<em>Springer-Verlag</em>
</p>
<p>Feng, D. (2008)
&quot;Bayesian Hidden Markov Normal Mixture Models with Application to MRI Tissue Classification&quot;
<em>Ph. D. Dissertation, The University of Iowa</em>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  #Example 1: get all neighbours of a 1D graph.
  mask &lt;- c(0,0,rep(1,4),0,1,1,0,0,1,1,1)
  getNeighbors(mask, neiStruc=2)
  
  #Example 2: get all neighbours of a 2D graph based on neighbourhood structure
  #           corresponding to the second-order Markov random field.
  mask &lt;- matrix(1, nrow=2, ncol=3)
  getNeighbors(mask, neiStruc=c(2,2,2,2))
  
  #Example 3: get all neighbours of a 3D graph based on 6 neighbours structure
  #           where the neighbours of a vertex comprise its available
  #           N,S,E,W, upper and lower adjacencies. To achieve it, there
  #           are several ways, including the two below.
  mask &lt;- array(1, dim=rep(3,3))
  n61 &lt;- matrix(c(2,2,0,0,
                  0,2,0,0,
                  0,0,0,0), nrow=3, byrow=TRUE)
  n62 &lt;- matrix(c(2,0,0,0,
                  0,2,0,0,
                  2,0,0,0), nrow=3, byrow=TRUE)
  n1 &lt;- getNeighbors(mask, neiStruc=n61)
  n2 &lt;- getNeighbors(mask, neiStruc=n62)
  n1 &lt;- apply(n1, 1, sort)
  n2 &lt;- apply(n2, 1, sort)
  all(n1==n2)
  
  #Example 4: get all neighbours of a 3D graph based on 18 neighbours structure
  #           where the neighbours of a vertex comprise its available
  #           adjacencies sharing the same edges or faces.
  #           To achieve it, there are several ways, including the one below.
  
  n18 &lt;- matrix(c(2,2,2,2,
                  0,2,2,2,
                  0,0,2,2), nrow=3, byrow=TRUE)  
  mask &lt;- array(1, dim=rep(3,3))
  getNeighbors(mask, neiStruc=n18)
</code></pre>

<hr>
<h2 id='gibbsGMM'>Fit a mixture of Gaussians to the observed data.</h2><span id='topic+gibbsGMM'></span>

<h3>Description</h3>

<p>Fit a mixture of Gaussians to the observed data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gibbsGMM(y, niter = 1000, nburn = 500, priors = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gibbsGMM_+3A_y">y</code></td>
<td>
<p>A vector of observed pixel data.</p>
</td></tr>
<tr><td><code id="gibbsGMM_+3A_niter">niter</code></td>
<td>
<p>The number of iterations of the algorithm to perform.</p>
</td></tr>
<tr><td><code id="gibbsGMM_+3A_nburn">nburn</code></td>
<td>
<p>The number of iterations to discard as burn-in.</p>
</td></tr>
<tr><td><code id="gibbsGMM_+3A_priors">priors</code></td>
<td>
<p>A list of priors for the parameters of the model.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix containing MCMC samples for the parameters of the mixture model.
</p>

<hr>
<h2 id='gibbsNorm'>Fit a univariate normal (Gaussian) distribution to the observed data.</h2><span id='topic+gibbsNorm'></span>

<h3>Description</h3>

<p>Fit a univariate normal (Gaussian) distribution to the observed data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gibbsNorm(y, niter = 1000, priors = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gibbsNorm_+3A_y">y</code></td>
<td>
<p>A vector of observed pixel data.</p>
</td></tr>
<tr><td><code id="gibbsNorm_+3A_niter">niter</code></td>
<td>
<p>The number of iterations of the algorithm to perform.</p>
</td></tr>
<tr><td><code id="gibbsNorm_+3A_priors">priors</code></td>
<td>
<p>A list of priors for the parameters of the model.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing MCMC samples for the mean and standard deviation.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rnorm(100,mean=5,sd=2)
res.norm &lt;- gibbsNorm(y, priors=list(mu=0, mu.sd=1e6, sigma=1e-3, sigma.nu=1e-3))
summary(res.norm$mu[501:1000])
summary(res.norm$sigma[501:1000])
</code></pre>

<hr>
<h2 id='gibbsPotts'>Fit a hidden Potts model to the observed data, using a fixed value of beta.</h2><span id='topic+gibbsPotts'></span>

<h3>Description</h3>

<p>Fit a hidden Potts model to the observed data, using a fixed value of beta.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gibbsPotts(y, labels, beta, mu, sd, neighbors, blocks, priors, niter = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gibbsPotts_+3A_y">y</code></td>
<td>
<p>A vector of observed pixel data.</p>
</td></tr>
<tr><td><code id="gibbsPotts_+3A_labels">labels</code></td>
<td>
<p>A matrix of pixel labels.</p>
</td></tr>
<tr><td><code id="gibbsPotts_+3A_beta">beta</code></td>
<td>
<p>The inverse temperature parameter of the Potts model.</p>
</td></tr>
<tr><td><code id="gibbsPotts_+3A_mu">mu</code></td>
<td>
<p>A vector of means for the mixture components.</p>
</td></tr>
<tr><td><code id="gibbsPotts_+3A_sd">sd</code></td>
<td>
<p>A vector of standard deviations for the mixture components.</p>
</td></tr>
<tr><td><code id="gibbsPotts_+3A_neighbors">neighbors</code></td>
<td>
<p>A matrix of all neighbors in the lattice, one row per pixel.</p>
</td></tr>
<tr><td><code id="gibbsPotts_+3A_blocks">blocks</code></td>
<td>
<p>A list of pixel indices, dividing the lattice into independent blocks.</p>
</td></tr>
<tr><td><code id="gibbsPotts_+3A_priors">priors</code></td>
<td>
<p>A list of priors for the parameters of the model.</p>
</td></tr>
<tr><td><code id="gibbsPotts_+3A_niter">niter</code></td>
<td>
<p>The number of iterations of the algorithm to perform.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix containing MCMC samples for the parameters of the Potts model.
</p>

<hr>
<h2 id='initSedki'>Initialize the ABC algorithm using the method of Sedki et al. (2013)</h2><span id='topic+initSedki'></span>

<h3>Description</h3>

<p>Initialize the ABC algorithm using the method of Sedki et al. (2013)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>initSedki(y, neighbors, blocks, param = list(npart = 10000), priors = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="initSedki_+3A_y">y</code></td>
<td>
<p>A vector of observed pixel data.</p>
</td></tr>
<tr><td><code id="initSedki_+3A_neighbors">neighbors</code></td>
<td>
<p>A matrix of all neighbours in the lattice, one row per pixel.</p>
</td></tr>
<tr><td><code id="initSedki_+3A_blocks">blocks</code></td>
<td>
<p>A list of pixel indices, dividing the lattice into independent blocks.</p>
</td></tr>
<tr><td><code id="initSedki_+3A_param">param</code></td>
<td>
<p>A list of options for the ABC-SMC algorithm.</p>
</td></tr>
<tr><td><code id="initSedki_+3A_priors">priors</code></td>
<td>
<p>A list of priors for the parameters of the model.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix containing SMC samples for the parameters of the Potts model.
</p>


<h3>References</h3>

<p>Sedki, M.; Pudlo, P.; Marin, J.-M.; Robert, C. P. &amp; Cornuet, J.-M. (2013) &quot;Efficient learning in ABC algorithms&quot; <a href="https://arxiv.org/abs/1210.1388">arXiv:1210.1388</a>
</p>

<hr>
<h2 id='mcmcPotts'>Fit the hidden Potts model using a Markov chain Monte Carlo algorithm.</h2><span id='topic+mcmcPotts'></span>

<h3>Description</h3>

<p>Fit the hidden Potts model using a Markov chain Monte Carlo algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcmcPotts(
  y,
  neighbors,
  blocks,
  priors,
  mh,
  niter = 55000,
  nburn = 5000,
  truth = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcmcPotts_+3A_y">y</code></td>
<td>
<p>A vector of observed pixel data.</p>
</td></tr>
<tr><td><code id="mcmcPotts_+3A_neighbors">neighbors</code></td>
<td>
<p>A matrix of all neighbors in the lattice, one row per pixel.</p>
</td></tr>
<tr><td><code id="mcmcPotts_+3A_blocks">blocks</code></td>
<td>
<p>A list of pixel indices, dividing the lattice into independent blocks.</p>
</td></tr>
<tr><td><code id="mcmcPotts_+3A_priors">priors</code></td>
<td>
<p>A list of priors for the parameters of the model.</p>
</td></tr>
<tr><td><code id="mcmcPotts_+3A_mh">mh</code></td>
<td>
<p>A list of options for the Metropolis-Hastings algorithm.</p>
</td></tr>
<tr><td><code id="mcmcPotts_+3A_niter">niter</code></td>
<td>
<p>The number of iterations of the algorithm to perform.</p>
</td></tr>
<tr><td><code id="mcmcPotts_+3A_nburn">nburn</code></td>
<td>
<p>The number of iterations to discard as burn-in.</p>
</td></tr>
<tr><td><code id="mcmcPotts_+3A_truth">truth</code></td>
<td>
<p>A matrix containing the ground truth for the pixel labels.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix containing MCMC samples for the parameters of the Potts model.
</p>

<hr>
<h2 id='mcmcPottsNoData'>Simulate pixel labels using chequerboard Gibbs sampling.</h2><span id='topic+mcmcPottsNoData'></span>

<h3>Description</h3>

<p>Simulate pixel labels using chequerboard Gibbs sampling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcmcPottsNoData(beta, k, neighbors, blocks, niter = 1000, random = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcmcPottsNoData_+3A_beta">beta</code></td>
<td>
<p>The inverse temperature parameter of the Potts model.</p>
</td></tr>
<tr><td><code id="mcmcPottsNoData_+3A_k">k</code></td>
<td>
<p>The number of unique labels.</p>
</td></tr>
<tr><td><code id="mcmcPottsNoData_+3A_neighbors">neighbors</code></td>
<td>
<p>A matrix of all neighbors in the lattice, one row per pixel.</p>
</td></tr>
<tr><td><code id="mcmcPottsNoData_+3A_blocks">blocks</code></td>
<td>
<p>A list of pixel indices, dividing the lattice into independent blocks.</p>
</td></tr>
<tr><td><code id="mcmcPottsNoData_+3A_niter">niter</code></td>
<td>
<p>The number of iterations of the algorithm to perform.</p>
</td></tr>
<tr><td><code id="mcmcPottsNoData_+3A_random">random</code></td>
<td>
<p>Whether to initialize the labels using random or deterministic starting values.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following elements:
</p>

<dl>
<dt><code>alloc</code></dt><dd><p>An n by k matrix containing the number of times that pixel i was allocated to label j.</p>
</dd>
<dt><code>z</code></dt><dd><p>An <code>(n+1)</code> by k matrix containing the final sample from the Potts model after niter iterations of chequerboard Gibbs.</p>
</dd>
<dt><code>sum</code></dt><dd><p>An <code>niter</code> by 1 matrix containing the sum of like neighbors, i.e. the sufficient statistic of the Potts model, at each iteration.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'># Swendsen-Wang for a 2x2 lattice
neigh &lt;- matrix(c(5,2,5,3,  1,5,5,4,  5,4,1,5,  3,5,2,5), nrow=4, ncol=4, byrow=TRUE)
blocks &lt;- list(c(1,4), c(2,3))
res.Gibbs &lt;- mcmcPottsNoData(0.7, 3, neigh, blocks, niter=200)
res.Gibbs$z
res.Gibbs$sum[200]
</code></pre>

<hr>
<h2 id='res'>Simulation from the Potts model using single-site Gibbs updates.</h2><span id='topic+res'></span>

<h3>Description</h3>

<p>100 iterations of Gibbs sampling for a <code class="reqn">500 \times 500</code> lattice
with <code class="reqn">\beta=0.22</code> and <code class="reqn">k=2</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>res
</code></pre>


<h3>Format</h3>

<p>A <code>list</code> containing 7 variables.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mcmcPotts">mcmcPotts</a></code>
</p>

<hr>
<h2 id='res2'>Simulation from the Potts model using single-site Gibbs updates.</h2><span id='topic+res2'></span>

<h3>Description</h3>

<p>100 iterations of Gibbs sampling for a <code class="reqn">500 \times 500</code> lattice
with <code class="reqn">\beta=0.44</code> and <code class="reqn">k=2</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>res2
</code></pre>


<h3>Format</h3>

<p>A <code>list</code> containing 7 variables.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mcmcPotts">mcmcPotts</a></code>
</p>

<hr>
<h2 id='res3'>Simulation from the Potts model using single-site Gibbs updates.</h2><span id='topic+res3'></span>

<h3>Description</h3>

<p>100 iterations of Gibbs sampling for a <code class="reqn">500 \times 500</code> lattice
with <code class="reqn">\beta=0.88</code> and <code class="reqn">k=2</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>res3
</code></pre>


<h3>Format</h3>

<p>A <code>list</code> containing 7 variables.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mcmcPotts">mcmcPotts</a></code>
</p>

<hr>
<h2 id='res4'>Simulation from the Potts model using single-site Gibbs updates.</h2><span id='topic+res4'></span>

<h3>Description</h3>

<p>100 iterations of Gibbs sampling for a <code class="reqn">500 \times 500</code> lattice
with <code class="reqn">\beta=1.32</code> and <code class="reqn">k=2</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>res4
</code></pre>


<h3>Format</h3>

<p>A <code>list</code> containing 7 variables.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mcmcPotts">mcmcPotts</a></code>
</p>

<hr>
<h2 id='res5'>Simulation from the Potts model using single-site Gibbs updates.</h2><span id='topic+res5'></span>

<h3>Description</h3>

<p>5000 iterations of Gibbs sampling for a <code class="reqn">500 \times 500</code> lattice
with <code class="reqn">\beta=1.32</code> and <code class="reqn">k=2</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>res5
</code></pre>


<h3>Format</h3>

<p>A <code>list</code> containing 4 variables.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mcmcPottsNoData">mcmcPottsNoData</a></code>
</p>

<hr>
<h2 id='smcPotts'>Fit the hidden Potts model using approximate Bayesian computation with sequential Monte Carlo (ABC-SMC).</h2><span id='topic+smcPotts'></span>

<h3>Description</h3>

<p>Fit the hidden Potts model using approximate Bayesian computation with sequential Monte Carlo (ABC-SMC).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smcPotts(
  y,
  neighbors,
  blocks,
  param = list(npart = 10000, nstat = 50),
  priors = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="smcPotts_+3A_y">y</code></td>
<td>
<p>A vector of observed pixel data.</p>
</td></tr>
<tr><td><code id="smcPotts_+3A_neighbors">neighbors</code></td>
<td>
<p>A matrix of all neighbors in the lattice, one row per pixel.</p>
</td></tr>
<tr><td><code id="smcPotts_+3A_blocks">blocks</code></td>
<td>
<p>A list of pixel indices, dividing the lattice into independent blocks.</p>
</td></tr>
<tr><td><code id="smcPotts_+3A_param">param</code></td>
<td>
<p>A list of options for the ABC-SMC algorithm.</p>
</td></tr>
<tr><td><code id="smcPotts_+3A_priors">priors</code></td>
<td>
<p>A list of priors for the parameters of the model.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix containing SMC samples for the parameters of the Potts model.
</p>

<hr>
<h2 id='sufficientStat'>Calculate the sufficient statistic of the Potts model for the given labels.</h2><span id='topic+sufficientStat'></span>

<h3>Description</h3>

<p>Calculate the sufficient statistic of the Potts model for the given labels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sufficientStat(labels, neighbors, blocks, k)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sufficientStat_+3A_labels">labels</code></td>
<td>
<p>A matrix of pixel labels.</p>
</td></tr>
<tr><td><code id="sufficientStat_+3A_neighbors">neighbors</code></td>
<td>
<p>A matrix of all neighbors in the lattice, one row per pixel.</p>
</td></tr>
<tr><td><code id="sufficientStat_+3A_blocks">blocks</code></td>
<td>
<p>A list of pixel indices, dividing the lattice into independent blocks.</p>
</td></tr>
<tr><td><code id="sufficientStat_+3A_k">k</code></td>
<td>
<p>The number of unique labels.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The sum of like neighbors.
</p>

<hr>
<h2 id='swNoData'>Simulate pixel labels using the Swendsen-Wang algorithm.</h2><span id='topic+swNoData'></span>

<h3>Description</h3>

<p>The algorithm of Swendsen &amp; Wang (1987) forms clusters of neighbouring pixels,
then updates all of the labels within a cluster to the same value. When
simulating from the prior, such as a Potts model without an external field,
this algorithm is very efficient.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>swNoData(beta, k, neighbors, blocks, niter = 1000, random = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="swNoData_+3A_beta">beta</code></td>
<td>
<p>The inverse temperature parameter of the Potts model.</p>
</td></tr>
<tr><td><code id="swNoData_+3A_k">k</code></td>
<td>
<p>The number of unique labels.</p>
</td></tr>
<tr><td><code id="swNoData_+3A_neighbors">neighbors</code></td>
<td>
<p>A matrix of all neighbors in the lattice, one row per pixel.</p>
</td></tr>
<tr><td><code id="swNoData_+3A_blocks">blocks</code></td>
<td>
<p>A list of pixel indices, dividing the lattice into independent blocks.</p>
</td></tr>
<tr><td><code id="swNoData_+3A_niter">niter</code></td>
<td>
<p>The number of iterations of the algorithm to perform.</p>
</td></tr>
<tr><td><code id="swNoData_+3A_random">random</code></td>
<td>
<p>Whether to initialize the labels using random or deterministic starting values.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following elements:
</p>

<dl>
<dt><code>alloc</code></dt><dd><p>An n by k matrix containing the number of times that pixel i was allocated to label j.</p>
</dd>
<dt><code>z</code></dt><dd><p>An <code>(n+1)</code> by k matrix containing the final sample from the Potts model after niter iterations of Swendsen-Wang.</p>
</dd>
<dt><code>sum</code></dt><dd><p>An <code>niter</code> by 1 matrix containing the sum of like neighbors, i.e. the sufficient statistic of the Potts model, at each iteration.</p>
</dd>
</dl>



<h3>References</h3>

<p>Swendsen, R. H. &amp; Wang, J.-S. (1987) &quot;Nonuniversal critical dynamics in Monte Carlo simulations&quot; <em>Physical Review Letters</em> <b>58</b>(2), 86&ndash;88, DOI: doi: <a href="https://doi.org/10.1103/PhysRevLett.58.86">10.1103/PhysRevLett.58.86</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Swendsen-Wang for a 2x2 lattice
neigh &lt;- matrix(c(5,2,5,3,  1,5,5,4,  5,4,1,5,  3,5,2,5), nrow=4, ncol=4, byrow=TRUE)
blocks &lt;- list(c(1,4), c(2,3))
res.sw &lt;- swNoData(0.7, 3, neigh, blocks, niter=200)
res.sw$z
res.sw$sum[200]
</code></pre>

<hr>
<h2 id='synth'>Simulation from the Potts model using Swendsen-Wang.</h2><span id='topic+synth'></span>

<h3>Description</h3>

<p>Simulations for a <code class="reqn">500 \times 500</code> lattice for fixed values
of the inverse temperature parameter, <code class="reqn">\beta</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>synth
</code></pre>


<h3>Format</h3>

<p>A <code>list</code> containing 5 variables:
</p>

<dl>
<dt>0.22</dt><dd><p>simulations for <code class="reqn">\beta = 0.22</code></p>
</dd>
<dt>0.44</dt><dd><p>simulations for <code class="reqn">\beta = 0.44</code></p>
</dd>
<dt>0.88</dt><dd><p>simulations for <code class="reqn">\beta = 0.88</code></p>
</dd>
<dt>1.32</dt><dd><p>simulations for <code class="reqn">\beta = 1.32</code></p>
</dd>
<dt>tm</dt><dd><p>time taken by the simulations</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="#topic+swNoData">swNoData</a></code>
</p>

<hr>
<h2 id='testResample'>Test the residual resampling algorithm.</h2><span id='topic+testResample'></span>

<h3>Description</h3>

<p>Test the residual resampling algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>testResample(values, weights, pseudo)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="testResample_+3A_values">values</code></td>
<td>
<p>A vector of SMC particles.</p>
</td></tr>
<tr><td><code id="testResample_+3A_weights">weights</code></td>
<td>
<p>A vector of importance weights for each particle.</p>
</td></tr>
<tr><td><code id="testResample_+3A_pseudo">pseudo</code></td>
<td>
<p>A matrix of pseudo-data for each particle.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following elements:
</p>

<dl>
<dt><code>beta</code></dt><dd><p>A vector of resampled particles.</p>
</dd>
<dt><code>wt</code></dt><dd><p>The new importance weights, after resampling.</p>
</dd>
<dt><code>pseudo</code></dt><dd><p>A matrix of pseudo-data for each particle.</p>
</dd>
<dt><code>idx</code></dt><dd><p>The indices of the parents of the resampled particles.</p>
</dd>
</dl>



<h3>References</h3>

<p>Liu, J. S. &amp; Chen, R. (1998) &quot;Sequential Monte Carlo Methods for Dynamic Systems&quot;
<em>J. Am. Stat. Assoc.</em> <b>93</b>(443): 1032&ndash;1044, DOI: doi: <a href="https://doi.org/10.1080/01621459.1998.10473765">10.1080/01621459.1998.10473765</a>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
