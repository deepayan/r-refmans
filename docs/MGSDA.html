<!DOCTYPE html><html><head><title>Help for package MGSDA</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {MGSDA}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#classifyV'>
<p>Classification for MGSDA</p></a></li>
<li><a href='#cv.dLDA'>
<p>Cross-validation for MGSDA</p></a></li>
<li><a href='#dLDA'>
<p>Estimate the matrix of discriminant vectors using L_1 penalty on the rows</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Multi-Group Sparse Discriminant Analysis</td>
</tr>
<tr>
<td>Version:</td>
<td>1.6.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-09-03</td>
</tr>
<tr>
<td>Author:</td>
<td>Irina Gaynanova</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Irina Gaynanova &lt;irinagn@umich.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements Multi-Group Sparse Discriminant Analysis proposal of I.Gaynanova, J.Booth and M.Wells (2016), Simultaneous sparse estimation of canonical vectors in the p&gt;&gt;N setting, JASA &lt;<a href="https://doi.org/10.1080%2F01621459.2015.1034318">doi:10.1080/01621459.2015.1034318</a>&gt;.</td>
</tr>
<tr>
<td>Imports:</td>
<td>MASS, stats</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-09-03 20:15:39 UTC; irinag</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-09-03 21:00:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='classifyV'>
Classification for MGSDA
</h2><span id='topic+classifyV'></span>

<h3>Description</h3>

<p>Classify observations in the test set using the supplied matrix of canonical vectors V and the training set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>classifyV(Xtrain, Ytrain, Xtest, V, prior = TRUE, tol1 = 1e-10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="classifyV_+3A_xtrain">Xtrain</code></td>
<td>

<p>A Nxp data matrix; N observations on the rows and p features on the columns.
</p>
</td></tr>
<tr><td><code id="classifyV_+3A_ytrain">Ytrain</code></td>
<td>

<p>A N vector containing the group labels. Should be coded as 1,2,...,G, where G is the number of groups.
</p>
</td></tr>
<tr><td><code id="classifyV_+3A_xtest">Xtest</code></td>
<td>

<p>A Mxp data matrix; M test observations on the rows and p features on the columns.
</p>
</td></tr>
<tr><td><code id="classifyV_+3A_v">V</code></td>
<td>

<p>A pxr matrix of canonical vectors that is used to classify observations.
</p>
</td></tr>
<tr><td><code id="classifyV_+3A_prior">prior</code></td>
<td>

<p>A logical indicating whether to put larger weights to the groups of larger size; the default value is TRUE.
</p>
</td></tr>
<tr><td><code id="classifyV_+3A_tol1">tol1</code></td>
<td>

<p>Tolerance level for the eigenvalues of <code class="reqn">V^tWV</code>. If some eigenvalues are less than <code>tol</code>, the low-rank version of <code>V</code> is used for classification.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For a new observation with the value x, the classification is performed based on the smallest Mahalanobis distance in the projected space:
</p>
<p style="text-align: center;"><code class="reqn">\min_{1\le g \le G}(V^tx-Z_g)(V^tWV)^{-1}(V^tx-Z_g)</code>
</p>

<p>where <code class="reqn">Z_g</code> are the group-specific means of the training dataset in the projected space and <code class="reqn">W</code> is the sample within-group covariance matrix.   
</p>
<p>If <code>prior=T</code>, then the above distance is adjusted by <code class="reqn">-2\log\frac{n_g}{N}</code>, where <code class="reqn">n_g</code> is the size of group g.
</p>


<h3>Value</h3>

<p>Returns a vector of length M with predicted group labels for the test set.





</p>


<h3>Author(s)</h3>

<p>Irina Gaynanova
</p>


<h3>References</h3>

<p>I.Gaynanova, J.Booth and M.Wells (2016) &quot;Simultaneous Sparse Estimation of Canonical Vectors in the p&gt;&gt;N setting.&quot;, JASA, 111(514), 696-706.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Example 1
# generate training data
n=10
p=100
G=3
ytrain=rep(1:G,each=n)
set.seed(1)
xtrain=matrix(rnorm(p*n*G),n*G,p)
# find V
V=dLDA(xtrain,ytrain,lambda=0.1)
sum(rowSums(V)!=0)
# generate test data
m=20
set.seed(3)
xtest=matrix(rnorm(p*m),m,p)
# perform classification
ytest=classifyV(xtrain,ytrain,xtest,V)
</code></pre>

<hr>
<h2 id='cv.dLDA'>
Cross-validation for MGSDA
</h2><span id='topic+cv.dLDA'></span>

<h3>Description</h3>

<p>Chooses optimal tuning parameter lambda for function dLDA based on the m-fold cross-validation mean squared error
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.dLDA(Xtrain, Ytrain, lambdaval = NULL, nl = 100, msep = 5, eps = 1e-6,
    l_min_ratio = ifelse(n&lt;p,0.1,0.0001),myseed=NULL,prior=TRUE,rho=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv.dLDA_+3A_xtrain">Xtrain</code></td>
<td>

<p>A Nxp data matrix; N observations on the rows and p features on the columns
</p>
</td></tr>
<tr><td><code id="cv.dLDA_+3A_ytrain">Ytrain</code></td>
<td>

<p>A N vector containing the group labels. Should be coded as 1,2,...,G, where G is the number of groups
</p>
</td></tr>
<tr><td><code id="cv.dLDA_+3A_lambdaval">lambdaval</code></td>
<td>

<p>Optional user-supplied sequence of tuning parameters; the default value is NULL and <code>cv.dLDA</code> chooses its own sequence
</p>
</td></tr>
<tr><td><code id="cv.dLDA_+3A_nl">nl</code></td>
<td>

<p>Number of lambda values; the default value is 50
</p>
</td></tr>
<tr><td><code id="cv.dLDA_+3A_msep">msep</code></td>
<td>

<p>Number of cross-validation folds; the default value is 5
</p>
</td></tr>
<tr><td><code id="cv.dLDA_+3A_eps">eps</code></td>
<td>

<p>Tolerance level for the convergence of the optimization algorithm; the default value is 1e-6
</p>
</td></tr>
<tr><td><code id="cv.dLDA_+3A_l_min_ratio">l_min_ratio</code></td>
<td>

<p>Smallest value for lambda, as a fraction of <code>lambda.max</code>, the data-derived value for which all coefficients are zero; the default value is 0.1 if the number of samples <code>n</code> is larger than the number of variables <code>p</code>, and is 0.001 otherwise.
</p>
</td></tr>
<tr><td><code id="cv.dLDA_+3A_myseed">myseed</code></td>
<td>

<p>Optional specification of random seed for generating the folds; the default value is NULL.
</p>
</td></tr>
<tr><td><code id="cv.dLDA_+3A_prior">prior</code></td>
<td>

<p>A logical indicating whether to put larger weights to the groups of larger size; the default value is TRUE.
</p>
</td></tr>
<tr><td><code id="cv.dLDA_+3A_rho">rho</code></td>
<td>
<p>A scalar that ensures the objective function is bounded from below; the default value is 1.
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>lambdaval</code></td>
<td>
<p>The sequence of tuning parameters used</p>
</td></tr>
<tr><td><code>error_mean</code></td>
<td>
<p>The mean cross-validated number of misclassified observations - a vector of length <code>length(lambdaval)</code></p>
</td></tr>
<tr><td><code>error_se</code></td>
<td>
<p>The standard error associated with each value of <code>error_mean</code></p>
</td></tr>
<tr><td><code>lambda_min</code></td>
<td>
<p>The value of tuning parameter that has the minimal mean cross-validation error</p>
</td></tr>
<tr><td><code>f</code></td>
<td>
<p>The mean cross-validated number of non-zero features - a vector of length <code>length(lambdaval)</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Irina Gaynanova
</p>


<h3>References</h3>

<p>I.Gaynanova, J.Booth and M.Wells (2016). &quot;Simultaneous sparse estimation of canonical vectors in the p&gt;&gt;N setting&quot;, JASA, 111(514), 696-706.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Example 1
n=10
p=100
G=3
ytrain=rep(1:G,each=n)
set.seed(1)
xtrain=matrix(rnorm(p*n*G),n*G,p)
# find optimal tuning parameter
out.cv=cv.dLDA(xtrain,ytrain)
# find V
V=dLDA(xtrain,ytrain,lambda=out.cv$lambda_min)
# number of non-zero features
sum(rowSums(V)!=0)
</code></pre>

<hr>
<h2 id='dLDA'>
Estimate the matrix of discriminant vectors using L_1 penalty on the rows
</h2><span id='topic+dLDA'></span>

<h3>Description</h3>

<p>Solve Multi-Group Sparse Discriminant Anlalysis problem for the supplied value of the tuning parameter lambda.


</p>


<h3>Usage</h3>

<pre><code class='language-R'>dLDA(xtrain, ytrain, lambda, Vinit = NULL,eps=1e-6,maxiter=1000,rho=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dLDA_+3A_xtrain">xtrain</code></td>
<td>

<p>A Nxp data matrix; N observations on the rows and p features on the columns.
</p>
</td></tr>
<tr><td><code id="dLDA_+3A_ytrain">ytrain</code></td>
<td>

<p>A N-vector containing the group labels. Should be coded as 1,2,...,G, where G is the number of groups.
</p>
</td></tr>
<tr><td><code id="dLDA_+3A_lambda">lambda</code></td>
<td>

<p>Tuning parameter.
</p>
</td></tr>
<tr><td><code id="dLDA_+3A_vinit">Vinit</code></td>
<td>

<p>A px(G-1) optional initial value for the optimization algorithm; the default value is NULL.
</p>
</td></tr>
<tr><td><code id="dLDA_+3A_eps">eps</code></td>
<td>

<p>Tolerance level for the convergence of the optimization algorithm; the default value is 1e-6.
</p>
</td></tr>
<tr><td><code id="dLDA_+3A_maxiter">maxiter</code></td>
<td>

<p>Maximal number of iterations for the optimization algorithm; the default value is 1000.
</p>
</td></tr>
<tr><td><code id="dLDA_+3A_rho">rho</code></td>
<td>
<p>A scalar that ensures the objective function is bounded from below; the default value is 1.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Solves the following optimization problem:
</p>
<p style="text-align: center;"><code class="reqn">\min_V \frac12 Tr(V^tWV+\rho V^tDD^tV)-Tr(D^tV)+\lambda\sum_{i=1}^p\|v_i\|_2</code>
</p>

<p>Here W is the within-group sample covariance matrix and D is the matrix of orthogonal contrasts between the group means, both are constructed based on the supplied values of <code>xtrain</code> and <code>ytrain</code>.
</p>
<p>When <code class="reqn">G=2</code>, the row penalty reduces to vector L_1 penalty.
</p>


<h3>Value</h3>

<p>Returns a px(G-1) matrix of canonical vectors V.





</p>


<h3>Author(s)</h3>

<p>Irina Gaynanova
</p>


<h3>References</h3>

<p>I.Gaynanova, J.Booth and M.Wells (2016) &quot;Simultaneous Sparse Estimation of Canonical Vectors in the p&gt;&gt;N setting&quot;, JASA, 111(514), 696-706.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example 1
n=10
p=100
G=3
ytrain=rep(1:G,each=n)
set.seed(1)
xtrain=matrix(rnorm(p*n*G),n*G,p)
V=dLDA(xtrain,ytrain,lambda=0.1)
sum(rowSums(V)!=0) # number of non-zero rows
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
