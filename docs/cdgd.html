<!DOCTYPE html><html><head><title>Help for package cdgd</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {cdgd}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#cdgd0_manual'><p>Perform unconditional decomposition with nuisance functions estimated beforehand</p></a></li>
<li><a href='#cdgd0_ml'><p>Perform unconditional decomposition via machine learning</p></a></li>
<li><a href='#cdgd0_pa'><p>Perform unconditional decomposition via parametric models</p></a></li>
<li><a href='#cdgd1_manual'><p>Perform conditional decomposition with nuisance functions estimated beforehand</p></a></li>
<li><a href='#cdgd1_ml'><p>Perform conditional decomposition via machine learning</p></a></li>
<li><a href='#cdgd1_pa'><p>Perform conditional decomposition via parametric models</p></a></li>
<li><a href='#exp_data'><p>Simulated example data</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Causal Decomposition of Group Disparities</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3.4</td>
</tr>
<tr>
<td>Description:</td>
<td>The framework of causal decomposition of group disparities developed by
    Yu and Elwert (2023)
    &lt;<a href="https://arxiv.org/abs/2306.16591">arXiv:2306.16591</a>&gt;.
    This package implements the decomposition estimators
    that are based on efficient influence functions. For the
    nuisance functions of the estimators, both parametric and
    nonparametric options are provided, as well as manual options in case
    the default models are not satisfying.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/ang-yu/cdgd">https://github.com/ang-yu/cdgd</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/ang-yu/cdgd/issues">https://github.com/ang-yu/cdgd/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>caret (&ge; 6.0.0)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>gbm (&ge; 2.1.8), nnet (&ge; 7.3.0), ranger (&ge; 0.14.1)</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-01-06 14:01:48 UTC; Ang</td>
</tr>
<tr>
<td>Author:</td>
<td>Ang Yu <a href="https://orcid.org/0000-0002-1828-0165"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cre,
    cph] (&lt;https://ang-yu.github.io/&gt;)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Ang Yu &lt;ang_yu@outlook.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-01-08 14:00:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='cdgd0_manual'>Perform unconditional decomposition with nuisance functions estimated beforehand</h2><span id='topic+cdgd0_manual'></span>

<h3>Description</h3>

<p>This function gives the user full control over the estimation of the nuisance functions.
For the unconditional decomposition, three nuisance functions (YgivenGX.Pred_D0, YgivenGX.Pred_D1, and DgivenGX.Pred) need to be estimated.
The nuisance functions should be estimated using cross-fitting if Donsker class is not assumed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cdgd0_manual(
  Y,
  D,
  G,
  YgivenGX.Pred_D1,
  YgivenGX.Pred_D0,
  DgivenGX.Pred,
  data,
  alpha = 0.05
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cdgd0_manual_+3A_y">Y</code></td>
<td>
<p>Outcome. The name of a numeric variable.</p>
</td></tr>
<tr><td><code id="cdgd0_manual_+3A_d">D</code></td>
<td>
<p>Treatment status. The name of a binary numeric variable taking values of 0 and 1.</p>
</td></tr>
<tr><td><code id="cdgd0_manual_+3A_g">G</code></td>
<td>
<p>Advantaged group membership. The name of a binary numeric variable taking values of 0 and 1.</p>
</td></tr>
<tr><td><code id="cdgd0_manual_+3A_ygivengx.pred_d1">YgivenGX.Pred_D1</code></td>
<td>
<p>A numeric vector of predicted Y values given X, G, and D=1. Vector length=nrow(data).</p>
</td></tr>
<tr><td><code id="cdgd0_manual_+3A_ygivengx.pred_d0">YgivenGX.Pred_D0</code></td>
<td>
<p>A numeric vector of predicted Y values given X, G, and D=0. Vector length=nrow(data).</p>
</td></tr>
<tr><td><code id="cdgd0_manual_+3A_dgivengx.pred">DgivenGX.Pred</code></td>
<td>
<p>A numeric vector of predicted D values given X and G. Vector length=nrow(data).</p>
</td></tr>
<tr><td><code id="cdgd0_manual_+3A_data">data</code></td>
<td>
<p>A data frame.</p>
</td></tr>
<tr><td><code id="cdgd0_manual_+3A_alpha">alpha</code></td>
<td>
<p>1-alpha confidence interval.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of estimates.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># This example will take a minute to run.

data(exp_data)

Y="outcome"
D="treatment"
G="group_a"
X=c("Q","confounder")
data=exp_data

set.seed(1)

### estimate the nuisance functions with cross-fitting
sample1 &lt;- sample(nrow(data), floor(nrow(data)/2), replace=FALSE)
sample2 &lt;- setdiff(1:nrow(data), sample1)

### outcome regression model

message &lt;- utils::capture.output( YgivenDGX.Model.sample1 &lt;-
    caret::train(stats::as.formula(paste(Y, paste(D,G,paste(X,collapse="+"),sep="+"), sep="~")),
             data=data[sample1,], method="ranger", trControl=caret::trainControl(method="cv"),
             tuneGrid=expand.grid(mtry=c(2,4),splitrule=c("variance"),min.node.size=c(50,100))) )
message &lt;- utils::capture.output( YgivenDGX.Model.sample2 &lt;-
    caret::train(stats::as.formula(paste(Y, paste(D,G,paste(X,collapse="+"),sep="+"), sep="~")),
             data=data[sample2,], method="ranger", trControl=caret::trainControl(method="cv"),
             tuneGrid=expand.grid(mtry=c(2,4),splitrule=c("variance"),min.node.size=c(50,100))) )

### propensity score model
data[,D] &lt;- as.factor(data[,D])
levels(data[,D]) &lt;- c("D0","D1")  # necessary for caret implementation of ranger

message &lt;- utils::capture.output( DgivenGX.Model.sample1 &lt;-
    caret::train(stats::as.formula(paste(D, paste(G,paste(X,collapse="+"),sep="+"), sep="~")),
             data=data[sample1,], method="ranger",
             trControl=caret::trainControl(method="cv", classProbs=TRUE),
             tuneGrid=expand.grid(mtry=c(1,2),splitrule=c("gini"),min.node.size=c(50,100))) )
message &lt;- utils::capture.output( DgivenGX.Model.sample2 &lt;-
    caret::train(stats::as.formula(paste(D, paste(G,paste(X,collapse="+"),sep="+"), sep="~")),
             data=data[sample2,], method="ranger",
             trControl=caret::trainControl(method="cv", classProbs=TRUE),
             tuneGrid=expand.grid(mtry=c(1,2),splitrule=c("gini"),min.node.size=c(50,100))) )

data[,D] &lt;- as.numeric(data[,D])-1

### cross-fitted predictions
YgivenGX.Pred_D0 &lt;- YgivenGX.Pred_D1 &lt;- DgivenGX.Pred &lt;- rep(NA, nrow(data))

pred_data &lt;- data
pred_data[,D] &lt;- 0
YgivenGX.Pred_D0[sample2] &lt;- stats::predict(YgivenDGX.Model.sample1, newdata = pred_data[sample2,])
YgivenGX.Pred_D0[sample1] &lt;- stats::predict(YgivenDGX.Model.sample2, newdata = pred_data[sample1,])

pred_data &lt;- data
pred_data[,D] &lt;- 1
YgivenGX.Pred_D1[sample2] &lt;- stats::predict(YgivenDGX.Model.sample1, newdata = pred_data[sample2,])
YgivenGX.Pred_D1[sample1] &lt;- stats::predict(YgivenDGX.Model.sample2, newdata = pred_data[sample1,])

pred_data &lt;- data
DgivenGX.Pred[sample2] &lt;- stats::predict(DgivenGX.Model.sample1,
    newdata = pred_data[sample2,], type="prob")[,2]
DgivenGX.Pred[sample1] &lt;- stats::predict(DgivenGX.Model.sample2,
    newdata = pred_data[sample1,], type="prob")[,2]

results &lt;- cdgd0_manual(Y=Y,D=D,G=G,
                       YgivenGX.Pred_D0=YgivenGX.Pred_D0,
                       YgivenGX.Pred_D1=YgivenGX.Pred_D1,
                       DgivenGX.Pred=DgivenGX.Pred,
                       data=data)

results
</code></pre>

<hr>
<h2 id='cdgd0_ml'>Perform unconditional decomposition via machine learning</h2><span id='topic+cdgd0_ml'></span>

<h3>Description</h3>

<p>Perform unconditional decomposition via machine learning
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cdgd0_ml(Y, D, G, X, data, algorithm, alpha = 0.05, trim = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cdgd0_ml_+3A_y">Y</code></td>
<td>
<p>Outcome. The name of a numeric variable (can be binary and take values of 0 and 1).</p>
</td></tr>
<tr><td><code id="cdgd0_ml_+3A_d">D</code></td>
<td>
<p>Treatment status. The name of a binary numeric variable taking values of 0 and 1.</p>
</td></tr>
<tr><td><code id="cdgd0_ml_+3A_g">G</code></td>
<td>
<p>Advantaged group membership. The name of a binary numeric variable taking values of 0 and 1.</p>
</td></tr>
<tr><td><code id="cdgd0_ml_+3A_x">X</code></td>
<td>
<p>Confounders. A vector of variables names.</p>
</td></tr>
<tr><td><code id="cdgd0_ml_+3A_data">data</code></td>
<td>
<p>A data frame.</p>
</td></tr>
<tr><td><code id="cdgd0_ml_+3A_algorithm">algorithm</code></td>
<td>
<p>The ML algorithm for modelling. &quot;nnet&quot; for neural network, &quot;ranger&quot; for random forests, &quot;gbm&quot; for generalized boosted models.</p>
</td></tr>
<tr><td><code id="cdgd0_ml_+3A_alpha">alpha</code></td>
<td>
<p>1-alpha confidence interval.</p>
</td></tr>
<tr><td><code id="cdgd0_ml_+3A_trim">trim</code></td>
<td>
<p>Threshold for trimming the propensity score. When trim=a, individuals with propensity scores lower than a or higher than 1-a will be dropped.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of estimates.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># This example will take a minute to run.

data(exp_data)

set.seed(1)

results &lt;- cdgd0_ml(
Y="outcome",
D="treatment",
G="group_a",
X=c("Q","confounder"),
data=exp_data,
algorithm="gbm")

results[[1]]
</code></pre>

<hr>
<h2 id='cdgd0_pa'>Perform unconditional decomposition via parametric models</h2><span id='topic+cdgd0_pa'></span>

<h3>Description</h3>

<p>Perform unconditional decomposition via parametric models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cdgd0_pa(Y, D, G, X, data, alpha = 0.05, trim = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cdgd0_pa_+3A_y">Y</code></td>
<td>
<p>Outcome. The name of a numeric variable (can be binary and take values of 0 and 1).</p>
</td></tr>
<tr><td><code id="cdgd0_pa_+3A_d">D</code></td>
<td>
<p>Treatment status. The name of a binary numeric variable taking values of 0 and 1.</p>
</td></tr>
<tr><td><code id="cdgd0_pa_+3A_g">G</code></td>
<td>
<p>Advantaged group membership. The name of a binary numeric variable taking values of 0 and 1.</p>
</td></tr>
<tr><td><code id="cdgd0_pa_+3A_x">X</code></td>
<td>
<p>Confounders. A vector of variable names.</p>
</td></tr>
<tr><td><code id="cdgd0_pa_+3A_data">data</code></td>
<td>
<p>A data frame.</p>
</td></tr>
<tr><td><code id="cdgd0_pa_+3A_alpha">alpha</code></td>
<td>
<p>1-alpha confidence interval.</p>
</td></tr>
<tr><td><code id="cdgd0_pa_+3A_trim">trim</code></td>
<td>
<p>Threshold for trimming the propensity score. When trim=a, individuals with propensity scores lower than a or higher than 1-a will be dropped.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of estimates.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(exp_data)

results &lt;- cdgd0_pa(
Y="outcome",
D="treatment",
G="group_a",
X=c("Q","confounder"),
data=exp_data)

results[[1]]
</code></pre>

<hr>
<h2 id='cdgd1_manual'>Perform conditional decomposition with nuisance functions estimated beforehand</h2><span id='topic+cdgd1_manual'></span>

<h3>Description</h3>

<p>This function gives user full control over the estimation of the nuisance functions.
For the unconditional decomposition, ten nuisance functions need to be estimated.
The nuisance functions should be estimated using cross-fitting if Donsker class is not assumed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cdgd1_manual(
  Y,
  D,
  G,
  YgivenGXQ.Pred_D0,
  YgivenGXQ.Pred_D1,
  DgivenGXQ.Pred,
  Y0givenQ.Pred_G0,
  Y0givenQ.Pred_G1,
  Y1givenQ.Pred_G0,
  Y1givenQ.Pred_G1,
  DgivenQ.Pred_G0,
  DgivenQ.Pred_G1,
  GgivenQ.Pred,
  data,
  alpha = 0.05
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cdgd1_manual_+3A_y">Y</code></td>
<td>
<p>Outcome. The name of a numeric variable.</p>
</td></tr>
<tr><td><code id="cdgd1_manual_+3A_d">D</code></td>
<td>
<p>Treatment status. The name of a binary numeric variable taking values of 0 and 1.</p>
</td></tr>
<tr><td><code id="cdgd1_manual_+3A_g">G</code></td>
<td>
<p>Advantaged group membership. The name of a binary numeric variable taking values of 0 and 1.</p>
</td></tr>
<tr><td><code id="cdgd1_manual_+3A_ygivengxq.pred_d0">YgivenGXQ.Pred_D0</code></td>
<td>
<p>A numeric vector of predicted Y values given X, G, and D=0. Vector length=nrow(data).</p>
</td></tr>
<tr><td><code id="cdgd1_manual_+3A_ygivengxq.pred_d1">YgivenGXQ.Pred_D1</code></td>
<td>
<p>A numeric vector of predicted Y values given X, G, and D=1. Vector length=nrow(data).</p>
</td></tr>
<tr><td><code id="cdgd1_manual_+3A_dgivengxq.pred">DgivenGXQ.Pred</code></td>
<td>
<p>A numeric vector of predicted D values given X and G. Vector length=nrow(data).</p>
</td></tr>
<tr><td><code id="cdgd1_manual_+3A_y0givenq.pred_g0">Y0givenQ.Pred_G0</code></td>
<td>
<p>A numeric vector of predicted Y(0) values given Q and G=0. Vector length=nrow(data).</p>
</td></tr>
<tr><td><code id="cdgd1_manual_+3A_y0givenq.pred_g1">Y0givenQ.Pred_G1</code></td>
<td>
<p>A numeric vector of predicted Y(0) values given Q and G=0. Vector length=nrow(data).</p>
</td></tr>
<tr><td><code id="cdgd1_manual_+3A_y1givenq.pred_g0">Y1givenQ.Pred_G0</code></td>
<td>
<p>A numeric vector of predicted Y(1) values given Q and G=0. Vector length=nrow(data).</p>
</td></tr>
<tr><td><code id="cdgd1_manual_+3A_y1givenq.pred_g1">Y1givenQ.Pred_G1</code></td>
<td>
<p>A numeric vector of predicted Y(1) values given Q and G=1. Vector length=nrow(data).</p>
</td></tr>
<tr><td><code id="cdgd1_manual_+3A_dgivenq.pred_g0">DgivenQ.Pred_G0</code></td>
<td>
<p>A numeric vector of predicted D values given Q and G=0. Vector length=nrow(data).</p>
</td></tr>
<tr><td><code id="cdgd1_manual_+3A_dgivenq.pred_g1">DgivenQ.Pred_G1</code></td>
<td>
<p>A numeric vector of predicted D values given Q and G=0. Vector length=nrow(data).</p>
</td></tr>
<tr><td><code id="cdgd1_manual_+3A_ggivenq.pred">GgivenQ.Pred</code></td>
<td>
<p>A numeric vector of predicted G values given Q. Vector length=nrow(data).</p>
</td></tr>
<tr><td><code id="cdgd1_manual_+3A_data">data</code></td>
<td>
<p>A data frame.</p>
</td></tr>
<tr><td><code id="cdgd1_manual_+3A_alpha">alpha</code></td>
<td>
<p>1-alpha confidence interval.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataframe of estimates.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># This example will take a minute to run.

data(exp_data)

Y="outcome"
D="treatment"
G="group_a"
X="confounder"
Q="Q"
data=exp_data

set.seed(1)

### estimate the nuisance functions with cross-fitting
sample1 &lt;- sample(nrow(data), floor(nrow(data)/2), replace=FALSE)
sample2 &lt;- setdiff(1:nrow(data), sample1)

### outcome regression model

message &lt;- utils::capture.output( YgivenDGXQ.Model.sample1 &lt;-
  caret::train(stats::as.formula(paste(Y, paste(D,G,Q,paste(X,collapse="+"),sep="+"), sep="~")),
               data=data[sample1,], method="ranger",
               trControl=caret::trainControl(method="cv"),
               tuneGrid=expand.grid(mtry=c(2,4),splitrule=c("variance"),min.node.size=c(50,100))) )
message &lt;- utils::capture.output( YgivenDGXQ.Model.sample2 &lt;-
  caret::train(stats::as.formula(paste(Y, paste(D,G,Q,paste(X,collapse="+"),sep="+"), sep="~")),
               data=data[sample2,], method="ranger",
               trControl=caret::trainControl(method="cv"),
               tuneGrid=expand.grid(mtry=c(2,4),splitrule=c("variance"),min.node.size=c(50,100))) )

### propensity score model
data[,D] &lt;- as.factor(data[,D])
levels(data[,D]) &lt;- c("D0","D1")  # necessary for caret implementation of ranger

message &lt;- utils::capture.output( DgivenGXQ.Model.sample1 &lt;-
    caret::train(stats::as.formula(paste(D, paste(G,Q,paste(X,collapse="+"),sep="+"), sep="~")),
                 data=data[sample1,], method="ranger",
                 trControl=caret::trainControl(method="cv", classProbs=TRUE),
                 tuneGrid=expand.grid(mtry=c(1,2),splitrule=c("gini"),min.node.size=c(50,100))) )
message &lt;- utils::capture.output( DgivenGXQ.Model.sample2 &lt;-
    caret::train(stats::as.formula(paste(D, paste(G,Q,paste(X,collapse="+"),sep="+"), sep="~")),
                 data=data[sample2,], method="ranger",
                 trControl=caret::trainControl(method="cv", classProbs=TRUE),
                 tuneGrid=expand.grid(mtry=c(1,2),splitrule=c("gini"),min.node.size=c(50,100))) )

data[,D] &lt;- as.numeric(data[,D])-1

### cross-fitted predictions
YgivenGXQ.Pred_D0 &lt;- YgivenGXQ.Pred_D1 &lt;- DgivenGXQ.Pred &lt;- rep(NA, nrow(data))

pred_data &lt;- data
pred_data[,D] &lt;- 0
YgivenGXQ.Pred_D0[sample2] &lt;- stats::predict(YgivenDGXQ.Model.sample1,
    newdata = pred_data[sample2,])
YgivenGXQ.Pred_D0[sample1] &lt;- stats::predict(YgivenDGXQ.Model.sample2,
    newdata = pred_data[sample1,])

pred_data &lt;- data
pred_data[,D] &lt;- 1
YgivenGXQ.Pred_D1[sample2] &lt;- stats::predict(YgivenDGXQ.Model.sample1,
    newdata = pred_data[sample2,])
YgivenGXQ.Pred_D1[sample1] &lt;- stats::predict(YgivenDGXQ.Model.sample2,
    newdata = pred_data[sample1,])

pred_data &lt;- data
DgivenGXQ.Pred[sample2] &lt;- stats::predict(DgivenGXQ.Model.sample1,
    newdata = pred_data[sample2,], type="prob")[,2]
DgivenGXQ.Pred[sample1] &lt;- stats::predict(DgivenGXQ.Model.sample2,
    newdata = pred_data[sample1,], type="prob")[,2]

### Estimate E(Y_d | Q,g)
YgivenGXQ.Pred_D1_ncf &lt;- YgivenGXQ.Pred_D0_ncf &lt;- DgivenGXQ.Pred_ncf &lt;- rep(NA, nrow(data))
# ncf stands for non-cross-fitted

pred_data &lt;- data
pred_data[,D] &lt;- 1
YgivenGXQ.Pred_D1_ncf[sample1] &lt;- stats::predict(YgivenDGXQ.Model.sample1,
    newdata = pred_data[sample1,])
YgivenGXQ.Pred_D1_ncf[sample2] &lt;- stats::predict(YgivenDGXQ.Model.sample2,
    newdata = pred_data[sample2,])

pred_data &lt;- data
pred_data[,D] &lt;- 0
YgivenGXQ.Pred_D0_ncf[sample1] &lt;- stats::predict(YgivenDGXQ.Model.sample1,
    newdata = pred_data[sample1,])
YgivenGXQ.Pred_D0_ncf[sample2] &lt;- stats::predict(YgivenDGXQ.Model.sample2,
    newdata = pred_data[sample2,])

DgivenGXQ.Pred_ncf[sample1] &lt;- stats::predict(DgivenGXQ.Model.sample1,
    newdata = pred_data[sample1,], type="prob")[,2]
DgivenGXQ.Pred_ncf[sample2] &lt;- stats::predict(DgivenGXQ.Model.sample2,
    newdata = pred_data[sample2,], type="prob")[,2]

# IPOs for modelling E(Y_d | Q,g)
IPO_D0_ncf &lt;- (1-data[,D])/(1-DgivenGXQ.Pred_ncf)/mean((1-data[,D])/(1-DgivenGXQ.Pred_ncf))*
    (data[,Y]-YgivenGXQ.Pred_D0_ncf) + YgivenGXQ.Pred_D0_ncf
IPO_D1_ncf &lt;- data[,D]/DgivenGXQ.Pred_ncf/mean(data[,D]/DgivenGXQ.Pred_ncf)*
    (data[,Y]-YgivenGXQ.Pred_D1_ncf) + YgivenGXQ.Pred_D1_ncf

data_temp &lt;- data[,c(G,Q)]
data_temp$IPO_D0_ncf &lt;- IPO_D0_ncf
data_temp$IPO_D1_ncf &lt;- IPO_D1_ncf

message &lt;- utils::capture.output( Y0givenGQ.Model.sample1 &lt;-
    caret::train(stats::as.formula(paste("IPO_D0_ncf", paste(G,Q,sep="+"), sep="~")),
                 data=data_temp[sample1,], method="ranger",
                 trControl=caret::trainControl(method="cv"),
                 tuneGrid=expand.grid(mtry=1,splitrule=c("variance"),min.node.size=c(25,50))) )
message &lt;- utils::capture.output( Y0givenGQ.Model.sample2 &lt;-
    caret::train(stats::as.formula(paste("IPO_D0_ncf", paste(G,Q,sep="+"), sep="~")),
                 data=data_temp[sample2,], method="ranger",
                 trControl=caret::trainControl(method="cv"),
                 tuneGrid=expand.grid(mtry=1,splitrule=c("variance"),min.node.size=c(25,50))) )
message &lt;- utils::capture.output( Y1givenGQ.Model.sample1 &lt;-
    caret::train(stats::as.formula(paste("IPO_D1_ncf", paste(G,Q,sep="+"), sep="~")),
                 data=data_temp[sample1,], method="ranger",
                 trControl=caret::trainControl(method="cv"),
                 tuneGrid=expand.grid(mtry=1,splitrule=c("variance"),min.node.size=c(25,50))) )
message &lt;- utils::capture.output( Y1givenGQ.Model.sample2 &lt;-
    caret::train(stats::as.formula(paste("IPO_D1_ncf", paste(G,Q,sep="+"), sep="~")),
                 data=data_temp[sample2,], method="ranger",
                 trControl=caret::trainControl(method="cv"),
                 tuneGrid=expand.grid(mtry=1,splitrule=c("variance"),min.node.size=c(25,50))) )

Y0givenQ.Pred_G0 &lt;- Y0givenQ.Pred_G1 &lt;- Y1givenQ.Pred_G0 &lt;- Y1givenQ.Pred_G1 &lt;- rep(NA, nrow(data))

pred_data &lt;- data
pred_data[,G] &lt;- 1
# cross-fitting is used
Y0givenQ.Pred_G1[sample2] &lt;- stats::predict(Y0givenGQ.Model.sample1, newdata = pred_data[sample2,])
Y0givenQ.Pred_G1[sample1] &lt;- stats::predict(Y0givenGQ.Model.sample2, newdata = pred_data[sample1,])
Y1givenQ.Pred_G1[sample2] &lt;- stats::predict(Y1givenGQ.Model.sample1, newdata = pred_data[sample2,])
Y1givenQ.Pred_G1[sample1] &lt;- stats::predict(Y1givenGQ.Model.sample2, newdata = pred_data[sample1,])

pred_data &lt;- data
pred_data[,G] &lt;- 0
# cross-fitting is used
Y0givenQ.Pred_G0[sample2] &lt;- stats::predict(Y0givenGQ.Model.sample1, newdata = pred_data[sample2,])
Y0givenQ.Pred_G0[sample1] &lt;- stats::predict(Y0givenGQ.Model.sample2, newdata = pred_data[sample1,])
Y1givenQ.Pred_G0[sample2] &lt;- stats::predict(Y1givenGQ.Model.sample1, newdata = pred_data[sample2,])
Y1givenQ.Pred_G0[sample1] &lt;- stats::predict(Y1givenGQ.Model.sample2, newdata = pred_data[sample1,])

### Estimate E(D | Q,g')
data[,D] &lt;- as.factor(data[,D])
levels(data[,D]) &lt;- c("D0","D1")  # necessary for caret implementation of ranger

message &lt;- utils::capture.output( DgivenGQ.Model.sample1 &lt;-
    caret::train(stats::as.formula(paste(D, paste(G,Q,sep="+"), sep="~")),
                 data=data[sample1,], method="ranger",
                 trControl=caret::trainControl(method="cv", classProbs=TRUE),
                 tuneGrid=expand.grid(mtry=1,splitrule=c("gini"),min.node.size=c(25,50))) )
message &lt;- utils::capture.output( DgivenGQ.Model.sample2 &lt;-
    caret::train(stats::as.formula(paste(D, paste(G,Q,sep="+"), sep="~")),
                 data=data[sample2,], method="ranger",
                 trControl=caret::trainControl(method="cv", classProbs=TRUE),
                 tuneGrid=expand.grid(mtry=1,splitrule=c("gini"),min.node.size=c(25,50))) )

data[,D] &lt;- as.numeric(data[,D])-1

DgivenQ.Pred_G0 &lt;- DgivenQ.Pred_G1 &lt;- rep(NA, nrow(data))

pred_data &lt;- data
pred_data[,G] &lt;- 0
DgivenQ.Pred_G0[sample2] &lt;- stats::predict(DgivenGQ.Model.sample1,
    newdata = pred_data[sample2,], type="prob")[,2]
DgivenQ.Pred_G0[sample1] &lt;- stats::predict(DgivenGQ.Model.sample2,
    newdata = pred_data[sample1,], type="prob")[,2]

pred_data &lt;- data
pred_data[,G] &lt;- 1
DgivenQ.Pred_G1[sample2] &lt;- stats::predict(DgivenGQ.Model.sample1,
   newdata = pred_data[sample2,], type="prob")[,2]
DgivenQ.Pred_G1[sample1] &lt;- stats::predict(DgivenGQ.Model.sample2,
   newdata = pred_data[sample1,], type="prob")[,2]

### Estimate p_g(Q)=Pr(G=g | Q)
data[,G] &lt;- as.factor(data[,G])
levels(data[,G]) &lt;- c("G0","G1")  # necessary for caret implementation of ranger

message &lt;- utils::capture.output( GgivenQ.Model.sample1 &lt;-
    caret::train(stats::as.formula(paste(G, paste(Q,sep="+"), sep="~")),
                 data=data[sample1,], method="ranger",
                 trControl=caret::trainControl(method="cv", classProbs=TRUE),
                 tuneGrid=expand.grid(mtry=1,splitrule=c("gini"),min.node.size=c(25,50))) )
message &lt;- utils::capture.output( GgivenQ.Model.sample2 &lt;-
    caret::train(stats::as.formula(paste(G, paste(Q,sep="+"), sep="~")),
                 data=data[sample2,], method="ranger",
                 trControl=caret::trainControl(method="cv", classProbs=TRUE),
                 tuneGrid=expand.grid(mtry=1,splitrule=c("gini"),min.node.size=c(25,50))) )

data[,G] &lt;- as.numeric(data[,G])-1

GgivenQ.Pred &lt;- rep(NA, nrow(data))
GgivenQ.Pred[sample2] &lt;- stats::predict(GgivenQ.Model.sample1,
    newdata = data[sample2,], type="prob")[,2]
GgivenQ.Pred[sample1] &lt;- stats::predict(GgivenQ.Model.sample2,
    newdata = data[sample1,], type="prob")[,2]


results &lt;- cdgd1_manual(Y=Y,D=D,G=G,
                        YgivenGXQ.Pred_D0=YgivenGXQ.Pred_D0,
                        YgivenGXQ.Pred_D1=YgivenGXQ.Pred_D1,
                        DgivenGXQ.Pred=DgivenGXQ.Pred,
                        Y0givenQ.Pred_G0=Y0givenQ.Pred_G0,
                        Y0givenQ.Pred_G1=Y0givenQ.Pred_G1,
                        Y1givenQ.Pred_G0=Y1givenQ.Pred_G0,
                        Y1givenQ.Pred_G1=Y1givenQ.Pred_G1,
                        DgivenQ.Pred_G0=DgivenQ.Pred_G0,
                        DgivenQ.Pred_G1=DgivenQ.Pred_G1,
                        GgivenQ.Pred=GgivenQ.Pred,
                        data,alpha=0.05)

results
</code></pre>

<hr>
<h2 id='cdgd1_ml'>Perform conditional decomposition via machine learning</h2><span id='topic+cdgd1_ml'></span>

<h3>Description</h3>

<p>Perform conditional decomposition via machine learning
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cdgd1_ml(Y, D, G, X, Q, data, algorithm, alpha = 0.05, trim1 = 0, trim2 = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cdgd1_ml_+3A_y">Y</code></td>
<td>
<p>Outcome. The name of a numeric variable (can be binary and take values of 0 and 1).</p>
</td></tr>
<tr><td><code id="cdgd1_ml_+3A_d">D</code></td>
<td>
<p>Treatment status. The name of a binary numeric variable taking values of 0 and 1.</p>
</td></tr>
<tr><td><code id="cdgd1_ml_+3A_g">G</code></td>
<td>
<p>Advantaged group membership. The name of a binary numeric variable taking values of 0 and 1.</p>
</td></tr>
<tr><td><code id="cdgd1_ml_+3A_x">X</code></td>
<td>
<p>Confounders. A vector of variables names.</p>
</td></tr>
<tr><td><code id="cdgd1_ml_+3A_q">Q</code></td>
<td>
<p>Conditional set. A vector of names of numeric variables.</p>
</td></tr>
<tr><td><code id="cdgd1_ml_+3A_data">data</code></td>
<td>
<p>A data frame.</p>
</td></tr>
<tr><td><code id="cdgd1_ml_+3A_algorithm">algorithm</code></td>
<td>
<p>The ML algorithm for modelling. &quot;nnet&quot; for neural network, &quot;ranger&quot; for random forests, &quot;gbm&quot; for generalized boosted models.</p>
</td></tr>
<tr><td><code id="cdgd1_ml_+3A_alpha">alpha</code></td>
<td>
<p>1-alpha confidence interval.</p>
</td></tr>
<tr><td><code id="cdgd1_ml_+3A_trim1">trim1</code></td>
<td>
<p>Threshold for trimming the propensity score. When trim1=a, individuals with propensity scores lower than a or higher than 1-a will be dropped.</p>
</td></tr>
<tr><td><code id="cdgd1_ml_+3A_trim2">trim2</code></td>
<td>
<p>Threshold for trimming the G given Q predictions. When trim2=a, individuals with G given Q predictions lower than a or higher than 1-a will be dropped.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataframe of estimates.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># This example will take a minute to run.

data(exp_data)

set.seed(1)

results &lt;- cdgd1_ml(
Y="outcome",
D="treatment",
G="group_a",
X="confounder",
Q="Q",
data=exp_data,
algorithm="gbm")

results
</code></pre>

<hr>
<h2 id='cdgd1_pa'>Perform conditional decomposition via parametric models</h2><span id='topic+cdgd1_pa'></span>

<h3>Description</h3>

<p>Perform conditional decomposition via parametric models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cdgd1_pa(Y, D, G, X, Q, data, alpha = 0.05, trim1 = 0, trim2 = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cdgd1_pa_+3A_y">Y</code></td>
<td>
<p>Outcome. The name of a numeric variable (can be binary and take values of 0 and 1).</p>
</td></tr>
<tr><td><code id="cdgd1_pa_+3A_d">D</code></td>
<td>
<p>Treatment status. The name of a binary numeric variable taking values of 0 and 1.</p>
</td></tr>
<tr><td><code id="cdgd1_pa_+3A_g">G</code></td>
<td>
<p>Advantaged group membership. The name of a binary numeric variable taking values of 0 and 1.</p>
</td></tr>
<tr><td><code id="cdgd1_pa_+3A_x">X</code></td>
<td>
<p>Confounders. A vector of variable names.</p>
</td></tr>
<tr><td><code id="cdgd1_pa_+3A_q">Q</code></td>
<td>
<p>Conditional set. A vector of variable names.</p>
</td></tr>
<tr><td><code id="cdgd1_pa_+3A_data">data</code></td>
<td>
<p>A data frame.</p>
</td></tr>
<tr><td><code id="cdgd1_pa_+3A_alpha">alpha</code></td>
<td>
<p>1-alpha confidence interval.</p>
</td></tr>
<tr><td><code id="cdgd1_pa_+3A_trim1">trim1</code></td>
<td>
<p>Threshold for trimming the propensity score. When trim1=a, individuals with propensity scores lower than a or higher than 1-a will be dropped.</p>
</td></tr>
<tr><td><code id="cdgd1_pa_+3A_trim2">trim2</code></td>
<td>
<p>Threshold for trimming the G given Q predictions. When trim2=a, individuals with G given Q predictions lower than a or higher than 1-a will be dropped.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataframe of estimates.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(exp_data)

results &lt;- cdgd1_pa(
Y="outcome",
D="treatment",
G="group_a",
X="confounder",
Q="Q",
data=exp_data)

results
</code></pre>

<hr>
<h2 id='exp_data'>Simulated example data</h2><span id='topic+exp_data'></span>

<h3>Description</h3>

<p>Simulated example data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(exp_data)
</code></pre>


<h3>Format</h3>

<p>An object of class <code>data.frame</code> with 1000 rows and 5 columns.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
