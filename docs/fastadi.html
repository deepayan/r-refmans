<!DOCTYPE html><html><head><title>Help for package fastadi</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {fastadi}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#fastadi-package'><p>fastadi: Self-Tuning Data Adaptive Matrix Imputation</p></a></li>
<li><a href='#adaptive_imputation'><p>Create an Adaptive Imputation object</p></a></li>
<li><a href='#adaptive_impute'><p>AdaptiveImpute</p></a></li>
<li><a href='#adaptive_initialize'><p>AdaptiveInitialize</p></a></li>
<li><a href='#citation_impute'><p>CitationImpute</p></a></li>
<li><a href='#masked_approximation_impl'><p>Expand an SVD only at observed values of a sparse matrix</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Self-Tuning Data Adaptive Matrix Imputation</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.1</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements the AdaptiveImpute matrix completion
    algorithm of 'Intelligent Initialization and Adaptive Thresholding for
    Iterative Matrix Completion',
    <a href="https://amstat.tandfonline.com/doi/abs/10.1080/10618600.2018.1518238">https://amstat.tandfonline.com/doi/abs/10.1080/10618600.2018.1518238</a>.
    AdaptiveImpute is useful for embedding sparsely observed matrices,
    often out performs competing matrix completion algorithms, and
    self-tunes its hyperparameter, making usage easy.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/RoheLab/fastadi">https://github.com/RoheLab/fastadi</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/RoheLab/fastadi/issues">https://github.com/RoheLab/fastadi/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>LRMF3, Matrix, R (&ge; 3.1)</td>
</tr>
<tr>
<td>Imports:</td>
<td>ellipsis, glue, logger, methods, Rcpp, RSpectra,</td>
</tr>
<tr>
<td>Suggests:</td>
<td>invertiforms, covr, knitr, rmarkdown, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-09-07 04:52:31 UTC; alex</td>
</tr>
<tr>
<td>Author:</td>
<td>Alex Hayes <a href="https://orcid.org/0000-0002-4985-5160"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre, cph],
  Juhee Cho [aut],
  Donggyu Kim [aut],
  Karl Rohe [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Alex Hayes &lt;alexpghayes@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-09-07 06:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='fastadi-package'>fastadi: Self-Tuning Data Adaptive Matrix Imputation</h2><span id='topic+fastadi'></span><span id='topic+fastadi-package'></span>

<h3>Description</h3>

<p>Implements the AdaptiveImpute matrix completion algorithm of 'Intelligent Initialization and Adaptive Thresholding for Iterative Matrix Completion', &lt;https://amstat.tandfonline.com/doi/abs/10.1080/10618600.2018.1518238&gt;. AdaptiveImpute is useful for embedding sparsely observed matrices, often out performs competing matrix completion algorithms, and self-tunes its hyperparameter, making usage easy.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Alex Hayes <a href="mailto:alexpghayes@gmail.com">alexpghayes@gmail.com</a> (<a href="https://orcid.org/0000-0002-4985-5160">ORCID</a>) [copyright holder]
</p>
<p>Authors:
</p>

<ul>
<li><p> Juhee Cho
</p>
</li>
<li><p> Donggyu Kim
</p>
</li>
<li><p> Karl Rohe
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/RoheLab/fastadi">https://github.com/RoheLab/fastadi</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/RoheLab/fastadi/issues">https://github.com/RoheLab/fastadi/issues</a>
</p>
</li></ul>


<hr>
<h2 id='adaptive_imputation'>Create an Adaptive Imputation object</h2><span id='topic+adaptive_imputation'></span>

<h3>Description</h3>

<p><code>adaptive_imputation</code> objects are a subclass of
<code><a href="LRMF3.html#topic+svd_like">LRMF3::svd_like()</a></code>, with an additional field <code>alpha</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adaptive_imputation(u, d, v, alpha, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="adaptive_imputation_+3A_u">u</code></td>
<td>
<p>A <em>matrix</em> &quot;left singular-ish&quot; vectors.</p>
</td></tr>
<tr><td><code id="adaptive_imputation_+3A_d">d</code></td>
<td>
<p>A <em>vector</em> of &quot;singular-ish&quot; values.</p>
</td></tr>
<tr><td><code id="adaptive_imputation_+3A_v">v</code></td>
<td>
<p>A <em>matrix</em> of &quot;right singular-ish&quot; vectors.</p>
</td></tr>
<tr><td><code id="adaptive_imputation_+3A_alpha">alpha</code></td>
<td>
<p>Value of <code>alpha</code> after final iteration.</p>
</td></tr>
<tr><td><code id="adaptive_imputation_+3A_...">...</code></td>
<td>
<p>Optional additional items to pass to the constructor.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <code>adaptive_imputation</code> object.
</p>

<hr>
<h2 id='adaptive_impute'>AdaptiveImpute</h2><span id='topic+adaptive_impute'></span><span id='topic+adaptive_impute.sparseMatrix'></span><span id='topic+adaptive_impute.LRMF'></span>

<h3>Description</h3>

<p>An implementation of the <code>AdaptiveImpute</code> algorithm for matrix completion
for sparse matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adaptive_impute(
  X,
  rank,
  ...,
  initialization = c("svd", "adaptive-initialize", "approximate"),
  max_iter = 200L,
  check_interval = 1L,
  epsilon = 1e-07,
  additional = NULL
)

## S3 method for class 'sparseMatrix'
adaptive_impute(
  X,
  rank,
  ...,
  initialization = c("svd", "adaptive-initialize", "approximate"),
  additional = NULL
)

## S3 method for class 'LRMF'
adaptive_impute(
  X,
  rank,
  ...,
  epsilon = 1e-07,
  max_iter = 200L,
  check_interval = 1L
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="adaptive_impute_+3A_x">X</code></td>
<td>
<p>A sparse matrix of <code><a href="Matrix.html#topic+sparseMatrix">Matrix::sparseMatrix()</a></code> class.</p>
</td></tr>
<tr><td><code id="adaptive_impute_+3A_rank">rank</code></td>
<td>
<p>Desired rank (integer) to use in the low rank approximation.
Must be at least <code>2L</code> and at most the rank of <code>X</code>. Note that the rank
of <code>X</code> is typically unobserved and computations may be unstable or
even fail when <code>rank</code> is near or exceeds this threshold.</p>
</td></tr>
<tr><td><code id="adaptive_impute_+3A_...">...</code></td>
<td>
<p>Unused additional arguments.</p>
</td></tr>
<tr><td><code id="adaptive_impute_+3A_initialization">initialization</code></td>
<td>
<p>How to initialize the low rank approximation.
Options are:
</p>

<ul>
<li> <p><code>"svd"</code> (default). In the initialization step, this treats
unobserved values as zeroes.
</p>
</li>
<li> <p><code>"adaptive-initialize"</code>. In the initialization step, this treats
unobserved values as actually unobserved. However, the current
<code>AdaptiveInitialize</code> implementation relies on dense matrix
computations that are only suitable for relatively small matrices.
</p>
</li>
<li> <p><code>"approximate"</code>. An approximate variant of <code>AdaptiveInitialize</code>
that is less computationally expensive. See <code>adaptive_initialize</code>
for details.
</p>
</li></ul>

<p>Note that initialization matters as <code>AdaptiveImpute</code> optimizes
a non-convex objective. The current theory shows that initializing
with <code>AdaptiveInitialize</code> leads to a consistent estimator, but it
isn't know if this is the case for SVD initialization. Empirically
we have found that SVD initialization works well nonetheless.</p>
</td></tr>
<tr><td><code id="adaptive_impute_+3A_max_iter">max_iter</code></td>
<td>
<p>Maximum number of iterations to perform (integer). Defaults
to <code>200L</code>. In practice 10 or so iterations will get you a decent
approximation to use in exploratory analysis, and and 50-100 will get
you most of the way to convergence. Must be at least <code>1L</code>.</p>
</td></tr>
<tr><td><code id="adaptive_impute_+3A_check_interval">check_interval</code></td>
<td>
<p>Integer specifying how often to perform convergence
checks. Defaults to <code>1L</code>. In practice, check for convergence requires
a norm calculation that is expensive for large matrices and decreasing
the frequency of convergence checks will reduce computation time. Can
also be set to <code>NULL</code>, which case <code>max_iter</code> iterations of the algorithm
will occur with no possibility of stopping due to small relative change
in the imputed matrix. In this case <code>delta</code> will be reported as <code>Inf</code>.</p>
</td></tr>
<tr><td><code id="adaptive_impute_+3A_epsilon">epsilon</code></td>
<td>
<p>Convergence criteria, measured in terms of relative change
in Frobenius norm of the full imputed matrix. Defaults to <code>1e-7</code>.</p>
</td></tr>
<tr><td><code id="adaptive_impute_+3A_additional">additional</code></td>
<td>
<p>Ignored except when <code>alpha_method = "approximate"</code>
in which case it controls the precise of the approximation to <code>alpha</code>.
The approximate computation of <code>alpha</code> will always understand <code>alpha</code>,
but the approximation will be better for larger values of <code>additional</code>.
We recommend making <code>additional</code> as large as computationally tolerable.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A low rank matrix factorization represented by an
<code><a href="#topic+adaptive_imputation">adaptive_imputation()</a></code> object.
</p>


<h3>References</h3>


<ol>
<li><p> Cho, Juhee, Donggyu Kim, and Karl Rohe. “Asymptotic Theory for
Estimating the Singular Vectors and Values of a Partially-Observed
Low Rank Matrix with Noise.” Statistica Sinica, 2018.
https://doi.org/10.5705/ss.202016.0205.
</p>
</li>
<li><p> ———. “Intelligent Initialization and Adaptive Thresholding for
Iterative Matrix Completion: Some Statistical and Algorithmic Theory for
Adaptive-Impute.” Journal of Computational and Graphical Statistics 28,
no. 2 (April 3, 2019): 323–33.
https://doi.org/10.1080/10618600.2018.1518238.
</p>
</li></ol>



<h3>Examples</h3>

<pre><code class='language-R'>
mf &lt;- adaptive_impute(ml100k, rank = 3L, max_iter = 5L, check_interval = NULL)
mf

</code></pre>

<hr>
<h2 id='adaptive_initialize'>AdaptiveInitialize</h2><span id='topic+adaptive_initialize'></span><span id='topic+adaptive_initialize.sparseMatrix'></span>

<h3>Description</h3>

<p>An implementation of the <code>AdaptiveInitialize</code> algorithm for
matrix imputation for sparse matrices. At the moment the implementation
is only suitable for small matrices with on the order of thousands
of rows and columns at most.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adaptive_initialize(
  X,
  rank,
  ...,
  p_hat = NULL,
  alpha_method = c("exact", "approximate"),
  additional = NULL
)

## S3 method for class 'sparseMatrix'
adaptive_initialize(
  X,
  rank,
  ...,
  p_hat = NULL,
  alpha_method = c("exact", "approximate"),
  additional = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="adaptive_initialize_+3A_x">X</code></td>
<td>
<p>A sparse matrix of <code>sparseMatrix</code> class. Explicit (observed)
zeroes in <code>X</code> can be dropped for</p>
</td></tr>
<tr><td><code id="adaptive_initialize_+3A_rank">rank</code></td>
<td>
<p>Desired rank (integer) to use in the low rank approximation.
Must be at least <code>2L</code> and at most the rank of <code>X</code>.</p>
</td></tr>
<tr><td><code id="adaptive_initialize_+3A_...">...</code></td>
<td>
<p>Ignored.</p>
</td></tr>
<tr><td><code id="adaptive_initialize_+3A_p_hat">p_hat</code></td>
<td>
<p>The portion of <code>X</code> that is observed. Defaults to <code>NULL</code>,
in which case <code>p_hat</code> is set to the number of observed elements of
<code>X</code>. Primarily for internal use in <code><a href="#topic+citation_impute">citation_impute()</a></code> or
advanced users.</p>
</td></tr>
<tr><td><code id="adaptive_initialize_+3A_alpha_method">alpha_method</code></td>
<td>
<p>Either <code>"exact"</code> or <code>"approximate"</code>, defaulting to
<code>"exact"</code>. <code>"exact"</code> is computationally expensive and requires taking
a complete SVD of matrix of size <code>nrow(X)</code> x <code>nrow(X)</code>, and matches
the <code>AdaptiveInitialize</code> algorithm exactly. <code>"approximate"</code>
departs from the <code>AdaptiveInitialization</code> algorithm to compute
a truncated SVD of rank <code>rank</code> + <code>additional</code> instead of a complete
SVD. This reduces computational burden, but the resulting estimates
of singular-ish values will not be penalized as much as in the
<code>AdaptiveInitialize</code> algorithm.</p>
</td></tr>
<tr><td><code id="adaptive_initialize_+3A_additional">additional</code></td>
<td>
<p>Ignored except when <code>alpha_method = "approximate"</code>
in which case it controls the precise of the approximation to <code>alpha</code>.
The approximate computation of <code>alpha</code> will always understand <code>alpha</code>,
but the approximation will be better for larger values of <code>additional</code>.
We recommend making <code>additional</code> as large as computationally tolerable.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A low rank matrix factorization represented by an
<code><a href="#topic+adaptive_imputation">adaptive_imputation()</a></code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
mf &lt;- adaptive_initialize(
  ml100k,
  rank = 3,
  alpha_method = "approximate",
  additional = 2
)

mf

</code></pre>

<hr>
<h2 id='citation_impute'>CitationImpute</h2><span id='topic+citation_impute'></span><span id='topic+citation_impute.sparseMatrix'></span><span id='topic+citation_impute.LRMF'></span>

<h3>Description</h3>

<p>An implementation of the <code>AdaptiveImpute</code> algorithm using efficient
sparse matrix computations, specialized for the case when missing
values in the upper triangle are taken to be <em>explicitly observed</em>
zeros, as opposed to missing values. This is primarily
useful for spectral decompositions of adjacency matrices of graphs
with (near) tree structure, such as citation networks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>citation_impute(
  X,
  rank,
  ...,
  initialization = c("svd", "adaptive-initialize", "approximate"),
  max_iter = 200L,
  check_interval = 1L,
  epsilon = 1e-07,
  additional = NULL
)

## S3 method for class 'sparseMatrix'
citation_impute(
  X,
  rank,
  ...,
  initialization = c("svd", "adaptive-initialize", "approximate"),
  additional = NULL
)

## S3 method for class 'LRMF'
citation_impute(
  X,
  rank,
  ...,
  epsilon = 1e-07,
  max_iter = 200L,
  check_interval = 1L
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="citation_impute_+3A_x">X</code></td>
<td>
<p>A <em>square</em> sparse matrix of <code><a href="Matrix.html#topic+sparseMatrix">Matrix::sparseMatrix()</a></code> class.
Implicit zeros in the upper triangle of this matrix are considered
observed and predictions on these elements contribute to the
objective function minimized by <code>AdaptiveImpute</code>.</p>
</td></tr>
<tr><td><code id="citation_impute_+3A_rank">rank</code></td>
<td>
<p>Desired rank (integer) to use in the low rank approximation.
Must be at least <code>2L</code> and at most the rank of <code>X</code>. Note that the rank
of <code>X</code> is typically unobserved and computations may be unstable or
even fail when <code>rank</code> is near or exceeds this threshold.</p>
</td></tr>
<tr><td><code id="citation_impute_+3A_...">...</code></td>
<td>
<p>Unused additional arguments.</p>
</td></tr>
<tr><td><code id="citation_impute_+3A_initialization">initialization</code></td>
<td>
<p>How to initialize the low rank approximation.
Options are:
</p>

<ul>
<li> <p><code>"svd"</code> (default). In the initialization step, this treats
unobserved values as zeroes.
</p>
</li>
<li> <p><code>"adaptive-initialize"</code>. In the initialization step, this treats
unobserved values as actually unobserved. However, the current
<code>AdaptiveInitialize</code> implementation relies on dense matrix
computations that are only suitable for relatively small matrices.
</p>
</li>
<li> <p><code>"approximate"</code>. An approximate variant of <code>AdaptiveInitialize</code>
that is less computationally expensive. See <code>adaptive_initialize</code>
for details.
</p>
</li></ul>

<p>Note that initialization matters as <code>AdaptiveImpute</code> optimizes
a non-convex objective. The current theory shows that initializing
with <code>AdaptiveInitialize</code> leads to a consistent estimator, but it
isn't know if this is the case for SVD initialization. Empirically
we have found that SVD initialization works well nonetheless.</p>
</td></tr>
<tr><td><code id="citation_impute_+3A_max_iter">max_iter</code></td>
<td>
<p>Maximum number of iterations to perform (integer). Defaults
to <code>200L</code>. In practice 10 or so iterations will get you a decent
approximation to use in exploratory analysis, and and 50-100 will get
you most of the way to convergence. Must be at least <code>1L</code>.</p>
</td></tr>
<tr><td><code id="citation_impute_+3A_check_interval">check_interval</code></td>
<td>
<p>Integer specifying how often to perform convergence
checks. Defaults to <code>1L</code>. In practice, check for convergence requires
a norm calculation that is expensive for large matrices and decreasing
the frequency of convergence checks will reduce computation time. Can
also be set to <code>NULL</code>, which case <code>max_iter</code> iterations of the algorithm
will occur with no possibility of stopping due to small relative change
in the imputed matrix. In this case <code>delta</code> will be reported as <code>Inf</code>.</p>
</td></tr>
<tr><td><code id="citation_impute_+3A_epsilon">epsilon</code></td>
<td>
<p>Convergence criteria, measured in terms of relative change
in Frobenius norm of the full imputed matrix. Defaults to <code>1e-7</code>.</p>
</td></tr>
<tr><td><code id="citation_impute_+3A_additional">additional</code></td>
<td>
<p>Ignored except when <code>alpha_method = "approximate"</code>
in which case it controls the precise of the approximation to <code>alpha</code>.
The approximate computation of <code>alpha</code> will always understand <code>alpha</code>,
but the approximation will be better for larger values of <code>additional</code>.
We recommend making <code>additional</code> as large as computationally tolerable.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If OpenMP is available, <code>citation_impute</code> will automatically
use <code>getOption("Ncpus", 1L)</code> OpenMP threads to parallelize some
key computations. Note that some computations are performed with
the Armadillo C++ linear algebra library and may also be parallelized
dependent on your BLAS and LAPACK installations and configurations.
</p>


<h3>Value</h3>

<p>A low rank matrix factorization represented by an
<code><a href="#topic+adaptive_imputation">adaptive_imputation()</a></code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# create a (binary) square sparse matrix to demonstrate on

set.seed(887)

n &lt;- 10
A &lt;- rsparsematrix(n, n, 0.1, rand.x = NULL)

mf &lt;- citation_impute(A, rank = 3L, max_iter = 1L, check_interval = NULL)
mf


</code></pre>

<hr>
<h2 id='masked_approximation_impl'>Expand an SVD only at observed values of a sparse matrix</h2><span id='topic+masked_approximation_impl'></span>

<h3>Description</h3>

<p>TODO: describe what it looks like for dimensions to match up between
<code>s</code> and <code>mask</code>. See <code>vignette("sparse-computations")</code> for mathematical
details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>masked_approximation_impl(U, V, row, col)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="masked_approximation_impl_+3A_u">U</code></td>
<td>
<p>Low-rank matrix of left singular-ish vectors.</p>
</td></tr>
<tr><td><code id="masked_approximation_impl_+3A_v">V</code></td>
<td>
<p>Low-rank matrix of right singular-ish vectors.</p>
</td></tr>
<tr><td><code id="masked_approximation_impl_+3A_row">row</code></td>
<td>
<p>Zero-based row indices of observed elements.</p>
</td></tr>
<tr><td><code id="masked_approximation_impl_+3A_col">col</code></td>
<td>
<p>Zero-based col indices of observed elements.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The idea is to populate <code>U</code>, <code>d</code> and <code>V</code> with using the
elements of an SVD-like list. You can generate <code>row</code> and <code>col</code>
most easily from a sparse masking Matrix (Matrix package),
coercing to triplet format, and extracting <code>mask@i</code> for <code>row</code>
and <code>mask@j</code> for column.
</p>


<h3>Value</h3>

<p>A sparse matrix representing the low-rank reconstruction
from <code>U</code>, <code>d</code> and <code>V</code>, only at the index pairs indicated by
<code>row</code> and <code>col</code>.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
