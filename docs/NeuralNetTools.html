<!DOCTYPE html><html><head><title>Help for package NeuralNetTools</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {NeuralNetTools}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bias_lines'><p>Plot connection weights for bias lines</p></a></li>
<li><a href='#bias_points'><p>Plot bias points</p></a></li>
<li><a href='#garson'><p>Variable importance using Garson's algorithm</p></a></li>
<li><a href='#get_ys'><p>Get y locations for layers in <code>plotnet</code></p></a></li>
<li><a href='#layer_lines'><p>Plot connection weights</p></a></li>
<li><a href='#layer_points'><p>Plot neural network nodes</p></a></li>
<li><a href='#lekgrps'><p>Create optional barplot for <code>lekprofile</code> groups</p></a></li>
<li><a href='#lekprofile'><p>Sensitivity analysis using Lek's profile method</p></a></li>
<li><a href='#neuraldat'><p>Simulated dataset for function examples</p></a></li>
<li><a href='#neuralskips'><p>Get weights for the skip layer in a neural network</p></a></li>
<li><a href='#neuralweights'><p>Get weights for a neural network</p></a></li>
<li><a href='#olden'><p>Variable importance using connection weights</p></a></li>
<li><a href='#plotnet'><p>Plot a neural network model</p></a></li>
<li><a href='#pred_sens'><p>Predicted values for Lek profile method</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Visualization and Analysis Tools for Neural Networks</td>
</tr>
<tr>
<td>Version:</td>
<td>1.5.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-01-06</td>
</tr>
<tr>
<td>Author:</td>
<td>Marcus W. Beck [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Marcus W. Beck &lt;mbafs2012@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Visualization and analysis tools to aid in the interpretation of
    neural network models.  Functions are available for plotting,
    quantifying variable importance, conducting a sensitivity analysis, and
    obtaining a simple list of model weights.</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/fawda123/NeuralNetTools/issues">https://github.com/fawda123/NeuralNetTools/issues</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://creativecommons.org/publicdomain/zero/1.0/legalcode">CC0</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Imports:</td>
<td>ggplot2 (&ge; 2.0.0), nnet, reshape2, scales, tidyr</td>
</tr>
<tr>
<td>Suggests:</td>
<td>caret, neuralnet, RSNNS, knitr, rmarkdown</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.1.1)</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-01-06 13:59:38 UTC; MarcusBeck</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-01-06 15:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='bias_lines'>Plot connection weights for bias lines</h2><span id='topic+bias_lines'></span>

<h3>Description</h3>

<p>Plot connection weights for bias lines in <code><a href="#topic+plotnet">plotnet</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bias_lines(
  bias_x,
  bias_y,
  mod_in,
  nid,
  rel_rsc,
  all_out,
  pos_col,
  neg_col,
  struct,
  y_names,
  x_range,
  y_range,
  layer_x,
  line_stag,
  max_sp
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bias_lines_+3A_bias_x">bias_x</code></td>
<td>
<p>numeric vector x axis locations for bias lines</p>
</td></tr>
<tr><td><code id="bias_lines_+3A_bias_y">bias_y</code></td>
<td>
<p>numeric vector y axis locations for bias lines</p>
</td></tr>
<tr><td><code id="bias_lines_+3A_mod_in">mod_in</code></td>
<td>
<p>neural network model object</p>
</td></tr>
<tr><td><code id="bias_lines_+3A_nid">nid</code></td>
<td>
<p>logical value indicating if neural interpretation diagram is plotted, default <code>TRUE</code></p>
</td></tr>
<tr><td><code id="bias_lines_+3A_rel_rsc">rel_rsc</code></td>
<td>
<p>numeric indicating the scaling range for the width of connection weights in a neural interpretation diagram. Default is <code>NULL</code> for no rescaling.</p>
</td></tr>
<tr><td><code id="bias_lines_+3A_all_out">all_out</code></td>
<td>
<p>chr string indicating names of response variables for which connections are plotted, default all</p>
</td></tr>
<tr><td><code id="bias_lines_+3A_pos_col">pos_col</code></td>
<td>
<p>chr string indicating color of positive connection weights, default <code>'black'</code></p>
</td></tr>
<tr><td><code id="bias_lines_+3A_neg_col">neg_col</code></td>
<td>
<p>chr string indicating color of negative connection weights, default <code>'grey'</code></p>
</td></tr>
<tr><td><code id="bias_lines_+3A_struct">struct</code></td>
<td>
<p>numeric vector for network structure</p>
</td></tr>
<tr><td><code id="bias_lines_+3A_y_names">y_names</code></td>
<td>
<p>chr string for names of output variables</p>
</td></tr>
<tr><td><code id="bias_lines_+3A_x_range">x_range</code></td>
<td>
<p>numeric of x axis range for base plot</p>
</td></tr>
<tr><td><code id="bias_lines_+3A_y_range">y_range</code></td>
<td>
<p>numeric of x axis range for base plot</p>
</td></tr>
<tr><td><code id="bias_lines_+3A_layer_x">layer_x</code></td>
<td>
<p>numeric indicating locations of layers on x axis</p>
</td></tr>
<tr><td><code id="bias_lines_+3A_line_stag">line_stag</code></td>
<td>
<p>numeric value that specifies distance of connection weights from nodes</p>
</td></tr>
<tr><td><code id="bias_lines_+3A_max_sp">max_sp</code></td>
<td>
<p>logical indicating if space is maximized in plot</p>
</td></tr>
</table>

<hr>
<h2 id='bias_points'>Plot bias points</h2><span id='topic+bias_points'></span>

<h3>Description</h3>

<p>Plot bias points in <code><a href="#topic+plotnet">plotnet</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bias_points(
  bias_x,
  bias_y,
  layer_name,
  node_labs,
  x_range,
  y_range,
  circle_cex,
  cex_val,
  bord_col,
  circle_col
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bias_points_+3A_bias_x">bias_x</code></td>
<td>
<p>numeric vector of values for x locations</p>
</td></tr>
<tr><td><code id="bias_points_+3A_bias_y">bias_y</code></td>
<td>
<p>numeric vector for y location</p>
</td></tr>
<tr><td><code id="bias_points_+3A_layer_name">layer_name</code></td>
<td>
<p>string indicating text to put in node</p>
</td></tr>
<tr><td><code id="bias_points_+3A_node_labs">node_labs</code></td>
<td>
<p>logical indicating of node labels are included</p>
</td></tr>
<tr><td><code id="bias_points_+3A_x_range">x_range</code></td>
<td>
<p>numeric of x axis range for base plot</p>
</td></tr>
<tr><td><code id="bias_points_+3A_y_range">y_range</code></td>
<td>
<p>numeric of y axis range for base plot</p>
</td></tr>
<tr><td><code id="bias_points_+3A_circle_cex">circle_cex</code></td>
<td>
<p>numeric value indicating size of nodes, default 5</p>
</td></tr>
<tr><td><code id="bias_points_+3A_cex_val">cex_val</code></td>
<td>
<p>numeric value indicating size of text labels, default 1</p>
</td></tr>
<tr><td><code id="bias_points_+3A_bord_col">bord_col</code></td>
<td>
<p>chr string indicating border color around nodes, default <code>'lightblue'</code></p>
</td></tr>
<tr><td><code id="bias_points_+3A_circle_col">circle_col</code></td>
<td>
<p>chr string indicating color of nodes</p>
</td></tr>
</table>

<hr>
<h2 id='garson'>Variable importance using Garson's algorithm</h2><span id='topic+garson'></span><span id='topic+garson.default'></span><span id='topic+garson.numeric'></span><span id='topic+garson.nnet'></span><span id='topic+garson.mlp'></span><span id='topic+garson.nn'></span><span id='topic+garson.train'></span>

<h3>Description</h3>

<p>Relative importance of input variables in neural networks using Garson's algorithm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>garson(mod_in, ...)

## Default S3 method:
garson(
  mod_in,
  x_names,
  y_names,
  bar_plot = TRUE,
  x_lab = NULL,
  y_lab = NULL,
  ...
)

## S3 method for class 'numeric'
garson(mod_in, struct, ...)

## S3 method for class 'nnet'
garson(mod_in, ...)

## S3 method for class 'mlp'
garson(mod_in, ...)

## S3 method for class 'nn'
garson(mod_in, ...)

## S3 method for class 'train'
garson(mod_in, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="garson_+3A_mod_in">mod_in</code></td>
<td>
<p>input model object or a list of model weights as returned from <code><a href="#topic+neuralweights">neuralweights</a></code> if using the default method</p>
</td></tr>
<tr><td><code id="garson_+3A_...">...</code></td>
<td>
<p>arguments passed to other methods</p>
</td></tr>
<tr><td><code id="garson_+3A_x_names">x_names</code></td>
<td>
<p>chr string of input variable names, obtained from the model object</p>
</td></tr>
<tr><td><code id="garson_+3A_y_names">y_names</code></td>
<td>
<p>chr string of response variable names, obtained from the model object</p>
</td></tr>
<tr><td><code id="garson_+3A_bar_plot">bar_plot</code></td>
<td>
<p>logical indicating if a <code>ggplot</code> object is returned (default <code>T</code>), otherwise numeric values are returned</p>
</td></tr>
<tr><td><code id="garson_+3A_x_lab">x_lab</code></td>
<td>
<p>chr string of alternative names to be used for explanatory variables in the figure, default is taken from <code>mod_in</code></p>
</td></tr>
<tr><td><code id="garson_+3A_y_lab">y_lab</code></td>
<td>
<p>chr string of alternative name to be used for the y-axis in the figure</p>
</td></tr>
<tr><td><code id="garson_+3A_struct">struct</code></td>
<td>
<p>numeric vector equal in length to the number of layers in the network.  Each number indicates the number of nodes in each layer starting with the input and ending with the output.  An arbitrary number of hidden layers can be included.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The weights that connect variables in a neural network are partially analogous to parameter coefficients in a standard regression model and can be used to describe relationships between variables. The weights dictate the relative influence of information that is processed in the network such that input variables that are not relevant in their correlation with a response variable are suppressed by the weights. The opposite effect is seen for weights assigned to explanatory variables that have strong positive or negative associations with a response variable. An obvious difference between a neural network and a regression model is that the number of weights is excessive in the former case. This characteristic is advantageous in that it makes neural networks very flexible for modeling non-linear functions with multiple interactions, although interpretation of the effects of specific variables is of course challenging.
</p>
<p>A method described in Garson 1991 (also see Goh 1995) identifies the relative importance of explanatory variables for a single response variables in a supervised neural network by deconstructing the model weights. The relative importance (or strength of association) of a specific explanatory variable for the response variable can be determined by identifying all weighted connections between the nodes of interest. That is, all weights connecting the specific input node that pass through the hidden layer to the response variable are identified. This is repeated for all other explanatory variables until a list of all weights that are specific to each input variable is obtained. The connections are tallied for each input node and scaled relative to all other inputs. A single value is obtained for each explanatory variable that describes the relationship with the response variable in the model (see the appendix in Goh 1995 for a more detailed description). The original algorithm indicates relative importance as the absolute magnitude from zero to one such the direction of the response cannot be determined.  
</p>
<p>Misleading results may be produced if the neural network was created with a skip-layer using <code>skip = TRUE</code> with the <code><a href="nnet.html#topic+nnet">nnet</a></code> or <code><a href="caret.html#topic+train">train</a></code> functions.  Garson's algorithm does not describe the effects of skip layer connections on estimates of variable importance.  As such, these values are removed prior to estimating variable importance.  
</p>
<p>The algorithm currently only works for neural networks with one hidden layer and one response variable.
</p>


<h3>Value</h3>

<p>A <code><a href="ggplot2.html#topic+ggplot">ggplot</a></code> object for plotting if <code>bar_plot = FALSE</code>, otherwise a <code>data.frame</code> of relative importance values for each input variable.  The default aesthetics for <code><a href="ggplot2.html#topic+ggplot">ggplot</a></code> can be further modified, as shown with the examples.
</p>


<h3>References</h3>

<p>Beck, M.W. 2018. NeuralNetTools: Visualization and Analysis Tools for Neural Networks. Journal of Statistical Software. 85(11):1-20.
</p>
<p>Garson, G.D. 1991. Interpreting neural network connection weights. Artificial Intelligence Expert. 6(4):46-51.
</p>
<p>Goh, A.T.C. 1995. Back-propagation neural networks for modeling complex systems. Artificial Intelligence in Engineering. 9(3):143-151.
</p>
<p>Olden, J.D., Jackson, D.A. 2002. Illuminating the 'black-box': a randomization approach for understanding variable contributions in artificial neural networks. Ecological Modelling. 154:135-150.
</p>
<p>Olden, J.D., Joy, M.K., Death, R.G. 2004. An accurate comparison of methods for quantifying variable importance in artificial neural networks using simulated data. Ecological Modelling. 178:389-397.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+olden">olden</a></code> for a more flexible approach for variable importance
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## using numeric input

wts_in &lt;- c(13.12, 1.49, 0.16, -0.11, -0.19, -0.16, 0.56, -0.52, 0.81)
struct &lt;- c(2, 2, 1) #two inputs, two hidden, one output 

garson(wts_in, struct)

## using nnet

library(nnet)

data(neuraldat) 
set.seed(123)

mod &lt;- nnet(Y1 ~ X1 + X2 + X3, data = neuraldat, size = 5)
 
garson(mod)  

## Not run: 
## using RSNNS, no bias layers

library(RSNNS)

x &lt;- neuraldat[, c('X1', 'X2', 'X3')]
y &lt;- neuraldat[, 'Y1']
mod &lt;- mlp(x, y, size = 5)

garson(mod)

## using neuralnet

library(neuralnet)

mod &lt;- neuralnet(Y1 ~ X1 + X2 + X3, data = neuraldat, hidden = 5)

garson(mod)

## using caret

library(caret)

mod &lt;- train(Y1 ~ X1 + X2 + X3, method = 'nnet', data = neuraldat, linout = TRUE)

garson(mod)

## modify the plot using ggplot2 syntax
library(ggplot2)

mod &lt;- nnet(Y1 ~ X1 + X2 + X3, data = neuraldat, size = 5)

cols &lt;- heat.colors(10)
garson(mod) +
  scale_y_continuous('Rel. Importance', limits = c(-1, 1)) + 
  scale_fill_gradientn(colours = cols) + 
  scale_colour_gradientn(colours = cols)

## End(Not run)
</code></pre>

<hr>
<h2 id='get_ys'>Get y locations for layers in <code><a href="#topic+plotnet">plotnet</a></code></h2><span id='topic+get_ys'></span>

<h3>Description</h3>

<p>Get y locations for input, hidden, output layers in <code><a href="#topic+plotnet">plotnet</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_ys(lyr, max_sp, struct, y_range)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_ys_+3A_lyr">lyr</code></td>
<td>
<p>numeric indicating layer for getting y locations</p>
</td></tr>
<tr><td><code id="get_ys_+3A_max_sp">max_sp</code></td>
<td>
<p>logical indicating if space is maximized in plot</p>
</td></tr>
<tr><td><code id="get_ys_+3A_struct">struct</code></td>
<td>
<p>numeric vector for network structure</p>
</td></tr>
<tr><td><code id="get_ys_+3A_y_range">y_range</code></td>
<td>
<p>numeric vector indicating limits of y axis</p>
</td></tr>
</table>

<hr>
<h2 id='layer_lines'>Plot connection weights</h2><span id='topic+layer_lines'></span>

<h3>Description</h3>

<p>Plot connection weights in <code><a href="#topic+plotnet">plotnet</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layer_lines(
  mod_in,
  h_layer,
  layer1 = 1,
  layer2 = 2,
  out_layer = FALSE,
  nid,
  rel_rsc,
  all_in,
  pos_col,
  neg_col,
  x_range,
  y_range,
  line_stag,
  x_names,
  layer_x,
  struct,
  max_sp,
  prune_col = NULL,
  prune_lty = "dashed",
  skip
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layer_lines_+3A_mod_in">mod_in</code></td>
<td>
<p>neural network model object</p>
</td></tr>
<tr><td><code id="layer_lines_+3A_h_layer">h_layer</code></td>
<td>
<p>numeric indicating which connections to plot for the layer</p>
</td></tr>
<tr><td><code id="layer_lines_+3A_layer1">layer1</code></td>
<td>
<p>numeric indicating order of first layer (for multiple hiden layers)</p>
</td></tr>
<tr><td><code id="layer_lines_+3A_layer2">layer2</code></td>
<td>
<p>numeric indicating order of second layer (for multiple hiden layers)</p>
</td></tr>
<tr><td><code id="layer_lines_+3A_out_layer">out_layer</code></td>
<td>
<p>logical indicating if the lines are for the output layer</p>
</td></tr>
<tr><td><code id="layer_lines_+3A_nid">nid</code></td>
<td>
<p>logical value indicating if neural interpretation diagram is plotted, default <code>TRUE</code></p>
</td></tr>
<tr><td><code id="layer_lines_+3A_rel_rsc">rel_rsc</code></td>
<td>
<p>numeric indicating the scaling range for the width of connection weights in a neural interpretation diagram. Default is <code>NULL</code> for no rescaling.</p>
</td></tr>
<tr><td><code id="layer_lines_+3A_all_in">all_in</code></td>
<td>
<p>chr string indicating names of input variables for which connections are plotted, default all</p>
</td></tr>
<tr><td><code id="layer_lines_+3A_pos_col">pos_col</code></td>
<td>
<p>chr string indicating color of positive connection weights, default <code>'black'</code></p>
</td></tr>
<tr><td><code id="layer_lines_+3A_neg_col">neg_col</code></td>
<td>
<p>chr string indicating color of negative connection weights, default <code>'grey'</code></p>
</td></tr>
<tr><td><code id="layer_lines_+3A_x_range">x_range</code></td>
<td>
<p>numeric of x axis range for base plot</p>
</td></tr>
<tr><td><code id="layer_lines_+3A_y_range">y_range</code></td>
<td>
<p>numeric of y axis range for base plot</p>
</td></tr>
<tr><td><code id="layer_lines_+3A_line_stag">line_stag</code></td>
<td>
<p>numeric value that specifies distance of connection weights from nodes</p>
</td></tr>
<tr><td><code id="layer_lines_+3A_x_names">x_names</code></td>
<td>
<p>chr string for names of input variables</p>
</td></tr>
<tr><td><code id="layer_lines_+3A_layer_x">layer_x</code></td>
<td>
<p>numeric indicating locations of layers on x axis</p>
</td></tr>
<tr><td><code id="layer_lines_+3A_struct">struct</code></td>
<td>
<p>numeric vector for network structure</p>
</td></tr>
<tr><td><code id="layer_lines_+3A_max_sp">max_sp</code></td>
<td>
<p>logical indicating if space is maximized in plot</p>
</td></tr>
<tr><td><code id="layer_lines_+3A_prune_col">prune_col</code></td>
<td>
<p>chr string indicating color of pruned connections, otherwise not shown</p>
</td></tr>
<tr><td><code id="layer_lines_+3A_prune_lty">prune_lty</code></td>
<td>
<p>line type for pruned connections, passed to <code><a href="graphics.html#topic+segments">segments</a></code></p>
</td></tr>
<tr><td><code id="layer_lines_+3A_skip">skip</code></td>
<td>
<p>logical to plot connections for skip layer</p>
</td></tr>
</table>

<hr>
<h2 id='layer_points'>Plot neural network nodes</h2><span id='topic+layer_points'></span>

<h3>Description</h3>

<p>Plot neural network nodes in <code><a href="#topic+plotnet">plotnet</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layer_points(
  layer,
  x_loc,
  x_range,
  layer_name,
  cex_val,
  circle_cex,
  bord_col,
  in_col,
  node_labs,
  line_stag,
  var_labs,
  x_names,
  y_names,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layer_points_+3A_layer">layer</code></td>
<td>
<p>specifies which layer, integer from <code>struct</code></p>
</td></tr>
<tr><td><code id="layer_points_+3A_x_loc">x_loc</code></td>
<td>
<p>indicates x location for layer, integer from <code>layer_x</code></p>
</td></tr>
<tr><td><code id="layer_points_+3A_x_range">x_range</code></td>
<td>
<p>numeric for total range of x-axis</p>
</td></tr>
<tr><td><code id="layer_points_+3A_layer_name">layer_name</code></td>
<td>
<p>string indicating text to put in node</p>
</td></tr>
<tr><td><code id="layer_points_+3A_cex_val">cex_val</code></td>
<td>
<p>numeric indicating size of point text</p>
</td></tr>
<tr><td><code id="layer_points_+3A_circle_cex">circle_cex</code></td>
<td>
<p>numeric indicating size of circles</p>
</td></tr>
<tr><td><code id="layer_points_+3A_bord_col">bord_col</code></td>
<td>
<p>chr string indicating border color around nodes, default <code>lightblue</code></p>
</td></tr>
<tr><td><code id="layer_points_+3A_in_col">in_col</code></td>
<td>
<p>chr string indicating interior color of nodes</p>
</td></tr>
<tr><td><code id="layer_points_+3A_node_labs">node_labs</code></td>
<td>
<p>logical indicating if node labels are to be plotted</p>
</td></tr>
<tr><td><code id="layer_points_+3A_line_stag">line_stag</code></td>
<td>
<p>numeric indicating distance between of text from nodes</p>
</td></tr>
<tr><td><code id="layer_points_+3A_var_labs">var_labs</code></td>
<td>
<p>chr string for variable labels</p>
</td></tr>
<tr><td><code id="layer_points_+3A_x_names">x_names</code></td>
<td>
<p>chr string for alternative names of input nodes</p>
</td></tr>
<tr><td><code id="layer_points_+3A_y_names">y_names</code></td>
<td>
<p>chr string for alternative names of output nodes</p>
</td></tr>
<tr><td><code id="layer_points_+3A_...">...</code></td>
<td>
<p>values passed to <code><a href="#topic+get_ys">get_ys</a></code></p>
</td></tr>
</table>

<hr>
<h2 id='lekgrps'>Create optional barplot for <code><a href="#topic+lekprofile">lekprofile</a></code> groups</h2><span id='topic+lekgrps'></span>

<h3>Description</h3>

<p>Create optional barplot of constant values of each variable for each group used with <code><a href="#topic+lekprofile">lekprofile</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lekgrps(grps, position = "dodge", grp_nms = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lekgrps_+3A_grps">grps</code></td>
<td>
<p><code><a href="base.html#topic+data.frame">data.frame</a></code> of values for each variable in each group used to create groups in <code><a href="#topic+lekprofile">lekprofile</a></code></p>
</td></tr>
<tr><td><code id="lekgrps_+3A_position">position</code></td>
<td>
<p>chr string indicating bar position (e.g., 'dodge', 'fill', 'stack'), passed to <code><a href="ggplot2.html#topic+geom_bar">geom_bar</a></code></p>
</td></tr>
<tr><td><code id="lekgrps_+3A_grp_nms">grp_nms</code></td>
<td>
<p>optional chr string of alternative names for groups in legend</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="ggplot2.html#topic+ggplot">ggplot</a></code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## enters used with kmeans clustering
x &lt;- neuraldat[, c('X1', 'X2', 'X3')]
grps &lt;- kmeans(x, 6)$center

lekgrps(grps)
</code></pre>

<hr>
<h2 id='lekprofile'>Sensitivity analysis using Lek's profile method</h2><span id='topic+lekprofile'></span><span id='topic+lekprofile.default'></span><span id='topic+lekprofile.nnet'></span><span id='topic+lekprofile.mlp'></span><span id='topic+lekprofile.train'></span><span id='topic+lekprofile.nn'></span>

<h3>Description</h3>

<p>Conduct a sensitivity analysis of model responses in a neural network to input variables using Lek's profile method
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lekprofile(mod_in, ...)

## Default S3 method:
lekprofile(
  mod_in,
  xvars,
  ysel = NULL,
  xsel = NULL,
  steps = 100,
  group_vals = seq(0, 1, by = 0.2),
  val_out = FALSE,
  group_show = FALSE,
  grp_nms = NULL,
  position = "dodge",
  ...
)

## S3 method for class 'nnet'
lekprofile(mod_in, xsel = NULL, ysel = NULL, ...)

## S3 method for class 'mlp'
lekprofile(mod_in, xvars, yvars, xsel = NULL, ysel = NULL, ...)

## S3 method for class 'train'
lekprofile(mod_in, xsel = NULL, ysel = NULL, ...)

## S3 method for class 'nn'
lekprofile(mod_in, xsel = NULL, ysel = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lekprofile_+3A_mod_in">mod_in</code></td>
<td>
<p>input object for which an organized model list is desired.  The input can be an object of class <code>nnet</code> or <code>mlp</code></p>
</td></tr>
<tr><td><code id="lekprofile_+3A_...">...</code></td>
<td>
<p>arguments passed to other methods</p>
</td></tr>
<tr><td><code id="lekprofile_+3A_xvars">xvars</code></td>
<td>
<p><code><a href="base.html#topic+data.frame">data.frame</a></code> of explanatory variables used to create the input model, only needed for <code>mlp</code> objects</p>
</td></tr>
<tr><td><code id="lekprofile_+3A_ysel">ysel</code></td>
<td>
<p>chr string indicating which response variables to plot if more than one, defaults to all</p>
</td></tr>
<tr><td><code id="lekprofile_+3A_xsel">xsel</code></td>
<td>
<p>chr string of names of explanatory variables to plot, defaults to all</p>
</td></tr>
<tr><td><code id="lekprofile_+3A_steps">steps</code></td>
<td>
<p>numeric value indicating number of observations to evaluate for each explanatory variable from minimum to maximum value, default 100</p>
</td></tr>
<tr><td><code id="lekprofile_+3A_group_vals">group_vals</code></td>
<td>
<p>numeric vector with values from 0-1 indicating quantile values at which to hold other explanatory variables constant or a single value indicating number of clusters to define grouping scheme, see details</p>
</td></tr>
<tr><td><code id="lekprofile_+3A_val_out">val_out</code></td>
<td>
<p>logical value indicating if actual sensitivity values are returned rather than a plot, default <code>FALSE</code></p>
</td></tr>
<tr><td><code id="lekprofile_+3A_group_show">group_show</code></td>
<td>
<p>logical if a barplot is returned that shows the values at which explanatory variables were held constant while not being evaluated</p>
</td></tr>
<tr><td><code id="lekprofile_+3A_grp_nms">grp_nms</code></td>
<td>
<p>optional chr string of alternative names for groups in legend</p>
</td></tr>
<tr><td><code id="lekprofile_+3A_position">position</code></td>
<td>
<p>chr string indicating bar position (e.g., 'dodge', 'fill', 'stack'), passed to <code><a href="ggplot2.html#topic+geom_bar">geom_bar</a></code>, used if <code>group_show = TRUE</code></p>
</td></tr>
<tr><td><code id="lekprofile_+3A_yvars">yvars</code></td>
<td>
<p><code><a href="base.html#topic+data.frame">data.frame</a></code> of explanatory variables used to create the input model, only needed for <code>mlp</code> objects</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Lek profile method is described briefly in Lek et al. 1996 and in more detail in Gevrey et al. 2003. The profile method is fairly generic and can be extended to any statistical model in R with a predict method.  However, it is one of few methods used to evaluate sensitivity in neural networks.
</p>
<p>The profile method can be used to evaluate the effect of explanatory variables by returning a plot of the predicted response across the range of values for each separate variable.  The original profile method evaluated the effects of each variable while holding the remaining explanatory variables at different quantiles (e.g., minimum, 20th percentile, maximum).  This is implemented in in the function by creating a matrix of values for explanatory variables where the number of rows is the number of observations and the number of columns is the number of explanatory variables. All explanatory variables are held at their mean (or other constant value) while the variable of interest is sequenced from its minimum to maximum value across the range of observations. This matrix (or data frame) is then used to predict values of the response variable from a fitted model object. This is repeated for each explanatory variable to obtain all response curves.  Values passed to <code>group_vals</code> must range from zero to one to define the quantiles for holding unevaluated explanatory variables. 
</p>
<p>An alternative implementation of the profile method is to group the unevaluated explanatory variables using groupings defined by the statistical properties of the data.  Covariance among predictors may present unlikely scenarios if holding all unevaluated variables at the same level.  To address this issue, the function provides an option to hold unevaluated variable at mean values defined by natural clusters in the data.  <code><a href="stats.html#topic+kmeans">kmeans</a></code> clustering is used on the input <code>data.frame</code> of explanatory variables if the argument passed to <code>group_vals</code> is an integer value greater than one.  The centers of the clusters are then used as constant values for the unevaluated variables.  An arbitrary grouping scheme can also be passed to <code>group_vals</code> as a <code>data.frame</code> where the user can specify exact values for holding each value constant (see the examples). 
</p>
<p>For all plots, the legend with the 'Groups' label indicates the colors that correspond to each group.  The groups describe the values at which unevaluated explanatory variables were held constant, either as specific quantiles, group assignments based on clustering, or in the arbitrary grouping defined by the user.  The constant values of each explanatory variable for each group can be viewed as a barplot by using <code>group_show = TRUE</code>.
</p>
<p>Note that there is no predict method for neuralnet objects from the nn package.  The lekprofile method for nn objects uses the nnet package to recreate the input model, which is then used for the sensitivity predictions.  This approach only works for networks with one hidden layer.
</p>


<h3>Value</h3>

<p>A <code><a href="ggplot2.html#topic+ggplot">ggplot</a></code> object for plotting if <code>val_out  =  FALSE</code>, otherwise a two-element <code>list</code> is returned with a <code>data.frame</code> in long form showing the predicted responses at different values of the explanatory variables and the grouping scheme that was used to hold unevaluated variables constant.
</p>


<h3>References</h3>

<p>Beck, M.W. 2018. NeuralNetTools: Visualization and Analysis Tools for Neural Networks. Journal of Statistical Software. 85(11):1-20.
</p>
<p>Lek, S., Delacoste, M., Baran, P., Dimopoulos, I., Lauga, J., Aulagnier, S. 1996. Application of neural networks to modelling nonlinear relationships in Ecology. Ecological Modelling. 90:39-52.
</p>
<p>Gevrey, M., Dimopoulos, I., Lek, S. 2003. Review and comparison of methods to study the contribution of variables in artificial neural network models. Ecological Modelling. 160:249-264.
</p>
<p>Olden, J.D., Joy, M.K., Death, R.G. 2004. An accurate comparison of methods for quantifying variable importance in artificial neural networks using simulated data. Ecological Modelling. 178:389-397.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## using nnet

library(nnet)

set.seed(123)

mod &lt;- nnet(Y1 ~ X1 + X2 + X3, data = neuraldat, size = 5)
 
lekprofile(mod)  

## Not run: 
## using RSNNS, no bias layers

library(RSNNS)

x &lt;- neuraldat[, c('X1', 'X2', 'X3')]
y &lt;- neuraldat[, 'Y1', drop = FALSE]

mod &lt;- mlp(x, y, size = 5)

lekprofile(mod, xvars = x)

## using neuralnet

library(neuralnet)

mod &lt;- neuralnet(Y1 ~ X1 + X2 + X3, data = neuraldat, hidden = 5)

lekprofile(mod)

## back to nnet, not using formula to create model
## y variable must have a name attribute

mod &lt;- nnet(x, y, size = 5)

lekprofile(mod)

## using caret

library(caret)

mod &lt;- train(Y1 ~ X1 + X2 + X3, method = 'nnet', data = neuraldat, linout = TRUE)

lekprofile(mod)

## group by clusters instead of sequencing by quantiles

mod &lt;- nnet(Y1 ~ X1 + X2 + X3, data = neuraldat, size = 5)
 
lekprofile(mod, group_vals = 6) # six clusters

## enter an arbitrary grouping scheme for the group values
## i.e. hold all values at 0.5
group_vals &lt;- rbind(rep(0.5, length = ncol(x)))
group_vals &lt;- data.frame(group_vals)
names(group_vals) &lt;- names(group_vals)

lekprofile(mod, group_vals = group_vals, xsel = 'X3')

## End(Not run)
</code></pre>

<hr>
<h2 id='neuraldat'>Simulated dataset for function examples</h2><span id='topic+neuraldat'></span>

<h3>Description</h3>

<p>A simulated dataset of 2000 observations containing two response variables and three explanatory variables.  Explanatory variables were sampled from a standard normal distribution.  Response variables were linear combinations of the explanatory variables.  The response variables Y1 and Y2 are standardized from 0 to 1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>neuraldat
</code></pre>


<h3>Format</h3>

<p>A data frame with 2000 rows and 5 variables:
</p>

<dl>
<dt>Y1</dt><dd><p>numeric</p>
</dd>
<dt>Y2</dt><dd><p>numeric</p>
</dd>
<dt>X1</dt><dd><p>numeric</p>
</dd>
<dt>X2</dt><dd><p>numeric</p>
</dd>
<dt>X3</dt><dd><p>numeric</p>
</dd>
</dl>
<p>...

</p>

<hr>
<h2 id='neuralskips'>Get weights for the skip layer in a neural network</h2><span id='topic+neuralskips'></span><span id='topic+neuralskips.nnet'></span>

<h3>Description</h3>

<p>Get weights for the skip layer in a neural network, only valid for networks created using <code>skip = TRUE</code> with the <code><a href="nnet.html#topic+nnet">nnet</a></code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>neuralskips(mod_in, ...)

## S3 method for class 'nnet'
neuralskips(mod_in, rel_rsc = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="neuralskips_+3A_mod_in">mod_in</code></td>
<td>
<p>input object for which an organized model list is desired.</p>
</td></tr>
<tr><td><code id="neuralskips_+3A_...">...</code></td>
<td>
<p>arguments passed to other methods</p>
</td></tr>
<tr><td><code id="neuralskips_+3A_rel_rsc">rel_rsc</code></td>
<td>
<p>numeric indicating the scaling range for the width of connection weights in a neural interpretation diagram. Default is <code>NULL</code> for no rescaling.  Scaling is relative to all weights, not just those in the primary network.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is similar to <code><a href="#topic+neuralweights">neuralweights</a></code> except only the skip layer weights are returned.
</p>


<h3>Value</h3>

<p>Returns a list of connections for each output node, where each element of the list is the connection for each input node in sequential order to the respective output node.  The first weight in each element is not the bias connection, unlike the results for <code><a href="#topic+neuralweights">neuralweights</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(neuraldat)
set.seed(123)

## using nnet

library(nnet)

mod &lt;- nnet(Y1 ~ X1 + X2 + X3, data = neuraldat, size = 5, linout = TRUE, 
 skip = TRUE)
 
neuralskips(mod)  

</code></pre>

<hr>
<h2 id='neuralweights'>Get weights for a neural network</h2><span id='topic+neuralweights'></span><span id='topic+neuralweights.numeric'></span><span id='topic+neuralweights.nnet'></span><span id='topic+neuralweights.mlp'></span><span id='topic+neuralweights.nn'></span>

<h3>Description</h3>

<p>Get weights for a neural network in an organized list by extracting values from a neural network object.  This function is generally not called by itself.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>neuralweights(mod_in, ...)

## S3 method for class 'numeric'
neuralweights(mod_in, rel_rsc = NULL, struct, ...)

## S3 method for class 'nnet'
neuralweights(mod_in, rel_rsc = NULL, ...)

## S3 method for class 'mlp'
neuralweights(mod_in, rel_rsc = NULL, ...)

## S3 method for class 'nn'
neuralweights(mod_in, rel_rsc = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="neuralweights_+3A_mod_in">mod_in</code></td>
<td>
<p>input object for which an organized model list is desired.  The input can be an object of class <code>numeric</code>, <code>nnet</code>, <code>mlp</code>, or <code>nn</code></p>
</td></tr>
<tr><td><code id="neuralweights_+3A_...">...</code></td>
<td>
<p>arguments passed to other methods</p>
</td></tr>
<tr><td><code id="neuralweights_+3A_rel_rsc">rel_rsc</code></td>
<td>
<p>numeric indicating the scaling range for the width of connection weights in a neural interpretation diagram. Default is <code>NULL</code> for no rescaling.</p>
</td></tr>
<tr><td><code id="neuralweights_+3A_struct">struct</code></td>
<td>
<p>numeric vector equal in length to the number of layers in the network.  Each number indicates the number of nodes in each layer starting with the input and ending with the output.  An arbitrary number of hidden layers can be included.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Each element of the returned list is named using the construct 'layer node', e.g. 'out 1' is the first node of the output layer.  Hidden layers are named using three values for instances with more than one hidden layer, e.g., 'hidden 1 1' is the first node in the first hidden layer, 'hidden 1 2' is the second node in the first hidden layer, 'hidden 2 1' is the first node in the second hidden layer, etc.  The values in each element of the list represent the weights entering the specific node from the preceding layer in sequential order, starting with the bias layer if applicable.  For example, the elements in a list item for 'hidden 1 1' of a neural network with a 3 5 1 structure (3 inputs, 5 hidden nodes, 1 output) would have four values indicating the weights in sequence from the bias layer, first input layer, second input layer, and third input layer going to the first hidden node.    
</p>
<p>The function will remove direct weight connections between input and output layers if the neural network was created with a skip-layer using <code>skip = TRUE</code> with the <code><a href="nnet.html#topic+nnet">nnet</a></code>  or <code><a href="caret.html#topic+train">train</a></code> functions.  This may produce misleading results when evaluating variable performance with the <code><a href="#topic+garson">garson</a></code> function.
</p>


<h3>Value</h3>

<p>Returns a two-element list with the first element being a vector indicating the number of nodes in each layer of the neural network and the second element being a named list of weight values for the input model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(neuraldat)
set.seed(123)

## using numeric input

wts_in &lt;- c(13.12, 1.49, 0.16, -0.11, -0.19, -0.16, 0.56, -0.52, 0.81)
struct &lt;- c(2, 2, 1) #two inputs, two hidden, one output 

neuralweights(wts_in, struct = struct)

## using nnet

library(nnet)

mod &lt;- nnet(Y1 ~ X1 + X2 + X3, data = neuraldat, size = 5, linout = TRUE)
 
neuralweights(mod)  

## Not run: 
## using RSNNS, no bias layers

library(RSNNS)

x &lt;- neuraldat[, c('X1', 'X2', 'X3')]
y &lt;- neuraldat[, 'Y1']
mod &lt;- mlp(x, y, size = 5, linOut = TRUE)

neuralweights(mod)

# pruned model using code from RSSNS pruning demo
pruneFuncParams &lt;- list(max_pr_error_increase = 10.0, pr_accepted_error = 1.0, 
 no_of_pr_retrain_cycles = 1000, min_error_to_stop = 0.01, init_matrix_value = 1e-6, 
 input_pruning = TRUE, hidden_pruning = TRUE)
mod &lt;- mlp(x, y, size = 5, pruneFunc = "OptimalBrainSurgeon", 
 pruneFuncParams = pruneFuncParams)

neuralweights(mod)

## using neuralnet

library(neuralnet)

mod &lt;- neuralnet(Y1 ~ X1 + X2 + X3, data = neuraldat, hidden = 5)

neuralweights(mod)

## End(Not run)
</code></pre>

<hr>
<h2 id='olden'>Variable importance using connection weights</h2><span id='topic+olden'></span><span id='topic+olden.default'></span><span id='topic+olden.numeric'></span><span id='topic+olden.nnet'></span><span id='topic+olden.mlp'></span><span id='topic+olden.nn'></span><span id='topic+olden.train'></span>

<h3>Description</h3>

<p>Relative importance of input variables in neural networks as the sum of the product of raw input-hidden, hidden-output connection weights, proposed by Olden et al. 2004.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>olden(mod_in, ...)

## Default S3 method:
olden(
  mod_in,
  x_names,
  y_names,
  out_var = NULL,
  bar_plot = TRUE,
  x_lab = NULL,
  y_lab = NULL,
  skip_wts = NULL,
  ...
)

## S3 method for class 'numeric'
olden(mod_in, struct, ...)

## S3 method for class 'nnet'
olden(mod_in, ...)

## S3 method for class 'mlp'
olden(mod_in, ...)

## S3 method for class 'nn'
olden(mod_in, ...)

## S3 method for class 'train'
olden(mod_in, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="olden_+3A_mod_in">mod_in</code></td>
<td>
<p>input model object or a list of model weights as returned from <code><a href="#topic+neuralweights">neuralweights</a></code> if using the default method</p>
</td></tr>
<tr><td><code id="olden_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods</p>
</td></tr>
<tr><td><code id="olden_+3A_x_names">x_names</code></td>
<td>
<p>chr string of input variable names, obtained from the model object</p>
</td></tr>
<tr><td><code id="olden_+3A_y_names">y_names</code></td>
<td>
<p>chr string of response variable names, obtained from the model object</p>
</td></tr>
<tr><td><code id="olden_+3A_out_var">out_var</code></td>
<td>
<p>chr string indicating the response variable in the neural network object to be evaluated.  Only one input is allowed for models with more than one response.  Names must be of the form <code>'Y1'</code>, <code>'Y2'</code>, etc. if using numeric values as weight inputs for <code>mod_in</code>.</p>
</td></tr>
<tr><td><code id="olden_+3A_bar_plot">bar_plot</code></td>
<td>
<p>logical indicating if a <code>ggplot</code> object is returned (default <code>T</code>), otherwise numeric values are returned</p>
</td></tr>
<tr><td><code id="olden_+3A_x_lab">x_lab</code></td>
<td>
<p>chr string of alternative names to be used for explanatory variables in the figure, default is taken from <code>mod_in</code></p>
</td></tr>
<tr><td><code id="olden_+3A_y_lab">y_lab</code></td>
<td>
<p>chr string of alternative names to be used for response variable in the figure, default is taken from <code>out_var</code></p>
</td></tr>
<tr><td><code id="olden_+3A_skip_wts">skip_wts</code></td>
<td>
<p>vector from <code><a href="#topic+neuralskips">neuralskips</a></code> for <code><a href="nnet.html#topic+nnet">nnet</a></code> models with skip-layer connections</p>
</td></tr>
<tr><td><code id="olden_+3A_struct">struct</code></td>
<td>
<p>numeric vector equal in length to the number of layers in the network.  Each number indicates the number of nodes in each layer starting with the input and ending with the output.  An arbitrary number of hidden layers can be included.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This method is similar to Garson's algorithm (Garson 1991, modified by Goh 1995) in that the connection weights between layers of a neural network form the basis for determining variable importance.  However, Olden et al. 2004 describe a connection weights algorithm that consistently out-performed Garson's algorithm in representing the true variable importance in simulated datasets.  This &lsquo;Olden&rsquo; method calculates variable importance as the product of the raw input-hidden and hidden-output connection weights between each input and output neuron and sums the product across all hidden neurons.  An advantage of this approach is the relative contributions of each connection weight are maintained in terms of both magnitude and sign as compared to Garson's algorithm which only considers the absolute magnitude. For example, connection weights that change sign (e.g., positive to negative) between the input-hidden to hidden-output layers would have a cancelling effect whereas Garson's algorithm may provide misleading results based on the absolute magnitude.  An additional advantage is that Olden's algorithm is capable of evaluating neural networks with multiple hidden layers wheras Garson's was developed for networks with a single hidden layer.   
</p>
<p>The importance values assigned to each variable are in units that are based directly on the summed product of the connection weights.  The actual values should only be interpreted based on relative sign and magnitude between explanatory variables.  Comparisons between different models should not be made.
</p>
<p>The Olden function also works with networks that have skip layers by adding the input-output connection weights to the final summed product of all input-hidden and hidden-output connections.  This was not described in the original method so interpret with caution. 
</p>
<p>By default, the results are shown only for the first response variable for networks with multiple output nodes.  The plotted response variable can be changed with <code>out_var</code>.
</p>


<h3>Value</h3>

<p>A <code><a href="ggplot2.html#topic+ggplot">ggplot</a></code> object for plotting if <code>bar_plot = FALSE</code>, otherwise a <code>data.frame</code> of relative importance values for each input variable.
</p>


<h3>References</h3>

<p>Beck, M.W. 2018. NeuralNetTools: Visualization and Analysis Tools for Neural Networks. Journal of Statistical Software. 85(11):1-20.
</p>
<p>Garson, G.D. 1991. Interpreting neural network connection weights. Artificial Intelligence Expert. 6(4):46-51.
</p>
<p>Goh, A.T.C. 1995. Back-propagation neural networks for modeling complex systems. Artificial Intelligence in Engineering. 9(3):143-151.
</p>
<p>Olden, J.D., Jackson, D.A. 2002. Illuminating the 'black-box': a randomization approach for understanding variable contributions in artificial neural networks. Ecological Modelling. 154:135-150.
</p>
<p>Olden, J.D., Joy, M.K., Death, R.G. 2004. An accurate comparison of methods for quantifying variable importance in artificial neural networks using simulated data. Ecological Modelling. 178:389-397.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## using numeric input

wts_in &lt;- c(13.12, 1.49, 0.16, -0.11, -0.19, -0.16, 0.56, -0.52, 0.81)
struct &lt;- c(2, 2, 1) #two inputs, two hidden, one output 

olden(wts_in, struct)

## using nnet

library(nnet)

data(neuraldat) 
set.seed(123)

mod &lt;- nnet(Y1 ~ X1 + X2 + X3, data = neuraldat, size = 5)
 
olden(mod)  

## Not run: 
## View the difference for a model w/ skip layers

set.seed(123)

mod &lt;- nnet(Y1 ~ X1 + X2 + X3, data = neuraldat, size = 5, skip = TRUE)

olden(mod)

## using RSNNS, no bias layers

library(RSNNS)

x &lt;- neuraldat[, c('X1', 'X2', 'X3')]
y &lt;- neuraldat[, 'Y1']
mod &lt;- mlp(x, y, size = 5)

olden(mod)

## using neuralnet

library(neuralnet)

mod &lt;- neuralnet(Y1 ~ X1 + X2 + X3, data = neuraldat, hidden = 5)

olden(mod)

## using caret

library(caret)

mod &lt;- train(Y1 ~ X1 + X2 + X3, method = 'nnet', data = neuraldat, linout = TRUE)

olden(mod)

## multiple hidden layers

x &lt;- neuraldat[, c('X1', 'X2', 'X3')]
y &lt;- neuraldat[, 'Y1']
mod &lt;- mlp(x, y, size = c(5, 7, 6), linOut = TRUE)

olden(mod)

## End(Not run)
</code></pre>

<hr>
<h2 id='plotnet'>Plot a neural network model</h2><span id='topic+plotnet'></span><span id='topic+plotnet.default'></span><span id='topic+plotnet.nnet'></span><span id='topic+plotnet.numeric'></span><span id='topic+plotnet.mlp'></span><span id='topic+plotnet.nn'></span><span id='topic+plotnet.train'></span>

<h3>Description</h3>

<p>Plot a neural interpretation diagram for a neural network object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotnet(mod_in, ...)

## Default S3 method:
plotnet(
  mod_in,
  x_names,
  y_names,
  struct = NULL,
  nid = TRUE,
  all_out = TRUE,
  all_in = TRUE,
  bias = TRUE,
  bias_y = 0.95,
  rel_rsc = c(1, 7),
  circle_cex = 5,
  node_labs = TRUE,
  var_labs = TRUE,
  line_stag = NULL,
  cex_val = 1,
  alpha_val = 1,
  circle_col = "lightblue",
  pos_col = "black",
  neg_col = "grey",
  bord_col = "lightblue",
  max_sp = FALSE,
  pad_x = 1,
  prune_col = NULL,
  prune_lty = "dashed",
  skip = NULL,
  ...
)

## S3 method for class 'nnet'
plotnet(mod_in, x_names = NULL, y_names = NULL, skip = FALSE, ...)

## S3 method for class 'numeric'
plotnet(mod_in, struct, x_names = NULL, y_names = NULL, ...)

## S3 method for class 'mlp'
plotnet(
  mod_in,
  x_names = NULL,
  y_names = NULL,
  prune_col = NULL,
  prune_lty = "dashed",
  ...
)

## S3 method for class 'nn'
plotnet(mod_in, x_names = NULL, y_names = NULL, ...)

## S3 method for class 'train'
plotnet(mod_in, x_names = NULL, y_names = NULL, skip = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotnet_+3A_mod_in">mod_in</code></td>
<td>
<p>neural network object or numeric vector of weights</p>
</td></tr>
<tr><td><code id="plotnet_+3A_...">...</code></td>
<td>
<p>additional arguments passed to or from other methods</p>
</td></tr>
<tr><td><code id="plotnet_+3A_x_names">x_names</code></td>
<td>
<p>chr string indicating names for input variables, default from model object</p>
</td></tr>
<tr><td><code id="plotnet_+3A_y_names">y_names</code></td>
<td>
<p>chr string indicating names for output variables, default from model object</p>
</td></tr>
<tr><td><code id="plotnet_+3A_struct">struct</code></td>
<td>
<p>numeric vector equal in length to the number of layers in the network.  Each number indicates the number of nodes in each layer starting with the input and ending with the output.  An arbitrary number of hidden layers can be included.</p>
</td></tr>
<tr><td><code id="plotnet_+3A_nid">nid</code></td>
<td>
<p>logical value indicating if neural interpretation diagram is plotted, default <code>TRUE</code></p>
</td></tr>
<tr><td><code id="plotnet_+3A_all_out">all_out</code></td>
<td>
<p>chr string indicating names of response variables for which connections are plotted, default all</p>
</td></tr>
<tr><td><code id="plotnet_+3A_all_in">all_in</code></td>
<td>
<p>chr string indicating names of input variables for which connections are plotted, default all</p>
</td></tr>
<tr><td><code id="plotnet_+3A_bias">bias</code></td>
<td>
<p>logical value indicating if bias nodes and connections are plotted, default <code>TRUE</code></p>
</td></tr>
<tr><td><code id="plotnet_+3A_bias_y">bias_y</code></td>
<td>
<p>value from 0 to 1 for locattion of bias nodes on y-axis</p>
</td></tr>
<tr><td><code id="plotnet_+3A_rel_rsc">rel_rsc</code></td>
<td>
<p>numeric indicating the scaling range for the width of connection weights</p>
</td></tr>
<tr><td><code id="plotnet_+3A_circle_cex">circle_cex</code></td>
<td>
<p>numeric value indicating size of nodes, default 5</p>
</td></tr>
<tr><td><code id="plotnet_+3A_node_labs">node_labs</code></td>
<td>
<p>logical value indicating if labels are plotted directly on nodes, default <code>TRUE</code></p>
</td></tr>
<tr><td><code id="plotnet_+3A_var_labs">var_labs</code></td>
<td>
<p>logical value indicating if variable names are plotted next to nodes, default <code>TRUE</code></p>
</td></tr>
<tr><td><code id="plotnet_+3A_line_stag">line_stag</code></td>
<td>
<p>numeric value that specifies distance of connection weights from nodes</p>
</td></tr>
<tr><td><code id="plotnet_+3A_cex_val">cex_val</code></td>
<td>
<p>numeric value indicating size of text labels, default 1</p>
</td></tr>
<tr><td><code id="plotnet_+3A_alpha_val">alpha_val</code></td>
<td>
<p>numeric value (0-1) indicating transparency of connections, default 1</p>
</td></tr>
<tr><td><code id="plotnet_+3A_circle_col">circle_col</code></td>
<td>
<p>chr string indicating color of nodes, default <code>'lightblue'</code>, or two element list with first element indicating color of input nodes and second indicating color of remaining nodes</p>
</td></tr>
<tr><td><code id="plotnet_+3A_pos_col">pos_col</code></td>
<td>
<p>chr string indicating color of positive connection weights, default <code>'black'</code></p>
</td></tr>
<tr><td><code id="plotnet_+3A_neg_col">neg_col</code></td>
<td>
<p>chr string indicating color of negative connection weights, default <code>'grey'</code></p>
</td></tr>
<tr><td><code id="plotnet_+3A_bord_col">bord_col</code></td>
<td>
<p>chr string indicating border color around nodes, default <code>'lightblue'</code></p>
</td></tr>
<tr><td><code id="plotnet_+3A_max_sp">max_sp</code></td>
<td>
<p>logical value indicating if space between nodes in each layer is maximized, default <code>FALSE</code></p>
</td></tr>
<tr><td><code id="plotnet_+3A_pad_x">pad_x</code></td>
<td>
<p>numeric for increasing or decreasing padding on the x-axis, values less than one will increase padding and values greater than one will decrease padding</p>
</td></tr>
<tr><td><code id="plotnet_+3A_prune_col">prune_col</code></td>
<td>
<p>chr string indicating color of pruned connections, otherwise not shown</p>
</td></tr>
<tr><td><code id="plotnet_+3A_prune_lty">prune_lty</code></td>
<td>
<p>line type for pruned connections, passed to <code><a href="graphics.html#topic+segments">segments</a></code></p>
</td></tr>
<tr><td><code id="plotnet_+3A_skip">skip</code></td>
<td>
<p>logical if skip layer connections are plotted instead of the primary network</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function plots a neural network as a neural interpretation diagram as in Ozesmi and Ozesmi (1999). Options to plot without color-coding or shading of weights are also provided.  The default settings plot positive weights between layers as black lines and negative weights as grey lines. Line thickness is in proportion to relative magnitude of each weight. The first layer includes only input variables with nodes labelled arbitrarily as I1 through In for n input variables.  One through many hidden layers are plotted with each node in each layer labelled as H1 through Hn.  The output layer is plotted last with nodes labeled as O1 through On.  Bias nodes connected to the hidden and output layers are also shown.  Neural networks created using <code><a href="RSNNS.html#topic+mlp">mlp</a></code> do not show bias layers.
</p>
<p>A primary network and a skip layer network can be plotted for <code><a href="nnet.html#topic+nnet">nnet</a></code> models with a skip layer connection.  The default is to plot the primary network, whereas the skip layer network can be viewed with <code>skip = TRUE</code>.  If <code>nid = TRUE</code>, the line widths for both the primary and skip layer plots are relative to all weights.  Viewing both plots is recommended to see which network has larger relative weights.  Plotting a network with only a skip layer (i.e., no hidden layer, <code>size = 0</code>) will include bias connections to the output layer, whereas these are not included in the plot of the skip layer if <code>size</code> is greater than zero.
</p>
<p>The numeric method for plotting requires the input weights to be in a specific order given the structure of the network.  An additional argument <code>struct</code> (from <code><a href="#topic+neuralweights">neuralweights</a></code> is also required that lists the number of nodes in the input, hidden, and output layers.  The example below for the numeric input shows the correct weight vector for a simple neural network model with two input variables, one output variable, and one hidden layer with two nodes.  Bias nodes are also connected to the hidden and output layer.  Using the plot syntax of I, H, O, and B for input, hidden, output, and bias to indicate weighted connections between layers, the correct weight order for the <code>mod_in</code> vector is B1-H1, I1-H1, I2-H1, B1-H2, I1-H2, I2-H2, B2-O1, H1-O1, H2-O1.  For a generic network (three layers) with n input nodes, j hidden nodes, and k output nodes, the weights are ordered as the connections from B1, I1,...,In to H1,...,Hj, then B2, H1,..,Hj to O1,...,Ok.
</p>


<h3>Value</h3>

<p>A graphics object unless <code>wts_only = TRUE</code>, then neural network weights from <code><a href="#topic+neuralweights">neuralweights</a></code>.
</p>


<h3>References</h3>

<p>Beck, M.W. 2018. NeuralNetTools: Visualization and Analysis Tools for Neural Networks. Journal of Statistical Software. 85(11):1-20.
</p>
<p>Ozesmi, S.L., Ozesmi, U. 1999. An artificial neural network approach to spatial habitat modeling with interspecific interaction. Ecological Modelling. 116:15-31.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## using numeric input

# B1-H1, I1-H1, I2-H1, B1-H2, I1-H2, I2-H2, B2-O1, H1-O1, H2-O1.
wts_in &lt;- c(13.12, 1.49, 0.16, -0.11, -0.19, -0.16, 0.56, -0.52, 0.81)
struct &lt;- c(2, 2, 1) #two inputs, two hidden, one output 

plotnet(wts_in, struct = struct)

# numeric input, two hidden layers

# B1-H11, I1-H11, I2-H11, B1-H12, I1-H12, I2-H12, B2-H21, H11-H21, H12-H21, 
# B2-H22, H11-H22, H12-H22, B3-O1, H21-O1, H22-O1 
wts_in &lt;- c(1.12, 1.49, 0.16, -0.11, -0.19, -0.16, 0.5, 0.2, -0.12, -0.1, 
 0.89, 0.9, 0.56, -0.52, 0.81)
struct &lt;- c(2, 2, 2, 1) # two inputs, two (two nodes each), one output 

plotnet(wts_in, struct = struct)

## using nnet

library(nnet)

data(neuraldat) 
set.seed(123)

mod &lt;- nnet(Y1 ~ X1 + X2 + X3, data = neuraldat, size = 5)
 
plotnet(mod)  

## plot the skip layer from nnet model

mod &lt;- nnet(Y1 ~ X1 + X2 + X3, data = neuraldat, size = 5, skip = TRUE)

plotnet(mod, skip = TRUE)  

## Not run: 
## using RSNNS, no bias layers

library(RSNNS)

x &lt;- neuraldat[, c('X1', 'X2', 'X3')]
y &lt;- neuraldat[, 'Y1']
mod &lt;- mlp(x, y, size = 5)

plotnet(mod)

# pruned model using code from RSSNS pruning demo
pruneFuncParams &lt;- list(max_pr_error_increase = 10.0, pr_accepted_error = 1.0, 
 no_of_pr_retrain_cycles = 1000, min_error_to_stop = 0.01, init_matrix_value = 1e-6, 
 input_pruning = TRUE, hidden_pruning = TRUE)
mod &lt;- mlp(x, y, size = 5, pruneFunc = "OptimalBrainSurgeon", 
 pruneFuncParams = pruneFuncParams)

plotnet(mod)
plotnet(mod, prune_col = 'lightblue')

## using neuralnet

library(neuralnet)

mod &lt;- neuralnet(Y1 ~ X1 + X2 + X3, data = neuraldat, hidden = 5)

plotnet(mod)

## using caret

library(caret)

mod &lt;- train(Y1 ~ X1 + X2 + X3, method = 'nnet', data = neuraldat, linout = TRUE)

plotnet(mod)

## a more complicated network with categorical response
AND &lt;- c(rep(0, 7), 1)
OR &lt;- c(0, rep(1, 7))
 
binary_data &lt;- data.frame(expand.grid(c(0, 1), c(0, 1), c(0, 1)), AND, OR)
 
mod &lt;- neuralnet(AND + OR ~ Var1 + Var2 + Var3, binary_data, 
 hidden = c(6, 12, 8), rep = 10, err.fct = 'ce', linear.output = FALSE)
 
plotnet(mod)

## recreate the previous example with numeric inputs

# get the weights and structure in the right format
wts &lt;- neuralweights(mod)
struct &lt;- wts$struct
wts &lt;- unlist(wts$wts)

# plot
plotnet(wts, struct = struct)

## color input nodes by relative importance
mod &lt;- nnet(Y1 ~ X1 + X2 + X3, data = neuraldat, size = 5)
 
rel_imp &lt;- garson(mod, bar_plot = FALSE)$rel_imp
cols &lt;- colorRampPalette(c('lightgreen', 'darkgreen'))(3)[rank(rel_imp)]
 
plotnet(mod, circle_col = list(cols, 'lightblue'))

## End(Not run)
</code></pre>

<hr>
<h2 id='pred_sens'>Predicted values for Lek profile method</h2><span id='topic+pred_sens'></span>

<h3>Description</h3>

<p>Get predicted values for Lek Profile method, used iteratively in <code><a href="#topic+lekprofile">lekprofile</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pred_sens(mat_in, mod_in, var_sel, step_val, grps, ysel)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pred_sens_+3A_mat_in">mat_in</code></td>
<td>
<p><code>data.frame</code> of only the explanatory variables used to create model</p>
</td></tr>
<tr><td><code id="pred_sens_+3A_mod_in">mod_in</code></td>
<td>
<p>any model object with a predict method</p>
</td></tr>
<tr><td><code id="pred_sens_+3A_var_sel">var_sel</code></td>
<td>
<p>chr string of explanatory variable to select</p>
</td></tr>
<tr><td><code id="pred_sens_+3A_step_val">step_val</code></td>
<td>
<p>number of values to sequence range of selected explanatory variable</p>
</td></tr>
<tr><td><code id="pred_sens_+3A_grps">grps</code></td>
<td>
<p>matrix of values for holding explanatory values constant, one column per variable and one row per group</p>
</td></tr>
<tr><td><code id="pred_sens_+3A_ysel">ysel</code></td>
<td>
<p>chr string of response variable names for correct labelling</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Gets predicted output for a model's response variable based on matrix of explanatory variables that are restricted following Lek's profile method. The selected explanatory variable is sequenced across a range of values. All other explanatory variables are held constant at the values in <code>grps</code>.
</p>


<h3>Value</h3>

<p>A <code><a href="base.html#topic+list">list</a></code> of predictions where each element is a <code><a href="base.html#topic+data.frame">data.frame</a></code> with the predicted value of the response and the values of the explanatory variable defined by <code>var_sel</code>.  Each element of the list corresponds to a group defined by the rows in <code>grps</code> at which the other explanatory variables were held constant.
</p>


<h3>See Also</h3>

<p>lekprofile
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## using nnet

library(nnet)

data(neuraldat) 
set.seed(123)

mod &lt;- nnet(Y1 ~ X1 + X2 + X3, data = neuraldat, size = 5)

mat_in &lt;- neuraldat[, c('X1', 'X2', 'X3')]
grps &lt;- apply(mat_in, 2, quantile, seq(0, 1, by = 0.2))

pred_sens(mat_in, mod, 'X1', 100, grps, 'Y1')
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
