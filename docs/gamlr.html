<!DOCTYPE html><html><head><title>Help for package gamlr</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {gamlr}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#AICc'><p>Corrected AIC</p></a></li>
<li><a href='#cv.gamlr'><p> Cross Validation for gamlr</p></a></li>
<li><a href='#doubleML'><p>double ML</p></a></li>
<li><a href='#gamlr'><p>Gamma-Lasso regression</p></a></li>
<li><a href='#hockey'><p> NHL hockey data</p></a></li>
<li><a href='#naref'><p>NA reference level</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Gamma Lasso Regression</td>
</tr>
<tr>
<td>Version:</td>
<td>1.13-8</td>
</tr>
<tr>
<td>Author:</td>
<td>Matt Taddy &lt;mataddy@gmail.com&gt;  </td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Matt Taddy &lt;mataddy@gmail.com&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.15), Matrix, methods, graphics, stats</td>
</tr>
<tr>
<td>Suggests:</td>
<td>parallel</td>
</tr>
<tr>
<td>Description:</td>
<td>The gamma lasso algorithm provides regularization paths corresponding to a range of non-convex cost functions between L0 and L1 norms.  As much as possible, usage for this package is analogous to that for the glmnet package (which does the same thing for penalization between L1 and L2 norms).  For details see: Taddy (2017 JCGS), 'One-Step Estimator Paths for Concave Regularization', &lt;<a href="https://doi.org/10.48550/arXiv.1308.5623">doi:10.48550/arXiv.1308.5623</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/TaddyLab/gamlr">https://github.com/TaddyLab/gamlr</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-04-16 17:22:29 UTC; mataddy</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-04-16 17:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='AICc'>Corrected AIC</h2><span id='topic+AICc'></span>

<h3>Description</h3>

<p> Corrected AIC calculation. </p>


<h3>Usage</h3>

<pre><code class='language-R'>AICc(object, k=2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="AICc_+3A_object">object</code></td>
<td>
<p> Some model object that you can call <code>logLik</code> on (such as a <code>gamlr</code> or <code>glm</code> fit). </p>
</td></tr>
<tr><td><code id="AICc_+3A_k">k</code></td>
<td>
<p> The usual <code>AIC</code> complexity penalty.  <code>k</code> defaults to 2. </p>
</td></tr>
</table>


<h3>Details</h3>

<p> This works just like usual AIC, but instead calculates the small sample (or high dimensional) corrected version  from Hurvich and Tsai
</p>
<p style="text-align: center;"><code class="reqn">AICc = -2\log LHD + k*df*\frac{n}{n-df-1}.</code>
</p>



<h3>Value</h3>

<p>A numeric value for every model evaluated.
</p>


<h3>Author(s)</h3>

<p>Matt Taddy <a href="mailto:mataddy@gmail.com">mataddy@gmail.com</a>
</p>


<h3>References</h3>

<p>Hurvich, C. M. and C-L Tsai, 1989.  &quot;Regression and Time Series Model Selection in Small Samples&quot;, Biometrika 76.</p>


<h3>See Also</h3>

<p>gamlr, hockey</p>

<hr>
<h2 id='cv.gamlr'> Cross Validation for gamlr</h2><span id='topic+cv.gamlr'></span><span id='topic+plot.cv.gamlr'></span><span id='topic+coef.cv.gamlr'></span><span id='topic+predict.cv.gamlr'></span>

<h3>Description</h3>

<p>Cross validation for gamma lasso penalty selection. </p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.gamlr(x, y, nfold=5, foldid=NULL, verb=FALSE, cl=NULL, ...)
## S3 method for class 'cv.gamlr'
plot(x, select=TRUE, df=TRUE, ...)
## S3 method for class 'cv.gamlr'
coef(object, select=c("1se","min"), ...)
## S3 method for class 'cv.gamlr'
predict(object, newdata, select=c("1se","min"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv.gamlr_+3A_x">x</code></td>
<td>
<p> Covariates; see <code>gamlr</code>. </p>
</td></tr>
<tr><td><code id="cv.gamlr_+3A_y">y</code></td>
<td>
<p> Response; see <code>gamlr</code>. </p>
</td></tr>
<tr><td><code id="cv.gamlr_+3A_nfold">nfold</code></td>
<td>
<p> The number of cross validation folds. </p>
</td></tr>
<tr><td><code id="cv.gamlr_+3A_foldid">foldid</code></td>
<td>
<p> An optional length-n vector of fold memberships for each observation.  If specified, this dictates <code>nfold</code>.</p>
</td></tr>
<tr><td><code id="cv.gamlr_+3A_verb">verb</code></td>
<td>
<p> Whether to print progress through folds. </p>
</td></tr>
<tr><td><code id="cv.gamlr_+3A_cl">cl</code></td>
<td>
<p>possible <code>parallel</code> library cluster.  If this is not-<code>NULL</code>, the CV folds are executed in parallel.  This copies the data <code>nfold</code> times, so make sure you have the memory space.</p>
</td></tr>
<tr><td><code id="cv.gamlr_+3A_...">...</code></td>
<td>
<p> Arguments to <code>gamlr</code>. </p>
</td></tr>
<tr><td><code id="cv.gamlr_+3A_object">object</code></td>
<td>
<p> A gamlr object.</p>
</td></tr>
<tr><td><code id="cv.gamlr_+3A_newdata">newdata</code></td>
<td>
<p> New <code>x</code> data for prediction.</p>
</td></tr>
<tr><td><code id="cv.gamlr_+3A_select">select</code></td>
<td>
<p> In prediction and coefficient extraction, 
select which &quot;best&quot; model to return: 
<code>select="min"</code> is that with minimum average OOS deviance,
and  <code>select="1se"</code> is that whose average OOS deviance is
no more than 1 standard error away from the minimum. In <code>plot</code>, whether to draw these selections.</p>
</td></tr>
<tr><td><code id="cv.gamlr_+3A_df">df</code></td>
<td>
<p> Whether to add to the plot degrees of freedom along the top axis.</p>
</td></tr>
</table>


<h3>Details</h3>

<p> Fits a <code>gamlr</code> regression to the full dataset, and then performs <code>nfold</code> 
cross validation to evaluate out-of-sample (OOS)
performance  for different penalty weights.
</p>
<p><code>plot.cv.gamlr</code> can be used to plot the results: it 
shows mean OOS deviance with 1se error bars.
</p>


<h3>Value</h3>

<table>
<tr><td><code>gamlr</code></td>
<td>
<p> The full-data fitted <code>gamlr</code> object.</p>
</td></tr>
<tr><td><code>nfold</code></td>
<td>
<p> The number of CV folds. </p>
</td></tr>
<tr><td><code>foldid</code></td>
<td>
<p> The length-n vector of fold memberships. </p>
</td></tr>
<tr><td><code>cvm</code></td>
<td>
<p> Mean OOS deviance by <code>gamlr\$lambda</code> </p>
</td></tr>
<tr><td><code>cvs</code></td>
<td>
<p> The standard errors on <code>cvm</code>.</p>
</td></tr>
<tr><td><code>seg.min</code></td>
<td>
<p> The index of minimum <code>cvm</code>. </p>
</td></tr>
<tr><td><code>seg.1se</code></td>
<td>
<p> The index of <code>1se</code> <code>cvm</code> (see details). </p>
</td></tr>
<tr><td><code>lambda.min</code></td>
<td>
<p> Penalty at minimum <code>cvm</code>. </p>
</td></tr>
<tr><td><code>lambda.1se</code></td>
<td>
<p> Penalty at <code>1se</code> <code>cvm</code>. </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Matt Taddy <a href="mailto:mataddy@gmail.com">mataddy@gmail.com</a>
</p>


<h3>References</h3>

<p>Taddy (2017 JCGS), One-Step Estimator Paths for Concave Regularization, http://arxiv.org/abs/1308.5623</p>


<h3>See Also</h3>

<p>gamlr, hockey</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 100
p &lt;- 100

xvar &lt;- matrix(ncol=p,nrow=p)
for(i in 1:p) for(j in i:p) xvar[i,j] &lt;- 0.5^{abs(i-j)}
x &lt;- matrix(rnorm(p*n), nrow=n)%*%chol(xvar)
beta &lt;- matrix( (-1)^(1:p)*exp(-(1:p)/10) )
mu = x%*%beta
y &lt;- mu + rnorm(n,sd=sd(as.vector(mu))/2)

## fit with gamma=1 concavity
cvfit &lt;- cv.gamlr(x, y, gamma=1, verb=TRUE)

coef(cvfit)[1:3,] # 1se default
coef(cvfit, select="min")[1:3,] # min OOS deviance

predict(cvfit, x[1:2,], select="min")
predict(cvfit$gamlr, x[1:2,], select=cvfit$seg.min)

par(mfrow=c(1,2))
plot(cvfit)
plot(cvfit$gamlr)

</code></pre>

<hr>
<h2 id='doubleML'>double ML</h2><span id='topic+doubleML'></span>

<h3>Description</h3>

<p>double (i.e., double) Machine Learning for treatment effect estimation </p>


<h3>Usage</h3>

<pre><code class='language-R'>doubleML(x, d, y, nfold=2, foldid=NULL, family="gaussian", cl=NULL, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="doubleML_+3A_x">x</code></td>
<td>
<p> Covariates; see <code>gamlr</code>. </p>
</td></tr>
<tr><td><code id="doubleML_+3A_d">d</code></td>
<td>
<p> The matrix of treatment variables.  Each column is used as a response by <code>gamlr</code> during the residualization procedure.</p>
</td></tr>
<tr><td><code id="doubleML_+3A_y">y</code></td>
<td>
<p> Response; see <code>gamlr</code>. </p>
</td></tr>
<tr><td><code id="doubleML_+3A_nfold">nfold</code></td>
<td>
<p> The number of cross validation folds. </p>
</td></tr>
<tr><td><code id="doubleML_+3A_foldid">foldid</code></td>
<td>
<p> An optional length-n vector of fold memberships for each observation.  If specified, this dictates <code>nfold</code>.</p>
</td></tr>
<tr><td><code id="doubleML_+3A_family">family</code></td>
<td>
<p> Response model type for the treatment prediction; 
either &quot;gaussian&quot;, &quot;poisson&quot;, or &quot;binomial&quot;.  This can be either be a single family shared by all columns of <code>d</code> or a vector of families of length <code>ncol(d)</code></p>
</td></tr>
<tr><td><code id="doubleML_+3A_cl">cl</code></td>
<td>
<p>possible <code>parallel</code> library cluster.  If this is not-<code>NULL</code>, the CV folds are executed in parallel.  This copies the data <code>nfold</code> times, so make sure you have the memory space.</p>
</td></tr>
<tr><td><code id="doubleML_+3A_...">...</code></td>
<td>
<p> Arguments to all the <code>gamlr</code> regressions. </p>
</td></tr>
</table>


<h3>Details</h3>

<p> Performs the double ML procedure of Chernozhukov et al. (2017) to produce an unbiased estimate of the average linear treatment effects of <code>d</code> on <code>y</code>.  This procedure uses <code>gamlr</code> to regress <code>y</code> and each column of <code>d</code> onto <code>x</code>.  In the cross-fitting routine described in Taddy (2019), these regressions are trained on a portion of the data and the out-of-sample residuals are calculated on the left-out fold.  Model selection for these residualization steps is based on the AICc selection rule.  The response residuals are then regressed onto the treatment residuals using <code>lm</code> and the resulting estimates and standard errors are unbiased for the treatment effects under the assumptions of Chernozhukov et al.
</p>


<h3>Value</h3>

<p>A fitted <code>lm</code> object estimating the treatment effect of <code>d</code> on <code>y</code>.  The <code>lm</code> function has been called with <code>x=TRUE, y=TRUE</code> such that this object contains the residualized <code>d</code> as <code>x</code> and residualized <code>y</code> as <code>y</code>.
</p>


<h3>Author(s)</h3>

<p>Matt Taddy <a href="mailto:mataddy@gmail.com">mataddy@gmail.com</a>
</p>


<h3>References</h3>

<p>Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James (The Econometrics Journal, 2017), Double/debiased machine learning for treatment and structural parameters
</p>
<p>Matt Taddy, 2019.  Business Data Science, McGraw-Hill</p>


<h3>See Also</h3>

<p>gamlr, hockey, AICc</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(hockey)
who &lt;- which(colnames(player)=="SIDNEY_CROSBY")
s &lt;- sample.int(nrow(player),10000) # subsample for a fast example
doubleML(x=player[s,-who], d=player[s,who], y=goal$homegoal[s], standardize=FALSE)

</code></pre>

<hr>
<h2 id='gamlr'>Gamma-Lasso regression</h2><span id='topic+gamlr'></span><span id='topic+predict.gamlr'></span><span id='topic+plot.gamlr'></span><span id='topic+coef.gamlr'></span><span id='topic+logLik.gamlr'></span>

<h3>Description</h3>

<p> Adaptive L1 penalized regression estimation. </p>


<h3>Usage</h3>

<pre><code class='language-R'>gamlr(x, y, 
   family=c("gaussian","binomial","poisson"),
   gamma=0,nlambda=100, lambda.start=Inf,  
   lambda.min.ratio=0.01, free=NULL, standardize=TRUE, 
   obsweight=NULL,varweight=NULL,
   prexx=(p&lt;500),
   tol=1e-7,maxit=1e5,verb=FALSE, ...)

## S3 method for class 'gamlr'
plot(x, against=c("pen","dev"), 
    col=NULL, select=TRUE, df=TRUE, ...)
## S3 method for class 'gamlr'
coef(object, select=NULL, k=2, corrected=TRUE, ...)
## S3 method for class 'gamlr'
predict(object, newdata,
            type = c("link", "response"), ...)
## S3 method for class 'gamlr'
logLik(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gamlr_+3A_x">x</code></td>
<td>
<p> A dense <code>matrix</code> 
or sparse <code>Matrix</code> of covariates,
with <code>ncol(x)</code> variables and 
<code>nrow(x)==length(y)</code> observations.
This should not include the intercept.</p>
</td></tr>
<tr><td><code id="gamlr_+3A_y">y</code></td>
<td>
<p>A vector of response values. 
There is almost no argument checking, 
so be careful to match <code>y</code> with the appropriate <code>family</code>
</p>
</td></tr>
<tr><td><code id="gamlr_+3A_family">family</code></td>
<td>
<p> Response model type; 
either &quot;gaussian&quot;, &quot;poisson&quot;, or &quot;binomial&quot;.  
Note that for &quot;binomial&quot;, <code>y</code> is in <code class="reqn">[0,1]</code>. </p>
</td></tr>
<tr><td><code id="gamlr_+3A_gamma">gamma</code></td>
<td>
<p> Penalty concavity tuning parameter; see details. 
Zero (default) yields the lasso,
and higher values correspond to a more concave penalty.  
</p>
</td></tr>
<tr><td><code id="gamlr_+3A_nlambda">nlambda</code></td>
<td>
<p> Number of regularization path segments. </p>
</td></tr>
<tr><td><code id="gamlr_+3A_lambda.start">lambda.start</code></td>
<td>
<p> Initial penalty value.  Default of <code>Inf</code>
implies the infimum lambda that returns all zero
coefficients.  This is the largest absolute coefficient gradient at the null model. </p>
</td></tr>
<tr><td><code id="gamlr_+3A_lambda.min.ratio">lambda.min.ratio</code></td>
<td>
<p> The smallest penalty weight 
(expected L1 cost) as a ratio of the path start value.  
Our default is always 0.01; note that this differs from <code>glmnet</code>
whose default depends upon the dimension of <code>x</code>. </p>
</td></tr>
<tr><td><code id="gamlr_+3A_free">free</code></td>
<td>
<p> Free variables: indices of the columns of <code>x</code> which will be unpenalized.</p>
</td></tr>
<tr><td><code id="gamlr_+3A_standardize">standardize</code></td>
<td>
<p> Whether to standardize 
the coefficients to have standard deviation of one.  
This is equivalent to multiplying the L1 penalty 
by each coefficient standard deviation. </p>
</td></tr>
<tr><td><code id="gamlr_+3A_obsweight">obsweight</code></td>
<td>
<p>For <code>family="gaussian"</code> only, weights on each observation in the weighted least squares objective.  For
other resonse families, <code>obsweights</code> are overwritten by IRLS weights.  Defaults to <code>rep(1,n)</code>.</p>
</td></tr>
<tr><td><code id="gamlr_+3A_varweight">varweight</code></td>
<td>
<p>Multipliers on the penalty associated with each covariate coefficient.  Must be non-negative. These are further multiplied by <code class="reqn">sd(x_j)</code> if <code>standardize=TRUE</code>.  Defaults to <code>rep(1,p)</code>.</p>
</td></tr>
<tr><td><code id="gamlr_+3A_prexx">prexx</code></td>
<td>
<p> Only possible for <code>family="gaussian"</code>: whether to use pre-calculated weighted variable covariances in gradient calculations.  This leads to massive speed-ups for big-n datasets, but can be slow for <code class="reqn">p&gt;n</code> datasets. See note.
</p>
</td></tr>
<tr><td><code id="gamlr_+3A_tol">tol</code></td>
<td>
<p> Optimization convergence tolerance relative to the null model deviance for each 
inner coordinate-descent loop.  This is measured against the 
maximum coordinate change times deviance curvature after full parameter-set update. </p>
</td></tr>
<tr><td><code id="gamlr_+3A_maxit">maxit</code></td>
<td>
<p> Max iterations for a single segment
coordinate descent routine. </p>
</td></tr>
<tr><td><code id="gamlr_+3A_verb">verb</code></td>
<td>
<p> Whether to print some output for each path segment. </p>
</td></tr>
<tr><td><code id="gamlr_+3A_object">object</code></td>
<td>
<p> A gamlr object.</p>
</td></tr>
<tr><td><code id="gamlr_+3A_against">against</code></td>
<td>
<p> Whether to plot paths 
against log penalty or deviance.</p>
</td></tr>
<tr><td><code id="gamlr_+3A_select">select</code></td>
<td>
<p> In <code>coef</code> (and <code>predict</code>, which calls <code>coef</code>), the index of path segments
for which you want coefficients or prediction (e.g., do <code>select=which.min(BIC(object))</code> for BIC selection).  If null, the segments are selected via our <code>AICc</code> function with <code>k</code> as specified (see also <code>corrected</code>).  If <code>select=0</code> all segments are returned.
</p>
<p>In <code>plot</code>,
<code>select</code> is just a flag for whether to add lines marking AICc and BIC selected models.</p>
</td></tr>
<tr><td><code id="gamlr_+3A_k">k</code></td>
<td>
<p> If <code>select=NULL</code> in <code>coef</code> or <code>predict</code>, the <code>AICc</code> complexity penalty.  <code>k</code> defaults to the usual 2. </p>
</td></tr>
<tr><td><code id="gamlr_+3A_corrected">corrected</code></td>
<td>
<p> A flag that swaps corrected (for high dimensional bias) <code>AICc</code> in for the standard <code>AIC</code>.  You almost always want <code>corrected=TRUE</code>, unless you want to apply the BIC in which case use <code>k=log(n), corrected=FALSE</code>. </p>
</td></tr>
<tr><td><code id="gamlr_+3A_newdata">newdata</code></td>
<td>
<p> New <code>x</code> data for prediction.</p>
</td></tr>
<tr><td><code id="gamlr_+3A_type">type</code></td>
<td>
<p> Either &quot;link&quot; for the linear equation, 
or &quot;response&quot; for predictions transformed 
to the same domain as <code>y</code>.</p>
</td></tr>
<tr><td><code id="gamlr_+3A_col">col</code></td>
<td>
<p> A single plot color, 
or vector of length <code>ncol(x)</code> colors for each coefficient
regularization path. <code>NULL</code> uses the <code>matplot</code> default <code>1:6</code>. </p>
</td></tr>
<tr><td><code id="gamlr_+3A_df">df</code></td>
<td>
<p> Whether to add to the plot degrees of freedom along the top axis.</p>
</td></tr>
<tr><td><code id="gamlr_+3A_...">...</code></td>
<td>
<p> Extra arguments to each method.  Most importantly, from 
<code>predict.gamlr</code> these are arguments to <code>coef.gamlr</code>. </p>
</td></tr>
</table>


<h3>Details</h3>

<p> Finds posterior modes along a regularization path
of <em>adapted L1 penalties</em> via coordinate descent.
</p>
<p>Each path segment <code class="reqn">t</code> minimizes the objective -<code class="reqn">(\phi/n)</code>logLHD<code class="reqn">(\beta_1
  ... \beta_p) + \sum \omega_j\lambda|\beta_j|</code>, where <code class="reqn">\phi</code> is the
exponential family dispersion parameter (<code class="reqn">\sigma^2</code> for
<code>family="gaussian"</code>, one otherwise).  Weights <code class="reqn">\omega_j</code> are  
set as <code class="reqn">1/(1+\gamma|b_j^{t-1}|)</code> where <code class="reqn">b_j^{t-1}</code> is our estimate of <code class="reqn">\beta_j</code> for the previous path segment (or zero if <code class="reqn">t=0</code>).  This adaptation is what makes the penalization &lsquo;concave&rsquo;; see Taddy (2013) for details.
</p>
<p><code>plot.gamlr</code> can be used to graph the results: it 
shows the regularization paths for penalized <code class="reqn">\beta</code>, with degrees of freedom along the top axis and minimum AICc selection marked.  
</p>
<p><code>logLik.gamlr</code> returns log likelihood along the regularization path.  It is based on the <code>deviance</code>, and is correct only up to static constants; 
e.g., for a Poisson it is off by <code class="reqn">\sum_i y_i(\log y_i-1)</code> (the saturated log likelihood) and for a Gaussian it is off by likelihood constants <code class="reqn">(n/2)(1+\log2\pi)</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>lambda</code></td>
<td>
<p>The path of fitted <em>prior expected</em> L1 penalties.</p>
</td></tr>
<tr><td><code>nobs</code></td>
<td>
<p> The number of observations.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>Intercepts.</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>Regression coefficients.</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>Approximate degrees of freedom.</p>
</td></tr>
<tr><td><code>deviance</code></td>
<td>
<p>Fitted deviance: 
<code class="reqn">(-2/\phi)</code>( logLHD.fitted - logLHD.saturated ). </p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>Number of optimization iterations by segment, broken into coordinate descent cycles and IRLS re-weightings for <code>family!="gaussian"</code>. </p>
</td></tr>
<tr><td><code>family</code></td>
<td>
<p>The exponential family model.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Under <code>prexx=TRUE</code> (requires <code>family="gaussian"</code>), weighted covariances <code class="reqn">(VX)'X</code> and <code class="reqn">(VX)'y</code>, weighted column sums of <code class="reqn">VX</code>, and column means <code class="reqn">\bar{x}</code> will be pre-calculated. Here <code class="reqn">V</code> is the diagonal matrix of least squares weights (<code>obsweights</code>, so <code class="reqn">V</code> defaults to <code class="reqn">I</code>).  It is not necessary (they will be built by <code>gamlr</code> otherwise), but you have the option to pre-calculate these sufficient statistics yourself as arguments <code>vxx</code> (<code>matrix</code> or <code>dspMatrix</code>), <code>vxy</code>, <code>vxsum</code>, and <code>xbar</code> (all <code>vectors</code>) respectively.  Search <code>PREXX</code> in <code>gamlr.R</code> to see the steps involved, and notice that there is very little argument checking &ndash; do at your own risk.  Note that <code>xbar</code> is an <em>unweighted</em> calculation, even if <code class="reqn">V \neq I</code>.   For really Big Data you can then run with <code>x=NULL</code> (e.g., if these statistics were calculated on distributed machines and full design is unavailable). <em>Beware:</em> in this <code>x=NULL</code> case our deviance (and df, if <code>gamma&gt;0</code>) calculations are incorrect and selection rules will always return the smallest-lambda model.
</p>


<h3>Author(s)</h3>

<p>Matt Taddy <a href="mailto:mataddy@gmail.com">mataddy@gmail.com</a>
</p>


<h3>References</h3>

<p>Taddy (2017 JCGS), One-Step Estimator Paths for Concave Regularization, http://arxiv.org/abs/1308.5623</p>


<h3>See Also</h3>

<p>cv.gamlr, AICc, hockey</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### a low-D test (highly multi-collinear)

n &lt;- 1000
p &lt;- 3
xvar &lt;- matrix(0.9, nrow=p,ncol=p)
diag(xvar) &lt;- 1
x &lt;- matrix(rnorm(p*n), nrow=n)%*%chol(xvar)
y &lt;- 4 + 3*x[,1] + -1*x[,2] + rnorm(n)

## run models to extra small lambda 1e-3xlambda.start
fitlasso &lt;- gamlr(x, y, gamma=0, lambda.min.ratio=1e-3) # lasso
fitgl &lt;- gamlr(x, y, gamma=2, lambda.min.ratio=1e-3) # small gamma
fitglbv &lt;- gamlr(x, y, gamma=10, lambda.min.ratio=1e-3) # big gamma

par(mfrow=c(1,3))
ylim = range(c(fitglbv$beta@x))
plot(fitlasso, ylim=ylim, col="navy")
plot(fitgl, ylim=ylim, col="maroon")
plot(fitglbv, ylim=ylim, col="darkorange")

 </code></pre>

<hr>
<h2 id='hockey'> NHL hockey data </h2><span id='topic+hockey'></span><span id='topic+config'></span><span id='topic+goal'></span><span id='topic+team'></span><span id='topic+player'></span>

<h3>Description</h3>

<p>Every NHL goal from fall 2002 through the 2014 cup finals.</p>


<h3>Details</h3>

<p> The data comprise of information about 
play configuration and the
players on ice  (including goalies) for every
goal from 2002-03 to 2012-14 NHL seasons. 
Collected using A. C. Thomas's <code>nlhscrapr</code> package. 
See the Chicago hockey analytics project at <code>github.com/mataddy/hockey</code>.</p>


<h3>Value</h3>

<table>
<tr><td><code>goal</code></td>
<td>
<p> Info about each goal scored, including <code>homegoal</code> &ndash; an indicator for the home team scoring.</p>
</td></tr>
<tr><td><code>player</code></td>
<td>
<p> Sparse Matrix with entries for who was on the ice for each goal: +1 for a home team player, -1 for an away team player, zero otherwise. </p>
</td></tr>
<tr><td><code>team</code></td>
<td>
<p> Sparse Matrix with indicators for each team*season interaction: +1 for home team, -1 for away team. </p>
</td></tr>
<tr><td><code>config</code></td>
<td>
<p> Special teams info. For example,
<code>S5v4</code> is a 5 on 4 powerplay,
+1 if it is for the home-team and -1 for the away team.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Matt Taddy, <a href="mailto:mataddy@gmail.com">mataddy@gmail.com</a>
</p>


<h3>References</h3>

<p>Gramacy, Jensen, and Taddy (2013): &quot;Estimating Player
Contribution in Hockey with Regularized Logistic Regression&quot;, the Journal of Quantitative Analysis in Sport.
</p>
<p>Gramacy, Taddy, and Tian (2015): &quot;Hockey Player Performance via Regularized Logistic Regression&quot;, the Handbook of statistical methods for design and analysis in sports.
</p>


<h3>See Also</h3>

<p>gamlr</p>


<h3>Examples</h3>

<pre><code class='language-R'>## design 
data(hockey)
x &lt;- cbind(config,team,player)
y &lt;- goal$homegoal

## fit the plus-minus regression model
## (non-player effects are unpenalized)

fit &lt;- gamlr(x, y, 
  lambda.min.ratio=0.05, nlambda=40, ## just so it runs in under 5 sec
  free=1:(ncol(config)+ncol(team)),
  standardize=FALSE, family="binomial")
plot(fit)

## look at estimated player [career] effects
B &lt;- coef(fit)[colnames(player),]
sum(B!=0) # number of measurable effects (AICc selection)
B[order(-B)[1:10]] # 10 biggest

## convert to 2013-2014 season partial plus-minus
now &lt;- goal$season=="20132014"
pm &lt;- colSums(player[now,names(B)]*c(-1,1)[y[now]+1]) # traditional plus minus
ng &lt;- colSums(abs(player[now,names(B)])) # total number of goals
# The individual effect on probability that a
# given goal is for vs against that player's team
p &lt;- 1/(1+exp(-B)) 
# multiply ng*p - ng*(1-p) to get expected plus-minus
ppm &lt;- ng*(2*p-1)

# organize the data together and print top 20
effect &lt;- data.frame(b=round(B,3),ppm=round(ppm,3),pm=pm)
effect &lt;- effect[order(-effect$ppm),]
print(effect[1:20,])
</code></pre>

<hr>
<h2 id='naref'>NA reference level</h2><span id='topic+naref'></span>

<h3>Description</h3>

<p> Set NA as the reference level for factor variables and do imputation on missing values for numeric variables.  This is useful to build model matrices for regularized regression, and for dealing with missing values, as in Taddy 2019.</p>


<h3>Usage</h3>

<pre><code class='language-R'>naref(x, impute=FALSE, pzero=0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="naref_+3A_x">x</code></td>
<td>
<p> A data frame. </p>
</td></tr>
<tr><td><code id="naref_+3A_impute">impute</code></td>
<td>
<p> Logical, whether to impute missing values in numeric columns.</p>
</td></tr>
<tr><td><code id="naref_+3A_pzero">pzero</code></td>
<td>
<p> If <code>impute==TRUE</code>, then if more than <code>pzero</code> of the values in a column are zero do zero imputation, else do mean imputation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p> For every <code>factor</code> or <code>character</code> column in <code>x</code>, <code>naref</code> sets <code>NA</code> as the reference level for a <code>factor</code> variable. Columns coded as <code>character</code> class are first converted to factors via <span class="rlang"><b>R</b></span>factor(x).  If <code>impute=TRUE</code> then the numeric columns are converted to two columns, one appended <code>.x</code> that contains imputed values and another appended <code>.miss</code> which is a binary variable indicating whether the original value was missing.  Numeric columns are returned without change if <code>impute=FALSE</code> or if they do not contain any missing values.  
</p>


<h3>Value</h3>

<p>A data frame where the factor and character columns have been converted to factors with reference level <code>NA</code>, and if <code>impute=TRUE</code> the missing values in numeric columns have been imputed and a flag for missingness has been added.  See details.   
</p>


<h3>Author(s)</h3>

<p>Matt Taddy <a href="mailto:mataddy@gmail.com">mataddy@gmail.com</a>
</p>


<h3>References</h3>

<p>Matt Taddy, 2019.  &quot;Business Data Science&quot;, McGraw-Hill</p>


<h3>Examples</h3>

<pre><code class='language-R'>( x &lt;- data.frame(a=factor(c(1,2,3)),b=c(1,NA,3)) )
naref(x, impute=TRUE)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
