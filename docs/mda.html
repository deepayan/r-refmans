<!DOCTYPE html><html><head><title>Help for package mda</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {mda}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bruto'>
<p>Fit an Additive Spline Model by Adaptive Backfitting</p></a></li>
<li><a href='#coef.fda'>
<p>Produce coefficients for an fda or mda object</p></a></li>
<li><a href='#confusion'><p>Confusion Matrices</p></a></li>
<li><a href='#ESL.mixture'><p>Mixture example from &quot;Elements of Statistical Learning&quot;</p></a></li>
<li><a href='#fda'><p>Flexible Discriminant Analysis</p></a></li>
<li><a href='#gen.ridge'><p>Penalized Regression</p></a></li>
<li><a href='#glass'><p>Glass Identification Database</p></a></li>
<li><a href='#laplacian'><p>create penalty object for two-dimensional smoothing.</p></a></li>
<li><a href='#mars'><p>Multivariate Adaptive Regression Splines</p></a></li>
<li><a href='#mda'><p>Mixture Discriminant Analysis</p></a></li>
<li><a href='#mda.start'><p>Initialization for Mixture Discriminant Analysis</p></a></li>
<li><a href='#model.matrix.mars'><p>Produce a Design Matrix from a &lsquo;mars&rsquo; Object</p></a></li>
<li><a href='#mspline'>
<p>Vector Smoothing Spline</p></a></li>
<li><a href='#plot.fda'><p>Plot for Flexible Discriminant Analysis</p></a></li>
<li><a href='#polyreg'><p>Polynomial Regression</p></a></li>
<li><a href='#predict.bruto'><p>Predict method for BRUTO Objects</p></a></li>
<li><a href='#predict.fda'><p>Classify by Flexible Discriminant Analysis</p></a></li>
<li><a href='#predict.mars'><p>Predict method for MARS Objects</p></a></li>
<li><a href='#predict.mda'><p>Classify by Mixture Discriminant Analysis</p></a></li>
<li><a href='#softmax'><p>Find the Maximum in Each Row of a Matrix</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>0.5-4</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-06-22</td>
</tr>
<tr>
<td>Author:</td>
<td>S original by Trevor Hastie &amp; Robert Tibshirani. Original R port by Friedrich Leisch, Kurt Hornik and Brian D. Ripley. Balasubramanian Narasimhan has contributed to the upgrading of the code.</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Trevor Hastie &lt;hastie@stanford.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Mixture and flexible discriminant analysis, multivariate
        adaptive regression splines (MARS), BRUTO, and vector-response smoothing splines.
	Hastie, Tibshirani and Friedman (2009) "Elements of Statistical Learning (second edition, chap 12)" Springer, New York. </td>
</tr>
<tr>
<td>Title:</td>
<td>Mixture and Flexible Discriminant Analysis</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0), stats, class</td>
</tr>
<tr>
<td>Suggests:</td>
<td>earth, testthat</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-06-23 00:29:08 UTC; hastie</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-06-23 04:00:02 UTC</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
</table>
<hr>
<h2 id='bruto'>
Fit an Additive Spline Model by Adaptive Backfitting
</h2><span id='topic+bruto'></span>

<h3>Description</h3>

<p>Fit an additive spline model by adaptive backfitting.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bruto(x, y, w, wp, dfmax, cost, maxit.select, maxit.backfit, 
      thresh = 0.0001, trace.bruto = FALSE, start.linear = TRUE,
      fit.object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bruto_+3A_x">x</code></td>
<td>
<p>a matrix of numeric predictors (does not include the column
of 1s).</p>
</td></tr>
<tr><td><code id="bruto_+3A_y">y</code></td>
<td>
<p>a vector or matrix of responses.</p>
</td></tr>
<tr><td><code id="bruto_+3A_w">w</code></td>
<td>
<p>optional observation weight vector.</p>
</td></tr>
<tr><td><code id="bruto_+3A_wp">wp</code></td>
<td>
<p>optional weight vector for each column of <code>y</code>; the RSS
and GCV criteria use a weighted sum of squared residuals.</p>
</td></tr>
<tr><td><code id="bruto_+3A_dfmax">dfmax</code></td>
<td>
<p>a vector of maximum df (degrees of freedom) for each
term.</p>
</td></tr>
<tr><td><code id="bruto_+3A_cost">cost</code></td>
<td>
<p>cost per degree of freedom; default is 2.</p>
</td></tr>
<tr><td><code id="bruto_+3A_maxit.select">maxit.select</code></td>
<td>
<p>maximum number of iterations during the selection
stage.</p>
</td></tr>
<tr><td><code id="bruto_+3A_maxit.backfit">maxit.backfit</code></td>
<td>
<p>maximum number of iterations for the final
backfit stage (with fixed lambda).</p>
</td></tr> 
<tr><td><code id="bruto_+3A_thresh">thresh</code></td>
<td>
<p>convergence threshold (default is 0.0001); iterations
cease when the relative change in GCV is below this threshold.</p>
</td></tr> 
<tr><td><code id="bruto_+3A_trace.bruto">trace.bruto</code></td>
<td>
<p>logical flag.  If <code>TRUE</code> (default) a progress
report is printed during the fitting.</p>
</td></tr>
<tr><td><code id="bruto_+3A_start.linear">start.linear</code></td>
<td>
<p>logical flag.  If <code>TRUE</code> (default), the model
starts with the linear fit.</p>
</td></tr>
<tr><td><code id="bruto_+3A_fit.object">fit.object</code></td>
<td>
<p>This the object returned by <code>bruto()</code>; if
supplied, the same model is fit to the presumably new <code>y</code>.</p>
</td></tr>
<tr><td><code id="bruto_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A multiresponse additive model fit object of class <code>"bruto"</code> is
returned.  The model is fit by adaptive backfitting using smoothing
splines.  If there are <code>np</code> columns in <code>y</code>, then <code>np</code>
additive models are fit, but the same amount of smoothing (df) is 
used for  each term.  The procedure chooses between <code>df = 0</code>
(term omitted), <code>df = 1</code> (term linear) or <code>df &gt; 0</code> (term
fitted by smoothing spline).  The model selection is based on an
approximation to the  GCV criterion, which is used at each step of the
backfitting procedure.  Once the selection process stops, the model is
backfit using the chosen amount of smoothing.
</p>
<p>A bruto object has the following components of interest:
</p>
<table>
<tr><td><code>lambda</code></td>
<td>
<p>a vector of chosen smoothing parameters, one for each
column of <code>x</code>.</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>the df chosen for each column of <code>x</code>.</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>a factor with levels <code>"excluded"</code>, <code>"linear"</code> or
<code>"smooth"</code>, indicating the status of each column of <code>x</code>.</p>
</td></tr>
<tr><td><code>gcv.select gcv.backfit df.select</code></td>
<td>
<p>The sequence of gcv values and
df selected during the execution of the function.</p>
</td></tr>
<tr><td><code>nit</code></td>
<td>
<p>the number of iterations used.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>a matrix of fitted values.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>a matrix of residuals.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the call that produced this object.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Trevor Hastie and Rob Tibshirani,
<em>Generalized Additive Models</em>,
Chapman and Hall, 1990 (page 262).
</p>
<p>Trevor Hastie, Rob Tibshirani and Andreas Buja
&ldquo;Flexible Discriminant Analysis by Optimal Scoring&rdquo;
JASA 1994, 89, 1255-1270.
</p>


<h3>See Also</h3>

<p><code>predict.bruto</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(trees)
fit1 &lt;- bruto(trees[,-3], trees[3])
fit1$type
fit1$df
## examine the fitted functions
par(mfrow=c(1,2), pty="s")
Xp &lt;- matrix(sapply(trees[1:2], mean), nrow(trees), 2, byrow=TRUE)
for(i in 1:2) {
  xr &lt;- sapply(trees, range)
  Xp1 &lt;- Xp; Xp1[,i] &lt;- seq(xr[1,i], xr[2,i], len=nrow(trees))
  Xf &lt;- predict(fit1, Xp1)
  plot(Xp1[ ,i], Xf, xlab=names(trees)[i], ylab="", type="l")
}
</code></pre>

<hr>
<h2 id='coef.fda'>
Produce coefficients for an fda or mda object
</h2><span id='topic+coef.fda'></span><span id='topic+coef.mda'></span>

<h3>Description</h3>

<p>a method for coef for extracting the canonical coefficients from an fda or mda object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fda'
coef(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef.fda_+3A_object">object</code></td>
<td>
<p>an <code>fda</code> or <code>mda</code> object.
</p>
</td></tr>
<tr><td><code id="coef.fda_+3A_...">...</code></td>
<td>

<p>not relevant
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See the references for details.
</p>


<h3>Value</h3>

<p>A coefficient matrix</p>


<h3>Author(s)</h3>

<p>Trevor Hastie and Robert Tibshirani
</p>


<h3>References</h3>

<p>&ldquo;Flexible Disriminant Analysis by Optimal Scoring&rdquo;  by Hastie,
Tibshirani and Buja, 1994, JASA, 1255-1270.
</p>
<p>&ldquo;Penalized Discriminant Analysis&rdquo; by Hastie, Buja and Tibshirani, 1995,
Annals of Statistics, 73-102.
</p>
<p>&ldquo;Elements of Statisical Learning - Data Mining, Inference and
Prediction&rdquo; (2nd edition, Chapter 12) by Hastie, Tibshirani and
Friedman, 2009, Springer
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.fda">predict.fda</a></code>,
<code><a href="#topic+plot.fda">plot.fda</a></code>,
<code><a href="#topic+mars">mars</a></code>,
<code><a href="#topic+bruto">bruto</a></code>,
<code><a href="#topic+polyreg">polyreg</a></code>,
<code><a href="#topic+softmax">softmax</a></code>,
<code><a href="#topic+confusion">confusion</a></code>,

</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
irisfit &lt;- fda(Species ~ ., data = iris)
coef(irisfit)
mfit=mda(Species~.,data=iris,subclass=2)
coef(mfit)
</code></pre>

<hr>
<h2 id='confusion'>Confusion Matrices</h2><span id='topic+confusion'></span><span id='topic+confusion.default'></span><span id='topic+confusion.list'></span><span id='topic+confusion.fda'></span>

<h3>Description</h3>

<p>Compute the confusion matrix between two factors, or for an fda or
mda object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default S3 method:
confusion(object, true, ...)
## S3 method for class 'fda'
confusion(object, data, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="confusion_+3A_object">object</code></td>
<td>
<p>the predicted factor, or an fda or mda model object.</p>
</td></tr>
<tr><td><code id="confusion_+3A_true">true</code></td>
<td>
<p>the true factor.</p>
</td></tr>
<tr><td><code id="confusion_+3A_data">data</code></td>
<td>
<p>a data frame (list) containing the test data.</p>
</td></tr>
<tr><td><code id="confusion_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a generic function.
</p>


<h3>Value</h3>

<p>For the default method essentially <code>table(object, true)</code>, but
with some useful attribute(s).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fda">fda</a></code>,
<code><a href="#topic+predict.fda">predict.fda</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
irisfit &lt;- fda(Species ~ ., data = iris)
confusion(predict(irisfit, iris), iris$Species)
##            Setosa Versicolor Virginica 
##     Setosa     50          0         0
## Versicolor      0         48         1
##  Virginica      0          2        49
## attr(, "error"):
## [1] 0.02
</code></pre>

<hr>
<h2 id='ESL.mixture'>Mixture example from &quot;Elements of Statistical Learning&quot;</h2><span id='topic+ESL.mixture'></span>

<h3>Description</h3>

<p>A list with training data and other details for the mixture example
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(ESL.mixture)</code></pre>


<h3>Format</h3>

<p>This list contains the following elements:
</p>

<dl>
<dt>x</dt><dd><p>a 200x2 matrix of predictors.</p>
</dd>
<dt>y</dt><dd><p>a 200 vector of y values taking values 0 or 1.</p>
</dd>
<dt>xnew</dt><dd><p>a 6831x2 matrix of prediction points, on a 69x99 grid.</p>
</dd>
<dt>prob</dt><dd><p>a vector of 6831 probabilities - the true probabilities
of a 1 at each point in <code>xnew</code>.</p>
</dd>
<dt>marginal</dt><dd><p>the marginal distribution of the predictors t each
point in <code>xnew</code>.</p>
</dd>
<dt>px1</dt><dd><p>grid values for first coordinate in <code>xnew</code>.</p>
</dd>
<dt>px2</dt><dd><p>grid values for second coordinate in <code>xnew</code>.</p>
</dd>
<dt>means</dt><dd><p>a 20 x 2 matrix of means used in the generation of
these data.</p>
</dd>
</dl>



<h3>Source</h3>

<p>&quot;Elements of Statistical Learning (second edition)&quot;, Hastie, T., Tibshirani, R. and
Friedman, J. (2009), Springer, New York.
<a href="https://hastie.su.domains/ElemStatLearn/">https://hastie.su.domains/ElemStatLearn/</a>
</p>

<hr>
<h2 id='fda'>Flexible Discriminant Analysis</h2><span id='topic+fda'></span><span id='topic+print.fda'></span>

<h3>Description</h3>

<p>Flexible discriminant analysis.</p>


<h3>Usage</h3>

<pre><code class='language-R'>fda(formula, data, weights, theta, dimension, eps, method,
    keep.fitted, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fda_+3A_formula">formula</code></td>
<td>
<p>of the form <code>y~x</code> it describes the response and
the predictors.  The formula can be more complicated, such as
<code>y~log(x)+z</code> etc (see <code><a href="stats.html#topic+formula">formula</a></code> for more details).
The response should be a factor representing the response variable,
or any vector that can be coerced to such (such as a logical
variable).</p>
</td></tr>
<tr><td><code id="fda_+3A_data">data</code></td>
<td>
<p>data frame containing the variables in the formula
(optional).</p>
</td></tr>
<tr><td><code id="fda_+3A_weights">weights</code></td>
<td>
<p>an optional vector of observation weights.</p>
</td></tr>
<tr><td><code id="fda_+3A_theta">theta</code></td>
<td>
<p>an optional matrix of class scores, typically with less
than <code>J-1</code> columns.</p>
</td></tr>
<tr><td><code id="fda_+3A_dimension">dimension</code></td>
<td>
<p>The dimension of the solution, no greater than
<code>J-1</code>, where <code>J</code> is the number classes.  Default is
<code>J-1</code>.</p>
</td></tr>
<tr><td><code id="fda_+3A_eps">eps</code></td>
<td>
<p>a threshold for small singular values for excluding
discriminant variables; default is <code>.Machine$double.eps</code>.</p>
</td></tr>
<tr><td><code id="fda_+3A_method">method</code></td>
<td>
<p>regression method used in optimal scaling.  Default is
linear regression via the function <code>polyreg</code>, resulting in
linear discriminant analysis.  Other possibilities are <code>mars</code>
and <code>bruto</code>.  For Penalized Discriminant analysis
<code>gen.ridge</code> is appropriate.</p>
</td></tr>
<tr><td><code id="fda_+3A_keep.fitted">keep.fitted</code></td>
<td>
<p>a logical variable, which determines whether the
(sometimes large) component <code>"fitted.values"</code> of the <code>fit</code>
component of the returned fda object should be kept.  The default is
<code>TRUE</code> if <code>n * dimension &lt; 5000</code>.</p>
</td></tr>
<tr><td><code id="fda_+3A_...">...</code></td>
<td>
<p>additional arguments to <code>method</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an object of class <code>"fda"</code>.  Use <code>predict</code> to extract
discriminant variables, posterior probabilities or predicted class
memberships.  Other extractor functions are <code>coef</code>,
<code>confusion</code> and <code>plot</code>.
</p>
<p>The object has the following components:
</p>
<table>
<tr><td><code>percent.explained</code></td>
<td>
<p>the percent between-group variance explained
by each dimension (relative to the total explained.)</p>
</td></tr>
<tr><td><code>values</code></td>
<td>
<p>optimal scaling regression sum-of-squares for each
dimension (see reference).  The usual discriminant analysis
eigenvalues are given by <code>values / (1-values)</code>, which are used
to define <code>percent.explained</code>.</p>
</td></tr>
<tr><td><code>means</code></td>
<td>
<p>class means in the discriminant space.  These are also
scaled versions of the final theta's or class scores, and can be
used in a subsequent call to <code>fda</code> (this only makes sense if
some columns of theta are omitted&mdash;see the references).</p>
</td></tr> 
<tr><td><code>theta.mod</code></td>
<td>
<p>(internal) a class scoring matrix which allows
<code>predict</code> to work properly.</p>
</td></tr>
<tr><td><code>dimension</code></td>
<td>
<p>dimension of discriminant space.</p>
</td></tr>
<tr><td><code>prior</code></td>
<td>
<p>class proportions for the training data.</p>
</td></tr>
<tr><td><code>fit</code></td>
<td>
<p>fit object returned by <code>method</code>.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the call that created this object (allowing it to be
<code>update</code>-able)</p>
</td></tr>
<tr><td><code>confusion</code></td>
<td>
<p>confusion matrix when classifying the training data.</p>
</td></tr>
</table>
<p>The <code>method</code> functions are required to take arguments <code>x</code>
and <code>y</code> where both can be matrices, and should produce a matrix
of <code>fitted.values</code> the same size as <code>y</code>.  They can take
additional arguments <code>weights</code> and should all have a <code>...</code>
for safety sake.  Any arguments to <code>method</code> can be passed on via
the <code>...</code> argument of <code>fda</code>.  The default method
<code><a href="#topic+polyreg">polyreg</a></code> has a <code>degree</code> argument which allows
polynomial regression of the required total degree.  See the
documentation for <code><a href="#topic+predict.fda">predict.fda</a></code> for further requirements
of <code>method</code>. The package <code>earth</code> is suggested for this
package as well; <code>earth</code> is a more detailed implementation of
the mars model, and works as a <code>method</code> argument.
</p>


<h3>Author(s)</h3>

<p>Trevor Hastie and Robert Tibshirani
</p>


<h3>References</h3>

<p>&ldquo;Flexible Disriminant Analysis by Optimal Scoring&rdquo;  by Hastie,
Tibshirani and Buja, 1994, JASA, 1255-1270.
</p>
<p>&ldquo;Penalized Discriminant Analysis&rdquo; by Hastie, Buja and Tibshirani, 1995,
Annals of Statistics, 73-102.
</p>
<p>&ldquo;Elements of Statisical Learning - Data Mining, Inference and
Prediction&rdquo; (2nd edition, Chapter 12) by Hastie, Tibshirani and
Friedman, 2009, Springer
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.fda">predict.fda</a></code>,
<code><a href="#topic+plot.fda">plot.fda</a></code>,
<code><a href="#topic+mars">mars</a></code>,
<code><a href="#topic+bruto">bruto</a></code>,
<code><a href="#topic+polyreg">polyreg</a></code>,
<code><a href="#topic+softmax">softmax</a></code>,
<code><a href="#topic+confusion">confusion</a></code>,

</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
irisfit &lt;- fda(Species ~ ., data = iris)
irisfit
## fda(formula = Species ~ ., data = iris)
##
## Dimension: 2 
##
## Percent Between-Group Variance Explained:
##     v1     v2 
##  99.12 100.00 
##
## Degrees of Freedom (per dimension): 5 
##
## Training Misclassification Error: 0.02 ( N = 150 )

confusion(irisfit, iris)
##            Setosa Versicolor Virginica 
##     Setosa     50          0         0
## Versicolor      0         48         1
##  Virginica      0          2        49
## attr(, "error"):
## [1] 0.02

plot(irisfit)

coef(irisfit)
##           [,1]        [,2]
## [1,] -2.126479 -6.72910343
## [2,] -0.837798  0.02434685
## [3,] -1.550052  2.18649663
## [4,]  2.223560 -0.94138258
## [5,]  2.838994  2.86801283

marsfit &lt;- fda(Species ~ ., data = iris, method = mars)
marsfit2 &lt;- update(marsfit, degree = 2)
marsfit3 &lt;- update(marsfit, theta = marsfit$means[, 1:2]) 
## this refits the model, using the fitted means (scaled theta's)
## from marsfit to start the iterations
</code></pre>

<hr>
<h2 id='gen.ridge'>Penalized Regression</h2><span id='topic+gen.ridge'></span><span id='topic+predict.gen.ridge'></span>

<h3>Description</h3>

<p>Perform a penalized regression, as used in penalized discriminant
analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gen.ridge(x, y, weights, lambda=1, omega, df, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gen.ridge_+3A_x">x</code>, <code id="gen.ridge_+3A_y">y</code>, <code id="gen.ridge_+3A_weights">weights</code></td>
<td>
<p>the x and y matrix and possibly a weight vector.</p>
</td></tr>
<tr><td><code id="gen.ridge_+3A_lambda">lambda</code></td>
<td>
<p>the shrinkage penalty coefficient.</p>
</td></tr>
<tr><td><code id="gen.ridge_+3A_omega">omega</code></td>
<td>
<p>a penalty object; omega is the eigendecomposition of
the penalty matrix, and need not have full rank.  By default,
standard ridge is used.</p>
</td></tr>
<tr><td><code id="gen.ridge_+3A_df">df</code></td>
<td>
<p>an alternative way to prescribe lambda, using the notion
of equivalent degrees of freedom.</p>
</td></tr>
<tr><td><code id="gen.ridge_+3A_...">...</code></td>
<td>
<p>currently not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A generalized ridge regression, where the coefficients are penalized
according to omega.  See the function definition for further details.
No functions are provided for producing one dimensional penalty
objects (omega).
<code><a href="#topic+laplacian">laplacian</a>()</code> creates a two-dimensional penalty
object, suitable for (small) images.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+laplacian">laplacian</a></code></p>

<hr>
<h2 id='glass'>Glass Identification Database</h2><span id='topic+glass'></span>

<h3>Description</h3>

<p>The <code>glass</code> data frame has 214 observations and 10 variables,
representing glass fragments.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(glass)</code></pre>


<h3>Format</h3>

<p>This data frame contains the following columns:
</p>

<dl>
<dt>RI</dt><dd><p>refractive index</p>
</dd>
<dt>Na</dt><dd><p>weight percent in corresponding oxide</p>
</dd>
<dt>Mg</dt><dd><p>weight percent in corresponding oxide</p>
</dd>
<dt>Al</dt><dd><p>weight percent in corresponding oxide</p>
</dd>
<dt>Si</dt><dd><p>weight percent in corresponding oxide</p>
</dd>
<dt>K</dt><dd><p>weight percent in corresponding oxide</p>
</dd>
<dt>Ca</dt><dd><p>weight percent in corresponding oxide</p>
</dd>
<dt>Ba</dt><dd><p>weight percent in corresponding oxide</p>
</dd>
<dt>Fe</dt><dd><p>weight percent in corresponding oxide</p>
</dd>
<dt>Type</dt><dd><p>Type of glass:
</p>

<dl>
<dt>1</dt><dd><p>building_windows_float_processed,</p>
</dd>
<dt>2</dt><dd><p>building_windows_non_float_processed,</p>
</dd>
<dt>3</dt><dd><p>vehicle_windows_float_processed,</p>
</dd>
<dt>4</dt><dd><p>vehicle_windows_non_float_processed (none in this
database),</p>
</dd>
<dt>5</dt><dd><p>containers,</p>
</dd>
<dt>6</dt><dd><p>tableware,</p>
</dd>
<dt>7</dt><dd><p>headlamps</p>
</dd>
</dl>

</dd>
</dl>



<h3>Source</h3>

<p>P. M. Murphy and D. W. Aha (1999),
UCI Repository of Machine Learning Databases,
<a href="http://archive.ics.uci.edu/ml/datasets/glass+identification">http://archive.ics.uci.edu/ml/datasets/glass+identification</a>
</p>

<hr>
<h2 id='laplacian'>create penalty object for two-dimensional smoothing.
</h2><span id='topic+laplacian'></span>

<h3>Description</h3>

<p>Creates a penalty matrix  for use by <code>gen.ridge</code> for
two-dimensional smoothing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>laplacian(size, compose)
laplacian(size = 16, compose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="laplacian_+3A_size">size</code></td>
<td>
<p>dimension of the image is <code>size x size</code>; default is 16.</p>
</td></tr>
<tr><td><code id="laplacian_+3A_compose">compose</code></td>
<td>
<p>default is <code>compose=FALSE</code>, which means the
penalty is returned as an eigen-decomposition. If <code>compose=TRUE</code>, a penalty matrix is returned.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Formulas are used to construct a laplacian for smoothing a square image.
</p>


<h3>Value</h3>

<p>If <code>compose=FALSE</code>, an eigen-decomposition object is
returned. The <code>vectors</code> component is a <code>size^2 x size^2</code>
orthogonal matrix, and the <code>$values</code> component is a <code>size^2</code>
vector of non-negative eigen-values. If <code>compose=TRUE</code>, these are
multiplied together to form a single matrix.
</p>


<h3>Author(s)</h3>

<p>Trevor Hastie &lt;hastie@stanford.edu
</p>


<h3>References</h3>

<p>Here we follow very closely the material on page 635 in JASA 1991 of O'Sullivan's article on discretized Laplacian Smoothing
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gen.ridge">gen.ridge</a></code>,<code><a href="#topic+fda">fda</a></code>
</p>

<hr>
<h2 id='mars'>Multivariate Adaptive Regression Splines</h2><span id='topic+mars'></span>

<h3>Description</h3>

<p>Multivariate adaptive regression splines.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mars(x, y, w, wp, degree, nk, penalty, thresh, prune, trace.mars,
     forward.step, prevfit, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mars_+3A_x">x</code></td>
<td>
<p>a matrix containing the independent variables.</p>
</td></tr>
<tr><td><code id="mars_+3A_y">y</code></td>
<td>
<p>a vector containing the response variable, or in the case of
multiple responses, a matrix whose columns are the response values
for each variable.</p>
</td></tr>
<tr><td><code id="mars_+3A_w">w</code></td>
<td>
<p>an optional vector of observation weights (currently ignored).</p>
</td></tr>
<tr><td><code id="mars_+3A_wp">wp</code></td>
<td>
<p>an optional vector of response weights.</p>
</td></tr>
<tr><td><code id="mars_+3A_degree">degree</code></td>
<td>
<p>an optional integer specifying maximum interaction
degree (default is 1).</p>
</td></tr>
<tr><td><code id="mars_+3A_nk">nk</code></td>
<td>
<p>an optional integer specifying the maximum number of model
terms.</p>
</td></tr>
<tr><td><code id="mars_+3A_penalty">penalty</code></td>
<td>
<p>an optional value specifying the cost per degree of
freedom charge (default is 2).</p>
</td></tr>
<tr><td><code id="mars_+3A_thresh">thresh</code></td>
<td>
<p>an optional value specifying forward stepwise stopping
threshold (default is 0.001).</p>
</td></tr>
<tr><td><code id="mars_+3A_prune">prune</code></td>
<td>
<p>an optional logical value specifying whether the model
should be pruned in a backward stepwise fashion (default is
<code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="mars_+3A_trace.mars">trace.mars</code></td>
<td>
<p>an optional logical value specifying whether info
should be printed along the way (default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="mars_+3A_forward.step">forward.step</code></td>
<td>
<p>an optional logical value specifying whether
forward stepwise process should be carried out (default is
<code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="mars_+3A_prevfit">prevfit</code></td>
<td>
<p>optional data structure from previous fit.  To see the
effect of changing the penalty parameter, one can use prevfit with
<code>forward.step = FALSE</code>.</p>
</td></tr>
<tr><td><code id="mars_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"mars"</code>, which is a list with the following
components:
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>call used to <code>mars</code>.</p>
</td></tr>
<tr><td><code>all.terms</code></td>
<td>
<p>term numbers in full model.  <code>1</code> is the constant
term.  Remaining terms are in pairs (<code>2 3</code>, <code>4 5</code>, and so
on).  <code>all.terms</code> indicates nonsingular set of terms.</p>
</td></tr>
<tr><td><code>selected.terms</code></td>
<td>
<p>term numbers in selected model.</p>
</td></tr>
<tr><td><code>penalty</code></td>
<td>
<p>the input penalty value.</p>
</td></tr>
<tr><td><code>degree</code></td>
<td>
<p>the input degree value.</p>
</td></tr>
<tr><td><code>thresh</code></td>
<td>
<p>the input threshold value.</p>
</td></tr>
<tr><td><code>gcv</code></td>
<td>
<p>gcv of chosen model.</p>
</td></tr>
<tr><td><code>factor</code></td>
<td>
<p>matrix with <code class="reqn">ij</code>-th element equal to 1 if term
<code class="reqn">i</code> has a factor of the form <code class="reqn">x_j &gt; c</code>, equal to <code class="reqn">-1</code> if
term <code class="reqn">i</code> has a factor of the form <code class="reqn">x_j \le c</code>, and to 0 if
<code class="reqn">xj</code> is not in term <code class="reqn">i</code>.</p>
</td></tr>
<tr><td><code>cuts</code></td>
<td>
<p>matrix with <code class="reqn">ij</code>-th element equal to the cut point
<code class="reqn">c</code> for variable <code class="reqn">j</code> in term <code class="reqn">i</code>.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>residuals from fit.</p>
</td></tr>
<tr><td><code>fitted</code></td>
<td>
<p>fitted values from fit.</p>
</td></tr>
<tr><td><code>lenb</code></td>
<td>
<p>length of full model.</p>
</td></tr>
<tr><td><code>coefficients</code></td>
<td>
<p>least squares coefficients for final model.</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>a matrix of basis functions obtained from the input x
matrix.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This function was coded from scratch, and did not use any of
Friedman's mars code.  It gives quite similar results to  Friedman's
program in our tests, but not exactly the same results.  We have not 
implemented Friedman's anova decomposition nor are categorical
predictors handled properly yet.  Our version does handle multiple
response variables, however.  </p>


<h3>Author(s)</h3>

<p>Trevor Hastie and Robert Tibshirani
</p>


<h3>References</h3>

<p>J. Friedman, &ldquo;Multivariate Adaptive Regression Splines&rdquo; (with
discussion) (1991). 
<em>Annals of Statistics</em>, <b>19</b>/1, 1&ndash;141.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.mars">predict.mars</a></code>,
<code><a href="#topic+model.matrix.mars">model.matrix.mars</a></code>.
</p>
<p>Package <span class="pkg">earth</span> also provides multivariate adaptive regression
spline models based on the Hastie/Tibshirani mars code in package
<span class="pkg">mda</span>, adding some extra features. It can be used in the
<code>method</code>
argument of <code>fda</code> or <code>mda</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(trees)
fit1 &lt;- mars(trees[,-3], trees[3])
showcuts &lt;- function(obj)
{
  tmp &lt;- obj$cuts[obj$sel, ]
  dimnames(tmp) &lt;- list(NULL, names(trees)[-3])
  tmp
}
showcuts(fit1)

## examine the fitted functions
par(mfrow=c(1,2), pty="s")
Xp &lt;- matrix(sapply(trees[1:2], mean), nrow(trees), 2, byrow=TRUE)
for(i in 1:2) {
  xr &lt;- sapply(trees, range)
  Xp1 &lt;- Xp; Xp1[,i] &lt;- seq(xr[1,i], xr[2,i], len=nrow(trees))
  Xf &lt;- predict(fit1, Xp1)
  plot(Xp1[ ,i], Xf, xlab=names(trees)[i], ylab="", type="l")
}
</code></pre>

<hr>
<h2 id='mda'>Mixture Discriminant Analysis</h2><span id='topic+mda'></span><span id='topic+print.mda'></span>

<h3>Description</h3>

<p>Mixture discriminant analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mda(formula, data, subclasses, sub.df, tot.df, dimension, eps,
    iter, weights, method, keep.fitted, trace, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mda_+3A_formula">formula</code></td>
<td>
<p>of the form <code>y~x</code> it describes the response and
the predictors.  The formula can be more complicated, such as
<code>y~log(x)+z</code> etc (see <code><a href="stats.html#topic+formula">formula</a></code> for more details).
The response should be a factor representing the response variable,
or any vector that can be coerced to such (such as a logical
variable).</p>
</td></tr>
<tr><td><code id="mda_+3A_data">data</code></td>
<td>
<p>data frame containing the variables in the formula
(optional).</p>
</td></tr>
<tr><td><code id="mda_+3A_subclasses">subclasses</code></td>
<td>
<p>Number of subclasses per class, default is 3.  Can be
a vector with a number for each class.</p>
</td></tr>
<tr><td><code id="mda_+3A_sub.df">sub.df</code></td>
<td>
<p>If subclass centroid shrinking is performed, what is the
effective degrees of freedom of the centroids per class.  Can be a
scalar, in which case the same number is used for each class, else a
vector.</p>
</td></tr>
<tr><td><code id="mda_+3A_tot.df">tot.df</code></td>
<td>
<p>The total df for all the centroids can be specified
rather than separately per class.</p>
</td></tr>
<tr><td><code id="mda_+3A_dimension">dimension</code></td>
<td>
<p>The dimension of the reduced model.  If we know our
final model will be confined to a discriminant subspace (of the
subclass centroids), we can specify this in advance and have the EM
algorithm operate in this subspace.</p>
</td></tr>
<tr><td><code id="mda_+3A_eps">eps</code></td>
<td>
<p>A numerical threshold for automatically truncating the
dimension.</p>
</td></tr>
<tr><td><code id="mda_+3A_iter">iter</code></td>
<td>
<p>A limit on the total number of iterations,  default is 5.</p>
</td></tr>
<tr><td><code id="mda_+3A_weights">weights</code></td>
<td>
<p><em>NOT</em> observation weights!  This is a special
weight structure, which for each class assigns a weight (prior
probability) to each of the observations in that class of belonging
to one of the subclasses.  The default is provided by a call to
<code>mda.start(x, g, subclasses, trace, ...)</code> (by this time
<code>x</code> and <code>g</code> are known).  See the help for
<code><a href="#topic+mda.start">mda.start</a></code>.  Arguments for <code>mda.start</code> can be
provided via the <code>...</code> argument to mda, and the
<code>weights</code> argument need never be accessed.  A previously fit
mda object can be supplied, in which case the final subclass
<code>responsibility</code> weights are used for <code>weights</code>.  This 
allows the iterations from a previous fit to be continued.</p>
</td></tr>
<tr><td><code id="mda_+3A_method">method</code></td>
<td>
<p>regression method used in optimal scaling.  Default is
linear regression via the function <code>polyreg</code>, resulting in the
usual mixture model.  Other possibilities are <code>mars</code> and 
<code>bruto</code>.  For penalized mixture discriminant models
<code>gen.ridge</code> is appropriate.</p>
</td></tr>
<tr><td><code id="mda_+3A_keep.fitted">keep.fitted</code></td>
<td>
<p>a logical variable, which determines whether the
(sometimes large) component <code>"fitted.values"</code> of the <code>fit</code>
component of the returned <code>mda</code> object should be kept.  The
default is <code>TRUE</code> if <code>n * dimension &lt; 5000</code>.</p>
</td></tr> 
<tr><td><code id="mda_+3A_trace">trace</code></td>
<td>
<p>if <code>TRUE</code>, iteration information is printed.  Note
that the deviance reported is for the posterior class likelihood,
and not the full likelihood, which is used to drive the EM algorithm
under <code>mda</code>.  In general the latter is not available.</p>
</td></tr>
<tr><td><code id="mda_+3A_...">...</code></td>
<td>
<p>additional arguments to <code>mda.start</code> and to
<code>method</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>c("mda", "fda")</code>.  The most useful extractor
is <code>predict</code>, which can make many types of predictions from this
object.  It can also be plotted, and any functions useful for fda
objects will work here too, such as <code>confusion</code> and <code>coef</code>.
</p>
<p>The object has the following components:
</p>
<table>
<tr><td><code>percent.explained</code></td>
<td>
<p>the percent between-group variance explained
by each dimension (relative to the total explained.)</p>
</td></tr>
<tr><td><code>values</code></td>
<td>
<p>optimal scaling regression sum-of-squares for each
dimension (see reference).</p>
</td></tr>
<tr><td><code>means</code></td>
<td>
<p>subclass means in the discriminant space.  These are also
scaled versions of the final theta's or class scores, and can be
used in a subsequent call to <code>mda</code> (this only makes sense if
some columns of theta are omitted&mdash;see the references)</p>
</td></tr>
<tr><td><code>theta.mod</code></td>
<td>
<p>(internal) a class scoring matrix which allows
<code>predict</code> to work properly.</p>
</td></tr>
<tr><td><code>dimension</code></td>
<td>
<p>dimension of discriminant space.</p>
</td></tr>
<tr><td><code>sub.prior</code></td>
<td>
<p>subclass membership priors, computed in the fit.  No
effort is currently spent in trying to keep these above a threshold.</p>
</td></tr>
<tr><td><code>prior</code></td>
<td>
<p>class proportions for the training data.</p>
</td></tr>
<tr><td><code>fit</code></td>
<td>
<p>fit object returned by <code>method</code>.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the call that created this object (allowing it to be
<code>update</code>-able).</p>
</td></tr>
<tr><td><code>confusion</code></td>
<td>
<p>confusion matrix when classifying the training data.</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>These are the subclass membership probabilities for
each member of the training set; see the weights argument.</p>
</td></tr>
<tr><td><code>assign.theta</code></td>
<td>
<p>a pointer list which identifies which elements of
certain lists belong to individual classes.</p>
</td></tr>
<tr><td><code>deviance</code></td>
<td>
<p>The multinomial log-likelihood of the fit.  Even though
the full log-likelihood drives the iterations, we cannot in general
compute it because of the flexibility of the <code>method</code> used.
The deviance can increase with the iterations, but generally does not.</p>
</td></tr>
</table>
<p>The <code>method</code> functions are required to take arguments <code>x</code>
and <code>y</code> where both can be matrices, and should produce a matrix
of <code>fitted.values</code> the same size as <code>y</code>.  They can take
additional arguments <code>weights</code> and should all have a <code>...</code>
for safety sake.  Any arguments to method() can be passed on via the
<code>...</code> argument of <code>mda</code>.  The default method
<code>polyreg</code> has a <code>degree</code> argument which allows polynomial
regression of the required total degree.  See the documentation for
<code><a href="#topic+predict.fda">predict.fda</a></code> for further requirements of <code>method</code>.
The package <code>earth</code> is suggested for this package as well;
<code>earth</code> is a more detailed implementation of the mars model, and
works as a <code>method</code> argument.
</p>
<p>The function <code>mda.start</code> creates the starting weights; it takes
additional arguments which can be passed in via the <code>...</code>
argument to <code>mda</code>.  See the documentation for <code>mda.start</code>.
</p>


<h3>Author(s)</h3>

<p>Trevor Hastie and Robert Tibshirani
</p>


<h3>References</h3>

<p>&ldquo;Flexible Disriminant Analysis by Optimal Scoring&rdquo; by Hastie,
Tibshirani and Buja, 1994, JASA, 1255-1270.
</p>
<p>&ldquo;Penalized Discriminant Analysis&rdquo; by Hastie, Buja and Tibshirani, 1995,
Annals of Statistics, 73-102
</p>
<p>&ldquo;Discriminant Analysis by Gaussian Mixtures&rdquo; by Hastie and
Tibshirani, 1996, JRSS-B, 155-176.
</p>
<p>&ldquo;Elements of Statisical Learning - Data Mining, Inference and
Prediction&rdquo; (2nd edition, Chapter 12) by Hastie, Tibshirani and
Friedman, 2009, Springer
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.mda">predict.mda</a></code>,
<code><a href="#topic+mars">mars</a></code>,
<code><a href="#topic+bruto">bruto</a></code>,
<code><a href="#topic+polyreg">polyreg</a></code>,
<code><a href="#topic+gen.ridge">gen.ridge</a></code>,
<code><a href="#topic+softmax">softmax</a></code>,
<code><a href="#topic+confusion">confusion</a></code>


</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
irisfit &lt;- mda(Species ~ ., data = iris)
irisfit
## Call:
## mda(formula = Species ~ ., data = iris)
##
## Dimension: 4
##
## Percent Between-Group Variance Explained:
##     v1     v2     v3     v4
##  96.02  98.55  99.90 100.00
##
## Degrees of Freedom (per dimension): 5
##
## Training Misclassification Error: 0.02 ( N = 150 )
##
## Deviance: 15.102

data(glass)
# random sample of size 100
samp &lt;- c(1, 3, 4, 11, 12, 13, 14, 16, 17, 18, 19, 20, 27, 28, 31,
          38, 42, 46, 47, 48, 49, 52, 53, 54, 55, 57, 62, 63, 64, 65,
          67, 68, 69, 70, 72, 73, 78, 79, 83, 84, 85, 87, 91, 92, 94,
          99, 100, 106, 107, 108, 111, 112, 113, 115, 118, 121, 123,
          124, 125, 126, 129, 131, 133, 136, 139, 142, 143, 145, 147,
          152, 153, 156, 159, 160, 161, 164, 165, 166, 168, 169, 171,
          172, 173, 174, 175, 177, 178, 181, 182, 185, 188, 189, 192,
          195, 197, 203, 205, 211, 212, 214) 
glass.train &lt;- glass[samp,]
glass.test &lt;- glass[-samp,]
glass.mda &lt;- mda(Type ~ ., data = glass.train)
predict(glass.mda, glass.test, type="post") # abbreviations are allowed
confusion(glass.mda,glass.test)
</code></pre>

<hr>
<h2 id='mda.start'>Initialization for Mixture Discriminant Analysis</h2><span id='topic+mda.start'></span>

<h3>Description</h3>

<p>Provide starting weights for the <code>mda</code> function which
performs discriminant analysis by gaussian mixtures.</p>


<h3>Usage</h3>

<pre><code class='language-R'>mda.start(x, g, subclasses = 3, trace.mda.start = FALSE,
          start.method = c("kmeans", "lvq"), tries = 5,
          criterion = c("misclassification", "deviance"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mda.start_+3A_x">x</code></td>
<td>
<p>The x data, or an mda object.</p>
</td></tr>
<tr><td><code id="mda.start_+3A_g">g</code></td>
<td>
<p>The response vector g.</p>
</td></tr>
<tr><td><code id="mda.start_+3A_subclasses">subclasses</code></td>
<td>
<p>number of subclasses per class, as in <code>mda</code>.</p>
</td></tr>
<tr><td><code id="mda.start_+3A_trace.mda.start">trace.mda.start</code></td>
<td>
<p>Show results of each iteration.</p>
</td></tr>
<tr><td><code id="mda.start_+3A_start.method">start.method</code></td>
<td>
<p>Either <code>"kmeans"</code> or <code>"lvq"</code>.  The
latter requires package <b>class</b> (from the <b>VR</b> package
bundle.</p>
</td></tr>
<tr><td><code id="mda.start_+3A_tries">tries</code></td>
<td>
<p>Number of random starts.</p>
</td></tr>
<tr><td><code id="mda.start_+3A_criterion">criterion</code></td>
<td>
<p>By default, classification errors on the training
data.  Posterior deviance is also an option.</p>
</td></tr>
<tr><td><code id="mda.start_+3A_...">...</code></td>
<td>
<p>arguments to be passed to the mda fitter when using
posterior deviance.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of weight matrices, one for each class.
</p>

<hr>
<h2 id='model.matrix.mars'>Produce a Design Matrix from a &lsquo;mars&rsquo; Object</h2><span id='topic+model.matrix.mars'></span>

<h3>Description</h3>

<p>Produce a design matrix from a &lsquo;mars&rsquo; object.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mars'
model.matrix(object, x, which, full = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="model.matrix.mars_+3A_object">object</code></td>
<td>
<p>a mars object.</p>
</td></tr>
<tr><td><code id="model.matrix.mars_+3A_x">x</code></td>
<td>
<p>optional argument; if supplied, the mars basis functions are
evaluated at these new observations.</p>
</td></tr>
<tr><td><code id="model.matrix.mars_+3A_which">which</code></td>
<td>
<p>which columns should be used. The default is to use the
columns described by the component <code>selected.terms</code> on
<code>object</code>.</p>
</td></tr>
<tr><td><code id="model.matrix.mars_+3A_full">full</code></td>
<td>
<p>if <code>TRUE</code> the entire set of columns are selected,
even redundant ones.  This is used for updating a mars fit.</p>
</td></tr>
<tr><td><code id="model.matrix.mars_+3A_...">...</code></td>
<td>
<p>further arguments to be passed from or to methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A model matrix corresponding to the selected columns.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mars">mars</a></code>,
<code><a href="#topic+predict.mars">predict.mars</a></code>
</p>

<hr>
<h2 id='mspline'>
Vector Smoothing Spline
</h2><span id='topic+mspline'></span>

<h3>Description</h3>

<p>Fit a smoothing spline to a matrix of responses, single x.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mspline(x, y, w, df = 5, lambda, thresh = 1e-04, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mspline_+3A_x">x</code></td>
<td>

<p>x variable (numeric vector).
</p>
</td></tr>
<tr><td><code id="mspline_+3A_y">y</code></td>
<td>

<p>response matrix.
</p>
</td></tr>
<tr><td><code id="mspline_+3A_w">w</code></td>
<td>

<p>optional weight vector, defaults to a vector of ones.
</p>
</td></tr>
<tr><td><code id="mspline_+3A_df">df</code></td>
<td>

<p>requested degrees of freedom, as in <code>smooth.spline</code>.
</p>
</td></tr>
<tr><td><code id="mspline_+3A_lambda">lambda</code></td>
<td>

<p>can provide penalty instead of df.
</p>
</td></tr>
<tr><td><code id="mspline_+3A_thresh">thresh</code></td>
<td>

<p>convergence threshold for df inversion (to lambda).
</p>
</td></tr>
<tr><td><code id="mspline_+3A_...">...</code></td>
<td>

<p>holdall for other arguments.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is based on the ingredients of <code>smooth.spline</code>,
and allows for simultaneous smoothing of multiple responses
</p>


<h3>Value</h3>

<p>A list is returned, with  a number of components, only some of which
are of interest. These are
</p>
<table>
<tr><td><code>lambda</code></td>
<td>
<p>The value of lambda used (in case df was supplied)</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>The df used (in case lambda was supplied)</p>
</td></tr>
<tr><td><code>s</code></td>
<td>
<p>A matrix like <code>y</code> of smoothed responses</p>
</td></tr>
<tr><td><code>lev</code></td>
<td>
<p>Self influences (diagonal of smoother matrix)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Trevor Hastie
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+smooth.spline">smooth.spline</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x=rnorm(100)
y=matrix(rnorm(100*10),100,10)
fit=mspline(x,y,df=5)
</code></pre>

<hr>
<h2 id='plot.fda'>Plot for Flexible Discriminant Analysis</h2><span id='topic+plot.fda'></span>

<h3>Description</h3>

<p>Plot in discriminant (canonical) coordinates a <code>fda</code> or (by inheritance) a <code>mda</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fda'
plot(x, data, coords, group, colors, pch, mcolors, mpch, pcex, mcex, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.fda_+3A_x">x</code></td>
<td>
<p>an object of class <code>"fda"</code>.</p>
</td></tr>
<tr><td><code id="plot.fda_+3A_data">data</code></td>
<td>
<p>the data to plot in the discriminant coordinates. If
<code>group="true"</code>, then data should be a data frame with the same variables that were used in the fit. If <code>group="predicted"</code>,
<code>data</code> need not contain the response variable, and can in fact
be the correctly-sized <code>"x"</code> matrix.</p>
</td></tr>
<tr><td><code id="plot.fda_+3A_coords">coords</code></td>
<td>
<p>vector of coordinates to plot, with default
<code>coords="c(1,2)"</code>. All pairs of plots are produced.</p>
</td></tr>
<tr><td><code id="plot.fda_+3A_group">group</code></td>
<td>
<p>if <code>group="true"</code> (the default), each point is color
and symbol coded according to the response in <code>data</code>. If
<code>group="predicted"</code>, the class of each point is predicted from
the model, and used instead.</p>
</td></tr>
<tr><td><code id="plot.fda_+3A_colors">colors</code></td>
<td>
<p>a vector of colors to be used in the plotting.</p>
</td></tr>
<tr><td><code id="plot.fda_+3A_pch">pch</code></td>
<td>
<p>a vector of plotting characters.</p>
</td></tr>
<tr><td><code id="plot.fda_+3A_mcolors">mcolors</code></td>
<td>
<p>a vector of colors for the class centroids; default is <code>colors</code>.</p>
</td></tr>
<tr><td><code id="plot.fda_+3A_mpch">mpch</code></td>
<td>
<p>a vector of plotting characters for the centroids.</p>
</td></tr>
<tr><td><code id="plot.fda_+3A_pcex">pcex</code></td>
<td>
<p>character expansion factor for the points; defualt is <code>pcex="0.5"</code>.</p>
</td></tr>
<tr><td><code id="plot.fda_+3A_mcex">mcex</code></td>
<td>
<p>character expansion factor for the centroids; defualt is <code>pcex="2.5"</code>.</p>
</td></tr>
<tr><td><code id="plot.fda_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+fda">fda</a></code>,
<code><a href="#topic+mda">mda</a></code>,
<code><a href="#topic+predict.fda">predict.fda</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
irisfit &lt;- fda(Species ~ ., data = iris)
plot(irisfit)
data(ESL.mixture)
## Not a data frame
mixture.train=ESL.mixture[c("x","y")] 
mixfit=mda(y~x, data=mixture.train)
plot(mixfit, mixture.train)
plot(mixfit, data=ESL.mixture$xnew, group="pred")
</code></pre>

<hr>
<h2 id='polyreg'>Polynomial Regression</h2><span id='topic+polyreg'></span><span id='topic+predict.polyreg'></span>

<h3>Description</h3>

<p>Simple minded polynomial regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>polyreg(x, y, w, degree = 1, monomial = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="polyreg_+3A_x">x</code></td>
<td>
<p>predictor matrix.</p>
</td></tr>
<tr><td><code id="polyreg_+3A_y">y</code></td>
<td>
<p>response matrix.</p>
</td></tr>
<tr><td><code id="polyreg_+3A_w">w</code></td>
<td>
<p>optional (positive) weights.</p>
</td></tr>
<tr><td><code id="polyreg_+3A_degree">degree</code></td>
<td>
<p>total degree of polynomial basis (default is 1).</p>
</td></tr>
<tr><td><code id="polyreg_+3A_monomial">monomial</code></td>
<td>
<p>If <code>TRUE</code> a monomial basis is used (no cross
terms).  Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="polyreg_+3A_...">...</code></td>
<td>
<p>currently not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A polynomial regression fit, containing the essential ingredients for
its <code>predict</code> method.
</p>

<hr>
<h2 id='predict.bruto'>Predict method for BRUTO Objects</h2><span id='topic+predict.bruto'></span>

<h3>Description</h3>

<p>Predicted values based on &lsquo;bruto&rsquo; additive spline models which are fit
by adaptive backfitting.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bruto'
predict(object, newdata, type=c("fitted", "terms"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.bruto_+3A_object">object</code></td>
<td>
<p>a fitted bruto object</p>
</td></tr>
<tr><td><code id="predict.bruto_+3A_newdata">newdata</code></td>
<td>
<p>values at which predictions are to be made.</p>
</td></tr>
<tr><td><code id="predict.bruto_+3A_type">type</code></td>
<td>
<p>if type is <code>"fitted"</code>, the fitted values are
returned.  If type is <code>"terms"</code>, a list of fitted terms is
returned, each with an <code>x</code> and <code>y</code> component.  These can
be used to show the fitted functions.</p>
</td></tr>
<tr><td><code id="predict.bruto_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Either a fit matrix or a list of fitted terms.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bruto">bruto</a></code>,
<code><a href="stats.html#topic+predict">predict</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(trees)
fit1 &lt;- bruto(trees[,-3], trees[3])
fitted.terms &lt;- predict(fit1, as.matrix(trees[,-3]), type = "terms")
par(mfrow=c(1,2), pty="s")
for(tt in fitted.terms) plot(tt, type="l")
</code></pre>

<hr>
<h2 id='predict.fda'>Classify by Flexible Discriminant Analysis</h2><span id='topic+predict.fda'></span>

<h3>Description</h3>

<p>Classify observations in conjunction with <code>fda</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fda'
predict(object, newdata, type, prior, dimension, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.fda_+3A_object">object</code></td>
<td>
<p>an object of class <code>"fda"</code>.</p>
</td></tr>
<tr><td><code id="predict.fda_+3A_newdata">newdata</code></td>
<td>
<p>new data at which to make predictions.  If missing, the
training data is used.</p>
</td></tr>
<tr><td><code id="predict.fda_+3A_type">type</code></td>
<td>
<p>kind of predictions: <code>type = "class"</code> (default)
produces a fitted factor, <code>type = "variates"</code> produces a matrix
of discriminant (canonical) variables, <code>type = "posterior"</code> produces a
matrix of posterior probabilities (based on a gaussian assumption),
and <code>type = "hierarchical"</code> produces the predicted class in
sequence for models of all dimensions.</p>
</td></tr>
<tr><td><code id="predict.fda_+3A_prior">prior</code></td>
<td>
<p>the prior probability vector for each class; the
default is the training sample proportions.</p>
</td></tr>
<tr><td><code id="predict.fda_+3A_dimension">dimension</code></td>
<td>
<p>the dimension of the space to be used, no larger
than the dimension component  of <code>object</code>.</p>
</td></tr>
<tr><td><code id="predict.fda_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An appropriate object depending on <code>type</code>. <code>object</code> has a
component <code>fit</code> which is regression fit produced by the
<code>method</code> argument to <code>fda</code>.  There should be a
<code>predict</code> method for this object which is invoked.  This method
should itself take as input <code>object</code> and optionally <code>newdata</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fda">fda</a></code>,
<code><a href="#topic+mars">mars</a></code>,
<code><a href="#topic+bruto">bruto</a></code>,
<code><a href="#topic+polyreg">polyreg</a></code>,
<code><a href="#topic+softmax">softmax</a></code>,
<code><a href="#topic+confusion">confusion</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
irisfit &lt;- fda(Species ~ ., data = iris)
irisfit
## Call:
## fda(x = iris$x, g = iris$g)
## 
## Dimension: 2 
##
## Percent Between-Group Variance Explained:
##     v1  v2 
##  99.12 100
confusion(predict(irisfit, iris), iris$Species)
##            Setosa Versicolor Virginica
##     Setosa     50          0         0
## Versicolor      0         48         1
##  Virginica      0          2        49
## attr(, "error"):
## [1] 0.02
</code></pre>

<hr>
<h2 id='predict.mars'>Predict method for MARS Objects</h2><span id='topic+predict.mars'></span>

<h3>Description</h3>

<p>Predicted values based on &lsquo;mars&rsquo; multivariate adaptive regression
spline models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mars'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.mars_+3A_object">object</code></td>
<td>
<p>an object of class <code>"mars"</code>.</p>
</td></tr>
<tr><td><code id="predict.mars_+3A_newdata">newdata</code></td>
<td>
<p>values at which predictions are to be made.</p>
</td></tr>
<tr><td><code id="predict.mars_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the fitted values.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mars">mars</a></code>,
<code><a href="stats.html#topic+predict">predict</a></code>,
<code><a href="#topic+model.matrix.mars">model.matrix.mars</a></code>
</p>

<hr>
<h2 id='predict.mda'>Classify by Mixture Discriminant Analysis</h2><span id='topic+predict.mda'></span>

<h3>Description</h3>

<p>Classify observations in conjunction with <code>mda</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mda'
predict(object, newdata, type, prior, dimension, g, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.mda_+3A_object">object</code></td>
<td>
<p>a fitted mda object.</p>
</td></tr>
<tr><td><code id="predict.mda_+3A_newdata">newdata</code></td>
<td>
<p>new data at which to make predictions.  If missing, the
training data is used.</p>
</td></tr>
<tr><td><code id="predict.mda_+3A_type">type</code></td>
<td>
<p>kind of predictions: <code>type = "class"</code> (default)
produces a fitted factor, <code>type = "variates"</code> produces a matrix
of discriminant variables (note that the maximal dimension is
determined by the number of subclasses), <code>type = "posterior"</code>
produces a matrix of posterior probabilities (based on a gaussian
assumption), <code>type = "hierarchical"</code> produces the predicted
class in sequence for models of dimensions specified by
<code>dimension</code> argument.</p>
</td></tr>
<tr><td><code id="predict.mda_+3A_prior">prior</code></td>
<td>
<p>the prior probability vector for each class; the
default is the training sample proportions.</p>
</td></tr>
<tr><td><code id="predict.mda_+3A_dimension">dimension</code></td>
<td>
<p>the dimension of the space to be used, no larger
than the dimension component of <code>object</code>, and in general less
than the number of subclasses.  <code>dimension</code> can be a vector for
use with <code>type = "hierarchical"</code>.</p>
</td></tr>
<tr><td><code id="predict.mda_+3A_g">g</code></td>
<td>
<p>???</p>
</td></tr>
<tr><td><code id="predict.mda_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An appropriate object depending on <code>type</code>.  <code>object</code> has a
component <code>fit</code> which is regression fit produced by the
<code>method</code> argument to <code>mda</code>.  There should be a
<code>predict</code> method for this object which is invoked.  This method
should itself take as input <code>object</code> and optionally <code>newdata</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mda">mda</a></code>,
<code><a href="#topic+fda">fda</a></code>,
<code><a href="#topic+mars">mars</a></code>,
<code><a href="#topic+bruto">bruto</a></code>,
<code><a href="#topic+polyreg">polyreg</a></code>,
<code><a href="#topic+softmax">softmax</a></code>,
<code><a href="#topic+confusion">confusion</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(glass)
samp &lt;- sample(1:nrow(glass), 100)
glass.train &lt;- glass[samp,]
glass.test &lt;- glass[-samp,]
glass.mda &lt;- mda(Type ~ ., data = glass.train)
predict(glass.mda, glass.test, type = "post") # abbreviations are allowed
confusion(glass.mda, glass.test)
</code></pre>

<hr>
<h2 id='softmax'>Find the Maximum in Each Row of a Matrix</h2><span id='topic+softmax'></span>

<h3>Description</h3>

<p>Find the maximum in each row of a matrix.</p>


<h3>Usage</h3>

<pre><code class='language-R'>softmax(x, gap = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="softmax_+3A_x">x</code></td>
<td>
<p>a numeric matrix.</p>
</td></tr>
<tr><td><code id="softmax_+3A_gap">gap</code></td>
<td>
<p>if <code>TRUE</code>, the difference between the largest and
next largest column is returned.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A factor with levels the column labels of <code>x</code> and values the
columns corresponding to the maximum column. If <code>gap = TRUE</code> a
list is returned, the second component of which is the difference
between the largest and next largest column of <code>x</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.fda">predict.fda</a></code>,
<code><a href="#topic+confusion">confusion</a></code>,
<code><a href="#topic+fda">fda</a></code> 
<code><a href="#topic+mda">mda</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
irisfit &lt;- fda(Species ~ ., data = iris)
posteriors &lt;- predict(irisfit, type = "post")
confusion(softmax(posteriors), iris[, "Species"])
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
