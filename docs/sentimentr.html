<!DOCTYPE html><html><head><title>Help for package sentimentr</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {sentimentr}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#as_key'><p>Create/Manipulate Hash Keys</p></a></li>
<li><a href='#available_data'><p>Get Available Data</p></a></li>
<li><a href='#average_downweighted_zero'><p>Downweighted Zeros Averaging</p></a></li>
<li><a href='#combine_data'><p>Combine <span class="pkg">sentimentr</span>'s Sentiment Data Sets</p></a></li>
<li><a href='#course_evaluations'><p>Student Course Evaluation Comments</p></a></li>
<li><a href='#crowdflower_deflategate'><p>Twitter Tweets About the Deflategate</p></a></li>
<li><a href='#crowdflower_products'><p>Twitter Tweets About the Products</p></a></li>
<li><a href='#crowdflower_self_driving_cars'><p>Twitter Tweets About Self Driving Cars</p></a></li>
<li><a href='#crowdflower_weather'><p>Twitter Tweets About the Weather</p></a></li>
<li><a href='#emotion'><p>Compute Emotion Rate</p></a></li>
<li><a href='#emotion_by'><p>Emotion Rate By Groups</p></a></li>
<li><a href='#extract_emotion_terms'><p>Extract Emotion Words</p></a></li>
<li><a href='#extract_profanity_terms'><p>Extract Profanity Words</p></a></li>
<li><a href='#extract_sentiment_terms'><p>Extract Sentiment Words</p></a></li>
<li><a href='#general_rescale'><p>Rescale a Numeric Vector</p></a></li>
<li><a href='#get_sentences'><p>Get Sentences</p></a></li>
<li><a href='#get_sentences2'><p>Get Sentences (Deprecated)</p></a></li>
<li><a href='#highlight'><p>Polarity Text Highlighting</p></a></li>
<li><a href='#hotel_reviews'><p>Hotel Reviews</p></a></li>
<li><a href='#hu_liu_apex_reviews'><p>Apex AD2600 Progressive-scan DVD player Product Reviews From Amazon</p></a></li>
<li><a href='#hu_liu_cannon_reviews'><p>Cannon G3 Camera Product Reviews From Amazon</p></a></li>
<li><a href='#hu_liu_jukebox_reviews'><p>Creative Labs Nomad Jukebox Zen Xtra 40GB Product Reviews From Amazon</p></a></li>
<li><a href='#hu_liu_nikon_reviews'><p>Nikon Coolpix 4300 Product Reviews From Amazon</p></a></li>
<li><a href='#hu_liu_nokia_reviews'><p>Nokia 6610 Product Reviews From Amazon</p></a></li>
<li><a href='#kaggle_movie_reviews'><p>Movie Reviews</p></a></li>
<li><a href='#kotzias_reviews_amazon_cells'><p>Kotzias Reviews: Amazon Cells</p></a></li>
<li><a href='#kotzias_reviews_imdb'><p>Kotzias Reviews: IMBD</p></a></li>
<li><a href='#kotzias_reviews_yelp'><p>Kotzias Reviews: Yelp</p></a></li>
<li><a href='#nyt_articles'><p>Sentiment Scored New York Times Articles</p></a></li>
<li><a href='#plot.emotion'><p>Plots a emotion object</p></a></li>
<li><a href='#plot.emotion_by'><p>Plots a emotion_by object</p></a></li>
<li><a href='#plot.profanity'><p>Plots a profanity object</p></a></li>
<li><a href='#plot.profanity_by'><p>Plots a profanity_by object</p></a></li>
<li><a href='#plot.sentiment'><p>Plots a sentiment object</p></a></li>
<li><a href='#plot.sentiment_by'><p>Plots a sentiment_by object</p></a></li>
<li><a href='#presidential_debates_2012'><p>2012 U.S. Presidential Debates</p></a></li>
<li><a href='#print.extract_emotion_terms'><p>Prints an extract_emotion_terms Object</p></a></li>
<li><a href='#print.extract_profanity_terms'><p>Prints an extract_profanity_terms Object</p></a></li>
<li><a href='#print.extract_sentiment_terms'><p>Prints an extract_sentiment_terms Object</p></a></li>
<li><a href='#print.validate_sentiment'><p>Prints a validate_sentiment Object</p></a></li>
<li><a href='#profanity'><p>Compute Profanity Rate</p></a></li>
<li><a href='#profanity_by'><p>Profanity Rate By Groups</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#sam_i_am'><p>Sam I Am Text</p></a></li>
<li><a href='#sentiment'><p>Polarity Score (Sentiment Analysis)</p></a></li>
<li><a href='#sentiment_attributes'><p>Extract Sentiment Attributes from Text</p></a></li>
<li><a href='#sentiment_by'><p>Polarity Score (Sentiment Analysis) By Groups</p></a></li>
<li><a href='#sentimentr'><p>Calculate Text Polarity Sentiment</p></a></li>
<li><a href='#uncombine'><p>Ungroup a <code>sentiment_by</code> Object to the Sentence Level</p></a></li>
<li><a href='#validate_sentiment'><p>Validate Sentiment Score Sign Against Known Results</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Calculate Text Polarity Sentiment</td>
</tr>
<tr>
<td>Version:</td>
<td>2.9.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Tyler Rinker &lt;tyler.rinker@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Calculate text polarity sentiment at the sentence level and
         optionally aggregate by rows or grouping variable(s).</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.4.0)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat</td>
</tr>
<tr>
<td>Imports:</td>
<td>data.table, ggplot2, graphics, grid, lexicon (&ge; 1.2.1),
methods, stats, stringi, syuzhet, textclean (&ge; 0.6.1),
textshape (&ge; 1.3.0), utils</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/trinker/sentimentr">https://github.com/trinker/sentimentr</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/trinker/sentimentr/issues">https://github.com/trinker/sentimentr/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-10-12 01:16:43 UTC; TylerRinker</td>
</tr>
<tr>
<td>Author:</td>
<td>Tyler Rinker [aut, cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-10-12 08:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='as_key'>Create/Manipulate Hash Keys</h2><span id='topic+as_key'></span><span id='topic+update_key'></span><span id='topic+update_polarity_table'></span><span id='topic+update_valence_shifter_table'></span><span id='topic+is_key'></span>

<h3>Description</h3>

<p><code>as_key</code> - Create your own hash keys from a data frame for use in key
arguments such as <code>polarity_dt</code> in the <code>sentiment</code> function.
</p>
<p><code>update_key</code> - Add/remove terms to a current key.
</p>
<p><code>update_polarity_table</code> - Wrapper for <code>update_key</code> specifically for
updating polarity tables.
</p>
<p><code>update_valence_shifter_table</code> - Wrapper for <code>update_key</code> 
specifically for updating valence shifter tables.
</p>
<p><code>is_key</code> - Logical check if an object is a key.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_key(x, comparison = lexicon::hash_valence_shifters, sentiment = TRUE, ...)

update_key(
  key,
  drop = NULL,
  x = NULL,
  comparison = lexicon::hash_valence_shifters,
  sentiment = FALSE,
  ...
)

update_polarity_table(
  key,
  drop = NULL,
  x = NULL,
  comparison = lexicon::hash_valence_shifters,
  sentiment = FALSE,
  ...
)

update_valence_shifter_table(
  key,
  drop = NULL,
  x = NULL,
  comparison = lexicon::hash_sentiment_jockers_rinker,
  sentiment = FALSE,
  ...
)

is_key(key, sentiment = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as_key_+3A_x">x</code></td>
<td>
<p>A <code><a href="base.html#topic+data.frame">data.frame</a></code> with the first column containing
polarized words and the second containing polarity values.</p>
</td></tr>
<tr><td><code id="as_key_+3A_comparison">comparison</code></td>
<td>
<p>A <code><a href="base.html#topic+data.frame">data.frame</a></code> to compare to <code>x</code>.
If elements in <code>x</code>'s column 1 matches <code>comparison</code>'s column 1 the
accompanying row will be removed from <code>x</code>.  This is useful to ensure
<code>polarity_dt</code> words are not also found in <code>valence_shifters_dt</code> in
<code><a href="#topic+sentiment">sentiment</a></code>.  Use <code>comparison = NULL</code> to skip
this comparison.</p>
</td></tr>
<tr><td><code id="as_key_+3A_sentiment">sentiment</code></td>
<td>
<p>logical.  If <code>TRUE</code> checking expects column 2 of the
input keys/<code><a href="base.html#topic+data.frame">data.frame</a></code> are expected to be numeric.</p>
</td></tr>
<tr><td><code id="as_key_+3A_key">key</code></td>
<td>
<p>A <span class="pkg">sentimentr</span> hash key.</p>
</td></tr>
<tr><td><code id="as_key_+3A_drop">drop</code></td>
<td>
<p>A vector of terms to drop.</p>
</td></tr>
<tr><td><code id="as_key_+3A_...">...</code></td>
<td>
<p>ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For updating keys via <code>update_key</code> note that a 
<code>polarity_dt</code> and <code>valence_shifters_dt</code> are the primary dictionary 
keys used in the <span class="pkg">sentimentr</span> package.  The <code>polarity_dt</code> takes a 
2 column <code>data.frame</code> (named x and y) with the first column being 
character and containing the words and the second column being numeric values
that are positive or negative.  <code>valence_shifters_dt</code> takes a 2 column 
<code>data.frame</code> (named x and y) with the first column being character and 
containing the words and the second column being integer corresponding to:
(1) negators, (2) amplifiers, (3) de-amplifiers, and (4) dversative 
conjunctions (i.e., 'but', 'however', and 'although').  Also, note that if 
you are updating a <code>valence_shifters_dt</code> you need an appropriate 
<code>comparison</code>; most likely, <code>comparison = sentimentr::polarity_dt</code>.
</p>


<h3>Value</h3>

<p>Returns a <span class="pkg">data.table</span> object that can be used as a hash key.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>key &lt;- data.frame(
    words = sample(letters),
    polarity = rnorm(26),
    stringsAsFactors = FALSE
)

(mykey &lt;- as_key(key))

## Looking up values
mykey[c("a", "k")][[2]]

## Drop terms from key
update_key(mykey, drop = c("f", "h"))

## Add terms to key
update_key(mykey, x = data.frame(x = c("dog", "cat"), y = c(1, -1)))

## Add terms &amp; drop to/from a key
update_key(mykey, drop = c("f", "h"), x = data.frame(x = c("dog", "cat"), y = c(1, -1)))

## Explicity key type (wrapper for `update_key` for sentiment table.
## See `update_valence_shifter_table` a corresponding valence shifter updater.
library(lexicon)
updated_hash_sentiment &lt;- sentimentr:::update_polarity_table(lexicon::hash_sentiment_huliu,
    x = data.frame(
        words = c('frickin', 'hairy'),
        polarity = c(-1, -1),
        stringsAsFactors = FALSE
    )
)

## Checking if you have a key
is_key(mykey)
is_key(key)
is_key(mtcars)
is_key(update_key(mykey, drop = c("f", "h")))

## Using syuzhet's sentiment lexicons
## Not run: 
library(syuzhet)
(bing_key &lt;- as_key(syuzhet:::bing))
as_key(syuzhet:::afinn)
as_key(syuzhet:::syuzhet_dict)

sam &lt;- gsub("Sam-I-am", "Sam I am", sam_i_am)
sentiment(sam, , polarity_dt = bing_key)

## The nrc dictionary in syuzhet requires a bit of data wrangling before it 
## is in the correct shape to convert to a key.  

library(syuzhet)
library(tidyverse)

nrc_key &lt;- syuzhet:::nrc %&gt;% 
    dplyr::filter(
        sentiment %in% c('positive', 'negative'),
        lang == 'english'
    ) %&gt;%
    dplyr::select(-lang) %&gt;% 
    mutate(value = ifelse(sentiment == 'negative', value * -1, value)) %&gt;%
    dplyr::group_by(word) %&gt;%
    dplyr::summarize(y = mean(value)) %&gt;%
    sentimentr::as_key()
    
sentiment(sam, polarity_dt = nrc_key)

## The lexicon package contains a preformatted nrc sentiment hash table that 
## can be used instead.
sentiment(sam, polarity_dt = lexicon::hash_sentiment_nrc)

## End(Not run)

## Using 2 vectors of words
## Not run: 
install.packages("tm.lexicon.GeneralInquirer", repos="http://datacube.wu.ac.at", type="source")
require("tm.lexicon.GeneralInquirer")

positive &lt;- terms_in_General_Inquirer_categories("Positiv")
negative &lt;- terms_in_General_Inquirer_categories("Negativ")

geninq &lt;- data.frame(
    x = c(positive, negative),
    y = c(rep(1, length(positive)), rep(-1, length(negative))),
    stringsAsFactors = FALSE
) %&gt;%
    as_key()

geninq_pol &lt;- with(presidential_debates_2012,
    sentiment_by(dialogue,
    person,
    polarity_dt = geninq
))

geninq_pol %&gt;% plot()

## End(Not run)
</code></pre>

<hr>
<h2 id='available_data'>Get Available Data</h2><span id='topic+available_data'></span><span id='topic+sentimentr_data'></span>

<h3>Description</h3>

<p>See available <span class="pkg">sentimentr</span> data a data.frame.  Note that 
<code>sentimentr_data</code> is the main function to be used but 
<code>available_data</code> is exposed to allow other packages to use the
functionality in a generic way.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>available_data(regex = NULL, package = "sentimentr", ...)

sentimentr_data(regex = NULL, package = "sentimentr", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="available_data_+3A_regex">regex</code></td>
<td>
<p>A regex to search for within the data columns.</p>
</td></tr>
<tr><td><code id="available_data_+3A_package">package</code></td>
<td>
<p>The name of the package to extract data from.</p>
</td></tr>
<tr><td><code id="available_data_+3A_...">...</code></td>
<td>
<p>Other arguments passed to <code>grep</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a data.frame
</p>


<h3>Examples</h3>

<pre><code class='language-R'>sentimentr_data()
available_data() ## generic version for export
available_data(package = 'datasets')
sentimentr_data('^hu')
sentimentr_data('^(hu|kot)')
combine_data(sentimentr_data('^(hu|kot)')[[1]])

## Not run: 
if (!require("pacman")) install.packages("pacman")
pacman::p_load(sentimentr, tidyverse, magrittr)

sentiment_data &lt;- sentimentr_data('^hu') %&gt;%
    pull(Data) %&gt;%
    combine_data() %&gt;%
    mutate(id = seq_len(n())) %&gt;%
    as_tibble()
    
sentiment_test &lt;- sentiment_data %&gt;%
    select(-sentiment) %&gt;%
    get_sentences() %$%
    sentiment(., by = c('id'))

testing &lt;- sentiment_data %&gt;%
    left_join(sentiment_test, by = 'id') %&gt;%
    as_tibble() %&gt;%
    mutate(
        actual = sign(sentiment),
        predicted = sign(ave_sentiment)
    )

testing %$%
    ftable(predicted, actual)

## End(Not run)
</code></pre>

<hr>
<h2 id='average_downweighted_zero'>Downweighted Zeros Averaging</h2><span id='topic+average_downweighted_zero'></span><span id='topic+average_weighted_mixed_sentiment'></span><span id='topic+average_mean'></span>

<h3>Description</h3>

<p><code>average_downweighted_zero</code>- Downweight the zeros in a vector for 
averaging.  This is useful in the context of language where we don't want the 
neutral sentences to have such a strong influence on the general sentiment of 
the discourse with multiple sentences.  Essentially, this means neutral 
sentences are seen as having less emotional impact than a polarized sentence.
</p>
<p><code>average_weighted_mixed_sentiment</code>- Upweight the negative values in a 
vector while also downweighting the zeros in a vector.  Useful for small text
chunks with several sentences in which some one states a negative sentence
but then uses the social convention of several positive sentences in an 
attempt to negate the impact of the negative.  The affective state isn't
a neutral but a slightly lessened negative state.
</p>
<p><code>average_mean</code>- Standard mean averaging with <code>na.rm</code> set to <code>TRUE</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>average_downweighted_zero(x, na.rm = TRUE, ...)

average_weighted_mixed_sentiment(
  x,
  mixed.less.than.zero.weight = 4,
  na.rm = TRUE,
  ...
)

average_mean(x, na.rm = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="average_downweighted_zero_+3A_x">x</code></td>
<td>
<p>A numeric vector.</p>
</td></tr>
<tr><td><code id="average_downweighted_zero_+3A_na.rm">na.rm</code></td>
<td>
<p>logical.  Should <code>NA</code> values should be stripped before the 
computation proceeds.</p>
</td></tr>
<tr><td><code id="average_downweighted_zero_+3A_mixed.less.than.zero.weight">mixed.less.than.zero.weight</code></td>
<td>
<p>The weighting factor to multiply the 
negative elements of the vector by (this increases the intensity of the 
negatives in the numerator of the mean formula).</p>
</td></tr>
<tr><td><code id="average_downweighted_zero_+3A_...">...</code></td>
<td>
<p>ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a scalar summary of the re-weighted average
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c(1, 2, 0, 0, 0, -1)
mean(x)
average_downweighted_zero(x)
average_downweighted_zero(c(NA, x))
mean(c(0, 0, 0, x))
average_downweighted_zero(c(0, 0, 0, x))
</code></pre>

<hr>
<h2 id='combine_data'>Combine <span class="pkg">sentimentr</span>'s Sentiment Data Sets</h2><span id='topic+combine_data'></span>

<h3>Description</h3>

<p>Combine trusted sentiment data sets from <span class="pkg">sentimentr</span>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>combine_data(
  data = c("course_evaluations", "hotel_reviews", "kaggle_movie_reviews",
    "kotzias_reviews_amazon_cells", "kotzias_reviews_imdb", "kotzias_reviews_yelp",
    "nyt_articles"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="combine_data_+3A_data">data</code></td>
<td>
<p>A character vector of <span class="pkg">sentimentr</span> data sets.</p>
</td></tr>
<tr><td><code id="combine_data_+3A_...">...</code></td>
<td>
<p>ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an rbinded <span class="pkg">data.table</span> of sentiment data with the source
added as column.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>combine_data()
combine_data(c("kotzias_reviews_amazon_cells", "kotzias_reviews_imdb", 
    "kotzias_reviews_yelp"))
</code></pre>

<hr>
<h2 id='course_evaluations'>Student Course Evaluation Comments</h2><span id='topic+course_evaluations'></span>

<h3>Description</h3>

<p>A dataset containing a subset of comments and rating from Welch &amp; Mihalcea's 
(2017) data set filtered to include comments with a one or more unambiguous 
sentiment rating.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(course_evaluations)
</code></pre>


<h3>Format</h3>

<p>A data frame with 566 rows and 2 variables
</p>


<h3>Details</h3>

 
<ul>
<li><p> sentiment. A numeric sentiment score
</p>
</li>
<li><p> text. The text from the evaluation
</p>
</li></ul>



<h3>References</h3>

<p>Welch, C. and Mihalcea, R. (2017). Targeted sentiment to 
understand student comments. In Proceedings of the International Conference 
on Computational Linguistics (COLING 2016). <br /> <br />
Original URL: http://web.eecs.umich.edu/~mihalcea/downloads.html#GroundedEmotions
</p>

<hr>
<h2 id='crowdflower_deflategate'>Twitter Tweets About the Deflategate</h2><span id='topic+crowdflower_deflategate'></span>

<h3>Description</h3>

<p>A dataset containing Twitter tweets about Tom Brady's deflated ball scandal, 
taken from Crowdflower.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(crowdflower_deflategate)
</code></pre>


<h3>Format</h3>

<p>A data frame with 11,786 rows and 2 variables
</p>


<h3>Details</h3>

 
<ul>
<li><p> sentiment. A human scoring of the text.
</p>
</li>
<li><p> text. The sentences from the tweet.
</p>
</li></ul>



<h3>References</h3>

<p>Original URL: https://www.crowdflower.com/data-for-everyone
</p>

<hr>
<h2 id='crowdflower_products'>Twitter Tweets About the Products</h2><span id='topic+crowdflower_products'></span>

<h3>Description</h3>

<p>A dataset containing Twitter tweets about various products, taken from 
Crowdflower.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(crowdflower_products)
</code></pre>


<h3>Format</h3>

<p>A data frame with 3,548 rows and 2 variables
</p>


<h3>Details</h3>

 
<ul>
<li><p> sentiment. A human scoring of the text.
</p>
</li>
<li><p> text. The sentences from the tweet.
</p>
</li></ul>



<h3>References</h3>

<p>Cavender-Bares, K., (2013). Judge emotion about brands &amp; products. <br /> <br />
Original URL: https://www.crowdflower.com/data-for-everyone
</p>

<hr>
<h2 id='crowdflower_self_driving_cars'>Twitter Tweets About Self Driving Cars</h2><span id='topic+crowdflower_self_driving_cars'></span>

<h3>Description</h3>

<p>A dataset containing Twitter tweets about self driving cars, taken from
Crowdflower.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(crowdflower_self_driving_cars)
</code></pre>


<h3>Format</h3>

<p>A data frame with 6,943 rows and 2 variables
</p>


<h3>Details</h3>


<ul>
<li><p> sentiment. A human scoring of the text.
</p>
</li>
<li><p> text. The sentences from the tweet.
</p>
</li></ul>



<h3>References</h3>

<p>Original URL: https://www.crowdflower.com/data-for-everyone
</p>

<hr>
<h2 id='crowdflower_weather'>Twitter Tweets About the Weather</h2><span id='topic+crowdflower_weather'></span>

<h3>Description</h3>

<p>A dataset containing Twitter tweets about the weather, taken from
Crowdflower.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(crowdflower_weather)
</code></pre>


<h3>Format</h3>

<p>A data frame with 763 rows and 2 variables
</p>


<h3>Details</h3>

 
<ul>
<li><p> sentiment. A human scoring of the text.
</p>
</li>
<li><p> text. The sentences from the tweet. 
</p>
</li></ul>



<h3>References</h3>

<p>Original URL: https://www.crowdflower.com/data-for-everyone
</p>

<hr>
<h2 id='emotion'>Compute Emotion Rate</h2><span id='topic+emotion'></span>

<h3>Description</h3>

<p>Detect the rate of emotion at the sentence level.  This method uses a simple
dictionary lookup to find emotion words and then compute the rate per sentence.
The <code>emotion</code> score ranges between 0 (no emotion used) and 1 (all
words used were emotional).  Note that a single emotion phrase would count as 
just one in the <code>emotion_count</code> column but would count as two words in
the <code>word_count</code> column.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>emotion(
  text.var,
  emotion_dt = lexicon::hash_nrc_emotions,
  valence_shifters_dt = lexicon::hash_valence_shifters,
  drop.unused.emotions = FALSE,
  un.as.negation = TRUE,
  un.as.negation.warn = isTRUE(all.equal(valence_shifters_dt,
    lexicon::hash_nrc_emotions)),
  n.before = 5,
  n.after = 2,
  retention_regex = "[^[:alpha:];:,']",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="emotion_+3A_text.var">text.var</code></td>
<td>
<p>The text variable.  Can be a <code>get_sentences</code> object or
a raw character vector though <code>get_sentences</code> is preferred as it avoids
the repeated cost of doing sentence boundary disambiguation every time
<code>sentiment</code> is run.</p>
</td></tr>
<tr><td><code id="emotion_+3A_emotion_dt">emotion_dt</code></td>
<td>
<p>A <span class="pkg">data.table</span> with a <code>token</code> and <code>emotion</code>
column (<code>tokens</code> are nested within the <code>emotion</code>s.  The table
cannot contain any duplicate rows and must have the <code>token</code> column set
as the key column (see <code>?data.table::setkey</code>).  The default emotion
table is <code>lexicon::hash_nrc_emotions</code>.</p>
</td></tr>
<tr><td><code id="emotion_+3A_valence_shifters_dt">valence_shifters_dt</code></td>
<td>
<p>A <span class="pkg">data.table</span> of valence shifters that
can alter a polarized word's meaning and an integer key for negators (1),
amplifiers [intensifiers] (2), de-amplifiers [downtoners] (3) and adversative 
conjunctions (4) with x and y as column names.  For this purpose only 
negators is required/used.</p>
</td></tr>
<tr><td><code id="emotion_+3A_drop.unused.emotions">drop.unused.emotions</code></td>
<td>
<p>logical.  If <code>TRUE</code> unused/unfound emotion
levels will not be included in the output.</p>
</td></tr>
<tr><td><code id="emotion_+3A_un.as.negation">un.as.negation</code></td>
<td>
<p>logical.  If <code>TRUE</code> then emotion words prefixed
with an 'un-' are treated as a negation.  For example,<code>"unhappy"</code> would 
be treated as <code>"not happy"</code>.  If an emotion word has an un- version in the
<code>emotion_dt</code> then no substitution is performed and an optional warning
will be given.</p>
</td></tr>
<tr><td><code id="emotion_+3A_un.as.negation.warn">un.as.negation.warn</code></td>
<td>
<p>logical.  If <code>TRUE</code> and if 
<code>un.as.negation</code> id <code>TRUE</code>, then a warning will be given if the 
-un version of an emotion term is already found within the <code>emotion_dt</code>.
Note that the default <code>emotion_dt</code>, <code>lexicon::hash_nrc_emotions</code>, 
will not give a warning unless it is explicitly set to do so.  There are
a number of emotion words in <code>lexicon::hash_nrc_emotions</code> that contain
un- prefixed versions already in the dictionary. Use:
<code>emotion('', un.as.negation.warn = TRUE)</code> to see these un- prefixed
emotion words that are contained within <code>lexicon::hash_nrc_emotions</code>.</p>
</td></tr>
<tr><td><code id="emotion_+3A_n.before">n.before</code></td>
<td>
<p>The number of words to consider as negated before
the emotion word.  To consider the entire beginning portion of a sentence
use <code>n.before = Inf</code>.  Note that a comma, colon, or semicolon acts as a 
boundary for considered words.  Only words between the emotion word and these
punctuation types will be considered.</p>
</td></tr>
<tr><td><code id="emotion_+3A_n.after">n.after</code></td>
<td>
<p>The number of words to consider as negated after
the emotion word.  To consider the entire ending portion of a sentence
use <code>n.after = Inf</code>.  Note that a comma, colon, or semicolon acts as a 
boundary for considered words.  Only words between the emotion word and these
punctuation types will be considered.</p>
</td></tr>
<tr><td><code id="emotion_+3A_retention_regex">retention_regex</code></td>
<td>
<p>A regex of what characters to keep.  All other 
characters will be removed.  Note that when this is used all text is lower 
case format.  Only adjust this parameter if you really understand how it is 
used.  Note that swapping the <code>\\{p}</code> for <code>[^[:alpha:];:,\']</code> may 
retain more alpha letters but will likely decrease speed.</p>
</td></tr>
<tr><td><code id="emotion_+3A_...">...</code></td>
<td>
<p>ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a <span class="pkg">data.table</span> of:
</p>

<ul>
<li><p> element_id - The id number of the original vector passed to <code>emotion</code>
</p>
</li>
<li><p> sentence_id - The id number of the sentences within each <code>element_id</code>
</p>
</li>
<li><p> word_count - Word count
</p>
</li>
<li><p> emotion_type - Type designation from the <code>emotion</code> column of the <code>emotion_dt</code> table
</p>
</li>
<li><p> emotion_count - Count of the number of emotion words of that <code>emotion_type</code>
</p>
</li>
<li><p> emotion - A score of the percentage of emotion words of that <code>emotion_type</code>
</p>
</li></ul>



<h3>References</h3>

<p>Plutchik, R. (1962). The emotions: Facts and theories, and a new 
model. Random House studies in psychology. Random House.<br /><br />
Plutchik, R. (2001). The nature of emotions: Human emotions have deep 
evolutionary roots, a fact that may explain their complexity and provide tools 
for clinical practice. American Scientist , 89 (4), 344-350.
</p>


<h3>See Also</h3>

<p>Other emotion functions: 
<code><a href="#topic+emotion_by">emotion_by</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>mytext &lt;- c(
    "I am not afraid of you",
    NA,
    "",
    "I love it [not really]", 
    "I'm not angry with you", 
    "I hate it when you lie to me.  It's so humiliating",
    "I'm not happpy anymore.  It's time to end it",
    "She's a darn good friend to me",
    "I went to the terrible store",
    "There is hate and love in each of us",
    "I'm no longer angry!  I'm really experiencing peace but not true joy.",
    
    paste("Out of the night that covers me, Black as the Pit from pole to", 
      "pole, I thank whatever gods may be For my unconquerable soul."
     ),
    paste("In the fell clutch of circumstance I have not winced nor cried",
        "aloud. Under the bludgeonings of chance My head is bloody, but unbowed."
    ),
    paste("Beyond this place of wrath and tears Looms but the Horror of the", 
        "shade, And yet the menace of the years Finds, and shall find, me unafraid."
    ),
    paste("It matters not how strait the gate, How charged with punishments", 
        "the scroll, I am the master of my fate: I am the captain of my soul."
    )    
    
)

## works on a character vector but not the preferred method avoiding the 
## repeated cost of doing sentence boundary disambiguation every time 
## `emotion` is run
emotion(mytext)

## preferred method avoiding paying the cost 
split_text &lt;- get_sentences(mytext)
(emo &lt;- emotion(split_text))
emotion(split_text, drop.unused.emotions = TRUE)

## Not run: 
plot(emo)
plot(emo, drop.unused.emotions = FALSE)
plot(emo, facet = FALSE)
plot(emo, facet = 'negated')

library(data.table)
fear &lt;- emo[
    emotion_type == 'fear', ][, 
    text := unlist(split_text)][]
    
fear[emotion &gt; 0,]

brady &lt;- get_sentences(crowdflower_deflategate)
brady_emotion &lt;- emotion(brady)
brady_emotion

## End(Not run)
</code></pre>

<hr>
<h2 id='emotion_by'>Emotion Rate By Groups</h2><span id='topic+emotion_by'></span>

<h3>Description</h3>

<p>Approximate the emotion of text by grouping variable(s).  For a
full description of the emotion detection algorithm see 
<code><a href="#topic+emotion">emotion</a></code>.  See <code><a href="#topic+emotion">emotion</a></code>
for more details about the algorithm, the emotion/valence shifter keys
that can be passed into the function, and other arguments that can be passed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>emotion_by(text.var, by = NULL, group.names, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="emotion_by_+3A_text.var">text.var</code></td>
<td>
<p>The text variable.  Also takes a <code>emotionr</code> or
<code>emotion_by</code> object.</p>
</td></tr>
<tr><td><code id="emotion_by_+3A_by">by</code></td>
<td>
<p>The grouping variable(s).  Default <code>NULL</code> uses the original
row/element indices; if you used a column of 12 rows for <code>text.var</code>
these 12 rows will be used as the grouping variable.  Also takes a single
grouping variable or a list of 1 or more grouping variables.</p>
</td></tr>
<tr><td><code id="emotion_by_+3A_group.names">group.names</code></td>
<td>
<p>A vector of names that corresponds to group.  Generally
for internal use.</p>
</td></tr>
<tr><td><code id="emotion_by_+3A_...">...</code></td>
<td>
<p>Other arguments passed to <code><a href="#topic+emotion">emotion</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a <span class="pkg">data.table</span> with grouping variables plus:
</p>

<ul>
<li><p>  element_id - The id number of the original vector passed to <code>emotion</code>
</p>
</li>
<li><p>  sentence_id - The id number of the sentences within each <code>element_id</code>
</p>
</li>
<li><p>  word_count - Word count <code><a href="base.html#topic+sum">sum</a></code>med by grouping variable
</p>
</li>
<li><p> emotion_type - Type designation from the <code>emotion</code> column of the <code>emotion_dt</code> table
</p>
</li>
<li><p>  emotion_count - The number of profanities used by grouping variable
</p>
</li>
<li><p>  sd - Standard deviation (<code><a href="stats.html#topic+sd">sd</a></code>) of the sentence level emotion rate by grouping variable
</p>
</li>
<li><p>  ave_emotion - Emotion rate
</p>
</li></ul>



<h3>Chaining</h3>

<p>See the  <code><a href="#topic+sentiment_by">sentiment_by</a></code> for details about <span class="pkg">sentimentr</span> chaining.
</p>


<h3>See Also</h3>

<p>Other emotion functions: 
<code><a href="#topic+emotion">emotion</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
mytext &lt;- c(
    "I am not afraid of you",
    NA,
    "",
    "I love it [not really]", 
    "I'm not angry with you", 
    "I hate it when you lie to me.  It's so humiliating",
    "I'm not happpy anymore.  It's time to end it",
    "She's a darn good friend to me",
    "I went to the terrible store",
    "There is hate and love in each of us",
    "I'm no longer angry!  I'm really experiencing peace but not true joy.",
    
    paste("Out of the night that covers me, Black as the Pit from pole to", 
      "pole, I thank whatever gods may be For my unconquerable soul.",
      "In the fell clutch of circumstance I have not winced nor cried",
      "aloud. Under the bludgeonings of chance My head is bloody, but unbowed.",
      "Beyond this place of wrath and tears Looms but the Horror of the", 
      "shade, And yet the menace of the years Finds, and shall find, me unafraid.",
      "It matters not how strait the gate, How charged with punishments", 
      "the scroll, I am the master of my fate: I am the captain of my soul."
    )    
    
)

## works on a character vector but not the preferred method avoiding the 
## repeated cost of doing sentence boundary disambiguation every time 
## `emotion` is run
emotion(mytext)
emotion_by(mytext)

## preferred method avoiding paying the cost 
mytext &lt;- get_sentences(mytext)

emotion_by(mytext)
get_sentences(emotion_by(mytext))

(myemotion &lt;- emotion_by(mytext))
stats::setNames(get_sentences(emotion_by(mytext)),
    round(myemotion[["ave_emotion"]], 3))

pres &lt;- get_sentences(presidential_debates_2012)
pres_emo_sent &lt;- emotion_by(pres)

## method 1
pres_emo_per_time &lt;- presidential_debates_2012 %&gt;%
    get_sentences() %&gt;%
    emotion_by(by = c('person', 'time'))
    
pres_emo_per_time

## method 2
library(magrittr)
presidential_debates_2012 %&gt;%
    get_sentences() %$%
    emotion_by(., by = c('person', 'time'))

## method 3
presidential_debates_2012 %&gt;%
    get_sentences() %$%
    emotion_by(dialogue, by = list(person, time))

## method 4
presidential_debates_2012 %&gt;%
    get_sentences() %&gt;%
    with(emotion_by(dialogue, by = list(person, time)))

plot(pres_emo_sent)
plot(pres_emo_per_time)

## End(Not run)
</code></pre>

<hr>
<h2 id='extract_emotion_terms'>Extract Emotion Words</h2><span id='topic+extract_emotion_terms'></span>

<h3>Description</h3>

<p>Extract the emotion words from a text.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_emotion_terms(
  text.var,
  emotion_dt = lexicon::hash_nrc_emotions,
  un.as.negation = TRUE,
  retention_regex = "[^[:alpha:];:,']",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_emotion_terms_+3A_text.var">text.var</code></td>
<td>
<p>The text variable.  Can be a <code>get_sentences</code> object or
a raw character vector though <code>get_sentences</code> is preferred as it avoids
the repeated cost of doing sentence boundary disambiguation every time
<code>emotion</code> is run.</p>
</td></tr>
<tr><td><code id="extract_emotion_terms_+3A_emotion_dt">emotion_dt</code></td>
<td>
<p>A <span class="pkg">data.table</span> with a <code>token</code> and <code>emotion</code>
column (<code>tokens</code> are nested within the <code>emotion</code>s.  The table
cannot contain any duplicate rows and must have the <code>token</code> column set
as the key column (see <code>?data.table::setkey</code>).  The default emotion
table is <code>lexicon::hash_nrc_emotions</code>.</p>
</td></tr>
<tr><td><code id="extract_emotion_terms_+3A_un.as.negation">un.as.negation</code></td>
<td>
<p>logical.  If <code>TRUE</code> then emotion words prefixed
with an 'un-' are treated as a negation.  For example,<code>"unhappy"</code> would 
be treated as <code>"not happy"</code>.  If an emotion word has an un- version in the
<code>emotion_dt</code> then no substitution is performed and an optional warning
will be given.</p>
</td></tr>
<tr><td><code id="extract_emotion_terms_+3A_retention_regex">retention_regex</code></td>
<td>
<p>A regex of what characters to keep.  All other 
characters will be removed.  Note that when this is used all text is lower 
case format.  Only adjust this parameter if you really understand how it is 
used.  Note that swapping the <code>\\{p}</code> for <code>[^[:alpha:];:,\']</code> may 
retain more alpha letters but will likely decrease speed.</p>
</td></tr>
<tr><td><code id="extract_emotion_terms_+3A_...">...</code></td>
<td>
<p>Ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a <span class="pkg">data.table</span> with a columns of emotion terms.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
mytext &lt;- c(
    "I am not afraid of you",
    NA,
    "",
    "I love it [not really]", 
    "I'm not angry with you", 
    "I hate it when you lie to me.  It's so humiliating",
    "I'm not happpy anymore.  It's time to end it",
    "She's a darn good friend to me",
    "I went to the terrible store",
    "There is hate and love in each of us",
    "I'm no longer angry!  I'm really experiencing peace but not true joy.",
    
    paste("Out of the night that covers me, Black as the Pit from pole to", 
      "pole, I thank whatever gods may be For my unconquerable soul.",
      "In the fell clutch of circumstance I have not winced nor cried",
      "aloud. Under the bludgeonings of chance My head is bloody, but unbowed.",
      "Beyond this place of wrath and tears Looms but the Horror of the", 
      "shade, And yet the menace of the years Finds, and shall find, me unafraid.",
      "It matters not how strait the gate, How charged with punishments", 
      "the scroll, I am the master of my fate: I am the captain of my soul."
    )    
    
)

mytext2 &lt;- get_sentences(mytext)
emotion(mytext2)

emo_words &lt;- extract_emotion_terms(mytext2)
emo_words
emo_words$sentence
emo_words[, c('anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust')]

attributes(emo_words)$counts
attributes(emo_words)$elements

## directly ona  character string (not recommended: use `get_sentences` first)
extract_emotion_terms(mytext)

brady &lt;- get_sentences(crowdflower_deflategate)
brady_emo &lt;- extract_emotion_terms(brady)

brady_emo
attributes(brady_emo)$counts
attributes(brady_emo)$elements

## End(Not run)
</code></pre>

<hr>
<h2 id='extract_profanity_terms'>Extract Profanity Words</h2><span id='topic+extract_profanity_terms'></span>

<h3>Description</h3>

<p>Extract the profanity words from a text.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_profanity_terms(
  text.var,
  profanity_list = unique(tolower(lexicon::profanity_alvarez)),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_profanity_terms_+3A_text.var">text.var</code></td>
<td>
<p>The text variable.  Can be a <code>get_sentences</code> object or
a raw character vector though <code>get_sentences</code> is preferred as it avoids
the repeated cost of doing sentence boundary disambiguation every time
<code>profanity</code> is run.</p>
</td></tr>
<tr><td><code id="extract_profanity_terms_+3A_profanity_list">profanity_list</code></td>
<td>
<p>A atomic character vector of profane words.  The 
<span class="pkg">lexicon</span> package has lists that can be used, including: 
</p>

<ul>
<li> <p><code>lexicon::profanity_alvarez</code>
</p>
</li>
<li> <p><code>lexicon::profanity_arr_bad</code>
</p>
</li>
<li> <p><code>lexicon::profanity_banned</code>
</p>
</li>
<li> <p><code>lexicon::profanity_zac_anger</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="extract_profanity_terms_+3A_...">...</code></td>
<td>
<p>Ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a <span class="pkg">data.table</span> with a columns of profane terms.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
bw &lt;- sample(lexicon::profanity_alvarez, 4)
mytext &lt;- c(
   sprintf('do you %s like this %s?  It is %s. But I hate really bad dogs', bw[1], bw[2], bw[3]),
   'I am the best friend.',
   NA,
   sprintf('I %s hate this %s', bw[3], bw[4]),
   "Do you really like it?  I'm not happy"
)


x &lt;- get_sentences(mytext)
profanity(x)

prof_words &lt;- extract_profanity_terms(x)
prof_words
prof_words$sentence
prof_words$neutral
prof_words$profanity
data.table::as.data.table(prof_words)

attributes(extract_profanity_terms(x))$counts
attributes(extract_profanity_terms(x))$elements


brady &lt;- get_sentences(crowdflower_deflategate)
brady_swears &lt;- extract_profanity_terms(brady)

attributes(extract_profanity_terms(brady))$counts
attributes(extract_profanity_terms(brady))$elements

## End(Not run)
</code></pre>

<hr>
<h2 id='extract_sentiment_terms'>Extract Sentiment Words</h2><span id='topic+extract_sentiment_terms'></span>

<h3>Description</h3>

<p>Extract the sentiment words from a text.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_sentiment_terms(
  text.var,
  polarity_dt = lexicon::hash_sentiment_jockers_rinker,
  hyphen = "",
  retention_regex = "\\d:\\d|\\d\\s|[^[:alpha:]',;: ]",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_sentiment_terms_+3A_text.var">text.var</code></td>
<td>
<p>The text variable.</p>
</td></tr>
<tr><td><code id="extract_sentiment_terms_+3A_polarity_dt">polarity_dt</code></td>
<td>
<p>A <span class="pkg">data.table</span> of positive/negative words and
weights with x and y as column names.</p>
</td></tr>
<tr><td><code id="extract_sentiment_terms_+3A_hyphen">hyphen</code></td>
<td>
<p>The character string to replace hyphens with.  Default replaces
with nothing so 'sugar-free' becomes 'sugarfree'.  Setting <code>hyphen = " "</code>
would result in a space between words (e.g., 'sugar free').</p>
</td></tr>
<tr><td><code id="extract_sentiment_terms_+3A_retention_regex">retention_regex</code></td>
<td>
<p>A regex of what characters to keep.  All other 
characters will be removed.  Note that when this is used all text is lower 
case format.  Only adjust this parameter if you really understand how it is 
used.  Note that swapping the <code>\\{p}</code> for <code>[^[:alpha:];:,\']</code> may 
retain more alpha letters but will likely decrease speed.</p>
</td></tr>
<tr><td><code id="extract_sentiment_terms_+3A_...">...</code></td>
<td>
<p>Ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a <span class="pkg">data.table</span> with columns of positive and 
negative terms.  In addition, the attributes <code>$counts</code> and <code>$elements</code>
return an aggregated count of the usage of the words and a detailed sentiment
score of each word use.  See the examples for more.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(data.table)
set.seed(10)
x &lt;- get_sentences(sample(hu_liu_cannon_reviews[[2]], 1000, TRUE))
sentiment(x)

pol_words &lt;- extract_sentiment_terms(x)
pol_words
pol_words$sentence
pol_words$neutral
data.table::as.data.table(pol_words)

attributes(extract_sentiment_terms(x))$counts
attributes(extract_sentiment_terms(x))$elements

## Not run: 
library(wordcloud)
library(data.table)

set.seed(10)
x &lt;- get_sentences(sample(hu_liu_cannon_reviews[[2]], 1000, TRUE))
sentiment_words &lt;- extract_sentiment_terms(x)

sentiment_counts &lt;- attributes(sentiment_words)$counts
sentiment_counts[polarity &gt; 0,]

par(mfrow = c(1, 3), mar = c(0, 0, 0, 0))
## Positive Words
with(
    sentiment_counts[polarity &gt; 0,],
    wordcloud(words = words, freq = n, min.freq = 1,
          max.words = 200, random.order = FALSE, rot.per = 0.35,
          colors = brewer.pal(8, "Dark2"), scale = c(4.5, .75)
    )
)
mtext("Positive Words", side = 3, padj = 5)

## Negative Words
with(
    sentiment_counts[polarity &lt; 0,],
    wordcloud(words = words, freq = n, min.freq = 1,
          max.words = 200, random.order = FALSE, rot.per = 0.35,
          colors = brewer.pal(8, "Dark2"), scale = c(4.5, 1)
    )
)
mtext("Negative Words", side = 3, padj = 5)

sentiment_counts[, 
    color := ifelse(polarity &gt; 0, 'red', 
        ifelse(polarity &lt; 0, 'blue', 'gray70')
    )]

## Positive &amp; Negative Together
with(
    sentiment_counts[polarity != 0,],
    wordcloud(words = words, freq = n, min.freq = 1,
          max.words = 200, random.order = FALSE, rot.per = 0.35,
          colors = color, ordered.colors = TRUE, scale = c(5, .75)
    )
)
mtext("Positive (red) &amp; Negative (blue) Words", side = 3, padj = 5)

## End(Not run)
</code></pre>

<hr>
<h2 id='general_rescale'>Rescale a Numeric Vector</h2><span id='topic+general_rescale'></span>

<h3>Description</h3>

<p>Rescale a numeric vector with the option to make signed (-1, 1, or 0) and 
retain zero as neutral.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>general_rescale(
  x,
  lower = -1,
  upper = 1,
  mute = NULL,
  keep.zero = lower &lt; 0,
  sign = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="general_rescale_+3A_x">x</code></td>
<td>
<p>A numeric vector.</p>
</td></tr>
<tr><td><code id="general_rescale_+3A_lower">lower</code></td>
<td>
<p>An upper limit to rescale to.</p>
</td></tr>
<tr><td><code id="general_rescale_+3A_upper">upper</code></td>
<td>
<p>A lower limit to rescale to.</p>
</td></tr>
<tr><td><code id="general_rescale_+3A_mute">mute</code></td>
<td>
<p>A positive value greater than 1 to lower the extremes and pull 
the fractions up.  This becomes the denominator in a power to raise each 
element by (sign is retained) where the numerator is 1.  This is useful for 
mellowing out the extremes.</p>
</td></tr>
<tr><td><code id="general_rescale_+3A_keep.zero">keep.zero</code></td>
<td>
<p>logical.  If <code>TRUE</code> the zeros are kept as neutral.</p>
</td></tr>
<tr><td><code id="general_rescale_+3A_sign">sign</code></td>
<td>
<p>logical.  If <code>TRUE</code> the vector will be scaled as (-1, 1, or 0)</p>
</td></tr>
<tr><td><code id="general_rescale_+3A_...">...</code></td>
<td>
<p>ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a rescaled vector of the same length as <code>x</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
general_rescale(c(1, 0, -1))
general_rescale(c(1, 0, -1, 1.4, -2))
general_rescale(c(1, 0, -1, 1.4, -2), lower = 0, upper = 1)
general_rescale(c(NA, -4:3))
general_rescale(c(NA, -4:3), keep.zero = FALSE)
general_rescale(c(NA, -4:3), keep.zero = FALSE, lower = 0, upper = 100)

## mute extreme values
set.seed(10)
x &lt;- sort(c(NA, -100, -10, 0, rnorm(10, 0, .1), 10, 100), na.last = FALSE)
general_rescale(x)
general_rescale(x, mute = 5)
general_rescale(x, mute = 10)
general_rescale(x, mute = 100)
</code></pre>

<hr>
<h2 id='get_sentences'>Get Sentences</h2><span id='topic+get_sentences'></span>

<h3>Description</h3>

<p><code>get_sentences</code> - Get sentences from a character vector, <code>sentiment</code>, or
<code>sentiment_by</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_sentences(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_sentences_+3A_x">x</code></td>
<td>
<p>A character vector, <code>sentiment</code>, or <code>sentiment_by</code> object.</p>
</td></tr>
<tr><td><code id="get_sentences_+3A_...">...</code></td>
<td>
<p>Other arguments passed to <code><a href="textshape.html#topic+split_sentence">split_sentence</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list of vectors of sentences.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- data.frame(
    w = c('Person 1', 'Person 2'),
    x = c(paste0(
        "Mr. Brown comes! He says hello. i give him coffee.  i will ",
        "go at 5 p. m. eastern time.  Or somewhere in between!go there"
    ), "One more thought for the road! I am going now.  Good day."),
    y = state.name[c(32, 38)], 
    z = c(.456, .124),
    stringsAsFactors = FALSE
)
get_sentences(dat$x)
get_sentences(dat)
</code></pre>

<hr>
<h2 id='get_sentences2'>Get Sentences (Deprecated)</h2><span id='topic+get_sentences2'></span>

<h3>Description</h3>

<p>Deprecated, use <code><a href="#topic+get_sentences">get_sentences</a></code> instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_sentences2(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_sentences2_+3A_x">x</code></td>
<td>
<p>A character vector, <code>sentiment</code>, or <code>sentiment_by</code> object.</p>
</td></tr>
<tr><td><code id="get_sentences2_+3A_...">...</code></td>
<td>
<p>Ignored.</p>
</td></tr>
</table>

<hr>
<h2 id='highlight'>Polarity Text Highlighting</h2><span id='topic+highlight'></span>

<h3>Description</h3>

<p>Highlight sentences within elements (row IDs) by sentiment polarity
(positive = green; negative = pink) as an html file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>highlight(
  x,
  file = file.path(tempdir(), "polarity.html"),
  open = TRUE,
  digits = 3,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="highlight_+3A_x">x</code></td>
<td>
<p>A <code>sentiment_by</code> object.</p>
</td></tr>
<tr><td><code id="highlight_+3A_file">file</code></td>
<td>
<p>A name of the html file output.</p>
</td></tr>
<tr><td><code id="highlight_+3A_open">open</code></td>
<td>
<p>logical.  If <code>TRUE</code> the text highlighting document will
attempt to be opened.</p>
</td></tr>
<tr><td><code id="highlight_+3A_digits">digits</code></td>
<td>
<p>The number of digits to print for each row level average
sentiment score.</p>
</td></tr>
<tr><td><code id="highlight_+3A_...">...</code></td>
<td>
<p>Ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Generates an html document with text highlighting.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(data.table)
dat &lt;- presidential_debates_2012
setDT(dat)

dat[, gr:={gr= paste(person, time); cumsum(c(TRUE, gr[-1]!= gr[-.N]))}]
dat &lt;- dat[, list(person=person[1L], time=time[1L], dialogue=paste(dialogue,
    collapse = ' ')), by = gr][,gr:= NULL][, 
    dialogue_split := get_sentences(dialogue)][]

(sent_dat &lt;- with(dat, sentiment_by(dialogue_split, list(person, time))))

highlight(sent_dat)

## tidy approach
library(dplyr)
library(magrittr)

hu_liu_cannon_reviews %&gt;%
    filter(review_id %in% sample(unique(review_id), 3)) %&gt;%
    mutate(review = get_sentences(text)) %$%
    sentiment_by(review, review_id) %&gt;%
    highlight()

## End(Not run)
</code></pre>

<hr>
<h2 id='hotel_reviews'>Hotel Reviews</h2><span id='topic+hotel_reviews'></span>

<h3>Description</h3>

<p>A dataset containing a random sample (n = 5000 of 1,621,956) of Wang, Lu, &amp; 
Zhai's (2011) hotel reviews data set scraped by the authors from 
Original URL: http://www.tripadvisor.com.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(hotel_reviews)
</code></pre>


<h3>Format</h3>

<p>A data frame with 5000 rows and 2 variables
</p>


<h3>Details</h3>

 
<ul>
<li><p> sentiment. The overall rating for the experience
</p>
</li>
<li><p> text. The text review of the hotel
</p>
</li></ul>



<h3>References</h3>

<p>Wang, H., Lu, Y., and Zhai, C. (2011). Latent aspect rating 
analysis without aspect keyword supervision. In Proceedings of the 17th ACM 
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD'2011), 618-626. <br /> <br />
Original URL: 'http://sifaka.cs.uiuc.edu/~wang296/Data/index.html'
</p>

<hr>
<h2 id='hu_liu_apex_reviews'>Apex AD2600 Progressive-scan DVD player Product Reviews From Amazon</h2><span id='topic+hu_liu_apex_reviews'></span>

<h3>Description</h3>

<p>A dataset containing Amazon product reviews for the Apex AD2600 Progressive-scan DVD player.  This
data set was compiled by Hu and Liu (2004).  Where a sentence contains more
than one opinion score and average of all scores is used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(hu_liu_apex_reviews)
</code></pre>


<h3>Format</h3>

<p>A data frame with 740 rows and 3 variables
</p>


<h3>Details</h3>


<ul>
<li><p> sentiment. Hu and Liu (2004)'s average opinion rating for a
sentence.  Negative and positive reflects direction, a negative or positive
sentiment.  Opinion strength varies between 3 (strongest), and 1 (weakest).
number.  The review number.
</p>
</li>
<li><p> text. The text from the review.
</p>
</li>
<li><p> review_id. The review number.
</p>
</li></ul>



<h3>References</h3>

<p>Minqing Hu and Bing Liu. (2004). Mining and summarizing customer reviews.
Proceedings of the ACM SIGKDD International Conference on
Knowledge Discovery &amp; Data Mining (KDD-04).
</p>
<p>Minqing Hu and Bing Liu. (2004).&quot;Mining Opinion Features in Customer
Reviews. Proceedings of Nineteeth National Conference on
Artificial Intelligence (AAAI-2004).
</p>
<p>Original URL: &lsquo;<span class="file">https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html</span>&rsquo;
</p>

<hr>
<h2 id='hu_liu_cannon_reviews'>Cannon G3 Camera Product Reviews From Amazon</h2><span id='topic+hu_liu_cannon_reviews'></span>

<h3>Description</h3>

<p>A dataset containing Amazon product reviews for the Cannon G3 Camera.  This
data set was compiled by Hu and Liu (2004).  Where a sentence contains more
than one opinion score and average of all scores is used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(hu_liu_cannon_reviews)
</code></pre>


<h3>Format</h3>

<p>A data frame with 597 rows and 3 variables
</p>


<h3>Details</h3>


<ul>
<li><p> sentiment. Hu and Liu (2004)'s average opinion rating for a
sentence.  Negative and positive reflects direction, a negative or positive
sentiment.  Opinion strength varies between 3 (strongest), and 1 (weakest).
number.  The review number.
</p>
</li>
<li><p> text. The text from the review.
</p>
</li>
<li><p> review_id. The review number.
</p>
</li></ul>



<h3>References</h3>

<p>Minqing Hu and Bing Liu. (2004). Mining and summarizing customer reviews.
Proceedings of the ACM SIGKDD International Conference on
Knowledge Discovery &amp; Data Mining (KDD-04).
</p>
<p>Minqing Hu and Bing Liu. (2004).&quot;Mining Opinion Features in Customer
Reviews. Proceedings of Nineteeth National Conference on
Artificial Intelligence (AAAI-2004).
</p>
<p>Original URL:  &lsquo;<span class="file">https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html</span>&rsquo;
</p>

<hr>
<h2 id='hu_liu_jukebox_reviews'>Creative Labs Nomad Jukebox Zen Xtra 40GB Product Reviews From Amazon</h2><span id='topic+hu_liu_jukebox_reviews'></span>

<h3>Description</h3>

<p>A dataset containing Amazon product reviews for the Creative Labs Nomad Jukebox Zen Xtra 40GB.  This
data set was compiled by Hu and Liu (2004).  Where a sentence contains more
than one opinion score and average of all scores is used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(hu_liu_jukebox_reviews)
</code></pre>


<h3>Format</h3>

<p>A data frame with 1716 rows and 3 variables
</p>


<h3>Details</h3>


<ul>
<li><p> sentiment. Hu and Liu (2004)'s average opinion rating for a
sentence.  Negative and positive reflects direction, a negative or positive
sentiment.  Opinion strength varies between 3 (strongest), and 1 (weakest).
number.  The review number.
</p>
</li>
<li><p> text. The text from the review.
</p>
</li>
<li><p> review_id. The review number.
</p>
</li></ul>



<h3>References</h3>

<p>Minqing Hu and Bing Liu. (2004). Mining and summarizing customer reviews.
Proceedings of the ACM SIGKDD International Conference on
Knowledge Discovery &amp; Data Mining (KDD-04).
</p>
<p>Minqing Hu and Bing Liu. (2004).&quot;Mining Opinion Features in Customer
Reviews. Proceedings of Nineteeth National Conference on
Artificial Intelligence (AAAI-2004).
</p>
<p>Original URL: &lsquo;<span class="file">https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html</span>&rsquo;
</p>

<hr>
<h2 id='hu_liu_nikon_reviews'>Nikon Coolpix 4300 Product Reviews From Amazon</h2><span id='topic+hu_liu_nikon_reviews'></span>

<h3>Description</h3>

<p>A dataset containing Amazon product reviews for the Nikon Coolpix 4300.  This
data set was compiled by Hu and Liu (2004).  Where a sentence contains more
than one opinion score and average of all scores is used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(hu_liu_nikon_reviews)
</code></pre>


<h3>Format</h3>

<p>A data frame with 346 rows and 3 variables
</p>


<h3>Details</h3>


<ul>
<li><p> sentiment. Hu and Liu (2004)'s average opinion rating for a
sentence.  Negative and positive reflects direction, a negative or positive
sentiment.  Opinion strength varies between 3 (strongest), and 1 (weakest).
number.  The review number.
</p>
</li>
<li><p> text. The text from the review.
</p>
</li>
<li><p> review_id. The review number.
</p>
</li></ul>



<h3>References</h3>

<p>Minqing Hu and Bing Liu. (2004). Mining and summarizing customer reviews.
Proceedings of the ACM SIGKDD International Conference on
Knowledge Discovery &amp; Data Mining (KDD-04).
</p>
<p>Minqing Hu and Bing Liu. (2004).&quot;Mining Opinion Features in Customer
Reviews. Proceedings of Nineteeth National Conference on
Artificial Intelligence (AAAI-2004).
</p>
<p>&lsquo;<span class="file">https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html</span>&rsquo;
</p>

<hr>
<h2 id='hu_liu_nokia_reviews'>Nokia 6610 Product Reviews From Amazon</h2><span id='topic+hu_liu_nokia_reviews'></span>

<h3>Description</h3>

<p>A dataset containing Amazon product reviews for the Nokia 6610.  This
data set was compiled by Hu and Liu (2004).  Where a sentence contains more
than one opinion score and average of all scores is used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(hu_liu_nokia_reviews)
</code></pre>


<h3>Format</h3>

<p>A data frame with 546 rows and 3 variables
</p>


<h3>Details</h3>


<ul>
<li><p> sentiment. Hu and Liu (2004)'s average opinion rating for a
sentence.  Negative and positive reflects direction, a negative or positive
sentiment.  Opinion strength varies between 3 (strongest), and 1 (weakest).
number.  The review number.
</p>
</li>
<li><p> text. The text from the review.
</p>
</li>
<li><p> review_id. The review number.
</p>
</li></ul>



<h3>References</h3>

<p>Minqing Hu and Bing Liu. (2004). Mining and summarizing customer reviews.
Proceedings of the ACM SIGKDD International Conference on
Knowledge Discovery &amp; Data Mining (KDD-04).
</p>
<p>Minqing Hu and Bing Liu. (2004).&quot;Mining Opinion Features in Customer
Reviews. Proceedings of Nineteeth National Conference on
Artificial Intelligence (AAAI-2004).
</p>
<p>Original URL: &lsquo;<span class="file">https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html</span>&rsquo;
</p>

<hr>
<h2 id='kaggle_movie_reviews'>Movie Reviews</h2><span id='topic+kaggle_movie_reviews'></span>

<h3>Description</h3>

<p>A dataset containing sentiment scored movie reviews from a Kaggle competition
posted by University of Michigan SI650.  The data was originally collected 
from opinmind.com.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(kaggle_movie_reviews)
</code></pre>


<h3>Format</h3>

<p>A data frame with 7,086 rows and 2 variables
</p>


<h3>Details</h3>

 
<ul>
<li><p> sentiment. A numeric sentiment score
</p>
</li>
<li><p> text. The text from the review 
</p>
</li></ul>



<h3>References</h3>

<p>Original URL: https://www.kaggle.com/c/si650winter11/data
</p>

<hr>
<h2 id='kotzias_reviews_amazon_cells'>Kotzias Reviews: Amazon Cells</h2><span id='topic+kotzias_reviews_amazon_cells'></span>

<h3>Description</h3>

<p>A dataset containing a list of 4 review data sets.  Each data set contains
sentences with a positive (1) or negative review (-1) taken from reviews of
products, movies, &amp; restaurants.  The data, compiled by Kotzias, Denil, De Freitas,
&amp; Smyth (2015), was originally taken from amazon.com, imdb.com, &amp; yelp.com.
Kotzias et al. (2015) provide the following description in the README:
&quot;For each website, there exist 500 positive and
500 negative sentences. Those were selected randomly for larger datasets of
reviews. We attempted to select sentences that have a clearly positive or
negative connotation [sic], the goal was for no neutral sentences to be selected.
This data set has been manipulated from the original to be split apart by
element (sentence split).  The original 0/1 metric has also been converted
to -1/1.  Please cite Kotzias et al. (2015) if you reuse the data here.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(kotzias_reviews_amazon_cells)
</code></pre>


<h3>Format</h3>

<p>A data frame with 1,067 rows and 2 variables
</p>


<h3>Details</h3>


<ul>
<li><p> sentiment. A human scoring of the text. 
</p>
</li>
<li><p> text. The sentences from the review.
</p>
</li></ul>



<h3>References</h3>

<p>Kotzias, D., Denil, M., De Freitas, N. &amp; Smyth,P. (2015). From
group to individual labels using deep features. Proceedings of the 21th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining.
597-606. Original URL: http://mdenil.com/media/papers/2015-deep-multi-instance-learning.pdf
</p>

<hr>
<h2 id='kotzias_reviews_imdb'>Kotzias Reviews: IMBD</h2><span id='topic+kotzias_reviews_imdb'></span>

<h3>Description</h3>

<p>A dataset containing a list of 4 review data sets.  Each data set contains
sentences with a positive (1) or negative review (-1) taken from reviews of
products, movies, &amp; restaurants.  The data, compiled by Kotzias, Denil, De Freitas,
&amp; Smyth (2015), was originally taken from amazon.com, imdb.com, &amp; yelp.com.
Kotzias et al. (2015) provide the following description in the README:
&quot;For each website, there exist 500 positive and
500 negative sentences. Those were selected randomly for larger datasets of
reviews. We attempted to select sentences that have a clearly positive or
negative connotation [sic], the goal was for no neutral sentences to be selected.
This data set has been manipulated from the original to be split apart by
element (sentence split).  The original 0/1 metric has also been converted
to -1/1.  Please cite Kotzias et al. (2015) if you reuse the data here.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(kotzias_reviews_imdb)
</code></pre>


<h3>Format</h3>

<p>A data frame with 1,041 rows and 2 variables
</p>


<h3>Details</h3>


<ul>
<li><p> sentiment. A human scoring of the text.
</p>
</li>
<li><p> text. The sentences from the review.
</p>
</li></ul>



<h3>References</h3>

<p>Kotzias, D., Denil, M., De Freitas, N. &amp; Smyth,P. (2015). From
group to individual labels using deep features. Proceedings of the 21th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining.
597-606. Original URL: http://mdenil.com/media/papers/2015-deep-multi-instance-learning.pdf
</p>

<hr>
<h2 id='kotzias_reviews_yelp'>Kotzias Reviews: Yelp</h2><span id='topic+kotzias_reviews_yelp'></span>

<h3>Description</h3>

<p>A dataset containing a list of 4 review data sets.  Each data set contains
sentences with a positive (1) or negative review (-1) taken from reviews of
products, movies, &amp; restaurants.  The data, compiled by Kotzias, Denil, De Freitas,
&amp; Smyth (2015), was originally taken from amazon.com, imdb.com, &amp; yelp.com.
Kotzias et al. (2015) provide the following description in the README:
&quot;For each website, there exist 500 positive and
500 negative sentences. Those were selected randomly for larger datasets of
reviews. We attempted to select sentences that have a clearly positive or
negative connotation [sic], the goal was for no neutral sentences to be selected.
This data set has been manipulated from the original to be split apart by
element (sentence split).  The original 0/1 metric has also been converted
to -1/1.  Please cite Kotzias et al. (2015) if you reuse the data here.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(kotzias_reviews_yelp)
</code></pre>


<h3>Format</h3>

<p>A data frame with 1,040 rows and 2 variables
</p>


<h3>Details</h3>


<ul>
<li><p> sentiment. A human scoring of the text.
</p>
</li>
<li><p> text. The sentences from the review.
</p>
</li></ul>



<h3>References</h3>

<p>Kotzias, D., Denil, M., De Freitas, N. &amp; Smyth,P. (2015). From
group to individual labels using deep features. Proceedings of the 21th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining.
597-606. Original URL: http://mdenil.com/media/papers/2015-deep-multi-instance-learning.pdf
</p>

<hr>
<h2 id='nyt_articles'>Sentiment Scored New York Times Articles</h2><span id='topic+nyt_articles'></span>

<h3>Description</h3>

<p>A dataset containing Hutto &amp; Gilbert's (2014) sentiment scored New York Times 
articles.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(nyt_articles)
</code></pre>


<h3>Format</h3>

<p>A data frame with 5,179 rows and 2 variables
</p>


<h3>Details</h3>

 
<ul>
<li><p> sentiment. A numeric sentiment score
</p>
</li>
<li><p> text. The text from the article
</p>
</li></ul>
 
<p>Vadar's Liscense:
</p>
<p>The MIT License (MIT)
</p>
<p>Copyright (c) 2016 C.J. Hutto
</p>
<p>Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the &quot;Software&quot;), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:
</p>
<p>The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.
</p>
<p>THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</p>


<h3>References</h3>

<p>Hutto, C.J. &amp; Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model
for Sentiment Analysis of Social Media Text. Eighth International Conference
on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.
</p>
<p>Original URL: https://github.com/cjhutto/vaderSentiment
</p>

<hr>
<h2 id='plot.emotion'>Plots a emotion object</h2><span id='topic+plot.emotion'></span>

<h3>Description</h3>

<p>Plots a emotion object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'emotion'
plot(
  x,
  transformation.function = syuzhet::get_dct_transform,
  drop.unused.emotions = TRUE,
  facet = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.emotion_+3A_x">x</code></td>
<td>
<p>The emotion object.</p>
</td></tr>
<tr><td><code id="plot.emotion_+3A_transformation.function">transformation.function</code></td>
<td>
<p>A transformation function to smooth the emotion
scores.</p>
</td></tr>
<tr><td><code id="plot.emotion_+3A_drop.unused.emotions">drop.unused.emotions</code></td>
<td>
<p>logical.  If <code>TRUE</code> unused/unfound emotion
levels will not be included in the output.</p>
</td></tr>
<tr><td><code id="plot.emotion_+3A_facet">facet</code></td>
<td>
<p>logical or one of <code>c('emotion', 'negated')</code>.  If <code>TRUE</code> 
or <code>'emotion'</code> the plot will be facetted by Emotion Type.  If <code>FALSE</code> 
all types will be plotted in the same window.  If <code>"negated"</code> the emotions
will be in the same plot window but broken out by negated or non-negated types.</p>
</td></tr>
<tr><td><code id="plot.emotion_+3A_...">...</code></td>
<td>
<p>Other arguments passed to <code><a href="syuzhet.html#topic+get_transformed_values">get_transformed_values</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Utilizes Matthew Jocker's <span class="pkg">syuzhet</span> package to calculate smoothed
emotion across the duration of the text.
</p>


<h3>Value</h3>

<p>Returns a <span class="pkg">ggplot2</span> object.
</p>

<hr>
<h2 id='plot.emotion_by'>Plots a emotion_by object</h2><span id='topic+plot.emotion_by'></span>

<h3>Description</h3>

<p>Plots a emotion_by object.  Red centers are average emotion.  Alpha
jittered dots are raw sentence level emotion data.  Boxes are boxplots.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'emotion_by'
plot(x, ordered = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.emotion_by_+3A_x">x</code></td>
<td>
<p>The emotion_by object.</p>
</td></tr>
<tr><td><code id="plot.emotion_by_+3A_ordered">ordered</code></td>
<td>
<p>logical.  If <code>TRUE</code> order the output grouping by emotion.</p>
</td></tr>
<tr><td><code id="plot.emotion_by_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a <span class="pkg">ggplot2</span> object.
</p>

<hr>
<h2 id='plot.profanity'>Plots a profanity object</h2><span id='topic+plot.profanity'></span>

<h3>Description</h3>

<p>Plots a profanity object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'profanity'
plot(x, transformation.function = syuzhet::get_dct_transform, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.profanity_+3A_x">x</code></td>
<td>
<p>The profanity object.</p>
</td></tr>
<tr><td><code id="plot.profanity_+3A_transformation.function">transformation.function</code></td>
<td>
<p>A transformation function to smooth the profanity
scores.</p>
</td></tr>
<tr><td><code id="plot.profanity_+3A_...">...</code></td>
<td>
<p>Other arguments passed to <code><a href="syuzhet.html#topic+get_transformed_values">get_transformed_values</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Utilizes Matthew Jocker's <span class="pkg">syuzhet</span> package to calculate smoothed
profanity across the duration of the text.
</p>


<h3>Value</h3>

<p>Returns a <span class="pkg">ggplot2</span> object.
</p>

<hr>
<h2 id='plot.profanity_by'>Plots a profanity_by object</h2><span id='topic+plot.profanity_by'></span>

<h3>Description</h3>

<p>Plots a profanity_by object.  Red centers are average profanity.  Alpha
jittered dots are raw sentence level profanity data.  Boxes are boxplots.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'profanity_by'
plot(x, ordered = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.profanity_by_+3A_x">x</code></td>
<td>
<p>The profanity_by object.</p>
</td></tr>
<tr><td><code id="plot.profanity_by_+3A_ordered">ordered</code></td>
<td>
<p>logical.  If <code>TRUE</code> order the output grouping by profanity.</p>
</td></tr>
<tr><td><code id="plot.profanity_by_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a <span class="pkg">ggplot2</span> object.
</p>

<hr>
<h2 id='plot.sentiment'>Plots a sentiment object</h2><span id='topic+plot.sentiment'></span>

<h3>Description</h3>

<p>Plots a sentiment object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sentiment'
plot(x, transformation.function = syuzhet::get_dct_transform, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.sentiment_+3A_x">x</code></td>
<td>
<p>The sentiment object.</p>
</td></tr>
<tr><td><code id="plot.sentiment_+3A_transformation.function">transformation.function</code></td>
<td>
<p>A transformation function to smooth the sentiment
scores.</p>
</td></tr>
<tr><td><code id="plot.sentiment_+3A_...">...</code></td>
<td>
<p>Other arguments passed to <code><a href="syuzhet.html#topic+get_transformed_values">get_transformed_values</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Utilizes Matthew Jocker's <span class="pkg">syuzhet</span> package to calculate smoothed
sentiment across the duration of the text.
</p>


<h3>Value</h3>

<p>Returns a <span class="pkg">ggplot2</span> object.
</p>

<hr>
<h2 id='plot.sentiment_by'>Plots a sentiment_by object</h2><span id='topic+plot.sentiment_by'></span>

<h3>Description</h3>

<p>Plots a sentiment_by object.  Red centers are average sentiment.  Alpha
jittered dots are raw sentence level sentiment data.  Boxes are boxplots.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sentiment_by'
plot(x, ordered = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.sentiment_by_+3A_x">x</code></td>
<td>
<p>The sentiment_by object.</p>
</td></tr>
<tr><td><code id="plot.sentiment_by_+3A_ordered">ordered</code></td>
<td>
<p>logical.  If <code>TRUE</code> order the output grouping by sentiment.</p>
</td></tr>
<tr><td><code id="plot.sentiment_by_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a <span class="pkg">ggplot2</span> object.
</p>

<hr>
<h2 id='presidential_debates_2012'>2012 U.S. Presidential Debates</h2><span id='topic+presidential_debates_2012'></span>

<h3>Description</h3>

<p>A dataset containing a cleaned version of all three presidential debates for
the 2012 election.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(presidential_debates_2012)
</code></pre>


<h3>Format</h3>

<p>A data frame with 2912 rows and 4 variables
</p>


<h3>Details</h3>


<ul>
<li><p> person. The speaker
</p>
</li>
<li><p> tot. Turn of talk
</p>
</li>
<li><p> dialogue. The words spoken
</p>
</li>
<li><p> time. Variable indicating which of the three debates the dialogue is from
</p>
</li></ul>


<hr>
<h2 id='print.extract_emotion_terms'>Prints an extract_emotion_terms Object</h2><span id='topic+print.extract_emotion_terms'></span>

<h3>Description</h3>

<p>Prints an extract_emotion_terms object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'extract_emotion_terms'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.extract_emotion_terms_+3A_x">x</code></td>
<td>
<p>An extract_emotion_terms object.</p>
</td></tr>
<tr><td><code id="print.extract_emotion_terms_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
</table>

<hr>
<h2 id='print.extract_profanity_terms'>Prints an extract_profanity_terms Object</h2><span id='topic+print.extract_profanity_terms'></span>

<h3>Description</h3>

<p>Prints an extract_profanity_terms object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'extract_profanity_terms'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.extract_profanity_terms_+3A_x">x</code></td>
<td>
<p>An extract_profanity_terms object.</p>
</td></tr>
<tr><td><code id="print.extract_profanity_terms_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
</table>

<hr>
<h2 id='print.extract_sentiment_terms'>Prints an extract_sentiment_terms Object</h2><span id='topic+print.extract_sentiment_terms'></span>

<h3>Description</h3>

<p>Prints an extract_sentiment_terms object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'extract_sentiment_terms'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.extract_sentiment_terms_+3A_x">x</code></td>
<td>
<p>An extract_sentiment_terms object.</p>
</td></tr>
<tr><td><code id="print.extract_sentiment_terms_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
</table>

<hr>
<h2 id='print.validate_sentiment'>Prints a validate_sentiment Object</h2><span id='topic+print.validate_sentiment'></span>

<h3>Description</h3>

<p>Prints a validate_sentiment object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'validate_sentiment'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.validate_sentiment_+3A_x">x</code></td>
<td>
<p>A <code>validate_sentiment</code> Object</p>
</td></tr>
<tr><td><code id="print.validate_sentiment_+3A_...">...</code></td>
<td>
<p>ignored.</p>
</td></tr>
</table>

<hr>
<h2 id='profanity'>Compute Profanity Rate</h2><span id='topic+profanity'></span>

<h3>Description</h3>

<p>Detect the rate of profanity at the sentence level.  This method uses a simple
dictionary lookup to find profane words and then compute the rate per sentence.
The <code>profanity</code> score ranges between 0 (no profanity used) and 1 (all
words used were profane).  Note that a single profane phrase would count as 
just one in the <code>profanity_count</code> column but would count as two words in
the <code>word_count</code> column.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>profanity(
  text.var,
  profanity_list = unique(tolower(lexicon::profanity_alvarez)),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="profanity_+3A_text.var">text.var</code></td>
<td>
<p>The text variable.  Can be a <code>get_sentences</code> object or
a raw character vector though <code>get_sentences</code> is preferred as it avoids
the repeated cost of doing sentence boundary disambiguation every time
<code>sentiment</code> is run.</p>
</td></tr>
<tr><td><code id="profanity_+3A_profanity_list">profanity_list</code></td>
<td>
<p>A atomic character vector of profane words.  The 
<span class="pkg">lexicon</span> package has lists that can be used, including: 
</p>

<ul>
<li> <p><code>unique(tolower(lexicon::profanity_alvarez))</code>
</p>
</li>
<li> <p><code>lexicon::profanity_arr_bad</code>
</p>
</li>
<li> <p><code>lexicon::profanity_banned</code>
</p>
</li>
<li> <p><code>lexicon::profanity_zac_anger</code>
</p>
</li>
<li> <p><code>lexicon::profanity_racist</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="profanity_+3A_...">...</code></td>
<td>
<p>ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a <span class="pkg">data.table</span> of:
</p>

<ul>
<li><p>  element_id - The id number of the original vector passed to <code>profanity</code>
</p>
</li>
<li><p>  sentence_id - The id number of the sentences within each <code>element_id</code>
</p>
</li>
<li><p>  word_count - Word count
</p>
</li>
<li><p>  profanity_count - Count of the number of profane words
</p>
</li>
<li><p> profanity - A score of the percentage of profane words
</p>
</li></ul>



<h3>See Also</h3>

<p>Other profanity functions: 
<code><a href="#topic+profanity_by">profanity_by</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
bw &lt;- sample(unique(tolower(lexicon::profanity_alvarez)), 4)
mytext &lt;- c(
   sprintf('do you like this %s?  It is %s. But I hate really bad dogs', bw[1], bw[2]),
   'I am the best friend.',
   NA,
   sprintf('I %s hate this %s', bw[3], bw[4]),
   "Do you really like it?  I'm not happy"
)

## works on a character vector but not the preferred method avoiding the 
## repeated cost of doing sentence boundary disambiguation every time 
## `profanity` is run
profanity(mytext)

## preferred method avoiding paying the cost 
mytext2 &lt;- get_sentences(mytext)
profanity(mytext2)

plot(profanity(mytext2))

brady &lt;- get_sentences(crowdflower_deflategate)
brady_swears &lt;- profanity(brady)
brady_swears

## Distribution of profanity proportion for all comments
hist(brady_swears$profanity)
sum(brady_swears$profanity &gt; 0)

## Distribution of proportions for those profane comments
hist(brady_swears$profanity[brady_swears$profanity &gt; 0])

combo &lt;- combine_data()
combo_sentences &lt;- get_sentences(crowdflower_deflategate)
racist &lt;- profanity(combo_sentences, profanity_list = lexicon::profanity_racist)
combo_sentences[racist$profanity &gt; 0, ]$text
extract_profanity_terms(
    combo_sentences[racist$profanity &gt; 0, ]$text, 
    profanity_list = lexicon::profanity_racist
)

## Remove jerry, que, and illegal from the list
library(textclean)

racist2 &lt;- profanity(
    combo_sentences, 
    profanity_list = textclean::drop_element_fixed(
        lexicon::profanity_racist, 
        c('jerry', 'illegal', 'que')
    )
)
combo_sentences[racist2$profanity &gt; 0, ]$text

## End(Not run)
</code></pre>

<hr>
<h2 id='profanity_by'>Profanity Rate By Groups</h2><span id='topic+profanity_by'></span>

<h3>Description</h3>

<p>Approximate the profanity of text by grouping variable(s).  For a
full description of the profanity detection algorithm see 
<code><a href="#topic+profanity">profanity</a></code>.  See <code><a href="#topic+profanity">profanity</a></code>
for more details about the algorithm, the profanity/valence shifter keys
that can be passed into the function, and other arguments that can be passed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>profanity_by(text.var, by = NULL, group.names, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="profanity_by_+3A_text.var">text.var</code></td>
<td>
<p>The text variable.  Also takes a <code>profanityr</code> or
<code>profanity_by</code> object.</p>
</td></tr>
<tr><td><code id="profanity_by_+3A_by">by</code></td>
<td>
<p>The grouping variable(s).  Default <code>NULL</code> uses the original
row/element indices; if you used a column of 12 rows for <code>text.var</code>
these 12 rows will be used as the grouping variable.  Also takes a single
grouping variable or a list of 1 or more grouping variables.</p>
</td></tr>
<tr><td><code id="profanity_by_+3A_group.names">group.names</code></td>
<td>
<p>A vector of names that corresponds to group.  Generally
for internal use.</p>
</td></tr>
<tr><td><code id="profanity_by_+3A_...">...</code></td>
<td>
<p>Other arguments passed to <code><a href="#topic+profanity">profanity</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a <span class="pkg">data.table</span> with grouping variables plus:
</p>

<ul>
<li><p>  element_id - The id number of the original vector passed to <code>profanity</code>
</p>
</li>
<li><p>  sentence_id - The id number of the sentences within each <code>element_id</code>
</p>
</li>
<li><p>  word_count - Word count <code><a href="base.html#topic+sum">sum</a></code>med by grouping variable
</p>
</li>
<li><p>  profanity_count - The number of profanities used by grouping variable
</p>
</li>
<li><p>  sd - Standard deviation (<code><a href="stats.html#topic+sd">sd</a></code>) of the sentence level profanity rate by grouping variable
</p>
</li>
<li><p>  ave_profanity - Profanity rate
</p>
</li></ul>



<h3>Chaining</h3>

<p>See the  <code><a href="#topic+sentiment_by">sentiment_by</a></code> for details about <span class="pkg">sentimentr</span> chaining.
</p>


<h3>See Also</h3>

<p>Other profanity functions: 
<code><a href="#topic+profanity">profanity</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
bw &lt;- sample(lexicon::profanity_alvarez, 4)
mytext &lt;- c(
   sprintf('do you like this %s?  It is %s. But I hate really bad dogs', bw[1], bw[2]),
   'I am the best friend.',
   NA,
   sprintf('I %s hate this %s', bw[3], bw[4]),
   "Do you really like it?  I'm not happy"
)

## works on a character vector but not the preferred method avoiding the 
## repeated cost of doing sentence boundary disambiguation every time 
## `profanity` is run
profanity(mytext)
profanity_by(mytext)

## preferred method avoiding paying the cost 
mytext &lt;- get_sentences(mytext)

profanity_by(mytext)
get_sentences(profanity_by(mytext))

(myprofanity &lt;- profanity_by(mytext))
stats::setNames(get_sentences(profanity_by(mytext)),
    round(myprofanity[["ave_profanity"]], 3))

brady &lt;- get_sentences(crowdflower_deflategate)
library(data.table)
bp &lt;- profanity_by(brady)
crowdflower_deflategate[bp[ave_profanity &gt; 0,]$element_id, ]

vulgars &lt;- bp[["ave_profanity"]] &gt; 0
stats::setNames(get_sentences(bp)[vulgars],
    round(bp[["ave_profanity"]][vulgars], 3))
    
bt &lt;- data.table(crowdflower_deflategate)[, 
    source := ifelse(grepl('^RT', text), 'retweet', 'OP')][,
    belichick := grepl('\\bb[A-Za-z]+l[A-Za-z]*ch', text, ignore.case = TRUE)][]

prof_bel &lt;- with(bt, profanity_by(text, by = list(source, belichick)))

plot(prof_bel)

## End(Not run)
</code></pre>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic+replace_grade'></span><span id='topic+replace_rating'></span><span id='topic+replace_emoticon'></span><span id='topic+replace_emoji'></span><span id='topic+replace_emoji_identifier'></span><span id='topic+replace_internet_slang'></span><span id='topic+replace_word_elongation'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>textclean</dt><dd><p><code><a href="textclean.html#topic+replace_emoji">replace_emoji</a></code>, <code><a href="textclean.html#topic+replace_emoji">replace_emoji_identifier</a></code>, <code><a href="textclean.html#topic+replace_emoticon">replace_emoticon</a></code>, <code><a href="textclean.html#topic+replace_grade">replace_grade</a></code>, <code><a href="textclean.html#topic+replace_internet_slang">replace_internet_slang</a></code>, <code><a href="textclean.html#topic+replace_rating">replace_rating</a></code>, <code><a href="textclean.html#topic+replace_word_elongation">replace_word_elongation</a></code></p>
</dd>
</dl>

<hr>
<h2 id='sam_i_am'>Sam I Am Text</h2><span id='topic+sam_i_am'></span>

<h3>Description</h3>

<p>A dataset containing a character vector of the text from Seuss's 'Sam I Am'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(sam_i_am)
</code></pre>


<h3>Format</h3>

<p>A character vector with 169 elements
</p>


<h3>References</h3>

<p>Seuss, Dr. (1960). Green Eggs and Ham.
</p>

<hr>
<h2 id='sentiment'>Polarity Score (Sentiment Analysis)</h2><span id='topic+sentiment'></span>

<h3>Description</h3>

<p>Approximate the sentiment (polarity) of text by sentence.  This function allows
the user to easily alter (add, change, replace) the default polarity an 
valence shifters dictionaries to suit the context dependent needs of a particular
data set.  See the <code>polarity_dt</code> and <code>valence_shifters_dt</code> arguments
for more information.  Other hyper-parameters may add additional fine tuned 
control of the algorithm that may boost performance in different contexts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sentiment(
  text.var,
  polarity_dt = lexicon::hash_sentiment_jockers_rinker,
  valence_shifters_dt = lexicon::hash_valence_shifters,
  hyphen = "",
  amplifier.weight = 0.8,
  n.before = 5,
  n.after = 2,
  question.weight = 1,
  adversative.weight = 0.25,
  neutral.nonverb.like = FALSE,
  missing_value = 0,
  retention_regex = "\\d:\\d|\\d\\s|[^[:alpha:]',;: ]",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sentiment_+3A_text.var">text.var</code></td>
<td>
<p>The text variable.  Can be a <code>get_sentences</code> object or
a raw character vector though <code>get_sentences</code> is preferred as it avoids
the repeated cost of doing sentence boundary disambiguation every time
<code>sentiment</code> is run.</p>
</td></tr>
<tr><td><code id="sentiment_+3A_polarity_dt">polarity_dt</code></td>
<td>
<p>A <span class="pkg">data.table</span> of positive/negative words and
weights with x and y as column names.  The <span class="pkg">lexicon</span> package has several 
dictionaries that can be used, including: 
</p>

<ul>
<li> <p><code>lexicon::hash_sentiment_jockers_rinker</code>
</p>
</li>
<li> <p><code>lexicon::hash_sentiment_jockers</code>
</p>
</li>
<li> <p><code>lexicon::emojis_sentiment</code>
</p>
</li>
<li> <p><code>lexicon::hash_sentiment_emojis</code>
</p>
</li>
<li> <p><code>lexicon::hash_sentiment_huliu</code>
</p>
</li>
<li> <p><code>lexicon::hash_sentiment_loughran_mcdonald</code>
</p>
</li>
<li> <p><code>lexicon::hash_sentiment_nrc</code>
</p>
</li>
<li> <p><code>lexicon::hash_sentiment_senticnet</code>
</p>
</li>
<li> <p><code>lexicon::hash_sentiment_sentiword</code>
</p>
</li>
<li> <p><code>lexicon::hash_sentiment_slangsd</code>
</p>
</li>
<li> <p><code>lexicon::hash_sentiment_socal_google</code>
</p>
</li></ul>

<p>Additionally, the 
<code>as_key</code> function can be used to make a sentiment frame suitable for
<code>polarity_dt</code>.  This takes a 2 column data.frame with the first column
being words and the second column being polarity values.  Note that as of 
version 1.0.0 <span class="pkg">sentimentr</span> switched from the Liu &amp; HU (2004) dictionary
as the default to Jocker's (2017) dictionary from the <span class="pkg">syuzhet</span> package.
Use <code>lexicon::hash_sentiment_huliu</code> to obtain the old behavior.</p>
</td></tr>
<tr><td><code id="sentiment_+3A_valence_shifters_dt">valence_shifters_dt</code></td>
<td>
<p>A <span class="pkg">data.table</span> of valence shifters that
can alter a polarized word's meaning and an integer key for negators (1),
amplifiers [intensifiers] (2), de-amplifiers [downtoners] (3) and adversative 
conjunctions (4) with x and y as column names.</p>
</td></tr>
<tr><td><code id="sentiment_+3A_hyphen">hyphen</code></td>
<td>
<p>The character string to replace hyphens with.  Default replaces
with nothing so 'sugar-free' becomes 'sugarfree'.  Setting <code>hyphen = " "</code>
would result in a space between words (e.g., 'sugar free').  Typically use 
either &quot; &quot; or default &quot;&quot;.</p>
</td></tr>
<tr><td><code id="sentiment_+3A_amplifier.weight">amplifier.weight</code></td>
<td>
<p>The weight to apply to amplifiers/de-amplifiers 
[intensifiers/downtoners] (values from 0 to 1).  This value will multiply the 
polarized terms by 1 + this value.</p>
</td></tr>
<tr><td><code id="sentiment_+3A_n.before">n.before</code></td>
<td>
<p>The number of words to consider as valence shifters before
the polarized word.  To consider the entire beginning portion of a sentence
use <code>n.before = Inf</code>.</p>
</td></tr>
<tr><td><code id="sentiment_+3A_n.after">n.after</code></td>
<td>
<p>The number of words to consider as valence shifters after
the polarized word.  To consider the entire ending portion of a sentence
use <code>n.after = Inf</code>.</p>
</td></tr>
<tr><td><code id="sentiment_+3A_question.weight">question.weight</code></td>
<td>
<p>The weighting of questions (values from 0 to 1).
Default is 1.  A 0 corresponds with the belief that questions (pure questions)
are not polarized.  A weight may be applied based on the evidence that the
questions function with polarized sentiment.  In an opinion tasks such as a
course evalaution the questions are more likely polarized, not designed to
gain information.  On the other hand, in a setting with more natural dialogue,
the question is less likely polarized and is likely to function as a means
to gather information.</p>
</td></tr>
<tr><td><code id="sentiment_+3A_adversative.weight">adversative.weight</code></td>
<td>
<p>The weight to give to adversative conjunctions or 
contrasting conjunctions (e.g., &quot;but&quot;) that overrule the previous clause 
(Halliday &amp; Hasan, 2013).  Weighting a contrasting statement stems from the 
belief that the adversative conjunctions like &quot;but&quot;, &quot;however&quot;, and &quot;although&quot; 
amplify the current clause and/or down weight the prior clause.  If an 
adversative conjunction is located before the polarized word in the context 
cluster the cluster is up-weighted 1 + number of occurrences of the 
adversative conjunctions before the polarized word times the
weight given (<code class="reqn">1 + N_{adversative\,conjunctions} * z_2</code> where <code class="reqn">z_2</code> 
is the <code>adversative.weight</code>).  Conversely, an adversative conjunction 
found after the polarized word in a context cluster down weights the cluster 
1 - number of occurrences of the adversative conjunctions after the polarized 
word times the weight given (<code class="reqn">1 + N_{adversative\,conjunctions}*-1 * z_2</code>).  
These are added to the deamplifier and amplifier weights and thus the down 
weight is constrained to -1 as the lower bound.  Set to zero to remove 
adversative conjunction weighting.</p>
</td></tr>
<tr><td><code id="sentiment_+3A_neutral.nonverb.like">neutral.nonverb.like</code></td>
<td>
<p>logical.  If <code>TRUE</code>, and 'like' is found
in the <code>polarity_dt</code>, when the word 'like' is preceded by one of the 
following linking verbs: <code>"'s"</code>, <code>"was"</code>, <code>"is"</code>, <code>"has"</code>, 
<code>"am"</code>, <code>"are"</code>, <code>"'re"</code>, <code>"had"</code>, or <code>"been"</code> it is 
neutralized as this non-verb form of like is not likely polarized.  This is a 
poor man's part of speech tagger, maintaining the balance between speed and 
accuracy.  The word 'like', as a verb, tends to be polarized and is usually 
preceded by a noun or pronoun, not one of the linking verbs above.  This 
hyper parameter doesn't always yield improved results depending on the context 
of where the text data comes from.  For example, it is likely to be more 
useful in literary works, where like is often used in non-verb form, than 
product comments.  Use of this parameter will add compute time, this must be 
weighed against the need for accuracy and the likeliness that more accurate 
results will come from setting this argument to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="sentiment_+3A_missing_value">missing_value</code></td>
<td>
<p>A value to replace <code>NA</code>/<code>NaN</code> with.  Use
<code>NULL</code> to retain missing values.</p>
</td></tr>
<tr><td><code id="sentiment_+3A_retention_regex">retention_regex</code></td>
<td>
<p>A regex of what characters to keep.  All other 
characters will be removed.  Note that when this is used all text is lower 
case format.  Only adjust this parameter if you really understand how it is 
used.  Note that swapping the <code>\\p{L}</code> for <code>[^[:alpha:];:,\']</code> may 
retain more alpha letters but will likely decrease speed.  See examples below 
for how to test the need for <code>\\p{L}</code>.</p>
</td></tr>
<tr><td><code id="sentiment_+3A_...">...</code></td>
<td>
<p>Ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The equation used by the algorithm to assign value to polarity of
each sentence fist utilizes the sentiment dictionary to tag polarized words.  
Each paragraph
(<code class="reqn">p_i = \{s_1, s_2, ..., s_n\}</code>) composed of
sentences, is broken into element sentences
(<code class="reqn">s_i,j = \{w_1, w_2, ..., w_n\}</code>) where <code class="reqn">w</code>
are the words within sentences.  Each sentence (<code class="reqn">s_j</code>) is broken into a
an ordered bag of words.  Punctuation is removed with the exception of pause
punctuations (commas, colons, semicolons) which are considered a word within
the sentence.  I will denote pause words as <code class="reqn">cw</code> (comma words) for
convenience.  We can represent these words as an i,j,k notation as
<code class="reqn">w_{i,j,k}</code>.  For example <code class="reqn">w_{3,2,5}</code> would be the fifth word of the
second sentence of the third paragraph.  While I use the term paragraph this
merely represent a complete turn of talk.  For example t may be a cell level
response in a questionnaire composed of sentences.
</p>
<p>The words in each sentence (<code class="reqn">w_{i,j,k}</code>) are searched and compared to a
dictionary of polarized words (e.g., Jockers (2017) dictionary found in 
the <span class="pkg">lexicon</span> package).  Positive (<code class="reqn">w_{i,j,k}^{+}</code>) and
negative (<code class="reqn">w_{i,j,k}^{-}</code>) words are tagged with a <code class="reqn">+1</code> 
and <code class="reqn">-1</code> respectively.  I will denote polarized words as <code class="reqn">pw</code> for 
convenience. These will form a polar cluster (<code class="reqn">c_{i,j,l}</code>) 
which is a subset of the a sentence 
(<code class="reqn">c_{i,j,l} \subseteq s_i,j </code>).
</p>
<p>The polarized context cluster (<code class="reqn">c_{i,j,l}</code>) of words is pulled from around
the polarized word (<code class="reqn">pw</code>) and defaults to 4 words before and two words
after <code class="reqn">pw</code>) to be considered as valence shifters.  The cluster can be represented as
(<code class="reqn">c_{i,j,l} = \{pw_{i,j,k - nb}, ..., pw_{i,j,k} , ..., pw_{i,j,k - na}\}</code>),
where <code class="reqn">nb</code> &amp; <code class="reqn">na</code> are the parameters <code>n.before</code> and <code>n.after</code>
set by the user.  The words in this polarized context cluster are tagged as
neutral (<code class="reqn">w_{i,j,k}^{0}</code>), negator (<code class="reqn">w_{i,j,k}^{n}</code>),
amplifier [intensifier]] (<code class="reqn">w_{i,j,k}^{a}</code>), or de-amplifier
[downtoner] (<code class="reqn">w_{i,j,k}^{d}</code>). Neutral words hold no value in 
the equation but do affect word count (<code class="reqn">n</code>).  Each polarized word is then 
weighted (<code class="reqn">w</code>) based on the weights from the <code>polarity_dt</code> argument 
and then further weighted by the function and number of the valence shifters 
directly surrounding the positive or negative word (<code class="reqn">pw</code>).  Pause 
(<code class="reqn">cw</code>) locations (punctuation that denotes a pause including commas, 
colons, and semicolons) are indexed and considered in calculating the upper 
and lower bounds in the polarized context cluster. This is because these marks 
indicate a change in thought and words prior are not necessarily connected 
with words after these punctuation marks.  The lower bound of the polarized 
context cluster is constrained to 
<code class="reqn">\max \{pw_{i,j,k - nb}, 1, \max \{cw_{i,j,k} &lt; pw_{i,j,k}\}\}</code> and the upper bound is
constrained to <code class="reqn">\min \{pw_{i,j,k + na}, w_{i,jn}, \min \{cw_{i,j,k} &gt; pw_{i,j,k}\}\}</code>
where <code class="reqn">w_{i,jn}</code> is the number of words in the sentence.
</p>
<p>The core value in the cluster, the polarized word is acted upon by valence
shifters. Amplifiers (intensifiers) increase the polarity by 1.8 (.8 is the default weight
(<code class="reqn">z</code>)).  Amplifiers (<code class="reqn">w_{i,j,k}^{a}</code>) become de-amplifiers if the context
cluster contains an odd number of negators (<code class="reqn">w_{i,j,k}^{n}</code>).  De-amplifiers
(downtoners) work to decrease the polarity.  Negation (<code class="reqn">w_{i,j,k}^{n}</code>) acts on
amplifiers/de-amplifiers as discussed but also flip the sign of the polarized
word.  Negation is determined by raising -1 to the power of the number of
negators (<code class="reqn">w_{i,j,k}^{n}</code>) + 2.  Simply, this is a result of a belief that two
negatives equal a positive, 3 negatives a negative and so on.
</p>
<p>The adversative conjunctions (i.e., 'but', 'however', and 'although') also 
weight the context cluster.  A adversative conjunction before the polarized 
word (<code class="reqn">w_{adversative\,conjunction}, ..., w_{i, j, k}^{p}</code>) up-weights 
the cluster by 
<code class="reqn">1 + z_2 * \{|w_{adversative\,conjunction}|, ..., w_{i, j, k}^{p}\}</code> 
(.85 is the default weight (<code class="reqn">z_2</code>)).  An adversative conjunction after 
the polarized word down-weights the cluster by
<code class="reqn">1 + \{w_{i, j, k}^{p}, ..., |w_{adversative\,conjunction}| * -1\} * z_2</code>.  
The number of occurrences before and after the polarized word are multiplied by
1 and -1 respectively and then summed within context cluster.  It is this
value that is multiplied by the weight and added to 1. This
corresponds to the belief that an adversative conjunction makes the next 
clause of greater values while lowering the value placed on the prior clause.
</p>
<p>The researcher may provide a weight <code class="reqn">z</code> to be utilized with
amplifiers/de-amplifiers (default is .8; de-amplifier weight is constrained
to -1 lower bound).  Last, these weighted context clusters (<code class="reqn">c_{i,j,l}</code>) are
summed (<code class="reqn">c'_{i,j}</code>) and divided by the square root of the word count (<code class="reqn">\sqrt{w_{i,jn}}</code>) yielding an <strong>unbounded
polarity score</strong> (<code class="reqn">\delta</code>) for each sentence.
</p>
<p style="text-align: center;"><code class="reqn">\delta=\frac{c'_{i,j}}{\sqrt{w_{i,jn}}}</code>
</p>

<p>Where:
</p>
<p style="text-align: center;"><code class="reqn">c'_{i,j}=\sum{((1 + w_{amp} + w_{deamp})\cdot w_{i,j,k}^{p}(-1)^{2 + w_{neg}})}</code>
</p>

<p style="text-align: center;"><code class="reqn">w_{amp}= (w_{b} &gt; 1) + \sum{(w_{neg}\cdot (z \cdot w_{i,j,k}^{a}))}</code>
</p>

<p style="text-align: center;"><code class="reqn">w_{deamp} = \max(w_{deamp'}, -1)</code>
</p>

<p style="text-align: center;"><code class="reqn">w_{deamp'}= (w_{b} &lt; 1) + \sum{(z(- w_{neg}\cdot w_{i,j,k}^{a} + w_{i,j,k}^{d}))}</code>
</p>

<p style="text-align: center;"><code class="reqn">w_{b} = 1 + z_2 * w_{b'}</code>
</p>

<p style="text-align: center;"><code class="reqn">w_{b'} = \sum{\\(|w_{adversative\,conjunction}|, ..., w_{i, j, k}^{p}, w_{i, j, k}^{p}, ..., |w_{adversative\,conjunction}| * -1}\\)</code>
</p>

<p style="text-align: center;"><code class="reqn">w_{neg}= \left(\sum{w_{i,j,k}^{n}}\right) \bmod {2}</code>
</p>



<h3>Value</h3>

<p>Returns a <span class="pkg">data.table</span> of:
</p>

<ul>
<li><p>  element_id - The id number of the original vector passed to <code>sentiment</code>
</p>
</li>
<li><p>  sentence_id - The id number of the sentences within each <code>element_id</code>
</p>
</li>
<li><p>  word_count - Word count
</p>
</li>
<li><p>  sentiment - Sentiment/polarity score (note: sentiments less than zero is negative, 0 is neutral, and greater than zero positive polarity)
</p>
</li></ul>



<h3>Note</h3>

<p>The polarity score is dependent upon the polarity dictionary used.
This function defaults to a combined and augmented version of Jocker's (2017) 
[originally exported by the <span class="pkg">syuzhet</span> package] &amp; Rinker's augmented Hu &amp; Liu (2004) 
dictionaries in the <span class="pkg">lexicon</span> package, however, this may not be appropriate, for 
example, in the context of children in a classroom.  The user may (is 
encouraged) to provide/augment the dictionary (see the <code>as_key</code> 
function).  For instance the word &quot;sick&quot; in a high school setting may mean 
that something is good, whereas &quot;sick&quot; used by a typical adult indicates 
something is not right or negative connotation (<strong>deixis</strong>).
</p>


<h3>References</h3>

<p>Jockers, M. L. (2017). Syuzhet: Extract sentiment and plot arcs 
from text. Retrieved from https://github.com/mjockers/syuzhet
</p>
<p>Hu, M., &amp; Liu, B. (2004). Mining opinion features in customer
reviews. National Conference on Artificial Intelligence.
</p>
<p>Halliday, M. A. K. &amp; Hasan, R. (2013). Cohesion in English. New York, NY: Routledge.
</p>
<p><a href="https://www.slideshare.net/jeffreybreen/r-by-example-mining-twitter-for">https://www.slideshare.net/jeffreybreen/r-by-example-mining-twitter-for</a>
</p>
<p><a href="http://hedonometer.org/papers.html">http://hedonometer.org/papers.html</a> Links to papers on hedonometrics
</p>


<h3>See Also</h3>

<p>Original URL: https://github.com/trestletech/Sermon-Sentiment-Analysis
</p>
<p>Other sentiment functions: 
<code><a href="#topic+sentiment_by">sentiment_by</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>mytext &lt;- c(
   'do you like it?  But I hate really bad dogs',
   'I am the best friend.',
   "Do you really like it?  I'm not a fan",
   "It's like a tree."
)

## works on a character vector but not the preferred method avoiding the 
## repeated cost of doing sentence boundary disambiguation every time 
## `sentiment` is run.  For small batches the loss is minimal.
## Not run: 
sentiment(mytext)

## End(Not run)

## preferred method avoiding paying the cost 
mytext &lt;- get_sentences(mytext)
sentiment(mytext)
sentiment(mytext, question.weight = 0)

sam_dat &lt;- get_sentences(gsub("Sam-I-am", "Sam I am", sam_i_am))
(sam &lt;- sentiment(sam_dat))
plot(sam)
plot(sam, scale_range = TRUE, low_pass_size = 5)
plot(sam, scale_range = TRUE, low_pass_size = 10)
    
## Not run: ## legacy transform functions from suuzhet
plot(sam, transformation.function = syuzhet::get_transformed_values)
plot(sam, transformation.function = syuzhet::get_transformed_values,  
    scale_range = TRUE, low_pass_size = 5)

## End(Not run)

y &lt;- get_sentences(
    "He was not the sort of man that one would describe as especially handsome."
)
sentiment(y)
sentiment(y, n.before=Inf)

## Not run: ## Categorize the polarity (tidyverse vs. data.table):
library(dplyr)
sentiment(mytext) %&gt;%
as_tibble() %&gt;%
    mutate(category = case_when(
        sentiment &lt; 0 ~ 'Negative', 
        sentiment == 0 ~ 'Neutral', 
        sentiment &gt; 0 ~ 'Positive'
    ) %&gt;%
    factor(levels = c('Negative', 'Neutral', 'Positive'))
)

library(data.table)
dt &lt;- sentiment(mytext)[, category := factor(fcase(
        sentiment &lt; 0, 'Negative', 
        sentiment == 0, 'Neutral', 
        sentiment &gt; 0, 'Positive'
    ), levels = c('Negative', 'Neutral', 'Positive'))][]
dt

## End(Not run)

dat &lt;- data.frame(
    w = c('Person 1', 'Person 2'),
    x = c(paste0(
        "Mr. Brown is nasty! He says hello. i give him rage.  i will ",
        "go at 5 p. m. eastern time.  Angry thought in between!go there"
    ), "One more thought for the road! I am going now.  Good day and good riddance."),
    y = state.name[c(32, 38)], 
    z = c(.456, .124),
    stringsAsFactors = FALSE
)
sentiment(get_sentences(dat$x))
sentiment(get_sentences(dat))

## Not run: 
## tidy approach
library(dplyr)
library(magrittr)

hu_liu_cannon_reviews %&gt;%
   mutate(review_split = get_sentences(text)) %$%
   sentiment(review_split)

## End(Not run)

## Emojis
## Not run: 
## Load R twitter data
x &lt;- read.delim(system.file("docs/r_tweets.txt", package = "textclean"), 
    stringsAsFactors = FALSE)

x

library(dplyr); library(magrittr)

## There are 2 approaches
## Approach 1: Replace with words
x %&gt;%
    mutate(Tweet = replace_emoji(Tweet)) %$%
    sentiment(Tweet)

## Approach 2: Replace with identifier token
combined_emoji &lt;- update_polarity_table(
    lexicon::hash_sentiment_jockers_rinker,
    x = lexicon::hash_sentiment_emojis
)

x %&gt;%
    mutate(Tweet = replace_emoji_identifier(Tweet)) %$%
    sentiment(Tweet, polarity_dt = combined_emoji)
    
## Use With Non-ASCII
## Warning: sentimentr has not been tested with languages other than English.
## The example below is how one might use sentimentr if you believe the 
## language you are working with are similar enough in grammar to for
## sentimentr to be viable (likely Germanic languages)
## english_sents &lt;- c(
##     "I hate bad people.",
##     "I like yummy cookie.",
##     "I don't love you anymore; sorry."
## )

## Roughly equivalent to the above English
danish_sents &lt;- stringi::stri_unescape_unicode(c(
    "Jeg hader d\\u00e5rlige mennesker.", 
    "Jeg kan godt lide l\\u00e6kker is.", 
    "Jeg elsker dig ikke mere; undskyld."
))

danish_sents

## Polarity terms
polterms &lt;- stringi::stri_unescape_unicode(
    c('hader', 'd\\u00e5rlige', 'undskyld', 'l\\u00e6kker', 'kan godt', 'elsker')
)

## Make polarity_dt
danish_polarity &lt;- as_key(data.frame(
    x = stringi::stri_unescape_unicode(polterms),
    y = c(-1, -1, -1, 1, 1, 1)
))

## Make valence_shifters_dt
danish_valence_shifters &lt;- as_key(
    data.frame(x='ikke', y="1"), 
    sentiment = FALSE, 
    comparison = NULL
)

sentiment(
    danish_sents, 
    polarity_dt = danish_polarity, 
    valence_shifters_dt = danish_valence_shifters, 
    retention_regex = "\\d:\\d|\\d\\s|[^\\p{L}',;: ]"
)

## A way to test if you need [:alpha:] vs \p{L} in `retention_regex`:

## 1. Does it wreck some of the non-ascii characters by default?
sentimentr:::make_sentence_df2(danish_sents) 

## 2.Does this?
sentimentr:::make_sentence_df2(danish_sents, "\\d:\\d|\\d\\s|[^\\p{L}',;: ]")

## If you answer yes to #1 but no to #2 you likely want \p{L}

## End(Not run)
</code></pre>

<hr>
<h2 id='sentiment_attributes'>Extract Sentiment Attributes from Text</h2><span id='topic+sentiment_attributes'></span>

<h3>Description</h3>

<p>This function utilizes <span class="pkg">gofastr</span> and <span class="pkg">termco</span> to extract sentiment
based attributes (attributes concerning polarized words and valence 
shifters) from a text.  Attributes include the rate of polarized terms
and valence shifters relative to number of words.  Additionally, coocurrence
rates for valence shifters are computed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sentiment_attributes(
  text.var,
  polarity_dt = lexicon::hash_sentiment_jockers_rinker,
  valence_shifters_dt = lexicon::hash_valence_shifters,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sentiment_attributes_+3A_text.var">text.var</code></td>
<td>
<p>The text variable.</p>
</td></tr>
<tr><td><code id="sentiment_attributes_+3A_polarity_dt">polarity_dt</code></td>
<td>
<p>A <span class="pkg">data.table</span> of positive/negative words and
weights with x and y as column names.</p>
</td></tr>
<tr><td><code id="sentiment_attributes_+3A_valence_shifters_dt">valence_shifters_dt</code></td>
<td>
<p>A <span class="pkg">data.table</span> of valence shifters that
can alter a polarized word's meaning and an integer key for negators (1),
amplifiers(2), de-amplifiers (3) and adversative conjunctions (4) with x and 
y as column names.</p>
</td></tr>
<tr><td><code id="sentiment_attributes_+3A_...">...</code></td>
<td>
<p>ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list of four items:
</p>
<table>
<tr><td><code>Meta</code></td>
<td>
<p>The number of words, sentences, and questions in the text</p>
</td></tr>
<tr><td><code>Attributes</code></td>
<td>
<p>The rate of sentiment attributes relative to the number of words</p>
</td></tr>
<tr><td><code>Polarized_Cooccurrences</code></td>
<td>
<p>The rate that valence shifters cooccur with a polarized word in the same sentence</p>
</td></tr>
<tr><td><code>Cooccurrences</code></td>
<td>
<p>A cooccurrence matrix of sentiment attributes; 'polarized' is the sum of positive and negative</p>
</td></tr>
</table>


<h3>Note</h3>

<p><span class="pkg">gofastr</span> and <span class="pkg">termco</span> must be installed.  If they are not (which
they are not part of <span class="pkg">sentimentr</span> install) then the function will prompt
you to attempt to install them using <code>install.packages</code> and
<code>ghit::install_github</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
sentiment_attributes(presidential_debates_2012$dialogue)

## End(Not run)
</code></pre>

<hr>
<h2 id='sentiment_by'>Polarity Score (Sentiment Analysis) By Groups</h2><span id='topic+sentiment_by'></span>

<h3>Description</h3>

<p>Approximate the sentiment (polarity) of text by grouping variable(s).  For a
full description of the sentiment detection algorithm see 
<code><a href="#topic+sentiment">sentiment</a></code>.  See <code><a href="#topic+sentiment">sentiment</a></code>
for more details about the algorithm, the sentiment/valence shifter keys
that can be passed into the function, and other arguments that can be passed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sentiment_by(
  text.var,
  by = NULL,
  averaging.function = sentimentr::average_downweighted_zero,
  group.names,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sentiment_by_+3A_text.var">text.var</code></td>
<td>
<p>The text variable.  Also takes a <code>sentimentr</code> or
<code>sentiment_by</code> object.</p>
</td></tr>
<tr><td><code id="sentiment_by_+3A_by">by</code></td>
<td>
<p>The grouping variable(s).  Default <code>NULL</code> uses the original
row/element indices; if you used a column of 12 rows for <code>text.var</code>
these 12 rows will be used as the grouping variable.  Also takes a single
grouping variable or a list of 1 or more grouping variables.</p>
</td></tr>
<tr><td><code id="sentiment_by_+3A_averaging.function">averaging.function</code></td>
<td>
<p>A function for performing the group by averaging.
The default, <code><a href="#topic+average_downweighted_zero">average_downweighted_zero</a></code>, downweights 
zero values in the averaging.  Note that the function must handle 
<code>NA</code>s.  The <span class="pkg">sentimentr</span> functions 
<code>average_weighted_mixed_sentiment</code> and <code>average_mean</code> are also 
available.  The former upweights negative when the analysts suspects the 
speaker is likely to surround negatives with positives (mixed) as a polite 
social convention but still the affective state is negative.  The later is a 
standard mean average.</p>
</td></tr>
<tr><td><code id="sentiment_by_+3A_group.names">group.names</code></td>
<td>
<p>A vector of names that corresponds to group.  Generally
for internal use.</p>
</td></tr>
<tr><td><code id="sentiment_by_+3A_...">...</code></td>
<td>
<p>Other arguments passed to <code><a href="#topic+sentiment">sentiment</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a <span class="pkg">data.table</span> with grouping variables plus:
</p>

<ul>
<li><p>  element_id - The id number of the original vector passed to <code>sentiment</code>
</p>
</li>
<li><p>  sentence_id - The id number of the sentences within each <code>element_id</code>
</p>
</li>
<li><p>  word_count - Word count <code><a href="base.html#topic+sum">sum</a></code>med by grouping variable
</p>
</li>
<li><p>  sd - Standard deviation (<code><a href="stats.html#topic+sd">sd</a></code>) of the sentiment/polarity score by grouping variable
</p>
</li>
<li><p>  ave_sentiment - Sentiment/polarity score <code><a href="base.html#topic+mean">mean</a></code> average by grouping variable
</p>
</li></ul>



<h3>Chaining</h3>

<p><span class="pkg">sentimentr</span> uses non-standard evaluation when you use <code>with()</code> OR 
<code>%$%</code> (<span class="pkg">magrittr</span>) and looks for the vectors within the data set 
passed to it. There is one exception to this...when you pass a 
<code>get_sentences()</code> object to <code>sentiment_by()</code> to the first argument 
which is <code>text.var</code> it calls the <code>sentiment_by.get_sentences_data_frame</code> 
method which requires <code>text.var</code> to be a <code>get_sentences_data_frame</code> 
object. Because this object is a <code>data.frame</code> its method knows this and 
knows it can access the columns of the <code>get_sentences_data_frame</code> object 
directly (usually <code>text.var</code> is an atomic vector), it just needs the 
names of the columns to grab.
</p>
<p>To illustrate this point understand that all three of these approaches 
result in exactly the same output:
</p>
<pre>
## method 1
presidential_debates_2012 %&gt;%
    get_sentences() %&gt;%
    sentiment_by(by = c('person', 'time'))

## method 2
presidential_debates_2012 %&gt;%
    get_sentences() %$%
    sentiment_by(., by = c('person', 'time'))

## method 3
presidential_debates_2012 %&gt;%
    get_sentences() %$%
    sentiment_by(dialogue, by = list(person, time))
</pre>
<p>Also realize that a <code>get_sentences_data_frame</code> object also has a column
with a <code>get_sentences_character</code> class column which also has a method in
<span class="pkg">sentimentr</span>.
</p>
<p>When you use <code>with()</code> OR <code>%$%</code> then you're not actually passing
the <code>get_sentences_data_frame</code> object to <span class="pkg">sentimentr</span> and hence the
<code>sentiment_by.get_sentences_data_frame</code> method isn't called rather
<code>sentiment_by</code> is evaluated in the environment/data of the
<code>get_sentences_data_frame object</code>. You can force the object passed this
way to be evaluated as a <code>get_sentences_data_frame</code> object and thus
calling the <code>sentiment_by.get_sentences_data_frame</code> method by using the
<code>.</code> operator as I've done in method 2 above. Otherwise you pass the name
of the text column which is actually a <code>get_sentences_character class</code>
and it calls its own method. In this case the by argument expects vectors or
a list of vectors and since it's being evaluated within the data set you can
use <code>list()</code>.
</p>


<h3>See Also</h3>

<p>Other sentiment functions: 
<code><a href="#topic+sentiment">sentiment</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>mytext &lt;- c(
   'do you like it?  It is red. But I hate really bad dogs',
   'I am the best friend.',
   "Do you really like it?  I'm not happy"
)

## works on a character vector but not the preferred method avoiding the 
## repeated cost of doing sentence boundary disambiguation every time 
## `sentiment` is run
## Not run: 
sentiment(mytext)
sentiment_by(mytext)

## End(Not run)

## preferred method avoiding paying the cost 
mytext &lt;- get_sentences(mytext)

sentiment_by(mytext)
sentiment_by(mytext, averaging.function = average_mean)
sentiment_by(mytext, averaging.function = average_weighted_mixed_sentiment)
get_sentences(sentiment_by(mytext))

(mysentiment &lt;- sentiment_by(mytext, question.weight = 0))
stats::setNames(get_sentences(sentiment_by(mytext, question.weight = 0)),
    round(mysentiment[["ave_sentiment"]], 3))

pres_dat &lt;- get_sentences(presidential_debates_2012)

## Not run: 
## less optimized way
with(presidential_debates_2012, sentiment_by(dialogue, person))

## End(Not run)

## Not run: 
sentiment_by(pres_dat, 'person')

(out &lt;- sentiment_by(pres_dat, c('person', 'time')))
plot(out)
plot(uncombine(out))

sentiment_by(out, presidential_debates_2012$person)
with(presidential_debates_2012, sentiment_by(out, time))

highlight(with(presidential_debates_2012, sentiment_by(out, list(person, time))))

## End(Not run)

## Not run: 
## tidy approach
library(dplyr)
library(magrittr)

hu_liu_cannon_reviews %&gt;%
   mutate(review_split = get_sentences(text)) %$%
   sentiment_by(review_split)

## End(Not run)
</code></pre>

<hr>
<h2 id='sentimentr'>Calculate Text Polarity Sentiment</h2><span id='topic+sentimentr'></span><span id='topic+package-sentiment'></span>

<h3>Description</h3>

<p>Calculate text polarity sentiment in the English language at the sentence 
level and optionally aggregate by rows or grouping variable(s).
</p>

<hr>
<h2 id='uncombine'>Ungroup a <code>sentiment_by</code> Object to the Sentence Level</h2><span id='topic+uncombine'></span>

<h3>Description</h3>

<p>Ungroup a <code>sentiment_by</code> object, stretching to the <code>element_id</code> and
<code>sentence_id</code> levels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>uncombine(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="uncombine_+3A_x">x</code></td>
<td>
<p>A <code>sentiment_by</code> object.</p>
</td></tr>
<tr><td><code id="uncombine_+3A_...">...</code></td>
<td>
<p>Ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a <span class="pkg">data.table</span> with grouping variables plus:
</p>

<ul>
<li><p>  element_id - The id number of the original vector passed to <code>sentiment</code>
</p>
</li>
<li><p>  word_count - Word count <code><a href="base.html#topic+sum">sum</a></code>med by grouping variable
</p>
</li>
<li><p>  sd - Standard deviation (<code><a href="stats.html#topic+sd">sd</a></code>) of the sentiment/polarity score by grouping variable
</p>
</li>
<li><p>  ave_sentiment - Sentiment/polarity score <code><a href="base.html#topic+mean">mean</a></code> average by grouping variable
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>mytext &lt;- c(
   'do you like it?  But I hate really bad dogs',
   'I am the best friend.',
   "Do you really like it?  I'm not happy"
)

mytext &lt;- get_sentences(mytext)
(x &lt;- sentiment_by(mytext))
uncombine(x)

## Not run: 
(y &lt;- with(
    presidential_debates_2012, 
    sentiment_by(
        text.var = get_sentences(dialogue), 
        by = list(person, time)
    )
))
uncombine(y)

## End(Not run)
</code></pre>

<hr>
<h2 id='validate_sentiment'>Validate Sentiment Score Sign Against Known Results</h2><span id='topic+validate_sentiment'></span>

<h3>Description</h3>

<p>Provides a multiclass macroaverage/microaverage of precision, recall, 
accuracy, and F-score for the sign of the predicted sentiment against known 
sentiment scores.  There are three classes sentiment analysis generally 
predicts: positive (&gt; 0), negative (&lt; 0) and neutral (= 0).  In assessing 
model performance one can use macro- or micro- averaging across classes.  
Macroaveraging allows every class to have an equal say.  Microaveraging gives 
larger say to larger classes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>validate_sentiment(predicted, actual, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="validate_sentiment_+3A_predicted">predicted</code></td>
<td>
<p>A numeric vector of predicted sentiment scores or a 
<span class="pkg">sentimentr</span> object that returns sentiment scores.</p>
</td></tr>
<tr><td><code id="validate_sentiment_+3A_actual">actual</code></td>
<td>
<p>A numeric vector of known sentiment ratings.</p>
</td></tr>
<tr><td><code id="validate_sentiment_+3A_...">...</code></td>
<td>
<p>ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a <code><a href="base.html#topic+data.frame">data.frame</a></code> with a macroaveraged and 
microaveraged model validation scores.  Additionally, the
<code><a href="base.html#topic+data.frame">data.frame</a></code> has the following attributes:
</p>
<table>
<tr><td><code>confusion_matrix</code></td>
<td>
<p>A confusion matrix of all classes</p>
</td></tr>
<tr><td><code>class_confusion_matrices</code></td>
<td>
<p>A <code><a href="base.html#topic+list">list</a></code> of class level (class vs. all) confusion matrices</p>
</td></tr>
<tr><td><code>macro_stats</code></td>
<td>
<p>A <code><a href="base.html#topic+data.frame">data.frame</a></code> of the macroaverged class level stats before averaging</p>
</td></tr>
<tr><td><code>mda</code></td>
<td>
<p>Mean Directional Accuracy</p>
</td></tr>
<tr><td><code>mare</code></td>
<td>
<p>Mean Absolute Rescaled Error</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Mean Absolute Rescaled Error (MARE) is defined as: 
<code class="reqn">\frac{\sum{|actual - predicted|}}{2n}</code> and gives a sense of, on average, 
how far off were the rescaled predicted values (-1 to 1) from the rescaled 
actual values (-1 to 1).  A value of 0 means perfect accuracy.  A value of
1 means perfectly wrong every time.  A value of .5 represents expected value
for random guessing.  This measure is related to 
<a href="https://en.wikipedia.org/wiki/Mean_absolute_error">Mean Absolute Error</a>.
</p>


<h3>References</h3>

<p><a href="https://www.youtube.com/watch?v=OwwdYHWRB5E&amp;index=31&amp;list=PL6397E4B26D00A269">https://www.youtube.com/watch?v=OwwdYHWRB5E&amp;index=31&amp;list=PL6397E4B26D00A269</a> <br />
<a href="https://en.wikipedia.org/wiki/Mean_Directional_Accuracy_(MDA)">https://en.wikipedia.org/wiki/Mean_Directional_Accuracy_(MDA)</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>actual &lt;- c(1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, 1,-1)
predicted &lt;- c(1, 0, 1, -1, 1, 0, -1, -1, -1, -1, 0, 1,-1)
validate_sentiment(predicted, actual)

scores &lt;- hu_liu_cannon_reviews$sentiment
mod &lt;- sentiment_by(get_sentences(hu_liu_cannon_reviews$text))

validate_sentiment(mod$ave_sentiment, scores)
validate_sentiment(mod, scores)

x &lt;- validate_sentiment(mod, scores)
attributes(x)$confusion_matrix
attributes(x)$class_confusion_matrices
attributes(x)$macro_stats

## Annie Swafford Example
swafford &lt;- data.frame(
    text = c(
        "I haven't been sad in a long time.",
        "I am extremely happy today.",
        "It's a good day.",
        "But suddenly I'm only a little bit happy.",
        "Then I'm not happy at all.",
        "In fact, I am now the least happy person on the planet.",
        "There is no happiness left in me.",
        "Wait, it's returned!",
        "I don't feel so bad after all!"
    ), 
    actual = c(.8, 1, .8, -.1, -.5, -1, -1, .5, .6), 
    stringsAsFactors = FALSE
)

pred &lt;- sentiment_by(swafford$text) 
validate_sentiment(
    pred,
    actual = swafford$actual
)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
