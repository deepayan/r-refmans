<!DOCTYPE html><html lang="en"><head><title>Help for package bda</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {bda}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bda'>
<p>Binned (Grouped) Data Analysis</p></a></li>
<li><a href='#binning'><p>Data Binning</p></a></li>
<li><a href='#bootkde'><p>Density estimation for data with rounding errors</p></a></li>
<li><a href='#bootPRO'><p>Effectiveness Evaluation based on PROs with bootstrapping method</p></a></li>
<li><a href='#BreastCancer'><p>Breast Cancer Data</p></a></li>
<li><a href='#EUFirmSize'><p>Firm size data of 10 EU countries</p></a></li>
<li><a href='#fit.FSD'><p>Fitting firm size-age distributions</p></a></li>
<li><a href='#fit.GBP'><p>Fitting Mixture Model of Generalized Beta and Pareto</p></a></li>
<li><a href='#fit.lognormal'><p>Fitting log-normal distributions</p></a></li>
<li><a href='#fit.Pareto'><p>Fit a Pareto Distribution to Binned Data</p></a></li>
<li><a href='#fit.PRO'><p>Fitting distributions to Patient-reported Outcome Data</p></a></li>
<li><a href='#fnm'><p>Distribution of Two Finite Gaussian Mixtures</p></a></li>
<li><a href='#FSD'><p>Firm size data</p></a></li>
<li><a href='#ImportFSD'><p>Import Firm Size and Firm Age Data</p></a></li>
<li><a href='#lps.variance'><p>compute the variance of the local polynomial regression function</p></a></li>
<li><a href='#lpsmooth'><p>non-parametric regression</p></a></li>
<li><a href='#mediation.test'><p>The Sobel mediation test</p></a></li>
<li><a href='#mlnorm'><p>The mixed lognormal distribution</p></a></li>
<li><a href='#mnorm'><p>The mixed normal distribution</p></a></li>
<li><a href='#ofc'><p>occipitofrontal head circumference data</p></a></li>
<li><a href='#Pain'><p>Pain data</p></a></li>
<li><a href='#Pareto'><p>The Pareto distribution</p></a></li>
<li><a href='#pro.test'>
<p>Test effectiveness based on PROs</p></a></li>
<li><a href='#VAS'><p>Algorithms for Visual Analogue Scales</p></a></li>
<li><a href='#wkde'>
<p>Compute a Binned Kernel Density Estimate for Weighted Data</p></a></li>
<li><a href='#Zipf.Normalize'><p>Zipf Normalization</p></a></li>
<li><a href='#ZipfPlot'><p>Draw Zipf Plot</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Version:</td>
<td>18.3.2</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-08-25</td>
</tr>
<tr>
<td>Title:</td>
<td>Binned Data Analysis</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Bin Wang &lt;bwang831@gmail.com&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0), boot</td>
</tr>
<tr>
<td>Description:</td>
<td>Algorithms developed for binned data analysis,
  gene expression data analysis and
  measurement error models for ordinal data analysis. </td>
</tr>
<tr>
<td>License:</td>
<td>Unlimited</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-08-26 00:19:44 UTC; alkb</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-08-26 03:50:02 UTC</td>
</tr>
<tr>
<td>Author:</td>
<td>Bin Wang [aut, cre]</td>
</tr>
</table>
<hr>
<h2 id='bda'>
Binned (Grouped) Data Analysis
</h2><span id='topic+bda'></span>

<h3>Description</h3>

<p>This package collects algorithms for binned (grouped) data analysis.
</p>


<h3>References</h3>

<p>Wang B. (2020).
<em>A Zipf-plot based normalization method for high-throughput RNA-seq
data</em>. 
PLOS ONE 15(4): e0230594.
</p>
<p>Wang, B. and Wang, X-F. (2015).
<em>Fitting The Generalized Lambda Distribution To Pre-binned Data</em>.
Journal of Statistical Computation and Simulation. Vol. 86 , Iss. 9, 1785-1797.
</p>
<p>Ge C, Zhang S-G, Wang B (2020) Modeling the joint distribution of firm size and firm age based on grouped data. PLoS ONE 15(7): e0235282. doi.org/10.1371/journal.pone.0235282
</p>
<p>Wang, B. and Wertelecki, W. (2013) Density Estimation for Data With Rounding Errors, Computational Statistics and Data Analysis, 65: 4-12. doi: 10.1016/j.csda.2012.02.016.
</p>
<p>Wang, X-F. and Wang, B. (2011) Deconvolution Estimation in Measurement Error Models: The R Package decon, Journal of Statistical Software, 39(10), 1-24.
</p>

<hr>
<h2 id='binning'>Data Binning</h2><span id='topic+binning'></span><span id='topic+binning.default'></span><span id='topic+binning.histogram'></span><span id='topic+plot.bdata'></span><span id='topic+print.bdata'></span><span id='topic+.bdaConnect'></span>

<h3>Description</h3>

<p>To bin a univariate data set in to a consecutive bins.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  binning(x, counts, breaks,lower.limit, upper.limit)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="binning_+3A_x">x</code></td>
<td>
<p>A vector of raw data. 'NA' values will be automatically 
removed.</p>
</td></tr>
<tr><td><code id="binning_+3A_counts">counts</code></td>
<td>
<p>Frequencies or counts of observations in different 
classes (bins)</p>
</td></tr>
<tr><td><code id="binning_+3A_breaks">breaks</code></td>
<td>
<p>The break points for data binning.</p>
</td></tr>
<tr><td><code id="binning_+3A_lower.limit">lower.limit</code>, <code id="binning_+3A_upper.limit">upper.limit</code></td>
<td>
<p>The lower and upper limits of the bins.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To create a 'bdata' object. If 'x' is given, a histogram will be
created. Otherwise, create a histogram-type data using 'counts' and 
'breaks' (or class limits with 'lower.limit' and/or 'upper.limit').
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>ll</code></td>
<td>
<p>lower limits</p>
</td></tr>
<tr><td><code>ul</code></td>
<td>
<p>upper limits</p>
</td></tr>
<tr><td><code>freq</code></td>
<td>
<p>frequencies</p>
</td></tr>
<tr><td><code>xhist</code></td>
<td>
<p>histogram</p>
</td></tr>
<tr><td><code>xZipf</code></td>
<td>
<p>Zipf plot</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- c(10, 21, 56,79,114,122,110,85,85,61,47,49,47,44,31,20,11,4,4)
x &lt;- 14.5 + c(0:length(y))
out1 &lt;- binning(counts=y, breaks=x)
plot(out1)

z = rnorm(100, 34.5, 1.6)
out1 &lt;- binning(z)
plot(out1)

data(FSD)

x &lt;- as.numeric(FirmAge[38,]);
age &lt;- c(0,1:6,11,16,21,26,38);
y &lt;- binning(counts=x, lower.limit=age)
plot(y)
plot(y, type="Zipf")

x &lt;- as.numeric(FirmSize[38,]);
names(FirmSize)
ll &lt;- c(1,5,10,20,50,100,250,500,1000,2500,5000,10000);
ul &lt;- c(4,9,19,49,99,249,499,999,2499,4999,9999,Inf)
y &lt;- binning(counts=x, lower.limit=ll,upper.limit=ul)
plot(y)
plot(y, type="Zipf")
</code></pre>

<hr>
<h2 id='bootkde'>Density estimation for data with rounding errors</h2><span id='topic+bootkde'></span>

<h3>Description</h3>

<p>To estimate density function based on data with rounding errors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> bootkde(x,freq,a,b,from, to, gridsize=512L,method='boot')
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bootkde_+3A_x">x</code>, <code id="bootkde_+3A_freq">freq</code></td>
<td>
<p>raw data if 'freq' missing, otherwise as distinct values with frequencies.</p>
</td></tr>
<tr><td><code id="bootkde_+3A_a">a</code>, <code id="bootkde_+3A_b">b</code></td>
<td>
<p>the lower and upper bounds of the rounding error.</p>
</td></tr>
<tr><td><code id="bootkde_+3A_from">from</code>, <code id="bootkde_+3A_to">to</code>, <code id="bootkde_+3A_gridsize">gridsize</code></td>
<td>
<p>start point, end point and size of a fine 
grid where the distribution will be evaluated.</p>
</td></tr>
<tr><td><code id="bootkde_+3A_method">method</code></td>
<td>
<p>type estimate: &quot;mle&quot; or &quot;Berkson&quot;, or &quot;boot&quot; (default).</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>y</code></td>
<td>
<p>Estimated values of the smooth function over a fine grid.</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>grid points where the smoothed function are evaluated.</p>
</td></tr>
<tr><td><code>pars</code></td>
<td>
<p>return 'bw' for Berkson method, or return the mean and SD for MLE method</p>
</td></tr>
</table>


<h3>References</h3>

<p>Wang, B and Wertelecki, W, (2013)
Computational Statistics and Data Analysis,
65: 4-12.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> #data(ofc)
 #x0 = round(ofc$Head)
 x0 = round(rnorm(100,34.5,1.6))
 fx1 = bootkde(x0,a=-0.5,b=0.5,method="Berkson")
 </code></pre>

<hr>
<h2 id='bootPRO'>Effectiveness Evaluation based on PROs with bootstrapping method</h2><span id='topic+bootPRO'></span>

<h3>Description</h3>

<p>Effectiveness Evaluation based on PROs with bootstrapping method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> bootPRO(x,type="relative",MCID,iter=999,conf.level=0.95)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bootPRO_+3A_x">x</code></td>
<td>
<p>Data frame of repeated PRO measures. First column gives the
treatment groups: 'group' or 'treat'; Repeated measures listed after
column 1.</p>
</td></tr>
<tr><td><code id="bootPRO_+3A_type">type</code></td>
<td>
<p>use absolute changes (&quot;absolute&quot;) or relative (percent)
changes (&quot;relative&quot;).</p>
</td></tr>
<tr><td><code id="bootPRO_+3A_mcid">MCID</code></td>
<td>
<p>A positive value to define responders.</p>
</td></tr>
<tr><td><code id="bootPRO_+3A_iter">iter</code></td>
<td>
<p>number of iterations used by the bootstrap algorithm.</p>
</td></tr>
<tr><td><code id="bootPRO_+3A_conf.level">conf.level</code></td>
<td>
<p>Confidence level of the bootstrapping confidence interval.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>NONE.</p>


<h3>References</h3>

<p>To be updated.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(Pain)
  x &lt;- pain[,c(2,3:9,11:17)]
  grp &lt;- rep("treat",nrow(x))
  grp[x[,1]==0] &lt;- "control"
  x[,1] &lt;- grp
  #bootPRO(x,type='mean')
  #bootPRO(x,type='mean',MCID=1)
  #bootPRO(x,type='mean',MCID=1.5)
  #bootPRO(x,type='mean',MCID=2)

  #bootPRO(x,type='rel')
  #bootPRO(x,type='relative',MCID=.2)
  #bootPRO(x,type='relative',MCID=.3)
  ##bootPRO(x,type='relative',MCID=.5)
 </code></pre>

<hr>
<h2 id='BreastCancer'>Breast Cancer Data</h2><span id='topic+normal'></span><span id='topic+primary'></span><span id='topic+meta'></span><span id='topic+LCL'></span>

<h3>Description</h3>

<p>Cleaned breast cancer data from TCGA. 
Three datasets: primary, normal and meta.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(BreastCancer)</code></pre>


<h3>Value</h3>

<p>None</p>


<h3>References</h3>

<p>TCGA(2012)
Comprehensive molecular portraits of human breast tumours.
Nature,490(7418),61-70.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> data(BreastCancer)
 head(normal)
 head(primary)
 head(meta)
</code></pre>

<hr>
<h2 id='EUFirmSize'>Firm size data of 10 EU countries</h2><span id='topic+EUFirmSize'></span>

<h3>Description</h3>

<p>Mining and quarrying firm employment sizes.
Classes: &quot;0-9&quot;,&quot;10-19&quot;,&quot;20-49&quot;,&quot;50-249&quot;,&quot;250+&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(EUFirmSize)</code></pre>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>Class</code></td>
<td>
<p>Firm size classes</p>
</td></tr>
<tr><td><code>Year20xx</code></td>
<td>
<p>Firm size data of 10 EU countries for year 20xx.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Eurostat, Enterprises in Europe, Data 1994-95, fifth report Edition, European
Commission, Brussels, 1998.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> data(EUFirmSize)
 head(EUFirmSize)
</code></pre>

<hr>
<h2 id='fit.FSD'>Fitting firm size-age distributions</h2><span id='topic+fit.FSD'></span><span id='topic+fit.GLD'></span><span id='topic+fit.Copula'></span><span id='topic+print.FSD'></span><span id='topic+plot.FSD'></span><span id='topic+lines.FSD'></span>

<h3>Description</h3>

<p>To fit firm size and/or age distributions based on binned data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit.FSD(x,breaks,dist)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fit.FSD_+3A_x">x</code></td>
<td>
<p>'x' can be a vector or a matrix, or a 'histogram'.</p>
</td></tr>
<tr><td><code id="fit.FSD_+3A_breaks">breaks</code></td>
<td>
<p>a matrix with two columns if 'x' is a matrix. Otherwise, it is a vector. Can be missing if 'x' is a vector and 'x' will be grouped uisng the default parameters with <code>hist</code>. </p>
</td></tr>
<tr><td><code id="fit.FSD_+3A_dist">dist</code></td>
<td>
<p>distribution type for 'x' and/or 'y'. Options include <code>Weibull</code>, <code>gpd</code> &ndash; generalized Pareto distribution, <code>pd</code> or
<code>Pareto</code>, <code>EWD</code>&ndash; exponentiated Weibull distribtion.</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>x.fit</code>, <code>y.fit</code></td>
<td>
<p>Fitted marginal distribution for row- and column-data when 'x' is a matrix.</p>
</td></tr>
<tr><td><code>Psi</code></td>
<td>
<p>Plackett estimate of the Psi. ONLY for 'fit.FSD2'</p>
</td></tr>
</table>
<p>If each of <code>x.fit</code> and <code>y.fit</code>, the following values are available:
</p>
<table role = "presentation">
<tr><td><code>xhist</code></td>
<td>
<p>histogram.</p>
</td></tr>
<tr><td><code>dist</code></td>
<td>
<p>distribution type. Options include <code>Weibull</code>,
<code>gpd</code> &ndash; generalized Pareto distribution, <code>pd</code> or
<code>Pareto</code>, <code>EWD</code>&ndash; exponentiated Weibull distribtion.</p>
</td></tr>
<tr><td><code>size</code></td>
<td>
<p>Total number of observations (sample size)</p>
</td></tr>
<tr><td><code>pars</code></td>
<td>
<p>Estimates of parameters.</p>
</td></tr>
<tr><td><code>y</code>, <code>y2</code>, <code>x</code></td>
<td>
<p>the pdf (<code>y</code>) and cdf (<code>y2</code>) values evaluated on a grid <code>x</code></p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rweibull(1000,2,1)
(out &lt;- fit.FSD(x))

data(FSD)
b &lt;- c(0,1:5,6,11,16,21,26,38,Inf);
x &lt;- as.numeric(FirmAge[38,]);
## not run
##(out &lt;- fit.FSD(x))#treated as raw data
xh &lt;- binning(counts=x, breaks=b)
(out &lt;- fit.FSD(xh))
#(out &lt;- fit.FSD(xh,dist="ewd"))
(out &lt;- fit.FSD(xh,dist="pd"))
(out &lt;- fit.FSD(xh,dist="gpd"))


x &lt;- as.numeric(FirmSize[nrow(FirmSize),])
brks.size &lt;- c(0,4.5,9.5,19.5,49.5,99.5,249.5,499.5,
               999.5,2499.5,4999.5, 9999.5,Inf)
xh &lt;- binning(counts=x,breaks=brks.size)
Fn &lt;- cumsum(x)/sum(x);Fn
k &lt;- length(Fn)
i &lt;- c((k-5):(k-1))
out1 &lt;- fit.GLD(xh, qtl=brks.size[i+1],
               qtl.levels=Fn[i],lbound=0)

i &lt;- c(2,3,4,10,11)
out2 &lt;- fit.GLD(xh, qtl=brks.size[i+1],
               qtl.levels=Fn[i],lbound=0)

i &lt;- c(1,2,8,9,10)
out3 &lt;- fit.GLD(xh, qtl=brks.size[i+1],
               qtl.levels=Fn[i],lbound=0)

plot(xh,xlim=c(0,120))
lines(out1, col=2)
lines(out2, col=3)
lines(out3, col=4)

ZipfPlot(xh,plot=TRUE)
lines(log(1-out1$y2)~log(out1$x), col=2)
lines(log(1-out2$y2)~log(out2$x), col=3)
lines(log(1-out3$y2)~log(out3$x), col=4)

## sample codes for the figures and tables in the PLoS ONE manuscript

## Table 1 *****************************************************
#xtable(Firm2)

## Figure 1 ****************************************************

#rm(list=ls())
#require(bda)
#data(FSD)

##postscript(file='fig1.eps',paper='letter')
#par(mfrow=c(2,2))

#tmp &lt;- ImportFSD(FirmAge, year=2014,type="age");
#xh &lt;- tmp$age
#plot(xh, xlab="X", main="(a) Histogram of 2014 firm age data")
#fit0 &lt;- fit.FSD(xh, dist='exp')
#lines(fit0$x.fit$ly~fit0$x.fit$lx, lty=2)

#ZipfPlot(fit0, lty=2,
#         xlab="log(b)",ylab="log(r)",
#         main="(b) Zipf plot of firm age (2014)")

#tmp &lt;- ImportFSD(FirmAge, year=1988,type="age");
#xh2 &lt;- tmp$age
#fit1 &lt;- fit.FSD(xh2, dist='exp')
#ZipfPlot(fit1, lty=2,
#         xlab="log(b)",ylab="log(r)",
#         main="(c) Zipf plot of firm age (1988)")

#ZipfPlot(fit0, lty=2, type='l',
#         xlab="log(b)",ylab="log(r)",
#         main="(d) Zipf plots of firm age (1979-2014)")
#for(year in 1979:2014){
#    tmp &lt;- ImportFSD(FirmAge, year=year,type="age")
#    tmp2 &lt;- ZipfPlot(tmp$age, plot=FALSE)
#    lines(tmp2)
#}

#dev.off()

## Figure 2 ****************************************************

#rm(list=ls())
#require(bda)
#data(FSD)

#postscript(file='fig2.eps',paper='letter')
#par(mfrow=c(2,2))
## plot (a)
#tmp &lt;- ImportFSD(FirmSize, year=2014,type="size");
#yh &lt;- tmp$size
#plot(yh, xlab="Y", main="(a) Histogram of firm size (2014)")

## plot (b)
#lbrks &lt;- log(yh$breaks); lbrks[1] &lt;- log(1)
#cnts &lt;- yh$freq
#xhist2 &lt;- binning(counts=cnts,breaks=lbrks)
#plot(xhist2,xlab="log(Y)",
#     main="(b) Histogram of firm size (2014, log-scale)")

## plot (c)
#ZipfPlot(yh, plot=TRUE,
#         main="(c) Zipf plot firm size (2014)",
#         xlab="log(r)",ylab="log(b)")

## plot (d)

#ZipfPlot(yh, plot=TRUE,type='l',
#         main="(d) Zipf plots of firm size (1977-2014)",
#         xlab="log(r)",ylab="log(b)")

#res &lt;- NULL
#for(year in 1977:2014){
#    tmp &lt;- ImportFSD(FirmSize,year=year,type="size");
#    yh0 &lt;- tmp$size
#    zipf1 &lt;- ZipfPlot(yh0,plot.new=FALSE)
#    lines(zipf1)
#    res &lt;- c(res, zipf1$slope)
#}

#dev.off()

#mean(res); sd(res)
#quantile(res, prob=c(0.025,0.097))


## Figure 3a &amp; 3b ****************************************************
#rm(list=ls())
#require(bda)
#data(FSD)

#postscript(file='fig3a.eps',paper='letter')

tmp &lt;- ImportFSD(FirmAge, year=2014,type="age");
xh &lt;- tmp$age
(fit1 &lt;- fit.FSD(xh, dist='exp'));
(fit2 &lt;- fit.FSD(xh, dist='weibull'));
#(fit3 &lt;- fit.FSD(xh, dist='ewd')); #slow
#(fit0 &lt;- fit.FSD(xh, dist='gpd')); # test, not used
(fit4 &lt;- fit.FSD(xh, dist='gld'));

#plot(xh, xlab="X", main="(a) Fitted Distribtions")
#lines(fit1, lty=1, col=1,lwd=3)
#lines(fit2, lty=2, col=1,lwd=3)
#lines(fit3, lty=3, col=1,lwd=3)
#lines(fit4, lty=4, col=1,lwd=3)
#lines(fit0, lty=4, col=2,lwd=3)

legend("topright",cex=2,
       legend=c("EXP","Weibull","EWD","GLD"),
       lty=c(1:4),lwd=rep(3,4),col=rep(1,4))
dev.off()

## Table 3
#r2 &lt;- c(fit1$x.fit$Dn.Zipf, fit2$x.fit$Dn.Zipf,
#        fit3$x.fit$Dn.Zipf, fit4$x.fit$Dn.Zipf)
#dn &lt;- c(fit1$x.fit$Dn, fit2$x.fit$Dn,
#        fit3$x.fit$Dn, fit4$x.fit$Dn)
#aic &lt;- c(fit1$x.fit$AIC, fit2$x.fit$AIC,
#         fit3$x.fit$AIC, fit4$x.fit$AIC)
#bic &lt;- c(fit1$x.fit$BIC, fit2$x.fit$BIC,
#         fit3$x.fit$BIC, fit4$x.fit$BIC)
#aicc &lt;- c(fit1$x.fit$AICc, fit2$x.fit$AICc,
#          fit3$x.fit$AICc, fit4$x.fit$AICc)
#tbl3 &lt;- data.frame(
#    Dn.Zipf = r2, Dn=dn, AIC=aic, BIC=bic,AICc=aicc)
#rownames(tbl3) &lt;- c("EXP","WD","EWD","GLD")
##save(tbl3, file='tbl3.Rdata')
#tbl3
#require(xtable)
#xtable(tbl3,digits=c(0,4,4,0,0,0))


#postscript(file='fig3b.eps',paper='letter')
#par(mfrow=c(1,1))
#ZipfPlot(fit1, plot.new=TRUE,col=1,lwd=3,lty=1,
#         xlab="log(x)",ylab="log(S(x))",
#         main="(b) Zipf Plots of Fitted Distribtions")
#ZipfPlot(fit2, plot.new=FALSE,col=1,lty=2,lwd=3)
#ZipfPlot(fit3, plot.new=FALSE,col=1,lty=3,lwd=3)
#ZipfPlot(fit4, plot.new=FALSE,col=1,lty=4,lwd=3)

#legend("bottomleft",cex=2,
#       legend=c("EXP","Weibull","EWD","GLD"),
#       lty=c(1:4),lwd=rep(3,4),col=rep(1,4))
#dev.off()

## TABLE 4 ############################################
## More Results **************************************

#mysummary &lt;- function(x,dist){
#    .winner &lt;- function(x,DIST=dist){
#        isele &lt;- which(x==min(x))
#        if(length(isele)&gt;1)
#            warning("multiple winners, only the first is used")
#        DIST[isele[1]]
#    }
#    .mysum &lt;- function(x,y) sum(y==x)
#    mu1 &lt;- tapply(x$Dn.Zipf,x$Dist, mean, na.rm=TRUE)
#    mu2 &lt;- tapply(x$Dn,x$Dist, mean, na.rm=TRUE)
#    sd1 &lt;- tapply(x$Dn.Zipf,x$Dist, sd, na.rm=TRUE)
#    sd2 &lt;- tapply(x$Dn,x$Dist, sd, na.rm=TRUE)
#    win1 &lt;- tapply(x$Dn.Zipf,x$Year, .winner,DIST=dist)
#    win2 &lt;- tapply(x$Dn,x$Year, .winner,DIST=dist)
#    win3 &lt;- tapply(x$BIC,x$Year, .winner,DIST=dist)
#    dist0 &lt;- levels(as.factor(x$Dist))
    
#    out1 &lt;- sapply(dist0, .mysum,y=win1)
#    out2 &lt;- sapply(dist0, .mysum,y=win2)
#    out3 &lt;- sapply(dist0, .mysum,y=win3)
#    #sele1 &lt;- match(names(out1), dist0)
#    #sele2 &lt;- match(names(out2), dist0)
#    #sele3 &lt;- match(names(out3), dist0)
    
#    out &lt;- data.frame(Mean.Dn.Zipf=mu1, SD.Dn.Zipf=sd1,
#                      Mean.Dn=mu2, SD.Dn=sd2,
#                      Win.Zipf=out1,
#                      Win.Dn=out2,
#                      Win.BIC=out3)
#    out
#}

#require(bda) #version 14.3.11+
#data(FSD)

#Dn.Zipf &lt;- NULL
#Dn &lt;- NULL
#BIC &lt;- NULL
#Year &lt;- NULL
#DIST &lt;- NULL; dist0 &lt;- c("EXP","WD","EWD","GLD")
#for(year in 1983:2014){
#    tmp &lt;- ImportFSD(FirmAge, year=year,type="age");
#    xh &lt;- tmp$age
#    fit1 &lt;- fit.FSD(xh, dist='exp');fit1
#    DIST &lt;- c(DIST,"EXP")
#    Year &lt;- c(Year, year)
#    Dn.Zipf &lt;- c(Dn.Zipf, fit1$x.fit$Dn.Zipf)
#    Dn &lt;- c(Dn, fit1$x.fit$Dn)
#    BIC &lt;- c(BIC, fit1$x.fit$BIC)
#    fit1 &lt;- fit.FSD(xh, dist='weibull');fit1
#    DIST &lt;- c(DIST,"WD")
#    Year &lt;- c(Year, year)
#    Dn.Zipf &lt;- c(Dn.Zipf, fit1$x.fit$Dn.Zipf)
#    Dn &lt;- c(Dn, fit1$x.fit$Dn)
#    BIC &lt;- c(BIC, fit1$x.fit$BIC)
#    fit1 &lt;- fit.FSD(xh, dist='ewd');fit1
#    DIST &lt;- c(DIST,"EWD")
#    Year &lt;- c(Year, year)
#    Dn.Zipf &lt;- c(Dn.Zipf, fit1$x.fit$Dn.Zipf)
#    Dn &lt;- c(Dn, fit1$x.fit$Dn)
#    BIC &lt;- c(BIC, fit1$x.fit$BIC)
#    fit1 &lt;- fit.FSD(xh, dist='gld');fit1
#    DIST &lt;- c(DIST,"GLD")
#    Year &lt;- c(Year, year)
#    Dn.Zipf &lt;- c(Dn.Zipf, fit1$x.fit$Dn.Zipf)
#    Dn &lt;- c(Dn, fit1$x.fit$Dn)
#    BIC &lt;- c(BIC, fit1$x.fit$BIC)
#}

#RES &lt;- data.frame(Year=Year,Dist=DIST,Dn.Zipf=Dn.Zipf,Dn=Dn,BIC=BIC)
#save(RES, file='tbl4.Rdata')
#load(file='tbl4.Rdata')

#DIST &lt;- c("EXP","WD","EWD","GLD")
#sele &lt;- RES$Year&gt;=1983 &amp; RES$Year&lt;=1987;sum(sele)
#(out &lt;- mysummary(RES[sele,],dist=DIST))
#xtable(out[c(2,4,1,3),],digits=c(0,4,4,4,4,0,0,0))

#sele &lt;- RES$Year&gt;=1988 &amp; RES$Year&lt;=1992;sum(sele)
#(out &lt;- mysummary(RES[sele,],dist=DIST))
#xtable(out[c(2,4,1,3),],digits=c(0,4,4,4,4,0,0,0))

#sele &lt;- RES$Year&gt;=1993 &amp; RES$Year&lt;=1997;sum(sele)
#(out &lt;- mysummary(RES[sele,],dist=DIST))
#xtable(out[c(2,4,1,3),],digits=c(0,4,4,4,4,0,0,0))

#sele &lt;- RES$Year&gt;=1998 &amp; RES$Year&lt;=2002;sum(sele)
#(out &lt;- mysummary(RES[sele,],dist=DIST))
#xtable(out[c(2,4,1,3),],digits=c(0,4,4,4,4,0,0,0))

#sele &lt;- RES$Year&gt;=2003 &amp; RES$Year&lt;=2014;sum(sele)
#(out &lt;- mysummary(RES[sele,],dist=DIST))
#xtable(out[c(2,4,1,3),],digits=c(0,4,4,4,4,0,0,0))

#(out &lt;- mysummary(RES,dist=DIST))
#xtable(out[c(2,4,1,3),],digits=c(0,4,4,4,4,0,0,0))

## Figure 4a &amp; 4b ******************************************
#rm(list=ls())
#require(bda)
#data(FSD)


tmp &lt;- ImportFSD(FirmSize, year=2014,type="size");
yh &lt;- tmp$size

(fit1 &lt;- fit.FSD(yh, dist='lognormal'));
(fit2 &lt;- fit.FSD(yh, dist='pareto'));
(fit3 &lt;- fit.FSD(yh, dist='gpd'));
(fit4 &lt;- fit.FSD(yh, dist='gld'));

#postscript(file='fig4a.eps',paper='letter')
#par(mfrow=c(1,1))
#plot(yh, xlab="Y", main="(a) Fitted Distributions",
#     xlim=c(0,50))
#lines(fit1, lty=1, col=1,lwd=3)
#lines(fit2, lty=2, col=1,lwd=3)
#lines(fit3, lty=3, col=1,lwd=3)
#lines(fit4, lty=4, col=1,lwd=3)


#legend("topright",cex=2,
#       legend=c("LN","PD","GPD","GLD"),
#       lty=c(1:4),lwd=rep(3,4),col=rep(1,4))
#dev.off()

#postscript(file='fig4b.eps',paper='letter')
#par(mfrow=c(1,1))
#ZipfPlot(fit1, plot.new=TRUE,col=1,lwd=3,lty=1,
#         xlab="log(y)",ylab="log(S(y))",
#         main="(b) Zipf Plots of Fitted Distribtions")
#ZipfPlot(fit2, plot.new=FALSE,col=1,lty=2,lwd=3)
#ZipfPlot(fit3, plot.new=FALSE,col=1,lty=3,lwd=3)
#ZipfPlot(fit4, plot.new=FALSE,col=1,lty=4,lwd=3)

#legend("bottomleft",cex=2,
#       legend=c("LN","PD","GPD","GLD"),
#       lty=c(1:4),lwd=rep(3,4),col=rep(1,4))
#dev.off()

## TABLE 5 ############################################
## More information about firm size


#Dn.Zipf &lt;- NULL
#Dn &lt;- NULL
#BIC &lt;- NULL
#Year &lt;- NULL
#DIST &lt;- NULL;

#dist0 &lt;- c("LN","PD","GPD","GLD")
#for(year in 1977:2014){
#    DIST &lt;- c(DIST,dist0)
#    Year &lt;- c(Year, rep(year,length(dist0)))
#    tmp &lt;- ImportFSD(FirmSize, year=year,type="size");
#    xh &lt;- tmp$size
#    fit1 &lt;- fit.FSD(xh, dist='lognormal');
#    Dn.Zipf &lt;- c(Dn.Zipf, fit1$x.fit$Dn.Zipf)
#    Dn &lt;- c(Dn, fit1$x.fit$Dn)
#    BIC &lt;- c(BIC, fit1$x.fit$BIC)
#    fit1 &lt;- fit.FSD(xh, dist='pd');
#    Dn.Zipf &lt;- c(Dn.Zipf, fit1$x.fit$Dn.Zipf)
#    Dn &lt;- c(Dn, fit1$x.fit$Dn)
#    BIC &lt;- c(BIC, fit1$x.fit$BIC)
#    fit1 &lt;- fit.FSD(xh, dist='gpd');
#    Dn.Zipf &lt;- c(Dn.Zipf, fit1$x.fit$Dn.Zipf)
#    Dn &lt;- c(Dn, fit1$x.fit$Dn)
#    BIC &lt;- c(BIC, fit1$x.fit$BIC)
#    fit1 &lt;- fit.FSD(xh, dist='gld');
#    Dn.Zipf &lt;- c(Dn.Zipf, fit1$x.fit$Dn.Zipf)
#    Dn &lt;- c(Dn, fit1$x.fit$Dn)
#    BIC &lt;- c(BIC, fit1$x.fit$BIC)
#}

#RES &lt;- data.frame(Year=Year,Dist=DIST,Dn.Zipf=Dn.Zipf,Dn=Dn,BIC=BIC)
#save(RES, file='tbl5.Rdata')
#load(file='tbl5.Rdata')

#dist0 &lt;- c("LN","PD","GPD","GLD")
#(out &lt;- mysummary(RES,dist=dist0))
#require(xtable)
#xtable(out[c(3,2,1),],digits=c(0,4,4,4,4,0,0,0))


## Figure 5  ****************************************************

#xy &lt;- tmp &lt;- ImportFSD(Firm2);
#out1 &lt;- fit.FSD(xy$xy, breaks=xy$breaks,dist=c("EWD","GPD"));out1
#out2 &lt;- fit.FSD(xy$xy, breaks=xy$breaks,dist=c("EWD","GLD"));out2
#out3 &lt;- fit.FSD(xy$xy, breaks=xy$breaks,dist=c("GLD","GPD"));out3
#out4 &lt;- fit.FSD(xy$xy, breaks=xy$breaks,dist=c("GLD","GLD"));out4

#postscript(file="fig5.eps",paper='letter')

#par(mfrow=c(2,2))
#out &lt;- out1
#res2 &lt;- plot(out,grid.size=40,nlevels=30,
#             ylim=c(0,15),xlim=c(0,30),
#             xlab="Firm Age",ylab="Firm Size",
#             main="(a) Contour Plot -- (EWD+GPD)")

#out &lt;- out2
#res2 &lt;- plot(out,grid.size=40,nlevels=30,
#             ylim=c(0,15),xlim=c(0,30),
#             xlab="Firm Age",ylab="Firm Size",
#             main="(b) Contour Plot -- (EWD+GLD)")
#out &lt;- out3
#res2 &lt;- plot(out,grid.size=40,nlevels=30,
#             ylim=c(0,15),xlim=c(0,30),
#             xlab="Firm Age",ylab="Firm Size",
#             main="(c) Contour Plot -- (GLD+GPD)")
#
#out &lt;- out4
#res2 &lt;- plot(out,grid.size=40,nlevels=30,
#             ylim=c(0,15),xlim=c(0,30),
#             xlab="Firm Age",ylab="Firm Size",
#             main="(d) Contour Plot -- (GLD+GLD)")
##dev.off()


## Figure 6  ****************************************************
#xy2 &lt;- tmp &lt;- ImportFSD(Firm2);

## this is an example showing how to use partial data to fit FSD.
## get marginal frequency distribution for firm age:
#(X &lt;- apply(xy2$xy,1,sum));
#brks.age &lt;- c(0,1,2,3,4,5,6,11,16,21,26,38,Inf)
## get marginal frequency distribution for firm size:
#(Y &lt;- apply(xy2$xy,2,sum));
#(Y &lt;- Y[-1])
#brks.size &lt;- c(5,10,20,50,100,250,500,1000,2500,5000,10000,Inf)
#mxy2 &lt;- xy2$xy[,-1]

#(fitx1 &lt;- fit.FSD(X, breaks=brks.age, dist="ewd"))
#(fitx2 &lt;- fit.FSD(X, breaks=brks.age, dist="gld"))
#(fity1 &lt;- fit.FSD(Y, breaks=brks.size, dist="gld"))
#(fity2 &lt;- fit.FSD(Y, breaks=brks.size, dist="pd"))
#(fity3 &lt;- fit.FSD(Y, breaks=brks.size, dist="gpd"))

#(out11 &lt;- fit.Copula(fitx1, fity1, mxy2))
#(out12 &lt;- fit.Copula(fitx1, fity2, mxy2))
#(out21 &lt;- fit.Copula(fitx2, fity1, mxy2))
#(out22 &lt;- fit.Copula(fitx2, fity2, mxy2))
#(out13 &lt;- fit.Copula(fitx1, fity3, mxy2))
#(out23 &lt;- fit.Copula(fitx2, fity3, mxy2))

##postscript(file="fig6.eps",paper='letter')

#par(mfrow=c(1,1))
#plot(out11,grid.size=40,nlevels=20,lty=2,
#     ylim=c(4.8,16),xlim=c(0,18),
#     xlab="Firm Age",ylab="Firm Size",
#     main="(d) Contour Plot -- (GLD+GLD)")
#plot(out12,grid.size=40,nlevels=30, col=2, plot.new=FALSE)
## or use the command below
## plot(out2,grid.size=50,nlevels=50, col=4, add=TRUE)
##dev.off()

</code></pre>

<hr>
<h2 id='fit.GBP'>Fitting Mixture Model of Generalized Beta and Pareto</h2><span id='topic+fit.GBP'></span><span id='topic+dGBP'></span><span id='topic+pGBP'></span>

<h3>Description</h3>

<p>To fit a mixture model of generalize beta and Pareto to grouped data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  fit.GBP(x,breaks)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fit.GBP_+3A_x">x</code></td>
<td>
<p>'x' can be a vector or a matrix, or a 'histogram'.</p>
</td></tr>
<tr><td><code id="fit.GBP_+3A_breaks">breaks</code></td>
<td>
<p>a matrix with two columns if 'x' is a matrix. Otherwise, it is a vector. Can be missing if 'x' is a vector and 'x' will be grouped uisng the default parameters with <code>hist</code>. </p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>pars</code></td>
<td>
<p>estimated parameters.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(FSD)
x &lt;- as.numeric(FirmSize[nrow(FirmSize),])
brks.size &lt;- c(0,4.5,9.5,19.5,49.5,99.5,249.5,499.5,
               999.5,2499.5,4999.5, 9999.5,Inf)
xhist1 &lt;- binning(counts=x,breaks=brks.size)

(out &lt;- fit.GBP(x,brks.size))
(out &lt;- fit.GBP(xhist1))

plot(xhist1,xlim=c(0,110))
x0 &lt;- seq(0,110,length=1000)
f0 &lt;- dGBP(x0,out)
lines(f0~x0, col=2,lwd=2)

ZipfPlot(xhist1,plot=TRUE)
F0 &lt;- pGBP(brks.size,out)
lines(log(1-F0)~log(brks.size), col=2)
</code></pre>

<hr>
<h2 id='fit.lognormal'>Fitting log-normal distributions</h2><span id='topic+fit.lnorm'></span><span id='topic+fit.lognormal'></span><span id='topic+mixlognormal'></span><span id='topic+print.mixlognormal'></span><span id='topic+ddeg'></span><span id='topic+NGS.normalize'></span><span id='topic+fit.mlnorm'></span>

<h3>Description</h3>

<p>To fit log-normal distributions to raw data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit.lognormal(x, k=1,normal=FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fit.lognormal_+3A_x">x</code></td>
<td>
<p>Raw data or grouped data</p>
</td></tr>
<tr><td><code id="fit.lognormal_+3A_k">k</code></td>
<td>
<p>number of components, Default: 1</p>
</td></tr>
<tr><td><code id="fit.lognormal_+3A_normal">normal</code></td>
<td>
<p>Fit normal mixture models if 'normal=TRUE'; otherwise
fit log-normal mixture models.</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>p0</code></td>
<td>
<p>The estimated proportion of zeros.</p>
</td></tr>
<tr><td><code>p</code>, <code>mean</code>, <code>sigma</code></td>
<td>
<p>The fitted parameters of mixing coefficients, means and standard deviations of the k normal components.</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>The sample size of data.</p>
</td></tr>
<tr><td><code>npar</code></td>
<td>
<p>Number of parameters to be estimated.</p>
</td></tr>
<tr><td><code>llk</code></td>
<td>
<p>Estimated log-likelihood.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'> mu = -.5
 s = 2
</code></pre>

<hr>
<h2 id='fit.Pareto'>Fit a Pareto Distribution to Binned Data</h2><span id='topic+fit.Pareto'></span>

<h3>Description</h3>

<p>Fit a Pareto distribution to binned data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  fit.Pareto(x, xm, method='mle')
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fit.Pareto_+3A_x">x</code></td>
<td>
<p>grouped data</p>
</td></tr>
<tr><td><code id="fit.Pareto_+3A_xm">xm</code></td>
<td>
<p>The location parameter: lower bound of the
support of the distribution</p>
</td></tr>
<tr><td><code id="fit.Pareto_+3A_method">method</code></td>
<td>
<p>fitting method:
'mle'=maximum likelihood estimate,
'percentile'=percentile matching.</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>xm</code></td>
<td>
<p>fitted  location parameter</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>fitted scale parameter</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>xm &lt;- 0.5
alpha &lt;- 1.0

x &lt;- rPareto(1000, xm, alpha)
(out &lt;- fit.Pareto(x,method='mle'))
(out &lt;- fit.Pareto(x,method='ls'))

xbrks &lt;- c(0,4.5,9.5,19.5,49.5,99.5,249.5,499.5,999.5,
           2499.5,4999.5,9999.5,Inf)
xhist &lt;- binning(x, breaks=xbrks)

(out &lt;- fit.Pareto(xhist))
(out &lt;- fit.Pareto(xhist,method='mle'))
(out &lt;- fit.Pareto(xhist,method='ls'))
(out &lt;- fit.Pareto(xhist,xm=.5,method='mle'))

</code></pre>

<hr>
<h2 id='fit.PRO'>Fitting distributions to Patient-reported Outcome Data</h2><span id='topic+fit.PRO'></span>

<h3>Description</h3>

<p>To fit PRO data to distributions including GLD, PD, GPD, Weibull, EWD
and other families.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit.PRO(x,dist,x.range,nclass)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fit.PRO_+3A_x">x</code></td>
<td>
<p>'x' can be a vector or a matrix, a 'histogram', or binned data.</p>
</td></tr>
<tr><td><code id="fit.PRO_+3A_dist">dist</code></td>
<td>
<p>distribution type for 'x' and/or 'y'. Options include <code>Weibull</code>, <code>gpd</code> &ndash; generalized Pareto distribution, <code>pd</code> or
<code>Pareto</code>, <code>EWD</code>&ndash; exponentiated Weibull distribtion.</p>
</td></tr>
<tr><td><code id="fit.PRO_+3A_x.range">x.range</code></td>
<td>
<p>Specifies the range of data. Used only for interval PRO data.</p>
</td></tr>
<tr><td><code id="fit.PRO_+3A_nclass">nclass</code></td>
<td>
<p>Number of classes/bins. Used only for interval PRO data.</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>x.fit</code>, <code>y.fit</code></td>
<td>
<p>Fitted marginal distribution for row- and column-data when 'x' is a matrix.</p>
</td></tr>
<tr><td><code>Psi</code></td>
<td>
<p>Plackett estimate of the Psi. ONLY for 'fit.FSD2'</p>
</td></tr>
</table>
<p>If each of <code>x.fit</code> and <code>y.fit</code>, the following values are available:
</p>
<table role = "presentation">
<tr><td><code>xhist</code></td>
<td>
<p>histogram.</p>
</td></tr>
<tr><td><code>dist</code></td>
<td>
<p>distribution type. Options include <code>Weibull</code>,
<code>gpd</code> &ndash; generalized Pareto distribution, <code>pd</code> or
<code>Pareto</code>, <code>EWD</code>&ndash; exponentiated Weibull distribtion.</p>
</td></tr>
<tr><td><code>size</code></td>
<td>
<p>Total number of observations (sample size)</p>
</td></tr>
<tr><td><code>pars</code></td>
<td>
<p>Estimates of parameters.</p>
</td></tr>
<tr><td><code>y</code>, <code>y2</code>, <code>x</code></td>
<td>
<p>the pdf (<code>y</code>) and cdf (<code>y2</code>) values evaluated on a grid <code>x</code></p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rweibull(1000,2,1)
(out &lt;- fit.PRO(x))

</code></pre>

<hr>
<h2 id='fnm'>Distribution of Two Finite Gaussian Mixtures</h2><span id='topic+fnm'></span><span id='topic+tkde'></span>

<h3>Description</h3>

 
<p>To compute the values of the density and distribution functions of
two finite Gaussian mixture models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  fnm(p1,p2,mu1,mu2,sig1,sig2,from,to)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fnm_+3A_p1">p1</code>, <code id="fnm_+3A_p2">p2</code></td>
<td>
<p>mixing coefficients.</p>
</td></tr>
<tr><td><code id="fnm_+3A_mu1">mu1</code>, <code id="fnm_+3A_mu2">mu2</code></td>
<td>
<p>vectors of the mean values of the Gaussian components.</p>
</td></tr>
<tr><td><code id="fnm_+3A_sig1">sig1</code>, <code id="fnm_+3A_sig2">sig2</code></td>
<td>
<p>vectors of the SD values of the Gaussian components.</p>
</td></tr>
<tr><td><code id="fnm_+3A_from">from</code>, <code id="fnm_+3A_to">to</code></td>
<td>
<p>to specify the range of data.</p>
</td></tr> 
</table>


<h3>Value</h3>

<p>Return the densities ('y') and probabilities ('Fx')
over a grid of 'x'.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Pain)

group &lt;- pain$treat
x &lt;- pain$recall0
y &lt;- pain$recall1
#out &lt;- tkde(x,y,group)
out &lt;- tkde(x,y,group,type='percent')
plot(out$risk,type='l')
abline(h=1,col='gray')

plot(out$responder,type='l',ylim=c(-.28,.1))
lines(out$resp$ll~out$resp$x,lty=2,col=1+(out$resp$p&lt;0.05))
lines(out$resp$ul~out$resp$x,lty=2,col=1+(out$resp$p&lt;0.05))
abline(h=0,col='gray')

plot(out$g2,type='l')
lines(out$g1,col=2)

</code></pre>

<hr>
<h2 id='FSD'>Firm size data</h2><span id='topic+FSD'></span><span id='topic+Employment2'></span><span id='topic+Firm2'></span><span id='topic+Job2'></span><span id='topic+FirmAge'></span><span id='topic+FirmDeathAge'></span><span id='topic+FirmDeathSize'></span><span id='topic+FirmEmploymentAge'></span><span id='topic+FirmEmploymentSize'></span><span id='topic+FirmJobAge'></span><span id='topic+FirmJobSize'></span><span id='topic+FirmSize'></span>

<h3>Description</h3>

<p>2014 US Private-sector firm size data.
</p>


<h3>References</h3>

<p>https://www.sba.gov/advocacy/firm-size-data, Accessed on 2018-11-16
</p>

<hr>
<h2 id='ImportFSD'>Import Firm Size and Firm Age Data</h2><span id='topic+ImportFSD'></span>

<h3>Description</h3>

<p>To read firm size and/or firm age data from built-in datasets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> ImportFSD(x,type,year)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ImportFSD_+3A_x">x</code></td>
<td>
<p>A built-in firm size and/or firm age dataset.</p>
</td></tr>
<tr><td><code id="ImportFSD_+3A_type">type</code></td>
<td>
<p>type of data: &quot;size&quot; or &quot;age&quot;. If missing, read the age data from rows and the size data from columns.</p>
</td></tr>
<tr><td><code id="ImportFSD_+3A_year">year</code></td>
<td>
<p>The number year the firm size/age data to be read. If missing, assume the year is 2014.</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>xy</code></td>
<td>
<p>a matrix of joint frequency distribution table.</p>
</td></tr>
<tr><td><code>breaks</code></td>
<td>
<p>the class boundaries for firm age as a component <code>age</code>, and for firm size as a component <code>size</code>.</p>
</td></tr>
<tr><td><code>size</code></td>
<td>
<p>a 'bdata' subject of the firm size data</p>
</td></tr>
<tr><td><code>age</code></td>
<td>
<p>a 'bdata' object of the firm age data</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
 data(FSD)
 ## bivariate data
 xy = ImportFSD(Firm2)
 ## firm age of 2013
 x = ImportFSD(FirmAge, type="age", year=2013)
 ## firm size of 2013
 y = ImportFSD(FirmSize, type="size", year=2013)

</code></pre>

<hr>
<h2 id='lps.variance'>compute the variance of the local polynomial regression function</h2><span id='topic+lps.variance'></span>

<h3>Description</h3>

<p>To compute the variance of the local polynomial regression function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  lps.variance(y,x,bw, method="Rice")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lps.variance_+3A_y">y</code>, <code id="lps.variance_+3A_x">x</code></td>
<td>
<p>Two numerical vectors: <code>y</code> is the response and
<code>x</code> is the predictor.</p>
</td></tr>
<tr><td><code id="lps.variance_+3A_bw">bw</code></td>
<td>
<p>Smoothing parameter.  Is used only when <code>method='Wasserman'</code>
or <code>method='heteroscedastic'</code>.</p>
</td></tr>
<tr><td><code id="lps.variance_+3A_method">method</code></td>
<td>

<p>We use four method to compute the variance of r(x):
Method 1) Larry Wasserman&ndash;nearly unbiased.  This method based on
an lps object;
Method 2) Rice 1984
Method 3) Gasser et al (1986) &ndash; a variation of method 3.
Method 4) For heteroscedastic errors. Need to estimate based on an
lpr object. Yu and Jones (2004).
Defaulty method: <code>Rice</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the variance of r(x).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n = 100
x=rnorm(n)
y=x^2+rnorm(n)
bw = lps.variance
par(mfrow=c(1,1))
out=lpsmooth(y,x)
#plot(out, scb=TRUE, type='l')
vrx = lps.variance(y,x)
out=lpsmooth(y,x,sd.y=sqrt(vrx), bw=0.5)
plot(y~x, pch='.')
lines(out, col=2)

x0 = seq(min(x),  max(x), length=100)
y0 = x0^2
lines(y0~x0, col=4)

 </code></pre>

<hr>
<h2 id='lpsmooth'>non-parametric regression</h2><span id='topic+lpsmooth'></span><span id='topic+npr'></span><span id='topic+wlpsmooth'></span><span id='topic+bootsmooth'></span><span id='topic+print.scb'></span>

<h3>Description</h3>

<p>To fit nonparametric regression model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> lpsmooth(y,x, bw, sd.y,lscv=FALSE, adaptive=FALSE,
 	  from, to, gridsize,conf.level=0.95)
 npr(y,x,sd.x,bw,kernel='decon',optimal=FALSE,adaptive=FALSE,
     x0,from, to, gridsize,conf.level=0.95)
 wlpsmooth(y,x,w,s.x,bw,from,to,gridsize,conf.level=0.95)
 bootsmooth(y,x,type="relative",iter=100,conf.level=0.95)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lpsmooth_+3A_y">y</code>, <code id="lpsmooth_+3A_x">x</code></td>
<td>
<p>Two numerical vectors.</p>
</td></tr>
<tr><td><code id="lpsmooth_+3A_w">w</code></td>
<td>
<p>weights</p>
</td></tr>
<tr><td><code id="lpsmooth_+3A_s.x">s.x</code></td>
<td>
<p>standard deviation of the measurement error &ndash; Laplacian
errors are assumed.</p>
</td></tr>
<tr><td><code id="lpsmooth_+3A_x0">x0</code>, <code id="lpsmooth_+3A_from">from</code>, <code id="lpsmooth_+3A_to">to</code>, <code id="lpsmooth_+3A_gridsize">gridsize</code></td>
<td>
<p>'x0' is the grid points where the fitted 
values will be evaluated. If it is missing, define a fine grid using 
the start point (&quot;from&quot;), end point (&quot;to&quot;) and size (&quot;gridsize&quot;).</p>
</td></tr>
<tr><td><code id="lpsmooth_+3A_bw">bw</code></td>
<td>
<p>Smoothing parameter.  Numeric or character value is 
allowed.  If missing, adaptive (LSCV) bandwidth selector will 
be used.</p>
</td></tr>
<tr><td><code id="lpsmooth_+3A_kernel">kernel</code></td>
<td>
<p>kernel type: &quot;normal&quot;,&quot;gauss&quot;,&quot;nw&quot;,&quot;decon&quot; (default),
&quot;lp&quot;,&quot;nadaraya-watson&quot;</p>
</td></tr>
<tr><td><code id="lpsmooth_+3A_lscv">lscv</code>, <code id="lpsmooth_+3A_adaptive">adaptive</code></td>
<td>
<p>If <code>lscv = FALSE</code>, use the given
bandwidth to fit lpr directly.  If <code>lscv = TRUE</code> and
<code>adaptive = FALSE</code>, compute lscv bandwidth and fit lpr.
Initial bandwidth should be given.  If <code>lscv = TRUE</code> and
<code>adaptive = TURE</code>, compute lscv bandwidth, then compute
varying smoothing parameter, then fit lpr.  This algorithm
could be extremeely slow when the sample size is very large.</p>
</td></tr>
<tr><td><code id="lpsmooth_+3A_optimal">optimal</code></td>
<td>
<p>Search for optimal bandwidth if TRUE.</p>
</td></tr> 
<tr><td><code id="lpsmooth_+3A_sd.y">sd.y</code></td>
<td>
<p>Standard deviation of <code>y</code>.</p>
</td></tr>
<tr><td><code id="lpsmooth_+3A_sd.x">sd.x</code></td>
<td>
<p>Standard deviation of the measurement error <code>x</code>.</p>
</td></tr>
<tr><td><code id="lpsmooth_+3A_conf.level">conf.level</code></td>
<td>
<p>Confidence level.</p>
</td></tr>
<tr><td><code id="lpsmooth_+3A_iter">iter</code></td>
<td>
<p>Bootstrapping iteration number.</p>
</td></tr>
<tr><td><code id="lpsmooth_+3A_type">type</code></td>
<td>
<p>&quot;relative&quot; changes or &quot;absolute&quot; changes for effectiveness
evaluation.</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>y</code></td>
<td>
<p>Estimated values of the smooth function over a fine grid.</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>grid points where the smoothed function are evaluated.</p>
</td></tr>
<tr><td><code>x0</code>, <code>y0</code></td>
<td>
<p>cleaned data of x and y.</p>
</td></tr>
<tr><td><code>conf.level</code></td>
<td>
<p>confidence level of the simultaneous confidence bands.</p>
</td></tr>
<tr><td><code>pars</code></td>
<td>
<p>estimate parameters including smoothing bandwidth, and parameters for the tube formula.</p>
</td></tr>
<tr><td><code>ucb</code>, <code>lcb</code></td>
<td>
<p>upper and lower confidence bands.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>function called</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
 x &lt;- rnorm(100,34.5,1.5)
 e &lt;- rnorm(100,0,2)
 y &lt;- (x-32)^2 + e
 out &lt;- lpsmooth(y,x)
 out
 plot(out, type='l')
 x0 &lt;- seq(min(x),max(x),length=100)
 y0 &lt;- (x0-32)^2
 lines(x0, y0, col=2)
 points(x, y, pch="*", col=4)


 </code></pre>

<hr>
<h2 id='mediation.test'>The Sobel mediation test</h2><span id='topic+mediation.test'></span>

<h3>Description</h3>

<p>To compute statistics and p-values for the Sobel test.  Results for three versions of &quot;Sobel test&quot; are provided: Sobel test, Aroian test and Goodman test. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'> mediation.test(mv,iv,dv)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mediation.test_+3A_mv">mv</code></td>
<td>
<p>The mediator variable.</p>
</td></tr>
<tr><td><code id="mediation.test_+3A_iv">iv</code></td>
<td>
<p>The independent variable.</p>
</td></tr>
<tr><td><code id="mediation.test_+3A_dv">dv</code></td>
<td>
<p>The dependent variable.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To test whether a mediator carries the influence on an IV to a DV.
Missing values will be automatically excluded with a warning.
</p>


<h3>Value</h3>

<p>a table showing the values of the test statistics (z-values) and the corresponding p-values for three tests, namely the Sobel test, Aroian test and Goodman test, respectively.
</p>


<h3>Author(s)</h3>

<p>B. Wang  <a href="mailto:bwang@southalabama.edu">bwang@southalabama.edu</a>
</p>


<h3>References</h3>

<p>MacKinnon, D. P., &amp; Dwyer, J. H. (1993). Estimating mediated effects in prevention studies. <em>Evaluation Review</em>, 17, 144-158.
</p>
<p>MacKinnon, D. P., Warsi, G., &amp; Dwyer, J. H. (1995). A simulation study of mediated effect measures. <em>Multivariate Behavioral Research</em>, 30, 41-62.
</p>
<p>Preacher, K. J., &amp; Hayes, A. F. (2004). SPSS and SAS procedures for estimating indirect effects in simple mediation models. <em>Behavior Research Methods,Instruments, &amp; Computers</em>, 36, 717-731.
</p>
<p>Preacher, K. J., &amp; Hayes, A. F. (2008). asymptotic and resampling strategies for assessing and comparing indirect effects in multiple mediator models. <em>Behavior Research Methods, Instruments, &amp; Computers</em>, 40, 879-891.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>mv = rnorm(100)
iv = rnorm(100)
dv = rnorm(100)
mediation.test(mv,iv,dv)
</code></pre>

<hr>
<h2 id='mlnorm'>The mixed lognormal distribution</h2><span id='topic+pmlnorm'></span><span id='topic+dmlnorm'></span><span id='topic+qmlnorm'></span><span id='topic+rmlnorm'></span>

<h3>Description</h3>

 
<p>Density, distribution function, quantile function and random
generation for the lognormal mixture distribution with means equal to
'mu' and standard deviations equal to 's'.</p>


<h3>Usage</h3>

<pre><code class='language-R'>	dmlnorm(x,p,mean,sd)
	pmlnorm(q,p,mean,sd)
	qmlnorm(prob,p,mean,sd)
	rmlnorm(n,p,mean,sd)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mlnorm_+3A_x">x</code>, <code id="mlnorm_+3A_q">q</code></td>
<td>
<p>vector of quantiles in dmixnorm and pmixnorm. 
In qmixnorm, 'x' is a vector of probabilities.</p>
</td></tr>
<tr><td><code id="mlnorm_+3A_p">p</code></td>
<td>
<p>proportions of the mixture components.</p>
</td></tr>
<tr><td><code id="mlnorm_+3A_prob">prob</code></td>
<td>
<p>A vector of probabilities.</p>
</td></tr>
<tr><td><code id="mlnorm_+3A_n">n</code></td>
<td>
<p>number of observations. If 'length(n) &gt; 1', the length is
taken to be the number required.</p>
</td></tr> 
<tr><td><code id="mlnorm_+3A_mean">mean</code></td>
<td>
<p>vector of means</p>
</td></tr>
<tr><td><code id="mlnorm_+3A_sd">sd</code></td>
<td>
<p>vector of standard deviations</p>
</td></tr>
</table>


<h3>Value</h3>

<p>return the density, probability, quantile and random value for the four functions, respectively.</p>


<h3>Examples</h3>

<pre><code class='language-R'>
p &lt;- c(.4,.6)
mu &lt;- c(1,4)
s &lt;- c(2,3)
dmlnorm(c(0,1,2,20),p,mu,s)
pmlnorm(c(0,1,2,20),p,mu,s)
qmlnorm(c(0,1,.2,.20),p,mu,s)
rmlnorm(3,p,mu,s)

</code></pre>

<hr>
<h2 id='mnorm'>The mixed normal distribution</h2><span id='topic+pmnorm'></span><span id='topic+dmnorm'></span><span id='topic+qmnorm'></span><span id='topic+rmnorm'></span>

<h3>Description</h3>

 
<p>Density, distribution function, quantile function and random
generation for the normal mixture distribution with means equal to
'mu' and standard deviations equal to 's'.</p>


<h3>Usage</h3>

<pre><code class='language-R'>	dmnorm(x,p,mean,sd)
	pmnorm(q,p,mean,sd)
	qmnorm(prob,p,mean,sd)
	rmnorm(n,p,mean,sd)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mnorm_+3A_x">x</code>, <code id="mnorm_+3A_q">q</code></td>
<td>
<p>vector of quantiles in dmixnorm and pmixnorm. 
In qmixnorm, 'x' is a vector of probabilities.</p>
</td></tr>
<tr><td><code id="mnorm_+3A_p">p</code></td>
<td>
<p>proportions of the mixture components.</p>
</td></tr>
<tr><td><code id="mnorm_+3A_prob">prob</code></td>
<td>
<p>A vector of probabilities.</p>
</td></tr>
<tr><td><code id="mnorm_+3A_n">n</code></td>
<td>
<p>number of observations. If 'length(n) &gt; 1', the length is
taken to be the number required.</p>
</td></tr> 
<tr><td><code id="mnorm_+3A_mean">mean</code></td>
<td>
<p>vector of means</p>
</td></tr>
<tr><td><code id="mnorm_+3A_sd">sd</code></td>
<td>
<p>vector of standard deviations</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Return the density, probability, quantile and random value, respectively.</p>


<h3>Examples</h3>

<pre><code class='language-R'>
p &lt;- c(.4,.6)
mu &lt;- c(1,4)
s &lt;- c(2,3)
dmnorm(c(0,1,2,20),p,mu,s)
pmnorm(c(0,1,2,20),p,mu,s)
qmnorm(c(0,1,.2,.20),p,mu,s)
rmnorm(3,p,mu,s)

</code></pre>

<hr>
<h2 id='ofc'>occipitofrontal head circumference data</h2><span id='topic+ofc'></span>

<h3>Description</h3>

<p>OFC data for singleton live births with gestational age at least 38 weeks.
</p>


<h3>Format</h3>

<p>A data frame with 2019 observations on 4 variables.
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>Year</code>  </td><td style="text-align: left;"> numeric  </td><td style="text-align: left;"> 2006 -- 2009 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>Sex</code>   </td><td style="text-align: left;"> character  </td><td style="text-align: left;"> 'male' or 'female' </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>Gestation</code>  </td><td style="text-align: left;"> numeric  </td><td style="text-align: left;"> Gestational age (in weeks). </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>Head</code>   </td><td style="text-align: left;"> numeric  </td><td style="text-align: left;"> head size. </td>
</tr>
<tr>
 <td style="text-align: left;">
  </td>
</tr>

</table>



<h3>References</h3>

<p>Wang, B and Wertelecki, W, (2013)
Computational Statistics and Data Analysis,
65: 4-12.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> data(ofc)
 head(ofc)
</code></pre>

<hr>
<h2 id='Pain'>Pain data</h2><span id='topic+Pain'></span><span id='topic+pain'></span>

<h3>Description</h3>

<p>Pain data using VAS.
</p>


<h3>Format</h3>

<p>A data frame with 203 records on 17 variables.
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>group</code>  </td><td style="text-align: left;"> character  </td><td style="text-align: left;"> 'control' or 'treatment' group </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>t01-t07</code>   </td><td style="text-align: left;"> numeric  </td><td style="text-align: left;"> VAS measures at T0 from diary </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>t11-t17</code>   </td><td style="text-align: left;"> numeric  </td><td style="text-align: left;"> VAS measures at T1 from diary </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>rvas0</code>   </td><td style="text-align: left;"> numeric  </td><td style="text-align: left;"> recalled VAS average at T0 </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>rvas1</code>   </td><td style="text-align: left;"> numeric  </td><td style="text-align: left;"> recalled VAS average at T1
  </td>
</tr>

</table>



<h3>References</h3>

<p>To be updated
</p>

<hr>
<h2 id='Pareto'>The Pareto distribution</h2><span id='topic+pPareto'></span><span id='topic+dPareto'></span><span id='topic+qPareto'></span><span id='topic+rPareto'></span><span id='topic+pmixPU'></span><span id='topic+qmixPU'></span>

<h3>Description</h3>

 
<p>Density, distribution function, quantile function and random
generation for the Pareto distribution.</p>


<h3>Usage</h3>

<pre><code class='language-R'>	dPareto(x,xm,alpha)
	pPareto(q,xm,alpha)
	qPareto(p,xm,alpha)
	rPareto(n,xm,alpha)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Pareto_+3A_x">x</code>, <code id="Pareto_+3A_q">q</code></td>
<td>
<p>vector of quantiles in dmixnorm and pmixnorm. 
In qmixnorm, 'x' is a vector of probabilities.</p>
</td></tr>
<tr><td><code id="Pareto_+3A_p">p</code></td>
<td>
<p>A vector of probabilities.</p>
</td></tr>
<tr><td><code id="Pareto_+3A_n">n</code></td>
<td>
<p>number of observations. If 'length(n) &gt; 1', the length is
taken to be the number required.</p>
</td></tr> 
<tr><td><code id="Pareto_+3A_xm">xm</code>, <code id="Pareto_+3A_alpha">alpha</code></td>
<td>
<p>parameters of the Pareto distribution.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>NONE</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 xm = 0.1
 alpha = 1
 dPareto(.5, xm,alpha)

</code></pre>

<hr>
<h2 id='pro.test'>
Test effectiveness based on PROs
</h2><span id='topic+pro.test'></span>

<h3>Description</h3>

<p>Tests for effectiveness evaluations based on PROs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> pro.test(x,y,group,cutoff,x.range,type)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pro.test_+3A_x">x</code>, <code id="pro.test_+3A_y">y</code></td>
<td>

<p>vector of PROs at T0 and T1.
</p>
</td></tr>
<tr><td><code id="pro.test_+3A_group">group</code></td>
<td>

<p>Group assignment: control or treatment.
</p>
</td></tr>
<tr><td><code id="pro.test_+3A_cutoff">cutoff</code></td>
<td>

<p>Class boundaries to define states. Works only when 'x' and 'y' are
numeric.
</p>
</td></tr>
<tr><td><code id="pro.test_+3A_x.range">x.range</code></td>
<td>

<p>Range of the scores for 'x' and 'y'.
</p>
</td></tr>
<tr><td><code id="pro.test_+3A_type">type</code></td>
<td>

<p>Data (grouping/binning) type: 'vas', 'nrs', 'wbf'.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To be added.
</p>


<h3>Value</h3>

<p>To be added.
</p>


<h3>References</h3>

<p>To be added.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>states &lt;- c("low", "moderate", "high")
x0 &lt;- sample(states, size=100, replace=TRUE)
x1 &lt;- sample(states, size=100, replace=TRUE)
grp &lt;- c(rep("control",50),rep("treatment",50))
pro.test(x=x0,y=x1,group=grp)
</code></pre>

<hr>
<h2 id='VAS'>Algorithms for Visual Analogue Scales</h2><span id='topic+VAS.ecdf'></span><span id='topic+VAS.pdf'></span><span id='topic+VAS.npr'></span><span id='topic+wdekde'></span><span id='topic+print.VAS'></span><span id='topic+plot.VAS'></span><span id='topic+lines.VAS'></span>

<h3>Description</h3>

<p>Algorithms for VAS. The algorithms are applicable to other numerical variables with measurement errors as well.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> VAS.ecdf(x,w,alpha=0.05)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="VAS_+3A_x">x</code></td>
<td>
<p>Raw data</p>
</td></tr>
<tr><td><code id="VAS_+3A_w">w</code></td>
<td>
<p>weights</p>
</td></tr>
<tr><td><code id="VAS_+3A_alpha">alpha</code></td>
<td>
<p>Significance level for confidence bands.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Estimate of the emprical distribution function.
</p>
<table role = "presentation">
<tr><td><code>x</code></td>
<td>
<p>grid points</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>ECDF value Fn(x)</p>
</td></tr>
<tr><td><code>lb</code>, <code>ub</code></td>
<td>
<p>lower and upper confidence bands of ECDF.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>significance level</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>raw data</p>
</td></tr>
<tr><td><code>ecdf</code></td>
<td>
<p>Draw ECDF if TRUE.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'> x &lt;- rnorm(100, -2.6, 3.1)
</code></pre>

<hr>
<h2 id='wkde'>
Compute a Binned Kernel Density Estimate for Weighted Data
</h2><span id='topic+wkde'></span><span id='topic+bw.blscv'></span><span id='topic+bw.wnrd'></span><span id='topic+bw.wnrd0'></span>

<h3>Description</h3>

<p>Returns x and y coordinates of the binned
kernel density estimate of the probability
density of the weighted data.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'> wkde(x, w, bandwidth, freq=FALSE, gridsize = 401L, range.x, 
 	 truncate = TRUE, na.rm = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="wkde_+3A_x">x</code></td>
<td>

<p>vector of observations from the distribution whose density is to
be estimated.  Missing values are not allowed.
</p>
</td></tr>
<tr><td><code id="wkde_+3A_w">w</code></td>
<td>

<p>The weights of <code>x</code>.  The weight <code>w_i</code> of any 
observation <code>x_i</code> should
be non-negative.  If <code>x_i=0</code>, <code>x_i</code> will be removed
from the analysis.
</p>
</td></tr>
<tr><td><code id="wkde_+3A_bandwidth">bandwidth</code></td>
<td>

<p>the kernel bandwidth smoothing parameter.  Larger values of
<code>bandwidth</code> make smoother estimates, smaller values of
<code>bandwidth</code> make less smooth estimates.  Automatic bandwidth 
selectors are developed.  Options include <code>wnrd</code>, <code>wnrd0</code>,
<code>wmise</code>,<code>blscv</code>, and <code>awmise</code>.
</p>
</td></tr>
<tr><td><code id="wkde_+3A_freq">freq</code></td>
<td>

<p>An indicator showing whether <code>w</code> is a vector of frequecies
(counts) or weights.
</p>
</td></tr>
<tr><td><code id="wkde_+3A_gridsize">gridsize</code></td>
<td>

<p>the number of equally spaced points at which to estimate
the density.
</p>
</td></tr>
<tr><td><code id="wkde_+3A_range.x">range.x</code></td>
<td>

<p>vector containing the minimum and maximum values of <code>x</code>
at which to compute the estimate.
The default is the minimum and maximum data values, extended by the
support of the kernel.
</p>
</td></tr>
<tr><td><code id="wkde_+3A_truncate">truncate</code></td>
<td>

<p>logical flag: if <code>TRUE</code>, data with <code>x</code> values outside the
range specified by <code>range.x</code> are ignored.
</p>
</td></tr>
<tr><td><code id="wkde_+3A_na.rm">na.rm</code></td>
<td>

<p>logical flag: if <code>TRUE</code>, <code>NA</code> values will be ignored; 
otherwise, the program will be halted with error information.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The default bandwidth, <code>"wnrd0"</code>, is computed using a rule-of-thumb for 
choosing the bandwidth of a Gaussian kernel density estimator 
based on weighted data.  It defaults to 0.9 times the
minimum of the standard deviation and the interquartile range
divided by 1.34 times the sample size to the negative one-fifth
power (= Silverman's ‘rule of thumb’, Silverman (1986, page 48,
eqn (3.31)) _unless_ the quartiles coincide when a positive result
will be guaranteed.
</p>
<p><code>"wnrd"</code> is the more common variation given by Scott (1992), using
factor 1.06.
</p>
<p><code>"wmise"</code> is a completely automatic optimal bandwidth selector
using the least-squares cross-validation (LSCV) method by minimizing the 
integrated squared errors (ISE). 
</p>


<h3>Value</h3>

<p>a list containing the following components:
</p>
<table role = "presentation">
<tr><td><code>x</code></td>
<td>

<p>vector of sorted <code>x</code> values at which the estimate was computed.
</p>
</td></tr>
<tr><td><code>y</code></td>
<td>

<p>vector of density estimates
at the corresponding <code>x</code>.
</p>
</td></tr>
<tr><td><code>bw</code></td>
<td>

<p>optimal bandwidth.
</p>
</td></tr>
<tr><td><code>sp</code></td>
<td>

<p>sensitivity parameter, none <code>NA</code> if adaptive 
bandwidth selector is used.
</p>
</td></tr>
</table>


<h3>References</h3>

<p>Wand, M. P. and Jones, M. C. (1995).
<em>Kernel Smoothing.</em>
Chapman and Hall, London.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 mu = 34.5; s=1.5; n = 3000
 x = round(rnorm(n, mu, s),1)
 x0 = seq(min(x)-s,max(x)+s, length=100)
 f0 = dnorm(x0,mu, s)

 xt = table(x); n = length(x)
 x1 = as.numeric(names(xt))
 w1 = as.numeric(xt)
 (h1 &lt;- bw.wnrd0(x1, w1))
 (h2 &lt;- bw.wnrd0(x1,w1,n=n))
 
 est1 &lt;- wkde(x1,w1, bandwidth=h1)
 est2 &lt;- wkde(x1,w1, bandwidth=h2)
 est3 &lt;- wkde(x1,w1, bandwidth='awmise')
 est4 &lt;- wkde(x1,w1, bandwidth='wmise')
 est5 &lt;- wkde(x1,w1, bandwidth='blscv')

 est0 = density(x1,bw="SJ",weights=w1/sum(w1)); 

 plot(f0~x0, xlim=c(min(x),max(x)), ylim=c(0,.30), type="l")
 lines(est0, col=2, lty=2, lwd=2)

 lines(est1, col=2)
 lines(est2, col=3)
 lines(est3, col=4)
 lines(est4, col=5)
 lines(est5, col=6)
 legend(max(x),.3,xjust=1,yjust=1,cex=.8,
  legend=c("N(34.5,1.5)", "SJ", "wnrd0",
  "wnrd0(n)","awmise","wmise","blscv"),
  col = c(1,2,2,3,4,5,6), lty=c(1,2,1,1,1,1,1),
  lwd=c(1,2,1,1,1,1,1))

</code></pre>

<hr>
<h2 id='Zipf.Normalize'>Zipf Normalization</h2><span id='topic+Zipf.Normalize'></span>

<h3>Description</h3>

 
<p>Zipf plot based normalization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  Zipf.Normalize(x, y, cutoff=6,optim=FALSE, method)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Zipf.Normalize_+3A_x">x</code>, <code id="Zipf.Normalize_+3A_y">y</code></td>
<td>
<p>data: two vectors.</p>
</td></tr>
<tr><td><code id="Zipf.Normalize_+3A_cutoff">cutoff</code></td>
<td>
<p>a large enought value such that the values larger
than the <code>cutoff</code> (approximately) follows a power law distribution.</p>
</td></tr>
<tr><td><code id="Zipf.Normalize_+3A_optim">optim</code></td>
<td>
<p>Find the optimal normalization parameters if TRUE</p>
</td></tr>
<tr><td><code id="Zipf.Normalize_+3A_method">method</code></td>
<td>
<p>use both power transformation and scalingby default.
If 'scaling' is specified, skip power transformation. </p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>x</code></td>
<td>
<p>reference profile (not normalized)</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>normalized profile</p>
</td></tr>
<tr><td><code>scaler</code></td>
<td>
<p>Linear rescaling normalization parameter estimate</p>
</td></tr>
<tr><td><code>power</code></td>
<td>
<p>power transformation parameter estimate</p>
</td></tr>
<tr><td><code>scaler.optim</code></td>
<td>
<p>Optimized estimate of the linear rescaling parameter</p>
</td></tr>
<tr><td><code>power.optim</code></td>
<td>
<p>Optimzed estimate of the power transformation parameter.</p>
</td></tr>
<tr><td><code>mat.optim</code></td>
<td>
<p>A matrix of the objective function values generated to find the optimal estimates.</p>
</td></tr>
<tr><td><code>coef</code></td>
<td>
<p>Coefficient table to display the estimates.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Wang, B. (2020)
A Zipf-plot based normalization method for high-throughput RNA-Seq data.
PLoS ONE, (in press).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(LCL)
names(LCL)
x &lt;- LCL$p47
y &lt;- LCL$p107
outx &lt;- ZipfPlot(x)
plot(outx,type='l')
outy &lt;- ZipfPlot(y)
lines(outy,col=2)

out2 &lt;- Zipf.Normalize(x,y)
outy2 &lt;- ZipfPlot(out2$y)
lines(outy2,col=4)

  </code></pre>

<hr>
<h2 id='ZipfPlot'>Draw Zipf Plot</h2><span id='topic+ZipfPlot'></span><span id='topic+ZipfPlot.default'></span><span id='topic+ZipfPlot.FSD'></span><span id='topic+ZipfPlot.bdata'></span><span id='topic+ZipfPlot.histogram'></span>

<h3>Description</h3>

 
<p>Draw Zipf Plot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  ZipfPlot(x, x0, plot=FALSE,plot.new=TRUE, weights,...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ZipfPlot_+3A_x">x</code></td>
<td>
<p>data: two vectors.</p>
</td></tr>
<tr><td><code id="ZipfPlot_+3A_x0">x0</code></td>
<td>
<p>low bound to filter data.</p>
</td></tr>
<tr><td><code id="ZipfPlot_+3A_plot">plot</code></td>
<td>
<p>Draw Zipf plot if <code>TRUE</code></p>
</td></tr>
<tr><td><code id="ZipfPlot_+3A_plot.new">plot.new</code></td>
<td>
<p>whether draw a new plot.</p>
</td></tr>
<tr><td><code id="ZipfPlot_+3A_weights">weights</code></td>
<td>
<p>Compute weighted least squares line
if <code>weights</code> is given.</p>
</td></tr>
<tr><td><code id="ZipfPlot_+3A_...">...</code></td>
<td>
<p>plotting parameters.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None</p>


<h3>References</h3>

<p>Wang, B. (2020)
A Zipf-plot based normalization method for high-throughput RNA-Seq data.
PLoS ONE, (in press).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(LCL)
names(LCL)
x &lt;- LCL$p47
y &lt;- LCL$p107
outx &lt;- ZipfPlot(x)
plot(outx,type='l')
outy &lt;- ZipfPlot(y)
lines(outy,col=2)

out2 &lt;- Zipf.Normalize(x,y)
outy2 &lt;- ZipfPlot(out2$y)
lines(outy2,col=4)

  </code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
