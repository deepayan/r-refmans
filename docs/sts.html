<!DOCTYPE html><html lang="en"><head><title>Help for package sts</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {sts}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#sts-package'><p>A Structural Topic and Sentiment-Discourse Model for Text Analysis</p></a></li>
<li><a href='#esthcpp'><p>Estimate Hessian Matrix</p></a></li>
<li><a href='#estimateRegns'><p>Regression Table Estimation</p></a></li>
<li><a href='#findRepresentativeDocs'><p>Function for Identifying Documents that Load Heavily on a Topic</p></a></li>
<li><a href='#heldoutLikelihood'><p>Compute Heldout Log-Likelihood</p></a></li>
<li><a href='#lgaecpp'><p>Estimate Gradient</p></a></li>
<li><a href='#lpbdcpp'><p>Estimate approximate ELBO</p></a></li>
<li><a href='#plot.STS'><p>Function for plotting STS objects</p></a></li>
<li><a href='#plotRepresentativeDocs'><p>Plot Documents that Load Heavily on a Topic</p></a></li>
<li><a href='#printRegnTables'><p>Print Estimated Regression Tables</p></a></li>
<li><a href='#printTopWords'><p>Print Top Words that Load Heavily on each Topic</p></a></li>
<li><a href='#sts'><p>Variational EM for the Structural Topic and Sentiment-Discourse (STS) Model</p></a></li>
<li><a href='#summary.STS'><p>Summary Function for the STS objects</p></a></li>
<li><a href='#topicExclusivity'><p>Compute Exclusivity</p></a></li>
<li><a href='#topicSemanticCoherence'><p>Compute Semantic Coherence</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Estimation of the Structural Topic and Sentiment-Discourse Model
for Text Analysis</td>
</tr>
<tr>
<td>Version:</td>
<td>1.4</td>
</tr>
<tr>
<td>Date:</td>
<td>2025-01-25</td>
</tr>
<tr>
<td>Author:</td>
<td>Shawn Mankad [aut, cre],
  Li Chen [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Shawn Mankad &lt;smankad@ncsu.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>The Structural Topic and Sentiment-Discourse (STS) model allows researchers to estimate topic models with document-level metadata that determines both topic prevalence and sentiment-discourse. The sentiment-discourse is modeled as a document-level latent variable for each topic that modulates the word frequency within a topic. These latent topic sentiment-discourse variables are controlled by the document-level metadata. The STS model can be useful for regression analysis with text data in addition to topic modeling’s traditional use of descriptive analysis. The method was developed in Chen and Mankad (2024) &lt;<a href="https://doi.org/10.1287%2Fmnsc.2022.00261">doi:10.1287/mnsc.2022.00261</a>&gt;. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Imports:</td>
<td>glmnet, matrixStats, slam, foreach, doParallel, parallel, stm,
Matrix, mvtnorm, ggplot2, tm</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-01-25 22:25:01 UTC; shawnmankad</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-01-25 23:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='sts-package'>A Structural Topic and Sentiment-Discourse Model for Text Analysis</h2><span id='topic+sts-package'></span>

<h3>Description</h3>

<p>This package implements the Structural Topic and Sentiment-Discourse
(STS) model, which allows researchers to estimate topic models with
document-level metadata that determines both topic prevalence and
sentiment-discourse. The sentiment-discourse is modeled as a document-level
latent variable for each topic that modulates the word frequency within a
topic. These latent topic sentiment-discourse variables are controlled by
the document-level metadata. The STS model can be useful for regression
analysis with text data in addition to topic modeling's traditional
use of descriptive analysis.
</p>


<h3>Details</h3>

<p>Function to fit the model: <code><a href="#topic+sts">sts</a></code>
</p>
<p>Functions for Post-Estimation: <code><a href="#topic+estimateRegns">estimateRegns</a></code>
<code><a href="#topic+topicExclusivity">topicExclusivity</a></code> <code><a href="#topic+topicSemanticCoherence">topicSemanticCoherence</a></code>
<code><a href="#topic+heldoutLikelihood">heldoutLikelihood</a></code> <code><a href="#topic+plotRepresentativeDocs">plotRepresentativeDocs</a></code>
<code><a href="#topic+findRepresentativeDocs">findRepresentativeDocs</a></code> <code><a href="#topic+printTopWords">printTopWords</a></code>
<code><a href="#topic+plot.STS">plot.STS</a></code>
</p>


<h3>Author(s)</h3>

<p>Author: Shawn Mankad and Li Chen
</p>
<p>Maintainer: Shawn Mankad <a href="mailto:smankad@ncsu.edu">smankad@ncsu.edu</a>
</p>


<h3>References</h3>

<p>Chen L. and Mankad, S. (2024) &quot;A Structural Topic and
Sentiment-Discourse Model for Text Analysis&quot; Management Science.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sts">sts</a></code>
</p>

<hr>
<h2 id='esthcpp'>Estimate Hessian Matrix</h2><span id='topic+esthcpp'></span>

<h3>Description</h3>

<p>Estimates the Hessian matrix needed for the Variational E-step in C++
</p>


<h3>Usage</h3>

<pre><code class='language-R'>esthcpp(alpha_d, kappa_t, kappa_s, Sigma_Inv, doc, V, mv)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="esthcpp_+3A_alpha_d">alpha_d</code></td>
<td>
<p>the estimated alpha variables for the given document</p>
</td></tr>
<tr><td><code id="esthcpp_+3A_kappa_t">kappa_t</code></td>
<td>
<p>the estimated kappa_t coefficients</p>
</td></tr>
<tr><td><code id="esthcpp_+3A_kappa_s">kappa_s</code></td>
<td>
<p>the estimated kappa_s coefficients</p>
</td></tr>
<tr><td><code id="esthcpp_+3A_sigma_inv">Sigma_Inv</code></td>
<td>
<p>the inverse covariance matrix</p>
</td></tr>
<tr><td><code id="esthcpp_+3A_doc">doc</code></td>
<td>
<p>the sparse matrix representation of the document, with two rows, and columns equal to the number of unique
vocabulary words in the document.</p>
</td></tr>
<tr><td><code id="esthcpp_+3A_v">V</code></td>
<td>
<p>the size of the vocabulary</p>
</td></tr>
<tr><td><code id="esthcpp_+3A_mv">mv</code></td>
<td>
<p>the baseline log-transformed occurrence rate of each word in the corpus</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The Hessian matrix
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("tm"); library("stm"); library("sts")
temp&lt;-textProcessor(documents=gadarian$open.ended.response,
metadata=gadarian, verbose = FALSE)
out &lt;- prepDocuments(temp$documents, temp$vocab, temp$meta, verbose = FALSE)
out$meta$noTreatment &lt;- ifelse(out$meta$treatment == 1, -1, 1)
## low max iteration number just for testing
sts_estimate &lt;- sts(~ treatment*pid_rep, ~ noTreatment, out, K = 3, maxIter = 2)
# for document #1: 
hessian &lt;- esthcpp(alpha_d = sts_estimate$alpha[1,], kappa_t=sts_estimate$kappa$kappa_t, 
kappa_s=sts_estimate$kappa$kappa_s, Sigma_Inv = sts_estimate$sigma_inv, 
doc = out$documents[[1]], V=length(sts_estimate$vocab), mv = sts_estimate$mv)

</code></pre>

<hr>
<h2 id='estimateRegns'>Regression Table Estimation</h2><span id='topic+estimateRegns'></span>

<h3>Description</h3>

<p>Estimates regression tables for prevalence and sentiment/discourse.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimateRegns(object, prevalence_sentiment, corpus)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="estimateRegns_+3A_object">object</code></td>
<td>
<p>an sts object</p>
</td></tr>
<tr><td><code id="estimateRegns_+3A_prevalence_sentiment">prevalence_sentiment</code></td>
<td>
<p>A formula object with no response variable or a
design matrix with the covariates. If a formula, the variables must be
contained in corpus$meta.</p>
</td></tr>
<tr><td><code id="estimateRegns_+3A_corpus">corpus</code></td>
<td>
<p>The document term matrix to be modeled in a sparse term count matrix with one row
per document and one column per term. The object must be a list of with each element
corresponding to a document. Each document is represented
as an integer matrix with two rows, and columns equal to the number of unique
vocabulary words in the document.  The first row contains the 1-indexed
vocabulary entry and the second row contains the number of times that term
appears. This is the same format in the <code><a href="stm.html#topic+stm">stm</a></code> package.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Estimate Gamma coefficients (along with standard errors, p-values, etc.) to
assess how document-level meta-data determine prevalence and sentiment/discourse
</p>


<h3>Value</h3>

<p>a list of tables with regression coefficient estimates. The first
num-topic elements pertain to prevalence; the latter  num-topic elements
pertain to sentiment-discourse.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("tm"); library("stm"); library("sts")
temp&lt;-textProcessor(documents=gadarian$open.ended.response,
metadata=gadarian, verbose = FALSE)
out &lt;- prepDocuments(temp$documents, temp$vocab, temp$meta, verbose = FALSE)
out$meta$noTreatment &lt;- ifelse(out$meta$treatment == 1, -1, 1)
## low max iteration number just for testing
sts_estimate &lt;- sts(~ treatment*pid_rep, ~ noTreatment, out, K = 3, maxIter = 2)
regns &lt;- estimateRegns(sts_estimate, ~treatment*pid_rep, out)
printRegnTables(x = regns)

</code></pre>

<hr>
<h2 id='findRepresentativeDocs'>Function for Identifying Documents that Load Heavily on a Topic</h2><span id='topic+findRepresentativeDocs'></span>

<h3>Description</h3>

<p>Extracts documents with the highest prevalence for a given topic
</p>


<h3>Usage</h3>

<pre><code class='language-R'>findRepresentativeDocs(object, corpus_text, topic, n = 3)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="findRepresentativeDocs_+3A_object">object</code></td>
<td>
<p>Model output from sts</p>
</td></tr>
<tr><td><code id="findRepresentativeDocs_+3A_corpus_text">corpus_text</code></td>
<td>
<p>vector of text documents, usually contained in the output of prepDocuments</p>
</td></tr>
<tr><td><code id="findRepresentativeDocs_+3A_topic">topic</code></td>
<td>
<p>a single topic number</p>
</td></tr>
<tr><td><code id="findRepresentativeDocs_+3A_n">n</code></td>
<td>
<p>number of documents to extract</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
#Examples with the Gadarian Data
library("tm"); library("stm"); library("sts")
temp&lt;-textProcessor(documents=gadarian$open.ended.response,
metadata=gadarian, verbose = FALSE)
out &lt;- prepDocuments(temp$documents, temp$vocab, temp$meta, verbose = FALSE)
out$meta$noTreatment &lt;- ifelse(out$meta$treatment == 1, -1, 1)
## low max iteration number just for testing
sts_estimate &lt;- sts(~ treatment*pid_rep, ~ noTreatment, out, K = 3, maxIter = 2)
docs &lt;- findRepresentativeDocs(sts_estimate, out$meta$open.ended.response, topic = 3, n = 4)
plotRepresentativeDocs(docs, text.cex = 0.7, width = 100)

</code></pre>

<hr>
<h2 id='heldoutLikelihood'>Compute Heldout Log-Likelihood</h2><span id='topic+heldoutLikelihood'></span>

<h3>Description</h3>

<p>Compute the heldout log-likelihood of the STS model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>heldoutLikelihood(object, missing)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="heldoutLikelihood_+3A_object">object</code></td>
<td>
<p>an sts object, typically after applying <code><a href="stm.html#topic+make.heldout">make.heldout</a></code></p>
</td></tr>
<tr><td><code id="heldoutLikelihood_+3A_missing">missing</code></td>
<td>
<p>list of which words and documents are in the heldout set</p>
</td></tr>
</table>


<h3>Value</h3>

<p>expected.heldout is the average of the held-out log-likelihood values
for each document.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("tm"); library("stm"); library("sts")
temp&lt;-textProcessor(documents=gadarian$open.ended.response,
metadata=gadarian, verbose = FALSE)
out &lt;- prepDocuments(temp$documents, temp$vocab, temp$meta, verbose = FALSE)
out$meta$noTreatment &lt;- ifelse(out$meta$treatment == 1, -1, 1)
out_ho &lt;- make.heldout(out$documents, out$vocab)
out_ho$meta &lt;- out$meta
## low max iteration number just for testing
sts_estimate &lt;- sts(~ treatment*pid_rep, ~ noTreatment, out_ho, K = 3, maxIter = 2, verbose = FALSE)
heldoutLikelihood(sts_estimate, out_ho$missing)$expected.heldout

</code></pre>

<hr>
<h2 id='lgaecpp'>Estimate Gradient</h2><span id='topic+lgaecpp'></span>

<h3>Description</h3>

<p>Estimates the Gradient needed for the Variational E-step in C++
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lgaecpp(alpha_d, Sigma_Inv, kappa_t, kappa_s, mu_d, doc, V, mv)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lgaecpp_+3A_alpha_d">alpha_d</code></td>
<td>
<p>the estimated alpha variables for the given document</p>
</td></tr>
<tr><td><code id="lgaecpp_+3A_sigma_inv">Sigma_Inv</code></td>
<td>
<p>the inverse covariance matrix</p>
</td></tr>
<tr><td><code id="lgaecpp_+3A_kappa_t">kappa_t</code></td>
<td>
<p>the estimated kappa_t coefficients</p>
</td></tr>
<tr><td><code id="lgaecpp_+3A_kappa_s">kappa_s</code></td>
<td>
<p>the estimated kappa_s coefficients</p>
</td></tr>
<tr><td><code id="lgaecpp_+3A_mu_d">mu_d</code></td>
<td>
<p>the fitted values based on document-level variables * estimated Gamma for each document.</p>
</td></tr>
<tr><td><code id="lgaecpp_+3A_doc">doc</code></td>
<td>
<p>the sparse matrix representation of the document, with two rows, and columns equal to the number of unique
vocabulary words in the document.</p>
</td></tr>
<tr><td><code id="lgaecpp_+3A_v">V</code></td>
<td>
<p>the size of the vocabulary</p>
</td></tr>
<tr><td><code id="lgaecpp_+3A_mv">mv</code></td>
<td>
<p>the baseline log-transformed occurrence rate of each word in the corpus</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The Hessian matrix
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("tm"); library("stm"); library("sts")
temp&lt;-textProcessor(documents=gadarian$open.ended.response,
metadata=gadarian, verbose = FALSE)
out &lt;- prepDocuments(temp$documents, temp$vocab, temp$meta, verbose = FALSE)
out$meta$noTreatment &lt;- ifelse(out$meta$treatment == 1, -1, 1)
## low max iteration number just for testing
sts_estimate &lt;- sts(~ treatment*pid_rep, ~ noTreatment, out, K = 3, maxIter = 2)
# for document #1: 
gradient &lt;- lgaecpp(alpha_d = sts_estimate$alpha[1,], kappa_t=sts_estimate$kappa$kappa_t, 
kappa_s=sts_estimate$kappa$kappa_s, Sigma_Inv = sts_estimate$sigma_inv, doc = out$documents[[1]], 
V=length(sts_estimate$vocab), mv = sts_estimate$mv, mu_d = sts_estimate$mu[,1])

</code></pre>

<hr>
<h2 id='lpbdcpp'>Estimate approximate ELBO</h2><span id='topic+lpbdcpp'></span>

<h3>Description</h3>

<p>Estimates the approximate ELBO needed for the Variational E-step in C++
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lpbdcpp(alpha_d, Sigma_Inv, kappa_t, kappa_s, mu_d, doc, V, mv)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lpbdcpp_+3A_alpha_d">alpha_d</code></td>
<td>
<p>the estimated alpha variables for the given document</p>
</td></tr>
<tr><td><code id="lpbdcpp_+3A_sigma_inv">Sigma_Inv</code></td>
<td>
<p>the inverse covariance matrix</p>
</td></tr>
<tr><td><code id="lpbdcpp_+3A_kappa_t">kappa_t</code></td>
<td>
<p>the estimated kappa_t coefficients</p>
</td></tr>
<tr><td><code id="lpbdcpp_+3A_kappa_s">kappa_s</code></td>
<td>
<p>the estimated kappa_s coefficients</p>
</td></tr>
<tr><td><code id="lpbdcpp_+3A_mu_d">mu_d</code></td>
<td>
<p>the fitted values based on document-level variables * estimated Gamma for each document.</p>
</td></tr>
<tr><td><code id="lpbdcpp_+3A_doc">doc</code></td>
<td>
<p>the sparse matrix representation of the document, with two rows, and columns equal to the number of unique
vocabulary words in the document.</p>
</td></tr>
<tr><td><code id="lpbdcpp_+3A_v">V</code></td>
<td>
<p>the size of the vocabulary</p>
</td></tr>
<tr><td><code id="lpbdcpp_+3A_mv">mv</code></td>
<td>
<p>the baseline log-transformed occurrence rate of each word in the corpus</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The Hessian matrix
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("tm"); library("stm"); library("sts")
temp&lt;-textProcessor(documents=gadarian$open.ended.response,
metadata=gadarian, verbose = FALSE)
out &lt;- prepDocuments(temp$documents, temp$vocab, temp$meta, verbose = FALSE)
out$meta$noTreatment &lt;- ifelse(out$meta$treatment == 1, -1, 1)
## low max iteration number just for testing
sts_estimate &lt;- sts(~ treatment*pid_rep, ~ noTreatment, out, K = 3, maxIter = 2)
# for document #1: 
aelbo &lt;- lpbdcpp(alpha_d = sts_estimate$alpha[1,], kappa_t=sts_estimate$kappa$kappa_t, 
kappa_s=sts_estimate$kappa$kappa_s, Sigma_Inv = sts_estimate$sigma_inv, doc = out$documents[[1]], 
V=length(sts_estimate$vocab), mv = sts_estimate$mv, mu_d = sts_estimate$mu[,1])

</code></pre>

<hr>
<h2 id='plot.STS'>Function for plotting STS objects</h2><span id='topic+plot.STS'></span>

<h3>Description</h3>

<p>Produces a plot of the most likely words and their probabilities for each topic for different levels
of sentiment for an STS object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'STS'
plot(
  x,
  n = 10,
  topics = NULL,
  lowerPercentile = 0.05,
  upperPercentile = 0.95,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.STS_+3A_x">x</code></td>
<td>
<p>Model output from sts.</p>
</td></tr>
<tr><td><code id="plot.STS_+3A_n">n</code></td>
<td>
<p>Sets the number of words used to label each topic.  In perspective
plots it approximately sets the total number of words in the plot.
n must be greater than or equal to 2</p>
</td></tr>
<tr><td><code id="plot.STS_+3A_topics">topics</code></td>
<td>
<p>Vector of topics to display. Defaults to all topics.</p>
</td></tr>
<tr><td><code id="plot.STS_+3A_lowerpercentile">lowerPercentile</code></td>
<td>
<p>Percentile to calculate a representative negative sentiment document.</p>
</td></tr>
<tr><td><code id="plot.STS_+3A_upperpercentile">upperPercentile</code></td>
<td>
<p>Percentile to calculate a representative positive sentiment document.</p>
</td></tr>
<tr><td><code id="plot.STS_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to plotting functions.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
#Examples with the Gadarian Data
library("tm"); library("stm"); library("sts")
temp&lt;-textProcessor(documents=gadarian$open.ended.response,
metadata=gadarian, verbose = FALSE)
out &lt;- prepDocuments(temp$documents, temp$vocab, temp$meta, verbose = FALSE)
out$meta$noTreatment &lt;- ifelse(out$meta$treatment == 1, -1, 1)
## low max iteration number just for testing
sts_estimate &lt;- sts(~ treatment*pid_rep, ~ noTreatment, out, K = 3, maxIter = 2)
plot(sts_estimate)
plot(sts_estimate, n = 10, topic = c(1,2))

</code></pre>

<hr>
<h2 id='plotRepresentativeDocs'>Plot Documents that Load Heavily on a Topic</h2><span id='topic+plotRepresentativeDocs'></span>

<h3>Description</h3>

<p>Produces a plot of the text of documents that load most heavily on topics for an STS object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotRepresentativeDocs(object, text.cex = 1, width = 100)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plotRepresentativeDocs_+3A_object">object</code></td>
<td>
<p>Model output from sts.</p>
</td></tr>
<tr><td><code id="plotRepresentativeDocs_+3A_text.cex">text.cex</code></td>
<td>
<p>Size of the text; Defaults to 1</p>
</td></tr>
<tr><td><code id="plotRepresentativeDocs_+3A_width">width</code></td>
<td>
<p>Size of the plotting window; Defaults to 100</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
#Examples with the Gadarian Data
library("tm"); library("stm"); library("sts")
temp&lt;-textProcessor(documents=gadarian$open.ended.response,
metadata=gadarian, verbose = FALSE)
out &lt;- prepDocuments(temp$documents, temp$vocab, temp$meta, verbose = FALSE)
out$meta$noTreatment &lt;- ifelse(out$meta$treatment == 1, -1, 1)
## low max iteration number just for testing
sts_estimate &lt;- sts(~ treatment*pid_rep, ~ noTreatment, out, K = 3, maxIter = 2)
docs &lt;- findRepresentativeDocs(sts_estimate, out$meta$open.ended.response, topic = 3, n = 1)
plotRepresentativeDocs(docs, text.cex = 0.7, width = 100)

</code></pre>

<hr>
<h2 id='printRegnTables'>Print Estimated Regression Tables</h2><span id='topic+printRegnTables'></span>

<h3>Description</h3>

<p>Prints estimated regression tables from estimateRegnTables()
</p>


<h3>Usage</h3>

<pre><code class='language-R'>printRegnTables(
  x,
  topics = NULL,
  digits = max(3L, getOption("digits") - 3L),
  signif.stars = getOption("show.signif.stars"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="printRegnTables_+3A_x">x</code></td>
<td>
<p>the estimated regression tables from estimateRegnTables()</p>
</td></tr>
<tr><td><code id="printRegnTables_+3A_topics">topics</code></td>
<td>
<p>Vector of topics to display. Defaults to all topics.</p>
</td></tr>
<tr><td><code id="printRegnTables_+3A_digits">digits</code></td>
<td>
<p>minimum number of significant digits to be used for most numbers.</p>
</td></tr>
<tr><td><code id="printRegnTables_+3A_signif.stars">signif.stars</code></td>
<td>
<p>logical; if TRUE, P-values are additionally encoded
visually as ‘significance stars’ in order to help scanning of long
coefficient tables. It defaults to the show.signif.stars slot of options.</p>
</td></tr>
<tr><td><code id="printRegnTables_+3A_...">...</code></td>
<td>
<p>other arguments suitable for stats::printCoefmat()</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Prints estimated regression tables from estimateRegnTables() to console
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("tm"); library("stm"); library("sts")
temp&lt;-textProcessor(documents=gadarian$open.ended.response,
metadata=gadarian, verbose = FALSE)
out &lt;- prepDocuments(temp$documents, temp$vocab, temp$meta, verbose = FALSE)
out$meta$noTreatment &lt;- ifelse(out$meta$treatment == 1, -1, 1)
## low max iteration number just for testing
sts_estimate &lt;- sts(~ treatment*pid_rep, ~ noTreatment, out, K = 3, maxIter = 2)
regns &lt;- estimateRegns(sts_estimate, ~treatment*pid_rep, out)
printRegnTables(x = regns)

</code></pre>

<hr>
<h2 id='printTopWords'>Print Top Words that Load Heavily on each Topic</h2><span id='topic+printTopWords'></span>

<h3>Description</h3>

<p>Prints the top words for each document for low, average, and high levels of sentiment-discourse
</p>


<h3>Usage</h3>

<pre><code class='language-R'>printTopWords(object, n = 10, lowerPercentile = 0.05, upperPercentile = 0.95)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="printTopWords_+3A_object">object</code></td>
<td>
<p>Model output from sts</p>
</td></tr>
<tr><td><code id="printTopWords_+3A_n">n</code></td>
<td>
<p>number of words to print to console for each topic</p>
</td></tr>
<tr><td><code id="printTopWords_+3A_lowerpercentile">lowerPercentile</code></td>
<td>
<p>Percentile to calculate a representative negative sentiment document.</p>
</td></tr>
<tr><td><code id="printTopWords_+3A_upperpercentile">upperPercentile</code></td>
<td>
<p>Percentile to calculate a representative positive sentiment document.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
#Examples with the Gadarian Data
library("tm"); library("stm"); library("sts")
temp&lt;-textProcessor(documents=gadarian$open.ended.response,
metadata=gadarian, verbose = FALSE)
out &lt;- prepDocuments(temp$documents, temp$vocab, temp$meta, verbose = FALSE)
out$meta$noTreatment &lt;- ifelse(out$meta$treatment == 1, -1, 1)
## low max iteration number just for testing
sts_estimate &lt;- sts(~ treatment*pid_rep, ~ noTreatment, out, K = 3, maxIter = 2)
printTopWords(sts_estimate)

</code></pre>

<hr>
<h2 id='sts'>Variational EM for the Structural Topic and Sentiment-Discourse (STS) Model</h2><span id='topic+sts'></span>

<h3>Description</h3>

<p>Estimation of the STS Model using variational EM.
The function takes sparse representation of a document-term matrix, covariates
for each document, and an integer number of topics and returns fitted model
parameters. See an overview of functions in the package here:
<code><a href="#topic+sts-package">sts-package</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sts(
  prevalence_sentiment,
  initializationVar,
  corpus,
  K,
  maxIter = 100,
  convTol = 1e-05,
  initialization = "anchor",
  kappaEstimation = "adjusted",
  verbose = TRUE,
  parallelize = FALSE,
  stmSeed = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sts_+3A_prevalence_sentiment">prevalence_sentiment</code></td>
<td>
<p>A formula object with no response variable or a
design matrix with the covariates. The variables must be
contained in corpus$meta.</p>
</td></tr>
<tr><td><code id="sts_+3A_initializationvar">initializationVar</code></td>
<td>
<p>A formula with a single variable for use in the initialization of latent sentiment. This argument
is usually the key experimental variable (e.g., review rating binary indicator of experiment/control group).</p>
</td></tr>
<tr><td><code id="sts_+3A_corpus">corpus</code></td>
<td>
<p>The document term matrix to be modeled in a sparse term count matrix with one row
per document and one column per term. The object must be a list of with each element
corresponding to a document. Each document is represented
as an integer matrix with two rows, and columns equal to the number of unique
vocabulary words in the document.  The first row contains the 1-indexed
vocabulary entry and the second row contains the number of times that term
appears. This is the same format in the <code><a href="stm.html#topic+stm">stm</a></code> package.</p>
</td></tr>
<tr><td><code id="sts_+3A_k">K</code></td>
<td>
<p>A positive integer (of size 2 or greater) representing
the desired number of topics.</p>
</td></tr>
<tr><td><code id="sts_+3A_maxiter">maxIter</code></td>
<td>
<p>A positive integer representing the max number of VEM iterations allowed.</p>
</td></tr>
<tr><td><code id="sts_+3A_convtol">convTol</code></td>
<td>
<p>Convergence tolerance for the variational EM estimation algorithm; Default value = 1e-5.</p>
</td></tr>
<tr><td><code id="sts_+3A_initialization">initialization</code></td>
<td>
<p>Character argument that allows the user to specify an initialization
method. The default choice, <code>"anchor"</code> to initialize prevalence according to anchor words and
the key experimental covariate identified in argument <code>initializationVar</code>. One can also use
<code>"stm"</code>, which uses a fitted STM model (Roberts et al. 2014, 2016)
to initialize coefficients related to prevalence and sentiment-discourse.</p>
</td></tr>
<tr><td><code id="sts_+3A_kappaestimation">kappaEstimation</code></td>
<td>
<p>A character input specifying how kappa should be estimated. <code>"lasso"</code> allows for
penalties on the L1 norm.  We estimate a regularization path and then select the optimal
shrinkage parameter using AIC. <code>"adjusted"</code> (default) utilizes the lasso penalty with an adjusted aggregated Poisson regression.
All options use an approximation framework developed in Taddy (2013) called
Distributed Multinomial Regression which utilizes a factorized poisson
approximation to the multinomial.  See Li and Mankad (2024) on the implementation here.</p>
</td></tr>
<tr><td><code id="sts_+3A_verbose">verbose</code></td>
<td>
<p>A logical flag indicating whether information should be
printed to the screen.</p>
</td></tr>
<tr><td><code id="sts_+3A_parallelize">parallelize</code></td>
<td>
<p>A logical flag indicating whether to parallelize the estimation using all but one CPU cores on your local machine.</p>
</td></tr>
<tr><td><code id="sts_+3A_stmseed">stmSeed</code></td>
<td>
<p>A prefit STM model object to initialize the STS model. Note this is ignored unless initialization = &quot;stm&quot;</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is the main function for estimating the Structural Topic and
Sentiment-Discourse (STS) Model. Users provide a corpus of documents and a
number of topics.  Each word in a document comes from exactly one topic and
each document is represented by the proportion of its words that come from
each of the topics. The document-specific content covariates affect how much
(prevalence) and the way in which a topic is discussed (sentiment-discourse).
</p>


<h3>Value</h3>

<p>An object of class sts
</p>
<table role = "presentation">
<tr><td><code>alpha</code></td>
<td>
<p>Estimated prevalence and sentiment-discourse values for each document and topic</p>
</td></tr>
<tr><td><code>gamma</code></td>
<td>
<p>Estimated regression coefficients that determine prevalence and sentiment/discourse for each topic</p>
</td></tr>
<tr><td><code>kappa</code></td>
<td>
<p>Estimated kappa coefficients that determine sentiment-discourse and the topic-word distributions</p>
</td></tr>
<tr><td><code>sigma_inv</code></td>
<td>
<p>Inverse of the covariance matrix for the alpha parameters</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>Covariance matrix for the alpha parameters</p>
</td></tr>
<tr><td><code>elbo</code></td>
<td>
<p>the ELBO at each iteration of the estimation algorithm</p>
</td></tr>
<tr><td><code>mv</code></td>
<td>
<p>the baseline log-transformed occurrence rate of each word in the corpus</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>
<p>Time elapsed in seconds</p>
</td></tr>
<tr><td><code>vocab</code></td>
<td>
<p>Vocabulary vector used</p>
</td></tr>
<tr><td><code>mu</code></td>
<td>
<p>Mean (fitted) values for alpha based on document-level variables * estimated
Gamma for each document</p>
</td></tr>
</table>


<h3>References</h3>

<p>Roberts, M., Stewart, B., Tingley, D., and Airoldi, E. (2013)
&quot;The structural topic model and applied social science.&quot; In Advances in
Neural Information Processing Systems Workshop on Topic Models: Computation,
Application, and Evaluation.
</p>
<p>Roberts M., Stewart, B. and Airoldi, E. (2016) &quot;A model of text for
experimentation in the social sciences&quot; Journal of the American Statistical
Association.
</p>
<p>Chen L. and Mankad, S. (2024) &quot;A Structural Topic and Sentiment-Discourse Model
for Text Analysis&quot; Management Science.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+estimateRegns">estimateRegns</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#An example using the Gadarian data from the stm package.  From Raw text to 
# fitted model using textProcessor() which leverages the tm Package
library("tm"); library("stm"); library("sts")
temp&lt;-textProcessor(documents=gadarian$open.ended.response,
metadata=gadarian, verbose = FALSE)
out &lt;- prepDocuments(temp$documents, temp$vocab, temp$meta, verbose = FALSE)
out$meta$noTreatment &lt;- ifelse(out$meta$treatment == 1, -1, 1)
## low max iteration number just for testing
sts_estimate &lt;- sts(~ treatment*pid_rep, ~ noTreatment, out, K = 3, maxIter = 1, verbose = FALSE)
</code></pre>

<hr>
<h2 id='summary.STS'>Summary Function for the STS objects</h2><span id='topic+summary.STS'></span><span id='topic+print.STS'></span>

<h3>Description</h3>

<p>Function to report on the contents of STS objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'STS'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="summary.STS_+3A_object">object</code></td>
<td>
<p>An STS object.</p>
</td></tr>
<tr><td><code id="summary.STS_+3A_...">...</code></td>
<td>
<p>Additional arguments affecting the summary</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Summary prints a short statement about the model and then runs
<code><a href="#topic+printTopWords">printTopWords</a></code>.
</p>

<hr>
<h2 id='topicExclusivity'>Compute Exclusivity</h2><span id='topic+topicExclusivity'></span>

<h3>Description</h3>

<p>Calculate an exclusivity metric for an STS model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>topicExclusivity(object, M = 10, frexw = 0.7)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="topicExclusivity_+3A_object">object</code></td>
<td>
<p>Model output from sts</p>
</td></tr>
<tr><td><code id="topicExclusivity_+3A_m">M</code></td>
<td>
<p>the number of top words to consider per topic</p>
</td></tr>
<tr><td><code id="topicExclusivity_+3A_frexw">frexw</code></td>
<td>
<p>the frex weight</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Roberts et al 2014 proposed an exclusivity measure to help with topic model
selection.
</p>
<p>The exclusivity measure includes some information on word frequency as well.
It is based on the FREX
labeling metric (see Roberts et al. 2014) with the weight set to .7 in
favor of exclusivity by default.
</p>


<h3>Value</h3>

<p>a numeric vector containing exclusivity for each topic
</p>


<h3>References</h3>

<p>Mimno, D., Wallach, H. M., Talley, E., Leenders, M., and
McCallum, A. (2011, July). &quot;Optimizing semantic coherence in topic models.&quot;
In Proceedings of the Conference on Empirical Methods in
Natural Language Processing (pp. 262-272). Association for
Computational Linguistics. Chicago
</p>
<p>Bischof and Airoldi (2012) &quot;Summarizing topical content with word frequency
and exclusivity&quot; In Proceedings of the International Conference on
Machine Learning.
</p>
<p>Roberts, M., Stewart, B., Tingley, D., Lucas, C., Leder-Luis, J.,
Gadarian, S., Albertson, B., et al. (2014).
&quot;Structural topic models for open ended survey responses.&quot; American Journal
of Political Science, 58(4), 1064-1082.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
#An example using the Gadarian data from the stm package.  
# From Raw text to fitted model using textProcessor() which leverages the 
# tm Package
library("tm"); library("stm"); library("sts")
temp&lt;-textProcessor(documents=gadarian$open.ended.response,
metadata=gadarian, verbose = FALSE)
out &lt;- prepDocuments(temp$documents, temp$vocab, temp$meta, verbose = FALSE)
out$meta$noTreatment &lt;- ifelse(out$meta$treatment == 1, -1, 1)
## low max iteration number just for testing
sts_estimate &lt;- sts(~ treatment*pid_rep, ~ noTreatment, out, K = 3, maxIter = 2)
topicExclusivity(sts_estimate)

</code></pre>

<hr>
<h2 id='topicSemanticCoherence'>Compute Semantic Coherence</h2><span id='topic+topicSemanticCoherence'></span>

<h3>Description</h3>

<p>Calculates semantic coherence for an STS model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>topicSemanticCoherence(object, corpus, M = 10)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="topicSemanticCoherence_+3A_object">object</code></td>
<td>
<p>Model output from sts</p>
</td></tr>
<tr><td><code id="topicSemanticCoherence_+3A_corpus">corpus</code></td>
<td>
<p>The document term matrix to be modeled in a sparse term count matrix with one row
per document and one column per term. The object must be a list of with each element
corresponding to a document. Each document is represented
as an integer matrix with two rows, and columns equal to the number of unique
vocabulary words in the document.  The first row contains the 1-indexed
vocabulary entry and the second row contains the number of times that term
appears. This is the same format in the <code><a href="stm.html#topic+stm">stm</a></code> package.</p>
</td></tr>
<tr><td><code id="topicSemanticCoherence_+3A_m">M</code></td>
<td>
<p>the number of top words to consider per topic</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a numeric vector containing semantic coherence for each topic
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#An example using the Gadarian data from the stm package.  From Raw text to 
# fitted model using textProcessor() which leverages the tm Package
library("tm"); library("stm"); library("sts")
temp&lt;-textProcessor(documents=gadarian$open.ended.response,
metadata=gadarian, verbose = FALSE)
out &lt;- prepDocuments(temp$documents, temp$vocab, temp$meta, verbose = FALSE)
out$meta$noTreatment &lt;- ifelse(out$meta$treatment == 1, -1, 1)
## low max iteration number just for testing
sts_estimate &lt;- sts(~ treatment*pid_rep, ~ noTreatment, out, K = 3, maxIter = 2, verbose = FALSE)
topicSemanticCoherence(sts_estimate, out)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
