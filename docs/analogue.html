<!DOCTYPE html><html lang="en"><head><title>Help for package analogue</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {analogue}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#analogue-package'>
<p>Analogue and weighted averaging methods for palaeoecology</p></a></li>
<li><a href='#abernethy'><p>Abernethy Forest Pollen Sequence</p></a></li>
<li><a href='#analog'><p>Analogue matching</p></a></li>
<li><a href='#analogue-internal'><p>Internal analogue Functions</p></a></li>
<li><a href='#bayesF'><p>Bayes factors</p></a></li>
<li><a href='#bootstrap'><p>Bootstrap estimation and errors</p></a></li>
<li><a href='#bootstrap.wa'><p>Bootstrap estimation and errors for WA models</p></a></li>
<li><a href='#bootstrapObject'><p>Bootstrap object description</p></a></li>
<li><a href='#caterpillarPlot'>
<p>Caterpillar plot of species' WA optima and tolerance range.</p></a></li>
<li><a href='#chooseTaxa'><p>Select taxa (variables) on basis of maximum abundance attained</p>
and number of occurrences.</a></li>
<li><a href='#cma'><p>Close modern analogues</p></a></li>
<li><a href='#compare'><p>Compare proxies across two data sets</p></a></li>
<li><a href='#crossval'><p>Cross-validation of palaeoecological transfer function models</p></a></li>
<li><a href='#densityplot.residLen'><p>Lattice density plot for residual lengths</p></a></li>
<li><a href='#deshrink'>
<p>Deshrinking techniques for WA transfer functions</p></a></li>
<li><a href='#dissimilarities'><p>Extract dissimilarity coefficients from models</p></a></li>
<li><a href='#distance'><p>Flexibly calculate dissimilarity or distance measures</p></a></li>
<li><a href='#evenSample'><p>Number of samples per gradient segments</p></a></li>
<li><a href='#fitted.logitreg'><p>Fitted values for the training set from logistic regression</p>
models</a></li>
<li><a href='#fuse'><p>Fused dissimilarities</p></a></li>
<li><a href='#getK'><p>Extract and set the number of analogues</p></a></li>
<li><a href='#gradientDist'>
<p>Positions of samples along a unit-length ordination gradient.</p></a></li>
<li><a href='#hist.residLen'><p>Histogram plot for residual lengths</p></a></li>
<li><a href='#histogram.residLen'><p>Lattice histogram plot for residual lengths</p></a></li>
<li><a href='#ImbrieKipp'><p>Imbrie and Kipp foraminifera training set</p></a></li>
<li><a href='#join'><p>Merge species data sets on common columns (species)</p></a></li>
<li><a href='#logitreg'><p>Logistic regression models for assessing analogues/non-analogues</p></a></li>
<li><a href='#mat'><p>Modern Analogue Technique transfer function models</p></a></li>
<li><a href='#mcarlo'><p>Monte Carlo simulation of dissimilarities</p></a></li>
<li><a href='#minDC'><p>Extract minimum dissimilarities</p></a></li>
<li><a href='#n2'><p>Calculate Hill's N2 diversity measure</p></a></li>
<li><a href='#optima'><p>Weighted averaging optima and tolerance ranges</p></a></li>
<li><a href='#panel.Loess'><p>Loess smooths to stratigraphic diagrams</p></a></li>
<li><a href='#panel.Stratiplot'><p>Panel function for stratigraphic diagrams</p></a></li>
<li><a href='#pcr'><p>Prinicpal component regression transfer function models</p></a></li>
<li><a href='#performance'><p>Transfer function model performance statistics</p></a></li>
<li><a href='#plot.dissimilarities'><p>Plots the distribution of extracted dissimilarities</p></a></li>
<li><a href='#plot.evenSample'><p>Plot distribution of samples along gradient</p></a></li>
<li><a href='#plot.logitreg'><p>Produces plots of analogue logistic regression models</p></a></li>
<li><a href='#plot.mat'><p>Plot diagnostics for a mat object</p></a></li>
<li><a href='#plot.mcarlo'><p>Plot Monte Carlo simulated dissimilarity distributions</p></a></li>
<li><a href='#plot.minDC'><p>Plot of minimum dissimilarity per sample</p></a></li>
<li><a href='#plot.prcurve'>
<p>Plot a fitted principal curve in PCA space</p></a></li>
<li><a href='#plot.residLen'><p>Plot method for residual lengths</p></a></li>
<li><a href='#plot.roc'><p>Plot ROC curves and associated diagnostics</p></a></li>
<li><a href='#plot.sppResponse'><p>Plot species responses along gradients or latent variables</p></a></li>
<li><a href='#plot.wa'><p>Plot diagnostics for a weighted averaging model</p></a></li>
<li><a href='#Pollen'><p>North American Modern Pollen Database</p></a></li>
<li><a href='#prcurve'>
<p>Fits a principal curve to m-dimensional data</p></a></li>
<li><a href='#predict.logitreg'><p>Posterior probability of analogue-ness for fossil samples</p></a></li>
<li><a href='#predict.mat'><p>Predict method for Modern Analogue Technique models</p></a></li>
<li><a href='#predict.pcr'><p>Predicted values from a principal components regression</p></a></li>
<li><a href='#predict.prcurve'><p>Predict new locations &amp; fitted values on a principal curve</p></a></li>
<li><a href='#predict.wa'><p>Predict from a weighted average model</p></a></li>
<li><a href='#rankDC'><p>Rank correlation between environmental and species dissimilarities.</p></a></li>
<li><a href='#reconPlot'><p>Stratigraphic plots of palaeoenvironmental reconstructions</p></a></li>
<li><a href='#residLen'><p>Squared residual length diagnostics</p></a></li>
<li><a href='#residuals.prcurve'>
<p>Residuals of a principal curve fit.</p></a></li>
<li><a href='#rlgh'><p>Round Loch of Glenhead Diatoms</p></a></li>
<li><a href='#RMSEP'><p>Root mean square error of prediction</p></a></li>
<li><a href='#roc'><p>ROC curve analysis</p></a></li>
<li><a href='#scores.prcurve'><p><code>scores</code> method for principal curve objects of</p>
class <code>"prcurve"</code>.</a></li>
<li><a href='#screeplot'><p>Screeplots of model results</p></a></li>
<li><a href='#smoothers'>
<p>Smoother plugin function for use in fitting a principal curve</p></a></li>
<li><a href='#splitSample'>
<p>Select samples from along an environmental gradient</p></a></li>
<li><a href='#sppResponse'><p>Species responses along gradients.</p></a></li>
<li><a href='#stdError'><p>Standard error of MAT fitted and predicted values</p></a></li>
<li><a href='#Stratiplot'><p>Palaeoecological stratigraphic diagrams</p></a></li>
<li><a href='#summary.analog'><p>Summarise analogue matching results</p></a></li>
<li><a href='#summary.bootstrap.mat'><p>Summarise bootstrap resampling for MAT models</p></a></li>
<li><a href='#summary.cma'><p>Summarise the extraction of close modern analogues</p></a></li>
<li><a href='#summary.mat'><p>Summarise Modern Analogue Technique models</p></a></li>
<li><a href='#summary.predict.mat'><p>Summarise MAT model predictions</p></a></li>
<li><a href='#swapdiat'><p>SWAP sub-fossil diatom and pH training set</p></a></li>
<li><a href='#swappH'><p>SWAP sub-fossil diatom and pH training set</p></a></li>
<li><a href='#timetrack'><p>Timetracks of change in species composition</p></a></li>
<li><a href='#tortula'><p>Morphological data for ten taxa of the genus Tortula</p></a></li>
<li><a href='#tran'><p>Common data transformations and standardizations</p></a></li>
<li><a href='#varExpl'>
<p>Variance explained by ordination axes</p></a></li>
<li><a href='#wa'><p>Weighted averaging transfer functions</p></a></li>
<li><a href='#weightedCor'>
<p>Weighted correlation test of WA reconstruction</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Analogue and Weighted Averaging Methods for Palaeoecology</td>
</tr>
<tr>
<td>Version:</td>
<td>0.17-7</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-09-02</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0), vegan (&ge; 2.2-0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>mgcv, MASS, stats, graphics, grid, brglm, princurve (&ge;
2.0.2), lattice</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Gavin L. Simpson &lt;ucfagls@gmail.com&gt;</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/gavinsimpson/analogue/issues">https://github.com/gavinsimpson/analogue/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Description:</td>
<td>Fits Modern Analogue Technique and Weighted Averaging transfer
  	     function models for prediction of environmental data from species
	     data, and related methods used in palaeoecology.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>true</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/gavinsimpson/analogue">https://github.com/gavinsimpson/analogue</a></td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-09-20 20:12:51 UTC; au690221</td>
</tr>
<tr>
<td>Author:</td>
<td>Gavin L. Simpson <a href="https://orcid.org/0000-0002-9084-8413"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Jari Oksanen [aut],
  Martin Maechler [ctb]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-09-21 08:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='analogue-package'>
Analogue and weighted averaging methods for palaeoecology
</h2><span id='topic+analogue-package'></span><span id='topic+analogue'></span>

<h3>Description</h3>

<p><span class="pkg">analogue</span> is a package for quantitative palaeoecology with a
focus on analogue methods, transfer functions, and data handling and
display.
</p>


<h3>Analogue methods</h3>

<p><span class="pkg">analogue</span> provides functions for analogue matching and the modern
analogue technique (MAT) via <code><a href="#topic+analog">analog</a></code> and
<code><a href="#topic+mat">mat</a></code>. A wide range of dissimilarity coefficients are
available via the <code><a href="#topic+distance">distance</a></code> function.
</p>
<p>Additional analysis of modern and no-analogue problems is facilitated
via a range of functions implementing many methods from the
literature. In particular, the receiver operating characteric (ROC)
curves method of Gavin et al (2003) is available in <code><a href="#topic+roc">roc</a></code>
and a related method employing direct logistic regression modelling
(Simpson &amp; Birks, 2012) is available in <code><a href="#topic+logitreg">logitreg</a></code>.
</p>


<h3>Transfer function methods</h3>

<p>Several approaches to fitting transfer function models are provided by
<span class="pkg">analogue</span>:
</p>

<dl>
<dt> <code><a href="#topic+wa">wa</a></code>: </dt><dd><p>Simple and tolerance-downweighted weighted
averaging with classical, inverse, and monotonic spline
deshrinking.</p>
</dd>
<dt> <code><a href="#topic+mat">mat</a></code>: </dt><dd><p>The modern analogue technique (MAT).</p>
</dd>
<dt> <code><a href="#topic+pcr">pcr</a></code>: </dt><dd><p>Principal components regression with
ecologically meaningful transformations</p>
</dd>
</dl>

<p>A range of functions for working with and exploring training sets and
palaeoenvironmental reconstructions is also included in
<span class="pkg">analogue</span>. These include
</p>

<dl>
<dt> <code><a href="#topic+crossval">crossval</a></code> </dt><dd><p>leave-one-out, repeated k-fold, and
bootstrap cross-validation methods.</p>
</dd>
<dt> <code><a href="#topic+compare">compare</a></code> </dt><dd><p>compare properties of taxa or other
proxies across modern and fossil data sets.</p>
</dd>
<dt> <code><a href="#topic+evenSample">evenSample</a></code> </dt><dd><p>are training set samples evenly
distributed along the gradient of interest?</p>
</dd>
<dt> <code><a href="#topic+splitSample">splitSample</a></code> </dt><dd><p>splits a gradient into a set of
bins or chunks and samples evenly from within each chunk to create
a representative test set for cross-validation.</p>
</dd>
<dt> <code><a href="#topic+timetrack">timetrack</a></code> </dt><dd><p>overlays a fossil or secondary data
set on to an (constrained) ordination of a modern or reference
data set.</p>
</dd>
<dt> <code><a href="#topic+weightedCor">weightedCor</a></code> </dt><dd><p>implements the weighted
correlation test of a Weighted Averaging reconstruction as proposed
by Telford &amp; Birks (2011).</p>
</dd>
</dl>



<h3>Utilities</h3>

<p><span class="pkg">analogue</span> provides a range of utilities for working with palaeo
data.
</p>

<dl>
<dt> <code><a href="#topic+tran">tran</a></code> </dt><dd><p>a range of transformations applicable to
or commonly used with palaeo data.</p>
</dd>
<dt> <code><a href="#topic+Stratiplot">Stratiplot</a></code> </dt><dd><p>draws stratigraphic diagrams using
the Lattice package.</p>
</dd>
<dt> <code><a href="#topic+join">join</a></code> </dt><dd><p>merging of modern/training set and
fossil data sets.</p>
</dd>
<dt> <code><a href="#topic+chooseTaxa">chooseTaxa</a></code> </dt><dd><p>selects taxa that meet certain
abundance and occurrence criteria.</p>
</dd>
</dl>



<h3>Documentation</h3>

<p>A full tutorial and worked example for the main features of analogue
matching and MAT is avilable in the vignette
</p>

<dl>
<dt><code>analogue_methods</code></dt><dd><p>Analogue Methods in Palaeoecology</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Gavin L. Simpson, Jari Oksanen
</p>
<p>Maintainer: Gavin L. Simpson &lt;ucfagls@gmail.com&gt;
</p>


<h3>References</h3>

<p>Gavin, D.G., Oswald, W.W., Wahl, E.R. and Williams, J.W. (2003) A
statistical approach to evaluating distance metrics and analog
assignments for pollen records. <em>Quaternary Research</em>
<strong>60</strong>, 356&ndash;367.
</p>
<p>Simpson, G.L. &amp; Birks H.J.B. (2012) Statistical Learning in
Palaeolimnology. In Birks, H.J.B, Lotter, A.F. Juggins S., and Smol,
J.P. (Eds) <em>Tracking Environmental Change Using Lake Sediments,
Volume 5: Data Handling and Numerical Techniques</em>. Springer,
Dordrecht.
</p>
<p>Telford R.J. and Birks, H.J.B. (2011) A novel method for assessing the
statistical significance of quantitative reconstructions inferred from
biotic assemblages. <em>Quanternary Science Reviews</em>
<strong>30</strong>:1272-1278.
</p>

<hr>
<h2 id='abernethy'>Abernethy Forest Pollen Sequence</h2><span id='topic+abernethy'></span>

<h3>Description</h3>

<p>The classic pollen data set from Abernethy Forest in the Scottish
highlands, UK. The data originate from the work of Hilary Birks and
Rolf Mathewes (1978) and have been analysed in several texts on
quantitative numerical palaeoecology.
</p>
<p>The data set consists of 36 pollen taxa from 49 levels, with two
additional variables; <code>Age</code>, the age of each sample, and
<code>Depth</code> the depth (in cm) below the surface of the peat sequence
from which the core was taken.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(abernethy)
</code></pre>


<h3>Format</h3>

<p><code>abernethy</code> is a data frame with 49 samples on 36 species plus
sample Age and Depth (in cm).
</p>


<h3>Source</h3>

<p>These data were provided in electronic format by Prof. H. John
B. Birks. The original source is Birks and Mathewes (1978).
</p>


<h3>References</h3>

<p>Birks, H.H. and Mathewes, R.W. (1978) Studies in the vegetational
history of Scotland. <em>New Phytologist</em> <strong>80</strong>, 455-484.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(abernethy)
head(abernethy)

(plt &lt;- Stratiplot(Age ~ . - Depth,
                   data = chooseTaxa(abernethy, n.occ = 5, max.abun = 10),
                   type = "poly"))
</code></pre>

<hr>
<h2 id='analog'>Analogue matching</h2><span id='topic+analog'></span><span id='topic+analog.default'></span><span id='topic+analog.distance'></span><span id='topic+print.analog'></span>

<h3>Description</h3>

<p>Analogue matching is a  more general implementation of the modern
analogue methodology than MAT, where we are only interested in
identifying sufficiently similar samples from a modern training as
being suitable modern analogues for one or more fossil samples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>analog(x, ...)

## Default S3 method:
analog(x, y,
       method = c("euclidean", "SQeuclidean", "chord", "SQchord",
                  "bray", "chi.square", "SQchi.square",
                  "information", "chi.distance", "manhattan",
                  "kendall", "gower", "alt.gower", "mixed"),
       keep.train = TRUE, ...)

## S3 method for class 'distance'
analog(x, train = NULL, keep.train = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="analog_+3A_x">x</code>, <code id="analog_+3A_y">y</code></td>
<td>
<p>data frames with same columns. <code>x</code> is training data
and <code>y</code>, the test data.</p>
</td></tr>
<tr><td><code id="analog_+3A_method">method</code></td>
<td>
<p>character string naming the dissimilarity methods to be
used. See Details below.</p>
</td></tr>
<tr><td><code id="analog_+3A_keep.train">keep.train</code></td>
<td>
<p>logical; should the dissimilarity matrix for the
training set be stored?</p>
</td></tr>
<tr><td><code id="analog_+3A_train">train</code></td>
<td>
<p>a pre-computed dissimilarity matrix for the training set
samples. Objects of classes <code>"dist"</code>, <code>"vegdist"</code>, and
<code>"distance"</code> are currently accepted.</p>
</td></tr>
<tr><td><code id="analog_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>analog</code> implements analogue matching <em>sensu</em> Flower et
al (1997) and Simpson et al (2005), where the aim is to identify
suitable close analogues of fossil samples from a modern training
set. These results are generally used within ecological restoration,
but the identification of close modern analogues for fossil samples is
also used as a technique for assessing transfer function
reconstructions.
</p>
<p><code>analog</code> is a simple and very general function that generates a
pairwise dissimilarity matrix for the modern training set, and a second
matrix containing the pairwise dissimilarities between each fossil
sample and each sample in the training set. These results can then be
assessed using other functions and to extract the close modern
analogues using function <code><a href="#topic+cma">cma</a></code>. See the See Also section
below.
</p>
<p>Analysis of the pairwise dissimilarity matrix for the modern training
set can be used to decide on a suitable dissimilarity threshold
for defining close modern analogues. By default this matrix is
returned as part of the output from the <code>analog</code> function.
</p>


<h3>Value</h3>

<p>A list of class <code>"analog"</code> with the following components:
</p>
<table role = "presentation">
<tr><td><code>analogs</code></td>
<td>
<p>matrix of pairwise dissimilarities between each fossil
sample (<code>y</code>) and each sample in the modern training set
(<code>x</code>).</p>
</td></tr>
<tr><td><code>train</code></td>
<td>
<p>if argument <code>keep.train</code> is <code>TRUE</code> then a
pairwise dissimilarity matrix for the modern training set.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matched function call.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>character; the dissimilarity coefficient used.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>References</h3>

<p>Flower, R.J., Juggins, S. and Battarbee, R.W. (1997) Matching diatom
assemblages in lake sediment cores and modern surface sediment
samples: the implications for lake conservation and restoration with
special reference to acidified systems. <em>Hydrobiologia</em>
<strong>344</strong>; 27&ndash;40.
</p>
<p>Simpson, G.L., Shilland, E.M., Winterbottom, J. M. and Keay, J. (2005)
Defining reference conditions for acidified waters using a modern
analogue approach. <em>Environmental Pollution</em> <strong>137</strong>;
119&ndash;133.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+distance">distance</a></code> for the function that calculates the
dissimilarity matrices.
<code><a href="#topic+cma">cma</a></code> for extraction of close modern analogues.
<code><a href="#topic+dissimilarities">dissimilarities</a></code> and <code><a href="#topic+plot.dissimilarities">plot.dissimilarities</a></code>
for analysis of distribution of pairwise dissimilarity matrix for
modern training set.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Imbrie and Kipp example
## load the example data
data(ImbrieKipp)
data(SumSST)
data(V12.122)

## merge training and test set on columns
dat &lt;- join(ImbrieKipp, V12.122, verbose = TRUE)

## extract the merged data sets and convert to proportions
ImbrieKipp &lt;- dat[[1]] / 100
V12.122 &lt;- dat[[2]] / 100

## Imbrie and Kipp foraminfera sea-surface temperature

## analog matching between SWAP and RLGH core
ik.analog &lt;- analog(ImbrieKipp, V12.122, method = "chord")
ik.analog
summary(ik.analog)

## Can take pre-computed dissimilarity objects
d1 &lt;- distance(ImbrieKipp, V12.122)
d2 &lt;- distance(ImbrieKipp)
ik &lt;- analog(d1, d2, keep.train = TRUE)
ik

</code></pre>

<hr>
<h2 id='analogue-internal'>Internal analogue Functions</h2><span id='topic+cumWmean'></span><span id='topic+cummean'></span><span id='topic+minDij'></span><span id='topic+maxBias'></span><span id='topic+.simpleCap'></span><span id='topic+wmean'></span><span id='topic+w.avg'></span><span id='topic+RowSums'></span><span id='topic+ColSums'></span><span id='topic+deshrink.pred'></span><span id='topic+sppN2'></span><span id='topic+w.tol'></span><span id='topic+WApred'></span><span id='topic+WATpred'></span><span id='topic+fixUpTol'></span><span id='topic+rdaFit'></span><span id='topic+scores.rdaFit'></span>

<h3>Description</h3>

<p>Internal analogue functions
</p>


<h3>Details</h3>

<p>These are not to be called by the user.
</p>

<hr>
<h2 id='bayesF'>Bayes factors</h2><span id='topic+bayesF'></span><span id='topic+print.bayesF'></span><span id='topic+plot.bayesF'></span>

<h3>Description</h3>

<p>Calculates Bayes factors or likelihood ratios of analogue and
no-analogue results.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bayesF(x, prior = rep(0.5, 2))

## S3 method for class 'bayesF'
plot(x, group = "all", xlab = NULL, ylab = "Pr (A+ | d)",
        col = "red", abline.col = "lightgrey", abline.lty = "dashed", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bayesF_+3A_x">x</code></td>
<td>
<p>for <code>bayesF</code> an object of class <code>roc</code>. For the plot
method, an object of class <code>bayesF</code>, usually the result of a
call to <code>bayesF</code>.</p>
</td></tr>
<tr><td><code id="bayesF_+3A_prior">prior</code></td>
<td>
<p>numeric; the prior probabilities of analogue and
no-analogue, provided as a vector of length 2 whose elements sum to
1. If not provided, the function will use the relative occurences of
analogue and no analogue situations used to evaluate the ROC curve.</p>
</td></tr>
<tr><td><code id="bayesF_+3A_group">group</code></td>
<td>
<p>character vector of length 1 giving the name of the group
to plot, or <code>"all"</code> to plot all groups in <code>x</code>.</p>
</td></tr>
<tr><td><code id="bayesF_+3A_xlab">xlab</code>, <code id="bayesF_+3A_ylab">ylab</code></td>
<td>
<p>the x- and y-axis labels for the plot.</p>
</td></tr>
<tr><td><code id="bayesF_+3A_col">col</code></td>
<td>
<p>colour of the line used to draw the posterior probability.</p>
</td></tr>
<tr><td><code id="bayesF_+3A_abline.col">abline.col</code></td>
<td>
<p>colour of the vertical line drawn to indicate the
optimal dissimilarity determined from the ROC curve.</p>
</td></tr>
<tr><td><code id="bayesF_+3A_abline.lty">abline.lty</code></td>
<td>
<p>Line type for indicator of optimal ROC dissimilarity
threshold. See <code><a href="graphics.html#topic+par">par</a></code> for the allowed line types.</p>
</td></tr>
<tr><td><code id="bayesF_+3A_...">...</code></td>
<td>
<p>other plot arguments passed to plotting
functions. Currently ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>LR(+), is the likelihood ratio of a positive test result, that
the value of <em>d</em> assigns the sample to the group it belongs
to. LR(-) is the likelihood ratio of a negative test result, that the
value of <em>d</em> assigns the sample to the wrong group.
</p>
<p>LR(+) is defined as <code class="reqn">LR(+) = TPF / FPF</code> (or sensitivity / (1 -
specificity)), and LR(-) is defined as <code class="reqn">LR(-) = FPF / TNF</code> (or (1
- sensitivity) / specificity), in Henderson (1993).
</p>
<p>The posterior probability of analogue given a dissimilarity is the 
LR(+) likelihood ratio values multiplied by the prior odds of
analogue, for given values of the dissimilarity, and is then converted
to a  probability.
</p>
<p>The plotting function currently only draws the posterior probability
of analogue based on the Bayes factor or likelihood ratio of a
positive event (analogue).
</p>


<h3>Value</h3>

<p>For <code>plot.bayesF</code> a plot on the currently active device.
</p>
<p>For <code>bayesF</code>, a list containing the results of computing Bayes
factors for each group in <code>x</code>. Each component of this list is
itself a list with the following components:
</p>
<table role = "presentation">
<tr><td><code>bayesF</code>, <code>posterior.odds</code>, <code>posterior.probs</code>, <code>prior.prob</code></td>
<td>
<p>Bayes
factors, posterior odds and probabilities and prior probabilities of
true analogue and true non-analogue events. Each components is a list
with two components; <code>pos</code> (for true analogue events) and
<code>neg</code> (for true non-analogue events). The components of
<code>prior.prob</code> are vectors of length 1, whilst components of the
other lists are numeric vectors.</p>
</td></tr>
<tr><td><code>roc.points</code></td>
<td>
<p>numeric; the points at which the ROC curve was
evaluated.</p>
</td></tr>
<tr><td><code>optimal</code></td>
<td>
<p>numeric; the optimal dissimilarity as assessed by the
ROC curve.</p>
</td></tr>
<tr><td><code>max.roc</code></td>
<td>
<p>numeric; the position along the ROC curve at which the
slope of the ROC curve is maximal. This is the index of this point
on the curve, and can be used to extract the element of
<code>bayesF</code>, <code>posterior.odds</code> and <code>posterior.probs</code> for
the optimal dissimilarity.</p>
</td></tr>

</table>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>References</h3>

<p>Brown, C.D., and Davis, H.T. (2006) Receiver operating characteristics
curves and related decision measures: A tutorial. <em>Chemometrics
and Intelligent Laboratory Systems</em> <b>80</b>, 24&ndash;38.
</p>
<p>Gavin, D.G., Oswald, W.W., Wahl, E.R. and Williams, J.W. (2003) A
statistical approach to evaluating distance metrics and analog
assignments for pollen records. <em>Quaternary Research</em>
<strong>60</strong>, 356&ndash;367.
</p>
<p>Henderson, A.R. (1993) Assessing test accuracy and its clinical
consequences: a primer for receiver operating characteristic curve
analysis. <em>Annals of Clinical Biochemistry</em> <strong>30</strong>,
834&ndash;846.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+roc">roc</a></code> and <code><a href="#topic+plot.bayesF">plot.bayesF</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load the example data
data(swapdiat, swappH, rlgh)

## merge training and test set on columns
dat &lt;- join(swapdiat, rlgh, verbose = TRUE)

## extract the merged data sets and convert to proportions
swapdiat &lt;- dat[[1]] / 100
rlgh &lt;- dat[[2]] / 100

## fit an analogue matching (AM) model using the squared chord distance
## measure - need to keep the training set dissimilarities
swap.ana &lt;- analog(swapdiat, rlgh, method = "SQchord",
                   keep.train = TRUE)

## fit the ROC curve to the SWAP diatom data using the AM results
## Generate a grouping for the SWAP lakes
METHOD &lt;- if (getRversion() &lt; "3.1.0") {"ward"} else {"ward.D"}
clust &lt;- hclust(as.dist(swap.ana$train), method = METHOD)
grps &lt;- cutree(clust, 12)

## fit the ROC curve
swap.roc &lt;- roc(swap.ana, groups = grps)
swap.roc

## calculate the Bayes factors of analogue and no-analogue
## (uses observed probabilities of analogue/no-analogue
swap.bayes &lt;- bayesF(swap.roc)
swap.bayes

## plot the probability of analogue
plot(swap.bayes)

## Not run: 
## calculate the Bayes factors of analogue and no-analogue
## with prior probabilities c(0.5, 0.05)
swap.bayes2 &lt;- bayesF(swap.roc, prior = c(0.5, 0.05))
swap.bayes

## plot the probability of analogue
plot(swap.bayes2)

## End(Not run)
</code></pre>

<hr>
<h2 id='bootstrap'>Bootstrap estimation and errors </h2><span id='topic+bootstrap'></span><span id='topic+bootstrap.default'></span><span id='topic+bootstrap.mat'></span><span id='topic+print.bootstrap.mat'></span><span id='topic+residuals.bootstrap.mat'></span><span id='topic+resid.bootstrap.mat'></span><span id='topic+print.residuals.bootstrap.mat'></span><span id='topic+fitted.bootstrap.mat'></span><span id='topic+print.fitted.bootstrap.mat'></span>

<h3>Description</h3>

<p>Function to calculate bootstrap statistics for transfer function
models such as bootstrap estimates, model RMSEP, sample specific
errors for predictions and summary statistics such as bias and
<code class="reqn">R^2</code> between oberved and estimated
environment.
</p>
<p><code><a href="stats.html#topic+residuals">residuals</a></code> method for objects of class
<code>"bootstrap.mat"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
bootstrap(object, ...)

## Default S3 method:
bootstrap(object, ...)

## S3 method for class 'mat'
bootstrap(object, newdata, newenv, k,
          weighted = FALSE, n.boot = 1000, ...)

## S3 method for class 'bootstrap.mat'
fitted(object, k, ...)

## S3 method for class 'bootstrap.mat'
residuals(object, which = c("model", "bootstrap"), ...) 
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bootstrap_+3A_object">object</code></td>
<td>
<p>an R object of class <code>"mat"</code> for which bootstrap
statistics are to be generated, or an object of class
<code>"bootstrap.mat"</code> from which fitted values or residuals are
extracted.</p>
</td></tr>
<tr><td><code id="bootstrap_+3A_newdata">newdata</code></td>
<td>
<p>a data frame containing samples for which bootstrap
predictions and sample specific errors are to be generated. May be
missing &mdash; See Details. <code>"newdata"</code> must have the same number
of columns as the training set data.</p>
</td></tr>
<tr><td><code id="bootstrap_+3A_newenv">newenv</code></td>
<td>
<p>a vector containing environmental data for samples
in <code>"newdata"</code>. Used to calculate full suite of errors for new
data such as a test set with known environmental values. May be
missing &mdash; See Details. <code>"newenv"</code> must have the same number
of rows as <code>"newdata"</code>.</p>
</td></tr>
<tr><td><code id="bootstrap_+3A_k">k</code></td>
<td>
<p>numeric; how many modern analogues to use to generate the
bootstrap statistics (and, if requested, the predictions), fitted
values or residuals.</p>
</td></tr>
<tr><td><code id="bootstrap_+3A_weighted">weighted</code></td>
<td>
<p>logical; should the weighted mean of the environment
for the <code>"k"</code> modern analogues be used instead of the mean?</p>
</td></tr>
<tr><td><code id="bootstrap_+3A_n.boot">n.boot</code></td>
<td>
<p>Number of bootstrap samples to take.</p>
</td></tr>
<tr><td><code id="bootstrap_+3A_which">which</code></td>
<td>
<p>character; which set of residuals to return, the
model residuals or the residuals of the bootstrap-derived
estimates?</p>
</td></tr>
<tr><td><code id="bootstrap_+3A_...">...</code></td>
<td>
<p>arguments passed to other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>bootstrap</code> is a fairly flexible function, and can be called with
or without arguments <code>newdata</code> and <code>newenv</code>.
</p>
<p>If called with only <code>object</code> specified, then bootstrap estimates
for the training set data are returned. In this case, the returned
object will not include component <code>predictions</code>.
</p>
<p>If called with both <code>object</code> and <code>newdata</code>, then in addition
to the above, bootstrap estimates for the new samples are also
calculated and returned. In this case, component <code>predictions</code>
will contain the apparent and bootstrap derived predictions and
sample-specific errors for the new samples.
</p>
<p>If called with <code>object</code>, <code>newdata</code> and <code>newenv</code>, then
the full <code>bootstrap</code> object is returned (as described in the
Value section below). With environmental data now available for the
new samples, residuals, RMSE(P) and <code class="reqn">R^2</code> and bias statistics can
be calculated.
</p>
<p>The individual components of <code>predictions</code> are the same as those
described in the components relating to the training set data. For
example, <code>returned.object$predictions$bootstrap</code> contains the
components as <code>returned.object$bootstrap</code>.
</p>
<p>It is not usual for environmental data to be available for the new
samples for which predictions are required. In normal
palaeolimnological studies, it is more likely that <code>newenv</code> will
not be available as we are dealing with sediment core samples from the
past for which environmental data are not available. However, if
sufficient training set samples are available to justify producing a
training and a test set, then <code>newenv</code> will be available, and
<code>bootstrap</code> can accomodate this extra information and calculate
apparent and bootstrap estimates for the test set, allowing an
independent assessment of the RMSEP of the model to be performed.
</p>
<p>Typical usage of <code>residuals</code> is
</p>
<pre>
    resid(object, which = c("model", "bootstrap"), \dots)</pre>


<h3>Value</h3>

<p>For <code>bootstrap.mat</code> an object of class <code>"bootstrap.mat"</code> is
returned. This is a complex object with many components and is
described in <code><a href="#topic+bootstrapObject">bootstrapObject</a></code>.
</p>
<p>For <code>residuals</code>, a list containg the requested residuals and
metadata, with the following components:
</p>
<table role = "presentation">
<tr><td><code>model</code></td>
<td>
<p>Leave one out residuals for the MAT-estimated model.</p>
</td></tr>
<tr><td><code>bootstrap</code></td>
<td>
<p>residuals for the bootstrapped MAT model.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>numeric; indicating the size of model used in estimates and
predictions.</p>
</td></tr>
<tr><td><code>n.boot</code></td>
<td>
<p>numeric; the number of bootstrap samples taken.</p>
</td></tr>
<tr><td><code>auto</code></td>
<td>
<p>logical; whether <code>"k"</code> was choosen automatically or
user-selected.</p>
</td></tr>
<tr><td><code>weighted</code></td>
<td>
<p>logical; whether the weighted mean was used instead of
the mean of the environment for <em>k</em>-closest analogues.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Gavin L. Simpson </p>


<h3>References</h3>

<p>Birks, H.J.B., Line, J.M., Juggins, S., Stevenson, A.C. and ter Braak,
C.J.F. (1990). Diatoms and pH reconstruction. <em>Philosophical
Transactions of the Royal Society of London; Series B</em>, <strong>327</strong>;
263&ndash;278.
</p>


<h3>See Also</h3>

<p><code>mat</code>, <code>plot.mat</code>, <code>summary.bootstrap.mat</code>,
<code><a href="stats.html#topic+residuals">residuals</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Imbrie and Kipp example
## load the example data
data(ImbrieKipp)
data(SumSST)
data(V12.122)

## merge training and test set on columns
dat &lt;- join(ImbrieKipp, V12.122, verbose = TRUE)

## extract the merged data sets and convert to proportions
ImbrieKipp &lt;- dat[[1]] / 100
V12.122 &lt;- dat[[2]] / 100

## Imbrie and Kipp foraminfera sea-surface temperature 
## fit the MAT model using the squared chord distance measure
ik.mat &lt;- mat(ImbrieKipp, SumSST, method = "SQchord")

## bootstrap training set
## IGNORE_RDIFF_BEGIN
ik.boot &lt;- bootstrap(ik.mat, n.boot = 100)
ik.boot
summary(ik.boot)
## IGNORE_RDIFF_END

## Bootstrap fitted values for training set
## IGNORE_RDIFF_BEGIN
fitted(ik.boot)
## IGNORE_RDIFF_END

## residuals
resid(ik.boot) # uses abbreviated form

</code></pre>

<hr>
<h2 id='bootstrap.wa'>Bootstrap estimation and errors for WA models</h2><span id='topic+bootstrap.wa'></span><span id='topic+print.bootstrap.wa'></span>

<h3>Description</h3>

<p>Function to calculate bootstrap statistics for transfer function
models such as bootstrap estimates, model RMSEP, sample specific
errors for predictions and summary statistics such as bias and
<code class="reqn">R^2</code> between oberved and estimated environment.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'wa'
bootstrap(object, n.boot = 1000, verbose = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bootstrap.wa_+3A_object">object</code></td>
<td>
<p>an R object of class <code>"wa"</code> for which bootstrap
statistics are to be generated.</p>
</td></tr>
<tr><td><code id="bootstrap.wa_+3A_n.boot">n.boot</code></td>
<td>
<p>numeric; the number of bootstrap samples to draw.</p>
</td></tr>
<tr><td><code id="bootstrap.wa_+3A_verbose">verbose</code></td>
<td>
<p>logical; should bootstrap progress be printed to the
console?</p>
</td></tr>
<tr><td><code id="bootstrap.wa_+3A_...">...</code></td>
<td>
<p>arguments passed to other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <code><a href="#topic+bootstrap.mat">bootstrap.mat</a></code> for further details. This method is
not as feature packed as <code><a href="#topic+bootstrap.mat">bootstrap.mat</a></code> but can be used
to evaluate the model performance of WA transfer function models.
</p>


<h3>Value</h3>

<p>An object with the same components as <code><a href="#topic+predict.wa">predict.wa</a></code>.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson </p>


<h3>References</h3>

<p>Birks, H.J.B., Line, J.M., Juggins, S., Stevenson, A.C. and ter Braak,
C.J.F. (1990). Diatoms and pH reconstruction. <em>Philosophical
Transactions of the Royal Society of London; Series B</em>, <strong>327</strong>;
263&ndash;278.
</p>


<h3>See Also</h3>

<p><code>wa</code>, <code>plot.wa</code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Imbrie and Kipp
data(ImbrieKipp)
data(SumSST)
ik.wa &lt;- wa(SumSST ~ ., data = ImbrieKipp, tol.dw = TRUE,
            min.tol = 2, small.tol = "min")
ik.wa

## compare actual tolerances to working values
with(ik.wa, rbind(tolerances, model.tol))

## bootstrap the WA model
ik.boot &lt;- bootstrap(ik.wa, n.boot = 100)

## performance statistics
performance(ik.boot)
</code></pre>

<hr>
<h2 id='bootstrapObject'>Bootstrap object description</h2><span id='topic+bootstrapObject'></span>

<h3>Description</h3>

<p>Objects of class <code><a href="#topic+bootstrap.mat">bootstrap.mat</a></code> are a complex containing
many sub-components. This object is described here in more detail.
</p>


<h3>Details</h3>

<p>A large object is returned with some or all of the following depending
on whether <code>newdata</code> and <code>newenv</code> are supplied or not.
</p>

<dl>
<dt><code>observed</code>: </dt><dd><p>vector of observed environmental values.</p>
</dd>
<dt><code>model</code>: </dt><dd><p>a list containing the apparent or non-bootstrapped
estimates for the training set. With the following components:
</p>

<dl>
<dt><code>estimated</code>: </dt><dd><p>estimated values for the response</p>
</dd>
<dt><code>residuals</code>: </dt><dd><p>model residuals</p>
</dd>
<dt><code>r.squared</code>: </dt><dd><p>Apparent <code class="reqn">R^2</code> between observed and
estimated values  of response</p>
</dd>
<dt><code>avg.bias</code>: </dt><dd><p>Average bias of the model residuals</p>
</dd>
<dt><code>max.bias</code>: </dt><dd><p>Maximum bias of the model residuals</p>
</dd>
<dt><code>rmse</code>: </dt><dd><p>Apparent error (RMSE) for the model.</p>
</dd>
<dt><code>k</code>: </dt><dd><p>numeric; indicating the size of model used in
estimates and predictions</p>
</dd>
</dl>

</dd>
<dt><code>bootstrap</code>: </dt><dd><p>a list containing the bootstrap estimates for the
training set. With the following components:
</p>

<dl>
<dt><code>estimated</code>: </dt><dd><p>Bootstrap estimates for the response</p>
</dd>
<dt><code>residuals</code>: </dt><dd><p>Bootstrap residuals for the response</p>
</dd>
<dt><code>r.squared</code>: </dt><dd><p>Bootstrap derived <code class="reqn">R^2</code> between observed
and estimated values of the response</p>
</dd>
<dt><code>avg.bias</code>: </dt><dd><p>Average bias of the bootstrap derived model
residuals</p>
</dd>
<dt><code>max.bias</code>: </dt><dd><p>Maximum bias of the bootstrap derived model
residuals</p>
</dd>
<dt><code>rmsep</code>: </dt><dd><p>Bootstrap derived RMSEP for the model</p>
</dd>
<dt><code>s1</code>: </dt><dd><p>Bootstrap derived S1 error component for the
model</p>
</dd>
<dt><code>s2</code>: </dt><dd><p>Bootstrap derived S2 error component for the
model</p>
</dd> 
<dt><code>k</code>: </dt><dd><p>numeric; indicating the size of model used in
estimates and predictions</p>
</dd>
</dl>

</dd>
<dt><code>sample.errors</code>: </dt><dd><p>a list containing the bootstrap-derived sample
specific errors for the training set. With the following
components:
</p>

<dl>
<dt><code>rmsep</code>: </dt><dd><p>Bootstrap derived RMSEP for the training set
samples</p>
</dd> 
<dt><code>s1</code>: </dt><dd><p>Bootstrap derived S1 error component for training
set samples</p>
</dd>
<dt><code>s2</code>: </dt><dd><p>Bootstrap derived S2 error component for training
set samples</p>
</dd>
</dl>

</dd>
<dt><code>weighted</code>: </dt><dd><p>logical; whether the weighted mean was used instead of
the mean of the environment for <em>k</em>-closest analogues</p>
</dd>
<dt><code>auto</code>: </dt><dd><p>logical; whether <code>"k"</code> was choosen automatically or
user-selected</p>
</dd>
<dt><code>n.boot</code>: </dt><dd><p>numeric; the number of bootstrap samples taken</p>
</dd>
<dt><code>call</code>: </dt><dd><p>the matched call</p>
</dd>
<dt><code>type</code>: </dt><dd><p>model type</p>
</dd>
<dt><code>predictions</code>: </dt><dd><p>a list containing the apparent and
bootstrap-derived estimates for the new data, with the following
components:
</p>

<dl>
<dt><code>observed</code>: </dt><dd><p>the observed values for the new samples &mdash;
only if <code>newenv</code> is provided</p>
</dd>
<dt><code>model</code>: </dt><dd><p>a list containing the apparent or
non-bootstrapped estimates for the new samples. A list with the
same components as <code>model</code>, above</p>
</dd>
<dt><code>bootstrap</code>: </dt><dd><p>a list containing the bootstrap estimates
for the new samples, with some or all of the same components as
<code>bootstrap</code>, above</p>
</dd>
<dt><code>sample.errors</code>: </dt><dd><p>a list containing the bootstrap-derived
sample specific errors for the new samples, with some or all of
the same components as <code>sample.errors</code>, above</p>
</dd>
</dl>

</dd>
</dl>



<h3>Author(s)</h3>

<p>Gavin L. Simpson </p>


<h3>See Also</h3>

<p><code>mat</code>, <code>plot.mat</code>, <code>summary.bootstrap.mat</code>,
<code><a href="stats.html#topic+residuals">residuals</a></code></p>

<hr>
<h2 id='caterpillarPlot'>
Caterpillar plot of species' WA optima and tolerance range.
</h2><span id='topic+caterpillarPlot'></span><span id='topic+caterpillarPlot.default'></span><span id='topic+caterpillarPlot.data.frame'></span><span id='topic+caterpillarPlot.wa'></span><span id='topic+caterpillar'></span>

<h3>Description</h3>

<p>Draws a caterpillar plot of the weighted average optima and tolerance
range for each of the species in a data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default S3 method:
caterpillarPlot(x, tol, mult = 1, decreasing = TRUE,
                labels, xlab = NULL, pch = 21, bg = "white",
                col = "black", lcol = col, lwd = 2, frame.plot = FALSE, ...)

## S3 method for class 'data.frame'
caterpillarPlot(x, env, useN2 = TRUE, xlab, ...)

## S3 method for class 'wa'
caterpillarPlot(x, type = c("observed","model"), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="caterpillarPlot_+3A_x">x</code></td>
<td>
<p>For the <code>default</code> method, a numeric vector of species
optima. For the <code>data.frame</code> method a species data matrix or data
frame. For the <code>wa</code> method an object of class <code>"wa"</code>.</p>
</td></tr>
<tr><td><code id="caterpillarPlot_+3A_tol">tol</code></td>
<td>
<p>numeric; vector of species tolerances.</p>
</td></tr>
<tr><td><code id="caterpillarPlot_+3A_env">env</code></td>
<td>
<p>numeric; variable for which optima and tolerances are
required.</p>
</td></tr>
<tr><td><code id="caterpillarPlot_+3A_usen2">useN2</code></td>
<td>
<p>logical; should Hill's N2 values be used to produce
un-biased tolerances?</p>
</td></tr>
<tr><td><code id="caterpillarPlot_+3A_decreasing">decreasing</code></td>
<td>
<p>logical; should the sort order of the species be
increasing or decreasing?</p>
</td></tr>
<tr><td><code id="caterpillarPlot_+3A_mult">mult</code></td>
<td>
<p>numeric; multiplication factor for species' tolerances.</p>
</td></tr>
<tr><td><code id="caterpillarPlot_+3A_labels">labels</code></td>
<td>
<p>character; vector of labels for the species names with
which to annotate the y-axis. If missing, <code>names(x)</code> is used.</p>
</td></tr>
<tr><td><code id="caterpillarPlot_+3A_xlab">xlab</code></td>
<td>
<p>character; the x-axis label. If <code>NULL</code>, the default,
a description of <code>env</code> is used.</p>
</td></tr>
<tr><td><code id="caterpillarPlot_+3A_pch">pch</code>, <code id="caterpillarPlot_+3A_bg">bg</code>, <code id="caterpillarPlot_+3A_col">col</code></td>
<td>
<p>The plotting character to use and its background and
foreground colour. See <code><a href="graphics.html#topic+par">par</a></code>.</p>
</td></tr>
<tr><td><code id="caterpillarPlot_+3A_lcol">lcol</code>, <code id="caterpillarPlot_+3A_lwd">lwd</code></td>
<td>
<p>The colour and line width to use for the tolerance
range.</p>
</td></tr>
<tr><td><code id="caterpillarPlot_+3A_type">type</code></td>
<td>
<p>character; <code>"observed"</code> uses the actual tolerances
observed from the data. <code>"model"</code> uses the tolerances used in
the WA model where very small tolerances have been reset for some
definition of small.</p>
</td></tr>
<tr><td><code id="caterpillarPlot_+3A_frame.plot">frame.plot</code></td>
<td>
<p>logical; should a box be drawn round the plot?</p>
</td></tr>
<tr><td><code id="caterpillarPlot_+3A_...">...</code></td>
<td>
<p>Additional graphical arguments to be passed on to
plotting functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function may also be called using the short form name
<code>caterpillar</code>:
</p>
<pre>
caterpillar(x, ...)
  </pre>


<h3>Value</h3>

<p>The function results in a plot on the currently active device. A data
frame with components <code>Optima</code> and <code>Tolerance</code> is returned
invisibly.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson
</p>


<h3>See Also</h3>

<p>For the underlying computations <code><a href="#topic+optima">optima</a></code> and
<code><a href="vegan.html#topic+tolerance">tolerance</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ImbrieKipp)
data(SumSST)

## default plot
caterpillar(ImbrieKipp, SumSST)

## customisation
opttol &lt;-
    caterpillar(ImbrieKipp, SumSST, col = "red2",
                bg = "yellow", lcol = "blue",
                xlab = expression(Summer ~ Sea ~ Surface ~
                                 Temperature~(degree*C)))

## invisibly returns the optima and tolerances
head(opttol)
</code></pre>

<hr>
<h2 id='chooseTaxa'>Select taxa (variables) on basis of maximum abundance attained
and number of occurrences.</h2><span id='topic+chooseTaxa'></span><span id='topic+chooseTaxa.default'></span>

<h3>Description</h3>

<p>Select taxa (variables) from an object on the basis of one or both of
maximum abundance and number of occurrences greater than user-specified
values. This is a simple utility function to encapsulate this common
task in filtering palaeoecological data sets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chooseTaxa(object, ...)

## Default S3 method:
chooseTaxa(object, n.occ = 1, max.abun = 0,
           type = c("AND","OR"), value = TRUE, na.rm = FALSE,
           ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="chooseTaxa_+3A_object">object</code></td>
<td>
<p>an R object for which a suitable method exists. The
default method assumes a matrix-like object such as a data
frame or a numeric matrix.</p>
</td></tr>
<tr><td><code id="chooseTaxa_+3A_n.occ">n.occ</code></td>
<td>
<p>numeric; number of occurrences representing the lower
limit for selection. A taxon is included in the returned subset if
it is present a total of <code>n.occ</code> times or more. See argument
<code>type</code> for a modifier which might exclude the taxon even if it
would be included on the basis of <code>n.occ</code>.</p>
</td></tr>
<tr><td><code id="chooseTaxa_+3A_max.abun">max.abun</code></td>
<td>
<p>numeric; maximum abundance representing the lower
limit for selection. A taxon is included in the returned subset if
it attains abundance equal to or greater than <code>max.abun</code> in one
or more sample. See argument <code>type</code> for a modifier which might
exclude the taxon even if it would be included on the basis
of <code>max.abun</code>.</p>
</td></tr>
<tr><td><code id="chooseTaxa_+3A_type">type</code></td>
<td>
<p>character; one of <code>"AND"</code> or <code>"OR"</code>, controlling
how the criteria <code>n.occ</code> and <code>max.abun</code> are combined to
generate a subset of the variables in <code>object</code>.</p>
</td></tr>
<tr><td><code id="chooseTaxa_+3A_value">value</code></td>
<td>
<p>logical; should the data for the selected taxa be
returned? If <code>TRUE</code>, the default, the data for the chosen taxa
are returned. If <code>FALSE</code>, a logical vector is returned,
indicating which taxa met the selection criteria.</p>
</td></tr>
<tr><td><code id="chooseTaxa_+3A_na.rm">na.rm</code></td>
<td>
<p>logical; should missing values <code>NA</code>s be excluded
from the calculation of abundances and occurrence?</p>
</td></tr>
<tr><td><code id="chooseTaxa_+3A_...">...</code></td>
<td>
<p>arguments passed on to subsequent methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If <code>value = TRUE</code>,  returns the supplied data frame or matrix
with a subset of columns (taxa) that meet the criteria chosen. If
<code>value = FALSE</code>, a logical vector is returned.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ImbrieKipp)
IK2 &lt;- chooseTaxa(ImbrieKipp, n.occ = 5)
dim(ImbrieKipp)
dim(IK2)

## return a logical vector to select species/columns
chooseTaxa(ImbrieKipp, n.occ = 5, value = FALSE)
</code></pre>

<hr>
<h2 id='cma'>Close modern analogues</h2><span id='topic+cma'></span><span id='topic+cma.default'></span><span id='topic+cma.analog'></span><span id='topic+cma.mat'></span><span id='topic+cma.predict.mat'></span><span id='topic+print.cma'></span><span id='topic+plot.cma'></span>

<h3>Description</h3>

<p>Extracts and formats close modern analogue samples from a modern
reference set that are closer than a defined cut off threshold.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cma(object, ...)

## Default S3 method:
cma(object, ...)

## S3 method for class 'analog'
cma(object, cutoff, prob = c(0.01, 0.025, 0.05), ...)

## S3 method for class 'mat'
cma(object, k, cutoff, prob = c(0.01, 0.025, 0.05), ...)

## S3 method for class 'predict.mat'
cma(object, k, cutoff, prob = c(0.01, 0.025,
0.05), ...)

## S3 method for class 'cma'
plot(x, method = c("overplot", "jitter", "stack"),
   jitter = 0.1, vertical = FALSE,
   draw.quant = TRUE, xlab = NULL, ylab = "",
   main = "", cex.axis = NULL, ...,
   col.quant = "red", lty.quant= "dashed")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cma_+3A_object">object</code></td>
<td>
<p>an object for which close modern analogues are to be
returned. Currently only for objects of class <code><a href="#topic+analog">analog</a></code>.</p>
</td></tr>
<tr><td><code id="cma_+3A_k">k</code></td>
<td>
<p>numeric; the number of analogues to return.</p>
</td></tr>
<tr><td><code id="cma_+3A_cutoff">cutoff</code></td>
<td>
<p>numeric; critical value determining level below which
samples from the modern reference set are defined as close modern
analogues. May be missing, in which case the 2.5% quantile of the
training set dissimilarities is used unless <code>object$train</code> is
<code>NULL</code>, in which case <code>"cutoff"</code> must be supplied.</p>
</td></tr>
<tr><td><code id="cma_+3A_prob">prob</code></td>
<td>
<p>numeric vector of probabilities with values in [0,1], for
which quantiles of the distribution of training set dissimilarities
will be calculated. See <code><a href="stats.html#topic+quantile">quantile</a></code>.</p>
</td></tr>
<tr><td><code id="cma_+3A_...">...</code></td>
<td>
<p>arguments to be passed to other <code><a href="#topic+cma">cma</a></code>
methods or additional arguments passed to <code><a href="graphics.html#topic+stripchart">stripchart</a></code>.</p>
</td></tr>
<tr><td><code id="cma_+3A_x">x</code></td>
<td>
<p>an object of class <code>"cma"</code>.</p>
</td></tr>
<tr><td><code id="cma_+3A_method">method</code></td>
<td>
<p>the method to be used to separate coincident points. The
default method<br /><code>"overplot"</code> causes such points to be
overplotted, but it is also possible to specify <code>"jitter"</code> to 
jitter the points, or <code>"stack"</code> have coincident points
stacked.  The last method only makes sense for very granular data.</p>
</td></tr>
<tr><td><code id="cma_+3A_jitter">jitter</code></td>
<td>
<p>when <code>method="jitter"</code> is used, <code>jitter</code> gives
the amount of jittering applied.</p>
</td></tr>
<tr><td><code id="cma_+3A_vertical">vertical</code></td>
<td>
<p>when vertical is <code>TRUE</code> the plots are drawn
vertically rather than the default horizontal.</p>
</td></tr>
<tr><td><code id="cma_+3A_draw.quant">draw.quant</code></td>
<td>
<p>logical; should the quantiles be drawn on the stripchart?</p>
</td></tr>
<tr><td><code id="cma_+3A_xlab">xlab</code>, <code id="cma_+3A_ylab">ylab</code>, <code id="cma_+3A_main">main</code></td>
<td>
<p>Graphical parameters</p>
</td></tr>
<tr><td><code id="cma_+3A_cex.axis">cex.axis</code></td>
<td>
<p>The magnification to be used for axis annotation
relative to the current setting of <code>cex</code>. See
<code><a href="graphics.html#topic+par">par</a></code>.</p>
</td></tr>
<tr><td><code id="cma_+3A_col.quant">col.quant</code>, <code id="cma_+3A_lty.quant">lty.quant</code></td>
<td>
<p>colour and line type in which to drawn the
quantile lines.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The plot method is simply a wrapper to <code><a href="graphics.html#topic+stripchart">stripchart</a></code>.
</p>
<p>The methods for <code>mat</code> and <code>predict.mat</code> objects allow the
user to select the k-closest analogues (argument <code>k</code>) or those
samples as close or closer than a stated threshold of dissimilarity
(argument <code>cutoff</code>). Only one of <code>k</code> and <code>cutoff</code> may
be specified. If neither is specified, <code><a href="#topic+getK">getK</a></code> is used to
extract the value for <code>k</code> stored within <code>object</code>. As such,
the default is to return the automatically selected set of <code>k</code>
closest samples, behaviour that is consistent with other functions in
the package.
</p>


<h3>Value</h3>

<p>For the plot method, a plot on the current device. Invisibly the
plotted data are returned; see Note for further details.
</p>
<p>A list of class <code>"cma"</code> with the following components:
</p>
<table role = "presentation">
<tr><td><code>close</code></td>
<td>
<p>a named list of named vectors of close modern analogues
and their dissimilarities. The names of the list components are the
names of the fossil samples. The named vector in each
component of <code>close</code> is the distances for the close modern
analogues from the training set that are as close as <code>cutoff</code>,
or closer, to the fossil sample.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>cutoff</code></td>
<td>
<p>the cutoff threshold used to define close modern
analogues.</p>
</td></tr>
<tr><td><code>quant</code></td>
<td>
<p>numeric vector of the requested quantiles. Note returned
by the <code><a href="#topic+predict.mat">predict.mat</a></code> method.</p>
</td></tr>
<tr><td><code>probs</code></td>
<td>
<p>the probabilities of the requested quantiles.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>character; the dissimilarity coefficient used</p>
</td></tr>
<tr><td><code>n.analogs</code></td>
<td>
<p>numeric vector of the number of analogues per fossil
sample.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Only objects of classes <code><a href="#topic+analog">analog</a></code>, <code><a href="#topic+mat">mat</a></code>, and
<code><a href="#topic+predict.mat">predict.mat</a></code> are supported.
</p>
<p>The plot method invisibly returns a list with the following
components:
</p>

<dl>
<dt><code>distances</code></dt><dd><p>a vector of stacked distances extracted from
<code>object</code>.</p>
</dd>
<dt><code>groups</code></dt><dd><p>a factor listing the fossil sample for which the
distances are the distances to the close modern analogues for the
training set.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>References</h3>

<p>Flower, R.J., Juggins, S. and Battarbee, R.W. (1997) Matching diatom
assemblages in lake sediment cores and modern surface sediment
samples: the implications for lake conservation and restoration with
special reference to acidified systems. <em>Hydrobiologia</em>
<strong>344</strong>; 27&ndash;40.
</p>
<p>Simpson, G.L., Shilland, E.M., Winterbottom, J. M. and Keay, J. (2005)
Defining reference conditions for acidified waters using a modern
analogue approach. <em>Environmental Pollution</em> <strong>137</strong>;
119&ndash;133.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+analog">analog</a></code>, <code><a href="graphics.html#topic+stripchart">stripchart</a></code>, or
<code><a href="graphics.html#topic+boxplot">boxplot</a></code> for an alternative representation.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Imbrie and Kipp example
## load the example data
data(ImbrieKipp)
data(SumSST)
data(V12.122)

## merge training and test set on columns
dat &lt;- join(ImbrieKipp, V12.122, verbose = TRUE)

## extract the merged data sets and convert to proportions
ImbrieKipp &lt;- dat[[1]] / 100
V12.122 &lt;- dat[[2]] / 100

## analog matching between SWAP and RLGH reference samples
(ik.ana &lt;- analog(ImbrieKipp, V12.122, method = "chord"))

## close modern analogues
(ik.cma &lt;- cma(ik.ana, cutoff = 0.4))
summary(ik.cma)

## plot the results
plot(ik.cma)

</code></pre>

<hr>
<h2 id='compare'>Compare proxies across two data sets</h2><span id='topic+compare'></span><span id='topic+compare.default'></span>

<h3>Description</h3>

<p><code>compare()</code> compares a proxy dataset with a training set or other
data set that is considered to be the master. A range of metrics is
returned, currently for samples only.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compare(x, ...)

## Default S3 method:
compare(x, y, env,
        by = c("sites", "species"),
        ordination = "rda",
        method = "chord",
        transform = NULL,
        n2limit = 5L,
        ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="compare_+3A_x">x</code></td>
<td>
<p>data frame; training set samples to compare against</p>
</td></tr>
<tr><td><code id="compare_+3A_y">y</code></td>
<td>
<p>data frame; passive or core samples</p>
</td></tr>
<tr><td><code id="compare_+3A_env">env</code></td>
<td>
<p>numeric vector of environmental or contraint data for
residual length ordination. Ignored if <code>by = "species"</code>.</p>
</td></tr>
<tr><td><code id="compare_+3A_by">by</code></td>
<td>
<p>character; compare data sets by sites or species
(proxies).</p>
</td></tr>
<tr><td><code id="compare_+3A_ordination">ordination</code></td>
<td>
<p>character; which constrained ordination method to
use</p>
</td></tr>
<tr><td><code id="compare_+3A_method">method</code></td>
<td>
<p>character; which dissimilarity method to use. See
<code>distance</code>.</p>
</td></tr>
<tr><td><code id="compare_+3A_transform">transform</code></td>
<td>
<p>character: should a transformation be applied to the
data. Ignored.</p>
</td></tr>
<tr><td><code id="compare_+3A_n2limit">n2limit</code></td>
<td>
<p>integer; the criterion for indicating species with
potentially poorly estimated optima. The default value of <code>5L</code>
is one suggested by R. Telford.</p>
</td></tr>
<tr><td><code id="compare_+3A_...">...</code></td>
<td>
<p>arguments passed to other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>ToDo
</p>


<h3>Value</h3>

<p>If <code>by = "species"</code> a data frame of diagnostics for each species
(proxy) in <code>y</code> relative to <code>x</code>. If <code>by = "sites"</code>, the
diagnostics are for each sample (row) in <code>y</code>. Depending on the
value of <code>by</code> some of the following columns will be returned
</p>
<table role = "presentation">
<tr><td><code>sumMissing</code></td>
<td>
<p>numeric; abundance sum for species missing
from the training set <code>x</code>.</p>
</td></tr>
<tr><td><code>sumPoorOpt</code></td>
<td>
<p>numeric; abundance sum for species with
potentially poorly estimated optima.</p>
</td></tr>
<tr><td><code>closestSamp</code></td>
<td>
<p>numeric; minimum dissimilarity to a sample
in the training data <code>x</code>.</p>
</td></tr>
<tr><td><code>residLen</code></td>
<td>
<p>numeric; the squared residual length for each
sample in <code>y</code>. A measure of how well the sample fits into the
species-environment relationship of a constrained ordination. See
<code><a href="#topic+residLen">residLen</a></code> for details. Not given if <code>env</code> is not
provided.</p>
</td></tr>
<tr><td><code>inTrain</code></td>
<td>
<p>logical; simple indicator of whether a species
in <code>y</code> is present in the training data <code>x</code>.</p>
</td></tr>
<tr><td><code>n2</code></td>
<td>
<p>numeric; Hill's N2 for each species in <code>y</code>.</p>
</td></tr>
<tr><td><code>n2Train</code></td>
<td>
<p>numeric; as for <code>n2</code> but computed from the
training data <code>x</code>.</p>
</td></tr>
<tr><td><code>max</code></td>
<td>
<p>numeric; the maximum abundance of each species
computed using <code>y</code>.</p>
</td></tr>
<tr><td><code>maxTrain</code></td>
<td>
<p>numeric; as for <code>max</code> but computed using
the training data <code>x</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ImbrieKipp, V12.122, SumSST)
compare(ImbrieKipp, V12.122, env = SumSST, ordination = "rda",
        method = "chord")
</code></pre>

<hr>
<h2 id='crossval'>Cross-validation of palaeoecological transfer function models</h2><span id='topic+crossval'></span><span id='topic+crossval.wa'></span><span id='topic+crossval.pcr'></span><span id='topic+print.crossval'></span>

<h3>Description</h3>

<p>Performs leave-one-out, <em>k</em>-fold, <em>n</em> <em>k</em>-fold and
bootstrap cross-validation of palaeoecological transfer function models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crossval(obj, ...)

## S3 method for class 'wa'
crossval(obj, method = c("LOO","kfold","bootstrap"),
         nboot = 100, nfold = 10, folds = 5,
         verbose = getOption("verbose"), ...)

## S3 method for class 'pcr'
crossval(obj, method = c("LOO","kfold","bootstrap"),
         ncomp, nboot = 100, nfold = 10, folds = 5,
         verbose = getOption("verbose"), ...)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="crossval_+3A_obj">obj</code></td>
<td>
<p>A fitted transfer function model. Currently, only objects
of class <code><a href="#topic+wa">wa</a></code> and <code><a href="#topic+pcr">pcr</a></code> are supported.</p>
</td></tr>
<tr><td><code id="crossval_+3A_method">method</code></td>
<td>
<p>character; type of cross-validation.</p>
</td></tr>
<tr><td><code id="crossval_+3A_ncomp">ncomp</code></td>
<td>
<p>numeric; number of components to fit, as in models with
<code>1:ncomp</code> components.</p>
</td></tr>
<tr><td><code id="crossval_+3A_nboot">nboot</code></td>
<td>
<p>numeric; number of bootstrap samples.</p>
</td></tr>
<tr><td><code id="crossval_+3A_nfold">nfold</code></td>
<td>
<p>numeric; number of chunks into which the training data
are split. The <em>k</em> in <em>k</em>-fold.</p>
</td></tr>
<tr><td><code id="crossval_+3A_folds">folds</code></td>
<td>
<p>numeric; the number of times <em>k</em>-fold CV is
performed.</p>
</td></tr>
<tr><td><code id="crossval_+3A_verbose">verbose</code></td>
<td>
<p>logical; should progress of the CV be displayed?</p>
</td></tr>
<tr><td><code id="crossval_+3A_...">...</code></td>
<td>
<p>Arguments passed to other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an object of class <code>"crossval"</code>, a list with the
following components:
</p>
<table role = "presentation">
<tr><td><code>fitted.values</code></td>
<td>
<p>numeric vector; the cross-validated estimates of
the response.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>numeric vector; residuals computed from the
cross-validated estimates of the response.</p>
</td></tr>
<tr><td><code>performance</code></td>
<td>
<p>data frame; cross-validation performance statistics
for the model.</p>
</td></tr>
<tr><td><code>CVparams</code></td>
<td>
<p>list; parameters holding details of the
cross-validation process.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>See Also</h3>

<p><code><a href="#topic+wa">wa</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Load the Imbrie &amp; Kipp data and
## summer sea-surface temperatures
data(ImbrieKipp)
data(SumSST)
     
## fit the WA model
mod &lt;- wa(SumSST ~., data = ImbrieKipp)
mod

## Leave one out CV
cv.loo &lt;- crossval(mod)
cv.loo

## k-fold CV (k == 10)
cv.kfold &lt;- crossval(mod, method = "kfold", kfold = 10, folds = 1)
cv.kfold

## n k-fold CV (k == 10, n = 10)
cv.nkfold &lt;- crossval(mod, method = "kfold", kfold = 10, folds = 10)
cv.nkfold

## bootstrap with 100 bootstrap samples
cv.boot &lt;- crossval(mod, method = "bootstrap", nboot = 100)
cv.boot

## extract fitted values and residuals
fitted(cv.boot)
resid(cv.boot)

## Principal Components Regression
mpcr &lt;- pcr(SumSST ~., data = ImbrieKipp, ncomp = 10)
crossval(mpcr, method = "kfold", kfold = 10, folds = 2, ncomp = 10)

crossval(mpcr, method = "bootstrap", nboot = 100, ncomp = 10)
</code></pre>

<hr>
<h2 id='densityplot.residLen'>Lattice density plot for residual lengths</h2><span id='topic+densityplot.residLen'></span><span id='topic+densityplot'></span>

<h3>Description</h3>

<p>Lattice <code><a href="lattice.html#topic+densityplot">densityplot</a></code> method for
<code><a href="#topic+residLen">residLen</a></code> objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'residLen'
densityplot(x, ..., xlab = NULL, ylab = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="densityplot.residLen_+3A_x">x</code></td>
<td>
<p>Object of class <code>"residLen"</code>, the result of a call to
<code><a href="#topic+residLen">residLen</a></code>.</p>
</td></tr>
<tr><td><code id="densityplot.residLen_+3A_xlab">xlab</code>, <code id="densityplot.residLen_+3A_ylab">ylab</code></td>
<td>
<p>Axis labels. If not supplied, suitable defaults are
generated, depending on whether RDA or CCA was used as the
underlying ordination model.</p>
</td></tr>
<tr><td><code id="densityplot.residLen_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to
<code><a href="lattice.html#topic+densityplot">densityplot</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an object of class <code>"trellis"</code>. See
<code><a href="lattice.html#topic+densityplot">densityplot</a></code> for details.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>See Also</h3>

<p><code><a href="#topic+residLen">residLen</a></code>, <code><a href="#topic+plot.residLen">plot.residLen</a></code>,
<code><a href="#topic+hist.residLen">hist.residLen</a></code>, <code><a href="#topic+histogram.residLen">histogram.residLen</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load the Imbrie and Kipp example data
data(ImbrieKipp, SumSST, V12.122)

## squared residual lengths for Core V12.122
rlens &lt;- residLen(ImbrieKipp, SumSST, V12.122)
rlens

## plot the density functions of the residual distances
densityplot(rlens)

</code></pre>

<hr>
<h2 id='deshrink'>
Deshrinking techniques for WA transfer functions
</h2><span id='topic+deshrink'></span><span id='topic+deshrinkPred'></span>

<h3>Description</h3>

<p>In Weighted Averaging models averages are taken twice and thus WA
estimates shrink towards the training set mean and need to be
deshrunk.<code>deshrink</code> performs this deshrinking using several
techniques, whilst <code>deshrinkPred</code> will deshrink WA estimates for
new samples given a set of deshrinking coefficients.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>deshrink(env, wa.env,
         type = c("inverse", "classical", "expanded", "none",
                  "monotonic"))

deshrinkPred(x, coef,
         type = c("inverse", "classical", "expanded", "none",
                  "monotonic"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="deshrink_+3A_env">env</code></td>
<td>
<p>numeric; original environmental values.</p>
</td></tr>
<tr><td><code id="deshrink_+3A_wa.env">wa.env</code></td>
<td>
<p>numeric; initial weighted average estimates.</p>
</td></tr>
<tr><td><code id="deshrink_+3A_type">type</code></td>
<td>
<p>character; the type of deshrinking. One of
<code>"inverse"</code>, <code>"classical"</code>, <code>"expand"</code>,
<code>"none"</code>.</p>
</td></tr>
<tr><td><code id="deshrink_+3A_x">x</code></td>
<td>
<p>numeric; estimates to be deshrunk.</p>
</td></tr>
<tr><td><code id="deshrink_+3A_coef">coef</code></td>
<td>
<p>numeric; deshrinking coefficients to use. Currently needs
to be a vector of length 2. These should be supplied in the order
<code class="reqn">\beta_0,\beta_1</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>deshrinkPred</code> a numeric vector of deshrunk estimates.
</p>
<p>For an object of class <code>"deshrink"</code>, inheriting from class
<code>"list"</code>, with two components. The type of deshrinking performed
is stroed within attribute <code>"type"</code>. The componets of the
returned object are:
</p>
<table role = "presentation">
<tr><td><code>coefficients</code></td>
<td>
<p>The deshrinking coefficients used.</p>
</td></tr>
<tr><td><code>env</code></td>
<td>
<p>The deshrunk WA estimates.</p>
</td></tr>
</table>


<h3>Warning </h3>

<p><code>deshrinkPred</code>, does not currently check that
the correct coefficients have been supplied in the correct order.</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson &amp; Jari Oksanen
</p>


<h3>References</h3>

<p>Birks, H.J.B. (1995) Quantitative environmental reconstructions. In
<em>Statistical modelling of Quaternary science data</em> (eds.~D.Maddy
&amp; J.S. Brew). Quaternary Research Association technical guide
5. Quaternary Research Association, Cambridge.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+wa">wa</a></code>
</p>

<hr>
<h2 id='dissimilarities'>Extract dissimilarity coefficients from models </h2><span id='topic+dissimilarities'></span><span id='topic+dissimilarities.analog'></span><span id='topic+dissimilarities.mat'></span><span id='topic+dissim'></span>

<h3>Description</h3>

<p>Extracts a vector of dissimilarity coefficients from an object for
further analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
dissimilarities(object, ...)
dissim(object, ...)

## S3 method for class 'analog'
dissimilarities(object, which = c("train", "analogs"),
                 ...)

## S3 method for class 'mat'
dissimilarities(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dissimilarities_+3A_object">object</code></td>
<td>
<p>an R object from which the dissimilarity values are to
be extracted. Currently only for objects of class <code>"analog"</code>.</p>
</td></tr>
<tr><td><code id="dissimilarities_+3A_which">which</code></td>
<td>
<p>character; which set of dissimilarities should be
extracted. One of <code>"train"</code> or <code>"analogs"</code>.</p>
</td></tr>
<tr><td><code id="dissimilarities_+3A_...">...</code></td>
<td>
<p>arguments passed to other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function can be called using the much shorter name
<code>"dissim"</code>.
</p>


<h3>Value</h3>

<p>A vector of dissimilarities.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>See Also</h3>

<p><code><a href="#topic+analog">analog</a></code>, <code><a href="#topic+plot.dissimilarities">plot.dissimilarities</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## Imbrie and Kipp example
## load the example data
data(ImbrieKipp)
data(SumSST)
data(V12.122)

## merge training and test set on columns
dat &lt;- join(ImbrieKipp, V12.122, verbose = TRUE)

## extract the merged data sets and convert to proportions
ImbrieKipp &lt;- dat[[1]] / 100
V12.122 &lt;- dat[[2]] / 100

## analog matching between SWAPImbrie &amp; Kipp and V12.122 core
ik.analog &lt;- analog(ImbrieKipp, V12.122, method = "chord")
ik.analog
summary(ik.analog)

## compare training set dissimilarities with normals
## and derive cut-offs
ik.dij &lt;- dissim(ik.analog)
plot(ik.dij)

</code></pre>

<hr>
<h2 id='distance'>Flexibly calculate dissimilarity or distance measures</h2><span id='topic+distance'></span><span id='topic+distance.default'></span><span id='topic+distance.join'></span><span id='topic+oldDistance'></span><span id='topic+oldDistance.default'></span><span id='topic+oldDistance.join'></span>

<h3>Description</h3>

<p>Flexibly calculates distance or dissimilarity measures between a
training set <code>x</code> and a fossil or test set <code>y</code>. If
<code>y</code> is not supplied then the pairwise dissimilarities between
samples in the training set, <code>x</code>, are calculated.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
distance(x, ...)

## Default S3 method:
distance(x, y, method = "euclidean", weights = NULL,
         R = NULL, dist = FALSE, double.zero = FALSE, ...)

## S3 method for class 'join'
distance(x, ...)

oldDistance(x, ...)
## Default S3 method:
oldDistance(x, y, method = c("euclidean", "SQeuclidean",
            "chord", "SQchord", "bray", "chi.square",
            "SQchi.square", "information", "chi.distance",
            "manhattan", "kendall", "gower", "alt.gower",
            "mixed"),
            fast = TRUE,
            weights = NULL, R = NULL, ...)
## S3 method for class 'join'
oldDistance(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="distance_+3A_x">x</code></td>
<td>
<p>data frame or matrix containing the training set samples, or
and object of class <code><a href="#topic+join">join</a></code>.</p>
</td></tr>
<tr><td><code id="distance_+3A_y">y</code></td>
<td>
<p>data frame or matrix containing the fossil or test set
samples.</p>
</td></tr>
<tr><td><code id="distance_+3A_method">method</code></td>
<td>
<p>character; which choice of dissimilarity coefficient to
use. One of the listed options. See Details below.</p>
</td></tr>
<tr><td><code id="distance_+3A_weights">weights</code></td>
<td>
<p>numeric; vector of weights for each descriptor.</p>
</td></tr>
<tr><td><code id="distance_+3A_r">R</code></td>
<td>
<p>numeric; vector of ranges for each descriptor.</p>
</td></tr>
<tr><td><code id="distance_+3A_dist">dist</code></td>
<td>
<p>logical; should the dissimilarity matrix be returned as
an object of class <code>"dist"</code>? Ignored if <code>y</code> is supplied.</p>
</td></tr>
<tr><td><code id="distance_+3A_double.zero">double.zero</code></td>
<td>
<p>logical; if <code>FALSE</code>, the default, double
zeroes are not counted in the distance calculation. If <code>TRUE</code>,
absences of a variable in both samples counts as a similarity
between the two samples. Currently this only affects methods
<code>"mixed"</code> and <code>"metric.mixed"</code> forms of Gower's general
coefficient.</p>
</td></tr>
<tr><td><code id="distance_+3A_fast">fast</code></td>
<td>
<p>logical; should fast versions of the dissimilarities be
calculated? See details below.</p>
</td></tr>
<tr><td><code id="distance_+3A_...">...</code></td>
<td>
<p>arguments passed to other methods</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A range of dissimilarity coefficients can be used to calculate
dissimilarity between samples. The following are currently available:
</p>

<table>
<tr>
 <td style="text-align: left;">
    <code>euclidean</code>
    </td><td style="text-align: left;"> <code class="reqn">d_{jk} = \sqrt{\sum_i (x_{ij}-x_{ik})^2}</code>
    </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>SQeuclidean</code>
    </td><td style="text-align: left;"> <code class="reqn">d_{jk} = \sum_i (x_{ij}-x_{ik})^2</code>
    </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>chord</code>
    </td><td style="text-align: left;"> <code class="reqn">d_{jk} = \sqrt{\sum_i
	(\sqrt{x_{ij}}-\sqrt{x_{ik}})^2}</code>
    </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>SQchord</code>
    </td><td style="text-align: left;"> <code class="reqn">d_{jk} = \sum_i (\sqrt{x_{ij}}-\sqrt{x_{ik}})^2</code>
    </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>bray</code>
    </td><td style="text-align: left;"> <code class="reqn">d_{jk} = \frac{\sum_i |x_{ij} - x_{ik}|}{\sum_i (x_{ij} +
	x_{ik})}</code>
    </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>chi.square</code>
    </td><td style="text-align: left;"> <code class="reqn">d_{jk} = \sqrt{\sum_i \frac{(x_{ij} - x_{ik})^2}{x_{ij} +
	x_{ik}}}</code>
    </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>SQchi.square</code>
    </td><td style="text-align: left;"> <code class="reqn">d_{jk} = \sum_i \frac{(x_{ij} - x_{ik})^2}{x_{ij} +
	x_{ik}}</code>
    </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>information</code>
    </td><td style="text-align: left;"> <code class="reqn">d_{jk} = \sum_i (p_{ij}log(\frac{2p_{ij}}{p_{ij} + p_{ik}})
      + p_{ik}log(\frac{2p_{ik}}{p_{ij} + p_{ik}}))</code>
    </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>chi.distance</code>
    </td><td style="text-align: left;"> <code class="reqn">d_{jk} = \sqrt{\sum_i (x_{ij}-x_{ik})^2 / (x_{i+} /
	x_{++})}</code>
    </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>manhattan</code>
    </td><td style="text-align: left;"> <code class="reqn">d_{jk} = \sum_i (|x_{ij}-x_{ik}|)</code>
    </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>kendall</code>
    </td><td style="text-align: left;"> <code class="reqn">d_{jk} = \sum_i MAX_i - minimum(x_{ij}, x_{ik})</code>
    </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>gower</code>
    </td><td style="text-align: left;"> <code class="reqn">d_{jk} = \sum_i\frac{|p_{ij} -
	  p_{ik}|}{R_i}</code>
    </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>alt.gower</code>
    </td><td style="text-align: left;"> <code class="reqn">d_{jk} = \sqrt{2\sum_i\frac{|p_{ij} -
	  p_{ik}|}{R_i}}</code>
    </td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;"> where <code class="reqn">R_i</code> is the range of proportions for
    descriptor (variable) <code class="reqn">i</code>
    </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>mixed</code>
    </td><td style="text-align: left;"> <code class="reqn">d_{jk} = \frac{\sum_{i=1}^p w_{i}s_{jki}}{\sum_{i=1}^p
	w_{i}}</code>
    </td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;"> where <code class="reqn">w_i</code> is the weight for descriptor <code class="reqn">i</code> and
    <code class="reqn">s_{jki}</code> is the similarity </td>
</tr>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;"> between samples <code class="reqn">j</code> and <code class="reqn">k</code> for descriptor (variable)
    <code class="reqn">i</code>.
    </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code>metric.mixed</code>
    </td><td style="text-align: left;"> as for <code>mixed</code> but with ordinal variables converted to
    ranks and handled as quantitative variables in Gower's mixed
    coefficient.
  </td>
</tr>

</table>

<p>Argument <code>fast</code> determines whether fast C versions of some of the
dissimilarity coefficients are used. The fast versions make use of
<code><a href="stats.html#topic+dist">dist</a></code> for <code>method</code>s <code>"euclidean"</code>,
<code>"SQeuclidean"</code>, <code>"chord"</code>, <code>"SQchord"</code>, and
<code><a href="vegan.html#topic+vegdist">vegdist</a></code> for <code>method</code> == <code>"bray"</code>. These
fast versions are used only when <code>x</code> is supplied, not when
<code>y</code> is also supplied. Future versions of <code>distance</code> will
include fast C versions of all the dissimilary coefficients and for
cases where <code>y</code> is supplied.
</p>


<h3>Value</h3>

<p>A matrix of dissimilarities where columns are the samples in
<code>y</code> and the rows the samples in <code>x</code>. If <code>y</code> is
not provided then a square, symmetric matrix of pairwise sample
dissimilarities for the training set <code>x</code> is returned, unless
argument <code>dist</code> is <code>TRUE</code>, in which case an object of class
<code>"dist"</code> is returned. See <code><a href="stats.html#topic+dist">dist</a></code>.
</p>
<p>The dissimilarity coefficient used (<code>method</code>) is returned as
attribute <code>"method"</code>. Attribute <code>"type"</code> indicates whether
the object was computed on a single data matrix (<code>"symmetric"</code>)
or across two matrices (i.e. the dissimilarties between the rows of
two matrices; <code>"asymmetric"</code>.
</p>


<h3>Warning</h3>

<p>For <code>method = "mixed"</code> it is essential that a factor in <code>x</code>
and <code>y</code> have the same levels in the two data frames. Previous
versions of analogue would work even if this was not the case, which
will have generated incorrect dissimilarities for <code>method =
  "mixed"</code> for cases where factors for a given species had different
levels in <code>x</code> to <code>y</code>. 
</p>
<p><code>distance</code> now checks for matching levels for each species
(column) recorded as a factor. If the factor for any individual
species has different levels in <code>x</code> and <code>y</code>, an error will
be issued.
</p>


<h3>Note</h3>

<p>The dissimilarities are calculated in native R code. As such, other
implementations (see See Also below) will be quicker. This is done for
one main reason - it is hoped to allow a user defined function to be
supplied as argument <code>"method"</code> to allow for user-extension of
the available coefficients.
</p>
<p>The other advantage of <code>distance</code> over other implementations, is
the simplicity of calculating only the required pairwise sample
dissimilarities between each fossil sample (<code>y</code>) and each
training set sample (<code>x</code>). To do this in other implementations,
you would need to merge the two sets of samples, calculate the full
dissimilarity matrix and then subset it to achieve similar results.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson and Jari Oksanen (improvements leading to
method <code>"metric.mixed"</code> and proper handling of ordinal data via
Podani's (1999) modification of Gower's general coefficient in method
<code>"mixed"</code>).</p>


<h3>References</h3>

<p>Faith, D.P., Minchin, P.R. and Belbin, L. (1987) Compositional
dissimilarity as a robust measure of ecological
distance. <em>Vegetatio</em> <strong>69</strong>, 57&ndash;68.
</p>
<p>Gavin, D.G., Oswald, W.W., Wahl, E.R. and Williams, J.W. (2003) A
statistical approach to evaluating distance metrics and analog
assignments for pollen records. <em>Quaternary Research</em>
<strong>60</strong>, 356&ndash;367.
</p>
<p>Kendall, D.G. (1970) A mathematical approach to
seriation. <em>Philosophical Transactions of the Royal Society of
London - Series B</em> <strong>269</strong>, 125&ndash;135.
</p>
<p>Legendre, P. and Legendre, L. (1998) <em>Numerical Ecology</em>, 2nd
English Edition. Elsevier Science BV, The Netherlands.
</p>
<p>Overpeck, J.T., Webb III, T. and Prentice I.C. (1985) Quantitative
interpretation of fossil pollen spectra: dissimilarity coefficients and
the method of modern analogues. <em>Quaternary Research</em> <strong>23</strong>,
87&ndash;108.
</p>
<p>Podani, J. (1999) Extending Gower's General Coefficient of Similarity
to Ordinal Characters. <em>Taxon</em> <strong>48</strong>, 331&ndash;340).
</p>
<p>Prentice, I.C. (1980) Multidimensional scaling as a research tool in
Quaternary palynology: a review of theory and methods. <em>Review of
Palaeobiology and Palynology</em> <strong>31</strong>, 71&ndash;104.
</p>


<h3>See Also</h3>

<p><code><a href="vegan.html#topic+vegdist">vegdist</a></code> in package <span class="pkg">vegan</span>,
<code><a href="cluster.html#topic+daisy">daisy</a></code> in package <span class="pkg">cluster</span>, and
<code><a href="stats.html#topic+dist">dist</a></code> provide comparable functionality for the
case of missing <code>y</code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## simple example using dummy data
train &lt;- data.frame(matrix(abs(runif(200)), ncol = 10))
rownames(train) &lt;- LETTERS[1:20]
colnames(train) &lt;- as.character(1:10)
fossil &lt;- data.frame(matrix(abs(runif(100)), ncol = 10))
colnames(fossil) &lt;- as.character(1:10)
rownames(fossil) &lt;- letters[1:10]

## calculate distances/dissimilarities between train and fossil
## samples
test &lt;- distance(train, fossil)

## using a different coefficient, chi-square distance
test &lt;- distance(train, fossil, method = "chi.distance")

## calculate pairwise distances/dissimilarities for training
## set samples
test2 &lt;- distance(train)

## Using distance on an object of class join
dists &lt;- distance(join(train, fossil))
str(dists)

## calculate Gower's general coefficient for mixed data
## first, make a couple of variables factors

## fossil[,4] &lt;- factor(sample(rep(1:4, length = 10), 10))
## train[,4] &lt;- factor(sample(rep(1:4, length = 20), 20))
## ## now fit the mixed coefficient
## test3 &lt;- distance(train, fossil, "mixed")

## ## Example from page 260 of Legendre &amp; Legendre (1998)
x1 &lt;- t(c(2,2,NA,2,2,4,2,6))
x2 &lt;- t(c(1,3,3,1,2,2,2,5))
Rj &lt;- c(1,4,2,4,1,3,2,5) # supplied ranges

## 1 - distance(x1, x2, method = "mixed", R = Rj)

## note this gives ~0.66 as Legendre &amp; Legendre describe the
## coefficient as a similarity coefficient. Hence here we do
## 1 - Dij here to get the same answer.

## Tortula example from Podani (1999)
data(tortula)
Dij &lt;- distance(tortula[, -1], method = "mixed") # col 1 includes Taxon ID

## Only one ordered factor
data(mite.env, package = "vegan")
Dij &lt;- distance(mite.env, method = "mixed")

## Some variables are constant
data(BCI.env, package = "vegan")
Dij &lt;- distance(BCI.env, method = "mixed")
</code></pre>

<hr>
<h2 id='evenSample'>Number of samples per gradient segments</h2><span id='topic+evenSample'></span>

<h3>Description</h3>

<p>The number of samples in sections along the gradient is a useful
diagnostic as to the quality of reconstructions at gradient values
within those sections.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>evenSample(grad, n = 10)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="evenSample_+3A_grad">grad</code></td>
<td>
<p>numeric; vector of gradient values</p>
</td></tr>
<tr><td><code id="evenSample_+3A_n">n</code></td>
<td>
<p>number of segments to partition the gradient into</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The sampling design of a training set, i.e. the number of samples taken
at points along the gradient, can influence the uncertainty in the
transfer function predictions at those values of the gradient. Poorly
sampled sections of the gradient may have far larger RMSEP than the
overall model RMSEP.
</p>


<h3>Value</h3>

<p>Numeric vector of length <code>n</code> containing the numbers of samples
per gradient segment.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(SumSST)
ev &lt;- evenSample(SumSST) ## not an even sample...
plot(ev)
</code></pre>

<hr>
<h2 id='fitted.logitreg'>Fitted values for the training set from logistic regression
models</h2><span id='topic+fitted.logitreg'></span>

<h3>Description</h3>

<p>Extracts fitted values for training set samples from logistic
regression models fitted to each group of samples that describe the
probability two samples are analogues (from the same group) as a
function of dissimilarity between the paired samples. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'logitreg'
fitted(object, combined = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fitted.logitreg_+3A_object">object</code></td>
<td>
<p>an object of class <code>"logitreg"</code> resulting from a
call to <code><a href="#topic+logitreg">logitreg</a></code>.</p>
</td></tr>
<tr><td><code id="fitted.logitreg_+3A_combined">combined</code></td>
<td>
<p>logical; should the fitted values for the overall
combined analysis be returned.</p>
</td></tr>
<tr><td><code id="fitted.logitreg_+3A_...">...</code></td>
<td>
<p>arguments passed to other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If <code>combined == FALSE</code> (the default) then a matrix of fitted
probabilities, where the rows are the training set samples and the
columns the groupings, is returned. If <code>combined == TRUE</code>, then a
list with components <code>"group"</code> and
<code>"combined"</code>. <code>"group"</code> is a matrix of fitted probabilities
as above. <code>"combined"</code> is a vector of fitted values for the
entire set of pairwise comparisons considered.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+logitreg">logitreg</a></code> for example usage.
</p>

<hr>
<h2 id='fuse'>Fused dissimilarities</h2><span id='topic+fuse'></span><span id='topic+fuse.matrix'></span><span id='topic+fuse.dist'></span>

<h3>Description</h3>

<p>Combines dissimilarities from two or more dissimilarity objects into a
single dissimilarity object so that both original dissimilarities
contribute equally. Weighted combinations of the original objects can
also be created.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fuse(..., weights = NULL)

## S3 method for class 'matrix'
fuse(..., weights = NULL)

## S3 method for class 'dist'
fuse(..., weights = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fuse_+3A_...">...</code></td>
<td>
<p>objects to fuse. Methods currently exist for objects of
class <code>"matrix"</code> and <code>"dist"</code> objects. Method dispatch is
performed on the first object specified in <code>...</code>. A minimum
of two objects must be supplied.</p>
</td></tr>
<tr><td><code id="fuse_+3A_weights">weights</code></td>
<td>
<p>numeric; vector of weights that sum to 1, one weight
for each object supplied in <code>...</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Fuses, or combines, dissimilarity objects in a very flexible way to
create a single dissimilarity object that incorporates the separate
dissimilarities. In analogue matching, we may wish to combine
information from two or more proxies, such as diatoms and cladocera,
or from biological and chemical or physical data in the case of
matching modern samples.
</p>
<p>The function can also be used to fuse dissimilarity objects created
from a single data set but using different dissimilarity
coefficients. In this way one could create a new dissimilarity object
combining dissimilarity based on abundance data and presence absence
data into a single measure.
</p>
<p><code>fuse</code> uses the method of Melssen et al. (2006) to combine
dissimilarities. The dissimilarities in each dissimilarity object are
scaled so that the maximum dissimilarity in each object is 1. The
scaled dissimilarity objects are then weighted according to the
supplied weights. If no weights are supplied (the default) the
dissimilarity objects are weighted equally; <code>weights = rep(1/N,
    N)</code>, where <code>N</code> is the number of dissimilarity objects fused.
</p>
<p style="text-align: center;"><code class="reqn">D_{fused}(j, k) = \sum_{i = 1}^N w_i D_{ijk}</code>
</p>

<p>where <code class="reqn">D_{fused}(j, k)</code> is the fused dissimilarity
between samples <code class="reqn">j</code> and <code class="reqn">k</code>, <code class="reqn">w_i</code> is the weight
assigned to the <code class="reqn">i</code>th dissimilarity object and
<code class="reqn">D_{ijk}</code> is the dissimilarity between <code class="reqn">j</code> and
<code class="reqn">k</code> for the <code class="reqn">i</code>th dissimilarity object.
</p>


<h3>Value</h3>

<p><code>fuse</code> returns an object of class <code>"dist"</code> with the
attribute <code>"method"</code> set to <code>"fuse"</code>.
</p>
<p>This is the case even if the supplied objects are full dissimilarity
matrices. If you want a full dissimilarity object, use
<code><a href="stats.html#topic+as.matrix.dist">as.matrix.dist</a></code> on the returned object.
</p>
<p>The returned object contains an extra attribute <code>"weights"</code>,
which records the weights used in the fusing.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>References</h3>

<p>Melssen W., Wehrens R. and Buydens L. (2006) Supervised Kohonen
networks for classification problems. <em>Chemometrics and
intelligent laboratory systems</em> <strong>83</strong>, 99&ndash;113.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+dist">dist</a></code>, <code><a href="vegan.html#topic+vegdist">vegdist</a></code>,
<code><a href="#topic+distance">distance</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>train1 &lt;- data.frame(matrix(abs(runif(100)), ncol = 10))
train2 &lt;- data.frame(matrix(sample(c(0,1), 100, replace = TRUE),
                     ncol = 10))
rownames(train1) &lt;- rownames(train2) &lt;- LETTERS[1:10]
colnames(train1) &lt;- colnames(train2) &lt;- as.character(1:10)

d1 &lt;- vegdist(train1, method = "bray")
d2 &lt;- vegdist(train2, method = "jaccard")

dd &lt;- fuse(d1, d2, weights = c(0.6, 0.4))
dd
str(dd)
</code></pre>

<hr>
<h2 id='getK'>Extract and set the number of analogues</h2><span id='topic+getK'></span><span id='topic+getK.default'></span><span id='topic+getK.mat'></span><span id='topic+getK.bootstrap.mat'></span><span id='topic+getK.predict.mat'></span><span id='topic+setK+3C-'></span><span id='topic+setK+3C-.default'></span><span id='topic+setK+3C-.mat'></span>

<h3>Description</h3>

<p>An extractor function to access the number of analogues used in
particular models. The stored value of <code class="reqn">k</code> can be updated using
<code>setK</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getK(object, ...)

## S3 method for class 'mat'
getK(object, weighted = FALSE, ...)

## S3 method for class 'bootstrap.mat'
getK(object, which = c("bootstrap", "model"),
     prediction = FALSE, ...)

## S3 method for class 'predict.mat'
getK(object, which = c("model", "bootstrap"),
     ...)

setK(object, weighted = FALSE) &lt;- value

## S3 replacement method for class 'mat'
setK(object, weighted = FALSE) &lt;- value

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getK_+3A_object">object</code></td>
<td>
<p>an R object; currently only for objects of class
<code><a href="#topic+mat">mat</a></code> and class <code><a href="#topic+bootstrap.mat">bootstrap.mat</a></code>.</p>
</td></tr>
<tr><td><code id="getK_+3A_weighted">weighted</code></td>
<td>
<p>logical; extract/set number of analogues for a
weighted or un-weighted model?</p>
</td></tr>
<tr><td><code id="getK_+3A_which">which</code></td>
<td>
<p>character; which <em>k</em> should be extracted, the one
from the model or the one from the bootstrap results?</p>
</td></tr>
<tr><td><code id="getK_+3A_prediction">prediction</code></td>
<td>
<p>logical; should the extracted <em>k</em> be the one
that is minimum for the test set (<code>newdata</code>) or the model
(<code>object</code>).</p>
</td></tr>
<tr><td><code id="getK_+3A_...">...</code></td>
<td>
<p>further arguments to other methods.</p>
</td></tr>
<tr><td><code id="getK_+3A_value">value</code></td>
<td>
<p>integer; replacement value for <code class="reqn">k</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>getK</code> is a generic accessor function, and <code>setK&lt;-</code> is a generic
replacement function.
</p>
<p>Objects of class <code><a href="#topic+bootstrap.mat">bootstrap.mat</a></code> contain several different
<code>k</code>'s. If no predictions are performed, there will be two
<code>k</code>'s, one for the model and one from bootstrapping the
model. Where predictions are performed <strong>with</strong> <code>newenv</code>
supplied, in addition to the <code>k</code>'s above, there will be two
<code>k</code>' for the predictions, one for the model-based and one for the
bootstrap-based predictions. To select <code>k</code> for the predictions,
use <code>prediction = TRUE</code>. Argument <code>which</code> determines whether
the model-based or the bootstrap-based <code>k</code> is returned.
</p>


<h3>Value</h3>

<p>For <code>getK</code>, an integer value that is the number of analogues stored
for use. The returned object has attributes &ldquo;auto&rdquo; and
&ldquo;weighted&rdquo;. &ldquo;auto&rdquo; refers to whether the extracted value
of <code class="reqn">k</code> was set automatically (<code>TRUE</code>) or by the user
(<code>FALSE</code>). &ldquo;weighted&rdquo; states if the returned value is for
a <code>weighted</code> analysis or an un-<code>weighted</code> analysis (<code>FALSE</code>).
</p>
<p>For <code>setK&lt;-</code>, the updated object.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>See Also</h3>

<p><code><a href="#topic+mat">mat</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Imbrie and Kipp Sea Surface Temperature
data(ImbrieKipp)
data(SumSST)
data(V12.122)

## merge training set and core samples
dat &lt;- join(ImbrieKipp, V12.122, verbose = TRUE)

## extract the merged data sets and convert to proportions
ImbrieKipp &lt;- dat[[1]] / 100
ImbrieKippCore &lt;- dat[[2]] / 100

## fit a MAT model
ik.mat &lt;- mat(ImbrieKipp, SumSST, method = "chord")

## How many analogues gives lowest RMSE?
getK(ik.mat)
## note that this value was chosen automatically

## Now set k to be 10
setK(ik.mat) &lt;- 10

## check
getK(ik.mat)

</code></pre>

<hr>
<h2 id='gradientDist'>
Positions of samples along a unit-length ordination gradient.
</h2><span id='topic+gradientDist'></span><span id='topic+gradientDist.default'></span><span id='topic+gradientDist.cca'></span><span id='topic+gradientDist.prcurve'></span>

<h3>Description</h3>

<p>Extracts information as to the locations of samples along an
ordination gradient. <code>gradientDist()</code> standardises the entire
gradient to the interval 0, ..., 1, to allow comparison between
methods or data sets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gradientDist(object, ...)

## Default S3 method:
gradientDist(object, na.rm = TRUE, ...)

## S3 method for class 'cca'
gradientDist(object, na.rm = TRUE, axis = 1L,
             scaling = 0, ...)

## S3 method for class 'prcurve'
gradientDist(object, na.rm = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gradientDist_+3A_object">object</code></td>
<td>
<p>an R object of an appropriate type. For the default
method, any R object that can be coerced to a vector.</p>
</td></tr>
<tr><td><code id="gradientDist_+3A_na.rm">na.rm</code></td>
<td>
<p>logical; should missing values be removed?</p>
</td></tr>
<tr><td><code id="gradientDist_+3A_axis">axis</code></td>
<td>
<p>numeric, length 1; the ordination axis to take as the
gradient.</p>
</td></tr>
<tr><td><code id="gradientDist_+3A_scaling">scaling</code></td>
<td>
<p>Scaling to apply to the site scores. Default is to do
no scaling. See <code><a href="vegan.html#topic+scores.cca">scores.cca</a></code> for details.</p>
</td></tr>
<tr><td><code id="gradientDist_+3A_...">...</code></td>
<td>
<p>additional arguments passed to other methods. In the
<code>"cca"</code> method, these are also passed to
<code><a href="vegan.html#topic+scores.cca">scores.cca</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector of positions along the gradient, scaled to the range
0, ..., 1.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson
</p>


<h3>See Also</h3>

<p>See <code><a href="vegan.html#topic+cca">cca</a></code> and <code><a href="#topic+prcurve">prcurve</a></code> for functions that
produce objects that <code>gradientDist()</code> can work with.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(abernethy)

## Remove the Depth and Age variables
abernethy2 &lt;- abernethy[, -(37:38)]

## Fit PCA
aber.pca &lt;- rda(abernethy2)

## Distance along the first PCA axis
gradientDist(aber.pca)
</code></pre>

<hr>
<h2 id='hist.residLen'>Histogram plot for residual lengths</h2><span id='topic+hist.residLen'></span>

<h3>Description</h3>

<p>Base graphics histogram plot method for <code><a href="#topic+residLen">residLen</a></code>
objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'residLen'
hist(x, breaks = "Sturges", freq = TRUE,
     probs = c(0.9, 0.95, 0.99), ncol = 1, lcol = "red",
     llty = "dashed", xlab = NULL, ylab = NULL,
     main = "Residual distances", rug = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hist.residLen_+3A_x">x</code></td>
<td>
<p>Object of class <code>"residLen"</code>, the result of a call to
<code><a href="#topic+residLen">residLen</a></code>.</p>
</td></tr>
<tr><td><code id="hist.residLen_+3A_breaks">breaks</code></td>
<td>
<p>How breakpoints for the histogram are determined. See
<code>hist</code> for more details.</p>
</td></tr>
<tr><td><code id="hist.residLen_+3A_freq">freq</code></td>
<td>
<p>logical; if <code>TRUE</code>, the histogram graphic is a
representation of frequencies, the <code>counts</code> component of the
result; if <code>FALSE</code>, probability densities, component
<code>density</code>, are plotted (so that the histogram has a total area
of one). Defaults to <code>TRUE</code></p>
</td></tr>
<tr><td><code id="hist.residLen_+3A_probs">probs</code></td>
<td>
<p>numeric; vector of probability quantiles to compute from
the sets of residual distances.</p>
</td></tr>
<tr><td><code id="hist.residLen_+3A_ncol">ncol</code></td>
<td>
<p>numeric; number of columns for the plot layout. Choices
are <code>1</code> or <code>2</code>. Determines whether the histograms are
plotted above or beside each other.</p>
</td></tr>
<tr><td><code id="hist.residLen_+3A_lcol">lcol</code>, <code id="hist.residLen_+3A_llty">llty</code></td>
<td>
<p>colour and line-type for the quantiles.</p>
</td></tr>
<tr><td><code id="hist.residLen_+3A_xlab">xlab</code>, <code id="hist.residLen_+3A_ylab">ylab</code></td>
<td>
<p>Axis labels. If not supplied, suitable defaults are
generated, depending on whether RDA or CCA was used as the
underlying ordination model.</p>
</td></tr>
<tr><td><code id="hist.residLen_+3A_main">main</code></td>
<td>
<p>character; title for the plot.</p>
</td></tr>
<tr><td><code id="hist.residLen_+3A_rug">rug</code></td>
<td>
<p>logical; should rug plots of the actual distances be drawn?</p>
</td></tr>
<tr><td><code id="hist.residLen_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <code>hist</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A plot on the current device.
</p>
<p>Returns a list with two components (<code>train</code> and <code>passive</code>),
each of which is an object returned by <code>hist</code>.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>See Also</h3>

<p><code><a href="#topic+residLen">residLen</a></code>, <code><a href="#topic+plot.residLen">plot.residLen</a></code>,
<code><a href="#topic+histogram.residLen">histogram.residLen</a></code>, <code><a href="#topic+densityplot.residLen">densityplot.residLen</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load the Imbrie and Kipp example data
data(ImbrieKipp, SumSST, V12.122)

## squared residual lengths for Core V12.122
rlens &lt;- residLen(ImbrieKipp, SumSST, V12.122)
rlens

## plot a histogram of the residual distances
hist(rlens)

</code></pre>

<hr>
<h2 id='histogram.residLen'>Lattice histogram plot for residual lengths</h2><span id='topic+histogram.residLen'></span><span id='topic+histogram'></span>

<h3>Description</h3>

<p>Lattice <code><a href="lattice.html#topic+histogram">histogram</a></code> method for
<code><a href="#topic+residLen">residLen</a></code> objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'residLen'
histogram(x, ..., xlab = NULL, ylab = NULL,
          type = c("percent", "count", "density"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="histogram.residLen_+3A_x">x</code></td>
<td>
<p>Object of class <code>"residLen"</code>, the result of a call to
<code><a href="#topic+residLen">residLen</a></code>.</p>
</td></tr>
<tr><td><code id="histogram.residLen_+3A_xlab">xlab</code>, <code id="histogram.residLen_+3A_ylab">ylab</code></td>
<td>
<p>Axis labels. If not supplied, suitable defaults are
generated, depending on whether RDA or CCA was used as the
underlying ordination model.</p>
</td></tr>
<tr><td><code id="histogram.residLen_+3A_type">type</code></td>
<td>
<p>Character string indicating type of histogram to be
drawn. <code>"percent"</code> and <code>"count"</code> give relative frequency
and frequency histograms, and can be misleading when breakpoints are
not equally spaced. <code>"density"</code> produces a density scale
histogram.
</p>
<p>See <code><a href="lattice.html#topic+histogram">histogram</a></code> for further details.</p>
</td></tr>
<tr><td><code id="histogram.residLen_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to
<code><a href="lattice.html#topic+histogram">histogram</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an object of class <code>"trellis"</code>. See
<code><a href="lattice.html#topic+histogram">histogram</a></code> for details.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>See Also</h3>

<p><code><a href="#topic+residLen">residLen</a></code>, <code><a href="#topic+plot.residLen">plot.residLen</a></code>,
<code><a href="#topic+hist.residLen">hist.residLen</a></code>, <code><a href="#topic+densityplot.residLen">densityplot.residLen</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load the Imbrie and Kipp example data
data(ImbrieKipp, SumSST, V12.122)

## squared residual lengths for Core V12.122
rlens &lt;- residLen(ImbrieKipp, SumSST, V12.122)
rlens

## plot a histogram of the residual distances
histogram(rlens)

</code></pre>

<hr>
<h2 id='ImbrieKipp'>Imbrie and Kipp foraminifera training set</h2><span id='topic+ImbrieKipp'></span><span id='topic+SumSST'></span><span id='topic+WinSST'></span><span id='topic+Salinity'></span><span id='topic+V12.122'></span>

<h3>Description</h3>

<p>The classic Imbrie and Kipp (1971) training set of counts on 27
species of foraminifera from 61 ocean sediment core surface samples
and associated measures of summer and winter sea-surface temperatures
and salinity at each location.
</p>
<p>110 sediment cores samples from core V12-122 are also supplied in
<code>V12.122</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(SumSST)
data(WinSST)
data(Salinity)
data(V12.122)
</code></pre>


<h3>Format</h3>

<p><code>ImbrieKipp</code> is a data frame with 61 observations on the
following 27 species:
</p>

<dl>
<dt><code>O.univ</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.cglob</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.ruber</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.tenel</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.saccu</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.rubes</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.pacL</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.pacR</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.bullo</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.falco</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.calid</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.aequi</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.gluti</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.duter</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.infla</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.trnL</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.trnR</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.crasf</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.scitu</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.mentu</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>P.obliq</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>C.nitid</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>S.dehis</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.digit</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>Other</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.quin</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.hirsu</code></dt><dd><p>a numeric vector</p>
</dd>
</dl>

<p>Summer and Winter sea-surface temperatures, and salinity values for
the 61 sites in the Imbrie and Kipp training set (<code>ImbrieKipp</code>):
</p>

<dl>
<dt><code>SumSST</code></dt><dd><p>a numeric vector of summer sea-surface water
temperatures. Values are in degrees C.</p>
</dd>
<dt><code>WinSST</code></dt><dd><p>a numeric vector of winter sea-surface water
temperatures. Values are in degrees C.</p>
</dd>
<dt><code>Salinity</code></dt><dd><p>a numeric vector of sea water salinity
values.</p>
</dd>
</dl>

<p><code>V12.122</code> is a data frame with 110 observations from core
V12-122 on the following 28 species:
</p>

<dl>
<dt><code>O.univ</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.cglob</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.ruber</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.tenel</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.saccu</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.rubes</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.pacL</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.pacR</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.bullo</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.falco</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.calid</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.aequi</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.gluti</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.duter</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.infla</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.trnL</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.trnR</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.crasf</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.scitu</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.mentu</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>P.obliq</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>C.nitid</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>S.dehis</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.digit</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.hexag</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>G.cglom</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>cfH.pel</code></dt><dd><p>a numeric vector</p>
</dd>
<dt><code>Other</code></dt><dd><p>a numeric vector</p>
</dd>
</dl>



<h3>Source</h3>

<p>Imbrie and Kipp (1971) TODO: Get the full reference.
</p>
<p>These data were provided in electronic format by Prof. H. John
B. Birks.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ImbrieKipp)
head(ImbrieKipp)

data(SumSST)
data(WinSST)
data(Salinity)

plot(cbind(SumSST, WinSST, Salinity))

data(V12.122)
head(V12.122)
</code></pre>

<hr>
<h2 id='join'>Merge species data sets on common columns (species)</h2><span id='topic+join'></span><span id='topic+head.join'></span><span id='topic+tail.join'></span>

<h3>Description</h3>

<p>Merges any number of species matrices on their common columns to
create a new data set with number of columns equal to the number of
unqiue columns across all data frames. Needed for analysis of fossil
data sets with respect to training set samples. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>join(..., verbose = FALSE, na.replace = TRUE, split = TRUE, value = 0,
     type = c("outer", "left", "inner"))

## S3 method for class 'join'
head(x, ...)

## S3 method for class 'join'
tail(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="join_+3A_...">...</code></td>
<td>
<p>for <code>join</code>, data frames containing the data sets to
be merged. For the <code><a href="utils.html#topic+head">head</a></code> and <code><a href="utils.html#topic+tail">tail</a></code>
methods, additional arguments to <code><a href="utils.html#topic+head">head</a></code> and
<code><a href="utils.html#topic+tail">tail</a></code>, in particular <code>"n"</code> to control the
number of rows of each <code>join</code>ed data set to display.</p>
</td></tr> 
<tr><td><code id="join_+3A_verbose">verbose</code></td>
<td>
<p>logical; if <code>TRUE</code>, the function prints out the
dimensions of the data frames in <code>"\dots"</code>, as well as those of
the returned, merged data frame.</p>
</td></tr>
<tr><td><code id="join_+3A_na.replace">na.replace</code></td>
<td>
<p>logical; samples where a column in one data frame
that have no matching column in the other will contain missing
values (<code>NA</code>). If <code>na.replace</code> is <code>TRUE</code>, these
missing values are replaced with zeros. This is standard practice in
ecology and palaeoecology. If you want to replace with another
value, then set <code>na.replace</code> to <code>FALSE</code> and do the
replacement later.</p>
</td></tr>
<tr><td><code id="join_+3A_split">split</code></td>
<td>
<p>logical; should the merged data sets samples be split
back into individual data frames, but now with common columns
(i.e. species)?</p>
</td></tr>
<tr><td><code id="join_+3A_value">value</code></td>
<td>
<p>numeric; value to replace <code>NA</code> with if
<code>na.replace</code> is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="join_+3A_type">type</code></td>
<td>
<p>logical; type of join to perform. <code>"outer"</code> returns
the <em>union</em> of the variables in data frames to be merged, such
that the resulting objects have columns for all variables found
across all the data frames to be merged. <code>"left"</code> returns the
left outer (or the left) join, such that the merged data frames
contain the set of variables found in the first supplied data
frame. <code>"inner"</code> returns the inner join, such that the merged
data frame contain the intersection of the variables in the supplied
data frames. See Details.</p>
</td></tr>
<tr><td><code id="join_+3A_x">x</code></td>
<td>
<p>an object of class <code>"join"</code>, usually the result of a
call to <code><a href="#topic+join">join</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When merging multiple data frames the set of variables in the merged
data can be determined via a number of routes. <code>join</code> provides
for two (currently) join types; the <em>outer</em> join and the
<em>left outer</em> (or simply the <em>left</em>) join. Which type of join
is performed is determined by the argument <code>type</code>.
</p>
<p>The <em>outer</em> join returns the union of the set of variables found
in the data frames to be merged. This means that the resulting data
frame(s) contain columns for all the variable observed across all the
data frames supplied for merging.
</p>
<p>With the <em>left outer</em> join the resulting data frame(s) contain
only the set of variables found in the first data frame provided.
</p>
<p>The <em>inner</em> join returns the intersection of the set of variables
found in the supplied data frames. The resulting data frame(s)
contains the variables common to all supplied data frames.
</p>


<h3>Value</h3>

<p>If <code>split = TRUE</code>, an object of class <code>"join"</code>, a list of
data frames, with as many components as the number of data frames
originally merged.
</p>
<p>Otherwise, an object of class <code>c("join", "data.frame")</code>, a data
frame containing the merged data sets.
</p>
<p><code>head.join</code> and <code>tail.join</code> return a list, each component of
which is the  result of a call to <code><a href="utils.html#topic+head">head</a></code> or
<code><a href="utils.html#topic+tail">tail</a></code> on each data set compont of the joined object.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+merge">merge</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## load the example data
data(swapdiat, swappH, rlgh)

## merge training and test set on columns
dat &lt;- join(swapdiat, rlgh, verbose = TRUE)

## extract the merged data sets and convert to proportions
swapdiat &lt;- dat[[1]] / 100
rlgh &lt;- dat[[2]] / 100

## merge training and test set using left join
head(join(swapdiat, rlgh, verbose = TRUE, type = "left"))

## load the example data
data(ImbrieKipp, SumSST, V12.122)

## merge training and test set on columns
dat &lt;- join(ImbrieKipp, V12.122, verbose = TRUE)

## extract the merged data sets and convert to proportions
ImbrieKipp &lt;- dat[[1]] / 100
V12.122 &lt;- dat[[2]] / 100

## show just the first few lines of each data set
head(dat, n = 4)

## show just the last few lines of each data set
tail(dat, n = 4)

## merge training and test set using inner join
head(join(ImbrieKipp, V12.122, verbose = TRUE, type = "inner"))

## merge training and test set using outer join and replace
## NA with -99.9
head(join(ImbrieKipp, V12.122, verbose = TRUE, value = -99.9))
</code></pre>

<hr>
<h2 id='logitreg'>Logistic regression models for assessing analogues/non-analogues</h2><span id='topic+logitreg'></span><span id='topic+logitreg.default'></span><span id='topic+logitreg.analog'></span><span id='topic+print.logitreg'></span><span id='topic+summary.logitreg'></span><span id='topic+print.summary.logitreg'></span>

<h3>Description</h3>

<p>Fits logistic regression models to each level of <code>group</code> to
model the probability of two samples being analogues conditional upon
the dissimilarity between the two samples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>logitreg(object, groups, k = 1, ...)

## Default S3 method:
logitreg(object, groups, k = 1,
         biasReduced = FALSE, ...)

## S3 method for class 'analog'
logitreg(object, groups, k = 1, ...)

## S3 method for class 'logitreg'
summary(object, p = 0.9, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="logitreg_+3A_object">object</code></td>
<td>
<p>for <code>logitreg</code>; a full dissimilarity matrix. For
<code>summary.logitreg</code> an object of class <code>"logitreg"</code>, the
result of a call to <code>logitreg</code>.</p>
</td></tr>
<tr><td><code id="logitreg_+3A_groups">groups</code></td>
<td>
<p>factor (or object that can be coerced to one) containing
the group membership for each sample in <code>object</code>.</p>
</td></tr>
<tr><td><code id="logitreg_+3A_k">k</code></td>
<td>
<p>numeric; the <code>k</code> closest analogues to use in the model
fitting.</p>
</td></tr>
<tr><td><code id="logitreg_+3A_biasreduced">biasReduced</code></td>
<td>
<p>logical; should Firth's method for bias reduced
logistic regression be used to fit the models? If <code>TRUE</code>, model
fits are performed via <code><a href="brglm.html#topic+brglm">brglm</a></code>. The default,
<code>FALSE</code>, indicates that models will be fitted via the standard
<code><a href="stats.html#topic+glm">glm</a></code> function.</p>
</td></tr>
<tr><td><code id="logitreg_+3A_p">p</code></td>
<td>
<p>probability at which to predict the dose needed.</p>
</td></tr>
<tr><td><code id="logitreg_+3A_...">...</code></td>
<td>
<p>arguments passed to other methods. These arguments are
passed on to <code><a href="stats.html#topic+glm">glm</a></code> or <code><a href="brglm.html#topic+brglm">brglm</a></code>. See their
respective helps pages for details. Note that <code>logitreg</code> sets
internally the <code>formula</code>, <code>data</code>, and <code>family</code>
arguments and hence can not be specified by the user.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Fits logistic regression models to each level of <code>group</code> to
model the probability of two samples being analogues (i.e. in the same
group) conditional upon the dissimilarity between the two samples.
</p>
<p>This function can be seen as a way of directly modelling the
probability that two sites are analogues, conditional upon
dissimilarity, that can also be done less directly using
<code><a href="#topic+roc">roc</a></code> and <code><a href="#topic+bayesF">bayesF</a></code>.
</p>
<p>Often, the number of true analogues in the training set is small, both
in absolute terms and as a proportion of comparisons. Logistic
regression is known to suffer from a small-sample bias. Firth's method
of bias reduction is a general solution to this problem and is
implemented in <code>logitreg</code> through the <span class="pkg">brglm</span> package of
Ioannis Kosmidis.
</p>


<h3>Value</h3>

<p><code>logitreg</code> returns an object of class <code>"logitreg"</code>; a list
whose components are objects returned by <code><a href="stats.html#topic+glm">glm</a></code>. See
<code><a href="stats.html#topic+glm">glm</a></code> for further details on the returned objects.
</p>
<p>The components of this list take their names from <code>group</code>.
</p>
<p>For <code>summary.logitreg</code> an object of class
<code>"summary.logitreg"</code>, a data frame with summary statistics of the
model fits. The components of this data frame are:
</p>
<table role = "presentation">
<tr><td><code>In</code>, <code>Out</code></td>
<td>
<p>The number of analogue and non-analogue dissimilarities
analysed in each group,</p>
</td></tr>
<tr><td><code>Est.(Dij)</code>, <code>Std.Err</code></td>
<td>
<p>Coefficient and its standard error for
dissimilarity from the logit model,</p>
</td></tr>
<tr><td><code>Z-value</code>, <code>p-value</code></td>
<td>
<p>Wald statistic and associated p-value for each
logit model.</p>
</td></tr>
<tr><td><code>Dij(p=?)</code>, <code>Std.Err(Dij)</code></td>
<td>
<p>The dissimilarity at which the posterior
probability of two samples being analogues is equal to <code class="reqn">p</code>, and
its standard error. These are computed using
<code><a href="MASS.html#topic+dose.p">dose.p</a></code>.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The function may generate warnings from function
<code><a href="stats.html#topic+glm.fit">glm.fit</a></code>. These should be investigated and not simply
ignored.
</p>
<p>If the message is concerns fitted probabilities being numerically 0 or
1, then check the fitted values of each of the models. These may well
be numerically 0 or 1. Heed the warning in <code><a href="stats.html#topic+glm">glm</a></code> and read
the reference cited therein which <strong>may</strong> indicate problems with
the fitted models, such as (quasi-)complete separation.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>References</h3>

<p>Firth, D. (1993). Bias reduction of maximum likelihood
estimates. <em>Biometrika</em> <strong>80</strong>, 27-38.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+roc">roc</a></code>, <code><a href="#topic+bayesF">bayesF</a></code>, <code><a href="stats.html#topic+glm">glm</a></code>, and
<code><a href="brglm.html#topic+brglm">brglm</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load the example data
data(swapdiat, swappH, rlgh)

## merge training and test set on columns
dat &lt;- join(swapdiat, rlgh, verbose = TRUE)

## extract the merged data sets and convert to proportions
swapdiat &lt;- dat[[1]] / 100
rlgh &lt;- dat[[2]] / 100

## fit an analogue matching (AM) model using the squared chord distance
## measure - need to keep the training set dissimilarities
swap.ana &lt;- analog(swapdiat, rlgh, method = "SQchord",
                   keep.train = TRUE)

## fit the ROC curve to the SWAP diatom data using the AM results
## Generate a grouping for the SWAP lakes
METHOD &lt;- if (getRversion() &lt; "3.1.0") {"ward"} else {"ward.D"}
clust &lt;- hclust(as.dist(swap.ana$train), method = METHOD)
grps &lt;- cutree(clust, 6)

## fit the logit models to the analog object
swap.lrm &lt;- logitreg(swap.ana, grps)
swap.lrm

## summary statistics
summary(swap.lrm)

## plot the fitted logit curves
plot(swap.lrm, conf.type = "polygon")

## extract fitted posterior probabilities for training samples
## for the individual groups
fit &lt;- fitted(swap.lrm)
head(fit)

## compute posterior probabilities of analogue-ness for the rlgh
## samples. Here we take the dissimilarities between fossil and
## training samples from the `swap.ana` object rather than re-
## compute them
pred &lt;- predict(swap.lrm, newdata = swap.ana$analogs)
head(pred)

## Bias reduction
## fit the logit models to the analog object
swap.brlrm &lt;- logitreg(swap.ana, grps, biasReduced = TRUE)
summary(swap.brlrm)
</code></pre>

<hr>
<h2 id='mat'>Modern Analogue Technique transfer function models</h2><span id='topic+mat'></span><span id='topic+mat.default'></span><span id='topic+mat.formula'></span><span id='topic+fitted.mat'></span><span id='topic+residuals.mat'></span><span id='topic+resid.mat'></span><span id='topic+print.residuals.mat'></span><span id='topic+print.mat'></span><span id='topic+print.fitted.mat'></span>

<h3>Description</h3>

<p>Modern Analogue Technique (MAT) transfer function models for
palaeoecology. The fitted values are the, possibly weighted, averages
of the environment for the <em>k</em>-closest modern analogues. MAT is a
<em>k</em>-NN method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mat(x, ...)

## Default S3 method:
mat(x, y,
    method = c("euclidean", "SQeuclidean", "chord", "SQchord",
               "bray", "chi.square", "SQchi.square",
               "information", "chi.distance", "manhattan",
               "kendall", "gower", "alt.gower", "mixed"),
    kmax, ...)

## S3 method for class 'formula'
mat(formula, data, subset, na.action,
    method = c("euclidean", "SQeuclidean", "chord", "SQchord",
               "bray", "chi.square", "SQchi.square",
               "information", "chi.distance", "manhattan",
               "kendall", "gower", "alt.gower", "mixed"),
    model = FALSE, ...)

## S3 method for class 'mat'
fitted(object, k, weighted = FALSE, ...)

## S3 method for class 'mat'
residuals(object, k, weighted = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mat_+3A_x">x</code></td>
<td>
<p>a data frame containing the training set data, usually
species data.</p>
</td></tr>
<tr><td><code id="mat_+3A_y">y</code></td>
<td>
<p>a vector containing the response variable, usually
environmental data to be predicted from <code>x</code>.</p>
</td></tr>
<tr><td><code id="mat_+3A_formula">formula</code></td>
<td>
<p>a symbolic description of the model to be fit. The
details of model specification are given below.</p>
</td></tr>
<tr><td><code id="mat_+3A_data">data</code></td>
<td>
<p>an optional data frame, list or environment (or object
coercible by <code><a href="base.html#topic+as.data.frame">as.data.frame</a></code> to a data frame) containing
the variables in the model.  If not found in <code>data</code>, the
variables are taken from <code>environment(formula)</code>, typically the
environment from which <code>wa</code> is called.</p>
</td></tr>
<tr><td><code id="mat_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations
to be used in the fitting process.</p>
</td></tr>
<tr><td><code id="mat_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when
the data contain <code>NA</code>s.  The default is set by the
<code>na.action</code> setting of <code><a href="base.html#topic+options">options</a></code>, and is
<code><a href="stats.html#topic+na.fail">na.fail</a></code> if that is unset.  The &quot;factory-fresh&quot; default
is <code><a href="stats.html#topic+na.omit">na.omit</a></code>.  Another possible value is <code>NULL</code>, no
action. Value <code><a href="stats.html#topic+na.exclude">na.exclude</a></code> can be useful.</p>
</td></tr>
<tr><td><code id="mat_+3A_method">method</code></td>
<td>
<p>a character string indicating the dissimilarity
(distance) coefficient to be used to define modern analogues. See
Details, below.</p>
</td></tr>
<tr><td><code id="mat_+3A_model">model</code></td>
<td>
<p>logical; If <code>TRUE</code> the model frame of the fit is
returned.</p>
</td></tr>
<tr><td><code id="mat_+3A_kmax">kmax</code></td>
<td>
<p>numeric; limit the maximum number of analogues considered
during fitting. By default, <code>kmax</code> is equal to <code class="reqn">n - 1</code>,
where <code class="reqn">n</code> is the number of sites. For large data sets this is
just wasteful as we wouldn't expect to be averaging over the entire
training set. <code>kmax</code> can be used to restrict the upper limit on
the number of analogues considered.</p>
</td></tr>
<tr><td><code id="mat_+3A_object">object</code></td>
<td>
<p>an object of class <code><a href="#topic+mat">mat</a></code>.</p>
</td></tr>
<tr><td><code id="mat_+3A_k">k</code></td>
<td>
<p>numeric; the <em>k</em>-closest analogue models' for which
fitted values and residuals are returned. Overides the default stored in
the object.</p>
</td></tr>
<tr><td><code id="mat_+3A_weighted">weighted</code></td>
<td>
<p>logical; should weighted averages be used instead of
simple averages?</p>
</td></tr>
<tr><td><code id="mat_+3A_...">...</code></td>
<td>
<p>arguments can be passed to <code><a href="#topic+distance">distance</a></code> to
provide additional optios required for some dissimilarities.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Modern Analogue Technique (MAT) is perhaps the simplest of the
transfer function models used in palaeoecology. An estimate of the
environment, <code class="reqn">x</code>, for the response for a fossil sample, <code class="reqn">y</code>,
is the, possibly weighted, mean of that variable across the
<em>k</em>-closest modern analogues selected from a modern  training set
of samples. If used, weights are the reciprocal of the dissimilarity
between the fossil sample and each modern analogue.
</p>
<p>A typical model has the form <code>response ~ terms</code> where
<code>response</code> is the (numeric) response data frame and <code>terms</code>
is a series of terms which specifies a linear predictor for
<code>response</code>. A typical form for <code>terms</code> is <code>.</code>,
which is shorthand for &quot;all variables&quot; in <code>data</code>. If <code>.</code> is
used, <code>data</code> must also be provided. If specific species
(variables) are required then <code>terms</code> should take the form
<code>spp1 + spp2 + spp3</code>.
</p>
<p>Pairwise sample dissimilarity is defined by dissimilarity or
distance coefficients. A variety of coefficients are supported &mdash; see
<code><a href="#topic+distance">distance</a></code> for details of the supported coefficients.
</p>
<p><em>k</em> is chosen by the user. The simplest choice for <em>k</em> is to
evaluate the RMSE of the difference between the predicted and observed
values of the environmental variable of interest for the training set
samples for a sequence of models with increasing <em>k</em>. The number
of analogues chosen is the value of <em>k</em> that has lowest
RMSE. However, it should be noted that this value is biased as the
data used to build the model are also used to test the predictive
power.
</p>
<p>An alternative approach is to employ an optimisation data set on which
to evaluate the size of <code class="reqn">k</code> that provides the lowest RMSEP. This
may be impractical with smaller sample sizes.
</p>
<p>A third option is to bootstrap re-sample the training set many times. At
each bootstrap sample, predictions for samples in the bootstrap test
set can be made for <code class="reqn">k = 1, ..., n</code>, where <code class="reqn">n</code> is the
number of samples in the training set. <code class="reqn">k</code> can be chosen from the
model with the lowest RMSEP. See function <code><a href="#topic+bootstrap.mat">bootstrap.mat</a></code> for
further details on choosing <code class="reqn">k</code>.
</p>
<p>The output from <code><a href="#topic+summary.mat">summary.mat</a></code> can be used to choose
<code class="reqn">k</code> in the first case above. For predictions on an optimsation or
test set see <code><a href="#topic+predict.mat">predict.mat</a></code>. For bootstrap resampling of
<code><a href="#topic+mat">mat</a></code> models, see <code><a href="#topic+bootstrap.mat">bootstrap.mat</a>.</code>
</p>
<p>The fitted values are for the training set and are taken as the,
possibly weighted, mean of the environmental variable in question
across the <em>k</em>-closest analogues. The fitted value for each
sample does <b>not</b> include a contribution from itself &mdash; it is
the closest analogue, having zero dissimilarity. This spurious
distance is ignored and analogues are ordered in terms of the non-zero
distances to other samples in the training set, with the
<em>k</em>-closest contributing to the fitted value.
</p>
<p>Typical usages for <code>residuals.mat</code> are:
</p>
<pre>
    resid(object, k, weighted = FALSE, \dots)
  </pre>


<h3>Value</h3>

<p><code>mat</code> returns an object of class <code>mat</code> with the following
components: 
</p>
<table role = "presentation">
<tr><td><code>standard</code></td>
<td>
<p>list; the model statistics based on simple
averages of <em>k</em>-closest analogues. See below.</p>
</td></tr>
<tr><td><code>weighted</code></td>
<td>
<p>list; the model statistics based on weighted  of
<em>k</em>-closest analogues. See below.</p>
</td></tr>
<tr><td><code>Dij</code></td>
<td>
<p>matrix of pairwise sample dissimilarities for the training
set <code>x</code>.</p>
</td></tr>
<tr><td><code>orig.x</code></td>
<td>
<p>the original training set data.</p>
</td></tr>
<tr><td><code>orig.y</code></td>
<td>
<p>the original environmental data or response, <code>y</code>.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matched function call.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>the dissimilarity coefficient used.</p>
</td></tr>
</table>
<p>If <code>model = TRUE</code> then additional components <code>"terms"</code> and
<code>"model"</code> are returned containing the <code><a href="stats.html#topic+terms">terms</a></code> object
and model frame used.
</p>
<p><code>fitted.mat</code> returns a list with the following components:
</p>
<table role = "presentation">
<tr><td><code>estimated</code></td>
<td>
<p>numeric; a vector of fitted values.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>numeric; this is the <em>k</em>-closest analogue model with
lowest apparent RMSE.</p>
</td></tr>
<tr><td><code>weighted</code></td>
<td>
<p>logical; are the fitted values the weighted averages
of the environment for the <em>k</em>-closest analogues. If
<code>FALSE</code>, the fitted values are the average of the environment
for the <em>k</em>-closest analogues.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The object returned by <code>mat</code> contains lists <code>"standard"</code> and
<code>"weighted"</code> both with the following elements:
</p>

<dl>
<dt><code>est</code></dt><dd><p>a matrix of estimated values for the training set
samples for models using <code class="reqn">k</code> analogues, where <code class="reqn">k = 1,
	..., n</code>. <code class="reqn">n</code> is the number of smaples in the training
set. Rows contain the values for each model of size <code class="reqn">k</code>, with
colums containing the estimates for each training set sample.</p>
</dd>
<dt><code>resid</code></dt><dd><p>matrix; as for <code>"est"</code>, but containing the
model residuals.</p>
</dd>
<dt><code>rmsep</code></dt><dd><p>vector; containing the leave-one-out root mean square
error or prediction.</p>
</dd>
<dt><code>avg.bias</code></dt><dd><p>vector; contains the average bias (mean of
residuals) for models using <em>k</em> analogues, where <code class="reqn">k = 1,
	..., n</code>. <code class="reqn">n</code> is the number of smaples in the training set.</p>
</dd>
<dt><code>max.bias</code></dt><dd><p>vector; as for <code>"avg.bias"</code>, but
containing the maximum bias statistics.</p>
</dd>
<dt><code>r.squared</code></dt><dd><p>vector; as for <code>"avg.bias"</code>, but
containing the <code class="reqn">R^2</code> statistics.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>References</h3>

<p>Gavin, D.G., Oswald, W.W., Wahl, E.R. and Williams, J.W. (2003) A
statistical approach to evaluating distance metrics and analog
assignments for pollen records. <em>Quaternary Research</em>
<strong>60</strong>, 356&ndash;367.
</p>
<p>Overpeck, J.T., Webb III, T. and Prentice I.C. (1985) Quantitative
interpretation of fossil pollen spectra: dissimilarity coefficients and
the method of modern analogues. <em>Quaternary Research</em> <strong>23</strong>,
87&ndash;108.
</p>
<p>Prell, W.L. (1985) The stability of low-latitude sea-surface
temperatures: an evaluation of the CLIMAP reconstruction with emphasis
on the positive SST anomalies, Report TR 025. U.S. Department of
Energy, Washington, D.C.
</p>
<p>Sawada, M., Viau, A.E., Vettoretti, G., Peltier, W.R. and Gajewski,
K. (2004) Comparison of North-American pollen-based temperature and
global lake-status with CCCma AGCM2 output at 6 ka. <em>Quaternary
Science Reviews</em> <strong>23</strong>, 87&ndash;108.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+summary.mat">summary.mat</a></code>, <code><a href="#topic+bootstrap.mat">bootstrap.mat</a></code> for boostrap
resampling of MAT models, <code><a href="#topic+predict.mat">predict.mat</a></code> for making
predictions from MAT models, <code><a href="#topic+fitted.mat">fitted.mat</a></code> and
<code><a href="#topic+resid.mat">resid.mat</a></code> for extraction of fitted values and residuals
from MAT models respectively. <code><a href="#topic+plot.mat">plot.mat</a></code> provides a
<code><a href="stats.html#topic+plot.lm">plot.lm</a></code>-like plotting tool for MAT models.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Imbrie and Kipp Sea Surface Temperature
data(ImbrieKipp)
data(SumSST)
data(V12.122)

## merge training set and core samples
dat &lt;- join(ImbrieKipp, V12.122, verbose = TRUE)

## extract the merged data sets and convert to proportions
ImbrieKipp &lt;- dat[[1]] / 100
ImbrieKippCore &lt;- dat[[2]] / 100

## fit the MAT model using the squared chord distance measure
ik.mat &lt;- mat(ImbrieKipp, SumSST, method = "chord")
ik.mat

## model summary
summary(ik.mat)

## fitted values
fitted(ik.mat)

## model residuals
resid(ik.mat)

## draw summary plots of the model
par(mfrow = c(2,2))
plot(ik.mat)
par(mfrow = c(1,1))

## reconstruct for the V12.122 core data
coreV12.mat &lt;- predict(ik.mat, V12.122, k = 3)
coreV12.mat
summary(coreV12.mat)

## draw the reconstruction
reconPlot(coreV12.mat, use.labels = TRUE, display.error = "bars",
          xlab = "Depth", ylab = "SumSST")

## fit the MAT model using the squared chord distance measure
## and restrict the number of analogues we fit models for to 1:20
ik.mat2 &lt;- mat(ImbrieKipp, SumSST, method = "chord", kmax = 20)
ik.mat2

</code></pre>

<hr>
<h2 id='mcarlo'>Monte Carlo simulation of dissimilarities</h2><span id='topic+mcarlo'></span><span id='topic+mcarlo.default'></span><span id='topic+mcarlo.mat'></span><span id='topic+mcarlo.analog'></span><span id='topic+print.mcarlo'></span>

<h3>Description</h3>

<p>Permutations and Monte Carlo simulations to define critical values for
dissimilarity coefficients for use in MAT reconstructions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcarlo(object, ...)

## Default S3 method:
mcarlo(object, nsamp = 10000,
       type = c("paired", "complete", "bootstrap", "permuted"),
       replace = FALSE, 
       method = c("euclidean", "SQeuclidean", "chord", "SQchord",
                  "bray", "chi.square", "SQchi.square",
                  "information", "chi.distance", "manhattan",
                  "kendall", "gower", "alt.gower", "mixed"),
       is.dcmat = FALSE, diag = FALSE, ...)

## S3 method for class 'mat'
mcarlo(object, nsamp = 10000,
       type = c("paired", "complete", "bootstrap", "permuted"),
       replace = FALSE, diag = FALSE, ...)

## S3 method for class 'analog'
mcarlo(object, nsamp = 10000,
       type = c("paired", "complete", "bootstrap", "permuted"),
       replace = FALSE, diag = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mcarlo_+3A_object">object</code></td>
<td>
<p>an R object. Currently only object's of class
<code>"mat"</code>, <code>"analog"</code> or matrix-like object of species data
allowed.</p>
</td></tr>
<tr><td><code id="mcarlo_+3A_nsamp">nsamp</code></td>
<td>
<p>numeric; number of permutations or simulations to draw.</p>
</td></tr>
<tr><td><code id="mcarlo_+3A_type">type</code></td>
<td>
<p>character; the type of permutation or simulation to
perform. See Details, below.</p>
</td></tr>
<tr><td><code id="mcarlo_+3A_replace">replace</code></td>
<td>
<p>logical; should sampling be done with replacement?</p>
</td></tr>
<tr><td><code id="mcarlo_+3A_method">method</code></td>
<td>
<p>character; for raw species matrices, the dissimilarity
coefficient to use. This is predefined when fitting a MAT model with
<code><a href="#topic+mat">mat</a></code> or analogue matching via <code><a href="#topic+analogue">analogue</a></code>
and is ignored in the <code>"mcarlo"</code> methods for classes
<code>"mat"</code> and <code>"analog"</code>.</p>
</td></tr>
<tr><td><code id="mcarlo_+3A_is.dcmat">is.dcmat</code></td>
<td>
<p>logical; is <code>"object"</code> a dissimilarity
matrix. Not meant for general use; used internally by <code>"mat"</code>
and <code>"analogue"</code> methods to instruct the <code>"default"</code>
method that <code>"object"</code> is already a dissimilarity matrix, so
there is no need to recalculate.</p>
</td></tr>
<tr><td><code id="mcarlo_+3A_diag">diag</code></td>
<td>
<p>logical; should the dissimilarities include the diagonal
(zero) values of the dissimilarity matrix. See Details.</p>
</td></tr>
<tr><td><code id="mcarlo_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Only <code>"type"</code> <code>"paired"</code> and <code>"bootstrap"</code> are
currently implemented.
</p>
<p><code><a href="#topic+distance">distance</a></code> produces square, symmetric
dissimilarity matrices for training sets. The upper triangle of these
matrices is a duplicate of the lower triangle, and as such is
redundant. <code>mcarlo</code> works on the lower triangle of these
dissimilarity matrices, representing all pairwise dissimilarity values
for training set samples. The default is <strong>not</strong> to include the
diagonal (zero) values of the dissimilarity matrix. If you feel that
these diagonal (zero) values are part of the population of
dissimilarities then use <code>"diag = TRUE"</code> to include them in the
permutations.
</p>


<h3>Value</h3>

<p>A vector of simulated dissimilarities of length <code>"nsamp"</code>. The
<code>"method"</code> used is stored in attribute <code>"method"</code>.
</p>


<h3>Note</h3>

<p>The performance of these permutation and simulation techniques still
needs to be studied. This function is provided for pedagogic
reasons. Although recommended by Sawada et al (2004), sampling with
replacement (<code>"replace = TRUE"</code>) and including diagonal (zero)
values (<code>"diag = TRUE"</code>) simulates too many zero distances. This
is because the same training set sample can, on occasion be drawn
twice leading to a zero distance. It is impossible to find in nature
two samples that will be perfectly similar, and as such sampling
<strong>with</strong> replacement <strong>and</strong> <code>"diag = TRUE"</code> seems
undesirable at best.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>References</h3>

<p>Sawada, M., Viau, A.E., Vettoretti, G., Peltier, W.R. and Gajewski,
K. (2004) Comparison of North-American pollen-based temperature and
global lake-status with CCCma AGCM2 output at 6 ka. <em>Quaternary
Science Reviews</em> <strong>23</strong>, 87&ndash;108.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mat">mat</a></code> for fitting MAT models and
<code><a href="#topic+analog">analog</a></code> for analogue matching.
<code><a href="#topic+roc">roc</a></code> as an alternative method for determining critical
values for dissimilarity measures when one has grouped data.
</p>
<p><code><a href="#topic+plot.mcarlo">plot.mcarlo</a></code> provides a plotting method to visualise the
distribution of simulated dissimilarities.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Imbrie and Kipp example
## load the example data
data(ImbrieKipp)
data(SumSST)
data(V12.122)

## merge training and test set on columns
dat &lt;- join(ImbrieKipp, V12.122, verbose = TRUE)

## extract the merged data sets and convert to proportions
ImbrieKipp &lt;- dat[[1]] / 100
V12.122 &lt;- dat[[2]] / 100

## perform the modified method of Sawada (2004) - paired sampling,
## with replacement
ik.mcarlo &lt;- mcarlo(ImbrieKipp, method = "chord", nsamp = 1000,
                    type = "paired", replace = FALSE)
ik.mcarlo

## plot the simulated distribution
layout(matrix(1:2, ncol = 1))
plot(ik.mcarlo)
layout(1)
</code></pre>

<hr>
<h2 id='minDC'>Extract minimum dissimilarities</h2><span id='topic+minDC'></span><span id='topic+minDC.default'></span><span id='topic+minDC.predict.mat'></span><span id='topic+minDC.analog'></span><span id='topic+minDC.wa'></span><span id='topic+print.minDC'></span>

<h3>Description</h3>

<p>Minimum dissimilarity is a useful indicator of reliability of
reconstructions performed via MAT and other methods, and for analogue
matching. Minimum dissimilarity for a sample is the smallest
dissimilarity between it and the training set samples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>minDC(x, ...)

## Default S3 method:
minDC(x, ...)

## S3 method for class 'predict.mat'
minDC(x, ...)

## S3 method for class 'analog'
minDC(x, probs = c(0.01, 0.02, 0.05, 0.1), ...)

## S3 method for class 'wa'
minDC(x, y,
      method = c("euclidean", "SQeuclidean", "chord", "SQchord",
                 "bray", "chi.square", "SQchi.square", "information",
                 "chi.distance", "manhattan", "kendall", "gower",
                 "alt.gower", "mixed"),
      percent = FALSE, probs = c(0.01, 0.025, 0.05, 0.1), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="minDC_+3A_x">x</code></td>
<td>
<p>an object of class <code>"predict.mat"</code>, <code>"analog"</code> or
a object with a component named <code>"minDC"</code>.</p>
</td></tr>
<tr><td><code id="minDC_+3A_probs">probs</code></td>
<td>
<p>numeric; vector of probabilities with values in [0,1].</p>
</td></tr>
<tr><td><code id="minDC_+3A_y">y</code></td>
<td>
<p>an optional matrix-like object containing fossil samples for
which the minimum dissimilarities to training samples are to be
calculated.</p>
</td></tr>
<tr><td><code id="minDC_+3A_method">method</code></td>
<td>
<p>character; which choice of dissimilarity coefficient to
use. One of the listed options. See <code><a href="#topic+distance">distance</a></code>.</p>
</td></tr>
<tr><td><code id="minDC_+3A_percent">percent</code></td>
<td>
<p>logical; Are the data percentages? If <code>TRUE</code>,
the data (<code>x</code> and <code>y</code>) will be divided by 100 to convert
them to the proportions expected by <code><a href="#topic+distance">distance</a></code>.</p>
</td></tr>
<tr><td><code id="minDC_+3A_...">...</code></td>
<td>
<p>other arguments to be passed to other methods. Currently
ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>minDC</code> returns an object of class <code>"minDC"</code>.
</p>
<p>An object of class <code>minDC</code> is a list with some or all of the
following components:
</p>
<table role = "presentation">
<tr><td><code>minDC</code></td>
<td>
<p>numeric; vector of minimum dissimilarities.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>character; the dissimilarity coefficient used.</p>
</td></tr>
<tr><td><code>quantiles</code></td>
<td>
<p>numeric; named vector of probability quantiles for
distribution of dissimilarities of modern training set.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The <code>"default"</code> method of <code>minDC</code> will attempt to extract the
relevant component of the object in question. This may be useful until a
specific <code>minDC</code> method is written for a given class.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.mat">predict.mat</a></code>, and <code><a href="#topic+plot.minDC">plot.minDC</a></code> for a
plotting method.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Imbrie and Kipp example
## load the example data
data(ImbrieKipp)
data(SumSST)
data(V12.122)

## merge training and test set on columns
dat &lt;- join(ImbrieKipp, V12.122, verbose = TRUE)

## extract the merged data sets and convert to proportions
ImbrieKipp &lt;- dat[[1]] / 100
V12.122 &lt;- dat[[2]] / 100

## fit the MAT model using the squared chord distance measure
ik.mat &lt;- mat(ImbrieKipp, SumSST, method = "SQchord")
ik.mat

## reconstruct for the V12-122 core data
v12.mat &lt;- predict(ik.mat, V12.122)

## extract the minimum DC values
v12.mdc &lt;- minDC(v12.mat)
v12.mdc

## draw a plot of minimum DC by time
plot(v12.mdc, use.labels = TRUE, xlab = "Depth (cm.)")
</code></pre>

<hr>
<h2 id='n2'>Calculate Hill's N2 diversity measure</h2><span id='topic+n2'></span><span id='topic+n2.default'></span>

<h3>Description</h3>

<p>Hills N2 is a measure of species diversity, commonly referred to as
&quot;effective&quot; diversity. If computed on the rows (samples) then the
&quot;effective&quot; number of species in each row is returned, whereas, if
computed on the columns (species) then the &quot;effective&quot; number of
occurences of each species in the data set is returned.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>n2(x, ...)

## Default S3 method:
n2(x, which = c("species", "sites"), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="n2_+3A_x">x</code></td>
<td>
<p>matrix or data frame of species data</p>
</td></tr>
<tr><td><code id="n2_+3A_which">which</code></td>
<td>
<p>character; compute N2 on the rows (<code>"sites"</code>) or the
columns (<code>"species"</code>) of <code>x</code></p>
</td></tr>
<tr><td><code id="n2_+3A_...">...</code></td>
<td>
<p>arguments passed to other methods</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector of N2 values.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(swapdiat)
sppN2 &lt;- n2(swapdiat, "species")
head(sppN2)
sampN2 &lt;- n2(swapdiat, "sites")
head(sampN2)
</code></pre>

<hr>
<h2 id='optima'>Weighted averaging optima and tolerance ranges</h2><span id='topic+optima'></span><span id='topic+optima.default'></span><span id='topic+print.optima'></span><span id='topic+print.tolerance'></span><span id='topic+tolerance.default'></span><span id='topic+as.data.frame.optima'></span><span id='topic+as.data.frame.tolerance'></span>

<h3>Description</h3>

<p>Computes weighted average optima and tolerance ranges from species
abundances and values of the environment.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optima(x, ...)

## Default S3 method:
optima(x, env, boot = FALSE, nboot = 1000,
       alpha = 0.05, ...)

## Default S3 method:
tolerance(x, env, useN2 = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="optima_+3A_x">x</code></td>
<td>
<p>Species data matrix or data frame.</p>
</td></tr>
<tr><td><code id="optima_+3A_env">env</code></td>
<td>
<p>Numeric; variable for which optima or tolerances are
required.</p>
</td></tr>
<tr><td><code id="optima_+3A_boot">boot</code>, <code id="optima_+3A_nboot">nboot</code></td>
<td>
<p>logical (<code>boot</code>), numeric (<code>nboot</code>);
should bootstrap resampling be employed to estimate the optima, and
if so how many bootstrap samples to draw?</p>
</td></tr>
<tr><td><code id="optima_+3A_alpha">alpha</code></td>
<td>
<p>numeric; 1 - <code>alpha</code> gives the coverage for the
percentile bootstrap confidence interval.</p>
</td></tr>
<tr><td><code id="optima_+3A_usen2">useN2</code></td>
<td>
<p>logical; should Hill's N2 values be used to produce
un-biased tolerances?</p>
</td></tr>
<tr><td><code id="optima_+3A_...">...</code></td>
<td>
<p>Arguments passed to other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Both functions return a named vector containing the WA optima or
tolerances for the environmental gradient specified by <code>env</code>.
</p>


<h3>Note</h3>

<p>Objects of class <code>"optima"</code> or <code>"tolerance"</code> can be coerced
to data frames using methods for <code><a href="base.html#topic+as.data.frame">as.data.frame</a></code>.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>See Also</h3>

<p><code><a href="#topic+wa">wa</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Load the Imbrie &amp; Kipp data and
## summer sea-surface temperatures
data(ImbrieKipp)
data(SumSST)

## WA optima
(opt &lt;- optima(ImbrieKipp, SumSST))

## WA tolerances
(tol &lt;- tolerance(ImbrieKipp, SumSST, useN2 = TRUE))

## caterpillar plot
caterpillarPlot(opt, tol)

## convert to data frame
as.data.frame(opt)
as.data.frame(tol)

## bootstrap WA optima - 100 resamples too low for SD &amp; pCI
bopt &lt;- optima(ImbrieKipp, SumSST, boot = TRUE, nboot = 100)
head(bopt)

</code></pre>

<hr>
<h2 id='panel.Loess'>Loess smooths to stratigraphic diagrams</h2><span id='topic+panel.Loess'></span>

<h3>Description</h3>

<p>A modified version of <code><a href="lattice.html#topic+panel.loess">panel.loess</a></code>, for drawing Loess
smooths on stratigraphic diagrams.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>panel.Loess(x, y,
            span = 1/3, degree = 1,
            family = c("symmetric","gaussian"),
            evaluation = 50,
            lwd = plot.line$lwd,
            lty = plot.line$lty,
            col,
            col.line = plot.line$col,
            type,
            ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="panel.Loess_+3A_x">x</code>, <code id="panel.Loess_+3A_y">y</code></td>
<td>
<p>variables defining the contents of the panel.</p>
</td></tr>
<tr><td><code id="panel.Loess_+3A_span">span</code>, <code id="panel.Loess_+3A_degree">degree</code>, <code id="panel.Loess_+3A_family">family</code>, <code id="panel.Loess_+3A_evaluation">evaluation</code></td>
<td>
<p>arguments passed to
<code><a href="stats.html#topic+loess.smooth">loess.smooth</a></code>, for which <code>panel.Loess</code> is a
wrapper.</p>
</td></tr>
<tr><td><code id="panel.Loess_+3A_lwd">lwd</code>, <code id="panel.Loess_+3A_lty">lty</code>, <code id="panel.Loess_+3A_col">col</code>, <code id="panel.Loess_+3A_col.line">col.line</code></td>
<td>
<p>graphical parameters.</p>
</td></tr>
<tr><td><code id="panel.Loess_+3A_type">type</code></td>
<td>
<p>for compatibility with <code><a href="lattice.html#topic+panel.loess">panel.loess</a></code>, but is
ignored within the function.</p>
</td></tr>
<tr><td><code id="panel.Loess_+3A_...">...</code></td>
<td>
<p>graphical parameters can be supplied. Color can usually
be specified by <code>col</code>, <code>col.line</code>, the latter overriding
the first for lines.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The standard panel function <code><a href="lattice.html#topic+panel.loess">panel.loess</a></code> treats the data
as the x-axis acting as the time component. For stratigraphic plots
where time flows along the y-axis, we want the smoother to be fitted
with the x-axis data as the response and the time component (y-axis)
as the predictor.
</p>
<p>This modified version of <code><a href="lattice.html#topic+panel.loess">panel.loess</a></code> flips the two axes
to produce the desired effect. Note also that it does not have
argument <code>horizontal</code> as this is not required or supported by
<code><a href="#topic+Stratiplot">Stratiplot</a></code>. In other respects, <code>panel.Loess</code> is
equivalent to the lattice panel function <code><a href="lattice.html#topic+panel.loess">panel.loess</a></code>.
</p>
<p>User should note that warnings can be generated by the fitting
function if span is set too small for species with few
observations. In such cases, the user is directed to the help page for
<code><a href="stats.html#topic+loess.smooth">loess.smooth</a></code>, but increasing <code>span</code> slightly can
often stop the warnings.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson, slightly modified from the Lattice function
<code><a href="lattice.html#topic+panel.loess">panel.loess</a></code> by Deepayan Sarkar.</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+loess.smooth">loess.smooth</a></code>, <code><a href="lattice.html#topic+panel.loess">panel.loess</a></code>.</p>

<hr>
<h2 id='panel.Stratiplot'>Panel function for stratigraphic diagrams</h2><span id='topic+panel.Stratiplot'></span>

<h3>Description</h3>

<p>A Lattice panel function for drawing individuals panels on
stratigraphic diagrams using a range of plot types commonly used
within palaeoecology.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>panel.Stratiplot(x, y,
                 type = "l",
                 col,
                 pch = plot.symbol$pch,
                 cex = plot.symbol$cex,
                 col.line = plot.line$col,
                 col.symbol = plot.symbol$col,
                 col.refline = ref.line$col,
                 col.smooth = "red",
                 col.poly = plot.line$col,
                 lty = plot.line$lty,
                 lwd = plot.line$lwd,
                 lty.smooth = plot.line$lty,
                 lwd.smooth = 2,
                 lwd.h = 3,
                 fill = plot.symbol$fill,
                 zones = NULL,
                 col.zones = plot.line$col,
                 lty.zones = plot.line$lty,
                 lwd.zones = plot.line$lwd,
                 gridh = -1, gridv = -1,
                 ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="panel.Stratiplot_+3A_x">x</code>, <code id="panel.Stratiplot_+3A_y">y</code></td>
<td>
<p>variables defining the contents of the panel.</p>
</td></tr>
<tr><td><code id="panel.Stratiplot_+3A_type">type</code></td>
<td>
<p>character vector consisting of one or more of the
following: <code>"l"</code>, <code>"p"</code>, <code>"o"</code>, <code>"b"</code>, <code>"h"</code>,
<code>"g"</code>, <code>"smooth"</code>, and <code>"poly"</code>. It <code>type</code> has more
than one element, the effects of each component are combined, though
note that some elements will over-plot, and hence obscure, earlier
elements.
</p>
<p>For <code>type</code>s <code>"l"</code>, <code>"p"</code>, <code>"o"</code>, <code>"b"</code> and
<code>"g"</code> the standard Lattice interpretation is observed. See
<code><a href="lattice.html#topic+panel.xyplot">panel.xyplot</a></code> for further details. Note that
<code>type "b"</code> is the same as <code>type "o"</code>.
</p>
<p><code>"g"</code> adds a reference grid using <code><a href="lattice.html#topic+panel.grid">panel.grid</a></code> in the
background.
</p>
<p>For <code>"h"</code>, histogram-like bars are plotted from the
<strong>y-axis</strong>, not from the x-axis with <code><a href="base.html#topic+plot">plot</a></code> and
<code><a href="lattice.html#topic+panel.loess">panel.loess</a></code>.
</p>
<p>For <code>"smooth"</code> a loess fit is added to each panel using
<code><a href="#topic+panel.Loess">panel.Loess</a></code>.
</p>
<p>For <code>"poly"</code>, a shaded polygon, or silhouette, is drawn for each
panel.</p>
</td></tr>
<tr><td><code id="panel.Stratiplot_+3A_col">col</code>, <code id="panel.Stratiplot_+3A_col.line">col.line</code>, <code id="panel.Stratiplot_+3A_col.symbol">col.symbol</code>, <code id="panel.Stratiplot_+3A_col.poly">col.poly</code>, <code id="panel.Stratiplot_+3A_col.refline">col.refline</code>, <code id="panel.Stratiplot_+3A_col.smooth">col.smooth</code>, <code id="panel.Stratiplot_+3A_col.zones">col.zones</code></td>
<td>
<p>colour parameters. For all but <code>col.smooth</code>,
default colours are obtained from <code>plot.symbol</code> and
<code>plot.line</code> using
<code><a href="lattice.html#topic+trellis.par.get">trellis.par.get</a></code>. <code>col.refline</code> controls the
colour of the reference line drawn at value 0 on the x-axis, as well
as the colour of the grid lines if drawn.</p>
</td></tr>
<tr><td><code id="panel.Stratiplot_+3A_pch">pch</code>, <code id="panel.Stratiplot_+3A_cex">cex</code>, <code id="panel.Stratiplot_+3A_lty">lty</code>, <code id="panel.Stratiplot_+3A_lwd">lwd</code>, <code id="panel.Stratiplot_+3A_fill">fill</code></td>
<td>
<p>other graphical parameters, defaults
for which are obtained from <code>plot.symbol</code> and <code>plot.line</code>
using <code><a href="lattice.html#topic+trellis.par.get">trellis.par.get</a></code>.</p>
</td></tr>
<tr><td><code id="panel.Stratiplot_+3A_lty.smooth">lty.smooth</code></td>
<td>
<p>line type for the loess smoother. The default is
obtained from <code>plot.line</code> using
<code><a href="lattice.html#topic+trellis.par.get">trellis.par.get</a></code>.</p>
</td></tr>
<tr><td><code id="panel.Stratiplot_+3A_lwd.smooth">lwd.smooth</code>, <code id="panel.Stratiplot_+3A_lwd.h">lwd.h</code></td>
<td>
<p>The line width for the loess smoother and
histogram-like bars respectively.</p>
</td></tr>
<tr><td><code id="panel.Stratiplot_+3A_zones">zones</code></td>
<td>
<p>numeric; vector of zone boundary positions on scale of
the depth/time (y-)axis.</p>
</td></tr>
<tr><td><code id="panel.Stratiplot_+3A_lty.zones">lty.zones</code>, <code id="panel.Stratiplot_+3A_lwd.zones">lwd.zones</code></td>
<td>
<p>line type and width for the zone
markers. The defaults are obtained from <code>plot.line</code>.</p>
</td></tr>
<tr><td><code id="panel.Stratiplot_+3A_gridh">gridh</code>, <code id="panel.Stratiplot_+3A_gridv">gridv</code></td>
<td>
<p>numeric arguments corresponding to <code>h</code> and
<code>v</code> of <code><a href="lattice.html#topic+panel.grid">panel.grid</a></code>, which control the number of
grid lines drawn.</p>
</td></tr>
<tr><td><code id="panel.Stratiplot_+3A_...">...</code></td>
<td>
<p>extra arguments passed on to the underlying panel
functions; <code><a href="lattice.html#topic+panel.points">panel.points</a></code>,
<code><a href="lattice.html#topic+panel.lines">panel.lines</a></code>,
<code><a href="lattice.html#topic+panel.segments">panel.segments</a></code>,
<code><a href="lattice.html#topic+panel.polygon">panel.polygon</a></code>, 
<code><a href="#topic+panel.Loess">panel.Loess</a></code> and <code><a href="lattice.html#topic+panel.refline">panel.refline</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Creates stratigraphic scatter plots of <code>x</code> and <code>y</code>, with
various modifications possible via the type argument.
</p>
<p>Zones can be drawn on the panels by supplying the numeric vector of
zone boundaries as argument <code>zones</code>. The panel function will then
draw horizontal lines across the panels at the desired y-axis
locations. Note that the panel function does <strong>not</strong> attempt to
identify the zone boundaries automatically; these must be determined
via a chronological (constrained) cluster analysis function or
similar.
</p>
<p>Note that all the arguments controlling the display can be supplied
directly to a high-level call of the function <code><a href="#topic+Stratiplot">Stratiplot</a></code>.
</p>


<h3>Note</h3>

<p>Histogram-like bars (<code>type = "h"</code>) are drawn with <code>lineend =
  "butt"</code> to improve their appearance. This can not be changed by the
user and you can't include that grid parameter in any call to
<code>panel.Stratiplot</code> that uses <code>type = "h"</code>.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>See Also</h3>

<p><code><a href="#topic+Stratiplot">Stratiplot</a></code>, <code><a href="#topic+panel.Loess">panel.Loess</a></code>,
<code><a href="lattice.html#topic+panel.xyplot">panel.xyplot</a></code>.</p>

<hr>
<h2 id='pcr'>Prinicpal component regression transfer function models</h2><span id='topic+pcr'></span><span id='topic+pcr.default'></span><span id='topic+pcr.formula'></span><span id='topic+print.pcr'></span><span id='topic+Hellinger'></span><span id='topic+ChiSquare'></span><span id='topic+performance.pcr'></span><span id='topic+fitted.pcr'></span><span id='topic+coef.pcr'></span><span id='topic+residuals.pcr'></span><span id='topic+screeplot.pcr'></span><span id='topic+eigenvals.pcr'></span>

<h3>Description</h3>

<p>Fits a palaeoecological transfer function model using principal
component regression, using an optional transformation of the matrix
of predictor variables when these are species abundance data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
## Default S3 method:
pcr(x, y, ncomp, tranFun, ...)

## S3 method for class 'formula'
pcr(formula, data, subset, na.action, ..., model = FALSE)

Hellinger(x, ...)

ChiSquare(x, apply = FALSE, parms)

## S3 method for class 'pcr'
performance(object, ...)

## S3 method for class 'pcr'
residuals(object, comps = NULL, ...)

## S3 method for class 'pcr'
fitted(object, comps = NULL, ...)

## S3 method for class 'pcr'
coef(object, comps = NULL, ...)

## S3 method for class 'pcr'
screeplot(x, restrict = NULL,
          display = c("RMSE","avgBias","maxBias","R2"),
          xlab = NULL, ylab = NULL, main = NULL, sub = NULL, ...)

## S3 method for class 'pcr'
eigenvals(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pcr_+3A_x">x</code></td>
<td>
<p>Matrix or data frame of predictor variables. Usually species
composition or abundance data for transfer function models. For
<code>screeplot</code> and <code>eigenvals</code>, an object of class
<code>"pcr"</code>.</p>
</td></tr>
<tr><td><code id="pcr_+3A_y">y</code></td>
<td>
<p>Numeric vector; the response variable to be modelled.</p>
</td></tr>
<tr><td><code id="pcr_+3A_ncomp">ncomp</code></td>
<td>
<p>numeric; number of principal components to build models
for. If not supplied the largest possible number of components is
determined.</p>
</td></tr>
<tr><td><code id="pcr_+3A_tranfun">tranFun</code></td>
<td>
<p>function; a function or name of a function that
performs a transformation of the predictor variables <code>x</code>. The
function must be self-contained as no arguments are passed to the
function when it is applied. See Details for more information.</p>
</td></tr>
<tr><td><code id="pcr_+3A_formula">formula</code></td>
<td>
<p>a model formula.</p>
</td></tr>
<tr><td><code id="pcr_+3A_data">data</code></td>
<td>
<p>an optional data frame, list or environment (or object
coercible by <code><a href="base.html#topic+as.data.frame">as.data.frame</a></code> to a data frame) containing
the variables specified on the RHS of the model formula. If not found in
<code>data</code>, the  variables are taken from
<code>environment(formula)</code>, typically the environment from which
<code>pcr</code> is called.</p>
</td></tr>
<tr><td><code id="pcr_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations
to be used in the fitting process.</p>
</td></tr>
<tr><td><code id="pcr_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when
the data contain <code>NA</code>s.  The default is set by the
<code>na.action</code> setting of <code>options</code>, and is <code>na.fail</code> if
that is unset.  The 'factory-fresh' default is <code>na.omit</code>.
Another possible value is <code>NULL</code>, no action. Value
<code>na.exclude</code> can be useful.</p>
</td></tr>
<tr><td><code id="pcr_+3A_model">model</code></td>
<td>
<p>logical; If <code>TRUE</code> the model frame is returned?</p>
</td></tr>
<tr><td><code id="pcr_+3A_apply">apply</code></td>
<td>
<p>logical; should an existing tranformation, using
pre-computed meta-parameters, be applied?</p>
</td></tr>
<tr><td><code id="pcr_+3A_parms">parms</code></td>
<td>
<p>list; a named list of parameters computed during model
fitting that can be used to apply the transformation during
prediction.</p>
</td></tr>
<tr><td><code id="pcr_+3A_object">object</code></td>
<td>
<p>an object of class <code>"pcr"</code>.</p>
</td></tr>
<tr><td><code id="pcr_+3A_comps">comps</code></td>
<td>
<p>numeric; which components to return.</p>
</td></tr>
<tr><td><code id="pcr_+3A_restrict">restrict</code></td>
<td>
<p>numeric; limit the number of components on the
screeplot.</p>
</td></tr>
<tr><td><code id="pcr_+3A_display">display</code></td>
<td>
<p>character; which model performance statistic should be
drawn on the screeplot?</p>
</td></tr>
<tr><td><code id="pcr_+3A_xlab">xlab</code>, <code id="pcr_+3A_ylab">ylab</code>, <code id="pcr_+3A_main">main</code>, <code id="pcr_+3A_sub">sub</code></td>
<td>
<p>character; labels for the plot.</p>
</td></tr>
<tr><td><code id="pcr_+3A_...">...</code></td>
<td>
<p>Arguments passed to other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When applying cross-validation (CV) to transfer function models, any
transformation of the predictors must be applied separately during
each iteration of the CV procedure to the part of the data used in
fitting the model. In the same way, any samples to be predicted from
the model must use any meta-parameters derived from the training data
only. For examle, centring is appled to the training data only and the
variables means used to centre the training data are used to centre
the test samples. The variable means should not be computed on a
combination of the training and test samples.
</p>
<p>When using PCR, we might wish to apply a transformation to the species
data predictor variables such that the PCA of those data preserves a
dissimilarity coefficient other than the Euclidean distance. This
transformation is applied to allow PCA to better describe patterns in
the species data (Legendre &amp; Gallagher 2001).
</p>
<p>How this is handled in <code>pcr</code> is to take a user-supplied function
that takes a single argument, the matrix of predictor variables. The
function should return a matrix of the same dimension as the input. If
any meta-parameters are required for subsequent use in prediction,
these should be returned as attribute <code>"parms"</code>, attached to the
matrix.
</p>
<p>Two example transformation functions are provided implementing the
Hellinger and Chi Square transformations of Legendre &amp; Gallagher
(2001). Users can base their transformation functions on
these. <code>ChiSquare()</code> illustrates how meta-parameters should be
returned as the attribute <code>"parms"</code>.
</p>


<h3>Value</h3>

<p>Returns an object of class <code>"pcr"</code>, a list with the
following components:
</p>
<table role = "presentation">
<tr><td><code>fitted.values</code></td>
<td>
<p>matrix; the PCR estimates of the response. The
columns contain fitted values using C components, where C is the Cth
column of the matrix.</p>
</td></tr>
<tr><td><code>coefficients</code></td>
<td>
<p>matrix; regression coefficients for the
PCR. Columns as per <code>fitted</code> above.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>matrix; residuals, where the Cth column represents a
PCR model using C components.</p>
</td></tr>
<tr><td><code>scores</code></td>
<td>
</td></tr>
<tr><td><code>loadings</code></td>
<td>
</td></tr>
<tr><td><code>Yloadings</code></td>
<td>
</td></tr>
<tr><td><code>xMeans</code></td>
<td>
<p>numeric; means of the predictor variables in the
training data.</p>
</td></tr>
<tr><td><code>yMean</code></td>
<td>
<p>numeric; mean of the response variable in the training
data.</p>
</td></tr>
<tr><td><code>varExpl</code></td>
<td>
<p>numeric; variance explained by the PCR model. These are
the squares of the singular values.</p>
</td></tr>
<tr><td><code>totvar</code></td>
<td>
<p>numeric; total variance in the training data</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>tranFun</code></td>
<td>
<p>transformation function used. <code>NA</code> if none
supplied/used.</p>
</td></tr>
<tr><td><code>tranParms</code></td>
<td>
<p>list; meta parameters used to computed the
transformed training data.</p>
</td></tr>
<tr><td><code>performance</code></td>
<td>
<p>data frame; cross-validation performance statistics
for the model.</p>
</td></tr>
<tr><td><code>ncomp</code></td>
<td>
<p>numeric; number of principal components computed</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>See Also</h3>

<p><code><a href="#topic+wa">wa</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Load the Imbrie &amp; Kipp data and
## summer sea-surface temperatures
data(ImbrieKipp)
data(SumSST)

## normal interface and apply Hellinger transformation
mod &lt;- pcr(ImbrieKipp, SumSST, tranFun = Hellinger)
mod

## formula interface, but as above
mod2 &lt;- pcr(SumSST ~ ., data = ImbrieKipp, tranFun = Hellinger)
mod2

## Several standard methods are available
fitted(mod, comps = 1:4)
resid(mod, comps = 1:4)
coef(mod, comps = 1:4)

## Eigenvalues can be extracted
eigenvals(mod)

## screeplot method
screeplot(mod)
</code></pre>

<hr>
<h2 id='performance'>Transfer function model performance statistics</h2><span id='topic+performance'></span><span id='topic+print.performance'></span><span id='topic+performance.wa'></span><span id='topic+performance.predict.wa'></span><span id='topic+performance.bootstrap.wa'></span><span id='topic+performance.crossval'></span>

<h3>Description</h3>

<p>A simple extractor function to access the model performance statistics
of transfer function models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>performance(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="performance_+3A_object">object</code></td>
<td>
<p>A transfer function object.</p>
</td></tr>
<tr><td><code id="performance_+3A_...">...</code></td>
<td>
<p>Arguments passed to other methods. Currently ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>performance</code> is a generic function for use with a number of
fitted models objects in <span class="pkg">analogue</span>. The available methods are:
</p>

<dl>
<dt><code><a href="#topic+wa">wa</a></code> </dt><dd><p>Weighted Averaging Models.</p>
</dd>
<dt><code><a href="#topic+predict.wa">predict.wa</a></code> </dt><dd><p>Predictions from a Weighted Average
Model.</p>
</dd>
<dt><code><a href="#topic+pcr">pcr</a></code> </dt><dd><p>Principal Component Regression models.</p>
</dd>
<dt><code><a href="#topic+bootstrap.wa">bootstrap.wa</a></code> </dt><dd><p>Bootstrapped Weighted Averaging
Models.</p>
</dd>
<dt><code><a href="#topic+crossval">crossval</a></code> </dt><dd><p>Cross-validated models fitted via
<code><a href="#topic+crossval">crossval</a></code>.</p>
</dd>
</dl>



<h3>Value</h3>

<p>A named vector containing the extracted model performance statistics.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson </p>


<h3>See Also</h3>

<p><code><a href="#topic+wa">wa</a></code>, <code><a href="#topic+predict.wa">predict.wa</a></code>,
<code><a href="#topic+bootstrap.wa">bootstrap.wa</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ImbrieKipp)
data(SumSST)

## fit the WA model
mod &lt;- wa(SumSST ~., data = ImbrieKipp)
mod

## the model performance statistics
performance(mod)
</code></pre>

<hr>
<h2 id='plot.dissimilarities'>Plots the distribution of extracted dissimilarities</h2><span id='topic+plot.dissimilarities'></span>

<h3>Description</h3>

<p>Produces a plot of the distribution of the extracted dissimilarities
and a reference normal distribution with comparable mean and sd.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'dissimilarities'
plot(x, prob = 0.05,
     legend = TRUE, n.rnorm = 1e+05, col = "black",
     col.ref = "red", lty = "solid", lty.quant = "dotted",
     xlab = NULL, ylab = NULL, main = NULL, sub = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.dissimilarities_+3A_x">x</code></td>
<td>
<p>an object of class <code>"dissimilarities"</code>.</p>
</td></tr>
<tr><td><code id="plot.dissimilarities_+3A_prob">prob</code></td>
<td>
<p>numeric; density probability defining the threshold for
close modern analogues.</p>
</td></tr>
<tr><td><code id="plot.dissimilarities_+3A_legend">legend</code></td>
<td>
<p>logical; draw a legend on the plotted figure?</p>
</td></tr>
<tr><td><code id="plot.dissimilarities_+3A_n.rnorm">n.rnorm</code></td>
<td>
<p>numeric; number of random normal deviates for reference
line.</p>
</td></tr>
<tr><td><code id="plot.dissimilarities_+3A_col">col</code>, <code id="plot.dissimilarities_+3A_col.ref">col.ref</code></td>
<td>
<p>colours for the dissimilarity and reference
density functions drawn.</p>
</td></tr>
<tr><td><code id="plot.dissimilarities_+3A_lty">lty</code>, <code id="plot.dissimilarities_+3A_lty.quant">lty.quant</code></td>
<td>
<p>line types for the dissimilarity and reference
density functions drawn.</p>
</td></tr>
<tr><td><code id="plot.dissimilarities_+3A_xlab">xlab</code>, <code id="plot.dissimilarities_+3A_ylab">ylab</code></td>
<td>
<p>character; x- and y-axis labels.</p>
</td></tr>
<tr><td><code id="plot.dissimilarities_+3A_main">main</code>, <code id="plot.dissimilarities_+3A_sub">sub</code></td>
<td>
<p>character; main and subtitle for the plot.</p>
</td></tr>
<tr><td><code id="plot.dissimilarities_+3A_...">...</code></td>
<td>
<p>graphical arguments passed to other graphics functions.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A plot on the currently active device.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>See Also</h3>

<p><code><a href="#topic+dissimilarities">dissimilarities</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Imbrie and Kipp example
## load the example data
data(ImbrieKipp)
data(SumSST)
data(V12.122)

## merge training and test set on columns
dat &lt;- join(ImbrieKipp, V12.122, verbose = TRUE)

## extract the merged data sets and convert to proportions
ImbrieKipp &lt;- dat[[1]] / 100
V12.122 &lt;- dat[[2]] / 100

## analog matching between SWAPImbrie &amp; Kipp and V12.122 core
ik.analog &lt;- analog(ImbrieKipp, V12.122, method = "chord")
ik.analog
summary(ik.analog)

## compare training set dissimilarities with normals
## and derive cut-offs
ik.dij &lt;- dissim(ik.analog)
plot(ik.dij)
</code></pre>

<hr>
<h2 id='plot.evenSample'>Plot distribution of samples along gradient</h2><span id='topic+plot.evenSample'></span>

<h3>Description</h3>

<p><code><a href="base.html#topic+plot">plot</a></code> method for objects produced by
<code><a href="#topic+evenSample">evenSample</a></code>. Draws a histogram of the number of samples
per gradient segment.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'evenSample'
plot(x, add = FALSE, xlim = NULL, ylim = NULL, col = "grey",
     border = "grey", lty = NULL, ylab, xlab, main = NULL, sub = NULL,
     ann = TRUE, axes = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.evenSample_+3A_x">x</code></td>
<td>
<p>an object of class <code>"evenSample"</code>, resulting from a call
to <code><a href="#topic+evenSample">evenSample</a></code></p>
</td></tr>
<tr><td><code id="plot.evenSample_+3A_add">add</code></td>
<td>
<p>logical; should the histogram of counts be added to an
existing plot?</p>
</td></tr>
<tr><td><code id="plot.evenSample_+3A_xlim">xlim</code>, <code id="plot.evenSample_+3A_ylim">ylim</code></td>
<td>
<p>numeric; user-specified axis limits. If not suplied
suitable defaults are used</p>
</td></tr>
<tr><td><code id="plot.evenSample_+3A_col">col</code>, <code id="plot.evenSample_+3A_border">border</code></td>
<td>
<p>colours for the fill and border of the histogram
bars respectively.</p>
</td></tr>
<tr><td><code id="plot.evenSample_+3A_lty">lty</code></td>
<td>
<p>the line type with which to draw the histogram bars</p>
</td></tr>
<tr><td><code id="plot.evenSample_+3A_ylab">ylab</code>, <code id="plot.evenSample_+3A_xlab">xlab</code>, <code id="plot.evenSample_+3A_main">main</code>, <code id="plot.evenSample_+3A_sub">sub</code></td>
<td>
<p>character strings used to label the plot</p>
</td></tr>
<tr><td><code id="plot.evenSample_+3A_ann">ann</code></td>
<td>
<p>logical; should the default annotations be added to the
plot. This relates to the plot main and sub titles and the x and y
axis labels.</p>
</td></tr>
<tr><td><code id="plot.evenSample_+3A_axes">axes</code></td>
<td>
<p>logical; should plot axes be drawn?</p>
</td></tr>
<tr><td><code id="plot.evenSample_+3A_...">...</code></td>
<td>
<p>additional arguments passed to/from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A plot is draw is drawn on the currently active device.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson
</p>


<h3>See Also</h3>

<p><code><a href="#topic+evenSample">evenSample</a></code> for the function used to create objects used
by this plot method.
</p>

<hr>
<h2 id='plot.logitreg'>Produces plots of analogue logistic regression models</h2><span id='topic+plot.logitreg'></span>

<h3>Description</h3>

<p>Draws the fitted logistic regression function describing the posterior
probability that two sites are analogues conditional upon the
dissimilarity between the two samples. Confidence intervals are also
computed and displayed if requested.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'logitreg'
plot(x, group = "all", npred = 100,
     conf.int = 0.9, conf.type = c("none", "polygon", "lines"),
     xlab = expression(D[ij]), ylab = "Pr (A+ | d)",
     rug = TRUE, ticksize = 0.02,
     col = "red", ref.col = "lightgrey",
     lwd = 2, conf.lwd = 1, conf.lty = "dashed",
     shade = "lightgrey", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.logitreg_+3A_x">x</code></td>
<td>
<p>object to plot; an object of class <code>"logitreg"</code>, usually
the result of a call to <code><a href="#topic+logitreg">logitreg</a></code>.</p>
</td></tr>
<tr><td><code id="plot.logitreg_+3A_group">group</code></td>
<td>
<p>The group to plot the logit model for. Can be one of the
group labels or <code>"Combined"</code> to draw the individual logit
models. Alternatively, and the default, is to use <code>"all"</code>,
which divides the plotting region into the required number of
plotting regions and draws all the fitted curves.</p>
</td></tr>
<tr><td><code id="plot.logitreg_+3A_npred">npred</code></td>
<td>
<p>number of points at which the fitted curves are evaluated
for plotting purposes.</p>
</td></tr>
<tr><td><code id="plot.logitreg_+3A_conf.int">conf.int</code></td>
<td>
<p>numeric; the confidence interval required.</p>
</td></tr>
<tr><td><code id="plot.logitreg_+3A_conf.type">conf.type</code></td>
<td>
<p>character; how should the confidence interval be
drawn. Default is not to draw the confidence interval.</p>
</td></tr>
<tr><td><code id="plot.logitreg_+3A_xlab">xlab</code>, <code id="plot.logitreg_+3A_ylab">ylab</code></td>
<td>
<p>character; the x and y axis labels.</p>
</td></tr>
<tr><td><code id="plot.logitreg_+3A_rug">rug</code></td>
<td>
<p>logical; should rug plots be drawn?</p>
</td></tr>
<tr><td><code id="plot.logitreg_+3A_ticksize">ticksize</code></td>
<td>
<p>The size of the tick marks used in rug plots.</p>
</td></tr>
<tr><td><code id="plot.logitreg_+3A_col">col</code></td>
<td>
<p>The colour in which to draw the line representing the
fitted values.</p>
</td></tr>
<tr><td><code id="plot.logitreg_+3A_ref.col">ref.col</code></td>
<td>
<p>The colour of the reference lines drawn at 0 and 1.</p>
</td></tr>
<tr><td><code id="plot.logitreg_+3A_lwd">lwd</code></td>
<td>
<p>The line width in which to draw the line representing the
fitted values.</p>
</td></tr>
<tr><td><code id="plot.logitreg_+3A_conf.lwd">conf.lwd</code>, <code id="plot.logitreg_+3A_conf.lty">conf.lty</code></td>
<td>
<p>Line width and line type for the confidence
interval. Only used if <code>conf.type = "lines"</code> is specified.</p>
</td></tr>
<tr><td><code id="plot.logitreg_+3A_shade">shade</code></td>
<td>
<p>The colour for the fill and border of the confidence
interval if <code>conf.type = "polygon"</code> is specified.</p>
</td></tr>
<tr><td><code id="plot.logitreg_+3A_...">...</code></td>
<td>
<p>arguments passed on to <code>plot</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A plot on the current device.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>See Also</h3>

<p><code><a href="#topic+logitreg">logitreg</a></code> for an example, <code><a href="#topic+roc">roc</a></code></p>

<hr>
<h2 id='plot.mat'>Plot diagnostics for a mat object</h2><span id='topic+plot.mat'></span>

<h3>Description</h3>

<p>Five plots (selectable by <code>which</code>) are currently available: a
plot of estimated against observed values, a plot of residuals against
estimated values, and screeplots of the apparent RMSE, average bias
and maximum bias for MAT models of size <code class="reqn">k</code>, where <code class="reqn">k = 1,
    \dots, n</code>. By default, the first three and &lsquo;5&rsquo; are provided.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mat'
plot(x,
     which = c(1:3, 5),
     weighted = FALSE,
     k,
     caption = c("Inferred vs Observed", "Residuals vs Fitted",
                 "Leave-one-out errors", "Average bias",
                 "Maximum bias"),
     max.bias = TRUE,
     n.bias = 10,
     restrict = 20,
     sub.caption = NULL,
     main = "",
     ask = prod(par("mfcol")) &lt; length(which) &amp;&amp;
                                  dev.interactive(),
     ...,
     panel = if (add.smooth) panel.smooth else points,
     add.smooth = getOption("add.smooth"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.mat_+3A_x">x</code></td>
<td>
<p>an object of class <code>"mat"</code>.</p>
</td></tr>
<tr><td><code id="plot.mat_+3A_which">which</code></td>
<td>
<p>which aspects of the <code>"mat"</code> object to plot if a
subset of the plots is required, specify a subset of the numbers
<code>1:5</code>.</p>
</td></tr>
<tr><td><code id="plot.mat_+3A_weighted">weighted</code></td>
<td>
<p>logical; should the analysis use weighted mean of env
data of analogues as fitted/estimated values?</p>
</td></tr>
<tr><td><code id="plot.mat_+3A_k">k</code></td>
<td>
<p>numeric; the number of analogues to use. If missing <code>k</code>
is chosen automatically as the <code>k</code> that achieves lowest RMSE.</p>
</td></tr>
<tr><td><code id="plot.mat_+3A_caption">caption</code></td>
<td>
<p>captions to appear above the plots.</p>
</td></tr>
<tr><td><code id="plot.mat_+3A_max.bias">max.bias</code></td>
<td>
<p>logical, should max bias lines be added to residuals.</p>
</td></tr>
<tr><td><code id="plot.mat_+3A_n.bias">n.bias</code></td>
<td>
<p>numeric, number of sections to calculate maximum bias
for.</p>
</td></tr>
<tr><td><code id="plot.mat_+3A_restrict">restrict</code></td>
<td>
<p>logical; restrict comparison of k-closest model to
<code class="reqn">k \le</code> <code>restrict</code>.</p>
</td></tr>
<tr><td><code id="plot.mat_+3A_sub.caption">sub.caption</code></td>
<td>
<p>common title-above figures if there are multiple;
used as &lsquo;sub&rsquo; (s.&lsquo;title&rsquo;) otherwise.  If <code>NULL</code>,
as by default, a possibly shortened version of
<code>deparse(x$call)</code> is used.</p>
</td></tr>
<tr><td><code id="plot.mat_+3A_main">main</code></td>
<td>
<p>title to each plot-in addition to the above
<code>caption</code>.</p>
</td></tr>
<tr><td><code id="plot.mat_+3A_ask">ask</code></td>
<td>
<p>logical; if <code>TRUE</code>, the user is <em>ask</em>ed before
each plot, see <code>par(ask=.)</code>.</p>
</td></tr>
<tr><td><code id="plot.mat_+3A_...">...</code></td>
<td>
<p>graphical arguments passed to other graphics functions.</p>
</td></tr>
<tr><td><code id="plot.mat_+3A_panel">panel</code></td>
<td>
<p>panel function.  The useful alternative to
<code>points</code>, <code>panel.smooth</code>, can be chosen by
<code>add.smooth = TRUE</code>.</p>
</td></tr>
<tr><td><code id="plot.mat_+3A_add.smooth">add.smooth</code></td>
<td>
<p>logical indicating if a smoother should be added to
fitted &amp; residuals plots; see also <code>panel</code> above.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This plotting function is modelled closely on <code><a href="stats.html#topic+plot.lm">plot.lm</a></code>
and many of the conventions and defaults for that function are
replicated here.
</p>
<p><code>sub.caption</code> - by default the function call - is shown as a
subtitle (under the x-axis title) on each plot when plots are on
separate pages, or as a subtitle in the outer margin (if any) when
there are multiple plots per page.
</p>


<h3>Value</h3>

<p>One or more plots, drawn on the current device.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson. Code borrows heavily from <code><a href="stats.html#topic+plot.lm">plot.lm</a></code>.</p>


<h3>See Also</h3>

<p><code><a href="#topic+mat">mat</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Imbrie and Kipp example
## load the example data
data(ImbrieKipp)
data(SumSST)
data(V12.122)

## merge training and test set on columns
dat &lt;- join(ImbrieKipp, V12.122, verbose = TRUE)

## extract the merged data sets and convert to proportions
ImbrieKipp &lt;- dat[[1]] / 100
V12.122 &lt;- dat[[2]] / 100

## MAT
ik.mat &lt;- mat(ImbrieKipp, SumSST, method = "chord")

## summary plot of MAT model
layout(matrix(1:4, ncol = 2, byrow = TRUE))
plot(ik.mat)
layout(1)

</code></pre>

<hr>
<h2 id='plot.mcarlo'>Plot Monte Carlo simulated dissimilarity distributions</h2><span id='topic+plot.mcarlo'></span>

<h3>Description</h3>

<p>A <code><a href="stats.html#topic+plot.lm">plot.lm</a></code>-like plotting function for objects of class
<code>"mcarlo"</code> to visualise the simulated distribution of
dissimilarities.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mcarlo'
plot(x,
     which = c(1:2),
     alpha = 0.05,
     caption = c("Distribution of dissimilarities",
       expression(paste("Simulated probability Pr (Dissim &lt; ",
           alpha, ")"))),
     col.poly = "lightgrey",
     border.poly = "lightgrey",
     ask = prod(par("mfcol")) &lt; length(which) &amp;&amp;
                                  dev.interactive(),
     ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.mcarlo_+3A_x">x</code></td>
<td>
<p>an object of class <code>"mcarlo"</code>, usually the result of a
call to <code><a href="#topic+mcarlo">mcarlo</a></code>.</p>
</td></tr>
<tr><td><code id="plot.mcarlo_+3A_which">which</code></td>
<td>
<p>numeric; which of the plots should be produced?</p>
</td></tr>
<tr><td><code id="plot.mcarlo_+3A_alpha">alpha</code></td>
<td>
<p>numeric; the Monte Carlo significance level to be marked
on the cumulative frequency plot.</p>
</td></tr>
<tr><td><code id="plot.mcarlo_+3A_caption">caption</code></td>
<td>
<p>character, length 2; captions to appear above the
plots.</p>
</td></tr>
<tr><td><code id="plot.mcarlo_+3A_col.poly">col.poly</code>, <code id="plot.mcarlo_+3A_border.poly">border.poly</code></td>
<td>
<p>character; the colour to draw the region
and border of the polygon enclosing the Monte Carlo significance on
the cummulative frequency plot.</p>
</td></tr> 
<tr><td><code id="plot.mcarlo_+3A_ask">ask</code></td>
<td>
<p>logical; should the function wait for user confirmation to
draw each plot? If missing, the function makes a reasonable attempt
to guess the current situation and act accordingly.</p>
</td></tr>
<tr><td><code id="plot.mcarlo_+3A_...">...</code></td>
<td>
<p>additional graphical parameters to be passed to the
plotting functions. Currently ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The &quot;Distribution of dissimilarities&quot; plot produces a histogram and
kernel density estimate of the distribution of simulated dissimilarity
values.
</p>
<p>The &quot;Simulated probability&quot; plot shows a cumulative probability
function of the simulated dissimlarity values, and highlights the
proportion of the curve that is less than <code>alpha</code>.
</p>


<h3>Value</h3>

<p>One or more plots on the current device.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>References</h3>

<p>Sawada, M., Viau, A.E., Vettoretti, G., Peltier, W.R. and Gajewski,
K. (2004) Comparison of North-American pollen-based temperature and
global lake-status with CCCma AGCM2 output at 6 ka. <em>Quaternary
Science Reviews</em> <strong>23</strong>, 87&ndash;108.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mcarlo">mcarlo</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Imbrie and Kipp example
## load the example data
data(ImbrieKipp)
data(SumSST)
data(V12.122)

## merge training and test set on columns
dat &lt;- join(ImbrieKipp, V12.122, verbose = TRUE)

## extract the merged data sets and convert to proportions
ImbrieKipp &lt;- dat[[1]] / 100
V12.122 &lt;- dat[[2]] / 100

## perform the modified method of Sawada (2004) - paired sampling,
## with replacement
ik.mcarlo &lt;- mcarlo(ImbrieKipp, method = "chord", nsamp = 1000,
                    type = "paired", replace = FALSE)
ik.mcarlo

## plot the simulated distribution
layout(matrix(1:2, ncol = 1))
plot(ik.mcarlo)
layout(1)
</code></pre>

<hr>
<h2 id='plot.minDC'>Plot of minimum dissimilarity per sample</h2><span id='topic+plot.minDC'></span>

<h3>Description</h3>

<p>Minimum dissimilarity is a useful indicator of reliability of
reconstructions performed via MAT and other methods, and for analogue
matching. Minimum dissimilarity for a sample is the smallest
dissimilarity between it and the training set samples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'minDC'
plot(x, depths, use.labels = FALSE,
           quantiles = TRUE, rev.x = TRUE, type = "l",
           xlim, ylim, xlab = "", ylab = "Dissimilarity",
           main = "", sub = NULL,
           col.quantile = "red", lty.quantile = "dotted",
           ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.minDC_+3A_x">x</code></td>
<td>
<p>an object of class <code>"minDC"</code>.</p>
</td></tr>
<tr><td><code id="plot.minDC_+3A_depths">depths</code></td>
<td>
<p>numeric; a vector of depths for which predicted values
exist or will be generated. Can be missing, in which case,
<strong>if</strong> <code>use.labels = TRUE</code>, the function will attempt to
derive suitable values for you. See Details below.</p>
</td></tr>
<tr><td><code id="plot.minDC_+3A_use.labels">use.labels</code></td>
<td>
<p>logical; should <code>reconPlot</code> attempt to derive
values for argument <code>depths</code> from the names of the predicted
values? Only use if <code>depths</code> is missing. See Details below.</p>
</td></tr>
<tr><td><code id="plot.minDC_+3A_quantiles">quantiles</code></td>
<td>
<p>logical; should the probability quantiles be drawn on
the plot?</p>
</td></tr>
<tr><td><code id="plot.minDC_+3A_rev.x">rev.x</code></td>
<td>
<p>logical; should the depth/age axis be reversed (drawn
from high to low)?</p>
</td></tr>
<tr><td><code id="plot.minDC_+3A_type">type</code></td>
<td>
<p>type of line drawn. See <code><a href="graphics.html#topic+par">par</a></code> and argument
<code>"type"</code>.</p>
</td></tr>
<tr><td><code id="plot.minDC_+3A_xlab">xlab</code>, <code id="plot.minDC_+3A_ylab">ylab</code></td>
<td>
<p>character; the x- and y-axis labels respectively.</p>
</td></tr>
<tr><td><code id="plot.minDC_+3A_main">main</code>, <code id="plot.minDC_+3A_sub">sub</code></td>
<td>
<p>character; main title and subtitle for the plot.</p>
</td></tr>
<tr><td><code id="plot.minDC_+3A_xlim">xlim</code>, <code id="plot.minDC_+3A_ylim">ylim</code></td>
<td>
<p>numeric, length 2; the x- and y-limits for the
plotted axes. If not provided, the function will calculate
appropriate values to cover the range of plotted values and any
quantile lines (if requested via <code>"quantiles"</code>.</p>
</td></tr>
<tr><td><code id="plot.minDC_+3A_col.quantile">col.quantile</code></td>
<td>
<p>colour in which to draw the quantile lines.</p>
</td></tr>
<tr><td><code id="plot.minDC_+3A_lty.quantile">lty.quantile</code></td>
<td>
<p>line type in which to draw the quantile lines.</p>
</td></tr>
<tr><td><code id="plot.minDC_+3A_...">...</code></td>
<td>
<p>arguments to be passed to methods, such as graphical
parameters (see <code><a href="graphics.html#topic+par">par</a></code>). Currently ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Conventionally, these plots are drawn on a depth or an age
scale. Argument <code>depths</code> is used to provide the depth or age
axis, against which the predicted values are plotted.
</p>
<p>If <code>depths</code> is not provided, then the function will try to
derive the appropriate values from the labels of the predictions if
<code>use.labels = TRUE</code>. You must provide <code>depths</code> or set
<code>use.labels = TRUE</code> otherwise an error will result. The derived
labels will be coerced to numerics. If your labels are coercible, then
you'll either get nonsense on the plot or an error from R. If so,
provide suitable values for <code>depths</code>.
</p>


<h3>Value</h3>

<p>A plot on the currently active device.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>See Also</h3>

<p><code><a href="#topic+minDC">minDC</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Imbrie and Kipp example
## load the example data
data(ImbrieKipp)
data(SumSST)
data(V12.122)

## merge training and test set on columns
dat &lt;- join(ImbrieKipp, V12.122, verbose = TRUE)

## extract the merged data sets and convert to proportions
ImbrieKipp &lt;- dat[[1]] / 100
V12.122 &lt;- dat[[2]] / 100

## fit the MAT model using the chord distance measure
(ik.mat &lt;- mat(ImbrieKipp, SumSST, method = "chord"))

## reconstruct for the RLGH core data
v12.mat &lt;- predict(ik.mat, V12.122)

## extract the minimum DC values
v12.mdc &lt;- minDC(v12.mat)
v12.mdc

## draw a plot of minimum DC by time
plot(v12.mdc, use.labels = TRUE, xlab = "Depth (cm.)")
</code></pre>

<hr>
<h2 id='plot.prcurve'>
Plot a fitted principal curve in PCA space
</h2><span id='topic+plot.prcurve'></span><span id='topic+lines.prcurve'></span>

<h3>Description</h3>

<p>Projects the principal curve into PCA space and draws it and the
underlying data in a biplot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'prcurve'
plot(x, axes = 1:2, scaling = 0, segments = TRUE,
     col = "red", col.seg = "forestgreen", lwd = 2,
     lwd.seg = 1, ...)

## S3 method for class 'prcurve'
lines(x, axes = 1:2, scaling = 0, segments = TRUE,
      col = "red", col.seg = "forestgreen", lwd = 2,
      lwd.seg = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.prcurve_+3A_x">x</code></td>
<td>
<p>an object of class <code>"prcurve"</code>.</p>
</td></tr>
<tr><td><code id="plot.prcurve_+3A_axes">axes</code></td>
<td>
<p>numeric vector of length 2; this is passed to the
<code>choices</code> argument of the <code><a href="vegan.html#topic+scores">scores</a></code>
function.</p>
</td></tr>
<tr><td><code id="plot.prcurve_+3A_scaling">scaling</code></td>
<td>
<p>numeric; the scaling to use. See
<code><a href="vegan.html#topic+scores.rda">scores.rda</a></code> for the available options. The default is
not to scale the scores, but <code>scaling = 1</code> might be a useful
alternative.</p>
</td></tr>
<tr><td><code id="plot.prcurve_+3A_segments">segments</code></td>
<td>
<p>logical; should segments be drawn between the observed
points to the location on the principal curve on to which they
project.</p>
</td></tr>
<tr><td><code id="plot.prcurve_+3A_col">col</code></td>
<td>
<p>The colour to draw the principal curve in.</p>
</td></tr>
<tr><td><code id="plot.prcurve_+3A_col.seg">col.seg</code></td>
<td>
<p>The colour to draw the segments in.</p>
</td></tr>
<tr><td><code id="plot.prcurve_+3A_lwd">lwd</code>, <code id="plot.prcurve_+3A_lwd.seg">lwd.seg</code></td>
<td>
<p>The line thickness used to draw the
principal curve and segments respectively.</p>
</td></tr>
<tr><td><code id="plot.prcurve_+3A_...">...</code></td>
<td>
<p>additional arguments passed on to <code>points</code> when
drawing the observations in PCA space.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A plot on the currently active device. The function does not return
anything.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson
</p>


<h3>See Also</h3>

<p><code><a href="#topic+prcurve">prcurve</a></code>; <code><a href="vegan.html#topic+rda">rda</a></code> for the code used to perform
the PCA.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Load the Abernethy Forest data
data(abernethy)

## Remove the Depth and Age variables
abernethy2 &lt;- abernethy[, -(37:38)]

## Fit the principal curve using varying complexity of smoothers
## for each species
aber.pc &lt;- prcurve(abernethy2, method = "ca", trace = TRUE,
                   vary = TRUE, penalty = 1.4)

## Plot the curve
plot(aber.pc)

## The lines() method can be used to add the principal curve to an
## existing plot
ord &lt;- rda(abernethy2)
plot(ord, scaling = 1)
lines(aber.pc, scaling = 1)
</code></pre>

<hr>
<h2 id='plot.residLen'>Plot method for residual lengths</h2><span id='topic+plot.residLen'></span>

<h3>Description</h3>

<p>Base graphics plot method for <code><a href="#topic+residLen">residLen</a></code> objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'residLen'
plot(x, probs = c(0.9, 0.95, 0.99), ncol = 1,
     lcol = "red", llty = "dashed", xlab = NULL, ylab = NULL,
     main = "Residual distances", rug = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.residLen_+3A_x">x</code></td>
<td>
<p>Object of class <code>"residLen"</code>, the result of a call to
<code><a href="#topic+residLen">residLen</a></code>.</p>
</td></tr>
<tr><td><code id="plot.residLen_+3A_probs">probs</code></td>
<td>
<p>numeric; vector of probability quantiles to compute from
the sets of residual distances.</p>
</td></tr>
<tr><td><code id="plot.residLen_+3A_ncol">ncol</code></td>
<td>
<p>numeric; number of columns for the plot layout. Choices
are <code>1</code> or <code>2</code>. Determines whether the histograms are
plotted above or beside each other.</p>
</td></tr>
<tr><td><code id="plot.residLen_+3A_lcol">lcol</code>, <code id="plot.residLen_+3A_llty">llty</code></td>
<td>
<p>colour and line-type for the quantiles.</p>
</td></tr>
<tr><td><code id="plot.residLen_+3A_xlab">xlab</code>, <code id="plot.residLen_+3A_ylab">ylab</code></td>
<td>
<p>Axis labels. If not supplied, suitable defaults are
generated, depending on whether RDA or CCA was used as the
underlying ordination model.</p>
</td></tr>
<tr><td><code id="plot.residLen_+3A_main">main</code></td>
<td>
<p>character; title for the plot.</p>
</td></tr>
<tr><td><code id="plot.residLen_+3A_rug">rug</code></td>
<td>
<p>logical; should rug plots of the actual distances be drawn?</p>
</td></tr>
<tr><td><code id="plot.residLen_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <code>plot</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A plot on the current device.
</p>
<p>Returns, invisibly, a list with two components (<code>train</code> and
<code>passive</code>), each and object of the type returned by
<code>density</code>.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>See Also</h3>

<p><code><a href="#topic+residLen">residLen</a></code>, <code><a href="#topic+plot.residLen">plot.residLen</a></code>,
<code><a href="#topic+histogram.residLen">histogram.residLen</a></code>, <code><a href="#topic+densityplot.residLen">densityplot.residLen</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load the Imbrie and Kipp example data
data(ImbrieKipp, SumSST, V12.122)

## squared residual lengths for Core V12.122
rlens &lt;- residLen(ImbrieKipp, SumSST, V12.122)
rlens

## plot a histogram of the residual distances
plot(rlens)

</code></pre>

<hr>
<h2 id='plot.roc'>Plot ROC curves and associated diagnostics</h2><span id='topic+plot.roc'></span>

<h3>Description</h3>

<p>Produces up to four plots (selectable by <code>"which"</code>) from the
results of a call to <code><a href="#topic+roc">roc</a></code>, including the ROC curve
itself.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'roc'
plot(x,
     which = c(1:3,5),
     group = "Combined",
     prior = NULL,
     show.stats = TRUE,
     abline.col = "grey",
     abline.lty = "dashed",
     inGroup.col = "red",
     outGroup.col = "blue",
     lty = "solid",
     caption = c("ROC curve", "Dissimilarity profiles",
                 "TPF - FPF vs Dissimilarity",
                 "Likelihood ratios"),
     legend = "topright",
     ask = prod(par("mfcol")) &lt; length(which) &amp;&amp; dev.interactive(),
     ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.roc_+3A_x">x</code></td>
<td>
<p>an object of class <code>"roc"</code>.</p>
</td></tr>
<tr><td><code id="plot.roc_+3A_which">which</code></td>
<td>
<p>numeric vector; which aspects of <code>"roc"</code> object to
plot if a subset of the plots is required, specify a subset of the
numbers <code>1:5</code>.</p>
</td></tr>
<tr><td><code id="plot.roc_+3A_group">group</code></td>
<td>
<p>character vector of length 1 giving the name of the group
to plot.</p>
</td></tr>
<tr><td><code id="plot.roc_+3A_prior">prior</code></td>
<td>
<p>numeric vector of length 2 (e.g. <code>c(0.5, 0.5)</code>)
specifiying the prior probabilities of analogue and
no-analogue. Used to generate posterior probability of analogue
using Bayes factors in plot 5 (<code>which = 5</code>).</p>
</td></tr>
<tr><td><code id="plot.roc_+3A_show.stats">show.stats</code></td>
<td>
<p>logical; should concise summary statistics of the
ROC analysis be displayed on the plot?</p>
</td></tr>
<tr><td><code id="plot.roc_+3A_abline.col">abline.col</code></td>
<td>
<p>character string or numeric value; the
colour used to draw vertical lines at the optimal dissimilarity on
the plots.</p>
</td></tr>
<tr><td><code id="plot.roc_+3A_abline.lty">abline.lty</code></td>
<td>
<p>Line type for indicator of optimal ROC dissimilarity
threshold. See <code><a href="graphics.html#topic+par">par</a></code> for the allowed line types.</p>
</td></tr>
<tr><td><code id="plot.roc_+3A_ingroup.col">inGroup.col</code></td>
<td>
<p>character string or numeric value; the colour used
to draw the density curve for the dissimilarities between sites in
the same group.</p>
</td></tr>
<tr><td><code id="plot.roc_+3A_outgroup.col">outGroup.col</code></td>
<td>
<p>character string or numeric value; the colour used
to draw the density curve for the dissimilarities between sites not in
the same group.</p>
</td></tr>
<tr><td><code id="plot.roc_+3A_lty">lty</code></td>
<td>
<p>vector of at most length 2 (elements past the second in
longer vectors are ignored) line types. The first element of
<code>lty</code> will be used where a single line is drawn on a
plot. Where two lines are drawn (for analogue and non-analogue
cases), the first element pertains to the analogue group and the
second element to the non-analogue group. See <code><a href="graphics.html#topic+par">par</a></code> for
the allowed line types.</p>
</td></tr>
<tr><td><code id="plot.roc_+3A_caption">caption</code></td>
<td>
<p>vector of character strings, containing the captions to
appear above each plot.</p>
</td></tr>
<tr><td><code id="plot.roc_+3A_legend">legend</code></td>
<td>
<p>character; position of legends drawn on plots. See
Details section in <code><a href="graphics.html#topic+legend">legend</a></code> for keywords that can be
specified.</p>
</td></tr>
<tr><td><code id="plot.roc_+3A_ask">ask</code></td>
<td>
<p>logical; if <code>TRUE</code>, the user is <em>ask</em>ed before
each plot, see <code>par(ask=.)</code>.</p>
</td></tr>
<tr><td><code id="plot.roc_+3A_...">...</code></td>
<td>
<p>graphical arguments passed to other graphics functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This plotting function is modelled closely on <code><a href="stats.html#topic+plot.lm">plot.lm</a></code>
and many of the conventions and defaults for that function are
replicated here.
</p>
<p>First, some definitions:
</p>

<dl>
<dt><strong>TPF</strong></dt><dd><p> True Positive Fraction, also known as
<em>sensitivity</em>.</p>
</dd>
<dt><strong>TNF</strong></dt><dd><p> True Negative Fraction, also known as
<em>specificity</em>.</p>
</dd>
<dt><strong>FPF</strong></dt><dd><p> False Positive Fraction; the complement of TNF,
calculated as 1 - TNF. This is often referred to a 1 -
specificity. A false positive is also known as a type I error.</p>
</dd> 
<dt><strong>FNF</strong></dt><dd><p> False Negative Fraction; the complement of TPF,
calculated as 1 - TPF. A false negative is also known as a type II
error. </p>
</dd>
<dt><strong>AUC</strong></dt><dd><p> The Area Under the ROC Curve.</p>
</dd>
</dl>

<p>The &quot;ROC curve&quot; plot (<code>which = 1</code>,) draws the ROC curve itself as
a plot of the False Positive Fraction against the True Positive
Fraction. A diagonal 1:1 line represents no ability for the
dissimilarity coefficient to differentiate between groups. The AUC
statistic may also be displayed (see argument <code>"show.stats"</code>
above).
</p>
<p>The &quot;Dissimilarity profile&quot; plot (<code>which = 2</code>), draws the density
functions of the dissimilarity values (<em>d</em>) for the correctly
assigned samples and the incorrectly assigned samples. A dissimilarity
coefficient that is able to well distinguish the sample groupings will
have density functions for the correctly and incorrectly assigned
samples that have little overlap. Conversely, a poorly discriminating
dissimilarity coefficient will have density profiles for the two
assignments that overlap considerably. The point where the two curves
cross is the optimal dissimilarity or critical value, <em>d'</em>. This
represents the point where the difference between TPF and FPF is
maximal. The value of <em>d</em> at the point where the difference
between TPF and FPF is maximal will not neccesarily be the
same as the value of <em>d'</em> where the density profiles cross. This
is because the ROC curve has been estimated at discrete points
<em>d</em>, which may not include excatly the optimal <em>d'</em>, but
which should be close to this value if the ROC curve is not sampled on
too coarse an interval.
</p>
<p>The &quot;TPF - FPF vs Dissimilarity&quot; plot, draws the difference between
the ROC curve and the 1:1 line. The point where the ROC curve is
farthest from the 1:1 line is the point at which the ROC curve has
maximal slope. This is the optimal value for <em>d</em>, as discussed
above.
</p>
<p>The &quot;Likelihood ratios&quot; plot, draws two definitions of the
slope of the ROC curve as the likelihood functions LR(+), and
LR(-). LR(+), is the likelihood ratio of a positive test result, that
the value of <em>d</em> assigns the sample to the group it belongs
to. LR(-) is the likelihood ratio of a negative test result, that the
value of <em>d</em> assigns the sample to the wrong group.
</p>
<p>LR(+) is defined as <code class="reqn">LR(+) = TPF / FPF</code> (or sensitivity / (1 -
specificity)), and LR(-) is defined as <code class="reqn">LR(-) = FPF / TNF</code> (or (1
- sensitivity) / specificity), in Henderson (1993).
</p>
<p>The &ldquo;probability of analogue&rdquo; plot, draws the posterior
probability of analogue given a dissimilarity. This is the
LR(+) likelihood ratio values multiplied by the prior odds of
analogue, for given values of the dissimilarity, and is then converted
to a  probability. 
</p>


<h3>Value</h3>

<p>One or more plots, drawn on the current device.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson. Code borrows heavily from <code><a href="stats.html#topic+plot.lm">plot.lm</a></code>.</p>


<h3>References</h3>

<p>Brown, C.D., and Davis, H.T. (2006) Receiver operating characteristics
curves and related decision measures: A tutorial. <em>Chemometrics
and Intelligent Laboratory Systems</em> <b>80</b>, 24&ndash;38.
</p>
<p>Gavin, D.G., Oswald, W.W., Wahl, E.R. and Williams, J.W. (2003) A
statistical approach to evaluating distance metrics and analog
assignments for pollen records. <em>Quaternary Research</em>
<strong>60</strong>, 356&ndash;367.
</p>
<p>Henderson, A.R. (1993) Assessing test accuracy and its clinical
consequences: a primer for receiver operating characteristic curve
analysis. <em>Annals of Clinical Biochemistry</em> <strong>30</strong>,
834&ndash;846.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+roc">roc</a></code> for a complete example</p>

<hr>
<h2 id='plot.sppResponse'>Plot species responses along gradients or latent variables</h2><span id='topic+plot.sppResponse'></span>

<h3>Description</h3>

<p>Observed abundances and fitted response curves along
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sppResponse'
plot(x,
     which = seq_along(x),
     display = c("observed", "fitted"),
     xlab = "Gradient", ylab = "Abundance",
     main = NULL,
     lcol = "red",
     lwd = 2,
     ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.sppResponse_+3A_x">x</code></td>
<td>
<p>an object of class <code>"sppResponse"</code>.</p>
</td></tr>
<tr><td><code id="plot.sppResponse_+3A_which">which</code></td>
<td>
<p>numeric or logical vector; which species (components of
<code>x</code>) to plot.</p>
</td></tr>
<tr><td><code id="plot.sppResponse_+3A_display">display</code></td>
<td>
<p>character; plot the observed data or the fitted species
response curve or both?</p>
</td></tr>
<tr><td><code id="plot.sppResponse_+3A_xlab">xlab</code>, <code id="plot.sppResponse_+3A_ylab">ylab</code>, <code id="plot.sppResponse_+3A_main">main</code></td>
<td>
<p>titles for the x and y axis label and the main
title. If <code>main</code> is <code>NULL</code>, suitable main titles are taken
from <code>x</code>.</p>
</td></tr>
<tr><td><code id="plot.sppResponse_+3A_lwd">lwd</code></td>
<td>
<p>numeric; width of the line used to draw the fitted species
response function if drawn.</p>
</td></tr>
<tr><td><code id="plot.sppResponse_+3A_lcol">lcol</code></td>
<td>
<p>the colour to use to draw the fitted species response.</p>
</td></tr>
<tr><td><code id="plot.sppResponse_+3A_...">...</code></td>
<td>
<p>graphical arguments passed <code>plot</code> and <code>lines</code>
used internally.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Currently, no attempt is made to split the device into an appropriate
number of panels. This is up to the user to decide. See the Examples
section of <code><a href="#topic+sppResponse">sppResponse</a></code> for one way to handle this.
</p>


<h3>Value</h3>

<p>One or more plots, drawn on the current device.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>See Also</h3>

<p><code><a href="#topic+sppResponse">sppResponse</a></code> for a complete example using fitted
responses along principal curves.</p>

<hr>
<h2 id='plot.wa'>Plot diagnostics for a weighted averaging model</h2><span id='topic+plot.wa'></span>

<h3>Description</h3>

<p>Two plots (selectable by <code>which</code>) are currently available: a
plot of estimated against observed values, a plot of residuals against
estimated values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'wa'
plot(x,
     which = 1:2,
     caption = c("Inferred vs Observed", "Residuals vs Fitted"),
     max.bias = TRUE,
     n.bias = 10,
     sub.caption = NULL,
     main = "",
     ask = prod(par("mfcol")) &lt; length(which) &amp;&amp;
                                  dev.interactive(),
     ...,
     panel = if (add.smooth) panel.smooth else points,
     add.smooth = getOption("add.smooth"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.wa_+3A_x">x</code></td>
<td>
<p>an object of class <code>"wa"</code>.</p>
</td></tr>
<tr><td><code id="plot.wa_+3A_which">which</code></td>
<td>
<p>which aspects of the <code>"wa"</code> object to plot if a
subset of the plots is required, specify a subset of the numbers
<code>1:2</code>.</p>
</td></tr>
<tr><td><code id="plot.wa_+3A_caption">caption</code></td>
<td>
<p>captions to appear above the plots.</p>
</td></tr>
<tr><td><code id="plot.wa_+3A_max.bias">max.bias</code></td>
<td>
<p>logical, should max bias lines be added to residuals.</p>
</td></tr>
<tr><td><code id="plot.wa_+3A_n.bias">n.bias</code></td>
<td>
<p>numeric, number of sections to calculate maximum bias
for.</p>
</td></tr>
<tr><td><code id="plot.wa_+3A_sub.caption">sub.caption</code></td>
<td>
<p>common title-above figures if there are multiple;
used as &lsquo;sub&rsquo; (s.&lsquo;title&rsquo;) otherwise.  If <code>NULL</code>,
as by default, a possibly shortened version of
<code>deparse(x$call)</code> is used.</p>
</td></tr>
<tr><td><code id="plot.wa_+3A_main">main</code></td>
<td>
<p>title to each plot-in addition to the above
<code>caption</code>.</p>
</td></tr>
<tr><td><code id="plot.wa_+3A_ask">ask</code></td>
<td>
<p>logical; if <code>TRUE</code>, the user is <em>ask</em>ed before
each plot, see <code>par(ask=.)</code>.</p>
</td></tr>
<tr><td><code id="plot.wa_+3A_...">...</code></td>
<td>
<p>graphical arguments passed to other graphics functions.</p>
</td></tr>
<tr><td><code id="plot.wa_+3A_panel">panel</code></td>
<td>
<p>panel function.  The useful alternative to
<code>points</code>, <code>panel.smooth</code>, can be chosen by
<code>add.smooth = TRUE</code>.</p>
</td></tr>
<tr><td><code id="plot.wa_+3A_add.smooth">add.smooth</code></td>
<td>
<p>logical indicating if a smoother should be added to
fitted &amp; residuals plots; see also <code>panel</code> above.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This plotting function is modelled closely on <code><a href="stats.html#topic+plot.lm">plot.lm</a></code>
and many of the conventions and defaults for that function are
replicated here.
</p>
<p><code>sub.caption</code> - by default the function call - is shown as a
subtitle (under the x-axis title) on each plot when plots are on
separate pages, or as a subtitle in the outer margin (if any) when
there are multiple plots per page.
</p>


<h3>Value</h3>

<p>One or more plots, drawn on the current device.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson. Code borrows heavily from <code><a href="stats.html#topic+plot.lm">plot.lm</a></code>.</p>


<h3>See Also</h3>

<p><code><a href="#topic+mat">mat</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## see full example in ?wa

</code></pre>

<hr>
<h2 id='Pollen'>North American Modern Pollen Database</h2><span id='topic+Pollen'></span><span id='topic+Biome'></span><span id='topic+Climate'></span><span id='topic+Location'></span>

<h3>Description</h3>

<p>A database of modern pollen samples from a network of sites from North
America and Greenland, compiled by Whitmore et al. (2005). Associated
climatic and vegetation data are also record. The version of the NAMPD
included here is latest version 1-7.3 (February 2013), as of January
2016.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Pollen)

data(Climate)

data(Biome)

data(Location)
</code></pre>


<h3>Format</h3>

<p>For <code>Pollen</code>, a data frame of 4833 samples and 135 columns (the
unique identifier and 134 pollen taxa).
</p>
<p>For <code>Biome</code>, a data frame of 4833 samples on, currently, a single
vegetation variable (plus the unique identifier):
</p>

<dl>
<dt><code>ID2</code></dt><dd><p>Unique, sequential number assigned by NAMPD.</p>
</dd>
<dt><code>Fedorova</code></dt><dd><p>Factor; Vegetation type (Biome) See Whitmore
et al. (2005), Figs 3 &amp; 4. Reclassified biomes from Fedorova et al
(1994).</p>
</dd>
</dl>

<p>For <code>Location</code>, a data frame of the latitude and longitude
locational data for 4833 samples.
</p>

<dl>
<dt><code>ID2</code></dt><dd><p>Unique, sequential number assigned by NAMPD.</p>
</dd>
<dt><code>Latitude</code></dt><dd><p>Latitude of the sampling location in decimal
degrees.</p>
</dd>
<dt><code>Longitude</code></dt><dd><p>Longitude of the sampling location in decimal
degrees.</p>
</dd>
</dl>

<p>For <code>Climate</code>, a data frame with 4833 observations on the
following 32 variables.
</p>

<dl>
<dt><code>ID2</code></dt><dd><p>Unique, sequential number assigned by NAMPD.</p>
</dd>
<dt><code>t[jan:dec]</code></dt><dd><p>numeric vectors; Mean monthly temperatures
for the indicated month. Degrees C.</p>
</dd>
<dt><code>p[jan:dec]</code></dt><dd><p>numeric vectors; Mean total monthly
precipitation (mm) for the indicated month.</p>
</dd>
<dt><code>tave</code></dt><dd><p>numeric; annual average temperature (Degrees C)</p>
</dd>
<dt><code>tmax</code></dt><dd><p>numeric; maximum temperature (in Degrees C)
observed over the period of record.</p>
</dd>
<dt><code>tmin</code></dt><dd><p>numeric; minimum temperature (in Degrees C)
observed over the period of record.</p>
</dd>
<dt><code>gdd0</code></dt><dd><p>numeric; Growing degree days computed using a
base of 0 degrees C.</p>
</dd>
<dt><code>gdd5</code></dt><dd><p>numeric; Growing degree days computed using a
base of 5 degrees C.</p>
</dd>
<dt><code>mtco</code></dt><dd><p>numeric; mean temperature of the coldest month.</p>
</dd>
<dt><code>mtwa</code></dt><dd><p>numeric; mean temperature of the warmest month.</p>
</dd>
<dt><code>annp</code></dt><dd><p>numeric; mean annual total precipitation (mm).</p>
</dd>
</dl>



<h3>Details</h3>

<p>These datasets were extracted from Version 1.7 of the North American
Modern Pollen Database.
</p>
<p>All pollen species were included, however, only the Vegetation type
(Biome) field of the AVHRR data and selected Climatic variables were
extracted. Requests for additional variables to be included in the
versions of the data included in the package should me sent to the
package maintainer.
</p>


<h3>Warning</h3>

<p>Note that the data for the pollen species are a mixture of types. The
<code>DataForm</code> variable contains information on the type of data
included for each site. The codes are:
</p>

<dl>
<dt>RC</dt><dd><p>raw counts</p>
</dd>
<dt>RP</dt><dd><p>raw counts expressed as percentages</p>
</dd>
<dt>DC</dt><dd><p>digitised counts</p>
</dd>
<dt>DP</dt><dd><p>digitised counts expressed as percentages</p>
</dd>
<dt>PM</dt><dd><p>counts in permille</p>
</dd>
</dl>

<p>This value is not known for all samples.
</p>


<h3>Source</h3>

<p>The database is currently archived electronically at:
</p>
<p><a href="https://williamspaleolab.github.io/datavis/">https://williamspaleolab.github.io/datavis/</a>
</p>


<h3>References</h3>

<p>Whitmore, J., Gajewski, K., Sawada, M., Williams, J.W., Shuman, B.,
Bartlein, P.J., Minckley, T., Viau, A.E., Webb III, T., Anderson,
P.M., and Brubaker L.B., 2005. North American and Greenland modern
pollen data for multi-scale paleoecological and paleoclimatic
applications. <em>Quaternary Science Reviews</em>
<strong>24</strong>:1828&ndash;1848.
</p>
<p>Williams, J.W., Shuman, B., Bartlein, P.J., Whitmore, J., Gajewski,
K., Sawada, M., Minckley, T., Shafer, S., Viau, A.E., Webb, III, T.,
Anderson, P.M., Brubaker, L.B., Whitlock, C. and Davis, O.K., 2006.
<em>An Atlas of Pollen-Vegetation-Climate Relationships for the
United States and Canada</em>. American Association of Stratigraphic
Palynologists Foundation, Dallas, TX, 293p.
</p>
<p>Williams, J.W. and Shuman, B., 2008. Obtaining accurate and precise
environmental reconstructions from the modern analog technique and
North American surface pollen dataset. <em>Quaternary Science
Reviews</em>, <strong>27</strong>: 669&ndash;687.
</p>
<p>Fedorova, I.T., Volkova, Y.A., Varlyguin, E., 1994. <em>World
vegetation cover. Digital raster data on a 30-minute cartesian
orthonormal geodetic (lat/long) 1080x2160 grid.</em> In: Global
Ecosystems Database Version 2.0. USDOC/NOAA National Geophysical
Data Center, Bould, CO.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Pollen)

data(Climate)

data(Biome)

data(Location)
</code></pre>

<hr>
<h2 id='prcurve'>
Fits a principal curve to m-dimensional data
</h2><span id='topic+prcurve'></span><span id='topic+initCurve'></span><span id='topic+print.prcurve'></span>

<h3>Description</h3>

<p>A principal curve is a non-parametric generalisation of the principal
component and is a curve that passes through the <em>middle</em> of a
cloud of data points for a certain definition of &lsquo;middle&rsquo;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prcurve(X, method = c("ca", "pca", "random", "user"), start = NULL,
        smoother = smoothSpline, complexity, vary = FALSE,
        maxComp, finalCV = FALSE, axis = 1, rank = FALSE,
        stretch = 2, maxit = 10, trace = FALSE, thresh = 0.001,
        plotit = FALSE, ...)

initCurve(X, method = c("ca", "pca", "random", "user"), rank = FALSE,
          axis = 1, start)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="prcurve_+3A_x">X</code></td>
<td>
<p>a matrix-like object containing the variables to which the
principal curve is to be fitted.</p>
</td></tr>
<tr><td><code id="prcurve_+3A_method">method</code></td>
<td>
<p>character; method to use when initialising the principal
curve. <code>"ca"</code> fits a correspondence analysis to <code>X</code> and uses
the <code>axis</code>-th axis scores as the initial curve. <code>"pca"</code> does
the same but fits a principal components analysis to
<code>X</code>. <code>"random"</code> produces a random ordering as the initial
curve.</p>
</td></tr>
<tr><td><code id="prcurve_+3A_start">start</code></td>
<td>
<p>numeric vector specifying the initial curve when
<code>method = "user"</code>. Must be of length <code>nrow(X)</code>.</p>
</td></tr>
<tr><td><code id="prcurve_+3A_smoother">smoother</code></td>
<td>
<p>function; the choice of smoother used to fit the
principal curve. Currently, the only options are
<code>smoothSpline</code>, which is a wrapper to
<code><a href="stats.html#topic+smooth.spline">smooth.spline</a></code>, and <code>smoothGAM</code>, which is a
wrapper to  <code><a href="mgcv.html#topic+gam">gam</a></code>.</p>
</td></tr>
<tr><td><code id="prcurve_+3A_complexity">complexity</code></td>
<td>
<p>numeric; the complexity of the fitted smooth
functions.
</p>
<p>The function passed as argument <code>smoother</code> should arrange for
this argument to be passed on to relevant aspect of the underlying
smoother. In the case of <code>smoothSpline</code>, complexity is the
<code>df</code> argument of <code><a href="stats.html#topic+smooth.spline">smooth.spline</a></code>.</p>
</td></tr>
<tr><td><code id="prcurve_+3A_vary">vary</code></td>
<td>
<p>logical; should the complexity of the smoother fitted to
each variable in <code>X</code> be allowed to vary (i.e. to allow a more or
less smooth function for a particular variable. If <code>FALSE</code> the
median complexity over all <em>m</em> variables is chosen as the fixed
complexity for all <em>m</em> smooths.</p>
</td></tr>
<tr><td><code id="prcurve_+3A_maxcomp">maxComp</code></td>
<td>
<p>numeric; the upper limt on the allowed complexity.</p>
</td></tr>
<tr><td><code id="prcurve_+3A_finalcv">finalCV</code></td>
<td>
<p>logial; should a final fit of the smooth function be
performed using cross validation?</p>
</td></tr>
<tr><td><code id="prcurve_+3A_axis">axis</code></td>
<td>
<p>numeric; the ordinaion axis to use as the initial curve.</p>
</td></tr>
<tr><td><code id="prcurve_+3A_rank">rank</code></td>
<td>
<p>logical; should rank position on the gradient be used? Not
yet implemented.</p>
</td></tr>
<tr><td><code id="prcurve_+3A_stretch">stretch</code></td>
<td>
<p>numeric; a factor by which the curve can be
extrapolated when points are projected.  Default is 2 (times the
last segment length).</p>
</td></tr>
<tr><td><code id="prcurve_+3A_maxit">maxit</code></td>
<td>
<p>numeric; the maximum number of iterations.</p>
</td></tr>
<tr><td><code id="prcurve_+3A_trace">trace</code></td>
<td>
<p>logical; print progress on the iterations be printed to
the console?</p>
</td></tr>
<tr><td><code id="prcurve_+3A_thresh">thresh</code></td>
<td>
<p>numeric; convergence threshold on shortest distances to
the curve. The algorithm is considered to have converged when the
latest iteration produces a total residual distance to the curve
that is within <code>thresh</code> of the value obtained during the
previous iteration.</p>
</td></tr>
<tr><td><code id="prcurve_+3A_plotit">plotit</code></td>
<td>
<p>logical; should the fitting process be plotted? If
<code>TRUE</code>, then the fitted principal curve and observations in
<code>X</code> are plotted in principal component space.</p>
</td></tr>
<tr><td><code id="prcurve_+3A_...">...</code></td>
<td>
<p>additional arguments are passed solely on to the function
<code>smoother</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"prcurve"</code> with the following components:
</p>
<table role = "presentation">
<tr><td><code>s</code></td>
<td>
<p>a matrix corresponding to <code>X</code>, giving their projections
onto the curve.</p>
</td></tr>
<tr><td><code>tag</code></td>
<td>
<p>an index, such that <code>s[tag, ]</code> is smooth.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>for each point, its arc-length from the beginning of the
curve.</p>
</td></tr>
<tr><td><code>dist</code></td>
<td>
<p>the sum-of-squared distances from the points to their
projections.</p>
</td></tr>
<tr><td><code>converged</code></td>
<td>
<p>logical; did the algorithm converge?</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>numeric; the number of iterations performed.</p>
</td></tr>
<tr><td><code>totalDist</code></td>
<td>
<p>numeric; total sum-of-squared distances.</p>
</td></tr>
<tr><td><code>complexity</code></td>
<td>
<p>numeric vector; the complexity of the smoother
fitted to each variable in <code>X</code>.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>ordination</code></td>
<td>
<p>an object of class <code>"rda"</code>, the result of a
call to <code><a href="vegan.html#topic+rda">rda</a></code>. This is a principal components analysis
of the input data <code>X</code>.</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>a copy of the data used to fit the principal curve.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The fitting function uses function
<code><a href="princurve.html#topic+project_to_curve">project_to_curve</a></code> in package <span class="pkg">princurve</span>
to find the projection of the data on to the fitted curve.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson
</p>


<h3>See Also</h3>

<p><code><a href="#topic+smoothGAM">smoothGAM</a></code> and <code><a href="#topic+smoothSpline">smoothSpline</a></code> for the
wrappers fitting smooth functions to each variable.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Load Abernethy Forest data set
data(abernethy)

## Remove the Depth and Age variables
abernethy2 &lt;- abernethy[, -(37:38)]

## Fit the principal curve using the median complexity over
## all species
aber.pc &lt;- prcurve(abernethy2, method = "ca", trace = TRUE,
                   vary = FALSE, penalty = 1.4)

## Extract fitted values
fit &lt;- fitted(aber.pc) ## locations on curve
abun &lt;- fitted(aber.pc, type = "smooths") ## fitted response

## Fit the principal curve using varying complexity of smoothers
## for each species
aber.pc2 &lt;- prcurve(abernethy2, method = "ca", trace = TRUE,
                    vary = TRUE, penalty = 1.4)

## Predict new locations
take &lt;- abernethy2[1:10, ]
pred &lt;- predict(aber.pc2, take)

## Not run: 
## Fit principal curve using a GAM - currently slow ~10secs
aber.pc3 &lt;- prcurve(abernethy2 / 100, method = "ca", trace = TRUE,
                    vary = TRUE, smoother = smoothGAM, bs = "cr", family = mgcv::betar())

## End(Not run)
</code></pre>

<hr>
<h2 id='predict.logitreg'>Posterior probability of analogue-ness for fossil samples</h2><span id='topic+predict.logitreg'></span>

<h3>Description</h3>

<p>Predict the posterior probability of analogue-ness between fossil and
training set samples based on logistic regression fits.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
## S3 method for class 'logitreg'
predict(object, newdata, group = "all", k = 1, ...)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.logitreg_+3A_object">object</code></td>
<td>
<p>an object of class <code>"logitreg"</code>, resulting from a
call to <code><a href="#topic+logitreg">logitreg</a></code>.</p>
</td></tr>
<tr><td><code id="predict.logitreg_+3A_newdata">newdata</code></td>
<td>
<p>an object of class <code>"distance"</code> representing the
dissimilarity between fossil samples and those from the training
set.</p>
</td></tr>
<tr><td><code id="predict.logitreg_+3A_group">group</code></td>
<td>
<p>character; for which group(s) are predictions
sought. <code>"all"</code>, the default, indicates that predictions for
all groups and the combined analysis are produced. Currently
ignored.</p>
</td></tr>
<tr><td><code id="predict.logitreg_+3A_k">k</code></td>
<td>
<p>numeric; the number of close modern analogues to each fossil
sample to consider. Not currently used and may be removed from the
method as varying <code>k</code> should require refitting the logistic
regression model with that number of close modern analogues
considered.</p>
</td></tr>
<tr><td><code id="predict.logitreg_+3A_...">...</code></td>
<td>
<p>additional arguments passed to other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix of posterior probabilities of analogue-ness is returned. The
rows represent the fossil samples and the columns the groupings. There
are <code class="reqn">g+1</code> columns where <code class="reqn">g</code> is the number of groups. The
<code class="reqn">g+1</code>th column represents the Combined analysis which is the
overall posterior probability that a fossil sample is an analogue to a
training set sample (i.e. to any one of the groups).
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+logitreg">logitreg</a></code> for example usage. <code><a href="#topic+cma">cma</a></code> for
extraction of close modern analogue information and
<code><a href="#topic+analog">analog</a></code> for the underlying computations.
</p>

<hr>
<h2 id='predict.mat'>Predict method for Modern Analogue Technique models</h2><span id='topic+predict.mat'></span><span id='topic+print.predict.mat'></span>

<h3>Description</h3>

<p>Predicted values based on a MAT model object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mat'
predict(object, newdata, k, weighted = FALSE,
        bootstrap = FALSE, n.boot = 1000,
        probs = c(0.01, 0.025, 0.05, 0.1), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.mat_+3A_object">object</code></td>
<td>
<p>an object of <code><a href="#topic+mat">mat</a></code>.</p>
</td></tr>
<tr><td><code id="predict.mat_+3A_newdata">newdata</code></td>
<td>
<p>data frame; required only if predictions for some new
data are required. Mst have the same number of columns, in same
order, as <code>x</code> in <code><a href="#topic+mat">mat</a></code>. See example below or
<code><a href="#topic+join">join</a></code> for how to do this. If <code>newdata</code> not
provided, the fitted values are returned.</p>
</td></tr>
<tr><td><code id="predict.mat_+3A_k">k</code></td>
<td>
<p>number of analogues to use. If missing, <code>k</code> is chosen
automatically as the <code>k</code> that achieves lowest RMSE.</p>
</td></tr>
<tr><td><code id="predict.mat_+3A_weighted">weighted</code></td>
<td>
<p>logical; should the analysis use the weighted mean of
environmental data of analogues as predicted values?</p>
</td></tr>
<tr><td><code id="predict.mat_+3A_bootstrap">bootstrap</code></td>
<td>
<p>logical; should bootstrap derived estimates and
sample specific errors be calculated-ignored if <code>newdata</code> is
missing.</p>
</td></tr>
<tr><td><code id="predict.mat_+3A_n.boot">n.boot</code></td>
<td>
<p>numeric; the number of bootstrap samples to take.</p>
</td></tr>
<tr><td><code id="predict.mat_+3A_probs">probs</code></td>
<td>
<p>numeric; vector of probabilities with values in [0,1].</p>
</td></tr>
<tr><td><code id="predict.mat_+3A_...">...</code></td>
<td>
<p>arguments passed to of from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function returns one or more of three sets of results depending
on the supplied arguments:
</p>

<dl>
<dt>Fitted values:</dt><dd><p>the fitted values of the <code><a href="#topic+mat">mat</a></code>
model are returned if <code>newdata</code> is missing.</p>
</dd>
<dt>Predicted values:</dt><dd><p>the predicted values for some new samples
are returned if <code>newdata</code> is supplied. Summary model
statistics and estimated values for the training set are also
returned.</p>
</dd>
<dt>Bootstrap predictions and standard errors:</dt><dd><p>if <code>newdata</code>
is supplied and <code>bootstrap = TRUE</code>, the predicted values for
<code>newdata</code> plus bootstrap estimates and standard errors for the
new samples and the training set are returned.</p>
</dd>
</dl>

<p>The latter is simply a wrapper for <code>bootstrap(model, newdata,
    ...)</code> - see <code><a href="#topic+bootstrap.mat">bootstrap.mat</a></code>.
</p>


<h3>Value</h3>

<p>A object of class <code>predict.mat</code> is returned if <code>newdata</code> is
supplied, otherwise an object of <code><a href="#topic+fitted.mat">fitted.mat</a></code> is
returned. If <code>bootstrap = FALSE</code> then not all components will be
returned.
</p>
<table role = "presentation">
<tr><td><code>observed</code></td>
<td>
<p>vector of observed environmental values.</p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p>a list containing the model or non-bootstrapped
estimates for the training set. With the following components:
</p>

<dl>
<dt><code>estimated</code></dt><dd><p>estimated values for <code>"y"</code>, the
environment.</p>
</dd>
<dt><code>residuals</code></dt><dd><p>model residuals.</p>
</dd>
<dt><code>r.squared</code></dt><dd><p>Model <code class="reqn">R^2</code> between observed and
estimated values  of <code>"y"</code>.</p>
</dd>
<dt><code>avg.bias</code></dt><dd><p>Average bias of the model residuals.</p>
</dd>
<dt><code>max.bias</code></dt><dd><p>Maximum bias of the model residuals.</p>
</dd>
<dt><code>rmsep</code></dt><dd><p>Model error (RMSEP).</p>
</dd>
<dt><code>k</code></dt><dd><p>numeric; indicating the size of model used in
estimates and predictions.</p>
</dd>
</dl>

</td></tr>
<tr><td><code>bootstrap</code></td>
<td>
<p>a list containing the bootstrap estimates for the
training set. With the following components:
</p>

<dl>
<dt><code>estimated</code></dt><dd><p>Bootstrap estimates for <code>"y"</code>.</p>
</dd>
<dt><code>residuals</code></dt><dd><p>Bootstrap residuals for <code>"y"</code>.</p>
</dd>
<dt><code>r.squared</code></dt><dd><p>Bootstrap derived <code class="reqn">R^2</code> between observed
and estimated values of <code>"y"</code>.</p>
</dd>
<dt><code>avg.bias</code></dt><dd><p>Average bias of the bootstrap derived model
residuals.</p>
</dd>
<dt><code>max.bias</code></dt><dd><p>Maximum bias of the bootstrap derived model
residuals.</p>
</dd>
<dt><code>rmsep</code></dt><dd><p>Bootstrap derived RMSEP for the model.</p>
</dd>
<dt><code>s1</code></dt><dd><p>Bootstrap derived S1 error component for the
model.</p>
</dd>
<dt><code>s2</code></dt><dd><p>Bootstrap derived S2 error component for the
model.</p>
</dd> 
<dt><code>k</code></dt><dd><p>numeric; indicating the size of model used in
estimates and predictions.</p>
</dd>
</dl>

</td></tr>
<tr><td><code>sample.errors</code></td>
<td>
<p>a list containing the bootstrap-derived sample
specific errors for the training set. With the following components:
</p>

<dl>
<dt><code>rmsep</code></dt><dd><p>Bootstrap derived RMSEP for the training set
samples.</p>
</dd> 
<dt><code>s1</code></dt><dd><p>Bootstrap derived S1 error component for training
set samples.</p>
</dd>
<dt><code>s2</code></dt><dd><p>Bootstrap derived S2 error component for training
set samples.</p>
</dd>
</dl>

</td></tr>
<tr><td><code>weighted</code></td>
<td>
<p>logical; whether the weighted mean was used instead of
the mean of the environment for <em>k</em>-closest analogues.</p>
</td></tr>
<tr><td><code>auto</code></td>
<td>
<p>logical; whether <code>"k"</code> was choosen automatically or
user-selected.</p>
</td></tr>
<tr><td><code>n.boot</code></td>
<td>
<p>numeric; the number of bootstrap samples taken.</p>
</td></tr>
<tr><td><code>predictions</code></td>
<td>
<p>a list containing the model and
bootstrap-derived estimates for the new data, with the following
components:
</p>

<dl>
<dt><code>observed</code></dt><dd><p>the observed values for the new samples &mdash;
only if <code>newenv</code> is provided.</p>
</dd>
<dt><code>model</code></dt><dd><p>a list containing the model or
non-bootstrapped estimates for the new samples. A list with the
same components as <code>model</code>, above.
</p>
</dd>
<dt><code>bootstrap</code></dt><dd><p>a list containing the bootstrap estimates
for the new samples, with some or all of the same components as
<code>bootstrap</code>, above.</p>
</dd>
<dt><code>sample.errors</code></dt><dd><p>a list containing the bootstrap-derived
sample specific errors for the new samples, with some or all of
the same components as <code>sample.errors</code>, above.</p>
</dd>
</dl>

</td></tr>
<tr><td><code>method</code></td>
<td>
<p>the dissimilarity measure used to fit the underlying MAT
models.</p>
</td></tr>
<tr><td><code>quantiles</code></td>
<td>
<p>probability quantiles of the pairwise dissimilarities
computed from the training set.</p>
</td></tr>
<tr><td><code>minDC</code></td>
<td>
<p>smallest distances between each sample in <code>newdata</code>
and the training set samples.</p>
</td></tr>
<tr><td><code>Dij</code></td>
<td>
<p>dissimilarities between <code>newdata</code> and training set
samples.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>References</h3>

<p>Birks, H.J.B., Line, J.M., Juggins, S., Stevenson, A.C. and ter Braak,
C.J.F. (1990). Diatoms and pH reconstruction. <em>Philosophical
Transactions of the Royal Society of London; Series B</em>, <strong>327</strong>;
263&ndash;278.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mat">mat</a></code>, <code><a href="#topic+bootstrap.mat">bootstrap.mat</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Imbrie and Kipp example
## load the example data
data(ImbrieKipp)
data(SumSST)
data(V12.122)

## merge training and test set on columns
dat &lt;- join(ImbrieKipp, V12.122, verbose = TRUE)

## extract the merged data sets and convert to proportions
ImbrieKipp &lt;- dat[[1]] / 100
V12.122 &lt;- dat[[2]] / 100

## fit the MAT model using the chord distance measure
(ik.mat &lt;- mat(ImbrieKipp, SumSST, method = "chord"))

## predict for V12.122 data
predict(ik.mat, V12.122)

</code></pre>

<hr>
<h2 id='predict.pcr'>Predicted values from a principal components regression</h2><span id='topic+predict.pcr'></span>

<h3>Description</h3>

<p>Calculates predicted values from a fitted principal components
regression model. Leave-one-out, bootstrap or n k-fold crossvalidated
predictions are also implemented.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pcr'
predict(object, newdata, ncomp = object$ncomp,
        CV = c("none", "LOO", "bootstrap", "kfold"),
        verbose = FALSE, nboot = 100, kfold = 10, folds = 5,
        ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.pcr_+3A_object">object</code></td>
<td>
<p>a fitted model of class <code>"pcr"</code>, the result of a
call to <code><a href="#topic+pcr">pcr</a></code>.</p>
</td></tr>
<tr><td><code id="predict.pcr_+3A_newdata">newdata</code></td>
<td>
<p>data frame of new observations for which predictions
are sought.</p>
</td></tr>
<tr><td><code id="predict.pcr_+3A_ncomp">ncomp</code></td>
<td>
<p>numeric; the PCR components for which predictions are
sought. If <code>ncomp = c</code>, predictions for components <code>1:c</code>
are produced.</p>
</td></tr>
<tr><td><code id="predict.pcr_+3A_cv">CV</code></td>
<td>
<p>character; the type of crossvalidation required. Currently,
no crossvalidation methods are implemented.</p>
</td></tr>
<tr><td><code id="predict.pcr_+3A_verbose">verbose</code></td>
<td>
<p>logical; should progress on crossvalidation be printed
to the console?</p>
</td></tr>
<tr><td><code id="predict.pcr_+3A_nboot">nboot</code></td>
<td>
<p>numeric; the number of bootstrap samples to draw.</p>
</td></tr>
<tr><td><code id="predict.pcr_+3A_kfold">kfold</code></td>
<td>
<p>numeric; the number of folds to split data into.</p>
</td></tr>
<tr><td><code id="predict.pcr_+3A_folds">folds</code></td>
<td>
<p>numeric; the number of repetitions of k-fold CV.</p>
</td></tr>
<tr><td><code id="predict.pcr_+3A_...">...</code></td>
<td>
<p>arguments passed to other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>predict.pcr</code> arranges for any transformation applied to the
training data to be applied to the <code>newdata</code> prior to
prediction.
</p>


<h3>Value</h3>

<p>A matrix of predicted values with rows representing samples in
<code>newdata</code> and columns, the PCR components requested via
<code>ncomp</code>.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>See Also</h3>

<p><code><a href="#topic+pcr">pcr</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Load the Imbrie &amp; Kipp data and
## summer sea-surface temperatures
data(ImbrieKipp)
data(SumSST)

## choose 10 samples to act as a test set, for illustration
take &lt;- c(5,58,31,51,42,28,30,57,8,50)

## normal interface and apply Hellinger transformation
mod &lt;- pcr(ImbrieKipp[-take, ], SumSST[-take], tranFun = Hellinger)

## predictions
predict(mod, ImbrieKipp[take, ], ncomp = 4)

## predictions
set.seed(123)
predict(mod, ImbrieKipp[take, ], ncomp = 4, CV = "bootstrap",
        nboot = 100)

</code></pre>

<hr>
<h2 id='predict.prcurve'>Predict new locations &amp; fitted values on a principal curve</h2><span id='topic+predict.prcurve'></span><span id='topic+fitted.prcurve'></span>

<h3>Description</h3>

<p>Locations on a fitted principal curve are predicted by projecting the
new observations in <code class="reqn">m</code> dimensions on to the corresponding closest
point on the curve. Fitted values for data used to fit the curve are
available with respect to the principal curve or to the individual
smooth functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'prcurve'
predict(object, newdata, ...)

## S3 method for class 'prcurve'
fitted(object, type = c("curve","smooths"), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.prcurve_+3A_object">object</code></td>
<td>

<p>an object of class <code><a href="#topic+prcurve">prcurve</a></code>.
</p>
</td></tr>
<tr><td><code id="predict.prcurve_+3A_newdata">newdata</code></td>
<td>

<p>a matrix or data frame of new observations within the space of the
orginal data. Variables are matched against those of the original
data via their <code>names</code> or <code>colnames</code>. If a data frame is
supplied, it is converted to a matrix via <code><a href="base.html#topic+data.matrix">data.matrix</a></code>.
</p>
</td></tr>
<tr><td><code id="predict.prcurve_+3A_type">type</code></td>
<td>

<p>character; the type of fitted values to return.
</p>
</td></tr>
<tr><td><code id="predict.prcurve_+3A_...">...</code></td>
<td>

<p>other arguments passed to other methods. Not currently used.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Fitting a principal curve involves two procedures. In one, the current
curve is bent towards the data via the fitting of spline functions
with distance along the curve as the predictor variable and each
variable in turn as the response. The second procedure, a projection
step, involves projecting the observed points in <code class="reqn">m</code> dimensions on
to locations along the current curve to which they are closest in the
hyperspace.
</p>
<p>Given a fitted curve, the projection step can be used to find new
points on the fitted curve by projecting the new points located in the
hyperspace on to points on the curve to which they are closest.
</p>
<p>Fitted values are available for the data used to the fit the principal
curve. There are two types of fitted value available. For <code>type =
  "curve"</code>, the fitted locations on the principal curve. For <code>type
  = "smooths"</code>, the fitted values of the variables from the individual
smooth functions with respect to distance along the principal curve.
</p>


<h3>Value</h3>

<p>A matrix of points in the space of the original data. Rows correspond
to the new samples and columns to the variables (ordered as per the
original data used to fit the curve).
</p>
<p>How these points are ordered along the fitted curve is contained in
attributed <code>tag</code>.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+prcurve">prcurve</a></code> for details on fitting principal curves and
an example.
</p>

<hr>
<h2 id='predict.wa'>Predict from a weighted average model</h2><span id='topic+predict.wa'></span><span id='topic+print.predict.wa'></span>

<h3>Description</h3>

<p>Model predictions and cross-validation predictions for weighted
averaging transfer function models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'wa'
predict(object, newdata,
        CV = c("none", "LOO", "bootstrap", "nfold"),
        verbose = FALSE, n.boot = 100, nfold = 5, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.wa_+3A_object">object</code></td>
<td>
<p>an object of class <code>"wa"</code>, usually the result of a
call to <code><a href="#topic+wa">wa</a></code></p>
</td></tr>
<tr><td><code id="predict.wa_+3A_newdata">newdata</code></td>
<td>
<p>An optional data frame in which to look for variables
with which to predict. If omitted, the fitted values are used.</p>
</td></tr>
<tr><td><code id="predict.wa_+3A_cv">CV</code></td>
<td>
<p>Should cross-validation be performed? Leave-one-out
(<code>"LOO"</code>), bootstrap (<code>"bootstrap"</code>) and <code class="reqn">k</code>-fold
(<code>"nfold"</code>) CV are currently available.</p>
</td></tr>
<tr><td><code id="predict.wa_+3A_verbose">verbose</code></td>
<td>
<p>Should CV progress be printed to the console?</p>
</td></tr>
<tr><td><code id="predict.wa_+3A_n.boot">n.boot</code></td>
<td>
<p>The number of bootstrap samples or <code class="reqn">k</code>-fold steps.</p>
</td></tr>
<tr><td><code id="predict.wa_+3A_nfold">nfold</code></td>
<td>
<p>Number of subsets in <code class="reqn">k</code>-fold CV.</p>
</td></tr>
<tr><td><code id="predict.wa_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Not all CV methods produce the same output. <code>CV = "bootstrap"</code> and
<code>CV = "nfold"</code> produce sample specific errors.
</p>


<h3>Value</h3>

<p>An object of class <code>"predict.wa"</code>, a list with the following
components:
</p>
<table role = "presentation">
<tr><td><code>pred</code></td>
<td>
<p>A list with components <code>pred</code> and <code>rmsep</code>
containing the predicted values and the sample specific errors if
available.</p>
</td></tr>
<tr><td><code>performance</code></td>
<td>
<p>A list with model performance statistics.</p>
</td></tr>
<tr><td><code>model.pred</code></td>
<td>
<p>A list with components <code>pred</code> and <code>rmsep</code>
containing the predicted values for the training set samples and the
sample specific errors if available.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matched function call.</p>
</td></tr>
<tr><td><code>CV.method</code></td>
<td>
<p>The CV method used.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Gavin L. Simpson and Jari Oksanen (<code class="reqn">k</code>-fold CV)</p>


<h3>References</h3>

<p>Birks, H.J.B., Line, J.M., Juggins, S., Stevenson, A.C. and ter Braak,
C.J.F. (1990). Diatoms and pH reconstruction. <em>Philosophical
Transactions of the Royal Society of London; Series B</em>, <strong>327</strong>;
263&ndash;278.</p>


<h3>See Also</h3>

<p><code><a href="#topic+wa">wa</a></code>, <code><a href="#topic+predict.mat">predict.mat</a></code>,
<code><a href="#topic+performance">performance</a></code>, <code><a href="#topic+reconPlot">reconPlot</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Imbrie and Kipp
data(ImbrieKipp)
ImbrieKipp &lt;- ImbrieKipp / 100
data(SumSST)
ik.wa &lt;- wa(SumSST ~ ., data = ImbrieKipp, tol.dw = TRUE,
            min.tol = 2, small.tol = "min")
ik.wa

## load V12.122 core data
data(V12.122)
V12.122 &lt;- V12.122 / 100

## predict summer sea-surface temperature for V12.122 core
set.seed(2)
v12.pred &lt;- predict(ik.wa, V12.122, CV = "bootstrap", n.boot = 100)

## draw the fitted reconstruction
reconPlot(v12.pred, use.labels = TRUE, display = "bars")

## extract the model performance stats
performance(v12.pred)

</code></pre>

<hr>
<h2 id='rankDC'>Rank correlation between environmental and species dissimilarities.</h2><span id='topic+rankDC'></span><span id='topic+print.rankDC'></span><span id='topic+plot.rankDC'></span><span id='topic+dotplot.rankDC'></span><span id='topic+dotplot'></span>

<h3>Description</h3>

<p>Computes the rank correlation between distances between sites in terms
of gradient variables, such as environmental ones, and species
composition.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rankDC(g, y, dc = c("chord", "bray", "euclidean", "chi.square", "gower"),
       method = "spearman")

## S3 method for class 'rankDC'
plot(x, sort = TRUE, decreasing = FALSE,
     xlab = "Rank correlation", color = "blue",
     pch = 21, bg = "blue", lcolor = "grey",
     lty = "solid", ...)

## S3 method for class 'rankDC'
dotplot(x, data = NULL, sort = TRUE, decreasing = FALSE,
     xlab = "Rank correlation", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rankDC_+3A_g">g</code></td>
<td>
<p>the gradient values; usually a data frame of environmental
data.</p>
</td></tr>
<tr><td><code id="rankDC_+3A_y">y</code></td>
<td>
<p>the species data; usually a data frame.</p>
</td></tr>
<tr><td><code id="rankDC_+3A_dc">dc</code></td>
<td>
<p>character; the set of dissimilarity coefficients for which
rank correlations with gradient distance are to be computed.</p>
</td></tr>
<tr><td><code id="rankDC_+3A_method">method</code></td>
<td>
<p>character; the correlation method to use. See the
<code>method</code> argument of <code><a href="stats.html#topic+cor">cor</a></code>.</p>
</td></tr>
<tr><td><code id="rankDC_+3A_x">x</code></td>
<td>
<p>an object of class <code>"rankDC"</code>.</p>
</td></tr>
<tr><td><code id="rankDC_+3A_data">data</code></td>
<td>
<p>NULL; ignored, only present for purposes of conformance to
generic definition.</p>
</td></tr>
<tr><td><code id="rankDC_+3A_sort">sort</code>, <code id="rankDC_+3A_decreasing">decreasing</code></td>
<td>
<p>logical; should observations be sorted prior
to plotting? If so, should they be sorted in order of decreasing
size?</p>
</td></tr>
<tr><td><code id="rankDC_+3A_xlab">xlab</code></td>
<td>
<p>The x-axis label for the plot.</p>
</td></tr>
<tr><td><code id="rankDC_+3A_color">color</code>, <code id="rankDC_+3A_pch">pch</code>, <code id="rankDC_+3A_bg">bg</code>, <code id="rankDC_+3A_lcolor">lcolor</code>, <code id="rankDC_+3A_lty">lty</code></td>
<td>
<p>arguments passed to
<code><a href="graphics.html#topic+dotchart">dotchart</a></code>.</p>
</td></tr>
<tr><td><code id="rankDC_+3A_...">...</code></td>
<td>
<p>arguments passed to other methods, including
<code><a href="graphics.html#topic+dotchart">dotchart</a></code> and <code><a href="lattice.html#topic+dotplot">dotplot</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named vector of rank correlations is returned.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson, based on <code><a href="vegan.html#topic+rankindex">rankindex</a></code> from the
vegan package.</p>


<h3>See Also</h3>

<p><code><a href="vegan.html#topic+rankindex">rankindex</a></code> from package vegan. For the
<code>dotplot</code> method, see <code><a href="lattice.html#topic+dotplot">dotplot</a></code>. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(swappH)
data(swapdiat)

rc &lt;- rankDC(swappH, swapdiat, dc = c("chord","euclidean","gower"))

## base plot uses dotchart()
plot(rc)

## lattice version of the base plot
dotplot(rc)

</code></pre>

<hr>
<h2 id='reconPlot'>Stratigraphic plots of palaeoenvironmental reconstructions</h2><span id='topic+reconPlot'></span><span id='topic+reconPlot.default'></span><span id='topic+reconPlot.predict.mat'></span><span id='topic+reconPlot.predict.wa'></span>

<h3>Description</h3>

<p>Draws a palaeoenvironmental reconstruction of predicted environmental
values for sub-fossil assemblages.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
reconPlot(x, ...)

## Default S3 method:
reconPlot(x, depths, errors,
          display.error = c("none", "bars", "lines"),
          rev.x = TRUE,
          col.error = "grey", lty.error = "dashed",
          type = "l",
          xlim, ylim,
          xlab = "", ylab = "", main = "",
          ...)

## S3 method for class 'predict.mat'
reconPlot(x, depths, use.labels = FALSE,
          predictions = c("model", "bootstrap"),
          display.error = c("none", "bars", "lines"),
          sample.specific = TRUE, ...)

## S3 method for class 'predict.wa'
reconPlot(x, depths, use.labels = FALSE,
          display.error = c("none", "bars", "lines"),
          sample.specific = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="reconPlot_+3A_x">x</code></td>
<td>
<p>An R object.</p>
</td></tr>
<tr><td><code id="reconPlot_+3A_depths">depths</code></td>
<td>
<p>numeric; a vector of depths for which predicted values
exist or will be generated. Can be missing, in which case,
<strong>if</strong> <code>use.labels = TRUE</code>, the function will attempt to
derive suitable values for you. See Details below.</p>
</td></tr>
<tr><td><code id="reconPlot_+3A_errors">errors</code></td>
<td>
<p>numeric; a vector of errors for plotting error bars or
lines.</p>
</td></tr>
<tr><td><code id="reconPlot_+3A_display.error">display.error</code></td>
<td>
<p>character; hown should error bars be drawn on the
plot? One of <code>"none"</code>, <code>"bars"</code>, or <code>"lines"</code>. If
<code>"bars"</code>, error bars are drawn for each sample. If
<code>"lines"</code>, lines are drawn enclosing the region prediction +/-
RMSEP.</p>
</td></tr>
<tr><td><code id="reconPlot_+3A_rev.x">rev.x</code></td>
<td>
<p>logical; should the depth/age axis be reversed (drawn
from high to low)?</p>
</td></tr>
<tr><td><code id="reconPlot_+3A_col.error">col.error</code>, <code id="reconPlot_+3A_lty.error">lty.error</code></td>
<td>
<p>the colour and type of line drawn. See
<code><a href="graphics.html#topic+par">par</a></code> and arguments <code>"col"</code> and <code>"lty"</code>.</p>
</td></tr>
<tr><td><code id="reconPlot_+3A_type">type</code></td>
<td>
<p>type of line drawn. See <code><a href="graphics.html#topic+par">par</a></code> and argument
<code>"type"</code>.</p>
</td></tr>
<tr><td><code id="reconPlot_+3A_xlab">xlab</code>, <code id="reconPlot_+3A_ylab">ylab</code></td>
<td>
<p>character; the x- and y-axis labels respectively.</p>
</td></tr>
<tr><td><code id="reconPlot_+3A_main">main</code></td>
<td>
<p>character; main title for the plot.</p>
</td></tr>
<tr><td><code id="reconPlot_+3A_xlim">xlim</code>, <code id="reconPlot_+3A_ylim">ylim</code></td>
<td>
<p>numeric, length 2; the x- and y-limits for the
plotted axes. If not provided, the function will calculate
appropriate values to cover the range of plotted values and any
error bars (if requested via <code>"error.bars"</code>.</p>
</td></tr>
<tr><td><code id="reconPlot_+3A_use.labels">use.labels</code></td>
<td>
<p>logical; should <code>reconPlot</code> attempt to derive
values for argument <code>depths</code> from the names of the predicted
values? Only use if <code>depths</code> is missing. See Details below.</p>
</td></tr>
<tr><td><code id="reconPlot_+3A_predictions">predictions</code></td>
<td>
<p>character; one of <code>"model"</code> or
<code>"bootstrap"</code>. Which type of predicted values should be
plotted? The model predictions (<code>"model"</code>) or the
bootstrap-derived predictions (<code>"bootstrap"</code>).</p>
</td></tr>
<tr><td><code id="reconPlot_+3A_sample.specific">sample.specific</code></td>
<td>
<p>logical; should sample specific errors be used?
Only for <code>predictions = "bootstrap"</code>.</p>
</td></tr>
<tr><td><code id="reconPlot_+3A_...">...</code></td>
<td>
<p>arguments passed to other graphics functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Conventionally, these plots are drawn on a depth or an age
scale. Argument <code>depths</code> is used to provide the depth or age
axis, against which the predicted values are plotted.
</p>
<p>If <code>depths</code> is not provided, then the function will try to
derive the appropriate values from the labels of the predictions if
<code>use.labels = TRUE</code>. You must provide <code>depths</code> or set
<code>use.labels = TRUE</code> otherwise an error will result. The derived
labels will be coerced to numerics. If your labels are not coercible,
then you'll either get nonsense on the plot or an error from R. If so,
provide suitable values for <code>depths</code>.
</p>


<h3>Value</h3>

<p>A plot on the currently active device.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson </p>


<h3>See Also</h3>

<p><code><a href="#topic+mat">mat</a></code>, and <code><a href="#topic+predict.mat">predict.mat</a></code> for MAT
transfer functions and <code><a href="#topic+wa">wa</a></code> and <code><a href="#topic+predict.wa">predict.wa</a></code>
for WA models.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Imbrie and Kipp example
## load the example data
data(ImbrieKipp)
data(SumSST)
data(V12.122)

## merge training and test set on columns
dat &lt;- join(ImbrieKipp, V12.122, verbose = TRUE)

## extract the merged data sets and convert to proportions
ImbrieKipp &lt;- dat[[1]] / 100
V12.122 &lt;- dat[[2]] / 100

## Fit a MAT model
(ik.mat &lt;- mat(ImbrieKipp, SumSST, method = "chord"))

## Reconstruct pH for the RLGH core
v12.pH &lt;- predict(ik.mat, V12.122)

## draw the reconstruction
reconPlot(v12.pH, use.labels = TRUE, display.error = "bars",
          xlab = "Depth", ylab = "Summer Seas-surface Temperature")
</code></pre>

<hr>
<h2 id='residLen'>Squared residual length diagnostics</h2><span id='topic+residLen'></span><span id='topic+print.residLen'></span><span id='topic+fittedY'></span><span id='topic+sqrlLinear'></span><span id='topic+sqrlUnimodal'></span>

<h3>Description</h3>

<p>The squared residual length between the fitted values of a constrained
ordination and the original species data is one diagnostic for
transfer function models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>residLen(X, env, passive, method = c("cca","rda"))

fittedY(ord, newdata, colsum)

sqrlLinear(Y, fitted)

sqrlUnimodal(Y, colsum, fitted)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="residLen_+3A_x">X</code></td>
<td>
<p>data frame; the training set species data.</p>
</td></tr>
<tr><td><code id="residLen_+3A_env">env</code></td>
<td>
<p>vector; the training set environmental data.</p>
</td></tr>
<tr><td><code id="residLen_+3A_passive">passive</code></td>
<td>
<p>data frame; the passive samples species data.</p>
</td></tr>
<tr><td><code id="residLen_+3A_method">method</code></td>
<td>
<p>the ordination technique to use. One of <code>"rda"</code> or
<code>"cca"</code>, with the latter the default.</p>
</td></tr>
<tr><td><code id="residLen_+3A_ord">ord</code></td>
<td>
<p>an ordination object, the result of a call to
<code><a href="vegan.html#topic+cca">cca</a></code> or <code><a href="vegan.html#topic+rda">rda</a></code>.</p>
</td></tr>
<tr><td><code id="residLen_+3A_newdata">newdata</code></td>
<td>
<p>Species data matrix for passive samples. Must have same
columns as data used to fit <code>ord</code>.</p>
</td></tr>
<tr><td><code id="residLen_+3A_colsum">colsum</code></td>
<td>
<p>column (species) sums for training set data used to fit
<code>ord</code>.</p>
</td></tr>
<tr><td><code id="residLen_+3A_y">Y</code></td>
<td>
<p>Original species data matrix, the response for which squared
residual lengths are to be computed.</p>
</td></tr>
<tr><td><code id="residLen_+3A_fitted">fitted</code></td>
<td>
<p>The fitted values of the response derived from the
constrained ordination model.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The squared residual lengths are computed for the training set samples
and the passive samples separately. Passive samples that are poorly
fitted in the transfer function model will have large squared residual
distances between the observed species data and the fitted values from
the constrained ordination.
</p>
<p><code>residLen</code> is the main user-interface function and can be called
with either the training data and passive samples.
</p>
<p><code>fittedY</code> returns the fitted approximation of the passive sample
response data (i.e. species data). <code>sqrlLinear</code> and
<code>sqrlUnimodal</code> return the squared residual distances between the
observed species data and the fitted values from the constrained
ordination model.
</p>


<h3>Value</h3>

<p><code>fittedY</code> returns a matrix of fitted species abundances for
passive samples.
</p>
<p><code>sqrlLinear</code> and <code>sqrlUnimodal</code> return a vector of
residual distances.
</p>
<p><code>residLen</code> returns an object of class <code>"residLen"</code> with the
attribute <code>"method"</code> set to <code>"method"</code>. This object is a
list with the following components:
</p>
<table role = "presentation">
<tr><td><code>train</code>, <code>passive</code></td>
<td>
<p>The squared residual lengths for the training
set and the passive samples.</p>
</td></tr>
<tr><td><code>ordination</code></td>
<td>
<p>The fitted ordination.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>References</h3>

<p>Ter Braak C.J.F. and Smilauer P. (2002) CANOCO Reference manual and
CanoDraw for Windows User's guide: Software for Canonical Ordination
(version 4.5). Microcomputer Power (Ithaca, NY, USA), 500pp.
</p>


<h3>See Also</h3>

<p><code><a href="vegan.html#topic+cca">cca</a></code> and <code><a href="vegan.html#topic+predict.cca">predict.cca</a></code> for some
of the underlying computations.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load the Imbrie and Kipp example data
data(ImbrieKipp, SumSST, V12.122)

## squared residual lengths for Core V12.122
rlens &lt;- residLen(ImbrieKipp, SumSST, V12.122)
rlens

## as before but using linear RDA
residLen(ImbrieKipp, SumSST, V12.122, method = "rda")
</code></pre>

<hr>
<h2 id='residuals.prcurve'>
Residuals of a principal curve fit.
</h2><span id='topic+residuals.prcurve'></span><span id='topic+resid.prcurve'></span>

<h3>Description</h3>

<p>Returns various representations of the residuals of a principal curve
fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'prcurve'
residuals(object, which = c("distance", "raw", "smooths", "pca"),
          ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="residuals.prcurve_+3A_object">object</code></td>
<td>
<p>an object of class <code>"prcurve"</code>, the result of a
call to <code><a href="#topic+prcurve">prcurve</a></code>.</p>
</td></tr>
<tr><td><code id="residuals.prcurve_+3A_which">which</code></td>
<td>
<p>character; the type of residuals to return. See Details.</p>
</td></tr>
<tr><td><code id="residuals.prcurve_+3A_...">...</code></td>
<td>
<p>arguments passed to other methods. See Details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Various types of residual are available for the principal curve. In a
departure from the usual convention, which residuals are returned is
controlled via the <code>which</code> argument. This is to allow users to
pass a <code>type</code> argument to the <code>residuals</code> method for the
function used to fit the individual smooth functions when <code>which
    = "smooths"</code>.
</p>
<p>The types of residuals available are
</p>

<dl>
<dt><code>"distance"</code></dt><dd><p>the default residual for a principal
curve. This residual is taken as the Euclidean distance between each
observations and the point on the principal curve to which it
projects, in full multivariate space.</p>
</dd>
<dt><code>"raw"</code></dt><dd><p>raw residuals are the basis for
<code>"distance"</code> residuals, and are the difference between the
observed and fitted values (position on the curve) for each
observation in terms of each variable in the data set. These
residuals are in the form of a matrix with number of observation
<em>rows</em> and number of variables <em>cols</em>.</p>
</dd>
<dt><code>"smooths"</code></dt><dd><p>these residuals are the result of calling
<code>residuals()</code> on each of the smooth models fitted to the
individual variables. See below for further details. A matrix of
the same dimensions as for <code>which = "raw"</code> is returned.</p>
</dd>
<dt><code>"pca"</code></dt><dd><p>similar to the raw residuals, but expressed in
terms of the principal components of the input data. Hence these
residuals are the difference between each observation's location
in PCA space and their corresponding location on the curve.</p>
</dd>
</dl>

<p>For <code>"smooths"</code> residuals, what is returned is governed by the
<code>residuals</code> method available for the smooth model fitted to the
individual variables. For principal curves fitted using the
<code><a href="#topic+smoothSpline">smoothSpline</a></code> plugin, see
<code><a href="stats.html#topic+smooth.spline">smooth.spline</a></code>. For principal curves fitted via the
<code><a href="#topic+smoothGAM">smoothGAM</a></code> plugin, see
<code><a href="mgcv.html#topic+residuals.gam">residuals.gam</a></code>.
</p>
<p>... can be used to pass additional arguments to these
<code>residuals</code> methods. In particular, the <code>type</code> argument is
commonly used to choose which type of residual is returned by the
specific methods.
</p>
<p>In the case of principal curves fitted using the plugin
<code><a href="#topic+smoothSpline">smoothSpline</a></code>, residuals for <code>which = "smooths"</code> are
only available if the the additional argument <code>keep.data</code> was
specified during fitting via <code><a href="#topic+prcurve">prcurve</a></code>. See the examples
for an illustration of this usage.
</p>


<h3>Value</h3>

<p>A vector of residual distances (<code>which = "distance"</code>) or a matrix
of residuals (for the other options).
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>See Also</h3>

<p><code><a href="#topic+prcurve">prcurve</a></code> for fitting a principal curve.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  
  ## Load Abernethy Forest data set
  data(abernethy)

  ## Remove the Depth and Age variables
  abernethy2 &lt;- abernethy[, -(37:38)]
  
  ## Fit the principal curve, preserving the data in the smooth.spline
  ## smooth functions fitted via keep.data = TRUE
  aber.pc &lt;- prcurve(abernethy2, method = "ca", keep.data = TRUE)

  ## default "distance" residuals
  res &lt;- resid(aber.pc)
  head(res)

  ## residuals from the underlying smooth models, also illustrates
  ## how to select specific types of residual from the individual
  ## method using argument 'type'
  res &lt;- resid(aber.pc, which = "smooths", type = "deviance")
  dim(res)
  head(res[, 1:5])		# just show a few species residuals
  
</code></pre>

<hr>
<h2 id='rlgh'>Round Loch of Glenhead Diatoms</h2><span id='topic+rlgh'></span>

<h3>Description</h3>

<p>Diatom sub-fossil assemblage counts from the Round Loch of Glenhead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(rlgh)</code></pre>


<h3>Format</h3>

<p>A data frame with 101 observations on 139 diatom species.
</p>


<h3>Details</h3>

<p>The samples are taken from various depths down a sediment core
retrieved from the Round Loch of Glenhead, Galloway, Scotland.
</p>


<h3>References</h3>

<p>Jones, V.J., Stevenson, A.C. and Battarbee, R.W. (1989) Acidification
of lakes in Galloway, south west Scotland &mdash; A diatom and pollen
study of the post-glacial history of the Round Loch of
Glenhead. <em>Journal of Ecology</em> <strong>77(1)</strong>, 1&ndash;23.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(rlgh)
</code></pre>

<hr>
<h2 id='RMSEP'>Root mean square error of prediction</h2><span id='topic+RMSEP'></span><span id='topic+RMSEP.default'></span><span id='topic+RMSEP.mat'></span><span id='topic+RMSEP.bootstrap.mat'></span><span id='topic+RMSEP.bootstrap.wa'></span>

<h3>Description</h3>

<p>Calculates or extracts the RMSEP from transfer function models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RMSEP(object, ...)

## S3 method for class 'mat'
RMSEP(object, k, weighted = FALSE,
          ...)

## S3 method for class 'bootstrap.mat'
RMSEP(object, type = c("birks1990", "standard"),
          ...)

## S3 method for class 'bootstrap.wa'
RMSEP(object, type = c("birks1990", "standard"),
          ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="RMSEP_+3A_object">object</code></td>
<td>
<p>An R object.</p>
</td></tr>
<tr><td><code id="RMSEP_+3A_k">k</code></td>
<td>
<p>numeric; the number of analogues to use in calculating the
RMSEP. May be missing. If missing, <code>k</code> is extracted from the
model using <code><a href="#topic+getK">getK</a></code>.</p>
</td></tr>
<tr><td><code id="RMSEP_+3A_weighted">weighted</code></td>
<td>
<p>logical; Return the RMSEP for the weighted or
unweighted model? The default is for an unweighted model.</p>
</td></tr>
<tr><td><code id="RMSEP_+3A_type">type</code></td>
<td>
<p>The type of RMSEP to return/calculate. See Details,
below.</p>
</td></tr>
<tr><td><code id="RMSEP_+3A_...">...</code></td>
<td>
<p>Arguments passed to other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are two forms of RMSEP in common usage. Within palaeoecology,
the RMSEP of Birks et al. (1990) is most familiar:
</p>
<p style="text-align: center;"><code class="reqn">\mathrm{RMSEP} = \sqrt{s_1^2 + s_2^2}</code>
</p>

<p>where where <code class="reqn">s_1</code> is the standard deviation of the
out-of-bag (OOB) residuals and <code class="reqn">s_2</code> is the mean bias or the
mean of the OOB residuals.
</p>
<p>In the wider statistical literature, the following form of RMSEP is
more commonly used:
</p>
<p style="text-align: center;"><code class="reqn">\mathrm{RMSEP} = \sqrt{\frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{n}}</code>
</p>

<p>where <code class="reqn">y_i</code> are the observed values and <code class="reqn">\hat{y}_i</code> the
transfer function predictions/fitted values.
</p>
<p>The first form of RMSEP is returned by default or if <code>type =
    "birks1990"</code> is supplied. The latter form is returned if <code>type
    = "standard"</code> is supplied.
</p>
<p>The RMSEP for objects of class <code>"mat"</code> is a leave-one-out
cross-validated RMSEP, and is calculated as for <code>type =
    "standard"</code>.
</p>


<h3>Value</h3>

<p>A numeric vector of length 1 that is the RMSEP of <code>object</code>.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>References</h3>

<p>Birks, H.J.B., Line, J.M., Juggins, S., Stevenson, A.C. and ter Braak,
C.J.F. (1990). Diatoms and pH reconstruction. <em>Philosophical
Transactions of the Royal Society of London; Series B</em>, <strong>327</strong>;
263&ndash;278.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mat">mat</a></code>, <code><a href="#topic+bootstrap">bootstrap</a></code>, <code><a href="#topic+wa">wa</a></code>,
<code><a href="#topic+bootstrap.wa">bootstrap.wa</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Imbrie and Kipp example
## load the example data
data(ImbrieKipp)
data(SumSST)
data(V12.122)

## merge training and test set on columns
dat &lt;- join(ImbrieKipp, V12.122, verbose = TRUE)

## extract the merged data sets and convert to proportions
ImbrieKipp &lt;- dat[[1]] / 100
V12.122 &lt;- dat[[2]] / 100

## fit the MAT model using the squared chord distance measure
(ik.mat &lt;- mat(ImbrieKipp, SumSST, method = "chord"))

## Leave-one-out RMSEP for the MAT model
RMSEP(ik.mat)

## bootstrap training set
(ik.boot &lt;- bootstrap(ik.mat, n.boot = 100))

## extract the Birks et al (1990) RMSEP
RMSEP(ik.boot)

## Calculate the alternative formulation
RMSEP(ik.boot, type = "standard")

</code></pre>

<hr>
<h2 id='roc'>ROC curve analysis</h2><span id='topic+roc'></span><span id='topic+roc.default'></span><span id='topic+roc.mat'></span><span id='topic+roc.analog'></span><span id='topic+print.roc'></span><span id='topic+summary.roc'></span><span id='topic+print.summary.roc'></span>

<h3>Description</h3>

<p>Fits Receiver Operator Characteristic (ROC) curves to training set
data. Used to determine the critical value of a dissimilarity
coefficient that best descriminate between assemblage-types in
palaeoecological data sets, whilst minimising the false positive
error rate (FPF).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>roc(object, groups, k = 1, ...)

## Default S3 method:
roc(object, groups, k = 1, thin = FALSE,
    max.len = 10000, ...)

## S3 method for class 'mat'
roc(object, groups, k = 1, ...)

## S3 method for class 'analog'
roc(object, groups, k = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="roc_+3A_object">object</code></td>
<td>
<p>an R object.</p>
</td></tr>
<tr><td><code id="roc_+3A_groups">groups</code></td>
<td>
<p>a vector of group memberships, one entry per
sample in the training set data. Can be a factor, and will be
coerced to one if supplied vecvtor is not a factor.</p>
</td></tr>
<tr><td><code id="roc_+3A_k">k</code></td>
<td>
<p>numeric; the <code>k</code> closest analogues to use to calculate
ROC curves.</p>
</td></tr>
<tr><td><code id="roc_+3A_thin">thin</code></td>
<td>
<p>logical; should the points on the ROC curve be thinned?
See Details, below.</p>
</td></tr>
<tr><td><code id="roc_+3A_max.len">max.len</code></td>
<td>
<p>numeric; length of analolgue and non-analogue
vectors. Used as limit to thin points on ROC curve to.</p>
</td></tr>
<tr><td><code id="roc_+3A_...">...</code></td>
<td>
<p>arguments passed to/from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A ROC curve is generated from the within-group and between-group
dissimilarities.
</p>
<p>For each level of the grouping vector (<code>groups</code>) the
dissimilarity between each group member and it's k closest analogues
within that group are compared with the k closest dissimilarities
between the non-group member and group member samples.
</p>
<p>If one is able to discriminate between members of different group on
the basis of assemblage dissimilarity, then the dissimilarities
between samples within a group will be small compared to the
dissimilarities between group members and non group members.
</p>
<p><code>thin</code> is useful for large problems, where the number of analogue
and non-analogue distances can conceivably be large and thus overflow
the largest number R can work with. This option is also useful to
speed up computations for large problems. If <code>thin == TRUE</code>, then
the larger of the analogue or non-analogue distances is thinned to a
maximum length of <code>max.len</code>. The smaller set of distances is
scaled proportionally. In thinning, we approximate the distribution of
distances by taking <code>max.len</code> (or a fraction of <code>max.len</code>
for the smaller set of distances) equally-spaced probability
quantiles of the distribution as a new set of distances. 
</p>


<h3>Value</h3>

<p>A list with two components; i, <code>statistics</code>, a summary of ROC
statistics for each level of <code>groups</code> and a combined ROC
analysis, and ii, <code>roc</code>, a list of ROC objects, one per level of
<code>groups</code>. For the latter, each ROC object is a list, with the
following components: 
</p>
<table role = "presentation">
<tr><td><code>TPF</code></td>
<td>
<p>The true positive fraction.</p>
</td></tr>
<tr><td><code>FPE</code></td>
<td>
<p>The false positive error</p>
</td></tr>


<tr><td><code>optimal</code></td>
<td>
<p>The optimal dissimilarity value, asessed where the
slope of the ROC curve is maximal.</p>
</td></tr>
<tr><td><code>AUC</code></td>
<td>
<p>The area under the ROC curve.</p>
</td></tr>
<tr><td><code>se.fit</code></td>
<td>
<p>Standard error of the AUC estimate.</p>
</td></tr>
<tr><td><code>n.in</code></td>
<td>
<p>numeric; the number of samples within the current group.</p>
</td></tr>
<tr><td><code>n.out</code></td>
<td>
<p>numeric; the number of samples not in the current group.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>The p-value of a Wilcoxon rank sum test on the two sets
of dissimilarities. This is also known as a Mann-Whitney test.</p>
</td></tr>
<tr><td><code>roc.points</code></td>
<td>
<p>The unique dissimilarities at which the ROC curve
was evaluated</p>
</td></tr>
<tr><td><code>max.roc</code></td>
<td>
<p>numeric; the position along the ROC curve at which the
slope of the ROC curve is maximal. This is the index of this point
on the curve.</p>
</td></tr>
<tr><td><code>prior</code></td>
<td>
<p>numeric, length 2. Vector of observed prior probabilities
of true analogue and true non-analogues in the group.</p>
</td></tr>
<tr><td><code>analogue</code></td>
<td>
<p>a list with components <code>yes</code> and <code>no</code>
containing the dissimilarities for true analogue and true
non-analogues in the group.</p>
</td></tr>





</table>


<h3>Author(s)</h3>

<p>Gavin L. Simpson, based on code from Thomas Lumley to optimise
the calculation of the ROC curve.</p>


<h3>References</h3>

<p>Brown, C.D., and Davis, H.T. (2006) Receiver operating characteristics
curves and related decision measures: A tutorial. <em>Chemometrics
and Intelligent Laboratory Systems</em> <b>80</b>, 24&ndash;38.
</p>
<p>Gavin, D.G., Oswald, W.W., Wahl, E.R. and Williams, J.W. (2003) A
statistical approach to evaluating distance metrics and analog
assignments for pollen records. <em>Quaternary Research</em>
<strong>60</strong>, 356&ndash;367.
</p>
<p>Henderson, A.R. (1993) Assessing test accuracy and its clinical
consequences: a primer for receiver operating characteristic curve
analysis. <em>Annals of Clinical Biochemistry</em> <strong>30</strong>,
834&ndash;846.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mat">mat</a></code> for fitting of MAT models.
<code><a href="#topic+bootstrap.mat">bootstrap.mat</a></code> and <code><a href="#topic+mcarlo">mcarlo</a></code> for alternative
methods for selecting critical values of analogue-ness for
dissimilarity coefficients.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load the example data
data(swapdiat, swappH, rlgh)

## merge training and test set on columns
dat &lt;- join(swapdiat, rlgh, verbose = TRUE)

## extract the merged data sets and convert to proportions
swapdiat &lt;- dat[[1]] / 100
rlgh &lt;- dat[[2]] / 100

## fit an analogue matching (AM) model using the squared chord distance
## measure - need to keep the training set dissimilarities
swap.ana &lt;- analog(swapdiat, rlgh, method = "SQchord",
                   keep.train = TRUE)

## fit the ROC curve to the SWAP diatom data using the AM results
## Generate a grouping for the SWAP lakes
METHOD &lt;- if (getRversion() &lt; "3.1.0") {"ward"} else {"ward.D"}
clust &lt;- hclust(as.dist(swap.ana$train), method = METHOD)
grps &lt;- cutree(clust, 12)

## fit the ROC curve
swap.roc &lt;- roc(swap.ana, groups = grps)
swap.roc

## draw the ROC curve
plot(swap.roc, 1)

## draw the four default diagnostic plots
layout(matrix(1:4, ncol = 2))
plot(swap.roc)
layout(1)
</code></pre>

<hr>
<h2 id='scores.prcurve'><code><a href="vegan.html#topic+scores">scores</a></code> method for principal curve objects of
class <code>"prcurve"</code>.</h2><span id='topic+scores.prcurve'></span>

<h3>Description</h3>

<p>A <code><a href="vegan.html#topic+scores">scores</a></code> method to extract the position on the
curve to which each observation projects (<code>display = "curve"</code>) or
the coordinates of the curve in the dimensions of the input data
(<code>display = "dimensions"</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'prcurve'
scores(x, display = c("curve", "dimensions"), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="scores.prcurve_+3A_x">x</code></td>
<td>
<p>an object of class <code>"prcurve"</code>, usually from a call to
<code><a href="#topic+prcurve">prcurve</a></code>.</p>
</td></tr>
<tr><td><code id="scores.prcurve_+3A_display">display</code></td>
<td>
<p>character; which type of scores to
extract. <code>display = "curve"</code> returns the position along the curve
onto which each observation projects; this can be used like a PCA axis
score. <code>display = "dimensions"</code> returns the coordinates of the
curve in the dimensions of the original data.</p>
</td></tr>
<tr><td><code id="scores.prcurve_+3A_...">...</code></td>
<td>
<p>Arguments passed to other methods. Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If <code>display = "curve"</code> a 1-column matrix is returned with a row
for each observation in the input data. If <code>display =
  "dimensions"</code>, a matrix of coordinates for the principal curve is
returned. The dimensions of this matrix relate to the dimensions of
the input data; if there were <code class="reqn">n</code> samples (rows) and <code class="reqn">m</code>
variables (columns) then the matrix returned by <code>scores.prcurve</code>
will have <code class="reqn">n</code> rows and <code class="reqn">m</code> columns.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>See Also</h3>

<p><code><a href="#topic+prcurve">prcurve</a></code> for fitting principal curves to data.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Load the Abernethy Forest data set
data(abernethy)

## Remove the Depth and Age variables
abernethy2 &lt;- abernethy[, -(37:38)]

## Fit the principal curve using varying complexity of smoothers
## for each species
aber.pc &lt;- prcurve(abernethy2, method = "ca", trace = TRUE,
                   vary = TRUE, penalty = 1.4)

## Extract position on the curve
pos &lt;- scores(aber.pc, display = "curve")
head(pos)

## Extract the coordinates of the curve
coord &lt;- scores(aber.pc, display = "dimensions")
dim(coord)
all.equal(dim(coord), dim(abernethy2))
</code></pre>

<hr>
<h2 id='screeplot'>Screeplots of model results</h2><span id='topic+screeplot.mat'></span><span id='topic+screeplot.bootstrap.mat'></span>

<h3>Description</h3>

<p>Draws screeplots of performance statistics for models of varying complexity.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
## S3 method for class 'mat'
screeplot(x, k, restrict = 20,
          display = c("rmsep", "avg.bias",
                      "max.bias", "r.squared"),
          weighted = FALSE,  col = "red", xlab = NULL,
          ylab = NULL, main = NULL, sub = NULL, ...)

## S3 method for class 'bootstrap.mat'
screeplot(x, k, restrict = 20,
          display = c("rmsep","avg.bias","max.bias",
                      "r.squared"),
          legend = TRUE, loc.legend = "topright",
          col = c("red", "blue"),
          xlab = NULL, ylab = NULL,
          main = NULL, sub = NULL,
          ...,
          lty = c("solid","dashed"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="screeplot_+3A_x">x</code></td>
<td>
<p>object of class <code><a href="#topic+mat">mat</a></code> and <code>bootstrap.mat</code>.</p>
</td></tr>
<tr><td><code id="screeplot_+3A_k">k</code></td>
<td>
<p>number of analogues to use. If missing 'k' is chosen
automatically as the 'k' that achieves lowest RMSE.</p>
</td></tr> 
<tr><td><code id="screeplot_+3A_restrict">restrict</code></td>
<td>
<p>logical; restrict comparison of k-closest model to k
<code class="reqn">&lt;=</code> <code>restrict</code>.</p>
</td></tr>
<tr><td><code id="screeplot_+3A_display">display</code></td>
<td>
<p>which aspect of <code>x</code> to plot? Partial match.</p>
</td></tr>
<tr><td><code id="screeplot_+3A_weighted">weighted</code></td>
<td>
<p>logical; should the analysis use weighted mean of env
data of analogues as fitted/estimated values?</p>
</td></tr>
<tr><td><code id="screeplot_+3A_xlab">xlab</code>, <code id="screeplot_+3A_ylab">ylab</code></td>
<td>
<p>x- and y-axis labels respectively.</p>
</td></tr>
<tr><td><code id="screeplot_+3A_main">main</code>, <code id="screeplot_+3A_sub">sub</code></td>
<td>
<p>main and subtitle for the plot.</p>
</td></tr>
<tr><td><code id="screeplot_+3A_legend">legend</code></td>
<td>
<p>logical; should a legend be displayed on the figure?</p>
</td></tr>
<tr><td><code id="screeplot_+3A_loc.legend">loc.legend</code></td>
<td>
<p>character; a keyword for the location of the
legend. See <code><a href="graphics.html#topic+legend">legend</a></code> for details of allowed keywords.</p>
</td></tr>
<tr><td><code id="screeplot_+3A_col">col</code></td>
<td>
<p>Colours for lines drawn on the screeplot. Method for class
<code>"bootstrap.mat"</code> takes a vector of two colours.</p>
</td></tr>
<tr><td><code id="screeplot_+3A_lty">lty</code></td>
<td>
<p>vector detailing the line type to use in drawing the
screeplot of the apparent and bootstrap statistics,
respectively. Code currently assumes that <code>length(lty)</code> is 2.</p>
</td></tr>
<tr><td><code id="screeplot_+3A_...">...</code></td>
<td>
<p>arguments passed to other graphics functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Screeplots are often used to graphically show the results of
cross-validation or other estimate of model performance across a range
of model complexity.
</p>
<p>Four measures of model performance are currently available: i) root
mean square error of prediction (RMSEP); ii) average bias &mdash; the
mean of the model residuals; iii) maximum bias &mdash; the maximum average
bias calculated for each of <em>n</em> sections of the gradient of the
environmental variable; and v) model <code class="reqn">R^2</code>.
</p>
<p>For the maximum bias statistic, the response (environmental) gradient
is split into <em>n</em> = 10 sections.
</p>
<p>For the <code><a href="#topic+bootstrap">bootstrap</a></code> method, apparent and bootstrap
versions of these statistics are available and plotted.
</p>


<h3>Note</h3>

<p>Currently only models of class <code><a href="#topic+mat">mat</a></code> and
<code><a href="#topic+bootstrap.mat">bootstrap.mat</a></code> are supported.
</p>


<h3>Author(s)</h3>

<p>Gavin Simpson</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+screeplot">screeplot</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## Imbrie and Kipp example
## load the example data
data(ImbrieKipp)
data(SumSST)
data(V12.122)

## merge training and test set on columns
dat &lt;- join(ImbrieKipp, V12.122, verbose = TRUE)

## extract the merged data sets and convert to proportions
ImbrieKipp &lt;- dat[[1]] / 100
V12.122 &lt;- dat[[2]] / 100

## fit the MAT model using the chord distance measure
(ik.mat &lt;- mat(ImbrieKipp, SumSST, method = "chord"))

screeplot(ik.mat)
</code></pre>

<hr>
<h2 id='smoothers'>
Smoother plugin function for use in fitting a principal curve
</h2><span id='topic+smoothSpline'></span><span id='topic+smoothGAM'></span>

<h3>Description</h3>

<p>Functions to be used as plugins to <code><a href="#topic+prcurve">prcurve</a></code> that fit
smooth functions to each variable that, when combined, give the
principal curve. The functions act as wrappers to the main fitting
functions, which currently include <code><a href="stats.html#topic+smooth.spline">smooth.spline</a></code> and
<code><a href="mgcv.html#topic+gam">gam</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smoothSpline(lambda, x, choose = TRUE, complexity, ...,
             penalty = 1, cv = FALSE, keep.data = FALSE,
             control.spar = list(low = 0))

smoothGAM(lambda, x, choose = TRUE, complexity, bs = "tp", ...,
          family = gaussian(), method = "REML", select = FALSE,
          control = gam.control())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="smoothers_+3A_lambda">lambda</code></td>
<td>
<p>the current projection function; the position that each
sample projects to on the current principal curve. This is the
predictor variable or covariate in the smooth function.</p>
</td></tr>
<tr><td><code id="smoothers_+3A_x">x</code></td>
<td>
<p>numeric vector; used as the response variable in the smooth
function. The principal curve algorithm fits a separate scatterplot
smoother (or similar smoother) to each  variable in <code>X</code>
in turn as the response.</p>
</td></tr>
<tr><td><code id="smoothers_+3A_choose">choose</code></td>
<td>
<p>logical; should the underlying smoother function be
allowed to choose the degree of smooth complexity for each
variable?</p>
</td></tr>
<tr><td><code id="smoothers_+3A_complexity">complexity</code></td>
<td>
<p>numeric; the complexity of the fitted smooth
functions.</p>
</td></tr>
<tr><td><code id="smoothers_+3A_penalty">penalty</code>, <code id="smoothers_+3A_cv">cv</code>, <code id="smoothers_+3A_keep.data">keep.data</code>, <code id="smoothers_+3A_control.spar">control.spar</code></td>
<td>
<p>arguments to
<code><a href="stats.html#topic+smooth.spline">smooth.spline</a></code>.</p>
</td></tr>
<tr><td><code id="smoothers_+3A_bs">bs</code>, <code id="smoothers_+3A_family">family</code></td>
<td>
<p>arguments to <code><a href="mgcv.html#topic+s">s</a></code>.</p>
</td></tr>
<tr><td><code id="smoothers_+3A_method">method</code>, <code id="smoothers_+3A_select">select</code>, <code id="smoothers_+3A_control">control</code></td>
<td>
<p>arguments to <code><a href="mgcv.html#topic+gam">gam</a></code>.</p>
</td></tr>
<tr><td><code id="smoothers_+3A_...">...</code></td>
<td>
<p>arguments passed on the the underlying function
<code><a href="stats.html#topic+smooth.spline">smooth.spline</a></code> and users should read that function's
help page for further details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"prcurveSmoother"</code> with the following
components:
</p>
<table role = "presentation">
<tr><td><code>lambda</code></td>
<td>
<p>for each observations, its arc-length from the beginning
of the curve.</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>numeric vector of response values.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>numeric vector of fitted values for the
observations generated from the fitted smooth function.</p>
</td></tr>
<tr><td><code>complexity</code></td>
<td>
<p>numeric; the degrees of freedom used for the smooth
function. The exact details of what these pertain to are in the help
for the respective fitting functions <code><a href="stats.html#topic+smooth.spline">smooth.spline</a></code> and
<code><a href="mgcv.html#topic+gam">gam</a></code>.</p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p>the object fitted by the wrapped fitting function.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Gavin L. Simpson
</p>


<h3>See Also</h3>

<p><code><a href="#topic+prcurve">prcurve</a></code> for how these functions are used.
</p>

<hr>
<h2 id='splitSample'>
Select samples from along an environmental gradient
</h2><span id='topic+splitSample'></span>

<h3>Description</h3>

<p>Select samples from along an environmental gradient by splitting
the gradient into discrete chunks and sample within each chunk. This
allows a test set to be selected which covers the environmental
gradient of the training set, for example.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>splitSample(env, chunk = 10, take, nchunk,
            fill = c("head", "tail", "random"),
            maxit = 1000)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="splitSample_+3A_env">env</code></td>
<td>
<p>numeric; vector of samples representing the gradient values.</p>
</td></tr>
<tr><td><code id="splitSample_+3A_chunk">chunk</code></td>
<td>
<p>numeric; number of chunks to split the gradient into.</p>
</td></tr>
<tr><td><code id="splitSample_+3A_take">take</code></td>
<td>
<p>numeric; how many samples to take from the gradient. Can
not be missing.</p>
</td></tr>
<tr><td><code id="splitSample_+3A_nchunk">nchunk</code></td>
<td>
<p>numeric; number of samples per chunk. Must be a vector
of length <code>chunk</code> and <code>sum(chunk)</code> must equal
<code>take</code>. Can be missing (the default), in which case some simple
heuristics are used to determine the number of samples chosen per
chunk. See Details.</p>
</td></tr>
<tr><td><code id="splitSample_+3A_fill">fill</code></td>
<td>
<p>character; the type of filling of chunks to perform. See
Details.</p>
</td></tr>
<tr><td><code id="splitSample_+3A_maxit">maxit</code></td>
<td>
<p>numeric; maximum number of iterations in which to try to
sample <code>take</code> observations. Basically here to stop the loop
going on forever.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The gradient is split into <code>chunk</code> sections and samples are
selected from each chunk to result in a sample of length
<code>take</code>. If <code>take</code> is divisible by <code>chunk</code> without
remainder then there will an equal number of samples selected from
each chunk. Where <code>chunk</code> is not a multiple of <code>take</code> and
<code>nchunk</code> is not specified then extra samples have to be allocated
to some of the chunks to reach the required number of samples
selected.
</p>
<p>An additional complication is that some chunks of the gradient may
have fewer than <code>nchunk</code> samples and therefore more samples need
to be selected from the remaining chunks until <code>take</code> samples are
chosen.
</p>
<p>If <code>nchunk</code> is supplied, it must be a vector stating exactly how
many samples to select from each chunk. If <code>chunk</code> is not
supplied, then the number of samples per chunk is determined as
follows:
</p>

<ol>
<li><p> An intial allocation of <code>floor(take / chunk)</code> is assigned
to each chunk
</p>
</li>
<li><p> If any chunks have fewer samples than this initial allocation,
these elements of <code>nchunk</code> are reset to the number of  samples
in those chunks
</p>
</li>
<li><p> Sequentially an extra sample is allocated to each chunk with
sufficient available samples until <code>take</code> samples are
selected.
</p>
</li></ol>

<p>Argument <code>fill</code> controls the order in which the chunks are
filled. <code>fill = "head"</code> fills from the low to the high end of the
gradient, whilst <code>fill = "tail"</code> fills in the opposite
direction. Chunks are filled in random order if <code>fill =
  "random"</code>. In all cases no chunk is filled by more than one extra
sample until all chunks that can supply one extra sample are
filled. In the case of <code>fill = "head"</code> or <code>fill = "tail"</code>
this entails moving along the gradient from one end to the other
allocating an extra sample to available chunks before starting along
the gradient again. For <code>fill = "random"</code>, a random order of
chunks to fill is determined, if an extra sample is allocated to each
chunk in the random order and <code>take</code> samples are still not
selected, filling begins again using the same random ordering. In
other words, the random order of chunks to fill is chosen only once.
</p>


<h3>Value</h3>

<p>A numeric vector of indices of selected samples. This vector has
attribute <code>lengths</code> which indicates how many samples were
actually chosen from each chunk.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(swappH)

## take a test set of 20 samples along the pH gradient
test1 &lt;- splitSample(swappH, chunk = 10, take = 20)
test1
swappH[test1]

## take a larger sample where some chunks don't have many samples
## do random filling
set.seed(3)
test2 &lt;- splitSample(swappH, chunk = 10, take = 70, fill = "random")
test2
swappH[test2]
</code></pre>

<hr>
<h2 id='sppResponse'>Species responses along gradients.</h2><span id='topic+sppResponse'></span><span id='topic+sppResponse.prcurve'></span>

<h3>Description</h3>

<p>The fitted responses of species along gradients are
estimated or extracted from appropriate objects.</p>


<h3>Usage</h3>

<pre><code class='language-R'>sppResponse(x, ...)

## S3 method for class 'prcurve'
sppResponse(x, n = 100, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sppResponse_+3A_x">x</code></td>
<td>
<p>an R object.</p>
</td></tr>
<tr><td><code id="sppResponse_+3A_n">n</code></td>
<td>
<p>numeric; the number of locations on the gradient to evaluate
the response curve.</p>
</td></tr>
<tr><td><code id="sppResponse_+3A_...">...</code></td>
<td>
<p>additional arguments passed to other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>sppResponse</code> estimates species responses along indicated
gradients.
</p>
<p>There is currently no <code>"default"</code> method and the only specified
method supplied is for objects fitted by <code><a href="#topic+prcurve">prcurve</a></code>. This
method extracts the fitted responses of species along the principal
curve and is a useful diagnostic for identifying overly-complex
curves.
</p>


<h3>Value</h3>

<p>A list is returned with components <code>observed</code> and
<code>fitted.values</code> containing the observed and fitted values of the
species response and gradient respectively. Each is a list with two
components, <code>gradient</code> and <code>response</code>, containing the
gradient and response values.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>See Also</h3>

<p><code><a href="#topic+prcurve">prcurve</a></code> for one function that can be used with
<code>sppResponse</code>. A <code><a href="base.html#topic+plot">plot</a></code> method is available; see
<code><a href="#topic+plot.sppResponse">plot.sppResponse</a></code> for details.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Load the Abernethy Forest data set
data(abernethy)

## Remove the Depth and Age variables
abernethy2 &lt;- abernethy[, -(37:38)]

## Fit the principal curve using varying complexity of smoothers
## for each species
aber.pc &lt;- prcurve(abernethy2, method = "ca", trace = TRUE,
                   vary = TRUE, penalty = 1.4)

## Extract the fitted species response curves
resp &lt;- sppResponse(aber.pc)

## Look at only the most abundant/frequently occurring taxa
take &lt;- chooseTaxa(abernethy2, max.abun = 25, n.occ = 10, value = FALSE)
layout(matrix(1:12, ncol = 3))  	# split device into panels 
plot(resp, which = take)
layout(1)				# reset device
</code></pre>

<hr>
<h2 id='stdError'>Standard error of MAT fitted and predicted values</h2><span id='topic+stdError'></span><span id='topic+stdError.mat'></span><span id='topic+stdError.predict.mat'></span>

<h3>Description</h3>

<p>Computes the (weighted) standard deviation of the environment for the
<em>k</em>-closest analogues for each sample. This was proposed as one
measure of reconstruction uncertainty for MAT models (ter Braak,
1995).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stdError(object, ...)

## S3 method for class 'mat'
stdError(object, k, weighted = FALSE, ...)

## S3 method for class 'predict.mat'
stdError(object, k, weighted = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="stdError_+3A_object">object</code></td>
<td>
<p>Object for which the uncertainty measure is to be
computed. Currently methods for <code><a href="#topic+mat">mat</a></code> and
<code><a href="#topic+predict.mat">predict.mat</a></code>.</p>
</td></tr>
<tr><td><code id="stdError_+3A_k">k</code></td>
<td>
<p>numeric; how many analogues to take? If missing, the default,
<code>k</code> is chosen using <code><a href="#topic+getK">getK</a></code>.</p>
</td></tr>
<tr><td><code id="stdError_+3A_weighted">weighted</code></td>
<td>
<p>logical; use a weighted computation?</p>
</td></tr>
<tr><td><code id="stdError_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to other methods. Currently
not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Two types of standard error can be produced depending upon whether the
mean or weighted mean of <code class="reqn">y</code> for the <code class="reqn">k</code> closest analogues is
used for the MAT predictions. If <code>weighted = FALSE</code> then the
usual standard deviation of the response for the <code class="reqn">k</code> closest
analogues is returned, whereas for <code>weighted = TRUE</code> a weighted
standard deviation is used. The weights are the inverse of the
dissimilarity between the target observation and each of the <code class="reqn">k</code>
closest analogues.
</p>


<h3>Value</h3>

<p>A named numeric vector of weighted standard deviations of the
environment for the <em>k</em> closest analogues used to compute the MAT
predicted values.
</p>
<p>The returned vector has attributes <code>"k"</code> and <code>"auto"</code>,
indicating the number of analogues used and whether this was
determined from <code>object</code> or supplied by the user.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>References</h3>

<p>Simpson, G.L. (2012) Analogue methods in palaeolimnology. In Birks,
H.J.B, Lotter, A.F. Juggins S., and Smol, J.P. (Eds) <em>Tracking
Environmental Change Using Lake Sediments, Volume 5: Data Handling and
Numerical Techniques</em>. Springer, Dordrecht. 
</p>
<p>ter Braak, C.J.F. (1995) Non-linear methods for multivariate
statistical calibration and their use in palaeoecology: a comparison
of inverse (<em>k</em>-nearest neighbours, partial least squares, and
weighted averaging partial least squares) and classical
approaches. <em>Chemometrics and Intelligent Laboratory Systems</em>
<strong>28</strong>:165&ndash;180.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+minDC">minDC</a></code>, <code><a href="#topic+mat">mat</a></code>,
<code><a href="#topic+predict.mat">predict.mat</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Imbrie and Kipp Sea Surface Temperature
data(ImbrieKipp)
data(SumSST)
data(V12.122)

## merge training set and core samples
dat &lt;- join(ImbrieKipp, V12.122, verbose = TRUE)

## extract the merged data sets and convert to proportions
ImbrieKipp &lt;- dat[[1]] / 100
ImbrieKippCore &lt;- dat[[2]] / 100

## fit the MAT model using the squared chord distance measure
ik.mat &lt;- mat(ImbrieKipp, SumSST, method = "SQchord")

## standard errors - unweighted
stdError(ik.mat)
## standard errors - weighted version for above
stdError(ik.mat, k = getK(ik.mat), weighted = TRUE)

## standard errors - weighted; note this uses more (7) analogues
## than the above as this model had lowest LOO error
stdError(ik.mat, weighted = TRUE)

## reconstruct for the V12-122 core data
coreV12.mat &lt;- predict(ik.mat, V12.122, k = 3)
## standard errors
stdError(coreV12.mat)
</code></pre>

<hr>
<h2 id='Stratiplot'>Palaeoecological stratigraphic diagrams</h2><span id='topic+Stratiplot'></span><span id='topic+Stratiplot.default'></span><span id='topic+Stratiplot.formula'></span><span id='topic+Stratiplot.matrix'></span>

<h3>Description</h3>

<p>Draws palaeoecological stratigraphic diagrams of one or more variables
as a function of depth/age, with the time dimension flowing from the
bottom to the top of the y-axis, using the <span class="pkg">Lattice</span> graphics
package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Stratiplot(x, ...)

## Default S3 method:
Stratiplot(x, y, type = "l", ylab = NULL, xlab = "",
           pages = 1, rev = TRUE, ylim, sort = c("none", "wa", "var"),
           svar = NULL, rev.sort = FALSE, strip = FALSE, topPad =6,
           varTypes = "relative", absoluteSize = 0.5,
           zoneNames = NULL, drawLegend = TRUE, na.action = "na.omit",
           labelValues = NULL, labelAt = NULL, labelRot = 60, yticks,
           ...)

## S3 method for class 'formula'
Stratiplot(formula, data, subset,
           na.action = "na.pass", type = "l",
           ylab = NULL, xlab = "", pages = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Stratiplot_+3A_x">x</code></td>
<td>
<p>matrix-like object; the variables to be plotted.</p>
</td></tr>
<tr><td><code id="Stratiplot_+3A_y">y</code></td>
<td>
<p>numeric vector of depths/ages corresponding to rows in
<code>x</code>. Length of <code>y</code> must be the same as <code>nrow(x)</code> or
exactly equal to <code>nrow(x) / ncol(x)</code>. See Details.</p>
</td></tr>
<tr><td><code id="Stratiplot_+3A_formula">formula</code></td>
<td>
<p>an object of class <code>"formula"</code> (or one that can be
coerced to that class): a symbolic description of the model to be
fitted. The details of plot specification are given under
&lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="Stratiplot_+3A_type">type</code></td>
<td>
<p>character; The type of plotting. Can be a vector. Note
that not all Lattice &lsquo;type&rsquo;s are supported and some new types
are allowed. See <code><a href="#topic+panel.Stratiplot">panel.Stratiplot</a></code> for further
details.</p>
</td></tr>
<tr><td><code id="Stratiplot_+3A_data">data</code></td>
<td>
<p>an optional data frame, list or environment (or object
coercible by <code><a href="base.html#topic+as.data.frame">as.data.frame</a></code> to a data frame) containing
the variables to plot. If not found in <code>data</code>, the variables
are taken from <code>environment(formula)</code>, typically the
environment from which <code>Stratiplot</code> is called.</p>
</td></tr>
<tr><td><code id="Stratiplot_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations
to be  used in the fitting process.</p>
</td></tr>
<tr><td><code id="Stratiplot_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when
the data contain <code>NA</code>s. The default is <code>"na.omit"</code> for the
default method, which strips <code>NA</code>s from the stacked data,
whereas the default for the formula method is <code>"na.pass"</code> which
results in <code>NA</code> being passed on to the plotting function. See
Details for further information.</p>
</td></tr>
<tr><td><code id="Stratiplot_+3A_ylab">ylab</code>, <code id="Stratiplot_+3A_xlab">xlab</code></td>
<td>
<p>the x- and y-axis labels.</p>
</td></tr>
<tr><td><code id="Stratiplot_+3A_pages">pages</code></td>
<td>
<p>numeric; the number of pages to draw the plot over. May
be useful for data sets with many species.</p>
</td></tr>
<tr><td><code id="Stratiplot_+3A_rev">rev</code></td>
<td>
<p>logical; should the y-axis limits be reversed</p>
</td></tr>
<tr><td><code id="Stratiplot_+3A_ylim">ylim</code></td>
<td>
<p>user supplied limits for the y-axis (time/depth). If not
supplied, suitable limits will be determined from the data. As such,
in general use <code>ylim</code> need not be supplied. If you choose to
supply your own <code>ylim</code> values, note the default for argument
<code>rev</code>; the default will reverse the values you supply to
<code>ylim</code>.</p>
</td></tr>
<tr><td><code id="Stratiplot_+3A_sort">sort</code></td>
<td>
<p>character; how should the variables (columns) of <code>x</code>
be sorted on the plot. <code>"wa"</code> sorts by weighted averages
of variable <code>svar</code> if not <code>NULL</code> or of <code>y</code>
otherwise. The default when <code>"wa"</code> is specified is to
order by wiehgted average of the depth/time axis &ndash; <code>y</code>. If
<code>"var"</code>, then ordering is done as per the <strong>order</strong> of
<code>svar</code>.</p>
</td></tr>
<tr><td><code id="Stratiplot_+3A_svar">svar</code></td>
<td>
<p>vector; optional variable to sort columns of <code>x</code> by.</p>
</td></tr>
<tr><td><code id="Stratiplot_+3A_rev.sort">rev.sort</code></td>
<td>
<p>logical; should the sorting order be reversed.</p>
</td></tr>
<tr><td><code id="Stratiplot_+3A_strip">strip</code></td>
<td>
<p>logical; Should panels have strips containing variable
labels drawn on them? Default is <code>FALSE</code>, which labels each
panel with a label resulting in a more familiar plot style.</p>
</td></tr>
<tr><td><code id="Stratiplot_+3A_toppad">topPad</code></td>
<td>
<p>numeric; additional padding for the top axis to
accomodate long variable names. This is a temporary fudge until the
actual space required can be automagically calculated from the
variable names themselves. The currently gets most of the way there,
but <code>topPad</code> is used to add some extra space if required.</p>
</td></tr>
<tr><td><code id="Stratiplot_+3A_vartypes">varTypes</code></td>
<td>
<p>a character vector of length 1 or equal in length to
the number of variables plotted. If length 1, the vector is expanded
to the required length. Two values are allowed; i.
<code>"relative"</code>, and ii. <code>"absolute"</code>. <code>"relative"</code>
treats the indicated variable as a percentage type variable and the
panel width is scaled relative to the percentage type variables
plotted. <code>"absolute"</code> treats the indicated variable as an
absolute variable whose panel width should be independent of the
other panels. Use <code>"absolute"</code> for variables that are not
species compositional data or other percentage type data.</p>
</td></tr>
<tr><td><code id="Stratiplot_+3A_absolutesize">absoluteSize</code></td>
<td>
<p>numeric, length 1. This controls the width of
panels for variables marked as <code>"absolute"</code>, and is the
proportion of the largest non-<code>"absolute"</code> panel.</p>
</td></tr>
<tr><td><code id="Stratiplot_+3A_zonenames">zoneNames</code></td>
<td>
<p>character vector of labels, one per zone, with which
to label the zone legend, if drawn (see argument
<code>drawLegend</code>). See Details.</p>
</td></tr> 
<tr><td><code id="Stratiplot_+3A_drawlegend">drawLegend</code></td>
<td>
<p>logical; should a legend for the zones</p>
</td></tr>
<tr><td><code id="Stratiplot_+3A_labelvalues">labelValues</code></td>
<td>
<p>a vector of labels for the variables
plotted. Should be equal in length to the number or variables in the
resulting plot. The main use for <code>labelValues</code> is to provide
non-standard labels for the variables, including a vector of
expressions. See Examples for an illustration of this.</p>
</td></tr>
<tr><td><code id="Stratiplot_+3A_labelat">labelAt</code>, <code id="Stratiplot_+3A_labelrot">labelRot</code></td>
<td>
<p>these control the placement and rotation,
respectively, of the variable labels. <code>labelAt</code> is the
coordinate at which the label is drawn; currently only one value is
used so you can't place labels in different locations depending on
which panel is drawn. This will be fixed in a future version. The
default location for the label is the panel mid-point.
<code>labelAt</code> controls the rotation of the label; it is a numeric
value in degree.</p>
</td></tr>
<tr><td><code id="Stratiplot_+3A_yticks">yticks</code></td>
<td>
<p>This is passed to the <code>scales</code> argument of
<code><a href="lattice.html#topic+xyplot">xyplot</a></code> as component <code>at</code>. This should be a
numeric vector of tick locations for the y (depth/age) axis. Setting
this to <code>NULL</code> or <code>FALSE</code> suppresses ticks on the y
axis. The default uses <code>TRUE</code>, which uses the default choices
for <code>at</code> used by <code><a href="lattice.html#topic+xyplot">xyplot</a></code>.</p>
</td></tr>
<tr><td><code id="Stratiplot_+3A_...">...</code></td>
<td>
<p>additional arguments passed to
<code><a href="#topic+panel.Stratiplot">panel.Stratiplot</a></code> and the underlying
<code><a href="lattice.html#topic+xyplot">xyplot</a></code> function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function now includes preliminary code to handle both relative
(proportional or percentage data) and absolute data types, and
mixtures thereof. Mixtures can be specified by supplying a vector of
types to <code>varTypes</code>, in the same order as the variables are drawn
on the plot.
</p>
<p>Plots can be specified symbolically using a formula. A typical model
has the form <code>Y ~ variables</code>, where <code>Y</code> is either the core
depths or sample ages/dates (to be plotted on the y-axis) and
<code>variables</code> is a series of terms which specifies the variables to
plot against <code>Y</code>. Terms should be specified with the form
<code>var1 + var2 + var3</code> to plot only those variables. Other,
standard, notation for formulae apply, such as model formulae used in
<code><a href="stats.html#topic+lm">lm</a></code>.
</p>
<p>For the formula method the default for argument <code>na.action</code> is
<code>"na.pass"</code>, which results in any <code>NA</code> values being passed
on to the plotting code. This allows for plotting of proxies that been
measured on different levels of the stratigraphy. Should you wish to
have <code>NA</code> removed from the data before plotting, use
<code>na.action = "na.omit"</code>, though do note this will remove all rows
where any column/variable takes the value <code>NA</code>. The default
<code>Stratiplot</code> method, which is used by the formula method for
plotting, will strip any <code>NA</code> values from the data provided to
it. This allows the function to correctly handle the situation where
proxies are measured on different levels of the core <em>and</em> you
are displaying the data using lines of polygons. If the <code>NA</code> were
not dropped by <code>Stratiplot.default</code>, any <code>NA</code> values would
show up as breaks in the line or polygon drawn for each panel.
</p>
<p>In summary, the two methods have different defaults for
<code>na.action</code> to allow them to handle proxies measured on different
levels of the same core. This does mean that you can not use the
formula interface <strong>and</strong> strip NA's at the
<code>Stratiplot.default</code> level. If you need that level of control use
<code>Stratiplot.default</code> directly by not providing a formula as
argument <code>x</code> and by supplying data for the y-axis via argument
<code>y</code>. See Examples for an illustration of these features.
</p>
<p>Note that <code>formula</code> is <strong>not</strong> passed on to
<code><a href="lattice.html#topic+xyplot">xyplot</a></code>. Instead, the formula is parsed and
evaluated within <code>Stratiplot</code> and an appropriate data structure
formed to facilitate plotting via <code><a href="lattice.html#topic+xyplot">xyplot</a></code>. As
such, the special features of <span class="pkg">Lattice</span> formulae cannot be used.
</p>
<p>If zones are drawn on the stratigraphic plot, the <code>zoneNames</code>
argument can be used to supply a set of names with which to label the
zones using a legend. This legend is drawn on the right-hand side of
the the straigraphic diagram if <code>drawLegend = TRUE</code> is
supplied. The <code>zoneNames</code> must be supplied in stratigraphic
order, as that is the order in which they are drawn in the
legend. Whether this ordering is reversed or not will depend on the
value of argument <code>rev</code>. It is up to the user to provide the
labels in the correct order. Zones are specified by the zone
boundaries (excluding the core sequence top and bottom), and as a
result 1 more label is required than the number of zone boundaries
supplied. If no <code>zoneNames</code> is not supplied, but a legend is
requested, suitable names will be produced. If you do not wish to have
any labelling at all, then set <code>zoneNames = ""</code> as this will get
recycled to the correct length. See the Example section for an
illustration of how this drawing zones works. 
</p>


<h3>Value</h3>

<p>A side effect of calling <code>Stratiplot</code> is that a plot is drawn on
the currently active device. A Lattice plot object of class
<code>"trellis"</code> is returned invisibly. This is a change from pre
0.17-0 version of the package.
</p>


<h3>Note</h3>

<p>The function currently doesn't know about ages/dates and will
interpret these as &lsquo;depths&rsquo; instead. This will be fixed in a
future version.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson.</p>


<h3>See Also</h3>

<p><code><a href="lattice.html#topic+xyplot">xyplot</a></code>,
<code><a href="#topic+panel.Stratiplot">panel.Stratiplot</a></code>, <code><a href="#topic+panel.Loess">panel.Loess</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(V12.122)
Depths &lt;- as.numeric(rownames(V12.122))

(plt &lt;- Stratiplot(Depths ~ O.univ + G.ruber + G.tenel + G.pacR,
                   data = V12.122,  type = c("h","l","g","smooth")))

## Order taxa by WA in depth --- ephasises change over time
(plt &lt;- Stratiplot(Depths ~ O.univ + G.ruber + G.tenel + G.pacR,
                   data = V12.122, type = c("h"), sort = "wa"))

## Using the default interface
spp.want &lt;- c("O.univ","G.ruber","G.tenel","G.pacR")
(plt &lt;- Stratiplot(V12.122[, spp.want], y = Depths,
                   type = c("poly", "g")))

## Adding zones to a Stratigraphic plot
## Default labelling and draw zone legend
## Here we choose 4 arbitrary Depths as the zone boundaries
set.seed(123)
Zones &lt;-sample(Depths, 4)
Stratiplot(Depths ~ O.univ + G.ruber + G.tenel + G.pacR,
           data = V12.122, type = c("poly","g"),
           zones = Zones)

## As before, but supplying your own zone labels
zone.labs &lt;- c("A","B","C","D","E")
Stratiplot(Depths ~ O.univ + G.ruber + G.tenel + G.pacR,
           data = V12.122, type = c("poly","g"),
           zones = Zones, zoneNames = zone.labs)

## Suppress the drawing of the zone legend
Stratiplot(Depths ~ O.univ + G.ruber + G.tenel + G.pacR,
           data = V12.122, type = c("poly","g"),
           zones = Zones, drawLegend = FALSE)

## Add zones and draw a legend, but do not label the zones
Stratiplot(Depths ~ O.univ + G.ruber + G.tenel + G.pacR,
           data = V12.122, type = c("poly","g"),
           zones = Zones, zoneNames = "")

## Show illustration of NA handling
set.seed(42)
dat &lt;- data.frame(Depth = 1:20, LOI = runif(20), TC = NA)
dat &lt;- within(dat, TC[sample(20, 10)] &lt;- runif(10))
## default is 'na.action = "na.pass"'
Stratiplot(Depth ~ LOI + TC, data = dat, type = c("l","p"))
## to remove rows with NA, use 'na.action = "na.omit"'
Stratiplot(Depth ~ LOI + TC, data = dat, type = c("l","p"),
           na.action = "na.omit")

## Example of two proxies measured on different levels of core
## (Here measurements on alternate levels)
set.seed(5)
dat2a &lt;- data.frame(Depth = seq(1, by = 2, length = 20), LOI = runif(20))
dat2b &lt;- data.frame(Depth = seq(0, by = 2, length = 20), TC = runif(20))
dat2 &lt;- join(dat2a, dat2b, na.replace = FALSE, split = FALSE)
dat2 &lt;- dat2[order(dat2$Depth), ]
head(dat2)

## Default is to allow NA through formula, but drop them when plotting
Stratiplot(Depth ~ LOI + TC, data = dat2, type = c("l","p"))

## compare with this if we didn't suppress NA in default Stratiplot
## method (can't use formula interface for this yet
Stratiplot(dat2[,-1], dat2[,1], type = c("l","p"),
           na.action = "na.pass")
## Notice no lines are draw as there a no "sections" ithout missing
## levels. If you want/desire this behaviour then you can't use formula
## interface yet as there is no way to specify the na.action separately

## Works with matrices
M &lt;- as.matrix(V12.122)
Stratiplot(M, Depths, type = c("h"))

## Custom variable labels using expressions
df &lt;- data.frame(Age = 1:10, Var1 = rnorm(10), Var2 = rnorm(10),
                 Var3 = rnorm(10))
## Use a vector of expressions to label variables on plot
## See ?plotmath for syntax of expressions
exprs &lt;- expression(delta^{15}*N,       # label for Var1
                    delta^{18}*O,       # label for Var2
                    delta^{13}*C)       # label for Var3
Stratiplot(Age ~ ., data = df, labelValues = exprs, varTypes = "absolute")
</code></pre>

<hr>
<h2 id='summary.analog'>Summarise analogue matching results</h2><span id='topic+summary.analog'></span><span id='topic+print.summary.analog'></span>

<h3>Description</h3>

<p><code><a href="base.html#topic+summary">summary</a></code> method for class <code>"analog"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'analog'
summary(object, display = c("dist", "names", "quantiles"),
        k = 10, probs = c(0.01, 0.02, 0.05, 0.1, 0.2), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="summary.analog_+3A_object">object</code></td>
<td>
<p>an object of class <code>"analog"</code>, usually as a result
of a call to <code><a href="#topic+analog">analog</a></code>.</p>
</td></tr>
<tr><td><code id="summary.analog_+3A_display">display</code></td>
<td>
<p>character; one or more of the listed
choices. Determines which aspects of the <code><a href="#topic+analog">analog</a></code>
results are summarised.</p>
</td></tr>
<tr><td><code id="summary.analog_+3A_k">k</code></td>
<td>
<p>number of analogues to use. If missing, <code>k</code> is chosen
automatically as the <code>k</code> that achieves lowest RMSE.</p>
</td></tr>
<tr><td><code id="summary.analog_+3A_probs">probs</code></td>
<td>
<p>numeric; giving the probabilities of the distribution to
return quantiles for. See <code><a href="stats.html#topic+quantile">quantile</a></code>.</p>
</td></tr>
<tr><td><code id="summary.analog_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with one or more of the components listed below. Attributes
<code>"method"</code>, <code>"train"</code>, <code>"call"</code> and <code>"k"</code> contain
the dissimilarity coefficient used, whether the training set
dissimilarities were saved, the matched function call and the number
of close analogues to return respectively.
</p>
<table role = "presentation">
<tr><td><code>dists</code></td>
<td>
<p>a matrix of dissimilarities between training set samples
and fossil samples. The number of rows is given by argument
<code>k</code>. There is a column for each fossil sample.</p>
</td></tr>
<tr><td><code>names</code></td>
<td>
<p>a matrix of names of samples from the training set that
are analogues for each fossil sample. The number of rows is given by
argument <code>k</code>. There is a column for each fossil sample.</p>
</td></tr>
<tr><td><code>quantiles</code></td>
<td>
<p>numeric; the quantiles of the distribution of the
pairwise dissimilarities for the training set for probabilities
<code>prob</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>See Also</h3>

<p><code><a href="#topic+analog">analog</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## continue the RLGH example from ?join
example(join)

## analog matching between SWAP and RLGH core
swap.analog &lt;- analog(swapdiat, rlgh, method = "chord")
swap.analog
summary(swap.analog)

## End(Not run)
</code></pre>

<hr>
<h2 id='summary.bootstrap.mat'>Summarise bootstrap resampling for MAT models</h2><span id='topic+summary.bootstrap.mat'></span><span id='topic+print.summary.bootstrap.mat'></span>

<h3>Description</h3>

<p><code><a href="base.html#topic+summary">summary</a></code> method for class <code>"bootstrap.mat"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bootstrap.mat'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="summary.bootstrap.mat_+3A_object">object</code></td>
<td>
<p>an object of class <code>"bootstrap.mat"</code>, usually the
result of a call to <code><a href="#topic+bootstrap.mat">bootstrap.mat</a></code>.</p>
</td></tr>
<tr><td><code id="summary.bootstrap.mat_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with the following components:
</p>
<table role = "presentation">
<tr><td><code>observed</code></td>
<td>
<p>vector of observed environmental values.</p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p>a list containing the apparent or non-bootstrapped
estimates for the training set. With the following components:
</p>

<dl>
<dt><code>estimated</code>: </dt><dd><p>estimated values for the response</p>
</dd>
<dt><code>residuals</code>: </dt><dd><p>model residuals</p>
</dd>
<dt><code>r.squared</code>: </dt><dd><p>Apparent <code class="reqn">R^2</code> between observed and
estimated values  of <code>y</code></p>
</dd>
<dt><code>avg.bias</code>: </dt><dd><p>Average bias of the model residuals</p>
</dd>
<dt><code>max.bias</code>: </dt><dd><p>Maximum bias of the model residuals</p>
</dd>
<dt><code>rmse</code>: </dt><dd><p>Apparent error (RMSE) for the model</p>
</dd>
<dt><code>k</code>: </dt><dd><p>numeric; indicating the size of model used in
estimates and predictions</p>
</dd>
</dl>

</td></tr>
<tr><td><code>bootstrap</code></td>
<td>
<p>a list containing the bootstrap estimates for the
training set. With the following components:
</p>

<dl>
<dt><code>estimated</code>: </dt><dd><p>Bootstrap estimates for the response</p>
</dd>
<dt><code>residuals</code>: </dt><dd><p>Bootstrap residuals for the response</p>
</dd>
<dt><code>r.squared</code>: </dt><dd><p>Bootstrap derived <code class="reqn">R^2</code> between observed
and estimated values of the response</p>
</dd>
<dt><code>avg.bias</code>: </dt><dd><p>Average bias of the bootstrap derived model
residuals</p>
</dd>
<dt><code>max.bias</code>: </dt><dd><p>Maximum bias of the bootstrap derived model
residuals</p>
</dd>
<dt><code>rmsep</code>: </dt><dd><p>Bootstrap derived RMSEP for the model</p>
</dd>
<dt><code>s1</code>: </dt><dd><p>Bootstrap derived S1 error component for the
model</p>
</dd>
<dt><code>s2</code>: </dt><dd><p>Bootstrap derived S2 error component for the
model</p>
</dd> 
<dt><code>k</code>: </dt><dd><p>numeric; indicating the size of model used in
estimates and predictions</p>
</dd>
</dl>

</td></tr>
<tr><td><code>sample.errors</code></td>
<td>
<p>a list containing the bootstrap-derived sample
specific errors for the training set. With the following components:
</p>

<dl>
<dt><code>rmsep</code>: </dt><dd><p>Bootstrap derived RMSEP for the training set
samples</p>
</dd> 
<dt><code>s1</code>: </dt><dd><p>Bootstrap derived S1 error component for training
set samples</p>
</dd>
<dt><code>s2</code>: </dt><dd><p>Bootstrap derived S2 error component for training
set samples</p>
</dd>
</dl>

</td></tr>
<tr><td><code>weighted</code></td>
<td>
<p>logical; whether the weighted mean was used instead of
the mean of the environment for <em>k</em>-closest analogues</p>
</td></tr>
<tr><td><code>auto</code></td>
<td>
<p>logical; whether <code>k</code> was choosen automatically or
user-selected</p>
</td></tr>
<tr><td><code>n.boot</code></td>
<td>
<p>numeric; the number of bootstrap samples taken</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matched call</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>model type</p>
</td></tr>
<tr><td><code>predictions</code></td>
<td>
<p>a list containing the apparent and
bootstrap-derived estimates for the new data, with the following
components:
</p>

<dl>
<dt><code>observed</code>: </dt><dd><p>the observed values for the new samples &mdash;
only if <code>newenv</code> is provided</p>
</dd>
<dt><code>model</code>: </dt><dd><p>a list containing the apparent or
non-bootstrapped estimates for the new samples. A list with the
same components as <code>apparent</code>, above</p>
</dd>
<dt><code>bootstrap</code>: </dt><dd><p>a list containing the bootstrap estimates
for the new samples, with some or all of the same components as
<code>bootstrap</code>, above</p>
</dd>
<dt><code>sample.errors</code>: </dt><dd><p>a list containing the bootstrap-derived
sample specific errors for the new samples, with some or all of
the same components as <code>sample.errors</code>, above</p>
</dd>
</dl>

</td></tr>
</table>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>See Also</h3>

<p><code><a href="#topic+bootstrap.mat">bootstrap.mat</a></code>, <code><a href="#topic+mat">mat</a></code>,
<code><a href="base.html#topic+summary">summary</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## continue the RLGH example from ?join
example(join)

## fit the MAT model using the squared chord distance measure
swap.mat &lt;- mat(swapdiat, swappH, method = "SQchord")

## bootstrap training set
swap.boot &lt;- bootstrap(swap.mat, k = 10, n.boot = 100)
swap.boot
summary(swap.boot)

## End(Not run)
</code></pre>

<hr>
<h2 id='summary.cma'>Summarise the extraction of close modern analogues</h2><span id='topic+summary.cma'></span><span id='topic+print.summary.cma'></span>

<h3>Description</h3>

<p><code><a href="base.html#topic+summary">summary</a></code> method for class <code>"cma"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cma'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="summary.cma_+3A_object">object</code></td>
<td>
<p>an object of class <code>"cma"</code>, usually the
result of a call to <code><a href="#topic+cma">cma</a></code>.</p>
</td></tr>
<tr><td><code id="summary.cma_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"summary.cma"</code> with the components of an
object of class <code><a href="#topic+cma">cma</a></code>, plus:  
</p>
<table role = "presentation">
<tr><td><code>distances</code></td>
<td>
<p>a matrix of distances/dissimilarities. Individual
columns contain the ordered close modern analogues for individual
fossil samples. Rows of this matrix refer to the
<code class="reqn">k^{\mathrm{th}}</code> closest analogue for each fossil
sample. See notes below.</p>
</td></tr>
<tr><td><code>samples</code></td>
<td>
<p>a matrix of sample names from the reference set that
are close modern analogues for a fossil sample. Individual
columns contain the ordered close modern analogues for individual 
fossil samples. Rows of this matrix refer to the
<code class="reqn">k^{\mathrm{th}}</code> closest analogue for each fossil sample.
See notes below.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Currently, only objects of class <code><a href="#topic+analog">analog</a></code> are supported.
The number of rows in the returned matrices is equal to the maximum
number of close modern analogues identified for an individual fossil
sample. If no close modern analogues exist for an individual fossil
sample are identified, then the relevant column in <code>"distances"</code>
will contain all missing values and in <code>"samples"</code> the string
<code>"none"</code>. Rows of individual columns will be padded with missing
values if the number of close modern analogues for that sample is less
than the maximum number of close modern analogues identified for a
single sample.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>See Also</h3>

<p><code><a href="#topic+cma">cma</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## continue the RLGH example from ?join
example(join)

## analog matching between SWAP and RLGH core
swap.analog &lt;- analog(swapdiat, rlgh, method = "chord")
swap.analog
summary(swap.analog)

## close modern analogues
swap.cma &lt;- cma(swap.analog, cutoff = 0.6)
swap.cma
summary(swap.cma)

## End(Not run)
</code></pre>

<hr>
<h2 id='summary.mat'>Summarise Modern Analogue Technique models</h2><span id='topic+summary.mat'></span><span id='topic+print.summary.mat'></span>

<h3>Description</h3>

<p><code><a href="base.html#topic+summary">summary</a></code> method for class <code>"mat"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mat'
summary(object, k = 10,
        digits = max(2, getOption("digits") - 4), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="summary.mat_+3A_object">object</code></td>
<td>
<p>an object of class <code>"cma"</code>, usually the
result of a call to <code><a href="#topic+cma">cma</a></code>.</p>
</td></tr>
<tr><td><code id="summary.mat_+3A_k">k</code></td>
<td>
<p>numeric; maximum modern analogues to use to summarise model
fits.</p>
</td></tr>
<tr><td><code id="summary.mat_+3A_digits">digits</code></td>
<td>
<p>numeric; the number of significant digits with which to
format results.</p>
</td></tr>
<tr><td><code id="summary.mat_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the components below. The number of analogues used,
<em>k</em> is returned as attribute <code>"k"</code>.
</p>
<table role = "presentation">
<tr><td><code>summ</code></td>
<td>
<p>a data.frame containing the model fits for training set
samples. See notes below.</p>
</td></tr>
<tr><td><code>tbl</code></td>
<td>
<p>matrix of summary statistics for an un-weighted model.</p>
</td></tr>
<tr><td><code>tbl.W</code></td>
<td>
<p>matrix of summary statistics for a weighted model.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matched function call</p>
</td></tr>
<tr><td><code>quantiles</code></td>
<td>
<p>the quantiles of the distribution of pairwise
dissimilarities for the training set, for <code>"probs = c(0.01,
      0.02, 0.05, 0.1, 0.2)"</code>.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The returned component <code>"summ"</code> contains the following:
</p>

<dl>
<dt>Obs:</dt><dd><p>the observed responses for the training set samples.</p>
</dd>
<dt>Est:</dt><dd><p>the fitted values of the response for training set
samples based on the average of <em>k</em>-closest analogues.</p>
</dd>
<dt>Resi:</dt><dd><p>the residuals of the fitted model based on the average
of <em>k</em>-closest analogues.</p>
</dd>
<dt>W.Est:</dt><dd><p>the fitted values of the response for training set
samples based on the weighted average of <em>k</em>-closest
analogues.</p>
</dd>
<dt>W.Resi:</dt><dd><p>the residuals of the fitted model based on the
weighted average of <em>k</em>-closest analogues.</p>
</dd>
<dt>minDC:</dt><dd><p>dissimilarity of closest analogue in training set for
each training set sample.</p>
</dd>
<dt>minResi:</dt><dd><p>smallest residual for an un-weighted model of size
<code>"k"</code>.</p>
</dd>
<dt>k:</dt><dd><p>size of model leading to minimal residual,
<code>"minResi"</code>.</p>
</dd>
<dt>minW.Resi:</dt><dd><p>smallest residual for a weighted model of size
<code>"k.W"</code>.</p>
</dd>
<dt>k.W:</dt><dd><p>size of model leading to minimal residual,
<code>"minW.Resi"</code>.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>See Also</h3>

<p><code><a href="#topic+mat">mat</a></code>, <code><a href="base.html#topic+summary">summary</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## continue the RLGH example from ?join
example(join)

## fit the MAT model using the squared chord distance measure
swap.mat &lt;- mat(swapdiat, swappH, method = "SQchord")
swap.mat

## model summary
summary(swap.mat)

## model summary - evaluating models using k = 1, ..., 20
## analogues instead of the default, 10.
summary(swap.mat, k = 20)

## End(Not run)
</code></pre>

<hr>
<h2 id='summary.predict.mat'>Summarise MAT model predictions</h2><span id='topic+summary.predict.mat'></span><span id='topic+print.summary.predict.mat'></span>

<h3>Description</h3>

<p><code><a href="base.html#topic+summary">summary</a></code> method for objects of class <code>"predict.mat"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'predict.mat'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="summary.predict.mat_+3A_object">object</code></td>
<td>
<p>an object of class <code>"predict.mat"</code>, usually the
result of a call to <code><a href="#topic+predict.mat">predict.mat</a></code>.</p>
</td></tr>
<tr><td><code id="summary.predict.mat_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"summary.predict.mat"</code>, see
<code><a href="#topic+predict.mat">predict.mat</a></code> for more details.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.mat">predict.mat</a></code>, <code><a href="#topic+mat">mat</a></code>,
<code><a href="#topic+bootstrap.mat">bootstrap.mat</a></code> and <code><a href="base.html#topic+summary">summary</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## continue the RLGH example from ?join
example(join)

## fit the MAT model using the squared chord distance measure
swap.mat &lt;- mat(swapdiat, swappH, method = "SQchord")

## predict for RLGH data
swap.pred &lt;- predict(swap.mat, rlgh, bootstrap = FALSE)
summary(swap.pred)

## End(Not run)
</code></pre>

<hr>
<h2 id='swapdiat'>SWAP sub-fossil diatom and pH training set</h2><span id='topic+swapdiat'></span>

<h3>Description</h3>

<p>The Surface Waters Acidifcation Project (SWAP) Palaeolimnology
Programme diatom surface sample training set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(swapdiat)</code></pre>


<h3>Format</h3>

<p>A data frame of observations on 277 diatom taxa for the 167-lake SWAP
diatom-pH training set.
</p>


<h3>Details</h3>

<p>This data set contains the original 167-lake SWAP diatom-pH
training set.
</p>
<p>The variable names (<code><a href="base.html#topic+colnames">colnames</a></code>) are DIATCODE codes for
individual taxa.
</p>


<h3>References</h3>

<p>Stevenson, A.C., Juggins, S., Birks, H.J.B., Anderson, D.S., Anderson,
N.J., Battarbee, R.W., Berge, F., Davis, R.B., Flower, R.J., Haworth,
E.Y., Jones, V.J., Kingston, J.C., Kreiser, A.M., Line, J.M., Munro,
M.A.R., and Renberg, I. (1995). <em>The Surface Waters Acidification
Project Palaeolimnology programme: modern diatom/lake-water chemistry
data-set</em>. ENSIS Publishing, London.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+swappH">swappH</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(swapdiat)
</code></pre>

<hr>
<h2 id='swappH'>SWAP sub-fossil diatom and pH training set</h2><span id='topic+swappH'></span>

<h3>Description</h3>

<p>The Surface Waters Acidifcation Project (SWAP) Palaeolimnology
Programme diatom-pH surface sample training set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(swappH)</code></pre>


<h3>Format</h3>

<p>Numeric vector containing the pH of the 167 lakes from the SWAP
diatom-pH training set. The values are the average of 4 quarterly
samples.
</p>


<h3>References</h3>

<p>Stevenson, A.C., Juggins, S., Birks, H.J.B., Anderson, D.S., Anderson,
N.J., Battarbee, R.W., Berge, F., Davis, R.B., Flower, R.J., Haworth,
E.Y., Jones, V.J., Kingston, J.C., Kreiser, A.M., Line, J.M., Munro,
M.A.R., and Renberg, I. (1995). <em>The Surface Waters Acidification
Project Palaeolimnology programme: modern diatom/lake-water chemistry
data-set</em>. ENSIS Publishing, London.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+swapdiat">swapdiat</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(swappH)
str(swappH)
</code></pre>

<hr>
<h2 id='timetrack'>Timetracks of change in species composition</h2><span id='topic+timetrack'></span><span id='topic+print.timetrack'></span><span id='topic+plot.timetrack'></span><span id='topic+points.timetrack'></span><span id='topic+fitted.timetrack'></span><span id='topic+scores.timetrack'></span><span id='topic+predict.timetrack'></span>

<h3>Description</h3>

<p>Project passive (e.g. sediment core) samples into an ordination of a
set of training samples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>timetrack(X, passive, env, method = c("cca", "rda"),
          transform = "none", formula, scaling = 3,
          rank = "full", join = "left", correlation = FALSE,
          hill = FALSE, ...)

## S3 method for class 'timetrack'
fitted(object, which = c("passive", "ordination"),
       model = NULL, choices = 1:2, ...)

## S3 method for class 'timetrack'
predict(object, newdata, ...)

## S3 method for class 'timetrack'
scores(x, which = c("ordination", "passive"),
       scaling = x$scaling, choices = 1:2, display = "sites", ...)

## S3 method for class 'timetrack'
plot(x, choices = 1:2, display = c("wa", "lc"),
     order, type = c("p", "n"), ptype = c("l", "p", "o", "b", "n"),
     pch = c(1,2), col = c("black","red"), lty = "solid", lwd = 1,
     xlim = NULL, ylim = NULL, ...)

## S3 method for class 'timetrack'
points(x, choices = 1:2, which = c("passive", "ordination"),
       display = c("wa","lc"), order, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="timetrack_+3A_x">X</code></td>
<td>
<p>matrix-like object containing the training set or reference
samples.</p>
</td></tr>
<tr><td><code id="timetrack_+3A_passive">passive</code></td>
<td>
<p>matrix-like object containing the samples to be
projected into the ordination of <code>X</code>. Usually a set of sediment
core samples.</p>
</td></tr>
<tr><td><code id="timetrack_+3A_env">env</code></td>
<td>
<p>optional data frame of environmental or constraining
variables. If provided, a constrained ordination of <code>X</code> is
performed. If <code>formula</code> is supplied variables named in
<code>formula</code> are looked up with <code>env</code>.</p>
</td></tr>
<tr><td><code id="timetrack_+3A_method">method</code></td>
<td>
<p>character, resolving to an ordination function available
in <span class="pkg">vegan</span>. Currently only <code>"cca"</code>, the default, and
<code>"rda"</code> are supported.</p>
</td></tr>
<tr><td><code id="timetrack_+3A_transform">transform</code></td>
<td>
<p>character; the name of the transformation to apply to
both <code>X</code> and <code>passive</code>. The transformations are performed
using <code>tran</code> and valid options are given by that function's
<code>method</code> argument.</p>
</td></tr>
<tr><td><code id="timetrack_+3A_formula">formula</code></td>
<td>
<p>a one-sided model formula; if provided, it defines the
right hand side of the model formula for the ordination function and
is supplied as argument <code>formula</code> to the ordination
function. E.g.~<code>formula = ~ var1 + var2</code>. If supplied then
<code>env</code> must also be supplied</p>
</td></tr>
<tr><td><code id="timetrack_+3A_scaling">scaling</code></td>
<td>
<p>numeric or character; the ordination scaling to
apply. Useful options are likely to be <code>1</code> or <code>3</code> where
the focus is on the samples. For <code>character</code>, see options in
<code><a href="vegan.html#topic+scores.cca">scores.cca</a></code>: character version of the useful
scalings are <code>"sites"</code> and <code>"symmetric"</code>. See arguments
<code>correlation</code> and <code>hill</code>.</p>
</td></tr>
<tr><td><code id="timetrack_+3A_correlation">correlation</code>, <code id="timetrack_+3A_hill">hill</code></td>
<td>
<p>logical; additional arguments passed to
<code><a href="vegan.html#topic+predict.cca">predict.cca</a></code> and
<code><a href="vegan.html#topic+predict.rda">predict.rda</a></code>. See <code><a href="vegan.html#topic+scores.cca">scores.cca</a></code>
for details.</p>
</td></tr>
<tr><td><code id="timetrack_+3A_rank">rank</code></td>
<td>
<p>character; see argument of same name in function
<code><a href="vegan.html#topic+predict.cca">predict.cca</a></code> or
<code><a href="vegan.html#topic+predict.rda">predict.rda</a></code>.</p>
</td></tr>
<tr><td><code id="timetrack_+3A_join">join</code></td>
<td>
<p>character; the tpe of join to perform. See
<code><a href="#topic+join">join</a></code> for details of possible choices, but the default,
<code>"left"</code> is most generally applicable.</p>
</td></tr>
<tr><td><code id="timetrack_+3A_object">object</code>, <code id="timetrack_+3A_x">x</code></td>
<td>
<p>an object of class <code>"timetrack"</code>.</p>
</td></tr>
<tr><td><code id="timetrack_+3A_which">which</code></td>
<td>
<p>character; which fitted values should be returned?</p>
</td></tr>
<tr><td><code id="timetrack_+3A_model">model</code></td>
<td>
<p>character; which ordination component should be used for
the fitted values; the constrained or unconstrained part? See
<code><a href="vegan.html#topic+fitted.cca">fitted.cca</a></code> for details, but essentially, one of
<code>"CCA"</code> for the constrained part and <code>"CA"</code> for the
unconstrained part. If <code>NULL</code>, the default, <code>"CA"</code> is used
unless the underlying ordination was constrained, in which case
<code>"CCA"</code> is used.</p>
</td></tr>
<tr><td><code id="timetrack_+3A_choices">choices</code></td>
<td>
<p>numeric; the length-2 vector of ordination axes to
plot.</p>
</td></tr>
<tr><td><code id="timetrack_+3A_newdata">newdata</code></td>
<td>
<p>a data frame of new observations for which locations in
the plot (or a timetrack) are required. This need not have exactly
the same set of species as the fitted ordination as internally only
those species in <code>newdata</code> that were included in the data used
for the ordination will be retained. In addition, if a
transformation was applied to the species data used to fit the
ordination, the same transformation will be automatically applied to
<code>newdata</code> using <code><a href="#topic+tran">tran</a></code>.</p>
</td></tr>
<tr><td><code id="timetrack_+3A_display">display</code></td>
<td>
<p>character; which type of sites scores to display? See
<code><a href="vegan.html#topic+scores.cca">scores.cca</a></code> for details.</p>
</td></tr>
<tr><td><code id="timetrack_+3A_order">order</code></td>
<td>
<p>numeric; vector of indices to use to reorder the passive
samples. Useful to get passive samples into temporal order for
plotting with a line.</p>
</td></tr>
<tr><td><code id="timetrack_+3A_type">type</code></td>
<td>
<p>character; the type of plotting required for the training
set samples. Options are <code>"p"</code> for points or <code>"n"</code> to not
draw training set samples.</p>
</td></tr>
<tr><td><code id="timetrack_+3A_ptype">ptype</code></td>
<td>
<p>character; controls how the time track should be
drawn. Default is draw the passive samples connected by a line in
the order in which they appear in the data. With <code>ptype = "p"</code>
no line is drawn. The other types have their usual meaning from
<code><a href="graphics.html#topic+plot.default">plot.default</a></code>.</p>
</td></tr>
<tr><td><code id="timetrack_+3A_pch">pch</code></td>
<td>
<p>The length-2 vector of plotting characters. The first
element is used for the ordination samples, the second for the
passive samples.</p>
</td></tr>
<tr><td><code id="timetrack_+3A_col">col</code></td>
<td>
<p>The length-2 vector of plotting colours. The first
element is used for the ordination samples, the second for the
passive samples.</p>
</td></tr>
<tr><td><code id="timetrack_+3A_lty">lty</code>, <code id="timetrack_+3A_lwd">lwd</code></td>
<td>
<p>graphical parameters for the plotted time track for
<code>ptype != "p"</code>.</p>
</td></tr>
<tr><td><code id="timetrack_+3A_xlim">xlim</code>, <code id="timetrack_+3A_ylim">ylim</code></td>
<td>
<p>user specified axis limits for the plot.</p>
</td></tr>
<tr><td><code id="timetrack_+3A_...">...</code></td>
<td>
<p>arguments passed to other methods.
<code>timetrack</code> passes arguments on to <code>tran</code> and the
ordination function given in <code>method</code>. <code>fitted</code> passes
arguments on to other <code>fitted</code> methods as
appropriate. <code>plot</code> passes arguments on to the underlying
plotting functions. <code>predict</code> passes arguments on to
<code><a href="#topic+tran">tran</a></code> for use in applyign the transformation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The timetrack is a way to visualise changes in species composition
from sediment core samples within an underlying reference ordination
or, usually, training set samples. This technique has been most often
applied in situations where the underlying ordination is a constrained
ordination and thence the timetrack of sediment core samples within
the ordination reflects both the change in species composition and the
indicative changes in the constraining variables.
</p>
<p>The sediment core samples are projected passively into the underlying
ordination. By projected passively, the locations of the core samples
are predicted on the basis of the ordination species scores. A common
set of species (columns) is required to passively place the sediment
samples into the ordination. To achieve this, the left outer join of
the species compositions of the training set and passive set
is determined; the left outer join results in the passive data matrix
having the same set of species (variables; columns) as the training
set. Any training set species not in the passive set are added to
the passive set with abundance 0. Any passive species not in the
training set are removed from the passive set.
</p>


<h3>Value</h3>

<p>The <code>plot</code> method results in a plot on the currently active
device, whilst the <code>fitted</code> and <code>scores</code> methods return the
matrix of fitted locations on the set of ordination axes.
</p>
<p><code>timetrack</code> returns an object of class <code>"timetrack"</code>, a list
with the following components:
</p>
<table role = "presentation">
<tr><td><code>ordination</code></td>
<td>
<p>the ordination object, the result of the call to
the function of the name <code>method</code>.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>the matrix of fitted locations for the passive
samples on the ordination axes.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>the ordination function used.</p>
</td></tr>
<tr><td><code>formula</code></td>
<td>
<p>if supplied, the model formula used to define the
ordination model.</p>
</td></tr>
<tr><td><code>scaling</code></td>
<td>
<p>the ordination scaling applied.</p>
</td></tr>
<tr><td><code>rank</code></td>
<td>
<p>The rank or the number of axes used in the
approximation. The default is to use all axes (full rank) of the
<code>"model"</code>.</p>
</td></tr> 
<tr><td><code>model</code></td>
<td>
<p>Show constrained (<code>"CCA"</code>) or unconstrained
(<code>"CA"</code>) results.</p>
</td></tr>
<tr><td><code>labels</code></td>
<td>
<p>a list of names for the <code>X</code>, <code>passive</code>, and
<code>env</code> arguments.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>The matched function call.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>The training data.</p>
</td></tr>
<tr><td><code>transform</code></td>
<td>
<p>The transformation applied, if any.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Gavin L. Simpson
</p>


<h3>See Also</h3>

<p><code><a href="vegan.html#topic+cca">cca</a></code> and <code><a href="vegan.html#topic+rda">rda</a></code> for the
underlying ordination functions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load the RLGH and SWAP data sets
data(rlgh, swapdiat)

## Fit the timetrack ordination
mod &lt;- timetrack(swapdiat, rlgh, transform = "hellinger",
                 method = "rda")
mod

## Plot the timetrack
plot(mod, ptype = "b", col = c("forestgreen", "orange"), lwd = 2)

## Other options (reorder the time track)
ord &lt;- rev(seq_len(nrow(rlgh)))
plot(mod, choices = 2:3, order = ord, ptype = "b",
     col = c("forestgreen", "orange"), lwd = 2)

## illustrating use of the formula
data(swappH)
mod2 &lt;- timetrack(swapdiat, rlgh, env = data.frame(pH = swappH),
                  transform = "hellinger", method = "rda",
                  formula = ~ pH)
mod2
plot(mod2)

## scores and fitted methods
## IGNORE_RDIFF_BEGIN
head(fitted(mod, type = "passive"))
head(scores(mod, type = "passive"))
## IGNORE_RDIFF_END

## predict locations in timetrack for new observations
take &lt;- rlgh[1:50, ]
take &lt;- take[ , colSums(take) &gt; 0]
mod3 &lt;- predict(mod, newdata = take)
class(mod3) ## returns a timetrack object
take &lt;- rlgh[-(1:50), ]
take &lt;- take[ , colSums(take) &gt; 0]
mod4 &lt;- predict(mod, newdata = take)

## build a plot up from base parts
plot(mod, type = "n", ptype = "n")
points(mod, which = "ordination", col = "grey", pch = 19, cex = 0.7)
points(mod3, which = "passive", col = "red")
points(mod4, which = "passive", col = "blue")

## Fit the timetrack ordination - passing scaling args
mod &lt;- timetrack(swapdiat, rlgh, transform = "hellinger",
                 method = "rda", scaling = "sites",
                 correlation = TRUE)
mod
plot(mod)

</code></pre>

<hr>
<h2 id='tortula'>Morphological data for ten taxa of the genus Tortula</h2><span id='topic+tortula'></span>

<h3>Description</h3>

<p>These data are observations on a series of seven morphological
variables for individuals in of the <em>Tortula</em>
sect. <em>Rurales</em> De Not. (<em>Pottiaceae, Musci</em>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(tortula)
</code></pre>


<h3>Format</h3>

<p><code>tortula</code> is a data frame of seven morphological measurements on
14 individuals from the genus <em>Tortula</em>.
</p>

<dl>
<dt><code>Taxon</code></dt><dd><p>factor; the species of <em>Tortula</em></p>
</dd>
<dt><code>Hydroid</code></dt><dd><p>logical; presence of hydroid cells</p>
</dd>
<dt><code>LeafOutline</code></dt><dd><p>ordered; shape of the leaf outline</p>
</dd>
<dt><code>Denticulation</code></dt><dd><p>ordered; degree of denticulation</p>
</dd>
<dt><code>ApexShape</code></dt><dd><p>ordered; shape of the leaf apex</p>
</dd>
<dt><code>Length</code></dt><dd><p>numeric, leaf length</p>
</dd>
<dt><code>Diameter</code></dt><dd><p>numeric; leaf diameter</p>
</dd>
<dt><code>Papillae</code></dt><dd><p>numeric; number of papillae per cell</p>
</dd>
</dl>

<p>The last three variables are the average of ten replicate samples from
the same herbarium capsule.
</p>


<h3>Source</h3>

<p>The data were presented in Podani (1999).
</p>


<h3>References</h3>

<p>Podani, J. (1999) Extending Gower's coefficient of similarity to
ordinal characters. <em>Taxon</em> <strong>48</strong>, 331-340.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(tortula)
head(tortula)
str(tortula)
</code></pre>

<hr>
<h2 id='tran'>Common data transformations and standardizations</h2><span id='topic+tran'></span><span id='topic+tran.default'></span><span id='topic+tran.formula'></span>

<h3>Description</h3>

<p>Provides common data transformations and standardizations useful for
palaeoecological data. The function acts as a wrapper to function
<code><a href="vegan.html#topic+decostand">decostand</a></code> in package vegan for several of the
available options.
</p>
<p>The <code>formula</code> method allows a convenient method for selecting or
excluding subsets of variables before applying the chosen
transformation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default S3 method:
tran(x, method, a = 1, b = 0, p = 2, base = exp(1),
     na.rm = FALSE, na.value = 0, ...)

## S3 method for class 'formula'
tran(formula, data = NULL, subset = NULL,
     na.action = na.pass, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tran_+3A_x">x</code></td>
<td>
<p>A matrix-like object.</p>
</td></tr>
<tr><td><code id="tran_+3A_method">method</code></td>
<td>
<p>transformation or standardization method to apply. See
Details for available options.</p>
</td></tr>
<tr><td><code id="tran_+3A_a">a</code></td>
<td>
<p>Constant to multiply <code>x</code> by. <code>method = "log"</code>
only. Can be a vector, in which case the vector of values to
multiply each column of <code>x</code> by.</p>
</td></tr>
<tr><td><code id="tran_+3A_b">b</code></td>
<td>
<p>Constant to add to <code>x</code> before taking logs. <code>method
      = "log"</code> only. Can be a vector, in which case the vector of values
to add to each column of <code>x</code>.</p>
</td></tr>
<tr><td><code id="tran_+3A_p">p</code></td>
<td>
<p>The power to use in the power transformation.</p>
</td></tr>
<tr><td><code id="tran_+3A_base">base</code></td>
<td>
<p>the base with respect to which logarithms are
computed. See <code><a href="base.html#topic+log">log</a></code> for further details. The default is
to compute natural logarithms.</p>
</td></tr>
<tr><td><code id="tran_+3A_na.rm">na.rm</code></td>
<td>
<p>Should missing values be removed before some computations?</p>
</td></tr>
<tr><td><code id="tran_+3A_na.value">na.value</code></td>
<td>
<p>The value with which to replace missing values
(<code>NA</code>).</p>
</td></tr>
<tr><td><code id="tran_+3A_...">...</code></td>
<td>
<p>Arguments passed to <code><a href="vegan.html#topic+decostand">decostand</a></code>, or
other <code>tran</code> methods.</p>
</td></tr>
<tr><td><code id="tran_+3A_formula">formula</code></td>
<td>
<p>A model formula describing the variables to be
transformed. The formula should have only a right hand side,
e.g.~<code>~ foo + bar</code>.</p>
</td></tr>
<tr><td><code id="tran_+3A_data">data</code>, <code id="tran_+3A_subset">subset</code>, <code id="tran_+3A_na.action">na.action</code></td>
<td>
<p>See <code><a href="stats.html#topic+model.frame">model.frame</a></code> for
details on these arguments. <code>data</code> will generally be the
object or environment within which the variables in the forumla are
searched for.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function offers following transformation and standardization
methods for community data:
</p>

<ul>
<li> <p><code>sqrt</code>: take the square roots of the observed values.
</p>
</li>
<li> <p><code>cubert</code>: take the cube root of the observed values.
</p>
</li>
<li> <p><code>rootroot</code>: take the fourth root of the observed
values. This is also known as the root root transformation (Field
et al 1982).
</p>
</li>
<li> <p><code>log</code>: take the logarithms of the observed values. The
tansformation applied can be modified by constants <code>a</code> and
<code>b</code> and the <code>base</code> of the logarithms. The transformation
applied is <code class="reqn">x^* = \log_{\mathrm{base}}(ax + b)</code>
</p>
</li>
<li> <p><code>log1p</code>: computes <code class="reqn">log(1 + x)</code> accurately also for
<code class="reqn">|x| &lt;&lt; 1</code> via <code><a href="base.html#topic+log1p">log1p</a></code>. Note the arguments <code>a</code>
and <code>b</code> have no effect in this method.
</p>
</li>
<li> <p><code>expm1</code>: computes <code class="reqn">exp(x) - 1)</code> accurately for
<code class="reqn">|x| &lt;&lt; 1</code> via <code><a href="base.html#topic+expm1">expm1</a></code>.
</p>
</li>
<li> <p><code>reciprocal</code>: returns the multiplicative inverse or
reciprocal, <code class="reqn">1/x</code>, of the observed values.
</p>
</li>
<li> <p><code>freq</code>: divide by column (variable, species)  maximum and
multiply by the number of non-zero items, so that the average of
non-zero entries is 1 (Oksanen 1983).
</p>
</li>
<li> <p><code>center</code>: centre all variables to zero mean.
</p>
</li>
<li> <p><code>range</code>: standardize values into range 0 ... 1. If all
values are constant, they will be transformed to 0.
</p>
</li>
<li> <p><code>percent</code>: convert observed count values to percentages.
</p>
</li>
<li> <p><code>proportion</code>: convert observed count values to proportions.
</p>
</li>
<li> <p><code>standardize</code>: scale <code>x</code> to zero mean and unit
variance.
</p>
</li>
<li> <p><code>pa</code>: scale <code>x</code> to presence/absence scale (0/1).
</p>
</li>
<li> <p><code>missing</code>: replace missing values with <code>na.value</code>.
</p>
</li>
<li> <p><code>chi.square</code>: divide by row sums and square root of
column sums, and adjust for square root of matrix total
(Legendre &amp; Gallagher 2001). When used with the Euclidean
distance, the distances should be similar to the the
Chi-square distance used in correspondence analysis. However, the
results from <code><a href="stats.html#topic+cmdscale">cmdscale</a></code> would still differ, since
CA is a weighted ordination method.
</p>
</li>
<li> <p><code>hellinger</code>: square root of observed values that have
first been divided by row (site) sums (Legendre &amp; Gallagher 2001).
</p>
</li>
<li> <p><code>wisconsin</code>: applies the Wisconsin double
standardization, where columns (species, variables) are first
standardized by maxima and then sites (rows) by site totals.
</p>
</li>
<li> <p><code>pcent2prop</code>: convert percentages to proportions.
</p>
</li>
<li> <p><code>prop2pcent</code>: convert proportions to percentages.
</p>
</li>
<li> <p><code>logRatio</code>: applies a log ransformation (see <code>log</code>
above) to the data, then centres the data by rows (by subtraction of
the mean for row <em>i</em> from the observations in row
<em>i</em>). Using this transformation subsequent to PCA results in
Aitchison's Log Ratio Analysis (LRA), a means of dealing with closed
compositional data such as common in palaeoecology (Aitchison, 1983).
</p>
</li>
<li> <p><code>power</code>: applies a power tranformation.
</p>
</li>
<li> <p><code>rowCentre</code>, <code>rowCenter</code>: Centres <code>x</code> by rows
through the subtraction of the corresponding row mean from the
observations in the row.
</p>
</li>
<li> <p><code>colCentre</code> <code>colCenter</code>: Centres <code>x</code> by columns
through the subtraction of the corresponding column mean from the
observations in the row.
</p>
</li>
<li> <p><code>none</code> <code>none</code>: no transformation is applied.
</p>
</li></ul>



<h3>Value</h3>

<p>Returns the suitably transformed or standardized <code>x</code>. If <code>x</code>
is a data frame, the returned value is like-wise a data frame. The
returned object also has an attribute <code>"tran"</code> giving the name of
applied transformation or standardization <code>"method"</code>.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson. Much of the functionality of <code>tran</code> is
provided by <code><a href="vegan.html#topic+decostand">decostand</a></code>, written by Jari Oksanen.</p>


<h3>References</h3>

<p>Aitchison, J. (1983) Principal components analysis of compositional
data. <em>Biometrika</em> <strong>70</strong>(1); 57&ndash;65.
</p>
<p>Field, J.G., Clarke, K.R., &amp; Warwick, R.M. (1982) A practical strategy
for analysing multispecies distributions patterns. <em>Marine
Ecology Progress Series</em> <strong>8</strong>; 37&ndash;52.
</p>
<p>Legendre, P. &amp; Gallagher, E.D. (2001) Ecologically meaningful
transformations for ordination of species data. <em>Oecologia</em>
<strong>129</strong>; 271-280.
</p>
<p>Oksanen, J. (1983) Ordination of boreal heath-like vegetation with
principal component analysis, correspondence analysis and
multidimensional scaling. <em>Vegetatio</em> <strong>52</strong>; 181-189.
</p>


<h3>See Also</h3>

<p><code><a href="vegan.html#topic+decostand">decostand</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(swapdiat)
## convert percentages to proportions
sptrans &lt;- tran(swapdiat, "pcent2prop")

## apply Hellinger transformation
spHell &lt;- tran(swapdiat, "hellinger")

## Dummy data to illustrate formula method
d &lt;- data.frame(A = runif(10), B = runif(10), C = runif(10))
## simulate some missings
d[sample(10,3), 1] &lt;- NA
## apply tran using formula
tran(~ . - B, data = d, na.action = na.pass,
     method = "missing", na.value = 0)
</code></pre>

<hr>
<h2 id='varExpl'>
Variance explained by ordination axes
</h2><span id='topic+varExpl'></span><span id='topic+varExpl.default'></span><span id='topic+varExpl.cca'></span><span id='topic+varExpl.prcurve'></span>

<h3>Description</h3>

<p>Extracts information about the variance explained by ordination axes
and expresses it in a variety of ways.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>varExpl(object, ...)

## S3 method for class 'cca'
varExpl(object, axes = 1L, cumulative = FALSE,
        pcent = FALSE, ...)

## S3 method for class 'prcurve'
varExpl(object, pcent = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="varExpl_+3A_object">object</code></td>
<td>
<p>an R object of an appropriate type. Currently only for
objects that inherit from classes <code>"cca"</code> or <code>"prcurve"</code>.</p>
</td></tr>
<tr><td><code id="varExpl_+3A_axes">axes</code></td>
<td>
<p>numeric vector indicating which axes to compute variance
explained for.</p>
</td></tr>
<tr><td><code id="varExpl_+3A_cumulative">cumulative</code></td>
<td>
<p>logical; should the variance be explained as a
cumulative sum over the axes?</p>
</td></tr>
<tr><td><code id="varExpl_+3A_pcent">pcent</code></td>
<td>
<p>logical; should the variance explained be expressed as a
percentage of the total variance.</p>
</td></tr>
<tr><td><code id="varExpl_+3A_...">...</code></td>
<td>
<p>additional arguments passed to other methods. Currently
not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector variance explained by each axis.
</p>


<h3>Author(s)</h3>

<p>Gavin L. Simpson
</p>


<h3>See Also</h3>

<p>See <code><a href="vegan.html#topic+cca">cca</a></code> and <code><a href="#topic+prcurve">prcurve</a></code> for functions that
produce objects that <code>varExpl()</code> can work with.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(abernethy)

## Remove the Depth and Age variables
abernethy2 &lt;- abernethy[, -(37:38)]

## Fit PCA
aber.pca &lt;- rda(abernethy2)

## Distance along the first PCA axis
varExpl(aber.pca)
</code></pre>

<hr>
<h2 id='wa'>Weighted averaging transfer functions</h2><span id='topic+wa'></span><span id='topic+wa.default'></span><span id='topic+wa.formula'></span><span id='topic+print.wa'></span><span id='topic+fitted.wa'></span><span id='topic+residuals.wa'></span><span id='topic+coef.wa'></span><span id='topic+waFit'></span>

<h3>Description</h3>

<p>Implements the weighted averaging transfer function
methodology. Tolerance down-weighting and inverse and classicial
deshrinking are supported.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wa(x, ...)

## Default S3 method:
wa(x, env,
   deshrink = c("inverse", "classical", "expanded", "none", "monotonic"),
   tol.dw = FALSE, useN2 = TRUE,
   na.tol = c("min","mean","max"),
   small.tol = c("min","mean","fraction","absolute"),
   min.tol = NULL, f = 0.1, ...)

## S3 method for class 'formula'
wa(formula, data, subset, na.action,
   deshrink = c("inverse", "classical", "expanded", "none", "monotonic"),
   tol.dw = FALSE, useN2 = TRUE, na.tol = c("min","mean","max"),
   small.tol = c("min","mean","fraction","absolute"), min.tol = NULL,
   f = 0.1,..., model = FALSE)

## S3 method for class 'wa'
fitted(object, ...)

## S3 method for class 'wa'
residuals(object, ...)

## S3 method for class 'wa'
coef(object, ...)

waFit(x, y, tol.dw, useN2, deshrink, na.tol, small.tol,
      min.tol, f)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="wa_+3A_x">x</code></td>
<td>
<p>The species training set data</p>
</td></tr>
<tr><td><code id="wa_+3A_env">env</code>, <code id="wa_+3A_y">y</code></td>
<td>
<p>The response vector</p>
</td></tr>
<tr><td><code id="wa_+3A_deshrink">deshrink</code></td>
<td>
<p>Which deshrinking method to use? One of
<code>"inverse"</code> or <code>"classical"</code>, <code>"expanded"</code>,
<code>"none"</code>, or <code>"monotonic"</code>.</p>
</td></tr>
<tr><td><code id="wa_+3A_tol.dw">tol.dw</code></td>
<td>
<p>logical; should species with wider tolerances be given
lower weight?</p>
</td></tr>
<tr><td><code id="wa_+3A_usen2">useN2</code></td>
<td>
<p>logical; should Hill's N2 values be used to produce
un-biased tolerances?</p>
</td></tr>
<tr><td><code id="wa_+3A_na.tol">na.tol</code></td>
<td>
<p>character; method to use to replace missing (<code>NA</code>)
tolerances in WA computations. Missing values are replaced with the
minimum, average or maximum tolerance observed that is not
missing.</p>
</td></tr>
<tr><td><code id="wa_+3A_small.tol">small.tol</code></td>
<td>
<p>character; method to replace small tolerances. See Details.</p>
</td></tr>
<tr><td><code id="wa_+3A_min.tol">min.tol</code></td>
<td>
<p>numeric; threshold below which tolerances are treated
as being &lsquo;small&rsquo;. Default is not to replace small tolerances.</p>
</td></tr>
<tr><td><code id="wa_+3A_f">f</code></td>
<td>
<p>numeric, <code class="reqn">0 &lt; f &lt; 1</code>; fraction of environmental gradient
<code>env</code> to replace small tolerances with if <code>small.tol =
      "fraction"</code> is specified.</p>
</td></tr>
<tr><td><code id="wa_+3A_formula">formula</code></td>
<td>
<p>a model formula</p>
</td></tr>
<tr><td><code id="wa_+3A_data">data</code></td>
<td>
<p>an optional data frame, list or environment (or object
coercible by <code><a href="base.html#topic+as.data.frame">as.data.frame</a></code> to a data frame) containing
the variables specified on the RHS of the model formula. If not found in
<code>data</code>, the  variables are taken from
<code>environment(formula)</code>, typically the environment from which
<code>wa</code> is called.</p>
</td></tr>
<tr><td><code id="wa_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations
to be used in the fitting process.</p>
</td></tr>
<tr><td><code id="wa_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when
the data contain <code>NA</code>s.  The default is set by the
<code>na.action</code> setting of <code>options</code>, and is <code>na.fail</code> if
that is unset.  The 'factory-fresh' default is <code>na.omit</code>.
Another possible value is <code>NULL</code>, no action. Value
<code>na.exclude</code> can be useful.</p>
</td></tr>
<tr><td><code id="wa_+3A_model">model</code></td>
<td>
<p>logical. If <code>TRUE</code> the model frame is returned.</p>
</td></tr>
<tr><td><code id="wa_+3A_object">object</code></td>
<td>
<p>an Object of class <code>"wa"</code>, the result of a call to
<code>wa</code>.</p>
</td></tr>
<tr><td><code id="wa_+3A_...">...</code></td>
<td>
<p>arguments to other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A typical model has the form <code>response ~ terms</code>
where <code>response</code> is the (numeric) response vector (the variable to
be predicted) and <code>terms</code> is a series of terms which specifies a
linear predictor for <code>response</code>.  A terms specification of the
form <code>first + second</code> indicates all the terms in <code>first</code>
together with all the terms in <code>second</code> with duplicates
removed. A specification of <code>.</code> is shorthand for all terms in
<code>data</code> not already included in the model.
</p>
<p>Species that have very small tolerances can dominate reconstructed
values if tolerance down-weighting is used. In <code>wa</code>, small
tolerances are defined as a tolerance that is <code class="reqn">&lt;</code>
<code>min.tol</code>. The default is to not replace small tolerances, and
the user needs to specify suitable values of <code>min.tol</code>. Function
<code><a href="vegan.html#topic+tolerance">tolerance</a></code> may be of use in computing tolerances before
fitting the WA model.
</p>
<p>Small tolerances can be adjusted in several ways:
</p>

<dl>
<dt><code>min</code> </dt><dd><p>small tolerances are replaced by the smallest
observed tolerance that is greater than, or equal to,
<code>min.tol</code>. With this method, the replaced values will be no
smaller than any other observed tolerance. This is the default in
<span class="pkg">analogue</span>.</p>
</dd>
<dt><code>mean</code> </dt><dd><p>small tolerances are replaced by the average
observed tolerance from the set that are greater than, or equal
to, <code>min.tol</code>.</p>
</dd>
<dt><code>fraction</code> </dt><dd><p>small tolerances are replaced by the
fraction, <code>f</code>, of the observed environmental gradient in the
training set, <code>env</code>.</p>
</dd>
<dt><code>absolute</code> </dt><dd><p>small tolerances are replaced by
<code>min.tol</code>.</p>
</dd>
</dl>

<p>Function <code>waFit</code> is the workhorse implementing the actual WA
computations. It performs no checks on the input data and returns a
simple list containing the optima, tolernances, model tolerances,
fitted values, coefficients and the numbers of samples and
species. See Value below for details of each component.
</p>


<h3>Value</h3>

<p>An object of class <code>"wa"</code>, a list with the following components:
</p>
<table role = "presentation">
<tr><td><code>wa.optima</code></td>
<td>
<p>The WA optima for each species in the model.</p>
</td></tr>
<tr><td><code>tolerances</code></td>
<td>
<p>The actual tolerances calculated (these are weighted
standard deviations).</p>
</td></tr>
<tr><td><code>model.tol</code></td>
<td>
<p>The tolerances used in the WA model
computations. These will be similar to <code>tol</code>, but will no
contain any <code>NA</code>s and any small tolerances will have been
replaced with the appropriate value.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>The fitted values of the response for each of the
training set samples.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Model residuals.</p>
</td></tr>
<tr><td><code>coefficients</code></td>
<td>
<p>Deshrinking coefficients. Note that in the case of
<code>deshrink = "monotonic"</code> this is a list with components <code>sm</code>
(the representation of the smooth term as returned by
<code><a href="mgcv.html#topic+smoothCon">smoothCon</a></code>) and <code>p</code> (solutions to the least squares
fit with monotonic constraints, the result of a call to
<code><a href="mgcv.html#topic+pcls">pcls</a></code>).</p>
</td></tr>
<tr><td><code>rmse</code></td>
<td>
<p>The RMSE of the model.</p>
</td></tr>
<tr><td><code>r.squared</code></td>
<td>
<p>The coefficient of determination of the observed and
fitted values of the response.</p>
</td></tr>
<tr><td><code>avg.bias</code>, <code>max.bias</code></td>
<td>
<p>The average and maximum bias statistics.</p>
</td></tr>
<tr><td><code>n.samp</code>, <code>n.spp</code></td>
<td>
<p>The number of samples and species in the training
set.</p>
</td></tr>
<tr><td><code>deshrink</code></td>
<td>
<p>The deshrinking regression method used.</p>
</td></tr>
<tr><td><code>tol.dw</code></td>
<td>
<p>logical; was tolerance down-weighting applied?</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>The matched function call.</p>
</td></tr>
<tr><td><code>orig.x</code></td>
<td>
<p>The training set species data.</p>
</td></tr>
<tr><td><code>orig.env</code></td>
<td>
<p>The response data for the training set.</p>
</td></tr>
<tr><td><code>options.tol</code></td>
<td>
<p>A list, containing the values of the arguments
<code>useN2</code>, <code>na.tol</code>, <code>small.tol</code>, <code>min.tol</code>, and
<code>f</code>.</p>
</td></tr>
<tr><td><code>terms</code>, <code>model</code></td>
<td>
<p>Model <code><a href="stats.html#topic+terms">terms</a></code> and <code><a href="stats.html#topic+model.frame">model.frame</a></code>
components. Only returned by the <code>formula</code> method of
<code>wa</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Gavin L. Simpson and Jari Oksanen</p>


<h3>See Also</h3>

<p><code><a href="#topic+mat">mat</a></code> for an alternative transfer function method.</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(ImbrieKipp)
data(SumSST)

## fit the WA model
mod &lt;- wa(SumSST ~., data = ImbrieKipp)
mod

## extract the fitted values
fitted(mod)

## residuals for the training set
residuals(mod)

## deshrinking coefficients
coef(mod)

## diagnostics plots
par(mfrow = c(1,2))
plot(mod)
par(mfrow = c(1,1))

## caterpillar plot of optima and tolerances
caterpillarPlot(mod)                 ## observed tolerances
caterpillarPlot(mod, type = "model") ## with tolerances used in WA model

## plot diagnostics for the WA model
par(mfrow = c(1,2))
plot(mod)
par(mfrow = c(1,1))

## tolerance DW
mod2 &lt;- wa(SumSST ~ ., data = ImbrieKipp, tol.dw = TRUE,
           min.tol = 2, small.tol = "min")
mod2

## compare actual tolerances to working values
with(mod2, rbind(tolerances, model.tol))

## tolerance DW
mod3 &lt;- wa(SumSST ~ ., data = ImbrieKipp, tol.dw = TRUE,
           min.tol = 2, small.tol = "mean")
mod3

## fit a WA model with monotonic deshrinking
mod4 &lt;- wa(SumSST ~., data = ImbrieKipp, deshrink = "monotonic")
mod4

## extract the fitted values
fitted(mod4)

## residuals for the training set
residuals(mod4)


</code></pre>

<hr>
<h2 id='weightedCor'>
Weighted correlation test of WA reconstruction
</h2><span id='topic+weightedCor'></span><span id='topic+weightedCor.default'></span><span id='topic+print.weightedCor'></span><span id='topic+plot.weightedCor'></span>

<h3>Description</h3>

<p>Weighted correlation between WA optima from training set and axis 1
scores of constrained ordination fitted to fossil data with WA model
predictions for fossil samples as constraints.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default S3 method:
weightedCor(x, env, fossil, method = c("rda", "cca"),
            test = TRUE, type = c("simulate", "permute"), sim = 999,
            verbose = TRUE, ...)

## S3 method for class 'weightedCor'
plot(x,
     type = c("bubble", "null"),
     weighted = TRUE,
     size = 0.25,
     xlab = paste(x$env, "WA Optima"),
     ylab = "Axis 1 Score",
     xlim,
     main = "",
     sub = NULL,
     border = "gray75",
     col = "gray75",
     obscol = "red",
     fg = "black", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="weightedCor_+3A_x">x</code></td>
<td>
<p>training set covariates, a matrix-like object usually of
species/proxy data. For the <code>plot</code> method, an object of class
<code>"weightedCor"</code>, the result of a call to <code>weightedCor</code>.</p>
</td></tr>
<tr><td><code id="weightedCor_+3A_env">env</code></td>
<td>
<p>training set response, a vector usually of environmental
data.</p>
</td></tr>
<tr><td><code id="weightedCor_+3A_fossil">fossil</code></td>
<td>
<p>matrix of fossil/core species/proxy data for which a
reconstruction is sought.</p>
</td></tr>
<tr><td><code id="weightedCor_+3A_method">method</code></td>
<td>
<p>constrained ordination method. One of <code>"rda"</code> and
<code>"cca"</code>. Currently only <code>"rda"</code> is supported.</p>
</td></tr>
<tr><td><code id="weightedCor_+3A_test">test</code></td>
<td>
<p>logical; should the observed correlation be tested?</p>
</td></tr>
<tr><td><code id="weightedCor_+3A_type">type</code></td>
<td>
<p>the type of test to apply. One of <code>"simulate"</code> or
<code>"permute"</code>. The latter is currently not implemented. For the
<code>plot</code> method, the type of plot to produce.</p>
</td></tr>
<tr><td><code id="weightedCor_+3A_sim">sim</code></td>
<td>
<p>numeric; number of simulations or permutations to permform
as part of the test</p>
</td></tr>
<tr><td><code id="weightedCor_+3A_verbose">verbose</code></td>
<td>
<p>logical; should the progress of the test be shown via a
progress bar?</p>
</td></tr>
<tr><td><code id="weightedCor_+3A_...">...</code></td>
<td>
<p>arguments passed to other methods. In the case of the
<code>plot</code> method, additional graphical parameters can be
supplied.</p>
</td></tr>
<tr><td><code id="weightedCor_+3A_weighted">weighted</code></td>
<td>
<p>logical; should the null distribution plotted be of
the weighted or normal correlation.</p>
</td></tr>
<tr><td><code id="weightedCor_+3A_size">size</code></td>
<td>
<p>numeric; the size of the largest bubble in inches. See
<code><a href="graphics.html#topic+symbols">symbols</a></code> and argument <code>inches</code> for details.</p>
</td></tr>
<tr><td><code id="weightedCor_+3A_xlim">xlim</code>, <code id="weightedCor_+3A_xlab">xlab</code>, <code id="weightedCor_+3A_ylab">ylab</code>, <code id="weightedCor_+3A_main">main</code>, <code id="weightedCor_+3A_sub">sub</code></td>
<td>
<p>graphical parameters with their
usual meaning.</p>
</td></tr>
<tr><td><code id="weightedCor_+3A_border">border</code>, <code id="weightedCor_+3A_col">col</code></td>
<td>
<p>The border and fill colours for the histogram
bars.</p>
</td></tr>
<tr><td><code id="weightedCor_+3A_fg">fg</code></td>
<td>
<p>The colour of the bubbles drawn on the bubble plot.</p>
</td></tr>
<tr><td><code id="weightedCor_+3A_obscol">obscol</code></td>
<td>
<p>The colour of the indicator for the observed
correlation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>plot</code> method produces a plot on the current
device. <code>weightedCor()</code> returns a list with the following
components:
</p>
<table role = "presentation">
<tr><td><code>wtdCorrel</code>, <code>Correl</code></td>
<td>
<p>numeric; the observed weighted and standard
correlation.</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>data frame; containing the training set WA Optima, axis 1
species scores, and mean abundance for each species.</p>
</td></tr>
<tr><td><code>ord</code></td>
<td>
<p>the fitted constrained ordination.</p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p>the fitted WA model.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>the ordination method used.</p>
</td></tr>
<tr><td><code>ndist</code></td>
<td>
<p>the null distribution produced. <code>NULL</code> if argument
<code>test</code> was <code>FALSE</code>.</p>
</td></tr>
<tr><td><code>sim</code></td>
<td>
<p>numeric; the number of simulations or permutations used to
test the observed correlations.</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>the type of test performed.</p>
</td></tr>
<tr><td><code>env</code></td>
<td>
<p>the deparsed version of <code>env</code> argument. Used for
plotting.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matched function call.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Gavin L. Simpson
</p>


<h3>References</h3>

<p>Telford R.J. and Birks, H.J.B. (2011) A novel method for assessing the
statistical significance of quantitative reconstructions inferred from
biotic assemblages. <em>Quanternary Science Reviews</em>
<strong>30</strong>:1272-1278.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+wa">wa</a></code> for details on fitting weighted average models.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ImbrieKipp, SumSST, V12.122)

Cor &lt;- weightedCor(ImbrieKipp, env = SumSST,
                   fossil = V12.122, type = "simulate", sim = 49)
Cor

plot(Cor)
plot(Cor, type = "null")
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
