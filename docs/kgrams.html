<!DOCTYPE html><html><head><title>Help for package kgrams</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {kgrams}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#kgrams-package'><p>kgrams: Classical k-gram Language Models</p></a></li>
<li><a href='#+25+2B+25'><p>String concatenation</p></a></li>
<li><a href='#dictionary'><p>Word dictionaries</p></a></li>
<li><a href='#EOS'><p>Special Tokens</p></a></li>
<li><a href='#kgram_freqs'><p>k-gram Frequency Tables</p></a></li>
<li><a href='#language_model'><p>k-gram Language Models</p></a></li>
<li><a href='#midsummer'><p>A Midsummer Night's Dream</p></a></li>
<li><a href='#much_ado'><p>Much Ado About Nothing</p></a></li>
<li><a href='#parameters'><p>Language Model Parameters</p></a></li>
<li><a href='#perplexity'><p>Language Model Perplexities</p></a></li>
<li><a href='#preprocess'><p>Text preprocessing</p></a></li>
<li><a href='#probability'><p>Language Model Probabilities</p></a></li>
<li><a href='#query'><p>Query k-gram frequency tables or dictionaries</p></a></li>
<li><a href='#sample_sentences'><p>Random Text Generation</p></a></li>
<li><a href='#smoothers'><p>k-gram Probability Smoothers</p></a></li>
<li><a href='#tknz_sent'><p>Sentence tokenizer</p></a></li>
<li><a href='#word_context'><p>Word-context conditional expression</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Classical k-gram Language Models</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.0</td>
</tr>
<tr>
<td>Description:</td>
<td>
        Training and evaluating k-gram language models in R, 
        supporting several probability smoothing techniques, 
        perplexity computations, random text generation and more.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppProgress</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp, rlang, methods, utils, RcppProgress (&ge; 0.1), Rdpack</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat (&ge; 3.0.0), covr, knitr, rmarkdown</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>RdMacros:</td>
<td>Rdpack</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://vgherard.github.io/kgrams/">https://vgherard.github.io/kgrams/</a>,
<a href="https://github.com/vgherard/kgrams">https://github.com/vgherard/kgrams</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/vgherard/kgrams/issues">https://github.com/vgherard/kgrams/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-10-06 10:39:20 UTC; Valerio</td>
</tr>
<tr>
<td>Author:</td>
<td>Valerio Gherardi <a href="https://orcid.org/0000-0002-8215-3013"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Valerio Gherardi &lt;vgherard840@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-10-06 14:30:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='kgrams-package'>kgrams: Classical k-gram Language Models</h2><span id='topic+kgrams'></span><span id='topic+kgrams-package'></span>

<h3>Description</h3>

<p>Training and evaluating k-gram language models in R, supporting several probability smoothing techniques, perplexity computations, random text generation and more.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Valerio Gherardi <a href="mailto:vgherard840@gmail.com">vgherard840@gmail.com</a> (<a href="https://orcid.org/0000-0002-8215-3013">ORCID</a>)
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://vgherard.github.io/kgrams/">https://vgherard.github.io/kgrams/</a>
</p>
</li>
<li> <p><a href="https://github.com/vgherard/kgrams">https://github.com/vgherard/kgrams</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/vgherard/kgrams/issues">https://github.com/vgherard/kgrams/issues</a>
</p>
</li></ul>


<hr>
<h2 id='+25+2B+25'>String concatenation</h2><span id='topic++25+2B+25'></span>

<h3>Description</h3>

<p>String concatenation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lhs %+% rhs
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="+2B25+2B2B+2B25_+3A_lhs">lhs</code></td>
<td>
<p>a string or vector of strings.</p>
</td></tr>
<tr><td><code id="+2B25+2B2B+2B25_+3A_rhs">rhs</code></td>
<td>
<p>a string or vector of strings.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The expression <code>lhs %+% rhs</code> is equivalent to
<code>paste(lhs, rhs, sep = " ", collapse = NULL, recycle0 = FALSE)</code>.
See <a href="base.html#topic+paste">paste</a> for more details.
</p>


<h3>Value</h3>

<p>a string or vector of strings.
</p>


<h3>Author(s)</h3>

<p>Valerio Gherardi
</p>
<p>Brief synthax for string concatenation.
</p>


<h3>See Also</h3>

<p><a href="base.html#topic+paste">paste</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>"i love" %+% c("cats", "jazz", "you")
</code></pre>

<hr>
<h2 id='dictionary'>Word dictionaries</h2><span id='topic+dictionary'></span><span id='topic+dictionary.kgram_freqs'></span><span id='topic+dictionary.character'></span><span id='topic+dictionary.connection'></span><span id='topic+as_dictionary'></span><span id='topic+as_dictionary.kgrams_dictionary'></span><span id='topic+as_dictionary.character'></span><span id='topic+as.character.kgrams_dictionary'></span>

<h3>Description</h3>

<p>Construct or coerce to and from a dictionary.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dictionary(object, ...)

## S3 method for class 'kgram_freqs'
dictionary(object, size = NULL, cov = NULL, thresh = NULL, ...)

## S3 method for class 'character'
dictionary(
  object,
  .preprocess = identity,
  size = NULL,
  cov = NULL,
  thresh = NULL,
  ...
)

## S3 method for class 'connection'
dictionary(
  object,
  .preprocess = identity,
  size = NULL,
  cov = NULL,
  thresh = NULL,
  max_lines = Inf,
  batch_size = max_lines,
  ...
)

as_dictionary(object)

## S3 method for class 'kgrams_dictionary'
as_dictionary(object)

## S3 method for class 'character'
as_dictionary(object)

## S3 method for class 'kgrams_dictionary'
as.character(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dictionary_+3A_object">object</code></td>
<td>
<p>object from which to extract a dictionary, or to be coerced to
dictionary.</p>
</td></tr>
<tr><td><code id="dictionary_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
<tr><td><code id="dictionary_+3A_size">size</code></td>
<td>
<p>either <code>NULL</code> or a length one positive integer. Predefined size of the
required dictionary (the top <code>size</code> most frequent words are retained).</p>
</td></tr>
<tr><td><code id="dictionary_+3A_cov">cov</code></td>
<td>
<p>either <code>NULL</code> or a length one numeric between <code>0</code> and <code>1</code>.
Predefined text coverage fraction of the dictionary
(the most frequent words providing the required coverage are retained).</p>
</td></tr>
<tr><td><code id="dictionary_+3A_thresh">thresh</code></td>
<td>
<p>either <code>NULL</code> or length one a positive integer.
Minimum word count threshold to include a word in the dictionary.</p>
</td></tr>
<tr><td><code id="dictionary_+3A_.preprocess">.preprocess</code></td>
<td>
<p>a function taking a character vector as input and returning
a character vector as output. Optional preprocessing transformation
applied to text before creating the dictionary.</p>
</td></tr>
<tr><td><code id="dictionary_+3A_max_lines">max_lines</code></td>
<td>
<p>a length one positive integer or <code>Inf</code>.
Maximum number of lines to be read from the <code>connection</code>.
If <code>Inf</code>, keeps reading until the End-Of-File.</p>
</td></tr>
<tr><td><code id="dictionary_+3A_batch_size">batch_size</code></td>
<td>
<p>a length one positive integer less than or equal to
<code>max_lines</code>.Size of text batches when reading text from
<code>connection</code>.</p>
</td></tr>
<tr><td><code id="dictionary_+3A_x">x</code></td>
<td>
<p>a <code>dictionary</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These generic functions are used to build <code>dictionary</code> objects,
or to coerce from other formats to <code>dictionary</code>, and from a
<code>dictionary</code> to a character vector. By now, the only
non-trivial type coercible to <code>dictionary</code> is <code>character</code>,
in which case each entry of the input vector is considered as a single word.
Coercion from <code>dictionary</code> to <code>character</code> returns the list of
words included in the dictionary as a regular character vector.
</p>
<p>Dictionaries can be extracted from <code>kgram_freqs</code> objects, or <em>built</em>
from text coming either directly from a character vector or a connection.
</p>
<p>A single preprocessing transformation can be applied before processing the
text for unique words. After preprocessing,
<em>anything delimited by one or more white space characters</em>
in the transformed text input <em>is counted as a word</em> and may be added
to the dictionary modulo additional constraints.
</p>
<p>The possible constraints for including a word in the dictionary can be of
three types: (i) fixed size of dictionary, implemented by the <code>size</code>
argument; (ii) fixed text covering fraction, as specified by the <code>cov</code>
argument; or (iii) minimum word count threshold, <code>thresh</code> argument.
<em>Only one of these constraints can be applied at a time</em>,
so that specifying more than one of <code>size</code>, <code>cov</code> or <code>thresh</code>
results in an error.
</p>


<h3>Value</h3>

<p>A <code>dictionary</code> for <code>dictionary()</code> and
<code>as_dictionary()</code>, a character vector for the <code>as.character()</code>
method.
</p>


<h3>Author(s)</h3>

<p>Valerio Gherardi
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Building a dictionary from Shakespeare's "Much Ado About Nothing"

dict &lt;- dictionary(much_ado)
length(dict)
query(dict, "leonato") # TRUE
query(dict, c("thy", "thou")) # c(TRUE, TRUE)
query(dict, "smartphones") # FALSE

# Getting list of words as regular character vector
words &lt;- as.character(dict)
head(words)

# Building a dictionary from a list of words
dict &lt;- as_dictionary(c("i", "the", "a"))

</code></pre>

<hr>
<h2 id='EOS'>Special Tokens</h2><span id='topic+EOS'></span><span id='topic+BOS'></span><span id='topic+UNK'></span><span id='topic+special_tokens'></span>

<h3>Description</h3>

<p>Return Begin-Of-Sentence, End-Of-Sentence and Unknown-Word special tokens.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EOS()

BOS()

UNK()
</code></pre>


<h3>Details</h3>

<p>These functions return the internal representation of BOS, EOS and UNK tokens
respectively. Their actual returned values are irrelevant and their only
purpose is to simplify queries of k-gram counts and probabilities involving
the special tokens, as shown in the examples.
</p>


<h3>Value</h3>

<p>a string representing the appropriate special token.
</p>


<h3>Author(s)</h3>

<p>Valerio Gherardi
</p>


<h3>Examples</h3>

<pre><code class='language-R'>f &lt;- kgram_freqs("a b b a b", 2)
query(f, c(BOS(), EOS(), UNK()))

m &lt;- language_model(f, "add_k", k = 1)
probability(c("a", "b") %|% BOS(), m)
probability("a b b a" %+% EOS(), m)

# The actual values of BOS(), EOS() and UNK() are irrelevant
c(BOS(), EOS(), UNK())


</code></pre>

<hr>
<h2 id='kgram_freqs'>k-gram Frequency Tables</h2><span id='topic+kgram_freqs'></span><span id='topic+kgram_freqs.numeric'></span><span id='topic+kgram_freqs.kgram_freqs'></span><span id='topic+kgram_freqs.character'></span><span id='topic+kgram_freqs.connection'></span><span id='topic+process_sentences'></span><span id='topic+process_sentences.character'></span><span id='topic+process_sentences.connection'></span>

<h3>Description</h3>

<p>Extract k-gram frequency counts from a text or a connection.
</p>


<h4>Principal methods supported by objects of class <code>kgram_freqs</code></h4>


<ul>
<li> <p><code>query()</code>: query k-gram counts from the table.
See <a href="#topic+query">query</a>
</p>
</li>
<li> <p><code>probability()</code>: compute word continuation and sentence probabilities
using Maximum Likelihood estimates. See <a href="#topic+probability">probability</a>.
</p>
</li>
<li> <p><code>language_model()</code>: build a k-gram language model using various
probability smoothing techniques. See <a href="#topic+language_model">language_model</a>.
</p>
</li></ul>




<h3>Usage</h3>

<pre><code class='language-R'>kgram_freqs(object, ...)

## S3 method for class 'numeric'
kgram_freqs(
  object,
  .preprocess = identity,
  .tknz_sent = identity,
  dict = NULL,
  ...
)

## S3 method for class 'kgram_freqs'
kgram_freqs(object, ...)

## S3 method for class 'character'
kgram_freqs(
  object,
  N,
  .preprocess = identity,
  .tknz_sent = identity,
  dict = NULL,
  open_dict = is.null(dict),
  verbose = FALSE,
  ...
)

## S3 method for class 'connection'
kgram_freqs(
  object,
  N,
  .preprocess = identity,
  .tknz_sent = identity,
  dict = NULL,
  open_dict = is.null(dict),
  verbose = FALSE,
  max_lines = Inf,
  batch_size = max_lines,
  ...
)

process_sentences(
  text,
  freqs,
  .preprocess = attr(freqs, ".preprocess"),
  .tknz_sent = attr(freqs, ".tknz_sent"),
  open_dict = TRUE,
  in_place = TRUE,
  verbose = FALSE,
  ...
)

## S3 method for class 'character'
process_sentences(
  text,
  freqs,
  .preprocess = attr(freqs, ".preprocess"),
  .tknz_sent = attr(freqs, ".tknz_sent"),
  open_dict = TRUE,
  in_place = TRUE,
  verbose = FALSE,
  ...
)

## S3 method for class 'connection'
process_sentences(
  text,
  freqs,
  .preprocess = attr(freqs, ".preprocess"),
  .tknz_sent = attr(freqs, ".tknz_sent"),
  open_dict = TRUE,
  in_place = TRUE,
  verbose = FALSE,
  max_lines = Inf,
  batch_size = max_lines,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kgram_freqs_+3A_object">object</code></td>
<td>
<p>any type allowed by the available methods. The type defines the
behaviour of <code>kgram_freqs()</code> as a default constructor, a copy
constructor or a constructor of a non-trivial object. See ‘Details’.</p>
</td></tr>
<tr><td><code id="kgram_freqs_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
<tr><td><code id="kgram_freqs_+3A_.preprocess">.preprocess</code></td>
<td>
<p>a function taking a character vector as input and returning
a character vector as output. Optional preprocessing transformation
applied to text before k-gram tokenization. See  ‘Details’.</p>
</td></tr>
<tr><td><code id="kgram_freqs_+3A_.tknz_sent">.tknz_sent</code></td>
<td>
<p>a function taking a character vector as input and
returning a character vector as output. Optional sentence tokenization step
applied to text after preprocessing and before k-gram tokenization. See
‘Details’.</p>
</td></tr>
<tr><td><code id="kgram_freqs_+3A_dict">dict</code></td>
<td>
<p>anything coercible to class
<a href="#topic+dictionary">dictionary</a>. Optional pre-specified word dictionary.</p>
</td></tr>
<tr><td><code id="kgram_freqs_+3A_n">N</code></td>
<td>
<p>a length one integer. Maximum order of k-grams to be considered.</p>
</td></tr>
<tr><td><code id="kgram_freqs_+3A_open_dict">open_dict</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code>. If <code>TRUE</code>, any new
word encountered during processing not appearing in the original dictionary
is included into the dictionary. Otherwise, new words are replaced by an
unknown word token. It is by default <code>TRUE</code> if <code>dict</code> is
specified, <code>FALSE</code> otherwise.</p>
</td></tr>
<tr><td><code id="kgram_freqs_+3A_verbose">verbose</code></td>
<td>
<p>Print current progress to the console.</p>
</td></tr>
<tr><td><code id="kgram_freqs_+3A_max_lines">max_lines</code></td>
<td>
<p>a length one positive integer or <code>Inf</code>.
Maximum number of lines to be read from the <code>connection</code>.
If <code>Inf</code>, keeps reading until the End-Of-File.</p>
</td></tr>
<tr><td><code id="kgram_freqs_+3A_batch_size">batch_size</code></td>
<td>
<p>a length one positive integer less than or equal to
<code>max_lines</code>.Size of text batches when reading text from
<code>connection</code>.</p>
</td></tr>
<tr><td><code id="kgram_freqs_+3A_text">text</code></td>
<td>
<p>a character vector or a connection. Source of text from which
k-gram frequencies are to be extracted.</p>
</td></tr>
<tr><td><code id="kgram_freqs_+3A_freqs">freqs</code></td>
<td>
<p>a <code>kgram_freqs</code> object, to which new k-gram counts from
<code>text</code> are to be added.</p>
</td></tr>
<tr><td><code id="kgram_freqs_+3A_in_place">in_place</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code>. Should the initial
<code>kgram_freqs</code> object be modified in place?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>kgram_freqs()</code> is a generic constructor for
objects of class <code>kgram_freqs</code>, i.e. k-gram frequency tables. The
constructor from <code>integer</code> returns an empty 'kgram_freqs' of fixed
order, with an optional
predefined dictionary (which can be empty) and <code>.preprocess</code> and
<code>.tknz_sent</code> functions to be used as defaults in other <code>kgram_freqs</code>
methods. The constructor from <code>kgram_freqs</code> returns a copy of an
existing object, and it is provided because, in general, <code>kgram_freqs</code>
objects have reference semantics, as discussed below.
</p>
<p>The following discussion focuses on <code>process_sentences()</code> generic, as
well as on the <code>character</code> and <code>connection</code> methods of the
constructor <code>kgram_freqs()</code>. These functions extract k-gram
frequency counts from a text source, which may be either a character vector
or a connection. The second option is useful if one wants to avoid loading
the full text corpus in physical memory, allowing to process text from
different sources such as files, compressed files or URLs.
</p>
<p>The returned object is of class <code>kgram_freqs</code> (a thin wrapper
around the internal C++ class where all k-gram computations take place).
<code>kgram_freqs</code> objects have methods for querying bare k-gram frequencies
(<a href="#topic+query">query</a>) and maximum likelihood estimates of sentence
probabilities or word continuation probabilities
(see <a href="#topic+probability">probability</a>)) . More importantly
<code>kgram_freqs</code> objects are used to create <a href="#topic+language_model">language_model</a>
objects, which support various probability smoothing techniques.
</p>
<p>The function <code>kgram_freqs()</code> is used to <em>construct</em> a new
<code>kgram_freqs</code> object, initializing it with the k-gram counts from
the <code>text</code> input, whereas <code>process_sentences()</code> is used to
add k-gram counts from a new <code>text</code> to an <em>existing</em>
<code>kgram_freqs</code> object, <code>freqs</code>. In this second case, the initial
object <code>freqs</code> can either be modified in place
(for <code>in_place == TRUE</code>, the default) or by making a copy
(<code>in_place == FALSE</code>), see the examples below.
The final object is returned invisibly when modifying in place,
visibly in the second case. It is worth to mention that modifying in place
a <code>kgram_freqs</code> object <code>freqs</code> will also affect
<code>language_model</code> objects created from <code>freqs</code> with
<code>language_model()</code>, which will also be updated with the new information.
If one wants to avoid this behaviour, one can make copies using either the
<code>kgram_freqs()</code> copy constructor, or the <code>in_place = FALSE</code>
argument.
</p>
<p>The <code>dict</code> argument allows to provide an initial set of known
words. Subsequently, one can either work with such a closed dictionary
(<code>open_dict == FALSE</code>), or extended the dictionary with all
new words encountered during k-gram processing
(<code>open_dict == TRUE</code>)  .
</p>
<p>The <code>.preprocess</code> and <code>.tknz_sent</code> functions are applied
<em>before</em> k-gram counting takes place, and are in principle
arbitrary transformations of the original text.
<em>After</em> preprocessing and sentence tokenization, each line of the
transformed input is presented to the k-gram counting algorithm as a separate
sentence (these sentences are implicitly padded
with <code>N - 1</code> Begin-Of-Sentence (BOS) and one End-Of-Sentence (EOS)
tokens, respectively. This is illustrated in the examples). For basic
usage, this package offers the utilities <a href="#topic+preprocess">preprocess</a> and
<a href="#topic+tknz_sent">tknz_sent</a>. Notice that, strictly speaking, there is
some redundancy in these two arguments, as the processed input to the k-gram
counting algorithm is <code>.tknz_sent(.preprocess(text))</code>.
They appear explicitly as separate arguments for two main reasons:
</p>

<ul>
<li><p> The presence of <code>.tknz_sent</code> is a reminder of the
fact that sentences have to be explicitly separeted in different entries
of the processed input, in order for <code>kgram_freqs()</code> to append the
correct Begin-Of-Sentence and End-Of-Sentence paddings to each sentence.
</p>
</li>
<li><p> At prediction time (e.g. with <a href="#topic+probability">probability</a>), by default only
<code>.preprocess</code> is applied when computing conditional probabilities,
whereas both <code>.preprocess()</code> and <code>.tknz_sent()</code> are
applied when computing sentence absolute probabilities.
</p>
</li></ul>



<h3>Value</h3>

<p>A <code>kgram_freqs</code> class object: k-gram frequency table storing
k-gram counts from text. For <code>process_sentences()</code>, the updated
<code>kgram_freqs</code> object is returned invisibly if <code>in_place</code> is
<code>TRUE</code>, visibly otherwise.
</p>


<h3>Author(s)</h3>

<p>Valerio Gherardi
</p>


<h3>See Also</h3>

<p><a href="#topic+query">query</a>, <a href="#topic+probability">probability</a>
<a href="#topic+language_model">language_model</a>, <a href="#topic+dictionary">dictionary</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Build a k-gram frequency table from a character vector

f &lt;- kgram_freqs("a b b a a", 3)
f
summary(f)
query(f, c("a", "b")) # c(3, 2)
query(f, c("a b", "a" %+% EOS(), BOS() %+% "a b")) # c(1, 1, 1)
query(f, "a b b a") # NA (counts for k-grams of order k &gt; 3 are not known)

process_sentences("b", f)
query(f, c("a", "b")) # c(3, 3): 'f' is updated in place

f1 &lt;- process_sentences("b", f, in_place = FALSE)
query(f, c("a", "b")) # c(3, 3): 'f' is copied
query(f1, c("a", "b")) # c(3, 4): the new 'f1' stores the updated counts




# Build a k-gram frequency table from a file connection

## Not run: 
f &lt;- kgram_freqs(file("my_text_file.txt"), 3)

## End(Not run)


# Build a k-gram frequency table from an URL connection
## Not run: 
f &lt;- kgram_freqs(url("http://my.website/my_text_file.txt"), 3)

## End(Not run)
</code></pre>

<hr>
<h2 id='language_model'>k-gram Language Models</h2><span id='topic+language_model'></span><span id='topic+language_model.language_model'></span><span id='topic+language_model.kgram_freqs'></span>

<h3>Description</h3>

<p>Build a k-gram language model.
</p>


<h4>Principal methods supported by objects of class <code>language_model</code></h4>


<ul>
<li> <p><code>probability()</code>: compute word continuation and sentence probabilities.
See <a href="#topic+probability">probability</a>.
</p>
</li>
<li> <p><code>sample_sentences()</code>: generate random text by sampling from the
language model probability distribution at arbitary temperature. See
<a href="#topic+sample_sentences">sample_sentences</a>.
</p>
</li>
<li> <p><code>perplexity()</code>: Compute the language model perplexity on a test
corpus. See <a href="#topic+perplexity">perplexity</a>.
</p>
</li></ul>




<h3>Usage</h3>

<pre><code class='language-R'>language_model(object, ...)

## S3 method for class 'language_model'
language_model(object, ...)

## S3 method for class 'kgram_freqs'
language_model(object, smoother = "ml", N = param(object, "N"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="language_model_+3A_object">object</code></td>
<td>
<p>an object which stores the information required to build the
k-gram model. At present, necessarily a <code>kgram_freqs</code> object, or a
<code>language_model</code> object of which a copy is desired (see Details).</p>
</td></tr>
<tr><td><code id="language_model_+3A_...">...</code></td>
<td>
<p>possible additional parameters required by the smoother.</p>
</td></tr>
<tr><td><code id="language_model_+3A_smoother">smoother</code></td>
<td>
<p>a length one character vector. Indicates the smoothing
technique to be applied to compute k-gram continuation probabilities. A list
of available smoothers can be obtained with <code>smoothers()</code>, and
further information on a particular smoother through
<code>info()</code>.</p>
</td></tr>
<tr><td><code id="language_model_+3A_n">N</code></td>
<td>
<p>a length one integer. Maximum order of k-grams to use in the language
model. This muss be less than or equal to the order of the underlying
<code>kgram_freqs</code> object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These generics are used to construct objects of class <code>language_model</code>.
The <code>language_model</code> method is only needed to create copies of
<code>language_model</code> objects (that is to say, new copies which are not
altered by methods which modify the original object in place,
see e.g. <a href="#topic+parameters">parameters</a>). The discussion below focuses on
language models and the <code>kgram_freqs</code> method.
</p>
<p><a href="#topic+kgrams">kgrams</a> supports several k-gram language models, including
Interpolated Kneser-Ney, Stupid Backoff and others
(see <a href="#topic+smoothers">smoothers</a>). The objects created by
<code>language_models()</code> have methods for computing word continuation and
sentence probabilities (see <a href="#topic+probability">probability</a>),
random text generation (see <a href="#topic+sample_sentences">sample_sentences</a>)
and other type of language modeling tasks such as computing perplexities and
word prediction accuracies.
</p>
<p>Smoothers have often tuning parameters, which need to be specified by
(exact) name through the <code>...</code> arguments; otherwise,
<code>language_model()</code> will use default values and, once per session, throw
a warning. <code>info(smoother)</code> lists all parameters needed by a
specific smoother, together with their allowed parameter space.
</p>
<p>The run-time of <code>language_model()</code> may vary substantially for different
smoothing methods, depending on whether or not a method requires the
computation of additional quantities (that is to say, beyond k-gram counts)
for its operativity (this is, for instance, the case for the Kneser-Ney
smoother).
</p>


<h3>Value</h3>

<p>A <code>language_model</code> object.
</p>


<h3>Author(s)</h3>

<p>Valerio Gherardi
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create an interpolated Kneser-Ney 2-gram language model

freqs &lt;- kgram_freqs("a a b a a b a b a b a b", 2)
model &lt;- language_model(freqs, "kn", D = 0.5)
model
summary(model)
probability("a" %|% "b", model)

# For more examples, see ?probability, ?sample_sentences and ?perplexity.

</code></pre>

<hr>
<h2 id='midsummer'>A Midsummer Night's Dream</h2><span id='topic+midsummer'></span>

<h3>Description</h3>

<p>The entire play &quot;A Midsummer Night's Dream&quot; from William Shakespeare.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>midsummer
</code></pre>


<h3>Format</h3>

<p>A length one character vector, containing the entire text of
&quot;A Midsummer Night's Dream&quot; from William Shakespeare. The script used
for generating this file is available
<a href="https://github.com/vgherard/kgrams/blob/main/data-raw/shakespeare.R">here</a>
</p>


<h3>Source</h3>

<p><a href="https://www.folger.edu/">https://www.folger.edu/</a>
</p>


<h3>See Also</h3>

<p><a href="#topic+much_ado">much_ado</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>midsummer[840]

</code></pre>

<hr>
<h2 id='much_ado'>Much Ado About Nothing</h2><span id='topic+much_ado'></span>

<h3>Description</h3>

<p>The entire play &quot;Much Ado About Nothing&quot; from William Shakespeare.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>much_ado
</code></pre>


<h3>Format</h3>

<p>A length one character vector, containing the entire text of
&quot;Much Ado About Nothing&quot; from William Shakespeare. The script used
for generating this file is available
<a href="https://github.com/vgherard/kgrams/blob/main/data-raw/shakespeare.R">here</a>
</p>


<h3>Source</h3>

<p><a href="https://www.folger.edu/">https://www.folger.edu/</a>
</p>


<h3>See Also</h3>

<p><a href="#topic+midsummer">midsummer</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>much_ado[840]

</code></pre>

<hr>
<h2 id='parameters'>Language Model Parameters</h2><span id='topic+parameters'></span><span id='topic+param'></span><span id='topic+param.kgram_freqs'></span><span id='topic+param+3C-'></span>

<h3>Description</h3>

<p>Get and set parameters of a language model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>param(object, which)

## S3 method for class 'kgram_freqs'
param(object, which)

param(object, which) &lt;- value

parameters(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="parameters_+3A_object">object</code></td>
<td>
<p>a <code>language_model</code> or <code>kgram_freqs</code> class object.</p>
</td></tr>
<tr><td><code id="parameters_+3A_which">which</code></td>
<td>
<p>a string. Name of the parameter to get or set.</p>
</td></tr>
<tr><td><code id="parameters_+3A_value">value</code></td>
<td>
<p>new value for the parameter specified by <code>which</code>. Typically
a length one numeric.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions are used to retrieve or modify the parameters of a
<code>language_model</code> or a <code>kgram_freqs</code> object. Any object of,
or inheriting from, any of these two classes has at least two parameters:
</p>

<ul>
<li> <p><code>N</code>: higher order of k-grams considered in the model for
<code>language_model</code>, or stored in memory for <code>kgram_freqs</code>.
</p>
</li>
<li> <p><code>V</code>: size of the dictionary (excluding the special tokens
<code>BOS()</code>, <code>EOS()</code>, <code>UNK()</code>).
</p>
</li></ul>

<p>For an object of class <code>kgram_freqs</code>, these are the only parameters,
and they are read-only. <code>language_model</code>s allow to set <code>N</code> less
than or equal to the order of the underlying <code>kgram_freqs</code> object.
</p>
<p>In addition to these, <code>language_model</code>s can have additional parameters,
e.g. discount values or interpolation constants, depending on the particular
smoother employed by the model. A list of parameters available for a given
smoother can be obtained through <code>info()</code>
(see <a href="#topic+smoothers">smoothers</a>).
</p>
<p>It is important to mention that setting a parameter is an in-place operation.
This implies that if, say, object <code>m</code> is a <code>language_model</code> object,
the code <code>m1 &lt;- m ; param(m1, which) &lt;- value</code> will set the parameter
<code>which</code> to <code>value</code> both for <code>m1</code> <em>and</em> <code>m</code>. The
reason for this is that, behind the scenes, both <code>m</code> and <code>m1</code> are
pointers to the same C++ object. In order to create a true copy, one can use
the copy constructor <code>language_model()</code>, see
<a href="#topic+language_model">language_model</a>.
</p>


<h3>Value</h3>

<p>a list for <code>parameters()</code>, a single value, typically numeric,
for <code>param()</code> and <code>param()&lt;-</code> (the new value, in this last case).
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Get and set k-gram model parameters

f &lt;- kgram_freqs("a a b a b", 3)
param(f, "N")
parameters(f)

m &lt;- language_model(f, "sbo", lambda = 0.2)
param(m, "V")
param(m, "lambda")
param(m, "N") &lt;- 2
param(m, "lambda") &lt;- 0.4

if (FALSE) {
        param(m, "V") &lt;- 5 # Error: dictionary size cannot be set.  
}

if (FALSE) {
        param(f, "N") &lt;- 4 # Error: parameters of 'kgram_freqs' cannot be set  
}

m1 &lt;- m
param(m1, "lambda") &lt;- 0.5
param(m, "lambda") # 0.5 ; param() modifies 'm' by reference!

m2 &lt;- language_model(m) # This creates a true copy
param(m2, "lambda") &lt;- 0.6
param(m, "lambda") # 0.5

</code></pre>

<hr>
<h2 id='perplexity'>Language Model Perplexities</h2><span id='topic+perplexity'></span><span id='topic+perplexity.character'></span><span id='topic+perplexity.connection'></span>

<h3>Description</h3>

<p>Compute language model perplexities on a test corpus.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>perplexity(
  text,
  model,
  .preprocess = attr(model, ".preprocess"),
  .tknz_sent = attr(model, ".tknz_sent"),
  exp = TRUE,
  ...
)

## S3 method for class 'character'
perplexity(
  text,
  model,
  .preprocess = attr(model, ".preprocess"),
  .tknz_sent = attr(model, ".tknz_sent"),
  exp = TRUE,
  detailed = FALSE,
  ...
)

## S3 method for class 'connection'
perplexity(
  text,
  model,
  .preprocess = attr(model, ".preprocess"),
  .tknz_sent = attr(model, ".tknz_sent"),
  exp = TRUE,
  batch_size = Inf,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="perplexity_+3A_text">text</code></td>
<td>
<p>a character vector or connection. Test corpus from which
language model perplexity is computed.</p>
</td></tr>
<tr><td><code id="perplexity_+3A_model">model</code></td>
<td>
<p>an object of class <code>language_model</code>.</p>
</td></tr>
<tr><td><code id="perplexity_+3A_.preprocess">.preprocess</code></td>
<td>
<p>a function taking a character vector as input and
returning a character vector as output. Preprocessing transformation
applied to input before computing perplexity.</p>
</td></tr>
<tr><td><code id="perplexity_+3A_.tknz_sent">.tknz_sent</code></td>
<td>
<p>a function taking a character vector as input and
returning a character vector as output. Optional sentence tokenization step
applied before computing perplexity.</p>
</td></tr>
<tr><td><code id="perplexity_+3A_exp">exp</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code>. If <code>TRUE</code>, returns the actual
perplexity - exponential of cross-entropy per token - otherwise returns its
natural logarithm.</p>
</td></tr>
<tr><td><code id="perplexity_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
<tr><td><code id="perplexity_+3A_detailed">detailed</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code>. If <code>TRUE</code>, the output has
a <code>"details"</code> attribute, which is a data-frame containing the
cross-entropy of each individual sentence tokenized from <code>text</code>.</p>
</td></tr>
<tr><td><code id="perplexity_+3A_batch_size">batch_size</code></td>
<td>
<p>a length one positive integer or <code>Inf</code>.
Size of text batches when reading text from a <code>connection</code>.
If <code>Inf</code>, all input text is processed in a single batch.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These generic functions are used to compute a <code>language_model</code>
perplexity on a test corpus, which may be either a plain character vector
of text, or a connection from which text can be read in batches.
The second option is useful if one wants to avoid loading
the full text in physical memory, and allows to process text from
different sources such as files, compressed files or URLs.
</p>
<p>&quot;Perplexity&quot; is defined here, following Ref.
(Chen and Goodman 1999), as the exponential of the normalized
language model cross-entropy with the test corpus. Cross-entropy is
normalized by the total number of words in the corpus, where we include
the End-Of-Sentence tokens, but not the Begin-Of-Sentence tokens, in the
word count.
</p>
<p>The custom .preprocess and .tknz_sent arguments allow to apply
transformations to the text corpus before the perplexity computation takes
place. By default, the same functions used during model building are
employed, c.f. <a href="#topic+kgram_freqs">kgram_freqs</a> and <a href="#topic+language_model">language_model</a>.
</p>
<p>A note of caution is in order. Perplexity is not defined for all language
models available in <a href="#topic+kgrams">kgrams</a>. For instance, smoother
<code>"sbo"</code> (i.e. Stupid Backoff (Brants et al. 2007))
does not produce normalized probabilities,
and this is signaled by a warning (shown once per session) if the user
attempts to compute the perplexity for such a model.
In these cases, when possible, perplexity computations are performed
anyway case, as the results might still be useful (e.g. to tune the model's
parameters), even if their probabilistic interpretation does no longer hold.
</p>


<h3>Value</h3>

<p>a number. Perplexity of the language model on the test corpus.
</p>


<h3>Author(s)</h3>

<p>Valerio Gherardi
</p>


<h3>References</h3>

<p>Brants T, Popat AC, Xu P, Och FJ, Dean J (2007).
&ldquo;Large Language Models in Machine Translation.&rdquo;
In <em>Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</em>, 858&ndash;867.
<a href="https://aclanthology.org/D07-1090/">https://aclanthology.org/D07-1090/</a>.<br /><br /> Chen SF, Goodman J (1999).
&ldquo;An empirical study of smoothing techniques for language modeling.&rdquo;
<em>Computer Speech &amp; Language</em>, <b>13</b>(4), 359&ndash;394.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Train 4-, 6-, and 8-gram models on Shakespeare's "Much Ado About Nothing",
# compute their perplexities on the training and test corpora.
# We use Shakespeare's "A Midsummer Night's Dream" as test.


train &lt;- much_ado
test &lt;- midsummer

tknz &lt;- function(text) tknz_sent(text, keep_first = TRUE)
f &lt;- kgram_freqs(train, 8, .tknz_sent = tknz)
m &lt;- language_model(f, "kn", D = 0.75)

# Compute perplexities for 4-, 6-, and 8-gram models 
FUN &lt;- function(N) {
        param(m, "N") &lt;- N
        c(train = perplexity(train, m), test = perplexity(test, m))
        }
sapply(c("N = 4" = 4, "N = 6" = 6, "N = 8" = 8), FUN)


</code></pre>

<hr>
<h2 id='preprocess'>Text preprocessing</h2><span id='topic+preprocess'></span>

<h3>Description</h3>

<p>A minimal text preprocessing utility.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>preprocess(input, erase = "[^.?!:;'[:alnum:][:space:]]", lower_case = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="preprocess_+3A_input">input</code></td>
<td>
<p>a character vector.</p>
</td></tr>
<tr><td><code id="preprocess_+3A_erase">erase</code></td>
<td>
<p>a length one character vector. Regular expression matching
parts of text to be <em>erased</em> from input. The default removes anything
not  alphanumeric (<code>[A-z0-9]</code>), space (white space, tab,
vertical tab, newline, form feed, carriage return), apostrophes or
punctuation characters (<code>"[.?!:;]"</code>).</p>
</td></tr>
<tr><td><code id="preprocess_+3A_lower_case">lower_case</code></td>
<td>
<p>a length one logical vector. If TRUE, puts everything to
lower case.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The expressions <code>preprocess(x, erase = pattern, lower_case = TRUE)</code> and
<code>preprocess(x, erase = pattern, lower_case = FALSE)</code> are roughly
equivalent to <code>tolower(gsub(pattern, "", x))</code> and
<code>gsub(pattern, "", x)</code>, respectively, provided that the regular
expression 'pattern' is correctly recognized by R.
</p>
<p><strong>Note.</strong> This function, as well as <a href="#topic+tknz_sent">tknz_sent</a>, are included
in the library for illustrative purposes only, and are not optimized for
performance. Furthermore (for performance reasons) the function has a
separate implementation for Windows and UNIX OS types, respectively, so that
results obtained in the two cases may differ slightly.
In contexts that require full reproducibility, users are encouraged to define
their own preprocessing and tokenization custom functions - or to work with
externally processed data.
</p>


<h3>Value</h3>

<p>a character vector containing the processed output.
</p>


<h3>Author(s)</h3>

<p>Valerio Gherardi
</p>


<h3>Examples</h3>

<pre><code class='language-R'>preprocess("#This Is An Example@-@!#")
</code></pre>

<hr>
<h2 id='probability'>Language Model Probabilities</h2><span id='topic+probability'></span><span id='topic+probability.kgrams_word_context'></span><span id='topic+probability.character'></span>

<h3>Description</h3>

<p>Compute sentence probabilities and word continuation conditional
probabilities from a language model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>probability(object, model, .preprocess = attr(model, ".preprocess"), ...)

## S3 method for class 'kgrams_word_context'
probability(object, model, .preprocess = attr(model, ".preprocess"), ...)

## S3 method for class 'character'
probability(
  object,
  model,
  .preprocess = attr(model, ".preprocess"),
  .tknz_sent = attr(model, ".tknz_sent"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="probability_+3A_object">object</code></td>
<td>
<p>a character vector for sentence probabilities,
a word-context conditional expression created with the
conditional operator <code style="white-space: pre;">&#8288;%|%&#8288;</code> (see <a href="#topic+word_context">word_context</a>).
for word continuation probabilities.</p>
</td></tr>
<tr><td><code id="probability_+3A_model">model</code></td>
<td>
<p>an object of class <code>language_model</code>.</p>
</td></tr>
<tr><td><code id="probability_+3A_.preprocess">.preprocess</code></td>
<td>
<p>a function taking a character vector as input and
returning a character vector as output. Preprocessing transformation
applied to input before computing probabilities</p>
</td></tr>
<tr><td><code id="probability_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
<tr><td><code id="probability_+3A_.tknz_sent">.tknz_sent</code></td>
<td>
<p>a function taking a character vector as input and
returning a character vector as output. Optional sentence tokenization step
applied before computing sentence probabilities.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The generic function <code>probability()</code> is used to obtain both sentence
unconditional probabilities (such as Prob(&quot;I was starting to feel drunk&quot;))
and word continuation conditional probabilities (such as
Prob(&quot;you&quot; | &quot;i love&quot;)). In plain words, these probabilities answer the
following related but conceptually different questions:
</p>

<ul>
<li><p> Sentence probability Prob(s): what is the probability that extracting a
single sentence (from a corpus of text, say) we will obtain exactly 's'?
</p>
</li>
<li><p> Continuation probability Prob(w|c): what is the probability that a given
context 'c' will be followed exactly by the word 'w'?
</p>
</li></ul>

<p>In order to compute continuation probabilities (i.e. Prob(w|c)), one must
create conditional expressions with the infix operator <code style="white-space: pre;">&#8288;%|%&#8288;</code>, as shown in
the examples below. Both <code>probability</code> and <code style="white-space: pre;">&#8288;%|%&#8288;</code> are vectorized with
respect to words (left hand side of <code style="white-space: pre;">&#8288;%|%&#8288;</code>), but the context must be a length
one character (right hand side of <code style="white-space: pre;">&#8288;%|%&#8288;</code>).
</p>
<p>The input is treated as in <a href="#topic+query">query</a> for what concerns word
tokenization: anything delimited by (one or more) white space(s) is
tokenized as a word. For sentence probabilities, Begin-Of-Sentence and
End-Of-Sentence paddings are implicitly added to the input, but specifying
them explicitly does not produce wrong results as BOS and EOS tokens are
ignored by <code>probability()</code> (see the examples below). For continuation
probabilities, any context of more than <code>N - 1</code> words (where
<code>N</code> is the k-gram order the language model) is truncated to the last
<code>N - 1</code> words.
</p>
<p>By default, the same <code>.preprocess()</code> and <code>.tknz_sent()</code>
functions used during model building are applied to the input, but this can
be overriden with arbitrary functions. Notice that the
<code>.tknz_sent</code> can be useful (for sentence probabilities) if
e.g. the input is a length one unprocessed character vector.
</p>


<h3>Value</h3>

<p>a numeric vector. Probabilities of the sentences or word
continuations.
</p>


<h3>Author(s)</h3>

<p>Valerio Gherardi
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Usage of probability()

f &lt;- kgram_freqs("a b b a b a b", 2)

m &lt;- language_model(f, "add_k", k = 1)
probability(c("a", "b", EOS(), UNK()) %|% BOS(), m) # c(0.4, 0.2, 0.2, 0.2)
probability("a" %|% UNK(), m) # not NA

</code></pre>

<hr>
<h2 id='query'>Query k-gram frequency tables or dictionaries</h2><span id='topic+query'></span><span id='topic+query.kgram_freqs'></span><span id='topic+query.kgrams_dictionary'></span>

<h3>Description</h3>

<p>Return the frequency count of k-grams in a k-gram frequency table, or
whether words are contained in a dictionary.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>query(object, x)

## S3 method for class 'kgram_freqs'
query(object, x)

## S3 method for class 'kgrams_dictionary'
query(object, x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="query_+3A_object">object</code></td>
<td>
<p>a <code>kgram_freqs</code> or <code>dictionary</code> class object.</p>
</td></tr>
<tr><td><code id="query_+3A_x">x</code></td>
<td>
<p>a character vector. A list of k-grams if <code>object</code> is of class
<code>kgram_freqs</code>, a list of words if <code>object</code> is a <code>dictionary</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This generic has slightly different behaviors when querying
for the presence of words in a dictionary and for k-gram counts
in a frequency table respectively.
For words, <code>query()</code> looks for exact matches between the input and the
dictionary entries. Queries of Begin-Of-Sentence (<code>BOS()</code>) and
End-Of-Sentence (<code>EOS()</code>) tokens always return <code>TRUE</code>, and queries
of the Unknown-Word token return <code>FALSE</code>
(see <a href="#topic+special_tokens">special_tokens</a>).
</p>
<p>On the other hand, queries of k-gram counts first perform a word level
tokenization, so that anything separated by one or more space characters
in the input is considered as a single word (thus, for instance queries of
strings such as <code>"i love you"</code>, <code>" i love you"</code>), or
<code>"i love you "</code>) all produce the same outcome). Moreover,
querying for any word outside the underlying dictionary returns the counts
corresponding to the Unknown-Word token (<code>UNK()</code>) (e.g., if
the word <code>"prcsrn"</code> is outside the dictionary, querying
<code>"i love prcsrn"</code> is the same as querying
<code>paste("i love", UNK())</code>). Queries from k-grams of order <code>k &gt; N</code>
will return <code>NA</code>.
</p>
<p>A subsetting equivalent of query, with synthax <code>object[x]</code> is available
(see the examples).
<code>query(object, x)</code>. The query of the empty string <code>""</code> returns the
total count of words, including the <code>EOS</code> and <code>UNK</code> tokens, but not
the <code>BOS</code> token.
</p>
<p>See also the examples below.
</p>


<h3>Value</h3>

<p>an integer vector, containing k-gram counts of <code>x</code>, if
<code>object</code> is a <code>kgram_freqs</code> class object, a logical vector if
<code>object</code> is a <code>dictionary</code>. Vectorized over <code>x</code>.
</p>


<h3>Author(s)</h3>

<p>Valerio Gherardi
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Querying a k-gram frequency table
f &lt;- kgram_freqs("a a b a b b a b", N = 2)
query(f, c("a", "b")) # query single words
query(f, c("a b")) # query a 2-gram
identical(query(f, "c"), query(f, "d"))  # TRUE, both "c" and "d" are &lt;UNK&gt;
identical(query(f, UNK()), query(f, "c")) # TRUE
query(f, EOS()) # 1, since text is a single sentence
f[c("b b", "b")] # query with subsetting synthax 
f[""] # 9 (includes the EOS token)

# Querying a dictionary
d &lt;- as_dictionary(c("a", "b"))
query(d, c("a", "b", "c")) # query some words
query(d, c(BOS(), EOS(), UNK())) # c(TRUE, TRUE, FALSE)
d["a"] # query with subsetting synthax

</code></pre>

<hr>
<h2 id='sample_sentences'>Random Text Generation</h2><span id='topic+sample_sentences'></span>

<h3>Description</h3>

<p>Sample sentences from a language model's probability distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sample_sentences(model, n, max_length, t = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sample_sentences_+3A_model">model</code></td>
<td>
<p>an object of class <code>language_model</code>.</p>
</td></tr>
<tr><td><code id="sample_sentences_+3A_n">n</code></td>
<td>
<p>an integer. Number of sentences to sample.</p>
</td></tr>
<tr><td><code id="sample_sentences_+3A_max_length">max_length</code></td>
<td>
<p>an integer. Maximum length of sampled sentences.</p>
</td></tr>
<tr><td><code id="sample_sentences_+3A_t">t</code></td>
<td>
<p>a positive number. Sampling temperature (optional); see Details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function samples sentences according the prescribed language model's
probability distribution, with an optional temperature parameter.
The temperature transform of a probability distribution is defined by
<code>p(t) = exp(log(p) / t) / Z(t)</code> where <code>Z(t)</code> is the partition
function, fixed by the normalization condition <code>sum(p(t)) = 1</code>.
</p>
<p>Sampling is performed word by word, using the already sampled string
as context, starting from the Begin-Of-Sentence context (i.e. <code>N - 1</code>
BOS tokens). Sampling stops either when an End-Of-Sentence token is
encountered, or when the string exceeds <code>max_length</code>, in which case
a truncated output is returned.
</p>
<p>Some language models may give a non-zero probability to the the Unknown word
token, but this is never produced in text generated by
<code>sample_sentences()</code>: when randomly sampled, it is simply ignored.
</p>
<p>Finally, a word of caution on some special smoothers: <code>"sbo"</code> smoother
(Stupid Backoff), does not produce normalized continuation probabilities,
but rather continuation <em>scores</em>. Sampling is here performed by assuming
that Stupid Backoff scores are <em>proportional</em> to actual probabilities.
'ml' smoother (Maximum Likelihood) does not assign probabilities when the
k-gram count of the context is zero. When this happens, the next word is
chosen uniformly at random from the model's dictionary.
</p>


<h3>Value</h3>

<p>a character vector of length <code>n</code>. Random sentences generated
from the language model's distribution.
</p>


<h3>Author(s)</h3>

<p>Valerio Gherardi
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Sample sentences from 8-gram Kneser-Ney model trained on Shakespeare's
# "Much Ado About Nothing"



### Prepare the model and set seed
freqs &lt;- kgram_freqs(much_ado, 8, .tknz_sent = tknz_sent)
model &lt;- language_model(freqs, "kn", D = 0.75)
set.seed(840)

sample_sentences(model, n = 3, max_length = 10)

### Sampling at high temperature
sample_sentences(model, n = 3, max_length = 10, t = 100)

### Sampling at low temperature
sample_sentences(model, n = 3, max_length = 10, t = 0.01)


</code></pre>

<hr>
<h2 id='smoothers'>k-gram Probability Smoothers</h2><span id='topic+smoothers'></span><span id='topic+info'></span>

<h3>Description</h3>

<p>Information on available k-gram continuation probability smoothers.
</p>


<h4>List of smoothers currently supported by <code>kgrams</code></h4>


<ul>
<li> <p><code>"ml"</code>: Maximum Likelihood estimate
(Markov 1913).
</p>
</li>
<li> <p><code>"add_k"</code>: Add-k smoothing
(Dale and Laplace 1995; Lidstone 1920; Johnson 1932; Jeffreys 1998).
</p>
</li>
<li> <p><code>"abs"</code>: Absolute discounting (Ney and Essen 1991).
</p>
</li>
<li> <p><code>"wb"</code>: Witten-Bell smoothing (Bell et al. 1990; Witten and Bell 1991)
</p>
</li>
<li> <p><code>"kn"</code>: Interpolated Kneser-Ney.
(Kneser and Ney 1995; Chen and Goodman 1999).
</p>
</li>
<li> <p><code>"mkn"</code>: Interpolated modified Kneser-Ney.
(Chen and Goodman 1999).
</p>
</li>
<li> <p><code>"sbo"</code>: Stupid Backoff (Brants et al. 2007).
</p>
</li></ul>




<h3>Usage</h3>

<pre><code class='language-R'>smoothers()

info(smoother)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="smoothers_+3A_smoother">smoother</code></td>
<td>
<p>a string. Code name of probability smoother.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>smoothers()</code> returns a character vector, the list of code names
of probability smoothers available in <a href="#topic+kgrams">kgrams</a>.
<code>info(smoother)</code> returns <code>NULL</code> (invisibly) and prints some
information on the selected smoothing technique.
</p>


<h3>Author(s)</h3>

<p>Valerio Gherardi
</p>


<h3>References</h3>

<p>Bell TC, Cleary JG, Witten IH (1990).
<em>Text compression</em>.
Prentice-Hall, Inc.<br /><br /> Brants T, Popat AC, Xu P, Och FJ, Dean J (2007).
&ldquo;Large Language Models in Machine Translation.&rdquo;
In <em>Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</em>, 858&ndash;867.
<a href="https://aclanthology.org/D07-1090/">https://aclanthology.org/D07-1090/</a>.<br /><br /> Chen SF, Goodman J (1999).
&ldquo;An empirical study of smoothing techniques for language modeling.&rdquo;
<em>Computer Speech &amp; Language</em>, <b>13</b>(4), 359&ndash;394.<br /><br /> Dale AI, Laplace P (1995).
<em>Philosophical essay on probabilities</em>.
Springer.<br /><br /> Jeffreys H (1998).
<em>The theory of probability</em>.
OUP Oxford.<br /><br /> Johnson WE (1932).
&ldquo;Probability: The deductive and inductive problems.&rdquo;
<em>Mind</em>, <b>41</b>(164), 409&ndash;423.<br /><br /> Kneser R, Ney H (1995).
&ldquo;Improved backing-off for M-gram language modeling.&rdquo;
<em>1995 International Conference on Acoustics, Speech, and Signal Processing</em>, <b>1</b>, 181-184 vol.1.<br /><br /> Lidstone GJ (1920).
&ldquo;Note on the general case of the Bayes-Laplace formula for inductive or a posteriori probabilities.&rdquo;
<em>Transactions of the Faculty of Actuaries</em>, <b>8</b>(182-192), 13.<br /><br /> Markov AA (1913).
&ldquo;Essai d'une Recherche Statistique Sur le Texte du Roman Eugene Oneguine.&rdquo;
<em>Bull. Acad. Imper. Sci. St. Petersburg</em>, <b>7</b>.<br /><br /> Ney H, Essen U (1991).
&ldquo;On smoothing techniques for bigram-based natural language modelling.&rdquo;
In <em>Acoustics, Speech, and Signal Processing, IEEE International Conference on</em>, 825&ndash;828.
IEEE Computer Society.<br /><br /> Witten IH, Bell TC (1991).
&ldquo;The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression.&rdquo;
<em>Ieee transactions on information theory</em>, <b>37</b>(4), 1085&ndash;1094.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># List available smoothers
smoothers()

# Get information on smoother "kn", i.e. Interpolated Kneser-Ney
info("kn")


</code></pre>

<hr>
<h2 id='tknz_sent'>Sentence tokenizer</h2><span id='topic+tknz_sent'></span>

<h3>Description</h3>

<p>Extract sentences from a batch of text lines.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tknz_sent(input, EOS = "[.?!:;]+", keep_first = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tknz_sent_+3A_input">input</code></td>
<td>
<p>a character vector.</p>
</td></tr>
<tr><td><code id="tknz_sent_+3A_eos">EOS</code></td>
<td>
<p>a regular expression matching an End-Of-Sentence delimiter.</p>
</td></tr>
<tr><td><code id="tknz_sent_+3A_keep_first">keep_first</code></td>
<td>
<p>TRUE or FALSE? Should the first character of the matches
be appended to the returned sentences (with a space)?</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>tknz_sent()</code> splits text into sentences, where sentence delimiters are
specified by a regular expression through the <code>EOS</code> argument.
Specifically, when an EOS token is found, the next sentence begins at the
first position in the input string not containing any of the EOS tokens
<em>or white space</em> (so that entries like <code>"Hi there!!!"</code> or
<code>"Hello . . ."</code> are both recognized as a single sentence).
</p>
<p>If <code>keep_first</code> is <code>FALSE</code>, the delimiters are stripped off from
the returned sequences. Otherwise, the first character of the substrings
matching the <code>EOS</code> regular expressions are appended to the corresponding
sentences, preceded by a white space.
</p>
<p>In the absence of any <code>EOS</code> delimiter, <code>tknz_sent()</code>
returns the input as is, since parts of text corresponding to different
entries of the input vector <code>x</code> are understood as parts of separate
sentences.
</p>
<p><strong>Note.</strong> This function, as well as <a href="#topic+preprocess">preprocess</a>, are included
in the library for illustrative purposes only, and are not optimized for
performance. Furthermore (for performance reasons) the function has a
separate implementation for Windows and UNIX OS types, respectively, so that
results obtained in the two cases may differ slightly.
In contexts that require full reproducibility, users are encouraged to define
their own preprocessing and tokenization custom functions - or to work with
externally processed data.
</p>


<h3>Value</h3>

<p>a character vector, each entry of which corresponds to a single
sentence.
</p>


<h3>Author(s)</h3>

<p>Valerio Gherardi
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tknz_sent("Hi there! I'm using kgrams.")
</code></pre>

<hr>
<h2 id='word_context'>Word-context conditional expression</h2><span id='topic+word_context'></span><span id='topic++25+7C+25'></span>

<h3>Description</h3>

<p>Create word-context conditional expression with the <code style="white-space: pre;">&#8288;%|%&#8288;</code> operator.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>word %|% context
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="word_context_+3A_word">word</code></td>
<td>
<p>a character vector. Word or words to include as the variable
part of the conditional expression.</p>
</td></tr>
<tr><td><code id="word_context_+3A_context">context</code></td>
<td>
<p>a character vector of length one. The fixed (or &quot;given&quot;) part
of the conditional expression.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The intuitive meaning of the operator <code style="white-space: pre;">&#8288;%|%&#8288;</code> is that of the mathematical
symbol <code>|</code> (given). This operator is used to create conditional expressions
representing the occurrence of some word after a given context (for instance,
the expression <code>"you" %|% "i love"</code> would represent the occurrence of
the word <code>"you"</code> after the string &quot;i love&quot;). The purpose of <code style="white-space: pre;">&#8288;%|%&#8288;</code> is to
create objects which can be given as input to probability() (see
<a href="#topic+probability">probability</a> for further examples).
</p>


<h3>Value</h3>

<p>a <code>word_context</code> class object.
</p>


<h3>Author(s)</h3>

<p>Valerio Gherardi
</p>


<h3>Examples</h3>

<pre><code class='language-R'>f &lt;- kgram_freqs(much_ado, 2, .tknz_sent = tknz_sent)
m &lt;- language_model(f, "kn", D = 0.5)
probability("leonato" %|% "enter", m)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
