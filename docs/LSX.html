<!DOCTYPE html><html lang="en-US"><head><title>Help for package LSX</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {LSX}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#as.coefficients_textmodel'><p>Coerce various objects to coefficients_textmodel</p>
This is a helper function used in <code style="white-space: pre;">&#8288;summary.textmodel_*&#8288;</code>.</a></li>
<li><a href='#as.seedwords'><p>Convert a list or a dictionary to seed words</p></a></li>
<li><a href='#as.statistics_textmodel'><p>Coerce various objects to statistics_textmodel</p></a></li>
<li><a href='#as.summary.textmodel'><p>Assign the summary.textmodel class to a list</p></a></li>
<li><a href='#as.textmodel_lss'><p>Create a dummy textmodel_lss object from external objects</p></a></li>
<li><a href='#bootstrap_lss'><p>[experimental] Compute polarity scores with different hyper-parameters</p></a></li>
<li><a href='#coef.textmodel_lss'><p>Extract model coefficients from a fitted textmodel_lss object</p></a></li>
<li><a href='#cohesion'><p>Computes cohesion of components of latent semantic analysis</p></a></li>
<li><a href='#data_dictionary_ideology'><p>Seed words for analysis of left-right political ideology</p></a></li>
<li><a href='#data_dictionary_sentiment'><p>Seed words for analysis of positive-negative sentiment</p></a></li>
<li><a href='#data_textmodel_lss_russianprotests'><p>A fitted LSS model on street protest in Russia</p></a></li>
<li><a href='#diagnosys'><p>Identify noisy documents in a corpus</p></a></li>
<li><a href='#optimize_lss'><p>[experimental] Compute variance ratios with different hyper-parameters</p></a></li>
<li><a href='#predict.textmodel_lss'><p>Prediction method for textmodel_lss</p></a></li>
<li><a href='#print.coefficients_textmodel'><p>Print methods for textmodel features estimates</p>
This is a helper function used in <code>print.summary.textmodel</code>.</a></li>
<li><a href='#print.statistics_textmodel'><p>Implements print methods for textmodel_statistics</p></a></li>
<li><a href='#print.summary.textmodel'><p>print method for summary.textmodel</p></a></li>
<li><a href='#seedwords'><p>Seed words for Latent Semantic Analysis</p></a></li>
<li><a href='#smooth_lss'><p>Smooth predicted polarity scores</p></a></li>
<li><a href='#textmodel_lss'><p>Fit a Latent Semantic Scaling model</p></a></li>
<li><a href='#textplot_components'><p>[experimental] Plot clusters of word vectors</p></a></li>
<li><a href='#textplot_simil'><p>Plot similarity between seed words</p></a></li>
<li><a href='#textplot_terms'><p>Plot polarity scores of words</p></a></li>
<li><a href='#textstat_context'><p>Identify context words</p></a></li>
<li><a href='#weight_seeds'><p>Internal function to generate equally-weighted seed set</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Semi-Supervised Algorithm for Document Scaling</td>
</tr>
<tr>
<td>Version:</td>
<td>1.4.2</td>
</tr>
<tr>
<td>Description:</td>
<td>A word embeddings-based semi-supervised model for document scaling Watanabe (2020) &lt;<a href="https://doi.org/10.1080%2F19312458.2020.1832976">doi:10.1080/19312458.2020.1832976</a>&gt;.
    LSS allows users to analyze large and complex corpora on arbitrary dimensions with seed words exploiting efficiency of word embeddings (SVD, Glove).
    It can generate word vectors on a users-provided corpus or incorporate a pre-trained word vectors.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods, quanteda (&ge; 2.0), quanteda.textstats, stringi,
digest, Matrix, RSpectra, proxyC, stats, ggplot2, ggrepel,
reshape2, locfit</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, spelling, knitr, rmarkdown, wordvector, irlba,
rsvd, rsparse</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/koheiw/LSX/issues">https://github.com/koheiw/LSX/issues</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://koheiw.github.io/LSX/">https://koheiw.github.io/LSX/</a></td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-01-09 00:26:52 UTC; watan</td>
</tr>
<tr>
<td>Author:</td>
<td>Kohei Watanabe [aut, cre, cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Kohei Watanabe &lt;watanabe.kohei@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-01-09 00:40:16 UTC</td>
</tr>
</table>
<hr>
<h2 id='as.coefficients_textmodel'>Coerce various objects to coefficients_textmodel
This is a helper function used in <code style="white-space: pre;">&#8288;summary.textmodel_*&#8288;</code>.</h2><span id='topic+as.coefficients_textmodel'></span>

<h3>Description</h3>

<p>Coerce various objects to coefficients_textmodel
This is a helper function used in <code style="white-space: pre;">&#8288;summary.textmodel_*&#8288;</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.coefficients_textmodel(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="as.coefficients_textmodel_+3A_x">x</code></td>
<td>
<p>an object to be coerced</p>
</td></tr>
</table>

<hr>
<h2 id='as.seedwords'>Convert a list or a dictionary to seed words</h2><span id='topic+as.seedwords'></span>

<h3>Description</h3>

<p>Convert a list or a dictionary to seed words
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.seedwords(x, upper = 1, lower = 2, concatenator = "_")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="as.seedwords_+3A_x">x</code></td>
<td>
<p>a list of characters vectors or a <a href="quanteda.html#topic+dictionary">dictionary</a> object.</p>
</td></tr>
<tr><td><code id="as.seedwords_+3A_upper">upper</code></td>
<td>
<p>numeric index or key for seed words for higher scores.</p>
</td></tr>
<tr><td><code id="as.seedwords_+3A_lower">lower</code></td>
<td>
<p>numeric index or key for seed words for lower scores.</p>
</td></tr>
<tr><td><code id="as.seedwords_+3A_concatenator">concatenator</code></td>
<td>
<p>character to replace separators of multi-word seed words.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>named numeric vector for seed words with polarity scores
</p>

<hr>
<h2 id='as.statistics_textmodel'>Coerce various objects to statistics_textmodel</h2><span id='topic+as.statistics_textmodel'></span>

<h3>Description</h3>

<p>This is a helper function used in <code style="white-space: pre;">&#8288;summary.textmodel_*&#8288;</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.statistics_textmodel(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="as.statistics_textmodel_+3A_x">x</code></td>
<td>
<p>an object to be coerced</p>
</td></tr>
</table>

<hr>
<h2 id='as.summary.textmodel'>Assign the summary.textmodel class to a list</h2><span id='topic+as.summary.textmodel'></span>

<h3>Description</h3>

<p>Assign the summary.textmodel class to a list
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.summary.textmodel(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="as.summary.textmodel_+3A_x">x</code></td>
<td>
<p>a named list</p>
</td></tr>
</table>

<hr>
<h2 id='as.textmodel_lss'>Create a dummy textmodel_lss object from external objects</h2><span id='topic+as.textmodel_lss'></span>

<h3>Description</h3>

<p>Create a dummy textmodel_lss object from a numeric vector, dense matrix or an
existing textmodel_lss object. Pre-trained word-embedding models could used
to perform LSS through this function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.textmodel_lss(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="as.textmodel_lss_+3A_x">x</code></td>
<td>
<p>an object from which a dummy <a href="#topic+textmodel_lss">textmodel_lss</a> object is created.</p>
</td></tr>
<tr><td><code id="as.textmodel_lss_+3A_...">...</code></td>
<td>
<p>arguments used to create a dummy object. <code>seeds</code> must be given
when <code>x</code> is a dense matrix.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A named numeric vector and a dense matrix are set to <code>beta</code> and
<code>embedding</code> respectively. A dense matrix should have column names for
words.
</p>


<h3>Value</h3>

<p>a dummy <a href="#topic+textmodel_lss">textmodel_lss</a> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>v &lt;- c("a" = 0.1, "z" = -0.2, "d" = 0.3, "h" = -0.05)
lss &lt;- as.textmodel_lss(v)

</code></pre>

<hr>
<h2 id='bootstrap_lss'>[experimental] Compute polarity scores with different hyper-parameters</h2><span id='topic+bootstrap_lss'></span>

<h3>Description</h3>

<p>A function to compute polarity scores of words and documents by resampling
hyper-parameters from a fitted LSS model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bootstrap_lss(
  x,
  what = c("seeds", "k"),
  mode = c("terms", "coef", "predict"),
  remove = FALSE,
  from = 100,
  to = NULL,
  by = 50,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bootstrap_lss_+3A_x">x</code></td>
<td>
<p>a fitted textmodel_lss object.</p>
</td></tr>
<tr><td><code id="bootstrap_lss_+3A_what">what</code></td>
<td>
<p>choose the hyper-parameter to resample in bootstrapping.</p>
</td></tr>
<tr><td><code id="bootstrap_lss_+3A_mode">mode</code></td>
<td>
<p>choose the type of the result of bootstrapping. If <code>coef</code>,
returns the polarity scores of words; if <code>terms</code>, returns words sorted by
the polarity scores in descending order; if <code>predict</code>, returns the polarity
scores of documents.</p>
</td></tr>
<tr><td><code id="bootstrap_lss_+3A_remove">remove</code></td>
<td>
<p>if <code>TRUE</code>, remove each seed word when <code>what = "seeds"</code>.</p>
</td></tr>
<tr><td><code id="bootstrap_lss_+3A_from">from</code>, <code id="bootstrap_lss_+3A_to">to</code>, <code id="bootstrap_lss_+3A_by">by</code></td>
<td>
<p>passed to <code>seq()</code> to generate values for <code>k</code>; only used
when <code>what = "k"</code>.</p>
</td></tr>
<tr><td><code id="bootstrap_lss_+3A_verbose">verbose</code></td>
<td>
<p>show messages if <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="bootstrap_lss_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <code><a href="#topic+as.textmodel_lss">as.textmodel_lss()</a></code> and
<code><a href="stats.html#topic+predict">predict()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>bootstrap_lss()</code> creates LSS fitted textmodel_lss objects internally by
resampling hyper-parameters and computes polarity of words or documents.
The resulting matrix can be used to asses the validity and the reliability
of seeds or k.
</p>
<p>Note that the objects created by <code><a href="#topic+as.textmodel_lss">as.textmodel_lss()</a></code> does not contain data, users
must pass <code>newdata</code> via <code>...</code> when <code>mode = "predict"</code>.
</p>

<hr>
<h2 id='coef.textmodel_lss'>Extract model coefficients from a fitted textmodel_lss object</h2><span id='topic+coef.textmodel_lss'></span><span id='topic+coefficients.textmodel_lss'></span>

<h3>Description</h3>

<p><code>coef()</code> extract model coefficients from a fitted <code>textmodel_lss</code>
object.  <code>coefficients()</code> is an alias.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'textmodel_lss'
coef(object, ...)

coefficients.textmodel_lss(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="coef.textmodel_lss_+3A_object">object</code></td>
<td>
<p>a fitted <a href="#topic+textmodel_lss">textmodel_lss</a> object.</p>
</td></tr>
<tr><td><code id="coef.textmodel_lss_+3A_...">...</code></td>
<td>
<p>not used.</p>
</td></tr>
</table>

<hr>
<h2 id='cohesion'>Computes cohesion of components of latent semantic analysis</h2><span id='topic+cohesion'></span>

<h3>Description</h3>

<p>Computes cohesion of components of latent semantic analysis
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cohesion(x, bandwidth = 10)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cohesion_+3A_x">x</code></td>
<td>
<p>a fitted <code>textmodel_lss</code></p>
</td></tr>
<tr><td><code id="cohesion_+3A_bandwidth">bandwidth</code></td>
<td>
<p>size of window for smoothing</p>
</td></tr>
</table>

<hr>
<h2 id='data_dictionary_ideology'>Seed words for analysis of left-right political ideology</h2><span id='topic+data_dictionary_ideology'></span>

<h3>Description</h3>

<p>Seed words for analysis of left-right political ideology
</p>


<h3>Examples</h3>

<pre><code class='language-R'>as.seedwords(data_dictionary_ideology)
</code></pre>

<hr>
<h2 id='data_dictionary_sentiment'>Seed words for analysis of positive-negative sentiment</h2><span id='topic+data_dictionary_sentiment'></span>

<h3>Description</h3>

<p>Seed words for analysis of positive-negative sentiment
</p>


<h3>References</h3>

<p>Turney, P. D., &amp; Littman, M. L. (2003). Measuring Praise and
Criticism: Inference of Semantic Orientation from Association. ACM Trans.
Inf. Syst., 21(4), 315–346. <a href="https://doi.org/10.1145/944012.944013">doi:10.1145/944012.944013</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>as.seedwords(data_dictionary_sentiment)
</code></pre>

<hr>
<h2 id='data_textmodel_lss_russianprotests'>A fitted LSS model on street protest in Russia</h2><span id='topic+data_textmodel_lss_russianprotests'></span>

<h3>Description</h3>

<p>This model was trained on a Russian media corpus (newspapers, TV transcripts
and newswires) to analyze framing of street protests. The scale is protests
as &quot;freedom of expression&quot; (high) vs &quot;social disorder&quot; (low). Although some
slots are missing in this object (because the model was imported from the
original Python implementation), it allows you to scale texts using
<code>predict</code>.
</p>


<h3>References</h3>

<p>Lankina, Tomila, and Kohei Watanabe. “'Russian Spring' or 'Spring
Betrayal'? The Media as a Mirror of Putin's Evolving Strategy in Ukraine.”
Europe-Asia Studies 69, no. 10 (2017): 1526–56.
<a href="https://doi.org/10.1080/09668136.2017.1397603">doi:10.1080/09668136.2017.1397603</a>.
</p>

<hr>
<h2 id='diagnosys'>Identify noisy documents in a corpus</h2><span id='topic+diagnosys'></span>

<h3>Description</h3>

<p>Identify noisy documents in a corpus
</p>


<h3>Usage</h3>

<pre><code class='language-R'>diagnosys(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="diagnosys_+3A_x">x</code></td>
<td>
<p>character or <code><a href="quanteda.html#topic+corpus">quanteda::corpus()</a></code> object whose texts will be diagnosed.</p>
</td></tr>
<tr><td><code id="diagnosys_+3A_...">...</code></td>
<td>
<p>extra arguments passed to <code><a href="quanteda.html#topic+tokens">quanteda::tokens()</a></code>.</p>
</td></tr>
</table>

<hr>
<h2 id='optimize_lss'>[experimental] Compute variance ratios with different hyper-parameters</h2><span id='topic+optimize_lss'></span>

<h3>Description</h3>

<p>[experimental] Compute variance ratios with different hyper-parameters
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optimize_lss(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="optimize_lss_+3A_x">x</code></td>
<td>
<p>a fitted textmodel_lss object.</p>
</td></tr>
<tr><td><code id="optimize_lss_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <a href="#topic+bootstrap_lss">bootstrap_lss</a>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>optimize_lss()</code> computes variance ratios with different values of
hyper-parameters using <a href="#topic+bootstrap_lss">bootstrap_lss</a>. The variance ration <code class="reqn">v</code> is defined
as </p>
<p style="text-align: center;"><code class="reqn">v = \sigma^2_{documents} / \sigma^2_{words}.</code>
</p>
<p> It maximizes
when the model best distinguishes between the documents on the latent scale.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# the unit of analysis is not sentences
dfmt_grp &lt;- dfm_group(dfmt)

# choose best k
v1 &lt;- optimize_lss(lss, what = "k", from = 50,
                   newdata = dfmt_grp, verbose = TRUE)
plot(names(v1), v1)

# find bad seed words
v2 &lt;- optimize_lss(lss, what = "seeds", remove = TRUE,
                   newdata = dfmt_grp, verbose = TRUE)
barplot(v2, las = 2)

## End(Not run)

</code></pre>

<hr>
<h2 id='predict.textmodel_lss'>Prediction method for textmodel_lss</h2><span id='topic+predict.textmodel_lss'></span>

<h3>Description</h3>

<p>Prediction method for textmodel_lss
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'textmodel_lss'
predict(
  object,
  newdata = NULL,
  se_fit = FALSE,
  density = FALSE,
  rescale = TRUE,
  cut = NULL,
  min_n = 0L,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.textmodel_lss_+3A_object">object</code></td>
<td>
<p>a fitted LSS textmodel.</p>
</td></tr>
<tr><td><code id="predict.textmodel_lss_+3A_newdata">newdata</code></td>
<td>
<p>a dfm on which prediction should be made.</p>
</td></tr>
<tr><td><code id="predict.textmodel_lss_+3A_se_fit">se_fit</code></td>
<td>
<p>if <code>TRUE</code>, returns standard error of document scores.</p>
</td></tr>
<tr><td><code id="predict.textmodel_lss_+3A_density">density</code></td>
<td>
<p>if <code>TRUE</code>, returns frequency of polarity words in documents.</p>
</td></tr>
<tr><td><code id="predict.textmodel_lss_+3A_rescale">rescale</code></td>
<td>
<p>if <code>TRUE</code>, normalizes polarity scores using <code>scale()</code>.</p>
</td></tr>
<tr><td><code id="predict.textmodel_lss_+3A_cut">cut</code></td>
<td>
<p>a vector of one or two percentile values to dichotomized polarty
scores of words. When two values are given, words between them receive zero
polarity.</p>
</td></tr>
<tr><td><code id="predict.textmodel_lss_+3A_min_n">min_n</code></td>
<td>
<p>set the minimum number of polarity words in documents.</p>
</td></tr>
<tr><td><code id="predict.textmodel_lss_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Polarity scores of documents are the means of polarity scores of
words weighted by their frequency. When <code>se_fit = TRUE</code>, this function
returns the weighted means, their standard errors, and the number of
polarity words in the documents. When <code>rescale = TRUE</code>, it converts the raw
polarity scores to z sores for easier interpretation. When <code>rescale = FALSE</code> and <code>cut</code> is used, polarity scores of documents are bounded by
[-1.0, 1.0].
</p>
<p>Documents tend to receive extreme polarity scores when they have only few
polarity words. This is problematic when LSS is applied to short documents
(e.g. social media posts) or individual sentences, but users can alleviate
this problem by adding zero polarity words to short documents using
<code>min_n</code>. This setting does not affect empty documents.
</p>

<hr>
<h2 id='print.coefficients_textmodel'>Print methods for textmodel features estimates
This is a helper function used in <code>print.summary.textmodel</code>.</h2><span id='topic+print.coefficients_textmodel'></span>

<h3>Description</h3>

<p>Print methods for textmodel features estimates
This is a helper function used in <code>print.summary.textmodel</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'coefficients_textmodel'
print(x, digits = max(3L, getOption("digits") - 3L), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.coefficients_textmodel_+3A_x">x</code></td>
<td>
<p>a coefficients_textmodel object</p>
</td></tr>
<tr><td><code id="print.coefficients_textmodel_+3A_digits">digits</code></td>
<td>
<p>minimal number of <em>significant digits</em>, see
<code><a href="base.html#topic+print.default">print.default()</a></code></p>
</td></tr>
<tr><td><code id="print.coefficients_textmodel_+3A_...">...</code></td>
<td>
<p>additional arguments not used</p>
</td></tr>
</table>

<hr>
<h2 id='print.statistics_textmodel'>Implements print methods for textmodel_statistics</h2><span id='topic+print.statistics_textmodel'></span>

<h3>Description</h3>

<p>Implements print methods for textmodel_statistics
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'statistics_textmodel'
print(x, digits = max(3L, getOption("digits") - 3L), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.statistics_textmodel_+3A_x">x</code></td>
<td>
<p>a textmodel_wordscore_statistics object</p>
</td></tr>
<tr><td><code id="print.statistics_textmodel_+3A_digits">digits</code></td>
<td>
<p>minimal number of <em>significant digits</em>, see
<code><a href="base.html#topic+print.default">print.default()</a></code></p>
</td></tr>
<tr><td><code id="print.statistics_textmodel_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods</p>
</td></tr>
</table>

<hr>
<h2 id='print.summary.textmodel'>print method for summary.textmodel</h2><span id='topic+print.summary.textmodel'></span>

<h3>Description</h3>

<p>print method for summary.textmodel
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'summary.textmodel'
print(x, digits = max(3L, getOption("digits") - 3L), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.summary.textmodel_+3A_x">x</code></td>
<td>
<p>a <code>summary.textmodel</code> object</p>
</td></tr>
<tr><td><code id="print.summary.textmodel_+3A_digits">digits</code></td>
<td>
<p>minimal number of <em>significant digits</em>, see
<code><a href="base.html#topic+print.default">print.default()</a></code></p>
</td></tr>
<tr><td><code id="print.summary.textmodel_+3A_...">...</code></td>
<td>
<p>additional arguments not used</p>
</td></tr>
</table>

<hr>
<h2 id='seedwords'>Seed words for Latent Semantic Analysis</h2><span id='topic+seedwords'></span>

<h3>Description</h3>

<p>Seed words for Latent Semantic Analysis
</p>


<h3>Usage</h3>

<pre><code class='language-R'>seedwords(type)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="seedwords_+3A_type">type</code></td>
<td>
<p>type of seed words currently only for sentiment (<code>sentiment</code>)
or political ideology (<code>ideology</code>).</p>
</td></tr>
</table>


<h3>References</h3>

<p>Turney, P. D., &amp; Littman, M. L. (2003). Measuring Praise and
Criticism: Inference of Semantic Orientation from Association. ACM Trans.
Inf. Syst., 21(4), 315–346. <a href="https://doi.org/10.1145/944012.944013">doi:10.1145/944012.944013</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>seedwords('sentiment')
</code></pre>

<hr>
<h2 id='smooth_lss'>Smooth predicted polarity scores</h2><span id='topic+smooth_lss'></span>

<h3>Description</h3>

<p>Smooth predicted polarity scores by local polynomial regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smooth_lss(
  x,
  lss_var = "fit",
  date_var = "date",
  span = 0.1,
  group = NULL,
  from = NULL,
  to = NULL,
  by = "day",
  engine = c("loess", "locfit"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="smooth_lss_+3A_x">x</code></td>
<td>
<p>a <a href="base.html#topic+data.frame">data.frame</a> containing polarity scores and dates.</p>
</td></tr>
<tr><td><code id="smooth_lss_+3A_lss_var">lss_var</code></td>
<td>
<p>the name of the column in <code>x</code> for polarity scores.</p>
</td></tr>
<tr><td><code id="smooth_lss_+3A_date_var">date_var</code></td>
<td>
<p>the name of the column in <code>x</code> for dates.</p>
</td></tr>
<tr><td><code id="smooth_lss_+3A_span">span</code></td>
<td>
<p>the level of smoothing.</p>
</td></tr>
<tr><td><code id="smooth_lss_+3A_group">group</code></td>
<td>
<p>the name of the column in <code>x</code> to smooth the data by group.</p>
</td></tr>
<tr><td><code id="smooth_lss_+3A_from">from</code>, <code id="smooth_lss_+3A_to">to</code>, <code id="smooth_lss_+3A_by">by</code></td>
<td>
<p>the the range and the internal of the smoothed scores;
passed to <a href="base.html#topic+seq.Date">seq.Date</a>.</p>
</td></tr>
<tr><td><code id="smooth_lss_+3A_engine">engine</code></td>
<td>
<p>specifies the function to be used for smoothing.</p>
</td></tr>
<tr><td><code id="smooth_lss_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the smoothing function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Smoothing is performed using <code><a href="stats.html#topic+loess">stats::loess()</a></code> or <code><a href="locfit.html#topic+locfit">locfit::locfit()</a></code>.
When the <code>x</code> has more than 10000 rows, it is usually better to choose
the latter by setting <code>engine = "locfit"</code>. In this case, <code>span</code> is passed to
<code>locfit::lp(nn = span)</code>.
</p>

<hr>
<h2 id='textmodel_lss'>Fit a Latent Semantic Scaling model</h2><span id='topic+textmodel_lss'></span><span id='topic+textmodel_lss.dfm'></span><span id='topic+textmodel_lss.fcm'></span>

<h3>Description</h3>

<p>Latent Semantic Scaling (LSS) is a word embedding-based semisupervised algorithm
for document scaling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textmodel_lss(x, ...)

## S3 method for class 'dfm'
textmodel_lss(
  x,
  seeds,
  terms = NULL,
  k = 300,
  slice = NULL,
  weight = "count",
  cache = FALSE,
  simil_method = "cosine",
  engine = c("RSpectra", "irlba", "rsvd"),
  auto_weight = FALSE,
  include_data = FALSE,
  group_data = FALSE,
  verbose = FALSE,
  ...
)

## S3 method for class 'fcm'
textmodel_lss(
  x,
  seeds,
  terms = NULL,
  w = 50,
  max_count = 10,
  weight = "count",
  cache = FALSE,
  simil_method = "cosine",
  engine = c("rsparse"),
  auto_weight = FALSE,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="textmodel_lss_+3A_x">x</code></td>
<td>
<p>a dfm or fcm created by <code><a href="quanteda.html#topic+dfm">quanteda::dfm()</a></code> or <code><a href="quanteda.html#topic+fcm">quanteda::fcm()</a></code></p>
</td></tr>
<tr><td><code id="textmodel_lss_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the underlying engine.</p>
</td></tr>
<tr><td><code id="textmodel_lss_+3A_seeds">seeds</code></td>
<td>
<p>a character vector or named numeric vector that contains seed
words. If seed words contain &quot;*&quot;, they are interpreted as glob patterns.
See <a href="quanteda.html#topic+valuetype">quanteda::valuetype</a>.</p>
</td></tr>
<tr><td><code id="textmodel_lss_+3A_terms">terms</code></td>
<td>
<p>a character vector or named numeric vector that specify words
for which polarity scores will be computed; if a numeric vector, words' polarity
scores will be weighted accordingly; if <code>NULL</code>, all the features of
<code><a href="quanteda.html#topic+dfm">quanteda::dfm()</a></code> or <code><a href="quanteda.html#topic+fcm">quanteda::fcm()</a></code> will be used.</p>
</td></tr>
<tr><td><code id="textmodel_lss_+3A_k">k</code></td>
<td>
<p>the number of singular values requested to the SVD engine. Only used
when <code>x</code> is a <code>dfm</code>.</p>
</td></tr>
<tr><td><code id="textmodel_lss_+3A_slice">slice</code></td>
<td>
<p>a number or indices of the components of word vectors used to
compute similarity; <code>slice &lt; k</code> to further truncate word vectors; useful
for diagnosys and simulation.</p>
</td></tr>
<tr><td><code id="textmodel_lss_+3A_weight">weight</code></td>
<td>
<p>weighting scheme passed to <code><a href="quanteda.html#topic+dfm_weight">quanteda::dfm_weight()</a></code>. Ignored
when <code>engine</code> is &quot;rsparse&quot;.</p>
</td></tr>
<tr><td><code id="textmodel_lss_+3A_cache">cache</code></td>
<td>
<p>if <code>TRUE</code>, save result of SVD for next execution with identical
<code>x</code> and settings. Use the <code>base::options(lss_cache_dir)</code> to change the
location cache files to be save.</p>
</td></tr>
<tr><td><code id="textmodel_lss_+3A_simil_method">simil_method</code></td>
<td>
<p>specifies method to compute similarity between features.
The value is passed to <code><a href="quanteda.textstats.html#topic+textstat_simil">quanteda.textstats::textstat_simil()</a></code>, &quot;cosine&quot; is
used otherwise.</p>
</td></tr>
<tr><td><code id="textmodel_lss_+3A_engine">engine</code></td>
<td>
<p>select the engine to factorize <code>x</code> to generate word vectors. Choose
from <code><a href="RSpectra.html#topic+svds">RSpectra::svds()</a></code>, <code><a href="irlba.html#topic+irlba">irlba::irlba()</a></code>, <code><a href="rsvd.html#topic+rsvd">rsvd::rsvd()</a></code>, and
<code><a href="rsparse.html#topic+GloVe">rsparse::GloVe()</a></code>.</p>
</td></tr>
<tr><td><code id="textmodel_lss_+3A_auto_weight">auto_weight</code></td>
<td>
<p>automatically determine weights to approximate the
polarity of terms to seed words. See details.</p>
</td></tr>
<tr><td><code id="textmodel_lss_+3A_include_data">include_data</code></td>
<td>
<p>if <code>TRUE</code>, fitted model includes the dfm supplied as <code>x</code>.</p>
</td></tr>
<tr><td><code id="textmodel_lss_+3A_group_data">group_data</code></td>
<td>
<p>if <code>TRUE</code>, apply <code>dfm_group(x)</code> before saving the dfm.</p>
</td></tr>
<tr><td><code id="textmodel_lss_+3A_verbose">verbose</code></td>
<td>
<p>show messages if <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="textmodel_lss_+3A_w">w</code></td>
<td>
<p>the size of word vectors. Used only when <code>x</code> is a <code>fcm</code>.</p>
</td></tr>
<tr><td><code id="textmodel_lss_+3A_max_count">max_count</code></td>
<td>
<p>passed to <code>x_max</code> in <code>rsparse::GloVe$new()</code> where cooccurrence
counts are ceiled to this threshold. It should be changed according to the
size of the corpus. Used only when <code>x</code> is a <code>fcm</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Latent Semantic Scaling (LSS) is a semisupervised document scaling
method. <code>textmodel_lss()</code> constructs word vectors from use-provided
documents (<code>x</code>) and weights words (<code>terms</code>) based on their semantic
proximity to seed words (<code>seeds</code>). Seed words are any known polarity words
(e.g. sentiment words) that users should manually choose. The required
number of seed words are usually 5 to 10 for each end of the scale.
</p>
<p>If <code>seeds</code> is a named numeric vector with positive and negative values, a
bipolar LSS model is construct; if <code>seeds</code> is a character vector, a
unipolar LSS model. Usually bipolar models perform better in document
scaling because both ends of the scale are defined by the user.
</p>
<p>A seed word's polarity score computed by <code>textmodel_lss()</code> tends to diverge
from its original score given by the user because it's score is affected
not only by its original score but also by the original scores of all other
seed words. If <code>auto_weight = TRUE</code>, the original scores are weighted
automatically using <code><a href="stats.html#topic+optim">stats::optim()</a></code> to minimize the squared difference
between seed words' computed and original scores. Weighted scores are saved
in <code>seed_weighted</code> in the object.
</p>
<p>Please visit the <a href="https://koheiw.github.io/LSX/">package website</a> for examples.
</p>


<h3>References</h3>

<p>Watanabe, Kohei. 2020. &quot;Latent Semantic Scaling: A Semisupervised
Text Analysis Technique for New Domains and Languages&quot;, Communication
Methods and Measures. <a href="https://doi.org/10.1080/19312458.2020.1832976">doi:10.1080/19312458.2020.1832976</a>.
</p>
<p>Watanabe, Kohei. 2017. &quot;Measuring News Bias: Russia's Official News Agency
ITAR-TASS' Coverage of the Ukraine Crisis&quot; European Journal of
Communication. <a href="https://doi.org/10.1177/0267323117695735">doi:10.1177/0267323117695735</a>.
</p>

<hr>
<h2 id='textplot_components'>[experimental] Plot clusters of word vectors</h2><span id='topic+textplot_components'></span>

<h3>Description</h3>

<p>Experimental function to find clusters of word vectors
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textplot_components(
  x,
  n = 5,
  method = "ward.D2",
  scale = c("absolute", "relative")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="textplot_components_+3A_x">x</code></td>
<td>
<p>a fitted <code>textmodel_lss</code>.</p>
</td></tr>
<tr><td><code id="textplot_components_+3A_n">n</code></td>
<td>
<p>the number of cluster.</p>
</td></tr>
<tr><td><code id="textplot_components_+3A_method">method</code></td>
<td>
<p>the method for hierarchical clustering.</p>
</td></tr>
<tr><td><code id="textplot_components_+3A_scale">scale</code></td>
<td>
<p>change the scale of y-axis.</p>
</td></tr>
</table>

<hr>
<h2 id='textplot_simil'>Plot similarity between seed words</h2><span id='topic+textplot_simil'></span>

<h3>Description</h3>

<p>Plot similarity between seed words
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textplot_simil(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="textplot_simil_+3A_x">x</code></td>
<td>
<p>fitted textmodel_lss object.</p>
</td></tr>
</table>

<hr>
<h2 id='textplot_terms'>Plot polarity scores of words</h2><span id='topic+textplot_terms'></span>

<h3>Description</h3>

<p>Plot polarity scores of words
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textplot_terms(
  x,
  highlighted = NULL,
  max_highlighted = 50,
  max_words = 1000,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="textplot_terms_+3A_x">x</code></td>
<td>
<p>a fitted textmodel_lss object.</p>
</td></tr>
<tr><td><code id="textplot_terms_+3A_highlighted">highlighted</code></td>
<td>
<p><a href="quanteda.html#topic+pattern">quanteda::pattern</a> to select words to highlight. If a
<a href="quanteda.html#topic+dictionary">quanteda::dictionary</a> is passed, words in the top-level categories are
highlighted in different colors.</p>
</td></tr>
<tr><td><code id="textplot_terms_+3A_max_highlighted">max_highlighted</code></td>
<td>
<p>the maximum number of words to highlight. When
<code>highlighted = NULL</code>, words to highlight are randomly selected
proportionally to <code>polarity ^ 2 * log(frequency)</code>.</p>
</td></tr>
<tr><td><code id="textplot_terms_+3A_max_words">max_words</code></td>
<td>
<p>the maximum number of words to plot. Words are randomly
sampled to keep the number below the limit.</p>
</td></tr>
<tr><td><code id="textplot_terms_+3A_...">...</code></td>
<td>
<p>passed to underlying functions. See the Details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Users can customize the plots through <code>...</code>, which is
passed to <code><a href="ggplot2.html#topic+geom_text">ggplot2::geom_text()</a></code> and <code><a href="ggrepel.html#topic+geom_text_repel">ggrepel::geom_text_repel()</a></code>. The
colors are specified internally but users can override the settings by appending
<code><a href="ggplot2.html#topic+scale_manual">ggplot2::scale_colour_manual()</a></code> or <code><a href="ggplot2.html#topic+scale_brewer">ggplot2::scale_colour_brewer()</a></code>. The
legend title can also be modified using <code><a href="ggplot2.html#topic+labs">ggplot2::labs()</a></code>.
</p>

<hr>
<h2 id='textstat_context'>Identify context words</h2><span id='topic+textstat_context'></span><span id='topic+char_context'></span>

<h3>Description</h3>

<p>Identify context words using user-provided patterns.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>textstat_context(
  x,
  pattern,
  valuetype = c("glob", "regex", "fixed"),
  case_insensitive = TRUE,
  window = 10,
  min_count = 10,
  remove_pattern = TRUE,
  n = 1,
  skip = 0,
  ...
)

char_context(
  x,
  pattern,
  valuetype = c("glob", "regex", "fixed"),
  case_insensitive = TRUE,
  window = 10,
  min_count = 10,
  remove_pattern = TRUE,
  p = 0.001,
  n = 1,
  skip = 0
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="textstat_context_+3A_x">x</code></td>
<td>
<p>a tokens object created by <code><a href="quanteda.html#topic+tokens">quanteda::tokens()</a></code>.</p>
</td></tr>
<tr><td><code id="textstat_context_+3A_pattern">pattern</code></td>
<td>
<p><code><a href="quanteda.html#topic+pattern">quanteda::pattern()</a></code> to specify target words.</p>
</td></tr>
<tr><td><code id="textstat_context_+3A_valuetype">valuetype</code></td>
<td>
<p>the type of pattern matching: <code>"glob"</code> for &quot;glob&quot;-style
wildcard expressions; <code>"regex"</code> for regular expressions; or <code>"fixed"</code> for
exact matching. See <code><a href="quanteda.html#topic+valuetype">quanteda::valuetype()</a></code> for details.</p>
</td></tr>
<tr><td><code id="textstat_context_+3A_case_insensitive">case_insensitive</code></td>
<td>
<p>if <code>TRUE</code>, ignore case when matching.</p>
</td></tr>
<tr><td><code id="textstat_context_+3A_window">window</code></td>
<td>
<p>size of window for collocation analysis.</p>
</td></tr>
<tr><td><code id="textstat_context_+3A_min_count">min_count</code></td>
<td>
<p>minimum frequency of words within the window to be
considered as collocations.</p>
</td></tr>
<tr><td><code id="textstat_context_+3A_remove_pattern">remove_pattern</code></td>
<td>
<p>if <code>TRUE</code>, keywords do not contain target words.</p>
</td></tr>
<tr><td><code id="textstat_context_+3A_n">n</code></td>
<td>
<p>integer vector specifying the number of elements to be concatenated
in each n-gram.  Each element of this vector will define a <code class="reqn">n</code> in the
<code class="reqn">n</code>-gram(s) that are produced.</p>
</td></tr>
<tr><td><code id="textstat_context_+3A_skip">skip</code></td>
<td>
<p>integer vector specifying the adjacency skip size for tokens
forming the n-grams, default is 0 for only immediately neighbouring words.
For <code>skipgrams</code>, <code>skip</code> can be a vector of integers, as the
&quot;classic&quot; approach to forming skip-grams is to set skip = <code class="reqn">k</code> where
<code class="reqn">k</code> is the distance for which <code class="reqn">k</code> or fewer skips are used to
construct the <code class="reqn">n</code>-gram.  Thus a &quot;4-skip-n-gram&quot; defined as <code>skip = 0:4</code> produces results that include 4 skips, 3 skips, 2 skips, 1 skip, and 0
skips (where 0 skips are typical n-grams formed from adjacent words).  See
Guthrie et al (2006).</p>
</td></tr>
<tr><td><code id="textstat_context_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <code><a href="quanteda.textstats.html#topic+textstat_keyness">quanteda.textstats::textstat_keyness()</a></code>.</p>
</td></tr>
<tr><td><code id="textstat_context_+3A_p">p</code></td>
<td>
<p>threshold for statistical significance of collocations.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="quanteda.textstats.html#topic+textstat_keyness">quanteda.textstats::textstat_keyness()</a></code>
</p>

<hr>
<h2 id='weight_seeds'>Internal function to generate equally-weighted seed set</h2><span id='topic+weight_seeds'></span>

<h3>Description</h3>

<p>Internal function to generate equally-weighted seed set
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weight_seeds(seeds, type)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
