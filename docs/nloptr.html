<!DOCTYPE html><html><head><title>Help for package nloptr</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {nloptr}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#nloptr-package'><p>R interface to NLopt</p></a></li>
<li><a href='#auglag'><p>Augmented Lagrangian Algorithm</p></a></li>
<li><a href='#bobyqa'><p>Bound Optimization by Quadratic Approximation</p></a></li>
<li><a href='#ccsaq'><p>Conservative Convex Separable Approximation with Affine Approximation plus Quadratic Penalty</p></a></li>
<li><a href='#check.derivatives'><p>Check analytic gradients of a function using finite difference</p>
approximations</a></li>
<li><a href='#cobyla'><p>Constrained Optimization by Linear Approximations</p></a></li>
<li><a href='#crs2lm'><p>Controlled Random Search</p></a></li>
<li><a href='#direct'><p>DIviding RECTangles Algorithm for Global Optimization</p></a></li>
<li><a href='#is.nloptr'><p>R interface to NLopt</p></a></li>
<li><a href='#isres'><p>Improved Stochastic Ranking Evolution Strategy</p></a></li>
<li><a href='#lbfgs'><p>Low-storage BFGS</p></a></li>
<li><a href='#mlsl'><p>Multi-level Single-linkage</p></a></li>
<li><a href='#mma'><p>Method of Moving Asymptotes</p></a></li>
<li><a href='#neldermead'><p>Nelder-Mead Simplex</p></a></li>
<li><a href='#newuoa'><p>New Unconstrained Optimization with quadratic Approximation</p></a></li>
<li><a href='#nl.grad'><p>Numerical Gradients and Jacobians</p></a></li>
<li><a href='#nl.opts'><p>Setting NL Options</p></a></li>
<li><a href='#nloptr'><p>R interface to NLopt</p></a></li>
<li><a href='#nloptr.get.default.options'><p>Return a data.frame with all the options that can be supplied to nloptr.</p></a></li>
<li><a href='#nloptr.print.options'><p>Print description of nloptr options</p></a></li>
<li><a href='#print.nloptr'><p>Print results after running nloptr</p></a></li>
<li><a href='#sbplx'><p>Subplex Algorithm</p></a></li>
<li><a href='#slsqp'><p>Sequential Quadratic Programming (SQP)</p></a></li>
<li><a href='#stogo'><p>Stochastic Global Optimization</p></a></li>
<li><a href='#tnewton'><p>Preconditioned Truncated Newton</p></a></li>
<li><a href='#varmetric'><p>Shifted Limited-memory Variable-metric</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>R Interface to NLopt</td>
</tr>
<tr>
<td>Version:</td>
<td>2.0.3</td>
</tr>
<tr>
<td>Description:</td>
<td>
    Solve optimization problems using an R interface to NLopt. NLopt is a 
    free/open-source library for nonlinear optimization, providing a common
    interface for a number of different free optimization routines available
    online as well as original implementations of various other algorithms.
    See <a href="https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/">https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/</a> for more
    information on the available algorithms. Building from included sources 
    requires 'CMake'. On Linux and 'macOS', if a suitable system build of 
    NLopt (2.7.0 or later) is found, it is used; otherwise, it is built 
    from included sources via 'CMake'. On Windows, NLopt is obtained through 
    'rwinlib' for 'R &lt;= 4.1.x' or grabbed from the 'Rtools42 toolchain' for 
    'R &gt;= 4.2.0'.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/LGPL-3">LGPL (&ge; 3)</a></td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>cmake (&gt;= 3.2.0) which is used only on Linux or
macOS systems when no system build of nlopt (&gt;= 2.7.0) can be
found.</td>
</tr>
<tr>
<td>Biarch:</td>
<td>true</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.0</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>testthat</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, xml2, testthat (&ge; 3.0.0), covr</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://astamm.github.io/nloptr/index.html">https://astamm.github.io/nloptr/index.html</a>,
<a href="https://github.com/astamm/nloptr">https://github.com/astamm/nloptr</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/astamm/nloptr/issues">https://github.com/astamm/nloptr/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-05-26 14:40:24 UTC; stamm-a</td>
</tr>
<tr>
<td>Author:</td>
<td>Jelmer Ypma [aut],
  Steven G. Johnson [aut] (author of the NLopt C library),
  Hans W. Borchers [ctb],
  Dirk Eddelbuettel [ctb],
  Brian Ripley [ctb] (build process on multiple OS),
  Kurt Hornik [ctb] (build process on multiple OS),
  Julien Chiquet [ctb],
  Avraham Adler <a href="https://orcid.org/0000-0002-3039-0703"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb] (removal deprecated calls from tests),
  Xiongtao Dai [ctb],
  Aymeric Stamm <a href="https://orcid.org/0000-0002-8725-3654"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb, cre],
  Jeroen Ooms [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Aymeric Stamm &lt;aymeric.stamm@math.cnrs.fr&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-05-26 15:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='nloptr-package'>R interface to NLopt</h2><span id='topic+nloptr-package'></span>

<h3>Description</h3>

<p>nloptr is an R interface to NLopt, a free/open-source library for nonlinear
optimization started by Steven G. Johnson, providing a common interface for
a number of different free optimization routines available online as well as
original implementations of various other algorithms. The NLopt library is
available under the GNU Lesser General Public License (LGPL), and the
copyrights are owned by a variety of authors. Most of the information here
has been taken from <a href="https://nlopt.readthedocs.io/en/latest/">the NLopt website</a>,
where more details are available.
</p>


<h3>Details</h3>

<p>NLopt addresses general nonlinear optimization problems of the form:
</p>
<p>min f(x) x in R^n
</p>
<p>s.t.  g(x) &lt;= 0 h(x) = 0 lb &lt;= x &lt;= ub
</p>
<p>where f is the objective function to be minimized and x represents the n
optimization parameters. This problem may optionally be subject to the bound
constraints (also called box constraints), lb and ub. For partially or
totally unconstrained problems the bounds can take -Inf or Inf. One may also
optionally have m nonlinear inequality constraints (sometimes called a
nonlinear programming problem), which can be specified in g(x), and equality
constraints that can be specified in h(x). Note that not all of the
algorithms in NLopt can handle constraints.
</p>
<p>An optimization problem can be solved with the general nloptr interface, or
using one of the wrapper functions for the separate algorithms; auglag,
bobyqa, cobyla, crs2lm, direct, lbfgs, mlsl, mma, neldermead, newuoa, sbplx,
slsqp, stogo, tnewton, varmetric.
</p>

<table>
<tr>
 <td style="text-align: left;"> Package: </td><td style="text-align: left;"> nloptr</td>
</tr>
<tr>
 <td style="text-align: left;"> Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;"> Version: </td><td style="text-align: left;">
0.9.9</td>
</tr>
<tr>
 <td style="text-align: left;"> Date: </td><td style="text-align: left;"> 2013-11-22</td>
</tr>
<tr>
 <td style="text-align: left;"> License: </td><td style="text-align: left;"> L-GPL</td>
</tr>
<tr>
 <td style="text-align: left;"> LazyLoad: </td><td style="text-align: left;">
yes</td>
</tr>
<tr>
 <td style="text-align: left;"> </td>
</tr>

</table>



<h3>Note</h3>

<p>See ?nloptr for more examples.
</p>


<h3>Author(s)</h3>

<p>Steven G. Johnson and others (C code) <br /> Jelmer Ypma (R interface)
<br /> Hans W. Borchers (wrappers)
</p>


<h3>References</h3>

<p>Steven G. Johnson, The NLopt nonlinear-optimization package,
<a href="https://nlopt.readthedocs.io/en/latest/">https://nlopt.readthedocs.io/en/latest/</a>
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+optim">optim</a></code> <code><a href="stats.html#topic+nlm">nlm</a></code> <code><a href="stats.html#topic+nlminb">nlminb</a></code>
<code>Rsolnp::Rsolnp</code> <code>Rsolnp::solnp</code> <code><a href="#topic+nloptr">nloptr</a></code>
<code><a href="#topic+auglag">auglag</a></code> <code><a href="#topic+bobyqa">bobyqa</a></code> <code><a href="#topic+cobyla">cobyla</a></code>
<code><a href="#topic+crs2lm">crs2lm</a></code> <code><a href="#topic+direct">direct</a></code> <code><a href="#topic+isres">isres</a></code>
<code><a href="#topic+lbfgs">lbfgs</a></code> <code><a href="#topic+mlsl">mlsl</a></code> <code><a href="#topic+mma">mma</a></code>
<code><a href="#topic+neldermead">neldermead</a></code> <code><a href="#topic+newuoa">newuoa</a></code> <code><a href="#topic+sbplx">sbplx</a></code>
<code><a href="#topic+slsqp">slsqp</a></code> <code><a href="#topic+stogo">stogo</a></code> <code><a href="#topic+tnewton">tnewton</a></code>
<code><a href="#topic+varmetric">varmetric</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Example problem, number 71 from the Hock-Schittkowsky test suite.
#
# \min_{x} x1*x4*(x1 + x2 + x3) + x3
# s.t.
#    x1*x2*x3*x4 &gt;= 25
#    x1^2 + x2^2 + x3^2 + x4^2 = 40
#    1 &lt;= x1,x2,x3,x4 &lt;= 5
#
# we re-write the inequality as
#   25 - x1*x2*x3*x4 &lt;= 0
#
# and the equality as
#   x1^2 + x2^2 + x3^2 + x4^2 - 40 = 0
#
# x0 = (1,5,5,1)
#
# optimal solution = (1.00000000, 4.74299963, 3.82114998, 1.37940829)


library('nloptr')

#
# f(x) = x1*x4*(x1 + x2 + x3) + x3
#
eval_f &lt;- function( x ) {
    return( list( "objective" = x[1]*x[4]*(x[1] + x[2] + x[3]) + x[3],
                  "gradient" = c( x[1] * x[4] + x[4] * (x[1] + x[2] + x[3]),
                                  x[1] * x[4],
                                  x[1] * x[4] + 1.0,
                                  x[1] * (x[1] + x[2] + x[3]) ) ) )
}

# constraint functions
# inequalities
eval_g_ineq &lt;- function( x ) {
    constr &lt;- c( 25 - x[1] * x[2] * x[3] * x[4] )

    grad   &lt;- c( -x[2]*x[3]*x[4],
                 -x[1]*x[3]*x[4],
                 -x[1]*x[2]*x[4],
                 -x[1]*x[2]*x[3] )
    return( list( "constraints"=constr, "jacobian"=grad ) )
}

# equalities
eval_g_eq &lt;- function( x ) {
    constr &lt;- c( x[1]^2 + x[2]^2 + x[3]^2 + x[4]^2 - 40 )

    grad   &lt;- c(  2.0*x[1],
                  2.0*x[2],
                  2.0*x[3],
                  2.0*x[4] )
    return( list( "constraints"=constr, "jacobian"=grad ) )
}

# initial values
x0 &lt;- c( 1, 5, 5, 1 )

# lower and upper bounds of control
lb &lt;- c( 1, 1, 1, 1 )
ub &lt;- c( 5, 5, 5, 5 )


local_opts &lt;- list( "algorithm" = "NLOPT_LD_MMA",
                    "xtol_rel"  = 1.0e-7 )
opts &lt;- list( "algorithm" = "NLOPT_LD_AUGLAG",
              "xtol_rel"  = 1.0e-7,
              "maxeval"   = 1000,
              "local_opts" = local_opts )

res &lt;- nloptr( x0=x0,
               eval_f=eval_f,
               lb=lb,
               ub=ub,
               eval_g_ineq=eval_g_ineq,
               eval_g_eq=eval_g_eq,
               opts=opts)
print( res )
</code></pre>

<hr>
<h2 id='auglag'>Augmented Lagrangian Algorithm</h2><span id='topic+auglag'></span>

<h3>Description</h3>

<p>The Augmented Lagrangian method adds additional terms to the unconstrained
objective function, designed to emulate a Lagrangian multiplier.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>auglag(
  x0,
  fn,
  gr = NULL,
  lower = NULL,
  upper = NULL,
  hin = NULL,
  hinjac = NULL,
  heq = NULL,
  heqjac = NULL,
  localsolver = c("COBYLA"),
  localtol = 1e-06,
  ineq2local = FALSE,
  nl.info = FALSE,
  control = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="auglag_+3A_x0">x0</code></td>
<td>
<p>starting point for searching the optimum.</p>
</td></tr>
<tr><td><code id="auglag_+3A_fn">fn</code></td>
<td>
<p>objective function that is to be minimized.</p>
</td></tr>
<tr><td><code id="auglag_+3A_gr">gr</code></td>
<td>
<p>gradient of the objective function; will be provided provided is
<code>NULL</code> and the solver requires derivatives.</p>
</td></tr>
<tr><td><code id="auglag_+3A_lower">lower</code>, <code id="auglag_+3A_upper">upper</code></td>
<td>
<p>lower and upper bound constraints.</p>
</td></tr>
<tr><td><code id="auglag_+3A_hin">hin</code>, <code id="auglag_+3A_hinjac">hinjac</code></td>
<td>
<p>defines the inequality constraints, <code>hin(x) &gt;= 0</code></p>
</td></tr>
<tr><td><code id="auglag_+3A_heq">heq</code>, <code id="auglag_+3A_heqjac">heqjac</code></td>
<td>
<p>defines the equality constraints, <code>heq(x) = 0</code>.</p>
</td></tr>
<tr><td><code id="auglag_+3A_localsolver">localsolver</code></td>
<td>
<p>available local solvers: COBYLA, LBFGS, MMA, or SLSQP.</p>
</td></tr>
<tr><td><code id="auglag_+3A_localtol">localtol</code></td>
<td>
<p>tolerance applied in the selected local solver.</p>
</td></tr>
<tr><td><code id="auglag_+3A_ineq2local">ineq2local</code></td>
<td>
<p>logical; shall the inequality constraints be treated by
the local solver?; not possible at the moment.</p>
</td></tr>
<tr><td><code id="auglag_+3A_nl.info">nl.info</code></td>
<td>
<p>logical; shall the original NLopt info been shown.</p>
</td></tr>
<tr><td><code id="auglag_+3A_control">control</code></td>
<td>
<p>list of options, see <code>nl.opts</code> for help.</p>
</td></tr>
<tr><td><code id="auglag_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This method combines the objective function and the nonlinear
inequality/equality constraints (if any) in to a single function:
essentially, the objective plus a &lsquo;penalty&rsquo; for any violated constraints.
</p>
<p>This modified objective function is then passed to another optimization
algorithm with no nonlinear constraints. If the constraints are violated by
the solution of this sub-problem, then the size of the penalties is
increased and the process is repeated; eventually, the process must converge
to the desired solution (if it exists).
</p>
<p>Since all of the actual optimization is performed in this subsidiary
optimizer, the subsidiary algorithm that you specify determines whether the
optimization is gradient-based or derivative-free.
</p>
<p>The local solvers available at the moment are <code style="white-space: pre;">&#8288;COBYLA'' (for the derivative-free approach) and &#8288;</code>LBFGS&rdquo;, <code style="white-space: pre;">&#8288;MMA'', or &#8288;</code>SLSQP&rdquo; (for smooth
functions). The tolerance for the local solver has to be provided.
</p>
<p>There is a variant that only uses penalty functions for equality constraints
while inequality constraints are passed through to the subsidiary algorithm
to be handled directly; in this case, the subsidiary algorithm must handle
inequality constraints.  (At the moment, this variant has been turned off
because of problems with the NLOPT library.)
</p>


<h3>Value</h3>

<p>List with components:
</p>
<table>
<tr><td><code>par</code></td>
<td>
<p>the optimal solution found so far.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>the function value corresponding to <code>par</code>.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>number of (outer) iterations, see <code>maxeval</code>.</p>
</td></tr>
<tr><td><code>global_solver</code></td>
<td>
<p>the global NLOPT solver used.</p>
</td></tr>
<tr><td><code>local_solver</code></td>
<td>
<p>the local NLOPT solver used, LBFGS or COBYLA.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>integer code indicating successful completion
(&gt; 0) or a possible error number (&lt; 0).</p>
</td></tr>
<tr><td><code>message</code></td>
<td>
<p>character string produced by NLopt and giving additional
information.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Birgin and Martinez provide their own free implementation of the
method as part of the TANGO project; other implementations can be found in
semi-free packages like LANCELOT.
</p>


<h3>Author(s)</h3>

<p>Hans W. Borchers
</p>


<h3>References</h3>

<p>Andrew R. Conn, Nicholas I. M. Gould, and Philippe L. Toint, &ldquo;A
globally convergent augmented Lagrangian algorithm for optimization with
general constraints and simple bounds,&rdquo; SIAM J. Numer. Anal. vol. 28, no.
2, p. 545-572 (1991).
</p>
<p>E. G. Birgin and J. M. Martinez, &ldquo;Improving ultimate convergence of an
augmented Lagrangian method,&quot; Optimization Methods and Software vol. 23, no.
2, p. 177-195 (2008).
</p>


<h3>See Also</h3>

<p><code>alabama::auglag</code>, <code>Rsolnp::solnp</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x0 &lt;- c(1, 1)
fn &lt;- function(x) (x[1]-2)^2 + (x[2]-1)^2
hin &lt;- function(x) -0.25*x[1]^2 - x[2]^2 + 1    # hin &gt;= 0
heq &lt;- function(x) x[1] - 2*x[2] + 1            # heq == 0
gr &lt;- function(x) nl.grad(x, fn)
hinjac &lt;- function(x) nl.jacobian(x, hin)
heqjac &lt;- function(x) nl.jacobian(x, heq)

auglag(x0, fn, gr = NULL, hin = hin, heq = heq) # with COBYLA
# $par:     0.8228761 0.9114382
# $value:   1.393464
# $iter:    1001

auglag(x0, fn, gr = NULL, hin = hin, heq = heq, localsolver = "SLSQP")
# $par:     0.8228757 0.9114378
# $value:   1.393465
# $iter     173

##  Example from the alabama::auglag help page
fn &lt;- function(x) (x[1] + 3*x[2] + x[3])^2 + 4 * (x[1] - x[2])^2
heq &lt;- function(x) x[1] + x[2] + x[3] - 1
hin &lt;- function(x) c(6*x[2] + 4*x[3] - x[1]^3 - 3, x[1], x[2], x[3])

auglag(runif(3), fn, hin = hin, heq = heq, localsolver="lbfgs")
# $par:     2.380000e-09 1.086082e-14 1.000000e+00
# $value:   1
# $iter:    289

##  Powell problem from the Rsolnp::solnp help page
x0 &lt;- c(-2, 2, 2, -1, -1)
fn1  &lt;- function(x) exp(x[1]*x[2]*x[3]*x[4]*x[5])
eqn1 &lt;-function(x)
	c(x[1]*x[1]+x[2]*x[2]+x[3]*x[3]+x[4]*x[4]+x[5]*x[5],
	  x[2]*x[3]-5*x[4]*x[5],
	  x[1]*x[1]*x[1]+x[2]*x[2]*x[2])

auglag(x0, fn1, heq = eqn1, localsolver = "mma")
# $par: -3.988458e-10 -1.654201e-08 -3.752028e-10  8.904445e-10  8.926336e-10
# $value:   1
# $iter:    1001

</code></pre>

<hr>
<h2 id='bobyqa'>Bound Optimization by Quadratic Approximation</h2><span id='topic+bobyqa'></span>

<h3>Description</h3>

<p>BOBYQA performs derivative-free bound-constrained optimization using an
iteratively constructed quadratic approximation for the objective function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bobyqa(
  x0,
  fn,
  lower = NULL,
  upper = NULL,
  nl.info = FALSE,
  control = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bobyqa_+3A_x0">x0</code></td>
<td>
<p>starting point for searching the optimum.</p>
</td></tr>
<tr><td><code id="bobyqa_+3A_fn">fn</code></td>
<td>
<p>objective function that is to be minimized.</p>
</td></tr>
<tr><td><code id="bobyqa_+3A_lower">lower</code>, <code id="bobyqa_+3A_upper">upper</code></td>
<td>
<p>lower and upper bound constraints.</p>
</td></tr>
<tr><td><code id="bobyqa_+3A_nl.info">nl.info</code></td>
<td>
<p>logical; shall the original NLopt info been shown.</p>
</td></tr>
<tr><td><code id="bobyqa_+3A_control">control</code></td>
<td>
<p>list of options, see <code>nl.opts</code> for help.</p>
</td></tr>
<tr><td><code id="bobyqa_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is an algorithm derived from the BOBYQA Fortran subroutine of Powell,
converted to C and modified for the NLOPT stopping criteria.
</p>


<h3>Value</h3>

<p>List with components:
</p>
<table>
<tr><td><code>par</code></td>
<td>
<p>the optimal solution found so far.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>the function value corresponding to <code>par</code>.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>number of (outer) iterations, see <code>maxeval</code>.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>integer code indicating successful completion (&gt; 0)
or a possible error number (&lt; 0).</p>
</td></tr>
<tr><td><code>message</code></td>
<td>
<p>character string produced by NLopt and giving additional
information.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Because BOBYQA constructs a quadratic approximation of the objective,
it may perform poorly for objective functions that are not
twice-differentiable.
</p>


<h3>References</h3>

<p>M. J. D. Powell. &ldquo;The BOBYQA algorithm for bound constrained
optimization without derivatives,&rdquo; Department of Applied Mathematics and
Theoretical Physics, Cambridge England, technical reportNA2009/06 (2009).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cobyla">cobyla</a></code>, <code><a href="#topic+newuoa">newuoa</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
fr &lt;- function(x) {   ## Rosenbrock Banana function
    100 * (x[2] - x[1]^2)^2 + (1 - x[1])^2
}
(S &lt;- bobyqa(c(0, 0, 0), fr, lower = c(0, 0, 0), upper = c(0.5, 0.5, 0.5)))

</code></pre>

<hr>
<h2 id='ccsaq'>Conservative Convex Separable Approximation with Affine Approximation plus Quadratic Penalty</h2><span id='topic+ccsaq'></span>

<h3>Description</h3>

<p>This is a variant of CCSA (&quot;conservative convex separable approximation&quot;)
which, instead of constructing local MMA approximations, constructs simple
quadratic approximations (or rather, affine approximations plus a quadratic
penalty term to stay conservative)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ccsaq(
  x0,
  fn,
  gr = NULL,
  lower = NULL,
  upper = NULL,
  hin = NULL,
  hinjac = NULL,
  nl.info = FALSE,
  control = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ccsaq_+3A_x0">x0</code></td>
<td>
<p>starting point for searching the optimum.</p>
</td></tr>
<tr><td><code id="ccsaq_+3A_fn">fn</code></td>
<td>
<p>objective function that is to be minimized.</p>
</td></tr>
<tr><td><code id="ccsaq_+3A_gr">gr</code></td>
<td>
<p>gradient of function <code>fn</code>; will be calculated numerically if
not specified.</p>
</td></tr>
<tr><td><code id="ccsaq_+3A_lower">lower</code>, <code id="ccsaq_+3A_upper">upper</code></td>
<td>
<p>lower and upper bound constraints.</p>
</td></tr>
<tr><td><code id="ccsaq_+3A_hin">hin</code></td>
<td>
<p>function defining the inequality constraints, that is
<code>hin&gt;=0</code> for all components.</p>
</td></tr>
<tr><td><code id="ccsaq_+3A_hinjac">hinjac</code></td>
<td>
<p>Jacobian of function <code>hin</code>; will be calculated
numerically if not specified.</p>
</td></tr>
<tr><td><code id="ccsaq_+3A_nl.info">nl.info</code></td>
<td>
<p>logical; shall the original NLopt info been shown.</p>
</td></tr>
<tr><td><code id="ccsaq_+3A_control">control</code></td>
<td>
<p>list of options, see <code>nl.opts</code> for help.</p>
</td></tr>
<tr><td><code id="ccsaq_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with components:
</p>
<table>
<tr><td><code>par</code></td>
<td>
<p>the optimal solution found so far.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>the function value corresponding to <code>par</code>.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>number of (outer) iterations, see <code>maxeval</code>.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>integer code indicating successful completion (&gt; 1)
or a possible error number (&lt; 0).</p>
</td></tr>
<tr><td><code>message</code></td>
<td>
<p>character string produced by NLopt and giving additional
information.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>&ldquo;Globally convergent&rdquo; does not mean that this algorithm converges to
the global optimum; it means that it is guaranteed to converge to some local
minimum from any feasible starting point.
</p>


<h3>References</h3>

<p>Krister Svanberg, &ldquo;A class of globally convergent optimization
methods based on conservative convex separable approximations,&rdquo; SIAM J.
Optim. 12 (2), p. 555-573 (2002).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mma">mma</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##  Solve the Hock-Schittkowski problem no. 100 with analytic gradients
x0.hs100 &lt;- c(1, 2, 0, 4, 0, 1, 1)
fn.hs100 &lt;- function(x) {
    (x[1]-10)^2 + 5*(x[2]-12)^2 + x[3]^4 + 3*(x[4]-11)^2 + 10*x[5]^6 +
                  7*x[6]^2 + x[7]^4 - 4*x[6]*x[7] - 10*x[6] - 8*x[7]
}
hin.hs100 &lt;- function(x) {
    h &lt;- numeric(4)
    h[1] &lt;- 127 - 2*x[1]^2 - 3*x[2]^4 - x[3] - 4*x[4]^2 - 5*x[5]
    h[2] &lt;- 282 - 7*x[1] - 3*x[2] - 10*x[3]^2 - x[4] + x[5]
    h[3] &lt;- 196 - 23*x[1] - x[2]^2 - 6*x[6]^2 + 8*x[7]
    h[4] &lt;- -4*x[1]^2 - x[2]^2 + 3*x[1]*x[2] -2*x[3]^2 - 5*x[6]	+11*x[7]
    return(h)
}
gr.hs100 &lt;- function(x) {
   c(  2 * x[1] -  20,
      10 * x[2] - 120,
       4 * x[3]^3,
       6 * x[4] - 66,
      60 * x[5]^5,
      14 * x[6]   - 4 * x[7] - 10,
       4 * x[7]^3 - 4 * x[6] -  8 )}
hinjac.hs100 &lt;- function(x) {
    matrix(c(4*x[1], 12*x[2]^3, 1, 8*x[4], 5, 0, 0,
        7, 3, 20*x[3], 1, -1, 0, 0,
        23, 2*x[2], 0, 0, 0, 12*x[6], -8,
        8*x[1]-3*x[2], 2*x[2]-3*x[1], 4*x[3], 0, 0, 5, -11), 4, 7, byrow=TRUE)
}

# incorrect result with exact jacobian
S &lt;- ccsaq(x0.hs100, fn.hs100, gr = gr.hs100,
            hin = hin.hs100, hinjac = hinjac.hs100,
            nl.info = TRUE, control = list(xtol_rel = 1e-8))


S &lt;- ccsaq(x0.hs100, fn.hs100, hin = hin.hs100,
            nl.info = TRUE, control = list(xtol_rel = 1e-8))

</code></pre>

<hr>
<h2 id='check.derivatives'>Check analytic gradients of a function using finite difference
approximations</h2><span id='topic+check.derivatives'></span>

<h3>Description</h3>

<p>This function compares the analytic gradients of a function with a finite
difference approximation and prints the results of these checks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check.derivatives(
  .x,
  func,
  func_grad,
  check_derivatives_tol = 1e-04,
  check_derivatives_print = "all",
  func_grad_name = "grad_f",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check.derivatives_+3A_.x">.x</code></td>
<td>
<p>point at which the comparison is done.</p>
</td></tr>
<tr><td><code id="check.derivatives_+3A_func">func</code></td>
<td>
<p>function to be evaluated.</p>
</td></tr>
<tr><td><code id="check.derivatives_+3A_func_grad">func_grad</code></td>
<td>
<p>function calculating the analytic gradients.</p>
</td></tr>
<tr><td><code id="check.derivatives_+3A_check_derivatives_tol">check_derivatives_tol</code></td>
<td>
<p>option determining when differences between the
analytic gradient and its finite difference approximation are flagged as an
error.</p>
</td></tr>
<tr><td><code id="check.derivatives_+3A_check_derivatives_print">check_derivatives_print</code></td>
<td>
<p>option related to the amount of output. 'all'
means that all comparisons are shown, 'errors' only shows comparisons that
are flagged as an error, and 'none' shows the number of errors only.</p>
</td></tr>
<tr><td><code id="check.derivatives_+3A_func_grad_name">func_grad_name</code></td>
<td>
<p>option to change the name of the gradient function
that shows up in the output.</p>
</td></tr>
<tr><td><code id="check.derivatives_+3A_...">...</code></td>
<td>
<p>further arguments passed to the functions func and func_grad.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The return value contains a list with the analytic gradient, its
finite difference approximation, the relative errors, and vector comparing
the relative errors to the tolerance.
</p>


<h3>Author(s)</h3>

<p>Jelmer Ypma
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nloptr">nloptr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library('nloptr')

# example with correct gradient
f &lt;- function( x, a ) {
	return( sum( ( x - a )^2 ) )
}

f_grad &lt;- function( x, a ) {
	return( 2*( x - a ) )
}

check.derivatives( .x=1:10, func=f, func_grad=f_grad,
    check_derivatives_print='none', a=runif(10) )

# example with incorrect gradient
f_grad &lt;- function( x, a ) {
	return( 2*( x - a ) + c(0,.1,rep(0,8)) )
}

check.derivatives( .x=1:10, func=f, func_grad=f_grad,
    check_derivatives_print='errors', a=runif(10) )

# example with incorrect gradient of vector-valued function
g &lt;- function( x, a ) {
	return( c( sum(x-a), sum( (x-a)^2 ) ) )
}

g_grad &lt;- function( x, a ) {
	return( rbind( rep(1,length(x)) + c(0,.01,rep(0,8)), 2*(x-a) + c(0,.1,rep(0,8)) ) )
}

check.derivatives( .x=1:10, func=g, func_grad=g_grad,
    check_derivatives_print='all', a=runif(10) )

</code></pre>

<hr>
<h2 id='cobyla'>Constrained Optimization by Linear Approximations</h2><span id='topic+cobyla'></span>

<h3>Description</h3>

<p>COBYLA is an algorithm for derivative-free optimization with nonlinear
inequality and equality constraints (but see below).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cobyla(
  x0,
  fn,
  lower = NULL,
  upper = NULL,
  hin = NULL,
  nl.info = FALSE,
  control = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cobyla_+3A_x0">x0</code></td>
<td>
<p>starting point for searching the optimum.</p>
</td></tr>
<tr><td><code id="cobyla_+3A_fn">fn</code></td>
<td>
<p>objective function that is to be minimized.</p>
</td></tr>
<tr><td><code id="cobyla_+3A_lower">lower</code>, <code id="cobyla_+3A_upper">upper</code></td>
<td>
<p>lower and upper bound constraints.</p>
</td></tr>
<tr><td><code id="cobyla_+3A_hin">hin</code></td>
<td>
<p>function defining the inequality constraints, that is
<code>hin&gt;=0</code> for all components.</p>
</td></tr>
<tr><td><code id="cobyla_+3A_nl.info">nl.info</code></td>
<td>
<p>logical; shall the original NLopt info been shown.</p>
</td></tr>
<tr><td><code id="cobyla_+3A_control">control</code></td>
<td>
<p>list of options, see <code>nl.opts</code> for help.</p>
</td></tr>
<tr><td><code id="cobyla_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It constructs successive linear approximations of the objective function and
constraints via a simplex of n+1 points (in n dimensions), and optimizes
these approximations in a trust region at each step.
</p>
<p>COBYLA supports equality constraints by transforming them into two
inequality constraints. As this does not give full satisfaction with the
implementation in NLOPT, it has not been made available here.
</p>


<h3>Value</h3>

<p>List with components:
</p>
<table>
<tr><td><code>par</code></td>
<td>
<p>the optimal solution found so far.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>the function value corresponding to <code>par</code>.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>number of (outer) iterations, see <code>maxeval</code>.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>integer code indicating successful completion (&gt; 0)
or a possible error number (&lt; 0).</p>
</td></tr>
<tr><td><code>message</code></td>
<td>
<p>character string produced by NLopt and giving additional
information.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The original code, written in Fortran by Powell, was converted in C
for the Scipy project.
</p>


<h3>Author(s)</h3>

<p>Hans W. Borchers
</p>


<h3>References</h3>

<p>M. J. D. Powell, &ldquo;A direct search optimization method that
models the objective and constraint functions by linear interpolation,&rdquo; in
Advances in Optimization and Numerical Analysis, eds. S. Gomez and J.-P.
Hennart (Kluwer Academic: Dordrecht, 1994), p. 51-67.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bobyqa">bobyqa</a></code>, <code><a href="#topic+newuoa">newuoa</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### Solve Hock-Schittkowski no. 100
x0.hs100 &lt;- c(1, 2, 0, 4, 0, 1, 1)
fn.hs100 &lt;- function(x) {
    (x[1]-10)^2 + 5*(x[2]-12)^2 + x[3]^4 + 3*(x[4]-11)^2 + 10*x[5]^6 +
                  7*x[6]^2 + x[7]^4 - 4*x[6]*x[7] - 10*x[6] - 8*x[7]
}
hin.hs100 &lt;- function(x) {
    h &lt;- numeric(4)
    h[1] &lt;- 127 - 2*x[1]^2 - 3*x[2]^4 - x[3] - 4*x[4]^2 - 5*x[5]
    h[2] &lt;- 282 - 7*x[1] - 3*x[2] - 10*x[3]^2 - x[4] + x[5]
    h[3] &lt;- 196 - 23*x[1] - x[2]^2 - 6*x[6]^2 + 8*x[7]
    h[4] &lt;- -4*x[1]^2 - x[2]^2 + 3*x[1]*x[2] -2*x[3]^2 - 5*x[6]	+11*x[7]
    return(h)
}

S &lt;- cobyla(x0.hs100, fn.hs100, hin = hin.hs100,
            nl.info = TRUE, control = list(xtol_rel = 1e-8, maxeval = 2000))
## Optimal value of objective function:  680.630057374431

</code></pre>

<hr>
<h2 id='crs2lm'>Controlled Random Search</h2><span id='topic+crs2lm'></span>

<h3>Description</h3>

<p>The Controlled Random Search (CRS) algorithm (and in particular, the CRS2
variant) with the &lsquo;local mutation&rsquo; modification.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crs2lm(
  x0,
  fn,
  lower,
  upper,
  maxeval = 10000,
  pop.size = 10 * (length(x0) + 1),
  ranseed = NULL,
  xtol_rel = 1e-06,
  nl.info = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crs2lm_+3A_x0">x0</code></td>
<td>
<p>initial point for searching the optimum.</p>
</td></tr>
<tr><td><code id="crs2lm_+3A_fn">fn</code></td>
<td>
<p>objective function that is to be minimized.</p>
</td></tr>
<tr><td><code id="crs2lm_+3A_lower">lower</code>, <code id="crs2lm_+3A_upper">upper</code></td>
<td>
<p>lower and upper bound constraints.</p>
</td></tr>
<tr><td><code id="crs2lm_+3A_maxeval">maxeval</code></td>
<td>
<p>maximum number of function evaluations.</p>
</td></tr>
<tr><td><code id="crs2lm_+3A_pop.size">pop.size</code></td>
<td>
<p>population size.</p>
</td></tr>
<tr><td><code id="crs2lm_+3A_ranseed">ranseed</code></td>
<td>
<p>prescribe seed for random number generator.</p>
</td></tr>
<tr><td><code id="crs2lm_+3A_xtol_rel">xtol_rel</code></td>
<td>
<p>stopping criterion for relative change reached.</p>
</td></tr>
<tr><td><code id="crs2lm_+3A_nl.info">nl.info</code></td>
<td>
<p>logical; shall the original NLopt info been shown.</p>
</td></tr>
<tr><td><code id="crs2lm_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The CRS algorithms are sometimes compared to genetic algorithms, in that
they start with a random population of points, and randomly evolve these
points by heuristic rules. In this case, the evolution somewhat resembles a
randomized Nelder-Mead algorithm.
</p>
<p>The published results for CRS seem to be largely empirical.
</p>


<h3>Value</h3>

<p>List with components:
</p>
<table>
<tr><td><code>par</code></td>
<td>
<p>the optimal solution found so far.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>the function value corresponding to <code>par</code>.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>number of (outer) iterations, see <code>maxeval</code>.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>integer code indicating successful completion (&gt; 0)
or a possible error number (&lt; 0).</p>
</td></tr>
<tr><td><code>message</code></td>
<td>
<p>character string produced by NLopt and giving additional
information.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The initial population size for CRS defaults to <code>10x(n+1)</code> in
<code>n</code> dimensions, but this can be changed; the initial population must be
at least <code>n+1</code>.
</p>


<h3>References</h3>

<p>W. L. Price, &ldquo;Global optimization by controlled random
search,&rdquo; J. Optim. Theory Appl. 40 (3), p. 333-348 (1983).
</p>
<p>P. Kaelo and M. M. Ali, &ldquo;Some variants of the controlled random search
algorithm for global optimization,&rdquo; J. Optim. Theory Appl. 130 (2), 253-264
(2006).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### Minimize the Hartmann6 function
hartmann6 &lt;- function(x) {
    n &lt;- length(x)
    a &lt;- c(1.0, 1.2, 3.0, 3.2)
    A &lt;- matrix(c(10.0,  0.05, 3.0, 17.0,
                   3.0, 10.0,  3.5,  8.0,
                  17.0, 17.0,  1.7,  0.05,
                   3.5,  0.1, 10.0, 10.0,
                   1.7,  8.0, 17.0,  0.1,
                   8.0, 14.0,  8.0, 14.0), nrow=4, ncol=6)
    B  &lt;- matrix(c(.1312,.2329,.2348,.4047,
                   .1696,.4135,.1451,.8828,
                   .5569,.8307,.3522,.8732,
                   .0124,.3736,.2883,.5743,
                   .8283,.1004,.3047,.1091,
                   .5886,.9991,.6650,.0381), nrow=4, ncol=6)
    fun &lt;- 0.0
    for (i in 1:4) {
        fun &lt;- fun - a[i] * exp(-sum(A[i,]*(x-B[i,])^2))
    }
    return(fun)
}

S &lt;- mlsl(x0 = rep(0, 6), hartmann6, lower = rep(0,6), upper = rep(1,6),
            nl.info = TRUE, control=list(xtol_rel=1e-8, maxeval=1000))
## Number of Iterations....: 4050
## Termination conditions:  maxeval: 10000	xtol_rel: 1e-06
## Number of inequality constraints:  0
## Number of equality constraints:    0
## Optimal value of objective function:  -3.32236801141328
## Optimal value of controls:
##     0.2016893 0.1500105 0.4768738 0.2753326 0.3116516 0.6573004

</code></pre>

<hr>
<h2 id='direct'>DIviding RECTangles Algorithm for Global Optimization</h2><span id='topic+direct'></span><span id='topic+directL'></span>

<h3>Description</h3>

<p>DIRECT is a deterministic search algorithm based on systematic division of
the search domain into smaller and smaller hyperrectangles. The DIRECT_L
makes the algorithm more biased towards local search (more efficient for
functions without too many minima).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>direct(
  fn,
  lower,
  upper,
  scaled = TRUE,
  original = FALSE,
  nl.info = FALSE,
  control = list(),
  ...
)

directL(
  fn,
  lower,
  upper,
  randomized = FALSE,
  original = FALSE,
  nl.info = FALSE,
  control = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="direct_+3A_fn">fn</code></td>
<td>
<p>objective function that is to be minimized.</p>
</td></tr>
<tr><td><code id="direct_+3A_lower">lower</code>, <code id="direct_+3A_upper">upper</code></td>
<td>
<p>lower and upper bound constraints.</p>
</td></tr>
<tr><td><code id="direct_+3A_scaled">scaled</code></td>
<td>
<p>logical; shall the hypercube be scaled before starting.</p>
</td></tr>
<tr><td><code id="direct_+3A_original">original</code></td>
<td>
<p>logical; whether to use the original implementation by
Gablonsky &ndash; the performance is mostly similar.</p>
</td></tr>
<tr><td><code id="direct_+3A_nl.info">nl.info</code></td>
<td>
<p>logical; shall the original NLopt info been shown.</p>
</td></tr>
<tr><td><code id="direct_+3A_control">control</code></td>
<td>
<p>list of options, see <code>nl.opts</code> for help.</p>
</td></tr>
<tr><td><code id="direct_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the function.</p>
</td></tr>
<tr><td><code id="direct_+3A_randomized">randomized</code></td>
<td>
<p>logical; shall some randomization be used to decide which
dimension to halve next in the case of near-ties.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The DIRECT and DIRECT-L algorithms start by rescaling the bound constraints
to a hypercube, which gives all dimensions equal weight in the search
procedure. If your dimensions do not have equal weight, e.g. if you have a
&ldquo;long and skinny&rdquo; search space and your function varies at about the same
speed in all directions, it may be better to use unscaled variant of the
DIRECT algorithm.
</p>
<p>The algorithms only handle finite bound constraints which must be provided.
The original versions may include some support for arbitrary nonlinear
inequality, but this has not been tested.
</p>
<p>The original versions do not have randomized or unscaled variants, so these
options will be disregarded for these versions.
</p>


<h3>Value</h3>

<p>List with components:
</p>
<table>
<tr><td><code>par</code></td>
<td>
<p>the optimal solution found so far.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>the function value corresponding to <code>par</code>.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>number of (outer) iterations, see <code>maxeval</code>.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>integer code indicating successful completion (&gt; 0)
or a possible error number (&lt; 0).</p>
</td></tr>
<tr><td><code>message</code></td>
<td>
<p>character string produced by NLopt and giving additional
information.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The DIRECT_L algorithm should be tried first.
</p>


<h3>Author(s)</h3>

<p>Hans W. Borchers
</p>


<h3>References</h3>

<p>D. R. Jones, C. D. Perttunen, and B. E. Stuckmann,
&ldquo;Lipschitzian optimization without the lipschitz constant,&rdquo; J.
Optimization Theory and Applications, vol. 79, p. 157 (1993).
</p>
<p>J. M. Gablonsky and C. T. Kelley, &ldquo;A locally-biased form of the DIRECT
algorithm,&quot; J. Global Optimization, vol. 21 (1), p. 27-37 (2001).
</p>


<h3>See Also</h3>

<p>The <code>dfoptim</code> package will provide a pure R version of this
algorithm.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### Minimize the Hartmann6 function
hartmann6 &lt;- function(x) {
    n &lt;- length(x)
    a &lt;- c(1.0, 1.2, 3.0, 3.2)
    A &lt;- matrix(c(10.0,  0.05, 3.0, 17.0,
                   3.0, 10.0,  3.5,  8.0,
                  17.0, 17.0,  1.7,  0.05,
                   3.5,  0.1, 10.0, 10.0,
                   1.7,  8.0, 17.0,  0.1,
                   8.0, 14.0,  8.0, 14.0), nrow=4, ncol=6)
    B  &lt;- matrix(c(.1312,.2329,.2348,.4047,
                   .1696,.4135,.1451,.8828,
                   .5569,.8307,.3522,.8732,
                   .0124,.3736,.2883,.5743,
                   .8283,.1004,.3047,.1091,
                   .5886,.9991,.6650,.0381), nrow=4, ncol=6)
    fun &lt;- 0.0
    for (i in 1:4) {
        fun &lt;- fun - a[i] * exp(-sum(A[i,]*(x-B[i,])^2))
    }
    return(fun)
}
S &lt;- directL(hartmann6, rep(0,6), rep(1,6),
             nl.info = TRUE, control=list(xtol_rel=1e-8, maxeval=1000))
## Number of Iterations....: 500
## Termination conditions:  stopval: -Inf
##     xtol_rel: 1e-08,  maxeval: 500,  ftol_rel: 0,  ftol_abs: 0
## Number of inequality constraints:  0
## Number of equality constraints:    0
## Current value of objective function:  -3.32236800687327
## Current value of controls:
##     0.2016884 0.1500025 0.4768667 0.2753391 0.311648 0.6572931

</code></pre>

<hr>
<h2 id='is.nloptr'>R interface to NLopt</h2><span id='topic+is.nloptr'></span>

<h3>Description</h3>

<p>is.nloptr preforms checks to see if a fully specified problem is supplied to
nloptr. Mostly for internal use.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.nloptr(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.nloptr_+3A_x">x</code></td>
<td>
<p>object to be tested.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Logical. Return TRUE if all tests were passed, otherwise return
FALSE or exit with Error.
</p>


<h3>Author(s)</h3>

<p>Jelmer Ypma
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nloptr">nloptr</a></code>
</p>

<hr>
<h2 id='isres'>Improved Stochastic Ranking Evolution Strategy</h2><span id='topic+isres'></span>

<h3>Description</h3>

<p>The Improved Stochastic Ranking Evolution Strategy (ISRES) algorithm for
nonlinearly constrained global optimization (or at least semi-global:
although it has heuristics to escape local optima.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isres(
  x0,
  fn,
  lower,
  upper,
  hin = NULL,
  heq = NULL,
  maxeval = 10000,
  pop.size = 20 * (length(x0) + 1),
  xtol_rel = 1e-06,
  nl.info = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="isres_+3A_x0">x0</code></td>
<td>
<p>initial point for searching the optimum.</p>
</td></tr>
<tr><td><code id="isres_+3A_fn">fn</code></td>
<td>
<p>objective function that is to be minimized.</p>
</td></tr>
<tr><td><code id="isres_+3A_lower">lower</code>, <code id="isres_+3A_upper">upper</code></td>
<td>
<p>lower and upper bound constraints.</p>
</td></tr>
<tr><td><code id="isres_+3A_hin">hin</code></td>
<td>
<p>function defining the inequality constraints, that is
<code>hin&gt;=0</code> for all components.</p>
</td></tr>
<tr><td><code id="isres_+3A_heq">heq</code></td>
<td>
<p>function defining the equality constraints, that is <code>heq==0</code>
for all components.</p>
</td></tr>
<tr><td><code id="isres_+3A_maxeval">maxeval</code></td>
<td>
<p>maximum number of function evaluations.</p>
</td></tr>
<tr><td><code id="isres_+3A_pop.size">pop.size</code></td>
<td>
<p>population size.</p>
</td></tr>
<tr><td><code id="isres_+3A_xtol_rel">xtol_rel</code></td>
<td>
<p>stopping criterion for relative change reached.</p>
</td></tr>
<tr><td><code id="isres_+3A_nl.info">nl.info</code></td>
<td>
<p>logical; shall the original NLopt info been shown.</p>
</td></tr>
<tr><td><code id="isres_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The evolution strategy is based on a combination of a mutation rule (with a
log-normal step-size update and exponential smoothing) and differential
variation (a Nelder-Mead-like update rule). The fitness ranking is simply
via the objective function for problems without nonlinear constraints, but
when nonlinear constraints are included the stochastic ranking proposed by
Runarsson and Yao is employed.
</p>
<p>This method supports arbitrary nonlinear inequality and equality constraints
in addition to the bound constraints.
</p>


<h3>Value</h3>

<p>List with components:
</p>
<table>
<tr><td><code>par</code></td>
<td>
<p>the optimal solution found so far.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>the function value corresponding to <code>par</code>.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>number of (outer) iterations, see <code>maxeval</code>.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>integer code indicating successful completion (&gt; 0)
or a possible error number (&lt; 0).</p>
</td></tr>
<tr><td><code>message</code></td>
<td>
<p>character string produced by NLopt and giving additional
information.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The initial population size for CRS defaults to <code>20x(n+1)</code> in
<code>n</code> dimensions, but this can be changed; the initial population must be
at least <code>n+1</code>.
</p>


<h3>Author(s)</h3>

<p>Hans W. Borchers
</p>


<h3>References</h3>

<p>Thomas Philip Runarsson and Xin Yao, &ldquo;Search biases in
constrained evolutionary optimization,&rdquo; IEEE Trans. on Systems, Man, and
Cybernetics Part C: Applications and Reviews, vol. 35 (no. 2), pp. 233-243
(2005).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### Rosenbrock Banana objective function
fn &lt;- function(x)
    return( 100 * (x[2] - x[1] * x[1])^2 + (1 - x[1])^2 )

x0 &lt;- c( -1.2, 1 )
lb &lt;- c( -3, -3 )
ub &lt;- c(  3,  3 )

isres(x0 = x0, fn = fn, lower = lb, upper = ub)

</code></pre>

<hr>
<h2 id='lbfgs'>Low-storage BFGS</h2><span id='topic+lbfgs'></span>

<h3>Description</h3>

<p>Low-storage version of the Broyden-Fletcher-Goldfarb-Shanno (BFGS) method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lbfgs(
  x0,
  fn,
  gr = NULL,
  lower = NULL,
  upper = NULL,
  nl.info = FALSE,
  control = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lbfgs_+3A_x0">x0</code></td>
<td>
<p>initial point for searching the optimum.</p>
</td></tr>
<tr><td><code id="lbfgs_+3A_fn">fn</code></td>
<td>
<p>objective function to be minimized.</p>
</td></tr>
<tr><td><code id="lbfgs_+3A_gr">gr</code></td>
<td>
<p>gradient of function <code>fn</code>; will be calculated numerically if
not specified.</p>
</td></tr>
<tr><td><code id="lbfgs_+3A_lower">lower</code>, <code id="lbfgs_+3A_upper">upper</code></td>
<td>
<p>lower and upper bound constraints.</p>
</td></tr>
<tr><td><code id="lbfgs_+3A_nl.info">nl.info</code></td>
<td>
<p>logical; shall the original NLopt info been shown.</p>
</td></tr>
<tr><td><code id="lbfgs_+3A_control">control</code></td>
<td>
<p>list of control parameters, see <code>nl.opts</code> for help.</p>
</td></tr>
<tr><td><code id="lbfgs_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to the function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The low-storage (or limited-memory) algorithm is a member of the class of
quasi-Newton optimization methods. It is well suited for optimization
problems with a large number of variables.
</p>
<p>One parameter of this algorithm is the number <code>m</code> of gradients to
remember from previous optimization steps. NLopt sets <code>m</code> to a
heuristic value by default. It can be changed by the NLopt function
<code>set_vector_storage</code>.
</p>


<h3>Value</h3>

<p>List with components:
</p>
<table>
<tr><td><code>par</code></td>
<td>
<p>the optimal solution found so far.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>the function value corresponding to <code>par</code>.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>number of (outer) iterations, see <code>maxeval</code>.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>integer code indicating successful completion (&gt; 0)
or a possible error number (&lt; 0).</p>
</td></tr>
<tr><td><code>message</code></td>
<td>
<p>character string produced by NLopt and giving additional
information.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Based on a Fortran implementation of the low-storage BFGS algorithm
written by L. Luksan, and posted under the GNU LGPL license.
</p>


<h3>Author(s)</h3>

<p>Hans W. Borchers
</p>


<h3>References</h3>

<p>J. Nocedal, &ldquo;Updating quasi-Newton matrices with limited
storage,&rdquo; Math. Comput. 35, 773-782 (1980).
</p>
<p>D. C. Liu and J. Nocedal, &ldquo;On the limited memory BFGS method for large
scale optimization,&rdquo; Math. Programming 45, p. 503-528 (1989).
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+optim">optim</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
flb &lt;- function(x) {
    p &lt;- length(x)
    sum(c(1, rep(4, p-1)) * (x - c(1, x[-p])^2)^2)
}
# 25-dimensional box constrained: par[24] is *not* at the boundary
S &lt;- lbfgs(rep(3, 25), flb, lower=rep(2, 25), upper=rep(4, 25),
           nl.info = TRUE, control = list(xtol_rel=1e-8))
## Optimal value of objective function:  368.105912874334
## Optimal value of controls: 2  ...  2  2.109093  4

</code></pre>

<hr>
<h2 id='mlsl'>Multi-level Single-linkage</h2><span id='topic+mlsl'></span>

<h3>Description</h3>

<p>The &ldquo;Multi-Level Single-Linkage&rdquo; (MLSL) algorithm for global optimization
searches by a sequence of local optimizations from random starting points.
A modification of MLSL is included using a low-discrepancy sequence (LDS)
instead of pseudorandom numbers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mlsl(
  x0,
  fn,
  gr = NULL,
  lower,
  upper,
  local.method = "LBFGS",
  low.discrepancy = TRUE,
  nl.info = FALSE,
  control = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mlsl_+3A_x0">x0</code></td>
<td>
<p>initial point for searching the optimum.</p>
</td></tr>
<tr><td><code id="mlsl_+3A_fn">fn</code></td>
<td>
<p>objective function that is to be minimized.</p>
</td></tr>
<tr><td><code id="mlsl_+3A_gr">gr</code></td>
<td>
<p>gradient of function <code>fn</code>; will be calculated numerically if
not specified.</p>
</td></tr>
<tr><td><code id="mlsl_+3A_lower">lower</code>, <code id="mlsl_+3A_upper">upper</code></td>
<td>
<p>lower and upper bound constraints.</p>
</td></tr>
<tr><td><code id="mlsl_+3A_local.method">local.method</code></td>
<td>
<p>only <code>BFGS</code> for the moment.</p>
</td></tr>
<tr><td><code id="mlsl_+3A_low.discrepancy">low.discrepancy</code></td>
<td>
<p>logical; shall a low discrepancy variation be used.</p>
</td></tr>
<tr><td><code id="mlsl_+3A_nl.info">nl.info</code></td>
<td>
<p>logical; shall the original NLopt info been shown.</p>
</td></tr>
<tr><td><code id="mlsl_+3A_control">control</code></td>
<td>
<p>list of options, see <code>nl.opts</code> for help.</p>
</td></tr>
<tr><td><code id="mlsl_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>MLSL is a <code style="white-space: pre;">&#8288;multistart' algorithm: it works by doing a sequence of local optimizations (using some other local optimization algorithm) from random or low-discrepancy starting points.  MLSL is distinguished, however by a &#8288;</code>clustering' heuristic that helps it to avoid repeated searches of the same
local optima, and has some theoretical guarantees of finding all local
optima in a finite number of local minimizations.
</p>
<p>The local-search portion of MLSL can use any of the other algorithms in
NLopt, and in particular can use either gradient-based or derivative-free
algorithms.  For this wrapper only gradient-based <code>L-BFGS</code> is available
as local method.
</p>


<h3>Value</h3>

<p>List with components:
</p>
<table>
<tr><td><code>par</code></td>
<td>
<p>the optimal solution found so far.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>the function value corresponding to <code>par</code>.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>number of (outer) iterations, see <code>maxeval</code>.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>integer code indicating successful completion (&gt; 0)
or a possible error number (&lt; 0).</p>
</td></tr>
<tr><td><code>message</code></td>
<td>
<p>character string produced by NLopt and giving additional
information.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>If you don't set a stopping tolerance for your local-optimization
algorithm, MLSL defaults to <code>ftol_rel=1e-15</code> and <code>xtol_rel=1e-7</code>
for the local searches.
</p>


<h3>Author(s)</h3>

<p>Hans W. Borchers
</p>


<h3>References</h3>

<p>A. H. G. Rinnooy Kan and G. T. Timmer, &ldquo;Stochastic global
optimization methods&rdquo; Mathematical Programming, vol. 39, p. 27-78 (1987).
</p>
<p>Sergei Kucherenko and Yury Sytsko, &ldquo;Application of deterministic
low-discrepancy sequences in global optimization,&rdquo; Computational
Optimization and Applications, vol. 30, p. 297-318 (2005).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+direct">direct</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### Minimize the Hartmann6 function
hartmann6 &lt;- function(x) {
    n &lt;- length(x)
    a &lt;- c(1.0, 1.2, 3.0, 3.2)
    A &lt;- matrix(c(10.0,  0.05, 3.0, 17.0,
                   3.0, 10.0,  3.5,  8.0,
                  17.0, 17.0,  1.7,  0.05,
                   3.5,  0.1, 10.0, 10.0,
                   1.7,  8.0, 17.0,  0.1,
                   8.0, 14.0,  8.0, 14.0), nrow=4, ncol=6)
    B  &lt;- matrix(c(.1312,.2329,.2348,.4047,
                   .1696,.4135,.1451,.8828,
                   .5569,.8307,.3522,.8732,
                   .0124,.3736,.2883,.5743,
                   .8283,.1004,.3047,.1091,
                   .5886,.9991,.6650,.0381), nrow=4, ncol=6)
    fun &lt;- 0.0
    for (i in 1:4) {
        fun &lt;- fun - a[i] * exp(-sum(A[i,]*(x-B[i,])^2))
    }
    return(fun)
}
S &lt;- mlsl(x0 = rep(0, 6), hartmann6, lower = rep(0,6), upper = rep(1,6),
            nl.info = TRUE, control=list(xtol_rel=1e-8, maxeval=1000))
## Number of Iterations....: 1000
## Termination conditions:
##   stopval: -Inf, xtol_rel: 1e-08, maxeval: 1000, ftol_rel: 0, ftol_abs: 0
## Number of inequality constraints:  0
## Number of equality constraints:    0
## Current value of objective function:  -3.32236801141552
## Current value of controls:
##   0.2016895 0.1500107 0.476874 0.2753324 0.3116516 0.6573005

</code></pre>

<hr>
<h2 id='mma'>Method of Moving Asymptotes</h2><span id='topic+mma'></span>

<h3>Description</h3>

<p>Globally-convergent method-of-moving-asymptotes (MMA) algorithm for
gradient-based local optimization, including nonlinear inequality
constraints (but not equality constraints).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mma(
  x0,
  fn,
  gr = NULL,
  lower = NULL,
  upper = NULL,
  hin = NULL,
  hinjac = NULL,
  nl.info = FALSE,
  control = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mma_+3A_x0">x0</code></td>
<td>
<p>starting point for searching the optimum.</p>
</td></tr>
<tr><td><code id="mma_+3A_fn">fn</code></td>
<td>
<p>objective function that is to be minimized.</p>
</td></tr>
<tr><td><code id="mma_+3A_gr">gr</code></td>
<td>
<p>gradient of function <code>fn</code>; will be calculated numerically if
not specified.</p>
</td></tr>
<tr><td><code id="mma_+3A_lower">lower</code>, <code id="mma_+3A_upper">upper</code></td>
<td>
<p>lower and upper bound constraints.</p>
</td></tr>
<tr><td><code id="mma_+3A_hin">hin</code></td>
<td>
<p>function defining the inequality constraints, that is
<code>hin&gt;=0</code> for all components.</p>
</td></tr>
<tr><td><code id="mma_+3A_hinjac">hinjac</code></td>
<td>
<p>Jacobian of function <code>hin</code>; will be calculated
numerically if not specified.</p>
</td></tr>
<tr><td><code id="mma_+3A_nl.info">nl.info</code></td>
<td>
<p>logical; shall the original NLopt info been shown.</p>
</td></tr>
<tr><td><code id="mma_+3A_control">control</code></td>
<td>
<p>list of options, see <code>nl.opts</code> for help.</p>
</td></tr>
<tr><td><code id="mma_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is an improved CCSA (&quot;conservative convex separable approximation&quot;)
variant of the original MMA algorithm published by Svanberg in 1987, which
has become popular for topology optimization. Note:
</p>


<h3>Value</h3>

<p>List with components:
</p>
<table>
<tr><td><code>par</code></td>
<td>
<p>the optimal solution found so far.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>the function value corresponding to <code>par</code>.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>number of (outer) iterations, see <code>maxeval</code>.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>integer code indicating successful completion (&gt; 1)
or a possible error number (&lt; 0).</p>
</td></tr>
<tr><td><code>message</code></td>
<td>
<p>character string produced by NLopt and giving additional
information.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>&ldquo;Globally convergent&rdquo; does not mean that this algorithm converges to
the global optimum; it means that it is guaranteed to converge to some local
minimum from any feasible starting point.
</p>


<h3>Author(s)</h3>

<p>Hans W. Borchers
</p>


<h3>References</h3>

<p>Krister Svanberg, &ldquo;A class of globally convergent optimization
methods based on conservative convex separable approximations,&rdquo; SIAM J.
Optim. 12 (2), p. 555-573 (2002).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+slsqp">slsqp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##  Solve the Hock-Schittkowski problem no. 100 with analytic gradients
x0.hs100 &lt;- c(1, 2, 0, 4, 0, 1, 1)
fn.hs100 &lt;- function(x) {
    (x[1]-10)^2 + 5*(x[2]-12)^2 + x[3]^4 + 3*(x[4]-11)^2 + 10*x[5]^6 +
                  7*x[6]^2 + x[7]^4 - 4*x[6]*x[7] - 10*x[6] - 8*x[7]
}
hin.hs100 &lt;- function(x) {
    h &lt;- numeric(4)
    h[1] &lt;- 127 - 2*x[1]^2 - 3*x[2]^4 - x[3] - 4*x[4]^2 - 5*x[5]
    h[2] &lt;- 282 - 7*x[1] - 3*x[2] - 10*x[3]^2 - x[4] + x[5]
    h[3] &lt;- 196 - 23*x[1] - x[2]^2 - 6*x[6]^2 + 8*x[7]
    h[4] &lt;- -4*x[1]^2 - x[2]^2 + 3*x[1]*x[2] -2*x[3]^2 - 5*x[6]	+11*x[7]
    return(h)
}
gr.hs100 &lt;- function(x) {
   c(  2 * x[1] -  20,
      10 * x[2] - 120,
       4 * x[3]^3,
       6 * x[4] - 66,
      60 * x[5]^5,
      14 * x[6]   - 4 * x[7] - 10,
       4 * x[7]^3 - 4 * x[6] -  8 )}
hinjac.hs100 &lt;- function(x) {
    matrix(c(4*x[1], 12*x[2]^3, 1, 8*x[4], 5, 0, 0,
        7, 3, 20*x[3], 1, -1, 0, 0,
        23, 2*x[2], 0, 0, 0, 12*x[6], -8,
        8*x[1]-3*x[2], 2*x[2]-3*x[1], 4*x[3], 0, 0, 5, -11), 4, 7, byrow=TRUE)
}

# incorrect result with exact jacobian
S &lt;- mma(x0.hs100, fn.hs100, gr = gr.hs100,
            hin = hin.hs100, hinjac = hinjac.hs100,
            nl.info = TRUE, control = list(xtol_rel = 1e-8))


# This example is put in donttest because it runs for more than
# 40 seconds under 32-bit Windows. The difference in time needed
# to execute the code between 32-bit Windows and 64-bit Windows
# can probably be explained by differences in rounding/truncation
# on the different systems. On Windows 32-bit more iterations
# are needed resulting in a longer runtime.
# correct result with inexact jacobian
S &lt;- mma(x0.hs100, fn.hs100, hin = hin.hs100,
            nl.info = TRUE, control = list(xtol_rel = 1e-8))


</code></pre>

<hr>
<h2 id='neldermead'>Nelder-Mead Simplex</h2><span id='topic+neldermead'></span>

<h3>Description</h3>

<p>An implementation of almost the original Nelder-Mead simplex algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>neldermead(
  x0,
  fn,
  lower = NULL,
  upper = NULL,
  nl.info = FALSE,
  control = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="neldermead_+3A_x0">x0</code></td>
<td>
<p>starting point for searching the optimum.</p>
</td></tr>
<tr><td><code id="neldermead_+3A_fn">fn</code></td>
<td>
<p>objective function that is to be minimized.</p>
</td></tr>
<tr><td><code id="neldermead_+3A_lower">lower</code>, <code id="neldermead_+3A_upper">upper</code></td>
<td>
<p>lower and upper bound constraints.</p>
</td></tr>
<tr><td><code id="neldermead_+3A_nl.info">nl.info</code></td>
<td>
<p>logical; shall the original NLopt info been shown.</p>
</td></tr>
<tr><td><code id="neldermead_+3A_control">control</code></td>
<td>
<p>list of options, see <code>nl.opts</code> for help.</p>
</td></tr>
<tr><td><code id="neldermead_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Provides explicit support for bound constraints, using essentially the method
proposed in Box.  Whenever a new point would lie outside the bound
constraints the point is moved back exactly onto the constraint.
</p>


<h3>Value</h3>

<p>List with components:
</p>
<table>
<tr><td><code>par</code></td>
<td>
<p>the optimal solution found so far.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>the function value corresponding to <code>par</code>.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>number of (outer) iterations, see <code>maxeval</code>.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>integer code indicating successful completion (&gt; 0)
or a possible error number (&lt; 0).</p>
</td></tr>
<tr><td><code>message</code></td>
<td>
<p>character string produced by NLopt and giving additional
information.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The author of NLopt would tend to recommend the Subplex method
instead.
</p>


<h3>Author(s)</h3>

<p>Hans W. Borchers
</p>


<h3>References</h3>

<p>J. A. Nelder and R. Mead, &ldquo;A simplex method for function
minimization,&rdquo; The Computer Journal 7, p. 308-313 (1965).
</p>
<p>M. J. Box, &ldquo;A new method of constrained optimization and a comparison with
other methods,&rdquo; Computer J. 8 (1), 42-52 (1965).
</p>


<h3>See Also</h3>

<p><code>dfoptim::nmk</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Fletcher and Powell's helic valley
fphv &lt;- function(x)
    100*(x[3] - 10*atan2(x[2], x[1])/(2*pi))^2 +
        (sqrt(x[1]^2 + x[2]^2) - 1)^2 +x[3]^2
x0 &lt;- c(-1, 0, 0)
neldermead(x0, fphv)    #  1 0 0

# Powell's Singular Function (PSF)
psf &lt;- function(x)  (x[1] + 10*x[2])^2 + 5*(x[3] - x[4])^2 +
                    (x[2] - 2*x[3])^4 + 10*(x[1] - x[4])^4
x0 &lt;- c(3, -1, 0, 1)
neldermead(x0, psf)     #  0 0 0 0, needs maximum number of function calls

## Not run: 
# Bounded version of Nelder-Mead
rosenbrock &lt;- function(x) { ## Rosenbrock Banana function
    100 * (x[2] - x[1]^2)^2 + (1 - x[1])^2 + 
    100 * (x[3] - x[2]^2)^2 + (1 - x[2])^2
}
lower &lt;- c(-Inf, 0,   0)
upper &lt;- c( Inf, 0.5, 1)
x0 &lt;- c(0, 0.1, 0.1)
S &lt;- neldermead(c(0, 0.1, 0.1), rosenbrock, lower, upper, nl.info = TRUE)
# $xmin = c(0.7085595, 0.5000000, 0.2500000)
# $fmin = 0.3353605
## End(Not run)

</code></pre>

<hr>
<h2 id='newuoa'>New Unconstrained Optimization with quadratic Approximation</h2><span id='topic+newuoa'></span>

<h3>Description</h3>

<p>NEWUOA solves quadratic subproblems in a spherical trust regionvia a
truncated conjugate-gradient algorithm. For bound-constrained problems,
BOBYQA shold be used instead, as Powell developed it as an enhancement
thereof for bound constraints.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>newuoa(x0, fn, nl.info = FALSE, control = list(), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="newuoa_+3A_x0">x0</code></td>
<td>
<p>starting point for searching the optimum.</p>
</td></tr>
<tr><td><code id="newuoa_+3A_fn">fn</code></td>
<td>
<p>objective function that is to be minimized.</p>
</td></tr>
<tr><td><code id="newuoa_+3A_nl.info">nl.info</code></td>
<td>
<p>logical; shall the original NLopt info been shown.</p>
</td></tr>
<tr><td><code id="newuoa_+3A_control">control</code></td>
<td>
<p>list of options, see <code>nl.opts</code> for help.</p>
</td></tr>
<tr><td><code id="newuoa_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is an algorithm derived from the NEWUOA Fortran subroutine of Powell,
converted to C and modified for the NLOPT stopping criteria.
</p>


<h3>Value</h3>

<p>List with components:
</p>
<table>
<tr><td><code>par</code></td>
<td>
<p>the optimal solution found so far.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>the function value corresponding to <code>par</code>.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>number of (outer) iterations, see <code>maxeval</code>.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>integer code indicating successful completion (&gt; 0)
or a possible error number (&lt; 0).</p>
</td></tr>
<tr><td><code>message</code></td>
<td>
<p>character string produced by NLopt and giving additional
information.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>NEWUOA may be largely superseded by BOBYQA.
</p>


<h3>Author(s)</h3>

<p>Hans W. Borchers
</p>


<h3>References</h3>

<p>M. J. D. Powell. &ldquo;The BOBYQA algorithm for bound constrained
optimization without derivatives,&rdquo; Department of Applied Mathematics and
Theoretical Physics, Cambridge England, technical reportNA2009/06 (2009).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bobyqa">bobyqa</a></code>, <code><a href="#topic+cobyla">cobyla</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
fr &lt;- function(x) {   ## Rosenbrock Banana function
    100 * (x[2] - x[1]^2)^2 + (1 - x[1])^2
}
(S &lt;- newuoa(c(1, 2), fr))

</code></pre>

<hr>
<h2 id='nl.grad'>Numerical Gradients and Jacobians</h2><span id='topic+nl.grad'></span><span id='topic+nl.jacobian'></span>

<h3>Description</h3>

<p>Provides numerical gradients and jacobians.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nl.grad(x0, fn, heps = .Machine$double.eps^(1/3), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nl.grad_+3A_x0">x0</code></td>
<td>
<p>point as a vector where the gradient is to be calculated.</p>
</td></tr>
<tr><td><code id="nl.grad_+3A_fn">fn</code></td>
<td>
<p>scalar function of one or several variables.</p>
</td></tr>
<tr><td><code id="nl.grad_+3A_heps">heps</code></td>
<td>
<p>step size to be used.</p>
</td></tr>
<tr><td><code id="nl.grad_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Both functions apply the &ldquo;central difference formula&rdquo; with step size as
recommended in the literature.
</p>


<h3>Value</h3>

<p><code>grad</code> returns the gradient as a vector; <code>jacobian</code>
returns the Jacobian as a matrix of usual dimensions.
</p>


<h3>Author(s)</h3>

<p>Hans W. Borchers
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  fn1 &lt;- function(x) sum(x^2)
  nl.grad(seq(0, 1, by = 0.2), fn1)
  ## [1] 0.0  0.4  0.8  1.2  1.6  2.0
  nl.grad(rep(1, 5), fn1)
  ## [1] 2  2  2  2  2

  fn2 &lt;- function(x) c(sin(x), cos(x))
  x &lt;- (0:1)*2*pi
  nl.jacobian(x, fn2)
  ##      [,1] [,2]
  ## [1,]    1    0
  ## [2,]    0    1
  ## [3,]    0    0
  ## [4,]    0    0

</code></pre>

<hr>
<h2 id='nl.opts'>Setting NL Options</h2><span id='topic+nl.opts'></span>

<h3>Description</h3>

<p>Sets and changes the NLOPT options.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nl.opts(optlist = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nl.opts_+3A_optlist">optlist</code></td>
<td>
<p>list of options, see below.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The following options can be set (here with default values):
</p>
<p><code>stopval = -Inf, # stop minimization at this value</code><br /> <code>xtol_rel =
1e-6, # stop on small optimization step</code><br /> <code>maxeval = 1000, # stop on
this many function evaluations</code><br /> <code>ftol_rel = 0.0, # stop on change
times function value</code><br /> <code>ftol_abs = 0.0, # stop on small change of
function value</code><br /> <code>check_derivatives = FALSE</code>
</p>


<h3>Value</h3>

<p>returns a list with default and changed options.
</p>


<h3>Note</h3>

<p>There are more options that can be set for solvers in NLOPT. These
cannot be set through their wrapper functions. To see the full list of
options and algorithms, type <code>nloptr.print.options()</code>.
</p>


<h3>Author(s)</h3>

<p>Hans W. Borchers
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
nl.opts(list(xtol_rel = 1e-8, maxeval = 2000))

</code></pre>

<hr>
<h2 id='nloptr'>R interface to NLopt</h2><span id='topic+nloptr'></span>

<h3>Description</h3>

<p>nloptr is an R interface to NLopt, a free/open-source library for nonlinear
optimization started by Steven G. Johnson, providing a common interface for
a number of different free optimization routines available online as well as
original implementations of various other algorithms. The NLopt library is
available under the GNU Lesser General Public License (LGPL), and the
copyrights are owned by a variety of authors. Most of the information here
has been taken from <a href="https://nlopt.readthedocs.io/en/latest/">the NLopt website</a>,
where more details are available.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nloptr(
  x0,
  eval_f,
  eval_grad_f = NULL,
  lb = NULL,
  ub = NULL,
  eval_g_ineq = NULL,
  eval_jac_g_ineq = NULL,
  eval_g_eq = NULL,
  eval_jac_g_eq = NULL,
  opts = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nloptr_+3A_x0">x0</code></td>
<td>
<p>vector with starting values for the optimization.</p>
</td></tr>
<tr><td><code id="nloptr_+3A_eval_f">eval_f</code></td>
<td>
<p>function that returns the value of the objective function. It
can also return gradient information at the same time in a list with
elements &quot;objective&quot; and &quot;gradient&quot; (see below for an example).</p>
</td></tr>
<tr><td><code id="nloptr_+3A_eval_grad_f">eval_grad_f</code></td>
<td>
<p>function that returns the value of the gradient of the
objective function. Not all of the algorithms require a gradient.</p>
</td></tr>
<tr><td><code id="nloptr_+3A_lb">lb</code></td>
<td>
<p>vector with lower bounds of the controls (use -Inf for controls
without lower bound), by default there are no lower bounds for any of the
controls.</p>
</td></tr>
<tr><td><code id="nloptr_+3A_ub">ub</code></td>
<td>
<p>vector with upper bounds of the controls (use Inf for controls
without upper bound), by default there are no upper bounds for any of the
controls.</p>
</td></tr>
<tr><td><code id="nloptr_+3A_eval_g_ineq">eval_g_ineq</code></td>
<td>
<p>function to evaluate (non-)linear inequality constraints
that should hold in the solution.  It can also return gradient information
at the same time in a list with elements &quot;constraints&quot; and &quot;jacobian&quot; (see
below for an example).</p>
</td></tr>
<tr><td><code id="nloptr_+3A_eval_jac_g_ineq">eval_jac_g_ineq</code></td>
<td>
<p>function to evaluate the jacobian of the (non-)linear
inequality constraints that should hold in the solution.</p>
</td></tr>
<tr><td><code id="nloptr_+3A_eval_g_eq">eval_g_eq</code></td>
<td>
<p>function to evaluate (non-)linear equality constraints that
should hold in the solution.  It can also return gradient information at the
same time in a list with elements &quot;constraints&quot; and &quot;jacobian&quot; (see below for
an example).</p>
</td></tr>
<tr><td><code id="nloptr_+3A_eval_jac_g_eq">eval_jac_g_eq</code></td>
<td>
<p>function to evaluate the jacobian of the (non-)linear
equality constraints that should hold in the solution.</p>
</td></tr>
<tr><td><code id="nloptr_+3A_opts">opts</code></td>
<td>
<p>list with options. The option &quot;algorithm&quot; is required. Check the
<a href="https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/">NLopt website</a>
for a full list of available algorithms. Other options control the
termination conditions (minf_max, ftol_rel, ftol_abs, xtol_rel, xtol_abs,
maxeval, maxtime). Default is xtol_rel = 1e-4. More information
<a href="https://nlopt.readthedocs.io/en/latest/NLopt_Introduction/#termination-conditions">here</a>.
A full description of all options is shown by the function
<code>nloptr.print.options()</code>.
</p>
<p>Some algorithms with equality constraints require the option local_opts,
which contains a list with an algorithm and a termination condition for the
local algorithm. See ?<code>nloptr-package</code> for an example.
</p>
<p>The option print_level controls how much output is shown during the
optimization process. Possible values: </p>

<table>
<tr>
 <td style="text-align: left;"> 0 (default) </td><td style="text-align: left;"> no
output </td>
</tr>
<tr>
 <td style="text-align: left;"> 1 </td><td style="text-align: left;"> show iteration number and value of objective function </td>
</tr>
<tr>
 <td style="text-align: left;">
2 </td><td style="text-align: left;"> 1 + show value of (in)equalities </td>
</tr>
<tr>
 <td style="text-align: left;"> 3 </td><td style="text-align: left;"> 2 + show value of
controls </td>
</tr>

</table>

<p>The option check_derivatives (default = FALSE) can be used to run to compare
the analytic gradients with finite difference approximations.  The option
check_derivatives_print ('all' (default), 'errors', 'none') controls the
output of the derivative checker, if it is run, showing all comparisons,
only those that resulted in an error, or none.  The option
check_derivatives_tol (default = 1e-04), determines when a difference
between an analytic gradient and its finite difference approximation is
flagged as an error.</p>
</td></tr>
<tr><td><code id="nloptr_+3A_...">...</code></td>
<td>
<p>arguments that will be passed to the user-defined objective and
constraints functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>NLopt addresses general nonlinear optimization problems of the form:
</p>
<p>min f(x) x in R^n
</p>
<p>s.t.  g(x) &lt;= 0 h(x) = 0 lb &lt;= x &lt;= ub
</p>
<p>where f is the objective function to be minimized and x represents the n
optimization parameters. This problem may optionally be subject to the bound
constraints (also called box constraints), lb and ub. For partially or
totally unconstrained problems the bounds can take -Inf or Inf. One may also
optionally have m nonlinear inequality constraints (sometimes called a
nonlinear programming problem), which can be specified in g(x), and equality
constraints that can be specified in h(x). Note that not all of the
algorithms in NLopt can handle constraints.
</p>


<h3>Value</h3>

<p>The return value contains a list with the inputs, and additional
elements
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>the call that was made to solve</p>
</td></tr>
<tr><td><code>status</code></td>
<td>
<p>integer value with the status of the optimization (0 is
success)</p>
</td></tr>
<tr><td><code>message</code></td>
<td>
<p>more informative message with the status of the optimization</p>
</td></tr>
<tr><td><code>iterations</code></td>
<td>
<p>number of iterations that were executed</p>
</td></tr>
<tr><td><code>objective</code></td>
<td>
<p>value if the objective function in the solution</p>
</td></tr>
<tr><td><code>solution</code></td>
<td>
<p>optimal value of the controls</p>
</td></tr>
<tr><td><code>version</code></td>
<td>
<p>version of NLopt that was used</p>
</td></tr>
</table>


<h3>Note</h3>

<p>See ?<code>nloptr-package</code> for an extended example.
</p>


<h3>Author(s)</h3>

<p>Steven G. Johnson and others (C code) <br /> Jelmer Ypma (R interface)
</p>


<h3>References</h3>

<p>Steven G. Johnson, The NLopt nonlinear-optimization package,
<a href="https://nlopt.readthedocs.io/en/latest/">https://nlopt.readthedocs.io/en/latest/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nloptr.print.options">nloptr.print.options</a></code>
<code><a href="#topic+check.derivatives">check.derivatives</a></code>
<code><a href="stats.html#topic+optim">optim</a></code>
<code><a href="stats.html#topic+nlm">nlm</a></code>
<code><a href="stats.html#topic+nlminb">nlminb</a></code>
<code>Rsolnp::Rsolnp</code>
<code>Rsolnp::solnp</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library('nloptr')

## Rosenbrock Banana function and gradient in separate functions
eval_f &lt;- function(x) {
    return( 100 * (x[2] - x[1] * x[1])^2 + (1 - x[1])^2 )
}

eval_grad_f &lt;- function(x) {
    return( c( -400 * x[1] * (x[2] - x[1] * x[1]) - 2 * (1 - x[1]),
                200 * (x[2] - x[1] * x[1])) )
}


# initial values
x0 &lt;- c( -1.2, 1 )

opts &lt;- list("algorithm"="NLOPT_LD_LBFGS",
             "xtol_rel"=1.0e-8)

# solve Rosenbrock Banana function
res &lt;- nloptr( x0=x0,
               eval_f=eval_f,
               eval_grad_f=eval_grad_f,
               opts=opts)
print( res )


## Rosenbrock Banana function and gradient in one function
# this can be used to economize on calculations
eval_f_list &lt;- function(x) {
    return( list( "objective" = 100 * (x[2] - x[1] * x[1])^2 + (1 - x[1])^2,
                  "gradient"  = c( -400 * x[1] * (x[2] - x[1] * x[1]) - 2 * (1 - x[1]),
                                    200 * (x[2] - x[1] * x[1])) ) )
}

# solve Rosenbrock Banana function using an objective function that
# returns a list with the objective value and its gradient
res &lt;- nloptr( x0=x0,
               eval_f=eval_f_list,
               opts=opts)
print( res )



# Example showing how to solve the problem from the NLopt tutorial.
#
# min sqrt( x2 )
# s.t. x2 &gt;= 0
#      x2 &gt;= ( a1*x1 + b1 )^3
#      x2 &gt;= ( a2*x1 + b2 )^3
# where
# a1 = 2, b1 = 0, a2 = -1, b2 = 1
#
# re-formulate constraints to be of form g(x) &lt;= 0
#      ( a1*x1 + b1 )^3 - x2 &lt;= 0
#      ( a2*x1 + b2 )^3 - x2 &lt;= 0

library('nloptr')


# objective function
eval_f0 &lt;- function( x, a, b ){
    return( sqrt(x[2]) )
}

# constraint function
eval_g0 &lt;- function( x, a, b ) {
    return( (a*x[1] + b)^3 - x[2] )
}

# gradient of objective function
eval_grad_f0 &lt;- function( x, a, b ){
    return( c( 0, .5/sqrt(x[2]) ) )
}

# jacobian of constraint
eval_jac_g0 &lt;- function( x, a, b ) {
    return( rbind( c( 3*a[1]*(a[1]*x[1] + b[1])^2, -1.0 ),
                   c( 3*a[2]*(a[2]*x[1] + b[2])^2, -1.0 ) ) )
}


# functions with gradients in objective and constraint function
# this can be useful if the same calculations are needed for
# the function value and the gradient
eval_f1 &lt;- function( x, a, b ){
    return( list("objective"=sqrt(x[2]),
                 "gradient"=c(0,.5/sqrt(x[2])) ) )
}

eval_g1 &lt;- function( x, a, b ) {
    return( list( "constraints"=(a*x[1] + b)^3 - x[2],
                  "jacobian"=rbind( c( 3*a[1]*(a[1]*x[1] + b[1])^2, -1.0 ),
                                    c( 3*a[2]*(a[2]*x[1] + b[2])^2, -1.0 ) ) ) )
}


# define parameters
a &lt;- c(2,-1)
b &lt;- c(0, 1)

# Solve using NLOPT_LD_MMA with gradient information supplied in separate function
res0 &lt;- nloptr( x0=c(1.234,5.678),
                eval_f=eval_f0,
                eval_grad_f=eval_grad_f0,
                lb = c(-Inf,0),
                ub = c(Inf,Inf),
                eval_g_ineq = eval_g0,
                eval_jac_g_ineq = eval_jac_g0,
                opts = list("algorithm"="NLOPT_LD_MMA"),
                a = a,
                b = b )
print( res0 )

# Solve using NLOPT_LN_COBYLA without gradient information
res1 &lt;- nloptr( x0=c(1.234,5.678),
                eval_f=eval_f0,
                lb = c(-Inf,0),
                ub = c(Inf,Inf),
                eval_g_ineq = eval_g0,
                opts = list("algorithm"="NLOPT_LN_COBYLA"),
                a = a,
                b = b )
print( res1 )


# Solve using NLOPT_LD_MMA with gradient information in objective function
res2 &lt;- nloptr( x0=c(1.234,5.678),
                eval_f=eval_f1,
                lb = c(-Inf,0),
                ub = c(Inf,Inf),
                eval_g_ineq = eval_g1,
                opts = list("algorithm"="NLOPT_LD_MMA", "check_derivatives"=TRUE),
                a = a,
                b = b )
print( res2 )

</code></pre>

<hr>
<h2 id='nloptr.get.default.options'>Return a data.frame with all the options that can be supplied to nloptr.</h2><span id='topic+nloptr.get.default.options'></span>

<h3>Description</h3>

<p>This function returns a data.frame with all the options that can be supplied
to <code><a href="#topic+nloptr">nloptr</a></code>. The data.frame contains the default
values of the options and an explanation. A user-friendly way to show these
options is by using the function
<code><a href="#topic+nloptr.print.options">nloptr.print.options</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nloptr.get.default.options()
</code></pre>


<h3>Value</h3>

<p>The return value contains a <code>data.frame</code> with the following
elements
</p>
<table>
<tr><td><code>name</code></td>
<td>
<p>name of the option</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>type (numeric, logical, integer, character)</p>
</td></tr>
<tr><td><code>possible_values</code></td>
<td>
<p>string explaining the values the option can take</p>
</td></tr>
<tr><td><code>default</code></td>
<td>
<p>default value of the option (as a string)</p>
</td></tr>
<tr><td><code>is_termination_condition</code></td>
<td>
<p>is this option part of the termination
conditions?</p>
</td></tr>
<tr><td><code>description</code></td>
<td>
<p>description of the option (taken from NLopt website
if it's an option that is passed on to NLopt).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jelmer Ypma
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nloptr">nloptr</a></code>
<code><a href="#topic+nloptr.print.options">nloptr.print.options</a></code>
</p>

<hr>
<h2 id='nloptr.print.options'>Print description of nloptr options</h2><span id='topic+nloptr.print.options'></span>

<h3>Description</h3>

<p>This function prints a list of all the options that can be set when solving
a minimization problem using <code>nloptr</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nloptr.print.options(opts.show = NULL, opts.user = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nloptr.print.options_+3A_opts.show">opts.show</code></td>
<td>
<p>list or vector with names of options. A description will be
shown for the options in this list. By default, a description of all options
is shown.</p>
</td></tr>
<tr><td><code id="nloptr.print.options_+3A_opts.user">opts.user</code></td>
<td>
<p>object containing user supplied options. This argument is
optional. It is used when <code>nloptr.print.options</code> is called from
<code>nloptr</code>. In that case options are listed if <code>print_options_doc</code>
is set to <code>TRUE</code> when passing a minimization problem to <code>nloptr</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jelmer Ypma
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nloptr">nloptr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library('nloptr')
nloptr.print.options()

nloptr.print.options( opts.show = c("algorithm", "check_derivatives") )

opts &lt;- list("algorithm"="NLOPT_LD_LBFGS",
             "xtol_rel"=1.0e-8)
nloptr.print.options( opts.user = opts )

</code></pre>

<hr>
<h2 id='print.nloptr'>Print results after running nloptr</h2><span id='topic+print.nloptr'></span>

<h3>Description</h3>

<p>This function prints the nloptr object that holds the results from a
minimization using <code>nloptr</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'nloptr'
print(x, show.controls = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.nloptr_+3A_x">x</code></td>
<td>
<p>object containing result from minimization.</p>
</td></tr>
<tr><td><code id="print.nloptr_+3A_show.controls">show.controls</code></td>
<td>
<p>Logical or vector with indices. Should we show the
value of the control variables in the solution? If <code>show.controls</code> is a
vector with indices, it is used to select which control variables should be
shown. This can be useful if the model contains a set of parameters of
interest and a set of nuisance parameters that are not of immediate
interest.</p>
</td></tr>
<tr><td><code id="print.nloptr_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jelmer Ypma
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nloptr">nloptr</a></code>
</p>

<hr>
<h2 id='sbplx'>Subplex Algorithm</h2><span id='topic+sbplx'></span>

<h3>Description</h3>

<p>Subplex is a variant of Nelder-Mead that uses Nelder-Mead on a sequence of
subspaces.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sbplx(
  x0,
  fn,
  lower = NULL,
  upper = NULL,
  nl.info = FALSE,
  control = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sbplx_+3A_x0">x0</code></td>
<td>
<p>starting point for searching the optimum.</p>
</td></tr>
<tr><td><code id="sbplx_+3A_fn">fn</code></td>
<td>
<p>objective function that is to be minimized.</p>
</td></tr>
<tr><td><code id="sbplx_+3A_lower">lower</code>, <code id="sbplx_+3A_upper">upper</code></td>
<td>
<p>lower and upper bound constraints.</p>
</td></tr>
<tr><td><code id="sbplx_+3A_nl.info">nl.info</code></td>
<td>
<p>logical; shall the original NLopt info been shown.</p>
</td></tr>
<tr><td><code id="sbplx_+3A_control">control</code></td>
<td>
<p>list of options, see <code>nl.opts</code> for help.</p>
</td></tr>
<tr><td><code id="sbplx_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>SUBPLEX is claimed to be much more efficient and robust than the original
Nelder-Mead, while retaining the latter's facility with discontinuous
objectives.
</p>
<p>This implementation has explicit support for bound constraints (via the
method in the Box paper as described on the <code>neldermead</code> help page).
</p>


<h3>Value</h3>

<p>List with components:
</p>
<table>
<tr><td><code>par</code></td>
<td>
<p>the optimal solution found so far.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>the function value corresponding to <code>par</code>.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>number of (outer) iterations, see <code>maxeval</code>.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>integer code indicating successful completion (&gt; 0)
or a possible error number (&lt; 0).</p>
</td></tr>
<tr><td><code>message</code></td>
<td>
<p>character string produced by NLopt and giving additional
information.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>It is the request of Tom Rowan that reimplementations of his algorithm
shall not use the name &lsquo;subplex&rsquo;.
</p>


<h3>References</h3>

<p>T. Rowan, &ldquo;Functional Stability Analysis of Numerical
Algorithms&rdquo;, Ph.D.  thesis, Department of Computer Sciences, University of
Texas at Austin, 1990.
</p>


<h3>See Also</h3>

<p><code>subplex::subplex</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Fletcher and Powell's helic valley
fphv &lt;- function(x)
    100*(x[3] - 10*atan2(x[2], x[1])/(2*pi))^2 +
        (sqrt(x[1]^2 + x[2]^2) - 1)^2 +x[3]^2
x0 &lt;- c(-1, 0, 0)
sbplx(x0, fphv)    #  1 0 0

# Powell's Singular Function (PSF)
psf &lt;- function(x)  (x[1] + 10*x[2])^2 + 5*(x[3] - x[4])^2 +
                    (x[2] - 2*x[3])^4 + 10*(x[1] - x[4])^4
x0 &lt;- c(3, -1, 0, 1)
sbplx(x0, psf, control = list(maxeval = Inf, ftol_rel = 1e-6))  #  0 0 0 0 (?)

</code></pre>

<hr>
<h2 id='slsqp'>Sequential Quadratic Programming (SQP)</h2><span id='topic+slsqp'></span>

<h3>Description</h3>

<p>Sequential (least-squares) quadratic programming (SQP) algorithm for
nonlinearly constrained, gradient-based optimization, supporting both
equality and inequality constraints.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>slsqp(
  x0,
  fn,
  gr = NULL,
  lower = NULL,
  upper = NULL,
  hin = NULL,
  hinjac = NULL,
  heq = NULL,
  heqjac = NULL,
  nl.info = FALSE,
  control = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="slsqp_+3A_x0">x0</code></td>
<td>
<p>starting point for searching the optimum.</p>
</td></tr>
<tr><td><code id="slsqp_+3A_fn">fn</code></td>
<td>
<p>objective function that is to be minimized.</p>
</td></tr>
<tr><td><code id="slsqp_+3A_gr">gr</code></td>
<td>
<p>gradient of function <code>fn</code>; will be calculated numerically if
not specified.</p>
</td></tr>
<tr><td><code id="slsqp_+3A_lower">lower</code>, <code id="slsqp_+3A_upper">upper</code></td>
<td>
<p>lower and upper bound constraints.</p>
</td></tr>
<tr><td><code id="slsqp_+3A_hin">hin</code></td>
<td>
<p>function defining the inequality constraints, that is
<code>hin&gt;=0</code> for all components.</p>
</td></tr>
<tr><td><code id="slsqp_+3A_hinjac">hinjac</code></td>
<td>
<p>Jacobian of function <code>hin</code>; will be calculated
numerically if not specified.</p>
</td></tr>
<tr><td><code id="slsqp_+3A_heq">heq</code></td>
<td>
<p>function defining the equality constraints, that is <code>heq==0</code>
for all components.</p>
</td></tr>
<tr><td><code id="slsqp_+3A_heqjac">heqjac</code></td>
<td>
<p>Jacobian of function <code>heq</code>; will be calculated
numerically if not specified.</p>
</td></tr>
<tr><td><code id="slsqp_+3A_nl.info">nl.info</code></td>
<td>
<p>logical; shall the original NLopt info been shown.</p>
</td></tr>
<tr><td><code id="slsqp_+3A_control">control</code></td>
<td>
<p>list of options, see <code>nl.opts</code> for help.</p>
</td></tr>
<tr><td><code id="slsqp_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithm optimizes successive second-order (quadratic/least-squares)
approximations of the objective function (via BFGS updates), with
first-order (affine) approximations of the constraints.
</p>


<h3>Value</h3>

<p>List with components:
</p>
<table>
<tr><td><code>par</code></td>
<td>
<p>the optimal solution found so far.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>the function value corresponding to <code>par</code>.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>number of (outer) iterations, see <code>maxeval</code>.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>integer code indicating successful completion (&gt; 1)
or a possible error number (&lt; 0).</p>
</td></tr>
<tr><td><code>message</code></td>
<td>
<p>character string produced by NLopt and giving additional
information.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>See more infos at
<a href="https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/">https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/</a>.
</p>


<h3>Author(s)</h3>

<p>Hans W. Borchers
</p>


<h3>References</h3>

<p>Dieter Kraft, &ldquo;A software package for sequential quadratic
programming&rdquo;, Technical Report DFVLR-FB 88-28, Institut fuer Dynamik der
Flugsysteme, Oberpfaffenhofen, July 1988.
</p>


<h3>See Also</h3>

<p><code>alabama::auglag</code>, <code>Rsolnp::solnp</code>,
<code>Rdonlp2::donlp2</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##  Solve the Hock-Schittkowski problem no. 100
x0.hs100 &lt;- c(1, 2, 0, 4, 0, 1, 1)
fn.hs100 &lt;- function(x) {
    (x[1]-10)^2 + 5*(x[2]-12)^2 + x[3]^4 + 3*(x[4]-11)^2 + 10*x[5]^6 +
                  7*x[6]^2 + x[7]^4 - 4*x[6]*x[7] - 10*x[6] - 8*x[7]
}
hin.hs100 &lt;- function(x) {
    h &lt;- numeric(4)
    h[1] &lt;- 127 - 2*x[1]^2 - 3*x[2]^4 - x[3] - 4*x[4]^2 - 5*x[5]
    h[2] &lt;- 282 - 7*x[1] - 3*x[2] - 10*x[3]^2 - x[4] + x[5]
    h[3] &lt;- 196 - 23*x[1] - x[2]^2 - 6*x[6]^2 + 8*x[7]
    h[4] &lt;- -4*x[1]^2 - x[2]^2 + 3*x[1]*x[2] -2*x[3]^2 - 5*x[6]	+11*x[7]
    return(h)
}

S &lt;- slsqp(x0.hs100, fn = fn.hs100,     # no gradients and jacobians provided
           hin = hin.hs100,
           control = list(xtol_rel = 1e-8, check_derivatives = TRUE))
S
## Optimal value of objective function:  690.622270249131   *** WRONG ***

# Even the numerical derivatives seem to be too tight.
# Let's try with a less accurate jacobian.

hinjac.hs100 &lt;- function(x) nl.jacobian(x, hin.hs100, heps = 1e-2)
S &lt;- slsqp(x0.hs100, fn = fn.hs100,
           hin = hin.hs100, hinjac = hinjac.hs100,
           control = list(xtol_rel = 1e-8))
S
## Optimal value of objective function:  680.630057392593   *** CORRECT ***

</code></pre>

<hr>
<h2 id='stogo'>Stochastic Global Optimization</h2><span id='topic+stogo'></span>

<h3>Description</h3>

<p>StoGO is a global optimization algorithm that works by systematically
dividing the search space into smaller hyper-rectangles.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stogo(
  x0,
  fn,
  gr = NULL,
  lower = NULL,
  upper = NULL,
  maxeval = 10000,
  xtol_rel = 1e-06,
  randomized = FALSE,
  nl.info = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stogo_+3A_x0">x0</code></td>
<td>
<p>initial point for searching the optimum.</p>
</td></tr>
<tr><td><code id="stogo_+3A_fn">fn</code></td>
<td>
<p>objective function that is to be minimized.</p>
</td></tr>
<tr><td><code id="stogo_+3A_gr">gr</code></td>
<td>
<p>optional gradient of the objective function.</p>
</td></tr>
<tr><td><code id="stogo_+3A_lower">lower</code>, <code id="stogo_+3A_upper">upper</code></td>
<td>
<p>lower and upper bound constraints.</p>
</td></tr>
<tr><td><code id="stogo_+3A_maxeval">maxeval</code></td>
<td>
<p>maximum number of function evaluations.</p>
</td></tr>
<tr><td><code id="stogo_+3A_xtol_rel">xtol_rel</code></td>
<td>
<p>stopping criterion for relative change reached.</p>
</td></tr>
<tr><td><code id="stogo_+3A_randomized">randomized</code></td>
<td>
<p>logical; shall a randomizing variant be used?</p>
</td></tr>
<tr><td><code id="stogo_+3A_nl.info">nl.info</code></td>
<td>
<p>logical; shall the original NLopt info been shown.</p>
</td></tr>
<tr><td><code id="stogo_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>StoGO is a global optimization algorithm that works by systematically
dividing the search space (which must be bound-constrained) into smaller
hyper-rectangles via a branch-and-bound technique, and searching them by a
gradient-based local-search algorithm (a BFGS variant), optionally including
some randomness.
</p>


<h3>Value</h3>

<p>List with components:
</p>
<table>
<tr><td><code>par</code></td>
<td>
<p>the optimal solution found so far.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>the function value corresponding to <code>par</code>.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>number of (outer) iterations, see <code>maxeval</code>.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>integer code indicating successful completion (&gt; 0)
or a possible error number (&lt; 0).</p>
</td></tr>
<tr><td><code>message</code></td>
<td>
<p>character string produced by NLopt and giving additional
information.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Only bound-constrained problems are supported by this algorithm.
</p>


<h3>Author(s)</h3>

<p>Hans W. Borchers
</p>


<h3>References</h3>

<p>S. Zertchaninov and K. Madsen, &ldquo;A C++ Programme for Global
Optimization,&rdquo; IMM-REP-1998-04, Department of Mathematical Modelling,
Technical University of Denmark.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### Rosenbrock Banana objective function
fn &lt;- function(x)
    return( 100 * (x[2] - x[1] * x[1])^2 + (1 - x[1])^2 )

x0 &lt;- c( -1.2, 1 )
lb &lt;- c( -3, -3 )
ub &lt;- c(  3,  3 )

stogo(x0 = x0, fn = fn, lower = lb, upper = ub)

</code></pre>

<hr>
<h2 id='tnewton'>Preconditioned Truncated Newton</h2><span id='topic+tnewton'></span>

<h3>Description</h3>

<p>Truncated Newton methods, also calledNewton-iterative methods, solve an
approximating Newton system using a conjugate-gradient approach and are
related to limited-memory BFGS.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tnewton(
  x0,
  fn,
  gr = NULL,
  lower = NULL,
  upper = NULL,
  precond = TRUE,
  restart = TRUE,
  nl.info = FALSE,
  control = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tnewton_+3A_x0">x0</code></td>
<td>
<p>starting point for searching the optimum.</p>
</td></tr>
<tr><td><code id="tnewton_+3A_fn">fn</code></td>
<td>
<p>objective function that is to be minimized.</p>
</td></tr>
<tr><td><code id="tnewton_+3A_gr">gr</code></td>
<td>
<p>gradient of function <code>fn</code>; will be calculated numerically if
not specified.</p>
</td></tr>
<tr><td><code id="tnewton_+3A_lower">lower</code>, <code id="tnewton_+3A_upper">upper</code></td>
<td>
<p>lower and upper bound constraints.</p>
</td></tr>
<tr><td><code id="tnewton_+3A_precond">precond</code></td>
<td>
<p>logical; preset L-BFGS with steepest descent.</p>
</td></tr>
<tr><td><code id="tnewton_+3A_restart">restart</code></td>
<td>
<p>logical; restarting L-BFGS with steepest descent.</p>
</td></tr>
<tr><td><code id="tnewton_+3A_nl.info">nl.info</code></td>
<td>
<p>logical; shall the original NLopt info been shown.</p>
</td></tr>
<tr><td><code id="tnewton_+3A_control">control</code></td>
<td>
<p>list of options, see <code>nl.opts</code> for help.</p>
</td></tr>
<tr><td><code id="tnewton_+3A_...">...</code></td>
<td>
<p>additional arguments passed to the function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Truncated Newton methods are based on approximating the objective with a
quadratic function and applying an iterative scheme such as the linear
conjugate-gradient algorithm.
</p>


<h3>Value</h3>

<p>List with components:
</p>
<table>
<tr><td><code>par</code></td>
<td>
<p>the optimal solution found so far.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>the function value corresponding to <code>par</code>.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>number of (outer) iterations, see <code>maxeval</code>.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>integer code indicating successful completion (&gt; 1)
or a possible error number (&lt; 0).</p>
</td></tr>
<tr><td><code>message</code></td>
<td>
<p>character string produced by NLopt and giving additional
information.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Less reliable than Newton's method, but can handle very large
problems.
</p>


<h3>Author(s)</h3>

<p>Hans W. Borchers
</p>


<h3>References</h3>

<p>R. S. Dembo and T. Steihaug, &ldquo;Truncated Newton algorithms for
large-scale optimization,&rdquo; Math. Programming 26, p. 190-212 (1982).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lbfgs">lbfgs</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
flb &lt;- function(x) {
    p &lt;- length(x)
    sum(c(1, rep(4, p-1)) * (x - c(1, x[-p])^2)^2)
}
# 25-dimensional box constrained: par[24] is *not* at boundary
S &lt;- tnewton(rep(3, 25), flb, lower=rep(2, 25), upper=rep(4, 25),
                nl.info = TRUE, control = list(xtol_rel=1e-8))
## Optimal value of objective function:  368.105912874334
## Optimal value of controls: 2  ...  2  2.109093  4

</code></pre>

<hr>
<h2 id='varmetric'>Shifted Limited-memory Variable-metric</h2><span id='topic+varmetric'></span>

<h3>Description</h3>

<p>Shifted limited-memory variable-metric algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>varmetric(
  x0,
  fn,
  gr = NULL,
  rank2 = TRUE,
  lower = NULL,
  upper = NULL,
  nl.info = FALSE,
  control = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="varmetric_+3A_x0">x0</code></td>
<td>
<p>initial point for searching the optimum.</p>
</td></tr>
<tr><td><code id="varmetric_+3A_fn">fn</code></td>
<td>
<p>objective function to be minimized.</p>
</td></tr>
<tr><td><code id="varmetric_+3A_gr">gr</code></td>
<td>
<p>gradient of function <code>fn</code>; will be calculated numerically if
not specified.</p>
</td></tr>
<tr><td><code id="varmetric_+3A_rank2">rank2</code></td>
<td>
<p>logical; if true uses a rank-2 update method, else rank-1.</p>
</td></tr>
<tr><td><code id="varmetric_+3A_lower">lower</code>, <code id="varmetric_+3A_upper">upper</code></td>
<td>
<p>lower and upper bound constraints.</p>
</td></tr>
<tr><td><code id="varmetric_+3A_nl.info">nl.info</code></td>
<td>
<p>logical; shall the original NLopt info been shown.</p>
</td></tr>
<tr><td><code id="varmetric_+3A_control">control</code></td>
<td>
<p>list of control parameters, see <code>nl.opts</code> for help.</p>
</td></tr>
<tr><td><code id="varmetric_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to the function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Variable-metric methods are a variant of the quasi-Newton methods,
especially adapted to large-scale unconstrained (or bound constrained)
minimization.
</p>


<h3>Value</h3>

<p>List with components:
</p>
<table>
<tr><td><code>par</code></td>
<td>
<p>the optimal solution found so far.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>the function value corresponding to <code>par</code>.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>number of (outer) iterations, see <code>maxeval</code>.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>integer code indicating successful completion (&gt; 0)
or a possible error number (&lt; 0).</p>
</td></tr>
<tr><td><code>message</code></td>
<td>
<p>character string produced by NLopt and giving additional
information.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Based on L. Luksan's Fortran implementation of a shifted
limited-memory variable-metric algorithm.
</p>


<h3>Author(s)</h3>

<p>Hans W. Borchers
</p>


<h3>References</h3>

<p>J. Vlcek and L. Luksan, &ldquo;Shifted limited-memory variable metric
methods for large-scale unconstrained minimization,&rdquo; J. Computational Appl.
Math. 186, p. 365-390 (2006).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lbfgs">lbfgs</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
flb &lt;- function(x) {
    p &lt;- length(x)
    sum(c(1, rep(4, p-1)) * (x - c(1, x[-p])^2)^2)
}
# 25-dimensional box constrained: par[24] is *not* at the boundary
S &lt;- varmetric(rep(3, 25), flb, lower=rep(2, 25), upper=rep(4, 25),
           nl.info = TRUE, control = list(xtol_rel=1e-8))
## Optimal value of objective function:  368.105912874334
## Optimal value of controls: 2  ...  2  2.109093  4

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
