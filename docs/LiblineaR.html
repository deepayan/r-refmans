<!DOCTYPE html><html><head><title>Help for package LiblineaR</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {LiblineaR}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#heuristicC'><p>Fast Heuristics For The Estimation Of the C Constant Of A Support Vector Machine.</p></a></li>
<li><a href='#LiblineaR'><p>Linear predictive models estimation based on the LIBLINEAR C/C++ Library.</p></a></li>
<li><a href='#predict.LiblineaR'><p>Predictions with LiblineaR model</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Title:</td>
<td>Linear Predictive Models Based on the LIBLINEAR C/C++ Library</td>
</tr>
<tr>
<td>Version:</td>
<td>2.10-23</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-12-10</td>
</tr>
<tr>
<td>Description:</td>
<td>A wrapper around the LIBLINEAR C/C++ library for machine
        learning (available at
        <a href="https://www.csie.ntu.edu.tw/~cjlin/liblinear/">https://www.csie.ntu.edu.tw/~cjlin/liblinear/</a>). LIBLINEAR is
        a simple library for solving large-scale regularized linear
        classification and regression. It currently supports
        L2-regularized classification (such as logistic regression,
        L2-loss linear SVM and L1-loss linear SVM) as well as
        L1-regularized classification (such as L2-loss linear SVM and
        logistic regression) and L2-regularized support vector
        regression (with L1- or L2-loss). The main features of
        LiblineaR include multi-class classification (one-vs-the rest,
        and Crammer &amp; Singer method), cross validation for model
        selection, probability estimates (logistic regression only) or
        weights for unbalanced data. The estimation of the models is
        particularly fast as compared to other libraries.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>Suggests:</td>
<td>SparseM, Matrix</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods</td>
</tr>
<tr>
<td>URL:</td>
<td>&lt;<a href="https://dnalytics.com/software/liblinear/&amp;gt;">https://dnalytics.com/software/liblinear/&gt;</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-12-11 07:59:21 UTC; thibault</td>
</tr>
<tr>
<td>Author:</td>
<td>Thibault Helleputte [cre, aut, cph],
  Jérôme Paul [aut],
  Pierre Gramme [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Thibault Helleputte &lt;thibault.helleputte@dnalytics.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-12-11 08:30:08 UTC</td>
</tr>
</table>
<hr>
<h2 id='heuristicC'>Fast Heuristics For The Estimation Of the C Constant Of A Support Vector Machine.</h2><span id='topic+heuristicC'></span>

<h3>Description</h3>

<p><code>heuristicC</code> implements a heuristics proposed by Thorsten Joachims in
order to make fast estimates of a convenient value for the C constant used by
support vector machines. This implementation only works for linear support
vector machines.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>heuristicC(data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="heuristicC_+3A_data">data</code></td>
<td>
<p>a nxp data matrix. Each row stands for an example (sample, point)
and each column stands for a dimension (feature, variable)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A value for the C constant is returned, computed as follows:<br />
<code class="reqn">\frac{1}{\frac{1}{n}\sum_{i=1}^{n}\sqrt{G[i,i]}}</code>
where
<code class="reqn">G=\code{data}\%*\%t(\code{data})</code>
</p>


<h3>Note</h3>

<p>Classification models usually perform better if each dimension of the
data is first centered and scaled. If data are scaled, it is better to
compute the heuristics on the scaled data as well.
</p>


<h3>Author(s)</h3>

<p>Thibault Helleputte <a href="mailto:thibault.helleputte@dnalytics.com">thibault.helleputte@dnalytics.com</a>
</p>


<h3>References</h3>


<ul>
<li> 
<p>T. Joachims<br />
<em>SVM light</em> (2002)<br />
<a href="http://svmlight.joachims.org">http://svmlight.joachims.org</a>
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+LiblineaR">LiblineaR</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)

x=iris[,1:4]
y=factor(iris[,5])
train=sample(1:dim(iris)[1],100)

xTrain=x[train,]
xTest=x[-train,]
yTrain=y[train]
yTest=y[-train]

# Center and scale data
s=scale(xTrain,center=TRUE,scale=TRUE)

# Sparse Logistic Regression
t=6

co=heuristicC(s)
m=LiblineaR(data=s,labels=yTrain,type=t,cost=co,bias=TRUE,verbose=FALSE)


</code></pre>

<hr>
<h2 id='LiblineaR'>Linear predictive models estimation based on the LIBLINEAR C/C++ Library.</h2><span id='topic+LiblineaR'></span>

<h3>Description</h3>

<p><code>LiblineaR</code> allows the estimation of predictive linear models for 
classification and regression, such as L1- or L2-regularized logistic 
regression, L1- or L2-regularized L2-loss support vector classification, 
L2-regularized L1-loss support vector classification and multi-class support 
vector classification. It also supports L2-regularized support vector regression 
(with L1- or L2-loss). The estimation of the models is particularly fast as 
compared to other libraries. The implementation is based on the LIBLINEAR C/C++ 
library for machine learning.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LiblineaR(
  data,
  target,
  type = 0,
  cost = 1,
  epsilon = 0.01,
  svr_eps = NULL,
  bias = 1,
  wi = NULL,
  cross = 0,
  verbose = FALSE,
  findC = FALSE,
  useInitC = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LiblineaR_+3A_data">data</code></td>
<td>
<p>a nxp data matrix. Each row stands for an example (sample,
point) and each column stands for a dimension (feature, variable). Sparse
matrices of class matrix.csr, matrix.csc and matrix.coo from package 
SparseM are accepted. Sparse matrices of class dgCMatrix, dgRMatrix or 
dgTMatrix from package Matrix are also accepted. Note that C code at the 
core of LiblineaR package corresponds to a row-based sparse format. Hence, 
dgCMatrix, dgTMatrix, matrix.csc and matrix.csr inputs are first 
transformed into matrix.csr or dgRMatrix formats, which requires small 
extra computation time.</p>
</td></tr>
<tr><td><code id="LiblineaR_+3A_target">target</code></td>
<td>
<p>a response vector for prediction tasks with one value for 
each of the n rows of <code>data</code>. For classification, the values 
correspond to class labels and can be a 1xn matrix, a simple vector or a 
factor. For regression, the values correspond to the values to predict, and
can be a 1xn matrix or a simple vector.</p>
</td></tr>
<tr><td><code id="LiblineaR_+3A_type">type</code></td>
<td>
<p><code>LiblineaR</code> can produce 10 types of (generalized) linear 
models, by combining several types of loss functions and regularization 
schemes. The regularization can be L1 or L2, and the losses can be the
regular L2-loss for SVM (hinge loss), L1-loss for SVM, or the logistic loss
for logistic regression. The default value for <code>type</code> is 0. See
details below. Valid options are:
</p>

<dl>
<dt>for multi-class classification</dt><dd>

<ul>
<li><p> 0 &ndash; L2-regularized logistic regression (primal)
</p>
</li>
<li><p> 1 &ndash; L2-regularized L2-loss support vector classification (dual)
</p>
</li>
<li><p> 2 &ndash; L2-regularized L2-loss support vector classification (primal)
</p>
</li>
<li><p> 3 &ndash; L2-regularized L1-loss support vector classification (dual)
</p>
</li>
<li><p> 4 &ndash; support vector classification by Crammer and Singer
</p>
</li>
<li><p> 5 &ndash; L1-regularized L2-loss support vector classification
</p>
</li>
<li><p> 6 &ndash; L1-regularized logistic regression
</p>
</li>
<li><p> 7 &ndash; L2-regularized logistic regression (dual)
</p>
</li></ul>

</dd>
<dt>for regression</dt><dd>

<ul>
<li><p> 11 &ndash; L2-regularized L2-loss support vector regression (primal)
</p>
</li>
<li><p> 12 &ndash; L2-regularized L2-loss support vector regression (dual)
</p>
</li>
<li><p> 13 &ndash; L2-regularized L1-loss support vector regression (dual)
</p>
</li></ul>

</dd>
</dl>
</td></tr>
<tr><td><code id="LiblineaR_+3A_cost">cost</code></td>
<td>
<p>cost of constraints violation (default: 1). Rules the trade-off
between regularization and correct classification on <code>data</code>. It can be
seen as the inverse of a regularization constant. See information on the
'C' constant in details below. A usually good baseline heuristics to tune
this constant is provided by the <code>heuristicC</code> function of this
package.</p>
</td></tr>
<tr><td><code id="LiblineaR_+3A_epsilon">epsilon</code></td>
<td>
<p>set tolerance of termination criterion for optimization.
If <code>NULL</code>, the LIBLINEAR defaults are used, which are:
</p>

<dl>
<dt>if <code>type</code> is 0, 2, 5 or 6</dt><dd><p><code>epsilon</code>=0.01</p>
</dd>
<dt>if <code>type</code> is 1, 3, 4, 7, 12 or 13</dt><dd><p><code>epsilon</code>=0.1</p>
</dd>
</dl>



<p>The meaning of <code>epsilon</code> is as follows:
</p>

<dl>
<dt>if <code>type</code> is 0 or 2:</dt><dd>
<p><code class="reqn">|f'(w)|_{2} \le \code{epsilon} \times \min (pos,neg) / l \times |f'(w_{0})|_{2}</code>,
where f is the primal function and pos/neg are # of positive/negative data (default 0.01)</p>
</dd>
<dt>if <code>type</code> is 11:</dt><dd>
<p><code class="reqn">|f'(w)|_{2} \le \code{epsilon} \times |f'(w_{0})|_{2},</code>
where f is the primal function (default 0.001)</p>
</dd>
<dt>if <code>type</code> is 1, 3, 4 or 7:</dt><dd>
<p>Dual maximal violation <code class="reqn">\le \code{epsilon}</code> 
(default 0.1)</p>
</dd>
<dt>if <code>type</code> is 5 or 6:</dt><dd>
<p><code class="reqn">|f'(w)|_\infty \le \code{epsilon}\times \min(pos,neg)/l\ |f'(w_{0})|_\infty,</code>
where f is the primal function (default 0.01)</p>
</dd>
<dt>if <code>type</code> is 12 or 13:</dt><dd>
<p><code class="reqn">|f'(\alpha)|_1 \le \code{epsilon}\times |f'(\alpha_{0})|_1,</code>
where f is the dual function (default 0.1)</p>
</dd>
</dl>
</td></tr>
<tr><td><code id="LiblineaR_+3A_svr_eps">svr_eps</code></td>
<td>
<p>set tolerance margin (epsilon) in regression loss function of SVR. Not used for classification methods.</p>
</td></tr>
<tr><td><code id="LiblineaR_+3A_bias">bias</code></td>
<td>
<p>if bias &gt; 0, instance <code>data</code> becomes [<code>data</code>; <code>bias</code>]; if &lt;= 0, no bias term added (default 1).</p>
</td></tr>
<tr><td><code id="LiblineaR_+3A_wi">wi</code></td>
<td>
<p>a named vector of weights for the different classes, used for
asymmetric class sizes. Not all factor levels have to be supplied (default
weight: 1). All components have to be named according to the corresponding
class label. Not used in regression mode.</p>
</td></tr>
<tr><td><code id="LiblineaR_+3A_cross">cross</code></td>
<td>
<p>if an integer value k&gt;0 is specified, a k-fold cross validation
on <code>data</code> is performed to assess the quality of the model via a
measure of the accuracy. Note that this metric might not be appropriate if
classes are largely unbalanced. Default is 0.</p>
</td></tr>
<tr><td><code id="LiblineaR_+3A_verbose">verbose</code></td>
<td>
<p>if <code>TRUE</code>, information are printed. Default is
<code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="LiblineaR_+3A_findc">findC</code></td>
<td>
<p>if <code>findC</code> is <code>TRUE</code> runs a cross-validation of <code>cross</code> folds to find the best cost (C) value (works only for type 0 and 2).
Cross validation is conducted many times under parameters C = start_C, 2*start_C, 4*start_C, 8*start_C, ..., and finds the best one with the highest cross validation accuracy.
The procedure stops when the models of all folds become stable or C reaches the maximal value of 1024.</p>
</td></tr>
<tr><td><code id="LiblineaR_+3A_useinitc">useInitC</code></td>
<td>
<p>if <code>useInitC</code> is <code>TRUE</code> (default) <code>cost</code> is used as the smallest start_C value of the search range (<code>findC</code> has to be <code>TRUE</code>).
If <code>useInitC</code> is <code>FALSE</code>, then the procedure calculates a small enough start_C.</p>
</td></tr>
<tr><td><code id="LiblineaR_+3A_...">...</code></td>
<td>
<p>for backwards compatibility, parameter <code>labels</code> may be
provided instead of <code>target</code>. A warning will then be issued, or an
error if both are present. Other extra parameters are ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For details for the implementation of LIBLINEAR, see the README file of the
original c/c++ LIBLINEAR library at
<a href="https://www.csie.ntu.edu.tw/~cjlin/liblinear/">https://www.csie.ntu.edu.tw/~cjlin/liblinear/</a>.
</p>


<h3>Value</h3>

<p>If <code>cross</code>&gt;0, the average accuracy (classification) or mean square error (regression) computed over <code>cross</code> runs of cross-validation is returned.<br /><br />
Otherwise, an object of class <code>"LiblineaR"</code> containing the fitted model is returned, including:
</p>
<table>
<tr><td><code>TypeDetail</code></td>
<td>
<p>A string decsribing the type of model fitted, as determined by <code>type</code>.</p>
</td></tr>
<tr><td><code>Type</code></td>
<td>
<p>An integer corresponding to <code>type</code>.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>A matrix with the model weights. If <code>bias</code> &gt;0, <code>W</code> contains p+1 columns, the last being the bias term. The columns are named according to the names of <code>data</code>, if provided, or <code>"Wx"</code> where <code>"x"</code> ranges from 1 to the number of dimensions. The bias term is named <code>"Bias"</code>.If the number of classes is 2, or if in regression mode rather than classification, the matrix only has one row. If the number of classes is k&gt;2 (classification), it has k rows. Each row i corresponds then to a linear model discriminating between class i and all the other classes. If there are more than 2 classes, rows are named according to the class i which is opposed to the other classes.</p>
</td></tr>
<tr><td><code>Bias</code></td>
<td>
<p>The value of <code>bias</code></p>
</td></tr>
<tr><td><code>ClassNames</code></td>
<td>
<p>A vector containing the class names. This entry is not returned in case of regression models.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Classification models usually perform better if each dimension of the data is first centered and scaled.
</p>


<h3>Author(s)</h3>

<p>Thibault Helleputte <a href="mailto:thibault.helleputte@dnalytics.com">thibault.helleputte@dnalytics.com</a> and<br />
Jerome Paul <a href="mailto:jerome.paul@dnalytics.com">jerome.paul@dnalytics.com</a> and Pierre Gramme.<br />
Based on C/C++-code by Chih-Chung Chang and Chih-Jen Lin
</p>


<h3>References</h3>


<ul>
<li> 
<p>For more information on LIBLINEAR itself, refer to:<br />
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin.<br />
<em>LIBLINEAR: A Library for Large Linear Classification,</em><br />
Journal of Machine Learning Research 9(2008), 1871-1874.<br />
<a href="https://www.csie.ntu.edu.tw/~cjlin/liblinear/">https://www.csie.ntu.edu.tw/~cjlin/liblinear/</a>
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+predict.LiblineaR">predict.LiblineaR</a></code>, <code><a href="#topic+heuristicC">heuristicC</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
attach(iris)

x=iris[,1:4]
y=factor(iris[,5])
train=sample(1:dim(iris)[1],100)

xTrain=x[train,]
xTest=x[-train,]
yTrain=y[train]
yTest=y[-train]

# Center and scale data
s=scale(xTrain,center=TRUE,scale=TRUE)

# Find the best model with the best cost parameter via 10-fold cross-validations
tryTypes=c(1:6)
tryCosts=c(1000,0.001)
bestCost=NA
bestAcc=0
bestType=NA

for(ty in tryTypes){
	for(co in tryCosts){
		acc=LiblineaR(data=s,target=yTrain,type=ty,cost=co,bias=1,cross=5,verbose=FALSE)
		cat("Results for C=",co," : ",acc," accuracy.\n",sep="")
		if(acc&gt;bestAcc){
			bestCost=co
			bestAcc=acc
			bestType=ty
		}
	}
}

cat("Best model type is:",bestType,"\n")
cat("Best cost is:",bestCost,"\n")
cat("Best accuracy is:",bestAcc,"\n")

# Re-train best model with best cost value.
m=LiblineaR(data=s,target=yTrain,type=bestType,cost=bestCost,bias=1,verbose=FALSE)

# Scale the test data
s2=scale(xTest,attr(s,"scaled:center"),attr(s,"scaled:scale"))

# Make prediction
pr=FALSE
if(bestType==0 || bestType==7) pr=TRUE

p=predict(m,s2,proba=pr,decisionValues=TRUE)

# Display confusion matrix
res=table(p$predictions,yTest)
print(res)

# Compute Balanced Classification Rate
BCR=mean(c(res[1,1]/sum(res[,1]),res[2,2]/sum(res[,2]),res[3,3]/sum(res[,3])))
print(BCR)

#' #############################################

# Example of the use of a sparse matrix of class matrix.csr :

if(require(SparseM)){

 # Sparsifying the iris dataset:
 iS=apply(iris[,1:4],2,function(a){a[a&lt;quantile(a,probs=c(0.25))]=0;return(a)})
 irisSparse&lt;-as.matrix.csr(iS)

 # Applying a similar methodology as above:
 xTrain=irisSparse[train,]
 xTest=irisSparse[-train,]

 # Re-train best model with best cost value.
 m=LiblineaR(data=xTrain,target=yTrain,type=bestType,cost=bestCost,bias=1,verbose=FALSE)

 # Make prediction
 p=predict(m,xTest,proba=pr,decisionValues=TRUE)

}

#' #############################################

# Example of the use of a sparse matrix of class dgCMatrix :

if(require(Matrix)){

 # Sparsifying the iris dataset:
 iS=apply(iris[,1:4],2,function(a){a[a&lt;quantile(a,probs=c(0.25))]=0;return(a)})
 irisSparse&lt;-as(iS,"sparseMatrix")

 # Applying a similar methodology as above:
 xTrain=irisSparse[train,]
 xTest=irisSparse[-train,]

 # Re-train best model with best cost value.
 m=LiblineaR(data=xTrain,target=yTrain,type=bestType,cost=bestCost,bias=1,verbose=FALSE)

 # Make prediction
 p=predict(m,xTest,proba=pr,decisionValues=TRUE)

}

#############################################

# Try regression instead, to predict sepal length on the basis of sepal width and petal width:

xTrain=iris[c(1:25,51:75,101:125),2:3]
yTrain=iris[c(1:25,51:75,101:125),1]
xTest=iris[c(26:50,76:100,126:150),2:3]
yTest=iris[c(26:50,76:100,126:150),1]

# Center and scale data
s=scale(xTrain,center=TRUE,scale=TRUE)

# Estimate MSE in cross-vaidation on a train set
MSECross=LiblineaR(data = s, target = yTrain, type = 13, cross = 5, svr_eps=.01)

# Build the model
m=LiblineaR(data = s, target = yTrain, type = 13, cross=0, svr_eps=.01)

# Test it, after test data scaling:
s2=scale(xTest,attr(s,"scaled:center"),attr(s,"scaled:scale"))
pred=predict(m,s2)$predictions
MSETest=mean((yTest-pred)^2)

# Was MSE well estimated?
print(MSETest-MSECross)

# Distribution of errors
print(summary(yTest-pred))



</code></pre>

<hr>
<h2 id='predict.LiblineaR'>Predictions with LiblineaR model</h2><span id='topic+predict.LiblineaR'></span>

<h3>Description</h3>

<p>The function applies a model (classification or regression) produced by the <code>LiblineaR</code> function to every row of a
data matrix and returns the model predictions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'LiblineaR'
predict(object, newx, proba = FALSE, decisionValues = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.LiblineaR_+3A_object">object</code></td>
<td>
<p>Object of class <code>"LiblineaR"</code>, created by
<code>LiblineaR</code>.</p>
</td></tr>
<tr><td><code id="predict.LiblineaR_+3A_newx">newx</code></td>
<td>
<p>An n x p matrix containing the new input data. A vector will be
transformed to a n x 1 matrix. Sparse matrices of class matrix.csr, 
matrix.csc and matrix.coo from package SparseM are accepted. Sparse 
matrices of class dgCMatrix, dgRMatrix or dgTMatrix from package Matrix are
also accepted. Note that C code at the core of LiblineaR package 
corresponds to a row-based sparse format. Hence, dgCMatrix, dgTMatrix, 
matrix.csc and matrix.csr inputs are first transformed into matrix.csr or 
dgRMatrix formats, which requires small extra computation time.</p>
</td></tr>
<tr><td><code id="predict.LiblineaR_+3A_proba">proba</code></td>
<td>
<p>Logical indicating whether class probabilities should be
computed and returned. Only possible if the model was fitted with
<code>type</code>=0, <code>type</code>=6 or <code>type</code>=7, i.e. a Logistic Regression.
Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="predict.LiblineaR_+3A_decisionvalues">decisionValues</code></td>
<td>
<p>Logical indicating whether model decision values should
be computed and returned. Only possible for classification models
(<code>type</code>&lt;10). Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="predict.LiblineaR_+3A_...">...</code></td>
<td>
<p>Currently not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>By default, the returned value is a list with a single entry:
</p>
<table>
<tr><td><code>predictions</code></td>
<td>
<p>A vector of predicted labels (or values for regression).</p>
</td></tr>
</table>
<p>If <code>proba</code> is set to <code>TRUE</code>, and the model is a logistic
regression, an additional entry is returned:
</p>
<table>
<tr><td><code>probabilities</code></td>
<td>
<p>An n x k matrix (k number of classes) of the class
probabilities. The columns of this matrix are named after class labels.</p>
</td></tr>
</table>
<p>If <code>decisionValues</code> is set to <code>TRUE</code>, and the model is not a
regression model, an additional entry is returned:
</p>
<table>
<tr><td><code>decisionValues</code></td>
<td>
<p>An n x k matrix (k number of classes) of the model
decision values. The columns of this matrix are named after class labels.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>If the data on which the model has been fitted have been centered
and/or scaled, it is very important to apply the same process on the
<code>newx</code> data as well, with the scale and center values of the training
data.
</p>


<h3>Author(s)</h3>

<p>Thibault Helleputte <a href="mailto:thibault.helleputte@dnalytics.com">thibault.helleputte@dnalytics.com</a> and<br />
Jerome Paul <a href="mailto:jerome.paul@dnalytics.com">jerome.paul@dnalytics.com</a> and Pierre Gramme.<br />
Based on C/C++-code by Chih-Chung Chang and Chih-Jen Lin
</p>


<h3>References</h3>


<ul>
<li> 
<p>For more information on LIBLINEAR itself, refer to:<br />
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin.<br />
<em>LIBLINEAR: A Library for Large Linear Classification,</em><br />
Journal of Machine Learning Research 9(2008), 1871-1874.<br />
<a href="https://www.csie.ntu.edu.tw/~cjlin/liblinear/">https://www.csie.ntu.edu.tw/~cjlin/liblinear/</a>
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+LiblineaR">LiblineaR</a></code>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
