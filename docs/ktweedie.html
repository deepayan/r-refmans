<!DOCTYPE html><html lang="en"><head><title>Help for package ktweedie</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ktweedie}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ktweedie-package'><p>ktweedie: 'Tweedie' Compound Poisson Model in the Reproducing Kernel Hilbert Space</p></a></li>
<li><a href='#as.kernelMatrix'><p>Assing kernelMatrix class to matrix objects</p></a></li>
<li><a href='#dat'><p>A demo dataset</p></a></li>
<li><a href='#dots'><p>Kernel Functions</p></a></li>
<li><a href='#kernel-class'><p>Class &quot;kernel&quot; &quot;rbfkernel&quot; &quot;polykernel&quot;, &quot;tanhkernel&quot;, &quot;vanillakernel&quot;</p></a></li>
<li><a href='#kernelMatrix'><p>Kernel Matrix functions</p></a></li>
<li><a href='#ktd_cv'><p>Cross validation for tuning the regularization coefficient in the kernel Tweedie model</p></a></li>
<li><a href='#ktd_cv2d'><p>Cross validation for jointly tuning the regularization coefficient and kernel parameter in the Kernel Tweedie Model</p></a></li>
<li><a href='#ktd_estimate'><p>Estimate kernel Tweedie model coefficients</p></a></li>
<li><a href='#ktd_predict'><p>Predict outcome using fitted kernel Tweedie model</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>'Tweedie' Compound Poisson Model in the Reproducing Kernel
Hilbert Space</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-10-14</td>
</tr>
<tr>
<td>Description:</td>
<td>Kernel-based 'Tweedie' compound Poisson gamma model using high-dimensional predictors for the analyses of zero-inflated response variables. The package features built-in estimation, prediction and cross-validation tools and supports choice of different kernel functions. For more details, please see Yi Lian, Archer Yi Yang, Boxiang Wang, Peng Shi &amp; Robert William Platt (2023) &lt;<a href="https://doi.org/10.1080%2F00401706.2022.2156615">doi:10.1080/00401706.2022.2156615</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Depends:</td>
<td>methods, R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, tweedie</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-10-14 22:06:48 UTC; YLIAN</td>
</tr>
<tr>
<td>Author:</td>
<td>Yi Lian [aut, cre],
  Archer Yi Yang [aut, cph],
  Boxiang Wang [aut],
  Peng Shi [aut],
  Robert W. Platt [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Yi Lian &lt;yi.lian@mail.mcgill.ca&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-10-14 22:32:43 UTC</td>
</tr>
</table>
<hr>
<h2 id='ktweedie-package'>ktweedie: 'Tweedie' Compound Poisson Model in the Reproducing Kernel Hilbert Space</h2><span id='topic+ktweedie'></span><span id='topic+ktweedie-package'></span>

<h3>Description</h3>

<p>Kernel-based 'Tweedie' compound Poisson gamma model using high-dimensional predictors for the analyses of zero-inflated response variables. The package features built-in estimation, prediction and cross-validation tools and supports choice of different kernel functions. For more details, please see Yi Lian, Archer Yi Yang, Boxiang Wang, Peng Shi &amp; Robert William Platt (2023) <a href="https://doi.org/10.1080/00401706.2022.2156615">doi:10.1080/00401706.2022.2156615</a>.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Yi Lian <a href="mailto:yi.lian@mail.mcgill.ca">yi.lian@mail.mcgill.ca</a>
</p>
<p>Authors:
</p>

<ul>
<li><p> Archer Yi Yang [copyright holder]
</p>
</li>
<li><p> Boxiang Wang
</p>
</li>
<li><p> Peng Shi
</p>
</li>
<li><p> Robert W. Platt
</p>
</li></ul>


<hr>
<h2 id='as.kernelMatrix'>Assing kernelMatrix class to matrix objects</h2><span id='topic+kernelMatrix-class'></span><span id='topic+as.kernelMatrix'></span><span id='topic+as.kernelMatrix-methods'></span><span id='topic+as.kernelMatrix+2Cmatrix-method'></span>

<h3>Description</h3>

<p><code>as.kernelMatrix</code> in package <span class="pkg">KERE</span> can be used 
to coerce the kernelMatrix class to matrix objects representing a
kernel matrix.  These matrices can then be used with the kernelMatrix
interfaces which most of the functions in <span class="pkg">KERE</span> support.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'matrix'
as.kernelMatrix(x, center = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="as.kernelMatrix_+3A_x">x</code></td>
<td>
<p>matrix to be assigned the <code>kernelMatrix</code> class </p>
</td></tr>
<tr><td><code id="as.kernelMatrix_+3A_center">center</code></td>
<td>
<p>center the kernel matrix in feature space (default: FALSE) </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Alexandros Karatzoglou <br />
<a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kernelMatrix">kernelMatrix</a></code>, <code><a href="#topic+dots">dots</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Create toy data
x &lt;- rbind(matrix(rnorm(10),,2),matrix(rnorm(10,mean=3),,2))
y &lt;- matrix(c(rep(1,5),rep(-1,5)))

### Use as.kernelMatrix to label the cov. matrix as a kernel matrix
### which is eq. to using a linear kernel 

K &lt;- as.kernelMatrix(crossprod(t(x)))

K

</code></pre>

<hr>
<h2 id='dat'>A demo dataset</h2><span id='topic+dat'></span>

<h3>Description</h3>

<p>A simulated dataset with covariate matrix <code>x</code> of size 30 x 5 and an outcome vector <code>y</code> of length 30.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(dat)
</code></pre>


<h3>Format</h3>

<p>A list with 2 items:
</p>

<dl>
<dt>x</dt><dd><p>Covariate matrix</p>
</dd>
<dt>y</dt><dd><p>Outcome vector</p>
</dd>
</dl>



<h3>Details</h3>

<p><code>x</code> is generated from standard normal distribution. <code>y</code> is generated from Tweedie distribution with mean equal to exp(sin(x) %*% (6, -4, 0, 0, 0)). Only the first two variables are associated with the outcome.
</p>

<hr>
<h2 id='dots'>Kernel Functions</h2><span id='topic+dots'></span><span id='topic+kernels'></span><span id='topic+rbfdot'></span><span id='topic+polydot'></span><span id='topic+tanhdot'></span><span id='topic+vanilladot'></span><span id='topic+laplacedot'></span><span id='topic+besseldot'></span><span id='topic+anovadot'></span><span id='topic+fourierdot'></span><span id='topic+splinedot'></span><span id='topic+kpar'></span><span id='topic+kfunction'></span><span id='topic+show+2Ckernel-method'></span>

<h3>Description</h3>

<p>The kernel generating functions provided in KERE. <br />
The Gaussian RBF kernel <code class="reqn">k(x,x') = \exp(-\sigma \|x - x'\|^2)</code> <br />
The Polynomial kernel <code class="reqn">k(x,x') = (scale &lt;x, x'&gt; + offset)^{degree}</code><br />
The Linear kernel <code class="reqn">k(x,x') = &lt;x, x'&gt;</code><br />
The Hyperbolic tangent kernel <code class="reqn">k(x, x') = \tanh(scale &lt;x, x'&gt; +  offset)</code><br />
The Laplacian kernel <code class="reqn">k(x,x') = \exp(-\sigma \|x - x'\|)</code> <br />
The Bessel kernel <code class="reqn">k(x,x') = (- Bessel_{(\nu+1)}^n \sigma \|x - x'\|^2)</code> <br />
The ANOVA RBF kernel <code class="reqn">k(x,x') = \sum_{1\leq i_1 \ldots &lt; i_D \leq
      N} \prod_{d=1}^D k(x_{id}, {x'}_{id})</code> where k(x,x) is a Gaussian
RBF kernel. <br />
The Spline kernel <code class="reqn"> \prod_{d=1}^D 1 + x_i x_j + x_i x_j min(x_i,
    x_j)  - \frac{x_i + x_j}{2} min(x_i,x_j)^2 +
    \frac{min(x_i,x_j)^3}{3}</code> \
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rbfdot(sigma = 1)

polydot(degree = 1, scale = 1, offset = 1)

tanhdot(scale = 1, offset = 1)

vanilladot()

laplacedot(sigma = 1)

besseldot(sigma = 1, order = 1, degree = 1)

anovadot(sigma = 1, degree = 1)

splinedot()
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dots_+3A_sigma">sigma</code></td>
<td>
<p>The inverse kernel width used by the Gaussian the
Laplacian, the Bessel and the ANOVA kernel </p>
</td></tr>
<tr><td><code id="dots_+3A_degree">degree</code></td>
<td>
<p>The degree of the polynomial, bessel or ANOVA
kernel function. This has to be an positive integer.</p>
</td></tr>
<tr><td><code id="dots_+3A_scale">scale</code></td>
<td>
<p>The scaling parameter of the polynomial and tangent
kernel is a convenient way of normalizing
patterns without the need to modify the data itself</p>
</td></tr>
<tr><td><code id="dots_+3A_offset">offset</code></td>
<td>
<p>The offset used in a polynomial or hyperbolic tangent
kernel</p>
</td></tr>
<tr><td><code id="dots_+3A_order">order</code></td>
<td>
<p>The order of the Bessel function to be used as a kernel</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The kernel generating functions are used to initialize a kernel
function
which calculates the dot (inner) product between two feature vectors in a
Hilbert Space. These functions can be passed as a <code>kernel</code> argument on almost all
functions in <span class="pkg">KERE</span>(e.g., <code>ksvm</code>, <code>kpca</code>  etc).
</p>
<p>Although using one of the existing kernel functions as a
<code>kernel</code> argument in various functions in <span class="pkg">KERE</span> has the
advantage that optimized code is used to calculate various kernel expressions,
any other function implementing a dot product of class <code>kernel</code> can also be used as a kernel
argument. This allows the user to use, test and develop special kernels
for a given data set or algorithm.
</p>


<h3>Value</h3>

<p>Return an S4 object of class <code>kernel</code> which extents the
<code>function</code> class. The resulting function implements the given
kernel calculating the inner (dot) product between two vectors.
</p>
<table role = "presentation">
<tr><td><code>kpar</code></td>
<td>
<p>a list containing the kernel parameters (hyperparameters)
used.</p>
</td></tr>
</table>
<p>The kernel parameters can be accessed by the <code>kpar</code> function.
</p>


<h3>Note</h3>

<p>If the offset in the Polynomial kernel is set to 0, we obtain homogeneous polynomial
kernels, for positive values, we have inhomogeneous
kernels. Note that for negative values the kernel does not satisfy Mercer's
condition and thus the optimizers may fail. <br />
</p>
<p>In the Hyperbolic tangent kernel if the offset is negative the likelihood of obtaining a kernel
matrix that is not positive definite is much higher (since then even some
diagonal elements may be negative), hence if this kernel has to be used, the
offset should always be positive. Note, however, that this is no guarantee
that the kernel will be positive.
</p>


<h3>Author(s)</h3>

<p>Alexandros Karatzoglou<br />
<a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+kernelMatrix">kernelMatrix</a> </code>, <code><a href="#topic+kernelMult">kernelMult</a></code>, <code><a href="#topic+kernelPol">kernelPol</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>rbfkernel &lt;- rbfdot(sigma = 0.1)
rbfkernel

kpar(rbfkernel)

## create two vectors
x &lt;- rnorm(10)
y &lt;- rnorm(10)

## calculate dot product
rbfkernel(x,y)

</code></pre>

<hr>
<h2 id='kernel-class'>Class &quot;kernel&quot; &quot;rbfkernel&quot; &quot;polykernel&quot;, &quot;tanhkernel&quot;, &quot;vanillakernel&quot;</h2><span id='topic+rbfkernel-class'></span><span id='topic+polykernel-class'></span><span id='topic+vanillakernel-class'></span><span id='topic+tanhkernel-class'></span><span id='topic+anovakernel-class'></span><span id='topic+besselkernel-class'></span><span id='topic+laplacekernel-class'></span><span id='topic+splinekernel-class'></span><span id='topic+fourierkernel-class'></span><span id='topic+kfunction-class'></span><span id='topic+kernel-class'></span><span id='topic+kpar+2Ckernel-method'></span>

<h3>Description</h3>

<p>  The built-in kernel classes in <span class="pkg">KERE</span></p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("rbfkernel")</code>,
<code>new{"polykernel"}</code>, <code>new{"tanhkernel"}</code>,
<code>new{"vanillakernel"}</code>, <code>new{"anovakernel"}</code>,
<code>new{"besselkernel"}</code>, <code>new{"laplacekernel"}</code>,
<code>new{"splinekernel"}</code> or by calling the <code>rbfdot</code>, <code>polydot</code>, <code>tanhdot</code>,
<code>vanilladot</code>, <code>anovadot</code>, <code>besseldot</code>, <code>laplacedot</code>,
<code>splinedot</code> functions etc..
</p>


<h3>Slots</h3>


<dl>
<dt><code>.Data</code>:</dt><dd><p>Object of class <code>"function"</code> containing
the kernel function </p>
</dd>
<dt><code>kpar</code>:</dt><dd><p>Object of class <code>"list"</code> containing the
kernel parameters </p>
</dd>
</dl>



<h3>Extends</h3>

<p>Class <code>"kernel"</code>, directly.
Class <code>"function"</code>, by class <code>"kernel"</code>.
</p>


<h3>Methods</h3>


<dl>
<dt>kernelMatrix</dt><dd><p><code>signature(kernel = "rbfkernel", x =
	"matrix")</code>: computes the kernel matrix</p>
</dd>
<dt>kernelMult</dt><dd><p><code>signature(kernel = "rbfkernel", x =
	"matrix")</code>: computes the quadratic kernel expression</p>
</dd>
<dt>kernelPol</dt><dd><p><code>signature(kernel = "rbfkernel", x =
	"matrix")</code>: computes the kernel expansion</p>
</dd>
<dt>kernelFast</dt><dd><p><code>signature(kernel = "rbfkernel", x =
	"matrix"),,a</code>: computes parts or the full kernel matrix, mainly
used in kernel algorithms where columns of the kernel matrix are
computed per invocation </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Alexandros Karatzoglou<br /> <a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a> </p>


<h3>See Also</h3>

<p><code><a href="#topic+dots">dots</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
rbfkernel &lt;- rbfdot(sigma = 0.1)
rbfkernel
is(rbfkernel)
kpar(rbfkernel)

</code></pre>

<hr>
<h2 id='kernelMatrix'>Kernel Matrix functions</h2><span id='topic+kernelMatrix'></span><span id='topic+kernelMult'></span><span id='topic+kernelPol'></span><span id='topic+kernelFast'></span><span id='topic+kernelPol+2Ckernel-method'></span><span id='topic+kernelMatrix+2Ckernel-method'></span><span id='topic+kernelMult+2Ckernel-method'></span><span id='topic+kernelFast+2Ckernel-method'></span><span id='topic+kernelMatrix+2Crbfkernel-method'></span><span id='topic+kernelMatrix+2Cpolykernel-method'></span><span id='topic+kernelMatrix+2Cvanillakernel-method'></span><span id='topic+kernelMatrix+2Ctanhkernel-method'></span><span id='topic+kernelMatrix+2Claplacekernel-method'></span><span id='topic+kernelMatrix+2Canovakernel-method'></span><span id='topic+kernelMatrix+2Csplinekernel-method'></span><span id='topic+kernelMatrix+2Cbesselkernel-method'></span><span id='topic+kernelMult+2Crbfkernel+2CANY-method'></span><span id='topic+kernelMult+2Csplinekernel+2CANY-method'></span><span id='topic+kernelMult+2Cpolykernel+2CANY-method'></span><span id='topic+kernelMult+2Ctanhkernel+2CANY-method'></span><span id='topic+kernelMult+2Claplacekernel+2CANY-method'></span><span id='topic+kernelMult+2Cbesselkernel+2CANY-method'></span><span id='topic+kernelMult+2Canovakernel+2CANY-method'></span><span id='topic+kernelMult+2Cvanillakernel+2CANY-method'></span><span id='topic+kernelMult+2Ccharacter+2CkernelMatrix-method'></span><span id='topic+kernelPol+2Crbfkernel-method'></span><span id='topic+kernelPol+2Csplinekernel-method'></span><span id='topic+kernelPol+2Cpolykernel-method'></span><span id='topic+kernelPol+2Ctanhkernel-method'></span><span id='topic+kernelPol+2Cvanillakernel-method'></span><span id='topic+kernelPol+2Canovakernel-method'></span><span id='topic+kernelPol+2Cbesselkernel-method'></span><span id='topic+kernelPol+2Claplacekernel-method'></span><span id='topic+kernelFast+2Crbfkernel-method'></span><span id='topic+kernelFast+2Csplinekernel-method'></span><span id='topic+kernelFast+2Cpolykernel-method'></span><span id='topic+kernelFast+2Ctanhkernel-method'></span><span id='topic+kernelFast+2Cvanillakernel-method'></span><span id='topic+kernelFast+2Canovakernel-method'></span><span id='topic+kernelFast+2Cbesselkernel-method'></span><span id='topic+kernelFast+2Claplacekernel-method'></span>

<h3>Description</h3>

<p><code>kernelMatrix</code> calculates the kernel matrix <code class="reqn">K_{ij} = k(x_i,x_j)</code> or <code class="reqn">K_{ij} =
    k(x_i,y_j)</code>.<br />
<code>kernelPol</code> computes the quadratic kernel expression  <code class="reqn">H = z_i z_j
    k(x_i,x_j)</code>, <code class="reqn">H = z_i k_j k(x_i,y_j)</code>.<br />
<code>kernelMult</code> calculates the kernel expansion <code class="reqn">f(x_i) =
      \sum_{i=1}^m z_i  k(x_i,x_j)</code><br />
<code>kernelFast</code> computes the kernel matrix, identical
to <code>kernelMatrix</code>, except that it also requires the squared
norm of the first argument as additional input, useful in iterative
kernel matrix calculations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'kernel'
kernelMatrix(kernel, x, y = NULL)

## S4 method for signature 'kernel'
kernelPol(kernel, x, y = NULL, z, k = NULL)

## S4 method for signature 'kernel'
kernelMult(kernel, x, y = NULL, z, blocksize = 256)

## S4 method for signature 'kernel'
kernelFast(kernel, x, y, a)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kernelMatrix_+3A_kernel">kernel</code></td>
<td>
<p>the kernel function to be used to calculate the kernel
matrix.
This has to be a function of class <code>kernel</code>, i.e. which can be
generated either one of the build in 
kernel generating functions (e.g., <code>rbfdot</code> etc.) or a user defined
function of class <code>kernel</code> taking two vector arguments and returning a scalar.</p>
</td></tr>
<tr><td><code id="kernelMatrix_+3A_x">x</code></td>
<td>
<p>a data matrix to be used to calculate the kernel matrix.</p>
</td></tr>
<tr><td><code id="kernelMatrix_+3A_y">y</code></td>
<td>
<p>second data matrix to calculate the kernel matrix.</p>
</td></tr>
<tr><td><code id="kernelMatrix_+3A_z">z</code></td>
<td>
<p>a suitable vector or matrix</p>
</td></tr>
<tr><td><code id="kernelMatrix_+3A_k">k</code></td>
<td>
<p>a suitable vector or matrix</p>
</td></tr>
<tr><td><code id="kernelMatrix_+3A_a">a</code></td>
<td>
<p>the squared norm of <code>x</code>, e.g., <code>rowSums(x^2)</code></p>
</td></tr>
<tr><td><code id="kernelMatrix_+3A_blocksize">blocksize</code></td>
<td>
<p>the kernel expansion computations are done block wise
to avoid storing the kernel matrix into memory. <code>blocksize</code>
defines the size of the computational blocks.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Common functions used during kernel based computations.<br />
The <code>kernel</code> parameter can be set to any function, of class
kernel, which computes the inner product in feature space between two
vector arguments. <span class="pkg">KERE</span> provides the most popular kernel functions
which can be initialized by using the following
functions:
</p>

<ul>
<li> <p><code>rbfdot</code> Radial Basis kernel function
</p>
</li>
<li> <p><code>polydot</code> Polynomial kernel function
</p>
</li>
<li> <p><code>vanilladot</code> Linear kernel function
</p>
</li>
<li> <p><code>tanhdot</code> Hyperbolic tangent kernel function
</p>
</li>
<li> <p><code>laplacedot</code> Laplacian kernel function
</p>
</li>
<li> <p><code>besseldot</code> Bessel kernel function
</p>
</li>
<li> <p><code>anovadot</code> ANOVA RBF kernel function
</p>
</li>
<li> <p><code>splinedot</code> the Spline kernel 
</p>
</li></ul>
<p>  (see example.)
</p>
<p><code>kernelFast</code> is mainly used in situations where columns of the
kernel matrix are computed per invocation. In these cases,
evaluating the norm of each row-entry over and over again would
cause significant computational overhead.
</p>


<h3>Value</h3>

<p><code>kernelMatrix</code> returns a symmetric diagonal semi-definite matrix.<br />
<code>kernelPol</code> returns a matrix.<br />
<code>kernelMult</code> usually returns a one-column matrix.
</p>


<h3>Author(s)</h3>

<p>Alexandros Karatzoglou <br />
<a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+rbfdot">rbfdot</a></code>, <code><a href="#topic+polydot">polydot</a></code>,
<code><a href="#topic+tanhdot">tanhdot</a></code>, <code><a href="#topic+vanilladot">vanilladot</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## use the spam data
x &lt;- matrix(rnorm(10*10),10,10)

## initialize kernel function 
rbf &lt;- rbfdot(sigma = 0.05)
rbf

## calculate kernel matrix
kernelMatrix(rbf, x)

y &lt;- matrix(rnorm(10*1),10,1)


## calculate the quadratic kernel expression
kernelPol(rbf, x, ,y)

## calculate the kernel expansion
kernelMult(rbf, x, ,y)
</code></pre>

<hr>
<h2 id='ktd_cv'>Cross validation for tuning the regularization coefficient in the kernel Tweedie model</h2><span id='topic+ktd_cv'></span>

<h3>Description</h3>

<p><code>ktd_cv()</code> performs cross-validation to determine the optimal regularization coefficient of the <code>ktweedie</code> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ktd_cv(x, y, kern, lambda, nfolds = 5, rho = 1.5, loss = "LL", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ktd_cv_+3A_x">x</code></td>
<td>
<p>Covariate matrix.</p>
</td></tr>
<tr><td><code id="ktd_cv_+3A_y">y</code></td>
<td>
<p>Outcome vector (e.g. insurance cost).</p>
</td></tr>
<tr><td><code id="ktd_cv_+3A_kern">kern</code></td>
<td>
<p>Choice of kernel. See <code><a href="#topic+dots">dots</a></code> for details on supported kernel functions.</p>
</td></tr>
<tr><td><code id="ktd_cv_+3A_lambda">lambda</code></td>
<td>
<p>A vector of candidate regularization coefficients used in cross-validation.</p>
</td></tr>
<tr><td><code id="ktd_cv_+3A_nfolds">nfolds</code></td>
<td>
<p>Number of folds in cross-validation. Default is 5.</p>
</td></tr>
<tr><td><code id="ktd_cv_+3A_rho">rho</code></td>
<td>
<p>The power parameter of the Tweedie model. Default is 1.5 and can take any real value between 1 and 2.</p>
</td></tr>
<tr><td><code id="ktd_cv_+3A_loss">loss</code></td>
<td>
<p>Criterion used in cross-validation. &quot;LL&quot; for log likelihood, &quot;RMSE&quot; for root mean squared error, &quot;MAD&quot; for mean absolute difference. Default is &quot;LL&quot;.</p>
</td></tr>
<tr><td><code id="ktd_cv_+3A_...">...</code></td>
<td>
<p>Optional arguments to be passed to <code><a href="#topic+ktd_estimate">ktd_estimate</a>()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>ktd_cv()</code> is a built-in wrapper for cross-validation for the choice of regularization coefficient.
</p>


<h3>Value</h3>

<p>A list of two items.
</p>

<ol>
<li><p> LL or RMSE or MAD: a vector of validation error based on the user-specified <code>loss</code>, named by the corresponding <code>lambda</code> values;
</p>
</li>
<li><p> Best_lambda: the <code>lambda</code> value in the pair that generates the best loss;
</p>
</li></ol>



<h3>See Also</h3>

<p><code><a href="#topic+ktd_cv2d">ktd_cv2d</a></code>, <code><a href="#topic+ktd_estimate">ktd_estimate</a></code>, <code><a href="#topic+ktd_predict">ktd_predict</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Provide a sequence of candidate values to the argument lambda.
# ktd_cv() will perform cross-validation to determine which is the best.
( cv1d &lt;- ktd_cv(x = dat$x, y = dat$y,
                 kern = rbfdot(sigma = 1e-8),
                 lambda = 10^(-8:-1),
                 nfolds = 5) )
</code></pre>

<hr>
<h2 id='ktd_cv2d'>Cross validation for jointly tuning the regularization coefficient and kernel parameter in the Kernel Tweedie Model</h2><span id='topic+ktd_cv2d'></span>

<h3>Description</h3>

<p><code>ktd_cv2d()</code> performs 2-dimensional random search from user-specified ranges to determine the optimal pair of regularization coefficient and kernel parameter of the <code>ktweedie</code> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ktd_cv2d(
  x,
  y,
  kernfunc,
  lambda,
  sigma,
  ncoefs,
  nfolds = 5,
  rho = 1.5,
  loss = "LL",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ktd_cv2d_+3A_x">x</code></td>
<td>
<p>Covariate matrix.</p>
</td></tr>
<tr><td><code id="ktd_cv2d_+3A_y">y</code></td>
<td>
<p>Outcome vector (e.g. insurance cost).</p>
</td></tr>
<tr><td><code id="ktd_cv2d_+3A_kernfunc">kernfunc</code></td>
<td>
<p>Choice of kernel function. See <code><a href="#topic+dots">dots</a></code> for details on supported kernel functions.</p>
</td></tr>
<tr><td><code id="ktd_cv2d_+3A_lambda">lambda</code></td>
<td>
<p>A vector of length two indicating the lower and upper bound from which candidate regularization coefficient values are sampled uniformly on the log scale.</p>
</td></tr>
<tr><td><code id="ktd_cv2d_+3A_sigma">sigma</code></td>
<td>
<p>A vector of length two indicating the lower and upper bound from which candidate kernel parameter values are sampled uniformly on the log scale.</p>
</td></tr>
<tr><td><code id="ktd_cv2d_+3A_ncoefs">ncoefs</code></td>
<td>
<p>The number of candidate <code>lambda</code> and <code>sigma</code> pairs to be evaluated.</p>
</td></tr>
<tr><td><code id="ktd_cv2d_+3A_nfolds">nfolds</code></td>
<td>
<p>Number of folds in cross-validation. Default is 5.</p>
</td></tr>
<tr><td><code id="ktd_cv2d_+3A_rho">rho</code></td>
<td>
<p>The power parameter of the Tweedie model. Default is 1.5 and can take any real value between 1 and 2.</p>
</td></tr>
<tr><td><code id="ktd_cv2d_+3A_loss">loss</code></td>
<td>
<p>Criterion used in cross-validation. &quot;LL&quot; for log likelihood, &quot;RMSE&quot; for root mean squared error, &quot;MAD&quot; for mean absolute difference. Default is &quot;LL&quot;.</p>
</td></tr>
<tr><td><code id="ktd_cv2d_+3A_...">...</code></td>
<td>
<p>Optional arguments to be passed to <code>ktd_estimate()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>ktd_cv2d()</code> is a built-in wrapper for 2D random search for the regularization coefficient and kernel parameter. For kernel functions with greater than one parameters, <code>ktd_cv2d()</code> supports the tuning of the first one.
</p>


<h3>Value</h3>

<p>A list of three items.
</p>

<ol>
<li><p> LL or RMSE or MAD: a vector of validation error based on the user-specified <code>loss</code>, named by the corresponding <code>lambda</code> and <code>sigma</code> values;
</p>
</li>
<li><p> Best_lambda: the <code>lambda</code> value in the pair that generates the best loss;
</p>
</li>
<li><p> Best_sigma: the <code>sigma</code> value in the pair that generates the best loss.
</p>
</li></ol>



<h3>See Also</h3>

<p><code><a href="#topic+ktd_cv">ktd_cv</a></code>, <code><a href="#topic+ktd_estimate">ktd_estimate</a></code>, <code><a href="#topic+ktd_predict">ktd_predict</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Cross-validation
# Provide the kernel function name (e.g. rbfdot) to the argument kernfunc,
# NOT the kernel function object, e.g. rbfdot(sigma = 1).
# Provide ranges where the candidate lambdas and sigmas are drawn from
# to the arguments lambda and sigma.
# The number of pairs of candidates to select from is specified by ncoefs.
( cv2d &lt;- ktd_cv2d(x = dat$x, y = dat$y,
                   kernfunc = rbfdot,
                   lambda = c(1e-3, 1e0),
                   sigma = c(1e-3, 1e0),
                   ncoefs = 10) )
### Followed by fitting
fit &lt;- ktd_estimate(x = dat$x, y = dat$y,
                    kern = rbfdot(sigma = cv2d$Best_sigma),
                    lam1 = cv2d$Best_lambda)
</code></pre>

<hr>
<h2 id='ktd_estimate'>Estimate kernel Tweedie model coefficients</h2><span id='topic+ktd_estimate'></span>

<h3>Description</h3>

<p><code>ktd_estimate()</code> estimates the coefficients of the kernel Tweedie model <code>ktweedie</code> and the sparse kernel Tweedie model <code>sktweedie</code>. The log of the expected Tweedie mean is modeled by a function in the reproducing kernel Hilbert space. The <code>sktweedie</code> has an integrated feature selection component that induces sparsity by applying weights on the features and penalizing the weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ktd_estimate(
  x,
  y,
  kern,
  lam1,
  rho = 1.5,
  ftol = 1e-08,
  partol = 1e-08,
  abstol = 0,
  maxit = 1e+06,
  sparsity = FALSE,
  lam2 = 0,
  innerpartol = 1e-06,
  innermaxit = 1e+06,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ktd_estimate_+3A_x">x</code></td>
<td>
<p>Covariate matrix.</p>
</td></tr>
<tr><td><code id="ktd_estimate_+3A_y">y</code></td>
<td>
<p>Outcome vector (e.g. insurance cost).</p>
</td></tr>
<tr><td><code id="ktd_estimate_+3A_kern">kern</code></td>
<td>
<p>Choice of kernel. See <code><a href="#topic+dots">dots</a></code> for details on supported kernel functions.</p>
</td></tr>
<tr><td><code id="ktd_estimate_+3A_lam1">lam1</code></td>
<td>
<p>A vector of regularization coefficients.</p>
</td></tr>
<tr><td><code id="ktd_estimate_+3A_rho">rho</code></td>
<td>
<p>The power parameter of the Tweedie model. Default is 1.5 and can take any real value between 1 and 2.</p>
</td></tr>
<tr><td><code id="ktd_estimate_+3A_ftol">ftol</code></td>
<td>
<p>Stopping criterion based on objective function value. Default is 1e-8. See Details.</p>
</td></tr>
<tr><td><code id="ktd_estimate_+3A_partol">partol</code></td>
<td>
<p>Stopping criterion based on the coefficient values. Default is 1e-8. See Details.</p>
</td></tr>
<tr><td><code id="ktd_estimate_+3A_abstol">abstol</code></td>
<td>
<p>Stopping criterion based on absolute value of the objective function. Default is 0.</p>
</td></tr>
<tr><td><code id="ktd_estimate_+3A_maxit">maxit</code></td>
<td>
<p>Maximum number of iterations.</p>
</td></tr>
<tr><td><code id="ktd_estimate_+3A_sparsity">sparsity</code></td>
<td>
<p>Logical If true, the <code>sktweedie</code> model with variable selection will be used. Default is false, for the <code>ktweedie</code> model.</p>
</td></tr>
<tr><td><code id="ktd_estimate_+3A_lam2">lam2</code></td>
<td>
<p>Regularization coefficient for the sparsity-inducing penalty in the <code>sktweedie</code> model.</p>
</td></tr>
<tr><td><code id="ktd_estimate_+3A_innerpartol">innerpartol</code></td>
<td>
<p>Stopping criterion for the inner loops that update kernel parameters and weights based on the coefficient values. See Details.</p>
</td></tr>
<tr><td><code id="ktd_estimate_+3A_innermaxit">innermaxit</code></td>
<td>
<p>Maximum number of iterations for the inner loops that update kernel parameters and variable weights. See Details.</p>
</td></tr>
<tr><td><code id="ktd_estimate_+3A_verbose">verbose</code></td>
<td>
<p>Logical indicating whether to show details of each update.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>ktd_estimate()</code> stops when the absolute difference between the objective function values of the last two updates is smaller than <code>ftol</code>, or the sum of absolute differences between the coefficients of the last two updates is smaller than <code>partol</code>, or the objective function values is below <code>abstol</code>, before <code>maxit</code> is reached. For the <code>sktweedie</code> model, there are inner loops for the update of kernel regression coefficients and regularization weights. The <code>innerpartol</code> and <code>innermaxit</code> arguments are the counterparts of <code>partol</code> and <code>maxit</code> for the inner loops.
</p>


<h3>Value</h3>

<p>A list of three items.
</p>

<ol>
<li> <p><code>estimates</code>: a list containing the final objective function values and kernel Tweedie regression coefficients for each <code>lam1</code>.
</p>
</li>
<li> <p><code>data</code>: stores the inputs, including the predictor matrix, the kernel function used in the fitting and <code>lam1</code>.
</p>
</li>
<li> <p><code>sparsity</code>: a logical variable indicating whether the <code>ktweedie</code> or <code>sktweedie</code> is fitted.
</p>
</li></ol>



<h3>See Also</h3>

<p><code><a href="#topic+ktd_cv">ktd_cv</a></code>, <code><a href="#topic+ktd_cv2d">ktd_cv2d</a></code>, <code><a href="#topic+ktd_predict">ktd_predict</a></code>, <code><a href="#topic+rbfdot">rbfdot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>###### ktweedie ######
# Provide a sequence of candidate values to the argument lam1.
# Provide a kernel object to the argument kern.
lam1.seq &lt;- c(1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1)
fit.ktd &lt;- ktd_estimate(x = dat$x, y = dat$y,
                        kern = rbfdot(sigma = 1e-8),
                        lam1 = lam1.seq)
###### sktweedie ######
# Set sparsity to TRUE and a lam2 to control the level of sparsity
# Decrease lam2 if "WARNING: All weights are zero..."
fit.sktd &lt;- ktd_estimate(x = dat$x,
                         y = dat$y,
                         kern = rbfdot(sigma = 0.1),
                         lam1 = 5,
                         sparsity = TRUE,
                         lam2 = 1)
# variables with fitted weight equal to 0 are not selected

</code></pre>

<hr>
<h2 id='ktd_predict'>Predict outcome using fitted kernel Tweedie model</h2><span id='topic+ktd_predict'></span>

<h3>Description</h3>

<p><code>ktd_predict()</code> predicts the outcome with fitted <code>ktweedie</code> or <code>sktweedie</code> model at the user supplied new data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ktd_predict(model, newdata, which.lam1 = 1, type = "link")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ktd_predict_+3A_model">model</code></td>
<td>
<p>Fitted model from <code><a href="#topic+ktd_estimate">ktd_estimate</a></code></p>
</td></tr>
<tr><td><code id="ktd_predict_+3A_newdata">newdata</code></td>
<td>
<p>New x matrix for the prediction. If not provided, it will be the x matrix used to fit <code>model</code>.</p>
</td></tr>
<tr><td><code id="ktd_predict_+3A_which.lam1">which.lam1</code></td>
<td>
<p>The index of the <code>lam1</code> in <code>model</code> used in the prediction. Default is 1.</p>
</td></tr>
<tr><td><code id="ktd_predict_+3A_type">type</code></td>
<td>
<p>The type of prediction to be made - &quot;<code>link</code>&quot; for the linear predictor and &quot;<code>response</code>&quot; for the predicted outcome. Default is &quot;<code>link</code>&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>ktd_predict()</code> uses the fitted model from <code><a href="#topic+ktd_estimate">ktd_estimate</a></code> to estimate the mean outcome for new data points.
</p>


<h3>Value</h3>

<p>A list named <code>prediction</code> containing the vector of predicted outcomes.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ktd_estimate">ktd_estimate</a></code>, <code><a href="#topic+ktd_cv">ktd_cv</a></code>, <code><a href="#topic+ktd_cv2d">ktd_cv2d</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Fit a ktweedie model
fit &lt;- ktd_estimate(x = dat$x, y = dat$y,
                    kern = rbfdot(sigma = 1e-6),
                    lam1 = 10^(-5:1))
# Generate newx at which predictions are to be made.
# The newdata should have the same dimension as the original trainig data.
newx &lt;- matrix(rnorm(10 * ncol(dat$x)), nrow = 10)
pred &lt;- ktd_predict(model = fit, newdata = newx,
                    which.lam1 = 3, type = "link")
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
