<!DOCTYPE html><html><head><title>Help for package llama</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {llama}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#analysis'>
<p>Analysis functions</p></a></li>
<li><a href='#bsFolds'>
<p>Bootstrapping folds</p></a></li>
<li><a href='#classify'>
<p>Classification model</p></a></li>
<li><a href='#classifyPairs'>
<p>Classification model for pairs of algorithms</p></a></li>
<li><a href='#cluster'>
<p>Cluster model</p></a></li>
<li><a href='#cvFolds'>
<p>Cross-validation folds</p></a></li>
<li><a href='#helpers'>
<p>Helpers</p></a></li>
<li><a href='#imputeCensored'>
<p>Impute censored values</p></a></li>
<li><a href='#input'>
<p>Read data</p></a></li>
<li><a href='#llama-package'>
<p>Leveraging Learning to Automatically Manage Algorithms</p></a></li>
<li><a href='#misc'>
<p>Convenience functions</p></a></li>
<li><a href='#misclassificationPenalties'>
<p>Misclassification penalty</p></a></li>
<li><a href='#normalize'>
<p>Normalize features</p></a></li>
<li><a href='#parscores'>
<p>Penalized average runtime score</p></a></li>
<li><a href='#plot'>
<p>Plot convenience functions to visualise selectors</p></a></li>
<li><a href='#regression'>
<p>Regression model</p></a></li>
<li><a href='#regressionPairs'>
<p>Regression model for pairs of algorithms</p></a></li>
<li><a href='#satsolvers'>
<p>Example data for Leveraging Learning to Automatically Manage Algorithms</p></a></li>
<li><a href='#successes'>
<p>Success</p></a></li>
<li><a href='#trainTest'>
<p>Train / test split</p></a></li>
<li><a href='#tune'>
<p>Tune the hyperparameters of the machine learning algorithm underlying a model</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Leveraging Learning to Automatically Manage Algorithms</td>
</tr>
<tr>
<td>Version:</td>
<td>0.10.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2021-03-16</td>
</tr>
<tr>
<td>Author:</td>
<td>Lars Kotthoff [aut,cre],
    Bernd Bischl [aut],
    Barry Hurley [ctb],
    Talal Rahwan [ctb],
    Damir Pulatov [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Lars Kotthoff &lt;larsko@uwyo.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides functionality to train and evaluate algorithm selection models for portfolios.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0), mlr (&ge; 2.5)</td>
</tr>
<tr>
<td>Imports:</td>
<td>rJava, parallelMap, ggplot2, checkmate, BBmisc, plyr,
data.table</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, ParamHelpers</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/BSD-3-Clause">BSD_3_clause</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://bitbucket.org/lkotthoff/llama">https://bitbucket.org/lkotthoff/llama</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-03-16 18:47:39 UTC; larsko</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-03-16 22:40:12 UTC</td>
</tr>
</table>
<hr>
<h2 id='analysis'>
Analysis functions
</h2><span id='topic+contributions'></span>

<h3>Description</h3>

<p>Functions for analysing portfolios.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>contributions(data = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="analysis_+3A_data">data</code></td>
<td>

<p>the data to use. The structure returned by <code>input</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>contributions</code> analyses the marginal contributions of the algorithms in
the portfolio to its overall performance. More specifically, the Shapley value
for a specific algorithm is computed as the &quot;value&quot; of the portfolio with the
algorithm minus the &quot;value&quot; without the algorithm. This is done over all
possible portfolio compositions.
</p>
<p>It is automatically determined whether the performance value is to be minimised
or maximised.
</p>


<h3>Value</h3>

<p>A table listing the Shapley values for each algorithm in the portfolio.
The higher the value, the more the respective algorithm contributes to the
overall performance of the portfolio.
</p>


<h3>Author(s)</h3>

<p>Lars Kotthoff
</p>


<h3>References</h3>

<p>Rahwan, T., Michalak, T. (2013)
A Game Theoretic Approach to Measure Contributions in Algorithm Portfolios.
<em>Technical Report RR-13-11, University of Oxford</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(Sys.getenv("RUN_EXPENSIVE") == "true") {
data(satsolvers)

contributions(satsolvers)
}
</code></pre>

<hr>
<h2 id='bsFolds'>
Bootstrapping folds
</h2><span id='topic+bsFolds'></span>

<h3>Description</h3>

<p>Take data produced by <code>input</code> and amend it with (optionally)
stratified folds determined through bootstrapping.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bsFolds(data, nfolds = 10L, stratify = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bsFolds_+3A_data">data</code></td>
<td>

<p>the data to use. The structure returned by <code>input</code>.
</p>
</td></tr>
<tr><td><code id="bsFolds_+3A_nfolds">nfolds</code></td>
<td>

<p>the number of folds. Defaults to 10.
</p>
</td></tr>
<tr><td><code id="bsFolds_+3A_stratify">stratify</code></td>
<td>

<p>whether to stratify the folds. Makes really only sense for classification
models. Defaults to <code>FALSE</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Partitions the data set into folds. Stratification, if requested, is done by the
best algorithm, i.e. the one with the best performance. The distribution of the
best algorithms in each fold will be approximately the same. For each fold, the
training index set is assembled through .632 bootstrap. The remaining indices
are used for testing. There is no guarantee on the sizes of either sets. The
sets of indices are added to the original data set and returned.
</p>
<p>If the data set has train and test partitions already, they are overwritten.
</p>


<h3>Value</h3>

<table>
<tr><td><code>train</code></td>
<td>
<p>a list of index sets for training.</p>
</td></tr>
<tr><td><code>test</code></td>
<td>
<p>a list of index sets for testing.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>the original members of <code>data</code>. See <code><a href="#topic+input">input</a></code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lars Kotthoff
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cvFolds">cvFolds</a></code>, <code><a href="#topic+trainTest">trainTest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(satsolvers)
folds = bsFolds(satsolvers)

# use 5 folds instead of the default 10
folds5 = bsFolds(satsolvers, 5L)

# stratify
foldsU = bsFolds(satsolvers, stratify=TRUE)
</code></pre>

<hr>
<h2 id='classify'>
Classification model
</h2><span id='topic+classify'></span>

<h3>Description</h3>

<p>Build a classification model that predicts the algorithm to use based on the
features of the problem.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>classify(classifier = NULL, data = NULL,
    pre = function(x, y=NULL) { list(features=x) },
    save.models = NA, use.weights = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="classify_+3A_classifier">classifier</code></td>
<td>

<p>the mlr classifier to use. See examples.
</p>
<p>The argument can also be a list of such classifiers.
</p>
</td></tr>
<tr><td><code id="classify_+3A_data">data</code></td>
<td>

<p>the data to use with training and test sets. The structure returned by
one of the partitioning functions.
</p>
</td></tr>
<tr><td><code id="classify_+3A_pre">pre</code></td>
<td>

<p>a function to preprocess the data. Currently only <code>normalize</code>.
Optional. Does nothing by default.
</p>
</td></tr>
<tr><td><code id="classify_+3A_save.models">save.models</code></td>
<td>

<p>Whether to serialize and save the models trained during evaluation of the
model. If not <code>NA</code>, will be used as a prefix for the file name.
</p>
</td></tr>
<tr><td><code id="classify_+3A_use.weights">use.weights</code></td>
<td>

<p>Whether to use instance weights if supported. Default <code>TRUE</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>classify</code> takes the training and test sets in <code>data</code> and
processes it using <code>pre</code> (if supplied). <code>classifier</code> is called to
induce a classifier. The learned model is used to make predictions on the test
set(s).
</p>
<p>The evaluation across the training and test sets will be parallelized
automatically if a suitable backend for parallel computation is loaded.
The <code>parallelMap</code> level is &quot;llama.fold&quot;.
</p>
<p>If the given classifier supports case weights and <code>use.weights</code> is
<code>TRUE</code>, the performance difference between the best and the worst algorithm
is passed as a weight for each instance.
</p>
<p>If a list of classifiers is supplied in <code>classifier</code>, ensemble
classification is performed. That is, the models are trained and used to make
predictions independently. For each instance, the final prediction is determined
by majority vote of the predictions of the individual models &ndash; the class that
occurs most often is chosen. If the list given as <code>classifier</code> contains a
member <code>.combine</code> that is a function, it is assumed to be a classifier with
the same properties as the other ones and will be used to combine the ensemble
predictions instead of majority voting. This classifier is passed the original
features and the predictions of the classifiers in the ensemble.
</p>
<p>If the prediction of a stacked learner is <code>NA</code>, the prediction will be
<code>NA</code> for the score.
</p>
<p>If <code>save.models</code> is not <code>NA</code>, the models trained during evaluation are
serialized into files. Each file contains a list with members <code>model</code> (the
mlr model), <code>train.data</code> (the mlr task with the training data), and
<code>test.data</code> (the data frame with the test data used to make predictions).
The file name starts with <code>save.models</code>, followed by the ID of the machine
learning model, followed by &quot;combined&quot; if the model combines predictions of
other models, followed by the number of the fold. Each model for each fold is
saved in a different file.
</p>


<h3>Value</h3>

<table>
<tr><td><code>predictions</code></td>
<td>
<p>a data frame with the predictions for each instance and test
set. The columns of the data frame are the instance ID columns (as determined
by <code>input</code>), the algorithm, the score of the algorithm, and the iteration
(e.g. the number of the fold for cross-validation). More than one prediction
may be made for each instance and iteration. The score corresponds to the
number of classifiers that predicted the respective algorithm, or the sum of
probabilities that this classifier was the best. If stacking is used, the score
corresponds to the output of the stacked classifier.</p>
</td></tr>
<tr><td><code>predictor</code></td>
<td>
<p>a function that encapsulates the classifier learned on the
<em>entire</em> data set. Can be called with data for the same features with the
same feature names as the training data to obtain predictions in the same
format as the <code>predictions</code> member.</p>
</td></tr>
<tr><td><code>models</code></td>
<td>
<p>the list of models trained on the <em>entire</em> data set. This is
meant for debugging/inspection purposes and does not include any models used to
combine predictions of individual models.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lars Kotthoff
</p>


<h3>References</h3>

<p>Kotthoff, L., Miguel, I., Nightingale, P. (2010)
Ensemble Classification for Constraint Solver Configuration.
<em>16th International Conference on Principles and Practices of Constraint Programming</em>, 321&ndash;329.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+classifyPairs">classifyPairs</a></code>, <code><a href="#topic+cluster">cluster</a></code>, <code><a href="#topic+regression">regression</a></code>,
<code><a href="#topic+regressionPairs">regressionPairs</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(Sys.getenv("RUN_EXPENSIVE") == "true") {
data(satsolvers)
folds = cvFolds(satsolvers)

res = classify(classifier=makeLearner("classif.J48"), data=folds)
# the total number of successes
sum(successes(folds, res))
# predictions on the entire data set
res$predictor(satsolvers$data[satsolvers$features])

res = classify(classifier=makeLearner("classif.svm"), data=folds)

# use probabilities instead of labels
res = classify(classifier=makeLearner("classif.randomForest", predict.type = "prob"), data=folds)

# ensemble classification
rese = classify(classifier=list(makeLearner("classif.J48"),
                                makeLearner("classif.IBk"),
                                makeLearner("classif.svm")),
                data=folds)

# ensemble classification with a classifier to combine predictions
rese = classify(classifier=list(makeLearner("classif.J48"),
                                makeLearner("classif.IBk"),
                                makeLearner("classif.svm"),
                                .combine=makeLearner("classif.J48")),
                data=folds)
}
</code></pre>

<hr>
<h2 id='classifyPairs'>
Classification model for pairs of algorithms
</h2><span id='topic+classifyPairs'></span>

<h3>Description</h3>

<p>Build a classification model for each pair of algorithms that predicts which one
is better based on the features of the problem. Predictions are aggregated to
determine the best overall algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>classifyPairs(classifier = NULL, data = NULL,
    pre = function(x, y=NULL) { list(features=x) }, combine = NULL,
    save.models = NA, use.weights = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="classifyPairs_+3A_classifier">classifier</code></td>
<td>

<p>the mlr classifier to use. See examples.
</p>
</td></tr>
<tr><td><code id="classifyPairs_+3A_data">data</code></td>
<td>

<p>the data to use with training and test sets. The structure returned by
one of the partitioning functions.
</p>
</td></tr>
<tr><td><code id="classifyPairs_+3A_pre">pre</code></td>
<td>

<p>a function to preprocess the data. Currently only <code>normalize</code>.
Optional. Does nothing by default.
</p>
</td></tr>
<tr><td><code id="classifyPairs_+3A_combine">combine</code></td>
<td>

<p>The classifier function to predict the overall best algorithm given the
predictions for pairs of algorithms. Optional. By default, the overall best
algorithm is determined by majority vote.
</p>
</td></tr>
<tr><td><code id="classifyPairs_+3A_save.models">save.models</code></td>
<td>

<p>Whether to serialize and save the models trained during evaluation of the
model. If not <code>NA</code>, will be used as a prefix for the file name.
</p>
</td></tr>
<tr><td><code id="classifyPairs_+3A_use.weights">use.weights</code></td>
<td>

<p>Whether to use instance weights if supported. Default <code>TRUE</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>classifyPairs</code> takes the training and test sets in <code>data</code> and
processes it using <code>pre</code> (if supplied). <code>classifier</code> is called to
induce a classifier for each pair of algorithms to predict which one is better.
If <code>combine</code> is not supplied, the best overall algorithm is determined
by majority vote. If it is supplied, it is assumed to be a classifier with the
same properties as the other one. This classifier is passed the original
features and the predictions for each pair of algorithms.
</p>
<p>Which algorithm is better of a pair is determined by comparing their performance
scores. Whether a lower performance number is better or not is determined by
what was specified when the LLAMA data frame was created.
</p>
<p>The evaluation across the training and test sets will be parallelized
automatically if a suitable backend for parallel computation is loaded.
The <code>parallelMap</code> level is &quot;llama.fold&quot;.
</p>
<p>If the given classifier supports case weights and <code>use.weights</code> is
<code>TRUE</code>, the performance difference between the best and the worst algorithm
is passed as a weight for each instance.
</p>
<p>If all predictions of an underlying machine learning model are <code>NA</code>, it
will count as 0 towards the score.
</p>
<p>Training this model can take a very long time. Given <code>n</code> algorithms,
<code>choose(n, 2)</code> models are trained and evaluated. This is significantly
slower than the other approaches that train a single model or one for each
algorithm.
</p>
<p>If <code>save.models</code> is not <code>NA</code>, the models trained during evaluation are
serialized into files. Each file contains a list with members <code>model</code> (the
mlr model), <code>train.data</code> (the mlr task with the training data), and
<code>test.data</code> (the data frame with the test data used to make predictions).
The file name starts with <code>save.models</code>, followed by the ID of the machine
learning model, followed by &quot;combined&quot; if the model combines predictions of
other models, followed by the number of the fold. Each model for each fold is
saved in a different file.
</p>


<h3>Value</h3>

<table>
<tr><td><code>predictions</code></td>
<td>
<p>a data frame with the predictions for each instance and test
set. The columns of the data frame are the instance ID columns (as determined
by <code>input</code>), the algorithm, the score of the algorithm, and the iteration
(e.g. the number of the fold for cross-validation). More than one prediction
may be made for each instance and iteration. The score corresponds to the
number of times the respective algorithm was predicted to be better. If
stacking is used, only the best algorithm for each algorithm-instance pair is
predicted with a score of 1.</p>
</td></tr>
<tr><td><code>predictor</code></td>
<td>
<p>a function that encapsulates the classifier learned on the
<em>entire</em> data set. Can be called with data for the same features with the
same feature names as the training data to obtain predictions in the same
format as the <code>predictions</code> member.</p>
</td></tr>
<tr><td><code>models</code></td>
<td>
<p>the models for each pair of algorithms trained on the
<em>entire</em> data set. This is meant for debugging/inspection purposes and
does not include any models used to combine predictions of individual models.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lars Kotthoff
</p>


<h3>References</h3>

<p>Xu, L., Hutter, F., Hoos, H. H., Leyton-Brown, K. (2011)
Hydra-MIP: Automated Algorithm Configuration and Selection for Mixed Integer Programming.
<em>RCRA Workshop on Experimental Evaluation of Algorithms for Solving
Problems with Combinatorial Explosion</em>, 16&ndash;30.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+classify">classify</a></code>, <code><a href="#topic+cluster">cluster</a></code>, <code><a href="#topic+regression">regression</a></code>,
<code><a href="#topic+regressionPairs">regressionPairs</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(Sys.getenv("RUN_EXPENSIVE") == "true") {
data(satsolvers)
folds = cvFolds(satsolvers)

res = classifyPairs(classifier=makeLearner("classif.J48"), data=folds)
# the total number of successes
sum(successes(folds, res))
# predictions on the entire data set
res$predictor(satsolvers$data[satsolvers$features])

# use probabilities instead of labels
res = classifyPairs(classifier=makeLearner("classif.randomForest",
                                predict.type = "prob"), data=folds)

# combine predictions using J48 induced classifier instead of majority vote
res = classifyPairs(classifier=makeLearner("classif.J48"),
                    data=folds,
                    combine=makeLearner("classif.J48"))
}
</code></pre>

<hr>
<h2 id='cluster'>
Cluster model
</h2><span id='topic+cluster'></span>

<h3>Description</h3>

<p>Build a cluster model that predicts the algorithm to use based on the features
of the problem.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cluster(clusterer = NULL, data = NULL,
    bestBy = "performance",
    pre = function(x, y=NULL) { list(features=x) },
    save.models = NA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cluster_+3A_clusterer">clusterer</code></td>
<td>

<p>the mlr clustering function to use. See examples.
</p>
<p>The argument can also be a list of such functions.
</p>
</td></tr>
<tr><td><code id="cluster_+3A_data">data</code></td>
<td>

<p>the data to use with training and test sets. The structure returned by
one of the partitioning functions.
</p>
</td></tr>
<tr><td><code id="cluster_+3A_bestby">bestBy</code></td>
<td>

<p>the criteria by which to determine the best algorithm in a cluster. Can be one
of &quot;performance&quot;, &quot;count&quot;, &quot;successes&quot;. Optional. Defaults to &quot;performance&quot;.
</p>
</td></tr>
<tr><td><code id="cluster_+3A_pre">pre</code></td>
<td>

<p>a function to preprocess the data. Currently only <code>normalize</code>.
Optional. Does nothing by default.
</p>
</td></tr>
<tr><td><code id="cluster_+3A_save.models">save.models</code></td>
<td>

<p>Whether to serialize and save the models trained during evaluation of the
model. If not <code>NA</code>, will be used as a prefix for the file name.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>cluster</code> takes <code>data</code> and processes it using <code>pre</code> (if
supplied). <code>clusterer</code> is called to cluster the data. For each cluster, the
best algorithm is identified according to the criteria given in <code>bestBy</code>.
If <code>bestBy</code> is &quot;performance&quot;, the best algorithm is the one with the best
overall performance across all instances in the cluster. If it is &quot;count&quot;, the
best algorithm is the one that has the best performance most often. If it is
&quot;successes&quot;, the best algorithm is the one with the highest number of successes
across all instances in the cluster. The learned model is used to cluster the
test data and predict algorithms accordingly.
</p>
<p>The evaluation across the training and test sets will be parallelized
automatically if a suitable backend for parallel computation is loaded.
The <code>parallelMap</code> level is &quot;llama.fold&quot;.
</p>
<p>If a list of clusterers is supplied in <code>clusterer</code>, ensemble
clustering is performed. That is, the models are trained and used to make
predictions independently. For each instance, the final prediction is determined
by majority vote of the predictions of the individual models &ndash; the class that
occurs most often is chosen. If the list given as <code>clusterer</code> contains a
member <code>.combine</code> that is a function, it is assumed to be a classifier with
the same properties as classifiers given to <code>classify</code> and will be used to
combine the ensemble predictions instead of majority voting. This classifier is
passed the original features and the predictions of the classifiers in the
ensemble.
</p>
<p>If all predictions of an underlying machine learning model are <code>NA</code>, the
prediction will be <code>NA</code> for the algorithm and <code>-Inf</code> for the score if
the performance value is to be maximised, <code>Inf</code> otherwise.
</p>
<p>If <code>save.models</code> is not <code>NA</code>, the models trained during evaluation are
serialized into files. Each file contains a list with members <code>model</code> (the
mlr model), <code>train.data</code> (the mlr task with the training data), and
<code>test.data</code> (the data frame with the test data used to make predictions).
The file name starts with <code>save.models</code>, followed by the ID of the machine
learning model, followed by &quot;combined&quot; if the model combines predictions of
other models, followed by the number of the fold. Each model for each fold is
saved in a different file.
</p>


<h3>Value</h3>

<table>
<tr><td><code>predictions</code></td>
<td>
<p>a data frame with the predictions for each instance and test
set. The columns of the data frame are the instance ID columns (as determined
by <code>input</code>), the algorithm, the score of the algorithm, and the iteration
(e.g. the number of the fold for cross-validation). More than one prediction
may be made for each instance and iteration. The score corresponds to the
cumulative performance value for the algorithm of the cluster the instance was
assigned to. That is, if <code>bestBy</code> is &quot;performance&quot;, it is the sum of the
performance over all training instances. If <code>bestBy</code> is &quot;count&quot;, the score
corresponds to the number of training instances that the respective algorithm
was the best on, and if it is &quot;successes&quot; it corresponds to the number of
training instances where the algorithm was successful. If more than one
clustering algorithm is used, the score corresponds to the sum of all instances
across all clusterers. If stacking is used, the prediction is simply the best
algorithm with a score of 1.</p>
</td></tr>
<tr><td><code>predictor</code></td>
<td>
<p>a function that encapsulates the model learned on the
<em>entire</em> data set. Can be called with data for the same features with the
same feature names as the training data to obtain predictions in the same
format as the <code>predictions</code> member.</p>
</td></tr>
<tr><td><code>models</code></td>
<td>
<p>the list of models trained on the <em>entire</em> data set. This is
meant for debugging/inspection purposes and does not include any models used to
combine predictions of individual models.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lars Kotthoff
</p>


<h3>See Also</h3>

<p><code><a href="#topic+classify">classify</a></code>, <code><a href="#topic+classifyPairs">classifyPairs</a></code>, <code><a href="#topic+regression">regression</a></code>,
<code><a href="#topic+regressionPairs">regressionPairs</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(Sys.getenv("RUN_EXPENSIVE") == "true") {
data(satsolvers)
folds = cvFolds(satsolvers)

res = cluster(clusterer=makeLearner("cluster.XMeans"), data=folds, pre=normalize)
# the total number of successes
sum(successes(folds, res))
# predictions on the entire data set
res$predictor(satsolvers$data[satsolvers$features])

# determine best by number of successes
res = cluster(clusterer=makeLearner("cluster.XMeans"), data=folds,
    bestBy="successes", pre=normalize)
sum(successes(folds, res))

# ensemble clustering
rese = cluster(clusterer=list(makeLearner("cluster.XMeans"),
    makeLearner("cluster.SimpleKMeans"), makeLearner("cluster.EM")),
    data=folds, pre=normalize)

# ensemble clustering with a classifier to combine predictions
rese = cluster(clusterer=list(makeLearner("cluster.XMeans"),
    makeLearner("cluster.SimpleKMeans"), makeLearner("cluster.EM"),
    .combine=makeLearner("classif.J48")), data=folds, pre=normalize)
}
</code></pre>

<hr>
<h2 id='cvFolds'>
Cross-validation folds
</h2><span id='topic+cvFolds'></span>

<h3>Description</h3>

<p>Take data produced by <code>input</code> and amend it with (optionally)
stratified folds for cross-validation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cvFolds(data, nfolds = 10L, stratify = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cvFolds_+3A_data">data</code></td>
<td>

<p>the data to use. The structure returned by <code>input</code>.
</p>
</td></tr>
<tr><td><code id="cvFolds_+3A_nfolds">nfolds</code></td>
<td>

<p>the number of folds. Defaults to 10. If -1 is given, leave-one-out
cross-validation folds are produced.
</p>
</td></tr>
<tr><td><code id="cvFolds_+3A_stratify">stratify</code></td>
<td>

<p>whether to stratify the folds. Makes really only sense for classification
models. Defaults to <code>FALSE</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Partitions the data set into folds. Stratification, if requested, is done by the
best algorithm, i.e. the one with the best performance. The distribution of the
best algorithms in each fold will be approximately the same. The folds are
assembled into training and test sets by combining $n-1$ folds for training and
using the remaining fold for testing. The sets of indices are added to the
original data set and returned.
</p>
<p>If the data set has train and test partitions already, they are overwritten.
</p>


<h3>Value</h3>

<table>
<tr><td><code>train</code></td>
<td>
<p>a list of index sets for training.</p>
</td></tr>
<tr><td><code>test</code></td>
<td>
<p>a list of index sets for testing.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>the original members of <code>data</code>. See <code><a href="#topic+input">input</a></code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lars Kotthoff
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bsFolds">bsFolds</a></code>, <code><a href="#topic+trainTest">trainTest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(satsolvers)
folds = cvFolds(satsolvers)

# use 5 folds instead of the default 10
folds5 = cvFolds(satsolvers, 5L)

# stratify
foldsU = cvFolds(satsolvers, stratify=TRUE)
</code></pre>

<hr>
<h2 id='helpers'>
Helpers
</h2><span id='topic+print.llama.data'></span><span id='topic+print.llama.model'></span><span id='topic+makeRLearner.classif.constant'></span><span id='topic+predictLearner.classif.constant'></span><span id='topic+trainLearner.classif.constant'></span>

<h3>Description</h3>

<p>S3 helper methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'llama.data'
print(x, ...)
## S3 method for class 'llama.model'
print(x, ...)
## S3 method for class 'classif.constant'
makeRLearner()
## S3 method for class 'classif.constant'
predictLearner(.learner, .model, .newdata, ...)
## S3 method for class 'classif.constant'
trainLearner(.learner, .task, .subset, .weights, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="helpers_+3A_x">x</code></td>
<td>

<p>the object to print.
</p>
</td></tr>
<tr><td><code id="helpers_+3A_.learner">.learner</code></td>
<td>

<p>learner.
</p>
</td></tr>
<tr><td><code id="helpers_+3A_.model">.model</code></td>
<td>

<p>model.
</p>
</td></tr>
<tr><td><code id="helpers_+3A_.newdata">.newdata</code></td>
<td>

<p>new data.
</p>
</td></tr>
<tr><td><code id="helpers_+3A_.task">.task</code></td>
<td>

<p>task.
</p>
</td></tr>
<tr><td><code id="helpers_+3A_.subset">.subset</code></td>
<td>

<p>subset.
</p>
</td></tr>
<tr><td><code id="helpers_+3A_.weights">.weights</code></td>
<td>

<p>weights.
</p>
</td></tr>
<tr><td><code id="helpers_+3A_...">...</code></td>
<td>

<p>ignored.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lars Kotthoff
</p>

<hr>
<h2 id='imputeCensored'>
Impute censored values
</h2><span id='topic+imputeCensored'></span>

<h3>Description</h3>

<p>Impute the performance values that are censored, i.e. for which the respective
algorithm was not successful.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>imputeCensored(data = NULL, estimator = makeLearner("regr.lm"),
    epsilon = 0.1, maxit = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="imputeCensored_+3A_data">data</code></td>
<td>
<p>the data to check for censored values to impute. The structure
returned by <code>input</code>.</p>
</td></tr>
<tr><td><code id="imputeCensored_+3A_estimator">estimator</code></td>
<td>
<p>the mlr regressor to use to impute the censored values.</p>
</td></tr>
<tr><td><code id="imputeCensored_+3A_epsilon">epsilon</code></td>
<td>
<p>the convergence criterion. Default 0.1.</p>
</td></tr>
<tr><td><code id="imputeCensored_+3A_maxit">maxit</code></td>
<td>
<p>the maximum number of iterations. Default 1000.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function checks for each algorithm if there are censored values by checking
for which problem instances the algorithm was not successful. It trains a model
to predict the performance value for those instances using the given estimator
based on the performance values of the instances where the algorithm was
successful and the problem features. It then uses the results of this initial
prediction to train a new model on the entire data and predict the performance
values for those problems where the algorithm was successful again. This process
is repeated until the maximum difference between predictions in two successive
iterations is less than <code>epsilon</code> or more than <code>maxit</code> iterations have
been performed.
</p>
<p>It is up to the user to check whether the imputed values make sense. In
particular, for solver runtime data and timeouts one would expect that the
imputed values are above the timeout threshold, indicating at what time the
algorithms that have timed out would have solved the problem. No effort is made
to enforce such application-specific constraints.
</p>


<h3>Value</h3>

<p>The data structure with imputed censored values. The original data is saved in
the <code>original_data</code> member.
</p>


<h3>Author(s)</h3>

<p>Lars Kotthoff
</p>


<h3>References</h3>

<p>Josef Schmee and Gerald J. Hahn (1979)
A Simple Method for Regression Analysis with Censored Data.
<em>Technometrics</em> 21, no. 4, 417-432.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(Sys.getenv("RUN_EXPENSIVE") == "true") {
data(satsolvers)
imputed = imputeCensored(satsolvers)
}
</code></pre>

<hr>
<h2 id='input'>
Read data
</h2><span id='topic+input'></span>

<h3>Description</h3>

<p>Reads performance data that can be used to train and evaluate models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>input(features, performances, algorithmFeatures = NULL, successes = NULL, costs = NULL, 
		extra = NULL, minimize = T, perfcol = "performance")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="input_+3A_features">features</code></td>
<td>

<p>data frame that contains the feature values for each problem instance and a
non-empty set of ID columns.
</p>
</td></tr>
<tr><td><code id="input_+3A_algorithmfeatures">algorithmFeatures</code></td>
<td>

<p>data frame that contains the feature values for each algorithm and a
non-empty set of algorithm ID columns. Optional. 
</p>
</td></tr>
<tr><td><code id="input_+3A_performances">performances</code></td>
<td>

<p>data frame that contains the performance values for each problem instance and
a non-empty set of ID columns.
</p>
</td></tr>
<tr><td><code id="input_+3A_successes">successes</code></td>
<td>

<p>data frame that contains the success values (<code>TRUE</code>/<code>FALSE</code>) for
each algorithm on each problem instance and a non-empty set of ID columns. The
names of the columns in this data set should be the same as the names of the
columns in <code>performances</code>. Optional.
</p>
</td></tr>
<tr><td><code id="input_+3A_costs">costs</code></td>
<td>

<p>either a single number, a data frame or a list that specifies
the cost of the features. If a number is specified, it is assumed to denote
the cost for all problem instances (i.e. the cost is always the same). If a
data frame is given, it is assumed to have one column for each feature with
the same name as the feature where each value gives the cost and a non-empty
set of ID columns. If a list is specified, it is assumed to have a member
<code>groups</code> that specifies which features belong to which group and a member
<code>values</code> that is a data frame in the same format as before. Optional.
</p>
</td></tr>
<tr><td><code id="input_+3A_extra">extra</code></td>
<td>

<p>data frame containing any extra information about the instances and a
non-empty set of ID columns. This is not used in modelling, but can be used
e.g. for visualisation. Optional.
</p>
</td></tr>
<tr><td><code id="input_+3A_minimize">minimize</code></td>
<td>

<p>whether the minimum performance value is best. Default true.
</p>
</td></tr>
<tr><td><code id="input_+3A_perfcol">perfcol</code></td>
<td>

<p>name of the column that stores performance values when algorithm features 
are provided. Default performance.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>input</code> takes a list of data frames and processes them as follows. The
feature and performance data are joined by looking for common column names in
the two data frames (usually an ID of the problem instance). For each problem,
the best algorithm according to the given performance data is computed. If more
than one algorithm has the best performance, all of them are returned.
</p>
<p>The data frame for algorithmic features is optional. When it is provided, 
the existing data is joined by algorithm names. The final data frame is 
reshaped into 'long' format. 
</p>
<p>The data frame that describes whether an algorithm was successful on a problem
is optional. If <code>parscores</code> or <code>successes</code> are to be used to
evaluate the learned models, this argument is required however and will lead to
error messages if not supplied.
</p>
<p>Similarly, feature costs are optional.
</p>
<p>If <code>successes</code> is given, it is used to determine the best algorithm on each
problem instance. That is, an algorithm can only be best if it was successful.
If no algorithm was successful, the value will be <code>NA</code>. Special care should
be taken when preparing the performance values for unsuccessful algorithms. For
example, if the performance measure is runtime and success is determined by
whether the algorithm was able to find a solution within a timeout, the
performance value for unsuccessful algorithms should be the timeout value. If
the algorithm failed because of some other reason in a short amount of time,
specifying this small amount of time may confuse some of the algorithm selection
model learners.
</p>


<h3>Value</h3>

<table>
<tr><td><code>data</code></td>
<td>
<p>the combined data (features, performance, successes).</p>
</td></tr>
<tr><td><code>best</code></td>
<td>
<p>a list of the best algorithms.</p>
</td></tr>
<tr><td><code>ids</code></td>
<td>
<p>a list of names denoting the instance ID columns.</p>
</td></tr>
<tr><td><code>features</code></td>
<td>
<p>a list of names denoting problem features.</p>
</td></tr>
<tr><td><code>algorithmFeatures</code></td>
<td>
<p>a list of names denoting algorithm features. 'NULL' if no algorithm features are provided.</p>
</td></tr>
<tr><td><code>algorithmNames</code></td>
<td>
<p>a list of algorithm names. 'NULL' if no algorithm features are provided. See 'performance' field in that case.</p>
</td></tr>
<tr><td><code>algos</code></td>
<td>
<p>a column that stores names of algorithms. 'NULL' if no algorithm features are provided.</p>
</td></tr>
<tr><td><code>performance</code></td>
<td>
<p>a list of names denoting algorithm performances. If algorithm features are provided, 
a column name that stores algorithm performances. </p>
</td></tr>
<tr><td><code>success</code></td>
<td>
<p>a list of names denoting algorithm successes. If algorithm features are provided, 
a column name that stores algorithm successes. </p>
</td></tr>
<tr><td><code>minimize</code></td>
<td>
<p>true if the smaller performance values are better, else false.</p>
</td></tr>
<tr><td><code>cost</code></td>
<td>
<p>a list of names denoting feature costs.</p>
</td></tr>
<tr><td><code>costGroups</code></td>
<td>
<p>a list of list of names denoting which features belong to
which group. Only returned if cost groups are given as input.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lars Kotthoff
</p>


<h3>Examples</h3>

<pre><code class='language-R'># features.csv looks something like
# ID,width,height
# 0,1.2,3
# ...
# performance.csv:
# ID,alg1,alg2
# 0,2,5
# ...
# success.csv:
# ID,alg1,alg2
# 0,T,F
# ...
#input(read.csv("features.csv"), read.csv("performance.csv"),
#    read.csv("success.csv"), costs=10)

# costs.csv:
# ID,width,height
# 0,3,4.5
# ...
#input(read.csv("features.csv"), read.csv("performance.csv"),
#    read.csv("success.csv"), costs=read.csv("costs.csv"))

# costGroups.csv:
# ID,group1,group2
# 0,3,4.5
# ...
#input(read.csv("features.csv"), read.csv("performance.csv"),
#    read.csv("success.csv"),
#    costs=list(groups=list(group1=c("height"), group2=c("width")),
#               values=read.csv("costGroups.csv")))
</code></pre>

<hr>
<h2 id='llama-package'>
Leveraging Learning to Automatically Manage Algorithms
</h2><span id='topic+llama-package'></span><span id='topic+llama'></span>

<h3>Description</h3>

<p>Leveraging Learning to Automatically Manage Algorithms provides functionality to
read and process performance data for algorithms, facilitate building models
that predict which algorithm to use in which scenario and ways of evaluating
them.
</p>


<h3>Details</h3>

<p>The package provides functions to read performance data, build performance
models that enable selection of algorithms (using external machine learning
functions) and evaluate those models.
</p>
<p>Data is input using <code>input</code> and can then be used to learn
performance models. There are currently four main ways to create models.
Classification (<code>classify</code>) creates a single machine learning model
that predicts the algorithm to use as a label. Classification of pairs of
algorithms (<code>classifyPairs</code>) creates a classification model for each pair
of algorithms that predicts which one is better and aggregates these predictions
to determine the best overall algorithm. Clustering (<code>cluster</code>) clusters
the problems to solve and assigns the best algorithm to each cluster. Regression
(<code>regression</code>) trains a separate or single model (depending on the types of features available)
for all algorithms, predicts the performance on a problem independently and chooses the algorithm
with the best predicted performance. Regression of pairs of algorithms
(<code>regressionPairs</code>) is similar to <code>classifyPairs</code>, but predicts the
performance difference between each pair of algorithms. Similar to <code>regression</code>, 
<code>regressionPairs</code> can also build a single model for all pairs of algorithms, 
depending on the types of features available to the function. 
</p>
<p>Various functions to split the data into training and test set(s) and to
evaluate the performance of the learned models are provided.
</p>
<p>LLAMA uses the mlr package to access the implementation of machine learning
algorithms in R.
</p>
<p>The model building functions are using the <code>parallelMap</code> package to
parallelize across the data partitions (e.g. cross-validation folds) with level
&quot;llama.fold&quot; and &quot;llama.tune&quot; for tuning. By default, everything is run
sequentially. By loading a suitable backend (e.g. through
<code>parallelStartSocket(2)</code> for parallelization across 2 CPUs using sockets),
the model building will be parallelized automatically and transparently. Note
that this does <em>not</em> mean that all machine learning algorithms used for
building models can be parallelized safely. For functions that are not thread
safe, use <code>parallelStartSocket</code> to run in separate processes.
</p>


<h3>Author(s)</h3>

<p>Lars Kotthoff, Bernd Bischl
</p>
<p>contributions by Barry Hurley, Talal Rahwan, Damir Pulatov
</p>
<p>Maintainer: Lars Kotthoff &lt;larsko@uwyo.edu&gt;
</p>


<h3>References</h3>

<p>Kotthoff, L. (2013)
LLAMA: Leveraging Learning to Automatically Manage Algorithms.
<em>arXiv:1306.1031</em>.
</p>
<p>Kotthoff, L. (2014)
Algorithm Selection for Combinatorial Search Problems: A survey.
<em>AI Magazine</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(Sys.getenv("RUN_EXPENSIVE") == "true") {
data(satsolvers)
folds = cvFolds(satsolvers)

model = classify(classifier=makeLearner("classif.J48"), data=folds)
# print the total number of successes
print(sum(successes(folds, model)))
# print the total misclassification penalty
print(sum(misclassificationPenalties(folds, model)))
# print the total PAR10 score
print(sum(parscores(folds, model)))

# number of total successes for virtual best solver for comparison
print(sum(successes(satsolvers, vbs, addCosts = FALSE)))

# print predictions on the entire data set
print(model$predictor(subset(satsolvers$data, TRUE, satsolvers$features)))

# train a regression model
model = regression(regressor=makeLearner("regr.lm"), data=folds)
# print the total number of successes
print(sum(successes(folds, model)))
}
</code></pre>

<hr>
<h2 id='misc'>
Convenience functions
</h2><span id='topic+vbs'></span><span id='topic+singleBest'></span><span id='topic+singleBestByCount'></span><span id='topic+singleBestByPar'></span><span id='topic+singleBestBySuccesses'></span><span id='topic+predTable'></span>

<h3>Description</h3>

<p>Convenience functions for computing and working with predictions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vbs(data = NULL)
singleBest(data = NULL)
singleBestByCount(data = NULL)
singleBestByPar(data = NULL, factor = 10)
singleBestBySuccesses(data = NULL)
predTable(predictions = NULL, bestOnly = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="misc_+3A_data">data</code></td>
<td>

<p>the data to use. The structure returned by <code>input</code>.
</p>
</td></tr>
<tr><td><code id="misc_+3A_factor">factor</code></td>
<td>

<p>the penalization factor to use for non-successful choices. Default 10.
</p>
</td></tr>
<tr><td><code id="misc_+3A_predictions">predictions</code></td>
<td>

<p>the list of predictions.
</p>
</td></tr>
<tr><td><code id="misc_+3A_bestonly">bestOnly</code></td>
<td>

<p>whether to tabulate only the respective best algorithm for each instance.
Default <code>TRUE</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>vbs</code> and <code>singleBest</code> take a data frame of input data and return
predictions that correspond to the virtual best and the single best algorithm,
respectively. The virtual best picks the best algorithm for each instance. If no
algorithm solved in the instance, <code>NA</code> is returned. The single best picks
the algorithm that has the best cumulative performance over the entire data set.
</p>
<p><code>singleBestByCount</code> returns the algorithm that has the best performance
the highest number of times over the entire data set. Only whether or not an
algorithm is the best matters for this, not the difference to other algorithms.
</p>
<p><code>singleBestByPar</code> aggregates the PAR score over the entire data set and
returns the algorithm with the lowest overall PAR score.
<code>singleBestBySuccesses</code> counts the number of successes over the data set
and returns the algorithm with the highest overall number of successes.
</p>
<p><code>predTable</code> tabulates the predicted algorithms in the same way that
<code>table</code> does. If <code>bestOnly</code> is <code>FALSE</code>, all algorithms are
considered &ndash; for example for regression models, predictions are made for all
algorithms, so the table will simply show the number of instances for each
algorithm. Set <code>bestOnly</code> to <code>TRUE</code> to tabulate only the best
algorithm for each instance.
</p>


<h3>Value</h3>

<p>A data frame with the predictions for each instance. The columns of the data
frame are the instance ID columns (as determined by <code>input</code>), the
algorithm, the score of the algorithm, and the iteration (always 1). The score
is 1 if the respective algorithm is chosen for the instance, 0 otherwise. More
than one prediction may be made for each instance and iteration.
</p>
<p>For <code>predTable</code>, a table.
</p>


<h3>Author(s)</h3>

<p>Lars Kotthoff
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(Sys.getenv("RUN_EXPENSIVE") == "true") {
data(satsolvers)

# number of total successes for virtual best solver
print(sum(successes(satsolvers, vbs)))
# number of total successes for single best solver by count
print(sum(successes(satsolvers, singleBestByCount)))

# sum of PAR10 scores for single best solver by PAR10 score
print(sum(parscores(satsolvers, singleBestByPar)))

# number of total successes for single best solver by successes
print(sum(successes(satsolvers, singleBestBySuccesses)))

# print a table of the best solvers per instance
print(predTable(vbs(satsolvers)))
}
</code></pre>

<hr>
<h2 id='misclassificationPenalties'>
Misclassification penalty
</h2><span id='topic+misclassificationPenalties'></span>

<h3>Description</h3>

<p>Calculates the penalty incurred because of making incorrect decisions, i.e.
choosing suboptimal algorithms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>misclassificationPenalties(data, model, addCosts = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="misclassificationPenalties_+3A_data">data</code></td>
<td>
<p>the data used to induce the model. The same as given to
<code>classify</code>, <code>classifyPairs</code>, <code>cluster</code> or
<code>regression</code>.</p>
</td></tr>
<tr><td><code id="misclassificationPenalties_+3A_model">model</code></td>
<td>
<p>the algorithm selection model. Can be either a model
returned by one of the model-building functions or a function that returns
predictions such as <code>vbs</code> or the predictor function of a trained
model.</p>
</td></tr>
<tr><td><code id="misclassificationPenalties_+3A_addcosts">addCosts</code></td>
<td>
<p>does nothing. Only here for compatibility with the other
evaluation functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Compares the performance of the respective chosen algorithm to the performance
of the best algorithm for each datum. Returns the absolute difference. This
denotes the penalty for choosing a suboptimal algorithm, e.g. the additional
time required to solve a problem or reduction in solution quality incurred. The
misclassification penalty of the virtual best is always zero.
</p>
<p>If the model returns <code>NA</code> (e.g. because no algorithm solved the instance),
<code>0</code> is returned as misclassification penalty.
</p>
<p><code>data</code> may contain a train/test partition or not. This makes a difference
when computing the misclassification penalties for the single best algorithm.
If no train/test split is present, the single best algorithm is determined on
the entire data. If it is present, the single best algorithm is determined on
each test partition. That is, the single best is local to the partition and may
vary across partitions.
</p>


<h3>Value</h3>

<p>A list of the misclassification penalties.
</p>


<h3>Author(s)</h3>

<p>Lars Kotthoff
</p>


<h3>See Also</h3>

<p><code><a href="#topic+parscores">parscores</a></code>, <code><a href="#topic+successes">successes</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(Sys.getenv("RUN_EXPENSIVE") == "true") {
data(satsolvers)
folds = cvFolds(satsolvers)

model = classify(classifier=makeLearner("classif.J48"), data=folds)
sum(misclassificationPenalties(folds, model))
}
</code></pre>

<hr>
<h2 id='normalize'>
Normalize features
</h2><span id='topic+normalize'></span>

<h3>Description</h3>

<p>Normalize input data so that the values for all features cover the same range -1
to 1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>normalize(rawfeatures, meta = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="normalize_+3A_rawfeatures">rawfeatures</code></td>
<td>

<p>data frame with the feature values to normalize.
</p>
</td></tr>
<tr><td><code id="normalize_+3A_meta">meta</code></td>
<td>

<p>meta data to use for the normalization. If supplied should be a list with
members <code>minValues</code> that contains the minimum values for all features and
<code>maxValues</code> that contains the maximum values for all features. Will be
computed if not supplied.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>normalize</code> subtracts the minimum (supplied or computed) from all values of
a feature, divides by the difference between maximum and minimum, multiplies by
2 and subtracts 1. The range of the values for all features will be -1 to 1.
</p>


<h3>Value</h3>

<table>
<tr><td><code>features</code></td>
<td>
<p>the normalized feature vectors.</p>
</td></tr>
<tr><td><code>meta</code></td>
<td>
<p>the minimum and maximum values for each feature before
normalization. Can be used in subsequent calls to <code>normalize</code> for new
data.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lars Kotthoff
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(Sys.getenv("RUN_EXPENSIVE") == "true") {
data(satsolvers)
folds = cvFolds(satsolvers)

cluster(clusterer=makeLearner("cluster.XMeans"), data=folds, pre=normalize)
}
</code></pre>

<hr>
<h2 id='parscores'>
Penalized average runtime score
</h2><span id='topic+parscores'></span>

<h3>Description</h3>

<p>Calculates the penalized average runtime score which is commonly used for
evaluating satisfiability solvers on a set of problems.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>parscores(data, model, factor = 10, timeout, addCosts = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="parscores_+3A_data">data</code></td>
<td>
<p>the data used to induce the model. The same as given to
<code>classify</code>, <code>classifyPairs</code>, <code>cluster</code> or
<code>regression</code>.</p>
</td></tr>
<tr><td><code id="parscores_+3A_model">model</code></td>
<td>
<p>the algorithm selection model. Can be either a model
returned by one of the model-building functions or a function that returns
predictions such as <code>vbs</code> or the predictor function of a trained
model.</p>
</td></tr>
<tr><td><code id="parscores_+3A_factor">factor</code></td>
<td>
<p>the penalization factor to use for non-successful choices.
Default 10.</p>
</td></tr>
<tr><td><code id="parscores_+3A_timeout">timeout</code></td>
<td>
<p>the timeout value to be multiplied by the penalization factor.
If not specified, the maximum performance value of all algorithms on the
entire data is used.</p>
</td></tr>
<tr><td><code id="parscores_+3A_addcosts">addCosts</code></td>
<td>
<p>whether to add feature costs. You should not need to set this
manually, the default of <code>NULL</code> will have LLAMA figure out
automatically depending on the model whether to add costs or not. This
should always be true (the default) except for comparison algorithms (i.e.
single best and virtual best).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Returns the penalized average runtime performances of the respective chosen
algorithm on each problem instance.
</p>
<p>If feature costs have been given and <code>addCosts</code> is <code>TRUE</code>, the cost of
the used features or feature groups is added to the performance of the chosen
algorithm. The used features are determined by examining the the <code>features</code>
member of <code>data</code>, not the model. If after that the performance value is
above the timeout value, the timeout value multiplied by the factor is assumed.
</p>
<p>If the model returns <code>NA</code> (e.g. because no algorithm solved the instance),
<code>timeout * factor</code> is returned as PAR score.
</p>
<p><code>data</code> may contain a train/test partition or not. This makes a difference
when computing the PAR scores for the single best algorithm. If no train/test
split is present, the single best algorithm is determined on the entire data. If
it is present, the single best algorithm is determined on each test partition.
That is, the single best is local to the partition and may vary across
partitions.
</p>


<h3>Value</h3>

<p>A list of the penalized average runtimes.
</p>


<h3>Author(s)</h3>

<p>Lars Kotthoff
</p>


<h3>See Also</h3>

<p><code><a href="#topic+misclassificationPenalties">misclassificationPenalties</a></code>, <code><a href="#topic+successes">successes</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(Sys.getenv("RUN_EXPENSIVE") == "true") {
data(satsolvers)
folds = cvFolds(satsolvers)

model = classify(classifier=makeLearner("classif.J48"), data=folds)
sum(parscores(folds, model))

# use factor of 5 instead of 10.
sum(parscores(folds, model, 5))

# explicitly specify timeout.
sum(parscores(folds, model, timeout = 3600))
}
</code></pre>

<hr>
<h2 id='plot'>
Plot convenience functions to visualise selectors
</h2><span id='topic+perfScatterPlot'></span>

<h3>Description</h3>

<p>Functions to plot the performance of selectors and compare them to others.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>perfScatterPlot(metric, modelx, modely, datax, datay=datax,
    addCostsx=NULL, addCostsy=NULL, pargs=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_+3A_metric">metric</code></td>
<td>
<p>the metric used to evaluate the model. Can be one of
<code>misclassificationPenalties</code>, <code>parscores</code> or <code>successes</code>.</p>
</td></tr>
<tr><td><code id="plot_+3A_modelx">modelx</code></td>
<td>
<p>the algorithm selection model to be plotted on the x axis. Can
be either a model returned by one of the model-building functions or a
function that returns predictions such as <code>vbs</code> or the predictor
function of a trained model.</p>
</td></tr>
<tr><td><code id="plot_+3A_modely">modely</code></td>
<td>
<p>the algorithm selection model to be plotted on the y axis. Can
be either a model returned by one of the model-building functions or a
function that returns predictions such as <code>vbs</code> or the predictor
function of a trained model.</p>
</td></tr>
<tr><td><code id="plot_+3A_datax">datax</code></td>
<td>
<p>the data used to evaluate <code>modelx</code>. Will be passed to the
<code>metric</code> function.</p>
</td></tr>
<tr><td><code id="plot_+3A_datay">datay</code></td>
<td>
<p>the data used to evaluate <code>modely</code>. Can be omitted if the
same as for <code>modelx</code>. Will be passed to the <code>metric</code> function.</p>
</td></tr>
<tr><td><code id="plot_+3A_addcostsx">addCostsx</code></td>
<td>
<p>whether to add feature costs for <code>modelx</code>. You should
not need to set this manually, the default of <code>NULL</code> will have LLAMA
figure out automatically depending on the model whether to add costs or
not. This should always be true (the default) except for comparison
algorithms (i.e. single best and virtual best).</p>
</td></tr>
<tr><td><code id="plot_+3A_addcostsy">addCostsy</code></td>
<td>
<p>whether to add feature costs for <code>modely</code>. You should
not need to set this manually, the default of <code>NULL</code> will have LLAMA
figure out automatically depending on the model whether to add costs or
not. This should always be true (the default) except for comparison
algorithms (i.e. single best and virtual best).</p>
</td></tr>
<tr><td><code id="plot_+3A_pargs">pargs</code></td>
<td>
<p>any arguments to be passed to <code>geom_points</code>.</p>
</td></tr>
<tr><td><code id="plot_+3A_...">...</code></td>
<td>
<p>any additional arguments to be passed to the metrics. For example
the penalisation factor for <code>parscores</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>perfScatterPlot</code> creates a scatter plot that compares the performances of
two algorithm selectors. It plots the performance on each instance in the data
set for <code>modelx</code> on the x axis versus <code>modely</code> on the y axis. In
addition, a diagonal line is drawn to denote the line of equal performance for
both selectors.
</p>


<h3>Value</h3>

<p>A <code>ggplot</code> object.
</p>


<h3>Author(s)</h3>

<p>Lars Kotthoff
</p>


<h3>See Also</h3>

<p><code><a href="#topic+misclassificationPenalties">misclassificationPenalties</a></code>, <code><a href="#topic+parscores">parscores</a></code>, <code><a href="#topic+successes">successes</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(Sys.getenv("RUN_EXPENSIVE") == "true") {
data(satsolvers)
folds = cvFolds(satsolvers)
model = classify(classifier=makeLearner("classif.J48"), data=folds)

# Simple plot to compare our selector to the single best in terms of PAR10 score
library(ggplot2)
perfScatterPlot(parscores,
        model, singleBest,
        folds, satsolvers) +
    scale_x_log10() + scale_y_log10() +
    xlab("J48") + ylab("single best")

# additional aesthetics for points
perfScatterPlot(parscores,
        model, singleBest,
        folds, satsolvers,
        pargs=aes(colour = scorex)) +
    scale_x_log10() + scale_y_log10() +
    xlab("J48") + ylab("single best")
}
</code></pre>

<hr>
<h2 id='regression'>
Regression model
</h2><span id='topic+regression'></span>

<h3>Description</h3>

<p>Build a regression model that predicts the algorithm to use based on the
features of the problem and optionally features of the algorithms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regression(regressor = NULL, data = NULL,
    pre = function(x, y=NULL) { list(features=x) },
    combine = NULL, expand = identity, save.models = NA,
    use.weights = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="regression_+3A_regressor">regressor</code></td>
<td>

<p>the mlr regressor to use. See examples.
</p>
</td></tr>
<tr><td><code id="regression_+3A_data">data</code></td>
<td>

<p>the data to use with training and test sets. The structure returned by
one of the partitioning functions.
</p>
</td></tr>
<tr><td><code id="regression_+3A_pre">pre</code></td>
<td>

<p>a function to preprocess the data. Currently only <code>normalize</code>.
Optional. Does nothing by default.
</p>
</td></tr>
<tr><td><code id="regression_+3A_combine">combine</code></td>
<td>

<p>the function used to combine the predictions of the individual regression
models for stacking. Default <code>NULL</code>. See details.
</p>
</td></tr>
<tr><td><code id="regression_+3A_expand">expand</code></td>
<td>

<p>a function that takes a matrix of performance predictions (columns are
algorithms, rows problem instances) and transforms it into a matrix with the
same number of rows. Only meaningful if <code>combine</code> is not null. Default is
the identity function, which will leave the matrix unchanged. See examples.
</p>
</td></tr>
<tr><td><code id="regression_+3A_save.models">save.models</code></td>
<td>

<p>Whether to serialize and save the models trained during evaluation of the
model. If not <code>NA</code>, will be used as a prefix for the file name.
</p>
</td></tr>
<tr><td><code id="regression_+3A_use.weights">use.weights</code></td>
<td>

<p>Whether to use instance weights if supported. Default <code>TRUE</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>regression</code> takes <code>data</code> and processes it using <code>pre</code> (if
supplied). If no algorithm features are provided, <code>regressor</code> is called to induce separate regression models for
each of the algorithms to predict its performance. When algorithm features are present, 
<code>regressor</code> is called to induce one regression model for all algorithms to predict their performance.
The best algorithm is
determined from the predicted performances by examining whether performance is
to be minimized or not, as specified when creating the data structure through
<code>input</code>.
</p>
<p>The evaluation across the training and test sets will be parallelized
automatically if a suitable backend for parallel computation is loaded.
The <code>parallelMap</code> level is &quot;llama.fold&quot;.
</p>
<p>If <code>combine</code> is not null, it is assumed to be an mlr classifier and will be
used to learn a model to predict the best algorithm given the original features
and the performance predictions for the individual algorithms. <code>combine</code> option
is currently not supported with algorithm features. If this
classifier supports weights and <code>use.weights</code> is <code>TRUE</code>, they will be
passed as the difference between the best and the worst algorithm. Optionally,
<code>expand</code> can be used to supply a function that will modify the predictions
before giving them to the classifier, e.g. augment the performance predictions
with the pairwise differences (see examples).
</p>
<p>If all predictions of an underlying machine learning model are <code>NA</code>, the
prediction will be <code>NA</code> for the algorithm and <code>-Inf</code> for the score if
the performance value is to be maximised, <code>Inf</code> otherwise.
</p>
<p>If <code>save.models</code> is not <code>NA</code>, the models trained during evaluation are
serialized into files. Each file contains a list with members <code>model</code> (the
mlr model), <code>train.data</code> (the mlr task with the training data), and
<code>test.data</code> (the data frame with the test data used to make predictions).
The file name starts with <code>save.models</code>, followed by the ID of the machine
learning model, followed by &quot;combined&quot; if the model combines predictions of
other models, followed by the number of the fold. Each model for each fold is
saved in a different file.
</p>


<h3>Value</h3>

<table>
<tr><td><code>predictions</code></td>
<td>
<p>a data frame with the predictions for each instance and test
set. The columns of the data frame are the instance ID columns (as determined
by <code>input</code>), the algorithm, the score of the algorithm, and the iteration
(e.g. the number of the fold for cross-validation). More than one prediction
may be made for each instance and iteration. The score corresponds to the
predicted performance value. If stacking is used, each prediction is simply the
best algorithm with a score of 1.</p>
</td></tr>
<tr><td><code>predictor</code></td>
<td>
<p>a function that encapsulates the regression model learned on
the <em>entire</em> data set. Can be called with data for the same features with
the same feature names as the training data to obtain predictions in the same
format as the <code>predictions</code> member.</p>
</td></tr>
<tr><td><code>models</code></td>
<td>
<p>the list of models trained on the <em>entire</em> data set. This is
meant for debugging/inspection purposes and does not include any models used to
combine predictions of individual models.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lars Kotthoff
</p>


<h3>References</h3>

<p>Kotthoff, L. (2012)
Hybrid Regression-Classification Models for Algorithm Selection.
<em>20th European Conference on Artificial Intelligence</em>, 480&ndash;485.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+classify">classify</a></code>, <code><a href="#topic+classifyPairs">classifyPairs</a></code>, <code><a href="#topic+cluster">cluster</a></code>,
<code><a href="#topic+regressionPairs">regressionPairs</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(Sys.getenv("RUN_EXPENSIVE") == "true") {
data(satsolvers)
folds = cvFolds(satsolvers)

res = regression(regressor=makeLearner("regr.lm"), data=folds)
# the total number of successes
sum(successes(folds, res))
# predictions on the entire data set
res$predictor(satsolvers$data[satsolvers$features])

res = regression(regressor=makeLearner("regr.ksvm"), data=folds)

# combine performance predictions using classifier
ress = regression(regressor=makeLearner("regr.ksvm"),
                  data=folds,
                  combine=makeLearner("classif.J48"))

# add pairwise differences to performance predictions before running classifier
ress = regression(regressor=makeLearner("regr.ksvm"),
                  data=folds,
                  combine=makeLearner("classif.J48"),
                  expand=function(x) { cbind(x, combn(c(1:ncol(x)), 2,
                         function(y) { abs(x[,y[1]] - x[,y[2]]) })) })
}
</code></pre>

<hr>
<h2 id='regressionPairs'>
Regression model for pairs of algorithms
</h2><span id='topic+regressionPairs'></span>

<h3>Description</h3>

<p>Builds regression models for each pair of algorithms that predict the
performance difference based on the features of the problem and optionally features of the algorithms. 
The sum over all pairs that involve a particular algorithm is aggregated as the score of the
algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regressionPairs(regressor = NULL, data = NULL,
    pre = function(x, y=NULL) { list(features=x) }, combine = NULL,
    save.models = NA, use.weights = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="regressionPairs_+3A_regressor">regressor</code></td>
<td>

<p>the regression function to use. Must accept a formula of the values to predict
and a data frame with features. Return value should be a structure that can be
given to <code>predict</code> along with new data. See examples.
</p>
</td></tr>
<tr><td><code id="regressionPairs_+3A_data">data</code></td>
<td>

<p>the data to use with training and test sets. The structure returned by
one of the partitioning functions.
</p>
</td></tr>
<tr><td><code id="regressionPairs_+3A_pre">pre</code></td>
<td>

<p>a function to preprocess the data. Currently only <code>normalize</code>.
Optional. Does nothing by default.
</p>
</td></tr>
<tr><td><code id="regressionPairs_+3A_combine">combine</code></td>
<td>

<p>the function used to combine the predictions of the individual regression
models for stacking. Default <code>NULL</code>. See details.
</p>
</td></tr>
<tr><td><code id="regressionPairs_+3A_save.models">save.models</code></td>
<td>

<p>Whether to serialize and save the models trained during evaluation of the
model. If not <code>NA</code>, will be used as a prefix for the file name.
</p>
</td></tr>
<tr><td><code id="regressionPairs_+3A_use.weights">use.weights</code></td>
<td>

<p>Whether to use instance weights if supported. Default <code>TRUE</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>regressionPairs</code> takes the training and test sets in <code>data</code> and
processes it using <code>pre</code> (if supplied). If no algorithm features are provided, 
<code>regressor</code> is called to induce a regression model for each pair of 
algorithms to predict the performance difference between them. 
When algorithm features are present, <code>regressor</code> is called to
induce one regression model for all pairs of algorithms to predict the performance 
difference between them. If <code>combine</code> is not supplied, the best overall
algorithm is determined by summing the performance differences over all pairs
for each algorithm and ranking them by this sum. The algorithm with the largest
value is chosen. If it is supplied, it is assumed to be an mlr classifier. This
classifier is passed the original features and the predictions for each pair of
algorithms. <code>combine</code> option is currently not supported with algorithm features.
If the classifier supports weights and <code>use.weights</code> is
<code>TRUE</code>, the performance difference between the best and the worst algorithm
is passed as weight.
</p>
<p>The aggregated score for each algorithm quantifies how much better it is than
the other algorithms, where bigger values are better. Positive numbers denote
that the respective algorithm usually exhibits better performance than most of
the other algorithms, while negative numbers denote that it is usually worse.
</p>
<p>The evaluation across the training and test sets will be parallelized
automatically if a suitable backend for parallel computation is loaded.
The <code>parallelMap</code> level is &quot;llama.fold&quot;.
</p>
<p>Training this model can take a very long time. Given <code>n</code> algorithms,
<code>choose(n, 2) * n</code> models are trained and evaluated. This is significantly
slower than the other approaches that train a single model or one for each
algorithm. Even with algorithmic features present, when only a single model is trained, 
the process still takes a long time due to the amount of data. 
</p>
<p>If all predictions of an underlying machine learning model are <code>NA</code>, the
prediction will be <code>NA</code> for the algorithm and <code>-Inf</code> for the score if
the performance value is to be maximised, <code>Inf</code> otherwise.
</p>
<p>If <code>save.models</code> is not <code>NA</code>, the models trained during evaluation are
serialized into files. Each file contains a list with members <code>model</code> (the
mlr model), <code>train.data</code> (the mlr task with the training data), and
<code>test.data</code> (the data frame with the test data used to make predictions).
The file name starts with <code>save.models</code>, followed by the ID of the machine
learning model, followed by &quot;combined&quot; if the model combines predictions of
other models, followed by the number of the fold. Each model for each fold is
saved in a different file.
</p>


<h3>Value</h3>

<table>
<tr><td><code>predictions</code></td>
<td>
<p>a data frame with the predictions for each instance and test
set. The columns of the data frame are the instance ID columns (as determined
by <code>input</code>), the algorithm, the score of the algorithm, and the iteration
(e.g. the number of the fold for cross-validation). More than one prediction
may be made for each instance and iteration. The score corresponds to how much
better performance the algorithm delivers compared to the other algorithms in
the portfolio. If stacking is used, each prediction is simply the best
algorithm with a score of 1.</p>
</td></tr>
<tr><td><code>predictor</code></td>
<td>
<p>a function that encapsulates the classifier learned on the
<em>entire</em> data set. Can be called with data for the same features with the
same feature names as the training data to obtain predictions in the same
format as the <code>predictions</code> member.</p>
</td></tr>
<tr><td><code>models</code></td>
<td>
<p>the models for each pair of algorithms trained on the
<em>entire</em> data set. This is meant for debugging/inspection purposes and
does not include any models used to combine predictions of individual models.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lars Kotthoff
</p>


<h3>See Also</h3>

<p><code><a href="#topic+classify">classify</a></code>, <code><a href="#topic+classifyPairs">classifyPairs</a></code>, <code><a href="#topic+cluster">cluster</a></code>,
<code><a href="#topic+regression">regression</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(Sys.getenv("RUN_EXPENSIVE") == "true") {
data(satsolvers)
folds = cvFolds(satsolvers)

model = regressionPairs(regressor=makeLearner("regr.lm"), data=folds)
# the total number of successes
sum(successes(folds, model))
# predictions on the entire data set
model$predictor(satsolvers$data[satsolvers$features])

# combine predictions using J48 induced classifier
model = regressionPairs(regressor=makeLearner("regr.lm"), data=folds,
    combine=makeLearner("classif.J48"))
}
</code></pre>

<hr>
<h2 id='satsolvers'>
Example data for Leveraging Learning to Automatically Manage Algorithms
</h2><span id='topic+satsolvers'></span>

<h3>Description</h3>

<p>Performance data for 19 SAT solvers on 2433 SAT instances.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(satsolvers)</code></pre>


<h3>Format</h3>

<p><code>satsolvers</code> is a list in the format returned by <code>input</code> and
expected by the other functions of LLAMA. The list has the following components.
</p>

<dl>
<dt>data:</dt><dd><p>The original input data merged. That is, the data frames processed
by <code>input</code> in a single data frame with the following additional
columns.
</p>

<dl>
<dt>best:</dt><dd><p>The algorithm(s) with the best performance for each row.</p>
</dd>
<dt>*_success:</dt><dd><p>For each algorithm whether it was successful on the
respective row.</p>
</dd>
</dl>
</dd>
<dt>features:</dt><dd><p>The names of the columns that contain feature values.</p>
</dd>
<dt>performance:</dt><dd><p>The names of the columns that contain performance data.</p>
</dd>
<dt>success:</dt><dd><p>The names of the columns indicating whether an algorithm was
successful.</p>
</dd>
<dt>minimize:</dt><dd><p>Whether the performance is to be minimized.</p>
</dd>
<dt>cost:</dt><dd><p>The names of the columns that contain the feature group
computation cost for each instance.</p>
</dd>
<dt>costGroups:</dt><dd><p>A list the maps the names of the feature groups to the list
of feature names that are contained in it.</p>
</dd>
</dl>



<h3>Details</h3>

<p>Performance data for 19 SAT solvers on 2433 SAT instances. For each instance, 36
features were measured. In addition to the performance (time) on each instance,
data on whether a solver timed out on an instance is included. The cost to
compute all features is included as well.
</p>


<h3>Source</h3>

<p>Hurley, B., Kotthoff, L., Malitsky, Y., O'Sullivan, B. (2014)
Proteus: A Hierarchical Portfolio of Solvers and Transformations.
<em>Eleventh International Conference on Integration of Artificial
Intelligence (AI) and Operations Research (OR) techniques in Constraint
Programming</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+input">input</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(satsolvers)
</code></pre>

<hr>
<h2 id='successes'>
Success
</h2><span id='topic+successes'></span>

<h3>Description</h3>

<p>Was the problem solved successfully using the chosen algorithm?
</p>


<h3>Usage</h3>

<pre><code class='language-R'>successes(data, model, timeout, addCosts = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="successes_+3A_data">data</code></td>
<td>
<p>the data used to induce the model. The same as given to
<code>classify</code>, <code>classifyPairs</code>, <code>cluster</code> or
<code>regression</code>.</p>
</td></tr>
<tr><td><code id="successes_+3A_model">model</code></td>
<td>
<p>the algorithm selection model. Can be either a model
returned by one of the model-building functions or a function that returns
predictions such as <code>vbs</code> or the predictor function of a trained
model.</p>
</td></tr>
<tr><td><code id="successes_+3A_timeout">timeout</code></td>
<td>
<p>the timeout value to be multiplied by the penalization factor.
If not specified, the maximum performance value of all algorithms on the
entire data is used.</p>
</td></tr>
<tr><td><code id="successes_+3A_addcosts">addCosts</code></td>
<td>
<p>whether to add feature costs. You should not need to set this
manually, the default of <code>NULL</code> will have LLAMA figure out
automatically depending on the model whether to add costs or not. This
should always be true (the default) except for comparison algorithms (i.e.
single best and virtual best).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Returns <code>TRUE</code> if the chosen algorithm successfully solved the problem
instance, <code>FALSE</code> otherwise for each problem instance.
</p>
<p>If feature costs have been given and <code>addCosts</code> is <code>TRUE</code>, the cost of
the used features or feature groups is added to the performance of the chosen
algorithm. The used features are determined by examining the the <code>features</code>
member of <code>data</code>, not the model. If after that the performance value is
above the timeout value, <code>FALSE</code> is assumed. If whether an algorithm was
successful is not determined by performance and feature costs, don't pass costs
when creating the LLAMA data frame.
</p>
<p>If the model returns <code>NA</code> (e.g. because no algorithm solved the instance),
<code>FALSE</code> is returned as success.
</p>
<p><code>data</code> may contain a train/test partition or not. This makes a difference
when computing the successes for the single best algorithm. If no train/test
split is present, the single best algorithm is determined on the entire data. If
it is present, the single best algorithm is determined on each test partition.
That is, the single best is local to the partition and may vary across
partitions.
</p>


<h3>Value</h3>

<p>A list of the success values.
</p>


<h3>Author(s)</h3>

<p>Lars Kotthoff
</p>


<h3>See Also</h3>

<p><code><a href="#topic+misclassificationPenalties">misclassificationPenalties</a></code>, <code><a href="#topic+parscores">parscores</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(Sys.getenv("RUN_EXPENSIVE") == "true") {
data(satsolvers)
folds = cvFolds(satsolvers)

model = classify(classifier=makeLearner("classif.J48"), data=folds)
sum(successes(folds, model))
}
</code></pre>

<hr>
<h2 id='trainTest'>
Train / test split
</h2><span id='topic+trainTest'></span>

<h3>Description</h3>

<p>Split a data set into train and test set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trainTest(data, trainpart = 0.6, stratify = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="trainTest_+3A_data">data</code></td>
<td>

<p>the data to use. The structure returned by <code>input</code>.
</p>
</td></tr>
<tr><td><code id="trainTest_+3A_trainpart">trainpart</code></td>
<td>

<p>the fraction of the data to use for training. Default 0.6.
</p>
</td></tr>
<tr><td><code id="trainTest_+3A_stratify">stratify</code></td>
<td>

<p>whether to stratify the folds. Makes really only sense for classification
models. Defaults to <code>FALSE</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Partitions the data set into training and test set according to the specified
fraction. The training and test index sets are added to the original data and
returned. If requested, the distribution of the best algorithms in training and
test set is approximately the same, i.e. the sets are stratified.
</p>
<p>If the data set has train and test partitions already, they are overwritten.
</p>


<h3>Value</h3>

<table>
<tr><td><code>train</code></td>
<td>
<p>a (one-element) list of index sets for training.</p>
</td></tr>
<tr><td><code>test</code></td>
<td>
<p>a (one-element) list of index sets for testing.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>the original members of <code>data</code>. See <code><a href="#topic+input">input</a></code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Lars Kotthoff
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bsFolds">bsFolds</a></code>, <code><a href="#topic+cvFolds">cvFolds</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(satsolvers)
trainTest = trainTest(satsolvers)

# use 50-50 split instead of 60-40
trainTest1 = trainTest(satsolvers, 0.5)

# stratify
trainTestU = trainTest(satsolvers, stratify=TRUE)
</code></pre>

<hr>
<h2 id='tune'>
Tune the hyperparameters of the machine learning algorithm underlying a model
</h2><span id='topic+tuneModel'></span>

<h3>Description</h3>

<p>Functions to tune the hyperparameters of the machine learning algorithm
underlying a model with respect to a performance measure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tuneModel(ldf, llama.fun, learner, design, metric = parscores, nfolds = 10L,
    quiet = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tune_+3A_ldf">ldf</code></td>
<td>
<p>the LLAMA data to use. The structure returned by <code>input</code>.</p>
</td></tr>
<tr><td><code id="tune_+3A_llama.fun">llama.fun</code></td>
<td>
<p>the LLAMA model building function.</p>
</td></tr>
<tr><td><code id="tune_+3A_learner">learner</code></td>
<td>
<p>the mlr learner to use.</p>
</td></tr>
<tr><td><code id="tune_+3A_design">design</code></td>
<td>
<p>the data frame denoting the parameter values to try. Can be
produced with the <code>ParamHelpers</code> package. See examples.</p>
</td></tr>
<tr><td><code id="tune_+3A_metric">metric</code></td>
<td>
<p>the metric used to evaluate the model. Can be one of
<code>misclassificationPenalties</code>, <code>parscores</code> or <code>successes</code>.</p>
</td></tr>
<tr><td><code id="tune_+3A_nfolds">nfolds</code></td>
<td>
<p>the number of folds. Defaults to 10. If -1 is given,
leave-one-out cross-validation folds are produced.</p>
</td></tr>
<tr><td><code id="tune_+3A_quiet">quiet</code></td>
<td>
<p>whether to output information on the intermediate values and
progress during tuning.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>tuneModel</code> finds the hyperparameters from the set denoted by <code>design</code>
of the machine learning algorithm <code>learner</code> that give the best performance
with respect to the measure <code>metric</code> for the LLAMA model type
<code>llama.fun</code> on data <code>ldf</code>. It uses a nested cross-validation
internally; the number of inner folds is given through <code>nfolds</code>, the number
of outer folds is either determined by any existing partitions of <code>ldf</code> or,
if none are present, by <code>nfolds</code> as well.
</p>
<p>During each iteration of the inner cross-validation, all parameter sets
specified in <code>design</code> are evaluated and the one with the best performance
value chosen. The mean performance over all instances in the data is logged for
all evaluations. This parameter set is then used to build and evaluate a model
in the outer cross-validation. The predictions made by this model along with the
parameter values used to train it are returned.
</p>
<p>Finally, a normal (not-nested) cross-validation is performed to find the best
parameter values on the <em>entire</em> data set. The predictor of this model
along with the parameter values used to train it is returned. The interface
corresponds to the normal LLAMA model-building functions in that respect &ndash; the
returned data structure is the same with a few additional values.
</p>
<p>The evaluation across the folds sets will be parallelized automatically if a
suitable backend for parallel computation is loaded. The <code>parallelMap</code>
level is &quot;llama.tune&quot;.
</p>


<h3>Value</h3>

<table>
<tr><td><code>predictions</code></td>
<td>
<p>a data frame with the predictions for each instance and test
set. The structure is the same as for the underlying model building function
and the predictions are the ones made by the models trained with the best
parameter values for the respective fold.</p>
</td></tr>
<tr><td><code>predictor</code></td>
<td>
<p>a function that encapsulates the classifier learned on the
<em>entire</em> data set with the best parameter values determined on the
<em>entire</em> data set. Can be called with data for the same features with the
same feature names as the training data to obtain predictions in the same
format as the <code>predictions</code> member.</p>
</td></tr>
<tr><td><code>models</code></td>
<td>
<p>the list of models trained on the <em>entire</em> data set. This is
meant for debugging/inspection purposes.</p>
</td></tr>
<tr><td><code>parvals</code></td>
<td>
<p>the best parameter values on the entire data set used for
training the <code>predictor</code> model.</p>
</td></tr>
<tr><td><code>inner.parvals</code></td>
<td>
<p>the best parameter values during each iteration of the
outer cross-validation. These parameters were used to train the models that
made the predictions in <code>predictions</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Bernd Bischl, Lars Kotthoff
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(Sys.getenv("RUN_EXPENSIVE") == "true") {
library(ParamHelpers)
data(satsolvers)

learner = makeLearner("classif.J48")
# parameter set for J48
ps = makeParamSet(makeIntegerParam("M", lower = 1, upper = 100))
# generate 10 random parameter sets
design = generateRandomDesign(10, ps)
# tune with respect to PAR10 score (default) with 10 outer and inner folds
# (default)
res = tuneModel(satsolvers, classify, learner, design)
}
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
