<!DOCTYPE html><html lang="en"><head><title>Help for package ppclust</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ppclust}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ppclust-package'><p>Probabilistic and Possibilistic Cluster Analysis</p></a></li>
<li><a href='#as.ppclust'>
<p>Convert object to &lsquo;ppclust&rsquo; class</p></a></li>
<li><a href='#comp.omega'>
<p>Compute the possibilistic penalty argument for PCM</p></a></li>
<li><a href='#crisp'>
<p>Crisp the fuzzy membership degrees</p></a></li>
<li><a href='#ekm'>
<p>K-Means Clustering Using Different Seeding Techniques</p></a></li>
<li><a href='#fcm'>
<p>Fuzzy C-Means Clustering</p></a></li>
<li><a href='#fcm2'>
<p>Type-2 Fuzzy C-Means Clustering</p></a></li>
<li><a href='#fpcm'>
<p>Fuzzy Possibilistic C-Means Clustering</p></a></li>
<li><a href='#fpppcm'>
<p>Fuzzy Possibilistic Product Partition C-Means Clustering</p></a></li>
<li><a href='#get.dmetrics'>
<p>List the names of distance metrics</p></a></li>
<li><a href='#gg'><p>Gath-Geva Clustering Algorithm</p></a></li>
<li><a href='#gk'>
<p>Gustafson-Kessel Clustering</p></a></li>
<li><a href='#gkpfcm'>
<p>Gustafson-Kessel Clustering Using PFCM</p></a></li>
<li><a href='#hcm'>
<p>Hard C-Means Clustering</p></a></li>
<li><a href='#is.ppclust'>
<p>Check the class of object for &lsquo;ppclust&rsquo;</p></a></li>
<li><a href='#mfpcm'>
<p>Modified Fuzzy Possibilistic C-Means Clustering</p></a></li>
<li><a href='#pca'>
<p>Possibilistic Clustering Algorithm</p></a></li>
<li><a href='#pcm'>
<p>Possibilistic C-Means Clustering</p></a></li>
<li><a href='#pcmr'>
<p>Possibilistic C-Means Clustering with Repulsion</p></a></li>
<li><a href='#pfcm'>
<p>Possibilistic Fuzzy C-Means Clustering Algorithm</p></a></li>
<li><a href='#plotcluster'>
<p>Plot Clustering Results</p></a></li>
<li><a href='#ppclust2'>
<p>Convert &lsquo;ppclust&rsquo; objects to the other types of cluster objects</p></a></li>
<li><a href='#summary.ppclust'>
<p>Summarize the clustering results</p></a></li>
<li><a href='#upfc'>
<p>Unsupervised Possibilistic Fuzzy C-Means Clustering Algorithm</p></a></li>
<li><a href='#x12'>
<p>Synthetic data set of two variables</p></a></li>
<li><a href='#x16'>
<p>Synthetic data set of two variables forming two clusters</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Probabilistic and Possibilistic Cluster Analysis</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.0.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2020-02-08</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Author:</td>
<td>Zeynel Cebeci [aut, cre],
  Figen Yildiz [aut],
  Alper Tuna Kavlak [aut],
  Cagatay Cebeci [aut],
  Hasan Onder [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Zeynel Cebeci &lt;zcebeci@cukurova.edu.tr&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Partitioning clustering divides the objects in a data set into non-overlapping subsets or clusters by using the prototype-based probabilistic and possibilistic clustering algorithms. This package covers a set of the functions for Fuzzy C-Means (Bezdek, 1974) &lt;<a href="https://doi.org/10.1080%2F01969727308546047">doi:10.1080/01969727308546047</a>&gt;, Possibilistic C-Means (Krishnapuram &amp; Keller, 1993) &lt;<a href="https://doi.org/10.1109%2F91.227387">doi:10.1109/91.227387</a>&gt;, Possibilistic Fuzzy C-Means (Pal et al, 2005) &lt;<a href="https://doi.org/10.1109%2FTFUZZ.2004.840099">doi:10.1109/TFUZZ.2004.840099</a>&gt;, Possibilistic Clustering Algorithm (Yang et al, 2006) &lt;<a href="https://doi.org/10.1016%2Fj.patcog.2005.07.005">doi:10.1016/j.patcog.2005.07.005</a>&gt;, Possibilistic C-Means with Repulsion (Wachs et al, 2006) &lt;<a href="https://doi.org/10.1007%2F3-540-31662-0_6">doi:10.1007/3-540-31662-0_6</a>&gt; and the other variants of hard and soft clustering algorithms. The cluster prototypes and membership matrices required by these partitioning algorithms are initialized with different initialization techniques that are available in the package 'inaparc'. As the distance metrics, not only the Euclidean distance but also a set of the commonly used distance metrics are available to use with some of the algorithms in the package. </td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.3.0)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Imports:</td>
<td>graphics, grDevices, inaparc, MASS, stats, utils, methods</td>
</tr>
<tr>
<td>Suggests:</td>
<td>cluster, factoextra, fclust, knitr, rmarkdown, vegclust</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-12-13 17:18:57 UTC; hornik</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-12-13 17:27:57 UTC</td>
</tr>
</table>
<hr>
<h2 id='ppclust-package'>Probabilistic and Possibilistic Cluster Analysis</h2><span id='topic+ppclust-package'></span>

<h3>Description</h3>

<p>Partitioning clustering simply divides the objects in a data set into non-overlapping subsets or clusters by using the prototype-based probabilistic and possibilistic clustering algorithms. The package covers various functions for K-Means (MacQueen, 1967), Fuzzy C-Means (Bezdek, 1974), Possibilitic C-Means (Krishnapuram &amp; Keller, 1993;1996), Possibilistic and Fuzzy C-Means (Pal et al, 2005), Possibilistic Clustering Algorithm (Yang et al, 2006), Possibilistic C-Means with Repulsion (Wachs et al, 2006), Unsupervised Possibilistic Fuzzy C-Means (Wu et al, 2010) and the other variant algorithms which produce hard, fuzzy and possibilistic partitions of numeric data sets. The cluster prototypes and membership matrices required by the partitioning algorithms can be initialized with many initialization techniques that are available in the package &lsquo;<span class="pkg">inaparc</span>&rsquo;. As the distance metrics, not only the Euclidean distance but also a set of the commonly used distance metrics are available to use with some of the algorithms in the package.
</p>


<h3>Details</h3>

<p>The goal of prototype-based algorithms as the most widely-used group of partitioning clustering algorithms is to partition a data set of <em>n</em> objects with <em>p</em> features into <em>k</em>, a pre-defined number of clusters which are the non-hierchical subsets of data. On the clustering context, a prototype is a data item that represents or characterizes a cluster. Usually, a prototype can be regarded as the most central point in a data subspace (Tan et al. 2006). 
</p>
<p>Among the partitioning-based algorithms, the hard clustering algorithms, i.e. K-means, assume that each data point belongs to one cluster;  however, in practice clusters may overlap and data points may belong to more than one cluster.  In this case the membership degrees of a data point to clusters should be a value between zero and one. This idea has been modelled with the fuzzy clustering algorithms. The Fuzzy C-means proposed by Bezdek (FCM) (Bezdek, 1981), is the well-known partitioning based fuzzy algorithm. It assigns a fuzzy membership degree to each data point based on its distances to the cluster centers. If a data point is closer to a cluster center, its membership to this cluster be higher than its memberships to the other clusters. 
</p>
<p>As an extension of the basic FCM algorithm, Gustafson and Kessel (GK) clustering algorithm employs an adaptive distance norm in order to detect clusters with different geometrical shapes (Babuska, 2001; Correa et al, 2011). Gath and Geva (1989) revealed that the fuzzy maximum likelihood estimates (FMLE) clustering algorithm can be used to detect clusters of varying shapes, sizes and densities. 
</p>
<p>In the related literature it has been revealed that FCM is sensitive to noise and outliers in the data sets. Krishnapuram and Keller proposed and later improved the Possibilistic C-means (PCM) algorithm in order to avoid the effect of outliers or noises on the clustering performance (Krishnapuram &amp; Keller, 1993;1996). Althouh PCM solves the problems due to noise and outliers by relaxing the probabilistic constraint of FCM, it has the disadvantage to  generate coincident clusters with poor initializations. In order to overcome this problem, several variants of FCM and PCM have been proposed. Fuzzy Possibilistic C-means (FPCM) (Pal et al, 1997) is one of the mixed algorithms for simultaneously constructing memberships and typicalities. However FPCM has some problems because of the row sum constraint with the typicality values that produces unrealistic typicality values for large data sets. By adding a repulsion term forcing clusters far away from each other, an extension of PCM with repulsion has been introduced by Timm et al (2004). Pal et al (2005) proposed the Possibilistic Fuzzy C-means (PFCM) to overcome the noise sensitivity defect of FCM, to eliminate the coincident clusters problem of PCM and to eliminate the row sum constraints of FPCM.
</p>
<p>Possibilistic Clustering Algorithm (PCA) proposed by Yang and Wu (2006) was in the front lines of another direction of algorithms improving FCM and PCM. The authors argued that the resulting membership of their proposed algorithm becomes an exponential function, so that it is robust to noise and outliers. Recently, Wu et al (2010) introduced the Unsupervised Possibilistic Fuzzy Clustering (UPFC) algorithm. UPFC is an extension of PCA by combining FCM and PCA algorithms in order to overcome the noise sensitivity problem of FCM and the coincident clusters problem of PCM. When compared to previous algorithms, UPFC seems promising algorithm for clustering in fuzzy and possibilistic environments since it needs not a probabilistic initialization.
</p>


<h3>Basic Notations</h3>

<p>Given a data set <code class="reqn">\mathbf{X}</code> describing <code class="reqn">n</code> data objects, the probabilistic and possibilistic clustering algorithms partition data into <code class="reqn">k</code>, a predefined number of clusters through the minimization of their related objective functions with some probabilistic or possibilistic constraints. 
</p>
<p><code class="reqn">\mathbf{X} = \{\vec{x}_1, \vec{x}_2,\dots, \vec{x}_n\} \subseteq \Re^p</code> is the data set for <code class="reqn">n</code> objects in the <em>p</em>-dimensional data space <code class="reqn">\Re</code>, 
where:
</p>

<ul>
<li> <p><code class="reqn">n</code> is the number of objects in the data set, <code class="reqn">1\leq n\leq \infty</code> 
</p>
</li>
<li> <p><code class="reqn">p</code> is the number of features or variables which describes the data objects,
</p>
</li>
<li> <p><code class="reqn">\vec{x}_i</code> is <em>p</em>-length data vector for the i<em>th</em> data object. 
</p>
</li></ul>

<p>On the clustering context, clusters are mostly represented by their prototypes. The prototypes are generally the centers of clusters which can be either centroids or medoids. The probabilistic and possibilistic partitioning clustering algorithms start with initialization of a cluster prototype matrix <code class="reqn">\mathbf{V}</code>, and updates it through the iteration steps until it is stabilized. 
</p>
<p><code class="reqn">\mathbf{V} = \{\vec{v}_1, \vec{v}_2, \dots, \vec{v}_k\} \subseteq\Re^n</code> is the protoype matrix of the clusters, where:
</p>

<ul>
<li> <p><code class="reqn">k</code> is the number of clusters, <code class="reqn">1\leq k\leq n</code> 
</p>
</li>
<li> <p><code class="reqn">\vec{v}_j</code> is the <em>p</em>-length prototype vector for the j<em>th</em> cluster
</p>
</li></ul>
 
<p>The clustering algorithms compute the membership degrees of data objects by using some distance metrics for calculation of their proximities to the cluster centers. 
</p>
<p><code class="reqn">d(\vec{x}_i, \vec{v}_j)</code> is the distance measure between the data object <code class="reqn">\vec{x}_i</code> and cluster prototype <code class="reqn">\vec{v}_j</code>. In general, the squared Euclidean distance metric are used in most of the applications: 
</p>
<p><code class="reqn">d_{sq.euclidean}(\vec{x}_i, \vec{v}_j) = d^2(\vec{x}_i, \vec{v}_j) = \mid\mid \vec{x}_i - \vec{v}_j\mid \mid^2 \; = \; (\vec{x}_i - \vec{v}_j)^T \cdot (\vec{x}_i - \vec{v}_j)</code> 
</p>
<p>The clustering algorithms usually are run with the standard and squared Euclidean distance norms, which induce hyperspherical clusters. Therefore they are able find the clusters with the same shape and orientation because the norm inducing matrix is an identity matrix <code class="reqn">\mathbf{A} = \mathbf{I}</code>. On the other hand, the distance metrics can be employed with a <code class="reqn">n \times n</code> diagonal norm inducing matrix <code class="reqn">\mathbf{A} = \mathbf{I} \; 1/\sigma_j^2</code> which modifies the distances depending on the direction in which the distance is measured (Timm et al, 2004; Balasko et al 2005). In this case, the squared Euclidean distance with the norm matrix <code class="reqn">\mathbf{A}</code> becomes:
</p>
<p><code class="reqn">d_{sq.euclidean}(\vec{x}_i, \vec{v}_j) = d_{A}^2(\vec{x}_i, \vec{v}_j) = \mid\mid \vec{x}_i - \vec{v}_j\mid \mid_A^2 \; = \; (\vec{x}_i - \vec{v}_j)^T \mathbf{A} (\vec{x}_i - \vec{v}_j)</code> 
</p>
<p><code class="reqn">\mathbf{A}</code> can be also formed as the inverse of the <code class="reqn">n \times n</code> covariance matrix <code class="reqn">\mathbf{F}</code>: 
</p>
<p><code class="reqn">\mathbf{A} = \mathbf{F}^{-1}</code>, where:
</p>
<p><code class="reqn">\mathbf{F} = \frac{1}{n} \sum\limits_{i=1}^n (\vec{x}_i - \bar{x})^T (\vec{x}_i - \bar{x})</code>.  
</p>
<p>Where <code class="reqn">\bar{x}</code> stands for the sample mean of the data. When the distance is induced with the norm matrix <code class="reqn">\mathbf{A}</code> the Mahalanobis norm on <code class="reqn">\Re^n</code> can be written as follows: 
</p>
<p><code class="reqn">d_{mahalanobis}(\vec{x}_i, \vec{v}_j)_A = \; (\vec{x}_i - \vec{v}_j)^T \mathbf{A}_j (\vec{x}_i - \vec{v}_j)</code> 
</p>
<p>The membership degrees are the measures specifying the amount of belongingness of the data objects to the different clusters. A data point nearer to the center of a cluster has a higher degree of membership to this cluster. 
</p>
<p><code class="reqn">\mathbf{U} = [u_{ij}]</code> is the matrix for an hard, fuzzy or possibilistic partition of <code class="reqn">\mathbf{X}</code>, where:
</p>

<ul>
<li> <p><code class="reqn">u_{ij} = u_{i}(\vec{x}_{j})</code> is the membership degree of <code class="reqn">\vec{x}_i</code> to the j<em>th</em> cluster 
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Zeynel Cebeci, Figen Yildiz, A. Tuna Kavlak, Cagatay Cebeci &amp; Hasan Onder</p>


<h3>References</h3>

<p>Babuska, R. (2001). Fuzzy and neural control. DISC Course Lecture Notes. Delft University of Technology. Delft, the Netherlands. &lt;<a href="https://tr.scribd.com/document/209211977/Fuzzy-and-Neural-Control">https://tr.scribd.com/document/209211977/Fuzzy-and-Neural-Control</a>&gt;.
</p>
<p>Balasko, B., Abonyi, J. &amp; Feil, B. (2005). Fuzzy clustering and data analysis toolbox. Department of Process Eng., Univ. of Veszprem, Veszprem. 
</p>
<p>Bezdek, J.C. (1981). <em>Pattern recognition with fuzzy objective function algorithms</em>. Plenum, NY. &lt;<a href="https://isbnsearch.org/isbn/0306406713">ISBN:0306406713</a>&gt;
</p>
<p>Cebeci, Z. &amp; Yildiz, F. (2015). Bulanik C-Ortalamalar algoritmasýnýn farklý kume buyuklukleri icin hesaplama performansi ve kumeleme gecerliliginin karsilastirilmasi, In Proc.pf <em>9. Ulusal Zootekni Bilim Kongresi</em>, Sep. 2015, Konya. pp. 227-239. <a href="https://doi.org/10.13140/RG.2.1.2909.9288">doi:10.13140/RG.2.1.2909.9288</a>
</p>
<p>Cebeci, Z., Kavlak, A.T. &amp; Yildiz, F. (2017). Validation of fuzzy and possibilistic clustering results, in Proc. of <em>2017 Int. Artificial Intelligence &amp; Data Processing Symposium</em>, IEEE. pp. 1-7. <a href="https://doi.org/10.1109/IDAP.2017.8090183">doi:10.1109/IDAP.2017.8090183</a>
</p>
<p>Cebeci, Z. (2018). Initialization of membership degree matrix for fast convergence of Fuzzy C-Means clustering, in Proc. of <em>2018 Int.Conf.on Artificial Intelligence &amp; Data Processing</em>, pp. 1-5. IEEE. <a href="https://doi.org/10.1109/IDAP.2018.8620920">doi:10.1109/IDAP.2018.8620920</a>
</p>
<p>Cebeci, Z. &amp; Cebeci, C. (2018) kpeaks: an R package for quick selection of k for cluster analysis, in Proc. of <em>2018 Int. Conf. on Artificial Intelligence &amp; Data Processing</em>, pp. 1-7, IEEE. <a href="https://doi.org/10.1109/IDAP.2018.8620896">doi:10.1109/IDAP.2018.8620896</a>
</p>
<p>Cebeci, Z. (2019). Comparison of internal validity indices for fuzzy clustering. <em>Journal of Agricultural Informatics</em>, 10(2):1-14. <a href="https://doi.org/10.17700/jai.2019.10.2.537">doi:10.17700/jai.2019.10.2.537</a>
</p>
<p>Correa, C., Valero, C., Barreiro, P., Diago, M. P., &amp; Tardáguila, J. (2011). A comparison of fuzzy clustering algorithms applied to feature extraction on vineyard. In <em>Proc. of the 14th Conf. of the Spanish Assoc. for Artificial Intelligence</em>. &lt;<a href="http://oa.upm.es/9246/">http://oa.upm.es/9246/</a>&gt;.
</p>
<p>Gath, I. &amp; Geva, A.B. (1989). Unsupervised optimal fuzzy clustering. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 11 (7): 773-781. &lt;doi:10.1109/34.192473&gt;
</p>
<p>Gustafson, D. E. &amp; Kessel, W. C. (1979). Fuzzy clustering with a fuzzy covariance matrix. In <em>Proc. of IEEE Conf. on Decision and Control including the 17th Symposium on Adaptive Processes</em>, San Diego. pp. 761-766. &lt;doi:10.1109/CDC.1978.268028&gt;
</p>
<p>Pal, N.R., Pal, K., &amp; Bezdek, J.C. (1997). A mixed c-means clustering model. In <em>Proc. of the 6th IEEE Int. Conf. on Fuzzy Systems</em>, 1, pp. 11-21. &lt;doi:10.1109/FUZZY.1997.616338&gt;
</p>
<p>Pal, N.R., Pal, S.K., Keller,J.M. &amp; Bezdek, J.C. (2005). A possibilistic fuzzy c-means clustering algorithm. <em>IEEE Transactions on Fuzzy Systems</em>, 13 (4): 517-530. &lt;doi: 10.1109/TFUZZ.2004.840099&gt;
</p>
<p>Tan, P. N., Steinbach, M., &amp; Kumar, V. (2006). Cluster analysis: Basic concepts and algorithms. In <em>Introduction to Data Mining</em>. Pearson Addison Wesley. &lt;<a href="http://www-users.cs.umn.edu/~kumar/dmbook/ch8.pdf">http://www-users.cs.umn.edu/~kumar/dmbook/ch8.pdf</a>&gt;
</p>
<p>Timm, H., Borgelt, C., Doring, C. &amp; Kruse, R. (2004). An extension to possibilistic fuzzy cluster analysis. <em>Fuzzy Sets and Systems</em>, 147 (1): 3-16. &lt;doi:10.1016/j.fss.2003.11.009&gt;
</p>
<p>Yang, M. S. &amp; Wu, K. L. (2006). Unsupervised possibilistic clustering. <em>Pattern Recognition</em>, 39(1): 5-21. &lt;doi:10.1016/j.patcog.2005.07.005&gt;
</p>
<p>Wu, X., Wu, B., Sun, J. &amp; Fu, H. (2010). Unsupervised possibilistic fuzzy clustering. <em>J. of Information &amp; Computational Sci.</em>, 7 (5): 1075-1080.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+as.ppclust">as.ppclust</a></code>,
<code><a href="#topic+comp.omega">comp.omega</a></code>,
<code><a href="#topic+fcm">fcm</a></code>,
<code><a href="#topic+fcm2">fcm2</a></code>,
<code><a href="#topic+fpcm">fpcm</a></code>,
<code><a href="#topic+fpppcm">fpppcm</a></code>,
<code><a href="#topic+gg">gg</a></code>,
<code><a href="#topic+gk">gk</a></code>,
<code><a href="#topic+gkpfcm">gkpfcm</a></code>,
<code><a href="#topic+hcm">hcm</a></code>,
<code><a href="#topic+is.ppclust">is.ppclust</a></code>
<code><a href="#topic+pca">pca</a></code>,
<code><a href="#topic+ppclust2">ppclust2</a></code>
<code><a href="#topic+pcm">pcm</a></code>,
<code><a href="#topic+pcmr">pcmr</a></code>,
<code><a href="#topic+pfcm">pfcm</a></code>,
<code><a href="#topic+summary.ppclust">summary.ppclust</a></code>,
<code><a href="#topic+upfc">upfc</a></code>
</p>

<hr>
<h2 id='as.ppclust'>
Convert object to &lsquo;ppclust&rsquo; class
</h2><span id='topic+as.ppclust'></span>

<h3>Description</h3>

<p>Converts an object of the classes <code><a href="cluster.html#topic+fanny.object">fanny.object</a></code>, <code><a href="fclust.html#topic+summary.fclust">summary.fclust</a></code>, <code><a href="stats.html#topic+kmeans">kmeans</a></code> or <code><a href="vegclust.html#topic+vegclust">vegclust</a></code> to &lsquo;ppclust&rsquo; class.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.ppclust(objx, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="as.ppclust_+3A_objx">objx</code></td>
<td>
<p>an object to be converted to an instance of <code>ppclust</code> class.</p>
</td></tr>
<tr><td><code id="as.ppclust_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an object of <code>ppclust</code> class.</p>


<h3>Author(s)</h3>

<p>Zeynel Cebeci
</p>


<h3>See Also</h3>

<p><code><a href="#topic+is.ppclust">is.ppclust</a></code>,
<code><a href="#topic+ppclust2">ppclust2</a></code>,
<code><a href="#topic+summary.ppclust">summary.ppclust</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
# Create an fclust object
ofc &lt;- fclust::FKM(X=iris[,1:4], k=3)

# Test the class of object 'ofc' 
class(ofc)

# Convert 'ofc' to ppclust object
opc &lt;- as.ppclust(ofc)

# Test the class of 'opc' object
class(opc)
</code></pre>

<hr>
<h2 id='comp.omega'>
Compute the possibilistic penalty argument for PCM
</h2><span id='topic+comp.omega'></span>

<h3>Description</h3>

<p>Computes the vector <code class="reqn">\vec{\Omega}</code>, the possibilistic penalty argument for Possibilistic C-Means clustering analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>comp.omega(d, u, m=2, pco=NULL, K=1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="comp.omega_+3A_d">d</code></td>
<td>
<p>a numeric matrix containing the distances of objects to the centers of clusters.</p>
</td></tr>
<tr><td><code id="comp.omega_+3A_u">u</code></td>
<td>
<p>a numeric matrix containing the fuzzy or possibilistic membership degrees of the data objects.</p>
</td></tr>
<tr><td><code id="comp.omega_+3A_pco">pco</code></td>
<td>
<p>an object of &lsquo;ppclust&rsquo; class that contains the results from a clustering algorithm. If it is supplied, there is no need to input the arguments <code>d</code>, <code>u</code> and <code>m</code> separately because they are parsed from the object <code>pco</code> itself.</p>
</td></tr>
<tr><td><code id="comp.omega_+3A_m">m</code></td>
<td>
<p>a number for the fuzzy exponent. The default is 2.</p>
</td></tr>
<tr><td><code id="comp.omega_+3A_k">K</code></td>
<td>
<p>a number greater than 0 to be used as the weight of penalty term. The default is 1.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The vector <code class="reqn">\vec{\Omega}</code> is called possibilistic penalty term that controls the variance of clusters (Gosztolya &amp; Szilagyi, 2015). In general, it is computed by using the fuzzy intra cluster distances from a previous run of FCM. This vector of mobilization scale parameters related with the spread of clusters contains the distance values at where membership degrees becomes 0.5 for each cluster (Timm et al, 2004; Wachs et al, 2006; Alamelumangai, 2014). 
</p>
<p style="text-align: center;"><code class="reqn">\vec{\Omega} = K \frac{\sum\limits_{i=1}^n u_{ij}^m \; d^2(\vec{x}_i,\vec{v}_j)}{\sum\limits_{i=1}^n u_{ij}^m}\; ;\; 1\leq j\leq k</code>
</p>

<p>Where:
<code class="reqn">K</code> is a coefficent, <code class="reqn">K \in (0,\infty)</code>. It is generally chosen to be 1.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>omega</code></td>
<td>
<p>a numeric vector containing the mobilization scale values for each cluster.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Zeynel Cebeci
</p>


<h3>References</h3>

<p>Alamelumangai N. (2014). Computer aided segmentation of mammary carcinoma on ultrasound images using soft computing techniques. PhD Thesis, Anna Univ., IN. &lt;<a href="http://hdl.handle.net/10603/50590">http://hdl.handle.net/10603/50590</a>&gt;
</p>
<p>Wachs, J., Shapira, O., &amp; Stern, H. (2006). A method to enhance the &lsquo;Possibilistic C-Means with Repulsion&rsquo; algorithm based on cluster validity index. In <em>Applied Soft Computing Technologies: The Challenge of Complexity</em>, pp. 77-87. Springer, Berlin, Heidelberg. &lt;doi: 10.1007/3-540-31662-0_6&gt;
</p>
<p>Gosztolya, G. &amp; Szilagyi, L. (2015). Application of fuzzy and possibilistic c-means clustering models in blind speaker clustering. <em>Acta Polytechnica Hungarica</em>, 12(7):41-56. &lt;<a href="http://publicatio.bibl.u-szeged.hu/6151/1/2015-acta-polytechnica.pdf">http://publicatio.bibl.u-szeged.hu/6151/1/2015-acta-polytechnica.pdf</a>&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pcm">pcm</a></code>,
<code><a href="#topic+pcmr">pcmr</a></code>,
<code><a href="#topic+fpcm">fpcm</a></code>,
<code><a href="#topic+gkpfcm">gkpfcm</a></code>,
<code><a href="#topic+pfcm">pfcm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
x &lt;- iris[,-5]

# Run FCM 
res.fcm &lt;- fcm(x=x, centers=3)

# Compute the mobilization scale values using the results from FCM
vomg1 &lt;- comp.omega(pco=res.fcm)
vomg1

# Compute the mobilization scale values using the distances and memberships from FCM
vomg2 &lt;- comp.omega(res.fcm$d, res.fcm$u, m=3)
vomg2

# Compute the mobilization scale values with the K value of 10
vomg3 &lt;- comp.omega(res.fcm$d, res.fcm$u, m=2, K=10)
vomg3
</code></pre>

<hr>
<h2 id='crisp'>
Crisp the fuzzy membership degrees
</h2><span id='topic+crisp'></span>

<h3>Description</h3>

<p>Crisps the fuzzy and possibilistic membership degrees from the fuzzy or possibilistic clustering algorithms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crisp(u, method, tv)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="crisp_+3A_u">u</code></td>
<td>
<p>a numeric matrix containing the fuzzy or possibilistic membership degrees of the data objects.</p>
</td></tr>
<tr><td><code id="crisp_+3A_method">method</code></td>
<td>
<p>a string for selection of the crisping method. The default is <span class="option">max</span> that assigns the data object to the cluster in which the object has maximum membership. The alternative is <span class="option">threshold</span> that assigns the objects to a cluster if its maximum membership degree is greater than <code>tv</code>, a threshold value.</p>
</td></tr>
<tr><td><code id="crisp_+3A_tv">tv</code></td>
<td>
<p>a number for the threshold membership degree. The default is 0.5 with the <code>method</code> is <span class="option">threshold</span> if it is not speficied by the user.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>crisp</code> produces the crisp or hard membership degrees of the objects in order to place them into only one cluster. 
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>cluster</code></td>
<td>
<p>a numeric vector containing the indexes (labels) of clusters for the maximum membership of the objects.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Zeynel Cebeci
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
x &lt;- iris[,1:4]

# Run FCM 
res.fcm &lt;- fcm(x, centers=3)

# Crisp the fuzzy memberships degrees and plot the crisp memberships
cllabels &lt;- crisp(res.fcm$u)
plot(x, col=cllabels)
</code></pre>

<hr>
<h2 id='ekm'>
K-Means Clustering Using Different Seeding Techniques
</h2><span id='topic+ekm'></span>

<h3>Description</h3>

<p>The function <code>ekm</code> partitions a numeric data set by using the K-means clustering algorithm. It is a wrapper function of the standard <code><a href="stats.html#topic+kmeans">kmeans</a></code> function with more initialization (seeding) techniques and output values obtained in the multiple starts of the algorithm. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ekm(x, centers, dmetric="euclidean", alginitv="hartiganwong",  
    nstart=1, iter.max=1000, stand=FALSE, numseed)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ekm_+3A_x">x</code></td>
<td>
<p>a numeric vector, data frame or matrix.</p>
</td></tr>
<tr><td><code id="ekm_+3A_centers">centers</code></td>
<td>
<p>an integer specifying the number of clusters or a numeric matrix containing the initial cluster centers.</p>
</td></tr>
<tr><td><code id="ekm_+3A_dmetric">dmetric</code></td>
<td>
<p>a string for the distance calculation method. The default is <span class="option">euclidean</span> for the Euclidean distances.</p>
</td></tr>
<tr><td><code id="ekm_+3A_alginitv">alginitv</code></td>
<td>
<p>a string for the initialization of cluster prototypes matrix. The default is <span class="option">hartiganwong</span> for Hartigan-Wong seeding method. See <code><a href="inaparc.html#topic+get.algorithms">get.algorithms</a></code> for the alternative options.</p>
</td></tr>
<tr><td><code id="ekm_+3A_nstart">nstart</code></td>
<td>
<p>an integer for the number of starts for clustering. The default is 1.</p>
</td></tr>
<tr><td><code id="ekm_+3A_iter.max">iter.max</code></td>
<td>
<p>an integer for the maximum number of iterations allowed. The default is 1000.</p>
</td></tr>
<tr><td><code id="ekm_+3A_stand">stand</code></td>
<td>
<p>a logical flag to standardize data. Its default value is <code>FALSE</code>. If its value is <code>TRUE</code>, the data matrix <code>x</code> is standardized.</p>
</td></tr>
<tr><td><code id="ekm_+3A_numseed">numseed</code></td>
<td>
<p>a seeding number to set the seed of R's random number generator.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>K-Means (KM) clustering algorithm partitions a data set of <code class="reqn">n</code> objects into <code class="reqn">k</code>, a pre-defined number of clusters. It is an iterative relocation algorithm, and in each iteration step the objects are assigned to the nearest cluster by using the Euclidean distances. The objective of KM is to minimize total intra-cluster variance (or the sum of squared errors):
</p>
<p><code class="reqn">J_{KM}(\mathbf{X}; \mathbf{V})=\sum\limits_{i=1}^n d^2(\vec{x}_i, \vec{v}_j)</code>
</p>
<p>In the above equation for <code class="reqn">J_{KM}</code>:
</p>
<p><code class="reqn">d^2(\vec{x}_i, \vec{v}_j)</code> is the distance measure between the object <code class="reqn">\vec{x}_j</code> and cluster prototype <code class="reqn">\vec{v}_i</code>.The Euclidean distance metric is usually employed with the implementations of K-means.
</p>
<p>See <code><a href="stats.html#topic+kmeans">kmeans</a></code> and <code><a href="#topic+ppclust-package">ppclust-package</a></code> for more details about the terms of the objective function <code class="reqn">J_{KM}</code>.
</p>
<p>The update equation of the cluster prototypes:
</p>
<p><code class="reqn">\vec{v}_{j} =\frac{1}{n_j} \sum\limits_{i=1}^{n_j} x_{ij} \;;\; 1 \leq j \leq k</code>
</p>


<h3>Value</h3>

<p>an object of class &lsquo;ppclust&rsquo;, which is a list consists of the following items:
</p>
<table role = "presentation">
<tr><td><code>x</code></td>
<td>
<p>a numeric matrix containing the processed data set.</p>
</td></tr>
<tr><td><code>v</code></td>
<td>
<p>a numeric matrix containing the final cluster prototypes (centers of clusters).</p>
</td></tr>
<tr><td><code>u</code></td>
<td>
<p>a numeric matrix containing the crisp membership degrees of the data objects.</p>
</td></tr>
<tr><td><code>d</code></td>
<td>
<p>a numeric matrix containing the distances of objects to the final cluster prototypes.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>an integer for the number of clusters.</p>
</td></tr>
<tr><td><code>cluster</code></td>
<td>
<p>a numeric vector containing the cluster labels found by defuzzying the fuzzy membership degrees of the objects.</p>
</td></tr>
<tr><td><code>csize</code></td>
<td>
<p>a numeric vector containing the number of objects in the clusters.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>an integer vector for the number of iterations in each start of the algorithm.</p>
</td></tr>
<tr><td><code>best.start</code></td>
<td>
<p>an integer for the index of start with the minimum objective functional.</p>
</td></tr>
<tr><td><code>func.val</code></td>
<td>
<p>a numeric vector for the objective function values in each start of the algorithm.</p>
</td></tr>
<tr><td><code>comp.time</code></td>
<td>
<p>a numeric vector for the execution time in each start of the algorithm.</p>
</td></tr>
<tr><td><code>stand</code></td>
<td>
<p>a logical value, <code>TRUE</code> shows that <code>x</code> data set contains the standardized values of raw data.</p>
</td></tr>
<tr><td><code>wss</code></td>
<td>
<p>a number for the within-cluster sum of squares for each cluster.</p>
</td></tr>
<tr><td><code>bwss</code></td>
<td>
<p>a number for the between-cluster sum of squares.</p>
</td></tr>
<tr><td><code>tss</code></td>
<td>
<p>a number for the total within-cluster sum of squares.</p>
</td></tr>
<tr><td><code>twss</code></td>
<td>
<p>a number for the total sum of squares.</p>
</td></tr>
<tr><td><code>algorithm</code></td>
<td>
<p>a string for the name of partitioning algorithm. It is &lsquo;KM&rsquo; with this function.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>a string for the matched function call generating this &lsquo;ppclust&rsquo; object.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Zeynel Cebeci, Figen Yildiz &amp; Hasan Onder
</p>


<h3>References</h3>

<p>MacQueen, J.B. (1967). Some methods for classification and analysis of multivariate observations. 
In <em>Proc. of 5th Berkeley Symp. on Mathematical Statistics and Probability</em>, Berkeley, University of California Press, 1: 281-297. &lt;<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.308.8619&amp;rep=rep1&amp;type=pdf">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.308.8619&amp;rep=rep1&amp;type=pdf</a>&gt;
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+kmeans">kmeans</a></code>,
<code><a href="#topic+fcm">fcm</a></code>,
<code><a href="#topic+fcm2">fcm2</a></code>,
<code><a href="#topic+fpcm">fpcm</a></code>,
<code><a href="#topic+fpppcm">fpppcm</a></code>,
<code><a href="#topic+gg">gg</a></code>,
<code><a href="#topic+gk">gk</a></code>,
<code><a href="#topic+gkpfcm">gkpfcm</a></code>,
<code><a href="#topic+hcm">hcm</a></code>,
<code><a href="#topic+pca">pca</a></code>,
<code><a href="#topic+pcm">pcm</a></code>,
<code><a href="#topic+pcmr">pcmr</a></code>,
<code><a href="#topic+pfcm">pfcm</a></code>,
<code><a href="#topic+upfc">upfc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load dataset iris 
data(iris)
x &lt;- iris[,-5]

# Run EKM for 3 clusters
res.ekm &lt;- ekm(x, centers=3)

# Print and plot the clustering result
print(res.ekm$cluster)
plot(x, col=res.ekm$cluster, pch=16)
</code></pre>

<hr>
<h2 id='fcm'>
Fuzzy C-Means Clustering
</h2><span id='topic+fcm'></span>

<h3>Description</h3>

<p>Partitions a numeric data set by using the Fuzzy C-Means (FCM) clustering algorithm (Bezdek, 1974;1981).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fcm(x, centers, memberships, m=2, dmetric="sqeuclidean", pw = 2, 
    alginitv="kmpp", alginitu="imembrand", 
    nstart=1, iter.max=1000, con.val=1e-09, 
    fixcent=FALSE, fixmemb=FALSE, stand=FALSE, numseed)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fcm_+3A_x">x</code></td>
<td>
<p>a numeric vector, data frame or matrix.</p>
</td></tr>
<tr><td><code id="fcm_+3A_centers">centers</code></td>
<td>
<p>an integer specifying the number of clusters or a numeric matrix containing the initial cluster centers.</p>
</td></tr>
<tr><td><code id="fcm_+3A_memberships">memberships</code></td>
<td>
<p>a numeric matrix containing the initial membership degrees. If missing, it is internally generated.</p>
</td></tr>
<tr><td><code id="fcm_+3A_m">m</code></td>
<td>
<p>a number greater than 1 to be used as the fuzziness exponent or fuzzifier. The default is 2.</p>
</td></tr>
<tr><td><code id="fcm_+3A_dmetric">dmetric</code></td>
<td>
<p>a string for the distance metric. The default is <span class="option">sqeuclidean</span> for the squared Euclidean distances. See <code><a href="#topic+get.dmetrics">get.dmetrics</a></code> for the alternative options.</p>
</td></tr>
<tr><td><code id="fcm_+3A_pw">pw</code></td>
<td>
<p>a number for the power of Minkowski distance calculation. The default is 2 if the <code>dmetric</code> is <span class="option">minkowski</span>.</p>
</td></tr>
<tr><td><code id="fcm_+3A_alginitv">alginitv</code></td>
<td>
<p>a string for the initialization of cluster prototypes matrix. The default is <span class="option">kmpp</span> for K-means++ initialization method (Arthur &amp; Vassilvitskii, 2007). For the list of alternative options see <code><a href="inaparc.html#topic+get.algorithms">get.algorithms</a></code>.</p>
</td></tr>
<tr><td><code id="fcm_+3A_alginitu">alginitu</code></td>
<td>
<p>a string for the initialization of memberships degrees matrix. The default is <span class="option">imembrand</span> for random sampling of initial membership degrees.</p>
</td></tr>
<tr><td><code id="fcm_+3A_nstart">nstart</code></td>
<td>
<p>an integer for the number of starts for clustering. The default is 1.</p>
</td></tr>
<tr><td><code id="fcm_+3A_iter.max">iter.max</code></td>
<td>
<p>an integer for the maximum number of iterations allowed. The default is 1000.</p>
</td></tr>
<tr><td><code id="fcm_+3A_con.val">con.val</code></td>
<td>
<p>a number for the convergence value between the iterations. The default is 1e-09.</p>
</td></tr>
<tr><td><code id="fcm_+3A_fixcent">fixcent</code></td>
<td>
<p>a logical flag to make the initial cluster centers not changed along the different starts of the algorithm. The default is <code>FALSE</code>. If it is <code>TRUE</code>, the initial centers are not changed in the successive starts of the algorithm when the <code>nstart</code> is greater than 1.</p>
</td></tr>
<tr><td><code id="fcm_+3A_fixmemb">fixmemb</code></td>
<td>
<p>a logical flag to make the initial membership degrees not changed along the different starts of the algorithm. The default is <code>FALSE</code>. If it is <code>TRUE</code>, the initial memberships are not changed in the successive starts of the algorithm when the <code>nstart</code> is greater than 1.</p>
</td></tr>
<tr><td><code id="fcm_+3A_stand">stand</code></td>
<td>
<p>a logical flag to standardize data. Its default value is <code>FALSE</code>. If its value is <code>TRUE</code>, the data matrix <code>x</code> is standardized.</p>
</td></tr>
<tr><td><code id="fcm_+3A_numseed">numseed</code></td>
<td>
<p>an optional seeding number to set the seed of R's random number generator.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Fuzzy C-Means (FCM) clustering algorithm was firstly studied by Dunn (1973) and generalized by Bezdek in 1974 (Bezdek, 1981). Unlike K-means algorithm, each data object is not the member of only one cluster but is the member of all clusters with varying degrees of memberhip between 0 and 1. It is an iterative clustering algorithm that partitions the data set into a predefined <em>k</em> partitions by minimizing the weighted within group sum of squared errors. The objective function of FCM is:
</p>
<p><code class="reqn">J_{FCM}(\mathbf{X}; \mathbf{V}, \mathbf{U})=\sum\limits_{i=1}^n u_{ij}^m d^2(\vec{x}_i, \vec{v}_j)</code>
</p>
<p>In the objective function, <code class="reqn">m</code> is the fuzzifier to specify the amount of 'fuzziness' of the clustering result; <code class="reqn">1 \leq m \leq \infty</code>. It is usually chosen as 2. The higher values of <code class="reqn">m</code> result with the more fuzzy clusters while the lower values give harder clusters. If it is 1, FCM becomes an hard algorithm and produces the same results with K-means.
</p>
<p>FCM must satisfy the following constraints:
</p>
<p><code class="reqn">u_{ij}=[0,1] \;\;;\; 1 \leq i\leq n \;, 1 \leq j\leq k</code> 
</p>
<p><code class="reqn">0 \leq \sum\limits_{i=1}^n u_{ij} \leq n \;\;;\; 1 \leq j\leq k</code>
</p>
<p><code class="reqn">\sum\limits_{j=1}^k u_{ij} = 1 \;\;;\; 1 \leq i\leq n</code>
</p>
<p>The objective function of FCM is minimized by using the following update equations:
</p>
<p><code class="reqn">u_{ij} =\Bigg[\sum\limits_{j=1}^k \Big(\frac{d^2(\vec{x}_i, \vec{v}_j)}{d^2(\vec{x}_i, \vec{v}_l)}\Big)^{1/(m-1)} \Bigg]^{-1} \;\;; {1\leq i\leq n},\; {1\leq l \leq k}</code> 
</p>
<p><code class="reqn">\vec{v}_{j} =\frac{\sum\limits_{i=1}^n u_{ij}^m \vec{x}_i}{\sum\limits_{i=1}^n u_{ij}^m} \;\;; {1\leq j\leq k}</code>
</p>


<h3>Value</h3>

<p>an object of class &lsquo;ppclust&rsquo;, which is a list consists of the following items:
</p>
<table role = "presentation">
<tr><td><code>x</code></td>
<td>
<p>a numeric matrix containing the processed data set.</p>
</td></tr>
<tr><td><code>v</code></td>
<td>
<p>a numeric matrix containing the final cluster prototypes (centers of clusters).</p>
</td></tr>
<tr><td><code>u</code></td>
<td>
<p>a numeric matrix containing the fuzzy memberships degrees of the data objects.</p>
</td></tr>
<tr><td><code>d</code></td>
<td>
<p>a numeric matrix containing the distances of objects to the final cluster prototypes.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>an integer for the number of clusters.</p>
</td></tr>
<tr><td><code>m</code></td>
<td>
<p>a number for the fuzzifier.</p>
</td></tr>
<tr><td><code>cluster</code></td>
<td>
<p>a numeric vector containing the cluster labels found by defuzzying the fuzzy membership degrees of the objects.</p>
</td></tr>
<tr><td><code>csize</code></td>
<td>
<p>a numeric vector containing the number of objects in the clusters.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>an integer vector for the number of iterations in each start of the algorithm.</p>
</td></tr>
<tr><td><code>best.start</code></td>
<td>
<p>an integer for the index of start that produced the minimum objective functional.</p>
</td></tr>
<tr><td><code>func.val</code></td>
<td>
<p>a numeric vector for the objective function values in each start of the algorithm.</p>
</td></tr>
<tr><td><code>comp.time</code></td>
<td>
<p>a numeric vector for the execution time in each start of the algorithm.</p>
</td></tr>
<tr><td><code>stand</code></td>
<td>
<p>a logical value, <code>TRUE</code> shows that data set <code>x</code> contains the standardized values of raw data.</p>
</td></tr>
<tr><td><code>wss</code></td>
<td>
<p>a number for the within-cluster sum of squares for each cluster.</p>
</td></tr>
<tr><td><code>bwss</code></td>
<td>
<p>a number for the between-cluster sum of squares.</p>
</td></tr>
<tr><td><code>tss</code></td>
<td>
<p>a number for the total within-cluster sum of squares.</p>
</td></tr>
<tr><td><code>twss</code></td>
<td>
<p>a number for the total sum of squares.</p>
</td></tr>
<tr><td><code>algorithm</code></td>
<td>
<p>a string for the name of partitioning algorithm. It is &lsquo;FCM&rsquo; with this function.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>a string for the matched function call generating this &lsquo;ppclust&rsquo; object.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Zeynel Cebeci, Figen Yildiz &amp; Alper Tuna Kavlak
</p>


<h3>References</h3>

<p>Arthur, D. &amp; Vassilvitskii, S. (2007). K-means++: The advantages of careful seeding, in <em>Proc. of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms</em>, pp. 1027-1035. &lt;http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf&gt;
</p>
<p>Dunn, J.C. (1973). A fuzzy relative of the ISODATA process and its use in detecting compact well-separated clusters. <em>J. Cybernetics</em>, 3(3):32-57. &lt;doi:10.1080/01969727308546046&gt;
</p>
<p>Bezdek, J.C. (1974). Cluster validity with fuzzy sets. <em>J. Cybernetics</em>, 3: 58-73. &lt;doi:10.1080/01969727308546047&gt;
</p>
<p>Bezdek J.C. (1981). <em>Pattern recognition with fuzzy objective function algorithms</em>. Plenum, NY. &lt;<a href="https://isbnsearch.org/isbn/0306406713">ISBN:0306406713</a>&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ekm">ekm</a></code>,
<code><a href="#topic+fcm2">fcm2</a></code>,
<code><a href="#topic+fpcm">fpcm</a></code>,
<code><a href="#topic+fpppcm">fpppcm</a></code>,
<code><a href="#topic+gg">gg</a></code>,
<code><a href="#topic+gk">gk</a></code>,
<code><a href="#topic+gkpfcm">gkpfcm</a></code>,
<code><a href="#topic+hcm">hcm</a></code>,
<code><a href="#topic+pca">pca</a></code>,
<code><a href="#topic+pcm">pcm</a></code>,
<code><a href="#topic+pcmr">pcmr</a></code>,
<code><a href="#topic+pfcm">pfcm</a></code>,
<code><a href="#topic+upfc">upfc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load dataset iris 
data(iris)
x &lt;- iris[,-5]

# Initialize the prototype matrix using K-means++ algorithm
v &lt;- inaparc::kmpp(x, k=3)$v

# Initialize the memberships degrees matrix 
u &lt;- inaparc::imembrand(nrow(x), k=3)$u

# Run FCM with the initial prototypes and memberships
fcm.res &lt;- fcm(x, centers=v, memberships=u, m=2)

# Show the fuzzy membership degrees for the top 5 objects
head(fcm.res$u, 5)
</code></pre>

<hr>
<h2 id='fcm2'>
Type-2 Fuzzy C-Means Clustering
</h2><span id='topic+fcm2'></span>

<h3>Description</h3>

<p>Partitions a numeric data set by using the Type-2 Fuzzy C-Means (FCM2) clustering algorithm (Rhee &amp; Hwang, 2001). It has been reported that it is effective for spherical clusters in the data sets, but it fails when the data sets contain non-spherical and complex structures (Gosain &amp; Dahiya, 2016).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fcm2(x, centers, memberships, m=2, dmetric="sqeuclidean", pw = 2, 
    alginitv="kmpp", alginitu="imembrand", 
    nstart=1, iter.max=1000, con.val=1e-09, 
    fixcent=FALSE, fixmemb=FALSE, stand=FALSE, numseed)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fcm2_+3A_x">x</code></td>
<td>
<p>a numeric vector, data frame or matrix.</p>
</td></tr>
<tr><td><code id="fcm2_+3A_centers">centers</code></td>
<td>
<p>an integer specifying the number of clusters or a numeric matrix containing the initial cluster centers.</p>
</td></tr>
<tr><td><code id="fcm2_+3A_memberships">memberships</code></td>
<td>
<p>a numeric matrix containing the initial membership degrees. If missing, it is internally generated.</p>
</td></tr>
<tr><td><code id="fcm2_+3A_m">m</code></td>
<td>
<p>a number greater than 1 to be used as the fuzziness exponent or fuzzifier. The default is 2.</p>
</td></tr>
<tr><td><code id="fcm2_+3A_dmetric">dmetric</code></td>
<td>
<p>a string for the distance metric. The default is <span class="option">sqeuclidean</span> for the squared Euclidean distances. See <code><a href="#topic+get.dmetrics">get.dmetrics</a></code> for the alternative options.</p>
</td></tr>
<tr><td><code id="fcm2_+3A_pw">pw</code></td>
<td>
<p>a number for the power of Minkowski distance calculation. The default is 2 if the <code>dmetric</code> is <span class="option">minkowski</span>.</p>
</td></tr>
<tr><td><code id="fcm2_+3A_alginitv">alginitv</code></td>
<td>
<p>a string for the initialization of cluster prototypes matrix. The default is <span class="option">kmpp</span> for K-means++ initialization method (Arthur &amp; Vassilvitskii, 2007). For the list of alternative options, see <code><a href="inaparc.html#topic+get.algorithms">get.algorithms</a></code>.</p>
</td></tr>
<tr><td><code id="fcm2_+3A_alginitu">alginitu</code></td>
<td>
<p>a string for the initialization of memberships degrees matrix. The default is <span class="option">imembrand</span> for random sampling of initial membership degrees.</p>
</td></tr>
<tr><td><code id="fcm2_+3A_nstart">nstart</code></td>
<td>
<p>an integer for the number of starts for clustering. The default is 1.</p>
</td></tr>
<tr><td><code id="fcm2_+3A_iter.max">iter.max</code></td>
<td>
<p>an integer for the maximum number of iterations allowed. The default is 1000.</p>
</td></tr>
<tr><td><code id="fcm2_+3A_con.val">con.val</code></td>
<td>
<p>a number for the convergence value between the iterations. The default is 1e-09.</p>
</td></tr>
<tr><td><code id="fcm2_+3A_fixcent">fixcent</code></td>
<td>
<p>a logical flag to make the initial cluster centers not changed along the different starts of the algorithm. The default is <code>FALSE</code>. If it is <code>TRUE</code>, the initial centers are not changed in the successive starts of the algorithm when the <code>nstart</code> is greater than 1.</p>
</td></tr>
<tr><td><code id="fcm2_+3A_fixmemb">fixmemb</code></td>
<td>
<p>a logical flag to make the initial membership degrees not changed along the different starts of the algorithm. The default is <code>FALSE</code>. If it is <code>TRUE</code>, the initial memberships are not changed in the successive starts of the algorithm when the <code>nstart</code> is greater than 1.</p>
</td></tr>
<tr><td><code id="fcm2_+3A_stand">stand</code></td>
<td>
<p>a logical flag to standardize data. Its default value is <code>FALSE</code>. If its value is <code>TRUE</code>, the data matrix <code>x</code> is standardized.</p>
</td></tr>
<tr><td><code id="fcm2_+3A_numseed">numseed</code></td>
<td>
<p>an optional seeding number to set the seed of R's random number generator.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the Type-2 Fuzzy C-Means (T2FCM) clustering algorithm proposed by (Rhee &amp; Hwang, 2001), the idea is that all data points should not have the same contribution in computing the cluster prototypes, instead the data points with higher membership value should contribute much more in the cluster prototypes (Gosain &amp; Dahiya, 2016). Based on this idea, a type-2 membership value is computed by using the following equation:
</p>
<p><code class="reqn">a_{ij} =  u_{ij}^m - \frac{1 - u_{ij}^m}{2}</code>
</p>
<p>In the above equation, <code class="reqn">u_{ij}</code> and <code class="reqn">a_{ij}</code> respectively stand for the Type-1 (standard)  and Type-2 membership values. <code class="reqn">a_{ij}</code> is only used to update the cluster prototypes.
</p>
<p>The objective function of FCM2 and other terms are the same with those of FCM except the following update equation for the cluster prototypes:
</p>
<p><code class="reqn">\vec{v}_{j} =\frac{\sum\limits_{i=1}^n a_{ij}^m \vec{x}_i}{\sum\limits_{i=1}^n a_{ij}^m} \;\;; 1 \leq j\leq k</code>
</p>


<h3>Value</h3>

<p>an object of class &lsquo;ppclust&rsquo;, which is a list consists of the following items:
</p>
<table role = "presentation">
<tr><td><code>x</code></td>
<td>
<p>a numeric matrix containing the processed data set.</p>
</td></tr>
<tr><td><code>v</code></td>
<td>
<p>a numeric matrix containing the final cluster prototypes (centers of clusters).</p>
</td></tr>
<tr><td><code>u</code></td>
<td>
<p>a numeric matrix containing the fuzzy memberships degrees of the data objects.</p>
</td></tr>
<tr><td><code>d</code></td>
<td>
<p>a numeric matrix containing the distances of objects to the final cluster prototypes.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>an integer for the number of clusters.</p>
</td></tr>
<tr><td><code>m</code></td>
<td>
<p>a number for the fuzzifier.</p>
</td></tr>
<tr><td><code>cluster</code></td>
<td>
<p>a numeric vector containing the cluster labels found by defuzzying the fuzzy membership degrees of the objects.</p>
</td></tr>
<tr><td><code>csize</code></td>
<td>
<p>a numeric vector containing the number of objects in the clusters.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>an integer vector for the number of iterations in each start of the algorithm.</p>
</td></tr>
<tr><td><code>best.start</code></td>
<td>
<p>an integer for the index of start that produced the minimum objective functional.</p>
</td></tr>
<tr><td><code>func.val</code></td>
<td>
<p>a numeric vector for the objective function values in each start of the algorithm.</p>
</td></tr>
<tr><td><code>comp.time</code></td>
<td>
<p>a numeric vector for the execution time in each start of the algorithm.</p>
</td></tr>
<tr><td><code>stand</code></td>
<td>
<p>a logical value, <code>TRUE</code> shows that data set <code>x</code> contains the standardized values of raw data.</p>
</td></tr>
<tr><td><code>wss</code></td>
<td>
<p>a number for the within-cluster sum of squares for each cluster.</p>
</td></tr>
<tr><td><code>bwss</code></td>
<td>
<p>a number for the between-cluster sum of squares.</p>
</td></tr>
<tr><td><code>tss</code></td>
<td>
<p>a number for the total within-cluster sum of squares.</p>
</td></tr>
<tr><td><code>twss</code></td>
<td>
<p>a number for the total sum of squares.</p>
</td></tr>
<tr><td><code>algorithm</code></td>
<td>
<p>a string for the name of partitioning algorithm. It is &lsquo;FCM&rsquo; with this function.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>a string for the matched function call generating this &lsquo;ppclust&rsquo; object.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Zeynel Cebeci &amp; Alper Tuna Kavlak
</p>


<h3>References</h3>

<p>Arthur, D. &amp; Vassilvitskii, S. (2007). K-means++: The advantages of careful seeding, in <em>Proc. of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms</em>, p. 1027-1035. &lt;<a href="http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf">http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf</a>&gt;
</p>
<p>Rhee, F.C.H. &amp; Hwang, C. (2001). A type-2 fuzzy c-means clustering algorithm. In &lt;<em>IEEE 9th IFSA World Congress and 20th NAFIPS Conf. 2001.</em>, 4:1926-1929. &lt;doi:10.1109/NAFIPS.2001.944361&gt;
</p>
<p>Gosain, A. &amp; Dahiya, S. (2016). Performance analysis of various fuzzy clustering algorithms: A review. <em>Procedia Comp. Sci.</em>, 79:100-111. &lt;doi:10.1016/j.procs.2016.03.014&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ekm">ekm</a></code>,
<code><a href="#topic+fcm">fcm</a></code>,
<code><a href="#topic+fpcm">fpcm</a></code>,
<code><a href="#topic+fpppcm">fpppcm</a></code>,
<code><a href="#topic+gg">gg</a></code>,
<code><a href="#topic+gk">gk</a></code>,
<code><a href="#topic+gkpfcm">gkpfcm</a></code>,
<code><a href="#topic+hcm">hcm</a></code>,
<code><a href="#topic+pca">pca</a></code>,
<code><a href="#topic+pcm">pcm</a></code>,
<code><a href="#topic+pcmr">pcmr</a></code>,
<code><a href="#topic+pfcm">pfcm</a></code>,
<code><a href="#topic+upfc">upfc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load dataset X12
data(x12)

# Initialize the prototype matrix using K-means++ algorithm
v &lt;- inaparc::kmpp(x12, k=2)$v
# Initialize the membership degrees matrix 
u &lt;- inaparc::imembrand(nrow(x12), k=2)$u

# Run FCM2 with the initial prototypes and memberships
fcm2.res &lt;- fcm2(x12, centers=v, memberships=u, m=2)

# Show the fuzzy membership degrees for the top 5 objects
head(fcm2.res$u, 5)
</code></pre>

<hr>
<h2 id='fpcm'>
Fuzzy Possibilistic C-Means Clustering
</h2><span id='topic+fpcm'></span>

<h3>Description</h3>

<p>Partitions a numeric data set by using the Fuzzy and Possibilistic C-Means (FPCM) clustering algorithm (Pal et al, 1997).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fpcm(x, centers, memberships, m=2, eta=2,  
     dmetric="sqeuclidean", pw=2, alginitv="kmpp", alginitu="imembrand", 
     nstart=1, iter.max=1000, con.val=1e-09, 
     fixcent=FALSE, fixmemb=FALSE, stand=FALSE, numseed)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fpcm_+3A_x">x</code></td>
<td>
<p>a numeric vector, data frame or matrix.</p>
</td></tr>
<tr><td><code id="fpcm_+3A_centers">centers</code></td>
<td>
<p>an integer specifying the number of clusters or a numeric matrix containing the initial cluster centers.</p>
</td></tr>
<tr><td><code id="fpcm_+3A_memberships">memberships</code></td>
<td>
<p>a numeric matrix containing the initial membership degrees. If missing, it is internally generated.</p>
</td></tr>
<tr><td><code id="fpcm_+3A_m">m</code></td>
<td>
<p>a number greater than 1 to be used as the fuzziness exponent or fuzzifier. The default is 2.</p>
</td></tr>
<tr><td><code id="fpcm_+3A_eta">eta</code></td>
<td>
<p>a number greater than 1 to be used as the typicality exponent. The default is 3.</p>
</td></tr>
<tr><td><code id="fpcm_+3A_dmetric">dmetric</code></td>
<td>
<p>a string for the distance metric. The default is <span class="option">sqeuclidean</span> for the squared Euclidean distances. See <code><a href="#topic+get.dmetrics">get.dmetrics</a></code> for the alternative options.</p>
</td></tr>
<tr><td><code id="fpcm_+3A_pw">pw</code></td>
<td>
<p>a number for the power of Minkowski distance calculation. The default is 2 if the <code>dmetric</code> is <span class="option">minkowski</span>.</p>
</td></tr>
<tr><td><code id="fpcm_+3A_alginitv">alginitv</code></td>
<td>
<p>a string for the initialization of cluster prototypes matrix. The default is <span class="option">kmpp</span> for K-means++ initialization method (Arthur &amp; Vassilvitskii, 2007). For the list of alternative options see <code><a href="inaparc.html#topic+get.algorithms">get.algorithms</a></code>.</p>
</td></tr>
<tr><td><code id="fpcm_+3A_alginitu">alginitu</code></td>
<td>
<p>a string for the initialization of memberships degrees matrix. The default is <span class="option">imembrand</span> for random sampling of initial membership degrees.</p>
</td></tr>
<tr><td><code id="fpcm_+3A_nstart">nstart</code></td>
<td>
<p>an integer for the number of starts for clustering. The default is 1.</p>
</td></tr>
<tr><td><code id="fpcm_+3A_iter.max">iter.max</code></td>
<td>
<p>an integer for the maximum number of iterations allowed. The default is 1000.</p>
</td></tr>
<tr><td><code id="fpcm_+3A_con.val">con.val</code></td>
<td>
<p>a number for the convergence value between the iterations. The default is 1e-09.</p>
</td></tr>
<tr><td><code id="fpcm_+3A_fixcent">fixcent</code></td>
<td>
<p>a logical flag to make the initial cluster centers not changed along the different starts of the algorithm. The default is <code>FALSE</code>. If it is <code>TRUE</code>, the initial centers are not changed in the successive starts of the algorithm when the <code>nstart</code> is greater than 1.</p>
</td></tr>
<tr><td><code id="fpcm_+3A_fixmemb">fixmemb</code></td>
<td>
<p>a logical flag to make the initial membership degrees not changed along the different starts of the algorithm. The default is <code>FALSE</code>. If it is <code>TRUE</code>, the initial memberships are not changed in the successive starts of the algorithm when the <code>nstart</code> is greater than 1.</p>
</td></tr>
<tr><td><code id="fpcm_+3A_stand">stand</code></td>
<td>
<p>a logical flag to standardize data. Its default value is <code>FALSE</code>. If its value is <code>TRUE</code>, the data matrix <code>x</code> is standardized.</p>
</td></tr>
<tr><td><code id="fpcm_+3A_numseed">numseed</code></td>
<td>
<p>a seeding number to set the seed of R's random number generator.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Fuzzy and Possibilistic C Means (FPCM) algorithm which has been proposed by Pal et al (1997) indended to combine the characteristics of FCM and PCM, and hence, was also so-called <dfn>Mixed C-Means</dfn> (MCM) algorithm.
</p>
<p>The objective function of FPCM is:
</p>
<p><code class="reqn">J_{FPCM}(\mathbf{X}; \mathbf{V}, \mathbf{U}, \mathbf{T})=\sum\limits_{i=1}^n (u_{ij}^m + t_{ij}^\eta) \; d^2(\vec{x}_i, \vec{v}_j)</code>
</p>
<p>In the above equation:
</p>
<p><code class="reqn">\mathbf{X} = \{\vec{x}_1, \vec{x}_2,\dots, \vec{x}_n\} \subseteq\Re^p</code> is the data set for <code class="reqn">n</code> objects in the <em>p</em>-dimensional data space <code class="reqn">\Re</code>, 
</p>
<p><code class="reqn">\mathbf{V} = \{\vec{v}_1, \vec{v}_2, \dots, \vec{v}_k\} \subseteq\Re^n</code> is the protoype matrix of the clusters,
</p>
<p><code class="reqn">\mathbf{U} = \{u_{ij}\}</code> is the matrix for a fuzzy partition of <code class="reqn">\mathbf{X}</code>, 
</p>
<p><code class="reqn">\mathbf{T} = \{t_{ij}\}</code> is the matrix for a possibilistic partition of <code class="reqn">\mathbf{X}</code>, 
</p>
<p><code class="reqn">d^2(\vec{x}_i, \vec{v}_j)</code> is the squared Euclidean distance between the object <code class="reqn">\vec{x}_j</code> and cluster prototype <code class="reqn">\vec{v}_i</code>. 
</p>
<p><code class="reqn">d^2(\vec{x}_i , \vec{v}_j) = ||\vec{x}_i - \vec{v}_j||^2 = (\vec{x}_i - \vec{v}_j)^T (\vec{x}_i - \vec{v}_j)</code>
</p>
<p><code class="reqn">m</code> is the fuzzifier to specify the amount of fuzziness for the clustering; <code class="reqn">1\leq m\leq \infty</code>. It is usually chosen as 2. 
</p>
<p><code class="reqn">\eta</code> is the typicality exponent to specify the amount of typicality for the clustering; <code class="reqn">1\leq \eta\leq \infty</code>. It is usually chosen as 2. 
</p>
<p>FPCM must satisfy the following constraints:
</p>
<p><code class="reqn">\sum\limits_{j=1}^k u_{ij} = 1 \;\;;\; 1 \leq i\leq n</code>
</p>
<p><code class="reqn">\sum\limits_{i=1}^n t_{ij} = 1 \;\;;\; 1 \leq j\leq k</code>
</p>
<p>The objective function of FPCM is minimized by using the following update equations:
</p>
<p><code class="reqn">u_{ij} =\Bigg[\sum\limits_{j=1}^k \Big(\frac{d^2(\vec{x}_i, \vec{v}_j)}{d^2(\vec{x}_i, \vec{v}_l)}\Big)^{1/(m-1)} \Bigg]^{-1} \;\;; 1 \leq i \leq n,\; 1 \leq l \leq k</code>
</p>
<p><code class="reqn">t_{ij} =\Bigg[\sum\limits_{l=1}^n \Big(\frac{d^2(\vec{x}_i, \vec{v}_j)}{d^2(\vec{x}_i, \vec{v}_l)}\Big)^{1/(\eta-1)} \Bigg]^{-1} \;\;; 1 \leq i \leq n, \; 1 \leq j \leq k</code>
</p>
<p><code class="reqn">\vec{v}_{j} =\frac{\sum\limits_{i=1}^n (u_{ij}^m + t_{ij}^\eta) \vec{x}_i}{\sum\limits_{i=1}^n (u_{ij}^m + t_{ij}^\eta)} \;\;; {1\leq j\leq k}</code>
</p>


<h3>Value</h3>

<p>an object of class &lsquo;ppclust&rsquo;, which is a list consists of the following items:
</p>
<table role = "presentation">
<tr><td><code>x</code></td>
<td>
<p>a numeric matrix containing the processed data set.</p>
</td></tr>
<tr><td><code>v</code></td>
<td>
<p>a numeric matrix containing the final cluster prototypes (centers of clusters).</p>
</td></tr>
<tr><td><code>u</code></td>
<td>
<p>a numeric matrix containing the fuzzy memberships degrees of the data objects.</p>
</td></tr>
<tr><td><code>d</code></td>
<td>
<p>a numeric matrix containing the distances of objects to the final cluster prototypes.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>an integer for the number of clusters.</p>
</td></tr>
<tr><td><code>m</code></td>
<td>
<p>a number for the fuzzifier.</p>
</td></tr>
<tr><td><code>eta</code></td>
<td>
<p>a number for the typicality exponent.</p>
</td></tr>
<tr><td><code>cluster</code></td>
<td>
<p>a numeric vector containing the cluster labels found by defuzzying the fuzzy membership degrees of the objects.</p>
</td></tr>
<tr><td><code>csize</code></td>
<td>
<p>a numeric vector containing the number of objects in the clusters.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>an integer vector for the number of iterations in each start of the algorithm.</p>
</td></tr>
<tr><td><code>best.start</code></td>
<td>
<p>an integer for the index of start that produced the minimum objective functional.</p>
</td></tr>
<tr><td><code>func.val</code></td>
<td>
<p>a numeric vector for the objective function values in each start of the algorithm.</p>
</td></tr>
<tr><td><code>comp.time</code></td>
<td>
<p>a numeric vector for the execution time in each start of the algorithm.</p>
</td></tr>
<tr><td><code>stand</code></td>
<td>
<p>a logical value, <code>TRUE</code> shows that data set <code>x</code> contains the standardized values of raw data.</p>
</td></tr>
<tr><td><code>wss</code></td>
<td>
<p>a number for the within-cluster sum of squares for each cluster.</p>
</td></tr>
<tr><td><code>bwss</code></td>
<td>
<p>a number for the between-cluster sum of squares.</p>
</td></tr>
<tr><td><code>tss</code></td>
<td>
<p>a number for the total within-cluster sum of squares.</p>
</td></tr>
<tr><td><code>twss</code></td>
<td>
<p>a number for the total sum of squares.</p>
</td></tr>
<tr><td><code>algorithm</code></td>
<td>
<p>a string for the name of partitioning algorithm. It is &lsquo;FCM&rsquo; with this function.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>a string for the matched function call generating this &lsquo;ppclust&rsquo; object.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Zeynel Cebeci, Alper Tuna Kavlak &amp; Figen Yildiz
</p>


<h3>References</h3>

<p>Arthur, D. &amp; Vassilvitskii, S. (2007). K-means++: The advantages of careful seeding, in <em>Proc. of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms</em>, p. 1027-1035. &lt;<a href="http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf">http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf</a>&gt;
</p>
<p>Pal, N.R., Pal, K., &amp; Bezdek, J.C. (1997). A mixed c-means clustering model. In <em>Proc. of the 6th IEEE Int. Conf. on Fuzzy Systems</em>, 1, pp. 11-21. &lt;doi:10.1109/FUZZY.1997.616338&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ekm">ekm</a></code>,
<code><a href="#topic+fcm">fcm</a></code>,
<code><a href="#topic+fcm2">fcm2</a></code>,
<code><a href="#topic+fpppcm">fpppcm</a></code>,
<code><a href="#topic+gg">gg</a></code>,
<code><a href="#topic+gk">gk</a></code>,
<code><a href="#topic+gkpfcm">gkpfcm</a></code>,
<code><a href="#topic+hcm">hcm</a></code>,
<code><a href="#topic+pca">pca</a></code>,
<code><a href="#topic+pcm">pcm</a></code>,
<code><a href="#topic+pcmr">pcmr</a></code>,
<code><a href="#topic+pfcm">pfcm</a></code>,
<code><a href="#topic+upfc">upfc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load dataset iris 
data(iris)
x &lt;- iris[,-5]

# Initialize the prototype matrix using K-means++
v &lt;- inaparc::kmpp(x, k=3)$v

# Initialize the memberships degrees matrix 
u &lt;- inaparc::imembrand(nrow(x), k=3)$u

# Run FPCM with the initial prototypes and memberships
fpcm.res &lt;- fpcm(x, centers=v, memberships=u, m=2, eta=2)

# Show the fuzzy membership degrees for the top 5 objects
head(fpcm.res$u, 5)

# Show the possibilistic membership degrees for the top 5 objects
head(fpcm.res$t, 5)
</code></pre>

<hr>
<h2 id='fpppcm'>
Fuzzy Possibilistic Product Partition C-Means Clustering
</h2><span id='topic+fpppcm'></span>

<h3>Description</h3>

<p>Partitions a numeric data set by using the Fuzzy Possibilistic Product Partition C-Means (FPPPCM) clustering algorithm which has been proposed by Szilagyi &amp; Szilagyi (2014).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fpppcm(x, centers, memberships, m=2, eta=2, K=1, omega, 
    dmetric="sqeuclidean", pw=2, alginitv="kmpp", alginitu="imembrand", 
    nstart=1, iter.max=1000, con.val=1e-09, 
    fixcent=FALSE, fixmemb=FALSE, stand=FALSE, numseed)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fpppcm_+3A_x">x</code></td>
<td>
<p>a numeric vector, data frame or matrix.</p>
</td></tr>
<tr><td><code id="fpppcm_+3A_centers">centers</code></td>
<td>
<p>an integer specifying the number of clusters or a numeric matrix containing the initial cluster centers.</p>
</td></tr>
<tr><td><code id="fpppcm_+3A_memberships">memberships</code></td>
<td>
<p>a numeric matrix containing the initial membership degrees. If missing, it is internally generated.</p>
</td></tr>
<tr><td><code id="fpppcm_+3A_m">m</code></td>
<td>
<p>a number greater than 1 to be used as the fuzziness exponent. The default is 2.</p>
</td></tr>
<tr><td><code id="fpppcm_+3A_eta">eta</code></td>
<td>
<p>a number greater than 1 to be used as the typicality exponent. The default is 2.</p>
</td></tr>
<tr><td><code id="fpppcm_+3A_k">K</code></td>
<td>
<p>a number greater than 0 to be used as the weight of penalty term. The default is 1.</p>
</td></tr>
<tr><td><code id="fpppcm_+3A_omega">omega</code></td>
<td>
<p>a numeric vector of reference distances. If missing, it is internally generated.</p>
</td></tr>
<tr><td><code id="fpppcm_+3A_dmetric">dmetric</code></td>
<td>
<p>a string for the distance metric. The default is <span class="option">sqeuclidean</span> for the squared Euclidean distances. See <code><a href="#topic+get.dmetrics">get.dmetrics</a></code> for the alternative options.</p>
</td></tr>
<tr><td><code id="fpppcm_+3A_pw">pw</code></td>
<td>
<p>a number for the power of Minkowski distance calculation. The default is 2 if the <code>dmetric</code> is <span class="option">minkowski</span>.</p>
</td></tr>
<tr><td><code id="fpppcm_+3A_alginitv">alginitv</code></td>
<td>
<p>a string for the initialization of cluster prototypes matrix. The default is <span class="option">kmpp</span> for K-means++ initialization method (Arthur &amp; Vassilvitskii, 2007). For the list of alternative options see <code><a href="inaparc.html#topic+get.algorithms">get.algorithms</a></code>.</p>
</td></tr>
<tr><td><code id="fpppcm_+3A_alginitu">alginitu</code></td>
<td>
<p>a string for the initialization of memberships degrees matrix. The default is <span class="option">imembrand</span> for random sampling of initial membership degrees.</p>
</td></tr>
<tr><td><code id="fpppcm_+3A_nstart">nstart</code></td>
<td>
<p>an integer for the number of starts for clustering. The default is 1.</p>
</td></tr>
<tr><td><code id="fpppcm_+3A_iter.max">iter.max</code></td>
<td>
<p>an integer for the maximum number of iterations allowed. The default is 1000.</p>
</td></tr>
<tr><td><code id="fpppcm_+3A_con.val">con.val</code></td>
<td>
<p>a number for the convergence value between the iterations. The default is 1e-09.</p>
</td></tr>
<tr><td><code id="fpppcm_+3A_fixcent">fixcent</code></td>
<td>
<p>a logical flag to fix the initial cluster centers. The default is <code>FALSE</code>. If it is <code>TRUE</code>, the initial centers are not changed in the successive starts of the algorithm when the <code>nstart</code> is greater than 1.</p>
</td></tr>
<tr><td><code id="fpppcm_+3A_fixmemb">fixmemb</code></td>
<td>
<p>a logical flag to fix the initial membership degrees. The default is <code>FALSE</code>. If it is <code>TRUE</code>, the initial memberships are not changed in the successive starts of the algorithm when the <code>nstart</code> is greater than 1.</p>
</td></tr>
<tr><td><code id="fpppcm_+3A_stand">stand</code></td>
<td>
<p>a logical flag to standardize data. Its default value is <code>FALSE</code>. If its value is <code>TRUE</code>, the data matrix <code>x</code> is standardized.</p>
</td></tr>
<tr><td><code id="fpppcm_+3A_numseed">numseed</code></td>
<td>
<p>a seeding number to set the seed of R's random number generator.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Fuzzy Possibilistic Product Partition C-Means (FPPPCM) clustering algorithm aimed to eliminate the effect of outliers in the other fuzzy and possibilistic clustering algorithms. The algorithm includes a probabilistic and a possibilistic term via multiplicative way instead of additive combination (Gosztolya &amp; Szilagyi, 2015). The objective function of the algorithm as follows:
</p>
<p><code class="reqn">J_{FPPPCM}(\mathbf{X}; \mathbf{V}, \mathbf{U}, \mathbf{T})=\sum\limits_{j=1}^k \sum\limits_{i=1}^n u_{ij}^m \big[ t_{ij}^\eta \; d^2(\vec{x}_i, \vec{v}_j) + \Omega_j (1-t_{ij})^\eta \big]</code>
</p>
<p>The fuzzy membership degrees in the probabilistic part of the objective function <code class="reqn">J_{FPPPCM}</code> is updated as follows:
</p>
<p><code class="reqn">u_{ij} = \frac{\Big[t_{ij}^\eta \; d^2(\vec{x}_i, \vec{v}_j) \; + \; \Omega_j (1-t_{ij})^\eta \Big]^{-1/(m-1)}}{\Big[ \sum\limits_{l=1}^k t_{il}^\eta \; d^2(\vec{x}_i, \vec{v}_l) \; + \; \Omega_l (1-t_{il})^\eta \Big]^{-1/(m-1)}} \;;\; 1 \leq i \leq n, \; 1 \leq j \leq k</code>
</p>
<p>The typicality degrees in the possibilistic part of the objective function <code class="reqn">J_{FPPPCM}</code> is calculated as follows:
</p>
<p><code class="reqn">t_{ij} =\Bigg[1 + \Big(\frac{d^2(\vec{x}_i, \vec{v}_j)}{\Omega_j}\Big)^{1/(\eta -1)}\Bigg]^{-1} \;;\; 1 \leq i \leq n, \; 1 \leq j \leq k</code>
</p>
<p><code class="reqn">m</code> is the fuzzifier to specify the amount of fuzziness for the clustering; <code class="reqn">1\leq m\leq \infty</code>. It is usually chosen as 2. 
</p>
<p><code class="reqn">\eta</code> is the typicality exponent to specify the amount of typicality for the clustering; <code class="reqn">1\leq \eta\leq \infty</code>. It is usually chosen as 2. 
</p>
<p><code class="reqn">\Omega</code> is the possibilistic penalty term to control the variance of the clusters.
</p>
<p>The update equation for cluster prototypes:
</p>
<p><code class="reqn">\vec{v}_j =\frac{\sum\limits_{i=1}^n u_{ij}^m \; t_{ij}^\eta \; \vec{x}_i}{\sum\limits_{i=1}^n u_{ij}^m \; t_{ij}^\eta} \;;\; 1 \leq j \leq k</code>
</p>


<h3>Value</h3>

<p>an object of class &lsquo;ppclust&rsquo;, which is a list consists of the following items:
</p>
<table role = "presentation">
<tr><td><code>v</code></td>
<td>
<p>a numeric matrix containing the final cluster prototypes.</p>
</td></tr>
<tr><td><code>t</code></td>
<td>
<p>a numeric matrix containing the typicality degrees of the data objects.</p>
</td></tr>
<tr><td><code>d</code></td>
<td>
<p>a numeric matrix containing the distances of objects to the final cluster prototypes.</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>a numeric matrix containing the processed data set.</p>
</td></tr>
<tr><td><code>cluster</code></td>
<td>
<p>a numeric vector containing the cluster labels found by defuzzifying the typicality degrees of the objects.</p>
</td></tr>
<tr><td><code>csize</code></td>
<td>
<p>a numeric vector for the number of objects in the clusters.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>an integer for the number of clusters.</p>
</td></tr>
<tr><td><code>m</code></td>
<td>
<p>a number for the used fuzziness exponent.</p>
</td></tr>
<tr><td><code>eta</code></td>
<td>
<p>a number for the used typicality exponent.</p>
</td></tr>
<tr><td><code>omega</code></td>
<td>
<p>a numeric vector of reference distances.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>an integer vector for the number of iterations in each start of the algorithm.</p>
</td></tr>
<tr><td><code>best.start</code></td>
<td>
<p>an integer for the index of start that produced the minimum objective functional.</p>
</td></tr>
<tr><td><code>func.val</code></td>
<td>
<p>a numeric vector for the objective function values in each start of the algorithm.</p>
</td></tr>
<tr><td><code>comp.time</code></td>
<td>
<p>a numeric vector for the execution time in each start of the algorithm.</p>
</td></tr>
<tr><td><code>stand</code></td>
<td>
<p>a logical value, <code>TRUE</code> shows that <code>x</code> data set contains the standardized values of raw data.</p>
</td></tr>
<tr><td><code>wss</code></td>
<td>
<p>a number for the within-cluster sum of squares for each cluster.</p>
</td></tr>
<tr><td><code>bwss</code></td>
<td>
<p>a number for the between-cluster sum of squares.</p>
</td></tr>
<tr><td><code>tss</code></td>
<td>
<p>a number for the total within-cluster sum of squares.</p>
</td></tr>
<tr><td><code>twss</code></td>
<td>
<p>a number for the total sum of squares.</p>
</td></tr>
<tr><td><code>algorithm</code></td>
<td>
<p>a string for the name of partitioning algorithm. It is &lsquo;PCM&rsquo; with this function.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>a string for the matched function call generating this &lsquo;ppclust&rsquo; object.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Zeynel Cebeci, Alper Tuna Kavlak &amp; Figen Yildiz
</p>


<h3>References</h3>

<p>Arthur, D. &amp; Vassilvitskii, S. (2007). K-means++: The advantages of careful seeding, in <em>Proc. of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms</em>, p. 1027-1035. &lt;<a href="http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf">http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf</a>&gt;
</p>
<p>Szilagyi, L. &amp; Szilagyi, S. M. (2014). Generalization rules for the suppressed fuzzy c-means clustering algorithm. <em>Neurocomputing</em>, 139:298-309. &lt;doi:10.1016/j.neucom.2014.02.027&gt;
</p>
<p>Gosztolya, G. &amp; Szilagyi, L. (2015). Application of fuzzy and possibilistic c-means clustering models in blind speaker clustering. <em>Acta Polytechnica Hungarica</em>, 12(7):41-56. &lt;<a href="http://publicatio.bibl.u-szeged.hu/6151/1/2015-acta-polytechnica.pdf">http://publicatio.bibl.u-szeged.hu/6151/1/2015-acta-polytechnica.pdf</a>&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ekm">ekm</a></code>,
<code><a href="#topic+fcm">fcm</a></code>,
<code><a href="#topic+fcm2">fcm2</a></code>,
<code><a href="#topic+fpcm">fpcm</a></code>,
<code><a href="#topic+gg">gg</a></code>,
<code><a href="#topic+gk">gk</a></code>,
<code><a href="#topic+gkpfcm">gkpfcm</a></code>,
<code><a href="#topic+hcm">hcm</a></code>,
<code><a href="#topic+pca">pca</a></code>,
<code><a href="#topic+pcm">pcm</a></code>,
<code><a href="#topic+pcmr">pcmr</a></code>,
<code><a href="#topic+upfc">upfc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load dataset X16
data(x16)
x &lt;- x16[,-3]
# Initialize the prototype matrix using K-means++
v &lt;- inaparc::kmpp(x, k=2)$v
# Initialize the memberships degrees matrix 
u &lt;- inaparc::imembrand(nrow(x), k=2)$u

# Run FPPPCM 
res.fpppcm &lt;- fpppcm(x, centers=v, memberships=u, m=2, eta=2)

# Display typicality degrees 
res.fpppcm$t

# Run FPPPCM for eta=3
res.fpppcm &lt;- fpppcm(x, centers=v, memberships=u, m=2, eta=3)

# Display typicality degrees 
res.fpppcm$t
</code></pre>

<hr>
<h2 id='get.dmetrics'>
List the names of distance metrics
</h2><span id='topic+get.dmetrics'></span>

<h3>Description</h3>

<p>Displays the distance metric options for calculation of the distances between the data objects and the cluster centers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get.dmetrics(dmt="all")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get.dmetrics_+3A_dmt">dmt</code></td>
<td>
<p>a string for the type of distance metrics. The default is <span class="option">all</span> for the list of all of the distance metrics which are available in the package. The other options are <span class="option">l1</span>, <span class="option">l2</span>, <span class="option">lp</span>, <span class="option">sl2</span> and <span class="option">sc</span> for L1, L2, Lp, squared L2 and squared Chord, respectively.</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>dmlist</code></td>
<td>
<p>a two-column matrix containing the distance metrics and their descriptions.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Zeynel Cebeci
</p>


<h3>References</h3>

<p>Podani, J. (2000). <em>Introduction to the Exploration of Multivariate Biological Data</em>. Backhuys Publishers, Leiden, The Netherlands. 407 pages. &lt;<a href="https://isbnsearch.org/isbn/9057820676">ISBN 90-5782-067-6</a>&gt;
</p>
<p>McCune, B., &amp; Grace, J. B. (2002). <em>Analysis of ecological communities</em>. Gleneden Beach, Oregon: MjM Software Design. &lt;ISBN:0972129006&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ppclust-package">ppclust-package</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Get all distance metrics
dmlist &lt;- get.dmetrics(dmt="all")
dmlist

# Get only L2 type distance metrics
dmlist &lt;- get.dmetrics(dmt="l2")
dmlist
</code></pre>

<hr>
<h2 id='gg'>Gath-Geva Clustering Algorithm</h2><span id='topic+gg'></span>

<h3>Description</h3>

<p>Partitions a numeric data set by using the Gath-Geva (GG) clustering algorithm (Gath &amp; Geva, 1989). The function <code>gg</code> is based on the code in <code>fuzzy.GG</code> function of the package <span class="pkg">advclust</span> by Bagus and Pramana (2016) with some more extended initialization methods and distance metrics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gg(x, centers, memberships, m=2, ggversion="simple", 
   dmetric="sqeuclidean", pw = 2, alginitv="kmpp", 
   alginitu="imembrand", nstart=1, iter.max=1e03, con.val=1e-09, 
   fixcent=FALSE, fixmemb=FALSE, stand=FALSE, numseed)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gg_+3A_x">x</code></td>
<td>
<p>a numeric vector, data frame or matrix.</p>
</td></tr>
<tr><td><code id="gg_+3A_centers">centers</code></td>
<td>
<p>an integer specifying the number of clusters or a numeric matrix containing the initial cluster centers.</p>
</td></tr>
<tr><td><code id="gg_+3A_memberships">memberships</code></td>
<td>
<p>a numeric matrix containing the initial membership degrees. If missing, it is internally generated.</p>
</td></tr>
<tr><td><code id="gg_+3A_ggversion">ggversion</code></td>
<td>
<p>a string for the version of Gath-Geva algorithm. The default is <span class="option">simple</span> for the simplified version (Hoepner et al (1999). Use <span class="option">original</span> for the original Gath-Geva algorithm (Gath &amp; Geva, 1989).</p>
</td></tr>
<tr><td><code id="gg_+3A_m">m</code></td>
<td>
<p>a number greater than 1 to be used as the fuzziness exponent or fuzzifier. The default is 2.</p>
</td></tr>
<tr><td><code id="gg_+3A_dmetric">dmetric</code></td>
<td>
<p>a string for the distance metric. The default is <span class="option">sqeuclidean</span> for the squared Euclidean distances. See <code><a href="#topic+get.dmetrics">get.dmetrics</a></code> for the alternative options.</p>
</td></tr>
<tr><td><code id="gg_+3A_pw">pw</code></td>
<td>
<p>a number for the power of Minkowski distance calculation. The default is 2 if the <code>dmetric</code> is <span class="option">minkowski</span>.</p>
</td></tr>
<tr><td><code id="gg_+3A_alginitv">alginitv</code></td>
<td>
<p>a string for the initialization of cluster prototypes matrix. The default is <span class="option">kmpp</span> for K-means++ initialization method (Arthur &amp; Vassilvitskii, 2007). For the list of alternative options see <code><a href="inaparc.html#topic+get.algorithms">get.algorithms</a></code>.</p>
</td></tr>
<tr><td><code id="gg_+3A_alginitu">alginitu</code></td>
<td>
<p>a string for the initialization of memberships degrees matrix. The default is <span class="option">imembrand</span> for random sampling of initial membership degrees.</p>
</td></tr>
<tr><td><code id="gg_+3A_nstart">nstart</code></td>
<td>
<p>an integer for the number of starts for clustering. The default is 1.</p>
</td></tr>
<tr><td><code id="gg_+3A_iter.max">iter.max</code></td>
<td>
<p>an integer for the maximum number of iterations allowed. The default is 1000.</p>
</td></tr>
<tr><td><code id="gg_+3A_con.val">con.val</code></td>
<td>
<p>a number for the convergence value between the iterations. The default is 1e-09.</p>
</td></tr>
<tr><td><code id="gg_+3A_fixcent">fixcent</code></td>
<td>
<p>a logical flag to make the initial cluster centers not changed along the different starts of the algorithm. The default is <code>FALSE</code>. If it is <code>TRUE</code>, the initial centers are not changed in the successive starts of the algorithm when the <code>nstart</code> is greater than 1.</p>
</td></tr>
<tr><td><code id="gg_+3A_fixmemb">fixmemb</code></td>
<td>
<p>a logical flag to make the initial membership degrees not changed along the different starts of the algorithm. The default is <code>FALSE</code>. If it is <code>TRUE</code>, the initial memberships are not changed in the successive starts of the algorithm when the <code>nstart</code> is greater than 1.</p>
</td></tr>
<tr><td><code id="gg_+3A_stand">stand</code></td>
<td>
<p>a logical flag to standardize data. Its default value is <code>FALSE</code>. If its value is <code>TRUE</code>, the data matrix <code>x</code> is standardized.</p>
</td></tr>
<tr><td><code id="gg_+3A_numseed">numseed</code></td>
<td>
<p>a seeding number to set the seed of R's random number generator.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Gath and Geva (1989) proposed that the fuzzy maximum likelihood estimates (FMLE) clustering algorithm can be used to detect clusters of varying shapes, sizes and densities. Instead of using Euclidean distance as in FCM, Gath-Geva (GG) algorithm uses a distance norm based on fuzzy maximum likelihood estimates as described by Bezdek &amp; Dunn (1975). These are the Gauss distances, and hence, GG is also so-called Gaussian Mixture Decomposition.  
</p>
<p>The objective function of GG is:
</p>
<p><code class="reqn">J_{GG}(\mathbf{X}; \mathbf{V}, \mathbf{A}, \mathbf{U}) = \sum\limits_{i=1}^n \sum\limits_{j=1}^k  u_{ij}^m d_{A_j}(\vec{x}_i, \vec{v}_j)</code>
</p>
<p>In the above equation, <code class="reqn">d_{A_j}(\vec{x}_i, \vec{v}_j)</code> is the Gauss distance between the object <code class="reqn">\vec{x}_j</code> and cluster prototype <code class="reqn">\vec{v}_i</code>. 
</p>
<p><code class="reqn">d_{A_j}(\vec{x}_i, \vec{v}_j) = \frac{(2 \pi)^{\frac{n}{2}}\sqrt{\det{\mathbf{A}_j}}}{\alpha_j} \; exp\big(\frac{1}{2} (\vec{x}_i - \vec{v}_j)^T \mathbf{A}_j^{-1}(\vec{x}_i - \vec{v}_j)\big)</code>
</p>
<p><code class="reqn">\mathbf{A}_j = \frac{\sum\limits_{i=1}^n u_{ij}^m (\vec{x}_i - \vec{v}_j)^T (\vec{x}_i - \vec{v}_j)}{\sum\limits_{i=1}^n u_{ij}^m} \;;\; 1 \leq j \leq k</code>
</p>
<p>The argument <code class="reqn">\alpha_j</code> is the prior probability for belonging <code class="reqn">\vec{x}_i</code> to the cluster <code class="reqn">j</code>:
</p>
<p><code class="reqn">{\alpha_j} = \frac{\sum\limits_{i=1}^n u_{ij}^m}{n}</code>
</p>
<p>The argument <code class="reqn">\alpha_j</code> is used to evaluate the size of a cluster since bigger clusters attract more elements.
</p>
<p><code class="reqn">m</code> is the fuzzifier to specify the amount of fuzziness for the clustering. Although it is 1 in the original FMLE algorithm (Bezdek &amp; Dunn, 1975) a higher value of it (<code class="reqn">1\leq m\leq \infty</code>) can be used to make the partition more fuzzy compensating the exponential term of the distance norm (Balasko et al, 2005).  
</p>
<p>The objective function of GG is minimized by using the following update equations:
</p>
<p><code class="reqn">u_{ij} =\Bigg[\sum\limits_{j=1}^k \Big(\frac{d_{A_j}(\vec{x}_i, \vec{v}_j)}{d_{A_l}(\vec{x}_i, \vec{v}_l)}\Big)^{1/(m-1)} \Bigg]^{-1} \;\;; 1 \leq i \leq n,\; 1 \leq l \leq k</code>
</p>
<p><code class="reqn">\vec{v}_{j} =\frac{\sum\limits_{i=1}^n u_{ij}^m \vec{x}_i}{\sum\limits_{i=1}^n u_{ij}^m} \;\;; 1 \leq j \leq k</code>
</p>


<h3>Value</h3>

<p>an object of class &lsquo;ppclust&rsquo;, which is a list consists of the following items:
</p>
<table role = "presentation">
<tr><td><code>x</code></td>
<td>
<p>a numeric matrix containing the processed data set.</p>
</td></tr>
<tr><td><code>v</code></td>
<td>
<p>a numeric matrix containing the final cluster prototypes (centers of clusters).</p>
</td></tr>
<tr><td><code>u</code></td>
<td>
<p>a numeric matrix containing the fuzzy memberships degrees of the data objects.</p>
</td></tr>
<tr><td><code>d</code></td>
<td>
<p>a numeric matrix containing the distances of objects to the final cluster prototypes.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>an integer for the number of clusters.</p>
</td></tr>
<tr><td><code>m</code></td>
<td>
<p>a number for the fuzzifier.</p>
</td></tr>
<tr><td><code>cluster</code></td>
<td>
<p>a numeric vector containing the cluster labels found by defuzzying the fuzzy membership degrees of the objects.</p>
</td></tr>
<tr><td><code>csize</code></td>
<td>
<p>a numeric vector containing the number of objects in the clusters.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>an integer vector for the number of iterations in each start of the algorithm.</p>
</td></tr>
<tr><td><code>best.start</code></td>
<td>
<p>an integer for the index of start that produced the minimum objective functional.</p>
</td></tr>
<tr><td><code>func.val</code></td>
<td>
<p>a numeric vector for the objective function values in each start of the algorithm.</p>
</td></tr>
<tr><td><code>comp.time</code></td>
<td>
<p>a numeric vector for the execution time in each start of the algorithm.</p>
</td></tr>
<tr><td><code>stand</code></td>
<td>
<p>a logical value, <code>TRUE</code> shows that data set <code>x</code> contains the standardized values of raw data.</p>
</td></tr>
<tr><td><code>wss</code></td>
<td>
<p>a number for the within-cluster sum of squares for each cluster.</p>
</td></tr>
<tr><td><code>bwss</code></td>
<td>
<p>a number for the between-cluster sum of squares.</p>
</td></tr>
<tr><td><code>tss</code></td>
<td>
<p>a number for the total within-cluster sum of squares.</p>
</td></tr>
<tr><td><code>twss</code></td>
<td>
<p>a number for the total sum of squares.</p>
</td></tr>
<tr><td><code>algorithm</code></td>
<td>
<p>a string for the name of partitioning algorithm. It is &lsquo;FCM&rsquo; with this function.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>a string for the matched function call generating this &lsquo;ppclust&rsquo; object.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Due to the exponential distance norm, this algorithm needs a good initialization since it converges to a near local optimum. So, usually another clustering algorithm, i.e. FCM, is used to initialize the partition matrix <code class="reqn">\mathbf{U}</code> (Balasko et al, 2005).
</p>


<h3>Author(s)</h3>

<p>Zeynel Cebeci
</p>


<h3>References</h3>

<p>Arthur, D. &amp; Vassilvitskii, S. (2007). K-means++: The advantages of careful seeding, in <em>Proc. of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms</em>, p. 1027-1035. &lt;<a href="http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf">http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf</a>&gt;
</p>
<p>Bezdek, J.C. &amp; Dunn J.C. (1975). Optimal fuzzy partitions: A heuristic for estimating the parameters in a mixture of normal dustrubutions. <em>IEEE Transactions on Computers</em>, C-24(8):835-838. &lt;doi: 10.1109/T-C.1975.224317&gt;
</p>
<p>Gath, I. &amp; Geva, A.B. (1989). Unsupervised optimal fuzzy clustering. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 11 (7): 773-781. &lt;doi:10.1109/34.192473&gt;
</p>
<p>Hoeppner, F., Klawonn, F., Kruse, R. &amp; Runkler, T. (1999). <em>Fuzzy cluster analysis</em>. New York: John Wiley and Sons. &lt;ISBN:0471988642&gt;
</p>
<p>Balasko, B., Abonyi, J. &amp; Feil, B. (2005). Fuzzy clustering and data analysis toolbox. Department of Process Eng., Univ. of Veszprem, Veszprem.
</p>
<p>Bagus, A. F. &amp; Pramana, S. (2016). advclust: Object Oriented Advanced Clustering. R package version 0.4. <a href="https://CRAN.R-project.org/package=advclust">https://CRAN.R-project.org/package=advclust</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ekm">ekm</a></code>,
<code><a href="#topic+fcm">fcm</a></code>,
<code><a href="#topic+fcm2">fcm2</a></code>,
<code><a href="#topic+fpcm">fpcm</a></code>,
<code><a href="#topic+fpppcm">fpppcm</a></code>,
<code><a href="#topic+gk">gk</a></code>,
<code><a href="#topic+gkpfcm">gkpfcm</a></code>,
<code><a href="#topic+hcm">hcm</a></code>,
<code><a href="#topic+pca">pca</a></code>,
<code><a href="#topic+pcm">pcm</a></code>,
<code><a href="#topic+pcmr">pcmr</a></code>,
<code><a href="#topic+pfcm">pfcm</a></code>,
<code><a href="#topic+upfc">upfc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Load dataset iris 
data(iris)
x &lt;- iris[,-5]

# Initialize the prototype matrix using Inofrep algorithm
v &lt;- inaparc::inofrep(x, k=3)$v
# Initialize the memberships degrees matrix 
u &lt;- inaparc::imembrand(nrow(x), k=3)$u

# Run Gath &amp; Geva with the initial prototypes and memberships
gg.res &lt;- gg(x, centers=v, memberships=u, m=2)

# Show the fuzzy memberships degrees for the top 5 objects
head(gg.res$u, 5)

## End(Not run)
</code></pre>

<hr>
<h2 id='gk'>
Gustafson-Kessel Clustering
</h2><span id='topic+gk'></span>

<h3>Description</h3>

<p>Partitions a numeric data set by using the Gustafson-Kessel (GK) clustering algorithm (Gustafson &amp; Kessel, 1979). Unlike FCM using the Euclidean distance, GK uses cluster specific Mahalanobis distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gk(x, centers, memberships, m=2,  
   dmetric="sqeuclidean", pw = 2, alginitv="kmpp", 
   alginitu="imembrand", nstart=1, iter.max=1e03, con.val=1e-09, 
   fixcent=FALSE, fixmemb=FALSE, stand=FALSE, numseed)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gk_+3A_x">x</code></td>
<td>
<p>a numeric vector, data frame or matrix.</p>
</td></tr>
<tr><td><code id="gk_+3A_centers">centers</code></td>
<td>
<p>an integer specifying the number of clusters or a numeric matrix containing the initial cluster centers.</p>
</td></tr>
<tr><td><code id="gk_+3A_memberships">memberships</code></td>
<td>
<p>a numeric matrix containing the initial membership degrees. If missing, it is internally generated.</p>
</td></tr>
<tr><td><code id="gk_+3A_m">m</code></td>
<td>
<p>a number greater than 1 to be used as the fuzziness exponent or fuzzifier. The default is 2.</p>
</td></tr>
<tr><td><code id="gk_+3A_dmetric">dmetric</code></td>
<td>
<p>a string for the distance metric. The default is <span class="option">sqeuclidean</span> for the squared Euclidean distances. See <code><a href="#topic+get.dmetrics">get.dmetrics</a></code> for the alternative options.</p>
</td></tr>
<tr><td><code id="gk_+3A_pw">pw</code></td>
<td>
<p>a number for the power of Minkowski distance calculation. The default is 2 if the <code>dmetric</code> is <span class="option">minkowski</span>.</p>
</td></tr>
<tr><td><code id="gk_+3A_alginitv">alginitv</code></td>
<td>
<p>a string for the initialization of cluster prototypes matrix. The default is <span class="option">kmpp</span> for K-means++ initialization method (Arthur &amp; Vassilvitskii, 2007). For the list of alternative options see <code><a href="inaparc.html#topic+get.algorithms">get.algorithms</a></code>.</p>
</td></tr>
<tr><td><code id="gk_+3A_alginitu">alginitu</code></td>
<td>
<p>a string for the initialization of memberships degrees matrix. The default is <span class="option">imembrand</span> for random sampling of initial membership degrees.</p>
</td></tr>
<tr><td><code id="gk_+3A_nstart">nstart</code></td>
<td>
<p>an integer for the number of starts for clustering. The default is 1.</p>
</td></tr>
<tr><td><code id="gk_+3A_iter.max">iter.max</code></td>
<td>
<p>an integer for the maximum number of iterations allowed. The default is 1000.</p>
</td></tr>
<tr><td><code id="gk_+3A_con.val">con.val</code></td>
<td>
<p>a number for the convergence value between the iterations. The default is 1e-09.</p>
</td></tr>
<tr><td><code id="gk_+3A_fixcent">fixcent</code></td>
<td>
<p>a logical flag to make the initial cluster centers not changed along the different starts of the algorithm. The default is <code>FALSE</code>. If it is <code>TRUE</code>, the initial centers are not changed in the successive starts of the algorithm when the <code>nstart</code> is greater than 1.</p>
</td></tr>
<tr><td><code id="gk_+3A_fixmemb">fixmemb</code></td>
<td>
<p>a logical flag to make the initial membership degrees not changed along the different starts of the algorithm. The default is <code>FALSE</code>. If it is <code>TRUE</code>, the initial memberships are not changed in the successive starts of the algorithm when the <code>nstart</code> is greater than 1.</p>
</td></tr>
<tr><td><code id="gk_+3A_stand">stand</code></td>
<td>
<p>a logical flag to standardize data. Its default value is <code>FALSE</code>. If its value is <code>TRUE</code>, the data matrix <code>x</code> is standardized.</p>
</td></tr>
<tr><td><code id="gk_+3A_numseed">numseed</code></td>
<td>
<p>a seeding number to set the seed of R's random number generator.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>As an extension of basic FCM algorithm, Gustafson and Kessel (GK) clustering algorithm employs an adaptive distance norm in order to detect clusters with different geometrical shapes (Babuska, 2001; Correa et al, 2011). 
</p>
<p>The objective function of GK is:
</p>
<p><code class="reqn">J_{GK}(\mathbf{X}; \mathbf{V}, \mathbf{A}, \mathbf{U}) = \sum\limits_{i=1}^n \sum\limits_{j=1}^k  u_{ij}^m d_{A_j}(\vec{x}_i, \vec{v}_j)</code>
</p>
<p>In the above equation <code class="reqn">d_{A_j}(\vec{x}_i, \vec{v}_j)</code> is the Mahalanobis distance between the data object <code class="reqn">\vec{x}_j</code> and cluster prototype <code class="reqn">\vec{v}_i</code>.
</p>
<p><code class="reqn">d_{A_j}(\vec{x}_i, \vec{v}_j) = (\vec{x}_i - \vec{v}_j)^T \mathbf{A}_j (\vec{x}_i - \vec{v}_j)</code> 
</p>
<p>As it is understood from the above equation, each cluster has its own norm-inducing matrix in GK, so that the norm matrix <code class="reqn">\mathbf{A}</code> is a <em>k</em>-length tuples of the cluster-specific norm-inducing matrices:
</p>
<p><code class="reqn">\mathbf{A}=\{\mathbf{A}_1, \mathbf{A}_2, \dots, \mathbf{A}_k\}</code>. 
</p>
<p>The objective function of GK cannot be directly minimized with respect to <code class="reqn">\mathbf{A}_j</code> since it is
linear in <code class="reqn">\mathbf{A}_j</code>. For obtaining a feasible solution, <code class="reqn">\mathbf{A}_j</code> must be constrained in some way. One of the usual ways of accomplishing this is to constrain the determinant of <code class="reqn">\mathbf{A}_j</code> (Babuska, 2001):
</p>
<p><code class="reqn">\mid\mathbf{A}_j\mid=\rho_j \;,\; \rho_j &gt; 0 \;,\; 1 \leq j \leq k</code>
</p>
<p>Allowing the matrix <code class="reqn">\mathbf{A}_j</code> to vary with its determinant fixed corresponds to optimizing the shape of cluster while its volume remains constant. By using the Lagrange-multiplier method, the norm-inducing matrix for the cluster <code class="reqn">j</code> is defined as follows (Babuska, 2001):
</p>
<p><code class="reqn">\mathbf{A}_j = [\rho_i \; det(\mathbf{F}_j)]^{-1/n}{\mathbf{F}_j}^{-1}</code>,
</p>
<p>where:
</p>
<p><code class="reqn">n</code> is the number of data objects,
</p>
<p><code class="reqn">\rho_j</code> represents the volume of cluster <code class="reqn">j</code>,
</p>
<p><code class="reqn">\mathbf{F}_j</code> is the fuzzy covariance matrix for the cluster <code class="reqn">j</code>, and is calculated as follows:
</p>
<p><code class="reqn">\mathbf{F}_j = \frac{\sum\limits_{i=1}^n u_{ij}^m \; d^2(\vec{x}_i, \vec{v}_j)}{\sum\limits_{i=1}^n u_{ij}^m}</code>
</p>
<p><code class="reqn">m</code> is the fuzzifier to specify the amount of fuzziness for the clustering; <code class="reqn">1\leq m\leq \infty</code>. It is usually chosen as 2. 
</p>
<p>GK must satisfy the following constraints:
</p>
<p><code class="reqn">\sum\limits_{j=1}^k u_{ij} = 1 \;\;;\; 1 \leq i \leq n</code>
</p>
<p><code class="reqn">\sum\limits_{i=1}^n u_{ij} &gt; 0 \;\;;\; 1 \leq j \leq k</code>
</p>
<p>The objective function of GK is minimized by using the following update equations:
</p>
<p><code class="reqn">u_{ij} =\Bigg[\sum\limits_{j=1}^k \Big(\frac{d_{A_j}(\vec{x}_i, \vec{v}_j)}{d_{A_l}(\vec{x}_i, \vec{v}_l)}\Big)^{1/(m-1)} \Bigg]^{-1} \;\;; 1 \leq i \leq n,\; 1 \leq l \leq k</code>
</p>
<p><code class="reqn">\vec{v}_{j} =\frac{\sum\limits_{i=1}^n u_{ij}^m \vec{x}_i}{\sum\limits_{i=1}^n u_{ij}^m} \;\;; 1 \leq j \leq k</code>
</p>


<h3>Value</h3>

<p>an object of class &lsquo;ppclust&rsquo;, which is a list consists of the following items:
</p>
<table role = "presentation">
<tr><td><code>x</code></td>
<td>
<p>a numeric matrix containing the processed data set.</p>
</td></tr>
<tr><td><code>v</code></td>
<td>
<p>a numeric matrix containing the final cluster prototypes (centers of clusters).</p>
</td></tr>
<tr><td><code>u</code></td>
<td>
<p>a numeric matrix containing the fuzzy memberships degrees of the data objects.</p>
</td></tr>
<tr><td><code>d</code></td>
<td>
<p>a numeric matrix containing the distances of objects to the final cluster prototypes.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>an integer for the number of clusters.</p>
</td></tr>
<tr><td><code>m</code></td>
<td>
<p>a number for the fuzzifier.</p>
</td></tr>
<tr><td><code>cluster</code></td>
<td>
<p>a numeric vector containing the cluster labels found by defuzzying the fuzzy membership degrees of the objects.</p>
</td></tr>
<tr><td><code>csize</code></td>
<td>
<p>a numeric vector containing the number of objects in the clusters.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>an integer vector for the number of iterations in each start of the algorithm.</p>
</td></tr>
<tr><td><code>best.start</code></td>
<td>
<p>an integer for the index of start that produced the minimum objective functional.</p>
</td></tr>
<tr><td><code>func.val</code></td>
<td>
<p>a numeric vector for the objective function values in each start of the algorithm.</p>
</td></tr>
<tr><td><code>comp.time</code></td>
<td>
<p>a numeric vector for the execution time in each start of the algorithm.</p>
</td></tr>
<tr><td><code>stand</code></td>
<td>
<p>a logical value, <code>TRUE</code> shows that data set <code>x</code> contains the standardized values of raw data.</p>
</td></tr>
<tr><td><code>wss</code></td>
<td>
<p>a number for the within-cluster sum of squares for each cluster.</p>
</td></tr>
<tr><td><code>bwss</code></td>
<td>
<p>a number for the between-cluster sum of squares.</p>
</td></tr>
<tr><td><code>tss</code></td>
<td>
<p>a number for the total within-cluster sum of squares.</p>
</td></tr>
<tr><td><code>twss</code></td>
<td>
<p>a number for the total sum of squares.</p>
</td></tr>
<tr><td><code>algorithm</code></td>
<td>
<p>a string for the name of partitioning algorithm. It is &lsquo;FCM&rsquo; with this function.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>a string for the matched function call generating this &lsquo;ppclust&rsquo; object.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Zeynel Cebeci &amp; Cagatay Cebeci
</p>


<h3>References</h3>

<p>Arthur, D. &amp; Vassilvitskii, S. (2007). K-means++: The advantages of careful seeding, in <em>Proc. of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms</em>, pp. 1027-1035. <a href="http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf">http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf</a>
</p>
<p>Gustafson, D. E. &amp; Kessel, W. C. (1979). Fuzzy clustering with a fuzzy covariance matrix. In <em>Proc. of IEEE Conf. on Decision and Control including the 17th Symposium on Adaptive Processes</em>, San Diego. pp. 761-766. <a href="https://doi.org/10.1109/CDC.1978.268028">doi:10.1109/CDC.1978.268028</a>
</p>
<p>Babuska, R. (2001). Fuzzy and neural control. DISC Course Lecture Notes. Delft University of Technology. Delft, the Netherlands. <a href="https://tr.scribd.com/document/209211977/Fuzzy-and-Neural-Control">https://tr.scribd.com/document/209211977/Fuzzy-and-Neural-Control</a>.
</p>
<p>Correa, C., Valero, C., Barreiro, P., Diago, M. P., &amp; Tardáguila, J. (2011). A comparison of fuzzy clustering algorithms applied to feature extraction on vineyard. In <em>Proc. of the 14th Conf. of the Spanish Assoc. for Artificial Intelligence</em>. <a href="http://oa.upm.es/9246/">http://oa.upm.es/9246/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ekm">ekm</a></code>,
<code><a href="#topic+fcm">fcm</a></code>,
<code><a href="#topic+fcm2">fcm2</a></code>,
<code><a href="#topic+fpcm">fpcm</a></code>,
<code><a href="#topic+fpppcm">fpppcm</a></code>,
<code><a href="#topic+gg">gg</a></code>,
<code><a href="#topic+gkpfcm">gkpfcm</a></code>,
<code><a href="#topic+hcm">hcm</a></code>,
<code><a href="#topic+pca">pca</a></code>,
<code><a href="#topic+pcm">pcm</a></code>,
<code><a href="#topic+pcmr">pcmr</a></code>,
<code><a href="#topic+pfcm">pfcm</a></code>,
<code><a href="#topic+upfc">upfc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Load dataset iris 
data(iris)
x &lt;- iris[,-5]

# Initialize the prototype matrix using K-means++
v &lt;- inaparc::kmpp(x, k=3)$v

# Initialize the membership degrees matrix 
u &lt;- inaparc::imembrand(nrow(x), k=3)$u

# Run FCM with the initial prototypes and memberships
gk.res &lt;- gk(x, centers=v, memberships=u, m=2)

# Show the fuzzy membership degrees for the top 5 objects
head(gk.res$u, 5)

## End(Not run)
</code></pre>

<hr>
<h2 id='gkpfcm'>
Gustafson-Kessel Clustering Using PFCM
</h2><span id='topic+gkpfcm'></span>

<h3>Description</h3>

<p>Partitions a numeric data set by using the Gustafson-Kessel (GK) algorithm within the PFCM (Possibilistic Fuzzy C-Means) clustering algorithm (Ojeda-Magaina et al, 2006).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gkpfcm(x, centers, memberships, m=2, eta=2, K=1, omega, a, b, 
    dmetric="sqeuclidean", pw = 2, alginitv="kmpp", 
    alginitu="imembrand", nstart=1, iter.max=1000, con.val=1e-09, 
    fixcent=FALSE, fixmemb=FALSE, stand=FALSE, numseed)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gkpfcm_+3A_x">x</code></td>
<td>
<p>a numeric vector, data frame or matrix.</p>
</td></tr>
<tr><td><code id="gkpfcm_+3A_centers">centers</code></td>
<td>
<p>an integer specifying the number of clusters or a numeric matrix containing the initial cluster centers.</p>
</td></tr>
<tr><td><code id="gkpfcm_+3A_memberships">memberships</code></td>
<td>
<p>a numeric matrix containing the initial membership degrees. If missing, it is internally generated.</p>
</td></tr>
<tr><td><code id="gkpfcm_+3A_m">m</code></td>
<td>
<p>a number greater than 1 to be used as the fuzziness exponent or fuzzifier. The default is 2.</p>
</td></tr>
<tr><td><code id="gkpfcm_+3A_eta">eta</code></td>
<td>
<p>a number greater than 1 to be used as the typicality exponent. The default is 2.</p>
</td></tr>
<tr><td><code id="gkpfcm_+3A_a">a</code></td>
<td>
<p>a number for the relative importance of the fuzzy part of the objective function. The default is 1.</p>
</td></tr>
<tr><td><code id="gkpfcm_+3A_b">b</code></td>
<td>
<p>a number for the relative importance of the possibilistic part of the objective function. The default is 1.</p>
</td></tr>
<tr><td><code id="gkpfcm_+3A_k">K</code></td>
<td>
<p>a number greater than 0 to be used as the weight of penalty term. The default is 1.</p>
</td></tr>
<tr><td><code id="gkpfcm_+3A_omega">omega</code></td>
<td>
<p>a numeric vector of reference distances. If missing, it is internally generated.</p>
</td></tr>
<tr><td><code id="gkpfcm_+3A_dmetric">dmetric</code></td>
<td>
<p>a string for the distance metric. The default is <span class="option">sqeuclidean</span> for the squared Euclidean distances. See <code><a href="#topic+get.dmetrics">get.dmetrics</a></code> for the alternative options.</p>
</td></tr>
<tr><td><code id="gkpfcm_+3A_pw">pw</code></td>
<td>
<p>a number for the power of Minkowski distance calculation. The default is 2 if the <code>dmetric</code> is <span class="option">minkowski</span>.</p>
</td></tr>
<tr><td><code id="gkpfcm_+3A_alginitv">alginitv</code></td>
<td>
<p>a string for the initialization of cluster prototypes matrix. The default is <span class="option">kmpp</span> for K-means++ initialization method (Arthur &amp; Vassilvitskii, 2007). For the list of alternative options see <code><a href="inaparc.html#topic+get.algorithms">get.algorithms</a></code>.</p>
</td></tr>
<tr><td><code id="gkpfcm_+3A_alginitu">alginitu</code></td>
<td>
<p>a string for the initialization of memberships degrees matrix. The default is <span class="option">imembrand</span> for random sampling of initial membership degrees.</p>
</td></tr>
<tr><td><code id="gkpfcm_+3A_nstart">nstart</code></td>
<td>
<p>an integer for the number of starts for clustering. The default is 1.</p>
</td></tr>
<tr><td><code id="gkpfcm_+3A_iter.max">iter.max</code></td>
<td>
<p>an integer for the maximum number of iterations allowed. The default is 1000.</p>
</td></tr>
<tr><td><code id="gkpfcm_+3A_con.val">con.val</code></td>
<td>
<p>a number for the convergence value between the iterations. The default is 1e-09.</p>
</td></tr>
<tr><td><code id="gkpfcm_+3A_fixcent">fixcent</code></td>
<td>
<p>a logical flag to make the initial cluster centers not changed along the different starts of the algorithm. The default is <code>FALSE</code>. If it is <code>TRUE</code>, the initial centers are not changed in the successive starts of the algorithm when the <code>nstart</code> is greater than 1.</p>
</td></tr>
<tr><td><code id="gkpfcm_+3A_fixmemb">fixmemb</code></td>
<td>
<p>a logical flag to make the initial membership degrees not changed along the different starts of the algorithm. The default is <code>FALSE</code>. If it is <code>TRUE</code>, the initial memberships are not changed in the successive starts of the algorithm when the <code>nstart</code> is greater than 1.</p>
</td></tr>
<tr><td><code id="gkpfcm_+3A_stand">stand</code></td>
<td>
<p>a logical flag to standardize data. Its default value is <code>FALSE</code>. If its value is <code>TRUE</code>, the data matrix <code>x</code> is standardized.</p>
</td></tr>
<tr><td><code id="gkpfcm_+3A_numseed">numseed</code></td>
<td>
<p>a seeding number to set the seed of R's random number generator.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Gustafson-Kessel clustering within Possibilistic Fuzzy C-Means (GKPFCM) algorithm is an improvement for PFCM algorithm that consists of the modification of the distance metric for <code class="reqn">d_{ij_A}</code>. The original PFCM uses the Euclidean distance whereas GKPFCM uses the Mahalanobis distance with GK algorithm. Babuska et al (2002) have proposed an improvement for calculating the covariance matrix <code class="reqn">\mathbf{F}_j</code> as follows:
</p>
<p><code class="reqn">\mathbf{F}_j := (1 - \gamma) \mathbf{F}_j + \gamma (\mathbf{F}_0)^{1/n} \mathbf{I}</code>
</p>
<p>In the above equation, <code class="reqn">\mathbf{F}_j</code> involves a weighted identity matrix. The eigenvalues <code class="reqn">\lambda_{ij}</code> and the eigenvectors <code class="reqn">\Phi_{ij}</code> of the resulting matrix are calculated, and the maximum eigenvalue <code class="reqn">\lambda_{i,max} = max_{j}/ \lambda_{ij}</code> is determined. With the obtained results, <code class="reqn">\lambda_{i,max} = \lambda_{ij}/\beta, \forall j</code>, which satisfies <code class="reqn">\lambda_{i,max} / \lambda_{ij} \geq \beta</code> is calculated. Finally, <code class="reqn">\mathbf{F}_j</code> is recomputed with the following equation:
</p>
<p><code class="reqn">\mathbf{F}_j = [\Phi_{j,1},\dots, \Phi_{j,n}] diag(\lambda_{j,1}, \dots, \lambda_{j,n}) [\Phi_{j,1},\dots, \Phi_{j,n}]^{-1} \;\; \forall j</code>
</p>
<p>The objective function of GKPFCM is:
</p>
<p><code class="reqn">J_{GKPFCM}(\mathbf{X}; \mathbf{V}, \mathbf{A}, \mathbf{U}) = \sum\limits_{i=1}^n \sum\limits_{j=1}^k  u_{ij}^m d_{A_j}(\vec{x}_i, \vec{v}_j)</code>
</p>
<p><code class="reqn">m</code> is the fuzzifier to specify the amount of fuzziness for the clustering; <code class="reqn">1\leq m\leq \infty</code>. It is usually chosen as 2. 
</p>
<p><code class="reqn">\eta</code> is the typicality exponent to specify the amount of typicality for the clustering; <code class="reqn">1\leq \eta\leq \infty</code>. It is usually chosen as 2. 
</p>
<p>The objective function <code class="reqn">J_{GKPFCM}</code> is minimized by using the following update equations:
</p>
<p><code class="reqn">u_{ij} =\Bigg[\sum\limits_{j=1}^k \Big(\frac{d_{A_j}(\vec{x}_i, \vec{v}_j)}{d_{A_j}(\vec{x}_i, \vec{v}_l)}\Big)^{2/(m-1)} \Bigg]^{-1} \;\;; 1\leq i \leq n \;,\; 1 \leq l \leq k</code>
</p>
<p><code class="reqn">t_{ij} =\Bigg[\sum\limits_{j=1}^k \Big(\frac{d_{A_j}(\vec{x}_i, \vec{v}_j))}{d_{A_j}(\vec{x}_i, \vec{v}_l))}\Big)^{2/(\eta-1)} \Bigg]^{-1} \;;\; 1 \leq i \leq n \;;\, 1 \leq l \leq k</code>
</p>
<p><code class="reqn">\vec{v}_{j} =\frac{\sum\limits_{i=1}^n (u_{ij}^m + t_{ij}^\eta) \vec{x}_i}{\sum\limits_{i=1}^n (u_{ij}^m + t_{ij}^\eta)} \;\;; {1\leq j\leq k}</code>
</p>


<h3>Value</h3>

<p>an object of class &lsquo;ppclust&rsquo;, which is a list consists of the following items:
</p>
<table role = "presentation">
<tr><td><code>x</code></td>
<td>
<p>a numeric matrix containing the processed data set.</p>
</td></tr>
<tr><td><code>v</code></td>
<td>
<p>a numeric matrix containing the final cluster prototypes (centers of clusters).</p>
</td></tr>
<tr><td><code>u</code></td>
<td>
<p>a numeric matrix containing the fuzzy memberships degrees of the data objects.</p>
</td></tr>
<tr><td><code>d</code></td>
<td>
<p>a numeric matrix containing the distances of objects to the final cluster prototypes.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>an integer for the number of clusters.</p>
</td></tr>
<tr><td><code>m</code></td>
<td>
<p>a number for the fuzzifier.</p>
</td></tr>
<tr><td><code>eta</code></td>
<td>
<p>a number greater than 1 to be used as the typicality exponent.</p>
</td></tr>
<tr><td><code>a</code></td>
<td>
<p>a number for the relative importance of the fuzzy part of the objective function.</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>a number for the relative importance of the possibilistic part of the objective function.</p>
</td></tr>
<tr><td><code>omega</code></td>
<td>
<p>a numeric vector of reference distances.</p>
</td></tr>
<tr><td><code>cluster</code></td>
<td>
<p>a numeric vector containing the cluster labels found by defuzzying the fuzzy membership degrees of the objects.</p>
</td></tr>
<tr><td><code>csize</code></td>
<td>
<p>a numeric vector containing the number of objects in the clusters.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>an integer vector for the number of iterations in each start of the algorithm.</p>
</td></tr>
<tr><td><code>best.start</code></td>
<td>
<p>an integer for the index of start that produced the minimum objective functional.</p>
</td></tr>
<tr><td><code>func.val</code></td>
<td>
<p>a numeric vector for the objective function values in each start of the algorithm.</p>
</td></tr>
<tr><td><code>comp.time</code></td>
<td>
<p>a numeric vector for the execution time in each start of the algorithm.</p>
</td></tr>
<tr><td><code>stand</code></td>
<td>
<p>a logical value, <code>TRUE</code> shows that data set <code>x</code> contains the standardized values of raw data.</p>
</td></tr>
<tr><td><code>wss</code></td>
<td>
<p>a number for the within-cluster sum of squares for each cluster.</p>
</td></tr>
<tr><td><code>bwss</code></td>
<td>
<p>a number for the between-cluster sum of squares.</p>
</td></tr>
<tr><td><code>tss</code></td>
<td>
<p>a number for the total within-cluster sum of squares.</p>
</td></tr>
<tr><td><code>twss</code></td>
<td>
<p>a number for the total sum of squares.</p>
</td></tr>
<tr><td><code>algorithm</code></td>
<td>
<p>a string for the name of partitioning algorithm. It is &lsquo;FCM&rsquo; with this function.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>a string for the matched function call generating this &lsquo;ppclust&rsquo; object.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Zeynel Cebeci &amp; Cagatay Cebeci
</p>


<h3>References</h3>

<p>Arthur, D. &amp; Vassilvitskii, S. (2007). K-means++: The advantages of careful seeding, in <em>Proc. of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms</em>, p. 1027-1035. &lt;<a href="http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf">http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf</a>&gt;
</p>
<p>Gustafson, D. E. &amp; Kessel, W. C. (1979). Fuzzy clustering with a fuzzy covariance matrix. In <em>Proc. of IEEE Conf. on Decision and Control including the 17th Symposium on Adaptive Processes</em>, San Diego. pp. 761-766. &lt;doi:10.1109/CDC.1978.268028&gt;
</p>
<p>Babuska, R., van der Veen, P. J. &amp;  Kaymak, U. (2002). Improved covariance estimation for Gustafson-Kessel clustering. In <em>Proc. of Int. Conf. on Fuzzy Systems</em>, Hawaii, 2002, pp. 1081-1085. &lt;<a href="https://tr.scribd.com/document/209211977/Fuzzy-and-Neural-Control">https://tr.scribd.com/document/209211977/Fuzzy-and-Neural-Control</a>&gt;.
</p>
<p>Ojeda-Magaina, B., Ruelas, R., Corona-Nakamura, M. A. &amp; Andina, D. (2006). An improvement to the possibilistic fuzzy c-means clustering algorithm. In <em>Proc. of IEEE World Automation Congress, 2006 (WAC'06)</em>. pp. 1-8. &lt;doi:10.1109/WAC.2006.376056&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ekm">ekm</a></code>,
<code><a href="#topic+fcm">fcm</a></code>,
<code><a href="#topic+fcm2">fcm2</a></code>,
<code><a href="#topic+fpcm">fpcm</a></code>,
<code><a href="#topic+fpppcm">fpppcm</a></code>,
<code><a href="#topic+gg">gg</a></code>,
<code><a href="#topic+gk">gk</a></code>,
<code><a href="#topic+hcm">hcm</a></code>,
<code><a href="#topic+pca">pca</a></code>,
<code><a href="#topic+pcm">pcm</a></code>,
<code><a href="#topic+pcmr">pcmr</a></code>,
<code><a href="#topic+pfcm">pfcm</a></code>,
<code><a href="#topic+upfc">upfc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Load dataset iris 
data(iris)
x &lt;- iris[,-5]

# Initialize the prototype matrix using K-means++
v &lt;- inaparc::kmpp(x, k=3)$v

# Initialize the memberships degrees matrix 
u &lt;- inaparc::imembrand(nrow(x), k=3)$u

# Run FCM with the initial prototypes and memberships
gkpfcm.res &lt;- gkpfcm(x, centers=v, memberships=u, m=2)

# Show the fuzzy membership degrees for the top 5 objects
head(gkpfcm.res$u, 5)

## End(Not run)
</code></pre>

<hr>
<h2 id='hcm'>
Hard C-Means Clustering
</h2><span id='topic+hcm'></span>

<h3>Description</h3>

<p>Partitions a numeric data set by using Hard C-Means (HCM) clustering algorithm (or K-Means) which has been proposed by MacQueen(1967). The function <code>hcm</code> is an extension of the basic <code><a href="stats.html#topic+kmeans">kmeans</a></code> with more input arguments and output values in order to make the clustering results comparable with those of other fuzzy and possibilistic algorithms. For instance, not only the Euclidean distance metric but also a number of distance metrics such as the squared Euclidean distance, the squared Chord distance etc. can be employed with the function <code>hcm</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hcm(x, centers, dmetric="euclidean", pw=2, alginitv="kmpp",  
   nstart=1, iter.max=1000, con.val=1e-9, stand=FALSE, numseed)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hcm_+3A_x">x</code></td>
<td>
<p>a numeric vector, data frame or matrix.</p>
</td></tr>
<tr><td><code id="hcm_+3A_centers">centers</code></td>
<td>
<p>an integer specifying the number of clusters or a numeric matrix containing the initial cluster centers.</p>
</td></tr>
<tr><td><code id="hcm_+3A_dmetric">dmetric</code></td>
<td>
<p>a string for the distance metric. The default is <span class="option">euclidean</span> for the squared Euclidean distances. See <code><a href="#topic+get.dmetrics">get.dmetrics</a></code> for the alternative options.</p>
</td></tr>
<tr><td><code id="hcm_+3A_pw">pw</code></td>
<td>
<p>a number for the power of Minkowski distance calculation. The default is 2 if the <code>dmetric</code> is <span class="option">minkowski</span>.</p>
</td></tr>
<tr><td><code id="hcm_+3A_alginitv">alginitv</code></td>
<td>
<p>a string for the initialization of cluster prototypes matrix. The default is <span class="option">kmpp</span> for K-means++ initialization method (Arthur &amp; Vassilvitskii, 2007). For the list of alternative options see <code><a href="inaparc.html#topic+get.algorithms">get.algorithms</a></code>.</p>
</td></tr>
<tr><td><code id="hcm_+3A_nstart">nstart</code></td>
<td>
<p>an integer for the number of starts for clustering. The default is 1.</p>
</td></tr>
<tr><td><code id="hcm_+3A_iter.max">iter.max</code></td>
<td>
<p>an integer for the maximum number of iterations allowed. The default is 1000.</p>
</td></tr>
<tr><td><code id="hcm_+3A_con.val">con.val</code></td>
<td>
<p>a number for the convergence value between the iterations. The default is 1e-09.</p>
</td></tr>
<tr><td><code id="hcm_+3A_stand">stand</code></td>
<td>
<p>a logical flag to standardize data. Its default value is <code>FALSE</code>. If its value is <code>TRUE</code>, the data matrix <code>x</code> is standardized.</p>
</td></tr>
<tr><td><code id="hcm_+3A_numseed">numseed</code></td>
<td>
<p>a seeding number to set the seed of R's random number generator.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Hard C-Means (HCM) clustering algorithm (or K-means) partitions a data set into <em>k</em> groups, so-called clusters. The objective function of HCM is:
</p>
<p><code class="reqn">J_{HCM}(\mathbf{X}; \mathbf{V})=\sum\limits_{i=1}^n d^2(\vec{x}_i, \vec{v}_j)</code>
</p>
<p>See <code><a href="#topic+ppclust-package">ppclust-package</a></code> for the details about the terms in the above equation of <code class="reqn">J_{HCM}</code>.
</p>
<p>The update equation for membership degrees is:
</p>
<p><code class="reqn">u_{ij} = \left\{
                 \begin{array}{rl}
                   1 &amp; if \; d^2(\vec{x}_i, \vec{v}_j) = min_{1\leq l\leq k} \; (d^2(\vec{x}_i, \vec{v}_l)) \\
                   0 &amp; otherwise
                 \end{array}
               \right. </code>
</p>
<p>The update equation for cluster prototypes is:
</p>
<p><code class="reqn">\vec{v}_{j} =\frac{\sum\limits_{i=1}^n u_{ij} \vec{x}_i}{\sum\limits_{i=1}^n u_{ij}} \;\;; {1\leq j\leq k}</code>
</p>


<h3>Value</h3>

<p>an object of class &lsquo;ppclust&rsquo;, which is a list consists of the following items:
</p>
<table role = "presentation">
<tr><td><code>x</code></td>
<td>
<p>a numeric matrix containing the processed data set.</p>
</td></tr>
<tr><td><code>v</code></td>
<td>
<p>a numeric matrix containing the final cluster prototypes (centers of clusters).</p>
</td></tr>
<tr><td><code>u</code></td>
<td>
<p>a numeric matrix containing the hard membership degrees of the data objects.</p>
</td></tr>
<tr><td><code>d</code></td>
<td>
<p>a numeric matrix containing the distances of objects to the final cluster prototypes.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>an integer for the number of clusters.</p>
</td></tr>
<tr><td><code>cluster</code></td>
<td>
<p>a numeric vector containing the cluster labels of the data objects.</p>
</td></tr>
<tr><td><code>csize</code></td>
<td>
<p>a numeric vector containing the number of objects in the clusters.</p>
</td></tr>
<tr><td><code>best.start</code></td>
<td>
<p>an integer for the index of start with the minimum objective functional.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>an integer vector for the number of iterations in each start of the algorithm.</p>
</td></tr>
<tr><td><code>func.val</code></td>
<td>
<p>a numeric vector for the objective function values of each start of the algorithm.</p>
</td></tr>
<tr><td><code>comp.time</code></td>
<td>
<p>a numeric vector for the execution time of each start of the algorithm.</p>
</td></tr>
<tr><td><code>wss</code></td>
<td>
<p>a numeric vector containing the within-cluster sum of squares for each cluster.</p>
</td></tr>
<tr><td><code>bwss</code></td>
<td>
<p>a number for the between-cluster sum of squares.</p>
</td></tr>
<tr><td><code>tss</code></td>
<td>
<p>a number for the total within-cluster sum of squares.</p>
</td></tr>
<tr><td><code>twss</code></td>
<td>
<p>a number for the total sum of squares.</p>
</td></tr>
<tr><td><code>stand</code></td>
<td>
<p>a logical value, <code>TRUE</code> shows that <code>x</code> data set contains the standardized values of raw data.</p>
</td></tr>
<tr><td><code>algorithm</code></td>
<td>
<p>a string for the name of partitioning algorithm. It is &lsquo;HCM&rsquo; with this function.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>a string for the matched function call generating this &lsquo;ppclust&rsquo; object.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Zeynel Cebeci &amp; Figen Yildiz
</p>


<h3>References</h3>

<p>Arthur, D. &amp; Vassilvitskii, S. (2007). K-means++: The advantages of careful seeding, in <em>Proc. of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms</em>, p. 1027-1035. &lt;<a href="http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf">http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf</a>&gt;
</p>
<p>MacQueen, J.B. (1967). Some methods for classification and analysis of multivariate observations. In <em>Proc. of 5th Berkeley Symp. on Mathematical Statistics and Probability</em>, Berkeley, Univ. of California Press, 1: 281-297. &lt;<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.308.8619&amp;rep=rep1&amp;type=pdf">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.308.8619&amp;rep=rep1&amp;type=pdf</a>&gt;
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+kmeans">kmeans</a></code>,
<code><a href="#topic+ekm">ekm</a></code>,
<code><a href="#topic+fcm">fcm</a></code>,
<code><a href="#topic+fcm2">fcm2</a></code>,
<code><a href="#topic+fpcm">fpcm</a></code>,
<code><a href="#topic+fpppcm">fpppcm</a></code>,
<code><a href="#topic+gg">gg</a></code>,
<code><a href="#topic+gk">gk</a></code>,
<code><a href="#topic+gkpfcm">gkpfcm</a></code>,
<code><a href="#topic+pca">pca</a></code>,
<code><a href="#topic+pcm">pcm</a></code>,
<code><a href="#topic+pcmr">pcmr</a></code>,
<code><a href="#topic+pfcm">pfcm</a></code>,
<code><a href="#topic+upfc">upfc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Load dataset iris 
data(iris)
x &lt;- iris[,-5]

# Initialize the prototype matrix using K-means++
v &lt;- inaparc::kmpp(x, k=3)$v

# Run HCM with the initial prototypes
res.hcm &lt;- hcm(x, centers=v)

# Print, summarize and plot the clustering result
res.hcm$cluster
summary(res.hcm$cluster)
plot(x, col=res.hcm$cluster, pch=16)

## End(Not run)
</code></pre>

<hr>
<h2 id='is.ppclust'>
Check the class of object for &lsquo;ppclust&rsquo;
</h2><span id='topic+is.ppclust'></span>

<h3>Description</h3>

<p>Checks the class of given object whether it is an instance of the <code>ppclust</code> class or not. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.ppclust(objx)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.ppclust_+3A_objx">objx</code></td>
<td>
<p>an object to be checked for its class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if <code>objx</code> is a valid <code>ppclust</code> object and <code>FALSE</code> for the other types of object classes.
</p>


<h3>Author(s)</h3>

<p>Zeynel Cebeci
</p>


<h3>See Also</h3>

<p><code><a href="#topic+as.ppclust">as.ppclust</a></code>,
<code><a href="#topic+ppclust2">ppclust2</a></code>,
<code><a href="#topic+summary.ppclust">summary.ppclust</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)

# Run FCM for 3 clusters
res.fcm &lt;- fcm(x=iris[,1:4], centers=2)

# Test for a ppclust object returned by the fcm function
is.ppclust(res.fcm)

# Test for a matrix object
x &lt;- matrix(nrow=3, ncol=2, c(1,4,2,5,7,8))
is.ppclust(x)
</code></pre>

<hr>
<h2 id='mfpcm'>
Modified Fuzzy Possibilistic C-Means Clustering
</h2><span id='topic+mfpcm'></span>

<h3>Description</h3>

<p>Partitions a numeric data set by using the Modified Fuzzy and Possibilistic C-Means (MFPCM) clustering algorithm (Saad &amp; Alimi, 2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mfpcm(x, centers, memberships, m=2, eta=2,  
      dmetric="sqeuclidean", pw=2, alginitv="kmpp", alginitu="imembrand", 
      nstart=1, iter.max=1000, con.val=1e-09, 
      fixcent=FALSE, fixmemb=FALSE, stand=FALSE, numseed)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mfpcm_+3A_x">x</code></td>
<td>
<p>a numeric vector, data frame or matrix.</p>
</td></tr>
<tr><td><code id="mfpcm_+3A_centers">centers</code></td>
<td>
<p>an integer specifying the number of clusters or a numeric matrix containing the initial cluster centers.</p>
</td></tr>
<tr><td><code id="mfpcm_+3A_memberships">memberships</code></td>
<td>
<p>a numeric matrix containing the initial membership degrees. If missing, it is internally generated.</p>
</td></tr>
<tr><td><code id="mfpcm_+3A_m">m</code></td>
<td>
<p>a number greater than 1 to be used as the fuzziness exponent or fuzzifier. The default is 2.</p>
</td></tr>
<tr><td><code id="mfpcm_+3A_eta">eta</code></td>
<td>
<p>a number greater than 1 to be used as the typicality exponent. The default is 3.</p>
</td></tr>
<tr><td><code id="mfpcm_+3A_dmetric">dmetric</code></td>
<td>
<p>a string for the distance metric. The default is <span class="option">sqeuclidean</span> for the squared Euclidean distances. See <code><a href="#topic+get.dmetrics">get.dmetrics</a></code> for the alternative options.</p>
</td></tr>
<tr><td><code id="mfpcm_+3A_pw">pw</code></td>
<td>
<p>a number for the power of Minkowski distance calculation. The default is 2 if the <code>dmetric</code> is <span class="option">minkowski</span>.</p>
</td></tr>
<tr><td><code id="mfpcm_+3A_alginitv">alginitv</code></td>
<td>
<p>a string for the initialization of cluster prototypes matrix. The default is <span class="option">kmpp</span> for K-means++ initialization method (Arthur &amp; Vassilvitskii, 2007). For the list of alternative options see <code><a href="inaparc.html#topic+get.algorithms">get.algorithms</a></code>.</p>
</td></tr>
<tr><td><code id="mfpcm_+3A_alginitu">alginitu</code></td>
<td>
<p>a string for the initialization of memberships degrees matrix. The default is <span class="option">imembrand</span> for random sampling of initial membership degrees.</p>
</td></tr>
<tr><td><code id="mfpcm_+3A_nstart">nstart</code></td>
<td>
<p>an integer for the number of starts for clustering. The default is 1.</p>
</td></tr>
<tr><td><code id="mfpcm_+3A_iter.max">iter.max</code></td>
<td>
<p>an integer for the maximum number of iterations allowed. The default is 1000.</p>
</td></tr>
<tr><td><code id="mfpcm_+3A_con.val">con.val</code></td>
<td>
<p>a number for the convergence value between the iterations. The default is 1e-09.</p>
</td></tr>
<tr><td><code id="mfpcm_+3A_fixcent">fixcent</code></td>
<td>
<p>a logical flag to make the initial cluster centers not changed along the different starts of the algorithm. The default is <code>FALSE</code>. If it is <code>TRUE</code>, the initial centers are not changed in the successive starts of the algorithm when the <code>nstart</code> is greater than 1.</p>
</td></tr>
<tr><td><code id="mfpcm_+3A_fixmemb">fixmemb</code></td>
<td>
<p>a logical flag to make the initial membership degrees not changed along the different starts of the algorithm. The default is <code>FALSE</code>. If it is <code>TRUE</code>, the initial memberships are not changed in the successive starts of the algorithm when the <code>nstart</code> is greater than 1.</p>
</td></tr>
<tr><td><code id="mfpcm_+3A_stand">stand</code></td>
<td>
<p>a logical flag to standardize data. Its default value is <code>FALSE</code>. If its value is <code>TRUE</code>, the data matrix <code>x</code> is standardized.</p>
</td></tr>
<tr><td><code id="mfpcm_+3A_numseed">numseed</code></td>
<td>
<p>a seeding number to set the seed of R's random number generator.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Modified Fuzzy and Possibilistic C Means (MFPCM) algorithm was proposed by Pal et al (1997) intented to incorporate a weight parameter to the objective function of FPCM as follows:
</p>
<p><code class="reqn">J_{MFPCM}(\mathbf{X}; \mathbf{V}, \mathbf{U}, \mathbf{T})=\sum\limits_{i=1}^n u_{ij}^m w_{ij}^m \; d^{2m}(\vec{x}_i, \vec{v}_j) + t_{ij}^\eta w_{ij}^\eta \; d^{2\eta}(\vec{x}_i, \vec{v}_j)</code>
</p>
<p>In the above ojective function, every data object is considered to has its own weight in relation to every cluster. Therefore it is expected that the weight permits to have a better classification especially in the case of noise data (Saad &amp; Alimi, 2009). The weight is calculated with the following equation: 
</p>
<p><code class="reqn">w_{ij} = exp \Bigg[- \frac{d^2(\vec{x}_i, \vec{v}_j)}{\sum\limits_{i=1}^n d^2(\vec{x}_i, \bar{v}) \frac{k}{n}} \Bigg]</code>
</p>
<p>The objective function of MFPCM is minimized by using the following update equations:
</p>
<p><code class="reqn">u_{ij} =\Bigg[\sum\limits_{j=1}^k \Big(\frac{d^2(\vec{x}_i, \vec{v}_j)}{d^2(\vec{x}_i, \vec{v}_l)}\Big)^{2m/(m-1)} \Bigg]^{-1} \;\;; 1 \leq i \leq n,\; 1 \leq l \leq k</code>
</p>
<p><code class="reqn">t_{ij} =\Bigg[\sum\limits_{l=1}^n \Big(\frac{d^2(\vec{x}_i, \vec{v}_j)}{d^2(\vec{x}_i, \vec{v}_l)}\Big)^{2\eta/(\eta-1)} \Bigg]^{-1} \;\;; 1 \leq i \leq n, \; 1 \leq j \leq k</code>
</p>
<p><code class="reqn">\vec{v}_{j} =\frac{\sum\limits_{i=1}^n (u_{ij}^m w_{ij}^m + t_{ij}^\eta w_{ij}^\eta) \vec{x}_i}{\sum\limits_{i=1}^n (u_{ij}^m w_{ij}^m + t_{ij}^\eta) w_{ij}^\eta} \;\;; {1\leq j\leq k}</code>
</p>


<h3>Value</h3>

<p>an object of class &lsquo;ppclust&rsquo;, which is a list consists of the following items:
</p>
<table role = "presentation">
<tr><td><code>x</code></td>
<td>
<p>a numeric matrix containing the processed data set.</p>
</td></tr>
<tr><td><code>v</code></td>
<td>
<p>a numeric matrix containing the final cluster prototypes (centers of clusters).</p>
</td></tr>
<tr><td><code>u</code></td>
<td>
<p>a numeric matrix containing the fuzzy memberships degrees of the data objects.</p>
</td></tr>
<tr><td><code>d</code></td>
<td>
<p>a numeric matrix containing the distances of objects to the final cluster prototypes.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>an integer for the number of clusters.</p>
</td></tr>
<tr><td><code>m</code></td>
<td>
<p>a number for the fuzzifier.</p>
</td></tr>
<tr><td><code>eta</code></td>
<td>
<p>a number for the typicality exponent.</p>
</td></tr>
<tr><td><code>cluster</code></td>
<td>
<p>a numeric vector containing the cluster labels found by defuzzying the fuzzy membership degrees of the objects.</p>
</td></tr>
<tr><td><code>csize</code></td>
<td>
<p>a numeric vector containing the number of objects in the clusters.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>an integer vector for the number of iterations in each start of the algorithm.</p>
</td></tr>
<tr><td><code>best.start</code></td>
<td>
<p>an integer for the index of start that produced the minimum objective functional.</p>
</td></tr>
<tr><td><code>func.val</code></td>
<td>
<p>a numeric vector for the objective function values in each start of the algorithm.</p>
</td></tr>
<tr><td><code>comp.time</code></td>
<td>
<p>a numeric vector for the execution time in each start of the algorithm.</p>
</td></tr>
<tr><td><code>stand</code></td>
<td>
<p>a logical value, <code>TRUE</code> shows that data set <code>x</code> contains the standardized values of raw data.</p>
</td></tr>
<tr><td><code>wss</code></td>
<td>
<p>a number for the within-cluster sum of squares for each cluster.</p>
</td></tr>
<tr><td><code>bwss</code></td>
<td>
<p>a number for the between-cluster sum of squares.</p>
</td></tr>
<tr><td><code>tss</code></td>
<td>
<p>a number for the total within-cluster sum of squares.</p>
</td></tr>
<tr><td><code>twss</code></td>
<td>
<p>a number for the total sum of squares.</p>
</td></tr>
<tr><td><code>algorithm</code></td>
<td>
<p>a string for the name of partitioning algorithm. It is &lsquo;FCM&rsquo; with this function.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>a string for the matched function call generating this &lsquo;ppclust&rsquo; object.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Zeynel Cebeci, Alper Tuna Kavlak &amp; Figen Yildiz
</p>


<h3>References</h3>

<p>Arthur, D. &amp; Vassilvitskii, S. (2007). K-means++: The advantages of careful seeding, in <em>Proc. of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms</em>, p. 1027-1035. &lt;<a href="http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf">http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf</a>&gt;
</p>
<p>Saad, M. F. &amp; Alimi, A. M. (2009). Modified fuzzy possibilistic c-means. In <em>Proc. of the Int.  Multiconference of Engineers and Computer Scientists</em>, 1: 18-20. &lt;ISBN:978-988-17012-2-0&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ekm">ekm</a></code>,
<code><a href="#topic+fcm">fcm</a></code>,
<code><a href="#topic+fcm2">fcm2</a></code>,
<code><a href="#topic+fpppcm">fpppcm</a></code>,
<code><a href="#topic+gg">gg</a></code>,
<code><a href="#topic+gk">gk</a></code>,
<code><a href="#topic+gkpfcm">gkpfcm</a></code>,
<code><a href="#topic+hcm">hcm</a></code>,
<code><a href="#topic+pca">pca</a></code>,
<code><a href="#topic+pcm">pcm</a></code>,
<code><a href="#topic+pcmr">pcmr</a></code>,
<code><a href="#topic+pfcm">pfcm</a></code>,
<code><a href="#topic+upfc">upfc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load dataset iris 
data(iris)
x &lt;- iris[,-5]

# Initialize the prototype matrix using K-means++
v &lt;- inaparc::kmpp(x, k=3)$v

# Initialize the memberships degrees matrix 
u &lt;- inaparc::imembrand(nrow(x), k=3)$u

# Run FCM with the initial prototypes and memberships
mfpcm.res &lt;- mfpcm(x, centers=v, memberships=u, m=2, eta=2)

# Show the fuzzy membership degrees for the top 5 objects
head(mfpcm.res$u, 5)

# Show the possibilistic membership degrees for the top 5 objects
head(mfpcm.res$t, 5)
</code></pre>

<hr>
<h2 id='pca'>
Possibilistic Clustering Algorithm
</h2><span id='topic+pca'></span>

<h3>Description</h3>

<p>Partitions a numeric data set by using the Possibilistic Clustering Algorithm (PCA) which has been proposed by Yang &amp; Wu (2006).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pca(x, centers, memberships, m=2, eta=2, 
   dmetric="sqeuclidean", pw=2, alginitv="kmpp", alginitu="imembrand", 
   nstart=1, iter.max=1000, con.val=1e-09, 
   fixcent=FALSE, fixmemb=FALSE, stand=FALSE, numseed)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pca_+3A_x">x</code></td>
<td>
<p>a numeric vector, data frame or matrix.</p>
</td></tr>
<tr><td><code id="pca_+3A_centers">centers</code></td>
<td>
<p>an integer specifying the number of clusters or a numeric matrix containing the initial cluster centers.</p>
</td></tr>
<tr><td><code id="pca_+3A_memberships">memberships</code></td>
<td>
<p>a numeric matrix containing the initial membership degrees. If missing, it is internally generated.</p>
</td></tr>
<tr><td><code id="pca_+3A_m">m</code></td>
<td>
<p>a number greater than 1 to be used as the fuzziness exponent. The default is 2.</p>
</td></tr>
<tr><td><code id="pca_+3A_eta">eta</code></td>
<td>
<p>a number greater than 1 to be used as the typicality exponent. The default is 2.</p>
</td></tr>
<tr><td><code id="pca_+3A_dmetric">dmetric</code></td>
<td>
<p>a string for the distance metric. The default is <span class="option">sqeuclidean</span> for the squared Euclidean distances. See <code><a href="#topic+get.dmetrics">get.dmetrics</a></code> for the alternative options.</p>
</td></tr>
<tr><td><code id="pca_+3A_pw">pw</code></td>
<td>
<p>a number for the power of Minkowski distance calculation. The default is 2 if the <code>dmetric</code> is <span class="option">minkowski</span>.</p>
</td></tr>
<tr><td><code id="pca_+3A_alginitv">alginitv</code></td>
<td>
<p>a string for the initialization of cluster prototypes matrix. The default is <span class="option">kmpp</span> for K-means++ initialization method (Arthur &amp; Vassilvitskii, 2007). For the list of alternative options see <code><a href="inaparc.html#topic+get.algorithms">get.algorithms</a></code>.</p>
</td></tr>
<tr><td><code id="pca_+3A_alginitu">alginitu</code></td>
<td>
<p>a string for the initialization of memberships degrees matrix. The default is <span class="option">imembrand</span> for random sampling of initial membership degrees.</p>
</td></tr>
<tr><td><code id="pca_+3A_nstart">nstart</code></td>
<td>
<p>an integer for the number of starts for clustering. The default is 1.</p>
</td></tr>
<tr><td><code id="pca_+3A_iter.max">iter.max</code></td>
<td>
<p>an integer for the maximum number of iterations allowed. The default is 1000.</p>
</td></tr>
<tr><td><code id="pca_+3A_con.val">con.val</code></td>
<td>
<p>a number for the convergence value between the iterations. The default is 1e-09.</p>
</td></tr>
<tr><td><code id="pca_+3A_fixcent">fixcent</code></td>
<td>
<p>a logical flag to fix the initial cluster centers. The default is <code>FALSE</code>. If it is <code>TRUE</code>, the initial centers are not changed in the successive starts of the algorithm when the <code>nstart</code> is greater than 1.</p>
</td></tr>
<tr><td><code id="pca_+3A_fixmemb">fixmemb</code></td>
<td>
<p>a logical flag to fix the initial membership degrees. The default is <code>FALSE</code>. If it is <code>TRUE</code>, the initial memberships are not changed in the successive starts of the algorithm when the <code>nstart</code> is greater than 1.</p>
</td></tr>
<tr><td><code id="pca_+3A_stand">stand</code></td>
<td>
<p>a logical flag to standardize data. Its default value is <code>FALSE</code>. If its value is <code>TRUE</code>, the data matrix <code>x</code> is standardized.</p>
</td></tr>
<tr><td><code id="pca_+3A_numseed">numseed</code></td>
<td>
<p>a seeding number to set the seed of R's random number generator.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Unlike the Possibilistic C-Means (PCM) algorithm requiring the results of a previous run of Fuzzy C-Means (FCM) clustering in order to calculate the parameter <code class="reqn">\Omega</code>, Possibilistic Clustering Algorithm (PCA) is based on the FCM objective function, the partition coefficient (PC) and partition entropy (PE) validity indexes. So that PCA directly computes the typicality values and needs not run FCM beforehand to compute this parameter. The resulting membership becomes the exponential function, hence, it is reported that it is robust to noise and outliers (Yang &amp; Wu, 2006). However, Wu et al (2010) reported that PCA is very sensitive to initializations and sometimes generates coincident clusters.
</p>
<p>The objective function of PCA is:
</p>
<p><code class="reqn">J_{PCA}(\mathbf{X}; \mathbf{V}, \mathbf{T})=\sum\limits_{j=1}^k \sum\limits_{i=1}^n t_{ij}^m \; d^2(\vec{x}_i, \vec{v}_j) + \frac{\beta}{m^2\sqrt{k}} \sum\limits_{j=1}^k \sum\limits_{i=1}^n (t_{ij}^m \; log \; t_{ij}^m - t_{ij}^m)</code>
</p>
<p>Where:
</p>
<p><code class="reqn">t_{ij} = exp\Big(- \frac{m \sqrt{k} \; d^2(\vec{x}_i, \vec{v}_j)}{\beta}\Big) \;\;; {1\leq i\leq n},\; {1\leq j\leq k}</code>
</p>
<p>The update equation for cluster prototypes:
</p>
<p><code class="reqn">\vec{v}_{j} =\frac{\sum\limits_{i=1}^n t_{ij}^m \; \vec{x}_i}{\sum\limits_{i=1}^n t_{ij}^m} \;\;; {1\leq j\leq k}</code>
</p>
<p>Where:
</p>
<p><code class="reqn">\beta = \frac{\sum\limits_{i=1}^n \; d^2(\vec{x}_i, \overline{x})}{n}</code> with <code class="reqn">\overline{x}=\frac{\sum\limits_{i=1}^n \vec{x}_i}{n}</code>
</p>


<h3>Value</h3>

<p>an object of class &lsquo;ppclust&rsquo;, which is a list consists of the following items:
</p>
<table role = "presentation">
<tr><td><code>v</code></td>
<td>
<p>a numeric matrix containing the final cluster prototypes.</p>
</td></tr>
<tr><td><code>u</code></td>
<td>
<p>a numeric matrix containing the fuzzy membership degrees of the data objects.</p>
</td></tr>
<tr><td><code>t</code></td>
<td>
<p>a numeric matrix containing the typicality degrees of the data objects.</p>
</td></tr>
<tr><td><code>d</code></td>
<td>
<p>a numeric matrix containing the distances of objects to the final cluster prototypes.</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>a numeric matrix containing the processed data set.</p>
</td></tr>
<tr><td><code>cluster</code></td>
<td>
<p>a numeric vector containing the cluster labels found by defuzzifying the typicality degrees of the objects.</p>
</td></tr>
<tr><td><code>csize</code></td>
<td>
<p>a numeric vector containing the number of objects in the clusters.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>an integer for the number of clusters.</p>
</td></tr>
<tr><td><code>m</code></td>
<td>
<p>a number for the fuziness exponent.</p>
</td></tr>
<tr><td><code>eta</code></td>
<td>
<p>a number for the typicality exponent.</p>
</td></tr>
<tr><td><code>omega</code></td>
<td>
<p>a numeric vector of reference distances.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>an integer vector for the number of iterations in each start of the algorithm.</p>
</td></tr>
<tr><td><code>best.start</code></td>
<td>
<p>an integer for the index of start that produced the minimum objective functional.</p>
</td></tr>
<tr><td><code>func.val</code></td>
<td>
<p>a numeric vector for the objective function values in each start of the algorithm.</p>
</td></tr>
<tr><td><code>comp.time</code></td>
<td>
<p>a numeric vector for the execution time in each start of the algorithm.</p>
</td></tr>
<tr><td><code>stand</code></td>
<td>
<p>a logical value, <code>TRUE</code> shows that data set <code>x</code> contains the standardized values of raw data.</p>
</td></tr>
<tr><td><code>wss</code></td>
<td>
<p>a number for the within-cluster sum of squares for each cluster.</p>
</td></tr>
<tr><td><code>bwss</code></td>
<td>
<p>a number for the between-cluster sum of squares.</p>
</td></tr>
<tr><td><code>tss</code></td>
<td>
<p>a number for the total within-cluster sum of squares.</p>
</td></tr>
<tr><td><code>twss</code></td>
<td>
<p>a number for the total sum of squares.</p>
</td></tr>
<tr><td><code>algorithm</code></td>
<td>
<p>a string for the name of partitioning algorithm. It is &lsquo;PCM&rsquo; with this function.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>a string for the matched function call generating this &lsquo;ppclust&rsquo; object.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Zeynel Cebeci, Alper Tuna Kavlak
</p>


<h3>References</h3>

<p>Arthur, D. &amp; Vassilvitskii, S. (2007). K-means++: The advantages of careful seeding, in <em>Proc. of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms</em>, p. 1027-1035. &lt;<a href="http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf">http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf</a>&gt;
</p>
<p>Yang, M. S. &amp; Wu, K. L. (2006). Unsupervised possibilistic clustering. <em>Pattern Recognition</em>, 39(1): 5-21. &lt;doi:10.1016/j.patcog.2005.07.005&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ekm">ekm</a></code>,
<code><a href="#topic+fcm">fcm</a></code>,
<code><a href="#topic+fcm2">fcm2</a></code>,
<code><a href="#topic+fpcm">fpcm</a></code>,
<code><a href="#topic+fpppcm">fpppcm</a></code>,
<code><a href="#topic+gg">gg</a></code>,
<code><a href="#topic+gk">gk</a></code>,
<code><a href="#topic+gkpfcm">gkpfcm</a></code>,
<code><a href="#topic+hcm">hcm</a></code>,
<code><a href="#topic+pcm">pcm</a></code>,
<code><a href="#topic+pcmr">pcmr</a></code>,
<code><a href="#topic+pfcm">pfcm</a></code>,
<code><a href="#topic+upfc">upfc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load dataset X16
data(x16)
x &lt;- x16[,-3]

# Initialize the prototype matrix using K-means++
v &lt;- inaparc::kmpp(x, k=2)$v
# Initialize the membership degrees matrix 
u &lt;- inaparc::imembrand(nrow(x), k=2)$u

# Run PCA
pca.res &lt;- pca(x, centers=v, memberships=u, m=2, eta=2)

# Display the fuzzy membership degrees 
print(round(pca.res$u,2))

# Display the typicality degrees
print(round(pca.res$t,2))
</code></pre>

<hr>
<h2 id='pcm'>
Possibilistic C-Means Clustering
</h2><span id='topic+pcm'></span>

<h3>Description</h3>

<p>Partitions a numeric multidimensional data set by using the Possibilistic C-Means (PCM) clustering algorithm which has been proposed by Krishnapuram &amp; Keller (1993, 1996).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pcm(x, centers, memberships, eta=2, K=1, omega, oftype = 1,
    dmetric="sqeuclidean", pw=2, fcmrun=TRUE, 
    alginitv="kmpp", alginitu="imembrand", 
    nstart=1, iter.max=1000, con.val=1e-09, 
    fixcent=FALSE, fixmemb=FALSE, stand=FALSE, numseed)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pcm_+3A_x">x</code></td>
<td>
<p>a numeric vector, data frame or matrix.</p>
</td></tr>
<tr><td><code id="pcm_+3A_centers">centers</code></td>
<td>
<p>an integer specifying the number of clusters or a numeric matrix containing the initial cluster centers.</p>
</td></tr>
<tr><td><code id="pcm_+3A_memberships">memberships</code></td>
<td>
<p>a numeric matrix containing the initial membership degrees. If missing, it is internally generated.</p>
</td></tr>
<tr><td><code id="pcm_+3A_eta">eta</code></td>
<td>
<p>a number greater than 1 to be used as the typicality exponent. The default is 2.</p>
</td></tr>
<tr><td><code id="pcm_+3A_k">K</code></td>
<td>
<p>a number greater than 0 to be used as the weight of penalty term. The default is 1.</p>
</td></tr>
<tr><td><code id="pcm_+3A_omega">omega</code></td>
<td>
<p>a numeric vector of reference distances. If missing, it is internally generated.</p>
</td></tr>
<tr><td><code id="pcm_+3A_oftype">oftype</code></td>
<td>
<p>an integer for the type of objective function. The default option is <span class="option">1</span> for the PCM objective function in Krishnapuram &amp; Keller (1993). Use <span class="option">2</span> for PCM objective function in Krishnapuram &amp; Keller (1996).</p>
</td></tr>
<tr><td><code id="pcm_+3A_dmetric">dmetric</code></td>
<td>
<p>a string for the distance metric. The default is <span class="option">sqeuclidean</span> for the squared Euclidean distances. See <code><a href="#topic+get.dmetrics">get.dmetrics</a></code> for the alternative options.</p>
</td></tr>
<tr><td><code id="pcm_+3A_pw">pw</code></td>
<td>
<p>a number for the power of Minkowski distance calculation. The default is <span class="option">2</span> if the <code>dmetric</code> is <span class="option">minkowski</span>.</p>
</td></tr>
<tr><td><code id="pcm_+3A_alginitv">alginitv</code></td>
<td>
<p>a string for the initialization of cluster prototypes matrix. The default is <span class="option">kmpp</span> for K-means++ initialization method (Arthur &amp; Vassilvitskii, 2007). For the list of alternative options see <code><a href="inaparc.html#topic+get.algorithms">get.algorithms</a></code>.</p>
</td></tr>
<tr><td><code id="pcm_+3A_alginitu">alginitu</code></td>
<td>
<p>a string for the initialization of memberships degrees matrix. The default is <span class="option">imembrand</span> for random sampling of initial membership degrees.</p>
</td></tr>
<tr><td><code id="pcm_+3A_fcmrun">fcmrun</code></td>
<td>
<p>a logical value for using the results from a previous FCM run. The default is <span class="option">TRUE</span> for starting the algorithm with the initial centers and memberships from FCM run. Set the value to <span class="option">FALSE</span> to cancel to use the results of FCM run.</p>
</td></tr>
<tr><td><code id="pcm_+3A_nstart">nstart</code></td>
<td>
<p>an integer for the number of starts for clustering. The default is 1.</p>
</td></tr>
<tr><td><code id="pcm_+3A_iter.max">iter.max</code></td>
<td>
<p>an integer for the maximum number of iterations allowed. The default is 1000.</p>
</td></tr>
<tr><td><code id="pcm_+3A_con.val">con.val</code></td>
<td>
<p>a number for the convergence value between the iterations. The default is 1e-09.</p>
</td></tr>
<tr><td><code id="pcm_+3A_fixcent">fixcent</code></td>
<td>
<p>a logical value to fix the initial cluster centers. The default is <code>FALSE</code>. If it is <code>TRUE</code>, the initial centers are not changed in the successive starts of the algorithm when the <code>nstart</code> is greater than 1.</p>
</td></tr>
<tr><td><code id="pcm_+3A_fixmemb">fixmemb</code></td>
<td>
<p>a logical value to fix the initial membership degrees. The default is <code>FALSE</code>. If it is <code>TRUE</code>, the initial memberships are not changed in the successive starts of the algorithm when the <code>nstart</code> is greater than 1.</p>
</td></tr>
<tr><td><code id="pcm_+3A_stand">stand</code></td>
<td>
<p>a logical value to standardize data. The default value is <code>FALSE</code>. If its value is <code>TRUE</code>, the data matrix <code>x</code> is standardized.</p>
</td></tr>
<tr><td><code id="pcm_+3A_numseed">numseed</code></td>
<td>
<p>a seeding number to set the seed of R's random number generator.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Possibilistic C-Means (PCM) clustering algorithm was proposed by Krishnapuram and Keller (1993) to overcome the noise problem of FCM (Nasraoui, 1999). The algorithm differs from the other clustering algorithms in that the resulting partition of the data can be interpreted as a possibilistic partition, and the membership values can be interpreted as degrees of possibility of the points belonging to the classes, i.e., the compatibilities of the points with the class prototypes (Krishnapuram &amp; Keller, 1993). PCM relaxes the probabilistic constraint on the sum of the memberships of an object to all clusters in FCM. However, PCM must run on the fuzzy clustering results of FCM in order to calculate the parameter <code class="reqn">\Omega</code>. Although it solves the noise sensitivity problem of FCM, the performance of PCM depends heavily on the initialization and often deteriorates due to the coincident clustering problem (Filippone et al, 2007). 
</p>
<p>The objective function of PCM is:
</p>
<p><code class="reqn">J_{PCM}(\mathbf{X}; \mathbf{V}, \mathbf{T})=\sum\limits_{i=1}^n t_{ij}^\eta \; d^2(\vec{x}_i, \vec{v}_j) + \sum\limits_{j=1}^k \Omega_j \sum\limits_{i=1}^n (1 - t_{ij})^\eta</code>
</p>
<p>In the objective function above, the first term leads to a minimization of the weighted distances while the second term suppresses the trivial solution (Timm et al, 2004). 
</p>
<p>Krishnapuram and Keller later proposed an alternative objective function for PCM (Krishnapuram &amp; Keller, 1996):
</p>
<p><code class="reqn">J_{PCM_2}(\mathbf{X}; \mathbf{V}, \mathbf{T})=\sum\limits_{i=1}^n t_{ij}^\eta \; d^2(\vec{x}_i, \vec{v}_j) + \sum\limits_{j=1}^k \Omega_j \sum\limits_{i=1}^n (t_{ij}^\eta \; \log{t_{ij}^\eta}  - t_{ij}^\eta)</code>
</p>
<p>Where:
</p>
<p><code class="reqn">\vec{\Omega} = K \sum\limits_{i=1}^n u_{ij}^m \; d^2(\vec{x}_i, \vec{v}_j) / \sum\limits_{i=1}^n u_{ij}^m</code>
</p>
<p>Since the membership calculation in PCM is possibilistic, the membership degrees can be treated as typicality values which measure how typical a data object is for a given cluster, regardless of all other clusters. The update equation of typicality degrees remains identical to those of FCM, and is derived from the objective function of PCM is:
</p>
<p><code class="reqn">t_{ij} =\Bigg[1 + \Big(\frac{d^2(\vec{x}_i, \vec{v}_j)}{\Omega_j}\Big)^{(1/(m-1)}\Bigg]^{-1} \;\;; 1 \leq i \leq  n,\; 1 \leq j \leq k</code>
</p>
<p>The update equation for cluster prototypes is the same with those of FCM:
</p>
<p><code class="reqn">\vec{v}_{j} =\frac{\sum\limits_{i=1}^n t_{ij}^m \vec{x}_i}{\sum\limits_{i=1}^n t_{ij}^m} \;\;; 1 \leq j \leq k</code>
</p>


<h3>Value</h3>

<p>an object of class &lsquo;ppclust&rsquo;, which is a list consists of the following items:
</p>
<table role = "presentation">
<tr><td><code>v</code></td>
<td>
<p>a numeric matrix containing the final cluster prototypes.</p>
</td></tr>
<tr><td><code>t</code></td>
<td>
<p>a numeric matrix containing the typicality degrees of the data objects.</p>
</td></tr>
<tr><td><code>d</code></td>
<td>
<p>a numeric matrix containing the distances of objects to the final cluster prototypes.</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>a numeric matrix containing the processed data set.</p>
</td></tr>
<tr><td><code>cluster</code></td>
<td>
<p>a numeric vector containing the cluster labels found by defuzzifying the typicality degrees of the objects.</p>
</td></tr>
<tr><td><code>csize</code></td>
<td>
<p>a numeric vector containing the number of objects in the clusters.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>an integer for the number of clusters.</p>
</td></tr>
<tr><td><code>eta</code></td>
<td>
<p>a number for the typicality exponent.</p>
</td></tr>
<tr><td><code>omega</code></td>
<td>
<p>a numeric vector of reference distances.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>an integer vector for the number of iterations in each start of the algorithm.</p>
</td></tr>
<tr><td><code>best.start</code></td>
<td>
<p>an integer for the index of start that produced the minimum objective functional.</p>
</td></tr>
<tr><td><code>func.val</code></td>
<td>
<p>a numeric vector for the objective function values in each start of the algorithm.</p>
</td></tr>
<tr><td><code>comp.time</code></td>
<td>
<p>a numeric vector for the execution time in each start of the algorithm.</p>
</td></tr>
<tr><td><code>stand</code></td>
<td>
<p>a logical value, <code>TRUE</code> shows that data set <code>x</code> contains the standardized values of raw data.</p>
</td></tr>
<tr><td><code>wss</code></td>
<td>
<p>a number for the within-cluster sum of squares for each cluster.</p>
</td></tr>
<tr><td><code>bwss</code></td>
<td>
<p>a number for the between-cluster sum of squares.</p>
</td></tr>
<tr><td><code>tss</code></td>
<td>
<p>a number for the total within-cluster sum of squares.</p>
</td></tr>
<tr><td><code>twss</code></td>
<td>
<p>a number for the total sum of squares.</p>
</td></tr>
<tr><td><code>algorithm</code></td>
<td>
<p>a string for the name of partitioning algorithm. It is &lsquo;PCM&rsquo; with this function.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>a string for the matched function call generating this &lsquo;ppclust&rsquo; object.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>PCM usually leads to acceptable results, although it suffers from stability problems if it is not initialized with the corresponding probabilistic algorithm. Therefore, it needs fuzzy partitioning results of a probabilistic algorithm, i.e. FCM, in order to compute some input arguments such as the cluster prototypes and mobilization scale parameter (Timm et al, 2004).
</p>


<h3>Author(s)</h3>

<p>Zeynel Cebeci, Alper Tuna Kavlak
</p>


<h3>References</h3>

<p>Arthur, D. &amp; Vassilvitskii, S. (2007). K-means++: The advantages of careful seeding, in <em>Proc. of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms</em>, p. 1027-1035. &lt;<a href="http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf">http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf</a>&gt;
</p>
<p>Krishnapuram, R. &amp; Keller, J.M. (1993). A possibilistic approach to clustering. <em>IEEE Transactions on Fuzzy Systems</em>, 1(2):98-110. &lt;doi:10.1109/91.227387&gt;
</p>
<p>Krishnapuram, R. &amp; Keller, J.M. (1996). The possibilistic c-means algorithm: insights and recommendations. <em>IEEE Transactions on Fuzzy Systems</em>, 4(3):385-393. &lt;doi:10.1109/91.531779&gt;
</p>
<p>Timm, H., Borgelt, C., Doring, C. &amp; Kruse, R. (2004). An extension to possibilistic fuzzy cluster analysis. <em>Fuzzy Sets and Systems</em>, 147 (1): 3-16. &lt;doi:10.1016/j.fss.2003.11.009&gt;
</p>
<p>Filippone, M., Masulli, F., &amp; Rovetta, S. (2007). Possibilistic clustering in feature space. In <em>Int. Workshop on Fuzzy Logic and Applications</em>, pp. 219-226. Springer, Berlin, Heidelberg. &lt;doi:10.1007/978-3-540-73400-0_27&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ekm">ekm</a></code>,
<code><a href="#topic+fcm">fcm</a></code>,
<code><a href="#topic+fcm2">fcm2</a></code>,
<code><a href="#topic+fpcm">fpcm</a></code>,
<code><a href="#topic+fpppcm">fpppcm</a></code>,
<code><a href="#topic+gg">gg</a></code>,
<code><a href="#topic+gk">gk</a></code>,
<code><a href="#topic+gkpfcm">gkpfcm</a></code>,
<code><a href="#topic+hcm">hcm</a></code>,
<code><a href="#topic+pca">pca</a></code>,
<code><a href="#topic+pcmr">pcmr</a></code>,
<code><a href="#topic+pfcm">pfcm</a></code>,
<code><a href="#topic+upfc">upfc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load the dataset X12
data(x12)

# Initialize the prototype matrix using K-means++
v &lt;- inaparc::kmpp(x12, k=2)$v
# Initialize the memberships degrees matrix 
u &lt;- inaparc::imembrand(nrow(x12), k=2)$u

# Run FCM with the initial prototypes and memberships
fcm.res &lt;- fcm(x12, centers=v, memberships=u, m=2)

# Run PCM with the prototypes and memberships from FCM run
pcm.res &lt;- pcm(x12, centers=fcm.res$v, memberships=fcm.res$u, eta=2)

# Display the typicality degrees
print(pcm.res$t)

# Plot the crisp memberships using maximum typicality degrees
plotcluster(pcm.res, mt="t", trans=TRUE )

# Plot the crisp memberships using the typicality degrees &gt; 0.5
plotcluster(pcm.res, mt="t", cm="threshold", tv=0.5)
</code></pre>

<hr>
<h2 id='pcmr'>
Possibilistic C-Means Clustering with Repulsion
</h2><span id='topic+pcmr'></span>

<h3>Description</h3>

<p>Partitions a numeric data set by using the Possibilistic C-Means with Repulsion (PCMR) clustering algorithm which has been proposed by Wachs et al (2006).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pcmr(x, centers, memberships, eta=2, K=1, omega, gamma=15,
    dmetric="sqeuclidean", pw=2, alginitv="kmpp", alginitu="imembrand",
    nstart=1, iter.max=1000, con.val=1e-09, 
    fixcent=FALSE, fixmemb=FALSE, stand=FALSE, numseed)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pcmr_+3A_x">x</code></td>
<td>
<p>a numeric vector, data frame or matrix.</p>
</td></tr>
<tr><td><code id="pcmr_+3A_centers">centers</code></td>
<td>
<p>an integer specifying the number of clusters or a numeric matrix containing the initial cluster centers.</p>
</td></tr>
<tr><td><code id="pcmr_+3A_memberships">memberships</code></td>
<td>
<p>a numeric matrix containing the initial membership degrees. If missing, it is internally generated.</p>
</td></tr>
<tr><td><code id="pcmr_+3A_eta">eta</code></td>
<td>
<p>a number greater than 1 to be used as the typicality exponent. The default is 2.</p>
</td></tr>
<tr><td><code id="pcmr_+3A_k">K</code></td>
<td>
<p>a number greater than 0 to be used as the weight of penalty term. The default is 1.</p>
</td></tr>
<tr><td><code id="pcmr_+3A_omega">omega</code></td>
<td>
<p>a numeric vector of reference distances. If missing, it is internally generated.</p>
</td></tr>
<tr><td><code id="pcmr_+3A_gamma">gamma</code></td>
<td>
<p>a number for normalization. Gamma value can be in the range of 0.1 and 200, but generally 10 is used. In Shapira &amp; Wachs(2004)
gamma = 15 gave the best accuracy for PCMR.</p>
</td></tr>
<tr><td><code id="pcmr_+3A_dmetric">dmetric</code></td>
<td>
<p>a string for the distance metric. The default is <span class="option">sqeuclidean</span> for the squared Euclidean distances. See <code><a href="#topic+get.dmetrics">get.dmetrics</a></code> for the alternative options.</p>
</td></tr>
<tr><td><code id="pcmr_+3A_pw">pw</code></td>
<td>
<p>a number for the power of Minkowski distance calculation. The default is 2 if the <code>dmetric</code> is <span class="option">minkowski</span>.</p>
</td></tr>
<tr><td><code id="pcmr_+3A_alginitv">alginitv</code></td>
<td>
<p>a string for the initialization of cluster prototypes matrix. The default is <span class="option">kmpp</span> for K-means++ initialization method (Arthur &amp; Vassilvitskii, 2007). For the list of alternative options see <code><a href="inaparc.html#topic+get.algorithms">get.algorithms</a></code>.</p>
</td></tr>
<tr><td><code id="pcmr_+3A_alginitu">alginitu</code></td>
<td>
<p>a string for the initialization of memberships degrees matrix. The default is <span class="option">imembrand</span> for random sampling of initial membership degrees.</p>
</td></tr>
<tr><td><code id="pcmr_+3A_nstart">nstart</code></td>
<td>
<p>an integer for the number of starts for clustering. The default is 1.</p>
</td></tr>
<tr><td><code id="pcmr_+3A_iter.max">iter.max</code></td>
<td>
<p>an integer for the maximum number of iterations allowed. The default is 1000.</p>
</td></tr>
<tr><td><code id="pcmr_+3A_con.val">con.val</code></td>
<td>
<p>a number for the convergence value between the iterations. The default is 1e-09.</p>
</td></tr>
<tr><td><code id="pcmr_+3A_fixcent">fixcent</code></td>
<td>
<p>a logical flag to fix the initial cluster centers. The default is <code>FALSE</code>. If it is <code>TRUE</code>, the initial centers are not changed in the successive starts of the algorithm when the <code>nstart</code> is greater than 1.</p>
</td></tr>
<tr><td><code id="pcmr_+3A_fixmemb">fixmemb</code></td>
<td>
<p>a logical flag to fix the initial membership degrees. The default is <code>FALSE</code>. If it is <code>TRUE</code>, the initial memberships are not changed in the successive starts of the algorithm when the <code>nstart</code> is greater than 1.</p>
</td></tr>
<tr><td><code id="pcmr_+3A_stand">stand</code></td>
<td>
<p>a logical flag to standardize data. Its default value is <code>FALSE</code>. If its value is <code>TRUE</code>, the data matrix <code>x</code> is standardized.</p>
</td></tr>
<tr><td><code id="pcmr_+3A_numseed">numseed</code></td>
<td>
<p>a seeding number to set the seed of R's random number generator.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Possibilistic C-Means with Repulsion (PCMR) aims to  minimize  the  intracluster  distances while maximizing the intercluster distances without using implicitly the constraints of FCM, but by adding a cluster repulsion term to the objective function of PCM (Wachs et al, 2006).  
</p>
<p><code class="reqn">J_{PCMR}(\mathbf{X}; \mathbf{V}, \mathbf{T})=\sum\limits_{i=1}^n t_{ij}^\eta \; d^2(\vec{x}_i, \vec{v}_j) + \sum\limits_{j=1}^k \Omega_j \sum\limits_{i=1}^n (1-t_{ij})^\eta + \gamma \sum\limits_{j=1}^k \sum\limits_{l=1, l \neq j}^k (1/d^2(\vec{v}_j, \vec{v}_l))</code>
</p>
<p>Where <code class="reqn">\gamma</code> is a weighting factor, and <code class="reqn">t_{ij}</code> satisfies:
</p>
<p><code class="reqn">t_{ij} \in [0,1], \forall j</code>
</p>
<p>The repulsion term is relevant if the clusters are close enough. When the distance increases it becomes smaller until it  is compensated by the  attraction of  the clusters. On the other  hand, if the clusters are sufficiently spread out, and the intercluster distance decreases (due to the first two terms), the attraction of the cluster can be compensated only by the repulsion term. 
</p>
<p>The update equation for the cluster prototypes:
</p>
<p><code class="reqn">\vec{v}_j =\frac{\sum\limits_{i=1}^n t_{ij} \vec{x}_i - \gamma \sum\limits_{j=1}^k v_j \; (1/ d^2(\vec{v}_j, \vec{v}_l))}{\sum\limits_{i=1}^n t_{ij} - \gamma \sum\limits_{j=1}^k v_j \; (1/ d^2(\vec{v}_j, \vec{v}_l))} \;;\; 1 \leq l \leq k</code>
</p>


<h3>Value</h3>

<p>an object of class &lsquo;ppclust&rsquo;, which is a list consists of the following items:
</p>
<table role = "presentation">
<tr><td><code>v</code></td>
<td>
<p>a numeric matrix containing the final cluster prototypes.</p>
</td></tr>
<tr><td><code>t</code></td>
<td>
<p>a numeric matrix containing the typicality degrees of the data objects.</p>
</td></tr>
<tr><td><code>d</code></td>
<td>
<p>a numeric matrix containing the distances of objects to the final cluster prototypes.</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>a numeric matrix containing the processed data set.</p>
</td></tr>
<tr><td><code>cluster</code></td>
<td>
<p>a numeric vector containing the cluster labels found by defuzzifying the typicality degrees of the objects.</p>
</td></tr>
<tr><td><code>csize</code></td>
<td>
<p>a numeric vector containing the number of objects in the clusters.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>an integer for the number of clusters.</p>
</td></tr>
<tr><td><code>eta</code></td>
<td>
<p>a number for the typicality exponent.</p>
</td></tr>
<tr><td><code>omega</code></td>
<td>
<p>a numeric vector of reference distances.</p>
</td></tr>
<tr><td><code>gamma</code></td>
<td>
<p>a number for normalization.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>an integer vector for the number of iterations in each start of the algorithm.</p>
</td></tr>
<tr><td><code>best.start</code></td>
<td>
<p>an integer for the index of start that produced the minimum objective functional.</p>
</td></tr>
<tr><td><code>func.val</code></td>
<td>
<p>a numeric vector for the objective function values in each start of the algorithm.</p>
</td></tr>
<tr><td><code>comp.time</code></td>
<td>
<p>a numeric vector for the execution time in each start of the algorithm.</p>
</td></tr>
<tr><td><code>stand</code></td>
<td>
<p>a logical value, <code>TRUE</code> shows that <code>x</code> data set contains the standardized values of raw data.</p>
</td></tr>
<tr><td><code>wss</code></td>
<td>
<p>a number for the within-cluster sum of squares for each cluster.</p>
</td></tr>
<tr><td><code>bwss</code></td>
<td>
<p>a number for the between-cluster sum of squares.</p>
</td></tr>
<tr><td><code>tss</code></td>
<td>
<p>a number for the total within-cluster sum of squares.</p>
</td></tr>
<tr><td><code>twss</code></td>
<td>
<p>a number for the total sum of squares.</p>
</td></tr>
<tr><td><code>algorithm</code></td>
<td>
<p>a string for the name of partitioning algorithm. It is &lsquo;PCM&rsquo; with this function.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>a string for the matched function call generating this &lsquo;ppclust&rsquo; object.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Zeynel Cebeci, A. Tuna Kavlak, Figen Yildiz
</p>


<h3>References</h3>

<p>Arthur, D. &amp; Vassilvitskii, S. (2007). K-means++: The advantages of careful seeding, in <em>Proc. of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms</em>, p. 1027-1035. &lt;<a href="http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf">http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf</a>&gt;
</p>
<p>Wachs, J., Shapira, O. &amp; Stern, H. (2006). A method to enhance the 'Possibilistic C-Means with Repulsion' algorithm based on cluster validity index. In <em>Applied Soft Computing Technologies: The Challenge of Complexity</em>, pp. 77-87. Springer, Berlin, Heidelberg. &lt;doi:10.1007/3-540-31662-0_6&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ekm">ekm</a></code>,
<code><a href="#topic+fcm">fcm</a></code>,
<code><a href="#topic+fcm2">fcm2</a></code>,
<code><a href="#topic+fpcm">fpcm</a></code>,
<code><a href="#topic+fpppcm">fpppcm</a></code>,
<code><a href="#topic+gg">gg</a></code>,
<code><a href="#topic+gk">gk</a></code>,
<code><a href="#topic+gkpfcm">gkpfcm</a></code>,
<code><a href="#topic+hcm">hcm</a></code>,
<code><a href="#topic+pca">pca</a></code>,
<code><a href="#topic+pcm">pcm</a></code>,
<code><a href="#topic+pfcm">pfcm</a></code>,
<code><a href="#topic+upfc">upfc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load data set X12
data(x12)

# Initialize the prototype matrix using K-means++
v &lt;- inaparc::kmpp(x12, k=2)$v
# Initialize the memberships degrees matrix 
u &lt;- inaparc::imembrand(nrow(x12), k=2)$u

# Run FCM with the initial prototypes and memberships
fcm.res &lt;- fcm(x12, centers=v, memberships=u, m=2)

# Run PCMR with the prototypes and memberships from FCM run
pcmr.res &lt;- pcmr(x12, centers=fcm.res$v, memberships=fcm.res$u, eta=2)

# Show the typicality degrees for the top 5 objects
head(pcmr.res$t, 5)

# Plot the crisp memberships using maximum typicality degrees
plotcluster(pcmr.res, mt="t", cm="max")

# Plot the crisp memberships using the typicality degrees &gt; 0.5
plotcluster(pcmr.res, mt="t", cm="threshold", tv=0.5)
</code></pre>

<hr>
<h2 id='pfcm'>
Possibilistic Fuzzy C-Means Clustering Algorithm
</h2><span id='topic+pfcm'></span>

<h3>Description</h3>

<p>Partitions a numeric data set by using the Possibilistic Fuzzy C-Means (PFCM) clustering algorithm proposed by Pal et al (2005).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pfcm(x, centers, memberships, m=2, eta=2, K=1, omega, a, b,
    dmetric="sqeuclidean", pw=2, alginitv="kmpp", alginitu="imembrand", 
    nstart=1, iter.max=1000, con.val=1e-09, 
    fixcent=FALSE, fixmemb=FALSE, stand=FALSE, numseed)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pfcm_+3A_x">x</code></td>
<td>
<p>a numeric vector, data frame or matrix.</p>
</td></tr>
<tr><td><code id="pfcm_+3A_centers">centers</code></td>
<td>
<p>an integer specifying the number of clusters or a numeric matrix containing the initial cluster centers.</p>
</td></tr>
<tr><td><code id="pfcm_+3A_memberships">memberships</code></td>
<td>
<p>a numeric matrix containing the initial membership degrees. If missing, it is internally generated.</p>
</td></tr>
<tr><td><code id="pfcm_+3A_m">m</code></td>
<td>
<p>a number greater than 1 to be used as the fuzziness exponent. The default is 2.</p>
</td></tr>
<tr><td><code id="pfcm_+3A_eta">eta</code></td>
<td>
<p>a number greater than 1 to be used as the typicality exponent. The default is 2.</p>
</td></tr>
<tr><td><code id="pfcm_+3A_a">a</code></td>
<td>
<p>a number for the relative importance of the fuzzy part of the objective function. The default is 1.</p>
</td></tr>
<tr><td><code id="pfcm_+3A_b">b</code></td>
<td>
<p>a number for the relative importance of the possibilistic part of the objective function. The default is 1.</p>
</td></tr>
<tr><td><code id="pfcm_+3A_k">K</code></td>
<td>
<p>a number greater than 0 to be used as the weight of penalty term. The default is 1.</p>
</td></tr>
<tr><td><code id="pfcm_+3A_omega">omega</code></td>
<td>
<p>a numeric vector of reference distances. If missing, it is internally generated.</p>
</td></tr>
<tr><td><code id="pfcm_+3A_dmetric">dmetric</code></td>
<td>
<p>a string for the distance metric. The default is <span class="option">sqeuclidean</span> for the squared Euclidean distances. See <code><a href="#topic+get.dmetrics">get.dmetrics</a></code> for the alternative options.</p>
</td></tr>
<tr><td><code id="pfcm_+3A_pw">pw</code></td>
<td>
<p>a number for the power of Minkowski distance calculation. The default is 2 if the <code>dmetric</code> is <span class="option">minkowski</span>.</p>
</td></tr>
<tr><td><code id="pfcm_+3A_alginitv">alginitv</code></td>
<td>
<p>a string for the initialization of cluster prototypes matrix. The default is <span class="option">kmpp</span> for K-means++ initialization method (Arthur &amp; Vassilvitskii, 2007). For the list of alternative options see <code><a href="inaparc.html#topic+get.algorithms">get.algorithms</a></code>.</p>
</td></tr>
<tr><td><code id="pfcm_+3A_alginitu">alginitu</code></td>
<td>
<p>a string for the initialization of memberships degrees matrix. The default is <span class="option">imembrand</span> for random sampling of initial membership degrees.</p>
</td></tr>
<tr><td><code id="pfcm_+3A_nstart">nstart</code></td>
<td>
<p>an integer for the number of starts for clustering. The default is 1.</p>
</td></tr>
<tr><td><code id="pfcm_+3A_iter.max">iter.max</code></td>
<td>
<p>an integer for the maximum number of iterations allowed. The default is 1000.</p>
</td></tr>
<tr><td><code id="pfcm_+3A_con.val">con.val</code></td>
<td>
<p>a number for the convergence value between the iterations. The default is 1e-09.</p>
</td></tr>
<tr><td><code id="pfcm_+3A_fixcent">fixcent</code></td>
<td>
<p>a logical flag to fix the initial cluster centers. The default is <code>FALSE</code>. If it is <code>TRUE</code>, the initial centers are not changed in the successive starts of the algorithm when the <code>nstart</code> is greater than 1.</p>
</td></tr>
<tr><td><code id="pfcm_+3A_fixmemb">fixmemb</code></td>
<td>
<p>a logical flag to fix the initial membership degrees. The default is <code>FALSE</code>. If it is <code>TRUE</code>, the initial memberships are not changed in the successive starts of the algorithm when the <code>nstart</code> is greater than 1.</p>
</td></tr>
<tr><td><code id="pfcm_+3A_stand">stand</code></td>
<td>
<p>a logical flag to standardize data. Its default value is <code>FALSE</code>. If its value is <code>TRUE</code>, the data matrix <code>x</code> is standardized.</p>
</td></tr>
<tr><td><code id="pfcm_+3A_numseed">numseed</code></td>
<td>
<p>a seeding number to set the seed of R's random number generator.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In FPCM, the constraint corresponding to the sum of all the typicality values of all data objects to a cluster must be equal to one causes problems; particularly for a big data set. In order to avoid this problem Pal et al (2005) proposed Possibilistic Fuzzy C-Means (PFCM) clustering algorithm with following objective function:
</p>
<p><code class="reqn">J_{PFCM}(\mathbf{X}; \mathbf{V}, \mathbf{U}, \mathbf{T})=\sum\limits_{j=1}^k \sum\limits_{i=1}^n (a \; u_{ij}^m + b \; t_{ij}^\eta) \; d^2(\vec{x}_i, \vec{v}_j) + \sum\limits_{j=1}^k \Omega_j \sum\limits_{i=1}^n (1-t_{ij})^\eta</code>
</p>
<p>The fuzzy membership degrees in the probabilistic part of the objective function <code class="reqn">J_{PFCM}</code> is calculated in the same way as in FCM, as follows:
</p>
<p><code class="reqn">u_{ij} =\Bigg[\sum\limits_{j=1}^k \Big(\frac{d^2(\vec{x}_i, \vec{v}_j)}{d^2(\vec{x}_i, \vec{v}_l)}\Big)^{1/(m-1)} \Bigg]^{-1} \;;\; 1 \leq i \leq n, \; 1 \leq l \leq k</code>
</p>
<p>The typicality degrees in the possibilistic part of the objective function <code class="reqn">J_{PFCM}</code> is calculated as follows:
</p>
<p><code class="reqn">t_{ij} =\Bigg[1 + \Big(\frac{b \; d^2(\vec{x}_i, \vec{v}_j)}{\Omega_j}\Big)^{1/(\eta -1)}\Bigg]^{-1} \;;\; 1 \leq i \leq n, \; 1 \leq j \leq k</code>
</p>
<p>The constraints with PFCM are:
</p>
<p><code class="reqn">0 \leq u_{ij}, t_{ij} \leq 1</code>
</p>
<p><code class="reqn">0 \leq \sum\limits_{i=1}^n u_{ij} \leq n \;\;;\; 1 \leq j \leq k</code>
</p>
<p><code class="reqn">0 \leq \sum\limits_{j=1}^k t_{ij} \leq k \;\;;\; 1 \leq i \leq n</code>
</p>
<p><code class="reqn">\sum\limits_{j=1}^k u_{ij} = 1 \;\;;\; 1 \leq i \leq n</code>
</p>
<p><code class="reqn">a</code> and <code class="reqn">b</code> are the coefficients to define the relative importance of fuzzy membership and typicality degrees for weighting the probabilistic and possibilistic terms of the objective function, <code class="reqn"> a &gt; 0; \; b &gt; 0</code>.
</p>
<p><code class="reqn">m</code> is the fuzzifier to specify the amount of fuzziness for the clustering; <code class="reqn">1\leq m\leq \infty</code>. It is usually chosen as 2. 
</p>
<p><code class="reqn">\eta</code> is the typicality exponent to specify the amount of typicality for the clustering; <code class="reqn">1\leq \eta\leq \infty</code>. It is usually chosen as 2. 
</p>
<p><code class="reqn">\Omega</code> is the possibilistic penalty terms for controlling the variance of the clusters.
</p>
<p>The update equation for cluster prototypes:
</p>
<p><code class="reqn">\vec{v}_j =\frac{\sum\limits_{i=1}^n (a \; u_{ij}^m + b \; t_{ij}^\eta) \; \vec{x}_i}{\sum\limits_{i=1}^n (a \; u_{ij}^m + b \; t_{ij}^\eta)} \;;\; 1 \leq j \leq k</code>
</p>


<h3>Value</h3>

<p>an object of class &lsquo;ppclust&rsquo;, which is a list consists of the following items:
</p>
<table role = "presentation">
<tr><td><code>v</code></td>
<td>
<p>a numeric matrix containing the final cluster prototypes.</p>
</td></tr>
<tr><td><code>t</code></td>
<td>
<p>a numeric matrix containing the typicality degrees of the data objects.</p>
</td></tr>
<tr><td><code>d</code></td>
<td>
<p>a numeric matrix containing the distances of objects to the final cluster prototypes.</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>a numeric matrix containing the processed data set.</p>
</td></tr>
<tr><td><code>cluster</code></td>
<td>
<p>a numeric vector containing the cluster labels found by defuzzifying the typicality degrees of the objects.</p>
</td></tr>
<tr><td><code>csize</code></td>
<td>
<p>a numeric vector for the number of objects in the clusters.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>an integer for the number of clusters.</p>
</td></tr>
<tr><td><code>m</code></td>
<td>
<p>a number for the used fuzziness exponent.</p>
</td></tr>
<tr><td><code>eta</code></td>
<td>
<p>a number for the used typicality exponent.</p>
</td></tr>
<tr><td><code>a</code></td>
<td>
<p>a number for the fuzzy part of the objective function.</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>a number for the possibilistic part of the objective function.</p>
</td></tr>
<tr><td><code>omega</code></td>
<td>
<p>a numeric vector of reference distances.</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>an integer vector for the number of iterations in each start of the algorithm.</p>
</td></tr>
<tr><td><code>best.start</code></td>
<td>
<p>an integer for the index of start that produced the minimum objective functional.</p>
</td></tr>
<tr><td><code>func.val</code></td>
<td>
<p>a numeric vector for the objective function values in each start of the algorithm.</p>
</td></tr>
<tr><td><code>comp.time</code></td>
<td>
<p>a numeric vector for the execution time in each start of the algorithm.</p>
</td></tr>
<tr><td><code>stand</code></td>
<td>
<p>a logical value, <code>TRUE</code> shows that <code>x</code> data set contains the standardized values of raw data.</p>
</td></tr>
<tr><td><code>wss</code></td>
<td>
<p>a number for the within-cluster sum of squares for each cluster.</p>
</td></tr>
<tr><td><code>bwss</code></td>
<td>
<p>a number for the between-cluster sum of squares.</p>
</td></tr>
<tr><td><code>tss</code></td>
<td>
<p>a number for the total within-cluster sum of squares.</p>
</td></tr>
<tr><td><code>twss</code></td>
<td>
<p>a number for the total sum of squares.</p>
</td></tr>
<tr><td><code>algorithm</code></td>
<td>
<p>a string for the name of partitioning algorithm. It is &lsquo;PCM&rsquo; with this function.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>a string for the matched function call generating this &lsquo;ppclust&rsquo; object.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Zeynel Cebeci, Alper Tuna Kavlak &amp; Figen Yildiz
</p>


<h3>References</h3>

<p>Arthur, D. &amp; Vassilvitskii, S. (2007). K-means++: The advantages of careful seeding, in <em>Proc. of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms</em>, p. 1027-1035. &lt;<a href="http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf">http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf</a>&gt;
</p>
<p>Pal, N. R., Pal, K. &amp; Bezdek, J. C. (2005). A possibilistic fuzzy c-means clustering algorithm. <em>IEEE
Trans. Fuzzy Systems</em>, 13 (4): 517-530. &lt;doi:10.1109/TFUZZ.2004.840099&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ekm">ekm</a></code>,
<code><a href="#topic+fcm">fcm</a></code>,
<code><a href="#topic+fcm2">fcm2</a></code>,
<code><a href="#topic+fpcm">fpcm</a></code>,
<code><a href="#topic+fpppcm">fpppcm</a></code>,
<code><a href="#topic+gg">gg</a></code>,
<code><a href="#topic+gk">gk</a></code>,
<code><a href="#topic+gkpfcm">gkpfcm</a></code>,
<code><a href="#topic+hcm">hcm</a></code>,
<code><a href="#topic+pca">pca</a></code>,
<code><a href="#topic+pcm">pcm</a></code>,
<code><a href="#topic+pcmr">pcmr</a></code>,
<code><a href="#topic+upfc">upfc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load the dataset X12
data(x12)

# Set the initial centers of clusters
v0 &lt;- matrix(nrow=2, ncol=2, c(-3.34, 1.67, 1.67, 0.00), byrow=FALSE)

# Run FCM with the initial centers in v0
res.fcm &lt;- fcm(x12, centers=v0, m=2)

# Run PFCM with the final centers and memberhips from FCM
res.pfcm &lt;- pfcm(x12, centers=res.fcm$v, memberships=res.fcm$u, m=2, eta=2)

# Show the typicality and fuzzy membership degrees from PFCM
res.pfcm$t
res.pfcm$u
</code></pre>

<hr>
<h2 id='plotcluster'>
Plot Clustering Results
</h2><span id='topic+plotcluster'></span>

<h3>Description</h3>

<p>Plots clustering results from a cluster analysis with &lsquo;ppclust&rsquo;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotcluster(objx, mt, cm, tv, cp=1, pt=19, trans=FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plotcluster_+3A_objx">objx</code></td>
<td>
<p>an object of &lsquo;ppclust&rsquo; class.</p>
</td></tr>
<tr><td><code id="plotcluster_+3A_mt">mt</code></td>
<td>
<p>a character to specify the membership type. The default is <span class="option">u</span> for fuzzy membership degrees. The alternative option is  <span class="option">t</span> for typicality degrees. The default is <span class="option">t</span> for the algorithms which produce both types of membership matrices.</p>
</td></tr>
<tr><td><code id="plotcluster_+3A_cm">cm</code></td>
<td>
<p>a character to specify the crisping method. The default is <span class="option">max</span> for using maximum degree of memberships for each data obejct. The alternative option is  <span class="option">threshold</span> for the membership degrees exceeding a user-specified threshold value with <code>tv</code>.</p>
</td></tr>
<tr><td><code id="plotcluster_+3A_tv">tv</code></td>
<td>
<p>a number specifying the threshold membership degree when the value of <code>cm</code> is selected as <span class="option">threshold</span>. The value of <code>tv</code> should be between 0 and 1. The default is <span class="option">max</span> for using maximum degree of memberships for each data obejct. The alternative option is <span class="option">threshold</span> for the 0.5.</p>
</td></tr>
<tr><td><code id="plotcluster_+3A_cp">cp</code></td>
<td>
<p>an integer for the index of available color palettes. The default is 1. The options are 2, 3, 4 and 5 for different color themes.</p>
</td></tr>
<tr><td><code id="plotcluster_+3A_pt">pt</code></td>
<td>
<p>an integer specifying the ASCII code of a point character to be used in plotting. The default is 19 for solid circles. Use <span class="option">#</span> for displaying the cluster with their cluster labels.</p>
</td></tr>
<tr><td><code id="plotcluster_+3A_trans">trans</code></td>
<td>
<p>a logical value for the type of plots. The default is <span class="option">FALSE</span> for solid point colors. The alternative option is <span class="option">TRUE</span> for transparent point colors.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Zeynel Cebeci
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Run FCM for 3 clusters on the data set Iris
res.fcm &lt;- fcm(x=iris[,-5], centers=3)

par(ask=TRUE)
# Plot the clustering results with solid colors
plotcluster(res.fcm, cp=1)

# Plot the same clustering results with transparent colors
plotcluster(res.fcm, cp=1, trans=TRUE)

# Plot the same clustering results for the memberships &gt; 0.75
plotcluster(res.fcm, cp=1, cm="threshold", tv=0.75, trans=TRUE)
par(ask=FALSE)
</code></pre>

<hr>
<h2 id='ppclust2'>
Convert &lsquo;ppclust&rsquo; objects to the other types of cluster objects
</h2><span id='topic+ppclust2'></span>

<h3>Description</h3>

<p>Converts an object of &lsquo;ppclust&rsquo; class to the other types of cluster objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ppclust2(objx, otype, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ppclust2_+3A_objx">objx</code></td>
<td>
<p>an object of <code>ppclust</code> class.</p>
</td></tr>
<tr><td><code id="ppclust2_+3A_otype">otype</code></td>
<td>
<p>target object class type for conversion.</p>
</td></tr>
<tr><td><code id="ppclust2_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an object of <code><a href="cluster.html#topic+fanny.object">fanny.object</a></code>, <code><a href="fclust.html#topic+summary.fclust">summary.fclust</a></code>, <code><a href="stats.html#topic+kmeans">kmeans</a></code> or <code><a href="vegclust.html#topic+vegclust">vegclust</a></code> class.
</p>


<h3>Author(s)</h3>

<p>Zeynel Cebeci
</p>


<h3>See Also</h3>

<p><code><a href="#topic+as.ppclust">as.ppclust</a></code>,
<code><a href="#topic+is.ppclust">is.ppclust</a></code>,
<code><a href="#topic+summary.ppclust">summary.ppclust</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
# Create a object of ppclust
opc &lt;- fcm(x=iris[,1:4], centers=3)

# Check the class of opc object
is.ppclust(opc)

# Convert ppclust object 'opc' to the fanny object
ofc &lt;- ppclust2(opc, otype="fanny")

# Check the class of 'ofc' for ppclust
is.ppclust(ofc)

# Check the class of 'ofc'
class(ofc)

# Convert ppclust object 'opc' to fclust object
ofc &lt;- ppclust2(opc, otype="fclust")

# Check the class of 'ofc'
class(ofc)
</code></pre>

<hr>
<h2 id='summary.ppclust'>
Summarize the clustering results
</h2><span id='topic+summary.ppclust'></span>

<h3>Description</h3>

<p>Summarizes the clustering results for an object which is an instance of &lsquo;ppclust&rsquo; class.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ppclust'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="summary.ppclust_+3A_object">object</code></td>
<td>
<p>an object of <code>ppclust</code> class to be summarized.</p>
</td></tr>
<tr><td><code id="summary.ppclust_+3A_...">...</code></td>
<td>
<p>additional arguments for S3 method summary.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>summary of the clustering results from the object of <code>ppclust</code> class.</p>


<h3>Author(s)</h3>

<p>Zeynel Cebeci
</p>


<h3>See Also</h3>

<p><code><a href="#topic+as.ppclust">as.ppclust</a></code>,
<code><a href="#topic+is.ppclust">is.ppclust</a></code>,
<code><a href="#topic+ppclust2">ppclust2</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
# Run FCM for three clusters
res.fcm &lt;- fcm(x=iris[,1:4], centers=3)

# Summarize the result
summary(res.fcm)
</code></pre>

<hr>
<h2 id='upfc'>
Unsupervised Possibilistic Fuzzy C-Means Clustering Algorithm
</h2><span id='topic+upfc'></span>

<h3>Description</h3>

<p>Partitions a numeric data set by using the Unsupervised Possibilistic Fuzzy C-Means clustering algorithm which has been proposed by Wu et al (2010).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>upfc(x, centers, memberships, m=2, eta=2, a, b,   
    dmetric="sqeuclidean", pw=2, alginitv="kmpp", alginitu="imembrand", 
    nstart=1, iter.max=1000, con.val=1e-09, 
    fixcent=FALSE, fixmemb=FALSE, stand=FALSE, numseed)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="upfc_+3A_x">x</code></td>
<td>
<p>a numeric vector, data frame or matrix.</p>
</td></tr>
<tr><td><code id="upfc_+3A_centers">centers</code></td>
<td>
<p>an integer specifying the number of clusters or a numeric matrix containing the initial cluster centers.</p>
</td></tr>
<tr><td><code id="upfc_+3A_memberships">memberships</code></td>
<td>
<p>a numeric matrix containing the initial membership degrees. If missing, it is internally generated.</p>
</td></tr>
<tr><td><code id="upfc_+3A_m">m</code></td>
<td>
<p>a number greater than 1 to be used as the fuzziness exponent. The default is 2.</p>
</td></tr>
<tr><td><code id="upfc_+3A_eta">eta</code></td>
<td>
<p>a number greater than 1 to be used as the typicality exponent. The default is 2.</p>
</td></tr>
<tr><td><code id="upfc_+3A_a">a</code></td>
<td>
<p>a number for the relative importance of the fuzzy part of the objective function. The default is 1.</p>
</td></tr>
<tr><td><code id="upfc_+3A_b">b</code></td>
<td>
<p>a number for the relative importance of the possibilistic part of the objective function. The default is 1.</p>
</td></tr>
<tr><td><code id="upfc_+3A_dmetric">dmetric</code></td>
<td>
<p>a string for the distance metric. The default is <span class="option">sqeuclidean</span> for the squared Euclidean distances. See <code><a href="#topic+get.dmetrics">get.dmetrics</a></code> for the alternative options.</p>
</td></tr>
<tr><td><code id="upfc_+3A_pw">pw</code></td>
<td>
<p>a number for the power of Minkowski distance calculation. The default is 2 if the <code>dmetric</code> is <span class="option">minkowski</span>.</p>
</td></tr>
<tr><td><code id="upfc_+3A_alginitv">alginitv</code></td>
<td>
<p>a string for the initialization of cluster prototypes matrix. The default is <span class="option">kmpp</span> for K-means++ initialization method (Arthur &amp; Vassilvitskii, 2007). For the list of alternative options see <code><a href="inaparc.html#topic+get.algorithms">get.algorithms</a></code>.</p>
</td></tr>
<tr><td><code id="upfc_+3A_alginitu">alginitu</code></td>
<td>
<p>a string for the initialization of memberships degrees matrix. The default is <span class="option">imembrand</span> for random sampling of initial membership degrees.</p>
</td></tr>
<tr><td><code id="upfc_+3A_nstart">nstart</code></td>
<td>
<p>an integer for the number of starts for clustering. The default is 1.</p>
</td></tr>
<tr><td><code id="upfc_+3A_iter.max">iter.max</code></td>
<td>
<p>an integer for the maximum number of iterations allowed. The default is 1000.</p>
</td></tr>
<tr><td><code id="upfc_+3A_con.val">con.val</code></td>
<td>
<p>a number for the convergence value between the iterations. The default is 1e-09.</p>
</td></tr>
<tr><td><code id="upfc_+3A_fixcent">fixcent</code></td>
<td>
<p>a logical flag to fix the initial cluster centers. The default is <code>FALSE</code>. If it is <code>TRUE</code>, the initial centers are not changed in the successive starts of the algorithm when the <code>nstart</code> is greater than 1.</p>
</td></tr>
<tr><td><code id="upfc_+3A_fixmemb">fixmemb</code></td>
<td>
<p>a logical flag to fix the initial membership degrees. The default is <code>FALSE</code>. If it is <code>TRUE</code>, the initial memberships are not changed in the successive starts of the algorithm when the <code>nstart</code> is greater than 1.</p>
</td></tr>
<tr><td><code id="upfc_+3A_stand">stand</code></td>
<td>
<p>a logical flag to standardize data. Its default value is <code>FALSE</code>. If its value is <code>TRUE</code>, the data matrix <code>x</code> is standardized.</p>
</td></tr>
<tr><td><code id="upfc_+3A_numseed">numseed</code></td>
<td>
<p>a seeding number to set the seed of R's random number generator.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Unsupervised Possibilistic Fuzzy C-Means (UPFC) is an extension of Possibilistic Clustering Algorithm (PCA) by Yang &amp; Wu (2006). Wu et al (2010) reported that PCA is very sensitive to initializations and sometimes generates coincident clusters, and proposed the algorithm UPFC to overcome this problem with inspiration by Pal et al's PFCM algorithm (Pal et al, 2005). The algorithm UPFC produces both possibilistic and probabilistic memberships simultaneously, and overcomes the noise sensitivity problem of FCM and the coincident clusters problem of PCA.
</p>
<p>The objective function of UPFC is:
</p>
<p><code class="reqn">J_{UPFC}(\mathbf{X}; \mathbf{V}, \mathbf{U}, \mathbf{T})=\sum\limits_{j=1}^k \sum\limits_{i=1}^n (a \; u_{ij}^m + b \; t_{ij}^\eta \; d^2(\vec{x}_i, \vec{v}_j) + \frac{\beta}{n^2\sqrt{k}} \; \sum\limits_{j=1}^k \sum\limits_{i=1}^n (t_{ij}^\eta \; log \; t_{ij}^\eta - t_{ij}^\eta)</code>
</p>
<p>Where:
</p>
<p><code class="reqn">u_{ij} =\Bigg[\sum\limits_{j=1}^k \Big(\frac{d^2(\vec{x}_i, \vec{v}_j)}{d^2(\vec{x}_i, \vec{v}_l)}\Big)^{1/(m-1)} \Bigg]^{-1} \;\;; 1 \leq i \leq n, \; 1 \leq l \leq k</code>
</p>
<p><code class="reqn">t_{ij} = exp\Big(- \frac{b \; n \; \sqrt{k} \; d^2(\vec{x}_i, \vec{v}_j)}{\beta}\Big) \;\;; {1\leq i\leq n},\; {1\leq j\leq k}</code>
</p>
<p>Where:
</p>
<p><code class="reqn">\beta = \frac{\sum\limits_{i=1}^n \; d^2(\vec{x}_i, \overline{x})}{n}</code> with <code class="reqn">\overline{x}=\frac{\sum\limits_{i=1}^n \vec{x}_i}{n}</code>
</p>
<p>The constraints with UPFC are:
</p>
<p><code class="reqn">0 \leq \sum\limits_{i=1}^n u_{ij} \leq n \;\;;\; 1 \leq j\leq k</code>
</p>
<p><code class="reqn">\sum\limits_{j=1}^k u_{ij} = 1 \;\;;\; 1 \leq i\leq n</code>
</p>
<p><code class="reqn">a</code> and <code class="reqn">b</code> are the coefficients to define the relative importance of fuzzy membership and typicality degrees in the objective function, <code class="reqn"> a &gt; 0; \; b &gt; 0</code>.
</p>
<p>The update equation for cluster prototypes:
</p>
<p><code class="reqn">\vec{v}_j =\frac{\sum\limits_{i=1}^n (a \; u_{ij}^m + b \; t_{ij}^\eta) \; \vec{x}_i}{\sum\limits_{i=1}^n (a \; u_{ij}^m + b \; t_{ij}^\eta)} \;\;; {1\leq j\leq k}</code>
</p>


<h3>Value</h3>

<p>an object of class &lsquo;ppclust&rsquo;, which is a list consists of the following items:
</p>
<table role = "presentation">
<tr><td><code>v</code></td>
<td>
<p>a numeric matrix containing the final cluster prototypes.</p>
</td></tr>
<tr><td><code>t</code></td>
<td>
<p>a numeric matrix containing the typicality degrees of the data objects.</p>
</td></tr>
<tr><td><code>d</code></td>
<td>
<p>a numeric matrix containing the distances of objects to the final cluster prototypes.</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>a numeric matrix containing the processed data set.</p>
</td></tr>
<tr><td><code>cluster</code></td>
<td>
<p>a numeric vector containing the cluster labels found by defuzzifying the typicality degrees of the objects.</p>
</td></tr>
<tr><td><code>csize</code></td>
<td>
<p>a numeric vector containing the number of objects in the clusters.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>an integer for the number of clusters.</p>
</td></tr>
<tr><td><code>m</code></td>
<td>
<p>a number for the used fuzziness exponent.</p>
</td></tr>
<tr><td><code>eta</code></td>
<td>
<p>a number for the used typicality exponent.</p>
</td></tr>
<tr><td><code>a</code></td>
<td>
<p>a number for the fuzzy part of the objective function.</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>a number for the possibilistic part of the objective function.</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>a numeric vector of normalization...</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>
<p>an integer vector for the number of iterations in each start of the algorithm.</p>
</td></tr>
<tr><td><code>best.start</code></td>
<td>
<p>an integer for the index of start that produced the minimum objective functional.</p>
</td></tr>
<tr><td><code>func.val</code></td>
<td>
<p>a numeric vector for the objective function values in each start of the algorithm.</p>
</td></tr>
<tr><td><code>comp.time</code></td>
<td>
<p>a numeric vector for the execution time in each start of the algorithm.</p>
</td></tr>
<tr><td><code>stand</code></td>
<td>
<p>a logical value, <code>TRUE</code> shows that <code>x</code> data set contains the standardized values of raw data.</p>
</td></tr>
<tr><td><code>wss</code></td>
<td>
<p>a number for the within-cluster sum of squares for each cluster.</p>
</td></tr>
<tr><td><code>bwss</code></td>
<td>
<p>a number for the between-cluster sum of squares.</p>
</td></tr>
<tr><td><code>tss</code></td>
<td>
<p>a number for the total within-cluster sum of squares.</p>
</td></tr>
<tr><td><code>twss</code></td>
<td>
<p>a number for the total sum of squares.</p>
</td></tr>
<tr><td><code>algorithm</code></td>
<td>
<p>a string for the name of partitioning algorithm. It is &lsquo;PCM&rsquo; with this function.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>a string for the matched function call generating this &lsquo;ppclust&rsquo; object.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Zeynel Cebeci, Figen Yildiz &amp; Alper Tuna Kavlak
</p>


<h3>References</h3>

<p>Arthur, D. &amp; Vassilvitskii, S. (2007). K-means++: The advantages of careful seeding, in <em>Proc. of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms</em>, p. 1027-1035. &lt;<a href="http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf">http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf</a>&gt;
</p>
<p>Pal, N. R., Pal, K. &amp; Bezdek, J. C. (2005). A possibilistic fuzzy c-means clustering algorithm. <em>IEEE
Trans. Fuzzy Systems</em>, 13 (4): 517-530. &lt;doi:10.1109/TFUZZ.2004.840099&gt;
</p>
<p>Yang, M. S. &amp; Wu, K. L. (2006). Unsupervised possibilistic clustering. <em>Pattern Recognition</em>, 39(1): 5-21. &lt;doi:10.1016/j.patcog.2005.07.005&gt;
</p>
<p>Wu, X., Wu, B., Sun, J. &amp; Fu, H. (2010). Unsupervised possibilistic fuzzy clustering. <em> J. of Information &amp; Computational Sci.</em>, 7 (5): 1075-1080. &lt;<a href="https://www.researchgate.net/publication/267691137_Unsupervised_Possibilistic_Fuzzy_Clustering">https://www.researchgate.net/publication/267691137_Unsupervised_Possibilistic_Fuzzy_Clustering</a>&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ekm">ekm</a></code>,
<code><a href="#topic+fcm">fcm</a></code>,
<code><a href="#topic+fcm2">fcm2</a></code>,
<code><a href="#topic+fpcm">fpcm</a></code>,
<code><a href="#topic+fpppcm">fpppcm</a></code>,
<code><a href="#topic+gg">gg</a></code>,
<code><a href="#topic+gk">gk</a></code>,
<code><a href="#topic+gkpfcm">gkpfcm</a></code>,
<code><a href="#topic+hcm">hcm</a></code>,
<code><a href="#topic+pca">pca</a></code>,
<code><a href="#topic+pcm">pcm</a></code>,
<code><a href="#topic+pcmr">pcmr</a></code>,
<code><a href="#topic+pfcm">pfcm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load dataset X16
data(x16)
x &lt;- x16[,-3]

# Initialize the prototype matrix using K-means++
v &lt;- inaparc::kmpp(x, k=2)$v
# Initialize the memberships degrees matrix 
u &lt;- inaparc::imembrand(nrow(x), k=2)$u

# Run UPFC
res.upfc &lt;- upfc(x, centers=v, memberships=u, eta=2)

# Display the fuzzy membership degrees
print(round(res.upfc$u, 2))

# Display the typicality degrees
print(round(res.upfc$t, 2))
</code></pre>

<hr>
<h2 id='x12'>
Synthetic data set of two variables
</h2><span id='topic+x12'></span>

<h3>Description</h3>

<p>A synthetic data set described by Pal et al (2005). It consists of two continous variables forming two well-separated clusters in addition to two noise. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(x12)</code></pre>


<h3>Format</h3>

<p>A data frame with 12 rows and 2 numeric variables:
</p>

<dl>
<dt>p1</dt><dd><p>a numeric variable ranging from -5.0 to 5.0</p>
</dd>
<dt>p2</dt><dd><p>a numeric variable ranging from -1.67 to 10.0</p>
</dd>
</dl>



<h3>Note</h3>

<p>The data set <code>x12</code> is recommended to test the performances of the possibilistic and probabilistic clustering algorithms.
</p>


<h3>References</h3>

<p>Pal, N. R., Pal, K. &amp; Bezdek, J. C. (2005). A possibilistic fuzzy c-means clustering algorithm. <em>IEEE
Trans. Fuzzy Systems</em>, 13 (4): 517-530. &lt;doi:10.1109/TFUZZ.2004.840099&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(x12)
# Descriptive statistics of the data set
summary(x12)
# Plot the data set
pairs(x12, pch=19, cex=2)
</code></pre>

<hr>
<h2 id='x16'>
Synthetic data set of two variables forming two clusters 
</h2><span id='topic+x16'></span>

<h3>Description</h3>

<p>A synthetic data set described by Yang &amp; Wu (2006). It consists of two continous variables forming two well-separated clusters in addition to two noise points. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(x16)</code></pre>


<h3>Format</h3>

<p>A data frame with 16 rows and 2 numeric variables:
</p>

<dl>
<dt>p1</dt><dd><p>a numeric variable ranging from 50 to 150</p>
</dd>
<dt>p2</dt><dd><p>a numeric variable ranging from 145 to 200</p>
</dd>
<dt>cl</dt><dd><p>a numeric variable ranging from 1 to 4</p>
</dd>
</dl>



<h3>Note</h3>

<p>The data set <code>x16</code> is recommended to test the performances of the possibilistic and noise clustering algorithms.
</p>


<h3>References</h3>

<p>Yang, M. S. &amp; Wu, K. L. (2006). Unsupervised possibilistic clustering. <em>Pattern Recognition</em>, 39(1): 5-21. &lt;doi:10.1016/j.patcog.2005.07.005&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(x16)
x &lt;- x16[,-3]
# descriptive statistics of the variables
summary(x)
# scatter plots for the variable pairs
pairs(x, col=x16$cl, pch=20, cex=2)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
