<!DOCTYPE html><html><head><title>Help for package drf</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {drf}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#create_dot_body'><p>Writes each node information</p>
If it is a leaf node: show it in different color, show number of samples, show leaf id
If it is a non-leaf node: show its splitting variable and splitting value</a></li>
<li><a href='#drf'><p>Distributional Random Forests</p></a></li>
<li><a href='#export_graphviz'><p>Export a tree in DOT format.</p>
This function generates a GraphViz representation of the tree,
which is then written into 'dot_string'.</a></li>
<li><a href='#get_sample_weights'><p>Given a trained forest and test data, compute the training sample weights for each test point.</p></a></li>
<li><a href='#get_tree'><p>Retrieve a single tree from a trained forest object.</p></a></li>
<li><a href='#leaf_stats.default'><p>A default leaf_stats for forests classes without a leaf_stats method</p>
that always returns NULL.</a></li>
<li><a href='#leaf_stats.drf'><p>Calculate summary stats given a set of samples for regression forests.</p></a></li>
<li><a href='#medianHeuristic'><p>Compute the median heuristic for the MMD bandwidth choice</p></a></li>
<li><a href='#plot.drf_tree'><p>Plot a DRF tree object.</p></a></li>
<li><a href='#predict.drf'><p>Predict with a drf forest</p></a></li>
<li><a href='#print.drf'><p>Print a DRF forest object.</p></a></li>
<li><a href='#print.drf_tree'><p>Print a DRF tree object.</p></a></li>
<li><a href='#split_frequencies'><p>Calculate which features the forest split on at each depth.</p></a></li>
<li><a href='#variable_importance'><p>Calculate a simple measure of 'importance' for each feature.</p></a></li>
<li><a href='#variableImportance'><p>Variable importance based on MMD</p></a></li>
<li><a href='#weighted.quantile'><p>Weighted quantiles</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Distributional Random Forests</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.0</td>
</tr>
<tr>
<td>Author:</td>
<td>Loris Michel, Domagoj Cevid</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Loris Michel &lt;michel@stat.math.ethz.ch&gt;</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/lorismichel/drf/issues">https://github.com/lorismichel/drf/issues</a></td>
</tr>
<tr>
<td>Description:</td>
<td>An implementation of distributional random forests as introduced in Cevid &amp; Michel &amp; Meinshausen &amp; Buhlmann (2020) &lt;<a href="https://doi.org/10.48550/arXiv.2005.14458">doi:10.48550/arXiv.2005.14458</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppEigen</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.4.4)</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, fastDummies, Matrix, methods, Rcpp (&ge; 0.12.15),
transport</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>Suggests:</td>
<td>DiagrammeR</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>GNU make</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/lorismichel/drf">https://github.com/lorismichel/drf</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-03-29 07:56:36 UTC; loris</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-03-29 09:30:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='create_dot_body'>Writes each node information
If it is a leaf node: show it in different color, show number of samples, show leaf id
If it is a non-leaf node: show its splitting variable and splitting value</h2><span id='topic+create_dot_body'></span>

<h3>Description</h3>

<p>Writes each node information
If it is a leaf node: show it in different color, show number of samples, show leaf id
If it is a non-leaf node: show its splitting variable and splitting value
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_dot_body(tree, index = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_dot_body_+3A_tree">tree</code></td>
<td>
<p>the tree to convert</p>
</td></tr>
<tr><td><code id="create_dot_body_+3A_index">index</code></td>
<td>
<p>the index of the current node</p>
</td></tr>
</table>

<hr>
<h2 id='drf'>Distributional Random Forests</h2><span id='topic+drf'></span>

<h3>Description</h3>

<p>Trains a distributional random forest that can be used to estimate
statistical functional F(P(Y | X)) for possibly multivariate response Y.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>drf(
  X,
  Y,
  num.trees = 500,
  splitting.rule = "FourierMMD",
  num.features = 10,
  bandwidth = NULL,
  response.scaling = TRUE,
  node.scaling = FALSE,
  sample.weights = NULL,
  clusters = NULL,
  equalize.cluster.weights = FALSE,
  sample.fraction = 0.5,
  mtry = min(ceiling(sqrt(ncol(X)) + 20), ncol(X)),
  min.node.size = 15,
  honesty = TRUE,
  honesty.fraction = 0.5,
  honesty.prune.leaves = TRUE,
  alpha = 0.05,
  imbalance.penalty = 0,
  ci.group.size = 2,
  compute.oob.predictions = TRUE,
  num.threads = NULL,
  seed = stats::runif(1, 0, .Machine$integer.max),
  compute.variable.importance = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="drf_+3A_x">X</code></td>
<td>
<p>The covariates used in the regression. Can be either a matrix of numerical values, or a data.frame with characters and factors. In the latter case,
one-hot-encoding will be implicitely used.</p>
</td></tr>
<tr><td><code id="drf_+3A_y">Y</code></td>
<td>
<p>The (multivariate) outcome. A matrix or data.frame of numeric values.</p>
</td></tr>
<tr><td><code id="drf_+3A_num.trees">num.trees</code></td>
<td>
<p>Number of trees grown in the forest. Default is 500.</p>
</td></tr>
<tr><td><code id="drf_+3A_splitting.rule">splitting.rule</code></td>
<td>
<p>a character value. The type of splitting rule used, can be either &quot;CART&quot; or &quot;FourierMMD&quot;.</p>
</td></tr>
<tr><td><code id="drf_+3A_num.features">num.features</code></td>
<td>
<p>a numeric value, in case of &quot;FourierMMD&quot;, the number of random features to sample.</p>
</td></tr>
<tr><td><code id="drf_+3A_bandwidth">bandwidth</code></td>
<td>
<p>a numeric value, the bandwidth of the Gaussian kernel used in case of &quot;FourierMMD&quot;, by default the value is NULL and the median heuristic is used.</p>
</td></tr>
<tr><td><code id="drf_+3A_response.scaling">response.scaling</code></td>
<td>
<p>a boolean value, should the reponses be globally scaled at first.</p>
</td></tr>
<tr><td><code id="drf_+3A_node.scaling">node.scaling</code></td>
<td>
<p>a boolean value, should the responses be scaled or not by node.</p>
</td></tr>
<tr><td><code id="drf_+3A_sample.weights">sample.weights</code></td>
<td>
<p>(experimental) Weights given to an observation in estimation.
If NULL, each observation is given the same weight. Default is NULL.</p>
</td></tr>
<tr><td><code id="drf_+3A_clusters">clusters</code></td>
<td>
<p>Vector of integers or factors specifying which cluster each observation corresponds to.
Default is NULL (ignored).</p>
</td></tr>
<tr><td><code id="drf_+3A_equalize.cluster.weights">equalize.cluster.weights</code></td>
<td>
<p>If FALSE, each unit is given the same weight (so that bigger
clusters get more weight). If TRUE, each cluster is given equal weight in the forest. In this case,
during training, each tree uses the same number of observations from each drawn cluster: If the
smallest cluster has K units, then when we sample a cluster during training, we only give a random
K elements of the cluster to the tree-growing procedure. When estimating average treatment effects,
each observation is given weight 1/cluster size, so that the total weight of each cluster is the
same. Note that, if this argument is FALSE, sample weights may also be directly adjusted via the
sample.weights argument. If this argument is TRUE, sample.weights must be set to NULL. Default is
FALSE.</p>
</td></tr>
<tr><td><code id="drf_+3A_sample.fraction">sample.fraction</code></td>
<td>
<p>Fraction of the data used to build each tree.
Note: If honesty = TRUE, these subsamples will
further be cut by a factor of honesty.fraction. Default is 0.5.</p>
</td></tr>
<tr><td><code id="drf_+3A_mtry">mtry</code></td>
<td>
<p>Number of variables tried for each split. Default is
<code class="reqn">\sqrt p + 20</code> where p is the number of variables.</p>
</td></tr>
<tr><td><code id="drf_+3A_min.node.size">min.node.size</code></td>
<td>
<p>A target for the minimum number of observations in each tree leaf. Note that nodes
with size smaller than min.node.size can occur, as in the original randomForest package.
Default is 5.</p>
</td></tr>
<tr><td><code id="drf_+3A_honesty">honesty</code></td>
<td>
<p>Whether to use honest splitting (i.e., sub-sample splitting). Default is TRUE.
For a detailed description of honesty, honesty.fraction, honesty.prune.leaves, and recommendations for
parameter tuning, see the grf reference for more information (initial source)
<a href="https://grf-labs.github.io/grf/REFERENCE.html#honesty-honesty-fraction-honesty-prune-leaves">algorithm reference</a>.</p>
</td></tr>
<tr><td><code id="drf_+3A_honesty.fraction">honesty.fraction</code></td>
<td>
<p>The fraction of data that will be used for determining splits if honesty = TRUE. Corresponds
to set J1 in the notation of the paper. Default is 0.5 (i.e. half of the data is used for
determining splits).</p>
</td></tr>
<tr><td><code id="drf_+3A_honesty.prune.leaves">honesty.prune.leaves</code></td>
<td>
<p>If TRUE, prunes the estimation sample tree such that no leaves
are empty. If FALSE, keep the same tree as determined in the splits sample (if an empty leave is encountered, that
tree is skipped and does not contribute to the estimate). Setting this to FALSE may improve performance on
small/marginally powered data, but requires more trees (note: tuning does not adjust the number of trees).
Only applies if honesty is enabled. Default is TRUE.</p>
</td></tr>
<tr><td><code id="drf_+3A_alpha">alpha</code></td>
<td>
<p>A tuning parameter that controls the maximum imbalance of a split. Default is 0.05.</p>
</td></tr>
<tr><td><code id="drf_+3A_imbalance.penalty">imbalance.penalty</code></td>
<td>
<p>A tuning parameter that controls how harshly imbalanced splits are penalized. Default is 0.</p>
</td></tr>
<tr><td><code id="drf_+3A_ci.group.size">ci.group.size</code></td>
<td>
<p>The forest will grow ci.group.size trees on each subsample.
In order to provide confidence intervals, ci.group.size must
be at least 2. Default is 2.</p>
</td></tr>
<tr><td><code id="drf_+3A_compute.oob.predictions">compute.oob.predictions</code></td>
<td>
<p>Whether OOB predictions on training set should be precomputed. Default is TRUE.</p>
</td></tr>
<tr><td><code id="drf_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads used in training. By default, the number of threads is set
to the maximum hardware concurrency.</p>
</td></tr>
<tr><td><code id="drf_+3A_seed">seed</code></td>
<td>
<p>The seed of the C++ random number generator.</p>
</td></tr>
<tr><td><code id="drf_+3A_compute.variable.importance">compute.variable.importance</code></td>
<td>
<p>boolean, should the variable importance be computed in the object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A trained distributional random forest object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Train a distributional random forest with CART splitting rule.
n &lt;- 100
p &lt;- 2
X &lt;- matrix(rnorm(n * p), n, p)
Y &lt;- X + matrix(rnorm(n * p), ncol=p)
drf.forest &lt;- drf(X = X, Y = Y)

# Predict conditional correlation.
X.test &lt;- matrix(0, 101, p)
X.test[, 1] &lt;- seq(-2, 2, length.out = 101)
cor.pred &lt;- predict(drf.forest, X.test, functional = "cor")

# Predict on out-of-bag training samples.
cor.oob.pred &lt;- predict(drf.forest,  functional = "cor")

# Train a distributional random forest with "FourierMMD" splitting rule.
n &lt;- 100
p &lt;- 2
X &lt;- matrix(rnorm(n * p), n, p)
Y &lt;- X + matrix(rnorm(n * p), ncol=p)
drf.forest &lt;- drf(X = X, Y = Y, splitting.rule = "FourierMMD", num.features = 10)

# Predict conditional correlation.
X.test &lt;- matrix(0, 101, p)
X.test[, 1] &lt;- seq(-2, 2, length.out = 101)
cor.pred &lt;- predict(drf.forest, X.test, functional = "cor")

# Predict on out-of-bag training samples.
cor.oob.pred &lt;- predict(drf.forest,  functional = "cor")

</code></pre>

<hr>
<h2 id='export_graphviz'>Export a tree in DOT format.
This function generates a GraphViz representation of the tree,
which is then written into 'dot_string'.</h2><span id='topic+export_graphviz'></span>

<h3>Description</h3>

<p>Export a tree in DOT format.
This function generates a GraphViz representation of the tree,
which is then written into 'dot_string'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>export_graphviz(tree)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="export_graphviz_+3A_tree">tree</code></td>
<td>
<p>the tree to convert</p>
</td></tr>
</table>

<hr>
<h2 id='get_sample_weights'>Given a trained forest and test data, compute the training sample weights for each test point.</h2><span id='topic+get_sample_weights'></span>

<h3>Description</h3>

<p>During normal prediction, these weights are computed as an intermediate step towards producing estimates.
This function allows for examining the weights directly, so they could be potentially be used as the
input to a different analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_sample_weights(forest, newdata = NULL, num.threads = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_sample_weights_+3A_forest">forest</code></td>
<td>
<p>The trained forest.</p>
</td></tr>
<tr><td><code id="get_sample_weights_+3A_newdata">newdata</code></td>
<td>
<p>Points at which predictions should be made. If NULL,
makes out-of-bag predictions on the training set instead
(i.e., provides predictions at Xi using only trees that did
not use the i-th training example).#' @param max.depth Maximum depth of splits to consider.</p>
</td></tr>
<tr><td><code id="get_sample_weights_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads used in training. If set to NULL, the software
automatically selects an appropriate amount.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A sparse matrix where each row represents a test sample, and each column is a sample in the
training data. The value at (i, j) gives the weight of training sample j for test sample i.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
p &lt;- 10
n &lt;- 100
X &lt;- matrix(2 * runif(n * p) - 1, n, p)
Y &lt;- (X[, 1] &gt; 0) + 2 * rnorm(n)
rrf &lt;- drf(X, matrix(Y,ncol=1), mtry = p)
sample.weights.oob &lt;- get_sample_weights(rrf)

n.test &lt;- 15
X.test &lt;- matrix(2 * runif(n.test * p) - 1, n.test, p)
sample.weights &lt;- get_sample_weights(rrf, X.test)

## End(Not run)

</code></pre>

<hr>
<h2 id='get_tree'>Retrieve a single tree from a trained forest object.</h2><span id='topic+get_tree'></span>

<h3>Description</h3>

<p>Retrieve a single tree from a trained forest object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_tree(forest, index)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_tree_+3A_forest">forest</code></td>
<td>
<p>The trained forest.</p>
</td></tr>
<tr><td><code id="get_tree_+3A_index">index</code></td>
<td>
<p>The index of the tree to retrieve.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A DRF tree object containing the below attributes.
drawn_samples: a list of examples that were used in training the tree. This includes
examples that were used in choosing splits, as well as the examples that populate the leaf
nodes. Put another way, if honesty is enabled, this list includes both subsamples from the
split (J1 and J2 in the notation of the paper).
num_samples: the number of examples used in training the tree.
nodes: a list of objects representing the nodes in the tree, starting with the root node. Each
node will contain an 'is_leaf' attribute, which indicates whether it is an interior or leaf node.
Interior nodes contain the attributes 'left_child' and 'right_child', which give the indices of
their children in the list, as well as 'split_variable', and 'split_value', which describe the
split that was chosen. Leaf nodes only have the attribute 'samples', which is a list of the
training examples that the leaf contains. Note that if honesty is enabled, this list will only
contain examples from the second subsample that was used to 'repopulate' the tree (J2 in the
notation of the paper).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Train a quantile forest.
n &lt;- 50
p &lt;- 10
X &lt;- matrix(rnorm(n * p), n, p)
Y &lt;- X[, 1] * rnorm(n)
q.forest &lt;- quantile_forest(X, Y, quantiles = c(0.1, 0.5, 0.9))

# Examine a particular tree.
q.tree &lt;- get_tree(q.forest, 3)
q.tree$nodes

## End(Not run)

</code></pre>

<hr>
<h2 id='leaf_stats.default'>A default leaf_stats for forests classes without a leaf_stats method
that always returns NULL.</h2><span id='topic+leaf_stats.default'></span>

<h3>Description</h3>

<p>A default leaf_stats for forests classes without a leaf_stats method
that always returns NULL.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default S3 method:
leaf_stats(forest, samples, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="leaf_stats.default_+3A_forest">forest</code></td>
<td>
<p>Any forest</p>
</td></tr>
<tr><td><code id="leaf_stats.default_+3A_samples">samples</code></td>
<td>
<p>The samples to include in the calculations.</p>
</td></tr>
<tr><td><code id="leaf_stats.default_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td></tr>
</table>

<hr>
<h2 id='leaf_stats.drf'>Calculate summary stats given a set of samples for regression forests.</h2><span id='topic+leaf_stats.drf'></span>

<h3>Description</h3>

<p>Calculate summary stats given a set of samples for regression forests.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'drf'
leaf_stats(forest, samples, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="leaf_stats.drf_+3A_forest">forest</code></td>
<td>
<p>The GRF forest</p>
</td></tr>
<tr><td><code id="leaf_stats.drf_+3A_samples">samples</code></td>
<td>
<p>The samples to include in the calculations.</p>
</td></tr>
<tr><td><code id="leaf_stats.drf_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named vector containing summary stats
</p>

<hr>
<h2 id='medianHeuristic'>Compute the median heuristic for the MMD bandwidth choice</h2><span id='topic+medianHeuristic'></span>

<h3>Description</h3>

<p>Compute the median heuristic for the MMD bandwidth choice
</p>


<h3>Usage</h3>

<pre><code class='language-R'>medianHeuristic(Y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="medianHeuristic_+3A_y">Y</code></td>
<td>
<p>the response matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the median heuristic
</p>

<hr>
<h2 id='plot.drf_tree'>Plot a DRF tree object.</h2><span id='topic+plot.drf_tree'></span>

<h3>Description</h3>

<p>Plot a DRF tree object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'drf_tree'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.drf_tree_+3A_x">x</code></td>
<td>
<p>The tree to plot</p>
</td></tr>
<tr><td><code id="plot.drf_tree_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td></tr>
</table>

<hr>
<h2 id='predict.drf'>Predict with a drf forest</h2><span id='topic+predict.drf'></span>

<h3>Description</h3>

<p>Predict with a drf forest
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'drf'
predict(
  object,
  newdata = NULL,
  transformation = NULL,
  functional = NULL,
  num.threads = NULL,
  custom.functional = function(y, w) apply(y, 2, sum(y * w)),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.drf_+3A_object">object</code></td>
<td>
<p>The trained drf forest.</p>
</td></tr>
<tr><td><code id="predict.drf_+3A_newdata">newdata</code></td>
<td>
<p>Points at which predictions should be made. If NULL, makes out-of-bag
predictions on the training set instead (i.e., provides predictions at
Xi using only trees that did not use the i-th training example). Note
that this matrix (or vector) should have the number of columns as the training
matrix, and that the columns must appear in the same order.</p>
</td></tr>
<tr><td><code id="predict.drf_+3A_transformation">transformation</code></td>
<td>
<p>a function giving a transformation of the responses, by default if NULL, the identity <code>function(y) y</code> is used.</p>
</td></tr>
<tr><td><code id="predict.drf_+3A_functional">functional</code></td>
<td>
<p>which type of statistical functional. One option between:
</p>

<ul>
<li><p>&quot;mean&quot;the conditional mean, the returned value is a list containing a matrix <code>mean</code> of size <code>n</code> x <code>f</code>,
where <code>n</code> denotes the number of observation in <code>newdata</code> and <code>f</code> the dimension of the <code>transformation</code>.
</p>
</li>
<li><p>&quot;sd&quot;the conditional standard deviation, the returned value is a list containing a matrix <code>sd</code> of size <code>n</code> x <code>f</code>,
where <code>n</code> denotes the number of observation in <code>newdata</code> and <code>f</code> the dimension of the <code>transformation</code>.
</p>
</li>
<li><p>&quot;quantile&quot;the conditional quantiles, the returned value is a list containing an array <code>quantile</code> of size <code>n</code> x <code>f</code>  x <code>q</code>,
where <code>n</code> denotes the number of observation in <code>newdata</code>, <code>f</code> the dimension of the <code>transformation</code> and <code>q</code> the number of desired quantiles.
</p>
</li>
<li><p>&quot;cor&quot;the conditional correlation, the returned value is a list containing an array <code>cor</code> of size <code>n</code> x <code>f</code>  x <code>f</code>,
where <code>n</code> denotes the number of observation in <code>newdata</code>, <code>f</code> the dimension of the <code>transformation</code>.
</p>
</li>
<li><p>&quot;cov&quot;the conditional covariance, the returned value is a list containing an array <code>cor</code> of size <code>n</code> x <code>f</code>  x <code>f</code>,
where <code>n</code> denotes the number of observation in <code>newdata</code>, <code>f</code> the dimension of the <code>transformation</code>.
</p>
</li>
<li><p>&quot;normalPredictionScore&quot;a prediction score based on an asymptotic normality assumption, the returned value is a list containing a list of functions <code>normalPredictionScore</code> of size <code>n</code>,
where <code>n</code> denotes the number of observation in <code>newdata</code>. Here the transformation should be uni-dimensional.
</p>
</li>
<li><p>&quot;custom&quot;a custom function provided by the user, the returned value is a list containing a matrix <code>custom</code> of size <code>n</code> x <code>f</code>,
where <code>n</code> denotes the number of observation in <code>newdata</code> and <code>f</code> the dimension of the output of the function <code>custom.functional</code>.
</p>
</li>
<li><p>&quot;MQ&quot;multivariate quantiles, return a list containing a matrix of the inputed ranks u (that should be provided as an argument of the predict function) along with a list of the different corresponding MQ (same size as u).
</p>
</li></ul>
</td></tr>
<tr><td><code id="predict.drf_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads used in training. If set to NULL, the software
automatically selects an appropriate amount.</p>
</td></tr>
<tr><td><code id="predict.drf_+3A_custom.functional">custom.functional</code></td>
<td>
<p>a function giving the custom functional when <code>functional</code> is set to &quot;custom&quot;. This should be a function <code>f(y,w)</code> using the
training response matrix <code>y</code> and the weights <code>w</code> at a single testing point.</p>
</td></tr>
<tr><td><code id="predict.drf_+3A_...">...</code></td>
<td>
<p>additional parameters.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing an entry with the same name as the functional selected.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Train a distributional random forest with CART splitting rule.
n &lt;- 100
p &lt;- 2
X &lt;- matrix(rnorm(n * p), n, p)
Y &lt;- X + matrix(rnorm(n * p), ncol=p)
drf.forest &lt;- drf(X = X, Y = Y)

# Predict conditional correlation.
X.test &lt;- matrix(0, 101, p)
X.test[, 1] &lt;- seq(-2, 2, length.out = 101)
cor.pred &lt;- predict(drf.forest, X.test, functional = "cor")

# Predict on out-of-bag training samples.
cor.oob.pred &lt;- predict(drf.forest,  functional = "cor")

# Train a distributional random forest with "FourierMMD" splitting rule.
n &lt;- 100
p &lt;- 2
X &lt;- matrix(rnorm(n * p), n, p)
Y &lt;- X + matrix(rnorm(n * p), ncol=p)
drf.forest &lt;- drf(X = X, Y = Y, splitting.rule = "FourierMMD", num.features = 10)

# Predict conditional correlation.
X.test &lt;- matrix(0, 101, p)
X.test[, 1] &lt;- seq(-2, 2, length.out = 101)
cor.pred &lt;- predict(drf.forest, X.test, functional = "cor")

# Predict on out-of-bag training samples.
cor.oob.pred &lt;- predict(drf.forest,  functional = "cor")


</code></pre>

<hr>
<h2 id='print.drf'>Print a DRF forest object.</h2><span id='topic+print.drf'></span>

<h3>Description</h3>

<p>Print a DRF forest object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'drf'
print(x, decay.exponent = 2, max.depth = 4, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.drf_+3A_x">x</code></td>
<td>
<p>The tree to print.</p>
</td></tr>
<tr><td><code id="print.drf_+3A_decay.exponent">decay.exponent</code></td>
<td>
<p>A tuning parameter that controls the importance of split depth.</p>
</td></tr>
<tr><td><code id="print.drf_+3A_max.depth">max.depth</code></td>
<td>
<p>The maximum depth of splits to consider.</p>
</td></tr>
<tr><td><code id="print.drf_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td></tr>
</table>

<hr>
<h2 id='print.drf_tree'>Print a DRF tree object.</h2><span id='topic+print.drf_tree'></span>

<h3>Description</h3>

<p>Print a DRF tree object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'drf_tree'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.drf_tree_+3A_x">x</code></td>
<td>
<p>The tree to print.</p>
</td></tr>
<tr><td><code id="print.drf_tree_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td></tr>
</table>

<hr>
<h2 id='split_frequencies'>Calculate which features the forest split on at each depth.</h2><span id='topic+split_frequencies'></span>

<h3>Description</h3>

<p>Calculate which features the forest split on at each depth.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>split_frequencies(forest, max.depth = 4)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="split_frequencies_+3A_forest">forest</code></td>
<td>
<p>The trained forest.</p>
</td></tr>
<tr><td><code id="split_frequencies_+3A_max.depth">max.depth</code></td>
<td>
<p>Maximum depth of splits to consider.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix of split depth by feature index, where each value
is the number of times the feature was split on at that depth.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Train a quantile forest.
n &lt;- 50
p &lt;- 10
X &lt;- matrix(rnorm(n * p), n, p)
Y &lt;- X[, 1] * rnorm(n)
q.forest &lt;- quantile_forest(X, Y, quantiles = c(0.1, 0.5, 0.9))

# Calculate the split frequencies for this forest.
split_frequencies(q.forest)

## End(Not run)

</code></pre>

<hr>
<h2 id='variable_importance'>Calculate a simple measure of 'importance' for each feature.</h2><span id='topic+variable_importance'></span>

<h3>Description</h3>

<p>A simple weighted sum of how many times feature i was split on at each depth in the forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>variable_importance(forest, decay.exponent = 2, max.depth = 4)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="variable_importance_+3A_forest">forest</code></td>
<td>
<p>The trained forest.</p>
</td></tr>
<tr><td><code id="variable_importance_+3A_decay.exponent">decay.exponent</code></td>
<td>
<p>A tuning parameter that controls the importance of split depth.</p>
</td></tr>
<tr><td><code id="variable_importance_+3A_max.depth">max.depth</code></td>
<td>
<p>Maximum depth of splits to consider.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list specifying an 'importance value' for each feature.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Train a quantile forest.
n &lt;- 50
p &lt;- 10
X &lt;- matrix(rnorm(n * p), n, p)
Y &lt;- X[, 1] * rnorm(n)
q.forest &lt;- quantile_forest(X, Y, quantiles = c(0.1, 0.5, 0.9))

# Calculate the 'importance' of each feature.
variable_importance(q.forest)

## End(Not run)

</code></pre>

<hr>
<h2 id='variableImportance'>Variable importance based on MMD</h2><span id='topic+variableImportance'></span>

<h3>Description</h3>

<p>compute an mmd-based variable importance for the drf fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>variableImportance(
  object,
  h = NULL,
  response.scaling = TRUE,
  type = "difference"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="variableImportance_+3A_object">object</code></td>
<td>
<p>an S3 object of class drf.</p>
</td></tr>
<tr><td><code id="variableImportance_+3A_h">h</code></td>
<td>
<p>the bandwidth parameter, default to NULL using then the median heuristic.</p>
</td></tr>
<tr><td><code id="variableImportance_+3A_response.scaling">response.scaling</code></td>
<td>
<p>a boolean value indicating if the responses should be scaled globally beforehand.</p>
</td></tr>
<tr><td><code id="variableImportance_+3A_type">type</code></td>
<td>
<p>the type of importance, could be either &quot;raw&quot;, the plain MMD values, &quot;relative&quot;, the ratios to the observed MMD or &quot;difference&quot;, the excess to the observed MMD</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector of variable importance values.
</p>

<hr>
<h2 id='weighted.quantile'>Weighted quantiles</h2><span id='topic+weighted.quantile'></span>

<h3>Description</h3>

<p>Weighted quantiles
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weighted.quantile(x, w, probs = seq(0, 1, 0.25), na.rm = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="weighted.quantile_+3A_x">x</code></td>
<td>
<p>a vector of observations</p>
</td></tr>
<tr><td><code id="weighted.quantile_+3A_w">w</code></td>
<td>
<p>a vector of weights</p>
</td></tr>
<tr><td><code id="weighted.quantile_+3A_probs">probs</code></td>
<td>
<p>the given probabilities for which we want to get quantiles</p>
</td></tr>
<tr><td><code id="weighted.quantile_+3A_na.rm">na.rm</code></td>
<td>
<p>should we remove missing values.</p>
</td></tr>
</table>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
