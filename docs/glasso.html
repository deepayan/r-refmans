<!DOCTYPE html><html><head><title>Help for package glasso</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {glasso}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#glasso'><p>Graphical lasso</p></a></li>
<li><a href='#glassopath'><p>Compute the Graphical lasso  along a path</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Graphical Lasso: Estimation of Gaussian Graphical Models</td>
</tr>
<tr>
<td>Version:</td>
<td>1.11</td>
</tr>
<tr>
<td>Author:</td>
<td>Jerome Friedman, Trevor Hastie and Rob Tibshirani</td>
</tr>
<tr>
<td>Description:</td>
<td>Estimation of a sparse inverse covariance matrix using a lasso (L1)
             penalty. Facilities are provided for estimates along a path of values
	     for the regularization parameter.</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Rob Tibshirani &lt;tibs@stat.stanford.edu&gt;</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="http://www-stat.stanford.edu/~tibs/glasso">http://www-stat.stanford.edu/~tibs/glasso</a></td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-09-24 21:37:23 UTC; robtibshirani</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-10-01 18:40:07 UTC</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
</table>
<hr>
<h2 id='glasso'>Graphical lasso</h2><span id='topic+glasso'></span>

<h3>Description</h3>

<p>Estimates a sparse inverse covariance matrix using a lasso (L1) penalty
</p>


<h3>Usage</h3>

<pre><code class='language-R'>glasso(s, rho, nobs=NULL, zero=NULL, thr=1.0e-4, maxit=1e4,  approx=FALSE, 
penalize.diagonal=TRUE, start=c("cold","warm"), 
w.init=NULL,wi.init=NULL, trace=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="glasso_+3A_s">s</code></td>
<td>
<p>Covariance matrix:p by p matrix (symmetric)</p>
</td></tr>
<tr><td><code id="glasso_+3A_rho">rho</code></td>
<td>
<p>(Non-negative) regularization parameter for lasso. rho=0 means no regularization. Can be a scalar (usual) or a symmetric p by p matrix, or a vector of length p. In the latter case, the penalty matrix has jkth element sqrt(rho[j]*rho[k]).</p>
</td></tr>
<tr><td><code id="glasso_+3A_nobs">nobs</code></td>
<td>
<p>Number of observations used in computation of the  covariance matrix s. This quantity is need to compute the value of  log-likelihood.
If not specified, loglik will be returned as NA.</p>
</td></tr>
<tr><td><code id="glasso_+3A_zero">zero</code></td>
<td>
<p>(Optional) indices of entries of inverse covariance to be constrained to be zero. The input should be a matrix with two columns, each row indicating
the indices of elements to be constrained to be zero. The solution must be symmetric, so you need only  specify one of (j,k) and (k,j). An entry in the zero matrix
overrides any entry in the rho matrix for a given element.</p>
</td></tr>
<tr><td><code id="glasso_+3A_thr">thr</code></td>
<td>
<p>Threshold for convergence. Default value is 1e-4.  Iterations stop when average absolute parameter change is less than thr * ave(abs(offdiag(s)))</p>
</td></tr>
<tr><td><code id="glasso_+3A_maxit">maxit</code></td>
<td>
<p>Maximum number of iterations of outer loop. Default 10,000</p>
</td></tr>
<tr><td><code id="glasso_+3A_approx">approx</code></td>
<td>
<p>Approximation flag: if true, computes Meinhausen-Buhlmann(2006)
approximation</p>
</td></tr>
<tr><td><code id="glasso_+3A_penalize.diagonal">penalize.diagonal</code></td>
<td>
<p>Should diagonal of inverse covariance be penalized?
Dafault TRUE.</p>
</td></tr>
<tr><td><code id="glasso_+3A_start">start</code></td>
<td>
<p>Type of start. Cold start is default. Using Warm start, can provide starting values for w and wi</p>
</td></tr>
<tr><td><code id="glasso_+3A_w.init">w.init</code></td>
<td>
<p>Optional starting values for estimated covariance matrix (p by p).
Only needed when start=&quot;warm&quot; is specified</p>
</td></tr>
<tr><td><code id="glasso_+3A_wi.init">wi.init</code></td>
<td>
<p>Optional starting values for estimated inverse covariance matrix (p by p)
Only needed when start=&quot;warm&quot; is specified</p>
</td></tr>
<tr><td><code id="glasso_+3A_trace">trace</code></td>
<td>
<p>Flag for printing out information as iterations proceed.
Default FALSE</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Estimates a sparse inverse covariance matrix using a lasso (L1) penalty,
using the approach of Friedman, Hastie and Tibshirani (2007).
The Meinhausen-Buhlmann (2006) approximation is also implemented. 
The algorithm can also be used to estimate a graph with missing edges,
by specifying which edges to omit in the zero argument, and setting rho=0.
Or both fixed zeroes for some elements and regularization on the other elements
can be specified.
</p>
<p>This version 1.7 uses a block diagonal screening rule to speed up
computations considerably. Details are given in the paper &quot;New insights
and fast computations for the graphical lasso&quot; by Daniela Witten, Jerry
Friedman, and Noah Simon, to appear in &quot;Journal of Computational and
Graphical Statistics&quot;. The idea is as follows: it is possible to quickly
check whether the solution to the graphical lasso problem will be block
diagonal, for a given value of the tuning parameter. If so, then one can
simply apply the graphical lasso algorithm to each block separately,
leading to massive speed improvements.
</p>


<h3>Value</h3>

<p>A list with components
</p>
<table>
<tr><td><code>w</code></td>
<td>
<p>Estimated covariance matrix</p>
</td></tr>
<tr><td><code>wi</code></td>
<td>
<p>Estimated inverse covariance matrix</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>
<p>Value of maximized log-likelihodo+penalty</p>
</td></tr>
<tr><td><code>errflag</code></td>
<td>
<p>Memory allocation error flag: 0 means no error; !=0 means
memory allocation error - no output returned</p>
</td></tr>
<tr><td><code>approx</code></td>
<td>
<p>Value of input argument approx</p>
</td></tr>
<tr><td><code>del</code></td>
<td>
<p>Change in parameter value at convergence</p>
</td></tr>
<tr><td><code>niter</code></td>
<td>
<p>Number of iterations of outer loop used by algorithm</p>
</td></tr>
</table>


<h3>References</h3>

<p>Jerome Friedman, Trevor Hastie and Robert Tibshirani (2007).
Sparse inverse covariance estimation with the lasso.
Biostatistics 2007. http://www-stat.stanford.edu/~tibs/ftp/graph.pdf
</p>
<p>Meinshausen, N. and  Buhlmann, P.(2006)
High dimensional graphs
and variable selection with the lasso.
Annals of Statistics,34, p1436-1462.
</p>
<p>Daniela Witten, Jerome Friedman, and Noah Simon (2011). New insights and
faster computations for the graphical lasso. To appear in Journal of
Computational and Graphical Statistics.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

set.seed(100)

x&lt;-matrix(rnorm(50*20),ncol=20)
s&lt;- var(x)
a&lt;-glasso(s, rho=.01)
aa&lt;-glasso(s,rho=.02, w.init=a$w, wi.init=a$wi)

# example with structural zeros and no regularization,
# from Whittaker's Graphical models book  page xxx.

s=c(10,1,5,4,10,2,6,10,3,10)
S=matrix(0,nrow=4,ncol=4)
S[row(S)&gt;=col(S)]=s
S=(S+t(S))
diag(S)&lt;-10
zero&lt;-matrix(c(1,3,2,4),ncol=2,byrow=TRUE)
a&lt;-glasso(S,0,zero=zero)
</code></pre>

<hr>
<h2 id='glassopath'>Compute the Graphical lasso  along a path</h2><span id='topic+glassopath'></span>

<h3>Description</h3>

<p>Estimates a sparse inverse covariance matrix using a lasso (L1) penalty, along a path
of values for the regularization parameter
</p>


<h3>Usage</h3>

<pre><code class='language-R'>glassopath(s, rholist=NULL, thr=1.0e-4, maxit=1e4,  approx=FALSE, 
penalize.diagonal=TRUE, w.init=NULL,wi.init=NULL, trace=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="glassopath_+3A_s">s</code></td>
<td>
<p>Covariance matrix:p by p matrix (symmetric)</p>
</td></tr>
<tr><td><code id="glassopath_+3A_rholist">rholist</code></td>
<td>
<p>Vector of non-negative regularization parameters for the lasso.
Should be  increasing from smallest to largest; actual path is computed from largest
to smallest value of rho).
If NULL, 10 values in a   (hopefully reasonable) range are used. Note that
the same parameter rholist[j] is used for all entries of the inverse covariance matrix;
different penalties for different entries are not allowed. </p>
</td></tr>
<tr><td><code id="glassopath_+3A_thr">thr</code></td>
<td>
<p>Threshold for convergence. Default value is 1e-4.  Iterations stop when average absolute parameter change is less than thr * ave(abs(offdiag(s)))</p>
</td></tr>
<tr><td><code id="glassopath_+3A_maxit">maxit</code></td>
<td>
<p>Maximum number of iterations of outer loop. Default 10,000</p>
</td></tr>
<tr><td><code id="glassopath_+3A_approx">approx</code></td>
<td>
<p>Approximation flag: if true, computes Meinhausen-Buhlmann(2006)
approximation</p>
</td></tr>
<tr><td><code id="glassopath_+3A_penalize.diagonal">penalize.diagonal</code></td>
<td>
<p>Should diagonal of inverse covariance be penalized?
Dafault TRUE.</p>
</td></tr>
<tr><td><code id="glassopath_+3A_w.init">w.init</code></td>
<td>
<p>Optional starting values for estimated covariance matrix (p by p).
Only needed when start=&quot;warm&quot; is specified</p>
</td></tr>
<tr><td><code id="glassopath_+3A_wi.init">wi.init</code></td>
<td>
<p>Optional starting values for estimated inverse covariance matrix (p by p)
Only needed when start=&quot;warm&quot; is specified</p>
</td></tr>
<tr><td><code id="glassopath_+3A_trace">trace</code></td>
<td>
<p>Flag for printing out information as iterations proceed. trace=0 means no printing; trace=1 means outer level printing;  trace=2 means  full printing
Default FALSE</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Estimates a sparse inverse covariance matrix using a lasso (L1) penalty, along a path of
regularization paramaters,
using the approach of Friedman, Hastie and Tibshirani (2007).
The Meinhausen-Buhlmann (2006) approximation is also implemented. 
The algorithm can also be used to estimate a graph with missing edges,
by specifying which edges to omit in the zero argument, and setting rho=0.
Or both fixed zeroes for some elements and regularization on the other elements
can be specified.
</p>
<p>This version 1.7 uses a block diagonal screening rule to speed up
computations considerably. Details are given in the paper &quot;New insights
and fast computations for the graphical lasso&quot; by Daniela Witten, Jerry
Friedman, and Noah Simon, to appear in &quot;Journal of Computational and
Graphical Statistics&quot;. The idea is as follows: it is possible to quickly
check whether the solution to the graphical lasso problem will be block
diagonal, for a given value of the tuning parameter. If so, then one can
simply apply the graphical lasso algorithm to each block separately,
leading to massive speed improvements.  
</p>


<h3>Value</h3>

<p>A list with components
</p>
<table>
<tr><td><code>w</code></td>
<td>
<p>Estimated covariance matrices, an array  of dimension (nrow(s),ncol(n), length(rholist))</p>
</td></tr>
<tr><td><code>wi</code></td>
<td>
<p>Estimated inverse covariance matrix, an array  of dimension (nrow(s),ncol(n), length(rholist))</p>
</td></tr>
<tr><td><code>approx</code></td>
<td>
<p>Value of input argument approx</p>
</td></tr>
<tr><td><code>rholist</code></td>
<td>
<p>Values of regularization parameter used</p>
</td></tr>
<tr><td><code>errflag</code></td>
<td>
<p>values of error flag (0 means no memory allocation error)</p>
</td></tr>
</table>


<h3>References</h3>

<p>Jerome Friedman, Trevor Hastie and Robert Tibshirani (2007).
Sparse inverse covariance estimation with the lasso.
Biostatistics 2007. http://www-stat.stanford.edu/~tibs/ftp/graph.pdf
</p>
<p>Meinshausen, N. and  Buhlmann, P.(2006)
High dimensional graphs
and variable selection with the lasso.
Annals of Statistics,34, p1436-1462.
</p>
<p>Daniela Witten, Jerome Friedman, Noah Simon (2011).
New insights and fast computation for the graphical lasso. To appear in
Journal of Computational and Graphical Statistics.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

set.seed(100)

x&lt;-matrix(rnorm(50*20),ncol=20)
s&lt;- var(x)
a&lt;-glassopath(s)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
