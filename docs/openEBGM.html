<!DOCTYPE html><html><head><title>Help for package openEBGM</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {openEBGM}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#autoHyper'><p>Semi-automated hyperparameter estimation</p></a></li>
<li><a href='#autoSquash'><p>Automated data squashing</p></a></li>
<li><a href='#caers'><p>Dietary supplement reports and products</p></a></li>
<li><a href='#caers_raw'><p>Raw CAERS data</p></a></li>
<li><a href='#ebgm'><p>Calculate EBGM scores</p></a></li>
<li><a href='#ebScores'><p>Construct an openEBGM object</p></a></li>
<li><a href='#exploreHypers'><p>Explore various hyperparameter estimates</p></a></li>
<li><a href='#hyperEM'><p>Estimate hyperparameters using an EM algorithm</p></a></li>
<li><a href='#negLL'><p>Likelihood without zero counts</p></a></li>
<li><a href='#negLLsquash'><p>Likelihood with data squashing and no zero counts</p></a></li>
<li><a href='#negLLzero'><p>Likelihood with zero counts</p></a></li>
<li><a href='#negLLzeroSquash'><p>Likelihood with data squashing &amp; zero counts</p></a></li>
<li><a href='#openEBGM-package'><p>openEBGM: EBGM Disproportionality Scores for Adverse Event Data Mining</p></a></li>
<li><a href='#plot.openEBGM'><p>Plot an openEBGM object</p></a></li>
<li><a href='#print.openEBGM'><p>Print an openEBGM object</p></a></li>
<li><a href='#processRaw'><p>Process raw data</p></a></li>
<li><a href='#Qn'><p>Calculate Qn</p></a></li>
<li><a href='#quantBisect'><p>Find quantiles of the posterior distribution</p></a></li>
<li><a href='#squashData'><p>Squash data for hyperparameter estimation</p></a></li>
<li><a href='#summary.openEBGM'><p>Summarize an openEBGM object</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>EBGM Disproportionality Scores for Adverse Event Data Mining</td>
</tr>
<tr>
<td>Version:</td>
<td>0.9.1</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>John Ihrie &lt;John.Ihrie@fda.hhs.gov&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>An implementation of DuMouchel's (1999) &lt;<a href="https://doi.org/10.1080%2F00031305.1999.10474456">doi:10.1080/00031305.1999.10474456</a>&gt;
  Bayesian data mining method for the market basket problem.
  Calculates Empirical Bayes Geometric Mean (EBGM) and posterior quantile scores
  using the Gamma-Poisson Shrinker (GPS) model to find unusually large cell
  counts in large, sparse contingency tables. Can be used to find unusually high
  reporting rates of adverse events associated with products. In general, can be
  used to mine any database where the co-occurrence of two variables or items is
  of interest. Also calculates relative and proportional reporting ratios.
  Builds on the work of the 'PhViD' package, from which much of the code is
  derived. Some of the added features include stratification to adjust for
  confounding variables and data squashing to improve computational efficiency.
  Includes an implementation of the EM algorithm for hyperparameter estimation
  loosely derived from the 'mederrRank' package.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.2.3)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://journal.r-project.org/archive/2017/RJ-2017-063/index.html">https://journal.r-project.org/archive/2017/RJ-2017-063/index.html</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Imports:</td>
<td>data.table (&ge; 1.10.0), ggplot2 (&ge; 2.2.1), stats (&ge; 3.2.3)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>DEoptim (&ge; 2.2), dplyr (&ge; 0.5.0), knitr (&ge; 1.15.1),
rmarkdown (&ge; 1.2), testthat (&ge; 1.0.2), tidyr (&ge; 0.6.0)</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-09-14 18:50:09 UTC; John.Ihrie</td>
</tr>
<tr>
<td>Author:</td>
<td>John Ihrie [cre, aut],
  Travis Canida [aut],
  Isma√Øl Ahmed [ctb] (author of 'PhViD' package (derived code)),
  Antoine Poncet [ctb] (author of 'PhViD'),
  Sergio Venturini [ctb] (author of 'mederrRank' package (derived code)),
  Jessica Myers [ctb] (author of 'mederrRank')</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-09-14 22:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='autoHyper'>Semi-automated hyperparameter estimation</h2><span id='topic+autoHyper'></span>

<h3>Description</h3>

<p><code>autoHyper</code> finds a single hyperparameter estimate using an algorithm
that evaluates results from multiple starting points (see
<code><a href="#topic+exploreHypers">exploreHypers</a></code>). The algorithm verifies that the optimization
converges within the bounds of the parameter space and that the chosen
estimate (smallest negative log-likelihood) is similar to at least
one (see <code>min_conv</code> argument) of the other convergent solutions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>autoHyper(
  data,
  theta_init,
  squashed = TRUE,
  zeroes = FALSE,
  N_star = 1,
  tol = c(0.05, 0.05, 0.2, 0.2, 0.025),
  min_conv = 1,
  param_limit = 100,
  max_pts = 20000,
  conf_ints = FALSE,
  conf_level = c("95", "80", "90", "99")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="autoHyper_+3A_data">data</code></td>
<td>
<p>A data frame from <code><a href="#topic+processRaw">processRaw</a></code> containing columns
named <em>N</em>, <em>E</em>, and (if squashed) <em>weight</em>.</p>
</td></tr>
<tr><td><code id="autoHyper_+3A_theta_init">theta_init</code></td>
<td>
<p>A data frame of initial hyperparameter guesses with
columns ordered as:
<code class="reqn">\alpha_1, \beta_1, \alpha_2, \beta_2, P</code>.</p>
</td></tr>
<tr><td><code id="autoHyper_+3A_squashed">squashed</code></td>
<td>
<p>A scalar logical (<code>TRUE</code> or <code>FALSE</code>) indicating
whether or not data squashing was used.</p>
</td></tr>
<tr><td><code id="autoHyper_+3A_zeroes">zeroes</code></td>
<td>
<p>A scalar logical specifying if zero counts are included.</p>
</td></tr>
<tr><td><code id="autoHyper_+3A_n_star">N_star</code></td>
<td>
<p>A positive scalar whole number value for the minimum count
size to be used for hyperparameter estimation. If zeroes are used, set
<code>N_star</code> to <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="autoHyper_+3A_tol">tol</code></td>
<td>
<p>A numeric vector of tolerances for determining how close the
chosen estimate must be to at least <code>min_conv</code> convergent solutions.
Order is <code class="reqn">\alpha_1</code>, <code class="reqn">\beta_1</code>, <code class="reqn">\alpha_2</code>, <code class="reqn">\beta_2</code>,
<code class="reqn">P</code>.</p>
</td></tr>
<tr><td><code id="autoHyper_+3A_min_conv">min_conv</code></td>
<td>
<p>A scalar positive whole number for defining the minimum
number of convergent solutions that must be close to the convergent
solution with the smallest negative log-likelihood. Must be at least one
and at most one less than the number of rows in <code>theta_init</code>.</p>
</td></tr>
<tr><td><code id="autoHyper_+3A_param_limit">param_limit</code></td>
<td>
<p>A scalar numeric value for the largest acceptable value
for the <code class="reqn">\alpha</code> and <code class="reqn">\beta</code> estimates. Used to help protect
against unreasonable/erroneous estimates.</p>
</td></tr>
<tr><td><code id="autoHyper_+3A_max_pts">max_pts</code></td>
<td>
<p>A scalar whole number for the largest number of data points
allowed. Used to help prevent extremely long run times.</p>
</td></tr>
<tr><td><code id="autoHyper_+3A_conf_ints">conf_ints</code></td>
<td>
<p>A scalar logical indicating if confidence intervals and
standard errors should be returned.</p>
</td></tr>
<tr><td><code id="autoHyper_+3A_conf_level">conf_level</code></td>
<td>
<p>A scalar string for the confidence level used if confidence
intervals are requested.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithm first attempts to find a consistently convergent
solution using <code><a href="stats.html#topic+nlminb">nlminb</a></code>. If it fails, it will next try
<code><a href="stats.html#topic+nlm">nlm</a></code>. If it still fails, it will try
<code><a href="stats.html#topic+optim">optim</a></code> (<em>method = &quot;BFGS&quot;</em>). If all three
approaches fail, the function returns an error message.
</p>
<p>Since this function runs multiple optimization procedures, it is
best to start with 5 or less initial starting points (rows in
<code>theta_init</code>). If the function runs in a reasonable amount of time,
this number can be increased.
</p>
<p>This function should not be used with very large data sets since
each optimization call will take a long time. <code><a href="#topic+squashData">squashData</a></code> can
be used first to reduce the size of the data.
</p>
<p>It is recommended to use <code>N_star = 1</code> when practical. Data
squashing (see <code><a href="#topic+squashData">squashData</a></code>) can be used to further reduce the
number of data points.
</p>
<p>Asymptotic normal confidence intervals, if requested, use standard
errors calculated from the observed Fisher information matrix as discussed
in DuMouchel (1999).
</p>


<h3>Value</h3>

<p>A list containing the following elements:
</p>

<ul>
<li><p><em>method</em>: A scalar character string for the method used to
find the hyperparameter estimate (possibilities are
&ldquo;<code>nlminb</code>&rdquo;, &ldquo;<code>nlm</code>&rdquo;, and
&ldquo;<code>bfgs</code>&rdquo;).
</p>
</li>
<li><p><em>estimates</em>: A named numeric vector of length 5 for the
hyperparameter estimate corresponding to the smallest log-likelihood.
</p>
</li>
<li><p><em>conf_int</em>: A data frame including the standard errors and
confidence limits. Only included if <code>conf_ints = TRUE</code>.
</p>
</li>
<li><p><em>num_close</em>: A scalar integer for the number of other
convergent solutions that were close (within tolerance) to the chosen
estimate.
</p>
</li>
<li><p><em>theta_hats</em>: A data frame for the estimates corresponding
to the initial starting points defined by <code>theta_init</code>. See
<code><a href="#topic+exploreHypers">exploreHypers</a></code>.
</p>
</li></ul>



<h3>References</h3>

<p>DuMouchel W (1999). &quot;Bayesian Data Mining in Large Frequency
Tables, With an Application to the FDA Spontaneous Reporting System.&quot;
<em>The American Statistician</em>, 53(3), 177-190.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+nlminb">nlminb</a></code>, <code><a href="stats.html#topic+nlm">nlm</a></code>, and
<code><a href="stats.html#topic+optim">optim</a></code> for optimization details
</p>
<p><code><a href="#topic+squashData">squashData</a></code> for data preparation
</p>
<p>Other hyperparameter estimation functions: 
<code><a href="#topic+exploreHypers">exploreHypers</a>()</code>,
<code><a href="#topic+hyperEM">hyperEM</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data.table::setDTthreads(2)  #only needed for CRAN checks
#Start with 2 or more guesses
theta_init &lt;- data.frame(
  alpha1 = c(0.5, 1),
  beta1  = c(0.5, 1),
  alpha2 = c(2,   3),
  beta2  = c(2,   3),
  p      = c(0.1, 0.2)
)
data(caers)
proc &lt;- processRaw(caers)
squashed &lt;- squashData(proc, bin_size = 300, keep_pts = 10)
squashed &lt;- squashData(squashed, count = 2, bin_size = 13, keep_pts = 10)
suppressWarnings(
  hypers &lt;- autoHyper(squashed, theta_init = theta_init)
)
print(hypers)

</code></pre>

<hr>
<h2 id='autoSquash'>Automated data squashing</h2><span id='topic+autoSquash'></span>

<h3>Description</h3>

<p><code>autoSquash</code> squashes data by calling <code><a href="#topic+squashData">squashData</a></code> once for
each count (<em>N</em>), removing the need to repeatedly squash the same data
set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>autoSquash(
  data,
  keep_pts = c(100, 75, 50, 25),
  cut_offs = c(500, 1000, 10000, 1e+05, 5e+05, 1e+06, 5e+06),
  num_super_pts = c(50, 75, 150, 500, 750, 1000, 2000, 5000)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="autoSquash_+3A_data">data</code></td>
<td>
<p>A data frame (typically from <code><a href="#topic+processRaw">processRaw</a></code>) containing
columns named <em>N</em>, <em>E</em>, and (possibly) <em>weight</em>. Can contain
additional columns, which will be ignored.</p>
</td></tr>
<tr><td><code id="autoSquash_+3A_keep_pts">keep_pts</code></td>
<td>
<p>A vector of whole numbers for the number of points to leave
unsquashed for each count (<em>N</em>). See the 'Details' section.</p>
</td></tr>
<tr><td><code id="autoSquash_+3A_cut_offs">cut_offs</code></td>
<td>
<p>A vector of whole numbers for the cutoff values of unsquashed
data used to determine how many &quot;super points&quot; to end up with after
squashing each count (<em>N</em>). See the 'Details' section.</p>
</td></tr>
<tr><td><code id="autoSquash_+3A_num_super_pts">num_super_pts</code></td>
<td>
<p>A vector of whole numbers for the number of
&quot;super points&quot; to end up with after squashing each count (<em>N</em>). Length
must be 1 more than length of <code>cut_offs</code>. See the 'Details' section.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <code><a href="#topic+squashData">squashData</a></code> for details on squashing a given
count (<em>N</em>).
</p>
<p>The elements in <code>keep_pts</code> determine how many points are left
unsquashed for each count (<em>N</em>). The first element in <code>keep_pts</code>
is used for the smallest <em>N</em> (usually 1). Each successive element is
used for each successive <em>N</em>. Once the last element is reached, it is
used for all other <em>N</em>.
</p>
<p>For counts that are squashed, <code>cut_offs</code> and
<code>num_super_pts</code> determine how the points are squashed. For instance,
by default, if a given <em>N</em> contains less than 500 points to be
squashed, then those points are squashed to 50 &quot;super points&quot;.
</p>


<h3>Value</h3>

<p>A data frame with column names <em>N</em>, <em>E</em>, and
<em>weight</em> containing the reduced data set.
</p>


<h3>References</h3>

<p>DuMouchel W, Pregibon D (2001). &quot;Empirical Bayes Screening for
Multi-item Associations.&quot; In <em>Proceedings of the Seventh ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining</em>, KDD '01,
pp. 67-76. ACM, New York, NY, USA. ISBN 1-58113-391-X.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+processRaw">processRaw</a></code> for data preparation and
<code><a href="#topic+squashData">squashData</a></code> for squashing individual counts
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data.table::setDTthreads(2)  #only needed for CRAN checks
data(caers)
proc &lt;- processRaw(caers)
table(proc$N)

squash1 &lt;- autoSquash(proc)
ftable(squash1[, c("N", "weight")])

## Not run: squash2 &lt;- autoSquash(proc, keep_pts = c(50, 5))
## Not run: ftable(squash2[, c("N", "weight")])

## Not run: 
  squash3 &lt;- autoSquash(proc, keep_pts = 100,
                        cut_offs = c(250, 500),
                        num_super_pts = c(20, 60, 125))

## End(Not run)
## Not run: ftable(squash3[, c("N", "weight")])

</code></pre>

<hr>
<h2 id='caers'>Dietary supplement reports and products</h2><span id='topic+caers'></span>

<h3>Description</h3>

<p>A dataset for dietary supplement adverse event reports from 2012 containing
CAERS product and adverse event reports as reported to the FDA. This
particular dataset contains only products which were reported to be dietary
supplements (industry code 54) reported in the year 2012. and includes 2874
unique product names and 1328 unique adverse events. There are a total of
3356 unique reports. In addition, there is also one stratification variable,
indicating whether the patient is male or female
</p>


<h3>Usage</h3>

<pre><code class='language-R'>caers
</code></pre>


<h3>Format</h3>

<p>A data frame with 20156 rows and 4 variables:
</p>

<dl>
<dt><code>id</code></dt><dd><p>Identification number</p>
</dd>
<dt><code>var1</code></dt><dd><p>Name of the product</p>
</dd>
<dt><code>var2</code></dt><dd><p>Name of the symptom/event category</p>
</dd>
<dt><code>strat1</code></dt><dd><p>Gender of the patient associated with report</p>
</dd>
</dl>



<h3>Details</h3>

<p>Further details about the data can be found using the links below.
</p>


<h3>Source</h3>

<p>CFSAN Adverse Event Reporting System (FDA Center for Food Safety and
Nutrition)
</p>
<p><a href="https://www.fda.gov/food/compliance-enforcement-food">https://www.fda.gov/food/compliance-enforcement-food</a>
</p>
<p><a href="https://www.fda.gov/media/97035/download">https://www.fda.gov/media/97035/download</a>
</p>

<hr>
<h2 id='caers_raw'>Raw CAERS data</h2><span id='topic+caers_raw'></span>

<h3>Description</h3>

<p>A small subset of raw, publicly available CAERS data used to demonstrate how
to prepare data for use by <span class="pkg">openEBGM</span>'s functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>caers_raw
</code></pre>


<h3>Format</h3>

<p>A data frame with 117 rows and 6 variables:
</p>

<dl>
<dt><code>RA_Report..</code></dt><dd><p>CAERS report identification number.</p>
</dd>
<dt><code>PRI_Reported.Brand.Product.Name</code></dt><dd><p>The verbatim brands and/or
product names indicated to have been used by the consumer reported
to have experienced the adverse event.</p>
</dd>
<dt><code>CI_Age.at.Adverse.Event</code></dt><dd><p>The age of the consumer reported to
have experienced the adverse event, in units specified by
<code>CI_Age.Unit</code>.</p>
</dd>
<dt><code>CI_Age.Unit</code></dt><dd><p>The time unit (day, week, month, year) of the
age provided in the <code>CI_Age.at.Adverse.Event</code> data field for the
consumer reported to have experienced the adverse event.</p>
</dd>
<dt><code>CI_Gender</code></dt><dd><p>The sex of the individual reported to have
experienced the adverse event.</p>
</dd>
<dt><code>SYM_One.Row.Coded.Symptoms</code></dt><dd><p>The symptom(s) experienced by the
injured consumer as specified by the reporter and coded by FDA according
to the Medical Data Dictionary for Regulatory Activities (MedDRA).</p>
</dd>
</dl>



<h3>Details</h3>

<p>The column names appear exactly as they would if you had used
<code>read.csv()</code> to pull the data directly from the website below.
</p>
<p>Further details about the data can be found using the links below.
</p>


<h3>Source</h3>

<p>CFSAN Adverse Event Reporting System (FDA Center for Food Safety and
Nutrition)
</p>
<p><a href="https://www.fda.gov/food/compliance-enforcement-food">https://www.fda.gov/food/compliance-enforcement-food</a>
</p>
<p><a href="https://www.fda.gov/media/97035/download">https://www.fda.gov/media/97035/download</a>
</p>

<hr>
<h2 id='ebgm'>Calculate EBGM scores</h2><span id='topic+ebgm'></span>

<h3>Description</h3>

<p><code>ebgm</code> calculates the Empirical Bayes Geometric Mean (<em>EBGM</em>),
which is &lsquo;the geometric mean of the empirical Bayes posterior
distribution of the &ldquo;true&rdquo; <em>RR</em>&rsquo; (DuMouchel 1999, see Eq.11). The
<em>EBGM</em> is essentially a version of the relative reporting ratio
(<em>RR</em>) that uses Bayesian shrinkage.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ebgm(theta_hat, N, E, qn, digits = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ebgm_+3A_theta_hat">theta_hat</code></td>
<td>
<p>A numeric vector of hyperparameter estimates (likely from
<code><a href="#topic+autoHyper">autoHyper</a></code> or from directly minimizing
<code><a href="#topic+negLLsquash">negLLsquash</a></code>) ordered as:
<code class="reqn">\alpha_1, \beta_1, \alpha_2, \beta_2, P</code>.</p>
</td></tr>
<tr><td><code id="ebgm_+3A_n">N</code></td>
<td>
<p>A whole number vector of actual counts from
<code><a href="#topic+processRaw">processRaw</a></code>.</p>
</td></tr>
<tr><td><code id="ebgm_+3A_e">E</code></td>
<td>
<p>A numeric vector of expected counts from <code><a href="#topic+processRaw">processRaw</a></code>.</p>
</td></tr>
<tr><td><code id="ebgm_+3A_qn">qn</code></td>
<td>
<p>A numeric vector of posterior probabilities that <code class="reqn">\lambda</code> came
from the first component of the mixture, given <em>N = n</em> (i.e., the
mixture fraction). See function <code><a href="#topic+Qn">Qn</a></code>.</p>
</td></tr>
<tr><td><code id="ebgm_+3A_digits">digits</code></td>
<td>
<p>A scalar whole number that determines the number of decimal
places used when rounding the results.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The hyperparameter estimates (<code>theta_hat</code>) are:
</p>

<ul>
<li><p><code class="reqn">\alpha_1, \beta_1</code>: Parameter estimates of the first
component of the prior distribution
</p>
</li>
<li><p><code class="reqn">\alpha_2, \beta_2</code>: Parameter estimates of the second
component
</p>
</li>
<li><p><code class="reqn">P</code>: Mixture fraction estimate of the prior distribution
</p>
</li></ul>



<h3>Value</h3>

<p>A numeric vector of EBGM scores.
</p>


<h3>References</h3>

<p>DuMouchel W (1999). &quot;Bayesian Data Mining in Large Frequency
Tables, With an Application to the FDA Spontaneous Reporting System.&quot;
<em>The American Statistician</em>, 53(3), 177-190.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+autoHyper">autoHyper</a></code>,  <code><a href="#topic+exploreHypers">exploreHypers</a></code>,
<code><a href="#topic+negLLsquash">negLLsquash</a></code>, <code><a href="#topic+negLL">negLL</a></code>,
<code><a href="#topic+negLLzero">negLLzero</a></code>, and <code><a href="#topic+negLLzeroSquash">negLLzeroSquash</a></code> for
hyperparameter estimation.
</p>
<p><code><a href="#topic+processRaw">processRaw</a></code> for finding counts.
</p>
<p><code><a href="#topic+Qn">Qn</a></code> for finding mixture fractions.
</p>
<p>Other posterior distribution functions: 
<code><a href="#topic+Qn">Qn</a>()</code>,
<code><a href="#topic+quantBisect">quantBisect</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data.table::setDTthreads(2)  #only needed for CRAN checks
theta_init &lt;- data.frame(
  alpha1 = c(0.5, 1),
  beta1  = c(0.5, 1),
  alpha2 = c(2,   3),
  beta2  = c(2,   3),
  p      = c(0.1, 0.2)
)
data(caers)
proc &lt;- processRaw(caers)
squashed &lt;- squashData(proc, bin_size = 300, keep_pts = 10)
squashed &lt;- squashData(squashed, count = 2, bin_size = 13, keep_pts = 10)
theta_hat &lt;- autoHyper(data = squashed, theta_init = theta_init)$estimates
qn &lt;- Qn(theta_hat, N = proc$N, E = proc$E)
proc$EBGM &lt;- ebgm(theta_hat, N = proc$N, E = proc$E, qn = qn)
head(proc)

</code></pre>

<hr>
<h2 id='ebScores'>Construct an openEBGM object</h2><span id='topic+ebScores'></span>

<h3>Description</h3>

<p><code>ebScores</code> calculates EBGM scores as well as the quantiles from the
posterior distribution and returns an object of class openEBGM.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ebScores(processed, hyper_estimate, quantiles = c(5, 95), digits = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ebScores_+3A_processed">processed</code></td>
<td>
<p>A data frame resulting from running <code><a href="#topic+processRaw">processRaw</a></code>.</p>
</td></tr>
<tr><td><code id="ebScores_+3A_hyper_estimate">hyper_estimate</code></td>
<td>
<p>A list resulting from running <code><a href="#topic+autoHyper">autoHyper</a></code>.</p>
</td></tr>
<tr><td><code id="ebScores_+3A_quantiles">quantiles</code></td>
<td>
<p>Either a numeric vector of desired quantiles to be
calculated from the posterior distribution or NULL for no calculation of
quantiles.</p>
</td></tr>
<tr><td><code id="ebScores_+3A_digits">digits</code></td>
<td>
<p>A whole number scalar specifying how many decimal places to
use for rounding <em>EBGM</em> and the quantiles scores.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function takes the processed data as well as the hyperparameter
estimates and instantiates an object of class openEBGM. This object then
contains additional calculations, such as the EBGM score, and the quantiles
that are supplied by the quantiles parameter at the time of calling the
function.
</p>
<p>The function allows for the choice of an arbitrary amount of
quantiles or no quantiles at all to be calculated. This may be helpful for
large datasets, or when the EBGM score is the only metric of interest.
</p>


<h3>Value</h3>

<p>An openEBGM object (class S3) containing:
</p>

<ul>
<li><p><em>data</em>: A data frame containing the results (scores, etc.).
</p>
</li>
<li><p><em>hyper_parameters</em>: A list containing the hyperparameter
estimation results.
</p>
</li>
<li><p><em>quantiles</em>: The chosen percentiles.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>data.table::setDTthreads(2)  #only needed for CRAN checks
theta_init &lt;- data.frame(
  alpha1 = c(0.5, 1),
  beta1  = c(0.5, 1),
  alpha2 = c(2,   3),
  beta2  = c(2,   3),
  p      = c(0.1, 0.2)
)
data(caers)
proc &lt;- processRaw(caers)
squashed &lt;- squashData(proc, bin_size = 300, keep_pts = 10)
squashed &lt;- squashData(squashed, count = 2, bin_size = 13, keep_pts = 10)
suppressWarnings(
  hypers &lt;- autoHyper(data = squashed, theta_init = theta_init)
)
obj &lt;- ebScores(processed = proc, hyper_estimate = hypers, quantiles = 5)
print(obj)

</code></pre>

<hr>
<h2 id='exploreHypers'>Explore various hyperparameter estimates</h2><span id='topic+exploreHypers'></span>

<h3>Description</h3>

<p><code>exploreHypers</code> finds hyperparameter estimates using a variety of
starting points to examine the consistency of the optimization procedure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>exploreHypers(
  data,
  theta_init,
  squashed = TRUE,
  zeroes = FALSE,
  N_star = 1,
  method = c("nlminb", "nlm", "bfgs"),
  param_limit = 100,
  max_pts = 20000,
  std_errors = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="exploreHypers_+3A_data">data</code></td>
<td>
<p>A data frame from <code><a href="#topic+processRaw">processRaw</a></code> containing columns
named <em>N</em>, <em>E</em>, and (if squashed) <em>weight</em>.</p>
</td></tr>
<tr><td><code id="exploreHypers_+3A_theta_init">theta_init</code></td>
<td>
<p>A data frame of initial hyperparameter guesses with
columns ordered as:
<code class="reqn">\alpha_1, \beta_1, \alpha_2, \beta_2, P</code>.</p>
</td></tr>
<tr><td><code id="exploreHypers_+3A_squashed">squashed</code></td>
<td>
<p>A scalar logical (<code>TRUE</code> or <code>FALSE</code>) indicating
whether or not data squashing was used.</p>
</td></tr>
<tr><td><code id="exploreHypers_+3A_zeroes">zeroes</code></td>
<td>
<p>A scalar logical specifying if zero counts are included.</p>
</td></tr>
<tr><td><code id="exploreHypers_+3A_n_star">N_star</code></td>
<td>
<p>A positive scalar whole number value for the minimum count
size to be used for hyperparameter estimation. If zeroes are used, set
<code>N_star</code> to <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="exploreHypers_+3A_method">method</code></td>
<td>
<p>A scalar string indicating which optimization procedure is to
be used. Choices are <code>"nlminb"</code>, <code>"nlm"</code>, or <code>"bfgs"</code>.</p>
</td></tr>
<tr><td><code id="exploreHypers_+3A_param_limit">param_limit</code></td>
<td>
<p>A scalar numeric value for the largest acceptable value
for the <code class="reqn">\alpha</code> and <code class="reqn">\beta</code> estimates. Used to help protect
against unreasonable/erroneous estimates.</p>
</td></tr>
<tr><td><code id="exploreHypers_+3A_max_pts">max_pts</code></td>
<td>
<p>A scalar whole number for the largest number of data points
allowed. Used to help prevent extremely long run times.</p>
</td></tr>
<tr><td><code id="exploreHypers_+3A_std_errors">std_errors</code></td>
<td>
<p>A scalar logical indicating if standard errors should be
returned for the hyperparameter estimates.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>method</code> argument determines which optimization procedure
is used. All the options use functions from the <code><a href="stats.html#topic+stats">stats</a></code>
package:
</p>

<ul>
<li><p><code>"nlminb":</code> <code><a href="stats.html#topic+nlminb">nlminb</a></code>
</p>
</li>
<li><p><code>"nlm":</code> <code><a href="stats.html#topic+nlm">nlm</a></code>
</p>
</li>
<li><p><code>"bfgs":</code> <code><a href="stats.html#topic+optim">optim</a></code> (<em>method = &quot;BFGS&quot;</em>)
</p>
</li></ul>

<p>Since this function runs multiple optimization procedures, it is
best to start with 5 or less initial starting points (rows in
<code>theta_init</code>). If the function runs in a reasonable amount of time,
this number can be increased.
</p>
<p>This function should not be used with very large data sets unless
data squashing is used first since each optimization call will take a long
time.
</p>
<p>It is recommended to use <code>N_star = 1</code> when practical. Data
squashing (see <code><a href="#topic+squashData">squashData</a></code>) can be used to reduce the number
of data points.
</p>
<p>The <em>converge</em> column in the resulting data frame was
determined by examining the convergence <em>code</em> of the chosen
optimization method. In some instances, the code is somewhat ambiguous. The
determination of <em>converge</em> was intended to be conservative (leaning
towards FALSE when questionable). See the documentation for the chosen
method for details about <em>code</em>.
</p>
<p>Standard errors, if requested, are calculated using the observed
Fisher information matrix as discussed in DuMouchel (1999).
</p>


<h3>Value</h3>

<p>A list including the data frame <code>estimates</code> of hyperparameter
estimates corresponding to the initial guesses from <code>theta_init</code> (plus
convergence results):
</p>

<ul>
<li><p><em>code</em>: The convergence code returned by the chosen
optimization function (see <code><a href="stats.html#topic+nlminb">nlminb</a></code>,
<code><a href="stats.html#topic+nlm">nlm</a></code>, and <code><a href="stats.html#topic+optim">optim</a></code> for details).
</p>
</li>
<li><p><em>converge</em>: A logical indicating whether or not convergence
was reached. See &quot;Details&quot; section for more information.
</p>
</li>
<li><p><em>in_bounds</em>: A logical indicating whether or not the
estimates were within the bounds of the parameter space (upper bound
for <code class="reqn">\alpha_1, \beta_1, \alpha_2, and \beta_2</code> was determined by
the <code>param_limit</code> argument).
</p>
</li>
<li><p><em>minimum</em>: The negative log-likelihood value corresponding
to the estimated optimal value of the hyperparameter.
</p>
</li></ul>

<p>Also returns the data frame <code>std_errs</code> if standard errors are
requested.
</p>


<h3>Warning</h3>

<p>Make sure to properly specify the <code>squashed</code>,
<code>zeroes</code>, and <code>N_star</code> arguments for your data set, since these
will determine the appropriate likelihood function. Also, this function
will not filter out data points. For instance, if you use <code>N_star = 2</code>
you must filter out the ones and zeroes (if present) from <code>data</code> prior
to using this function.
</p>


<h3>References</h3>

<p>DuMouchel W (1999). &quot;Bayesian Data Mining in Large Frequency
Tables, With an Application to the FDA Spontaneous Reporting System.&quot;
<em>The American Statistician</em>, 53(3), 177-190.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+nlminb">nlminb</a></code>, <code><a href="stats.html#topic+nlm">nlm</a></code>, and
<code><a href="stats.html#topic+optim">optim</a></code> for optimization details
</p>
<p><code><a href="#topic+squashData">squashData</a></code> for data preparation
</p>
<p>Other hyperparameter estimation functions: 
<code><a href="#topic+autoHyper">autoHyper</a>()</code>,
<code><a href="#topic+hyperEM">hyperEM</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data.table::setDTthreads(2)  #only needed for CRAN checks
#Start with 2 or more guesses
theta_init &lt;- data.frame(
  alpha1 = c(0.5, 1),
  beta1  = c(0.5, 1),
  alpha2 = c(2,   3),
  beta2  = c(2,   3),
  p      = c(0.1, 0.2)
)
data(caers)
proc &lt;- processRaw(caers)
squashed &lt;- squashData(proc, bin_size = 300, keep_pts = 10)
squashed &lt;- squashData(squashed, count = 2, bin_size = 13, keep_pts = 10)
suppressWarnings(
  exploreHypers(squashed, theta_init = theta_init)
)

</code></pre>

<hr>
<h2 id='hyperEM'>Estimate hyperparameters using an EM algorithm</h2><span id='topic+hyperEM'></span>

<h3>Description</h3>

<p><code>hyperEM</code> finds hyperparameter estimates using a variation on the
Expectation-Maximization (EM) algorithm known as the Expectation/Conditional
Maximization (ECM) algorithm (Meng et al, 1993). The algorithm estimates each
element of the hyperparameter vector, <code class="reqn">\theta</code>, while holding fixed
(conditioning on) the other parameters in the vector. Alternatively, it can
estimate both parameters for each distribution in the mixture while holding
the parameters from the other distribution and the mixing fraction fixed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hyperEM(
  data,
  theta_init_vec,
  squashed = TRUE,
  zeroes = FALSE,
  N_star = 1,
  method = c("score", "nlminb"),
  profile = c("parameter", "distribution"),
  LL_tol = 1e-04,
  consecutive = 100,
  param_lower = 1e-05,
  param_upper = 20,
  print_level = 2,
  max_iter = 5000,
  conf_ints = FALSE,
  conf_level = c("95", "80", "90", "99"),
  track = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hyperEM_+3A_data">data</code></td>
<td>
<p>A data frame from <code><a href="#topic+processRaw">processRaw</a></code> or
<code><a href="#topic+squashData">squashData</a></code> containing columns named <em>N</em>, <em>E</em>, and
(if squashed) <em>weight</em>.</p>
</td></tr>
<tr><td><code id="hyperEM_+3A_theta_init_vec">theta_init_vec</code></td>
<td>
<p>A numeric vector of initial hyperparameter guesses
ordered as: <code class="reqn">\alpha_1, \beta_1, \alpha_2, \beta_2, P</code>.</p>
</td></tr>
<tr><td><code id="hyperEM_+3A_squashed">squashed</code></td>
<td>
<p>A scalar logical (<code>TRUE</code> or <code>FALSE</code>) indicating
whether or not data squashing was used.</p>
</td></tr>
<tr><td><code id="hyperEM_+3A_zeroes">zeroes</code></td>
<td>
<p>A scalar logical specifying if zero counts are included.</p>
</td></tr>
<tr><td><code id="hyperEM_+3A_n_star">N_star</code></td>
<td>
<p>A positive scalar whole number value for the minimum count
size to be used for hyperparameter estimation. If zeroes are used, set
<code>N_star</code> to <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="hyperEM_+3A_method">method</code></td>
<td>
<p>A scalar string indicating which method (i.e. score functions
or log-likelihood function) to use for the maximization steps. Possible
values are <code>"score"</code> and <code>"nlminb"</code>.</p>
</td></tr>
<tr><td><code id="hyperEM_+3A_profile">profile</code></td>
<td>
<p>A scalar string indicating which method to use to optimize the
log-likelihood function if <code>method = "nlminb"</code> (ignored if
<code>method = "score"</code>).  <code>profile = "parameter"</code> optimizes one
parameter (<code class="reqn">\alpha</code> or <code class="reqn">\beta</code>) from the log-likelihood function at
a time.  <code>profile = "distribution"</code> optimizes one distribution from
the mixture at a time (<code class="reqn">\alpha</code> and <code class="reqn">\beta</code> simultaneously).</p>
</td></tr>
<tr><td><code id="hyperEM_+3A_ll_tol">LL_tol</code></td>
<td>
<p>A scalar numeric value for the tolerance used for determining
when the change in log-likelihood at each iteration is sufficiently small.
Used for convergence assessment.</p>
</td></tr>
<tr><td><code id="hyperEM_+3A_consecutive">consecutive</code></td>
<td>
<p>A positive scalar whole number value for the number of
consecutive iterations the change in log-likelihood must be below
<code>LL_tol</code> in order to reach convergence. Larger values reduce the
chance of getting stuck in a flat region of the curve.</p>
</td></tr>
<tr><td><code id="hyperEM_+3A_param_lower">param_lower</code></td>
<td>
<p>A scalar numeric value for the smallest acceptable value
for each <code class="reqn">\alpha</code> and <code class="reqn">\beta</code> estimate.</p>
</td></tr>
<tr><td><code id="hyperEM_+3A_param_upper">param_upper</code></td>
<td>
<p>A scalar numeric value for the largest acceptable value
for each <code class="reqn">\alpha</code> and <code class="reqn">\beta</code> estimate.</p>
</td></tr>
<tr><td><code id="hyperEM_+3A_print_level">print_level</code></td>
<td>
<p>A value that determines how much information is printed
during execution. Possible value are <code>0</code> for no printing, <code>1</code> for
minimal information, and <code>2</code> for maximal information.</p>
</td></tr>
<tr><td><code id="hyperEM_+3A_max_iter">max_iter</code></td>
<td>
<p>A positive scalar whole number value for the maximum number
of iterations to use.</p>
</td></tr>
<tr><td><code id="hyperEM_+3A_conf_ints">conf_ints</code></td>
<td>
<p>A scalar logical indicating if confidence intervals and
standard errors should be returned.</p>
</td></tr>
<tr><td><code id="hyperEM_+3A_conf_level">conf_level</code></td>
<td>
<p>A scalar string for the confidence level used if confidence
intervals are requested.</p>
</td></tr>
<tr><td><code id="hyperEM_+3A_track">track</code></td>
<td>
<p>A scalar logical indicating whether or not to retain the
hyperparameter estimates and log-likelihood value at each iteration. Can be
used for plotting to better understand the algorithm's behavior.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>method = "score"</code>, the maximization step finds a root
of the score function. If <code>method = "nlminb"</code>,
<code><a href="stats.html#topic+nlminb">nlminb</a></code> is used to find a minimum of the negative
log-likelihood function.
</p>
<p>If <code>method = "score"</code> and <code>zeroes = FALSE</code>, then
<code>'N_star'</code> must equal <code>1</code>.
</p>
<p>If <code>method = "score"</code>, the model is reparameterized. The
parameters are transformed to force the parameter space to include all real
numbers. This approach addresses numerical issues at the edge of the
parameter space. The reparameterization follows:
<code class="reqn">\alpha_{prime} = log(\alpha)</code>, <code class="reqn">\beta_{prime} = log(\beta)</code>, and
<code class="reqn">P_{prime} = tan(pi * P - pi / 2)</code>. However, the values returned in
<code>estimates</code> are on the original scale (back-transformed).
</p>
<p>On every 100th iteration, the procedure described in Millar (2011)
is used to accelerate the estimate of <code class="reqn">\theta</code>.
</p>
<p>The score vector and its Euclidean norm should be close to zero at
a local maximum and can therefore be used to help assess the reliability of
the results. A local maximum might not be the global MLE, however.
</p>
<p>Asymptotic normal confidence intervals, if requested, use standard
errors calculated from the observed Fisher information matrix as discussed
in DuMouchel (1999).
</p>


<h3>Value</h3>

<p>A list including the following:
</p>

<ul>
<li><p><em>estimates</em>: The maximum likelihood estimate (MLE) of the
hyperparameter, <code class="reqn">\theta</code>.
</p>
</li>
<li><p><em>conf_int</em>: A data frame including the standard errors and
confidence limits for <code>estimates</code>. Only included if
<code>conf_ints = TRUE</code>.
</p>
</li>
<li><p><em>maximum</em>: The log-likelihood function evaluated at
<code>estimates</code>.
</p>
</li>
<li><p><em>method</em>: The method used in the maximization step.
</p>
</li>
<li><p><em>elapsed</em>: The elapsed function execution time in seconds.
</p>
</li>
<li><p><em>iters</em>: The number of iterations used.
</p>
</li>
<li><p><em>score</em>: The score functions (i.e. score vector) evaluated
at <code>estimates</code>. All elements should be close to zero.
</p>
</li>
<li><p><em>score_norm</em>: The Euclidean norm of the score vector
evaluated at <code>estimates</code>. Should be close to zero.
</p>
</li>
<li><p><em>tracking</em>: The estimates of <code class="reqn">\theta</code> at each iteration
and the log-likelihood function evaluated at those estimates. Unless
<code>track = TRUE</code>, only shows the starting point of the algorithm.
</p>
</li></ul>



<h3>References</h3>

<p>DuMouchel W (1999). &quot;Bayesian Data Mining in Large Frequency
Tables, With an Application to the FDA Spontaneous Reporting System.&quot;
<em>The American Statistician</em>, 53(3), 177-190.
</p>
<p>Meng X-L, Rubin D (1993). &quot;Maximum likelihood estimation via the
ECM algorithm: A general framework&quot;, <em>Biometrika</em>, 80(2), 267-278.
</p>
<p>Millar, Russell B (2011). &quot;Maximum Likelihood Estimation and
Inference&quot;, <em>John Wiley &amp; Sons, Ltd</em>, 111-112.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+uniroot">uniroot</a></code> for finding a zero of the score
function and <code><a href="stats.html#topic+nlminb">nlminb</a></code> for minimizing the negative
log-likelihood function
</p>
<p>Other hyperparameter estimation functions: 
<code><a href="#topic+autoHyper">autoHyper</a>()</code>,
<code><a href="#topic+exploreHypers">exploreHypers</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data.table::setDTthreads(2)  #only needed for CRAN checks
data(caers)
proc &lt;- processRaw(caers)
squashed &lt;- squashData(proc, bin_size = 300, keep_pts = 10)
squashed &lt;- squashData(squashed, count = 2, bin_size = 13, keep_pts = 10)
hypers &lt;- hyperEM(squashed, theta_init_vec = c(1, 1, 2, 2, .3),
                  consecutive = 10, LL_tol = 1e-03)

</code></pre>

<hr>
<h2 id='negLL'>Likelihood without zero counts</h2><span id='topic+negLL'></span>

<h3>Description</h3>

<p><code>negLL</code> computes the negative log-likelihood based on the conditional
marginal distribution of the counts, <em>N</em>, given that <em>N &gt;= N*</em>,
where <em>N*</em> is the smallest count used for estimating the hyperparameters
(DuMouchel et al. 2001). This function is minimized to estimate the
hyperparameters of the prior distribution. Use this function when neither
zero counts nor data squashing are being used. Generally this function is not
recommended unless using a small data set since data squashing (see
<code><a href="#topic+squashData">squashData</a></code> and <code><a href="#topic+negLLsquash">negLLsquash</a></code>) can increase
efficiency (DuMouchel et al. 2001).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>negLL(theta, N, E, N_star = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="negLL_+3A_theta">theta</code></td>
<td>
<p>A numeric vector of hyperparameters ordered as:
<code class="reqn">\alpha_1, \beta_1, \alpha_2, \beta_2, P</code>.</p>
</td></tr>
<tr><td><code id="negLL_+3A_n">N</code></td>
<td>
<p>A whole number vector of actual counts from
<code><a href="#topic+processRaw">processRaw</a></code>.</p>
</td></tr>
<tr><td><code id="negLL_+3A_e">E</code></td>
<td>
<p>A numeric vector of expected counts from <code><a href="#topic+processRaw">processRaw</a></code>.</p>
</td></tr>
<tr><td><code id="negLL_+3A_n_star">N_star</code></td>
<td>
<p>A scalar whole number for the minimum count size used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The conditional marginal distribution for the counts, <em>N</em>,
given that <em>N &gt;= N*</em>, is based on a mixture of two negative binomial
distributions. The hyperparameters for the prior distribution (mixture of
gammas) are estimated by optimizing the likelihood equation from this
conditional marginal distribution. It is recommended to use <code>N_star =
  1</code> when practical.
</p>
<p>The hyperparameters are:
</p>

<ul>
<li><p><code class="reqn">\alpha_1, \beta_1</code>: Parameters of the first component of the
marginal distribution of the counts (also the prior distribution)
</p>
</li>
<li><p><code class="reqn">\alpha_2, \beta_2</code>: Parameters of the second component
</p>
</li>
<li><p><code class="reqn">P</code>: Mixture fraction
</p>
</li></ul>

<p>This function will not need to be called directly if using
<code><a href="#topic+exploreHypers">exploreHypers</a></code> or <code><a href="#topic+autoHyper">autoHyper</a></code>.
</p>


<h3>Value</h3>

<p>A scalar negative log-likelihood value
</p>


<h3>Warnings</h3>

<p>Make sure <em>N_star</em> matches the smallest actual count in <em>N</em>
before using this function. Filter <em>N</em> and <em>E</em> if needed.
</p>
<p>Make sure the data were not squashed before using this function.
</p>


<h3>References</h3>

<p>DuMouchel W, Pregibon D (2001). &quot;Empirical Bayes Screening for
Multi-item Associations.&quot; In <em>Proceedings of the Seventh ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining</em>, KDD '01,
pp. 67-76. ACM, New York, NY, USA. ISBN 1-58113-391-X.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+nlm">nlm</a></code>, <code><a href="stats.html#topic+nlminb">nlminb</a></code>, and
<code><a href="stats.html#topic+optim">optim</a></code> for optimization
</p>
<p>Other negative log-likelihood functions: 
<code><a href="#topic+negLLsquash">negLLsquash</a>()</code>,
<code><a href="#topic+negLLzeroSquash">negLLzeroSquash</a>()</code>,
<code><a href="#topic+negLLzero">negLLzero</a>()</code>
</p>

<hr>
<h2 id='negLLsquash'>Likelihood with data squashing and no zero counts</h2><span id='topic+negLLsquash'></span>

<h3>Description</h3>

<p><code>negLLsquash</code> computes the negative log-likelihood based on the
conditional marginal distribution of the counts, <em>N</em>, given that
<em>N &gt;= N*</em>, where <em>N*</em> is the smallest count used for estimating the
hyperparameters. This function is minimized to estimate the hyperparameters
of the prior distribution. Use this function when zero counts are not used
and data squashing is used as described by DuMouchel et al. (2001). This
function is the likelihood function that should usually be chosen.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>negLLsquash(theta, ni, ei, wi, N_star = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="negLLsquash_+3A_theta">theta</code></td>
<td>
<p>A numeric vector of hyperparameters ordered as:
<code class="reqn">\alpha_1, \beta_1, \alpha_2, \beta_2, P</code>.</p>
</td></tr>
<tr><td><code id="negLLsquash_+3A_ni">ni</code></td>
<td>
<p>A whole number vector of squashed actual counts from
<code><a href="#topic+squashData">squashData</a></code>.</p>
</td></tr>
<tr><td><code id="negLLsquash_+3A_ei">ei</code></td>
<td>
<p>A numeric vector of squashed expected counts from
<code><a href="#topic+squashData">squashData</a></code>.</p>
</td></tr>
<tr><td><code id="negLLsquash_+3A_wi">wi</code></td>
<td>
<p>A whole number vector of bin weights from <code><a href="#topic+squashData">squashData</a></code>.</p>
</td></tr>
<tr><td><code id="negLLsquash_+3A_n_star">N_star</code></td>
<td>
<p>A scalar whole number for the minimum count size used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The conditional marginal distribution for the counts, <em>N</em>,
given that <em>N &gt;= N*</em>, is based on a mixture of two negative binomial
distributions. The hyperparameters for the prior distribution (mixture of
gammas) are estimated by optimizing the likelihood equation from this
conditional marginal distribution. It is recommended to use <code>N_star =
  1</code> when practical.
</p>
<p>The hyperparameters are:
</p>

<ul>
<li><p><code class="reqn">\alpha_1, \beta_1</code>: Parameters of the first component of the
marginal distribution of the counts (also the prior distribution)
</p>
</li>
<li><p><code class="reqn">\alpha_2, \beta_2</code>: Parameters of the second component
</p>
</li>
<li><p><code class="reqn">P</code>: Mixture fraction
</p>
</li></ul>

<p>This function will not need to be called directly if using
<code><a href="#topic+exploreHypers">exploreHypers</a></code> or <code><a href="#topic+autoHyper">autoHyper</a></code>.
</p>


<h3>Value</h3>

<p>A scalar negative log-likelihood value
</p>


<h3>Warnings</h3>

<p>Make sure <em>N_star</em> matches the smallest actual count in <em>ni</em>
before using this function. Filter <em>ni</em>, <em>ei</em>, and <em>wi</em> if
needed.
</p>
<p>Make sure the data were actually squashed (see <code><a href="#topic+squashData">squashData</a></code>)
before using this function.
</p>


<h3>References</h3>

<p>DuMouchel W, Pregibon D (2001). &quot;Empirical Bayes Screening for
Multi-item Associations.&quot; In <em>Proceedings of the Seventh ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining</em>, KDD '01,
pp. 67-76. ACM, New York, NY, USA. ISBN 1-58113-391-X.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+nlm">nlm</a></code>, <code><a href="stats.html#topic+nlminb">nlminb</a></code>, and
<code><a href="stats.html#topic+optim">optim</a></code> for optimization and <code><a href="#topic+squashData">squashData</a></code>
for data squashing
</p>
<p>Other negative log-likelihood functions: 
<code><a href="#topic+negLLzeroSquash">negLLzeroSquash</a>()</code>,
<code><a href="#topic+negLLzero">negLLzero</a>()</code>,
<code><a href="#topic+negLL">negLL</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data.table::setDTthreads(2)  #only needed for CRAN checks
theta_init &lt;- c(1, 1, 3, 3, .2)  #initial guess
data(caers)
proc &lt;- processRaw(caers)
squashed &lt;- squashData(proc, bin_size = 300, keep_pts = 10)
squashed &lt;- squashData(squashed, count = 2, bin_size = 13, keep_pts = 10)
negLLsquash(theta = theta_init, ni = squashed$N, ei = squashed$E,
            wi = squashed$weight)
#For hyperparameter estimation...
stats::nlminb(start = theta_init, objective = negLLsquash, ni = squashed$N,
              ei = squashed$E, wi = squashed$weight)

</code></pre>

<hr>
<h2 id='negLLzero'>Likelihood with zero counts</h2><span id='topic+negLLzero'></span>

<h3>Description</h3>

<p><code>negLLzero</code> computes the negative log-likelihood based on the
unconditional marginal distribution of <em>N</em> (equation 12 in DuMouchel
1999, except taking negative natural log). This function is minimized to
estimate the hyperparameters of the prior distribution. Use this function if
including zero counts but not squashing data. Generally this function is not
recommended (<code><a href="#topic+negLLsquash">negLLsquash</a></code> is typically more efficient).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>negLLzero(theta, N, E)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="negLLzero_+3A_theta">theta</code></td>
<td>
<p>A numeric vector of hyperparameters ordered as:
<code class="reqn">\alpha_1, \beta_1, \alpha_2, \beta_2, P</code>.</p>
</td></tr>
<tr><td><code id="negLLzero_+3A_n">N</code></td>
<td>
<p>A whole number vector of actual counts from
<code><a href="#topic+processRaw">processRaw</a></code>.</p>
</td></tr>
<tr><td><code id="negLLzero_+3A_e">E</code></td>
<td>
<p>A numeric vector of expected counts from <code><a href="#topic+processRaw">processRaw</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The marginal distribution of the counts, <em>N</em>, is a mixture of
two negative binomial distributions. The hyperparameters for the prior
distribution (mixture of gammas) are estimated by optimizing the likelihood
equation from this marginal distribution.
</p>
<p>The hyperparameters are:
</p>

<ul>
<li><p><code class="reqn">\alpha_1, \beta_1</code>: Parameters of the first component of the
marginal distribution of the counts (also the prior distribution)
</p>
</li>
<li><p><code class="reqn">\alpha_2, \beta_2</code>: Parameters of the second component
</p>
</li>
<li><p><code class="reqn">P</code>: Mixture fraction
</p>
</li></ul>

<p>This function will not need to be called directly if using
<code><a href="#topic+exploreHypers">exploreHypers</a></code> or <code><a href="#topic+autoHyper">autoHyper</a></code>.
</p>


<h3>Value</h3>

<p>A scalar negative log-likelihood value.
</p>


<h3>Warnings</h3>

<p>Make sure <em>N</em> actually contains zeroes before using this function. You
should have used the <code>zeroes = TRUE</code> option when calling the
<code><a href="#topic+processRaw">processRaw</a></code> function.
</p>
<p>Make sure the data were not squashed before using this function.
</p>


<h3>References</h3>

<p>DuMouchel W (1999). &quot;Bayesian Data Mining in Large Frequency
Tables, With an Application to the FDA Spontaneous Reporting System.&quot;
<em>The American Statistician</em>, 53(3), 177-190.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+nlm">nlm</a></code>, <code><a href="stats.html#topic+nlminb">nlminb</a></code>, and
<code><a href="stats.html#topic+optim">optim</a></code> for optimization
</p>
<p>Other negative log-likelihood functions: 
<code><a href="#topic+negLLsquash">negLLsquash</a>()</code>,
<code><a href="#topic+negLLzeroSquash">negLLzeroSquash</a>()</code>,
<code><a href="#topic+negLL">negLL</a>()</code>
</p>

<hr>
<h2 id='negLLzeroSquash'>Likelihood with data squashing &amp; zero counts</h2><span id='topic+negLLzeroSquash'></span>

<h3>Description</h3>

<p><code>negLLzeroSquash</code> computes the negative log-likelihood based on the
unconditional marginal distribution of <em>N</em> (DuMouchel et al. 2001).
This function is minimized to estimate the hyperparameters of the prior
distribution. Use this function if including zero counts and using data
squashing. Generally this function is not recommended unless convergence
issues occur without zero counts (<code><a href="#topic+negLLsquash">negLLsquash</a></code> is typically more
efficient).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>negLLzeroSquash(theta, ni, ei, wi)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="negLLzeroSquash_+3A_theta">theta</code></td>
<td>
<p>A numeric vector of hyperparameters ordered as:
<code class="reqn">\alpha_1, \beta_1, \alpha_2, \beta_2, P</code>.</p>
</td></tr>
<tr><td><code id="negLLzeroSquash_+3A_ni">ni</code></td>
<td>
<p>A whole number vector of squashed actual counts from
<code><a href="#topic+squashData">squashData</a></code>.</p>
</td></tr>
<tr><td><code id="negLLzeroSquash_+3A_ei">ei</code></td>
<td>
<p>A numeric vector of squashed expected counts from
<code><a href="#topic+squashData">squashData</a></code>.</p>
</td></tr>
<tr><td><code id="negLLzeroSquash_+3A_wi">wi</code></td>
<td>
<p>A whole number vector of bin weights from <code><a href="#topic+squashData">squashData</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The marginal distribution of the counts, <em>N</em>, is a mixture of
two negative binomial distributions. The hyperparameters for the prior
distribution (mixture of gammas) are estimated by optimizing the likelihood
equation from this marginal distribution.
</p>
<p>The hyperparameters are:
</p>

<ul>
<li><p><code class="reqn">\alpha_1, \beta_1</code>: Parameters of the first component of the
marginal distribution of the counts (also the prior distribution)
</p>
</li>
<li><p><code class="reqn">\alpha_2, \beta_2</code>: Parameters of the second component
</p>
</li>
<li><p><code class="reqn">P</code>: Mixture fraction
</p>
</li></ul>

<p>This function will not need to be called directly if using
<code><a href="#topic+exploreHypers">exploreHypers</a></code> or <code><a href="#topic+autoHyper">autoHyper</a></code>.
</p>


<h3>Value</h3>

<p>A scalar negative log-likelihood value.
</p>


<h3>Warnings</h3>

<p>Make sure <em>ni</em> actually contains zeroes before using this function.
You should have used the <code>zeroes = TRUE</code> option when calling the
<code>processRaw</code> function.
</p>
<p>Make sure the data were actually squashed (see <code><a href="#topic+squashData">squashData</a></code>)
before using this function.
</p>


<h3>References</h3>

<p>DuMouchel W, Pregibon D (2001). &quot;Empirical Bayes Screening for
Multi-item Associations.&quot; In <em>Proceedings of the Seventh ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining</em>, KDD '01,
pp. 67-76. ACM, New York, NY, USA. ISBN 1-58113-391-X.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+nlm">nlm</a></code>, <code><a href="stats.html#topic+nlminb">nlminb</a></code>, and
<code><a href="stats.html#topic+optim">optim</a></code> for optimization and <code><a href="#topic+squashData">squashData</a></code>
for data squashing
</p>
<p>Other negative log-likelihood functions: 
<code><a href="#topic+negLLsquash">negLLsquash</a>()</code>,
<code><a href="#topic+negLLzero">negLLzero</a>()</code>,
<code><a href="#topic+negLL">negLL</a>()</code>
</p>

<hr>
<h2 id='openEBGM-package'>openEBGM: EBGM Disproportionality Scores for Adverse Event Data Mining</h2><span id='topic+openEBGM'></span><span id='topic+openEBGM-package'></span>

<h3>Description</h3>

<p>An implementation of DuMouchel's (1999) <a href="https://doi.org/10.1080/00031305.1999.10474456">doi:10.1080/00031305.1999.10474456</a> Bayesian data mining method for the market basket problem. Calculates Empirical Bayes Geometric Mean (EBGM) and posterior quantile scores using the Gamma-Poisson Shrinker (GPS) model to find unusually large cell counts in large, sparse contingency tables. Can be used to find unusually high reporting rates of adverse events associated with products. In general, can be used to mine any database where the co-occurrence of two variables or items is of interest. Also calculates relative and proportional reporting ratios. Builds on the work of the 'PhViD' package, from which much of the code is derived. Some of the added features include stratification to adjust for confounding variables and data squashing to improve computational efficiency. Includes an implementation of the EM algorithm for hyperparameter estimation loosely derived from the 'mederrRank' package.
</p>


<h3>Data preparation &amp; squashing functions</h3>

<p>The data preparation function, <code><a href="#topic+processRaw">processRaw</a></code>, converts raw data
into actual and expected counts for product/event pairs.
<code><a href="#topic+processRaw">processRaw</a></code> also adds the relative reporting ratio (RR) and
proportional reporting ratio (PRR). The data squashing function,
<code><a href="#topic+squashData">squashData</a></code>, implements the simple version of data squashing
described in DuMouchel et al. (2001). Data squashing can be used to reduce
computational burden.
</p>


<h3>Negative log-likelihood functions</h3>

<p>The negative log-likelihood functions (<code><a href="#topic+negLL">negLL</a></code>,
<code><a href="#topic+negLLsquash">negLLsquash</a></code>, <code><a href="#topic+negLLzero">negLLzero</a></code>, and
<code><a href="#topic+negLLzeroSquash">negLLzeroSquash</a></code>) provide the means of calculating the
negative log-likelihoods as mentioned in the DuMouchel papers. DuMouchel
uses the likelihood function, based on the marginal distributions of the
counts, to estimate the hyperparameters of the prior distribution.
</p>


<h3>Hyperparameter estimation functions</h3>

<p>The hyperparameter estimation functions (<code><a href="#topic+exploreHypers">exploreHypers</a></code> and
<code><a href="#topic+autoHyper">autoHyper</a></code>) use gradient-based approaches to estimate the
hyperparameters, <code class="reqn">\theta</code>, of the prior distribution (gamma mixture)
using the negative log-likelihood functions from the marginal distributions
of the counts (negative binomial). <code class="reqn">\theta</code> is a vector containing five
parameters (<code class="reqn">\alpha_1</code>, <code class="reqn">\beta_1</code>, <code class="reqn">\alpha_2</code>, <code class="reqn">\beta_2</code>,
and <code class="reqn">P</code>). <code><a href="#topic+hyperEM">hyperEM</a></code> estimates <code class="reqn">\theta</code> using a version
of the EM algorithm.
</p>


<h3>Posterior distribution functions</h3>

<p>The posterior distribution functions calculate the mixture fraction
(<code><a href="#topic+Qn">Qn</a></code>), geometric mean (<code><a href="#topic+ebgm">ebgm</a></code>), and quantiles
(<code><a href="#topic+quantBisect">quantBisect</a></code>) of the posterior distribution. Alternatively,
<code><a href="#topic+ebScores">ebScores</a></code> can be used to create an object of class openEBGM
that contains the EBGM and quantiles scores. Appropriate methods exist for
the generic functions <code><a href="base.html#topic+print">print</a></code>,
<code><a href="base.html#topic+summary">summary</a></code>, and <code><a href="graphics.html#topic+plot">plot</a></code> for openEBGM
objects.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: John Ihrie <a href="mailto:John.Ihrie@fda.hhs.gov">John.Ihrie@fda.hhs.gov</a>
</p>
<p>Authors:
</p>

<ul>
<li><p> Travis Canida <a href="mailto:Travis.Canida@fda.hhs.gov">Travis.Canida@fda.hhs.gov</a>
</p>
</li></ul>

<p>Other contributors:
</p>

<ul>
<li><p> Isma√Øl Ahmed (author of 'PhViD' package (derived code)) [contributor]
</p>
</li>
<li><p> Antoine Poncet (author of 'PhViD') [contributor]
</p>
</li>
<li><p> Sergio Venturini (author of 'mederrRank' package (derived code)) [contributor]
</p>
</li>
<li><p> Jessica Myers (author of 'mederrRank') [contributor]
</p>
</li></ul>



<h3>References</h3>

<p>Ahmed I, Poncet A (2016). <span class="pkg">PhViD</span>: an R package for
PharmacoVigilance signal Detection. <em>R package version 1.0.8</em>.
</p>
<p>Venturini S, Myers J (2015). <span class="pkg">mederrRank</span>: Bayesian Methods
for Identifying the Most Harmful Medication Errors. <em>R package version
0.0.8</em>.
</p>
<p>DuMouchel W (1999). &quot;Bayesian Data Mining in Large Frequency
Tables, With an Application to the FDA Spontaneous Reporting System.&quot;
<em>The American Statistician</em>, 53(3), 177-190.
</p>
<p>DuMouchel W, Pregibon D (2001). &quot;Empirical Bayes Screening for
Multi-item Associations.&quot; In <em>Proceedings of the Seventh ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining</em>, KDD '01,
pp. 67-76. ACM, New York, NY, USA. ISBN 1-58113-391-X.
</p>
<p>Evans SJW, Waller P, Davis S (2001). &quot;Use of Proportional
Reporting Ratios (PRRs) for Signal Generation from Spontaneous Adverse Drug
Reaction Reports.&quot; <em>Pharmacoepidemiology and Drug Safety</em>, 10(6),
483-486.
</p>
<p>FDA (2017). &quot;CFSAN Adverse Event Reporting System (CAERS).&quot;
URL <a href="https://www.fda.gov/food/compliance-enforcement-food/cfsan-adverse-event-reporting-system-caers">https://www.fda.gov/food/compliance-enforcement-food/cfsan-adverse-event-reporting-system-caers</a>.
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://journal.r-project.org/archive/2017/RJ-2017-063/index.html">https://journal.r-project.org/archive/2017/RJ-2017-063/index.html</a>
</p>
</li></ul>


<hr>
<h2 id='plot.openEBGM'>Plot an openEBGM object</h2><span id='topic+plot.openEBGM'></span>

<h3>Description</h3>

<p>Plot an openEBGM object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'openEBGM'
plot(x, y = NULL, event = NULL, plot.type = "bar", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.openEBGM_+3A_x">x</code></td>
<td>
<p>An openEBGM object constructed by <code>ebScores()</code></p>
</td></tr>
<tr><td><code id="plot.openEBGM_+3A_y">y</code></td>
<td>
<p>Unused parameter to satisfy generic function requirement</p>
</td></tr>
<tr><td><code id="plot.openEBGM_+3A_event">event</code></td>
<td>
<p>An (optional) specification of an event to subset the data by.</p>
</td></tr>
<tr><td><code id="plot.openEBGM_+3A_plot.type">plot.type</code></td>
<td>
<p>A character vector specifying which type of plot should be
output. See details.</p>
</td></tr>
<tr><td><code id="plot.openEBGM_+3A_...">...</code></td>
<td>
<p>Arguments to be passed to methods</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are three different types of plots that the plot function may
produce when called on an openEBGM object. These are
</p>

<ul>
<li><p> bar
</p>
</li>
<li><p> shrinkage
</p>
</li>
<li><p> histogram
</p>
</li></ul>

<p>A bar chart displays the top ten product-symptom EBGM scores, as
well as error bars which display the highest and lowest of the quantiles
chosen at the time of instantiating the openEBGM object. A shrinkage plot
plots EBGM score on the y axis, and the natural log of the RR on the x
axis. This plot is also called a squid plot and was conceived by Stuart
Chirtel. Finally, a histogram simply displays a histogram of the EBGM
scores.
</p>

<hr>
<h2 id='print.openEBGM'>Print an openEBGM object</h2><span id='topic+print.openEBGM'></span>

<h3>Description</h3>

<p>Print an openEBGM object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'openEBGM'
print(x, threshold = 2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.openEBGM_+3A_x">x</code></td>
<td>
<p>An openEBGM object constructed by <code>ebScores()</code></p>
</td></tr>
<tr><td><code id="print.openEBGM_+3A_threshold">threshold</code></td>
<td>
<p>A numeric value indicating the minimum threshold
for QUANT or EBGM values to display.</p>
</td></tr>
<tr><td><code id="print.openEBGM_+3A_...">...</code></td>
<td>
<p>Arguments to be passed to other methods</p>
</td></tr>
</table>

<hr>
<h2 id='processRaw'>Process raw data</h2><span id='topic+processRaw'></span>

<h3>Description</h3>

<p><code>processRaw</code> finds the actual and expected counts using the methodology
described by DuMouchel (1999); however, an adjustment is applied to expected
counts to prevent double-counting (i.e., using unique marginal ID counts
instead of contingency table marginal totals). Also calculates the relative
reporting ratio (<em>RR</em>) and the proportional reporting ratio
(<em>PRR</em>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>processRaw(
  data,
  stratify = FALSE,
  zeroes = FALSE,
  digits = 2,
  max_cats = 10,
  list_ids = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="processRaw_+3A_data">data</code></td>
<td>
<p>A data frame containing columns named: <em>id</em>, <em>var1</em>,
and <em>var2</em>. Possibly includes columns for stratification variables
identified by the substring <em>'strat'</em> (e.g. <em>strat_age</em>). Other
columns will be ignored.</p>
</td></tr>
<tr><td><code id="processRaw_+3A_stratify">stratify</code></td>
<td>
<p>A logical scalar (TRUE or FALSE) specifying if stratification
is to be used.</p>
</td></tr>
<tr><td><code id="processRaw_+3A_zeroes">zeroes</code></td>
<td>
<p>A logical scalar specifying if zero counts should be included.
Using zero counts is only recommended for small data sets because it will
dramatically increase run time.</p>
</td></tr>
<tr><td><code id="processRaw_+3A_digits">digits</code></td>
<td>
<p>A whole number scalar specifying how many decimal places to
use for rounding <em>RR</em> and <em>PRR</em>.</p>
</td></tr>
<tr><td><code id="processRaw_+3A_max_cats">max_cats</code></td>
<td>
<p>A whole number scalar specifying the maximum number of
categories to allow in any given stratification variable. Used to help
prevent a situation where the user forgets to categorize a continuous
variable, such as age.</p>
</td></tr>
<tr><td><code id="processRaw_+3A_list_ids">list_ids</code></td>
<td>
<p>A logical scalar specifying if a column for pipe-concatenated
IDs should be returned.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An <em>id</em> column must be included in <code>data</code>. If your data
set does not include IDs, make a column of unique IDs using <code>df$id &lt;-
  1:nrow(df)</code>. However, unique IDs should only be constructed if the cells in
the contingency table are mutually exclusive. For instance, unique IDs for
each row in <code>data</code> are not appropriate with CAERS data since a given
report can include multiple products and/or adverse events.
</p>
<p>Stratification variables are identified by searching for the
substring <em>'strat'</em>. Only variables containing <em>'strat'</em> (case
sensitive) will be used as stratification variables. <em>PRR</em>
calculations ignore stratification, but <em>E</em> and <em>RR</em> calculations
are affected. A warning will be displayed if any stratum contains less than
50 unique IDs.
</p>
<p>If a <em>PRR</em> calculation results in division by zero, <code>Inf</code>
is returned.
</p>


<h3>Value</h3>

<p>A data frame with actual counts (<em>N</em>), expected counts
(<em>E</em>), relative reporting ratio (<em>RR</em>), and proportional
reporting ratio (<em>PRR</em>) for <em>var1-var2</em> pairs. Also includes a
column for IDs (<em>ids</em>) if <code>list_ids = TRUE</code>.
</p>


<h3>Warnings</h3>

<p>Use of the <code>zeroes = TRUE</code> option will result in a considerable
increase in runtime. Using zero counts is not recommended if the contingency
table is moderate or large in size (~500K cells or larger). However, using
zeroes could be useful if the optimization algorithm fails to converge
when estimating hyperparameters.
</p>
<p>Any columns in <code>data</code> containing the substring <em>'strat'</em> in the
column name will be considered stratification variables, so verify that you
do not have any extraneous columns with that substring.
</p>


<h3>References</h3>

<p>DuMouchel W (1999). &quot;Bayesian Data Mining in Large Frequency
Tables, With an Application to the FDA Spontaneous Reporting System.&quot;
<em>The American Statistician</em>, 53(3), 177-190.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data.table::setDTthreads(2)  #only needed for CRAN checks
var1 &lt;- c("product_A", rep("product_B", 3), "product_C",
          rep("product_A", 2), rep("product_B", 2), "product_C")
var2 &lt;- c("event_1", rep("event_2", 2), rep("event_3", 2),
          "event_2", rep("event_3", 3), "event_1")
strat1 &lt;- c(rep("Male", 5), rep("Female", 3), rep("Male", 2))
strat2 &lt;- c(rep("age_cat1", 5), rep("age_cat1", 3), rep("age_cat2", 2))
dat &lt;- data.frame(
  var1 = var1, var2 = var2, strat1 = strat1, strat2 = strat2,
  stringsAsFactors = FALSE
)
dat$id &lt;- 1:nrow(dat)
processRaw(dat)
suppressWarnings(
  processRaw(dat, stratify = TRUE)
)
processRaw(dat, zeroes = TRUE)
suppressWarnings(
  processRaw(dat, stratify = TRUE, zeroes = TRUE)
)
processRaw(dat, list_ids = TRUE)

</code></pre>

<hr>
<h2 id='Qn'>Calculate Qn</h2><span id='topic+Qn'></span>

<h3>Description</h3>

<p><code>Qn</code> calculates <code class="reqn">Q_n</code>, the posterior probability that <code class="reqn">\lambda</code>
came from the first component of the mixture, given <em>N = n</em> (Eq. 6,
DuMouchel 1999). <code class="reqn">Q_n</code> is the mixture fraction for the posterior
distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Qn(theta_hat, N, E)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Qn_+3A_theta_hat">theta_hat</code></td>
<td>
<p>A numeric vector of hyperparameter estimates (likely from
<code><a href="#topic+autoHyper">autoHyper</a></code> or from directly minimizing
<code><a href="#topic+negLLsquash">negLLsquash</a></code>) ordered as:
<code class="reqn">\alpha_1, \beta_1, \alpha_2, \beta_2, P</code>.</p>
</td></tr>
<tr><td><code id="Qn_+3A_n">N</code></td>
<td>
<p>A whole number vector of actual counts from
<code><a href="#topic+processRaw">processRaw</a></code>.</p>
</td></tr>
<tr><td><code id="Qn_+3A_e">E</code></td>
<td>
<p>A numeric vector of expected counts from <code><a href="#topic+processRaw">processRaw</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The hyperparameter estimates (<code>theta_hat</code>) are:
</p>

<ul>
<li><p><code class="reqn">\alpha_1, \beta_1</code>: Parameter estimates of the first
component of the prior distribution
</p>
</li>
<li><p><code class="reqn">\alpha_2, \beta_2</code>: Parameter estimates of the second
component
</p>
</li>
<li><p><code class="reqn">P</code>: Mixture fraction estimate of the prior distribution
</p>
</li></ul>



<h3>Value</h3>

<p>A numeric vector of probabilities.
</p>


<h3>References</h3>

<p>DuMouchel W (1999). &quot;Bayesian Data Mining in Large Frequency
Tables, With an Application to the FDA Spontaneous Reporting System.&quot;
<em>The American Statistician</em>, 53(3), 177-190.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+autoHyper">autoHyper</a></code>,  <code><a href="#topic+exploreHypers">exploreHypers</a></code>,
<code><a href="#topic+negLLsquash">negLLsquash</a></code>, <code><a href="#topic+negLL">negLL</a></code>,
<code><a href="#topic+negLLzero">negLLzero</a></code>, and <code><a href="#topic+negLLzeroSquash">negLLzeroSquash</a></code> for
hyperparameter estimation.
</p>
<p><code><a href="#topic+processRaw">processRaw</a></code> for finding counts.
</p>
<p>Other posterior distribution functions: 
<code><a href="#topic+ebgm">ebgm</a>()</code>,
<code><a href="#topic+quantBisect">quantBisect</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data.table::setDTthreads(2)  #only needed for CRAN checks
theta_init &lt;- data.frame(
  alpha1 = c(0.5, 1),
  beta1  = c(0.5, 1),
  alpha2 = c(2,   3),
  beta2  = c(2,   3),
  p      = c(0.1, 0.2)
)
data(caers)
proc &lt;- processRaw(caers)
squashed &lt;- squashData(proc, bin_size = 300, keep_pts = 10)
squashed &lt;- squashData(squashed, count = 2, bin_size = 13, keep_pts = 10)
theta_hat &lt;- autoHyper(data = squashed, theta_init = theta_init)$estimates
qn &lt;- Qn(theta_hat, N = proc$N, E = proc$E)
head(qn)

</code></pre>

<hr>
<h2 id='quantBisect'>Find quantiles of the posterior distribution</h2><span id='topic+quantBisect'></span>

<h3>Description</h3>

<p><code>quantBisect</code> finds the desired quantile of the posterior distribution
using the bisection method. Used to create credibility limits.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>quantBisect(
  percent,
  theta_hat,
  N,
  E,
  qn,
  digits = 2,
  limits = c(-1e+05, 1e+05),
  max_iter = 2000
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="quantBisect_+3A_percent">percent</code></td>
<td>
<p>A numeric scalar between 1 and 99 for the desired
percentile (e.g., 5 for 5th percentile).</p>
</td></tr>
<tr><td><code id="quantBisect_+3A_theta_hat">theta_hat</code></td>
<td>
<p>A numeric vector of hyperparameter estimates (likely from
<code><a href="#topic+autoHyper">autoHyper</a></code> or from directly minimizing
<code><a href="#topic+negLLsquash">negLLsquash</a></code>) ordered as:
<code class="reqn">\alpha_1, \beta_1, \alpha_2, \beta_2, P</code>.</p>
</td></tr>
<tr><td><code id="quantBisect_+3A_n">N</code></td>
<td>
<p>A whole number vector of actual counts from
<code><a href="#topic+processRaw">processRaw</a></code>.</p>
</td></tr>
<tr><td><code id="quantBisect_+3A_e">E</code></td>
<td>
<p>A numeric vector of expected counts from <code><a href="#topic+processRaw">processRaw</a></code>.</p>
</td></tr>
<tr><td><code id="quantBisect_+3A_qn">qn</code></td>
<td>
<p>A numeric vector of posterior probabilities that <code class="reqn">\lambda</code> came
from the first component of the mixture, given <em>N = n</em> (i.e., the
mixture fraction). See function <code><a href="#topic+Qn">Qn</a></code>.</p>
</td></tr>
<tr><td><code id="quantBisect_+3A_digits">digits</code></td>
<td>
<p>A scalar whole number that determines the number of decimal
places used when rounding the results.</p>
</td></tr>
<tr><td><code id="quantBisect_+3A_limits">limits</code></td>
<td>
<p>A whole number vector of length 2 for the upper and lower
bounds of the search space.</p>
</td></tr>
<tr><td><code id="quantBisect_+3A_max_iter">max_iter</code></td>
<td>
<p>A whole number scalar for the maximum number of iterations.
Used to prevent infinite loops.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The hyperparameter estimates (<code>theta_hat</code>) are:
</p>

<ul>
<li><p><code class="reqn">\alpha_1, \beta_1</code>: Parameter estimates of the first
component of the prior distribution
</p>
</li>
<li><p><code class="reqn">\alpha_2, \beta_2</code>: Parameter estimates of the second
component
</p>
</li>
<li><p><code class="reqn">P</code>: Mixture fraction estimate of the prior distribution
</p>
</li></ul>

<p>Although this function can find any quantile of the posterior
distribution, it will often be used to calculate the 5th and 95th
percentiles to create a 90% credibility interval.
</p>
<p>The quantile is calculated by solving for <code class="reqn">x</code> in the general
equation <code class="reqn">F(x) = cutoff</code>, or equivalently, <code class="reqn">F(x) - cutoff = 0</code>,
where <code class="reqn">F(x)</code> is the cumulative distribution function of the posterior
distribution and <code class="reqn">cutoff</code> is the appropriate cutoff level (e.g., 0.05
for the 5th percentile).
</p>


<h3>Value</h3>

<p>A numeric vector of quantile estimates.
</p>


<h3>Warning</h3>

<p>The <code>digits</code> argument determines the tolerance for the bisection
algorithm. The more decimal places you want returned, the longer the run
time.
</p>


<h3>See Also</h3>

<p><a href="https://en.wikipedia.org/wiki/Bisection_method">https://en.wikipedia.org/wiki/Bisection_method</a>
</p>
<p><code><a href="#topic+autoHyper">autoHyper</a></code>,  <code><a href="#topic+exploreHypers">exploreHypers</a></code>,
<code><a href="#topic+negLLsquash">negLLsquash</a></code>, <code><a href="#topic+negLL">negLL</a></code>,
<code><a href="#topic+negLLzero">negLLzero</a></code>, and <code><a href="#topic+negLLzeroSquash">negLLzeroSquash</a></code> for
hyperparameter estimation.
</p>
<p><code><a href="#topic+processRaw">processRaw</a></code> for finding counts.
</p>
<p><code><a href="#topic+Qn">Qn</a></code> for finding mixture fractions.
</p>
<p>Other posterior distribution functions: 
<code><a href="#topic+Qn">Qn</a>()</code>,
<code><a href="#topic+ebgm">ebgm</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data.table::setDTthreads(2)  #only needed for CRAN checks
theta_init &lt;- data.frame(
  alpha1 = c(0.5, 1),
  beta1  = c(0.5, 1),
  alpha2 = c(2,   3),
  beta2  = c(2,   3),
  p      = c(0.1, 0.2)
)
data(caers)
proc &lt;- processRaw(caers)
squashed &lt;- squashData(proc, bin_size = 300, keep_pts = 10)
squashed &lt;- squashData(squashed, count = 2, bin_size = 13, keep_pts = 10)
theta_hat &lt;- autoHyper(data = squashed, theta_init = theta_init)$estimates
qn &lt;- Qn(theta_hat, N = proc$N, E = proc$E)
proc$QUANT_05 &lt;- quantBisect(percent = 5, theta = theta_hat, N = proc$N,
                             E = proc$E, qn = qn)
## Not run: proc$QUANT_95 &lt;- quantBisect(percent = 95, theta = theta_hat,
                                      N = proc$N, E = proc$E, qn = qn)
## End(Not run)
head(proc)

</code></pre>

<hr>
<h2 id='squashData'>Squash data for hyperparameter estimation</h2><span id='topic+squashData'></span>

<h3>Description</h3>

<p><code>squashData</code> squashes data by binning expected counts, <em>E</em>, for a
given actual count, <em>N</em>, using bin means as the expected counts for
the reduced data set. The squashed points are weighted by bin size. Data
can be squashed to reduce computational burden (see DuMouchel et al.,
2001) when estimating the hyperparameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>squashData(
  data,
  count = 1,
  bin_size = 50,
  keep_pts = 100,
  min_bin = 50,
  min_pts = 500
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="squashData_+3A_data">data</code></td>
<td>
<p>A data frame (typically from <code><a href="#topic+processRaw">processRaw</a></code> or a
previous call to <code><a href="#topic+squashData">squashData</a></code>) containing columns named
<em>N</em>, <em>E</em>, and (possibly) <em>weight</em>. Can contain additional
columns, which will be ignored.</p>
</td></tr>
<tr><td><code id="squashData_+3A_count">count</code></td>
<td>
<p>A non-negative scalar whole number for the count size, <em>N</em>,
used for binning</p>
</td></tr>
<tr><td><code id="squashData_+3A_bin_size">bin_size</code></td>
<td>
<p>A scalar whole number (&gt;= 2)</p>
</td></tr>
<tr><td><code id="squashData_+3A_keep_pts">keep_pts</code></td>
<td>
<p>A nonnegative scalar whole number for number of points with
the largest expected counts to leave unsquashed. Used to help prevent
&ldquo;oversquashing&rdquo;.</p>
</td></tr>
<tr><td><code id="squashData_+3A_min_bin">min_bin</code></td>
<td>
<p>A positive scalar whole number for the minimum number of bins
needed. Used to help prevent &ldquo;oversquashing&rdquo;.</p>
</td></tr>
<tr><td><code id="squashData_+3A_min_pts">min_pts</code></td>
<td>
<p>A positive scalar whole number for the minimum number of
original (unsquashed) points needed for squashing. Used to help prevent
&ldquo;oversquashing&rdquo;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Can be used iteratively (count = 1, then 2, etc.).
</p>
<p>The <em>N</em> column in <code>data</code> will be coerced using
<code><a href="base.html#topic+as.integer">as.integer</a></code>, and <em>E</em> will be coerced using
<code><a href="base.html#topic+as.numeric">as.numeric</a></code>. Missing data are not allowed.
</p>
<p>Since the distribution of expected counts, <em>E</em>, tends to be
skewed to the right, the largest <em>E</em>s are not squashed by default.
This behavior can be changed by setting the <code>keep_pts</code> argument to
zero (0); however, this is not recommended. Squashing the largest <em>E</em>s
could result in a large loss of information, so it is recommended to use a
value of 100 or more for <code>keep_pts</code>.
</p>
<p>Values for <code>keep_pts</code>, <code>min_bin</code>, and <code>min_pts</code>
should typically be at least as large as the default values.
</p>


<h3>Value</h3>

<p>A data frame with column names <em>N</em>, <em>E</em>, and
<em>weight</em> containing the reduced data set.
</p>


<h3>References</h3>

<p>DuMouchel W, Pregibon D (2001). &quot;Empirical Bayes Screening for
Multi-item Associations.&quot; In <em>Proceedings of the Seventh ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining</em>, KDD '01,
pp. 67-76. ACM, New York, NY, USA. ISBN 1-58113-391-X.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+processRaw">processRaw</a></code> for data preparation and
<code><a href="#topic+autoSquash">autoSquash</a></code> for automatically squashing an entire data set in
one function call
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(483726)
dat &lt;- data.frame(
  var1 = letters[1:26], var2 = LETTERS[1:26],
  N = c(rep(0, 11), rep(1, 10), rep(2, 4), rep(3, 1)),
  E = round(abs(c(rnorm(11, 0), rnorm(10, 1), rnorm(4, 2), rnorm(1, 3))), 3),
  stringsAsFactors = FALSE
)
(zeroes &lt;- squashData(dat, count = 0, bin_size = 3, keep_pts = 1,
                      min_bin = 2, min_pts = 2))
(ones &lt;- squashData(zeroes, bin_size = 2, keep_pts = 1,
                    min_bin = 2, min_pts = 2))
(twos &lt;- squashData(ones, count = 2, bin_size = 2, keep_pts = 1,
                    min_bin = 2, min_pts = 2))

squashData(zeroes, bin_size = 2, keep_pts = 0,
           min_bin = 2, min_pts = 2)
squashData(zeroes, bin_size = 2, keep_pts = 1,
           min_bin = 2, min_pts = 2)
squashData(zeroes, bin_size = 2, keep_pts = 2,
           min_bin = 2, min_pts = 2)
squashData(zeroes, bin_size = 2, keep_pts = 3,
           min_bin = 2, min_pts = 2)

</code></pre>

<hr>
<h2 id='summary.openEBGM'>Summarize an openEBGM object</h2><span id='topic+summary.openEBGM'></span>

<h3>Description</h3>

<p>Summarize an openEBGM object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'openEBGM'
summary(object, plot.out = TRUE, log.trans = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.openEBGM_+3A_object">object</code></td>
<td>
<p>An openEBGM object constructed by <code><a href="#topic+ebScores">ebScores</a></code></p>
</td></tr>
<tr><td><code id="summary.openEBGM_+3A_plot.out">plot.out</code></td>
<td>
<p>A logical value indicating whether or not a histogram of the
EBGM scores should be displayed</p>
</td></tr>
<tr><td><code id="summary.openEBGM_+3A_log.trans">log.trans</code></td>
<td>
<p>A logical value indicating whether or not the data should be
log-transformed.</p>
</td></tr>
<tr><td><code id="summary.openEBGM_+3A_...">...</code></td>
<td>
<p>Additional arguments affecting the summary produced</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function provides a brief summary of the results of the
calculations performed in the <code><a href="#topic+ebScores">ebScores</a></code> function. In
particular, it provides the numerical summary of the EBGM and
QUANT_* vectors.
</p>
<p>Additionally, calling <code><a href="base.html#topic+summary">summary</a></code> on an openEBGM
object will produce a histogram of the EBGM scores. By setting the
log.trans parameter to TRUE, one can convert the EBGM score to
EBlog2, which is a Bayesian version of the information criterion
(DuMouchel).
</p>


<h3>References</h3>

<p>DuMouchel W (1999). &quot;Bayesian Data Mining in Large Frequency
Tables, With an Application to the FDA Spontaneous Reporting System.&quot;
<em>The American Statistician</em>, 53(3), 177-190.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data.table::setDTthreads(2)  #only needed for CRAN checks
theta_init &lt;- data.frame(
  alpha1 = c(0.5, 1),
  beta1  = c(0.5, 1),
  alpha2 = c(2,   3),
  beta2  = c(2,   3),
  p      = c(0.1, 0.2)
)
data(caers)
proc &lt;- processRaw(caers)
squashed &lt;- squashData(proc, bin_size = 300, keep_pts = 10)
squashed &lt;- squashData(squashed, count = 2, bin_size = 13, keep_pts = 10)
suppressWarnings(
  hypers &lt;- autoHyper(data = squashed, theta_init = theta_init)
)
ebout &lt;- ebScores(processed = proc, hyper_estimate = hypers, quantiles = 5)
summary(ebout)
## Not run: summary(ebout, plot.out = FALSE)
## Not run: summary(ebout, log.trans = TRUE)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
