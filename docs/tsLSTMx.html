<!DOCTYPE html><html><head><title>Help for package tsLSTMx</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {tsLSTMx}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#best_model_on_validation'><p>Evaluate the best LSTM model on the validation set</p></a></li>
<li><a href='#check_and_format_data'><p>Check and Format Data</p></a></li>
<li><a href='#compare_predicted_vs_actual'><p>Compare predicted and actual values for training and validation sets</p></a></li>
<li><a href='#convert_to_numeric_matrices'><p>Function to convert columns to numeric matrices</p></a></li>
<li><a href='#convert_to_tensors'><p>Function to convert data to TensorFlow tensors</p></a></li>
<li><a href='#define_early_stopping'><p>Function to define early stopping callback</p></a></li>
<li><a href='#embed_columns'><p>Embed columns and create a new data frame</p></a></li>
<li><a href='#forecast_best_model'><p>Perform forecasting using the best model</p></a></li>
<li><a href='#initialize_tensorflow'><p>Function to initialize TensorFlow and enable eager execution</p></a></li>
<li><a href='#predict_y_values'><p>Predict y values for the training and validation sets using the best LSTM model</p></a></li>
<li><a href='#reshape_for_lstm'><p>Function to reshape input data for LSTM</p></a></li>
<li><a href='#split_data'><p>Split data into training and validation sets</p></a></li>
<li><a href='#ts_lstm_x_tuning'><p>Time Series LSTM Hyperparameter Tuning</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Predict Time Series Using LSTM Model Including Exogenous
Variable to Denote Zero Values</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.0</td>
</tr>
<tr>
<td>Author:</td>
<td>Sandip Garai [aut, cre],
  Krishna Pada Sarkar [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Sandip Garai &lt;sandipnicksandy@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>It is a versatile tool for predicting time series data using Long Short-Term Memory (LSTM) models. It is specifically designed to handle time series with an exogenous variable, allowing users to denote whether data was available for a particular period or not. The package encompasses various functionalities, including hyperparameter tuning, custom loss function support, model evaluation, and one-step-ahead forecasting. With an emphasis on ease of use and flexibility, it empowers users to explore, evaluate, and deploy LSTM models for accurate time series predictions and forecasting in diverse applications. More details can be found in Garai and Paul (2023) &lt;<a href="https://doi.org/10.1016%2Fj.iswa.2023.200202">doi:10.1016/j.iswa.2023.200202</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Imports:</td>
<td>tensorflow, AllMetrics, keras, reticulate</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-01-11 18:04:43 UTC; user</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-01-12 09:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='best_model_on_validation'>Evaluate the best LSTM model on the validation set</h2><span id='topic+best_model_on_validation'></span>

<h3>Description</h3>

<p>This function evaluates the performance of the best LSTM model on the provided validation set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>best_model_on_validation(best_model, X_val, y_val)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="best_model_on_validation_+3A_best_model">best_model</code></td>
<td>
<p>The best LSTM model obtained from hyperparameter tuning.</p>
</td></tr>
<tr><td><code id="best_model_on_validation_+3A_x_val">X_val</code></td>
<td>
<p>The validation set input data.</p>
</td></tr>
<tr><td><code id="best_model_on_validation_+3A_y_val">y_val</code></td>
<td>
<p>The validation set target data.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The validation loss of the best model on the provided validation set.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data &lt;- data.frame(
  Date = as.Date(c("01-04-18", "02-04-18", "03-04-18", "04-04-18", "05-04-18",
                   "06-04-18", "07-04-18", "08-04-18", "09-04-18", "10-04-18",
                   "11-04-18", "12-04-18", "13-04-18", "14-04-18", "15-04-18",
                   "16-04-18", "17-04-18", "18-04-18", "19-04-18", "20-04-18"),
                 format = "%d-%m-%y"),
  A = c(0, 0, 4, 12, 20, 16, 16, 0, 12, 18, 12, 18, 18, 0, 0, 33, 31, 38, 76, 198)
)
check_and_format_data(data)
# Add a new column 'X' based on the values in the second column
data$X &lt;- ifelse(data$A != 0, 1, 0)

result_embed &lt;- embed_columns(data = data, n_lag = 2)
new_data &lt;- result_embed$data_frame
embedded_colnames &lt;- result_embed$column_names

result_split &lt;- split_data(new_data = new_data, val_ratio = 0.1)
train_data &lt;- result_split$train_data
validation_data &lt;- result_split$validation_data
train_data &lt;- result_split$train_data
validation_data &lt;- result_split$validation_data
embedded_colnames &lt;- result_embed$column_names
numeric_matrices &lt;- convert_to_numeric_matrices(train_data = train_data,
                                                validation_data = validation_data,
                                                embedded_colnames = embedded_colnames)
X_train &lt;- numeric_matrices$X_train
y_train &lt;- numeric_matrices$y_train
X_val &lt;- numeric_matrices$X_val
y_val &lt;- numeric_matrices$y_val

#' initialize_tensorflow()

X_train &lt;- numeric_matrices$X_train
X_val &lt;- numeric_matrices$X_val
reshaped_data &lt;- reshape_for_lstm(X_train = X_train, X_val = X_val)
X_train &lt;- reshaped_data$X_train
X_val &lt;- reshaped_data$X_val
X_train &lt;- reshaped_data$X_train
y_train &lt;- numeric_matrices$y_train
X_val &lt;- reshaped_data$X_val
y_val &lt;- numeric_matrices$y_val
tf &lt;- reticulate::import("tensorflow")
tensors &lt;- convert_to_tensors(X_train = X_train, y_train = y_train, X_val = X_val, y_val = y_val)
X_train &lt;- tensors$X_train
y_train &lt;- tensors$y_train
X_val &lt;- tensors$X_val
y_val &lt;- tensors$y_val
n_patience &lt;- 50
early_stopping &lt;- define_early_stopping(n_patience = n_patience)

X_train &lt;- tensors$X_train
X_val &lt;- tensors$X_val

y_train &lt;- tensors$y_train
y_val &lt;- tensors$y_val

embedded_colnames &lt;- result_embed$column_names

# Define your custom loss function
custom_loss &lt;- function(y_true, y_pred) {
  condition &lt;- tf$math$equal(y_true, 0)
  loss &lt;- tf$math$reduce_mean(tf$math$square(y_true - y_pred))  # Remove 'axis'
  loss &lt;- tf$where(condition, tf$constant(0), loss)
  return(loss)
}

early_stopping &lt;- define_early_stopping(n_patience = n_patience)

grid_search_results &lt;- ts_lstm_x_tuning(
  X_train, y_train, X_val, y_val,
  embedded_colnames, custom_loss, early_stopping,
  n_lag = 2, # desired lag value
  lstm_units_list = c(32),
  learning_rate_list = c(0.001, 0.01),
  batch_size_list = c(32),
  dropout_list = c(0.2),
  l1_reg_list = c(0.001),
  l2_reg_list = c(0.001),
  n_iter = 10,
  n_verbose = 0 # or 1
)

results_df &lt;- grid_search_results$results_df
all_histories &lt;- grid_search_results$all_histories
lstm_models &lt;- grid_search_results$lstm_models

# Find the row with the minimum val_loss_mae in results_df
min_val_loss_row &lt;- results_df[which.min(results_df$val_loss_mae), ]

# Extract hyperparameters from the row
best_lstm_units &lt;- min_val_loss_row$lstm_units
best_learning_rate &lt;- min_val_loss_row$learning_rate
best_batch_size &lt;- min_val_loss_row$batch_size
best_n_lag &lt;- min_val_loss_row$n_lag
best_dropout &lt;- min_val_loss_row$dropout
best_l1_reg &lt;- min_val_loss_row$l1_reg
best_l2_reg &lt;- min_val_loss_row$l2_reg

# Generate the lstm_model_name for the best model
best_model_name &lt;- paste0("lstm_model_lu_", best_lstm_units, "_lr_", best_learning_rate,
                          "_bs_", best_batch_size, "_lag_", best_n_lag,
                          "_do_", best_dropout, "_l1_", best_l1_reg, "_l2_", best_l2_reg)

# Generate the history_name for the best model
best_history_name &lt;- paste0("history_lu_", best_lstm_units, "_lr_", best_learning_rate,
                            "_bs_", best_batch_size, "_lag_", best_n_lag,
                            "_do_", best_dropout, "_l1_", best_l1_reg, "_l2_", best_l2_reg)

# Access the best model from lstm_models
best_model &lt;- lstm_models[[best_model_name]]

best_model_details &lt;- data.frame(min_val_loss_row)

colnames(best_model_details) &lt;- colnames(results_df)

# Access the best model from lstm_models
best_history &lt;- all_histories[[best_history_name]]

validation_loss_best &lt;- best_model_on_validation(best_model, X_val, y_val)


</code></pre>

<hr>
<h2 id='check_and_format_data'>Check and Format Data</h2><span id='topic+check_and_format_data'></span>

<h3>Description</h3>

<p>This function checks the compatibility of a given data frame and performs necessary formatting.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_and_format_data(data, n.head = 6)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_and_format_data_+3A_data">data</code></td>
<td>
<p>A data frame containing a 'Date' column and a numeric column 'A'.</p>
</td></tr>
<tr><td><code id="check_and_format_data_+3A_n.head">n.head</code></td>
<td>
<p>Number of rows to display from the formatted data frame (default is 6).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function checks the format of the 'Date' column and ensures it is in the format 'dd-mm-yy'.
It also checks the presence of the 'A' column and ensures it contains numeric values.
</p>


<h3>Value</h3>

<p>A formatted data frame with the specified number of rows displayed.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data &lt;- data.frame(
  Date = as.Date(c("01-04-18", "02-04-18", "03-04-18", "04-04-18", "05-04-18",
                   "06-04-18", "07-04-18", "08-04-18", "09-04-18", "10-04-18",
                   "11-04-18", "12-04-18", "13-04-18", "14-04-18", "15-04-18",
                   "16-04-18", "17-04-18", "18-04-18", "19-04-18", "20-04-18"),
                 format = "%d-%m-%y"),
  A = c(0, 0, 4, 12, 20, 16, 16, 0, 12, 18, 12, 18, 18, 0, 0, 33, 31, 38, 76, 198)
)
check_and_format_data(data)
# Add a new column 'X' based on the values in the second column
data$X &lt;- ifelse(data$A != 0, 1, 0)

</code></pre>

<hr>
<h2 id='compare_predicted_vs_actual'>Compare predicted and actual values for training and validation sets</h2><span id='topic+compare_predicted_vs_actual'></span>

<h3>Description</h3>

<p>This function compares the predicted and actual values for the training and validation sets and computes metrics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compare_predicted_vs_actual(
  train_data,
  validation_data,
  y_train_pred,
  y_val_pred
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compare_predicted_vs_actual_+3A_train_data">train_data</code></td>
<td>
<p>The training set data, including actual y values.</p>
</td></tr>
<tr><td><code id="compare_predicted_vs_actual_+3A_validation_data">validation_data</code></td>
<td>
<p>The validation set data, including actual y values.</p>
</td></tr>
<tr><td><code id="compare_predicted_vs_actual_+3A_y_train_pred">y_train_pred</code></td>
<td>
<p>Predicted y values for the training set.</p>
</td></tr>
<tr><td><code id="compare_predicted_vs_actual_+3A_y_val_pred">y_val_pred</code></td>
<td>
<p>Predicted y values for the validation set.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing data frames with the comparison of actual vs. predicted values for training and validation sets,
as well as metrics for the training and validation sets.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data &lt;- data.frame(
  Date = as.Date(c("01-04-18", "02-04-18", "03-04-18", "04-04-18", "05-04-18",
                   "06-04-18", "07-04-18", "08-04-18", "09-04-18", "10-04-18",
                   "11-04-18", "12-04-18", "13-04-18", "14-04-18", "15-04-18",
                   "16-04-18", "17-04-18", "18-04-18", "19-04-18", "20-04-18"),
                 format = "%d-%m-%y"),
  A = c(0, 0, 4, 12, 20, 16, 16, 0, 12, 18, 12, 18, 18, 0, 0, 33, 31, 38, 76, 198)
)
check_and_format_data(data)
# Add a new column 'X' based on the values in the second column
data$X &lt;- ifelse(data$A != 0, 1, 0)

result_embed &lt;- embed_columns(data = data, n_lag = 2)
new_data &lt;- result_embed$data_frame
embedded_colnames &lt;- result_embed$column_names

result_split &lt;- split_data(new_data = new_data, val_ratio = 0.1)
train_data &lt;- result_split$train_data
validation_data &lt;- result_split$validation_data
train_data &lt;- result_split$train_data
validation_data &lt;- result_split$validation_data
embedded_colnames &lt;- result_embed$column_names
numeric_matrices &lt;- convert_to_numeric_matrices(train_data = train_data,
                                                validation_data = validation_data,
                                                embedded_colnames = embedded_colnames)
X_train &lt;- numeric_matrices$X_train
y_train &lt;- numeric_matrices$y_train
X_val &lt;- numeric_matrices$X_val
y_val &lt;- numeric_matrices$y_val

#' initialize_tensorflow()

X_train &lt;- numeric_matrices$X_train
X_val &lt;- numeric_matrices$X_val
reshaped_data &lt;- reshape_for_lstm(X_train = X_train, X_val = X_val)
X_train &lt;- reshaped_data$X_train
X_val &lt;- reshaped_data$X_val
X_train &lt;- reshaped_data$X_train
y_train &lt;- numeric_matrices$y_train
X_val &lt;- reshaped_data$X_val
y_val &lt;- numeric_matrices$y_val
tf &lt;- reticulate::import("tensorflow")
tensors &lt;- convert_to_tensors(X_train = X_train, y_train = y_train, X_val = X_val, y_val = y_val)
X_train &lt;- tensors$X_train
y_train &lt;- tensors$y_train
X_val &lt;- tensors$X_val
y_val &lt;- tensors$y_val
n_patience &lt;- 50
early_stopping &lt;- define_early_stopping(n_patience = n_patience)

X_train &lt;- tensors$X_train
X_val &lt;- tensors$X_val

y_train &lt;- tensors$y_train
y_val &lt;- tensors$y_val

embedded_colnames &lt;- result_embed$column_names

# Define your custom loss function
custom_loss &lt;- function(y_true, y_pred) {
  condition &lt;- tf$math$equal(y_true, 0)
  loss &lt;- tf$math$reduce_mean(tf$math$square(y_true - y_pred))  # Remove 'axis'
  loss &lt;- tf$where(condition, tf$constant(0), loss)
  return(loss)
}

early_stopping &lt;- define_early_stopping(n_patience = n_patience)

grid_search_results &lt;- ts_lstm_x_tuning(
  X_train, y_train, X_val, y_val,
  embedded_colnames, custom_loss, early_stopping,
  n_lag = 2, # desired lag value
  lstm_units_list = c(32),
  learning_rate_list = c(0.001, 0.01),
  batch_size_list = c(32),
  dropout_list = c(0.2),
  l1_reg_list = c(0.001),
  l2_reg_list = c(0.001),
  n_iter = 10,
  n_verbose = 0 # or 1
)

results_df &lt;- grid_search_results$results_df
all_histories &lt;- grid_search_results$all_histories
lstm_models &lt;- grid_search_results$lstm_models

# Find the row with the minimum val_loss_mae in results_df
min_val_loss_row &lt;- results_df[which.min(results_df$val_loss_mae), ]

# Extract hyperparameters from the row
best_lstm_units &lt;- min_val_loss_row$lstm_units
best_learning_rate &lt;- min_val_loss_row$learning_rate
best_batch_size &lt;- min_val_loss_row$batch_size
best_n_lag &lt;- min_val_loss_row$n_lag
best_dropout &lt;- min_val_loss_row$dropout
best_l1_reg &lt;- min_val_loss_row$l1_reg
best_l2_reg &lt;- min_val_loss_row$l2_reg

# Generate the lstm_model_name for the best model
best_model_name &lt;- paste0("lstm_model_lu_", best_lstm_units, "_lr_", best_learning_rate,
                          "_bs_", best_batch_size, "_lag_", best_n_lag,
                          "_do_", best_dropout, "_l1_", best_l1_reg, "_l2_", best_l2_reg)

# Generate the history_name for the best model
best_history_name &lt;- paste0("history_lu_", best_lstm_units, "_lr_", best_learning_rate,
                            "_bs_", best_batch_size, "_lag_", best_n_lag,
                            "_do_", best_dropout, "_l1_", best_l1_reg, "_l2_", best_l2_reg)

# Access the best model from lstm_models
best_model &lt;- lstm_models[[best_model_name]]

best_model_details &lt;- data.frame(min_val_loss_row)

colnames(best_model_details) &lt;- colnames(results_df)

# Access the best model from lstm_models
best_history &lt;- all_histories[[best_history_name]]

validation_loss_best &lt;- best_model_on_validation(best_model, X_val, y_val)
predicted_values &lt;- predict_y_values(best_model, X_train, X_val, train_data, validation_data)
y_train_pred &lt;- predicted_values$y_train_pred
y_val_pred &lt;- predicted_values$y_val_pred
comparison &lt;- compare_predicted_vs_actual(train_data, validation_data, y_train_pred, y_val_pred)
compare_train &lt;- comparison$compare_train
compare_val &lt;- comparison$compare_val
metrics_train &lt;- comparison$metrics_train
metrics_val &lt;- comparison$metrics_val


</code></pre>

<hr>
<h2 id='convert_to_numeric_matrices'>Function to convert columns to numeric matrices</h2><span id='topic+convert_to_numeric_matrices'></span>

<h3>Description</h3>

<p>This function converts specific columns in the data frames to numeric matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>convert_to_numeric_matrices(train_data, validation_data, embedded_colnames)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="convert_to_numeric_matrices_+3A_train_data">train_data</code></td>
<td>
<p>Training data frame.</p>
</td></tr>
<tr><td><code id="convert_to_numeric_matrices_+3A_validation_data">validation_data</code></td>
<td>
<p>Validation data frame.</p>
</td></tr>
<tr><td><code id="convert_to_numeric_matrices_+3A_embedded_colnames">embedded_colnames</code></td>
<td>
<p>Names of the embedded columns.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing numeric matrices for training and validation sets.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data &lt;- data.frame(
  Date = as.Date(c("01-04-18", "02-04-18", "03-04-18", "04-04-18", "05-04-18",
                   "06-04-18", "07-04-18", "08-04-18", "09-04-18", "10-04-18",
                   "11-04-18", "12-04-18", "13-04-18", "14-04-18", "15-04-18",
                   "16-04-18", "17-04-18", "18-04-18", "19-04-18", "20-04-18"),
                 format = "%d-%m-%y"),
  A = c(0, 0, 4, 12, 20, 16, 16, 0, 12, 18, 12, 18, 18, 0, 0, 33, 31, 38, 76, 198)
)
check_and_format_data(data)
# Add a new column 'X' based on the values in the second column
data$X &lt;- ifelse(data$A != 0, 1, 0)

result_embed &lt;- embed_columns(data = data, n_lag = 2)
new_data &lt;- result_embed$data_frame
embedded_colnames &lt;- result_embed$column_names

result_split &lt;- split_data(new_data = new_data, val_ratio = 0.1)
train_data &lt;- result_split$train_data
validation_data &lt;- result_split$validation_data
train_data &lt;- result_split$train_data
validation_data &lt;- result_split$validation_data
embedded_colnames &lt;- result_embed$column_names
numeric_matrices &lt;- convert_to_numeric_matrices(train_data = train_data,
                                                validation_data = validation_data,
                                                embedded_colnames = embedded_colnames)
X_train &lt;- numeric_matrices$X_train
y_train &lt;- numeric_matrices$y_train
X_val &lt;- numeric_matrices$X_val
y_val &lt;- numeric_matrices$y_val

#' initialize_tensorflow()


</code></pre>

<hr>
<h2 id='convert_to_tensors'>Function to convert data to TensorFlow tensors</h2><span id='topic+convert_to_tensors'></span>

<h3>Description</h3>

<p>This function converts input data to TensorFlow tensors for compatibility with TensorFlow and keras models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>convert_to_tensors(X_train, y_train, X_val, y_val)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="convert_to_tensors_+3A_x_train">X_train</code></td>
<td>
<p>Numeric matrix representing the training input data.</p>
</td></tr>
<tr><td><code id="convert_to_tensors_+3A_y_train">y_train</code></td>
<td>
<p>Numeric vector representing the training output data.</p>
</td></tr>
<tr><td><code id="convert_to_tensors_+3A_x_val">X_val</code></td>
<td>
<p>Numeric matrix representing the validation input data.</p>
</td></tr>
<tr><td><code id="convert_to_tensors_+3A_y_val">y_val</code></td>
<td>
<p>Numeric vector representing the validation output data.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing TensorFlow tensors for training and validation data.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data &lt;- data.frame(
  Date = as.Date(c("01-04-18", "02-04-18", "03-04-18", "04-04-18", "05-04-18",
                   "06-04-18", "07-04-18", "08-04-18", "09-04-18", "10-04-18",
                   "11-04-18", "12-04-18", "13-04-18", "14-04-18", "15-04-18",
                   "16-04-18", "17-04-18", "18-04-18", "19-04-18", "20-04-18"),
                 format = "%d-%m-%y"),
  A = c(0, 0, 4, 12, 20, 16, 16, 0, 12, 18, 12, 18, 18, 0, 0, 33, 31, 38, 76, 198)
)
check_and_format_data(data)
# Add a new column 'X' based on the values in the second column
data$X &lt;- ifelse(data$A != 0, 1, 0)

result_embed &lt;- embed_columns(data = data, n_lag = 2)
new_data &lt;- result_embed$data_frame
embedded_colnames &lt;- result_embed$column_names

result_split &lt;- split_data(new_data = new_data, val_ratio = 0.1)
train_data &lt;- result_split$train_data
validation_data &lt;- result_split$validation_data
train_data &lt;- result_split$train_data
validation_data &lt;- result_split$validation_data
embedded_colnames &lt;- result_embed$column_names
numeric_matrices &lt;- convert_to_numeric_matrices(train_data = train_data,
                                                validation_data = validation_data,
                                                embedded_colnames = embedded_colnames)
X_train &lt;- numeric_matrices$X_train
y_train &lt;- numeric_matrices$y_train
X_val &lt;- numeric_matrices$X_val
y_val &lt;- numeric_matrices$y_val

#' initialize_tensorflow()

X_train &lt;- numeric_matrices$X_train
X_val &lt;- numeric_matrices$X_val
reshaped_data &lt;- reshape_for_lstm(X_train = X_train, X_val = X_val)
X_train &lt;- reshaped_data$X_train
X_val &lt;- reshaped_data$X_val
X_train &lt;- reshaped_data$X_train
y_train &lt;- numeric_matrices$y_train
X_val &lt;- reshaped_data$X_val
y_val &lt;- numeric_matrices$y_val
tf &lt;- reticulate::import("tensorflow")
tensors &lt;- convert_to_tensors(X_train = X_train, y_train = y_train, X_val = X_val, y_val = y_val)
X_train &lt;- tensors$X_train
y_train &lt;- tensors$y_train
X_val &lt;- tensors$X_val
y_val &lt;- tensors$y_val


</code></pre>

<hr>
<h2 id='define_early_stopping'>Function to define early stopping callback</h2><span id='topic+define_early_stopping'></span>

<h3>Description</h3>

<p>This function defines an early stopping callback for keras models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>define_early_stopping(n_patience)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="define_early_stopping_+3A_n_patience">n_patience</code></td>
<td>
<p>Integer specifying the number of epochs with no improvement after which training will be stopped.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A keras early stopping callback.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data &lt;- data.frame(
  Date = as.Date(c("01-04-18", "02-04-18", "03-04-18", "04-04-18", "05-04-18",
                   "06-04-18", "07-04-18", "08-04-18", "09-04-18", "10-04-18",
                   "11-04-18", "12-04-18", "13-04-18", "14-04-18", "15-04-18",
                   "16-04-18", "17-04-18", "18-04-18", "19-04-18", "20-04-18"),
                 format = "%d-%m-%y"),
  A = c(0, 0, 4, 12, 20, 16, 16, 0, 12, 18, 12, 18, 18, 0, 0, 33, 31, 38, 76, 198)
)
check_and_format_data(data)
# Add a new column 'X' based on the values in the second column
data$X &lt;- ifelse(data$A != 0, 1, 0)

result_embed &lt;- embed_columns(data = data, n_lag = 2)
new_data &lt;- result_embed$data_frame
embedded_colnames &lt;- result_embed$column_names

result_split &lt;- split_data(new_data = new_data, val_ratio = 0.1)
train_data &lt;- result_split$train_data
validation_data &lt;- result_split$validation_data
train_data &lt;- result_split$train_data
validation_data &lt;- result_split$validation_data
embedded_colnames &lt;- result_embed$column_names
numeric_matrices &lt;- convert_to_numeric_matrices(train_data = train_data,
                                                validation_data = validation_data,
                                                embedded_colnames = embedded_colnames)
X_train &lt;- numeric_matrices$X_train
y_train &lt;- numeric_matrices$y_train
X_val &lt;- numeric_matrices$X_val
y_val &lt;- numeric_matrices$y_val

#' initialize_tensorflow()

X_train &lt;- numeric_matrices$X_train
X_val &lt;- numeric_matrices$X_val
reshaped_data &lt;- reshape_for_lstm(X_train = X_train, X_val = X_val)
X_train &lt;- reshaped_data$X_train
X_val &lt;- reshaped_data$X_val
X_train &lt;- reshaped_data$X_train
y_train &lt;- numeric_matrices$y_train
X_val &lt;- reshaped_data$X_val
y_val &lt;- numeric_matrices$y_val
tf &lt;- reticulate::import("tensorflow")
tensors &lt;- convert_to_tensors(X_train = X_train, y_train = y_train, X_val = X_val, y_val = y_val)
X_train &lt;- tensors$X_train
y_train &lt;- tensors$y_train
X_val &lt;- tensors$X_val
y_val &lt;- tensors$y_val
n_patience &lt;- 50
early_stopping &lt;- define_early_stopping(n_patience = n_patience)


</code></pre>

<hr>
<h2 id='embed_columns'>Embed columns and create a new data frame</h2><span id='topic+embed_columns'></span>

<h3>Description</h3>

<p>This function takes a data frame and embeds specified columns to create a new data frame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>embed_columns(data, n_lag = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="embed_columns_+3A_data">data</code></td>
<td>
<p>A data frame containing the original columns.</p>
</td></tr>
<tr><td><code id="embed_columns_+3A_n_lag">n_lag</code></td>
<td>
<p>Number of lags for embedding.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the new data frame and column names of the embedded columns.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data &lt;- data.frame(
  Date = as.Date(c("01-04-18", "02-04-18", "03-04-18", "04-04-18", "05-04-18",
                   "06-04-18", "07-04-18", "08-04-18", "09-04-18", "10-04-18",
                   "11-04-18", "12-04-18", "13-04-18", "14-04-18", "15-04-18",
                   "16-04-18", "17-04-18", "18-04-18", "19-04-18", "20-04-18"),
                 format = "%d-%m-%y"),
  A = c(0, 0, 4, 12, 20, 16, 16, 0, 12, 18, 12, 18, 18, 0, 0, 33, 31, 38, 76, 198)
)
check_and_format_data(data)
# Add a new column 'X' based on the values in the second column
data$X &lt;- ifelse(data$A != 0, 1, 0)

result_embed &lt;- embed_columns(data = data, n_lag = 2)
new_data &lt;- result_embed$data_frame
embedded_colnames &lt;- result_embed$column_names


</code></pre>

<hr>
<h2 id='forecast_best_model'>Perform forecasting using the best model</h2><span id='topic+forecast_best_model'></span>

<h3>Description</h3>

<p>This function performs forecasting using the best-trained model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>forecast_best_model(
  best_model,
  best_learning_rate,
  custom_loss,
  n_lag = 2,
  new_data,
  test,
  forecast_steps
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="forecast_best_model_+3A_best_model">best_model</code></td>
<td>
<p>The best-trained LSTM model.</p>
</td></tr>
<tr><td><code id="forecast_best_model_+3A_best_learning_rate">best_learning_rate</code></td>
<td>
<p>The best learning rate used during training.</p>
</td></tr>
<tr><td><code id="forecast_best_model_+3A_custom_loss">custom_loss</code></td>
<td>
<p>The custom loss function used during training.</p>
</td></tr>
<tr><td><code id="forecast_best_model_+3A_n_lag">n_lag</code></td>
<td>
<p>The lag value used during training.</p>
</td></tr>
<tr><td><code id="forecast_best_model_+3A_new_data">new_data</code></td>
<td>
<p>The input data for forecasting.</p>
</td></tr>
<tr><td><code id="forecast_best_model_+3A_test">test</code></td>
<td>
<p>The test data frame containing the input data for forecasting.</p>
</td></tr>
<tr><td><code id="forecast_best_model_+3A_forecast_steps">forecast_steps</code></td>
<td>
<p>The number of steps to forecast.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the forecasted values, actual vs. forecasted data frame, and metrics for forecasting.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data &lt;- data.frame(
  Date = as.Date(c("01-04-18", "02-04-18", "03-04-18", "04-04-18", "05-04-18",
                   "06-04-18", "07-04-18", "08-04-18", "09-04-18", "10-04-18",
                   "11-04-18", "12-04-18", "13-04-18", "14-04-18", "15-04-18",
                   "16-04-18", "17-04-18", "18-04-18", "19-04-18", "20-04-18"),
                 format = "%d-%m-%y"),
  A = c(0, 0, 4, 12, 20, 16, 16, 0, 12, 18, 12, 18, 18, 0, 0, 33, 31, 38, 76, 198)
)
check_and_format_data(data)
# Add a new column 'X' based on the values in the second column
data$X &lt;- ifelse(data$A != 0, 1, 0)

result_embed &lt;- embed_columns(data = data, n_lag = 2)
new_data &lt;- result_embed$data_frame
embedded_colnames &lt;- result_embed$column_names

result_split &lt;- split_data(new_data = new_data, val_ratio = 0.1)
train_data &lt;- result_split$train_data
validation_data &lt;- result_split$validation_data
train_data &lt;- result_split$train_data
validation_data &lt;- result_split$validation_data
embedded_colnames &lt;- result_embed$column_names
numeric_matrices &lt;- convert_to_numeric_matrices(train_data = train_data,
                                                validation_data = validation_data,
                                                embedded_colnames = embedded_colnames)
X_train &lt;- numeric_matrices$X_train
y_train &lt;- numeric_matrices$y_train
X_val &lt;- numeric_matrices$X_val
y_val &lt;- numeric_matrices$y_val

#' initialize_tensorflow()

X_train &lt;- numeric_matrices$X_train
X_val &lt;- numeric_matrices$X_val
reshaped_data &lt;- reshape_for_lstm(X_train = X_train, X_val = X_val)
X_train &lt;- reshaped_data$X_train
X_val &lt;- reshaped_data$X_val
X_train &lt;- reshaped_data$X_train
y_train &lt;- numeric_matrices$y_train
X_val &lt;- reshaped_data$X_val
y_val &lt;- numeric_matrices$y_val
tf &lt;- reticulate::import("tensorflow")
tensors &lt;- convert_to_tensors(X_train = X_train, y_train = y_train, X_val = X_val, y_val = y_val)
X_train &lt;- tensors$X_train
y_train &lt;- tensors$y_train
X_val &lt;- tensors$X_val
y_val &lt;- tensors$y_val
n_patience &lt;- 50
early_stopping &lt;- define_early_stopping(n_patience = n_patience)

X_train &lt;- tensors$X_train
X_val &lt;- tensors$X_val

y_train &lt;- tensors$y_train
y_val &lt;- tensors$y_val

embedded_colnames &lt;- result_embed$column_names

# Define your custom loss function
custom_loss &lt;- function(y_true, y_pred) {
  condition &lt;- tf$math$equal(y_true, 0)
  loss &lt;- tf$math$reduce_mean(tf$math$square(y_true - y_pred))  # Remove 'axis'
  loss &lt;- tf$where(condition, tf$constant(0), loss)
  return(loss)
}

early_stopping &lt;- define_early_stopping(n_patience = n_patience)

grid_search_results &lt;- ts_lstm_x_tuning(
  X_train, y_train, X_val, y_val,
  embedded_colnames, custom_loss, early_stopping,
  n_lag = 2, # desired lag value
  lstm_units_list = c(32),
  learning_rate_list = c(0.001, 0.01),
  batch_size_list = c(32),
  dropout_list = c(0.2),
  l1_reg_list = c(0.001),
  l2_reg_list = c(0.001),
  n_iter = 10,
  n_verbose = 0 # or 1
)

results_df &lt;- grid_search_results$results_df
all_histories &lt;- grid_search_results$all_histories
lstm_models &lt;- grid_search_results$lstm_models

# Find the row with the minimum val_loss_mae in results_df
min_val_loss_row &lt;- results_df[which.min(results_df$val_loss_mae), ]

# Extract hyperparameters from the row
best_lstm_units &lt;- min_val_loss_row$lstm_units
best_learning_rate &lt;- min_val_loss_row$learning_rate
best_batch_size &lt;- min_val_loss_row$batch_size
best_n_lag &lt;- min_val_loss_row$n_lag
best_dropout &lt;- min_val_loss_row$dropout
best_l1_reg &lt;- min_val_loss_row$l1_reg
best_l2_reg &lt;- min_val_loss_row$l2_reg

# Generate the lstm_model_name for the best model
best_model_name &lt;- paste0("lstm_model_lu_", best_lstm_units, "_lr_", best_learning_rate,
                          "_bs_", best_batch_size, "_lag_", best_n_lag,
                          "_do_", best_dropout, "_l1_", best_l1_reg, "_l2_", best_l2_reg)

# Generate the history_name for the best model
best_history_name &lt;- paste0("history_lu_", best_lstm_units, "_lr_", best_learning_rate,
                            "_bs_", best_batch_size, "_lag_", best_n_lag,
                            "_do_", best_dropout, "_l1_", best_l1_reg, "_l2_", best_l2_reg)

# Access the best model from lstm_models
best_model &lt;- lstm_models[[best_model_name]]

best_model_details &lt;- data.frame(min_val_loss_row)

colnames(best_model_details) &lt;- colnames(results_df)

# Access the best model from lstm_models
best_history &lt;- all_histories[[best_history_name]]

validation_loss_best &lt;- best_model_on_validation(best_model, X_val, y_val)
predicted_values &lt;- predict_y_values(best_model, X_train, X_val, train_data, validation_data)
y_train_pred &lt;- predicted_values$y_train_pred
y_val_pred &lt;- predicted_values$y_val_pred
comparison &lt;- compare_predicted_vs_actual(train_data, validation_data, y_train_pred, y_val_pred)
compare_train &lt;- comparison$compare_train
compare_val &lt;- comparison$compare_val
metrics_train &lt;- comparison$metrics_train
metrics_val &lt;- comparison$metrics_val

test &lt;- data.frame(
  Date = as.Date(c("01-04-23", "02-04-23", "03-04-23", "04-04-23", "05-04-23",
                   "06-04-23", "07-04-23", "08-04-23", "09-04-23", "10-04-23",
                   "11-04-23", "12-04-23", "13-04-23", "14-04-23", "15-04-23",
                   "16-04-23", "17-04-23", "18-04-23", "19-04-23", "20-04-23"),
                 format = "%d-%m-%y"),
  A = c(0, 0, 15, 4, -31, 24, 14, 0, 0, 33, 38, 33, 29, 29, 25, 0, 44, 67, 162, 278)
)

test$X &lt;- ifelse(test$A != 0, 1, 0)

n_forecast &lt;- nrow(test)

# Perform one-step-ahead forecasting
forecast_steps &lt;- n_forecast
current_row &lt;- nrow(new_data)
forecast_results &lt;- forecast_best_model(best_model, best_learning_rate,
                                        custom_loss, n_lag = 2,
                                        new_data, test,
                                        forecast_steps)

# Access the results
forecast_values &lt;- forecast_results$forecast_values
actual_vs_forecast &lt;- forecast_results$actual_vs_forecast
metrics_forecast &lt;- forecast_results$metrics_forecast


</code></pre>

<hr>
<h2 id='initialize_tensorflow'>Function to initialize TensorFlow and enable eager execution</h2><span id='topic+initialize_tensorflow'></span>

<h3>Description</h3>

<p>This function initializes TensorFlow and enables eager execution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>initialize_tensorflow()
</code></pre>


<h3>Value</h3>

<p>No return value, called for smooth running
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
initialize_tensorflow()


</code></pre>

<hr>
<h2 id='predict_y_values'>Predict y values for the training and validation sets using the best LSTM model</h2><span id='topic+predict_y_values'></span>

<h3>Description</h3>

<p>This function predicts y values for the training and validation sets using the provided LSTM model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_y_values(best_model, X_train, X_val, train_data, validation_data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_y_values_+3A_best_model">best_model</code></td>
<td>
<p>The best LSTM model obtained from hyperparameter tuning.</p>
</td></tr>
<tr><td><code id="predict_y_values_+3A_x_train">X_train</code></td>
<td>
<p>The training set input data.</p>
</td></tr>
<tr><td><code id="predict_y_values_+3A_x_val">X_val</code></td>
<td>
<p>The validation set input data.</p>
</td></tr>
<tr><td><code id="predict_y_values_+3A_train_data">train_data</code></td>
<td>
<p>The training set data, including x values.</p>
</td></tr>
<tr><td><code id="predict_y_values_+3A_validation_data">validation_data</code></td>
<td>
<p>The validation set data, including x values.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the predicted y values for the training and validation sets.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data &lt;- data.frame(
  Date = as.Date(c("01-04-18", "02-04-18", "03-04-18", "04-04-18", "05-04-18",
                   "06-04-18", "07-04-18", "08-04-18", "09-04-18", "10-04-18",
                   "11-04-18", "12-04-18", "13-04-18", "14-04-18", "15-04-18",
                   "16-04-18", "17-04-18", "18-04-18", "19-04-18", "20-04-18"),
                 format = "%d-%m-%y"),
  A = c(0, 0, 4, 12, 20, 16, 16, 0, 12, 18, 12, 18, 18, 0, 0, 33, 31, 38, 76, 198)
)
check_and_format_data(data)
# Add a new column 'X' based on the values in the second column
data$X &lt;- ifelse(data$A != 0, 1, 0)

result_embed &lt;- embed_columns(data = data, n_lag = 2)
new_data &lt;- result_embed$data_frame
embedded_colnames &lt;- result_embed$column_names

result_split &lt;- split_data(new_data = new_data, val_ratio = 0.1)
train_data &lt;- result_split$train_data
validation_data &lt;- result_split$validation_data
train_data &lt;- result_split$train_data
validation_data &lt;- result_split$validation_data
embedded_colnames &lt;- result_embed$column_names
numeric_matrices &lt;- convert_to_numeric_matrices(train_data = train_data,
                                                validation_data = validation_data,
                                                embedded_colnames = embedded_colnames)
X_train &lt;- numeric_matrices$X_train
y_train &lt;- numeric_matrices$y_train
X_val &lt;- numeric_matrices$X_val
y_val &lt;- numeric_matrices$y_val

#' initialize_tensorflow()

X_train &lt;- numeric_matrices$X_train
X_val &lt;- numeric_matrices$X_val
reshaped_data &lt;- reshape_for_lstm(X_train = X_train, X_val = X_val)
X_train &lt;- reshaped_data$X_train
X_val &lt;- reshaped_data$X_val
X_train &lt;- reshaped_data$X_train
y_train &lt;- numeric_matrices$y_train
X_val &lt;- reshaped_data$X_val
y_val &lt;- numeric_matrices$y_val
tf &lt;- reticulate::import("tensorflow")
tensors &lt;- convert_to_tensors(X_train = X_train, y_train = y_train, X_val = X_val, y_val = y_val)
X_train &lt;- tensors$X_train
y_train &lt;- tensors$y_train
X_val &lt;- tensors$X_val
y_val &lt;- tensors$y_val
n_patience &lt;- 50
early_stopping &lt;- define_early_stopping(n_patience = n_patience)

X_train &lt;- tensors$X_train
X_val &lt;- tensors$X_val

y_train &lt;- tensors$y_train
y_val &lt;- tensors$y_val

embedded_colnames &lt;- result_embed$column_names

# Define your custom loss function
custom_loss &lt;- function(y_true, y_pred) {
  condition &lt;- tf$math$equal(y_true, 0)
  loss &lt;- tf$math$reduce_mean(tf$math$square(y_true - y_pred))  # Remove 'axis'
  loss &lt;- tf$where(condition, tf$constant(0), loss)
  return(loss)
}

early_stopping &lt;- define_early_stopping(n_patience = n_patience)

grid_search_results &lt;- ts_lstm_x_tuning(
  X_train, y_train, X_val, y_val,
  embedded_colnames, custom_loss, early_stopping,
  n_lag = 2, # desired lag value
  lstm_units_list = c(32),
  learning_rate_list = c(0.001, 0.01),
  batch_size_list = c(32),
  dropout_list = c(0.2),
  l1_reg_list = c(0.001),
  l2_reg_list = c(0.001),
  n_iter = 10,
  n_verbose = 0 # or 1
)

results_df &lt;- grid_search_results$results_df
all_histories &lt;- grid_search_results$all_histories
lstm_models &lt;- grid_search_results$lstm_models

# Find the row with the minimum val_loss_mae in results_df
min_val_loss_row &lt;- results_df[which.min(results_df$val_loss_mae), ]

# Extract hyperparameters from the row
best_lstm_units &lt;- min_val_loss_row$lstm_units
best_learning_rate &lt;- min_val_loss_row$learning_rate
best_batch_size &lt;- min_val_loss_row$batch_size
best_n_lag &lt;- min_val_loss_row$n_lag
best_dropout &lt;- min_val_loss_row$dropout
best_l1_reg &lt;- min_val_loss_row$l1_reg
best_l2_reg &lt;- min_val_loss_row$l2_reg

# Generate the lstm_model_name for the best model
best_model_name &lt;- paste0("lstm_model_lu_", best_lstm_units, "_lr_", best_learning_rate,
                          "_bs_", best_batch_size, "_lag_", best_n_lag,
                          "_do_", best_dropout, "_l1_", best_l1_reg, "_l2_", best_l2_reg)

# Generate the history_name for the best model
best_history_name &lt;- paste0("history_lu_", best_lstm_units, "_lr_", best_learning_rate,
                            "_bs_", best_batch_size, "_lag_", best_n_lag,
                            "_do_", best_dropout, "_l1_", best_l1_reg, "_l2_", best_l2_reg)

# Access the best model from lstm_models
best_model &lt;- lstm_models[[best_model_name]]

best_model_details &lt;- data.frame(min_val_loss_row)

colnames(best_model_details) &lt;- colnames(results_df)

# Access the best model from lstm_models
best_history &lt;- all_histories[[best_history_name]]

validation_loss_best &lt;- best_model_on_validation(best_model, X_val, y_val)
predicted_values &lt;- predict_y_values(best_model, X_train, X_val, train_data, validation_data)
y_train_pred &lt;- predicted_values$y_train_pred
y_val_pred &lt;- predicted_values$y_val_pred


</code></pre>

<hr>
<h2 id='reshape_for_lstm'>Function to reshape input data for LSTM</h2><span id='topic+reshape_for_lstm'></span>

<h3>Description</h3>

<p>This function reshapes input data to be compatible with LSTM models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reshape_for_lstm(X_train, X_val)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reshape_for_lstm_+3A_x_train">X_train</code></td>
<td>
<p>Numeric matrix representing the training input data.</p>
</td></tr>
<tr><td><code id="reshape_for_lstm_+3A_x_val">X_val</code></td>
<td>
<p>Numeric matrix representing the validation input data.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing reshaped training and validation input data.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data &lt;- data.frame(
  Date = as.Date(c("01-04-18", "02-04-18", "03-04-18", "04-04-18", "05-04-18",
                   "06-04-18", "07-04-18", "08-04-18", "09-04-18", "10-04-18",
                   "11-04-18", "12-04-18", "13-04-18", "14-04-18", "15-04-18",
                   "16-04-18", "17-04-18", "18-04-18", "19-04-18", "20-04-18"),
                 format = "%d-%m-%y"),
  A = c(0, 0, 4, 12, 20, 16, 16, 0, 12, 18, 12, 18, 18, 0, 0, 33, 31, 38, 76, 198)
)
check_and_format_data(data)
# Add a new column 'X' based on the values in the second column
data$X &lt;- ifelse(data$A != 0, 1, 0)

result_embed &lt;- embed_columns(data = data, n_lag = 2)
new_data &lt;- result_embed$data_frame
embedded_colnames &lt;- result_embed$column_names

result_split &lt;- split_data(new_data = new_data, val_ratio = 0.1)
train_data &lt;- result_split$train_data
validation_data &lt;- result_split$validation_data
train_data &lt;- result_split$train_data
validation_data &lt;- result_split$validation_data
embedded_colnames &lt;- result_embed$column_names
numeric_matrices &lt;- convert_to_numeric_matrices(train_data = train_data,
                                                validation_data = validation_data,
                                                embedded_colnames = embedded_colnames)
X_train &lt;- numeric_matrices$X_train
y_train &lt;- numeric_matrices$y_train
X_val &lt;- numeric_matrices$X_val
y_val &lt;- numeric_matrices$y_val

#' initialize_tensorflow()

X_train &lt;- numeric_matrices$X_train
X_val &lt;- numeric_matrices$X_val
reshaped_data &lt;- reshape_for_lstm(X_train = X_train, X_val = X_val)
X_train &lt;- reshaped_data$X_train
X_val &lt;- reshaped_data$X_val


</code></pre>

<hr>
<h2 id='split_data'>Split data into training and validation sets</h2><span id='topic+split_data'></span>

<h3>Description</h3>

<p>This function takes a data frame and splits it into training and validation sets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>split_data(new_data, val_ratio = 0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="split_data_+3A_new_data">new_data</code></td>
<td>
<p>The data frame to be split.</p>
</td></tr>
<tr><td><code id="split_data_+3A_val_ratio">val_ratio</code></td>
<td>
<p>The ratio of the data to be used for validation (default is 0.1).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the training and validation data frames.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data &lt;- data.frame(
  Date = as.Date(c("01-04-18", "02-04-18", "03-04-18", "04-04-18", "05-04-18",
                   "06-04-18", "07-04-18", "08-04-18", "09-04-18", "10-04-18",
                   "11-04-18", "12-04-18", "13-04-18", "14-04-18", "15-04-18",
                   "16-04-18", "17-04-18", "18-04-18", "19-04-18", "20-04-18"),
                 format = "%d-%m-%y"),
  A = c(0, 0, 4, 12, 20, 16, 16, 0, 12, 18, 12, 18, 18, 0, 0, 33, 31, 38, 76, 198)
)
check_and_format_data(data)
# Add a new column 'X' based on the values in the second column
data$X &lt;- ifelse(data$A != 0, 1, 0)

result_embed &lt;- embed_columns(data = data, n_lag = 2)
new_data &lt;- result_embed$data_frame
embedded_colnames &lt;- result_embed$column_names

result_split &lt;- split_data(new_data = new_data, val_ratio = 0.1)
train_data &lt;- result_split$train_data
validation_data &lt;- result_split$validation_data


</code></pre>

<hr>
<h2 id='ts_lstm_x_tuning'>Time Series LSTM Hyperparameter Tuning</h2><span id='topic+ts_lstm_x_tuning'></span>

<h3>Description</h3>

<p>This function performs hyperparameter tuning for a Time Series LSTM model using a grid search approach.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ts_lstm_x_tuning(
  X_train,
  y_train,
  X_val,
  y_val,
  embedded_colnames,
  custom_loss,
  early_stopping,
  n_lag = 2,
  lstm_units_list = c(32),
  learning_rate_list = c(0.001, 0.01),
  batch_size_list = c(32),
  dropout_list = c(0.2),
  l1_reg_list = c(0.001),
  l2_reg_list = c(0.001),
  n_iter = 10,
  n_verbose = 0
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ts_lstm_x_tuning_+3A_x_train">X_train</code></td>
<td>
<p>Numeric matrix, the training input data.</p>
</td></tr>
<tr><td><code id="ts_lstm_x_tuning_+3A_y_train">y_train</code></td>
<td>
<p>Numeric vector, the training target data.</p>
</td></tr>
<tr><td><code id="ts_lstm_x_tuning_+3A_x_val">X_val</code></td>
<td>
<p>Numeric matrix, the validation input data.</p>
</td></tr>
<tr><td><code id="ts_lstm_x_tuning_+3A_y_val">y_val</code></td>
<td>
<p>Numeric vector, the validation target data.</p>
</td></tr>
<tr><td><code id="ts_lstm_x_tuning_+3A_embedded_colnames">embedded_colnames</code></td>
<td>
<p>Character vector, column names of the embedded features.</p>
</td></tr>
<tr><td><code id="ts_lstm_x_tuning_+3A_custom_loss">custom_loss</code></td>
<td>
<p>Function, custom loss function for the LSTM model.</p>
</td></tr>
<tr><td><code id="ts_lstm_x_tuning_+3A_early_stopping">early_stopping</code></td>
<td>
<p>keras early stopping callback.</p>
</td></tr>
<tr><td><code id="ts_lstm_x_tuning_+3A_n_lag">n_lag</code></td>
<td>
<p>Integer, desired lag value.</p>
</td></tr>
<tr><td><code id="ts_lstm_x_tuning_+3A_lstm_units_list">lstm_units_list</code></td>
<td>
<p>Numeric vector, list of LSTM units to search over.</p>
</td></tr>
<tr><td><code id="ts_lstm_x_tuning_+3A_learning_rate_list">learning_rate_list</code></td>
<td>
<p>Numeric vector, list of learning rates to search over.</p>
</td></tr>
<tr><td><code id="ts_lstm_x_tuning_+3A_batch_size_list">batch_size_list</code></td>
<td>
<p>Numeric vector, list of batch sizes to search over.</p>
</td></tr>
<tr><td><code id="ts_lstm_x_tuning_+3A_dropout_list">dropout_list</code></td>
<td>
<p>Numeric vector, list of dropout rates to search over.</p>
</td></tr>
<tr><td><code id="ts_lstm_x_tuning_+3A_l1_reg_list">l1_reg_list</code></td>
<td>
<p>Numeric vector, list of L1 regularization values to search over.</p>
</td></tr>
<tr><td><code id="ts_lstm_x_tuning_+3A_l2_reg_list">l2_reg_list</code></td>
<td>
<p>Numeric vector, list of L2 regularization values to search over.</p>
</td></tr>
<tr><td><code id="ts_lstm_x_tuning_+3A_n_iter">n_iter</code></td>
<td>
<p>Integer, number of epochs for each model training.</p>
</td></tr>
<tr><td><code id="ts_lstm_x_tuning_+3A_n_verbose">n_verbose</code></td>
<td>
<p>Integer, level of verbosity during training (0 or 1).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the results data frame, all histories, and LSTM models.
</p>


<h3>References</h3>

<p>Garai, S., &amp; Paul, R. K. (2023). Development of MCS based-ensemble models using CEEMDAN decomposition and machine intelligence. Intelligent Systems with Applications, 18, 200202.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data &lt;- data.frame(
  Date = as.Date(c("01-04-18", "02-04-18", "03-04-18", "04-04-18", "05-04-18",
                   "06-04-18", "07-04-18", "08-04-18", "09-04-18", "10-04-18",
                   "11-04-18", "12-04-18", "13-04-18", "14-04-18", "15-04-18",
                   "16-04-18", "17-04-18", "18-04-18", "19-04-18", "20-04-18"),
                 format = "%d-%m-%y"),
  A = c(0, 0, 4, 12, 20, 16, 16, 0, 12, 18, 12, 18, 18, 0, 0, 33, 31, 38, 76, 198)
)
check_and_format_data(data)
# Add a new column 'X' based on the values in the second column
data$X &lt;- ifelse(data$A != 0, 1, 0)

result_embed &lt;- embed_columns(data = data, n_lag = 2)
new_data &lt;- result_embed$data_frame
embedded_colnames &lt;- result_embed$column_names

result_split &lt;- split_data(new_data = new_data, val_ratio = 0.1)
train_data &lt;- result_split$train_data
validation_data &lt;- result_split$validation_data
train_data &lt;- result_split$train_data
validation_data &lt;- result_split$validation_data
embedded_colnames &lt;- result_embed$column_names
numeric_matrices &lt;- convert_to_numeric_matrices(train_data = train_data,
                                                validation_data = validation_data,
                                                embedded_colnames = embedded_colnames)
X_train &lt;- numeric_matrices$X_train
y_train &lt;- numeric_matrices$y_train
X_val &lt;- numeric_matrices$X_val
y_val &lt;- numeric_matrices$y_val

#' initialize_tensorflow()

X_train &lt;- numeric_matrices$X_train
X_val &lt;- numeric_matrices$X_val
reshaped_data &lt;- reshape_for_lstm(X_train = X_train, X_val = X_val)
X_train &lt;- reshaped_data$X_train
X_val &lt;- reshaped_data$X_val
X_train &lt;- reshaped_data$X_train
y_train &lt;- numeric_matrices$y_train
X_val &lt;- reshaped_data$X_val
y_val &lt;- numeric_matrices$y_val
tf &lt;- reticulate::import("tensorflow")
tensors &lt;- convert_to_tensors(X_train = X_train, y_train = y_train, X_val = X_val, y_val = y_val)
X_train &lt;- tensors$X_train
y_train &lt;- tensors$y_train
X_val &lt;- tensors$X_val
y_val &lt;- tensors$y_val
n_patience &lt;- 50
early_stopping &lt;- define_early_stopping(n_patience = n_patience)

X_train &lt;- tensors$X_train
X_val &lt;- tensors$X_val

y_train &lt;- tensors$y_train
y_val &lt;- tensors$y_val

embedded_colnames &lt;- result_embed$column_names

# Define your custom loss function
custom_loss &lt;- function(y_true, y_pred) {
  condition &lt;- tf$math$equal(y_true, 0)
  loss &lt;- tf$math$reduce_mean(tf$math$square(y_true - y_pred))  # Remove 'axis'
  loss &lt;- tf$where(condition, tf$constant(0), loss)
  return(loss)
}

early_stopping &lt;- define_early_stopping(n_patience = n_patience)

grid_search_results &lt;- ts_lstm_x_tuning(
  X_train, y_train, X_val, y_val,
  embedded_colnames, custom_loss, early_stopping,
  n_lag = 2, # desired lag value
  lstm_units_list = c(32),
  learning_rate_list = c(0.001, 0.01),
  batch_size_list = c(32),
  dropout_list = c(0.2),
  l1_reg_list = c(0.001),
  l2_reg_list = c(0.001),
  n_iter = 10,
  n_verbose = 0 # or 1
)

results_df &lt;- grid_search_results$results_df
all_histories &lt;- grid_search_results$all_histories
lstm_models &lt;- grid_search_results$lstm_models

# Find the row with the minimum val_loss_mae in results_df
min_val_loss_row &lt;- results_df[which.min(results_df$val_loss_mae), ]

# Extract hyperparameters from the row
best_lstm_units &lt;- min_val_loss_row$lstm_units
best_learning_rate &lt;- min_val_loss_row$learning_rate
best_batch_size &lt;- min_val_loss_row$batch_size
best_n_lag &lt;- min_val_loss_row$n_lag
best_dropout &lt;- min_val_loss_row$dropout
best_l1_reg &lt;- min_val_loss_row$l1_reg
best_l2_reg &lt;- min_val_loss_row$l2_reg

# Generate the lstm_model_name for the best model
best_model_name &lt;- paste0("lstm_model_lu_", best_lstm_units, "_lr_", best_learning_rate,
                          "_bs_", best_batch_size, "_lag_", best_n_lag,
                          "_do_", best_dropout, "_l1_", best_l1_reg, "_l2_", best_l2_reg)

# Generate the history_name for the best model
best_history_name &lt;- paste0("history_lu_", best_lstm_units, "_lr_", best_learning_rate,
                            "_bs_", best_batch_size, "_lag_", best_n_lag,
                            "_do_", best_dropout, "_l1_", best_l1_reg, "_l2_", best_l2_reg)

# Access the best model from lstm_models
best_model &lt;- lstm_models[[best_model_name]]

best_model_details &lt;- data.frame(min_val_loss_row)

colnames(best_model_details) &lt;- colnames(results_df)

# Access the best model from lstm_models
best_history &lt;- all_histories[[best_history_name]]


</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
