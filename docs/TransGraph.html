<!DOCTYPE html><html lang="en"><head><title>Help for package TransGraph</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {TransGraph}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#Evaluation.DAG'><p>Evaluation function for the estimated DAG.</p></a></li>
<li><a href='#Evaluation.GGM'><p>Evaluation function for the estimated GGM.</p></a></li>
<li><a href='#layer_adj'><p>The function of converting the adjacency matrix into the topological layer.</p></a></li>
<li><a href='#tensor.GGM.trans'><p>Transfer learning for tensor graphical models.</p></a></li>
<li><a href='#Theta.est'><p>Sparse precision matrix estimation.</p></a></li>
<li><a href='#Theta.tuning'><p>Sparse precision matrix estimation with tuning parameters.</p></a></li>
<li><a href='#TLLiNGAM'><p>Learning linear non-Gaussian DAG via topological layers.</p></a></li>
<li><a href='#trans_GGMM'><p>Transfer learning of high-dimensional Gaussian graphical mixture models.</p></a></li>
<li><a href='#trans_mean'><p>Transfer learning for mean estimation.</p></a></li>
<li><a href='#trans_precision'><p>Transfer learning for vector-valued precision matrix (graphical model).</p></a></li>
<li><a href='#trans.local.DAG'><p>Structural transfer learning of non-Gaussian DAG.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Transfer Graph Learning</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.1</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Mingyang Ren &lt;renmingyang17@mails.ucas.ac.cn&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Transfer learning, aiming to use auxiliary domains to help improve learning of the target domain of interest when multiple heterogeneous datasets are available, has always been a hot topic in statistical machine learning. The recent transfer learning methods with statistical guarantees mainly focus on the overall parameter transfer for supervised models in the ideal case with the informative auxiliary domains with overall similarity. In contrast, transfer learning for unsupervised graph learning is in its infancy and largely follows the idea of overall parameter transfer as for supervised learning. 
             In this package, the transfer learning for several complex graphical models is implemented, including Tensor Gaussian graphical models, non-Gaussian directed acyclic graph (DAG), and Gaussian graphical mixture models. Notably, this package promotes local transfer at node-level and subgroup-level in DAG structural learning and Gaussian graphical mixture models, respectively, which are more flexible and robust than the existing overall parameter transfer. As by-products, transfer learning for undirected graphical model (precision matrix) via D-trace loss, transfer learning for mean vector estimation, and single non-Gaussian learning via topological layer method are also included in this package. 
             Moreover, the aggregation of auxiliary information is an important issue in transfer learning, and this package provides multiple user-friendly aggregation methods, including sample weighting, similarity weighting, and most informative selection.    
             Reference: 
             Ren, M., Zhen Y., and Wang J. (2022) &lt;<a href="https://doi.org/10.48550/arXiv.2211.09391">doi:10.48550/arXiv.2211.09391</a>&gt; "Transfer learning for tensor graphical models".    
             Ren, M., He X., and Wang J. (2023) &lt;<a href="https://doi.org/10.48550/arXiv.2310.10239">doi:10.48550/arXiv.2310.10239</a>&gt; "Structural transfer learning of non-Gaussian DAG".    
             Zhao, R., He X., and Wang J. (2022) <a href="https://jmlr.org/papers/v23/21-1173.html">https://jmlr.org/papers/v23/21-1173.html</a> "Learning linear non-Gaussian directed acyclic graph with diverging number of nodes".</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>MASS, rTensor, Tlasso, glasso, clime, doParallel, expm,
HeteroGGM, dcov, huge, EvaluationMeasures</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-10-19 09:57:48 UTC; 10259</td>
</tr>
<tr>
<td>Author:</td>
<td>Mingyang Ren <a href="https://orcid.org/0000-0002-8061-9940"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Ruixuan Zhao [aut],
  Xin He [aut],
  Junhui Wang [aut]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-10-19 10:40:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='Evaluation.DAG'>Evaluation function for the estimated DAG.</h2><span id='topic+Evaluation.DAG'></span>

<h3>Description</h3>

<p>Evaluation function for the estimated DAG.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Evaluation.DAG(estimated.adjace, true.adjace, type.adj=2)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Evaluation.DAG_+3A_estimated.adjace">estimated.adjace</code></td>
<td>
<p>The target data, a n * p matrix, where n is the sample size and p is data dimension.</p>
</td></tr>
<tr><td><code id="Evaluation.DAG_+3A_true.adjace">true.adjace</code></td>
<td>
<p>The auxiliary data in K auxiliary domains, a list with K elements, each of which is a nk * p matrix, where nk is the sample size of the k-th auxiliary domain.</p>
</td></tr>
<tr><td><code id="Evaluation.DAG_+3A_type.adj">type.adj</code></td>
<td>
<p>The type of adjacency matrix. 1: the entries of matrix contains just two value, 0 and 1, which indicate the existence of edges; 2 (default): the matrix also measures connection strength, and 0 means no edge.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A result list including Recall, FDR, F1score, MCC, Hamming Distance,and estimated error of adjacency matrix on F-norm.
</p>


<h3>Author(s)</h3>

<p>Ruixaun Zhao <a href="mailto:ruixuanzhao2-c@my.cityu.edu.hk">ruixuanzhao2-c@my.cityu.edu.hk</a>.
</p>


<h3>References</h3>

<p>Zhao, R., He X., and Wang J. (2022). Learning linear non-Gaussian directed acyclic graph with diverging number of nodes. Journal of Machine Learning Research.
</p>

<hr>
<h2 id='Evaluation.GGM'>Evaluation function for the estimated GGM.</h2><span id='topic+Evaluation.GGM'></span>

<h3>Description</h3>

<p>Evaluation function for the estimated GGM.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Evaluation.GGM(est.precision, true.precision)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Evaluation.GGM_+3A_est.precision">est.precision</code></td>
<td>
<p>The estimated precision matrix.</p>
</td></tr>
<tr><td><code id="Evaluation.GGM_+3A_true.precision">true.precision</code></td>
<td>
<p>The true precision matrix.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A result list including Recall, FDR, F1score, MCC, Hamming Distance,and estimated error of adjacency matrix on F-norm.
</p>


<h3>Author(s)</h3>

<p>Mingyang Ren <a href="mailto:renmingyang17@mails.ucas.ac.cn">renmingyang17@mails.ucas.ac.cn</a>.
</p>

<hr>
<h2 id='layer_adj'>The function of converting the adjacency matrix into the topological layer.</h2><span id='topic+layer_adj'></span>

<h3>Description</h3>

<p>The function of converting the adjacency matrix into the topological layer.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layer_adj(true_adjace)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="layer_adj_+3A_true_adjace">true_adjace</code></td>
<td>
<p>a p * p adjacency matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Layer_true: a p * 2 matrix to store the information of layer. The first column is the node label, and the second column is the corresponding layer labels.
</p>


<h3>Author(s)</h3>

<p>Mingyang Ren <a href="mailto:renmingyang17@mails.ucas.ac.cn">renmingyang17@mails.ucas.ac.cn</a>.
</p>


<h3>References</h3>

<p>Zhao, R., He X., and Wang J. (2022). Learning linear non-Gaussian directed acyclic graph with diverging number of nodes. Journal of Machine Learning Research.
</p>

<hr>
<h2 id='tensor.GGM.trans'>Transfer learning for tensor graphical models.</h2><span id='topic+tensor.GGM.trans'></span>

<h3>Description</h3>

<p>The main function for Transfer learning for tensor graphical models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tensor.GGM.trans(t.data, A.data, A.lambda, A.orac = NULL, c=0.6,
                        t.lambda.int.trans=NULL, t.lambda.int.aggr=NULL,
                        theta.algm="cd", cov.select="inverse",
                        cov.select.agg.size = "inverse",
                        cov.select.agg.diff = "tensor.prod",
                        symmetric = TRUE, init.method="Tlasso",
                        init.method.aux="Tlasso", mode.set = NULL,
                        init.iter.Tlasso=2, cn.lam2=seq(0.1,2,length.out =10),
                        c.lam.Tlasso=20, c.lam.sepa=20, adjust.BIC=FALSE,
                        normalize = TRUE, inti.the=TRUE, sel.ind="fit")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tensor.GGM.trans_+3A_t.data">t.data</code></td>
<td>
<p>The tensor data in the target domain, a p1 * p2 * ... * pM * n array, where n is the sample size and pm is dimension of the m-th tensor mode. M should be larger than 2.</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_a.data">A.data</code></td>
<td>
<p>The tensor data in auxiliary domains, a list with K elements, each of which is a p1 * p2 * ... * pM * nk array, where nk is the sample size of the k-th auxiliary domain.</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_a.lambda">A.lambda</code></td>
<td>
<p>The tuning parameters used for initialization in auxiliary domains, a list with K elements, each of which is a M-dimensional vector corresponding to M modes.</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_a.orac">A.orac</code></td>
<td>
<p>The set of informative auxiliary domains, and the default setting is NULL, which means that no set is specified.</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_c">c</code></td>
<td>
<p>The c of subjects in the target domain are used for initialization of the transfer learning, and the remaining 1-c of subjects are used for the model selection step. The default setting is 0.8.</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_t.lambda.int.trans">t.lambda.int.trans</code></td>
<td>
<p>The tuning parameters used for initialization in the target domain (based on c subjects used for transfer learning), that is, the tuning lambda for Tlasso (PAMI, 2020) &amp; Separable method (JCGS, 2022)</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_t.lambda.int.aggr">t.lambda.int.aggr</code></td>
<td>
<p>The tuning parameters used for initialization in the target domain (based on 1-c subjects used for the model selection step).</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_theta.algm">theta.algm</code></td>
<td>
<p>The optimization algorithm used to solve <code class="reqn">\widehat{\Omega}</code> in step 2(b), which can be selected as &quot;admm&quot; (ADMM algorithm) or &quot;cd&quot; (coordinate descent).</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_cov.select">cov.select</code></td>
<td>
<p>Methods used to calculate covariance matrices for initialization in both target and auxiliary domains, which can be selected as &quot;tensor.prod&quot; (tensor product based on tensor subject and the initial estimate of the precision matrix, TPAMI, 2020) and &quot;inverse&quot; (direct inversion of the initial estimate of the precision matrix)</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_cov.select.agg.size">cov.select.agg.size</code></td>
<td>
<p>Methods used to calculate covariance matrices for model selection step in the target domain.</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_cov.select.agg.diff">cov.select.agg.diff</code></td>
<td>
<p>Methods used to calculate covariance matrices for model selection step in the target domain.</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_symmetric">symmetric</code></td>
<td>
<p>Whether to symmetrize the final estimated precision matrices, and the default is True.</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_init.method">init.method</code></td>
<td>
<p>The initialization method for tensor precision matrices in the target domain, which can be selected as &quot;Tlasso&quot; (PAMI, 2020) &amp; &quot;sepa&quot; (Separable method, JCGS, 2022). Note that the &quot;sepa&quot; method has not been included in the current version of this R package to circumvent code ownership issues.</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_init.method.aux">init.method.aux</code></td>
<td>
<p>The initialization method for tensor precision matrices in auxiliary domains.</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_mode.set">mode.set</code></td>
<td>
<p>Whether to estimate only the specified mode, and the default setting is NULL, which means estimating all mode.</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_init.iter.tlasso">init.iter.Tlasso</code></td>
<td>
<p>The number of maximal iteration when using Tlasso for initialization, default is 2.</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_cn.lam2">cn.lam2</code></td>
<td>
<p>The coefficient set in tuning parameters used to solve <code class="reqn">\widehat{\Omega}</code> in step 2(b), default is seq(0.1,1,length.out =10).</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_c.lam.tlasso">c.lam.Tlasso</code></td>
<td>
<p>The coefficient in tuning parameters for initialization (when using Tlasso): <code class="reqn">c.lam.Tlasso * \sqrt( pm * \log(pm)/( n*p1*...*pM ))</code>, default is 20 suggested in (PAMI, 2020).</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_c.lam.sepa">c.lam.sepa</code></td>
<td>
<p>The coefficient in tuning parameters for initialization (when using sepa): <code class="reqn">c.lam.sepa * \sqrt( pm * \log(pm)/( n*p1*...*pM ))</code>.</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_adjust.bic">adjust.BIC</code></td>
<td>
<p>Whether to use the adjusted BIC to select lambda2, the default setting is F.</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_normalize">normalize</code></td>
<td>
<p>The normalization method of precision matrix. When using Tlasso, <code class="reqn">\Omega_{11} = 1</code> if normalize = F and <code class="reqn">\| \Omega_{11} \|_{F} = 1</code> if normalize = T. Default value is T.</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_inti.the">inti.the</code></td>
<td>
<p>T: the initial values in Step 2(b) is Omega0.</p>
</td></tr>
<tr><td><code id="tensor.GGM.trans_+3A_sel.ind">sel.ind</code></td>
<td>
<p>The approach to model selection, which can be selected from c(&quot;fit&quot;, &quot;predict&quot;).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A result list including:
</p>

<dl>
<dt>Omega.list</dt><dd><p>The final estimation result of the target precision matrices after the model selection of transfer learning-based estimation and initial estimation (in which the initial covariance matrices of auxiliary domains is weighted by sample sizes).</p>
</dd>
<dt>Omega.sym.list</dt><dd><p>The symmetrized final estimation result in Omega.list.</p>
</dd>
<dt>Omega.list.diff</dt><dd><p>The final estimation result of the target precision matrices after the model selection of transfer learning-based estimation and initial estimation (in which the initial covariance matrices of auxiliary domains is weighted by the differences with the target domain).</p>
</dd>
<dt>Omega.sym.list.diff</dt><dd><p>The symmetrized final estimation result in Omega.list.diff.</p>
</dd>
<dt>res.trans.list</dt><dd><p>Transfer learning-based estimation results.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Mingyang Ren <a href="mailto:renmingyang17@mails.ucas.ac.cn">renmingyang17@mails.ucas.ac.cn</a>, Yaoming Zhen, and Junhui Wang
</p>


<h3>References</h3>

<p>Ren, M., Zhen Y., and Wang J. (2022). Transfer learning for tensor graphical models.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(TransGraph)
library(Tlasso)
# load example data from github repository
# Please refer to https://github.com/Ren-Mingyang/example_data_TransGraph
# for detailed data information
githublink = "https://github.com/Ren-Mingyang/example_data_TransGraph/"
load(url(paste0(githublink,"raw/main/example.data.tensorGGM.RData")))
t.data = example.data$t.data
A.data = example.data$A.data
t.Omega.true.list = example.data$t.Omega.true.list
normalize = TRUE

K = length(A.data)
p.vec = dim(t.data)
M = length(p.vec) - 1
n = p.vec[M+1]
p.vec = p.vec[1:M]
tla.lambda = 20*sqrt( p.vec*log(p.vec) / ( n * prod(p.vec) ))
A.lambda = list()
for (k in 1:K) {
  A.lambda[[k]] = 20*sqrt( log(p.vec) / ( dim(A.data[[k]])[M+1] * prod(p.vec) ))
}

res.final = tensor.GGM.trans(t.data, A.data, A.lambda, normalize = normalize)
Tlasso.Omega.list = Tlasso.fit(t.data, lambda.vec = tla.lambda,
                    norm.type = 1+as.numeric(normalize))

i.Omega = as.data.frame(t(unlist(est.analysis(res.final$Omega.list, t.Omega.true.list))))
i.Omega.diff = t(unlist(est.analysis(res.final$Omega.list.diff, t.Omega.true.list)))
i.Omega.diff = as.data.frame(i.Omega.diff)
i.Tlasso = as.data.frame(t(unlist(est.analysis(Tlasso.Omega.list, t.Omega.true.list))))
i.Omega.diff     # proposed.v
i.Omega          # proposed
i.Tlasso         # Tlasso





</code></pre>

<hr>
<h2 id='Theta.est'>Sparse precision matrix estimation.</h2><span id='topic+Theta.est'></span>

<h3>Description</h3>

<p>The fast sparse precision matrix estimation in step 2(b).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Theta.est(S.hat.A, delta.hat, lam2=0.1, Omega.hat0=NULL,
                 n=100, max_iter=10, eps=1e-3, method = "cd")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Theta.est_+3A_s.hat.a">S.hat.A</code></td>
<td>
<p>The sample covariance matrix.</p>
</td></tr>
<tr><td><code id="Theta.est_+3A_delta.hat">delta.hat</code></td>
<td>
<p>The divergence matrix estimated in step 2(a). If the precision matrix is estimated in the common case (Liu and Luo, 2015, JMVA), it can be set to zero matrix.</p>
</td></tr>
<tr><td><code id="Theta.est_+3A_lam2">lam2</code></td>
<td>
<p>A float value, a tuning parameter.</p>
</td></tr>
<tr><td><code id="Theta.est_+3A_omega.hat0">Omega.hat0</code></td>
<td>
<p>The initial values of the precision matrix, which can be unspecified.</p>
</td></tr>
<tr><td><code id="Theta.est_+3A_n">n</code></td>
<td>
<p>The sample size.</p>
</td></tr>
<tr><td><code id="Theta.est_+3A_max_iter">max_iter</code></td>
<td>
<p>Int, maximum number of cycles of the algorithm.</p>
</td></tr>
<tr><td><code id="Theta.est_+3A_eps">eps</code></td>
<td>
<p>A float value, algorithm termination threshold.</p>
</td></tr>
<tr><td><code id="Theta.est_+3A_method">method</code></td>
<td>
<p>The optimization algorithm, which can be selected as &quot;admm&quot; (ADMM algorithm) or &quot;cd&quot; (coordinate descent).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A result list including:
</p>

<dl>
<dt>Theta.hat.m</dt><dd><p>The optimal precision matrix.</p>
</dd>
<dt>BIC.summary</dt><dd><p>The summary of BICs.</p>
</dd>
<dt>Theta.hat.list.m</dt><dd><p>The precision matrices corresponding to a sequence of tuning parameters.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Mingyang Ren <a href="mailto:renmingyang17@mails.ucas.ac.cn">renmingyang17@mails.ucas.ac.cn</a>.
</p>


<h3>References</h3>

<p>Ren, M., Zhen Y., and Wang J. (2022). Transfer learning for tensor graphical models.
Liu, W. and Luo X. (2015). Fast and adaptive sparse precision matrix estimation in high dimensions, Journal of Multivariate Analysis.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>p = 20
n = 200
omega = diag(rep(1,p))
for (i in 1:p) {
  for (j in 1:p) {
    omega[i,j] = 0.3^(abs(i-j))*(abs(i-j) &lt; 2)
  }
}
Sigma = solve(omega)
X = MASS::mvrnorm(n, rep(0,p), Sigma)
S.hat.A = cov(X)
delta.hat = diag(rep(1,p)) - diag(rep(1,p))
omega.hat = Theta.est(S.hat.A, delta.hat, lam2=0.2)



</code></pre>

<hr>
<h2 id='Theta.tuning'>Sparse precision matrix estimation with tuning parameters.</h2><span id='topic+Theta.tuning'></span>

<h3>Description</h3>

<p>The fast sparse precision matrix estimation in step 2(b).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Theta.tuning(lambda2, S.hat.A, delta.hat, Omega.hat0, n.A,
                    theta.algm="cd", adjust.BIC=FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Theta.tuning_+3A_lambda2">lambda2</code></td>
<td>
<p>A vector, a sequence of tuning parameters.</p>
</td></tr>
<tr><td><code id="Theta.tuning_+3A_s.hat.a">S.hat.A</code></td>
<td>
<p>The sample covariance matrix.</p>
</td></tr>
<tr><td><code id="Theta.tuning_+3A_delta.hat">delta.hat</code></td>
<td>
<p>The divergence matrix estimated in step 2(a). If the precision matrix is estimated in the common case (Liu and Luo, 2015, JMVA), it can be set to zero matrix.</p>
</td></tr>
<tr><td><code id="Theta.tuning_+3A_omega.hat0">Omega.hat0</code></td>
<td>
<p>The initial values of the precision matrix.</p>
</td></tr>
<tr><td><code id="Theta.tuning_+3A_n.a">n.A</code></td>
<td>
<p>The sample size.</p>
</td></tr>
<tr><td><code id="Theta.tuning_+3A_theta.algm">theta.algm</code></td>
<td>
<p>The optimization algorithm used to solve <code class="reqn">\widehat{\Omega}</code> in step 2(b), which can be selected as &quot;admm&quot; (ADMM algorithm) or &quot;cd&quot; (coordinate descent).</p>
</td></tr>
<tr><td><code id="Theta.tuning_+3A_adjust.bic">adjust.BIC</code></td>
<td>
<p>Whether to use the adjusted BIC to select lambda2, the default setting is F.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A result list including:
</p>

<dl>
<dt>Theta.hat.m</dt><dd><p>The optimal precision matrix.</p>
</dd>
<dt>BIC.summary</dt><dd><p>The summary of BICs.</p>
</dd>
<dt>Theta.hat.list.m</dt><dd><p>The precision matrices corresponding to a sequence of tuning parameters.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Mingyang Ren <a href="mailto:renmingyang17@mails.ucas.ac.cn">renmingyang17@mails.ucas.ac.cn</a>.
</p>


<h3>References</h3>

<p>Ren, M., Zhen Y., and Wang J. (2022). Transfer learning for tensor graphical models.
Liu, W. and Luo X. (2015). Fast and adaptive sparse precision matrix estimation in high dimensions, Journal of Multivariate Analysis.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>p = 20
n = 200
omega = diag(rep(1,p))
for (i in 1:p) {
  for (j in 1:p) {
    omega[i,j] = 0.3^(abs(i-j))*(abs(i-j) &lt; 2)
  }
}
Sigma = solve(omega)
X = MASS::mvrnorm(n, rep(0,p), Sigma)
S.hat.A = cov(X)
delta.hat = diag(rep(1,p)) - diag(rep(1,p))
lambda2 = seq(0.1,0.5,length.out =10)
res = Theta.tuning(lambda2, S.hat.A, delta.hat, n.A=n)
omega.hat = res$Theta.hat.m



</code></pre>

<hr>
<h2 id='TLLiNGAM'>Learning linear non-Gaussian DAG via topological layers.</h2><span id='topic+TLLiNGAM'></span>

<h3>Description</h3>

<p>Learning linear non-Gaussian DAG via topological layers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TLLiNGAM (X, hardth=0.3, criti.val=0.01, precision.refit = TRUE,
                 precision.method="glasso", B.refit=TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="TLLiNGAM_+3A_x">X</code></td>
<td>
<p>The n * p sample matrix, where n is the sample size and p is data dimension.</p>
</td></tr>
<tr><td><code id="TLLiNGAM_+3A_hardth">hardth</code></td>
<td>
<p>The hard threshold of regression.</p>
</td></tr>
<tr><td><code id="TLLiNGAM_+3A_criti.val">criti.val</code></td>
<td>
<p>The critical value of independence test based on distance covariance.</p>
</td></tr>
<tr><td><code id="TLLiNGAM_+3A_precision.refit">precision.refit</code></td>
<td>
<p>Whether to perform regression for re-fitting the coefficients in the precision matrix to improve estimation accuracy, after determining the non-zero elements of the precision matrix. The default is True.</p>
</td></tr>
<tr><td><code id="TLLiNGAM_+3A_precision.method">precision.method</code></td>
<td>
<p>Methods for Estimating Precision Matrix, which can be selected from &quot;glasso&quot; and &quot;CLIME&quot;.</p>
</td></tr>
<tr><td><code id="TLLiNGAM_+3A_b.refit">B.refit</code></td>
<td>
<p>Whether to perform regression for re-fitting the coefficients in structural equation models to improve estimation accuracy, after determining the parent sets of all nodes. The default is True.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A result list including:
</p>

<dl>
<dt>A</dt><dd><p>The information of layer.</p>
</dd>
<dt>B</dt><dd><p>The coefficients in structural equation models.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Ruixuan Zhao <a href="mailto:ruixuanzhao2-c@my.cityu.edu.hk">ruixuanzhao2-c@my.cityu.edu.hk</a>, Xin He, and Junhui Wang
</p>


<h3>References</h3>

<p>Zhao, R., He X., and Wang J. (2022). Learning linear non-Gaussian directed acyclic graph with diverging number of nodes. Journal of Machine Learning Research.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(TransGraph)
# load example data from github repository
# Please refer to https://github.com/Ren-Mingyang/example_data_TransGraph
# for detailed data information
githublink = "https://github.com/Ren-Mingyang/example_data_TransGraph/"
load(url(paste0(githublink,"raw/main/example.data.singleDAG.RData")))
true_adjace = example.data.singleDAG$true_adjace
t.data = example.data.singleDAG$X
res.single = TLLiNGAM(t.data)
Evaluation.DAG(res.single$B, true_adjace)$Eval_result


</code></pre>

<hr>
<h2 id='trans_GGMM'>Transfer learning of high-dimensional Gaussian graphical mixture models.</h2><span id='topic+trans_GGMM'></span>

<h3>Description</h3>

<p>Transfer learning of high-dimensional Gaussian graphical mixture models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trans_GGMM(t.data, lambda.t, M, A.data, lambda.A.list, M.A.vec,
                  pseudo.cov="soft", cov.method="opt", cn.lam2=0.5, clambda.m=1,
                  theta.algm="cd", initial.selection="K-means", preselect.aux=0,
                  sel.type="L2", trace=FALSE )
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="trans_GGMM_+3A_t.data">t.data</code></td>
<td>
<p>The target data, a n * p matrix, where n is the sample size and p is data dimension.</p>
</td></tr>
<tr><td><code id="trans_GGMM_+3A_lambda.t">lambda.t</code></td>
<td>
<p>A list, the sequences of the tuning parameters (lambda1, lambda2, and lambda3) used in the initialization of the target domain.</p>
</td></tr>
<tr><td><code id="trans_GGMM_+3A_m">M</code></td>
<td>
<p>Int, a selected upper bound of the true numbers of subgroups in the target domain.</p>
</td></tr>
<tr><td><code id="trans_GGMM_+3A_a.data">A.data</code></td>
<td>
<p>The auxiliary data in K auxiliary domains, a list with K elements, each of which is a nk * p matrix, where nk is the sample size of the k-th auxiliary domain.</p>
</td></tr>
<tr><td><code id="trans_GGMM_+3A_lambda.a.list">lambda.A.list</code></td>
<td>
<p>A list consisting of K lists, the k-th list is the sequences of the tuning parameters (lambda1, lambda2, and lambda3) used in the initialization of the k-th auxiliary domain.</p>
</td></tr>
<tr><td><code id="trans_GGMM_+3A_m.a.vec">M.A.vec</code></td>
<td>
<p>A vector composed of K integers, the k-th element is a selected upper bound of the true numbers of subgroups in the k-th auxiliary domain.</p>
</td></tr>
<tr><td><code id="trans_GGMM_+3A_pseudo.cov">pseudo.cov</code></td>
<td>
<p>The method for calculating pseudo covariance matricex in auxiliary domains, which can be selected from &quot;soft&quot;(default, subgroups based on samples of soft clustering via posterior probability ) and &quot;hard&quot; (subgroups based on samples of hard clustering).</p>
</td></tr>
<tr><td><code id="trans_GGMM_+3A_cov.method">cov.method</code></td>
<td>
<p>The method of aggregating K auxiliary covariance matrices, which can be selected as &quot;size&quot; (the sum weighted by the sample sizes), &quot;weight&quot; (the sum weighted by the differences) or &quot;opt&quot; (select the optimal one).</p>
</td></tr>
<tr><td><code id="trans_GGMM_+3A_cn.lam2">cn.lam2</code></td>
<td>
<p>A vector or a float value: the coefficients set in tuning parameters used to solve the target precision matrix, default is cn.lam2*sqrt( log(p) / n ).</p>
</td></tr>
<tr><td><code id="trans_GGMM_+3A_clambda.m">clambda.m</code></td>
<td>
<p>The coefficients set in tuning parameters used in transfer learning for mean eatimation, and the default setting is clambda.m * sqrt( log(p) / n ).</p>
</td></tr>
<tr><td><code id="trans_GGMM_+3A_theta.algm">theta.algm</code></td>
<td>
<p>The optimization algorithm used to solve the precision, which can be selected as &quot;admm&quot; (ADMM algorithm) or &quot;cd&quot; (coordinate descent).</p>
</td></tr>
<tr><td><code id="trans_GGMM_+3A_initial.selection">initial.selection</code></td>
<td>
<p>The different initial values from two clustering methods, which can be selected from c(&quot;K-means&quot;,&quot;dbscan&quot;).</p>
</td></tr>
<tr><td><code id="trans_GGMM_+3A_preselect.aux">preselect.aux</code></td>
<td>
<p>Whether to pre-select informative auxiliary domains based on the distance between initially estimated auxiliary and target parameters. The default is 0, which means that pre-selection will not be performed. If &quot;preselect.aux&quot; is specified as a real number greater than zero, then the threshold value is forpreselect.aux<em>s</em>sqrt( log(p) / n ).</p>
</td></tr>
<tr><td><code id="trans_GGMM_+3A_sel.type">sel.type</code></td>
<td>
<p>If pre-selection should be performed, &quot;sel.type&quot; is the type of distance. The default is L2 norm, and can be specified as &quot;L1&quot; to use L1 norm.</p>
</td></tr>
<tr><td><code id="trans_GGMM_+3A_trace">trace</code></td>
<td>
<p>The logical variable, whether or not to output the number of identified subgroups during the search for parameters in the initialization.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A result list including:
</p>

<dl>
<dt>res.target</dt><dd><p>A list including transfer learning results of the target domain.</p>
</dd>
<dt>res.target$opt_Mu_hat</dt><dd><p>The final estimation of means in all detected subgroups via transfer learning.</p>
</dd>
<dt>res.target$opt_Theta_hat</dt><dd><p>The final estimation of precision matrices in all detected subgroups via transfer learning.</p>
</dd>
<dt>res.target0</dt><dd><p>A list including initial results of the target domain.</p>
</dd>
<dt>res.target0$opt_Mu_hat</dt><dd><p>The initial estimation of means in all detected subgroups.</p>
</dd>
<dt>res.target0$opt_Theta_hat</dt><dd><p> The initial estimation of precision matrices in all detected subgroups.</p>
</dd>
<dt>t.res</dt><dd><p>A list including results of the transfer precision matrix for each subgroup.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Mingyang Ren <a href="mailto:renmingyang17@mails.ucas.ac.cn">renmingyang17@mails.ucas.ac.cn</a>.
</p>


<h3>References</h3>

<p>Ren, M. and Wang J. (2023). Local transfer learning of Gaussian graphical mixture models.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>"Will be supplemented in the next version."


</code></pre>

<hr>
<h2 id='trans_mean'>Transfer learning for mean estimation.</h2><span id='topic+trans_mean'></span>

<h3>Description</h3>

<p>Transfer learning for mean estimation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trans_mean(t.mean.m, A.mean, n, clambda=1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="trans_mean_+3A_t.mean.m">t.mean.m</code></td>
<td>
<p>The estimated target p-dimensional mean vector, where p is mean dimension.</p>
</td></tr>
<tr><td><code id="trans_mean_+3A_a.mean">A.mean</code></td>
<td>
<p>A K*p matrix with the k-th row being the estimated p-dimensional mean vector of the k-th auxiliary domain.</p>
</td></tr>
<tr><td><code id="trans_mean_+3A_n">n</code></td>
<td>
<p>The target sample size.</p>
</td></tr>
<tr><td><code id="trans_mean_+3A_clambda">clambda</code></td>
<td>
<p>The coefficients set in tuning parameters used in transfer learning for mean eatimation, and the default setting is clambda.m * sqrt( log(p) / n ).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>t.mean.m.hat: The transfer learning estimation of the target p-dimensional mean vector.
</p>


<h3>Author(s)</h3>

<p>Mingyang Ren <a href="mailto:renmingyang17@mails.ucas.ac.cn">renmingyang17@mails.ucas.ac.cn</a>.
</p>


<h3>References</h3>

<p>Ren, M. and Wang J. (2023). Local transfer learning of Gaussian graphical mixture models.
</p>

<hr>
<h2 id='trans_precision'>Transfer learning for vector-valued precision matrix (graphical model).</h2><span id='topic+trans_precision'></span>

<h3>Description</h3>

<p>The transfer learning for vector-valued precision matrix via D-trace loss method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trans_precision(t.data=NULL, A.data=NULL, precision.method="CLIME",
                       cov.method="opt", cn.lam2=seq(1,2.5,length.out=10),
                       theta.algm="cd", adjust.BIC=FALSE, symmetry=TRUE,
                       preselect.aux=0, sel.type="L2", input.A.cov=FALSE,
                       A.cov=NULL, nA.vec=NULL, t.Theta.hat0=NULL,
                       t.n=NULL, correlation=FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="trans_precision_+3A_t.data">t.data</code></td>
<td>
<p>The target data, a n * p matrix, where n is the sample size and p is data dimension.</p>
</td></tr>
<tr><td><code id="trans_precision_+3A_a.data">A.data</code></td>
<td>
<p>The auxiliary data in K auxiliary domains, a list with K elements, each of which is a nk * p matrix, where nk is the sample size of the k-th auxiliary domain.</p>
</td></tr>
<tr><td><code id="trans_precision_+3A_precision.method">precision.method</code></td>
<td>
<p>The initial method of estimating the target precision matrix, which can be selected as &quot;CLIME&quot; or &quot;glasso&quot;.</p>
</td></tr>
<tr><td><code id="trans_precision_+3A_cov.method">cov.method</code></td>
<td>
<p>The method of aggregating K auxiliary covariance matrices, which can be selected as &quot;size&quot; (the sum weighted by the sample sizes), &quot;weight&quot; (the sum weighted by the differences) or &quot;opt&quot; (select the optimal one).</p>
</td></tr>
<tr><td><code id="trans_precision_+3A_cn.lam2">cn.lam2</code></td>
<td>
<p>A vector or a float value: the coefficients set in tuning parameters used to solve the target precision matrix, default is cn.lam2*sqrt( log(p) / n ).</p>
</td></tr>
<tr><td><code id="trans_precision_+3A_theta.algm">theta.algm</code></td>
<td>
<p>The optimization algorithm used to solve the precision, which can be selected as &quot;admm&quot; (ADMM algorithm) or &quot;cd&quot; (coordinate descent).</p>
</td></tr>
<tr><td><code id="trans_precision_+3A_adjust.bic">adjust.BIC</code></td>
<td>
<p>Whether to use the adjusted BIC to select lambda2, the default setting is FALSE.</p>
</td></tr>
<tr><td><code id="trans_precision_+3A_symmetry">symmetry</code></td>
<td>
<p>Whether to symmetrize the final estimated precision matrices, and the default is True.</p>
</td></tr>
<tr><td><code id="trans_precision_+3A_preselect.aux">preselect.aux</code></td>
<td>
<p>Whether to pre-select informative auxiliary domains based on the distance between initially estimated auxiliary and target parameters. The default is 0, which means that pre-selection will not be performed. If &quot;preselect.aux&quot; is specified as a real number greater than zero, then the threshold value is forpreselect.aux<em>s</em>sqrt( log(p) / n ).</p>
</td></tr>
<tr><td><code id="trans_precision_+3A_sel.type">sel.type</code></td>
<td>
<p>If pre-selection should be performed, &quot;sel.type&quot; is the type of distance. The default is L2 norm, and can be specified as &quot;L1&quot; to use L1 norm.</p>
</td></tr>
<tr><td><code id="trans_precision_+3A_input.a.cov">input.A.cov</code></td>
<td>
<p>Whether to input the covariance matrices of the auxiliary domains. The default setting is FALSE, which means that the raw data of the auxiliary domain is input, and the covariance will be calculated within this function. If input.A.cov=T, then the calculated covariance matrices must be input through parameter &quot;A.cov&quot;, and parameter &quot;A.data&quot; can be defaulted at this time. This setting is suitable for situations where raw data cannot be obtained but the covariance matrix can be obtained.</p>
</td></tr>
<tr><td><code id="trans_precision_+3A_a.cov">A.cov</code></td>
<td>
<p>If input.A.cov=T, the &quot;A.cov&quot; must be auxiliary covariance matrices in K auxiliary domains, a list with K elements, each of which is a p * p matrix.</p>
</td></tr>
<tr><td><code id="trans_precision_+3A_na.vec">nA.vec</code></td>
<td>
<p>If input.A.cov=T, the &quot;nA.vec&quot; must be a vector consisting of sample sizes of K auxiliary domains.</p>
</td></tr>
<tr><td><code id="trans_precision_+3A_t.theta.hat0">t.Theta.hat0</code></td>
<td>
<p>Whether to input the estimated target precision matrix based on the target domain only, and the default setting is NULL. If &quot;t.Theta.hat0&quot; is specified as an estimated precision matrix, it will not be recalculated in the initialization phase. This parameter mainly plays a role in transfer learning of GGMMs.</p>
</td></tr>
<tr><td><code id="trans_precision_+3A_t.n">t.n</code></td>
<td>
<p>Whether to input the target sample size, and the default setting is NULL. This parameter mainly plays a role in transfer learning of GGMMs.</p>
</td></tr>
<tr><td><code id="trans_precision_+3A_correlation">correlation</code></td>
<td>
<p>Whether to use correlation matrix for initial parameters in both target and auxiliary domains. The default setting is F.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A result list including:
</p>

<dl>
<dt>Theta.hat</dt><dd><p>The target precision matrix via transfer learning.</p>
</dd>
<dt>Theta.hat0</dt><dd><p>The initial target precision matrix.</p>
</dd>
<dt>k.check</dt><dd><p>The number of the optimal auxiliary domain.</p>
</dd>
<dt>N</dt><dd><p>The minimum sample size for auxiliary domain.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Mingyang Ren <a href="mailto:renmingyang17@mails.ucas.ac.cn">renmingyang17@mails.ucas.ac.cn</a>.
</p>


<h3>References</h3>

<p>Ren, M., Zhen Y., and Wang J. (2022). Transfer learning for tensor graphical models.
Ren, M., He X., and Wang J. (2023). Structural transfer learning of non-Gaussian DAG.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(TransGraph)
# load example data from github repository
# Please refer to https://github.com/Ren-Mingyang/example_data_TransGraph
# for detailed data information
githublink = "https://github.com/Ren-Mingyang/example_data_TransGraph/"
load(url(paste0(githublink,"raw/main/example.data.GGM.RData")))
t.data = example.data.GGM$target.list$t.data
t.precision = example.data.GGM$target.list$t.precision
A.data = example.data.GGM$A.data
A.data.infor = example.data.GGM$A.data.infor

# using all auxiliary domains
res.trans.weight = trans_precision(t.data, A.data, cov.method="weight")
res.trans.opt = trans_precision(t.data, A.data, cov.method="opt")
res.trans.size = trans_precision(t.data, A.data, cov.method="size")
Theta.trans.weight = res.trans.weight$Theta.hat
Theta.trans.opt = res.trans.opt$Theta.hat
Theta.trans.size = res.trans.size$Theta.hat
Theta.single = res.trans.weight$Theta.hat0  # initial rough estimation via the target domain
Theta.single[abs(Theta.single)&lt;0.0001] = 0

Evaluation.GGM(Theta.single, t.precision)
Evaluation.GGM(Theta.trans.weight, t.precision)
Evaluation.GGM(Theta.trans.opt, t.precision)
Evaluation.GGM(Theta.trans.size, t.precision)

# using informative auxiliary domains
res.trans.size.oracle = trans_precision(t.data, A.data.infor, cov.method="size")
Evaluation.GGM(res.trans.size.oracle$Theta.hat, t.precision)



</code></pre>

<hr>
<h2 id='trans.local.DAG'>Structural transfer learning of non-Gaussian DAG.</h2><span id='topic+trans.local.DAG'></span>

<h3>Description</h3>

<p>Structural transfer learning of non-Gaussian DAG.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trans.local.DAG(t.data, A.data, hardth=0.5, hardth.A=hardth, criti.val=0.01,
                       precision.method="glasso", precision.method.A = "CLIME",
                       cov.method="opt", cn.lam2=seq(1,2.5,length.out=10),
                       precision.refit=TRUE, ini.prec=TRUE, cut.off=TRUE,
                       preselect.aux=0, sel.type="L2")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="trans.local.DAG_+3A_t.data">t.data</code></td>
<td>
<p>The target data, a n * p matrix, where n is the sample size and p is data dimension.</p>
</td></tr>
<tr><td><code id="trans.local.DAG_+3A_a.data">A.data</code></td>
<td>
<p>The auxiliary data in K auxiliary domains, a list with K elements, each of which is a nk * p matrix, where nk is the sample size of the k-th auxiliary domain.</p>
</td></tr>
<tr><td><code id="trans.local.DAG_+3A_hardth">hardth</code></td>
<td>
<p>The hard threshold of regression in the target domain.</p>
</td></tr>
<tr><td><code id="trans.local.DAG_+3A_hardth.a">hardth.A</code></td>
<td>
<p>The hard threshold of regression in the auxiliary domains.</p>
</td></tr>
<tr><td><code id="trans.local.DAG_+3A_criti.val">criti.val</code></td>
<td>
<p>The critical value of independence test based on distance covariance, and the default setting is 0.01.</p>
</td></tr>
<tr><td><code id="trans.local.DAG_+3A_precision.method">precision.method</code></td>
<td>
<p>The initial method of estimating the target precision matrix, which can be selected as &quot;CLIME&quot; or &quot;glasso&quot;.</p>
</td></tr>
<tr><td><code id="trans.local.DAG_+3A_precision.method.a">precision.method.A</code></td>
<td>
<p>The initial method of estimating the auxiliary precision matrices, which can be selected as &quot;CLIME&quot; or &quot;glasso&quot;.</p>
</td></tr>
<tr><td><code id="trans.local.DAG_+3A_cov.method">cov.method</code></td>
<td>
<p>The method of aggregating K auxiliary covariance matrices, which can be selected as &quot;size&quot; (the sum weighted by the sample sizes), &quot;weight&quot; (the sum weighted by the differences), or &quot;opt&quot; (select the optimal one).</p>
</td></tr>
<tr><td><code id="trans.local.DAG_+3A_cn.lam2">cn.lam2</code></td>
<td>
<p>A vector or a float value: the coefficients set in tuning parameters used to solve the target precision matrix, default is cn.lam2*sqrt( log(p) / n ).</p>
</td></tr>
<tr><td><code id="trans.local.DAG_+3A_precision.refit">precision.refit</code></td>
<td>
<p>Whether to perform regression for re-fitting the coefficients in the precision matrix to improve estimation accuracy, after determining the non-zero elements of the precision matrix. The default is True.</p>
</td></tr>
<tr><td><code id="trans.local.DAG_+3A_ini.prec">ini.prec</code></td>
<td>
<p>Whether to store the initial estimation of the precision matrix, and the default is True.</p>
</td></tr>
<tr><td><code id="trans.local.DAG_+3A_cut.off">cut.off</code></td>
<td>
<p>Whether to truncate the finally estimated coefficients in the structural equation models at threshold &quot;hardth&quot;, and the default is True.</p>
</td></tr>
<tr><td><code id="trans.local.DAG_+3A_preselect.aux">preselect.aux</code></td>
<td>
<p>Whether to pre-select informative auxiliary domains based on the distance between initially estimated auxiliary and target parameters. The default is 0, which means that pre-selection will not be performed. If &quot;preselect.aux&quot; is specified as a real number greater than zero, then the threshold value is forpreselect.aux<em>s</em>sqrt( log(p) / n ).</p>
</td></tr>
<tr><td><code id="trans.local.DAG_+3A_sel.type">sel.type</code></td>
<td>
<p>If pre-selection should be performed, &quot;sel.type&quot; is the type of distance. The default is L2 norm, and can be specified as &quot;L1&quot; to use L1 norm.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A result list including:
</p>

<dl>
<dt>A</dt><dd><p>The information of layer.</p>
</dd>
<dt>B</dt><dd><p>The coefficients in structural equation models.</p>
</dd>
<dt>prec.res0</dt><dd><p>The results about estimating the prscision matrix via transfer learning.</p>
</dd>
<dt>prec.res0$Theta.hat</dt><dd><p>The estimated prscision matrix via transfer learning.</p>
</dd>
<dt>prec.res0$Theta.hat0</dt><dd><p>The estimated prscision matrix based on the target domain only.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Mingyang Ren <a href="mailto:renmingyang17@mails.ucas.ac.cn">renmingyang17@mails.ucas.ac.cn</a>, Xin He, and Junhui Wang
</p>


<h3>References</h3>

<p>Ren, M., He X., and Wang J. (2023). Structural transfer learning of non-Gaussian DAG.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(TransGraph)
# load example data from github repository
# Please refer to https://github.com/Ren-Mingyang/example_data_TransGraph
# for detailed data information
githublink = "https://github.com/Ren-Mingyang/example_data_TransGraph/"
load(url(paste0(githublink,"raw/main/example.data.DAG.RData")))
t.data = example.data.DAG$target.DAG.data$X
true_adjace = example.data.DAG$target.DAG.data$true_adjace
A.data = example.data.DAG$auxiliary.DAG.data$X.list.A

# transfer method
res.trans = trans.local.DAG(t.data, A.data)
# Topological Layer method-based single-task learning (JLMR, 2022)
res.single = TLLiNGAM(t.data)

Evaluation.DAG(res.trans$B, true_adjace)$Eval_result
Evaluation.DAG(res.single$B, true_adjace)$Eval_result



</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
