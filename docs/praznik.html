<!DOCTYPE html><html lang="en"><head><title>Help for package praznik</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {praznik}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#praznik-package'><p>Tools for information-based feature selection and scoring</p></a></li>
<li><a href='#CMI'><p>Conditional mutual information maximisation filter</p></a></li>
<li><a href='#CMIM'><p>Minimal conditional mutual information maximisation filter</p></a></li>
<li><a href='#cmiMatrix'><p>Conditional mutual information matrix with a common condition</p></a></li>
<li><a href='#cmiScores'><p>Conditional mutual information scores</p></a></li>
<li><a href='#DISR'><p>Double input symmetrical relevance filter</p></a></li>
<li><a href='#dnmiMatrix'><p>Directional normalised mutual information matrix</p></a></li>
<li><a href='#hScores'><p>Entropy scores</p></a></li>
<li><a href='#icmiMatrix'><p>Conditional mutual information matrix with a common variable</p></a></li>
<li><a href='#impScores'><p>Gini impurity scores</p></a></li>
<li><a href='#jhScores'><p>Joint entropy scores</p></a></li>
<li><a href='#JIM'><p>Joint impurity filter</p></a></li>
<li><a href='#JMI'><p>Joint mutual information filter</p></a></li>
<li><a href='#JMI3'><p>Third-order joint mutual information filter</p></a></li>
<li><a href='#JMIM'><p>Minimal joint mutual information maximisation filter</p></a></li>
<li><a href='#jmiMatrix'><p>Joint mutual information matrix</p></a></li>
<li><a href='#jmiScores'><p>Joint mutual information scores</p></a></li>
<li><a href='#joinf'><p>Join factors</p></a></li>
<li><a href='#kInverse'><p>Inverse Kendall transform</p></a></li>
<li><a href='#kTransform'><p>Kendall transformation</p></a></li>
<li><a href='#MadelonD'><p>Pre-discretised Madelon dataset</p></a></li>
<li><a href='#maxCmiScores'><p>Maximal pairwise conditional mutual information scores</p></a></li>
<li><a href='#maxJmiScores'><p>Maximal pairwise joint mutual information scores</p></a></li>
<li><a href='#MIM'><p>Mutual information maximisation filter</p></a></li>
<li><a href='#miMatrix'><p>Mutual information matrix</p></a></li>
<li><a href='#minCmiScores'><p>Minimal pairwise conditional mutual information scores</p></a></li>
<li><a href='#minMaxCmiScores'><p>Extreme values of pairwise conditional mutual information scores</p></a></li>
<li><a href='#miScores'><p>Mutual information scores</p></a></li>
<li><a href='#MRMR'><p>Minimum redundancy maximal relevancy filter</p></a></li>
<li><a href='#NJMIM'><p>Minimal normalised joint mutual information maximisation filter</p></a></li>
<li><a href='#njmiMatrix'><p>Normalised joint mutual information matrix</p></a></li>
<li><a href='#njmiScores'><p>Normalised joint mutual information scores</p></a></li>
<li><a href='#nmiMatrix'><p>Normalised mutual information matrix</p></a></li>
<li><a href='#triScores'><p>Mutual information of feature triples</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Tools for Information-Based Feature Selection and Scoring</td>
</tr>
<tr>
<td>Version:</td>
<td>11.0.0</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Description:</td>
<td>A toolbox of fast, native and parallel implementations of various information-based importance criteria estimators and feature selection filters based on them, inspired by the overview by Brown, Pocock, Zhao and Lujan (2012) <a href="https://www.jmlr.org/papers/v13/brown12a.html">https://www.jmlr.org/papers/v13/brown12a.html</a>.
 Contains, among other, minimum redundancy maximal relevancy ('mRMR') method by Peng, Long and Ding (2005) &lt;<a href="https://doi.org/10.1109%2FTPAMI.2005.159">doi:10.1109/TPAMI.2005.159</a>&gt;; joint mutual information ('JMI') method by Yang and Moody (1999) <a href="https://papers.nips.cc/paper/1779-data-visualization-and-feature-selection-new-algorithms-for-nongaussian-data">https://papers.nips.cc/paper/1779-data-visualization-and-feature-selection-new-algorithms-for-nongaussian-data</a>; double input symmetrical relevance ('DISR') method by Meyer and Bontempi (2006) &lt;<a href="https://doi.org/10.1007%2F11732242_9">doi:10.1007/11732242_9</a>&gt; as well as joint mutual information maximisation ('JMIM') method by Bennasar, Hicks and Setchi (2015) &lt;<a href="https://doi.org/10.1016%2Fj.eswa.2015.07.007">doi:10.1016/j.eswa.2015.07.007</a>&gt;.</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://gitlab.com/mbq/praznik/-/issues">https://gitlab.com/mbq/praznik/-/issues</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://gitlab.com/mbq/praznik">https://gitlab.com/mbq/praznik</a></td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-05-20 14:57:31 UTC; mbq</td>
</tr>
<tr>
<td>Author:</td>
<td>Miron B. Kursa <a href="https://orcid.org/0000-0001-7672-648X"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Miron B. Kursa &lt;m@mbq.me&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-05-20 15:30:01 UTC</td>
</tr>
</table>
<hr>
<h2 id='praznik-package'>Tools for information-based feature selection and scoring</h2><span id='topic+praznik'></span><span id='topic+praznik-package'></span>

<h3>Description</h3>

<p>Praznik is a collection of tools for information theory-based feature selection and scoring.
</p>


<h3>Details</h3>

<p>The first part of the package consists of efficient implementations of several popular information filters, feature selection approaches based on greedy optimisation of certain feature inclusion criterion.
In a nutshell, an algorithm of this class requires an information system <code class="reqn">(X,Y)</code> and a predefined number of features to selected <code class="reqn">k</code>, and works like this.
To start, it estimates mutual information between each feature and the decision, find a feature with maximal such score and stores it as a first on a list of selected features, <code class="reqn">S</code>.
Then, it estimates a value of a certain criterion function <code class="reqn">J(X,Y,S)</code> for each feature <code class="reqn">X</code>; this function also depends on <code class="reqn">Y</code> and the set of already selected features <code class="reqn">S</code>.
As in the first step, the previously unselected feature with a greatest value of the criterion function is selected next.
This is repeated until the method would gather <code class="reqn">k</code> features, or, in case of some methods, when no more informative features can be found.
The methods implemented in praznik consider the following criteria.
</p>
<p>The mutual information maximisation filter, <code><a href="#topic+MIM">MIM</a></code>, simply selects top-<code class="reqn">k</code> features of best mutual information, that is
</p>
<p style="text-align: center;"><code class="reqn">J_{MIM}=I(X;Y).</code>
</p>

<p>The minimal conditional mutual information maximisation proposed by F. Fleauret, <code><a href="#topic+CMIM">CMIM</a></code>, uses
</p>
<p style="text-align: center;"><code class="reqn">J_{CMIM}(X)=\min(I(X;Y),\min_{W\in S} I(X;Y|W));</code>
</p>

<p>this method is also effectively identical to the information fragments method.
</p>
<p>The minimum redundancy maximal relevancy proposed by H. Peng et al., <code><a href="#topic+MRMR">MRMR</a></code>, uses
</p>
<p style="text-align: center;"><code class="reqn">J_{MRMR}=I(X;Y)-\frac{1}{|S|}\sum_{W\in S} I(X;W).</code>
</p>

<p>The joint mutual information filter by H. Yang and J. Moody, <code><a href="#topic+JMI">JMI</a></code>, uses
</p>
<p style="text-align: center;"><code class="reqn">J_{JMI}=\sum_{W\in S} I(X,W;Y).</code>
</p>

<p>The double input symmetrical relevance filter by P. Meyer and G. Bontempi, <code><a href="#topic+DISR">DISR</a></code>, uses
</p>
<p style="text-align: center;"><code class="reqn">J_{DISR}(X)=\sum_{W\in S} \frac{I(X,W;Y)}{H(X,W,Y)}.</code>
</p>

<p>The minimal joint mutual information maximisation filter by M. Bennasar, Y. Hicks and R. Setchi, <code><a href="#topic+JMIM">JMIM</a></code>, uses
</p>
<p style="text-align: center;"><code class="reqn">J_{JMIM}=\min_{W\in S} I(X,W;Y).</code>
</p>

<p>The minimal normalised joint mutual information maximisation filter by the same authors, <code><a href="#topic+NJMIM">NJMIM</a></code>, uses
</p>
<p style="text-align: center;"><code class="reqn">J_{NJMIM}=\min_{W\in S} \frac{I(X,W;Y)}{H(X,W,Y)}.</code>
</p>

<p>The third-order joint mutual information filter by Sechidis et al., <code><a href="#topic+JMI3">JMI3</a></code>, uses
</p>
<p style="text-align: center;"><code class="reqn">J(X)=\frac{1}{2}\sum_{(U,W)\in S^2; U\neq W} I(X,U,W;Y).</code>
</p>

<p>While <code><a href="#topic+CMIM">CMIM</a></code>, <code><a href="#topic+JMIM">JMIM</a></code> and <code><a href="#topic+NJMIM">NJMIM</a></code> consider minimal value over already selected features, they may use a somewhat more sophisticated and faster algorithm.
</p>
<p>The second part of the package provides methods for scoring features, useful on its own as well as building blocks for more sophisticated algorithms.
In particular, the package exposes the following functions:
</p>
<p><code><a href="#topic+hScores">hScores</a></code> returns 
</p>
<p style="text-align: center;"><code class="reqn">H(X).</code>
</p>

<p><code><a href="#topic+jhScores">jhScores</a></code> returns 
</p>
<p style="text-align: center;"><code class="reqn">H(X,Y).</code>
</p>

<p><code><a href="#topic+miScores">miScores</a></code> returns 
</p>
<p style="text-align: center;"><code class="reqn">I(X;Y).</code>
</p>

<p><code><a href="#topic+cmiScores">cmiScores</a></code> returns, for a given condition vector <code class="reqn">Z</code>,
</p>
<p style="text-align: center;"><code class="reqn">I(X;Y|Z).</code>
</p>

<p><code><a href="#topic+jmiScores">jmiScores</a></code> returns
</p>
<p style="text-align: center;"><code class="reqn">I(X,Z;Y).</code>
</p>

<p><code><a href="#topic+njmiScores">njmiScores</a></code> returns
</p>
<p style="text-align: center;"><code class="reqn">\frac{I(X,Z;Y)}{H(X,Y,Z)}.</code>
</p>

<p><code><a href="#topic+minCmiScores">minCmiScores</a></code>, <code><a href="#topic+maxCmiScores">maxCmiScores</a></code> and <code><a href="#topic+minMaxCmiScores">minMaxCmiScores</a></code> return
</p>
<p style="text-align: center;"><code class="reqn">\min_j I(X_i;Y|X_j)</code>
</p>
<p> and/or </p>
<p style="text-align: center;"><code class="reqn">\max_j I(X_i;Y|X_j).</code>
</p>

<p><code><a href="#topic+maxJmiScores">maxJmiScores</a></code> returns
</p>
<p style="text-align: center;"><code class="reqn">\max_{j\neq i} I(X_i,X_j;Y).</code>
</p>

<p><code><a href="#topic+triScores">triScores</a></code> returns, for every triple of features,
</p>
<p style="text-align: center;"><code class="reqn">I(X_i;X_j;X_k).</code>
</p>

<p>These function generally also have their <code>*Matrix</code> counterparts, which efficiently build a matrix of scores between all pairs of features.
This is especially useful for network inference tasks.
</p>
<p>Estimation of mutual information and its generalisations is a hard task; still, praznik aims at speed and simplicity and hence only offers basic, maximum likelihood estimator applicable on discrete data.
For convenience, praznik automatically and silently coerces non-factor inputs into about ten equally-spaced bins, following the heuristic often used in literature.
</p>
<p>Furthermore, praznik provides <code><a href="#topic+kTransform">kTransform</a></code> function for converting continuous features into discrete ones with Kendall transformation, a novel approach based on Kendall correlation coefficient which allows for multivariate reasoning based on monotonicity agreement.
</p>
<p>Additionally, praznik has a limited, experimental support for replacing entropic statistics with Gini impurity-based; in such framework, entropy is replaced by Gini impurity
</p>
<p style="text-align: center;"><code class="reqn">g(X):=1-\sum_x p_x^2,</code>
</p>

<p>which leads to an impurity gain
</p>
<p style="text-align: center;"><code class="reqn">G(X;Y):=g(Y)-E(g(Y)|X)=\sum_{xy}\frac{p_{xy}^2}{p_x}-\sum_{y} p_y^2,</code>
</p>

<p>a counterpart of mutual information or information gain.
It does not possess most of elegant properties of mutual information, yet values of both are usually highly correlated; moreover, Gini gain is computationally easier to calculate, hence it often replaces MI in performance-sensitive applications, like optimising splits in decision trees.
</p>
<p>In a present version, praznik includes <code><a href="#topic+impScores">impScores</a></code> for generating values of <code class="reqn">G</code> for all features (an analog of <code><a href="#topic+miScores">miScores</a></code>, as well as <code><a href="#topic+JIM">JIM</a></code>, a Gini gain-based feature selection method otherwise identical to <code><a href="#topic+JMI">JMI</a></code>.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Miron B. Kursa <a href="mailto:m@mbq.me">m@mbq.me</a> (<a href="https://orcid.org/0000-0001-7672-648X">ORCID</a>)
</p>


<h3>References</h3>

<p>&quot;Praznik: High performance information-based feature selection&quot; M.B. Kursa. SoftwareX 16, 100819 (2021).
</p>
<p>&quot;Conditional Likelihood Maximisation: A Unifying Framework for Information Theoretic Feature Selection&quot; G. Brown et al. JMLR (2012).
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://gitlab.com/mbq/praznik">https://gitlab.com/mbq/praznik</a>
</p>
</li>
<li><p> Report bugs at <a href="https://gitlab.com/mbq/praznik/-/issues">https://gitlab.com/mbq/praznik/-/issues</a>
</p>
</li></ul>


<hr>
<h2 id='CMI'>Conditional mutual information maximisation filter</h2><span id='topic+CMI'></span>

<h3>Description</h3>

<p>The method starts with a feature of a maximal mutual information with the decision <code class="reqn">Y</code>.
Then, it greedily adds feature <code class="reqn">X</code> with a maximal value of the following criterion:
</p>
<p style="text-align: center;"><code class="reqn">J(X)=I(X;Y|S),</code>
</p>

<p>where <code class="reqn">S</code> is the set of already selected features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CMI(X, Y, k = 3, threads = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="CMI_+3A_x">X</code></td>
<td>
<p>Attribute table, given as a data frame with either factors (preferred), booleans, integers (treated as categorical) or reals (which undergo automatic categorisation; see below for details).
Single vector will be interpreted as a data.frame with one column.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="CMI_+3A_y">Y</code></td>
<td>
<p>Decision attribute; should be given as a factor, but other options are accepted, exactly like for attributes.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="CMI_+3A_k">k</code></td>
<td>
<p>Number of attributes to select.
Must not exceed <code>ncol(X)</code>.</p>
</td></tr>
<tr><td><code id="CMI_+3A_threads">threads</code></td>
<td>
<p>Number of threads to use; default value, 0, means all available to OpenMP.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with two elements: <code>selection</code>, a vector of indices of the selected features in the selection order, and <code>score</code>, a vector of corresponding feature scores.
Names of both vectors will correspond to the names of features in <code>X</code>.
Both vectors will be at most of a length <code>k</code>, as the selection may stop sooner, even during initial selection, in which case both vectors will be empty.
</p>


<h3>Note</h3>

<p>The method requires input to be discrete to use empirical estimators of distribution, and, consequently, information gain or entropy.
To allow smoother user experience, praznik automatically coerces non-factor vectors in inputs, which requires additional time, memory and may yield confusing results &ndash; the best practice is to convert data to factors prior to feeding them in this function.
Real attributes are cut into about 10 equally-spaced bins, following the heuristic often used in literature.
Precise number of cuts depends on the number of objects; namely, it is <code class="reqn">n/3</code>, but never less than 2 and never more than 10.
Integers (which technically are also numeric) are treated as categorical variables (for compatibility with similar software), so in a very different way &ndash; one should be aware that an actually numeric attribute which happens to be an integer could be coerced into a <code class="reqn">n</code>-level categorical, which would have a perfect mutual information score and would likely become a very disruptive false positive.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(MadelonD)
CMI(MadelonD$X,MadelonD$Y,20)
</code></pre>

<hr>
<h2 id='CMIM'>Minimal conditional mutual information maximisation filter</h2><span id='topic+CMIM'></span>

<h3>Description</h3>

<p>The method starts with a feature of a maximal mutual information with the decision <code class="reqn">Y</code>.
Then, it greedily adds feature <code class="reqn">X</code> with a maximal value of the following criterion:
</p>
<p style="text-align: center;"><code class="reqn">J(X)=\min(I(X;Y),\min_{W\in S} I(X;Y|W)),</code>
</p>

<p>where <code class="reqn">S</code> is the set of already selected features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CMIM(X, Y, k = 3, threads = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="CMIM_+3A_x">X</code></td>
<td>
<p>Attribute table, given as a data frame with either factors (preferred), booleans, integers (treated as categorical) or reals (which undergo automatic categorisation; see below for details).
Single vector will be interpreted as a data.frame with one column.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="CMIM_+3A_y">Y</code></td>
<td>
<p>Decision attribute; should be given as a factor, but other options are accepted, exactly like for attributes.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="CMIM_+3A_k">k</code></td>
<td>
<p>Number of attributes to select.
Must not exceed <code>ncol(X)</code>.</p>
</td></tr>
<tr><td><code id="CMIM_+3A_threads">threads</code></td>
<td>
<p>Number of threads to use; default value, 0, means all available to OpenMP.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with two elements: <code>selection</code>, a vector of indices of the selected features in the selection order, and <code>score</code>, a vector of corresponding feature scores.
Names of both vectors will correspond to the names of features in <code>X</code>.
Both vectors will be at most of a length <code>k</code>, as the selection may stop sooner, even during initial selection, in which case both vectors will be empty.
</p>


<h3>Note</h3>

<p>The method requires input to be discrete to use empirical estimators of distribution, and, consequently, information gain or entropy.
To allow smoother user experience, praznik automatically coerces non-factor vectors in inputs, which requires additional time, memory and may yield confusing results &ndash; the best practice is to convert data to factors prior to feeding them in this function.
Real attributes are cut into about 10 equally-spaced bins, following the heuristic often used in literature.
Precise number of cuts depends on the number of objects; namely, it is <code class="reqn">n/3</code>, but never less than 2 and never more than 10.
Integers (which technically are also numeric) are treated as categorical variables (for compatibility with similar software), so in a very different way &ndash; one should be aware that an actually numeric attribute which happens to be an integer could be coerced into a <code class="reqn">n</code>-level categorical, which would have a perfect mutual information score and would likely become a very disruptive false positive.
</p>


<h3>References</h3>

<p>&quot;Fast Binary Feature Selection using Conditional Mutual Information Maximisation&quot; F. Fleuret, JMLR (2004)
</p>
<p>&quot;Object recognition with informative features and linear classification&quot; M. Vidal-Naquet and S. Ullman, IEEE Conference on Computer Vision and Pattern Recognition (2003).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(MadelonD)
CMIM(MadelonD$X,MadelonD$Y,20)
</code></pre>

<hr>
<h2 id='cmiMatrix'>Conditional mutual information matrix with a common condition</h2><span id='topic+cmiMatrix'></span>

<h3>Description</h3>

<p>Calculates conditional mutual information between each two features given another one, that is
</p>
<p style="text-align: center;"><code class="reqn">I(X_i;X_j|Z).</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>cmiMatrix(X, Z, zeroDiag = TRUE, threads = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cmiMatrix_+3A_x">X</code></td>
<td>
<p>Attribute table, given as a data frame with either factors (preferred), booleans, integers (treated as categorical) or reals (which undergo automatic categorisation; see below for details).
Single vector will be interpreted as a data.frame with one column.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="cmiMatrix_+3A_z">Z</code></td>
<td>
<p>Condition; should be given as a factor, but other options are accepted, as for features.</p>
</td></tr>
<tr><td><code id="cmiMatrix_+3A_zerodiag">zeroDiag</code></td>
<td>
<p>Boolean flag, whether the diagonal should be filled with zeroes, or with degenerated scores for two identical copies of a feature.</p>
</td></tr>
<tr><td><code id="cmiMatrix_+3A_threads">threads</code></td>
<td>
<p>Number of threads to use; default value, 0, means all available to OpenMP.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numerical matrix with scores, with row and column names copied from <code>X</code>.
</p>


<h3>Note</h3>

<p>The method requires input to be discrete to use empirical estimators of distribution, and, consequently, information gain or entropy.
To allow smoother user experience, praznik automatically coerces non-factor vectors in inputs, which requires additional time, memory and may yield confusing results &ndash; the best practice is to convert data to factors prior to feeding them in this function.
Real attributes are cut into about 10 equally-spaced bins, following the heuristic often used in literature.
Precise number of cuts depends on the number of objects; namely, it is <code class="reqn">n/3</code>, but never less than 2 and never more than 10.
Integers (which technically are also numeric) are treated as categorical variables (for compatibility with similar software), so in a very different way &ndash; one should be aware that an actually numeric attribute which happens to be an integer could be coerced into a <code class="reqn">n</code>-level categorical, which would have a perfect mutual information score and would likely become a very disruptive false positive.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cmiMatrix(iris[,-5],iris[,5])
</code></pre>

<hr>
<h2 id='cmiScores'>Conditional mutual information scores</h2><span id='topic+cmiScores'></span>

<h3>Description</h3>

<p>Calculates conditional mutual information between each features and the decision, that is
</p>
<p style="text-align: center;"><code class="reqn">I(X;Y|Z).</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>cmiScores(X, Y, Z, threads = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cmiScores_+3A_x">X</code></td>
<td>
<p>Attribute table, given as a data frame with either factors (preferred), booleans, integers (treated as categorical) or reals (which undergo automatic categorisation; see below for details).
Single vector will be interpreted as a data.frame with one column.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="cmiScores_+3A_y">Y</code></td>
<td>
<p>Decision attribute; should be given as a factor, but other options are accepted, exactly like for attributes.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="cmiScores_+3A_z">Z</code></td>
<td>
<p>Condition; should be given as a factor, but other options are accepted, as for features.</p>
</td></tr>
<tr><td><code id="cmiScores_+3A_threads">threads</code></td>
<td>
<p>Number of threads to use; default value, 0, means all available to OpenMP.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numerical vector with conditional mutual information scores, with names copied from <code>X</code>.
</p>


<h3>Note</h3>

<p>The method requires input to be discrete to use empirical estimators of distribution, and, consequently, information gain or entropy.
To allow smoother user experience, praznik automatically coerces non-factor vectors in inputs, which requires additional time, memory and may yield confusing results &ndash; the best practice is to convert data to factors prior to feeding them in this function.
Real attributes are cut into about 10 equally-spaced bins, following the heuristic often used in literature.
Precise number of cuts depends on the number of objects; namely, it is <code class="reqn">n/3</code>, but never less than 2 and never more than 10.
Integers (which technically are also numeric) are treated as categorical variables (for compatibility with similar software), so in a very different way &ndash; one should be aware that an actually numeric attribute which happens to be an integer could be coerced into a <code class="reqn">n</code>-level categorical, which would have a perfect mutual information score and would likely become a very disruptive false positive.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cmiScores(iris[,-5],iris$Species,iris$Sepal.Length)
</code></pre>

<hr>
<h2 id='DISR'>Double input symmetrical relevance filter</h2><span id='topic+DISR'></span>

<h3>Description</h3>

<p>The method starts with a feature of a maximal mutual information with the decision <code class="reqn">Y</code>.
Then, it greedily adds feature <code class="reqn">X</code> with a maximal value of the following criterion:
</p>
<p style="text-align: center;"><code class="reqn">J(X)=\sum_{W\in S} \frac{I(X,W;Y)}{H(X,W,Y)},</code>
</p>

<p>where <code class="reqn">S</code> is the set of already selected features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DISR(X, Y, k = 3, threads = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="DISR_+3A_x">X</code></td>
<td>
<p>Attribute table, given as a data frame with either factors (preferred), booleans, integers (treated as categorical) or reals (which undergo automatic categorisation; see below for details).
Single vector will be interpreted as a data.frame with one column.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="DISR_+3A_y">Y</code></td>
<td>
<p>Decision attribute; should be given as a factor, but other options are accepted, exactly like for attributes.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="DISR_+3A_k">k</code></td>
<td>
<p>Number of attributes to select.
Must not exceed <code>ncol(X)</code>.</p>
</td></tr>
<tr><td><code id="DISR_+3A_threads">threads</code></td>
<td>
<p>Number of threads to use; default value, 0, means all available to OpenMP.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with two elements: <code>selection</code>, a vector of indices of the selected features in the selection order, and <code>score</code>, a vector of corresponding feature scores.
Names of both vectors will correspond to the names of features in <code>X</code>.
Both vectors will be at most of a length <code>k</code>, as the selection may stop sooner, even during initial selection, in which case both vectors will be empty.
</p>


<h3>Note</h3>

<p>DISR is a normalised version of <code><a href="#topic+JMI">JMI</a></code>; <code><a href="#topic+JMIM">JMIM</a></code> and <code><a href="#topic+NJMIM">NJMIM</a></code> are modifications of JMI and DISR in which minimal joint information over already selected features is used instead of a sum.
</p>
<p>The method requires input to be discrete to use empirical estimators of distribution, and, consequently, information gain or entropy.
To allow smoother user experience, praznik automatically coerces non-factor vectors in inputs, which requires additional time, memory and may yield confusing results &ndash; the best practice is to convert data to factors prior to feeding them in this function.
Real attributes are cut into about 10 equally-spaced bins, following the heuristic often used in literature.
Precise number of cuts depends on the number of objects; namely, it is <code class="reqn">n/3</code>, but never less than 2 and never more than 10.
Integers (which technically are also numeric) are treated as categorical variables (for compatibility with similar software), so in a very different way &ndash; one should be aware that an actually numeric attribute which happens to be an integer could be coerced into a <code class="reqn">n</code>-level categorical, which would have a perfect mutual information score and would likely become a very disruptive false positive.
</p>


<h3>References</h3>

<p>&quot;On the Use of Variable Complementarity for Feature Selection in Cancer Classification&quot; P. Meyer and G. Bontempi, (2006)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(MadelonD)
DISR(MadelonD$X,MadelonD$Y,20)
</code></pre>

<hr>
<h2 id='dnmiMatrix'>Directional normalised mutual information matrix</h2><span id='topic+dnmiMatrix'></span>

<h3>Description</h3>

<p>Calculates directed normalised mutual information between each two features, that is
</p>
<p style="text-align: center;"><code class="reqn">\frac{I(X_i,X_j)}{H(X_j)}.</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>dnmiMatrix(X, zeroDiag = TRUE, threads = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dnmiMatrix_+3A_x">X</code></td>
<td>
<p>Attribute table, given as a data frame with either factors (preferred), booleans, integers (treated as categorical) or reals (which undergo automatic categorisation; see below for details).
Single vector will be interpreted as a data.frame with one column.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="dnmiMatrix_+3A_zerodiag">zeroDiag</code></td>
<td>
<p>Boolean flag, whether the diagonal should be filled with zeroes, or with degenerated scores for two identical copies of a feature.</p>
</td></tr>
<tr><td><code id="dnmiMatrix_+3A_threads">threads</code></td>
<td>
<p>Number of threads to use; default value, 0, means all available to OpenMP.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numerical matrix with scores, with row and column names copied from <code>X</code>.
</p>


<h3>Note</h3>

<p>The method requires input to be discrete to use empirical estimators of distribution, and, consequently, information gain or entropy.
To allow smoother user experience, praznik automatically coerces non-factor vectors in inputs, which requires additional time, memory and may yield confusing results &ndash; the best practice is to convert data to factors prior to feeding them in this function.
Real attributes are cut into about 10 equally-spaced bins, following the heuristic often used in literature.
Precise number of cuts depends on the number of objects; namely, it is <code class="reqn">n/3</code>, but never less than 2 and never more than 10.
Integers (which technically are also numeric) are treated as categorical variables (for compatibility with similar software), so in a very different way &ndash; one should be aware that an actually numeric attribute which happens to be an integer could be coerced into a <code class="reqn">n</code>-level categorical, which would have a perfect mutual information score and would likely become a very disruptive false positive.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dnmiMatrix(iris)
</code></pre>

<hr>
<h2 id='hScores'>Entropy scores</h2><span id='topic+hScores'></span>

<h3>Description</h3>

<p>Calculates entropy of each feature, that is
</p>
<p style="text-align: center;"><code class="reqn">H(X).</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>hScores(X, threads = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hScores_+3A_x">X</code></td>
<td>
<p>Attribute table, given as a data frame with either factors (preferred), booleans, integers (treated as categorical) or reals (which undergo automatic categorisation; see below for details).
Single vector will be interpreted as a data.frame with one column.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="hScores_+3A_threads">threads</code></td>
<td>
<p>Number of threads to use; default value, 0, means all available to OpenMP.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numerical vector with entropy scores, with names copied from <code>X</code>.
</p>


<h3>Note</h3>

<p>The method requires input to be discrete to use empirical estimators of distribution, and, consequently, information gain or entropy.
To allow smoother user experience, praznik automatically coerces non-factor vectors in inputs, which requires additional time, memory and may yield confusing results &ndash; the best practice is to convert data to factors prior to feeding them in this function.
Real attributes are cut into about 10 equally-spaced bins, following the heuristic often used in literature.
Precise number of cuts depends on the number of objects; namely, it is <code class="reqn">n/3</code>, but never less than 2 and never more than 10.
Integers (which technically are also numeric) are treated as categorical variables (for compatibility with similar software), so in a very different way &ndash; one should be aware that an actually numeric attribute which happens to be an integer could be coerced into a <code class="reqn">n</code>-level categorical, which would have a perfect mutual information score and would likely become a very disruptive false positive.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>hScores(iris[,-5])
</code></pre>

<hr>
<h2 id='icmiMatrix'>Conditional mutual information matrix with a common variable</h2><span id='topic+icmiMatrix'></span>

<h3>Description</h3>

<p>Calculates conditional mutual information between each feature and the decision given each other feature, that is
</p>
<p style="text-align: center;"><code class="reqn">I(X_i;Y|X_j).</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>icmiMatrix(X, Y, threads = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="icmiMatrix_+3A_x">X</code></td>
<td>
<p>Attribute table, given as a data frame with either factors (preferred), booleans, integers (treated as categorical) or reals (which undergo automatic categorisation; see below for details).
Single vector will be interpreted as a data.frame with one column.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="icmiMatrix_+3A_y">Y</code></td>
<td>
<p>Decision attribute; should be given as a factor, but other options are accepted, exactly like for attributes.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="icmiMatrix_+3A_threads">threads</code></td>
<td>
<p>Number of threads to use; default value, 0, means all available to OpenMP.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numerical matrix with scores, with row and column names copied from <code>X</code>.
</p>


<h3>Note</h3>

<p>The method requires input to be discrete to use empirical estimators of distribution, and, consequently, information gain or entropy.
To allow smoother user experience, praznik automatically coerces non-factor vectors in inputs, which requires additional time, memory and may yield confusing results &ndash; the best practice is to convert data to factors prior to feeding them in this function.
Real attributes are cut into about 10 equally-spaced bins, following the heuristic often used in literature.
Precise number of cuts depends on the number of objects; namely, it is <code class="reqn">n/3</code>, but never less than 2 and never more than 10.
Integers (which technically are also numeric) are treated as categorical variables (for compatibility with similar software), so in a very different way &ndash; one should be aware that an actually numeric attribute which happens to be an integer could be coerced into a <code class="reqn">n</code>-level categorical, which would have a perfect mutual information score and would likely become a very disruptive false positive.
</p>
<p>Diagonal is always zero with this score.
The function name comes from the reasoning that this is an &quot;interaction-CMI&quot; showing how feature pairs interact in explaining the decision.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>icmiMatrix(iris[,-5],iris[,5])
</code></pre>

<hr>
<h2 id='impScores'>Gini impurity scores</h2><span id='topic+impScores'></span>

<h3>Description</h3>

<p>Calculates Gini impurity between each feature and the decision, that is
</p>
<p style="text-align: center;"><code class="reqn">G(X;Y)=\sum_{xy} \frac{p_{xy}^2}{p_x}-\sum_y p_y^2.</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>impScores(X, Y, threads = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="impScores_+3A_x">X</code></td>
<td>
<p>Attribute table, given as a data frame with either factors (preferred), booleans, integers (treated as categorical) or reals (which undergo automatic categorisation; see below for details).
Single vector will be interpreted as a data.frame with one column.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="impScores_+3A_y">Y</code></td>
<td>
<p>Decision attribute; should be given as a factor, but other options are accepted, exactly like for attributes.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="impScores_+3A_threads">threads</code></td>
<td>
<p>Number of threads to use; default value, 0, means all available to OpenMP.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numerical vector with Gini impurity scores, with names copied from <code>X</code>.
</p>


<h3>Note</h3>

<p>The method requires input to be discrete to use empirical estimators of distribution, and, consequently, information gain or entropy.
To allow smoother user experience, praznik automatically coerces non-factor vectors in inputs, which requires additional time, memory and may yield confusing results &ndash; the best practice is to convert data to factors prior to feeding them in this function.
Real attributes are cut into about 10 equally-spaced bins, following the heuristic often used in literature.
Precise number of cuts depends on the number of objects; namely, it is <code class="reqn">n/3</code>, but never less than 2 and never more than 10.
Integers (which technically are also numeric) are treated as categorical variables (for compatibility with similar software), so in a very different way &ndash; one should be aware that an actually numeric attribute which happens to be an integer could be coerced into a <code class="reqn">n</code>-level categorical, which would have a perfect mutual information score and would likely become a very disruptive false positive.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>impScores(iris[,-5],iris$Species)
</code></pre>

<hr>
<h2 id='jhScores'>Joint entropy scores</h2><span id='topic+jhScores'></span>

<h3>Description</h3>

<p>Calculates joint entropy of each feature and a condition <code>Y</code>, that is
</p>
<p style="text-align: center;"><code class="reqn">H(X,Y).</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>jhScores(X, Y, threads = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="jhScores_+3A_x">X</code></td>
<td>
<p>Attribute table, given as a data frame with either factors (preferred), booleans, integers (treated as categorical) or reals (which undergo automatic categorisation; see below for details).
Single vector will be interpreted as a data.frame with one column.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="jhScores_+3A_y">Y</code></td>
<td>
<p>Decision attribute; should be given as a factor, but other options are accepted, exactly like for attributes.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="jhScores_+3A_threads">threads</code></td>
<td>
<p>Number of threads to use; default value, 0, means all available to OpenMP.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numerical vector with entropy scores, with names copied from <code>X</code>.
</p>


<h3>Note</h3>

<p>The method requires input to be discrete to use empirical estimators of distribution, and, consequently, information gain or entropy.
To allow smoother user experience, praznik automatically coerces non-factor vectors in inputs, which requires additional time, memory and may yield confusing results &ndash; the best practice is to convert data to factors prior to feeding them in this function.
Real attributes are cut into about 10 equally-spaced bins, following the heuristic often used in literature.
Precise number of cuts depends on the number of objects; namely, it is <code class="reqn">n/3</code>, but never less than 2 and never more than 10.
Integers (which technically are also numeric) are treated as categorical variables (for compatibility with similar software), so in a very different way &ndash; one should be aware that an actually numeric attribute which happens to be an integer could be coerced into a <code class="reqn">n</code>-level categorical, which would have a perfect mutual information score and would likely become a very disruptive false positive.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>jhScores(iris[,-5],iris[,5])
</code></pre>

<hr>
<h2 id='JIM'>Joint impurity filter</h2><span id='topic+JIM'></span>

<h3>Description</h3>

<p>The method starts with a feature of a maximal impurity gain with the decision <code class="reqn">Y</code>.
Then, it greedily adds feature <code class="reqn">X</code> with a maximal value of the following criterion:
</p>
<p style="text-align: center;"><code class="reqn">J(X)=\sum_{W\in S} G(X,W;Y),</code>
</p>

<p>where <code class="reqn">S</code> is the set of already selected features, and
</p>
<p style="text-align: center;"><code class="reqn">G(X;Y)=\sum_{xy}\frac{p_{xy}^2}{p_x}-\sum_{y} p_y^2</code>
</p>

<p>is the Gini impurity gain from partitioning <code class="reqn">Y</code> according to <code class="reqn">X</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>JIM(X, Y, k = 3, threads = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="JIM_+3A_x">X</code></td>
<td>
<p>Attribute table, given as a data frame with either factors (preferred), booleans, integers (treated as categorical) or reals (which undergo automatic categorisation; see below for details).
Single vector will be interpreted as a data.frame with one column.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="JIM_+3A_y">Y</code></td>
<td>
<p>Decision attribute; should be given as a factor, but other options are accepted, exactly like for attributes.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="JIM_+3A_k">k</code></td>
<td>
<p>Number of attributes to select.
Must not exceed <code>ncol(X)</code>.</p>
</td></tr>
<tr><td><code id="JIM_+3A_threads">threads</code></td>
<td>
<p>Number of threads to use; default value, 0, means all available to OpenMP.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with two elements: <code>selection</code>, a vector of indices of the selected features in the selection order, and <code>score</code>, a vector of corresponding feature scores.
Names of both vectors will correspond to the names of features in <code>X</code>.
Both vectors will be at most of a length <code>k</code>, as the selection may stop sooner, even during initial selection, in which case both vectors will be empty.
</p>


<h3>Note</h3>

<p>This is an impurity-based version of <code><a href="#topic+JMI">JMI</a></code>; expect similar results in slightly shorter time.
</p>
<p>The method requires input to be discrete to use empirical estimators of distribution, and, consequently, information gain or entropy.
To allow smoother user experience, praznik automatically coerces non-factor vectors in inputs, which requires additional time, memory and may yield confusing results &ndash; the best practice is to convert data to factors prior to feeding them in this function.
Real attributes are cut into about 10 equally-spaced bins, following the heuristic often used in literature.
Precise number of cuts depends on the number of objects; namely, it is <code class="reqn">n/3</code>, but never less than 2 and never more than 10.
Integers (which technically are also numeric) are treated as categorical variables (for compatibility with similar software), so in a very different way &ndash; one should be aware that an actually numeric attribute which happens to be an integer could be coerced into a <code class="reqn">n</code>-level categorical, which would have a perfect mutual information score and would likely become a very disruptive false positive.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(MadelonD)
JIM(MadelonD$X,MadelonD$Y,20)
</code></pre>

<hr>
<h2 id='JMI'>Joint mutual information filter</h2><span id='topic+JMI'></span>

<h3>Description</h3>

<p>The method starts with a feature of a maximal mutual information with the decision <code class="reqn">Y</code>.
Then, it greedily adds feature <code class="reqn">X</code> with a maximal value of the following criterion:
</p>
<p style="text-align: center;"><code class="reqn">J(X)=\sum_{W\in S} I(X,W;Y),</code>
</p>

<p>where <code class="reqn">S</code> is the set of already selected features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>JMI(X, Y, k = 3, threads = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="JMI_+3A_x">X</code></td>
<td>
<p>Attribute table, given as a data frame with either factors (preferred), booleans, integers (treated as categorical) or reals (which undergo automatic categorisation; see below for details).
Single vector will be interpreted as a data.frame with one column.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="JMI_+3A_y">Y</code></td>
<td>
<p>Decision attribute; should be given as a factor, but other options are accepted, exactly like for attributes.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="JMI_+3A_k">k</code></td>
<td>
<p>Number of attributes to select.
Must not exceed <code>ncol(X)</code>.</p>
</td></tr>
<tr><td><code id="JMI_+3A_threads">threads</code></td>
<td>
<p>Number of threads to use; default value, 0, means all available to OpenMP.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with two elements: <code>selection</code>, a vector of indices of the selected features in the selection order, and <code>score</code>, a vector of corresponding feature scores.
Names of both vectors will correspond to the names of features in <code>X</code>.
Both vectors will be at most of a length <code>k</code>, as the selection may stop sooner, even during initial selection, in which case both vectors will be empty.
</p>


<h3>Note</h3>

<p><code><a href="#topic+DISR">DISR</a></code> is a normalised version of JMI; <code><a href="#topic+JMIM">JMIM</a></code> and <code><a href="#topic+NJMIM">NJMIM</a></code> are modifications of JMI and DISR in which minimal joint information over already selected features is used instead of a sum.
</p>
<p>The method requires input to be discrete to use empirical estimators of distribution, and, consequently, information gain or entropy.
To allow smoother user experience, praznik automatically coerces non-factor vectors in inputs, which requires additional time, memory and may yield confusing results &ndash; the best practice is to convert data to factors prior to feeding them in this function.
Real attributes are cut into about 10 equally-spaced bins, following the heuristic often used in literature.
Precise number of cuts depends on the number of objects; namely, it is <code class="reqn">n/3</code>, but never less than 2 and never more than 10.
Integers (which technically are also numeric) are treated as categorical variables (for compatibility with similar software), so in a very different way &ndash; one should be aware that an actually numeric attribute which happens to be an integer could be coerced into a <code class="reqn">n</code>-level categorical, which would have a perfect mutual information score and would likely become a very disruptive false positive.
</p>


<h3>References</h3>

<p>&quot;Data Visualization and Feature Selection: New Algorithms for Nongaussian Data&quot; H. Yang and J. Moody, NIPS (1999)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(MadelonD)
JMI(MadelonD$X,MadelonD$Y,20)
</code></pre>

<hr>
<h2 id='JMI3'>Third-order joint mutual information filter</h2><span id='topic+JMI3'></span>

<h3>Description</h3>

<p>The method starts with two features: <code class="reqn">X_1</code> of a maximal mutual information with the decision <code class="reqn">Y</code>, and <code class="reqn">X_2</code> of a maximal value of <code class="reqn">I(X_1,X_2;Y)</code>, as would be selected second by a regular <code><a href="#topic+JMI">JMI</a></code>.
Then, it greedily adds feature <code class="reqn">X</code> with a maximal value of the following criterion:
</p>
<p style="text-align: center;"><code class="reqn">J(X)=\frac{1}{2}\sum_{(U,W)\in S^2; U\neq W} I(X,U,W;Y),</code>
</p>

<p>where <code class="reqn">S</code> is the set of already selected features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>JMI3(X, Y, k = 3, threads = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="JMI3_+3A_x">X</code></td>
<td>
<p>Attribute table, given as a data frame with either factors (preferred), booleans, integers (treated as categorical) or reals (which undergo automatic categorisation; see below for details).
Single vector will be interpreted as a data.frame with one column.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="JMI3_+3A_y">Y</code></td>
<td>
<p>Decision attribute; should be given as a factor, but other options are accepted, exactly like for attributes.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="JMI3_+3A_k">k</code></td>
<td>
<p>Number of attributes to select.
Must not exceed <code>ncol(X)</code>.</p>
</td></tr>
<tr><td><code id="JMI3_+3A_threads">threads</code></td>
<td>
<p>Number of threads to use; default value, 0, means all available to OpenMP.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with two elements: <code>selection</code>, a vector of indices of the selected features in the selection order, and <code>score</code>, a vector of corresponding feature scores.
Names of both vectors will correspond to the names of features in <code>X</code>.
Both vectors will be at most of a length <code>k</code>, as the selection may stop sooner, even during initial selection, in which case both vectors will be empty.
</p>


<h3>Note</h3>

<p>This method has a complexity of <code class="reqn">O(k^2\cdot m \cdot n)</code>, while other filters have <code class="reqn">O(k\cdot m \cdot n)</code> &mdash; for larger <code class="reqn">k</code>, it will be substantially slower.
In the original paper, special shrinkage estimator of MI is used; in praznik, all algorithms use ML estimators, so is <code>JMI3</code>.
</p>
<p>The method requires input to be discrete to use empirical estimators of distribution, and, consequently, information gain or entropy.
To allow smoother user experience, praznik automatically coerces non-factor vectors in inputs, which requires additional time, memory and may yield confusing results &ndash; the best practice is to convert data to factors prior to feeding them in this function.
Real attributes are cut into about 10 equally-spaced bins, following the heuristic often used in literature.
Precise number of cuts depends on the number of objects; namely, it is <code class="reqn">n/3</code>, but never less than 2 and never more than 10.
Integers (which technically are also numeric) are treated as categorical variables (for compatibility with similar software), so in a very different way &ndash; one should be aware that an actually numeric attribute which happens to be an integer could be coerced into a <code class="reqn">n</code>-level categorical, which would have a perfect mutual information score and would likely become a very disruptive false positive.
</p>


<h3>References</h3>

<p>&quot;Efficient feature selection using shrinkage estimators&quot; K. Sechidis, L. Azzimonti, A. Pocock, G. Corani, J. Weatherall and G. Brown. Machine Learning, 108 (8-9), pp. 1261-1286 (2019)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: data(MadelonD)
JMI3(MadelonD$X,MadelonD$Y,20)
## End(Not run)
</code></pre>

<hr>
<h2 id='JMIM'>Minimal joint mutual information maximisation filter</h2><span id='topic+JMIM'></span>

<h3>Description</h3>

<p>The method starts with a feature of a maximal mutual information with the decision <code class="reqn">Y</code>.
Then, it greedily adds feature <code class="reqn">X</code> with a maximal value of the following criterion:
</p>
<p style="text-align: center;"><code class="reqn">J(X)=\min_{W\in S} I(X,W;Y),</code>
</p>

<p>where <code class="reqn">S</code> is the set of already selected features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>JMIM(X, Y, k = 3, threads = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="JMIM_+3A_x">X</code></td>
<td>
<p>Attribute table, given as a data frame with either factors (preferred), booleans, integers (treated as categorical) or reals (which undergo automatic categorisation; see below for details).
Single vector will be interpreted as a data.frame with one column.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="JMIM_+3A_y">Y</code></td>
<td>
<p>Decision attribute; should be given as a factor, but other options are accepted, exactly like for attributes.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="JMIM_+3A_k">k</code></td>
<td>
<p>Number of attributes to select.
Must not exceed <code>ncol(X)</code>.</p>
</td></tr>
<tr><td><code id="JMIM_+3A_threads">threads</code></td>
<td>
<p>Number of threads to use; default value, 0, means all available to OpenMP.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with two elements: <code>selection</code>, a vector of indices of the selected features in the selection order, and <code>score</code>, a vector of corresponding feature scores.
Names of both vectors will correspond to the names of features in <code>X</code>.
Both vectors will be at most of a length <code>k</code>, as the selection may stop sooner, even during initial selection, in which case both vectors will be empty.
</p>


<h3>Note</h3>

<p><code><a href="#topic+NJMIM">NJMIM</a></code> is a normalised version of JMIM; <code><a href="#topic+JMI">JMI</a></code> and <code><a href="#topic+DISR">DISR</a></code> are modifications of JMIM and NJMIM in which a sum of joint information over already selected features is used instead of a minimum.
</p>
<p>The method requires input to be discrete to use empirical estimators of distribution, and, consequently, information gain or entropy.
To allow smoother user experience, praznik automatically coerces non-factor vectors in inputs, which requires additional time, memory and may yield confusing results &ndash; the best practice is to convert data to factors prior to feeding them in this function.
Real attributes are cut into about 10 equally-spaced bins, following the heuristic often used in literature.
Precise number of cuts depends on the number of objects; namely, it is <code class="reqn">n/3</code>, but never less than 2 and never more than 10.
Integers (which technically are also numeric) are treated as categorical variables (for compatibility with similar software), so in a very different way &ndash; one should be aware that an actually numeric attribute which happens to be an integer could be coerced into a <code class="reqn">n</code>-level categorical, which would have a perfect mutual information score and would likely become a very disruptive false positive.
</p>


<h3>References</h3>

<p>&quot;Feature selection using Joint Mutual Information Maximisation&quot; M. Bennasar, Y. Hicks and R. Setchi, (2015)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(MadelonD)
JMIM(MadelonD$X,MadelonD$Y,20)
</code></pre>

<hr>
<h2 id='jmiMatrix'>Joint mutual information matrix</h2><span id='topic+jmiMatrix'></span>

<h3>Description</h3>

<p>Calculates mutual information between each feature and a joint mix of each other feature with a given feature, that is
</p>
<p style="text-align: center;"><code class="reqn">I(X_i;X_j,Z).</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>jmiMatrix(X, Z, zeroDiag = TRUE, threads = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="jmiMatrix_+3A_x">X</code></td>
<td>
<p>Attribute table, given as a data frame with either factors (preferred), booleans, integers (treated as categorical) or reals (which undergo automatic categorisation; see below for details).
Single vector will be interpreted as a data.frame with one column.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="jmiMatrix_+3A_z">Z</code></td>
<td>
<p>Condition; should be given as a factor, but other options are accepted, as for features.</p>
</td></tr>
<tr><td><code id="jmiMatrix_+3A_zerodiag">zeroDiag</code></td>
<td>
<p>Boolean flag, whether the diagonal should be filled with zeroes, or with degenerated scores for two identical copies of a feature.</p>
</td></tr>
<tr><td><code id="jmiMatrix_+3A_threads">threads</code></td>
<td>
<p>Number of threads to use; default value, 0, means all available to OpenMP.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numerical matrix with scores, with row and column names copied from <code>X</code>.
</p>


<h3>Note</h3>

<p>The method requires input to be discrete to use empirical estimators of distribution, and, consequently, information gain or entropy.
To allow smoother user experience, praznik automatically coerces non-factor vectors in inputs, which requires additional time, memory and may yield confusing results &ndash; the best practice is to convert data to factors prior to feeding them in this function.
Real attributes are cut into about 10 equally-spaced bins, following the heuristic often used in literature.
Precise number of cuts depends on the number of objects; namely, it is <code class="reqn">n/3</code>, but never less than 2 and never more than 10.
Integers (which technically are also numeric) are treated as categorical variables (for compatibility with similar software), so in a very different way &ndash; one should be aware that an actually numeric attribute which happens to be an integer could be coerced into a <code class="reqn">n</code>-level categorical, which would have a perfect mutual information score and would likely become a very disruptive false positive.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>jmiMatrix(iris[,-5],iris[,5])
</code></pre>

<hr>
<h2 id='jmiScores'>Joint mutual information scores</h2><span id='topic+jmiScores'></span>

<h3>Description</h3>

<p>Calculates joint mutual information between each feature joint with some other vector <code>Z</code> with the decision, that is
</p>
<p style="text-align: center;"><code class="reqn">I(X,Z;Y).</code>
</p>

<p>This is the same as conditional mutual information between X and Y plus a constant that depends on Y and Z, that is
</p>
<p style="text-align: center;"><code class="reqn">I(X,Z;Y)=I(X;Y|Z)+I(Y;Z).</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>jmiScores(X, Y, Z, threads = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="jmiScores_+3A_x">X</code></td>
<td>
<p>Attribute table, given as a data frame with either factors (preferred), booleans, integers (treated as categorical) or reals (which undergo automatic categorisation; see below for details).
Single vector will be interpreted as a data.frame with one column.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="jmiScores_+3A_y">Y</code></td>
<td>
<p>Decision attribute; should be given as a factor, but other options are accepted, exactly like for attributes.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="jmiScores_+3A_z">Z</code></td>
<td>
<p>Other vector; should be given as a factor, but other options are accepted, as for features.</p>
</td></tr>
<tr><td><code id="jmiScores_+3A_threads">threads</code></td>
<td>
<p>Number of threads to use; default value, 0, means all available to OpenMP.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numerical vector with joint mutual information scores, with names copied from <code>X</code>.
</p>


<h3>Note</h3>

<p>The method requires input to be discrete to use empirical estimators of distribution, and, consequently, information gain or entropy.
To allow smoother user experience, praznik automatically coerces non-factor vectors in inputs, which requires additional time, memory and may yield confusing results &ndash; the best practice is to convert data to factors prior to feeding them in this function.
Real attributes are cut into about 10 equally-spaced bins, following the heuristic often used in literature.
Precise number of cuts depends on the number of objects; namely, it is <code class="reqn">n/3</code>, but never less than 2 and never more than 10.
Integers (which technically are also numeric) are treated as categorical variables (for compatibility with similar software), so in a very different way &ndash; one should be aware that an actually numeric attribute which happens to be an integer could be coerced into a <code class="reqn">n</code>-level categorical, which would have a perfect mutual information score and would likely become a very disruptive false positive.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>jmiScores(iris[,-5],iris$Species,iris$Sepal.Length)
</code></pre>

<hr>
<h2 id='joinf'>Join factors</h2><span id='topic+joinf'></span>

<h3>Description</h3>

<p>Convenience function for joining factors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>joinf(...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="joinf_+3A_...">...</code></td>
<td>
<p>One or more features to merge. 
Given as single vectors or data.frames. 
Accepted feature types are factor (preferred), booleans, integers (treated as categorical) or reals (which undergo automatic categorisation). 
<code>NA</code>s are not allowed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Joint factor, with levels <code>l1</code> to <code>l&lt;n&gt;</code>.
Vacant combinations are dropped.
</p>


<h3>Note</h3>

<p>You can pass a single vector to this function to see how praznik interprets it.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>joinf(c(1,2,1,2),c(1,1,2,2))
</code></pre>

<hr>
<h2 id='kInverse'>Inverse Kendall transform</h2><span id='topic+kInverse'></span>

<h3>Description</h3>

<p>This function attempts to reverse Kendall transformation using a simple ranking agreement method, which always restores original ranking if the input corresponds to one, or a reasonable best-effort guess if not.
Namely, each objects gets a score based on its relation with each other object, 2 points for a win (<code>'&gt;'</code>) and 1 point for a tie (<code>'='</code>); these scores are used to calculate ranks.
This function can also be directly given greater-than scores, for instance confidence scores from some classifier trained on Kendall-transformed data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kInverse(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kInverse_+3A_x">x</code></td>
<td>
<p>A Kendall-transformed feature to be converted back into a ranking.
To be interpreted as a such, it must be a factor with levels being a subset of <code>'&lt;'</code>, <code>'&gt;'</code> or <code>'='</code>.
Alternatively, it may be a numeric vector of greater-than scores.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector of ranks corresponding to <code>x</code>.
</p>


<h3>Note</h3>

<p>An order of elements in <code>x</code> is crucial; if it is not the same as generated by the <code><a href="#topic+kTransform">kTransform</a></code>, results will be wrong.
This function cannot assert that the order is correct.
</p>


<h3>References</h3>

<p>&quot;Kendall transformation brings a robust categorical representation of ordinal data&quot; M.B. Kursa. SciRep 12, 8341 (2022).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>kInverse(kTransform(1:7))
</code></pre>

<hr>
<h2 id='kTransform'>Kendall transformation</h2><span id='topic+kTransform'></span>

<h3>Description</h3>

<p>Kendall transformation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kTransform(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kTransform_+3A_x">x</code></td>
<td>
<p>Vector or data frame to be Kendall-transformed; allowed feature types are numeric, integer (treated as numeric), ordered factor, logical and unordered factor with two or less levels.
<code>NA</code> and non-finite values are allowed; <code>NaN</code> is treated as <code>NA</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A transformed vector or data frame with transformed columns.
</p>


<h3>References</h3>

<p>&quot;Kendall transformation brings a robust categorical representation of ordinal data&quot; M.B. Kursa. SciRep 12, 8341 (2022).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>kTransform(data.frame(Asc=1:3,Desc=3:1,Vsh=c(2,1,2)))
</code></pre>

<hr>
<h2 id='MadelonD'>Pre-discretised Madelon dataset</h2><span id='topic+MadelonD'></span>

<h3>Description</h3>

<p>Madelon is a synthetic data set from the NIPS 2003 feature selection challenge, generated by Isabelle Guyon.
It contains 480 irrelevant and 20 relevant features, including 5 informative and 15 redundant.
In this version, the originally numerical features have been pre-cut into 10 bins, as well as their names have been altered to reveal 20 relevant features (as identified by the Boruta method).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(MadelonD)
</code></pre>


<h3>Format</h3>

<p>A list with two elements, <code>X</code> containing a data frame with predictors, and <code>Y</code>, the decision. 
Features are in the same order as in the original data; the names of relevant ones start with <code>Rel</code>, while of irrelevant ones with <code>Irr</code>.
</p>


<h3>Source</h3>

<p><a href="https://archive.ics.uci.edu/ml/datasets/Madelon">https://archive.ics.uci.edu/ml/datasets/Madelon</a>
</p>

<hr>
<h2 id='maxCmiScores'>Maximal pairwise conditional mutual information scores</h2><span id='topic+maxCmiScores'></span>

<h3>Description</h3>

<p>For each feature, calculates the conditional mutual information between this feature and the decision, 
conditioned on all other features, and returns the maximal value, that is
</p>
<p style="text-align: center;"><code class="reqn">max_j I(X_i;Y|X_j).</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>maxCmiScores(X, Y, threads = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="maxCmiScores_+3A_x">X</code></td>
<td>
<p>Attribute table, given as a data frame with either factors (preferred), booleans, integers (treated as categorical) or reals (which undergo automatic categorisation; see below for details).
Single vector will be interpreted as a data.frame with one column.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="maxCmiScores_+3A_y">Y</code></td>
<td>
<p>Decision attribute; should be given as a factor, but other options are accepted, exactly like for attributes.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="maxCmiScores_+3A_threads">threads</code></td>
<td>
<p>Number of threads to use; default value, 0, means all available to OpenMP.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numerical vector with maximal pairwise conditional mutual information scores, with names copied from <code>X</code>.
</p>


<h3>Note</h3>

<p>The method requires input to be discrete to use empirical estimators of distribution, and, consequently, information gain or entropy.
To allow smoother user experience, praznik automatically coerces non-factor vectors in inputs, which requires additional time, memory and may yield confusing results &ndash; the best practice is to convert data to factors prior to feeding them in this function.
Real attributes are cut into about 10 equally-spaced bins, following the heuristic often used in literature.
Precise number of cuts depends on the number of objects; namely, it is <code class="reqn">n/3</code>, but never less than 2 and never more than 10.
Integers (which technically are also numeric) are treated as categorical variables (for compatibility with similar software), so in a very different way &ndash; one should be aware that an actually numeric attribute which happens to be an integer could be coerced into a <code class="reqn">n</code>-level categorical, which would have a perfect mutual information score and would likely become a very disruptive false positive.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>maxCmiScores(iris[,-5],iris$Species)
</code></pre>

<hr>
<h2 id='maxJmiScores'>Maximal pairwise joint mutual information scores</h2><span id='topic+maxJmiScores'></span>

<h3>Description</h3>

<p>Calculates joint mutual information between each joint feature pair with the decision, and yields maximal value
for each feature, that is
</p>
<p style="text-align: center;"><code class="reqn">max_{j\neq i} I(X_i,X_j;Y).</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>maxJmiScores(X, Y, threads = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="maxJmiScores_+3A_x">X</code></td>
<td>
<p>Attribute table, given as a data frame with either factors (preferred), booleans, integers (treated as categorical) or reals (which undergo automatic categorisation; see below for details).
Single vector will be interpreted as a data.frame with one column.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="maxJmiScores_+3A_y">Y</code></td>
<td>
<p>Decision attribute; should be given as a factor, but other options are accepted, exactly like for attributes.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="maxJmiScores_+3A_threads">threads</code></td>
<td>
<p>Number of threads to use; default value, 0, means all available to OpenMP.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numerical vector with maximal pairwise joint mutual information scores, with names copied from <code>X</code>.
</p>


<h3>Note</h3>

<p>The method requires input to be discrete to use empirical estimators of distribution, and, consequently, information gain or entropy.
To allow smoother user experience, praznik automatically coerces non-factor vectors in inputs, which requires additional time, memory and may yield confusing results &ndash; the best practice is to convert data to factors prior to feeding them in this function.
Real attributes are cut into about 10 equally-spaced bins, following the heuristic often used in literature.
Precise number of cuts depends on the number of objects; namely, it is <code class="reqn">n/3</code>, but never less than 2 and never more than 10.
Integers (which technically are also numeric) are treated as categorical variables (for compatibility with similar software), so in a very different way &ndash; one should be aware that an actually numeric attribute which happens to be an integer could be coerced into a <code class="reqn">n</code>-level categorical, which would have a perfect mutual information score and would likely become a very disruptive false positive.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>maxJmiScores(iris[,-5],iris$Species)
</code></pre>

<hr>
<h2 id='MIM'>Mutual information maximisation filter</h2><span id='topic+MIM'></span>

<h3>Description</h3>

<p>Calculates mutual information between all features and the decision, then returns top k.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MIM(X, Y, k = 3, threads = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="MIM_+3A_x">X</code></td>
<td>
<p>Attribute table, given as a data frame with either factors (preferred), booleans, integers (treated as categorical) or reals (which undergo automatic categorisation; see below for details).
Single vector will be interpreted as a data.frame with one column.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="MIM_+3A_y">Y</code></td>
<td>
<p>Decision attribute; should be given as a factor, but other options are accepted, exactly like for attributes.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="MIM_+3A_k">k</code></td>
<td>
<p>Number of attributes to select.
Must not exceed <code>ncol(X)</code>.</p>
</td></tr>
<tr><td><code id="MIM_+3A_threads">threads</code></td>
<td>
<p>Number of threads to use; default value, 0, means all available to OpenMP.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with two elements: <code>selection</code>, a vector of indices of the selected features in the selection order, and <code>score</code>, a vector of corresponding feature scores.
Names of both vectors will correspond to the names of features in <code>X</code>.
Both vectors will be at most of a length <code>k</code>, as the selection may stop sooner, even during initial selection, in which case both vectors will be empty.
</p>


<h3>Note</h3>

<p>The method requires input to be discrete to use empirical estimators of distribution, and, consequently, information gain or entropy.
To allow smoother user experience, praznik automatically coerces non-factor vectors in inputs, which requires additional time, memory and may yield confusing results &ndash; the best practice is to convert data to factors prior to feeding them in this function.
Real attributes are cut into about 10 equally-spaced bins, following the heuristic often used in literature.
Precise number of cuts depends on the number of objects; namely, it is <code class="reqn">n/3</code>, but never less than 2 and never more than 10.
Integers (which technically are also numeric) are treated as categorical variables (for compatibility with similar software), so in a very different way &ndash; one should be aware that an actually numeric attribute which happens to be an integer could be coerced into a <code class="reqn">n</code>-level categorical, which would have a perfect mutual information score and would likely become a very disruptive false positive.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(MadelonD)
MIM(MadelonD$X,MadelonD$Y,20)
</code></pre>

<hr>
<h2 id='miMatrix'>Mutual information matrix</h2><span id='topic+miMatrix'></span>

<h3>Description</h3>

<p>Calculates mutual information between each two features, that is
</p>
<p style="text-align: center;"><code class="reqn">I(X_i,X_j).</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>miMatrix(X, zeroDiag = TRUE, threads = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="miMatrix_+3A_x">X</code></td>
<td>
<p>Attribute table, given as a data frame with either factors (preferred), booleans, integers (treated as categorical) or reals (which undergo automatic categorisation; see below for details).
Single vector will be interpreted as a data.frame with one column.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="miMatrix_+3A_zerodiag">zeroDiag</code></td>
<td>
<p>Boolean flag, whether the diagonal should be filled with zeroes, or with degenerated scores for two identical copies of a feature.</p>
</td></tr>
<tr><td><code id="miMatrix_+3A_threads">threads</code></td>
<td>
<p>Number of threads to use; default value, 0, means all available to OpenMP.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numerical matrix with scores, with row and column names copied from <code>X</code>.
</p>


<h3>Note</h3>

<p>The method requires input to be discrete to use empirical estimators of distribution, and, consequently, information gain or entropy.
To allow smoother user experience, praznik automatically coerces non-factor vectors in inputs, which requires additional time, memory and may yield confusing results &ndash; the best practice is to convert data to factors prior to feeding them in this function.
Real attributes are cut into about 10 equally-spaced bins, following the heuristic often used in literature.
Precise number of cuts depends on the number of objects; namely, it is <code class="reqn">n/3</code>, but never less than 2 and never more than 10.
Integers (which technically are also numeric) are treated as categorical variables (for compatibility with similar software), so in a very different way &ndash; one should be aware that an actually numeric attribute which happens to be an integer could be coerced into a <code class="reqn">n</code>-level categorical, which would have a perfect mutual information score and would likely become a very disruptive false positive.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>miMatrix(iris)
</code></pre>

<hr>
<h2 id='minCmiScores'>Minimal pairwise conditional mutual information scores</h2><span id='topic+minCmiScores'></span>

<h3>Description</h3>

<p>For each feature, calculates the conditional mutual information between this feature and the decision, 
conditioned on all other features, and returns the minimal value, that is
</p>
<p style="text-align: center;"><code class="reqn">min_j I(X_i;Y|X_j).</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>minCmiScores(X, Y, threads = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="minCmiScores_+3A_x">X</code></td>
<td>
<p>Attribute table, given as a data frame with either factors (preferred), booleans, integers (treated as categorical) or reals (which undergo automatic categorisation; see below for details).
Single vector will be interpreted as a data.frame with one column.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="minCmiScores_+3A_y">Y</code></td>
<td>
<p>Decision attribute; should be given as a factor, but other options are accepted, exactly like for attributes.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="minCmiScores_+3A_threads">threads</code></td>
<td>
<p>Number of threads to use; default value, 0, means all available to OpenMP.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numerical vector with minimal pairwise conditional mutual information scores, with names copied from <code>X</code>.
</p>


<h3>Note</h3>

<p>The method requires input to be discrete to use empirical estimators of distribution, and, consequently, information gain or entropy.
To allow smoother user experience, praznik automatically coerces non-factor vectors in inputs, which requires additional time, memory and may yield confusing results &ndash; the best practice is to convert data to factors prior to feeding them in this function.
Real attributes are cut into about 10 equally-spaced bins, following the heuristic often used in literature.
Precise number of cuts depends on the number of objects; namely, it is <code class="reqn">n/3</code>, but never less than 2 and never more than 10.
Integers (which technically are also numeric) are treated as categorical variables (for compatibility with similar software), so in a very different way &ndash; one should be aware that an actually numeric attribute which happens to be an integer could be coerced into a <code class="reqn">n</code>-level categorical, which would have a perfect mutual information score and would likely become a very disruptive false positive.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>minCmiScores(iris[,-5],iris$Species)
</code></pre>

<hr>
<h2 id='minMaxCmiScores'>Extreme values of pairwise conditional mutual information scores</h2><span id='topic+minMaxCmiScores'></span>

<h3>Description</h3>

<p>For each feature, calculates the conditional mutual information between this feature and the decision, 
conditioned on all other features, and returns extreme values, that is
</p>
<p style="text-align: center;"><code class="reqn">min_j I(X_i;Y|X_j)</code>
</p>

<p>and
</p>
<p style="text-align: center;"><code class="reqn">max_j I(X_i;Y|X_j).</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>minMaxCmiScores(X, Y, threads = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="minMaxCmiScores_+3A_x">X</code></td>
<td>
<p>Attribute table, given as a data frame with either factors (preferred), booleans, integers (treated as categorical) or reals (which undergo automatic categorisation; see below for details).
Single vector will be interpreted as a data.frame with one column.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="minMaxCmiScores_+3A_y">Y</code></td>
<td>
<p>Decision attribute; should be given as a factor, but other options are accepted, exactly like for attributes.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="minMaxCmiScores_+3A_threads">threads</code></td>
<td>
<p>Number of threads to use; default value, 0, means all available to OpenMP.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numerical matrix with minimal (first row) and maximal (second row) pairwise conditional mutual information scores, with names copied from <code>X</code>.
</p>


<h3>Note</h3>

<p>The method requires input to be discrete to use empirical estimators of distribution, and, consequently, information gain or entropy.
To allow smoother user experience, praznik automatically coerces non-factor vectors in inputs, which requires additional time, memory and may yield confusing results &ndash; the best practice is to convert data to factors prior to feeding them in this function.
Real attributes are cut into about 10 equally-spaced bins, following the heuristic often used in literature.
Precise number of cuts depends on the number of objects; namely, it is <code class="reqn">n/3</code>, but never less than 2 and never more than 10.
Integers (which technically are also numeric) are treated as categorical variables (for compatibility with similar software), so in a very different way &ndash; one should be aware that an actually numeric attribute which happens to be an integer could be coerced into a <code class="reqn">n</code>-level categorical, which would have a perfect mutual information score and would likely become a very disruptive false positive.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>minMaxCmiScores(iris[,-5],iris$Species)
</code></pre>

<hr>
<h2 id='miScores'>Mutual information scores</h2><span id='topic+miScores'></span>

<h3>Description</h3>

<p>Calculates mutual information between each feature and the decision, that is
</p>
<p style="text-align: center;"><code class="reqn">I(X,Y).</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>miScores(X, Y, threads = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="miScores_+3A_x">X</code></td>
<td>
<p>Attribute table, given as a data frame with either factors (preferred), booleans, integers (treated as categorical) or reals (which undergo automatic categorisation; see below for details).
Single vector will be interpreted as a data.frame with one column.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="miScores_+3A_y">Y</code></td>
<td>
<p>Decision attribute; should be given as a factor, but other options are accepted, exactly like for attributes.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="miScores_+3A_threads">threads</code></td>
<td>
<p>Number of threads to use; default value, 0, means all available to OpenMP.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numerical vector with mutual information scores, with names copied from <code>X</code>.
</p>


<h3>Note</h3>

<p>The method requires input to be discrete to use empirical estimators of distribution, and, consequently, information gain or entropy.
To allow smoother user experience, praznik automatically coerces non-factor vectors in inputs, which requires additional time, memory and may yield confusing results &ndash; the best practice is to convert data to factors prior to feeding them in this function.
Real attributes are cut into about 10 equally-spaced bins, following the heuristic often used in literature.
Precise number of cuts depends on the number of objects; namely, it is <code class="reqn">n/3</code>, but never less than 2 and never more than 10.
Integers (which technically are also numeric) are treated as categorical variables (for compatibility with similar software), so in a very different way &ndash; one should be aware that an actually numeric attribute which happens to be an integer could be coerced into a <code class="reqn">n</code>-level categorical, which would have a perfect mutual information score and would likely become a very disruptive false positive.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>miScores(iris[,-5],iris$Species)
</code></pre>

<hr>
<h2 id='MRMR'>Minimum redundancy maximal relevancy filter</h2><span id='topic+MRMR'></span>

<h3>Description</h3>

<p>The method starts with a feature of a maximal mutual information with the decision <code class="reqn">Y</code>.
Then, it greedily adds feature <code class="reqn">X</code> with a maximal value of the following criterion:
</p>
<p style="text-align: center;"><code class="reqn">J(X)=I(X;Y)-\frac{1}{|S|}\sum_{W\in S} I(X;W),</code>
</p>

<p>where <code class="reqn">S</code> is the set of already selected features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MRMR(X, Y, k = if (positive) ncol(X) else 3, positive = FALSE, threads = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="MRMR_+3A_x">X</code></td>
<td>
<p>Attribute table, given as a data frame with either factors (preferred), booleans, integers (treated as categorical) or reals (which undergo automatic categorisation; see below for details).
Single vector will be interpreted as a data.frame with one column.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="MRMR_+3A_y">Y</code></td>
<td>
<p>Decision attribute; should be given as a factor, but other options are accepted, exactly like for attributes.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="MRMR_+3A_k">k</code></td>
<td>
<p>Number of attributes to select.
Must not exceed <code>ncol(X)</code>.</p>
</td></tr>
<tr><td><code id="MRMR_+3A_positive">positive</code></td>
<td>
<p>If true, algorithm won't return features with negative scores (i.e., with redundancy term higher than the relevance therm).
In that case, <code>k</code> controls the maximal number of returned features, and is set to 'ncol(X)' by default.</p>
</td></tr>
<tr><td><code id="MRMR_+3A_threads">threads</code></td>
<td>
<p>Number of threads to use; default value, 0, means all available to OpenMP.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with two elements: <code>selection</code>, a vector of indices of the selected features in the selection order, and <code>score</code>, a vector of corresponding feature scores.
Names of both vectors will correspond to the names of features in <code>X</code>.
Both vectors will be at most of a length <code>k</code>, as the selection may stop sooner, even during initial selection, in which case both vectors will be empty.
</p>


<h3>Note</h3>

<p>The method requires input to be discrete to use empirical estimators of distribution, and, consequently, information gain or entropy.
To allow smoother user experience, praznik automatically coerces non-factor vectors in inputs, which requires additional time, memory and may yield confusing results &ndash; the best practice is to convert data to factors prior to feeding them in this function.
Real attributes are cut into about 10 equally-spaced bins, following the heuristic often used in literature.
Precise number of cuts depends on the number of objects; namely, it is <code class="reqn">n/3</code>, but never less than 2 and never more than 10.
Integers (which technically are also numeric) are treated as categorical variables (for compatibility with similar software), so in a very different way &ndash; one should be aware that an actually numeric attribute which happens to be an integer could be coerced into a <code class="reqn">n</code>-level categorical, which would have a perfect mutual information score and would likely become a very disruptive false positive.
</p>


<h3>References</h3>

<p>&quot;Feature Selection Based on Mutual Information: Criteria of Max-Dependency, Max-Relevance, and Min-Redundancy&quot; H. Peng et al. IEEE Pattern Analysis and Machine Intelligence (PAMI) (2005)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(MadelonD)
MRMR(MadelonD$X,MadelonD$Y,20)
</code></pre>

<hr>
<h2 id='NJMIM'>Minimal normalised joint mutual information maximisation filter</h2><span id='topic+NJMIM'></span>

<h3>Description</h3>

<p>The method starts with a feature of a maximal mutual information with the decision <code class="reqn">Y</code>.
Then, it greedily adds feature <code class="reqn">X</code> with a maximal value of the following criterion:
</p>
<p style="text-align: center;"><code class="reqn">J(X)=\min_{W\in S} \frac{I(X,W;Y)}{H(X,W,Y)},</code>
</p>

<p>where <code class="reqn">S</code> is the set of already selected features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NJMIM(X, Y, k = 3, threads = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="NJMIM_+3A_x">X</code></td>
<td>
<p>Attribute table, given as a data frame with either factors (preferred), booleans, integers (treated as categorical) or reals (which undergo automatic categorisation; see below for details).
Single vector will be interpreted as a data.frame with one column.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="NJMIM_+3A_y">Y</code></td>
<td>
<p>Decision attribute; should be given as a factor, but other options are accepted, exactly like for attributes.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="NJMIM_+3A_k">k</code></td>
<td>
<p>Number of attributes to select.
Must not exceed <code>ncol(X)</code>.</p>
</td></tr>
<tr><td><code id="NJMIM_+3A_threads">threads</code></td>
<td>
<p>Number of threads to use; default value, 0, means all available to OpenMP.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with two elements: <code>selection</code>, a vector of indices of the selected features in the selection order, and <code>score</code>, a vector of corresponding feature scores.
Names of both vectors will correspond to the names of features in <code>X</code>.
Both vectors will be at most of a length <code>k</code>, as the selection may stop sooner, even during initial selection, in which case both vectors will be empty.
</p>


<h3>Note</h3>

<p>NJMIM is a normalised version of <code><a href="#topic+JMIM">JMIM</a></code>; <code><a href="#topic+JMI">JMI</a></code> and <code><a href="#topic+DISR">DISR</a></code> are modifications of JMIM and NJMIM in which a sum of joint information over already selected features is used instead of a minimum.
It stops returning features when the best score reaches 0.
</p>
<p>The method requires input to be discrete to use empirical estimators of distribution, and, consequently, information gain or entropy.
To allow smoother user experience, praznik automatically coerces non-factor vectors in inputs, which requires additional time, memory and may yield confusing results &ndash; the best practice is to convert data to factors prior to feeding them in this function.
Real attributes are cut into about 10 equally-spaced bins, following the heuristic often used in literature.
Precise number of cuts depends on the number of objects; namely, it is <code class="reqn">n/3</code>, but never less than 2 and never more than 10.
Integers (which technically are also numeric) are treated as categorical variables (for compatibility with similar software), so in a very different way &ndash; one should be aware that an actually numeric attribute which happens to be an integer could be coerced into a <code class="reqn">n</code>-level categorical, which would have a perfect mutual information score and would likely become a very disruptive false positive.
</p>


<h3>References</h3>

<p>&quot;Feature selection using Joint Mutual Information Maximisation&quot; M. Bennasar, Y. Hicks and R. Setchi, (2015)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(MadelonD)
NJMIM(MadelonD$X,MadelonD$Y,20)
</code></pre>

<hr>
<h2 id='njmiMatrix'>Normalised joint mutual information matrix</h2><span id='topic+njmiMatrix'></span>

<h3>Description</h3>

<p>Calculates normalised mutual information between each feature and a joint mix of each other feature with a given feature, that is
</p>
<p style="text-align: center;"><code class="reqn">\frac{I(X_i;X_j,Z)}{H(X_i,X_j,Z)}.</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>njmiMatrix(X, Z, zeroDiag = TRUE, threads = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="njmiMatrix_+3A_x">X</code></td>
<td>
<p>Attribute table, given as a data frame with either factors (preferred), booleans, integers (treated as categorical) or reals (which undergo automatic categorisation; see below for details).
Single vector will be interpreted as a data.frame with one column.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="njmiMatrix_+3A_z">Z</code></td>
<td>
<p>Condition; should be given as a factor, but other options are accepted, as for features.</p>
</td></tr>
<tr><td><code id="njmiMatrix_+3A_zerodiag">zeroDiag</code></td>
<td>
<p>Boolean flag, whether the diagonal should be filled with zeroes, or with degenerated scores for two identical copies of a feature.</p>
</td></tr>
<tr><td><code id="njmiMatrix_+3A_threads">threads</code></td>
<td>
<p>Number of threads to use; default value, 0, means all available to OpenMP.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numerical matrix with scores, with row and column names copied from <code>X</code>.
</p>


<h3>Note</h3>

<p>The method requires input to be discrete to use empirical estimators of distribution, and, consequently, information gain or entropy.
To allow smoother user experience, praznik automatically coerces non-factor vectors in inputs, which requires additional time, memory and may yield confusing results &ndash; the best practice is to convert data to factors prior to feeding them in this function.
Real attributes are cut into about 10 equally-spaced bins, following the heuristic often used in literature.
Precise number of cuts depends on the number of objects; namely, it is <code class="reqn">n/3</code>, but never less than 2 and never more than 10.
Integers (which technically are also numeric) are treated as categorical variables (for compatibility with similar software), so in a very different way &ndash; one should be aware that an actually numeric attribute which happens to be an integer could be coerced into a <code class="reqn">n</code>-level categorical, which would have a perfect mutual information score and would likely become a very disruptive false positive.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>njmiMatrix(iris[,-5],iris[,5])
</code></pre>

<hr>
<h2 id='njmiScores'>Normalised joint mutual information scores</h2><span id='topic+njmiScores'></span>

<h3>Description</h3>

<p>Calculated normalised mutual information between each feature joint with some other vector <code>Z</code> and the decision, that is
</p>
<p style="text-align: center;"><code class="reqn">\frac{I(X,Z;Y)}{H(X,Y,Z)}.</code>
</p>

<p>This is the same as in the criterion used by <code><a href="#topic+DISR">DISR</a></code> and <code><a href="#topic+NJMIM">NJMIM</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>njmiScores(X, Y, Z, threads = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="njmiScores_+3A_x">X</code></td>
<td>
<p>Attribute table, given as a data frame with either factors (preferred), booleans, integers (treated as categorical) or reals (which undergo automatic categorisation; see below for details).
Single vector will be interpreted as a data.frame with one column.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="njmiScores_+3A_y">Y</code></td>
<td>
<p>Decision attribute; should be given as a factor, but other options are accepted, exactly like for attributes.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="njmiScores_+3A_z">Z</code></td>
<td>
<p>Other vector; should be given as a factor, but other options are accepted, as for features.</p>
</td></tr>
<tr><td><code id="njmiScores_+3A_threads">threads</code></td>
<td>
<p>Number of threads to use; default value, 0, means all available to OpenMP.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numerical vector with the normalised joint mutual information scores, with names copied from <code>X</code>.
</p>


<h3>Note</h3>

<p>The method requires input to be discrete to use empirical estimators of distribution, and, consequently, information gain or entropy.
To allow smoother user experience, praznik automatically coerces non-factor vectors in inputs, which requires additional time, memory and may yield confusing results &ndash; the best practice is to convert data to factors prior to feeding them in this function.
Real attributes are cut into about 10 equally-spaced bins, following the heuristic often used in literature.
Precise number of cuts depends on the number of objects; namely, it is <code class="reqn">n/3</code>, but never less than 2 and never more than 10.
Integers (which technically are also numeric) are treated as categorical variables (for compatibility with similar software), so in a very different way &ndash; one should be aware that an actually numeric attribute which happens to be an integer could be coerced into a <code class="reqn">n</code>-level categorical, which would have a perfect mutual information score and would likely become a very disruptive false positive.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>njmiScores(iris[,-5],iris$Species,iris$Sepal.Length)
</code></pre>

<hr>
<h2 id='nmiMatrix'>Normalised mutual information matrix</h2><span id='topic+nmiMatrix'></span>

<h3>Description</h3>

<p>Calculates normalised mutual information between each two features, that is
</p>
<p style="text-align: center;"><code class="reqn">\frac{I(X_i,X_j)}{H(X_i,X_j)}.</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>nmiMatrix(X, zeroDiag = TRUE, threads = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nmiMatrix_+3A_x">X</code></td>
<td>
<p>Attribute table, given as a data frame with either factors (preferred), booleans, integers (treated as categorical) or reals (which undergo automatic categorisation; see below for details).
Single vector will be interpreted as a data.frame with one column.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="nmiMatrix_+3A_zerodiag">zeroDiag</code></td>
<td>
<p>Boolean flag, whether the diagonal should be filled with zeroes, or with degenerated scores for two identical copies of a feature.</p>
</td></tr>
<tr><td><code id="nmiMatrix_+3A_threads">threads</code></td>
<td>
<p>Number of threads to use; default value, 0, means all available to OpenMP.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numerical matrix with scores, with row and column names copied from <code>X</code>.
</p>


<h3>Note</h3>

<p>The method requires input to be discrete to use empirical estimators of distribution, and, consequently, information gain or entropy.
To allow smoother user experience, praznik automatically coerces non-factor vectors in inputs, which requires additional time, memory and may yield confusing results &ndash; the best practice is to convert data to factors prior to feeding them in this function.
Real attributes are cut into about 10 equally-spaced bins, following the heuristic often used in literature.
Precise number of cuts depends on the number of objects; namely, it is <code class="reqn">n/3</code>, but never less than 2 and never more than 10.
Integers (which technically are also numeric) are treated as categorical variables (for compatibility with similar software), so in a very different way &ndash; one should be aware that an actually numeric attribute which happens to be an integer could be coerced into a <code class="reqn">n</code>-level categorical, which would have a perfect mutual information score and would likely become a very disruptive false positive.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>nmiMatrix(iris)
</code></pre>

<hr>
<h2 id='triScores'>Mutual information of feature triples</h2><span id='topic+triScores'></span>

<h3>Description</h3>

<p>Calculates mutual information of each triple of features, that is
</p>
<p style="text-align: center;"><code class="reqn">I(X_i;X_j;X_k).</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>triScores(X, threads = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="triScores_+3A_x">X</code></td>
<td>
<p>Attribute table, given as a data frame with either factors (preferred), booleans, integers (treated as categorical) or reals (which undergo automatic categorisation; see below for details).
Single vector will be interpreted as a data.frame with one column.
<code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="triScores_+3A_threads">threads</code></td>
<td>
<p>Number of threads to use; default value, 0, means all available to OpenMP.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with four columns; first three (<code>Var1</code>, <code>Var2</code> and <code>Var3</code>) are names of features, fourth, <code>MI</code> is the value of the mutual information.
The order of features does not matter, hence only </p>
<p style="text-align: center;"><code class="reqn">n(n-1)(n-2)/6</code>
</p>
<p> unique, sorted triples are evaluated.
</p>


<h3>Note</h3>

<p>The method requires input to be discrete to use empirical estimators of distribution, and, consequently, information gain or entropy.
To allow smoother user experience, praznik automatically coerces non-factor vectors in inputs, which requires additional time, memory and may yield confusing results &ndash; the best practice is to convert data to factors prior to feeding them in this function.
Real attributes are cut into about 10 equally-spaced bins, following the heuristic often used in literature.
Precise number of cuts depends on the number of objects; namely, it is <code class="reqn">n/3</code>, but never less than 2 and never more than 10.
Integers (which technically are also numeric) are treated as categorical variables (for compatibility with similar software), so in a very different way &ndash; one should be aware that an actually numeric attribute which happens to be an integer could be coerced into a <code class="reqn">n</code>-level categorical, which would have a perfect mutual information score and would likely become a very disruptive false positive.
</p>
<p>In a current version, the maximal number of features accepted is 2345, which gives a bit less than 2^32 triples.
The equation used for calculation is
</p>
<p style="text-align: center;"><code class="reqn">I(X_i;X_j;X_k)=I(X_i;X_k)+I(X_j;X_k)-I(X_i,X_j;X_k).</code>
</p>

<p>Henceforth, please mind that rounding errors may occur and influence reproducibility.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>triScores(iris)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
