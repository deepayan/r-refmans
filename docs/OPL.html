<!DOCTYPE html><html lang="en"><head><title>Help for package OPL</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {OPL}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#make_cate'><p>Function to calculate the Causal Treatment Effect</p></a></li>
<li><a href='#OPL'><p>OPL: Optimal Policy Learning Package</p></a></li>
<li><a href='#opl_dt_c'><p>Optimal Policy Learning with Decision Tree</p></a></li>
<li><a href='#opl_dt_max_choice'><p>User selection on multiple choice</p></a></li>
<li><a href='#opl_lc_c'><p>Linear Combination Based Policy Learning</p></a></li>
<li><a href='#opl_tb_c'><p>Threshold-based policy learning at specific values</p></a></li>
<li><a href='#overlapping'><p>Testing overlap between old and new policy sample</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Optimal Policy Learning</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.2</td>
</tr>
<tr>
<td>Author:</td>
<td>Federico Brogi [aut, cre],
  Barbara Guardabascio [aut],
  Giovanni Cerulli [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Federico Brogi &lt;federicobrogi@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides functions for optimal policy learning in socioeconomic applications helping users to learn the most effective policies based 
	on data in order to maximize empirical welfare. Specifically, 'OPL' allows to find "treatment assignment rules" that maximize the overall 
	welfare, defined as the sum  of the policy effects estimated over all the policy beneficiaries. Documentation about 'OPL' is provided by  
	several international articles via Athey et al (2021, &lt;<a href="https://doi.org/10.3982%2FECTA15732">doi:10.3982/ECTA15732</a>&gt;), Kitagawa et al (2018, &lt;<a href="https://doi.org/10.3982%2FECTA13288">doi:10.3982/ECTA13288</a>&gt;),
        Cerulli (2022, &lt;<a href="https://doi.org/10.1080%2F13504851.2022.2032577">doi:10.1080/13504851.2022.2032577</a>&gt;), the paper by Cerulli (2021, &lt;<a href="https://doi.org/10.1080%2F13504851.2020.1820939">doi:10.1080/13504851.2020.1820939</a>&gt;) 
	and the book by Gareth et al (2013, &lt;<a href="https://doi.org/10.1007%2F978-1-4614-7138-7">doi:10.1007/978-1-4614-7138-7</a>&gt;).</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, dplyr, ggplot2, pander, randomForest, tidyr</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-02-27 12:47:38 UTC; UTENTE</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-02-27 13:00:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='make_cate'>Function to calculate the Causal Treatment Effect</h2><span id='topic+make_cate'></span>

<h3>Description</h3>

<p>Predicting conditional average treatment effect (CATE) on a new policy based on the training over an old policy
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make_cate(
  model,
  train_data,
  test_data,
  w,
  x,
  y,
  family = gaussian(),
  ntree = 100,
  mtry = 2,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="make_cate_+3A_model">model</code></td>
<td>
<p>A <code>model</code> object used for estimation.</p>
</td></tr>
<tr><td><code id="make_cate_+3A_train_data">train_data</code></td>
<td>
<p>The training dataset.</p>
</td></tr>
<tr><td><code id="make_cate_+3A_test_data">test_data</code></td>
<td>
<p>The test dataset.</p>
</td></tr>
<tr><td><code id="make_cate_+3A_w">w</code></td>
<td>
<p>Set the treatment variable.</p>
</td></tr>
<tr><td><code id="make_cate_+3A_x">x</code></td>
<td>
<p>set Independent variables for the model.</p>
</td></tr>
<tr><td><code id="make_cate_+3A_y">y</code></td>
<td>
<p>Set the outcome variable.</p>
</td></tr>
<tr><td><code id="make_cate_+3A_family">family</code></td>
<td>
<p>The family type for the model (e.g., 'binomial').</p>
</td></tr>
<tr><td><code id="make_cate_+3A_ntree">ntree</code></td>
<td>
<p>Number of trees for the Random Forest model.</p>
</td></tr>
<tr><td><code id="make_cate_+3A_mtry">mtry</code></td>
<td>
<p>Number of variables to consider at each tree split in the Random Forest model.</p>
</td></tr>
<tr><td><code id="make_cate_+3A_verbose">verbose</code></td>
<td>
<p>Set TRUE to print the output on the console.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object containing the estimated causal treatment effect results.
</p>


<h3>References</h3>


<ul>
<li><p> Athey, S., and Wager S. 2021. Policy Learning with Observational Data, Econometrica, 89, 1, 133–161.
</p>
</li>
<li><p> Cerulli, G. 2021. Improving econometric prediction by machine learning, Applied Economics Letters, 28, 16, 1419-1425.
</p>
</li>
<li><p> Cerulli, G. 2022. Optimal treatment assignment of a threshold-based policy: empirical protocol and related issues, Applied Economics Letters, DOI: 10.1080/13504851.2022.2032577.
</p>
</li>
<li><p> Gareth, J., Witten, D., Hastie, D.T., Tibshirani, R. 2013. An Introduction to Statistical Learning : with Applications in R. New York, Springer.
</p>
</li>
<li><p> Kitagawa, T., and A. Tetenov. 2018. Who Should Be Treated? Empirical Welfare Maximization Methods for Treatment Choice, Econometrica, 86, 2, 591–616.
</p>
</li></ul>


<hr>
<h2 id='OPL'>OPL: Optimal Policy Learning Package</h2><span id='topic+OPL'></span>

<h3>Description</h3>

<p>The OPL package provides tools for estimating and optimizing policy assignment rules
based on machine learning and econometric techniques.
</p>


<h3>Main functions</h3>


<ul>
<li> <p><code>make_cate()</code>: Computes conditional average treatment effects.
</p>
</li>
<li> <p><code>opl_tb()</code>: Optimal policy learning for threshold-based policies.
</p>
</li>
<li> <p><code>opl_lc()</code>: Optimal policy learning for linear combination policies.
</p>
</li>
<li> <p><code>opl_dt()</code>: Optimal policy learning for decision tree-based policies.
</p>
</li></ul>



<h3>Installation</h3>

<p>To install the package from CRAN, use:
<code>install.packages("OPL")</code>
</p>


<h3>Acknowledgments</h3>

<p>The development of this software was supported by FOSSR (Fostering Open Science in Social Science Research), a project funded by the European Union - NextGenerationEU under the NPRR Grant agreement n. MURIR0000008.
</p>

<hr>
<h2 id='opl_dt_c'>Optimal Policy Learning with Decision Tree</h2><span id='topic+opl_dt_c'></span>

<h3>Description</h3>

<p>Implementing ex-ante treatment assignment using as policy class a 2-layer fixed-depth decision-tree at specific splitting variables and threshold values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>opl_dt_c(make_cate_result, z, w, c1 = NA, c2 = NA, c3 = NA, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="opl_dt_c_+3A_make_cate_result">make_cate_result</code></td>
<td>
<p>A data frame resulting from the <code>make_cate</code> function, containing the predicted treatment effects (<code>my_cate</code>) and other variables for treatment assignment.</p>
</td></tr>
<tr><td><code id="opl_dt_c_+3A_z">z</code></td>
<td>
<p>A character vector containing the names of the variables used for treatment assignment.</p>
</td></tr>
<tr><td><code id="opl_dt_c_+3A_w">w</code></td>
<td>
<p>A string representing the treatment indicator variable name.</p>
</td></tr>
<tr><td><code id="opl_dt_c_+3A_c1">c1</code></td>
<td>
<p>Value of the threshold value c1 for the first splitting variable. This number must be chosen between 0 and 1.</p>
</td></tr>
<tr><td><code id="opl_dt_c_+3A_c2">c2</code></td>
<td>
<p>Value of the threshold value c2 for the second splitting variable. This number must be chosen between 0 and 1.</p>
</td></tr>
<tr><td><code id="opl_dt_c_+3A_c3">c3</code></td>
<td>
<p>Value of the threshold value c3 for the third splitting variable. This number must be chosen between 0 and 1.</p>
</td></tr>
<tr><td><code id="opl_dt_c_+3A_verbose">verbose</code></td>
<td>
<p>Set TRUE to print the output on the console.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing:
</p>

<ul>
<li> <p><code>W_opt_constr</code>: The maximum average constrained welfare.
</p>
</li>
<li> <p><code>W_opt_unconstr</code>: The average unconstrained welfare.
</p>
</li>
<li> <p><code>units_to_be_treated</code>: A data frame of the units to be treated based on the optimal policy.
</p>
</li>
<li><p> A plot showing the optimal policy assignment.
</p>
</li></ul>



<h3>References</h3>


<ul>
<li><p> Athey, S., and Wager S. 2021. Policy Learning with Observational Data, Econometrica, 89, 1, 133–161.
</p>
</li>
<li><p> Cerulli, G. 2021. Improving econometric prediction by machine learning, Applied Economics Letters, 28, 16, 1419-1425.
</p>
</li>
<li><p> Cerulli, G. 2022. Optimal treatment assignment of a threshold-based policy: empirical protocol and related issues, Applied Economics Letters, DOI: 10.1080/13504851.2022.2032577.
</p>
</li>
<li><p> Gareth, J., Witten, D., Hastie, D.T., Tibshirani, R. 2013. An Introduction to Statistical Learning : with Applications in R. New York, Springer.
</p>
</li>
<li><p> Kitagawa, T., and A. Tetenov. 2018. Who Should Be Treated? Empirical Welfare Maximization Methods for Treatment Choice, Econometrica, 86, 2, 591–616.
</p>
</li></ul>


<hr>
<h2 id='opl_dt_max_choice'>User selection on multiple choice</h2><span id='topic+opl_dt_max_choice'></span>

<h3>Description</h3>

<p>Function that allows the user to select a row of maximum welfare among the rows with maximum welfare constrained. The function prints out the result and requires user input to select the row.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>opl_dt_max_choice(nc, col_max, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="opl_dt_max_choice_+3A_nc">nc</code></td>
<td>
<p>Numeber of max welfare.</p>
</td></tr>
<tr><td><code id="opl_dt_max_choice_+3A_col_max">col_max</code></td>
<td>
<p>Row index for max constrained welfare.</p>
</td></tr>
<tr><td><code id="opl_dt_max_choice_+3A_verbose">verbose</code></td>
<td>
<p>Set TRUE to print the output on the console.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Return the user's selection as an input.
</p>

<hr>
<h2 id='opl_lc_c'>Linear Combination Based Policy Learning</h2><span id='topic+opl_lc_c'></span>

<h3>Description</h3>

<p>Implementing ex-ante treatment assignment using as policy class a linear-combination approach at specific parameters' values c1, c2, and c3 for the linear-combination of variables var1 and var2: c1<em>var1+c2</em>var2&gt;=c3.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>opl_lc_c(make_cate_result, z, w, c1 = NA, c2 = NA, c3 = NA, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="opl_lc_c_+3A_make_cate_result">make_cate_result</code></td>
<td>
<p>A data frame containing the input data. It must include
a column named <code>my_cate</code> representing conditional average treatment effects (CATE) generated using make_cate function.</p>
</td></tr>
<tr><td><code id="opl_lc_c_+3A_z">z</code></td>
<td>
<p>A character vector of length 2 specifying the column names of the two
threshold variables to be standardized.</p>
</td></tr>
<tr><td><code id="opl_lc_c_+3A_w">w</code></td>
<td>
<p>A character string specifying the column name indicating treatment assignment (binary variable).</p>
</td></tr>
<tr><td><code id="opl_lc_c_+3A_c1">c1</code></td>
<td>
<p>Threshold for var1 given by the user or optimized by the the function. This number must be chosen between 0 and 1.</p>
</td></tr>
<tr><td><code id="opl_lc_c_+3A_c2">c2</code></td>
<td>
<p>Threshold for var2 given by the user or optimized by the the function. This number must be chosen between 0 and 1.</p>
</td></tr>
<tr><td><code id="opl_lc_c_+3A_c3">c3</code></td>
<td>
<p>Third parameter of the linear-combination. This number must be chosen between 0 and 1.</p>
</td></tr>
<tr><td><code id="opl_lc_c_+3A_verbose">verbose</code></td>
<td>
<p>Set TRUE to print the output on the console.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function performs the following steps:
</p>

<ul>
<li><p> Standardizes the threshold variables using a min-max scaling technique.
</p>
</li>
<li><p> Determines the optimal treatment assignment based on the linear
combination of the threshold variables.
</p>
</li>
<li><p> Performs a grid search to estimate the optimal policy.
</p>
</li>
<li><p> Outputs a plot visualizing the optimal treatment assignments.
</p>
</li>
<li><p> Prints the main results, including the percentage of treated units,
the unconstrained and constrained welfare, and the policy parameters.
</p>
</li></ul>



<h3>Value</h3>

<p>The function returns a data frame containing the standardized variables
and treatment assignments, and prints a summary of the results and a plot
showing the optimal policy assignment.
</p>


<h3>References</h3>


<ul>
<li><p> Athey, S., and Wager S. 2021. Policy Learning with Observational Data, Econometrica, 89, 1, 133–161.
</p>
</li>
<li><p> Cerulli, G. 2021. Improving econometric prediction by machine learning, Applied Economics Letters, 28, 16, 1419-1425.
</p>
</li>
<li><p> Cerulli, G. 2022. Optimal treatment assignment of a threshold-based policy: empirical protocol and related issues, Applied Economics Letters, DOI: 10.1080/13504851.2022.2032577.
</p>
</li>
<li><p> Gareth, J., Witten, D., Hastie, D.T., Tibshirani, R. 2013. An Introduction to Statistical Learning : with Applications in R. New York, Springer.
</p>
</li>
<li><p> Kitagawa, T., and A. Tetenov. 2018. Who Should Be Treated? Empirical Welfare Maximization Methods for Treatment Choice, Econometrica, 86, 2, 591–616.
</p>
</li></ul>


<hr>
<h2 id='opl_tb_c'>Threshold-based policy learning at specific values</h2><span id='topic+opl_tb_c'></span>

<h3>Description</h3>

<p>Implementing ex-ante treatment assignment using as policy class a threshold-based (or quadrant)
approach at specific threshold values c1 and c2 for respectively the selection variables var1 and var2.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>opl_tb_c(make_cate_result, z, w, c1 = NA, c2 = NA, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="opl_tb_c_+3A_make_cate_result">make_cate_result</code></td>
<td>
<p>A data frame containing the input data. It must include
a column named <code>my_cate</code> representing conditional average treatment effects (CATE) generated using make_cate function.</p>
</td></tr>
<tr><td><code id="opl_tb_c_+3A_z">z</code></td>
<td>
<p>A character vector of length 2 specifying the column names of the two
threshold variables to be standardized.</p>
</td></tr>
<tr><td><code id="opl_tb_c_+3A_w">w</code></td>
<td>
<p>A character string specifying the column name indicating treatment assignment (binary variable).</p>
</td></tr>
<tr><td><code id="opl_tb_c_+3A_c1">c1</code></td>
<td>
<p>Threshold for var1 given by the user or optimized by the the function. This number must be chosen between 0 and 1.</p>
</td></tr>
<tr><td><code id="opl_tb_c_+3A_c2">c2</code></td>
<td>
<p>Threshold for var2 given by the user or optimized by the the function. This number must be chosen between 0 and 1.</p>
</td></tr>
<tr><td><code id="opl_tb_c_+3A_verbose">verbose</code></td>
<td>
<p>Set TRUE to print the output on the console.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function:
</p>

<ol>
<li><p> Standardizes the threshold variables to a 0-1 range.
</p>
</li>
<li><p> Identifies the optimal thresholds based on grid search for maximizing constrained welfare.
</p>
</li>
<li><p> Computes and displays key statistics, including average welfare measures and the percentage of treated units.
</p>
</li></ol>



<h3>Value</h3>

<p>The function invisibly returns the input data frame augmented with the following columns:
</p>

<ul>
<li> <p><code>z[1]_std</code>: Standardized version of the first threshold variable.
</p>
</li>
<li> <p><code>z[2]_std</code>: Standardized version of the second threshold variable.
</p>
</li>
<li> <p><code>units_to_be_treated</code>: Binary indicator for whether a unit should be treated based on the optimal policy.
</p>
</li></ul>

<p>Additionally, the function:
</p>

<ul>
<li><p> Prints the main results summary, including optimal threshold values, average constrained and unconstrained welfare, and treatment proportions.
</p>
</li>
<li><p> Displays a scatter plot visualizing the policy assignment.
</p>
</li></ul>



<h3>References</h3>


<ul>
<li><p> Athey, S., and Wager S. 2021. Policy Learning with Observational Data, Econometrica, 89, 1, 133–161.
</p>
</li>
<li><p> Cerulli, G. 2021. Improving econometric prediction by machine learning, Applied Economics Letters, 28, 16, 1419-1425.
</p>
</li>
<li><p> Cerulli, G. 2022. Optimal treatment assignment of a threshold-based policy: empirical protocol and related issues, Applied Economics Letters, DOI: 10.1080/13504851.2022.2032577.
</p>
</li>
<li><p> Gareth, J., Witten, D., Hastie, D.T., Tibshirani, R. 2013. An Introduction to Statistical Learning : with Applications in R. New York, Springer.
</p>
</li>
<li><p> Kitagawa, T., and A. Tetenov. 2018. Who Should Be Treated? Empirical Welfare Maximization Methods for Treatment Choice, Econometrica, 86, 2, 591–616.
</p>
</li></ul>


<hr>
<h2 id='overlapping'>Testing overlap between old and new policy sample</h2><span id='topic+overlapping'></span>

<h3>Description</h3>

<p>Function to perform overlap analysis between train and test datasets.
The function performs principal component analysis (PCA) on the covariates for both sets
and calculates the Kolmogorov-Smirnov test for overlap.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>overlapping(train_data, test_data, x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="overlapping_+3A_train_data">train_data</code></td>
<td>
<p>Train Dataset indicating the old policy sample.</p>
</td></tr>
<tr><td><code id="overlapping_+3A_test_data">test_data</code></td>
<td>
<p>Test Dataset indicating the new policy sample.</p>
</td></tr>
<tr><td><code id="overlapping_+3A_x">x</code></td>
<td>
<p>Vector of predictor variables.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function prints the superposition graph and the results of the Kolmogorov-Smirnov test.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
