<!DOCTYPE html><html><head><title>Help for package QCA</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {QCA}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#About+20the+20QCA+20package'>
<p>QCA: A Package for Qualitative Comparative Analysis</p></a></li>
<li><a href='#_Cebotari+20and+20Vink'><p>Ethnic protest in Europe</p></a></li>
<li><a href='#_Hino'><p>Time-Difference</p></a></li>
<li><a href='#_Legacy+20datasets'><p>Legacy datasets</p></a></li>
<li><a href='#_Lipset'><p>Lipset's indicators for the survival of democracy during the inter-war period.</p></a></li>
<li><a href='#_Nieuwbeerta'><p>Class voting in post-World War era</p></a></li>
<li><a href='#_Ragin+20and+20Strand'><p>University recognition of a graduate student union</p></a></li>
<li><a href='#calibrate'><p>Calibrate raw data to crisp or fuzzy sets</p></a></li>
<li><a href='#causalChain'><p>Perform CNA - coincidence analysis using QCA</p></a></li>
<li><a href='#complexity'><p>Number of combinations at a given complexity layer</p></a></li>
<li><a href='#findRows'><p>Find untenable configurations</p></a></li>
<li><a href='#findTh'><p>Find calibration thresholds</p></a></li>
<li><a href='#fuzzyand+2C+20fuzzyor'><p>Logical operations</p></a></li>
<li><a href='#generate'><p>Generate a custom data structure</p></a></li>
<li><a href='#Implicant+20matrix+20functions+3A+20allExpressions+2C+20createMatrix+2C+20getRow'><p>Functions Related to the Implicant Matrix</p></a></li>
<li><a href='#minimize'><p>Minimize a truth table</p></a></li>
<li><a href='#modelFit'><p>Theory evaluation</p></a></li>
<li><a href='#Parameters+20of+20fit'><p>Calculate parameters of fit</p></a></li>
<li><a href='#PI+20chart+20functions+3A+20makeChart+2C+20findmin+2C+20solveChart'><p>Create and solve a prime implicants chart</p></a></li>
<li><a href='#QCA+20internal+20functions'><p>Internal Functions</p></a></li>
<li><a href='#retention'><p>Compute the retention probability of a csQCA solution</p></a></li>
<li><a href='#runGUI'><p>run the GUI shiny app for the QCA package</p></a></li>
<li><a href='#superSubset+2C+20findSubsets+2C+20findSupersets'><p>Functions to find subsets or supersets</p></a></li>
<li><a href='#truthTable'><p>Create a truth table</p></a></li>
<li><a href='#Xplot'><p>Display the distribution of points for a single condition</p></a></li>
<li><a href='#XYplot'><p>Create an XY plot</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>3.22</td>
</tr>
<tr>
<td>Title:</td>
<td>Qualitative Comparative Analysis</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6.0), admisc (&gt; 0.34)</td>
</tr>
<tr>
<td>LazyData:</td>
<td>yes</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods, shiny, declared, venn, lpSolve</td>
</tr>
<tr>
<td>Description:</td>
<td>An extensive set of functions to perform Qualitative Comparative Analysis:
       crisp sets ('csQCA'), temporal ('tQCA'), multi-value ('mvQCA')
       and fuzzy sets ('fsQCA'), using a GUI - graphical user interface.
       'QCA' is a methodology that bridges the qualitative and quantitative
       divide in social science research. It uses a Boolean minimization
       algorithm, resulting in a minimal causal configuration associated
       with a given phenomenon.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-15 09:10:55 UTC; dusadrian</td>
</tr>
<tr>
<td>Author:</td>
<td>Adrian Dusa <a href="https://orcid.org/0000-0002-3525-9253"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre, cph],
  Ciprian Paduraru <a href="https://orcid.org/0000-0002-4518-374X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb],
  jQuery Foundation [cph] (jQuery library and jQuery UI library),
  jQuery contributors [ctb, cph] (jQuery library; authors listed in
    inst/gui/www/lib/jquery-AUTHORS.txt),
  Vasil Dinkov [ctb, cph] (jquery.smartmenus.js library),
  Dmitry Baranovskiy [ctb, cph] (raphael.js library),
  Emmanuel Quentin [ctb, cph] (raphael.inline_text_editing.js library),
  Jimmy Breck-McKye [ctb, cph] (raphael-paragraph.js library),
  Alrik Thiem [aut] (from version 1.0-0 up to version 1.1-3)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Adrian Dusa &lt;dusa.adrian@unibuc.ro&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-15 09:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='About+20the+20QCA+20package'>
QCA: A Package for Qualitative Comparative Analysis
</h2><span id='topic+QCA-package'></span>

<h3>Description</h3>

<p>The package <span class="pkg">QCA</span> contains functions to perform Qualitative Comparative
Analysis, complemented with a graphical user interface. It implements the
comparative method as first described by Ragin (1987), and extended by Cronqvist
and Berg-Schlosser (2009) and Ragin (2000, 2008). QCA is a bridge between the
qualitative and quantitative research methodologies, making use of the qualitative
procedures in a systematic, algorithmic way (therefore increasing the
&ldquo;confidence&rdquo; in the results, as understood by quantitative researchers).
</p>
<p>The Quine-McCluskey minimization algorithms implemented in this package are
mathematically exact, as described by Dusa (2007b), Dusa (2010), Dusa and Thiem
(2015) and Dusa (2018). They all return the same, relevant set of prime implicants
for <dfn>csQCA</dfn> (binary crisp sets QCA), <dfn>mvQCA</dfn> (multi-value QCA) and
<dfn>fsQCA</dfn> (fuzzy-sets QCA).
</p>
<p>The package also showcases functionality for other types of QCA like <dfn>tsQCA</dfn>
(temporal QCA), see Caren and Panofsky (2005), Ragin and Strand (2008) and more
recently also causal chains similar to those from the package <b><span class="pkg">cna</span></b>
(see Ambuehl et al 2015).
</p>
<p>The results of the <b><span class="pkg">QCA</span></b> package are consistent with (and sometimes
better than) the results of the other software packages for QCA, most notably
<b><span class="pkg">fs/QCA</span></b> by Ragin and Davey (2014) and <b><span class="pkg">Tosmana</span></b> by
Cronqvist and Berg-Schlosser (2009). A comparison of several such software is
provided by Thiem and Dusa (2013).
</p>
<p>More recent versions bring major improvements and additions, most notably: - a new
minimization algorithm called CCubes (Consistency Cubes), that is hundreds of times
faster than the previous eQMC; - conjunctural directional expectations; - extension
to categorical data.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
    Package: </td><td style="text-align: left;"> QCA</td>
</tr>
<tr>
 <td style="text-align: left;">
    Type:    </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
    Version: </td><td style="text-align: left;"> 3.22</td>
</tr>
<tr>
 <td style="text-align: left;">
    Date:    </td><td style="text-align: left;"> 2024-03-15</td>
</tr>
<tr>
 <td style="text-align: left;">
    License: </td><td style="text-align: left;"> GPL (&gt;= 3)</td>
</tr>
<tr>
 <td style="text-align: left;">
    </td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Adrian Dusa<br />
Department of Sociology<br />
University of Bucharest<br />
<a href="mailto:dusa.adrian@unibuc.ro">dusa.adrian@unibuc.ro</a>
</p>


<h3>References</h3>

<p>Ambuehl, M. et al (2015) <em>A Package for Coincidence Analysis (CNA),
R Package Version 2.0 [Computer Program]</em>,
<a href="https://cran.r-project.org/package=cna/">CRAN</a>.
</p>
<p>Caren, N.; Panofsky, A. (2005) &ldquo;TQCA: A Technique for Adding
Temporality to Qualitative Comparative Analysis.&rdquo;
<em>Sociological Methods &amp; Research</em> vol.34, no.2, pp.147-172.
</p>
<p>Cronqvist, L. (2016) <em>Tosmana: Tool for Small-N Analysis, Version 1.522
[Computer Program]</em>. Trier: University of Trier. url: <a href="https://www.tosmana.net/">https://www.tosmana.net/</a>
</p>
<p>Dusa, A. (2007a) &ldquo;User manual for the QCA(GUI) package in R&rdquo;.
<em>Journal of Business Research</em> vol.60, no.5, pp.576-586,
<a href="https://doi.org/10.1016/j.jbusres.2007.01.002">doi:10.1016/j.jbusres.2007.01.002</a>
</p>
<p>Dusa, A. (2007b) <em>Enhancing Quine-McCluskey</em>.
WP 2007-49, <a href="https://compasss.org/working-papers-series/">COMPASSS Working Papers series</a>.
</p>
<p>Dusa, A. (2010) &ldquo;A Mathematical Approach to the Boolean Minimization
Problem.&rdquo; <em>Quality &amp; Quantity</em> vol.44, no.1, pp.99-113,
<a href="https://doi.org/10.1007/s11135-008-9183-x">doi:10.1007/s11135-008-9183-x</a>
</p>
<p>Dusa, A.; Thiem, A. (2015) &ldquo;Enhancing the Minimization of Boolean and
Multivalue Output Functions With eQMC&rdquo; <em>Journal of Mathematical Sociology</em>
vol.39, no.2, pp.92-108,<br />
<a href="https://doi.org/10.1080/0022250X.2014.897949">doi:10.1080/0022250X.2014.897949</a>
</p>
<p>Dusa, A. (2018) &ldquo;Consistency Cubes: A Fast, Efficient Method for Boolean
Minimization&rdquo;, R Journal, <a href="https://doi.org/10.32614/RJ-2018-080">doi:10.32614/RJ-2018-080</a>
</p>
<p>Ragin, C.C. (1987) <em>The Comparative Method: Moving beyond Qualitative and
Quantitative Strategies</em>. Berkeley: University of California Press.
</p>
<p>Ragin, C.C. (2000) <em>Fuzzy-Set Social Science</em>. Chicago: University of
Chicago Press.
</p>
<p>Ragin, C.C. (2008) <em>Redesigning Social Inquiry: Fuzzy Sets and Beyond</em>.
Chicago: University of Chicago Press.
</p>
<p>Ragin, C.C.; Strand, S.I. (2008) &ldquo;Using Qualitative Comparative
Analysis to Study Causal Order: Comment on Caren and Panofsky (2005)&rdquo;.
<em>Sociological Methods &amp; Research</em> vol.36, no.4, pp.431-441.
</p>
<p>Ragin, C.C.; Davey, S. (2014) <em>fs/QCA: Fuzzy-Set/Qualitative
Comparative Analysis, Version 2.5 [Computer Program]</em>.
Irvine: Department of Sociology, University of California.
</p>
<p>Thiem, A.; Dusa, A. (2013) &ldquo;Boolean Minimization in Social
Science Research: A Review of Current Software for Qualitative Comparative
Analysis (QCA).&rdquo; <em>Social Science Computer Review</em> vol.31, no.4, pp.505-521.
</p>

<hr>
<h2 id='_Cebotari+20and+20Vink'>Ethnic protest in Europe</h2><span id='topic+CVR'></span><span id='topic+CVF'></span>

<h3>Description</h3>

<p>This data set was used by Cebotari and Vink (2013), and it was taken here from the
associated replication file Cebotari and Vink (2015).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(CVR)
data(CVF)
</code></pre>


<h3>Format</h3>

                  
<p>A data frame containing 29 cases (ethnic minorities) and the following 6 columns:
</p>

<table>
<tr>
 <td style="text-align: right;">
    
<b><code>DEMOC</code></b>
    </td><td style="text-align: left;"> Level of democracy: (contextual factor), based on a democracy index ranking countries </td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> on a scale from strong autocracies (0) to strong democracies (10). The fuzzy scores were </td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> calibrated using an exclusion threshold of 2, a crossover of 7 and an inclusion </td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> threshold of 9.5.</td>
</tr>
<tr>
 <td style="text-align: right;">

<b><code>ETHFRACT</code></b>
    </td><td style="text-align: left;"> Degree of ethnic fractionalization: (contextual factor), with raw scores ranging from a </td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> homogenous society (0) to a highly fragmented country (1). The fuzzy scores were </td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> calibrated using an exclusion threshold of 0, a crossover of 0.495 and an inclusion </td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> threshold of 0.8.</td>
</tr>
<tr>
 <td style="text-align: right;">
    
<b><code>GEOCON</code></b>
    </td><td style="text-align: left;"> Territorial concentration: (group-related factor) with raw data coded as: widely </td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> dispersed (0) and primarily urban minorities (1) considered territorially dispersed </td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> minorities, and ethnic comunities majoritary in a region (2) and entirely concentrated </td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> in one region (3) considered as territorially concentrated minorities. The fuzzy scores </td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> were calibrated using an exclusion threshold of 0, a crossover of 1.25 and an inclusion </td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> threshold of 3.</td>
</tr>
<tr>
 <td style="text-align: right;">

<b><code>POLDIS</code></b>
    </td><td style="text-align: left;"> Political discrimination: (group-related factor) captures discrimination practices </td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> toward minority groups that vary from no discrimination (0) to exclusive and repressive</td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> policies toward a minority group (4). The fuzzy scores were calibrated using an exclusion</td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> threshold of 0, a crossover of 0.75 and an inclusion threshold of 3.  </td>
</tr>
<tr>
 <td style="text-align: right;">
    
<b><code>NATPRIDE</code></b>
    </td><td style="text-align: left;"> National pride: (group-related factor) with raw scores ranging from</td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> &lsquo;not at all proud&rsquo; (0) to &lsquo;very proud&rsquo; (3). The fuzzy scores were calibrated using </td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> an exclusion threshold of 0.5, a crossover of 1.5 and an inclusion threshold of 2.5.</td>
</tr>
<tr>
 <td style="text-align: right;">
    
<b><code>PROTEST</code></b>
    </td><td style="text-align: left;"> Outcome, ethnopolitical protest: measured on a range from 0 to 5 with higher values </td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> indicating more intense protest actions. The fuzzy scores were calibrated using an </td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> exclusion threshold of 0.5, a crossover of 1.5 and an inclusion threshold of 3.</td>
</tr>
<tr>
 <td style="text-align: right;">
</td>
</tr>

</table>



<h3>Details</h3>

<p>There are two different versions of the Cebotari and Vink data: <b><code>CVR</code></b> contains
the raw data, and <b><code>CVF</code></b> contains the data calibrated to fuzzy-sets.
</p>


<h3>References</h3>

<p>Cebotari, V.; Vink, M.P. (2013) &ldquo;A Configurational Analysis of Ethnic
Protest in Europe&rdquo;. <em>International Journal of Comparative Sociology</em>
vol.54, no.4, pp.298-324.
</p>
<p>Cebotari, V.; Vink, M.P. (2015) &ldquo;Replication Data for: A configurational
analysis of ethnic protest in Europe&rdquo;, <a href="https://doi.org/10.7910/DVN/PT2IB9">doi:10.7910/DVN/PT2IB9</a>, Harvard Dataverse, V2
</p>

<hr>
<h2 id='_Hino'>Time-Difference</h2><span id='topic+HC'></span>

<h3>Description</h3>

<p>This data set was used by Hino (2009), to demonstrate the Time-Difference QCA.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(HC)
</code></pre>


<h3>Format</h3>

                  
<p>A data frame containing 15 cases (countries) and the following 5 columns:
</p>

<table>
<tr>
 <td style="text-align: right;">
    
<b><code>FOREIGN</code></b>
    </td><td style="text-align: left;"> Percentage of foreign population. </td>
</tr>
<tr>
 <td style="text-align: right;">

<b><code>UNEMP</code></b>
    </td><td style="text-align: left;"> Percentage of unemployed population. </td>
</tr>
<tr>
 <td style="text-align: right;">
    
<b><code>CONV</code></b>
    </td><td style="text-align: left;"> Party system convergence. </td>
</tr>
<tr>
 <td style="text-align: right;">

<b><code>PRES80</code></b>
    </td><td style="text-align: left;"> Presence of extreme-right parties in 1980s. </td>
</tr>
<tr>
 <td style="text-align: right;">
    
<b><code>VOTE</code></b>
    </td><td style="text-align: left;"> Outcome, vote share of extreme-right parties.</td>
</tr>
<tr>
 <td style="text-align: right;">
</td>
</tr>

</table>



<h3>Details</h3>

<p>For all columns in the data, a value of 1 means a positive difference between 1990
and 1980, and a value of 0 means negative or zero difference, except for the condition
CONV, which is the inverse of the condition DIVERT in the raw data. The condition
PRES80 does not have a time difference, it represents a simple presence / absence
of extreme-right parties in the 1980s.
</p>


<h3>References</h3>

<p>Hiro, A. (2009) &ldquo;Time-Series QCA. Studying Temporal Change through Boolean Analysis&rdquo;.
<em>Sociological Theory and Methods</em>, vol.24, no.2, pp.247-265.
</p>

<hr>
<h2 id='_Legacy+20datasets'>Legacy datasets</h2><span id='topic+_Legacy+20datasets'></span>

<h3>Description</h3>

<p>The following datasets are no longer part of this package in the formal documentation,
but have been added to ensure backwards compatibility with prior publications.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(d.AS)
data(d.Bas)
data(d.biodiversity)
data(d.BWB)
data(d.CS)
data(d.CZH)
data(d.education)
data(d.Emm)
data(d.graduate)
data(d.health)
data(d.HK)
data(d.HMN)
data(d.homeless)
data(d.jobsecurity)
data(d.Kil)
data(d.Kro)
data(d.napoleon)
data(d.partybans)
data(d.represent)
data(d.RS)
data(d.SA)
data(d.socialsecurity)
data(d.SS)
data(d.stakeholder)
data(d.transport)
data(d.urban)
data(Emme)
data(HarKem)
data(Krook)
data(RagStr)
data(Rokkan)
</code></pre>

<hr>
<h2 id='_Lipset'>Lipset's indicators for the survival of democracy during the inter-war period.</h2><span id='topic+LR'></span><span id='topic+LC'></span><span id='topic+LM'></span><span id='topic+LF'></span>

<h3>Description</h3>

<p>This dataset is taken from Lipset (1959), as used by Rihoux and De Meur (2009),
Cronqvist and Berg-Schlosser (2009) and Ragin (2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(LR)
data(LC)
data(LM)
data(LF)
</code></pre>


<h3>Format</h3>

<p>A data frame containing 18 rows and the following 6 columns:
</p>

<table>
<tr>
 <td style="text-align: right;">
<b><code>DEV</code></b>
    </td><td style="text-align: left;"> Level of development: it is the GDP per capita (USD) in the raw data, calibrated in the </td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> binary crisp version to 0 if below 550 USD and 1 otherwise. For the multi-value crisp </td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> version, two thresholds were used: 550 and 850 USD. </td>
</tr>
<tr>
 <td style="text-align: right;">

<b><code>URB</code></b>
    </td><td style="text-align: left;"> Level of urbanization: percent of the population in towns with 20000 or more </td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> inhabitants, calibrated in the crisp versions to 0 if below 50% and 1 if above. </td>
</tr>
<tr>
 <td style="text-align: right;">

<b><code>LIT</code></b>
    </td><td style="text-align: left;"> Level of literacy: percent of the literate population, calibrated in the crisp versions </td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> to 0 if below 75% and 1 if above. </td>
</tr>
<tr>
 <td style="text-align: right;">

<b><code>IND</code></b>
    </td><td style="text-align: left;"> Level of industrialization: percent of the industrial labor force, calibrated in the </td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> crisp versions to 0 if below 30% and 1 if above. </td>
</tr>
<tr>
 <td style="text-align: right;">

<b><code>STB</code></b>
    </td><td style="text-align: left;"> Government stability: a &ldquo;political-institutional&rdquo; condition added to the previous </td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> four &ldquo;socioeconomic&rdquo; ones. The raw data has the number of cabinets which governed </td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> in the period under study, calibrated in the crisp versions to 0 if 10 or above and to 1 </td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> if below 10. </td>
</tr>
<tr>
 <td style="text-align: right;">

<b><code>SURV</code></b>
    </td><td style="text-align: left;"> Outcome: survival of democracy during the inter-war period: calibrated to 0 if negative, </td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> and 1 if positive raw data.</td>
</tr>
<tr>
 <td style="text-align: right;">
</td>
</tr>

</table>



<h3>Details</h3>

<p>There are four different versions of the Lipset data:
</p>

<table>
<tr>
 <td style="text-align: right;">
<b><code>LR</code></b>  </td><td style="text-align: left;"> contains the raw data </td>
</tr>
<tr>
 <td style="text-align: right;">
<b><code>LC</code></b>  </td><td style="text-align: left;"> is the same data calibrated to binary crisp sets </td>
</tr>
<tr>
 <td style="text-align: right;">
<b><code>LM</code></b>  </td><td style="text-align: left;"> is calibrated to multi-value sets </td>
</tr>
<tr>
 <td style="text-align: right;">
<b><code>LF</code></b>  </td><td style="text-align: left;"> is calibrated to fuzzy-sets </td>
</tr>
<tr>
 <td style="text-align: right;">
</td>
</tr>

</table>



<h3>References</h3>

<p>Lipset, S. M. (1959) &ldquo;Some Social Requisites of Democracy: Economic
Development and Political Legitimacy&rdquo;, <em>American Political Science Review</em>
vol.53, pp.69-105.
</p>
<p>Cronqvist, L.; Berg-Schlosser, D. (2009) &ldquo;Multi-Value QCA (mvQCA)&rdquo;, in
Rihoux, B.; Ragin, C. (eds.) <em>Configurational Comparative Methods. Qualitative
Comparative Analysis (QCA) and Related Techniques</em>, SAGE.
</p>
<p>Rihoux, B.; De Meur, G. (2009) &ldquo;Crisp Sets Qualitative Comparative Analysis
(mvQCA)&rdquo;, in Rihoux, B.; Ragin, C. (eds.) <em>Configurational Comparative Methods.
Qualitative Comparative Analysis (QCA) and Related Techniques</em>, SAGE.
</p>
<p>Ragin, C. (2009) &ldquo;Qualitative Comparative Analysis Using Fuzzy-Sets (fsQCA)&rdquo;,
in Rihoux, B.; Ragin, C. (eds.) <em>Configurational Comparative Methods.
Qualitative Comparative Analysis (QCA) and Related Techniques</em>, SAGE.
</p>

<hr>
<h2 id='_Nieuwbeerta'>Class voting in post-World War era</h2><span id='topic+NF'></span>

<h3>Description</h3>

<p>This fuzzy dataset is an adaptation from Ragin (2005, 2008), the data itself being attributed
to Nieuwbeerta (1995).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(NF)
</code></pre>


<h3>Format</h3>

                  
<p>A data frame containing 12 cases (countries) and the following 5 columns:
</p>

<table>
<tr>
 <td style="text-align: right;">
    
<b><code>A</code></b>
    </td><td style="text-align: left;"> degree of membership in the set of highly <em>affluent</em> countries </td>
</tr>
<tr>
 <td style="text-align: right;">

<b><code>I</code></b>
    </td><td style="text-align: left;"> degree of membership in the set of countries with substantial levels of income <em>inequality</em></td>
</tr>
<tr>
 <td style="text-align: right;">
    
<b><code>M</code></b>
    </td><td style="text-align: left;"> degree of membership in the set of countries with a high percentage of workers employed </td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> in <em>manufacturing</em> </td>
</tr>
<tr>
 <td style="text-align: right;">

<b><code>U</code></b>
    </td><td style="text-align: left;"> degree of membership in the set of countries with strong <em>unions</em> </td>
</tr>
<tr>
 <td style="text-align: right;">
    
<b><code>W</code></b>
    </td><td style="text-align: left;"> outcome: degree of membership in the set of countries with <em>weak</em> class voting </td>
</tr>
<tr>
 <td style="text-align: right;">
  </td>
</tr>

</table>



<h3>Details</h3>

<p>All fuzzy sets in this data are constructed on a six-values scale, for demonstrative purposes.
</p>
<p>In the original dataset, the outcome <code>W</code> is presented as the first column.
</p>


<h3>References</h3>

<p>Nieuwbeerta, P. (1995) <em>The Democratic Class Struggle in Twenty Countries: 1945:1990</em>. 
Amsterdam: Thesis Publishers.
</p>
<p>Ragin, C.C. (2005) &ldquo;From fuzzy sets to crisp truth tables&rdquo;.
WP 2004-28, <a href="https://compasss.org/working-papers-series/">COMPASSS Working Papers series</a>.
</p>
<p>Ragin, C.C. (2008) <em>Redesigning Social Inquiry: Fuzzy Sets and Beyond</em>. 
Chicago: University of Chicago Press.
</p>

<hr>
<h2 id='_Ragin+20and+20Strand'>University recognition of a graduate student union</h2><span id='topic+RS'></span>

<h3>Description</h3>

<p>Original data used by Caren and Panofsky (2005), and reanalysed by Ragin 
and Strand (2008).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(RS)</code></pre>


<h3>Format</h3>

<p>A data frame containing 17 cases and the following 6 columns:
</p>

<table>
<tr>
 <td style="text-align: right;">
<b><code>P</code></b>   </td><td style="text-align: left;"> Public university               </td>
</tr>
<tr>
 <td style="text-align: right;">
<b><code>E</code></b>   </td><td style="text-align: left;"> Support of elite allies         </td>
</tr>
<tr>
 <td style="text-align: right;">
<b><code>A</code></b>   </td><td style="text-align: left;"> National union affiliation      </td>
</tr>
<tr>
 <td style="text-align: right;">
<b><code>S</code></b>   </td><td style="text-align: left;"> Strike or a strike threat       </td>
</tr>
<tr>
 <td style="text-align: right;">
<b><code>EBA</code></b> </td><td style="text-align: left;"> <b>E</b> happens before <b>A</b></td>
</tr>
<tr>
 <td style="text-align: right;">
<b><code>REC</code></b> </td><td style="text-align: left;"> Union recognition               </td>
</tr>
<tr>
 <td style="text-align: right;">
</td>
</tr>

</table>
                                                                    


<h3>Details</h3>

<p>The causal conditions are <b><code>P</code></b>, <b><code>E</code></b>, <b><code>A</code></b> and <b><code>S</code></b>.
All of them are binary crisp with two values: <b><code>0 = No</code></b> and <b><code>1 = Yes</code></b>.
</p>
<p>The column <b><code>EBA</code></b> is not a causal condition, specifying in which case the
causal condition <b><code>E</code></b> happens before the causal condition <b><code>A</code></b>. It has
two values (<b><code>0 = No</code></b> and <b><code>1 = Yes</code></b>) plus the placeholder <b><code>"-"</code></b>
to signal a &ldquo;don't care&rdquo;.
</p>
<p>The outcome is the union recognition <b><code>EBA</code></b>, also binary crisp with two values:
<b><code>0 = No</code></b> and <b><code>1 = Yes</code></b>.
</p>


<h3>Source</h3>

<p>Caren, N; Panofsky, A. (2005) &ldquo;TQCA: A Technique for Adding Temporality 
to Qualitative Comparative Analysis.&rdquo; <em>Sociological Methods &amp; Research</em>
vol. 34, no.2, pp.147-172, <br />
<a href="https://doi.org/10.1177/0049124105277197">doi:10.1177/0049124105277197</a>.
</p>
<p>Ragin, C.C.; Strand, S.I. (2008) &ldquo;Using Qualitative Comparative 
Analysis to Study Causal Order: Comment on Caren and Panofsky (2005).&rdquo; 
<em>Sociological Methods &amp; Research</em> vol.36, no.4, pp.431-441,
<a href="https://doi.org/10.1177/0049124107313903">doi:10.1177/0049124107313903</a>.
</p>

<hr>
<h2 id='calibrate'>Calibrate raw data to crisp or fuzzy sets</h2><span id='topic+calibrate'></span>

<h3>Description</h3>

<p>This function transforms (calibrates) the raw data to either crisp or fuzzy sets
values, using both the <dfn>direct</dfn> and the <dfn>indirect</dfn> methods of calibration.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calibrate(x, type = "fuzzy", method = "direct", thresholds = NA,
          logistic = TRUE, idm = 0.95, ecdf = FALSE, below = 1, above = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calibrate_+3A_x">x</code></td>
<td>
<p>A numerical causal condition.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_type">type</code></td>
<td>
<p>Calibration type, either <b><code>"crisp"</code></b> or <b><code>"fuzzy"</code></b>.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_method">method</code></td>
<td>
<p>Calibration method, either <b><code>"direct"</code></b>, <b><code>"indirect"</code></b>
or <b><code>"TFR"</code></b>.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_thresholds">thresholds</code></td>
<td>
<p>A vector of (named) thresholds.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_logistic">logistic</code></td>
<td>
<p>Calibrate to fuzzy sets using the logistic function.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_idm">idm</code></td>
<td>
<p>The set inclusion degree of membership for the logistic function.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_ecdf">ecdf</code></td>
<td>
<p>Calibrate to fuzzy sets using the empirical cumulative distribution 
function of the raw data.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_below">below</code></td>
<td>
<p>Numeric (non-negative), determines the shape below crossover.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_above">above</code></td>
<td>
<p>Numeric (non-negative), determines the shape above crossover.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_...">...</code></td>
<td>
<p>Additional parameters, mainly for backwards compatibility.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calibration is a transformational process from raw numerical data (interval or
ratio level of measurement) to set membership scores, based on a certain number of
qualitative anchors.
</p>
<p>When <b><code>type = "crisp"</code></b>, the process is similar to recoding the original
values to a number of categories defined by the number of thresholds. For one
threshold, the calibration produces two categories (intervals): 0 if below, 1 if above.
For two thresholds, the calibration produces three categories: 0 if below the first threshold,
1 if in the interval between the thresholds and 2 if above the second threshold etc.
</p>
<p>When <b><code>type = "fuzzy"</code></b>, calibration produces fuzzy set membership scores, using
three anchors for the increasing or decreasing <dfn>s-shaped</dfn> distributions (including
the logistic function), and six anchors for the increasing or decreasing <dfn>bell-shaped</dfn>
distributions.
</p>
<p>The argument <b><code>thresholds</code></b> can be specified either as a simple numeric vector, or as a
named numeric vector. If used as a named vector, for the first category of <dfn>s-shaped</dfn>
distributions, the names of the thresholds should be:<br />
</p>

<table>
<tr>
 <td style="text-align: right;">
    <b><code>"e"</code></b> </td><td style="text-align: left;"> for the full set exclusion</td>
</tr>
<tr>
 <td style="text-align: right;">
    <b><code>"c"</code></b> </td><td style="text-align: left;"> for the set crossover</td>
</tr>
<tr>
 <td style="text-align: right;">
    <b><code>"i"</code></b> </td><td style="text-align: left;"> for the full set inclusion</td>
</tr>
<tr>
 <td style="text-align: right;">
</td>
</tr>

</table>

<p>For the second category of <dfn>bell-shaped</dfn> distributions, the names of the thresholds
should be:<br />
</p>

<table>
<tr>
 <td style="text-align: right;">
    <b><code>"e1"</code></b> </td><td style="text-align: left;"> for the first (left) threshold for full set exclusion</td>
</tr>
<tr>
 <td style="text-align: right;">
    <b><code>"c1"</code></b> </td><td style="text-align: left;"> for the first (left) threshold for set crossover</td>
</tr>
<tr>
 <td style="text-align: right;">
    <b><code>"i1"</code></b> </td><td style="text-align: left;"> for the first (left) threshold for full set inclusion</td>
</tr>
<tr>
 <td style="text-align: right;">
    <b><code>"i2"</code></b> </td><td style="text-align: left;"> for the second (right) threshold for full set inclusion</td>
</tr>
<tr>
 <td style="text-align: right;">
    <b><code>"c2"</code></b> </td><td style="text-align: left;"> for the second (right) threshold for set crossover</td>
</tr>
<tr>
 <td style="text-align: right;">
    <b><code>"e2"</code></b> </td><td style="text-align: left;"> for the second (right) threshold for full set exclusion</td>
</tr>
<tr>
 <td style="text-align: right;">
</td>
</tr>

</table>

<p>If used as a simple numerical vector, the order of the values matter.
</p>
<p>If <b><code>e</code></b> <code class="reqn">&lt;</code> <b><code>c</code></b> <code class="reqn">&lt;</code> <b><code>i</code></b>, then the membership 
function is increasing from <b><code>e</code></b> to <b><code>i</code></b>. If <b><code>i</code></b> <code class="reqn">&lt;</code> 
<b><code>c</code></b> <code class="reqn">&lt;</code> <b><code>e</code></b>, then the membership function is decreasing from 
<b><code>i</code></b> to <b><code>e</code></b>.
</p>
<p>Same for the <dfn>bell-shaped</dfn> distribution, if <b><code>e1</code></b> <code class="reqn">&lt;</code> <b><code>c1</code></b>
<code class="reqn">&lt;</code> <b><code>i1</code></b> <code class="reqn">\le</code> <b><code>i2</code></b> <code class="reqn">&lt;</code> <b><code>c2</code></b> <code class="reqn">&lt;</code>
<b><code>e2</code></b>, then the membership function is first increasing from <b><code>e1</code></b>
to <b><code>i1</code></b>, then flat between <b><code>i1</code></b> and  <b><code>i2</code></b>, and then
decreasing from <b><code>i2</code></b> to <b><code>e2</code></b>. In contrast, if <b><code>i1</code></b>
<code class="reqn">&lt;</code> <b><code>c1</code></b> <code class="reqn">&lt;</code> <b><code>e1</code></b> <code class="reqn">\le</code> <b><code>e2</code></b> <code class="reqn">&lt;</code>
<b><code>c2</code></b> <code class="reqn">&lt;</code> <b><code>i1</code></b>, then the membership function is first decreasing
from <b><code>i1</code></b> to <b><code>e1</code></b>, then flat between <b><code>e1</code></b> and 
<b><code>e2</code></b>, and finally increasing from <b><code>e2</code></b> to <b><code>i2</code></b>.
</p>
<p>When <b><code>logistic = TRUE</code></b> (the default), the argument <b><code>idm</code></b> specifies the
inclusion degree of membership for the logistic function. If <b><code>logistic = FALSE</code></b>, the
function returns linear <dfn>s-shaped</dfn> or <dfn>bell-shaped</dfn> distributions (curved using the
arguments <b><code>below</code></b> and <b><code>above</code></b>), unless activating the argument
<b><code>ecdf</code></b>.
</p>
<p>If there is no prior knowledge on the shape of the distribution, the argument <b><code>ecdf</code></b>
asks the computer to determine the underlying distribution of the empirical, observed points,
and the calibrated measures are found along that distribution.
</p>
<p>Both <b><code>logistic</code></b> and <b><code>ecdf</code></b> arguments can be used only for <dfn>s-shaped</dfn>
distributions (using 3 thresholds), and they are mutually exclusive.
</p>
<p>The parameters <b><code>below</code></b> and <b><code>above</code></b> (active only when both
<b><code>logistic</code></b> and <b><code>ecdf</code></b> are deactivated, establish the degree of
concentration and  dilation (convex or concave shape) between the threshold and crossover:<br />
</p>

<table>
<tr>
 <td style="text-align: left;">
    <b><code>0 &lt; below &lt; 1</code></b> </td><td style="text-align: left;"> dilates in a concave shape below the crossover</td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>below = 1</code></b>     </td><td style="text-align: left;"> produces a linear shape (neither convex, nor concave)</td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>below &gt; 1</code></b>     </td><td style="text-align: left;"> concentrates in a convex shape below the crossover</td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>0 &lt; above &lt; 1</code></b> </td><td style="text-align: left;"> dilates in a concave shape above the crossover</td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>above = 1</code></b>     </td><td style="text-align: left;"> produces a linear shape (neither convex, nor concave)</td>
</tr>
<tr>
 <td style="text-align: left;">
    <b><code>above &gt; 1</code></b>     </td><td style="text-align: left;"> concentrates in a convex shape above the crossover</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>Usually, <b><code>below</code></b> and <b><code>above</code></b> have equal values, unless specific reasons
exist to make them different.
</p>
<p>For the <b><code>type = "fuzzy"</code></b> it is also possible to use the <b><code>"indirect"</code></b>
method to calibrate the data, using a procedure first introduced by Ragin (2008). The indirect method
assumes a vector of thresholds to cut the original data into equal intervals, then it applies
a (quasi)binomial logistic regression with a fractional polynomial equation.
</p>
<p>The results are also fuzzy between 0 and 1, but the method is entirely different: it has no anchors
(specific to the direct method), and it doesn't need to specify a calibration function to calculate
the scores with.
</p>
<p>The third method applied to fuzzy calibrations is called <b><code>"TFR"</code></b> and calibrates categorical
data (such as Likert type response scales) to fuzzy values using the Totally Fuzzy and Relative
method (Chelli and Lemmi, 1995).
</p>


<h3>Value</h3>

<p>A numeric vector of set membership scores, either crisp (starting from 0 with increments of 1),
or fuzzy numeric values between 0 and 1.
</p>


<h3>Author(s)</h3>

<p>Adrian Dusa
</p>


<h3>References</h3>

<p>Cheli, B.; Lemmi, A. (1995) &ldquo;A 'Totally' Fuzzy and Relative
Approach to the Multidimensional Analysis of Poverty&rdquo;. In <em>Economic Notes</em>,
vol.1, pp.115-134.
</p>
<p>Dusa, A. (2019) <em>QCA with R. A Comprehensive Resource</em>. 
Springer International Publishing, <a href="https://doi.org/10.1007/978-3-319-75668-4">doi:10.1007/978-3-319-75668-4</a>.
</p>
<p>Ragin, C. (2008) &ldquo;Fuzzy Sets: Calibration Versus Measurement.&rdquo;
In <em>The Oxford Handbook of Political Methodology</em>, edited by Janet
Box-Steffensmeier, Henry E. Brady, and David Collier, pp.87-121.
Oxford: Oxford University Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# generate heights for 100 people
# with an average of 175cm and a standard deviation of 10cm
set.seed(12345)
x &lt;- rnorm(n = 100, mean = 175, sd = 10)


cx &lt;- calibrate(x, type = "crisp", thresholds = 175)
plot(x, cx, main="Binary crisp set using 1 threshold",
     xlab = "Raw data", ylab = "Calibrated data", yaxt="n")
axis(2, at = 0:1)


cx &lt;- calibrate(x, type = "crisp", thresholds = c(170, 180))
plot(x, cx, main="3 value crisp set using 2 thresholds",
     xlab = "Raw data", ylab = "Calibrated data", yaxt="n")
axis(2, at = 0:2)


# calibrate to a increasing, s-shaped fuzzy-set
cx &lt;- calibrate(x, thresholds = "e=165, c=175, i=185")
plot(x, cx, main = "Membership scores in the set of tall people", 
     xlab = "Raw data", ylab = "Calibrated data")

     
# calibrate to an decreasing, s-shaped fuzzy-set
cx &lt;- calibrate(x, thresholds = "i=165, c=175, e=185")
plot(x, cx, main = "Membership scores in the set of short people", 
     xlab = "Raw data", ylab = "Calibrated data")


# when not using the logistic function, linear increase
cx &lt;- calibrate(x, thresholds = "e=165, c=175, i=185", logistic = FALSE)
plot(x, cx, main = "Membership scores in the set of tall people", 
     xlab = "Raw data", ylab = "Calibrated data")


# tweaking the parameters "below" and "above" the crossover,
# at value 3.5 approximates a logistic distribution, when e=155 and i=195
cx &lt;- calibrate(x, thresholds = "e=155, c=175, i=195", logistic = FALSE,
      below = 3.5, above = 3.5)
plot(x, cx, main = "Membership scores in the set of tall people", 
     xlab = "Raw data", ylab = "Calibrated data")


# calibrate to a bell-shaped fuzzy set
cx &lt;- calibrate(x, thresholds = "e1=155, c1=165, i1=175, i2=175, c2=185, e2=195",
      below = 3, above = 3)
plot(x, cx, main = "Membership scores in the set of average height",
     xlab = "Raw data", ylab = "Calibrated data")


# calibrate to an inverse bell-shaped fuzzy set
cx &lt;- calibrate(x, thresholds = "i1=155, c1=165, e1=175, e2=175, c2=185, i2=195",
      below = 3, above = 3)
plot(x, cx, main = "Membership scores in the set of non-average height",
     xlab = "Raw data", ylab = "Calibrated data")


# the default values of "below" and "above" will produce a triangular shape
cx &lt;- calibrate(x, thresholds = "e1=155, c1=165, i1=175, i2=175, c2=185, e2=195")
plot(x, cx, main = "Membership scores in the set of average height",
     xlab = "Raw data", ylab = "Calibrated data")


# different thresholds to produce a linear trapezoidal shape
cx &lt;- calibrate(x, thresholds = "e1=155, c1=165, i1=172, i2=179, c2=187, e2=195")
plot(x, cx, main = "Membership scores in the set of average height",
     xlab = "Raw data", ylab = "Calibrated data")


# larger values of above and below will increase membership in or out of the set
cx &lt;- calibrate(x, thresholds = "e1=155, c1=165, i1=175, i2=175, c2=185, e2=195",
      below = 10, above = 10)
plot(x, cx, main = "Membership scores in the set of average height",
     xlab = "Raw data", ylab = "Calibrated data")


# while extremely large values will produce virtually crisp results
cx &lt;- calibrate(x, thresholds = "e1=155, c1=165, i1=175, i2=175, c2=185, e2=195",
      below = 10000, above = 10000)
plot(x, cx, main = "Binary crisp scores in the set of average height",
     xlab = "Raw data", ylab = "Calibrated data", yaxt="n")
axis(2, at = 0:1)
abline(v = c(165, 185), col = "red", lty = 2)

# check if crisp
round(cx, 0)


# using the empirical cumulative distribution function
# require manually setting logistic to FALSE
cx &lt;- calibrate(x, thresholds = "e=155, c=175, i=195", logistic = FALSE,
      ecdf = TRUE)
plot(x, cx, main = "Membership scores in the set of tall people", 
     xlab = "Raw data", ylab = "Calibrated data")


## the indirect method, per capita income data from Ragin (2008)
inc &lt;- c(40110, 34400, 25200, 24920, 20060, 17090, 15320, 13680, 11720,
         11290, 10940, 9800, 7470, 4670, 4100, 4070, 3740, 3690, 3590,
         2980, 1000, 650, 450, 110)

cinc &lt;- calibrate(inc, method = "indirect",
        thresholds = "1000, 4000, 5000, 10000, 20000")

plot(inc, cinc, main = "Membership scores in the set of high income", 
     xlab = "Raw data", ylab = "Calibrated data")
     

# calibrating categorical data
set.seed(12345)
values &lt;- sample(1:7, 100, replace = TRUE)

TFR &lt;- calibrate(values, method = "TFR")

table(round(TFR, 3))
</code></pre>

<hr>
<h2 id='causalChain'>Perform CNA - coincidence analysis using QCA</h2><span id='topic+causalChain'></span>

<h3>Description</h3>

<p>This function mimics the functionality in the package <span class="pkg">cna</span>, finding all possible
necessary and sufficient solutions for all possible outcomes in a specific dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>      
causalChain(data, ordering = NULL, strict = FALSE, pi.cons = 0, pi.depth = 0,
      sol.cons = 0, sol.cov = 1, sol.depth = 0, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="causalChain_+3A_data">data</code></td>
<td>
<p>A data frame containing calibrated causal conditions.</p>
</td></tr>
<tr><td><code id="causalChain_+3A_ordering">ordering</code></td>
<td>
<p>A character string, or a list of character vectors specifying
the causal ordering of the causal conditions.</p>
</td></tr>
<tr><td><code id="causalChain_+3A_strict">strict</code></td>
<td>
<p>Logical, prevents causal conditions on the same temporal level to
act as outcomes for each other.</p>
</td></tr>
<tr><td><code id="causalChain_+3A_pi.cons">pi.cons</code></td>
<td>
<p>Numerical fuzzy value between 0 and 1, minimal consistency
threshold for a prime implicant to be declared as sufficient.</p>
</td></tr>
<tr><td><code id="causalChain_+3A_pi.depth">pi.depth</code></td>
<td>
<p>Integer, a maximum number of causal conditions to be used
when searching for conjunctive prime implicants.</p>
</td></tr>
<tr><td><code id="causalChain_+3A_sol.cons">sol.cons</code></td>
<td>
<p>Numerical fuzzy value between 0 and 1, minimal consistency 
threshold for a model to be declared as sufficient.</p>
</td></tr>
<tr><td><code id="causalChain_+3A_sol.cov">sol.cov</code></td>
<td>
<p>Numerical fuzzy value between 0 and 1, minimal coverage 
threshold for a model to be declared as necessary.</p>
</td></tr>
<tr><td><code id="causalChain_+3A_sol.depth">sol.depth</code></td>
<td>
<p>Integer, a maximum number of prime implicants to be used when
searching for disjunctive models.</p>
</td></tr>
<tr><td><code id="causalChain_+3A_...">...</code></td>
<td>
<p>Other arguments to be passed to functions <code><a href="#topic+minimize">minimize</a>()</code>
and <code><a href="#topic+truthTable">truthTable</a>()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Although claiming to be a novel technique, coincidence analysis is yet another form
of Boolean minimization. What it does is very similar and results in the same set of
solutions as performing separate QCA analyses where every causal condition from the
<b><code>data</code></b> is considered an outcome.
</p>
<p>This function aims to demonstrate this affirmation and show that results from package
<span class="pkg">cna</span> can be obtained with package <span class="pkg">QCA</span>. It is not intended to offer a
complete replacement for the function <b><code>cna()</code></b>, but only to
replicate its so called &ldquo;asf&rdquo; - atomic solution formulas.
</p>
<p>The three most important arguments from function <b><code>cna()</code></b> have direct
correspondents in function <b><code><a href="#topic+minimize">minimize</a>()</code></b>:
</p>

<table>
<tr>
 <td style="text-align: right;">
<b><code>con</code></b> </td><td style="text-align: left;"> corresponds to <b><code>sol.cons</code></b>.</td>
</tr>
<tr>
 <td style="text-align: right;">
<b><code>con.msc</code></b> </td><td style="text-align: left;"> corresponds to <b><code>pi.cons</code></b>.</td>
</tr>
<tr>
 <td style="text-align: right;">
<b><code>cov</code></b> </td><td style="text-align: left;"> corresponds to <b><code>sol.cov</code></b>.
</td>
</tr>

</table>

<p>Two other arguments from function <b><code>cna()</code></b> have been directly
imported in this function, to complete the list of arguments that generate the same
results.
</p>
<p>The argument <b><code>ordering</code></b> splits the causal conditions in different temporal
levels, where prior arguments can act as causal conditions, but not as outcomes for the
subsequent temporal conditions. One simple way to split conditions is to use a list
object, where different components act as different temporal levels, in the order of
their index in the list: conditions from the first component act as the oldest causal
factors, while those from the and the last component are part of the most recent temporal
level.
</p>
<p>Another, perhaps simpler way to express the same thing is to use a single character,
where factors on the same level are separated with a comma, and temporal levels are
separated by the sign <b><code>&lt;</code></b>.
</p>
<p>A possible example is: <b><code>"A, B, C &lt; D, E &lt; F"</code></b>.
</p>
<p>Here, there are three temporal levels and conditions A, B and C can act as causal factors
for the conditions D, E and F, while the reverse is not possible. Given that D, E and F
happen in a subsequent temporal levels, they cannot act as causal conditions for A, B or C.
The same thing is valid with D and E, which can act as causal conditions for F, whereas
F cannot act as a causal condition for D or E, and certainly not for A, B or C.
</p>
<p>The argument <b><code>strict</code></b> controls whether causal conditions from the same temporal
level may be outcomes for each other. If activated, none of A, B and C can act as causal
conditions for the other two, and the same thing happens in the next temporal level where
neither D nor E can be causally related to each other.
</p>
<p>Although the two functions reach the same results, they follow different methods. The input
for the minimization behind the function <b><code>cna()</code></b> is a coincidence list,
while in package <span class="pkg">QCA</span> the input for the minimization procedure is a truth table. The
difference is subtle but important, with the most important difference that package
<span class="pkg">cna</span> is not exhaustive.
</p>
<p>To find a set of solutions in a reasonable time, the formal choice in package <span class="pkg">cna</span> is
to deliberately stop the search at certain (default) depths of complexity. Users are free
to experiment with these depths from the argument <b><code>maxstep</code></b>, but there is no
guarantee the results will be exhaustive.
</p>
<p>On the other hand, the function <b><code>causalChain()</code></b> and generally all related
functions from package <span class="pkg">QCA</span> are spending more time to make sure the search is
exhaustive. Depths can be set via the arguments <b><code>pi.depth</code></b> and
<b><code>sol.depth</code></b>, but unlike package <span class="pkg">cna</span> these are not mandatory.
</p>
<p>By default, the package <span class="pkg">QCA</span> employes a different search algorithm based on
Consistency Cubes (Dusa, 2018), analysing all possible combinations of causal conditions
and all possible combinations of their respective levels. The structure of the input
dataset (number of causal conditions, number of levels, number of unique rows in the
truth table) has a direct implication on the search time, as all of those characteristics
become entry parameters when calculating all possible combinations.
</p>
<p>Consequently, two kinds of depth arguments are provided: 
</p>

<table>
<tr>
 <td style="text-align: right;">
<b><code>pi.depth</code></b> </td><td style="text-align: left;"> the maximum number of causal conditions needed to conjunctively</td>
</tr>
<tr>
 <td style="text-align: right;">
 </td><td style="text-align: left;"> construct a prime implicant; it is the complexity level where the search</td>
</tr>
<tr>
 <td style="text-align: right;">
 </td><td style="text-align: left;"> can be stopped, as long as the PI chart can be solved.</td>
</tr>
<tr>
 <td style="text-align: right;">
<b><code>sol.depth</code></b> </td><td style="text-align: left;"> the maximum number of prime implicants needed to disjunctively</td>
</tr>
<tr>
 <td style="text-align: right;">
 </td><td style="text-align: left;"> build a solution model that covers all initial positive output configurations.
</td>
</tr>

</table>

<p>These arguments introduce a possible new way of deriving prime implicants and solution models,
that can lead to different results (i.e. even more parsimonious) compared to the classical
Quine-McCluskey. When either of them is modified from the default value of 0, the minimization
method is automatically set to <b><code>"CCubes"</code></b> and the remainders are automatically
included in the minimization.
</p>
<p>The higher these depths, the higher the search time. Connversely, the search time can be
significantly shorter if these depths are smaller. Irrespective of how large 
<b><code>pi.depth</code></b> is, the algorithm will always stop at a maximum complexity level
where no new, non-redundant prime implicants are found. The argument <b><code>sol.depth</code></b>
is relevant only when activating the argument <b><code>all.sol</code></b> to solve the PI chart.
</p>
<p>The argument <b><code>sol.cons</code></b> introduces another method of solving the PI chart.
Normally, once the solution models are found among all possible combinations of
<b><code>k</code></b> prime implicants, consistencies and coverages are subsequently
calculated. When <b><code>sol.cons</code></b> is lower than 1, then models are searched
based on their consistencies, which should be at least equal to this threshold.
</p>
<p>Exhaustiveness is guaranteed in package <span class="pkg">QCA</span> precisely because it uses a truth table
as an input for the minimization procedure. The only exception is the option of finding
solutions based on their consistency, with the argument <b><code>sol.cons</code></b>: for large
PI charts, time can quickly increase to infinity, to identify all possible irredundant
(disjunctions that are not subsets of previously found) disjunctive models. In such a
situation, the number of combinations of all possible numbers of prime implicants is
potentially too large to be solved in a polynomial time and if not otherwise specified in
the argument <b><code>sol.depth</code></b> the function <b><code>causalChain()</code></b> silently
sets a complexity level of 7 prime implicants per model.
</p>
<p>When minimizing a dataset instead of a truth table, unless otherwise specified, the 
argument <b><code>incl.cut</code></b> is automatically set to the minimum value between
<b><code>pi.cons</code></b> and <b><code>sol.cons</code></b>, then passed to the function
<b><code>truthTable()</code></b>.
</p>


<h3>Value</h3>

<p>A list of length equal to the number of columns in the <b><code>data</code></b>. Each component
contains the result of the QCA minimization for that specific column acting as an outcome.
</p>


<h3>Author(s)</h3>

<p>Adrian Dusa
</p>


<h3>See Also</h3>

<p><code><a href="#topic+minimize">minimize</a></code>, <code><a href="#topic+truthTable">truthTable</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# The following examples assume the package cna is installed
library(cna)
cna(d.educate, what = "a")

# same results with
cc &lt;- causalChain(d.educate)
cc

# inclusion and coverage scores can be inspected for each outcome
cc$E$IC


# another example, function cna() requires specific complexity depths
cna(d.women, maxstep = c(3, 4, 9), what = "a")

# same results with, no specific depths are required
causalChain(d.women)


# multivalue data require a different function in package cna
mvcna(d.pban, ordering = list(c("C", "F", "T", "V"), "PB"),
      cov = 0.95, maxstep = c(6, 6, 10), what = "a")

# same results again, simpler command
causalChain(d.pban, ordering = "C, F, T, V &lt; PB", sol.cov = 0.95)


# specifying a lower consistency threshold for the solutions
mvcna(d.pban, ordering = list(c("C", "F", "T", "V"), "PB"), con = .93,
      maxstep = c(6, 6, 10), what = "a")

# same thing with
causalChain(d.pban, ordering = "C, F, T, V &lt; PB", pi.cons = 0.93,
            sol.cons = 0.95)


# setting consistency thresholds for the PIs, solutions and also
# a coverage threshold for the solution (note that an yet another
# function for fuzzy sets is needed in package cna)

dat2 &lt;- d.autonomy[15:30, c("AU","RE", "CN", "DE")]
fscna(dat2, ordering = list("AU"), con = .9, con.msc = .85, cov = .85,
      what = "a")

# again, the same results using the same function:
causalChain(dat2, ordering = "AU", sol.cons = 0.9, pi.cons = 0.85,
            sol.cov = 0.85)

## End(Not run)
</code></pre>

<hr>
<h2 id='complexity'>Number of combinations at a given complexity layer</h2><span id='topic+complexity'></span>

<h3>Description</h3>

<p>This function calculates the number of all possible combinations of conditions
(including all levels for each condition), at a given complexity layer.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>complexity(n, layers, noflevels, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="complexity_+3A_n">n</code></td>
<td>
<p>Numeric scalar, the number of input conditions.</p>
</td></tr>
<tr><td><code id="complexity_+3A_layers">layers</code></td>
<td>
<p>Numeric vector, the complexity layer(s) with values from 1 to n.</p>
</td></tr>
<tr><td><code id="complexity_+3A_noflevels">noflevels</code></td>
<td>
<p>Numeric vector containing the number of levels for each
of the n conditions.</p>
</td></tr>
<tr><td><code id="complexity_+3A_...">...</code></td>
<td>
<p>Other arguments, mainly for internal use.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These are the number of combinations which the CCubes algorithm (Dusa, 2018) checks
to determine the prime implicants from a minimization process.
</p>
<p>In the bottom-up approach, CCubes first checks for single conditions (combinations of 
both presence and absence, or more levels if multi-value), then all possible combinations
of levels for two conditions etc.
</p>
<p>The precise equation that partitions the search space into complexity layers is:
</p>
<p style="text-align: center;"><code class="reqn">
    \sum_{c = 1}^{k} {k\choose c} \prod_{s = 1}^{c} l_s
</code>
</p>

<p>where <code class="reqn">l</code> stands for the number of levels for each combination of <code class="reqn">c</code>
conditions out of <code class="reqn">k</code>.
</p>


<h3>Value</h3>

<p>A numeric vector.
</p>


<h3>Author(s)</h3>

<p>Adrian Dusa
</p>


<h3>References</h3>

<p>Dusa, A. (2018) &ldquo;Consistency Cubes: A Fast, Efficient Method for Boolean
Minimization&rdquo;, R Journal, <a href="https://doi.org/10.32614/RJ-2018-080">doi:10.32614/RJ-2018-080</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>complexity(3) # all layers from 1 to 3

complexity(5, layers = 2)
</code></pre>

<hr>
<h2 id='findRows'>Find untenable configurations</h2><span id='topic+findRows'></span>

<h3>Description</h3>

<p>This function finds various types of untenable assumptions that are used when
excluding certain configurations from the minimization process.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>findRows(obj = NULL, expression = "", observed = FALSE, type = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="findRows_+3A_obj">obj</code></td>
<td>
<p>A truth table (an object of class <code>"QCA_tt"</code>) or an equivalent
numerical matrix.</p>
</td></tr>
<tr><td><code id="findRows_+3A_expression">expression</code></td>
<td>
<p>String: a QCA expression written in sum of products form.</p>
</td></tr>
<tr><td><code id="findRows_+3A_observed">observed</code></td>
<td>
<p>Logical: also return subset relations for observed
configurations, when <code>obj</code> is a truth table.</p>
</td></tr>
<tr><td><code id="findRows_+3A_type">type</code></td>
<td>
<p>Numeric vector, specifying the type(s) of untenable configurations.</p>
</td></tr>
<tr><td><code id="findRows_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to function
<code><a href="#topic+truthTable">truthTable</a>()</code>, for the negation of the outcome.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The primary purpose is to find untenable assumptions to be excluded from the
Boolean minimization process. For this reason, the input is most of the times a
truth table, but for demonstration purposes it can also be a simple matrix
having column names.
</p>
<p>It started as a function to find rows that are subsets of a given SOP expression,
and it developed to cover even more untenable asumptions.
</p>
<p>Subset rows can be anything, from remainders to the observed configurations:
positive output, negative output and contradictions). By default, the function
returns only the subset configurations for the remaiders, but activating the
argument <b><code>observed</code></b> adds the corresponding observed configurations
to the output.
</p>
<p>It might ocasionally find negative output configurations or contradictions, but
that doesn't have any side effect because they are going to be excluded from the
minimization anyways, unless contradictions are included in the minimization.
The only category that really matters if they are identified or not, are the
positive output configurations.
</p>
<p>The contradictory simplifying assumptions (CSAs) are those which are used for
both the presence and the absence of the outcome, while simultaneous subset
relations (SSRs) when observed configurations are sufficient for both the
presence and the absence of the outcome. CSAs and SSRs are incoherent
conterfactuals, part of a category called Untenable Assumptions.
</p>
<p>This function takes does what is normally done with a series of commands, in a
more integrated and systematic way.
</p>
<p>Providing a truth table is sufficient to perform all these tasks, because a
truth table already contains all necessary information of how it was produced,
most importantly the inclusion cut-off(s). By default, it uses the same options
to produce a truth table for the negation of the outcome (if the input truth
table was created for its presence, or the other way round), and minimizes both
to inspect their simplifying assumptions to detect which are contradictory.
</p>
<p>Identical simplifying assumptions that found in both parsimonious solutions are
declared as contradictory. Observed configurations that are sufficient for both
the presence and the absence of the outcome are incoherent because of the
simultaneous subset relations problem.
</p>
<p>The following types of untenable assumptions can be searched for:
</p>

<table>
<tr>
 <td style="text-align: right;">
    <code>0</code> </td><td style="text-align: left;"> all of them</td>
</tr>
<tr>
 <td style="text-align: right;">
    <code>1</code> </td><td style="text-align: left;"> subsets of a given <b><code>expression</code></b> (default)</td>
</tr>
<tr>
 <td style="text-align: right;">
    <code>2</code> </td><td style="text-align: left;"> contradictory simplifying assumptions</td>
</tr>
<tr>
 <td style="text-align: right;">
    <code>3</code> </td><td style="text-align: left;"> simultaneous subset relations</td>
</tr>
<tr>
 <td style="text-align: right;">
</td>
</tr>

</table>

<p>To find contradictory simplifying assumptions, a truth table for the negated
outcome is constructed, using the <b><code>incl.cut</code></b> argument from the
<b><code>obj</code></b> input object. If the inclusion cut-off has a single value, the
same is used for the negated outcome, and if it has two values the second is used.
</p>
<p>If very specific cutoff values are needed for the negation of the outcome, these
can be provided via the <b><code>...</code></b> argument, that will be passed to
function <b><code><a href="#topic+truthTable">truthTable</a>()</code></b>.
</p>


<h3>Value</h3>

<p>A numeric vector of row numbers from the truth table.
</p>


<h3>Author(s)</h3>

<p>Adrian Dusa
</p>


<h3>See Also</h3>

<p><code><a href="#topic+truthTable">truthTable</a></code>, <code><a href="#topic+minimize">minimize</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># Lipset's binary crisp version
ttLC &lt;- truthTable(LC, "SURV", show.cases = TRUE)

findRows(obj = ttLC, "DEV*~IND*STB")

## all subset rows from the truth table, also for observed configurations
findRows(obj = ttLC, "DEV*~IND*STB", observed = TRUE)


# Lipset's fuzzy version
ttLF &lt;- truthTable(LF, outcome = "SURV", incl.cut = 0.8)

findRows(obj = ttLF, type = 2) # contradictory simplifying assumptions


# Contradictory simplifying assumptions using different cutoff values
# for the _negation_ of the outcome

findRows(obj = ttLF, type = 2, incl.cut = 0.9, pri.cut = 0.7)
</code></pre>

<hr>
<h2 id='findTh'>Find calibration thresholds</h2><span id='topic+findTh'></span>

<h3>Description</h3>

<p>The purpose of this function is to automatically find calibration thresholds
for a numerical causal condition, to be split into separate groups.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>findTh(x, n = 1, hclustm = "complete", distm = "euclidean", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="findTh_+3A_x">x</code></td>
<td>
<p>A numerical causal condition.</p>
</td></tr>
<tr><td><code id="findTh_+3A_n">n</code></td>
<td>
<p>The number of thresholds to find.</p>
</td></tr>
<tr><td><code id="findTh_+3A_hclustm">hclustm</code></td>
<td>
<p>The agglomeration (clustering) method to be used.</p>
</td></tr>
<tr><td><code id="findTh_+3A_distm">distm</code></td>
<td>
<p>The distance measure to be used.</p>
</td></tr>
<tr><td><code id="findTh_+3A_...">...</code></td>
<td>
<p>Other arguments (mainly for backwards compatibility).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The process of calibration into crisp sets assumes expert knowledge about the
best threshold(s) that separate the raw data into the most meaningful
groups.
</p>
<p>In the absence of such knowledge, an automatic procedure might help grouping
the raw data according to statistical clustering techniques.
</p>
<p>The number of groups to split depends on the number of thresholds: one
thresholds splits into two groups, two thresholds splits into three groups etc.
</p>
<p>For more details about how many groups can be formed with how many thresholds,
see <b><code><a href="stats.html#topic+cutree">cutree</a>()</code></b>.
</p>
<p>More details about the clustering techniques used in this function are found
using <b><code><a href="stats.html#topic+hclust">hclust</a>()</code></b>, and also more details about different
distance measures can be found with <b><code><a href="stats.html#topic+dist">dist</a>()</code></b>. This
function uses their default values.
</p>


<h3>Value</h3>

<p>A numeric vector of length <b><code>n</code></b>.
</p>


<h3>Author(s)</h3>

<p>Adrian Dusa
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+cutree">cutree</a></code>, <code><a href="stats.html#topic+hclust">hclust</a></code>, <code><a href="stats.html#topic+dist">dist</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
# hypothetical list of country GDPs
gdp &lt;- c(460, 500, 900, 2000, 2100, 2400, 15000, 16000, 20000)


# find one threshold to separate into two groups
findTh(gdp)
# 8700


# find two thresholds to separate into two groups
findTh(gdp, n = 2)
# 8700 18000


# using different clustering methods
findTh(gdp, n = 2, hclustm = "ward.D2", distm = "canberra")
# 1450 8700

</code></pre>

<hr>
<h2 id='fuzzyand+2C+20fuzzyor'>Logical operations</h2><span id='topic+fuzzyand'></span><span id='topic+fuzzyor'></span>

<h3>Description</h3>

<p>These functions perform logical operations <code>AND</code> and <code>OR</code>, for binary crisp or fuzzy
set membership scores.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fuzzyand(..., na.rm = FALSE)

fuzzyor(..., na.rm = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fuzzyand+2B2C+2B20fuzzyor_+3A_...">...</code></td>
<td>
<p>Two or more numerical (calibrated) objects containing membership scores,
or a matrix / data frame of calibrated columns.</p>
</td></tr>
<tr><td><code id="fuzzyand+2B2C+2B20fuzzyor_+3A_na.rm">na.rm</code></td>
<td>
<p>Logical, indicating whether missing values should be removed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numerical vector of class <code>"QCA_fuzzy"</code>, with a <code>name</code> attribute expression</p>


<h3>Author(s)</h3>

<p>Adrian Dusa
</p>


<h3>Examples</h3>

<pre><code class='language-R'># -----
# Cebotari &amp; Vink (2013, 2015)

# DEMOC*GEOCON*NATPRIDE
using(CVF, fuzzyand(DEMOC, GEOCON, NATPRIDE))

# same thing with
fuzzyand(CVF[, c(1,3,5)])

# DEMOC*~GEOCON*NATPRIDE
fa &lt;- using(
    CVF,
    fuzzyand(DEMOC, 1 - GEOCON, NATPRIDE)
)
fa

attr(fa, "name")


# ETHFRACT + POLDIS
using(CVF, fuzzyor(ETHFRACT, POLDIS))

# same thing with
fuzzyor(CVF[, c(2,4)])

# ETHFRACT + ~POLDIS
fo &lt;- using(CVF, fuzzyor(ETHFRACT, 1 - POLDIS))
fo

attr(fo, "name")

</code></pre>

<hr>
<h2 id='generate'>Generate a custom data structure</h2><span id='topic+generate'></span>

<h3>Description</h3>

<p>This function acts as a DGS - Data Generating Structure for a certain SOP expression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate(expression = "", snames = "", noflevels, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generate_+3A_expression">expression</code></td>
<td>
<p>String: a SOP - sum of products expression.</p>
</td></tr>
<tr><td><code id="generate_+3A_snames">snames</code></td>
<td>
<p>A string containing the sets' names, separated by commas.</p>
</td></tr>
<tr><td><code id="generate_+3A_noflevels">noflevels</code></td>
<td>
<p>Numerical vector containing the number of levels for each set.</p>
</td></tr>
<tr><td><code id="generate_+3A_...">...</code></td>
<td>
<p>Other arguments, mainly for internal use.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Using the power of SOP expressions, this function can generate the data for any type
of expressions, either Boolean or multi-value.
</p>
<p>Causal conditions should always be separated by a product sign &quot;*&quot;, unless:
- they are single letters, or
- the set names are provided, or
- the expression is multi-value
</p>
<p>All conditions are considered binary crisp, unless the number of levels are provided
in conjunction with the set names, in the order of their specification from the
<b><code>snames</code></b> argument.
</p>
<p>This is an extension of the function <b><code>expand()</code></b> from package admisc,
the process of data generating process being essentially a Quine expansion to a
Disjunctive Normal Form.
</p>


<h3>Value</h3>

<p>A data frame.
</p>


<h3>Author(s)</h3>

<p>Adrian Dusa
</p>


<h3>Examples</h3>

<pre><code class='language-R'>generate(D + ~AB + B~C -&gt; Z)

# the positive configurations in their complete DNF expansion:
expanded &lt;- expand(D + ~AB + B~C, snames = c(A, B, C, D))
# ~A~B~CD + ~A~BCD + ~AB~CD + ~ABCD + A~B~CD + A~BCD +
# AB~CD + ABCD + ~AB~C~D + ~ABC~D + AB~C~D

# which has the equivalent simpler, initial expression:
simplify(expanded)
# D + ~AB + B~C

# same structure with different set names
# (note the mandatory use of the product sign *)
generate(Alpha + ~Beta*Gamma + Gamma*~Delta -&gt; Omicron)

# introducing an additional, irrelevant condition
# (note the product sign is not mandatory if providing the set names)
setnames &lt;- "Alpha, Beta, Gamma, Delta, Epsilon"
dat &lt;- generate(Alpha + ~BetaGamma + Gamma~Delta -&gt; Omicron, snames = setnames)

head(dat)

#   Alpha Beta Gamma Delta Epsilon Omicron
# 1     0    0     0     0       0       0
# 2     0    0     0     0       1       0
# 3     0    0     0     1       0       0
# 4     0    0     0     1       1       0
# 5     0    0     1     0       0       1
# 6     0    0     1     0       1       1

minimize(dat, outcome = Omicron)

# M1: Alpha + ~Beta*Gamma + Gamma*~Delta &lt;-&gt; Omicron
</code></pre>

<hr>
<h2 id='Implicant+20matrix+20functions+3A+20allExpressions+2C+20createMatrix+2C+20getRow'>Functions Related to the Implicant Matrix</h2><span id='topic+allExpressions'></span><span id='topic+createMatrix'></span><span id='topic+getRow'></span>

<h3>Description</h3>

<p>This is a set of functions dedicated to the implicant matrix, a space where all
causal configurations and their minimized solutions are found.
</p>
<p>They can produce all possible implicants and prime implicants, or all possible
combinations for a specific number of causal conditions and their number
of values (either binary or multi-value).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>allExpressions(noflevels = NULL, arrange = FALSE, depth, raw = FALSE, ...)

createMatrix(noflevels = NULL, ...)

getRow(row.no = NULL, noflevels = NULL, zerobased = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Implicant+2B20matrix+2B20functions+2B3A+2B20allExpressions+2B2C+2B20createMatrix+2B2C+2B20getRow_+3A_noflevels">noflevels</code></td>
<td>
<p>The number of levels (values) for each causal condition.</p>
</td></tr>
<tr><td><code id="Implicant+2B20matrix+2B20functions+2B3A+2B20allExpressions+2B2C+2B20createMatrix+2B2C+2B20getRow_+3A_arrange">arrange</code></td>
<td>
<p>Logical, if <code>TRUE</code> the result matrix is arranged for visual inspection.</p>
</td></tr>
<tr><td><code id="Implicant+2B20matrix+2B20functions+2B3A+2B20allExpressions+2B2C+2B20createMatrix+2B2C+2B20getRow_+3A_depth">depth</code></td>
<td>
<p>Integer, an upper number of causal conditions to form expressions with.</p>
</td></tr>
<tr><td><code id="Implicant+2B20matrix+2B20functions+2B3A+2B20allExpressions+2B2C+2B20createMatrix+2B2C+2B20getRow_+3A_raw">raw</code></td>
<td>
<p>Logical, if <code>TRUE</code> it returns the matrix indicating which conditions have
been minimized, using <b><code>-1</code></b>.</p>
</td></tr>
<tr><td><code id="Implicant+2B20matrix+2B20functions+2B3A+2B20allExpressions+2B2C+2B20createMatrix+2B2C+2B20getRow_+3A_row.no">row.no</code></td>
<td>
<p>A vector, the desired row numbers.</p>
</td></tr>
<tr><td><code id="Implicant+2B20matrix+2B20functions+2B3A+2B20allExpressions+2B2C+2B20createMatrix+2B2C+2B20getRow_+3A_zerobased">zerobased</code></td>
<td>
<p>Logical, if <b><code>TRUE</code></b> the first row number is zero.</p>
</td></tr>
<tr><td><code id="Implicant+2B20matrix+2B20functions+2B3A+2B20allExpressions+2B2C+2B20createMatrix+2B2C+2B20getRow_+3A_...">...</code></td>
<td>
<p>Other arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A truth table for binary crisp conditions is a matrix with <code class="reqn">2^k</code> rows, where
<code class="reqn">k</code> is the number of causal conditions.
</p>
<p>For multi-value causal conditions, the same equation can be generalised to:
</p>
<p><code class="reqn">v_{1} \cdot v_{2} \cdot \dots \cdot v_{k}</code>
</p>
<p>where <code class="reqn">v</code> is the number of values (levels) for every causal condition from <code class="reqn">1</code>
to <code class="reqn">k</code>.
</p>
<p>Implicant matrices contain all rows from the truth table, plus all of their supersets,
(all implicants and prime implicants), including the empty set (Dusa 2007, 2010).
</p>
<p>For a binary crisp set procedure, there are <code class="reqn">3^k - 1</code> possible expressions (groupings),
see Ragin (2010). Including the empty set (the situation when all causal conditions have
been minimized), the implicant matrix consists of exactly <code class="reqn">3^k</code> rows, including the
truth table configurations.
</p>
<p>In fact, <code class="reqn">3^k</code> is also obtained by the product:
</p>
<p><code class="reqn">(2 + 1) \cdot (2 + 1) \cdot \dots \cdot (2 + 1)</code>
</p>
<p>For multi-value causal conditions, the same equation can be generalised to:
</p>
<p><code class="reqn">(v_{1} + 1) \cdot (v_{2} + 1) \cdot \dots \cdot (v_{k} + 1)</code>
</p>
<p>where every number of levels in each causal conditions is incremented with 1, to allow
coding the minimization of literals in each (prime) implicant (see examples).
</p>
<p>The function <b><code>allExpressions()</code></b> creates a matrix which contains all possible implicants
and prime implicants, displayed in the original values form using the code <b><code>-1</code></b> to
point the minimized literals, while the other functions use the code <b><code>0</code></b>, all other
values being incremented with 1.
</p>
<p>Specifying a smaller <b><code>depth</code></b> automatically activates the argument <b><code>arrange</code></b>.
</p>
<p>When the argument <b><code>arrange</code></b> is activated, the output is arranged in the increasing
order of the number of conditions which form conjunctions, up to the maximum number specified
by the argument <b><code>depth</code></b> (which if <b><code>NULL</code></b>, it is considered equal to the
number of columns in the matrix).
</p>
<p>The function <b><code>createMatrix()</code></b> creates a base matrix for truth tables and implicant
matrices.
</p>
<p>The function <b><code>getRow()</code></b> takes the number of a row in the truth table or implicant matrix
(in its decimal form), and transforms it into its binary (or multi-base) representation,
as a configuration of binary or multi-values for each causal condition.
</p>
<p>Note that <code class="reqn">\textsf{R}</code> is a 1-based language (all numbers start from 1), and similarly positions
in vectors and matrices start with 1. For this reason, although (mathematicall) the binary
representation of the decimal number 0 (for example, at three causal conditions) is 0 0 0,
in <code class="reqn">\textsf{R}</code> that would be the &ldquo;first&rdquo; line in the implicant matrix, therefore 0 0 0 is
translated into the number 1, unless the argument <b><code>zerobased</code></b> is activated.
</p>


<h3>Value</h3>

<p>A matrix with <code class="reqn">k</code> columns and:
</p>
<p><code class="reqn">v_{1} \cdot v_{2} \cdot \dots \cdot v_{k}</code> rows if a truth table;
</p>
<p><code class="reqn">(v_{1} + 1) \cdot (v_{2} + 1) \cdot \dots \cdot (v_{k} + 1)</code> rows if an implicant matrix;
</p>
<p><code class="reqn">x</code> rows, equal to the length of <b><code>row.no</code></b>.
</p>


<h3>Author(s)</h3>

<p>Adrian Dusa
</p>


<h3>References</h3>

<p>Dusa, A. (2007b) <em>Enhancing Quine-McCluskey</em>.
WP 2007-49, <a href="https://compasss.org/working-papers-series/">COMPASSS Working Papers series</a>.
</p>
<p>Dusa, Adrian. 2010. &ldquo;A Mathematical Approach to the Boolean Minimization 
Problem.&rdquo; <em>Quality &amp; Quantity</em> vol.44, no.1, pp.99-113.
</p>
<p>Ragin, Charles C. (2000) <em>Fuzzy-Set Social Science</em>. Chicago: University of 
Chicago Press.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+expand.grid">expand.grid</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># three binary causal conditions, having two levels each: 0 and 1
noflevels &lt;- c(2, 2, 2)


# for three binary causal conditions
allExpressions(noflevels)

# the same matrix, this time arranged better
# (last rows represent the truth table)
allExpressions(noflevels, arrange = TRUE)

# show only the implicants (excluding the truth table)
allExpressions(noflevels, arrange = TRUE, depth = 2)

# using the raw form
allExpressions(noflevels, raw = TRUE)


# create a base truth table for 3 binary conditions
createMatrix(noflevels)

# its implicant matrix
createMatrix(noflevels + 1)

# create a base truth table where the second condition has three levels
createMatrix(c(2, 3, 2))



# deriving rows
rows &lt;- c(2, 4, 5, 7, 8, 10, 11, 13, 14, 16, 17)
mat &lt;- getRow(rows, noflevels + 1) # note the +1
rownames(mat) &lt;- rows
colnames(mat) &lt;- c("A", "B", "C")
mat

# implicant matrix     normal values
# 
#       A  B  C    |       A  B  C       
#    2  0  0  1    |    2  -  -  0       ~C      
#    4  0  1  0    |    4  -  0  -       ~B
#    5  0  1  1    |    5  -  0  0       ~B~C
#    7  0  2  0    |    7  -  1  -       B
#    8  0  2  1    |    8  -  1  0       B~C
#   10  1  0  0    |   10  0  -  -       ~A  
#   11  1  0  1    |   11  0  -  0       ~A~C                 
#   13  1  1  0    |   13  0  0  -       ~A~B   
#   14  1  1  1    |   14  0  0  0       ~A~B~C
#   16  1  2  0    |   16  0  1  -       ~AB
#   17  1  2  1    |   17  0  1  0       ~AB~C                   
</code></pre>

<hr>
<h2 id='minimize'>Minimize a truth table</h2><span id='topic+minimize'></span><span id='topic+eqmcc'></span>

<h3>Description</h3>

<p>This function performs the QCA minimization of an input truth table, or if the
input is a dataset the minimization it minimizes a set of causal conditions with
respect to an outcome. Three minimization methods are available: the classical
Quine-McCluskey, the enhanced Quine-McCluskey and the latest Consistency Cubes
algorithm that is built for performance.
</p>
<p>All algorithms return the same, exact solutions, see
<a href="https://journal.r-project.org/archive/2018/RJ-2018-080/">Dusa (2018)</a>
and <a href="https://www.tandfonline.com/doi/full/10.1080/0022250X.2014.897949">Dusa and Thiem (2015)</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>minimize(input, include = "", dir.exp = NULL, details = FALSE, pi.cons = 0,
         sol.cons = 0, all.sol = FALSE, row.dom = FALSE, first.min = FALSE,
         max.comb = 0, use.labels = FALSE, method = "CCubes", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="minimize_+3A_input">input</code></td>
<td>
<p>A truth table object (preferred) or a data frame containing
calibrated causal conditions and an outcome.</p>
</td></tr>
<tr><td><code id="minimize_+3A_include">include</code></td>
<td>
<p>A vector of other output values to include in the minimization
process.</p>
</td></tr>
<tr><td><code id="minimize_+3A_dir.exp">dir.exp</code></td>
<td>
<p>Character, a vector of directional expectations to derive the
intermediate solution.</p>
</td></tr>
<tr><td><code id="minimize_+3A_details">details</code></td>
<td>
<p>Logical, print more details about the solution.</p>
</td></tr>
<tr><td><code id="minimize_+3A_pi.cons">pi.cons</code></td>
<td>
<p>Numerical fuzzy value between 0 and 1, minimal consistency
threshold for a prime implicant to be declared as sufficient.</p>
</td></tr>
<tr><td><code id="minimize_+3A_sol.cons">sol.cons</code></td>
<td>
<p>Numerical fuzzy value between 0 and 1, minimal consistency
threshold for a model to be declared as sufficient.</p>
</td></tr>
<tr><td><code id="minimize_+3A_all.sol">all.sol</code></td>
<td>
<p>Logical, search for all possible models, including the non-minimal.</p>
</td></tr>
<tr><td><code id="minimize_+3A_row.dom">row.dom</code></td>
<td>
<p>Logical, perform row dominance in the prime implicants' chart to
eliminate redundant prime implicants.</p>
</td></tr>
<tr><td><code id="minimize_+3A_first.min">first.min</code></td>
<td>
<p>Logical, to return only the very first minimal solution (see
Details).</p>
</td></tr>
<tr><td><code id="minimize_+3A_max.comb">max.comb</code></td>
<td>
<p>Numeric real, to limit the size of the PI chart (see Details).</p>
</td></tr>
<tr><td><code id="minimize_+3A_use.labels">use.labels</code></td>
<td>
<p>Logical, use category labels if present (see Examples).</p>
</td></tr>
<tr><td><code id="minimize_+3A_method">method</code></td>
<td>
<p>Minimization method, one of &quot;CCubes&quot; (default), or &quot;QMC&quot; the
classical Quine-McCluskey, or &quot;eQMC&quot; the enhanced Quine-McCluskey.</p>
</td></tr>
<tr><td><code id="minimize_+3A_...">...</code></td>
<td>
<p>Other arguments, passed to other functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Most of the times, this function takes a truth table object as the <b><code>input</code></b>
for the minimization procedure, but the same argument can refer to a data frame
containing calibrated columns.
</p>
<p>For the later case, the function <b><code>minimize()</code></b> originally had some additional
formal arguments which were sent to the function <b><code><a href="#topic+truthTable">truthTable</a>()</code></b>:
<b><code>outcome</code></b>, <b><code>conditions</code></b>, <b><code>n.cut</code></b>,
<b><code>incl.cut</code></b>, <b><code>show.cases</code></b>, <b><code>use.letters</code></b> and
<b><code>inf.test</code></b>.
</p>
<p>All of these parameters are still possible with function <b><code>minimize()</code></b>, but
since they are sent to the <b><code><a href="#topic+truthTable">truthTable</a>()</code></b> function anyway, it is
unnecessary to duplicate their explanation here. The only situation which does need
an additional description relates to the argument <b><code>outcome</code></b>, where
unlike <b><code><a href="#topic+truthTable">truthTable</a>()</code></b> which accepts a single one, the function
<b><code>minimize()</code></b> accepts multiple outcomes and performs a minimization for
each of them (a situation when all columns are considered causal conditions).
</p>
<p>The argument <b><code>include</code></b> specifies which other truth table rows are
included in the minimization process. Most often, the remainders are included but
any value accepted in the argument <b><code>explain</code></b> is also accepted in the
argument <b><code>include</code></b>.
</p>
<p>The argument <b><code>dir.exp</code></b> is used to specify directional expectations, as
described by Ragin (2003). They can be specified using SOP (sum of products)
expressions, which opens up the possibility to experiment with conjunctural
directional expectations. &quot;Don't care&quot; conditions are simply left unspecified.
</p>
<p>If at least one of the conditions included in the analysis is multi-value, the
entire <b><code>dir.exp</code></b> expression should be specified in multi-value
notation using squared brackets. If a condition X is crisp or fuzzy, the
multi-value notation X[0] is interpreted as its absence, as in the ~X notation.
</p>
<p>Activating the <b><code>details</code></b> argument has the effect of printing parameters
of fit for each prime implicant and each overall model, the essential prime
implicants being listed in the top part of the table. It also prints the truth
table, in case the argument <b><code>input</code></b> has been provided as a data frame
instead of a truth table object.
</p>
<p>The default method (when <b><code>all.sol = FALSE</code></b>), is to find the minimal number
(<b><code>k</code></b>) of prime implicants needed to cover all initial positive output
configurations (minterms), then exhaustively search through all possible disjunctions of
<b><code>k</code></b> prime implicants which do cover those configurations.
</p>
<p>Once the PI chart is constructed using the prime implicants found in the previous
stages, the argument <b><code>row.dom</code></b> can be used to further eliminate
irrelevant prime implicants when solving the PI chart, applying the principle of row
dominance: if a prime implicant A covers the same (intial) positive output
configurations as another prime implicant B and in the same time covers
other configurations which B does not cover, then B is irrelevant and eliminated.
</p>
<p>A large number of causal conditions (i.e. over 15), combined with a large number of cases
(i.e. hundreds) usually produce a very large number of prime implicants, resulting in a huge
and extremely complex PI chart with sometimes thousands of rows and hundreds of columns.
</p>
<p>For such a complex PI chart, even finding a minimum is a formidable task, and exhaustively
solving it is very likely impossible in polynomial time. For this reason, after each level
of complexity the CCubes algorithm determines if the PI chart is too difficult, by
calculating the total number of combinations of minimum <b><code>k</code></b> PIs necessary to
cover all columns.
</p>
<p>The argument <b><code>max.comb</code></b> controls this maximum number of combinations. It is a
rational number counted in (fractions of) billions, defaulted at zero to signal searching
to the maximum possible extent. If the total number of combinations exceeds a positive
value of <b><code>max.comb</code></b>, the PI chart is determined as too complex, the search is
stopped and CCubes attempts to return all possible models using the PIs from the previous
levels of complexity, when the PI chart was still not too complex.
</p>
<p>In the extreme situation even this is not feasible, the argument <b><code>first.min</code></b>
controls returning only one (the very first found) minimal model, if at all possible.
</p>


<h3>Value</h3>

<p>An object of class <code>"qca"</code> when using a single outcome, or class <code>"mqca"</code>
when using multiple outcomes. These objects are lists having the following components:
</p>
<table>
<tr><td><code>tt</code></td>
<td>
<p>The truth table object.</p>
</td></tr>
<tr><td><code>options</code></td>
<td>
<p>Values for the various options used in the function (including defaults).</p>
</td></tr>
<tr><td><code>negatives</code></td>
<td>
<p>The line number(s) of the negative configuration(s).</p>
</td></tr>
<tr><td><code>initials</code></td>
<td>
<p>The initial positive configuration(s).</p>
</td></tr>
<tr><td><code>PIchart</code></td>
<td>
<p>A list containing the PI chart(s).</p>
</td></tr>
<tr><td><code>primes</code></td>
<td>
<p>The prime implicant(s).</p>
</td></tr>
<tr><td><code>solution</code></td>
<td>
<p>A list of solution model(s).</p>
</td></tr>
<tr><td><code>essential</code></td>
<td>
<p>A list of essential PI(s).</p>
</td></tr>
<tr><td><code>pims</code></td>
<td>
<p>A list of PI membership scores.</p>
</td></tr>
<tr><td><code>IC</code></td>
<td>
<p>The matrix containing the inclusion and coverage scores for the model(s).</p>
</td></tr>
<tr><td><code>SA</code></td>
<td>
<p>A list of simplifying assumptions.</p>
</td></tr>
<tr><td><code>i.sol</code></td>
<td>
<p>A list of components specific to intermediate model(s), each having
a PI chart, prime implicant membership scores, (non-simplifying) easy
counterfactuals and difficult counterfactuals.</p>
</td></tr>
<tr><td><code>complex</code></td>
<td>
<p>Flag solutions from a too complex PI chart</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>The user's command which produced all these objects and result(s).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Adrian Dusa
</p>


<h3>References</h3>

<p>Cebotari, V.; Vink, M.P. (2013) &ldquo;A Configurational Analysis of Ethnic
Protest in Europe&rdquo;. <em>International Journal of Comparative Sociology</em>
vol.54, no.4, pp.298-324, <a href="https://doi.org/10.1177/0020715213508567">doi:10.1177/0020715213508567</a>.
</p>
<p>Cebotari, V.; Vink, M.P. (2015) &ldquo;Replication Data for: A configurational
analysis of ethnic protest in Europe&rdquo;, Harvard Dataverse, V2, <a href="https://doi.org/10.7910/DVN/PT2IB9">doi:10.7910/DVN/PT2IB9</a>.
</p>
<p>Cronqvist, L.; Berg-Schlosser, D. (2009) &ldquo;Multi-Value QCA (mvQCA)&rdquo;, in
Rihoux, B.; Ragin, C. (eds.) <em>Configurational Comparative Methods. Qualitative
Comparative Analysis (QCA) and Related Techniques</em>, SAGE.
</p>
<p>Dusa, A.; Thiem, A. (2015) &ldquo;Enhancing the Minimization of Boolean and
Multivalue Output Functions With eQMC&rdquo; <em>Journal of Mathematical Sociology</em>
vol.39, no.2, pp.92-108,<br />
<a href="https://doi.org/10.1080/0022250X.2014.897949">doi:10.1080/0022250X.2014.897949</a>.
</p>
<p>Dusa, A. (2018) &ldquo;Consistency Cubes: A Fast, Efficient Method for Boolean
Minimization&rdquo;, R Journal vol.10, issue 2, pp. 357-370, <a href="https://doi.org/10.32614/RJ-2018-080">doi:10.32614/RJ-2018-080</a>
</p>
<p>Dusa, A. (2019) <em>QCA with R. A Comprehensive Resource</em>.
Springer International Publishing, <a href="https://doi.org/10.1007/978-3-319-75668-4">doi:10.1007/978-3-319-75668-4</a>.
</p>
<p>Ragin, C. (2003) <em>Recent Advances in Fuzzy-Set Methods and Their Application to
Policy Questions</em>. WP 2003-9,
<a href="https://compasss.org/working-papers-series/">COMPASSS Working Papers series</a>.
</p>
<p>Ragin, C. (2009) &ldquo;Qualitative Comparative Analysis Using Fuzzy-Sets (fsQCA)&rdquo;,
in Rihoux, B.; Ragin, C. (eds.) <em>Configurational Comparative Methods.
Qualitative Comparative Analysis (QCA) and Related Techniques</em>, SAGE.
</p>
<p>Ragin, C.C.; Strand, S.I. (2008) &ldquo;Using Qualitative Comparative
Analysis to Study Causal Order: Comment on Caren and Panofsky (2005).&rdquo;
<em>Sociological Methods &amp; Research</em> vol.36, no.4, pp.431-441,
<a href="https://doi.org/10.1177/0049124107313903">doi:10.1177/0049124107313903</a>.
</p>
<p>Rihoux, B.; De Meur, G. (2009) &ldquo;Crisp Sets Qualitative Comparative Analysis
(mvQCA)&rdquo;, in Rihoux, B.; Ragin, C. (eds.) <em>Configurational Comparative Methods.
Qualitative Comparative Analysis (QCA) and Related Techniques</em>, SAGE.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+truthTable">truthTable</a></code>, <code><a href="admisc.html#topic+factorize">factorize</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# -----
# Lipset binary crisp data

# the associated truth table
ttLC &lt;- truthTable(LC, SURV, sort.by = "incl, n", show.cases = TRUE)
ttLC

# conservative solution (Rihoux &amp; De Meur 2009, p.57)
cLC &lt;- minimize(ttLC)
cLC

# view the Venn diagram for the associated truth table
library(venn)
venn(cLC)

# add details and case names
minimize(ttLC, details = TRUE)

# negating the outcome
ttLCn &lt;- truthTable(LC, ~SURV, sort.by = "incl, n", show.cases = TRUE)
minimize(ttLCn)

# parsimonious solution, positive output
pLC &lt;- minimize(ttLC, include = "?", details = TRUE)
pLC

# the associated simplifying assumptions
pLC$SA

# parsimonious solution, negative output
pLCn &lt;- minimize(ttLCn, include = "?", details = TRUE)
pLCn


# -----
# Lipset multi-value crisp data (Cronqvist &amp; Berg-Schlosser 2009, p.80)

# truth table, conditions all columns from DEV to IND
# note the sequence operator ":"
ttLM &lt;- truthTable(LM, SURV, conditions = DEV:IND,
        sort.by = "incl", show.cases = TRUE)

# conservative solution, positive output
minimize(ttLM, details = TRUE)

# parsimonious solution, positive output
minimize(ttLM, include = "?", details = TRUE)

# negate the outcome
ttLMn &lt;- truthTable(LM, ~SURV, conditions = DEV:IND,
         sort.by = "incl", show.cases = TRUE)

# conservative solution, negative output
minimize(ttLMn, details = TRUE)

# parsimonious solution, positive output
minimize(ttLMn, include = "?", details = TRUE)


# -----
# Lipset fuzzy sets data (Ragin 2009, p.112)

ttLF &lt;- truthTable(LF, SURV, incl.cut = 0.8, sort.by = "incl", show.cases = TRUE)

# conservative solution
minimize(ttLF, details = TRUE)

# parsimonious solution
minimize(ttLF, include = "?", details = TRUE)

# intermediate solution
minimize(ttLF, include = "?", details = TRUE,
         dir.exp = c(DEV, URB, LIT, IND, STB))

# directional expectations can also be specified using a sequence
minimize(ttLF, include = "?", details = TRUE, dir.exp = DEV:STB)

# URB as a don't care condition (left unspecified) and
# conjunctural directional expectations
minimize(ttLF, include = "?", details = TRUE,
         dir.exp = c(DEV, STB, ~LIT*IND))


# -----
# Cebotari &amp; Vink (2013, 2015)

ttCVF &lt;- truthTable(CVF, outcome = PROTEST, incl.cut = 0.8,
                    sort.by = "incl, n", show.cases = TRUE)

pCVF &lt;- minimize(ttCVF, include = "?", details = TRUE)
pCVF

# inspect the PI chart
pCVF$PIchart

# DEMOC*ETHFRACT*~POLDIS is dominated by DEMOC*ETHFRACT*GEOCON
# using row dominance to solve the PI chart
pCVFrd &lt;- minimize(ttCVF, include = "?", row.dom = TRUE, details = TRUE)

# plot the prime implicants on the outcome
pims &lt;- pCVFrd$pims

par(mfrow = c(2, 2))
for(i in 1:4) {
    XYplot(pims[, i], CVF$PROTEST, cex.axis = 0.6)
}


# -----
# temporal QCA (Ragin &amp; Strand 2008) serving the input as a dataset,
# which will automatically be passed to truthTable() as an intermediary
# step before the minimization

minimize(RS, outcome = REC, details = TRUE)


# -----
# employ category labels

ttLF &lt;- truthTable(LF, SURV, incl.cut = 0.8, sort.by = "incl", show.cases = TRUE)

minimize(ttLF, include = "?", use.labels = TRUE)

# or

ttLF &lt;- truthTable(
    LF, SURV, incl.cut = 0.8, sort.by = "incl", show.cases = TRUE,
    use.labels = TRUE
)

minimize(ttLF, include = "?")


## End(Not run)
</code></pre>

<hr>
<h2 id='modelFit'>Theory evaluation</h2><span id='topic+modelFit'></span>

<h3>Description</h3>

<p>Function to enable theory evaluation, as introduced by Ragin (1987, p.118)
and extended Schneider &amp; Wageman (2012, p.295), by producing parameters of fit
for all possible intersections between a given theoretical statement (a SOP
expresison) and the solutions found by function <b><code><a href="#topic+minimize">minimize</a>()</code></b>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>modelFit(model, theory = "", select = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="modelFit_+3A_model">model</code></td>
<td>
<p>A minimization object of class <code>"QCA_min"</code>.</p>
</td></tr>
<tr><td><code id="modelFit_+3A_theory">theory</code></td>
<td>
<p>Character, a SOP expression.</p>
</td></tr>
<tr><td><code id="modelFit_+3A_select">select</code></td>
<td>
<p>Character or numerical vector to select one or more models.</p>
</td></tr>
<tr><td><code id="modelFit_+3A_...">...</code></td>
<td>
<p>Other arguments, mainly for internal use.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Following Ragin's (1987) original work, theory evaluation amounts to intersecting
a theoretical expectation with a model resulting from a minimization process.
</p>
<p>There are in fact four intersections: presence - presence, presence - absence,
absence - presence and absence - absence, where by &ldquo;absence&rdquo; is actually
meant a negation of an expression using the function <b><code><a href="admisc.html#topic+negate">negate</a>()</code></b>.
</p>
<p>When multiple models exist, all of them are automatically detected, negated and
intersection with the theory. Intersections and parameters of fit are going to
be produced using a single theoretical expression.
</p>
<p>In case of high model diversity, it is sometimes useful to select only a subset
to fit against theory. In such situations, the argument <code>select</code> can be
provided with either the name(s) of the model (for instance &quot;C1P5&quot; for intermediate
solutions) or simply the number of the model(s) of interest (for conservative
and parsimonious solutions).
</p>


<h3>Value</h3>

<p>A list containing objects of class <code>"QCA_pof"</code> with the parameters of fit. For a
single theoretical expression and a single model, the object is a simple
<code>"QCA_pof"</code> object.
</p>


<h3>Author(s)</h3>

<p>Adrian Dusa
</p>


<h3>References</h3>

<p>Ragin, C.C. (1987) <em>The Comparative Method: Moving beyond Qualitative and
Quantitative Strategies</em>. Berkeley: University of California Press.
</p>
<p>Schneider, C.Q.; Wagemann, C. (2012) <em>Set-Theoretic Methods
for the Social Sciences: A Guide to Qualitative Comparative Analysis (QCA)</em>.
Cambridge: Cambridge University Press.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+intersection">intersection</a></code>, <code><a href="admisc.html#topic+negate">negate</a></code>, <code><a href="#topic+pof">pof</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># Lipset fuzzy version data

ttLF &lt;- truthTable(LF, outcome = SURV, incl.cut = 0.8)


# parsimonious solution
pLF &lt;- minimize(ttLF, include = "?")

# modelFit(model = pLF, theory = "DEV*STB")


# hypothetical exploration of intermediate solutions
iLF &lt;- minimize(ttLF, include = "?", dir.exp = "1,0,0,0,0")

# modelFit(iLF, "DEV*STB")
</code></pre>

<hr>
<h2 id='Parameters+20of+20fit'>Calculate parameters of fit</h2><span id='topic+pof'></span><span id='topic+pofind'></span>

<h3>Description</h3>

<p>These functions returns inclusion (consistency) and coverage, plus PRI for sufficiency
and RoN for necessity. The function <b><code>pofind()</code></b> is a stripped down version
of the <b><code>pof()</code></b> function, to calculate parameters of fit for single conditions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pof(setms = NULL, outcome = NULL, data = NULL, relation = "necessity",
    use.labels = FALSE, inf.test = "", incl.cut = c(0.75, 0.5), add = NULL, ...)
    
pofind(data = NULL, outcome = "", conditions = "", relation = "necessity",
    use.labels = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Parameters+2B20of+2B20fit_+3A_setms">setms</code></td>
<td>
<p>A data frame or a single vector of (calibrated) set memberships, or an
expression written in sum of products form.</p>
</td></tr>
<tr><td><code id="Parameters+2B20of+2B20fit_+3A_outcome">outcome</code></td>
<td>
<p>The name of the outcome column from a calibrated data frame, or
the actual numerical column from the data frame, representing the outcome.</p>
</td></tr>
<tr><td><code id="Parameters+2B20of+2B20fit_+3A_data">data</code></td>
<td>
<p>A calibrated data frame.</p>
</td></tr>
<tr><td><code id="Parameters+2B20of+2B20fit_+3A_conditions">conditions</code></td>
<td>
<p>A single string containing the conditions' (columns) names
separated by commas, or a character vector of conditions' names.</p>
</td></tr>
<tr><td><code id="Parameters+2B20of+2B20fit_+3A_relation">relation</code></td>
<td>
<p>The set relation to <b><code>outcome</code></b>, either <b><code>"necessity"</code></b>
or <b><code>"sufficiency"</code></b>, partial words like <b><code>"suf"</code></b> being
accepted (see examples).</p>
</td></tr>
<tr><td><code id="Parameters+2B20of+2B20fit_+3A_use.labels">use.labels</code></td>
<td>
<p>Logical, use category labels if present.</p>
</td></tr>
<tr><td><code id="Parameters+2B20of+2B20fit_+3A_inf.test">inf.test</code></td>
<td>
<p>Specifies the statistical inference test to be performed (currently 
only <b><code>"binom"</code></b>) and the critical significance level. It can be either
a vector of length 2, or a single string containing both, separated by a comma.</p>
</td></tr>
<tr><td><code id="Parameters+2B20of+2B20fit_+3A_incl.cut">incl.cut</code></td>
<td>
<p>The inclusion cutoff(s): either a single value for the presence of the
output, or a vector of length 2, the second for the absence of the output. Used
only in conjunction with the argument <b><code>inf.test</code></b></p>
</td></tr>
<tr><td><code id="Parameters+2B20of+2B20fit_+3A_add">add</code></td>
<td>
<p>A function, or a list containing functions, to add more parameters of fit.</p>
</td></tr>
<tr><td><code id="Parameters+2B20of+2B20fit_+3A_...">...</code></td>
<td>
<p>Other arguments to be passed to the main function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <b><code>pof()</code></b> is one of the most flexible functions in the
<span class="pkg">QCA</span> package. Depending on particular situations, its arguments can be provided
in various formats which are automatically recognized and treated accordingly.
</p>
<p>When specified as a data frame, the argument <b><code>setms</code></b> contains any kind
of set membership scores:
</p>
<p>- calibrated causal conditions from the original data,
</p>
<p>- membership scores from the resulting combinations (component <b><code>coms</code></b>)
of function <b><code><a href="#topic+superSubset">superSubset</a>()</code></b>,
</p>
<p>- prime implicant membership scores (component <b><code>pims</code></b>) from function
<b><code><a href="#topic+minimize">minimize</a>()</code></b>,
</p>
<p>- any other, custom created combinations of set memberships.
</p>
<p>When specified as a matrix, <b><code>setms</code></b> contains the crisp causal combinations
similar to those found in the truth table. The number of columns in the matrix should be
equal to the number of causal conditions in the original <b><code>data</code></b>. If some of
them are minimized, they can be replaced by the numerical value <b><code>-1</code></b> (see
examples section). 
</p>
<p>More generally, <b><code>setms</code></b> can be a numerical vector of line numbers from the
implicant matrix (see function <b><code><a href="#topic+createMatrix">createMatrix</a>()</code></b>), which are automatically
transformed into their corresponding set membership scores. 
</p>
<p>The argument <b><code>setms</code></b> can also be a string expression, written in SOP - sum of
products form.
</p>
<p>For all other situations when <b><code>setms</code></b> is something else than a data frame, it
requires the original <b><code>data</code></b> to generate the set memberships.
</p>
<p>If character, the argument <b><code>outcome</code></b> is the name of the column from the
original <b><code>data</code></b>, to be explained (it is a good practice advice to specify
it using upper case letters, although it will nevertheless be converted to upper case,
by default).
</p>
<p>If the outcome column is multi-value, the argument <b><code>outcome</code></b> should use
the standard curly-bracket notation <b><code>X{value}</code></b>. Multiple values are
allowed, separated by a comma (for example <b><code>X{1,2}</code></b>). Negation of the
outcome can also be performed using the tilde <b><code>~</code></b> operator, for example
<b><code>~X{1,2}</code></b>, which is interpreted as: &quot;all values in X except 1 and 2&quot; and
it becomes the new outcome to be explained.
</p>
<p>The argument <b><code>outcome</code></b> can also be a numerical vector of set membership
values, either directly from the original data frame, or a recoded version (if
originally multi-value).
</p>
<p>The argument <b><code>inf.test</code></b> provides the possibility to perform statistical
inference tests, comparing the calculated inclusion score with a pair of thresholds
(<b><code>ic1</code></b> and <b><code>ic0</code></b>) specified in the argument <b><code>incl.cut</code></b>.
Currently, it can only perform binomial tests (<b><code>"binom"</code></b>), which means that
data should only be provided as binary crisp (not multivalue, not fuzzy).
</p>
<p>If the critical significance level is not provided, the default level of <b><code>0.05</code></b>
is taken.
</p>
<p>The resulting object will contain the calculated p-values (<code>pval1</code> and <code>pval0</code>) from two
separate, one-tailed tests with the alternative hypothesis that the true inclusion
score is:<br />
- greater than <b><code>ic1</code></b> (the inclusion cut-off for an output value of 1)<br />
- greater than <b><code>ic0</code></b> (the inclusion cut-off for an output value of 0)
</p>
<p>It should be noted that statistical tests are performing well only when the number
of cases is large, otherwise they are usually not significant.
</p>
<p>For the necessity relation, the standard measures of inclusion and coverage are
supplemented with the <b><code>RoN</code></b> (Relevance of Necessity) measure, as suggested by
Schneider &amp; Wagemann's (2012).
</p>
<p>The negation of both <b><code>setms</code></b> and <b><code>outcome</code></b> is accepted and
recognized using the Boolean subtraction from 1. If the names of the conditions are
provided via an optional (undocumented) argument <b><code>conditions</code></b>, the
column names of the <b><code>setms</code></b> object are negated using the function
<b><code><a href="admisc.html#topic+negate">negate</a>()</code></b>.
</p>
<p>The logical argument <b><code>neg.out</code></b> is deprecated, but backwards compatible.
<b><code>neg.out = TRUE</code></b> and a tilde <b><code>~</code></b> in the outcome name don't
cancel each other out, either one (or even both) signaling if the <b><code>outcome</code></b>
should be negated.
</p>
<p>The arguments from function <b><code>pofind()</code></b> are passed to the main function
<b><code>pof()</code></b> to calculate parameters of fit.
</p>


<h3>Author(s)</h3>

<p>Adrian Dusa
</p>


<h3>References</h3>

<p>Cebotari, V.; Vink, M.P. (2013) &ldquo;A Configurational Analysis of Ethnic
Protest in Europe&rdquo;. <em>International Journal of Comparative Sociology</em>
vol.54, no.4, pp.298-324, <a href="https://doi.org/10.1177/0020715213508567">doi:10.1177/0020715213508567</a>
</p>
<p>Schneider, C. and Wagemann, C. (2012) <em>Set-Theoretic Metods for the Social Sciences.
A Guide to Qualitative Comparative Analysis</em>. Cambridge: Cambridge University Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+minimize">minimize</a></code>, <code><a href="#topic+superSubset">superSubset</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# -----
# Cebotari &amp; Vink (2013) fuzzy data

conds &lt;- CVF[, 1:5]
PROTEST &lt;- CVF$PROTEST

# parameters of fit (default is necessity)
pof(conds, PROTEST)

# parameters of fit negating the conditions
pof(1 - conds, PROTEST)

# negating the outcome
pof(conds, 1 - PROTEST)

# parameters of fit for sufficiency
pof(conds, PROTEST, relation = "suf")

# also negating the outcome
pof(conds, 1 - PROTEST, relation = "suf")


# -----
# standard analysis of necessity
# using the "coms" component from superSubset()
nCVF &lt;- superSubset(CVF, outcome = PROTEST, incl.cut = 0.90, cov.cut = 0.6)

# also checking their necessity inclusion score in the negated outcome
pof(nCVF$coms, 1 - PROTEST)


# -----
# standard analysis of sufficiency
# using the "pims" component from minimize()

# conservative solution
cCVF &lt;- minimize(truthTable(CVF, outcome = PROTEST, incl.cut = 0.8))

# verify if their negations are also sufficient for the outcome
pof(1 - cCVF$pims, PROTEST, relation = "sufficiency")


# -----
# using a SOP expression, translated using the function translate()

pof(~NATPRIDE + GEOCON -&gt; PROTEST, data = CVF)

# same for the negation of the outcome
pof(~NATPRIDE + GEOCON -&gt; ~PROTEST, data = CVF)

# necessity is indicated by the reverse arrow
pof(~NATPRIDE + GEOCON &lt;- PROTEST, data = CVF)


# -----
# more parameters of fit, for instance Haesebrouck' consistency

inclH &lt;- function(x, y) {
    sum(fuzzyand(x, y)) /
    sum(fuzzyand(x, y) + sqrt(fuzzyor(x - y, 0) * x))
}

pof(~NATPRIDE + GEOCON -&gt; ~PROTEST, data = CVF, add = inclH)

## End(Not run)
</code></pre>

<hr>
<h2 id='PI+20chart+20functions+3A+20makeChart+2C+20findmin+2C+20solveChart'>Create and solve a prime implicants chart</h2><span id='topic+makeChart'></span><span id='topic+findmin'></span><span id='topic+solveChart'></span>

<h3>Description</h3>

<p>These functions help creating a demo for a prime implicant chart, and also
show how to solve it using a minimum number of prime implicants.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>makeChart(primes = "", configs = "", snames = "", mv = FALSE, collapse = "*", ...)

findmin(chart, ...)

solveChart(chart, row.dom = FALSE, all.sol = FALSE, depth = NULL, max.comb = 0,
           first.min = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PI+2B20chart+2B20functions+2B3A+2B20makeChart+2B2C+2B20findmin+2B2C+2B20solveChart_+3A_primes">primes</code></td>
<td>
<p>A string containing prime implicants, separated by commas, or a
matrix of implicants.</p>
</td></tr>                                
<tr><td><code id="PI+2B20chart+2B20functions+2B3A+2B20makeChart+2B2C+2B20findmin+2B2C+2B20solveChart_+3A_configs">configs</code></td>
<td>
<p>A string containing  causal configurations, separated by commas,
or a matrix of causal configurations in the implicants space.</p>
</td></tr>
<tr><td><code id="PI+2B20chart+2B20functions+2B3A+2B20makeChart+2B2C+2B20findmin+2B2C+2B20solveChart_+3A_snames">snames</code></td>
<td>
<p>A string containing the sets' names, separated by commas.</p>
</td></tr>
<tr><td><code id="PI+2B20chart+2B20functions+2B3A+2B20makeChart+2B2C+2B20findmin+2B2C+2B20solveChart_+3A_mv">mv</code></td>
<td>
<p>Logical, row and column names in multi-value notation.</p>
</td></tr>
<tr><td><code id="PI+2B20chart+2B20functions+2B3A+2B20makeChart+2B2C+2B20findmin+2B2C+2B20solveChart_+3A_collapse">collapse</code></td>
<td>
<p>Scalar character, how to collapse different parts of the row or
column names.</p>
</td></tr>
<tr><td><code id="PI+2B20chart+2B20functions+2B3A+2B20makeChart+2B2C+2B20findmin+2B2C+2B20solveChart_+3A_chart">chart</code></td>
<td>
<p>An object of class <code>"QCA_pic"</code> or a logical matrix.</p>
</td></tr>
<tr><td><code id="PI+2B20chart+2B20functions+2B3A+2B20makeChart+2B2C+2B20findmin+2B2C+2B20solveChart_+3A_row.dom">row.dom</code></td>
<td>
<p>Logical, apply row dominance to eliminate redundant prime implicants.</p>
</td></tr>
<tr><td><code id="PI+2B20chart+2B20functions+2B3A+2B20makeChart+2B2C+2B20findmin+2B2C+2B20solveChart_+3A_all.sol">all.sol</code></td>
<td>
<p>Derive all possible solutions, irrespective if the disjunctive number of
prime implicants is minimal or not.</p>
</td></tr>
<tr><td><code id="PI+2B20chart+2B20functions+2B3A+2B20makeChart+2B2C+2B20findmin+2B2C+2B20solveChart_+3A_depth">depth</code></td>
<td>
<p>A maximum number of prime implicants for any disjunctive solution.</p>
</td></tr>
<tr><td><code id="PI+2B20chart+2B20functions+2B3A+2B20makeChart+2B2C+2B20findmin+2B2C+2B20solveChart_+3A_max.comb">max.comb</code></td>
<td>
<p>Numeric, to stop searching for solutions (see Details).</p>
</td></tr>
<tr><td><code id="PI+2B20chart+2B20functions+2B3A+2B20makeChart+2B2C+2B20findmin+2B2C+2B20solveChart_+3A_first.min">first.min</code></td>
<td>
<p>Logical, to return only the very first minimal solution (see
Details).</p>
</td></tr>
<tr><td><code id="PI+2B20chart+2B20functions+2B3A+2B20makeChart+2B2C+2B20findmin+2B2C+2B20solveChart_+3A_...">...</code></td>
<td>
<p>Other arguments (mainly for backwards compatibility).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A PI chart, in this package, is a logical matrix (with <code>TRUE</code>/<code>FALSE</code> values),
containing the prime implicants on the rows and the observed positive output configurations
on the columns. Such a chart is produced by <b><code>makeChart()</code></b>, and it is useful to
visually determine which prime implicants (if any) are essential.
</p>
<p>When <b><code>primes</code></b> and <b><code>configs</code></b> are character, the individual sets
are identified using the function <b><code>translate()</code></b> from package admisc, using the
SOP - Sum Of Products form, which needs the set names in the absence of any other information.
If products are formed using the standard <b><code>*</code></b> operator, specifying the set names
is not mandatory.
</p>
<p>When <b><code>primes</code></b> and <b><code>configs</code></b> are matrices, they have to be specified
at implicants level, where the value <b><code>0</code></b> is interpreted as a minimized literal.
</p>
<p>The chart is subsequently processed algorithmically by <b><code>solveChart()</code></b> to find
the absolute minimal number <code>M</code> of rows (prime implicants) necessary to cover all columns,
then searches through all possible combinations of <code>M</code> rows, to find those which actually
cover the columns.
</p>
<p>The number of all possible combinations of <code>M</code> rows increases exponentially with the
number of prime implicants generated by the Quine-McCluskey minimization procedure, and the
solving time quickly grows towards infinity for large PI charts.
</p>
<p>To solve the chart in a minimal time, the redundant prime implicants need to first be eliminated.
This is the purpose of the argument <b><code>row.dom</code></b>. When activated, it eliminates the
dominated rows (those which cover a smaller number of columns than another, dominant prime
implicant).
</p>
<p>The identification of the full model space (including the non-minimal solutions) requires the
entire PI chart and is guaranteed to consume a lot of time (towards infinity for very large PI
charts). This is done by activating the argument <b><code>all.sol</code></b>, which automatically
deactivates the argument <b><code>row.dom</code></b>.
</p>
<p>The argument <b><code>depth</code></b> is relevant only when the argument <b><code>all.sol</code></b> is
activated, and it is automatically increased if the minimal number of rows <code>M</code> needed to
cover all columns is larger. By default, it bounds the disjunctive solutions to at most 5 prime
implicants, but this number can be increased to widen the search space, with a cost of
increasing the search time.
</p>
<p>The argument <b><code>max.comb</code></b> sets a maximum number of combinations to find solutions.
It is counted in (fractions of) billions, defaulted at zero to signal searching to the maximum
possible extent. If too complex, the search is stopped and the algorithm returns all found solutions
up to that point.
</p>
<p>For extremly difficult PI charts, the argument <b><code>first.min</code></b> controls returning only
one (the very first found) solution.
</p>


<h3>Value</h3>

<p>For <b><code>makeChart</code></b>: a logical matrix of class <code>"QCA_pic"</code>.
</p>
<p>For <b><code>findmin</code></b>: a numerical scalar.
</p>
<p>For <b><code>solveChart</code></b>: a matrix containing all possible combinations of PI chart rows
necessary to cover all its columns.
</p>


<h3>Author(s)</h3>

<p>Adrian Dusa
</p>


<h3>References</h3>

<p>Quine, W.V. (1952) <em>The Problem of Simplifying Truth Functions</em>,
The American Mathematical Monthly, vol.59, no.8. (Oct., 1952), pp.521-531.
</p>
<p>Ragin, Charles C. (1987) <em>The Comparative Method. Moving beyond qualitative and
quantitative strategies</em>, Berkeley: University of California Press
</p>


<h3>Examples</h3>

<pre><code class='language-R'># non-standard products, it needs the set names
chart &lt;- makeChart("a, b, ~c", "abc, a~b~c, a~bc, ~ab~c")

# same with unquoted expressions
chart &lt;- makeChart(c(a, b, ~c), c(abc, a~b~c, a~bc, ~ab~c))

chart
#      abc   a~b~c a~bc  ~ab~c
# a      x     x     x     -  
# b      x     -     -     x  
# ~c     -     x     -     x

findmin(chart)
# 2

solveChart(chart)
# first and second rows (a + b)
# and first and third rows (a + ~c)
# a is an essential prime implicant
#      a + b  a + ~c
#      [,1]   [,2]
# [1,]    1      1
# [2,]    2      3

# using SOP standard product sign
rows &lt;- "EF, ~GH, IJ"
cols &lt;- "~EF*~GH*IJ, EF*GH*~IJ, ~EF*GH*IJ, EF*~GH*~IJ"
chart &lt;- makeChart(rows, cols)
chart
#     ~EF*~GH*IJ EF*GH*~IJ ~EF*GH*IJ EF*~GH*~IJ
# EF      -          x         -         x     
# ~GH     x          -         -         x     
# IJ      x          -         x         -     

solveChart(chart)
# ~GH is redundant
#     EF + IJ
#      [,1]
# [1,]    1
# [2,]    3


# using implicant matrices
primes &lt;- matrix(c(2,2,1,0,2,2,0,2,2,2), nrow = 2)
configs &lt;- matrix(c(2,2,2,1,1,2,2,2,2,1,2,2,2,2,2), nrow = 3)
colnames(primes) &lt;- colnames(configs) &lt;- letters[1:5]

# the prime implicants: a~bce and acde
primes
#      a b c d e
# [1,] 2 1 2 0 2
# [2,] 2 0 2 2 2

# the initial causal combinations: a~bc~de, a~bcde and abcde
configs
#      a b c d e
# [1,] 2 1 2 1 2
# [2,] 2 1 2 2 2
# [3,] 2 2 2 2 2

chartLC &lt;- makeChart(primes, configs, collapse = "")
chartLC
#         a~bc~de a~bcde   abcde  
# a~bce      x       x       -   
# acde       -       x       x   

</code></pre>

<hr>
<h2 id='QCA+20internal+20functions'>Internal Functions</h2><span id='topic+.onAttach'></span><span id='topic+as.panel'></span><span id='topic+combint'></span><span id='topic+minimizeLoop'></span><span id='topic+print.aE'></span><span id='topic+print.chain'></span><span id='topic+print.fuzzy'></span><span id='topic+print.modelFit'></span><span id='topic+print.mqca'></span><span id='topic+print.panel'></span><span id='topic+print.pic'></span><span id='topic+print.pof'></span><span id='topic+print.qca'></span><span id='topic+print.sS'></span><span id='topic+print.tt'></span><span id='topic+rebuild'></span><span id='topic+removeRedundants'></span><span id='topic+replaceCategories'></span><span id='topic+verify.data'></span><span id='topic+verify.dir.exp'></span><span id='topic+verify.minimize'></span><span id='topic+verify.inf.test'></span><span id='topic+verify.qca'></span><span id='topic+verify.mqca'></span><span id='topic+verify.multivalue'></span><span id='topic+verify.tt'></span><span id='topic+d.AS'></span><span id='topic+d.autonomy'></span><span id='topic+d.Bas'></span><span id='topic+d.biodiversity'></span><span id='topic+d.BWB'></span><span id='topic+d.CS'></span><span id='topic+d.CZH'></span><span id='topic+d.educate'></span><span id='topic+d.education'></span><span id='topic+d.Emm'></span><span id='topic+d.graduate'></span><span id='topic+d.health'></span><span id='topic+d.HK'></span><span id='topic+d.HMN'></span><span id='topic+d.homeless'></span><span id='topic+d.jobsecurity'></span><span id='topic+d.Kil'></span><span id='topic+d.Kro'></span><span id='topic+d.napoleon'></span><span id='topic+d.pban'></span><span id='topic+d.partybans'></span><span id='topic+d.represent'></span><span id='topic+d.RS'></span><span id='topic+d.SA'></span><span id='topic+d.socialsecurity'></span><span id='topic+d.SS'></span><span id='topic+d.stakeholder'></span><span id='topic+d.transport'></span><span id='topic+d.urban'></span><span id='topic+d.women'></span><span id='topic+Emme'></span><span id='topic+HarKem'></span><span id='topic+Krook'></span><span id='topic+Quine'></span><span id='topic+RagStr'></span><span id='topic+Rokkan'></span><span id='topic+nec'></span><span id='topic+suf'></span><span id='topic+getSolution'></span><span id='topic+writeSolution'></span><span id='topic+prettyString'></span><span id='topic+rowDominance'></span><span id='topic+sortMatrix'></span><span id='topic+sortVector'></span><span id='topic+mvregexp'></span>

<h3>Description</h3>

<p>The above functions are internal in the QCA package which are not designed to
be called directly by the user. All of them are used by the <code>minimize</code> function,
except <code>sortMatrix</code> which is used by <code>allExpressions</code>. The verification
and error messages have been moved to the internal functions <code>verify.data</code>
and <code>verify.tt</code>.
</p>

<hr>
<h2 id='retention'>Compute the retention probability of a csQCA solution</h2><span id='topic+retention'></span>

<h3>Description</h3>

<p>This function computes the retention probability for a csQCA solution, under various
perturbation scenarios. It only works with  bivalent crisp-set data, containing the
binary values 0 or 1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>retention(data, outcome = "", conditions = "", incl.cut = 1, n.cut = 1,
         type = "corruption", dependent = TRUE, p.pert = 0.5, n.pert = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="retention_+3A_data">data</code></td>
<td>
<p>A dataset of bivalent crisp-set factors.</p>
</td></tr>
<tr><td><code id="retention_+3A_outcome">outcome</code></td>
<td>
<p>The name of the outcome.</p>
</td></tr>
<tr><td><code id="retention_+3A_conditions">conditions</code></td>
<td>
<p>A string containing the condition variables' names,
separated by commas.</p>
</td></tr>
<tr><td><code id="retention_+3A_incl.cut">incl.cut</code></td>
<td>
<p>The minimum sufficiency inclusion score for an output function 
value of &quot;1&quot;.</p>
</td></tr>
<tr><td><code id="retention_+3A_n.cut">n.cut</code></td>
<td>
<p>The minimum number of cases for a causal combination with a set
membership score above 0.5, for an output function value of &quot;0&quot; or &quot;1&quot;.</p>
</td></tr>
<tr><td><code id="retention_+3A_type">type</code></td>
<td>
<p>Simulate corruptions of values in the conditions (&quot;corruption&quot;),
or cases deleted entirely (&quot;deletion&quot;).</p>
</td></tr>
<tr><td><code id="retention_+3A_dependent">dependent</code></td>
<td>
<p>Logical, if <b><code>TRUE</code></b> indicating DPA - Dependent
Perturbations Assumption and if <b><code>FALSE</code></b> indicating
IPA - Independent Perturbations Assumption.</p>
</td></tr>
<tr><td><code id="retention_+3A_p.pert">p.pert</code></td>
<td>
<p>Probability of perturbation under independent (IPA) assumption.</p>
</td></tr>
<tr><td><code id="retention_+3A_n.pert">n.pert</code></td>
<td>
<p>Number of perturbations under dependent (DPA) assumption.</p>
</td></tr>
<tr><td><code id="retention_+3A_...">...</code></td>
<td>
<p>Other arguments, mainly for internal use.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The argument <b><code>data</code></b> requires a suitable data set, in the form of a data frame.
with the following structure: values of 0 and 1 for bivalent crisp-set variables.
</p>
<p>The argument <b><code>outcome</code></b> specifies the outcome to be explained, in upper-case
notation (e.g. <b><code>X</code></b>). 
</p>
<p>The argument <b><code>conditions</code></b> specifies the names of the condition variables.
If omitted, all variables in <b><code>data</code></b> are used except <b><code>outcome</code></b>.
</p>
<p>The argument <b><code>type</code></b> controls which type of perturbations should be simulated
to calculate the retention probability.
When <b><code>type = "corruption"</code></b>, it simulates changes of values in the conditions
(values of 0 become 1, and values of 1 become 0). When <b><code>type = "deletion"</code></b>,
it calculates the probability of retaining the same solution if a number of cases are
deleted from the original data.
</p>
<p>The argument <b><code>dependent</code></b> is a logical which choses between two categories of assumptions. 
If <b><code>dependent = TRUE</code></b> (the default) it indicates DPA - Dependent Perturbations Assumption,
when perturbations depend on each other and are tied to a fixed number of cases, ex-ante
(see Thiem, Spohel and Dusa, 2016).
If <b><code>dependent = FALSE</code></b>, it indicates IPA - Independent Perturbations Assumption, when
perturbations are assumed to occur independently of each other.
</p>
<p>The argument <b><code>n.cut</code></b> is one of the factors that decide which configurations
are coded as logical remainders or not, in conjunction with argument <b><code>incl.cut</code></b>.
Those configurations that contain fewer than <b><code>n.cut</code></b> cases with membership scores
above 0.5 are coded as logical remainders (<b><code>OUT = "?"</code></b>). If the number of such
cases is at least <b><code>n.cut</code></b>, configurations with an inclusion score of at least
<b><code>incl.cut</code></b> are coded positive (<b><code>OUT = "1"</code></b>), while configurations with an
inclusion score below <b><code>incl.cut</code></b> are coded negative (<b><code>OUT = "0"</code></b>).
</p>
<p>The argument <b><code>p.pert</code></b> specifies the probability of perturbation under the
IPA - independent perturbations assumption (when <b><code>dependent = FALSE</code></b>).
</p>
<p>The argument <b><code>n.pert</code></b> specifies the number of perturbations under the
DPA - dependent perturbations assumption (when <b><code>dependent = TRUE</code></b>). At least
one perturbation is needed to possibly change a csQCA solution, otherwise the solution will
remain the same (retention equal to 100%) if zero perturbations occur under this argument.
</p>


<h3>Author(s)</h3>

<p>Adrian Dusa
</p>


<h3>References</h3>

<p>Thiem, A.; Spoehel, R.; Dusa, A. (2015) &ldquo;Replication Package for:
Enhancing Sensitivity Diagnostics for Qualitative Comparative Analysis: A Combinatorial
Approach&rdquo;, Harvard Dataverse, V1. <a href="https://doi.org/10.7910/DVN/QE27H9">doi:10.7910/DVN/QE27H9</a>
</p>
<p>Thiem, A.; Spoehel, R.; Dusa, A. (2016) &ldquo;Enhancing Sensitivity
Diagnostics for Qualitative Comparative Analysis: A Combinatorial Approach.&rdquo;
<em>Political Analysis</em> vol.24, no.1, pp.104-120.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># the replication data, see Thiem, Spohel and Dusa (2015)
dat &lt;- data.frame(matrix(c(
    rep(1, 25), rep(0, 20), rep(c(0, 0, 1, 0, 0), 3),
    0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, rep(1, 7), 0, 1),
    nrow = 16, byrow = TRUE, dimnames = list(
    c("AT", "DK", "FI", "NO", "SE", "AU", "CA", "FR",
      "US", "DE", "NL", "CH", "JP", "NZ", "IE", "BE"),
    c("P", "U", "C", "S", "W"))
))


# calculate the retention probability, for 2.5% probability of data corruption
# under the IPA - independent perturbation assuption
retention(dat, outcome = "W", incl.cut = 1, type = "corruption",
       dependent = FALSE, p.pert = 0.025)

# the probability that a csQCA solution will change
1 - retention(dat, outcome = "W", incl.cut = 1, type = "corruption",
       dependent = FALSE, p.pert = 0.025)

</code></pre>

<hr>
<h2 id='runGUI'>run the GUI shiny app for the QCA package</h2><span id='topic+runGUI'></span>

<h3>Description</h3>

<p>Runs the graphical user interface app based on the <span class="pkg">shiny</span> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>runGUI(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="runGUI_+3A_x">x</code></td>
<td>
<p>Path to the shiny app.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is a wrapper to the <b><code><a href="shiny.html#topic+runApp">runApp</a>()</code></b> function in package
<span class="pkg">shiny</span>. If <b><code>x</code></b> is not provided, it automatically locates the <b><code>gui</code></b>
directory in the path where the QCA package has been installed, and runs it.
</p>
<p>The user interface has an interactive R console in the webpage. Commands are
parsed and evaluated into a dedicated environment, with efforts to capture errors and
warnings.
</p>


<h3>Author(s)</h3>

<p>Adrian Dusa
</p>

<hr>
<h2 id='superSubset+2C+20findSubsets+2C+20findSupersets'>Functions to find subsets or supersets</h2><span id='topic+superSubset'></span><span id='topic+findSubsets'></span><span id='topic+findSupersets'></span>

<h3>Description</h3>

<p>Functions to find a list of implicants that satisfy some restrictions (see details),
or to find  the corresponding row numbers in the implicant matrix, for all subsets,
or supersets, of a (prime) implicant or an initial causal configuration.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>superSubset(data, outcome = "", conditions = "", relation = "necessity",
    incl.cut = 1, cov.cut = 0, ron.cut = 0, pri.cut = 0, depth = NULL,
    use.letters = FALSE, use.labels = FALSE, add = NULL, ...)

findSubsets(input, noflevels = NULL, stop = NULL, ...)

findSupersets(input, noflevels = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="superSubset+2B2C+2B20findSubsets+2B2C+2B20findSupersets_+3A_data">data</code></td>
<td>
<p>A data frame with crisp (binary and multi-value) or fuzzy
causal conditions</p>
</td></tr>
<tr><td><code id="superSubset+2B2C+2B20findSubsets+2B2C+2B20findSupersets_+3A_outcome">outcome</code></td>
<td>
<p>The name of the outcome.</p>
</td></tr>
<tr><td><code id="superSubset+2B2C+2B20findSubsets+2B2C+2B20findSupersets_+3A_conditions">conditions</code></td>
<td>
<p>A string containing the conditions' names,
separated by commas.</p>
</td></tr>
<tr><td><code id="superSubset+2B2C+2B20findSubsets+2B2C+2B20findSupersets_+3A_relation">relation</code></td>
<td>
<p>The set relation to <b><code>outcome</code></b>, either
<b><code>"necessity"</code></b>, <b><code>"sufficiency"</code></b>,
<b><code>"necsuf"</code></b> or <b><code>"sufnec"</code></b>. Partial
words like <b><code>"suf"</code></b> are accepted.</p>
</td></tr>
<tr><td><code id="superSubset+2B2C+2B20findSubsets+2B2C+2B20findSupersets_+3A_incl.cut">incl.cut</code></td>
<td>
<p>The minimal inclusion score of the set relation.</p>
</td></tr>
<tr><td><code id="superSubset+2B2C+2B20findSubsets+2B2C+2B20findSupersets_+3A_cov.cut">cov.cut</code></td>
<td>
<p>The minimal coverage score of the set relation.</p>
</td></tr>
<tr><td><code id="superSubset+2B2C+2B20findSubsets+2B2C+2B20findSupersets_+3A_ron.cut">ron.cut</code></td>
<td>
<p>The minimal score for the <b><code>RoN</code></b> - relevance of necessity.</p>
</td></tr>
<tr><td><code id="superSubset+2B2C+2B20findSubsets+2B2C+2B20findSupersets_+3A_pri.cut">pri.cut</code></td>
<td>
<p>The minimal score for the <b><code>PRI</code></b> - proportional reduction in
inconsistency.</p>
</td></tr>
<tr><td><code id="superSubset+2B2C+2B20findSubsets+2B2C+2B20findSupersets_+3A_use.letters">use.letters</code></td>
<td>
<p>Logical, use simple letters instead of original conditions'
names.</p>
</td></tr>
<tr><td><code id="superSubset+2B2C+2B20findSubsets+2B2C+2B20findSupersets_+3A_use.labels">use.labels</code></td>
<td>
<p>Logical, use category labels if present.</p>
</td></tr>
<tr><td><code id="superSubset+2B2C+2B20findSubsets+2B2C+2B20findSupersets_+3A_noflevels">noflevels</code></td>
<td>
<p>A vector containing the number of levels for each causal
condition plus 1 (all subsets are located in the higher dimension, implicant
matrix)</p>
</td></tr>
<tr><td><code id="superSubset+2B2C+2B20findSubsets+2B2C+2B20findSupersets_+3A_input">input</code></td>
<td>
<p>A vector of row numbers where the (prime) implicants are located, or a 
matrix of configurations (only for supersets).</p>
</td></tr>
<tr><td><code id="superSubset+2B2C+2B20findSubsets+2B2C+2B20findSupersets_+3A_stop">stop</code></td>
<td>
<p>The maximum line number (subset) to stop at, and return</p>
</td></tr>
<tr><td><code id="superSubset+2B2C+2B20findSubsets+2B2C+2B20findSupersets_+3A_depth">depth</code></td>
<td>
<p>Integer, an upper number of causal conditions to form expressions with.</p>
</td></tr>
<tr><td><code id="superSubset+2B2C+2B20findSubsets+2B2C+2B20findSupersets_+3A_add">add</code></td>
<td>
<p>A function, or a list containing functions, to add more parameters of fit.</p>
</td></tr>
<tr><td><code id="superSubset+2B2C+2B20findSubsets+2B2C+2B20findSupersets_+3A_...">...</code></td>
<td>
<p>Other arguments, mainly for backward compatibility.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <b><code><a href="#topic+superSubset">superSubset</a>()</code></b> finds a list of implicants that satisfy
some restrictions referring to the inclusion and coverage with respect to the outcome,
under given assumptions of necessity and/or sufficiency.
</p>
<p>Ragin (2000) posits that under the necessity relation, instances of the outcome
constitute a subset of the instances of the cause(s). Conversely, under the
sufficiency relation, instances of the outcome constitute a superset of the instances
of the cause(s).
</p>
<p>When <b><code>relation = "necessity"</code></b> the function finds all implicants which are
supersets of the  outcome, then eliminates the redundant ones and returns the
surviving (minimal) supersets, provided they pass the inclusion and coverage
thresholds. If none of the surviving supersets pass these thresholds, the function
will find disjunctions of causal conditions, instead of conjunctions.
</p>
<p>When <b><code>relation = "sufficiency"</code></b> it finds all implicants which are subsets
of the outcome, and similarly eliminates the redundant ones and return the surviving
(minimal) subsets.
</p>
<p>When <b><code>relation = "necsuf"</code></b>, the relation is interpreted as necessity, and
<b><code>cov.cut</code></b> is automatically set equal to the inclusion cutoff
<b><code>incl.cut</code></b>. The same automatic equality is made for
<b><code>relation = "sufnec"</code></b>, when relation is interpreted as sufficiency.
</p>
<p>The argument <b><code>outcome</code></b> specifies the name of the outcome, and if
multi-value the argument can also specify the level to explain, using square brackets
notation.
</p>
<p>Outcomes can be negated using a tilde operator <b><code>~X</code></b>. The logical argument
<b><code>neg.out</code></b> is now deprecated, but still backwards compatible. Replaced by
the tilde in front of the outcome name, it controls whether <b><code>outcome</code></b> is
to be explained or its negation. If <b><code>outcome</code></b> is from a multivalent
variable, it has the effect that the disjunction of all remaining values becomes the
new outcome to be explained. <b><code>neg.out = TRUE</code></b> and a tilde <b><code>~</code></b>
in the outcome name don't cancel each other out, either one (or even both) signaling
if the <b><code>outcome</code></b> should be negated.
</p>
<p>If the argument <b><code>conditions</code></b> is not specified, all other columns in
<b><code>data</code></b> are used.
</p>
<p>Along with the standard measures of inclusion and coverage, the function also returns
<b><code>PRI</code></b> for sufficiency and <b><code>RoN</code></b> (relevance of necessity, see
Schneider &amp; Wagemann, 2012) for the necessity relation.
</p>
<p>A subset is a conjunction (an intersection) of causal conditions, with respect to a
larger (super)set, which is another (but more parsimonious) conjunction of causal
conditions.
</p>
<p>All subsets of a given set can be found in the so called &ldquo;implicant matrix&rdquo;,
which is a <code class="reqn">n^k</code> space, understood as all possible combinations of values in any
combination of bases <code class="reqn">n</code>, each causal condition having three or more levels
(Dusa, 2007, 2010).
</p>
<p>For every two levels of a binary causal conditions (values 0 and 1), there are three
levels in the implicants matrix:
</p>

<table>
<tr>
 <td style="text-align: right;">
     0 </td><td style="text-align: left;"> to mark a minimized literal</td>
</tr>
<tr>
 <td style="text-align: right;">
     1 </td><td style="text-align: left;"> to replace the value of 0 in the original binary condition</td>
</tr>
<tr>
 <td style="text-align: right;">
    -1 </td><td style="text-align: left;"> to replace the value of 1 in the original binary condition
</td>
</tr>

</table>

<p>A prime implicant is a superset of an initial combination of causal conditions, and 
the reverse is also true: the initial combination is a subset of a prime implicant.
</p>
<p>Any normal implicant (not prime) is a subset of a prime implicant, and in the
same time a superset of some initial causal combinations.
</p>
<p>Functions <b><code><a href="#topic+findSubsets">findSubsets</a>()</code></b> and <b><code><a href="#topic+findSupersets">findSupersets</a>()</code></b> find:
</p>

<table>
<tr>
 <td style="text-align: right;">
  </td><td style="text-align: left;"> - all possible such subsets for a given (prime) implicant, or</td>
</tr>
<tr>
 <td style="text-align: right;">
  </td><td style="text-align: left;"> - all possible supersets of an implicant or initial causal combination
</td>
</tr>

</table>

<p>in the implicant matrix.
</p>
<p>The argument <b><code>depth</code></b> can be used to impose an upper number of causal
conditions to form expressions with, it is the complexity level where the search is
stopped. Depth is set to a maximum by default, and the algorithm will always stop at
the maximum complexity level where no new, non-redundant prime implicants are found.
Reducing the depth below that maximum will also reduce computation time.
</p>
<p>For examples on how to add more parameters of fit via argument <b><code>add</code></b>, see
the function <b><code><a href="#topic+pof">pof</a>()</code></b>.
</p>


<h3>Value</h3>

<p>The result of the <b><code><a href="#topic+superSubset">superSubset</a>()</code></b> function is an object of class &quot;ss&quot;,
which is a list with the following components:
</p>
<table>
<tr><td><code>incl.cov</code></td>
<td>
<p>A data frame with the parameters of fit.</p>
</td></tr>
<tr><td><code>coms</code></td>
<td>
<p>A data frame with the (m)embersip (s)cores of the resulting
(co)mbinations.</p>
</td></tr>
</table>
<p>For <b><code><a href="#topic+findSubsets">findSubsets</a>()</code></b> and <b><code><a href="#topic+findSupersets">findSupersets</a>()</code></b>, a vector with the
row numbers corresponding to all possible subsets, or supersets, of a (prime)
implicant.
</p>


<h3>Author(s)</h3>

<p>Adrian Dusa
</p>


<h3>References</h3>

<p>Cebotari, V.; Vink, M.P. (2013) &ldquo;A Configurational Analysis of Ethnic
Protest in Europe&rdquo;. <em>International Journal of Comparative Sociology</em>
vol.54, no.4, pp.298-324, <a href="https://doi.org/10.1177/0020715213508567">doi:10.1177/0020715213508567</a>.
</p>
<p>Cebotari, Victor; Vink, Maarten Peter (2015) <em>Replication Data for: A
configurational analysis of ethnic protest in Europe</em>, Harvard Dataverse, V2,
<a href="https://doi.org/10.7910/DVN/PT2IB9">doi:10.7910/DVN/PT2IB9</a>.
</p>
<p>Dusa, A. (2007b) <em>Enhancing Quine-McCluskey</em>.
WP 2007-49, <a href="https://compasss.org/working-papers-series/">COMPASSS Working Papers series</a>.
</p>
<p>Dusa, Adrian (2010) &ldquo;A Mathematical Approach to the Boolean Minimization 
Problem.&rdquo; <em>Quality &amp; Quantity</em> vol.44, no.1, pp.99-113, <a href="https://doi.org/10.1007/s11135-008-9183-x">doi:10.1007/s11135-008-9183-x</a>.
</p>
<p>Lipset, S. M. (1959) &ldquo;Some Social Requisites of Democracy: Economic Development
and Political Legitimacy&rdquo;, <em>American Political Science Review</em> vol.53, pp.69-105.
</p>
<p>Schneider, Carsten Q.; Wagemann, Claudius (2012) <em>Set-Theoretic Methods 
for the Social Sciences: A Guide to Qualitative Comparative Analysis (QCA)</em>. 
Cambridge: Cambridge University Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+createMatrix">createMatrix</a></code>, <code><a href="#topic+getRow">getRow</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>    
# Lipset binary crisp sets
ssLC &lt;- superSubset(LC, "SURV")

library(venn)
x = list("SURV" = which(LC$SURV == 1),
         "STB" = which(ssLC$coms[, 1] == 1),
         "LIT" = which(ssLC$coms[, 2] == 1))
venn(x, cexil = 0.7)

# Lipset multi-value sets
superSubset(LM, "SURV")

# Cebotari &amp; Vink (2013) fuzzy data
# all necessary combinations with at least 0.9 inclusion and 0.6 coverage cut-offs
ssCVF &lt;- superSubset(CVF, outcome = "PROTEST", incl.cut = 0.90, cov.cut = 0.6)
ssCVF

# the membership scores for the first minimal combination (GEOCON)
ssCVF$coms$GEOCON

# same restrictions, for the negation of the outcome
superSubset(CVF, outcome = "~PROTEST", incl.cut = 0.90, cov.cut = 0.6)

# to find supersets or supersets, a hypothetical example using
# three binary causal conditions, having two levels each: 0 and 1
noflevels &lt;- c(2, 2, 2)

# second row of the implicant matrix: 0 0 1
# which in the "normal" base is:      - - 0
# the prime implicant being: ~C
(sub &lt;- findSubsets(input = 2, noflevels + 1))
#  5  8 11 14 17 20 23 26 


getRow(sub, noflevels + 1)

# implicant matrix   normal values
#      a  b  c    |       a  b  c       
#   5  0  1  1    |    5  -  0  0      ~b~c    
#   8  0  2  1    |    8  -  1  0      b~c
#  11  1  0  1    |   11  0  -  0      ~a~c
#  14  1  1  1    |   14  0  0  0      ~a~b~c
#  17  1  2  1    |   17  0  1  0      ~ab~c
#  20  2  0  1    |   20  1  -  0      a~c
#  23  2  1  1    |   23  1  0  0      a~b~c               
#  26  2  2  1    |   26  1  1  0      ab~c 


# stopping at maximum row number 20
findSubsets(input = 2, noflevels + 1, stop = 20)
#  5  8 11 14 17 20


# -----
# for supersets
findSupersets(input = 14, noflevels + 1)
#  2  4  5 10 11 13 14

findSupersets(input = 17, noflevels + 1)
#  2  7  8 10 11 16 17

# input as a matrix
(im &lt;- getRow(c(14, 17), noflevels + 1))

# implicant matrix   normal values
#  14  1  1  1    |   14  0  0  0       ~a~b~c
#  17  1  2  1    |   17  0  1  0       ~ab~c


sup &lt;- findSupersets(input = im, noflevels + 1)
sup
#  2  4  5  7  8 10 11 13 14 16 17


getRow(sup, noflevels + 1)

# implicant matrix   normal values
#      a  b  c    |       a  b  c       
#   2  0  0  1    |    2  -  -  0       ~c      
#   4  0  1  0    |    4  -  0  -       ~b
#   5  0  1  1    |    5  -  0  0       ~b~c
#   7  0  2  0    |    7  -  1  -       b
#   8  0  2  1    |    8  -  1  0       b~c
#  10  1  0  0    |   10  0  -  -       ~a  
#  11  1  0  1    |   11  0  -  0       ~a~c                 
#  13  1  1  0    |   13  0  0  -       ~a~b   
#  14  1  1  1    |   14  0  0  0       ~a~b~c
#  16  1  2  0    |   16  0  1  -       ~ab
#  17  1  2  1    |   17  0  1  0       ~ab~c
                             
</code></pre>

<hr>
<h2 id='truthTable'>Create a truth table</h2><span id='topic+truthTable'></span>

<h3>Description</h3>

<p>Function to create a truth table from all types of calibrated data (binary
crisp, multi-value crisp and fuzzy).
For fuzzy data, an improved verson of Ragin's (2008) procedure is applied to
assign cases to the vector space corners (the truth table rows).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>truthTable(data, outcome = "", conditions = "", incl.cut = 1, n.cut = 1, pri.cut = 0,
           exclude = NULL, complete = FALSE, use.letters = FALSE, use.labels = FALSE,
           show.cases = FALSE, dcc = FALSE, sort.by = "", inf.test = "", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="truthTable_+3A_data">data</code></td>
<td>
<p>A data frame containing calibrated causal conditions and an outcome.</p>
</td></tr>
<tr><td><code id="truthTable_+3A_outcome">outcome</code></td>
<td>
<p>String, the name of the outcome.</p>
</td></tr>
<tr><td><code id="truthTable_+3A_conditions">conditions</code></td>
<td>
<p>A single string containing the conditions' (columns) names
separated by commas, or a character vector of conditions' names.</p>
</td></tr>
<tr><td><code id="truthTable_+3A_incl.cut">incl.cut</code></td>
<td>
<p>The inclusion cut-off(s): either a single value for the presence of the
output, or a vector of length 2, the second for the absence of the output.</p>
</td></tr>
<tr><td><code id="truthTable_+3A_n.cut">n.cut</code></td>
<td>
<p>The minimum number of cases under which a truth table row is
declared as a remainder.</p>
</td></tr>
<tr><td><code id="truthTable_+3A_pri.cut">pri.cut</code></td>
<td>
<p>The minimal score for the <b><code>PRI</code></b> - proportional reduction in
inconsistency, under which a truth table row is declared as negative.</p>
</td></tr>
<tr><td><code id="truthTable_+3A_exclude">exclude</code></td>
<td>
<p>A vector of (remainder) row numbers from the truth table, to code as
negative output configurations.</p>
</td></tr>
<tr><td><code id="truthTable_+3A_complete">complete</code></td>
<td>
<p>Logical, print complete truth table.</p>
</td></tr>
<tr><td><code id="truthTable_+3A_use.letters">use.letters</code></td>
<td>
<p>Logical, use letters instead of causal conditions' names.</p>
</td></tr>
<tr><td><code id="truthTable_+3A_use.labels">use.labels</code></td>
<td>
<p>Logical, use category labels if present.</p>
</td></tr>
<tr><td><code id="truthTable_+3A_show.cases">show.cases</code></td>
<td>
<p>Logical, print case names.</p>
</td></tr>
<tr><td><code id="truthTable_+3A_dcc">dcc</code></td>
<td>
<p>Logical, if <b><code>show.cases = TRUE</code></b>, the cases being
displayed are the deviant cases consistency in kind.</p>
</td></tr>
<tr><td><code id="truthTable_+3A_sort.by">sort.by</code></td>
<td>
<p>Sort the truth table according to various columns.</p>
</td></tr>
<tr><td><code id="truthTable_+3A_inf.test">inf.test</code></td>
<td>
<p>Specifies the statistical inference test to be performed
(currently only <b><code>"binom"</code></b>) and the critical significance level.
It can be either a vector of length 2, or a single string containing both,
separated by a comma.</p>
</td></tr>
<tr><td><code id="truthTable_+3A_...">...</code></td>
<td>
<p>Other arguments (mainly for backward compatibility).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <b><code>data</code></b> should always be provided as a data frame, with calibrated columns.
</p>
<p>Calibration can be either crisp, with 2 or more values starting from 0, or fuzzy with
continous scores from 0 to 1. Raw data containing relative frequencies can
also be continous between 0 and 1, but these are not calibrated, fuzzy data.
</p>
<p>Some columns can contain the placeholder <b><code>"-"</code></b> indicating a &ldquo;don't care&rdquo;,
which is used to indicate the temporal order between other columns in tQCA. These
special columns are not causal conditions, hence no parameters of fit will be
calculated for them.
</p>
<p>The argument <b><code>outcome</code></b> specifies the column name to be explained.
If the outcome is a multivalue column, it can be specified in curly bracket notation,
indicating the value to be explained (the others being automatically converted to
zero).
</p>
<p>The outcome can be negated using a tilde operator <b><code>~X</code></b>. The logical
argument <b><code>neg.out</code></b> is now deprecated, but still backwards compatible.
Replaced by the tilde in front of the outcome name, it controls whether <b><code>outcome</code></b>
is to be explained or its negation. Note that using both <b><code>neg.out = TRUE</code></b> and a
tilde <b><code>~</code></b> in the outcome name cancel each other out.
</p>
<p>If the outcome column is multi-value, the argument <b><code>outcome</code></b> should use
the standard curly-bracket notation <b><code>X{value}</code></b>. Multiple values are
allowed, separated by a comma (for example <b><code>X{1,2}</code></b>). Negation of the
outcome can also be performed using the tilde <b><code>~</code></b> operator, for example
<b><code>~X{1,2}</code></b>, which is interpreted as: &quot;all values in X except 1 and 2&quot;
and it becomes the new outcome to be explained.
</p>
<p>The argument <b><code>conditions</code></b> specifies the causal conditions' names among the other
columns in the data. When this argument is not specified, all other columns except for
the outcome are taken as causal conditions.
</p>
<p>A good practice advice is to specify both <b><code>outcome</code></b> and
<b><code>conditions</code></b> as upper case letters. It is possible, in a next version,
to negate outcomes using lower case letters, a situation where it really does
matter how the outcome and/or conditions are specified.
</p>
<p>The argument <b><code>incl.cut</code></b> replaces both (deprecated, but still backwards
compatible) former arguments <b><code>incl.cut1</code></b> and <b><code>incl.cut0</code></b>.
Most of the analyses use the inclusion cutoff for the presence of the output
(code <b><code>"1"</code></b>). When users need both inclusion cutoffs (see below),
<b><code>incl.cut</code></b> can be specified as a vector of length 2, in the form:
<b><code>c(ic1, ic0)</code></b> where:
</p>

<table>
<tr>
 <td style="text-align: right;">
<b><code>ic1</code></b> </td><td style="text-align: left;"> is the inclusion cutoff for the presence of the output,</td>
</tr>
<tr>
 <td style="text-align: right;">
                  </td><td style="text-align: left;"> a minimum sufficiency inclusion score above which the output
                       value is coded with <code>"1"</code>.</td>
</tr>
<tr>
 <td style="text-align: right;"> </td>
</tr>
<tr>
 <td style="text-align: right;">
<b><code>ic0</code></b> </td><td style="text-align: left;"> is the inclusion cutoff for the absence of the output,</td>
</tr>
<tr>
 <td style="text-align: right;">
                  </td><td style="text-align: left;"> a maximum sufficiency inclusion score below which the output
                       value is coded with <code>"0"</code>.</td>
</tr>
<tr>
 <td style="text-align: right;">
</td>
</tr>

</table>

<p>If not specifically declared, the argument <b><code>ic0</code></b> is automatically set
equal to <b><code>ic1</code></b>, but otherwise <b><code>ic0</code></b> should always be lower
than <b><code>ic1</code></b>.
</p>
<p>Using these two cutoffs, as well as <b><code>pri.cut</code></b> the observed combinations are
coded with:
</p>

<table>
<tr>
 <td style="text-align: right;">
<b><code>"1"</code></b> </td><td style="text-align: left;"> if they have an inclusion score of at least <b><code>ic1</code></b></td>
</tr>
<tr>
 <td style="text-align: right;">
                  </td><td style="text-align: left;"> and a PRI score of at least <b><code>pri.cut</code></b></td>
</tr>
<tr>
 <td style="text-align: right;"> </td>
</tr>
<tr>
 <td style="text-align: right;">
<b><code>"C"</code></b> </td><td style="text-align: left;"> if they have an inclusion score below <b><code>ic1</code></b> and
                       at least <b><code>ic0</code></b> (contradiction)</td>
</tr>
<tr>
 <td style="text-align: right;"> </td>
</tr>
<tr>
 <td style="text-align: right;">
<b><code>"0"</code></b> </td><td style="text-align: left;"> if they have an inclusion score below <b><code>ic0</code></b> or </td>
</tr>
<tr>
 <td style="text-align: right;">
                  </td><td style="text-align: left;"> a PRI score below <b><code>pri.cut</code></b>
</td>
</tr>

</table>

<p>The argument <b><code>n.cut</code></b> specifies the frequency threshold under which a truth
table row is coded as a remainder, irrespective of its inclusion score.
</p>
<p>When argument <b><code>show.cases</code></b> is set to <b><code>TRUE</code></b>, the case names will be
printed at their corresponding row in the truth table. The resulting object always contains
the cases for each causal combination, even if not printed on the screen (the print function
can later be used to print them).
</p>
<p>The <b><code>sort.by</code></b> argument orders all configurations by any of the columns
present in the truth table. Typically, sorting occurs by their outcome value,
and/or by their inclusion score, and/or by their frequency, in any order.
</p>
<p>Sorting decreasingly (the default) or increasingly can be specified adding the signs
<b><code>-</code></b> or <b><code>+</code></b>, next after the column name in argument
<b><code>sort.by</code></b> (see examples). Note that <b><code>-</code></b> is redundant because
it is the default anyways.
</p>
<p>The order specified in this vector is the order in which the configurations will
be sorted. When sorting based on the OUTput column, remainders will always be sorted last.
</p>
<p>The argument <b><code>use.letters</code></b> controls using the original names of the causal
conditions, or replace them by single letters in alphabetical order. If the
causal conditions are already named with single letters, the original letters
will be used.
</p>
<p>The argument <b><code>inf.test</code></b> combines the inclusion score with a statistical
inference test, in order to assign values in the output column OUT. For the moment, it
is only the binomial test, which needs crisp data (it doesn't work with fuzzy sets).
Following a similar logic as above, for a given (specified) critical significance level,
the output for a truth table row will be coded as:
</p>

<table>
<tr>
 <td style="text-align: right;">
<b><code>"1"</code></b> </td><td style="text-align: left;"> if the true inclusion score is significanly higher than
                       <b><code>ic1</code></b>,</td>
</tr>
<tr>
 <td style="text-align: right;"> </td>
</tr>
<tr>
 <td style="text-align: right;">
<b><code>"C"</code></b> </td><td style="text-align: left;"> contradiction, if the true inclusion score is not significantly
                       higher than <b><code>ic1</code></b></td>
</tr>
<tr>
 <td style="text-align: right;">
                  </td><td style="text-align: left;"> but significantly higher than <b><code>ic0</code></b>,</td>
</tr>
<tr>
 <td style="text-align: right;"> </td>
</tr>
<tr>
 <td style="text-align: right;">
<b><code>"0"</code></b> </td><td style="text-align: left;"> if the true inclusion score is not significantly higher than
                       <b><code>ic0</code></b>.</td>
</tr>
<tr>
 <td style="text-align: right;">
</td>
</tr>

</table>

<p>It should be noted that statistical tests perform well only when the number of cases is
large, otherwise they are usually not significant. For a low number of cases, depending
on the inclusion cutoff value(s), it will be harder to code a value of <b><code>"1"</code></b>
in the output, and also harder to obtain contradictions if the true inclusion is not
signficantly higher than <b><code>ic0</code></b>.
</p>
<p>The argument <b><code>exclude</code></b> is used to exclude truth table rows from the
minimization process, from the positive configurations and/or from the remainders. This
is achieved by coding those configurations with a value of 0 in the OUTput column (thus
treating them as if they were observed as negative output configurations).
</p>
<p>The argument <b><code>complete</code></b> controls how to print the table on the screen, either
complete (when set to <b><code>TRUE</code></b>), or just the observed combinations (default).
For up to 7 causal conditions, the resulting object will always contain the complete
truth table, even if it's not printed on the screen. This is useful for multiple reasons:
researchers like to manually change output values in the truth table (sometimes including
in this way a remainder, for example), and it is also useful to plot Venn diagrams, each
truth table row having a correspondent intersection in the diagram.
</p>


<h3>Value</h3>

<p>An object of class <code>"tt"</code>, a list containing the following components:
</p>

<table>
<tr>
 <td style="text-align: right;">
<b>tt</b> </td><td style="text-align: left;"> The truth table itself.</td>
</tr>
<tr>
 <td style="text-align: right;">
<b>indexes</b> </td><td style="text-align: left;"> The line numbers for the observed causal configurations.</td>
</tr>
<tr>
 <td style="text-align: right;">
<b>noflevels</b> </td><td style="text-align: left;"> A vector with the number of values for each causal condition.</td>
</tr>
<tr>
 <td style="text-align: right;">
<b>initial.data</b> </td><td style="text-align: left;"> The initial data.</td>
</tr>
<tr>
 <td style="text-align: right;">
<b>recoded.data</b> </td><td style="text-align: left;"> The crisp version of the <b><code>initial.data</code></b>, if fuzzy.</td>
</tr>
<tr>
 <td style="text-align: right;">
<b>cases</b> </td><td style="text-align: left;"> The cases for each observed causal configuration.</td>
</tr>
<tr>
 <td style="text-align: right;">
<b>DCC</b> </td><td style="text-align: left;"> Deviant cases for consistency.</td>
</tr>
<tr>
 <td style="text-align: right;">
<b>minmat</b> </td><td style="text-align: left;"> The membership scores matrix of cases in the observed truth table combinations.</td>
</tr>
<tr>
 <td style="text-align: right;">
<b>categories</b> </td><td style="text-align: left;"> Category labels, if present in the data.</td>
</tr>
<tr>
 <td style="text-align: right;">
<b>multivalue</b> </td><td style="text-align: left;"> Logical flag, if either conditions or the outcome are multivalue.</td>
</tr>
<tr>
 <td style="text-align: right;">
<b>options</b> </td><td style="text-align: left;"> The command options used.</td>
</tr>
<tr>
 <td style="text-align: right;">
</td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Adrian Dusa
</p>


<h3>References</h3>

<p>Cronqvist, L.; Berg-Schlosser, D. (2009) &ldquo;Multi-Value QCA (mvQCA)&rdquo;, in
Rihoux, B.; Ragin, C. (eds.) <em>Configurational Comparative Methods. Qualitative
Comparative Analysis (QCA) and Related Techniques</em>, SAGE.
</p>
<p>Dusa, A. (2019) <em>QCA with R. A Comprehensive Resource</em>.
Springer International Publishing, <a href="https://doi.org/10.1007/978-3-319-75668-4">doi:10.1007/978-3-319-75668-4</a>.
</p>
<p>Lipset, S.M. (1959) &ldquo;Some Social Requisites of Democracy: Economic Development
and Political Legitimacy&rdquo;, <em>American Political Science Review</em> vol.53, pp.69-105.
</p>
<p>Ragin, C.C. (1987) <em>The Comparative Method: Moving beyond Qualitative and
Quantitative Strategies</em>. Berkeley: University of California Press.
</p>
<p>Ragin, C.C. (2008) <em>Redesigning Social Inquiry: Fuzzy Sets and Beyond</em>.
Chicago: University of Chicago Press.
</p>
<p>Ragin, C.C.; Strand, S.I. (2008) &ldquo;Using Qualitative Comparative
Analysis to Study Causal Order: Comment on Caren and Panofsky (2005).&rdquo;
<em>Sociological Methods &amp; Research</em> vol.36, no.4, pp.431-441.
</p>
<p>Schneider, C.Q.; Wagemann, C. (2012) <em>Set-Theoretic Methods
for the Social Sciences: A Guide to Qualitative Comparative Analysis (QCA)</em>.
Cambridge: Cambridge University Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+minimize">minimize</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># -----
# Lipset binary crisp data
ttLC &lt;- truthTable(LC, "SURV")

# inspect the truth table
ttLC

# print the cases too, even if not specifically asked for
print(ttLC, show.cases = TRUE)

# the printing function also supports the complete version
print(ttLC, show.cases = TRUE, complete = TRUE)

# formally asking the complete version
truthTable(LC, "SURV", complete = TRUE)

# sorting by multiple columns, decreasing by default
truthTable(LC, "SURV", complete = TRUE, sort.by = "incl, n")

# sort the truth table decreasing for inclusion, and increasing for n
# note that "-" is redundant, sorting is decreasing by default
truthTable(LC, "SURV", complete = TRUE, sort.by = "incl-, n+")


# -----
# Lipset multi-value crisp data (Cronqvist &amp; Berg-Schlosser 2009, p.80)
truthTable(LM, "SURV", sort.by = "incl")

# using a frequency cutoff equal to 2 cases
ttLM &lt;- truthTable(LM, "SURV", n.cut = 2, sort.by = "incl")
ttLM

# the observed combinations coded as remainders
ttLM$removed


# -----
# Cebotari &amp; Vink fuzzy data
ttCVF &lt;- truthTable(CVF, "PROTEST", incl.cut = 0.8, sort.by = "incl")

# view the Venn diagram for this truth table
library(venn)
venn(ttCVF)

# each intersection transparent by its inclusion score
venn(ttCVF, transparency = ttCVF$tt$incl)

# the truth table negating the outcome
truthTable(CVF, "~PROTEST", incl.cut = 0.8, sort.by = "incl")

# allow contradictions
truthTable(CVF, "PROTEST", incl.cut = c(0.8, 0.75), sort.by = "incl")


# -----
# Ragin and Strand data with temporal QCA
# truth table containing the "-" placeholder as a "don't care"
truthTable(RS, "REC")

</code></pre>

<hr>
<h2 id='Xplot'>Display the distribution of points for a single condition</h2><span id='topic+Xplot'></span>

<h3>Description</h3>

<p>This function creates a plot for a single vector of numerical values, arranging them
horizontally on the X axis from minimum to maximum.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Xplot(x, jitter = FALSE, at = pretty(x), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Xplot_+3A_x">x</code></td>
<td>
<p>A numeric vector.</p>
</td></tr>
<tr><td><code id="Xplot_+3A_jitter">jitter</code></td>
<td>
<p>Logical, vertically jitter the points.</p>
</td></tr>
<tr><td><code id="Xplot_+3A_at">at</code></td>
<td>
<p>The points at which tick-marks are to be drawn. Non-finite (infinite, NaN or NA)
values are omitted. By default, tickmark locations are automatically
computed, see the help file for <b><code>?pretty</code></b>.</p>
</td></tr>
<tr><td><code id="Xplot_+3A_...">...</code></td>
<td>
<p>Other graphical parameters from <b><code>?par</code></b></p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a special type of (scatter)plot, with points being arranged only on the
horizontal axis (it has no vertical axis). Useful when inspecting if points
are grouped into naturally occuring clusters, mainly for crisp calibration purposes.
</p>
<p>The argument <b><code>...</code></b> is used to pass arguments to the various graphical
parameters from <code>?par</code>, and also to the settings from <code>?jitter</code>.
</p>
<p>The points have a default <b><code>cex</code></b> (character expansion) value of 1, and
a default <b><code>pch</code></b> value of 1 (empty points), which can be modified
accordingly (for instance value 21 for filled points). When <b><code>pch = 21</code></b>,
the color for the margins of the points can be specified via the argument
<b><code>col</code></b>, while the argument <b><code>bg</code></b> will determine the fill color
of the points.
</p>
<p>The axis labels have a default <b><code>cex.axis</code></b> value of 0.8, which affects
both the tickmarks labels and the axis labels.
</p>
<p>When jittering the points, default values of 0.5 are used for the parameters
<b><code>factor</code></b> and <b><code>amount</code></b>, on the horizontal axis. More details
can be found in the base function <code><a href="base.html#topic+jitter">jitter</a>()</code>.
</p>
<p>Although the points are displayed in a single dimension, on the horizontal axis,
the R graphical window will still have the default squared shape, with a lot of 
empty space on the vertical axis. Users are free to create their custom code to 
determine the size of the graphics window, or simply resize it to a suitable
height.
</p>


<h3>Author(s)</h3>

<p>Adrian Dusa
</p>


<h3>See Also</h3>

<p><code><a href="graphics.html#topic+par">par</a></code>, <code><a href="graphics.html#topic+text">text</a></code>, <code><a href="base.html#topic+jitter">jitter</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Lipset's raw data
# plot the DEV (level of developent) causal condition
Xplot(LR$DEV)

# jitter the points vertically
Xplot(LR$DEV, jitter = TRUE)

# clip plotting between the range of min and max
Xplot(LR$DEV, jitter = TRUE, at = range(LR$DEV))
</code></pre>

<hr>
<h2 id='XYplot'>Create an XY plot</h2><span id='topic+XYplot'></span>

<h3>Description</h3>

<p>This function creates an XY plot from the first two columns of a dataframe/matrix, or from
two separate vectors of numeric values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>XYplot(x, y, data, relation = "sufficiency", mguides = TRUE,
       jitter = FALSE, clabels, enhance = FALSE, model = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="XYplot_+3A_x">x</code></td>
<td>
<p>Character, the name of the column from the <b><code>data</code></b> for the X axis,
or the coordinates of points in the plot (either a matrix/dataframe with at least
two columns, or a vector of numerical values for the X axis), or a valid SOP
expression.</p>
</td></tr>
<tr><td><code id="XYplot_+3A_y">y</code></td>
<td>
<p>Character, the name of the column from the <b><code>data</code></b> for the Y axis,
or the Y coordinates of points in the plot, optional if <b><code>x</code></b>
is a matrix/dataframe.</p>
</td></tr>
<tr><td><code id="XYplot_+3A_data">data</code></td>
<td>
<p>A calibrated dataset, only if <b><code>x</code></b> and <b><code>y</code></b> are names.</p>
</td></tr>
<tr><td><code id="XYplot_+3A_relation">relation</code></td>
<td>
<p>The set relation to Y, either <b><code>"sufficiency"</code></b> (default) or
<b><code>"necessity"</code></b>.</p>
</td></tr>
<tr><td><code id="XYplot_+3A_mguides">mguides</code></td>
<td>
<p>Logical, print the middle guides.</p>
</td></tr>
<tr><td><code id="XYplot_+3A_jitter">jitter</code></td>
<td>
<p>Logical, jitter the points.</p>
</td></tr>
<tr><td><code id="XYplot_+3A_clabels">clabels</code></td>
<td>
<p>A vector of case labels with the same length as <b><code>x</code></b> and
<b><code>y</code></b>, or a logical vector of the same length as the number of rows
in the <code>data</code> (if provided).</p>
</td></tr>
<tr><td><code id="XYplot_+3A_enhance">enhance</code></td>
<td>
<p>Logical, if <b><code>TRUE</code></b> print the points using different characters for each
of the five significant regions for process tracing.</p>
</td></tr>
<tr><td><code id="XYplot_+3A_model">model</code></td>
<td>
<p>Logical, for an enhanced plot specify if the SOP expression in argument
<b><code>x</code></b> is a solution model.</p>
</td></tr>
<tr><td><code id="XYplot_+3A_...">...</code></td>
<td>
<p>Other graphical parameters from <b><code>?par</code></b></p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <b><code>x</code></b> is a dataframe or a matrix, the axes labels will be taken
from the column names of <b><code>x</code></b>, otherwise they will be inferred from
the names of the <b><code>x</code></b> and <b><code>y</code></b> objects that are passed to
this function.
</p>
<p><b><code>x</code></b> can also be a string containing either the name of the column
for the X axis, or two column names separated by a comma, referring to the X
and Y axis respectively. When <b><code>x</code></b> contains both X and Y column names,
the next argument will be considered as the <b><code>data</code></b>.
</p>
<p>If <b><code>data</code></b> is provided, and the names of the X and Y columns are valid
R statements, quoting them is not even necessary and they can be negated using
either a tilde <b><code>"~"</code></b> or <b><code>"1 - "</code></b>.
</p>
<p>The numeric values should be restricted between 0 and 1, otherwise an error is
generated.
</p>
<p>The XY plot will also provide inclusion and coverage scores for a sufficiency
(along with <b><code>PRI</code></b>) or a necessity relation (along with <b><code>RoN</code></b>).
</p>
<p>The argument <b><code>x</code></b> can also be a SOP - sum of products expression, in which
case the relation is determined by the usual forward arrow <b><code>"=&gt;"</code></b> for
sufficiency and backward arrow <b><code>"&lt;="</code></b> for necessity.
</p>
<p>The argument <b><code>...</code></b> is used to pass arguments to the various graphical
parameters from <code>?par</code>, and also to the settings from <code>?jitter</code>.
</p>
<p>The points have a default <b><code>cex</code></b> (character expansion) value of 0.8, and
a default <b><code>pch</code></b> value of 21 (filled points), which can be modified
accordingly (for example with value 1 of empty points). When <b><code>pch = 21</code></b>,
the color for the margins of the points can be specified via the argument
<b><code>col</code></b>, while the argument <b><code>bg</code></b> will determine the fill color
of the points.
</p>
<p>The axes' labels have a default <b><code>cex.axis</code></b> value of 0.8, which affects
both the tickmarks labels and the axis labels.
</p>
<p>When jittering the points, default values of 0.01 are used for the parameters
<b><code>factor</code></b> and <b><code>amount</code></b>, on both horizontal and vertical axes.
</p>
<p>The argument <b><code>enhance</code></b> does all the work for the shape of the points and
their colors, acording to the five regions specified by Schneider &amp; Rohlfing (2016),
who augmented the classical XY plot with process tracing.
</p>
<p>The default enhanced XY plot has even more settings when the input SOP expression is
a minimization model (different colors, different regions where to place the labels etc.),
available by activating the argument <b><code>model</code></b>. The model is automatically
detected if the input for <b><code>x</code></b> is a minimization object.
</p>


<h3>Value</h3>

<p>A list of x and y values, especially useful when the points are jittered. 
</p>


<h3>Author(s)</h3>

<p>Adrian Dusa
</p>


<h3>References</h3>

<p>Schneider, C.; Wagemann, C. (2012) <em>Set-Theoretic Metods for the Social Sciences.
A Guide to Qualitative Comparative Analysis</em>. Cambridge: Cambridge University Press.
</p>
<p>Cebotari, V.; Vink, M.P. (2013) &ldquo;A Configurational Analysis of Ethnic
Protest in Europe&rdquo;. <em>International Journal of Comparative Sociology</em>
vol.54, no.4, pp.298-324.
</p>
<p>Schneider, C.; Rohlfing, I. (2016) &ldquo;Case Studies Nested in Fuzzy-set QCA on
Sufficiency. Formalizing Case Selection and Causal Inference&rdquo;. <em>Sociological
Methods and Research</em> vol.45, no.3, pp.536-568, <a href="https://doi.org/10.1177/0049124114532446">doi:10.1177/0049124114532446</a>
</p>


<h3>See Also</h3>

<p><code><a href="graphics.html#topic+par">par</a></code>, <code><a href="graphics.html#topic+text">text</a></code>, <code><a href="base.html#topic+jitter">jitter</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Cebotari &amp; Vink (2013)
# necessity relation between NATPRIDE and PROTEST
XYplot(CVF[, 5:6])

# same using two numeric vectors
XYplot(CVF$NATPRIDE, CVF$PROTEST)

# same using two column names
XYplot(NATPRIDE, PROTEST, data = CVF)

# since they are valid R statements, it works even without quotes
# (this only works in normal R console, not in the GUI version)
XYplot(NATPRIDE, PROTEST, data = CVF)

# negating the X axis, using numeric vectors
XYplot(1 - CVF$NATPRIDE, CVF$PROTEST)

# same thing using quotes
XYplot(1 - NATPRIDE, PROTEST, data = CVF)

# using tilde for negation
XYplot(~NATPRIDE, PROTEST, data = CVF)

# different color for the points
XYplot(~NATPRIDE, PROTEST, data = CVF, col = "blue")

# using a different character expansion for the axes
XYplot(~NATPRIDE, PROTEST , data = CVF, cex.axis = 0.9)

# custom axis labels
XYplot(~NATPRIDE, PROTEST, data = CVF, xlab = "Negation of NATPRIDE",
       ylab = "Outcome: PROTEST")

# necessity relation
XYplot(~NATPRIDE, PROTEST, data = CVF, relation = "necessity")

# jitter the points
XYplot(~NATPRIDE, PROTEST, data = CVF, jitter = TRUE)

# jitter with more amount
XYplot(~NATPRIDE, PROTEST, data = CVF, jitter = TRUE, amount = 0.02)

# adding labels to points
XYplot(~NATPRIDE, PROTEST, data = CVF, jitter = TRUE, cex = 0.8,
       clabels = rownames(CVF))

# or just the row numbers, since the row names are too long
XYplot(~NATPRIDE, PROTEST, data = CVF, jitter = TRUE, cex = 0.8,
       clabels = seq(nrow(CVF)))


# using a SOP expression (necessity relation)
XYplot(NATPRIDE &lt;- ~PROTEST, data = CVF, jitter = TRUE, cex = 0.8,
       clabels = seq(nrow(CVF)))


#-----
# enhanced XY plot for process tracing
XYplot(~NATPRIDE, PROTEST, data = CVF, enhance = TRUE, jitter = TRUE)


# enhanced XY plot for a solution model
ttCVF &lt;- truthTable(CVF, outcome = PROTEST, incl.cut = 0.85)
pCVF &lt;- minimize(ttCVF, include = "?")
XYplot(pCVF$solution[[1]], PROTEST, data = CVF, enhance = TRUE)


# same plot, using the solution as a SOP expression
XYplot(~NATPRIDE + DEMOC*GEOCON*POLDIS + DEMOC*ETHFRACT*GEOCON,
      PROTEST, data = CVF, enhance = TRUE, model = TRUE)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
