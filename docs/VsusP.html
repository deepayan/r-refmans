<!DOCTYPE html><html lang="en"><head><title>Help for package VsusP</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {VsusP}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#numNoiseCoeff'><p>Variable selection using shrinkage priors :: numNoiseCoeff</p></a></li>
<li><a href='#OptimalHbi'><p>Variable selection using shrinkage priors :: OptimalHbi</p></a></li>
<li><a href='#S2MVarSelection'><p>Variable selection using shrinkage priors :: S2MVarSelection</p></a></li>
<li><a href='#S2MVarSelectionV1'><p>Variable selection using shrinkage priors :: S2MVarSelectionV1</p></a></li>
<li><a href='#Sequential2Means'><p>Variable selection using shrinkage priors :: Sequential2Means</p></a></li>
<li><a href='#Sequential2MeansBeta'><p>Variable selection using shrinkage prior :: Sequential2MeansBeta</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Variable Selection using Shrinkage Priors</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Bayesian variable selection using shrinkage priors to identify significant variables in high-dimensional datasets. The package includes methods for determining the number of significant variables through innovative clustering techniques of posterior distributions, specifically utilizing the 2-Means and Sequential 2-Means (S2M) approaches. The package aims to simplify the variable selection process with minimal tuning required in statistical analysis.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>Imports:</td>
<td>bayesreg, stats</td>
</tr>
<tr>
<td>Suggests:</td>
<td>covr, MASS, knitr, rmarkdown, tinytex, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Nilson Chapagain &lt;nilson.chapagain@gmail.com&gt;</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/nilson01/VsusP-variable-selection-using-shrinkage-priors">https://github.com/nilson01/VsusP-variable-selection-using-shrinkage-priors</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/nilson01/VsusP-variable-selection-using-shrinkage-priors/issues">https://github.com/nilson01/VsusP-variable-selection-using-shrinkage-priors/issues</a></td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-06-24 08:16:11 UTC; nilson</td>
</tr>
<tr>
<td>Author:</td>
<td>Nilson Chapagain <a href="https://orcid.org/0000-0003-2962-2949"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Debdeep Pati [aut]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-06-25 14:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='numNoiseCoeff'>Variable selection using shrinkage priors :: numNoiseCoeff</h2><span id='topic+numNoiseCoeff'></span>

<h3>Description</h3>

<p>Variable selection using shrinkage priors :: numNoiseCoeff
</p>


<h3>Usage</h3>

<pre><code class='language-R'>numNoiseCoeff(Beta.i, b.i_r)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="numNoiseCoeff_+3A_beta.i">Beta.i</code></td>
<td>
<p>N by p matrix consisting of N posterior samples of p variables</p>
</td></tr>
<tr><td><code id="numNoiseCoeff_+3A_b.i_r">b.i_r</code></td>
<td>
<p>tuning parameter value from Sequential 2-means (S2M) variable selection algorithm.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>number of noise coefficients of numeric data type
</p>

<hr>
<h2 id='OptimalHbi'>Variable selection using shrinkage priors :: OptimalHbi</h2><span id='topic+OptimalHbi'></span>

<h3>Description</h3>

<p>OptimalHbi function will take b.i and H.b.i as input which comes from the result of TwoMeans function. It will return plot from which you can infer about H: the optimal value of the tuning parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>OptimalHbi(bi, Hbi)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="OptimalHbi_+3A_bi">bi</code></td>
<td>
<p>a vector holding the values of the tuning parameter specified by the user</p>
</td></tr>
<tr><td><code id="OptimalHbi_+3A_hbi">Hbi</code></td>
<td>
<p>The estimated number of signals corresponding to each b.i of numeric data type</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the optimal value (numeric) of tuning parameter and the associated H value
</p>


<h3>References</h3>

<p>Makalic, E. &amp; Schmidt, D. F.
High-Dimensional Bayesian Regularised Regression with the BayesReg Package
arXiv:1611.06649, 2016
</p>
<p>Li, H., &amp; Pati, D.
Variable selection using shrinkage priors
Computational Statistics &amp; Data Analysis, 107, 107-119.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 10
p &lt;- 5
X &lt;- matrix(rnorm(n * p), n, p)
beta &lt;- exp(rnorm(p))
Y &lt;- as.vector(X %*% beta + rnorm(n, 0, 1))
df &lt;- data.frame(X, Y)
rv.hs &lt;- bayesreg::bayesreg(Y ~ ., df, "gaussian", "horseshoe+", 110, 100)

Beta &lt;- t(rv.hs$beta)
lower &lt;- 0
upper &lt;- 1
l &lt;- 5
S2Mbeta &lt;- Sequential2MeansBeta(Beta, lower, upper, l)

bi &lt;- S2Mbeta$b.i
Hbi &lt;- S2Mbeta$H.b.i
OptimalHbi(bi, Hbi)

</code></pre>

<hr>
<h2 id='S2MVarSelection'>Variable selection using shrinkage priors :: S2MVarSelection</h2><span id='topic+S2MVarSelection'></span>

<h3>Description</h3>

<p>S2MVarSelection function will take S2M: a list obtained from the 2Means.variables function and H: the estimated number of signals obtained from the optimal.H.b.i function. This will give out the important subset of variables for the Gaussian Linear model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>S2MVarSelection(Beta, H = 5)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="S2MVarSelection_+3A_beta">Beta</code></td>
<td>
<p>matrix consisting of N posterior samples of p variables that is known either to user or from Sequential2Means function</p>
</td></tr>
<tr><td><code id="S2MVarSelection_+3A_h">H</code></td>
<td>
<p>Estimated number of signals obtained from the optimal.b.i function of numeric data type</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector containing indices of important subset of variables of dimension H X 1.
</p>


<h3>References</h3>

<p>Makalic, E. &amp; Schmidt, D. F.
High-Dimensional Bayesian Regularised Regression with the BayesReg Package
arXiv:1611.06649, 2016
</p>
<p>Li, H., &amp; Pati, D.
Variable selection using shrinkage priors
Computational Statistics &amp; Data Analysis, 107, 107-119.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 10
p &lt;- 5
X &lt;- matrix(rnorm(n * p), n, p)
beta &lt;- exp(rnorm(p))
Y &lt;- as.vector(X %*% beta + rnorm(n, 0, 1))
df &lt;- data.frame(X, Y)
# Fit a model using gaussian horseshoe+ for 200 samples
# # recommended n.samples is 5000 and burning is 2000
rv.hs &lt;- bayesreg::bayesreg(Y ~ ., df, "gaussian", "horseshoe+", 110, 100)

Beta &lt;- rv.hs$beta
H &lt;- 3
impVariablesGLM &lt;- S2MVarSelection(Beta, H)
impVariablesGLM

</code></pre>

<hr>
<h2 id='S2MVarSelectionV1'>Variable selection using shrinkage priors :: S2MVarSelectionV1</h2><span id='topic+S2MVarSelectionV1'></span>

<h3>Description</h3>

<p>S2MVarSelectionV1 function will take S2M: a list obtained from the 2Means.variables function and H: the estimated number of signals obtained from the optimal.b.i function. This will give out the important subset of variables for the Gaussian Linear model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>S2MVarSelectionV1(S2M, H = 5)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="S2MVarSelectionV1_+3A_s2m">S2M</code></td>
<td>
<p>List obtained from the 2Means.variables function</p>
</td></tr>
<tr><td><code id="S2MVarSelectionV1_+3A_h">H</code></td>
<td>
<p>Estimated number (numeric) of signals, obtained from the optimal.b.i function (default = newValue)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector of indices of important subset of variables for the Gaussian Linear modelof shape H X 1
</p>

<hr>
<h2 id='Sequential2Means'>Variable selection using shrinkage priors :: Sequential2Means</h2><span id='topic+Sequential2Means'></span>

<h3>Description</h3>

<p>Sequential2Means function will take as input X: design matrix, Y : response vector, t: vector of tuning parameter values from Sequential 2-means (S2M) variable selection algorithm. The function will return a list S2M which will hold p: the total number of variables, b.i: the values of the tuning parameter, H.b.i : the estimated number of signals corresponding to each b.i, abs.post.median: medians of the absolute values of the posterior samples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Sequential2Means(
  X,
  Y,
  b.i,
  prior = "horseshoe+",
  n.samples = 5000,
  burnin = 2000
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Sequential2Means_+3A_x">X</code></td>
<td>
<p>Design matrix of dimension n X p, where n = total data points and p = total number of features</p>
</td></tr>
<tr><td><code id="Sequential2Means_+3A_y">Y</code></td>
<td>
<p>Response vector of dimension n X 1</p>
</td></tr>
<tr><td><code id="Sequential2Means_+3A_b.i">b.i</code></td>
<td>
<p>Vector of tuning parameter values from Sequential 2-means (S2M) variable selection algorithm of dimension specified by user.</p>
</td></tr>
<tr><td><code id="Sequential2Means_+3A_prior">prior</code></td>
<td>
<p>Shrinkage prior distribution over the Beta. Available options are ridge regression: prior=&quot;rr&quot; or prior=&quot;ridge&quot;, lasso regression: prior=&quot;lasso&quot;, horseshoe regression: prior=&quot;hs&quot; or prior=&quot;horseshoe&quot;, and horseshoe+ regression : prior=&quot;hs+&quot; or prior=&quot;horseshoe+&quot; ( String data type)</p>
</td></tr>
<tr><td><code id="Sequential2Means_+3A_n.samples">n.samples</code></td>
<td>
<p>Number of posterior samples to generate of numeric data type</p>
</td></tr>
<tr><td><code id="Sequential2Means_+3A_burnin">burnin</code></td>
<td>
<p>Number of burn-in samples of numeric data type</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list S2M which will hold Beta, b.i, and H.b.i.
</p>
<table role = "presentation">
<tr><td><code>Beta</code></td>
<td>
<p>N by p matrix consisting of N posterior samples of p variables</p>
</td></tr>
<tr><td><code>b.i</code></td>
<td>
<p>the user specified vector holding the tuning parameter values</p>
</td></tr>
<tr><td><code>H.b.i</code></td>
<td>
<p>the estimated number of signals of numeric data type corresponding to each b.i</p>
</td></tr>
</table>


<h3>References</h3>

<p>Makalic, E. &amp; Schmidt, D. F.
High-Dimensional Bayesian Regularised Regression with the BayesReg Package
arXiv:1611.06649, 2016
</p>
<p>Li, H., &amp; Pati, D.
Variable selection using shrinkage priors
Computational Statistics &amp; Data Analysis, 107, 107-119.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># -----------------------------------------------------------------
# Example 1: Gaussian Model and Horseshoe prior
n &lt;- 10
p &lt;- 5
X &lt;- matrix(rnorm(n * p), n, p)
beta &lt;- exp(rnorm(p))
Y &lt;- as.vector(X %*% beta + rnorm(n, 0, 1))
b.i &lt;- seq(0, 1, 0.05)

# Sequential2Means with horseshoe+ using gibbs sampling
# recommended n.samples is 5000 and burning is 2000
S2M &lt;- Sequential2Means(X, Y, b.i, "horseshoe+", 110, 100)
Beta &lt;- S2M$Beta
H.b.i &lt;- S2M$H.b.i

# -----------------------------------------------------------------
# Example 2: Gaussian Model and ridge prior

n &lt;- 10
p &lt;- 5
X &lt;- matrix(rnorm(n * p), n, p)
beta &lt;- exp(rnorm(p))
Y &lt;- as.vector(X %*% beta + rnorm(n, 0, 1))
b.i &lt;- seq(0, 1, 0.05)
# Sequential2Means with ridge regression using gibbs sampling
# recommended n.samples is 5000 and burning is 2000
S2M &lt;- Sequential2Means(X, Y, b.i, "ridge", 110, 100)
Beta &lt;- S2M$Beta
H.b.i &lt;- S2M$H.b.i

</code></pre>

<hr>
<h2 id='Sequential2MeansBeta'>Variable selection using shrinkage prior :: Sequential2MeansBeta</h2><span id='topic+Sequential2MeansBeta'></span>

<h3>Description</h3>

<p>Sequential2MeansBeta function will take as input Beta : N by p matrix consisting of N posterior samples of p variables, lower : the lower bound of the chosen values of the tuning parameter, upper : the upper bound of the chosen values of the tuning parameter, and l :the number of chosen values of the tuning parameter. The function will return a list S2M which will hold p: the total number of variables, b.i: the values of the tuning parameter, H.b.i : the estimated number of signals corresponding to each b.i, abs.post.median: medians of the absolute values of the posterior samples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Sequential2MeansBeta(Beta, lower, upper, l)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Sequential2MeansBeta_+3A_beta">Beta</code></td>
<td>
<p>N by p matrix consisting of N posterior samples of p variables</p>
</td></tr>
<tr><td><code id="Sequential2MeansBeta_+3A_lower">lower</code></td>
<td>
<p>the lower bound of the chosen values of the tuning parameter of numeric data type.</p>
</td></tr>
<tr><td><code id="Sequential2MeansBeta_+3A_upper">upper</code></td>
<td>
<p>the upper bound of the chosen values of the tuning parameter of numeric data type.</p>
</td></tr>
<tr><td><code id="Sequential2MeansBeta_+3A_l">l</code></td>
<td>
<p>the number of chosen values of the tuning parameter of numeric data type.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list S2M which will hold p, b.i, and H.b.i:
</p>
<table role = "presentation">
<tr><td><code>p</code></td>
<td>
<p>total number of variables in the model</p>
</td></tr>
<tr><td><code>b.i</code></td>
<td>
<p>the vector values of the tuning parameter specified by the user</p>
</td></tr>
<tr><td><code>H.b.i</code></td>
<td>
<p>the estimated number of signals corresponding to each b.i of numeric data type</p>
</td></tr>
</table>


<h3>References</h3>

<p>Makalic, E. &amp; Schmidt, D. F.
High-Dimensional Bayesian Regularised Regression with the BayesReg Package
arXiv:1611.06649, 2016
</p>
<p>Li, H., &amp; Pati, D.
Variable selection using shrinkage priors
Computational Statistics &amp; Data Analysis, 107, 107-119.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# -----------------------------------------------------------------
# Example 1: Gaussian Model and Horseshoe prior

n &lt;- 10
p &lt;- 5
X &lt;- matrix(rnorm(n * p), n, p)
beta &lt;- exp(rnorm(p))
Y &lt;- as.vector(X %*% beta + rnorm(n, 0, 1))
df &lt;- data.frame(X, Y)

# beta samples for gaussian model using horseshow prior and gibbs sampling
rv.hs &lt;- bayesreg::bayesreg(Y ~ ., df, "gaussian", "horseshoe+", 110, 100)

Beta &lt;- t(rv.hs$beta)
lower &lt;- 0
upper &lt;- 1
l &lt;- 20
S2Mbeta &lt;- Sequential2MeansBeta(Beta, lower, upper, l)
H.b.i &lt;- S2Mbeta$H.b.i

# -----------------------------------------------------------------
# Example 2: normal model and lasso prior

#' n &lt;- 10
p &lt;- 5
X &lt;- matrix(rnorm(n * p), n, p)
beta &lt;- exp(rnorm(p))
Y &lt;- as.vector(X %*% beta + rnorm(n, 0, 1))
df &lt;- data.frame(X, Y)
rv.hs &lt;- bayesreg::bayesreg(Y ~ ., df, "normal", "lasso", 150, 100)

Beta &lt;- t(rv.hs$beta)
lower &lt;- 0
upper &lt;- 1
l &lt;- 15
S2Mbeta &lt;- Sequential2MeansBeta(Beta, lower, upper, l)
H.b.i &lt;- S2Mbeta$H.b.i

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
