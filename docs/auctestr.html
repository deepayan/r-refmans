<!DOCTYPE html><html><head><title>Help for package auctestr</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {auctestr}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#auc_compare'><p>Compare AUC values using the FBH method.</p></a></li>
<li><a href='#auctestr'><p>auctestr: Statistical Testing for AUC data.</p></a></li>
<li><a href='#fbh_test'><p>Apply z-test for difference between auc_1 and auc_2 using FBH method.</p></a></li>
<li><a href='#sample_experiment_data'><p>Performance of several predictive models over three different datasets, using multiple cutoff points for time within each dataset.</p></a></li>
<li><a href='#se_auc'><p>Compute standard error of AUC score, using its equivalence to the Wilcoxon statistic.</p></a></li>
<li><a href='#stouffer_z'><p>Compute aggregate z-score using Stouffer's method.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Statistical Testing for AUC Data</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Josh Gardner &lt;jpgard@umich.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Performs statistical testing to compare predictive 
	models based on multiple observations of the A' statistic (also known as 
	Area Under the Receiver Operating Characteristic Curve, or AUC). 
	Specifically, it implements a testing method based on the equivalence 
	between the A' statistic and the Wilcoxon statistic. 
	For more information, see Hanley and McNeil (1982) &lt;<a href="https://doi.org/10.1148%2Fradiology.143.1.7063747">doi:10.1148/radiology.143.1.7063747</a>&gt;. </td>
</tr>
<tr>
<td>Imports:</td>
<td>dplyr, tidyr</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.3.1)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.0.1</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, testthat</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2017-11-12 18:53:55 UTC; joshgardner</td>
</tr>
<tr>
<td>Author:</td>
<td>Josh Gardner [aut, cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2017-11-13 09:46:18 UTC</td>
</tr>
</table>
<hr>
<h2 id='auc_compare'>Compare AUC values using the FBH method.</h2><span id='topic+auc_compare'></span>

<h3>Description</h3>

<p>Apply the FBH method to compare <code>outcome_col</code> by <code>compare_col</code>, averaging
over <code>time_col</code> (due to non-independence) and then over <code>over_col</code> by
using Stouffer's Method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>auc_compare(df, compare_values, filter_value, time_col = "time",
  outcome_col = "auc", compare_col = "model_id", over_col = "dataset",
  n_col = "n", n_p_col = "n_p", n_n_col = "n_n",
  filter_col = "model_variant")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="auc_compare_+3A_df">df</code></td>
<td>
<p>DataFrame containing <code>time_col</code>, <code>outcome_col</code>,
<code>compare_col</code>, and <code>over_col</code>.</p>
</td></tr>
<tr><td><code id="auc_compare_+3A_compare_values">compare_values</code></td>
<td>
<p>names of models to compare (character vector of length 2).
These should match exactly the names as they appear in compare_col.</p>
</td></tr>
<tr><td><code id="auc_compare_+3A_filter_value">filter_value</code></td>
<td>
<p>(optional) keep only observations which contain
<code>filter_value</code> for <code>filter_col</code>.</p>
</td></tr>
<tr><td><code id="auc_compare_+3A_time_col">time_col</code></td>
<td>
<p>name of column in df representing time of observations
(z-scores are averaged over time_col within each model/dataset due to
non-independence). These can also be other dependent groupings, such as
cross-validation folds.</p>
</td></tr>
<tr><td><code id="auc_compare_+3A_outcome_col">outcome_col</code></td>
<td>
<p>name of column in df representing outcome to compare; this should
be Area Under the Receiver Operating Characteristic or A' statistic (this method
applies specifically to AUC and not other metrics (i.e., sensitivity, precision, F1)..</p>
</td></tr>
<tr><td><code id="auc_compare_+3A_compare_col">compare_col</code></td>
<td>
<p>name of column in df representing two conditions to compare
(should contain at least 2 unique values; these two values are specified as
<code>compare_values</code>).</p>
</td></tr>
<tr><td><code id="auc_compare_+3A_over_col">over_col</code></td>
<td>
<p>identifier for independent experiments, iterations, etc. over which
z-scores for models are to be compared (using Stouffer's Z).</p>
</td></tr>
<tr><td><code id="auc_compare_+3A_n_col">n_col</code></td>
<td>
<p>name of column in df with total number of observations in the sample
tested by each row.</p>
</td></tr>
<tr><td><code id="auc_compare_+3A_n_p_col">n_p_col</code></td>
<td>
<p>name of column in df with n_p, number of positive observations.</p>
</td></tr>
<tr><td><code id="auc_compare_+3A_n_n_col">n_n_col</code></td>
<td>
<p>name of column in df with n_n, number of negative observations.</p>
</td></tr>
<tr><td><code id="auc_compare_+3A_filter_col">filter_col</code></td>
<td>
<p>(optional) name of column in df to filter observations on; keep only
observations which contain <code>filter_value</code> for <code>filter_col</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric, overall z-score of comparison using the FBH method.
</p>


<h3>References</h3>

<p>Fogarty, Baker and Hudson, Case Studies in the use of ROC Curve Analysis
for Sensor-Based Estimates in Human Computer Interaction,
Proceedings of Graphics Interface (2005) pp. 129-136.
</p>
<p>Stouffer, S.A.; Suchman, E.A.; DeVinney, L.C.; Star, S.A.;
Williams, R.M. Jr. The American Soldier, Vol.1: Adjustment during Army Life (1949).
</p>


<h3>See Also</h3>

<p>Other fbh method: <code><a href="#topic+fbh_test">fbh_test</a></code>,
<code><a href="#topic+se_auc">se_auc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load sample experiment data
data(sample_experiment_data)
## compare VariantA of ModelA and ModelB
auc_compare(sample_experiment_data,
    compare_values = c('ModelA', 'ModelB'),
    filter_value = c('VariantA'),
    time_col = 'time',
    outcome_col = 'auc',
    compare_col = 'model_id',
    over_col = 'dataset',
    filter_col = 'model_variant')
## compare VariantC of ModelA and ModelB
auc_compare(sample_experiment_data,
    compare_values = c('ModelA', 'ModelB'),
    filter_value = c('VariantC'),
    time_col = 'time',
    outcome_col = 'auc',
    compare_col = 'model_id',
    over_col = 'dataset',
    filter_col = 'model_variant')
## compare ModelC, VariantA and VariantB
auc_compare(sample_experiment_data,
    compare_values = c('VariantA', 'VariantB'),
    filter_value = c('ModelC'),
    time_col = 'time',
    outcome_col = 'auc',
    compare_col = 'model_variant',
    over_col = 'dataset',
    filter_col = 'model_id')
</code></pre>

<hr>
<h2 id='auctestr'>auctestr: Statistical Testing for AUC data.</h2><span id='topic+auctestr'></span><span id='topic+auctestr-package'></span>

<h3>Description</h3>

<p>auctestr currently provides four main useful functions for statistical testing of the AUC, or A', statistic:
fbh_auc_compare, stouffer_z, fbh_test, and se_auc.
</p>

<hr>
<h2 id='fbh_test'>Apply z-test for difference between auc_1 and auc_2 using FBH method.</h2><span id='topic+fbh_test'></span>

<h3>Description</h3>

<p>Apply z-test for difference between auc_1 and auc_2 using FBH method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fbh_test(auc_1, auc_2, n_p, n_n)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fbh_test_+3A_auc_1">auc_1</code></td>
<td>
<p>value of A' statistic (or AUC, or Area Under the Receiver operating
characteristic curve) for the first group (numeric).</p>
</td></tr>
<tr><td><code id="fbh_test_+3A_auc_2">auc_2</code></td>
<td>
<p>value of A' statistic (or AUC, or Area Under the Receiver operating
characteristic curve) for the second group (numeric).</p>
</td></tr>
<tr><td><code id="fbh_test_+3A_n_p">n_p</code></td>
<td>
<p>number of positive observations (needed for calculation of standard
error of Wilcoxon statistic) (numeric).</p>
</td></tr>
<tr><td><code id="fbh_test_+3A_n_n">n_n</code></td>
<td>
<p>number of negative observations (needed for calculation of standard
error of Wilcoxon statistic) (numeric).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric, single aggregated z-score of comparison A'_1 - A'_2.
</p>


<h3>References</h3>

<p>Fogarty, Baker and Hudson, Case Studies in the use of ROC Curve Analysis
for Sensor-Based Estimates in Human Computer Interaction,
Proceedings of Graphics Interface (2005) pp. 129-136.
</p>


<h3>See Also</h3>

<p>Other fbh method: <code><a href="#topic+auc_compare">auc_compare</a></code>,
<code><a href="#topic+se_auc">se_auc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Two models with identical AUC return z-score of zero
fbh_test(0.56, 0.56, 1000, 2500)
## Compare two models; note that changing order changes sign of z-statistic
fbh_test(0.56, 0.59, 1000, 2500)
fbh_test(0.59, 0.56, 1000, 2500)
</code></pre>

<hr>
<h2 id='sample_experiment_data'>Performance of several predictive models over three different datasets, using multiple cutoff points for time within each dataset.</h2><span id='topic+sample_experiment_data'></span>

<h3>Description</h3>

<p>A dataset containing the performance of several predictive models over three different datasets,
where models are built using data from multiple time points (where time 1 has less data than time 2, but each subsequent
time point T also uses data from all prior time points up to that time t&lt;= T.) This represents the typical
output of a machine learning experiment where several models are being considered across multiple datasets, often
with different variants of each model type being considered (i.e., different hyperparameter settings of each model).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sample_experiment_data
</code></pre>


<h3>Format</h3>

<p>A data frame with 180 rows and 10 variables:
</p>

<dl>
<dt>auc</dt><dd><p>Area Under the Receiver Operating Characteristic Curve, or AUC, for this model configuration.</p>
</dd>
<dt>precision</dt><dd><p>Precision for this model configuration.</p>
</dd>
<dt>accuracy</dt><dd><p>Accuracy for this model configuration.</p>
</dd>
<dt>n</dt><dd><p>Number of observations in this dataset.</p>
</dd>
<dt>n_n</dt><dd><p>Number of negative observations (i.e., outcome == 0) in this dataset (required for standard error estimation of AUC statistic).</p>
</dd>
<dt>n_p</dt><dd><p>Number of positive observations (i.e., outcome == 1) in this dataset (required for standard error estimation of AUC statistic).</p>
</dd>
<dt>dataset</dt><dd><p>indicator for different datasets.</p>
</dd>
<dt>time</dt><dd><p>indicator for different time points used to build each dataset; these represent dependent observations of model performance.</p>
</dd>
<dt>model_id</dt><dd><p>Indicator for the statistical algorithm used (this could be 'Logistic Regression', 'SVM', etc.).</p>
</dd>
<dt>model_variant</dt><dd><p>Indicator for different variants of each model which are not equivalent and should be used individually (model should not be averaged over these, and instead should be held fixed when comparing to other model). Example of this could be various hyperparameter settings for a given model (i.e., cost for an SVM).</p>
</dd>
</dl>

<hr>
<h2 id='se_auc'>Compute standard error of AUC score, using its equivalence to the Wilcoxon statistic.</h2><span id='topic+se_auc'></span>

<h3>Description</h3>

<p>Compute standard error of AUC score, using its equivalence to the Wilcoxon statistic.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>se_auc(auc, n_p, n_n)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="se_auc_+3A_auc">auc</code></td>
<td>
<p>value of A' statistic (or AUC, or Area Under the Receiver operating
characteristic curve) (numeric).</p>
</td></tr>
<tr><td><code id="se_auc_+3A_n_p">n_p</code></td>
<td>
<p>number of positive cases (integer).</p>
</td></tr>
<tr><td><code id="se_auc_+3A_n_n">n_n</code></td>
<td>
<p>number of negative cases (integer).</p>
</td></tr>
</table>


<h3>References</h3>

<p>Hanley and McNeil, The meaning and use of the area under a receiver
operating characteristic (ROC) curve. Radiology (1982) 43 (1) pp. 29-36.
</p>
<p>Fogarty, Baker and Hudson, Case Studies in the use of ROC Curve Analysis
for Sensor-Based Estimates in Human Computer Interaction,
Proceedings of Graphics Interface (2005) pp. 129-136.
</p>


<h3>See Also</h3>

<p>Other fbh method: <code><a href="#topic+auc_compare">auc_compare</a></code>,
<code><a href="#topic+fbh_test">fbh_test</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>se_auc(0.75, 20, 200)
## standard error decreases when data become more balanced over
## positive/negative outcome class, holding sample size fixed
se_auc(0.75, 110, 110)
## standard error increases when sample size shrinks
se_auc(0.75, 20, 20)
</code></pre>

<hr>
<h2 id='stouffer_z'>Compute aggregate z-score using Stouffer's method.</h2><span id='topic+stouffer_z'></span>

<h3>Description</h3>

<p>Compute aggregate z-score using Stouffer's method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stouffer_z(z_vec, ignore.na = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stouffer_z_+3A_z_vec">z_vec</code></td>
<td>
<p>vector of z-scores (numeric).</p>
</td></tr>
<tr><td><code id="stouffer_z_+3A_ignore.na">ignore.na</code></td>
<td>
<p>should NA values be ignored? defaults to TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric, Z-score using Stouffer's method aggregated over <code>z_vec</code>.
</p>


<h3>References</h3>

<p>Stouffer, S.A.; Suchman, E.A.; DeVinney, L.C.; Star, S.A.;
Williams, R.M. Jr. The American Soldier, Vol.1: Adjustment during Army Life (1949).
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
