<!DOCTYPE html><html><head><title>Help for package sparsevb</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {sparsevb}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#sparsevb-package'><p>sparsevb: Spike-and-Slab Variational Bayes for Linear and Logistic Regression</p></a></li>
<li><a href='#svb.fit'><p>Fit Approximate Posteriors to Sparse Linear and Logistic Models</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Spike-and-Slab Variational Bayes for Linear and Logistic
Regression</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2021-1-04</td>
</tr>
<tr>
<td>Author:</td>
<td>Gabriel Clara [aut, cre],
  Botond Szabo [aut],
  Kolyan Ray [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Gabriel Clara &lt;gabriel.j.clara@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements variational Bayesian algorithms to perform scalable variable selection for sparse, high-dimensional linear and logistic regression models. Features include a novel prioritized updating scheme, which uses a preliminary estimator of the variational means during initialization to generate an updating order prioritizing large, more relevant, coefficients. Sparsity is induced via spike-and-slab priors with either Laplace or Gaussian slabs. By default, the heavier-tailed Laplace density is used. Formal derivations of the algorithms and asymptotic consistency results may be found in Kolyan Ray and Botond Szabo (2020) &lt;<a href="https://doi.org/10.1080%2F01621459.2020.1847121">doi:10.1080/01621459.2020.1847121</a>&gt; and Kolyan Ray, Botond Szabo, and Gabriel Clara (2020) &lt;<a href="https://doi.org/10.48550/arXiv.2010.11665">doi:10.48550/arXiv.2010.11665</a>&gt;.</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://gitlab.com/gclara/varpack/-/issues">https://gitlab.com/gclara/varpack/-/issues</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 1.0.5), selectiveInference (&ge; 1.2.5), glmnet (&ge;
4.0-2), stats</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo, RcppEnsmallen</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>C++11</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-01-12 04:11:21 UTC; gclara</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-01-15 09:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='sparsevb-package'>sparsevb: Spike-and-Slab Variational Bayes for Linear and Logistic Regression</h2><span id='topic+sparsevb'></span><span id='topic+sparsevb-package'></span>

<h3>Description</h3>

<p>Implements variational Bayesian algorithms to perform scalable variable selection for sparse, high-dimensional linear and logistic regression models. Features include a novel prioritized updating scheme, which uses a preliminary estimator of the variational means during initialization to generate an updating order prioritizing large, more relevant, coefficients. Sparsity is induced via spike-and-slab priors with either Laplace or Gaussian slabs. By default, the heavier-tailed Laplace density is used. Formal derivations of the algorithms and asymptotic consistency results may be found in Kolyan Ray and Botond Szabo (2020) &lt;doi:10.1080/01621459.2020.1847121&gt; and Kolyan Ray, Botond Szabo, and Gabriel Clara (2020) &lt;arXiv:2010.11665&gt;.
</p>


<h3>Details</h3>

<p>For details as they pertain to using the package, consult the
<code><a href="#topic+svb.fit">svb.fit</a></code> function help page. Detailed descriptions and
derivations of the variational algorithms with Laplace slabs may be found
in the references.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Gabriel Clara <a href="mailto:gabriel.j.clara@gmail.com">gabriel.j.clara@gmail.com</a>
</p>
<p>Authors:
</p>

<ul>
<li><p> Botond Szabo
</p>
</li>
<li><p> Kolyan Ray
</p>
</li></ul>



<h3>References</h3>

 <ul>
<li><p> Ray K. and Szabo B. Variational Bayes for
high-dimensional linear regression with sparse priors. (2020).
<em>Journal of the American Statistical Association</em>. </p>
</li>
<li><p> Ray K., Szabo
B., and Clara G. Spike and slab variational Bayes for high dimensional
logistic regression. (2020). <em>Advances in Neural Information
Processing Systems 33</em>.</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li><p> Report bugs at <a href="https://gitlab.com/gclara/varpack/-/issues">https://gitlab.com/gclara/varpack/-/issues</a>
</p>
</li></ul>


<hr>
<h2 id='svb.fit'>Fit Approximate Posteriors to Sparse Linear and Logistic Models</h2><span id='topic+svb.fit'></span>

<h3>Description</h3>

<p>Main function of the <code><a href="#topic+sparsevb">sparsevb</a></code> package. Computes
mean-field posterior approximations for both linear and logistic regression
models, including variable selection via sparsity-inducing spike and slab
priors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>svb.fit(
  X,
  Y,
  family = c("linear", "logistic"),
  slab = c("laplace", "gaussian"),
  mu,
  sigma = rep(1, ncol(X)),
  gamma,
  alpha,
  beta,
  prior_scale = 1,
  update_order,
  intercept = FALSE,
  noise_sd,
  max_iter = 1000,
  tol = 1e-05
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="svb.fit_+3A_x">X</code></td>
<td>
<p>A numeric design matrix, each row of which represents a vector of
covariates/independent variables/features. Though not required, it is
recommended to center and scale the columns to have norm
<code>sqrt(nrow(X))</code>.</p>
</td></tr>
<tr><td><code id="svb.fit_+3A_y">Y</code></td>
<td>
<p>An <code>nrow(X)</code>-dimensional response vector, numeric if
<code>family = "linear"</code> and binary if <code>family = "logistic"</code>.</p>
</td></tr>
<tr><td><code id="svb.fit_+3A_family">family</code></td>
<td>
<p>A character string selecting the regression model, either
<code>"linear"</code> or <code>"logistic"</code>.</p>
</td></tr>
<tr><td><code id="svb.fit_+3A_slab">slab</code></td>
<td>
<p>A character string specifying the prior slab density, either
<code>"laplace"</code> or <code>"gaussian"</code>.</p>
</td></tr>
<tr><td><code id="svb.fit_+3A_mu">mu</code></td>
<td>
<p>An <code>ncol(X)</code>-dimensional numeric vector, serving as initial
guess for the variational means. If omitted, <code>mu</code> will be estimated
via ridge regression to initialize the coordinate ascent algorithm.</p>
</td></tr>
<tr><td><code id="svb.fit_+3A_sigma">sigma</code></td>
<td>
<p>A positive <code>ncol(X)</code>-dimensional numeric vector, serving as
initial guess for the variational standard deviations.</p>
</td></tr>
<tr><td><code id="svb.fit_+3A_gamma">gamma</code></td>
<td>
<p>An <code>ncol(X)</code>-dimensional vector of probabilities, serving
as initial guess for the variational inclusion probabilities. If omitted,
<code>gamma</code> will be estimated via LASSO regression to initialize the
coordinate ascent algorithm.</p>
</td></tr>
<tr><td><code id="svb.fit_+3A_alpha">alpha</code></td>
<td>
<p>A positive numeric value, parametrizing the beta hyper-prior on
the inclusion probabilities. If omitted, <code>alpha</code> will be chosen
empirically via LASSO regression.</p>
</td></tr>
<tr><td><code id="svb.fit_+3A_beta">beta</code></td>
<td>
<p>A positive numeric value, parametrizing the beta hyper-prior on
the inclusion probabilities. If omitted, <code>beta</code> will be chosen
empirically via LASSO regression.</p>
</td></tr>
<tr><td><code id="svb.fit_+3A_prior_scale">prior_scale</code></td>
<td>
<p>A numeric value, controlling the scale parameter of the
prior slab density. Used as the scale parameter <code class="reqn">\lambda</code> when
<code>prior = "laplace"</code>, or as the standard deviation <code class="reqn">\sigma</code> if
<code>prior = "gaussian"</code>.</p>
</td></tr>
<tr><td><code id="svb.fit_+3A_update_order">update_order</code></td>
<td>
<p>A permutation of <code>1:ncol(X)</code>, giving the update
order of the coordinate-ascent algorithm. If omitted, a data driven
updating order is used, see <em>Ray and Szabo (2020)</em> in <em>Journal of
the American Statistical Association</em> for details.</p>
</td></tr>
<tr><td><code id="svb.fit_+3A_intercept">intercept</code></td>
<td>
<p>A Boolean variable, controlling if an intercept should be
included. NB: This feature is still experimental in logistic regression.</p>
</td></tr>
<tr><td><code id="svb.fit_+3A_noise_sd">noise_sd</code></td>
<td>
<p>A positive numerical value, serving as estimate for the
residual noise standard deviation in linear regression. If missing it will
be estimated, see <code>estimateSigma</code> from the <code>selectiveInference</code>
package for more details. Has no effect when <code>family = "logistic"</code>.</p>
</td></tr>
<tr><td><code id="svb.fit_+3A_max_iter">max_iter</code></td>
<td>
<p>A positive integer, controlling the maximum number of
iterations for the variational update loop.</p>
</td></tr>
<tr><td><code id="svb.fit_+3A_tol">tol</code></td>
<td>
<p>A small, positive numerical value, controlling the termination
criterion for maximum absolute differences between binary entropies of
successive iterates.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Suppose <code class="reqn">\theta</code> is the <code class="reqn">p</code>-dimensional true parameter. The
spike-and-slab prior for <code class="reqn">\theta</code> may be represented by the
hierarchical scheme </p>
<p style="text-align: center;"><code class="reqn">w \sim \mathrm{Beta}(\alpha, \beta),</code>
</p>
 <p style="text-align: center;"><code class="reqn">z_j
  \mid w \sim_{i.i.d.} \mathrm{Bernoulli}(w),</code>
</p>
 <p style="text-align: center;"><code class="reqn">\theta_j\mid z_j
  \sim_{ind.} (1-z_j)\delta_0 + z_j g.</code>
</p>
<p> Here, <code class="reqn">\delta_0</code> represents the
Dirac measure at <code class="reqn">0</code>. The slab <code class="reqn">g</code> may be taken either as a
<code class="reqn">\mathrm{Laplace}(0,\lambda)</code> or <code class="reqn">N(0,\sigma^2)</code> density. The
former has centered density </p>
<p style="text-align: center;"><code class="reqn">f_\lambda(x) = \frac{\lambda}{2}
  e^{-\lambda |x|}.</code>
</p>
<p> Given <code class="reqn">\alpha</code> and <code class="reqn">\beta</code>, the beta hyper-prior
has density </p>
<p style="text-align: center;"><code class="reqn">b(x\mid \alpha, \beta) = \frac{x^{\alpha - 1}(1 -
  x)^{\beta - 1}}{\int_0^1 t^{\alpha - 1}(1 - t)^{\beta - 1}\ \mathrm{d}t}.</code>
</p>

<p>A straightforward integration shows that the prior inclusion probability of
a coefficient is <code class="reqn">\frac{\alpha}{\alpha + \beta}</code>.
</p>


<h3>Value</h3>

<p>The approximate mean-field posterior, given as a named list
containing numeric vectors <code>"mu"</code>, <code>"sigma"</code>, <code>"gamma"</code>, and
a value <code>"intercept"</code>. The latter is set to <code>NA</code> in case
<code>intercept = FALSE</code>. In mathematical terms, the conditional
distribution of each <code class="reqn">\theta_j</code> is given by </p>
<p style="text-align: center;"><code class="reqn">\theta_j\mid \mu_j,
  \sigma_j, \gamma_j \sim_{ind.} \gamma_j N(\mu_j, \sigma^2) + (1-\gamma_j)
  \delta_0.</code>
</p>



<h3>Examples</h3>

<pre><code class='language-R'>
### Simulate a linear regression problem of size n times p, with sparsity level s ###

n &lt;- 250
p &lt;- 500
s &lt;- 5

### Generate toy data ###

X &lt;- matrix(rnorm(n*p), n, p) #standard Gaussian design matrix

theta &lt;- numeric(p)
theta[sample.int(p, s)] &lt;- runif(s, -3, 3) #sample non-zero coefficients in random locations

pos_TR &lt;- as.numeric(theta != 0) #true positives

Y &lt;- X %*% theta + rnorm(n) #add standard Gaussian noise

### Run the algorithm in linear mode with Laplace prior and prioritized initialization ###

test &lt;- svb.fit(X, Y, family = "linear")

posterior_mean &lt;- test$mu * test$gamma #approximate posterior mean

pos &lt;- as.numeric(test$gamma &gt; 0.5) #significant coefficients

### Assess the quality of the posterior estimates ###

TPR &lt;- sum(pos[which(pos_TR == 1)])/sum(pos_TR) #True positive rate

FDR &lt;- sum(pos[which(pos_TR != 1)])/max(sum(pos), 1) #False discovery rate

L2 &lt;- sqrt(sum((posterior_mean - theta)^2)) #L_2-error

MSPE &lt;- sqrt(sum((X %*% posterior_mean - Y)^2)/n) #Mean squared prediction error

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
