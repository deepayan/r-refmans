<!DOCTYPE html><html><head><title>Help for package crs</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {crs}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#crs-package'><p>Nonparametric Regression Splines with Continuous and Categorical Predictors</p></a></li>
<li><a href='#clsd'><p>Categorical Logspline Density</p></a></li>
<li><a href='#cps71'><p> Canadian High School Graduate Earnings</p></a></li>
<li><a href='#crs'><p>Categorical Regression Splines</p></a></li>
<li><a href='#crsiv'>
<p>Nonparametric Instrumental Regression</p></a></li>
<li><a href='#crsivderiv'>
<p>Nonparametric Instrumental Derivatives</p></a></li>
<li><a href='#crssigtest'><p>Regression Spline Significance Test with Mixed Data Types</p></a></li>
<li><a href='#Engel95'><p> 1995 British Family Expenditure Survey</p></a></li>
<li><a href='#frscv'><p>Categorical Factor Regression Spline Cross-Validation</p></a></li>
<li><a href='#frscvNOMAD'><p>Categorical Factor Regression Spline Cross-Validation</p></a></li>
<li><a href='#glp.model.matrix'><p>Utility function for constructing generalized polynomial smooths</p></a></li>
<li><a href='#gsl.bs'><p>GSL (GNU Scientific Library) B-spline/B-spline Derivatives</p></a></li>
<li><a href='#krscv'><p>Categorical Kernel Regression Spline Cross-Validation</p></a></li>
<li><a href='#krscvNOMAD'><p>Categorical Kernel Regression Spline Cross-Validation</p></a></li>
<li><a href='#npglpreg'><p>Generalized Local Polynomial Regression</p></a></li>
<li><a href='#snomadr'>
<p>R interface to NOMAD</p></a></li>
<li><a href='#tensor.prod.model.matrix'><p> Utility functions for constructing tensor product smooths</p></a></li>
<li><a href='#uniquecombs'><p>Find the unique rows in a matrix</p></a></li>
<li><a href='#wage1'><p> Cross-Sectional Data on Wages</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>0.15-37</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-12-31</td>
</tr>
<tr>
<td>Imports:</td>
<td>boot, stats, np, quantreg</td>
</tr>
<tr>
<td>Suggests:</td>
<td>logspline, quadprog, rgl</td>
</tr>
<tr>
<td>Title:</td>
<td>Categorical Regression Splines</td>
</tr>
<tr>
<td>Description:</td>
<td>Regression splines that handle a mix of continuous and categorical (discrete) data often encountered in applied settings. I would like to gratefully acknowledge support from the Natural Sciences and Engineering Research Council of Canada (NSERC, <a href="https://www.nserc-crsng.gc.ca">https://www.nserc-crsng.gc.ca</a>), the Social Sciences and Humanities Research Council of Canada (SSHRC, <a href="https://www.sshrc-crsh.gc.ca">https://www.sshrc-crsh.gc.ca</a>), and the Shared Hierarchical Academic Research Computing Network (SHARCNET, <a href="https://www.sharcnet.ca">https://www.sharcnet.ca</a>). We would also like to acknowledge the contributions of the GNU GSL authors. In particular, we adapt the GNU GSL B-spline routine gsl_bspline.c adding automated support for quantile knots (in addition to uniform knots), providing missing functionality for derivatives, and for extending the splines beyond their endpoints.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/JeffreyRacine/R-Package-crs">https://github.com/JeffreyRacine/R-Package-crs</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/JeffreyRacine/R-Package-crs/issues">https://github.com/JeffreyRacine/R-Package-crs/issues</a></td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-12-31 17:55:58 UTC; jracine</td>
</tr>
<tr>
<td>Author:</td>
<td>Jeffrey S. Racine [aut, cre],
  Zhenghua Nie [aut],
  Brian D. Ripley [ctb] (stepCV.R)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jeffrey S. Racine &lt;racinej@mcmaster.ca&gt;</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-01-07 00:20:20 UTC</td>
</tr>
</table>
<hr>
<h2 id='crs-package'>Nonparametric Regression Splines with Continuous and Categorical Predictors</h2><span id='topic+crs-package'></span>

<h3>Description</h3>

<p>This package provides a method for nonparametric regression that
combines the (global) approximation power of regression splines for
continuous predictors (&lsquo;<code>x</code>&rsquo;) with the (local) power of
kernel methods for categorical predictors (&lsquo;<code>z</code>&rsquo;). The
user also has the option of instead using indicator bases for the
categorical predictors. When the predictors contain both continuous
and categorical (discrete) data types, both approaches offer more
efficient estimation than the traditional sample-splitting
(i.e. &lsquo;frequency&rsquo;) approach where the data is first broken into
subsets governed by the categorical <code>z</code>.
</p>
<p>To cite the <span class="pkg">crs</span> package type: &lsquo;<code>citation("crs")</code>&rsquo;
(without the single quotes).
</p>
<p>For a listing of all routines in the <span class="pkg">crs</span> package type:
&lsquo;<code>library(help="crs")</code>&rsquo;.
</p>
<p>For a listing of all demos in the <span class="pkg">crs</span> package type:
&lsquo;<code>demo(package="crs")</code>&rsquo;.
</p>
<p>For a &lsquo;<code><a href="utils.html#topic+vignette">vignette</a></code>&rsquo; that presents an overview of the
<span class="pkg">crs</span> package type: &lsquo;<code>vignette("crs")</code>&rsquo;.
</p>


<h3>Details</h3>

<p>For the continuous predictors the regression spline model employs the
B-spline basis matrix using the B-spline routines in the GNU
Scientific Library (<a href="https://www.gnu.org/software/gsl/">https://www.gnu.org/software/gsl/</a>).
</p>
<p>The <code><a href="#topic+tensor.prod.model.matrix">tensor.prod.model.matrix</a></code> function is used to
construct multivariate tensor spline bases when <code>basis="tensor"</code>
and uses additive B-splines otherwise (i.e. when
<code>basis="additive"</code>).
</p>
<p>For the discrete predictors the product kernel function is of the
&lsquo;Li-Racine&rsquo; type (see Li and Racine (2007) for details) which is
formed by constructing products of one of the following univariate
kernels:
</p>

<dl>
<dt>(<code class="reqn">z</code> is discrete/nominal)</dt><dd>
<p><code class="reqn">l(z_i,z,\lambda) = 1 </code> if <code class="reqn">z_i=z</code>, and
<code class="reqn">\lambda</code> if <code class="reqn">z_i \neq z</code>. Note that
<code class="reqn">\lambda</code> must lie between <code class="reqn">0</code> and <code class="reqn">1</code>.
</p>
</dd>
<dt>(<code class="reqn">z</code> is discrete/ordinal)</dt><dd>
<p><code class="reqn">l(z_i,z,\lambda) = 1</code> if
<code class="reqn">|z_i-z|=0</code>, and
<code class="reqn">\lambda^{|z_i-z|}</code> if <code class="reqn">|z_i -
        z|\ge1</code>. Note that <code class="reqn">\lambda</code> must lie
between <code class="reqn">0</code> and <code class="reqn">1</code>.
</p>
</dd>
</dl>

<p>Alternatively, for the ordinal/nominal predictors the regression
spline model will use indicator basis functions.
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Racine <a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a> and Zhenghua Nie <a href="mailto:niez@mcmaster.ca">niez@mcmaster.ca</a>
</p>
<p>Maintainer: Jeffrey S. Racine <a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>
<p>I would like to gratefully acknowledge support from the Natural
Sciences and Engineering Research Council of Canada
(<a href="https://www.nserc-crsng.gc.ca">https://www.nserc-crsng.gc.ca</a>), the Social Sciences and Humanities
Research Council of Canada (<a href="https://www.sshrc-crsh.gc.ca">https://www.sshrc-crsh.gc.ca</a>), and the Shared
Hierarchical Academic Research Computing Network
(<a href="https://www.sharcnet.ca">https://www.sharcnet.ca</a>).
</p>


<h3>References</h3>

<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics:
Theory and Practice,</em> Princeton University Press.
</p>
<p>Ma, S. and J.S. Racine and L. Yang (2015), &ldquo;Spline
Regression in the Presence of Categorical Predictors,&rdquo; Journal of
Applied Econometrics, Volume 30, 705-717.
</p>
<p>Ma, S. and J.S. Racine (2013), &ldquo;Additive Regression
Splines with Irrelevant Categorical and Continuous Regressors,&rdquo;
Statistica Sinica, Volume 23, 515-541.
</p>

<hr>
<h2 id='clsd'>Categorical Logspline Density</h2><span id='topic+clsd'></span>

<h3>Description</h3>

 <p><code>clsd</code> computes the logspline density, density
derivative, distribution, and smoothed quantiles for a one (1)
dimensional continuous variable using the approach of Racine
(2013).</p>


<h3>Usage</h3>

<pre><code class='language-R'>clsd(x = NULL,
     beta = NULL,
     xeval = NULL,
     degree = NULL,
     segments = NULL,
     degree.min = 2,
     degree.max = 25,
     segments.min = 1,
     segments.max = 100,
     lbound = NULL,
     ubound = NULL,
     basis = "tensor",
     knots = "quantiles",
     penalty = NULL,
     deriv.index = 1,
     deriv = 1,
     elastic.max = TRUE,
     elastic.diff = 3,
     do.gradient = TRUE,
     er = NULL,
     monotone = TRUE,
     monotone.lb = -250,
     n.integrate = 500,
     nmulti = 1,
     method = c("L-BFGS-B", "Nelder-Mead", "BFGS", "CG", "SANN"),
     verbose = FALSE,
     quantile.seq = seq(.01,.99,by=.01),
     random.seed = 42,
     maxit = 10^5,
     max.attempts = 25,
     NOMAD = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clsd_+3A_x">x</code></td>
<td>
<p> a numeric vector of training data </p>
</td></tr>
<tr><td><code id="clsd_+3A_beta">beta</code></td>
<td>
<p> a numeric vector of coefficients (default <code>NULL</code>) </p>
</td></tr>
<tr><td><code id="clsd_+3A_xeval">xeval</code></td>
<td>
<p> a numeric vector of evaluation data </p>
</td></tr>  
<tr><td><code id="clsd_+3A_degree">degree</code></td>
<td>
<p> integer/vector specifying the polynomial degree of the
B-spline basis for each dimension of the continuous <code>x</code> (default
<code>degree=2</code>)</p>
</td></tr>
<tr><td><code id="clsd_+3A_segments">segments</code></td>
<td>
<p> integer/vector specifying the number of segments of the
B-spline basis for each dimension of the continuous <code>x</code>
(i.e. number of knots minus one) (default <code>segments=1</code>, i.e. Bezier
curve)</p>
</td></tr>
<tr><td><code id="clsd_+3A_segments.min">segments.min</code>, <code id="clsd_+3A_segments.max">segments.max</code></td>
<td>
<p> when <code>elastic.max=FALSE</code>, the
minimum/maximum segments of the B-spline basis for each of the
continuous predictors (default
<code>segments.min=1</code>,<code>segments.max=100</code>)</p>
</td></tr>
<tr><td><code id="clsd_+3A_degree.min">degree.min</code>, <code id="clsd_+3A_degree.max">degree.max</code></td>
<td>
<p> when <code>elastic.max=FALSE</code> the
minimum/maximum degree of the B-spline basis for each of the
continuous predictors (default <code>degree.min=2</code>,
<code>degree.max=25</code>)</p>
</td></tr>
<tr><td><code id="clsd_+3A_lbound">lbound</code>, <code id="clsd_+3A_ubound">ubound</code></td>
<td>

<p>lower/upper bound for the support of the density. For example, if
there is a priori knowledge that the density equals zero to the left
of 0, and has a discontinuity at 0, the user could specify lbound =
0. However, if the density is essentially zero near 0, one does not
need to specify lbound </p>
</td></tr>
<tr><td><code id="clsd_+3A_basis">basis</code></td>
<td>
<p> a character string (default <code>basis="tensor"</code>)
indicating whether the additive or tensor product B-spline basis
matrix for a multivariate polynomial spline or generalized B-spline
polynomial basis should be used </p>
</td></tr>
<tr><td><code id="clsd_+3A_knots">knots</code></td>
<td>
<p> a character string (default <code>knots="quantiles"</code>)
specifying where knots are to be placed. &lsquo;quantiles&rsquo; specifies
knots placed at equally spaced quantiles (equal number of observations
lie in each segment) and &lsquo;uniform&rsquo; specifies knots placed at
equally spaced intervals </p>
</td></tr>
<tr><td><code id="clsd_+3A_deriv">deriv</code></td>
<td>
<p> an integer <code>l</code> (default <code>deriv=1</code>) specifying
whether to compute the univariate <code>l</code>th partial derivative for
each continuous predictor (and difference in levels for each
categorical predictor) or not and if so what order. Note that if
<code>deriv</code> is higher than the spline degree of the associated
continuous predictor then the derivative will be zero and a warning
issued to this effect </p>
</td></tr>
<tr><td><code id="clsd_+3A_deriv.index">deriv.index</code></td>
<td>
<p> an integer <code>l</code> (default <code>deriv.index=1</code>)
specifying the index (currently only supports 1) of the variable whose
derivative is requested  </p>
</td></tr>
<tr><td><code id="clsd_+3A_nmulti">nmulti</code></td>
<td>

<p>integer number of times to restart the process of finding extrema of
the cross-validation function from different (random) initial
points (default <code>nmulti=1</code>)
</p>
</td></tr>
<tr><td><code id="clsd_+3A_penalty">penalty</code></td>
<td>
<p> the parameter to be used in the AIC criterion. The
method chooses the number of degrees plus number of segments
(knots-1) that maximizes <code>2*logl-penalty*(degree+segments)</code>. The
default is to use the penalty parameter of <code>log(n)/2</code> (<code>2</code>
would deliver standard AIC, <code>log(n)</code> standard BIC)</p>
</td></tr>
<tr><td><code id="clsd_+3A_elastic.max">elastic.max</code>, <code id="clsd_+3A_elastic.diff">elastic.diff</code></td>
<td>
<p> a logical value/integer indicating
whether to use &lsquo;elastic&rsquo; search bounds such that the optimal
degree/segment must lie <code>elastic.diff</code> units from the
respective search bounds </p>
</td></tr>
<tr><td><code id="clsd_+3A_do.gradient">do.gradient</code></td>
<td>
<p> a logical value indicating whether or not to use
the analytical gradient during optimization (defaults to <code>TRUE</code>)</p>
</td></tr>
<tr><td><code id="clsd_+3A_er">er</code></td>
<td>

<p>a scalar indicating the fraction of data range to extend
the tails (default <code>1/log(n)</code>, see <code><a href="grDevices.html#topic+extendrange">extendrange</a></code> for
further details)
</p>
</td></tr>
<tr><td><code id="clsd_+3A_monotone">monotone</code></td>
<td>
<p> a logical value indicating whether modify
the standard B-spline basis function so that it is tailored for
density estimation (default <code>TRUE</code>)
</p>
</td></tr>
<tr><td><code id="clsd_+3A_monotone.lb">monotone.lb</code></td>
<td>
<p> a negative bound specifying the lower bound on
the linear segment coefficients used when (<code>monotone=FALSE</code>)
</p>
</td></tr>
<tr><td><code id="clsd_+3A_n.integrate">n.integrate</code></td>
<td>

<p>the number of evenly spaced integration points on the extended range specified by <code>er</code> (defaults to <code>500</code>)
</p>
</td></tr>
<tr><td><code id="clsd_+3A_method">method</code></td>
<td>

<p>see <code><a href="stats.html#topic+optim">optim</a></code> for details
</p>
</td></tr>
<tr><td><code id="clsd_+3A_verbose">verbose</code></td>
<td>

<p>a logical value which when <code>TRUE</code> produces verbose output
during optimization
</p>
</td></tr>
<tr><td><code id="clsd_+3A_quantile.seq">quantile.seq</code></td>
<td>

<p>a sequence of numbers lying in <code class="reqn">[0,1]</code> on which quantiles from
the logspline distribution are obtained
</p>
</td></tr>
<tr><td><code id="clsd_+3A_random.seed">random.seed</code></td>
<td>
<p> seeds the random number generator for initial
parameter values when <code><a href="stats.html#topic+optim">optim</a></code> is called </p>
</td></tr>
<tr><td><code id="clsd_+3A_maxit">maxit</code></td>
<td>

<p>maximum number of iterations used by <code><a href="stats.html#topic+optim">optim</a></code>
</p>
</td></tr>
<tr><td><code id="clsd_+3A_max.attempts">max.attempts</code></td>
<td>

<p>maximum number of attempts to undertake if <code><a href="stats.html#topic+optim">optim</a></code>
fails for any set of initial parameters for each value of
<code>nmulti</code>
</p>
</td></tr>
<tr><td><code id="clsd_+3A_nomad">NOMAD</code></td>
<td>

<p>a logical value which when <code>TRUE</code> calls <code><a href="#topic+snomadr">snomadr</a></code>
to determine the optimal <code>degree</code> and <code>segments</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Typical usages are (see below for a  list of options and also
the examples at the end of this help file)
</p>
<pre>
    
    model &lt;- clsd(x)

    </pre>
<p><code>clsd</code> computes a logspline density estimate of a one (1)
dimensional continuous variable.
</p>
<p>The spline model employs the tensor product B-spline basis matrix for
a multivariate polynomial spline via the B-spline routines in the GNU
Scientific Library (<a href="https://www.gnu.org/software/gsl/">https://www.gnu.org/software/gsl/</a>) and the
<code><a href="#topic+tensor.prod.model.matrix">tensor.prod.model.matrix</a></code> function.
</p>
<p>When <code>basis="additive"</code> the model becomes additive in nature
(i.e. no interaction/tensor terms thus semiparametric not fully
nonparametric).
</p>
<p>When <code>basis="tensor"</code> the model uses the multivariate tensor
product basis.
</p>


<h3>Value</h3>

<p><code>clsd</code> returns a <code>clsd</code> object.  The generic functions
<code><a href="stats.html#topic+coef">coef</a></code>, <code><a href="stats.html#topic+fitted">fitted</a></code>, <code><a href="graphics.html#topic+plot">plot</a></code> and
<code><a href="base.html#topic+summary">summary</a></code> support objects of this type (<code>er=FALSE</code>
plots the density on the sample realizations (default is &lsquo;extended
range&rsquo; data), see <code>er</code> above, <code>distribution=TRUE</code> plots
the distribution). The returned object has the following components:
</p>
<table>
<tr><td><code>density</code></td>
<td>
<p> estimates of the density function
at the sample points</p>
</td></tr>
<tr><td><code>density.er</code></td>
<td>
<p> the density evaluated on the &lsquo;extended range&rsquo;
of the data </p>
</td></tr>
<tr><td><code>density.deriv</code></td>
<td>
<p> estimates of the derivative of the density function
at the sample points</p>
</td></tr>
<tr><td><code>density.deriv.er</code></td>
<td>
<p> estimates of the derivative of the density
function evaluated on the &lsquo;extended range&rsquo; of the data </p>
</td></tr>
<tr><td><code>distribution</code></td>
<td>
<p> estimates of the distribution function
at the sample points</p>
</td></tr>
<tr><td><code>distribution.er</code></td>
<td>
<p> the distribution evaluated on the &lsquo;extended range&rsquo;
of the data </p>
</td></tr>
<tr><td><code>xer</code></td>
<td>
<p> the &lsquo;extended range&rsquo; of the data </p>
</td></tr>
<tr><td><code>degree</code></td>
<td>
<p> integer/vector specifying the degree of the B-spline
basis for each dimension of the continuous <code>x</code></p>
</td></tr>
<tr><td><code>segments</code></td>
<td>
<p> integer/vector specifying the number of segments of
the B-spline basis for each dimension of the continuous <code>x</code></p>
</td></tr>
<tr><td><code>xq</code></td>
<td>
<p> vector of quantiles </p>
</td></tr>
<tr><td><code>tau</code></td>
<td>
<p> vector generated by <code>quantile.seq</code> or input by the
user (lying in <code>[0,1]</code>) from which the quantiles <code>xq</code> are
obtained</p>
</td></tr>
</table>


<h3>Usage Issues</h3>

<p>This function should be considered to be in &lsquo;beta&rsquo; status until
further notice.
</p>
<p>If smoother estimates are desired and <code>degree=degree.min</code>, increase
<code>degree.min</code> to, say, <code>degree.min=3</code>.
</p>
<p>The use of &lsquo;regression&rsquo; B-splines can lead to undesirable behavior at
the endpoints of the data (i.e. when <code>monotone=FALSE</code>). The
default &lsquo;density&rsquo; B-splines ought to be well-behaved in these regions.
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Racine <a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Racine, J.S. (2013), &ldquo;Logspline Mixed Data Density Estimation,&rdquo;
manuscript.
</p>


<h3>See Also</h3>

<p><code><a href="logspline.html#topic+logspline">logspline</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Old Faithful eruptions data histogram and clsd density

library(MASS)
data(faithful)
attach(faithful)

model &lt;- clsd(eruptions)

ylim &lt;- c(0,max(model$density,hist(eruptions,breaks=20,plot=FALSE)$density))

plot(model,ylim=ylim)

hist(eruptions,breaks=20,freq=FALSE,add=TRUE,lty=2)

rug(eruptions)

summary(model)

coef(model)

## Simulated data

set.seed(42)
require(logspline)

## Example - simulated data

n &lt;- 250
x &lt;- sort(rnorm(n))
f.dgp &lt;- dnorm(x)

model &lt;- clsd(x)

## Standard (cubic) estimate taken from the logspline package
## Compute MSEs

mse.clsd &lt;- mean((fitted(model)-f.dgp)^2)

model.logspline &lt;- logspline(x)

mse.logspline &lt;- mean((dlogspline(x,model.logspline)-f.dgp)^2)

ylim &lt;- c(0,max(fitted(model),dlogspline(x,model.logspline),f.dgp))

plot(model,
     ylim=ylim,
     sub=paste("MSE: logspline = ",format(mse.logspline),", clsd = ",
     format(mse.clsd)),
     lty=3,
     col=3)

xer &lt;- model$xer

lines(xer,dlogspline(xer,model.logspline),col=2,lty=2)
lines(xer,dnorm(xer),col=1,lty=1)

rug(x)

legend("topright",c("DGP",
                    paste("Cubic Logspline Density (package 'logspline', knots = ",
                          model.logspline$nknots,")",sep=""),
                    paste("clsd Density (degree = ", model$degree, ", segments = ",
                          model$segments,", penalty = ",round(model$penalty,2),")",sep="")),
       lty=1:3,
       col=1:3,
       bty="n",
       cex=0.75)

summary(model)

coef(model)

## Simulate data with known bounds

set.seed(42)
n &lt;- 10000
x &lt;- runif(n,0,1)

model &lt;- clsd(x,lbound=0,ubound=1)

plot(model)

## End(Not run) 
</code></pre>

<hr>
<h2 id='cps71'> Canadian High School Graduate Earnings  </h2><span id='topic+cps71'></span>

<h3>Description</h3>

<p>Canadian cross-section wage data consisting of a random sample taken
from the 1971 Canadian Census Public Use Tapes for male individuals
having common education (grade 13). There are 205 observations in total.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("cps71")</code></pre>


<h3>Format</h3>

<p> A data frame with 2 columns, and 205 rows.
</p>

<dl>
<dt>logwage</dt><dd><p> the first column, of type <code>numeric</code></p>
</dd>
<dt>age</dt><dd><p> the second column, of type <code>integer</code></p>
</dd>
</dl>



<h3>Source</h3>

<p> Aman Ullah </p>


<h3>References</h3>

<p>Pagan, A. and A. Ullah (1999), <em>Nonparametric Econometrics,</em>
Cambridge University Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Example - we compare the nonparametric local linear kernel regression
## method with the regression spline for the cps71 data. Note that there
## are no categorical predictors in this dataset so we are merely
## comparing and contrasting the two nonparametric estimates.

data(cps71)
attach(cps71)
require(np)

model.crs &lt;- crs(logwage~age,complexity="degree-knots")
model.np &lt;- npreg(logwage~age,regtype="ll")

plot(age,logwage,cex=0.25,col="grey",
     sub=paste("crs-CV = ", formatC(model.crs$cv.score,format="f",digits=3),
       ", npreg-CV = ", formatC(model.np$bws$fval,format="f",digits=3),sep=""))
lines(age,fitted(model.crs),lty=1,col=1)
lines(age,fitted(model.np),lty=2,col=2)

crs.txt &lt;- paste("crs (R-squared = ",formatC(model.crs$r.squared,format="f",digits=3),")",sep="")
np.txt &lt;- paste("ll-npreg (R-squared = ",formatC(model.np$R2,format="f",digits=3),")",sep="")

legend(22.5,15,c(crs.txt,np.txt),lty=c(1,2),col=c(1,2),bty="n")

summary(model.crs)
summary(model.np)
detach("package:np")
</code></pre>

<hr>
<h2 id='crs'>Categorical Regression Splines</h2><span id='topic+crs'></span><span id='topic+crs.default'></span><span id='topic+crs.formula'></span>

<h3>Description</h3>

 <p><code>crs</code> computes a regression spline estimate of a
one (1) dimensional dependent variable on an <code>r</code>-dimensional
vector of continuous and categorical
(<code><a href="base.html#topic+factor">factor</a></code>/<code><a href="base.html#topic+ordered">ordered</a></code>) predictors (Ma and
Racine (2013), Ma, Racine and Yang (2015)).</p>


<h3>Usage</h3>

<pre><code class='language-R'>crs(...)
## Default S3 method:
crs(xz,
    y,
    degree = NULL,
    segments = NULL,
    include = NULL,
    kernel = TRUE,
    lambda = NULL,
    complexity = c("degree-knots","degree","knots"),
    knots = c("quantiles","uniform","auto"),
    basis = c("auto","additive","tensor","glp"),
    deriv = 0,
    data.return = FALSE,
    prune = FALSE,
    model.return = FALSE,
    tau = NULL,
    weights = NULL,
    ...)

## S3 method for class 'formula'
crs(formula,
    data = list(),
    degree = NULL,
    segments = NULL,
    include = NULL,
    degree.max = 10, 
    segments.max = 10, 
    degree.min = 0, 
    segments.min = 1, 
    cv.df.min = 1,
    cv = c("nomad","exhaustive","none"),
    cv.threshold = 1000,
    cv.func = c("cv.ls","cv.gcv","cv.aic"),
    kernel = TRUE,
    lambda = NULL,
    lambda.discrete = FALSE,
    lambda.discrete.num = 100,
    complexity = c("degree-knots","degree","knots"),
    knots = c("quantiles","uniform","auto"),
    basis = c("auto","additive","tensor","glp"),
    deriv = 0,
    data.return = FALSE,
    prune = FALSE,
    model.return = FALSE,
    restarts = 0,
    random.seed = 42,
    max.bb.eval = 10000,
    initial.mesh.size.real = "r1.0e-01",
    initial.mesh.size.integer = "1",
    min.mesh.size.real = paste(sqrt(.Machine$double.eps)),
    min.mesh.size.integer = 1, 
    min.poll.size.real = 1,
    min.poll.size.integer = 1, 
    opts=list(),
    nmulti = 5,
    tau = NULL,
    weights = NULL,
    singular.ok = FALSE,
    ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crs_+3A_xz">xz</code></td>
<td>
<p> numeric (<code>x</code>) and or nominal/ordinal
(<code><a href="base.html#topic+factor">factor</a></code>/<code><a href="base.html#topic+ordered">ordered</a></code>) predictors (<code>z</code>) </p>
</td></tr>
<tr><td><code id="crs_+3A_y">y</code></td>
<td>
<p> a numeric vector of responses. </p>
</td></tr>
<tr><td><code id="crs_+3A_degree">degree</code></td>
<td>
<p> integer/vector specifying the polynomial degree of the
B-spline basis for each dimension of the continuous <code>x</code>
(default <code>degree=3</code>, i.e. cubic spline)</p>
</td></tr>
<tr><td><code id="crs_+3A_segments">segments</code></td>
<td>
<p> integer/vector specifying the number of segments of the
B-spline basis for each dimension of the continuous <code>x</code>
(i.e. number of knots minus one) (default <code>segments=1</code>, i.e. Bezier
curve)</p>
</td></tr>
<tr><td><code id="crs_+3A_include">include</code></td>
<td>
<p> integer/vector specifying whether each of the
nominal/ordinal (<code><a href="base.html#topic+factor">factor</a></code>/<code><a href="base.html#topic+ordered">ordered</a></code>) predictors
in <code>x</code> are included or omitted from the resulting estimate </p>
</td></tr>
<tr><td><code id="crs_+3A_lambda">lambda</code></td>
<td>
<p> a vector of bandwidths for each dimension of the
categorical <code>z</code></p>
</td></tr>
<tr><td><code id="crs_+3A_lambda.discrete">lambda.discrete</code></td>
<td>
<p> if <code>lambda.discrete=TRUE</code>, the bandwidth
will be discretized into <code>lambda.discrete.num+1</code> points and
<code>lambda</code> will be chosen from these points</p>
</td></tr>
<tr><td><code id="crs_+3A_lambda.discrete.num">lambda.discrete.num</code></td>
<td>
<p>a positive integer indicating the number of
discrete values that lambda can assume - this parameter will only be
used when <code>lambda.discrete=TRUE</code></p>
</td></tr>
<tr><td><code id="crs_+3A_formula">formula</code></td>
<td>
<p> a symbolic description of the model to be fit </p>
</td></tr>
<tr><td><code id="crs_+3A_data">data</code></td>
<td>
<p> an optional data frame containing the variables in the
model </p>
</td></tr>
<tr><td><code id="crs_+3A_cv">cv</code></td>
<td>
<p> a character string (default <code>cv="nomad"</code>) indicating
whether to use nonsmooth mesh adaptive direct search, exhaustive
search, or no search (i.e. use user supplied values for <code>degree</code>,
<code>segments</code>, and <code>lambda</code>) </p>
</td></tr>
<tr><td><code id="crs_+3A_cv.threshold">cv.threshold</code></td>
<td>
<p> an integer (default <code>cv.threshold=1000</code>) for
simple problems with no categorical predictors
(i.e. <code>kernel=FALSE</code> otherwise
<code><a href="stats.html#topic+optim">optim</a></code>/<code><a href="#topic+snomadr">snomadr</a></code> search is necessary) such
that, if the number of combinations of <code>degree</code>/<code>segments</code>
is less than the threshold and <code>cv="nomad"</code>, instead use
exhaustive search (<code>cv="exhaustive"</code>) </p>
</td></tr>
<tr><td><code id="crs_+3A_cv.func">cv.func</code></td>
<td>
<p>a character string (default <code>cv.func="cv.ls"</code>)
indicating which method to use to select smoothing
parameters. <code>cv.gcv</code> specifies generalized cross-validation
(Craven and Wahba (1979)), <code>cv.aic</code> specifies expected
Kullback-Leibler cross-validation (Hurvich, Simonoff, and Tsai
(1998)), and <code>cv.ls</code> specifies least-squares
cross-validation </p>
</td></tr>
<tr><td><code id="crs_+3A_kernel">kernel</code></td>
<td>
<p> a logical value (default <code>kernel=TRUE</code>)
indicating whether to use kernel smoothing or not </p>
</td></tr>
<tr><td><code id="crs_+3A_degree.max">degree.max</code></td>
<td>
<p> the maximum degree of the B-spline basis for
each of the continuous predictors (default <code>degree.max=10</code>)</p>
</td></tr>
<tr><td><code id="crs_+3A_segments.max">segments.max</code></td>
<td>
<p> the maximum segments of the B-spline basis for
each of the continuous predictors (default <code>segments.max=10</code>)</p>
</td></tr>
<tr><td><code id="crs_+3A_degree.min">degree.min</code></td>
<td>
<p> the minimum degree of the B-spline basis for
each of the continuous predictors (default <code>degree.min=0</code>)</p>
</td></tr>
<tr><td><code id="crs_+3A_segments.min">segments.min</code></td>
<td>
<p> the minimum segments of the B-spline basis for
each of the continuous predictors (default <code>segments.min=1</code>)</p>
</td></tr>
<tr><td><code id="crs_+3A_cv.df.min">cv.df.min</code></td>
<td>
<p> the minimum degrees of freedom to allow when
conducting NOMAD-based cross-validation (default
<code>cv.df.min=1</code>)</p>
</td></tr>
<tr><td><code id="crs_+3A_complexity">complexity</code></td>
<td>
<p>a character string (default
<code>complexity="degree-knots"</code>) indicating whether model
&lsquo;complexity&rsquo; is determined by the degree of the spline or by the
number of segments (i.e. number of knots minus one). This option
allows the user to use cross-validation to select either the spline
degree (number of knots held fixed) or the number of knots (spline
degree held fixed) or both the spline degree and number of knots
</p>
<p>For the continuous predictors the regression spline model employs
either the additive or tensor product B-spline basis matrix for a
multivariate polynomial spline via the B-spline routines in the GNU
Scientific Library (<a href="https://www.gnu.org/software/gsl/">https://www.gnu.org/software/gsl/</a>) and the
<code><a href="#topic+tensor.prod.model.matrix">tensor.prod.model.matrix</a></code> function </p>
</td></tr>
<tr><td><code id="crs_+3A_knots">knots</code></td>
<td>
<p> a character string (default <code>knots="quantiles"</code>)
specifying where knots are to be placed. &lsquo;quantiles&rsquo; specifies
knots placed at equally spaced quantiles (equal number of observations
lie in each segment) and &lsquo;uniform&rsquo; specifies knots placed at
equally spaced intervals. If <code>knots="auto"</code>, the knot type will
be automatically determined by cross-validation </p>
</td></tr>
<tr><td><code id="crs_+3A_basis">basis</code></td>
<td>
<p> a character string (default <code>basis="auto"</code>)
indicating whether the additive or tensor product B-spline basis
matrix for a multivariate polynomial spline or generalized B-spline
polynomial basis should be used. Note this can be automatically
determined by cross-validation if <code>cv="nomad"</code> or
<code>cv="exhaustive"</code> and <code>basis="auto"</code>, and is an
&lsquo;all or none&rsquo; proposition (i.e. interaction terms for all
predictors or for no predictors given the nature of &lsquo;tensor
products&rsquo;). Note also that if there is only one predictor this
defaults to <code>basis="additive"</code> to avoid unnecessary computation
as the spline bases are equivalent in this case </p>
</td></tr>
<tr><td><code id="crs_+3A_deriv">deriv</code></td>
<td>
<p> an integer <code>l</code> (default <code>deriv=0</code>) specifying
whether to compute the univariate <code>l</code>th partial derivative for
each continuous predictor (and difference in levels for each
categorical predictor) or not and if so what order. Note that if
<code>deriv</code> is higher than the spline degree of the associated
continuous predictor then the derivative will be zero and a warning
issued to this effect </p>
</td></tr>
<tr><td><code id="crs_+3A_data.return">data.return</code></td>
<td>
<p> a logical value indicating whether to return
<code>x,z,y</code> or not (default <code>data.return=FALSE</code>) </p>
</td></tr>
<tr><td><code id="crs_+3A_prune">prune</code></td>
<td>
<p> a logical value (default <code>prune=FALSE</code>) specifying
whether the (final) model is to be &lsquo;pruned&rsquo; using a stepwise
cross-validation criterion based upon a modified version of
<code><a href="MASS.html#topic+stepAIC">stepAIC</a></code> (see below for details) </p>
</td></tr>
<tr><td><code id="crs_+3A_model.return">model.return</code></td>
<td>
<p> a logical value indicating whether to return the
list of <code><a href="stats.html#topic+lm">lm</a></code> models or not when <code>kernel=TRUE</code>
(default <code>model.return=FALSE</code>) </p>
</td></tr>
<tr><td><code id="crs_+3A_restarts">restarts</code></td>
<td>
<p> integer specifying the number of times to restart the
process of finding extrema of the cross-validation function (for the
bandwidths only) from different (random) initial points </p>
</td></tr>
<tr><td><code id="crs_+3A_random.seed">random.seed</code></td>
<td>
<p> when it is not missing and not equal to 0, the
initial points will be generated using this seed when using
<code><a href="#topic+frscvNOMAD">frscvNOMAD</a></code> or <code><a href="#topic+krscvNOMAD">krscvNOMAD</a></code> and
<code>nmulti &gt; 0</code> </p>
</td></tr>
<tr><td><code id="crs_+3A_max.bb.eval">max.bb.eval</code></td>
<td>

<p>argument passed to the NOMAD solver (see <code><a href="#topic+snomadr">snomadr</a></code> for
further details)
</p>
</td></tr>
<tr><td><code id="crs_+3A_initial.mesh.size.real">initial.mesh.size.real</code></td>
<td>

<p>argument passed to the NOMAD solver (see <code><a href="#topic+snomadr">snomadr</a></code> for
further details)
</p>
</td></tr>
<tr><td><code id="crs_+3A_initial.mesh.size.integer">initial.mesh.size.integer</code></td>
<td>

<p>argument passed to the NOMAD solver (see <code><a href="#topic+snomadr">snomadr</a></code> for
further details)
</p>
</td></tr>
<tr><td><code id="crs_+3A_min.mesh.size.real">min.mesh.size.real</code></td>
<td>

<p>argument passed to the NOMAD solver (see <code><a href="#topic+snomadr">snomadr</a></code> for
further details)
</p>
</td></tr>
<tr><td><code id="crs_+3A_min.mesh.size.integer">min.mesh.size.integer</code></td>
<td>

<p>arguments passed to the NOMAD solver (see <code><a href="#topic+snomadr">snomadr</a></code> for
further details)
</p>
</td></tr>
<tr><td><code id="crs_+3A_min.poll.size.real">min.poll.size.real</code></td>
<td>

<p>arguments passed to the NOMAD solver (see <code><a href="#topic+snomadr">snomadr</a></code> for
further details)
</p>
</td></tr>
<tr><td><code id="crs_+3A_min.poll.size.integer">min.poll.size.integer</code></td>
<td>

<p>arguments passed to the NOMAD solver (see <code><a href="#topic+snomadr">snomadr</a></code> for
further details)
</p>
</td></tr>      
<tr><td><code id="crs_+3A_opts">opts</code></td>
<td>
<p> list of optional arguments to be passed to
<code><a href="#topic+snomadr">snomadr</a></code> </p>
</td></tr>
<tr><td><code id="crs_+3A_nmulti">nmulti</code></td>
<td>

<p>integer number of times to restart the process of finding extrema of
the cross-validation function from different (random) initial
points (default <code>nmulti=5</code>)
</p>
</td></tr>
<tr><td><code id="crs_+3A_tau">tau</code></td>
<td>

<p>if non-null a number in (0,1) denoting the quantile for which a quantile
regression spline is to be estimated rather than estimating the
conditional mean (default <code>tau=NULL</code>). Criterion function set
by <code>cv.func=</code> are modified accordingly to admit quantile regression.
</p>
</td></tr>
<tr><td><code id="crs_+3A_weights">weights</code></td>
<td>

<p>an optional vector of weights to be used in the fitting process.
Should be &lsquo;NULL&rsquo; or a numeric vector.  If non-NULL, weighted least
squares is used with weights &lsquo;weights&rsquo; (that is, minimizing
&lsquo;sum(w*e^2)&rsquo;); otherwise ordinary least squares is used.
</p>
</td></tr>
<tr><td><code id="crs_+3A_singular.ok">singular.ok</code></td>
<td>

<p>a logical value (default <code>singular.ok=FALSE</code>) that, when
<code>FALSE</code>, discards singular bases during cross-validation (a check
for ill-conditioned bases is performed).
</p>
</td></tr>
<tr><td><code id="crs_+3A_...">...</code></td>
<td>
<p> optional arguments </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Typical usages are (see below for a  list of options and also
the examples at the end of this help file)
</p>
<pre>
    ## Estimate the model and let the basis type be determined by
    ## cross-validation (i.e. cross-validation will determine whether to
    ## use the additive, generalized, or tensor product basis)
    
    model &lt;- crs(y~x1+x2)
    
    ## Estimate the model for a specified degree/segment/bandwidth
    ## combination and do not run cross-validation (will use the
    ## additive basis by default)
    
    model &lt;- crs(y~x1+factor(x2),cv="none",degree=3,segments=1,lambda=.1)
    
    ## Plot the mean and (asymptotic) error bounds

    plot(model,mean=TRUE,ci=TRUE)

    ## Plot the first partial derivative and (asymptotic) error bounds

    plot(model,deriv=1,ci=TRUE)    
    </pre>
<p><code>crs</code> computes a regression spline estimate of a one (1)
dimensional dependent variable on an <code>r</code>-dimensional vector of
continuous and categorical
(<code><a href="base.html#topic+factor">factor</a></code>/<code><a href="base.html#topic+ordered">ordered</a></code>) predictors.
</p>
<p>The regression spline model employs the tensor product B-spline basis
matrix for a multivariate polynomial spline via the B-spline routines
in the GNU Scientific Library (<a href="https://www.gnu.org/software/gsl/">https://www.gnu.org/software/gsl/</a>)
and the <code><a href="#topic+tensor.prod.model.matrix">tensor.prod.model.matrix</a></code> function.
</p>
<p>When <code>basis="additive"</code> the model becomes additive in nature
(i.e. no interaction/tensor terms thus semiparametric not fully
nonparametric).
</p>
<p>When <code>basis="tensor"</code> the model uses the multivariate tensor
product basis.
</p>
<p>When <code>kernel=FALSE</code> the model uses indicator basis functions for
the nominal/ordinal (<code><a href="base.html#topic+factor">factor</a></code>/<code><a href="base.html#topic+ordered">ordered</a></code>)
predictors rather than kernel weighting.
</p>
<p>When <code>kernel=TRUE</code> the product kernel function for the discrete
predictors is of the &lsquo;Li-Racine&rsquo; type (see Li and Racine (2007)
for details).
</p>
<p>When <code>cv="nomad"</code>, numerical search is undertaken using Nonsmooth
Optimization by Mesh Adaptive Direct Search (Abramson, Audet, Couture,
Dennis, Jr., and Le Digabel (2011)).
</p>
<p>When <code>kernel=TRUE</code> and <code>cv="exhaustive"</code>, numerical search
is undertaken using <code><a href="stats.html#topic+optim">optim</a></code> and the box-constrained
<code>L-BFGS-B</code> method (see <code><a href="stats.html#topic+optim">optim</a></code> for details). The user
may restart the algorithm as many times as desired via the
<code>restarts</code> argument (default <code>restarts=0</code>). The approach
ascends from <code>degree=0</code> (or <code>segments=0</code>) through
<code>degree.max</code> and for each value of <code>degree</code> (or
<code>segments</code>) searches for the optimal bandwidths. After the most
complex model has been searched then the optimal
<code>degree</code>/<code>segments</code>/<code>lambda</code> combination is
selected. If any element of the optimal <code>degree</code> (or
<code>segments</code>) vector coincides with <code>degree.max</code> (or <code>segments.max</code>) a warning
is produced and the user ought to restart their search with a larger
value of <code>degree.max</code> (or <code>segments.max</code>).
</p>
<p>Note that the default <code>plot</code> method for a <code>crs</code> object
provides some diagnostic measures, in particular, a) residuals versus
fitted values (useful for checking the assumption that
<code>E(u|x)=0</code>), b) a normal quantile-quantile plot which allows
residuals to be assessed for normality (<code><a href="stats.html#topic+qqnorm">qqnorm</a></code>), c) a
scale-location plot that is useful for checking the assumption that
the errors are iid and, in particular, that the variance is
homogeneous, and d) &lsquo;Cook's distance&rsquo; which computes the
single-case influence function. See below for other arguments for the
plot function for a <code>crs</code> object.
</p>
<p>Note that setting <code>prune=TRUE</code> produces a final &lsquo;pruning&rsquo;
of the model via a stepwise cross-validation criterion achieved by
modifying <code><a href="MASS.html#topic+stepAIC">stepAIC</a></code> and replacing <code>extractAIC</code>
with <code>extractCV</code> throughout the function. This option may be
enabled to remove potentially superfluous bases thereby improving the
finite-sample efficiency of the resulting model.  Note that if the
cross-validation score for the pruned model is no better than that for
the original model then the original model is returned with a warning
to this effect. Note also that this option can only be used when
<code>kernel=FALSE</code>.
</p>


<h3>Value</h3>

<p><code>crs</code> returns a <code>crs</code> object.  The generic functions
<code><a href="stats.html#topic+fitted">fitted</a></code> and <code><a href="stats.html#topic+residuals">residuals</a></code> extract (or
generate) estimated values and residuals. Furthermore, the functions
<code><a href="base.html#topic+summary">summary</a></code>, <code><a href="stats.html#topic+predict">predict</a></code>, and <code><a href="graphics.html#topic+plot">plot</a></code>
(options <code>mean=FALSE</code>, <code>deriv=i</code> where <code class="reqn">i</code> is an
integer, <code>ci=FALSE</code>, <code>persp.rgl=FALSE</code>,
<code>plot.behavior=c("plot","plot-data","data")</code>,
<code>xtrim=0.0</code>,<code>xq=0.5</code>) support objects of this type. The
returned object has the following components:
</p>
<table>
<tr><td><code>fitted.values</code></td>
<td>
<p> estimates of the regression function
(conditional mean) at the sample points or evaluation points </p>
</td></tr>
<tr><td><code>lwr</code>, <code>upr</code></td>
<td>
<p> lower/upper bound for a 95% confidence interval for
the <code>fitted.values</code> (conditional mean) obtained from
<code><a href="stats.html#topic+predict.lm">predict.lm</a></code> via the argument
<code>interval="confidence"</code></p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p> residuals computed at the sample points or
evaluation points </p>
</td></tr>
<tr><td><code>degree</code></td>
<td>
<p> integer/vector specifying the degree of the B-spline
basis for each dimension of the continuous <code>x</code></p>
</td></tr>
<tr><td><code>segments</code></td>
<td>
<p> integer/vector specifying the number of segments of
the B-spline basis for each dimension of the continuous <code>x</code></p>
</td></tr>
<tr><td><code>include</code></td>
<td>
<p> integer/vector specifying whether each of the
nominal/ordinal (<code><a href="base.html#topic+factor">factor</a></code>/<code><a href="base.html#topic+ordered">ordered</a></code>)
predictors <code>z</code> are included or omitted from the resulting
estimate if <code>kernel=FALSE</code> (see below)</p>
</td></tr>
<tr><td><code>kernel</code></td>
<td>
<p> a logical value indicating whether kernel smoothing was
used (<code>kernel=TRUE</code>) or not </p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p> vector of bandwidths used if <code>kernel=TRUE</code></p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p> a symbolic description of the model  </p>
</td></tr>
<tr><td><code>r.squared</code></td>
<td>
<p> coefficient of determination (Doksum and Samarov
(1995)) </p>
</td></tr>
<tr><td><code>model.lm</code></td>
<td>
<p> an object of &lsquo;<code><a href="base.html#topic+class">class</a></code>&rsquo;
&lsquo;<code><a href="stats.html#topic+lm">lm</a></code>&rsquo; if <code>kernel=FALSE</code> or a list of objects
of &lsquo;<code><a href="base.html#topic+class">class</a></code>&rsquo; &lsquo;<code><a href="stats.html#topic+lm">lm</a></code>&rsquo; if
<code>kernel=TRUE</code> (accessed by <code>model.lm[[1]]</code>,
<code>model.lm[[2]]</code>,...,. By way of example, if <code>foo</code> is a
<code>crs</code> object and <code>kernel=FALSE</code>, then <code>foo$model.lm</code>
is an object of &lsquo;<code><a href="base.html#topic+class">class</a></code>&rsquo;
&lsquo;<code><a href="stats.html#topic+lm">lm</a></code>&rsquo;, while objects of
&lsquo;<code><a href="base.html#topic+class">class</a></code>&rsquo; &lsquo;<code><a href="stats.html#topic+lm">lm</a></code>&rsquo; return the
<code><a href="stats.html#topic+model.frame">model.frame</a></code> in <code>model.lm$model</code> which can be
accessed via <code>foo$model.lm$model</code> where <code>foo</code> is the
<code>crs</code> object (the model frame <code>foo$model.lm$model</code>
contains the B-spline bases underlying the estimate which might be of
interest). Again by way of example, when <code>kernel=TRUE</code> then
<code>foo$model.lm[[1]]$model</code> contains the model frame for the first
unique combination of categorical predictors,
<code>foo$model.lm[[2]]$model</code> the second and so forth (the weights
will potentially differ for each model depending on the value(s) of
<code>lambda</code>)</p>
</td></tr>
<tr><td><code>deriv.mat</code></td>
<td>
<p> a matrix of derivatives (or differences in levels
for the categorical <code>z</code>) whose order is determined by
<code>deriv=</code> in the <code>crs</code> call </p>
</td></tr>
<tr><td><code>deriv.mat.lwr</code></td>
<td>
<p> a matrix of 95% coverage lower bounds for
<code>deriv.mat</code></p>
</td></tr>
<tr><td><code>deriv.mat.upr</code></td>
<td>
<p> a matrix of 95% coverage upper bounds for
<code>deriv.mat</code></p>
</td></tr>
<tr><td><code>hatvalues</code></td>
<td>
<p> the <code><a href="stats.html#topic+hatvalues">hatvalues</a></code> for the estimated model</p>
</td></tr>
<tr><td><code>P.hat</code></td>
<td>
<p> the kernel probability estimates corresponding to the
categorical predictors in the estimated model </p>
</td></tr>
</table>


<h3>Usage Issues</h3>

<p>Note that when <code>kernel=FALSE</code> <code><a href="base.html#topic+summary">summary</a></code> supports the
option <code>sigtest=TRUE</code> that conducts an F-test for significance
for each predictor.
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Racine <a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Abramson, M.A. and C. Audet and G. Couture and J.E. Dennis Jr. and and
S. Le Digabel (2011), &ldquo;The NOMAD project&rdquo;. Software available
at https://www.gerad.ca/nomad.
</p>
<p>Craven, P. and G. Wahba (1979), &ldquo;Smoothing Noisy Data With
Spline Functions,&rdquo; Numerische Mathematik, 13, 377-403.
</p>
<p>Doksum, K. and A. Samarov (1995), &ldquo;Nonparametric Estimation of
Global Functionals and a Measure of the Explanatory Power of
Covariates in Regression,&rdquo; The Annals of Statistics, 23 1443-1473.
</p>
<p>Hurvich, C.M. and J.S. Simonoff and C.L. Tsai (1998),
&ldquo;Smoothing Parameter Selection in Nonparametric Regression
Using an Improved Akaike Information Criterion,&rdquo; Journal of the
Royal Statistical Society B, 60, 271-293.
</p>
<p>Le Digabel, S. (2011), &ldquo;Algorithm 909: NOMAD: Nonlinear
Optimization With The MADS Algorithm&rdquo;. ACM Transactions on
Mathematical Software, 37(4):44:1-44:15.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics:
Theory and Practice,</em> Princeton University Press.
</p>
<p>Ma, S. and J.S. Racine and L. Yang (2015), &ldquo;Spline
Regression in the Presence of Categorical Predictors,&rdquo; Journal of
Applied Econometrics, Volume 30, 705-717.
</p>
<p>Ma, S. and J.S. Racine (2013), &ldquo;Additive Regression
Splines with Irrelevant Categorical and Continuous Regressors,&rdquo;
Statistica Sinica, Volume 23, 515-541.
</p>
<p>Racine, J.S. (2011), &ldquo;Cross-Validated Quantile Regression
Splines,&rdquo; manuscript.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+smooth.spline">smooth.spline</a></code>, <code><a href="stats.html#topic+loess">loess</a></code>, <code><a href="np.html#topic+npreg">npreg</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(42)
## Example - simulated data
n &lt;- 1000
num.eval &lt;- 50
x1 &lt;- runif(n)
x2 &lt;- runif(n)
z &lt;- rbinom(n,1,.5)
dgp &lt;- cos(2*pi*x1)+sin(2*pi*x2)+z
z &lt;- factor(z)
y &lt;- dgp + rnorm(n,sd=.5)

## Estimate a model with specified degree, segments, and bandwidth
model &lt;- crs(y~x1+x2+z,degree=c(5,5),
                       segments=c(1,1),
                       lambda=0.1,
                       cv="none",
                       kernel=TRUE)
summary(model)

## Perspective plot
x1.seq &lt;- seq(min(x1),max(x1),length=num.eval)
x2.seq &lt;- seq(min(x2),max(x2),length=num.eval)
x.grid &lt;- expand.grid(x1.seq,x2.seq)
newdata &lt;- data.frame(x1=x.grid[,1],x2=x.grid[,2],
                      z=factor(rep(0,num.eval**2),levels=c(0,1)))
z0 &lt;- matrix(predict(model,newdata=newdata),num.eval,num.eval)
newdata &lt;- data.frame(x1=x.grid[,1],x2=x.grid[,2],
                      z=factor(rep(1,num.eval),levels=c(0,1)))
z1 &lt;- matrix(predict(model,newdata=newdata),num.eval,num.eval)
zlim=c(min(z0,z1),max(z0,z1))
persp(x=x1.seq,y=x2.seq,z=z0,
      xlab="x1",ylab="x2",zlab="y",zlim=zlim,
      ticktype="detailed",      
      border="red",
      theta=45,phi=45)
par(new=TRUE)
persp(x=x1.seq,y=x2.seq,z=z1,
      xlab="x1",ylab="x2",zlab="y",zlim=zlim,
      theta=45,phi=45,
      ticktype="detailed",
      border="blue")

## Partial regression surface plot
plot(model,mean=TRUE,ci=TRUE)
## Not run: 
## A plot example where we extract the partial surfaces, confidence
## intervals etc. automatically generated by plot(mean=TRUE,...) but do
## not plot, rather save for separate use.
pdat &lt;- plot(model,mean=TRUE,ci=TRUE,plot.behavior="data")

## Column 1 is the (evaluation) predictor ([,1]), 2-4 ([,-1]) the mean,
## lwr, and upr (note the returned value is a 'list' hence pdat[[1]] is
## data for the first predictor, pdat[[2]] the second etc). Note that
## matplot() can plot this nicely.
matplot(pdat[[1]][,1],pdat[[1]][,-1],
        xlab=names(pdat[[1]][1]),ylab=names(pdat[[1]][2]),
        lty=c(1,2,2),col=c(1,2,2),type="l")

## End(Not run)
</code></pre>

<hr>
<h2 id='crsiv'>
Nonparametric Instrumental Regression
</h2><span id='topic+crsiv'></span>

<h3>Description</h3>

<p><code>crsiv</code> computes nonparametric estimation of an instrumental
regression function <code class="reqn">\varphi</code> defined by conditional moment
restrictions stemming from a structural econometric model: <code class="reqn">E [Y -
\varphi (Z,X) | W ] = 0</code>, and involving
endogenous variables <code class="reqn">Y</code> and <code class="reqn">Z</code>, exogenous variables <code class="reqn">X</code>,
and instruments <code class="reqn">W</code>. The function <code class="reqn">\varphi</code> is the solution
of an ill-posed inverse problem.
</p>
<p>When <code>method="Tikhonov"</code>, <code>crsiv</code> uses the approach of
Darolles, Fan, Florens and Renault (2011) modified for regression
splines (Darolles et al use local constant kernel weighting). When
<code>method="Landweber-Fridman"</code>, <code>crsiv</code> uses the approach of
Horowitz (2011) using the regression spline methodology implemented in
the <span class="pkg">crs</span> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crsiv(y,
      z,
      w,
      x = NULL,
      zeval = NULL,
      weval = NULL,
      xeval = NULL,
      alpha = NULL,
      alpha.min = 1e-10,
      alpha.max = 1e-01,
      alpha.tol = .Machine$double.eps^0.25,
      deriv = 0,
      iterate.max = 1000,
      iterate.diff.tol = 1.0e-08,
      constant = 0.5,
      penalize.iteration = TRUE,
      smooth.residuals = TRUE,
      start.from = c("Eyz","EEywz"),
      starting.values = NULL,
      stop.on.increase = TRUE,
      method = c("Landweber-Fridman","Tikhonov"),
      opts = list("MAX_BB_EVAL"=10000,
                  "EPSILON"=.Machine$double.eps,
                  "INITIAL_MESH_SIZE"="r1.0e-01",
                  "MIN_MESH_SIZE"=paste("r",sqrt(.Machine$double.eps),sep=""),
                  "MIN_POLL_SIZE"=paste("r",1,sep=""),
                  "DISPLAY_DEGREE"=0),
      ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crsiv_+3A_y">y</code></td>
<td>

<p>a one (1) dimensional numeric or integer vector of dependent data, each
element <code class="reqn">i</code> corresponding to each observation (row) <code class="reqn">i</code> of
<code>z</code>
</p>
</td></tr>
<tr><td><code id="crsiv_+3A_z">z</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of endogenous predictors. The data
types may be continuous, discrete (unordered and ordered factors),
or some combination thereof
</p>
</td></tr>
<tr><td><code id="crsiv_+3A_w">w</code></td>
<td>

<p>a <code class="reqn">q</code>-variate data frame of instruments. The data types may be
continuous, discrete (unordered and ordered factors), or some
combination thereof
</p>
</td></tr>
<tr><td><code id="crsiv_+3A_x">x</code></td>
<td>

<p>an <code class="reqn">r</code>-variate data frame of exogenous predictors. The data
types may be continuous, discrete (unordered and ordered factors),
or some combination thereof
</p>
</td></tr>
<tr><td><code id="crsiv_+3A_zeval">zeval</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of endogenous predictors on which the
regression will be estimated (evaluation data). By default, evaluation
takes place on the data provided by <code>z</code>
</p>
</td></tr>
<tr><td><code id="crsiv_+3A_weval">weval</code></td>
<td>

<p>a <code class="reqn">q</code>-variate data frame of instruments on which the regression
will be estimated (evaluation data). By default, evaluation
takes place on the data provided by <code>w</code>
</p>
</td></tr>
<tr><td><code id="crsiv_+3A_xeval">xeval</code></td>
<td>

<p>an <code class="reqn">r</code>-variate data frame of exogenous predictors on which the
regression will be estimated (evaluation data). By default,
evaluation takes place on the data provided by <code>x</code>
</p>
</td></tr>
<tr><td><code id="crsiv_+3A_alpha">alpha</code></td>
<td>

<p>a numeric scalar that, if supplied, is used rather than numerically
solving for <code>alpha</code>, when using <code>method="Tikhonov"</code>
</p>
</td></tr>
<tr><td><code id="crsiv_+3A_alpha.min">alpha.min</code></td>
<td>

<p>minimum of search range for <code class="reqn">\alpha</code>, the Tikhonov
regularization parameter, when using <code>method="Tikhonov"</code>
</p>
</td></tr>
<tr><td><code id="crsiv_+3A_alpha.max">alpha.max</code></td>
<td>

<p>maximum of search range for <code class="reqn">\alpha</code>, the Tikhonov
regularization parameter, when using  <code>method="Tikhonov"</code>
</p>
</td></tr>
<tr><td><code id="crsiv_+3A_alpha.tol">alpha.tol</code></td>
<td>

<p>the search tolerance for <code>optimize</code> when solving for
<code class="reqn">\alpha</code>, the Tikhonov regularization parameter, 
when using <code>method="Tikhonov"</code>
</p>
</td></tr>
<tr><td><code id="crsiv_+3A_iterate.max">iterate.max</code></td>
<td>

<p>an integer indicating the maximum number of iterations permitted
before termination occurs when using <code>method="Landweber-Fridman"</code>
</p>
</td></tr>
<tr><td><code id="crsiv_+3A_iterate.diff.tol">iterate.diff.tol</code></td>
<td>

<p>the search tolerance for the difference in the stopping rule from
iteration to iteration when using <code>method="Landweber-Fridman"</code>
(disable by setting to zero)
</p>
</td></tr>
<tr><td><code id="crsiv_+3A_constant">constant</code></td>
<td>

<p>the constant to use when using  <code>method="Landweber-Fridman"</code>
</p>
</td></tr>
<tr><td><code id="crsiv_+3A_method">method</code></td>
<td>

<p>the regularization method employed (default
<code>"Landweber-Fridman"</code>, see Horowitz (2011); see Darolles, Fan,
Florens and Renault (2011) for details for <code>"Tikhonov"</code>)
</p>
</td></tr>
<tr><td><code id="crsiv_+3A_penalize.iteration">penalize.iteration</code></td>
<td>
<p> a logical value indicating whether to
penalize the norm by the number of iterations or not (default
<code>TRUE</code>)
</p>
</td></tr>    
<tr><td><code id="crsiv_+3A_smooth.residuals">smooth.residuals</code></td>
<td>

<p>a logical value (defaults to <code>TRUE</code>) indicating whether to
optimize bandwidths for the regression of <code class="reqn">y-\varphi(z)</code>
on <code class="reqn">w</code> or for the regression of <code class="reqn">\varphi(z)</code> on
<code class="reqn">w</code> during iteration
</p>
</td></tr>
<tr><td><code id="crsiv_+3A_start.from">start.from</code></td>
<td>
<p> a character string indicating whether to start from
<code class="reqn">E(Y|z)</code> (default, <code>"Eyz"</code>) or from <code class="reqn">E(E(Y|z)|z)</code> (this can
be overridden by providing <code>starting.values</code> below)
</p>
</td></tr>  
<tr><td><code id="crsiv_+3A_starting.values">starting.values</code></td>
<td>
<p> a value indicating whether to commence
Landweber-Fridman assuming
<code class="reqn">\varphi_{-1}=starting.values</code> (proper
Landweber-Fridman) or instead begin from <code class="reqn">E(y|z)</code> (defaults to
<code>NULL</code>, see details below)
</p>
</td></tr>
<tr><td><code id="crsiv_+3A_stop.on.increase">stop.on.increase</code></td>
<td>

<p>a logical value (defaults to <code>TRUE</code>) indicating whether to halt
iteration if the stopping criterion (see below) increases over the
course of one iteration (i.e. it may be above the iteration tolerance
but increased)
</p>
</td></tr>
<tr><td><code id="crsiv_+3A_opts">opts</code></td>
<td>

<p>arguments passed to the NOMAD solver (see <code><a href="#topic+snomadr">snomadr</a></code> for
further details)
</p>
</td></tr>
<tr><td><code id="crsiv_+3A_deriv">deriv</code></td>
<td>
<p> an integer <code>l</code> (default <code>deriv=0</code>) specifying
whether to compute the univariate <code>l</code>th partial derivative for
each continuous predictor (and difference in levels for each
categorical predictor) or not and if so what order. Note that if
<code>deriv</code> is higher than the spline degree of the associated
continuous predictor then the derivative will be zero and a warning
issued to this effect (see important note below)
</p>
</td></tr>
<tr><td><code id="crsiv_+3A_...">...</code></td>
<td>

<p>additional arguments supplied to <code><a href="#topic+crs">crs</a></code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Tikhonov regularization requires computation of weight matrices of
dimension <code class="reqn">n\times n</code> which can be computationally costly
in terms of memory requirements and may be unsuitable
(i.e. unfeasible) for large datasets. Landweber-Fridman will be
preferred in such settings as it does not require construction and
storage of these weight matrices while it also avoids the need for
numerical optimization methods to determine <code class="reqn">\alpha</code>,
though it does require iteration that may be equally or even more
computationally demanding in terms of total computation time.
</p>
<p>When using <code>method="Landweber-Fridman"</code>, an optimal stopping rule
based upon <code class="reqn">||E(y|w)-E(\varphi_k(z,x)|w)||^2
  </code> is used to terminate
iteration. However, if local rather than global optima are encountered
the resulting estimates can be overly noisy. To best guard against
this eventuality set <code>nmulti</code> to a larger number than the default
<code>nmulti=5</code> for <code><a href="#topic+crs">crs</a></code> when using <code>cv="nomad"</code> or
instead use <code>cv="exhaustive"</code> if possible (this may not be
feasible for non-trivial problems).
</p>
<p>When using <code>method="Landweber-Fridman"</code>, iteration will terminate
when either the change in the value of
<code class="reqn">||(E(y|w)-E(\varphi_k(z,x)|w))/E(y|w)||^2
  </code> from iteration to iteration is
less than <code>iterate.diff.tol</code> or we hit <code>iterate.max</code> or
<code class="reqn">||(E(y|w)-E(\varphi_k(z,x)|w))/E(y|w)||^2
  </code> stops falling in value and
starts rising.
</p>
<p>When your problem is a simple one (e.g. univariate <code class="reqn">Z</code>, <code class="reqn">W</code>,
and <code class="reqn">X</code>) you might want to avoid <code>cv="nomad"</code> and instead use
<code>cv="exhaustive"</code> since exhaustive search may be feasible (for
<code>degree.max</code> and <code>segments.max</code> not overly large). This will
guarantee an exact solution for each iteration (i.e. there will be no
errors arising due to numerical search).
</p>
<p><code>demo(crsiv)</code>, <code>demo(crsiv_exog)</code>, and
<code>demo(crsiv_exog_persp)</code> provide flexible interactive
demonstrations similar to the example below that allow you to modify
and experiment with parameters such as the sample size, method, and so
forth in an interactive session.
</p>


<h3>Value</h3>

<p><code>crsiv</code> returns a <code><a href="#topic+crs">crs</a></code> object.  The generic
functions <code><a href="stats.html#topic+fitted">fitted</a></code> and <code><a href="stats.html#topic+residuals">residuals</a></code> extract
(or generate) estimated values and residuals. Furthermore, the
functions <code><a href="base.html#topic+summary">summary</a></code>, <code><a href="stats.html#topic+predict">predict</a></code>, and
<code><a href="graphics.html#topic+plot">plot</a></code> (options <code>mean=FALSE</code>, <code>deriv=i</code> where
<code class="reqn">i</code> is an integer, <code>ci=FALSE</code>,
<code>plot.behavior=c("plot","plot-data","data")</code>) support objects
of this type.
</p>
<p>See <code><a href="#topic+crs">crs</a></code> for details on the return object components.
</p>
<p>In addition to the standard <code><a href="#topic+crs">crs</a></code> components,
<code>crsiv</code> returns components <code>phi</code> and either <code>alpha</code>
when <code>method="Tikhonov"</code> or <code>phi</code>, <code>phi.mat</code>,
<code>num.iterations</code>, <code>norm.stop</code>, <code>norm.value</code> and
<code>convergence</code> when <code>method="Landweber-Fridman"</code>.
</p>


<h3>Note</h3>

<p>Using the option <code>deriv=</code> computes (effectively) the analytical
derivative of the estimated <code class="reqn">\varphi(Z,X)</code> and not that
using <code><a href="#topic+crsivderiv">crsivderiv</a></code>, which instead uses the method of
Florens and Racine (2012). Though both are statistically consistent,
practitioners may desire one over the other hence we provide both.
</p>


<h3>Note</h3>

<p>This function should be considered to be in &lsquo;beta test&rsquo; status until further notice.
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Racine <a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>, Samuele Centorrino
<a href="mailto:samuele.centorrino@univ-tlse1.fr">samuele.centorrino@univ-tlse1.fr</a>
</p>


<h3>References</h3>

<p>Carrasco, M. and J.P. Florens and E. Renault (2007), &ldquo;Linear
Inverse Problems in Structural Econometrics Estimation Based on
Spectral Decomposition and Regularization,&rdquo; In: James J. Heckman and
Edward E. Leamer, Editor(s), Handbook of Econometrics, Elsevier, 2007,
Volume 6, Part 2, Chapter 77, Pages 5633-5751
</p>
<p>Darolles, S. and Y. Fan and J.P. Florens and E. Renault (2011),
&ldquo;Nonparametric Instrumental Regression,&rdquo; Econometrica, 79,
1541-1565.
</p>
<p>Feve, F. and J.P. Florens (2010), &ldquo;The Practice of
Non-parametric Estimation by Solving Inverse Problems: The Example of
Transformation Models,&rdquo; Econometrics Journal, 13, S1-S27.
</p>
<p>Florens, J.P. and J.S. Racine (2012), &ldquo;Nonparametric
Instrumental Derivatives,&rdquo; Working Paper.
</p>
<p>Fridman, V. M. (1956), &ldquo;A Method of Successive Approximations
for Fredholm Integral Equations of the First Kind,&rdquo; Uspeskhi,
Math. Nauk., 11, 233-334, in Russian.
</p>
<p>Horowitz, J.L. (2011), &ldquo;Applied Nonparametric Instrumental
Variables Estimation,&rdquo; Econometrica, 79, 347-394.
</p>
<p>Landweber, L. (1951), &ldquo;An Iterative Formula for Fredholm
Integral Equations of the First Kind,&rdquo; American Journal of
Mathematics, 73, 615-24.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics:
Theory and Practice,</em> Princeton University Press.
</p>


<h3>See Also</h3>

<p><code><a href="np.html#topic+npreg">npreg</a></code>, <code><a href="#topic+crs">crs</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## This illustration was made possible by Samuele Centorrino
## &lt;samuele.centorrino@univ-tlse1.fr&gt;

set.seed(42)
n &lt;- 1500

## The DGP is as follows:

## 1) y = phi(z) + u

## 2) E(u|z) != 0 (endogeneity present)

## 3) Suppose there exists an instrument w such that z = f(w) + v and
## E(u|w) = 0

## 4) We generate v, w, and generate u such that u and z are
## correlated. To achieve this we express u as a function of v (i.e. u =
## gamma v + eps)

v &lt;- rnorm(n,mean=0,sd=0.27)
eps &lt;- rnorm(n,mean=0,sd=0.05)
u &lt;- -0.5*v + eps
w &lt;- rnorm(n,mean=0,sd=1)

## In Darolles et al (2011) there exist two DGPs. The first is
## phi(z)=z^2 and the second is phi(z)=exp(-abs(z)) (which is
## discontinuous and has a kink at zero).

fun1 &lt;- function(z) { z^2 }
fun2 &lt;- function(z) { exp(-abs(z)) }

z &lt;- 0.2*w + v

## Generate two y vectors for each function.

y1 &lt;- fun1(z) + u
y2 &lt;- fun2(z) + u

## You set y to be either y1 or y2 (ditto for phi) depending on which
## DGP you are considering:

y &lt;- y1
phi &lt;- fun1

## Create an evaluation dataset sorting on z (for plotting)

evaldata &lt;- data.frame(y,z,w)
evaldata &lt;- evaldata[order(evaldata$z),]

## Compute the non-IV regression spline estimator of E(y|z)

model.noniv &lt;- crs(y~z,opts=opts)
mean.noniv &lt;- predict(model.noniv,newdata=evaldata)

## Compute the IV-regression spline estimator of phi(z)

model.iv &lt;- crsiv(y=y,z=z,w=w)
phi.iv &lt;- predict(model.iv,newdata=evaldata)

## For the plots, restrict focal attention to the bulk of the data
## (i.e. for the plotting area trim out 1/4 of one percent from each
## tail of y and z)

trim &lt;- 0.0025

curve(phi,min(z),max(z),
      xlim=quantile(z,c(trim,1-trim)),
      ylim=quantile(y,c(trim,1-trim)),
      ylab="Y",
      xlab="Z",
      main="Nonparametric Instrumental Spline Regression",
      sub=paste("Landweber-Fridman: iterations = ", model.iv$num.iterations,sep=""),
      lwd=1,lty=1)

points(z,y,type="p",cex=.25,col="grey")

lines(evaldata$z,evaldata$z^2 -0.325*evaldata$z,lwd=1,lty=1)

lines(evaldata$z,phi.iv,col="blue",lwd=2,lty=2)

lines(evaldata$z,mean.noniv,col="red",lwd=2,lty=4)

legend(quantile(z,trim),quantile(y,1-trim),
       c(expression(paste(varphi(z),", E(y|z)",sep="")),
         expression(paste("Nonparametric ",hat(varphi)(z))),
         "Nonparametric E(y|z)"),
       lty=c(1,2,4),
       col=c("black","blue","red"),
       lwd=c(1,2,2))

## End(Not run) 
</code></pre>

<hr>
<h2 id='crsivderiv'>
Nonparametric Instrumental Derivatives
</h2><span id='topic+crsivderiv'></span>

<h3>Description</h3>

<p><code>crsivderiv</code> uses the approach of Florens and Racine (2012) to
compute the partial derivative of a nonparametric estimation of an
instrumental regression function <code class="reqn">\varphi</code> defined by
conditional moment restrictions stemming from a structural econometric
model: <code class="reqn">E [Y - \varphi (Z,X) | W ] = 0</code>, and involving endogenous variables <code class="reqn">Y</code> and <code class="reqn">Z</code> and
exogenous variables <code class="reqn">X</code> and instruments <code class="reqn">W</code>. The derivative
function <code class="reqn">\varphi'</code> is the solution of an ill-posed inverse
problem, and is computed using Landweber-Fridman regularization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crsivderiv(y,
           z,
           w,
           x = NULL,
           zeval = NULL,
           weval = NULL,
           xeval = NULL,
           iterate.max = 1000,
           iterate.diff.tol = 1.0e-08,
           constant = 0.5,
           penalize.iteration = TRUE,
           start.from = c("Eyz","EEywz"),
           starting.values = NULL,
           stop.on.increase = TRUE,
           smooth.residuals = TRUE,
           opts = list("MAX_BB_EVAL"=10000,
                       "EPSILON"=.Machine$double.eps,
                       "INITIAL_MESH_SIZE"="r1.0e-01",
                       "MIN_MESH_SIZE"=paste("r",sqrt(.Machine$double.eps),sep=""),
                       "MIN_POLL_SIZE"=paste("r",1,sep=""),
                       "DISPLAY_DEGREE"=0),
           ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crsivderiv_+3A_y">y</code></td>
<td>

<p>a one (1) dimensional numeric or integer vector of dependent data, each
element <code class="reqn">i</code> corresponding to each observation (row) <code class="reqn">i</code> of
<code>z</code>
</p>
</td></tr>
<tr><td><code id="crsivderiv_+3A_z">z</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of endogenous predictors. The data
types may be continuous, discrete (unordered and ordered factors),
or some combination thereof
</p>
</td></tr>
<tr><td><code id="crsivderiv_+3A_w">w</code></td>
<td>

<p>a <code class="reqn">q</code>-variate data frame of instruments. The data types may be
continuous, discrete (unordered and ordered factors), or some
combination thereof
</p>
</td></tr>
<tr><td><code id="crsivderiv_+3A_x">x</code></td>
<td>

<p>an <code class="reqn">r</code>-variate data frame of exogenous predictors. The data
types may be continuous, discrete (unordered and ordered factors),
or some combination thereof
</p>
</td></tr>
<tr><td><code id="crsivderiv_+3A_zeval">zeval</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of endogenous predictors on which the
regression will be estimated (evaluation data). By default, evaluation
takes place on the data provided by <code>z</code>
</p>
</td></tr>
<tr><td><code id="crsivderiv_+3A_weval">weval</code></td>
<td>

<p>a <code class="reqn">q</code>-variate data frame of instruments on which the regression
will be estimated (evaluation data). By default, evaluation
takes place on the data provided by <code>w</code>
</p>
</td></tr>
<tr><td><code id="crsivderiv_+3A_xeval">xeval</code></td>
<td>

<p>an <code class="reqn">r</code>-variate data frame of exogenous predictors on which the
regression will be estimated (evaluation data). By default,
evaluation takes place on the data provided by <code>x</code>
</p>
</td></tr>
<tr><td><code id="crsivderiv_+3A_iterate.max">iterate.max</code></td>
<td>

<p>an integer indicating the maximum number of iterations permitted
before termination occurs when using Landweber-Fridman iteration
</p>
</td></tr>
<tr><td><code id="crsivderiv_+3A_iterate.diff.tol">iterate.diff.tol</code></td>
<td>

<p>the search tolerance for the difference in the stopping rule from
iteration to iteration when using Landweber-Fridman
(disable by setting to zero)
</p>
</td></tr>
<tr><td><code id="crsivderiv_+3A_constant">constant</code></td>
<td>

<p>the constant to use when using  Landweber-Fridman iteration
</p>
</td></tr>
<tr><td><code id="crsivderiv_+3A_penalize.iteration">penalize.iteration</code></td>
<td>
<p> a logical value indicating whether to
penalize the norm by the number of iterations or not (default
<code>TRUE</code>)
</p>
</td></tr>    
<tr><td><code id="crsivderiv_+3A_start.from">start.from</code></td>
<td>
<p> a character string indicating whether to start from
<code class="reqn">E(Y|z)</code> (default, <code>"Eyz"</code>) or from <code class="reqn">E(E(Y|z)|z)</code> (this can
be overridden by providing <code>starting.values</code> below)
</p>
</td></tr>  
<tr><td><code id="crsivderiv_+3A_starting.values">starting.values</code></td>
<td>
<p> a value indicating whether to commence
Landweber-Fridman assuming
<code class="reqn">\varphi'_{-1}=starting.values</code> (proper
Landweber-Fridman) or instead begin from <code class="reqn">E(y|z)</code> (defaults to
<code>NULL</code>, see details below)
</p>
</td></tr>
<tr><td><code id="crsivderiv_+3A_stop.on.increase">stop.on.increase</code></td>
<td>

<p>a logical value (defaults to <code>TRUE</code>) indicating whether to halt
iteration if the stopping criterion (see below) increases over the
course of one iteration (i.e. it may be above the iteration tolerance
but increased)
</p>
</td></tr>
<tr><td><code id="crsivderiv_+3A_smooth.residuals">smooth.residuals</code></td>
<td>

<p>a logical value (defaults to <code>TRUE</code>) indicating whether to
optimize bandwidths for the regression of <code class="reqn">y-\varphi(z)</code>
on <code class="reqn">w</code> or for the regression of <code class="reqn">\varphi(z)</code> on
<code class="reqn">w</code> during iteration
</p>
</td></tr>
<tr><td><code id="crsivderiv_+3A_opts">opts</code></td>
<td>

<p>arguments passed to the NOMAD solver (see <code><a href="#topic+snomadr">snomadr</a></code> for
further details)
</p>
</td></tr>
<tr><td><code id="crsivderiv_+3A_...">...</code></td>
<td>

<p>additional arguments supplied to <code><a href="#topic+crs">crs</a></code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For Landweber-Fridman iteration, an optimal stopping rule based upon
<code class="reqn">||E(y|w)-E(\varphi_k(z,x)|w)||^2 </code>
is used to terminate iteration. However, if local rather than global
optima are encountered the resulting estimates can be overly noisy. To
best guard against this eventuality set <code>nmulti</code> to a larger
number than the default <code>nmulti=5</code> for <code><a href="#topic+crs">crs</a></code> when
using <code>cv="nomad"</code> or instead use <code>cv="exhaustive"</code> if
possible (this may not be feasible for non-trivial problems).
</p>
<p>When using Landweber-Fridman iteration, iteration will terminate
when either the change in the value of
<code class="reqn">||(E(y|w)-E(\varphi_k(z,x)|w))/E(y|w)||^2
  </code> from iteration to iteration is
less than <code>iterate.diff.tol</code> or we hit <code>iterate.max</code> or
<code class="reqn">||(E(y|w)-E(\varphi_k(z,x)|w))/E(y|w)||^2
  </code> stops falling in value and
starts rising.
</p>
<p>When your problem is a simple one (e.g. univariate <code class="reqn">Z</code>, <code class="reqn">W</code>,
and <code class="reqn">X</code>) you might want to avoid <code>cv="nomad"</code> and instead use
<code>cv="exhaustive"</code> since exhaustive search may be feasible (for
<code>degree.max</code> and <code>segments.max</code> not overly large). This will
guarantee an exact solution for each iteration (i.e. there will be no
errors arising due to numerical search).
</p>


<h3>Value</h3>

<p><code>crsivderiv</code> returns components <code>phi.prime</code>, <code>phi</code>,
<code>phi.prime.mat</code>, <code>num.iterations</code>, <code>norm.stop</code>,
<code>norm.value</code> and <code>convergence</code>.
</p>


<h3>Note</h3>

<p>This function currently supports univariate <code>z</code> only.
This function should be considered to be in &lsquo;beta test&rsquo; status until
further notice.
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Racine <a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Carrasco, M. and J.P. Florens and E. Renault (2007), &ldquo;Linear
Inverse Problems in Structural Econometrics Estimation Based on
Spectral Decomposition and Regularization,&rdquo; In: James J. Heckman and
Edward E. Leamer, Editor(s), Handbook of Econometrics, Elsevier, 2007,
Volume 6, Part 2, Chapter 77, Pages 5633-5751
</p>
<p>Darolles, S. and Y. Fan and J.P. Florens and E. Renault (2011),
&ldquo;Nonparametric Instrumental Regression,&rdquo; Econometrica, 79,
1541-1565.
</p>
<p>Feve, F. and J.P. Florens (2010), &ldquo;The Practice of
Non-parametric Estimation by Solving Inverse Problems: The Example of
Transformation Models,&rdquo; Econometrics Journal, 13, S1-S27.
</p>
<p>Florens, J.P. and J.S. Racine (2012), &ldquo;Nonparametric
Instrumental Derivatives,&rdquo; Working Paper.
</p>
<p>Fridman, V. M. (1956), &ldquo;A Method of Successive Approximations
for Fredholm Integral Equations of the First Kind,&rdquo; Uspeskhi,
Math. Nauk., 11, 233-334, in Russian.
</p>
<p>Horowitz, J.L. (2011), &ldquo;Applied Nonparametric Instrumental
Variables Estimation,&rdquo; Econometrica, 79, 347-394.
</p>
<p>Landweber, L. (1951), &ldquo;An Iterative Formula for Fredholm
Integral Equations of the First Kind,&rdquo; American Journal of
Mathematics, 73, 615-24.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics:
Theory and Practice,</em> Princeton University Press.
</p>


<h3>See Also</h3>

<p><code><a href="np.html#topic+npreg">npreg</a></code>, <code><a href="#topic+crsiv">crsiv</a></code>, <code><a href="#topic+crs">crs</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## This illustration was made possible by Samuele Centorrino
## &lt;samuele.centorrino@univ-tlse1.fr&gt;

set.seed(42)
n &lt;- 1000

## For trimming the plot (trim .5% from each tail)

trim &lt;- 0.005

## The DGP is as follows:

## 1) y = phi(z) + u

## 2) E(u|z) != 0 (endogeneity present)

## 3) Suppose there exists an instrument w such that z = f(w) + v and
## E(u|w) = 0

## 4) We generate v, w, and generate u such that u and z are
## correlated. To achieve this we express u as a function of v (i.e. u =
## gamma v + eps)

v &lt;- rnorm(n,mean=0,sd=0.27)
eps &lt;- rnorm(n,mean=0,sd=0.05)
u &lt;- -0.5*v + eps
w &lt;- rnorm(n,mean=0,sd=1)

## In Darolles et al (2011) there exist two DGPs. The first is
## phi(z)=z^2 and the second is phi(z)=exp(-abs(z)) (which is
## discontinuous and has a kink at zero).

fun1 &lt;- function(z) { z^2 }
fun2 &lt;- function(z) { exp(-abs(z)) }

z &lt;- 0.2*w + v

## Generate two y vectors for each function.

y1 &lt;- fun1(z) + u
y2 &lt;- fun2(z) + u

## You set y to be either y1 or y2 (ditto for phi) depending on which
## DGP you are considering:

y &lt;- y1
phi &lt;- fun1

## Sort on z (for plotting)

ivdata &lt;- data.frame(y,z,w,u,v)
ivdata &lt;- ivdata[order(ivdata$z),]
rm(y,z,w,u,v)
attach(ivdata)

model.ivderiv &lt;- crsivderiv(y=y,z=z,w=w)

ylim &lt;-c(quantile(model.ivderiv$phi.prime,trim),
         quantile(model.ivderiv$phi.prime,1-trim))

plot(z,model.ivderiv$phi.prime,
     xlim=quantile(z,c(trim,1-trim)),
     main="",
     ylim=ylim,
     xlab="Z",
     ylab="Derivative",
     type="l",
     lwd=2)
rug(z)

## End(Not run) 
</code></pre>

<hr>
<h2 id='crssigtest'>Regression Spline Significance Test with Mixed Data Types</h2><span id='topic+crssigtest'></span>

<h3>Description</h3>

<p><code>crssigtest</code> implements a consistent test of significance of
an explanatory variable in a nonparametric regression setting that is
analogous to a simple <code class="reqn">t</code>-test in a parametric regression
setting. The test is based on Ma and Racine (2011).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crssigtest(model = NULL,
           index = NULL,
           boot.num = 399,
           boot.type = c("residual","reorder"),
           random.seed = 42,
           boot = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crssigtest_+3A_model">model</code></td>
<td>

<p>a <code>crs</code> model object.
</p>
</td></tr>
<tr><td><code id="crssigtest_+3A_index">index</code></td>
<td>

<p>a vector of indices for the columns of <code>model$xz</code> for which the
test of significance is to be conducted. Defaults to (1,2,...,<code class="reqn">p</code>)
where <code class="reqn">p</code> is the number of columns in <code>model$xz</code>.
</p>
</td></tr>
<tr><td><code id="crssigtest_+3A_boot.num">boot.num</code></td>
<td>

<p>an integer value specifying the number of bootstrap replications to
use. Defaults to <code>399</code>.
</p>
</td></tr>
<tr><td><code id="crssigtest_+3A_boot.type">boot.type</code></td>
<td>

<p>whether to conduct &lsquo;residual&rsquo; bootstrapping (iid) or permute
(reorder) in place the predictor being tested when imposing the
null.
</p>
</td></tr>
<tr><td><code id="crssigtest_+3A_random.seed">random.seed</code></td>
<td>

<p>an integer used to seed R's random number generator. This is to
ensure replicability. Defaults to 42.
</p>
</td></tr>
<tr><td><code id="crssigtest_+3A_boot">boot</code></td>
<td>

<p>a logical value (default <code>TRUE</code>) indicating whether to compute
the bootstrap P-value or simply return the asymptotic P-value.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>crssigtest</code> returns an object of type
<code>sigtest</code>. <code><a href="base.html#topic+summary">summary</a></code> supports <code>sigtest</code>
objects. It has the following components:
</p>
<table>
<tr><td><code>index</code></td>
<td>
<p> the vector of indices input </p>
</td></tr>
<tr><td><code>P</code></td>
<td>
<p> the vector of bootstrap P-values for each statistic in <code>F</code></p>
</td></tr>
<tr><td><code>P.asy</code></td>
<td>
<p> the vector of asymptotic P-values for each statistic in index </p>
</td></tr>
<tr><td><code>F</code></td>
<td>
<p> the vector of pseudo F-statistics <code>F</code> </p>
</td></tr>
<tr><td><code>F.boot</code></td>
<td>
<p> the matrix of bootstrapped pseudo F-statistics
generated under the null (one column  for each statistic in <code>F</code>) </p>
</td></tr>  
<tr><td><code>df1</code></td>
<td>
<p> the vector of numerator degrees of freedom for each
statistic in <code>F</code> (based on the smoother matrix)</p>
</td></tr>
<tr><td><code>df2</code></td>
<td>
<p> the vector of denominator degrees of freedom for each
statistic in <code>F</code> (based on the smoother matrix) </p>
</td></tr>
<tr><td><code>rss</code></td>
<td>
<p> the vector of restricted sums of squared residuals for
each statistic in <code>F</code> </p>
</td></tr>
<tr><td><code>uss</code></td>
<td>
<p> the vector of unrestricted sums of squared residuals for
each statistic in <code>F</code> </p>
</td></tr>  
<tr><td><code>boot.num</code></td>
<td>
<p> the number of bootstrap replications </p>
</td></tr>
<tr><td><code>boot.type</code></td>
<td>
<p> the <code>boot.type</code> </p>
</td></tr>
<tr><td><code>xnames</code></td>
<td>
<p> the names of the variables in <code>model$xz</code> </p>
</td></tr>
</table>


<h3>Usage Issues</h3>

<p>This function should be considered to be in &lsquo;beta status&rsquo;
until further notice.
</p>
<p>Caution: bootstrap methods are, by their nature, <em>computationally
intensive</em>. This can be frustrating for users possessing large
datasets. For exploratory purposes, you may wish to override the
default number of bootstrap replications, say, setting them to
<code>boot.num=99</code>.
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Racine <a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics:
Theory and Practice,</em> Princeton University Press.
</p>
<p>Ma, S. and J.S. Racine, (2011), &ldquo;Inference for Regression
Splines with Categorical and Continuous Predictors,&rdquo; Working Paper.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
options(crs.messages=FALSE)
set.seed(42)

n &lt;- 1000
z &lt;- rbinom(n,1,.5)
x1 &lt;- rnorm(n)
x2 &lt;- runif(n,-2,2)
z &lt;- factor(z)
## z is irrelevant
y &lt;- x1 + x2 + rnorm(n)

model &lt;- crs(y~x1+x2+z,complexity="degree",segments=c(1,1))
summary(model)

model.sigtest &lt;- crssigtest(model)
summary(model.sigtest)

## End(Not run)
</code></pre>

<hr>
<h2 id='Engel95'> 1995 British Family Expenditure Survey  </h2><span id='topic+Engel95'></span>

<h3>Description</h3>

<p>British cross-section data consisting of a random sample taken from
the British Family Expenditure Survey for 1995. The households consist
of married couples with an employed head-of-household between the ages
of 25 and 55 years. There are 1655 household-level observations in
total.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("Engel95")</code></pre>


<h3>Format</h3>

<p> A data frame with 10 columns, and 1655 rows.
</p>

<dl>
<dt>food</dt><dd><p> expenditure share on food, of type <code>numeric</code></p>
</dd>
<dt>catering</dt><dd><p> expenditure share on catering, of type <code>numeric</code></p>
</dd>
<dt>alcohol</dt><dd><p> expenditure share on alcohol, of type <code>numeric</code></p>
</dd>
<dt>fuel</dt><dd><p> expenditure share on fuel, of type <code>numeric</code></p>
</dd>
<dt>motor</dt><dd><p> expenditure share on motor, of type <code>numeric</code></p>
</dd>
<dt>fares</dt><dd><p> expenditure share on fares, of type <code>numeric</code></p>
</dd>
<dt>leisure</dt><dd><p> expenditure share on leisure, of type <code>numeric</code></p>
</dd>
<dt>logexp</dt><dd><p> logarithm of total expenditure, of type <code>numeric</code></p>
</dd>
<dt>logwages</dt><dd><p> logarithm of total earnings, of type <code>numeric</code></p>
</dd>
<dt>nkids</dt><dd><p> number of children, of type <code>numeric</code></p>
</dd>  
</dl>



<h3>Source</h3>

<p> Richard Blundell and Dennis Kristensen </p>


<h3>References</h3>

<p>Blundell, R. and X. Chen and D. Kristensen (2007),
&ldquo;Semi-Nonparametric IV Estimation of Shape-Invariant Engel
Curves,&rdquo; Econometrica, 75, 1613-1669.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics:
Theory and Practice,</em> Princeton University Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Example - we compute nonparametric instrumental regression of an
## Engel curve for food expenditure shares using Landweber-Fridman
## iteration of Fredholm integral equations of the first kind.

## We consider an equation with an endogenous predictor ('z') and an
## instrument ('w'). Let y = phi(z) + u where phi(z) is the function of
## interest. Here E(u|z) is not zero hence the conditional mean E(y|z)
## does not coincide with the function of interest, but if there exists
## an instrument w such that E(u|w) = 0, then we can recover the
## function of interest by solving an ill-posed inverse problem.

data(Engel95)

## Sort on logexp (the endogenous predictor) for plotting purposes
## (i.e. so we can plot a curve for the fitted values versus logexp)

Engel95 &lt;- Engel95[order(Engel95$logexp),] 

attach(Engel95)

model.iv &lt;- crsiv(y=food,z=logexp,w=logwages,method="Landweber-Fridman")
phihat &lt;- model.iv$phi

## Compute the non-IV regression (i.e. regress y on z)

ghat &lt;- crs(food~logexp)

## For the plots, we restrict focal attention to the bulk of the data
## (i.e. for the plotting area trim out 1/4 of one percent from each
## tail of y and z). This is often helpful as estimates in the tails of
## the support are less reliable (i.e. more variable) so we are
## interested in examining the relationship 'where the action is'.

trim &lt;- 0.0025

plot(logexp,food,
     ylab="Food Budget Share",
     xlab="log(Total Expenditure)",
     xlim=quantile(logexp,c(trim,1-trim)),
     ylim=quantile(food,c(trim,1-trim)),
     main="Nonparametric Instrumental Regression Splines",
     type="p",
     cex=.5,
     col="lightgrey")

lines(logexp,phihat,col="blue",lwd=2,lty=2)

lines(logexp,fitted(ghat),col="red",lwd=2,lty=4)

legend(quantile(logexp,trim),quantile(food,1-trim),
       c(expression(paste("Nonparametric IV: ",hat(varphi)(logexp))),
         "Nonparametric Regression: E(food | logexp)"),
       lty=c(2,4),
       col=c("blue","red"),
       lwd=c(2,2),
       bty="n")

## End(Not run)
</code></pre>

<hr>
<h2 id='frscv'>Categorical Factor Regression Spline Cross-Validation</h2><span id='topic+frscv'></span>

<h3>Description</h3>

<p><code>frscv</code> computes exhaustive cross-validation directed search for
a regression spline estimate of a one (1) dimensional dependent
variable on an <code>r</code>-dimensional vector of continuous predictors
and nominal/ordinal (<code><a href="base.html#topic+factor">factor</a></code>/<code><a href="base.html#topic+ordered">ordered</a></code>)
predictors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>frscv(xz,
      y,
      degree.max = 10, 
      segments.max = 10,
      degree.min = 0,
      segments.min = 1, 
      complexity = c("degree-knots","degree","knots"),
      knots = c("quantiles","uniform","auto"),
      basis = c("additive","tensor","glp","auto"),
      cv.func = c("cv.ls","cv.gcv","cv.aic"),
      degree = degree,
      segments = segments,
      tau = NULL,
      weights = NULL,
      singular.ok = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="frscv_+3A_y">y</code></td>
<td>

<p>continuous univariate vector
</p>
</td></tr>
<tr><td><code id="frscv_+3A_xz">xz</code></td>
<td>
<p> continuous and/or nominal/ordinal
(<code><a href="base.html#topic+factor">factor</a></code>/<code><a href="base.html#topic+ordered">ordered</a></code>) predictors </p>
</td></tr>
<tr><td><code id="frscv_+3A_degree.max">degree.max</code></td>
<td>
<p> the maximum degree of the B-spline basis for
each of the continuous predictors (default <code>degree.max=10</code>)</p>
</td></tr>
<tr><td><code id="frscv_+3A_segments.max">segments.max</code></td>
<td>
<p> the maximum segments of the B-spline basis for
each of the continuous predictors (default <code>segments.max=10</code>)</p>
</td></tr>
<tr><td><code id="frscv_+3A_degree.min">degree.min</code></td>
<td>
<p> the minimum degree of the B-spline basis for
each of the continuous predictors (default <code>degree.min=0</code>)</p>
</td></tr>
<tr><td><code id="frscv_+3A_segments.min">segments.min</code></td>
<td>
<p> the minimum segments of the B-spline basis for
each of the continuous predictors (default <code>segments.min=1</code>)</p>
</td></tr>
<tr><td><code id="frscv_+3A_complexity">complexity</code></td>
<td>
<p>a character string (default
<code>complexity="degree-knots"</code>) indicating whether model
&lsquo;complexity&rsquo; is determined by the degree of the spline or by
the number of segments (&lsquo;knots&rsquo;). This option allows the user
to use cross-validation to select either the spline degree (number
of knots held fixed) or the number of knots (spline degree held
fixed) or both the spline degree and number of knots</p>
</td></tr>
<tr><td><code id="frscv_+3A_knots">knots</code></td>
<td>
<p> a character string (default <code>knots="quantiles"</code>)
specifying where knots are to be placed. &lsquo;quantiles&rsquo; specifies
knots placed at equally spaced quantiles (equal number of observations
lie in each segment) and &lsquo;uniform&rsquo; specifies knots placed at
equally spaced intervals. If <code>knots="auto"</code>, the knot type will
be automatically determined by cross-validation </p>
</td></tr>
<tr><td><code id="frscv_+3A_basis">basis</code></td>
<td>
<p> a character string (default <code>basis="additive"</code>)
indicating whether the additive or tensor product B-spline basis
matrix for a multivariate polynomial spline or generalized B-spline
polynomial basis should be used. Note this can be automatically
determined by cross-validation if <code>cv=TRUE</code> and
<code>basis="auto"</code>, and is an &lsquo;all or none&rsquo; proposition
(i.e. interaction terms for all predictors or for no predictors
given the nature of &lsquo;tensor products&rsquo;). Note also that if
there is only one predictor this defaults to <code>basis="additive"</code>
to avoid unnecessary computation as the spline bases are equivalent
in this case </p>
</td></tr>
<tr><td><code id="frscv_+3A_cv.func">cv.func</code></td>
<td>
<p>a character string (default <code>cv.func="cv.ls"</code>)
indicating which method to use to select smoothing
parameters. <code>cv.gcv</code> specifies generalized cross-validation
(Craven and Wahba (1979)), <code>cv.aic</code> specifies expected
Kullback-Leibler cross-validation (Hurvich, Simonoff, and Tsai
(1998)), and <code>cv.ls</code> specifies least-squares
cross-validation </p>
</td></tr>
<tr><td><code id="frscv_+3A_degree">degree</code></td>
<td>
<p> integer/vector specifying the degree of the B-spline
basis for each dimension of the continuous <code>x</code></p>
</td></tr>
<tr><td><code id="frscv_+3A_segments">segments</code></td>
<td>
<p> integer/vector specifying the number of segments of
the B-spline basis for each dimension of the continuous <code>x</code>
(i.e. number of knots minus one)</p>
</td></tr>
<tr><td><code id="frscv_+3A_tau">tau</code></td>
<td>

<p>if non-null a number in (0,1) denoting the quantile for which a quantile
regression spline is to be estimated rather than estimating the
conditional mean (default <code>tau=NULL</code>)
</p>
</td></tr>
<tr><td><code id="frscv_+3A_weights">weights</code></td>
<td>
<p> an optional vector of weights to be used in the
fitting process.  Should be &lsquo;NULL&rsquo; or a numeric vector.  If
non-NULL, weighted least squares is used with weights
&lsquo;weights&rsquo; (that is, minimizing &lsquo;sum(w*e^2)&rsquo;);
otherwise ordinary least squares is used.  </p>
</td></tr>
<tr><td><code id="frscv_+3A_singular.ok">singular.ok</code></td>
<td>

<p>a logical value (default <code>singular.ok=FALSE</code>) that, when
<code>FALSE</code>, discards singular bases during cross-validation (a check
for ill-conditioned bases is performed).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>frscv</code> computes exhaustive cross-validation for a regression
spline estimate of a one (1) dimensional dependent variable on an
<code>r</code>-dimensional vector of continuous and nominal/ordinal
(<code><a href="base.html#topic+factor">factor</a></code>/<code><a href="base.html#topic+ordered">ordered</a></code>) predictors. The optimal
<code>K</code>/<code>I</code> combination (i.e.\
<code>degree</code>/<code>segments</code>/<code>I</code>) is returned along with other
results (see below for return values).
</p>
<p>For the continuous predictors the regression spline model employs
either the additive or tensor product B-spline basis matrix for a
multivariate polynomial spline via the B-spline routines in the GNU
Scientific Library (<a href="https://www.gnu.org/software/gsl/">https://www.gnu.org/software/gsl/</a>) and the
<code><a href="#topic+tensor.prod.model.matrix">tensor.prod.model.matrix</a></code> function.
</p>
<p>For the nominal/ordinal (<code><a href="base.html#topic+factor">factor</a></code>/<code><a href="base.html#topic+ordered">ordered</a></code>)
predictors the regression spline model uses indicator basis functions.
</p>


<h3>Value</h3>

<p><code>frscv</code> returns a <code>crscv</code> object. Furthermore, the function
<code><a href="base.html#topic+summary">summary</a></code> supports objects of this type. The returned
objects have the following components:
</p>
<table>
<tr><td><code>K</code></td>
<td>
<p> scalar/vector containing optimal degree(s) of spline or
number of segments </p>
</td></tr>
<tr><td><code>I</code></td>
<td>
<p> scalar/vector containing an indicator of whether the
predictor is included or not for each dimension of the
nominal/ordinal (<code><a href="base.html#topic+factor">factor</a></code>/<code><a href="base.html#topic+ordered">ordered</a></code>)
predictors </p>
</td></tr>
<tr><td><code>K.mat</code></td>
<td>
<p> vector/matrix of values of <code>K</code> evaluated during search </p>
</td></tr>  
<tr><td><code>cv.func</code></td>
<td>
<p> objective function value at optimum </p>
</td></tr>
<tr><td><code>cv.func.vec</code></td>
<td>
<p> vector of objective function values at each degree
of spline or number of segments in <code>K.mat</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jeffrey S. Racine <a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Craven, P. and G. Wahba (1979), &ldquo;Smoothing Noisy Data With
Spline Functions,&rdquo; Numerische Mathematik, 13, 377-403.
</p>
<p>Hurvich, C.M. and J.S. Simonoff and C.L. Tsai (1998),
&ldquo;Smoothing Parameter Selection in Nonparametric Regression
Using an Improved Akaike Information Criterion,&rdquo; Journal of the
Royal Statistical Society B, 60, 271-293.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics:
Theory and Practice,</em> Princeton University Press.
</p>
<p>Ma, S. and J.S. Racine and L. Yang (2015), &ldquo;Spline
Regression in the Presence of Categorical Predictors,&rdquo; Journal of
Applied Econometrics, Volume 30, 705-717.
</p>
<p>Ma, S. and J.S. Racine (2013), &ldquo;Additive Regression
Splines with Irrelevant Categorical and Continuous Regressors,&rdquo;
Statistica Sinica, Volume 23, 515-541.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+loess">loess</a></code>, <code><a href="np.html#topic+npregbw">npregbw</a></code>, 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(42)
## Simulated data
n &lt;- 1000

x &lt;- runif(n)
z &lt;- round(runif(n,min=-0.5,max=1.5))
z.unique &lt;- uniquecombs(as.matrix(z))
ind &lt;-  attr(z.unique,"index")
ind.vals &lt;-  sort(unique(ind))
dgp &lt;- numeric(length=n)
for(i in 1:nrow(z.unique)) {
  zz &lt;- ind == ind.vals[i]
  dgp[zz] &lt;- z[zz]+cos(2*pi*x[zz])
}

y &lt;- dgp + rnorm(n,sd=.1)

xdata &lt;- data.frame(x,z=factor(z))

## Compute the optimal K and I, determine optimal number of knots, set
## spline degree for x to 3

cv &lt;- frscv(x=xdata,y=y,complexity="knots",degree=c(3))
summary(cv)
</code></pre>

<hr>
<h2 id='frscvNOMAD'>Categorical Factor Regression Spline Cross-Validation</h2><span id='topic+frscvNOMAD'></span>

<h3>Description</h3>

<p><code>frscvNOMAD</code> computes NOMAD-based (Nonsmooth
Optimization by Mesh Adaptive Direct Search, Abramson, Audet, Couture
and Le Digabel (2011)) cross-validation directed search for a
regression spline estimate of a one (1) dimensional dependent variable
on an <code>r</code>-dimensional vector of continuous predictors and
nominal/ordinal (<code><a href="base.html#topic+factor">factor</a></code>/<code><a href="base.html#topic+ordered">ordered</a></code>)
predictors.  </p>


<h3>Usage</h3>

<pre><code class='language-R'>frscvNOMAD(xz,
           y,
           degree.max = 10, 
           segments.max = 10, 
           degree.min = 0, 
           segments.min = 1,
           cv.df.min = 1,
           complexity = c("degree-knots","degree","knots"),
           knots = c("quantiles","uniform","auto"),
           basis = c("additive","tensor","glp","auto"),
           cv.func = c("cv.ls","cv.gcv","cv.aic"),
           degree = degree,
           segments = segments, 
           include = include, 
           random.seed = 42,
           max.bb.eval = 10000,
           initial.mesh.size.integer = "1",
           min.mesh.size.integer = "1", 
           min.poll.size.integer = "1", 
           opts=list(),
           nmulti = 0,
           tau = NULL,
           weights = NULL,
           singular.ok = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="frscvNOMAD_+3A_y">y</code></td>
<td>

<p>continuous univariate vector
</p>
</td></tr>
<tr><td><code id="frscvNOMAD_+3A_xz">xz</code></td>
<td>
<p> continuous and/or nominal/ordinal
(<code><a href="base.html#topic+factor">factor</a></code>/<code><a href="base.html#topic+ordered">ordered</a></code>) predictors </p>
</td></tr>
<tr><td><code id="frscvNOMAD_+3A_degree.max">degree.max</code></td>
<td>
<p> the maximum degree of the B-spline basis for
each of the continuous predictors (default <code>degree.max=10</code>)</p>
</td></tr>
<tr><td><code id="frscvNOMAD_+3A_segments.max">segments.max</code></td>
<td>
<p> the maximum segments of the B-spline basis for
each of the continuous predictors (default <code>segments.max=10</code>)</p>
</td></tr>
<tr><td><code id="frscvNOMAD_+3A_degree.min">degree.min</code></td>
<td>
<p> the minimum degree of the B-spline basis for
each of the continuous predictors (default <code>degree.min=0</code>)</p>
</td></tr>
<tr><td><code id="frscvNOMAD_+3A_segments.min">segments.min</code></td>
<td>
<p> the minimum segments of the B-spline basis for
each of the continuous predictors (default <code>segments.min=1</code>)</p>
</td></tr>
<tr><td><code id="frscvNOMAD_+3A_cv.df.min">cv.df.min</code></td>
<td>
<p> the minimum degrees of freedom to allow when
conducting cross-validation (default <code>cv.df.min=1</code>)</p>
</td></tr>
<tr><td><code id="frscvNOMAD_+3A_complexity">complexity</code></td>
<td>
<p>a character string (default
<code>complexity="degree-knots"</code>) indicating whether model
&lsquo;complexity&rsquo; is determined by the degree of the spline or by
the number of segments (&lsquo;knots&rsquo;). This option allows the user
to use cross-validation to select either the spline degree (number
of knots held fixed) or the number of knots (spline degree held
fixed) or both the spline degree and number of knots</p>
</td></tr>
<tr><td><code id="frscvNOMAD_+3A_knots">knots</code></td>
<td>
<p> a character string (default <code>knots="quantiles"</code>)
specifying where knots are to be placed. &lsquo;quantiles&rsquo; specifies
knots placed at equally spaced quantiles (equal number of observations
lie in each segment) and &lsquo;uniform&rsquo; specifies knots placed at
equally spaced intervals. If <code>knots="auto"</code>, the knot type will
be automatically determined by cross-validation </p>
</td></tr>
<tr><td><code id="frscvNOMAD_+3A_basis">basis</code></td>
<td>
<p> a character string (default <code>basis="additive"</code>)
indicating whether the additive or tensor product B-spline basis
matrix for a multivariate polynomial spline or generalized B-spline
polynomial basis should be used. Note this can be automatically
determined by cross-validation if <code>cv=TRUE</code> and
<code>basis="auto"</code>, and is an &lsquo;all or none&rsquo; proposition
(i.e. interaction terms for all predictors or for no predictors
given the nature of &lsquo;tensor products&rsquo;). Note also that if
there is only one predictor this defaults to <code>basis="additive"</code>
to avoid unnecessary computation as the spline bases are equivalent
in this case </p>
</td></tr>
<tr><td><code id="frscvNOMAD_+3A_cv.func">cv.func</code></td>
<td>
<p>a character string (default <code>cv.func="cv.ls"</code>)
indicating which method to use to select smoothing
parameters. <code>cv.gcv</code> specifies generalized cross-validation
(Craven and Wahba (1979)), <code>cv.aic</code> specifies expected
Kullback-Leibler cross-validation (Hurvich, Simonoff, and Tsai
(1998)), and <code>cv.ls</code> specifies least-squares
cross-validation  </p>
</td></tr>
<tr><td><code id="frscvNOMAD_+3A_degree">degree</code></td>
<td>
<p> integer/vector specifying the degree of the B-spline
basis for each dimension of the continuous <code>x</code></p>
</td></tr>
<tr><td><code id="frscvNOMAD_+3A_segments">segments</code></td>
<td>
<p> integer/vector specifying the number of segments of
the B-spline basis for each dimension of the continuous <code>x</code>
(i.e. number of knots minus one)</p>
</td></tr>
<tr><td><code id="frscvNOMAD_+3A_include">include</code></td>
<td>
<p> integer/vector for the categorical predictors. If it 
is not NULL,   it will be the initial value for the fitting</p>
</td></tr>
<tr><td><code id="frscvNOMAD_+3A_random.seed">random.seed</code></td>
<td>

<p>when it is not missing and not equal to 0, the initial points  will 
be generated using this seed when <code>nmulti &gt; 0</code>
</p>
</td></tr>
<tr><td><code id="frscvNOMAD_+3A_max.bb.eval">max.bb.eval</code></td>
<td>

<p>argument passed to the NOMAD solver (see <code><a href="#topic+snomadr">snomadr</a></code> for
further details)
</p>
</td></tr>
<tr><td><code id="frscvNOMAD_+3A_initial.mesh.size.integer">initial.mesh.size.integer</code></td>
<td>

<p>argument passed to the NOMAD solver (see <code><a href="#topic+snomadr">snomadr</a></code> for
further details)
</p>
</td></tr>
<tr><td><code id="frscvNOMAD_+3A_min.mesh.size.integer">min.mesh.size.integer</code></td>
<td>

<p>arguments passed to the NOMAD solver (see <code><a href="#topic+snomadr">snomadr</a></code> for
further details)
</p>
</td></tr>
<tr><td><code id="frscvNOMAD_+3A_min.poll.size.integer">min.poll.size.integer</code></td>
<td>

<p>arguments passed to the NOMAD solver (see <code><a href="#topic+snomadr">snomadr</a></code> for
further details)
</p>
</td></tr>
<tr><td><code id="frscvNOMAD_+3A_opts">opts</code></td>
<td>
<p> list of optional arguments to be passed to
<code><a href="#topic+snomadr">snomadr</a></code> </p>
</td></tr>
<tr><td><code id="frscvNOMAD_+3A_nmulti">nmulti</code></td>
<td>

<p>integer number of times to restart the process of finding extrema of
the cross-validation function from different (random) initial
points (default <code>nmulti=0</code>)
</p>
</td></tr>
<tr><td><code id="frscvNOMAD_+3A_tau">tau</code></td>
<td>

<p>if non-null a number in (0,1) denoting the quantile for which a quantile
regression spline is to be estimated rather than estimating the
conditional mean (default <code>tau=NULL</code>)
</p>
</td></tr>
<tr><td><code id="frscvNOMAD_+3A_weights">weights</code></td>
<td>
<p> an optional vector of weights to be used in the
fitting process.  Should be &lsquo;NULL&rsquo; or a numeric vector.  If
non-NULL, weighted least squares is used with weights
&lsquo;weights&rsquo; (that is, minimizing &lsquo;sum(w*e^2)&rsquo;);
otherwise ordinary least squares is used.  </p>
</td></tr>
<tr><td><code id="frscvNOMAD_+3A_singular.ok">singular.ok</code></td>
<td>

<p>a logical value (default <code>singular.ok=FALSE</code>) that, when
<code>FALSE</code>, discards singular bases during cross-validation (a check
for ill-conditioned bases is performed).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>frscvNOMAD</code> computes NOMAD-based cross-validation for a
regression spline estimate of a one (1) dimensional dependent variable
on an <code>r</code>-dimensional vector of continuous and nominal/ordinal
(<code><a href="base.html#topic+factor">factor</a></code>/<code><a href="base.html#topic+ordered">ordered</a></code>) predictors.  Numerical
search for the optimal <code>degree</code>/<code>segments</code>/<code>I</code> is
undertaken using <code><a href="#topic+snomadr">snomadr</a></code>.
</p>
<p>The optimal <code>K</code>/<code>I</code> combination is returned along with other
results (see below for return values).
</p>
<p>For the continuous predictors the regression spline model employs
either the additive or tensor product B-spline basis matrix for a
multivariate polynomial spline via the B-spline routines in the GNU
Scientific Library (<a href="https://www.gnu.org/software/gsl/">https://www.gnu.org/software/gsl/</a>) and the
<code><a href="#topic+tensor.prod.model.matrix">tensor.prod.model.matrix</a></code> function.
</p>
<p>For the nominal/ordinal (<code><a href="base.html#topic+factor">factor</a></code>/<code><a href="base.html#topic+ordered">ordered</a></code>)
predictors the regression spline model uses indicator basis functions.
</p>


<h3>Value</h3>

<p><code>frscvNOMAD</code> returns a <code>crscv</code> object. Furthermore, the function
<code><a href="base.html#topic+summary">summary</a></code> supports objects of this type. The returned
objects have the following components:
</p>
<table>
<tr><td><code>K</code></td>
<td>
<p> scalar/vector containing optimal degree(s) of spline or
number of segments </p>
</td></tr>
<tr><td><code>I</code></td>
<td>
<p> scalar/vector containing an indicator of whether the
predictor is included or not for each dimension of the
nominal/ordinal
(<code><a href="base.html#topic+factor">factor</a></code>/<code><a href="base.html#topic+ordered">ordered</a></code>) predictors </p>
</td></tr>
<tr><td><code>K.mat</code></td>
<td>
<p> vector/matrix of values of <code>K</code> evaluated during
search </p>
</td></tr>  
<tr><td><code>degree.max</code></td>
<td>
<p> the maximum degree of the B-spline basis for
each of the continuous predictors (default <code>degree.max=10</code>)</p>
</td></tr>
<tr><td><code>segments.max</code></td>
<td>
<p> the maximum segments of the B-spline basis for
each of the continuous predictors (default <code>segments.max=10</code>)</p>
</td></tr>
<tr><td><code>degree.min</code></td>
<td>
<p> the minimum degree of the B-spline basis for
each of the continuous predictors (default <code>degree.min=0</code>)</p>
</td></tr>
<tr><td><code>segments.min</code></td>
<td>
<p> the minimum segments of the B-spline basis for
each of the continuous predictors (default <code>segments.min=1</code>)</p>
</td></tr>
<tr><td><code>cv.func</code></td>
<td>
<p> objective function value at optimum </p>
</td></tr>
<tr><td><code>cv.func.vec</code></td>
<td>
<p> vector of objective function values at each degree
of spline or number of segments in <code>K.mat</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jeffrey S. Racine <a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a> and Zhenghua Nie <a href="mailto:niez@mcmaster.ca">niez@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Abramson, M.A. and C. Audet and G. Couture and J.E. Dennis Jr. and
S. Le Digabel (2011), &ldquo;The NOMAD project&rdquo;. Software available
at https://www.gerad.ca/nomad.
</p>
<p>Craven, P. and G. Wahba (1979), &ldquo;Smoothing Noisy Data With
Spline Functions,&rdquo; Numerische Mathematik, 13, 377-403.
</p>
<p>Hurvich, C.M. and J.S. Simonoff and C.L. Tsai (1998),
&ldquo;Smoothing Parameter Selection in Nonparametric Regression
Using an Improved Akaike Information Criterion,&rdquo; Journal of the Royal
Statistical Society B, 60, 271-293.
</p>
<p>Le Digabel, S. (2011), &ldquo;Algorithm 909: NOMAD: Nonlinear
Optimization With the MADS Algorithm&rdquo;. ACM Transactions on
Mathematical Software, 37(4):44:1-44:15.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics:
Theory and Practice,</em> Princeton University Press.
</p>
<p>Ma, S. and J.S. Racine and L. Yang (2015), &ldquo;Spline
Regression in the Presence of Categorical Predictors,&rdquo; Journal of
Applied Econometrics, Volume 30, 705-717.
</p>
<p>Ma, S. and J.S. Racine (2013), &ldquo;Additive Regression
Splines with Irrelevant Categorical and Continuous Regressors,&rdquo;
Statistica Sinica, Volume 23, 515-541.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+loess">loess</a></code>, <code><a href="np.html#topic+npregbw">npregbw</a></code>, 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(42)
## Simulated data
n &lt;- 1000

x &lt;- runif(n)
z &lt;- round(runif(n,min=-0.5,max=1.5))
z.unique &lt;- uniquecombs(as.matrix(z))
ind &lt;-  attr(z.unique,"index")
ind.vals &lt;-  sort(unique(ind))
dgp &lt;- numeric(length=n)
for(i in 1:nrow(z.unique)) {
  zz &lt;- ind == ind.vals[i]
  dgp[zz] &lt;- z[zz]+cos(2*pi*x[zz])
}

y &lt;- dgp + rnorm(n,sd=.1)

xdata &lt;- data.frame(x,z=factor(z))

## Compute the optimal K and I, determine optimal number of knots, set
## spline degree for x to 3

cv &lt;- frscvNOMAD(x=xdata,y=y,complexity="knots",degree=c(3),segments=c(5))
summary(cv)
</code></pre>

<hr>
<h2 id='glp.model.matrix'>Utility function for constructing generalized polynomial smooths</h2><span id='topic+glp.model.matrix'></span>

<h3>Description</h3>

<p>Produce model matrices for a generalized polynomial smooth from the
model matrices for the marginal bases of the smooth.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>glp.model.matrix(X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="glp.model.matrix_+3A_x">X</code></td>
<td>
<p>a list of model matrices for the marginal bases of a smooth</p>
</td></tr> 
</table>


<h3>Details</h3>

<p>This function computes a generalized polynomial where the
orders of each term entering the polynomial may vary.</p>


<h3>Value</h3>

<p>A model matrix for a generalized polynomial smooth.</p>


<h3>Author(s)</h3>

<p> Jeffrey S. Racine <a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a></p>


<h3>References</h3>

<p>Hall, P. and J.S. Racine (forthcoming), &ldquo;Cross-Validated
Generalized Local Polynomial Regression,&rdquo; Journal of Econometrics.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>X &lt;- list(matrix(1:4,2,2),matrix(5:10,2,3))
glp.model.matrix(X)
</code></pre>

<hr>
<h2 id='gsl.bs'>GSL (GNU Scientific Library) B-spline/B-spline Derivatives</h2><span id='topic+gsl.bs'></span><span id='topic+gsl.bs.default'></span><span id='topic+gsl.bs.predict'></span>

<h3>Description</h3>

 <p><code>gsl.bs</code> generates the B-spline basis matrix for a
polynomial spline and (optionally) the B-spline basis matrix
derivative of a specified order with respect to each predictor </p>


<h3>Usage</h3>

<pre><code class='language-R'>gsl.bs(...)
## Default S3 method:
gsl.bs(x,
       degree = 3,
       nbreak = 2,
       deriv = 0,
       x.min = NULL,
       x.max = NULL,
       intercept = FALSE,
       knots = NULL,
       ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gsl.bs_+3A_x">x</code></td>
<td>
<p> the predictor variable.  Missing values are not allowed </p>
</td></tr>
<tr><td><code id="gsl.bs_+3A_degree">degree</code></td>
<td>
<p> degree of the piecewise polynomial - default is
&lsquo;3&rsquo; (cubic spline) </p>
</td></tr>
<tr><td><code id="gsl.bs_+3A_nbreak">nbreak</code></td>
<td>
<p> number of breaks in each interval - default is &lsquo;2&rsquo;</p>
</td></tr>
<tr><td><code id="gsl.bs_+3A_deriv">deriv</code></td>
<td>
<p> the order of the derivative to be computed-default if
<code>0</code></p>
</td></tr>
<tr><td><code id="gsl.bs_+3A_x.min">x.min</code></td>
<td>
<p> the lower bound on which to construct the spline -
defaults to <code>min(x)</code></p>
</td></tr>
<tr><td><code id="gsl.bs_+3A_x.max">x.max</code></td>
<td>
<p> the upper bound on which to construct the spline -
defaults to <code>max(x)</code></p>
</td></tr>
<tr><td><code id="gsl.bs_+3A_intercept">intercept</code></td>
<td>
<p> if &lsquo;TRUE&rsquo;, an intercept is included in the
basis; default is &lsquo;FALSE&rsquo; </p>
</td></tr>
<tr><td><code id="gsl.bs_+3A_knots">knots</code></td>
<td>
<p> a vector (default <code>knots="NULL"</code>) specifying knots
for the spline basis (default enables uniform knots, otherwise those
provided are used)</p>
</td></tr>
<tr><td><code id="gsl.bs_+3A_...">...</code></td>
<td>
<p> optional arguments </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Typical usages are (see below for a  list of options and also
the examples at the end of this help file)
</p>
<pre>
    B &lt;- gsl.bs(x,degree=10)
    B.predict &lt;- predict(gsl.bs(x,degree=10),newx=xeval)
  </pre>


<h3>Value</h3>

<p><code>gsl.bs</code> returns a <code>gsl.bs</code> object.  A matrix of dimension
&lsquo;c(length(x), degree+nbreak-1)&rsquo;.  The generic function
<code><a href="stats.html#topic+predict">predict</a></code> extracts (or generates) predictions from the
returned object.
</p>
<p>A primary use is in modelling formulas to directly specify a piecewise
polynomial term in a model. See <a href="https://www.gnu.org/software/gsl/">https://www.gnu.org/software/gsl/</a>
for further details.
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Racine <a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics:
Theory and Practice,</em> Princeton University Press.
</p>
<p>Ma, S. and J.S. Racine and L. Yang (2015), &ldquo;Spline
Regression in the Presence of Categorical Predictors,&rdquo; Journal of
Applied Econometrics, Volume 30, 705-717.
</p>
<p>Ma, S. and J.S. Racine (2013), &ldquo;Additive Regression
Splines with Irrelevant Categorical and Continuous Regressors,&rdquo;
Statistica Sinica, Volume 23, 515-541.
</p>


<h3>See Also</h3>

<p><code><a href="splines.html#topic+bs">bs</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Plot the spline bases and their first order derivatives
x &lt;- seq(0,1,length=100)
matplot(x,gsl.bs(x,degree=5),type="l")
matplot(x,gsl.bs(x,degree=5,deriv=1),type="l")

## Regression example
n &lt;- 1000
x &lt;- sort(runif(n))
y &lt;- cos(2*pi*x) + rnorm(n,sd=.25)
B &lt;- gsl.bs(x,degree=5,intercept=FALSE)
plot(x,y,cex=.5,col="grey")
lines(x,fitted(lm(y~B)))
</code></pre>

<hr>
<h2 id='krscv'>Categorical Kernel Regression Spline Cross-Validation</h2><span id='topic+krscv'></span>

<h3>Description</h3>

<p><code>krscv</code> computes exhaustive cross-validation directed search for
a regression spline estimate of a one (1) dimensional dependent
variable on an <code>r</code>-dimensional vector of continuous and
nominal/ordinal (<code><a href="base.html#topic+factor">factor</a></code>/<code><a href="base.html#topic+ordered">ordered</a></code>)
predictors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>krscv(xz,
      y,
      degree.max = 10, 
      segments.max = 10,
      degree.min = 0,
      segments.min = 1, 
      restarts = 0,
      complexity = c("degree-knots","degree","knots"),
      knots = c("quantiles","uniform","auto"),
      basis = c("additive","tensor","glp","auto"),
      cv.func = c("cv.ls","cv.gcv","cv.aic"),
      degree = degree,
      segments = segments,
      tau = NULL,
      weights = NULL,
      singular.ok = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="krscv_+3A_y">y</code></td>
<td>

<p>continuous univariate vector
</p>
</td></tr>
<tr><td><code id="krscv_+3A_xz">xz</code></td>
<td>
<p> continuous and/or nominal/ordinal
(<code><a href="base.html#topic+factor">factor</a></code>/<code><a href="base.html#topic+ordered">ordered</a></code>) predictors </p>
</td></tr>
<tr><td><code id="krscv_+3A_degree.max">degree.max</code></td>
<td>
<p> the maximum degree of the B-spline basis for
each of the continuous predictors (default <code>degree.max=10</code>)</p>
</td></tr>
<tr><td><code id="krscv_+3A_segments.max">segments.max</code></td>
<td>
<p> the maximum segments of the B-spline basis for
each of the continuous predictors (default <code>segments.max=10</code>)</p>
</td></tr>
<tr><td><code id="krscv_+3A_degree.min">degree.min</code></td>
<td>
<p> the minimum degree of the B-spline basis for
each of the continuous predictors (default <code>degree.min=0</code>)</p>
</td></tr>
<tr><td><code id="krscv_+3A_segments.min">segments.min</code></td>
<td>
<p> the minimum segments of the B-spline basis for
each of the continuous predictors (default <code>segments.min=1</code>)</p>
</td></tr>
<tr><td><code id="krscv_+3A_restarts">restarts</code></td>
<td>

<p>number of times to restart <code><a href="stats.html#topic+optim">optim</a></code> from different initial
random values (default <code>restarts=0</code>) when searching for optimal
bandwidths for the categorical predictors for each unique <code>K</code>
combination (i.e.\ <code>degree</code>/<code>segments</code>)
</p>
</td></tr>
<tr><td><code id="krscv_+3A_complexity">complexity</code></td>
<td>
<p>a character string (default
<code>complexity="degree-knots"</code>) indicating whether model
&lsquo;complexity&rsquo; is determined by the degree of the spline or by
the number of segments (&lsquo;knots&rsquo;). This option allows the user
to use cross-validation to select either the spline degree (number
of knots held fixed) or the number of knots (spline degree held
fixed) or both the spline degree and number of knots</p>
</td></tr>
<tr><td><code id="krscv_+3A_knots">knots</code></td>
<td>
<p> a character string (default <code>knots="quantiles"</code>)
specifying where knots are to be placed. &lsquo;quantiles&rsquo; specifies
knots placed at equally spaced quantiles (equal number of observations
lie in each segment) and &lsquo;uniform&rsquo; specifies knots placed at
equally spaced intervals. If <code>knots="auto"</code>, the knot type will
be automatically determined by cross-validation </p>
</td></tr>
<tr><td><code id="krscv_+3A_basis">basis</code></td>
<td>
<p> a character string (default <code>basis="additive"</code>)
indicating whether the additive or tensor product B-spline basis
matrix for a multivariate polynomial spline or generalized B-spline
polynomial basis should be used. Note this can be automatically
determined by cross-validation if <code>cv=TRUE</code> and
<code>basis="auto"</code>, and is an &lsquo;all or none&rsquo; proposition
(i.e. interaction terms for all predictors or for no predictors
given the nature of &lsquo;tensor products&rsquo;). Note also that if
there is only one predictor this defaults to <code>basis="additive"</code>
to avoid unnecessary computation as the spline bases are equivalent
in this case </p>
</td></tr>
<tr><td><code id="krscv_+3A_cv.func">cv.func</code></td>
<td>
<p>a character string (default <code>cv.func="cv.ls"</code>)
indicating which method to use to select smoothing
parameters. <code>cv.gcv</code> specifies generalized cross-validation
(Craven and Wahba (1979)), <code>cv.aic</code> specifies expected
Kullback-Leibler cross-validation (Hurvich, Simonoff, and Tsai
(1998)), and <code>cv.ls</code> specifies least-squares
cross-validation  </p>
</td></tr>
<tr><td><code id="krscv_+3A_degree">degree</code></td>
<td>
<p> integer/vector specifying the degree of the B-spline
basis for each dimension of the continuous <code>x</code></p>
</td></tr>
<tr><td><code id="krscv_+3A_segments">segments</code></td>
<td>
<p> integer/vector specifying the number of segments of
the B-spline basis for each dimension of the continuous <code>x</code>
(i.e. number of knots minus one)</p>
</td></tr>
<tr><td><code id="krscv_+3A_tau">tau</code></td>
<td>

<p>if non-null a number in (0,1) denoting the quantile for which a quantile
regression spline is to be estimated rather than estimating the
conditional mean (default <code>tau=NULL</code>)
</p>
</td></tr>
<tr><td><code id="krscv_+3A_weights">weights</code></td>
<td>
<p> an optional vector of weights to be used in the
fitting process.  Should be &lsquo;NULL&rsquo; or a numeric vector.  If
non-NULL, weighted least squares is used with weights
&lsquo;weights&rsquo; (that is, minimizing &lsquo;sum(w*e^2)&rsquo;);
otherwise ordinary least squares is used.</p>
</td></tr>
<tr><td><code id="krscv_+3A_singular.ok">singular.ok</code></td>
<td>

<p>a logical value (default <code>singular.ok=FALSE</code>) that, when
<code>FALSE</code>, discards singular bases during cross-validation (a check
for ill-conditioned bases is performed).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>krscv</code> computes exhaustive cross-validation for a regression
spline estimate of a one (1) dimensional dependent variable on an
<code>r</code>-dimensional vector of continuous and nominal/ordinal
(<code><a href="base.html#topic+factor">factor</a></code>/<code><a href="base.html#topic+ordered">ordered</a></code>) predictors. The optimal
<code>K</code>/<code>lambda</code> combination is returned along with other
results (see below for return values). The method uses kernel
functions appropriate for categorical (ordinal/nominal) predictors
which avoids the loss in efficiency associated with sample-splitting
procedures that are typically used when faced with a mix of continuous
and nominal/ordinal (<code><a href="base.html#topic+factor">factor</a></code>/<code><a href="base.html#topic+ordered">ordered</a></code>)
predictors.
</p>
<p>For the continuous predictors the regression spline model employs
either the additive or tensor product B-spline basis matrix for a
multivariate polynomial spline via the B-spline routines in the GNU
Scientific Library (<a href="https://www.gnu.org/software/gsl/">https://www.gnu.org/software/gsl/</a>) and the
<code><a href="#topic+tensor.prod.model.matrix">tensor.prod.model.matrix</a></code> function.
</p>
<p>For the discrete predictors the product kernel function is of the
&lsquo;Li-Racine&rsquo; type (see Li and Racine (2007) for details).
</p>
<p>For each unique combination of <code>degree</code> and <code>segment</code>,
numerical search for the bandwidth vector <code>lambda</code> is undertaken
using <code><a href="stats.html#topic+optim">optim</a></code> and the box-constrained <code>L-BFGS-B</code>
method (see <code><a href="stats.html#topic+optim">optim</a></code> for details). The user may restart the
<code><a href="stats.html#topic+optim">optim</a></code> algorithm as many times as desired via the
<code>restarts</code> argument. The approach ascends from <code>K=0</code> through
<code>degree.max</code>/<code>segments.max</code> and for each value of <code>K</code>
searches for the optimal bandwidths for this value of <code>K</code>. After
the most complex model has been searched then the optimal
<code>K</code>/<code>lambda</code> combination is selected. If any element of the
optimal <code>K</code> vector coincides with
<code>degree.max</code>/<code>segments.max</code> a warning is produced and the
user ought to restart their search with a larger value of
<code>degree.max</code>/<code>segments.max</code>.
</p>


<h3>Value</h3>

<p><code>krscv</code> returns a <code>crscv</code> object. Furthermore, the
function <code><a href="base.html#topic+summary">summary</a></code> supports objects of this type. The
returned objects have the following components:
</p>
<table>
<tr><td><code>K</code></td>
<td>
<p> scalar/vector containing optimal degree(s) of spline or
number of segments </p>
</td></tr>
<tr><td><code>K.mat</code></td>
<td>
<p> vector/matrix of values of <code>K</code> evaluated during search </p>
</td></tr>  
<tr><td><code>restarts</code></td>
<td>
<p> number of restarts during search, if any </p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p> optimal bandwidths for categorical predictors </p>
</td></tr>
<tr><td><code>lambda.mat</code></td>
<td>
<p> vector/matrix of optimal bandwidths for each degree of spline </p>
</td></tr>
<tr><td><code>cv.func</code></td>
<td>
<p> objective function value at optimum </p>
</td></tr>
<tr><td><code>cv.func.vec</code></td>
<td>
<p> vector of objective function values at each degree
of spline or number of segments in <code>K.mat</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jeffrey S. Racine <a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Craven, P. and G. Wahba (1979), &ldquo;Smoothing Noisy Data With
Spline Functions,&rdquo; Numerische Mathematik, 13, 377-403.
</p>
<p>Hurvich, C.M. and J.S. Simonoff and C.L. Tsai (1998),
&ldquo;Smoothing Parameter Selection in Nonparametric Regression
Using an Improved Akaike Information Criterion,&rdquo; Journal of the
Royal Statistical Society B, 60, 271-293.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics:
Theory and Practice,</em> Princeton University Press.
</p>
<p>Ma, S. and J.S. Racine and L. Yang (2015), &ldquo;Spline
Regression in the Presence of Categorical Predictors,&rdquo; Journal of
Applied Econometrics, Volume 30, 705-717.
</p>
<p>Ma, S. and J.S. Racine (2013), &ldquo;Additive Regression
Splines with Irrelevant Categorical and Continuous Regressors,&rdquo;
Statistica Sinica, Volume 23, 515-541.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+loess">loess</a></code>, <code><a href="np.html#topic+npregbw">npregbw</a></code>, 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(42)
## Simulated data
n &lt;- 1000

x &lt;- runif(n)
z &lt;- round(runif(n,min=-0.5,max=1.5))
z.unique &lt;- uniquecombs(as.matrix(z))
ind &lt;-  attr(z.unique,"index")
ind.vals &lt;-  sort(unique(ind))
dgp &lt;- numeric(length=n)
for(i in 1:nrow(z.unique)) {
  zz &lt;- ind == ind.vals[i]
  dgp[zz] &lt;- z[zz]+cos(2*pi*x[zz])
}
y &lt;- dgp + rnorm(n,sd=.1)

xdata &lt;- data.frame(x,z=factor(z))

## Compute the optimal K and lambda, determine optimal number of knots, set
## spline degree for x to 3

cv &lt;- krscv(x=xdata,y=y,complexity="knots",degree=c(3))
summary(cv)
</code></pre>

<hr>
<h2 id='krscvNOMAD'>Categorical Kernel Regression Spline Cross-Validation</h2><span id='topic+krscvNOMAD'></span>

<h3>Description</h3>

<p><code>krscvNOMAD</code> computes NOMAD-based (Nonsmooth Optimization by Mesh
Adaptive Direct Search, Abramson, Audet, Couture and Le Digabel
(2011)) cross-validation directed search for a regression spline
estimate of a one (1) dimensional dependent variable on an
<code>r</code>-dimensional vector of continuous and nominal/ordinal
(<code><a href="base.html#topic+factor">factor</a></code>/<code><a href="base.html#topic+ordered">ordered</a></code>) predictors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>krscvNOMAD(xz,
           y,
           degree.max = 10, 
           segments.max = 10, 
           degree.min = 0, 
           segments.min = 1,
           cv.df.min = 1,
           complexity = c("degree-knots","degree","knots"),
           knots = c("quantiles","uniform","auto"),
           basis = c("additive","tensor","glp","auto"),
           cv.func = c("cv.ls","cv.gcv","cv.aic"),
           degree = degree,
           segments = segments,
           lambda = lambda, 
           lambda.discrete = FALSE, 
           lambda.discrete.num = 100, 
           random.seed = 42,
           max.bb.eval = 10000,
           initial.mesh.size.real = "r0.1",
           initial.mesh.size.integer = "1",
           min.mesh.size.real = paste("r",sqrt(.Machine$double.eps),sep=""),
           min.mesh.size.integer = "1", 
           min.poll.size.real = "1",
           min.poll.size.integer = "1", 
           opts=list(),
           nmulti = 0,
           tau = NULL,
           weights = NULL,
           singular.ok = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="krscvNOMAD_+3A_y">y</code></td>
<td>

<p>continuous univariate vector
</p>
</td></tr>
<tr><td><code id="krscvNOMAD_+3A_xz">xz</code></td>
<td>
<p> continuous and/or nominal/ordinal
(<code><a href="base.html#topic+factor">factor</a></code>/<code><a href="base.html#topic+ordered">ordered</a></code>) predictors </p>
</td></tr>
<tr><td><code id="krscvNOMAD_+3A_degree.max">degree.max</code></td>
<td>
<p> the maximum degree of the B-spline basis for
each of the continuous predictors (default <code>degree.max=10</code>)</p>
</td></tr>
<tr><td><code id="krscvNOMAD_+3A_segments.max">segments.max</code></td>
<td>
<p> the maximum segments of the B-spline basis for
each of the continuous predictors (default <code>segments.max=10</code>)</p>
</td></tr>
<tr><td><code id="krscvNOMAD_+3A_degree.min">degree.min</code></td>
<td>
<p> the minimum degree of the B-spline basis for
each of the continuous predictors (default <code>degree.min=0</code>)</p>
</td></tr>
<tr><td><code id="krscvNOMAD_+3A_segments.min">segments.min</code></td>
<td>
<p> the minimum segments of the B-spline basis for
each of the continuous predictors (default <code>segments.min=1</code>)</p>
</td></tr>
<tr><td><code id="krscvNOMAD_+3A_cv.df.min">cv.df.min</code></td>
<td>
<p> the minimum degrees of freedom to allow when
conducting cross-validation (default <code>cv.df.min=1</code>)</p>
</td></tr>
<tr><td><code id="krscvNOMAD_+3A_complexity">complexity</code></td>
<td>
<p>a character string (default
<code>complexity="degree-knots"</code>) indicating whether model
&lsquo;complexity&rsquo; is determined by the degree of the spline or by
the number of segments (&lsquo;knots&rsquo;). This option allows the user
to use cross-validation to select either the spline degree (number
of knots held fixed) or the number of knots (spline degree held
fixed) or both the spline degree and number of knots</p>
</td></tr>
<tr><td><code id="krscvNOMAD_+3A_knots">knots</code></td>
<td>
<p> a character string (default <code>knots="quantiles"</code>)
specifying where knots are to be placed. &lsquo;quantiles&rsquo; specifies
knots placed at equally spaced quantiles (equal number of observations
lie in each segment) and &lsquo;uniform&rsquo; specifies knots placed at
equally spaced intervals. If <code>knots="auto"</code>, the knot type will
be automatically determined by cross-validation </p>
</td></tr>
<tr><td><code id="krscvNOMAD_+3A_basis">basis</code></td>
<td>
<p> a character string (default <code>basis="additive"</code>)
indicating whether the additive or tensor product B-spline basis
matrix for a multivariate polynomial spline or generalized B-spline
polynomial basis should be used. Note this can be automatically
determined by cross-validation if <code>cv=TRUE</code> and
<code>basis="auto"</code>, and is an &lsquo;all or none&rsquo; proposition
(i.e. interaction terms for all predictors or for no predictors
given the nature of &lsquo;tensor products&rsquo;). Note also that if
there is only one predictor this defaults to <code>basis="additive"</code>
to avoid unnecessary computation as the spline bases are equivalent
in this case </p>
</td></tr>
<tr><td><code id="krscvNOMAD_+3A_cv.func">cv.func</code></td>
<td>
<p>a character string (default <code>cv.func="cv.ls"</code>)
indicating which method to use to select smoothing
parameters. <code>cv.gcv</code> specifies generalized cross-validation
(Craven and Wahba (1979)), <code>cv.aic</code> specifies expected
Kullback-Leibler cross-validation (Hurvich, Simonoff, and Tsai
(1998)), and <code>cv.ls</code> specifies least-squares
cross-validation  </p>
</td></tr>
<tr><td><code id="krscvNOMAD_+3A_degree">degree</code></td>
<td>
<p> integer/vector specifying the degree of the B-spline
basis for each dimension of the continuous <code>x</code></p>
</td></tr>
<tr><td><code id="krscvNOMAD_+3A_segments">segments</code></td>
<td>
<p> integer/vector specifying the number of segments of
the B-spline basis for each dimension of the continuous <code>x</code>
(i.e. number of knots minus one)</p>
</td></tr>
<tr><td><code id="krscvNOMAD_+3A_lambda">lambda</code></td>
<td>
<p> real/vector for the categorical predictors. If it is
not NULL, it will be the starting value(s) for lambda</p>
</td></tr>
<tr><td><code id="krscvNOMAD_+3A_lambda.discrete">lambda.discrete</code></td>
<td>
<p> if <code>lambda.discrete=TRUE</code>, the bandwidth
will be discretized into <code>lambda.discrete.num+1</code> points and
<code>lambda</code> will be chosen from these points</p>
</td></tr>
<tr><td><code id="krscvNOMAD_+3A_lambda.discrete.num">lambda.discrete.num</code></td>
<td>
<p>a positive integer indicating the number of
discrete values that lambda can assume - this parameter will only be
used when <code>lambda.discrete=TRUE</code></p>
</td></tr>
<tr><td><code id="krscvNOMAD_+3A_random.seed">random.seed</code></td>
<td>

<p>when it is not missing and not equal to 0, the initial points  will 
be generated using this seed when <code>nmulti &gt; 0</code>
</p>
</td></tr>
<tr><td><code id="krscvNOMAD_+3A_max.bb.eval">max.bb.eval</code></td>
<td>

<p>argument passed to the NOMAD solver (see <code><a href="#topic+snomadr">snomadr</a></code> for
further details)
</p>
</td></tr>
<tr><td><code id="krscvNOMAD_+3A_initial.mesh.size.real">initial.mesh.size.real</code></td>
<td>

<p>argument passed to the NOMAD solver (see <code><a href="#topic+snomadr">snomadr</a></code> for
further details)
</p>
</td></tr>
<tr><td><code id="krscvNOMAD_+3A_initial.mesh.size.integer">initial.mesh.size.integer</code></td>
<td>

<p>argument passed to the NOMAD solver (see <code><a href="#topic+snomadr">snomadr</a></code> for
further details)
</p>
</td></tr>
<tr><td><code id="krscvNOMAD_+3A_min.mesh.size.real">min.mesh.size.real</code></td>
<td>

<p>argument passed to the NOMAD solver (see <code><a href="#topic+snomadr">snomadr</a></code> for
further details)
</p>
</td></tr>
<tr><td><code id="krscvNOMAD_+3A_min.mesh.size.integer">min.mesh.size.integer</code></td>
<td>

<p>arguments passed to the NOMAD solver (see <code><a href="#topic+snomadr">snomadr</a></code> for
further details)
</p>
</td></tr>
<tr><td><code id="krscvNOMAD_+3A_min.poll.size.real">min.poll.size.real</code></td>
<td>

<p>arguments passed to the NOMAD solver (see <code><a href="#topic+snomadr">snomadr</a></code> for
further details)
</p>
</td></tr>
<tr><td><code id="krscvNOMAD_+3A_min.poll.size.integer">min.poll.size.integer</code></td>
<td>

<p>arguments passed to the NOMAD solver (see <code><a href="#topic+snomadr">snomadr</a></code> for
further details)
</p>
</td></tr>      
<tr><td><code id="krscvNOMAD_+3A_opts">opts</code></td>
<td>
<p> list of optional arguments to be passed to
<code><a href="#topic+snomadr">snomadr</a></code> </p>
</td></tr>
<tr><td><code id="krscvNOMAD_+3A_nmulti">nmulti</code></td>
<td>

<p>integer number of times to restart the process of finding extrema of
the cross-validation function from different (random) initial
points (default <code>nmulti=0</code>)
</p>
</td></tr>
<tr><td><code id="krscvNOMAD_+3A_tau">tau</code></td>
<td>

<p>if non-null a number in (0,1) denoting the quantile for which a quantile
regression spline is to be estimated rather than estimating the
conditional mean (default <code>tau=NULL</code>)
</p>
</td></tr>
<tr><td><code id="krscvNOMAD_+3A_weights">weights</code></td>
<td>

<p>an optional vector of weights to be used in the fitting process.
Should be &lsquo;NULL&rsquo; or a numeric vector.  If non-NULL, weighted least
squares is used with weights &lsquo;weights&rsquo; (that is, minimizing
&lsquo;sum(w*e^2)&rsquo;); otherwise ordinary least squares is used.
</p>
</td></tr>
<tr><td><code id="krscvNOMAD_+3A_singular.ok">singular.ok</code></td>
<td>

<p>a logical value (default <code>singular.ok=FALSE</code>) that, when
<code>FALSE</code>, discards singular bases during cross-validation (a check
for ill-conditioned bases is performed).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>krscvNOMAD</code> computes NOMAD-based cross-validation for a
regression spline estimate of a one (1) dimensional dependent variable
on an <code>r</code>-dimensional vector of continuous and nominal/ordinal
(<code><a href="base.html#topic+factor">factor</a></code>/<code><a href="base.html#topic+ordered">ordered</a></code>) predictors.  Numerical
search for the optimal <code>degree</code>/<code>segments</code>/<code>lambda</code> is
undertaken using <code><a href="#topic+snomadr">snomadr</a></code>.
</p>
<p>The optimal <code>K</code>/<code>lambda</code> combination is returned along with
other results (see below for return values). The method uses kernel
functions appropriate for categorical (ordinal/nominal) predictors
which avoids the loss in efficiency associated with sample-splitting
procedures that are typically used when faced with a mix of continuous
and nominal/ordinal (<code><a href="base.html#topic+factor">factor</a></code>/<code><a href="base.html#topic+ordered">ordered</a></code>)
predictors.
</p>
<p>For the continuous predictors the regression spline model employs
either the additive or tensor product B-spline basis matrix for a
multivariate polynomial spline via the B-spline routines in the GNU
Scientific Library (<a href="https://www.gnu.org/software/gsl/">https://www.gnu.org/software/gsl/</a>) and the
<code><a href="#topic+tensor.prod.model.matrix">tensor.prod.model.matrix</a></code> function.
</p>
<p>For the discrete predictors the product kernel function is of the
&lsquo;Li-Racine&rsquo; type (see Li and Racine (2007) for details).
</p>


<h3>Value</h3>

<p><code>krscvNOMAD</code> returns a <code>crscv</code> object. Furthermore, the
function <code><a href="base.html#topic+summary">summary</a></code> supports objects of this type. The
returned objects have the following components:
</p>
<table>
<tr><td><code>K</code></td>
<td>
<p> scalar/vector containing optimal degree(s) of spline or
number of segments </p>
</td></tr>
<tr><td><code>K.mat</code></td>
<td>
<p> vector/matrix of values of <code>K</code> evaluated during search </p>
</td></tr>  
<tr><td><code>degree.max</code></td>
<td>
<p> the maximum degree of the B-spline basis for
each of the continuous predictors (default <code>degree.max=10</code>)</p>
</td></tr>
<tr><td><code>segments.max</code></td>
<td>
<p> the maximum segments of the B-spline basis for
each of the continuous predictors (default <code>segments.max=10</code>)</p>
</td></tr>
<tr><td><code>degree.min</code></td>
<td>
<p> the minimum degree of the B-spline basis for
each of the continuous predictors (default <code>degree.min=0</code>)</p>
</td></tr>
<tr><td><code>segments.min</code></td>
<td>
<p> the minimum segments of the B-spline basis for
each of the continuous predictors (default <code>segments.min=1</code>)</p>
</td></tr>
<tr><td><code>restarts</code></td>
<td>
<p> number of restarts during search, if any </p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p> optimal bandwidths for categorical predictors </p>
</td></tr>
<tr><td><code>lambda.mat</code></td>
<td>
<p> vector/matrix of optimal bandwidths for each degree of spline </p>
</td></tr>
<tr><td><code>cv.func</code></td>
<td>
<p> objective function value at optimum </p>
</td></tr>
<tr><td><code>cv.func.vec</code></td>
<td>
<p> vector of objective function values at each degree
of spline or number of segments in <code>K.mat</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jeffrey S. Racine <a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a> and Zhenghua Nie <a href="mailto:niez@mcmaster.ca">niez@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Abramson, M.A. and C. Audet and G. Couture and J.E. Dennis Jr. and
S. Le Digabel (2011), &ldquo;The NOMAD project&rdquo;. Software available
at https://www.gerad.ca/nomad.
</p>
<p>Craven, P. and G. Wahba (1979), &ldquo;Smoothing Noisy Data With
Spline Functions,&rdquo; Numerische Mathematik, 13, 377-403.
</p>
<p>Hurvich, C.M. and J.S. Simonoff and C.L. Tsai (1998),
&ldquo;Smoothing Parameter Selection in Nonparametric Regression
Using an Improved Akaike Information Criterion,&rdquo; Journal of the
Royal Statistical Society B, 60, 271-293.
</p>
<p>Le Digabel, S. (2011), &ldquo;Algorithm 909: NOMAD: Nonlinear
Optimization With The MADS Algorithm&rdquo;. ACM Transactions on
Mathematical Software, 37(4):44:1-44:15.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics:
Theory and Practice,</em> Princeton University Press.
</p>
<p>Ma, S. and J.S. Racine and L. Yang (2015), &ldquo;Spline
Regression in the Presence of Categorical Predictors,&rdquo; Journal of
Applied Econometrics, Volume 30, 705-717.
</p>
<p>Ma, S. and J.S. Racine (2013), &ldquo;Additive Regression
Splines with Irrelevant Categorical and Continuous Regressors,&rdquo;
Statistica Sinica, Volume 23, 515-541.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+loess">loess</a></code>, <code><a href="np.html#topic+npregbw">npregbw</a></code>, 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(42)
## Simulated data
n &lt;- 1000

x &lt;- runif(n)
z &lt;- round(runif(n,min=-0.5,max=1.5))
z.unique &lt;- uniquecombs(as.matrix(z))
ind &lt;-  attr(z.unique,"index")
ind.vals &lt;-  sort(unique(ind))
dgp &lt;- numeric(length=n)
for(i in 1:nrow(z.unique)) {
  zz &lt;- ind == ind.vals[i]
  dgp[zz] &lt;- z[zz]+cos(2*pi*x[zz])
}
y &lt;- dgp + rnorm(n,sd=.1)

xdata &lt;- data.frame(x,z=factor(z))

## Compute the optimal K and lambda, determine optimal number of knots, set
## spline degree for x to 3

cv &lt;- krscvNOMAD(x=xdata,y=y,complexity="knots",degree=c(3),segments=c(5))
summary(cv)
</code></pre>

<hr>
<h2 id='npglpreg'>Generalized Local Polynomial Regression</h2><span id='topic+npglpreg'></span><span id='topic+npglpreg.default'></span><span id='topic+npglpreg.formula'></span>

<h3>Description</h3>

<p><code>npglpreg</code> computes a generalized local polynomial kernel
regression estimate (Hall and Racine (2015)) of a one (1)
dimensional dependent variable on an <code>r</code>-dimensional vector of
continuous and categorical
(<code><a href="base.html#topic+factor">factor</a></code>/<code><a href="base.html#topic+ordered">ordered</a></code>) predictors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npglpreg(...)

## Default S3 method:
npglpreg(tydat = NULL,
         txdat = NULL,
         eydat = NULL,
         exdat = NULL,
         bws = NULL,
         degree = NULL,
         leave.one.out = FALSE,
         ckertype = c("gaussian", "epanechnikov", "uniform", "truncated gaussian"),
         ckerorder = 2,
         ukertype = c("liracine", "aitchisonaitken"),
         okertype = c("liracine", "wangvanryzin"),
         bwtype = c("fixed", "generalized_nn", "adaptive_nn", "auto"),
         gradient.vec = NULL,
         gradient.categorical = FALSE,
         cv.shrink = TRUE,
         cv.maxPenalty = sqrt(.Machine$double.xmax),
         cv.warning = FALSE,
         Bernstein = TRUE,
         mpi = FALSE,
         ...)

## S3 method for class 'formula'
npglpreg(formula,
         data = list(),
         tydat = NULL,
         txdat = NULL,
         eydat = NULL,
         exdat = NULL,
         bws = NULL,
         degree = NULL,
         leave.one.out = FALSE,
         ckertype = c("gaussian", "epanechnikov","uniform","truncated gaussian"),
         ckerorder = 2,
         ukertype = c("liracine", "aitchisonaitken"),
         okertype = c("liracine", "wangvanryzin"),
         bwtype = c("fixed", "generalized_nn", "adaptive_nn", "auto"),
         cv = c("degree-bandwidth", "bandwidth", "none"),
         cv.func = c("cv.ls", "cv.aic"),
         nmulti = 5,
         random.seed = 42,
         degree.max = 10,
         degree.min = 0,
         bandwidth.max = .Machine$double.xmax,
         bandwidth.min = sqrt(.Machine$double.eps),
         bandwidth.min.numeric = 1.0e-02,
         bandwidth.switch = 1.0e+06,
         bandwidth.scale.categorical = 1.0e+04,
         max.bb.eval = 10000,
         min.epsilon = .Machine$double.eps,
         initial.mesh.size.real = 1,
         initial.mesh.size.integer = 1,
         min.mesh.size.real = sqrt(.Machine$double.eps),
         min.mesh.size.integer = 1, 
         min.poll.size.real = 1,
         min.poll.size.integer = 1, 
         opts=list(),
         restart.from.min = FALSE,
         gradient.vec = NULL,
         gradient.categorical = FALSE,
         cv.shrink = TRUE,
         cv.maxPenalty = sqrt(.Machine$double.xmax),
         cv.warning = FALSE,
         Bernstein = TRUE,
         mpi = FALSE,
         ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npglpreg_+3A_formula">formula</code></td>
<td>
<p> a symbolic description of the model to be fit </p>
</td></tr>
<tr><td><code id="npglpreg_+3A_data">data</code></td>
<td>
<p> an optional data frame containing the variables in the
model </p>
</td></tr>
<tr><td><code id="npglpreg_+3A_tydat">tydat</code></td>
<td>

<p>a one (1) dimensional numeric or integer vector of dependent data, each
element <code class="reqn">i</code> corresponding to each observation (row) <code class="reqn">i</code> of
<code>txdat</code>. Defaults to the training data used to
compute the bandwidth object
</p>
</td></tr>
<tr><td><code id="npglpreg_+3A_txdat">txdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of explanatory data (training data) used to
calculate the regression estimators. Defaults to the training data used to
compute the bandwidth object
</p>
</td></tr>
<tr><td><code id="npglpreg_+3A_eydat">eydat</code></td>
<td>

<p>a one (1) dimensional numeric or integer vector of the true values
of the dependent variable. Optional, and used only to calculate the
true errors
</p>
</td></tr>
<tr><td><code id="npglpreg_+3A_exdat">exdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of points on which the regression will be
estimated (evaluation data). By default,
evaluation takes place on the data provided by <code>txdat</code>
</p>
</td></tr>
<tr><td><code id="npglpreg_+3A_bws">bws</code></td>
<td>

<p>a  vector of bandwidths, with each element <code class="reqn">i</code> corresponding
to the bandwidth for column <code class="reqn">i</code> in <code>txdat</code>
</p>
</td></tr>
<tr><td><code id="npglpreg_+3A_degree">degree</code></td>
<td>
<p> integer/vector specifying the polynomial degree of the
for each dimension of the continuous <code>x</code> in <code>txdat</code></p>
</td></tr>
<tr><td><code id="npglpreg_+3A_leave.one.out">leave.one.out</code></td>
<td>

<p>a logical value to specify whether or not to compute the leave one
out sums. Will not work if <code>exdat</code> is specified. Defaults to
<code>FALSE</code>
</p>
</td></tr>
<tr><td><code id="npglpreg_+3A_ckertype">ckertype</code></td>
<td>

<p>character string used to specify the continuous kernel type.
Can be set as <code>gaussian</code>, <code>epanechnikov</code>, or
<code>uniform</code>. Defaults to <code>gaussian</code>.
</p>
</td></tr>
<tr><td><code id="npglpreg_+3A_ckerorder">ckerorder</code></td>
<td>

<p>numeric value specifying kernel order (one of
<code>(2,4,6,8)</code>). Kernel order specified along with a
<code>uniform</code> continuous kernel type will be ignored. Defaults to
<code>2</code>.
</p>
</td></tr>
<tr><td><code id="npglpreg_+3A_ukertype">ukertype</code></td>
<td>

<p>character string used to specify the unordered categorical kernel type.
Can be set as <code>aitchisonaitken</code> or <code>liracine</code>. Defaults to
<code>liracine</code>
</p>
</td></tr>
<tr><td><code id="npglpreg_+3A_okertype">okertype</code></td>
<td>

<p>character string used to specify the ordered categorical kernel type.
Can be set as <code>wangvanryzin</code> or <code>liracine</code>. Defaults to
<code>liracine</code>
</p>
</td></tr>
<tr><td><code id="npglpreg_+3A_bwtype">bwtype</code></td>
<td>

<p>character string used for the continuous variable bandwidth type,
specifying the type of bandwidth to compute and return in the
<code>bandwidth</code> object. If <code>bwtype="auto"</code>, the bandwidth type
type will be automatically determined by cross-validation. Defaults
to <code>fixed</code>. Option summary:<br />
<code>fixed</code>: compute fixed bandwidths <br />
<code>generalized_nn</code>: compute generalized nearest neighbor bandwidths <br />
<code>adaptive_nn</code>: compute adaptive nearest neighbor bandwidths
</p>
</td></tr>
<tr><td><code id="npglpreg_+3A_cv">cv</code></td>
<td>
<p> a character string (default <code>cv="nomad"</code>) indicating
whether to use nonsmooth mesh adaptive direct search, or no search
(i.e. use supplied values for <code>degree</code> and <code>bws</code>)
</p>
</td></tr>
<tr><td><code id="npglpreg_+3A_cv.func">cv.func</code></td>
<td>
<p>a character string (default <code>cv.func="cv.ls"</code>)
indicating which method to use to select smoothing
parameters. <code>cv.aic</code> specifies expected Kullback-Leibler
cross-validation (Hurvich, Simonoff, and Tsai (1998)), and
<code>cv.ls</code> specifies least-squares cross-validation
</p>
</td></tr>
<tr><td><code id="npglpreg_+3A_max.bb.eval">max.bb.eval</code></td>
<td>

<p>argument passed to the NOMAD solver (see <code><a href="#topic+snomadr">snomadr</a></code> for
further details)
</p>
</td></tr>
<tr><td><code id="npglpreg_+3A_min.epsilon">min.epsilon</code></td>
<td>

<p>argument passed to the NOMAD solver (see <code><a href="#topic+snomadr">snomadr</a></code> for
further details)
</p>
</td></tr>
<tr><td><code id="npglpreg_+3A_initial.mesh.size.real">initial.mesh.size.real</code></td>
<td>

<p>argument passed to the NOMAD solver (see <code><a href="#topic+snomadr">snomadr</a></code> for
further details)
</p>
</td></tr>
<tr><td><code id="npglpreg_+3A_initial.mesh.size.integer">initial.mesh.size.integer</code></td>
<td>

<p>argument passed to the NOMAD solver (see <code><a href="#topic+snomadr">snomadr</a></code> for
further details)
</p>
</td></tr>
<tr><td><code id="npglpreg_+3A_min.mesh.size.real">min.mesh.size.real</code></td>
<td>

<p>argument passed to the NOMAD solver (see <code><a href="#topic+snomadr">snomadr</a></code> for
further details)
</p>
</td></tr>
<tr><td><code id="npglpreg_+3A_min.mesh.size.integer">min.mesh.size.integer</code></td>
<td>

<p>arguments passed to the NOMAD solver (see <code><a href="#topic+snomadr">snomadr</a></code> for
further details)
</p>
</td></tr>
<tr><td><code id="npglpreg_+3A_min.poll.size.real">min.poll.size.real</code></td>
<td>

<p>arguments passed to the NOMAD solver (see <code><a href="#topic+snomadr">snomadr</a></code> for
further details)
</p>
</td></tr>
<tr><td><code id="npglpreg_+3A_min.poll.size.integer">min.poll.size.integer</code></td>
<td>

<p>arguments passed to the NOMAD solver (see <code><a href="#topic+snomadr">snomadr</a></code> for
further details)
</p>
</td></tr>      
<tr><td><code id="npglpreg_+3A_opts">opts</code></td>
<td>
<p> list of optional arguments passed to the NOMAD solver
(see <code><a href="#topic+snomadr">snomadr</a></code> for further details) </p>
</td></tr>
<tr><td><code id="npglpreg_+3A_nmulti">nmulti</code></td>
<td>

<p>integer number of times to restart the process of finding extrema of
the cross-validation function from different (random) initial
points (default <code>nmulti=5</code>)
</p>
</td></tr>
<tr><td><code id="npglpreg_+3A_random.seed">random.seed</code></td>
<td>
<p> when it is not missing and not equal to 0, the
initial points will be generated using this seed when using
<code><a href="#topic+snomadr">snomadr</a></code> </p>
</td></tr>
<tr><td><code id="npglpreg_+3A_degree.max">degree.max</code></td>
<td>
<p> the maximum degree of the polynomial for
each of the continuous predictors (default <code>degree.max=10</code>)</p>
</td></tr>
<tr><td><code id="npglpreg_+3A_degree.min">degree.min</code></td>
<td>
<p> the minimum degree of the polynomial for
each of the continuous predictors (default <code>degree.min=0</code>)</p>
</td></tr>
<tr><td><code id="npglpreg_+3A_bandwidth.max">bandwidth.max</code></td>
<td>
<p> the maximum bandwidth scale (i.e. number of
scaled standard deviations) for each of the continuous predictors
(default <code>bandwidth.max=.Machine$double.xmax</code>)</p>
</td></tr>
<tr><td><code id="npglpreg_+3A_bandwidth.min">bandwidth.min</code></td>
<td>
<p> the minimum bandwidth scale for each of the
categorical predictors (default <code>sqrt(.Machine$double.eps)</code>)</p>
</td></tr>
<tr><td><code id="npglpreg_+3A_bandwidth.min.numeric">bandwidth.min.numeric</code></td>
<td>
<p> the minimum bandwidth scale (i.e. number
of scaled standard deviations) for each of the continuous predictors
(default <code>bandwidth.min=1.0e-02</code>)</p>
</td></tr>
<tr><td><code id="npglpreg_+3A_bandwidth.switch">bandwidth.switch</code></td>
<td>
<p> the minimum bandwidth scale (i.e. number of
scaled standard deviations) for each of the continuous predictors
(default <code>bandwidth.switch=1.0e+06</code>) before the local polynomial
is treated as global during cross-validation at which point a global
categorical kernel weighted least squares fit is used for
computational efficiency</p>
</td></tr>
<tr><td><code id="npglpreg_+3A_bandwidth.scale.categorical">bandwidth.scale.categorical</code></td>
<td>
<p> the upper end for the rescaled
bandwidths for the categorical predictors (default
<code>bandwidth.scale.categorical=1.0e+04</code>) - the aim is to &lsquo;even up&rsquo;
the scale of the search parameters as much as possible, so when very
large scale factors are selected for the continuous predictors, a
larger value may improve search</p>
</td></tr>
<tr><td><code id="npglpreg_+3A_restart.from.min">restart.from.min</code></td>
<td>
<p> a logical value indicating to recommence
<code><a href="#topic+snomadr">snomadr</a></code> with the optimal values found from its first
invocation (typically quick but sometimes recommended in the field of
optimization)</p>
</td></tr>
<tr><td><code id="npglpreg_+3A_gradient.vec">gradient.vec</code></td>
<td>
<p> a vector corresponding to the order of the
partial (or cross-partial) and which variable the partial (or
cross-partial) derivative(s) are required </p>
</td></tr>
<tr><td><code id="npglpreg_+3A_gradient.categorical">gradient.categorical</code></td>
<td>
<p> a logical value indicating whether
discrete gradients (i.e. differences in the response from the base
value for each categorical predictor) are to be computed </p>
</td></tr>
<tr><td><code id="npglpreg_+3A_cv.shrink">cv.shrink</code></td>
<td>
<p> a logical value indicating whether to use ridging
(Seifert and Gasser (2000)) for ill-conditioned inversion during
cross-validation (default <code>cv.shrink=TRUE</code>) or to instead test
for ill-conditioned matrices and penalize heavily when this is the
case (much stronger condition imposed on cross-validation)</p>
</td></tr>
<tr><td><code id="npglpreg_+3A_cv.maxpenalty">cv.maxPenalty</code></td>
<td>
<p> a penalty applied during cross-validation when a
delete-one estimate is not finite or the polynomial basis is not of
full column rank </p>
</td></tr>
<tr><td><code id="npglpreg_+3A_cv.warning">cv.warning</code></td>
<td>
<p> a logical value indicating whether to issue an
immediate warning message when ill conditioned bases are encountered
during cross-validation (default <code>cv.warning=FALSE</code>) </p>
</td></tr>
<tr><td><code id="npglpreg_+3A_bernstein">Bernstein</code></td>
<td>
<p> a logical value indicating whether to use raw
polynomials or Bernstein polynomials (default) (note that a Bernstein
polynomial is also know as a Bezier curve which is also a
B-spline with no interior knots)</p>
</td></tr>
<tr><td><code id="npglpreg_+3A_mpi">mpi</code></td>
<td>
<p> a logical value (default <code>mpi=FALSE</code>) that, when
<code>mpi=TRUE</code>, can call the <code>npRmpi</code> rather than the <code>np</code>
package (note - code needs to mirror examples in the demo directory of
the <code>npRmpi</code> package, you need to broadcast loading of the
<code>crs</code> package, and need <code>.Rprofile</code> in your current
directory)</p>
</td></tr>
<tr><td><code id="npglpreg_+3A_...">...</code></td>
<td>

<p>additional arguments supplied to specify the regression type,
bandwidth type, kernel types, training data, and so on, detailed
below
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Typical usages are (see below for a  list of options and also
the examples at the end of this help file)
</p>
<pre>
    ## Conduct generalized local polynomial estimation
    
    model &lt;- npglpreg(y~x1+x2)
    
    ## Conduct degree 0 local polynomial estimation
    ## (i.e. Nadaraya-Watson)
    
    model &lt;- npglpreg(y~x1+x2,cv="bandwidth",degree=c(0,0))    
    
    ## Conduct degree 1 local polynomial estimation (i.e. local linear)
    
    model &lt;- npglpreg(y~x1+x2,cv="bandwidth",degree=c(1,1))    
    
    ## Conduct degree 2 local polynomial estimation (i.e. local
    ## quadratic)
    
    model &lt;- npglpreg(y~x1+x2,cv="bandwidth",degree=c(2,2))

    ## Plot the mean and bootstrap confidence intervals

    plot(model,ci=TRUE)

    ## Plot the first partial derivatives and bootstrap confidence
    ## intervals

    plot(model,deriv=1,ci=TRUE)

    ## Plot the first second partial derivatives and bootstrap
    ## confidence intervals

    plot(model,deriv=2,ci=TRUE)        
    
  </pre>
<p>This function is in beta status until further notice (eventually it
will be rolled into the np/npRmpi packages after the final status of
snomadr/NOMAD gets sorted out).
</p>
<p>Optimizing the cross-validation function jointly for bandwidths
(vectors of continuous parameters) and polynomial degrees (vectors of
integer parameters) constitutes a mixed-integer optimization
problem. These problems are not only &lsquo;hard&rsquo; from the numerical
optimization perspective, but are also computationally intensive
(contrast this to where we conduct, say, local linear regression which
sets the degree of the polynomial vector to a global value
<code>degree=1</code> hence we only need to optimize with respect to the
continuous bandwidths). Because of this we must be mindful of the
presence of local optima (the objective function is non-convex and
non-differentiable). Restarting search from different initial starting
points is recommended (see <code>nmulti</code>) and by default this is done
more than once. We encourage users to adopt &lsquo;multistarting&rsquo; and
to investigate the impact of changing default search parameters such
as <code>initial.mesh.size.real</code>, <code>initial.mesh.size.integer</code>,
<code>min.mesh.size.real</code>,
<code>min.mesh.size.integer</code>,<code>min.poll.size.real</code>, and
<code>min.poll.size.integer</code>. The default values were chosen based on
extensive simulation experiments and were chosen so as to yield robust
performance while being mindful of excessive computation - of course,
no one setting can be globally optimal.
</p>


<h3>Value</h3>

<p><code>npglpreg</code> returns a <code>npglpreg</code> object.  The generic
functions <code><a href="stats.html#topic+fitted">fitted</a></code> and <code><a href="stats.html#topic+residuals">residuals</a></code> extract
(or generate) estimated values and residuals. Furthermore, the
functions <code><a href="base.html#topic+summary">summary</a></code>, <code><a href="stats.html#topic+predict">predict</a></code>, and
<code><a href="graphics.html#topic+plot">plot</a></code> (options <code>deriv=0</code>, <code>ci=FALSE</code>
[<code>ci=TRUE</code> produces pointwise bootstrap error bounds],
<code>persp.rgl=FALSE</code>,
<code>plot.behavior=c("plot","plot-data","data")</code>,
<code>plot.errors.boot.num=99</code>,
<code>plot.errors.type=c("quantiles","standard")</code>
[<code>"quantiles"</code> produces percentiles determined by
<code>plot.errors.quantiles</code> below, <code>"standard"</code> produces error
bounds given by +/- 1.96 bootstrap standard deviations],
<code>plot.errors.quantiles=c(.025,.975)</code>, <code>xtrim=0.0</code>,
<code>xq=0.5</code>) support objects of this type. The returned object has
the following components:
</p>
<table>
<tr><td><code>fitted.values</code></td>
<td>
<p> estimates of the regression function
(conditional mean) at the sample points or evaluation points </p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p> residuals computed at the sample points or
evaluation points </p>
</td></tr>
<tr><td><code>degree</code></td>
<td>
<p> integer/vector specifying the degree of the polynomial
for each dimension of the continuous <code>x</code></p>
</td></tr>
<tr><td><code>gradient</code></td>
<td>
<p> the estimated gradient (vector) corresponding to the vector
<code>gradient.vec</code></p>
</td></tr>
<tr><td><code>gradient.categorical.mat</code></td>
<td>
<p> the estimated gradient (matrix) for
the categorical predictors </p>
</td></tr>
<tr><td><code>gradient.vec</code></td>
<td>
<p> the supplied <code>gradient.vec</code></p>
</td></tr>
<tr><td><code>bws</code></td>
<td>
<p> vector of bandwidths </p>
</td></tr>
<tr><td><code>bwtype</code></td>
<td>
<p> the supplied <code>bwtype</code></p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p> a symbolic description of the model  </p>
</td></tr>
<tr><td><code>r.squared</code></td>
<td>
<p> coefficient of determination (Doksum and Samarov (1995))</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Note that the use of raw polynomials (<code>Bernstein=FALSE</code>) for
approximation is appealing as they can be computed and differentiated
easily, however, they can be unstable (their inversion can be ill
conditioned) which can cause problems in some instances as the order
of the polynomial increases. This can hamper search when excessive
reliance on ridging to overcome ill conditioned inversion becomes
computationally burdensome.
</p>
<p><code>npglpreg</code> tries to detect whether this is an issue or not when
<code>Bernstein=FALSE</code> for each <code>numeric</code> predictor and will
adjust the search range for <code><a href="#topic+snomadr">snomadr</a></code> and the degree fed
to <code>npglpreg</code> if appropriate.
</p>
<p>However, if you suspect that this might be an issue for your specific
problem and you are using raw polynomials (<code>Bernstein=FALSE</code>),
you are encouraged to investigate this by limiting <code>degree.max</code>
to value less than the default value (say <code>3</code>). Alternatively,
you might consider re-scaling your <code>numeric</code> predictors to lie in
<code class="reqn">[0,1]</code> using <code><a href="base.html#topic+scale">scale</a></code>.
</p>
<p>For a given predictor <code class="reqn">x</code> you can readily determine if this is an
issue by considering the following: Suppose <code class="reqn">x</code> is given by
</p>
<pre>
    x &lt;- runif(100,10000,11000)
    y &lt;- x + rnorm(100,sd=1000)
  </pre>
<p>so that a polynomial of order, say, <code class="reqn">5</code> would be ill
conditioned. This would be apparent if you considered
</p>
<pre>
    X &lt;- poly(x,degree=5,raw=TRUE)
    solve(t(X)%*%X)
  </pre>
<p>which will throw an error when the polynomial is ill conditioned,
or
</p>
<pre>
    X &lt;- poly(x,degree=5,raw=TRUE)
    lm(y~X)
  </pre>
<p>which will return <code>NA</code> for one or more coefficients when this is
an issue.
</p>
<p>In such cases you might consider transforming your <code>numeric</code>
predictors along the lines of the following:
</p>
<pre>
    x &lt;- as.numeric(scale(x))
    X &lt;- poly(x,degree=5,raw=TRUE)
    solve(t(X)%*%X)
    lm(y~X)    
  </pre>
<p>Note that now your least squares coefficients (i.e. first derivative
of <code class="reqn">y</code> with respect to <code class="reqn">x</code>) represent the effect of a one
standard deviation change in <code class="reqn">x</code> and not a one unit change.
</p>
<p>Alternatively, you can use Bernstein polynomials by not setting
<code>Bernstein=FALSE</code>.
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Racine <a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a> and Zhenghua Nie <a href="mailto:niez@mcmaster.ca">niez@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Doksum, K. and A. Samarov (1995), &ldquo;Nonparametric Estimation of
Global Functionals and a Measure of the Explanatory Power of
Covariates in Regression,&rdquo; The Annals of Statistics, 23, 1443-1473.
</p>
<p>Hall, P. and J.S. Racine (2015), &ldquo;Infinite Order
Cross-Validated Local Polynomial Regression,&rdquo; Journal of Econometrics,
185, 510-525.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics:
Theory and Practice,</em> Princeton University Press.
</p>
<p>Seifert, B. and T. Gasser (2000), &ldquo;Data Adaptive Ridging in
Local Polynomial Regression,&rdquo; Journal of Computational and Graphical
Statistics, 9(2), 338-360.
</p>


<h3>See Also</h3>

<p><code><a href="np.html#topic+npreg">npreg</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
set.seed(42)
n &lt;- 100
x1 &lt;- runif(n,-2,2)
x2 &lt;- runif(n,-2,2)
y &lt;- x1^3 + rnorm(n,sd=1)

## Ideally the method should choose large bandwidths for x1 and x2 and a
## generalized polynomial that is a cubic for x1 and degree 0 for x2.

model &lt;- npglpreg(y~x1+x2,nmulti=1)
summary(model)

## Plot the partial means and percentile confidence intervals
plot(model,ci=T)
## Extract the data from the plot object and plot it separately
myplot.dat &lt;- plot(model,plot.behavior="data",ci=T)
matplot(myplot.dat[[1]][,1],myplot.dat[[1]][,-1],type="l")
matplot(myplot.dat[[2]][,1],myplot.dat[[2]][,-1],type="l")

## End(Not run)
</code></pre>

<hr>
<h2 id='snomadr'>
R interface to NOMAD
</h2><span id='topic+snomadr'></span>

<h3>Description</h3>

<p><code>snomadr</code> is an R interface to NOMAD (Nonsmooth Optimization by
Mesh Adaptive Direct Search, Abramson, Audet, Couture and Le Digabel
(2011)), an open source software C++ implementation of the Mesh Adaptive
Direct Search (MADS, Le Digabel (2011)) algorithm designed for
constrained optimization of blackbox functions.
</p>
<p>NOMAD is designed to find (local) solutions of mathematical optimization
problems of the form
</p>
<pre>
   min     f(x)
x in R^n

s.t.       g(x) &lt;= 0 
           x_L &lt;=  x   &lt;= x_U
</pre>           
<p>where <code class="reqn">f(x)\colon R^n \to R^k</code> is the objective
function, and <code class="reqn">g(x)\colon R^n \to R^m</code> are
the constraint functions. The vectors <code class="reqn">x_L</code> and <code class="reqn">x_U</code>
are the bounds on the variables <code class="reqn">x</code>. The functions <code class="reqn">f(x)</code> and
<code class="reqn">g(x)</code> can be nonlinear and nonconvex. The variables can be integer,
continuous real number, binary, and categorical.
</p>
<p>Kindly see <a href="https://www.gerad.ca/en/software/nomad">https://www.gerad.ca/en/software/nomad</a> and the
references below for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>snomadr(eval.f, 
        n, 
        bbin = NULL,  
        bbout = NULL, 
        x0 = NULL, 
        lb = NULL, 
        ub = NULL, 
        nmulti = 0,
        random.seed = 0,
        opts = list(),
        print.output = TRUE, 
        information = list(), 
        snomadr.environment = new.env(), 
        ... ) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="snomadr_+3A_eval.f">eval.f</code></td>
<td>

<p>function that returns the value of the objective function
</p>
</td></tr>
<tr><td><code id="snomadr_+3A_n">n</code></td>
<td>
 
<p>the number of variables
</p>
</td></tr>
<tr><td><code id="snomadr_+3A_bbin">bbin</code></td>
<td>

<p>types of variables. Variable types are 0 (CONTINUOUS), 1
(INTEGER), 2 (CATEGORICAL), 3 (BINARY)
</p>
</td></tr>
<tr><td><code id="snomadr_+3A_bbout">bbout</code></td>
<td>

<p>types of output of <code>eval.f</code>.  See the NOMAD User Guide
<a href="https://nomad-4-user-guide.readthedocs.io/en/latest/#">https://nomad-4-user-guide.readthedocs.io/en/latest/#</a>
</p>
</td></tr>
<tr><td><code id="snomadr_+3A_x0">x0</code></td>
<td>

<p>vector with starting values for the optimization. If it is
provided and nmulti is bigger than 1, <code>x0</code> will be the first
initial point for multiple initial points
</p>
</td></tr>
<tr><td><code id="snomadr_+3A_lb">lb</code></td>
<td>

<p>vector with lower bounds of the controls (use -1.0e19 for controls without lower bound)
</p>
</td></tr>
<tr><td><code id="snomadr_+3A_ub">ub</code></td>
<td>

<p>vector with upper bounds of the controls (use  1.0e19 for controls without upper bound)
</p>
</td></tr>
<tr><td><code id="snomadr_+3A_nmulti">nmulti</code></td>
<td>

<p>when it is missing, or it is equal to 0 and <code>x0</code> is provided,
<code>snomadRSolve</code> will be called to solve the problem. Otherwise,
<code>smultinomadRSolve</code> will be called
</p>
</td></tr>
<tr><td><code id="snomadr_+3A_random.seed">random.seed</code></td>
<td>

<p>when it is not missing and not equal to 0, the initial points  will 
be generated using this seed when <code>nmulti &gt; 0</code>
</p>
</td></tr>
<tr><td><code id="snomadr_+3A_opts">opts</code></td>
<td>

<p>list of options for NOMAD, see the NOMAD user guide
<a href="https://nomad-4-user-guide.readthedocs.io/en/latest/#">https://nomad-4-user-guide.readthedocs.io/en/latest/#</a>.  Options
can also be set by nomad.opt which should be in the folder where R
(<code>snomadr</code>) is executed. Options that affect the solution and
their defaults and some potential values are
</p>
<p><code>"MAX_BB_EVAL"=10000</code>
</p>
<p><code>"INITIAL_MESH_SIZE"=1</code>
</p>
<p><code>"MIN_MESH_SIZE"="r1.0e-10"</code>
</p>
<p><code>"MIN_POLL_SIZE"=1</code>
</p>
<p>Note that the <code>"r..."</code> denotes relative measurement (relative
to <code>lb</code> and <code>ub</code>)
</p>
<p>Note that decreasing the maximum number of black box evaluations
(<code>"MAX_BB_EVAL"</code>) will terminate search sooner and may result
in a less accurate solution. For complicated problems you may want
to increase this value. When experimenting it is desirable to set
<code>"DISPLAY_DEGREE"=1</code> (or a larger integer) to get some sense
for how the algorithm is progressing
</p>
</td></tr>
<tr><td><code id="snomadr_+3A_print.output">print.output</code></td>
<td>

<p>when FALSE, no output from <code>snomadr</code> is displayed on the
screen. If the NOMAD option <code>"DISPLAY_DEGREE"=0,</code> is set,
there will also be no output from NOMAD. Higher integer values for
<code>"DISPLAY_DEGREE"=</code> provide successively more detail
</p>
</td></tr>
<tr><td><code id="snomadr_+3A_information">information</code></td>
<td>

<p>is a list. <code>snomadr</code> will call <code>snomadRInfo</code> to return
the information about NOMAD according to the values of <code>"info"</code>,
<code>"version"</code> and <code>"help"</code>.
</p>
<p><code>"info"="-i"</code>: display the usage and copyright of NOMAD
</p>
<p><code>"version"="-v"</code>: display the version of NOMAD you are using
</p>
<p><code>"help"="-h"</code>: display all options
</p>
<p>You also can display a specific option, for example,
<code>"help"="-h x0"</code>, this will tell you how to set <code>x0</code>
</p>
</td></tr>
<tr><td><code id="snomadr_+3A_snomadr.environment">snomadr.environment</code></td>
<td>

<p>environment that is used to evaluate the functions. Use this to pass 
additional data or parameters to a function
</p>
</td></tr>
<tr><td><code id="snomadr_+3A_...">...</code></td>
<td>

<p>arguments that will be passed to the user-defined objective and
constraints functions. See the examples below
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>snomadr</code> is used in the <span class="pkg">crs</span> package to numerically
minimize an objective function with respect to the spline degree,
number of knots, and optionally the kernel bandwidths when using
<code><a href="#topic+crs">crs</a></code> with the option <code>cv="nomad"</code> (default). This
is a constrained mixed integer combinatoric problem and is known to
be computationally &lsquo;hard&rsquo;. See <code><a href="#topic+frscvNOMAD">frscvNOMAD</a></code> and
<code><a href="#topic+krscvNOMAD">krscvNOMAD</a></code> for the functions called when
<code>cv="nomad"</code> while using <code><a href="#topic+crs">crs</a></code>.
</p>
<p>However, the user should note that for simple problems involving one
predictor exhaustive search may be faster and potentially more
accurate, so please bear in mind that <code>cv="exhaustive"</code> can be
useful when using <code><a href="#topic+crs">crs</a></code>.
</p>
<p>Naturally, exhaustive search is also useful for verifying solutions
returned by <code>snomadr</code>. See <code><a href="#topic+frscv">frscv</a></code> and
<code><a href="#topic+krscv">krscv</a></code> for the functions called when
<code>cv="exhaustive"</code> while using <code><a href="#topic+crs">crs</a></code>.
</p>


<h3>Value</h3>

<p>The return value contains a list with the inputs, and additional elements
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>the call that was made to solve</p>
</td></tr>
<tr><td><code>status</code></td>
<td>
<p>integer value with the status of the optimization </p>
</td></tr>
<tr><td><code>message</code></td>
<td>
<p>more informative message with the status of the optimization</p>
</td></tr>
<tr><td><code>iterations</code></td>
<td>
<p>number of iterations that were executed, if multiple initial points are set, 
this number will be the sum for each initial point.</p>
</td></tr>
<tr><td><code>objective</code></td>
<td>
<p>value if the objective function in the solution</p>
</td></tr>
<tr><td><code>solution</code></td>
<td>
<p>optimal value of the controls</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Zhenghua Nie &lt;niez@mcmaster.ca&gt;
</p>


<h3>References</h3>

<p>Abramson, M.A. and C. Audet and G. Couture and J.E. Dennis Jr. and
S. Le Digabel (2011), &ldquo;The NOMAD project&rdquo;. Software available
at https://www.gerad.ca/en/software/nomad/
</p>
<p>Le Digabel, S. (2011), &ldquo;Algorithm 909: NOMAD: Nonlinear
Optimization With The MADS Algorithm&rdquo;. ACM Transactions on
Mathematical Software, 37(4):44:1-44:15.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+optim">optim</a></code>, <code><a href="stats.html#topic+nlm">nlm</a></code>, <code><a href="stats.html#topic+nlminb">nlminb</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## List all options
snomadr(information=list("help"="-h"))

## Print given option,  for example,  MESH_SIZE
snomadr(information=list("help"="-h MESH_SIZE"))

## Print the version of NOMAD
snomadr(information=list("version"="-v"))

## Print usage and copyright
snomadr(information=list("info"="-i"))

## This is the example found in
## NOMAD/examples/basic/library/single_obj/basic_lib.cpp

eval.f &lt;- function ( x ) {

    f &lt;- c(Inf, Inf, Inf);
    n &lt;- length (x);

    if ( n == 5 &amp;&amp; ( is.double(x) || is.integer(x) ) ) {
        f[1] &lt;- x[5];
        f[2] &lt;- sum ( (x-1)^2 ) - 25;
        f[3] &lt;- 25 - sum ( (x+1)^2 );
    }  

    return ( as.double(f) );
}

## Initial values
x0 &lt;- rep(0.0, 5 )

bbin &lt;-c(1, 1, 1, 1, 1)
## Bounds
lb &lt;- rep(-6.0,5 )
ub &lt;- c(5.0, 6.0, 7.0, 1000000, 100000)

bbout &lt;-c(0, 2, 1)
## Options
opts &lt;-list("MAX_BB_EVAL"=500,
            "MIN_MESH_SIZE"=0.001,
            "INITIAL_MESH_SIZE"=0.1,
            "MIN_POLL_SIZE"=1)

snomadr(eval.f=eval.f,n=5,  x0=x0, bbin=bbin, bbout=bbout, lb=lb, ub=ub, opts=opts)


## How to transfer other parameters into eval.f
##
## First example: supply additional arguments in user-defined functions
##

## objective function and gradient in terms of parameters
eval.f.ex1 &lt;- function(x, params) { 
    return( params[1]*x^2 + params[2]*x + params[3] ) 
}

## Define parameters that we want to use
params &lt;- c(1,2,3)

## Define initial value of the optimization problem
x0 &lt;- 0

## solve using snomadr 
snomadr( n          =1, 
        x0          = x0, 
        eval.f      = eval.f.ex1, 
        params      = params )


##
## Second example: define an environment that contains extra parameters
##

## Objective function and gradient in terms of parameters
## without supplying params as an argument
eval.f.ex2 &lt;- function(x) { 
    return( params[1]*x^2 + params[2]*x + params[3] ) 
}

## Define initial value of the optimization problem
x0 &lt;- 0

## Define a new environment that contains params
auxdata        &lt;- new.env()
auxdata$params &lt;- c(1,2,3)

## pass The environment that should be used to evaluate functions to snomadr 
snomadr(n                  =1, 
        x0                 = x0, 
        eval.f             = eval.f.ex2, 
        snomadr.environment = auxdata )

## Solve using algebra
cat( paste( "Minimizing f(x) = ax^2 + bx + c\n" ) )
cat( paste( "Optimal value of control is -b/(2a) = ", -params[2]/(2*params[1]), "\n" ) )
cat( paste( "With value of the objective function f(-b/(2a)) = ",
           eval.f.ex1( -params[2]/(2*params[1]), params ), "\n" ) )

## The following example is NOMAD/examples/advanced/multi_start/multi.cpp
## This will call smultinomadRSolve to resolve the problem.  
eval.f.ex1 &lt;- function(x, params) { 
    M&lt;-as.numeric(params$M)
    NBC&lt;-as.numeric(params$NBC)

    f&lt;-rep(0, M+1)
    x&lt;-as.numeric(x)

    x1 &lt;- rep(0.0, NBC)
    y1 &lt;- rep(0.0, NBC)

    x1[1]&lt;-x[1]
    x1[2]&lt;-x[2]
    y1[3]&lt;-x[3]
    x1[4]&lt;-x[4]
    y1[4]&lt;-x[5]

    epi &lt;- 6
    for(i in 5:NBC){
        x1[i]&lt;-x[epi]
        epi &lt;- epi+1
        y1[i]&lt;-x[epi]
        epi&lt;-epi+1
    }
    constraint &lt;- 0.0
    ic &lt;- 1
    f[ic]&lt;-constraint
    ic &lt;- ic+1

    constraint &lt;- as.numeric(1.0)
    distmax &lt;- as.numeric(0.0)
    avg_dist &lt;- as.numeric(0.0)
    dist1&lt;-as.numeric(0.0)

    for(i in 1:(NBC-1)){
        for (j in (i+1):NBC){
            dist1 &lt;- as.numeric((x1[i]-x1[j])*(x1[i]-x1[j])+(y1[i]-y1[j])*(y1[i]-y1[j]))
            
            if((dist1 &gt; distmax)) {distmax &lt;- as.numeric(dist1)}
            if((dist1[1]) &lt; 1) {constraint &lt;- constraint*sqrt(dist1)}
            else if((dist1) &gt; 14) {avg_dist &lt;- avg_dist+sqrt(dist1)}
        }
    }

    if(constraint &lt; 0.9999) constraint &lt;- 1001.0-constraint
    else constraint = sqrt(distmax)+avg_dist/(10.0*NBC)

    f[2] &lt;- 0.0
    f[M+1] &lt;- constraint 


    return(as.numeric(f) ) 
}

## Define parameters that we want to use
params&lt;-list()
NBC &lt;- 5
M &lt;- 2
n&lt;-2*NBC-3

params$NBC&lt;-NBC
params$M&lt;-M
x0&lt;-rep(0.1, n)
lb&lt;-rep(0, n)
ub&lt;-rep(4.5, n)

eval.f.ex1(x0, params)

bbout&lt;-c(2, 2, 0)
nmulti=5
bbin&lt;-rep(0, n)
## Define initial value of the optimization problem

## Solve using snomadRSolve
snomadr(n            = as.integer(n), 
        x0           = x0, 
        eval.f       = eval.f.ex1, 
        bbin         = bbin, 
        bbout        = bbout, 
        lb           = lb, 
        ub           = ub, 
        params       = params )

## Solve using smultinomadRSolve, if x0 is provided,  x0 will
## be the first initial point,  otherwise,  the program will
## check best_x.txt,  if it exists,  it will be read in as
## the first initial point. Other initial points will be
## generated by uniform distribution.
## nmulti represents the number of mads will run.
##
snomadr(n            = as.integer(n), 
        eval.f       = eval.f.ex1, 
        bbin         = bbin, 
        bbout        = bbout, 
        lb           = lb, 
        ub           = ub, 
        nmulti = as.integer(nmulti), 
        print.output = TRUE, 
        params       = params )

## End(Not run) 
</code></pre>

<hr>
<h2 id='tensor.prod.model.matrix'> Utility functions for constructing tensor product smooths</h2><span id='topic+tensor.prod.model.matrix'></span>

<h3>Description</h3>

<p>Produce model matrices or penalty matrices for a tensor product smooth from the model matrices or
penalty matrices for the marginal bases of the smooth.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tensor.prod.model.matrix(X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tensor.prod.model.matrix_+3A_x">X</code></td>
<td>
<p>a list of model matrices for the marginal bases of a smooth</p>
</td></tr> 
</table>


<h3>Details</h3>

<p> If <code>X[[1]]</code>, <code>X[[2]]</code> ... <code>X[[m]]</code> are the model matrices of the marginal bases of 
a tensor product smooth then the ith row of the model matrix for the whole tensor product smooth is given by
<code>X[[1]][i,]%x%X[[2]][i,]%x% ... X[[m]][i,]</code>, where <code>%x%</code> is the Kronecker product. Of course 
the routine operates column-wise, not row-wise!
</p>


<h3>Value</h3>

<p> Either a single model matrix for a tensor product smooth, or a list of penalty terms for a tensor
product smooth.   
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>References</h3>

<p>Wood, S.N. (2006) &ldquo;Low Rank Scale Invariant Tensor Product
Smooths for Generalized Additive Mixed Models&rdquo;. Biometrics
62(4):1025-1036
</p>


<h3>See Also</h3>

  <p><code><a href="mgcv.html#topic+te">te</a></code>, <code><a href="mgcv.html#topic+smooth.construct.tensor.smooth.spec">smooth.construct.tensor.smooth.spec</a></code>  </p>


<h3>Examples</h3>

<pre><code class='language-R'>X &lt;- list(matrix(1:4,2,2),matrix(5:10,2,3))
tensor.prod.model.matrix(X)
</code></pre>

<hr>
<h2 id='uniquecombs'>Find the unique rows in a matrix </h2><span id='topic+uniquecombs'></span>

<h3>Description</h3>

<p>This routine returns a matrix containing all the unique rows of the
matrix supplied as its argument. That is, all the duplicate rows are
stripped out. Note that the ordering of the rows on exit is not the same
as on entry. It also returns an index attribute for relating the result back 
to the original matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>uniquecombs(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="uniquecombs_+3A_x">x</code></td>
<td>
<p> is an <span class="rlang"><b>R</b></span> matrix (numeric) </p>
</td></tr>
</table>


<h3>Details</h3>

<p> Models with more parameters than unique combinations of
covariates are not identifiable. This routine provides a means of
evaluating the number of unique combinations of covariates in a
model. The routine calls compiled C code.
</p>


<h3>Value</h3>

<p>A matrix consisting of the unique rows of <code>x</code> (in arbitrary order).
</p>
<p>The matrix has an <code>"index"</code> attribute. <code>index[i]</code> gives the row of the returned 
matrix that contains row i of the original matrix. 
</p>


<h3>Author(s)</h3>

<p> Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a></p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+unique">unique</a></code> can do the same thing, including for
non-numeric matrices, but more slowly and without returning the
index.</p>


<h3>Examples</h3>

<pre><code class='language-R'>X&lt;-matrix(c(1,2,3,1,2,3,4,5,6,1,3,2,4,5,6,1,1,1),6,3,byrow=TRUE)
print(X)
Xu &lt;- uniquecombs(X);Xu
ind &lt;- attr(Xu,"index")
## find the value for row 3 of the original from Xu
Xu[ind[3],];X[3,]
</code></pre>

<hr>
<h2 id='wage1'> Cross-Sectional Data on Wages </h2><span id='topic+wage1'></span>

<h3>Description</h3>

<p> Cross-section wage data consisting of a random sample
taken from the U.S. Current Population Survey for the year 1976. There
are 526 observations in total. </p>


<h3>Usage</h3>

<pre><code class='language-R'>data("wage1")</code></pre>


<h3>Format</h3>

<p> A data frame with 24 columns, and 526 rows.
</p>

<dl>
<dt>wage</dt><dd><p>column 1, of type <code>numeric</code>, average hourly earnings</p>
</dd>
<dt>educ</dt><dd><p>column 2, of type <code>numeric</code>, years of education</p>
</dd>
<dt>exper</dt><dd><p>column 3, of type <code>numeric</code>, years potential experience</p>
</dd>
<dt>tenure</dt><dd><p>column 4, of type <code>numeric</code>, years with current employer</p>
</dd>
<dt>nonwhite</dt><dd><p>column 5, of type <code>factor</code>, =&ldquo;Nonwhite&rdquo; if nonwhite, &ldquo;White&rdquo; otherwise</p>
</dd>
<dt>female</dt><dd><p>column 6, of type <code>factor</code>, =&ldquo;Female&rdquo; if female, &ldquo;Male&rdquo; otherwise</p>
</dd>
<dt>married</dt><dd><p>column 7, of type <code>factor</code>, =&ldquo;Married&rdquo; if Married, &ldquo;Nonmarried&rdquo; otherwise</p>
</dd>
<dt>numdep</dt><dd><p>column 8, of type <code>numeric</code>, number of dependents</p>
</dd>
<dt>smsa</dt><dd><p>column 9, of type <code>numeric</code>, =1 if live in SMSA</p>
</dd>
<dt>northcen</dt><dd><p>column 10, of type <code>numeric</code>, =1 if live in north central U.S</p>
</dd>
<dt>south</dt><dd><p>column 11, of type <code>numeric</code>, =1 if live in southern region</p>
</dd>
<dt>west</dt><dd><p>column 12, of type <code>numeric</code>, =1 if live in western region</p>
</dd>
<dt>construc</dt><dd><p>column 13, of type <code>numeric</code>, =1 if work in construc. indus.</p>
</dd>
<dt>ndurman</dt><dd><p>column 14, of type <code>numeric</code>, =1 if in nondur. manuf. indus.</p>
</dd>
<dt>trcommpu</dt><dd><p>column 15, of type <code>numeric</code>, =1 if in trans, commun, pub ut</p>
</dd>
<dt>trade</dt><dd><p>column 16, of type <code>numeric</code>, =1 if in wholesale or retail</p>
</dd>
<dt>services</dt><dd><p>column 17, of type <code>numeric</code>, =1 if in services indus.</p>
</dd>
<dt>profserv</dt><dd><p>column 18, of type <code>numeric</code>, =1 if in prof. serv. indus.</p>
</dd>
<dt>profocc</dt><dd><p>column 19, of type <code>numeric</code>, =1 if in profess. occupation</p>
</dd>
<dt>clerocc</dt><dd><p>column 20, of type <code>numeric</code>, =1 if in clerical occupation</p>
</dd>
<dt>servocc</dt><dd><p>column 21, of type <code>numeric</code>, =1 if in service occupation</p>
</dd>
<dt>lwage</dt><dd><p>column 22, of type <code>numeric</code>, log(wage)</p>
</dd>
<dt>expersq</dt><dd><p>column 23, of type <code>numeric</code>, exper<code class="reqn">^2</code></p>
</dd>
<dt>tenursq</dt><dd><p>column 24, of type <code>numeric</code>, tenure<code class="reqn">^2</code></p>
</dd>
</dl>



<h3>Source</h3>

<p> Jeffrey M. Wooldridge </p>


<h3>References</h3>

<p>Wooldridge, J.M. (2000), <em>Introductory Econometrics: A Modern
Approach</em>, South-Western College Publishing.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data(wage1)

## Cross-validated model selection for spline degree and bandwidths Note
## - we override the default nmulti here to get a quick illustration
## (we don't advise doing this, in fact advise using more restarts in
## serious applications)

model &lt;- crs(lwage~married+
             female+
             nonwhite+                
             educ+
             exper+
             tenure,
             basis="additive",
             complexity="degree",
             data=wage1,
             segments=c(1,1,1),
             nmulti=1)

summary(model)

## Residual plots
plot(model)
## Partial mean plots (control for non axis predictors)
plot(model,mean=TRUE)
## Partial first derivative plots (control for non axis predictors)
plot(model,deriv=1)
## Partial second derivative plots (control for non axis predictors)
plot(model,deriv=2)
## Compare with local linear kernel regression
require(np)
model &lt;- npreg(lwage~married+
               female+
               nonwhite+                
               educ+
               exper+
               tenure,
               regtype="ll",
               bwmethod="cv.aic",
               data=wage1)

summary(model)

## Partial mean plots (control for non axis predictors)
plot(model,common.scale=FALSE)
## Partial first derivative plots (control for non axis predictors)
plot(model,gradients=TRUE,common.scale=FALSE)
detach("package:np")

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
