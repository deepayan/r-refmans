<!DOCTYPE html><html><head><title>Help for package lda</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {lda}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#lda-package'>
<p>Collapsed Gibbs Samplers and Related Utility Functions for LDA-type Models</p></a></li>
<li><a href='#cora'>
<p>A subset of the Cora dataset of scientific documents.</p></a></li>
<li><a href='#filter.words'>
<p>Functions to manipulate text corpora in LDA format.</p></a></li>
<li><a href='#lda.collapsed.gibbs.sampler'>
<p>Functions to Fit LDA-type models</p></a></li>
<li><a href='#lexicalize'>
<p>Generate LDA Documents from Raw Text</p></a></li>
<li><a href='#links.as.edgelist'>
<p>Convert a set of links keyed on source to a single list of edges.</p></a></li>
<li><a href='#newsgroups'>
<p>A collection of newsgroup messages with classes.</p></a></li>
<li><a href='#nubbi.collapsed.gibbs.sampler'>
<p>Collapsed Gibbs Sampling for the Networks Uncovered By Bayesian</p>
Inference (NUBBI) Model.</a></li>
<li><a href='#poliblog'>
<p>A collection of political blogs with ratings.</p></a></li>
<li><a href='#predictive.distribution'>
<p>Compute predictive distributions for fitted LDA-type models.</p></a></li>
<li><a href='#predictive.link.probability'>
<p>Use the RTM to predict whether a link exists between two documents.</p></a></li>
<li><a href='#read.documents'>
<p>Read LDA-formatted Document and Vocabulary Files</p></a></li>
<li><a href='#rtm.collapsed.gibbs.sampler'>
<p>Collapsed Gibbs Sampling for the Relational Topic Model (RTM).</p></a></li>
<li><a href='#sampson'>
<p>Sampson monk data</p></a></li>
<li><a href='#slda.predict'>
<p>Predict the response variable of documents using an sLDA model.</p></a></li>
<li><a href='#top.topic.words'>
<p>Get the Top Words and Documents in Each Topic</p></a></li>
<li><a href='#word.counts'>
<p>Compute Summary Statistics of a Corpus</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Collapsed Gibbs Sampling Methods for Topic Models</td>
</tr>
<tr>
<td>Version:</td>
<td>1.4.2</td>
</tr>
<tr>
<td>Date:</td>
<td>2015-11-22</td>
</tr>
<tr>
<td>Author:</td>
<td>Jonathan Chang</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jonathan Chang &lt;slycoder@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements latent Dirichlet allocation (LDA)
	     and related models.  This includes (but is not limited
	     to) sLDA, corrLDA, and the mixed-membership stochastic
	     blockmodel.  Inference for all of these models is
	     implemented via a fast collapsed Gibbs sampler written
	     in C.  Utility functions for reading/writing data
	     typically used in topic models, as well as tools for
	     examining posterior distributions are also included.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/LGPL-2">LGPL-2</a> | <a href="https://www.r-project.org/Licenses/LGPL-2.1">LGPL-2.1</a> | <a href="https://www.r-project.org/Licenses/LGPL-3">LGPL-3</a> [expanded from: LGPL]</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>Suggests:</td>
<td>Matrix, reshape2, ggplot2 (&ge; 1.0.0), penalized, nnet</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2015-11-22 08:13:39 UTC; jonathanchang</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2015-11-22 11:48:11</td>
</tr>
</table>
<hr>
<h2 id='lda-package'>
Collapsed Gibbs Samplers and Related Utility Functions for LDA-type Models
</h2><span id='topic+lda-package'></span><span id='topic+lda'></span>

<h3>Description</h3>

<p>This package contains functions to read in text corpora, fit
LDA-type models to them, and use the fitted models to explore the data
and make predictions.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> lda</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.3.2</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2012-05-22</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> BSD </td>
</tr>
<tr>
 <td style="text-align: left;">
LazyLoad: </td><td style="text-align: left;"> yes</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Jonathan Chang (<a href="mailto:slycoder@gmail.com">slycoder@gmail.com</a>)
Andrew Dai (<a href="mailto:a.dai@ed.ac.uk">a.dai@ed.ac.uk</a>)
</p>
<p>Special thanks to the following for their reports and comments:
Edo Airoldi,
Jordan Boyd-Graber,
Christopher E. Cramer,
James Danowski,
Khalid El-Arini,
Roger Levy,
Solomon Messing,
Joerg Reichardt 
</p>


<h3>References</h3>

<p><cite>Blei, David M. and Ng, Andrew and Jordan, Michael. Latent Dirichlet allocation. Journal of Machine Learning Research, 2003.</cite>
</p>


<h3>See Also</h3>

<p>Functions to fit models:
<code><a href="#topic+lda.collapsed.gibbs.sampler">lda.collapsed.gibbs.sampler</a></code>
<code><a href="#topic+slda.em">slda.em</a></code>
<code><a href="#topic+mmsb.collapsed.gibbs.sampler">mmsb.collapsed.gibbs.sampler</a></code>
<code><a href="#topic+nubbi.collapsed.gibbs.sampler">nubbi.collapsed.gibbs.sampler</a></code>
<code><a href="#topic+rtm.collapsed.gibbs.sampler">rtm.collapsed.gibbs.sampler</a></code>
</p>
<p>Functions to read/create corpora:
<code><a href="#topic+lexicalize">lexicalize</a></code>
<code><a href="#topic+read.documents">read.documents</a></code>
<code><a href="#topic+read.vocab">read.vocab</a></code>
</p>
<p>Functions to manipulate corpora:
<code><a href="#topic+concatenate.documents">concatenate.documents</a></code>
<code><a href="#topic+filter.words">filter.words</a></code>
<code><a href="#topic+shift.word.indices">shift.word.indices</a></code>
<code><a href="#topic+links.as.edgelist">links.as.edgelist</a></code>
</p>
<p>Functions to compute summary statistics on corpora:
<code><a href="#topic+word.counts">word.counts</a></code>
<code><a href="#topic+document.lengths">document.lengths</a></code>
</p>
<p>Functions which use the output of fitted models:
<code><a href="#topic+predictive.distribution">predictive.distribution</a></code>
<code><a href="#topic+top.topic.words">top.topic.words</a></code>
<code><a href="#topic+top.topic.documents">top.topic.documents</a></code>
<code><a href="#topic+predictive.link.probability">predictive.link.probability</a></code>
</p>
<p>Included data sets:
<code><a href="#topic+cora">cora</a></code>
<code><a href="#topic+poliblog">poliblog</a></code>
<code><a href="#topic+sampson">sampson</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## See demos for the following three common use cases:

## Not run: demo(lda)

## Not run: demo(slda)

## Not run: demo(mmsb)

## Not run: demo(rtm)
</code></pre>

<hr>
<h2 id='cora'>
A subset of the Cora dataset of scientific documents.
</h2><span id='topic+cora'></span><span id='topic+cora.documents'></span><span id='topic+cora.vocab'></span><span id='topic+cora.cites'></span><span id='topic+cora.titles'></span>

<h3>Description</h3>

<p>A collection of 2410 scientific documents in LDA format with links and titles
from the Cora search engine.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(cora.documents)
data(cora.vocab)
data(cora.cites)
data(cora.titles)
</code></pre>


<h3>Format</h3>

<p><code>cora.documents</code> and <code>cora.vocab</code>
comprise a corpus of 2410 documents conforming to the LDA format.
</p>
<p><code>cora.titles</code> is a character vector of titles for each
document (i.e., each entry of <code>cora.documents</code>).
</p>
<p><code>cora.cites</code> is a list representing the citations between the
documents in the collection (see related for format).
</p>


<h3>Source</h3>

<p><cite>Automating the construction of internet protals with machine
learning.  McCallum et al.  Information Retrieval.  2000.</cite>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lda.collapsed.gibbs.sampler">lda.collapsed.gibbs.sampler</a></code> for the format of the
corpus.
</p>
<p><code><a href="#topic+rtm.collapsed.gibbs.sampler">rtm.collapsed.gibbs.sampler</a></code> for the format of the
citation links.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(cora.documents)
data(cora.vocab)
data(cora.links)
data(cora.titles)
</code></pre>

<hr>
<h2 id='filter.words'>
Functions to manipulate text corpora in LDA format.
</h2><span id='topic+filter.words'></span><span id='topic+concatenate.documents'></span><span id='topic+shift.word.indices'></span>

<h3>Description</h3>

<p><code>concatenate.documents</code> concatenates a set of documents.
<code>filter.words</code> removes references to certain words
from a collection of documents.
<code>shift.word.indices</code> adjusts references to words by a fixed amount.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>concatenate.documents(...)
filter.words(documents, to.remove)
shift.word.indices(documents, amount)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="filter.words_+3A_...">...</code></td>
<td>

<p>For <code>concatenate.documents</code>, the set of corpora to be merged.  All
arguments to <code>...</code> must be corpora of the same length.  The
documents in the same position in each of the arguments will be
concatenated, i.e., the new document 1 will be the concatenation of
document 1 from argument 1, document 2 from argument 1, etc.
</p>
</td></tr>
<tr><td><code id="filter.words_+3A_documents">documents</code></td>
<td>

<p>For <code>filter.words</code> and <code>shift.word.indices</code>, the corpus to
be operated on.
</p>
</td></tr>
<tr><td><code id="filter.words_+3A_to.remove">to.remove</code></td>
<td>

<p>For <code>filter.words</code>, an integer vector of words to filter.
The words in each document which also exist in <code>to.remove</code> will be removed. 
</p>
</td></tr>
<tr><td><code id="filter.words_+3A_amount">amount</code></td>
<td>

<p>For <code>shift.word.indices</code>, an integer scalar by which to shift
the vocabulary in the corpus.   <code>amount</code> will be added to each
entry of the word field in the corpus.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A corpus with the documents merged/words filtered/words shifted.  The format of the
input and output corpora is described in <code><a href="#topic+lda.collapsed.gibbs.sampler">lda.collapsed.gibbs.sampler</a></code>.
</p>


<h3>Author(s)</h3>

<p>Jonathan Chang (<a href="mailto:slycoder@gmail.com">slycoder@gmail.com</a>)    
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lda.collapsed.gibbs.sampler">lda.collapsed.gibbs.sampler</a></code> for the format of
the return value.
</p>
<p><code><a href="#topic+word.counts">word.counts</a></code> to compute statistics associated with a
corpus.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(cora.documents)

## Just use a small subset for the example.
corpus &lt;- cora.documents[1:6]
## Get the word counts.
wc &lt;- word.counts(corpus)

## Only keep the words which occur more than 4 times.
filtered &lt;- filter.words(corpus,
                         as.numeric(names(wc)[wc &lt;= 4]))
## [[1]]
##      [,1] [,2] [,3] [,4] [,5]
## [1,]    1   23   34   37   44
## [2,]    4    1    3    4    1
##
## [[2]]
##      [,1] [,2]
## [1,]   34   94
## [2,]    1    1
## ... long output ommitted ...

## Shift the second half of the corpus.
shifted &lt;- shift.word.indices(filtered[4:6], 100)
## [[1]]
##      [,1] [,2] [,3]
## [1,]  134  281  307
## [2,]    2    5    7
##
## [[2]]
##      [,1] [,2]
## [1,]  101  123
## [2,]    1    4
##
## [[3]]
##      [,1] [,2]
## [1,]  101  194
## [2,]    6    3

## Combine the unshifted documents and the shifted documents.
concatenate.documents(filtered[1:3], shifted)
## [[1]]
##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]
## [1,]    1   23   34   37   44  134  281  307
## [2,]    4    1    3    4    1    2    5    7
##
## [[2]]
##      [,1] [,2] [,3] [,4]
## [1,]   34   94  101  123
## [2,]    1    1    1    4
##
## [[3]]
##      [,1] [,2] [,3] [,4] [,5] [,6]
## [1,]   34   37   44   94  101  194
## [2,]    4    1    7    1    6    3

</code></pre>

<hr>
<h2 id='lda.collapsed.gibbs.sampler'>
Functions to Fit LDA-type models
</h2><span id='topic+lda.collapsed.gibbs.sampler'></span><span id='topic+slda.em'></span><span id='topic+mmsb.collapsed.gibbs.sampler'></span><span id='topic+lda.cvb0'></span>

<h3>Description</h3>

<p>These functions use a collapsed Gibbs sampler to fit three different
models: latent Dirichlet allocation (LDA), the mixed-membership stochastic
blockmodel (MMSB), and supervised LDA (sLDA).  These functions take
sparsely represented input documents, perform inference, and return
point estimates of the latent parameters using the state at the last
iteration of Gibbs sampling. Multinomial logit for sLDA is supported 
using the multinom function from nnet package .
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lda.collapsed.gibbs.sampler(documents, K, vocab, num.iterations, alpha,
eta, initial = NULL, burnin = NULL, compute.log.likelihood = FALSE,
  trace = 0L, freeze.topics = FALSE)

slda.em(documents, K, vocab, num.e.iterations, num.m.iterations, alpha,
eta, annotations, params, variance, logistic = FALSE, lambda = 10,
regularise = FALSE, method = "sLDA", trace = 0L, MaxNWts=3000)

mmsb.collapsed.gibbs.sampler(network, K, num.iterations, alpha,
beta.prior, initial = NULL, burnin = NULL, trace = 0L)

lda.cvb0(documents, K, vocab, num.iterations, alpha, eta, trace = 0L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lda.collapsed.gibbs.sampler_+3A_documents">documents</code></td>
<td>

<p>A list whose length is equal to the number of documents, D.   Each
element of <var>documents</var> is an integer matrix with two rows.  Each
column of <var>documents[[i]]</var> (i.e., document <code class="reqn">i</code>) represents a
word occurring in the document.
</p>
<p><var>documents[[i]][1, j]</var> is a
0-indexed word identifier for the jth word in document i.  That is,
this should be an index - 1 into <var>vocab</var>.  <var>documents[[i]][2,
j]</var> is an integer specifying the number of times that word appears in
the document.
</p>
</td></tr>
<tr><td><code id="lda.collapsed.gibbs.sampler_+3A_network">network</code></td>
<td>

<p>For <code>mmsb.collapsed.gibbs.sampler</code>, a <code class="reqn">D \times D</code> 
matrix (coercible as logical) representing the adjacency matrix for
the network.  Note that elements on the diagonal are ignored.
</p>
</td></tr>
<tr><td><code id="lda.collapsed.gibbs.sampler_+3A_k">K</code></td>
<td>

<p>An integer representing the number of topics in the model.
</p>
</td></tr>
<tr><td><code id="lda.collapsed.gibbs.sampler_+3A_vocab">vocab</code></td>
<td>

<p>A character vector specifying the vocabulary words associated with
the word indices used in <var>documents</var>. 
</p>
</td></tr>
<tr><td><code id="lda.collapsed.gibbs.sampler_+3A_num.iterations">num.iterations</code></td>
<td>

<p>The number of sweeps of Gibbs sampling over the entire corpus to make.
</p>
</td></tr>
<tr><td><code id="lda.collapsed.gibbs.sampler_+3A_num.e.iterations">num.e.iterations</code></td>
<td>

<p>For <code>slda.em</code>, the number of Gibbs sampling sweeps to make over
the entire corpus for each iteration of EM. 
</p>
</td></tr>
<tr><td><code id="lda.collapsed.gibbs.sampler_+3A_num.m.iterations">num.m.iterations</code></td>
<td>

<p>For <code>slda.em</code>, the number of EM iterations to make.
</p>
</td></tr>
<tr><td><code id="lda.collapsed.gibbs.sampler_+3A_alpha">alpha</code></td>
<td>

<p>The scalar value of the Dirichlet hyperparameter for
topic proportions. 
</p>
</td></tr>
<tr><td><code id="lda.collapsed.gibbs.sampler_+3A_beta.prior">beta.prior</code></td>
<td>

<p>For <code>mmsb.collapsed.gibbs.sampler</code>, the the beta hyperparameter
for each entry of the block relations matrix.  This parameter should
be a length-2 list whose entries are <code class="reqn">K \times K</code> matrices.  The
elements of the two matrices comprise the two parameters for each beta variable. 
</p>
</td></tr>
<tr><td><code id="lda.collapsed.gibbs.sampler_+3A_eta">eta</code></td>
<td>

<p>The scalar value of the Dirichlet hyperparamater for topic
multinomials.
</p>
</td></tr>
<tr><td><code id="lda.collapsed.gibbs.sampler_+3A_initial">initial</code></td>
<td>

<p>A list of initial topic assignments for words.  It should be
in the same format as the <var>assignments</var> field of the return
value.  If this field is NULL, then the sampler will be initialized
with random assignments.
</p>
</td></tr>
<tr><td><code id="lda.collapsed.gibbs.sampler_+3A_burnin">burnin</code></td>
<td>

<p>A scalar integer indicating the number of Gibbs sweeps to consider
as burn-in (i.e., throw away) for <code>lda.collapsed.gibbs.sampler</code>
and <code>mmsb.collapsed.gibbs.sampler</code>.  If this parameter is non-NULL, it
will also have the side-effect of enabling the
<var>document_expects</var> field of the return value (see below for
details).  Note that burnin iterations do NOT count towards <var>num.iterations</var>.
</p>
</td></tr>
<tr><td><code id="lda.collapsed.gibbs.sampler_+3A_compute.log.likelihood">compute.log.likelihood</code></td>
<td>

<p>A scalar logical which when <code>TRUE</code> will cause the sampler to
compute the log likelihood of the words (to within a constant
factor) after each sweep over the variables.  The log likelihood for each
iteration is stored in the <var>log.likelihood</var> field of the result.
This is useful for assessing convergence, but slows things down a tiny
bit.</p>
</td></tr>
<tr><td><code id="lda.collapsed.gibbs.sampler_+3A_annotations">annotations</code></td>
<td>

<p>A length D numeric vector of covariates associated with each
document.  Only used by <code>slda.em</code> which models documents along
with numeric annotations associated with each document. When using the 
logistic option, annotations must be consecutive integers starting from 0.
</p>
</td></tr>
<tr><td><code id="lda.collapsed.gibbs.sampler_+3A_params">params</code></td>
<td>

<p>For <code>slda.em</code>, a length Kx(number of classes-1) numeric vector of 
regression coefficients at which the EM algorithm should be initialized.
</p>
</td></tr>
<tr><td><code id="lda.collapsed.gibbs.sampler_+3A_variance">variance</code></td>
<td>

<p>For <code>slda.em</code>, the variance associated with the Gaussian
response modeling the annotations in <var>annotations</var>.
</p>
</td></tr>
<tr><td><code id="lda.collapsed.gibbs.sampler_+3A_logistic">logistic</code></td>
<td>

<p>For <code>slda.em</code>, a scalar logical which, when <code>TRUE</code>, causes
the annotations to be modeled using a logistic response instead of a
Gaussian (the covariates must be consecutive integers starting from 
zero when used with sLDA).  
</p>
</td></tr>
<tr><td><code id="lda.collapsed.gibbs.sampler_+3A_lambda">lambda</code></td>
<td>

<p>When <var>regularise</var> is <code>TRUE</code>. This is a scalar that is the
standard deviation of the Gaussian prior on the regression coefficients.
</p>
</td></tr>
<tr><td><code id="lda.collapsed.gibbs.sampler_+3A_regularise">regularise</code></td>
<td>

<p>When <code>TRUE</code>, a Gaussian prior is used for the regression
coefficients. This requires the <code>penalized</code> package.
</p>
</td></tr>
<tr><td><code id="lda.collapsed.gibbs.sampler_+3A_method">method</code></td>
<td>

<p>For <code>slda.em</code>, a character indicating how to model the
annotations.  Only <code>"sLDA"</code>, the stock model given in the
references, is officially supported at the moment.
</p>
</td></tr>
<tr><td><code id="lda.collapsed.gibbs.sampler_+3A_trace">trace</code></td>
<td>

<p>When <code>trace</code> is greater than zero, diagnostic messages will be
output.  Larger values of <code>trace</code> imply more messages.
</p>
</td></tr>
<tr><td><code id="lda.collapsed.gibbs.sampler_+3A_maxnwts">MaxNWts</code></td>
<td>

<p>Input to the nnet's multinom function with a default value of 3000 maximum 
weights. Increasing this value may be necessary when using logistic sLDA with
a large number of topics at the necessary expense of longer run times. 
</p>
</td></tr>
<tr><td><code id="lda.collapsed.gibbs.sampler_+3A_freeze.topics">freeze.topics</code></td>
<td>

<p>When <code>TRUE</code>, topic assignments will occur but the counts of
words associated with topics will not change. <var>initial</var> should be
set when this option is used. This is best use for sampling test
documents.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A fitted model as a list with the following components:
</p>
<table>
<tr><td><code>assignments</code></td>
<td>
<p>A list of length D.  Each element of the list, say
<code>assignments[[i]]</code> is an integer vector of the same length as the
number of columns in <code>documents[[i]]</code> indicating the topic
assignment for each word.</p>
</td></tr>  
<tr><td><code>topics</code></td>
<td>
<p>A <code class="reqn">K \times V</code> matrix where each entry indicates the
number of times a word (column) was assigned to a topic (row).  The column
names should correspond to the vocabulary words given in <var>vocab</var>.</p>
</td></tr>
<tr><td><code>topic_sums</code></td>
<td>
<p>A length K vector where each entry indicates the
total number of times words were assigned to each topic.</p>
</td></tr>
<tr><td><code>document_sums</code></td>
<td>
<p>A <code class="reqn">K \times D</code> matrix where each entry is an
integer indicating the number of times words in each document
(column) were assigned to each topic (column).</p>
</td></tr>
<tr><td><code>log.likelihoods</code></td>
<td>
<p>Only for <code>lda.collapsed.gibbs.sampler</code>.  A
matrix with 2 rows and <code>num.iterations</code> columns of log likelihoods when the flag
<code>compute.log.likelihood</code> is set to <code>TRUE</code>.  The first row
contains the full log likelihood (including the prior), whereas the
second row contains the log likelihood of the observations
conditioned on the assignments.</p>
</td></tr>
<tr><td><code>document_expects</code></td>
<td>
<p>This field only exists if <var>burnin</var> is
non-NULL. This field is like document_sums but instead of only
aggregating counts for the last iteration, this field aggegates
counts over all iterations after burnin.</p>
</td></tr>  
<tr><td><code>net.assignments.left</code></td>
<td>
<p>Only for
<code>mmsb.collapsed.gibbs.sampler</code>.  A <code class="reqn">D \times D</code> integer matrix of
topic assignments for the source document corresponding to the link
between one document (row) and another (column).</p>
</td></tr>
<tr><td><code>net.assignments.right</code></td>
<td>
<p>Only for
<code>mmsb.collapsed.gibbs.sampler</code>.  A <code class="reqn">D \times D</code> integer matrix of
topic assignments for the destination document corresponding to the link
between one document (row) and another (column).</p>
</td></tr>
<tr><td><code>blocks.neg</code></td>
<td>
<p>Only for
<code>mmsb.collapsed.gibbs.sampler</code>.  A <code class="reqn">K \times K</code> integer
matrix indicating the number of times the source of a non-link was
assigned to a topic (row) and the destination was assigned to
another (column).</p>
</td></tr>
<tr><td><code>blocks.pos</code></td>
<td>
<p>Only for
<code>mmsb.collapsed.gibbs.sampler</code>.  A <code class="reqn">K \times K</code> integer
matrix indicating the number of times the source of a link was
assigned to a topic (row) and the destination was assigned to
another (column).</p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p>For <code>slda.em</code>, a model of type <code><a href="stats.html#topic+lm">lm</a></code>,
the regression
model fitted to the annotations.</p>
</td></tr>
<tr><td><code>coefs</code></td>
<td>
<p>For <code>slda.em</code>, a length Kx(number of classes-1) 
numeric vector of coefficients for the regression model.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>WARNING: This function does not compute precisely the correct thing
when the count associated with a word in a document is not 1 (this
is for speed reasons currently).  A workaround when a word appears
multiple times is to replicate the word across several columns of a
document.  This will likely be fixed in a future version.
</p>


<h3>Author(s)</h3>

<p>Jonathan Chang (<a href="mailto:slycoder@gmail.com">slycoder@gmail.com</a>)
</p>


<h3>References</h3>

<p><cite>Blei, David M. and Ng, Andrew and Jordan, Michael. Latent
Dirichlet allocation. Journal of Machine Learning Research, 2003.</cite>
</p>
<p><cite>Airoldi , Edoardo M.  and Blei, David M. and Fienberg, Stephen
E. and Xing, Eric P.  Mixed Membership Stochastic
Blockmodels. Journal of Machine Learning Research, 2008.</cite>
</p>
<p><cite>Blei, David M. and McAuliffe, John.  Supervised topic models.
Advances in Neural Information Processing Systems, 2008.</cite>
</p>
<p><cite>Griffiths, Thomas L. and Steyvers, Mark.  Finding scientific
topics.  Proceedings of the National Academy of Sciences, 2004.</cite>
</p>
<p><cite>Asuncion, A., Welling, M., Smyth, P., and Teh, Y. W.  On
smoothing and inference for topic models.  Uncertainty in Artificial Intelligence,
2009.</cite>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+read.documents">read.documents</a></code> and <code><a href="#topic+lexicalize">lexicalize</a></code> can be used
to generate the input data to these models.
</p>
<p><code><a href="#topic+top.topic.words">top.topic.words</a></code>,
<code><a href="#topic+predictive.distribution">predictive.distribution</a></code>, and <code><a href="#topic+slda.predict">slda.predict</a></code> for operations on the fitted models. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## See demos for the three functions:

## Not run: demo(lda)

## Not run: demo(slda)

## Not run: demo(mmsb)
</code></pre>

<hr>
<h2 id='lexicalize'>
Generate LDA Documents from Raw Text
</h2><span id='topic+lexicalize'></span>

<h3>Description</h3>

<p>This function reads raw text in <dfn>doclines</dfn> format and returns a
corpus and vocabulary suitable for the inference procedures
defined in the <span class="pkg">lda</span> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lexicalize(doclines, sep = " ", lower = TRUE, count = 1L, vocab = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lexicalize_+3A_doclines">doclines</code></td>
<td>

<p>A character vector of document lines to be used to construct
a corpus.  See details for a description of the format of these
lines. 
</p>
</td></tr>
<tr><td><code id="lexicalize_+3A_sep">sep</code></td>
<td>

<p>Separator string which is used to tokenize the input strings
(default &lsquo;<span class="samp">&#8288; &#8288;</span>&rsquo;).
</p>
</td></tr>
<tr><td><code id="lexicalize_+3A_lower">lower</code></td>
<td>

<p>Logical indicating whether or not to convert all tokens to
lowercase (default &lsquo;<span class="samp">&#8288;TRUE&#8288;</span>&rsquo;).
</p>
</td></tr>
<tr><td><code id="lexicalize_+3A_count">count</code></td>
<td>

<p>An integer scaling factor to be applied to feature counts.  A single
observation of a feature will be rendered as <var>count</var>
observations in the return value (the default
value, &lsquo;<span class="samp">&#8288;1&#8288;</span>&rsquo;, is appropriate in most cases).
</p>
</td></tr>
<tr><td><code id="lexicalize_+3A_vocab">vocab</code></td>
<td>

<p>If left unspecified (or <code>NULL</code>), the vocabulary for the corpus
will be automatically inferred from the observed tokens.  Otherwise,
this parameter should be a character vector specifying acceptable
tokens.  Tokens not appearing in this list will be filtered from the
documents. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function first tokenizes a character vector by splitting each
entry of the vector by <var>sep</var> (note that this is currently a fixed
separator, not a regular expression).  If <var>lower</var> is &lsquo;<span class="samp">&#8288;TRUE&#8288;</span>&rsquo;,
then the tokens are then all converted to lowercase.
</p>
<p>At this point, if <var>vocab</var> is <code>NULL</code>, then a vocabulary is
constructed from the set of unique tokens appearing across all
character vectors.  Otherwise, the tokens derived from the character
vectors are filtered so that only those appearing in <var>vocab</var> are
retained.
</p>
<p>Finally, token instances within each document (i.e., original
character string) are tabulated in the format described in
<code><a href="#topic+lda.collapsed.gibbs.sampler">lda.collapsed.gibbs.sampler</a></code>.
</p>


<h3>Value</h3>

<p>If <var>vocab</var> is unspecified or <code>NULL</code>, a list with two components:
</p>
<table>
<tr><td><code>documents</code></td>
<td>
<p>A list of document matrices in the format described in <code><a href="#topic+lda.collapsed.gibbs.sampler">lda.collapsed.gibbs.sampler</a></code>.</p>
</td></tr>
<tr><td><code>vocab</code></td>
<td>
<p>A character vector of unique tokens occurring in the corpus.</p>
</td></tr>
</table>


<h3>Note</h3>

<p> Because of the limited tokenization and filtering capabilities of this
function, it may not be useful in many cases.  This may be resolved
in a future release.
</p>


<h3>Author(s)</h3>

<p>Jonathan Chang (<a href="mailto:slycoder@gmail.com">slycoder@gmail.com</a>)  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lda.collapsed.gibbs.sampler">lda.collapsed.gibbs.sampler</a></code> for the format of
the return value.
</p>
<p><code><a href="#topic+read.documents">read.documents</a></code> to generate the same output from a file
encoded in LDA-C format.
</p>
<p><code><a href="#topic+word.counts">word.counts</a></code> to compute statistics associated with a
corpus.
</p>
<p><code><a href="#topic+concatenate.documents">concatenate.documents</a></code> for operations on a collection of documents.  
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Generate an example.
example &lt;- c("I am the very model of a modern major general",
             "I have a major headache")

corpus &lt;- lexicalize(example, lower=TRUE)

## corpus$documents:
## $documents[[1]]
##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## [1,]    0    1    2    3    4    5    6    7    8     9
## [2,]    1    1    1    1    1    1    1    1    1     1
## 
## $documents[[2]]
##      [,1] [,2] [,3] [,4] [,5]
## [1,]    0   10    6    8   11
## [2,]    1    1    1    1    1

## corpus$lexicon:
## $vocab
## [1] "i"        "am"       "the"      "very"     "model"    "of"      
## [7] "a"        "modern"   "major"    "general"  "have"     "headache"

## Only keep words that appear at least twice:
to.keep &lt;- corpus$vocab[word.counts(corpus$documents, corpus$vocab) &gt;= 2]

## Re-lexicalize, using this subsetted vocabulary
documents &lt;- lexicalize(example, lower=TRUE, vocab=to.keep)

## documents:
## [[1]]
##      [,1] [,2] [,3]
## [1,]    0    1    2
## [2,]    1    1    1
## 
## [[2]]
##      [,1] [,2] [,3]
## [1,]    0    1    2
## [2,]    1    1    1
</code></pre>

<hr>
<h2 id='links.as.edgelist'>
Convert a set of links keyed on source to a single list of edges.
</h2><span id='topic+links.as.edgelist'></span>

<h3>Description</h3>

<p>This function takes as input a collection of links (as used/described
by the model fitting functions in this package) and reproduces the
links as a matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>links.as.edgelist(links)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="links.as.edgelist_+3A_links">links</code></td>
<td>

<p>A list of links; the format of this is described in <code><a href="#topic+rtm.collapsed.gibbs.sampler">rtm.collapsed.gibbs.sampler</a></code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A two-column matrix where each row represents an edge.   Note that the
indices in this matrix are 1-indexed rather than 0-indexed.
</p>


<h3>Author(s)</h3>

<p>Jonathan Chang (<a href="mailto:slycoder@gmail.com">slycoder@gmail.com</a>)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rtm.collapsed.gibbs.sampler">rtm.collapsed.gibbs.sampler</a></code> for the input format.
<code><a href="#topic+predictive.link.probability">predictive.link.probability</a></code> is a usage example of the output
of this function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Take the citations for the first few documents of Cora.
data(cora.cites)

links &lt;- cora.cites[1:5]
links
## [[1]]
## [1] 484 389

## [[2]]
## integer(0)

## [[3]]
## integer(0)

## [[4]]
## [1] 177 416 533

## [[5]]
## [1] 153

links.as.edgelist(links)
##      [,1] [,2]
## [1,]    1  485
## [2,]    1  390
## [3,]    4  178
## [4,]    4  417
## [5,]    4  534
## [6,]    5  154
</code></pre>

<hr>
<h2 id='newsgroups'>
A collection of newsgroup messages with classes.
</h2><span id='topic+newsgroup'></span><span id='topic+newsgroup.train.documents'></span><span id='topic+newsgroup.test.documents'></span><span id='topic+newsgroup.train.labels'></span><span id='topic+newsgroup.test.labels'></span><span id='topic+newsgroup.vocab'></span><span id='topic+newsgroup.label.map'></span>

<h3>Description</h3>

<p>The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, 
partitioned (nearly) evenly across 20 different newsgroups. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(newsgroup.train.documents)
data(newsgroup.test.documents)
data(newsgroup.train.labels)
data(newsgroup.test.labels)
data(newsgroup.vocab)
data(newsgroup.label.map)
</code></pre>


<h3>Format</h3>

<p><code>newsgroup.train.documents</code> and <code>newsgroup.test.documents</code>
comprise a corpus of 20,000 newsgroup documents conforming to the LDA format,
partitioned into 11269 training and 7505 training and test cases evenly distributed
across 20 classes. 
</p>
<p><code>newsgroup.train.labels</code> is a numeric vector of length 11269 which gives
a class label from 1 to 20 for each training document in the corpus.
</p>
<p><code>newsgroup.test.labels</code> is a numeric vector of length 7505 which gives
a class label from 1 to 20 for each training document in the corpus.
</p>
<p><code>newsgroup.vocab</code> is the vocabulary of the corpus.
</p>
<p><code>newsgroup.label.map</code> maps the numeric class labels to actual class names.
</p>


<h3>Source</h3>

<p><cite>http://qwone.com/~jason/20Newsgroups/</cite>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lda.collapsed.gibbs.sampler">lda.collapsed.gibbs.sampler</a></code> for the format of the
corpus.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(newsgroup.train.documents)
data(newsgroup.test.documents)
data(newsgroup.train.labels)
data(newsgroup.test.labels)
data(newsgroup.vocab)
data(newsgroup.label.map)
</code></pre>

<hr>
<h2 id='nubbi.collapsed.gibbs.sampler'>
Collapsed Gibbs Sampling for the Networks Uncovered By Bayesian
Inference (NUBBI) Model.
</h2><span id='topic+nubbi.collapsed.gibbs.sampler'></span>

<h3>Description</h3>

<p>Fit a NUBBI model, which takes as input a collection of entities with
corresponding textual descriptions as well as a set of descriptions
for pairs of entities.  The NUBBI model the produces a latent space
description of both the entities and the relationships between them.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nubbi.collapsed.gibbs.sampler(contexts, pair.contexts, pairs, K.individual,
                              K.pair, vocab, num.iterations, alpha, eta, xi)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nubbi.collapsed.gibbs.sampler_+3A_contexts">contexts</code></td>
<td>

<p>The set of textual descriptions (i.e., documents) for individual
entities in LDA format (see
<code><a href="#topic+lda.collapsed.gibbs.sampler">lda.collapsed.gibbs.sampler</a></code> for details).
</p>
</td></tr>
<tr><td><code id="nubbi.collapsed.gibbs.sampler_+3A_pair.contexts">pair.contexts</code></td>
<td>

<p>A set of textual descriptions for pairs of entities, also in LDA format.
</p>
</td></tr>
<tr><td><code id="nubbi.collapsed.gibbs.sampler_+3A_pairs">pairs</code></td>
<td>

<p>Labelings as to which pair each element of <code>pair.contexts</code>
refer to.  This parameter should be an integer matrix with two columns
and the same number of rows as <code>pair.contexts</code>.  The two
elements in each row of <code>pairs</code> are 0-indexed indices into
<code>contexts</code> indicating which two entities that element of
<code>pair.contexts</code> describes.
</p>
</td></tr>
<tr><td><code id="nubbi.collapsed.gibbs.sampler_+3A_k.individual">K.individual</code></td>
<td>

<p>A scalar integer representing the number of topics for the individual entities.
</p>
</td></tr>
<tr><td><code id="nubbi.collapsed.gibbs.sampler_+3A_k.pair">K.pair</code></td>
<td>

<p>A scalar integer representing the number of topics for entity pairs.
</p>
</td></tr>
<tr><td><code id="nubbi.collapsed.gibbs.sampler_+3A_vocab">vocab</code></td>
<td>

<p>A character vector specifying the vocabulary words associated with
the word indices used in <var>contexts</var> and <var>pair.contexts</var>.
</p>
</td></tr>
<tr><td><code id="nubbi.collapsed.gibbs.sampler_+3A_num.iterations">num.iterations</code></td>
<td>

<p>The number of sweeps of Gibbs sampling over the entire corpus to make.
</p>
</td></tr>
<tr><td><code id="nubbi.collapsed.gibbs.sampler_+3A_alpha">alpha</code></td>
<td>

<p>The scalar value of the Dirichlet hyperparameter for
topic proportions.
</p>
</td></tr>
<tr><td><code id="nubbi.collapsed.gibbs.sampler_+3A_eta">eta</code></td>
<td>

<p>The scalar value of the Dirichlet hyperparamater for topic
multinomials.
</p>
</td></tr>
<tr><td><code id="nubbi.collapsed.gibbs.sampler_+3A_xi">xi</code></td>
<td>

<p>The scalar value of the Dirichlet hyperparamater for source
proportions.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The NUBBI model is a switching model wherein the description of each
entity-pair can be ascribed to either the first entity of the pair,
the second entity of the pair, or their relationship.  The NUBBI model
posits a latent space (i.e., topic model) over the individual entities, and a different
latent space over entity relationships.
</p>
<p>The collapsed Gibbs sampler used in this model is different than the
variational inference method proposed in the paper and is highly experimental.
</p>


<h3>Value</h3>

<p>A fitted model as a list with the same components as returned by
<code><a href="#topic+lda.collapsed.gibbs.sampler">lda.collapsed.gibbs.sampler</a></code> with the following additional components:
</p>
<table>
<tr><td><code>source_assignments</code></td>
<td>
<p>A list of <code>length(pair.contexts)</code> whose
elements <code>source_assignments[[i]]</code> are of the same length as
<code>pair.contexts[[i]]</code> where each entry is either 0 if the
sampler assigned the word to the first entity, 1 if the sampler
assigned the word to the second entity, or 2 if the sampler assigned
the word to the relationship between the two.</p>
</td></tr>
<tr><td><code>document_source_sums</code></td>
<td>
<p>A matrix with three columns and
<code>length(pair.contexts)</code> rows where each row indicates how many
words were assigned to the first entity of the pair, the second
entity of the pair, and the relationship between the two,
respectively.</p>
</td></tr>
<tr><td><code>document_sums</code></td>
<td>
<p>Semantically similar to the entry in
<code>lda.collapsed.gibbs.sampler</code>, except that it is a list whose
first <code>length(contexts)</code> correspond to the columns of the entry
in <code>lda.collapsed.gibbs.sampler</code> for the individual contexts,
and the remaining <code>length(pair.contexts)</code> entries correspond to
the columns for the pair contexts.</p>
</td></tr>
<tr><td><code>topics</code></td>
<td>
<p>Like the entry in <code>lda.collapsed.gibbs.sampler</code>,
except that it contains the concatenation of the <code>K.individual</code>
topics and the <code>K.pair</code> topics.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The underlying sampler is quite general and could potentially be used
for other models such as the author-topic model (McCallum et al.) and the citation
influence model (Dietz et al.).   Please examine the source code
and/or contact the author(s) for further details.
</p>


<h3>Author(s)</h3>

<p>Jonathan Chang (<a href="mailto:slycoder@gmail.com">slycoder@gmail.com</a>)
</p>


<h3>References</h3>

<p><cite>Chang, Jonathan and Boyd-Graber, Jordan and Blei, David M.
Connections between the lines: Augmenting social networks with text.
KDD, 2009.</cite>
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+lda.collapsed.gibbs.sampler">lda.collapsed.gibbs.sampler</a></code> for a description of the
input formats and similar models.
</p>
<p><code><a href="#topic+rtm.collapsed.gibbs.sampler">rtm.collapsed.gibbs.sampler</a></code> is a different kind of
model for document networks.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## See demo.

## Not run: demo(nubbi)
</code></pre>

<hr>
<h2 id='poliblog'>
A collection of political blogs with ratings.
</h2><span id='topic+poliblog'></span><span id='topic+poliblog.documents'></span><span id='topic+poliblog.vocab'></span><span id='topic+poliblog.ratings'></span>

<h3>Description</h3>

<p>A collection of 773 political blogs in LDA format with
conservative/liberal ratings.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(poliblog.documents)
data(poliblog.vocab)
data(poliblog.ratings)
</code></pre>


<h3>Format</h3>

<p><code>poliblog.documents</code> and <code>poliblog.vocab</code>
comprise a corpus of 773 political blogs conforming to the LDA format.
</p>
<p><code>poliblog.ratings</code> is a numeric vector of length 773 which gives
a rating of liberal (-100) or conservative (100) to each document in
the corpus.
</p>


<h3>Source</h3>

<p><cite>Blei, David M. and McAuliffe, John.  Supervised topic models.
Advances in Neural Information Processing Systems, 2008.</cite>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lda.collapsed.gibbs.sampler">lda.collapsed.gibbs.sampler</a></code> for the format of the
corpus.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(poliblog.documents)
data(poliblog.vocab)
data(poliblog.ratings)
</code></pre>

<hr>
<h2 id='predictive.distribution'>
Compute predictive distributions for fitted LDA-type models.
</h2><span id='topic+predictive.distribution'></span>

<h3>Description</h3>

<p>This function takes a fitted LDA-type model and computes a predictive
distribution for new words in a document.  This is useful for making
predictions about held-out words.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predictive.distribution(document_sums, topics, alpha, eta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predictive.distribution_+3A_document_sums">document_sums</code></td>
<td>

<p>A <code class="reqn">K \times D</code> matrix where each entry is a numeric proportional
to the probability of seeing a topic (row) conditioned on document
(column) (this entry is sometimes denoted <code class="reqn">\theta_{d,k}</code> in the
literature, see details).  Either the <var>document_sums</var> field or
the <var>document_expects</var> field from the output of
<code><a href="#topic+lda.collapsed.gibbs.sampler">lda.collapsed.gibbs.sampler</a></code> can be used.
</p>
</td></tr>
<tr><td><code id="predictive.distribution_+3A_topics">topics</code></td>
<td>

<p>A <code class="reqn">K \times V</code> matrix where each entry is a numeric proportional
to the probability of seeing the word (column) conditioned on topic
(row) (this entry is sometimes denoted <code class="reqn">\beta_{w,k}</code> in the
literature, see details).  The column names should correspond to the
words in the vocabulary.  The <var>topics</var> field from the output of
<code><a href="#topic+lda.collapsed.gibbs.sampler">lda.collapsed.gibbs.sampler</a></code> can be used.
</p>
</td></tr>
<tr><td><code id="predictive.distribution_+3A_alpha">alpha</code></td>
<td>

<p>The scalar value of the Dirichlet hyperparameter for
topic proportions.  See references for details.
</p>
</td></tr>
<tr><td><code id="predictive.distribution_+3A_eta">eta</code></td>
<td>

<p>The scalar value of the Dirichlet hyperparamater for topic
multinomials.  See references for details.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The formula used to compute predictive probability is <code class="reqn">p_d(w) =
    \sum_k (\theta_{d, k} + \alpha) (\beta_{w, k} + \eta)</code>.
</p>


<h3>Value</h3>

<p>A <code class="reqn">V \times D</code> matrix of the probability of seeing a word (row) in
a document (column).   The row names of the matrix are set to the
column names of <var>topics</var>.
</p>


<h3>Author(s)</h3>

<p>Jonathan Chang (<a href="mailto:slycoder@gmail.com">slycoder@gmail.com</a>)  
</p>


<h3>References</h3>

<p><cite>Blei, David M. and Ng, Andrew and Jordan, Michael. Latent Dirichlet allocation. Journal of Machine Learning Research, 2003.</cite>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lda.collapsed.gibbs.sampler">lda.collapsed.gibbs.sampler</a></code> for the format of
<var>topics</var> and <var>document_sums</var> and details of the model.
</p>
<p><code><a href="#topic+top.topic.words">top.topic.words</a></code> demonstrates another use for a fitted
topic matrix.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Fit a model (from demo(lda)).
data(cora.documents)
data(cora.vocab)

K &lt;- 10 ## Num clusters
result &lt;- lda.collapsed.gibbs.sampler(cora.documents,
                                      K,  ## Num clusters
                                      cora.vocab,
                                      25,  ## Num iterations
                                      0.1,
                                      0.1) 

## Predict new words for the first two documents
predictions &lt;-  predictive.distribution(result$document_sums[,1:2],
                                        result$topics,
                                        0.1, 0.1)

## Use top.topic.words to show the top 5 predictions in each document.
top.topic.words(t(predictions), 5)

##      [,1]         [,2]      
## [1,] "learning"   "learning"
## [2,] "algorithm"  "paper"   
## [3,] "model"      "problem" 
## [4,] "paper"      "results" 
## [5,] "algorithms" "system"  
</code></pre>

<hr>
<h2 id='predictive.link.probability'>
Use the RTM to predict whether a link exists between two documents.
</h2><span id='topic+predictive.link.probability'></span>

<h3>Description</h3>

<p>This function takes a fitted LDA-type model (e.g., LDA or RTM) and
makes predictions about the likelihood of a link existing between
pairs of documents.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predictive.link.probability(edgelist, document_sums, alpha, beta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predictive.link.probability_+3A_edgelist">edgelist</code></td>
<td>

<p>A two-column integer matrix where each row represents an edge on
which to make a prediction.  An edge is expressed as a pair of
integer indices (1-indexed) into the columns (i.e., documents) of
<var>document_sums</var> (see below). 
</p>
</td></tr>
<tr><td><code id="predictive.link.probability_+3A_document_sums">document_sums</code></td>
<td>

<p>A <code class="reqn">K \times D</code> matrix where each entry is a numeric proportional
to the probability of seeing a topic (row) conditioned on document
(column) (this entry is sometimes denoted <code class="reqn">\theta_{d,k}</code> in the
literature, see details).  The <var>document_sums</var> field or
the <var>document_expects</var> field from the output of
<code><a href="#topic+lda.collapsed.gibbs.sampler">lda.collapsed.gibbs.sampler</a></code> and
<code><a href="#topic+rtm.collapsed.gibbs.sampler">rtm.collapsed.gibbs.sampler</a></code> can be used.    
</p>
</td></tr>
<tr><td><code id="predictive.link.probability_+3A_alpha">alpha</code></td>
<td>

<p>The value of the Dirichlet hyperparamter generating the distribution
over <var>document_sums</var>.  This, in effect, smooths the similarity
between documents. 
</p>
</td></tr>
<tr><td><code id="predictive.link.probability_+3A_beta">beta</code></td>
<td>

<p>A numeric vector of regression weights which is used to determine
the similarity between two vectors (see details). Arguments will be
recycled to create a vector of length <code>dim(document_sums)[1]</code>. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Whether or not a link exists between two documents <code class="reqn">i</code> and <code class="reqn">j</code>
is a function of the weighted inner product of the
<code>document_sums[,i]</code> and <code>document_sums[,j]</code>.   After
normalizing <code>document_sums</code> column-wise, this inner
product is weighted by <var>beta</var>.
</p>
<p>This quantity is then passed to a
link probability function.  Like
<code><a href="#topic+rtm.collapsed.gibbs.sampler">rtm.collapsed.gibbs.sampler</a></code> in this package, only the
exponential link probability function is supported.  Note that
quantities are automatically scaled to be between 0 and 1.
</p>


<h3>Value</h3>

<p>A numeric vector of length <code>dim(edgelist)[1]</code>, representing the
probability of a link existing between each pair of documents given in
the edge list.  
</p>


<h3>Author(s)</h3>

<p>Jonathan Chang (<a href="mailto:slycoder@gmail.com">slycoder@gmail.com</a>)  
</p>


<h3>References</h3>

<p><cite>Chang, Jonathan and Blei, David M.
Relational Topic Models for Document Networks.  Artificial
intelligence and statistics. 2009.</cite>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rtm.collapsed.gibbs.sampler">rtm.collapsed.gibbs.sampler</a></code> for the format of
<var>document_sums</var>.  <code><a href="#topic+links.as.edgelist">links.as.edgelist</a></code> produces values
for <var>edgelist</var>.  <code><a href="#topic+predictive.distribution">predictive.distribution</a></code> makes
predictions about document content instead.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## See demo.

## Not run: demo(rtm)
</code></pre>

<hr>
<h2 id='read.documents'>
Read LDA-formatted Document and Vocabulary Files
</h2><span id='topic+read.documents'></span><span id='topic+read.vocab'></span>

<h3>Description</h3>

<p>These functions read in the document and vocabulary files associated
with a corpus.  The format of the files is the same as that used by
LDA-C (see below for details).  The return value of these functions
can be used by the inference procedures defined in the <span class="pkg">lda</span> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read.documents(filename = "mult.dat")

read.vocab(filename = "vocab.dat")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="read.documents_+3A_filename">filename</code></td>
<td>

<p>A length-1 character vector specifying the path to the
document/vocabulary file.  These are set to &lsquo;<span class="file">mult.dat</span>&rsquo; and
&lsquo;<span class="file">vocab.dat</span>&rsquo; by default.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The details of the format are also described in the readme for LDA-C.
</p>
<p>The format of the documents file is appropriate for typical text data
as it sparsely encodes observed features.  A single file encodes a
<dfn>corpus</dfn> (a collection of documents).  Each line of the file
encodes a single <dfn>document</dfn> (a feature vector).
</p>
<p>The line encoding a document begins with an integer followed by a
number of <dfn>feature-count pairs</dfn>, all separated by spaces.  A
feature-count pair consists of two integers separated by a colon.  The
first integer indicates the feature (note that this is zero-indexed!)
and the second integer indicates the count (i.e., value) of that
feature.  The initial integer of a line indicates how many
feature-count pairs are to be expected on that line.
</p>
<p>Note that we permit a feature to appear more than once on a line, in
which case the value for that feature will be the sum of all instances
(the behavior for such files is undefined for LDA-C).  For example, a
line reading &lsquo;<span class="samp">&#8288;4 7:1 0:2 7:3 1:1&#8288;</span>&rsquo;
will yield a document with feature 0 occurring twice, feature 1
occurring once, and feature 7 occurring four times, with all other
features occurring zero times.
</p>
<p>The format of the vocabulary is a set of newline separated strings
corresponding to features.  That is, the first line of the vocabulary
file will correspond to the label for feature 0, the second for
feature 1, etc.
</p>


<h3>Value</h3>

<p><code>read.documents</code> returns a list of matrices suitable as input for
the inference routines in <span class="pkg">lda</span>.  See
<code><a href="#topic+lda.collapsed.gibbs.sampler">lda.collapsed.gibbs.sampler</a></code> for details.
</p>
<p><code>read.vocab</code> returns a character vector of strings corresponding to
features. 
</p>


<h3>Author(s)</h3>

<p>Jonathan Chang (<a href="mailto:slycoder@gmail.com">slycoder@gmail.com</a>)
</p>


<h3>References</h3>

<p><cite>Blei, David M.  Latent Dirichlet Allocation in C. <a href="http://www.cs.princeton.edu/~blei/lda-c/index.html">http://www.cs.princeton.edu/~blei/lda-c/index.html</a></cite>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lda.collapsed.gibbs.sampler">lda.collapsed.gibbs.sampler</a></code> for the format of
the return value of <code>read.documents</code>.
</p>
<p><code><a href="#topic+lexicalize">lexicalize</a></code> to generate the same output from raw text data.
</p>
<p><code><a href="#topic+word.counts">word.counts</a></code> to compute statistics associated with a
corpus.
</p>
<p><code><a href="#topic+concatenate.documents">concatenate.documents</a></code> for operations on a collection of documents.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Read files using default values.
## Not run: setwd("corpus directory")
## Not run: documents &lt;- read.documents()
## Not run: vocab &lt;- read.vocab()

## Read files from another location.
## Not run: documents &lt;- read.documents("corpus directory/features")
## Not run: vocab &lt;- read.vocab("corpus directory/labels")
</code></pre>

<hr>
<h2 id='rtm.collapsed.gibbs.sampler'>
Collapsed Gibbs Sampling for the Relational Topic Model (RTM).
</h2><span id='topic+rtm.collapsed.gibbs.sampler'></span><span id='topic+rtm.em'></span>

<h3>Description</h3>

<p>Fit a generative topic model which accounts for both the words which
occur in a collection of documents as well as the links between the documents.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rtm.collapsed.gibbs.sampler(documents, links, K, vocab, num.iterations,
  alpha, eta, beta, trace = 0L, test.start = length(documents) + 1L)
rtm.em(documents, links, K, vocab, num.e.iterations, num.m.iterations,
        alpha, eta,
        lambda = sum(sapply(links, length))/(length(links) * (length(links) -1)/2),
  initial.beta = rep(3, K), trace = 0L,
  test.start = length(documents) + 1L, tempering = 0.0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rtm.collapsed.gibbs.sampler_+3A_documents">documents</code></td>
<td>

<p>A collection of documents in LDA format.  See
<code><a href="#topic+lda.collapsed.gibbs.sampler">lda.collapsed.gibbs.sampler</a></code> for details.
</p>
</td></tr>
<tr><td><code id="rtm.collapsed.gibbs.sampler_+3A_links">links</code></td>
<td>

<p>A list representing the connections between the documents.  This
list should be of the same length as the <var>documents</var>.  Each
element, <code>links[[i]]</code>, is an integer vector expressing connections
between document <var>i</var> and the 0-indexed documents pointed to by the
elements of the vector.
</p>
</td></tr>
<tr><td><code id="rtm.collapsed.gibbs.sampler_+3A_k">K</code></td>
<td>

<p>A scalar integer indicating the number of latent topics for the model.
</p>
</td></tr>
<tr><td><code id="rtm.collapsed.gibbs.sampler_+3A_vocab">vocab</code></td>
<td>

<p>A character vector specifying the vocabulary words associated with
the word indices used in <var>documents</var>.
</p>
</td></tr>
<tr><td><code id="rtm.collapsed.gibbs.sampler_+3A_num.iterations">num.iterations</code></td>
<td>

<p>The number of sweeps of Gibbs sampling over the entire corpus to make.
</p>
</td></tr>
<tr><td><code id="rtm.collapsed.gibbs.sampler_+3A_num.e.iterations">num.e.iterations</code></td>
<td>

<p>For <code>rtm.em</code>, the number of iterations in each Gibbs sampling E-step.
</p>
</td></tr>
<tr><td><code id="rtm.collapsed.gibbs.sampler_+3A_num.m.iterations">num.m.iterations</code></td>
<td>

<p>For <code>rtm.em</code>, the number of M-step iterations.
</p>
</td></tr>
<tr><td><code id="rtm.collapsed.gibbs.sampler_+3A_alpha">alpha</code></td>
<td>

<p>The scalar value of the Dirichlet hyperparameter for
topic proportions.
</p>
</td></tr>
<tr><td><code id="rtm.collapsed.gibbs.sampler_+3A_eta">eta</code></td>
<td>

<p>The scalar value of the Dirichlet hyperparamater for topic
multinomials.
</p>
</td></tr>
<tr><td><code id="rtm.collapsed.gibbs.sampler_+3A_beta">beta</code></td>
<td>

<p>A length <code>K</code> numeric of regression coefficients expressing the
relationship between each topic and the probability of link.
</p>
</td></tr>
<tr><td><code id="rtm.collapsed.gibbs.sampler_+3A_lambda">lambda</code></td>
<td>

<p>For <code>rtm.em</code>, the regularization parameter used when estimating
beta.  <var>lambda</var> expresses the number of non-links to simulate
among all
possible connections between documents.
</p>
</td></tr>
<tr><td><code id="rtm.collapsed.gibbs.sampler_+3A_initial.beta">initial.beta</code></td>
<td>

<p>For <code>rtm.em</code>, an initial value of <code>beta</code> at which to start
the EM process.
</p>
</td></tr>
<tr><td><code id="rtm.collapsed.gibbs.sampler_+3A_trace">trace</code></td>
<td>

<p>When <code>trace</code> is greater than zero, diagnostic messages will be
output.  Larger values of <code>trace</code> imply more messages.
</p>
</td></tr>
<tr><td><code id="rtm.collapsed.gibbs.sampler_+3A_test.start">test.start</code></td>
<td>

<p>Internal use only.
</p>
</td></tr>
<tr><td><code id="rtm.collapsed.gibbs.sampler_+3A_tempering">tempering</code></td>
<td>

<p>A numeric between 0 and 1 indicating how newly computed parameters should
be averaged with the previous iterations parameters.  By default, the new
values are used directly and the old value discarded.  When set to 1, the
new values are ignored and the initial values retained indefinitely.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Relational Topic Model uses LDA to model the content of
documents but adds connections between documents as dependent on the
similarity of the distribution of latent topic assignments.  (See
reference for details).
</p>
<p>Only the exponential link probability function
is implemented here.  Note that the collapsed Gibbs sampler is
different than the variational inference procedure proposed in the
paper and is extremely experimental.
</p>
<p><code>rtm.em</code> provides an EM-wrapper around
<code>rtm.collapsed.gibbs.sampler</code> which iteratively estimates the
regression parameters <code>beta</code>.
</p>


<h3>Value</h3>

<p>A fitted model as a list with the same components as returned by
<code><a href="#topic+lda.collapsed.gibbs.sampler">lda.collapsed.gibbs.sampler</a></code>.
</p>


<h3>Author(s)</h3>

<p>Jonathan Chang (<a href="mailto:slycoder@gmail.com">slycoder@gmail.com</a>)
</p>


<h3>References</h3>

<p><cite>Chang, Jonathan and Blei, David M.
Relational Topic Models for Document Networks.  Artificial
intelligence and statistics. 2009.</cite>
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+lda.collapsed.gibbs.sampler">lda.collapsed.gibbs.sampler</a></code> for a description of the
input formats and similar models.
</p>
<p><code><a href="#topic+nubbi.collapsed.gibbs.sampler">nubbi.collapsed.gibbs.sampler</a></code> is a different kind of
model for document networks.
</p>
<p><code><a href="#topic+predictive.link.probability">predictive.link.probability</a></code> makes predictions based on
the output of this model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## See demo.

## Not run: demo(rtm)
</code></pre>

<hr>
<h2 id='sampson'>
Sampson monk data
</h2><span id='topic+sampson'></span>

<h3>Description</h3>

<p>Various relationships between several monks at a monastery collected
over time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(sampson)</code></pre>


<h3>Format</h3>

<p><code>sampson</code> is a list whose entries are 18x18 matrices representing
the pairwise relationships between 18 monks.  The names of the monks
are given as the row/column names of each matrix.
</p>
<p>Each matrix encodes a different relationship (there are a total of 10)
described by the
corresponding name field of the list.
</p>


<h3>Source</h3>

<p><cite>F. S. Sampson.  A novitiate in a period of change: An experimental and
case study of social relationships.  PhD thesis, Cornell University. 1968.</cite>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mmsb.collapsed.gibbs.sampler">mmsb.collapsed.gibbs.sampler</a></code> is an example of a
function which can model the structure of this data set.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(sampson)
</code></pre>

<hr>
<h2 id='slda.predict'>
Predict the response variable of documents using an sLDA model.
</h2><span id='topic+slda.predict'></span><span id='topic+slda.predict.docsums'></span>

<h3>Description</h3>

<p>These functions take a fitted sLDA model and predict the value of
the response variable (or document-topic sums) for each given document.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>slda.predict(documents, topics, model, alpha, eta,
num.iterations = 100, average.iterations = 50, trace = 0L)

slda.predict.docsums(documents, topics, alpha, eta,
num.iterations = 100, average.iterations = 50, trace = 0L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="slda.predict_+3A_documents">documents</code></td>
<td>

<p>A list of document matrices comprising a corpus, in the format
described in <code><a href="#topic+lda.collapsed.gibbs.sampler">lda.collapsed.gibbs.sampler</a></code>.
</p>
</td></tr>
<tr><td><code id="slda.predict_+3A_topics">topics</code></td>
<td>

<p>A <code class="reqn">K \times V</code> matrix where each entry is an integer that is the
number of times the word (column) has been allocated to the topic
(row) (a normalised version of this is sometimes denoted
<code class="reqn">\beta_{w,k}</code> in the literature, see details).  The column names
should correspond to the words in the vocabulary.  The <var>topics</var>
field from the output of <code><a href="#topic+slda.em">slda.em</a></code> can be used.  
</p>
</td></tr>
<tr><td><code id="slda.predict_+3A_model">model</code></td>
<td>

<p>A fitted model relating a document's topic distribution to the
response variable.  The <var>model</var> field from the output of
<code><a href="#topic+slda.em">slda.em</a></code> can be used.
</p>
</td></tr>
<tr><td><code id="slda.predict_+3A_alpha">alpha</code></td>
<td>

<p>The scalar value of the Dirichlet hyperparameter for
topic proportions.  See references for details.
</p>
</td></tr>
<tr><td><code id="slda.predict_+3A_eta">eta</code></td>
<td>

<p>The scalar value of the Dirichlet hyperparamater for topic
multinomials.
</p>
</td></tr>
<tr><td><code id="slda.predict_+3A_num.iterations">num.iterations</code></td>
<td>

<p>Number of iterations of inference to perform on the documents.
</p>
</td></tr>
<tr><td><code id="slda.predict_+3A_average.iterations">average.iterations</code></td>
<td>

<p>Number of samples to average over to produce the predictions.
</p>
</td></tr>
<tr><td><code id="slda.predict_+3A_trace">trace</code></td>
<td>

<p>When <code>trace</code> is greater than zero, diagnostic messages will be
output.  Larger values of <code>trace</code> imply more messages.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Inference is first performed on the documents by using Gibbs sampling
and holding the word-topic matrix <code class="reqn">\beta_{w,k}</code> constant.  Typically
for a well-fit model only a small number of iterations are required to
obtain good fits for new documents.  These topic vectors are then
piped through <code>model</code> to yield numeric predictions associated
with each document.
</p>


<h3>Value</h3>

<p>For <code>slda.predict</code>, a numeric vector of the same length as
<code>documents</code> giving the predictions. For <code>slda.predict.docsums</code>, a
<code class="reqn">K \times N</code> matrix of document assignment counts.  
</p>


<h3>Author(s)</h3>

<p>Jonathan Chang (<a href="mailto:slycoder@gmail.com">slycoder@gmail.com</a>)
</p>


<h3>References</h3>

<p><cite>Blei, David M. and McAuliffe, John.  Supervised topic models.
Advances in Neural Information Processing Systems, 2008.</cite>
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+lda.collapsed.gibbs.sampler">lda.collapsed.gibbs.sampler</a></code> for a description of the
format of the input data, as well as more details on the model.
</p>
<p>See <code><a href="#topic+predictive.distribution">predictive.distribution</a></code> if you want to make
predictions about the contents of the documents instead of the
response variables.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## The sLDA demo shows an example usage of this function.
## Not run: demo(slda)
</code></pre>

<hr>
<h2 id='top.topic.words'>
Get the Top Words and Documents in Each Topic
</h2><span id='topic+top.topic.words'></span><span id='topic+top.topic.documents'></span>

<h3>Description</h3>

<p>This function takes a model fitted using
<code><a href="#topic+lda.collapsed.gibbs.sampler">lda.collapsed.gibbs.sampler</a></code> and returns a matrix of the
top words in each topic.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>top.topic.words(topics, num.words = 20, by.score = FALSE)
top.topic.documents(document_sums, num.documents = 20, alpha = 0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="top.topic.words_+3A_topics">topics</code></td>
<td>

<p>For <code>top.topic.words</code>, a <code class="reqn">K \times V</code> matrix where each entry is a numeric proportional
to the probability of seeing the word (column) conditioned on topic
(row) (this entry is sometimes denoted <code class="reqn">\beta_{w,k}</code> in the
literature, see details).  The column names should correspond to the words in the
vocabulary.  The <var>topics</var> field from the output of
<code><a href="#topic+lda.collapsed.gibbs.sampler">lda.collapsed.gibbs.sampler</a></code> can be used.
</p>
</td></tr>
<tr><td><code id="top.topic.words_+3A_num.words">num.words</code></td>
<td>

<p>For <code>top.topic.words</code>, the number of top words to return for each topic.
</p>
</td></tr>
<tr><td><code id="top.topic.words_+3A_document_sums">document_sums</code></td>
<td>

<p>For <code>top.topic.documents</code>, a <code class="reqn">K \times D</code> matrix where each entry is a numeric proportional
to the probability of seeing a topic (row) conditioned on the
document (column) (this entry is sometimes denoted <code class="reqn">\theta_{d,k}</code> in the
literature, see details).  The <var>document_sums</var> field from the output of
<code><a href="#topic+lda.collapsed.gibbs.sampler">lda.collapsed.gibbs.sampler</a></code> can be used.
</p>
</td></tr>
<tr><td><code id="top.topic.words_+3A_num.documents">num.documents</code></td>
<td>

<p>For <code>top.topic.documents</code>, the number of top documents to return for each topic.
</p>
</td></tr>
<tr><td><code id="top.topic.words_+3A_by.score">by.score</code></td>
<td>

<p>If <var>by.score</var> is set to <code>FALSE</code> (default), then words in
each topic will
be ranked according to probability mass for each word <code class="reqn">\beta_{w,
	k}</code>.  If <var>by.score</var> is <code>TRUE</code>, then words will be
ranked according to a score defined by <code class="reqn">\beta_{w, k} (\log
      \beta_{w,k} - 1 / K \sum_{k'} \log \beta_{w,k'})</code>.  
</p>
</td></tr>
<tr><td><code id="top.topic.words_+3A_alpha">alpha</code></td>
<td>

</td></tr>
</table>


<h3>Value</h3>

<p>For <code>top.topic.words</code>, a <code class="reqn">num.words \times K</code> character matrix where each column contains
the top words for that topic.  
</p>
<p>For <code>top.topic.documents</code>, a <code class="reqn">num.documents \times K</code> integer matrix where each column contains
the top documents for that topic.  The entries in the matrix are 
column-indexed references into <code>document_sums</code>.
</p>


<h3>Author(s)</h3>

<p>Jonathan Chang (<a href="mailto:slycoder@gmail.com">slycoder@gmail.com</a>)  
</p>


<h3>References</h3>

<p><cite>Blei, David M. and Ng, Andrew and Jordan, Michael. Latent Dirichlet allocation. Journal of Machine Learning Research, 2003.</cite>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lda.collapsed.gibbs.sampler">lda.collapsed.gibbs.sampler</a></code> for the format of <var>topics</var>.
</p>
<p><code><a href="#topic+predictive.distribution">predictive.distribution</a></code> demonstrates another use for a fitted
topic matrix.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## From demo(lda).

data(cora.documents)
data(cora.vocab)

K &lt;- 10 ## Num clusters
result &lt;- lda.collapsed.gibbs.sampler(cora.documents,
                                      K,  ## Num clusters
                                      cora.vocab,
                                      25,  ## Num iterations
                                      0.1,
                                      0.1) 

## Get the top words in the cluster
top.words &lt;- top.topic.words(result$topics, 5, by.score=TRUE)

## top.words:
##      [,1]             [,2]        [,3]       [,4]            [,5]      
## [1,] "decision"       "network"   "planning" "learning"      "design"  
## [2,] "learning"       "time"      "visual"   "networks"      "logic"   
## [3,] "tree"           "networks"  "model"    "neural"        "search"  
## [4,] "trees"          "algorithm" "memory"   "system"        "learning"
## [5,] "classification" "data"      "system"   "reinforcement" "systems" 
##      [,6]         [,7]       [,8]           [,9]           [,10]      
## [1,] "learning"   "models"   "belief"       "genetic"      "research" 
## [2,] "search"     "networks" "model"        "search"       "reasoning"
## [3,] "crossover"  "bayesian" "theory"       "optimization" "grant"    
## [4,] "algorithm"  "data"     "distribution" "evolutionary" "science"  
## [5,] "complexity" "hidden"   "markov"       "function"     "supported"
</code></pre>

<hr>
<h2 id='word.counts'>
Compute Summary Statistics of a Corpus
</h2><span id='topic+word.counts'></span><span id='topic+document.lengths'></span>

<h3>Description</h3>

<p>These functions compute summary statistics of a corpus.
<code>word.counts</code> computes the word counts for a set of documents,
while <code>documents.length</code> computes the length of the documents in
a corpus.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>word.counts(docs, vocab = NULL)

document.lengths(docs)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="word.counts_+3A_docs">docs</code></td>
<td>

<p>A list of matrices specifying the corpus.  See
<code><a href="#topic+lda.collapsed.gibbs.sampler">lda.collapsed.gibbs.sampler</a></code> for details on the
format of this variable.
</p>
</td></tr>
<tr><td><code id="word.counts_+3A_vocab">vocab</code></td>
<td>

<p>An optional character vector specifying the levels (i.e., labels) of
the vocabulary words.  If unspecified (or <code>NULL</code>), the levels
will be automatically inferred from the corpus.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>word.counts</code> returns an object of class &lsquo;<span class="samp">&#8288;table&#8288;</span>&rsquo; which
contains counts for the number of times each word appears in the input
corpus.  If <var>vocab</var> is specified, then the levels of the table
will be set to <var>vocab</var>.  Otherwise, the levels are automatically
inferred from the corpus (typically integers <var>0:(V-1)</var>, where
<var>V</var> indicates the number of unique words in the corpus).
</p>
<p><code>documents.length</code> returns a integer vector of length
<code>length(docs)</code>, each entry of which corresponds to the
<dfn>length</dfn> (sum of the counts of all features) of each document in
the corpus.
</p>


<h3>Author(s)</h3>

<p>Jonathan Chang (<a href="mailto:slycoder@gmail.com">slycoder@gmail.com</a>)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lda.collapsed.gibbs.sampler">lda.collapsed.gibbs.sampler</a></code> for the input format of
these functions.
</p>
<p><code><a href="#topic+read.documents">read.documents</a></code> and <code><a href="#topic+lexicalize">lexicalize</a></code> for ways of
generating the input to these functions.
</p>
<p><code><a href="#topic+concatenate.documents">concatenate.documents</a></code> for operations on a corpus.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Load the cora dataset.
data(cora.vocab)
data(cora.documents)

## Compute word counts using raw feature indices.
wc &lt;- word.counts(cora.documents)
head(wc)
##   0   1   2   3   4   5 
## 136 876  14 111  19  29 

## Recompute them using the levels defined by the vocab file.
wc &lt;- word.counts(cora.documents, cora.vocab)
head(wc)
##   computer  algorithms discovering    patterns      groups     protein 
##        136         876          14         111          19          29 

head(document.lengths(cora.documents))
## [1] 64 39 76 84 52 24
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
