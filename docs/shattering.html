<!DOCTYPE html><html><head><title>Help for package shattering</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {shattering}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#apply_classifier'><p>Apply a classifier induced with function build_classifier</p></a></li>
<li><a href='#build_classifier'><p>Produce a set of SVM classifiers</p></a></li>
<li><a href='#complexity_analysis'><p>Produce a PDF report analyzing the lower and upper shattering coefficient functions</p></a></li>
<li><a href='#compress_space'><p>Function to compress the space based on the equivalence relations.</p></a></li>
<li><a href='#equivalence_relation'><p>Function to compute equivalence relations among input space points.</p></a></li>
<li><a href='#estimate_number_hyperplanes'><p>Function to estimate the number of hyperplanes required to classify such a data sample.</p></a></li>
<li><a href='#number_regions'><p>Computes the maximal number of space regions</p></a></li>
<li><a href='#shattering'><p>shattering: A package to estimate the shattering coefficient for labeled data samples.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Estimate the Shattering Coefficient for a Particular Dataset</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.7</td>
</tr>
<tr>
<td>Description:</td>
<td>The Statistical Learning Theory (SLT) provides the theoretical background to ensure that a supervised algorithm generalizes the mapping f:X -&gt; Y given f is selected from its search space bias F. This formal result depends on the Shattering coefficient function N(F,2n) to upper bound the empirical risk minimization principle, from which one can estimate the necessary training sample size to ensure the probabilistic learning convergence and, most importantly, the characterization of the capacity of F, including its under and overfitting abilities while addressing specific target problems. In this context, we propose a new approach to estimate the maximal number of hyperplanes required to shatter a given sample, i.e., to separate every pair of points from one another, based on the recent contributions by Har-Peled and Jones in the dataset partitioning scenario, and use such foundation to analytically compute the Shattering coefficient function for both binary and multi-class problems. As main contributions, one can use our approach to study the complexity of the search space bias F, estimate training sample sizes, and parametrize the number of hyperplanes a learning algorithm needs to address some supervised task, what is specially appealing to deep neural networks. Reference: de Mello, R.F. (2019) "On the Shattering Coefficient of Supervised Learning Algorithms" &lt;<a href="https://doi.org/10.48550/arXiv.1911.05461">doi:10.48550/arXiv.1911.05461</a>&gt;; de Mello, R.F., Ponti, M.A. (2018, ISBN: 978-3319949888) "Machine Learning: A Practical Approach on the Statistical Learning Theory".</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1.9001</td>
</tr>
<tr>
<td>Imports:</td>
<td>FNN, pdist, slam, grDevices, Ryacas, rmarkdown, pracma, e1071,
graphics, NMF, stats</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-08-21 13:37:32 UTC; mello</td>
</tr>
<tr>
<td>Author:</td>
<td>Rodrigo F. de Mello
    <a href="https://orcid.org/0000-0002-0435-3992"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Rodrigo F. de Mello &lt;mellorf@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-08-21 13:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='apply_classifier'>Apply a classifier induced with function build_classifier</h2><span id='topic+apply_classifier'></span>

<h3>Description</h3>

<p>This function applies the set of SVM classifiers to perform the supervised learning task
based on the topological data analysis
</p>


<h3>Usage</h3>

<pre><code class='language-R'>apply_classifier(model, X, only.best.classifiers = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="apply_classifier_+3A_model">model</code></td>
<td>
<p>model built using function build_classifier</p>
</td></tr>
<tr><td><code id="apply_classifier_+3A_x">X</code></td>
<td>
<p>matrix defining the input space of your test set</p>
</td></tr>
<tr><td><code id="apply_classifier_+3A_only.best.classifiers">only.best.classifiers</code></td>
<td>
<p>if TRUE, only the most performing classification functions will be considered</p>
</td></tr>
</table>


<h3>Value</h3>

<p>prediction results
</p>


<h3>References</h3>

<p>de Mello, R.F. (2019) &quot;On the Shattering Coefficient of Supervised Learning Algorithms&quot; arXiv:<a href="https://arxiv.org/abs/1911.05461">https://arxiv.org/abs/1911.05461</a>
</p>
<p>de Mello, R.F., Ponti, M.A. (2018, ISBN: 978-3319949888) &quot;Machine Learning: A Practical Approach on the Statistical Learning Theory&quot;
</p>

<hr>
<h2 id='build_classifier'>Produce a set of SVM classifiers</h2><span id='topic+build_classifier'></span>

<h3>Description</h3>

<p>This function outputs a set of SVM classifiers to perform the supervised learning task
based on the topological data analysis
</p>


<h3>Usage</h3>

<pre><code class='language-R'>build_classifier(
  X,
  Y,
  train.size = 0.7,
  quantile.percentage = 1,
  min.points = 3,
  gamma.length = 50,
  cost = 10000,
  weights = c(0.25, 0.75),
  best.stdev.purity = 0
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="build_classifier_+3A_x">X</code></td>
<td>
<p>matrix defining the input space of your dataset</p>
</td></tr>
<tr><td><code id="build_classifier_+3A_y">Y</code></td>
<td>
<p>numerical vector defining the output space (labels/classes) of your dataset</p>
</td></tr>
<tr><td><code id="build_classifier_+3A_train.size">train.size</code></td>
<td>
<p>fraction of examples used for training</p>
</td></tr>
<tr><td><code id="build_classifier_+3A_quantile.percentage">quantile.percentage</code></td>
<td>
<p>real number to define the quantile of distances to be considered (e.g. 0.1 means 10%)</p>
</td></tr>
<tr><td><code id="build_classifier_+3A_min.points">min.points</code></td>
<td>
<p>minimal number of examples per classification region of the input space</p>
</td></tr>
<tr><td><code id="build_classifier_+3A_gamma.length">gamma.length</code></td>
<td>
<p>number of possible gamma parameters to test the radial kernel for SVM</p>
</td></tr>
<tr><td><code id="build_classifier_+3A_cost">cost</code></td>
<td>
<p>the cost for the SVM optimization</p>
</td></tr>
<tr><td><code id="build_classifier_+3A_weights">weights</code></td>
<td>
<p>weights to be used in our SVM optimization</p>
</td></tr>
<tr><td><code id="build_classifier_+3A_best.stdev.purity">best.stdev.purity</code></td>
<td>
<p>the stdev to compute data purity</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of classifiers composing the final classification model
</p>


<h3>References</h3>

<p>de Mello, R.F. (2019) &quot;On the Shattering Coefficient of Supervised Learning Algorithms&quot; arXiv:<a href="https://arxiv.org/abs/1911.05461">https://arxiv.org/abs/1911.05461</a>
</p>
<p>de Mello, R.F., Ponti, M.A. (2018, ISBN: 978-3319949888) &quot;Machine Learning: A Practical Approach on the Statistical Learning Theory&quot;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# require(NMF)
# 
# X = cbind(rnorm(mean=-1, sd=1, n=200), rnorm(mean=-1, sd=1, n=200))
# X = rbind(X, cbind(rnorm(mean=1, sd=1, n=200), rnorm(mean=1, sd=1, n=200)))
# Y = c(rep(-1,200), rep(+1,200))
# plot(X, col=Y+2, pch=20, cex=3, cex.axis=2)
# 
# model = build_classifier(X, Y, train.size=0.5, quantile.percentage=1, 
#		min.points=10, gamma.length=15, cost=10000)
# result = apply_classifier(model, X)
# points(X, col=as.numeric(result$classification.ensembled)+2, pch=20, cex=1.5)
# 
# x = seq(min(X), max(X), length=100)
# z = outer(x, x, function(x,y) { 
#	apply_classifier(model, as.matrix(cbind(x,y)))$classification.ensembled } )
# filled.contour(x,x,z)
# 
# x = seq(min(X), max(X), length=100)
# z = outer(x, x, function(x,y) { 
#	apply_classifier(model, as.matrix(cbind(x,y)), 
#		only.best.classifiers=TRUE)$classification.ensembled } )
# locator(1)
# filled.contour(x,x,z)
</code></pre>

<hr>
<h2 id='complexity_analysis'>Produce a PDF report analyzing the lower and upper shattering coefficient functions</h2><span id='topic+complexity_analysis'></span>

<h3>Description</h3>

<p>Full analysis on the lower and upper shattering coefficient functions for a given supervised dataset
</p>


<h3>Usage</h3>

<pre><code class='language-R'>complexity_analysis(
  X = NULL,
  Y = NULL,
  my.delta = 0.05,
  my.epsilon = 0.05,
  directory = tempdir(),
  file = "myreport",
  length = 10,
  quantile.percentage = 0.5,
  epsilon = 1e-07
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="complexity_analysis_+3A_x">X</code></td>
<td>
<p>matrix defining the input space of your dataset</p>
</td></tr>
<tr><td><code id="complexity_analysis_+3A_y">Y</code></td>
<td>
<p>numerical vector defining the output space (labels/classes) of your dataset</p>
</td></tr>
<tr><td><code id="complexity_analysis_+3A_my.delta">my.delta</code></td>
<td>
<p>upper bound for the probability of the empirical risk minimization principle (in range (0,1))</p>
</td></tr>
<tr><td><code id="complexity_analysis_+3A_my.epsilon">my.epsilon</code></td>
<td>
<p>acceptable divergence between the empirical and (expected) risks (in range (0,1))</p>
</td></tr>
<tr><td><code id="complexity_analysis_+3A_directory">directory</code></td>
<td>
<p>directory used to generate the report for your dataset</p>
</td></tr>
<tr><td><code id="complexity_analysis_+3A_file">file</code></td>
<td>
<p>name of the PDF file to be generated (without extension)</p>
</td></tr>
<tr><td><code id="complexity_analysis_+3A_length">length</code></td>
<td>
<p>number of points to divide the sample while computing the shattering coefficient</p>
</td></tr>
<tr><td><code id="complexity_analysis_+3A_quantile.percentage">quantile.percentage</code></td>
<td>
<p>real number to define the quantile of distances to be considered (e.g. 0.1 means 10%)</p>
</td></tr>
<tr><td><code id="complexity_analysis_+3A_epsilon">epsilon</code></td>
<td>
<p>a real threshold to be removed from distances in order to measure the open balls in the underlying topology</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list including the number of hyperplanes and the shattering coefficient function. A report is generated in the user-defined directory.
</p>


<h3>References</h3>

<p>de Mello, R.F. (2019) &quot;On the Shattering Coefficient of Supervised Learning Algorithms&quot; arXiv:<a href="https://arxiv.org/abs/1911.05461">https://arxiv.org/abs/1911.05461</a>
</p>
<p>de Mello, R.F., Ponti, M.A. (2018, ISBN: 978-3319949888) &quot;Machine Learning: A Practical Approach on the Statistical Learning Theory&quot;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Analyzing the complexity of the shattering coefficients functions 
# 	(lower and upper bounds) for the Iris dataset
# require(datasets)
# complexity_analysis(X=as.matrix(iris[,1:4]), Y=as.numeric(iris[,5]))
</code></pre>

<hr>
<h2 id='compress_space'>Function to compress the space based on the equivalence relations.</h2><span id='topic+compress_space'></span>

<h3>Description</h3>

<p>This function compresses the input space according to the equivalence relations, i.e., it compresses whenever an example has other elements inside its open ball but having the same class label as the ball-centered instance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compress_space(M, Y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compress_space_+3A_m">M</code></td>
<td>
<p>sparse matrix representing all equivalence relations</p>
</td></tr>
<tr><td><code id="compress_space_+3A_y">Y</code></td>
<td>
<p>numerical vector indentifying the output space of variables</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing sparse vectors (from package slam) identifying the equivalence relations
</p>

<hr>
<h2 id='equivalence_relation'>Function to compute equivalence relations among input space points.</h2><span id='topic+equivalence_relation'></span>

<h3>Description</h3>

<p>This function computes the greatest as possible open ball connecting a given input example to every other under the same class label, thus homogeneizing space regions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>equivalence_relation(
  X,
  Y,
  quantile.percentage = 1,
  epsilon = 0.001,
  chunk = 250
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="equivalence_relation_+3A_x">X</code></td>
<td>
<p>matrix indentifying the input space of variables</p>
</td></tr>
<tr><td><code id="equivalence_relation_+3A_y">Y</code></td>
<td>
<p>numerical vector indentifying the output space of variables</p>
</td></tr>
<tr><td><code id="equivalence_relation_+3A_quantile.percentage">quantile.percentage</code></td>
<td>
<p>real number to define the quantile of distances to be considered (e.g. 0.1 means 10%)</p>
</td></tr>
<tr><td><code id="equivalence_relation_+3A_epsilon">epsilon</code></td>
<td>
<p>a real threshold to be removed from distances in order to measure the open balls in the underlying topology</p>
</td></tr>
<tr><td><code id="equivalence_relation_+3A_chunk">chunk</code></td>
<td>
<p>number of elements to compute the Euclidean distances at once (if you set a large number, you might have memory limitations to perform the operations)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the equivalence relations in form of a list
</p>

<hr>
<h2 id='estimate_number_hyperplanes'>Function to estimate the number of hyperplanes required to classify such a data sample.</h2><span id='topic+estimate_number_hyperplanes'></span>

<h3>Description</h3>

<p>This function estimates the number of hyperplanes
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimate_number_hyperplanes(
  X,
  Y,
  length = 20,
  quantile.percentage = 0.05,
  epsilon = 1e-07
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimate_number_hyperplanes_+3A_x">X</code></td>
<td>
<p>matrix indentifying the input space of variables</p>
</td></tr>
<tr><td><code id="estimate_number_hyperplanes_+3A_y">Y</code></td>
<td>
<p>numerical vector indentifying the output space of variables</p>
</td></tr>
<tr><td><code id="estimate_number_hyperplanes_+3A_length">length</code></td>
<td>
<p>number of data points used to estimate the shattering coefficient</p>
</td></tr>
<tr><td><code id="estimate_number_hyperplanes_+3A_quantile.percentage">quantile.percentage</code></td>
<td>
<p>real number to define the quantile of distances to be considered (e.g. 0.1 means 10%)</p>
</td></tr>
<tr><td><code id="estimate_number_hyperplanes_+3A_epsilon">epsilon</code></td>
<td>
<p>a real threshold to be removed from distances in order to measure the open balls in the underlying topology</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame whose columns are: (1) the original sample size; (2) the reduced sample size after connecting homogeneous space regions; (3) the lower bound for the number of hyperplanes required to shatter the input space; and (4) the upper bound for the number of hyperplanes required to shatter the input space
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Generating some random dataset with 2 classes:
# 50 examples in class 1 and 50 in class 2 (last column)
data = cbind(rnorm(mean=1, sd=1, n=50), rnorm(mean=1, sd=1, n=50), rep(1, 50))
data = rbind(data, cbind(rnorm(mean=-1, sd=1, n=50), rnorm(mean=-1, sd=1, n=50), rep(2, 50)))

# Building up the input and output sets
X = data[,1:2]
Y = data[,3]

# Plotting our dataset using classes as colors
plot(X, col=Y, main="Original dataset", xlab="Attribute 1", ylab="Attribute 2")

# Here we estimate the number of hyperplanes required to shatter (divide) the given sample
# in all possible ways according to the organization of points in the input space
Hyperplanes = estimate_number_hyperplanes(X, Y, length=10, quantile.percentage=0.1, epsilon=1e-7)

</code></pre>

<hr>
<h2 id='number_regions'>Computes the maximal number of space regions</h2><span id='topic+number_regions'></span>

<h3>Description</h3>

<p>This function computes the maximal number of regions an R^n space can
be divided using m hyperplanes
</p>


<h3>Usage</h3>

<pre><code class='language-R'>number_regions(m, n)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="number_regions_+3A_m">m</code></td>
<td>
<p>number of hyperplanes</p>
</td></tr>
<tr><td><code id="number_regions_+3A_n">n</code></td>
<td>
<p>space dimensionality</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Maximal number of space regions
</p>


<h3>References</h3>

<p>de Mello, R.F. (2019) &quot;On the Shattering Coefficient of Supervised Learning Algorithms&quot; arXiv:<a href="https://arxiv.org/abs/1911.05461">https://arxiv.org/abs/1911.05461</a>
</p>
<p>de Mello, R.F., Ponti, M.A. (2018, ISBN: 978-3319949888) &quot;Machine Learning: A Practical Approach on the Statistical Learning Theory&quot;
</p>
<p>https://onionesquereality.wordpress.com/2012/11/23/maximum-number-of-regions-in-arrangement-of-hyperplanes/
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
number_regions(m=2, n=2)
</code></pre>

<hr>
<h2 id='shattering'>shattering: A package to estimate the shattering coefficient for labeled data samples.</h2><span id='topic+shattering'></span>

<h3>Description</h3>

<p>Description: The Statistical Learning Theory (SLT) provides the theoretical background to ensure that a supervised algorithm generalizes the mapping f:X -&gt; Y given f is selected from its search space bias F. This formal result depends on the Shattering coefficient function N(F,2n) to upper bound the empirical risk minimization principle, from which one can estimate the necessary training sample size to ensure the probabilistic learning convergence and, most importantly, the characterization of the capacity of F, including its under and overfitting abilities while addressing specific target problems. In this context, we propose a new approach to estimate the maximal number of hyperplanes required to shatter a given sample, i.e., to separate every pair of points from one another, based on the recent contributions by Har-Peled and Jones in the dataset partitioning scenario, and use such foundation to analytically compute the Shattering coefficient function for both binary and multi-class problems. As main contributions, one can use our approach to study the complexity of the search space bias F, estimate training sample sizes, and parametrize the number of hyperplanes a learning algorithm needs to address some supervised task, what is specially appealing to deep neural networks. Reference: https://arxiv.org/abs/1911.05461
</p>


<h3>References</h3>

<p>de Mello, R.F. (2019) &quot;On the Shattering Coefficient of Supervised Learning Algorithms&quot; arXiv:<a href="https://arxiv.org/abs/1911.05461">https://arxiv.org/abs/1911.05461</a>
</p>
<p>de Mello, R.F., Ponti, M.A. (2018, ISBN: 978-3319949888) &quot;Machine Learning: A Practical Approach on the Statistical Learning Theory&quot;
</p>


<h3>Shattering functions</h3>

<p>This packages comes with functions to estimate the shattering coefficient.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
