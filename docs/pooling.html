<!DOCTYPE html><html><head><title>Help for package pooling</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {pooling}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#pooling'><p>Fit Poolwise Regression Models</p></a></li>
<li><a href='#cond_logreg'><p>Conditional Logistic Regression with Measurement Error in One Covariate</p></a></li>
<li><a href='#dat_cond_logreg'><p>Dataset for Examples in cond_logreg</p></a></li>
<li><a href='#dat_p_gdfa'><p>Dataset for Examples in p_gdfa</p></a></li>
<li><a href='#dat_p_linreg_yerrors'><p>Dataset for Examples in p_linreg_yerrors</p></a></li>
<li><a href='#dat_p_ndfa'><p>Dataset for Examples in p_ndfa</p></a></li>
<li><a href='#form_pools'><p>Created a Pooled Dataset from a Subject-Specific One</p></a></li>
<li><a href='#p_dfa_xerrors'><p>Discriminant Function Approach for Estimating Odds Ratio with Normal Exposure</p>
Measured in Pools and Potentially Subject to Errors</a></li>
<li><a href='#p_dfa_xerrors2'><p>Discriminant Function Approach for Estimating Odds Ratio with Gamma Exposure</p>
Measured in Pools and Potentially Subject to Errors</a></li>
<li><a href='#p_gdfa'><p>Gamma Discriminant Function Approach for Estimating Odds Ratio with Exposure</p>
Measured in Pools and Potentially Subject to Multiplicative Lognormal Errors</a></li>
<li><a href='#p_gdfa_constant'><p>Gamma Discriminant Function Approach for Estimating Odds Ratio with Exposure</p>
Measured in Pools and Potentially Subject to Multiplicative Lognormal Errors
(Constant Odds Ratio Version)</a></li>
<li><a href='#p_gdfa_nonconstant'><p>Gamma Discriminant Function Approach for Estimating Odds Ratio with Exposure</p>
Measured in Pools and Potentially Subject to Multiplicative Lognormal Errors
(Non-constant Odds Ratio Version)</a></li>
<li><a href='#p_linreg_yerrors'><p>Linear Regression of Y vs. Covariates with Y Measured in Pools and</p>
(Potentially) Subject to Additive Normal Errors</a></li>
<li><a href='#p_logreg'><p>Poolwise Logistic Regression</p></a></li>
<li><a href='#p_logreg_xerrors'><p>Poolwise Logistic Regression with Normal Exposure Subject to Errors</p></a></li>
<li><a href='#p_logreg_xerrors2'><p>Poolwise Logistic Regression with Gamma Exposure Subject to Errors</p></a></li>
<li><a href='#p_ndfa'><p>Normal Discriminant Function Approach for Estimating Odds Ratio with Exposure</p>
Measured in Pools and Potentially Subject to Additive Normal Errors</a></li>
<li><a href='#p_ndfa_constant'><p>Normal Discriminant Function Approach for Estimating Odds Ratio with Exposure</p>
Measured in Pools and Potentially Subject to Additive Normal Errors (Constant
Odds Ratio Version)</a></li>
<li><a href='#p_ndfa_nonconstant'><p>Normal Discriminant Function Approach for Estimating Odds Ratio with Exposure</p>
Measured in Pools and Potentially Subject to Additive Normal Errors
(Non-constant Odds Ratio Version)</a></li>
<li><a href='#pdat1'><p>Dataset for Examples in p_dfa_xerrors and p_logreg_xerrors</p></a></li>
<li><a href='#pdat2'><p>Dataset for Examples in p_dfa_xerrors2 and p_logreg_xerrors2</p></a></li>
<li><a href='#plot_dfa'><p>Plot Log-OR vs. X for Normal Discriminant Function Approach</p></a></li>
<li><a href='#plot_dfa2'><p>Plot Log-OR vs. X for Gamma Discriminant Function Approach</p></a></li>
<li><a href='#plot_gdfa'><p>Plot Log-OR vs. X for Gamma Discriminant Function Approach</p></a></li>
<li><a href='#plot_ndfa'><p>Plot Log-OR vs. X for Normal Discriminant Function Approach</p></a></li>
<li><a href='#poolcost_t'><p>Visualize Total Costs for Pooling Design as a Function of Pool Size</p></a></li>
<li><a href='#poolcushion_t'><p>Visualize T-test Power for Pooling Design as Function of Processing Error</p>
Variance</a></li>
<li><a href='#poolpower_t'><p>Visualize T-test Power for Pooling Design</p></a></li>
<li><a href='#poolvar_t'><p>Visualize Ratio of Variance of Each Pooled Measurement to Variance of Each</p>
Unpooled Measurement as Function of Pool Size</a></li>
<li><a href='#simdata'><p>Dataset for a Paper Under Review</p></a></li>
<li><a href='#test_pe'><p>Test for Underestimated Processing Error Variance in Pooling Studies</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Fit Poolwise Regression Models</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.2</td>
</tr>
<tr>
<td>Date:</td>
<td>2020-02-12</td>
</tr>
<tr>
<td>Author:</td>
<td>Dane R. Van Domelen</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Dane R. Van Domelen &lt;vandomed@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Functions for calculating power and fitting regression models in studies where a biomarker is measured in "pooled" samples rather than for each individual. Approaches for handling measurement error follow the framework of Schisterman et al. (2010) &lt;<a href="https://doi.org/10.1002%2Fsim.3823">doi:10.1002/sim.3823</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.1</td>
</tr>
<tr>
<td>Imports:</td>
<td>cubature, data.table, dplyr, dvmisc, ggplot2, ggrepel,
mvtnorm, numDeriv, stats</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-02-13 01:26:52 UTC; vando</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-02-13 06:10:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='pooling'>Fit Poolwise Regression Models</h2><span id='topic+pooling'></span><span id='topic+pooling-package'></span>

<h3>Description</h3>

<p>Functions for calculating power and fitting regression models in studies
where a biomarker is measured in &quot;pooled&quot; samples rather than for each
individual. Approaches for handling measurement error follow the framework of
Schisterman et al. (2010) &lt;doi:10.1002/sim.3823&gt;.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> pooling </td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package </td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.1.2 </td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2020-02-12 </td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL-3 </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Dane R. Van Domelen <br /> <a href="mailto:vandomed@gmail.com">vandomed@gmail.com</a>
</p>


<h3>References</h3>

<p>Acknowledgment: This material is based upon work supported by the National
Science Foundation Graduate Research Fellowship under Grant No. DGE-0940903.
</p>

<hr>
<h2 id='cond_logreg'>Conditional Logistic Regression with Measurement Error in One Covariate</h2><span id='topic+cond_logreg'></span>

<h3>Description</h3>

<p>Compatible with individual or pooled measurements. Assumes a normal linear
model for exposure given other covariates, and additive normal errors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cond_logreg(g = rep(1, length(xtilde1)), xtilde1, xtilde0, c1 = NULL,
  c0 = NULL, errors = "processing", approx_integral = TRUE,
  estimate_var = FALSE, start_nonvar_var = c(0.01, 1),
  lower_nonvar_var = c(-Inf, 1e-04), upper_nonvar_var = c(Inf, Inf),
  jitter_start = 0.01, hcubature_list = list(tol = 1e-08),
  nlminb_list = list(control = list(trace = 1, eval.max = 500, iter.max =
  500)), hessian_list = list(method.args = list(r = 4)),
  nlminb_object = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cond_logreg_+3A_g">g</code></td>
<td>
<p>Numeric vector with pool sizes, i.e. number of members in each pool.</p>
</td></tr>
<tr><td><code id="cond_logreg_+3A_xtilde1">xtilde1</code></td>
<td>
<p>Numeric vector (or list of numeric vectors, if some
observations have replicates) with Xtilde values for cases.</p>
</td></tr>
<tr><td><code id="cond_logreg_+3A_xtilde0">xtilde0</code></td>
<td>
<p>Numeric vector (or list of numeric vectors, if some
observations have replicates) with Xtilde values for controls.</p>
</td></tr>
<tr><td><code id="cond_logreg_+3A_c1">c1</code></td>
<td>
<p>Numeric matrix with precisely measured covariates for cases.</p>
</td></tr>
<tr><td><code id="cond_logreg_+3A_c0">c0</code></td>
<td>
<p>Numeric matrix with precisely measured covariates for controls.</p>
</td></tr>
<tr><td><code id="cond_logreg_+3A_errors">errors</code></td>
<td>
<p>Character string specifying the errors that X is subject to.
Choices are <code>"none"</code>, <code>"measurement"</code> for measurement error,
<code>"processing"</code> for processing error (only relevant for pooled data), and
<code>"both"</code>.</p>
</td></tr>
<tr><td><code id="cond_logreg_+3A_approx_integral">approx_integral</code></td>
<td>
<p>Logical value for whether to use the probit
approximation for the logistic-normal integral, to avoid numerically
integrating X's out of the likelihood function.</p>
</td></tr>
<tr><td><code id="cond_logreg_+3A_estimate_var">estimate_var</code></td>
<td>
<p>Logical value for whether to return variance-covariance
matrix for parameter estimates.</p>
</td></tr>
<tr><td><code id="cond_logreg_+3A_start_nonvar_var">start_nonvar_var</code></td>
<td>
<p>Numeric vector of length 2 specifying starting value
for non-variance terms and variance terms, respectively.</p>
</td></tr>
<tr><td><code id="cond_logreg_+3A_lower_nonvar_var">lower_nonvar_var</code></td>
<td>
<p>Numeric vector of length 2 specifying lower bound for
non-variance terms and variance terms, respectively.</p>
</td></tr>
<tr><td><code id="cond_logreg_+3A_upper_nonvar_var">upper_nonvar_var</code></td>
<td>
<p>Numeric vector of length 2 specifying upper bound for
non-variance terms and variance terms, respectively.</p>
</td></tr>
<tr><td><code id="cond_logreg_+3A_jitter_start">jitter_start</code></td>
<td>
<p>Numeric value specifying standard deviation for mean-0
normal jitters to add to starting values for a second try at maximizing the
log-likelihood, should the initial call to <code><a href="stats.html#topic+nlminb">nlminb</a></code> result
in non-convergence. Set to <code>NULL</code> for no second try.</p>
</td></tr>
<tr><td><code id="cond_logreg_+3A_hcubature_list">hcubature_list</code></td>
<td>
<p>List of arguments to pass to
<code><a href="cubature.html#topic+hcubature">hcubature</a></code> for numerical integration. Only used if
<code>approx_integral = FALSE</code>.</p>
</td></tr>
<tr><td><code id="cond_logreg_+3A_nlminb_list">nlminb_list</code></td>
<td>
<p>List of arguments to pass to <code><a href="stats.html#topic+nlminb">nlminb</a></code>
for log-likelihood maximization.</p>
</td></tr>
<tr><td><code id="cond_logreg_+3A_hessian_list">hessian_list</code></td>
<td>
<p>List of arguments to pass to
<code><a href="numDeriv.html#topic+hessian">hessian</a></code> for approximating the Hessian matrix. Only
used if <code>estimate_var = TRUE</code>.</p>
</td></tr>
<tr><td><code id="cond_logreg_+3A_nlminb_object">nlminb_object</code></td>
<td>
<p>Object returned from <code><a href="stats.html#topic+nlminb">nlminb</a></code> in a
prior call. Useful for bypassing log-likelihood maximization if you just want
to re-estimate the Hessian matrix with different options.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing:
</p>

<ol>
<li><p> Numeric vector of parameter estimates.
</p>
</li>
<li><p> Variance-covariance matrix (if <code>estimate_var = TRUE</code>).
</p>
</li>
<li><p> Returned <code><a href="stats.html#topic+nlminb">nlminb</a></code> object from maximizing the
log-likelihood function.
</p>
</li>
<li><p> Akaike information criterion (AIC).
</p>
</li></ol>



<h3>References</h3>

<p>Saha-Chaudhuri, P., Umbach, D.M. and Weinberg, C.R. (2011) &quot;Pooled exposure
assessment for matched case-control studies.&quot; <em>Epidemiology</em>
<strong>22</strong>(5): 704&ndash;712.
</p>
<p>Schisterman, E.F., Vexler, A., Mumford, S.L. and Perkins, N.J. (2010) &quot;Hybrid
pooled-unpooled design for cost-efficient measurement of biomarkers.&quot;
<em>Stat. Med.</em> <strong>29</strong>(5): 597&ndash;613.
</p>
<p>Weinberg, C.R. and Umbach, D.M. (1999) &quot;Using pooled exposure assessment to
improve efficiency in case-control studies.&quot; <em>Biometrics</em> <strong>55</strong>:
718&ndash;726.
</p>
<p>Weinberg, C.R. and Umbach, D.M. (2014) &quot;Correction to 'Using pooled exposure
assessment to improve efficiency in case-control studies' by Clarice R.
Weinberg and David M. Umbach; 55, 718&ndash;726, September 1999.&quot;
<em>Biometrics</em> <strong>70</strong>: 1061.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load simulated data for 150 case pools and 150 control pools
data(dat_cond_logreg)
dat &lt;- dat_cond_logreg$dat
xtilde1 &lt;- dat_cond_logreg$xtilde1
xtilde0 &lt;- dat_cond_logreg$xtilde0

# Fit conditional logistic regression to estimate log-odds ratio for X and Y
# adjusted for C, using the precise poolwise summed exposure X. True log-OR
# for X is 0.5.
truth &lt;- cond_logreg(
  g = dat$g,
  xtilde1 = dat$x1,
  xtilde0 = dat$x0,
  c1 = dat$c1.model,
  c0 = dat$c0.model,
  errors = "neither"
)
truth$theta.hat

# Suppose X is subject to additive measurement error and processing error,
# and we observe Xtilde1 and Xtilde0 rather than X1 and X0. Fit model with
# Xtilde's, accounting for errors (numerical integration avoided by using
# probit approximation).
## Not run: 
corrected &lt;- cond_logreg(
  g = dat$g,
  xtilde1 = xtilde1,
  xtilde0 = xtilde0,
  c1 = dat$c1.model,
  c0 = dat$c0.model,
  errors = "both",
  approx_integral = TRUE
)
corrected$theta.hat

## End(Not run)


</code></pre>

<hr>
<h2 id='dat_cond_logreg'>Dataset for Examples in cond_logreg</h2><span id='topic+dat_cond_logreg'></span>

<h3>Description</h3>

<p>List containing (1) data frame with poolwise
(g, X1, X0, C1.model, C0.model, C1.match, C0.match) values, (2) list of
replicate Xtilde values for case pools, and (3) list of replicate Xtilde
values for control pools.
</p>


<h3>Source</h3>

<p>Simulated data in R.
</p>

<hr>
<h2 id='dat_p_gdfa'>Dataset for Examples in p_gdfa</h2><span id='topic+dat_p_gdfa'></span>

<h3>Description</h3>

<p>List containing (1) data frame with poolwise (g, Y, X, Xtilde) values, (2)
list with replicate Xtilde values, and (3) list with C values for members of
each pool.
</p>


<h3>Source</h3>

<p>Simulated data in R.
</p>

<hr>
<h2 id='dat_p_linreg_yerrors'>Dataset for Examples in p_linreg_yerrors</h2><span id='topic+dat_p_linreg_yerrors'></span>

<h3>Description</h3>

<p>List containing (1) data frame with poolwise (g, Y, X1, X2) values and (2)
list with replicate Y values.
</p>


<h3>Source</h3>

<p>Simulated data in R.
</p>

<hr>
<h2 id='dat_p_ndfa'>Dataset for Examples in p_ndfa</h2><span id='topic+dat_p_ndfa'></span>

<h3>Description</h3>

<p>List containing (1) data frame with poolwise (g, Y*, Y, X, Xtilde, C) values
and (2) list with replicate Xtilde values.
</p>


<h3>Source</h3>

<p>Simulated data in R.
</p>

<hr>
<h2 id='form_pools'>Created a Pooled Dataset from a Subject-Specific One</h2><span id='topic+form_pools'></span>

<h3>Description</h3>

<p>Useful for simulation studies on biospecimen pooling designs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>form_pools(dat, pool_sizes, num_each = NULL,
  prop_each = rep(1/length(pool_sizes), length(pool_sizes)))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="form_pools_+3A_dat">dat</code></td>
<td>
<p>Data frame with individual level data.</p>
</td></tr>
<tr><td><code id="form_pools_+3A_pool_sizes">pool_sizes</code></td>
<td>
<p>Integer vector of pool sizes ordered from largest to
smallest.</p>
</td></tr>
<tr><td><code id="form_pools_+3A_num_each">num_each</code></td>
<td>
<p>Integer vector specifying number of pools of each size.</p>
</td></tr>
<tr><td><code id="form_pools_+3A_prop_each">prop_each</code></td>
<td>
<p>Numeric vector specifying proportion of pools of each size.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Data frame.
</p>

<hr>
<h2 id='p_dfa_xerrors'>Discriminant Function Approach for Estimating Odds Ratio with Normal Exposure
Measured in Pools and Potentially Subject to Errors</h2><span id='topic+p_dfa_xerrors'></span>

<h3>Description</h3>

<p>Archived on 7/23/18. Please use <code><a href="#topic+p_ndfa">p_ndfa</a></code> instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>p_dfa_xerrors(g, y, xtilde, c = NULL, constant_or = TRUE,
  errors = "both", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="p_dfa_xerrors_+3A_g">g</code></td>
<td>
<p>Numeric vector of pool sizes, i.e. number of members in each pool.</p>
</td></tr>
<tr><td><code id="p_dfa_xerrors_+3A_y">y</code></td>
<td>
<p>Numeric vector of poolwise <code>Y</code> values (number of cases in each
pool).</p>
</td></tr>
<tr><td><code id="p_dfa_xerrors_+3A_xtilde">xtilde</code></td>
<td>
<p>Numeric vector (or list of numeric vectors, if some pools have
replicates) with <code>Xtilde</code> values.</p>
</td></tr>
<tr><td><code id="p_dfa_xerrors_+3A_c">c</code></td>
<td>
<p>Numeric matrix with poolwise <strong><code>C</code></strong> values (if any), with
one row for each pool. Can be a vector if there is only 1 covariate.</p>
</td></tr>
<tr><td><code id="p_dfa_xerrors_+3A_constant_or">constant_or</code></td>
<td>
<p>Logical value for whether to assume a constant OR for
<code>X</code>, which means that <code>sigsq_1 = sigsq_0</code>. If <code>NULL</code>, model is
fit with and without this assumption, and likelihood ratio test is performed
to test it.</p>
</td></tr>
<tr><td><code id="p_dfa_xerrors_+3A_errors">errors</code></td>
<td>
<p>Character string specifying the errors that <code>X</code> is subject
to. Choices are <code>"neither"</code>, <code>"processing"</code> for processing error
only, <code>"measurement"</code> for measurement error only, and <code>"both"</code>.</p>
</td></tr>
<tr><td><code id="p_dfa_xerrors_+3A_...">...</code></td>
<td>
<p>Additional arguments to pass to <code><a href="stats.html#topic+nlminb">nlminb</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of point estimates, variance-covariance matrix, object returned by
<code><a href="stats.html#topic+nlminb">nlminb</a></code>, and AIC, for one or two models depending on
<code>constant_or</code>. If <code>constant_or = NULL</code>, also returns result of a
likelihood ratio test for <code>H0: sigsq_1 = sigsq_0</code>, which is equivalent
to <code>H0: log-OR is constant</code>. If <code>constant_or = NULL</code>, returned
objects with names ending in 1 are for model that does not assume constant
log-OR, and those ending in 2 are for model that assumes constant log-OR.
</p>


<h3>References</h3>

<p>Lyles, R.H., Van Domelen, D.R., Mitchell, E.M. and Schisterman, E.F. (2015)
&quot;A discriminant function approach to adjust for processing and measurement
error When a biomarker is assayed in pooled samples.&quot;
<em>Int. J. Environ. Res. Public Health</em> <strong>12</strong>(11): 14723&ndash;14740.
</p>
<p>Schisterman, E.F., Vexler, A., Mumford, S.L. and Perkins, N.J. (2010) &quot;Hybrid
pooled-unpooled design for cost-efficient measurement of biomarkers.&quot;
<em>Stat. Med.</em> <strong>29</strong>(5): 597&ndash;613.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load dataset containing poolwise (Y, Xtilde, C) values for pools of size
# 1, 2, and 3. Xtilde values are affected by processing error.
data(pdat1)

# Estimate log-OR for X and Y adjusted for C, ignoring processing error
fit1 &lt;- p_dfa_xerrors(g = pdat1$g, y = pdat1$numcases, xtilde = pdat1$xtilde,
                      c = pdat1$c, errors = "neither")
fit1$estimates

# Repeat, but accounting for processing error. Closer to true log-OR of 0.5.
fit2 &lt;- p_dfa_xerrors(g = pdat1$g, y = pdat1$numcases, xtilde = pdat1$xtilde,
                      c = pdat1$c, errors = "processing")
fit2$estimates


</code></pre>

<hr>
<h2 id='p_dfa_xerrors2'>Discriminant Function Approach for Estimating Odds Ratio with Gamma Exposure
Measured in Pools and Potentially Subject to Errors</h2><span id='topic+p_dfa_xerrors2'></span>

<h3>Description</h3>

<p>Archived on 7/23/18. Please use <code><a href="#topic+p_gdfa">p_gdfa</a></code> instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>p_dfa_xerrors2(g, y, xtilde, c = NULL, constant_or = TRUE,
  errors = "both", integrate_tol = 1e-08,
  integrate_tol_hessian = integrate_tol, estimate_var = TRUE,
  fix_posdef = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="p_dfa_xerrors2_+3A_g">g</code></td>
<td>
<p>Numeric vector with pool sizes, i.e. number of members in each pool.</p>
</td></tr>
<tr><td><code id="p_dfa_xerrors2_+3A_y">y</code></td>
<td>
<p>Numeric vector with poolwise <code>Y</code> values, coded 0 if all members
are controls and 1 if all members are cases.</p>
</td></tr>
<tr><td><code id="p_dfa_xerrors2_+3A_xtilde">xtilde</code></td>
<td>
<p>Numeric vector (or list of numeric vectors, if some pools have
replicates) with <code>Xtilde</code> values.</p>
</td></tr>
<tr><td><code id="p_dfa_xerrors2_+3A_c">c</code></td>
<td>
<p>List where each element is a numeric matrix containing the
<strong><code>C</code></strong> values for members of a particular pool (1 row for each
member).</p>
</td></tr>
<tr><td><code id="p_dfa_xerrors2_+3A_constant_or">constant_or</code></td>
<td>
<p>Logical value for whether to assume a constant OR for
<code>X</code>, which means that <code>gamma_y = 0</code>. If <code>NULL</code>, model is
fit with and without this assumption, and likelihood ratio test is performed
to test it.</p>
</td></tr>
<tr><td><code id="p_dfa_xerrors2_+3A_errors">errors</code></td>
<td>
<p>Character string specifying the errors that <code>X</code> is subject
to. Choices are <code>"neither"</code>, <code>"processing"</code> for processing error
only, <code>"measurement"</code> for measurement error only, and <code>"both"</code>.</p>
</td></tr>
<tr><td><code id="p_dfa_xerrors2_+3A_integrate_tol">integrate_tol</code></td>
<td>
<p>Numeric value specifying the <code>tol</code> input to
<code><a href="cubature.html#topic+hcubature">hcubature</a></code>.</p>
</td></tr>
<tr><td><code id="p_dfa_xerrors2_+3A_integrate_tol_hessian">integrate_tol_hessian</code></td>
<td>
<p>Same as <code>integrate_tol</code>, but for use when
estimating the Hessian matrix only. Sometimes more precise integration
(i.e. smaller tolerance) helps prevent cases where the inverse Hessian is not
positive definite.</p>
</td></tr>
<tr><td><code id="p_dfa_xerrors2_+3A_estimate_var">estimate_var</code></td>
<td>
<p>Logical value for whether to return variance-covariance
matrix for parameter estimates.</p>
</td></tr>
<tr><td><code id="p_dfa_xerrors2_+3A_fix_posdef">fix_posdef</code></td>
<td>
<p>Logical value for whether to repeatedly reduce
<code>integrate_tol_hessian</code> by factor of 5 and re-estimate Hessian to try
to avoid non-positive definite variance-covariance matrix.</p>
</td></tr>
<tr><td><code id="p_dfa_xerrors2_+3A_...">...</code></td>
<td>
<p>Additional arguments to pass to <code><a href="stats.html#topic+nlminb">nlminb</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of point estimates, variance-covariance matrix, objects returned by
<code><a href="stats.html#topic+nlminb">nlminb</a></code>, and AICs, for one or two models depending on
<code>constant_or</code>. If <code>constant_or = NULL</code>, also returns result of a
likelihood ratio test for <code>H0: gamma_y = 0</code>, which is equivalent to
<code>H0: log-OR is constant</code>. If <code>constant_or = NULL</code>, returned objects
with names ending in 1 are for model that does not assume constant log-OR,
and those ending in 2 are for model that assumes constant log-OR.
</p>


<h3>References</h3>

<p>Lyles, R.H., Van Domelen, D.R., Mitchell, E.M. and Schisterman, E.F. (2015)
&quot;A discriminant function approach to adjust for processing and measurement
error When a biomarker is assayed in pooled samples.&quot;
<em>Int. J. Environ. Res. Public Health</em> <strong>12</strong>(11): 14723&ndash;14740.
</p>
<p>Mitchell, E.M, Lyles, R.H., and Schisterman, E.F. (2015) &quot;Positing, fitting,
and selecting regression models for pooled biomarker data.&quot; <em>Stat. Med</em>
<strong>34</strong>(17): 2544&ndash;2558.
</p>
<p>Schisterman, E.F., Vexler, A., Mumford, S.L. and Perkins, N.J. (2010) &quot;Hybrid
pooled-unpooled design for cost-efficient measurement of biomarkers.&quot;
<em>Stat. Med.</em> <strong>29</strong>(5): 597&ndash;613.
</p>
<p>Whitcomb, B.W., Perkins, N.J., Zhang, Z., Ye, A., and Lyles, R. H. (2012)
&quot;Assessment of skewed exposure in case-control studies with pooling.&quot;
<em>Stat. Med.</em> <strong>31</strong>: 2461&ndash;2472.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load dataset with (g, Y, Xtilde, C) values for 248 pools and list of C
# values for members of each pool. Xtilde values are affected by processing
# error.
data(pdat2)
dat &lt;- pdat2$dat
c.list &lt;- pdat2$c.list

# Estimate log-OR for X and Y adjusted for C, ignoring processing error
fit1 &lt;- p_dfa_xerrors2(
  g = dat$g,
  y = dat$y,
  xtilde = dat$xtilde,
  c = c.list,
  errors = "neither"
)
fit1$estimates

# Repeat, but accounting for processing error.
## Not run: 
fit2 &lt;- p_dfa_xerrors2(
  g = dat$g,
  y = dat$y,
  xtilde = dat$xtilde,
  c = c.list,
  errors = "processing",
  control = list(trace = 1)
)
fit2$estimates

## End(Not run)

</code></pre>

<hr>
<h2 id='p_gdfa'>Gamma Discriminant Function Approach for Estimating Odds Ratio with Exposure
Measured in Pools and Potentially Subject to Multiplicative Lognormal Errors</h2><span id='topic+p_gdfa'></span>

<h3>Description</h3>

<p>Assumes exposure given covariates and outcome is a constant-scale Gamma
regression. Pooled exposure measurements can be assumed precise or subject to
multiplicative lognormal processing error and/or measurement error.
Parameters are estimated using maximum likelihood.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>p_gdfa(g, y, xtilde, c = NULL, constant_or = TRUE,
  errors = "processing", estimate_var = TRUE,
  start_nonvar_var = c(0.01, 1), lower_nonvar_var = c(-Inf, 1e-04),
  upper_nonvar_var = c(Inf, Inf), jitter_start = 0.01,
  hcubature_list = list(tol = 1e-08), nlminb_list = list(control =
  list(trace = 1, eval.max = 500, iter.max = 500)),
  hessian_list = list(method.args = list(r = 4)), nlminb_object = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="p_gdfa_+3A_g">g</code></td>
<td>
<p>Numeric vector with pool sizes, i.e. number of members in each pool.</p>
</td></tr>
<tr><td><code id="p_gdfa_+3A_y">y</code></td>
<td>
<p>Numeric vector with poolwise Y values, coded 0 if all members are
controls and 1 if all members are cases.</p>
</td></tr>
<tr><td><code id="p_gdfa_+3A_xtilde">xtilde</code></td>
<td>
<p>Numeric vector (or list of numeric vectors, if some pools have
replicates) with Xtilde values.</p>
</td></tr>
<tr><td><code id="p_gdfa_+3A_c">c</code></td>
<td>
<p>List where each element is a numeric matrix containing the
<strong>C</strong> values for members of a particular pool (1 row for each member).</p>
</td></tr>
<tr><td><code id="p_gdfa_+3A_constant_or">constant_or</code></td>
<td>
<p>Logical value for whether to assume a constant OR for
X, which means that gamma_y = 0. If <code>NULL</code>, model is fit with and
without this assumption, and a likelihood ratio test is performed to test it.</p>
</td></tr>
<tr><td><code id="p_gdfa_+3A_errors">errors</code></td>
<td>
<p>Character string specifying the errors that <code>X</code> is subject
to. Choices are <code>"neither"</code>, <code>"processing"</code> for processing error
only, <code>"measurement"</code> for measurement error only, and <code>"both"</code>.</p>
</td></tr>
<tr><td><code id="p_gdfa_+3A_estimate_var">estimate_var</code></td>
<td>
<p>Logical value for whether to return variance-covariance
matrix for parameter estimates.</p>
</td></tr>
<tr><td><code id="p_gdfa_+3A_start_nonvar_var">start_nonvar_var</code></td>
<td>
<p>Numeric vector of length 2 specifying starting value
for non-variance terms and variance terms, respectively.</p>
</td></tr>
<tr><td><code id="p_gdfa_+3A_lower_nonvar_var">lower_nonvar_var</code></td>
<td>
<p>Numeric vector of length 2 specifying lower bound for
non-variance terms and variance terms, respectively.</p>
</td></tr>
<tr><td><code id="p_gdfa_+3A_upper_nonvar_var">upper_nonvar_var</code></td>
<td>
<p>Numeric vector of length 2 specifying upper bound for
non-variance terms and variance terms, respectively.</p>
</td></tr>
<tr><td><code id="p_gdfa_+3A_jitter_start">jitter_start</code></td>
<td>
<p>Numeric value specifying standard deviation for mean-0
normal jitters to add to starting values for a second try at maximizing the
log-likelihood, should the initial call to <code><a href="stats.html#topic+nlminb">nlminb</a></code> result
in non-convergence. Set to <code>NULL</code> for no second try.</p>
</td></tr>
<tr><td><code id="p_gdfa_+3A_hcubature_list">hcubature_list</code></td>
<td>
<p>List of arguments to pass to
<code><a href="cubature.html#topic+hcubature">hcubature</a></code> for numerical integration.</p>
</td></tr>
<tr><td><code id="p_gdfa_+3A_nlminb_list">nlminb_list</code></td>
<td>
<p>List of arguments to pass to <code><a href="stats.html#topic+nlminb">nlminb</a></code>
for log-likelihood maximization.</p>
</td></tr>
<tr><td><code id="p_gdfa_+3A_hessian_list">hessian_list</code></td>
<td>
<p>List of arguments to pass to
<code><a href="numDeriv.html#topic+hessian">hessian</a></code> for approximating the Hessian matrix. Only
used if <code>estimate_var = TRUE</code>.</p>
</td></tr>
<tr><td><code id="p_gdfa_+3A_nlminb_object">nlminb_object</code></td>
<td>
<p>Object returned from <code><a href="stats.html#topic+nlminb">nlminb</a></code> in a
prior call. Useful for bypassing log-likelihood maximization if you just want
to re-estimate the Hessian matrix with different options.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing:
</p>

<ol>
<li><p> Numeric vector of parameter estimates.
</p>
</li>
<li><p> Variance-covariance matrix.
</p>
</li>
<li><p> Returned <code><a href="stats.html#topic+nlminb">nlminb</a></code> object from maximizing the
log-likelihood function.
</p>
</li>
<li><p> Akaike information criterion (AIC).
</p>
</li></ol>

<p>If <code>constant_or = NULL</code>, two such lists are returned (one under a
constant odds ratio assumption and one not), along with a likelihood ratio
test for <code>H0: gamma_y = 0</code>, which is equivalent to
<code>H0: odds ratio is constant</code>.
</p>


<h3>References</h3>

<p>Lyles, R.H., Van Domelen, D.R., Mitchell, E.M. and Schisterman, E.F. (2015)
&quot;A discriminant function approach to adjust for processing and measurement
error When a biomarker is assayed in pooled samples.&quot;
<em>Int. J. Environ. Res. Public Health</em> <strong>12</strong>(11): 14723&ndash;14740.
</p>
<p>Mitchell, E.M, Lyles, R.H., and Schisterman, E.F. (2015) &quot;Positing, fitting,
and selecting regression models for pooled biomarker data.&quot; <em>Stat. Med</em>
<strong>34</strong>(17): 2544&ndash;2558.
</p>
<p>Schisterman, E.F., Vexler, A., Mumford, S.L. and Perkins, N.J. (2010) &quot;Hybrid
pooled-unpooled design for cost-efficient measurement of biomarkers.&quot;
<em>Stat. Med.</em> <strong>29</strong>(5): 597&ndash;613.
</p>
<p>Whitcomb, B.W., Perkins, N.J., Zhang, Z., Ye, A., and Lyles, R. H. (2012)
&quot;Assessment of skewed exposure in case-control studies with pooling.&quot;
<em>Stat. Med.</em> <strong>31</strong>: 2461&ndash;2472.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load data frame with (g, Y, X, Xtilde) values for 496 pools, list of C
# values for members of each pool, and list of Xtilde values where 25
# single-specimen pools have replicates. Xtilde values are affected by
# processing error and measurement error. True log-OR = 0.5, sigsq_p = 0.25,
# sigsq_m = 0.1.
data(dat_p_gdfa)
dat &lt;- dat_p_gdfa$dat
reps &lt;- dat_p_gdfa$reps
c.list &lt;- dat_p_gdfa$c.list

# Unobservable truth estimator - use precise X's
fit.unobservable &lt;- p_gdfa(
  g = dat$g,
  y = dat$y,
  xtilde = dat$x,
  c = c.list,
  errors = "neither"
)
fit.unobservable$estimates

# Naive estimator - use imprecise Xtilde's, but treat as precise
fit.naive &lt;- p_gdfa(
  g = dat$g,
  y = dat$y,
  xtilde = dat$xtilde,
  c = c.list,
  errors = "neither"
)
fit.naive$estimates

# Corrected estimator - use Xtilde's and account for errors (not using
# replicates here)
## Not run: 
fit.noreps &lt;- p_gdfa(
  g = dat$g,
  y = dat$y,
  xtilde = dat$xtilde,
  c = c.list,
  errors = "both"
)
fit.noreps$estimates

# Corrected estimator - use Xtilde's including 25 replicates
fit.reps &lt;- p_gdfa(
  g = dat$g,
  y = dat$y,
  xtilde = reps,
  c = c.list,
  errors = "both"
)
fit.reps$estimates

# Same as previous, but allowing for non-constant odds ratio.
fit.nonconstant &lt;- p_gdfa(
  g = dat$g,
  y = dat$y,
  xtilde = reps,
  c = c.list,
  constant_or = FALSE,
  errors = "both",
  hcubature_list = list(tol = 1e-4)
)
fit.nonconstant$estimates

# Visualize estimated log-OR vs. X based on previous model fit
p &lt;- plot_gdfa(
  estimates = fit.nonconstant$estimates,
  varcov = fit.nonconstant$theta.var,
  xrange = range(dat$xtilde[dat$g == 1]),
  cvals = mean(unlist(c))
)
p

## End(Not run)

</code></pre>

<hr>
<h2 id='p_gdfa_constant'>Gamma Discriminant Function Approach for Estimating Odds Ratio with Exposure
Measured in Pools and Potentially Subject to Multiplicative Lognormal Errors
(Constant Odds Ratio Version)</h2><span id='topic+p_gdfa_constant'></span>

<h3>Description</h3>

<p>See <code><a href="#topic+p_gdfa">p_gdfa</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>p_gdfa_constant(g, y, xtilde, c = NULL, errors = "processing",
  estimate_var = TRUE, start_nonvar_var = c(0.01, 1),
  lower_nonvar_var = c(-Inf, 1e-04), upper_nonvar_var = c(Inf, Inf),
  jitter_start = 0.01, hcubature_list = list(tol = 1e-08),
  nlminb_list = list(control = list(trace = 1, eval.max = 500, iter.max =
  500)), hessian_list = list(method.args = list(r = 4)),
  nlminb_object = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="p_gdfa_constant_+3A_g">g</code></td>
<td>
<p>Numeric vector with pool sizes, i.e. number of members in each pool.</p>
</td></tr>
<tr><td><code id="p_gdfa_constant_+3A_y">y</code></td>
<td>
<p>Numeric vector with poolwise Y values, coded 0 if all members are
controls and 1 if all members are cases.</p>
</td></tr>
<tr><td><code id="p_gdfa_constant_+3A_xtilde">xtilde</code></td>
<td>
<p>Numeric vector (or list of numeric vectors, if some pools have
replicates) with Xtilde values.</p>
</td></tr>
<tr><td><code id="p_gdfa_constant_+3A_c">c</code></td>
<td>
<p>List where each element is a numeric matrix containing the
<strong>C</strong> values for members of a particular pool (1 row for each member).</p>
</td></tr>
<tr><td><code id="p_gdfa_constant_+3A_errors">errors</code></td>
<td>
<p>Character string specifying the errors that X is subject to.
Choices are <code>"neither"</code>, <code>"processing"</code> for processing error only,
<code>"measurement"</code> for measurement error only, and <code>"both"</code>.</p>
</td></tr>
<tr><td><code id="p_gdfa_constant_+3A_estimate_var">estimate_var</code></td>
<td>
<p>Logical value for whether to return variance-covariance
matrix for parameter estimates.</p>
</td></tr>
<tr><td><code id="p_gdfa_constant_+3A_start_nonvar_var">start_nonvar_var</code></td>
<td>
<p>Numeric vector of length 2 specifying starting value
for non-variance terms and variance terms, respectively.</p>
</td></tr>
<tr><td><code id="p_gdfa_constant_+3A_lower_nonvar_var">lower_nonvar_var</code></td>
<td>
<p>Numeric vector of length 2 specifying lower bound for
non-variance terms and variance terms, respectively.</p>
</td></tr>
<tr><td><code id="p_gdfa_constant_+3A_upper_nonvar_var">upper_nonvar_var</code></td>
<td>
<p>Numeric vector of length 2 specifying upper bound for
non-variance terms and variance terms, respectively.</p>
</td></tr>
<tr><td><code id="p_gdfa_constant_+3A_jitter_start">jitter_start</code></td>
<td>
<p>Numeric value specifying standard deviation for mean-0
normal jitters to add to starting values for a second try at maximizing the
log-likelihood, should the initial call to <code><a href="stats.html#topic+nlminb">nlminb</a></code> result
in non-convergence. Set to <code>NULL</code> for no second try.</p>
</td></tr>
<tr><td><code id="p_gdfa_constant_+3A_hcubature_list">hcubature_list</code></td>
<td>
<p>List of arguments to pass to
<code><a href="cubature.html#topic+hcubature">hcubature</a></code> for numerical integration.</p>
</td></tr>
<tr><td><code id="p_gdfa_constant_+3A_nlminb_list">nlminb_list</code></td>
<td>
<p>List of arguments to pass to <code><a href="stats.html#topic+nlminb">nlminb</a></code>
for log-likelihood maximization.</p>
</td></tr>
<tr><td><code id="p_gdfa_constant_+3A_hessian_list">hessian_list</code></td>
<td>
<p>List of arguments to pass to
<code><a href="numDeriv.html#topic+hessian">hessian</a></code> for approximating the Hessian matrix. Only
used if <code>estimate_var = TRUE</code>.</p>
</td></tr>
<tr><td><code id="p_gdfa_constant_+3A_nlminb_object">nlminb_object</code></td>
<td>
<p>Object returned from <code><a href="stats.html#topic+nlminb">nlminb</a></code> in a
prior call. Useful for bypassing log-likelihood maximization if you just want
to re-estimate the Hessian matrix with different options.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing:
</p>

<ol>
<li><p> Numeric vector of parameter estimates.
</p>
</li>
<li><p> Variance-covariance matrix.
</p>
</li>
<li><p> Returned <code><a href="stats.html#topic+nlminb">nlminb</a></code> object from maximizing the
log-likelihood function.
</p>
</li>
<li><p> Akaike information criterion (AIC).
</p>
</li></ol>



<h3>References</h3>

<p>Lyles, R.H., Van Domelen, D.R., Mitchell, E.M. and Schisterman, E.F. (2015)
&quot;A discriminant function approach to adjust for processing and measurement
error When a biomarker is assayed in pooled samples.&quot;
<em>Int. J. Environ. Res. Public Health</em> <strong>12</strong>(11): 14723&ndash;14740.
</p>
<p>Mitchell, E.M, Lyles, R.H., and Schisterman, E.F. (2015) &quot;Positing, fitting,
and selecting regression models for pooled biomarker data.&quot; <em>Stat. Med</em>
<strong>34</strong>(17): 2544&ndash;2558.
</p>
<p>Schisterman, E.F., Vexler, A., Mumford, S.L. and Perkins, N.J. (2010) &quot;Hybrid
pooled-unpooled design for cost-efficient measurement of biomarkers.&quot;
<em>Stat. Med.</em> <strong>29</strong>(5): 597&ndash;613.
</p>
<p>Whitcomb, B.W., Perkins, N.J., Zhang, Z., Ye, A., and Lyles, R. H. (2012)
&quot;Assessment of skewed exposure in case-control studies with pooling.&quot;
<em>Stat. Med.</em> <strong>31</strong>: 2461&ndash;2472.
</p>

<hr>
<h2 id='p_gdfa_nonconstant'>Gamma Discriminant Function Approach for Estimating Odds Ratio with Exposure
Measured in Pools and Potentially Subject to Multiplicative Lognormal Errors
(Non-constant Odds Ratio Version)</h2><span id='topic+p_gdfa_nonconstant'></span>

<h3>Description</h3>

<p>See <code><a href="#topic+p_gdfa">p_gdfa</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>p_gdfa_nonconstant(g, y, xtilde, c = NULL, errors = "processing",
  estimate_var = TRUE, start_nonvar_var = c(0.01, 1),
  lower_nonvar_var = c(-Inf, 1e-04), upper_nonvar_var = c(Inf, Inf),
  jitter_start = 0.01, hcubature_list = list(tol = 1e-08),
  nlminb_list = list(control = list(trace = 1, eval.max = 500, iter.max =
  500)), hessian_list = list(method.args = list(r = 4)),
  nlminb_object = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="p_gdfa_nonconstant_+3A_g">g</code></td>
<td>
<p>Numeric vector with pool sizes, i.e. number of members in each pool.</p>
</td></tr>
<tr><td><code id="p_gdfa_nonconstant_+3A_y">y</code></td>
<td>
<p>Numeric vector with poolwise Y values, coded 0 if all members are
controls and 1 if all members are cases.</p>
</td></tr>
<tr><td><code id="p_gdfa_nonconstant_+3A_xtilde">xtilde</code></td>
<td>
<p>Numeric vector (or list of numeric vectors, if some pools have
replicates) with Xtilde values.</p>
</td></tr>
<tr><td><code id="p_gdfa_nonconstant_+3A_c">c</code></td>
<td>
<p>List where each element is a numeric matrix containing the
<strong>C</strong> values for members of a particular pool (1 row for each member).</p>
</td></tr>
<tr><td><code id="p_gdfa_nonconstant_+3A_errors">errors</code></td>
<td>
<p>Character string specifying the errors that X is subject to.
Choices are <code>"neither"</code>, <code>"processing"</code> for processing error only,
<code>"measurement"</code> for measurement error only, and <code>"both"</code>.</p>
</td></tr>
<tr><td><code id="p_gdfa_nonconstant_+3A_estimate_var">estimate_var</code></td>
<td>
<p>Logical value for whether to return variance-covariance
matrix for parameter estimates.</p>
</td></tr>
<tr><td><code id="p_gdfa_nonconstant_+3A_start_nonvar_var">start_nonvar_var</code></td>
<td>
<p>Numeric vector of length 2 specifying starting value
for non-variance terms and variance terms, respectively.</p>
</td></tr>
<tr><td><code id="p_gdfa_nonconstant_+3A_lower_nonvar_var">lower_nonvar_var</code></td>
<td>
<p>Numeric vector of length 2 specifying lower bound for
non-variance terms and variance terms, respectively.</p>
</td></tr>
<tr><td><code id="p_gdfa_nonconstant_+3A_upper_nonvar_var">upper_nonvar_var</code></td>
<td>
<p>Numeric vector of length 2 specifying upper bound for
non-variance terms and variance terms, respectively.</p>
</td></tr>
<tr><td><code id="p_gdfa_nonconstant_+3A_jitter_start">jitter_start</code></td>
<td>
<p>Numeric value specifying standard deviation for mean-0
normal jitters to add to starting values for a second try at maximizing the
log-likelihood, should the initial call to <code><a href="stats.html#topic+nlminb">nlminb</a></code> result
in non-convergence. Set to <code>NULL</code> for no second try.</p>
</td></tr>
<tr><td><code id="p_gdfa_nonconstant_+3A_hcubature_list">hcubature_list</code></td>
<td>
<p>List of arguments to pass to
<code><a href="cubature.html#topic+hcubature">hcubature</a></code> for numerical integration.</p>
</td></tr>
<tr><td><code id="p_gdfa_nonconstant_+3A_nlminb_list">nlminb_list</code></td>
<td>
<p>List of arguments to pass to <code><a href="stats.html#topic+nlminb">nlminb</a></code>
for log-likelihood maximization.</p>
</td></tr>
<tr><td><code id="p_gdfa_nonconstant_+3A_hessian_list">hessian_list</code></td>
<td>
<p>List of arguments to pass to
<code><a href="numDeriv.html#topic+hessian">hessian</a></code> for approximating the Hessian matrix. Only
used if <code>estimate_var = TRUE</code>.</p>
</td></tr>
<tr><td><code id="p_gdfa_nonconstant_+3A_nlminb_object">nlminb_object</code></td>
<td>
<p>Object returned from <code><a href="stats.html#topic+nlminb">nlminb</a></code> in a
prior call. Useful for bypassing log-likelihood maximization if you just want
to re-estimate the Hessian matrix with different options.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing:
</p>

<ol>
<li><p> Numeric vector of parameter estimates.
</p>
</li>
<li><p> Variance-covariance matrix.
</p>
</li>
<li><p> Returned <code><a href="stats.html#topic+nlminb">nlminb</a></code> object from maximizing the
log-likelihood function.
</p>
</li>
<li><p> Akaike information criterion (AIC).
</p>
</li></ol>



<h3>References</h3>

<p>Lyles, R.H., Van Domelen, D.R., Mitchell, E.M. and Schisterman, E.F. (2015)
&quot;A discriminant function approach to adjust for processing and measurement
error When a biomarker is assayed in pooled samples.&quot;
<em>Int. J. Environ. Res. Public Health</em> <strong>12</strong>(11): 14723&ndash;14740.
</p>
<p>Mitchell, E.M, Lyles, R.H., and Schisterman, E.F. (2015) &quot;Positing, fitting,
and selecting regression models for pooled biomarker data.&quot; <em>Stat. Med</em>
<strong>34</strong>(17): 2544&ndash;2558.
</p>
<p>Schisterman, E.F., Vexler, A., Mumford, S.L. and Perkins, N.J. (2010) &quot;Hybrid
pooled-unpooled design for cost-efficient measurement of biomarkers.&quot;
<em>Stat. Med.</em> <strong>29</strong>(5): 597&ndash;613.
</p>
<p>Whitcomb, B.W., Perkins, N.J., Zhang, Z., Ye, A., and Lyles, R. H. (2012)
&quot;Assessment of skewed exposure in case-control studies with pooling.&quot;
<em>Stat. Med.</em> <strong>31</strong>: 2461&ndash;2472.
</p>

<hr>
<h2 id='p_linreg_yerrors'>Linear Regression of Y vs. Covariates with Y Measured in Pools and
(Potentially) Subject to Additive Normal Errors</h2><span id='topic+p_linreg_yerrors'></span>

<h3>Description</h3>

<p>Assumes outcome given covariates is a normal-errors linear regression. Pooled
outcome measurements can be assumed precise or subject to additive normal
processing error and/or measurement error. Replicates are supported.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>p_linreg_yerrors(g, ytilde, x = NULL, errors = "processing",
  estimate_var = TRUE, start_nonvar_var = c(0.01, 1),
  lower_nonvar_var = c(-Inf, 1e-04), upper_nonvar_var = c(Inf, Inf),
  nlminb_list = list(control = list(trace = 1, eval.max = 500, iter.max =
  500)), hessian_list = list(method.args = list(r = 4)))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="p_linreg_yerrors_+3A_g">g</code></td>
<td>
<p>Numeric vector with pool sizes, i.e. number of members in each pool.</p>
</td></tr>
<tr><td><code id="p_linreg_yerrors_+3A_ytilde">ytilde</code></td>
<td>
<p>Numeric vector (or list of numeric vectors, if some pools have
replicates) with poolwise sum <code>Ytilde</code> values.</p>
</td></tr>
<tr><td><code id="p_linreg_yerrors_+3A_x">x</code></td>
<td>
<p>Numeric matrix with poolwise <strong><code>X</code></strong> values (if any), with
one row for each pool. Can be a vector if there is only 1 covariate.</p>
</td></tr>
<tr><td><code id="p_linreg_yerrors_+3A_errors">errors</code></td>
<td>
<p>Character string specifying the errors that <code>Y</code> is subject
to. Choices are <code>"neither"</code>, <code>"processing"</code> for processing error
only, <code>"measurement"</code> for measurement error only, and <code>"both"</code>.</p>
</td></tr>
<tr><td><code id="p_linreg_yerrors_+3A_estimate_var">estimate_var</code></td>
<td>
<p>Logical value for whether to return variance-covariance
matrix for parameter estimates.</p>
</td></tr>
<tr><td><code id="p_linreg_yerrors_+3A_start_nonvar_var">start_nonvar_var</code></td>
<td>
<p>Numeric vector of length 2 specifying starting value
for non-variance terms and variance terms, respectively.</p>
</td></tr>
<tr><td><code id="p_linreg_yerrors_+3A_lower_nonvar_var">lower_nonvar_var</code></td>
<td>
<p>Numeric vector of length 2 specifying lower bound for
non-variance terms and variance terms, respectively.</p>
</td></tr>
<tr><td><code id="p_linreg_yerrors_+3A_upper_nonvar_var">upper_nonvar_var</code></td>
<td>
<p>Numeric vector of length 2 specifying upper bound for
non-variance terms and variance terms, respectively.</p>
</td></tr>
<tr><td><code id="p_linreg_yerrors_+3A_nlminb_list">nlminb_list</code></td>
<td>
<p>List of arguments to pass to <code><a href="stats.html#topic+nlminb">nlminb</a></code>
for log-likelihood maximization.</p>
</td></tr>
<tr><td><code id="p_linreg_yerrors_+3A_hessian_list">hessian_list</code></td>
<td>
<p>List of arguments to pass to
<code><a href="numDeriv.html#topic+hessian">hessian</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The individual-level model of interest for Y|<strong>X</strong> is:
</p>
<p>Y = beta_0 + <strong>beta_x</strong>^T <strong>X</strong> + e, e ~ N(0, sigsq)
</p>
<p>The implied model for summed Y*|<strong>X*</strong> in a pool with g members is:
</p>
<p>Y* = g beta_0 + <strong>beta_x</strong>^T <strong>X*</strong> + e*, e* ~ N(0, g sigsq)
</p>
<p>The assay targets Ybar, the mean Y value for each pool, from which the sum Y*
can be calculated as Y* = g Ybar. But the Ybar's may be subject to processing
error and/or measurement error. Suppose Ybartilde is the imprecise version of
Ybar from the assay. If both errors are present, the assumed error structure
is:
</p>
<p>Ybartilde = Ybar + e_p I(g &gt; 1) + e_m, e_p ~ N(0, sigsq_p),
e_m ~ N(0, sigsq_m)
</p>
<p>with the processing error e_p and measurement error e_m assumed independent
of each other. This motivates a maximum likelihood analysis for estimating
<strong>theta</strong> = (beta_0, <strong>beta_x</strong>^T)^T based on observed
(Ytilde*, <strong>X</strong>*) values, where Ytilde* = g Ytildebar.
</p>


<h3>Value</h3>

<p>List containing:
</p>

<ol>
<li><p> Numeric vector of parameter estimates.
</p>
</li>
<li><p> Variance-covariance matrix (if <code>estimate_var = TRUE</code>).
</p>
</li>
<li><p> Returned <code><a href="stats.html#topic+nlminb">nlminb</a></code> object from maximizing the
log-likelihood function.
</p>
</li>
<li><p> Akaike information criterion (AIC).
</p>
</li></ol>



<h3>References</h3>

<p>Schisterman, E.F., Vexler, A., Mumford, S.L. and Perkins, N.J. (2010) &quot;Hybrid
pooled-unpooled design for cost-efficient measurement of biomarkers.&quot;
<em>Stat. Med.</em> <strong>29</strong>(5): 597&ndash;613.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load dataset containing data frame with (g, X1*, X2*, Y*, Ytilde*) values
# for 500 pools each of size 1, 2, and 3, and list of Ytilde values where 20
# of the single-specimen pools have replicates. Ytilde values are affected by
# processing error and measurement error; true parameter values are
# beta_0 = 0.25, beta_x1 = 0.5, beta_x2 = 0.25, sigsq = 1.
data(dat_p_linreg_yerrors)
dat &lt;- dat_p_linreg_yerrors$dat
reps &lt;- dat_p_linreg_yerrors$reps

# Fit Ytilde* vs. (X1*, X2*) ignoring errors in Ytilde (leads to loss of
# precision and overestimated sigsq, but no bias).
fit.naive &lt;- p_linreg_yerrors(
  g = dat$g,
  y = dat$y,
  x = dat[, c("x1", "x2")],
  errors = "neither"
)
fit.naive$theta.hat

# Account for errors in Ytilde*, without using replicates
fit.corrected.noreps &lt;- p_linreg_yerrors(
  g = dat$g,
  y = dat$ytilde,
  x = dat[, c("x1", "x2")],
  errors = "both"
)
fit.corrected.noreps$theta.hat

# Account for errors in Ytilde*, incorporating the 20 replicates
fit.corrected.reps &lt;- p_linreg_yerrors(
  g = dat$g,
  y = reps,
  x = dat[, c("x1", "x2")],
  errors = "both"
)
fit.corrected.reps$theta.hat

# In this trial, incorporating replicates resulted in much better estimates
# of sigsq (truly 1), sigsq_p (truly 0.4), and sigsq_m (truly = 0.2) but very
# similar regression coefficient estimates.
fit.corrected.noreps$theta.hat
fit.corrected.reps$theta.hat


</code></pre>

<hr>
<h2 id='p_logreg'>Poolwise Logistic Regression</h2><span id='topic+p_logreg'></span>

<h3>Description</h3>

<p>Fit homogeneous-pools logistic regression model described by Weinberg &amp;
Umbach (1999).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>p_logreg(g, y, x, method = "glm", prev = NULL, samp_y1y0 = NULL,
  estimate_var = TRUE, start = 0.01, lower = -Inf, upper = Inf,
  nlminb_list = list(control = list(trace = 1, eval.max = 500, iter.max =
  500)), hessian_list = list(method.args = list(r = 4)))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="p_logreg_+3A_g">g</code></td>
<td>
<p>Numeric vector with pool sizes, i.e. number of members in each pool.</p>
</td></tr>
<tr><td><code id="p_logreg_+3A_y">y</code></td>
<td>
<p>Numeric vector with poolwise <code>Y</code> values, coded 0 if all members
are controls and 1 if all members are cases.</p>
</td></tr>
<tr><td><code id="p_logreg_+3A_x">x</code></td>
<td>
<p>Numeric matrix with poolwise <strong><code>X</code></strong> values, with one row
for each pool. Can be a vector if there is only 1 predictor.</p>
</td></tr>
<tr><td><code id="p_logreg_+3A_method">method</code></td>
<td>
<p>Character string specifying method to use for estimation.
Choices are &quot;glm&quot; for <code><a href="stats.html#topic+glm">glm</a></code> function and <code>"ml"</code> for
maximum likelihood.</p>
</td></tr>
<tr><td><code id="p_logreg_+3A_prev">prev</code></td>
<td>
<p>Numeric value specifying disease prevalence, allowing
for valid estimation of the intercept with case-control sampling. Can specify
<code>samp_y1y0</code> instead if sampling rates are known.</p>
</td></tr>
<tr><td><code id="p_logreg_+3A_samp_y1y0">samp_y1y0</code></td>
<td>
<p>Numeric vector of length 2 specifying sampling probabilities
for cases and controls, allowing for valid estimation of the intercept with
case-control sampling. Can specify <code>prev</code> instead if it's easier.</p>
</td></tr>
<tr><td><code id="p_logreg_+3A_estimate_var">estimate_var</code></td>
<td>
<p>Logical value for whether to return variance-covariance
matrix for parameter estimates.</p>
</td></tr>
<tr><td><code id="p_logreg_+3A_start">start</code></td>
<td>
<p>Numeric value specifying starting values for parameters. Only
used if <code>method = "ml"</code>.</p>
</td></tr>
<tr><td><code id="p_logreg_+3A_lower">lower</code></td>
<td>
<p>Numeric value specifying lower bounds for parameters. Only used
if <code>method = "ml"</code>.</p>
</td></tr>
<tr><td><code id="p_logreg_+3A_upper">upper</code></td>
<td>
<p>Numeric value specifying upper bounds for parameters. Only used
if <code>method = "ml"</code>.</p>
</td></tr>
<tr><td><code id="p_logreg_+3A_nlminb_list">nlminb_list</code></td>
<td>
<p>List of arguments to pass to <code><a href="stats.html#topic+nlminb">nlminb</a></code>
for log-likelihood maximization.</p>
</td></tr>
<tr><td><code id="p_logreg_+3A_hessian_list">hessian_list</code></td>
<td>
<p>List of arguments to pass to
<code><a href="numDeriv.html#topic+hessian">hessian</a></code> for approximating the Hessian matrix. Only
used if <code>method = "ml"</code> and <code>estimate_var = TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing:
</p>

<ol>
<li><p> Numeric vector of parameter estimates.
</p>
</li>
<li><p> Variance-covariance matrix (if <code>estimate_var = TRUE</code>).
</p>
</li>
<li><p> Fitted <code><a href="stats.html#topic+glm">glm</a></code> object (if <code>method = "glm"</code>) or
returned <code><a href="stats.html#topic+nlminb">nlminb</a></code> object (if <code>method = "ml"</code>).
</p>
</li>
<li><p> Akaike information criterion (AIC).
</p>
</li></ol>



<h3>References</h3>

<p>Weinberg, C.R. and Umbach, D.M. (1999) &quot;Using pooled exposure assessment to
improve efficiency in case-control studies.&quot; <em>Biometrics</em> <strong>55</strong>:
718&ndash;726.
</p>
<p>Weinberg, C.R. and Umbach, D.M. (2014) &quot;Correction to 'Using pooled exposure
assessment to improve efficiency in case-control studies' by Clarice R.
Weinberg and David M. Umbach; 55, 718&ndash;726, September 1999.&quot;
<em>Biometrics</em> <strong>70</strong>: 1061.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load dataset containing (Y, Xtilde, C) values for pools of size 1, 2, and 3
data(pdat1)

# Estimate log-OR for Xtilde and Y adjusted for C
fit &lt;- p_logreg(g = pdat1$g, y = pdat1$allcases, x = pdat1[, c("xtilde", "c")])
fit$theta.hat


</code></pre>

<hr>
<h2 id='p_logreg_xerrors'>Poolwise Logistic Regression with Normal Exposure Subject to Errors</h2><span id='topic+p_logreg_xerrors'></span>

<h3>Description</h3>

<p>Assumes normal linear model for exposure given covariates, and additive
normal processing errors and measurement errors acting on the poolwise mean
exposure. Manuscript fully describing the approach is under review.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>p_logreg_xerrors(g, y, xtilde, c = NULL, errors = "processing",
  nondiff_pe = TRUE, nondiff_me = TRUE, constant_pe = TRUE,
  prev = NULL, samp_y1y0 = NULL, approx_integral = TRUE,
  estimate_var = TRUE, start_nonvar_var = c(0.01, 1),
  lower_nonvar_var = c(-Inf, 1e-04), upper_nonvar_var = c(Inf, Inf),
  jitter_start = 0.01, hcubature_list = list(tol = 1e-08),
  nlminb_list = list(control = list(trace = 1, eval.max = 500, iter.max =
  500)), hessian_list = list(method.args = list(r = 4)),
  nlminb_object = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="p_logreg_xerrors_+3A_g">g</code></td>
<td>
<p>Numeric vector with pool sizes, i.e. number of members in each pool.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors_+3A_y">y</code></td>
<td>
<p>Numeric vector with poolwise Y values, coded 0 if all members are
controls and 1 if all members are cases.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors_+3A_xtilde">xtilde</code></td>
<td>
<p>Numeric vector (or list of numeric vectors, if some pools have
replicates) with Xtilde values.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors_+3A_c">c</code></td>
<td>
<p>Numeric matrix with poolwise <strong>C</strong> values (if any), with one
row for each pool. Can be a vector if there is only 1 covariate.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors_+3A_errors">errors</code></td>
<td>
<p>Character string specifying the errors that X is subject to.
Choices are <code>"neither"</code>, <code>"processing"</code> for processing error
only, <code>"measurement"</code> for measurement error only, and <code>"both"</code>.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors_+3A_nondiff_pe">nondiff_pe</code></td>
<td>
<p>Logical value for whether to assume the processing error
variance is non-differential, i.e. the same in case pools and control pools.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors_+3A_nondiff_me">nondiff_me</code></td>
<td>
<p>Logical value for whether to assume the measurement error
variance is non-differential, i.e. the same in case pools and control pools.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors_+3A_constant_pe">constant_pe</code></td>
<td>
<p>Logical value for whether to assume the processing error
variance is constant with pool size. If <code>FALSE</code>, assumption is that
processing error variance increase with pool size such that, for example, the
processing error affecting a pool 2x as large as another has 2x the variance.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors_+3A_prev">prev</code></td>
<td>
<p>Numeric value specifying disease prevalence, allowing
for valid estimation of the intercept with case-control sampling. Can specify
<code>samp_y1y0</code> instead if sampling rates are known.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors_+3A_samp_y1y0">samp_y1y0</code></td>
<td>
<p>Numeric vector of length 2 specifying sampling probabilities
for cases and controls, allowing for valid estimation of the intercept with
case-control sampling. Can specify <code>prev</code> instead if it's easier.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors_+3A_approx_integral">approx_integral</code></td>
<td>
<p>Logical value for whether to use the probit
approximation for the logistic-normal integral, to avoid numerically
integrating X's out of the likelihood function.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors_+3A_estimate_var">estimate_var</code></td>
<td>
<p>Logical value for whether to return variance-covariance
matrix for parameter estimates.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors_+3A_start_nonvar_var">start_nonvar_var</code></td>
<td>
<p>Numeric vector of length 2 specifying starting value
for non-variance terms and variance terms, respectively.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors_+3A_lower_nonvar_var">lower_nonvar_var</code></td>
<td>
<p>Numeric vector of length 2 specifying lower bound for
non-variance terms and variance terms, respectively.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors_+3A_upper_nonvar_var">upper_nonvar_var</code></td>
<td>
<p>Numeric vector of length 2 specifying upper bound for
non-variance terms and variance terms, respectively.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors_+3A_jitter_start">jitter_start</code></td>
<td>
<p>Numeric value specifying standard deviation for mean-0
normal jitters to add to starting values for a second try at maximizing the
log-likelihood, should the initial call to <code><a href="stats.html#topic+nlminb">nlminb</a></code> result
in non-convergence. Set to <code>NULL</code> for no second try.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors_+3A_hcubature_list">hcubature_list</code></td>
<td>
<p>List of arguments to pass to
<code><a href="cubature.html#topic+hcubature">hcubature</a></code> for numerical integration. Only used if
<code>approx_integral = FALSE</code>.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors_+3A_nlminb_list">nlminb_list</code></td>
<td>
<p>List of arguments to pass to <code><a href="stats.html#topic+nlminb">nlminb</a></code>
for log-likelihood maximization.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors_+3A_hessian_list">hessian_list</code></td>
<td>
<p>List of arguments to pass to
<code><a href="numDeriv.html#topic+hessian">hessian</a></code> for approximating the Hessian matrix. Only
used if <code>estimate_var = TRUE</code>.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors_+3A_nlminb_object">nlminb_object</code></td>
<td>
<p>Object returned from <code><a href="stats.html#topic+nlminb">nlminb</a></code> in a
prior call. Useful for bypassing log-likelihood maximization if you just want
to re-estimate the Hessian matrix with different options.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing:
</p>

<ol>
<li><p> Numeric vector of parameter estimates.
</p>
</li>
<li><p> Variance-covariance matrix (if <code>estimate_var = TRUE</code>).
</p>
</li>
<li><p> Returned <code><a href="stats.html#topic+nlminb">nlminb</a></code> object from maximizing the
log-likelihood function.
</p>
</li>
<li><p> Akaike information criterion (AIC).
</p>
</li></ol>



<h3>References</h3>

<p>Schisterman, E.F., Vexler, A., Mumford, S.L. and Perkins, N.J. (2010) &quot;Hybrid
pooled-unpooled design for cost-efficient measurement of biomarkers.&quot;
<em>Stat. Med.</em> <strong>29</strong>(5): 597&ndash;613.
</p>
<p>Weinberg, C.R. and Umbach, D.M. (1999) &quot;Using pooled exposure assessment to
improve efficiency in case-control studies.&quot; <em>Biometrics</em> <strong>55</strong>:
718&ndash;726.
</p>
<p>Weinberg, C.R. and Umbach, D.M. (2014) &quot;Correction to 'Using pooled exposure
assessment to improve efficiency in case-control studies' by Clarice R.
Weinberg and David M. Umbach; 55, 718&ndash;726, September 1999.&quot;
<em>Biometrics</em> <strong>70</strong>: 1061.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load dataset containing (Y, Xtilde, C) values for pools of size 1, 2, and
# 3. Xtilde values are affected by processing error.
data(pdat1)

# Estimate log-OR for X and Y adjusted for C, ignoring processing error
fit1 &lt;- p_logreg_xerrors(
  g = pdat1$g,
  y = pdat1$allcases,
  xtilde = pdat1$xtilde,
  c = pdat1$c,
  errors = "neither"
)
fit1$theta.hat

# Repeat, but accounting for processing error. Closer to true log-OR of 0.5.
fit2 &lt;- p_logreg_xerrors(
  g = pdat1$g,
  y = pdat1$allcases,
  xtilde = pdat1$xtilde,
  c = pdat1$c,
  errors = "processing"
)
fit2$theta.hat


</code></pre>

<hr>
<h2 id='p_logreg_xerrors2'>Poolwise Logistic Regression with Gamma Exposure Subject to Errors</h2><span id='topic+p_logreg_xerrors2'></span>

<h3>Description</h3>

<p>Assumes constant-scale Gamma model for exposure given covariates, and
multiplicative lognormal processing errors and measurement errors acting on
the poolwise mean exposure. Manuscript fully describing the approach is
under review.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>p_logreg_xerrors2(g = NULL, y, xtilde, c = NULL,
  errors = "processing", nondiff_pe = TRUE, nondiff_me = TRUE,
  constant_pe = TRUE, prev = NULL, samp_y1y0 = NULL,
  estimate_var = TRUE, start_nonvar_var = c(0.01, 1),
  lower_nonvar_var = c(-Inf, 1e-04), upper_nonvar_var = c(Inf, Inf),
  jitter_start = 0.01, hcubature_list = list(tol = 1e-08),
  nlminb_list = list(control = list(trace = 1, eval.max = 500, iter.max =
  500)), hessian_list = list(method.args = list(r = 4)),
  nlminb_object = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="p_logreg_xerrors2_+3A_g">g</code></td>
<td>
<p>Numeric vector with pool sizes, i.e. number of members in each pool.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors2_+3A_y">y</code></td>
<td>
<p>Numeric vector with poolwise Y values, coded 0 if all members are
controls and 1 if all members are cases.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors2_+3A_xtilde">xtilde</code></td>
<td>
<p>Numeric vector (or list of numeric vectors, if some pools have
replicates) with Xtilde values.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors2_+3A_c">c</code></td>
<td>
<p>List where each element is a numeric matrix containing the
<strong>C</strong> values for members of a particular pool (1 row for each member).</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors2_+3A_errors">errors</code></td>
<td>
<p>Character string specifying the errors that X is subject to.
Choices are <code>"neither"</code>, <code>"processing"</code> for processing error
only, <code>"measurement"</code> for measurement error only, and <code>"both"</code>.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors2_+3A_nondiff_pe">nondiff_pe</code></td>
<td>
<p>Logical value for whether to assume the processing error
variance is non-differential, i.e. the same in case pools and control pools.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors2_+3A_nondiff_me">nondiff_me</code></td>
<td>
<p>Logical value for whether to assume the measurement error
variance is non-differential, i.e. the same in case pools and control pools.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors2_+3A_constant_pe">constant_pe</code></td>
<td>
<p>Logical value for whether to assume the processing error
variance is constant with pool size. If <code>FALSE</code>, assumption is that
processing error variance increase with pool size such that, for example, the
processing error affecting a pool 2x as large as another has 2x the variance.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors2_+3A_prev">prev</code></td>
<td>
<p>Numeric value specifying disease prevalence, allowing
for valid estimation of the intercept with case-control sampling. Can specify
<code>samp_y1y0</code> instead if sampling rates are known.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors2_+3A_samp_y1y0">samp_y1y0</code></td>
<td>
<p>Numeric vector of length 2 specifying sampling probabilities
for cases and controls, allowing for valid estimation of the intercept with
case-control sampling. Can specify <code>prev</code> instead if it's easier.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors2_+3A_estimate_var">estimate_var</code></td>
<td>
<p>Logical value for whether to return variance-covariance
matrix for parameter estimates.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors2_+3A_start_nonvar_var">start_nonvar_var</code></td>
<td>
<p>Numeric vector of length 2 specifying starting value
for non-variance terms and variance terms, respectively.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors2_+3A_lower_nonvar_var">lower_nonvar_var</code></td>
<td>
<p>Numeric vector of length 2 specifying lower bound for
non-variance terms and variance terms, respectively.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors2_+3A_upper_nonvar_var">upper_nonvar_var</code></td>
<td>
<p>Numeric vector of length 2 specifying upper bound for
non-variance terms and variance terms, respectively.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors2_+3A_jitter_start">jitter_start</code></td>
<td>
<p>Numeric value specifying standard deviation for mean-0
normal jitters to add to starting values for a second try at maximizing the
log-likelihood, should the initial call to <code><a href="stats.html#topic+nlminb">nlminb</a></code> result
in non-convergence. Set to <code>NULL</code> for no second try.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors2_+3A_hcubature_list">hcubature_list</code></td>
<td>
<p>List of arguments to pass to
<code><a href="cubature.html#topic+hcubature">hcubature</a></code> for numerical integration.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors2_+3A_nlminb_list">nlminb_list</code></td>
<td>
<p>List of arguments to pass to <code><a href="stats.html#topic+nlminb">nlminb</a></code>
for log-likelihood maximization.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors2_+3A_hessian_list">hessian_list</code></td>
<td>
<p>List of arguments to pass to
<code><a href="numDeriv.html#topic+hessian">hessian</a></code> for approximating the Hessian matrix. Only
used if <code>estimate_var = TRUE</code>.</p>
</td></tr>
<tr><td><code id="p_logreg_xerrors2_+3A_nlminb_object">nlminb_object</code></td>
<td>
<p>Object returned from <code><a href="stats.html#topic+nlminb">nlminb</a></code> in a
prior call. Useful for bypassing log-likelihood maximization if you just want
to re-estimate the Hessian matrix with different options.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing:
</p>

<ol>
<li><p> Numeric vector of parameter estimates.
</p>
</li>
<li><p> Variance-covariance matrix (if <code>estimate_var = TRUE</code>).
</p>
</li>
<li><p> Returned <code><a href="stats.html#topic+nlminb">nlminb</a></code> object from maximizing the
log-likelihood function.
</p>
</li>
<li><p> Akaike information criterion (AIC).
</p>
</li></ol>



<h3>References</h3>

<p>Mitchell, E.M, Lyles, R.H., and Schisterman, E.F. (2015) &quot;Positing, fitting,
and selecting regression models for pooled biomarker data.&quot; <em>Stat. Med</em>
<strong>34</strong>(17): 2544&ndash;2558.
</p>
<p>Schisterman, E.F., Vexler, A., Mumford, S.L. and Perkins, N.J. (2010) &quot;Hybrid
pooled-unpooled design for cost-efficient measurement of biomarkers.&quot;
<em>Stat. Med.</em> <strong>29</strong>(5): 597&ndash;613.
</p>
<p>Weinberg, C.R. and Umbach, D.M. (1999) &quot;Using pooled exposure assessment to
improve efficiency in case-control studies.&quot; <em>Biometrics</em> <strong>55</strong>:
718&ndash;726.
</p>
<p>Weinberg, C.R. and Umbach, D.M. (2014) &quot;Correction to 'Using pooled exposure
assessment to improve efficiency in case-control studies' by Clarice R.
Weinberg and David M. Umbach; 55, 718&ndash;726, September 1999.&quot;
<em>Biometrics</em> <strong>70</strong>: 1061.
</p>
<p>Whitcomb, B.W., Perkins, N.J., Zhang, Z., Ye, A., and Lyles, R. H. (2012)
&quot;Assessment of skewed exposure in case-control studies with pooling.&quot;
<em>Stat. Med.</em> <strong>31</strong>: 2461&ndash;2472.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load dataset with (g, Y, Xtilde, C) values for 248 pools and list of C
# values for members of each pool. Xtilde values are affected by processing
# error.
data(pdat2)
dat &lt;- pdat2$dat
c.list &lt;- pdat2$c.list

# Estimate log-OR for X and Y adjusted for C, ignoring processing error
fit1 &lt;- p_logreg_xerrors2(
  g = dat$g,
  y = dat$y,
  xtilde = dat$xtilde,
  c = c.list,
  errors = "neither"
)
fit1$theta.hat

# Repeat, but accounting for processing error.
## Not run: 
fit2 &lt;- p_logreg_xerrors2(
  g = dat$g,
  y = dat$y,
  xtilde = dat$xtilde,
  c = c.list,
  errors = "processing"
)
fit2$theta.hat

## End(Not run)


</code></pre>

<hr>
<h2 id='p_ndfa'>Normal Discriminant Function Approach for Estimating Odds Ratio with Exposure
Measured in Pools and Potentially Subject to Additive Normal Errors</h2><span id='topic+p_ndfa'></span>

<h3>Description</h3>

<p>Assumes exposure given covariates and outcome is a normal-errors linear
regression. Pooled exposure measurements can be assumed precise or subject to
additive normal processing error and/or measurement error. Parameters are
estimated using maximum likelihood.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>p_ndfa(g, y, xtilde, c = NULL, constant_or = TRUE,
  errors = "processing", start_nonvar_var = c(0.01, 1),
  lower_nonvar_var = c(-Inf, 1e-04), upper_nonvar_var = c(Inf, Inf),
  jitter_start = 0.01, nlminb_list = list(control = list(trace = 1,
  eval.max = 500, iter.max = 500)), hessian_list = list(method.args =
  list(r = 4)), nlminb_object = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="p_ndfa_+3A_g">g</code></td>
<td>
<p>Numeric vector of pool sizes, i.e. number of members in each pool.</p>
</td></tr>
<tr><td><code id="p_ndfa_+3A_y">y</code></td>
<td>
<p>Numeric vector of poolwise Y values (number of cases in each pool).</p>
</td></tr>
<tr><td><code id="p_ndfa_+3A_xtilde">xtilde</code></td>
<td>
<p>Numeric vector (or list of numeric vectors, if some pools have
replicates) with Xtilde values.</p>
</td></tr>
<tr><td><code id="p_ndfa_+3A_c">c</code></td>
<td>
<p>Numeric matrix with poolwise <strong>C</strong> values (if any), with one
row for each pool. Can be a vector if there is only 1 covariate.</p>
</td></tr>
<tr><td><code id="p_ndfa_+3A_constant_or">constant_or</code></td>
<td>
<p>Logical value for whether to assume a constant odds ratio
for X, which means that sigsq_1 = sigsq_0. If <code>NULL</code>, model is fit with
and without this assumption, and a likelihood ratio test is performed to test
it.</p>
</td></tr>
<tr><td><code id="p_ndfa_+3A_errors">errors</code></td>
<td>
<p>Character string specifying the errors that X is subject to.
Choices are <code>"neither"</code>, <code>"processing"</code> for processing error only,
<code>"measurement"</code> for measurement error only, and <code>"both"</code>.</p>
</td></tr>
<tr><td><code id="p_ndfa_+3A_start_nonvar_var">start_nonvar_var</code></td>
<td>
<p>Numeric vector of length 2 specifying starting value
for non-variance terms and variance terms, respectively.</p>
</td></tr>
<tr><td><code id="p_ndfa_+3A_lower_nonvar_var">lower_nonvar_var</code></td>
<td>
<p>Numeric vector of length 2 specifying lower bound for
non-variance terms and variance terms, respectively.</p>
</td></tr>
<tr><td><code id="p_ndfa_+3A_upper_nonvar_var">upper_nonvar_var</code></td>
<td>
<p>Numeric vector of length 2 specifying upper bound for
non-variance terms and variance terms, respectively.</p>
</td></tr>
<tr><td><code id="p_ndfa_+3A_jitter_start">jitter_start</code></td>
<td>
<p>Numeric value specifying standard deviation for mean-0
normal jitters to add to starting values for a second try at maximizing the
log-likelihood, should the initial call to <code><a href="stats.html#topic+nlminb">nlminb</a></code> result
in non-convergence. Set to <code>NULL</code> for no second try.</p>
</td></tr>
<tr><td><code id="p_ndfa_+3A_nlminb_list">nlminb_list</code></td>
<td>
<p>List of arguments to pass to <code><a href="stats.html#topic+nlminb">nlminb</a></code>
for log-likelihood maximization.</p>
</td></tr>
<tr><td><code id="p_ndfa_+3A_hessian_list">hessian_list</code></td>
<td>
<p>List of arguments to pass to
<code><a href="numDeriv.html#topic+hessian">hessian</a></code> for approximating the Hessian matrix. Only
used if <code>estimate_var = TRUE</code>.</p>
</td></tr>
<tr><td><code id="p_ndfa_+3A_nlminb_object">nlminb_object</code></td>
<td>
<p>Object returned from <code><a href="stats.html#topic+nlminb">nlminb</a></code> in a
prior call. Useful for bypassing log-likelihood maximization if you just want
to re-estimate the Hessian matrix with different options.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing:
</p>

<ol>
<li><p> Numeric vector of parameter estimates.
</p>
</li>
<li><p> Variance-covariance matrix.
</p>
</li>
<li><p> Returned <code><a href="stats.html#topic+nlminb">nlminb</a></code> object from maximizing the
log-likelihood function.
</p>
</li>
<li><p> Akaike information criterion (AIC).
</p>
</li></ol>

<p>If <code>constant_or = NULL</code>, two such lists are returned (one under a
constant odds ratio assumption and one not), along with a likelihood ratio
test for <code>H0: sigsq_1 = sigsq_0</code>, which is equivalent to
<code>H0: odds ratio is constant</code>.
</p>


<h3>References</h3>

<p>Lyles, R.H., Van Domelen, D.R., Mitchell, E.M. and Schisterman, E.F. (2015)
&quot;A discriminant function approach to adjust for processing and measurement
error When a biomarker is assayed in pooled samples.&quot;
<em>Int. J. Environ. Res. Public Health</em> <strong>12</strong>(11): 14723&ndash;14740.
</p>
<p>Schisterman, E.F., Vexler, A., Mumford, S.L. and Perkins, N.J. (2010) &quot;Hybrid
pooled-unpooled design for cost-efficient measurement of biomarkers.&quot;
<em>Stat. Med.</em> <strong>29</strong>(5): 597&ndash;613.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load data frame with (g, Y, X, Xtilde, C) values for 4,996 pools and list
# of Xtilde values where 25 subjects have replicates. Xtilde values are
# affected by processing error and measurement error. True log-OR = 0.5,
# sigsq = 1, sigsq_p = 0.5, sigsq_m = 0.1.
data(dat_p_ndfa)
dat &lt;- dat_p_ndfa$dat
reps &lt;- dat_p_ndfa$reps

# Unobservable truth estimator - use precise X's
fit.unobservable &lt;- p_ndfa(
  g = dat$g,
  y = dat$numcases,
  xtilde = dat$x,
  c = dat$c,
  errors = "neither"
)
fit.unobservable$estimates

# Naive estimator - use imprecise Xtilde's, but treat as precise
fit.naive &lt;- p_ndfa(
  g = dat$g,
  y = dat$numcases,
  xtilde = dat$xtilde,
  c = dat$c,
  errors = "neither"
)
fit.naive$estimates

# Corrected estimator - use Xtilde's and account for errors (not using
# replicates here)
## Not run: 
fit.noreps &lt;- p_ndfa(
  g = dat$g,
  y = dat$numcases,
  xtilde = dat$xtilde,
  c = dat$c,
  errors = "both"
)
fit.noreps$estimates

# Corrected estimator - use Xtilde's including 25 replicates
fit.reps &lt;- p_ndfa(
  g = dat$g,
  y = dat$numcases,
  xtilde = reps,
  c = dat$c,
  errors = "both"
)
fit.reps$estimates

# Same as previous, but allowing for non-constant odds ratio.
fit.nonconstant &lt;- p_ndfa(
  g = dat$g,
  y = dat$numcases,
  xtilde = reps,
  c = dat$c,
  constant_or = FALSE,
  errors = "both"
)
fit.nonconstant$estimates

# Visualize estimated log-OR vs. X based on previous model fit
p &lt;- plot_ndfa(
  estimates = fit.nonconstant$estimates,
  varcov = fit.nonconstant$theta.var,
  xrange = range(dat$xtilde[dat$g == 1]),
  cvals = mean(dat$c / dat$g)
)
p

# Likelihood ratio test for H0: odds ratio is constant.
test.constantOR &lt;- p_ndfa(
  g = dat$g,
  y = dat$numcases,
  xtilde = reps,
  c = dat$c,
  constant_or = NULL,
  errors = "both"
)
test.constantOR$lrt

## End(Not run)


</code></pre>

<hr>
<h2 id='p_ndfa_constant'>Normal Discriminant Function Approach for Estimating Odds Ratio with Exposure
Measured in Pools and Potentially Subject to Additive Normal Errors (Constant
Odds Ratio Version)</h2><span id='topic+p_ndfa_constant'></span>

<h3>Description</h3>

<p>Assumes exposure given covariates and outcome is a normal-errors linear
regression. Pooled exposure measurements can be assumed precise or subject to
additive normal processing error and/or measurement error. Parameters are
estimated using maximum likelihood.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>p_ndfa_constant(g, y, xtilde, c = NULL, errors = "processing",
  start_nonvar_var = c(0.01, 1), lower_nonvar_var = c(-Inf, 1e-04),
  upper_nonvar_var = c(Inf, Inf), jitter_start = 0.01,
  nlminb_list = list(control = list(trace = 1, eval.max = 500, iter.max =
  500)), hessian_list = list(method.args = list(r = 4)),
  nlminb_object = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="p_ndfa_constant_+3A_g">g</code></td>
<td>
<p>Numeric vector of pool sizes, i.e. number of members in each pool.</p>
</td></tr>
<tr><td><code id="p_ndfa_constant_+3A_y">y</code></td>
<td>
<p>Numeric vector of poolwise Y values (number of cases in each pool).</p>
</td></tr>
<tr><td><code id="p_ndfa_constant_+3A_xtilde">xtilde</code></td>
<td>
<p>Numeric vector (or list of numeric vectors, if some pools have
replicates) with Xtilde values.</p>
</td></tr>
<tr><td><code id="p_ndfa_constant_+3A_c">c</code></td>
<td>
<p>Numeric matrix with poolwise <strong>C</strong> values (if any), with one
row for each pool. Can be a vector if there is only 1 covariate.</p>
</td></tr>
<tr><td><code id="p_ndfa_constant_+3A_errors">errors</code></td>
<td>
<p>Character string specifying the errors that X is subject to.
Choices are <code>"neither"</code>, <code>"processing"</code> for processing error only,
<code>"measurement"</code> for measurement error only, and <code>"both"</code>.</p>
</td></tr>
<tr><td><code id="p_ndfa_constant_+3A_start_nonvar_var">start_nonvar_var</code></td>
<td>
<p>Numeric vector of length 2 specifying starting value
for non-variance terms and variance terms, respectively.</p>
</td></tr>
<tr><td><code id="p_ndfa_constant_+3A_lower_nonvar_var">lower_nonvar_var</code></td>
<td>
<p>Numeric vector of length 2 specifying lower bound for
non-variance terms and variance terms, respectively.</p>
</td></tr>
<tr><td><code id="p_ndfa_constant_+3A_upper_nonvar_var">upper_nonvar_var</code></td>
<td>
<p>Numeric vector of length 2 specifying upper bound for
non-variance terms and variance terms, respectively.</p>
</td></tr>
<tr><td><code id="p_ndfa_constant_+3A_jitter_start">jitter_start</code></td>
<td>
<p>Numeric value specifying standard deviation for mean-0
normal jitters to add to starting values for a second try at maximizing the
log-likelihood, should the initial call to <code><a href="stats.html#topic+nlminb">nlminb</a></code> result
in non-convergence. Set to <code>NULL</code> for no second try.</p>
</td></tr>
<tr><td><code id="p_ndfa_constant_+3A_nlminb_list">nlminb_list</code></td>
<td>
<p>List of arguments to pass to <code><a href="stats.html#topic+nlminb">nlminb</a></code>
for log-likelihood maximization.</p>
</td></tr>
<tr><td><code id="p_ndfa_constant_+3A_hessian_list">hessian_list</code></td>
<td>
<p>List of arguments to pass to
<code><a href="numDeriv.html#topic+hessian">hessian</a></code> for approximating the Hessian matrix. Only
used if <code>estimate_var = TRUE</code>.</p>
</td></tr>
<tr><td><code id="p_ndfa_constant_+3A_nlminb_object">nlminb_object</code></td>
<td>
<p>Object returned from <code><a href="stats.html#topic+nlminb">nlminb</a></code> in a
prior call. Useful for bypassing log-likelihood maximization if you just want
to re-estimate the Hessian matrix with different options.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing:
</p>

<ol>
<li><p> Numeric vector of parameter estimates.
</p>
</li>
<li><p> Variance-covariance matrix.
</p>
</li>
<li><p> Returned <code><a href="stats.html#topic+nlminb">nlminb</a></code> object from maximizing the
log-likelihood function.
</p>
</li>
<li><p> Akaike information criterion (AIC).
</p>
</li></ol>



<h3>References</h3>

<p>Lyles, R.H., Van Domelen, D.R., Mitchell, E.M. and Schisterman, E.F. (2015)
&quot;A discriminant function approach to adjust for processing and measurement
error When a biomarker is assayed in pooled samples.&quot;
<em>Int. J. Environ. Res. Public Health</em> <strong>12</strong>(11): 14723&ndash;14740.
</p>
<p>Schisterman, E.F., Vexler, A., Mumford, S.L. and Perkins, N.J. (2010) &quot;Hybrid
pooled-unpooled design for cost-efficient measurement of biomarkers.&quot;
<em>Stat. Med.</em> <strong>29</strong>(5): 597&ndash;613.
</p>

<hr>
<h2 id='p_ndfa_nonconstant'>Normal Discriminant Function Approach for Estimating Odds Ratio with Exposure
Measured in Pools and Potentially Subject to Additive Normal Errors
(Non-constant Odds Ratio Version)</h2><span id='topic+p_ndfa_nonconstant'></span>

<h3>Description</h3>

<p>Assumes exposure given covariates and outcome is a normal-errors linear
regression. Pooled exposure measurements can be assumed precise or subject to
additive normal processing error and/or measurement error. Parameters are
estimated using maximum likelihood.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>p_ndfa_nonconstant(g, y, xtilde, c = NULL, errors = "processing",
  start_nonvar_var = c(0.01, 1), lower_nonvar_var = c(-Inf, 1e-04),
  upper_nonvar_var = c(Inf, Inf), jitter_start = 0.01,
  nlminb_list = list(control = list(trace = 1, eval.max = 500, iter.max =
  500)), hessian_list = list(method.args = list(r = 4)),
  nlminb_object = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="p_ndfa_nonconstant_+3A_g">g</code></td>
<td>
<p>Numeric vector of pool sizes, i.e. number of members in each pool.</p>
</td></tr>
<tr><td><code id="p_ndfa_nonconstant_+3A_y">y</code></td>
<td>
<p>Numeric vector of poolwise Y values (number of cases in each pool).</p>
</td></tr>
<tr><td><code id="p_ndfa_nonconstant_+3A_xtilde">xtilde</code></td>
<td>
<p>Numeric vector (or list of numeric vectors, if some pools have
replicates) with Xtilde values.</p>
</td></tr>
<tr><td><code id="p_ndfa_nonconstant_+3A_c">c</code></td>
<td>
<p>Numeric matrix with poolwise <strong>C</strong> values (if any), with one
row for each pool. Can be a vector if there is only 1 covariate.</p>
</td></tr>
<tr><td><code id="p_ndfa_nonconstant_+3A_errors">errors</code></td>
<td>
<p>Character string specifying the errors that X is subject to.
Choices are <code>"neither"</code>, <code>"processing"</code> for processing error only,
<code>"measurement"</code> for measurement error only, and <code>"both"</code>.</p>
</td></tr>
<tr><td><code id="p_ndfa_nonconstant_+3A_start_nonvar_var">start_nonvar_var</code></td>
<td>
<p>Numeric vector of length 2 specifying starting value
for non-variance terms and variance terms, respectively.</p>
</td></tr>
<tr><td><code id="p_ndfa_nonconstant_+3A_lower_nonvar_var">lower_nonvar_var</code></td>
<td>
<p>Numeric vector of length 2 specifying lower bound for
non-variance terms and variance terms, respectively.</p>
</td></tr>
<tr><td><code id="p_ndfa_nonconstant_+3A_upper_nonvar_var">upper_nonvar_var</code></td>
<td>
<p>Numeric vector of length 2 specifying upper bound for
non-variance terms and variance terms, respectively.</p>
</td></tr>
<tr><td><code id="p_ndfa_nonconstant_+3A_jitter_start">jitter_start</code></td>
<td>
<p>Numeric value specifying standard deviation for mean-0
normal jitters to add to starting values for a second try at maximizing the
log-likelihood, should the initial call to <code><a href="stats.html#topic+nlminb">nlminb</a></code> result
in non-convergence. Set to <code>NULL</code> for no second try.</p>
</td></tr>
<tr><td><code id="p_ndfa_nonconstant_+3A_nlminb_list">nlminb_list</code></td>
<td>
<p>List of arguments to pass to <code><a href="stats.html#topic+nlminb">nlminb</a></code>
for log-likelihood maximization.</p>
</td></tr>
<tr><td><code id="p_ndfa_nonconstant_+3A_hessian_list">hessian_list</code></td>
<td>
<p>List of arguments to pass to
<code><a href="numDeriv.html#topic+hessian">hessian</a></code> for approximating the Hessian matrix. Only
used if <code>estimate_var = TRUE</code>.</p>
</td></tr>
<tr><td><code id="p_ndfa_nonconstant_+3A_nlminb_object">nlminb_object</code></td>
<td>
<p>Object returned from <code><a href="stats.html#topic+nlminb">nlminb</a></code> in a
prior call. Useful for bypassing log-likelihood maximization if you just want
to re-estimate the Hessian matrix with different options.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing:
</p>

<ol>
<li><p> Numeric vector of parameter estimates.
</p>
</li>
<li><p> Variance-covariance matrix.
</p>
</li>
<li><p> Returned <code><a href="stats.html#topic+nlminb">nlminb</a></code> object from maximizing the
log-likelihood function.
</p>
</li>
<li><p> Akaike information criterion (AIC).
</p>
</li></ol>



<h3>References</h3>

<p>Lyles, R.H., Van Domelen, D.R., Mitchell, E.M. and Schisterman, E.F. (2015)
&quot;A discriminant function approach to adjust for processing and measurement
error When a biomarker is assayed in pooled samples.&quot;
<em>Int. J. Environ. Res. Public Health</em> <strong>12</strong>(11): 14723&ndash;14740.
</p>
<p>Schisterman, E.F., Vexler, A., Mumford, S.L. and Perkins, N.J. (2010) &quot;Hybrid
pooled-unpooled design for cost-efficient measurement of biomarkers.&quot;
<em>Stat. Med.</em> <strong>29</strong>(5): 597&ndash;613.
</p>

<hr>
<h2 id='pdat1'>Dataset for Examples in p_dfa_xerrors and p_logreg_xerrors</h2><span id='topic+pdat1'></span>

<h3>Description</h3>

<p>Data frame with poolwise (g, Y*, Y, Xtilde, C) values.
</p>


<h3>Source</h3>

<p>Simulated data in R.
</p>

<hr>
<h2 id='pdat2'>Dataset for Examples in p_dfa_xerrors2 and p_logreg_xerrors2</h2><span id='topic+pdat2'></span>

<h3>Description</h3>

<p>List containing (1) data frame with poolwise (g, Y, Xtilde, C) values and
(2) list of C values for members of each pool.
</p>


<h3>Source</h3>

<p>Simulated data in R.
</p>

<hr>
<h2 id='plot_dfa'>Plot Log-OR vs. X for Normal Discriminant Function Approach</h2><span id='topic+plot_dfa'></span>

<h3>Description</h3>

<p>Archived on 7/23/2018. Please use <code><a href="#topic+plot_ndfa">plot_ndfa</a></code> instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_dfa(estimates, varcov = NULL, xrange, xname = "X", cvals = NULL,
  set_labels = NULL, set_panels = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_dfa_+3A_estimates">estimates</code></td>
<td>
<p>Numeric vector of point estimates for
<code>(gamma_0, gamma_y, gamma_c^T, sigsq)</code>.</p>
</td></tr>
<tr><td><code id="plot_dfa_+3A_varcov">varcov</code></td>
<td>
<p>Numeric matrix with variance-covariance matrix for
<code>estimates</code>. If <code>NULL</code>, 95% confidence bands are omitted.</p>
</td></tr>
<tr><td><code id="plot_dfa_+3A_xrange">xrange</code></td>
<td>
<p>Numeric vector specifying range of X values to plot.</p>
</td></tr>
<tr><td><code id="plot_dfa_+3A_xname">xname</code></td>
<td>
<p>Character vector specifying name of X variable, for
plot title and x-axis label.</p>
</td></tr>
<tr><td><code id="plot_dfa_+3A_cvals">cvals</code></td>
<td>
<p>Numeric vector or list of numeric vectors specifying covariate
values to use in log-odds ratio calculations.</p>
</td></tr>
<tr><td><code id="plot_dfa_+3A_set_labels">set_labels</code></td>
<td>
<p>Character vector of labels for the sets of covariate
values. Only used if <code>cvals</code> is a list.</p>
</td></tr>
<tr><td><code id="plot_dfa_+3A_set_panels">set_panels</code></td>
<td>
<p>Logical value for whether to use separate panels for each
set of covariate values, as opposed to using different colors on a single
plot.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Plot of log-OR vs. <code>X</code> generated by
<code><a href="ggplot2.html#topic+ggplot">ggplot</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Fit discriminant function model for poolwise Xtilde vs. (Y, C), without
# assuming a constant log-OR. Ignoring processing errors for simplicity.
data(pdat1)
fit &lt;- p_dfa_xerrors(g = pdat1$g, y = pdat1$numcases, xtilde = pdat1$xtilde,
                     c = pdat1$c, errors = "neither", constant_or = FALSE)

# Plot estimated log-OR vs. X at mean value for C
p &lt;- plot_dfa(estimates = fit$estimates, varcov = fit$theta.var,
              xrange = range(pdat1$xtilde / pdat1$g),
              cvals = mean(pdat1$c / pdat1$g))
p


</code></pre>

<hr>
<h2 id='plot_dfa2'>Plot Log-OR vs. X for Gamma Discriminant Function Approach</h2><span id='topic+plot_dfa2'></span>

<h3>Description</h3>

<p>Archived on 7/23/2018. Please use <code><a href="#topic+plot_gdfa">plot_gdfa</a></code> instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_dfa2(estimates, varcov = NULL, xrange, xname = "X",
  cvals = NULL, set_labels = NULL, set_panels = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_dfa2_+3A_estimates">estimates</code></td>
<td>
<p>Numeric vector of point estimates for
<code>(gamma_0, gamma_y, gamma_c^T, b1, b0)</code>.</p>
</td></tr>
<tr><td><code id="plot_dfa2_+3A_varcov">varcov</code></td>
<td>
<p>Numeric matrix with variance-covariance matrix for
<code>estimates</code>. If <code>NULL</code>, 95% confidence bands are omitted.</p>
</td></tr>
<tr><td><code id="plot_dfa2_+3A_xrange">xrange</code></td>
<td>
<p>Numeric vector specifying range of X values to plot.</p>
</td></tr>
<tr><td><code id="plot_dfa2_+3A_xname">xname</code></td>
<td>
<p>Character vector specifying name of X variable, for
plot title and x-axis label.</p>
</td></tr>
<tr><td><code id="plot_dfa2_+3A_cvals">cvals</code></td>
<td>
<p>Numeric vector or list of numeric vectors specifying covariate
values to use in log-odds ratio calculations.</p>
</td></tr>
<tr><td><code id="plot_dfa2_+3A_set_labels">set_labels</code></td>
<td>
<p>Character vector of labels for the sets of covariate
values. Only used if <code>cvals</code> is a list.</p>
</td></tr>
<tr><td><code id="plot_dfa2_+3A_set_panels">set_panels</code></td>
<td>
<p>Logical value for whether to use separate panels for each
set of covariate values, as opposed to using different colors on a single
plot.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Plot of log-OR vs. <code>X</code> generated by
<code><a href="ggplot2.html#topic+ggplot">ggplot</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Fit Gamma discriminant function model for poolwise Xtilde vs. (Y, C),
# without assuming a constant log-OR. Ignoring processing errors for simplicity.
data(pdat2)
dat &lt;- pdat2$dat
c.list &lt;- pdat2$c.list
fit &lt;- p_dfa_xerrors2(
  g = dat$g,
  y = dat$y,
  xtilde = dat$xtilde,
  c = c.list,
  errors = "neither",
  constant_or = FALSE
)

# Plot estimated log-OR vs. X at mean value for C
p &lt;- plot_dfa2(
  estimates = fit$estimates,
  varcov = fit$theta.var,
  xrange = range(dat$xtilde / dat$g),
  cvals = mean(unlist(c.list))
)
p


</code></pre>

<hr>
<h2 id='plot_gdfa'>Plot Log-OR vs. X for Gamma Discriminant Function Approach</h2><span id='topic+plot_gdfa'></span>

<h3>Description</h3>

<p>When <code><a href="#topic+p_gdfa">p_gdfa</a></code> is fit with <code>constant_or = FALSE</code>, the
log-OR for X depends on the value of X (and covariates, if any). This
function plots the log-OR vs. X for one or several sets of covariate values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_gdfa(estimates, varcov = NULL, xrange, xname = "X",
  cvals = NULL, set_labels = NULL, set_panels = TRUE, ncol = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_gdfa_+3A_estimates">estimates</code></td>
<td>
<p>Numeric vector of point estimates for
<code>(gamma_0, gamma_y, gamma_c^T, b1, b0)</code>.</p>
</td></tr>
<tr><td><code id="plot_gdfa_+3A_varcov">varcov</code></td>
<td>
<p>Numeric matrix with variance-covariance matrix for
<code>estimates</code>. If <code>NULL</code>, 95% confidence bands are omitted.</p>
</td></tr>
<tr><td><code id="plot_gdfa_+3A_xrange">xrange</code></td>
<td>
<p>Numeric vector specifying range of X values to plot.</p>
</td></tr>
<tr><td><code id="plot_gdfa_+3A_xname">xname</code></td>
<td>
<p>Character vector specifying name of X variable, for
plot title and x-axis label.</p>
</td></tr>
<tr><td><code id="plot_gdfa_+3A_cvals">cvals</code></td>
<td>
<p>Numeric vector or list of numeric vectors specifying covariate
values to use in log-odds ratio calculations.</p>
</td></tr>
<tr><td><code id="plot_gdfa_+3A_set_labels">set_labels</code></td>
<td>
<p>Character vector of labels for the sets of covariate
values. Only used if <code>cvals</code> is a list.</p>
</td></tr>
<tr><td><code id="plot_gdfa_+3A_set_panels">set_panels</code></td>
<td>
<p>Logical value for whether to use separate panels for each
set of covariate values, as opposed to using different colors on a single
plot.</p>
</td></tr>
<tr><td><code id="plot_gdfa_+3A_ncol">ncol</code></td>
<td>
<p>Integer value specifying number of columns for multi-panel
figure. Only used if there are multiple sets of covariate values (i.e.
<code>cvals</code> is a list).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Plot of log-OR vs. X generated by <code><a href="ggplot2.html#topic+ggplot">ggplot</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Fit Gamma discriminant function model for poolwise X vs. (Y, C), without
# assuming a constant log-OR. Note that data were generated with a constant
# log-OR of 0.5.
data(dat_p_gdfa)
dat &lt;- dat_p_gdfa$dat
c.list &lt;- dat_p_gdfa$c.list
fit &lt;- p_gdfa(
  g = dat$g,
  y = dat$y,
  xtilde = dat$x,
  c = c.list,
  errors = "neither",
  constant_or = FALSE
)

# Plot estimated log-OR vs. X, holding C fixed at the sample mean.
p &lt;- plot_gdfa(
  estimates = fit$estimates,
  varcov = fit$theta.var,
  xrange = range(dat$x[dat$g == 1]),
  cvals = mean(unlist(c.list))
)
p


</code></pre>

<hr>
<h2 id='plot_ndfa'>Plot Log-OR vs. X for Normal Discriminant Function Approach</h2><span id='topic+plot_ndfa'></span>

<h3>Description</h3>

<p>When <code><a href="#topic+p_ndfa">p_ndfa</a></code> is fit with <code>constant_or = FALSE</code>, the
log-OR for X depends on the value of X (and covariates, if any). This
function plots the log-OR vs. X for one or several sets of covariate values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_ndfa(estimates, varcov = NULL, xrange, xname = "X",
  cvals = NULL, set_labels = NULL, set_panels = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_ndfa_+3A_estimates">estimates</code></td>
<td>
<p>Numeric vector of point estimates for
<code>(gamma_0, gamma_y, gamma_c^T, sigsq)</code>.</p>
</td></tr>
<tr><td><code id="plot_ndfa_+3A_varcov">varcov</code></td>
<td>
<p>Numeric matrix with variance-covariance matrix for
<code>estimates</code>. If <code>NULL</code>, 95% confidence bands are omitted.</p>
</td></tr>
<tr><td><code id="plot_ndfa_+3A_xrange">xrange</code></td>
<td>
<p>Numeric vector specifying range of X values to plot.</p>
</td></tr>
<tr><td><code id="plot_ndfa_+3A_xname">xname</code></td>
<td>
<p>Character vector specifying name of X variable, for
plot title and x-axis label.</p>
</td></tr>
<tr><td><code id="plot_ndfa_+3A_cvals">cvals</code></td>
<td>
<p>Numeric vector or list of numeric vectors specifying covariate
values to use in log-odds ratio calculations.</p>
</td></tr>
<tr><td><code id="plot_ndfa_+3A_set_labels">set_labels</code></td>
<td>
<p>Character vector of labels for the sets of covariate
values. Only used if <code>cvals</code> is a list.</p>
</td></tr>
<tr><td><code id="plot_ndfa_+3A_set_panels">set_panels</code></td>
<td>
<p>Logical value for whether to use separate panels for each
set of covariate values, as opposed to using different colors on a single
plot.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Plot of log-OR vs. X generated by <code><a href="ggplot2.html#topic+ggplot">ggplot</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Fit discriminant function model for poolwise X vs. (Y, C), without assuming
# a constant log-OR. Note that data were generated with a constant log-OR of
# 0.5.
data(dat_p_ndfa)
dat &lt;- dat_p_ndfa$dat
fit &lt;- p_ndfa(
  g = dat$g,
  y = dat$numcases,
  xtilde = dat$x,
  c = dat$c,
  errors = "neither",
  constant_or = FALSE
)

# Plot estimated log-OR vs. X, holding C fixed at the sample mean.
p &lt;- plot_ndfa(
  estimates = fit$estimates,
  varcov = fit$theta.var,
  xrange = range(dat$x[dat$g == 1]),
  cvals = mean(dat$c / dat$g)
)
p


</code></pre>

<hr>
<h2 id='poolcost_t'>Visualize Total Costs for Pooling Design as a Function of Pool Size</h2><span id='topic+poolcost_t'></span>

<h3>Description</h3>

<p>Useful for determining whether pooling is a good idea, what pool size
minimizes costs, and how many assays are needed for a target power.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>poolcost_t(g = 1:10, d = NULL, mu1 = NULL, mu2 = NULL,
  sigsq = NULL, sigsq1 = sigsq, sigsq2 = sigsq, sigsq_p = 0,
  sigsq_m = 0, multiplicative = FALSE, alpha = 0.05, beta = 0.2,
  assay_cost = 100, other_costs = 0, labels = TRUE, ylim = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="poolcost_t_+3A_g">g</code></td>
<td>
<p>Numeric vector of pool sizes to include.</p>
</td></tr>
<tr><td><code id="poolcost_t_+3A_d">d</code></td>
<td>
<p>Numeric value specifying true difference in group means.</p>
</td></tr>
<tr><td><code id="poolcost_t_+3A_mu1">mu1</code>, <code id="poolcost_t_+3A_mu2">mu2</code></td>
<td>
<p>Numeric value specifying group means. Required if
<code>multiplicative = TRUE</code>.</p>
</td></tr>
<tr><td><code id="poolcost_t_+3A_sigsq">sigsq</code></td>
<td>
<p>Numeric value specifying the variance of observations.</p>
</td></tr>
<tr><td><code id="poolcost_t_+3A_sigsq1">sigsq1</code>, <code id="poolcost_t_+3A_sigsq2">sigsq2</code></td>
<td>
<p>Numeric value specifying the variance of observations
for each group.</p>
</td></tr>
<tr><td><code id="poolcost_t_+3A_sigsq_p">sigsq_p</code></td>
<td>
<p>Numeric value specifying the variance of processing errors.</p>
</td></tr>
<tr><td><code id="poolcost_t_+3A_sigsq_m">sigsq_m</code></td>
<td>
<p>Numeric value specifying the variance of measurement errors.</p>
</td></tr>
<tr><td><code id="poolcost_t_+3A_multiplicative">multiplicative</code></td>
<td>
<p>Logical value for whether to assume multiplicative
rather than additive errors.</p>
</td></tr>
<tr><td><code id="poolcost_t_+3A_alpha">alpha</code></td>
<td>
<p>Numeric value specifying type-1 error rate.</p>
</td></tr>
<tr><td><code id="poolcost_t_+3A_beta">beta</code></td>
<td>
<p>Numeric value specifying type-2 error rate.</p>
</td></tr>
<tr><td><code id="poolcost_t_+3A_assay_cost">assay_cost</code></td>
<td>
<p>Numeric value specifying cost of each assay.</p>
</td></tr>
<tr><td><code id="poolcost_t_+3A_other_costs">other_costs</code></td>
<td>
<p>Numeric value specifying other per-subject costs.</p>
</td></tr>
<tr><td><code id="poolcost_t_+3A_labels">labels</code></td>
<td>
<p>Logical value.</p>
</td></tr>
<tr><td><code id="poolcost_t_+3A_ylim">ylim</code></td>
<td>
<p>Numeric vector.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Plot of total costs vs. pool size generated by
<code><a href="ggplot2.html#topic+ggplot">ggplot</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Plot total study costs vs. pool size for d = 0.25, sigsq = 1, and costs of
# $100 per assay and $0 in other per-subject costs.
poolcost_t(d = 0.25, sigsq = 1)

# Repeat but with additive processing error and $10 in per-subject costs.
poolcost_t(d = 0.25, sigsq = 1, sigsq_p = 0.5, other_costs = 10)


</code></pre>

<hr>
<h2 id='poolcushion_t'>Visualize T-test Power for Pooling Design as Function of Processing Error
Variance</h2><span id='topic+poolcushion_t'></span>

<h3>Description</h3>

<p>Useful for choosing a sample size such that power will be adequate even if
the processing errors are larger than anticipated.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>poolcushion_t(g = NULL, n = NULL, d = NULL, mu1 = NULL,
  mu2 = NULL, sigsq = NULL, sigsq1 = sigsq, sigsq2 = sigsq,
  sigsq_p_predicted = 0, sigsq_p_range = NULL, sigsq_m = 0,
  multiplicative = FALSE, alpha = 0.05, beta = 0.2, labels = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="poolcushion_t_+3A_g">g</code></td>
<td>
<p>Numeric value specifying the pool size.</p>
</td></tr>
<tr><td><code id="poolcushion_t_+3A_n">n</code></td>
<td>
<p>Numeric value specifying the number of assays per group. If
unspecified, function figures out <code>n</code> required for
100 (1 - <code>beta</code>)% power when <code>sigsq_p = 0</code>.</p>
</td></tr>
<tr><td><code id="poolcushion_t_+3A_d">d</code></td>
<td>
<p>Numeric value specifying true difference in group means.</p>
</td></tr>
<tr><td><code id="poolcushion_t_+3A_mu1">mu1</code>, <code id="poolcushion_t_+3A_mu2">mu2</code></td>
<td>
<p>Numeric value specifying group means. Required if
<code>multiplicative = TRUE</code>.</p>
</td></tr>
<tr><td><code id="poolcushion_t_+3A_sigsq">sigsq</code></td>
<td>
<p>Numeric value specifying the variance of observations.</p>
</td></tr>
<tr><td><code id="poolcushion_t_+3A_sigsq1">sigsq1</code>, <code id="poolcushion_t_+3A_sigsq2">sigsq2</code></td>
<td>
<p>Numeric value specifying the variance of observations
for each group.</p>
</td></tr>
<tr><td><code id="poolcushion_t_+3A_sigsq_p_predicted">sigsq_p_predicted</code></td>
<td>
<p>Numeric value specifying predicted processing error
variance. Used to calculate <code>n</code> if <code>n</code> is unspecified.</p>
</td></tr>
<tr><td><code id="poolcushion_t_+3A_sigsq_p_range">sigsq_p_range</code></td>
<td>
<p>Numeric vector specifying range of processing error
variances to consider.</p>
</td></tr>
<tr><td><code id="poolcushion_t_+3A_sigsq_m">sigsq_m</code></td>
<td>
<p>Numeric value specifying the variance of measurement errors.</p>
</td></tr>
<tr><td><code id="poolcushion_t_+3A_multiplicative">multiplicative</code></td>
<td>
<p>Logical value for whether to assume multiplicative
rather than additive errors.</p>
</td></tr>
<tr><td><code id="poolcushion_t_+3A_alpha">alpha</code></td>
<td>
<p>Numeric value specifying type-1 error rate.</p>
</td></tr>
<tr><td><code id="poolcushion_t_+3A_beta">beta</code></td>
<td>
<p>Numeric value specifying type-2 error rate. Only used if
<code>n = NULL</code>.</p>
</td></tr>
<tr><td><code id="poolcushion_t_+3A_labels">labels</code></td>
<td>
<p>Logical value.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Plot generated by <code><a href="ggplot2.html#topic+ggplot">ggplot</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Determine optimal pool size and number of assays to detect a difference in
# group means of 0.5, with a common variance of 1, processing errors with
# variance of 0.1, and measurement errors with variance of 0.2. Assume costs
# of $100 per assay and $10 per subject.
poolcost_t(
  g = 1: 10,
  d = 0.5,
  sigsq = 1,
  sigsq_p = 0.1,
  sigsq_m = 0.2,
  assay_cost = 100,
  other_costs = 10
)

# Visualize how power of the study will be affected if the true processing
# error variance is not exactly 0.1.
poolcushion_t(
  g = 7,
  n = 29,
  d = 0.5,
  sigsq = 1,
  sigsq_p_predicted = 0.1,
  sigsq_m = 0.2
)


</code></pre>

<hr>
<h2 id='poolpower_t'>Visualize T-test Power for Pooling Design</h2><span id='topic+poolpower_t'></span>

<h3>Description</h3>

<p>Useful for assessing efficiency gains that might be achieved with a pooling
design.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>poolpower_t(g = c(1, 3, 10), d = NULL, mu1 = NULL, mu2 = NULL,
  sigsq = NULL, sigsq1 = sigsq, sigsq2 = sigsq, sigsq_p = 0,
  sigsq_m = 0, multiplicative = FALSE, alpha = 0.05, beta = 0.2,
  assay_cost = 100, other_costs = 0, labels = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="poolpower_t_+3A_g">g</code></td>
<td>
<p>Numeric vector of pool sizes to include.</p>
</td></tr>
<tr><td><code id="poolpower_t_+3A_d">d</code></td>
<td>
<p>Numeric value specifying true difference in group means.</p>
</td></tr>
<tr><td><code id="poolpower_t_+3A_mu1">mu1</code>, <code id="poolpower_t_+3A_mu2">mu2</code></td>
<td>
<p>Numeric value specifying group means. Required if
<code>multiplicative = TRUE</code>.</p>
</td></tr>
<tr><td><code id="poolpower_t_+3A_sigsq">sigsq</code></td>
<td>
<p>Numeric value specifying the variance of observations.</p>
</td></tr>
<tr><td><code id="poolpower_t_+3A_sigsq1">sigsq1</code>, <code id="poolpower_t_+3A_sigsq2">sigsq2</code></td>
<td>
<p>Numeric value specifying the variance of observations
for each group.</p>
</td></tr>
<tr><td><code id="poolpower_t_+3A_sigsq_p">sigsq_p</code></td>
<td>
<p>Numeric value specifying the variance of processing errors.</p>
</td></tr>
<tr><td><code id="poolpower_t_+3A_sigsq_m">sigsq_m</code></td>
<td>
<p>Numeric value specifying the variance of measurement errors.</p>
</td></tr>
<tr><td><code id="poolpower_t_+3A_multiplicative">multiplicative</code></td>
<td>
<p>Logical value for whether to assume multiplicative
rather than additive errors.</p>
</td></tr>
<tr><td><code id="poolpower_t_+3A_alpha">alpha</code></td>
<td>
<p>Numeric value specifying type-1 error rate.</p>
</td></tr>
<tr><td><code id="poolpower_t_+3A_beta">beta</code></td>
<td>
<p>Numeric value specifying type-2 error rate.</p>
</td></tr>
<tr><td><code id="poolpower_t_+3A_assay_cost">assay_cost</code></td>
<td>
<p>Numeric value specifying cost of each assay.</p>
</td></tr>
<tr><td><code id="poolpower_t_+3A_other_costs">other_costs</code></td>
<td>
<p>Numeric value specifying other per-subject costs.</p>
</td></tr>
<tr><td><code id="poolpower_t_+3A_labels">labels</code></td>
<td>
<p>Logical value.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Plot of power vs. total costs generated by
<code><a href="ggplot2.html#topic+ggplot">ggplot</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Plot power vs. total study costs for d = 0.25, sigsq = 1, and costs of $100
# per assay and $0 in other per-subject costs.
poolpower_t(d = 0.5, sigsq = 1, assay_cost = 100, other_costs = 0)

# Repeat but with $10 in per-subject costs.
poolpower_t(d = 0.5, sigsq = 1, assay_cost = 100, other_costs = 10)

# Back to no per-subject costs, but with processing and measurement error
poolpower_t(d = 0.5, sigsq = 1, sigsq_p = 0.2, sigsq_m = 0.1,
            assay_cost = 100, other_costs = 0)


</code></pre>

<hr>
<h2 id='poolvar_t'>Visualize Ratio of Variance of Each Pooled Measurement to Variance of Each
Unpooled Measurement as Function of Pool Size</h2><span id='topic+poolvar_t'></span>

<h3>Description</h3>

<p>Useful for determining whether pooling is a good idea, and finding the
optimal pool size if it is.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>poolvar_t(g = 1:10, mu1 = NULL, mu2 = NULL, sigsq = NULL,
  sigsq1 = sigsq, sigsq2 = sigsq, sigsq_p = 0, sigsq_m = 0,
  multiplicative = FALSE, assay_cost = 100, other_costs = 0,
  labels = TRUE, ylim = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="poolvar_t_+3A_g">g</code></td>
<td>
<p>Numeric vector of pool sizes to include.</p>
</td></tr>
<tr><td><code id="poolvar_t_+3A_mu1">mu1</code>, <code id="poolvar_t_+3A_mu2">mu2</code></td>
<td>
<p>Numeric value specifying group means. Required if
<code>multiplicative = TRUE</code>.</p>
</td></tr>
<tr><td><code id="poolvar_t_+3A_sigsq">sigsq</code></td>
<td>
<p>Numeric value specifying the variance of observations.</p>
</td></tr>
<tr><td><code id="poolvar_t_+3A_sigsq1">sigsq1</code>, <code id="poolvar_t_+3A_sigsq2">sigsq2</code></td>
<td>
<p>Numeric value specifying the variance of observations
for each group.</p>
</td></tr>
<tr><td><code id="poolvar_t_+3A_sigsq_p">sigsq_p</code></td>
<td>
<p>Numeric value specifying the variance of processing errors.</p>
</td></tr>
<tr><td><code id="poolvar_t_+3A_sigsq_m">sigsq_m</code></td>
<td>
<p>Numeric value specifying the variance of measurement errors.</p>
</td></tr>
<tr><td><code id="poolvar_t_+3A_multiplicative">multiplicative</code></td>
<td>
<p>Logical value for whether to assume multiplicative
rather than additive errors.</p>
</td></tr>
<tr><td><code id="poolvar_t_+3A_assay_cost">assay_cost</code></td>
<td>
<p>Numeric value specifying cost of each assay.</p>
</td></tr>
<tr><td><code id="poolvar_t_+3A_other_costs">other_costs</code></td>
<td>
<p>Numeric value specifying other per-subject costs.</p>
</td></tr>
<tr><td><code id="poolvar_t_+3A_labels">labels</code></td>
<td>
<p>Logical value.</p>
</td></tr>
<tr><td><code id="poolvar_t_+3A_ylim">ylim</code></td>
<td>
<p>Numeric vector.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Plot generated by <code><a href="ggplot2.html#topic+ggplot">ggplot</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Plot ratio of variances vs. pool size with default settings
poolvar_t(sigsq = 1)

# Add processing error and other per-subject costs
poolvar_t(sigsq = 1, sigsq_p = 0.2, other_costs = 0.1)


</code></pre>

<hr>
<h2 id='simdata'>Dataset for a Paper Under Review</h2><span id='topic+simdata'></span>

<h3>Description</h3>

<p>Simulated data intended to mimic the motivating example from a paper under
review. Generated under GLR with true log-OR = 0.05.
</p>


<h3>Source</h3>

<p>Simulated data in R.
</p>

<hr>
<h2 id='test_pe'>Test for Underestimated Processing Error Variance in Pooling Studies</h2><span id='topic+test_pe'></span>

<h3>Description</h3>

<p>In studies where a biomarker is measured in combined samples from multiple
subjects rather than for each individual, design parameters (e.g. optimal
pool size, sample size for 80% power) are very sensitive to the magnitude of
processing errors. This function provides a test that can be used midway
through data collection to test whether the processing error variance is
larger than initially assumed, in which case the pool size may need to be
adjusted.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>test_pe(xtilde, g, sigsq, sigsq_m = 0, multiplicative = FALSE,
  mu = NULL, alpha = 0.05, boots = 1000, seed = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="test_pe_+3A_xtilde">xtilde</code></td>
<td>
<p>Numeric vector of pooled measurements.</p>
</td></tr>
<tr><td><code id="test_pe_+3A_g">g</code></td>
<td>
<p>Numeric value specifying the pool size.</p>
</td></tr>
<tr><td><code id="test_pe_+3A_sigsq">sigsq</code></td>
<td>
<p>Numeric value specifying the variance of observations.</p>
</td></tr>
<tr><td><code id="test_pe_+3A_sigsq_m">sigsq_m</code></td>
<td>
<p>Numeric value specifying the variance of measurement errors.</p>
</td></tr>
<tr><td><code id="test_pe_+3A_multiplicative">multiplicative</code></td>
<td>
<p>Logical value for whether to assume multiplicative
rather than additive errors.</p>
</td></tr>
<tr><td><code id="test_pe_+3A_mu">mu</code></td>
<td>
<p>Numeric value specifying the mean of observations. Only used if
<code>multiplicative = TRUE</code>.</p>
</td></tr>
<tr><td><code id="test_pe_+3A_alpha">alpha</code></td>
<td>
<p>Numeric value specifying significance level for bootstrap
confidence interval.</p>
</td></tr>
<tr><td><code id="test_pe_+3A_boots">boots</code></td>
<td>
<p>Numeric value specifying the number of bootstrap samples to
take.</p>
</td></tr>
<tr><td><code id="test_pe_+3A_seed">seed</code></td>
<td>
<p>Numeric value specifying the random number seed, in case it is
important to be able to reproduce the lower bound.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The method is fully described in a manuscript currently under review.
Briefly, the test of interest is <code>H0: sigsq_p &lt;= c</code>, where
<code>sigsq_p</code> is the processing error variance and <code>c</code> is the value
assumed during study design. Under additive errors, a point estimate for
<code>sigsq_p</code> is given by:
</p>
<p><code>sigsq_p.hat = s2 - sigsq / g - sigsq_m</code>
</p>
<p>where <code>s2</code> is the sample variance of poolwise measurements, <code>g</code> is
the pool size, and <code>sigsq_m</code> is the measurement error variance which may
be 0 if the assay is known to be precise.
</p>
<p>Under multiplicative errors, the estimator is:
</p>
<p><code>sigsq_p.hat = [(s2 - sigsq / g) / (mu^2 + sigsq / g) - sigsq_m] /
(1 + sigsq_m)</code>.
</p>
<p>In either case, bootstrapping can be used to obtain a lower bound for a
one-sided confidence interval. If the lower bound is greater than <code>c</code>,
<code>H0</code> is rejected.
</p>


<h3>Value</h3>

<p>List containing point estimate and lower bound of confidence
interval.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate data for hypothetical study designed assuming sigsq_p = 0.1, but
# truly sigsq_p = 0.25. Have data collected for 40 pools of size 5, and wish
# to test H0: sigsq_p &lt;= 0.1. In this instance, a false negative occurs.
set.seed(123)
xtilde &lt;- replicate(n = 40, expr = mean(rnorm(5)) + rnorm(n = 1, sd = sqrt(0.25)))
(fit &lt;- test_pe(xtilde = xtilde, g = 5, sigsq = 1, sigsq_m = 0))


</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
