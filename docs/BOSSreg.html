<!DOCTYPE html><html><head><title>Help for package BOSSreg</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {BOSSreg}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#boss'><p>Best Orthogonalized Subset Selection (BOSS).</p></a></li>
<li><a href='#calc.ic'><p>Calculate an information criterion.</p></a></li>
<li><a href='#coef.boss'><p>Select coefficient vector(s) for BOSS.</p></a></li>
<li><a href='#coef.cv.boss'><p>Select coefficient vector based on cross-validation for BOSS or FS.</p></a></li>
<li><a href='#cv.boss'><p>Cross-validation for Best Orthogonalized Subset Selection (BOSS) and Forward Stepwise Selection (FS).</p></a></li>
<li><a href='#predict.boss'><p>Prediction given new data entries.</p></a></li>
<li><a href='#predict.cv.boss'><p>Prediction given new data entries.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Best Orthogonalized Subset Selection (BOSS)</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2021-3-6</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Sen Tian &lt;stian@stern.nyu.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Best Orthogonalized Subset Selection (BOSS) is a least-squares (LS) based subset selection method, that performs best subset selection upon an orthogonalized basis of ordered predictors, with the computational effort of a single ordinary LS fit. This package provides a highly optimized implementation of BOSS and estimates a heuristic degrees of freedom for BOSS, which can be plugged into an information criterion (IC) such as AICc in order to select the subset from candidates. It provides various choices of IC, including AIC, BIC, AICc, Cp and GCV. It also implements the forward stepwise selection (FS) with no additional computational cost, where the subset of FS is selected via cross-validation (CV). CV is also an option for BOSS. For details see: Tian, Hurvich and Simonoff (2021), "On the Use of Information Criteria for Subset Selection in Least Squares Regression", &lt;<a href="https://arxiv.org/abs/1911.10191">arXiv:1911.10191</a>&gt;.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Imports:</td>
<td>glmnet, Matrix, Rcpp, stats</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>Suggests:</td>
<td>devtools, ISLR, kableExtra, knitr, MASS, rmarkdown, sparsenet</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/sentian/BOSSreg">https://github.com/sentian/BOSSreg</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/sentian/BOSSreg/issues">https://github.com/sentian/BOSSreg/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-03-06 18:56:57 UTC; sentian</td>
</tr>
<tr>
<td>Author:</td>
<td>Sen Tian [aut, cre],
  Clifford Hurvich [aut],
  Jeffrey Simonoff [aut]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-03-06 19:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='boss'>Best Orthogonalized Subset Selection (BOSS).</h2><span id='topic+boss'></span>

<h3>Description</h3>


<ul>
<li><p> Compute the solution path of BOSS and forward stepwise selection (FS).
</p>
</li>
<li><p> Compute various information criteria based on a heuristic degrees of freedom (hdf)
that can serve as the selection rule to choose the subset given by BOSS.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>boss(
  x,
  y,
  maxstep = min(nrow(x) - intercept - 1, ncol(x)),
  intercept = TRUE,
  hdf.ic.boss = TRUE,
  mu = NULL,
  sigma = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="boss_+3A_x">x</code></td>
<td>
<p>A matrix of predictors, with <code>nrow(x)=length(y)=n</code> observations and
<code>ncol(x)=p</code> predictors. Intercept shall NOT be included.</p>
</td></tr>
<tr><td><code id="boss_+3A_y">y</code></td>
<td>
<p>A vector of response variable, with <code>length(y)=n</code>.</p>
</td></tr>
<tr><td><code id="boss_+3A_maxstep">maxstep</code></td>
<td>
<p>Maximum number of steps performed. Default is <code>min(n-1,p)</code> if <code>intercept=FALSE</code>,
and it is <code>min(n-2, p)</code> otherwise.</p>
</td></tr>
<tr><td><code id="boss_+3A_intercept">intercept</code></td>
<td>
<p>Logical, whether to include an intercept term. Default is TRUE.</p>
</td></tr>
<tr><td><code id="boss_+3A_hdf.ic.boss">hdf.ic.boss</code></td>
<td>
<p>Logical, whether to calculate the heuristic degrees of freedom (hdf)
and information criteria (IC) for BOSS. IC includes AIC, BIC, AICc, BICc, GCV,
Cp. Default is TRUE.</p>
</td></tr>
<tr><td><code id="boss_+3A_mu">mu</code></td>
<td>
<p>True mean vector, used in the calculation of hdf. Default is NULL, and is estimated via
least-squares (LS) regression of y upon x for n&gt;p, and 10-fold CV cross-validated lasso estimate for n&lt;=p.</p>
</td></tr>
<tr><td><code id="boss_+3A_sigma">sigma</code></td>
<td>
<p>True standard deviation of the error, used in the calculation of hdf. Default is NULL,
and is estimated via least-squares (LS) regression of y upon x for n&gt;p, and 10-fold cross-validated lasso
for n&lt;=p.</p>
</td></tr>
<tr><td><code id="boss_+3A_...">...</code></td>
<td>
<p>Extra parameters to allow flexibility. Currently none allows or requires, just for
the convinience of call from other parent functions like cv.boss.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes the full solution path given by BOSS and FS on a given
dataset (x,y) with n observations and p predictors. It also calculates
the heuristic degrees of freedom for BOSS, and various information criteria, which can further
be used to select the subset from the candidates. Please refer to the Vignette
for implementation details and Tian et al. (2021) for methodology details (links are given below).
</p>


<h3>Value</h3>


<ul>
<li><p> beta_fs: A matrix of regression coefficients for all the subsets given by FS,
from a null model until stop, with <code>nrow=p</code> and <code>ncol=min(n,p)+1</code>, where <code>min(n,p)</code> is
the maximum number of steps performed.
</p>
</li>
<li><p> beta_boss: A matrix of regression coefficients for all the subsets given by
BOSS, with <code>nrow=p</code> and <code>ncol=min(n,p)+1</code>. Note that unlike beta_fs and due to the nature of BOSS,
the number of non-zero components in columns of beta_boss may not be unique, i.e.
there maybe multiple columns corresponding to the same size of subset.
</p>
</li>
<li><p> steps_x: A vector of numbers representing which predictor joins at each step,
with <code>length(steps)=min(n,p)</code>. The ordering is determined by the partial correlation between a predictor <code class="reqn">x_j</code>
and the response <code>y</code>.
</p>
</li>
<li><p> steps_q: A vector of numbers representing which predictor joins at each step in the orthogonal basis,
with <code>length(steps)=min(n,p)</code>. BOSS takes the ordered predictors (ordering given in <code>steps_x</code>) and performs best
subset regression upon their orthogonal basis, which is essentially ordering the orthogonalized predictors by their
marginal correlations with the response <code>y</code>. For example, <code>steps_q=c(2,1)</code> indicates that the orthogonal basis of
<code>x_2</code> joins first.
</p>
</li>
<li><p> hdf_boss: A vector of heuristic degrees of freedom (hdf) for BOSS, with
<code>length(hdf_boss)=p+1</code>. Note that <code>hdf_boss=NULL</code> if n&lt;=p or <code>hdf.ic.boss=FALSE</code>.
</p>
</li>
<li><p> IC_boss: A list of information criteria (IC) for BOSS, where each element
in the list is a vector representing values of a given IC for each candidate subset
of BOSS (or each column in beta_boss). The output IC includes AIC, BIC, AICc, BICc,
GCV and Mallows' Cp. Note that each IC is calculated by plugging in hdf_boss.
</p>
</li>
<li><p> sigma: estimated error standard deviation. It is only returned when hdf is calculated, i.e. <code>hdf.ic.boss=TRUE</code>.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Sen Tian
</p>


<h3>References</h3>


<ul>
<li><p> Tian, S., Hurvich, C. and Simonoff, J. (2021), On the Use of Information Criteria
for Subset Selection in Least Squares Regression. https://arxiv.org/abs/1911.10191
</p>
</li>
<li><p> Reid, S., Tibshirani, R. and Friedman, J. (2016), A Study of Error Variance Estimation in Lasso Regression. Statistica Sinica,
P35-67, JSTOR.
</p>
</li>
<li><p> BOSSreg Vignette https://github.com/sentian/BOSSreg/blob/master/r-package/vignettes/BOSSreg.pdf
</p>
</li></ul>



<h3>See Also</h3>

<p><code>predict</code> and  <code>coef</code> methods for &quot;boss&quot; object, and the <code>cv.boss</code> function
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Generate a trivial dataset, X has mean 0 and norm 1, y has mean 0
set.seed(11)
n = 20
p = 5
x = matrix(rnorm(n*p), nrow=n, ncol=p)
x = scale(x, center = colMeans(x))
x = scale(x, scale = sqrt(colSums(x^2)))
beta = c(1, 1, 0, 0, 0)
y = x%*%beta + scale(rnorm(n, sd=0.01), center = TRUE, scale = FALSE)

## Fit the model
boss_result = boss(x, y)

## Get the coefficient vector selected by AICc-hdf (S3 method for boss)
beta_boss_aicc = coef(boss_result)
# the above is equivalent to the following
beta_boss_aicc = boss_result$beta_boss[, which.min(boss_result$IC_boss$aicc), drop=FALSE]
## Get the fitted values of BOSS-AICc-hdf (S3 method for boss)
mu_boss_aicc = predict(boss_result, newx=x)
# the above is equivalent to the following
mu_boss_aicc = cbind(1,x) %*% beta_boss_aicc

## Repeat the above process, but using Cp-hdf instead of AICc-hdf
## coefficient vector
beta_boss_cp = coef(boss_result, method.boss='cp')
beta_boss_cp = boss_result$beta_boss[, which.min(boss_result$IC_boss$cp), drop=FALSE]
## fitted values of BOSS-Cp-hdf
mu_boss_cp = predict(boss_result, newx=x, method.boss='cp')
mu_boss_cp = cbind(1,x) %*% beta_boss_cp
</code></pre>

<hr>
<h2 id='calc.ic'>Calculate an information criterion.</h2><span id='topic+calc.ic'></span>

<h3>Description</h3>

<p>Calculate a specified information criterion (IC) for an estimate or a group of estimates.
The choices of IC include AIC, BIC, AICc, BICc, GCV and Mallows' Cp.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc.ic(
  y_hat,
  y,
  ic = c("aicc", "bicc", "aic", "bic", "gcv", "cp"),
  df,
  sigma = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calc.ic_+3A_y_hat">y_hat</code></td>
<td>
<p>A vector of fitted values with <code>length(y_hat)=length(y)=n</code>, or
a matrix, with <code>nrow(coef)=length(y)=n</code> and <code>ncol(y_hat)=m</code>, containing m different fits.</p>
</td></tr>
<tr><td><code id="calc.ic_+3A_y">y</code></td>
<td>
<p>A vector of response variable, with <code>length(y)=n</code>.</p>
</td></tr>
<tr><td><code id="calc.ic_+3A_ic">ic</code></td>
<td>
<p>A specified IC to calculate. Default is AICc ('aicc'). Other choices include AIC ('aic'),
BIC ('bic'), BICc ('bicc'), GCV ('gcv') and Mallows' Cp ('cp').</p>
</td></tr>
<tr><td><code id="calc.ic_+3A_df">df</code></td>
<td>
<p>A number if y_hat is a vector, or a vector with <code>length(df)=ncol(y_hat)=m</code> if y_hat is
a matrix. df represents the degrees of freedom for each fit.</p>
</td></tr>
<tr><td><code id="calc.ic_+3A_sigma">sigma</code></td>
<td>
<p>Standard deviation of the error term. It only needs to be specified if the argument <code>ic='cp'</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function enables the computation of various common IC for model fits, which can
further be used to choose the optimal fit. This allows user comparing the effect of different IC.
In order to calculate an IC, degrees of freedoms (df) needs to be specified. To be more specific,
here are the formulas used to calculate each IC:
</p>
<p style="text-align: center;"><code class="reqn">AIC = \log(\frac{RSS}{n}) + 2\frac{df}{n}</code>
</p>

<p style="text-align: center;"><code class="reqn">BIC = \log(\frac{RSS}{n}) + \log(n)\frac{df}{n}</code>
</p>

<p style="text-align: center;"><code class="reqn">AICc = \log(\frac{RSS}{n}) + 2\frac{df+1}{n-df-2}</code>
</p>

<p style="text-align: center;"><code class="reqn">BICc = \log(\frac{RSS}{n}) + \log(n)\frac{df+1}{n-df-2}</code>
</p>

<p style="text-align: center;"><code class="reqn">GCV = \frac{RSS}{(n-df)^2}</code>
</p>

<p style="text-align: center;"><code class="reqn">Mallows' Cp = RSS + 2\times \sigma^2 \times df</code>
</p>



<h3>Value</h3>

<p>The value(s) of the specified IC for each fit.
</p>


<h3>Author(s)</h3>

<p>Sen Tian
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Generate a trivial dataset, X has mean 0 and norm 1, y has mean 0
set.seed(11)
n = 20
p = 5
x = matrix(rnorm(n*p), nrow=n, ncol=p)
x = scale(x, center = colMeans(x))
x = scale(x, scale = sqrt(colSums(x^2)))
beta = c(1, 1, 0, 0, 0)
y = x%*%beta + scale(rnorm(20, sd=0.01), center = TRUE, scale = FALSE)

## Fit the model
boss_result = boss(x, y)
## Print the values of AICc-hdf for all subsets given by BOSS
print(boss_result$IC_boss$aicc)
## calculate them manually using the calc.ic function
y_hat = cbind(rep(1,n),x)%*%boss_result$beta_boss
print(calc.ic(y_hat, y, df=boss_result$hdf_boss))
</code></pre>

<hr>
<h2 id='coef.boss'>Select coefficient vector(s) for BOSS.</h2><span id='topic+coef.boss'></span>

<h3>Description</h3>

<p>This function returns the optimal coefficient vector of BOSS selected by AICc
(by default) or other types of information criterion.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'boss'
coef(
  object,
  ic = c("aicc", "bicc", "aic", "bic", "gcv", "cp"),
  select.boss = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef.boss_+3A_object">object</code></td>
<td>
<p>The boss object, returned from calling the <code>boss</code> function.</p>
</td></tr>
<tr><td><code id="coef.boss_+3A_ic">ic</code></td>
<td>
<p>Which information criterion is used to select the optimal coefficient vector for BOSS.
The default is AICc-hdf.</p>
</td></tr>
<tr><td><code id="coef.boss_+3A_select.boss">select.boss</code></td>
<td>
<p>The index (or indicies) of columns in the coefficient matrix for which
one wants to select. By default (NULL) it's selected by the information criterion specified in
'ic'.</p>
</td></tr>
<tr><td><code id="coef.boss_+3A_...">...</code></td>
<td>
<p>Extra arguments (unused for now)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>select.boss</code> is specified, the function returns
corresponding column(s) in the coefficient matrix.
</p>
<p>If <code>select.boss</code> is unspecified, the function returns the optimal coefficient
vector selected by AICc-hdf (other choice of IC can be specified in the argument <code>ic</code>).
</p>


<h3>Value</h3>

<p>The chosen coefficient vector(s) for BOSS.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Generate a trivial dataset, X has mean 0 and norm 1, y has mean 0
set.seed(11)
n = 20
p = 5
x = matrix(rnorm(n*p), nrow=n, ncol=p)
x = scale(x, center = colMeans(x))
x = scale(x, scale = sqrt(colSums(x^2)))
beta = c(1, 1, 0, 0, 0)
y = x%*%beta + scale(rnorm(n, sd=0.01), center = TRUE, scale = FALSE)

## Fit the model
boss_result = boss(x, y)

## Get the coefficient vector selected by AICc-hdf (S3 method for boss)
beta_boss_aicc = coef(boss_result)
# the above is equivalent to the following
beta_boss_aicc = boss_result$beta_boss[, which.min(boss_result$IC_boss$aicc), drop=FALSE]
## Get the fitted values of BOSS-AICc-hdf (S3 method for boss)
mu_boss_aicc = predict(boss_result, newx=x)
# the above is equivalent to the following
mu_boss_aicc = cbind(1,x) %*% beta_boss_aicc

## Repeat the above process, but using Cp-hdf instead of AICc-hdf
## coefficient vector
beta_boss_cp = coef(boss_result, method.boss='cp')
beta_boss_cp = boss_result$beta_boss[, which.min(boss_result$IC_boss$cp), drop=FALSE]
## fitted values of BOSS-Cp-hdf
mu_boss_cp = predict(boss_result, newx=x, method.boss='cp')
mu_boss_cp = cbind(1,x) %*% beta_boss_cp
</code></pre>

<hr>
<h2 id='coef.cv.boss'>Select coefficient vector based on cross-validation for BOSS or FS.</h2><span id='topic+coef.cv.boss'></span>

<h3>Description</h3>

<p>This function returns coefficient vector that minimizes out-of-sample (OOS) cross
validation score.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cv.boss'
coef(object, method = c("boss", "fs"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef.cv.boss_+3A_object">object</code></td>
<td>
<p>The cv.boss object, returned from calling <code>cv.boss</code> function.</p>
</td></tr>
<tr><td><code id="coef.cv.boss_+3A_method">method</code></td>
<td>
<p>It can either be 'fs' or 'boss'. The default is 'boss'.</p>
</td></tr>
<tr><td><code id="coef.cv.boss_+3A_...">...</code></td>
<td>
<p>Extra arguments (unused for now).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The chosen coefficient vector for BOSS or FS.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Generate a trivial dataset, X has mean 0 and norm 1, y has mean 0
set.seed(11)
n = 20
p = 5
x = matrix(rnorm(n*p), nrow=n, ncol=p)
x = scale(x, center = colMeans(x))
x = scale(x, scale = sqrt(colSums(x^2)))
beta = c(1, 1, 0, 0, 0)
y = x%*%beta + scale(rnorm(20, sd=0.01), center = TRUE, scale = FALSE)

## Perform 10-fold CV without replication
boss_cv_result = cv.boss(x, y)
## Get the coefficient vector of BOSS that gives minimum CV OSS score (S3 method for cv.boss)
beta_boss_cv = coef(boss_cv_result)
# the above is equivalent to
boss_result = boss_cv_result$boss
beta_boss_cv = boss_result$beta_boss[, boss_cv_result$i.min.boss, drop=FALSE]
## Get the fitted values of BOSS-CV (S3 method for cv.boss)
mu_boss_cv = predict(boss_cv_result, newx=x)
# the above is equivalent to
mu_boss_cv = cbind(1,x) %*% beta_boss_cv

## Get the coefficient vector of FS that gives minimum CV OSS score (S3 method for cv.boss)
beta_fs_cv = coef(boss_cv_result, method='fs')
## Get the fitted values of FS-CV (S3 method for cv.boss)
mu_fs_cv = predict(boss_cv_result, newx=x, method='fs')
</code></pre>

<hr>
<h2 id='cv.boss'>Cross-validation for Best Orthogonalized Subset Selection (BOSS) and Forward Stepwise Selection (FS).</h2><span id='topic+cv.boss'></span>

<h3>Description</h3>

<p>Cross-validation for Best Orthogonalized Subset Selection (BOSS) and Forward Stepwise Selection (FS).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.boss(
  x,
  y,
  maxstep = min(nrow(x) - intercept - 1, ncol(x)),
  intercept = TRUE,
  n.folds = 10,
  n.rep = 1,
  show.warning = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv.boss_+3A_x">x</code></td>
<td>
<p>A matrix of predictors, see <code>boss</code>.</p>
</td></tr>
<tr><td><code id="cv.boss_+3A_y">y</code></td>
<td>
<p>A vector of response variable, see <code>boss</code>.</p>
</td></tr>
<tr><td><code id="cv.boss_+3A_maxstep">maxstep</code></td>
<td>
<p>Maximum number of steps performed. Default is <code>min(n-1,p)</code> if <code>intercept=FALSE</code>,
and it is <code>min(n-2, p)</code> otherwise.</p>
</td></tr>
<tr><td><code id="cv.boss_+3A_intercept">intercept</code></td>
<td>
<p>Logical, whether to fit an intercept term. Default is TRUE.</p>
</td></tr>
<tr><td><code id="cv.boss_+3A_n.folds">n.folds</code></td>
<td>
<p>The number of cross validation folds. Default is 10.</p>
</td></tr>
<tr><td><code id="cv.boss_+3A_n.rep">n.rep</code></td>
<td>
<p>The number of replications of cross validation. Default is 1.</p>
</td></tr>
<tr><td><code id="cv.boss_+3A_show.warning">show.warning</code></td>
<td>
<p>Whether to display a warning if CV is only performed for a subset of candidates.
e.g. when n&lt;p and 10-fold. Default is TRUE.</p>
</td></tr>
<tr><td><code id="cv.boss_+3A_...">...</code></td>
<td>
<p>Arguments to <code>boss</code>, such as <code>hdf.ic.boss</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function fits BOSS and FS (<code>boss</code>) on the full dataset, and performs <code>n.folds</code>
cross-validation. The cross-validation process can be repeated <code>n.rep</code> times to evaluate the
out-of-sample (OOS) performance for the candidate subsets given by both methods.
</p>


<h3>Value</h3>


<ul>
<li><p> boss: An object <code>boss</code> that fits on the full dataset.
</p>
</li>
<li><p> n.folds: The number of cross validation folds.
</p>
</li>
<li><p> cvm.fs: Mean OOS deviance for each candidate given by FS.
</p>
</li>
<li><p> cvm.boss: Mean OSS deviance for each candidate given by BOSS.
</p>
</li>
<li><p> i.min.fs: The index of minimum cvm.fs.
</p>
</li>
<li><p> i.min.boss: The index of minimum cvm.boss.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Sen Tian
</p>


<h3>References</h3>


<ul>
<li><p> Tian, S., Hurvich, C. and Simonoff, J. (2021), On the Use of Information Criteria
for Subset Selection in Least Squares Regression. https://arxiv.org/abs/1911.10191
</p>
</li>
<li><p> BOSSreg Vignette https://github.com/sentian/BOSSreg/blob/master/r-package/vignettes/BOSSreg.pdf
</p>
</li></ul>



<h3>See Also</h3>

<p><code>predict</code> and <code>coef</code> methods for <code>cv.boss</code> object, and the <code>boss</code> function
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Generate a trivial dataset, X has mean 0 and norm 1, y has mean 0
set.seed(11)
n = 20
p = 5
x = matrix(rnorm(n*p), nrow=n, ncol=p)
x = scale(x, center = colMeans(x))
x = scale(x, scale = sqrt(colSums(x^2)))
beta = c(1, 1, 0, 0, 0)
y = x%*%beta + scale(rnorm(20, sd=0.01), center = TRUE, scale = FALSE)

## Perform 10-fold CV without replication
boss_cv_result = cv.boss(x, y)
## Get the coefficient vector of BOSS that gives minimum CV OSS score (S3 method for cv.boss)
beta_boss_cv = coef(boss_cv_result)
# the above is equivalent to
boss_result = boss_cv_result$boss
beta_boss_cv = boss_result$beta_boss[, boss_cv_result$i.min.boss, drop=FALSE]
## Get the fitted values of BOSS-CV (S3 method for cv.boss)
mu_boss_cv = predict(boss_cv_result, newx=x)
# the above is equivalent to
mu_boss_cv = cbind(1,x) %*% beta_boss_cv

## Get the coefficient vector of FS that gives minimum CV OSS score (S3 method for cv.boss)
beta_fs_cv = coef(boss_cv_result, method='fs')
## Get the fitted values of FS-CV (S3 method for cv.boss)
mu_fs_cv = predict(boss_cv_result, newx=x, method='fs')
</code></pre>

<hr>
<h2 id='predict.boss'>Prediction given new data entries.</h2><span id='topic+predict.boss'></span>

<h3>Description</h3>

<p>This function returns the prediction(s) given new observation(s), for BOSS,
where the optimal coefficient vector is chosen via certain selection rule.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'boss'
predict(object, newx, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.boss_+3A_object">object</code></td>
<td>
<p>The boss object, returned from calling 'boss' function.</p>
</td></tr>
<tr><td><code id="predict.boss_+3A_newx">newx</code></td>
<td>
<p>A new data entry or several entries. It can be a vector, or a matrix with
<code>nrow(newx)</code> being the number of new entries and <code>ncol(newx)=p</code> being the
number of predictors. The function takes care of the intercept, NO need to add <code>1</code>
to <code>newx</code>.</p>
</td></tr>
<tr><td><code id="predict.boss_+3A_...">...</code></td>
<td>
<p>Extra arguments to be plugged into <code>coef</code>, such as <code>select.boss</code>,
see the description of <code>coef.boss</code> for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function basically calculates <code class="reqn">x * coef</code>, where <code>coef</code>
is a coefficient vector chosen by a selection rule. See more details about the default
and available choices of the selection rule in the description of <code>coef.boss</code>.
</p>


<h3>Value</h3>

<p>The prediction(s) for BOSS.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Generate a trivial dataset, X has mean 0 and norm 1, y has mean 0
set.seed(11)
n = 20
p = 5
x = matrix(rnorm(n*p), nrow=n, ncol=p)
x = scale(x, center = colMeans(x))
x = scale(x, scale = sqrt(colSums(x^2)))
beta = c(1, 1, 0, 0, 0)
y = x%*%beta + scale(rnorm(n, sd=0.01), center = TRUE, scale = FALSE)

## Fit the model
boss_result = boss(x, y)

## Get the coefficient vector selected by AICc-hdf (S3 method for boss)
beta_boss_aicc = coef(boss_result)
# the above is equivalent to the following
beta_boss_aicc = boss_result$beta_boss[, which.min(boss_result$IC_boss$aicc), drop=FALSE]
## Get the fitted values of BOSS-AICc-hdf (S3 method for boss)
mu_boss_aicc = predict(boss_result, newx=x)
# the above is equivalent to the following
mu_boss_aicc = cbind(1,x) %*% beta_boss_aicc

## Repeat the above process, but using Cp-hdf instead of AICc-hdf
## coefficient vector
beta_boss_cp = coef(boss_result, method.boss='cp')
beta_boss_cp = boss_result$beta_boss[, which.min(boss_result$IC_boss$cp), drop=FALSE]
## fitted values of BOSS-Cp-hdf
mu_boss_cp = predict(boss_result, newx=x, method.boss='cp')
mu_boss_cp = cbind(1,x) %*% beta_boss_cp
</code></pre>

<hr>
<h2 id='predict.cv.boss'>Prediction given new data entries.</h2><span id='topic+predict.cv.boss'></span>

<h3>Description</h3>

<p>This function returns the prediction(s) given new observation(s) for BOSS or FS,
where the optimal coefficient vector is chosen via cross-validation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cv.boss'
predict(object, newx, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.cv.boss_+3A_object">object</code></td>
<td>
<p>The cv.boss object, returned from calling <code>cv.boss</code> function.</p>
</td></tr>
<tr><td><code id="predict.cv.boss_+3A_newx">newx</code></td>
<td>
<p>A new data entry or several entries. It can be a vector, or a matrix with
<code>nrow(newx)</code> being the number of new entries and <code>ncol(newx)=p</code> being the
number of predictors. The function takes care of the intercept, NO need to add <code>1</code>
to <code>newx</code>.</p>
</td></tr>
<tr><td><code id="predict.cv.boss_+3A_...">...</code></td>
<td>
<p>Extra arguments to be plugged into <code>coef</code>, such as <code>method</code>,
see the description of <code>coef.cv.boss</code> for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The prediction for BOSS or FS.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Generate a trivial dataset, X has mean 0 and norm 1, y has mean 0
set.seed(11)
n = 20
p = 5
x = matrix(rnorm(n*p), nrow=n, ncol=p)
x = scale(x, center = colMeans(x))
x = scale(x, scale = sqrt(colSums(x^2)))
beta = c(1, 1, 0, 0, 0)
y = x%*%beta + scale(rnorm(20, sd=0.01), center = TRUE, scale = FALSE)

## Perform 10-fold CV without replication
boss_cv_result = cv.boss(x, y)
## Get the coefficient vector of BOSS that gives minimum CV OSS score (S3 method for cv.boss)
beta_boss_cv = coef(boss_cv_result)
# the above is equivalent to
boss_result = boss_cv_result$boss
beta_boss_cv = boss_result$beta_boss[, boss_cv_result$i.min.boss, drop=FALSE]
## Get the fitted values of BOSS-CV (S3 method for cv.boss)
mu_boss_cv = predict(boss_cv_result, newx=x)
# the above is equivalent to
mu_boss_cv = cbind(1,x) %*% beta_boss_cv

## Get the coefficient vector of FS that gives minimum CV OSS score (S3 method for cv.boss)
beta_fs_cv = coef(boss_cv_result, method='fs')
## Get the fitted values of FS-CV (S3 method for cv.boss)
mu_fs_cv = predict(boss_cv_result, newx=x, method='fs')
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
