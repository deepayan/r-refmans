<!DOCTYPE html><html><head><title>Help for package probably</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {probably}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#probably-package'><p>probably: Tools for Post-Processing Class Probability Estimates</p></a></li>
<li><a href='#.cal_table_breaks'><p>Probability Calibration table</p></a></li>
<li><a href='#append_class_pred'><p>Add a <code>class_pred</code> column</p></a></li>
<li><a href='#as_class_pred'><p>Coerce to a <code>class_pred</code> object</p></a></li>
<li><a href='#boosting_predictions'><p>Boosted regression trees predictions</p></a></li>
<li><a href='#cal_apply'><p>Applies a calibration to a set of existing predictions</p></a></li>
<li><a href='#cal_estimate_beta'><p>Uses a Beta calibration model to calculate new probabilities</p></a></li>
<li><a href='#cal_estimate_isotonic'><p>Uses an Isotonic regression model to calibrate model predictions.</p></a></li>
<li><a href='#cal_estimate_isotonic_boot'><p>Uses a bootstrapped Isotonic regression model to calibrate probabilities</p></a></li>
<li><a href='#cal_estimate_linear'><p>Uses a linear regression model to calibrate numeric predictions</p></a></li>
<li><a href='#cal_estimate_logistic'><p>Uses a logistic regression model to calibrate probabilities</p></a></li>
<li><a href='#cal_estimate_multinomial'><p>Uses a Multinomial calibration model to calculate new probabilities</p></a></li>
<li><a href='#cal_plot_breaks'><p>Probability calibration plots via binning</p></a></li>
<li><a href='#cal_plot_logistic'><p>Probability calibration plots via logistic regression</p></a></li>
<li><a href='#cal_plot_regression'><p>Regression calibration plots</p></a></li>
<li><a href='#cal_plot_windowed'><p>Probability calibration plots via moving windows</p></a></li>
<li><a href='#cal_validate_beta'><p>Measure performance with and without using Beta calibration</p></a></li>
<li><a href='#cal_validate_isotonic'><p>Measure performance with and without using isotonic regression calibration</p></a></li>
<li><a href='#cal_validate_isotonic_boot'><p>Measure performance with and without using bagged isotonic regression calibration</p></a></li>
<li><a href='#cal_validate_linear'><p>Measure performance with and without using linear regression calibration</p></a></li>
<li><a href='#cal_validate_logistic'><p>Measure performance with and without using logistic calibration</p></a></li>
<li><a href='#cal_validate_multinomial'><p>Measure performance with and without using multinomial calibration</p></a></li>
<li><a href='#class_pred'><p>Create a class prediction object</p></a></li>
<li><a href='#collect_metrics.cal_rset'><p>Obtain and format metrics produced by calibration validation</p></a></li>
<li><a href='#collect_predictions.cal_rset'><p>Obtain and format predictions produced by calibration validation</p></a></li>
<li><a href='#control_conformal_full'><p>Controlling the numeric details for conformal inference</p></a></li>
<li><a href='#int_conformal_cv'><p>Prediction intervals via conformal inference CV+</p></a></li>
<li><a href='#int_conformal_full'><p>Prediction intervals via conformal inference</p></a></li>
<li><a href='#int_conformal_quantile'><p>Prediction intervals via conformal inference and quantile regression</p></a></li>
<li><a href='#int_conformal_split'><p>Prediction intervals via split conformal inference</p></a></li>
<li><a href='#is_class_pred'><p>Test if an object inherits from <code>class_pred</code></p></a></li>
<li><a href='#levels.class_pred'><p>Extract <code>class_pred</code> levels</p></a></li>
<li><a href='#locate-equivocal'><p>Locate equivocal values</p></a></li>
<li><a href='#make_class_pred'><p>Create a <code>class_pred</code> vector from class probabilities</p></a></li>
<li><a href='#predict.int_conformal_full'><p>Prediction intervals from conformal methods</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#reportable_rate'><p>Calculate the reportable rate</p></a></li>
<li><a href='#required_pkgs.cal_estimate_beta'><p>S3 methods to track which additional packages are needed for specific</p>
calibrations</a></li>
<li><a href='#segment_naive_bayes'><p>Image segmentation predictions</p></a></li>
<li><a href='#species_probs'><p>Predictions on animal species</p></a></li>
<li><a href='#threshold_perf'><p>Generate performance metrics across probability thresholds</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Tools for Post-Processing Predicted Values</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.3</td>
</tr>
<tr>
<td>Description:</td>
<td>Models can be improved by post-processing class
    probabilities, by: recalibration, conversion to hard probabilities,
    assessment of equivocal zones, and other activities. 'probably'
    contains tools for conducting these operations as well as calibration
    tools and conformal inference techniques for regression models.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/tidymodels/probably">https://github.com/tidymodels/probably</a>,
<a href="https://probably.tidymodels.org">https://probably.tidymodels.org</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/tidymodels/probably/issues">https://github.com/tidymodels/probably/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6)</td>
</tr>
<tr>
<td>Imports:</td>
<td>butcher, cli, dplyr (&ge; 1.1.0), furrr, generics (&ge; 0.1.3),
ggplot2, hardhat, pillar, purrr, rlang (&ge; 1.0.4), tidyr (&ge;
1.3.0), tidyselect (&ge; 1.1.2), tune (&ge; 1.1.2), vctrs (&ge;
0.4.1), withr, workflows (&ge; 1.1.4), yardstick (&ge; 1.3.0)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>betacal, covr, knitr, MASS, mgcv, modeldata (&ge; 1.1.0), nnet,
parsnip (&ge; 1.2.0), quantregForest, randomForest, recipes,
rmarkdown, rsample, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>true</td>
</tr>
<tr>
<td>Config/Needs/website:</td>
<td>tidyverse/tidytemplate</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-02-23 01:25:11 UTC; max</td>
</tr>
<tr>
<td>Author:</td>
<td>Max Kuhn [aut, cre],
  Davis Vaughan [aut],
  Edgar Ruiz [aut],
  Posit Software, PBC [cph, fnd]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Max Kuhn &lt;max@posit.co&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-02-23 03:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='probably-package'>probably: Tools for Post-Processing Class Probability Estimates</h2><span id='topic+probably'></span><span id='topic+probably-package'></span>

<h3>Description</h3>

<p>Models can be improved by post-processing class probabilities, by: recalibration, conversion to hard probabilities, assessment of equivocal zones, and other activities. 'probably' contains tools for conducting these operations as well as calibration tools and conformal inference techniques for regression models.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Max Kuhn <a href="mailto:max@posit.co">max@posit.co</a>
</p>
<p>Authors:
</p>

<ul>
<li><p> Davis Vaughan <a href="mailto:davis@posit.co">davis@posit.co</a>
</p>
</li>
<li><p> Edgar Ruiz <a href="mailto:edgar@posit.co">edgar@posit.co</a>
</p>
</li></ul>

<p>Other contributors:
</p>

<ul>
<li><p> Posit Software, PBC [copyright holder, funder]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/tidymodels/probably">https://github.com/tidymodels/probably</a>
</p>
</li>
<li> <p><a href="https://probably.tidymodels.org">https://probably.tidymodels.org</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/tidymodels/probably/issues">https://github.com/tidymodels/probably/issues</a>
</p>
</li></ul>


<hr>
<h2 id='.cal_table_breaks'>Probability Calibration table</h2><span id='topic+.cal_table_breaks'></span><span id='topic+.cal_table_logistic'></span><span id='topic+.cal_table_windowed'></span>

<h3>Description</h3>

<p>Calibration table functions. They require a data.frame that
contains the predictions and probability columns. The output is another
<code>tibble</code> with segmented data that compares the accuracy of the probability
to the actual outcome.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.cal_table_breaks(
  .data,
  truth = NULL,
  estimate = NULL,
  .by = NULL,
  num_breaks = 10,
  conf_level = 0.9,
  event_level = c("auto", "first", "second"),
  ...
)

.cal_table_logistic(
  .data,
  truth = NULL,
  estimate = NULL,
  .by = NULL,
  conf_level = 0.9,
  smooth = TRUE,
  event_level = c("auto", "first", "second"),
  ...
)

.cal_table_windowed(
  .data,
  truth = NULL,
  estimate = NULL,
  .by = NULL,
  window_size = 0.1,
  step_size = window_size/2,
  conf_level = 0.9,
  event_level = c("auto", "first", "second"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".cal_table_breaks_+3A_.data">.data</code></td>
<td>
<p>An ungrouped data frame object containing predictions and
probability columns.</p>
</td></tr>
<tr><td><code id=".cal_table_breaks_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a factor). This should be an unquoted column name.</p>
</td></tr>
<tr><td><code id=".cal_table_breaks_+3A_estimate">estimate</code></td>
<td>
<p>A vector of column identifiers, or one of <code>dplyr</code> selector
functions to choose which variables contains the class probabilities. It
defaults to the prefix used by tidymodels (<code>.pred_</code>). The order of the
identifiers will be considered the same as the order of the levels of the
<code>truth</code> variable.</p>
</td></tr>
<tr><td><code id=".cal_table_breaks_+3A_.by">.by</code></td>
<td>
<p>The column identifier for the grouping variable. This should be
a single unquoted column name that selects a qualitative variable for
grouping. Default to <code>NULL</code>. When <code>.by = NULL</code> no grouping will take place.</p>
</td></tr>
<tr><td><code id=".cal_table_breaks_+3A_num_breaks">num_breaks</code></td>
<td>
<p>The number of segments to group the probabilities. It
defaults to 10.</p>
</td></tr>
<tr><td><code id=".cal_table_breaks_+3A_conf_level">conf_level</code></td>
<td>
<p>Confidence level to use in the visualization. It defaults
to 0.9.</p>
</td></tr>
<tr><td><code id=".cal_table_breaks_+3A_event_level">event_level</code></td>
<td>
<p>single string. Either &quot;first&quot; or &quot;second&quot; to specify which
level of truth to consider as the &quot;event&quot;. Defaults to &quot;auto&quot;, which allows
the function decide which one to use based on the type of model (binary,
multi-class or linear)</p>
</td></tr>
<tr><td><code id=".cal_table_breaks_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to the <code>tune_results</code> object.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li> <p><code>.cal_table_breaks()</code> - Splits the data into bins, based on the
number of breaks provided (<code>num_breaks</code>). The bins are even ranges, starting
at 0, and ending at 1.
</p>
</li>
<li> <p><code>.cal_table_logistic()</code> - Fits a logistic spline regression (GAM)
against the data. It then creates a table with the predictions based on 100
probabilities starting at 0, and ending at 1.
</p>
</li>
<li> <p><code>.cal_table_windowed()</code> - Creates a running percentage of the
probability that moves across the proportion of events.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>.cal_table_breaks(
  segment_logistic,
  Class,
  .pred_good
)

.cal_table_logistic(
  segment_logistic,
  Class,
  .pred_good
)

.cal_table_windowed(
  segment_logistic,
  Class,
  .pred_good
)
</code></pre>

<hr>
<h2 id='append_class_pred'>Add a <code>class_pred</code> column</h2><span id='topic+append_class_pred'></span>

<h3>Description</h3>

<p>This function is similar to <code><a href="#topic+make_class_pred">make_class_pred()</a></code>, but is useful when you have
a large number of class probability columns and want to use <code>tidyselect</code>
helpers. It appends the new <code>class_pred</code> vector as a column on the original
data frame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>append_class_pred(
  .data,
  ...,
  levels,
  ordered = FALSE,
  min_prob = 1/length(levels),
  name = ".class_pred"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="append_class_pred_+3A_.data">.data</code></td>
<td>
<p>A data frame or tibble.</p>
</td></tr>
<tr><td><code id="append_class_pred_+3A_...">...</code></td>
<td>
<p>One or more unquoted expressions separated by commas
to capture the columns of <code>.data</code> containing the class
probabilities. You can treat variable names like they are
positions, so you can use expressions like <code>x:y</code> to select ranges
of variables or use selector functions to choose which columns.
For <code>make_class_pred</code>, the columns for all class probabilities
should be selected (in the same order as the <code>levels</code> object).
For <code>two_class_pred</code>, a vector of class probabilities should be
selected.</p>
</td></tr>
<tr><td><code id="append_class_pred_+3A_levels">levels</code></td>
<td>
<p>A character vector of class levels. The length should be the
same as the number of selections made through <code>...</code>, or length <code>2</code>
for <code>make_two_class_pred()</code>.</p>
</td></tr>
<tr><td><code id="append_class_pred_+3A_ordered">ordered</code></td>
<td>
<p>A single logical to determine if the levels should be regarded
as ordered (in the order given). This results in a <code>class_pred</code> object
that is flagged as ordered.</p>
</td></tr>
<tr><td><code id="append_class_pred_+3A_min_prob">min_prob</code></td>
<td>
<p>A single numeric value. If any probabilities are less than
this value (by row), the row is marked as <em>equivocal</em>.</p>
</td></tr>
<tr><td><code id="append_class_pred_+3A_name">name</code></td>
<td>
<p>A single character value for the name of the appended
<code>class_pred</code> column.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>.data</code> with an extra <code>class_pred</code> column appended onto it.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# The following two examples are equivalent and demonstrate
# the helper, append_class_pred()

library(dplyr)

species_probs %&gt;%
  mutate(
    .class_pred = make_class_pred(
      .pred_bobcat, .pred_coyote, .pred_gray_fox,
      levels = levels(Species),
      min_prob = .5
    )
  )

lvls &lt;- levels(species_probs$Species)

append_class_pred(
  .data = species_probs,
  contains(".pred_"),
  levels = lvls,
  min_prob = .5
)

</code></pre>

<hr>
<h2 id='as_class_pred'>Coerce to a <code>class_pred</code> object</h2><span id='topic+as_class_pred'></span>

<h3>Description</h3>

<p><code>as_class_pred()</code> provides coercion to <code>class_pred</code> from other
existing objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_class_pred(x, which = integer(), equivocal = "[EQ]")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as_class_pred_+3A_x">x</code></td>
<td>
<p>A factor or ordered factor.</p>
</td></tr>
<tr><td><code id="as_class_pred_+3A_which">which</code></td>
<td>
<p>An integer vector specifying the locations of <code>x</code> to declare
as equivocal.</p>
</td></tr>
<tr><td><code id="as_class_pred_+3A_equivocal">equivocal</code></td>
<td>
<p>A single character specifying the equivocal label used
when printing.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- factor(c("Yes", "No", "Yes", "Yes"))
as_class_pred(x)

</code></pre>

<hr>
<h2 id='boosting_predictions'>Boosted regression trees predictions</h2><span id='topic+boosting_predictions'></span><span id='topic+boosting_predictions_oob'></span><span id='topic+boosting_predictions_test'></span>

<h3>Description</h3>

<p>Boosted regression trees predictions
</p>


<h3>Details</h3>

<p>These data have a set of holdout predictions from 10-fold
cross-validation and a separate collection of test set predictions from the
same boosted tree model. The data were generated using the <code>sim_regression</code>
function in the <span class="pkg">modeldata</span> package.
</p>


<h3>Value</h3>

<table>
<tr><td><code>boosting_predictions_oob</code>, <code>boosting_predictions_test</code></td>
<td>
<p>tibbles</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(boosting_predictions_oob)
str(boosting_predictions_oob)
str(boosting_predictions_test)
</code></pre>

<hr>
<h2 id='cal_apply'>Applies a calibration to a set of existing predictions</h2><span id='topic+cal_apply'></span><span id='topic+cal_apply.data.frame'></span><span id='topic+cal_apply.tune_results'></span><span id='topic+cal_apply.cal_object'></span>

<h3>Description</h3>

<p>Applies a calibration to a set of existing predictions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cal_apply(.data, object, pred_class = NULL, parameters = NULL, ...)

## S3 method for class 'data.frame'
cal_apply(.data, object, pred_class = NULL, parameters = NULL, ...)

## S3 method for class 'tune_results'
cal_apply(.data, object, pred_class = NULL, parameters = NULL, ...)

## S3 method for class 'cal_object'
cal_apply(.data, object, pred_class = NULL, parameters = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cal_apply_+3A_.data">.data</code></td>
<td>
<p>An object that can process a calibration object.</p>
</td></tr>
<tr><td><code id="cal_apply_+3A_object">object</code></td>
<td>
<p>The calibration object (<code>cal_object</code>).</p>
</td></tr>
<tr><td><code id="cal_apply_+3A_pred_class">pred_class</code></td>
<td>
<p>(Optional, classification only) Column identifier for the
hard class predictions (a factor vector). This column will be adjusted based
on changes to the calibrated probability columns.</p>
</td></tr>
<tr><td><code id="cal_apply_+3A_parameters">parameters</code></td>
<td>
<p>(Optional)  An optional tibble of tuning parameter values
that can be used to filter the predicted values before processing. Applies
only to <code>tune_results</code> objects.</p>
</td></tr>
<tr><td><code id="cal_apply_+3A_...">...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>cal_apply()</code> currently supports data.frames only. It extracts the <code>truth</code> and
the estimate columns names from the calibration object.
</p>


<h3>See Also</h3>

<p><a href="https://www.tidymodels.org/learn/models/calibration/">https://www.tidymodels.org/learn/models/calibration/</a>,
<code><a href="#topic+cal_estimate_beta">cal_estimate_beta()</a></code>, <code><a href="#topic+cal_estimate_isotonic">cal_estimate_isotonic()</a></code>,
<code><a href="#topic+cal_estimate_isotonic_boot">cal_estimate_isotonic_boot()</a></code>, <code><a href="#topic+cal_estimate_linear">cal_estimate_linear()</a></code>,
<code><a href="#topic+cal_estimate_logistic">cal_estimate_logistic()</a></code>, <code><a href="#topic+cal_estimate_multinomial">cal_estimate_multinomial()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# ------------------------------------------------------------------------------
# classification example

w_calibration &lt;- cal_estimate_logistic(segment_logistic, Class)

cal_apply(segment_logistic, w_calibration)
</code></pre>

<hr>
<h2 id='cal_estimate_beta'>Uses a Beta calibration model to calculate new probabilities</h2><span id='topic+cal_estimate_beta'></span><span id='topic+cal_estimate_beta.data.frame'></span><span id='topic+cal_estimate_beta.tune_results'></span><span id='topic+cal_estimate_beta.grouped_df'></span>

<h3>Description</h3>

<p>Uses a Beta calibration model to calculate new probabilities
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cal_estimate_beta(
  .data,
  truth = NULL,
  shape_params = 2,
  location_params = 1,
  estimate = dplyr::starts_with(".pred_"),
  parameters = NULL,
  ...
)

## S3 method for class 'data.frame'
cal_estimate_beta(
  .data,
  truth = NULL,
  shape_params = 2,
  location_params = 1,
  estimate = dplyr::starts_with(".pred_"),
  parameters = NULL,
  ...,
  .by = NULL
)

## S3 method for class 'tune_results'
cal_estimate_beta(
  .data,
  truth = NULL,
  shape_params = 2,
  location_params = 1,
  estimate = dplyr::starts_with(".pred_"),
  parameters = NULL,
  ...
)

## S3 method for class 'grouped_df'
cal_estimate_beta(
  .data,
  truth = NULL,
  shape_params = 2,
  location_params = 1,
  estimate = NULL,
  parameters = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cal_estimate_beta_+3A_.data">.data</code></td>
<td>
<p>An ungrouped <code>data.frame</code> object, or <code>tune_results</code> object,
that contains predictions and probability columns.</p>
</td></tr>
<tr><td><code id="cal_estimate_beta_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a factor). This should be an unquoted column name.</p>
</td></tr>
<tr><td><code id="cal_estimate_beta_+3A_shape_params">shape_params</code></td>
<td>
<p>Number of shape parameters to use. Accepted values are
1 and 2. Defaults to 2.</p>
</td></tr>
<tr><td><code id="cal_estimate_beta_+3A_location_params">location_params</code></td>
<td>
<p>Number of location parameters to use. Accepted values
1 and 0. Defaults to 1.</p>
</td></tr>
<tr><td><code id="cal_estimate_beta_+3A_estimate">estimate</code></td>
<td>
<p>A vector of column identifiers, or one of <code>dplyr</code> selector
functions to choose which variables contains the class probabilities. It
defaults to the prefix used by tidymodels (<code>.pred_</code>). The order of the
identifiers will be considered the same as the order of the levels of the
<code>truth</code> variable.</p>
</td></tr>
<tr><td><code id="cal_estimate_beta_+3A_parameters">parameters</code></td>
<td>
<p>(Optional)  An optional tibble of tuning parameter values
that can be used to filter the predicted values before processing. Applies
only to <code>tune_results</code> objects.</p>
</td></tr>
<tr><td><code id="cal_estimate_beta_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to the models or routines used to
calculate the new probabilities.</p>
</td></tr>
<tr><td><code id="cal_estimate_beta_+3A_.by">.by</code></td>
<td>
<p>The column identifier for the grouping variable. This should be
a single unquoted column name that selects a qualitative variable for
grouping. Default to <code>NULL</code>. When <code>.by = NULL</code> no grouping will take place.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function uses the <code><a href="betacal.html#topic+beta_calibration">betacal::beta_calibration()</a></code> function, and
retains the resulting model.
</p>


<h3>Multiclass Extension</h3>

<p>This method is designed to work with two classes. For multiclass, it creates
a set of &quot;one versus all&quot; calibrations for each class. After they are
applied to the data, the probability estimates are re-normalized to add to
one. This final step might compromise the calibration.
</p>


<h3>References</h3>

<p>Meelis Kull, Telmo M. Silva Filho, Peter Flach &quot;Beyond sigmoids:
How to obtain well-calibrated probabilities from binary classifiers with beta
calibration,&quot; <em>Electronic Journal of Statistics</em> 11(2), 5052-5080, (2017)
</p>


<h3>See Also</h3>

<p><a href="https://www.tidymodels.org/learn/models/calibration/">https://www.tidymodels.org/learn/models/calibration/</a>,
<code><a href="#topic+cal_validate_beta">cal_validate_beta()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># It will automatically identify the probability columns
# if passed a model fitted with tidymodels
cal_estimate_beta(segment_logistic, Class)
</code></pre>

<hr>
<h2 id='cal_estimate_isotonic'>Uses an Isotonic regression model to calibrate model predictions.</h2><span id='topic+cal_estimate_isotonic'></span><span id='topic+cal_estimate_isotonic.data.frame'></span><span id='topic+cal_estimate_isotonic.tune_results'></span><span id='topic+cal_estimate_isotonic.grouped_df'></span>

<h3>Description</h3>

<p>Uses an Isotonic regression model to calibrate model predictions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cal_estimate_isotonic(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred"),
  parameters = NULL,
  ...
)

## S3 method for class 'data.frame'
cal_estimate_isotonic(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred"),
  parameters = NULL,
  ...,
  .by = NULL
)

## S3 method for class 'tune_results'
cal_estimate_isotonic(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred"),
  parameters = NULL,
  ...
)

## S3 method for class 'grouped_df'
cal_estimate_isotonic(
  .data,
  truth = NULL,
  estimate = NULL,
  parameters = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cal_estimate_isotonic_+3A_.data">.data</code></td>
<td>
<p>An ungrouped <code>data.frame</code> object, or <code>tune_results</code> object,
that contains predictions and probability columns.</p>
</td></tr>
<tr><td><code id="cal_estimate_isotonic_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a factor). This should be an unquoted column name.</p>
</td></tr>
<tr><td><code id="cal_estimate_isotonic_+3A_estimate">estimate</code></td>
<td>
<p>A vector of column identifiers, or one of <code>dplyr</code> selector
functions to choose which variables contains the class probabilities. It
defaults to the prefix used by tidymodels (<code>.pred_</code>). The order of the
identifiers will be considered the same as the order of the levels of the
<code>truth</code> variable.</p>
</td></tr>
<tr><td><code id="cal_estimate_isotonic_+3A_parameters">parameters</code></td>
<td>
<p>(Optional)  An optional tibble of tuning parameter values
that can be used to filter the predicted values before processing. Applies
only to <code>tune_results</code> objects.</p>
</td></tr>
<tr><td><code id="cal_estimate_isotonic_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to the models or routines used to
calculate the new probabilities.</p>
</td></tr>
<tr><td><code id="cal_estimate_isotonic_+3A_.by">.by</code></td>
<td>
<p>The column identifier for the grouping variable. This should be
a single unquoted column name that selects a qualitative variable for
grouping. Default to <code>NULL</code>. When <code>.by = NULL</code> no grouping will take place.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function uses <code><a href="stats.html#topic+isoreg">stats::isoreg()</a></code> to create obtain the calibration
values for binary classification or numeric regression.
</p>


<h3>Multiclass Extension</h3>

<p>This method is designed to work with two classes. For multiclass, it creates
a set of &quot;one versus all&quot; calibrations for each class. After they are
applied to the data, the probability estimates are re-normalized to add to
one. This final step might compromise the calibration.
</p>


<h3>References</h3>

<p>Zadrozny, Bianca and Elkan, Charles. (2002). Transforming Classifier Scores
into Accurate Multiclass Probability Estimates. <em>Proceedings of the ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining.</em>
</p>


<h3>See Also</h3>

<p><a href="https://www.tidymodels.org/learn/models/calibration/">https://www.tidymodels.org/learn/models/calibration/</a>,
<code><a href="#topic+cal_validate_isotonic">cal_validate_isotonic()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># ------------------------------------------------------------------------------
# Binary Classification

# It will automatically identify the probability columns
# if passed a model fitted with tidymodels
cal_estimate_isotonic(segment_logistic, Class)

# Specify the variable names in a vector of unquoted names
cal_estimate_isotonic(segment_logistic, Class, c(.pred_poor, .pred_good))

# dplyr selector functions are also supported
cal_estimate_isotonic(segment_logistic, Class, dplyr::starts_with(".pred_"))

# ------------------------------------------------------------------------------
# Regression (numeric outcomes)

cal_estimate_isotonic(boosting_predictions_oob, outcome, .pred)
</code></pre>

<hr>
<h2 id='cal_estimate_isotonic_boot'>Uses a bootstrapped Isotonic regression model to calibrate probabilities</h2><span id='topic+cal_estimate_isotonic_boot'></span><span id='topic+cal_estimate_isotonic_boot.data.frame'></span><span id='topic+cal_estimate_isotonic_boot.tune_results'></span><span id='topic+cal_estimate_isotonic_boot.grouped_df'></span>

<h3>Description</h3>

<p>Uses a bootstrapped Isotonic regression model to calibrate probabilities
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cal_estimate_isotonic_boot(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred"),
  times = 10,
  parameters = NULL,
  ...
)

## S3 method for class 'data.frame'
cal_estimate_isotonic_boot(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred"),
  times = 10,
  parameters = NULL,
  ...,
  .by = NULL
)

## S3 method for class 'tune_results'
cal_estimate_isotonic_boot(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred"),
  times = 10,
  parameters = NULL,
  ...
)

## S3 method for class 'grouped_df'
cal_estimate_isotonic_boot(
  .data,
  truth = NULL,
  estimate = NULL,
  times = 10,
  parameters = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cal_estimate_isotonic_boot_+3A_.data">.data</code></td>
<td>
<p>An ungrouped <code>data.frame</code> object, or <code>tune_results</code> object,
that contains predictions and probability columns.</p>
</td></tr>
<tr><td><code id="cal_estimate_isotonic_boot_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a factor). This should be an unquoted column name.</p>
</td></tr>
<tr><td><code id="cal_estimate_isotonic_boot_+3A_estimate">estimate</code></td>
<td>
<p>A vector of column identifiers, or one of <code>dplyr</code> selector
functions to choose which variables contains the class probabilities. It
defaults to the prefix used by tidymodels (<code>.pred_</code>). The order of the
identifiers will be considered the same as the order of the levels of the
<code>truth</code> variable.</p>
</td></tr>
<tr><td><code id="cal_estimate_isotonic_boot_+3A_times">times</code></td>
<td>
<p>Number of bootstraps.</p>
</td></tr>
<tr><td><code id="cal_estimate_isotonic_boot_+3A_parameters">parameters</code></td>
<td>
<p>(Optional)  An optional tibble of tuning parameter values
that can be used to filter the predicted values before processing. Applies
only to <code>tune_results</code> objects.</p>
</td></tr>
<tr><td><code id="cal_estimate_isotonic_boot_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to the models or routines used to
calculate the new probabilities.</p>
</td></tr>
<tr><td><code id="cal_estimate_isotonic_boot_+3A_.by">.by</code></td>
<td>
<p>The column identifier for the grouping variable. This should be
a single unquoted column name that selects a qualitative variable for
grouping. Default to <code>NULL</code>. When <code>.by = NULL</code> no grouping will take place.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function uses <code><a href="stats.html#topic+isoreg">stats::isoreg()</a></code> to create obtain the calibration
values. It runs <code><a href="stats.html#topic+isoreg">stats::isoreg()</a></code> multiple times, and each time with a different
seed. The results are saved inside the returned <code>cal_object</code>.
</p>


<h3>Multiclass Extension</h3>

<p>This method is designed to work with two classes. For multiclass, it creates
a set of &quot;one versus all&quot; calibrations for each class. After they are
applied to the data, the probability estimates are re-normalized to add to
one. This final step might compromise the calibration.
</p>


<h3>See Also</h3>

<p><a href="https://www.tidymodels.org/learn/models/calibration/">https://www.tidymodels.org/learn/models/calibration/</a>,
<code><a href="#topic+cal_validate_isotonic_boot">cal_validate_isotonic_boot()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># It will automatically identify the probability columns
# if passed a model fitted with tidymodels
cal_estimate_isotonic_boot(segment_logistic, Class)
# Specify the variable names in a vector of unquoted names
cal_estimate_isotonic_boot(segment_logistic, Class, c(.pred_poor, .pred_good))
# dplyr selector functions are also supported
cal_estimate_isotonic_boot(segment_logistic, Class, dplyr::starts_with(".pred"))
</code></pre>

<hr>
<h2 id='cal_estimate_linear'>Uses a linear regression model to calibrate numeric predictions</h2><span id='topic+cal_estimate_linear'></span><span id='topic+cal_estimate_linear.data.frame'></span><span id='topic+cal_estimate_linear.tune_results'></span><span id='topic+cal_estimate_linear.grouped_df'></span>

<h3>Description</h3>

<p>Uses a linear regression model to calibrate numeric predictions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cal_estimate_linear(
  .data,
  truth = NULL,
  estimate = dplyr::matches("^.pred$"),
  smooth = TRUE,
  parameters = NULL,
  ...,
  .by = NULL
)

## S3 method for class 'data.frame'
cal_estimate_linear(
  .data,
  truth = NULL,
  estimate = dplyr::matches("^.pred$"),
  smooth = TRUE,
  parameters = NULL,
  ...,
  .by = NULL
)

## S3 method for class 'tune_results'
cal_estimate_linear(
  .data,
  truth = NULL,
  estimate = dplyr::matches("^.pred$"),
  smooth = TRUE,
  parameters = NULL,
  ...
)

## S3 method for class 'grouped_df'
cal_estimate_linear(
  .data,
  truth = NULL,
  estimate = NULL,
  smooth = TRUE,
  parameters = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cal_estimate_linear_+3A_.data">.data</code></td>
<td>
<p>Am ungrouped  <code>data.frame</code> object, or <code>tune_results</code> object,
that contains a prediction column.</p>
</td></tr>
<tr><td><code id="cal_estimate_linear_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the observed outcome data (that is
numeric). This should be an unquoted column name.</p>
</td></tr>
<tr><td><code id="cal_estimate_linear_+3A_estimate">estimate</code></td>
<td>
<p>Column identifier for the predicted values</p>
</td></tr>
<tr><td><code id="cal_estimate_linear_+3A_smooth">smooth</code></td>
<td>
<p>Applies to the linear models. It switches between a generalized
additive model using spline terms when <code>TRUE</code>, and simple linear regression
when <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="cal_estimate_linear_+3A_parameters">parameters</code></td>
<td>
<p>(Optional)  An optional tibble of tuning parameter values
that can be used to filter the predicted values before processing. Applies
only to <code>tune_results</code> objects.</p>
</td></tr>
<tr><td><code id="cal_estimate_linear_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to the models or routines used to
calculate the new predictions.</p>
</td></tr>
<tr><td><code id="cal_estimate_linear_+3A_.by">.by</code></td>
<td>
<p>The column identifier for the grouping variable. This should be
a single unquoted column name that selects a qualitative variable for
grouping. Default to <code>NULL</code>. When <code>.by = NULL</code> no grouping will take place.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function uses existing modeling functions from other packages to create
the calibration:
</p>

<ul>
<li> <p><code><a href="stats.html#topic+glm">stats::glm()</a></code> is used when <code>smooth</code> is set to <code>FALSE</code>
</p>
</li>
<li> <p><code><a href="mgcv.html#topic+gam">mgcv::gam()</a></code> is used when <code>smooth</code> is set to <code>TRUE</code>
</p>
</li></ul>

<p>These methods estimate the relationship in the unmodified predicted values
and then remove that trend when <code><a href="#topic+cal_apply">cal_apply()</a></code> is invoked.
</p>


<h3>See Also</h3>

<p><a href="https://www.tidymodels.org/learn/models/calibration/">https://www.tidymodels.org/learn/models/calibration/</a>,
<code><a href="#topic+cal_validate_linear">cal_validate_linear()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)
library(ggplot2)

head(boosting_predictions_test)

# ------------------------------------------------------------------------------
# Before calibration

y_rng &lt;- extendrange(boosting_predictions_test$outcome)

boosting_predictions_test %&gt;%
  ggplot(aes(outcome, .pred)) +
  geom_abline(lty = 2) +
  geom_point(alpha = 1 / 2) +
  geom_smooth(se = FALSE, col = "blue", linewidth = 1.2, alpha = 3 / 4) +
  coord_equal(xlim = y_rng, ylim = y_rng) +
  ggtitle("Before calibration")

# ------------------------------------------------------------------------------
# Smoothed trend removal

smoothed_cal &lt;-
  boosting_predictions_oob %&gt;%
  # It will automatically identify the predicted value columns when the
  # standard tidymodels naming conventions are used.
  cal_estimate_linear(outcome)
smoothed_cal

boosting_predictions_test %&gt;%
  cal_apply(smoothed_cal) %&gt;%
  ggplot(aes(outcome, .pred)) +
  geom_abline(lty = 2) +
  geom_point(alpha = 1 / 2) +
  geom_smooth(se = FALSE, col = "blue", linewidth = 1.2, alpha = 3 / 4) +
  coord_equal(xlim = y_rng, ylim = y_rng) +
  ggtitle("After calibration")

</code></pre>

<hr>
<h2 id='cal_estimate_logistic'>Uses a logistic regression model to calibrate probabilities</h2><span id='topic+cal_estimate_logistic'></span><span id='topic+cal_estimate_logistic.data.frame'></span><span id='topic+cal_estimate_logistic.tune_results'></span><span id='topic+cal_estimate_logistic.grouped_df'></span>

<h3>Description</h3>

<p>Uses a logistic regression model to calibrate probabilities
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cal_estimate_logistic(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred_"),
  smooth = TRUE,
  parameters = NULL,
  ...
)

## S3 method for class 'data.frame'
cal_estimate_logistic(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred_"),
  smooth = TRUE,
  parameters = NULL,
  ...,
  .by = NULL
)

## S3 method for class 'tune_results'
cal_estimate_logistic(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred_"),
  smooth = TRUE,
  parameters = NULL,
  ...
)

## S3 method for class 'grouped_df'
cal_estimate_logistic(
  .data,
  truth = NULL,
  estimate = NULL,
  smooth = TRUE,
  parameters = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cal_estimate_logistic_+3A_.data">.data</code></td>
<td>
<p>An ungrouped <code>data.frame</code> object, or <code>tune_results</code> object,
that contains predictions and probability columns.</p>
</td></tr>
<tr><td><code id="cal_estimate_logistic_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a factor). This should be an unquoted column name.</p>
</td></tr>
<tr><td><code id="cal_estimate_logistic_+3A_estimate">estimate</code></td>
<td>
<p>A vector of column identifiers, or one of <code>dplyr</code> selector
functions to choose which variables contains the class probabilities. It
defaults to the prefix used by tidymodels (<code>.pred_</code>). The order of the
identifiers will be considered the same as the order of the levels of the
<code>truth</code> variable.</p>
</td></tr>
<tr><td><code id="cal_estimate_logistic_+3A_smooth">smooth</code></td>
<td>
<p>Applies to the logistic models. It switches between logistic
spline when <code>TRUE</code>, and simple logistic regression when <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="cal_estimate_logistic_+3A_parameters">parameters</code></td>
<td>
<p>(Optional)  An optional tibble of tuning parameter values
that can be used to filter the predicted values before processing. Applies
only to <code>tune_results</code> objects.</p>
</td></tr>
<tr><td><code id="cal_estimate_logistic_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to the models or routines used to
calculate the new probabilities.</p>
</td></tr>
<tr><td><code id="cal_estimate_logistic_+3A_.by">.by</code></td>
<td>
<p>The column identifier for the grouping variable. This should be
a single unquoted column name that selects a qualitative variable for
grouping. Default to <code>NULL</code>. When <code>.by = NULL</code> no grouping will take place.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function uses existing modeling functions from other packages to create
the calibration:
</p>

<ul>
<li> <p><code><a href="stats.html#topic+glm">stats::glm()</a></code> is used when <code>smooth</code> is set to <code>FALSE</code>
</p>
</li>
<li> <p><code><a href="mgcv.html#topic+gam">mgcv::gam()</a></code> is used when <code>smooth</code> is set to <code>TRUE</code>
</p>
</li></ul>



<h4>Multiclass Extension</h4>

<p>This method has <em>not</em> been extended to multiclass outcomes. However, the
natural multiclass extension is <code><a href="#topic+cal_estimate_multinomial">cal_estimate_multinomial()</a></code>.
</p>



<h3>See Also</h3>

<p><a href="https://www.tidymodels.org/learn/models/calibration/">https://www.tidymodels.org/learn/models/calibration/</a>,
<code><a href="#topic+cal_validate_logistic">cal_validate_logistic()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># It will automatically identify the probability columns
# if passed a model fitted with tidymodels
cal_estimate_logistic(segment_logistic, Class)

# Specify the variable names in a vector of unquoted names
cal_estimate_logistic(segment_logistic, Class, c(.pred_poor, .pred_good))

# dplyr selector functions are also supported
cal_estimate_logistic(segment_logistic, Class, dplyr::starts_with(".pred_"))
</code></pre>

<hr>
<h2 id='cal_estimate_multinomial'>Uses a Multinomial calibration model to calculate new probabilities</h2><span id='topic+cal_estimate_multinomial'></span><span id='topic+cal_estimate_multinomial.data.frame'></span><span id='topic+cal_estimate_multinomial.tune_results'></span><span id='topic+cal_estimate_multinomial.grouped_df'></span>

<h3>Description</h3>

<p>Uses a Multinomial calibration model to calculate new probabilities
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cal_estimate_multinomial(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred_"),
  smooth = TRUE,
  parameters = NULL,
  ...
)

## S3 method for class 'data.frame'
cal_estimate_multinomial(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred_"),
  smooth = TRUE,
  parameters = NULL,
  ...,
  .by = NULL
)

## S3 method for class 'tune_results'
cal_estimate_multinomial(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred_"),
  smooth = TRUE,
  parameters = NULL,
  ...
)

## S3 method for class 'grouped_df'
cal_estimate_multinomial(
  .data,
  truth = NULL,
  estimate = NULL,
  smooth = TRUE,
  parameters = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cal_estimate_multinomial_+3A_.data">.data</code></td>
<td>
<p>An ungrouped <code>data.frame</code> object, or <code>tune_results</code> object,
that contains predictions and probability columns.</p>
</td></tr>
<tr><td><code id="cal_estimate_multinomial_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a factor). This should be an unquoted column name.</p>
</td></tr>
<tr><td><code id="cal_estimate_multinomial_+3A_estimate">estimate</code></td>
<td>
<p>A vector of column identifiers, or one of <code>dplyr</code> selector
functions to choose which variables contains the class probabilities. It
defaults to the prefix used by tidymodels (<code>.pred_</code>). The order of the
identifiers will be considered the same as the order of the levels of the
<code>truth</code> variable.</p>
</td></tr>
<tr><td><code id="cal_estimate_multinomial_+3A_smooth">smooth</code></td>
<td>
<p>Applies to the logistic models. It switches between logistic
spline when <code>TRUE</code>, and simple logistic regression when <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="cal_estimate_multinomial_+3A_parameters">parameters</code></td>
<td>
<p>(Optional)  An optional tibble of tuning parameter values
that can be used to filter the predicted values before processing. Applies
only to <code>tune_results</code> objects.</p>
</td></tr>
<tr><td><code id="cal_estimate_multinomial_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to the models or routines used to
calculate the new probabilities.</p>
</td></tr>
<tr><td><code id="cal_estimate_multinomial_+3A_.by">.by</code></td>
<td>
<p>The column identifier for the grouping variable. This should be
a single unquoted column name that selects a qualitative variable for
grouping. Default to <code>NULL</code>. When <code>.by = NULL</code> no grouping will take place.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code>smooth = FALSE</code>, <code><a href="nnet.html#topic+multinom">nnet::multinom()</a></code> function is used to estimate the
model, otherwise <code><a href="mgcv.html#topic+gam">mgcv::gam()</a></code> is used.
</p>


<h3>See Also</h3>

<p><a href="https://www.tidymodels.org/learn/models/calibration/">https://www.tidymodels.org/learn/models/calibration/</a>,
<code><a href="#topic+cal_validate_multinomial">cal_validate_multinomial()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(modeldata)
library(parsnip)
library(dplyr)

f &lt;-
  list(
    ~ -0.5 + 0.6 * abs(A),
    ~ ifelse(A &gt; 0 &amp; B &gt; 0, 1.0 + 0.2 * A / B, -2),
    ~ -0.6 * A + 0.50 * B - A * B
  )

set.seed(1)
tr_dat &lt;- sim_multinomial(500, eqn_1 = f[[1]], eqn_2 = f[[2]], eqn_3 = f[[3]])
cal_dat &lt;- sim_multinomial(500, eqn_1 = f[[1]], eqn_2 = f[[2]], eqn_3 = f[[3]])
te_dat &lt;- sim_multinomial(500, eqn_1 = f[[1]], eqn_2 = f[[2]], eqn_3 = f[[3]])

set.seed(2)
rf_fit &lt;-
  rand_forest() %&gt;%
  set_mode("classification") %&gt;%
  set_engine("randomForest") %&gt;%
  fit(class ~ ., data = tr_dat)

cal_pred &lt;-
  predict(rf_fit, cal_dat, type = "prob") %&gt;%
  bind_cols(cal_dat)
te_pred &lt;-
  predict(rf_fit, te_dat, type = "prob") %&gt;%
  bind_cols(te_dat)

cal_plot_windowed(cal_pred, truth = class, window_size = 0.1, step_size = 0.03)

smoothed_mn &lt;- cal_estimate_multinomial(cal_pred, truth = class)

new_test_pred &lt;- cal_apply(te_pred, smoothed_mn)

cal_plot_windowed(new_test_pred, truth = class, window_size = 0.1, step_size = 0.03)

</code></pre>

<hr>
<h2 id='cal_plot_breaks'>Probability calibration plots via binning</h2><span id='topic+cal_plot_breaks'></span><span id='topic+cal_plot_breaks.data.frame'></span><span id='topic+cal_plot_breaks.tune_results'></span><span id='topic+cal_plot_breaks.grouped_df'></span>

<h3>Description</h3>

<p>A plot is created to assess whether the observed rate of the event is about
the same as the predicted probability of the event from some model.
</p>
<p>A sequence of even, mutually exclusive bins are created from zero to one.
For each bin, the data whose predicted probability falls within the range
of the bin is used to calculate the observed event rate (along with confidence
intervals for the event rate).
If the predictions are well calibrated, the fitted curve should align with
the diagonal line.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cal_plot_breaks(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred"),
  num_breaks = 10,
  conf_level = 0.9,
  include_ribbon = TRUE,
  include_rug = TRUE,
  include_points = TRUE,
  event_level = c("auto", "first", "second"),
  ...
)

## S3 method for class 'data.frame'
cal_plot_breaks(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred"),
  num_breaks = 10,
  conf_level = 0.9,
  include_ribbon = TRUE,
  include_rug = TRUE,
  include_points = TRUE,
  event_level = c("auto", "first", "second"),
  ...,
  .by = NULL
)

## S3 method for class 'tune_results'
cal_plot_breaks(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred"),
  num_breaks = 10,
  conf_level = 0.9,
  include_ribbon = TRUE,
  include_rug = TRUE,
  include_points = TRUE,
  event_level = c("auto", "first", "second"),
  ...
)

## S3 method for class 'grouped_df'
cal_plot_breaks(
  .data,
  truth = NULL,
  estimate = NULL,
  num_breaks = 10,
  conf_level = 0.9,
  include_ribbon = TRUE,
  include_rug = TRUE,
  include_points = TRUE,
  event_level = c("auto", "first", "second"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cal_plot_breaks_+3A_.data">.data</code></td>
<td>
<p>An ungrouped data frame object containing predictions and
probability columns.</p>
</td></tr>
<tr><td><code id="cal_plot_breaks_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a factor). This should be an unquoted column name.</p>
</td></tr>
<tr><td><code id="cal_plot_breaks_+3A_estimate">estimate</code></td>
<td>
<p>A vector of column identifiers, or one of <code>dplyr</code> selector
functions to choose which variables contains the class probabilities. It
defaults to the prefix used by tidymodels (<code>.pred_</code>). The order of the
identifiers will be considered the same as the order of the levels of the
<code>truth</code> variable.</p>
</td></tr>
<tr><td><code id="cal_plot_breaks_+3A_num_breaks">num_breaks</code></td>
<td>
<p>The number of segments to group the probabilities. It
defaults to 10.</p>
</td></tr>
<tr><td><code id="cal_plot_breaks_+3A_conf_level">conf_level</code></td>
<td>
<p>Confidence level to use in the visualization. It defaults
to 0.9.</p>
</td></tr>
<tr><td><code id="cal_plot_breaks_+3A_include_ribbon">include_ribbon</code></td>
<td>
<p>Flag that indicates if the ribbon layer is to be
included. It defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="cal_plot_breaks_+3A_include_rug">include_rug</code></td>
<td>
<p>Flag that indicates if the Rug layer is to be included.
It defaults to <code>TRUE</code>. In the plot, the top side shows the frequency the
event occurring, and the bottom the frequency of the event not occurring.</p>
</td></tr>
<tr><td><code id="cal_plot_breaks_+3A_include_points">include_points</code></td>
<td>
<p>Flag that indicates if the point layer is to be included.</p>
</td></tr>
<tr><td><code id="cal_plot_breaks_+3A_event_level">event_level</code></td>
<td>
<p>single string. Either &quot;first&quot; or &quot;second&quot; to specify which
level of truth to consider as the &quot;event&quot;. Defaults to &quot;auto&quot;, which allows
the function decide which one to use based on the type of model (binary,
multi-class or linear)</p>
</td></tr>
<tr><td><code id="cal_plot_breaks_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to the <code>tune_results</code> object.</p>
</td></tr>
<tr><td><code id="cal_plot_breaks_+3A_.by">.by</code></td>
<td>
<p>The column identifier for the grouping variable. This should be
a single unquoted column name that selects a qualitative variable for
grouping. Default to <code>NULL</code>. When <code>.by = NULL</code> no grouping will take place.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot object.
</p>


<h3>See Also</h3>

<p><a href="https://www.tidymodels.org/learn/models/calibration/">https://www.tidymodels.org/learn/models/calibration/</a>,
<code><a href="#topic+cal_plot_windowed">cal_plot_windowed()</a></code>, <code><a href="#topic+cal_plot_logistic">cal_plot_logistic()</a></code>
</p>
<p><code><a href="#topic+cal_plot_logistic">cal_plot_logistic()</a></code>, <code><a href="#topic+cal_plot_windowed">cal_plot_windowed()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(ggplot2)
library(dplyr)

cal_plot_breaks(
  segment_logistic,
  Class,
  .pred_good
)

cal_plot_logistic(
  segment_logistic,
  Class,
  .pred_good
)

cal_plot_windowed(
  segment_logistic,
  Class,
  .pred_good
)

# The functions support dplyr groups

model &lt;- glm(Class ~ .pred_good, segment_logistic, family = "binomial")

preds &lt;- predict(model, segment_logistic, type = "response")

gl &lt;- segment_logistic %&gt;%
  mutate(.pred_good = 1 - preds, source = "glm")

combined &lt;- bind_rows(mutate(segment_logistic, source = "original"), gl)

combined %&gt;%
  cal_plot_logistic(Class, .pred_good, .by = source)

# The grouping can be faceted in ggplot2
combined %&gt;%
  cal_plot_logistic(Class, .pred_good, .by = source) +
  facet_wrap(~source) +
  theme(legend.position = "")
</code></pre>

<hr>
<h2 id='cal_plot_logistic'>Probability calibration plots via logistic regression</h2><span id='topic+cal_plot_logistic'></span><span id='topic+cal_plot_logistic.data.frame'></span><span id='topic+cal_plot_logistic.tune_results'></span><span id='topic+cal_plot_logistic.grouped_df'></span>

<h3>Description</h3>

<p>A logistic regression model is fit where the original outcome data are used
as the outcome and the estimated class probabilities for one class are used
as the predictor. If <code>smooth = TRUE</code>, a generalized additive model is fit
using <code><a href="mgcv.html#topic+gam">mgcv::gam()</a></code> and the default smoothing method. Otherwise, a simple
logistic regression is used.
</p>
<p>If the predictions are well calibrated, the fitted curve should align with
the diagonal line. Confidence intervals for the fitted line are also
shown.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cal_plot_logistic(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred"),
  conf_level = 0.9,
  smooth = TRUE,
  include_rug = TRUE,
  include_ribbon = TRUE,
  event_level = c("auto", "first", "second"),
  ...
)

## S3 method for class 'data.frame'
cal_plot_logistic(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred"),
  conf_level = 0.9,
  smooth = TRUE,
  include_rug = TRUE,
  include_ribbon = TRUE,
  event_level = c("auto", "first", "second"),
  ...,
  .by = NULL
)

## S3 method for class 'tune_results'
cal_plot_logistic(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred"),
  conf_level = 0.9,
  smooth = TRUE,
  include_rug = TRUE,
  include_ribbon = TRUE,
  event_level = c("auto", "first", "second"),
  ...
)

## S3 method for class 'grouped_df'
cal_plot_logistic(
  .data,
  truth = NULL,
  estimate = NULL,
  conf_level = 0.9,
  smooth = TRUE,
  include_rug = TRUE,
  include_ribbon = TRUE,
  event_level = c("auto", "first", "second"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cal_plot_logistic_+3A_.data">.data</code></td>
<td>
<p>An ungrouped data frame object containing predictions and
probability columns.</p>
</td></tr>
<tr><td><code id="cal_plot_logistic_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a factor). This should be an unquoted column name.</p>
</td></tr>
<tr><td><code id="cal_plot_logistic_+3A_estimate">estimate</code></td>
<td>
<p>A vector of column identifiers, or one of <code>dplyr</code> selector
functions to choose which variables contains the class probabilities. It
defaults to the prefix used by tidymodels (<code>.pred_</code>). The order of the
identifiers will be considered the same as the order of the levels of the
<code>truth</code> variable.</p>
</td></tr>
<tr><td><code id="cal_plot_logistic_+3A_conf_level">conf_level</code></td>
<td>
<p>Confidence level to use in the visualization. It defaults
to 0.9.</p>
</td></tr>
<tr><td><code id="cal_plot_logistic_+3A_smooth">smooth</code></td>
<td>
<p>A logical for using a generalized additive model with smooth
terms for the predictor via <code><a href="mgcv.html#topic+gam">mgcv::gam()</a></code> and <code><a href="mgcv.html#topic+s">mgcv::s()</a></code>.</p>
</td></tr>
<tr><td><code id="cal_plot_logistic_+3A_include_rug">include_rug</code></td>
<td>
<p>Flag that indicates if the Rug layer is to be included.
It defaults to <code>TRUE</code>. In the plot, the top side shows the frequency the
event occurring, and the bottom the frequency of the event not occurring.</p>
</td></tr>
<tr><td><code id="cal_plot_logistic_+3A_include_ribbon">include_ribbon</code></td>
<td>
<p>Flag that indicates if the ribbon layer is to be
included. It defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="cal_plot_logistic_+3A_event_level">event_level</code></td>
<td>
<p>single string. Either &quot;first&quot; or &quot;second&quot; to specify which
level of truth to consider as the &quot;event&quot;. Defaults to &quot;auto&quot;, which allows
the function decide which one to use based on the type of model (binary,
multi-class or linear)</p>
</td></tr>
<tr><td><code id="cal_plot_logistic_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to the <code>tune_results</code> object.</p>
</td></tr>
<tr><td><code id="cal_plot_logistic_+3A_.by">.by</code></td>
<td>
<p>The column identifier for the grouping variable. This should be
a single unquoted column name that selects a qualitative variable for
grouping. Default to <code>NULL</code>. When <code>.by = NULL</code> no grouping will take place.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot object.
</p>


<h3>See Also</h3>

<p><a href="https://www.tidymodels.org/learn/models/calibration/">https://www.tidymodels.org/learn/models/calibration/</a>,
<code><a href="#topic+cal_plot_windowed">cal_plot_windowed()</a></code>, <code><a href="#topic+cal_plot_breaks">cal_plot_breaks()</a></code>
</p>
<p><code><a href="#topic+cal_plot_breaks">cal_plot_breaks()</a></code>, <code><a href="#topic+cal_plot_windowed">cal_plot_windowed()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(ggplot2)
library(dplyr)

cal_plot_logistic(
  segment_logistic,
  Class,
  .pred_good
)

cal_plot_logistic(
  segment_logistic,
  Class,
  .pred_good,
  smooth = FALSE
)
</code></pre>

<hr>
<h2 id='cal_plot_regression'>Regression calibration plots</h2><span id='topic+cal_plot_regression'></span><span id='topic+cal_plot_regression.data.frame'></span><span id='topic+cal_plot_regression.tune_results'></span><span id='topic+cal_plot_regression.grouped_df'></span>

<h3>Description</h3>

<p>A scatter plot of the observed and predicted values is computed where the
axes are the same. When <code>smooth = TRUE</code>, a generalized additive model fit
is shown. If the predictions are well calibrated, the fitted curve should align with
the diagonal line.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cal_plot_regression(.data, truth = NULL, estimate = NULL, smooth = TRUE, ...)

## S3 method for class 'data.frame'
cal_plot_regression(
  .data,
  truth = NULL,
  estimate = NULL,
  smooth = TRUE,
  ...,
  .by = NULL
)

## S3 method for class 'tune_results'
cal_plot_regression(.data, truth = NULL, estimate = NULL, smooth = TRUE, ...)

## S3 method for class 'grouped_df'
cal_plot_regression(.data, truth = NULL, estimate = NULL, smooth = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cal_plot_regression_+3A_.data">.data</code></td>
<td>
<p>An ungrouped data frame object containing a prediction
column.</p>
</td></tr>
<tr><td><code id="cal_plot_regression_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true results
(numeric). This should be an unquoted column name.</p>
</td></tr>
<tr><td><code id="cal_plot_regression_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predictions.
This should be an unquoted column name</p>
</td></tr>
<tr><td><code id="cal_plot_regression_+3A_smooth">smooth</code></td>
<td>
<p>A logical: should a smoother curve be added.</p>
</td></tr>
<tr><td><code id="cal_plot_regression_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="ggplot2.html#topic+geom_point">ggplot2::geom_point()</a></code>.</p>
</td></tr>
<tr><td><code id="cal_plot_regression_+3A_.by">.by</code></td>
<td>
<p>The column identifier for the grouping variable. This should be
a single unquoted column name that selects a qualitative variable for
grouping. Default to <code>NULL</code>. When <code>.by = NULL</code> no grouping will take place.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cal_plot_regression(boosting_predictions_oob, outcome, .pred)

cal_plot_regression(boosting_predictions_oob, outcome, .pred,
  alpha = 1 / 6, cex = 3, smooth = FALSE
)

cal_plot_regression(boosting_predictions_oob, outcome, .pred,
  .by = id,
  alpha = 1 / 6, cex = 3, smooth = FALSE
)
</code></pre>

<hr>
<h2 id='cal_plot_windowed'>Probability calibration plots via moving windows</h2><span id='topic+cal_plot_windowed'></span><span id='topic+cal_plot_windowed.data.frame'></span><span id='topic+cal_plot_windowed.tune_results'></span><span id='topic+cal_plot_windowed.grouped_df'></span>

<h3>Description</h3>

<p>A plot is created to assess whether the observed rate of the event is about
the sample as the predicted probability of the event from some model. This
is similar to <code><a href="#topic+cal_plot_breaks">cal_plot_breaks()</a></code>, except that the bins are overlapping.
</p>
<p>A sequence of bins are created from zero to one. For each bin, the data whose
predicted probability falls within the range of the bin is used to calculate
the observed event rate (along with confidence intervals for the event rate).
</p>
<p>If the predictions are well calibrated, the fitted curve should align with
the diagonal line.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cal_plot_windowed(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred"),
  window_size = 0.1,
  step_size = window_size/2,
  conf_level = 0.9,
  include_ribbon = TRUE,
  include_rug = TRUE,
  include_points = TRUE,
  event_level = c("auto", "first", "second"),
  ...
)

## S3 method for class 'data.frame'
cal_plot_windowed(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred"),
  window_size = 0.1,
  step_size = window_size/2,
  conf_level = 0.9,
  include_ribbon = TRUE,
  include_rug = TRUE,
  include_points = TRUE,
  event_level = c("auto", "first", "second"),
  ...,
  .by = NULL
)

## S3 method for class 'tune_results'
cal_plot_windowed(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred"),
  window_size = 0.1,
  step_size = window_size/2,
  conf_level = 0.9,
  include_ribbon = TRUE,
  include_rug = TRUE,
  include_points = TRUE,
  event_level = c("auto", "first", "second"),
  ...
)

## S3 method for class 'grouped_df'
cal_plot_windowed(
  .data,
  truth = NULL,
  estimate = NULL,
  window_size = 0.1,
  step_size = window_size/2,
  conf_level = 0.9,
  include_ribbon = TRUE,
  include_rug = TRUE,
  include_points = TRUE,
  event_level = c("auto", "first", "second"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cal_plot_windowed_+3A_.data">.data</code></td>
<td>
<p>An ungrouped data frame object containing predictions and
probability columns.</p>
</td></tr>
<tr><td><code id="cal_plot_windowed_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a factor). This should be an unquoted column name.</p>
</td></tr>
<tr><td><code id="cal_plot_windowed_+3A_estimate">estimate</code></td>
<td>
<p>A vector of column identifiers, or one of <code>dplyr</code> selector
functions to choose which variables contains the class probabilities. It
defaults to the prefix used by tidymodels (<code>.pred_</code>). The order of the
identifiers will be considered the same as the order of the levels of the
<code>truth</code> variable.</p>
</td></tr>
<tr><td><code id="cal_plot_windowed_+3A_window_size">window_size</code></td>
<td>
<p>The size of segments. Used for the windowed probability
calculations. It defaults to 10% of segments.</p>
</td></tr>
<tr><td><code id="cal_plot_windowed_+3A_step_size">step_size</code></td>
<td>
<p>The gap between segments. Used for the windowed probability
calculations. It defaults to half the size of <code>window_size</code></p>
</td></tr>
<tr><td><code id="cal_plot_windowed_+3A_conf_level">conf_level</code></td>
<td>
<p>Confidence level to use in the visualization. It defaults
to 0.9.</p>
</td></tr>
<tr><td><code id="cal_plot_windowed_+3A_include_ribbon">include_ribbon</code></td>
<td>
<p>Flag that indicates if the ribbon layer is to be
included. It defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="cal_plot_windowed_+3A_include_rug">include_rug</code></td>
<td>
<p>Flag that indicates if the Rug layer is to be included.
It defaults to <code>TRUE</code>. In the plot, the top side shows the frequency the
event occurring, and the bottom the frequency of the event not occurring.</p>
</td></tr>
<tr><td><code id="cal_plot_windowed_+3A_include_points">include_points</code></td>
<td>
<p>Flag that indicates if the point layer is to be included.</p>
</td></tr>
<tr><td><code id="cal_plot_windowed_+3A_event_level">event_level</code></td>
<td>
<p>single string. Either &quot;first&quot; or &quot;second&quot; to specify which
level of truth to consider as the &quot;event&quot;. Defaults to &quot;auto&quot;, which allows
the function decide which one to use based on the type of model (binary,
multi-class or linear)</p>
</td></tr>
<tr><td><code id="cal_plot_windowed_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to the <code>tune_results</code> object.</p>
</td></tr>
<tr><td><code id="cal_plot_windowed_+3A_.by">.by</code></td>
<td>
<p>The column identifier for the grouping variable. This should be
a single unquoted column name that selects a qualitative variable for
grouping. Default to <code>NULL</code>. When <code>.by = NULL</code> no grouping will take place.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot object.
</p>


<h3>See Also</h3>

<p><a href="https://www.tidymodels.org/learn/models/calibration/">https://www.tidymodels.org/learn/models/calibration/</a>,
<code><a href="#topic+cal_plot_logistic">cal_plot_logistic()</a></code>, <code><a href="#topic+cal_plot_breaks">cal_plot_breaks()</a></code>
</p>
<p><code><a href="#topic+cal_plot_breaks">cal_plot_breaks()</a></code>, <code><a href="#topic+cal_plot_logistic">cal_plot_logistic()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(ggplot2)
library(dplyr)

cal_plot_windowed(
  segment_logistic,
  Class,
  .pred_good
)

# More breaks
cal_plot_windowed(
  segment_logistic,
  Class,
  .pred_good,
  window_size = 0.05
)
</code></pre>

<hr>
<h2 id='cal_validate_beta'>Measure performance with and without using Beta calibration</h2><span id='topic+cal_validate_beta'></span><span id='topic+cal_validate_beta.resample_results'></span><span id='topic+cal_validate_beta.rset'></span><span id='topic+cal_validate_beta.tune_results'></span>

<h3>Description</h3>

<p>This function uses resampling to measure the effect of calibrating predicted
values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cal_validate_beta(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred_"),
  metrics = NULL,
  save_pred = FALSE,
  ...
)

## S3 method for class 'resample_results'
cal_validate_beta(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred_"),
  metrics = NULL,
  save_pred = FALSE,
  ...
)

## S3 method for class 'rset'
cal_validate_beta(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred_"),
  metrics = NULL,
  save_pred = FALSE,
  ...
)

## S3 method for class 'tune_results'
cal_validate_beta(
  .data,
  truth = NULL,
  estimate = NULL,
  metrics = NULL,
  save_pred = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cal_validate_beta_+3A_.data">.data</code></td>
<td>
<p>An <code>rset</code> object or the results of <code><a href="tune.html#topic+fit_resamples">tune::fit_resamples()</a></code> with
a <code>.predictions</code> column.</p>
</td></tr>
<tr><td><code id="cal_validate_beta_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a factor). This should be an unquoted column name.</p>
</td></tr>
<tr><td><code id="cal_validate_beta_+3A_estimate">estimate</code></td>
<td>
<p>A vector of column identifiers, or one of <code>dplyr</code> selector
functions to choose which variables contains the class probabilities. It
defaults to the prefix used by tidymodels (<code>.pred_</code>). The order of the
identifiers will be considered the same as the order of the levels of the
<code>truth</code> variable.</p>
</td></tr>
<tr><td><code id="cal_validate_beta_+3A_metrics">metrics</code></td>
<td>
<p>A set of metrics passed created via <code><a href="yardstick.html#topic+metric_set">yardstick::metric_set()</a></code></p>
</td></tr>
<tr><td><code id="cal_validate_beta_+3A_save_pred">save_pred</code></td>
<td>
<p>Indicates whether to a column of post-calibration predictions.</p>
</td></tr>
<tr><td><code id="cal_validate_beta_+3A_...">...</code></td>
<td>
<p>Options to pass to <code><a href="#topic+cal_estimate_beta">cal_estimate_beta()</a></code>, such as the
<code>shape_params</code> and <code>location_params</code> arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions are designed to calculate performance with and without
calibration. They use resampling to measure out-of-sample effectiveness.
There are two ways to pass the data in:
</p>

<ul>
<li><p> If you have a data frame of predictions, an <code>rset</code> object can be created
via <span class="pkg">rsample</span> functions. See the example below.
</p>
</li>
<li><p> If you have already made a resampling object from the original data and
used it with <code><a href="tune.html#topic+fit_resamples">tune::fit_resamples()</a></code>, you can pass that object to the
calibration function and it will use the same resampling scheme. If a
different resampling scheme should be used, run
<code><a href="tune.html#topic+collect_predictions">tune::collect_predictions()</a></code> on the object and use the process in the
previous bullet point.
</p>
</li></ul>

<p>Please note that these functions do not apply to <code>tune_result</code> objects. The
notion of &quot;validation&quot; implies that the tuning parameter selection has been
resolved.
</p>
<p><code>collect_predictions()</code> can be used to aggregate the metrics for analysis.
</p>


<h3>Value</h3>

<p>The original object with a <code>.metrics_cal</code> column and, optionally,
an additional <code>.predictions_cal</code> column. The class <code>cal_rset</code> is also added.
</p>


<h3>Performance Metrics</h3>

<p>By default, the average of the Brier scores is returned. Any appropriate
<code><a href="yardstick.html#topic+metric_set">yardstick::metric_set()</a></code> can be used. The validation function compares the
average of the metrics before, and after the calibration.
</p>


<h3>See Also</h3>

<p><a href="https://www.tidymodels.org/learn/models/calibration/">https://www.tidymodels.org/learn/models/calibration/</a>,
<code><a href="#topic+cal_estimate_beta">cal_estimate_beta()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dplyr)

segment_logistic %&gt;%
  rsample::vfold_cv() %&gt;%
  cal_validate_beta(Class)

</code></pre>

<hr>
<h2 id='cal_validate_isotonic'>Measure performance with and without using isotonic regression calibration</h2><span id='topic+cal_validate_isotonic'></span><span id='topic+cal_validate_isotonic.resample_results'></span><span id='topic+cal_validate_isotonic.rset'></span><span id='topic+cal_validate_isotonic.tune_results'></span>

<h3>Description</h3>

<p>This function uses resampling to measure the effect of calibrating predicted
values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cal_validate_isotonic(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred"),
  metrics = NULL,
  save_pred = FALSE,
  ...
)

## S3 method for class 'resample_results'
cal_validate_isotonic(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred"),
  metrics = NULL,
  save_pred = FALSE,
  ...
)

## S3 method for class 'rset'
cal_validate_isotonic(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred"),
  metrics = NULL,
  save_pred = FALSE,
  ...
)

## S3 method for class 'tune_results'
cal_validate_isotonic(
  .data,
  truth = NULL,
  estimate = NULL,
  metrics = NULL,
  save_pred = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cal_validate_isotonic_+3A_.data">.data</code></td>
<td>
<p>An <code>rset</code> object or the results of <code><a href="tune.html#topic+fit_resamples">tune::fit_resamples()</a></code> with
a <code>.predictions</code> column.</p>
</td></tr>
<tr><td><code id="cal_validate_isotonic_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a factor). This should be an unquoted column name.</p>
</td></tr>
<tr><td><code id="cal_validate_isotonic_+3A_estimate">estimate</code></td>
<td>
<p>A vector of column identifiers, or one of <code>dplyr</code> selector
functions to choose which variables contains the class probabilities. It
defaults to the prefix used by tidymodels (<code>.pred_</code>). The order of the
identifiers will be considered the same as the order of the levels of the
<code>truth</code> variable.</p>
</td></tr>
<tr><td><code id="cal_validate_isotonic_+3A_metrics">metrics</code></td>
<td>
<p>A set of metrics passed created via <code><a href="yardstick.html#topic+metric_set">yardstick::metric_set()</a></code></p>
</td></tr>
<tr><td><code id="cal_validate_isotonic_+3A_save_pred">save_pred</code></td>
<td>
<p>Indicates whether to a column of post-calibration predictions.</p>
</td></tr>
<tr><td><code id="cal_validate_isotonic_+3A_...">...</code></td>
<td>
<p>Options to pass to <code><a href="#topic+cal_estimate_logistic">cal_estimate_logistic()</a></code>, such as the <code>smooth</code>
argument.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions are designed to calculate performance with and without
calibration. They use resampling to measure out-of-sample effectiveness.
There are two ways to pass the data in:
</p>

<ul>
<li><p> If you have a data frame of predictions, an <code>rset</code> object can be created
via <span class="pkg">rsample</span> functions. See the example below.
</p>
</li>
<li><p> If you have already made a resampling object from the original data and
used it with <code><a href="tune.html#topic+fit_resamples">tune::fit_resamples()</a></code>, you can pass that object to the
calibration function and it will use the same resampling scheme. If a
different resampling scheme should be used, run
<code><a href="tune.html#topic+collect_predictions">tune::collect_predictions()</a></code> on the object and use the process in the
previous bullet point.
</p>
</li></ul>

<p>Please note that these functions do not apply to <code>tune_result</code> objects. The
notion of &quot;validation&quot; implies that the tuning parameter selection has been
resolved.
</p>
<p><code>collect_predictions()</code> can be used to aggregate the metrics for analysis.
</p>


<h3>Value</h3>

<p>The original object with a <code>.metrics_cal</code> column and, optionally,
an additional <code>.predictions_cal</code> column. The class <code>cal_rset</code> is also added.
</p>


<h3>Performance Metrics</h3>

<p>By default, the average of the Brier scores (classification calibration) or the
root mean squared error (regression) is returned. Any appropriate
<code><a href="yardstick.html#topic+metric_set">yardstick::metric_set()</a></code> can be used. The validation function compares the
average of the metrics before, and after the calibration.
</p>


<h3>See Also</h3>

<p><a href="https://www.tidymodels.org/learn/models/calibration/">https://www.tidymodels.org/learn/models/calibration/</a>,
<code><a href="#topic+cal_estimate_isotonic">cal_estimate_isotonic()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dplyr)

segment_logistic %&gt;%
  rsample::vfold_cv() %&gt;%
  cal_validate_isotonic(Class)

</code></pre>

<hr>
<h2 id='cal_validate_isotonic_boot'>Measure performance with and without using bagged isotonic regression calibration</h2><span id='topic+cal_validate_isotonic_boot'></span><span id='topic+cal_validate_isotonic_boot.resample_results'></span><span id='topic+cal_validate_isotonic_boot.rset'></span><span id='topic+cal_validate_isotonic_boot.tune_results'></span>

<h3>Description</h3>

<p>This function uses resampling to measure the effect of calibrating predicted
values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cal_validate_isotonic_boot(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred"),
  metrics = NULL,
  save_pred = FALSE,
  ...
)

## S3 method for class 'resample_results'
cal_validate_isotonic_boot(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred"),
  metrics = NULL,
  save_pred = FALSE,
  ...
)

## S3 method for class 'rset'
cal_validate_isotonic_boot(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred"),
  metrics = NULL,
  save_pred = FALSE,
  ...
)

## S3 method for class 'tune_results'
cal_validate_isotonic_boot(
  .data,
  truth = NULL,
  estimate = NULL,
  metrics = NULL,
  save_pred = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cal_validate_isotonic_boot_+3A_.data">.data</code></td>
<td>
<p>An <code>rset</code> object or the results of <code><a href="tune.html#topic+fit_resamples">tune::fit_resamples()</a></code> with
a <code>.predictions</code> column.</p>
</td></tr>
<tr><td><code id="cal_validate_isotonic_boot_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a factor). This should be an unquoted column name.</p>
</td></tr>
<tr><td><code id="cal_validate_isotonic_boot_+3A_estimate">estimate</code></td>
<td>
<p>A vector of column identifiers, or one of <code>dplyr</code> selector
functions to choose which variables contains the class probabilities. It
defaults to the prefix used by tidymodels (<code>.pred_</code>). The order of the
identifiers will be considered the same as the order of the levels of the
<code>truth</code> variable.</p>
</td></tr>
<tr><td><code id="cal_validate_isotonic_boot_+3A_metrics">metrics</code></td>
<td>
<p>A set of metrics passed created via <code><a href="yardstick.html#topic+metric_set">yardstick::metric_set()</a></code></p>
</td></tr>
<tr><td><code id="cal_validate_isotonic_boot_+3A_save_pred">save_pred</code></td>
<td>
<p>Indicates whether to a column of post-calibration predictions.</p>
</td></tr>
<tr><td><code id="cal_validate_isotonic_boot_+3A_...">...</code></td>
<td>
<p>Options to pass to <code><a href="#topic+cal_estimate_isotonic_boot">cal_estimate_isotonic_boot()</a></code>, such as the
<code>times</code> argument.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions are designed to calculate performance with and without
calibration. They use resampling to measure out-of-sample effectiveness.
There are two ways to pass the data in:
</p>

<ul>
<li><p> If you have a data frame of predictions, an <code>rset</code> object can be created
via <span class="pkg">rsample</span> functions. See the example below.
</p>
</li>
<li><p> If you have already made a resampling object from the original data and
used it with <code><a href="tune.html#topic+fit_resamples">tune::fit_resamples()</a></code>, you can pass that object to the
calibration function and it will use the same resampling scheme. If a
different resampling scheme should be used, run
<code><a href="tune.html#topic+collect_predictions">tune::collect_predictions()</a></code> on the object and use the process in the
previous bullet point.
</p>
</li></ul>

<p>Please note that these functions do not apply to <code>tune_result</code> objects. The
notion of &quot;validation&quot; implies that the tuning parameter selection has been
resolved.
</p>
<p><code>collect_predictions()</code> can be used to aggregate the metrics for analysis.
</p>


<h3>Value</h3>

<p>The original object with a <code>.metrics_cal</code> column and, optionally,
an additional <code>.predictions_cal</code> column. The class <code>cal_rset</code> is also added.
</p>


<h3>Performance Metrics</h3>

<p>By default, the average of the Brier scores (classification calibration) or the
root mean squared error (regression) is returned. Any appropriate
<code><a href="yardstick.html#topic+metric_set">yardstick::metric_set()</a></code> can be used. The validation function compares the
average of the metrics before, and after the calibration.
</p>


<h3>See Also</h3>

<p><a href="https://www.tidymodels.org/learn/models/calibration/">https://www.tidymodels.org/learn/models/calibration/</a>,
<code><a href="#topic+cal_estimate_isotonic_boot">cal_estimate_isotonic_boot()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dplyr)

segment_logistic %&gt;%
  rsample::vfold_cv() %&gt;%
  cal_validate_isotonic_boot(Class)

</code></pre>

<hr>
<h2 id='cal_validate_linear'>Measure performance with and without using linear regression calibration</h2><span id='topic+cal_validate_linear'></span><span id='topic+cal_validate_linear.resample_results'></span><span id='topic+cal_validate_linear.rset'></span>

<h3>Description</h3>

<p>Measure performance with and without using linear regression calibration
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cal_validate_linear(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred"),
  metrics = NULL,
  save_pred = FALSE,
  ...
)

## S3 method for class 'resample_results'
cal_validate_linear(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred"),
  metrics = NULL,
  save_pred = FALSE,
  ...
)

## S3 method for class 'rset'
cal_validate_linear(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred"),
  metrics = NULL,
  save_pred = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cal_validate_linear_+3A_.data">.data</code></td>
<td>
<p>An <code>rset</code> object or the results of <code><a href="tune.html#topic+fit_resamples">tune::fit_resamples()</a></code> with
a <code>.predictions</code> column.</p>
</td></tr>
<tr><td><code id="cal_validate_linear_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a factor). This should be an unquoted column name.</p>
</td></tr>
<tr><td><code id="cal_validate_linear_+3A_estimate">estimate</code></td>
<td>
<p>A vector of column identifiers, or one of <code>dplyr</code> selector
functions to choose which variables contains the class probabilities. It
defaults to the prefix used by tidymodels (<code>.pred_</code>). The order of the
identifiers will be considered the same as the order of the levels of the
<code>truth</code> variable.</p>
</td></tr>
<tr><td><code id="cal_validate_linear_+3A_metrics">metrics</code></td>
<td>
<p>A set of metrics passed created via <code><a href="yardstick.html#topic+metric_set">yardstick::metric_set()</a></code></p>
</td></tr>
<tr><td><code id="cal_validate_linear_+3A_save_pred">save_pred</code></td>
<td>
<p>Indicates whether to a column of post-calibration predictions.</p>
</td></tr>
<tr><td><code id="cal_validate_linear_+3A_...">...</code></td>
<td>
<p>Options to pass to <code><a href="#topic+cal_estimate_logistic">cal_estimate_logistic()</a></code>, such as the <code>smooth</code>
argument.</p>
</td></tr>
</table>


<h3>Performance Metrics</h3>

<p>By default, the average of the root mean square error (RMSE) is returned.
Any appropriate <code><a href="yardstick.html#topic+metric_set">yardstick::metric_set()</a></code> can be used. The validation
function compares the average of the metrics before, and after the calibration.
</p>


<h3>See Also</h3>

<p><a href="https://www.tidymodels.org/learn/models/calibration/">https://www.tidymodels.org/learn/models/calibration/</a>,
<code><a href="#topic+cal_estimate_linear">cal_estimate_linear()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)
library(yardstick)
library(rsample)

head(boosting_predictions_test)

reg_stats &lt;- metric_set(rmse, ccc)

set.seed(828)
boosting_predictions_oob %&gt;%
  # Resample with 10-fold cross-validation
  vfold_cv() %&gt;%
  cal_validate_linear(truth = outcome, smooth = FALSE, metrics = reg_stats)
</code></pre>

<hr>
<h2 id='cal_validate_logistic'>Measure performance with and without using logistic calibration</h2><span id='topic+cal_validate_logistic'></span><span id='topic+cal_validate_logistic.resample_results'></span><span id='topic+cal_validate_logistic.rset'></span><span id='topic+cal_validate_logistic.tune_results'></span>

<h3>Description</h3>

<p>This function uses resampling to measure the effect of calibrating predicted
values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cal_validate_logistic(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred_"),
  metrics = NULL,
  save_pred = FALSE,
  ...
)

## S3 method for class 'resample_results'
cal_validate_logistic(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred_"),
  metrics = NULL,
  save_pred = FALSE,
  ...
)

## S3 method for class 'rset'
cal_validate_logistic(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred_"),
  metrics = NULL,
  save_pred = FALSE,
  ...
)

## S3 method for class 'tune_results'
cal_validate_logistic(
  .data,
  truth = NULL,
  estimate = NULL,
  metrics = NULL,
  save_pred = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cal_validate_logistic_+3A_.data">.data</code></td>
<td>
<p>An <code>rset</code> object or the results of <code><a href="tune.html#topic+fit_resamples">tune::fit_resamples()</a></code> with
a <code>.predictions</code> column.</p>
</td></tr>
<tr><td><code id="cal_validate_logistic_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a factor). This should be an unquoted column name.</p>
</td></tr>
<tr><td><code id="cal_validate_logistic_+3A_estimate">estimate</code></td>
<td>
<p>A vector of column identifiers, or one of <code>dplyr</code> selector
functions to choose which variables contains the class probabilities. It
defaults to the prefix used by tidymodels (<code>.pred_</code>). The order of the
identifiers will be considered the same as the order of the levels of the
<code>truth</code> variable.</p>
</td></tr>
<tr><td><code id="cal_validate_logistic_+3A_metrics">metrics</code></td>
<td>
<p>A set of metrics passed created via <code><a href="yardstick.html#topic+metric_set">yardstick::metric_set()</a></code></p>
</td></tr>
<tr><td><code id="cal_validate_logistic_+3A_save_pred">save_pred</code></td>
<td>
<p>Indicates whether to a column of post-calibration predictions.</p>
</td></tr>
<tr><td><code id="cal_validate_logistic_+3A_...">...</code></td>
<td>
<p>Options to pass to <code><a href="#topic+cal_estimate_logistic">cal_estimate_logistic()</a></code>, such as the <code>smooth</code>
argument.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions are designed to calculate performance with and without
calibration. They use resampling to measure out-of-sample effectiveness.
There are two ways to pass the data in:
</p>

<ul>
<li><p> If you have a data frame of predictions, an <code>rset</code> object can be created
via <span class="pkg">rsample</span> functions. See the example below.
</p>
</li>
<li><p> If you have already made a resampling object from the original data and
used it with <code><a href="tune.html#topic+fit_resamples">tune::fit_resamples()</a></code>, you can pass that object to the
calibration function and it will use the same resampling scheme. If a
different resampling scheme should be used, run
<code><a href="tune.html#topic+collect_predictions">tune::collect_predictions()</a></code> on the object and use the process in the
previous bullet point.
</p>
</li></ul>

<p>Please note that these functions do not apply to <code>tune_result</code> objects. The
notion of &quot;validation&quot; implies that the tuning parameter selection has been
resolved.
</p>
<p><code>collect_predictions()</code> can be used to aggregate the metrics for analysis.
</p>


<h3>Value</h3>

<p>The original object with a <code>.metrics_cal</code> column and, optionally,
an additional <code>.predictions_cal</code> column. The class <code>cal_rset</code> is also added.
</p>


<h3>Performance Metrics</h3>

<p>By default, the average of the Brier scores is returned. Any appropriate
<code><a href="yardstick.html#topic+metric_set">yardstick::metric_set()</a></code> can be used. The validation function compares the
average of the metrics before, and after the calibration.
</p>


<h3>See Also</h3>

<p><a href="https://www.tidymodels.org/learn/models/calibration/">https://www.tidymodels.org/learn/models/calibration/</a>,
<code><a href="#topic+cal_estimate_logistic">cal_estimate_logistic()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dplyr)

# ---------------------------------------------------------------------------
# classification example

segment_logistic %&gt;%
  rsample::vfold_cv() %&gt;%
  cal_validate_logistic(Class)

</code></pre>

<hr>
<h2 id='cal_validate_multinomial'>Measure performance with and without using multinomial calibration</h2><span id='topic+cal_validate_multinomial'></span><span id='topic+cal_validate_multinomial.resample_results'></span><span id='topic+cal_validate_multinomial.rset'></span><span id='topic+cal_validate_multinomial.tune_results'></span>

<h3>Description</h3>

<p>This function uses resampling to measure the effect of calibrating predicted
values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cal_validate_multinomial(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred_"),
  metrics = NULL,
  save_pred = FALSE,
  ...
)

## S3 method for class 'resample_results'
cal_validate_multinomial(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred_"),
  metrics = NULL,
  save_pred = FALSE,
  ...
)

## S3 method for class 'rset'
cal_validate_multinomial(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred_"),
  metrics = NULL,
  save_pred = FALSE,
  ...
)

## S3 method for class 'tune_results'
cal_validate_multinomial(
  .data,
  truth = NULL,
  estimate = NULL,
  metrics = NULL,
  save_pred = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cal_validate_multinomial_+3A_.data">.data</code></td>
<td>
<p>An <code>rset</code> object or the results of <code><a href="tune.html#topic+fit_resamples">tune::fit_resamples()</a></code> with
a <code>.predictions</code> column.</p>
</td></tr>
<tr><td><code id="cal_validate_multinomial_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a factor). This should be an unquoted column name.</p>
</td></tr>
<tr><td><code id="cal_validate_multinomial_+3A_estimate">estimate</code></td>
<td>
<p>A vector of column identifiers, or one of <code>dplyr</code> selector
functions to choose which variables contains the class probabilities. It
defaults to the prefix used by tidymodels (<code>.pred_</code>). The order of the
identifiers will be considered the same as the order of the levels of the
<code>truth</code> variable.</p>
</td></tr>
<tr><td><code id="cal_validate_multinomial_+3A_metrics">metrics</code></td>
<td>
<p>A set of metrics passed created via <code><a href="yardstick.html#topic+metric_set">yardstick::metric_set()</a></code></p>
</td></tr>
<tr><td><code id="cal_validate_multinomial_+3A_save_pred">save_pred</code></td>
<td>
<p>Indicates whether to a column of post-calibration predictions.</p>
</td></tr>
<tr><td><code id="cal_validate_multinomial_+3A_...">...</code></td>
<td>
<p>Options to pass to <code><a href="#topic+cal_estimate_logistic">cal_estimate_logistic()</a></code>, such as the <code>smooth</code>
argument.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions are designed to calculate performance with and without
calibration. They use resampling to measure out-of-sample effectiveness.
There are two ways to pass the data in:
</p>

<ul>
<li><p> If you have a data frame of predictions, an <code>rset</code> object can be created
via <span class="pkg">rsample</span> functions. See the example below.
</p>
</li>
<li><p> If you have already made a resampling object from the original data and
used it with <code><a href="tune.html#topic+fit_resamples">tune::fit_resamples()</a></code>, you can pass that object to the
calibration function and it will use the same resampling scheme. If a
different resampling scheme should be used, run
<code><a href="tune.html#topic+collect_predictions">tune::collect_predictions()</a></code> on the object and use the process in the
previous bullet point.
</p>
</li></ul>

<p>Please note that these functions do not apply to <code>tune_result</code> objects. The
notion of &quot;validation&quot; implies that the tuning parameter selection has been
resolved.
</p>
<p><code>collect_predictions()</code> can be used to aggregate the metrics for analysis.
</p>


<h3>Value</h3>

<p>The original object with a <code>.metrics_cal</code> column and, optionally,
an additional <code>.predictions_cal</code> column. The class <code>cal_rset</code> is also added.
</p>


<h3>Performance Metrics</h3>

<p>By default, the average of the Brier scores is returned. Any appropriate
<code><a href="yardstick.html#topic+metric_set">yardstick::metric_set()</a></code> can be used. The validation function compares the
average of the metrics before, and after the calibration.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cal_apply">cal_apply()</a></code>, <code><a href="#topic+cal_estimate_multinomial">cal_estimate_multinomial()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dplyr)

species_probs %&gt;%
  rsample::vfold_cv() %&gt;%
  cal_validate_multinomial(Species)

</code></pre>

<hr>
<h2 id='class_pred'>Create a class prediction object</h2><span id='topic+class_pred'></span>

<h3>Description</h3>

<p><code>class_pred()</code> creates a <code>class_pred</code> object from a factor or ordered
factor. You can optionally specify values of the factor to be set
as <em>equivocal</em>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>class_pred(x = factor(), which = integer(), equivocal = "[EQ]")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="class_pred_+3A_x">x</code></td>
<td>
<p>A factor or ordered factor.</p>
</td></tr>
<tr><td><code id="class_pred_+3A_which">which</code></td>
<td>
<p>An integer vector specifying the locations of <code>x</code> to declare
as equivocal.</p>
</td></tr>
<tr><td><code id="class_pred_+3A_equivocal">equivocal</code></td>
<td>
<p>A single character specifying the equivocal label used
when printing.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Equivocal values are those that you feel unsure about, and would like to
exclude from performance calculations or other metrics.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- factor(c("Yes", "No", "Yes", "Yes"))

# Create a class_pred object from a factor
class_pred(x)

# Say you aren't sure about that 2nd "Yes" value. You could mark it as
# equivocal.
class_pred(x, which = 3)

# Maybe you want a different equivocal label
class_pred(x, which = 3, equivocal = "eq_value")

</code></pre>

<hr>
<h2 id='collect_metrics.cal_rset'>Obtain and format metrics produced by calibration validation</h2><span id='topic+collect_metrics.cal_rset'></span>

<h3>Description</h3>

<p>Obtain and format metrics produced by calibration validation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cal_rset'
collect_metrics(x, summarize = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="collect_metrics.cal_rset_+3A_x">x</code></td>
<td>
<p>An object produced by one of the validation function (or class
<code>cal_rset</code>).</p>
</td></tr>
<tr><td><code id="collect_metrics.cal_rset_+3A_summarize">summarize</code></td>
<td>
<p>A logical; should metrics be summarized over resamples
(<code>TRUE</code>) or return the values for each individual resample. See
<code><a href="tune.html#topic+collect_predictions">tune::collect_metrics()</a></code> for more details.</p>
</td></tr>
<tr><td><code id="collect_metrics.cal_rset_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble
</p>

<hr>
<h2 id='collect_predictions.cal_rset'>Obtain and format predictions produced by calibration validation</h2><span id='topic+collect_predictions.cal_rset'></span>

<h3>Description</h3>

<p>Obtain and format predictions produced by calibration validation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cal_rset'
collect_predictions(x, summarize = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="collect_predictions.cal_rset_+3A_x">x</code></td>
<td>
<p>An object produced by one of the validation function (or class
<code>cal_rset</code>).</p>
</td></tr>
<tr><td><code id="collect_predictions.cal_rset_+3A_summarize">summarize</code></td>
<td>
<p>A logical; should predictions be summarized over resamples
(<code>TRUE</code>) or return the values for each individual resample. See
<code><a href="tune.html#topic+collect_predictions">tune::collect_predictions()</a></code> for more details.</p>
</td></tr>
<tr><td><code id="collect_predictions.cal_rset_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble
</p>

<hr>
<h2 id='control_conformal_full'>Controlling the numeric details for conformal inference</h2><span id='topic+control_conformal_full'></span>

<h3>Description</h3>

<p>Controlling the numeric details for conformal inference
</p>


<h3>Usage</h3>

<pre><code class='language-R'>control_conformal_full(
  method = "iterative",
  trial_points = 100,
  var_multiplier = 10,
  max_iter = 100,
  tolerance = .Machine$double.eps^0.25,
  progress = FALSE,
  required_pkgs = character(0),
  seed = sample.int(10^5, 1)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="control_conformal_full_+3A_method">method</code></td>
<td>
<p>The method for computing the intervals. The options are
<code>'search'</code> (using) <code><a href="stats.html#topic+uniroot">stats::uniroot()</a></code>, and <code>'grid'</code>.</p>
</td></tr>
<tr><td><code id="control_conformal_full_+3A_trial_points">trial_points</code></td>
<td>
<p>When <code>method = "grid"</code>, how many points should be
evaluated?</p>
</td></tr>
<tr><td><code id="control_conformal_full_+3A_var_multiplier">var_multiplier</code></td>
<td>
<p>A multiplier for the variance model that determines the
possible range of the bounds.</p>
</td></tr>
<tr><td><code id="control_conformal_full_+3A_max_iter">max_iter</code></td>
<td>
<p>When <code>method = "iterative"</code>, the maximum number of iterations.</p>
</td></tr>
<tr><td><code id="control_conformal_full_+3A_tolerance">tolerance</code></td>
<td>
<p>Tolerance value passed to <code><a href="base.html#topic+all.equal">all.equal()</a></code> to determine
convergence during the search computations.</p>
</td></tr>
<tr><td><code id="control_conformal_full_+3A_progress">progress</code></td>
<td>
<p>Should a progress bar be used to track execution?</p>
</td></tr>
<tr><td><code id="control_conformal_full_+3A_required_pkgs">required_pkgs</code></td>
<td>
<p>An optional character string for which packages are
required.</p>
</td></tr>
<tr><td><code id="control_conformal_full_+3A_seed">seed</code></td>
<td>
<p>A single integer used to control randomness when models are
(re)fit.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list object with the options given by the user.
</p>

<hr>
<h2 id='int_conformal_cv'>Prediction intervals via conformal inference CV+</h2><span id='topic+int_conformal_cv'></span><span id='topic+int_conformal_cv.default'></span><span id='topic+int_conformal_cv.resample_results'></span><span id='topic+int_conformal_cv.tune_results'></span>

<h3>Description</h3>

<p>Nonparametric prediction intervals can be computed for fitted regression
workflow objects using the CV+ conformal inference method described by
Barber <em>at al</em> (2018).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>int_conformal_cv(object, ...)

## Default S3 method:
int_conformal_cv(object, ...)

## S3 method for class 'resample_results'
int_conformal_cv(object, ...)

## S3 method for class 'tune_results'
int_conformal_cv(object, parameters, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="int_conformal_cv_+3A_object">object</code></td>
<td>
<p>An object from a tidymodels resampling or tuning function such
as <code><a href="tune.html#topic+fit_resamples">tune::fit_resamples()</a></code>, <code><a href="tune.html#topic+tune_grid">tune::tune_grid()</a></code>, or similar. The object
should have been produced in a way that the <code>.extracts</code> column contains the
fitted workflow for each resample (see the Details below).</p>
</td></tr>
<tr><td><code id="int_conformal_cv_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="int_conformal_cv_+3A_parameters">parameters</code></td>
<td>
<p>An tibble of tuning parameter values that can be
used to filter the predicted values before processing. This tibble should
select a single set of hyper-parameter values from the tuning results. This is
only required when a tuning object is passed to <code>object</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function implements the CV+ method found in Section 3 of Barber <em>at al</em>
(2018). It uses the resampled model fits and their associated holdout
residuals to make prediction intervals for regression models.
</p>
<p>This function prepares the objects for the computations. The <code><a href="stats.html#topic+predict">predict()</a></code>
method computes the intervals for new data.
</p>
<p>This method was developed for V-fold cross-validation (no repeats). Interval
coverage is unknown for any other resampling methods. The function will not
stop the computations for other types of resamples, but we have no way of
knowing whether the results are appropriate.
</p>


<h3>Value</h3>

<p>An object of class <code>"int_conformal_cv"</code> containing the information
to create intervals. The <code>predict()</code> method is used to produce the intervals.
</p>


<h3>References</h3>

<p>Rina Foygel Barber, Emmanuel J. Candès, Aaditya Ramdas, Ryan J. Tibshirani
&quot;Predictive inference with the jackknife+,&quot; <em>The Annals of Statistics</em>,
49(1), 486-507, 2021
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.int_conformal_cv">predict.int_conformal_cv()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(workflows)
library(dplyr)
library(parsnip)
library(rsample)
library(tune)
library(modeldata)

set.seed(2)
sim_train &lt;- sim_regression(200)
sim_new &lt;- sim_regression(5) %&gt;% select(-outcome)

sim_rs &lt;- vfold_cv(sim_train)

# We'll use a neural network model
mlp_spec &lt;-
  mlp(hidden_units = 5, penalty = 0.01) %&gt;%
  set_mode("regression")

# Use a control function that saves the predictions as well as the models.
# Consider using the butcher package in the extracts function to have smaller
# object sizes

ctrl &lt;- control_resamples(save_pred = TRUE, extract = I)

set.seed(3)
nnet_res &lt;-
  mlp_spec %&gt;%
  fit_resamples(outcome ~ ., resamples = sim_rs, control = ctrl)

nnet_int_obj &lt;- int_conformal_cv(nnet_res)
nnet_int_obj

predict(nnet_int_obj, sim_new)

</code></pre>

<hr>
<h2 id='int_conformal_full'>Prediction intervals via conformal inference</h2><span id='topic+int_conformal_full'></span><span id='topic+int_conformal_full.default'></span><span id='topic+int_conformal_full.workflow'></span>

<h3>Description</h3>

<p>Nonparametric prediction intervals can be computed for fitted workflow
objects using the conformal inference method described by Lei <em>at al</em> (2018).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>int_conformal_full(object, ...)

## Default S3 method:
int_conformal_full(object, ...)

## S3 method for class 'workflow'
int_conformal_full(object, train_data, ..., control = control_conformal_full())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="int_conformal_full_+3A_object">object</code></td>
<td>
<p>A fitted <code><a href="workflows.html#topic+workflow">workflows::workflow()</a></code> object.</p>
</td></tr>
<tr><td><code id="int_conformal_full_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="int_conformal_full_+3A_train_data">train_data</code></td>
<td>
<p>A data frame with the <em>original predictor data</em> used to
create the fitted workflow (predictors and outcomes). If the workflow does
not contain these values, pass them here. If the workflow used a recipe, this
should be the data that were inputs to the recipe (and not the product of a
recipe).</p>
</td></tr>
<tr><td><code id="int_conformal_full_+3A_control">control</code></td>
<td>
<p>A control object from <code><a href="#topic+control_conformal_full">control_conformal_full()</a></code> with the
numeric minutiae.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function implements what is usually called &quot;full conformal inference&quot;
(see Algorithm 1 in Lei <em>et al</em> (2018)) since it uses the entire training
set to compute the intervals.
</p>
<p>This function prepares the objects for the computations. The <code><a href="stats.html#topic+predict">predict()</a></code>
method computes the intervals for new data.
</p>
<p>For a given new_data observation, the predictors are appended to the original
training set. Then, different &quot;trial&quot; values of the outcome are substituted
in for that observation's outcome and the model is re-fit. From each model,
the residual associated with the trial value is compared to a quantile of the
distribution of the other residuals. Usually the absolute values of the
residuals are used. Once the residual of a trial value exceeds the
distributional quantile, the value is one of the bounds.
</p>
<p>The literature proposed using a grid search of trial values to find the two
points that correspond to the prediction intervals. To use this approach,
set <code>method = "grid"</code> in <code><a href="#topic+control_conformal_full">control_conformal_full()</a></code>. However, the default method
<code style="white-space: pre;">&#8288;"search&#8288;</code> uses two different one-dimensional iterative searches on either
side of the predicted value to find values that correspond to the prediction intervals.
</p>
<p>For medium to large data sets, the iterative search method is likely to
generate slightly smaller intervals. For small training sets, grid search
is more likely to have somewhat smaller intervals (and will be more stable).
Otherwise, the iterative search method is more precise and several folds
faster.
</p>
<p>To determine a range of possible values of the intervals, used by both methods,
the initial set of training set residuals are modeled using a Gamma generalized
linear model with a log link (see the reference by Aitkin below). For a new
sample, the absolute size of the residual is estimated and a multiple of
this value is computed as an initial guess of the search boundaries.
</p>


<h4>Speed</h4>

<p>The time it takes to compute the intervals depends on the training set
size, search parameters (i.e., convergence criterion, number of
iterations), the grid size, and the number of worker processes that are
used. For the last item, the computations can be parallelized using the
future and furrr packages.
</p>
<p>To use parallelism, the <code><a href="future.html#topic+plan">future::plan()</a></code> function can
be invoked to create a parallel backend. For example, let’s make an
initial workflow:
</p>
<div class="sourceCode r"><pre>library(tidymodels)
library(probably)
library(future)

tidymodels_prefer()

## Make a fitted workflow from some simulated data:
set.seed(121)
train_dat &lt;- sim_regression(200)
new_dat   &lt;- sim_regression(  5) %&gt;% select(-outcome)

lm_fit &lt;- 
  workflow() %&gt;% 
  add_model(linear_reg()) %&gt;% 
  add_formula(outcome ~ .) %&gt;% 
  fit(data = train_dat)

# Create the object to be used to make prediction intervals
lm_conform &lt;- int_conformal_full(lm_fit, train_dat)
</pre></div>
<p>We’ll use a <code>"multisession"</code> parallel processing plan to compute the
intervals for the five new samples in parallel:
</p>
<div class="sourceCode r"><pre>plan("multisession")

# This is run in parallel:
predict(lm_conform, new_dat)
</pre></div>
<div class="sourceCode"><pre>## # A tibble: 5 x 2
##   .pred_lower .pred_upper
##         &lt;dbl&gt;       &lt;dbl&gt;
## 1       -17.9        59.6
## 2       -33.7        51.1
## 3       -30.6        48.2
## 4       -17.3        59.6
## 5       -23.3        55.2
</pre></div>
<p>Using simulations, there are slightly sub-linear speed-ups when using
parallel processing to compute the row-wise intervals.
</p>
<p>In comparison with parametric intervals:
</p>
<div class="sourceCode r"><pre>predict(lm_fit, new_dat, type = "pred_int")
</pre></div>
<div class="sourceCode"><pre>## # A tibble: 5 x 2
##   .pred_lower .pred_upper
##         &lt;dbl&gt;       &lt;dbl&gt;
## 1       -19.2        59.1
## 2       -31.8        49.7
## 3       -31.0        47.6
## 4       -17.8        60.1
## 5       -23.6        54.3
</pre></div>



<h3>Value</h3>

<p>An object of class <code>"int_conformal_full"</code> containing the information
to create intervals (which includes the training set data). The <code>predict()</code>
method is used to produce the intervals.
</p>


<h3>References</h3>

<p>Jing Lei, Max G'Sell, Alessandro Rinaldo, Ryan J. Tibshirani and Larry
Wasserman (2018) Distribution-Free Predictive Inference for Regression,
<em>Journal of the American Statistical Association</em>, 113:523, 1094-1111
</p>
<p>Murray Aitkin, Modelling Variance Heterogeneity in Normal Regression Using
GLIM, <em>Journal of the Royal Statistical Society Series C: Applied Statistics</em>,
Volume 36, Issue 3, November 1987, Pages 332–339.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.int_conformal_full">predict.int_conformal_full()</a></code>
</p>

<hr>
<h2 id='int_conformal_quantile'>Prediction intervals via conformal inference and quantile regression</h2><span id='topic+int_conformal_quantile'></span><span id='topic+int_conformal_quantile.workflow'></span>

<h3>Description</h3>

<p>Nonparametric prediction intervals can be computed for fitted regression
workflow objects using the split conformal inference method described by
Romano <em>et al</em> (2019). To compute quantiles, this function uses Quantile
Random Forests instead of classic quantile regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>int_conformal_quantile(object, ...)

## S3 method for class 'workflow'
int_conformal_quantile(object, train_data, cal_data, level = 0.95, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="int_conformal_quantile_+3A_object">object</code></td>
<td>
<p>A fitted <code><a href="workflows.html#topic+workflow">workflows::workflow()</a></code> object.</p>
</td></tr>
<tr><td><code id="int_conformal_quantile_+3A_...">...</code></td>
<td>
<p>Options to pass to <code><a href="quantregForest.html#topic+quantregForest">quantregForest::quantregForest()</a></code> (such as the
number of trees).</p>
</td></tr>
<tr><td><code id="int_conformal_quantile_+3A_train_data">train_data</code>, <code id="int_conformal_quantile_+3A_cal_data">cal_data</code></td>
<td>
<p>Data frames with the <em>predictor and outcome data</em>.
<code>train_data</code> should be the same data used to produce <code>object</code> and <code>cal_data</code> is
used to produce predictions (and residuals). If the workflow used a recipe,
these should be the data that were inputs to the recipe (and not the product
of a recipe).</p>
</td></tr>
<tr><td><code id="int_conformal_quantile_+3A_level">level</code></td>
<td>
<p>The confidence level for the intervals.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that the significance level should be specified in this function
(instead of the <code>predict()</code> method).
</p>
<p><code>cal_data</code> should be large enough to get a good estimates of a extreme
quantile (e.g., the 95th for 95% interval) and should not include rows that
were in the original training set.
</p>
<p>Note that the because of the method used to construct the interval, it is
possible that the prediction intervals will not include the predicted value.
</p>


<h3>Value</h3>

<p>An object of class <code>"int_conformal_quantile"</code> containing the
information to create intervals (which includes <code>object</code>).
The <code>predict()</code> method is used to produce the intervals.
</p>


<h3>References</h3>

<p>Romano, Yaniv, Evan Patterson, and Emmanuel Candes. &quot;Conformalized quantile
regression.&quot; <em>Advances in neural information processing systems</em> 32 (2019).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.int_conformal_quantile">predict.int_conformal_quantile()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(workflows)
library(dplyr)
library(parsnip)
library(rsample)
library(tune)
library(modeldata)

set.seed(2)
sim_train &lt;- sim_regression(500)
sim_cal &lt;- sim_regression(200)
sim_new &lt;- sim_regression(5) %&gt;% select(-outcome)

# We'll use a neural network model
mlp_spec &lt;-
  mlp(hidden_units = 5, penalty = 0.01) %&gt;%
  set_mode("regression")

mlp_wflow &lt;-
  workflow() %&gt;%
  add_model(mlp_spec) %&gt;%
  add_formula(outcome ~ .)

mlp_fit &lt;- fit(mlp_wflow, data = sim_train)

mlp_int &lt;- int_conformal_quantile(mlp_fit, sim_train, sim_cal,
  level = 0.90
)
mlp_int

predict(mlp_int, sim_new)

</code></pre>

<hr>
<h2 id='int_conformal_split'>Prediction intervals via split conformal inference</h2><span id='topic+int_conformal_split'></span><span id='topic+int_conformal_split.default'></span><span id='topic+int_conformal_split.workflow'></span>

<h3>Description</h3>

<p>Nonparametric prediction intervals can be computed for fitted regression
workflow objects using the split conformal inference method described by
Lei <em>et al</em> (2018).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>int_conformal_split(object, ...)

## Default S3 method:
int_conformal_split(object, ...)

## S3 method for class 'workflow'
int_conformal_split(object, cal_data, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="int_conformal_split_+3A_object">object</code></td>
<td>
<p>A fitted <code><a href="workflows.html#topic+workflow">workflows::workflow()</a></code> object.</p>
</td></tr>
<tr><td><code id="int_conformal_split_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="int_conformal_split_+3A_cal_data">cal_data</code></td>
<td>
<p>A data frame with the <em>original predictor and outcome data</em>
used to produce predictions (and residuals). If the workflow used a recipe,
this should be the data that were inputs to the recipe (and not the product
of a recipe).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function implements what is usually called &quot;split conformal inference&quot;
(see Algorithm 1 in Lei <em>et al</em> (2018)).
</p>
<p>This function prepares the statistics for the interval computations. The
<code><a href="stats.html#topic+predict">predict()</a></code> method computes the intervals for new data and the signficance
level is specified there.
</p>
<p><code>cal_data</code> should be large enough to get a good estimates of a extreme
quantile (e.g., the 95th for 95% interval) and should not include rows that
were in the original training set.
</p>


<h3>Value</h3>

<p>An object of class <code>"int_conformal_split"</code> containing the
information to create intervals (which includes <code>object</code>).
The <code>predict()</code> method is used to produce the intervals.
</p>


<h3>References</h3>

<p>Lei, Jing, et al. &quot;Distribution-free predictive inference for regression.&quot;
<em>Journal of the American Statistical Association</em> 113.523 (2018): 1094-1111.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.int_conformal_split">predict.int_conformal_split()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(workflows)
library(dplyr)
library(parsnip)
library(rsample)
library(tune)
library(modeldata)

set.seed(2)
sim_train &lt;- sim_regression(500)
sim_cal &lt;- sim_regression(200)
sim_new &lt;- sim_regression(5) %&gt;% select(-outcome)

# We'll use a neural network model
mlp_spec &lt;-
  mlp(hidden_units = 5, penalty = 0.01) %&gt;%
  set_mode("regression")

mlp_wflow &lt;-
  workflow() %&gt;%
  add_model(mlp_spec) %&gt;%
  add_formula(outcome ~ .)

mlp_fit &lt;- fit(mlp_wflow, data = sim_train)

mlp_int &lt;- int_conformal_split(mlp_fit, sim_cal)
mlp_int

predict(mlp_int, sim_new, level = 0.90)

</code></pre>

<hr>
<h2 id='is_class_pred'>Test if an object inherits from <code>class_pred</code></h2><span id='topic+is_class_pred'></span>

<h3>Description</h3>

<p><code>is_class_pred()</code> checks if an object is a <code>class_pred</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_class_pred(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is_class_pred_+3A_x">x</code></td>
<td>
<p>An object.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- class_pred(factor(1:5))

is_class_pred(x)

</code></pre>

<hr>
<h2 id='levels.class_pred'>Extract <code>class_pred</code> levels</h2><span id='topic+levels.class_pred'></span>

<h3>Description</h3>

<p>The levels of a <code>class_pred</code> object do <em>not</em> include the equivocal value.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'class_pred'
levels(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="levels.class_pred_+3A_x">x</code></td>
<td>
<p>A <code>class_pred</code> object.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- class_pred(factor(1:5), which = 1)

# notice that even though `1` is not in the `class_pred` vector, the
# level remains from the original factor
levels(x)

</code></pre>

<hr>
<h2 id='locate-equivocal'>Locate equivocal values</h2><span id='topic+locate-equivocal'></span><span id='topic+is_equivocal'></span><span id='topic+which_equivocal'></span><span id='topic+any_equivocal'></span>

<h3>Description</h3>

<p>These functions provide multiple methods of checking for equivocal values,
and finding their locations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_equivocal(x)

which_equivocal(x)

any_equivocal(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="locate-equivocal_+3A_x">x</code></td>
<td>
<p>A <code>class_pred</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>is_equivocal()</code> returns a logical vector the same length as <code>x</code>
where <code>TRUE</code> means the value is equivocal.
</p>
<p><code>which_equivocal()</code> returns an integer vector specifying the locations
of the equivocal values.
</p>
<p><code>any_equivocal()</code> returns <code>TRUE</code> if there are any equivocal values.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- class_pred(factor(1:10), which = c(2, 5))

is_equivocal(x)

which_equivocal(x)

any_equivocal(x)

</code></pre>

<hr>
<h2 id='make_class_pred'>Create a <code>class_pred</code> vector from class probabilities</h2><span id='topic+make_class_pred'></span><span id='topic+make_two_class_pred'></span>

<h3>Description</h3>

<p>These functions can be used to convert class probability estimates to
<code>class_pred</code> objects with an optional equivocal zone.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make_class_pred(..., levels, ordered = FALSE, min_prob = 1/length(levels))

make_two_class_pred(
  estimate,
  levels,
  threshold = 0.5,
  ordered = FALSE,
  buffer = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make_class_pred_+3A_...">...</code></td>
<td>
<p>Numeric vectors corresponding to class probabilities. There should
be one for each level in <code>levels</code>, and <em>it is assumed that the vectors
are in the same order as <code>levels</code></em>.</p>
</td></tr>
<tr><td><code id="make_class_pred_+3A_levels">levels</code></td>
<td>
<p>A character vector of class levels. The length should be the
same as the number of selections made through <code>...</code>, or length <code>2</code>
for <code>make_two_class_pred()</code>.</p>
</td></tr>
<tr><td><code id="make_class_pred_+3A_ordered">ordered</code></td>
<td>
<p>A single logical to determine if the levels should be regarded
as ordered (in the order given). This results in a <code>class_pred</code> object
that is flagged as ordered.</p>
</td></tr>
<tr><td><code id="make_class_pred_+3A_min_prob">min_prob</code></td>
<td>
<p>A single numeric value. If any probabilities are less than
this value (by row), the row is marked as <em>equivocal</em>.</p>
</td></tr>
<tr><td><code id="make_class_pred_+3A_estimate">estimate</code></td>
<td>
<p>A single numeric vector corresponding to the class
probabilities of the first level in <code>levels</code>.</p>
</td></tr>
<tr><td><code id="make_class_pred_+3A_threshold">threshold</code></td>
<td>
<p>A single numeric value for the threshold to call a row to
be labeled as the first value of <code>levels</code>.</p>
</td></tr>
<tr><td><code id="make_class_pred_+3A_buffer">buffer</code></td>
<td>
<p>A numeric vector of length 1 or 2 for the buffer around
<code>threshold</code> that defines the equivocal zone (i.e., <code>threshold - buffer[1]</code> to
<code>threshold + buffer[2]</code>). A length 1 vector is recycled to length 2. The
default, <code>NULL</code>, is interpreted as no equivocal zone.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of class <code><a href="#topic+class_pred">class_pred</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dplyr)

good &lt;- segment_logistic$.pred_good
lvls &lt;- levels(segment_logistic$Class)

# Equivocal zone of .5 +/- .15
make_two_class_pred(good, lvls, buffer = 0.15)

# Equivocal zone of c(.5 - .05, .5 + .15)
make_two_class_pred(good, lvls, buffer = c(0.05, 0.15))

# These functions are useful alongside dplyr::mutate()
segment_logistic %&gt;%
  mutate(
    .class_pred = make_two_class_pred(
      estimate = .pred_good,
      levels = levels(Class),
      buffer = 0.15
    )
  )

# Multi-class example
# Note that we provide class probability columns in the same
# order as the levels
species_probs %&gt;%
  mutate(
    .class_pred = make_class_pred(
      .pred_bobcat, .pred_coyote, .pred_gray_fox,
      levels = levels(Species),
      min_prob = .5
    )
  )

</code></pre>

<hr>
<h2 id='predict.int_conformal_full'>Prediction intervals from conformal methods</h2><span id='topic+predict.int_conformal_full'></span><span id='topic+predict.int_conformal_cv'></span><span id='topic+predict.int_conformal_quantile'></span><span id='topic+predict.int_conformal_split'></span>

<h3>Description</h3>

<p>Prediction intervals from conformal methods
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'int_conformal_full'
predict(object, new_data, level = 0.95, ...)

## S3 method for class 'int_conformal_cv'
predict(object, new_data, level = 0.95, ...)

## S3 method for class 'int_conformal_quantile'
predict(object, new_data, ...)

## S3 method for class 'int_conformal_split'
predict(object, new_data, level = 0.95, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.int_conformal_full_+3A_object">object</code></td>
<td>
<p>An object produced by <code><a href="#topic+predict.int_conformal_full">predict.int_conformal_full()</a></code>.</p>
</td></tr>
<tr><td><code id="predict.int_conformal_full_+3A_new_data">new_data</code></td>
<td>
<p>A data frame of predictors.</p>
</td></tr>
<tr><td><code id="predict.int_conformal_full_+3A_level">level</code></td>
<td>
<p>The confidence level for the intervals.</p>
</td></tr>
<tr><td><code id="predict.int_conformal_full_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For the CV+. estimator produced by <code><a href="#topic+int_conformal_cv">int_conformal_cv()</a></code>, the intervals
are centered around the mean of the predictions produced by the
resample-specific model. For example, with 10-fold cross-validation, <code>.pred</code>
is the average of the predictions from the 10 models produced by each fold.
This may differ from the prediction generated from a model fit that was
trained on the entire training set, especially if the training sets are
small.
</p>


<h3>Value</h3>

<p>A tibble with columns <code>.pred_lower</code> and <code>.pred_upper</code>. If
the computations for the prediction bound fail, a missing value is used. For
objects produced by <code><a href="#topic+int_conformal_cv">int_conformal_cv()</a></code>, an additional <code>.pred</code> column
is  also returned (see Details below).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+int_conformal_full">int_conformal_full()</a></code>, <code><a href="#topic+int_conformal_cv">int_conformal_cv()</a></code>
</p>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic+fit'></span><span id='topic+augment'></span><span id='topic+required_pkgs'></span><span id='topic+collect_metrics'></span><span id='topic+collect_predictions'></span><span id='topic+as.factor'></span><span id='topic+as.ordered'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>generics</dt><dd><p><code><a href="generics.html#topic+coercion-factor">as.factor</a></code>, <code><a href="generics.html#topic+coercion-factor">as.ordered</a></code>, <code><a href="generics.html#topic+augment">augment</a></code>, <code><a href="generics.html#topic+fit">fit</a></code>, <code><a href="generics.html#topic+required_pkgs">required_pkgs</a></code></p>
</dd>
<dt>tune</dt><dd><p><code><a href="tune.html#topic+collect_predictions">collect_metrics</a></code>, <code><a href="tune.html#topic+collect_predictions">collect_predictions</a></code></p>
</dd>
</dl>

<hr>
<h2 id='reportable_rate'>Calculate the reportable rate</h2><span id='topic+reportable_rate'></span>

<h3>Description</h3>

<p>The <em>reportable rate</em> is defined as the percentage of class predictions
that are <em>not</em> equivocal.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reportable_rate(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reportable_rate_+3A_x">x</code></td>
<td>
<p>A <code>class_pred</code> object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The reportable rate is calculated as <code>(n_not_equivocal / n)</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- class_pred(factor(1:5), which = c(1, 2))

# 3 / 5
reportable_rate(x)

</code></pre>

<hr>
<h2 id='required_pkgs.cal_estimate_beta'>S3 methods to track which additional packages are needed for specific
calibrations</h2><span id='topic+required_pkgs.cal_estimate_beta'></span><span id='topic+required_pkgs.cal_estimate_linear_spline'></span><span id='topic+required_pkgs.cal_estimate_logistic_spline'></span><span id='topic+required_pkgs.cal_estimate_multinomial'></span><span id='topic+required_pkgs.cal_object'></span>

<h3>Description</h3>

<p>S3 methods to track which additional packages are needed for specific
calibrations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cal_estimate_beta'
required_pkgs(x, ...)

## S3 method for class 'cal_estimate_linear_spline'
required_pkgs(x, ...)

## S3 method for class 'cal_estimate_logistic_spline'
required_pkgs(x, ...)

## S3 method for class 'cal_estimate_multinomial'
required_pkgs(x, ...)

## S3 method for class 'cal_object'
required_pkgs(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="required_pkgs.cal_estimate_beta_+3A_x">x</code></td>
<td>
<p>A calibration object</p>
</td></tr>
<tr><td><code id="required_pkgs.cal_estimate_beta_+3A_...">...</code></td>
<td>
<p>Other arguments passed to methods</p>
</td></tr>
</table>

<hr>
<h2 id='segment_naive_bayes'>Image segmentation predictions</h2><span id='topic+segment_naive_bayes'></span><span id='topic+segment_logistic'></span>

<h3>Description</h3>

<p>Image segmentation predictions
</p>


<h3>Details</h3>

<p>These objects contain test set predictions for the cell segmentation
data from Hill, LaPan, Li and Haney (2007). Each data frame are the results
from different models (naive Bayes and logistic regression).
</p>


<h3>Value</h3>

<table>
<tr><td><code>segment_naive_bayes</code>, <code>segment_logistic</code></td>
<td>
<p>a tibble</p>
</td></tr>
</table>


<h3>Source</h3>

<p>Hill, LaPan, Li and Haney (2007). Impact of image segmentation on
high-content screening data quality for SK-BR-3 cells, <em>BMC
Bioinformatics</em>, Vol. 8, pg. 340,
<a href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-8-340">https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-8-340</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(segment_naive_bayes)
data(segment_logistic)
</code></pre>

<hr>
<h2 id='species_probs'>Predictions on animal species</h2><span id='topic+species_probs'></span>

<h3>Description</h3>

<p>Predictions on animal species
</p>


<h3>Details</h3>

<p>These data are holdout predictions from resampling for the animal
scat data of Reid (2015) based on a C5.0 classification model.
</p>


<h3>Value</h3>

<table>
<tr><td><code>species_probs</code></td>
<td>
<p>a tibble</p>
</td></tr>
</table>


<h3>Source</h3>

<p>Reid, R. E. B. (2015). A morphometric modeling approach to
distinguishing among bobcat, coyote and gray fox scats. <em>Wildlife
Biology</em>, 21(5), 254-262
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(species_probs)
str(species_probs)
</code></pre>

<hr>
<h2 id='threshold_perf'>Generate performance metrics across probability thresholds</h2><span id='topic+threshold_perf'></span><span id='topic+threshold_perf.data.frame'></span>

<h3>Description</h3>

<p><code>threshold_perf()</code> can take a set of class probability predictions
and determine performance characteristics across different values
of the probability threshold and any existing groups.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>threshold_perf(.data, ...)

## S3 method for class 'data.frame'
threshold_perf(
  .data,
  truth,
  estimate,
  thresholds = NULL,
  metrics = NULL,
  na_rm = TRUE,
  event_level = "first",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="threshold_perf_+3A_.data">.data</code></td>
<td>
<p>A tibble, potentially grouped.</p>
</td></tr>
<tr><td><code id="threshold_perf_+3A_...">...</code></td>
<td>
<p>Currently unused.</p>
</td></tr>
<tr><td><code id="threshold_perf_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true two-class results
(that is a factor). This should be an unquoted column name.</p>
</td></tr>
<tr><td><code id="threshold_perf_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predicted class probabilities
(that is a numeric). This should be an unquoted column name.</p>
</td></tr>
<tr><td><code id="threshold_perf_+3A_thresholds">thresholds</code></td>
<td>
<p>A numeric vector of values for the probability
threshold. If unspecified, a series
of values between 0.5 and 1.0 are used. <strong>Note</strong>: if this
argument is used, it must be named.</p>
</td></tr>
<tr><td><code id="threshold_perf_+3A_metrics">metrics</code></td>
<td>
<p>Either <code>NULL</code> or a <code><a href="yardstick.html#topic+metric_set">yardstick::metric_set()</a></code> with a list of
performance metrics to calculate. The metrics should all be oriented towards
hard class predictions (e.g. <code><a href="yardstick.html#topic+sens">yardstick::sensitivity()</a></code>,
<code><a href="yardstick.html#topic+accuracy">yardstick::accuracy()</a></code>, <code><a href="yardstick.html#topic+recall">yardstick::recall()</a></code>, etc.) and not
class probabilities. A set of default metrics is used when <code>NULL</code> (see
Details below).</p>
</td></tr>
<tr><td><code id="threshold_perf_+3A_na_rm">na_rm</code></td>
<td>
<p>A single logical: should missing data be removed?</p>
</td></tr>
<tr><td><code id="threshold_perf_+3A_event_level">event_level</code></td>
<td>
<p>A single string. Either <code>"first"</code> or <code>"second"</code> to specify
which level of <code>truth</code> to consider as the &quot;event&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that that the global option <code>yardstick.event_first</code> will be
used to determine which level is the event of interest. For more details,
see the Relevant level section of <code><a href="yardstick.html#topic+sens">yardstick::sens()</a></code>.
</p>
<p>The default calculated metrics are:
</p>

<ul>
<li> <p><code><a href="yardstick.html#topic+j_index">yardstick::j_index()</a></code>
</p>
</li>
<li> <p><code><a href="yardstick.html#topic+sens">yardstick::sens()</a></code>
</p>
</li>
<li> <p><code><a href="yardstick.html#topic+spec">yardstick::spec()</a></code>
</p>
</li>
<li> <p><code>distance = (1 - sens) ^ 2 + (1 - spec) ^ 2</code>
</p>
</li></ul>

<p>If a custom metric is passed that does not compute sensitivity and
specificity, the distance metric is not computed.
</p>


<h3>Value</h3>

<p>A tibble with columns: <code>.threshold</code>, <code>.estimator</code>, <code>.metric</code>,
<code>.estimate</code> and any existing groups.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)
data("segment_logistic")

# Set the threshold to 0.6
# &gt; 0.6 = good
# &lt; 0.6 = poor
threshold_perf(segment_logistic, Class, .pred_good, thresholds = 0.6)

# Set the threshold to multiple values
thresholds &lt;- seq(0.5, 0.9, by = 0.1)

segment_logistic %&gt;%
  threshold_perf(Class, .pred_good, thresholds)

# ---------------------------------------------------------------------------

# It works with grouped data frames as well
# Let's mock some resampled data
resamples &lt;- 5

mock_resamples &lt;- resamples %&gt;%
  replicate(
    expr = sample_n(segment_logistic, 100, replace = TRUE),
    simplify = FALSE
  ) %&gt;%
  bind_rows(.id = "resample")

resampled_threshold_perf &lt;- mock_resamples %&gt;%
  group_by(resample) %&gt;%
  threshold_perf(Class, .pred_good, thresholds)

resampled_threshold_perf

# Average over the resamples
resampled_threshold_perf %&gt;%
  group_by(.metric, .threshold) %&gt;%
  summarise(.estimate = mean(.estimate))

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
