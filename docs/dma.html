<!DOCTYPE html><html><head><title>Help for package dma</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {dma}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#dma'><p>Dynamic model averaging for continuous outcomes</p></a></li>
<li><a href='#dma-package'>
<p>Dynamic model averaging</p></a></li>
<li><a href='#logistic.dma'>
<p>Dynamic model averaging for binary outcomes</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Dynamic Model Averaging</td>
</tr>
<tr>
<td>Version:</td>
<td>1.4-0</td>
</tr>
<tr>
<td>Date:</td>
<td>2018-10-04</td>
</tr>
<tr>
<td>Author:</td>
<td>Tyler H. McCormick, Adrian Raftery, David Madigan, Sevvandi Kandanaarachchi [ctb], Hana Sevcikova [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Hana Sevcikova &lt;hanas@uw.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Dynamic model averaging for binary and continuous
        outcomes.</td>
</tr>
<tr>
<td>Imports:</td>
<td>MASS</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2018-10-04 23:38:01 UTC; hana</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2018-10-05 17:30:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='dma'>Dynamic model averaging for continuous outcomes</h2><span id='topic+dma'></span><span id='topic+dma.default'></span><span id='topic+print.dma'></span><span id='topic+plot.dma'></span><span id='topic+coef.dma'></span><span id='topic+makf4'></span><span id='topic+model.update3'></span><span id='topic+rm.Kalman'></span>

<h3>Description</h3>

<p>Implemtent dynamic model averaging for continuous outcomes as described in 
Raftery, A.E., Karny, M., and Ettler, P. (2010). Online Prediction Under Model 
Uncertainty Via Dynamic Model Averaging: Application to a Cold Rolling Mill. Technometrics 52:52-66. Along with the values
described below, plot() creates a plot of the posterior model probabilities over time and 
model-averaged fitted values and print() returns model matrix and posterior
model probabilities.  
There are TT time points, K models, and d total covariates.</p>


<h3>Usage</h3>

<pre><code class='language-R'>dma(x, y, models.which, lambda=0.99, gamma=0.99, 
 eps=.001/nrow(models.which), delay=0, initialperiod=200)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dma_+3A_x">x</code></td>
<td>

<p>TTxd matrix of system inputs
</p>
</td></tr>
<tr><td><code id="dma_+3A_y">y</code></td>
<td>

<p>TT-vector of system outputs
</p>
</td></tr>
<tr><td><code id="dma_+3A_models.which">models.which</code></td>
<td>

<p>Kxd matrix, with 1 row per model and 1 col per variable
indicating whether that variable is in the model
(the state theta is of dim (model.dim+1); the extra 1 for the intercept)
</p>
</td></tr>
<tr><td><code id="dma_+3A_lambda">lambda</code></td>
<td>

<p>parameter forgetting factor
</p>
</td></tr>
<tr><td><code id="dma_+3A_gamma">gamma</code></td>
<td>

<p>flatterning parameter for model updating
</p>
</td></tr>
<tr><td><code id="dma_+3A_eps">eps</code></td>
<td>

<p>regularization parameter for regularizing posterior model
model probabilities away from zero
</p>
</td></tr>
<tr><td><code id="dma_+3A_delay">delay</code></td>
<td>

<p>When y_t is controlled, only y_t-delay-1 and before 
are available. This is determined by the machine.
Note that delay as defined here corresponds to (k-1)
in the Ettler et al (2007, MixSim) paper.
Thus k=25 in the paper corresponds to delay=24.
</p>
</td></tr>
<tr><td><code id="dma_+3A_initialperiod">initialperiod</code></td>
<td>

<p>length of initial period. Performance is summarized with and
without the first initialperiod samples.
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>yhat.bymodel</code></td>
<td>
<p>TTxK matrix whose tk element gives yhat for yt for model k</p>
</td></tr>
<tr><td><code>yhat.ma</code></td>
<td>
<p>TT vector whose t element gives the model-averaged yhat for yt</p>
</td></tr>
<tr><td><code>pmp</code></td>
<td>
<p>TTxK matrix whose tk element is the post prob of model k at t</p>
</td></tr>
<tr><td><code>thetahat</code></td>
<td>
<p>KxTTx(nvar+1) array whose ktj element is the 
estimate of theta_j-1 for model k at t</p>
</td></tr>
<tr><td><code>Vtheta</code></td>
<td>
<p>KxTTx(nvar+1) array whose ktj element is the 
variance of theta_j-1 for model k at t</p>
</td></tr>
<tr><td><code>thetahat.ma</code></td>
<td>
<p>TTx(nvar+1) matrix whose tj element is the model-averaged 
estimate of theta_j-1 at t</p>
</td></tr>
<tr><td><code>Vtheta.ma</code></td>
<td>
<p>TTx(nvar+1) matrix whose tj element is the model-averaged
variance of thetahat_j-1 at t</p>
</td></tr>
<tr><td><code>mse.bymodel</code></td>
<td>
<p>MSE for each model</p>
</td></tr>
<tr><td><code>mse.ma</code></td>
<td>
<p>MSE of model-averaged prediction</p>
</td></tr>
<tr><td><code>mseinitialperiod.bymodel</code></td>
<td>
<p>MSE for each model excluding the first initialperiod samples</p>
</td></tr>
<tr><td><code>mseinitialperiod.ma</code></td>
<td>
<p>MSE of model averaging excluding the first initialperiod samples</p>
</td></tr>
<tr><td><code>model.forget</code></td>
<td>
<p>forgetting factor for the model switching matrix</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Adrian Raftery, Tyler H. McCormick</p>


<h3>References</h3>

<p>Raftery, A.E., Karny, M., and Ettler, P. (2010). Online Prediction Under Model 
Uncertainty Via Dynamic Model Averaging: Application to a Cold Rolling Mill. Technometrics 52:52-66.</p>


<h3>Examples</h3>

<pre><code class='language-R'>#simulate some data to test
#first, static coefficients
coef&lt;-c(1.8,3.4,-2,3,-2.8,3)
coefmat&lt;-cbind(rep(coef[1],200),rep(coef[2],200),
            rep(coef[3],200),rep(coef[4],200),
            rep(coef[5],200),rep(coef[6],200))
#then, dynamic ones
coefmat&lt;-cbind(coefmat,seq(1,2.45,length.out=nrow(coefmat)),
            seq(-.75,-2.75,length.out=nrow(coefmat)),
            c(rep(-1.5,nrow(coefmat)/2),rep(-.5,nrow(coefmat)/2)))
npar&lt;-ncol(coefmat)-1
dat&lt;-matrix(rnorm(200*(npar),0,1),200,(npar))
ydat&lt;-rowSums((cbind(rep(1,nrow(dat)),dat))[1:100,]*coefmat[1:100,])
ydat&lt;-c(ydat,rowSums((cbind(rep(1,nrow(dat)),dat)*coefmat)[-c(1:100),c(6:9)]))
mmat&lt;-matrix(c(c(1,0,1,0,0,rep(1,(npar-7)),0,0),
            c(rep(0,(npar-4)),rep(1,4)),rep(1,npar)),3,npar,byrow=TRUE)
dma.test&lt;-dma(dat,ydat,mmat,lambda=.99,gamma=.99,initialperiod=20)
plot(dma.test)
</code></pre>

<hr>
<h2 id='dma-package'>
Dynamic model averaging
</h2><span id='topic+dma-package'></span>

<h3>Description</h3>

<p>This package implements dynamic Bayesian model averaging as described for continuous outcomes in Raftery et al. (2010, Technometrics) and for binary outcomes in McCormick et al. (2011, Biometrics).
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> dma</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.4-0</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2018-10-4</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL2</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Tyler H. McCormick, Adrian Raftery, David Madigan, Sevvandi Kandanaarachchi [ctb], Hana Sevcikova [ctb]
</p>
<p>Maintainer: Hana Sevcikova &lt;hanas@uw.edu&gt;
</p>


<h3>References</h3>

<p>McCormick, T.M., Raftery, A.E., Madigan, D. and Burd, R.S. (2011) &quot;Dynamic Logistic Regression and 
Dynamic Model Averaging for Binary Classification.&quot; Biometrics, 66:1162-1173.
</p>
<p>Raftery, A.E., Karny, M., and Ettler, P. (2010). Online Prediction Under Model 
Uncertainty Via Dynamic Model Averaging: Application to a Cold Rolling Mill. Technometrics 52:52-66.
</p>

<hr>
<h2 id='logistic.dma'>
Dynamic model averaging for binary outcomes
</h2><span id='topic+logistic.dma'></span><span id='topic+logistic.dma.default'></span><span id='topic+print.logistic.dma'></span><span id='topic+plot.logistic.dma'></span><span id='topic+dlogr.init'></span><span id='topic+dlogr.predict'></span><span id='topic+dlogr.step'></span><span id='topic+laplace.fn'></span><span id='topic+tunemat.fn'></span><span id='topic+logdma.init'></span><span id='topic+logdma.update'></span><span id='topic+logdma.predict'></span><span id='topic+logdma.average'></span>

<h3>Description</h3>

<p>Implements dynamic model averaging for continuous outcomes as described in 
McCormick et al. (2011, Biometrics).  It can be either performed for all data at once (using <code>logistic.dma</code>), or dynamically for one observation at a time (combining the remaining functions, see Example).
Along with the values described below, plot() creates a plot of the posterior model probabilities over time and 
model-averaged fitted values (with smooth curve overlay) and print() returns model matrix and posterior
model probabilities.  There are K candidate
models, T time points, and d total covariates (including the intercept).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>logistic.dma(x, y, models.which, lambda = 0.99, alpha = 0.99,autotune = TRUE, 
    initmodelprobs = NULL, initialsamp = NULL)
 
logdma.init(x, y, models.which)

logdma.predict(fit, newx)

logdma.update(fit, newx, newy, lambda = 0.99, autotune = TRUE)

logdma.average(fit, alpha = 0.99, initmodelprobs = NULL)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="logistic.dma_+3A_x">x</code></td>
<td>
<p>T by (d-1) matrix of observed covariates.  Note that a column of 1's is added
automatically for the intercept. In <code>logdma.init</code>, this matrix contains only the training set.</p>
</td></tr>
<tr><td><code id="logistic.dma_+3A_y">y</code></td>
<td>
<p>T vector of binary responses. In <code>logdma.init</code>, these correspond to the training set only.</p>
</td></tr>
<tr><td><code id="logistic.dma_+3A_models.which">models.which</code></td>
<td>
<p>K by (d-1) matrix defining models.  A 1 indicates a covariate is included
in a particular model, a 0 if it is excluded.  Model averaging is done over all
modeld specified in models.which.</p>
</td></tr>
<tr><td><code id="logistic.dma_+3A_lambda">lambda</code></td>
<td>
<p>scalar forgetting factor with each model</p>
</td></tr>
<tr><td><code id="logistic.dma_+3A_alpha">alpha</code></td>
<td>
<p>scalar forgetting factor for model averaging</p>
</td></tr>
<tr><td><code id="logistic.dma_+3A_autotune">autotune</code></td>
<td>
<p> T/F indicates whether or not the automatic tuning procedure desribed in 
McCormick et al. should be applied.  Default is true.</p>
</td></tr>
<tr><td><code id="logistic.dma_+3A_initmodelprobs">initmodelprobs</code></td>
<td>
<p>K vector of starting probabilities for model averaging.  If null (default),
then use 1/K for each model.</p>
</td></tr>
<tr><td><code id="logistic.dma_+3A_initialsamp">initialsamp</code></td>
<td>
<p>scalar indicating how many observations to use for generating initial 
values.  If null (default), then use the first 10 percent of observations.</p>
</td></tr>
<tr><td><code id="logistic.dma_+3A_newx">newx</code>, <code id="logistic.dma_+3A_newy">newy</code></td>
<td>
<p>Subset of <code>x</code> and <code>y</code> corresponding to new observations.</p>
</td></tr>
<tr><td><code id="logistic.dma_+3A_fit">fit</code></td>
<td>
<p>List with estimation results that are outputs of functions <code>logdma.init</code>, <code>logdma.update</code> and <code>logdma.average</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>logistic.dma</code> is composed of three parts, which can be also used separately: First, the model is trained with a subset of the data  (function <code>logdma.init</code>), where the size of the training set is determined by <code>initialsamp</code>. Note that arguments <code>x</code> and <code>y</code> in <code>logdma.init</code> should contain the training subset only. Then, the estimation is updated with new observations (function <code>logdma.update</code>). Lastly, a dynamic model averaging is performed on the final estimates (function <code>logdma.average</code>). The updating, averaging and in addition predicting (<code>logdma.predict</code>) can be performed dynamically for one observation at a time, see Example below.
</p>


<h3>Value</h3>

<p>Functions <code>logistic.dma</code> and <code>logdma.average</code> return an object of class <code>logistic.dma</code>. Functions <code>logdma.init</code> and <code>logdma.update</code> return a list with estimation results which is a subset of the <code>logistic.dma</code> object. It has the following components:
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>T by (d-1) matrix of covariates</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>T by 1 vector of binary responses</p>
</td></tr>
<tr><td><code>models.which</code></td>
<td>
<p>K by (d-1) matrix of candidate models</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>scalar, tuning factor within models</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>scalar, tuning factor for model averaging</p>
</td></tr>
<tr><td><code>autotune</code></td>
<td>
<p>T/F, indicator of whether or not to use autotuning algorithm</p>
</td></tr>
<tr><td><code>alpha.used</code></td>
<td>
<p>T vector of alpha values used</p>
</td></tr>
<tr><td><code>theta</code></td>
<td>
<p>K by T by d array of dynamic logistic regression estimates for each model</p>
</td></tr>
<tr><td><code>vartheta</code></td>
<td>
<p>K by T by d array of dynamic logistic regression variances for each model</p>
</td></tr>
<tr><td><code>pmp</code></td>
<td>
<p>K by T array of posterior model probabilities</p>
</td></tr>
<tr><td><code>yhatdma</code></td>
<td>
<p>T vector of model-averaged predictions</p>
</td></tr>
<tr><td><code>yhatmodel</code></td>
<td>
<p>K by T vector of fitted values for each model</p>
</td></tr>
</table>
<p>Function <code>logdma.predict</code> returns a matrix with predictions corresponding to the <code>newx</code> covariates.
</p>


<h3>Author(s)</h3>

<p>Tyler H. McCormick, David Madigan, Adrian Raftery
</p>
<p>Sevvandi Kandanaarachchi and Hana Sevcikova implemented the &quot;streaming&quot; functionality, i.e. the original function was decomposed into standalone  parts that can be used separately for one observation at a time.
</p>


<h3>References</h3>

<p>McCormick, T.M., Raftery, A.E., Madigan, D. and Burd, R.S. (2011) &quot;Dynamic Logistic Regression and 
Dynamic Model Averaging for Binary Classification.&quot; Biometrics, 66:1162-1173.</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate some data to test
# first, static coefficients
coef &lt;- c(.08,-.4,-.1)
coefmat &lt;- cbind(rep(coef[1],200),rep(coef[2],200),rep(coef[3],200))
# then, dynamic ones
coefmat &lt;- cbind(coefmat,seq(1,.45,length.out=nrow(coefmat)),
            seq(-.75,-.15,length.out=nrow(coefmat)),
            c(rep(-1.5,nrow(coefmat)/2),rep(-.5,nrow(coefmat)/2)))
npar &lt;- ncol(coefmat)-1

# simulate data
set.seed(1234)
dat &lt;- matrix(rnorm(200*(npar),0,1),200,(npar))
ydat &lt;- exp(rowSums((cbind(rep(1,nrow(dat)),dat))[1:100,]*coefmat[1:100,]))/
          (1+exp(rowSums(cbind(rep(1,nrow(dat)),dat)[1:100,]*coefmat[1:100,])))
y &lt;- c(ydat,exp(rowSums(cbind(rep(1,nrow(dat)),dat)[-c(1:100),c(1,5,6)]*
               coefmat[-c(1:100),c(1,5,6)]))/
          (1+exp(rowSums(cbind(rep(1,nrow(dat)),dat)[-c(1:100),c(1,5,6)]*
               coefmat[-c(1:100),c(1,5,6)]))))
u &lt;- runif (length(y))
y &lt;- as.numeric (u &lt; y)

# Consider three candidate models
mmat &lt;- matrix(c(1,1,1,1,1,0,0,0,1,1,1,0,1,0,1),3,5, byrow = TRUE)

# Fit model and plot
# autotuning is turned off for this demonstration example
ldma.test &lt;- logistic.dma(dat, y, mmat, lambda = .99, alpha = .99, 
    autotune = FALSE, initialsamp = 20)
plot(ldma.test)

# Using DMA in a "streaming" mode
modl &lt;- logdma.init(dat[1:20,], y[1:20], mmat)
yhat &lt;- matrix(0, ncol=3, nrow=200)
for(i in 21:200){
  # if prediction is desired, use logdma.predict
  yhat[i,] &lt;- logdma.predict(modl, dat[i,])
  # update
  modl &lt;- logdma.update(modl, dat[i,], y[i], 
                lambda = .99, autotune = FALSE)
}
# the averaging step could be also done within the loop above
ldma.stream &lt;- logdma.average(modl, alpha = .99)
plot(ldma.stream)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
