<!DOCTYPE html><html><head><title>Help for package bark</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {bark}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bark'><p>Nonparametric Regression using Bayesian Additive Regression Kernels</p></a></li>
<li><a href='#bark-deprecated'><p>NonParametric Regression using Bayesian Additive Regression Kernels</p></a></li>
<li><a href='#bark-package'><p>bark:  Bayesian Additive Regression Trees</p></a></li>
<li><a href='#bark-package-deprecated'><p>Deprecated functions in package <span class="pkg">bark</span>.</p></a></li>
<li><a href='#sim_circle'><p>Simulate Data from Hyper-Sphere for Classification Problems</p></a></li>
<li><a href='#sim_Friedman1'><p>Simulated Regression Problem Friedman 1</p></a></li>
<li><a href='#sim_Friedman2'><p>Simulated Regression Problem Friedman 2</p></a></li>
<li><a href='#sim_Friedman3'><p>Simulated Regression Problem Friedman 3</p></a></li>
<li><a href='#sim.Circle-deprecated'><p>Simulate Data from Hyper-Sphere for Classification Problems</p></a></li>
<li><a href='#sim.Friedman1-deprecated'><p>Simulated Regression Problem Friedman 1</p></a></li>
<li><a href='#sim.Friedman2-deprecated'><p>Simulated Regression Problem Friedman 2</p></a></li>
<li><a href='#sim.Friedman3-deprecated'><p>Simulated Regression Problem Friedman 3</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Bayesian Additive Regression Kernels</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.4</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-04-17</td>
</tr>
<tr>
<td>Description:</td>
<td>Bayesian Additive Regression Kernels (BARK) provides
	an implementation for non-parametric function estimation using Levy
	Random Field priors for functions that may be represented as a
	sum of additive multivariate kernels.  Kernels are located at
	every data point as in Support Vector Machines, however, coefficients 
	may be heavily shrunk to zero under the Cauchy process prior, or even, 
	set to zero.  The number of active features is controlled by priors on
	precision parameters within the kernels, permitting feature selection. For 
	more details see Ouyang, Z (2008) "Bayesian Additive Regression Kernels",
	Duke University. PhD dissertation, Chapter 3 and Wolpert, R. L, Clyde, M.A, 
	and Tu, C. (2011) "Stochastic Expansions with Continuous Dictionaries Levy
	Adaptive Regression Kernels, Annals of Statistics Vol (39) pages 1916-1962 
	&lt;<a href="https://doi.org/10.1214%2F11-AOS889">doi:10.1214/11-AOS889</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://www.R-project.org">https://www.R-project.org</a>, <a href="https://github.com/merliseclyde/bark">https://github.com/merliseclyde/bark</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/merliseclyde/bark/issues">https://github.com/merliseclyde/bark/issues</a></td>
</tr>
<tr>
<td>Suggests:</td>
<td>BART, e1071, rmarkdown, knitr, roxygen2, testthat, covr</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>yes</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-04-17 23:44:53 UTC; clyde</td>
</tr>
<tr>
<td>Author:</td>
<td>Merlise Clyde [aut, cre, ths] (ORCID=0000-0002-3595-1872),
  Zhi Ouyang [aut],
  Robert Wolpert [ctb, ths]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Merlise Clyde &lt;clyde@stat.duke.edu&gt;</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-04-18 19:10:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='bark'>Nonparametric Regression using Bayesian Additive Regression Kernels</h2><span id='topic+bark'></span>

<h3>Description</h3>

<p>BARK is a Bayesian <em>sum-of-kernels</em> model.<br />
For numeric response <code class="reqn">y</code>, we have
<code class="reqn">y = f(x) + \epsilon</code>,
where <code class="reqn">\epsilon \sim N(0,\sigma^2)</code>.<br />
For a binary response <code class="reqn">y</code>, <code class="reqn">P(Y=1 | x) = F(f(x))</code>,
where <code class="reqn">F</code>
denotes the standard normal cdf (probit link).
<br />
In both cases, <code class="reqn">f</code> is the sum of many Gaussian kernel functions.
The goal is to have very flexible inference for the unknown
function <code class="reqn">f</code>.
BARK uses an approximation to a Cauchy process as the prior distribution
for the unknown function <code class="reqn">f</code>.
</p>
<p>Feature selection can be achieved through the inference
on the scale parameters in the Gaussian kernels.
BARK accepts four different types of prior distributions,
<em>e</em>, <em>d</em>, enabling
either soft shrinkage or  <em>se</em>, <em>sd</em>, enabling hard shrinkage for the scale
parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bark(
  formula,
  data,
  subset,
  na.action = na.omit,
  testdata = NULL,
  selection = TRUE,
  common_lambdas = TRUE,
  classification = FALSE,
  keepevery = 100,
  nburn = 100,
  nkeep = 100,
  printevery = 1000,
  keeptrain = FALSE,
  verbose = FALSE,
  fixed = list(),
  tune = list(lstep = 0.5, frequL = 0.2, dpow = 1, upow = 0, varphistep = 0.5, phistep =
    1),
  theta = list()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bark_+3A_formula">formula</code></td>
<td>
<p>model formula for the model with all predictors,
Y ~ X.  The X variables will be centered and scaled as part of model fitting.</p>
</td></tr>
<tr><td><code id="bark_+3A_data">data</code></td>
<td>
<p>a data frame.  Factors will be converted to numerical vectors based on
the using 'model.matrix'.</p>
</td></tr>
<tr><td><code id="bark_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations to be
used in the fitting process.</p>
</td></tr>
<tr><td><code id="bark_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when the data
contain NAs. The default is &quot;na.omit&quot;.</p>
</td></tr>
<tr><td><code id="bark_+3A_testdata">testdata</code></td>
<td>
<p>Dataframe with test data for out of sample prediction.<br />
Should have same structure as data.</p>
</td></tr>
<tr><td><code id="bark_+3A_selection">selection</code></td>
<td>
<p>Logical variable indicating whether variable 
dependent kernel parameters <code class="reqn">\lambda</code> may be set to zero in the MCMC; 
default is TRUE. <br /></p>
</td></tr>
<tr><td><code id="bark_+3A_common_lambdas">common_lambdas</code></td>
<td>
<p>Logical variable indicating whether
kernel parameters <code class="reqn">\lambda</code> should be predictor specific or common across
predictors;  default is TRUE.   Note if  <em>common_lambdas = TRUE</em> and 
<em>selection = TRUE</em> this applies just to the non-zero <code class="reqn">lambda_j</code>. <br /></p>
</td></tr>
<tr><td><code id="bark_+3A_classification">classification</code></td>
<td>
<p>TRUE/FALSE logical variable,
indicating a classification or regression problem.</p>
</td></tr>
<tr><td><code id="bark_+3A_keepevery">keepevery</code></td>
<td>
<p>Every keepevery draw is kept to be returned to the user</p>
</td></tr>
<tr><td><code id="bark_+3A_nburn">nburn</code></td>
<td>
<p>Number of MCMC iterations (nburn*keepevery)
to be treated as burn in.</p>
</td></tr>
<tr><td><code id="bark_+3A_nkeep">nkeep</code></td>
<td>
<p>Number of MCMC iterations kept for the posterior inference.<br />
nkeep*keepevery iterations after the burn in.</p>
</td></tr>
<tr><td><code id="bark_+3A_printevery">printevery</code></td>
<td>
<p>As the MCMC runs, a message is printed every printevery draws.</p>
</td></tr>
<tr><td><code id="bark_+3A_keeptrain">keeptrain</code></td>
<td>
<p>Logical, whether to keep results for training samples.</p>
</td></tr>
<tr><td><code id="bark_+3A_verbose">verbose</code></td>
<td>
<p>Logical, whether to print out messages</p>
</td></tr>
<tr><td><code id="bark_+3A_fixed">fixed</code></td>
<td>
<p>A list of fixed hyperparameters, using the default values if not
specified.<br />
alpha = 1: stable index, must be 1 currently.<br />
eps = 0.5: approximation parameter.<br />
gam = 5: intensity parameter.<br />
la = 1: first argument of the gamma prior on kernel scales.<br />
lb = 2: second argument of the gamma prior on kernel scales.<br />
pbetaa = 1: first argument of the beta prior on plambda.<br />
pbetab = 1: second argument of the beta prior on plambda.<br />
n: number of training samples, automatically generates.<br />
p: number of explanatory variables, automatically generates.<br />
meanJ: the expected number of kernels, automatically generates.</p>
</td></tr>
<tr><td><code id="bark_+3A_tune">tune</code></td>
<td>
<p>A list of tuning parameters, not expected to change.<br />
lstep: the stepsize of the lognormal random walk on lambda.<br />
frequL: the frequency to update L.<br />
dpow: the power on the death step.<br />
upow: the power on the update step.<br />
varphistep: the stepsize of the lognormal random walk on varphi.<br />
phistep: the stepsize of the lognormal random walk on phi.</p>
</td></tr>
<tr><td><code id="bark_+3A_theta">theta</code></td>
<td>
<p>A list of the starting values for the parameter theta,
use defaults if nothing is given.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>BARK is implemented using a Bayesian MCMC method.
At each MCMC interaction, we produce a draw from the joint posterior
distribution, i.e. a full configuration of regression coefficients,
kernel locations and kernel parameters.
</p>
<p>Thus, unlike a lot of other modelling methods in R,
we do not produce a single model object
from which fits and summaries may be extracted.
The output consists of values
<code class="reqn">f^*(x)</code> (and <code class="reqn">\sigma^*</code> in the numeric case)
where * denotes a particular draw.
The <code class="reqn">x</code> is either a row from the training data (x.train)
</p>


<h3>Value</h3>

<p><code>bark</code> returns an object of class 'bark' with a list, including:
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>the matched call</p>
</td></tr>
<tr><td><code>fixed</code></td>
<td>
<p>Fixed hyperparameters</p>
</td></tr>
<tr><td><code>tune</code></td>
<td>
<p>Tuning parameters used</p>
</td></tr>
<tr><td><code>theta.last</code></td>
<td>
<p>The last set of parameters from the posterior draw</p>
</td></tr>
<tr><td><code>theta.nvec</code></td>
<td>
<p>A matrix with nrow(x.train)<code class="reqn">+1</code> rows and (nkeep) columns,
recording the  number of kernels at each training sample</p>
</td></tr>
<tr><td><code>theta.varphi</code></td>
<td>
<p> A matrix with nrow(x.train)
<code class="reqn">+1</code> rows and (nkeep) columns,
recording the precision in the normal gamma prior
distribution for the regression coefficients</p>
</td></tr>
<tr><td><code>theta.beta</code></td>
<td>
<p>A matrix with nrow(x.train)<code class="reqn">+1</code> rows and (nkeep) columns,
recording the regression coefficients</p>
</td></tr>
<tr><td><code>theta.lambda</code></td>
<td>
<p>A matrix with ncol(x.train) rows and (nkeep) columns,
recording the kernel scale parameters</p>
</td></tr>
<tr><td><code>thea.phi</code></td>
<td>
<p>The vector of length nkeep,
recording the precision in regression Gaussian noise
(1 for the classification case)</p>
</td></tr>
<tr><td><code>yhat.train</code></td>
<td>
<p>A matrix with nrow(x.train) rows and (nkeep) columns.
Each column corresponds to a draw <code class="reqn">f^*</code> from
the posterior of <code class="reqn">f</code>
and each row corresponds to a row of x.train.
The <code class="reqn">(i,j)</code> value is <code class="reqn">f^*(x)</code> for
the <code class="reqn">j^{th}</code> kept draw of <code class="reqn">f</code>
and the <code class="reqn">i^{th}</code> row of x.train.<br />
For classification problems, this is the value
of the expectation for the underlying normal
random variable.<br />
Burn-in is dropped</p>
</td></tr>
<tr><td><code>yhat.test</code></td>
<td>
<p>Same as yhat.train but now the x's
are the rows of the test data;  NULL if testdata are not provided</p>
</td></tr>
<tr><td><code>yhat.train.mean</code></td>
<td>
<p>train data fits = row mean of yhat.train</p>
</td></tr>
<tr><td><code>yhat.test.mean</code></td>
<td>
<p>test data fits = row mean of yhat.test</p>
</td></tr>
</table>


<h3>References</h3>

<p>Ouyang, Zhi (2008) Bayesian Additive Regression Kernels.
Duke University. PhD dissertation, page 58.
</p>


<h3>See Also</h3>

<p>Other bark functions: 
<code><a href="#topic+bark-package-deprecated">bark-package-deprecated</a></code>,
<code><a href="#topic+bark-package">bark-package</a></code>,
<code><a href="#topic+sim_Friedman1">sim_Friedman1</a>()</code>,
<code><a href="#topic+sim_Friedman2">sim_Friedman2</a>()</code>,
<code><a href="#topic+sim_Friedman3">sim_Friedman3</a>()</code>,
<code><a href="#topic+sim_circle">sim_circle</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##Simulated regression example
# Friedman 2 data set, 200 noisy training, 1000 noise free testing
# Out of sample MSE in SVM (default RBF): 6500 (sd. 1600)
# Out of sample MSE in BART (default):    5300 (sd. 1000)
traindata &lt;- data.frame(sim_Friedman2(200, sd=125))
testdata &lt;- data.frame(sim_Friedman2(1000, sd=0))
# example with a very small number of iterations to illustrate usage
fit.bark.d &lt;- bark(y ~ ., data=traindata, testdata= testdata,
                   nburn=10, nkeep=10, keepevery=10,
                   classification=FALSE, 
                   common_lambdas = FALSE,
                   selection = FALSE)
boxplot(data.frame(fit.bark.d$theta.lambda))
mean((fit.bark.d$yhat.test.mean-testdata$y)^2)

 ##Simulate classification example
 # Circle 5 with 2 signals and three noisy dimensions
 # Out of sample erorr rate in SVM (default RBF): 0.110 (sd. 0.02)
 # Out of sample error rate in BART (default):    0.065 (sd. 0.02)
 traindata &lt;- sim_circle(200, dim=5)
 testdata &lt;- sim_circle(1000, dim=5)
 fit.bark.se &lt;- bark(y ~ ., 
                     data=data.frame(traindata), 
                     testdata= data.frame(testdata), 
                     classification=TRUE,
                     nburn=100, nkeep=200, )
 boxplot(as.data.frame(fit.bark.se$theta.lambda))
 mean((fit.bark.se$yhat.test.mean&gt;0)!=testdata$y)

</code></pre>

<hr>
<h2 id='bark-deprecated'>NonParametric Regression using Bayesian Additive Regression Kernels</h2><span id='topic+bark-deprecated'></span>

<h3>Description</h3>

<p>BARK is a Bayesian <em>sum-of-kernels</em> model.<br />
For numeric response <code class="reqn">y</code>, we have
<code class="reqn">y = f(x) + \epsilon</code>,
where <code class="reqn">\epsilon \sim N(0,\sigma^2)</code>.<br />
For a binary response <code class="reqn">y</code>, <code class="reqn">P(Y=1 | x) = F(f(x))</code>,
where <code class="reqn">F</code>
denotes the standard normal cdf (probit link).
<br />
In both cases, <code class="reqn">f</code> is the sum of many Gaussian kernel functions.
The goal is to have very flexible inference for the unknown
function <code class="reqn">f</code>.
BARK uses an approximation to a Cauchy process as the prior distribution
for the unknown function <code class="reqn">f</code>.
</p>
<p>Feature selection can be achieved through the inference
on the scale parameters in the Gaussian kernels.
BARK accepts four different types of prior distributions,
<em>e</em>, <em>d</em>, enabling
either soft shrinkage or  <em>se</em>, <em>sd</em>, enabling hard shrinkage for the scale
parameters.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="bark-deprecated_+3A_x.train">x.train</code></td>
<td>
<p>Explanatory variables for training (in sample) data.<br />
Must be a matrix of doubles,
with (as usual) rows corresponding to observations
and columns to variables.</p>
</td></tr>
<tr><td><code id="bark-deprecated_+3A_y.train">y.train</code></td>
<td>
<p>Dependent variable for training (in sample) data.<br />
If y is numeric a continuous response model is fit (normal errors).<br />
If y is a logical (or just has values 0 and 1),
then a binary response model with a probit link is fit.</p>
</td></tr>
<tr><td><code id="bark-deprecated_+3A_x.test">x.test</code></td>
<td>
<p>Explanatory variables for test (out of sample) data.<br />
Should have same structure as x.train.</p>
</td></tr>
<tr><td><code id="bark-deprecated_+3A_type">type</code></td>
<td>
<p>BARK type, <em>e</em>, <em>d</em>, <em>se</em>, or <em>sd</em>, default
choice is <em>se</em>.<br />
<em>e</em>: BARK with equal weights.<br />
<em>d</em>: BARK with different weights.<br />
<em>se</em>: BARK with selection and equal weights.<br />
<em>sd</em>: BARK with selection and different weights.<br /></p>
</td></tr>
<tr><td><code id="bark-deprecated_+3A_classification">classification</code></td>
<td>
<p>TRUE/FALSE logical variable,
indicating a classification or regression problem.</p>
</td></tr>
<tr><td><code id="bark-deprecated_+3A_keepevery">keepevery</code></td>
<td>
<p>Every keepevery draw is kept to be returned to the user</p>
</td></tr>
<tr><td><code id="bark-deprecated_+3A_nburn">nburn</code></td>
<td>
<p>Number of MCMC iterations (nburn*keepevery)
to be treated as burn in.</p>
</td></tr>
<tr><td><code id="bark-deprecated_+3A_nkeep">nkeep</code></td>
<td>
<p>Number of MCMC iterations kept for the posterior inference.<br />
nkeep*keepevery iterations after the burn in.</p>
</td></tr>
<tr><td><code id="bark-deprecated_+3A_printevery">printevery</code></td>
<td>
<p>As the MCMC runs, a message is printed every printevery draws.</p>
</td></tr>
<tr><td><code id="bark-deprecated_+3A_keeptrain">keeptrain</code></td>
<td>
<p>Logical, whether to keep results for training samples.</p>
</td></tr>
<tr><td><code id="bark-deprecated_+3A_fixed">fixed</code></td>
<td>
<p>A list of fixed hyperparameters, using the default values if not
specified.<br />
alpha = 1: stable index, must be 1 currently.<br />
eps = 0.5: approximation parameter.<br />
gam = 5: intensity parameter.<br />
la = 1: first argument of the gamma prior on kernel scales.<br />
lb = 2: second argument of the gamma prior on kernel scales.<br />
pbetaa = 1: first argument of the beta prior on plambda.<br />
pbetab = 1: second argument of the beta prior on plambda.<br />
n: number of training samples, automatically generates.<br />
p: number of explanatory variables, automatically generates.<br />
meanJ: the expected number of kernels, automatically generates.</p>
</td></tr>
<tr><td><code id="bark-deprecated_+3A_tune">tune</code></td>
<td>
<p>A list of tuning parameters, not expected to change.<br />
lstep: the stepsize of the lognormal random walk on lambda.<br />
frequL: the frequency to update L.<br />
dpow: the power on the death step.<br />
upow: the power on the update step.<br />
varphistep: the stepsize of the lognormal random walk on varphi.<br />
phistep: the stepsize of the lognormal random walk on phi.</p>
</td></tr>
<tr><td><code id="bark-deprecated_+3A_theta">theta</code></td>
<td>
<p>A list of the starting values for the parameter theta,
use defaults if nothing is given.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>BARK is implemented using a Bayesian MCMC method.
At each MCMC interaction, we produce a draw from the joint posterior
distribution, i.e. a full configuration of regression coefficients,
kernel locations and kernel parameters etc.
</p>
<p>Thus, unlike a lot of other modelling methods in R,
we do not produce a single model object
from which fits and summaries may be extracted.
The output consists of values
<code class="reqn">f^*(x)</code> (and <code class="reqn">\sigma^*</code> in the numeric case)
where * denotes a particular draw.
The <code class="reqn">x</code> is either a row from the training data (x.train)
</p>


<h3>Value</h3>

<p><code>bark</code> returns a list, including:
</p>
<table>
<tr><td><code>fixed</code></td>
<td>
<p>Fixed hyperparameters</p>
</td></tr>
<tr><td><code>tune</code></td>
<td>
<p>Tuning parameters used</p>
</td></tr>
<tr><td><code>theta.last</code></td>
<td>
<p>The last set of parameters from the posterior draw</p>
</td></tr>
<tr><td><code>theta.nvec</code></td>
<td>
<p>A matrix with nrow(x.train)<code class="reqn">+1</code> rows and (nkeep) columns,
recording the  number of kernels at each training sample</p>
</td></tr>
<tr><td><code>theta.varphi</code></td>
<td>
<p> A matrix with nrow(x.train)
<code class="reqn">+1</code> rows and (nkeep) columns,
recording the precision in the normal gamma prior
distribution for the regression coefficients</p>
</td></tr>
<tr><td><code>theta.beta</code></td>
<td>
<p>A matrix with nrow(x.train)<code class="reqn">+1</code> rows and (nkeep) columns,
recording the regression coefficients</p>
</td></tr>
<tr><td><code>theta.lambda</code></td>
<td>
<p>A matrix with ncol(x.train) rows and (nkeep) columns,
recording the kernel scale parameters</p>
</td></tr>
<tr><td><code>thea.phi</code></td>
<td>
<p>The vector of length nkeep,
recording the precision in regression Gaussian noise
(1 for the classification case)</p>
</td></tr>
<tr><td><code>yhat.train</code></td>
<td>
<p>A matrix with nrow(x.train) rows and (nkeep) columns.
Each column corresponds to a draw <code class="reqn">f^*</code> from
the posterior of <code class="reqn">f</code>
and each row corresponds to a row of x.train.
The <code class="reqn">(i,j)</code> value is <code class="reqn">f^*(x)</code> for
the <code class="reqn">j^{th}</code> kept draw of <code class="reqn">f</code>
and the <code class="reqn">i^{th}</code> row of x.train.<br />
For classification problems, this is the value
of the expectation for the underlying normal
random variable.<br />
Burn-in is dropped</p>
</td></tr>
<tr><td><code>yhat.test</code></td>
<td>
<p>Same as yhat.train but now the x's
are the rows of the test data</p>
</td></tr>
<tr><td><code>yhat.train.mean</code></td>
<td>
<p>train data fits = row mean of yhat.train</p>
</td></tr>
<tr><td><code>yhat.test.mean</code></td>
<td>
<p>test data fits = row mean of yhat.test</p>
</td></tr>
</table>


<h3>References</h3>

<p>Ouyang, Zhi (2008) Bayesian Additive Regression Kernels.
Duke University. PhD dissertation, page 58.
</p>


<h3>See Also</h3>

<p>Other bark deprecated functions: 
<code><a href="#topic+bark-package-deprecated">bark-package-deprecated</a></code>,
<code><a href="#topic+sim.Circle-deprecated">sim.Circle-deprecated</a></code>,
<code><a href="#topic+sim.Friedman1-deprecated">sim.Friedman1-deprecated</a></code>,
<code><a href="#topic+sim.Friedman2-deprecated">sim.Friedman2-deprecated</a></code>,
<code><a href="#topic+sim.Friedman3-deprecated">sim.Friedman3-deprecated</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Simulate regression example
#  Friedman 2 data set, 200 noisy training, 1000 noise free testing
#  Out of sample MSE in SVM (default RBF): 6500 (sd. 1600)
#  Out of sample MSE in BART (default):    5300 (sd. 1000)
traindata &lt;- sim_Friedman2(200, sd=125)
testdata &lt;- sim_Friedman2(1000, sd=0)
# example with a very small number of iterations to illustrate the method
fit.bark.d &lt;- bark_mat(traindata$x, traindata$y, testdata$x,
                  nburn=10, nkeep=10, keepevery=10,
                  classification=FALSE, type="d")
boxplot(data.frame(fit.bark.d$theta.lambda))
mean((fit.bark.d$yhat.test.mean-testdata$y)^2)

 # Simulate classification example
 #  Circle 5 with 2 signals and three noisy dimensions
 #  Out of sample erorr rate in SVM (default RBF): 0.110 (sd. 0.02)
 #  Out of sample error rate in BART (default):    0.065 (sd. 0.02)
 traindata &lt;- sim_circle(200, dim=5)
 testdata &lt;- sim_circle(1000, dim=5)
 fit.bark.se &lt;- bark_mat(traindata$x, traindata$y, testdata$x, classification=TRUE, type="se")
 boxplot(data.frame(fit.bark.se$theta.lambda))
 mean((fit.bark.se$yhat.test.mean&gt;0)!=testdata$y)

</code></pre>

<hr>
<h2 id='bark-package'>bark:  Bayesian Additive Regression Trees</h2><span id='topic+bark-package'></span>

<h3>Description</h3>

<p>Implementation of Bayesian Additive Regression Kernels
with Feature Selection for  Nonparametric Regression for
Gaussian regression
and classification for binary Probit models
</p>


<h3>Details</h3>

<p>BARK is a Bayesian <em>sum-of-kernels</em> model or because of the
Bayesian priors is a Bayesian Additive Regression Kernel model. <br />
For numeric response <code class="reqn">y</code>, we have
<code class="reqn">y = f(x) + \epsilon</code>,
where <code class="reqn">\epsilon \sim N(0,\sigma^2)</code>.<br />
For a binary response <code class="reqn">y</code>, <code class="reqn">P(Y=1 | x) = F(f(x))</code>, where <code class="reqn">F</code>
denotes the standard normal cdf (probit link).
In both cases, <code class="reqn">f</code> is the sum of many Gaussian kernel functions.
The goal is to have very flexible inference for the unknown
function <code class="reqn">f</code>.
bark  uses an approximated Cauchy process as the prior distribution
for the unknown function <code class="reqn">f</code>.
Feature selection can be achieved through the inference
on the scale parameters in the Gaussian kernels.
BARK accepts four different types of prior distributions through setting
values for <code>selection</code> (TRUE or FALSE), which allows scale parameters
for some variables to be set to zero, removing the variables from the
kernels <code>selection = TRUE</code>; this enables either soft shrinkage or hard 
shrinkage for the scale
parameters. The input <code>common_lambdas</code> (TRUE or FALSE) specifies whether
a common scale parameter should be used for all predictors (TRUE) or
if FALSE allows the scale parameters to differ across all variables
in the kernel.
</p>


<h3>References</h3>

<p>Ouyang, Zhi (2008) Bayesian Additive Regression Kernels.
Duke University. PhD dissertation, Chapter 3.
</p>


<h3>See Also</h3>

<p>Other bark functions: 
<code><a href="#topic+bark-package-deprecated">bark-package-deprecated</a></code>,
<code><a href="#topic+bark">bark</a>()</code>,
<code><a href="#topic+sim_Friedman1">sim_Friedman1</a>()</code>,
<code><a href="#topic+sim_Friedman2">sim_Friedman2</a>()</code>,
<code><a href="#topic+sim_Friedman3">sim_Friedman3</a>()</code>,
<code><a href="#topic+sim_circle">sim_circle</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 # Simulate regression example
 # Friedman 2 data set, 200 noisy training, 1000 noise free testing
 # Out of sample MSE in SVM (default RBF): 6500 (sd. 1600)
 # Out of sample MSE in BART (default):    5300 (sd. 1000)
 traindata &lt;- sim_Friedman2(200, sd=125)
 testdata &lt;- sim_Friedman2(1000, sd=0)
 fit.bark.d &lt;- bark(y ~ ., data = data.frame(traindata),
                    testdata = data.frame(testdata), 
                    classification = FALSE,
                    selection = FALSE,
                    common_lambdas = TRUE)
 boxplot(as.data.frame(fit.bark.d$theta.lambda))
 mean((fit.bark.d$yhat.test.mean-testdata$y)^2)
 # Simulate classification example
 # Circle 5 with 2 signals and three noisy dimensions
 # Out of sample erorr rate in SVM (default RBF): 0.110 (sd. 0.02)
 # Out of sample error rate in BART (default):    0.065 (sd. 0.02)
 traindata &lt;- sim_circle(200, dim=5)
 testdata &lt;- sim_circle(1000, dim=5)
 fit.bark.se &lt;- bark(y ~ ., data= data.frame(traindata), 
                     testdata= data.frame(testdata),
                     classification=TRUE, 
                     selection=TRUE,
                     common_lambdas = FALSE)
                   
 boxplot(as.data.frame(fit.bark.se$theta.lambda))
 mean((fit.bark.se$yhat.test.mean&gt;0)!=testdata$y)


</code></pre>

<hr>
<h2 id='bark-package-deprecated'>Deprecated functions in package <span class="pkg">bark</span>.</h2><span id='topic+bark-package-deprecated'></span><span id='topic+bark_mat'></span><span id='topic+sim.Friedman1'></span><span id='topic+sim.Friedman2'></span><span id='topic+sim.Friedman3'></span><span id='topic+sim.Circle'></span>

<h3>Description</h3>

<p>The functions listed below are deprecated and will be defunct in
the near future. When possible, alternative functions with similar
functionality are also mentioned. Help pages for deprecated functions are
available at <code>help("&lt;function&gt;-deprecated")</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bark_mat(
  x.train,
  y.train,
  x.test = matrix(0, 0, 0),
  type = "se",
  classification = FALSE,
  keepevery = 100,
  nburn = 100,
  nkeep = 100,
  printevery = 1000,
  keeptrain = FALSE,
  fixed = list(),
  tune = list(lstep = 0.5, frequL = 0.2, dpow = 1, upow = 0, varphistep = 0.5, phistep =
    1),
  theta = list()
)

sim.Friedman1(n, sd = 1)

sim.Friedman2(n, sd = 125)

sim.Friedman3(n, sd = 0.1)

sim.Circle(n, dim = 5)
</code></pre>


<h3>Value</h3>

<p>List of deprecated functions
</p>


<h3><code>bark_mat</code></h3>

<p>Old version with matrix inputs used for testing;
</p>


<h3><code>sim.Friedman1</code></h3>

<p>For <code>sim.Friedman1</code>, use <code><a href="#topic+sim_Friedman1">sim_Friedman1</a></code>.
</p>


<h3><code>sim.Friedman2</code></h3>

<p>For <code>sim.Friedman2</code>, use <code><a href="#topic+sim_Friedman2">sim_Friedman2</a></code>.
</p>


<h3><code>sim.Friedman3</code></h3>

<p>For <code>sim.Friedman3</code>, use <code><a href="#topic+sim_Friedman3">sim_Friedman3</a></code>.
</p>


<h3><code>sim.Circle</code></h3>

<p>For <code>sim.Circle</code>, use <code><a href="#topic+sim_circle">sim_circle</a></code>.
</p>


<h3>See Also</h3>

<p>Other bark deprecated functions: 
<code><a href="#topic+bark-deprecated">bark-deprecated</a></code>,
<code><a href="#topic+sim.Circle-deprecated">sim.Circle-deprecated</a></code>,
<code><a href="#topic+sim.Friedman1-deprecated">sim.Friedman1-deprecated</a></code>,
<code><a href="#topic+sim.Friedman2-deprecated">sim.Friedman2-deprecated</a></code>,
<code><a href="#topic+sim.Friedman3-deprecated">sim.Friedman3-deprecated</a></code>
</p>
<p>Other bark functions: 
<code><a href="#topic+bark-package">bark-package</a></code>,
<code><a href="#topic+bark">bark</a>()</code>,
<code><a href="#topic+sim_Friedman1">sim_Friedman1</a>()</code>,
<code><a href="#topic+sim_Friedman2">sim_Friedman2</a>()</code>,
<code><a href="#topic+sim_Friedman3">sim_Friedman3</a>()</code>,
<code><a href="#topic+sim_circle">sim_circle</a>()</code>
</p>

<hr>
<h2 id='sim_circle'>Simulate Data from Hyper-Sphere for Classification Problems</h2><span id='topic+sim_circle'></span>

<h3>Description</h3>

<p>The classification problem Circle is described in the BARK paper (2008).
Inputs are <em>dim</em> independent variables uniformly
distributed on the interval <code class="reqn">[-1,1]</code>, only the first 2
out of these <em>dim</em> are actually signals.
Outputs are created according to the formula
</p>
<p style="text-align: center;"><code class="reqn">y = 1(x1^2+x2^2 \le 2/\pi)</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>sim_circle(n, dim = 5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim_circle_+3A_n">n</code></td>
<td>
<p>number of data points to generate</p>
</td></tr>
<tr><td><code id="sim_circle_+3A_dim">dim</code></td>
<td>
<p>number of dimension of the problem, no less than 2</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with components
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>input values (independent variables)</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>0/1 output values (dependent variable)</p>
</td></tr>
</table>


<h3>References</h3>

<p>Ouyang, Zhi (2008) Bayesian Additive Regression Kernels.
Duke University. PhD dissertation, Chapter 3.
</p>


<h3>See Also</h3>

<p>Other bark simulation functions: 
<code><a href="#topic+sim_Friedman1">sim_Friedman1</a>()</code>,
<code><a href="#topic+sim_Friedman2">sim_Friedman2</a>()</code>,
<code><a href="#topic+sim_Friedman3">sim_Friedman3</a>()</code>
</p>
<p>Other bark functions: 
<code><a href="#topic+bark-package-deprecated">bark-package-deprecated</a></code>,
<code><a href="#topic+bark-package">bark-package</a></code>,
<code><a href="#topic+bark">bark</a>()</code>,
<code><a href="#topic+sim_Friedman1">sim_Friedman1</a>()</code>,
<code><a href="#topic+sim_Friedman2">sim_Friedman2</a>()</code>,
<code><a href="#topic+sim_Friedman3">sim_Friedman3</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>sim_circle(n=100, dim=5)

</code></pre>

<hr>
<h2 id='sim_Friedman1'>Simulated Regression Problem Friedman 1</h2><span id='topic+sim_Friedman1'></span>

<h3>Description</h3>

<p>The regression problem Friedman 1 as described in Friedman (1991) and
Breiman (1996). Inputs are 10 independent variables uniformly
distributed on the interval <code class="reqn">[0,1]</code>, only 5 out of these 10 are actually
used. Outputs are created according to
the formula
</p>
<p style="text-align: center;"><code class="reqn">y = 10 \sin(\pi x1 x2) + 20 (x3 - 0.5)^2 + 10 x4 + 5 x5 + e</code>
</p>

<p>where e is <code class="reqn">N(0,sd^2)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim_Friedman1(n, sd = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim_Friedman1_+3A_n">n</code></td>
<td>
<p>number of data points to create</p>
</td></tr>
<tr><td><code id="sim_Friedman1_+3A_sd">sd</code></td>
<td>
<p>standard deviation of noise, with default value 1</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with components
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>input values (independent variables)</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>output values (dependent variable)</p>
</td></tr>
</table>


<h3>References</h3>

<p>Breiman, Leo (1996) Bagging predictors. Machine Learning 24,
pages 123-140. <br />
Friedman, Jerome H. (1991) Multivariate adaptive regression
splines. The Annals of Statistics 19 (1), pages 1-67.
</p>


<h3>See Also</h3>

<p>Other bark simulation functions: 
<code><a href="#topic+sim_Friedman2">sim_Friedman2</a>()</code>,
<code><a href="#topic+sim_Friedman3">sim_Friedman3</a>()</code>,
<code><a href="#topic+sim_circle">sim_circle</a>()</code>
</p>
<p>Other bark functions: 
<code><a href="#topic+bark-package-deprecated">bark-package-deprecated</a></code>,
<code><a href="#topic+bark-package">bark-package</a></code>,
<code><a href="#topic+bark">bark</a>()</code>,
<code><a href="#topic+sim_Friedman2">sim_Friedman2</a>()</code>,
<code><a href="#topic+sim_Friedman3">sim_Friedman3</a>()</code>,
<code><a href="#topic+sim_circle">sim_circle</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>sim_Friedman1(100, sd=1)
</code></pre>

<hr>
<h2 id='sim_Friedman2'>Simulated Regression Problem Friedman 2</h2><span id='topic+sim_Friedman2'></span>

<h3>Description</h3>

<p>The regression problem Friedman 2 as described in Friedman (1991) and
Breiman (1996). Inputs are 4 independent variables uniformly
distributed over the ranges
</p>
<p style="text-align: center;"><code class="reqn">0 \le x1 \le 100</code>
</p>

<p style="text-align: center;"><code class="reqn">40 \pi \le x2 \le 560 \pi</code>
</p>

<p style="text-align: center;"><code class="reqn">0 \le x3 \le 1</code>
</p>

<p style="text-align: center;"><code class="reqn">1 \le x4 \le 11</code>
</p>

<p>The outputs are created according to the formula
</p>
<p style="text-align: center;"><code class="reqn">y = (x1^2 + (x2 x3 - (1/(x2 x4)))^2)^{0.5} + e</code>
</p>

<p>where e is <code class="reqn">N(0,sd^2)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim_Friedman2(n, sd = 125)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim_Friedman2_+3A_n">n</code></td>
<td>
<p>number of data points to create</p>
</td></tr>
<tr><td><code id="sim_Friedman2_+3A_sd">sd</code></td>
<td>
<p>Standard deviation of noise. The default value of 125 gives
a signal to noise ratio (i.e., the ratio of the standard deviations) of
3:1. Thus, the variance of the function itself (without noise)
accounts for 90% of the total variance.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with components
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>input values (independent variables)</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>output values (dependent variable)</p>
</td></tr>
</table>


<h3>References</h3>

<p>Breiman, Leo (1996) Bagging predictors. Machine Learning 24,
pages 123-140. <br />
Friedman, Jerome H. (1991) Multivariate adaptive regression
splines. The Annals of Statistics 19 (1), pages 1-67.
</p>


<h3>See Also</h3>

<p>Other bark simulation functions: 
<code><a href="#topic+sim_Friedman1">sim_Friedman1</a>()</code>,
<code><a href="#topic+sim_Friedman3">sim_Friedman3</a>()</code>,
<code><a href="#topic+sim_circle">sim_circle</a>()</code>
</p>
<p>Other bark functions: 
<code><a href="#topic+bark-package-deprecated">bark-package-deprecated</a></code>,
<code><a href="#topic+bark-package">bark-package</a></code>,
<code><a href="#topic+bark">bark</a>()</code>,
<code><a href="#topic+sim_Friedman1">sim_Friedman1</a>()</code>,
<code><a href="#topic+sim_Friedman3">sim_Friedman3</a>()</code>,
<code><a href="#topic+sim_circle">sim_circle</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>sim_Friedman2(100, sd=125)
</code></pre>

<hr>
<h2 id='sim_Friedman3'>Simulated Regression Problem Friedman 3</h2><span id='topic+sim_Friedman3'></span>

<h3>Description</h3>

<p>The regression problem Friedman 3 as described in Friedman (1991) and
Breiman (1996). Inputs are 4 independent variables uniformly
distributed over the ranges
</p>
<p style="text-align: center;"><code class="reqn">0 \le x1 \le 100</code>
</p>

<p style="text-align: center;"><code class="reqn">40 \pi \le x2 \le 560 \pi</code>
</p>

<p style="text-align: center;"><code class="reqn">0 \le x3 \le 1</code>
</p>

<p style="text-align: center;"><code class="reqn">1 \le x4 \le 11</code>
</p>

<p>The outputs are created according to the formula
</p>
<p style="text-align: center;"><code class="reqn">\mbox{atan}((x2 x3 - (1/(x2 x4)))/x1) + e</code>
</p>

<p>where e is <code class="reqn">N(0,sd^2)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim_Friedman3(n, sd = 0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim_Friedman3_+3A_n">n</code></td>
<td>
<p>number of data points to create</p>
</td></tr>
<tr><td><code id="sim_Friedman3_+3A_sd">sd</code></td>
<td>
<p>Standard deviation of noise. The default value of 125 gives
a signal to noise ratio (i.e., the ratio of the standard deviations) of
3:1. Thus, the variance of the function itself (without noise)
accounts for 90% of the total variance.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with components
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>input values (independent variables)</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>output values (dependent variable)</p>
</td></tr>
</table>


<h3>References</h3>

<p>Breiman, Leo (1996) Bagging predictors. Machine Learning 24,
pages 123-140. <br />
Friedman, Jerome H. (1991) Multivariate adaptive regression
splines. The Annals of Statistics 19 (1), pages 1-67.
</p>


<h3>See Also</h3>

<p>Other bark simulation functions: 
<code><a href="#topic+sim_Friedman1">sim_Friedman1</a>()</code>,
<code><a href="#topic+sim_Friedman2">sim_Friedman2</a>()</code>,
<code><a href="#topic+sim_circle">sim_circle</a>()</code>
</p>
<p>Other bark functions: 
<code><a href="#topic+bark-package-deprecated">bark-package-deprecated</a></code>,
<code><a href="#topic+bark-package">bark-package</a></code>,
<code><a href="#topic+bark">bark</a>()</code>,
<code><a href="#topic+sim_Friedman1">sim_Friedman1</a>()</code>,
<code><a href="#topic+sim_Friedman2">sim_Friedman2</a>()</code>,
<code><a href="#topic+sim_circle">sim_circle</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>sim_Friedman3(n=100, sd=0.1)
</code></pre>

<hr>
<h2 id='sim.Circle-deprecated'>Simulate Data from Hyper-Sphere for Classification Problems</h2><span id='topic+sim.Circle-deprecated'></span>

<h3>Description</h3>

<p>The classification problem Circle is described in the BARK paper (2008).
Inputs are <em>dim</em> independent variables uniformly
distributed on the interval <code class="reqn">[-1,1]</code>, only the first 2
out of these <em>dim</em> are actually signals.
Outputs are created according to the formula
</p>
<p style="text-align: center;"><code class="reqn">y = 1(x1^2+x2^2 \le 2/\pi)</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>sim.Circle(n, dim=5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim.Circle-deprecated_+3A_n">n</code></td>
<td>
<p>number of data points to generate</p>
</td></tr>
<tr><td><code id="sim.Circle-deprecated_+3A_dim">dim</code></td>
<td>
<p>number of dimension of the problem, no less than 2</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with components
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>input values (independent variables)</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>0/1 output values (dependent variable)</p>
</td></tr>
</table>


<h3>References</h3>

<p>Ouyang, Zhi (2008) Bayesian Additive Regression Kernels.
Duke University. PhD dissertation, Chapter 3.
<br />
</p>


<h3>See Also</h3>

<p>Other bark deprecated functions: 
<code><a href="#topic+bark-deprecated">bark-deprecated</a></code>,
<code><a href="#topic+bark-package-deprecated">bark-package-deprecated</a></code>,
<code><a href="#topic+sim.Friedman1-deprecated">sim.Friedman1-deprecated</a></code>,
<code><a href="#topic+sim.Friedman2-deprecated">sim.Friedman2-deprecated</a></code>,
<code><a href="#topic+sim.Friedman3-deprecated">sim.Friedman3-deprecated</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  sim.Circle(n=100, dim = 5)

## End(Not run)
</code></pre>

<hr>
<h2 id='sim.Friedman1-deprecated'>Simulated Regression Problem Friedman 1</h2><span id='topic+sim.Friedman1-deprecated'></span>

<h3>Description</h3>

<p>The regression problem Friedman 1 as described in Friedman (1991) and
Breiman (1996). Inputs are 10 independent variables uniformly
distributed on the interval <code class="reqn">[0,1]</code>, only 5 out of these 10 are actually
used. Outputs are created according to
the formula
</p>
<p style="text-align: center;"><code class="reqn">y = 10 \sin(\pi x1 x2) + 20 (x3 - 0.5)^2 + 10 x4 + 5 x5 + e</code>
</p>

<p>where e is <code class="reqn">N(0,sd^2)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim.Friedman1(n, sd=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim.Friedman1-deprecated_+3A_n">n</code></td>
<td>
<p>number of data points to create</p>
</td></tr>
<tr><td><code id="sim.Friedman1-deprecated_+3A_sd">sd</code></td>
<td>
<p>standard deviation of noise, with default value 1</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with components
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>input values (independent variables)</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>output values (dependent variable)</p>
</td></tr>
</table>


<h3>References</h3>

<p>Breiman, Leo (1996) Bagging predictors. Machine Learning 24,
pages 123-140. <br />
Friedman, Jerome H. (1991) Multivariate adaptive regression
splines. The Annals of Statistics 19 (1), pages 1-67.
</p>


<h3>See Also</h3>

<p>Other bark deprecated functions: 
<code><a href="#topic+bark-deprecated">bark-deprecated</a></code>,
<code><a href="#topic+bark-package-deprecated">bark-package-deprecated</a></code>,
<code><a href="#topic+sim.Circle-deprecated">sim.Circle-deprecated</a></code>,
<code><a href="#topic+sim.Friedman2-deprecated">sim.Friedman2-deprecated</a></code>,
<code><a href="#topic+sim.Friedman3-deprecated">sim.Friedman3-deprecated</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
sim.Friedman1(100, sd=1)

## End(Not run)
</code></pre>

<hr>
<h2 id='sim.Friedman2-deprecated'>Simulated Regression Problem Friedman 2</h2><span id='topic+sim.Friedman2-deprecated'></span>

<h3>Description</h3>

<p>The regression problem Friedman 2 as described in Friedman (1991) and
Breiman (1996). Inputs are 4 independent variables uniformly
distributed over the ranges
</p>
<p style="text-align: center;"><code class="reqn">0 \le x1 \le 100</code>
</p>

<p style="text-align: center;"><code class="reqn">40 \pi \le x2 \le 560 \pi</code>
</p>

<p style="text-align: center;"><code class="reqn">0 \le x3 \le 1</code>
</p>

<p style="text-align: center;"><code class="reqn">1 \le x4 \le 11</code>
</p>

<p>The outputs are created according to the formula
</p>
<p style="text-align: center;"><code class="reqn">y = (x1^2 + (x2 x3 - (1/(x2 x4)))^2)^{0.5} + e</code>
</p>

<p>where e is <code class="reqn">N(0,sd^2)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim.Friedman2(n, sd=125)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim.Friedman2-deprecated_+3A_n">n</code></td>
<td>
<p>number of data points to create</p>
</td></tr>
<tr><td><code id="sim.Friedman2-deprecated_+3A_sd">sd</code></td>
<td>
<p>Standard deviation of noise. The default value of 125 gives
a signal to noise ratio (i.e., the ratio of the standard deviations) of
3:1. Thus, the variance of the function itself (without noise)
accounts for 90% of the total variance.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with components
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>input values (independent variables)</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>output values (dependent variable)</p>
</td></tr>
</table>


<h3>References</h3>

<p>Breiman, Leo (1996) Bagging predictors. Machine Learning 24,
pages 123-140. <br />
Friedman, Jerome H. (1991) Multivariate adaptive regression
splines. The Annals of Statistics 19 (1), pages 1-67.
</p>


<h3>See Also</h3>

<p>Other bark deprecated functions: 
<code><a href="#topic+bark-deprecated">bark-deprecated</a></code>,
<code><a href="#topic+bark-package-deprecated">bark-package-deprecated</a></code>,
<code><a href="#topic+sim.Circle-deprecated">sim.Circle-deprecated</a></code>,
<code><a href="#topic+sim.Friedman1-deprecated">sim.Friedman1-deprecated</a></code>,
<code><a href="#topic+sim.Friedman3-deprecated">sim.Friedman3-deprecated</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
sim.Friedman2(100, sd=125)

## End(Not run)
</code></pre>

<hr>
<h2 id='sim.Friedman3-deprecated'>Simulated Regression Problem Friedman 3</h2><span id='topic+sim.Friedman3-deprecated'></span>

<h3>Description</h3>

<p>The regression problem Friedman 3 as described in Friedman (1991) and
Breiman (1996). Inputs are 4 independent variables uniformly
distributed over the ranges
</p>
<p style="text-align: center;"><code class="reqn">0 \le x1 \le 100</code>
</p>

<p style="text-align: center;"><code class="reqn">40 \pi \le x2 \le 560 \pi</code>
</p>

<p style="text-align: center;"><code class="reqn">0 \le x3 \le 1</code>
</p>

<p style="text-align: center;"><code class="reqn">1 \le x4 \le 11</code>
</p>

<p>The outputs are created according to the formula
</p>
<p style="text-align: center;"><code class="reqn">\mbox{atan}((x2 x3 - (1/(x2 x4)))/x1) + e</code>
</p>

<p>where e is <code class="reqn">N(0,sd^2)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim.Friedman3(n, sd=0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim.Friedman3-deprecated_+3A_n">n</code></td>
<td>
<p>number of data points to create</p>
</td></tr>
<tr><td><code id="sim.Friedman3-deprecated_+3A_sd">sd</code></td>
<td>
<p>Standard deviation of noise. The default value of 125 gives
a signal to noise ratio (i.e., the ratio of the standard deviations) of
3:1. Thus, the variance of the function itself (without noise)
accounts for 90% of the total variance.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with components
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>input values (independent variables)</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>output values (dependent variable)</p>
</td></tr>
</table>


<h3>References</h3>

<p>Breiman, Leo (1996) Bagging predictors. Machine Learning 24,
pages 123-140. <br />
Friedman, Jerome H. (1991) Multivariate adaptive regression
splines. The Annals of Statistics 19 (1), pages 1-67.
</p>


<h3>See Also</h3>

<p>Other bark deprecated functions: 
<code><a href="#topic+bark-deprecated">bark-deprecated</a></code>,
<code><a href="#topic+bark-package-deprecated">bark-package-deprecated</a></code>,
<code><a href="#topic+sim.Circle-deprecated">sim.Circle-deprecated</a></code>,
<code><a href="#topic+sim.Friedman1-deprecated">sim.Friedman1-deprecated</a></code>,
<code><a href="#topic+sim.Friedman2-deprecated">sim.Friedman2-deprecated</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
sim.Friedman3(n=100, sd=0.1)

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
