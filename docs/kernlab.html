<!DOCTYPE html><html><head><title>Help for package kernlab</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {kernlab}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#as.kernelMatrix'><p>Assing kernelMatrix class to matrix objects</p></a></li>
<li><a href='#couple'><p>Probabilities Coupling function</p></a></li>
<li><a href='#csi'><p>Cholesky decomposition with Side Information</p></a></li>
<li><a href='#csi-class'><p>Class &quot;csi&quot;</p></a></li>
<li><a href='#dots'><p>Kernel Functions</p></a></li>
<li><a href='#gausspr'><p> Gaussian processes for regression and classification</p></a></li>
<li><a href='#gausspr-class'><p>Class &quot;gausspr&quot;</p></a></li>
<li><a href='#inchol'><p>Incomplete Cholesky decomposition</p></a></li>
<li><a href='#inchol-class'><p>Class &quot;inchol&quot;</p></a></li>
<li><a href='#income'><p>Income Data</p></a></li>
<li><a href='#inlearn'><p>Onlearn object initialization</p></a></li>
<li><a href='#ipop'><p>Quadratic Programming Solver</p></a></li>
<li><a href='#ipop-class'><p>Class &quot;ipop&quot;</p></a></li>
<li><a href='#kcca'><p>Kernel Canonical Correlation Analysis</p></a></li>
<li><a href='#kcca-class'><p>Class &quot;kcca&quot;</p></a></li>
<li><a href='#kernel-class'><p>Class &quot;kernel&quot; &quot;rbfkernel&quot; &quot;polykernel&quot;, &quot;tanhkernel&quot;, &quot;vanillakernel&quot;</p></a></li>
<li><a href='#kernelMatrix'><p>Kernel Matrix functions</p></a></li>
<li><a href='#kfa'><p>Kernel Feature Analysis</p></a></li>
<li><a href='#kfa-class'><p>Class &quot;kfa&quot;</p></a></li>
<li><a href='#kha'><p>Kernel Principal Components Analysis</p></a></li>
<li><a href='#kha-class'><p>Class &quot;kha&quot;</p></a></li>
<li><a href='#kkmeans'><p>Kernel k-means</p></a></li>
<li><a href='#kmmd'><p>Kernel Maximum Mean Discrepancy.</p></a></li>
<li><a href='#kmmd-class'><p>Class &quot;kqr&quot;</p></a></li>
<li><a href='#kpca'><p>Kernel Principal Components Analysis</p></a></li>
<li><a href='#kpca-class'><p>Class &quot;kpca&quot;</p></a></li>
<li><a href='#kqr'><p>Kernel Quantile Regression.</p></a></li>
<li><a href='#kqr-class'><p>Class &quot;kqr&quot;</p></a></li>
<li><a href='#ksvm'><p>Support Vector Machines</p></a></li>
<li><a href='#ksvm-class'><p>Class &quot;ksvm&quot;</p></a></li>
<li><a href='#lssvm'><p>Least Squares Support Vector Machine</p></a></li>
<li><a href='#lssvm-class'><p>Class &quot;lssvm&quot;</p></a></li>
<li><a href='#musk'><p>Musk data set</p></a></li>
<li><a href='#onlearn'><p>Kernel Online Learning algorithms</p></a></li>
<li><a href='#onlearn-class'><p>Class &quot;onlearn&quot;</p></a></li>
<li><a href='#plot'><p>plot method for support vector object</p></a></li>
<li><a href='#prc-class'><p>Class &quot;prc&quot;</p></a></li>
<li><a href='#predict.gausspr'><p>predict method for Gaussian Processes object</p></a></li>
<li><a href='#predict.kqr'><p>Predict method for kernel Quantile Regression object</p></a></li>
<li><a href='#predict.ksvm'><p>predict method for support vector object</p></a></li>
<li><a href='#promotergene'><p>E. coli promoter gene sequences (DNA)</p></a></li>
<li><a href='#ranking'><p>Ranking</p></a></li>
<li><a href='#ranking-class'><p>Class &quot;ranking&quot;</p></a></li>
<li><a href='#reuters'><p>Reuters Text Data</p></a></li>
<li><a href='#rvm'><p>Relevance Vector Machine</p></a></li>
<li><a href='#rvm-class'><p>Class &quot;rvm&quot;</p></a></li>
<li><a href='#sigest'><p>Hyperparameter estimation for the Gaussian Radial Basis kernel</p></a></li>
<li><a href='#spam'><p>Spam E-mail Database</p></a></li>
<li><a href='#specc'><p>Spectral Clustering</p></a></li>
<li><a href='#specc-class'><p>Class &quot;specc&quot;</p></a></li>
<li><a href='#spirals'><p>Spirals Dataset</p></a></li>
<li><a href='#stringdot'><p>String Kernel Functions</p></a></li>
<li><a href='#ticdata'><p>The Insurance Company Data</p></a></li>
<li><a href='#vm-class'><p>Class &quot;vm&quot;</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>0.9-32</td>
</tr>
<tr>
<td>Title:</td>
<td>Kernel-Based Machine Learning Lab</td>
</tr>
<tr>
<td>Description:</td>
<td>Kernel-based machine learning methods for classification,
        regression, clustering, novelty detection, quantile regression
        and dimensionality reduction.  Among other methods 'kernlab'
        includes Support Vector Machines, Spectral Clustering, Kernel
        PCA, Gaussian Processes and a QP solver.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods, stats, grDevices, graphics</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>Yes</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-01-31 14:16:15 UTC; hornik</td>
</tr>
<tr>
<td>Author:</td>
<td>Alexandros Karatzoglou [aut, cre],
  Alex Smola [aut],
  Kurt Hornik <a href="https://orcid.org/0000-0003-4198-9911"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  National ICT Australia (NICTA) [cph],
  Michael A. Maniscalco [ctb, cph],
  Choon Hui Teo [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Alexandros Karatzoglou &lt;alexandros.karatzoglou@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-01-31 15:26:48 UTC</td>
</tr>
</table>
<hr>
<h2 id='as.kernelMatrix'>Assing kernelMatrix class to matrix objects</h2><span id='topic+kernelMatrix-class'></span><span id='topic+as.kernelMatrix'></span><span id='topic+as.kernelMatrix-methods'></span><span id='topic+as.kernelMatrix+2Cmatrix-method'></span>

<h3>Description</h3>

<p><code>as.kernelMatrix</code> in package <span class="pkg">kernlab</span> can be used 
to coerce the kernelMatrix class to matrix objects representing a
kernel matrix.  These matrices can then be used with the kernelMatrix
interfaces which most of the functions in <span class="pkg">kernlab</span> support.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'matrix'
as.kernelMatrix(x, center = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.kernelMatrix_+3A_x">x</code></td>
<td>
<p>matrix to be assigned the <code>kernelMatrix</code> class </p>
</td></tr>
<tr><td><code id="as.kernelMatrix_+3A_center">center</code></td>
<td>
<p>center the kernel matrix in feature space (default: FALSE) </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Alexandros Karatzoglou <br />
<a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kernelMatrix">kernelMatrix</a></code>, <code><a href="#topic+dots">dots</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Create toy data
x &lt;- rbind(matrix(rnorm(10),,2),matrix(rnorm(10,mean=3),,2))
y &lt;- matrix(c(rep(1,5),rep(-1,5)))

### Use as.kernelMatrix to label the cov. matrix as a kernel matrix
### which is eq. to using a linear kernel 

K &lt;- as.kernelMatrix(crossprod(t(x)))

K

svp2 &lt;- ksvm(K, y, type="C-svc")

svp2

</code></pre>

<hr>
<h2 id='couple'>Probabilities Coupling function</h2><span id='topic+couple'></span>

<h3>Description</h3>

<p><code>couple</code> is used to link class-probability estimates produced by
pairwise coupling  in multi-class classification problems.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>couple(probin, coupler = "minpair")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="couple_+3A_probin">probin</code></td>
<td>
<p> The pairwise coupled class-probability estimates</p>
</td></tr>
<tr><td><code id="couple_+3A_coupler">coupler</code></td>
<td>
<p>The type of coupler to use. Currently <code>minpar</code> and
<code>pkpd</code> and <code>vote</code> are supported (see reference for more
details).
If <code>vote</code> is selected the returned value is a primitive estimate
passed on given votes.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>As binary classification problems are much easier to solve many
techniques exist to decompose multi-class classification problems into
many binary classification problems (voting, error codes,
etc.). Pairwise coupling (one against one) constructs a rule for
discriminating between every pair of classes and then selecting the
class
with the most winning two-class decisions. 
By using Platt's probabilities output for SVM one can get a class
probability for each of the <code class="reqn">k(k-1)/2</code> models created in the pairwise 
classification. The couple method implements various techniques to combine
these probabilities.    
</p>


<h3>Value</h3>

<p>A matrix with the resulting probability estimates. 
</p>


<h3>Author(s)</h3>

<p>Alexandros Karatzoglou <br /> <a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a> </p>


<h3>References</h3>

<p>Ting-Fan Wu, Chih-Jen Lin, ruby C. Weng<br />
<em>Probability Estimates for Multi-class Classification by Pairwise
Coupling</em><br />
Neural Information Processing Symposium 2003 <br />
<a href="https://papers.neurips.cc/paper/2454-probability-estimates-for-multi-class-classification-by-pairwise-coupling.pdf">https://papers.neurips.cc/paper/2454-probability-estimates-for-multi-class-classification-by-pairwise-coupling.pdf</a>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+predict.ksvm">predict.ksvm</a></code>, <code><a href="#topic+ksvm">ksvm</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## create artificial pairwise probabilities
pairs &lt;- matrix(c(0.82,0.12,0.76,0.1,0.9,0.05),2)

couple(pairs)

couple(pairs, coupler="pkpd")

couple(pairs, coupler ="vote")
</code></pre>

<hr>
<h2 id='csi'>Cholesky decomposition with Side Information</h2><span id='topic+csi'></span><span id='topic+csi-methods'></span><span id='topic+csi+2Cmatrix-method'></span>

<h3>Description</h3>

<p>The <code>csi</code> function in <span class="pkg">kernlab</span> is an implementation of an
incomplete Cholesky decomposition algorithm which exploits side
information (e.g., classification labels, regression responses) to
compute a low rank decomposition of a kernel matrix from the data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'matrix'
csi(x, y, kernel="rbfdot", kpar=list(sigma=0.1), rank,
centering = TRUE, kappa = 0.99 ,delta = 40 ,tol = 1e-5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="csi_+3A_x">x</code></td>
<td>
<p>The data matrix indexed by row</p>
</td></tr>
<tr><td><code id="csi_+3A_y">y</code></td>
<td>
<p>the classification labels or regression responses. In
classification y is a <code class="reqn">m \times n</code> matrix where <code class="reqn">m</code>
the number of data and <code class="reqn">n</code> the number of classes <code class="reqn">y</code> and <code class="reqn">y_i</code> is 1 if
the corresponding x belongs to class i.</p>
</td></tr>
<tr><td><code id="csi_+3A_kernel">kernel</code></td>
<td>
<p>the kernel function used in training and predicting.
This parameter can be set to any function, of class <code>kernel</code>,
which computes the inner product in feature space between two
vector arguments. kernlab provides the most popular kernel functions
which can be used by setting the kernel parameter to the following
strings:
</p>

<ul>
<li> <p><code>rbfdot</code> Radial Basis kernel function &quot;Gaussian&quot;
</p>
</li>
<li> <p><code>polydot</code> Polynomial kernel function
</p>
</li>
<li> <p><code>vanilladot</code> Linear kernel function
</p>
</li>
<li> <p><code>tanhdot</code> Hyperbolic tangent kernel function
</p>
</li>
<li> <p><code>laplacedot</code> Laplacian kernel function
</p>
</li>
<li> <p><code>besseldot</code> Bessel kernel function
</p>
</li>
<li> <p><code>anovadot</code> ANOVA RBF kernel function
</p>
</li>
<li> <p><code>splinedot</code> Spline kernel
</p>
</li>
<li> <p><code>stringdot</code> String kernel
</p>
</li></ul>

<p>The kernel parameter can also be set to a user defined function of
class kernel by passing the function name as an argument.
</p>
</td></tr>
<tr><td><code id="csi_+3A_kpar">kpar</code></td>
<td>
<p>the list of hyper-parameters (kernel parameters).
This is a list which contains the parameters to be used with the
kernel function. Valid parameters for existing kernels are :
</p>

<ul>
<li> <p><code>sigma</code> inverse kernel width for the Radial Basis
kernel function &quot;rbfdot&quot; and the Laplacian kernel &quot;laplacedot&quot;.
</p>
</li>
<li> <p><code>degree, scale, offset</code> for the Polynomial kernel &quot;polydot&quot;
</p>
</li>
<li> <p><code>scale, offset</code> for the Hyperbolic tangent kernel
function &quot;tanhdot&quot;
</p>
</li>
<li> <p><code>sigma, order, degree</code> for the Bessel kernel &quot;besseldot&quot;. 
</p>
</li>
<li> <p><code>sigma, degree</code> for the ANOVA kernel &quot;anovadot&quot;.
</p>
</li></ul>

<p>Hyper-parameters for user defined kernels can be passed through the
kpar parameter as well.
</p>
</td></tr>
<tr><td><code id="csi_+3A_rank">rank</code></td>
<td>
<p>maximal rank of the computed kernel matrix</p>
</td></tr>
<tr><td><code id="csi_+3A_centering">centering</code></td>
<td>
<p>if <code>TRUE</code> centering is performed (default: TRUE)</p>
</td></tr>
<tr><td><code id="csi_+3A_kappa">kappa</code></td>
<td>
<p>trade-off between approximation of K and prediction of Y (default: 0.99)</p>
</td></tr>
<tr><td><code id="csi_+3A_delta">delta</code></td>
<td>
<p>number of columns of cholesky performed in advance (default: 40)</p>
</td></tr>
<tr><td><code id="csi_+3A_tol">tol</code></td>
<td>
<p>minimum gain at each iteration (default: 1e-4)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An incomplete cholesky decomposition calculates
<code class="reqn">Z</code> where <code class="reqn">K= ZZ'</code> <code class="reqn">K</code> being the kernel matrix.
Since the rank of a kernel matrix is usually low, <code class="reqn">Z</code> tends to
be smaller then the complete kernel matrix. The decomposed matrix can be
used to create memory efficient kernel-based algorithms without the
need to compute and store a complete kernel matrix in memory. <br />
<code>csi</code> uses the class labels, or regression responses to compute a
more appropriate approximation for the problem at hand considering the
additional information from the response variable. </p>


<h3>Value</h3>

<p>An S4 object of class &quot;csi&quot; which is an extension of the class
&quot;matrix&quot;. The object is the decomposed kernel matrix along with 
the slots :
</p>
<table>
<tr><td><code>pivots</code></td>
<td>
<p>Indices on which pivots where done</p>
</td></tr>
<tr><td><code>diagresidues</code></td>
<td>
<p>Residuals left on the diagonal</p>
</td></tr>
<tr><td><code>maxresiduals</code></td>
<td>
<p>Residuals picked for pivoting</p>
</td></tr>
<tr><td><code>predgain</code></td>
<td>
<p>predicted gain before adding each column</p>
</td></tr>
<tr><td><code>truegain</code></td>
<td>
<p>actual gain after adding each column</p>
</td></tr>
<tr><td><code>Q</code></td>
<td>
<p>QR decomposition of the kernel matrix</p>
</td></tr>
<tr><td><code>R</code></td>
<td>
<p>QR decomposition of the kernel matrix</p>
</td></tr>
</table>
<p>slots can be accessed either by <code>object@slot</code>
or by accessor functions with the same name
(e.g., <code>pivots(object))</code></p>


<h3>Author(s)</h3>

<p>Alexandros Karatzoglou (based on Matlab code by 
Francis Bach)<br />
<a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>References</h3>

 
<p>Francis R. Bach, Michael I. Jordan<br />
<em>Predictive low-rank decomposition for kernel methods.</em><br />
Proceedings of the Twenty-second International Conference on Machine Learning (ICML) 2005<br />
<a href="http://www.di.ens.fr/~fbach/bach_jordan_csi.pdf">http://www.di.ens.fr/~fbach/bach_jordan_csi.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+inchol">inchol</a></code>, <code><a href="base.html#topic+chol">chol</a></code>, <code><a href="#topic+csi-class">csi-class</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(iris)

## create multidimensional y matrix
yind &lt;- t(matrix(1:3,3,150))
ymat &lt;- matrix(0, 150, 3)
ymat[yind==as.integer(iris[,5])] &lt;- 1

datamatrix &lt;- as.matrix(iris[,-5])
# initialize kernel function
rbf &lt;- rbfdot(sigma=0.1)
rbf
Z &lt;- csi(datamatrix,ymat, kernel=rbf, rank = 30)
dim(Z)
pivots(Z)
# calculate kernel matrix
K &lt;- crossprod(t(Z))
# difference between approximated and real kernel matrix
(K - kernelMatrix(kernel=rbf, datamatrix))[6,]

</code></pre>

<hr>
<h2 id='csi-class'>Class &quot;csi&quot;</h2><span id='topic+csi-class'></span><span id='topic+Q'></span><span id='topic+R'></span><span id='topic+predgain'></span><span id='topic+truegain'></span><span id='topic+diagresidues+2Ccsi-method'></span><span id='topic+maxresiduals+2Ccsi-method'></span><span id='topic+pivots+2Ccsi-method'></span><span id='topic+predgain+2Ccsi-method'></span><span id='topic+truegain+2Ccsi-method'></span><span id='topic+Q+2Ccsi-method'></span><span id='topic+R+2Ccsi-method'></span>

<h3>Description</h3>

<p>The reduced Cholesky decomposition object</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("csi", ...)</code>.
or by calling the  <code>csi</code> function.</p>


<h3>Slots</h3>


<dl>
<dt><code>.Data</code>:</dt><dd><p>Object of class <code>"matrix"</code> contains
the decomposed matrix</p>
</dd>
<dt><code>pivots</code>:</dt><dd><p>Object of class <code>"vector"</code> contains
the pivots performed</p>
</dd>
<dt><code>diagresidues</code>:</dt><dd><p>Object of class <code>"vector"</code> contains
the diagonial residues</p>
</dd>
<dt><code>maxresiduals</code>:</dt><dd><p>Object of class <code>"vector"</code> contains
the maximum residues</p>
</dd>
<dt>predgain</dt><dd><p>Object of class <code>"vector"</code> contains
the predicted gain before adding each column</p>
</dd>
<dt>truegain</dt><dd><p>Object of class <code>"vector"</code> contains
the actual gain after adding each column</p>
</dd>
<dt>Q</dt><dd><p>Object of class <code>"matrix"</code> contains
Q from the QR decomposition of the kernel matrix</p>
</dd>
<dt>R</dt><dd><p>Object of class <code>"matrix"</code> contains
R from the QR decomposition of the kernel matrix</p>
</dd>
</dl>



<h3>Extends</h3>

<p>Class <code>"matrix"</code>, directly.
</p>


<h3>Methods</h3>


<dl>
<dt>diagresidues</dt><dd><p><code>signature(object = "csi")</code>: returns
the diagonial residues</p>
</dd>
<dt>maxresiduals</dt><dd><p><code>signature(object = "csi")</code>: returns
the maximum residues</p>
</dd>
<dt>pivots</dt><dd><p><code>signature(object = "csi")</code>: returns
the pivots performed</p>
</dd>
<dt>predgain</dt><dd><p><code>signature(object = "csi")</code>: returns
the predicted gain before adding each column</p>
</dd>
<dt>truegain</dt><dd><p><code>signature(object = "csi")</code>: returns
the actual gain after adding each column</p>
</dd>
<dt>Q</dt><dd><p><code>signature(object = "csi")</code>: returns
Q from the QR decomposition of the kernel matrix</p>
</dd>
<dt>R</dt><dd><p><code>signature(object = "csi")</code>: returns
R from the QR decomposition of the kernel matrix</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Alexandros Karatzoglou<br /> <a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>See Also</h3>

 <p><code><a href="#topic+csi">csi</a></code>, <code><a href="#topic+inchol-class">inchol-class</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)

## create multidimensional y matrix
yind &lt;- t(matrix(1:3,3,150))
ymat &lt;- matrix(0, 150, 3)
ymat[yind==as.integer(iris[,5])] &lt;- 1

datamatrix &lt;- as.matrix(iris[,-5])
# initialize kernel function
rbf &lt;- rbfdot(sigma=0.1)
rbf
Z &lt;- csi(datamatrix,ymat, kernel=rbf, rank = 30)
dim(Z)
pivots(Z)
# calculate kernel matrix
K &lt;- crossprod(t(Z))
# difference between approximated and real kernel matrix
(K - kernelMatrix(kernel=rbf, datamatrix))[6,]

</code></pre>

<hr>
<h2 id='dots'>Kernel Functions</h2><span id='topic+dots'></span><span id='topic+kernels'></span><span id='topic+rbfdot'></span><span id='topic+polydot'></span><span id='topic+tanhdot'></span><span id='topic+vanilladot'></span><span id='topic+laplacedot'></span><span id='topic+besseldot'></span><span id='topic+anovadot'></span><span id='topic+fourierdot'></span><span id='topic+splinedot'></span><span id='topic+kpar'></span><span id='topic+kfunction'></span><span id='topic+show+2Ckernel-method'></span>

<h3>Description</h3>

<p>The kernel generating functions provided in kernlab. <br />
The Gaussian RBF kernel <code class="reqn">k(x,x') = \exp(-\sigma \|x - x'\|^2)</code> <br />
The Polynomial kernel <code class="reqn">k(x,x') = (scale &lt;x, x'&gt; + offset)^{degree}</code><br />
The Linear kernel <code class="reqn">k(x,x') = &lt;x, x'&gt;</code><br />
The Hyperbolic tangent kernel <code class="reqn">k(x, x') = \tanh(scale &lt;x, x'&gt; +  offset)</code><br />
The Laplacian kernel <code class="reqn">k(x,x') = \exp(-\sigma \|x - x'\|)</code> <br />
The Bessel kernel <code class="reqn">k(x,x') = (- Bessel_{(\nu+1)}^n \sigma \|x - x'\|^2)</code> <br />
The ANOVA RBF kernel <code class="reqn">k(x,x') = \sum_{1\leq i_1 \ldots &lt; i_D \leq
      N} \prod_{d=1}^D k(x_{id}, {x'}_{id})</code> where k(x,x) is a Gaussian
RBF kernel. <br />
The Spline kernel <code class="reqn"> \prod_{d=1}^D 1 + x_i x_j + x_i x_j min(x_i,
    x_j)  - \frac{x_i + x_j}{2} min(x_i,x_j)^2 +
    \frac{min(x_i,x_j)^3}{3}</code> \
The String kernels (see <code>stringdot</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rbfdot(sigma = 1)

polydot(degree = 1, scale = 1, offset = 1)

tanhdot(scale = 1, offset = 1)

vanilladot()

laplacedot(sigma = 1)

besseldot(sigma = 1, order = 1, degree = 1)

anovadot(sigma = 1, degree = 1)

splinedot()
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dots_+3A_sigma">sigma</code></td>
<td>
<p>The inverse kernel width used by the Gaussian the
Laplacian, the Bessel and the ANOVA kernel </p>
</td></tr>
<tr><td><code id="dots_+3A_degree">degree</code></td>
<td>
<p>The degree of the polynomial, bessel or ANOVA
kernel function. This has to be an positive integer.</p>
</td></tr>
<tr><td><code id="dots_+3A_scale">scale</code></td>
<td>
<p>The scaling parameter of the polynomial and tangent
kernel is a convenient way of normalizing
patterns without the need to modify the data itself</p>
</td></tr>
<tr><td><code id="dots_+3A_offset">offset</code></td>
<td>
<p>The offset used in a polynomial or hyperbolic tangent
kernel</p>
</td></tr>
<tr><td><code id="dots_+3A_order">order</code></td>
<td>
<p>The order of the Bessel function to be used as a kernel</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The kernel generating functions are used to initialize a kernel
function
which calculates the dot (inner) product between two feature vectors in a
Hilbert Space. These functions can be passed as a <code>kernel</code> argument on almost all
functions in <span class="pkg">kernlab</span>(e.g., <code>ksvm</code>, <code>kpca</code>  etc).
</p>
<p>Although using one of the existing kernel functions as a
<code>kernel</code> argument in various functions in <span class="pkg">kernlab</span> has the
advantage that optimized code is used to calculate various kernel expressions,
any other function implementing a dot product of class <code>kernel</code> can also be used as a kernel
argument. This allows the user to use, test and develop special kernels
for a given data set or algorithm.
For details on the string kernels see <code>stringdot</code>.
</p>


<h3>Value</h3>

<p>Return an S4 object of class <code>kernel</code> which extents the
<code>function</code> class. The resulting function implements the given
kernel calculating the inner (dot) product between two vectors.
</p>
<table>
<tr><td><code>kpar</code></td>
<td>
<p>a list containing the kernel parameters (hyperparameters)
used.</p>
</td></tr>
</table>
<p>The kernel parameters can be accessed by the <code>kpar</code> function.
</p>


<h3>Note</h3>

<p>If the offset in the Polynomial kernel is set to $0$, we obtain homogeneous polynomial
kernels, for positive values, we have inhomogeneous
kernels. Note that for negative values the kernel does not satisfy Mercer's
condition and thus the optimizers may fail. <br />
</p>
<p>In the Hyperbolic tangent kernel if the offset is negative the likelihood of obtaining a kernel
matrix that is not positive definite is much higher (since then even some
diagonal elements may be negative), hence if this kernel has to be used, the
offset should always be positive. Note, however, that this is no guarantee
that the kernel will be positive.
</p>


<h3>Author(s)</h3>

<p>Alexandros Karatzoglou<br />
<a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>See Also</h3>

<p><code>stringdot</code>, <code><a href="#topic+kernelMatrix">kernelMatrix</a> </code>, <code><a href="#topic+kernelMult">kernelMult</a></code>, <code><a href="#topic+kernelPol">kernelPol</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>rbfkernel &lt;- rbfdot(sigma = 0.1)
rbfkernel

kpar(rbfkernel)

## create two vectors
x &lt;- rnorm(10)
y &lt;- rnorm(10)

## calculate dot product
rbfkernel(x,y)

</code></pre>

<hr>
<h2 id='gausspr'> Gaussian processes for regression and classification</h2><span id='topic+gausspr'></span><span id='topic+gausspr+2Cformula-method'></span><span id='topic+gausspr+2Cvector-method'></span><span id='topic+gausspr+2Cmatrix-method'></span><span id='topic+coef+2Cgausspr-method'></span><span id='topic+show+2Cgausspr-method'></span>

<h3>Description</h3>

<p><code>gausspr</code> is an implementation of Gaussian processes
for classification and regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>

## S4 method for signature 'formula'
gausspr(x, data=NULL, ..., subset, na.action = na.omit, scaled = TRUE)

## S4 method for signature 'vector'
gausspr(x,...)

## S4 method for signature 'matrix'
gausspr(x, y, scaled = TRUE, type= NULL, kernel="rbfdot",
          kpar="automatic", var=1, variance.model = FALSE, tol=0.0005,
          cross=0, fit=TRUE, ... , subset, na.action = na.omit)


</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gausspr_+3A_x">x</code></td>
<td>
<p>a symbolic description of the model to be fit or a matrix or
vector when a formula interface is not used. 
When not using a formula x is a matrix or vector containing the variables in the model</p>
</td></tr> 
<tr><td><code id="gausspr_+3A_data">data</code></td>
<td>
<p>an optional data frame containing the variables in the model.
By default the variables are taken from the environment which
&lsquo;gausspr&rsquo; is called from.</p>
</td></tr>
<tr><td><code id="gausspr_+3A_y">y</code></td>
<td>
<p>a response vector with one label for each row/component of <code>x</code>. Can be either
a factor (for classification tasks) or a numeric vector (for
regression).</p>
</td></tr>
<tr><td><code id="gausspr_+3A_type">type</code></td>
<td>
<p>Type of problem. Either &quot;classification&quot; or &quot;regression&quot;.
Depending on whether <code>y</code> is a factor or not, the default
setting for <code>type</code> is <code>classification</code> or <code>regression</code>,
respectively, but can be overwritten by setting an explicit value.<br /></p>
</td></tr>
<tr><td><code id="gausspr_+3A_scaled">scaled</code></td>
<td>
<p>A logical vector indicating the variables to be
scaled. If <code>scaled</code> is of length 1, the value is recycled as
many times as needed and all non-binary variables are scaled.
Per default, data are scaled internally (both <code>x</code> and <code>y</code>
variables) to zero mean and unit variance. The center and scale
values are returned and used for later predictions.</p>
</td></tr>
<tr><td><code id="gausspr_+3A_kernel">kernel</code></td>
<td>
<p>the kernel function used in training and predicting.
This parameter can be set to any function, of class kernel, which computes a dot product between two
vector arguments. kernlab provides the most popular kernel functions
which can be used by setting the kernel parameter to the following
strings:
</p>

<ul>
<li> <p><code>rbfdot</code> Radial Basis kernel function &quot;Gaussian&quot;
</p>
</li>
<li> <p><code>polydot</code> Polynomial kernel function
</p>
</li>
<li> <p><code>vanilladot</code> Linear kernel function
</p>
</li>
<li> <p><code>tanhdot</code> Hyperbolic tangent kernel function
</p>
</li>
<li> <p><code>laplacedot</code> Laplacian kernel function
</p>
</li>
<li> <p><code>besseldot</code> Bessel kernel function
</p>
</li>
<li> <p><code>anovadot</code> ANOVA RBF kernel function
</p>
</li>
<li> <p><code>splinedot</code> Spline kernel 
</p>
</li></ul>

<p>The kernel parameter can also be set to a user defined function of
class kernel by passing the function name as an argument.
</p>
</td></tr>
<tr><td><code id="gausspr_+3A_kpar">kpar</code></td>
<td>
<p>the list of hyper-parameters (kernel parameters).
This is a list which contains the parameters to be used with the
kernel function. Valid parameters for existing kernels are :
</p>

<ul>
<li> <p><code>sigma</code> inverse kernel width for the Radial Basis
kernel function &quot;rbfdot&quot; and the Laplacian kernel &quot;laplacedot&quot;.
</p>
</li>
<li> <p><code>degree, scale, offset</code> for the Polynomial kernel &quot;polydot&quot;
</p>
</li>
<li> <p><code>scale, offset</code> for the Hyperbolic tangent kernel
function &quot;tanhdot&quot;
</p>
</li>
<li> <p><code>sigma, order, degree</code> for the Bessel kernel &quot;besseldot&quot;. 
</p>
</li>
<li> <p><code>sigma, degree</code> for the ANOVA kernel &quot;anovadot&quot;.
</p>
</li></ul>

<p>Hyper-parameters for user defined kernels can be passed through the
kpar parameter as well.</p>
</td></tr>
<tr><td><code id="gausspr_+3A_var">var</code></td>
<td>
<p>the initial noise variance, (only for regression) (default
: 0.001)</p>
</td></tr>
<tr><td><code id="gausspr_+3A_variance.model">variance.model</code></td>
<td>
<p>build model for variance or standard deviation estimation (only for regression) (default
: FALSE)</p>
</td></tr>
<tr><td><code id="gausspr_+3A_tol">tol</code></td>
<td>
<p>tolerance of termination criterion (default: 0.001)</p>
</td></tr>
<tr><td><code id="gausspr_+3A_fit">fit</code></td>
<td>
<p>indicates whether the fitted values should be computed and
included in the model or not (default: 'TRUE')</p>
</td></tr>
<tr><td><code id="gausspr_+3A_cross">cross</code></td>
<td>
<p>if a integer value k&gt;0 is specified, a k-fold cross
validation on the training data is performed to assess the
quality of the model: the Mean Squared Error for regression</p>
</td></tr>
<tr><td><code id="gausspr_+3A_subset">subset</code></td>
<td>
<p>An index vector specifying the cases to be used in the
training sample.  (NOTE: If given, this argument must be
named.)</p>
</td></tr>
<tr><td><code id="gausspr_+3A_na.action">na.action</code></td>
<td>
<p>A function to specify the action to be taken if <code>NA</code>s are
found. The default action is <code>na.omit</code>, which leads to
rejection of cases with missing values on any required variable. An
alternative is <code>na.fail</code>, which causes an error if <code>NA</code>
cases are found. (NOTE: If given, this argument must be named.)</p>
</td></tr>
<tr><td><code id="gausspr_+3A_...">...</code></td>
<td>
<p> additional parameters</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A Gaussian process is specified by a mean and a covariance function.
The mean is a function of <code class="reqn">x</code> (which is often the zero function), and
the covariance
is a function <code class="reqn">C(x,x')</code> which expresses the expected covariance between the
value of the function <code class="reqn">y</code> at the points <code class="reqn">x</code> and <code class="reqn">x'</code>.
The actual function <code class="reqn">y(x)</code> in any data modeling problem is assumed to be
a single sample from this Gaussian distribution.
Laplace approximation is used for the parameter estimation in gaussian
processes for classification.<br />
</p>
<p>The predict function can return class probabilities for 
classification problems by setting the <code>type</code> parameter to &quot;probabilities&quot;.
For the regression setting the <code>type</code> parameter to &quot;variance&quot; or &quot;sdeviation&quot; returns the estimated variance or standard deviation at each predicted point.
</p>


<h3>Value</h3>

<p>An S4 object of class &quot;gausspr&quot; containing the fitted model along with
information.
Accessor functions can be used to access the slots of the
object which include :
</p>
<table>
<tr><td><code>alpha</code></td>
<td>
<p>The resulting model parameters</p>
</td></tr>
<tr><td><code>error</code></td>
<td>
<p>Training error (if fit == TRUE)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Alexandros Karatzoglou <br /> <a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>References</h3>

<p>C. K. I. Williams and D. Barber <br />
Bayesian classification with Gaussian processes. <br />
IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(12):1342-1351, 1998<br />
<a href="https://homepages.inf.ed.ac.uk/ckiw/postscript/pami_final.ps.gz">https://homepages.inf.ed.ac.uk/ckiw/postscript/pami_final.ps.gz</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.gausspr">predict.gausspr</a></code>, <code><a href="#topic+rvm">rvm</a></code>, <code><a href="#topic+ksvm">ksvm</a></code>, <code><a href="#topic+gausspr-class">gausspr-class</a></code>, <code><a href="#topic+lssvm">lssvm</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'># train model
data(iris)
test &lt;- gausspr(Species~.,data=iris,var=2)
test
alpha(test)

# predict on the training set
predict(test,iris[,-5])
# class probabilities 
predict(test, iris[,-5], type="probabilities")

# create regression data
x &lt;- seq(-20,20,0.1)
y &lt;- sin(x)/x + rnorm(401,sd=0.03)

# regression with gaussian processes
foo &lt;- gausspr(x, y)
foo

# predict and plot
ytest &lt;- predict(foo, x)
plot(x, y, type ="l")
lines(x, ytest, col="red")


#predict and variance
x = c(-4, -3, -2, -1,  0, 0.5, 1, 2)
y = c(-2,  0,  -0.5,1,  2, 1, 0, -1)
plot(x,y)
foo2 &lt;- gausspr(x, y, variance.model = TRUE)
xtest &lt;- seq(-4,2,0.2)
lines(xtest, predict(foo2, xtest))
lines(xtest,
      predict(foo2, xtest)+2*predict(foo2,xtest, type="sdeviation"),
      col="red")
lines(xtest,
      predict(foo2, xtest)-2*predict(foo2,xtest, type="sdeviation"),
      col="red")

</code></pre>

<hr>
<h2 id='gausspr-class'>Class &quot;gausspr&quot;</h2><span id='topic+gausspr-class'></span><span id='topic+alpha+2Cgausspr-method'></span><span id='topic+cross+2Cgausspr-method'></span><span id='topic+error+2Cgausspr-method'></span><span id='topic+kcall+2Cgausspr-method'></span><span id='topic+kernelf+2Cgausspr-method'></span><span id='topic+kpar+2Cgausspr-method'></span><span id='topic+lev+2Cgausspr-method'></span><span id='topic+type+2Cgausspr-method'></span><span id='topic+alphaindex+2Cgausspr-method'></span><span id='topic+xmatrix+2Cgausspr-method'></span><span id='topic+ymatrix+2Cgausspr-method'></span><span id='topic+scaling+2Cgausspr-method'></span>

<h3>Description</h3>

<p>The Gaussian Processes object class</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("gausspr", ...)</code>.
or by calling the <code>gausspr</code> function 
</p>


<h3>Slots</h3>


<dl>
<dt><code>tol</code>:</dt><dd><p>Object of class <code>"numeric"</code> contains
tolerance of termination criteria</p>
</dd>
<dt><code>kernelf</code>:</dt><dd><p>Object of class <code>"kfunction"</code> contains
the kernel function used</p>
</dd>
<dt><code>kpar</code>:</dt><dd><p>Object of class <code>"list"</code> contains the
kernel parameter used </p>
</dd>
<dt><code>kcall</code>:</dt><dd><p>Object of class <code>"list"</code> contains the used
function call </p>
</dd>
<dt><code>type</code>:</dt><dd><p>Object of class <code>"character"</code> contains
type of problem </p>
</dd>
<dt><code>terms</code>:</dt><dd><p>Object of class <code>"ANY"</code> contains the
terms representation of the symbolic model used (when using a formula)</p>
</dd>
<dt><code>xmatrix</code>:</dt><dd><p>Object of class <code>"input"</code> containing
the data matrix used </p>
</dd>
<dt><code>ymatrix</code>:</dt><dd><p>Object of class <code>"output"</code> containing the
response matrix</p>
</dd>
<dt><code>fitted</code>:</dt><dd><p>Object of class <code>"output"</code> containing the
fitted values  </p>
</dd>
<dt><code>lev</code>:</dt><dd><p>Object of class <code>"vector"</code> containing the
levels of the response (in case of classification) </p>
</dd>
<dt><code>nclass</code>:</dt><dd><p>Object of class <code>"numeric"</code> containing
the number of classes (in case of classification) </p>
</dd>
<dt><code>alpha</code>:</dt><dd><p>Object of class <code>"listI"</code> containing the
computes alpha values </p>
</dd>
<dt><code>alphaindex</code></dt><dd><p>Object of class <code>"list"</code> containing
the indexes for the alphas in various classes (in multi-class
problems).</p>
</dd>
<dt><code>sol</code></dt><dd><p>Object of class <code>"matrix"</code> containing the solution to the Gaussian Process formulation, it is used to compute the variance in regression problems.</p>
</dd> 
<dt><code>scaling</code></dt><dd><p>Object of class <code>"ANY"</code> containing
the scaling coefficients of the data (when case <code>scaled = TRUE</code> is used).</p>
</dd>
<dt><code>nvar</code>:</dt><dd><p>Object of class <code>"numeric"</code> containing the
computed variance</p>
</dd>
<dt><code>error</code>:</dt><dd><p>Object of class <code>"numeric"</code> containing the
training error</p>
</dd>
<dt><code>cross</code>:</dt><dd><p>Object of class <code>"numeric"</code> containing the
cross validation error</p>
</dd>
<dt><code>n.action</code>:</dt><dd><p>Object of class <code>"ANY"</code> containing the
action performed in NA </p>
</dd>
</dl>



<h3>Methods</h3>

 
<dl>
<dt>alpha</dt><dd><p><code>signature(object = "gausspr")</code>: returns the alpha
vector</p>
</dd>
<dt>cross</dt><dd><p><code>signature(object = "gausspr")</code>: returns the cross
validation error </p>
</dd>
<dt>error</dt><dd><p><code>signature(object = "gausspr")</code>: returns the
training error </p>
</dd>
<dt>fitted</dt><dd><p><code>signature(object = "vm")</code>: returns the fitted values </p>
</dd>
<dt>kcall</dt><dd><p><code>signature(object = "gausspr")</code>: returns the call performed</p>
</dd>
<dt>kernelf</dt><dd><p><code>signature(object = "gausspr")</code>: returns the
kernel function used</p>
</dd>
<dt>kpar</dt><dd><p><code>signature(object = "gausspr")</code>: returns the kernel
parameter used</p>
</dd>
<dt>lev</dt><dd><p><code>signature(object = "gausspr")</code>: returns the
response levels (in classification) </p>
</dd>
<dt>type</dt><dd><p><code>signature(object = "gausspr")</code>: returns the type
of problem</p>
</dd>
<dt>xmatrix</dt><dd><p><code>signature(object = "gausspr")</code>: returns the
data matrix used</p>
</dd>
<dt>ymatrix</dt><dd><p><code>signature(object = "gausspr")</code>: returns the
response matrix used</p>
</dd>
<dt>scaling</dt><dd><p><code>signature(object = "gausspr")</code>: returns the
scaling coefficients of the data (when <code>scaled = TRUE</code> is used)</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Alexandros Karatzoglou<br /> <a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+gausspr">gausspr</a></code>, 
<code><a href="#topic+ksvm-class">ksvm-class</a></code>,
<code><a href="#topic+vm-class">vm-class</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# train model
data(iris)
test &lt;- gausspr(Species~.,data=iris,var=2)
test
alpha(test)
error(test)
lev(test)
</code></pre>

<hr>
<h2 id='inchol'>Incomplete Cholesky decomposition</h2><span id='topic+inchol'></span><span id='topic+inchol+2Cmatrix-method'></span>

<h3>Description</h3>

<p><code>inchol</code> computes the incomplete Cholesky decomposition
of the kernel matrix from a data matrix. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>inchol(x, kernel="rbfdot", kpar=list(sigma=0.1), tol = 0.001, 
            maxiter = dim(x)[1], blocksize = 50, verbose = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="inchol_+3A_x">x</code></td>
<td>
<p>The data matrix indexed by row</p>
</td></tr>
<tr><td><code id="inchol_+3A_kernel">kernel</code></td>
<td>
<p>the kernel function used in training and predicting.
This parameter can be set to any function, of class <code>kernel</code>,
which computes the inner product in feature space between two
vector arguments. kernlab provides the most popular kernel functions
which can be used by setting the kernel parameter to the following
strings:
</p>

<ul>
<li> <p><code>rbfdot</code> Radial Basis kernel function &quot;Gaussian&quot;
</p>
</li>
<li> <p><code>polydot</code> Polynomial kernel function
</p>
</li>
<li> <p><code>vanilladot</code> Linear kernel function
</p>
</li>
<li> <p><code>tanhdot</code> Hyperbolic tangent kernel function
</p>
</li>
<li> <p><code>laplacedot</code> Laplacian kernel function
</p>
</li>
<li> <p><code>besseldot</code> Bessel kernel function
</p>
</li>
<li> <p><code>anovadot</code> ANOVA RBF kernel function
</p>
</li>
<li> <p><code>splinedot</code> Spline kernel 
</p>
</li></ul>

<p>The kernel parameter can also be set to a user defined function of
class kernel by passing the function name as an argument.
</p>
</td></tr>
<tr><td><code id="inchol_+3A_kpar">kpar</code></td>
<td>
<p>the list of hyper-parameters (kernel parameters).
This is a list which contains the parameters to be used with the
kernel function. Valid parameters for existing kernels are :
</p>

<ul>
<li> <p><code>sigma</code> inverse kernel width for the Radial Basis
kernel function &quot;rbfdot&quot; and the Laplacian kernel &quot;laplacedot&quot;.
</p>
</li>
<li> <p><code>degree, scale, offset</code> for the Polynomial kernel &quot;polydot&quot;
</p>
</li>
<li> <p><code>scale, offset</code> for the Hyperbolic tangent kernel
function &quot;tanhdot&quot;
</p>
</li>
<li> <p><code>sigma, order, degree</code> for the Bessel kernel &quot;besseldot&quot;. 
</p>
</li>
<li> <p><code>sigma, degree</code> for the ANOVA kernel &quot;anovadot&quot;.
</p>
</li></ul>

<p>Hyper-parameters for user defined kernels can be passed through the
kpar parameter as well.
</p>
</td></tr>
<tr><td><code id="inchol_+3A_tol">tol</code></td>
<td>
<p>algorithm stops when remaining pivots bring less accuracy
then <code>tol</code> (default: 0.001)</p>
</td></tr>
<tr><td><code id="inchol_+3A_maxiter">maxiter</code></td>
<td>
<p>maximum number of iterations and columns in <code class="reqn">Z</code></p>
</td></tr>
<tr><td><code id="inchol_+3A_blocksize">blocksize</code></td>
<td>
<p>add this many columns to matrix per iteration</p>
</td></tr>
<tr><td><code id="inchol_+3A_verbose">verbose</code></td>
<td>
<p>print info on algorithm convergence</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An incomplete cholesky decomposition calculates
<code class="reqn">Z</code> where <code class="reqn">K= ZZ'</code> <code class="reqn">K</code> being the kernel matrix.
Since the rank of a kernel matrix is usually low, <code class="reqn">Z</code> tends to be smaller
then the complete kernel matrix. The decomposed matrix can be
used to create memory efficient kernel-based algorithms without the
need to compute and store a complete kernel matrix in memory.</p>


<h3>Value</h3>

<p>An S4 object of class &quot;inchol&quot; which is an extension of the class
&quot;matrix&quot;. The object is the decomposed kernel matrix along with 
the slots :
</p>
<table>
<tr><td><code>pivots</code></td>
<td>
<p>Indices on which pivots where done</p>
</td></tr>
<tr><td><code>diagresidues</code></td>
<td>
<p>Residuals left on the diagonal</p>
</td></tr>
<tr><td><code>maxresiduals</code></td>
<td>
<p>Residuals picked for pivoting</p>
</td></tr>
</table>
<p>slots can be accessed either by <code>object@slot</code>
or by accessor functions with the same name (e.g., <code>pivots(object))</code></p>


<h3>Author(s)</h3>

<p>Alexandros Karatzoglou (based on Matlab code by 
S.V.N. (Vishy) Vishwanathan and Alex Smola)<br />
<a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>References</h3>

<p>Francis R. Bach, Michael I. Jordan<br />
<em>Kernel Independent Component Analysis</em><br />
Journal of Machine Learning Research  3, 1-48<br />
<a href="https://www.jmlr.org/papers/volume3/bach02a/bach02a.pdf">https://www.jmlr.org/papers/volume3/bach02a/bach02a.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+csi">csi</a></code>, <code><a href="#topic+inchol-class">inchol-class</a></code>, <code><a href="base.html#topic+chol">chol</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(iris)
datamatrix &lt;- as.matrix(iris[,-5])
# initialize kernel function
rbf &lt;- rbfdot(sigma=0.1)
rbf
Z &lt;- inchol(datamatrix,kernel=rbf)
dim(Z)
pivots(Z)
# calculate kernel matrix
K &lt;- crossprod(t(Z))
# difference between approximated and real kernel matrix
(K - kernelMatrix(kernel=rbf, datamatrix))[6,]

</code></pre>

<hr>
<h2 id='inchol-class'>Class &quot;inchol&quot; </h2><span id='topic+inchol-class'></span><span id='topic+diagresidues'></span><span id='topic+maxresiduals'></span><span id='topic+pivots'></span><span id='topic+diagresidues+2Cinchol-method'></span><span id='topic+maxresiduals+2Cinchol-method'></span><span id='topic+pivots+2Cinchol-method'></span>

<h3>Description</h3>

<p>The reduced Cholesky decomposition object</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("inchol", ...)</code>.
or by calling the  <code>inchol</code> function.</p>


<h3>Slots</h3>


<dl>
<dt><code>.Data</code>:</dt><dd><p>Object of class <code>"matrix"</code> contains
the decomposed matrix</p>
</dd>
<dt><code>pivots</code>:</dt><dd><p>Object of class <code>"vector"</code> contains
the pivots performed</p>
</dd>
<dt><code>diagresidues</code>:</dt><dd><p>Object of class <code>"vector"</code> contains
the diagonial residues</p>
</dd>
<dt><code>maxresiduals</code>:</dt><dd><p>Object of class <code>"vector"</code> contains
the maximum residues</p>
</dd>
</dl>



<h3>Extends</h3>

<p>Class <code>"matrix"</code>, directly.
</p>


<h3>Methods</h3>


<dl>
<dt>diagresidues</dt><dd><p><code>signature(object = "inchol")</code>: returns
the diagonial residues</p>
</dd>
<dt>maxresiduals</dt><dd><p><code>signature(object = "inchol")</code>: returns
the maximum residues</p>
</dd>
<dt>pivots</dt><dd><p><code>signature(object = "inchol")</code>: returns
the pivots performed</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Alexandros Karatzoglou<br /> <a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>See Also</h3>

 <p><code><a href="#topic+inchol">inchol</a></code>, <code><a href="#topic+csi-class">csi-class</a></code>, <code><a href="#topic+csi">csi</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
datamatrix &lt;- as.matrix(iris[,-5])
# initialize kernel function
rbf &lt;- rbfdot(sigma=0.1)
rbf
Z &lt;- inchol(datamatrix,kernel=rbf)
dim(Z)
pivots(Z)
diagresidues(Z)
maxresiduals(Z)
</code></pre>

<hr>
<h2 id='income'>Income Data</h2><span id='topic+income'></span>

<h3>Description</h3>

<p>Customer Income Data from a marketing survey.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(income)</code></pre>


<h3>Format</h3>

<p>A data frame with 14 categorical variables (8993 observations).
</p>
<p>Explanation of the variable names:
</p>

<table>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> 1 </td><td style="text-align: left;"> <code>INCOME</code> </td><td style="text-align: left;"> annual income of household </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;">   </td><td style="text-align: left;">               </td><td style="text-align: left;"> (Personal income if single) </td><td style="text-align: left;"> ordinal</td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> 2 </td><td style="text-align: left;"> <code>SEX</code> </td><td style="text-align: left;"> sex </td><td style="text-align: left;"> nominal</td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> 3 </td><td style="text-align: left;"> <code>MARITAL.STATUS</code> </td><td style="text-align: left;"> marital status </td><td style="text-align: left;"> nominal</td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> 4 </td><td style="text-align: left;"> <code>AGE</code> </td><td style="text-align: left;"> age </td><td style="text-align: left;"> ordinal</td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> 5 </td><td style="text-align: left;"> <code>EDUCATION</code> </td><td style="text-align: left;"> educational grade </td><td style="text-align: left;"> ordinal</td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> 6 </td><td style="text-align: left;"> <code>OCCUPATION</code> </td><td style="text-align: left;"> type of work </td><td style="text-align: left;"> nominal </td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> 7 </td><td style="text-align: left;"> <code>AREA</code> </td><td style="text-align: left;"> how long the interviewed person has lived</td><td style="text-align: left;">
    </td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;">   </td><td style="text-align: left;">             </td><td style="text-align: left;"> in the San Francisco/Oakland/San Jose area </td><td style="text-align: left;"> ordinal</td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> 8 </td><td style="text-align: left;"> <code>DUAL.INCOMES</code> </td><td style="text-align: left;"> dual incomes (if married) </td><td style="text-align: left;"> nominal</td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> 9 </td><td style="text-align: left;"> <code>HOUSEHOLD.SIZE</code> </td><td style="text-align: left;"> persons living in the
    household </td><td style="text-align: left;"> ordinal</td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> 10 </td><td style="text-align: left;"> <code>UNDER18</code> </td><td style="text-align: left;"> persons in household under 18 </td><td style="text-align: left;"> ordinal</td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> 11 </td><td style="text-align: left;"> <code>HOUSEHOLDER</code> </td><td style="text-align: left;"> householder status </td><td style="text-align: left;"> nominal</td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> 12 </td><td style="text-align: left;"> <code>HOME.TYPE</code> </td><td style="text-align: left;"> type of home </td><td style="text-align: left;"> nominal</td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> 13 </td><td style="text-align: left;"> <code>ETHNIC.CLASS</code> </td><td style="text-align: left;"> ethnic classification </td><td style="text-align: left;"> nominal</td>
</tr>
<tr>
 <td style="text-align: right;">
    </td><td style="text-align: left;"> 14 </td><td style="text-align: left;"> <code>LANGUAGE</code> </td><td style="text-align: left;"> language most often spoken at
    home </td><td style="text-align: left;"> nominal</td>
</tr>
<tr>
 <td style="text-align: right;">
  </td>
</tr>

</table>



<h3>Details</h3>

<p>A total of N=9409 questionnaires containing 502 questions were 
filled out by shopping mall customers in the San Francisco Bay area. 
The dataset is an extract from this survey. It consists of 
14 demographic attributes. The dataset is a mixture of nominal and
ordinal variables with a lot of missing data.
The goal is to predict the Anual Income of Household from the other 13
demographics attributes.
</p>


<h3>Source</h3>

<p>Impact Resources, Inc., Columbus, OH (1987).
</p>

<hr>
<h2 id='inlearn'>Onlearn object initialization</h2><span id='topic+inlearn'></span><span id='topic+inlearn+2Cnumeric-method'></span>

<h3>Description</h3>

<p>Online Kernel Algorithm object <code>onlearn</code> initialization function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
## S4 method for signature 'numeric'
inlearn(d, kernel = "rbfdot", kpar = list(sigma = 0.1),
        type = "novelty", buffersize = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="inlearn_+3A_d">d</code></td>
<td>
<p>the dimensionality of the data to be learned</p>
</td></tr>
<tr><td><code id="inlearn_+3A_kernel">kernel</code></td>
<td>
<p>the kernel function used in training and predicting.
This parameter can be set to any function, of class kernel, which computes a dot product between two
vector arguments. kernlab provides the most popular kernel functions
which can be used by setting the kernel parameter to the following
strings:
</p>

<ul>
<li> <p><code>rbfdot</code> Radial Basis kernel function &quot;Gaussian&quot;
</p>
</li>
<li> <p><code>polydot</code> Polynomial kernel function
</p>
</li>
<li> <p><code>vanilladot</code> Linear kernel function
</p>
</li>
<li> <p><code>tanhdot</code> Hyperbolic tangent kernel function
</p>
</li>
<li> <p><code>laplacedot</code> Laplacian kernel function
</p>
</li>
<li> <p><code>besseldot</code> Bessel kernel function
</p>
</li>
<li> <p><code>anovadot</code> ANOVA RBF kernel function
</p>
</li></ul>

<p>The kernel parameter can also be set to a user defined function of
class kernel by passing the function name as an argument.
</p>
</td></tr>
<tr><td><code id="inlearn_+3A_kpar">kpar</code></td>
<td>
<p>the list of hyper-parameters (kernel parameters).
This is a list which contains the parameters to be used with the
kernel function. For valid parameters for existing kernels are :
</p>

<ul>
<li> <p><code>sigma</code> inverse kernel width for the Radial Basis
kernel function &quot;rbfdot&quot; and the Laplacian kernel &quot;laplacedot&quot;.
</p>
</li>
<li> <p><code>degree, scale, offset</code> for the Polynomial kernel &quot;polydot&quot;
</p>
</li>
<li> <p><code>scale, offset</code> for the Hyperbolic tangent kernel
function &quot;tanhdot&quot;
</p>
</li>
<li> <p><code>sigma, order, degree</code> for the Bessel kernel &quot;besseldot&quot;.
</p>
</li>
<li> <p><code>sigma, degree</code> for the ANOVA kernel &quot;anovadot&quot;.
</p>
</li></ul>

<p>Hyper-parameters for user defined kernels can be passed through the
<code>kpar</code> parameter as well.</p>
</td></tr>
<tr><td><code id="inlearn_+3A_type">type</code></td>
<td>
<p>the type of problem to be learned by the online algorithm
:
<code>classification</code>, <code>regression</code>, <code>novelty</code></p>
</td></tr>
<tr><td><code id="inlearn_+3A_buffersize">buffersize</code></td>
<td>
<p>the size of the buffer to be used</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>inlearn</code> is used to initialize a blank <code>onlearn</code> object.
</p>


<h3>Value</h3>

<p>The function returns an <code>S4</code> object of class <code>onlearn</code> that
can be used by the <code>onlearn</code> function.
</p>


<h3>Author(s)</h3>

<p>Alexandros Karatzoglou<br />
<a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>See Also</h3>

 <p><code><a href="#topic+onlearn">onlearn</a></code>, <code><a href="#topic+onlearn-class">onlearn-class</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>
## create toy data set
x &lt;- rbind(matrix(rnorm(100),,2),matrix(rnorm(100)+3,,2))
y &lt;- matrix(c(rep(1,50),rep(-1,50)),,1)

## initialize onlearn object
on &lt;- inlearn(2, kernel = "rbfdot", kpar = list(sigma = 0.2),
              type = "classification")

## learn one data point at the time
for(i in sample(1:100,100))
on &lt;- onlearn(on,x[i,],y[i],nu=0.03,lambda=0.1)

sign(predict(on,x))

</code></pre>

<hr>
<h2 id='ipop'>Quadratic Programming Solver</h2><span id='topic+ipop'></span><span id='topic+ipop+2CANY+2Cmatrix-method'></span>

<h3>Description</h3>

<p>ipop solves the quadratic programming problem :<br />
<code class="reqn">\min(c'*x + 1/2 * x' * H * x)</code><br />
subject to: <br />
<code class="reqn">b &lt;= A * x &lt;= b + r</code><br />
<code class="reqn">l &lt;= x &lt;= u</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ipop(c, H, A, b, l, u, r, sigf = 7, maxiter = 40, margin = 0.05,
     bound = 10, verb = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ipop_+3A_c">c</code></td>
<td>
<p>Vector or one column matrix appearing in the quadratic function</p>
</td></tr>
<tr><td><code id="ipop_+3A_h">H</code></td>
<td>
<p>square matrix appearing in the quadratic function, or the
decomposed form <code class="reqn">Z</code> of the <code class="reqn">H</code> matrix where <code class="reqn">Z</code> is a
<code class="reqn">n x m</code> matrix with <code class="reqn">n &gt; m</code> and <code class="reqn">ZZ' = H</code>.</p>
</td></tr>
<tr><td><code id="ipop_+3A_a">A</code></td>
<td>
<p>Matrix defining the constrains under which we minimize the
quadratic function</p>
</td></tr>
<tr><td><code id="ipop_+3A_b">b</code></td>
<td>
<p>Vector or one column matrix defining the constrains</p>
</td></tr>
<tr><td><code id="ipop_+3A_l">l</code></td>
<td>
<p>Lower bound vector or one column matrix</p>
</td></tr>
<tr><td><code id="ipop_+3A_u">u</code></td>
<td>
<p>Upper bound vector or one column matrix</p>
</td></tr>
<tr><td><code id="ipop_+3A_r">r</code></td>
<td>
<p>Vector or one column matrix defining constrains</p>
</td></tr>
<tr><td><code id="ipop_+3A_sigf">sigf</code></td>
<td>
<p>Precision (default: 7 significant figures)</p>
</td></tr>
<tr><td><code id="ipop_+3A_maxiter">maxiter</code></td>
<td>
<p>Maximum number of iterations</p>
</td></tr>
<tr><td><code id="ipop_+3A_margin">margin</code></td>
<td>
<p>how close we get to the constrains</p>
</td></tr>
<tr><td><code id="ipop_+3A_bound">bound</code></td>
<td>
<p>Clipping bound for the variables</p>
</td></tr>
<tr><td><code id="ipop_+3A_verb">verb</code></td>
<td>
<p>Display convergence information during runtime</p>
</td></tr>
</table>


<h3>Details</h3>

<p>ipop uses an interior point method to solve the quadratic programming
problem. <br />
The <code class="reqn">H</code> matrix can also be provided in the decomposed form <code class="reqn">Z</code>
where <code class="reqn">ZZ' = H</code> in that case the Sherman Morrison Woodbury formula
is used internally.
</p>


<h3>Value</h3>

<p>An S4 object with the following slots
</p>
<table>
<tr><td><code>primal</code></td>
<td>
<p>Vector containing the primal solution of the quadratic problem</p>
</td></tr>
<tr><td><code>dual</code></td>
<td>
<p>The dual solution of the problem</p>
</td></tr>
<tr><td><code>how</code></td>
<td>
<p>Character string describing the type of convergence</p>
</td></tr>
</table>
<p>all slots can be accessed through accessor functions (see example)
</p>


<h3>Author(s)</h3>

<p>Alexandros Karatzoglou (based on Matlab code by Alex Smola) <br />
<a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>References</h3>

<p>R. J. Vanderbei<br />
<em>LOQO: An interior point code for quadratic programming</em><br />
Optimization Methods and Software 11, 451-484, 1999 <br />
<a href="https://vanderbei.princeton.edu/ps/loqo5.pdf">https://vanderbei.princeton.edu/ps/loqo5.pdf</a>
</p>


<h3>See Also</h3>

<p><code>solve.QP</code>, <code><a href="#topic+inchol">inchol</a></code>, <code><a href="#topic+csi">csi</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## solve the Support Vector Machine optimization problem
data(spam)

## sample a scaled part (500 points) of the spam data set
m &lt;- 500
set &lt;- sample(1:dim(spam)[1],m)
x &lt;- scale(as.matrix(spam[,-58]))[set,]
y &lt;- as.integer(spam[set,58])
y[y==2] &lt;- -1

##set C parameter and kernel
C &lt;- 5
rbf &lt;- rbfdot(sigma = 0.1)

## create H matrix etc.
H &lt;- kernelPol(rbf,x,,y)
c &lt;- matrix(rep(-1,m))
A &lt;- t(y)
b &lt;- 0
l &lt;- matrix(rep(0,m))
u &lt;- matrix(rep(C,m))
r &lt;- 0

sv &lt;- ipop(c,H,A,b,l,u,r)
sv
dual(sv)

</code></pre>

<hr>
<h2 id='ipop-class'>Class &quot;ipop&quot;</h2><span id='topic+ipop-class'></span><span id='topic+primal+2Cipop-method'></span><span id='topic+dual+2Cipop-method'></span><span id='topic+how+2Cipop-method'></span><span id='topic+primal'></span><span id='topic+dual'></span><span id='topic+how'></span>

<h3>Description</h3>

<p>The quadratic problem solver class</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("ipop", ...)</code>.
or by calling the <code>ipop</code> function.
</p>


<h3>Slots</h3>


<dl>
<dt><code>primal</code>:</dt><dd><p>Object of class <code>"vector"</code> the primal
solution of the problem</p>
</dd>
<dt><code>dual</code>:</dt><dd><p>Object of class <code>"numeric"</code> the dual of the
problem</p>
</dd>
<dt><code>how</code>:</dt><dd><p>Object of class <code>"character"</code> convergence information</p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>primal</dt><dd><p>Object of class <code>ipop</code></p>
</dd></dl>
<p>Return the primal of the problem
</p>
<dl>
<dt>dual</dt><dd><p>Object of class <code>ipop</code></p>
</dd></dl>
<p>Return the dual of the problem
</p>
<dl>
<dt>how</dt><dd><p>Object of class <code>ipop</code></p>
</dd></dl>
<p>Return information on convergence

</p>


<h3>Author(s)</h3>

<p>Alexandros Karatzoglou<br />
<a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+ipop">ipop</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## solve the Support Vector Machine optimization problem
data(spam)

## sample a scaled part (300 points) of the spam data set
m &lt;- 300
set &lt;- sample(1:dim(spam)[1],m)
x &lt;- scale(as.matrix(spam[,-58]))[set,]
y &lt;- as.integer(spam[set,58])
y[y==2] &lt;- -1

##set C parameter and kernel
C &lt;- 5
rbf &lt;- rbfdot(sigma = 0.1)

## create H matrix etc.
H &lt;- kernelPol(rbf,x,,y)
c &lt;- matrix(rep(-1,m))
A &lt;- t(y)
b &lt;- 0
l &lt;- matrix(rep(0,m))
u &lt;- matrix(rep(C,m))
r &lt;- 0

sv &lt;- ipop(c,H,A,b,l,u,r)
primal(sv)
dual(sv)
how(sv)

</code></pre>

<hr>
<h2 id='kcca'>Kernel Canonical Correlation Analysis</h2><span id='topic+kcca'></span><span id='topic+kcca+2Cmatrix-method'></span>

<h3>Description</h3>

<p>Computes the canonical correlation analysis in feature space.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'matrix'
kcca(x, y, kernel="rbfdot", kpar=list(sigma=0.1),
gamma = 0.1, ncomps = 10, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kcca_+3A_x">x</code></td>
<td>
<p>a matrix containing data index by row</p>
</td></tr>
<tr><td><code id="kcca_+3A_y">y</code></td>
<td>
<p>a matrix containing data index by row</p>
</td></tr>
<tr><td><code id="kcca_+3A_kernel">kernel</code></td>
<td>
<p>the kernel function used in training and predicting.
This parameter can be set to any function, of class kernel,
which computes a inner product in feature space between two
vector arguments. kernlab provides the most popular kernel functions
which can be used by setting the kernel parameter to the following
strings:
</p>

<ul>
<li> <p><code>rbfdot</code> Radial Basis kernel function &quot;Gaussian&quot;
</p>
</li>
<li> <p><code>polydot</code> Polynomial kernel function
</p>
</li>
<li> <p><code>vanilladot</code> Linear kernel function
</p>
</li>
<li> <p><code>tanhdot</code> Hyperbolic tangent kernel function
</p>
</li>
<li> <p><code>laplacedot</code> Laplacian kernel function
</p>
</li>
<li> <p><code>besseldot</code> Bessel kernel function
</p>
</li>
<li> <p><code>anovadot</code> ANOVA RBF kernel function
</p>
</li>
<li> <p><code>splinedot</code> Spline kernel 
</p>
</li></ul>

<p>The kernel parameter can also be set to a user defined function of
class kernel by passing the function name as an argument.
</p>
</td></tr>
<tr><td><code id="kcca_+3A_kpar">kpar</code></td>
<td>
<p>the list of hyper-parameters (kernel parameters).
This is a list which contains the parameters to be used with the
kernel function. Valid parameters for existing kernels are :
</p>

<ul>
<li> <p><code>sigma</code> inverse kernel width for the Radial Basis
kernel function &quot;rbfdot&quot; and the Laplacian kernel &quot;laplacedot&quot;.
</p>
</li>
<li> <p><code>degree, scale, offset</code> for the Polynomial kernel &quot;polydot&quot;
</p>
</li>
<li> <p><code>scale, offset</code> for the Hyperbolic tangent kernel
function &quot;tanhdot&quot;
</p>
</li>
<li> <p><code>sigma, order, degree</code> for the Bessel kernel &quot;besseldot&quot;. 
</p>
</li>
<li> <p><code>sigma, degree</code> for the ANOVA kernel &quot;anovadot&quot;.
</p>
</li></ul>

<p>Hyper-parameters for user defined kernels can be passed through the
kpar parameter as well.</p>
</td></tr>
<tr><td><code id="kcca_+3A_gamma">gamma</code></td>
<td>
<p>regularization parameter (default : 0.1)</p>
</td></tr>
<tr><td><code id="kcca_+3A_ncomps">ncomps</code></td>
<td>
<p>number of canonical components (default : 10) </p>
</td></tr>
<tr><td><code id="kcca_+3A_...">...</code></td>
<td>
<p>additional parameters for the <code>kpca</code> function</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The kernel version of canonical correlation analysis.
Kernel Canonical Correlation Analysis (KCCA) is a non-linear extension
of CCA. Given two random variables, KCCA aims at extracting the
information which is shared by the two random variables. More
precisely given <code class="reqn">x</code> and <code class="reqn">y</code> the purpose of KCCA is to provide
nonlinear mappings <code class="reqn">f(x)</code> and <code class="reqn">g(y)</code> such that their
correlation is maximized.
</p>


<h3>Value</h3>

<p>An S4 object containing the following slots:
</p>
<table>
<tr><td><code>kcor</code></td>
<td>
<p>Correlation coefficients in feature space</p>
</td></tr>
<tr><td><code>xcoef</code></td>
<td>
<p>estimated coefficients for the <code>x</code> variables in the
feature space</p>
</td></tr>
<tr><td><code>ycoef</code></td>
<td>
<p>estimated coefficients for the <code>y</code> variables in the
feature space</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Alexandros Karatzoglou <br />
<a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a>
</p>


<h3>References</h3>

<p> Malte Kuss, Thore Graepel <br /> 
<em>The Geometry Of Kernel Canonical Correlation Analysis</em><br />
<a href="https://www.microsoft.com/en-us/research/publication/the-geometry-of-kernel-canonical-correlation-analysis/">https://www.microsoft.com/en-us/research/publication/the-geometry-of-kernel-canonical-correlation-analysis/</a></p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+cancor">cancor</a></code>, <code><a href="#topic+kpca">kpca</a></code>, <code><a href="#topic+kfa">kfa</a></code>, <code><a href="#topic+kha">kha</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
## dummy data
x &lt;- matrix(rnorm(30),15)
y &lt;- matrix(rnorm(30),15)

kcca(x,y,ncomps=2)

</code></pre>

<hr>
<h2 id='kcca-class'>Class &quot;kcca&quot;</h2><span id='topic+kcca-class'></span><span id='topic+kcor'></span><span id='topic+xcoef'></span><span id='topic+ycoef'></span><span id='topic+kcor+2Ckcca-method'></span><span id='topic+xcoef+2Ckcca-method'></span><span id='topic+xvar+2Ckcca-method'></span><span id='topic+ycoef+2Ckcca-method'></span><span id='topic+yvar+2Ckcca-method'></span>

<h3>Description</h3>

<p>The &quot;kcca&quot; class  </p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("kcca", ...)</code>.
or by the calling the <code>kcca</code> function. 
</p>


<h3>Slots</h3>


<dl>
<dt><code>kcor</code>:</dt><dd><p>Object of class <code>"vector"</code> describing the correlations</p>
</dd>
<dt><code>xcoef</code>:</dt><dd><p>Object of class <code>"matrix"</code> estimated coefficients for the <code>x</code> variables</p>
</dd>
<dt><code>ycoef</code>:</dt><dd><p>Object of class <code>"matrix"</code> estimated coefficients for the <code>y</code> variables </p>
</dd>




</dl>



<h3>Methods</h3>


<dl>
<dt>kcor</dt><dd><p><code>signature(object = "kcca")</code>: returns the correlations</p>
</dd>
<dt>xcoef</dt><dd><p><code>signature(object = "kcca")</code>: returns the estimated coefficients for the <code>x</code> variables</p>
</dd>
<dt>ycoef</dt><dd><p><code>signature(object = "kcca")</code>: returns the estimated coefficients for the <code>y</code> variables </p>
</dd>




</dl>



<h3>Author(s)</h3>

<p>Alexandros Karatzoglou <br />
<a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+kcca">kcca</a></code>,
<code><a href="#topic+kpca-class">kpca-class</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## dummy data
x &lt;- matrix(rnorm(30),15)
y &lt;- matrix(rnorm(30),15)

kcca(x,y,ncomps=2)

</code></pre>

<hr>
<h2 id='kernel-class'>Class &quot;kernel&quot; &quot;rbfkernel&quot; &quot;polykernel&quot;, &quot;tanhkernel&quot;, &quot;vanillakernel&quot;</h2><span id='topic+rbfkernel-class'></span><span id='topic+polykernel-class'></span><span id='topic+vanillakernel-class'></span><span id='topic+tanhkernel-class'></span><span id='topic+anovakernel-class'></span><span id='topic+besselkernel-class'></span><span id='topic+laplacekernel-class'></span><span id='topic+splinekernel-class'></span><span id='topic+stringkernel-class'></span><span id='topic+fourierkernel-class'></span><span id='topic+kfunction-class'></span><span id='topic+kernel-class'></span><span id='topic+kpar+2Ckernel-method'></span>

<h3>Description</h3>

<p>  The built-in kernel classes in <span class="pkg">kernlab</span></p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("rbfkernel")</code>,
<code>new{"polykernel"}</code>, <code>new{"tanhkernel"}</code>,
<code>new{"vanillakernel"}</code>, <code>new{"anovakernel"}</code>,
<code>new{"besselkernel"}</code>, <code>new{"laplacekernel"}</code>,
<code>new{"splinekernel"}</code>, <code>new{"stringkernel"}</code>
</p>
<p>or by calling the <code>rbfdot</code>, <code>polydot</code>, <code>tanhdot</code>,
<code>vanilladot</code>, <code>anovadot</code>, <code>besseldot</code>, <code>laplacedot</code>,
<code>splinedot</code>, <code>stringdot</code> functions etc..
</p>


<h3>Slots</h3>


<dl>
<dt><code>.Data</code>:</dt><dd><p>Object of class <code>"function"</code> containing
the kernel function </p>
</dd>
<dt><code>kpar</code>:</dt><dd><p>Object of class <code>"list"</code> containing the
kernel parameters </p>
</dd>
</dl>



<h3>Extends</h3>

<p>Class <code>"kernel"</code>, directly.
Class <code>"function"</code>, by class <code>"kernel"</code>.
</p>


<h3>Methods</h3>


<dl>
<dt>kernelMatrix</dt><dd><p><code>signature(kernel = "rbfkernel", x =
	"matrix")</code>: computes the kernel matrix</p>
</dd>
<dt>kernelMult</dt><dd><p><code>signature(kernel = "rbfkernel", x =
	"matrix")</code>: computes the quadratic kernel expression</p>
</dd>
<dt>kernelPol</dt><dd><p><code>signature(kernel = "rbfkernel", x =
	"matrix")</code>: computes the kernel expansion</p>
</dd>
<dt>kernelFast</dt><dd><p><code>signature(kernel = "rbfkernel", x =
	"matrix"),,a</code>: computes parts or the full kernel matrix, mainly
used in kernel algorithms where columns of the kernel matrix are
computed per invocation </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Alexandros Karatzoglou<br /> <a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a> </p>


<h3>See Also</h3>

<p><code><a href="#topic+dots">dots</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
rbfkernel &lt;- rbfdot(sigma = 0.1)
rbfkernel
is(rbfkernel)
kpar(rbfkernel)

</code></pre>

<hr>
<h2 id='kernelMatrix'>Kernel Matrix functions</h2><span id='topic+kernelMatrix'></span><span id='topic+kernelMult'></span><span id='topic+kernelPol'></span><span id='topic+kernelFast'></span><span id='topic+kernelPol+2Ckernel-method'></span><span id='topic+kernelMatrix+2Ckernel-method'></span><span id='topic+kernelMult+2Ckernel-method'></span><span id='topic+kernelFast+2Ckernel-method'></span><span id='topic+kernelMatrix+2Crbfkernel-method'></span><span id='topic+kernelMatrix+2Cpolykernel-method'></span><span id='topic+kernelMatrix+2Cvanillakernel-method'></span><span id='topic+kernelMatrix+2Ctanhkernel-method'></span><span id='topic+kernelMatrix+2Claplacekernel-method'></span><span id='topic+kernelMatrix+2Canovakernel-method'></span><span id='topic+kernelMatrix+2Csplinekernel-method'></span><span id='topic+kernelMatrix+2Cbesselkernel-method'></span><span id='topic+kernelMatrix+2Cstringkernel-method'></span><span id='topic+kernelMult+2Crbfkernel+2CANY-method'></span><span id='topic+kernelMult+2Csplinekernel+2CANY-method'></span><span id='topic+kernelMult+2Cpolykernel+2CANY-method'></span><span id='topic+kernelMult+2Ctanhkernel+2CANY-method'></span><span id='topic+kernelMult+2Claplacekernel+2CANY-method'></span><span id='topic+kernelMult+2Cbesselkernel+2CANY-method'></span><span id='topic+kernelMult+2Canovakernel+2CANY-method'></span><span id='topic+kernelMult+2Cvanillakernel+2CANY-method'></span><span id='topic+kernelMult+2Ccharacter+2CkernelMatrix-method'></span><span id='topic+kernelMult+2Cstringkernel+2CANY-method'></span><span id='topic+kernelPol+2Crbfkernel-method'></span><span id='topic+kernelPol+2Csplinekernel-method'></span><span id='topic+kernelPol+2Cpolykernel-method'></span><span id='topic+kernelPol+2Ctanhkernel-method'></span><span id='topic+kernelPol+2Cvanillakernel-method'></span><span id='topic+kernelPol+2Canovakernel-method'></span><span id='topic+kernelPol+2Cbesselkernel-method'></span><span id='topic+kernelPol+2Claplacekernel-method'></span><span id='topic+kernelPol+2Cstringkernel-method'></span><span id='topic+kernelFast+2Crbfkernel-method'></span><span id='topic+kernelFast+2Csplinekernel-method'></span><span id='topic+kernelFast+2Cpolykernel-method'></span><span id='topic+kernelFast+2Ctanhkernel-method'></span><span id='topic+kernelFast+2Cvanillakernel-method'></span><span id='topic+kernelFast+2Canovakernel-method'></span><span id='topic+kernelFast+2Cbesselkernel-method'></span><span id='topic+kernelFast+2Claplacekernel-method'></span><span id='topic+kernelFast+2Cstringkernel-method'></span><span id='topic+kernelFast+2Csplinekernel-method'></span>

<h3>Description</h3>

<p><code>kernelMatrix</code> calculates the kernel matrix <code class="reqn">K_{ij} = k(x_i,x_j)</code> or <code class="reqn">K_{ij} =
    k(x_i,y_j)</code>.<br />
<code>kernelPol</code> computes the quadratic kernel expression  <code class="reqn">H = z_i z_j
    k(x_i,x_j)</code>, <code class="reqn">H = z_i k_j k(x_i,y_j)</code>.<br />
<code>kernelMult</code> calculates the kernel expansion <code class="reqn">f(x_i) =
      \sum_{i=1}^m z_i  k(x_i,x_j)</code><br />
<code>kernelFast</code> computes the kernel matrix, identical
to <code>kernelMatrix</code>, except that it also requires the squared
norm of the first argument as additional input, useful in iterative
kernel matrix calculations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'kernel'
kernelMatrix(kernel, x, y = NULL)

## S4 method for signature 'kernel'
kernelPol(kernel, x, y = NULL, z, k = NULL)

## S4 method for signature 'kernel'
kernelMult(kernel, x, y = NULL, z, blocksize = 256)

## S4 method for signature 'kernel'
kernelFast(kernel, x, y, a)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kernelMatrix_+3A_kernel">kernel</code></td>
<td>
<p>the kernel function to be used to calculate the kernel
matrix.
This has to be a function of class <code>kernel</code>, i.e. which can be
generated either one of the build in 
kernel generating functions (e.g., <code>rbfdot</code> etc.) or a user defined
function of class <code>kernel</code> taking two vector arguments and returning a scalar.</p>
</td></tr>
<tr><td><code id="kernelMatrix_+3A_x">x</code></td>
<td>
<p>a data matrix to be used to calculate the kernel matrix, or a
list of vector when a <code>stringkernel</code> is used</p>
</td></tr>
<tr><td><code id="kernelMatrix_+3A_y">y</code></td>
<td>
<p>second data matrix to calculate the kernel matrix, or a
list of vector when a <code>stringkernel</code> is used</p>
</td></tr>
<tr><td><code id="kernelMatrix_+3A_z">z</code></td>
<td>
<p>a suitable vector or matrix</p>
</td></tr>
<tr><td><code id="kernelMatrix_+3A_k">k</code></td>
<td>
<p>a suitable vector or matrix</p>
</td></tr>
<tr><td><code id="kernelMatrix_+3A_a">a</code></td>
<td>
<p>the squared norm of <code>x</code>, e.g., <code>rowSums(x^2)</code></p>
</td></tr>
<tr><td><code id="kernelMatrix_+3A_blocksize">blocksize</code></td>
<td>
<p>the kernel expansion computations are done block wise
to avoid storing the kernel matrix into memory. <code>blocksize</code>
defines the size of the computational blocks.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Common functions used during kernel based computations.<br />
The <code>kernel</code> parameter can be set to any function, of class
kernel, which computes the inner product in feature space between two
vector arguments. <span class="pkg">kernlab</span> provides the most popular kernel functions
which can be initialized by using the following
functions:
</p>

<ul>
<li> <p><code>rbfdot</code> Radial Basis kernel function
</p>
</li>
<li> <p><code>polydot</code> Polynomial kernel function
</p>
</li>
<li> <p><code>vanilladot</code> Linear kernel function
</p>
</li>
<li> <p><code>tanhdot</code> Hyperbolic tangent kernel function
</p>
</li>
<li> <p><code>laplacedot</code> Laplacian kernel function
</p>
</li>
<li> <p><code>besseldot</code> Bessel kernel function
</p>
</li>
<li> <p><code>anovadot</code> ANOVA RBF kernel function
</p>
</li>
<li> <p><code>splinedot</code> the Spline kernel 
</p>
</li></ul>
<p>  (see example.)
</p>
<p><code>kernelFast</code> is mainly used in situations where columns of the
kernel matrix are computed per invocation. In these cases,
evaluating the norm of each row-entry over and over again would
cause significant computational overhead.
</p>


<h3>Value</h3>

<p><code>kernelMatrix</code> returns a symmetric diagonal semi-definite matrix.<br />
<code>kernelPol</code> returns a matrix.<br />
<code>kernelMult</code> usually returns a one-column matrix.
</p>


<h3>Author(s)</h3>

<p>Alexandros Karatzoglou <br />
<a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+rbfdot">rbfdot</a></code>, <code><a href="#topic+polydot">polydot</a></code>,
<code><a href="#topic+tanhdot">tanhdot</a></code>, <code><a href="#topic+vanilladot">vanilladot</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## use the spam data
data(spam)
dt &lt;- as.matrix(spam[c(10:20,3000:3010),-58])

## initialize kernel function 
rbf &lt;- rbfdot(sigma = 0.05)
rbf

## calculate kernel matrix
kernelMatrix(rbf, dt)

yt &lt;- as.matrix(as.integer(spam[c(10:20,3000:3010),58]))
yt[yt==2] &lt;- -1

## calculate the quadratic kernel expression
kernelPol(rbf, dt, ,yt)

## calculate the kernel expansion
kernelMult(rbf, dt, ,yt)
</code></pre>

<hr>
<h2 id='kfa'>Kernel Feature Analysis</h2><span id='topic+kfa'></span><span id='topic+kfa+2Cformula-method'></span><span id='topic+kfa+2Cmatrix-method'></span><span id='topic+show+2Ckfa-method'></span><span id='topic+coef+2Ckfa-method'></span>

<h3>Description</h3>

<p>The Kernel Feature Analysis algorithm is an algorithm for extracting
structure from possibly high-dimensional data sets.
Similar to <code>kpca</code> a new basis for the data is found.
The data can then be projected on the new basis. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'formula'
kfa(x, data = NULL, na.action = na.omit, ...)

## S4 method for signature 'matrix'
kfa(x, kernel = "rbfdot", kpar = list(sigma = 0.1),
   features = 0, subset = 59, normalize = TRUE, na.action = na.omit)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kfa_+3A_x">x</code></td>
<td>
<p> The data matrix indexed by row or a formula
describing the model. Note, that an intercept is always
included, whether given in the formula or not.</p>
</td></tr>
<tr><td><code id="kfa_+3A_data">data</code></td>
<td>
<p>an optional data frame containing the variables in
the model
(when using a formula).</p>
</td></tr>
<tr><td><code id="kfa_+3A_kernel">kernel</code></td>
<td>
<p>the kernel function used in training and predicting.
This parameter can be set to any function, of class kernel, which
computes an inner product in feature space between two
vector arguments. <span class="pkg">kernlab</span> provides the most popular kernel functions
which can be used by setting the kernel parameter to the following
strings:
</p>

<ul>
<li> <p><code>rbfdot</code> Radial Basis kernel function &quot;Gaussian&quot;
</p>
</li>
<li> <p><code>polydot</code> Polynomial kernel function
</p>
</li>
<li> <p><code>vanilladot</code> Linear kernel function
</p>
</li>
<li> <p><code>tanhdot</code> Hyperbolic tangent kernel function
</p>
</li>
<li> <p><code>laplacedot</code> Laplacian kernel function
</p>
</li>
<li> <p><code>besseldot</code> Bessel kernel function
</p>
</li>
<li> <p><code>anovadot</code> ANOVA RBF kernel function
</p>
</li>
<li> <p><code>splinedot</code> Spline kernel 
</p>
</li></ul>

<p>The kernel parameter can also be set to a user defined function of
class kernel by passing the function name as an argument.
</p>
</td></tr>
<tr><td><code id="kfa_+3A_kpar">kpar</code></td>
<td>
<p>the list of hyper-parameters (kernel parameters).
This is a list which contains the parameters to be used with the
kernel function. Valid parameters for existing kernels are :
</p>

<ul>
<li> <p><code>sigma</code> inverse kernel width for the Radial Basis
kernel function &quot;rbfdot&quot; and the Laplacian kernel &quot;laplacedot&quot;.
</p>
</li>
<li> <p><code>degree, scale, offset</code> for the Polynomial kernel &quot;polydot&quot;
</p>
</li>
<li> <p><code>scale, offset</code> for the Hyperbolic tangent kernel
function &quot;tanhdot&quot;
</p>
</li>
<li> <p><code>sigma, order, degree</code> for the Bessel kernel &quot;besseldot&quot;.
</p>
</li>
<li> <p><code>sigma, degree</code> for the ANOVA kernel &quot;anovadot&quot;.
</p>
</li></ul>

<p>Hyper-parameters for user defined kernels can be passed through the
kpar parameter as well.</p>
</td></tr>
<tr><td><code id="kfa_+3A_features">features</code></td>
<td>
<p>Number of features (principal components) to
return. (default: 0 , all)</p>
</td></tr>
<tr><td><code id="kfa_+3A_subset">subset</code></td>
<td>
<p>the number of features sampled (used) from the data set</p>
</td></tr>
<tr><td><code id="kfa_+3A_normalize">normalize</code></td>
<td>
<p>normalize the feature selected (default: TRUE)</p>
</td></tr>  
<tr><td><code id="kfa_+3A_na.action">na.action</code></td>
<td>
<p>A function to specify the action to be taken if <code>NA</code>s are
found. The default action is <code>na.omit</code>, which leads to rejection of cases
with missing values on any required variable. An alternative
is <code>na.fail</code>, which causes an error if <code>NA</code> cases
are found. (NOTE: If given, this argument must be named.)</p>
</td></tr>
<tr><td><code id="kfa_+3A_...">...</code></td>
<td>
<p> additional parameters</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Kernel Feature analysis is similar to Kernel PCA, but instead of
extracting eigenvectors of the training dataset in feature space, it
approximates the eigenvectors by selecting training patterns which are good
basis vectors for the training set. It works by choosing a fixed size
subset of the data set and scaling it to unit length (under the kernel).
It then chooses the features that maximize the value of the inner
product (kernel function) with the rest of the patterns.
</p>


<h3>Value</h3>

<p><code>kfa</code> returns an object of class <code>kfa</code> containing the
features selected by the algorithm. 
</p>
<table>
<tr><td><code>xmatrix</code></td>
<td>
<p>contains the features selected</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>contains the sparse alpha vector</p>
</td></tr>
</table>
<p>The <code>predict</code> function can be used to embed new data points into to the
selected feature base.
</p>


<h3>Author(s)</h3>

<p>Alexandros Karatzoglou<br />
<a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>References</h3>

<p>Alex J. Smola, Olvi L. Mangasarian and Bernhard Schoelkopf<br />
<em>Sparse Kernel Feature Analysis</em><br />
Data Mining Institute Technical Report 99-04, October 1999<br />
<a href="ftp://ftp.cs.wisc.edu/pub/dmi/tech-reports/99-04.ps">ftp://ftp.cs.wisc.edu/pub/dmi/tech-reports/99-04.ps</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kpca">kpca</a></code>, <code><a href="#topic+kfa-class">kfa-class</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data(promotergene)
f &lt;- kfa(~.,data=promotergene,features=2,kernel="rbfdot",
         kpar=list(sigma=0.01))
plot(predict(f,promotergene),col=as.numeric(promotergene[,1]))
</code></pre>

<hr>
<h2 id='kfa-class'>Class &quot;kfa&quot;</h2><span id='topic+kfa-class'></span><span id='topic+alpha+2Ckfa-method'></span><span id='topic+alphaindex+2Ckfa-method'></span><span id='topic+kcall+2Ckfa-method'></span><span id='topic+kernelf+2Ckfa-method'></span><span id='topic+predict+2Ckfa-method'></span><span id='topic+xmatrix+2Ckfa-method'></span>

<h3>Description</h3>

<p>The class of the object returned by the Kernel Feature
Analysis <code>kfa</code> function</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("kfa", ...)</code> or by
calling the <code>kfa</code> method. The objects contain the features along with the
alpha values. 
</p>


<h3>Slots</h3>


<dl>
<dt><code>alpha</code>:</dt><dd><p>Object of class <code>"matrix"</code> containing the
alpha values </p>
</dd>
<dt><code>alphaindex</code>:</dt><dd><p>Object of class <code>"vector"</code> containing
the indexes of the selected feature</p>
</dd>
<dt><code>kernelf</code>:</dt><dd><p>Object of class <code>"kfunction"</code> containing
the kernel function used</p>
</dd>
<dt><code>xmatrix</code>:</dt><dd><p>Object of class <code>"matrix"</code> containing
the selected features</p>
</dd>
<dt><code>kcall</code>:</dt><dd><p>Object of class <code>"call"</code> containing the
<code>kfa</code> function call</p>
</dd>
<dt><code>terms</code>:</dt><dd><p>Object of class <code>"ANY"</code> containing the
formula terms</p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>alpha</dt><dd><p><code>signature(object = "kfa")</code>: returns the alpha values </p>
</dd>
<dt>alphaindex</dt><dd><p><code>signature(object = "kfa")</code>: returns the
index of the selected features</p>
</dd>
<dt>kcall</dt><dd><p><code>signature(object = "kfa")</code>: returns the function call </p>
</dd>
<dt>kernelf</dt><dd><p><code>signature(object = "kfa")</code>: returns the kernel
function used </p>
</dd>
<dt>predict</dt><dd><p><code>signature(object = "kfa")</code>: used to embed more
data points to the feature base</p>
</dd>
<dt>xmatrix</dt><dd><p><code>signature(object = "kfa")</code>: returns the
selected features. </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Alexandros Karatzoglou<br />
<a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+kfa">kfa</a></code>, <code><a href="#topic+kpca-class">kpca-class</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(promotergene)
f &lt;- kfa(~.,data=promotergene)
</code></pre>

<hr>
<h2 id='kha'>Kernel Principal Components Analysis</h2><span id='topic+kha'></span><span id='topic+kha+2Cformula-method'></span><span id='topic+kha+2Cmatrix-method'></span><span id='topic+predict+2Ckha-method'></span>

<h3>Description</h3>

<p>Kernel Hebbian Algorithm is a nonlinear iterative algorithm for principal
component analysis.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'formula'
kha(x, data = NULL, na.action, ...)

## S4 method for signature 'matrix'
kha(x, kernel = "rbfdot", kpar = list(sigma = 0.1), features = 5, 
         eta = 0.005, th = 1e-4, maxiter = 10000, verbose = FALSE,
        na.action = na.omit, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kha_+3A_x">x</code></td>
<td>
<p> The data matrix indexed by row
or a formula describing the model. Note, that an
intercept is always included, whether given in the formula or
not.</p>
</td></tr> 
<tr><td><code id="kha_+3A_data">data</code></td>
<td>
<p>an optional data frame containing the variables in
the model
(when using a formula).</p>
</td></tr>
<tr><td><code id="kha_+3A_kernel">kernel</code></td>
<td>
<p>the kernel function used in training and predicting.
This parameter can be set to any function, of class kernel, which
computes the inner product in feature space between two
vector arguments (see <code><a href="#topic+kernels">kernels</a></code>).
<span class="pkg">kernlab</span> provides the most popular kernel functions
which can be used by setting the kernel parameter to the following
strings:
</p>

<ul>
<li> <p><code>rbfdot</code> Radial Basis kernel function &quot;Gaussian&quot;
</p>
</li>
<li> <p><code>polydot</code> Polynomial kernel function
</p>
</li>
<li> <p><code>vanilladot</code> Linear kernel function
</p>
</li>
<li> <p><code>tanhdot</code> Hyperbolic tangent kernel function
</p>
</li>
<li> <p><code>laplacedot</code> Laplacian kernel function
</p>
</li>
<li> <p><code>besseldot</code> Bessel kernel function
</p>
</li>
<li> <p><code>anovadot</code> ANOVA RBF kernel function
</p>
</li>
<li> <p><code>splinedot</code> Spline kernel 
</p>
</li></ul>

<p>The kernel parameter can also be set to a user defined function of
class kernel by passing the function name as an argument.
</p>
</td></tr>
<tr><td><code id="kha_+3A_kpar">kpar</code></td>
<td>
<p>the list of hyper-parameters (kernel parameters).
This is a list which contains the parameters to be used with the
kernel function. Valid parameters for existing kernels are :
</p>

<ul>
<li> <p><code>sigma</code> inverse kernel width for the Radial Basis
kernel function &quot;rbfdot&quot; and the Laplacian kernel &quot;laplacedot&quot;.
</p>
</li>
<li> <p><code>degree, scale, offset</code> for the Polynomial kernel &quot;polydot&quot;
</p>
</li>
<li> <p><code>scale, offset</code> for the Hyperbolic tangent kernel
function &quot;tanhdot&quot;
</p>
</li>
<li> <p><code>sigma, order, degree</code> for the Bessel kernel &quot;besseldot&quot;. 
</p>
</li>
<li> <p><code>sigma, degree</code> for the ANOVA kernel &quot;anovadot&quot;.
</p>
</li></ul>

<p>Hyper-parameters for user defined kernels can be passed through the
kpar parameter as well.</p>
</td></tr>
<tr><td><code id="kha_+3A_features">features</code></td>
<td>
<p>Number of features (principal components) to
return. (default: 5)</p>
</td></tr>
<tr><td><code id="kha_+3A_eta">eta</code></td>
<td>
<p>The hebbian learning rate (default : 0.005)</p>
</td></tr>
<tr><td><code id="kha_+3A_th">th</code></td>
<td>
<p>the smallest value of the convergence step (default : 0.0001) </p>
</td></tr>
<tr><td><code id="kha_+3A_maxiter">maxiter</code></td>
<td>
<p>the maximum number of iterations.</p>
</td></tr>
<tr><td><code id="kha_+3A_verbose">verbose</code></td>
<td>
<p>print convergence every 100 iterations. (default : FALSE)</p>
</td></tr>
<tr><td><code id="kha_+3A_na.action">na.action</code></td>
<td>
<p>A function to specify the action to be taken if <code>NA</code>s are
found. The default action is <code>na.omit</code>, which leads to rejection of cases
with missing values on any required variable. An alternative
is <code>na.fail</code>, which causes an error if <code>NA</code> cases
are found. (NOTE: If given, this argument must be named.)</p>
</td></tr>
<tr><td><code id="kha_+3A_...">...</code></td>
<td>
<p> additional parameters</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The original form of KPCA can only be used on small data sets
since it requires the estimation of the eigenvectors of a full kernel
matrix. The Kernel Hebbian Algorithm iteratively estimates the Kernel
Principal Components with only linear order memory complexity.
(see ref. for more details)
</p>


<h3>Value</h3>

<p>An S4 object containing the principal component vectors along with the
corresponding normalization values. 
</p>
<table>
<tr><td><code>pcv</code></td>
<td>
<p>a matrix containing the principal component vectors (column
wise)</p>
</td></tr>
<tr><td><code>eig</code></td>
<td>
<p>The normalization values</p>
</td></tr>
<tr><td><code>xmatrix</code></td>
<td>
<p>The original data matrix</p>
</td></tr>
</table>
<p>all the slots of the object can be accessed by accessor functions.
</p>


<h3>Note</h3>

<p>The predict function can be used to embed new data on the new space</p>


<h3>Author(s)</h3>

<p>Alexandros Karatzoglou <br />
<a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>References</h3>

<p>Kwang In Kim, M.O. Franz and B. Schölkopf<br />
<em>Kernel Hebbian Algorithm for Iterative Kernel Principal Component Analysis</em><br />
Max-Planck-Institut für biologische Kybernetik, Tübingen (109)<br />
<a href="https://is.mpg.de/fileadmin/user_upload/files/publications/pdf2302.pdf">https://is.mpg.de/fileadmin/user_upload/files/publications/pdf2302.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kpca">kpca</a></code>, <code><a href="#topic+kfa">kfa</a></code>, <code><a href="#topic+kcca">kcca</a></code>, <code>pca</code></p>


<h3>Examples</h3>

<pre><code class='language-R'># another example using the iris
data(iris)
test &lt;- sample(1:150,70)

kpc &lt;- kha(~.,data=iris[-test,-5],kernel="rbfdot",
           kpar=list(sigma=0.2),features=2, eta=0.001, maxiter=65)

#print the principal component vectors
pcv(kpc)

#plot the data projection on the components
plot(predict(kpc,iris[,-5]),col=as.integer(iris[,5]),
     xlab="1st Principal Component",ylab="2nd Principal Component")

</code></pre>

<hr>
<h2 id='kha-class'>Class &quot;kha&quot;</h2><span id='topic+kha-class'></span><span id='topic+eig+2Ckha-method'></span><span id='topic+kcall+2Ckha-method'></span><span id='topic+kernelf+2Ckha-method'></span><span id='topic+pcv+2Ckha-method'></span><span id='topic+xmatrix+2Ckha-method'></span><span id='topic+eskm+2Ckha-method'></span>

<h3>Description</h3>

<p> The Kernel Hebbian Algorithm class</p>


<h3>Objects objects of class &quot;kha&quot;</h3>

<p>Objects can be created by calls of the form <code>new("kha", ...)</code>.
or by calling the <code>kha</code> function.
</p>


<h3>Slots</h3>


<dl>
<dt><code>pcv</code>:</dt><dd><p>Object of class <code>"matrix"</code> containing the
principal component vectors </p>
</dd>
<dt><code>eig</code>:</dt><dd><p>Object of class <code>"vector"</code> containing the
corresponding normalization values</p>
</dd>
<dt><code>eskm</code>:</dt><dd><p>Object of class <code>"vector"</code> containing the
kernel sum</p>
</dd>
<dt><code>kernelf</code>:</dt><dd><p>Object of class <code>"kfunction"</code> containing
the kernel function used</p>
</dd>
<dt><code>kpar</code>:</dt><dd><p>Object of class <code>"list"</code> containing the
kernel parameters used </p>
</dd>
<dt><code>xmatrix</code>:</dt><dd><p>Object of class <code>"matrix"</code> containing
the data matrix used </p>
</dd>
<dt><code>kcall</code>:</dt><dd><p>Object of class <code>"ANY"</code> containing the
function call </p>
</dd>
<dt><code>n.action</code>:</dt><dd><p>Object of class <code>"ANY"</code> containing the
action performed on NA </p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>eig</dt><dd><p><code>signature(object = "kha")</code>: returns the
normalization values </p>
</dd>
<dt>kcall</dt><dd><p><code>signature(object = "kha")</code>: returns the
performed call</p>
</dd>
<dt>kernelf</dt><dd><p><code>signature(object = "kha")</code>: returns the used
kernel function</p>
</dd>
<dt>pcv</dt><dd><p><code>signature(object = "kha")</code>: returns the principal
component vectors </p>
</dd>
<dt>eskm</dt><dd><p><code>signature(object = "kha")</code>: returns the kernel sum</p>
</dd>
<dt>predict</dt><dd><p><code>signature(object = "kha")</code>: embeds new data </p>
</dd>
<dt>xmatrix</dt><dd><p><code>signature(object = "kha")</code>: returns the used
data matrix </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Alexandros Karatzoglou<br /> <a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>See Also</h3>

 <p><code><a href="#topic+kha">kha</a></code>,
<code><a href="#topic+ksvm-class">ksvm-class</a></code>, 
<code><a href="#topic+kcca-class">kcca-class</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'># another example using the iris
data(iris)
test &lt;- sample(1:50,20)

kpc &lt;- kha(~.,data=iris[-test,-5], kernel="rbfdot",
           kpar=list(sigma=0.2),features=2, eta=0.001, maxiter=65)

#print the principal component vectors
pcv(kpc)
kernelf(kpc)
eig(kpc)
</code></pre>

<hr>
<h2 id='kkmeans'>Kernel k-means</h2><span id='topic+kkmeans'></span><span id='topic+kkmeans+2Cmatrix-method'></span><span id='topic+kkmeans+2Cformula-method'></span><span id='topic+kkmeans+2Clist-method'></span><span id='topic+kkmeans+2CkernelMatrix-method'></span>

<h3>Description</h3>

<p>A weighted kernel version of the famous k-means algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
## S4 method for signature 'formula'
kkmeans(x, data = NULL, na.action = na.omit, ...)

## S4 method for signature 'matrix'
kkmeans(x, centers, kernel = "rbfdot", kpar = "automatic",
        alg="kkmeans", p=1, na.action = na.omit, ...)

## S4 method for signature 'kernelMatrix'
kkmeans(x, centers, ...)

## S4 method for signature 'list'
kkmeans(x, centers, kernel = "stringdot",
        kpar = list(length=4, lambda=0.5),
        alg ="kkmeans", p = 1, na.action = na.omit, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kkmeans_+3A_x">x</code></td>
<td>
<p>the matrix of data to be clustered, or a symbolic
description of the model to be fit, or a kernel Matrix of class
<code>kernelMatrix</code>, or a list of character vectors.</p>
</td></tr>
<tr><td><code id="kkmeans_+3A_data">data</code></td>
<td>
<p>an optional data frame containing the variables in the model.
By default the variables are taken from the environment which
&lsquo;kkmeans&rsquo; is called from.</p>
</td></tr>
<tr><td><code id="kkmeans_+3A_centers">centers</code></td>
<td>
<p>Either the number of clusters or a matrix of initial cluster
centers. If the first a random initial partitioning is used.</p>
</td></tr>
<tr><td><code id="kkmeans_+3A_kernel">kernel</code></td>
<td>
<p>the kernel function used in training and predicting.
This parameter can be set to any function, of class kernel, which
computes a inner product in feature space between two
vector arguments (see <code>link{kernels}</code>). <span class="pkg">kernlab</span> provides the most popular kernel functions
which can be used by setting the kernel parameter to the following
strings:
</p>

<ul>
<li> <p><code>rbfdot</code> Radial Basis kernel  &quot;Gaussian&quot;
</p>
</li>
<li> <p><code>polydot</code> Polynomial kernel 
</p>
</li>
<li> <p><code>vanilladot</code> Linear kernel 
</p>
</li>
<li> <p><code>tanhdot</code> Hyperbolic tangent kernel 
</p>
</li>
<li> <p><code>laplacedot</code> Laplacian kernel 
</p>
</li>
<li> <p><code>besseldot</code> Bessel kernel 
</p>
</li>
<li> <p><code>anovadot</code> ANOVA RBF kernel 
</p>
</li>
<li> <p><code>splinedot</code> Spline kernel
</p>
</li>
<li> <p><code>stringdot</code> String kernel
</p>
</li></ul>

<p>Setting the kernel parameter to &quot;matrix&quot; treats <code>x</code> as a kernel
matrix calling the <code>kernelMatrix</code> interface.<br />
</p>
<p>The kernel parameter can also be set to a user defined function of
class kernel by passing the function name as an argument.
</p>
</td></tr>
<tr><td><code id="kkmeans_+3A_kpar">kpar</code></td>
<td>
<p>a character string or the list of hyper-parameters (kernel parameters).
The default character string <code>"automatic"</code> uses a heuristic the determine a
suitable value for the width parameter of the RBF kernel.<br />
</p>
<p>A list can also be used containing the parameters to be used with the
kernel function. Valid parameters for existing kernels are :
</p>

<ul>
<li> <p><code>sigma</code> inverse kernel width for the Radial Basis
kernel function &quot;rbfdot&quot; and the Laplacian kernel &quot;laplacedot&quot;.
</p>
</li>
<li> <p><code>degree, scale, offset</code> for the Polynomial kernel &quot;polydot&quot;
</p>
</li>
<li> <p><code>scale, offset</code> for the Hyperbolic tangent kernel
function &quot;tanhdot&quot;
</p>
</li>
<li> <p><code>sigma, order, degree</code> for the Bessel kernel &quot;besseldot&quot;. 
</p>
</li>
<li> <p><code>sigma, degree</code> for the ANOVA kernel &quot;anovadot&quot;.
</p>
</li>
<li> <p><code>length, lambda, normalized</code> for the &quot;stringdot&quot; kernel
where length is the length of the strings considered, lambda the
decay factor and normalized a logical parameter determining if the
kernel evaluations should be normalized.
</p>
</li></ul>

<p>Hyper-parameters for user defined kernels can be passed through the
kpar parameter as well.</p>
</td></tr>
<tr><td><code id="kkmeans_+3A_alg">alg</code></td>
<td>
<p>the algorithm to use. Options currently include
<code>kkmeans</code> and <code>kerninghan</code>. </p>
</td></tr>
<tr><td><code id="kkmeans_+3A_p">p</code></td>
<td>
<p>a parameter used to keep the affinity matrix positive semidefinite</p>
</td></tr>
<tr><td><code id="kkmeans_+3A_na.action">na.action</code></td>
<td>
<p>The action to perform on NA</p>
</td></tr>
<tr><td><code id="kkmeans_+3A_...">...</code></td>
<td>
<p>additional parameters</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>kernel k-means</code> uses the 'kernel trick' (i.e. implicitly projecting all data
into a non-linear feature space with the use of a kernel) in order to
deal with one of the major drawbacks of <code>k-means</code> that is that it cannot
capture clusters that are not linearly separable in input space.  <br />
The algorithm is implemented using the triangle inequality to avoid
unnecessary and computational expensive distance calculations.
This leads to significant speedup particularly on large data sets with
a high number of clusters. <br />
With a particular choice of weights this algorithm becomes
equivalent to Kernighan-Lin, and the norm-cut graph partitioning
algorithms. <br />
The function also support input in the form of a kernel matrix
or a list of characters for text clustering.<br />
The data can be passed to the <code>kkmeans</code> function in a <code>matrix</code> or a
<code>data.frame</code>, in addition <code>kkmeans</code> also supports input in the form of a
kernel matrix of class <code>kernelMatrix</code> or as a list of character
vectors where a string kernel has to be used.
</p>


<h3>Value</h3>

<p>An S4 object of class <code>specc</code> which extends the class <code>vector</code>
containing integers indicating the cluster to which
each point is allocated. The following slots contain useful information
</p>
<table>
<tr><td><code>centers</code></td>
<td>
<p>A matrix of cluster centers.</p>
</td></tr>
<tr><td><code>size</code></td>
<td>
<p>The number of point in each cluster</p>
</td></tr>
<tr><td><code>withinss</code></td>
<td>
<p>The within-cluster sum of squares for each cluster</p>
</td></tr>
<tr><td><code>kernelf</code></td>
<td>
<p>The kernel function used</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Alexandros Karatzoglou <br /> <a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>References</h3>

<p>Inderjit Dhillon, Yuqiang Guan, Brian Kulis<br />
A Unified view of Kernel k-means, Spectral Clustering and Graph
Partitioning<br />
UTCS Technical Report<br />
<a href="https://people.bu.edu/bkulis/pubs/spectral_techreport.pdf">https://people.bu.edu/bkulis/pubs/spectral_techreport.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+specc">specc</a></code>, <code><a href="#topic+kpca">kpca</a></code>, <code><a href="#topic+kcca">kcca</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## Cluster the iris data set.
data(iris)

sc &lt;- kkmeans(as.matrix(iris[,-5]), centers=3)

sc
centers(sc)
size(sc)
withinss(sc)


</code></pre>

<hr>
<h2 id='kmmd'>Kernel Maximum Mean Discrepancy.</h2><span id='topic+kmmd'></span><span id='topic+kmmd+2Cmatrix-method'></span><span id='topic+kmmd+2Clist-method'></span><span id='topic+kmmd+2CkernelMatrix-method'></span><span id='topic+show+2Ckmmd-method'></span><span id='topic+H0'></span><span id='topic+Asymbound'></span><span id='topic+Radbound'></span><span id='topic+mmdstats'></span><span id='topic+AsympH0'></span>

<h3>Description</h3>

<p>The Kernel Maximum Mean Discrepancy <code>kmmd</code> performs
a non-parametric distribution test.</p>


<h3>Usage</h3>

<pre><code class='language-R'>
## S4 method for signature 'matrix'
kmmd(x, y, kernel="rbfdot",kpar="automatic", alpha = 0.05,
     asymptotic = FALSE, replace = TRUE, ntimes = 150, frac = 1, ...)

## S4 method for signature 'kernelMatrix'
kmmd(x, y, Kxy, alpha = 0.05,
     asymptotic = FALSE, replace = TRUE, ntimes = 100, frac = 1, ...)

## S4 method for signature 'list'
kmmd(x, y, kernel="stringdot", 
     kpar = list(type = "spectrum", length = 4), alpha = 0.05,
     asymptotic = FALSE, replace = TRUE, ntimes = 150, frac = 1, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kmmd_+3A_x">x</code></td>
<td>
<p>data values, in a <code>matrix</code>,
<code>list</code>, or <code>kernelMatrix</code></p>
</td></tr>
<tr><td><code id="kmmd_+3A_y">y</code></td>
<td>
<p>data values, in a <code>matrix</code>,
<code>list</code>, or <code>kernelMatrix</code></p>
</td></tr>
<tr><td><code id="kmmd_+3A_kxy">Kxy</code></td>
<td>
<p><code>kernlMatrix</code> between <code class="reqn">x</code> and <code class="reqn">y</code> values (only for the
kernelMatrix interface)</p>
</td></tr>
<tr><td><code id="kmmd_+3A_kernel">kernel</code></td>
<td>
<p>the kernel function used in training and predicting.
This parameter can be set to any function, of class kernel, which computes a dot product between two
vector arguments. <code>kernlab</code> provides the most popular kernel functions
which can be used by setting the kernel parameter to the following
strings:
</p>

<ul>
<li> <p><code>rbfdot</code> Radial Basis kernel function &quot;Gaussian&quot;
</p>
</li>
<li> <p><code>polydot</code> Polynomial kernel function
</p>
</li>
<li> <p><code>vanilladot</code> Linear kernel function
</p>
</li>
<li> <p><code>tanhdot</code> Hyperbolic tangent kernel function
</p>
</li>
<li> <p><code>laplacedot</code> Laplacian kernel function
</p>
</li>
<li> <p><code>besseldot</code> Bessel kernel function
</p>
</li>
<li> <p><code>anovadot</code> ANOVA RBF kernel function
</p>
</li>
<li> <p><code>splinedot</code> Spline kernel 
</p>
</li>
<li> <p><code>stringdot</code> String kernel 
</p>
</li></ul>

<p>The kernel parameter can also be set to a user defined function of
class kernel by passing the function name as an argument.
</p>
</td></tr>
<tr><td><code id="kmmd_+3A_kpar">kpar</code></td>
<td>
<p>the list of hyper-parameters (kernel parameters).
This is a list which contains the parameters to be used with the
kernel function. Valid parameters for existing kernels are :
</p>

<ul>
<li> <p><code>sigma</code> inverse kernel width for the Radial Basis
kernel function &quot;rbfdot&quot; and the Laplacian kernel &quot;laplacedot&quot;.
</p>
</li>
<li> <p><code>degree, scale, offset</code> for the Polynomial kernel &quot;polydot&quot;
</p>
</li>
<li> <p><code>scale, offset</code> for the Hyperbolic tangent kernel
function &quot;tanhdot&quot;
</p>
</li>
<li> <p><code>sigma, order, degree</code> for the Bessel kernel &quot;besseldot&quot;. 
</p>
</li>
<li> <p><code>sigma, degree</code> for the ANOVA kernel &quot;anovadot&quot;.
</p>
</li>
<li> <p><code>lenght, lambda, normalized</code> for the &quot;stringdot&quot; kernel
where length is the length of the strings considered, lambda the
decay factor and normalized a logical parameter determining if the
kernel evaluations should be normalized.
</p>
</li></ul>

<p>Hyper-parameters for user defined kernels can be passed
through the <code>kpar</code> parameter as well. In the case of a Radial
Basis kernel function (Gaussian) kpar can also be set to the
string &quot;automatic&quot; which uses the heuristics in  'sigest' to
calculate a good 'sigma' value for the Gaussian RBF or
Laplace kernel, from the data. (default = &quot;automatic&quot;).
</p>
</td></tr>
<tr><td><code id="kmmd_+3A_alpha">alpha</code></td>
<td>
<p>the confidence level of the test (default: 0.05)</p>
</td></tr>
<tr><td><code id="kmmd_+3A_asymptotic">asymptotic</code></td>
<td>
<p>calculate the bounds asymptotically (suitable for
smaller datasets) (default: FALSE)</p>
</td></tr>
<tr><td><code id="kmmd_+3A_replace">replace</code></td>
<td>
<p>use replace when sampling for computing the asymptotic
bounds (default : TRUE)</p>
</td></tr>
<tr><td><code id="kmmd_+3A_ntimes">ntimes</code></td>
<td>
<p>number of times repeating the sampling procedure (default
: 150)</p>
</td></tr>
<tr><td><code id="kmmd_+3A_frac">frac</code></td>
<td>
<p>fraction of points to sample (frac : 1) </p>
</td></tr>
<tr><td><code id="kmmd_+3A_...">...</code></td>
<td>
<p>additional parameters.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>kmmd</code> calculates the kernel maximum mean discrepancy for
samples from two distributions and conducts a test as to whether the samples are
from different distributions with level <code>alpha</code>.
</p>


<h3>Value</h3>

<p>An S4 object of class <code>kmmd</code> containing the
results of whether the H0 hypothesis is rejected or not. H0 being
that the samples <code class="reqn">x</code> and <code class="reqn">y</code> come from the same distribution.
The object contains the following slots :
</p>
<table>
<tr><td><code>H0</code></td>
<td>
<p>is H0 rejected (logical)</p>
</td></tr>
<tr><td><code>AsympH0</code></td>
<td>
<p>is H0 rejected according to the asymptotic bound (logical)</p>
</td></tr>
<tr><td><code>kernelf</code></td>
<td>
<p>the kernel function used.</p>
</td></tr>
<tr><td><code>mmdstats</code></td>
<td>
<p>the test statistics (vector of two)</p>
</td></tr>
<tr><td><code>Radbound</code></td>
<td>
<p>the Rademacher bound</p>
</td></tr>
<tr><td><code>Asymbound</code></td>
<td>
<p>the asymptotic bound</p>
</td></tr>
</table>
<p>see <code>kmmd-class</code> for more details.
</p>


<h3>Author(s)</h3>

<p>Alexandros Karatzoglou <br /> <a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>References</h3>

<p>Gretton, A., K. Borgwardt, M. Rasch, B. Schoelkopf and A. Smola<br />
<em>A Kernel Method for the Two-Sample-Problem</em><br />
Neural Information Processing Systems 2006, Vancouver   <br />
<a href="https://papers.neurips.cc/paper/3110-a-kernel-method-for-the-two-sample-problem.pdf">https://papers.neurips.cc/paper/3110-a-kernel-method-for-the-two-sample-problem.pdf</a>
</p>


<h3>See Also</h3>

<p><code>ksvm</code></p>


<h3>Examples</h3>

<pre><code class='language-R'># create data
x &lt;- matrix(runif(300),100)
y &lt;- matrix(runif(300)+1,100)


mmdo &lt;- kmmd(x, y)

mmdo
</code></pre>

<hr>
<h2 id='kmmd-class'>Class &quot;kqr&quot;</h2><span id='topic+kmmd-class'></span><span id='topic+kernelf+2Ckmmd-method'></span><span id='topic+H0+2Ckmmd-method'></span><span id='topic+AsympH0+2Ckmmd-method'></span><span id='topic+Radbound+2Ckmmd-method'></span><span id='topic+Asymbound+2Ckmmd-method'></span><span id='topic+mmdstats+2Ckmmd-method'></span>

<h3>Description</h3>

<p>The Kernel Maximum Mean Discrepancy object class</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("kmmd", ...)</code>.
or by calling the <code>kmmd</code> function 
</p>


<h3>Slots</h3>


<dl>
<dt><code>kernelf</code>:</dt><dd><p>Object of class <code>"kfunction"</code> contains
the kernel function used</p>
</dd>
<dt><code>xmatrix</code>:</dt><dd><p>Object of class <code>"kernelMatrix"</code> containing the
data used </p>
</dd>
<dt>H0</dt><dd><p>Object of class <code>"logical"</code> contains value of : is H0 rejected (logical)</p>
</dd>
<dt><code>AsympH0</code></dt><dd><p>Object of class <code>"logical"</code> contains
value : is H0 rejected according to the asymptotic bound (logical)</p>
</dd>
<dt><code>mmdstats</code></dt><dd><p>Object of class <code>"vector"</code> contains the test statistics (vector of two)</p>
</dd>
<dt><code>Radbound</code></dt><dd><p>Object of class <code>"numeric"</code> contains the Rademacher bound</p>
</dd>
<dt><code>Asymbound</code></dt><dd><p>Object of class <code>"numeric"</code> contains the asymptotic bound</p>
</dd>
</dl>



<h3>Methods</h3>

 
<dl>
<dt>kernelf</dt><dd><p><code>signature(object = "kmmd")</code>: returns the
kernel function used</p>
</dd>
<dt>H0</dt><dd><p><code>signature(object = "kmmd")</code>: returns the value of H0
being rejected</p>
</dd>
<dt>AsympH0</dt><dd><p><code>signature(object = "kmmd")</code>: returns the value of H0
being rejected according to the asymptotic bound</p>
</dd>
<dt>mmdstats</dt><dd><p><code>signature(object = "kmmd")</code>: returns the values
of the mmd statistics</p>
</dd>
<dt>Radbound</dt><dd><p><code>signature(object = "kmmd")</code>: returns the
value of the Rademacher bound</p>
</dd>
<dt>Asymbound</dt><dd><p><code>signature(object = "kmmd")</code>: returns the
value of the asymptotic bound</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Alexandros Karatzoglou<br /> <a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+kmmd">kmmd</a></code>, 
</p>


<h3>Examples</h3>

<pre><code class='language-R'># create data
x &lt;- matrix(runif(300),100)
y &lt;- matrix(runif(300)+1,100)


mmdo &lt;- kmmd(x, y)

H0(mmdo)

</code></pre>

<hr>
<h2 id='kpca'>Kernel Principal Components Analysis</h2><span id='topic+kpca'></span><span id='topic+kpca+2Cformula-method'></span><span id='topic+kpca+2Cmatrix-method'></span><span id='topic+kpca+2CkernelMatrix-method'></span><span id='topic+kpca+2Clist-method'></span><span id='topic+predict+2Ckpca-method'></span>

<h3>Description</h3>

<p>Kernel Principal Components Analysis is a nonlinear form of principal
component analysis.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'formula'
kpca(x, data = NULL, na.action, ...)

## S4 method for signature 'matrix'
kpca(x, kernel = "rbfdot", kpar = list(sigma = 0.1),
    features = 0, th = 1e-4, na.action = na.omit, ...)

## S4 method for signature 'kernelMatrix'
kpca(x, features = 0, th = 1e-4, ...)

## S4 method for signature 'list'
kpca(x, kernel = "stringdot", kpar = list(length = 4, lambda = 0.5),
    features = 0, th = 1e-4, na.action = na.omit, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kpca_+3A_x">x</code></td>
<td>
<p>the data matrix indexed by row or a formula describing the
model, or a kernel Matrix of class <code>kernelMatrix</code>, or a list of character vectors</p>
</td></tr> 
<tr><td><code id="kpca_+3A_data">data</code></td>
<td>
<p>an optional data frame containing the variables in
the model (when using a formula).</p>
</td></tr>
<tr><td><code id="kpca_+3A_kernel">kernel</code></td>
<td>
<p>the kernel function used in training and predicting.
This parameter can be set to any function, of class kernel, which computes a dot product between two
vector arguments. kernlab provides the most popular kernel functions
which can be used by setting the kernel parameter to the following
strings:
</p>

<ul>
<li> <p><code>rbfdot</code> Radial Basis kernel function &quot;Gaussian&quot;
</p>
</li>
<li> <p><code>polydot</code> Polynomial kernel function
</p>
</li>
<li> <p><code>vanilladot</code> Linear kernel function
</p>
</li>
<li> <p><code>tanhdot</code> Hyperbolic tangent kernel function
</p>
</li>
<li> <p><code>laplacedot</code> Laplacian kernel function
</p>
</li>
<li> <p><code>besseldot</code> Bessel kernel function
</p>
</li>
<li> <p><code>anovadot</code> ANOVA RBF kernel function
</p>
</li>
<li> <p><code>splinedot</code> Spline kernel 
</p>
</li></ul>

<p>The kernel parameter can also be set to a user defined function of
class kernel by passing the function name as an argument.
</p>
</td></tr>
<tr><td><code id="kpca_+3A_kpar">kpar</code></td>
<td>
<p>the list of hyper-parameters (kernel parameters).
This is a list which contains the parameters to be used with the
kernel function. Valid parameters for existing kernels are :
</p>

<ul>
<li> <p><code>sigma</code> inverse kernel width for the Radial Basis
kernel function &quot;rbfdot&quot; and the Laplacian kernel &quot;laplacedot&quot;.
</p>
</li>
<li> <p><code>degree, scale, offset</code> for the Polynomial kernel &quot;polydot&quot;
</p>
</li>
<li> <p><code>scale, offset</code> for the Hyperbolic tangent kernel
function &quot;tanhdot&quot;
</p>
</li>
<li> <p><code>sigma, order, degree</code> for the Bessel kernel &quot;besseldot&quot;. 
</p>
</li>
<li> <p><code>sigma, degree</code> for the ANOVA kernel &quot;anovadot&quot;.
</p>
</li></ul>

<p>Hyper-parameters for user defined kernels can be passed through the
kpar parameter as well.</p>
</td></tr>
<tr><td><code id="kpca_+3A_features">features</code></td>
<td>
<p>Number of features (principal components) to
return. (default: 0 , all)</p>
</td></tr>
<tr><td><code id="kpca_+3A_th">th</code></td>
<td>
<p>the value of the eigenvalue under which principal
components are ignored (only valid when features =  0). (default : 0.0001) </p>
</td></tr>
<tr><td><code id="kpca_+3A_na.action">na.action</code></td>
<td>
<p>A function to specify the action to be taken if <code>NA</code>s are
found. The default action is <code>na.omit</code>, which leads to rejection of cases
with missing values on any required variable. An alternative
is <code>na.fail</code>, which causes an error if <code>NA</code> cases
are found. (NOTE: If given, this argument must be named.)</p>
</td></tr>
<tr><td><code id="kpca_+3A_...">...</code></td>
<td>
<p> additional parameters</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Using kernel functions one can efficiently compute
principal components in high-dimensional 
feature spaces, related to input space by some non-linear map.<br />
The data can be passed to the <code>kpca</code> function in a <code>matrix</code> or a
<code>data.frame</code>, in addition <code>kpca</code> also supports input in the form of a
kernel matrix of class <code>kernelMatrix</code> or as a list of character
vectors where a string kernel has to be used.
</p>


<h3>Value</h3>

<p>An S4 object containing the principal component vectors along with the
corresponding eigenvalues. 
</p>
<table>
<tr><td><code>pcv</code></td>
<td>
<p>a matrix containing the principal component vectors (column
wise)</p>
</td></tr>
<tr><td><code>eig</code></td>
<td>
<p>The corresponding eigenvalues</p>
</td></tr>
<tr><td><code>rotated</code></td>
<td>
<p>The original data projected (rotated) on the principal components</p>
</td></tr>
<tr><td><code>xmatrix</code></td>
<td>
<p>The original data matrix</p>
</td></tr>
</table>
<p>all the slots of the object can be accessed by accessor functions.
</p>


<h3>Note</h3>

<p>The predict function can be used to embed new data on the new space</p>


<h3>Author(s)</h3>

<p>Alexandros Karatzoglou <br />
<a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>References</h3>

<p>Schoelkopf B., A. Smola, K.-R. Mueller :<br />
<em>Nonlinear component analysis as a kernel eigenvalue problem</em><br />
Neural Computation 10, 1299-1319<br />
<a href="https://doi.org/10.1162/089976698300017467">doi:10.1162/089976698300017467</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kcca">kcca</a></code>, <code>pca</code></p>


<h3>Examples</h3>

<pre><code class='language-R'># another example using the iris
data(iris)
test &lt;- sample(1:150,20)

kpc &lt;- kpca(~.,data=iris[-test,-5],kernel="rbfdot",
            kpar=list(sigma=0.2),features=2)

#print the principal component vectors
pcv(kpc)

#plot the data projection on the components
plot(rotated(kpc),col=as.integer(iris[-test,5]),
     xlab="1st Principal Component",ylab="2nd Principal Component")

#embed remaining points 
emb &lt;- predict(kpc,iris[test,-5])
points(emb,col=as.integer(iris[test,5]))
</code></pre>

<hr>
<h2 id='kpca-class'>Class &quot;kpca&quot;</h2><span id='topic+kpca-class'></span><span id='topic+rotated'></span><span id='topic+eig+2Ckpca-method'></span><span id='topic+kcall+2Ckpca-method'></span><span id='topic+kernelf+2Ckpca-method'></span><span id='topic+pcv+2Ckpca-method'></span><span id='topic+rotated+2Ckpca-method'></span><span id='topic+xmatrix+2Ckpca-method'></span>

<h3>Description</h3>

<p> The Kernel Principal Components Analysis class</p>


<h3>Objects of class &quot;kpca&quot;</h3>

<p>Objects can be created by calls of the form <code>new("kpca", ...)</code>.
or by calling the <code>kpca</code> function.
</p>


<h3>Slots</h3>


<dl>
<dt><code>pcv</code>:</dt><dd><p>Object of class <code>"matrix"</code> containing the
principal component vectors </p>
</dd>
<dt><code>eig</code>:</dt><dd><p>Object of class <code>"vector"</code> containing the
corresponding eigenvalues</p>
</dd>
<dt><code>rotated</code>:</dt><dd><p>Object of class <code>"matrix"</code> containing the
projection of the data on the principal components</p>
</dd>
<dt><code>kernelf</code>:</dt><dd><p>Object of class <code>"function"</code> containing
the kernel function used</p>
</dd>
<dt><code>kpar</code>:</dt><dd><p>Object of class <code>"list"</code> containing the
kernel parameters used </p>
</dd>
<dt><code>xmatrix</code>:</dt><dd><p>Object of class <code>"matrix"</code> containing
the data matrix used </p>
</dd>
<dt><code>kcall</code>:</dt><dd><p>Object of class <code>"ANY"</code> containing the
function call </p>
</dd>
<dt><code>n.action</code>:</dt><dd><p>Object of class <code>"ANY"</code> containing the
action performed on NA </p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>eig</dt><dd><p><code>signature(object = "kpca")</code>: returns the eigenvalues </p>
</dd>
<dt>kcall</dt><dd><p><code>signature(object = "kpca")</code>: returns the
performed call</p>
</dd>
<dt>kernelf</dt><dd><p><code>signature(object = "kpca")</code>: returns the used
kernel function</p>
</dd>
<dt>pcv</dt><dd><p><code>signature(object = "kpca")</code>: returns the principal
component vectors </p>
</dd>
<dt>predict</dt><dd><p><code>signature(object = "kpca")</code>: embeds new data </p>
</dd>
<dt>rotated</dt><dd><p><code>signature(object = "kpca")</code>: returns the
projected data</p>
</dd>
<dt>xmatrix</dt><dd><p><code>signature(object = "kpca")</code>: returns the used
data matrix </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Alexandros Karatzoglou<br /> <a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+ksvm-class">ksvm-class</a></code>, 
<code><a href="#topic+kcca-class">kcca-class</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'># another example using the iris
data(iris)
test &lt;- sample(1:50,20)

kpc &lt;- kpca(~.,data=iris[-test,-5],kernel="rbfdot",
            kpar=list(sigma=0.2),features=2)

#print the principal component vectors
pcv(kpc)
rotated(kpc)
kernelf(kpc)
eig(kpc)
</code></pre>

<hr>
<h2 id='kqr'>Kernel Quantile Regression.</h2><span id='topic+kqr'></span><span id='topic+kqr+2Cformula-method'></span><span id='topic+kqr+2Cvector-method'></span><span id='topic+kqr+2Cmatrix-method'></span><span id='topic+kqr+2Clist-method'></span><span id='topic+kqr+2CkernelMatrix-method'></span><span id='topic+coef+2Ckqr-method'></span><span id='topic+show+2Ckqr-method'></span>

<h3>Description</h3>

<p>The Kernel Quantile Regression algorithm <code>kqr</code> performs
non-parametric Quantile Regression.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'formula'
kqr(x, data=NULL, ..., subset, na.action = na.omit, scaled = TRUE)

## S4 method for signature 'vector'
kqr(x,...)

## S4 method for signature 'matrix'
kqr(x, y, scaled = TRUE, tau = 0.5, C = 0.1, kernel = "rbfdot",
    kpar = "automatic", reduced = FALSE, rank = dim(x)[1]/6,
    fit = TRUE, cross = 0, na.action = na.omit)

## S4 method for signature 'kernelMatrix'
kqr(x, y, tau = 0.5, C = 0.1, fit = TRUE, cross = 0)

## S4 method for signature 'list'
kqr(x, y, tau = 0.5, C = 0.1, kernel = "strigdot",
    kpar= list(length=4, C=0.5), fit = TRUE, cross = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kqr_+3A_x">x</code></td>
<td>
<p>e data or a symbolic description of the model to be fit.
When not using a formula x can be a matrix or vector containing
the training data or a kernel matrix of class <code>kernelMatrix</code>
of the training data or a list of character vectors (for use
with the string kernel). Note, that the intercept is always
excluded, whether given in the formula or not.</p>
</td></tr>
<tr><td><code id="kqr_+3A_data">data</code></td>
<td>
<p>an optional data frame containing the variables in the model.
By default the variables are taken from the environment which
<code>kqr</code> is called from.</p>
</td></tr>
<tr><td><code id="kqr_+3A_y">y</code></td>
<td>
<p>a numeric vector or a column matrix containing the response.</p>
</td></tr>
<tr><td><code id="kqr_+3A_scaled">scaled</code></td>
<td>
<p>A logical vector indicating the variables to be
scaled. If <code>scaled</code> is of length 1, the value is recycled as
many times as needed and all non-binary variables are scaled.
Per default, data are scaled internally (both <code>x</code> and <code>y</code>
variables) to zero mean and unit variance. The center and scale
values are returned and used for later predictions. (default: TRUE)</p>
</td></tr>
<tr><td><code id="kqr_+3A_tau">tau</code></td>
<td>
<p>the quantile to be estimated, this is generally a number
strictly between 0 and 1. For 0.5 the median is calculated.
(default: 0.5)</p>
</td></tr>
<tr><td><code id="kqr_+3A_c">C</code></td>
<td>
<p>the cost regularization parameter. This parameter controls
the smoothness of the fitted function, essentially higher
values for C lead to less smooth functions.(default: 1)</p>
</td></tr>
<tr><td><code id="kqr_+3A_kernel">kernel</code></td>
<td>
<p>the kernel function used in training and predicting.
This parameter can be set to any function, of class kernel, which computes a dot product between two
vector arguments. <code>kernlab</code> provides the most popular kernel functions
which can be used by setting the kernel parameter to the following
strings:
</p>

<ul>
<li> <p><code>rbfdot</code> Radial Basis kernel function &quot;Gaussian&quot;
</p>
</li>
<li> <p><code>polydot</code> Polynomial kernel function
</p>
</li>
<li> <p><code>vanilladot</code> Linear kernel function
</p>
</li>
<li> <p><code>tanhdot</code> Hyperbolic tangent kernel function
</p>
</li>
<li> <p><code>laplacedot</code> Laplacian kernel function
</p>
</li>
<li> <p><code>besseldot</code> Bessel kernel function
</p>
</li>
<li> <p><code>anovadot</code> ANOVA RBF kernel function
</p>
</li>
<li> <p><code>splinedot</code> Spline kernel 
</p>
</li>
<li> <p><code>stringdot</code> String kernel 
</p>
</li></ul>

<p>The kernel parameter can also be set to a user defined function of
class kernel by passing the function name as an argument.
</p>
</td></tr>
<tr><td><code id="kqr_+3A_kpar">kpar</code></td>
<td>
<p>the list of hyper-parameters (kernel parameters).
This is a list which contains the parameters to be used with the
kernel function. Valid parameters for existing kernels are :
</p>

<ul>
<li> <p><code>sigma</code> inverse kernel width for the Radial Basis
kernel function &quot;rbfdot&quot; and the Laplacian kernel &quot;laplacedot&quot;.
</p>
</li>
<li> <p><code>degree, scale, offset</code> for the Polynomial kernel &quot;polydot&quot;
</p>
</li>
<li> <p><code>scale, offset</code> for the Hyperbolic tangent kernel
function &quot;tanhdot&quot;
</p>
</li>
<li> <p><code>sigma, order, degree</code> for the Bessel kernel &quot;besseldot&quot;. 
</p>
</li>
<li> <p><code>sigma, degree</code> for the ANOVA kernel &quot;anovadot&quot;.
</p>
</li>
<li> <p><code>lenght, lambda, normalized</code> for the &quot;stringdot&quot; kernel
where length is the length of the strings considered, lambda the
decay factor and normalized a logical parameter determining if the
kernel evaluations should be normalized.
</p>
</li></ul>

<p>Hyper-parameters for user defined kernels can be passed
through the <code>kpar</code> parameter as well. In the case of a Radial
Basis kernel function (Gaussian) kpar can also be set to the
string &quot;automatic&quot; which uses the heuristics in  'sigest' to
calculate a good 'sigma' value for the Gaussian RBF or
Laplace kernel, from the data. (default = &quot;automatic&quot;).
</p>
</td></tr>
<tr><td><code id="kqr_+3A_reduced">reduced</code></td>
<td>
<p>use an incomplete cholesky decomposition to calculate a
decomposed form <code class="reqn">Z</code> of the kernel Matrix <code class="reqn">K</code> (where <code class="reqn">K = ZZ'</code>) and
perform the calculations with <code class="reqn">Z</code>. This might be useful when
using <code>kqr</code> with large datasets since normally an n times n
kernel matrix would be computed. Setting <code>reduced</code> to <code>TRUE</code>
makes use of <code>csi</code> to compute a decomposed form instead and
thus only a <code class="reqn">n \times m</code> matrix where <code class="reqn">m &lt; n</code> and <code class="reqn">n</code> the sample size is
stored in memory (default: FALSE)</p>
</td></tr>
<tr><td><code id="kqr_+3A_rank">rank</code></td>
<td>
<p>the rank m of the decomposed matrix calculated when using an
incomplete cholesky decomposition. This parameter is only
taken into account when <code>reduced</code> is <code>TRUE</code>(default :
dim(x)[1]/6)</p>
</td></tr>
<tr><td><code id="kqr_+3A_fit">fit</code></td>
<td>
<p>indicates whether the fitted values should be computed and
included in the model or not (default: 'TRUE')</p>
</td></tr>
<tr><td><code id="kqr_+3A_cross">cross</code></td>
<td>
<p>if a integer value k&gt;0 is specified, a k-fold cross
validation on the training data is performed to assess the
quality of the model: the Pinball loss and the for quantile regression</p>
</td></tr>
<tr><td><code id="kqr_+3A_subset">subset</code></td>
<td>
<p>An index vector specifying the cases to be used in the
training sample.  (NOTE: If given, this argument must be
named.)</p>
</td></tr>
<tr><td><code id="kqr_+3A_na.action">na.action</code></td>
<td>
<p>A function to specify the action to be taken if <code>NA</code>s are
found. The default action is <code>na.omit</code>, which leads to
rejection of cases with missing values on any required variable. An
alternative is <code>na.fail</code>, which causes an error if <code>NA</code>
cases are found. (NOTE: If given, this argument must be named.)</p>
</td></tr>
<tr><td><code id="kqr_+3A_...">...</code></td>
<td>
<p>additional parameters.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In quantile regression a function is fitted to the data so that
it satisfies the property that a portion <code class="reqn">tau</code> of the data
<code class="reqn">y|n</code> is below the estimate. While the error bars of many
regression problems can be viewed as such estimates quantile
regression estimates this quantity directly. Kernel quantile regression
is similar to nu-Support Vector Regression in that it minimizes a
regularized loss function in RKHS. The difference between nu-SVR and
kernel quantile regression is in the type of loss function used which
in the case of quantile regression is the pinball loss (see reference
for details.). Minimizing the regularized loss boils down to a
quadratic problem which is solved using an interior point QP solver
<code>ipop</code> implemented in <code>kernlab</code>.
</p>


<h3>Value</h3>

<p>An S4 object of class <code>kqr</code> containing the fitted model along with
information.Accessor functions can be used to access the slots of the
object which include :
</p>
<table>
<tr><td><code>alpha</code></td>
<td>
<p>The resulting model parameters which can be also accessed
by <code>coef</code>.</p>
</td></tr>
<tr><td><code>kernelf</code></td>
<td>
<p>the kernel function used.</p>
</td></tr>
<tr><td><code>error</code></td>
<td>
<p>Training error (if fit == TRUE)</p>
</td></tr>
</table>
<p>see <code>kqr-class</code> for more details.
</p>


<h3>Author(s)</h3>

<p>Alexandros Karatzoglou <br /> <a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>References</h3>

<p>Ichiro Takeuchi, Quoc V. Le, Timothy D. Sears, Alexander J. Smola<br />
<em>Nonparametric Quantile Estimation</em><br />
Journal of Machine Learning Research 7,2006,1231-1264  <br />
<a href="https://www.jmlr.org/papers/volume7/takeuchi06a/takeuchi06a.pdf">https://www.jmlr.org/papers/volume7/takeuchi06a/takeuchi06a.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.kqr">predict.kqr</a></code>, <code><a href="#topic+kqr-class">kqr-class</a></code>, <code><a href="#topic+ipop">ipop</a></code>, <code><a href="#topic+rvm">rvm</a></code>, <code><a href="#topic+ksvm">ksvm</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># create data
x &lt;- sort(runif(300))
y &lt;- sin(pi*x) + rnorm(300,0,sd=exp(sin(2*pi*x)))

# first calculate the median
qrm &lt;- kqr(x, y, tau = 0.5, C=0.15)

# predict and plot
plot(x, y)
ytest &lt;- predict(qrm, x)
lines(x, ytest, col="blue")

# calculate 0.9 quantile
qrm &lt;- kqr(x, y, tau = 0.9, kernel = "rbfdot",
           kpar= list(sigma=10), C=0.15)
ytest &lt;- predict(qrm, x)
lines(x, ytest, col="red")

# calculate 0.1 quantile
qrm &lt;- kqr(x, y, tau = 0.1,C=0.15)
ytest &lt;- predict(qrm, x)
lines(x, ytest, col="green")

# print first 10 model coefficients
coef(qrm)[1:10]
</code></pre>

<hr>
<h2 id='kqr-class'>Class &quot;kqr&quot;</h2><span id='topic+kqr-class'></span><span id='topic+alpha+2Ckqr-method'></span><span id='topic+cross+2Ckqr-method'></span><span id='topic+error+2Ckqr-method'></span><span id='topic+kcall+2Ckqr-method'></span><span id='topic+kernelf+2Ckqr-method'></span><span id='topic+kpar+2Ckqr-method'></span><span id='topic+param+2Ckqr-method'></span><span id='topic+alphaindex+2Ckqr-method'></span><span id='topic+b+2Ckqr-method'></span><span id='topic+xmatrix+2Ckqr-method'></span><span id='topic+ymatrix+2Ckqr-method'></span><span id='topic+scaling+2Ckqr-method'></span>

<h3>Description</h3>

<p>The Kernel Quantile Regression object class</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("kqr", ...)</code>.
or by calling the <code>kqr</code> function 
</p>


<h3>Slots</h3>


<dl>
<dt><code>kernelf</code>:</dt><dd><p>Object of class <code>"kfunction"</code> contains
the kernel function used</p>
</dd>
<dt><code>kpar</code>:</dt><dd><p>Object of class <code>"list"</code> contains the
kernel parameter used </p>
</dd>
<dt><code>coef</code>:</dt><dd><p>Object of class <code>"ANY"</code> containing the model parameters</p>
</dd>
<dt><code>param</code>:</dt><dd><p>Object of class <code>"list"</code> contains the
cost parameter C and tau parameter used </p>
</dd>
<dt><code>kcall</code>:</dt><dd><p>Object of class <code>"list"</code> contains the used
function call </p>
</dd>
<dt><code>terms</code>:</dt><dd><p>Object of class <code>"ANY"</code> contains the
terms representation of the symbolic model used (when using a formula)</p>
</dd>
<dt><code>xmatrix</code>:</dt><dd><p>Object of class <code>"input"</code> containing
the data matrix used </p>
</dd>
<dt><code>ymatrix</code>:</dt><dd><p>Object of class <code>"output"</code> containing the
response matrix</p>
</dd>
<dt><code>fitted</code>:</dt><dd><p>Object of class <code>"output"</code> containing the
fitted values  </p>
</dd>
<dt><code>alpha</code>:</dt><dd><p>Object of class <code>"listI"</code> containing the
computes alpha values </p>
</dd>
<dt><code>b</code>:</dt><dd><p>Object of class <code>"numeric"</code> containing the
offset of the model.</p>
</dd>
<dt><code>scaling</code></dt><dd><p>Object of class <code>"ANY"</code> containing
the scaling coefficients of the data (when case <code>scaled = TRUE</code> is used).</p>
</dd>
<dt><code>error</code>:</dt><dd><p>Object of class <code>"numeric"</code> containing the
training error</p>
</dd>
<dt><code>cross</code>:</dt><dd><p>Object of class <code>"numeric"</code> containing the
cross validation error</p>
</dd>
<dt><code>n.action</code>:</dt><dd><p>Object of class <code>"ANY"</code> containing the
action performed in NA </p>
</dd>
<dt><code>nclass</code>:</dt><dd><p>Inherited from class <code>vm</code>, not used in kqr</p>
</dd>
<dt><code>lev</code>:</dt><dd><p>Inherited from class <code>vm</code>, not used in kqr</p>
</dd>
<dt><code>type</code>:</dt><dd><p>Inherited from class <code>vm</code>, not used in kqr</p>
</dd>
</dl>



<h3>Methods</h3>

 
<dl>
<dt>coef</dt><dd><p><code>signature(object = "kqr")</code>: returns the
coefficients (alpha) of the model</p>
</dd>
<dt>alpha</dt><dd><p><code>signature(object = "kqr")</code>: returns the alpha
vector (identical to <code>coef</code>)</p>
</dd>
<dt>b</dt><dd><p><code>signature(object = "kqr")</code>: returns the offset beta
of the model.</p>
</dd>
<dt>cross</dt><dd><p><code>signature(object = "kqr")</code>: returns the cross
validation error </p>
</dd>
<dt>error</dt><dd><p><code>signature(object = "kqr")</code>: returns the
training error </p>
</dd>
<dt>fitted</dt><dd><p><code>signature(object = "vm")</code>: returns the fitted values </p>
</dd>
<dt>kcall</dt><dd><p><code>signature(object = "kqr")</code>: returns the call performed</p>
</dd>
<dt>kernelf</dt><dd><p><code>signature(object = "kqr")</code>: returns the
kernel function used</p>
</dd>
<dt>kpar</dt><dd><p><code>signature(object = "kqr")</code>: returns the kernel
parameter used</p>
</dd>
<dt>param</dt><dd><p><code>signature(object = "kqr")</code>: returns the
cost regularization parameter C and tau used</p>
</dd>
<dt>xmatrix</dt><dd><p><code>signature(object = "kqr")</code>: returns the
data matrix used</p>
</dd>
<dt>ymatrix</dt><dd><p><code>signature(object = "kqr")</code>: returns the
response matrix used</p>
</dd>
<dt>scaling</dt><dd><p><code>signature(object = "kqr")</code>: returns the
scaling coefficients of the data (when <code>scaled = TRUE</code> is used)</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Alexandros Karatzoglou<br /> <a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+kqr">kqr</a></code>, 
<code><a href="#topic+vm-class">vm-class</a></code>,
<code><a href="#topic+ksvm-class">ksvm-class</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# create data
x &lt;- sort(runif(300))
y &lt;- sin(pi*x) + rnorm(300,0,sd=exp(sin(2*pi*x)))

# first calculate the median
qrm &lt;- kqr(x, y, tau = 0.5, C=0.15)

# predict and plot
plot(x, y)
ytest &lt;- predict(qrm, x)
lines(x, ytest, col="blue")

# calculate 0.9 quantile
qrm &lt;- kqr(x, y, tau = 0.9, kernel = "rbfdot",
           kpar = list(sigma = 10), C = 0.15)
ytest &lt;- predict(qrm, x)
lines(x, ytest, col="red")

# print model coefficients and other information
coef(qrm)
b(qrm)
error(qrm)
kernelf(qrm)
</code></pre>

<hr>
<h2 id='ksvm'>Support Vector Machines</h2><span id='topic+ksvm'></span><span id='topic+ksvm+2Cformula-method'></span><span id='topic+ksvm+2Cvector-method'></span><span id='topic+ksvm+2Cmatrix-method'></span><span id='topic+ksvm+2CkernelMatrix-method'></span><span id='topic+ksvm+2Clist-method'></span><span id='topic+show+2Cksvm-method'></span><span id='topic+coef+2Cksvm-method'></span>

<h3>Description</h3>

<p>Support Vector Machines are an excellent tool for classification, 
novelty detection, and regression. <code>ksvm</code> supports the 
well known C-svc, nu-svc, (classification) one-class-svc (novelty)
eps-svr, nu-svr (regression) formulations along with 
native multi-class classification formulations and 
the bound-constraint SVM formulations.<br />
<code>ksvm</code> also supports class-probabilities output and 
confidence intervals for regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'formula'
ksvm(x, data = NULL, ..., subset, na.action = na.omit, scaled = TRUE)

## S4 method for signature 'vector'
ksvm(x, ...)

## S4 method for signature 'matrix'
ksvm(x, y = NULL, scaled = TRUE, type = NULL,
     kernel ="rbfdot", kpar = "automatic",
     C = 1, nu = 0.2, epsilon = 0.1, prob.model = FALSE,
     class.weights = NULL, cross = 0, fit = TRUE, cache = 40,
     tol = 0.001, shrinking = TRUE, ..., 
     subset, na.action = na.omit)

## S4 method for signature 'kernelMatrix'
ksvm(x, y = NULL, type = NULL,
     C = 1, nu = 0.2, epsilon = 0.1, prob.model = FALSE,
     class.weights = NULL, cross = 0, fit = TRUE, cache = 40,
     tol = 0.001, shrinking = TRUE, ...)

## S4 method for signature 'list'
ksvm(x, y = NULL, type = NULL,
     kernel = "stringdot", kpar = list(length = 4, lambda = 0.5),
     C = 1, nu = 0.2, epsilon = 0.1, prob.model = FALSE,
     class.weights = NULL, cross = 0, fit = TRUE, cache = 40,
     tol = 0.001, shrinking = TRUE, ...,
     na.action = na.omit)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ksvm_+3A_x">x</code></td>
<td>
<p>a symbolic description of the model to be fit.  When not
using a formula x can be a matrix or vector containing the training
data 
or a kernel matrix of class <code>kernelMatrix</code> of the training data
or a list of character vectors (for use with the string
kernel). Note, that the intercept is always excluded, whether
given in the formula or not.</p>
</td></tr>
<tr><td><code id="ksvm_+3A_data">data</code></td>
<td>
<p>an optional data frame containing the training data, when using a formula.
By default the data is taken from the environment which
&lsquo;ksvm&rsquo; is called from.</p>
</td></tr>
<tr><td><code id="ksvm_+3A_y">y</code></td>
<td>
<p>a response vector with one label for each row/component of <code>x</code>. Can be either
a factor (for classification tasks) or a numeric vector (for
regression).</p>
</td></tr>
<tr><td><code id="ksvm_+3A_scaled">scaled</code></td>
<td>
<p>A logical vector indicating the variables to be
scaled. If <code>scaled</code> is of length 1, the value is recycled as
many times as needed and all non-binary variables are scaled.
Per default, data are scaled internally (both <code>x</code> and <code>y</code>
variables) to zero mean and unit variance. The center and scale
values are returned and used for later predictions.</p>
</td></tr>
<tr><td><code id="ksvm_+3A_type">type</code></td>
<td>
<p><code>ksvm</code> can be used for classification
, for regression, or for novelty detection.
Depending on whether <code>y</code> is
a factor or not, the default setting for <code>type</code> is <code>C-svc</code>
or <code>eps-svr</code>,
respectively, but can be overwritten by setting an explicit value.<br />
Valid options are:
</p>

<ul>
<li> <p><code>C-svc</code>   C classification
</p>
</li>
<li> <p><code>nu-svc</code>  nu classification
</p>
</li>
<li> <p><code>C-bsvc</code>  bound-constraint svm classification
</p>
</li>
<li> <p><code>spoc-svc</code>  Crammer, Singer native multi-class
</p>
</li>
<li> <p><code>kbb-svc</code>  Weston, Watkins native multi-class
</p>
</li>
<li> <p><code>one-svc</code>  novelty detection
</p>
</li>
<li> <p><code>eps-svr</code>  epsilon regression
</p>
</li>
<li> <p><code>nu-svr</code>   nu regression
</p>
</li>
<li> <p><code>eps-bsvr</code>  bound-constraint svm regression
</p>
</li></ul>

</td></tr>
<tr><td><code id="ksvm_+3A_kernel">kernel</code></td>
<td>
<p>the kernel function used in training and predicting.
This parameter can be set to any function, of class kernel, which
computes the inner product in feature space between two
vector arguments (see <code><a href="#topic+kernels">kernels</a></code>). <br />
kernlab provides the most popular kernel functions
which can be used by setting the kernel parameter to the following
strings:
</p>

<ul>
<li> <p><code>rbfdot</code> Radial Basis kernel &quot;Gaussian&quot;
</p>
</li>
<li> <p><code>polydot</code> Polynomial kernel
</p>
</li>
<li> <p><code>vanilladot</code> Linear kernel 
</p>
</li>
<li> <p><code>tanhdot</code> Hyperbolic tangent kernel 
</p>
</li>
<li> <p><code>laplacedot</code> Laplacian kernel 
</p>
</li>
<li> <p><code>besseldot</code> Bessel kernel 
</p>
</li>
<li> <p><code>anovadot</code> ANOVA RBF kernel 
</p>
</li>
<li> <p><code>splinedot</code> Spline kernel 
</p>
</li>
<li> <p><code>stringdot</code> String kernel 
</p>
</li></ul>

<p>Setting the kernel parameter to &quot;matrix&quot; treats <code>x</code> as a kernel
matrix calling the <code>kernelMatrix</code> interface.<br />
</p>
<p>The kernel parameter can also be set to a user defined function of
class kernel by passing the function name as an argument.
</p>
</td></tr>
<tr><td><code id="ksvm_+3A_kpar">kpar</code></td>
<td>
<p>the list of hyper-parameters (kernel parameters).
This is a list which contains the parameters to be used with the
kernel function. For valid parameters for existing kernels are :
</p>

<ul>
<li> <p><code>sigma</code> inverse kernel width for the Radial Basis
kernel function &quot;rbfdot&quot; and the Laplacian kernel &quot;laplacedot&quot;.
</p>
</li>
<li> <p><code>degree, scale, offset</code> for the Polynomial kernel &quot;polydot&quot;
</p>
</li>
<li> <p><code>scale, offset</code> for the Hyperbolic tangent kernel
function &quot;tanhdot&quot;
</p>
</li>
<li> <p><code>sigma, order, degree</code> for the Bessel kernel &quot;besseldot&quot;. 
</p>
</li>
<li> <p><code>sigma, degree</code> for the ANOVA kernel &quot;anovadot&quot;.
</p>
</li>
<li> <p><code>length, lambda, normalized</code> for the &quot;stringdot&quot; kernel
where length is the length of the strings considered, lambda the
decay factor and normalized a logical parameter determining if the
kernel evaluations should be normalized.
</p>
</li></ul>

<p>Hyper-parameters for user defined kernels can be passed through the
kpar parameter as well. In the case of a Radial Basis kernel function (Gaussian)
kpar can also be set to the string &quot;automatic&quot; which uses the heuristics in 
<code><a href="#topic+sigest">sigest</a></code> to calculate a good <code>sigma</code> value for the
Gaussian RBF or Laplace kernel, from the data.
(default = &quot;automatic&quot;).</p>
</td></tr>
<tr><td><code id="ksvm_+3A_c">C</code></td>
<td>
<p>cost of constraints violation (default: 1) this is the
&lsquo;C&rsquo;-constant of the regularization term in the Lagrange
formulation.</p>
</td></tr>
<tr><td><code id="ksvm_+3A_nu">nu</code></td>
<td>
<p>parameter needed for <code>nu-svc</code>,
<code>one-svc</code>, and <code>nu-svr</code>. The <code>nu</code>
parameter sets the upper bound on the training error and the lower
bound on the fraction of data points to become Support Vectors (default: 0.2).</p>
</td></tr>
<tr><td><code id="ksvm_+3A_epsilon">epsilon</code></td>
<td>
<p>epsilon in the insensitive-loss function used for
<code>eps-svr</code>, <code>nu-svr</code> and <code>eps-bsvm</code> (default: 0.1)</p>
</td></tr>
<tr><td><code id="ksvm_+3A_prob.model">prob.model</code></td>
<td>
<p>if set to <code>TRUE</code> builds a model for calculating class
probabilities or in case of regression, calculates the scaling
parameter of the Laplacian distribution fitted on the residuals.
Fitting is done  on output data created by performing a
3-fold cross-validation on the training data. For details see
references. (default: <code>FALSE</code>)</p>
</td></tr>
<tr><td><code id="ksvm_+3A_class.weights">class.weights</code></td>
<td>
<p>a named vector of weights for the different
classes, used for asymmetric class sizes. Not all factor levels have
to be supplied (default weight: 1). All components have to be named.</p>
</td></tr>
<tr><td><code id="ksvm_+3A_cache">cache</code></td>
<td>
<p>cache memory in MB (default 40)</p>
</td></tr>
<tr><td><code id="ksvm_+3A_tol">tol</code></td>
<td>
<p>tolerance of termination criterion (default: 0.001)</p>
</td></tr>
<tr><td><code id="ksvm_+3A_shrinking">shrinking</code></td>
<td>
<p>option whether to use the shrinking-heuristics
(default: <code>TRUE</code>)</p>
</td></tr>
<tr><td><code id="ksvm_+3A_cross">cross</code></td>
<td>
<p>if a integer value k&gt;0 is specified, a k-fold cross
validation on the training data is performed to assess the quality
of the model: the accuracy rate for classification and the Mean
Squared Error for regression</p>
</td></tr>
<tr><td><code id="ksvm_+3A_fit">fit</code></td>
<td>
<p>indicates whether the fitted values should be computed
and included in the model or not (default: <code>TRUE</code>)</p>
</td></tr>
<tr><td><code id="ksvm_+3A_...">...</code></td>
<td>
<p>additional parameters for the low level fitting function</p>
</td></tr>
<tr><td><code id="ksvm_+3A_subset">subset</code></td>
<td>
<p>An index vector specifying the cases to be used in the
training sample.  (NOTE: If given, this argument must be
named.)</p>
</td></tr>
<tr><td><code id="ksvm_+3A_na.action">na.action</code></td>
<td>
<p>A function to specify the action to be taken if <code>NA</code>s are
found. The default action is <code>na.omit</code>, which leads to rejection of cases
with missing values on any required variable. An alternative
is <code>na.fail</code>, which causes an error if <code>NA</code> cases
are found. (NOTE: If given, this argument must be named.)</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>ksvm</code> uses John Platt's SMO algorithm for solving the SVM QP problem an
most SVM formulations. On the <code>spoc-svc</code>, <code>kbb-svc</code>, <code>C-bsvc</code> and
<code>eps-bsvr</code> formulations a chunking algorithm based on the TRON QP
solver is used. <br />
For multiclass-classification with <code class="reqn">k</code> classes, <code class="reqn">k &gt; 2</code>, <code>ksvm</code> uses the
&lsquo;one-against-one&rsquo;-approach, in which <code class="reqn">k(k-1)/2</code> binary classifiers are
trained; the appropriate class is found by a voting scheme,
The <code>spoc-svc</code> and the <code>kbb-svc</code> formulations deal with the
multiclass-classification problems by solving a single quadratic problem involving all the classes.<br />
If the predictor variables include factors, the formula interface must be used to get a
correct model matrix. <br />
In classification when <code>prob.model</code> is <code>TRUE</code> a 3-fold cross validation is
performed on the data and a sigmoid function is fitted on the
resulting decision values <code class="reqn">f</code>.
The data can be passed to the <code>ksvm</code> function in a <code>matrix</code> or a
<code>data.frame</code>, in addition <code>ksvm</code> also supports input in the form of a
kernel matrix of class <code>kernelMatrix</code> or as a list of character
vectors where a string kernel has to be used.<br />
The <code>plot</code> function for binary classification <code>ksvm</code> objects
displays a contour plot of the decision values with the corresponding
support vectors highlighted.<br />
The predict function can return class probabilities for 
classification problems by setting the <code>type</code> parameter to
&quot;probabilities&quot;. <br />
The problem of model selection is partially addressed by an empirical
observation for the RBF kernels (Gaussian , Laplace) where the optimal values of the
<code class="reqn">sigma</code> width parameter are shown to lie in between the 0.1 and 0.9
quantile of the <code class="reqn">\|x- x'\|</code> statistics. When using an RBF kernel
and setting <code>kpar</code> to &quot;automatic&quot;, <code>ksvm</code> uses the <code>sigest</code> function
to estimate the quantiles and uses the median of the values.
</p>


<h3>Value</h3>

<p>An S4 object of class <code>"ksvm"</code> containing the fitted model,
Accessor functions can be used to access the slots of the object (see
examples) which include:
</p>
<table>
<tr><td><code>alpha</code></td>
<td>
<p>The resulting support vectors, (alpha vector) (possibly scaled).</p>
</td></tr>
<tr><td><code>alphaindex</code></td>
<td>
<p>The index of the resulting support vectors in the data
matrix. Note that this index refers to the pre-processed data (after
the possible effect of <code>na.omit</code> and <code>subset</code>)</p>
</td></tr>
<tr><td><code>coef</code></td>
<td>
<p>The corresponding coefficients times the training labels.</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>The negative intercept.</p>
</td></tr>
<tr><td><code>nSV</code></td>
<td>
<p>The number of Support Vectors</p>
</td></tr>
<tr><td><code>obj</code></td>
<td>
<p>The value of the objective function. In case of one-against-one classification this is a vector of values</p>
</td></tr> 
<tr><td><code>error</code></td>
<td>
<p>Training error</p>
</td></tr>
<tr><td><code>cross</code></td>
<td>
<p>Cross validation error, (when cross &gt; 0)</p>
</td></tr>
<tr><td><code>prob.model</code></td>
<td>
<p>Contains the width of the Laplacian fitted on the
residuals in case of regression, or the parameters of the sigmoid
fitted on the decision values in case of classification.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Data is scaled internally by default, usually yielding better results.</p>


<h3>Author(s)</h3>

<p>Alexandros Karatzoglou (SMO optimizers in C++ by Chih-Chung Chang &amp; Chih-Jen Lin)<br />
<a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a>
</p>


<h3>References</h3>


<ul>
<li>
<p>Chang Chih-Chung, Lin Chih-Jen<br />
<em>LIBSVM: a library for Support Vector Machines</em><br />
<a href="https://www.csie.ntu.edu.tw/~cjlin/libsvm/">https://www.csie.ntu.edu.tw/~cjlin/libsvm/</a>
</p>
</li>
<li>
<p>Chih-Wei Hsu, Chih-Jen Lin<br />
<em>BSVM</em>
<a href="https://www.csie.ntu.edu.tw/~cjlin/bsvm/">https://www.csie.ntu.edu.tw/~cjlin/bsvm/</a>
</p>
</li>
<li>
<p>J. Platt<br />
<em>Probabilistic outputs for support vector machines and comparison to regularized likelihood methods</em> <br />
Advances in Large Margin Classifiers, A. Smola, P. Bartlett, B. Schoelkopf and D. Schuurmans, Eds. Cambridge, MA: MIT Press, 2000.
</p>
</li>
<li>
<p>H.-T. Lin, C.-J. Lin and R. C. Weng<br />
<em>A note on Platt's probabilistic outputs for support vector machines</em><br />
<a href="https://www.csie.ntu.edu.tw/~htlin/paper/doc/plattprob.pdf">https://www.csie.ntu.edu.tw/~htlin/paper/doc/plattprob.pdf</a>
</p>
</li>
<li>
<p>C.-W. Hsu and C.-J. Lin <br />
<em>A comparison on methods for multi-class support vector machines</em><br />
IEEE Transactions on Neural Networks, 13(2002) 415-425.<br />
<a href="https://www.csie.ntu.edu.tw/~cjlin/papers/multisvm.pdf">https://www.csie.ntu.edu.tw/~cjlin/papers/multisvm.pdf</a>
</p>
</li>
<li>
<p>K. Crammer, Y. Singer<br />
<em>On the learnability and design of output codes for multiclass prolems</em><br />
Computational Learning Theory, 35-46, 2000.<br />
<a href="http://www.learningtheory.org/colt2000/papers/CrammerSinger.pdf">http://www.learningtheory.org/colt2000/papers/CrammerSinger.pdf</a>
</p>
</li>
<li>
<p>J. Weston, C. Watkins<br />
<em>Multi-class support vector machines</em>.
Technical Report CSD-TR-98-04,
Royal Holloway, University of London, Department of Computer Science.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+predict.ksvm">predict.ksvm</a></code>, <code><a href="#topic+ksvm-class">ksvm-class</a></code>, <code><a href="#topic+couple">couple</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>
## simple example using the spam data set
data(spam)

## create test and training set
index &lt;- sample(1:dim(spam)[1])
spamtrain &lt;- spam[index[1:floor(dim(spam)[1]/2)], ]
spamtest &lt;- spam[index[((ceiling(dim(spam)[1]/2)) + 1):dim(spam)[1]], ]

## train a support vector machine
filter &lt;- ksvm(type~.,data=spamtrain,kernel="rbfdot",
               kpar=list(sigma=0.05),C=5,cross=3)
filter

## predict mail type on the test set
mailtype &lt;- predict(filter,spamtest[,-58])

## Check results
table(mailtype,spamtest[,58])


## Another example with the famous iris data
data(iris)

## Create a kernel function using the build in rbfdot function
rbf &lt;- rbfdot(sigma=0.1)
rbf

## train a bound constraint support vector machine
irismodel &lt;- ksvm(Species~.,data=iris,type="C-bsvc",
                  kernel=rbf,C=10,prob.model=TRUE)

irismodel

## get fitted values
fitted(irismodel)

## Test on the training set with probabilities as output
predict(irismodel, iris[,-5], type="probabilities")


## Demo of the plot function
x &lt;- rbind(matrix(rnorm(120),,2),matrix(rnorm(120,mean=3),,2))
y &lt;- matrix(c(rep(1,60),rep(-1,60)))

svp &lt;- ksvm(x,y,type="C-svc")
plot(svp,data=x)


### Use kernelMatrix
K &lt;- as.kernelMatrix(crossprod(t(x)))

svp2 &lt;- ksvm(K, y, type="C-svc")

svp2

# test data
xtest &lt;- rbind(matrix(rnorm(20),,2),matrix(rnorm(20,mean=3),,2))
# test kernel matrix i.e. inner/kernel product of test data with
# Support Vectors

Ktest &lt;- as.kernelMatrix(crossprod(t(xtest),t(x[SVindex(svp2), ])))

predict(svp2, Ktest)


#### Use custom kernel 

k &lt;- function(x,y) {(sum(x*y) +1)*exp(-0.001*sum((x-y)^2))}
class(k) &lt;- "kernel"

data(promotergene)

## train svm using custom kernel
gene &lt;- ksvm(Class~.,data=promotergene[c(1:20, 80:100),],kernel=k,
             C=5,cross=5)

gene


#### Use text with string kernels
data(reuters)
is(reuters)
tsv &lt;- ksvm(reuters,rlabels,kernel="stringdot",
            kpar=list(length=5),cross=3,C=10)
tsv


## regression
# create data
x &lt;- seq(-20,20,0.1)
y &lt;- sin(x)/x + rnorm(401,sd=0.03)

# train support vector machine
regm &lt;- ksvm(x,y,epsilon=0.01,kpar=list(sigma=16),cross=3)
plot(x,y,type="l")
lines(x,predict(regm,x),col="red")
</code></pre>

<hr>
<h2 id='ksvm-class'>Class &quot;ksvm&quot; </h2><span id='topic+ksvm-class'></span><span id='topic+SVindex'></span><span id='topic+alphaindex'></span><span id='topic+prob.model'></span><span id='topic+scaling'></span><span id='topic+prior'></span><span id='topic+show'></span><span id='topic+param'></span><span id='topic+b'></span><span id='topic+obj'></span><span id='topic+nSV'></span><span id='topic+coef+2Cvm-method'></span><span id='topic+SVindex+2Cksvm-method'></span><span id='topic+alpha+2Cksvm-method'></span><span id='topic+alphaindex+2Cksvm-method'></span><span id='topic+cross+2Cksvm-method'></span><span id='topic+error+2Cksvm-method'></span><span id='topic+param+2Cksvm-method'></span><span id='topic+fitted+2Cksvm-method'></span><span id='topic+prior+2Cksvm-method'></span><span id='topic+prob.model+2Cksvm-method'></span><span id='topic+kernelf+2Cksvm-method'></span><span id='topic+kpar+2Cksvm-method'></span><span id='topic+lev+2Cksvm-method'></span><span id='topic+kcall+2Cksvm-method'></span><span id='topic+scaling+2Cksvm-method'></span><span id='topic+type+2Cksvm-method'></span><span id='topic+xmatrix+2Cksvm-method'></span><span id='topic+ymatrix+2Cksvm-method'></span><span id='topic+b+2Cksvm-method'></span><span id='topic+obj+2Cksvm-method'></span><span id='topic+nSV+2Cksvm-method'></span>

<h3>Description</h3>

<p>An S4 class containing the output (model) of the
<code>ksvm</code> Support Vector Machines function </p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("ksvm", ...)</code>
or by calls to the <code>ksvm</code> function.
</p>


<h3>Slots</h3>


<dl>
<dt><code>type</code>:</dt><dd><p>Object of class <code>"character"</code>  containing
the support vector machine type
(&quot;C-svc&quot;, &quot;nu-svc&quot;, &quot;C-bsvc&quot;, &quot;spoc-svc&quot;,
&quot;one-svc&quot;, &quot;eps-svr&quot;, &quot;nu-svr&quot;, &quot;eps-bsvr&quot;)</p>
</dd>
<dt><code>param</code>:</dt><dd><p>Object of class <code>"list"</code> containing the
Support Vector Machine parameters (C, nu, epsilon)</p>
</dd>
<dt><code>kernelf</code>:</dt><dd><p>Object of class <code>"function"</code> containing
the kernel function</p>
</dd>
<dt><code>kpar</code>:</dt><dd><p>Object of class <code>"list"</code> containing the
kernel function parameters (hyperparameters)</p>
</dd>
<dt><code>kcall</code>:</dt><dd><p>Object of class <code>"ANY"</code> containing the      <code>ksvm</code> function call</p>
</dd>
<dt><code>scaling</code>:</dt><dd><p>Object of class <code>"ANY"</code> containing the
scaling information performed on the data</p>
</dd>
<dt><code>terms</code>:</dt><dd><p>Object of class <code>"ANY"</code> containing the
terms representation of the symbolic model used (when using a formula)</p>
</dd>
<dt><code>xmatrix</code>:</dt><dd><p>Object of class <code>"input"</code> (<code>"list"</code>
for multiclass problems 
or <code>"matrix"</code> for binary classification and regression
problems) containing the support vectors calculated from
the data matrix used during computations (possibly scaled and
without NA). In the case of multi-class classification each list
entry contains the support vectors from each binary classification
problem from the one-against-one method.</p>
</dd>
<dt><code>ymatrix</code>:</dt><dd><p>Object of class <code>"output"</code>
the response <code>"matrix"</code> or <code>"factor"</code> or <code>"vector"</code> or
<code>"logical"</code></p>
</dd>
<dt><code>fitted</code>:</dt><dd><p>Object of class <code>"output"</code> with the fitted values,
predictions using the training set.</p>
</dd>
<dt><code>lev</code>:</dt><dd><p>Object of class <code>"vector"</code> with the levels of the
response (in the case of classification)</p>
</dd>
<dt><code>prob.model</code>:</dt><dd><p>Object of class <code>"list"</code> with the
class prob. model</p>
</dd>
<dt><code>prior</code>:</dt><dd><p>Object of class <code>"list"</code> with the
prior of the training set</p>
</dd>
<dt><code>nclass</code>:</dt><dd><p>Object of class <code>"numeric"</code>  containing
the number of classes (in the case of classification)</p>
</dd>
<dt><code>alpha</code>:</dt><dd><p>Object of class <code>"listI"</code> containing the
resulting alpha vector (<code>"list"</code> or <code>"matrix"</code> in case of multiclass classification) (support vectors)</p>
</dd>
<dt><code>coef</code>:</dt><dd><p>Object of class <code>"ANY"</code> containing the
resulting coefficients</p>
</dd>
<dt><code>alphaindex</code>:</dt><dd><p>Object of class <code>"list"</code> containing</p>
</dd>
<dt><code>b</code>:</dt><dd><p>Object of class <code>"numeric"</code> containing the
resulting offset </p>
</dd>
<dt><code>SVindex</code>:</dt><dd><p>Object of class <code>"vector"</code> containing
the indexes of the support vectors</p>
</dd>
<dt><code>nSV</code>:</dt><dd><p>Object of class <code>"numeric"</code> containing the
number of support vectors </p>
</dd>
<dt><code>obj</code>:</dt><dd><p>Object of class <code>vector</code> containing the value of the objective function. When using 
one-against-one in multiclass classification this is a vector.</p>
</dd>
<dt><code>error</code>:</dt><dd><p>Object of class <code>"numeric"</code> containing the
training error</p>
</dd>
<dt><code>cross</code>:</dt><dd><p>Object of class <code>"numeric"</code> containing the
cross-validation error </p>
</dd>
<dt><code>n.action</code>:</dt><dd><p>Object of class <code>"ANY"</code> containing the
action performed for NA </p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>SVindex</dt><dd><p><code>signature(object = "ksvm")</code>: return the indexes
of support vectors</p>
</dd>
<dt>alpha</dt><dd><p><code>signature(object = "ksvm")</code>: returns the complete
5    alpha vector (wit zero values)</p>
</dd>
<dt>alphaindex</dt><dd><p><code>signature(object = "ksvm")</code>: returns the
indexes of non-zero alphas (support vectors)</p>
</dd>
<dt>cross</dt><dd><p><code>signature(object = "ksvm")</code>: returns the
cross-validation error </p>
</dd>
<dt>error</dt><dd><p><code>signature(object = "ksvm")</code>: returns the training
error </p>
</dd>
<dt>obj</dt><dd><p><code>signature(object = "ksvm")</code>: returns the value of the objective function</p>
</dd>
<dt>fitted</dt><dd><p><code>signature(object = "vm")</code>: returns the fitted
values (predict on training set) </p>
</dd>
<dt>kernelf</dt><dd><p><code>signature(object = "ksvm")</code>: returns the kernel
function</p>
</dd>
<dt>kpar</dt><dd><p><code>signature(object = "ksvm")</code>: returns the kernel
parameters (hyperparameters)</p>
</dd>
<dt>lev</dt><dd><p><code>signature(object = "ksvm")</code>: returns the levels in
case of classification  </p>
</dd>
<dt>prob.model</dt><dd><p><code>signature(object="ksvm")</code>: returns class
prob. model values</p>
</dd>
<dt>param</dt><dd><p><code>signature(object="ksvm")</code>: returns 
the parameters of the SVM in a list (C, epsilon, nu etc.)</p>
</dd>
<dt>prior</dt><dd><p><code>signature(object="ksvm")</code>: returns 
the prior of the training set</p>
</dd>
<dt>kcall</dt><dd><p><code>signature(object="ksvm")</code>: returns the
<code>ksvm</code> function call</p>
</dd>
<dt>scaling</dt><dd><p><code>signature(object = "ksvm")</code>: returns the
scaling values </p>
</dd>
<dt>show</dt><dd><p><code>signature(object = "ksvm")</code>: prints the object information</p>
</dd>
<dt>type</dt><dd><p><code>signature(object = "ksvm")</code>: returns the problem type</p>
</dd>
<dt>xmatrix</dt><dd><p><code>signature(object = "ksvm")</code>: returns the data
matrix used</p>
</dd>
<dt>ymatrix</dt><dd><p><code>signature(object = "ksvm")</code>: returns the
response vector</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Alexandros Karatzoglou <br /> <a href="mailto:alexandros.karatzolgou@ci.tuwien.ac.at">alexandros.karatzolgou@ci.tuwien.ac.at</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+ksvm">ksvm</a></code>, 
<code><a href="#topic+rvm-class">rvm-class</a></code>,
<code><a href="#topic+gausspr-class">gausspr-class</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## simple example using the promotergene data set
data(promotergene)

## train a support vector machine
gene &lt;- ksvm(Class~.,data=promotergene,kernel="rbfdot",
             kpar=list(sigma=0.015),C=50,cross=4)
gene

# the kernel  function
kernelf(gene)
# the alpha values
alpha(gene)
# the coefficients
coef(gene)
# the fitted values
fitted(gene)
# the cross validation error
cross(gene)


</code></pre>

<hr>
<h2 id='lssvm'>Least Squares Support Vector Machine</h2><span id='topic+lssvm'></span><span id='topic+lssvm-methods'></span><span id='topic+lssvm+2Cformula-method'></span><span id='topic+lssvm+2Cvector-method'></span><span id='topic+lssvm+2Cmatrix-method'></span><span id='topic+lssvm+2Clist-method'></span><span id='topic+lssvm+2CkernelMatrix-method'></span><span id='topic+show+2Clssvm-method'></span><span id='topic+coef+2Clssvm-method'></span><span id='topic+predict+2Clssvm-method'></span>

<h3>Description</h3>

<p>The <code>lssvm</code> function is an
implementation of the Least Squares SVM. <code>lssvm</code> includes a
reduced version of Least Squares SVM using a decomposition of the
kernel matrix which is calculated by the <code>csi</code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
## S4 method for signature 'formula'
lssvm(x, data=NULL, ..., subset, na.action = na.omit, scaled = TRUE)

## S4 method for signature 'vector'
lssvm(x, ...)

## S4 method for signature 'matrix'
lssvm(x, y, scaled = TRUE, kernel = "rbfdot", kpar = "automatic",
      type = NULL, tau = 0.01, reduced = TRUE, tol = 0.0001,
      rank = floor(dim(x)[1]/3), delta = 40, cross = 0, fit = TRUE,
      ..., subset, na.action = na.omit)

## S4 method for signature 'kernelMatrix'
lssvm(x, y, type = NULL, tau = 0.01,
      tol = 0.0001, rank = floor(dim(x)[1]/3), delta = 40, cross = 0,
      fit = TRUE, ...)

## S4 method for signature 'list'
lssvm(x, y, scaled = TRUE,
      kernel = "stringdot", kpar = list(length=4, lambda = 0.5),
      type = NULL, tau = 0.01, reduced = TRUE, tol = 0.0001,
      rank = floor(dim(x)[1]/3), delta = 40, cross = 0, fit = TRUE,
      ..., subset)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lssvm_+3A_x">x</code></td>
<td>
<p>a symbolic description of the model to be fit, a matrix or
vector containing the training data when a formula interface is not
used or a <code>kernelMatrix</code> or a list of character vectors.</p>
</td></tr>
<tr><td><code id="lssvm_+3A_data">data</code></td>
<td>
<p>an optional data frame containing the variables in the model.
By default the variables are taken from the environment which
&lsquo;lssvm&rsquo; is called from.</p>
</td></tr>
<tr><td><code id="lssvm_+3A_y">y</code></td>
<td>
<p>a response vector with one label for each row/component of <code>x</code>. Can be either
a factor (for classification tasks) or a numeric vector (for
classification or regression - currently nor supported -).</p>
</td></tr>
<tr><td><code id="lssvm_+3A_scaled">scaled</code></td>
<td>
<p>A logical vector indicating the variables to be
scaled. If <code>scaled</code> is of length 1, the value is recycled as
many times as needed and all non-binary variables are scaled.
Per default, data are scaled internally to zero mean and unit
variance. The center and scale values are returned and used for later predictions.</p>
</td></tr>
<tr><td><code id="lssvm_+3A_type">type</code></td>
<td>
<p>Type of problem. Either &quot;classification&quot; or &quot;regression&quot;.
Depending on whether <code>y</code> is a factor or not, the default
setting for <code>type</code> is &quot;classification&quot; or &quot;regression&quot; respectively,
but can be overwritten by setting an explicit value. (regression is
currently not supported)<br /></p>
</td></tr>
<tr><td><code id="lssvm_+3A_kernel">kernel</code></td>
<td>
<p>the kernel function used in training and predicting.
This parameter can be set to any function, of class kernel, which computes a dot product between two
vector arguments. kernlab provides the most popular kernel functions
which can be used by setting the kernel parameter to the following
strings:
</p>

<ul>
<li> <p><code>rbfdot</code> Radial Basis kernel &quot;Gaussian&quot;
</p>
</li>
<li> <p><code>polydot</code> Polynomial kernel 
</p>
</li>
<li> <p><code>vanilladot</code> Linear kernel 
</p>
</li>
<li> <p><code>tanhdot</code> Hyperbolic tangent kernel 
</p>
</li>
<li> <p><code>laplacedot</code> Laplacian kernel 
</p>
</li>
<li> <p><code>besseldot</code> Bessel kernel 
</p>
</li>
<li> <p><code>anovadot</code> ANOVA RBF kernel 
</p>
</li>
<li> <p><code>splinedot</code> Spline kernel
</p>
</li>
<li> <p><code>stringdot</code> String kernel
</p>
</li></ul>

<p>Setting the kernel parameter to &quot;matrix&quot; treats <code>x</code> as a kernel
matrix calling the <code>kernelMatrix</code> interface.<br />
</p>
<p>The kernel parameter can also be set to a user defined function of
class kernel by passing the function name as an argument.
</p>
</td></tr>
<tr><td><code id="lssvm_+3A_kpar">kpar</code></td>
<td>

<p>the list of hyper-parameters (kernel parameters).
This is a list which contains the parameters to be used with the
kernel function. For valid parameters for existing kernels are :
</p>

<ul>
<li> <p><code>sigma</code> inverse kernel width for the Radial Basis
kernel function &quot;rbfdot&quot; and the Laplacian kernel &quot;laplacedot&quot;.
</p>
</li>
<li> <p><code>degree, scale, offset</code> for the Polynomial kernel &quot;polydot&quot;
</p>
</li>
<li> <p><code>scale, offset</code> for the Hyperbolic tangent kernel
function &quot;tanhdot&quot;
</p>
</li>
<li> <p><code>sigma, order, degree</code> for the Bessel kernel &quot;besseldot&quot;. 
</p>
</li>
<li> <p><code>sigma, degree</code> for the ANOVA kernel &quot;anovadot&quot;.
</p>
</li>
<li> <p><code>length, lambda, normalized</code> for the &quot;stringdot&quot; kernel
where length is the length of the strings considered, lambda the
decay factor and normalized a logical parameter determining if the
kernel evaluations should be normalized.
</p>
</li></ul>

<p>Hyper-parameters for user defined kernels can be passed through the
kpar parameter as well.<br />
</p>
<p><code>kpar</code> can also be set to the string &quot;automatic&quot; which uses the heuristics in 
<code><a href="#topic+sigest">sigest</a></code> to calculate a good <code>sigma</code> value for the
Gaussian RBF or Laplace kernel, from the data. (default = &quot;automatic&quot;).
</p>
</td></tr>
<tr><td><code id="lssvm_+3A_tau">tau</code></td>
<td>
<p>the regularization parameter (default 0.01) </p>
</td></tr>
<tr><td><code id="lssvm_+3A_reduced">reduced</code></td>
<td>
<p>if set to <code>FALSE</code> the full linear problem of the
lssvm is solved, when <code>TRUE</code> a reduced method using <code>csi</code> is used.</p>
</td></tr>
<tr><td><code id="lssvm_+3A_rank">rank</code></td>
<td>
<p>the maximal rank of the decomposed kernel matrix, see
<code>csi</code></p>
</td></tr>
<tr><td><code id="lssvm_+3A_delta">delta</code></td>
<td>
<p>number of columns of cholesky performed in advance, see
<code>csi</code> (default 40)</p>
</td></tr>
<tr><td><code id="lssvm_+3A_tol">tol</code></td>
<td>
<p>tolerance of termination criterion for the <code>csi</code>
function, lower tolerance leads to more precise approximation but
may increase the training time and the decomposed matrix size (default: 0.0001)</p>
</td></tr>
<tr><td><code id="lssvm_+3A_fit">fit</code></td>
<td>
<p>indicates whether the fitted values should be computed and
included in the model or not (default: 'TRUE')</p>
</td></tr>
<tr><td><code id="lssvm_+3A_cross">cross</code></td>
<td>
<p>if a integer value k&gt;0 is specified, a k-fold cross
validation on the training data is performed to assess the
quality of the model: the Mean Squared Error for regression</p>
</td></tr>
<tr><td><code id="lssvm_+3A_subset">subset</code></td>
<td>
<p>An index vector specifying the cases to be used in the
training sample.  (NOTE: If given, this argument must be
named.)</p>
</td></tr>
<tr><td><code id="lssvm_+3A_na.action">na.action</code></td>
<td>
<p>A function to specify the action to be taken if <code>NA</code>s are
found. The default action is <code>na.omit</code>, which leads to rejection of cases
with missing values on any required variable. An alternative
is <code>na.fail</code>, which causes an error if <code>NA</code> cases
are found. (NOTE: If given, this argument must be named.)</p>
</td></tr>
<tr><td><code id="lssvm_+3A_...">...</code></td>
<td>
<p> additional parameters</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Least Squares Support Vector Machines are reformulation to the
standard SVMs that lead to solving linear KKT systems.
The algorithm is based on the minimization of a classical penalized
least-squares cost function. The current implementation approximates
the kernel matrix by an incomplete Cholesky factorization obtained by
the <code><a href="#topic+csi">csi</a></code> function, thus the solution is an approximation
to the exact solution of the lssvm optimization problem. The quality
of the solution depends on the approximation and can be influenced by
the &quot;rank&quot; , &quot;delta&quot;, and &quot;tol&quot; parameters.
</p>


<h3>Value</h3>

<p>An S4 object of class <code>"lssvm"</code> containing the fitted model,
Accessor functions can be used to access the slots of the object (see
examples) which include:
</p>
<table>
<tr><td><code>alpha</code></td>
<td>
<p>the parameters of the <code>"lssvm"</code></p>
</td></tr>
<tr><td><code>coef</code></td>
<td>
<p>the model coefficients (identical to alpha)</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>the model offset.</p>
</td></tr>
<tr><td><code>xmatrix</code></td>
<td>
<p>the training data used by the model</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Alexandros Karatzoglou <br /> <a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>References</h3>

<p>J. A. K. Suykens and  J. Vandewalle<br />
<em>Least Squares Support Vector Machine Classifiers</em><br />
Neural Processing Letters vol. 9, issue 3, June 1999<br />
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ksvm">ksvm</a></code>, <code><a href="#topic+gausspr">gausspr</a></code>, <code><a href="#topic+csi">csi</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## simple example
data(iris)

lir &lt;- lssvm(Species~.,data=iris)

lir

lirr &lt;- lssvm(Species~.,data= iris, reduced = FALSE)

lirr

## Using the kernelMatrix interface

iris &lt;- unique(iris)

rbf &lt;- rbfdot(0.5)

k &lt;- kernelMatrix(rbf, as.matrix(iris[,-5]))

klir &lt;- lssvm(k, iris[, 5])

klir

pre &lt;- predict(klir, k)
</code></pre>

<hr>
<h2 id='lssvm-class'>Class &quot;lssvm&quot;</h2><span id='topic+lssvm-class'></span><span id='topic+alpha+2Clssvm-method'></span><span id='topic+b+2Clssvm-method'></span><span id='topic+cross+2Clssvm-method'></span><span id='topic+error+2Clssvm-method'></span><span id='topic+kcall+2Clssvm-method'></span><span id='topic+kernelf+2Clssvm-method'></span><span id='topic+kpar+2Clssvm-method'></span><span id='topic+param+2Clssvm-method'></span><span id='topic+lev+2Clssvm-method'></span><span id='topic+type+2Clssvm-method'></span><span id='topic+alphaindex+2Clssvm-method'></span><span id='topic+xmatrix+2Clssvm-method'></span><span id='topic+ymatrix+2Clssvm-method'></span><span id='topic+scaling+2Clssvm-method'></span><span id='topic+nSV+2Clssvm-method'></span>

<h3>Description</h3>

<p>The Gaussian Processes object </p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("lssvm", ...)</code>.
or by calling the <code>lssvm</code> function 
</p>


<h3>Slots</h3>


<dl>
<dt><code>kernelf</code>:</dt><dd><p>Object of class <code>"kfunction"</code> contains
the kernel function used</p>
</dd>
<dt><code>kpar</code>:</dt><dd><p>Object of class <code>"list"</code> contains the
kernel parameter used </p>
</dd>
<dt><code>param</code>:</dt><dd><p>Object of class <code>"list"</code> contains the
regularization parameter used.</p>
</dd>
<dt><code>kcall</code>:</dt><dd><p>Object of class <code>"call"</code> contains the used
function call </p>
</dd>
<dt><code>type</code>:</dt><dd><p>Object of class <code>"character"</code> contains
type of problem </p>
</dd>
<dt><code>coef</code>:</dt><dd><p>Object of class <code>"ANY"</code> contains
the model parameter </p>
</dd>
<dt><code>terms</code>:</dt><dd><p>Object of class <code>"ANY"</code> contains the
terms representation of the symbolic model used (when using a formula)</p>
</dd>
<dt><code>xmatrix</code>:</dt><dd><p>Object of class <code>"matrix"</code> containing
the data matrix used </p>
</dd>
<dt><code>ymatrix</code>:</dt><dd><p>Object of class <code>"output"</code> containing the
response matrix</p>
</dd>
<dt><code>fitted</code>:</dt><dd><p>Object of class <code>"output"</code> containing the
fitted values  </p>
</dd>
<dt><code>b</code>:</dt><dd><p>Object of class <code>"numeric"</code> containing the
offset </p>
</dd>
<dt><code>lev</code>:</dt><dd><p>Object of class <code>"vector"</code> containing the
levels of the response (in case of classification) </p>
</dd>
<dt><code>scaling</code>:</dt><dd><p>Object of class <code>"ANY"</code> containing the
scaling information performed on the data</p>
</dd>
<dt><code>nclass</code>:</dt><dd><p>Object of class <code>"numeric"</code> containing
the number of classes (in case of classification) </p>
</dd>
<dt><code>alpha</code>:</dt><dd><p>Object of class <code>"listI"</code> containing the
computes alpha values </p>
</dd>
<dt><code>alphaindex</code></dt><dd><p>Object of class <code>"list"</code> containing
the indexes for the alphas in various classes (in multi-class problems).</p>
</dd>
<dt><code>error</code>:</dt><dd><p>Object of class <code>"numeric"</code> containing the
training error</p>
</dd>
<dt><code>cross</code>:</dt><dd><p>Object of class <code>"numeric"</code> containing the
cross validation error</p>
</dd>
<dt><code>n.action</code>:</dt><dd><p>Object of class <code>"ANY"</code> containing the
action performed in NA </p>
</dd>
<dt><code>nSV</code>:</dt><dd><p>Object of class <code>"numeric"</code> containing the
number of model parameters </p>
</dd>
</dl>



<h3>Methods</h3>

 
<dl>
<dt>alpha</dt><dd><p><code>signature(object = "lssvm")</code>: returns the alpha
vector</p>
</dd>
<dt>cross</dt><dd><p><code>signature(object = "lssvm")</code>: returns the cross
validation error </p>
</dd>
<dt>error</dt><dd><p><code>signature(object = "lssvm")</code>: returns the
training error </p>
</dd>
<dt>fitted</dt><dd><p><code>signature(object = "vm")</code>: returns the fitted values </p>
</dd>
<dt>kcall</dt><dd><p><code>signature(object = "lssvm")</code>: returns the call performed</p>
</dd>
<dt>kernelf</dt><dd><p><code>signature(object = "lssvm")</code>: returns the
kernel function used</p>
</dd>
<dt>kpar</dt><dd><p><code>signature(object = "lssvm")</code>: returns the kernel
parameter used</p>
</dd>
<dt>param</dt><dd><p><code>signature(object = "lssvm")</code>: returns the regularization
parameter used</p>
</dd>
<dt>lev</dt><dd><p><code>signature(object = "lssvm")</code>: returns the
response levels (in classification) </p>
</dd>
<dt>type</dt><dd><p><code>signature(object = "lssvm")</code>: returns the type
of problem</p>
</dd>
<dt>scaling</dt><dd><p><code>signature(object = "ksvm")</code>: returns the
scaling values </p>
</dd>
<dt>xmatrix</dt><dd><p><code>signature(object = "lssvm")</code>: returns the
data matrix used</p>
</dd>
<dt>ymatrix</dt><dd><p><code>signature(object = "lssvm")</code>: returns the
response matrix used</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Alexandros Karatzoglou<br /> <a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+lssvm">lssvm</a></code>, 
<code><a href="#topic+ksvm-class">ksvm-class</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# train model
data(iris)
test &lt;- lssvm(Species~.,data=iris,var=2)
test
alpha(test)
error(test)
lev(test)
</code></pre>

<hr>
<h2 id='musk'>Musk data set</h2><span id='topic+musk'></span>

<h3>Description</h3>

<p>This dataset describes a set of 92 molecules of which 47 are judged
by human experts to be musks and the remaining 45 molecules are
judged to be non-musks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(musk)</code></pre>


<h3>Format</h3>

<p>A data frame with 476 observations on the following 167 variables.
</p>
<p>Variables 1-162  are &quot;distance features&quot; along rays.  The distances are
measured in hundredths of Angstroms.  The distances may be negative or
positive, since they are actually measured relative to an origin placed
along each ray.  The origin was defined by a &quot;consensus musk&quot; surface
that is no longer used.  Hence, any experiments with the data should
treat these feature values as lying on an arbitrary continuous scale.  In
particular, the algorithm should not make any use of the zero point or
the sign of each feature value. 
</p>
<p>Variable 163 is the distance of the oxygen atom in the molecule to a
designated point in 3-space. This is also called OXY-DIS.
</p>
<p>Variable 164  is the X-displacement from the designated point.
</p>
<p>Variable 165 is the Y-displacement from the designated point.
</p>
<p>Variable 166 is the  Z-displacement from the designated point. 
</p>
<p>Class: 0 for non-musk, and 1 for musk
</p>


<h3>Source</h3>

<p>UCI Machine Learning data repository <br />
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(musk)

muskm &lt;- ksvm(Class~.,data=musk,kernel="rbfdot",C=1000)

muskm

</code></pre>

<hr>
<h2 id='onlearn'>Kernel Online Learning algorithms</h2><span id='topic+onlearn'></span><span id='topic+onlearn+2Conlearn-method'></span>

<h3>Description</h3>

<p>Online Kernel-based Learning algorithms for classification, novelty
detection, and regression. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'onlearn'
onlearn(obj, x, y = NULL, nu = 0.2, lambda = 1e-04)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="onlearn_+3A_obj">obj</code></td>
<td>
<p><code>obj</code> an object of class <code>onlearn</code> created by the
initialization function <code>inlearn</code> containing the kernel to be
used during learning and the parameters of the
learned model</p>
</td></tr>
<tr><td><code id="onlearn_+3A_x">x</code></td>
<td>
<p>vector or matrix containing the data. Factors have
to be numerically coded. If <code>x</code> is a matrix the code is
run internally one sample at the time.</p>
</td></tr>
<tr><td><code id="onlearn_+3A_y">y</code></td>
<td>
<p>the class label in case of classification. Only binary
classification is supported and class labels have to be -1 or +1.
</p>
</td></tr>
<tr><td><code id="onlearn_+3A_nu">nu</code></td>
<td>
<p>the parameter similarly to the <code>nu</code> parameter in SVM
bounds the training error.</p>
</td></tr>
<tr><td><code id="onlearn_+3A_lambda">lambda</code></td>
<td>
<p>the learning rate</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The online algorithms are based on a simple stochastic gradient descent
method in feature space.
The state of the algorithm is stored in an object of class
<code>onlearn</code> and has to be passed to the function at each iteration.
</p>


<h3>Value</h3>

<p>The function returns an <code>S4</code> object of class <code>onlearn</code>
containing the model parameters and the last fitted value which can be
retrieved by the accessor method <code>fit</code>. The value returned in the
classification and novelty detection problem is the decision function
value phi.
The accessor methods <code>alpha</code> returns the model parameters. 
</p>


<h3>Author(s)</h3>

<p>Alexandros Karatzoglou<br />
<a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>References</h3>

<p> Kivinen J. Smola A.J. Williamson R.C. <br />
<em>Online Learning with Kernels</em><br />
IEEE Transactions on Signal Processing vol. 52, Issue 8, 2004<br />
<a href="https://alex.smola.org/papers/2004/KivSmoWil04.pdf">https://alex.smola.org/papers/2004/KivSmoWil04.pdf</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+inlearn">inlearn</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
## create toy data set
x &lt;- rbind(matrix(rnorm(100),,2),matrix(rnorm(100)+3,,2))
y &lt;- matrix(c(rep(1,50),rep(-1,50)),,1)

## initialize onlearn object
on &lt;- inlearn(2,kernel="rbfdot",kpar=list(sigma=0.2),
              type="classification")

ind &lt;- sample(1:100,100)
## learn one data point at the time
for(i in ind)
on &lt;- onlearn(on,x[i,],y[i],nu=0.03,lambda=0.1)

## or learn all the data 
on &lt;- onlearn(on,x[ind,],y[ind],nu=0.03,lambda=0.1)

sign(predict(on,x))
</code></pre>

<hr>
<h2 id='onlearn-class'>Class &quot;onlearn&quot;</h2><span id='topic+onlearn-class'></span><span id='topic+alpha+2Conlearn-method'></span><span id='topic+b+2Conlearn-method'></span><span id='topic+buffer+2Conlearn-method'></span><span id='topic+fit+2Conlearn-method'></span><span id='topic+kernelf+2Conlearn-method'></span><span id='topic+kpar+2Conlearn-method'></span><span id='topic+predict+2Conlearn-method'></span><span id='topic+rho+2Conlearn-method'></span><span id='topic+rho'></span><span id='topic+show+2Conlearn-method'></span><span id='topic+type+2Conlearn-method'></span><span id='topic+xmatrix+2Conlearn-method'></span><span id='topic+buffer'></span>

<h3>Description</h3>

<p> The class of objects used by the Kernel-based Online
learning algorithms</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("onlearn", ...)</code>.
or by calls to the function <code>inlearn</code>.
</p>


<h3>Slots</h3>


<dl>
<dt><code>kernelf</code>:</dt><dd><p>Object of class <code>"function"</code> containing
the used kernel function</p>
</dd>
<dt><code>buffer</code>:</dt><dd><p>Object of class <code>"numeric"</code> containing
the size of the buffer</p>
</dd>
<dt><code>kpar</code>:</dt><dd><p>Object of class <code>"list"</code> containing the
hyperparameters of the kernel function.</p>
</dd>
<dt><code>xmatrix</code>:</dt><dd><p>Object of class <code>"matrix"</code> containing
the data points (similar to support vectors) </p>
</dd>
<dt><code>fit</code>:</dt><dd><p>Object of class <code>"numeric"</code> containing the
decision function value of the last data point</p>
</dd>
<dt><code>onstart</code>:</dt><dd><p>Object of class <code>"numeric"</code> used for indexing </p>
</dd>
<dt><code>onstop</code>:</dt><dd><p>Object of class <code>"numeric"</code> used for indexing</p>
</dd>
<dt><code>alpha</code>:</dt><dd><p>Object of class <code>"ANY"</code> containing the
model parameters</p>
</dd>
<dt><code>rho</code>:</dt><dd><p>Object of class <code>"numeric"</code> containing model
parameter</p>
</dd>
<dt><code>b</code>:</dt><dd><p>Object of class <code>"numeric"</code> containing the offset</p>
</dd>
<dt><code>pattern</code>:</dt><dd><p>Object of class <code>"factor"</code> used for
dealing with factors</p>
</dd>
<dt><code>type</code>:</dt><dd><p>Object of class <code>"character"</code> containing
the problem type (classification, regression, or novelty </p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>alpha</dt><dd><p><code>signature(object = "onlearn")</code>: returns the model
parameters</p>
</dd>
<dt>b</dt><dd><p><code>signature(object = "onlearn")</code>: returns the offset </p>
</dd>
<dt>buffer</dt><dd><p><code>signature(object = "onlearn")</code>: returns the
buffer size</p>
</dd>
<dt>fit</dt><dd><p><code>signature(object = "onlearn")</code>: returns the last
decision function value</p>
</dd>
<dt>kernelf</dt><dd><p><code>signature(object = "onlearn")</code>: return the
kernel function used</p>
</dd>
<dt>kpar</dt><dd><p><code>signature(object = "onlearn")</code>: returns the
hyper-parameters used</p>
</dd>
<dt>onlearn</dt><dd><p><code>signature(obj = "onlearn")</code>: the learning function</p>
</dd>
<dt>predict</dt><dd><p><code>signature(object = "onlearn")</code>: the predict function</p>
</dd>
<dt>rho</dt><dd><p><code>signature(object = "onlearn")</code>: returns model parameter</p>
</dd>
<dt>show</dt><dd><p><code>signature(object = "onlearn")</code>: show function</p>
</dd>
<dt>type</dt><dd><p><code>signature(object = "onlearn")</code>: returns the type
of problem</p>
</dd>
<dt>xmatrix</dt><dd><p><code>signature(object = "onlearn")</code>: returns the
stored data points</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Alexandros Karatzoglou<br />
<a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+onlearn">onlearn</a></code>, <code><a href="#topic+inlearn">inlearn</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## create toy data set
x &lt;- rbind(matrix(rnorm(100),,2),matrix(rnorm(100)+3,,2))
y &lt;- matrix(c(rep(1,50),rep(-1,50)),,1)

## initialize onlearn object
on &lt;- inlearn(2,kernel="rbfdot",kpar=list(sigma=0.2),
              type="classification")

## learn one data point at the time
for(i in sample(1:100,100))
on &lt;- onlearn(on,x[i,],y[i],nu=0.03,lambda=0.1)

sign(predict(on,x))

</code></pre>

<hr>
<h2 id='plot'>plot method for support vector object</h2><span id='topic+plot.ksvm'></span><span id='topic+plot+2Cksvm+2Cmissing-method'></span><span id='topic+plot+2Cksvm-method'></span>

<h3>Description</h3>

<p>Plot a binary classification support vector machine object.
The <code>plot</code> function returns a contour plot of the decision values. </p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'ksvm'
plot(object, data=NULL, grid = 50, slice = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_+3A_object">object</code></td>
<td>
<p>a <code>ksvm</code> classification object created by the
<code>ksvm</code> function</p>
</td></tr>
<tr><td><code id="plot_+3A_data">data</code></td>
<td>
<p>a data frame or matrix containing data to be plotted</p>
</td></tr>
<tr><td><code id="plot_+3A_grid">grid</code></td>
<td>
<p>granularity for the contour plot.</p>
</td></tr>
<tr><td><code id="plot_+3A_slice">slice</code></td>
<td>
<p>a list of named numeric values for the dimensions held
constant (only needed if more than two variables are
used). Dimensions not specified are fixed at 0. </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Alexandros Karatzoglou<br />
<a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+ksvm">ksvm</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Demo of the plot function
x &lt;- rbind(matrix(rnorm(120),,2),matrix(rnorm(120,mean=3),,2))
y &lt;- matrix(c(rep(1,60),rep(-1,60)))

svp &lt;- ksvm(x,y,type="C-svc")
plot(svp,data=x)

</code></pre>

<hr>
<h2 id='prc-class'>Class &quot;prc&quot;</h2><span id='topic+prc-class'></span><span id='topic+eig'></span><span id='topic+pcv'></span><span id='topic+eig+2Cprc-method'></span><span id='topic+kcall+2Cprc-method'></span><span id='topic+kernelf+2Cprc-method'></span><span id='topic+pcv+2Cprc-method'></span><span id='topic+xmatrix+2Cprc-method'></span>

<h3>Description</h3>

<p>Principal Components Class</p>


<h3>Objects of class &quot;prc&quot;</h3>

<p>Objects from the class cannot be created directly but only contained
in other classes.</p>


<h3>Slots</h3>


<dl>
<dt><code>pcv</code>:</dt><dd><p>Object of class <code>"matrix"</code> containing the
principal component vectors </p>
</dd>
<dt><code>eig</code>:</dt><dd><p>Object of class <code>"vector"</code> containing the
corresponding eigenvalues</p>
</dd>
<dt><code>kernelf</code>:</dt><dd><p>Object of class <code>"kfunction"</code> containing
the kernel function used</p>
</dd>
<dt><code>kpar</code>:</dt><dd><p>Object of class <code>"list"</code> containing the
kernel parameters used </p>
</dd>
<dt><code>xmatrix</code>:</dt><dd><p>Object of class <code>"input"</code> containing
the data matrix used </p>
</dd>
<dt><code>kcall</code>:</dt><dd><p>Object of class <code>"ANY"</code> containing the
function call </p>
</dd>
<dt><code>n.action</code>:</dt><dd><p>Object of class <code>"ANY"</code> containing the
action performed on NA </p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>eig</dt><dd><p><code>signature(object = "prc")</code>: returns the eigenvalues </p>
</dd>
<dt>kcall</dt><dd><p><code>signature(object = "prc")</code>: returns the
performed call</p>
</dd>
<dt>kernelf</dt><dd><p><code>signature(object = "prc")</code>: returns the used
kernel function</p>
</dd>
<dt>pcv</dt><dd><p><code>signature(object = "prc")</code>: returns the principal
component vectors </p>
</dd>
<dt>predict</dt><dd><p><code>signature(object = "prc")</code>: embeds new data </p>
</dd>
<dt>xmatrix</dt><dd><p><code>signature(object = "prc")</code>: returns the used
data matrix </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Alexandros Karatzoglou<br /> <a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+kpca-class">kpca-class</a></code>,<code><a href="#topic+kha-class">kha-class</a></code>, <code><a href="#topic+kfa-class">kfa-class</a></code> 
</p>

<hr>
<h2 id='predict.gausspr'>predict method for Gaussian Processes object</h2><span id='topic+predict.gausspr'></span><span id='topic+predict+2Cgausspr-method'></span>

<h3>Description</h3>

<p>Prediction of test data using Gaussian Processes</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'gausspr'
predict(object, newdata, type = "response", coupler = "minpair")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.gausspr_+3A_object">object</code></td>
<td>
<p>an S4 object of class <code>gausspr</code> created by the
<code>gausspr</code> function</p>
</td></tr>
<tr><td><code id="predict.gausspr_+3A_newdata">newdata</code></td>
<td>
<p>a data frame or matrix containing new data</p>
</td></tr>
<tr><td><code id="predict.gausspr_+3A_type">type</code></td>
<td>
<p>one of <code>response</code>, <code>probabilities</code> 
indicating the type of output: predicted values or matrix of class
probabilities</p>
</td></tr>
<tr><td><code id="predict.gausspr_+3A_coupler">coupler</code></td>
<td>
<p>Coupling method used in the multiclass case, can be one
of <code>minpair</code> or <code>pkpd</code> (see reference for more details).</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>response</code></td>
<td>
<p>predicted classes (the classes with majority vote)
or the response value in regression.</p>
</td></tr>
<tr><td><code>probabilities</code></td>
<td>
<p>matrix of class probabilities (one column for each class and
one row for each input).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Alexandros Karatzoglou<br />
<a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>References</h3>


<ul>
<li>
<p>C. K. I. Williams and D. Barber <br />
Bayesian classification with Gaussian processes. <br />
IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(12):1342-1351, 1998<br />
<a href="https://homepages.inf.ed.ac.uk/ckiw/postscript/pami_final.ps.gz">https://homepages.inf.ed.ac.uk/ckiw/postscript/pami_final.ps.gz</a>
</p>
</li>
<li>
<p>T.F. Wu, C.J. Lin, R.C. Weng. <br />
<em>Probability estimates for Multi-class Classification by
Pairwise Coupling</em><br />
<a href="https://www.csie.ntu.edu.tw/~cjlin/papers/svmprob/svmprob.pdf">https://www.csie.ntu.edu.tw/~cjlin/papers/svmprob/svmprob.pdf</a>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
## example using the promotergene data set
data(promotergene)

## create test and training set
ind &lt;- sample(1:dim(promotergene)[1],20)
genetrain &lt;- promotergene[-ind, ]
genetest &lt;- promotergene[ind, ]

## train a support vector machine
gene &lt;- gausspr(Class~.,data=genetrain,kernel="rbfdot",
                kpar=list(sigma=0.015))
gene

## predict gene type probabilities on the test set
genetype &lt;- predict(gene,genetest,type="probabilities")
genetype
</code></pre>

<hr>
<h2 id='predict.kqr'>Predict method for kernel Quantile Regression object</h2><span id='topic+predict.kqr'></span><span id='topic+predict+2Ckqr-method'></span>

<h3>Description</h3>

<p>Prediction of test data for kernel quantile regression</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'kqr'
predict(object, newdata)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.kqr_+3A_object">object</code></td>
<td>
<p>an S4 object of class <code>kqr</code> created by the
<code>kqr</code> function</p>
</td></tr>
<tr><td><code id="predict.kqr_+3A_newdata">newdata</code></td>
<td>
<p>a data frame, matrix, or kernelMatrix containing new data</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The value of the quantile given by the computed <code>kqr</code>
model in a vector of length equal to the the rows of <code>newdata</code>.
</p>


<h3>Author(s)</h3>

<p>Alexandros Karatzoglou<br />
<a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>Examples</h3>

<pre><code class='language-R'># create data
x &lt;- sort(runif(300))
y &lt;- sin(pi*x) + rnorm(300,0,sd=exp(sin(2*pi*x)))

# first calculate the median
qrm &lt;- kqr(x, y, tau = 0.5, C=0.15)

# predict and plot
plot(x, y)
ytest &lt;- predict(qrm, x)
lines(x, ytest, col="blue")

# calculate 0.9 quantile
qrm &lt;- kqr(x, y, tau = 0.9, kernel = "rbfdot",
           kpar= list(sigma=10), C=0.15)
ytest &lt;- predict(qrm, x)
lines(x, ytest, col="red")
</code></pre>

<hr>
<h2 id='predict.ksvm'>predict method for support vector object</h2><span id='topic+predict.ksvm'></span><span id='topic+predict+2Cksvm-method'></span>

<h3>Description</h3>

<p>Prediction of test data using support vector machines</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'ksvm'
predict(object, newdata, type = "response", coupler = "minpair")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.ksvm_+3A_object">object</code></td>
<td>
<p>an S4 object of class <code>ksvm</code> created by the
<code>ksvm</code> function</p>
</td></tr>
<tr><td><code id="predict.ksvm_+3A_newdata">newdata</code></td>
<td>
<p>a data frame or matrix containing new data</p>
</td></tr>
<tr><td><code id="predict.ksvm_+3A_type">type</code></td>
<td>
<p>one of <code>response</code>, <code>probabilities</code>
,<code>votes</code>, <code>decision</code>
indicating the type of output: predicted values, matrix of class
probabilities, matrix of vote counts, or matrix of decision values.</p>
</td></tr>
<tr><td><code id="predict.ksvm_+3A_coupler">coupler</code></td>
<td>
<p>Coupling method used in the multiclass case, can be one
of <code>minpair</code> or <code>pkpd</code> (see reference for more details).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If <code>type(object)</code> is <code>C-svc</code>,
<code>nu-svc</code>, <code>C-bsvm</code> or <code>spoc-svc</code>
the vector returned depends on the argument <code>type</code>:
</p>
<table>
<tr><td><code>response</code></td>
<td>
<p>predicted classes (the classes with majority vote).</p>
</td></tr>
<tr><td><code>probabilities</code></td>
<td>
<p>matrix of class probabilities (one column for each class and
one row for each input).</p>
</td></tr>
<tr><td><code>votes</code></td>
<td>
<p>matrix of vote counts (one column for each class and one row
for each new input)</p>
</td></tr>
</table>
<p>If <code>type(object)</code> is <code>eps-svr</code>, <code>eps-bsvr</code> or
<code>nu-svr</code> a vector of predicted values is returned.
If <code>type(object)</code> is <code>one-classification</code> a vector of
logical values is returned.
</p>


<h3>Author(s)</h3>

<p>Alexandros Karatzoglou<br />
<a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>References</h3>


<ul>
<li>
<p>T.F. Wu, C.J. Lin, R.C. Weng. <br />
<em>Probability estimates for Multi-class Classification by
Pairwise Coupling</em><br />
<a href="https://www.csie.ntu.edu.tw/~cjlin/papers/svmprob/svmprob.pdf">https://www.csie.ntu.edu.tw/~cjlin/papers/svmprob/svmprob.pdf</a>
</p>
</li>
<li>
<p>H.T. Lin, C.J. Lin, R.C. Weng (2007),
A note on Platt's probabilistic outputs for support vector
machines.
<em>Machine Learning</em>, <b>68</b>, 267&ndash;276.
<a href="https://doi.org/10.1007/s10994-007-5018-6">doi:10.1007/s10994-007-5018-6</a>.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
## example using the promotergene data set
data(promotergene)

## create test and training set
ind &lt;- sample(1:dim(promotergene)[1],20)
genetrain &lt;- promotergene[-ind, ]
genetest &lt;- promotergene[ind, ]

## train a support vector machine
gene &lt;- ksvm(Class~.,data=genetrain,kernel="rbfdot",
             kpar=list(sigma=0.015),C=70,cross=4,prob.model=TRUE)
gene

## predict gene type probabilities on the test set
genetype &lt;- predict(gene,genetest,type="probabilities")
genetype
</code></pre>

<hr>
<h2 id='promotergene'>E. coli promoter gene sequences (DNA)</h2><span id='topic+promotergene'></span>

<h3>Description</h3>

<p>Promoters have a region where a protein (RNA polymerase) must make contact
and the helical DNA sequence must have a valid conformation so that
the two pieces of the contact region spatially align.
The data contains DNA sequences of promoters and non-promoters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(promotergene)</code></pre>


<h3>Format</h3>

<p>A data frame with 106 observations and 58 variables.
The first variable <code>Class</code> is a factor with levels <code>+</code> for a promoter gene  
and <code>-</code> for a non-promoter gene. 
The remaining 57 variables <code>V2 to V58</code> are factors describing the sequence. 
The DNA bases are coded as follows: <code>a</code> adenine <code>c</code> cytosine <code>g</code> 
guanine <code>t</code> thymine
</p>


<h3>Source</h3>

<p>UCI Machine Learning data repository <br />
<a href="https://archive.ics.uci.edu/ml/machine-learning-databases/molecular-biology/promoter-gene-sequences/">https://archive.ics.uci.edu/ml/machine-learning-databases/molecular-biology/promoter-gene-sequences/</a>
</p>


<h3>References</h3>

<p>Towell, G., Shavlik, J. and Noordewier, M. <br />
<em>Refinement of Approximate Domain Theories by Knowledge-Based
Artificial Neural Networks.</em> <br />
In Proceedings of the Eighth National Conference on Artificial Intelligence (AAAI-90)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(promotergene)

## Create classification model using Gaussian Processes

prom &lt;- gausspr(Class~.,data=promotergene,kernel="rbfdot",
                kpar=list(sigma=0.02),cross=4)
prom

## Create model using Support Vector Machines

promsv &lt;- ksvm(Class~.,data=promotergene,kernel="laplacedot",
               kpar="automatic",C=60,cross=4)
promsv
</code></pre>

<hr>
<h2 id='ranking'>Ranking</h2><span id='topic+ranking'></span><span id='topic+ranking+2Cmatrix-method'></span><span id='topic+ranking+2Clist-method'></span><span id='topic+ranking+2CkernelMatrix-method'></span>

<h3>Description</h3>

<p>A universal ranking algorithm which assigns importance/ranking to data points
given a query.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'matrix'
ranking(x, y,
        kernel ="rbfdot", kpar = list(sigma = 1),
        scale = FALSE, alpha = 0.99, iterations = 600,
        edgegraph = FALSE, convergence = FALSE ,...)

## S4 method for signature 'kernelMatrix'
ranking(x, y,
        alpha = 0.99, iterations = 600, convergence = FALSE,...)

## S4 method for signature 'list'
ranking(x, y,
        kernel = "stringdot", kpar = list(length = 4, lambda = 0.5),
        alpha = 0.99, iterations = 600, convergence = FALSE, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ranking_+3A_x">x</code></td>
<td>
<p>a matrix containing the data to be ranked, or the kernel
matrix of data to be ranked or a list of character vectors</p>
</td></tr>
<tr><td><code id="ranking_+3A_y">y</code></td>
<td>
<p>The index of the query point in the data matrix or a vector
of length equal to the rows of the data matrix having a one at the
index of the query points index and zero at all the other points.</p>
</td></tr>
<tr><td><code id="ranking_+3A_kernel">kernel</code></td>
<td>
<p>the kernel function used in training and predicting.
This parameter can be set to any function, of class kernel, which computes a dot product between two
vector arguments. kernlab provides the most popular kernel functions
which can be used by setting the kernel parameter to the following
strings:
</p>

<ul>
<li> <p><code>rbfdot</code> Radial Basis kernel function &quot;Gaussian&quot;
</p>
</li>
<li> <p><code>polydot</code> Polynomial kernel function
</p>
</li>
<li> <p><code>vanilladot</code> Linear kernel function
</p>
</li>
<li> <p><code>tanhdot</code> Hyperbolic tangent kernel function
</p>
</li>
<li> <p><code>laplacedot</code> Laplacian kernel function
</p>
</li>
<li> <p><code>besseldot</code> Bessel kernel function
</p>
</li>
<li> <p><code>anovadot</code> ANOVA RBF kernel function
</p>
</li>
<li> <p><code>splinedot</code> Spline kernel 
</p>
</li></ul>

<p>The kernel parameter can also be set to a user defined function of
class kernel by passing the function name as an argument.
</p>
</td></tr>
<tr><td><code id="ranking_+3A_kpar">kpar</code></td>
<td>
<p>the list of hyper-parameters (kernel parameters).
This is a list which contains the parameters to be used with the
kernel function. For valid parameters for existing kernels are :
</p>

<ul>
<li> <p><code>sigma</code> inverse kernel width for the Radial Basis
kernel function &quot;rbfdot&quot; and the Laplacian kernel &quot;laplacedot&quot;.
</p>
</li>
<li> <p><code>degree, scale, offset</code> for the Polynomial kernel &quot;polydot&quot;
</p>
</li>
<li> <p><code>scale, offset</code> for the Hyperbolic tangent kernel
function &quot;tanhdot&quot;
</p>
</li>
<li> <p><code>sigma, order, degree</code> for the Bessel kernel &quot;besseldot&quot;.
</p>
</li>
<li> <p><code>sigma, degree</code> for the ANOVA kernel &quot;anovadot&quot;.
</p>
</li></ul>

<p>Hyper-parameters for user defined kernels can be passed through the
kpar parameter as well.</p>
</td></tr>
<tr><td><code id="ranking_+3A_scale">scale</code></td>
<td>
<p>If TRUE the data matrix columns are scaled to zero mean
and unit variance.</p>
</td></tr>
<tr><td><code id="ranking_+3A_alpha">alpha</code></td>
<td>
<p> The <code>alpha</code> parameter takes values between 0 and 1
and is used to control the authoritative scores received from the
unlabeled points. For 0 no global structure is found the algorithm
ranks the points similarly to the original distance metric.</p>
</td></tr>
<tr><td><code id="ranking_+3A_iterations">iterations</code></td>
<td>
<p>Maximum number of iterations</p>
</td></tr>
<tr><td><code id="ranking_+3A_edgegraph">edgegraph</code></td>
<td>
<p>Construct edgegraph (only supported with the RBF
kernel)</p>
</td></tr>
<tr><td><code id="ranking_+3A_convergence">convergence</code></td>
<td>
<p>Include convergence matrix in results</p>
</td></tr>
<tr><td><code id="ranking_+3A_...">...</code></td>
<td>
<p>Additional arguments</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A simple universal ranking algorithm which exploits the intrinsic
global geometric structure of the data. In many real world
applications this should be superior to a local method in which the data
are simply ranked by pairwise Euclidean distances.
Firstly a weighted network is defined on the data and an authoritative
score is assigned to each query. The query points act as source nodes
that continually pump their authoritative scores to the remaining points
via the weighted network and the remaining points further spread the
scores they received to their neighbors. This spreading process is
repeated until convergence and the points are ranked according to their
score at the end of the iterations.
</p>


<h3>Value</h3>

<p>An S4 object of class <code>ranking</code> which extends the <code>matrix</code>
class.
The first column of the returned matrix contains the original index of
the points in the data matrix the second column contains the final
score received by each point and the third column the ranking of the point.
The object contains the following slots :
</p>
<table>
<tr><td><code>edgegraph</code></td>
<td>
<p>Containing the edgegraph of the data points. </p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>Containing the convergence matrix</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Alexandros Karatzoglou <br />
<a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>References</h3>

<p>D. Zhou, J. Weston, A. Gretton, O. Bousquet, B. Schoelkopf <br />
<em>Ranking on Data Manifolds</em><br />
Advances in Neural Information Processing Systems 16.<br />
MIT Press Cambridge Mass. 2004 <br />
<a href="https://papers.neurips.cc/paper/2447-ranking-on-data-manifolds.pdf">https://papers.neurips.cc/paper/2447-ranking-on-data-manifolds.pdf</a>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+ranking-class">ranking-class</a></code>, <code><a href="#topic+specc">specc</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(spirals)

## create data from spirals
ran &lt;- spirals[rowSums(abs(spirals) &lt; 0.55) == 2,]

## rank points according to similarity to the most upper left point  
ranked &lt;- ranking(ran, 54, kernel = "rbfdot",
                  kpar = list(sigma = 100), edgegraph = TRUE)
ranked[54, 2] &lt;- max(ranked[-54, 2])
c&lt;-1:86
op &lt;- par(mfrow = c(1, 2),pty="s")
plot(ran)
plot(ran, cex=c[ranked[,3]]/40)

</code></pre>

<hr>
<h2 id='ranking-class'>Class &quot;ranking&quot;</h2><span id='topic+ranking-class'></span><span id='topic+edgegraph'></span><span id='topic+convergence'></span><span id='topic+convergence+2Cranking-method'></span><span id='topic+edgegraph+2Cranking-method'></span><span id='topic+show+2Cranking-method'></span>

<h3>Description</h3>

<p>Object of the class <code>"ranking"</code> are created from the
<code>ranking</code> function and extend the class <code>matrix</code></p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("ranking", ...)</code>.
</p>


<h3>Slots</h3>


<dl>
<dt><code>.Data</code>:</dt><dd><p>Object of class <code>"matrix"</code> containing the
data ranking and scores</p>
</dd>
<dt><code>convergence</code>:</dt><dd><p>Object of class <code>"matrix"</code>
containing the convergence matrix</p>
</dd>
<dt><code>edgegraph</code>:</dt><dd><p>Object of class <code>"matrix"</code> containing
the edgegraph</p>
</dd>
</dl>



<h3>Extends</h3>

<p>Class <code>"matrix"</code>, directly.
</p>


<h3>Methods</h3>


<dl>
<dt>show</dt><dd><p><code>signature(object = "ranking")</code>: displays the
ranking score matrix</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Alexandros Karatzoglou <br />
<a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ranking">ranking</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(spirals)

## create data set to be ranked
ran&lt;-spirals[rowSums(abs(spirals)&lt;0.55)==2,]

## rank points according to "relevance" to point 54 (up left)
ranked&lt;-ranking(ran,54,kernel="rbfdot",
                kpar=list(sigma=100),edgegraph=TRUE)

ranked
edgegraph(ranked)[1:10,1:10]
</code></pre>

<hr>
<h2 id='reuters'>Reuters Text Data</h2><span id='topic+reuters'></span><span id='topic+rlabels'></span>

<h3>Description</h3>

<p>A small sample from the Reuters news data set.</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(reuters)</code></pre>


<h3>Format</h3>

<p>A list of 40 text documents along with the labels. <code>reuters</code>
contains the text documents and <code>rlabels</code> the labels in a vector.
</p>


<h3>Details</h3>

<p>This dataset contains a list of 40 text documents along with the
labels. The data consist out of 20 documents from the <code>acq</code>
category and 20 documents from the crude category. The labels are
stored in <code>rlabels</code>
</p>


<h3>Source</h3>

<p>Reuters</p>

<hr>
<h2 id='rvm'>Relevance Vector Machine</h2><span id='topic+rvm'></span><span id='topic+rvm-methods'></span><span id='topic+rvm+2Cformula-method'></span><span id='topic+rvm+2Clist-method'></span><span id='topic+rvm+2Cvector-method'></span><span id='topic+rvm+2CkernelMatrix-method'></span><span id='topic+rvm+2Cmatrix-method'></span><span id='topic+show+2Crvm-method'></span><span id='topic+predict+2Crvm-method'></span><span id='topic+coef+2Crvm-method'></span>

<h3>Description</h3>

<p>The Relevance Vector Machine is a Bayesian model for regression and
classification of identical functional form to the support vector
machine.
The <code>rvm</code> function currently supports only regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'formula'
rvm(x, data=NULL, ..., subset, na.action = na.omit)

## S4 method for signature 'vector'
rvm(x, ...)

## S4 method for signature 'matrix'
rvm(x, y, type="regression",
    kernel="rbfdot", kpar="automatic",
    alpha= ncol(as.matrix(x)), var=0.1, var.fix=FALSE, iterations=100,
    verbosity = 0, tol = .Machine$double.eps, minmaxdiff = 1e-3,
    cross = 0, fit = TRUE, ... , subset, na.action = na.omit) 

## S4 method for signature 'list'
rvm(x, y, type = "regression",
    kernel = "stringdot", kpar = list(length = 4, lambda = 0.5),
    alpha = 5, var = 0.1, var.fix = FALSE, iterations = 100,
    verbosity = 0, tol = .Machine$double.eps, minmaxdiff = 1e-3,
    cross = 0, fit = TRUE, ..., subset, na.action = na.omit)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rvm_+3A_x">x</code></td>
<td>
<p>a symbolic description of the model to be fit.
When not using a formula x can be a matrix or vector containing the training
data or a kernel matrix of class <code>kernelMatrix</code> of the training data
or a list of character vectors (for use with the string
kernel). Note, that the intercept is always excluded, whether
given in the formula or not.</p>
</td></tr>
<tr><td><code id="rvm_+3A_data">data</code></td>
<td>
<p>an optional data frame containing the variables in the model.
By default the variables are taken from the environment which
&lsquo;rvm&rsquo; is called from.</p>
</td></tr>
<tr><td><code id="rvm_+3A_y">y</code></td>
<td>
<p>a response vector with one label for each row/component of <code>x</code>. Can be either
a factor (for classification tasks) or a numeric vector (for
regression).</p>
</td></tr>
<tr><td><code id="rvm_+3A_type">type</code></td>
<td>
<p><code>rvm</code> can only be used for regression at the moment.</p>
</td></tr>
<tr><td><code id="rvm_+3A_kernel">kernel</code></td>
<td>
<p>the kernel function used in training and predicting.
This parameter can be set to any function, of class kernel, which computes a dot product between two
vector arguments. kernlab provides the most popular kernel functions
which can be used by setting the kernel parameter to the following
strings:
</p>

<ul>
<li> <p><code>rbfdot</code> Radial Basis kernel &quot;Gaussian&quot;
</p>
</li>
<li> <p><code>polydot</code> Polynomial kernel
</p>
</li>
<li> <p><code>vanilladot</code> Linear kernel 
</p>
</li>
<li> <p><code>tanhdot</code> Hyperbolic tangent kernel 
</p>
</li>
<li> <p><code>laplacedot</code> Laplacian kernel 
</p>
</li>
<li> <p><code>besseldot</code> Bessel kernel 
</p>
</li>
<li> <p><code>anovadot</code> ANOVA RBF kernel 
</p>
</li>
<li> <p><code>splinedot</code> Spline kernel 
</p>
</li>
<li> <p><code>stringdot</code> String kernel 
</p>
</li></ul>

<p>The kernel parameter can also be set to a user defined function of
class kernel by passing the function name as an argument.
</p>
</td></tr>
<tr><td><code id="rvm_+3A_kpar">kpar</code></td>
<td>
<p>the list of hyper-parameters (kernel parameters).
This is a list which contains the parameters to be used with the
kernel function. For valid parameters for existing kernels are :
</p>

<ul>
<li> <p><code>sigma</code> inverse kernel width for the Radial Basis
kernel function &quot;rbfdot&quot; and the Laplacian kernel &quot;laplacedot&quot;.
</p>
</li>
<li> <p><code>degree, scale, offset</code> for the Polynomial kernel &quot;polydot&quot;
</p>
</li>
<li> <p><code>scale, offset</code> for the Hyperbolic tangent kernel
function &quot;tanhdot&quot;
</p>
</li>
<li> <p><code>sigma, order, degree</code> for the Bessel kernel &quot;besseldot&quot;. 
</p>
</li>
<li> <p><code>sigma, degree</code> for the ANOVA kernel &quot;anovadot&quot;.
</p>
</li>
<li> <p><code>length, lambda, normalized</code> for the &quot;stringdot&quot; kernel
where length is the length of the strings considered, lambda the
decay factor and normalized a logical parameter determining if the
kernel evaluations should be normalized.
</p>
</li></ul>

<p>Hyper-parameters for user defined kernels can be passed through the
kpar parameter as well. In the case of a Radial Basis kernel function (Gaussian)
kpar can also be set to the string &quot;automatic&quot; which uses the heuristics in 
<code><a href="#topic+sigest">sigest</a></code> to calculate a good <code>sigma</code> value for the
Gaussian RBF or Laplace kernel, from the data.
(default = &quot;automatic&quot;).</p>
</td></tr>
<tr><td><code id="rvm_+3A_alpha">alpha</code></td>
<td>
<p>The initial alpha vector. Can be either a vector of
length equal to the number of data points or a single number.</p>
</td></tr>
<tr><td><code id="rvm_+3A_var">var</code></td>
<td>
<p>the initial noise variance</p>
</td></tr>
<tr><td><code id="rvm_+3A_var.fix">var.fix</code></td>
<td>
<p>Keep noise variance fix during iterations (default: FALSE)</p>
</td></tr>
<tr><td><code id="rvm_+3A_iterations">iterations</code></td>
<td>
<p>Number of iterations allowed (default: 100)</p>
</td></tr> 
<tr><td><code id="rvm_+3A_tol">tol</code></td>
<td>
<p>tolerance of termination criterion</p>
</td></tr>
<tr><td><code id="rvm_+3A_minmaxdiff">minmaxdiff</code></td>
<td>
<p>termination criteria. Stop when max difference is
equal to this parameter (default:1e-3) </p>
</td></tr>
<tr><td><code id="rvm_+3A_verbosity">verbosity</code></td>
<td>
<p>print information on algorithm convergence (default
= FALSE)</p>
</td></tr>
<tr><td><code id="rvm_+3A_fit">fit</code></td>
<td>
<p>indicates whether the fitted values should be computed and
included in the model or not (default: TRUE)</p>
</td></tr>
<tr><td><code id="rvm_+3A_cross">cross</code></td>
<td>
<p>if a integer value k&gt;0 is specified, a k-fold cross
validation on the training data is performed to assess the
quality of the model: the Mean Squared Error for regression</p>
</td></tr>
<tr><td><code id="rvm_+3A_subset">subset</code></td>
<td>
<p>An index vector specifying the cases to be used in the
training sample.  (NOTE: If given, this argument must be
named.)</p>
</td></tr>
<tr><td><code id="rvm_+3A_na.action">na.action</code></td>
<td>
<p>A function to specify the action to be taken if <code>NA</code>s are
found. The default action is <code>na.omit</code>, which leads to rejection of cases
with missing values on any required variable. An alternative
is <code>na.fail</code>, which causes an error if <code>NA</code> cases
are found. (NOTE: If given, this argument must be named.)</p>
</td></tr>
<tr><td><code id="rvm_+3A_...">...</code></td>
<td>
<p> additional parameters</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Relevance Vector Machine typically leads to sparser models
then the SVM. It also performs better in many cases (specially in
regression). 
</p>


<h3>Value</h3>

<p>An S4 object of class &quot;rvm&quot; containing the fitted model.
Accessor functions can be used to access the slots of the
object which include :
</p>
<table>
<tr><td><code>alpha</code></td>
<td>
<p>The resulting relevance vectors</p>
</td></tr>
<tr><td><code>alphaindex</code></td>
<td>
<p> The index of the resulting relevance vectors in the data
matrix</p>
</td></tr>
<tr><td><code>nRV</code></td>
<td>
<p>Number of relevance vectors</p>
</td></tr>
<tr><td><code>RVindex</code></td>
<td>
<p>The indexes of the relevance vectors</p>
</td></tr>
<tr><td><code>error</code></td>
<td>
<p>Training error (if <code>fit = TRUE</code>)</p>
</td></tr>
</table>
<p>...
</p>


<h3>Author(s)</h3>

<p> Alexandros Karatzoglou <br />
<a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>References</h3>

<p>Tipping, M. E.<br />
<em>Sparse Bayesian learning and the relevance vector machine</em><br />
Journal of Machine Learning Research  1, 211-244<br />
<a href="https://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf">https://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf</a>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+ksvm">ksvm</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># create data
x &lt;- seq(-20,20,0.1)
y &lt;- sin(x)/x + rnorm(401,sd=0.05)

# train relevance vector machine
foo &lt;- rvm(x, y)
foo
# print relevance vectors
alpha(foo)
RVindex(foo)

# predict and plot
ytest &lt;- predict(foo, x)
plot(x, y, type ="l")
lines(x, ytest, col="red")
</code></pre>

<hr>
<h2 id='rvm-class'>Class &quot;rvm&quot;</h2><span id='topic+rvm-class'></span><span id='topic+RVindex'></span><span id='topic+mlike'></span><span id='topic+nvar'></span><span id='topic+RVindex+2Crvm-method'></span><span id='topic+alpha+2Crvm-method'></span><span id='topic+cross+2Crvm-method'></span><span id='topic+error+2Crvm-method'></span><span id='topic+kcall+2Crvm-method'></span><span id='topic+kernelf+2Crvm-method'></span><span id='topic+kpar+2Crvm-method'></span><span id='topic+lev+2Crvm-method'></span><span id='topic+mlike+2Crvm-method'></span><span id='topic+nvar+2Crvm-method'></span><span id='topic+type+2Crvm-method'></span><span id='topic+xmatrix+2Crvm-method'></span><span id='topic+ymatrix+2Crvm-method'></span>

<h3>Description</h3>

<p>Relevance Vector Machine Class</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("rvm", ...)</code>.
or by calling the <code>rvm</code> function.
</p>


<h3>Slots</h3>


<dl>
<dt><code>tol</code>:</dt><dd><p>Object of class <code>"numeric"</code> contains
tolerance of termination criteria used.</p>
</dd>
<dt><code>kernelf</code>:</dt><dd><p>Object of class <code>"kfunction"</code> contains
the kernel function used </p>
</dd>
<dt><code>kpar</code>:</dt><dd><p>Object of class <code>"list"</code> contains the
hyperparameter used</p>
</dd>
<dt><code>kcall</code>:</dt><dd><p>Object of class <code>"call"</code> contains the
function call</p>
</dd>
<dt><code>type</code>:</dt><dd><p>Object of class <code>"character"</code> contains type
of problem</p>
</dd>
<dt><code>terms</code>:</dt><dd><p>Object of class <code>"ANY"</code>  containing the
terms representation of the symbolic model used (when using a
formula interface)</p>
</dd>
<dt><code>xmatrix</code>:</dt><dd><p>Object of class <code>"matrix"</code> contains the data
matrix used during computation</p>
</dd>
<dt><code>ymatrix</code>:</dt><dd><p>Object of class <code>"output"</code> contains the
response matrix</p>
</dd>
<dt><code>fitted</code>:</dt><dd><p>Object of class <code>"output"</code> with the fitted
values, (predict on training set).</p>
</dd>
<dt><code>lev</code>:</dt><dd><p>Object of class <code>"vector"</code> contains the
levels of the response (in classification)</p>
</dd>
<dt><code>nclass</code>:</dt><dd><p>Object of class <code>"numeric"</code> contains the
number of classes (in classification)</p>
</dd>
<dt><code>alpha</code>:</dt><dd><p>Object of class <code>"listI"</code> containing the the
resulting alpha vector</p>
</dd>
<dt><code>coef</code>:</dt><dd><p>Object of class <code>"ANY"</code> containing the the
resulting model parameters</p>
</dd>
<dt><code>nvar</code>:</dt><dd><p>Object of class <code>"numeric"</code> containing the
calculated variance (in case of regression)</p>
</dd>
<dt><code>mlike</code>:</dt><dd><p>Object of class <code>"numeric"</code> containing the
computed maximum likelihood</p>
</dd>
<dt><code>RVindex</code>:</dt><dd><p>Object of class <code>"vector"</code> containing
the indexes of the resulting relevance vectors </p>
</dd>
<dt><code>nRV</code>:</dt><dd><p>Object of class <code>"numeric"</code> containing the
number of relevance vectors</p>
</dd>
<dt><code>cross</code>:</dt><dd><p>Object of class <code>"numeric"</code> containing the
resulting cross validation error </p>
</dd>
<dt><code>error</code>:</dt><dd><p>Object of class <code>"numeric"</code> containing the
training error</p>
</dd>
<dt><code>n.action</code>:</dt><dd><p>Object of class <code>"ANY"</code> containing the
action performed on NA</p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>RVindex</dt><dd><p><code>signature(object = "rvm")</code>: returns the index
of the relevance vectors </p>
</dd>
<dt>alpha</dt><dd><p><code>signature(object = "rvm")</code>: returns the resulting
alpha vector</p>
</dd>
<dt>cross</dt><dd><p><code>signature(object = "rvm")</code>: returns the resulting
cross validation error</p>
</dd>
<dt>error</dt><dd><p><code>signature(object = "rvm")</code>: returns the training
error  </p>
</dd>
<dt>fitted</dt><dd><p><code>signature(object = "vm")</code>: returns the fitted values </p>
</dd>
<dt>kcall</dt><dd><p><code>signature(object = "rvm")</code>: returns the function call </p>
</dd>
<dt>kernelf</dt><dd><p><code>signature(object = "rvm")</code>: returns the used
kernel function </p>
</dd>
<dt>kpar</dt><dd><p><code>signature(object = "rvm")</code>: returns the parameters
of the kernel function</p>
</dd>
<dt>lev</dt><dd><p><code>signature(object = "rvm")</code>: returns the levels of
the response (in classification)</p>
</dd>
<dt>mlike</dt><dd><p><code>signature(object = "rvm")</code>: returns the estimated
maximum likelihood</p>
</dd>
<dt>nvar</dt><dd><p><code>signature(object = "rvm")</code>: returns the calculated
variance (in regression)</p>
</dd>
<dt>type</dt><dd><p><code>signature(object = "rvm")</code>: returns the type of problem</p>
</dd>
<dt>xmatrix</dt><dd><p><code>signature(object = "rvm")</code>: returns the data
matrix used during computation</p>
</dd>
<dt>ymatrix</dt><dd><p><code>signature(object = "rvm")</code>: returns the used response </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Alexandros Karatzoglou<br /> <a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+rvm">rvm</a></code>, 
<code><a href="#topic+ksvm-class">ksvm-class</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# create data
x &lt;- seq(-20,20,0.1)
y &lt;- sin(x)/x + rnorm(401,sd=0.05)

# train relevance vector machine
foo &lt;- rvm(x, y)
foo

alpha(foo)
RVindex(foo)
fitted(foo)
kernelf(foo)
nvar(foo)

## show slots
slotNames(foo)

</code></pre>

<hr>
<h2 id='sigest'>Hyperparameter estimation for the Gaussian Radial Basis kernel</h2><span id='topic+sigest'></span><span id='topic+sigest+2Cformula-method'></span><span id='topic+sigest+2Cmatrix-method'></span>

<h3>Description</h3>

<p>Given a range of values for the &quot;sigma&quot; inverse width parameter in the Gaussian Radial Basis kernel
for use with Support Vector Machines. The estimation is based on the
data to be used.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'formula'
sigest(x, data=NULL, frac = 0.5, na.action = na.omit, scaled = TRUE)
## S4 method for signature 'matrix'
sigest(x, frac = 0.5, scaled = TRUE, na.action = na.omit)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sigest_+3A_x">x</code></td>
<td>
<p>a symbolic description of the model upon the estimation is
based. When not using a formula x is a matrix or vector
containing the data</p>
</td></tr>
<tr><td><code id="sigest_+3A_data">data</code></td>
<td>
<p>an optional data frame containing the variables in the model.
By default the variables are taken from the environment which
&lsquo;ksvm&rsquo; is called from.</p>
</td></tr>
<tr><td><code id="sigest_+3A_frac">frac</code></td>
<td>
<p>Fraction of data to use for estimation. By default a quarter
of the data is used to estimate the range of the sigma hyperparameter.</p>
</td></tr>
<tr><td><code id="sigest_+3A_scaled">scaled</code></td>
<td>
<p>A logical vector indicating the variables to be
scaled. If <code>scaled</code> is of length 1, the value is recycled as
many times as needed and all non-binary variables are scaled.
Per default, data are scaled internally to zero mean and unit
variance
(since this the default action in <code>ksvm</code> as well). The center and scale
values are returned and used for later predictions. </p>
</td></tr>
<tr><td><code id="sigest_+3A_na.action">na.action</code></td>
<td>
<p>A function to specify the action to be taken if <code>NA</code>s are
found. The default action is <code>na.omit</code>, which leads to rejection of cases
with missing values on any required variable. An alternative
is <code>na.fail</code>, which causes an error if <code>NA</code> cases
are found. (NOTE: If given, this argument must be named.)</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>sigest</code> estimates the range of values for the sigma parameter
which would return good results when used with a Support Vector
Machine (<code>ksvm</code>). The estimation is based upon the 0.1 and 0.9 quantile 
of <code class="reqn">\|x -x'\|^2</code>. Basically any value in between those two bounds will
produce good results.
</p>


<h3>Value</h3>

<p>Returns a vector of length 3 defining the range (0.1 quantile, median
and 0.9 quantile) of
the sigma hyperparameter. 
</p>


<h3>Author(s)</h3>

<p>Alexandros Karatzoglou <br /> 
<a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>References</h3>

<p> B. Caputo, K. Sim, F. Furesjo, A. Smola, <br />
<em>Appearance-based object recognition using SVMs: which kernel should I use?</em><br />
Proc of NIPS workshop on Statitsical methods for computational experiments in visual processing and computer vision, Whistler, 2002.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ksvm">ksvm</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
## estimate good sigma values for promotergene
data(promotergene)
srange &lt;- sigest(Class~.,data = promotergene)
srange

s &lt;- srange[2]
s
## create test and training set
ind &lt;- sample(1:dim(promotergene)[1],20)
genetrain &lt;- promotergene[-ind, ]
genetest &lt;- promotergene[ind, ]

## train a support vector machine
gene &lt;- ksvm(Class~.,data=genetrain,kernel="rbfdot",
             kpar=list(sigma = s),C=50,cross=3)
gene

## predict gene type on the test set
promoter &lt;- predict(gene,genetest[,-1])

## Check results
table(promoter,genetest[,1])
</code></pre>

<hr>
<h2 id='spam'>Spam E-mail Database</h2><span id='topic+spam'></span>

<h3>Description</h3>

<p>A data set collected at Hewlett-Packard Labs, that classifies 4601
e-mails as spam or non-spam. In addition to this class label there are 57
variables indicating the frequency of certain words and characters in the
e-mail.</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(spam)</code></pre>


<h3>Format</h3>

<p>A data frame with 4601 observations and 58 variables.
</p>
<p>The first 48 variables contain the frequency of the variable name
(e.g., business) in the e-mail. If the variable name starts with num (e.g.,
num650) the it indicates the frequency of the corresponding number (e.g., 650).
The variables 49-54 indicate the frequency of the characters &lsquo;;&rsquo;, &lsquo;(&rsquo;, &lsquo;[&rsquo;, &lsquo;!&rsquo;,
&lsquo;$&rsquo;, and &lsquo;#&rsquo;. The variables 55-57 contain the average, longest 
and total run-length of capital letters. Variable 58 indicates the type of the
mail and is either <code>"nonspam"</code> or <code>"spam"</code>, i.e. unsolicited
commercial e-mail.</p>


<h3>Details</h3>

<p>The data set contains 2788 e-mails classified as <code>"nonspam"</code> and 1813
classified as <code>"spam"</code>.
</p>
<p>The &ldquo;spam&rdquo; concept is diverse: advertisements for products/web
sites, make money fast schemes, chain letters, pornography...
This collection of spam e-mails came from the collectors' postmaster and
individuals who had filed spam.  The collection of non-spam
e-mails came from filed work and personal e-mails, and hence
the word 'george' and the area code '650' are indicators of
non-spam.  These are useful when constructing a personalized
spam filter.  One would either have to blind such non-spam
indicators or get a very wide collection of non-spam to
generate a general purpose spam filter.
</p>


<h3>Source</h3>


<ul>
<li><p> Creators: Mark Hopkins, Erik Reeber, George Forman, Jaap Suermondt at
Hewlett-Packard Labs, 1501 Page Mill Rd., Palo Alto, CA 94304
</p>
</li>
<li><p> Donor: George Forman (gforman at nospam hpl.hp.com)  650-857-7835
</p>
</li></ul>

<p>These data have been taken from the UCI Repository Of Machine Learning
Databases at <a href="http://www.ics.uci.edu/~mlearn/MLRepository.html">http://www.ics.uci.edu/~mlearn/MLRepository.html</a></p>


<h3>References</h3>

<p>T. Hastie, R. Tibshirani, J.H. Friedman. <em>The Elements of Statistical
Learning.</em> Springer, 2001.
</p>

<hr>
<h2 id='specc'>Spectral Clustering</h2><span id='topic+specc'></span><span id='topic+specc+2Cmatrix-method'></span><span id='topic+specc+2Cformula-method'></span><span id='topic+specc+2Clist-method'></span><span id='topic+specc+2CkernelMatrix-method'></span><span id='topic+show+2Cspecc-method'></span>

<h3>Description</h3>

<p>A spectral clustering algorithm. Clustering is performed by
embedding the data into the subspace of the eigenvectors 
of an affinity matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'formula'
specc(x, data = NULL, na.action = na.omit, ...)

## S4 method for signature 'matrix'
specc(x, centers,
      kernel = "rbfdot", kpar = "automatic", 
      nystrom.red = FALSE, nystrom.sample = dim(x)[1]/6,
      iterations = 200, mod.sample = 0.75, na.action = na.omit, ...)

## S4 method for signature 'kernelMatrix'
specc(x, centers, nystrom.red = FALSE, iterations = 200, ...)

## S4 method for signature 'list'
specc(x, centers,
      kernel = "stringdot", kpar = list(length=4, lambda=0.5),
      nystrom.red = FALSE, nystrom.sample = length(x)/6,
      iterations = 200, mod.sample = 0.75, na.action = na.omit, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="specc_+3A_x">x</code></td>
<td>
<p>the matrix of data to be clustered, or a symbolic
description of the model to be fit, or a kernel Matrix of class
<code>kernelMatrix</code>, or a list of character vectors.</p>
</td></tr>
<tr><td><code id="specc_+3A_data">data</code></td>
<td>
<p>an optional data frame containing the variables in the model.
By default the variables are taken from the environment which
&lsquo;specc&rsquo; is called from.</p>
</td></tr>
<tr><td><code id="specc_+3A_centers">centers</code></td>
<td>
<p>Either the number of clusters or a set of initial cluster
centers. If the first, a random set of rows in the eigenvectors
matrix are chosen as the initial centers.</p>
</td></tr>
<tr><td><code id="specc_+3A_kernel">kernel</code></td>
<td>
<p>the kernel function used in computing the affinity matrix. 
This parameter can be set to any function, of class kernel, which computes a dot product between two
vector arguments. kernlab provides the most popular kernel functions
which can be used by setting the kernel parameter to the following
strings:
</p>

<ul>
<li> <p><code>rbfdot</code> Radial Basis kernel function &quot;Gaussian&quot;
</p>
</li>
<li> <p><code>polydot</code> Polynomial kernel function
</p>
</li>
<li> <p><code>vanilladot</code> Linear kernel function
</p>
</li>
<li> <p><code>tanhdot</code> Hyperbolic tangent kernel function
</p>
</li>
<li> <p><code>laplacedot</code> Laplacian kernel function
</p>
</li>
<li> <p><code>besseldot</code> Bessel kernel function
</p>
</li>
<li> <p><code>anovadot</code> ANOVA RBF kernel function
</p>
</li>
<li> <p><code>splinedot</code> Spline kernel 
</p>
</li>
<li> <p><code>stringdot</code> String kernel
</p>
</li></ul>

<p>The kernel parameter can also be set to a user defined function of
class kernel by passing the function name as an argument.
</p>
</td></tr>
<tr><td><code id="specc_+3A_kpar">kpar</code></td>
<td>
<p>a character string or the list of hyper-parameters (kernel parameters).
The default character string <code>"automatic"</code> uses a heuristic to determine a
suitable value for the width parameter of the RBF kernel.
The second option <code>"local"</code> (local scaling) uses a more advanced heuristic
and sets a width parameter for every point in the data set. This is
particularly useful when the data incorporates multiple scales.
A list can also be used containing the parameters to be used with the
kernel function. Valid parameters for existing kernels are :
</p>

<ul>
<li> <p><code>sigma</code> inverse kernel width for the Radial Basis
kernel function &quot;rbfdot&quot; and the Laplacian kernel &quot;laplacedot&quot;.
</p>
</li>
<li> <p><code>degree, scale, offset</code> for the Polynomial kernel &quot;polydot&quot;
</p>
</li>
<li> <p><code>scale, offset</code> for the Hyperbolic tangent kernel
function &quot;tanhdot&quot;
</p>
</li>
<li> <p><code>sigma, order, degree</code> for the Bessel kernel &quot;besseldot&quot;. 
</p>
</li>
<li> <p><code>sigma, degree</code> for the ANOVA kernel &quot;anovadot&quot;.
</p>
</li>
<li> <p><code>length, lambda, normalized</code> for the &quot;stringdot&quot; kernel
where length is the length of the strings considered, lambda the
decay factor and normalized a logical parameter determining if the
kernel evaluations should be normalized.
</p>
</li></ul>

<p>Hyper-parameters for user defined kernels can be passed through the
kpar parameter as well.</p>
</td></tr>
<tr><td><code id="specc_+3A_nystrom.red">nystrom.red</code></td>
<td>
<p>use nystrom method to calculate eigenvectors. When
<code>TRUE</code> a sample of the dataset is used to calculate the
eigenvalues, thus only a <code class="reqn">n x m</code> matrix where <code class="reqn">n</code> the sample size
is stored in memory (default: <code>FALSE</code></p>
</td></tr>
<tr><td><code id="specc_+3A_nystrom.sample">nystrom.sample</code></td>
<td>
<p>number of data points to use for estimating the
eigenvalues when using the nystrom method. (default : dim(x)[1]/6)</p>
</td></tr>
<tr><td><code id="specc_+3A_mod.sample">mod.sample</code></td>
<td>
<p>proportion of data to use when estimating sigma (default: 0.75)</p>
</td></tr>	
<tr><td><code id="specc_+3A_iterations">iterations</code></td>
<td>
<p>the maximum number of iterations allowed. </p>
</td></tr>
<tr><td><code id="specc_+3A_na.action">na.action</code></td>
<td>
<p>the action to perform on NA</p>
</td></tr>
<tr><td><code id="specc_+3A_...">...</code></td>
<td>
<p>additional parameters</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Spectral clustering  works by embedding the data points of the
partitioning problem into the
subspace of the <code class="reqn">k</code> largest eigenvectors of a normalized affinity/kernel matrix.
Using a simple clustering method like <code>kmeans</code> on the embedded points usually
leads to good performance. It can be shown that spectral clustering methods boil down to 
graph partitioning.<br />
The data can be passed to the <code>specc</code> function in a <code>matrix</code> or a
<code>data.frame</code>, in addition <code>specc</code> also supports input in the form of a
kernel matrix of class <code>kernelMatrix</code> or as a list of character
vectors where a string kernel has to be used.</p>


<h3>Value</h3>

<p>An S4 object of class <code>specc</code> which extends the class <code>vector</code>
containing integers indicating the cluster to which
each point is allocated. The following slots contain useful information
</p>
<table>
<tr><td><code>centers</code></td>
<td>
<p>A matrix of cluster centers.</p>
</td></tr>
<tr><td><code>size</code></td>
<td>
<p>The number of point in each cluster</p>
</td></tr>
<tr><td><code>withinss</code></td>
<td>
<p>The within-cluster sum of squares for each cluster</p>
</td></tr>
<tr><td><code>kernelf</code></td>
<td>
<p>The kernel function used</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Alexandros Karatzoglou <br /> <a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a>
</p>


<h3>References</h3>

<p>Andrew Y. Ng, Michael I. Jordan, Yair Weiss<br />
<em>On Spectral Clustering: Analysis and an Algorithm</em><br />
Neural Information Processing Symposium 2001<br />
<a href="https://papers.neurips.cc/paper/2092-on-spectral-clustering-analysis-and-an-algorithm.pdf">https://papers.neurips.cc/paper/2092-on-spectral-clustering-analysis-and-an-algorithm.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kkmeans">kkmeans</a></code>, <code><a href="#topic+kpca">kpca</a></code>, <code><a href="#topic+kcca">kcca</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## Cluster the spirals data set.
data(spirals)

sc &lt;- specc(spirals, centers=2)

sc
centers(sc)
size(sc)
withinss(sc)

plot(spirals, col=sc)

</code></pre>

<hr>
<h2 id='specc-class'>Class &quot;specc&quot;</h2><span id='topic+specc-class'></span><span id='topic+centers'></span><span id='topic+size'></span><span id='topic+withinss'></span><span id='topic+centers+2Cspecc-method'></span><span id='topic+withinss+2Cspecc-method'></span><span id='topic+size+2Cspecc-method'></span><span id='topic+kernelf+2Cspecc-method'></span>

<h3>Description</h3>

<p> The Spectral Clustering Class</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("specc", ...)</code>.
or by calling the function <code>specc</code>. 
</p>


<h3>Slots</h3>


<dl>
<dt><code>.Data</code>:</dt><dd><p>Object of class <code>"vector"</code> containing the cluster assignments</p>
</dd>
<dt><code>centers</code>:</dt><dd><p>Object of class <code>"matrix"</code> containing
the cluster centers</p>
</dd>
<dt><code>size</code>:</dt><dd><p>Object of class <code>"vector"</code> containing the
number of points in each  cluster</p>
</dd>
<dt><code>withinss</code>:</dt><dd><p>Object of class <code>"vector"</code> containing
the within-cluster sum of squares for each cluster</p>
</dd>
<dt><code>kernelf</code></dt><dd><p>Object of class <code>kernel</code> containing the
used kernel function.</p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>centers</dt><dd><p><code>signature(object = "specc")</code>: returns the
cluster centers</p>
</dd>
<dt>withinss</dt><dd><p><code>signature(object = "specc")</code>: returns the
within-cluster sum of squares for each cluster</p>
</dd>
<dt>size</dt><dd><p><code>signature(object = "specc")</code>: returns the number
of points in each cluster </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Alexandros Karatzoglou<br /> <a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+specc">specc</a></code>, 
<code><a href="#topic+kpca-class">kpca-class</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Cluster the spirals data set.
data(spirals)

sc &lt;- specc(spirals, centers=2)

centers(sc)
size(sc)
</code></pre>

<hr>
<h2 id='spirals'>Spirals Dataset</h2><span id='topic+spirals'></span>

<h3>Description</h3>

<p>A toy data set representing
two spirals with Gaussian noise. The data was created with 
the <code>mlbench.spirals</code> function in <code>mlbench</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(spirals)</code></pre>


<h3>Format</h3>

<p>A matrix with 300 observations and 2 variables.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(spirals)
plot(spirals)
</code></pre>

<hr>
<h2 id='stringdot'>String Kernel Functions</h2><span id='topic+stringdot'></span>

<h3>Description</h3>

<p>String kernels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stringdot(length = 4, lambda = 1.1, type = "spectrum", normalized = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stringdot_+3A_length">length</code></td>
<td>
<p>The length of the substrings considered</p>
</td></tr>
<tr><td><code id="stringdot_+3A_lambda">lambda</code></td>
<td>
<p>The decay factor</p>
</td></tr>
<tr><td><code id="stringdot_+3A_type">type</code></td>
<td>
<p>Type of string kernel, currently the following kernels are
supported : <br />
</p>
<p><code>spectrum</code> the kernel considers only matching substring of
exactly length <code class="reqn">n</code> (also know as string kernel). Each such matching
substring is given a constant weight. The length parameter in this
kernel has to be <code class="reqn">length &gt; 1</code>.<br />
</p>
<p><code>boundrange</code>
this kernel (also known as boundrange) considers only matching substrings of length less than or equal to a
given number N. This type of string kernel requires a length
parameter <code class="reqn">length &gt; 1</code><br />
</p>
<p><code>constant</code>
The kernel considers all matching substrings and assigns constant weight (e.g. 1) to each
of them. This <code>constant</code> kernel does not require any additional
parameter.<br />
</p>
<p><code>exponential</code>
Exponential Decay kernel where the substring weight decays as the
matching substring gets longer. The kernel requires a decay factor <code class="reqn">
      \lambda &gt; 1</code><br />
</p>
<p><code>string</code> essentially identical to the spectrum kernel, only
computed using a more conventional way.<br />
</p>
<p><code>fullstring</code> essentially identical to the boundrange kernel
only computed in a more conventional way. <br />
</p>
</td></tr>
<tr><td><code id="stringdot_+3A_normalized">normalized</code></td>
<td>
<p>normalize string kernel values, (default: <code>TRUE</code>)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The kernel generating functions are used to initialize a kernel function
which calculates the dot (inner) product between two feature vectors in a
Hilbert Space. These functions or their function generating names
can be passed as a <code>kernel</code> argument on almost all
functions in <span class="pkg">kernlab</span>(e.g., <code>ksvm</code>, <code>kpca</code>  etc.).
</p>
<p>The string kernels calculate similarities between two strings
(e.g. texts or sequences) by matching the common substring
in the strings.  Different types of string kernel exists and are
mainly distinguished by how the matching is performed i.e. some string
kernels count the exact matchings of <code class="reqn">n</code> characters (spectrum
kernel) between the strings, others allow gaps (mismatch kernel) etc.
</p>


<h3>Value</h3>

<p>Returns an S4 object of class <code>stringkernel</code> which extents the
<code>function</code> class. The resulting function implements the given
kernel calculating the inner (dot) product between two character vectors.
</p>
<table>
<tr><td><code>kpar</code></td>
<td>
<p>a list containing the kernel parameters (hyperparameters)
used.</p>
</td></tr>
</table>
<p>The kernel parameters can be accessed by the <code>kpar</code> function.
</p>


<h3>Note</h3>

<p> The <code>spectrum</code> and <code>boundrange</code> kernel are faster and
more efficient implementations of the <code>string</code> and
<code>fullstring</code> kernels
which will be still included in <code>kernlab</code> for the next two versions.
</p>


<h3>Author(s)</h3>

<p>Alexandros Karatzoglou<br />
<a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>See Also</h3>

 <p><code><a href="#topic+dots">dots</a> </code>, <code><a href="#topic+kernelMatrix">kernelMatrix</a> </code>, <code><a href="#topic+kernelMult">kernelMult</a></code>, <code><a href="#topic+kernelPol">kernelPol</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
sk &lt;- stringdot(type="string", length=5)

sk



</code></pre>

<hr>
<h2 id='ticdata'>The Insurance Company Data</h2><span id='topic+ticdata'></span>

<h3>Description</h3>

<p>This data set used in the CoIL 2000 Challenge contains information on customers of an insurance
company. The data consists of 86 variables and includes product usage data and socio-demographic
data derived from zip area codes.
The data was collected to answer the following question: Can you
predict who would be interested in buying a caravan insurance policy and give an explanation why ?
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(ticdata)</code></pre>


<h3>Format</h3>

<p>ticdata: Dataset to train and validate prediction models and build a description (9822
customer records). Each record consists of 86 attributes, containing
sociodemographic data (attribute 1-43) and product ownership (attributes 44-86).
The sociodemographic data is derived from zip codes. All customers
living in areas with the same zip code have the same sociodemographic
attributes. Attribute 86, <code>CARAVAN:Number of mobile home policies</code>, is the target variable.
</p>
<p>Data Format
</p>

<table>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 1 </td><td style="text-align: left;"> <code>STYPE</code> </td><td style="text-align: left;"> Customer Subtype</td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 2 </td><td style="text-align: left;"> <code>MAANTHUI</code> </td><td style="text-align: left;"> Number of houses 1 - 10</td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 3 </td><td style="text-align: left;"> <code>MGEMOMV</code> </td><td style="text-align: left;"> Avg size household 1 - 6</td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 4 </td><td style="text-align: left;"> <code>MGEMLEEF</code> </td><td style="text-align: left;"> Average age</td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 5 </td><td style="text-align: left;"> <code>MOSHOOFD</code> </td><td style="text-align: left;"> Customer main type</td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 6 </td><td style="text-align: left;"> <code>MGODRK</code> </td><td style="text-align: left;"> Roman catholic   </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 7 </td><td style="text-align: left;"> <code>MGODPR</code> </td><td style="text-align: left;"> Protestant ...   </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 8 </td><td style="text-align: left;"> <code>MGODOV</code> </td><td style="text-align: left;"> Other religion    </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 9 </td><td style="text-align: left;"> <code>MGODGE</code> </td><td style="text-align: left;"> No religion      </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 10 </td><td style="text-align: left;"> <code>MRELGE</code> </td><td style="text-align: left;"> Married        </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 11 </td><td style="text-align: left;"> <code>MRELSA</code> </td><td style="text-align: left;"> Living together  </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 12 </td><td style="text-align: left;"> <code>MRELOV</code> </td><td style="text-align: left;"> Other relation   </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 13 </td><td style="text-align: left;"> <code>MFALLEEN</code> </td><td style="text-align: left;"> Singles         </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 14 </td><td style="text-align: left;"> <code>MFGEKIND</code> </td><td style="text-align: left;"> Household without children    </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 15 </td><td style="text-align: left;"> <code>MFWEKIND</code> </td><td style="text-align: left;"> Household with children </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 16 </td><td style="text-align: left;"> <code>MOPLHOOG</code> </td><td style="text-align: left;"> High level education    </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 17 </td><td style="text-align: left;"> <code>MOPLMIDD</code> </td><td style="text-align: left;"> Medium level education  </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 18 </td><td style="text-align: left;"> <code>MOPLLAAG</code> </td><td style="text-align: left;"> Lower level education  </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 19 </td><td style="text-align: left;"> <code>MBERHOOG</code> </td><td style="text-align: left;"> High status      </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 20 </td><td style="text-align: left;"> <code>MBERZELF</code> </td><td style="text-align: left;"> Entrepreneur       </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 21 </td><td style="text-align: left;"> <code>MBERBOER</code> </td><td style="text-align: left;"> Farmer            </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 22 </td><td style="text-align: left;"> <code>MBERMIDD</code> </td><td style="text-align: left;"> Middle management  </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 23 </td><td style="text-align: left;"> <code>MBERARBG</code> </td><td style="text-align: left;"> Skilled labourers  </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 24 </td><td style="text-align: left;"> <code>MBERARBO</code> </td><td style="text-align: left;"> Unskilled labourers </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 25 </td><td style="text-align: left;"> <code>MSKA</code> </td><td style="text-align: left;"> Social class A          </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 26 </td><td style="text-align: left;"> <code>MSKB1</code> </td><td style="text-align: left;"> Social class B1        </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 27 </td><td style="text-align: left;"> <code>MSKB2</code> </td><td style="text-align: left;"> Social class B2        </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 28 </td><td style="text-align: left;"> <code>MSKC</code> </td><td style="text-align: left;"> Social class C           </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 29 </td><td style="text-align: left;"> <code>MSKD</code> </td><td style="text-align: left;"> Social class D           </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 30 </td><td style="text-align: left;"> <code>MHHUUR</code> </td><td style="text-align: left;"> Rented house        </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 31 </td><td style="text-align: left;"> <code>MHKOOP</code> </td><td style="text-align: left;"> Home owners         </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 32 </td><td style="text-align: left;"> <code>MAUT1</code> </td><td style="text-align: left;"> 1 car                </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 33 </td><td style="text-align: left;"> <code>MAUT2</code> </td><td style="text-align: left;"> 2 cars                </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 34 </td><td style="text-align: left;"> <code>MAUT0</code> </td><td style="text-align: left;"> No car   </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 35 </td><td style="text-align: left;"> <code>MZFONDS</code> </td><td style="text-align: left;"> National Health Service    </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 36 </td><td style="text-align: left;"> <code>MZPART</code> </td><td style="text-align: left;"> Private health insurance     </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 37 </td><td style="text-align: left;"> <code>MINKM30</code> </td><td style="text-align: left;"> Income  &gt;30.000      </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 38 </td><td style="text-align: left;"> <code>MINK3045</code> </td><td style="text-align: left;"> Income 30-45.000    </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 39 </td><td style="text-align: left;"> <code>MINK4575</code> </td><td style="text-align: left;"> Income 45-75.000    </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 40 </td><td style="text-align: left;"> <code>MINK7512</code> </td><td style="text-align: left;"> Income 75-122.000   </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 41 </td><td style="text-align: left;"> <code>MINK123M</code> </td><td style="text-align: left;"> Income &lt;123.000    </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 42 </td><td style="text-align: left;"> <code>MINKGEM</code> </td><td style="text-align: left;"> Average income       </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 43 </td><td style="text-align: left;"> <code>MKOOPKLA</code> </td><td style="text-align: left;"> Purchasing power class  </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 44 </td><td style="text-align: left;"> <code>PWAPART</code> </td><td style="text-align: left;"> Contribution private third party insurance  </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 45 </td><td style="text-align: left;"> <code>PWABEDR</code> </td><td style="text-align: left;"> Contribution third party insurance (firms)  </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 46 </td><td style="text-align: left;"> <code>PWALAND</code> </td><td style="text-align: left;"> Contribution third party insurance (agriculture) </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 47 </td><td style="text-align: left;"> <code>PPERSAUT</code> </td><td style="text-align: left;"> Contribution car policies          </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 48 </td><td style="text-align: left;"> <code>PBESAUT</code> </td><td style="text-align: left;"> Contribution delivery van policies  </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 49 </td><td style="text-align: left;"> <code>PMOTSCO</code> </td><td style="text-align: left;"> Contribution motorcycle/scooter policies   </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 50 </td><td style="text-align: left;"> <code>PVRAAUT</code> </td><td style="text-align: left;"> Contribution lorry policies             </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 51 </td><td style="text-align: left;"> <code>PAANHANG</code> </td><td style="text-align: left;"> Contribution trailer policies         </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 52 </td><td style="text-align: left;"> <code>PTRACTOR</code> </td><td style="text-align: left;"> Contribution tractor policies        </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 53 </td><td style="text-align: left;"> <code>PWERKT</code> </td><td style="text-align: left;"> Contribution agricultural machines policies   </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 54 </td><td style="text-align: left;"> <code>PBROM</code> </td><td style="text-align: left;"> Contribution moped policies       </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 55 </td><td style="text-align: left;"> <code>PLEVEN</code> </td><td style="text-align: left;"> Contribution life insurances     </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 56 </td><td style="text-align: left;"> <code>PPERSONG</code> </td><td style="text-align: left;"> Contribution private accident insurance policies </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 57 </td><td style="text-align: left;"> <code>PGEZONG</code> </td><td style="text-align: left;"> Contribution family accidents insurance policies </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 58 </td><td style="text-align: left;"> <code>PWAOREG</code> </td><td style="text-align: left;"> Contribution disability insurance policies </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 59 </td><td style="text-align: left;"> <code>PBRAND</code> </td><td style="text-align: left;"> Contribution fire policies           </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 60 </td><td style="text-align: left;"> <code>PZEILPL</code> </td><td style="text-align: left;"> Contribution surfboard policies    </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 61 </td><td style="text-align: left;"> <code>PPLEZIER</code> </td><td style="text-align: left;"> Contribution boat policies      </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 62 </td><td style="text-align: left;"> <code>PFIETS</code> </td><td style="text-align: left;"> Contribution bicycle policies       </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 63 </td><td style="text-align: left;"> <code>PINBOED</code> </td><td style="text-align: left;"> Contribution property insurance policies   </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 64 </td><td style="text-align: left;"> <code>PBYSTAND</code> </td><td style="text-align: left;"> Contribution social security insurance policies  </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 65 </td><td style="text-align: left;"> <code>AWAPART</code> </td><td style="text-align: left;"> Number of private third party insurance 1 - 12   </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 66 </td><td style="text-align: left;"> <code>AWABEDR</code> </td><td style="text-align: left;"> Number of third party insurance (firms) ...   </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 67 </td><td style="text-align: left;"> <code>AWALAND</code> </td><td style="text-align: left;"> Number of third party insurance (agriculture)  </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 68 </td><td style="text-align: left;"> <code>APERSAUT</code> </td><td style="text-align: left;"> Number of car policies          </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 69 </td><td style="text-align: left;"> <code>ABESAUT</code> </td><td style="text-align: left;"> Number of delivery van policies   </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 70 </td><td style="text-align: left;"> <code>AMOTSCO</code> </td><td style="text-align: left;"> Number of motorcycle/scooter policies  </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 71 </td><td style="text-align: left;"> <code>AVRAAUT</code> </td><td style="text-align: left;"> Number of lorry policies    </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 72 </td><td style="text-align: left;"> <code>AAANHANG</code> </td><td style="text-align: left;"> Number of trailer policies  </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 73 </td><td style="text-align: left;"> <code>ATRACTOR</code> </td><td style="text-align: left;"> Number of tractor policies  </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 74 </td><td style="text-align: left;"> <code>AWERKT</code> </td><td style="text-align: left;"> Number of agricultural machines policies     </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 75 </td><td style="text-align: left;"> <code>ABROM</code> </td><td style="text-align: left;"> Number of moped policies          </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 76 </td><td style="text-align: left;"> <code>ALEVEN</code> </td><td style="text-align: left;"> Number of life insurances        </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 77 </td><td style="text-align: left;"> <code>APERSONG</code> </td><td style="text-align: left;"> Number of private accident insurance policies   </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 78 </td><td style="text-align: left;"> <code>AGEZONG</code> </td><td style="text-align: left;"> Number of family accidents insurance policies   </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 79 </td><td style="text-align: left;"> <code>AWAOREG</code> </td><td style="text-align: left;"> Number of disability insurance policies         </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 80 </td><td style="text-align: left;"> <code>ABRAND</code> </td><td style="text-align: left;"> Number of fire policies            </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 81 </td><td style="text-align: left;"> <code>AZEILPL</code> </td><td style="text-align: left;"> Number of surfboard policies     </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 82 </td><td style="text-align: left;"> <code>APLEZIER</code> </td><td style="text-align: left;"> Number of boat policies        </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 83 </td><td style="text-align: left;"> <code>AFIETS</code> </td><td style="text-align: left;"> Number of bicycle policies         </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 84 </td><td style="text-align: left;"> <code>AINBOED</code> </td><td style="text-align: left;"> Number of property insurance policies   </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 85 </td><td style="text-align: left;"> <code>ABYSTAND</code> </td><td style="text-align: left;"> Number of social security insurance policies  </td>
</tr>
<tr>
 <td style="text-align: right;">
</td><td style="text-align: left;"> 86 </td><td style="text-align: left;"> <code>CARAVAN</code> </td><td style="text-align: left;"> Number of mobile home policies 0 - 1          </td>
</tr>
<tr>
 <td style="text-align: right;">
</td>
</tr>

</table>

<p>Note: All the variables starting with M are zipcode variables. They give information on the
distribution of that variable, e.g., Rented house, in the zipcode area of
the customer.
</p>


<h3>Details</h3>

<p>Information about the insurance company customers consists of 86
variables and includes
product usage data and socio-demographic data derived from zip area codes. The
data was supplied by the Dutch data mining company Sentient Machine
Research and is based on a real world business problem. The training
set contains over 5000 descriptions of customers, including the
information of whether or not they have a caravan insurance policy.
The test set contains 4000 customers. The test and data set are merged in the
ticdata set.
More information about the data set and the CoIL 2000 Challenge along
with publications based on the data set can be found at <a href="http://www.liacs.nl/~putten/library/cc2000/">http://www.liacs.nl/~putten/library/cc2000/</a>.
</p>


<h3>Source</h3>


<ul>
<li><p> UCI KDD Archive:<a href="http://kdd.ics.uci.edu">http://kdd.ics.uci.edu</a> 
</p>
</li>
<li><p> Donor:  Sentient Machine Research <br />
Peter van der Putten <br />
Sentient Machine Research <br />
Baarsjesweg 224 <br />
1058 AA Amsterdam <br />
The Netherlands <br />
+31 20 6186927 <br />
pvdputten@hotmail.com, putten@liacs.nl
</p>
</li></ul>



<h3>References</h3>

<p>Peter van der Putten, Michel de Ruiter, Maarten van
Someren  <em>CoIL Challenge 2000 Tasks and Results: Predicting and
Explaining Caravan Policy Ownership</em><br />
<a href="http://www.liacs.nl/~putten/library/cc2000/">http://www.liacs.nl/~putten/library/cc2000/</a></p>

<hr>
<h2 id='vm-class'>Class &quot;vm&quot; </h2><span id='topic+vm-class'></span><span id='topic+cross'></span><span id='topic+alpha'></span><span id='topic+error'></span><span id='topic+type'></span><span id='topic+kernelf'></span><span id='topic+xmatrix'></span><span id='topic+ymatrix'></span><span id='topic+lev'></span><span id='topic+kcall'></span><span id='topic+alpha+2Cvm-method'></span><span id='topic+cross+2Cvm-method'></span><span id='topic+error+2Cvm-method'></span><span id='topic+fitted+2Cvm-method'></span><span id='topic+kernelf+2Cvm-method'></span><span id='topic+kpar+2Cvm-method'></span><span id='topic+lev+2Cvm-method'></span><span id='topic+kcall+2Cvm-method'></span><span id='topic+type+2Cvm-method'></span><span id='topic+xmatrix+2Cvm-method'></span><span id='topic+ymatrix+2Cvm-method'></span>

<h3>Description</h3>

<p>An S4 VIRTUAL class used as a base for the various vector
machine classes in <span class="pkg">kernlab</span></p>


<h3>Objects from the Class</h3>

<p>Objects from the class cannot be created directly but only contained
in other classes.
</p>


<h3>Slots</h3>


<dl>
<dt><code>alpha</code>:</dt><dd><p>Object of class <code>"listI"</code> containing the
resulting alpha vector (list in case of multiclass classification) (support vectors)</p>
</dd>
<dt><code>type</code>:</dt><dd><p>Object of class <code>"character"</code>  containing
the vector machine type e.g.,
(&quot;C-svc&quot;, &quot;nu-svc&quot;, &quot;C-bsvc&quot;, &quot;spoc-svc&quot;,
&quot;one-svc&quot;, &quot;eps-svr&quot;, &quot;nu-svr&quot;, &quot;eps-bsvr&quot;)</p>
</dd>
<dt><code>kernelf</code>:</dt><dd><p>Object of class <code>"function"</code> containing
the kernel function</p>
</dd>
<dt><code>kpar</code>:</dt><dd><p>Object of class <code>"list"</code> containing the
kernel function parameters (hyperparameters)</p>
</dd>
<dt><code>kcall</code>:</dt><dd><p>Object of class <code>"call"</code> containing the function call</p>
</dd>
<dt><code>terms</code>:</dt><dd><p>Object of class <code>"ANY"</code> containing the
terms representation of the symbolic model used (when using a formula)</p>
</dd>
<dt><code>xmatrix</code>:</dt><dd><p>Object of class <code>"input"</code> the data
matrix used during computations (support vectors) (possibly scaled and without NA)</p>
</dd>
<dt><code>ymatrix</code>:</dt><dd><p>Object of class <code>"output"</code> the response matrix/vector </p>
</dd>
<dt><code>fitted</code>:</dt><dd><p>Object of class <code>"output"</code> with the fitted values,
predictions using the training set.</p>
</dd>
<dt><code>lev</code>:</dt><dd><p>Object of class <code>"vector"</code> with the levels of the
response (in the case of classification)</p>
</dd>
<dt><code>nclass</code>:</dt><dd><p>Object of class <code>"numeric"</code>  containing
the number of classes (in the case of classification)</p>
</dd>
<dt><code>error</code>:</dt><dd><p>Object of class <code>"vector"</code> containing the
training error</p>
</dd>
<dt><code>cross</code>:</dt><dd><p>Object of class <code>"vector"</code> containing the
cross-validation error </p>
</dd>
<dt><code>n.action</code>:</dt><dd><p>Object of class <code>"ANY"</code> containing the
action performed for NA </p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>alpha</dt><dd><p><code>signature(object = "vm")</code>: returns the complete
alpha vector (wit zero values)</p>
</dd>
<dt>cross</dt><dd><p><code>signature(object = "vm")</code>: returns the
cross-validation error </p>
</dd>
<dt>error</dt><dd><p><code>signature(object = "vm")</code>: returns the training
error </p>
</dd>
<dt>fitted</dt><dd><p><code>signature(object = "vm")</code>: returns the fitted
values (predict on training set) </p>
</dd>
<dt>kernelf</dt><dd><p><code>signature(object = "vm")</code>: returns the kernel
function</p>
</dd>
<dt>kpar</dt><dd><p><code>signature(object = "vm")</code>: returns the kernel
parameters (hyperparameters)</p>
</dd>
<dt>lev</dt><dd><p><code>signature(object = "vm")</code>: returns the levels in
case of classification  </p>
</dd>
<dt>kcall</dt><dd><p><code>signature(object="vm")</code>: returns the function call</p>
</dd>
<dt>type</dt><dd><p><code>signature(object = "vm")</code>: returns the problem type</p>
</dd>
<dt>xmatrix</dt><dd><p><code>signature(object = "vm")</code>: returns the data
matrix used(support vectors)</p>
</dd>
<dt>ymatrix</dt><dd><p><code>signature(object = "vm")</code>: returns the
response vector</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Alexandros Karatzoglou <br /> <a href="mailto:alexandros.karatzolgou@ci.tuwien.ac.at">alexandros.karatzolgou@ci.tuwien.ac.at</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+ksvm-class">ksvm-class</a></code>, 
<code><a href="#topic+rvm-class">rvm-class</a></code>,
<code><a href="#topic+gausspr-class">gausspr-class</a></code>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
