<!DOCTYPE html><html><head><title>Help for package pense</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {pense}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#.approx_match'><p>Approximate Value Matching</p></a></li>
<li><a href='#.bisquare_consistency_const'><p>Get the Constant for Consistency for the M-Scale Using the Bisquare Rho Function</p></a></li>
<li><a href='#.find_stable_bdb_bisquare'><p>Determine a breakdown point with stable numerical properties of the M-scale</p>
with Tukey's bisquare rho function.</a></li>
<li><a href='#.run_replicated_cv'><p>Run replicated K-fold CV with random splits</p></a></li>
<li><a href='#.standardize_data'><p>Standardize data</p></a></li>
<li><a href='#cd_algorithm_options'><p>Coordinate Descent (CD) Algorithm to Compute Penalized Elastic Net</p>
S-estimates</a></li>
<li><a href='#coef.pense_cvfit'><p>Extract Coefficient Estimates</p></a></li>
<li><a href='#coef.pense_fit'><p>Extract Coefficient Estimates</p></a></li>
<li><a href='#consistency_const'><p>Get the Constant for Consistency for the M-Scale</p></a></li>
<li><a href='#deprecated_en_options'><p>Deprecated</p></a></li>
<li><a href='#elnet'><p>Compute the Least Squares (Adaptive) Elastic Net Regularization Path</p></a></li>
<li><a href='#elnet_cv'><p>Cross-validation for Least-Squares (Adaptive) Elastic Net Estimates</p></a></li>
<li><a href='#en_admm_options'><p>Use the ADMM Elastic Net Algorithm</p></a></li>
<li><a href='#en_algorithm_options'><p>Control the Algorithm to Compute (Weighted) Least-Squares Elastic</p>
Net Estimates</a></li>
<li><a href='#en_cd_options'><p>Use Coordinate Descent to Solve Elastic Net Problems</p></a></li>
<li><a href='#en_dal_options'><p>Use the DAL Elastic Net Algorithm</p></a></li>
<li><a href='#en_lars_options'><p>Use the LARS Elastic Net Algorithm</p></a></li>
<li><a href='#en_ridge_options'><p>Ridge optimizer using an Augmented data matrix.</p>
Only available for Ridge problems ('alpha=0&ldquo;) and selected automatically in this case.</a></li>
<li><a href='#enpy'><p>Deprecated</p></a></li>
<li><a href='#enpy_initial_estimates'><p>ENPY Initial Estimates for EN S-Estimators</p></a></li>
<li><a href='#enpy_options'><p>Options for the ENPY Algorithm</p></a></li>
<li><a href='#initest_options'><p>Deprecated</p></a></li>
<li><a href='#mloc'><p>Compute the M-estimate of Location</p></a></li>
<li><a href='#mlocscale'><p>Compute the M-estimate of Location and Scale</p></a></li>
<li><a href='#mm_algorithm_options'><p>MM-Algorithm to Compute Penalized Elastic Net S- and M-Estimates</p></a></li>
<li><a href='#mscale'><p>Compute the M-Scale of Centered Values</p></a></li>
<li><a href='#mscale_algorithm_options'><p>Options for the M-scale Estimation Algorithm</p></a></li>
<li><a href='#mscale_derivative'><p>Compute the Gradient and Hessian of the M-Scale Function</p></a></li>
<li><a href='#mstep_options'><p>Deprecated</p></a></li>
<li><a href='#pense'><p>Compute (Adaptive) Elastic Net S-Estimates of Regression</p></a></li>
<li><a href='#pense_cv'><p>Cross-validation for (Adaptive) PENSE Estimates</p></a></li>
<li><a href='#pense_options'><p>Deprecated</p></a></li>
<li><a href='#pensem'><p>Deprecated Alias of pensem_cv</p></a></li>
<li><a href='#pensem_cv'><p>Compute Penalized Elastic Net M-Estimates from PENSE</p></a></li>
<li><a href='#plot.pense_cvfit'><p>Plot Method for Penalized Estimates With Cross-Validation</p></a></li>
<li><a href='#plot.pense_fit'><p>Plot Method for Penalized Estimates</p></a></li>
<li><a href='#predict.pense_cvfit'><p>Predict Method for PENSE Fits</p></a></li>
<li><a href='#predict.pense_fit'><p>Predict Method for PENSE Fits</p></a></li>
<li><a href='#prediction_performance'><p>Prediction Performance of Adaptive PENSE Fits</p></a></li>
<li><a href='#prinsens'><p>Principal Sensitivity Components</p></a></li>
<li><a href='#print.nsoptim_metrics'><p>Print Metrics</p></a></li>
<li><a href='#regmest'><p>Compute (Adaptive) Elastic Net M-Estimates of Regression</p></a></li>
<li><a href='#regmest_cv'><p>Cross-validation for (Adaptive) Elastic Net M-Estimates</p></a></li>
<li><a href='#residuals.pense_cvfit'><p>Extract Residuals</p></a></li>
<li><a href='#residuals.pense_fit'><p>Extract Residuals</p></a></li>
<li><a href='#rho_function'><p>List Available Rho Functions</p></a></li>
<li><a href='#starting_point'><p>Create Starting Points for the PENSE Algorithm</p></a></li>
<li><a href='#summary.pense_cvfit'><p>Summarize Cross-Validated PENSE Fit</p></a></li>
<li><a href='#tau_size'><p>Compute the Tau-Scale of Centered Values</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Penalized Elastic Net S/MM-Estimator of Regression</td>
</tr>
<tr>
<td>Version:</td>
<td>2.2.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-02-05</td>
</tr>
<tr>
<td>Copyright:</td>
<td>See the file COPYRIGHT for copyright details on some of the
functions and algorithms used.</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Biarch:</td>
<td>true</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://dakep.github.io/pense-rpkg/">https://dakep.github.io/pense-rpkg/</a>,
<a href="https://github.com/dakep/pense-rpkg">https://github.com/dakep/pense-rpkg</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/dakep/pense-rpkg/issues">https://github.com/dakep/pense-rpkg/issues</a></td>
</tr>
<tr>
<td>Description:</td>
<td>Robust penalized (adaptive) elastic net S and M estimators for
    linear regression. The methods are proposed in
    Cohen Freue, G. V., Kepplinger, D., Salibián-Barrera, M., and Smucler, E.
    (2019) <a href="https://projecteuclid.org/euclid.aoas/1574910036">https://projecteuclid.org/euclid.aoas/1574910036</a>.
    The package implements the extensions and algorithms described in
    Kepplinger, D. (2020) &lt;<a href="https://doi.org/10.14288%2F1.0392915">doi:10.14288/1.0392915</a>&gt;.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0), Matrix</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp, methods, parallel, lifecycle (&ge; 0.2.0), rlang (&ge;
0.4.0)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo (&ge; 0.9.600)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat (&ge; 2.1.0), knitr, rmarkdown, jsonlite</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.2</td>
</tr>
<tr>
<td>RdMacros:</td>
<td>lifecycle</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-02-07 01:48:59 UTC; david</td>
</tr>
<tr>
<td>Author:</td>
<td>David Kepplinger [aut, cre],
  Matías Salibián-Barrera [aut],
  Gabriela Cohen Freue [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>David Kepplinger &lt;david.kepplinger@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-02-07 08:12:32 UTC</td>
</tr>
</table>
<hr>
<h2 id='.approx_match'>Approximate Value Matching</h2><span id='topic+.approx_match'></span>

<h3>Description</h3>

<p>Approximate Value Matching
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.approx_match(x, table, eps)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".approx_match_+3A_x">x</code>, <code id=".approx_match_+3A_table">table</code></td>
<td>
<p>see <a href="base.html#topic+match">base::match</a> for details.</p>
</td></tr>
<tr><td><code id=".approx_match_+3A_eps">eps</code></td>
<td>
<p>numerical tolerance for matching.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector the same length as <code>x</code> with integers giving the position in
<code>table</code> of the first match if there is a match, or <code>NA_integer_</code>
otherwise.
</p>

<hr>
<h2 id='.bisquare_consistency_const'>Get the Constant for Consistency for the M-Scale Using the Bisquare Rho Function</h2><span id='topic+.bisquare_consistency_const'></span>

<h3>Description</h3>

<p>Get the Constant for Consistency for the M-Scale Using the Bisquare Rho Function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.bisquare_consistency_const(delta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".bisquare_consistency_const_+3A_delta">delta</code></td>
<td>
<p>desired breakdown point (between 0 and 0.5)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>consistency constant
</p>

<hr>
<h2 id='.find_stable_bdb_bisquare'>Determine a breakdown point with stable numerical properties of the M-scale
with Tukey's bisquare rho function.</h2><span id='topic+.find_stable_bdb_bisquare'></span>

<h3>Description</h3>

<p>The M-scale objective (and hence the S-loss) can have unbounded or very high
1st derivative. This can lead to numerical instability of the algorithms and
in turn excessive computation time.
This function chooses the breakdown point with lowest upper bound of the 1st
derivative from a range of bdp's in the vicinity of the desired bdp.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.find_stable_bdb_bisquare(
  n,
  desired_bdp,
  tolerance = 0.01,
  precision = 1e-04,
  interval = c(0.05, 0.5)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".find_stable_bdb_bisquare_+3A_n">n</code></td>
<td>
<p>number of observations in the sample</p>
</td></tr>
<tr><td><code id=".find_stable_bdb_bisquare_+3A_desired_bdp">desired_bdp</code></td>
<td>
<p>the desired breakdown point (between 0.05 and 0.5)</p>
</td></tr>
<tr><td><code id=".find_stable_bdb_bisquare_+3A_tolerance">tolerance</code></td>
<td>
<p>how far can the chosen bdp be away from the desired bdp.
The chosen bdp is guaranteed to be in the range given by <code>interval</code>.</p>
</td></tr>
<tr><td><code id=".find_stable_bdb_bisquare_+3A_precision">precision</code></td>
<td>
<p>granularity of the grid of considered bdp's.</p>
</td></tr>
<tr><td><code id=".find_stable_bdb_bisquare_+3A_interval">interval</code></td>
<td>
<p>restrict the chosen bdp to this interval.</p>
</td></tr>
</table>

<hr>
<h2 id='.run_replicated_cv'>Run replicated K-fold CV with random splits</h2><span id='topic+.run_replicated_cv'></span>

<h3>Description</h3>

<p>Run replicated K-fold CV with random splits
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.run_replicated_cv(
  std_data,
  cv_k,
  cv_repl,
  cv_est_fun,
  metric,
  par_cluster = NULL,
  handler_args = list()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".run_replicated_cv_+3A_std_data">std_data</code></td>
<td>
<p>standardized full data set
(standardized by <code>.standardize_data</code>)</p>
</td></tr>
<tr><td><code id=".run_replicated_cv_+3A_cv_k">cv_k</code></td>
<td>
<p>number of folds per CV split</p>
</td></tr>
<tr><td><code id=".run_replicated_cv_+3A_cv_repl">cv_repl</code></td>
<td>
<p>number of CV replications.</p>
</td></tr>
<tr><td><code id=".run_replicated_cv_+3A_cv_est_fun">cv_est_fun</code></td>
<td>
<p>function taking the standardized training set and
the indices of the left-out observations and returns a list of estimates.
The function always needs to return the same number of estimates!</p>
</td></tr>
<tr><td><code id=".run_replicated_cv_+3A_metric">metric</code></td>
<td>
<p>function taking a vector of prediction errors and
returning the scale of the prediction error.</p>
</td></tr>
<tr><td><code id=".run_replicated_cv_+3A_par_cluster">par_cluster</code></td>
<td>
<p>parallel cluster to parallelize computations.</p>
</td></tr>
<tr><td><code id=".run_replicated_cv_+3A_handler_args">handler_args</code></td>
<td>
<p>additional arguments to the handler function.</p>
</td></tr>
</table>

<hr>
<h2 id='.standardize_data'>Standardize data</h2><span id='topic+.standardize_data'></span>

<h3>Description</h3>

<p>Standardize data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.standardize_data(
  x,
  y,
  intercept,
  standardize,
  robust,
  sparse,
  mscale_opts,
  location_rho = "bisquare",
  cc,
  target_scale_x = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".standardize_data_+3A_x">x</code></td>
<td>
<p>predictor matrix. Can also be a list with components <code>x</code> and <code>y</code>,
in which case <code>y</code> is ignored.</p>
</td></tr>
<tr><td><code id=".standardize_data_+3A_y">y</code></td>
<td>
<p>response vector.</p>
</td></tr>
<tr><td><code id=".standardize_data_+3A_intercept">intercept</code></td>
<td>
<p>is an intercept included (i.e., should <code>y</code> be centered?)</p>
</td></tr>
<tr><td><code id=".standardize_data_+3A_standardize">standardize</code></td>
<td>
<p>standardize or not.</p>
</td></tr>
<tr><td><code id=".standardize_data_+3A_robust">robust</code></td>
<td>
<p>use robust standardization.</p>
</td></tr>
<tr><td><code id=".standardize_data_+3A_location_rho">location_rho</code></td>
<td>
<p>rho function for location estimate</p>
</td></tr>
<tr><td><code id=".standardize_data_+3A_cc">cc</code></td>
<td>
<p>cutoff value for the rho functions used in scale and location
estimates.</p>
</td></tr>
<tr><td><code id=".standardize_data_+3A_...">...</code></td>
<td>
<p>passed on to <code>mlocscale()</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with the following entries:
</p>

<hr>
<h2 id='cd_algorithm_options'>Coordinate Descent (CD) Algorithm to Compute Penalized Elastic Net
S-estimates</h2><span id='topic+cd_algorithm_options'></span>

<h3>Description</h3>

<p>Set options for the CD algorithm to compute adaptive EN S-estimates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cd_algorithm_options(
  max_it = 1000,
  reset_it = 8,
  linesearch_steps = 4,
  linesearch_mult = 0.5
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cd_algorithm_options_+3A_max_it">max_it</code></td>
<td>
<p>maximum number of iterations.</p>
</td></tr>
<tr><td><code id="cd_algorithm_options_+3A_reset_it">reset_it</code></td>
<td>
<p>number of iterations after which the residuals are
re-computed from scratch, to prevent numerical drifts from incremental
updates.</p>
</td></tr>
<tr><td><code id="cd_algorithm_options_+3A_linesearch_steps">linesearch_steps</code></td>
<td>
<p>maximum number of steps used for line search.</p>
</td></tr>
<tr><td><code id="cd_algorithm_options_+3A_linesearch_mult">linesearch_mult</code></td>
<td>
<p>multiplier to adjust the step size in the line
search.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>options for the CD algorithm to compute (adaptive) PENSE estimates.
</p>


<h3>See Also</h3>

<p>mm_algorithm_options to optimize the non-convex PENSE objective
function via a sequence of convex problems.
</p>

<hr>
<h2 id='coef.pense_cvfit'>Extract Coefficient Estimates</h2><span id='topic+coef.pense_cvfit'></span>

<h3>Description</h3>

<p>Extract coefficients from an adaptive PENSE (or LS-EN) regularization path with hyper-parameters
chosen by cross-validation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pense_cvfit'
coef(
  object,
  alpha = NULL,
  lambda = "min",
  se_mult = 1,
  sparse = NULL,
  standardized = FALSE,
  exact = deprecated(),
  correction = deprecated(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef.pense_cvfit_+3A_object">object</code></td>
<td>
<p>PENSE with cross-validated hyper-parameters to extract coefficients from.</p>
</td></tr>
<tr><td><code id="coef.pense_cvfit_+3A_alpha">alpha</code></td>
<td>
<p>Either a single number or <code>NULL</code> (default).
If given, only fits with the given <code>alpha</code> value are considered.
If <code>lambda</code> is a numeric value and <code>object</code> was fit with multiple <em>alpha</em>
values and no value is provided, the first value in <code>object$alpha</code> is used with a warning.</p>
</td></tr>
<tr><td><code id="coef.pense_cvfit_+3A_lambda">lambda</code></td>
<td>
<p>either a string specifying which penalty level to use
(<code>"min"</code>, <code>"se"</code>, <code style="white-space: pre;">&#8288;"{m}-se&#8288;</code>&quot;)
or a single numeric value of the penalty parameter. See details.</p>
</td></tr>
<tr><td><code id="coef.pense_cvfit_+3A_se_mult">se_mult</code></td>
<td>
<p>If <code>lambda = "se"</code>, the multiple of standard errors to tolerate.</p>
</td></tr>
<tr><td><code id="coef.pense_cvfit_+3A_sparse">sparse</code></td>
<td>
<p>should coefficients be returned as sparse or dense vectors?
Defaults to the sparsity setting of the given <code>object</code>.
Can also be set to <code>sparse = 'matrix'</code>, in which case a sparse matrix
is returned instead of a sparse vector.</p>
</td></tr>
<tr><td><code id="coef.pense_cvfit_+3A_standardized">standardized</code></td>
<td>
<p>return the standardized coefficients.</p>
</td></tr>
<tr><td><code id="coef.pense_cvfit_+3A_exact">exact</code>, <code id="coef.pense_cvfit_+3A_correction">correction</code></td>
<td>
<p>defunct.</p>
</td></tr>
<tr><td><code id="coef.pense_cvfit_+3A_...">...</code></td>
<td>
<p>currently not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>either a numeric vector or a sparse vector of type
<a href="Matrix.html#topic+sparseVector-class">dsparseVector</a>
of size <code class="reqn">p + 1</code>, depending on the <code>sparse</code> argument.
Note: prior to version 2.0.0 sparse coefficients were returned as sparse matrix of
type <em>dgCMatrix</em>.
To get a sparse matrix as in previous versions, use <code>sparse = 'matrix'</code>.
</p>


<h3>Hyper-parameters</h3>

<p>If <code>lambda = "{m}-se"</code> and <code>object</code> contains fitted estimates for every penalization
level in the sequence, use the fit the most parsimonious model with prediction performance
statistically indistinguishable from the best model.
This is determined to be the model with prediction performance within <code>m * cv_se</code>
from the best model.
If <code>lambda = "se"</code>, the multiplier <em>m</em> is taken from <code>se_mult</code>.
</p>
<p>By default all <em>alpha</em> hyper-parameters available in the fitted object are considered.
This can be overridden by supplying one or multiple values in parameter <code>alpha</code>.
For example, if <code>lambda = "1-se"</code> and <code>alpha</code> contains two values, the &quot;1-SE&quot; rule is applied
individually for each <code>alpha</code> value, and the fit with the better prediction error is considered.
</p>
<p>In case <code>lambda</code> is a number and <code>object</code> was fit for several <em>alpha</em> hyper-parameters,
<code>alpha</code> must also be given, or the first value in <code>object$alpha</code> is used with a warning.
</p>


<h3>See Also</h3>

<p>Other functions for extracting components: 
<code><a href="#topic+coef.pense_fit">coef.pense_fit</a>()</code>,
<code><a href="#topic+predict.pense_cvfit">predict.pense_cvfit</a>()</code>,
<code><a href="#topic+predict.pense_fit">predict.pense_fit</a>()</code>,
<code><a href="#topic+residuals.pense_cvfit">residuals.pense_cvfit</a>()</code>,
<code><a href="#topic+residuals.pense_fit">residuals.pense_fit</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Compute the PENSE regularization path for Freeny's revenue data
# (see ?freeny)
data(freeny)
x &lt;- as.matrix(freeny[ , 2:5])

regpath &lt;- pense(x, freeny$y, alpha = 0.5)
plot(regpath)

# Extract the coefficients at a certain penalization level
coef(regpath, lambda = regpath$lambda[[1]][[40]])

# What penalization level leads to good prediction performance?
set.seed(123)
cv_results &lt;- pense_cv(x, freeny$y, alpha = 0.5,
                       cv_repl = 2, cv_k = 4)
plot(cv_results, se_mult = 1)

# Extract the coefficients at the penalization level with
# smallest prediction error ...
coef(cv_results)
# ... or at the penalization level with prediction error
# statistically indistinguishable from the minimum.
coef(cv_results, lambda = '1-se')
</code></pre>

<hr>
<h2 id='coef.pense_fit'>Extract Coefficient Estimates</h2><span id='topic+coef.pense_fit'></span>

<h3>Description</h3>

<p>Extract coefficients from an adaptive PENSE (or LS-EN) regularization path fitted by <code><a href="#topic+pense">pense()</a></code>
or <code><a href="#topic+elnet">elnet()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pense_fit'
coef(
  object,
  lambda,
  alpha = NULL,
  sparse = NULL,
  standardized = FALSE,
  exact = deprecated(),
  correction = deprecated(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef.pense_fit_+3A_object">object</code></td>
<td>
<p>PENSE regularization path to extract coefficients from.</p>
</td></tr>
<tr><td><code id="coef.pense_fit_+3A_lambda">lambda</code></td>
<td>
<p>a single number for the penalty level.</p>
</td></tr>
<tr><td><code id="coef.pense_fit_+3A_alpha">alpha</code></td>
<td>
<p>Either a single number or <code>NULL</code> (default).
If given, only fits with the given <code>alpha</code> value are considered.
If <code>object</code> was fit with multiple <code>alpha</code> values, and no value is provided, the
first value in <code>object$alpha</code> is used with a warning.</p>
</td></tr>
<tr><td><code id="coef.pense_fit_+3A_sparse">sparse</code></td>
<td>
<p>should coefficients be returned as sparse or dense vectors? Defaults to the
sparsity setting in <code>object</code>.
Can also be set to <code>sparse = 'matrix'</code>, in which case a sparse matrix
is returned instead of a sparse vector.</p>
</td></tr>
<tr><td><code id="coef.pense_fit_+3A_standardized">standardized</code></td>
<td>
<p>return the standardized coefficients.</p>
</td></tr>
<tr><td><code id="coef.pense_fit_+3A_exact">exact</code>, <code id="coef.pense_fit_+3A_correction">correction</code></td>
<td>
<p>defunct.</p>
</td></tr>
<tr><td><code id="coef.pense_fit_+3A_...">...</code></td>
<td>
<p>currently not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>either a numeric vector or a sparse vector of type
<a href="Matrix.html#topic+sparseVector-class">dsparseVector</a>
of size <code class="reqn">p + 1</code>, depending on the <code>sparse</code> argument.
Note: prior to version 2.0.0 sparse coefficients were returned as sparse matrix
of type <em>dgCMatrix</em>.
To get a sparse matrix as in previous versions, use <code>sparse = 'matrix'</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+coef.pense_cvfit">coef.pense_cvfit()</a></code> for extracting coefficients from a PENSE fit with
hyper-parameters chosen by cross-validation
</p>
<p>Other functions for extracting components: 
<code><a href="#topic+coef.pense_cvfit">coef.pense_cvfit</a>()</code>,
<code><a href="#topic+predict.pense_cvfit">predict.pense_cvfit</a>()</code>,
<code><a href="#topic+predict.pense_fit">predict.pense_fit</a>()</code>,
<code><a href="#topic+residuals.pense_cvfit">residuals.pense_cvfit</a>()</code>,
<code><a href="#topic+residuals.pense_fit">residuals.pense_fit</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Compute the PENSE regularization path for Freeny's revenue data
# (see ?freeny)
data(freeny)
x &lt;- as.matrix(freeny[ , 2:5])

regpath &lt;- pense(x, freeny$y, alpha = 0.5)
plot(regpath)

# Extract the coefficients at a certain penalization level
coef(regpath, lambda = regpath$lambda[[1]][[40]])

# What penalization level leads to good prediction performance?
set.seed(123)
cv_results &lt;- pense_cv(x, freeny$y, alpha = 0.5,
                       cv_repl = 2, cv_k = 4)
plot(cv_results, se_mult = 1)

# Extract the coefficients at the penalization level with
# smallest prediction error ...
coef(cv_results)
# ... or at the penalization level with prediction error
# statistically indistinguishable from the minimum.
coef(cv_results, lambda = '1-se')
</code></pre>

<hr>
<h2 id='consistency_const'>Get the Constant for Consistency for the M-Scale</h2><span id='topic+consistency_const'></span>

<h3>Description</h3>

<p>Get the Constant for Consistency for the M-Scale
</p>


<h3>Usage</h3>

<pre><code class='language-R'>consistency_const(delta, rho)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="consistency_const_+3A_delta">delta</code></td>
<td>
<p>desired breakdown point (between 0 and 0.5)</p>
</td></tr>
<tr><td><code id="consistency_const_+3A_rho">rho</code></td>
<td>
<p>the name of the chosen <code class="reqn">\rho</code> function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>consistency constant
</p>


<h3>See Also</h3>

<p>Other miscellaneous functions: 
<code><a href="#topic+rho_function">rho_function</a>()</code>
</p>

<hr>
<h2 id='deprecated_en_options'>Deprecated</h2><span id='topic+deprecated_en_options'></span><span id='topic+en_options_aug_lars'></span><span id='topic+en_options_dal'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#deprecated"><img src="../help/figures/lifecycle-deprecated.svg" alt='[Deprecated]' /></a>
</p>
<p>Options for computing EN estimates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>en_options_aug_lars(use_gram = c("auto", "yes", "no"), eps = 1e-12)

en_options_dal(
  maxit = 100,
  eps = 1e-08,
  eta_mult = 2,
  eta_start_numerator = 0.01,
  eta_start,
  preconditioner = c("approx", "none", "diagonal"),
  verbosity = 0
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="deprecated_en_options_+3A_use_gram">use_gram</code></td>
<td>
<p><strong>ignored.</strong> Should the Gram matrix be pre-computed.</p>
</td></tr>
<tr><td><code id="deprecated_en_options_+3A_eps">eps</code></td>
<td>
<p><strong>ignored.</strong> Numeric tolerance for convergence.</p>
</td></tr>
<tr><td><code id="deprecated_en_options_+3A_maxit">maxit</code></td>
<td>
<p>maximum number of iterations allowed.</p>
</td></tr>
<tr><td><code id="deprecated_en_options_+3A_eta_mult">eta_mult</code></td>
<td>
<p>multiplier to increase eta at each iteration.</p>
</td></tr>
<tr><td><code id="deprecated_en_options_+3A_eta_start_numerator">eta_start_numerator</code></td>
<td>
<p>if <code>eta_start</code> is missing, it is defined
by <code>eta_start = eta_start_numerator / lambda</code>.</p>
</td></tr>
<tr><td><code id="deprecated_en_options_+3A_eta_start">eta_start</code></td>
<td>
<p><strong>ignored.</strong> The start value for eta.</p>
</td></tr>
<tr><td><code id="deprecated_en_options_+3A_preconditioner">preconditioner</code></td>
<td>
<p><strong>ignored.</strong> Preconditioner for the numerical solver. If none,
a standard solver will be used, otherwise the faster preconditioned
conjugate gradient is used.</p>
</td></tr>
<tr><td><code id="deprecated_en_options_+3A_verbosity">verbosity</code></td>
<td>
<p><strong>ignored.</strong></p>
</td></tr>
</table>


<h3>Functions</h3>


<ul>
<li> <p><code>en_options_aug_lars()</code>: Superseded by <code><a href="#topic+en_lars_options">en_lars_options()</a></code>.
</p>
</li>
<li> <p><code>en_options_dal()</code>: Superseded by <code><a href="#topic+en_dal_options">en_dal_options()</a></code>
</p>
</li></ul>


<h3>Warning</h3>

<p>Do not use these functions in new code.
They may be removed from future versions of the package.
</p>


<h3>See Also</h3>

<p>Other deprecated functions: 
<code><a href="#topic+enpy">enpy</a>()</code>,
<code><a href="#topic+initest_options">initest_options</a>()</code>,
<code><a href="#topic+mstep_options">mstep_options</a>()</code>,
<code><a href="#topic+pense_options">pense_options</a>()</code>,
<code><a href="#topic+pensem">pensem</a>()</code>
</p>

<hr>
<h2 id='elnet'>Compute the Least Squares (Adaptive) Elastic Net Regularization Path</h2><span id='topic+elnet'></span><span id='topic+adaelnet'></span><span id='topic+adaen'></span>

<h3>Description</h3>

<p>Compute least squares EN estimates for linear regression with optional observation
weights and penalty loadings.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>elnet(
  x,
  y,
  alpha,
  nlambda = 100,
  lambda_min_ratio,
  lambda,
  penalty_loadings,
  weights,
  intercept = TRUE,
  en_algorithm_opts,
  sparse = FALSE,
  eps = 1e-06,
  standardize = TRUE,
  correction = deprecated(),
  xtest = deprecated(),
  options = deprecated()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="elnet_+3A_x">x</code></td>
<td>
<p><code>n</code> by <code>p</code> matrix of numeric predictors.</p>
</td></tr>
<tr><td><code id="elnet_+3A_y">y</code></td>
<td>
<p>vector of response values of length <code>n</code>.
For binary classification, <code>y</code> should be a factor with 2 levels.</p>
</td></tr>
<tr><td><code id="elnet_+3A_alpha">alpha</code></td>
<td>
<p>elastic net penalty mixing parameter with <code class="reqn">0 \le \alpha \le 1</code>.
<code>alpha = 1</code> is the LASSO penalty, and <code>alpha = 0</code> the Ridge penalty.
Can be a vector of several values, but <code>alpha = 0</code> cannot be mixed with other values.</p>
</td></tr>
<tr><td><code id="elnet_+3A_nlambda">nlambda</code></td>
<td>
<p>number of penalization levels.</p>
</td></tr>
<tr><td><code id="elnet_+3A_lambda_min_ratio">lambda_min_ratio</code></td>
<td>
<p>Smallest value of the penalization level as a fraction of the largest
level (i.e., the smallest value for which all coefficients are zero).
The default depends on the sample size relative to the number of variables and <code>alpha</code>.
If more observations than variables are available, the default is <code>1e-3 * alpha</code>,
otherwise <code>1e-2 * alpha</code>.</p>
</td></tr>
<tr><td><code id="elnet_+3A_lambda">lambda</code></td>
<td>
<p>optional user-supplied sequence of penalization levels.
If given and not <code>NULL</code>, <code>nlambda</code> and <code>lambda_min_ratio</code> are ignored.</p>
</td></tr>
<tr><td><code id="elnet_+3A_penalty_loadings">penalty_loadings</code></td>
<td>
<p>a vector of positive penalty loadings (a.k.a. weights) for
different penalization of each coefficient.</p>
</td></tr>
<tr><td><code id="elnet_+3A_weights">weights</code></td>
<td>
<p>a vector of positive observation weights.</p>
</td></tr>
<tr><td><code id="elnet_+3A_intercept">intercept</code></td>
<td>
<p>include an intercept in the model.</p>
</td></tr>
<tr><td><code id="elnet_+3A_en_algorithm_opts">en_algorithm_opts</code></td>
<td>
<p>options for the EN algorithm. See <a href="#topic+en_algorithm_options">en_algorithm_options</a>
for details.</p>
</td></tr>
<tr><td><code id="elnet_+3A_sparse">sparse</code></td>
<td>
<p>use sparse coefficient vectors.</p>
</td></tr>
<tr><td><code id="elnet_+3A_eps">eps</code></td>
<td>
<p>numerical tolerance.</p>
</td></tr>
<tr><td><code id="elnet_+3A_standardize">standardize</code></td>
<td>
<p>standardize variables to have unit variance.
Coefficients are always returned in original scale.</p>
</td></tr>
<tr><td><code id="elnet_+3A_correction">correction</code></td>
<td>
<p>defunct. Correction for EN estimates is not supported anymore.</p>
</td></tr>
<tr><td><code id="elnet_+3A_xtest">xtest</code></td>
<td>
<p>defunct.</p>
</td></tr>
<tr><td><code id="elnet_+3A_options">options</code></td>
<td>
<p>deprecated. Use <code>en_algorithm_opts</code> instead.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The elastic net estimator for the linear regression model solves
the optimization problem
</p>
<p style="text-align: center;"><code class="reqn">argmin_{\mu, \beta}
  (1/2n) \sum_i w_i (y_i - \mu - x_i' \beta)^2 +
  \lambda \sum_j 0.5 (1 - \alpha) \beta_j^2 + \alpha l_j |\beta_j|  </code>
</p>

<p>with observation weights <code class="reqn">w_i</code> and penalty loadings <code class="reqn">l_j</code>.
</p>


<h3>Value</h3>

<p>a list-like object with the following items
</p>

<dl>
<dt><code>alpha</code></dt><dd><p>the sequence of <code>alpha</code> parameters.</p>
</dd>
<dt><code>lambda</code></dt><dd><p>a list of sequences of penalization levels, one per <code>alpha</code> parameter.</p>
</dd>
<dt><code>estimates</code></dt><dd><p>a list of estimates. Each estimate contains the following information:
</p>

<dl>
<dt><code>intercept</code></dt><dd><p>intercept estimate.</p>
</dd>
<dt><code>beta</code></dt><dd><p>beta (slope) estimate.</p>
</dd>
<dt><code>lambda</code></dt><dd><p>penalization level at which the estimate is computed.</p>
</dd>
<dt><code>alpha</code></dt><dd><p><em>alpha</em> hyper-parameter at which the estimate is computed.</p>
</dd>
<dt><code>statuscode</code></dt><dd><p>if <code style="white-space: pre;">&#8288;&gt; 0&#8288;</code> the algorithm experienced issues when
computing the estimate.</p>
</dd>
<dt><code>status</code></dt><dd><p>optional status message from the algorithm.</p>
</dd>
</dl>

</dd>
<dt><code>call</code></dt><dd><p>the original call.</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="#topic+pense">pense()</a></code> for an S-estimate of regression with elastic net penalty.
</p>
<p><code><a href="#topic+coef.pense_fit">coef.pense_fit()</a></code> for extracting coefficient estimates.
</p>
<p><code><a href="#topic+plot.pense_fit">plot.pense_fit()</a></code> for plotting the regularization path.
</p>
<p>Other functions for computing non-robust estimates: 
<code><a href="#topic+elnet_cv">elnet_cv</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Compute the LS-EN regularization path for Freeny's revenue data
# (see ?freeny)
data(freeny)
x &lt;- as.matrix(freeny[ , 2:5])

regpath &lt;- elnet(x, freeny$y, alpha = c(0.5, 0.75))
plot(regpath)
plot(regpath, alpha = 0.75)

# Extract the coefficients at a certain penalization level
coef(regpath, lambda = regpath$lambda[[1]][[5]],
     alpha = 0.75)

# What penalization level leads to good prediction performance?
set.seed(123)
cv_results &lt;- elnet_cv(x, freeny$y, alpha = c(0.5, 0.75),
                       cv_repl = 10, cv_k = 4,
                       cv_measure = "tau")
plot(cv_results, se_mult = 1.5)
plot(cv_results, se_mult = 1.5, what = "coef.path")


# Extract the coefficients at the penalization level with
# smallest prediction error ...
summary(cv_results)
coef(cv_results)
# ... or at the penalization level with prediction error
# statistically indistinguishable from the minimum.
summary(cv_results, lambda = "1.5-se")
coef(cv_results, lambda = "1.5-se")
</code></pre>

<hr>
<h2 id='elnet_cv'>Cross-validation for Least-Squares (Adaptive) Elastic Net Estimates</h2><span id='topic+elnet_cv'></span>

<h3>Description</h3>

<p>Perform (repeated) K-fold cross-validation for <code><a href="#topic+elnet">elnet()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>elnet_cv(
  x,
  y,
  lambda,
  cv_k,
  cv_repl = 1,
  cv_metric = c("rmspe", "tau_size", "mape", "auroc"),
  fit_all = TRUE,
  cl = NULL,
  ncores = deprecated(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="elnet_cv_+3A_x">x</code></td>
<td>
<p><code>n</code> by <code>p</code> matrix of numeric predictors.</p>
</td></tr>
<tr><td><code id="elnet_cv_+3A_y">y</code></td>
<td>
<p>vector of response values of length <code>n</code>.
For binary classification, <code>y</code> should be a factor with 2 levels.</p>
</td></tr>
<tr><td><code id="elnet_cv_+3A_lambda">lambda</code></td>
<td>
<p>optional user-supplied sequence of penalization levels.
If given and not <code>NULL</code>, <code>nlambda</code> and <code>lambda_min_ratio</code> are ignored.</p>
</td></tr>
<tr><td><code id="elnet_cv_+3A_cv_k">cv_k</code></td>
<td>
<p>number of folds per cross-validation.</p>
</td></tr>
<tr><td><code id="elnet_cv_+3A_cv_repl">cv_repl</code></td>
<td>
<p>number of cross-validation replications.</p>
</td></tr>
<tr><td><code id="elnet_cv_+3A_cv_metric">cv_metric</code></td>
<td>
<p>either a string specifying the performance metric to use, or a function to
evaluate prediction errors in a single CV replication.
If a function, the number of arguments define the data the function receives.
If the function takes a single argument, it is called with a single numeric vector of
prediction errors.
If the function takes two or more arguments, it is called with the predicted values as
first argument and the true values as second argument.
The function must always return a single numeric value quantifying the prediction performance.
The order of the given values corresponds to the order in the input data.</p>
</td></tr>
<tr><td><code id="elnet_cv_+3A_fit_all">fit_all</code></td>
<td>
<p>If <code>TRUE</code>, fit the model for all penalization levels.
Can also be any combination of <code>"min"</code> and <code>"{x}-se"</code>, in which case only models at the
penalization level with smallest average CV accuracy, or within <code>{x}</code> standard errors,
respectively.
Setting <code>fit_all</code> to <code>FALSE</code> is equivalent to <code>"min"</code>.
Applies to all <code>alpha</code> value.</p>
</td></tr>
<tr><td><code id="elnet_cv_+3A_cl">cl</code></td>
<td>
<p>a <a href="parallel.html#topic+makeCluster">parallel</a> cluster. Can only be used in combination with
<code>ncores = 1</code>.</p>
</td></tr>
<tr><td><code id="elnet_cv_+3A_ncores">ncores</code></td>
<td>
<p>deprecated and not used anymore.</p>
</td></tr>
<tr><td><code id="elnet_cv_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+elnet">elnet</a></code>
</p>

<dl>
<dt><code>alpha</code></dt><dd><p>elastic net penalty mixing parameter with <code class="reqn">0 \le \alpha \le 1</code>.
<code>alpha = 1</code> is the LASSO penalty, and <code>alpha = 0</code> the Ridge penalty.
Can be a vector of several values, but <code>alpha = 0</code> cannot be mixed with other values.</p>
</dd>
<dt><code>nlambda</code></dt><dd><p>number of penalization levels.</p>
</dd>
<dt><code>lambda_min_ratio</code></dt><dd><p>Smallest value of the penalization level as a fraction of the largest
level (i.e., the smallest value for which all coefficients are zero).
The default depends on the sample size relative to the number of variables and <code>alpha</code>.
If more observations than variables are available, the default is <code>1e-3 * alpha</code>,
otherwise <code>1e-2 * alpha</code>.</p>
</dd>
<dt><code>penalty_loadings</code></dt><dd><p>a vector of positive penalty loadings (a.k.a. weights) for
different penalization of each coefficient.</p>
</dd>
<dt><code>standardize</code></dt><dd><p>standardize variables to have unit variance.
Coefficients are always returned in original scale.</p>
</dd>
<dt><code>weights</code></dt><dd><p>a vector of positive observation weights.</p>
</dd>
<dt><code>intercept</code></dt><dd><p>include an intercept in the model.</p>
</dd>
<dt><code>sparse</code></dt><dd><p>use sparse coefficient vectors.</p>
</dd>
<dt><code>en_algorithm_opts</code></dt><dd><p>options for the EN algorithm. See <a href="#topic+en_algorithm_options">en_algorithm_options</a>
for details.</p>
</dd>
<dt><code>eps</code></dt><dd><p>numerical tolerance.</p>
</dd>
<dt><code>xtest</code></dt><dd><p>defunct.</p>
</dd>
<dt><code>options</code></dt><dd><p>deprecated. Use <code>en_algorithm_opts</code> instead.</p>
</dd>
<dt><code>correction</code></dt><dd><p>defunct. Correction for EN estimates is not supported anymore.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>The built-in CV metrics are
</p>

<dl>
<dt><code>"tau_size"</code></dt><dd><p><code class="reqn">\tau</code>-size of the prediction error, computed by
<code><a href="#topic+tau_size">tau_size()</a></code> (default).</p>
</dd>
<dt><code>"mape"</code></dt><dd><p>Median absolute prediction error.</p>
</dd>
<dt><code>"rmspe"</code></dt><dd><p>Root mean squared prediction error.</p>
</dd>
<dt><code>"auroc"</code></dt><dd><p>Area under the receiver operator characteristic curve (actually 1 - AUROC).
Only sensible for binary responses.</p>
</dd>
</dl>



<h3>Value</h3>

<p>a list-like object with the same components as returned by <code><a href="#topic+elnet">elnet()</a></code>,
plus the following:
</p>

<dl>
<dt><code>cvres</code></dt><dd><p>data frame of average cross-validated performance.</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="#topic+elnet">elnet()</a></code> for computing the LS-EN regularization path without cross-validation.
</p>
<p><code><a href="#topic+pense_cv">pense_cv()</a></code> for cross-validation of S-estimates of regression with elastic net penalty.
</p>
<p><code><a href="#topic+coef.pense_cvfit">coef.pense_cvfit()</a></code> for extracting coefficient estimates.
</p>
<p><code><a href="#topic+plot.pense_cvfit">plot.pense_cvfit()</a></code> for plotting the CV performance or the regularization path.
</p>
<p>Other functions for computing non-robust estimates: 
<code><a href="#topic+elnet">elnet</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Compute the LS-EN regularization path for Freeny's revenue data
# (see ?freeny)
data(freeny)
x &lt;- as.matrix(freeny[ , 2:5])

regpath &lt;- elnet(x, freeny$y, alpha = c(0.5, 0.75))
plot(regpath)
plot(regpath, alpha = 0.75)

# Extract the coefficients at a certain penalization level
coef(regpath, lambda = regpath$lambda[[1]][[5]],
     alpha = 0.75)

# What penalization level leads to good prediction performance?
set.seed(123)
cv_results &lt;- elnet_cv(x, freeny$y, alpha = c(0.5, 0.75),
                       cv_repl = 10, cv_k = 4,
                       cv_measure = "tau")
plot(cv_results, se_mult = 1.5)
plot(cv_results, se_mult = 1.5, what = "coef.path")


# Extract the coefficients at the penalization level with
# smallest prediction error ...
summary(cv_results)
coef(cv_results)
# ... or at the penalization level with prediction error
# statistically indistinguishable from the minimum.
summary(cv_results, lambda = "1.5-se")
coef(cv_results, lambda = "1.5-se")
</code></pre>

<hr>
<h2 id='en_admm_options'>Use the ADMM Elastic Net Algorithm</h2><span id='topic+en_admm_options'></span>

<h3>Description</h3>

<p>Use the ADMM Elastic Net Algorithm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>en_admm_options(max_it = 1000, step_size, acceleration = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="en_admm_options_+3A_max_it">max_it</code></td>
<td>
<p>maximum number of iterations.</p>
</td></tr>
<tr><td><code id="en_admm_options_+3A_step_size">step_size</code></td>
<td>
<p>step size for the algorithm.</p>
</td></tr>
<tr><td><code id="en_admm_options_+3A_acceleration">acceleration</code></td>
<td>
<p>acceleration factor for linearized ADMM.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>options for the ADMM EN algorithm.
</p>


<h3>See Also</h3>

<p>Other EN algorithms: 
<code><a href="#topic+en_cd_options">en_cd_options</a>()</code>,
<code><a href="#topic+en_dal_options">en_dal_options</a>()</code>,
<code><a href="#topic+en_lars_options">en_lars_options</a>()</code>
</p>

<hr>
<h2 id='en_algorithm_options'>Control the Algorithm to Compute (Weighted) Least-Squares Elastic
Net Estimates</h2><span id='topic+en_algorithm_options'></span>

<h3>Description</h3>

<p>The package supports different algorithms to compute the EN estimate
for weighted LS loss functions.
Each algorithm has certain characteristics that make it useful for some
problems.
To select a specific algorithm and adjust the options, use any of
the <code style="white-space: pre;">&#8288;en_***_options&#8288;</code> functions.
</p>


<h3>Details</h3>


<ul>
<li> <p><code><a href="#topic+en_lars_options">en_lars_options()</a></code>: Use the tuning-free LARS algorithm.
This computes <em>exact</em> (up to numerical errors) solutions to the EN-LS
problem.
It is not iterative and therefore can not benefit from approximate
solutions, but in turn guarantees that a solution will be found.
</p>
</li>
<li> <p><code><a href="#topic+en_cd_options">en_cd_options()</a></code>: Use an iterative coordinate descent algorithm which
needs <code class="reqn">O(n p)</code> operations per iteration and converges sub-linearly.
</p>
</li>
<li> <p><code><a href="#topic+en_admm_options">en_admm_options()</a></code>: Use an iterative ADMM-type algorithm which needs
<code class="reqn">O(n p)</code> operations per iteration and converges sub-linearly.
</p>
</li>
<li> <p><code><a href="#topic+en_dal_options">en_dal_options()</a></code>: Use the iterative Dual Augmented Lagrangian (DAL)
method.
DAL needs <code class="reqn">O(n^3 p^2)</code> operations per iteration, but converges
exponentially.
</p>
</li></ul>


<hr>
<h2 id='en_cd_options'>Use Coordinate Descent to Solve Elastic Net Problems</h2><span id='topic+en_cd_options'></span>

<h3>Description</h3>

<p>Use Coordinate Descent to Solve Elastic Net Problems
</p>


<h3>Usage</h3>

<pre><code class='language-R'>en_cd_options(max_it = 1000, reset_it = 8)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="en_cd_options_+3A_max_it">max_it</code></td>
<td>
<p>maximum number of iterations.</p>
</td></tr>
<tr><td><code id="en_cd_options_+3A_reset_it">reset_it</code></td>
<td>
<p>number of iterations after which the residuals are
re-computed from scratch, to prevent numerical drifts from incremental
updates.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other EN algorithms: 
<code><a href="#topic+en_admm_options">en_admm_options</a>()</code>,
<code><a href="#topic+en_dal_options">en_dal_options</a>()</code>,
<code><a href="#topic+en_lars_options">en_lars_options</a>()</code>
</p>

<hr>
<h2 id='en_dal_options'>Use the DAL Elastic Net Algorithm</h2><span id='topic+en_dal_options'></span>

<h3>Description</h3>

<p>Use the DAL Elastic Net Algorithm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>en_dal_options(
  max_it = 100,
  max_inner_it = 100,
  eta_multiplier = 2,
  eta_start_conservative = 0.01,
  eta_start_aggressive = 1,
  lambda_relchange_aggressive = 0.25
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="en_dal_options_+3A_max_it">max_it</code></td>
<td>
<p>maximum number of (outer) iterations.</p>
</td></tr>
<tr><td><code id="en_dal_options_+3A_max_inner_it">max_inner_it</code></td>
<td>
<p>maximum number of (inner) iterations in each outer iteration.</p>
</td></tr>
<tr><td><code id="en_dal_options_+3A_eta_multiplier">eta_multiplier</code></td>
<td>
<p>multiplier for the barrier parameter. In each iteration, the barrier must be more restrictive
(i.e., the multiplier must be &gt; 1).</p>
</td></tr>
<tr><td><code id="en_dal_options_+3A_eta_start_conservative">eta_start_conservative</code></td>
<td>
<p>conservative initial barrier parameter. This is used if the previous penalty is
undefined or too far away.</p>
</td></tr>
<tr><td><code id="en_dal_options_+3A_eta_start_aggressive">eta_start_aggressive</code></td>
<td>
<p>aggressive initial barrier parameter. This is used if the previous penalty is close.</p>
</td></tr>
<tr><td><code id="en_dal_options_+3A_lambda_relchange_aggressive">lambda_relchange_aggressive</code></td>
<td>
<p>how close must the lambda parameter from the previous penalty term be to use
an aggressive initial barrier parameter (i.e., what constitutes &quot;too far&quot;).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>options for the DAL EN algorithm.
</p>


<h3>See Also</h3>

<p>Other EN algorithms: 
<code><a href="#topic+en_admm_options">en_admm_options</a>()</code>,
<code><a href="#topic+en_cd_options">en_cd_options</a>()</code>,
<code><a href="#topic+en_lars_options">en_lars_options</a>()</code>
</p>

<hr>
<h2 id='en_lars_options'>Use the LARS Elastic Net Algorithm</h2><span id='topic+en_lars_options'></span>

<h3>Description</h3>

<p>Use the LARS Elastic Net Algorithm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>en_lars_options()
</code></pre>


<h3>See Also</h3>

<p>Other EN algorithms: 
<code><a href="#topic+en_admm_options">en_admm_options</a>()</code>,
<code><a href="#topic+en_cd_options">en_cd_options</a>()</code>,
<code><a href="#topic+en_dal_options">en_dal_options</a>()</code>
</p>

<hr>
<h2 id='en_ridge_options'>Ridge optimizer using an Augmented data matrix.
Only available for Ridge problems ('alpha=0&ldquo;) and selected automatically in this case.</h2><span id='topic+en_ridge_options'></span>

<h3>Description</h3>

<p>Ridge optimizer using an Augmented data matrix.
Only available for Ridge problems ('alpha=0&ldquo;) and selected automatically in this case.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>en_ridge_options()
</code></pre>

<hr>
<h2 id='enpy'>Deprecated</h2><span id='topic+enpy'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#deprecated"><img src="../help/figures/lifecycle-deprecated.svg" alt='[Deprecated]' /></a>
</p>
<p>Compute initial estimates for EN S-estimates using ENPY.
Superseded by <code><a href="#topic+enpy_initial_estimates">enpy_initial_estimates()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>enpy(x, y, alpha, lambda, delta, cc, options, en_options)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="enpy_+3A_x">x</code></td>
<td>
<p>data matrix with predictors.</p>
</td></tr>
<tr><td><code id="enpy_+3A_y">y</code></td>
<td>
<p>response vector.</p>
</td></tr>
<tr><td><code id="enpy_+3A_alpha">alpha</code>, <code id="enpy_+3A_lambda">lambda</code></td>
<td>
<p>EN penalty parameters (NOT adjusted for the number of
observations in <code>x</code>).</p>
</td></tr>
<tr><td><code id="enpy_+3A_delta">delta</code></td>
<td>
<p>desired breakdown point of the resulting estimator.</p>
</td></tr>
<tr><td><code id="enpy_+3A_cc">cc</code></td>
<td>
<p>tuning constant for the S-estimator. Default is to chosen based
on the breakdown point <code>delta</code>. Should never have to be changed.</p>
</td></tr>
<tr><td><code id="enpy_+3A_options">options</code></td>
<td>
<p><strong>ignored.</strong> Additional options for the initial estimator.</p>
</td></tr>
<tr><td><code id="enpy_+3A_en_options">en_options</code></td>
<td>
<p><strong>ignored.</strong> Additional options for the EN algorithm.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>coeff</code></td>
<td>
<p>A numeric matrix with one initial coefficient per column</p>
</td></tr>
<tr><td><code>objF</code></td>
<td>
<p>A vector of values of the objective function for the respective coefficient</p>
</td></tr>
</table>


<h3>Warning</h3>

<p>Do not use this function in new code.
It may be removed from future versions of the package.
</p>


<h3>See Also</h3>

<p>Other deprecated functions: 
<code><a href="#topic+deprecated_en_options">deprecated_en_options</a></code>,
<code><a href="#topic+initest_options">initest_options</a>()</code>,
<code><a href="#topic+mstep_options">mstep_options</a>()</code>,
<code><a href="#topic+pense_options">pense_options</a>()</code>,
<code><a href="#topic+pensem">pensem</a>()</code>
</p>

<hr>
<h2 id='enpy_initial_estimates'>ENPY Initial Estimates for EN S-Estimators</h2><span id='topic+enpy_initial_estimates'></span>

<h3>Description</h3>

<p>Compute initial estimates for the EN S-estimator using the EN-PY procedure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>enpy_initial_estimates(
  x,
  y,
  alpha,
  lambda,
  bdp = 0.25,
  cc,
  intercept = TRUE,
  penalty_loadings,
  enpy_opts = enpy_options(),
  mscale_opts = mscale_algorithm_options(),
  eps = 1e-06,
  sparse = FALSE,
  ncores = 1L
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="enpy_initial_estimates_+3A_x">x</code></td>
<td>
<p><code>n</code> by <code>p</code> matrix of numeric predictors.</p>
</td></tr>
<tr><td><code id="enpy_initial_estimates_+3A_y">y</code></td>
<td>
<p>vector of response values of length <code>n</code>.</p>
</td></tr>
<tr><td><code id="enpy_initial_estimates_+3A_alpha">alpha</code></td>
<td>
<p>elastic net penalty mixing parameter with <code class="reqn">0 \le \alpha \le 1</code>.
<code>alpha = 1</code> is the LASSO penalty, and <code>alpha = 0</code> the Ridge penalty.
Can be a vector of several values, but <code>alpha = 0</code> cannot be mixed with other values.</p>
</td></tr>
<tr><td><code id="enpy_initial_estimates_+3A_lambda">lambda</code></td>
<td>
<p>a vector of positive values of penalization levels.</p>
</td></tr>
<tr><td><code id="enpy_initial_estimates_+3A_bdp">bdp</code></td>
<td>
<p>desired breakdown point of the estimator, between 0.05 and 0.5. The actual
breakdown point may be slightly larger/smaller to avoid instabilities of the S-loss.</p>
</td></tr>
<tr><td><code id="enpy_initial_estimates_+3A_cc">cc</code></td>
<td>
<p>cutoff value for the bisquare rho function. By default, chosen to yield a consistent
estimate for the Normal distribution.</p>
</td></tr>
<tr><td><code id="enpy_initial_estimates_+3A_intercept">intercept</code></td>
<td>
<p>include an intercept in the model.</p>
</td></tr>
<tr><td><code id="enpy_initial_estimates_+3A_penalty_loadings">penalty_loadings</code></td>
<td>
<p>a vector of positive penalty loadings (a.k.a. weights) for different
penalization of each coefficient. Only allowed for <code>alpha</code> &gt; 0.</p>
</td></tr>
<tr><td><code id="enpy_initial_estimates_+3A_enpy_opts">enpy_opts</code></td>
<td>
<p>options for the EN-PY algorithm, created with the <code><a href="#topic+enpy_options">enpy_options()</a></code> function.</p>
</td></tr>
<tr><td><code id="enpy_initial_estimates_+3A_mscale_opts">mscale_opts</code></td>
<td>
<p>options for the M-scale estimation. See <code><a href="#topic+mscale_algorithm_options">mscale_algorithm_options()</a></code>
for details.</p>
</td></tr>
<tr><td><code id="enpy_initial_estimates_+3A_eps">eps</code></td>
<td>
<p>numerical tolerance.</p>
</td></tr>
<tr><td><code id="enpy_initial_estimates_+3A_sparse">sparse</code></td>
<td>
<p>use sparse coefficient vectors.</p>
</td></tr>
<tr><td><code id="enpy_initial_estimates_+3A_ncores">ncores</code></td>
<td>
<p>number of CPU cores to use in parallel. By default, only one CPU core is used.
Not supported on all platforms, in which case a warning is given.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If these manually computed initial estimates are intended as starting points for <code><a href="#topic+pense">pense()</a></code>,
they are by default <em>shared</em> for all penalization levels.
To restrict the use of the initial estimates to the penalty level they were computed for, use
<code>as_starting_point(..., specific = TRUE)</code>. See <code><a href="#topic+as_starting_point">as_starting_point()</a></code> for details.
</p>


<h3>References</h3>

<p>Cohen Freue, G.V.; Kepplinger, D.; Salibián-Barrera, M.; Smucler, E.
Robust elastic net estimators for variable selection and identification of
proteomic biomarkers.
<em>Ann. Appl. Stat.</em> <strong>13</strong> (2019), no. 4, 2065&ndash;2090 <a href="https://doi.org/10.1214/19-AOAS1269">doi:10.1214/19-AOAS1269</a>
</p>


<h3>See Also</h3>

<p>Other functions for initial estimates: 
<code><a href="#topic+prinsens">prinsens</a>()</code>,
<code><a href="#topic+starting_point">starting_point</a>()</code>
</p>

<hr>
<h2 id='enpy_options'>Options for the ENPY Algorithm</h2><span id='topic+enpy_options'></span>

<h3>Description</h3>

<p>Additional control options for the elastic net Peña-Yohai procedure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>enpy_options(
  max_it = 10,
  keep_psc_proportion = 0.5,
  en_algorithm_opts,
  keep_residuals_measure = c("threshold", "proportion"),
  keep_residuals_proportion = 0.5,
  keep_residuals_threshold = 2,
  retain_best_factor = 2,
  retain_max = 500
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="enpy_options_+3A_max_it">max_it</code></td>
<td>
<p>maximum number of EN-PY iterations.</p>
</td></tr>
<tr><td><code id="enpy_options_+3A_keep_psc_proportion">keep_psc_proportion</code></td>
<td>
<p>how many observations should to keep based on the Principal Sensitivity Components.</p>
</td></tr>
<tr><td><code id="enpy_options_+3A_en_algorithm_opts">en_algorithm_opts</code></td>
<td>
<p>options for the LS-EN algorithm. See <a href="#topic+en_algorithm_options">en_algorithm_options</a> for details.</p>
</td></tr>
<tr><td><code id="enpy_options_+3A_keep_residuals_measure">keep_residuals_measure</code></td>
<td>
<p>how to determine what observations to keep, based on their residuals.
If <code>proportion</code>, a fixed number of observations is kept.
If <code>threshold</code>, only observations with residuals below the threshold are kept.</p>
</td></tr>
<tr><td><code id="enpy_options_+3A_keep_residuals_proportion">keep_residuals_proportion</code></td>
<td>
<p>proportion of observations to kept based on their residuals.</p>
</td></tr>
<tr><td><code id="enpy_options_+3A_keep_residuals_threshold">keep_residuals_threshold</code></td>
<td>
<p>only observations with (standardized) residuals less than this threshold are kept.</p>
</td></tr>
<tr><td><code id="enpy_options_+3A_retain_best_factor">retain_best_factor</code></td>
<td>
<p>only keep candidates that are within this factor of the best candidate. If <code style="white-space: pre;">&#8288;&lt;= 1&#8288;</code>, only
keep candidates from the last iteration.</p>
</td></tr>
<tr><td><code id="enpy_options_+3A_retain_max">retain_max</code></td>
<td>
<p>maximum number of candidates, i.e., only the best <code>retain_max</code> candidates are retained.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The EN-PY procedure for computing initial estimates iteratively cleans the data of observations with possibly
outlying residual or high leverage. Least-squares elastic net (LS-EN) estimates are computed on the possibly clean
subsets. At each iteration, the Principal Sensitivity Components are computed to remove observations with potentially
high leverage. Among all the LS-EN estimates, the estimate with smallest M-scale of the residuals is selected.
Observations with largest residual for the selected estimate are removed and the next iteration is started.
</p>


<h3>Value</h3>

<p>options for the ENPY algorithm.
</p>

<hr>
<h2 id='initest_options'>Deprecated</h2><span id='topic+initest_options'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#deprecated"><img src="../help/figures/lifecycle-deprecated.svg" alt='[Deprecated]' /></a>
</p>
<p>Options for computing initial estimates via ENPY.
Superseded by <code><a href="#topic+enpy_options">enpy_options()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>initest_options(
  keep_solutions = 5,
  psc_method = c("exact", "rr"),
  maxit = 10,
  maxit_pense_refinement = 5,
  eps = 1e-06,
  psc_keep = 0.5,
  resid_keep_method = c("proportion", "threshold"),
  resid_keep_prop = 0.6,
  resid_keep_thresh = 2,
  mscale_eps = 1e-08,
  mscale_maxit = 200
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="initest_options_+3A_keep_solutions">keep_solutions</code></td>
<td>
<p>how many initial estimates should be kept to perform
full PENSE iterations?</p>
</td></tr>
<tr><td><code id="initest_options_+3A_psc_method">psc_method</code></td>
<td>
<p>The method to use for computing the principal sensitivity
components. See details for the possible choices.</p>
</td></tr>
<tr><td><code id="initest_options_+3A_maxit">maxit</code></td>
<td>
<p>maximum number of refinement iterations.</p>
</td></tr>
<tr><td><code id="initest_options_+3A_maxit_pense_refinement">maxit_pense_refinement</code></td>
<td>
<p><strong>ignored.</strong> Maximum number of PENSE iterations to refine
initial estimator.</p>
</td></tr>
<tr><td><code id="initest_options_+3A_eps">eps</code></td>
<td>
<p><strong>ignored.</strong> Numeric tolerance for convergence.</p>
</td></tr>
<tr><td><code id="initest_options_+3A_psc_keep">psc_keep</code></td>
<td>
<p>proportion of observations to keep based on the PSC scores.</p>
</td></tr>
<tr><td><code id="initest_options_+3A_resid_keep_method">resid_keep_method</code></td>
<td>
<p>How to clean the data based on large residuals.
If <code>"proportion"</code>, observations with the smallest
<code>resid_keep_prop</code> residuals will be retained.
If <code>"threshold"</code>, all observations with scaled residuals smaller
than the threshold <code>resid_keep_thresh</code> will be retained.</p>
</td></tr>
<tr><td><code id="initest_options_+3A_resid_keep_prop">resid_keep_prop</code>, <code id="initest_options_+3A_resid_keep_thresh">resid_keep_thresh</code></td>
<td>
<p>proportion or threshold for
observations to keep based on their residual.</p>
</td></tr>
<tr><td><code id="initest_options_+3A_mscale_eps">mscale_eps</code>, <code id="initest_options_+3A_mscale_maxit">mscale_maxit</code></td>
<td>
<p><strong>ignored.</strong> Maximum number of iterations and numeric
tolerance for the M-scale.</p>
</td></tr>
</table>


<h3>Warning</h3>

<p>Do not use this function in new code.
It may be removed from future versions of the package.
</p>


<h3>See Also</h3>

<p>Other deprecated functions: 
<code><a href="#topic+deprecated_en_options">deprecated_en_options</a></code>,
<code><a href="#topic+enpy">enpy</a>()</code>,
<code><a href="#topic+mstep_options">mstep_options</a>()</code>,
<code><a href="#topic+pense_options">pense_options</a>()</code>,
<code><a href="#topic+pensem">pensem</a>()</code>
</p>

<hr>
<h2 id='mloc'>Compute the M-estimate of Location</h2><span id='topic+mloc'></span>

<h3>Description</h3>

<p>Compute the M-estimate of location using an auxiliary estimate of the scale.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mloc(x, scale, rho, cc, opts = mscale_algorithm_options())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mloc_+3A_x">x</code></td>
<td>
<p>numeric values. Missing values are verbosely ignored.</p>
</td></tr>
<tr><td><code id="mloc_+3A_scale">scale</code></td>
<td>
<p>scale of the <code>x</code> values. If omitted, uses the <a href="stats.html#topic+mad">mad()</a>.</p>
</td></tr>
<tr><td><code id="mloc_+3A_rho">rho</code></td>
<td>
<p>the <code class="reqn">\rho</code> function to use. See <code><a href="#topic+rho_function">rho_function()</a></code> for available functions.</p>
</td></tr>
<tr><td><code id="mloc_+3A_cc">cc</code></td>
<td>
<p>value of the tuning constant for the chosen <code class="reqn">\rho</code> function.
By default, chosen to achieve 95% efficiency under the Normal distribution.</p>
</td></tr>
<tr><td><code id="mloc_+3A_opts">opts</code></td>
<td>
<p>a list of options for the M-estimating algorithm, see
<code><a href="#topic+mscale_algorithm_options">mscale_algorithm_options()</a></code> for details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a single numeric value, the M-estimate of location.
</p>


<h3>See Also</h3>

<p>Other functions to compute robust estimates of location and scale: 
<code><a href="#topic+mlocscale">mlocscale</a>()</code>,
<code><a href="#topic+mscale">mscale</a>()</code>,
<code><a href="#topic+tau_size">tau_size</a>()</code>
</p>

<hr>
<h2 id='mlocscale'>Compute the M-estimate of Location and Scale</h2><span id='topic+mlocscale'></span>

<h3>Description</h3>

<p>Simultaneous estimation of the location and scale by means of M-estimates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mlocscale(
  x,
  bdp = 0.25,
  scale_cc = consistency_const(bdp, "bisquare"),
  location_rho,
  location_cc,
  opts = mscale_algorithm_options()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mlocscale_+3A_x">x</code></td>
<td>
<p>numeric values. Missing values are verbosely ignored.</p>
</td></tr>
<tr><td><code id="mlocscale_+3A_bdp">bdp</code></td>
<td>
<p>desired breakdown point (between 0 and 0.5).</p>
</td></tr>
<tr><td><code id="mlocscale_+3A_scale_cc">scale_cc</code></td>
<td>
<p>cutoff value for the bisquare <code class="reqn">\rho</code> function for computing the
scale estimate.
By default, chosen to yield a consistent estimate for normally distributed values.</p>
</td></tr>
<tr><td><code id="mlocscale_+3A_location_rho">location_rho</code>, <code id="mlocscale_+3A_location_cc">location_cc</code></td>
<td>
<p><code class="reqn">\rho</code> function and cutoff value for computing
the location estimate.
See <code><a href="#topic+rho_function">rho_function()</a></code> for a list of available <code class="reqn">\rho</code> functions.</p>
</td></tr>
<tr><td><code id="mlocscale_+3A_opts">opts</code></td>
<td>
<p>a list of options for the M-estimating equation,
see <code><a href="#topic+mscale_algorithm_options">mscale_algorithm_options()</a></code> for details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector with 2 elements, the M-estimate of location and the M-scale estimate.
</p>


<h3>See Also</h3>

<p>Other functions to compute robust estimates of location and scale: 
<code><a href="#topic+mloc">mloc</a>()</code>,
<code><a href="#topic+mscale">mscale</a>()</code>,
<code><a href="#topic+tau_size">tau_size</a>()</code>
</p>

<hr>
<h2 id='mm_algorithm_options'>MM-Algorithm to Compute Penalized Elastic Net S- and M-Estimates</h2><span id='topic+mm_algorithm_options'></span>

<h3>Description</h3>

<p>Additional options for the MM algorithm to compute EN S- and M-estimates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mm_algorithm_options(
  max_it = 500,
  tightening = c("adaptive", "exponential", "none"),
  tightening_steps = 2,
  en_algorithm_opts
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mm_algorithm_options_+3A_max_it">max_it</code></td>
<td>
<p>maximum number of iterations.</p>
</td></tr>
<tr><td><code id="mm_algorithm_options_+3A_tightening">tightening</code></td>
<td>
<p>how to make inner iterations more precise as the algorithm
approaches a local minimum.</p>
</td></tr>
<tr><td><code id="mm_algorithm_options_+3A_tightening_steps">tightening_steps</code></td>
<td>
<p>for <em>adaptive</em> tightening strategy, how often to
tighten until the desired tolerance is attained.</p>
</td></tr>
<tr><td><code id="mm_algorithm_options_+3A_en_algorithm_opts">en_algorithm_opts</code></td>
<td>
<p>options for the inner LS-EN algorithm.
See <a href="#topic+en_algorithm_options">en_algorithm_options</a> for details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>options for the MM algorithm.
</p>


<h3>See Also</h3>

<p>cd_algorithm_options for a direct optimization of the non-convex
PENSE loss.
</p>

<hr>
<h2 id='mscale'>Compute the M-Scale of Centered Values</h2><span id='topic+mscale'></span>

<h3>Description</h3>

<p>Compute the M-scale without centering the values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mscale(
  x,
  bdp = 0.25,
  cc = consistency_const(bdp, "bisquare"),
  opts = mscale_algorithm_options(),
  delta = deprecated(),
  rho = deprecated(),
  eps = deprecated(),
  maxit = deprecated()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mscale_+3A_x">x</code></td>
<td>
<p>numeric values. Missing values are verbosely ignored.</p>
</td></tr>
<tr><td><code id="mscale_+3A_bdp">bdp</code></td>
<td>
<p>desired breakdown point (between 0 and 0.5).</p>
</td></tr>
<tr><td><code id="mscale_+3A_cc">cc</code></td>
<td>
<p>cutoff value for the bisquare rho function.
By default, chosen to yield a consistent estimate for the Normal distribution.</p>
</td></tr>
<tr><td><code id="mscale_+3A_opts">opts</code></td>
<td>
<p>a list of options for the M-scale estimation algorithm,
see <code><a href="#topic+mscale_algorithm_options">mscale_algorithm_options()</a></code> for details.</p>
</td></tr>
<tr><td><code id="mscale_+3A_delta">delta</code></td>
<td>
<p>deprecated. Use <code>bpd</code> instead.</p>
</td></tr>
<tr><td><code id="mscale_+3A_rho">rho</code>, <code id="mscale_+3A_eps">eps</code>, <code id="mscale_+3A_maxit">maxit</code></td>
<td>
<p>deprecated. Instead set control options for the algorithm
with the <code>opts</code> arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the M-estimate of scale.
</p>


<h3>See Also</h3>

<p>Other functions to compute robust estimates of location and scale: 
<code><a href="#topic+mlocscale">mlocscale</a>()</code>,
<code><a href="#topic+mloc">mloc</a>()</code>,
<code><a href="#topic+tau_size">tau_size</a>()</code>
</p>

<hr>
<h2 id='mscale_algorithm_options'>Options for the M-scale Estimation Algorithm</h2><span id='topic+mscale_algorithm_options'></span>

<h3>Description</h3>

<p>Options for the M-scale Estimation Algorithm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mscale_algorithm_options(max_it = 200, eps = 1e-08)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mscale_algorithm_options_+3A_max_it">max_it</code></td>
<td>
<p>maximum number of iterations.</p>
</td></tr>
<tr><td><code id="mscale_algorithm_options_+3A_eps">eps</code></td>
<td>
<p>numerical tolerance to check for convergence.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>options for the M-scale estimation algorithm.
</p>

<hr>
<h2 id='mscale_derivative'>Compute the Gradient and Hessian of the M-Scale Function</h2><span id='topic+mscale_derivative'></span><span id='topic+max_mscale_derivative'></span><span id='topic+max_mscale_grad_hess'></span>

<h3>Description</h3>

<p>Compute the derivative (gradient) or the Hessian of the M-scale function
evaluated at the point <code>x</code>.
</p>
<p>Compute the maximum derivative of the M-scale function with respect to each element over
a grid of values.
</p>
<p>Compute the maximum element in the gradient and Hessian of the M-scale
function with respect to each element over a grid of values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mscale_derivative(
  x,
  bdp = 0.25,
  order = 1,
  cc = consistency_const(bdp, "bisquare"),
  opts = mscale_algorithm_options()
)

max_mscale_derivative(
  x,
  grid,
  n_change,
  bdp = 0.25,
  cc = consistency_const(bdp, "bisquare"),
  opts = mscale_algorithm_options()
)

max_mscale_grad_hess(
  x,
  grid,
  n_change,
  bdp = 0.25,
  cc = consistency_const(bdp, "bisquare"),
  opts = mscale_algorithm_options()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mscale_derivative_+3A_x">x</code></td>
<td>
<p>numeric values. Missing values are verbosely ignored.</p>
</td></tr>
<tr><td><code id="mscale_derivative_+3A_bdp">bdp</code></td>
<td>
<p>desired breakdown point (between 0 and 0.5).</p>
</td></tr>
<tr><td><code id="mscale_derivative_+3A_order">order</code></td>
<td>
<p>compute the gradient (<code>order=1</code>) or the gradient and the
Hessian (<code>order=2</code>).</p>
</td></tr>
<tr><td><code id="mscale_derivative_+3A_cc">cc</code></td>
<td>
<p>cutoff value for the bisquare rho function.
By default, chosen to yield a consistent estimate for the
Normal distribution.</p>
</td></tr>
<tr><td><code id="mscale_derivative_+3A_opts">opts</code></td>
<td>
<p>a list of options for the M-scale estimation algorithm,
see <code><a href="#topic+mscale_algorithm_options">mscale_algorithm_options()</a></code> for details.</p>
</td></tr>
<tr><td><code id="mscale_derivative_+3A_grid">grid</code></td>
<td>
<p>a grid of values to replace the first 1 - <code>n_change</code> elements in<code> x</code>.</p>
</td></tr>
<tr><td><code id="mscale_derivative_+3A_n_change">n_change</code></td>
<td>
<p>the number of elements in <code>x</code> to replace with each value in <code>grid</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector of derivatives of the M-scale function, one per element in <code>x</code>.
</p>
<p>a vector with 4 elements:
</p>

<ol>
<li><p> the maximum absolute value of the gradient,
</p>
</li>
<li><p> the maximum absolute value of the Hessian elements,
</p>
</li>
<li><p> the M-scale associated with 1., and
</p>
</li>
<li><p> the M-scale associated with 2.
</p>
</li></ol>

<p>the maximum absolute derivative over the entire grid.
</p>


<h3>Functions</h3>


<ul>
<li> <p><code>max_mscale_derivative()</code>: maximum of the gradient
</p>
</li>
<li> <p><code>max_mscale_grad_hess()</code>: maximum of the gradient and hessian
</p>
</li></ul>

<hr>
<h2 id='mstep_options'>Deprecated</h2><span id='topic+mstep_options'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#deprecated"><img src="../help/figures/lifecycle-deprecated.svg" alt='[Deprecated]' /></a>
</p>
<p>Additional options for computing penalized EN MM-estimates.
Superseded by <code><a href="#topic+mm_algorithm_options">mm_algorithm_options()</a></code> and options supplied directly to <code><a href="#topic+pensem_cv">pensem_cv()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mstep_options(
  cc = 3.44,
  maxit = 1000,
  eps = 1e-06,
  adjust_bdp = FALSE,
  verbosity = 0,
  en_correction = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mstep_options_+3A_cc">cc</code></td>
<td>
<p><strong>ignored.</strong> Tuning constant for the M-estimator.</p>
</td></tr>
<tr><td><code id="mstep_options_+3A_maxit">maxit</code></td>
<td>
<p>maximum number of iterations allowed.</p>
</td></tr>
<tr><td><code id="mstep_options_+3A_eps">eps</code></td>
<td>
<p><strong>ignored.</strong> Numeric tolerance for convergence.</p>
</td></tr>
<tr><td><code id="mstep_options_+3A_adjust_bdp">adjust_bdp</code></td>
<td>
<p><strong>ignored.</strong> Should the breakdown point be adjusted based on the
effective degrees of freedom?</p>
</td></tr>
<tr><td><code id="mstep_options_+3A_verbosity">verbosity</code></td>
<td>
<p><strong>ignored.</strong> Verbosity of the algorithm.</p>
</td></tr>
<tr><td><code id="mstep_options_+3A_en_correction">en_correction</code></td>
<td>
<p><strong>ignored.</strong> Should the corrected EN estimator be used to choose
the optimal lambda with CV.
If <code>TRUE</code>, as by default, the estimator is &quot;bias corrected&quot;.</p>
</td></tr>
</table>


<h3>Warning</h3>

<p>Do not use this function in new code.
It may be removed from future versions of the package.
</p>


<h3>See Also</h3>

<p>Other deprecated functions: 
<code><a href="#topic+deprecated_en_options">deprecated_en_options</a></code>,
<code><a href="#topic+enpy">enpy</a>()</code>,
<code><a href="#topic+initest_options">initest_options</a>()</code>,
<code><a href="#topic+pense_options">pense_options</a>()</code>,
<code><a href="#topic+pensem">pensem</a>()</code>
</p>

<hr>
<h2 id='pense'>Compute (Adaptive) Elastic Net S-Estimates of Regression</h2><span id='topic+pense'></span><span id='topic+adapense'></span>

<h3>Description</h3>

<p>Compute elastic net S-estimates (PENSE estimates) along a grid of penalization levels with
optional penalty loadings for adaptive elastic net.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pense(
  x,
  y,
  alpha,
  nlambda = 50,
  nlambda_enpy = 10,
  lambda,
  lambda_min_ratio,
  enpy_lambda,
  penalty_loadings,
  intercept = TRUE,
  bdp = 0.25,
  cc,
  add_zero_based = TRUE,
  enpy_specific = FALSE,
  other_starts,
  carry_forward = TRUE,
  eps = 1e-06,
  explore_solutions = 10,
  explore_tol = 0.1,
  explore_it = 5,
  max_solutions = 1,
  comparison_tol = sqrt(eps),
  sparse = FALSE,
  ncores = 1,
  standardize = TRUE,
  algorithm_opts = mm_algorithm_options(),
  mscale_opts = mscale_algorithm_options(),
  enpy_opts = enpy_options(),
  cv_k = deprecated(),
  cv_objective = deprecated(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pense_+3A_x">x</code></td>
<td>
<p><code>n</code> by <code>p</code> matrix of numeric predictors.</p>
</td></tr>
<tr><td><code id="pense_+3A_y">y</code></td>
<td>
<p>vector of response values of length <code>n</code>.
For binary classification, <code>y</code> should be a factor with 2 levels.</p>
</td></tr>
<tr><td><code id="pense_+3A_alpha">alpha</code></td>
<td>
<p>elastic net penalty mixing parameter with <code class="reqn">0 \le \alpha \le 1</code>.
<code>alpha = 1</code> is the LASSO penalty, and <code>alpha = 0</code> the Ridge penalty.
Can be a vector of several values, but <code>alpha = 0</code> cannot be mixed with other values.</p>
</td></tr>
<tr><td><code id="pense_+3A_nlambda">nlambda</code></td>
<td>
<p>number of penalization levels.</p>
</td></tr>
<tr><td><code id="pense_+3A_nlambda_enpy">nlambda_enpy</code></td>
<td>
<p>number of penalization levels where the EN-PY initial estimate is computed.</p>
</td></tr>
<tr><td><code id="pense_+3A_lambda">lambda</code></td>
<td>
<p>optional user-supplied sequence of penalization levels. If given and not <code>NULL</code>,
<code>nlambda</code> and <code>lambda_min_ratio</code> are ignored.</p>
</td></tr>
<tr><td><code id="pense_+3A_lambda_min_ratio">lambda_min_ratio</code></td>
<td>
<p>Smallest value of the penalization level as a fraction of the largest
level (i.e., the smallest value for which all coefficients are zero). The default depends on
the sample size relative to the number of variables and <code>alpha</code>. If more observations than
variables are available, the default is <code>1e-3 * alpha</code>, otherwise <code>1e-2 * alpha</code>.</p>
</td></tr>
<tr><td><code id="pense_+3A_enpy_lambda">enpy_lambda</code></td>
<td>
<p>optional user-supplied sequence of penalization levels at which EN-PY
initial estimates are computed. If given and not <code>NULL</code>, <code>nlambda_enpy</code> is ignored.</p>
</td></tr>
<tr><td><code id="pense_+3A_penalty_loadings">penalty_loadings</code></td>
<td>
<p>a vector of positive penalty loadings (a.k.a. weights) for different
penalization of each coefficient. Only allowed for <code>alpha</code> &gt; 0.</p>
</td></tr>
<tr><td><code id="pense_+3A_intercept">intercept</code></td>
<td>
<p>include an intercept in the model.</p>
</td></tr>
<tr><td><code id="pense_+3A_bdp">bdp</code></td>
<td>
<p>desired breakdown point of the estimator, between 0.05 and 0.5. The actual
breakdown point may be slightly larger/smaller to avoid instabilities of the S-loss.</p>
</td></tr>
<tr><td><code id="pense_+3A_cc">cc</code></td>
<td>
<p>tuning constant for the S-estimator. Default is chosen based on the breakdown
point <code>bdp</code>. This affects the estimated coefficients only if
<code>standardize=TRUE</code>. Otherwise only the estimated scale of the residuals
would be affected.</p>
</td></tr>
<tr><td><code id="pense_+3A_add_zero_based">add_zero_based</code></td>
<td>
<p>also consider the 0-based regularization path. See details for a
description.</p>
</td></tr>
<tr><td><code id="pense_+3A_enpy_specific">enpy_specific</code></td>
<td>
<p>use the EN-PY initial estimates only at the penalization level they
are computed for. See details for a description.</p>
</td></tr>
<tr><td><code id="pense_+3A_other_starts">other_starts</code></td>
<td>
<p>a list of other staring points, created by <code><a href="#topic+starting_point">starting_point()</a></code>.
If the output of <code><a href="#topic+enpy_initial_estimates">enpy_initial_estimates()</a></code> is given, the starting points will be <em>shared</em>
among all penalization levels.
Note that if a the starting point is <em>specific</em> to a penalization level, this penalization
level is added to the grid of penalization levels (either the manually specified grid in
<code>lambda</code> or the automatically generated grid of size <code>nlambda</code>).
If <code>standardize = TRUE</code>, the starting points are also scaled.</p>
</td></tr>
<tr><td><code id="pense_+3A_carry_forward">carry_forward</code></td>
<td>
<p>carry the best solutions forward to the next penalty
level.</p>
</td></tr>
<tr><td><code id="pense_+3A_eps">eps</code></td>
<td>
<p>numerical tolerance.</p>
</td></tr>
<tr><td><code id="pense_+3A_explore_solutions">explore_solutions</code></td>
<td>
<p>number of solutions to compute up to the desired precision <code>eps</code>.</p>
</td></tr>
<tr><td><code id="pense_+3A_explore_tol">explore_tol</code>, <code id="pense_+3A_explore_it">explore_it</code></td>
<td>
<p>numerical tolerance and maximum number of iterations for
exploring possible solutions. The tolerance should be (much) looser than <code>eps</code> to be useful,
and the number of iterations should also be much smaller than the maximum number of
iterations given via <code>algorithm_opts</code>.</p>
</td></tr>
<tr><td><code id="pense_+3A_max_solutions">max_solutions</code></td>
<td>
<p>only retain up to <code>max_solutions</code> unique solutions per penalization level.</p>
</td></tr>
<tr><td><code id="pense_+3A_comparison_tol">comparison_tol</code></td>
<td>
<p>numeric tolerance to determine if two solutions are equal.
The comparison is first done on the absolute difference in the value of the objective
function at the solution If this is less than <code>comparison_tol</code>, two solutions are deemed
equal if the squared difference of the intercepts is less than <code>comparison_tol</code> and the
squared <code class="reqn">L_2</code> norm of the difference vector is less than <code>comparison_tol</code>.</p>
</td></tr>
<tr><td><code id="pense_+3A_sparse">sparse</code></td>
<td>
<p>use sparse coefficient vectors.</p>
</td></tr>
<tr><td><code id="pense_+3A_ncores">ncores</code></td>
<td>
<p>number of CPU cores to use in parallel. By default, only one CPU core is used.
Not supported on all platforms, in which case a warning is given.</p>
</td></tr>
<tr><td><code id="pense_+3A_standardize">standardize</code></td>
<td>
<p>logical flag to standardize the <code>x</code> variables prior to fitting the PENSE
estimates. Coefficients are always returned on the original scale. This can fail for
variables with a large proportion of a single value (e.g., zero-inflated data).
In this case, either compute with <code>standardize = FALSE</code> or standardize the data manually.</p>
</td></tr>
<tr><td><code id="pense_+3A_algorithm_opts">algorithm_opts</code></td>
<td>
<p>options for the MM algorithm to compute the estimates.
See <code><a href="#topic+mm_algorithm_options">mm_algorithm_options()</a></code> for details.</p>
</td></tr>
<tr><td><code id="pense_+3A_mscale_opts">mscale_opts</code></td>
<td>
<p>options for the M-scale estimation. See <code><a href="#topic+mscale_algorithm_options">mscale_algorithm_options()</a></code>
for details.</p>
</td></tr>
<tr><td><code id="pense_+3A_enpy_opts">enpy_opts</code></td>
<td>
<p>options for the ENPY initial estimates, created with the
<code><a href="#topic+enpy_options">enpy_options()</a></code> function. See <code><a href="#topic+enpy_initial_estimates">enpy_initial_estimates()</a></code> for details.</p>
</td></tr>
<tr><td><code id="pense_+3A_cv_k">cv_k</code>, <code id="pense_+3A_cv_objective">cv_objective</code></td>
<td>
<p>deprecated and ignored. See <code><a href="#topic+pense_cv">pense_cv()</a></code> for estimating
prediction performance via cross-validation.</p>
</td></tr>
<tr><td><code id="pense_+3A_...">...</code></td>
<td>
<p>ignored. See the section on deprecated parameters below.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list-like object with the following items
</p>

<dl>
<dt><code>alpha</code></dt><dd><p>the sequence of <code>alpha</code> parameters.</p>
</dd>
<dt><code>lambda</code></dt><dd><p>a list of sequences of penalization levels, one per <code>alpha</code> parameter.</p>
</dd>
<dt><code>estimates</code></dt><dd><p>a list of estimates. Each estimate contains the following information:
</p>

<dl>
<dt><code>intercept</code></dt><dd><p>intercept estimate.</p>
</dd>
<dt><code>beta</code></dt><dd><p>beta (slope) estimate.</p>
</dd>
<dt><code>lambda</code></dt><dd><p>penalization level at which the estimate is computed.</p>
</dd>
<dt><code>alpha</code></dt><dd><p><em>alpha</em> hyper-parameter at which the estimate is computed.</p>
</dd>
<dt><code>bdp</code></dt><dd><p>chosen breakdown-point.</p>
</dd>
<dt><code>objf_value</code></dt><dd><p>value of the objective function at the solution.</p>
</dd>
<dt><code>statuscode</code></dt><dd><p>if <code style="white-space: pre;">&#8288;&gt; 0&#8288;</code> the algorithm experienced issues when
computing the estimate.</p>
</dd>
<dt><code>status</code></dt><dd><p>optional status message from the algorithm.</p>
</dd>
</dl>

</dd>
<dt><code>bdp</code></dt><dd><p>the actual breakdown point used.</p>
</dd>
<dt><code>call</code></dt><dd><p>the original call.</p>
</dd>
</dl>



<h3>Strategies for Using Starting Points</h3>

<p>The function supports several different strategies to compute, and use the provided starting
points for optimizing the PENSE objective function.
</p>
<p>Starting points are computed internally but can also be supplied via <code>other_starts</code>.
By default, starting points are computed internally by the EN-PY procedure for penalization
levels supplied in <code>enpy_lambda</code> (or the automatically generated grid of length <code>nlambda_enpy</code>).
By default, starting points computed by the EN-PY procedure are <em>shared</em> for all penalization
levels in <code>lambda</code> (or the automatically generated grid of length <code>nlambda</code>).
If the starting points should be <em>specific</em> to the penalization level the starting points'
penalization level, set the <code>enpy_specific</code> argument to <code>TRUE</code>.
</p>
<p>In addition to EN-PY initial estimates, the algorithm can also use the &quot;0-based&quot; strategy if
<code>add_zero_based = TRUE</code> (by default). Here, the 0-vector is used to start the optimization at
the largest penalization level in <code>lambda</code>. At subsequent penalization levels, the solution at
the previous penalization level is also used as starting point.
</p>
<p>At every penalization level, all starting points are explored using the loose numerical
tolerance <code>explore_tol</code>. Only the best <code>explore_solutions</code> are computed to the stringent
numerical tolerance <code>eps</code>.
Finally, only the best <code>max_solutions</code> are retained and carried forward as starting points for
the subsequent penalization level.
</p>


<h3>Deprecated Arguments</h3>

<p>Starting with version 2.0.0, cross-validation is performed by separate function <code><a href="#topic+pense_cv">pense_cv()</a></code>.
Arguments related cross-validation cause an error when supplied to <code>pense()</code>.
Furthermore, the following arguments are deprecated as of version 2.0.0:
<code>initial</code>, <code>warm_reset</code>, <code>cl</code>, <code>options</code>, <code>init_options</code>, <code>en_options</code>.
If <code>pense()</code> is called with any of these arguments, warnings detail how to replace them.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pense_cv">pense_cv()</a></code> for selecting hyper-parameters via cross-validation.
</p>
<p><code><a href="#topic+coef.pense_fit">coef.pense_fit()</a></code> for extracting coefficient estimates.
</p>
<p><code><a href="#topic+plot.pense_fit">plot.pense_fit()</a></code> for plotting the regularization path.
</p>
<p>Other functions to compute robust estimates: 
<code><a href="#topic+regmest">regmest</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Compute the PENSE regularization path for Freeny's revenue data
# (see ?freeny)
data(freeny)
x &lt;- as.matrix(freeny[ , 2:5])

regpath &lt;- pense(x, freeny$y, alpha = 0.5)
plot(regpath)

# Extract the coefficients at a certain penalization level
coef(regpath, lambda = regpath$lambda[[1]][[40]])

# What penalization level leads to good prediction performance?
set.seed(123)
cv_results &lt;- pense_cv(x, freeny$y, alpha = 0.5,
                       cv_repl = 2, cv_k = 4)
plot(cv_results, se_mult = 1)

# Extract the coefficients at the penalization level with
# smallest prediction error ...
coef(cv_results)
# ... or at the penalization level with prediction error
# statistically indistinguishable from the minimum.
coef(cv_results, lambda = '1-se')
</code></pre>

<hr>
<h2 id='pense_cv'>Cross-validation for (Adaptive) PENSE Estimates</h2><span id='topic+pense_cv'></span><span id='topic+adapense_cv'></span>

<h3>Description</h3>

<p>Perform (repeated) K-fold cross-validation for <code><a href="#topic+pense">pense()</a></code>.
</p>
<p><code>adapense_cv()</code> is a convenience wrapper to compute adaptive
PENSE estimates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pense_cv(
  x,
  y,
  standardize = TRUE,
  lambda,
  cv_k,
  cv_repl = 1,
  cv_metric = c("tau_size", "mape", "rmspe", "auroc"),
  fit_all = TRUE,
  fold_starts = c("full", "enpy", "both"),
  cl = NULL,
  ...
)

adapense_cv(x, y, alpha, alpha_preliminary = 0, exponent = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pense_cv_+3A_x">x</code></td>
<td>
<p><code>n</code> by <code>p</code> matrix of numeric predictors.</p>
</td></tr>
<tr><td><code id="pense_cv_+3A_y">y</code></td>
<td>
<p>vector of response values of length <code>n</code>.
For binary classification, <code>y</code> should be a factor with 2 levels.</p>
</td></tr>
<tr><td><code id="pense_cv_+3A_standardize">standardize</code></td>
<td>
<p>whether to standardize the <code>x</code> variables prior to fitting
the PENSE estimates. Can also be set to <code>"cv_only"</code>, in which case the
input data is not standardized, but the training data in the CV folds is
scaled to match the scaling of the input data.
Coefficients are always returned on the original scale.
This can fail for variables with a large proportion of a single value
(e.g., zero-inflated data).
In this case, either compute with <code>standardize = FALSE</code> or standardize
the data manually.</p>
</td></tr>
<tr><td><code id="pense_cv_+3A_lambda">lambda</code></td>
<td>
<p>optional user-supplied sequence of penalization levels. If given and not <code>NULL</code>,
<code>nlambda</code> and <code>lambda_min_ratio</code> are ignored.</p>
</td></tr>
<tr><td><code id="pense_cv_+3A_cv_k">cv_k</code></td>
<td>
<p>number of folds per cross-validation.</p>
</td></tr>
<tr><td><code id="pense_cv_+3A_cv_repl">cv_repl</code></td>
<td>
<p>number of cross-validation replications.</p>
</td></tr>
<tr><td><code id="pense_cv_+3A_cv_metric">cv_metric</code></td>
<td>
<p>either a string specifying the performance metric to use, or a function to
evaluate prediction errors in a single CV replication.
If a function, the number of arguments define the data the function receives.
If the function takes a single argument, it is called with a single numeric vector of
prediction errors.
If the function takes two or more arguments, it is called with the predicted values as
first argument and the true values as second argument.
The function must always return a single numeric value quantifying the prediction performance.
The order of the given values corresponds to the order in the input data.</p>
</td></tr>
<tr><td><code id="pense_cv_+3A_fit_all">fit_all</code></td>
<td>
<p>If <code>TRUE</code>, fit the model for all penalization levels.
Can also be any combination of <code>"min"</code> and <code>"{x}-se"</code>, in which case only models at the
penalization level with smallest average CV accuracy, or within <code>{x}</code> standard errors,
respectively.
Setting <code>fit_all</code> to <code>FALSE</code> is equivalent to <code>"min"</code>.
Applies to all <code>alpha</code> value.</p>
</td></tr>
<tr><td><code id="pense_cv_+3A_fold_starts">fold_starts</code></td>
<td>
<p>how to determine starting values in the
cross-validation folds. If <code>"full"</code> (default), use the best solution from
the fit to the full data as starting value. This implies
<code>fit_all=TRUE</code>.
If <code>"enpy"</code> compute separate ENPY initial estimates in each fold.
The option <code>"both"</code> uses both.
These starts are in addition to the starts provided in <code>other_starts</code>.</p>
</td></tr>
<tr><td><code id="pense_cv_+3A_cl">cl</code></td>
<td>
<p>a <a href="parallel.html#topic+makeCluster">parallel</a> cluster. Can only be used in combination with
<code>ncores = 1</code>.</p>
</td></tr>
<tr><td><code id="pense_cv_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+pense">pense</a></code>
</p>

<dl>
<dt><code>nlambda</code></dt><dd><p>number of penalization levels.</p>
</dd>
<dt><code>lambda_min_ratio</code></dt><dd><p>Smallest value of the penalization level as a fraction of the largest
level (i.e., the smallest value for which all coefficients are zero). The default depends on
the sample size relative to the number of variables and <code>alpha</code>. If more observations than
variables are available, the default is <code>1e-3 * alpha</code>, otherwise <code>1e-2 * alpha</code>.</p>
</dd>
<dt><code>nlambda_enpy</code></dt><dd><p>number of penalization levels where the EN-PY initial estimate is computed.</p>
</dd>
<dt><code>penalty_loadings</code></dt><dd><p>a vector of positive penalty loadings (a.k.a. weights) for different
penalization of each coefficient. Only allowed for <code>alpha</code> &gt; 0.</p>
</dd>
<dt><code>enpy_lambda</code></dt><dd><p>optional user-supplied sequence of penalization levels at which EN-PY
initial estimates are computed. If given and not <code>NULL</code>, <code>nlambda_enpy</code> is ignored.</p>
</dd>
<dt><code>other_starts</code></dt><dd><p>a list of other staring points, created by <code><a href="#topic+starting_point">starting_point()</a></code>.
If the output of <code><a href="#topic+enpy_initial_estimates">enpy_initial_estimates()</a></code> is given, the starting points will be <em>shared</em>
among all penalization levels.
Note that if a the starting point is <em>specific</em> to a penalization level, this penalization
level is added to the grid of penalization levels (either the manually specified grid in
<code>lambda</code> or the automatically generated grid of size <code>nlambda</code>).
If <code>standardize = TRUE</code>, the starting points are also scaled.</p>
</dd>
<dt><code>intercept</code></dt><dd><p>include an intercept in the model.</p>
</dd>
<dt><code>bdp</code></dt><dd><p>desired breakdown point of the estimator, between 0.05 and 0.5. The actual
breakdown point may be slightly larger/smaller to avoid instabilities of the S-loss.</p>
</dd>
<dt><code>cc</code></dt><dd><p>tuning constant for the S-estimator. Default is chosen based on the breakdown
point <code>bdp</code>. This affects the estimated coefficients only if
<code>standardize=TRUE</code>. Otherwise only the estimated scale of the residuals
would be affected.</p>
</dd>
<dt><code>eps</code></dt><dd><p>numerical tolerance.</p>
</dd>
<dt><code>explore_solutions</code></dt><dd><p>number of solutions to compute up to the desired precision <code>eps</code>.</p>
</dd>
<dt><code>explore_tol,explore_it</code></dt><dd><p>numerical tolerance and maximum number of iterations for
exploring possible solutions. The tolerance should be (much) looser than <code>eps</code> to be useful,
and the number of iterations should also be much smaller than the maximum number of
iterations given via <code>algorithm_opts</code>.</p>
</dd>
<dt><code>max_solutions</code></dt><dd><p>only retain up to <code>max_solutions</code> unique solutions per penalization level.</p>
</dd>
<dt><code>comparison_tol</code></dt><dd><p>numeric tolerance to determine if two solutions are equal.
The comparison is first done on the absolute difference in the value of the objective
function at the solution If this is less than <code>comparison_tol</code>, two solutions are deemed
equal if the squared difference of the intercepts is less than <code>comparison_tol</code> and the
squared <code class="reqn">L_2</code> norm of the difference vector is less than <code>comparison_tol</code>.</p>
</dd>
<dt><code>add_zero_based</code></dt><dd><p>also consider the 0-based regularization path. See details for a
description.</p>
</dd>
<dt><code>enpy_specific</code></dt><dd><p>use the EN-PY initial estimates only at the penalization level they
are computed for. See details for a description.</p>
</dd>
<dt><code>carry_forward</code></dt><dd><p>carry the best solutions forward to the next penalty
level.</p>
</dd>
<dt><code>sparse</code></dt><dd><p>use sparse coefficient vectors.</p>
</dd>
<dt><code>ncores</code></dt><dd><p>number of CPU cores to use in parallel. By default, only one CPU core is used.
Not supported on all platforms, in which case a warning is given.</p>
</dd>
<dt><code>algorithm_opts</code></dt><dd><p>options for the MM algorithm to compute the estimates.
See <code><a href="#topic+mm_algorithm_options">mm_algorithm_options()</a></code> for details.</p>
</dd>
<dt><code>mscale_opts</code></dt><dd><p>options for the M-scale estimation. See <code><a href="#topic+mscale_algorithm_options">mscale_algorithm_options()</a></code>
for details.</p>
</dd>
<dt><code>enpy_opts</code></dt><dd><p>options for the ENPY initial estimates, created with the
<code><a href="#topic+enpy_options">enpy_options()</a></code> function. See <code><a href="#topic+enpy_initial_estimates">enpy_initial_estimates()</a></code> for details.</p>
</dd>
<dt><code>cv_k,cv_objective</code></dt><dd><p>deprecated and ignored. See <code><a href="#topic+pense_cv">pense_cv()</a></code> for estimating
prediction performance via cross-validation.</p>
</dd>
</dl>
</td></tr>
<tr><td><code id="pense_cv_+3A_alpha">alpha</code></td>
<td>
<p>elastic net penalty mixing parameter with <code class="reqn">0 \le \alpha \le 1</code>.
<code>alpha = 1</code> is the LASSO penalty, and <code>alpha = 0</code> the Ridge penalty.
Can be a vector of several values, but <code>alpha = 0</code> cannot be mixed with other values.</p>
</td></tr>
<tr><td><code id="pense_cv_+3A_alpha_preliminary">alpha_preliminary</code></td>
<td>
<p><code>alpha</code> parameter for the preliminary estimate.</p>
</td></tr>
<tr><td><code id="pense_cv_+3A_exponent">exponent</code></td>
<td>
<p>the exponent for computing the penalty loadings based on
the preliminary estimate.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The built-in CV metrics are
</p>

<dl>
<dt><code>"tau_size"</code></dt><dd><p><code class="reqn">\tau</code>-size of the prediction error, computed by
<code><a href="#topic+tau_size">tau_size()</a></code> (default).</p>
</dd>
<dt><code>"mape"</code></dt><dd><p>Median absolute prediction error.</p>
</dd>
<dt><code>"rmspe"</code></dt><dd><p>Root mean squared prediction error.</p>
</dd>
<dt><code>"auroc"</code></dt><dd><p>Area under the receiver operator characteristic curve (actually 1 - AUROC).
Only sensible for binary responses.</p>
</dd>
</dl>

<p><code>adapense_cv()</code> is a convenience wrapper which performs 3 steps:
</p>

<ol>
<li><p> compute preliminary estimates via
<code>pense_cv(..., alpha = alpha_preliminary)</code>,
</p>
</li>
<li><p> computes the penalty loadings from the estimate <code>beta</code> with best
prediction performance by
<code>adapense_loadings = 1 / abs(beta)^exponent</code>, and
</p>
</li>
<li><p> compute the adaptive PENSE estimates via
<code>pense_cv(..., penalty_loadings = adapense_loadings)</code>.
</p>
</li></ol>



<h3>Value</h3>

<p>a list-like object with the same components as returned by <code><a href="#topic+pense">pense()</a></code>,
plus the following:
</p>

<dl>
<dt><code>cvres</code></dt><dd><p>data frame of average cross-validated performance.</p>
</dd>
</dl>

<p>a list-like object as returned by <code><a href="#topic+pense_cv">pense_cv()</a></code> plus the following
</p>

<dl>
<dt><code>preliminary</code></dt><dd><p>the CV results for the preliminary estimate.</p>
</dd>
<dt><code>exponent</code></dt><dd><p>exponent used to compute the penalty loadings.</p>
</dd>
<dt><code>penalty_loadings</code></dt><dd><p>penalty loadings used for the
adaptive PENSE estimate.</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="#topic+pense">pense()</a></code> for computing regularized S-estimates without
cross-validation.
</p>
<p><code><a href="#topic+coef.pense_cvfit">coef.pense_cvfit()</a></code> for extracting coefficient estimates.
</p>
<p><code><a href="#topic+plot.pense_cvfit">plot.pense_cvfit()</a></code> for plotting the CV performance or the
regularization path.
</p>
<p>Other functions to compute robust estimates with CV: 
<code><a href="#topic+pensem_cv">pensem_cv</a>()</code>,
<code><a href="#topic+regmest_cv">regmest_cv</a>()</code>
</p>
<p>Other functions to compute robust estimates with CV: 
<code><a href="#topic+pensem_cv">pensem_cv</a>()</code>,
<code><a href="#topic+regmest_cv">regmest_cv</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Compute the adaptive PENSE regularization path for Freeny's
# revenue data (see ?freeny)
data(freeny)
x &lt;- as.matrix(freeny[ , 2:5])

## Either use the convenience function directly ...
set.seed(123)
ada_convenience &lt;- adapense_cv(x, freeny$y, alpha = 0.5,
                               cv_repl = 2, cv_k = 4)

## ... or compute the steps manually:
# Step 1: Compute preliminary estimates with CV
set.seed(123)
preliminary_estimate &lt;- pense_cv(x, freeny$y, alpha = 0,
                                 cv_repl = 2, cv_k = 4)
plot(preliminary_estimate, se_mult = 1)

# Step 2: Use the coefficients with best prediction performance
# to define the penalty loadings:
prelim_coefs &lt;- coef(preliminary_estimate, lambda = 'min')
pen_loadings &lt;- 1 / abs(prelim_coefs[-1])

# Step 3: Compute the adaptive PENSE estimates and estimate
# their prediction performance.
set.seed(123)
ada_manual &lt;- pense_cv(x, freeny$y, alpha = 0.5,
                       cv_repl = 2, cv_k = 4,
                       penalty_loadings = pen_loadings)

# Visualize the prediction performance and coefficient path of
# the adaptive PENSE estimates (manual vs. automatic)
def.par &lt;- par(no.readonly = TRUE)
layout(matrix(1:4, ncol = 2, byrow = TRUE))
plot(ada_convenience$preliminary)
plot(preliminary_estimate)
plot(ada_convenience)
plot(ada_manual)
par(def.par)
</code></pre>

<hr>
<h2 id='pense_options'>Deprecated</h2><span id='topic+pense_options'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#deprecated"><img src="../help/figures/lifecycle-deprecated.svg" alt='[Deprecated]' /></a>
</p>
<p>Additional options for computing penalized EN S-estimates.
Superseded by <code><a href="#topic+mm_algorithm_options">mm_algorithm_options()</a></code> and options supplied directly to <code><a href="#topic+pense">pense()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pense_options(
  delta = 0.25,
  maxit = 1000,
  eps = 1e-06,
  mscale_eps = 1e-08,
  mscale_maxit = 200,
  verbosity = 0,
  cc = NULL,
  en_correction = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pense_options_+3A_delta">delta</code></td>
<td>
<p>desired breakdown point of the resulting estimator.</p>
</td></tr>
<tr><td><code id="pense_options_+3A_maxit">maxit</code></td>
<td>
<p>maximum number of iterations allowed.</p>
</td></tr>
<tr><td><code id="pense_options_+3A_eps">eps</code></td>
<td>
<p>numeric tolerance for convergence.</p>
</td></tr>
<tr><td><code id="pense_options_+3A_mscale_eps">mscale_eps</code>, <code id="pense_options_+3A_mscale_maxit">mscale_maxit</code></td>
<td>
<p>maximum number of iterations and numeric
tolerance for the M-scale.</p>
</td></tr>
<tr><td><code id="pense_options_+3A_verbosity">verbosity</code></td>
<td>
<p><strong>ignored.</strong> Verbosity of the algorithm.</p>
</td></tr>
<tr><td><code id="pense_options_+3A_cc">cc</code></td>
<td>
<p><strong>ignored.</strong> Tuning constant for the S-estimator. Default is to chosen based
on the breakdown point <code>delta</code>. Should never have to be changed.</p>
</td></tr>
<tr><td><code id="pense_options_+3A_en_correction">en_correction</code></td>
<td>
<p><strong>ignored.</strong> Should the corrected EN estimator be used to choose
the optimal lambda with CV.
If <code>TRUE</code>, as by default, the estimator is &quot;bias corrected&quot;.</p>
</td></tr>
</table>


<h3>Warning</h3>

<p>Do not use this function in new code.
It may be removed from future versions of the package.
</p>


<h3>See Also</h3>

<p>Other deprecated functions: 
<code><a href="#topic+deprecated_en_options">deprecated_en_options</a></code>,
<code><a href="#topic+enpy">enpy</a>()</code>,
<code><a href="#topic+initest_options">initest_options</a>()</code>,
<code><a href="#topic+mstep_options">mstep_options</a>()</code>,
<code><a href="#topic+pensem">pensem</a>()</code>
</p>

<hr>
<h2 id='pensem'>Deprecated Alias of pensem_cv</h2><span id='topic+pensem'></span>

<h3>Description</h3>

<p><code>pensem()</code> is a deprecated alias for <code><a href="#topic+pensem_cv">pensem_cv()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pensem(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pensem_+3A_x">x</code></td>
<td>
<p>either a numeric matrix of predictor values, or a cross-validated
PENSE fit from <code><a href="#topic+pense_cv">pense_cv()</a></code>.</p>
</td></tr>
<tr><td><code id="pensem_+3A_...">...</code></td>
<td>
<p>ignored. See the section on deprecated parameters below.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other deprecated functions: 
<code><a href="#topic+deprecated_en_options">deprecated_en_options</a></code>,
<code><a href="#topic+enpy">enpy</a>()</code>,
<code><a href="#topic+initest_options">initest_options</a>()</code>,
<code><a href="#topic+mstep_options">mstep_options</a>()</code>,
<code><a href="#topic+pense_options">pense_options</a>()</code>
</p>

<hr>
<h2 id='pensem_cv'>Compute Penalized Elastic Net M-Estimates from PENSE</h2><span id='topic+pensem_cv'></span><span id='topic+pensem_cv.default'></span><span id='topic+pensem_cv.pense_cvfit'></span>

<h3>Description</h3>

<p>This is a convenience wrapper around <code><a href="#topic+pense_cv">pense_cv()</a></code> and <code><a href="#topic+regmest_cv">regmest_cv()</a></code>, for
the common use-case of computing
a highly-robust S-estimate followed by a more efficient M-estimate using
the scale of the residuals from the S-estimate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pensem_cv(x, ...)

## Default S3 method:
pensem_cv(
  x,
  y,
  alpha = 0.5,
  nlambda = 50,
  lambda_min_ratio,
  lambda_m,
  lambda_s,
  standardize = TRUE,
  penalty_loadings,
  intercept = TRUE,
  bdp = 0.25,
  ncores = 1,
  sparse = FALSE,
  eps = 1e-06,
  cc = 4.7,
  cv_k = 5,
  cv_repl = 1,
  cl = NULL,
  cv_metric = c("tau_size", "mape", "rmspe"),
  add_zero_based = TRUE,
  explore_solutions = 10,
  explore_tol = 0.1,
  explore_it = 5,
  max_solutions = 10,
  fit_all = TRUE,
  comparison_tol = sqrt(eps),
  algorithm_opts = mm_algorithm_options(),
  mscale_opts = mscale_algorithm_options(),
  nlambda_enpy = 10,
  enpy_opts = enpy_options(),
  ...
)

## S3 method for class 'pense_cvfit'
pensem_cv(
  x,
  scale,
  alpha,
  nlambda = 50,
  lambda_min_ratio,
  lambda_m,
  standardize = TRUE,
  penalty_loadings,
  intercept = TRUE,
  bdp = 0.25,
  ncores = 1,
  sparse = FALSE,
  eps = 1e-06,
  cc = 4.7,
  cv_k = 5,
  cv_repl = 1,
  cl = NULL,
  cv_metric = c("tau_size", "mape", "rmspe"),
  add_zero_based = TRUE,
  explore_solutions = 10,
  explore_tol = 0.1,
  explore_it = 5,
  max_solutions = 10,
  fit_all = TRUE,
  comparison_tol = sqrt(eps),
  algorithm_opts = mm_algorithm_options(),
  mscale_opts = mscale_algorithm_options(),
  x_train,
  y_train,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pensem_cv_+3A_x">x</code></td>
<td>
<p>either a numeric matrix of predictor values, or a cross-validated
PENSE fit from <code><a href="#topic+pense_cv">pense_cv()</a></code>.</p>
</td></tr>
<tr><td><code id="pensem_cv_+3A_...">...</code></td>
<td>
<p>ignored. See the section on deprecated parameters below.</p>
</td></tr>
<tr><td><code id="pensem_cv_+3A_y">y</code></td>
<td>
<p>vector of response values of length <code>n</code>.
For binary classification, <code>y</code> should be a factor with 2 levels.</p>
</td></tr>
<tr><td><code id="pensem_cv_+3A_alpha">alpha</code></td>
<td>
<p>elastic net penalty mixing parameter with <code class="reqn">0 \le \alpha \le 1</code>.
<code>alpha = 1</code> is the LASSO penalty, and <code>alpha = 0</code> the Ridge penalty.
Can be a vector of several values, but <code>alpha = 0</code> cannot be mixed with other values.</p>
</td></tr>
<tr><td><code id="pensem_cv_+3A_nlambda">nlambda</code></td>
<td>
<p>number of penalization levels.</p>
</td></tr>
<tr><td><code id="pensem_cv_+3A_lambda_min_ratio">lambda_min_ratio</code></td>
<td>
<p>Smallest value of the penalization level as a fraction of the largest
level (i.e., the smallest value for which all coefficients are zero). The default depends on
the sample size relative to the number of variables and <code>alpha</code>. If more observations than
variables are available, the default is <code>1e-3 * alpha</code>, otherwise <code>1e-2 * alpha</code>.</p>
</td></tr>
<tr><td><code id="pensem_cv_+3A_lambda_m">lambda_m</code>, <code id="pensem_cv_+3A_lambda_s">lambda_s</code></td>
<td>
<p>optional user-supplied sequence of penalization
levels for the S- and M-estimates.
If given and not <code>NULL</code>, <code>nlambda</code> and <code>lambda_min_ratio</code> are ignored for
the respective estimate (S and/or M).</p>
</td></tr>
<tr><td><code id="pensem_cv_+3A_standardize">standardize</code></td>
<td>
<p>logical flag to standardize the <code>x</code> variables prior to fitting the PENSE
estimates. Coefficients are always returned on the original scale. This can fail for
variables with a large proportion of a single value (e.g., zero-inflated data).
In this case, either compute with <code>standardize = FALSE</code> or standardize the data manually.</p>
</td></tr>
<tr><td><code id="pensem_cv_+3A_penalty_loadings">penalty_loadings</code></td>
<td>
<p>a vector of positive penalty loadings (a.k.a. weights) for different
penalization of each coefficient. Only allowed for <code>alpha</code> &gt; 0.</p>
</td></tr>
<tr><td><code id="pensem_cv_+3A_intercept">intercept</code></td>
<td>
<p>include an intercept in the model.</p>
</td></tr>
<tr><td><code id="pensem_cv_+3A_bdp">bdp</code></td>
<td>
<p>desired breakdown point of the estimator, between 0.05 and 0.5. The actual
breakdown point may be slightly larger/smaller to avoid instabilities of the S-loss.</p>
</td></tr>
<tr><td><code id="pensem_cv_+3A_ncores">ncores</code></td>
<td>
<p>number of CPU cores to use in parallel. By default, only one CPU core is used.
Not supported on all platforms, in which case a warning is given.</p>
</td></tr>
<tr><td><code id="pensem_cv_+3A_sparse">sparse</code></td>
<td>
<p>use sparse coefficient vectors.</p>
</td></tr>
<tr><td><code id="pensem_cv_+3A_eps">eps</code></td>
<td>
<p>numerical tolerance.</p>
</td></tr>
<tr><td><code id="pensem_cv_+3A_cc">cc</code></td>
<td>
<p>cutoff constant for Tukey's bisquare <code class="reqn">\rho</code> function in the
M-estimation objective function.</p>
</td></tr>
<tr><td><code id="pensem_cv_+3A_cv_k">cv_k</code></td>
<td>
<p>number of folds per cross-validation.</p>
</td></tr>
<tr><td><code id="pensem_cv_+3A_cv_repl">cv_repl</code></td>
<td>
<p>number of cross-validation replications.</p>
</td></tr>
<tr><td><code id="pensem_cv_+3A_cl">cl</code></td>
<td>
<p>a <a href="parallel.html#topic+makeCluster">parallel</a> cluster. Can only be used in combination with
<code>ncores = 1</code>.</p>
</td></tr>
<tr><td><code id="pensem_cv_+3A_cv_metric">cv_metric</code></td>
<td>
<p>either a string specifying the performance metric to use, or a function to
evaluate prediction errors in a single CV replication.
If a function, the number of arguments define the data the function receives.
If the function takes a single argument, it is called with a single numeric vector of
prediction errors.
If the function takes two or more arguments, it is called with the predicted values as
first argument and the true values as second argument.
The function must always return a single numeric value quantifying the prediction performance.
The order of the given values corresponds to the order in the input data.</p>
</td></tr>
<tr><td><code id="pensem_cv_+3A_add_zero_based">add_zero_based</code></td>
<td>
<p>also consider the 0-based regularization path. See details for a
description.</p>
</td></tr>
<tr><td><code id="pensem_cv_+3A_explore_solutions">explore_solutions</code></td>
<td>
<p>number of solutions to compute up to the desired precision <code>eps</code>.</p>
</td></tr>
<tr><td><code id="pensem_cv_+3A_explore_tol">explore_tol</code>, <code id="pensem_cv_+3A_explore_it">explore_it</code></td>
<td>
<p>numerical tolerance and maximum number of iterations for
exploring possible solutions. The tolerance should be (much) looser than <code>eps</code> to be useful,
and the number of iterations should also be much smaller than the maximum number of
iterations given via <code>algorithm_opts</code>.</p>
</td></tr>
<tr><td><code id="pensem_cv_+3A_max_solutions">max_solutions</code></td>
<td>
<p>only retain up to <code>max_solutions</code> unique solutions per penalization level.</p>
</td></tr>
<tr><td><code id="pensem_cv_+3A_fit_all">fit_all</code></td>
<td>
<p>If <code>TRUE</code>, fit the model for all penalization levels.
Can also be any combination of <code>"min"</code> and <code>"{x}-se"</code>, in which case only models at the
penalization level with smallest average CV accuracy, or within <code>{x}</code> standard errors,
respectively.
Setting <code>fit_all</code> to <code>FALSE</code> is equivalent to <code>"min"</code>.
Applies to all <code>alpha</code> value.</p>
</td></tr>
<tr><td><code id="pensem_cv_+3A_comparison_tol">comparison_tol</code></td>
<td>
<p>numeric tolerance to determine if two solutions are equal.
The comparison is first done on the absolute difference in the value of the objective
function at the solution If this is less than <code>comparison_tol</code>, two solutions are deemed
equal if the squared difference of the intercepts is less than <code>comparison_tol</code> and the
squared <code class="reqn">L_2</code> norm of the difference vector is less than <code>comparison_tol</code>.</p>
</td></tr>
<tr><td><code id="pensem_cv_+3A_algorithm_opts">algorithm_opts</code></td>
<td>
<p>options for the MM algorithm to compute the estimates.
See <code><a href="#topic+mm_algorithm_options">mm_algorithm_options()</a></code> for details.</p>
</td></tr>
<tr><td><code id="pensem_cv_+3A_mscale_opts">mscale_opts</code></td>
<td>
<p>options for the M-scale estimation. See <code><a href="#topic+mscale_algorithm_options">mscale_algorithm_options()</a></code>
for details.</p>
</td></tr>
<tr><td><code id="pensem_cv_+3A_nlambda_enpy">nlambda_enpy</code></td>
<td>
<p>number of penalization levels where the EN-PY initial estimate is computed.</p>
</td></tr>
<tr><td><code id="pensem_cv_+3A_enpy_opts">enpy_opts</code></td>
<td>
<p>options for the ENPY initial estimates, created with the
<code><a href="#topic+enpy_options">enpy_options()</a></code> function. See <code><a href="#topic+enpy_initial_estimates">enpy_initial_estimates()</a></code> for details.</p>
</td></tr>
<tr><td><code id="pensem_cv_+3A_scale">scale</code></td>
<td>
<p>initial scale estimate to use in the M-estimation.
By default the S-scale from the PENSE fit is used.</p>
</td></tr>
<tr><td><code id="pensem_cv_+3A_x_train">x_train</code>, <code id="pensem_cv_+3A_y_train">y_train</code></td>
<td>
<p>override arguments <code>x</code> and <code>y</code> as provided in the
call to <code>pense_cv()</code>. This is useful if the arguments in the <code>pense_cv()</code>
call are not available in the current environment.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The built-in CV metrics are
</p>

<dl>
<dt><code>"tau_size"</code></dt><dd><p><code class="reqn">\tau</code>-size of the prediction error, computed by
<code><a href="#topic+tau_size">tau_size()</a></code> (default).</p>
</dd>
<dt><code>"mape"</code></dt><dd><p>Median absolute prediction error.</p>
</dd>
<dt><code>"rmspe"</code></dt><dd><p>Root mean squared prediction error.</p>
</dd>
<dt><code>"auroc"</code></dt><dd><p>Area under the receiver operator characteristic curve (actually 1 - AUROC).
Only sensible for binary responses.</p>
</dd>
</dl>



<h3>Value</h3>

<p>an object of cross-validated regularized M-estimates as returned
from <code><a href="#topic+regmest_cv">regmest_cv()</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pense_cv">pense_cv()</a></code> to compute the starting S-estimate.
</p>
<p>Other functions to compute robust estimates with CV: 
<code><a href="#topic+pense_cv">pense_cv</a>()</code>,
<code><a href="#topic+regmest_cv">regmest_cv</a>()</code>
</p>

<hr>
<h2 id='plot.pense_cvfit'>Plot Method for Penalized Estimates With Cross-Validation</h2><span id='topic+plot.pense_cvfit'></span>

<h3>Description</h3>

<p>Plot the cross-validation performance or the coefficient path for fitted penalized
elastic net S- or LS-estimates of regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pense_cvfit'
plot(x, what = c("cv", "coef.path"), alpha = NULL, se_mult = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.pense_cvfit_+3A_x">x</code></td>
<td>
<p>fitted estimates with cross-validation information.</p>
</td></tr>
<tr><td><code id="plot.pense_cvfit_+3A_what">what</code></td>
<td>
<p>plot either the CV performance or the coefficient path.</p>
</td></tr>
<tr><td><code id="plot.pense_cvfit_+3A_alpha">alpha</code></td>
<td>
<p>If <code>what = "cv"</code>, only CV performance for fits with matching <code>alpha</code> are plotted.
In case <code>alpha</code> is missing or <code>NULL</code>, all fits in <code>x</code> are plotted.
If <code>what = "coef.path"</code>, plot the coefficient path for the fit with the given
hyper-parameter value or, in case <code>alpha</code> is missing, for the first value in <code>x$alpha</code>.</p>
</td></tr>
<tr><td><code id="plot.pense_cvfit_+3A_se_mult">se_mult</code></td>
<td>
<p>if plotting CV performance, multiplier of the estimated SE.</p>
</td></tr>
<tr><td><code id="plot.pense_cvfit_+3A_...">...</code></td>
<td>
<p>currently ignored.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other functions for plotting and printing: 
<code><a href="#topic+plot.pense_fit">plot.pense_fit</a>()</code>,
<code><a href="#topic+prediction_performance">prediction_performance</a>()</code>,
<code><a href="#topic+summary.pense_cvfit">summary.pense_cvfit</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Compute the PENSE regularization path for Freeny's revenue data
# (see ?freeny)
data(freeny)
x &lt;- as.matrix(freeny[ , 2:5])

regpath &lt;- pense(x, freeny$y, alpha = 0.5)
plot(regpath)

# Extract the coefficients at a certain penalization level
coef(regpath, lambda = regpath$lambda[[1]][[40]])

# What penalization level leads to good prediction performance?
set.seed(123)
cv_results &lt;- pense_cv(x, freeny$y, alpha = 0.5,
                       cv_repl = 2, cv_k = 4)
plot(cv_results, se_mult = 1)

# Extract the coefficients at the penalization level with
# smallest prediction error ...
coef(cv_results)
# ... or at the penalization level with prediction error
# statistically indistinguishable from the minimum.
coef(cv_results, lambda = '1-se')
</code></pre>

<hr>
<h2 id='plot.pense_fit'>Plot Method for Penalized Estimates</h2><span id='topic+plot.pense_fit'></span>

<h3>Description</h3>

<p>Plot the coefficient path for fitted penalized elastic net S- or LS-estimates of regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pense_fit'
plot(x, alpha, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.pense_fit_+3A_x">x</code></td>
<td>
<p>fitted estimates.</p>
</td></tr>
<tr><td><code id="plot.pense_fit_+3A_alpha">alpha</code></td>
<td>
<p>Plot the coefficient path for the fit with the given hyper-parameter value.
If missing of <code>NULL</code>, the first value in <code>x$alpha</code> is used.</p>
</td></tr>
<tr><td><code id="plot.pense_fit_+3A_...">...</code></td>
<td>
<p>currently ignored.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other functions for plotting and printing: 
<code><a href="#topic+plot.pense_cvfit">plot.pense_cvfit</a>()</code>,
<code><a href="#topic+prediction_performance">prediction_performance</a>()</code>,
<code><a href="#topic+summary.pense_cvfit">summary.pense_cvfit</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Compute the PENSE regularization path for Freeny's revenue data
# (see ?freeny)
data(freeny)
x &lt;- as.matrix(freeny[ , 2:5])

regpath &lt;- pense(x, freeny$y, alpha = 0.5)
plot(regpath)

# Extract the coefficients at a certain penalization level
coef(regpath, lambda = regpath$lambda[[1]][[40]])

# What penalization level leads to good prediction performance?
set.seed(123)
cv_results &lt;- pense_cv(x, freeny$y, alpha = 0.5,
                       cv_repl = 2, cv_k = 4)
plot(cv_results, se_mult = 1)

# Extract the coefficients at the penalization level with
# smallest prediction error ...
coef(cv_results)
# ... or at the penalization level with prediction error
# statistically indistinguishable from the minimum.
coef(cv_results, lambda = '1-se')
</code></pre>

<hr>
<h2 id='predict.pense_cvfit'>Predict Method for PENSE Fits</h2><span id='topic+predict.pense_cvfit'></span>

<h3>Description</h3>

<p>Predict response values using a PENSE (or LS-EN) regularization path with
hyper-parameters chosen by cross-validation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pense_cvfit'
predict(
  object,
  newdata,
  alpha = NULL,
  lambda = "min",
  se_mult = 1,
  exact = deprecated(),
  correction = deprecated(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.pense_cvfit_+3A_object">object</code></td>
<td>
<p>PENSE with cross-validated hyper-parameters to extract coefficients from.</p>
</td></tr>
<tr><td><code id="predict.pense_cvfit_+3A_newdata">newdata</code></td>
<td>
<p>an optional matrix of new predictor values.
If missing, the fitted values are computed.</p>
</td></tr>
<tr><td><code id="predict.pense_cvfit_+3A_alpha">alpha</code></td>
<td>
<p>Either a single number or <code>NULL</code> (default).
If given, only fits with the given <code>alpha</code> value are considered.
If <code>lambda</code> is a numeric value and <code>object</code> was fit with multiple <em>alpha</em>
values and no value is provided, the first value in <code>object$alpha</code> is used with a warning.</p>
</td></tr>
<tr><td><code id="predict.pense_cvfit_+3A_lambda">lambda</code></td>
<td>
<p>either a string specifying which penalty level to use
(<code>"min"</code>, <code>"se"</code>, <code style="white-space: pre;">&#8288;"{m}-se&#8288;</code>&quot;)
or a single numeric value of the penalty parameter. See details.</p>
</td></tr>
<tr><td><code id="predict.pense_cvfit_+3A_se_mult">se_mult</code></td>
<td>
<p>If <code>lambda = "se"</code>, the multiple of standard errors to tolerate.</p>
</td></tr>
<tr><td><code id="predict.pense_cvfit_+3A_exact">exact</code></td>
<td>
<p>deprecated. Always gives a warning if <code>lambda</code> is not part of the
fitted sequence and coefficients are interpolated.</p>
</td></tr>
<tr><td><code id="predict.pense_cvfit_+3A_correction">correction</code></td>
<td>
<p>defunct.</p>
</td></tr>
<tr><td><code id="predict.pense_cvfit_+3A_...">...</code></td>
<td>
<p>currently not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a numeric vector of residuals for the given penalization level.
</p>


<h3>Hyper-parameters</h3>

<p>If <code>lambda = "{m}-se"</code> and <code>object</code> contains fitted estimates for every penalization
level in the sequence, use the fit the most parsimonious model with prediction performance
statistically indistinguishable from the best model.
This is determined to be the model with prediction performance within <code>m * cv_se</code>
from the best model.
If <code>lambda = "se"</code>, the multiplier <em>m</em> is taken from <code>se_mult</code>.
</p>
<p>By default all <em>alpha</em> hyper-parameters available in the fitted object are considered.
This can be overridden by supplying one or multiple values in parameter <code>alpha</code>.
For example, if <code>lambda = "1-se"</code> and <code>alpha</code> contains two values, the &quot;1-SE&quot; rule is applied
individually for each <code>alpha</code> value, and the fit with the better prediction error is considered.
</p>
<p>In case <code>lambda</code> is a number and <code>object</code> was fit for several <em>alpha</em> hyper-parameters,
<code>alpha</code> must also be given, or the first value in <code>object$alpha</code> is used with a warning.
</p>


<h3>See Also</h3>

<p>Other functions for extracting components: 
<code><a href="#topic+coef.pense_cvfit">coef.pense_cvfit</a>()</code>,
<code><a href="#topic+coef.pense_fit">coef.pense_fit</a>()</code>,
<code><a href="#topic+predict.pense_fit">predict.pense_fit</a>()</code>,
<code><a href="#topic+residuals.pense_cvfit">residuals.pense_cvfit</a>()</code>,
<code><a href="#topic+residuals.pense_fit">residuals.pense_fit</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Compute the LS-EN regularization path for Freeny's revenue data
# (see ?freeny)
data(freeny)
x &lt;- as.matrix(freeny[ , 2:5])

regpath &lt;- elnet(x, freeny$y, alpha = 0.75)

# Predict the response using a specific penalization level
predict(regpath, newdata = freeny[1:5, 2:5],
        lambda = regpath$lambda[[1]][[10]])

# Extract the residuals at a certain penalization level
residuals(regpath, lambda = regpath$lambda[[1]][[5]])

# Select penalization level via cross-validation
set.seed(123)
cv_results &lt;- elnet_cv(x, freeny$y, alpha = 0.5,
                       cv_repl = 10, cv_k = 4)

# Predict the response using the "best" penalization level
predict(cv_results, newdata = freeny[1:5, 2:5])

# Extract the residuals at the "best" penalization level
residuals(cv_results)
# Extract the residuals at a more parsimonious penalization level
residuals(cv_results, lambda = "1.5-se")
</code></pre>

<hr>
<h2 id='predict.pense_fit'>Predict Method for PENSE Fits</h2><span id='topic+predict.pense_fit'></span>

<h3>Description</h3>

<p>Predict response values using a PENSE (or LS-EN) regularization path fitted by
<code><a href="#topic+pense">pense()</a></code>, <code><a href="#topic+regmest">regmest()</a></code> or <code><a href="#topic+elnet">elnet()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pense_fit'
predict(
  object,
  newdata,
  alpha = NULL,
  lambda,
  exact = deprecated(),
  correction = deprecated(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.pense_fit_+3A_object">object</code></td>
<td>
<p>PENSE regularization path to extract residuals from.</p>
</td></tr>
<tr><td><code id="predict.pense_fit_+3A_newdata">newdata</code></td>
<td>
<p>an optional matrix of new predictor values.
If missing, the fitted values are computed.</p>
</td></tr>
<tr><td><code id="predict.pense_fit_+3A_alpha">alpha</code></td>
<td>
<p>Either a single number or <code>NULL</code> (default).
If given, only fits with the given <code>alpha</code> value are considered.
If <code>object</code> was fit with multiple <code>alpha</code> values, and no value is provided, the
first value in <code>object$alpha</code> is used with a warning.</p>
</td></tr>
<tr><td><code id="predict.pense_fit_+3A_lambda">lambda</code></td>
<td>
<p>a single number for the penalty level.</p>
</td></tr>
<tr><td><code id="predict.pense_fit_+3A_exact">exact</code></td>
<td>
<p>defunct Always gives a warning if <code>lambda</code> is not part of the fitted
sequence and coefficients need to be interpolated.</p>
</td></tr>
<tr><td><code id="predict.pense_fit_+3A_correction">correction</code></td>
<td>
<p>defunct.</p>
</td></tr>
<tr><td><code id="predict.pense_fit_+3A_...">...</code></td>
<td>
<p>currently not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a numeric vector of residuals for the given penalization level.
</p>


<h3>See Also</h3>

<p>Other functions for extracting components: 
<code><a href="#topic+coef.pense_cvfit">coef.pense_cvfit</a>()</code>,
<code><a href="#topic+coef.pense_fit">coef.pense_fit</a>()</code>,
<code><a href="#topic+predict.pense_cvfit">predict.pense_cvfit</a>()</code>,
<code><a href="#topic+residuals.pense_cvfit">residuals.pense_cvfit</a>()</code>,
<code><a href="#topic+residuals.pense_fit">residuals.pense_fit</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Compute the LS-EN regularization path for Freeny's revenue data
# (see ?freeny)
data(freeny)
x &lt;- as.matrix(freeny[ , 2:5])

regpath &lt;- elnet(x, freeny$y, alpha = 0.75)

# Predict the response using a specific penalization level
predict(regpath, newdata = freeny[1:5, 2:5],
        lambda = regpath$lambda[[1]][[10]])

# Extract the residuals at a certain penalization level
residuals(regpath, lambda = regpath$lambda[[1]][[5]])

# Select penalization level via cross-validation
set.seed(123)
cv_results &lt;- elnet_cv(x, freeny$y, alpha = 0.5,
                       cv_repl = 10, cv_k = 4)

# Predict the response using the "best" penalization level
predict(cv_results, newdata = freeny[1:5, 2:5])

# Extract the residuals at the "best" penalization level
residuals(cv_results)
# Extract the residuals at a more parsimonious penalization level
residuals(cv_results, lambda = "1.5-se")
</code></pre>

<hr>
<h2 id='prediction_performance'>Prediction Performance of Adaptive PENSE Fits</h2><span id='topic+prediction_performance'></span><span id='topic+print.pense_pred_perf'></span>

<h3>Description</h3>

<p>Extract the prediction performance of one or more (adaptive) PENSE fits.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prediction_performance(..., alpha = NULL, lambda = "min", se_mult = 1)

## S3 method for class 'pense_pred_perf'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prediction_performance_+3A_...">...</code></td>
<td>
<p>one or more (adaptive) PENSE fits with cross-validation information.</p>
</td></tr>
<tr><td><code id="prediction_performance_+3A_alpha">alpha</code></td>
<td>
<p>Either a numeric vector or <code>NULL</code> (default).
If given, only fits with the given <code>alpha</code> value are considered.
If <code>lambda</code> is a numeric value and <code>object</code> was fit with multiple <code>alpha</code>
values, the parameter <code>alpha</code> must not be missing.</p>
</td></tr>
<tr><td><code id="prediction_performance_+3A_lambda">lambda</code></td>
<td>
<p>either a string specifying which penalty level to use
(<code>"min"</code>, <code>"se"</code>, <code style="white-space: pre;">&#8288;"{x}-se&#8288;</code>&quot;)
or a single numeric value of the penalty parameter. See details.</p>
</td></tr>
<tr><td><code id="prediction_performance_+3A_se_mult">se_mult</code></td>
<td>
<p>If <code>lambda = "se"</code>, the multiple of standard errors to tolerate.</p>
</td></tr>
<tr><td><code id="prediction_performance_+3A_x">x</code></td>
<td>
<p>an object with information on prediction performance created with <code>prediction_performance()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>lambda = "se"</code> and the cross-validation was performed with multiple replications, use the penalty level whit
prediction performance within <code>se_mult</code> of the best prediction performance.
</p>


<h3>Value</h3>

<p>a data frame with details about the prediction performance of the given PENSE fits. The data frame
has a custom print method summarizing the prediction performances.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+summary.pense_cvfit">summary.pense_cvfit()</a></code> for a summary of the fitted model.
</p>
<p>Other functions for plotting and printing: 
<code><a href="#topic+plot.pense_cvfit">plot.pense_cvfit</a>()</code>,
<code><a href="#topic+plot.pense_fit">plot.pense_fit</a>()</code>,
<code><a href="#topic+summary.pense_cvfit">summary.pense_cvfit</a>()</code>
</p>

<hr>
<h2 id='prinsens'>Principal Sensitivity Components</h2><span id='topic+prinsens'></span>

<h3>Description</h3>

<p>Compute Principal Sensitivity Components for Elastic Net Regression
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prinsens(
  x,
  y,
  alpha,
  lambda,
  intercept = TRUE,
  penalty_loadings,
  en_algorithm_opts,
  eps = 1e-06,
  sparse = FALSE,
  ncores = 1L,
  method = deprecated()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prinsens_+3A_x">x</code></td>
<td>
<p><code>n</code> by <code>p</code> matrix of numeric predictors.</p>
</td></tr>
<tr><td><code id="prinsens_+3A_y">y</code></td>
<td>
<p>vector of response values of length <code>n</code>.</p>
</td></tr>
<tr><td><code id="prinsens_+3A_alpha">alpha</code></td>
<td>
<p>elastic net penalty mixing parameter with <code class="reqn">0 \le \alpha \le 1</code>.
<code>alpha = 1</code> is the LASSO penalty, and <code>alpha = 0</code> the Ridge penalty.
Can be a vector of several values, but <code>alpha = 0</code> cannot be mixed with other values.</p>
</td></tr>
<tr><td><code id="prinsens_+3A_lambda">lambda</code></td>
<td>
<p>optional user-supplied sequence of penalization levels. If given and not <code>NULL</code>,
<code>nlambda</code> and <code>lambda_min_ratio</code> are ignored.</p>
</td></tr>
<tr><td><code id="prinsens_+3A_intercept">intercept</code></td>
<td>
<p>include an intercept in the model.</p>
</td></tr>
<tr><td><code id="prinsens_+3A_penalty_loadings">penalty_loadings</code></td>
<td>
<p>a vector of positive penalty loadings (a.k.a. weights) for different
penalization of each coefficient. Only allowed for <code>alpha</code> &gt; 0.</p>
</td></tr>
<tr><td><code id="prinsens_+3A_en_algorithm_opts">en_algorithm_opts</code></td>
<td>
<p>options for the LS-EN algorithm. See <a href="#topic+en_algorithm_options">en_algorithm_options</a> for details.</p>
</td></tr>
<tr><td><code id="prinsens_+3A_eps">eps</code></td>
<td>
<p>numerical tolerance.</p>
</td></tr>
<tr><td><code id="prinsens_+3A_sparse">sparse</code></td>
<td>
<p>use sparse coefficient vectors.</p>
</td></tr>
<tr><td><code id="prinsens_+3A_ncores">ncores</code></td>
<td>
<p>number of CPU cores to use in parallel. By default, only one CPU core is used.
Not supported on all platforms, in which case a warning is given.</p>
</td></tr>
<tr><td><code id="prinsens_+3A_method">method</code></td>
<td>
<p>defunct. PSCs are always computed for EN estimates. For the PY procedure for unpenalized estimation
use package <a href="https://cran.r-project.org/package=pyinit">pyinit</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of principal sensitivity components, one per element in <code>lambda</code>. Each PSC is itself a list
with items <code>lambda</code>, <code>alpha</code>, and <code>pscs</code>.
</p>


<h3>References</h3>

<p>Cohen Freue, G.V.; Kepplinger, D.; Salibián-Barrera, M.; Smucler, E.
Robust elastic net estimators for variable selection and identification of proteomic biomarkers.
<em>Ann. Appl. Stat.</em> <strong>13</strong> (2019), no. 4, 2065&ndash;2090 <a href="https://doi.org/10.1214/19-AOAS1269">doi:10.1214/19-AOAS1269</a>
</p>
<p>Pena, D., and Yohai, V.J.
A Fast Procedure for Outlier Diagnostics in Large Regression Problems.
<em>J. Amer. Statist. Assoc.</em> <strong>94</strong> (1999). no. 446, 434&ndash;445. <a href="https://doi.org/10.2307/2670164">doi:10.2307/2670164</a>
</p>


<h3>See Also</h3>

<p>Other functions for initial estimates: 
<code><a href="#topic+enpy_initial_estimates">enpy_initial_estimates</a>()</code>,
<code><a href="#topic+starting_point">starting_point</a>()</code>
</p>

<hr>
<h2 id='print.nsoptim_metrics'>Print Metrics</h2><span id='topic+print.nsoptim_metrics'></span>

<h3>Description</h3>

<p>Pretty-print a list of metrics from optimization algorithm (if <code>pense</code> was built with metrics enabled).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'nsoptim_metrics'
print(x, max_level = NA, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.nsoptim_metrics_+3A_x">x</code></td>
<td>
<p>metrics object for printing.</p>
</td></tr>
<tr><td><code id="print.nsoptim_metrics_+3A_max_level">max_level</code></td>
<td>
<p>maximum level of printing which is applied for printing nested metrics.</p>
</td></tr>
</table>

<hr>
<h2 id='regmest'>Compute (Adaptive) Elastic Net M-Estimates of Regression</h2><span id='topic+regmest'></span>

<h3>Description</h3>

<p>Compute elastic net M-estimates along a grid of penalization levels with optional
penalty loadings for adaptive elastic net.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regmest(
  x,
  y,
  alpha,
  nlambda = 50,
  lambda,
  lambda_min_ratio,
  scale,
  starting_points,
  penalty_loadings,
  intercept = TRUE,
  cc = 4.7,
  eps = 1e-06,
  explore_solutions = 10,
  explore_tol = 0.1,
  max_solutions = 10,
  comparison_tol = sqrt(eps),
  sparse = FALSE,
  ncores = 1,
  standardize = TRUE,
  algorithm_opts = mm_algorithm_options(),
  add_zero_based = TRUE,
  mscale_bdp = 0.25,
  mscale_opts = mscale_algorithm_options()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="regmest_+3A_x">x</code></td>
<td>
<p><code>n</code> by <code>p</code> matrix of numeric predictors.</p>
</td></tr>
<tr><td><code id="regmest_+3A_y">y</code></td>
<td>
<p>vector of response values of length <code>n</code>.
For binary classification, <code>y</code> should be a factor with 2 levels.</p>
</td></tr>
<tr><td><code id="regmest_+3A_alpha">alpha</code></td>
<td>
<p>elastic net penalty mixing parameter with <code class="reqn">0 \le \alpha \le 1</code>.
<code>alpha = 1</code> is the LASSO penalty, and <code>alpha = 0</code> the Ridge penalty.</p>
</td></tr>
<tr><td><code id="regmest_+3A_nlambda">nlambda</code></td>
<td>
<p>number of penalization levels.</p>
</td></tr>
<tr><td><code id="regmest_+3A_lambda">lambda</code></td>
<td>
<p>optional user-supplied sequence of penalization levels.
If given and not <code>NULL</code>, <code>nlambda</code> and <code>lambda_min_ratio</code> are ignored.</p>
</td></tr>
<tr><td><code id="regmest_+3A_lambda_min_ratio">lambda_min_ratio</code></td>
<td>
<p>Smallest value of the penalization level as a fraction of the
largest level (i.e., the smallest value for which all coefficients are zero).
The default depends on the sample size relative to the number of variables and <code>alpha</code>.
If more observations than variables are available, the default is <code>1e-3 * alpha</code>,
otherwise <code>1e-2 * alpha</code>.</p>
</td></tr>
<tr><td><code id="regmest_+3A_scale">scale</code></td>
<td>
<p>fixed scale of the residuals.</p>
</td></tr>
<tr><td><code id="regmest_+3A_starting_points">starting_points</code></td>
<td>
<p>a list of staring points, created by <code><a href="#topic+starting_point">starting_point()</a></code>.
The starting points are shared among all penalization levels.</p>
</td></tr>
<tr><td><code id="regmest_+3A_penalty_loadings">penalty_loadings</code></td>
<td>
<p>a vector of positive penalty loadings (a.k.a. weights)
for different penalization of each coefficient. Only allowed for <code>alpha</code> &gt; 0.</p>
</td></tr>
<tr><td><code id="regmest_+3A_intercept">intercept</code></td>
<td>
<p>include an intercept in the model.</p>
</td></tr>
<tr><td><code id="regmest_+3A_cc">cc</code></td>
<td>
<p>cutoff constant for Tukey's bisquare <code class="reqn">\rho</code> function.</p>
</td></tr>
<tr><td><code id="regmest_+3A_eps">eps</code></td>
<td>
<p>numerical tolerance.</p>
</td></tr>
<tr><td><code id="regmest_+3A_explore_solutions">explore_solutions</code></td>
<td>
<p>number of solutions to compute up to the desired precision <code>eps</code>.</p>
</td></tr>
<tr><td><code id="regmest_+3A_explore_tol">explore_tol</code></td>
<td>
<p>numerical tolerance for exploring possible solutions.
Should be (much) looser than <code>eps</code> to be useful.</p>
</td></tr>
<tr><td><code id="regmest_+3A_max_solutions">max_solutions</code></td>
<td>
<p>only retain up to <code>max_solutions</code> unique solutions per penalization level.</p>
</td></tr>
<tr><td><code id="regmest_+3A_comparison_tol">comparison_tol</code></td>
<td>
<p>numeric tolerance to determine if two solutions are equal.
The comparison is first done on the absolute difference in the value of the objective
function at the solution.
If this is less than <code>comparison_tol</code>, two solutions are deemed equal if the
squared difference of the intercepts is less than <code>comparison_tol</code> and the squared
<code class="reqn">L_2</code> norm of the difference vector is less than <code>comparison_tol</code>.</p>
</td></tr>
<tr><td><code id="regmest_+3A_sparse">sparse</code></td>
<td>
<p>use sparse coefficient vectors.</p>
</td></tr>
<tr><td><code id="regmest_+3A_ncores">ncores</code></td>
<td>
<p>number of CPU cores to use in parallel. By default, only one CPU core is used.
Not supported on all platforms, in which case a warning is given.</p>
</td></tr>
<tr><td><code id="regmest_+3A_standardize">standardize</code></td>
<td>
<p>logical flag to standardize the <code>x</code> variables prior to fitting the
M-estimates. Coefficients are always returned on the original scale.
This can fail for variables with a large proportion of a single value
(e.g., zero-inflated data). In this case, either compute with
<code>standardize = FALSE</code> or standardize the data manually.</p>
</td></tr>
<tr><td><code id="regmest_+3A_algorithm_opts">algorithm_opts</code></td>
<td>
<p>options for the MM algorithm to compute estimates.
See <code><a href="#topic+mm_algorithm_options">mm_algorithm_options()</a></code> for details.</p>
</td></tr>
<tr><td><code id="regmest_+3A_add_zero_based">add_zero_based</code></td>
<td>
<p>also consider the 0-based regularization path in addition to the given
starting points.</p>
</td></tr>
<tr><td><code id="regmest_+3A_mscale_bdp">mscale_bdp</code>, <code id="regmest_+3A_mscale_opts">mscale_opts</code></td>
<td>
<p>options for the M-scale estimate used to standardize
the predictors (if <code>standardize = TRUE</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list-like object with the following items
</p>

<dl>
<dt><code>alpha</code></dt><dd><p>the sequence of <code>alpha</code> parameters.</p>
</dd>
<dt><code>lambda</code></dt><dd><p>a list of sequences of penalization levels, one per <code>alpha</code> parameter.</p>
</dd>
<dt><code>scale</code></dt><dd><p>the used scale of the residuals.</p>
</dd>
<dt><code>estimates</code></dt><dd><p>a list of estimates. Each estimate contains the following information:
</p>

<dl>
<dt><code>intercept</code></dt><dd><p>intercept estimate.</p>
</dd>
<dt><code>beta</code></dt><dd><p>beta (slope) estimate.</p>
</dd>
<dt><code>lambda</code></dt><dd><p>penalization level at which the estimate is computed.</p>
</dd>
<dt><code>alpha</code></dt><dd><p><em>alpha</em> hyper-parameter at which the estimate is computed.</p>
</dd>
<dt><code>objf_value</code></dt><dd><p>value of the objective function at the solution.</p>
</dd>
<dt><code>statuscode</code></dt><dd><p>if <code style="white-space: pre;">&#8288;&gt; 0&#8288;</code> the algorithm experienced issues when
computing the estimate.</p>
</dd>
<dt><code>status</code></dt><dd><p>optional status message from the algorithm.</p>
</dd>
</dl>

</dd>
<dt><code>call</code></dt><dd><p>the original call.</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="#topic+regmest_cv">regmest_cv()</a></code> for selecting hyper-parameters via cross-validation.
</p>
<p><code><a href="#topic+coef.pense_fit">coef.pense_fit()</a></code> for extracting coefficient estimates.
</p>
<p><code><a href="#topic+plot.pense_fit">plot.pense_fit()</a></code> for plotting the regularization path.
</p>
<p>Other functions to compute robust estimates: 
<code><a href="#topic+pense">pense</a>()</code>
</p>

<hr>
<h2 id='regmest_cv'>Cross-validation for (Adaptive) Elastic Net M-Estimates</h2><span id='topic+regmest_cv'></span><span id='topic+adamest_cv'></span>

<h3>Description</h3>

<p>Perform (repeated) K-fold cross-validation for <code><a href="#topic+regmest">regmest()</a></code>.
</p>
<p><code>adamest_cv()</code> is a convenience wrapper to compute adaptive elastic-net M-estimates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regmest_cv(
  x,
  y,
  standardize = TRUE,
  lambda,
  cv_k,
  cv_repl = 1,
  cv_metric = c("tau_size", "mape", "rmspe", "auroc"),
  fit_all = TRUE,
  cl = NULL,
  ...
)

adamest_cv(x, y, alpha, alpha_preliminary = 0, exponent = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="regmest_cv_+3A_x">x</code></td>
<td>
<p><code>n</code> by <code>p</code> matrix of numeric predictors.</p>
</td></tr>
<tr><td><code id="regmest_cv_+3A_y">y</code></td>
<td>
<p>vector of response values of length <code>n</code>.
For binary classification, <code>y</code> should be a factor with 2 levels.</p>
</td></tr>
<tr><td><code id="regmest_cv_+3A_standardize">standardize</code></td>
<td>
<p>whether to standardize the <code>x</code> variables prior to fitting the PENSE estimates.
Can also be set to <code>"cv_only"</code>, in which case the input data is not standardized, but the
training data in the CV folds is scaled to match the scaling of the input data.
Coefficients are always returned on the original scale.
This can fail for variables with a large proportion of a single value
(e.g., zero-inflated data).
In this case, either compute with <code>standardize = FALSE</code> or standardize the data manually.</p>
</td></tr>
<tr><td><code id="regmest_cv_+3A_lambda">lambda</code></td>
<td>
<p>optional user-supplied sequence of penalization levels.
If given and not <code>NULL</code>, <code>nlambda</code> and <code>lambda_min_ratio</code> are ignored.</p>
</td></tr>
<tr><td><code id="regmest_cv_+3A_cv_k">cv_k</code></td>
<td>
<p>number of folds per cross-validation.</p>
</td></tr>
<tr><td><code id="regmest_cv_+3A_cv_repl">cv_repl</code></td>
<td>
<p>number of cross-validation replications.</p>
</td></tr>
<tr><td><code id="regmest_cv_+3A_cv_metric">cv_metric</code></td>
<td>
<p>either a string specifying the performance metric to use, or a function to
evaluate prediction errors in a single CV replication.
If a function, the number of arguments define the data the function receives.
If the function takes a single argument, it is called with a single numeric vector of
prediction errors.
If the function takes two or more arguments, it is called with the predicted values as
first argument and the true values as second argument.
The function must always return a single numeric value quantifying the prediction performance.
The order of the given values corresponds to the order in the input data.</p>
</td></tr>
<tr><td><code id="regmest_cv_+3A_fit_all">fit_all</code></td>
<td>
<p>If <code>TRUE</code>, fit the model for all penalization levels.
Can also be any combination of <code>"min"</code> and <code>"{x}-se"</code>, in which case only models at the
penalization level with smallest average CV accuracy, or within <code>{x}</code> standard errors,
respectively.
Setting <code>fit_all</code> to <code>FALSE</code> is equivalent to <code>"min"</code>.
Applies to all <code>alpha</code> value.</p>
</td></tr>
<tr><td><code id="regmest_cv_+3A_cl">cl</code></td>
<td>
<p>a <a href="parallel.html#topic+makeCluster">parallel</a> cluster. Can only be used in combination with
<code>ncores = 1</code>.</p>
</td></tr>
<tr><td><code id="regmest_cv_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+regmest">regmest</a></code>
</p>

<dl>
<dt><code>scale</code></dt><dd><p>fixed scale of the residuals.</p>
</dd>
<dt><code>nlambda</code></dt><dd><p>number of penalization levels.</p>
</dd>
<dt><code>lambda_min_ratio</code></dt><dd><p>Smallest value of the penalization level as a fraction of the
largest level (i.e., the smallest value for which all coefficients are zero).
The default depends on the sample size relative to the number of variables and <code>alpha</code>.
If more observations than variables are available, the default is <code>1e-3 * alpha</code>,
otherwise <code>1e-2 * alpha</code>.</p>
</dd>
<dt><code>penalty_loadings</code></dt><dd><p>a vector of positive penalty loadings (a.k.a. weights)
for different penalization of each coefficient. Only allowed for <code>alpha</code> &gt; 0.</p>
</dd>
<dt><code>starting_points</code></dt><dd><p>a list of staring points, created by <code><a href="#topic+starting_point">starting_point()</a></code>.
The starting points are shared among all penalization levels.</p>
</dd>
<dt><code>intercept</code></dt><dd><p>include an intercept in the model.</p>
</dd>
<dt><code>add_zero_based</code></dt><dd><p>also consider the 0-based regularization path in addition to the given
starting points.</p>
</dd>
<dt><code>cc</code></dt><dd><p>cutoff constant for Tukey's bisquare <code class="reqn">\rho</code> function.</p>
</dd>
<dt><code>eps</code></dt><dd><p>numerical tolerance.</p>
</dd>
<dt><code>explore_solutions</code></dt><dd><p>number of solutions to compute up to the desired precision <code>eps</code>.</p>
</dd>
<dt><code>explore_tol</code></dt><dd><p>numerical tolerance for exploring possible solutions.
Should be (much) looser than <code>eps</code> to be useful.</p>
</dd>
<dt><code>max_solutions</code></dt><dd><p>only retain up to <code>max_solutions</code> unique solutions per penalization level.</p>
</dd>
<dt><code>comparison_tol</code></dt><dd><p>numeric tolerance to determine if two solutions are equal.
The comparison is first done on the absolute difference in the value of the objective
function at the solution.
If this is less than <code>comparison_tol</code>, two solutions are deemed equal if the
squared difference of the intercepts is less than <code>comparison_tol</code> and the squared
<code class="reqn">L_2</code> norm of the difference vector is less than <code>comparison_tol</code>.</p>
</dd>
<dt><code>sparse</code></dt><dd><p>use sparse coefficient vectors.</p>
</dd>
<dt><code>ncores</code></dt><dd><p>number of CPU cores to use in parallel. By default, only one CPU core is used.
Not supported on all platforms, in which case a warning is given.</p>
</dd>
<dt><code>algorithm_opts</code></dt><dd><p>options for the MM algorithm to compute estimates.
See <code><a href="#topic+mm_algorithm_options">mm_algorithm_options()</a></code> for details.</p>
</dd>
<dt><code>mscale_bdp,mscale_opts</code></dt><dd><p>options for the M-scale estimate used to standardize
the predictors (if <code>standardize = TRUE</code>).</p>
</dd>
</dl>
</td></tr>
<tr><td><code id="regmest_cv_+3A_alpha">alpha</code></td>
<td>
<p>elastic net penalty mixing parameter with <code class="reqn">0 \le \alpha \le 1</code>.
<code>alpha = 1</code> is the LASSO penalty, and <code>alpha = 0</code> the Ridge penalty.</p>
</td></tr>
<tr><td><code id="regmest_cv_+3A_alpha_preliminary">alpha_preliminary</code></td>
<td>
<p><code>alpha</code> parameter for the preliminary estimate.</p>
</td></tr>
<tr><td><code id="regmest_cv_+3A_exponent">exponent</code></td>
<td>
<p>the exponent for computing the penalty loadings based on the
preliminary estimate.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The built-in CV metrics are
</p>

<dl>
<dt><code>"tau_size"</code></dt><dd><p><code class="reqn">\tau</code>-size of the prediction error, computed by
<code><a href="#topic+tau_size">tau_size()</a></code> (default).</p>
</dd>
<dt><code>"mape"</code></dt><dd><p>Median absolute prediction error.</p>
</dd>
<dt><code>"rmspe"</code></dt><dd><p>Root mean squared prediction error.</p>
</dd>
<dt><code>"auroc"</code></dt><dd><p>Area under the receiver operator characteristic curve (actually 1 - AUROC).
Only sensible for binary responses.</p>
</dd>
</dl>

<p><code>adamest_cv()</code> is a convenience wrapper which performs 3 steps:
</p>

<ol>
<li><p> compute preliminary estimates via <code>regmest_cv(..., alpha = alpha_preliminary)</code>,
</p>
</li>
<li><p> computes the penalty loadings from the estimate <code>beta</code> with best prediction performance by
<code>adamest_loadings = 1 / abs(beta)^exponent</code>, and
</p>
</li>
<li><p> compute the adaptive PENSE estimates via
<code>regmest_cv(..., penalty_loadings = adamest_loadings)</code>.
</p>
</li></ol>



<h3>Value</h3>

<p>a list-like object as returned by <code><a href="#topic+regmest">regmest()</a></code>, plus the following components:
</p>

<dl>
<dt><code>cvres</code></dt><dd><p>data frame of average cross-validated performance.</p>
</dd>
</dl>

<p>a list-like object as returned by <code><a href="#topic+adamest_cv">adamest_cv()</a></code> plus the following components:
</p>

<dl>
<dt><code>exponent</code></dt><dd><p>value of the exponent.</p>
</dd>
<dt><code>preliminary</code></dt><dd><p>CV results for the preliminary estimate.</p>
</dd>
<dt><code>penalty_loadings</code></dt><dd><p>penalty loadings used for the adaptive
elastic net M-estimate.</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="#topic+regmest">regmest()</a></code> for computing regularized S-estimates without cross-validation.
</p>
<p><code><a href="#topic+coef.pense_cvfit">coef.pense_cvfit()</a></code> for extracting coefficient estimates.
</p>
<p><code><a href="#topic+plot.pense_cvfit">plot.pense_cvfit()</a></code> for plotting the CV performance or the regularization path.
</p>
<p>Other functions to compute robust estimates with CV: 
<code><a href="#topic+pense_cv">pense_cv</a>()</code>,
<code><a href="#topic+pensem_cv">pensem_cv</a>()</code>
</p>
<p>Other functions to compute robust estimates with CV: 
<code><a href="#topic+pense_cv">pense_cv</a>()</code>,
<code><a href="#topic+pensem_cv">pensem_cv</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Compute the adaptive PENSE regularization path for Freeny's
# revenue data (see ?freeny)
data(freeny)
x &lt;- as.matrix(freeny[ , 2:5])

## Either use the convenience function directly ...
set.seed(123)
ada_convenience &lt;- adapense_cv(x, freeny$y, alpha = 0.5,
                               cv_repl = 2, cv_k = 4)

## ... or compute the steps manually:
# Step 1: Compute preliminary estimates with CV
set.seed(123)
preliminary_estimate &lt;- pense_cv(x, freeny$y, alpha = 0,
                                 cv_repl = 2, cv_k = 4)
plot(preliminary_estimate, se_mult = 1)

# Step 2: Use the coefficients with best prediction performance
# to define the penalty loadings:
prelim_coefs &lt;- coef(preliminary_estimate, lambda = 'min')
pen_loadings &lt;- 1 / abs(prelim_coefs[-1])

# Step 3: Compute the adaptive PENSE estimates and estimate
# their prediction performance.
set.seed(123)
ada_manual &lt;- pense_cv(x, freeny$y, alpha = 0.5,
                       cv_repl = 2, cv_k = 4,
                       penalty_loadings = pen_loadings)

# Visualize the prediction performance and coefficient path of
# the adaptive PENSE estimates (manual vs. automatic)
def.par &lt;- par(no.readonly = TRUE)
layout(matrix(1:4, ncol = 2, byrow = TRUE))
plot(ada_convenience$preliminary)
plot(preliminary_estimate)
plot(ada_convenience)
plot(ada_manual)
par(def.par)
</code></pre>

<hr>
<h2 id='residuals.pense_cvfit'>Extract Residuals</h2><span id='topic+residuals.pense_cvfit'></span>

<h3>Description</h3>

<p>Extract residuals from a PENSE (or LS-EN) regularization path with hyper-parameters
chosen by cross-validation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pense_cvfit'
residuals(
  object,
  alpha = NULL,
  lambda = "min",
  se_mult = 1,
  exact = deprecated(),
  correction = deprecated(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="residuals.pense_cvfit_+3A_object">object</code></td>
<td>
<p>PENSE with cross-validated hyper-parameters to extract coefficients from.</p>
</td></tr>
<tr><td><code id="residuals.pense_cvfit_+3A_alpha">alpha</code></td>
<td>
<p>Either a single number or <code>NULL</code> (default).
If given, only fits with the given <code>alpha</code> value are considered.
If <code>lambda</code> is a numeric value and <code>object</code> was fit with multiple <em>alpha</em>
values and no value is provided, the first value in <code>object$alpha</code> is used with a warning.</p>
</td></tr>
<tr><td><code id="residuals.pense_cvfit_+3A_lambda">lambda</code></td>
<td>
<p>either a string specifying which penalty level to use
(<code>"min"</code>, <code>"se"</code>, <code style="white-space: pre;">&#8288;"{m}-se&#8288;</code>&quot;)
or a single numeric value of the penalty parameter. See details.</p>
</td></tr>
<tr><td><code id="residuals.pense_cvfit_+3A_se_mult">se_mult</code></td>
<td>
<p>If <code>lambda = "se"</code>, the multiple of standard errors to tolerate.</p>
</td></tr>
<tr><td><code id="residuals.pense_cvfit_+3A_exact">exact</code></td>
<td>
<p>deprecated. Always gives a warning if <code>lambda</code> is not part of the fitted sequence and coefficients
are interpolated.</p>
</td></tr>
<tr><td><code id="residuals.pense_cvfit_+3A_correction">correction</code></td>
<td>
<p>defunct.</p>
</td></tr>
<tr><td><code id="residuals.pense_cvfit_+3A_...">...</code></td>
<td>
<p>currently not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a numeric vector of residuals for the given penalization level.
</p>


<h3>Hyper-parameters</h3>

<p>If <code>lambda = "{m}-se"</code> and <code>object</code> contains fitted estimates for every penalization
level in the sequence, use the fit the most parsimonious model with prediction performance
statistically indistinguishable from the best model.
This is determined to be the model with prediction performance within <code>m * cv_se</code>
from the best model.
If <code>lambda = "se"</code>, the multiplier <em>m</em> is taken from <code>se_mult</code>.
</p>
<p>By default all <em>alpha</em> hyper-parameters available in the fitted object are considered.
This can be overridden by supplying one or multiple values in parameter <code>alpha</code>.
For example, if <code>lambda = "1-se"</code> and <code>alpha</code> contains two values, the &quot;1-SE&quot; rule is applied
individually for each <code>alpha</code> value, and the fit with the better prediction error is considered.
</p>
<p>In case <code>lambda</code> is a number and <code>object</code> was fit for several <em>alpha</em> hyper-parameters,
<code>alpha</code> must also be given, or the first value in <code>object$alpha</code> is used with a warning.
</p>


<h3>See Also</h3>

<p>Other functions for extracting components: 
<code><a href="#topic+coef.pense_cvfit">coef.pense_cvfit</a>()</code>,
<code><a href="#topic+coef.pense_fit">coef.pense_fit</a>()</code>,
<code><a href="#topic+predict.pense_cvfit">predict.pense_cvfit</a>()</code>,
<code><a href="#topic+predict.pense_fit">predict.pense_fit</a>()</code>,
<code><a href="#topic+residuals.pense_fit">residuals.pense_fit</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Compute the LS-EN regularization path for Freeny's revenue data
# (see ?freeny)
data(freeny)
x &lt;- as.matrix(freeny[ , 2:5])

regpath &lt;- elnet(x, freeny$y, alpha = 0.75)

# Predict the response using a specific penalization level
predict(regpath, newdata = freeny[1:5, 2:5],
        lambda = regpath$lambda[[1]][[10]])

# Extract the residuals at a certain penalization level
residuals(regpath, lambda = regpath$lambda[[1]][[5]])

# Select penalization level via cross-validation
set.seed(123)
cv_results &lt;- elnet_cv(x, freeny$y, alpha = 0.5,
                       cv_repl = 10, cv_k = 4)

# Predict the response using the "best" penalization level
predict(cv_results, newdata = freeny[1:5, 2:5])

# Extract the residuals at the "best" penalization level
residuals(cv_results)
# Extract the residuals at a more parsimonious penalization level
residuals(cv_results, lambda = "1.5-se")
</code></pre>

<hr>
<h2 id='residuals.pense_fit'>Extract Residuals</h2><span id='topic+residuals.pense_fit'></span>

<h3>Description</h3>

<p>Extract residuals from a PENSE (or LS-EN) regularization path fitted by
<code><a href="#topic+pense">pense()</a></code>, <code><a href="#topic+regmest">regmest()</a></code> or <code><a href="#topic+elnet">elnet()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pense_fit'
residuals(
  object,
  alpha = NULL,
  lambda,
  exact = deprecated(),
  correction = deprecated(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="residuals.pense_fit_+3A_object">object</code></td>
<td>
<p>PENSE regularization path to extract residuals from.</p>
</td></tr>
<tr><td><code id="residuals.pense_fit_+3A_alpha">alpha</code></td>
<td>
<p>Either a single number or <code>NULL</code> (default).
If given, only fits with the given <code>alpha</code> value are considered.
If <code>object</code> was fit with multiple <code>alpha</code> values, and no value is provided, the
first value in <code>object$alpha</code> is used with a warning.</p>
</td></tr>
<tr><td><code id="residuals.pense_fit_+3A_lambda">lambda</code></td>
<td>
<p>a single number for the penalty level.</p>
</td></tr>
<tr><td><code id="residuals.pense_fit_+3A_exact">exact</code></td>
<td>
<p>defunct Always gives a warning if <code>lambda</code> is not part of
the fitted sequence and coefficients need to be interpolated.</p>
</td></tr>
<tr><td><code id="residuals.pense_fit_+3A_correction">correction</code></td>
<td>
<p>defunct.</p>
</td></tr>
<tr><td><code id="residuals.pense_fit_+3A_...">...</code></td>
<td>
<p>currently not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a numeric vector of residuals for the given penalization level.
</p>


<h3>See Also</h3>

<p>Other functions for extracting components: 
<code><a href="#topic+coef.pense_cvfit">coef.pense_cvfit</a>()</code>,
<code><a href="#topic+coef.pense_fit">coef.pense_fit</a>()</code>,
<code><a href="#topic+predict.pense_cvfit">predict.pense_cvfit</a>()</code>,
<code><a href="#topic+predict.pense_fit">predict.pense_fit</a>()</code>,
<code><a href="#topic+residuals.pense_cvfit">residuals.pense_cvfit</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Compute the LS-EN regularization path for Freeny's revenue data
# (see ?freeny)
data(freeny)
x &lt;- as.matrix(freeny[ , 2:5])

regpath &lt;- elnet(x, freeny$y, alpha = 0.75)

# Predict the response using a specific penalization level
predict(regpath, newdata = freeny[1:5, 2:5],
        lambda = regpath$lambda[[1]][[10]])

# Extract the residuals at a certain penalization level
residuals(regpath, lambda = regpath$lambda[[1]][[5]])

# Select penalization level via cross-validation
set.seed(123)
cv_results &lt;- elnet_cv(x, freeny$y, alpha = 0.5,
                       cv_repl = 10, cv_k = 4)

# Predict the response using the "best" penalization level
predict(cv_results, newdata = freeny[1:5, 2:5])

# Extract the residuals at the "best" penalization level
residuals(cv_results)
# Extract the residuals at a more parsimonious penalization level
residuals(cv_results, lambda = "1.5-se")
</code></pre>

<hr>
<h2 id='rho_function'>List Available Rho Functions</h2><span id='topic+rho_function'></span>

<h3>Description</h3>

<p>List Available Rho Functions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rho_function(rho)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rho_function_+3A_rho">rho</code></td>
<td>
<p>the name of the <code class="reqn">\rho</code> function to check for existence.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>if <code>rho</code> is missing returns a vector of supported <code class="reqn">\rho</code> function names, otherwise
the internal integer representation of the <code class="reqn">\rho</code> function.
</p>


<h3>See Also</h3>

<p>Other miscellaneous functions: 
<code><a href="#topic+consistency_const">consistency_const</a>()</code>
</p>

<hr>
<h2 id='starting_point'>Create Starting Points for the PENSE Algorithm</h2><span id='topic+starting_point'></span><span id='topic+as_starting_point'></span><span id='topic+as_starting_point.enpy_starting_points'></span><span id='topic+as_starting_point.pense_fit'></span><span id='topic+as_starting_point.pense_cvfit'></span>

<h3>Description</h3>

<p>Create a starting point for starting the PENSE algorithm in <code><a href="#topic+pense">pense()</a></code>.
Multiple starting points can be created by combining starting points via
<code>c(starting_point_1, starting_point_2, ...)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>starting_point(beta, intercept, lambda, alpha)

as_starting_point(object, specific = FALSE, ...)

## S3 method for class 'enpy_starting_points'
as_starting_point(object, specific = FALSE, ...)

## S3 method for class 'pense_fit'
as_starting_point(object, specific = FALSE, alpha, lambda, ...)

## S3 method for class 'pense_cvfit'
as_starting_point(
  object,
  specific = FALSE,
  alpha,
  lambda = c("min", "se"),
  se_mult = 1,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="starting_point_+3A_beta">beta</code></td>
<td>
<p>beta coefficients at the starting point. Can be a numeric vector,
a sparse vector of class <a href="Matrix.html#topic+sparseVector-class">dsparseVector</a>,
or a sparse matrix of class <a href="Matrix.html#topic+CsparseMatrix-class">dgCMatrix</a> with a single column.</p>
</td></tr>
<tr><td><code id="starting_point_+3A_intercept">intercept</code></td>
<td>
<p>intercept coefficient at the starting point.</p>
</td></tr>
<tr><td><code id="starting_point_+3A_lambda">lambda</code></td>
<td>
<p>optionally either a string specifying which penalty level to use
(<code>"min"</code> or <code>"se"</code>) or a numeric vector of the penalty levels to extract from <code>object</code>.
Penalization levels not present in <code>object</code> are ignored with a warning.
If <code>NULL</code>, all estimates in <code>object</code> are extracted.
If a numeric vector, <code>alpha</code> must be given and a single number.</p>
</td></tr>
<tr><td><code id="starting_point_+3A_alpha">alpha</code></td>
<td>
<p>optional value for the <code>alpha</code> hyper-parameter.
If given, only estimates with matching <code>alpha</code> values are extracted.
Values not present in <code>object</code> are ignored with a warning.</p>
</td></tr>
<tr><td><code id="starting_point_+3A_object">object</code></td>
<td>
<p>an object with estimates to use as starting points.</p>
</td></tr>
<tr><td><code id="starting_point_+3A_specific">specific</code></td>
<td>
<p>whether the estimates should be used as starting points only at
the penalization level they are computed for.
Defaults to using the estimates as starting points for all penalization levels.</p>
</td></tr>
<tr><td><code id="starting_point_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
<tr><td><code id="starting_point_+3A_se_mult">se_mult</code></td>
<td>
<p>If <code>lambda = "se"</code>, the multiple of standard errors to tolerate.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A starting points can either be <em>shared</em>, i.e., used for every penalization level PENSE
estimates are computed for, or <em>specific</em> to one penalization level.
To create a specific starting point, provide the penalization parameters <code>lambda</code> and <code>alpha</code>.
If <code>lambda</code> or <code>alpha</code> are missing, a shared starting point is created.
Shared and specific starting points can all be combined into a single list of starting points,
with <code><a href="#topic+pense">pense()</a></code> handling them correctly.
Note that specific starting points will lead to the <code>lambda</code> value being added to the
grid of penalization levels.
See <code><a href="#topic+pense">pense()</a></code> for details.
</p>
<p>Starting points computed via <code><a href="#topic+enpy_initial_estimates">enpy_initial_estimates()</a></code> are by default <em>shared</em> starting points
but can be transformed to <em>specific</em> starting points via
<code>as_starting_point(..., specific = TRUE)</code>.
</p>
<p>When creating starting points from cross-validated fits, it is possible to extract only the
estimate with best CV performance (<code>lambda = "min"</code>), or the estimate with CV performance
statistically indistinguishable from the best performance (<code>lambda = "se"</code>).
This is determined to be the estimate with prediction performance within
<code>se_mult * cv_se</code> from the best model.
</p>


<h3>Value</h3>

<p>an object of type <code>starting_points</code> to be used as starting point for <code><a href="#topic+pense">pense()</a></code>.
</p>


<h3>See Also</h3>

<p>Other functions for initial estimates: 
<code><a href="#topic+enpy_initial_estimates">enpy_initial_estimates</a>()</code>,
<code><a href="#topic+prinsens">prinsens</a>()</code>
</p>

<hr>
<h2 id='summary.pense_cvfit'>Summarize Cross-Validated PENSE Fit</h2><span id='topic+summary.pense_cvfit'></span><span id='topic+print.pense_cvfit'></span>

<h3>Description</h3>

<p>If <code>lambda = "se"</code> and <code>object</code> contains fitted estimates for every penalization level in the sequence, extract the
coefficients of the most parsimonious model with prediction performance statistically indistinguishable from the best
model. This is determined to be the model with prediction performance within <code>se_mult * cv_se</code> from the best model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pense_cvfit'
summary(object, alpha, lambda = "min", se_mult = 1, ...)

## S3 method for class 'pense_cvfit'
print(x, alpha, lambda = "min", se_mult = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.pense_cvfit_+3A_object">object</code>, <code id="summary.pense_cvfit_+3A_x">x</code></td>
<td>
<p>an (adaptive) PENSE fit with cross-validation information.</p>
</td></tr>
<tr><td><code id="summary.pense_cvfit_+3A_alpha">alpha</code></td>
<td>
<p>Either a single number or missing.
If given, only fits with the given <code>alpha</code> value are considered.
If <code>lambda</code> is a numeric value and <code>object</code> was fit with multiple <code>alpha</code>
values, the parameter <code>alpha</code> must not be missing.</p>
</td></tr>
<tr><td><code id="summary.pense_cvfit_+3A_lambda">lambda</code></td>
<td>
<p>either a string specifying which penalty level to use
(<code>"min"</code>, <code>"se"</code>, <code style="white-space: pre;">&#8288;"{x}-se&#8288;</code>&quot;)
or a single numeric value of the penalty parameter. See details.</p>
</td></tr>
<tr><td><code id="summary.pense_cvfit_+3A_se_mult">se_mult</code></td>
<td>
<p>If <code>lambda = "se"</code>, the multiple of standard errors to tolerate.</p>
</td></tr>
<tr><td><code id="summary.pense_cvfit_+3A_...">...</code></td>
<td>
<p>ignored.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+prediction_performance">prediction_performance()</a></code> for information about the estimated prediction performance.
</p>
<p><code><a href="#topic+coef.pense_cvfit">coef.pense_cvfit()</a></code> for extracting only the estimated coefficients.
</p>
<p>Other functions for plotting and printing: 
<code><a href="#topic+plot.pense_cvfit">plot.pense_cvfit</a>()</code>,
<code><a href="#topic+plot.pense_fit">plot.pense_fit</a>()</code>,
<code><a href="#topic+prediction_performance">prediction_performance</a>()</code>
</p>

<hr>
<h2 id='tau_size'>Compute the Tau-Scale of Centered Values</h2><span id='topic+tau_size'></span>

<h3>Description</h3>

<p>Compute the <code class="reqn">\tau</code>-scale without centering the values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tau_size(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tau_size_+3A_x">x</code></td>
<td>
<p>numeric values. Missing values are verbosely ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the <code class="reqn">\tau</code> estimate of scale of centered values.
</p>


<h3>See Also</h3>

<p>Other functions to compute robust estimates of location and scale: 
<code><a href="#topic+mlocscale">mlocscale</a>()</code>,
<code><a href="#topic+mloc">mloc</a>()</code>,
<code><a href="#topic+mscale">mscale</a>()</code>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
