<!DOCTYPE html><html><head><title>Help for package autostats</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {autostats}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#+25+26gt+3B+25'><p>Pipe operator</p></a></li>
<li><a href='#auto_anova'><p>auto anova</p></a></li>
<li><a href='#auto_boxplot'><p>auto_boxplot</p></a></li>
<li><a href='#auto_cor'><p>auto correlation</p></a></li>
<li><a href='#auto_model_accuracy'><p>auto model accuracy</p></a></li>
<li><a href='#auto_t_test'><p>auto t test</p></a></li>
<li><a href='#auto_tune_xgboost'><p>auto_tune_xgboost</p></a></li>
<li><a href='#auto_variable_contributions'><p>Plot Variable Contributions</p></a></li>
<li><a href='#cap_outliers'><p>cap_outliers</p></a></li>
<li><a href='#create_monotone_constraints'><p>create monotone constraints</p></a></li>
<li><a href='#determine_pred_type'><p>determine pred type</p></a></li>
<li><a href='#eval_preds'><p>eval_preds</p></a></li>
<li><a href='#f_charvec_to_formula'><p>charvec to formula</p></a></li>
<li><a href='#f_formula_to_charvec'><p>Formula_rhs to chr vec</p></a></li>
<li><a href='#f_modify_formula'><p>Modify Formula</p></a></li>
<li><a href='#get_params'><p>get params</p></a></li>
<li><a href='#impute_recosystem'><p>impute_recosystem</p></a></li>
<li><a href='#plot_coefs_glm'><p>plot glm coefs</p></a></li>
<li><a href='#plot_ctree'><p>plot ctree</p></a></li>
<li><a href='#plot_varimp_cforest'><p>plot cforest variable importance</p></a></li>
<li><a href='#plot_varimp_xgboost'><p>Plot varimp xgboost</p></a></li>
<li><a href='#tidy_cforest'><p>tidy conditional inference forest</p></a></li>
<li><a href='#tidy_ctree'><p>tidy ctree</p></a></li>
<li><a href='#tidy_formula'><p>tidy formula construction</p></a></li>
<li><a href='#tidy_glm'><p>tidy glm</p></a></li>
<li><a href='#tidy_predict'><p>tidy predict</p></a></li>
<li><a href='#tidy_shap'><p>tidy shap</p></a></li>
<li><a href='#tidy_xgboost'><p>tidy xgboost</p></a></li>
<li><a href='#visualize_model'><p>visualize model</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Auto Stats</td>
</tr>
<tr>
<td>Version:</td>
<td>0.4.1</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Harrison Tietze &lt;harrison4192@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Automatically do statistical exploration. Create formulas using 'tidyselect' syntax, and then determine cross-validated model accuracy and variable contributions using 'glm' and 'xgboost'. Contains additional helper functions to create and modify formulas. Has a flagship function to quickly determine relationships between categorical and continuous variables in the data set.</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>dplyr, stringr, tidyselect, purrr, janitor, tibble, rlang,
stats, rlist, broom, magrittr, ggeasy, ggplot2, jtools, gtools,
ggthemes, patchwork, tidyr, xgboost, parsnip, recipes, rsample,
tune, workflows, framecleaner, presenter, yardstick, dials,
party, data.table, nnet, recosystem, Ckmeans.1d.dp,
broom.mixed, igraph</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://harrison4192.github.io/autostats/">https://harrison4192.github.io/autostats/</a>,
<a href="https://github.com/Harrison4192/autostats">https://github.com/Harrison4192/autostats</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/Harrison4192/autostats/issues">https://github.com/Harrison4192/autostats/issues</a></td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, forcats, parallel, doParallel, hardhat,
flextable, glmnet, ggstance, Matrix, BBmisc, readr, lubridate,
ranger, XICOR</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-06-03 05:56:13 UTC; harrisontietze</td>
</tr>
<tr>
<td>Author:</td>
<td>Harrison Tietze [aut, cre]</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-06-04 09:44:44 UTC</td>
</tr>
</table>
<hr>
<h2 id='+25+26gt+3B+25'>Pipe operator</h2><span id='topic++25+3E+25'></span>

<h3>Description</h3>

<p>See <code>magrittr::<a href="magrittr.html#topic+pipe">%&gt;%</a></code> for details.
</p>


<h3>Value</h3>

<p>lhs %&gt;% rhs
</p>

<hr>
<h2 id='auto_anova'>auto anova</h2><span id='topic+auto_anova'></span>

<h3>Description</h3>

<p>A wrapper around lm and anova to run a regression of a continuous variable against categorical variables.
Used for determining the whether the mean of a continuous variable is statistically significant amongst different levels
of a categorical variable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>auto_anova(
  data,
  ...,
  baseline = c("mean", "median", "first_level", "user_supplied"),
  user_supplied_baseline = NULL,
  sparse = FALSE,
  pval_thresh = 0.1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="auto_anova_+3A_data">data</code></td>
<td>
<p>a data frame</p>
</td></tr>
<tr><td><code id="auto_anova_+3A_...">...</code></td>
<td>
<p>tidyselect specification or cols</p>
</td></tr>
<tr><td><code id="auto_anova_+3A_baseline">baseline</code></td>
<td>
<p>choose from &quot;mean&quot;, &quot;median&quot;, &quot;first_level&quot;, &quot;user_supplied&quot;. what is the baseline to compare each category to? can use the mean and median of the target variable as a global baseline</p>
</td></tr>
<tr><td><code id="auto_anova_+3A_user_supplied_baseline">user_supplied_baseline</code></td>
<td>
<p>if intercept is &quot;user_supplied&quot;, can enter a numeric value</p>
</td></tr>
<tr><td><code id="auto_anova_+3A_sparse">sparse</code></td>
<td>
<p>default FALSE; if true returns a truncated output with only significant results</p>
</td></tr>
<tr><td><code id="auto_anova_+3A_pval_thresh">pval_thresh</code></td>
<td>
<p>control significance level for sparse output filtering</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Columns can be inputted as unquoted names or tidyselect. Continuous and categorical variables are automatically determined.
If no character or factor column is present, the column with the lowest amount of unique values will be considered
the categorical variable.
</p>
<p>Description of columns in the output
</p>

<dl>
<dt><em>target</em></dt><dd><p> continuous variables</p>
</dd>
<dt><em>predictor</em></dt><dd><p> categorical variables</p>
</dd>
<dt><em>level</em></dt><dd><p> levels in the categorical variables</p>
</dd>
<dt><em>estimate</em></dt><dd><p> difference between level target mean and baseline</p>
</dd>
<dt><em>target_mean</em></dt><dd><p> target mean per level</p>
</dd>
<dt><em>n</em></dt><dd><p> rows in predictor level</p>
</dd>
<dt><em>std.error</em></dt><dd><p> standard error of target in predictor level</p>
</dd>
<dt><em>level_p.value</em></dt><dd><p> p.value for t.test of whether target mean differs significantly between level and baseline</p>
</dd>
<dt><em>level_significance</em></dt><dd><p> level p.value represented by stars</p>
</dd>
<dt><em>predictor_p.value</em></dt><dd><p> p.value for significance of entire predictor given by F test</p>
</dd>
<dt><em>predictor_significance</em></dt><dd><p> predictor p.value represented by stars</p>
</dd>
<dt><em>conclusion</em></dt><dd><p> text interpretation of tests</p>
</dd>
</dl>



<h3>Value</h3>

<p>data frame
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
iris %&gt;%
auto_anova(tidyselect::everything()) -&gt; iris_anova1


iris_anova1 %&gt;%
print(width = Inf)
</code></pre>

<hr>
<h2 id='auto_boxplot'>auto_boxplot</h2><span id='topic+auto_boxplot'></span>

<h3>Description</h3>

<p>Wraps <code><a href="ggplot2.html#topic+geom_boxplot">geom_boxplot</a></code> to simplify creating boxplots.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>auto_boxplot(
  .data,
  continuous_outcome,
  categorical_variable,
  categorical_facets = NULL,
  alpha = 0.3,
  width = 0.15,
  color_dots = "black",
  color_box = "red"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="auto_boxplot_+3A_.data">.data</code></td>
<td>
<p>data</p>
</td></tr>
<tr><td><code id="auto_boxplot_+3A_continuous_outcome">continuous_outcome</code></td>
<td>
<p>continuous y variable. unquoted column name</p>
</td></tr>
<tr><td><code id="auto_boxplot_+3A_categorical_variable">categorical_variable</code></td>
<td>
<p>categorical x variable. unquoted column name</p>
</td></tr>
<tr><td><code id="auto_boxplot_+3A_categorical_facets">categorical_facets</code></td>
<td>
<p>categorical facet variable. unquoted column name</p>
</td></tr>
<tr><td><code id="auto_boxplot_+3A_alpha">alpha</code></td>
<td>
<p>alpha points</p>
</td></tr>
<tr><td><code id="auto_boxplot_+3A_width">width</code></td>
<td>
<p>width of jitter</p>
</td></tr>
<tr><td><code id="auto_boxplot_+3A_color_dots">color_dots</code></td>
<td>
<p>dot color</p>
</td></tr>
<tr><td><code id="auto_boxplot_+3A_color_box">color_box</code></td>
<td>
<p>box color</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
iris %&gt;%
auto_boxplot(continuous_outcome = Petal.Width, categorical_variable = Species)
</code></pre>

<hr>
<h2 id='auto_cor'>auto correlation</h2><span id='topic+auto_cor'></span>

<h3>Description</h3>

<p>Finds the correlation between numeric variables in a data frame, chosen using tidyselect.
Additional parameters for the correlation test can be specified as in <code><a href="stats.html#topic+cor.test">cor.test</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>auto_cor(
  .data,
  ...,
  use = c("pairwise.complete.obs", "all.obs", "complete.obs", "everything",
    "na.or.complete"),
  method = c("pearson", "kendall", "spearman", "xicor"),
  include_nominals = TRUE,
  max_levels = 5L,
  sparse = TRUE,
  pval_thresh = 0.1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="auto_cor_+3A_.data">.data</code></td>
<td>
<p>data frame</p>
</td></tr>
<tr><td><code id="auto_cor_+3A_...">...</code></td>
<td>
<p>tidyselect cols</p>
</td></tr>
<tr><td><code id="auto_cor_+3A_use">use</code></td>
<td>
<p>method to deal with na. Default is to remove rows with NA</p>
</td></tr>
<tr><td><code id="auto_cor_+3A_method">method</code></td>
<td>
<p>correlation method. default is pearson, but also supports xicor.</p>
</td></tr>
<tr><td><code id="auto_cor_+3A_include_nominals">include_nominals</code></td>
<td>
<p>logicals, default TRUE. Dummify nominal variables?</p>
</td></tr>
<tr><td><code id="auto_cor_+3A_max_levels">max_levels</code></td>
<td>
<p>maximum numbers of dummies to be created from nominal variables</p>
</td></tr>
<tr><td><code id="auto_cor_+3A_sparse">sparse</code></td>
<td>
<p>logical, default TRUE. Filters and arranges cor table</p>
</td></tr>
<tr><td><code id="auto_cor_+3A_pval_thresh">pval_thresh</code></td>
<td>
<p>threshold to filter out weak correlations</p>
</td></tr>
</table>


<h3>Details</h3>

<p>includes the asymmetric correlation coefficient xi from <code><a href="XICOR.html#topic+xicor">xicor</a></code>
</p>


<h3>Value</h3>

<p>data frame of correlations
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
iris %&gt;%
auto_cor()

# don't use sparse if you're interested in only one target variable
iris %&gt;%
auto_cor(sparse = FALSE) %&gt;%
dplyr::filter(x == "Petal.Length")
</code></pre>

<hr>
<h2 id='auto_model_accuracy'>auto model accuracy</h2><span id='topic+auto_model_accuracy'></span>

<h3>Description</h3>

<p>Runs a cross validated xgboost and regularized linear regression, and reports accuracy metrics.
Automatically determines whether the provided formula is a regression or classification.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>auto_model_accuracy(
  data,
  formula,
  ...,
  n_folds = 4,
  as_flextable = TRUE,
  include_linear = FALSE,
  theme = "tron",
  seed = 1,
  mtry = 1,
  trees = 15L,
  min_n = 1L,
  tree_depth = 6L,
  learn_rate = 0.3,
  loss_reduction = 0,
  sample_size = 1,
  stop_iter = 10L,
  counts = FALSE,
  penalty = 0.015,
  mixture = 0.35
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="auto_model_accuracy_+3A_data">data</code></td>
<td>
<p>data frame</p>
</td></tr>
<tr><td><code id="auto_model_accuracy_+3A_formula">formula</code></td>
<td>
<p>formula</p>
</td></tr>
<tr><td><code id="auto_model_accuracy_+3A_...">...</code></td>
<td>
<p>any other params for xgboost</p>
</td></tr>
<tr><td><code id="auto_model_accuracy_+3A_n_folds">n_folds</code></td>
<td>
<p>number of cross validation folds</p>
</td></tr>
<tr><td><code id="auto_model_accuracy_+3A_as_flextable">as_flextable</code></td>
<td>
<p>if FALSE, returns a tibble</p>
</td></tr>
<tr><td><code id="auto_model_accuracy_+3A_include_linear">include_linear</code></td>
<td>
<p>if TRUE includes a regularized linear model</p>
</td></tr>
<tr><td><code id="auto_model_accuracy_+3A_theme">theme</code></td>
<td>
<p>make_flextable theme</p>
</td></tr>
<tr><td><code id="auto_model_accuracy_+3A_seed">seed</code></td>
<td>
<p>seed</p>
</td></tr>
<tr><td><code id="auto_model_accuracy_+3A_mtry">mtry</code></td>
<td>
<p># Randomly Selected Predictors; defaults to .75; (xgboost: colsample_bynode) (type: numeric, range 0 - 1) (or type: integer if <code>count = TRUE</code>)</p>
</td></tr>
<tr><td><code id="auto_model_accuracy_+3A_trees">trees</code></td>
<td>
<p># Trees (xgboost: nrounds) (type: integer, default: 500L)</p>
</td></tr>
<tr><td><code id="auto_model_accuracy_+3A_min_n">min_n</code></td>
<td>
<p>Minimal Node Size (xgboost: min_child_weight) (type: integer, default: 2L); [typical range: 2-10] Keep small value for highly imbalanced class data where leaf nodes can have smaller size groups. Otherwise increase size to prevent overfitting outliers.</p>
</td></tr>
<tr><td><code id="auto_model_accuracy_+3A_tree_depth">tree_depth</code></td>
<td>
<p>Tree Depth (xgboost: max_depth) (type: integer, default: 7L); Typical values: 3-10</p>
</td></tr>
<tr><td><code id="auto_model_accuracy_+3A_learn_rate">learn_rate</code></td>
<td>
<p>Learning Rate (xgboost: eta) (type: double, default: 0.05); Typical values: 0.01-0.3</p>
</td></tr>
<tr><td><code id="auto_model_accuracy_+3A_loss_reduction">loss_reduction</code></td>
<td>
<p>Minimum Loss Reduction (xgboost: gamma) (type: double, default: 1.0);  range: 0 to Inf; typical value: 0 - 20 assuming low-mid tree depth</p>
</td></tr>
<tr><td><code id="auto_model_accuracy_+3A_sample_size">sample_size</code></td>
<td>
<p>Proportion Observations Sampled (xgboost: subsample) (type: double, default: .75); Typical values: 0.5 - 1</p>
</td></tr>
<tr><td><code id="auto_model_accuracy_+3A_stop_iter">stop_iter</code></td>
<td>
<p># Iterations Before Stopping (xgboost: early_stop) (type: integer, default: 15L) only enabled if validation set is provided</p>
</td></tr>
<tr><td><code id="auto_model_accuracy_+3A_counts">counts</code></td>
<td>
<p>if <code>TRUE</code> specify <code>mtry</code> as an integer number of cols. Default <code>FALSE</code> to specify <code>mtry</code> as fraction of cols from 0 to 1</p>
</td></tr>
<tr><td><code id="auto_model_accuracy_+3A_penalty">penalty</code></td>
<td>
<p>linear regularization parameter</p>
</td></tr>
<tr><td><code id="auto_model_accuracy_+3A_mixture">mixture</code></td>
<td>
<p>linear model parameter, combines l1 and l2 regularization</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a table
</p>

<hr>
<h2 id='auto_t_test'>auto t test</h2><span id='topic+auto_t_test'></span>

<h3>Description</h3>

<p>Performs a t.test on 2 populations for numeric variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>auto_t_test(data, col, ..., var_equal = FALSE, abbrv = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="auto_t_test_+3A_data">data</code></td>
<td>
<p>dataframe</p>
</td></tr>
<tr><td><code id="auto_t_test_+3A_col">col</code></td>
<td>
<p>a column with 2 categories representing the 2 populations</p>
</td></tr>
<tr><td><code id="auto_t_test_+3A_...">...</code></td>
<td>
<p>numeric variables to perform t.test on. Default is to select all numeric variables</p>
</td></tr>
<tr><td><code id="auto_t_test_+3A_var_equal">var_equal</code></td>
<td>
<p>default FALSE; t.test parameter</p>
</td></tr>
<tr><td><code id="auto_t_test_+3A_abbrv">abbrv</code></td>
<td>
<p>default TRUE; remove some extra columns from output</p>
</td></tr>
</table>


<h3>Value</h3>

<p>dataframe
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

iris %&gt;%
 dplyr::filter(Species != "setosa") %&gt;%
 auto_t_test(col = Species)
</code></pre>

<hr>
<h2 id='auto_tune_xgboost'>auto_tune_xgboost</h2><span id='topic+auto_tune_xgboost'></span>

<h3>Description</h3>

<p>Automatically tunes an xgboost model using grid or bayesian optimization
</p>


<h3>Usage</h3>

<pre><code class='language-R'>auto_tune_xgboost(
  .data,
  formula,
  tune_method = c("grid", "bayes"),
  event_level = c("first", "second"),
  n_fold = 5L,
  n_iter = 100L,
  seed = 1,
  save_output = FALSE,
  parallel = TRUE,
  trees = tune::tune(),
  min_n = tune::tune(),
  mtry = tune::tune(),
  tree_depth = tune::tune(),
  learn_rate = tune::tune(),
  loss_reduction = tune::tune(),
  sample_size = tune::tune(),
  stop_iter = tune::tune(),
  counts = FALSE,
  tree_method = c("auto", "exact", "approx", "hist", "gpu_hist"),
  monotone_constraints = 0L,
  num_parallel_tree = 1L,
  lambda = 1,
  alpha = 0,
  scale_pos_weight = 1,
  verbosity = 0L
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="auto_tune_xgboost_+3A_.data">.data</code></td>
<td>
<p>dataframe</p>
</td></tr>
<tr><td><code id="auto_tune_xgboost_+3A_formula">formula</code></td>
<td>
<p>formula</p>
</td></tr>
<tr><td><code id="auto_tune_xgboost_+3A_tune_method">tune_method</code></td>
<td>
<p>method of tuning. defaults to grid</p>
</td></tr>
<tr><td><code id="auto_tune_xgboost_+3A_event_level">event_level</code></td>
<td>
<p>for binary classification, which factor level is the positive class. specify &quot;second&quot; for second level</p>
</td></tr>
<tr><td><code id="auto_tune_xgboost_+3A_n_fold">n_fold</code></td>
<td>
<p>integer. n folds in resamples</p>
</td></tr>
<tr><td><code id="auto_tune_xgboost_+3A_n_iter">n_iter</code></td>
<td>
<p>n iterations for tuning (bayes); paramter grid size (grid)</p>
</td></tr>
<tr><td><code id="auto_tune_xgboost_+3A_seed">seed</code></td>
<td>
<p>seed</p>
</td></tr>
<tr><td><code id="auto_tune_xgboost_+3A_save_output">save_output</code></td>
<td>
<p>FASLE. If set to TRUE will write the output as an rds file</p>
</td></tr>
<tr><td><code id="auto_tune_xgboost_+3A_parallel">parallel</code></td>
<td>
<p>default TRUE; If set to TRUE, will enable parallel processing on resamples for grid tuning</p>
</td></tr>
<tr><td><code id="auto_tune_xgboost_+3A_trees">trees</code></td>
<td>
<p># Trees (xgboost: nrounds) (type: integer, default: 500L)</p>
</td></tr>
<tr><td><code id="auto_tune_xgboost_+3A_min_n">min_n</code></td>
<td>
<p>Minimal Node Size (xgboost: min_child_weight) (type: integer, default: 2L); [typical range: 2-10] Keep small value for highly imbalanced class data where leaf nodes can have smaller size groups. Otherwise increase size to prevent overfitting outliers.</p>
</td></tr>
<tr><td><code id="auto_tune_xgboost_+3A_mtry">mtry</code></td>
<td>
<p># Randomly Selected Predictors; defaults to .75; (xgboost: colsample_bynode) (type: numeric, range 0 - 1) (or type: integer if <code>count = TRUE</code>)</p>
</td></tr>
<tr><td><code id="auto_tune_xgboost_+3A_tree_depth">tree_depth</code></td>
<td>
<p>Tree Depth (xgboost: max_depth) (type: integer, default: 7L); Typical values: 3-10</p>
</td></tr>
<tr><td><code id="auto_tune_xgboost_+3A_learn_rate">learn_rate</code></td>
<td>
<p>Learning Rate (xgboost: eta) (type: double, default: 0.05); Typical values: 0.01-0.3</p>
</td></tr>
<tr><td><code id="auto_tune_xgboost_+3A_loss_reduction">loss_reduction</code></td>
<td>
<p>Minimum Loss Reduction (xgboost: gamma) (type: double, default: 1.0);  range: 0 to Inf; typical value: 0 - 20 assuming low-mid tree depth</p>
</td></tr>
<tr><td><code id="auto_tune_xgboost_+3A_sample_size">sample_size</code></td>
<td>
<p>Proportion Observations Sampled (xgboost: subsample) (type: double, default: .75); Typical values: 0.5 - 1</p>
</td></tr>
<tr><td><code id="auto_tune_xgboost_+3A_stop_iter">stop_iter</code></td>
<td>
<p># Iterations Before Stopping (xgboost: early_stop) (type: integer, default: 15L) only enabled if validation set is provided</p>
</td></tr>
<tr><td><code id="auto_tune_xgboost_+3A_counts">counts</code></td>
<td>
<p>if <code>TRUE</code> specify <code>mtry</code> as an integer number of cols. Default <code>FALSE</code> to specify <code>mtry</code> as fraction of cols from 0 to 1</p>
</td></tr>
<tr><td><code id="auto_tune_xgboost_+3A_tree_method">tree_method</code></td>
<td>
<p>xgboost tree_method. default is <code>auto</code>. reference: <a href="https://xgboost.readthedocs.io/en/stable/treemethod.html">tree method docs</a></p>
</td></tr>
<tr><td><code id="auto_tune_xgboost_+3A_monotone_constraints">monotone_constraints</code></td>
<td>
<p>an integer vector with length of the predictor cols, of <code>-1, 1, 0</code> corresponding to decreasing, increasing, and no constraint respectively for the index of the predictor col. reference: <a href="https://xgboost.readthedocs.io/en/stable/tutorials/monotonic.html">monotonicity docs</a>.</p>
</td></tr>
<tr><td><code id="auto_tune_xgboost_+3A_num_parallel_tree">num_parallel_tree</code></td>
<td>
<p>should be set to the size of the forest being trained. default 1L</p>
</td></tr>
<tr><td><code id="auto_tune_xgboost_+3A_lambda">lambda</code></td>
<td>
<p>[default=.5] L2 regularization term on weights. Increasing this value will make model more conservative.</p>
</td></tr>
<tr><td><code id="auto_tune_xgboost_+3A_alpha">alpha</code></td>
<td>
<p>[default=.1] L1 regularization term on weights. Increasing this value will make model more conservative.</p>
</td></tr>
<tr><td><code id="auto_tune_xgboost_+3A_scale_pos_weight">scale_pos_weight</code></td>
<td>
<p>[default=1] Control the balance of positive and negative weights, useful for unbalanced classes. if set to TRUE, calculates sum(negative instances) / sum(positive instances). If first level is majority class, use values &lt; 1, otherwise normally values &gt;1 are used to balance the class distribution.</p>
</td></tr>
<tr><td><code id="auto_tune_xgboost_+3A_verbosity">verbosity</code></td>
<td>
<p>[default=1] Verbosity of printing messages. Valid values are 0 (silent), 1 (warning), 2 (info), 3 (debug).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Default is to tune all 7 xgboost parameters. Individual parameter values can be optionally fixed to reduce tuning complexity.
</p>


<h3>Value</h3>

<p>workflow object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>



iris %&gt;%
 framecleaner::create_dummies() -&gt; iris1

iris1 %&gt;%
 tidy_formula(target = Petal.Length) -&gt; petal_form

iris1 %&gt;%
 rsample::initial_split() -&gt; iris_split

iris_split %&gt;%
 rsample::analysis() -&gt; iris_train

iris_split %&gt;%
 rsample::assessment() -&gt; iris_val

## Not run: 
iris_train %&gt;%
 auto_tune_xgboost(formula = petal_form, n_iter = 10,
 parallel = FALSE, tune_method = "grid", mtry = .5) -&gt; xgb_tuned

xgb_tuned %&gt;%
 parsnip::fit(iris_train) %&gt;%
 parsnip::extract_fit_engine() -&gt; xgb_tuned_fit

xgb_tuned_fit %&gt;%
 tidy_predict(newdata = iris_val, form = petal_form) -&gt; iris_val1

## End(Not run)


</code></pre>

<hr>
<h2 id='auto_variable_contributions'>Plot Variable Contributions</h2><span id='topic+auto_variable_contributions'></span>

<h3>Description</h3>

<p>Return a variable importance plot and coefficient plot from a linear model. Used to easily visualize
the contributions of explanatory variables in a supervised model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>auto_variable_contributions(data, formula, scale = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="auto_variable_contributions_+3A_data">data</code></td>
<td>
<p>dataframe</p>
</td></tr>
<tr><td><code id="auto_variable_contributions_+3A_formula">formula</code></td>
<td>
<p>formula</p>
</td></tr>
<tr><td><code id="auto_variable_contributions_+3A_scale">scale</code></td>
<td>
<p>logical. If FALSE puts coefficients on original scale</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a ggplot object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
iris %&gt;%
framecleaner::create_dummies() %&gt;%
auto_variable_contributions(
 tidy_formula(., target = Petal.Width)
 )

iris %&gt;%
auto_variable_contributions(
tidy_formula(., target = Species)
)
</code></pre>

<hr>
<h2 id='cap_outliers'>cap_outliers</h2><span id='topic+cap_outliers'></span>

<h3>Description</h3>

<p>Caps the outliers of a numeric vector by percentiles. Also outputs a plot of the capped distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cap_outliers(x, q = 0.05, type = c("both", "upper", "lower"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cap_outliers_+3A_x">x</code></td>
<td>
<p>numeric vector</p>
</td></tr>
<tr><td><code id="cap_outliers_+3A_q">q</code></td>
<td>
<p>decimal input to the quantile function to set cap. default .05 caps at the 95 and 5th percentile</p>
</td></tr>
<tr><td><code id="cap_outliers_+3A_type">type</code></td>
<td>
<p>chr vector. where to cap: both, upper, or lower</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric vector
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
cap_outliers(iris$Petal.Width)

</code></pre>

<hr>
<h2 id='create_monotone_constraints'>create monotone constraints</h2><span id='topic+create_monotone_constraints'></span>

<h3>Description</h3>

<p>helper function to create the integer vector to pass to the <code>monotone_constraints</code> argument in xgboost
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_monotone_constraints(
  .data,
  formula,
  decreasing = NULL,
  increasing = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_monotone_constraints_+3A_.data">.data</code></td>
<td>
<p>dataframe, training data for tidy_xgboost</p>
</td></tr>
<tr><td><code id="create_monotone_constraints_+3A_formula">formula</code></td>
<td>
<p>formula used for tidy_xgboost</p>
</td></tr>
<tr><td><code id="create_monotone_constraints_+3A_decreasing">decreasing</code></td>
<td>
<p>character vector or tidyselect regular expression to designate decreasing cols</p>
</td></tr>
<tr><td><code id="create_monotone_constraints_+3A_increasing">increasing</code></td>
<td>
<p>character vector or tidyselect regular expression to designate increasing cols</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named integer vector with entries of 0, 1, -1
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


iris %&gt;%
framecleaner::create_dummies(Species) -&gt; iris_dummy

iris_dummy %&gt;%
 tidy_formula(target= Petal.Length) -&gt; petal_form

iris_dummy %&gt;%
 create_monotone_constraints(petal_form,
                             decreasing = tidyselect::matches("Petal|Species"),
                             increasing = "Sepal.Width")

</code></pre>

<hr>
<h2 id='determine_pred_type'>determine pred type</h2><span id='topic+determine_pred_type'></span>

<h3>Description</h3>

<p>determine pred type
</p>


<h3>Usage</h3>

<pre><code class='language-R'>determine_pred_type(x, original_col)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="determine_pred_type_+3A_x">x</code></td>
<td>
<p>a vector</p>
</td></tr>
<tr><td><code id="determine_pred_type_+3A_original_col">original_col</code></td>
<td>
<p>logical. is numeric</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a string
</p>

<hr>
<h2 id='eval_preds'>eval_preds</h2><span id='topic+eval_preds'></span>

<h3>Description</h3>

<p>Automatically evaluates predictions created by <code><a href="#topic+tidy_predict">tidy_predict</a></code>. No need to supply column names.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>eval_preds(.data, ..., softprob_model = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="eval_preds_+3A_.data">.data</code></td>
<td>
<p>dataframe as a result of <code><a href="#topic+tidy_predict">tidy_predict</a></code></p>
</td></tr>
<tr><td><code id="eval_preds_+3A_...">...</code></td>
<td>
<p>additional metrics from <a href="https://yardstick.tidymodels.org/articles/metric-types.html">yarstick</a> to be calculated</p>
</td></tr>
<tr><td><code id="eval_preds_+3A_softprob_model">softprob_model</code></td>
<td>
<p>character name of the model used to create multiclass probabilities</p>
</td></tr>
</table>


<h3>Value</h3>

<p>tibble of summarized metrics
</p>

<hr>
<h2 id='f_charvec_to_formula'>charvec to formula</h2><span id='topic+f_charvec_to_formula'></span>

<h3>Description</h3>

<p>takes the lhs and rhs of a formula as character vectors and outputs a formula
</p>


<h3>Usage</h3>

<pre><code class='language-R'>f_charvec_to_formula(lhs, rhs)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="f_charvec_to_formula_+3A_lhs">lhs</code></td>
<td>
<p>lhs atomic chr vec</p>
</td></tr>
<tr><td><code id="f_charvec_to_formula_+3A_rhs">rhs</code></td>
<td>
<p>rhs chr vec</p>
</td></tr>
</table>


<h3>Value</h3>

<p>formula
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
lhs &lt;- "Species"
rhs &lt;- c("Petal.Width", "Custom_Var")

f_charvec_to_formula(lhs, rhs)

</code></pre>

<hr>
<h2 id='f_formula_to_charvec'>Formula_rhs to chr vec</h2><span id='topic+f_formula_to_charvec'></span>

<h3>Description</h3>

<p>Accepts a formula and returns the rhs as a character vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>f_formula_to_charvec(f, include_lhs = FALSE, .data = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="f_formula_to_charvec_+3A_f">f</code></td>
<td>
<p>formula</p>
</td></tr>
<tr><td><code id="f_formula_to_charvec_+3A_include_lhs">include_lhs</code></td>
<td>
<p>FALSE. If TRUE, appends lhs to beginning of vector</p>
</td></tr>
<tr><td><code id="f_formula_to_charvec_+3A_.data">.data</code></td>
<td>
<p>dataframe for names if necessary</p>
</td></tr>
</table>


<h3>Value</h3>

<p>chr vector
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
iris %&gt;%
tidy_formula(target = Species, tidyselect::everything()) -&gt; f

f

f %&gt;%
f_formula_to_charvec()
</code></pre>

<hr>
<h2 id='f_modify_formula'>Modify Formula</h2><span id='topic+f_modify_formula'></span>

<h3>Description</h3>

<p>Modify components of a formula by adding / removing vars from the rhs or replacing the lhs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>f_modify_formula(
  f,
  rhs_remove = NULL,
  rhs_add = NULL,
  lhs_replace = NULL,
  negate = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="f_modify_formula_+3A_f">f</code></td>
<td>
<p>formula</p>
</td></tr>
<tr><td><code id="f_modify_formula_+3A_rhs_remove">rhs_remove</code></td>
<td>
<p>regex or character vector for dropping variables from the rhs</p>
</td></tr>
<tr><td><code id="f_modify_formula_+3A_rhs_add">rhs_add</code></td>
<td>
<p>character vector for adding variables to rhs</p>
</td></tr>
<tr><td><code id="f_modify_formula_+3A_lhs_replace">lhs_replace</code></td>
<td>
<p>string to replace formula lhs if supplied</p>
</td></tr>
<tr><td><code id="f_modify_formula_+3A_negate">negate</code></td>
<td>
<p>should <code>rhs_remove</code> keep or remove the specified vars. Set to <code>FALSE</code> to keep</p>
</td></tr>
</table>


<h3>Value</h3>

<p>formula
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
iris %&gt;%
tidy_formula(target = Species, tidyselect::everything()) -&gt; f

f

f %&gt;%
  f_modify_formula(
rhs_remove = c("Petal.Width", "Sepal.Length"),
rhs_add = "Custom_Variable"
)

f %&gt;%
  f_modify_formula(
rhs_remove = "Petal",
lhs_replace = "Petal.Length"
)
</code></pre>

<hr>
<h2 id='get_params'>get params</h2><span id='topic+get_params'></span><span id='topic+get_params.xgb.Booster'></span><span id='topic+get_params.workflow'></span>

<h3>Description</h3>

<p>s3 method to extract params of a model with names consistent for use in the 'autostats' package
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_params(model, ...)

## S3 method for class 'xgb.Booster'
get_params(model, ...)

## S3 method for class 'workflow'
get_params(model, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_params_+3A_model">model</code></td>
<td>
<p>a model</p>
</td></tr>
<tr><td><code id="get_params_+3A_...">...</code></td>
<td>
<p>additional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of params
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
iris %&gt;%
  framecleaner::create_dummies() -&gt; iris_dummies

iris_dummies %&gt;%
  tidy_formula(target = Petal.Length) -&gt; p_form

iris_dummies %&gt;%
  tidy_xgboost(p_form, mtry = .5, trees = 5L, loss_reduction = 2, sample_size = .7) -&gt; xgb

## reuse these parameters to find the cross validated error

rlang::exec(auto_model_accuracy, data = iris_dummies, formula = p_form, !!!get_params(xgb))
</code></pre>

<hr>
<h2 id='impute_recosystem'>impute_recosystem</h2><span id='topic+impute_recosystem'></span>

<h3>Description</h3>

<p>Imputes missing values of a numeric matrix using stochastic gradient descent.
<a href="https://CRAN.r-project.org/package=recosystem">recosystem</a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>impute_recosystem(
  .data,
  lrate = c(0.05, 0.1),
  costp_l1 = c(0, 0.05),
  costq_l1 = c(0, 0.05),
  costp_l2 = c(0, 0.05),
  costq_l2 = c(0, 0.05),
  nthread = 8,
  loss = "l2",
  niter = 15,
  verbose = FALSE,
  nfold = 4,
  seed = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="impute_recosystem_+3A_.data">.data</code></td>
<td>
<p>long format data frame</p>
</td></tr>
<tr><td><code id="impute_recosystem_+3A_lrate">lrate</code></td>
<td>
<p>learning rate</p>
</td></tr>
<tr><td><code id="impute_recosystem_+3A_costp_l1">costp_l1</code></td>
<td>
<p>l1 cost p</p>
</td></tr>
<tr><td><code id="impute_recosystem_+3A_costq_l1">costq_l1</code></td>
<td>
<p>l1 cost q</p>
</td></tr>
<tr><td><code id="impute_recosystem_+3A_costp_l2">costp_l2</code></td>
<td>
<p>l2 cost p</p>
</td></tr>
<tr><td><code id="impute_recosystem_+3A_costq_l2">costq_l2</code></td>
<td>
<p>l2 cost q</p>
</td></tr>
<tr><td><code id="impute_recosystem_+3A_nthread">nthread</code></td>
<td>
<p>nthreads</p>
</td></tr>
<tr><td><code id="impute_recosystem_+3A_loss">loss</code></td>
<td>
<p>loss function. also can use &quot;l1&quot;</p>
</td></tr>
<tr><td><code id="impute_recosystem_+3A_niter">niter</code></td>
<td>
<p>training iterations for tune</p>
</td></tr>
<tr><td><code id="impute_recosystem_+3A_verbose">verbose</code></td>
<td>
<p>show training loss?</p>
</td></tr>
<tr><td><code id="impute_recosystem_+3A_nfold">nfold</code></td>
<td>
<p>folds for tune validation</p>
</td></tr>
<tr><td><code id="impute_recosystem_+3A_seed">seed</code></td>
<td>
<p>seed for randomness</p>
</td></tr>
</table>


<h3>Details</h3>

<p>input is a long data frame with 3 columns: ID col, Item col (the column names from pivoting longer),
and the ratings (values from pivoting longer)
</p>
<p>pre-processing generally requires pivoting a wide user x item matrix to long format.
The missing values from the matrix must be retained as NA values in the rating column.
The values will be predicted and filled in by the algorithm.
Output is a long data frame with the same number of rows as input, but no missing values.
</p>
<p>This function automatically tunes the recosystem learner before applying. Parameter values can be supplied for tuning.
To avoid tuning, use single values for the parameters.
</p>


<h3>Value</h3>

<p>long format data frame
</p>

<hr>
<h2 id='plot_coefs_glm'>plot glm coefs</h2><span id='topic+plot_coefs_glm'></span>

<h3>Description</h3>

<p>plot glm coefs
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_coefs_glm(glm, font = c("", "HiraKakuProN-W3"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_coefs_glm_+3A_glm">glm</code></td>
<td>
<p>glm</p>
</td></tr>
<tr><td><code id="plot_coefs_glm_+3A_font">font</code></td>
<td>
<p>font</p>
</td></tr>
</table>


<h3>Value</h3>

<p>plot
</p>

<hr>
<h2 id='plot_ctree'>plot ctree</h2><span id='topic+plot_ctree'></span>

<h3>Description</h3>

<p>plot ctree
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_ctree(ctree_obj, plot_type = c("sample", "box", "bar"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_ctree_+3A_ctree_obj">ctree_obj</code></td>
<td>
<p>output of tidy_ctree</p>
</td></tr>
<tr><td><code id="plot_ctree_+3A_plot_type">plot_type</code></td>
<td>
<p>type of plot</p>
</td></tr>
</table>


<h3>Value</h3>

<p>decision tree plot
</p>

<hr>
<h2 id='plot_varimp_cforest'>plot cforest variable importance</h2><span id='topic+plot_varimp_cforest'></span>

<h3>Description</h3>

<p>plot cforest variable importance
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_varimp_cforest(cfar, font = c("", "HiraKakuProN-W3"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_varimp_cforest_+3A_cfar">cfar</code></td>
<td>
<p>cforest model</p>
</td></tr>
<tr><td><code id="plot_varimp_cforest_+3A_font">font</code></td>
<td>
<p>font</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot
</p>

<hr>
<h2 id='plot_varimp_xgboost'>Plot varimp xgboost</h2><span id='topic+plot_varimp_xgboost'></span>

<h3>Description</h3>

<p>recommended parameters to control;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_varimp_xgboost(
  xgb,
  top_n = 10L,
  aggregate = NULL,
  as_table = FALSE,
  formula = NULL,
  measure = c("Gain", "Cover", "Frequency"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_varimp_xgboost_+3A_xgb">xgb</code></td>
<td>
<p>xgb.Booster model</p>
</td></tr>
<tr><td><code id="plot_varimp_xgboost_+3A_top_n">top_n</code></td>
<td>
<p>top n important variables</p>
</td></tr>
<tr><td><code id="plot_varimp_xgboost_+3A_aggregate">aggregate</code></td>
<td>
<p>a character vector. Predictors containing the string will be aggregated, and renamed to that string.</p>
</td></tr>
<tr><td><code id="plot_varimp_xgboost_+3A_as_table">as_table</code></td>
<td>
<p>logical, default FALSE. If TRUE returns importances in a data frame</p>
</td></tr>
<tr><td><code id="plot_varimp_xgboost_+3A_formula">formula</code></td>
<td>
<p>formula for the model. Use to provide original names if xgboost is scrambling the names internally</p>
</td></tr>
<tr><td><code id="plot_varimp_xgboost_+3A_measure">measure</code></td>
<td>
<p>choose between Gain, Cover, or Frequency for xgboost importance measure</p>
</td></tr>
<tr><td><code id="plot_varimp_xgboost_+3A_...">...</code></td>
<td>
<p>additional arguments for <code><a href="xgboost.html#topic+xgb.ggplot.importance">xgb.ggplot.importance</a></code></p>
</td></tr>
</table>


<h3>Details</h3>


<dl>
<dt><code>top_n</code></dt><dd><p> number of features to include in the graph</p>
</dd>
</dl>



<h3>Value</h3>

<p>ggplot
</p>

<hr>
<h2 id='tidy_cforest'>tidy conditional inference forest</h2><span id='topic+tidy_cforest'></span>

<h3>Description</h3>

<p>Runs a conditional inference forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tidy_cforest(data, formula, seed = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tidy_cforest_+3A_data">data</code></td>
<td>
<p>dataframe</p>
</td></tr>
<tr><td><code id="tidy_cforest_+3A_formula">formula</code></td>
<td>
<p>formula</p>
</td></tr>
<tr><td><code id="tidy_cforest_+3A_seed">seed</code></td>
<td>
<p>seed integer</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a cforest model
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
iris %&gt;%
tidy_cforest(
  tidy_formula(., Petal.Width)
) -&gt; iris_cfor

iris_cfor

iris_cfor %&gt;%
visualize_model()
</code></pre>

<hr>
<h2 id='tidy_ctree'>tidy ctree</h2><span id='topic+tidy_ctree'></span>

<h3>Description</h3>

<p>tidy conditional inference tree. Creates easily interpretable decision tree models that be shown with the <code><a href="#topic+visualize_model">visualize_model</a></code> function.
Statistical significance required for a split , and minimum necessary samples in a terminal leaf can be controlled to create the desired tree visual.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tidy_ctree(.data, formula, minbucket = 7L, mincriterion = 0.95, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tidy_ctree_+3A_.data">.data</code></td>
<td>
<p>dataframe</p>
</td></tr>
<tr><td><code id="tidy_ctree_+3A_formula">formula</code></td>
<td>
<p>formula</p>
</td></tr>
<tr><td><code id="tidy_ctree_+3A_minbucket">minbucket</code></td>
<td>
<p>minimum amount of samples in terminal leaves, default is 7</p>
</td></tr>
<tr><td><code id="tidy_ctree_+3A_mincriterion">mincriterion</code></td>
<td>
<p>(1 - alpha)   value between 0 -1, default is .95. lowering this value creates more splits, but less significant</p>
</td></tr>
<tr><td><code id="tidy_ctree_+3A_...">...</code></td>
<td>
<p>optional parameters to <code><a href="party.html#topic+ctree_control">ctree_control</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>a ctree object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
iris %&gt;%
tidy_formula(., Sepal.Length) -&gt; sepal_form

iris %&gt;%
tidy_ctree(sepal_form) %&gt;%
visualize_model()

iris %&gt;%
tidy_ctree(sepal_form, minbucket = 30) %&gt;%
visualize_model(plot_type = "box")


</code></pre>

<hr>
<h2 id='tidy_formula'>tidy formula construction</h2><span id='topic+tidy_formula'></span>

<h3>Description</h3>

<p>Takes a dataframe and allows for use of tidyselect to construct a formula.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tidy_formula(data, target, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tidy_formula_+3A_data">data</code></td>
<td>
<p>dataframe</p>
</td></tr>
<tr><td><code id="tidy_formula_+3A_target">target</code></td>
<td>
<p>lhs</p>
</td></tr>
<tr><td><code id="tidy_formula_+3A_...">...</code></td>
<td>
<p>tidyselect. rhs</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a formula
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
iris %&gt;%
tidy_formula(Species, tidyselect::everything())
</code></pre>

<hr>
<h2 id='tidy_glm'>tidy glm</h2><span id='topic+tidy_glm'></span>

<h3>Description</h3>

<p>Runs either a linear regression, logistic regression, or multinomial classification. The model is
automatically determined based off the nature of the target variable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tidy_glm(data, formula)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tidy_glm_+3A_data">data</code></td>
<td>
<p>dataframe</p>
</td></tr>
<tr><td><code id="tidy_glm_+3A_formula">formula</code></td>
<td>
<p>formula</p>
</td></tr>
</table>


<h3>Value</h3>

<p>glm model
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# linear regression
iris %&gt;%
tidy_glm(
tidy_formula(., target = Petal.Width)) -&gt; glm1

glm1

glm1 %&gt;%
visualize_model()

# multinomial classification

tidy_formula(iris, target = Species) -&gt; species_form

iris %&gt;%
tidy_glm(species_form) -&gt; glm2


glm2 %&gt;%
visualize_model()

#  logistic regression
iris %&gt;%
dplyr::filter(Species != "setosa") %&gt;%
tidy_glm(species_form) -&gt; glm3

suppressWarnings({
glm3 %&gt;%
visualize_model()})
</code></pre>

<hr>
<h2 id='tidy_predict'>tidy predict</h2><span id='topic+tidy_predict'></span><span id='topic+tidy_predict.Rcpp_ENSEMBLE'></span><span id='topic+tidy_predict.glm'></span><span id='topic+tidy_predict.default'></span><span id='topic+tidy_predict.BinaryTree'></span><span id='topic+tidy_predict.xgb.Booster'></span><span id='topic+tidy_predict.lgb.Booster'></span>

<h3>Description</h3>

<p>tidy predict
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tidy_predict(
  model,
  newdata,
  form = NULL,
  olddata = NULL,
  bind_preds = FALSE,
  ...
)

## S3 method for class 'Rcpp_ENSEMBLE'
tidy_predict(model, newdata, form = NULL, ...)

## S3 method for class 'glm'
tidy_predict(model, newdata, form = NULL, ...)

## Default S3 method:
tidy_predict(model, newdata, form = NULL, ...)

## S3 method for class 'BinaryTree'
tidy_predict(model, newdata, form = NULL, ...)

## S3 method for class 'xgb.Booster'
tidy_predict(
  model,
  newdata,
  form = NULL,
  olddata = NULL,
  bind_preds = FALSE,
  ...
)

## S3 method for class 'lgb.Booster'
tidy_predict(
  model,
  newdata,
  form = NULL,
  olddata = NULL,
  bind_preds = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tidy_predict_+3A_model">model</code></td>
<td>
<p>model</p>
</td></tr>
<tr><td><code id="tidy_predict_+3A_newdata">newdata</code></td>
<td>
<p>dataframe</p>
</td></tr>
<tr><td><code id="tidy_predict_+3A_form">form</code></td>
<td>
<p>the formula used for the model</p>
</td></tr>
<tr><td><code id="tidy_predict_+3A_olddata">olddata</code></td>
<td>
<p>training data set</p>
</td></tr>
<tr><td><code id="tidy_predict_+3A_bind_preds">bind_preds</code></td>
<td>
<p>set to TURE if newdata is a dataset without any labels, to bind the new and old data with the predictions under the original target name</p>
</td></tr>
<tr><td><code id="tidy_predict_+3A_...">...</code></td>
<td>
<p>other parameters to pass to <code>predict</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>dataframe
</p>


<h3>Examples</h3>

<pre><code class='language-R'>iris %&gt;%
 framecleaner::create_dummies(Species) -&gt; iris_dummy

iris_dummy %&gt;%
 tidy_formula(target= Petal.Length) -&gt; petal_form

iris_dummy %&gt;%
 tidy_xgboost(
   petal_form,
   trees = 20,
   mtry = .5
 )  -&gt; xg1


xg1 %&gt;%
 tidy_predict(newdata = iris_dummy, form = petal_form) %&gt;%
 head()

</code></pre>

<hr>
<h2 id='tidy_shap'>tidy shap</h2><span id='topic+tidy_shap'></span>

<h3>Description</h3>

<p>plot and summarize shapley values from an xgboost model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tidy_shap(model, newdata, form = NULL, ..., top_n = 12, aggregate = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tidy_shap_+3A_model">model</code></td>
<td>
<p>xgboost model</p>
</td></tr>
<tr><td><code id="tidy_shap_+3A_newdata">newdata</code></td>
<td>
<p>dataframe similar to model input</p>
</td></tr>
<tr><td><code id="tidy_shap_+3A_form">form</code></td>
<td>
<p>formula used for model</p>
</td></tr>
<tr><td><code id="tidy_shap_+3A_...">...</code></td>
<td>
<p>additional parameters for shapley value</p>
</td></tr>
<tr><td><code id="tidy_shap_+3A_top_n">top_n</code></td>
<td>
<p>top n features</p>
</td></tr>
<tr><td><code id="tidy_shap_+3A_aggregate">aggregate</code></td>
<td>
<p>a character vector. Predictors containing the string will be aggregated, and renamed to that string.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>returns a list with the following entries
</p>

<dl>
<dt><em>shap_tbl</em></dt><dd><p>: table of shaply values</p>
</dd>
<dt><em>shap_summary</em></dt><dd><p>: table summarizing shapley values. Includes correlation between shaps and feature values.</p>
</dd>
<dt><em>swarmplot</em></dt><dd><p>: one plot showing the relation between shaps and features</p>
</dd>
<dt><em>scatterplots</em></dt><dd><p>: returns the top 9 most important features as determined by sum of absolute shapley values, as a facetted scatterplot of feature vs shap</p>
</dd>
</dl>



<h3>Value</h3>

<p>list
</p>

<hr>
<h2 id='tidy_xgboost'>tidy xgboost</h2><span id='topic+tidy_xgboost'></span>

<h3>Description</h3>

<p>Accepts a formula to run an xgboost model. Automatically determines whether the formula is
for classification or regression. Returns the xgboost model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tidy_xgboost(
  .data,
  formula,
  ...,
  mtry = 0.75,
  trees = 500L,
  min_n = 2L,
  tree_depth = 7L,
  learn_rate = 0.05,
  loss_reduction = 1,
  sample_size = 0.75,
  stop_iter = 15L,
  counts = FALSE,
  tree_method = c("auto", "exact", "approx", "hist", "gpu_hist"),
  monotone_constraints = 0L,
  num_parallel_tree = 1L,
  lambda = 0.5,
  alpha = 0.1,
  scale_pos_weight = 1,
  verbosity = 0L,
  validate = TRUE,
  booster = c("gbtree", "gblinear")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tidy_xgboost_+3A_.data">.data</code></td>
<td>
<p>dataframe</p>
</td></tr>
<tr><td><code id="tidy_xgboost_+3A_formula">formula</code></td>
<td>
<p>formula</p>
</td></tr>
<tr><td><code id="tidy_xgboost_+3A_...">...</code></td>
<td>
<p>additional parameters to be passed to  <code><a href="parsnip.html#topic+set_engine">set_engine</a></code></p>
</td></tr>
<tr><td><code id="tidy_xgboost_+3A_mtry">mtry</code></td>
<td>
<p># Randomly Selected Predictors; defaults to .75; (xgboost: colsample_bynode) (type: numeric, range 0 - 1) (or type: integer if <code>count = TRUE</code>)</p>
</td></tr>
<tr><td><code id="tidy_xgboost_+3A_trees">trees</code></td>
<td>
<p># Trees (xgboost: nrounds) (type: integer, default: 500L)</p>
</td></tr>
<tr><td><code id="tidy_xgboost_+3A_min_n">min_n</code></td>
<td>
<p>Minimal Node Size (xgboost: min_child_weight) (type: integer, default: 2L); [typical range: 2-10] Keep small value for highly imbalanced class data where leaf nodes can have smaller size groups. Otherwise increase size to prevent overfitting outliers.</p>
</td></tr>
<tr><td><code id="tidy_xgboost_+3A_tree_depth">tree_depth</code></td>
<td>
<p>Tree Depth (xgboost: max_depth) (type: integer, default: 7L); Typical values: 3-10</p>
</td></tr>
<tr><td><code id="tidy_xgboost_+3A_learn_rate">learn_rate</code></td>
<td>
<p>Learning Rate (xgboost: eta) (type: double, default: 0.05); Typical values: 0.01-0.3</p>
</td></tr>
<tr><td><code id="tidy_xgboost_+3A_loss_reduction">loss_reduction</code></td>
<td>
<p>Minimum Loss Reduction (xgboost: gamma) (type: double, default: 1.0);  range: 0 to Inf; typical value: 0 - 20 assuming low-mid tree depth</p>
</td></tr>
<tr><td><code id="tidy_xgboost_+3A_sample_size">sample_size</code></td>
<td>
<p>Proportion Observations Sampled (xgboost: subsample) (type: double, default: .75); Typical values: 0.5 - 1</p>
</td></tr>
<tr><td><code id="tidy_xgboost_+3A_stop_iter">stop_iter</code></td>
<td>
<p># Iterations Before Stopping (xgboost: early_stop) (type: integer, default: 15L) only enabled if validation set is provided</p>
</td></tr>
<tr><td><code id="tidy_xgboost_+3A_counts">counts</code></td>
<td>
<p>if <code>TRUE</code> specify <code>mtry</code> as an integer number of cols. Default <code>FALSE</code> to specify <code>mtry</code> as fraction of cols from 0 to 1</p>
</td></tr>
<tr><td><code id="tidy_xgboost_+3A_tree_method">tree_method</code></td>
<td>
<p>xgboost tree_method. default is <code>auto</code>. reference: <a href="https://xgboost.readthedocs.io/en/stable/treemethod.html">tree method docs</a></p>
</td></tr>
<tr><td><code id="tidy_xgboost_+3A_monotone_constraints">monotone_constraints</code></td>
<td>
<p>an integer vector with length of the predictor cols, of <code>-1, 1, 0</code> corresponding to decreasing, increasing, and no constraint respectively for the index of the predictor col. reference: <a href="https://xgboost.readthedocs.io/en/stable/tutorials/monotonic.html">monotonicity docs</a>.</p>
</td></tr>
<tr><td><code id="tidy_xgboost_+3A_num_parallel_tree">num_parallel_tree</code></td>
<td>
<p>should be set to the size of the forest being trained. default 1L</p>
</td></tr>
<tr><td><code id="tidy_xgboost_+3A_lambda">lambda</code></td>
<td>
<p>[default=.5] L2 regularization term on weights. Increasing this value will make model more conservative.</p>
</td></tr>
<tr><td><code id="tidy_xgboost_+3A_alpha">alpha</code></td>
<td>
<p>[default=.1] L1 regularization term on weights. Increasing this value will make model more conservative.</p>
</td></tr>
<tr><td><code id="tidy_xgboost_+3A_scale_pos_weight">scale_pos_weight</code></td>
<td>
<p>[default=1] Control the balance of positive and negative weights, useful for unbalanced classes. if set to TRUE, calculates sum(negative instances) / sum(positive instances). If first level is majority class, use values &lt; 1, otherwise normally values &gt;1 are used to balance the class distribution.</p>
</td></tr>
<tr><td><code id="tidy_xgboost_+3A_verbosity">verbosity</code></td>
<td>
<p>[default=1] Verbosity of printing messages. Valid values are 0 (silent), 1 (warning), 2 (info), 3 (debug).</p>
</td></tr>
<tr><td><code id="tidy_xgboost_+3A_validate">validate</code></td>
<td>
<p>default TRUE. report accuracy metrics on a validation set.</p>
</td></tr>
<tr><td><code id="tidy_xgboost_+3A_booster">booster</code></td>
<td>
<p>defaults to 'gbtree' for tree boosting but can be set to 'gblinear'</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In binary classification the target variable must be a factor with the first level set to the event of interest.
A higher probability will predict the first level.
</p>
<p>reference for parameters: <a href="https://xgboost.readthedocs.io/en/stable/parameter.html">xgboost docs</a>
</p>


<h3>Value</h3>

<p>xgb.Booster model
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
options(rlang_trace_top_env = rlang::current_env())


# regression on numeric variable

iris %&gt;%
 framecleaner::create_dummies(Species) -&gt; iris_dummy

iris_dummy %&gt;%
 tidy_formula(target= Petal.Length) -&gt; petal_form

iris_dummy %&gt;%
 tidy_xgboost(
   petal_form,
   trees = 20,
   mtry = .5
 )  -&gt; xg1


xg1 %&gt;%
 tidy_predict(newdata = iris_dummy, form = petal_form) -&gt; iris_preds

iris_preds %&gt;%
 eval_preds()




</code></pre>

<hr>
<h2 id='visualize_model'>visualize model</h2><span id='topic+visualize_model'></span><span id='topic+visualize_model.RandomForest'></span><span id='topic+visualize_model.BinaryTree'></span><span id='topic+visualize_model.glm'></span><span id='topic+visualize_model.multinom'></span><span id='topic+visualize_model.xgb.Booster'></span><span id='topic+visualize_model.default'></span>

<h3>Description</h3>

<p>s3 method to automatically visualize the output of of a model object. Additional arguments can be supplied for the original function.
Check the corresponding plot function documentation for any custom arguments.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>visualize_model(model, ...)

## S3 method for class 'RandomForest'
visualize_model(model, ..., method)

## S3 method for class 'BinaryTree'
visualize_model(model, ..., method)

## S3 method for class 'glm'
visualize_model(model, ..., method)

## S3 method for class 'multinom'
visualize_model(model, ..., method)

## S3 method for class 'xgb.Booster'
visualize_model(
  model,
  top_n = 10L,
  aggregate = NULL,
  as_table = FALSE,
  formula = NULL,
  measure = c("Gain", "Cover", "Frequency"),
  ...,
  method
)

## Default S3 method:
visualize_model(model, ..., method)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="visualize_model_+3A_model">model</code></td>
<td>
<p>a model</p>
</td></tr>
<tr><td><code id="visualize_model_+3A_...">...</code></td>
<td>
<p>additional arguments</p>
</td></tr>
<tr><td><code id="visualize_model_+3A_method">method</code></td>
<td>
<p>choose amongst different visualization methods</p>
</td></tr>
<tr><td><code id="visualize_model_+3A_top_n">top_n</code></td>
<td>
<p>return top n elements</p>
</td></tr>
<tr><td><code id="visualize_model_+3A_aggregate">aggregate</code></td>
<td>
<p>= summarize</p>
</td></tr>
<tr><td><code id="visualize_model_+3A_as_table">as_table</code></td>
<td>
<p>= false, table or graph,</p>
</td></tr>
<tr><td><code id="visualize_model_+3A_formula">formula</code></td>
<td>
<p>= formula,</p>
</td></tr>
<tr><td><code id="visualize_model_+3A_measure">measure</code></td>
<td>
<p>= c(&quot;Gain&quot;, &quot;Cover&quot;, &quot;Frequency&quot;)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a plot
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
