<!DOCTYPE html><html lang="en"><head><title>Help for package CalibratR</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {CalibratR}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#BBQ_CV'><p>BBQ_CV</p></a></li>
<li><a href='#binom_for_histogram'><p>binom_for_histogram</p></a></li>
<li><a href='#build_BBQ'><p>build_BBQ</p></a></li>
<li><a href='#build_GUESS'><p>build_GUESS</p></a></li>
<li><a href='#build_hist_binning'><p>build_hist_binning</p></a></li>
<li><a href='#calibrate'><p>calibrate</p></a></li>
<li><a href='#calibrate_me'><p>calibrate_me</p></a></li>
<li><a href='#calibrate_me_CV_errors'><p>calibrate_me_CV_errors</p></a></li>
<li><a href='#compare_models_visual'><p>compare_models_visual</p></a></li>
<li><a href='#evaluate_discrimination'><p>evaluate_discrimination</p></a></li>
<li><a href='#example'><p>example</p></a></li>
<li><a href='#format_values'><p>format_values</p></a></li>
<li><a href='#get_Brier_score'><p>get_Brier_score</p></a></li>
<li><a href='#get_CLE_class'><p>get_CLE_class</p></a></li>
<li><a href='#get_CLE_comparison'><p>get_CLE_comparison</p></a></li>
<li><a href='#get_ECE_equal_width'><p>get_ECE_equal_width</p></a></li>
<li><a href='#get_MCE_equal_width'><p>get_MCE_equal_width</p></a></li>
<li><a href='#getECE'><p>getECE</p></a></li>
<li><a href='#getMCE'><p>getMCE</p></a></li>
<li><a href='#getRMSE'><p>getRMSE</p></a></li>
<li><a href='#GUESS_CV'><p>GUESS_CV</p></a></li>
<li><a href='#hist_binning_CV'><p>hist_binning_CV</p></a></li>
<li><a href='#plot_class_distributions'><p>plot_class_distributions</p></a></li>
<li><a href='#plot_model'><p>plot_model</p></a></li>
<li><a href='#predict_BBQ'><p>predict_BBQ</p></a></li>
<li><a href='#predict_calibratR'><p>predict_calibratR</p></a></li>
<li><a href='#predict_GUESS'><p>predict_GUESS</p></a></li>
<li><a href='#predict_hist_binning'><p>predict_hist_binning</p></a></li>
<li><a href='#predict_model'><p>predict_model</p></a></li>
<li><a href='#rd_multiple_runs'><p>rd_multiple_runs</p></a></li>
<li><a href='#reliability_diagramm'><p>reliability_diagramm</p></a></li>
<li><a href='#scale_me'><p>scale_me</p></a></li>
<li><a href='#statistics_calibratR'><p>statistics_calibratR</p></a></li>
<li><a href='#transform_me'><p>transform_me</p></a></li>
<li><a href='#uncalibrated_CV'><p>uncalibrated_CV</p></a></li>
<li><a href='#visualize_calibrated_test_set'><p>visualize_calibrated_test_set</p></a></li>
<li><a href='#visualize_calibratR'><p>visualize_calibratR</p></a></li>
<li><a href='#visualize_distribution'><p>visualize_distribution</p></a></li>
<li><a href='#visualize_error_boxplot'><p>visualize_error_boxplot</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Mapping ML Scores to Calibrated Predictions</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.2</td>
</tr>
<tr>
<td>Author:</td>
<td>Johanna Schwarz, Dominik Heider</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Dominik Heider &lt;heiderd@mathematik.uni-marburg.de&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Transforms your uncalibrated Machine Learning scores to well-calibrated prediction estimates that can be interpreted as probability estimates. The implemented BBQ (Bayes Binning in Quantiles) model is taken from Naeini (2015, ISBN:0-262-51129-0). Please cite this paper: Schwarz J and Heider D, Bioinformatics 2019, 35(14):2458-2465.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/LGPL-3">LGPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>ggplot2, pROC, reshape2, parallel, foreach, stats,
fitdistrplus, doParallel</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-08-19 12:29:09 UTC; dominikheider</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-08-19 13:00:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='BBQ_CV'>BBQ_CV</h2><span id='topic+BBQ_CV'></span>

<h3>Description</h3>

<p>trains and evaluates the BBQ calibration model using <code>folds</code>-Cross-Validation (CV).
The <code>predicted</code> values are partitioned into n subsets. A BBQ model is constructed on (n-1) subsets; the remaining set is used
for testing the model. All test set predictions are merged and used to compute error metrics for the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BBQ_CV(actual, predicted, method_for_prediction = 0, n_folds = 10, seed,
  input)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="BBQ_CV_+3A_actual">actual</code></td>
<td>
<p>vector of observed class labels (0/1)</p>
</td></tr>
<tr><td><code id="BBQ_CV_+3A_predicted">predicted</code></td>
<td>
<p>vector of uncalibrated predictions</p>
</td></tr>
<tr><td><code id="BBQ_CV_+3A_method_for_prediction">method_for_prediction</code></td>
<td>
<p>0=selection, 1=averaging, Default: 0</p>
</td></tr>
<tr><td><code id="BBQ_CV_+3A_n_folds">n_folds</code></td>
<td>
<p>number of folds in the cross-validation, Default: 10</p>
</td></tr>
<tr><td><code id="BBQ_CV_+3A_seed">seed</code></td>
<td>
<p>random seed to alternate the split of data set partitions</p>
</td></tr>
<tr><td><code id="BBQ_CV_+3A_input">input</code></td>
<td>
<p>specify if the input was scaled or transformed, scaled=1, transformed=2</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list object containing the following components:
</p>
<table role = "presentation">
<tr><td><code>error</code></td>
<td>
<p>list object that summarizes discrimination and calibration errors obtained during the CV</p>
</td></tr>
<tr><td><code>pred_idx</code></td>
<td>
<p>which BBQ prediction method was used during CV, 0=selection, 1=averaging</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>&quot;BBQ&quot;</p>
</td></tr>
<tr><td><code>probs_CV</code></td>
<td>
<p>vector of calibrated predictions that was used during the CV</p>
</td></tr>
<tr><td><code>actual_CV</code></td>
<td>
<p>respective vector of true values (0 or 1) that was used during the CV</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'> ## Loading dataset in environment
 data(example)
 actual &lt;- example$actual
 predicted &lt;- example$predicted
 BBQ_model &lt;- CalibratR:::BBQ_CV(actual, predicted, method_for_prediction=0, n_folds=4, 123, 1)
</code></pre>

<hr>
<h2 id='binom_for_histogram'>binom_for_histogram</h2><span id='topic+binom_for_histogram'></span>

<h3>Description</h3>

<p>p_values from stats::binom.test for each bin, if bin is empty, a p-value of 2 is returned
</p>


<h3>Usage</h3>

<pre><code class='language-R'>binom_for_histogram(n_x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="binom_for_histogram_+3A_n_x">n_x</code></td>
<td>
<p>numeric vector of two integers. The first one is the number of cases in the bin; the second the number of instances in the bin</p>
</td></tr>
</table>


<h3>Value</h3>

<p>p-value from stats::binom.test method
</p>

<hr>
<h2 id='build_BBQ'>build_BBQ</h2><span id='topic+build_BBQ'></span>

<h3>Description</h3>

<p>This method builds a BBQ calibration model using the trainings set provided.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>build_BBQ(actual, predicted)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="build_BBQ_+3A_actual">actual</code></td>
<td>
<p>vector of observed class labels (0/1)</p>
</td></tr>
<tr><td><code id="build_BBQ_+3A_predicted">predicted</code></td>
<td>
<p>vector of uncalibrated predictions</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Based on the paper (and matlab code) : &quot;Obtaining Well Calibrated Probabilities Using Bayesian Binning&quot; by Naeini, Cooper and Hauskrecht: ; https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4410090/
</p>


<h3>Value</h3>

<p>returns the BBQ model which includes models for all evaluated binning schemes; the prunedmodel contains only a selection of BBQ models with the best Bayesian score
</p>

<hr>
<h2 id='build_GUESS'>build_GUESS</h2><span id='topic+build_GUESS'></span>

<h3>Description</h3>

<p>This method builds a GUESS calibration model using the trainings set provided.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>build_GUESS(actual, predicted)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="build_GUESS_+3A_actual">actual</code></td>
<td>
<p>vector of observed class labels (0/1)</p>
</td></tr>
<tr><td><code id="build_GUESS_+3A_predicted">predicted</code></td>
<td>
<p>vector of uncalibrated predictions</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns the trained GUESS model that can be used to calibrate a test set using the <code><a href="#topic+predict_GUESS">predict_GUESS</a></code> method
</p>


<h3>See Also</h3>

<p><code><a href="fitdistrplus.html#topic+denscomp">denscomp</a></code>
</p>

<hr>
<h2 id='build_hist_binning'>build_hist_binning</h2><span id='topic+build_hist_binning'></span>

<h3>Description</h3>

<p>calculate estimated probability per bin, input predicted and real score as numeric vector; builds a histogram binning model which can be used to calibrate uncalibrated predictions using the predict_histogramm_binning method
</p>


<h3>Usage</h3>

<pre><code class='language-R'>build_hist_binning(actual, predicted, bins = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="build_hist_binning_+3A_actual">actual</code></td>
<td>
<p>vector of observed class labels (0/1)</p>
</td></tr>
<tr><td><code id="build_hist_binning_+3A_predicted">predicted</code></td>
<td>
<p>vector of uncalibrated predictions</p>
</td></tr>
<tr><td><code id="build_hist_binning_+3A_bins">bins</code></td>
<td>
<p>number of bins that should be used to build the binning model, Default: decide_on_break estimates optimal number of bins</p>
</td></tr>
</table>


<h3>Details</h3>

<p>if trainings set is smaller then threshold (15 bins*5 elements=75), number of bins is decreased
</p>


<h3>Value</h3>

<p>returns the trained histogram model that can be used to calibrate a test set using the <code><a href="#topic+predict_hist_binning">predict_hist_binning</a></code> method
</p>

<hr>
<h2 id='calibrate'>calibrate</h2><span id='topic+calibrate'></span>

<h3>Description</h3>

<p>Builds selected calibration models on the supplied trainings values <code>actual</code> and <code>predicted</code> and returns them
to the user. New test instances can be calibrated using the <code><a href="#topic+predict_calibratR">predict_calibratR</a></code> function.
Returns cross-validated calibration and discrimination error values for the models if <code>evaluate_CV_error</code> is set to TRUE. Repeated cross-Validation can be time-consuming.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calibrate(actual, predicted, model_idx = c(1, 2, 3, 4, 5),
  evaluate_no_CV_error = TRUE, evaluate_CV_error = TRUE, folds = 10,
  n_seeds = 30, nCores = 4)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="calibrate_+3A_actual">actual</code></td>
<td>
<p>vector of observed class labels (0/1)</p>
</td></tr>
<tr><td><code id="calibrate_+3A_predicted">predicted</code></td>
<td>
<p>vector of uncalibrated predictions</p>
</td></tr>
<tr><td><code id="calibrate_+3A_model_idx">model_idx</code></td>
<td>
<p>which calibration models should be implemented, 1=hist_scaled, 2=hist_transformed, 3=BBQ_scaled, 4=BBQ_transformed, 5=GUESS, Default: c(1, 2, 3, 4, 5)</p>
</td></tr>
<tr><td><code id="calibrate_+3A_evaluate_no_cv_error">evaluate_no_CV_error</code></td>
<td>
<p>computes internal errors for calibration models that were trained on all available <code>actual</code>/<code>predicted</code> tuples. Testing is performed with the same set. Be careful to interpret those error values, as they are not cross-validated. Default: TRUE</p>
</td></tr>
<tr><td><code id="calibrate_+3A_evaluate_cv_error">evaluate_CV_error</code></td>
<td>
<p>computes cross-validation error. <code>folds</code> times cross validation is repeated <code>n_seeds</code> times with changing seeds. The trained models and the their calibration and discrimination errors are returned.
Evaluation of CV errors can take some time to compute, depending on the number of repetitions specified in <code>n_seeds</code>, Default: TRUE</p>
</td></tr>
<tr><td><code id="calibrate_+3A_folds">folds</code></td>
<td>
<p>number of folds in the cross-validation of the calibration model. If <code>folds</code> is set to 1, no CV is performed and <code>summary_CV</code> can be calculated. Default: 10</p>
</td></tr>
<tr><td><code id="calibrate_+3A_n_seeds">n_seeds</code></td>
<td>
<p><code>n_seeds</code> determines how often random data set partition is repeated with varying seed. If <code>folds</code> is 1, <code>n_seeds</code> should be set to 1, too. Default: 30</p>
</td></tr>
<tr><td><code id="calibrate_+3A_ncores">nCores</code></td>
<td>
<p><code>nCores</code> how many cores should be used during parallelisation. Default: 4</p>
</td></tr>
</table>


<h3>Details</h3>

<p>parallised execution of random data set splits for the Cross-Validation procedure over <code>n_seeds</code>
</p>


<h3>Value</h3>

<p>A list object with the following components:
</p>
<table role = "presentation">
<tr><td><code>calibration_models</code></td>
<td>
<p>a list of all trained calibration models, which can be used in the <code><a href="#topic+predict_calibratR">predict_calibratR</a></code> method.</p>
</td></tr>
<tr><td><code>summary_CV</code></td>
<td>
<p>a list containing information on the CV errors of the implemented models</p>
</td></tr>
<tr><td><code>summary_no_CV</code></td>
<td>
<p>a list containing information on the internal errors of the implemented models</p>
</td></tr>
<tr><td><code>predictions</code></td>
<td>
<p>calibrated predictions for the original <code>predicted</code> values</p>
</td></tr>
<tr><td><code>n_seeds</code></td>
<td>
<p>number of random data set partitions into training and test set for <code>folds</code>-times CV</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Johanna Schwarz
</p>


<h3>Examples</h3>

<pre><code class='language-R'> ## Loading dataset in environment
 data(example)
 actual &lt;- example$actual
 predicted &lt;- example$predicted

 ## Create calibration models
 calibration_model &lt;- calibrate(actual, predicted,
                              model_idx = c(1,2),
                              FALSE, FALSE, folds = 10, n_seeds = 1, nCores = 2)
</code></pre>

<hr>
<h2 id='calibrate_me'>calibrate_me</h2><span id='topic+calibrate_me'></span>

<h3>Description</h3>

<p>trains calibration models on the training set of <code>predicted</code>/<code>actual</code> value pairs.<code>model_idx</code> specifies which models should be trained.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calibrate_me(actual, predicted, model_idx)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="calibrate_me_+3A_actual">actual</code></td>
<td>
<p>vector of observed class labels (0/1)</p>
</td></tr>
<tr><td><code id="calibrate_me_+3A_predicted">predicted</code></td>
<td>
<p>vector of uncalibrated predictions</p>
</td></tr>
<tr><td><code id="calibrate_me_+3A_model_idx">model_idx</code></td>
<td>
<p>a single number from 1 to 5, indicating which calibration model should be implemented, 1=hist_scaled, 2=hist_transformed, 3=BBQ_scaled, 4=BBQ_transformed, 5=GUESS</p>
</td></tr>
</table>


<h3>Value</h3>

<p>depending on the value of <code>model_idx</code>, the respective calibration model is build on the input from <code>actual</code> and <code>predicted</code>
</p>

<hr>
<h2 id='calibrate_me_CV_errors'>calibrate_me_CV_errors</h2><span id='topic+calibrate_me_CV_errors'></span>

<h3>Description</h3>

<p>trains and evaluates calibration models using <code>n_seeds</code>-times repeated <code>folds</code>-Cross-Validation (CV).<code>model_idx</code> specifies which models should be trained.
<br /> Model training and evaluation is repeated <code>n_seeds</code>-times with a different training/test set partition scheme for the CV each time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calibrate_me_CV_errors(actual, predicted, model_idx, folds = 10, n_seeds,
  nCores)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="calibrate_me_CV_errors_+3A_actual">actual</code></td>
<td>
<p>vector of observed class labels (0/1)</p>
</td></tr>
<tr><td><code id="calibrate_me_CV_errors_+3A_predicted">predicted</code></td>
<td>
<p>vector of uncalibrated predictions</p>
</td></tr>
<tr><td><code id="calibrate_me_CV_errors_+3A_model_idx">model_idx</code></td>
<td>
<p>which calibration models should be implemented, 1=hist_scaled, 2=hist_transformed, 3=BBQ_scaled, 4=BBQ_transformed, 5=GUESS</p>
</td></tr>
<tr><td><code id="calibrate_me_CV_errors_+3A_folds">folds</code></td>
<td>
<p>number of folds in the cross-validation, Default: 10</p>
</td></tr>
<tr><td><code id="calibrate_me_CV_errors_+3A_n_seeds">n_seeds</code></td>
<td>
<p><code>n_seeds</code> determines how often random data set partition is repeated with varying seed</p>
</td></tr>
<tr><td><code id="calibrate_me_CV_errors_+3A_ncores">nCores</code></td>
<td>
<p><code>nCores</code> how many cores should be used during parallelisation. Default: 4</p>
</td></tr>
</table>


<h3>Details</h3>

<p>parallised execution over <code>n_seeds</code>
</p>


<h3>Value</h3>

<p>returns all trained calibration models that were built during the <code>n_seeds</code>-times repeated <code>folds</code>-CV.
<br /> Error values for each of the <code>n_seeds</code> CV runs are given.
</p>

<hr>
<h2 id='compare_models_visual'>compare_models_visual</h2><span id='topic+compare_models_visual'></span>

<h3>Description</h3>

<p>FUNCTION_DESCRIPTION
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compare_models_visual(models, seq = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="compare_models_visual_+3A_models">models</code></td>
<td>
<p>PARAM_DESCRIPTION</p>
</td></tr>
<tr><td><code id="compare_models_visual_+3A_seq">seq</code></td>
<td>
<p>sequence for which the calibrated predictions should be plotted, Default: NULL</p>
</td></tr>
</table>


<h3>Details</h3>

<p>DETAILS
</p>


<h3>Value</h3>

<p>OUTPUT_DESCRIPTION
</p>


<h3>See Also</h3>

<p><code><a href="ggplot2.html#topic+ggplot">ggplot</a></code>,<code><a href="ggplot2.html#topic+geom_line">geom_line</a></code>,<code><a href="ggplot2.html#topic+aes">aes</a></code>,<code><a href="ggplot2.html#topic+ylim">ylim</a></code>,<code><a href="ggplot2.html#topic+theme">theme</a></code>,<code><a href="ggplot2.html#topic+labs">labs</a></code>,<code><a href="ggplot2.html#topic+scale_color_brewer">scale_color_brewer</a></code>
<code><a href="reshape2.html#topic+melt">melt</a></code>
</p>

<hr>
<h2 id='evaluate_discrimination'>evaluate_discrimination</h2><span id='topic+evaluate_discrimination'></span>

<h3>Description</h3>

<p>computes various discrimination error values, namely: sensitivity, specificity, accuracy, positive predictive value (ppv), negative predictive value (npv) and AUC
</p>


<h3>Usage</h3>

<pre><code class='language-R'>evaluate_discrimination(actual, predicted, cutoff = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="evaluate_discrimination_+3A_actual">actual</code></td>
<td>
<p>vector of observed class labels (0/1)</p>
</td></tr>
<tr><td><code id="evaluate_discrimination_+3A_predicted">predicted</code></td>
<td>
<p>vector of uncalibrated predictions</p>
</td></tr>
<tr><td><code id="evaluate_discrimination_+3A_cutoff">cutoff</code></td>
<td>
<p>cut-off to be used for the computation of npv, ppv, sensitivity and specificity, Default: value that maximizes sensitivity and specificity (Youden-Index)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list object with the following components:
</p>
<table role = "presentation">
<tr><td><code>sens</code></td>
<td>
<p>sensitivity</p>
</td></tr>
<tr><td><code>spec</code></td>
<td>
<p>specificity</p>
</td></tr>
<tr><td><code>acc</code></td>
<td>
<p>accuracy</p>
</td></tr>
<tr><td><code>ppv</code></td>
<td>
<p>positive predictive value</p>
</td></tr>
<tr><td><code>npv</code></td>
<td>
<p>negative predictive value</p>
</td></tr>
<tr><td><code>cutoff</code></td>
<td>
<p>cut-off that was used to compute the error values</p>
</td></tr>
<tr><td><code>auc</code></td>
<td>
<p>AUC value</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="pROC.html#topic+roc">roc</a></code>
</p>

<hr>
<h2 id='example'>example</h2><span id='topic+example'></span>

<h3>Description</h3>

<p>list object containing 1) the simulated classifiers for two classes. Distributions are simulated from Gaussian distributions with
Normal(mean=1.5, sd=0) for class 1 and Normal(mean=0, sd=0) for class 0 instances. Each class consists of 100 instances.
and 2) A test set of 100 instances
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(example)
</code></pre>


<h3>Format</h3>

<p><code>predicted</code>=vector of 200 simulated classifier values; <code>actual</code>=their respective true class labels (0/1)</p>

<hr>
<h2 id='format_values'>format_values</h2><span id='topic+format_values'></span>

<h3>Description</h3>

<p>returns formatted input.
If specified, the uncalibrated input is mapped to the [0;1] range using scaling (<code><a href="#topic+scale_me">scale_me</a></code>) or transforming (<code><a href="#topic+transform_me">transform_me</a></code>)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>format_values(cases, control, input, min = NULL, max = NULL, mean = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="format_values_+3A_cases">cases</code></td>
<td>
<p>instances from class 1</p>
</td></tr>
<tr><td><code id="format_values_+3A_control">control</code></td>
<td>
<p>instances from class 0</p>
</td></tr>
<tr><td><code id="format_values_+3A_input">input</code></td>
<td>
<p>single integer (0, 1 or 2). specify if the input should be formatted (=0), formatted and scaled (=1)
or formatted and transformed (=2)</p>
</td></tr>
<tr><td><code id="format_values_+3A_min">min</code></td>
<td>
<p>min value of the original data set, default=calculated on input</p>
</td></tr>
<tr><td><code id="format_values_+3A_max">max</code></td>
<td>
<p>max value of the original data set, default=calculated on input</p>
</td></tr>
<tr><td><code id="format_values_+3A_mean">mean</code></td>
<td>
<p>mean value of the original data set, default=calculated on input</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list object with the following components:
</p>
<table role = "presentation">
<tr><td><code>formated_values</code></td>
<td>
<p>formatted input. If <code>input</code> is set to 1 (2), the input is additionally scaled (transformed) using the
method <code><a href="#topic+scale_me">scale_me</a></code> (<code><a href="#topic+transform_me">transform_me</a></code>)</p>
</td></tr>
<tr><td><code>min</code></td>
<td>
<p>minimum value among all instances</p>
</td></tr>
<tr><td><code>max</code></td>
<td>
<p>maximum value among all instances</p>
</td></tr>
<tr><td><code>mean</code></td>
<td>
<p>mean value among all instances</p>
</td></tr>
</table>

<hr>
<h2 id='get_Brier_score'>get_Brier_score</h2><span id='topic+get_Brier_score'></span>

<h3>Description</h3>

<p>FUNCTION_DESCRIPTION
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_Brier_score(actual, predicted)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_Brier_score_+3A_actual">actual</code></td>
<td>
<p>vector of observed class labels (0/1)</p>
</td></tr>
<tr><td><code id="get_Brier_score_+3A_predicted">predicted</code></td>
<td>
<p>vector of uncalibrated predictions</p>
</td></tr>
</table>


<h3>Details</h3>

<p>DETAILS
</p>


<h3>Value</h3>

<p>OUTPUT_DESCRIPTION
</p>

<hr>
<h2 id='get_CLE_class'>get_CLE_class</h2><span id='topic+get_CLE_class'></span>

<h3>Description</h3>

<p>calculates the class-specific classification error CLE in the test set.
The method computes the deviation of the calibrated predictions of class 1 instances from their true value 1.
For class 0 instances, <code>get_CLE_class</code> computes the deviation from 0.
Class 1 CLE is 0 when all class 1 instances have a calibrated prediction of 1 regardless of potential miscalibration of class 0 instances.
CLE calculation is helpful when miscalibration and -classification is more cost-sensitive for one class than for the other.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_CLE_class(actual, predicted, bins = 10)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_CLE_class_+3A_actual">actual</code></td>
<td>
<p>vector of observed class labels (0/1)</p>
</td></tr>
<tr><td><code id="get_CLE_class_+3A_predicted">predicted</code></td>
<td>
<p>vector of uncalibrated predictions</p>
</td></tr>
<tr><td><code id="get_CLE_class_+3A_bins">bins</code></td>
<td>
<p>number of bins for the equal-width binning model, default=10</p>
</td></tr>
</table>


<h3>Value</h3>

<p>object of class list containing the following components:
</p>
<table role = "presentation">
<tr><td><code>class_1</code></td>
<td>
<p>CLE of class 1 instances</p>
</td></tr>
<tr><td><code>class_0</code></td>
<td>
<p>CLE of class 0 instances</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="reshape2.html#topic+melt">melt</a></code>
<code><a href="ggplot2.html#topic+ggplot">ggplot</a></code>,<code><a href="ggplot2.html#topic+geom_line">geom_line</a></code>,<code><a href="ggplot2.html#topic+aes">aes</a></code>,<code><a href="ggplot2.html#topic+position_dodge">position_dodge</a></code>,<code><a href="ggplot2.html#topic+labs">labs</a></code>,<code><a href="ggplot2.html#topic+scale_colour_manual">scale_colour_manual</a></code>
</p>

<hr>
<h2 id='get_CLE_comparison'>get_CLE_comparison</h2><span id='topic+get_CLE_comparison'></span>

<h3>Description</h3>

<p>visualises how class 1 and class 0 classification error (CLE) differs in each trained calibration model.
Comparing class-specific CLE helps to choose a calibration model for applications were classification error is cost-sensitive for one class.
See <code><a href="#topic+get_CLE_class">get_CLE_class</a></code> for details on the implementation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_CLE_comparison(list_models)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_CLE_comparison_+3A_list_models">list_models</code></td>
<td>
<p>list object that contains all error values for all trained calibration models. For the specific format, see the calling function <code><a href="#topic+visualize_calibratR">visualize_calibratR</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot2
</p>

<hr>
<h2 id='get_ECE_equal_width'>get_ECE_equal_width</h2><span id='topic+get_ECE_equal_width'></span>

<h3>Description</h3>

<p>Expected Calibration Error (ECE); the model is divided into  10 equal-width bins (default) and the mean of the observed (0/1) vs. mean of predicted is calculated per bin, weighted by emperical frequency of elements in bin i
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_ECE_equal_width(actual, predicted, bins = 10)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_ECE_equal_width_+3A_actual">actual</code></td>
<td>
<p>vector of observed class labels (0/1)</p>
</td></tr>
<tr><td><code id="get_ECE_equal_width_+3A_predicted">predicted</code></td>
<td>
<p>vector of uncalibrated predictions</p>
</td></tr>
<tr><td><code id="get_ECE_equal_width_+3A_bins">bins</code></td>
<td>
<p>number of bins for the equal-width binning model</p>
</td></tr>
</table>


<h3>Value</h3>

<p>equal-width ECE value
</p>

<hr>
<h2 id='get_MCE_equal_width'>get_MCE_equal_width</h2><span id='topic+get_MCE_equal_width'></span>

<h3>Description</h3>

<p>Maximum Calibration Error (MCE), returns maximum calibration error for equal-width binning model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_MCE_equal_width(actual, predicted, bins = 10)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_MCE_equal_width_+3A_actual">actual</code></td>
<td>
<p>vector of observed class labels (0/1)</p>
</td></tr>
<tr><td><code id="get_MCE_equal_width_+3A_predicted">predicted</code></td>
<td>
<p>vector of uncalibrated predictions</p>
</td></tr>
<tr><td><code id="get_MCE_equal_width_+3A_bins">bins</code></td>
<td>
<p>number of bins for the binning model</p>
</td></tr>
</table>


<h3>Value</h3>

<p>equal-width MCE value
</p>

<hr>
<h2 id='getECE'>getECE</h2><span id='topic+getECE'></span>

<h3>Description</h3>

<p>Expected Calibration Error (ECE); the model is divided into 10 equal-width bins (default) and the mean of the observed (0/1) vs. mean of predicted is calculated per bin, weighted by emperical frequency of elements in bin i
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getECE(actual, predicted, n_bins = 10)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getECE_+3A_actual">actual</code></td>
<td>
<p>vector of observed class labels (0/1)</p>
</td></tr>
<tr><td><code id="getECE_+3A_predicted">predicted</code></td>
<td>
<p>vector of uncalibrated predictions</p>
</td></tr>
<tr><td><code id="getECE_+3A_n_bins">n_bins</code></td>
<td>
<p>number of bins of the underlying equal-frequency histogram, Default: 10</p>
</td></tr>
</table>


<h3>Value</h3>

<p>equal-frequency ECE value
</p>

<hr>
<h2 id='getMCE'>getMCE</h2><span id='topic+getMCE'></span>

<h3>Description</h3>

<p>Maximum Calibration Error (MCE), returns maximum calibration error for equal-frequency binning model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getMCE(actual, predicted, n_bins = 10)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getMCE_+3A_actual">actual</code></td>
<td>
<p>vector of observed class labels (0/1)</p>
</td></tr>
<tr><td><code id="getMCE_+3A_predicted">predicted</code></td>
<td>
<p>vector of uncalibrated predictions</p>
</td></tr>
<tr><td><code id="getMCE_+3A_n_bins">n_bins</code></td>
<td>
<p>number of bins of the underlying equal-frequency histogram, Default: 10</p>
</td></tr>
</table>


<h3>Value</h3>

<p>equal-frequency MCE value
</p>

<hr>
<h2 id='getRMSE'>getRMSE</h2><span id='topic+getRMSE'></span>

<h3>Description</h3>

<p>calculates the root of mean square error (RMSE) in the test set of calibrated predictions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getRMSE(actual, predicted)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getRMSE_+3A_actual">actual</code></td>
<td>
<p>vector of observed class labels (0/1)</p>
</td></tr>
<tr><td><code id="getRMSE_+3A_predicted">predicted</code></td>
<td>
<p>vector of uncalibrated predictions</p>
</td></tr>
</table>


<h3>Value</h3>

<p>RMSE value
</p>

<hr>
<h2 id='GUESS_CV'>GUESS_CV</h2><span id='topic+GUESS_CV'></span>

<h3>Description</h3>

<p>trains and evaluates the GUESS calibration model using <code>folds</code>-Cross-Validation (CV).
The <code>predicted</code> values are partitioned into n subsets. A GUESS model is constructed on (n-1) subsets; the remaining set is used
for testing the model. All test set predictions are merged and used to compute error metrics for the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GUESS_CV(actual, predicted, n_folds = 10, method_of_prediction = 2, seed,
  input)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="GUESS_CV_+3A_actual">actual</code></td>
<td>
<p>vector of observed class labels (0/1)</p>
</td></tr>
<tr><td><code id="GUESS_CV_+3A_predicted">predicted</code></td>
<td>
<p>vector of uncalibrated predictions</p>
</td></tr>
<tr><td><code id="GUESS_CV_+3A_n_folds">n_folds</code></td>
<td>
<p>number of folds for the cross-validation, Default: 10</p>
</td></tr>
<tr><td><code id="GUESS_CV_+3A_method_of_prediction">method_of_prediction</code></td>
<td>
<p>PARAM_DESCRIPTION, Default: 2</p>
</td></tr>
<tr><td><code id="GUESS_CV_+3A_seed">seed</code></td>
<td>
<p>random seed to alternate the split of data set partitions</p>
</td></tr>
<tr><td><code id="GUESS_CV_+3A_input">input</code></td>
<td>
<p>specify if the input was scaled or transformed, scaled=1, transformed=2</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list object containing the following components:
</p>
<table role = "presentation">
<tr><td><code>error</code></td>
<td>
<p>list object that summarizes discrimination and calibration errors obtained during the CV</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>&quot;GUESS&quot;</p>
</td></tr>
<tr><td><code>pred_idx</code></td>
<td>
<p>which prediction method was used during CV</p>
</td></tr>
<tr><td><code>probs_CV</code></td>
<td>
<p>vector of calibrated predictions that was used during the CV</p>
</td></tr>
<tr><td><code>actual_CV</code></td>
<td>
<p>respective vector of true values (0 or 1) that was used during the CV</p>
</td></tr>
</table>

<hr>
<h2 id='hist_binning_CV'>hist_binning_CV</h2><span id='topic+hist_binning_CV'></span>

<h3>Description</h3>

<p>trains and evaluates the histogram binning calibration model repeated <code>folds</code>-Cross-Validation (CV).
The <code>predicted</code> values are partitioned into n subsets. A histogram binning model is constructed on (n-1) subsets; the remaining set is used
for testing the model. All test set predictions are merged and used to compute error metrics for the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hist_binning_CV(actual, predicted, n_bins = 15, n_folds = 10, seed, input)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hist_binning_CV_+3A_actual">actual</code></td>
<td>
<p>vector of observed class labels (0/1)</p>
</td></tr>
<tr><td><code id="hist_binning_CV_+3A_predicted">predicted</code></td>
<td>
<p>vector of uncalibrated predictions</p>
</td></tr>
<tr><td><code id="hist_binning_CV_+3A_n_bins">n_bins</code></td>
<td>
<p>number of bins used in the histogram binning scheme, Default: 15</p>
</td></tr>
<tr><td><code id="hist_binning_CV_+3A_n_folds">n_folds</code></td>
<td>
<p>number of folds in the cross-validation, Default: 10</p>
</td></tr>
<tr><td><code id="hist_binning_CV_+3A_seed">seed</code></td>
<td>
<p>random seed to alternate the split of data set partitions</p>
</td></tr>
<tr><td><code id="hist_binning_CV_+3A_input">input</code></td>
<td>
<p>specify if the input was scaled or transformed, scaled=1, transformed=2</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list object containing the following components:
</p>
<table role = "presentation">
<tr><td><code>error</code></td>
<td>
<p>list object that summarizes discrimination and calibration errors obtained during the CV</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>&quot;hist&quot;</p>
</td></tr>
<tr><td><code>probs_CV</code></td>
<td>
<p>vector of calibrated predictions that was used during the CV</p>
</td></tr>
<tr><td><code>actual_CV</code></td>
<td>
<p>respective vector of true values (0 or 1) that was used during the CV</p>
</td></tr>
</table>

<hr>
<h2 id='plot_class_distributions'>plot_class_distributions</h2><span id='topic+plot_class_distributions'></span>

<h3>Description</h3>

<p>plots the the returned conditional class probabilities P(x|C) of GUESS_1 or GUESS_2 models. Which GUESS model is plotted can be specified in <code>pred_idx</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_class_distributions(build_guess_object, pred_idx)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot_class_distributions_+3A_build_guess_object">build_guess_object</code></td>
<td>
<p>output from build_GUESS()</p>
</td></tr>
<tr><td><code id="plot_class_distributions_+3A_pred_idx">pred_idx</code></td>
<td>
<p>if <code>pred_idx</code>=1 GUESS_1 is plotted; if <code>pred_idx</code>=2 GUESS_2 is plotted</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot object that visualizes the returned calibrated predicition estimates by GUESS_1 or GUESS_2
</p>


<h3>See Also</h3>

<p><code><a href="reshape2.html#topic+melt">melt</a></code>
<code><a href="ggplot2.html#topic+ggplot">ggplot</a></code>,<code><a href="ggplot2.html#topic+geom_line">geom_line</a></code>,<code><a href="ggplot2.html#topic+aes">aes</a></code>,<code><a href="ggplot2.html#topic+scale_colour_manual">scale_colour_manual</a></code>,<code><a href="ggplot2.html#topic+theme">theme</a></code>,<code><a href="ggplot2.html#topic+labs">labs</a></code>,<code><a href="ggplot2.html#topic+geom_vline">geom_vline</a></code>,<code><a href="ggplot2.html#topic+geom_text">geom_text</a></code>
</p>

<hr>
<h2 id='plot_model'>plot_model</h2><span id='topic+plot_model'></span>

<h3>Description</h3>

<p>this methods visualizes all implemented calibration models as a mapping function between original ML scores (x-axis) and
calibrated predictions (y-axis)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_model(calibration_model, seq = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot_model_+3A_calibration_model">calibration_model</code></td>
<td>
<p>output from the <code><a href="#topic+calibrate">calibrate</a></code> method.</p>
</td></tr>
<tr><td><code id="plot_model_+3A_seq">seq</code></td>
<td>
<p>sequence of ML scores over which the mapping function should be evaluated, Default: 100 scores from the minimum to the maximum of the original ML scores</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot object
</p>


<h3>See Also</h3>

<p><code><a href="reshape2.html#topic+melt">melt</a></code>
<code><a href="ggplot2.html#topic+ggplot">ggplot</a></code>,<code><a href="ggplot2.html#topic+geom_line">geom_line</a></code>,<code><a href="ggplot2.html#topic+aes">aes</a></code>,<code><a href="ggplot2.html#topic+ylim">ylim</a></code>,<code><a href="ggplot2.html#topic+scale_colour_manual">scale_colour_manual</a></code>,<code><a href="ggplot2.html#topic+theme">theme</a></code>,<code><a href="ggplot2.html#topic+labs">labs</a></code>,<code><a href="ggplot2.html#topic+geom_text">geom_text</a></code>,<code><a href="ggplot2.html#topic+geom_vline">geom_vline</a></code>
</p>

<hr>
<h2 id='predict_BBQ'>predict_BBQ</h2><span id='topic+predict_BBQ'></span>

<h3>Description</h3>

<p>FUNCTION_DESCRIPTION
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_BBQ(bbq, new, option)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict_BBQ_+3A_bbq">bbq</code></td>
<td>
<p>output from the <code><a href="#topic+build_BBQ">build_BBQ</a></code> method</p>
</td></tr>
<tr><td><code id="predict_BBQ_+3A_new">new</code></td>
<td>
<p>vector of uncalibrated probabilities</p>
</td></tr>
<tr><td><code id="predict_BBQ_+3A_option">option</code></td>
<td>
<p>either 1 or 0; averaging=1, selecting=0</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Based on the paper (and matlab code) : &quot;Obtaining Well Calibrated Probabilities Using Bayesian Binning&quot; by Naeini, Cooper and Hauskrecht: ; https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4410090/
</p>


<h3>Value</h3>

<p>a list object containing the following components:
</p>
<table role = "presentation">
<tr><td><code>predictions</code></td>
<td>
<p>contains a vector of calibrated predictions</p>
</td></tr>
<tr><td><code>pred_idx</code></td>
<td>
<p>which option was used (averaging or selecting)</p>
</td></tr>
<tr><td><code>significance_test_set</code></td>
<td>
<p>the percentage of <code>new</code> instances that was evaluated using significant prediction estimates</p>
</td></tr>
<tr><td><code>pred_per_bin</code></td>
<td>
<p>number of instances <code>new</code> in each bin of the selected model</p>
</td></tr>
</table>

<hr>
<h2 id='predict_calibratR'>predict_calibratR</h2><span id='topic+predict_calibratR'></span>

<h3>Description</h3>

<p>maps the uncalibrated predictions <code>new</code> into calibrated predictions using the passed over <code>calibration models</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_calibratR(calibration_models, new = NULL, nCores = 4)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict_calibratR_+3A_calibration_models">calibration_models</code></td>
<td>
<p>list of trained calibration models that were constructed using the <code><a href="#topic+calibrate">calibrate</a></code> method.
The list components <code>calibration_models</code> from the <code><a href="#topic+calibrate">calibrate</a></code> output can be used directly.</p>
</td></tr>
<tr><td><code id="predict_calibratR_+3A_new">new</code></td>
<td>
<p>vector of new uncalibrated instances. Default: 100 scores from the minimum to the maximum of the original ML scores</p>
</td></tr>
<tr><td><code id="predict_calibratR_+3A_ncores">nCores</code></td>
<td>
<p><code>nCores</code> how many cores should be used during parallelisation. Default: 4</p>
</td></tr>
</table>


<h3>Details</h3>

<p>if no <code>new</code> value is given, the function will evaluate a sequence of numbers ranging from the minimum to the maximum of the original values in the training set
</p>


<h3>Value</h3>

<p>list object with the following components:
</p>
<table role = "presentation">
<tr><td><code>predictions</code></td>
<td>
<p>a list containing the calibrated predictions for each calibration model</p>
</td></tr>
<tr><td><code>significance_test_set</code></td>
<td>
<p>a list containing the percentage of <code>new</code> instances for which prediction estimates are statistically significant</p>
</td></tr>
<tr><td><code>pred_per_bin</code></td>
<td>
<p>a list containing the number of instances in each bin for the binning models</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Johanna Schwarz
</p>


<h3>Examples</h3>

<pre><code class='language-R'> ## Loading dataset in environment
 data(example)
 test_set &lt;- example$test_set
 calibration_model &lt;- example$calibration_model

 ## Predict for test set
predictions &lt;-  predict_calibratR(calibration_model$calibration_models, new=test_set, nCores = 2)

</code></pre>

<hr>
<h2 id='predict_GUESS'>predict_GUESS</h2><span id='topic+predict_GUESS'></span>

<h3>Description</h3>

<p>returns calibrated predictions for the instances <code>new</code> using the trained GUESS calibration model <code>build_guess_object</code>.
Two different evaluation methods are available.
Method 1: returns the p-value for the score <code>new</code> under the distribution that is handed over in the <code>build_guess_object</code>
Method 2: returns the probability density value for the score <code>new</code> under the distribution that is handed over in the <code>build_guess_object</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_GUESS(build_guess_object, new, density_evaluation = 2,
  return_class_density = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict_GUESS_+3A_build_guess_object">build_guess_object</code></td>
<td>
<p>output from the <code><a href="#topic+build_GUESS">build_GUESS</a></code> method</p>
</td></tr>
<tr><td><code id="predict_GUESS_+3A_new">new</code></td>
<td>
<p>vector of uncalibrated probabilities</p>
</td></tr>
<tr><td><code id="predict_GUESS_+3A_density_evaluation">density_evaluation</code></td>
<td>
<p>which density evaluation method should be used to infer calculate probabilities, Default: 2</p>
</td></tr>
<tr><td><code id="predict_GUESS_+3A_return_class_density">return_class_density</code></td>
<td>
<p>if set to TRUE, class densities p(x|class) are returned, Default: FALSE</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>dens_case</code> and <code>dens_control</code> are only returned when <code>return_class_density</code> is set to TRUE
</p>


<h3>Value</h3>

<p>a list object containing the following components:
</p>
<table role = "presentation">
<tr><td><code>predictions</code></td>
<td>
<p>contains a vector of calibrated predictions</p>
</td></tr>
<tr><td><code>pred_idx</code></td>
<td>
<p>which density evaluation method was used</p>
</td></tr>
<tr><td><code>significance_test_set</code></td>
<td>
<p>the percentage of <code>new</code> instances that was evaluated using significant prediction estimates</p>
</td></tr>
<tr><td><code>dens_case</code></td>
<td>
<p>a vector containing the p(x|case) values</p>
</td></tr>
<tr><td><code>dens_control</code></td>
<td>
<p>a vector containing the p(x|control) values</p>
</td></tr>
</table>

<hr>
<h2 id='predict_hist_binning'>predict_hist_binning</h2><span id='topic+predict_hist_binning'></span>

<h3>Description</h3>

<p>predict for a new element using histogram binning
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_hist_binning(histogram, new)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict_hist_binning_+3A_histogram">histogram</code></td>
<td>
<p>the output of <code><a href="#topic+build_hist_binning">build_hist_binning</a></code></p>
</td></tr>
<tr><td><code id="predict_hist_binning_+3A_new">new</code></td>
<td>
<p>vector of uncalibrated probabilities</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list object containing the following components
</p>
<table role = "presentation">
<tr><td><code>predictions</code></td>
<td>
<p>contains a vector of calibrated predictions</p>
</td></tr>
<tr><td><code>significance_test_set</code></td>
<td>
<p>the percentage of <code>new</code> instances that was evaluated using significant prediction estimates</p>
</td></tr>
<tr><td><code>pred_per_bin</code></td>
<td>
<p>a table containing the number of instances from <code>new</code> for each bin of the final binning scheme of <code>histogram</code></p>
</td></tr>
</table>

<hr>
<h2 id='predict_model'>predict_model</h2><span id='topic+predict_model'></span>

<h3>Description</h3>

<p>calibrates the uncalibrated predictions <code>new</code> using <code>calibration_model</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_model(new, calibration_model, min, max, mean, inputtype)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict_model_+3A_new">new</code></td>
<td>
<p>vector of uncalibrated predictions</p>
</td></tr>
<tr><td><code id="predict_model_+3A_calibration_model">calibration_model</code></td>
<td>
<p>calibration model to be used for the calibration. Can be the output of <code><a href="#topic+build_BBQ">build_BBQ</a></code>,<code><a href="#topic+build_hist_binning">build_hist_binning</a></code> or <code><a href="#topic+build_GUESS">build_GUESS</a></code>.</p>
</td></tr>
<tr><td><code id="predict_model_+3A_min">min</code></td>
<td>
<p>minimum value of the original data set</p>
</td></tr>
<tr><td><code id="predict_model_+3A_max">max</code></td>
<td>
<p>maximum value of the original data set</p>
</td></tr>
<tr><td><code id="predict_model_+3A_mean">mean</code></td>
<td>
<p>mean value of the original data set</p>
</td></tr>
<tr><td><code id="predict_model_+3A_inputtype">inputtype</code></td>
<td>
<p>specify if the model was build on original (=0), scaled(=1) or transformed (=2) data</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector of calibrated predictions
</p>

<hr>
<h2 id='rd_multiple_runs'>rd_multiple_runs</h2><span id='topic+rd_multiple_runs'></span>

<h3>Description</h3>

<p>This functions plots all n reliability diagrams that were constructed during n-times repeated m-fold cross-validation (CV).
During calibration model evaluation, CV is repeated n times, so that eventually n reliability diagrams are obtained.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rd_multiple_runs(list_models)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rd_multiple_runs_+3A_list_models">list_models</code></td>
<td>
<p>list object that contains n-times the output from the <code><a href="#topic+reliability_diagramm">reliability_diagramm</a></code>. method.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list object that contains a reliability diagram that visualises all reliabilty diagrams that were constructed during n-times repeated m-fold cross-validation.
</p>


<h3>See Also</h3>

<p><code><a href="reshape2.html#topic+melt">melt</a></code>
<code><a href="ggplot2.html#topic+ggplot">ggplot</a></code>,<code><a href="ggplot2.html#topic+geom_line">geom_line</a></code>,<code><a href="ggplot2.html#topic+aes">aes</a></code>,<code><a href="ggplot2.html#topic+geom_abline">geom_abline</a></code>,<code><a href="ggplot2.html#topic+ylab">ylab</a></code>,<code><a href="ggplot2.html#topic+xlab">xlab</a></code>,<code><a href="ggplot2.html#topic+xlim">xlim</a></code>,<code><a href="ggplot2.html#topic+ylim">ylim</a></code>,<code><a href="ggplot2.html#topic+coord_fixed">coord_fixed</a></code>,<code><a href="ggplot2.html#topic+geom_text">geom_text</a></code>,<code><a href="ggplot2.html#topic+scale_color_discrete">scale_color_discrete</a></code>,<code><a href="ggplot2.html#topic+ggtitle">ggtitle</a></code>
</p>

<hr>
<h2 id='reliability_diagramm'>reliability_diagramm</h2><span id='topic+reliability_diagramm'></span>

<h3>Description</h3>

<p>Reliability curves allow checking if the predicted probabilities of a
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reliability_diagramm(actual, predicted, bins = 10, plot_rd = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="reliability_diagramm_+3A_actual">actual</code></td>
<td>
<p>vector of observed class labels (0/1)</p>
</td></tr>
<tr><td><code id="reliability_diagramm_+3A_predicted">predicted</code></td>
<td>
<p>vector of uncalibrated predictions</p>
</td></tr>
<tr><td><code id="reliability_diagramm_+3A_bins">bins</code></td>
<td>
<p>number of bins in the reliability diagram, Default: 10</p>
</td></tr>
<tr><td><code id="reliability_diagramm_+3A_plot_rd">plot_rd</code></td>
<td>
<p>should the reliability diagram be plotted, Default: TRUE</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list object containing the following elements
</p>
<table role = "presentation">
<tr><td><code>calibration_error</code></td>
<td>
</td></tr>
<tr><td><code>discrimination_error</code></td>
<td>
</td></tr>
<tr><td><code>rd_breaks</code></td>
<td>
</td></tr>
<tr><td><code>histogram_plot</code></td>
<td>
</td></tr>
<tr><td><code>diagram_plot</code></td>
<td>
</td></tr>
<tr><td><code>mean_pred_per_bin</code></td>
<td>
</td></tr>
<tr><td><code>accuracy_per_bin</code></td>
<td>
</td></tr>
<tr><td><code>freq_per_bin</code></td>
<td>
</td></tr>
<tr><td><code>sign</code></td>
<td>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="ggplot2.html#topic+ggplot">ggplot</a></code>,<code><a href="ggplot2.html#topic+stat_bin">stat_bin</a></code>,<code><a href="ggplot2.html#topic+aes">aes</a></code>,<code><a href="ggplot2.html#topic+scale_fill_manual">scale_fill_manual</a></code>,<code><a href="ggplot2.html#topic+theme">theme</a></code>,<code><a href="ggplot2.html#topic+labs">labs</a></code>,<code><a href="ggplot2.html#topic+geom_point">geom_point</a></code>,<code><a href="ggplot2.html#topic+xlim">xlim</a></code>,<code><a href="ggplot2.html#topic+ylim">ylim</a></code>,<code><a href="ggplot2.html#topic+geom_abline">geom_abline</a></code>,<code><a href="ggplot2.html#topic+geom_line">geom_line</a></code>,<code><a href="ggplot2.html#topic+geom_text">geom_text</a></code>,<code><a href="ggplot2.html#topic+geom_label">geom_label</a></code>,<code><a href="ggplot2.html#topic+coord_fixed">coord_fixed</a></code>
</p>

<hr>
<h2 id='scale_me'>scale_me</h2><span id='topic+scale_me'></span>

<h3>Description</h3>

<p>maps all instances in <code>x</code> to the [0;1] range using the equation:
<br /> y = (x-min)/(max-min)
<br /> If no values for min and max are given, they are calculated per default as min=min(x) and max=max(x)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scale_me(x, min = NULL, max = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="scale_me_+3A_x">x</code></td>
<td>
<p>vector of predictions</p>
</td></tr>
<tr><td><code id="scale_me_+3A_min">min</code></td>
<td>
<p>minimum of <code>x</code>, Default: NULL</p>
</td></tr>
<tr><td><code id="scale_me_+3A_max">max</code></td>
<td>
<p>maximum of <code>x</code>, Default: NULL</p>
</td></tr>
</table>


<h3>Details</h3>

<p>if <code>x</code> is greater (smaller) than <code>max</code> (<code>min</code>), its calibrated prediction is set to 1 (0) and warning is triggered.
</p>


<h3>Value</h3>

<p>scaled values of <code>x</code>
</p>

<hr>
<h2 id='statistics_calibratR'>statistics_calibratR</h2><span id='topic+statistics_calibratR'></span>

<h3>Description</h3>

<p>this method offers a variety of statistical evaluation methods for the output of the <code><a href="#topic+calibrate">calibrate</a></code> method.
All returned error values represent mean error values over the <code>n_seeds</code> times repeated 10-fold CV.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>statistics_calibratR(calibrate_object, t.test_partitions = TRUE,
  significance_models = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="statistics_calibratR_+3A_calibrate_object">calibrate_object</code></td>
<td>
<p>list that is returned from the <code><a href="#topic+calibrate">calibrate</a></code> function. The parameter <code>n_seeds</code> is available as a list component of the <code>calibrate_object</code></p>
</td></tr>
<tr><td><code id="statistics_calibratR_+3A_t.test_partitions">t.test_partitions</code></td>
<td>
<p>Performs a paired two sided t.test over the error values (ECE, CLE1, CLE0, MCE, AUC, sensitivity and specificity) from the
random partition splits comparing a possible significant difference in mean among the calibration models. All models and the original, scaled and transformed values are tested against each other.
The p_value and the effect size of the t.test are returned to the user. Can only be performed, if the <code>calibrate_object</code> contains a <code>summary_CV</code> list object, else, an error is returned.  Default: TRUE</p>
</td></tr>
<tr><td><code id="statistics_calibratR_+3A_significance_models">significance_models</code></td>
<td>
<p>returns important characteristics of the implemented calibration models, Default: TRUE</p>
</td></tr>
</table>


<h3>Details</h3>

<p>DETAILS
</p>


<h3>Value</h3>

<p>An object of class list, with the following components:
</p>
<table role = "presentation">
<tr><td><code>mean_calibration</code></td>
<td>
<p>mean of calibration error values (ECE_equal_width, MCE_equal_width, ECE_equal_freq, MCE_equal_freq, RMSE, Class 1 CLE, Class 0 CLE, Brier Score, Class 1 Brier Score, Class 0 Brier Score) over <code>n_seeds</code> times repeated 10-fold CV.
ECE and MCE are computed once using equal-width and once using equal-frequency binning for the construction of the underlying binning scheme.
Only returned, if <code>calibrate_object</code> contains a summary_CV list object.</p>
</td></tr>
<tr><td><code>standard_deviation</code></td>
<td>
<p>standard deviation of calibration error values over <code>n_seeds</code> times repeated 10-fold CV. Only returned, if <code>calibrate_object</code> contains a summary_CV list object.</p>
</td></tr>
<tr><td><code>var_coeff_calibration</code></td>
<td>
<p>variation coefficient of calibration error values over <code>n_seeds</code> times repeated 10-fold CV. Only returned, if <code>calibrate_object</code> contains a summary_CV list object.</p>
</td></tr>
<tr><td><code>mean_discrimination</code></td>
<td>
<p>mean of discrimination error (sensitivity, specificity, AUC, positive predictive value, negative predictive value, accuracy) values over <code>n_seeds</code> times repeated 10-fold CV. The &quot;cut-off&quot; is
the cut-off value that maximizes sensitivity and specificity. Only returned, if <code>calibrate_object</code> contains a summary_CV list object.</p>
</td></tr>
<tr><td><code>sd_discrimination</code></td>
<td>
<p>standard deviation of discrimination error values over <code>n_seeds</code> times repeated 10-fold CV. Only returned, if <code>calibrate_object</code> contains a summary_CV list object.</p>
</td></tr>
<tr><td><code>var_coeff_discrimination</code></td>
<td>
<p>variation coefficient of discrimination error values over <code>n_seeds</code> times repeated 10-fold CV. Only returned, if <code>calibrate_object</code> contains a summary_CV list object.</p>
</td></tr>
<tr><td><code>t.test_calibration</code></td>
<td>
<p>=list(p_value=t.test.calibration, effect_size=effect_size_calibration), only returned if t.test=TRUE</p>
</td></tr>
<tr><td><code>t.test_discrimination</code></td>
<td>
<p>=list(p_value=t.test.discrimination, effect_size=effect_size_discrimination), only returned if t.test=TRUE</p>
</td></tr>
<tr><td><code>significance_models</code></td>
<td>
<p>only returned if significance_models=TRUE</p>
</td></tr>
<tr><td><code>n_seeds</code></td>
<td>
<p>number of random data set partitions into training and test set for <code>folds</code>-times CV</p>
</td></tr>
<tr><td><code>original_values</code></td>
<td>
<p>list object that consists of the <code>actual</code> and <code>predicted</code> values of the original scores</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Johanna Schwarz
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+t.test">t.test</a></code>,<code><a href="stats.html#topic+friedman.test">friedman.test</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'> ## Loading dataset in environment
 data(example)
 calibration_model &lt;- example$calibration_model

 statistics &lt;- statistics_calibratR(calibration_model)
</code></pre>

<hr>
<h2 id='transform_me'>transform_me</h2><span id='topic+transform_me'></span>

<h3>Description</h3>

<p>maps all instances in <code>x_unscaled</code> to the [0;1] range using the equation:
<br />  y=exp(x)/(1+exp(x))
</p>


<h3>Usage</h3>

<pre><code class='language-R'>transform_me(x_unscaled, mean)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="transform_me_+3A_x_unscaled">x_unscaled</code></td>
<td>
<p>vector of predictions</p>
</td></tr>
<tr><td><code id="transform_me_+3A_mean">mean</code></td>
<td>
<p>mean of <code>x</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>values greater then exp(700)/ or smaller then exp(-700) are returned as &quot;Inf&quot;. To avoid NaN values, these &quot;Inf.&quot; values are turned into min(y) or max(y).
</p>


<h3>Value</h3>

<p>transformed values of <code>x_unscaled</code>
</p>

<hr>
<h2 id='uncalibrated_CV'>uncalibrated_CV</h2><span id='topic+uncalibrated_CV'></span>

<h3>Description</h3>

<p>performs <code>n_folds</code>-CV but with only input-preprocessing the test set. No calibration model is trained and evaluated in this method.
The <code>predicted</code> values are partitioned into n subsets. The training set is constructed on (n-1) subsets; the remaining set is used
for testing. Since no calibration model is used in this method, the test set predictions are only input-preprocessed (either scaled or transformed, depending on <code>input</code>).
All test set predictions are merged and used to compute error metrics for the input-preprocessing methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>uncalibrated_CV(actual, predicted, n_folds = 10, seed, input)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="uncalibrated_CV_+3A_actual">actual</code></td>
<td>
<p>vector of observed class labels (0/1)</p>
</td></tr>
<tr><td><code id="uncalibrated_CV_+3A_predicted">predicted</code></td>
<td>
<p>vector of uncalibrated predictions</p>
</td></tr>
<tr><td><code id="uncalibrated_CV_+3A_n_folds">n_folds</code></td>
<td>
<p>number of folds for the cross-validation, Default: 10</p>
</td></tr>
<tr><td><code id="uncalibrated_CV_+3A_seed">seed</code></td>
<td>
<p>random seed to alternate the split of data set partitions</p>
</td></tr>
<tr><td><code id="uncalibrated_CV_+3A_input">input</code></td>
<td>
<p>specify if the input was scaled or transformed, scaled=1, transformed=2</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list object containing the following components:
</p>
<table role = "presentation">
<tr><td><code>error</code></td>
<td>
<p>list object that summarizes discrimination and calibration errors obtained during the CV</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>&quot;uncalibrated&quot;</p>
</td></tr>
<tr><td><code>probs_CV</code></td>
<td>
<p>vector of input-preprocessed predictions that was used during the CV</p>
</td></tr>
<tr><td><code>actual_CV</code></td>
<td>
<p>respective vector of true values (0 or 1) that was used during the CV</p>
</td></tr>
</table>

<hr>
<h2 id='visualize_calibrated_test_set'>visualize_calibrated_test_set</h2><span id='topic+visualize_calibrated_test_set'></span>

<h3>Description</h3>

<p>plots a panel for all calibrated predictions from the respective calibration model. Allows visual comparison of the models output and their optimal cut off
</p>


<h3>Usage</h3>

<pre><code class='language-R'>visualize_calibrated_test_set(actual, predicted_list, cutoffs)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="visualize_calibrated_test_set_+3A_actual">actual</code></td>
<td>
<p>vector of observed class labels (0/1)</p>
</td></tr>
<tr><td><code id="visualize_calibrated_test_set_+3A_predicted_list">predicted_list</code></td>
<td>
<p>predict_calibratR$predictions object (list of calibrated predictions from calibration models)</p>
</td></tr>
<tr><td><code id="visualize_calibrated_test_set_+3A_cutoffs">cutoffs</code></td>
<td>
<p>vector of optimal cut-off thresholds for each calibration model</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot2 element for visual comparison of the evaluated calibration models
</p>


<h3>See Also</h3>

<p><code><a href="ggplot2.html#topic+ggplot">ggplot</a></code>,<code><a href="ggplot2.html#topic+geom_point">geom_point</a></code>,<code><a href="ggplot2.html#topic+scale_colour_manual">scale_colour_manual</a></code>,<code><a href="ggplot2.html#topic+xlab">xlab</a></code>,<code><a href="ggplot2.html#topic+ylab">ylab</a></code>,<code><a href="ggplot2.html#topic+geom_hline">geom_hline</a></code>,<code><a href="ggplot2.html#topic+ylim">ylim</a></code>
</p>

<hr>
<h2 id='visualize_calibratR'>visualize_calibratR</h2><span id='topic+visualize_calibratR'></span>

<h3>Description</h3>

<p>this method offers a variety of visualisations to compare implemented calibration models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>visualize_calibratR(calibrate_object, visualize_models = FALSE,
  plot_distributions = FALSE, rd_partitions = FALSE,
  training_set_calibrated = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="visualize_calibratR_+3A_calibrate_object">calibrate_object</code></td>
<td>
<p>the list component <code>calibration_models</code> from the <code><a href="#topic+calibrate">calibrate</a></code> method</p>
</td></tr>
<tr><td><code id="visualize_calibratR_+3A_visualize_models">visualize_models</code></td>
<td>
<p>returns the list components <code>plot_calibration_models</code> and <code>plot_single_models</code></p>
</td></tr>
<tr><td><code id="visualize_calibratR_+3A_plot_distributions">plot_distributions</code></td>
<td>
<p>returns a density distribution plot of the calibrated predictions after CV (External) or without CV (internal)</p>
</td></tr>
<tr><td><code id="visualize_calibratR_+3A_rd_partitions">rd_partitions</code></td>
<td>
<p>returns a reliability diagram for each model</p>
</td></tr>
<tr><td><code id="visualize_calibratR_+3A_training_set_calibrated">training_set_calibrated</code></td>
<td>
<p>returns a list of ggplots. Each plot represents the calibrated predictions by the respective calibration model of the training set.
If the list object <code>predictions</code> in the <code>calibrate_object</code> is empty, <code>training_set_calibrated</code> is returned as NULL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class list, with the following components:
</p>
<table role = "presentation">
<tr><td><code>histogram_distribution</code></td>
<td>
<p>returns a histogram of the original ML score distribution</p>
</td></tr>
<tr><td><code>density_calibration_internal</code></td>
<td>
<p>returns a list of density distribution plots for each calibration method, the original
and the two input-preprocessing methods scaling and transforming. The plot visualises the density distribution of the calibrated predictions of the training set. In this case, training and test set values are identical, so be careful to evaluate the plots.</p>
</td></tr>
<tr><td><code>density_calibration_external</code></td>
<td>
<p>returns a list of density distribution plots for each calibration method, the original
and the two input-preprocessing methods scaling and transforming. The plot visualises the density distribution of the calibrated predictions, that were returned during Cross Validation. If more than one repetition of CV was performed,
run number 1 is evaluated</p>
</td></tr>
<tr><td><code>plot_calibration_models</code></td>
<td>
<p> maps the original ML scores to their calibrated prediction estimates for each model.
This enables easy model comparison over the range of ML scores See also <code><a href="#topic+compare_models_visual">compare_models_visual</a></code>. </p>
</td></tr>
<tr><td><code>plot_single_models</code></td>
<td>
<p>returns a list of ggplots for each calibration model, also mapping the original ML scores to their calibrated prediction. Significance values are indicated.
See also <code><a href="#topic+plot_model">plot_model</a></code></p>
</td></tr>
<tr><td><code>rd_plot</code></td>
<td>
<p>returns a list of reliability diagrams for each of the implemented calibration models and the two input-preprocessing methods &quot;scaled&quot; and &quot;transformed&quot;. The returned plot visualises the calibrated predictions that
were returned for the test set during each of the n run of the n-times repeated CV. Each grey line represents one of the n runs. The blue line represents the median of all calibrated bin predictions.
Insignificant bin estimates are indicated with &quot;ns&quot;. If no CV was performed during calibration model building using the <code><a href="#topic+calibrate">calibrate</a></code> method, <code>rd_plot</code> is returned as NULL</p>
</td></tr>
<tr><td><code>calibration_error</code></td>
<td>
<p>returns a list of boxplots for the calibration error metrics ECE, MCE, CLE and RMSE. The n values for each model represent the obtained error values during the
n times repeated CV. If no CV was performed during calibration model building using the <code><a href="#topic+calibrate">calibrate</a></code> method, <code>calibration_error</code> is returned as NULL</p>
</td></tr>
<tr><td><code>discrimination_error</code></td>
<td>
<p>returns a list of boxplots for the discrimination error AUC, sensitivity and specificity. The n values for each model represent the obtained error values during the
n times repeated CV. If no CV was performed during calibration model building using the <code><a href="#topic+calibrate">calibrate</a></code> method, <code>discrimination_error</code> is returned as NULL</p>
</td></tr>
<tr><td><code>cle_class_specific_error</code></td>
<td>
<p>If no CV was performed during calibration model building using the <code><a href="#topic+calibrate">calibrate</a></code> method, <code>cle_class_specific_error</code> is returned as NULL</p>
</td></tr>
<tr><td><code>training_set_calibrated</code></td>
<td>
<p>returns a list of ggplots. Each plot represents the calibrated predictions by the respective calibration model of the training set.
If the list object <code>predictions</code> in the <code>calibrate_object</code> is empty, <code>training_set_calibrated</code> is returned as NULL.</p>
</td></tr>
<tr><td><code>GUESS_1_final_model</code></td>
<td>
<p>plots the the returned conditional probability p(x|Class) values of the GUESS_1 model</p>
</td></tr>
<tr><td><code>GUESS_2_final_model</code></td>
<td>
<p>plots the the returned conditional probability p(x|Class) values of the GUESS_2 model</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Johanna Schwarz
</p>


<h3>See Also</h3>

<p><code><a href="ggplot2.html#topic+ggplot">ggplot</a></code>,<code><a href="ggplot2.html#topic+geom_density">geom_density</a></code>,<code><a href="ggplot2.html#topic+aes">aes</a></code>,<code><a href="ggplot2.html#topic+scale_colour_manual">scale_colour_manual</a></code>,<code><a href="ggplot2.html#topic+scale_fill_manual">scale_fill_manual</a></code>,<code><a href="ggplot2.html#topic+labs">labs</a></code>,<code><a href="ggplot2.html#topic+geom_point">geom_point</a></code>,<code><a href="ggplot2.html#topic+geom_hline">geom_hline</a></code>,<code><a href="ggplot2.html#topic+theme">theme</a></code>,<code><a href="ggplot2.html#topic+element_text">element_text</a></code>
<code><a href="reshape2.html#topic+melt">melt</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Loading dataset in environment
 data(example)
 calibration_model &lt;- example$calibration_model

 visualisation &lt;- visualize_calibratR(calibration_model, plot_distributions=FALSE,
 rd_partitions=FALSE, training_set_calibrated=FALSE)
</code></pre>

<hr>
<h2 id='visualize_distribution'>visualize_distribution</h2><span id='topic+visualize_distribution'></span>

<h3>Description</h3>

<p>FUNCTION_DESCRIPTION
</p>


<h3>Usage</h3>

<pre><code class='language-R'>visualize_distribution(actual, predicted)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="visualize_distribution_+3A_actual">actual</code></td>
<td>
<p>vector of observed class labels (0/1)</p>
</td></tr>
<tr><td><code id="visualize_distribution_+3A_predicted">predicted</code></td>
<td>
<p>vector of uncalibrated predictions</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list object containing the following components:
</p>
<table role = "presentation">
<tr><td><code>plot_distribution</code></td>
<td>
<p>ggplot histogram that visualizes the observed class distributions</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>list object that summarizes all relevant parameters (mean, sd, number) of the observed class distributions</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="ggplot2.html#topic+ggplot">ggplot</a></code>,<code><a href="ggplot2.html#topic+geom_histogram">geom_histogram</a></code>,<code><a href="ggplot2.html#topic+aes">aes</a></code>,<code><a href="ggplot2.html#topic+scale_colour_manual">scale_colour_manual</a></code>,<code><a href="ggplot2.html#topic+scale_fill_manual">scale_fill_manual</a></code>,<code><a href="ggplot2.html#topic+labs">labs</a></code>
</p>

<hr>
<h2 id='visualize_error_boxplot'>visualize_error_boxplot</h2><span id='topic+visualize_error_boxplot'></span>

<h3>Description</h3>

<p>compares error values among different calibration models. A boxplots is created from the n error values that were obtained during the n-times repeated Cross-Validation procedure.
Different error values are implemented and can be compared:
<br /> discrimination error = sensitivity, specificity, accuracy, AUC (when <code>discrimination</code>=TRUE)
<br /> calibration error = ece, mce, rmse, class 0 cle, class 1 cle (when <code>discrimination</code>=FALSE)
For the calculation of the errors, see the respective methods listed in the &quot;see also&quot; section
</p>


<h3>Usage</h3>

<pre><code class='language-R'>visualize_error_boxplot(list_models, discrimination = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="visualize_error_boxplot_+3A_list_models">list_models</code></td>
<td>
<p>list object that contains all error values for all trained calibration models. For the specific format, see the calling function <code><a href="#topic+visualize_calibratR">visualize_calibratR</a></code>.</p>
</td></tr>
<tr><td><code id="visualize_error_boxplot_+3A_discrimination">discrimination</code></td>
<td>
<p>boolean (TRUE or FALSE). If TRUE, discrimination errors are compared between models; if FALSE calibration error is compared, Default: TRUE</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class list, with the following components:
<br /> if <code>discrimination</code>=TRUE
</p>
<table role = "presentation">
<tr><td><code>sens</code></td>
<td>
<p>ggplot2 boxplot that compares all evaluated calibration models with regard to sensitivity.</p>
</td></tr>
<tr><td><code>spec</code></td>
<td>
<p>ggplot2 boxplot that compares all evaluated calibration models with regard to specificity</p>
</td></tr>
<tr><td><code>acc</code></td>
<td>
<p>ggplot2 boxplot that compares all evaluated calibration models with regard to accuracy</p>
</td></tr>
<tr><td><code>auc</code></td>
<td>
<p>ggplot2 boxplot that compares all evaluated calibration models with regard to AUC</p>
</td></tr>
<tr><td><code>list_errors</code></td>
<td>
<p>list object that contains all discrimination error values that were used to construct the boxplots</p>
</td></tr>
</table>
<p><br /> if <code>discrimination</code>=FALSE
</p>
<table role = "presentation">
<tr><td><code>ece</code></td>
<td>
<p>ggplot2 boxplot that compares all evaluated calibration models with regard to expected calibration error</p>
</td></tr>
<tr><td><code>mce</code></td>
<td>
<p>ggplot2 boxplot that compares all evaluated calibration models with regard to maximum expected calibration error (MCE)</p>
</td></tr>
<tr><td><code>rmse</code></td>
<td>
<p>ggplot2 boxplot that compares all evaluated calibration models with regard to root mean square error (RMSE)</p>
</td></tr>
<tr><td><code>cle_0</code></td>
<td>
<p>ggplot2 boxplot that compares all evaluated calibration models with regard to class 0 classification error (CLE)</p>
</td></tr>
<tr><td><code>cle_1</code></td>
<td>
<p>ggplot2 boxplot that compares all evaluated calibration models with regard to class 1 classification error (CLE)</p>
</td></tr>
<tr><td><code>list_errors</code></td>
<td>
<p>list object that contains all calibration error values that were used to construct the boxplots</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="ggplot2.html#topic+ggplot">ggplot</a></code>,<code><a href="ggplot2.html#topic+aes">aes</a></code>,<code><a href="ggplot2.html#topic+ggtitle">ggtitle</a></code>,<code><a href="ggplot2.html#topic+scale_x_discrete">scale_x_discrete</a></code>,<code><a href="ggplot2.html#topic+geom_boxplot">geom_boxplot</a></code>,<code><a href="ggplot2.html#topic+theme">theme</a></code>,<code><a href="ggplot2.html#topic+element_text">element_text</a></code>
<code><a href="reshape2.html#topic+melt">melt</a></code>,<code><a href="#topic+get_CLE_class">get_CLE_class</a></code>,<code><a href="#topic+getECE">getECE</a></code>,<code><a href="#topic+getMCE">getMCE</a></code>,<code><a href="#topic+getRMSE">getRMSE</a></code>, <code><a href="#topic+evaluate_discrimination">evaluate_discrimination</a></code>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
