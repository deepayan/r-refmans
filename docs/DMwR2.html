<!DOCTYPE html><html lang="en"><head><title>Help for package DMwR2</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {DMwR2}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#DMwR2-package'>
<p>Functions and data for the second edition of the book &quot;Data Mining with R&quot;</p></a></li>
<li><a href='#algae'><p>Training data for predicting algae blooms</p></a></li>
<li><a href='#algae.sols'><p>The solutions for the test data set for predicting algae blooms</p></a></li>
<li><a href='#centralImputation'>
<p>Fill in NA values with central statistics</p></a></li>
<li><a href='#centralValue'>
<p>Obtain statistic of centrality</p></a></li>
<li><a href='#createEmbedDS'>
<p>Creates an embeded data set from an univariate time series</p></a></li>
<li><a href='#dist.to.knn'>
<p>An auxiliary function of <code>lofactor()</code></p></a></li>
<li><a href='#GSPC'><p>A set of daily quotes for SP500</p></a></li>
<li><a href='#kNN'>
<p>k-Nearest Neighbour Classification</p></a></li>
<li><a href='#knneigh.vect'>
<p>An auxiliary function of <code>lofactor()</code></p></a></li>
<li><a href='#knnImputation'>
<p>Fill in NA values with the values of the nearest neighbours</p></a></li>
<li><a href='#lofactor'>
<p>An implementation of the LOF algorithm</p></a></li>
<li><a href='#manyNAs'>
<p>Find rows with too many NA values</p></a></li>
<li><a href='#nrLinesFile'>
<p>Counts the number of lines of a file</p></a></li>
<li><a href='#outliers.ranking'>
<p>Obtain outlier rankings</p></a></li>
<li><a href='#reachability'>
<p>An auxiliary function of <code>lofactor()</code></p></a></li>
<li><a href='#rpartXse'>
<p>Obtain a tree-based model</p></a></li>
<li><a href='#rt.prune'>
<p>Prune a tree-based model using the SE rule</p></a></li>
<li><a href='#sales'><p>A data set with sale transaction reports</p></a></li>
<li><a href='#sampleCSV'>
<p>Drawing a random sample of lines from a CSV file</p></a></li>
<li><a href='#sampleDBMS'>
<p>Drawing a random sample of records of a table stored in a DBMS</p></a></li>
<li><a href='#SelfTrain'>
<p>Self train a model on semi-supervised data</p></a></li>
<li><a href='#sigs.PR'>
<p>Precision and recall of a set of predicted trading signals</p></a></li>
<li><a href='#SoftMax'>
<p>Normalize a set of continuous values using SoftMax</p></a></li>
<li><a href='#sp500'><p>A set of daily quotes for SP500 in CSV Format</p></a></li>
<li><a href='#test.algae'><p>Testing data for predicting algae blooms</p></a></li>
<li><a href='#tradeRecord-class'><p>Class &quot;tradeRecord&quot;</p></a></li>
<li><a href='#trading.signals'>
<p>Discretize a set of values into a set of trading signals</p></a></li>
<li><a href='#trading.simulator'>
<p>Simulate daily trading using a set of trading signals</p></a></li>
<li><a href='#tradingEvaluation'>
<p>Obtain a set of evaluation metrics for a set of trading actions</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Functions and Data for the Second Edition of "Data Mining with
R"</td>
</tr>
<tr>
<td>Description:</td>
<td>Functions and data accompanying the second edition of the book  "Data Mining with R, learning with case studies" by Luis Torgo, published by CRC Press.</td>
</tr>
<tr>
<td>Version:</td>
<td>0.0.2</td>
</tr>
<tr>
<td>Depends:</td>
<td>R(&ge; 3.0), methods</td>
</tr>
<tr>
<td>Imports:</td>
<td>xts (&ge; 0.9-7), zoo (&ge; 1.7-10), class (&ge; 7.3-14), rpart (&ge;
4.1-10), quantmod (&ge; 0.4-5), dplyr (&ge; 0.4.3), readr (&ge;
1.0.0), DBI (&ge; 0.5)</td>
</tr>
<tr>
<td>Date:</td>
<td>2016-10-12</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/ltorgo/DMwR2">https://github.com/ltorgo/DMwR2</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/ltorgo/DMwR2/issues">https://github.com/ltorgo/DMwR2/issues</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>LazyData:</td>
<td>yes</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2016-10-12 13:32:44 UTC; ltorgo</td>
</tr>
<tr>
<td>Author:</td>
<td>Luis Torgo [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Luis Torgo &lt;ltorgo@dcc.fc.up.pt&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2016-10-13 00:23:37</td>
</tr>
</table>
<hr>
<h2 id='DMwR2-package'>
Functions and data for the second edition of the book &quot;Data Mining with R&quot;
</h2><span id='topic+DMwR2-package'></span><span id='topic+DMwR2'></span>

<h3>Description</h3>

<p>This package includes functions and data accompanying the book
&quot;Data Mining with R, learning with case studies - 2nd Edition&quot; by Luis Torgo,
published by CRC Press
</p>


<h3>Author(s)</h3>

<p>Luis Torgo
</p>
<p>Maintainer: Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a>
</p>


<h3>References</h3>

<p>Torgo, L. (2016) <em>Data Mining using R: learning with case studies,
second edition</em>,
Chapman &amp; Hall/CRC (ISBN-13: 978-1482234893).
</p>
<p><a href="http://ltorgo.github.io/DMwR2">http://ltorgo.github.io/DMwR2</a>  
</p>

<hr>
<h2 id='algae'>Training data for predicting algae blooms</h2><span id='topic+algae'></span>

<h3>Description</h3>

<p>This data set contains observations on 11 variables as well
as the concentration levels of 7 harmful algae. Values were
measured in several European rivers. The 11 predictor
variables include 3 contextual variables (season, size and
speed) describing the water sample, plus 8 chemical
concentration measurements.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>algae</code></pre>


<h3>Format</h3>

<p>A data frame with 200 observations and 18 columns.</p>


<h3>Source</h3>

<p>ERUDIT <a href="http://www.erudit.de/">http://www.erudit.de/</a> - European Network for Fuzzy Logic and
Uncertainty Modelling in Information Technology. </p>

<hr>
<h2 id='algae.sols'>The solutions for the test data set for predicting algae blooms</h2><span id='topic+algae.sols'></span>

<h3>Description</h3>

<p>This data set contains the values of the 7 harmful algae for
the 140 test observations in the test set <code>test.algae</code>.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>algae.sols</code></pre>


<h3>Format</h3>

<p>A data frame with 140 observations and 7 columns.</p>


<h3>Source</h3>

<p>ERUDIT <a href="http://www.erudit.de/">http://www.erudit.de/</a> - European Network for Fuzzy Logic and
Uncertainty Modelling in Information Technology. </p>

<hr>
<h2 id='centralImputation'>
Fill in NA values with central statistics
</h2><span id='topic+centralImputation'></span>

<h3>Description</h3>

<p>This function fills in any NA value in all columns of a data frame
with the statistic of centrality (given by the function
<code>centralvalue()</code>) of the respective column.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>centralImputation(data)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="centralImputation_+3A_data">data</code></td>
<td>

<p>The data frame
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A new data frame with no NA values
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Torgo, L. (2016) <em>Data Mining using R: learning with case studies,
second edition</em>,
Chapman &amp; Hall/CRC (ISBN-13: 978-1482234893).
</p>
<p><a href="http://ltorgo.github.io/DMwR2">http://ltorgo.github.io/DMwR2</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+knnImputation">knnImputation</a></code>, <code><a href="#topic+centralValue">centralValue</a></code>, <code><a href="stats.html#topic+complete.cases">complete.cases</a></code>, <code><a href="stats.html#topic+na.omit">na.omit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(algae,package="DMwR2")
cleanAlgae &lt;- centralImputation(algae)
summary(cleanAlgae)
</code></pre>

<hr>
<h2 id='centralValue'>
Obtain statistic of centrality
</h2><span id='topic+centralValue'></span>

<h3>Description</h3>

<p>This function obtains a statistic of centrality of a variable given a
sample of its values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>centralValue(x, ws = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="centralValue_+3A_x">x</code></td>
<td>

<p>A vector of values (the sample).
</p>
</td></tr>
<tr><td><code id="centralValue_+3A_ws">ws</code></td>
<td>

<p>A vector of case weights (defaulting to NULL, i.e. no case weights).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the variable is numeric it returns de median of the given sample, if it
is a factor it returns the mode. In other cases it
tries to convert to a factor and then returns the mode.
</p>


<h3>Value</h3>

<p>A number if the variable is numeric. A string with the name of the
most frequent nominal value, otherwise.
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Torgo, L. (2016) <em>Data Mining using R: learning with case studies,
second edition</em>,
Chapman &amp; Hall/CRC (ISBN-13: 978-1482234893).
</p>
<p><a href="http://ltorgo.github.io/DMwR2">http://ltorgo.github.io/DMwR2</a>  
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+mean">mean</a></code>, <code><a href="stats.html#topic+median">median</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'># An example with numerical data
x &lt;- rnorm(100)
centralValue(x)
# An example with nominal data
y &lt;-
factor(sample(1:10,200,replace=TRUE),levels=1:10,labels=paste('v',1:10,sep=''))
centralValue(y)
</code></pre>

<hr>
<h2 id='createEmbedDS'>
Creates an embeded data set from an univariate time series
</h2><span id='topic+createEmbedDS'></span>

<h3>Description</h3>

<p>Function for creating and embeded data set from a univariate time
series given an embed size
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createEmbedDS(s, emb=4)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="createEmbedDS_+3A_s">s</code></td>
<td>

<p>A univariate time series (can be a numeric vector or a xts object)
</p>
</td></tr>
<tr><td><code id="createEmbedDS_+3A_emb">emb</code></td>
<td>

<p>The size of the embed for creating the data set (defaults to 4)
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function creates a data set corresponding to the embed of a
certain size of a given univariate time series.
</p>
<p>For instance for an embed of size 3 each row of the data set will
contain the value of the series at time t, t-1 and t-2.
</p>


<h3>Value</h3>

<p>Either a matrix or a multivariate xts, depending on whether the input
series is a numberic vector or a univariate xts, respectively.
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Torgo, L. (2016) <em>Data Mining using R: learning with case studies,
second edition</em>,
Chapman &amp; Hall/CRC (ISBN-13: 978-1482234893).
</p>
<p><a href="http://ltorgo.github.io/DMwR2">http://ltorgo.github.io/DMwR2</a>  
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+embed">embed</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## A simple example with a random time series
x &lt;- rnorm(100)
head(x)
dataSet &lt;- createEmbedDS(x,emb=5)
head(dataSet)
</code></pre>

<hr>
<h2 id='dist.to.knn'>
An auxiliary function of <code>lofactor()</code>
</h2><span id='topic+dist.to.knn'></span>

<h3>Description</h3>

<p>This function returns an object in which columns contain the
indices of the first k neighbors followed by the distances to each
of these neighbors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dist.to.knn(dataset, neighbors)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dist.to.knn_+3A_dataset">dataset</code></td>
<td>

<p>A data set that will be internally coerced into a matrix.
</p>
</td></tr>
<tr><td><code id="dist.to.knn_+3A_neighbors">neighbors</code></td>
<td>

<p>The number of neighbours.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is strongly based on the code provided by Acuna
et. al. (2009) for the previously available <code>dprep</code> package.
</p>


<h3>Value</h3>

<p>A matrix
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Acuna, E., and Members of the CASTLE group at UPR-Mayaguez, (2009).
<em>dprep: Data preprocessing and visualization functions for classification</em>. R package version 2.1.
</p>
<p>Torgo, L. (2016) <em>Data Mining using R: learning with case studies,
second edition</em>,
Chapman &amp; Hall/CRC (ISBN-13: 978-1482234893).
</p>
<p><a href="http://ltorgo.github.io/DMwR2">http://ltorgo.github.io/DMwR2</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lofactor">lofactor</a></code>
</p>

<hr>
<h2 id='GSPC'>A set of daily quotes for SP500</h2><span id='topic+GSPC'></span>

<h3>Description</h3>

<p>This is a <code>xts</code> object containing the daily quotes of
the SP500 sotck index from 1970-01-02 till 2009-09-15 (10,022
daily sessions). For each day information is given on the
Open, High, Low and Close prices, and also for the Volume and
Adjusted close price.   
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GSPC</code></pre>


<h3>Format</h3>

<p>A <code>xts</code> object with a data matrix with 10,022
rows and 6 columns.</p>


<h3>Source</h3>

<p>Yahoo Finance   </p>

<hr>
<h2 id='kNN'>
k-Nearest Neighbour Classification
</h2><span id='topic+kNN'></span>

<h3>Description</h3>

<p>This function provides a formula interface to the existing
<code>knn()</code> function of package <code>class</code>. On top of this type of
convinient interface, the function also allows standardization of the
given data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kNN(form, train, test, stand = TRUE, stand.stats = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kNN_+3A_form">form</code></td>
<td>

<p>An object of the class <code>formula</code> describing the functional form
of the classification model.
</p>
</td></tr>
<tr><td><code id="kNN_+3A_train">train</code></td>
<td>

<p>The data to be used as training set.
</p>
</td></tr>
<tr><td><code id="kNN_+3A_test">test</code></td>
<td>

<p>The data set for which we want to obtain the k-NN classification,
i.e. the test set.
</p>
</td></tr>
<tr><td><code id="kNN_+3A_stand">stand</code></td>
<td>

<p>A boolean indicating whether the training data should be previously
normalized before obtaining the k-NN predictions (defaults to TRUE).
</p>
</td></tr>
<tr><td><code id="kNN_+3A_stand.stats">stand.stats</code></td>
<td>

<p>This argument allows the user to supply the centrality and spread
statistics that will drive the standardization. If not supplied they
will default to the statistics used in the function <code>scale()</code>. If
supplied they should be a list with two components, each beig a vector
with as many positions as there are columns in the data set. The
first vector should contain the centrality statistics for each column,
while the second vector should contain the spread statistc values.
</p>
</td></tr>
<tr><td><code id="kNN_+3A_...">...</code></td>
<td>

<p>Any other parameters that will be forward to the <code>knn()</code>
function of package <code>class</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is essentially a convenience function that provides a
formula-based interface to the already existing <code>knn()</code>
function of package <code>class</code>. On top of this type of interface it
also incorporates some facilities in terms of standardization of the
data before the k-nearest neighbour classification algorithm is
applied. This algorithm is based on the distances between
observations, which are known to be very sensitive to different scales
of the variables and thus the usefulness of standardization.
</p>


<h3>Value</h3>

<p>The return value is the same as in the <code>knn()</code>
function of package <code>class</code>. This is a factor of classifications
of the test set cases. 
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Torgo, L. (2016) <em>Data Mining using R: learning with case studies,
second edition</em>,
Chapman &amp; Hall/CRC (ISBN-13: 978-1482234893).
</p>
<p><a href="http://ltorgo.github.io/DMwR2">http://ltorgo.github.io/DMwR2</a>  
</p>


<h3>See Also</h3>

<p><code><a href="class.html#topic+knn">knn</a></code>, <code><a href="class.html#topic+knn1">knn1</a></code>, <code><a href="class.html#topic+knn.cv">knn.cv</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## A small example with the IRIS data set
data(iris)

## Split in train + test set
idxs &lt;- sample(1:nrow(iris),as.integer(0.7*nrow(iris)))
trainIris &lt;- iris[idxs,]
testIris &lt;- iris[-idxs,]

## A 3-nearest neighbours model with no standardization
nn3 &lt;- kNN(Species ~ .,trainIris,testIris,stand=FALSE,k=3)

## The resulting confusion matrix
table(testIris[,'Species'],nn3)

## Now a 5-nearest neighbours model with standardization
nn5 &lt;- kNN(Species ~ .,trainIris,testIris,stand=TRUE,k=5)

## The resulting confusion matrix
table(testIris[,'Species'],nn5)


</code></pre>

<hr>
<h2 id='knneigh.vect'>
An auxiliary function of <code>lofactor()</code>
</h2><span id='topic+knneigh.vect'></span>

<h3>Description</h3>

<p>Function that returns the distance from a vector <code>x</code> to    its
k-nearest-neighbors in the matrix <code>data</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>knneigh.vect(x, data, k)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="knneigh.vect_+3A_x">x</code></td>
<td>

<p>An observation.
</p>
</td></tr>
<tr><td><code id="knneigh.vect_+3A_data">data</code></td>
<td>

<p>A data set that will be internally coerced into a matrix.
</p>
</td></tr>
<tr><td><code id="knneigh.vect_+3A_k">k</code></td>
<td>

<p>The number of neighbours.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is strongly based on the code provided by Acuna
et. al. (2009) for the previously available <code>dprep</code> package.
</p>


<h3>Value</h3>

<p>A vector.
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Torgo, L. (2016) <em>Data Mining using R: learning with case studies,
second edition</em>,
Chapman &amp; Hall/CRC (ISBN-13: 978-1482234893).
</p>
<p><a href="http://ltorgo.github.io/DMwR2">http://ltorgo.github.io/DMwR2</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lofactor">lofactor</a></code>
</p>

<hr>
<h2 id='knnImputation'>
Fill in NA values with the values of the nearest neighbours
</h2><span id='topic+knnImputation'></span>

<h3>Description</h3>

<p>Function that fills in all NA values using the k Nearest
Neighbours of each case with NA values.
By default it uses the values of the neighbours and 
obtains an weighted (by the distance to the case) average
of their values to fill in the unknows.
If meth='median' it uses the median/most frequent value,
instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>knnImputation(data, k = 10, scale = TRUE, meth = "weighAvg",
              distData = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="knnImputation_+3A_data">data</code></td>
<td>

<p>A data frame with the data set
</p>
</td></tr>
<tr><td><code id="knnImputation_+3A_k">k</code></td>
<td>

<p>The number of nearest neighbours to use (defaults to 10)
</p>
</td></tr>
<tr><td><code id="knnImputation_+3A_scale">scale</code></td>
<td>

<p>Boolean setting if the data should be scale before finding the nearest neighbours (defaults
to T)
</p>
</td></tr>
<tr><td><code id="knnImputation_+3A_meth">meth</code></td>
<td>

<p>String indicating the method used to calculate the value to fill in each
NA. Available values are 'median' or 'weighAvg' (the default).
</p>
</td></tr>
<tr><td><code id="knnImputation_+3A_distdata">distData</code></td>
<td>

<p>Optionally you may sepecify here a data frame containing the data set
that should be used to find the neighbours. This is usefull when
filling in NA values on a test set, where you should use only
information from the training set. This defaults to NULL, which means
that the neighbours will be searched in <code>data</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function uses the k-nearest neighbours to fill in the unknown (NA)
values in a data set. For each case with any NA value it will search for
its k most similar cases and use the values of these cases to fill in
the unknowns.
</p>
<p>If <code>meth='median'</code>  the function will use either the median (in
case of numeric variables) or the most frequent value (in case of
factors), of the neighbours to fill in the NAs. If
<code>meth='weighAvg'</code> the function will use a weighted average of the
values of the neighbours. The weights are given by <code>exp(-dist(k,x)</code>
where <code>dist(k,x)</code> is the euclidean distance between the case with
NAs (x) and the neighbour k.
</p>


<h3>Value</h3>

<p>A data frame without NA values
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Torgo, L. (2016) <em>Data Mining using R: learning with case studies,
second edition</em>,
Chapman &amp; Hall/CRC (ISBN-13: 978-1482234893).
</p>
<p><a href="http://ltorgo.github.io/DMwR2">http://ltorgo.github.io/DMwR2</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+centralImputation">centralImputation</a></code>, <code><a href="#topic+centralValue">centralValue</a></code>, <code><a href="stats.html#topic+complete.cases">complete.cases</a></code>, <code><a href="stats.html#topic+na.omit">na.omit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(algae)
cleanAlgae &lt;- knnImputation(algae)
summary(cleanAlgae)
</code></pre>

<hr>
<h2 id='lofactor'>
An implementation of the LOF algorithm
</h2><span id='topic+lofactor'></span>

<h3>Description</h3>

<p>This function obtain local outlier factors using the LOF
algorithm. Namely, given a data set it produces a vector of local
outlier factors for each case.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lofactor(data, k)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lofactor_+3A_data">data</code></td>
<td>

<p>A data set that will be internally coerced into a matrix.
</p>
</td></tr>
<tr><td><code id="lofactor_+3A_k">k</code></td>
<td>

<p>The number of neighbours that will be used in the calculation of the
local outlier factors.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function re-implements the code previously made available in the
<code>dprep</code> package (Acuna et. al., 2009) that was removed from
CRAN. This code in turn is an implementation of the LOF method by
Breunig et. al. (2000). See this reference to understand the full
details on how these local outlier factors are calculated for each
case in a data set.
</p>


<h3>Value</h3>

<p>The function returns a vector of local outlier factors (numbers). This
vector has as many values as there are rows in the original data set.
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Acuna, E., and Members of the CASTLE group at UPR-Mayaguez, (2009).
<em>dprep: Data preprocessing and visualization functions for classification</em>. R package version 2.1.
</p>
<p>Breunig, M., Kriegel, H., Ng, R., and Sander, J. (2000). <em>LOF: identifying
density-based local outliers</em>. In ACM Int. Conf. on Management of Data,
pages 93-104.
</p>
<p>Torgo, L. (2016) <em>Data Mining using R: learning with case studies,
second edition</em>,
Chapman &amp; Hall/CRC (ISBN-13: 978-1482234893).
</p>
<p><a href="http://ltorgo.github.io/DMwR2">http://ltorgo.github.io/DMwR2</a>  
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
lof.scores &lt;- lofactor(iris[,-5],10)
</code></pre>

<hr>
<h2 id='manyNAs'>
Find rows with too many NA values
</h2><span id='topic+manyNAs'></span>

<h3>Description</h3>

<p>Small utility function to obtain the number of the rows
in a data frame that have  a &quot;large&quot; number of 
unknown values.
&quot;Large&quot; can be defined either as a proportion of the
number of columns or as the number in itself.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>manyNAs(data, nORp = 0.2)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="manyNAs_+3A_data">data</code></td>
<td>

<p>A data frame with the data set.
</p>
</td></tr>
<tr><td><code id="manyNAs_+3A_norp">nORp</code></td>
<td>

<p>A number controlling when a row is considered to have too many NA values
(defaults to 0.2, i.e. 20% of the columns). If no rows satisfy the
constraint indicated by the user, a warning is generated.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector with the IDs of the rows with too many NA values. If there are
no rows with many NA values and error is generated.
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Torgo, L. (2016) <em>Data Mining using R: learning with case studies,
second edition</em>,
Chapman &amp; Hall/CRC (ISBN-13: 978-1482234893).
</p>
<p><a href="http://ltorgo.github.io/DMwR2">http://ltorgo.github.io/DMwR2</a>  
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+complete.cases">complete.cases</a></code>, <code><a href="stats.html#topic+na.omit">na.omit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(algae)
manyNAs(algae)
</code></pre>

<hr>
<h2 id='nrLinesFile'>
Counts the number of lines of a file
</h2><span id='topic+nrLinesFile'></span>

<h3>Description</h3>

<p>Function for counting the number of lines of very large text
files. Note that it only works on unix-based systems as it uses the
<code>wc</code> command line utility
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nrLinesFile(f)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nrLinesFile_+3A_f">f</code></td>
<td>

<p>A file name (a string)
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function creates a data set corresponding to the embed of a
certain size of a given univariate time series.
</p>
<p>For instance for an embed of size 3 each row of the data set will
contain the value of the series at time t, t-1 and t-2.
</p>


<h3>Value</h3>

<p>An integer
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Torgo, L. (2016) <em>Data Mining using R: learning with case studies,
second edition</em>,
Chapman &amp; Hall/CRC (ISBN-13: 978-1482234893).
</p>
<p><a href="http://ltorgo.github.io/DMwR2">http://ltorgo.github.io/DMwR2</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sampleCSV">sampleCSV</a></code>
</p>

<hr>
<h2 id='outliers.ranking'>
Obtain outlier rankings
</h2><span id='topic+outliers.ranking'></span>

<h3>Description</h3>

<p>This function uses hierarchical clustering to obtain a ranking of
outlierness for a  set of cases. The ranking is obtained on the basis
of the path each case follows within the merging steps of a
agglomerative hierarchical clustering method. See the references for
further technical details on how these rankings are obtained.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>outliers.ranking(data, test.data = NULL, method = "sizeDiff",
                 method.pars = NULL,
                 clus = list(dist = "euclidean",alg = "hclust",
                             meth = "ward.D"),
                 power = 1, verb = F)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="outliers.ranking_+3A_data">data</code></td>
<td>

<p>The data set to be ranked according to outlyingness. This parameter
can also be the distance matrix of your additional data set, in case
you wish to calculate these distances &quot;outside&quot; of this function.
</p>
</td></tr>
<tr><td><code id="outliers.ranking_+3A_test.data">test.data</code></td>
<td>

<p>If a data set is provided in this argument, then the rankings are
obtained for these cases and not for the cases provided in the
argument <code>data</code>. The clustering process driving the obtention of
the rankings is carried out on the union of the two sets of data
(<code>data</code> and <code>test.data</code>), but the resulting outlier ranking
factors are only for the observations belonging to this set. This
parameter defaults to <code>NULL</code>.
</p>
</td></tr>
<tr><td><code id="outliers.ranking_+3A_method">method</code></td>
<td>

<p>The method used to obtain the outlier ranking factors (see the
Details section). Defaults to <code>"sizeDiff"</code>.
</p>
</td></tr>
<tr><td><code id="outliers.ranking_+3A_method.pars">method.pars</code></td>
<td>

<p>A list with the parameter values specific to the method selected for
obtaining the outlier ranks (see the Details section).
</p>
</td></tr>
<tr><td><code id="outliers.ranking_+3A_clus">clus</code></td>
<td>

<p>This is a list that provides several parameters of the clustering
process that drives the calculation of the outlier raking factors. If
the parameter <code>data</code> is not a distance function, then this list
should contain a component named <code>dist</code> with a value that should
be one of the possible values of the parameter <code>method</code> the the
function <code>dist()</code> (see the help of this function for further
details). The list should also contain a component named <code>alg</code>
with the name of the clustering algorithm that should be
used. Currently, valid names are either &quot;hclust&quot; (the default) or
&quot;diana&quot;. Finally, in case the clustering algorithm is &quot;hclust&quot; then
the list should also contain a component named <code>meth</code> with the
name of the agglomerative method to use in the hierarchical clustering
algorithm. This should be a valid value of the parameter <code>method</code>
of the function <code>hclust()</code> (check its help page for further
details). 
</p>
</td></tr>
<tr><td><code id="outliers.ranking_+3A_power">power</code></td>
<td>

<p>Integer value. It allows to raise the distance matrix to some power
with the goal of &quot;amplifying&quot; the distance values (defaults to 1).
</p>
</td></tr>
<tr><td><code id="outliers.ranking_+3A_verb">verb</code></td>
<td>

<p>Boolean value that determines the level of verbosity of the function (default to FALSE).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function produces outlier ranking factors for a set of cases. The
methodology used for obtaining these factors is described in Section
4.4.1.3 of the book Data Mining with R (Torgo, 2010) and more details
can be obtained in Torgo (2007). The methodology is based on the
simple idea of using the information provided by an agglomerative
hierarchical clustering algorithm to infer the degree of outlyingness
of the observations. The basic assumption is that outliers should
offer &quot;more resistance&quot; to being clustered, i.e. being merged on large
groups of observations.
</p>
<p>The function was written to be used with the outcome of the
<code>hclust()</code> R function that implements several agglomerative
clustering methods. Although in theory the methodology could be used with
any other agglomerative hierarchical clustering algorithm, the fact is
that the code of this implementation strongly depends on the data
structures produced by the <code>hclust()</code> function. As such if you
wish to change the function to be able to use other clustering
algorithms you should ensure that the data structures it produces are
compatible with the requirements of our function. Specifically, your
clustering algorithm should produce a list with a component named
<code>merge</code> that should be a matrix describing the merging steps of
the clustering process (see the help page of the <code>hclust()</code>
function for a full description of this data structure). This is the
only data structure that is required by our function and that is used
from the object returned by clustering algorithm. The <code>diana()</code> clustering algorithm
also produces this type of information and thus can also be used with
our function by providing the value &quot;diana&quot; on the component
<code>alg</code> of the list forming the parameter <code>clus</code>.
</p>
<p>There are essentially two ways of using this function. The first
consists in giving it a data set on the parameter <code>data</code> and the
function will rank these observations according to their
outlyingness. The other consists in specifying two sets of data. One
is the set for which you want the outlyingness factors that should be given  on the
parameter <code>test.data</code>. The second  set is provided on the
<code>data</code> parameter and it is used  to increase
the ammount of data used in the clustering process to improve the
statistical reliability of the process.
</p>
<p>In the first way of using this function that was described above the
user can either supply the data set or the respective distance matrix.
If the data set is provided then the user should specify the type of
distance metric it should be used to calculate the distances between
the observations. This is done by including a distance calculation
method in the &quot;dist&quot; component of the list provided in parameter
<code>clus</code>. This method should be a valid value of the parameter
<code>method</code> of the R function <code>dist()</code> (see its help for details).
</p>
<p>This function currently implements three different methods for obtaing
outlier ranking factors from the clustering process. These are:
&quot;linear&quot;, &quot;sigmoid&quot; and &quot;sizeDiff&quot; (the default). Irrespectively, of
this method the outlyingness factor of observation X is obtained by:
OF_H(X) = max_i of_i(X), where i represents the different merging
steps of the clustering process and it goes from 1 to N-1, where N is
the size of the data set to be clustered. The three methods differ in
the way they calculate of_i(X) for each merging step. In  the &quot;linear&quot;
method of_i(X) = i / (N-1) * p(|g|), where g is the group to which X
belongs at the merging step i (each merging step involves two groups),
and |g| is the size of that group. The function p() is a penalization
factor depending on the size of the group. The larger this size the
smaller the value of p(), p(s) = I(s &lt; thr) * ( 1 - (s-1) / (N-2)),
where I() is an indicator function and thr is a threshold defined as
perc*N. The user should set the value of perc by including a component
named &quot;sz.perc&quot; in the list provided in the parameter
<code>method.pars</code>. In the &quot;sigmoid&quot; method of_i(X) = exp( -2 * (i -
(N-1))^2 / (N-1)^2) * p(|g|), where the p() function has the same
meaning as in the &quot;linear&quot; method but this time is defined as p(s) =
I(s &lt; 2*thr) * ( 1 - exp( -4 * (s-2*thr)^2 / (2*thr)^2)). Again thr is
perc*N and the user must set the value of perc by including a component
named &quot;sz.perc&quot; in the list provided in the parameter
<code>method.pars</code>. Finally, the method &quot;sizeDiff&quot; defines of_i(X) =
max ( 0, ( |g_y,i| - |g_x,i| ) / ( |g_y,i| + |g_x,i| ) ),
where g_y,i and g_x,i are the two groups involved in the merge at
step i, and g_x,i is the group which X belongs to. Note that if X
belongs to the larger of the two groups this will get X a value of
of_i() equals to zero.
</p>


<h3>Value</h3>

<p>The result of this function is a list with four components. Component
<code>rank.outliers</code> contains a vector with as many positions as there
are cases to rank, where position i of the vector contains the rank
order of the observation i. Component <code>prob.outliers</code> is another
vector with the same size this time containing the outlyingness factor
(the value of OF_H(X) described in the Details section) of each
observation. Component <code>h</code> contains the object returned by the
clustering process. Finally, component <code>dist</code> contains the
distance matrix used i nthe clustering process.
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Torgo, L. (2016) <em>Data Mining using R: learning with case studies,
second edition</em>,
Chapman &amp; Hall/CRC (ISBN-13: 978-1482234893).
</p>
<p><a href="http://ltorgo.github.io/DMwR2">http://ltorgo.github.io/DMwR2</a>  
</p>
<p>Torgo, L. (2007) : <em>Resource-bounded Fraud Detection</em>, 
in Progress in Artificial Intelligence, 13th Portuguese Conference on Artificial Intelligence, EPIA 2007, Neves et. al (eds.). LNAI, Springer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Some examples with algae frequencies in water samples
data(algae)

## Trying to obtain a reanking of the 200 samples
o &lt;- outliers.ranking(algae)

## As you may have observed the function complained about some problem
## with the dist() function. The problem is that the algae data frame
## contains columns (the first 3) that are factors and the dist() function
## assumes all numeric data.
## We can solve the problem by calculating the distance matrix "outside"
## using the daisy() function that handles mixed-mode data, as show in
## the code below that requires the R package "cluster" to be available
## dm &lt;- daisy(algae)
## o &lt;- outliers.ranking(dm)

## Now let us check the outlier ranking factors ordered by decreasing
## score of outlyingness
o$prob.outliers[o$rank.outliers]

## Another example with detection of fraudulent transactions
data(sales)

## trying to obtain the outlier ranks for the set of transactions of a
## salesperson regarding one particular product, taking into
## consideration the overall existing transactions of that product
s &lt;- sales[sales$Prod == 'p1',c(1,3:4)]  # transactions of product p1
tr &lt;- na.omit(s[s$ID != 'v431',-1])      # all except salesperson v431
ts &lt;- na.omit(s[s$ID == 'v431',-1])

o &lt;- outliers.ranking(data=tr,test.data=ts,
         clus=list(dist='euclidean',alg='hclust',meth='average'))
# The outlyingness factor of the transactions of this salesperson
o$prob.outliers

</code></pre>

<hr>
<h2 id='reachability'>
An auxiliary function of <code>lofactor()</code>
</h2><span id='topic+reachability'></span>

<h3>Description</h3>

<p>This function computes the reachability measure for each instance
of a dataset. This result is used later to compute the Local
Outlyingness Factor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reachability(distdata, k)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="reachability_+3A_distdata">distdata</code></td>
<td>

<p>The matrix of distances.
</p>
</td></tr>
<tr><td><code id="reachability_+3A_k">k</code></td>
<td>

<p>The  number of neighbors.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is strongly based on the code provided by Acuna
et. al. (2009) for the previously available <code>dprep</code> package.
</p>


<h3>Value</h3>

<p>A vector.
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Acuna, E., and Members of the CASTLE group at UPR-Mayaguez, (2009).
<em>dprep: Data preprocessing and visualization functions for classification</em>. R package version 2.1.
</p>
<p>Torgo, L. (2016) <em>Data Mining using R: learning with case studies,
second edition</em>,
Chapman &amp; Hall/CRC (ISBN-13: 978-1482234893).
</p>
<p><a href="http://ltorgo.github.io/DMwR2">http://ltorgo.github.io/DMwR2</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lofactor">lofactor</a></code>
</p>

<hr>
<h2 id='rpartXse'>
Obtain a tree-based model
</h2><span id='topic+rpartXse'></span>

<h3>Description</h3>

<p>This function is based on the tree-based framework provided by the
<code>rpart</code> package (Therneau et. al. 2010). It basically, integrates
the tree growth and tree post-pruning in a single function call. The
post-pruning phase is essentially the 1-SE rule described in the CART
book (Breiman et. al. 1984). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rpartXse(form, data, se = 1, cp = 0, minsplit = 6, verbose = F, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rpartXse_+3A_form">form</code></td>
<td>

<p>A formula describing the prediction problem
</p>
</td></tr>
<tr><td><code id="rpartXse_+3A_data">data</code></td>
<td>

<p>A data frame containg the training data to be used to obtain the
tree-based model
</p>
</td></tr>
<tr><td><code id="rpartXse_+3A_se">se</code></td>
<td>

<p>A value with the number of standard errors to use in the post-pruning
of the tree using the SE rule (defaults to 1)
</p>
</td></tr>
<tr><td><code id="rpartXse_+3A_cp">cp</code></td>
<td>

<p>A value that controls the stopping criteria used to stop the initial
tree growth (defaults to 0)
</p>
</td></tr>
<tr><td><code id="rpartXse_+3A_minsplit">minsplit</code></td>
<td>

<p>A value that controls the stopping criteria used to stop the initial
tree growth (defaults to 6)
</p>
</td></tr>
<tr><td><code id="rpartXse_+3A_verbose">verbose</code></td>
<td>

<p>The level of verbosity of the function (defaults to F)
</p>
</td></tr>
<tr><td><code id="rpartXse_+3A_...">...</code></td>
<td>

<p>Any other arguments that are passed to the <code>rpart()</code> function
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The x-SE rule for tree post-pruning is based on the 
cross-validation estimates of the error of the sub-trees of the
initially grown tree, together with the
standard errors of these estimates. These values are used to select
the final tree model. Namely, the selected tree is the smallest tree
with estimated error less than the B+x*SE, where B is the lowest
estimate of error and SE is the standard error of this B estimate.
</p>


<h3>Value</h3>

<p>A <code>rpart</code> object
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Therneau, T. M. and Atkinson, B.; port by Brian
Ripley. (2010). <em>rpart: Recursive Partitioning</em>. R package version
3.1-46.
</p>
<p>Breiman, L., Friedman, J., Olshen, R., and Stone,
C. (1984). <em>Classification and regression trees</em>. Statistics/Probability Series. Wadsworth &amp; Brooks/Cole
Advanced Books &amp; Software.
</p>
<p>Torgo, L. (2016) <em>Data Mining using R: learning with case studies,
second edition</em>,
Chapman &amp; Hall/CRC (ISBN-13: 978-1482234893).
</p>
<p><a href="http://ltorgo.github.io/DMwR2">http://ltorgo.github.io/DMwR2</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rt.prune">rt.prune</a></code>, <code><a href="rpart.html#topic+rpart">rpart</a></code>, <code><a href="rpart.html#topic+prune.rpart">prune.rpart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
tree &lt;- rpartXse(Species ~ ., iris)
tree

## A visual representation of the classification tree
## Not run: 
prettyTree(tree)

## End(Not run)
</code></pre>

<hr>
<h2 id='rt.prune'>
Prune a tree-based model using the SE rule
</h2><span id='topic+rt.prune'></span>

<h3>Description</h3>

<p>This function implements the SE post pruning rule described in the
CART book (Breiman et. al., 1984)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rt.prune(tree, se = 1, verbose = T, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rt.prune_+3A_tree">tree</code></td>
<td>

<p>An <code>rpart</code> object
</p>
</td></tr>
<tr><td><code id="rt.prune_+3A_se">se</code></td>
<td>

<p>The value of the SE threshold (defaulting to 1)
</p>
</td></tr>
<tr><td><code id="rt.prune_+3A_verbose">verbose</code></td>
<td>

<p>The level of verbosity (defaulting to T)
</p>
</td></tr>
<tr><td><code id="rt.prune_+3A_...">...</code></td>
<td>

<p>Any other arguments passed to the function <code>prune.rpart()</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The x-SE rule for tree post-pruning is based on the 
cross-validation estimates of the error of the sub-trees of the
initially grown tree, together with the
standard errors of these estimates. These values are used to select
the final tree model. Namely, the selected tree is the smallest tree
with estimated error less than the B+x*SE, where B is the lowest
estimate of error and SE is the standard error of this B estimate.
</p>


<h3>Value</h3>

<p>A <code>rpart</code> object
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Breiman, L., Friedman, J., Olshen, R., and Stone,
C. (1984). <em>Classification and regression trees</em>. Statistics/Probability Series. Wadsworth &amp; Brooks/Cole
Advanced Books &amp; Software.
</p>
<p>Torgo, L. (2016) <em>Data Mining using R: learning with case studies,
second edition</em>,
Chapman &amp; Hall/CRC (ISBN-13: 978-1482234893).
</p>
<p><a href="http://ltorgo.github.io/DMwR2">http://ltorgo.github.io/DMwR2</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rt.prune">rt.prune</a></code>, <code><a href="rpart.html#topic+rpart">rpart</a></code>, <code><a href="rpart.html#topic+prune.rpart">prune.rpart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
tree &lt;- rpartXse(Species ~ ., iris)
tree

## A visual representation of the classification tree
## Not run: 
prettyTree(tree)

## End(Not run)
</code></pre>

<hr>
<h2 id='sales'>A data set with sale transaction reports</h2><span id='topic+sales'></span>

<h3>Description</h3>

<p>This data frame contains 401,146 transaction reports. Each
report is made by a salesperson identified by an ID and
reports the quantity sold of some product. The data set
caontins information on 5 variables: ID (salesperson ID), Prod
(product ID), Quant (the sold quantity), Val (the reported
value of the transaction) and Insp (a factor containing
information on a inspection of the report with possible values
'ok',' fraud' or 'unkn').
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sales</code></pre>


<h3>Format</h3>

<p>A data frame with 401,146 rows and 5 columns</p>


<h3>Source</h3>

<p>Undisclosed</p>

<hr>
<h2 id='sampleCSV'>
Drawing a random sample of lines from a CSV file
</h2><span id='topic+sampleCSV'></span>

<h3>Description</h3>

<p>Function for obtaining a random sample of lines from a very large CSV
file, whitout having to load in the full data into memory. Targets
situations where the full data does not fit in the computer memory so
usage of the standard <code>sample</code> function is not possible.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sampleCSV(file, percORn, nrLines, header=TRUE, mxPerc=0.5)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sampleCSV_+3A_file">file</code></td>
<td>

<p>A file name (a string)
</p>
</td></tr>
<tr><td><code id="sampleCSV_+3A_percorn">percORn</code></td>
<td>

<p>Either the percentage of number of rows of the file or the actual
number of rows, the sample should have
</p>
</td></tr>
<tr><td><code id="sampleCSV_+3A_nrlines">nrLines</code></td>
<td>

<p>Optionally you may indicate the number of rows of the file if you
know it before-hand, otherwise the function will count them for you
</p>
</td></tr>
<tr><td><code id="sampleCSV_+3A_header">header</code></td>
<td>

<p>Whether the file has a header line or not (a Boolean value)
</p>
</td></tr>
<tr><td><code id="sampleCSV_+3A_mxperc">mxPerc</code></td>
<td>

<p>A maximum threshold for the percentage the sample is allowed to have
(defaults to 0.5)
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function can be used to draw a random sample of lines from a very
large CSV file. This is particularly usefull when you can not afford
to load the file into memory to use R functions like <code>sample</code> to
obtain the sample.
</p>
<p>The function obtains the sample of rows without actually loading the
full data into memory - only the final sample is loaded into main
memory.
</p>
<p>The function is based on unix-based utility programs (<code>perl</code> and <code>wc</code>) so
it is limited to this type of platforms. The function will not run on
other platforms (it will check the system variable <code>.Platform$OS.type</code>), although you may wish to check the function code and
see if you can adapt it to your platform.
</p>


<h3>Value</h3>

<p>A data frame
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Torgo, L. (2016) <em>Data Mining using R: learning with case studies,
second edition</em>,
Chapman &amp; Hall/CRC (ISBN-13: 978-1482234893).
</p>
<p><a href="http://ltorgo.github.io/DMwR2">http://ltorgo.github.io/DMwR2</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nrLinesFile">nrLinesFile</a></code>, <code><a href="base.html#topic+sample">sample</a></code>, <code><a href="#topic+sampleDBMS">sampleDBMS</a></code>
</p>

<hr>
<h2 id='sampleDBMS'>
Drawing a random sample of records of a table stored in a DBMS
</h2><span id='topic+sampleDBMS'></span>

<h3>Description</h3>

<p>Function for obtaining a random sample of records from a very large
table stored in a databased managment system, whitout having to load
in the full table into memory. Targets  situations where the full data
does not fit in the computer memory so 
usage of the standard <code>sample</code> function is not possible.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sampleDBMS(dbConn, tbl, percORn, mxPerc=0.5)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sampleDBMS_+3A_dbconn">dbConn</code></td>
<td>

<p>A data based connection object from the <code>DBI</code> package, that
contains the result of establishing the connection to your target
database in the respective database managment system.
</p>
</td></tr>
<tr><td><code id="sampleDBMS_+3A_tbl">tbl</code></td>
<td>

<p>A string containing the name of the (large) table in the database from which
you want draw a random sample of records.
</p>
</td></tr>
<tr><td><code id="sampleDBMS_+3A_percorn">percORn</code></td>
<td>

<p>Either the percentage of number of rows of the file or the actual
number of rows, the sample should have
</p>
</td></tr>
<tr><td><code id="sampleDBMS_+3A_mxperc">mxPerc</code></td>
<td>

<p>A maximum threshold for the percentage the sample is allowed to have
(defaults to 0.5)
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function can be used to draw a random sample of records from a very
large table of a database managment system. This is particularly
usefull when you can not afford 
to load the full table into memory to use R functions like <code>sample</code> to
obtain the sample.
</p>
<p>The function obtains the sample of rows without actually loading the
full data into memory - only the final sample is loaded into main
memory.
</p>
<p>The function assumes you have alread established and opened a
connection to the database and receives as argument the DBI connection
object. 
</p>


<h3>Value</h3>

<p>A data frame
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Torgo, L. (2016) <em>Data Mining using R: learning with case studies,
second edition</em>,
Chapman &amp; Hall/CRC (ISBN-13: 978-1482234893).
</p>
<p><a href="http://ltorgo.github.io/DMwR2">http://ltorgo.github.io/DMwR2</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sampleCSV">sampleCSV</a></code>, <code><a href="base.html#topic+sample">sample</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## A simple example over a table on a MySQL database
## Not run: 
library(DBI)
library(RMySQL)
drv &lt;- dbDriver("MySQL")  # Loading the MySQL driver
con &lt;- dbConnect(drv,dbname="myDB",  
                 username="myUSER",password="myPASS",
                 host="localhost")
d &lt;- sampleDBMS(con,"largeTable",10000)

## End(Not run)
</code></pre>

<hr>
<h2 id='SelfTrain'>
Self train a model on semi-supervised data
</h2><span id='topic+SelfTrain'></span>

<h3>Description</h3>

<p>This function can be used to learn a classification model from
semi-supervised data. This type of data includes observations for
which the class label is known as well as observation with unknown
class. The function implements a strategy known as self-training to be able
to cope with this semi-supervised learning problem. The function can
be applied to any classification algorithm that is able to obtain
class probabilities when asked to classify a set of test cases (see
the Details section).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SelfTrain(form,data,
          learner, learner.pars=list(),
          pred, pred.pars=list(),
          thrConf=0.9,
          maxIts=10,percFull=1,
          verbose=FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="SelfTrain_+3A_form">form</code></td>
<td>

<p>A formula describing the prediction problem.
</p>
</td></tr>
<tr><td><code id="SelfTrain_+3A_data">data</code></td>
<td>

<p>A data frame containing the available training set that is supposed to
contain some rows for which the value of the target variable is
unknown (i.e. equal to <code>NA</code>).
</p>
</td></tr>
<tr><td><code id="SelfTrain_+3A_learner">learner</code></td>
<td>

<p>An object of class <code>learner</code> (see <code>class?learner</code> for
details), indicating the base classification algorithm to use in the
self-training process.
</p>
</td></tr>
<tr><td><code id="SelfTrain_+3A_learner.pars">learner.pars</code></td>
<td>

<p>A <code>list</code> with parameters that are to be passed to the
<code>learner</code> function at each self-training iteration.
</p>
</td></tr>
<tr><td><code id="SelfTrain_+3A_pred">pred</code></td>
<td>

<p>A string with the name of a function that will carry out the
probabilistic classification tasks that will be necessary during the
self training process (see the Details section).
</p>
</td></tr>
<tr><td><code id="SelfTrain_+3A_pred.pars">pred.pars</code></td>
<td>

<p>A <code>list</code> with parameters that are to be passed to the
<code>pred</code> function at each self-training iteration when obtaining
the predictions of the models.
</p>
</td></tr>
<tr><td><code id="SelfTrain_+3A_thrconf">thrConf</code></td>
<td>

<p>A number between 0 and 1, indicating the required classification
confidence for an unlabelled case to be added to the labelled data set
with the label predicted predicted by the classification algorithm.
</p>
</td></tr>
<tr><td><code id="SelfTrain_+3A_maxits">maxIts</code></td>
<td>

<p>The maximum number of iterations of the self-training process.
</p>
</td></tr>
<tr><td><code id="SelfTrain_+3A_percfull">percFull</code></td>
<td>

<p>A number between 0 and 1. If the percentage of labelled cases
reaches this value the self-training process is stoped.
</p>
</td></tr>
<tr><td><code id="SelfTrain_+3A_verbose">verbose</code></td>
<td>

<p>A boolean indicating the verbosity level of the function.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Self-training (e.g. Yarowsky, 1995) is a well-known strategy to handle
classification problems where a subset of the available training data
has an unknown class label. The general idea is to use an iterative
process where at each step we try to augment the set of labelled cases
by &quot;asking&quot; the current classification model to label the unlabelled
cases and choosing the ones for which the model is more confident on
the assigned label to be added to the labeled set. With this extended
set of labelled cases a new classification model is learned and the
process is repeated until certain termination criteria are met.
</p>
<p>This implementation of the self-training algorithm is generic in the
sense that it can be used with any baseline classification learner
provided this model is able to produce confidence scores for its
predictions. The user needs to take care of the learning of the models
and of the classification of the unlabelled cases. This is done as
follows. The user supplies a <code>learner</code> object (see
<code>class?learner</code> for details) in parameter <code>learner</code> to represent the algorithm to be
used to obtain the classification models on each iteration of the 
self-training process. Furthermore, the user should create a function,
whose named should be given in the parameter <code>predFunc</code>, that
takes care of the classification of the currently unlabelled cases, on
each iteration. This function should be written so that it receives as
first argument the learned classification model (with the current
training set), and a data frame with test cases in the second
argument. This user-defined function should return a data frame with
two columns and as many rows as there are rows in the given test
set. The first column of this data frame should contain the assigned
class labels by the provided classification model, for the respective
test case. The second column should contain the confidence (a number
between 0 and 1) associated to that classification. See the Examples
section for an illustration of such user-defined function.
</p>
<p>This function implements the iterative process of self training. On
each iteration the provided learner is called with the set of labelled
cases within the given data set. Unlabelled cases should have the
value <code>NA</code> on the column of the target variable.  The obtained
classification model is then passed to the user-defined &quot;predFunc&quot;
function together with the subset of the data that is unlabelled. As
mentioned above this function returns a set of predicted class labels
and the respective confidence. Test cases with confidence above the
user-specified threshold (parameter <code>thrConf</code>) will be added to
the labelled training set, with the label assigned by the current
model. With this new training set a new classification model is
obtained and the overall process repeated.
</p>
<p>The self-training process stops if either there are no classifications
that reach the required confidence level, if the maximum number of
iterations is reached, or if the size of the current labelled training
set is alread the target percentage of the given data set.
</p>


<h3>Value</h3>

<p>This function returns a classification model. This will be an object of
the same class as the object returned by the base classification learned
provided by the user.
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Torgo, L. (2016) <em>Data Mining using R: learning with case studies,
second edition</em>,
Chapman &amp; Hall/CRC (ISBN-13: 978-1482234893).
</p>
<p><a href="http://ltorgo.github.io/DMwR2">http://ltorgo.github.io/DMwR2</a>  
</p>
<p>Yarowski, D. (1995). <em>Unsupervised word sense disambiguation
rivaling supervised methods</em>. In Proceedings of the 33rd Annual
Meeting of the association for Computational Linguistics (ACL), pages 189-196.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Small example with the Iris classification data set
data(iris)

## Dividing the data set into train and test sets
idx &lt;- sample(150,100)
tr &lt;- iris[idx,]
ts &lt;- iris[-idx,]

## Learn a tree with the full train set and test it
stdTree &lt;- rpartXse(Species~ .,tr,se=0.5)
table(predict(stdTree,ts,type='class'),ts$Species)

## Now let us create another training set with most of the target
## variable values unknown
trSelfT &lt;- tr
nas &lt;- sample(100,70)
trSelfT[nas,'Species'] &lt;- NA

## Learn a tree using only the labelled cases and test it
baseTree &lt;- rpartXse(Species~ .,trSelfT[-nas,],se=0.5)
table(predict(baseTree,ts,type='class'),ts$Species)

## The user-defined function that will be used in the self-training process
f &lt;- function(m,d) { 
      l &lt;- predict(m,d,type='class')
      c &lt;- apply(predict(m,d),1,max)
      data.frame(cl=l,p=c)
}

## Self train the same model using the semi-superside data and test the
## resulting model
treeSelfT &lt;- SelfTrain(Species~ .,trSelfT,'rpartXse',list(se=0.5),'f')
table(predict(treeSelfT,ts,type='class'),ts$Species)
</code></pre>

<hr>
<h2 id='sigs.PR'>
Precision and recall of a set of predicted trading signals
</h2><span id='topic+sigs.PR'></span>

<h3>Description</h3>

<p>This function calculates the values of Precision and Recall of a set
of predicted signals, given the set of true signals. The function
assumes three types of signals: 'b' (Buy), 's' (Sell) and 'h'
(Hold). The function returns the values of Precision and Recall for the
buy, sell and sell+buy signals.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sigs.PR(preds, trues)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sigs.PR_+3A_preds">preds</code></td>
<td>

<p>A factor with the predicted signals (values should be 'b','s', or 'h')
</p>
</td></tr>
<tr><td><code id="sigs.PR_+3A_trues">trues</code></td>
<td>

<p>A factor with the predicted signals (values should be 'b','s', or 'h')
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Precision and recall are two evaluation statistics often used to
evaluate predictions for rare events. In this case we are talking
about buy and sell opportunities.
</p>
<p>Precision is the proportion of the events signaled by a model that
actually occurred. Recall is a proportion of events that occurred that
the model was able to capture. Ideally, the models should aim to
obtain 100% precision and recall. However, it is often the case that
there is a trade-off between the two statistics.
</p>


<h3>Value</h3>

<p>A matrix with three rows and two columns. The columns are the values
of Precision and Recall, respectively. The rows are the values for the
three different events (sell, buy and sell+buy).
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Torgo, L. (2016) <em>Data Mining using R: learning with case studies,
second edition</em>,
Chapman &amp; Hall/CRC (ISBN-13: 978-1482234893).
</p>
<p><a href="http://ltorgo.github.io/DMwR2">http://ltorgo.github.io/DMwR2</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+trading.signals">trading.signals</a></code>, <code><a href="#topic+tradingEvaluation">tradingEvaluation</a></code>, <code><a href="#topic+trading.simulator">trading.simulator</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## A simple illustrative example use with random signals
ind &lt;- rnorm(sd=0.3,100)
sigs &lt;- trading.signals(ind,b.t=0.1,s.t=-0.1)
indT &lt;- rnorm(sd=0.3,100)
sigsT &lt;- trading.signals(indT,b.t=0.1,s.t=-0.1)
sigs.PR(sigs,sigsT)
</code></pre>

<hr>
<h2 id='SoftMax'>
Normalize a set of continuous values using SoftMax
</h2><span id='topic+SoftMax'></span>

<h3>Description</h3>

<p>Function for normalizing the range of values of a continuous variable
using the SoftMax function (Pyle, 199).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SoftMax(x, lambda = 2, avg = mean(x, na.rm = T), std = sd(x, na.rm = T))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="SoftMax_+3A_x">x</code></td>
<td>

<p>A vector with numeric values
</p>
</td></tr>
<tr><td><code id="SoftMax_+3A_lambda">lambda</code></td>
<td>

<p>A numeric value entering the formula of the soft max function (see
Details). Defaults to 2.
</p>
</td></tr>
<tr><td><code id="SoftMax_+3A_avg">avg</code></td>
<td>

<p>The statistic of centrality of the continuous variable being normalized
(defaults to the mean of the values in <code>x</code>).
</p>
</td></tr>
<tr><td><code id="SoftMax_+3A_std">std</code></td>
<td>

<p>The statistic of spread of the continuous variable being normalized
(defaults to the standard deviation of the values in <code>x</code>).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Soft Max normalization consist in transforming the value x into
</p>
<p>1 / [ 1+ exp( (x-AVG(x))/(LAMBDA*SD(X)/2*PI) ) ]
</p>


<h3>Value</h3>

<p>An object with the same dimensions as <code>x</code> but with the values normalized
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Pyle, D. (1999). <em>Data preparation for data mining</em>. Morgan Kaufmann.
</p>
<p>Torgo, L. (2016) <em>Data Mining using R: learning with case studies,
second edition</em>,
Chapman &amp; Hall/CRC (ISBN-13: 978-1482234893).
</p>
<p><a href="http://ltorgo.github.io/DMwR2">http://ltorgo.github.io/DMwR2</a>  
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+scale">scale</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## A simple example with the iris data set
data(iris)
summary(SoftMax(iris[["Petal.Length"]]))
summary(iris[["Petal.Length"]])
</code></pre>

<hr>
<h2 id='sp500'>A set of daily quotes for SP500 in CSV Format</h2><span id='topic+sp500'></span>

<h3>Description</h3>

<p>This is a <code>CSV</code> file containing the daily quotes of
the SP500 sotck index from 1970-01-02 till 2015-12-25 . For
each day information is given on the Open, High, Low and Close
prices, and also for the Volume and  Adjusted close price.   
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sp500</code></pre>


<h3>Format</h3>

<p>A <code>CSV</code> file with a data matrix.</p>


<h3>Source</h3>

<p>Yahoo Finance  </p>

<hr>
<h2 id='test.algae'>Testing data for predicting algae blooms</h2><span id='topic+test.algae'></span>

<h3>Description</h3>

<p>This data set contains observations on 11 variables as well
as the concentration levels of 7 harmful algae. Values were
measured in several European rivers. The 11 predictor
variables include 3 contextual variables (season, size and
speed) describing the water sample, plus 8 chemical
concentration measurements.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>test.algae</code></pre>


<h3>Format</h3>

<p>A data frame with 140 observations and 18 columns.</p>


<h3>Source</h3>

<p>ERUDIT <a href="http://www.erudit.de/">http://www.erudit.de/</a> - European Network for Fuzzy Logic and
Uncertainty Modelling in Information Technology. </p>

<hr>
<h2 id='tradeRecord-class'>Class &quot;tradeRecord&quot; </h2><span id='topic+tradeRecord'></span><span id='topic+tradeRecord-class'></span><span id='topic+plot+2CtradeRecord-method'></span><span id='topic+show+2CtradeRecord-method'></span><span id='topic+summary+2CtradeRecord-method'></span>

<h3>Description</h3>

<p>	 This is a class that contains the result of a call to
the function trading.simulator(). It contains information on the
trading performance of a set of signals on a given set of &quot;future&quot;
market quotes.</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>tradeRecord(...)</code>.
These objects contain information on i) the trading variables for each
day in the simulation period; ii) on the positions hold during this
period; iii) on the value used for transaction costs; iv) on the
initial capital for the simulation; v) on the function that implements
the trading policy used in the simulation; and vi) on the list of
parameters of this function.
</p>


<h3>Slots</h3>


<dl>
<dt><code>trading</code>:</dt><dd><p>Object of class <code>"xts"</code> containing the
information on the trading activities through the testing
period. This object has one line for each trading date. For each
date it includes information on the closing price of the market
(&quot;Close&quot;), on the order given at the end of that day (&quot;Order&quot;), on
the money available to the trader at the end of that day (&quot;Money&quot;),
on the number of stocks hold by the trader (&quot;N.Stocks&quot;), and on the
equity at the end of that day (&quot;Equity&quot;).</p>
</dd>
<dt><code>positions</code>:</dt><dd><p>Object of class <code>"matrix"</code> containing
the positions hold by the trader during the simulation
period. This is a matrix with seven columns, with as many rows as
the number of positions hold by the trader. The columns of this
matrix contain the type of position (&quot;pos.type&quot;), the number of
stocks of the position (&quot;N.stocks&quot;), the date when the position
was opened (&quot;Odate&quot;), the open price (&quot;Oprice&quot;), the closing date
(&quot;Cdate&quot;), the closing price (&quot;Cprice&quot;) and the percentage return
of the position (&quot;result&quot;). </p>
</dd>
<dt><code>trans.cost</code>:</dt><dd><p>Object of class <code>"numeric"</code> with the
monetary value of each transaction (market order). </p>
</dd>
<dt><code>init.cap</code>:</dt><dd><p>Object of class <code>"numeric"</code> with the
initial monetary value of the trader. </p>
</dd>
<dt><code>policy.func</code>:</dt><dd><p>Object of class <code>"character"</code> with
the name of the function that should be called at the end of each
day to decide what to do, i.e. the trading policy function. This
function is called with the vector of signals till the current
date, the market quotes till today, the current position of the
trader and the currently available money. </p>
</dd>
<dt><code>policy.pars</code>:</dt><dd><p>Object of class <code>"list"</code> containing
a list of extra parameters to be used when calling the trading
policy function (these depend on the function defined by the user). </p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>plot</dt><dd><p><code>signature(x = "tradeRecord", y = "ANY")</code>: provides
a graphical representation of the trading results.</p>
</dd>
<dt>show</dt><dd><p><code>signature(object = "tradeRecord")</code>: shows an
object in a proper way. </p>
</dd>
<dt>summary</dt><dd><p><code>signature(object = "tradeRecord")</code>: provides a
summary of the trading results. </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Torgo, L. (2016) <em>Data Mining using R: learning with case studies,
second edition</em>,
Chapman &amp; Hall/CRC (ISBN-13: 978-1482234893).
</p>
<p><a href="http://ltorgo.github.io/DMwR2">http://ltorgo.github.io/DMwR2</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+trading.simulator">trading.simulator</a></code>, <code><a href="#topic+tradingEvaluation">tradingEvaluation</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("tradeRecord")
</code></pre>

<hr>
<h2 id='trading.signals'>
Discretize a set of values into a set of trading signals
</h2><span id='topic+trading.signals'></span>

<h3>Description</h3>

<p>This function transforms a set of numeric values into a set of trading
signals according to two thresholds: one that establishes the limit
above which any value will be transformed into a buy signal ('b'), and
the other that sets the value below which we have a sell signal
('s'). Between the two thresholds we will have a hold signal ('h').
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trading.signals(vs, b.t, s.t)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="trading.signals_+3A_vs">vs</code></td>
<td>

<p>A vector with numeric values
</p>
</td></tr>
<tr><td><code id="trading.signals_+3A_b.t">b.t</code></td>
<td>

<p>A number representing the buy threshold
</p>
</td></tr>
<tr><td><code id="trading.signals_+3A_s.t">s.t</code></td>
<td>

<p>A number representing the sell threshold
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A factor with three possible values 'b' (buy), 's' (sell) or 'h' (hold)
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Torgo, L. (2016) <em>Data Mining using R: learning with case studies,
second edition</em>,
Chapman &amp; Hall/CRC (ISBN-13: 978-1482234893).
</p>
<p><a href="http://ltorgo.github.io/DMwR2">http://ltorgo.github.io/DMwR2</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+trading.signals">trading.signals</a></code>, <code><a href="#topic+tradingEvaluation">tradingEvaluation</a></code>, <code><a href="#topic+trading.simulator">trading.simulator</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>trading.signals(rnorm(sd=0.5,100),b.t=0.1,s.t=-0.12)
</code></pre>

<hr>
<h2 id='trading.simulator'>
Simulate daily trading using a set of trading signals
</h2><span id='topic+trading.simulator'></span>

<h3>Description</h3>

<p>This function can be used to obtain the trading performance of a set
of signals by simulating daily trading on a market with these signals
according to a user-defined trading policy. The idea is that the user
supplies the actual quotes for the simulation period together with the
trading signals to use during this period. On top of that the user
also supplies a function implementing the trading policy to use. The
result is a trading record for this period. This result can then be
inspected and used to obtain several trading performance metrics with
other functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trading.simulator(market, signals,
                  policy.func, policy.pars = list(),
                  trans.cost = 5, init.cap = 1e+06)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="trading.simulator_+3A_market">market</code></td>
<td>

<p>A <code>xts</code> object containing the market quotes for each day of the simulation
period. This object should contain at least the Open, High, Low and
Close quotes for each day. These quotes (with these exact names) are
used within the function and thus are required.
</p>
</td></tr>
<tr><td><code id="trading.simulator_+3A_signals">signals</code></td>
<td>

<p>A factor with as many signals as there are rows in the <code>market</code>
<code>xts</code> object, i.e. as many signals as there are trading days in
the simulation period. The signals should be 'b' for Buy, 's' for Sell
and 'h' for Hold (actually this information is solely processed within
the user-defined trading policy function which means that the values may be
whatever the writer of this function wants).
</p>
</td></tr>
<tr><td><code id="trading.simulator_+3A_policy.func">policy.func</code></td>
<td>

<p>A string with the name of the function that will be called at the end of
each day of the trading period. This user-defined function implements
the trading policy to be used in the simulation. See the Details section for
understanding what is the task of this function.
</p>
</td></tr>
<tr><td><code id="trading.simulator_+3A_policy.pars">policy.pars</code></td>
<td>

<p>A list with parameters that are passed to the user-defined trading
policy function when it is called at the end of each day.
</p>
</td></tr>
<tr><td><code id="trading.simulator_+3A_trans.cost">trans.cost</code></td>
<td>

<p>A number with the cost of each market transaction (defaults to 5
monetary units).
</p>
</td></tr>
<tr><td><code id="trading.simulator_+3A_init.cap">init.cap</code></td>
<td>

<p>A number with the initial amount of money available for trading at the start of the
simulation period (defaults to 1,000,000 monetary units).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function can be used to simulate daily trading according to a set
of signals. The main parameters of this function are the market quotes
for the simulation period and the model signals for this period. Two
other parameters are the name of the user-defined trading policy
function and its list of parameters. Finally, we can also specify the
cost of each transaction and the initial capital available for the
trader. The simulator will call the user-provided trading policy
function at the end of each daily section, and the function should
return the orders that it wants the simulator to carry out. The
simulator carries out these orders on the market and records all
activity on several data structures. The result of the simulator is an
object of class <code>tradeRecord</code> containing the information of this
simulation. This object can then be used in other functions to obtain
economic evaluation metrics or graphs of the trading activity.
</p>
<p>The key issue in using this function is to create the user-defined
trading policy function. These functions should be written using a
certain protocol, that is, they should be aware of how the simulator
will call them, and should return the information this simulator is
expecting. At the end of each daily session d, the simulator calls the
trading policy function with four main arguments plus any other
parameters the user has  provided in the call to the simulator in the
parameter <code>policy.pars</code>. These four arguments are (1) a vector
with the predicted signals until day d, (2) the market quotes (up to d), (3)
the currently opened positions, and (4) the money currently available to the
trader. The current positions are stored in a matrix with as many rows as there are open
positions at the end of day d. This matrix has four columns: &quot;pos.type&quot; that
can be 1 for a long position or -1 for a short position; &quot;N.stocks&quot;, which
is the number of stocks of the position; &quot;Odate&quot;, which is the day on which
the position was opened (a number between 1 and d); and &quot;Oprice&quot;, which
is the price at which the position was opened. The row names of this matrix
contain the IDs of the positions that are relevant when we want to indicate
the simulator that a certain position is to be closed.
All this information is provided by the simulator to ensure the user can
define a broad set of trading policy functions. The user-defined functions should
return a data frame with a set of orders that the simulator should carry out.
This data frame should include the following information (columns): &quot;order&quot;,
which should be 1 for buy orders and -1 for sell orders; &quot;order.type&quot;, which
should be 1 for market orders that are to be carried out immediately
(actually at next day open price),  2 for limit orders or 3 for stop orders; &quot;val&quot;,
which should be the quantity of stocks to trade for opening market orders, NA
for closing market orders, or a target price for limit and stop orders; &quot;action&quot;,
which should be &quot;open&quot; for orders that are opening a new position or &quot;close&quot; for
orders closing an existing position; and finally, &quot;posID&quot;, which should contain
the ID of the position that is being closed, if applicable.
</p>


<h3>Value</h3>

<p>An object of class <code>tradeRecord</code> (see 'class?tradeRecord' for details).
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Torgo, L. (2016) <em>Data Mining using R: learning with case studies,
second edition</em>,
Chapman &amp; Hall/CRC (ISBN-13: 978-1482234893).
</p>
<p><a href="http://ltorgo.github.io/DMwR2">http://ltorgo.github.io/DMwR2</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tradingEvaluation">tradingEvaluation</a></code>, <code><a href="#topic+tradeRecord-class">tradeRecord</a></code>,
<code><a href="#topic+trading.signals">trading.signals</a></code>,  <code><a href="#topic+sigs.PR">sigs.PR</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## An example partially taken from chapter 3 of my book Data Mining
## with R (Torgo,2010)

## First a trading policy function
## This function implements a strategy to trade on futures with
## long and short positions. Its main ideas are the following:
## - all decisions aretaken at the end of the day, that is, after
## knowing all daily quotes of the current session.
## - if at the end of day d our models issue a sell signal and  we
## currently  do not hold any opened position, we will open a short
## position  by issuing a sell order. When this order is carried out  by
## the market at a price  pr sometime in the  future, we will
## immediately post two other orders. The first is a buy limit order
## with  a limit price of pr - p%, where p% is a target profit margin.
## We will wait 10 days for this target to be reached. If the  order  is
## not carried out by this deadline, we will buy at the closing price
## of  the 10th day. The second order is a buy stop order with a  price
## limit  pr + l%. This order is placed with the goal of limiting our
## eventual  losses with this position. The order will be executed if
## the  market reaches the price pr + l%, thus limiting our possible
## losses  to l%.
## - if the end of the day signal is buy the strategy is more or less
## the inverse
## Not run: 
library(xts)
policy.1 &lt;- function(signals,market,opened.pos,money,
                     bet=0.2,hold.time=10,
                     exp.prof=0.025, max.loss= 0.05
                     )
  {
    d &lt;- NROW(market) # this is the ID of today
    orders &lt;- NULL
    nOs &lt;- NROW(opened.pos)
    # nothing to do!
    if (!nOs &amp;&amp; signals[d] == 'h') return(orders)

    # First lets check if we can open new positions
    # i) long positions
    if (signals[d] == 'b' &amp;&amp; !nOs) {
      quant &lt;- round(bet*money/market[d,'Close'],0)
      if (quant &gt; 0) 
        orders &lt;- rbind(orders,
              data.frame(order=c(1,-1,-1),order.type=c(1,2,3), 
                         val = c(quant,
                                 market[d,'Close']*(1+exp.prof),
                                 market[d,'Close']*(1-max.loss)
                                ),
                         action = c('open','close','close'),
                         posID = c(NA,NA,NA)
                        )
                       )

    # ii) short positions  
    } else if (signals[d] == 's' &amp;&amp; !nOs) {
      # this is the nr of stocks we already need to buy 
      # because of currently opened short positions
      need2buy &lt;- sum(opened.pos[opened.pos[,'pos.type']==-1,
                                 "N.stocks"])*market[d,'Close']
      quant &lt;- round(bet*(money-need2buy)/market[d,'Close'],0)
      if (quant &gt; 0)
        orders &lt;- rbind(orders,
              data.frame(order=c(-1,1,1),order.type=c(1,2,3), 
                         val = c(quant,
                                 market[d,'Close']*(1-exp.prof),
                                 market[d,'Close']*(1+max.loss)
                                ),
                         action = c('open','close','close'),
                         posID = c(NA,NA,NA)
                        )
                       )
    }
    
    # Now lets check if we need to close positions
    # because their holding time is over
    if (nOs) 
      for(i in 1:nOs) {
        if (d - opened.pos[i,'Odate'] &gt;= hold.time)
          orders &lt;- rbind(orders,
                data.frame(order=-opened.pos[i,'pos.type'],
                           order.type=1,
                           val = NA,
                           action = 'close',
                           posID = rownames(opened.pos)[i]
                          )
                         )
      }

    orders
  }

  ## Now let us play a bit with the SP500 quotes availabe in our package
  data(GSPC)

  ## Let us select the last 3 months as the simulation period
  market &lt;- last(GSPC,'3 months')
  
  ## now let us generate a set of random trading signals for
  ## illustration purpose only
  ndays &lt;- nrow(market)
  aRandomIndicator &lt;- rnorm(sd=0.3,ndays)
  theRandomSignals &lt;- trading.signals(aRandomIndicator,b.t=0.1,s.t=-0.1)

  ## now lets trade!
  tradeR &lt;- trading.simulator(market,theRandomSignals,
               'policy.1',list(exp.prof=0.05,bet=0.2,hold.time=10))

  ## a few stats on the trading performance
  summary(tradeR)
  tradingEvaluation(tradeR)

## End(Not run)
  ## See the performance graphically
  ## Not run: 
    plot(tradeR,market)
  
## End(Not run)

</code></pre>

<hr>
<h2 id='tradingEvaluation'>
Obtain a set of evaluation metrics for a set of trading actions
</h2><span id='topic+tradingEvaluation'></span>

<h3>Description</h3>

<p>This function receives as argument an object of class
<code>tradeRecord</code> that is the result of a call to the
<code>trading.simulator()</code> function and produces a set of evaluation
metrics of this simulation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tradingEvaluation(t)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tradingEvaluation_+3A_t">t</code></td>
<td>

<p>An object of call <code>tradeRecord</code> (see 'class?tradeRecord' for details)
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given the result of a trading simulation this function calculates:
</p>

<ul>
<li><p> The number of trades.
</p>
</li>
<li><p> The number of profitable trades.
</p>
</li>
<li><p> The percentage of profitable trades.
</p>
</li>
<li><p> The profit/loss of the simulation (i.e. the final result).
</p>
</li>
<li><p> The return of the simulation.
</p>
</li>
<li><p> The return over the buy and hold strategy.
</p>
</li>
<li><p> The maximum draw down of the simulation.
</p>
</li>
<li><p> The Sharpe Ration score.
</p>
</li>
<li><p> The average percentage return of the profitable trades.
</p>
</li>
<li><p> The average percentage return of the non-profitable trades.
</p>
</li>
<li><p> The average percentage return of all trades.
</p>
</li>
<li><p> The maximum return of all trades.
</p>
</li>
<li><p> The maximum percentage loss of all trades.
</p>
</li></ul>



<h3>Value</h3>

<p>A vector of evaluation metric values
</p>


<h3>Author(s)</h3>

<p> Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Torgo, L. (2016) <em>Data Mining using R: learning with case studies,
second edition</em>,
Chapman &amp; Hall/CRC (ISBN-13: 978-1482234893).
</p>
<p><a href="http://ltorgo.github.io/DMwR2">http://ltorgo.github.io/DMwR2</a>  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tradeRecord-class">tradeRecord</a></code>,  <code><a href="#topic+trading.simulator">trading.simulator</a></code>, <code><a href="#topic+trading.signals">trading.signals</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## An example partially taken from chapter 3 of my book Data Mining
## with R (Torgo,2010)

## First a trading policy function
## This function implements a strategy to trade on futures with
## long and short positions. Its main ideas are the following:
## - all decisions aretaken at the end of the day, that is, after
## knowing all daily quotes of the current session.
## - if at the end of day d our models issue a sell signal and  we
## currently  do not hold any opened position, we will open a short
## position  by issuing a sell order. When this order is carried out  by
## the market at a price  pr sometime in the  future, we will
## immediately post two other orders. The first is a buy limit order
## with  a limit price of pr - p%, where p% is a target profit margin.
## We will wait 10 days for this target to be reached. If the  order  is
## not carried out by this deadline, we will buy at the closing price
## of  the 10th day. The second order is a buy stop order with a  price
## limit  pr + l%. This order is placed with the goal of limiting our
## eventual  losses with this position. The order will be executed if
## the  market reaches the price pr + l%, thus limiting our possible
## losses  to l%.
## - if the end of the day signal is buy the strategy is more or less
## the inverse
## Not run: 
library(xts)
policy.1 &lt;- function(signals,market,opened.pos,money,
                     bet=0.2,hold.time=10,
                     exp.prof=0.025, max.loss= 0.05
                     )
  {
    d &lt;- NROW(market) # this is the ID of today
    orders &lt;- NULL
    nOs &lt;- NROW(opened.pos)
    # nothing to do!
    if (!nOs &amp;&amp; signals[d] == 'h') return(orders)

    # First lets check if we can open new positions
    # i) long positions
    if (signals[d] == 'b' &amp;&amp; !nOs) {
      quant &lt;- round(bet*money/market[d,'Close'],0)
      if (quant &gt; 0) 
        orders &lt;- rbind(orders,
              data.frame(order=c(1,-1,-1),order.type=c(1,2,3), 
                         val = c(quant,
                                 market[d,'Close']*(1+exp.prof),
                                 market[d,'Close']*(1-max.loss)
                                ),
                         action = c('open','close','close'),
                         posID = c(NA,NA,NA)
                        )
                       )

    # ii) short positions  
    } else if (signals[d] == 's' &amp;&amp; !nOs) {
      # this is the nr of stocks we already need to buy 
      # because of currently opened short positions
      need2buy &lt;- sum(opened.pos[opened.pos[,'pos.type']==-1,
                                 "N.stocks"])*market[d,'Close']
      quant &lt;- round(bet*(money-need2buy)/market[d,'Close'],0)
      if (quant &gt; 0)
        orders &lt;- rbind(orders,
              data.frame(order=c(-1,1,1),order.type=c(1,2,3), 
                         val = c(quant,
                                 market[d,'Close']*(1-exp.prof),
                                 market[d,'Close']*(1+max.loss)
                                ),
                         action = c('open','close','close'),
                         posID = c(NA,NA,NA)
                        )
                       )
    }
    
    # Now lets check if we need to close positions
    # because their holding time is over
    if (nOs) 
      for(i in 1:nOs) {
        if (d - opened.pos[i,'Odate'] &gt;= hold.time)
          orders &lt;- rbind(orders,
                data.frame(order=-opened.pos[i,'pos.type'],
                           order.type=1,
                           val = NA,
                           action = 'close',
                           posID = rownames(opened.pos)[i]
                          )
                         )
      }

    orders
  }

  ## Now let us play a bit with the SP500 quotes availabe in our package
  data(GSPC)

  ## Let us select the last 3 months as the simulation period
  market &lt;- last(GSPC,'3 months')
  
  ## now let us generate a set of random trading signals for
  ## illustration purpose only
  ndays &lt;- nrow(market)
  aRandomIndicator &lt;- rnorm(sd=0.3,ndays)
  theRandomSignals &lt;- trading.signals(aRandomIndicator,b.t=0.1,s.t=-0.1)

  ## now lets trade!
  tradeR &lt;- trading.simulator(market,theRandomSignals,
              'policy.1',list(exp.prof=0.05,bet=0.2,hold.time=10))

  ## a few stats on the trading performance
  tradingEvaluation(tradeR)

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
