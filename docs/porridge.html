<!DOCTYPE html><html><head><title>Help for package porridge</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {porridge}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#genRidgePenaltyMat'>
<p>Penalty parameter matrix for generalized ridge regression.</p></a></li>
<li><a href='#makeFoldsGLMcv'>
<p>Generate folds for cross-validation of generalized linear models.</p></a></li>
<li><a href='#optPenaltyGGMmixture.kCVauto'>
<p>Automatic search for optimal penalty parameter (mixture of GGMs).</p></a></li>
<li><a href='#optPenaltyGLM.kCVauto'>
<p>Automatic search for optimal penalty parameters of the targeted ridge GLM estimator.</p></a></li>
<li><a href='#optPenaltyGLMmultiT.kCVauto'>
<p>Automatic search for optimal penalty parameters of the targeted ridge GLM estimator.</p></a></li>
<li><a href='#optPenaltyPgen.kCVauto.banded'>
<p>Automatic search for optimal penalty parameter (generalized ridge precision).</p></a></li>
<li><a href='#optPenaltyPgen.kCVauto.groups'>
<p>Automatic search for optimal penalty parameter (generalized ridge precision).</p></a></li>
<li><a href='#optPenaltyPmultiT.kCVauto'>
<p>Automatic search for optimal penalty parameter (ridge precision with multi-targets).</p></a></li>
<li><a href='#optPenaltyPrep.kCVauto'>
<p>Automatic search for optimal penalty parameters (for precision estimation of data with replicates).</p></a></li>
<li><a href='#optPenaltyPrepEdiag.kCVauto'>
<p>Automatic search for optimal penalty parameters (for precision estimation of data with replicates).</p></a></li>
<li><a href='#porridge-package'><p>Ridge-Type Penalized Estimation of a Potpourri of Models.</p></a></li>
<li><a href='#ridgeGGMmixture'>
<p>Ridge penalized estimation of a mixture of GGMs.</p></a></li>
<li><a href='#ridgeGLM'>
<p>Ridge estimation of generalized linear models.</p></a></li>
<li><a href='#ridgeGLMdof'>
<p>Degrees of freedom of the generalized ridge estimator.</p></a></li>
<li><a href='#ridgeGLMmultiT'>
<p>Multi-targeted ridge estimation of generalized linear models.</p></a></li>
<li><a href='#ridgePgen'>
<p>Ridge estimation of the inverse covariance matrix with element-wise penalization and shrinkage.</p></a></li>
<li><a href='#ridgePgen.kCV'>
<p>K-fold cross-validated loglikelihood of ridge precision estimator.</p></a></li>
<li><a href='#ridgePgen.kCV.banded'>
<p>K-fold cross-validated loglikelihood of ridge precision estimator for banded precisions.</p></a></li>
<li><a href='#ridgePgen.kCV.groups'>
<p>K-fold cross-validated loglikelihood of ridge precision estimator with group-wise penalized variates.</p></a></li>
<li><a href='#ridgePmultiT'>
<p>Ridge estimation of the inverse covariance matrix with multi-target shrinkage.</p></a></li>
<li><a href='#ridgePrep'>
<p>Ridge penalized estimation of the precision matrix from data with replicates.</p></a></li>
<li><a href='#ridgePrepEdiag'>
<p>Ridge penalized estimation of the precision matrix from data with replicates.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Ridge-Type Penalized Estimation of a Potpourri of Models</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-02-21</td>
</tr>
<tr>
<td>Author:</td>
<td>Wessel N. van Wieringen
    <a href="https://orcid.org/0000-0002-5100-9123"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cre],
  Mehran Aflakparast [ctb] (part of the R-code of the mixture
    functionality)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Wessel N. van Wieringen &lt;w.vanwieringen@vumc.nl&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>The name of the package is derived from the French, 'pour' ridge, and provides functionality for ridge-type estimation of a potpourri of models. Currently, this estimation concerns that of various Gaussian graphical models from different study designs. Among others it considers the regular Gaussian graphical model and a mixture of such models. The porridge-package implements the estimation of the former either from i) data with replicated observations by penalized loglikelihood maximization using the regular ridge penalty on the parameters (van Wieringen, Chen, 2021) or ii) from non-replicated data by means of either a ridge estimator with multiple shrinkage targets (as presented in van Wieringen et al. 2020, &lt;<a href="https://doi.org/10.1016%2Fj.jmva.2020.104621">doi:10.1016/j.jmva.2020.104621</a>&gt;) or the generalized ridge estimator that allows for both the inclusion of quantitative and qualitative prior information on the precision matrix via element-wise penalization and shrinkage (van Wieringen, 2019, &lt;<a href="https://doi.org/10.1080%2F10618600.2019.1604374">doi:10.1080/10618600.2019.1604374</a>&gt;). Additionally, the porridge-package facilitates the ridge penalized estimation of a mixture of Gaussian graphical models (Aflakparast et al., 2018). On another note, the package also includes functionality for ridge-type estimation of the generalized linear model (as presented in van Wieringen, Binder, 2022, &lt;<a href="https://doi.org/10.1080%2F10618600.2022.2035231">doi:10.1080/10618600.2022.2035231</a>&gt;). </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://www.math.vu.nl/~wvanwie/">https://www.math.vu.nl/~wvanwie/</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>MASS, Matrix, stats, mvtnorm, Rcpp, methods, pracma</td>
</tr>
<tr>
<td>Suggests:</td>
<td>rags2ridges</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-02-21 19:34:06 UTC; wessel</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-02-21 20:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='genRidgePenaltyMat'>
Penalty parameter matrix for generalized ridge regression.
</h2><span id='topic+genRidgePenaltyMat'></span>

<h3>Description</h3>

<p>The function produces an unscaled penalty parameter matrix to be used in the generalized ridge regression estimator.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>genRidgePenaltyMat(pr, pc=pr, type="2dimA")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="genRidgePenaltyMat_+3A_pr">pr</code></td>
<td>
<p> A positive <code>integer</code>. The number of covariates if <code>type="common"</code> or 
<code>type="fused1dim"</code>. Or, the row dimension of a 2-dimensional covariate layout if <code>type="fused2dimA"</code> or <code>type="fused2dimD"</code>.</p>
</td></tr>
<tr><td><code id="genRidgePenaltyMat_+3A_pc">pc</code></td>
<td>
<p> A positive <code>integer</code>. The column dimension of a 2-dimensional covariate layout if <code>type="fused2dimA"</code> or <code>type="fused2dimD"</code>. Ignored if <code>type="common"</code> or <code>type="fused1dim"</code>. </p>
</td></tr>
<tr><td><code id="genRidgePenaltyMat_+3A_type">type</code></td>
<td>
<p> A <code>character</code>. Either <code>"common"</code>, <code>"fused1dim"</code>, <code>"fused2dimA"</code> or <code>"fused2dimD"</code>, see details. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Various ridge penalty matrices are implemented. 
</p>
<p>The <code>type="common"</code>-option supports the &lsquo;homogeneity&rsquo; ridge penalization proposed by Anatolyev (2020). The ridge penalty matrix <code class="reqn">\mathbf{\Delta}</code> for a <code class="reqn">p</code>-dimensional regression parameter <code class="reqn">\boldsymbol{\beta}</code> is such that:
</p>
<p style="text-align: center;"><code class="reqn">
\boldsymbol{\beta}^{\top} \mathbf{\Delta} \boldsymbol{\beta} \, \, = \, \, \boldsymbol{\beta}^{\top} ( \mathbf{I}_{pp} - p^{-1} \mathbf{1}_{pp}) \boldsymbol{\beta}  \, \, =  \, \, \sum\nolimits_{j=1}^p \big( \beta_j -
p^{-1} \sum\nolimits_{j'=1}^p \beta_{j'}\big)^2.
</code>
</p>

<p>This penalty matrix encourages shrinkage of the elements of <code class="reqn">\boldsymbol{\beta}</code> to a common effect value.
</p>
<p>The <code>type="fused1dim"</code>-option facilitates the 1-dimensional fused ridge estimation of Goeman (2008). The ridge penalty matrix <code class="reqn">\mathbf{\Delta}</code> for a <code class="reqn">p</code>-dimensional regression parameter <code class="reqn">\boldsymbol{\beta}</code> is such that:
</p>
<p style="text-align: center;"><code class="reqn">
\boldsymbol{\beta}^{\top} \mathbf{\Delta} \boldsymbol{\beta} \, \, = \, \, \sum\nolimits_{j=2}^p ( \beta_{j} - \beta_{j-1} )_2^2.
</code>
</p>

<p>This penalty matrix aims to shrink contiguous (as defined by their index) elements of <code class="reqn">\boldsymbol{\beta}</code> towards each other.
</p>
<p>The <code>type="fused2dimA"</code>- and  <code>type="fused2dimD"</code>-options facilitate 2-dimensional ridge estimation as proposed by Lettink et al. (2022). It assumes the regression parameter is endowed with a 2-dimensional layout. The columns of this layout have been stacked to form <code class="reqn">\boldsymbol{\beta}</code>. The 2-dimensional fused ridge estimation shrinks elements of <code class="reqn">\boldsymbol{\beta}</code> that are neighbors in the 2-dimensional layout towards each other. The two options use different notions of neighbors. If <code>type="fused2dimA"</code>, the ridge penalty matrix <code class="reqn">\mathbf{\Delta}</code> for a <code class="reqn">p</code>-dimensional regression parameter <code class="reqn">\boldsymbol{\beta}</code> is such that:
</p>
<p style="text-align: center;"><code class="reqn">
\boldsymbol{\beta}^{\top} \mathbf{\Delta} \boldsymbol{\beta} \, \,  = \, \,  \sum\nolimits_{j_r=1}^{p_r-1} \sum\nolimits_{j_c=1}^{p_c-1} [(\beta_{j_r,j_c+1} - \beta_{j_r,j_c})^2 + (\beta_{j_r+1, j_c} - \beta_{j_r, j_c})^2]
</code>
</p>

<p style="text-align: center;"><code class="reqn">
\mbox{ } \qquad \qquad \, \, \, \, + \sum\nolimits_{j_c=1}^{p_c-1} (\beta_{p_r,j_c+1} - \beta_{p_r,j_c})^2 +  \sum\nolimits_{j_r=1}^{p_r-1}  (\beta_{j_r+1, p_c} - \beta_{j_r, p_c})^2,
</code>
</p>

<p>where <code class="reqn">p_r</code> and <code class="reqn">p_c</code> are the row and column dimension, respectively, of the 2-dimensional layout. This penalty matrix intends to shrink the elements of <code class="reqn">\boldsymbol{\beta}</code> along the axes of the 2-dimensional layout. If <code>type="fused2dimD"</code>, the ridge penalty matrix <code class="reqn">\mathbf{\Delta}</code> for a <code class="reqn">p</code>-dimensional regression parameter <code class="reqn">\boldsymbol{\beta}</code> is such that:
</p>
<p style="text-align: center;"><code class="reqn">
\boldsymbol{\beta}^{\top} \mathbf{\Delta} \boldsymbol{\beta} \, \,  = \, \, 
\sum\nolimits_{j_r=1}^{p_r-1} \sum\nolimits_{j_c=1}^{p_c-2}  [(\beta_{j_r+1,j_c} - \beta_{j_r,j_c+1})^2 + (\beta_{j_r+1, j_c+2} - \beta_{j_r, j_c+1})^2]
</code>
</p>

<p style="text-align: center;"><code class="reqn">
\mbox{ } \qquad \, + \sum\nolimits_{j_r=1}^{p_r-1} [(\beta_{j_r+1,2} - \beta_{j_r,1})^2 + (\beta_{j_r+1, p_c-1} - \beta_{j_r, p_c})^2].
</code>
</p>

<p>This penalty matrix shrinks the elements of <code class="reqn">\boldsymbol{\beta}</code> along the diagonally to the axes of the 2-dimensional layout. The penalty matrices 
generated by <code>type="fused2dimA"</code>- and  <code>type="fused2dimD"</code>-options may be combined. 
</p>


<h3>Value</h3>

<p>The function returns a non-negative definite <code>matrix</code>.
</p>


<h3>Author(s)</h3>

<p>W.N. van Wieringen.
</p>


<h3>References</h3>

<p>Anatolyev, S. (2020), &quot;A ridge to homogeneity for linear models&quot;, <em>Journal of Statistical Computation and Simulation</em>, 90(13), 2455-2472.
</p>
<p>Goeman, J.J. (2008), &quot;Autocorrelated logistic ridge regression for prediction based on proteomics spectra&quot;, <em>Statistical Applications in Genetics and Molecular Biology</em>, 7(2).
</p>
<p>Lettink, A, Chinapaw, M.J.M., van Wieringen, W.N. (2022), &quot;Two-dimensional fused targeted ridge regression for health indicator prediction from accelerometer data&quot;, <em>submitted</em>.
</p>


<h3>See Also</h3>

<p><code>ridgeGLM</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># generate unscaled general penalty parameter matrix
Dfused &lt;- genRidgePenaltyMat(10, type="fused1dim")
</code></pre>

<hr>
<h2 id='makeFoldsGLMcv'>
Generate folds for cross-validation of generalized linear models.
</h2><span id='topic+makeFoldsGLMcv'></span>

<h3>Description</h3>

<p>Function that evaluates the targeted ridge estimator of the regression parameter of generalized linear models. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>makeFoldsGLMcv(fold, Y, stratified=TRUE, model="linear")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="makeFoldsGLMcv_+3A_fold">fold</code></td>
<td>
<p> An <code>integer</code>, the number of folds to be generated. </p>
</td></tr>
<tr><td><code id="makeFoldsGLMcv_+3A_y">Y</code></td>
<td>
<p> A <code>numeric</code> being the response vector. </p>
</td></tr>
<tr><td><code id="makeFoldsGLMcv_+3A_stratified">stratified</code></td>
<td>
<p> A <code>logical</code>. If <code>stratified=TRUE</code>, the folds are generated such the distribution of the response <code>Y</code> is (roughly) the same across folds. </p>
</td></tr>
<tr><td><code id="makeFoldsGLMcv_+3A_model">model</code></td>
<td>
<p> A <code>character</code>, either <code>"linear"</code> and <code>"logistic"</code> (a reference to the models currently implemented), indicative of the type of response for stratification. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>list</code> of length <code>fold</code>. Each list item is a fold.
</p>


<h3>Author(s)</h3>

<p>W.N. van Wieringen.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># set the sample size
n &lt;- 50

# set the true parameter
betas &lt;- (c(0:100) - 50) / 20

# generate covariate data
X &lt;- matrix(rnorm(length(betas)*n), nrow=n)

# sample the response
probs &lt;- exp(tcrossprod(betas, X)[1,]) / (1 + exp(tcrossprod(betas, X)[1,]))
Y     &lt;- numeric()
for (i in 1:n){
    Y &lt;- c(Y, sample(c(0,1), 1, prob=c(1-probs[i], probs[i])))
}

# generate folds 
folds &lt;- makeFoldsGLMcv(10, Y, model="logistic")
</code></pre>

<hr>
<h2 id='optPenaltyGGMmixture.kCVauto'>
Automatic search for optimal penalty parameter (mixture of GGMs).
</h2><span id='topic+optPenaltyGGMmixture.kCVauto'></span>

<h3>Description</h3>

<p>Function that performs an automatic search for the optimal penalty parameter for the <code>ridgeGGMmixture</code> call by employing Brent's
method to the calculation of a cross-validated (negative) log-likelihood score.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optPenaltyGGMmixture.kCVauto(Y, K, lambdaMin, lambdaMax, 
                lambdaInit=(lambdaMin+lambdaMax)/2, 
                fold=nrow(Y), target,               
                iWeights=matrix(sample(seq(0+1/nrow(Y), 
                                1-1/nrow(Y), by=1/(2*nrow(Y))), 
                                nrow(Y)*K, replace=TRUE), 
                                nrow=nrow(Y), ncol=K),
                nInit=100, minSuccDiff=10^(-10),
                minMixProp=0.01)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optPenaltyGGMmixture.kCVauto_+3A_y">Y</code></td>
<td>
<p> Data <code>matrix</code> with samples as rows and variates as columns. </p>
</td></tr>
<tr><td><code id="optPenaltyGGMmixture.kCVauto_+3A_k">K</code></td>
<td>
<p> A <code>numeric</code>, specifying the number of mixture components.</p>
</td></tr>
<tr><td><code id="optPenaltyGGMmixture.kCVauto_+3A_lambdamin">lambdaMin</code></td>
<td>
<p> A <code>numeric</code> giving the minimum value for the penalty parameter. </p>
</td></tr>
<tr><td><code id="optPenaltyGGMmixture.kCVauto_+3A_lambdamax">lambdaMax</code></td>
<td>
<p> A <code>numeric</code> giving the maximum value for the penalty parameter. </p>
</td></tr>
<tr><td><code id="optPenaltyGGMmixture.kCVauto_+3A_lambdainit">lambdaInit</code></td>
<td>
<p> A <code>numeric</code> giving the initial (starting) value for the penalty parameter. </p>
</td></tr>
<tr><td><code id="optPenaltyGGMmixture.kCVauto_+3A_fold">fold</code></td>
<td>
<p> A <code>numeric</code> or <code>integer</code> specifying the number of folds to apply in the cross-validation. </p>
</td></tr>
<tr><td><code id="optPenaltyGGMmixture.kCVauto_+3A_target">target</code></td>
<td>
<p> A semi-positive definite target <code>matrix</code> towards which the estimate is shrunken. </p>
</td></tr>
<tr><td><code id="optPenaltyGGMmixture.kCVauto_+3A_iweights">iWeights</code></td>
<td>
<p> Sample-specific positive component weight <code>matrix</code>. Rows correspond to samples, while columns to components. </p>
</td></tr> 
<tr><td><code id="optPenaltyGGMmixture.kCVauto_+3A_ninit">nInit</code></td>
<td>
<p> A <code>numeric</code> specifying the number of iterations. </p>
</td></tr>
<tr><td><code id="optPenaltyGGMmixture.kCVauto_+3A_minsuccdiff">minSuccDiff</code></td>
<td>
<p> A <code>numeric</code>: minimum successive difference (in terms of their penalized loglikelihood) between two succesive estimates to be achieved. </p>
</td></tr>
<tr><td><code id="optPenaltyGGMmixture.kCVauto_+3A_minmixprop">minMixProp</code></td>
<td>
<p> Smallest mixing probability tolerated. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a positive <code>numeric</code>, the cross-validated optimal penalty parameter.
</p>


<h3>Note</h3>

<p>The elements of <code>iWeights</code> may be larger than one as they are rescaled internally to sum to one.
</p>


<h3>Author(s)</h3>

<p>W.N. van Wieringen, M. Aflakparast.
</p>


<h3>References</h3>

<p>Aflakparast, M., de Gunst, M.C.M., van Wieringen, W.N. (2018), &quot;Reconstruction of molecular network evolution from cross-sectional omics data&quot;, <em>Biometrical Journal</em>, 60(3), 547-563.
</p>


<h3>See Also</h3>

<p><code>ridgeGGMmixture</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># define mixing proportions
pis &lt;- c(0.2, 0.3, 0.4)

# set dimension and sample size
p &lt;- 5
n &lt;- 100

# define population covariance matrices
diags       &lt;- list(rep(1,    p), 
                    rep(0.5,  p-1), 
                    rep(0.25, p-2), 
                    rep(0.1,  p-3))
Omega       &lt;- as.matrix(Matrix::bandSparse(p, 
                                            k   =-c(0:3), 
                                            diag=c(diags), 
                                            symm=TRUE))
Sigma1      &lt;- solve(Omega)
Omega       &lt;- matrix(0.3, p, p)
diag(Omega) &lt;- 1
Sigma2      &lt;- solve(Omega)
Sigma3      &lt;- cov(matrix(rnorm(p*n), ncol=p))

# mean vectors
mean1 &lt;- rep(0,p)
mean2 &lt;- rexp(p)
mean3 &lt;- rnorm(p)

# draw data data from GGM mixture
Z &lt;- sort(sample(c(1:3), n, prob=pis, replace=TRUE))
Y &lt;- rbind(mvtnorm::rmvnorm(sum(Z==1), mean=mean1, sigma=Sigma1),
           mvtnorm::rmvnorm(sum(Z==2), mean=mean2, sigma=Sigma2),
           mvtnorm::rmvnorm(sum(Z==3), mean=mean3, sigma=Sigma3))

# find optimal penalty parameter
### optLambda &lt;- optPenaltyGGMmixture.kCVauto(Y,  K=3,         
###                                          0.00001, 100,    
###                                          10, fold=5,      
###                                          target=0*Sigma1) 

# ridge penalized estimation of the GGM mixture
### ridgeGGMmixFit &lt;- ridgeGGMmixture(Y, 3, optLambda, target=0*Sigma1) 
</code></pre>

<hr>
<h2 id='optPenaltyGLM.kCVauto'>
Automatic search for optimal penalty parameters of the targeted ridge GLM estimator.
</h2><span id='topic+optPenaltyGLM.kCVauto'></span>

<h3>Description</h3>

<p>Function finds the optimal penalty parameter of the targeted ridge 
regression estimator of the generalized linear model parameter. The 
optimum is defined as the minimizer of the cross-validated loss 
associated with the estimator. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optPenaltyGLM.kCVauto(Y, X, U=matrix(ncol=0, nrow=length(Y)), lambdaInit, 
                      lambdaGinit=0, Dg=matrix(0, ncol=ncol(X), nrow=ncol(X)),
                      model="linear", target=rep(0, ncol(X)), 
                      folds=makeFoldsGLMcv(min(10, length(X)), Y, model=model),
                      loss="loglik", lambdaMin=10^(-5), 
                      lambdaGmin=10^(-5), minSuccDiff=10^(-5), maxIter=100, 
                      implementation="org")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optPenaltyGLM.kCVauto_+3A_y">Y</code></td>
<td>
<p>           A <code>numeric</code> being the response vector. </p>
</td></tr>
<tr><td><code id="optPenaltyGLM.kCVauto_+3A_x">X</code></td>
<td>
<p>           The design <code>matrix</code>. The number of rows should match the number of elements of <code>Y</code>. </p>
</td></tr>
<tr><td><code id="optPenaltyGLM.kCVauto_+3A_u">U</code></td>
<td>
<p>           The design <code>matrix</code> of the unpenalized covariates. The number of rows should match the number of elements of <code>Y</code>. </p>
</td></tr>
<tr><td><code id="optPenaltyGLM.kCVauto_+3A_lambdainit">lambdaInit</code></td>
<td>
<p>  A <code>numeric</code>, the initial (starting) values for the regular ridge penalty parameter. </p>
</td></tr>
<tr><td><code id="optPenaltyGLM.kCVauto_+3A_lambdaginit">lambdaGinit</code></td>
<td>
<p> A <code>numeric</code>, the initial (starting) values for the generalized ridge penalty parameter. </p>
</td></tr>
<tr><td><code id="optPenaltyGLM.kCVauto_+3A_dg">Dg</code></td>
<td>
<p>          A non-negative definite <code>matrix</code> of the unscaled generalized ridge penalty. </p>
</td></tr>
<tr><td><code id="optPenaltyGLM.kCVauto_+3A_model">model</code></td>
<td>
<p>       A <code>character</code>, either <code>"linear"</code> and <code>"logistic"</code> (a reference to the models currently implemented), indicating which generalized linear model model instance is to be fitted </p>
</td></tr>
<tr><td><code id="optPenaltyGLM.kCVauto_+3A_target">target</code></td>
<td>
<p>      A <code>numeric</code> towards which the estimate is shrunken. </p>
</td></tr>
<tr><td><code id="optPenaltyGLM.kCVauto_+3A_folds">folds</code></td>
<td>
<p>       A <code>list</code>. Each list item representing a fold. It is an <code>integer</code> vector indexing the samples that comprise the fold. This object can be generated with the <code>makeFoldsGLMcv</code> function. </p>
</td></tr>
<tr><td><code id="optPenaltyGLM.kCVauto_+3A_loss">loss</code></td>
<td>
<p>        A <code>character</code>, either <code>loss="loglik"</code> or <code>"sos"</code>, specifying loss criterion to be used in the cross-validation. Used only if <code>model="linear"</code>. </p>
</td></tr><tr><td><code id="optPenaltyGLM.kCVauto_+3A_lambdamin">lambdaMin</code></td>
<td>
<p> A positive <code>numeric</code>, the lower bound of search interval of the regular ridge penalty parameter. </p>
</td></tr>
<tr><td><code id="optPenaltyGLM.kCVauto_+3A_lambdagmin">lambdaGmin</code></td>
<td>
<p> A positive <code>numeric</code> larger than <code>lambdaGmin</code>, the upper bound of the search interval of the generalized ridge penalty parameter. </p>
</td></tr>
<tr><td><code id="optPenaltyGLM.kCVauto_+3A_minsuccdiff">minSuccDiff</code></td>
<td>
<p> A <code>numeric</code>, the minimum distance between the loglikelihoods of two successive iterations to be achieved. Used only if <code>model="logistic"</code>. </p>
</td></tr>
<tr><td><code id="optPenaltyGLM.kCVauto_+3A_maxiter">maxIter</code></td>
<td>
<p>     A <code>numeric</code> specifying the maximum number of iterations. Used only if <code>model="logistic"</code>. </p>
</td></tr>
<tr><td><code id="optPenaltyGLM.kCVauto_+3A_implementation">implementation</code></td>
<td>
<p> A <code>character</code>, either <code>"org"</code> or <code>"alt"</code>, specifying the implementation to be used. The implementations (should) only differ in computation efficiency. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a all-positive <code>numeric</code>, the cross-validated optimal penalty parameters. The average loglikelihood over the left-out samples is used as the cross-validation criterion. If <code>model="linear"</code>, also the average sum-of-squares over the left-out samples is offered as cross-validation criterion.
</p>


<h3>Note</h3>

<p>The joint selection of penalty parameters <code class="reqn">\lambda</code> and <code class="reqn">\lambda_g</code> through the optimization of the cross-validated loss may lead to a locally optimal choice. This is due to the fact that the penalties are to some extent communicating vessels. Both shrink towards the same target, only in slightly (dependending on the specifics of the generalized penalty matrix <code class="reqn">\Delta</code>) different ways. As such, the shrinkage achieved by one penalty may be partially compensated for by the other. This may hamper the algorithm in its search for the global optimizers.
</p>
<p>Moreover, the penalized IRLS (Iterative Reweighted Least Squares) algorithm for the evaluation of the generalized ridge logistic regression estimator and implemented in the <code>ridgeGLM</code>-function may fail to converge for small penalty parameter values in combination with a nonzero shrinkage target. This phenomenon propogates to the <code>optPenaltyGLM.kCVauto</code>-function.
</p>


<h3>Author(s)</h3>

<p>W.N. van Wieringen.
</p>


<h3>References</h3>

<p>van Wieringen, W.N. Binder, H. (2022), &quot;Sequential learning of regression models by penalized estimation&quot;, <em>accepted</em>.
</p>
<p>Lettink, A., Chinapaw, M.J.M., van Wieringen, W.N. et al. (2022), &quot;Two-dimensional fused targeted ridge regression for health indicator prediction from accelerometer data&quot;, <em>submitted</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># set the sample size
n &lt;- 50

# set the true parameter
betas &lt;- (c(0:100) - 50) / 20

# generate covariate data
X &lt;- matrix(rnorm(length(betas)*n), nrow=n)

# sample the response
probs &lt;- exp(tcrossprod(betas, X)[1,]) / (1 + exp(tcrossprod(betas, X)[1,]))
Y     &lt;- numeric()
for (i in 1:n){
    Y &lt;- c(Y, sample(c(0,1), 1, prob=c(1-probs[i], probs[i])))
}

# tune the penalty parameter
optLambda &lt;- optPenaltyGLM.kCVauto(Y, X, lambdaInit=1, fold=5, 
                                  target=betas/2, model="logistic", 
                                  minSuccDiff=10^(-3))

# estimate the logistic regression parameter
bHat &lt;- ridgeGLM(Y, X, lambda=optLambda, target=betas/2, model="logistic")
</code></pre>

<hr>
<h2 id='optPenaltyGLMmultiT.kCVauto'>
Automatic search for optimal penalty parameters of the targeted ridge GLM estimator.
</h2><span id='topic+optPenaltyGLMmultiT.kCVauto'></span>

<h3>Description</h3>

<p>Function finds the optimal penalty parameter of the targeted ridge 
regression estimator of the generalized linear model parameter. The 
optimum is defined as the minimizer of the cross-validated loss 
associated with the estimator. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optPenaltyGLMmultiT.kCVauto(Y, X, lambdaInit, model="linear", targetMat,
                      folds=makeFoldsGLMcv(min(10, length(X)), Y, model=model),
                      loss="loglik", lambdaMin=10^(-5),
                      minSuccDiff=10^(-5), maxIter=100)     
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optPenaltyGLMmultiT.kCVauto_+3A_y">Y</code></td>
<td>
<p>           A <code>numeric</code> being the response vector. </p>
</td></tr>
<tr><td><code id="optPenaltyGLMmultiT.kCVauto_+3A_x">X</code></td>
<td>
<p>           The design <code>matrix</code>. The number of rows should match the number of elements of <code>Y</code>. </p>
</td></tr>
<tr><td><code id="optPenaltyGLMmultiT.kCVauto_+3A_lambdainit">lambdaInit</code></td>
<td>
<p>  A <code>numeric</code> giving the starting values for search of the optimal penalty parameter. </p>
</td></tr>
<tr><td><code id="optPenaltyGLMmultiT.kCVauto_+3A_model">model</code></td>
<td>
<p>       A <code>character</code>, either <code>"linear"</code> and <code>"logistic"</code> (a reference to the models currently implemented), indicating which generalized linear model model instance is to be fitted. </p>
</td></tr>
<tr><td><code id="optPenaltyGLMmultiT.kCVauto_+3A_targetmat">targetMat</code></td>
<td>
<p>   A <code>matrix</code> with targets for the regression parameter as columns. </p>
</td></tr>
<tr><td><code id="optPenaltyGLMmultiT.kCVauto_+3A_folds">folds</code></td>
<td>
<p>       A <code>list</code>. Each list item representing a fold. It is an <code>integer</code> vector indexing the samples that comprise the fold. This object can be generated with the <code>makeFoldsGLMcv</code> function. </p>
</td></tr>
<tr><td><code id="optPenaltyGLMmultiT.kCVauto_+3A_loss">loss</code></td>
<td>
<p>        A <code>character</code>, either <code>loss="loglik"</code> or <code>"sos"</code>, specifying loss criterion to be used in the cross-validation. Used only if <code>model="linear"</code>. </p>
</td></tr>
<tr><td><code id="optPenaltyGLMmultiT.kCVauto_+3A_lambdamin">lambdaMin</code></td>
<td>
<p> A positive <code>numeric</code>, the lower bound of search interval of the regular ridge penalty parameter. </p>
</td></tr>
<tr><td><code id="optPenaltyGLMmultiT.kCVauto_+3A_minsuccdiff">minSuccDiff</code></td>
<td>
<p> A <code>numeric</code>, the minimum distance between the loglikelihoods of two successive iterations to be achieved. Used only if <code>model="logistic"</code>. </p>
</td></tr>
<tr><td><code id="optPenaltyGLMmultiT.kCVauto_+3A_maxiter">maxIter</code></td>
<td>
<p>     A <code>numeric</code> specifying the maximum number of iterations. Used only if <code>model="logistic"</code>. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns an all-positive <code>numeric</code>, the cross-validated optimal penalty parameters. The average loglikelihood over the left-out samples is used as the cross-validation criterion. If <code>model="linear"</code>, also the average sum-of-squares over the left-out samples is offered as cross-validation criterion.
</p>


<h3>Author(s)</h3>

<p>W.N. van Wieringen.
</p>


<h3>References</h3>

<p>van Wieringen, W.N. Binder, H. (2022), &quot;Sequential learning of regression models by penalized estimation&quot;, <em>accepted</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># set the sample size
n &lt;- 50

# set the true parameter
betas &lt;- (c(0:100) - 50) / 20

# generate covariate data
X &lt;- matrix(rnorm(length(betas)*n), nrow=n)

# sample the response
probs &lt;- exp(tcrossprod(betas, X)[1,]) / (1 + exp(tcrossprod(betas, X)[1,]))
Y     &lt;- numeric()
for (i in 1:n){
    Y &lt;- c(Y, sample(c(0,1), 1, prob=c(1-probs[i], probs[i])))
}

# create targets
targets &lt;- cbind(betas/2, rep(0, length(betas)))

# tune the penalty parameter
### optLambdas &lt;- optPenaltyGLMmultiT.kCVauto(Y, X, c(50,0.1), fold=5,            
###                                           targetMat=targets, model="logistic", 
###                                           minSuccDiff=10^(-3))                  

# estimate the logistic regression parameter
### bHat &lt;- ridgeGLMmultiT(Y, X, lambdas=optLambdas, targetMat=targets, model="logistic") 
</code></pre>

<hr>
<h2 id='optPenaltyPgen.kCVauto.banded'>
Automatic search for optimal penalty parameter (generalized ridge precision).
</h2><span id='topic+optPenaltyPgen.kCVauto.banded'></span>

<h3>Description</h3>

<p>Function that determines the optimal penalty parameters through maximization of the k-fold cross-validated log-likelihood score, with a penalization that encourages banded precisions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optPenaltyPgen.kCVauto.banded(Y, lambdaMin, lambdaMax, 
                          lambdaInit=(lambdaMin + lambdaMax)/2,
                          fold=nrow(Y), target, 
                          zeros=matrix(nrow=0, ncol=2), 
                          penalize.diag=TRUE, nInit=100, 
                          minSuccDiff=10^(-5)) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optPenaltyPgen.kCVauto.banded_+3A_y">Y</code></td>
<td>
<p> Data <code>matrix</code> with samples as rows and variates as columns. </p>
</td></tr>
<tr><td><code id="optPenaltyPgen.kCVauto.banded_+3A_lambdamin">lambdaMin</code></td>
<td>
<p> A <code>numeric</code> giving the minimum value for the penalty parameters. One value per group. Values should be specified in the same order as the first appearance of a group representative. </p>
</td></tr>
<tr><td><code id="optPenaltyPgen.kCVauto.banded_+3A_lambdamax">lambdaMax</code></td>
<td>
<p> A <code>numeric</code> giving the maximum value for the penalty parameters. One value per group. Values should be specified in the same order as the first appearance of a group representative. </p>
</td></tr>
<tr><td><code id="optPenaltyPgen.kCVauto.banded_+3A_lambdainit">lambdaInit</code></td>
<td>
<p> A <code>numeric</code> giving the initial (starting) value for the penalty parameters. One value per group. Values should be specified in the same order as the first appearance of a group representative. </p>
</td></tr>
<tr><td><code id="optPenaltyPgen.kCVauto.banded_+3A_fold">fold</code></td>
<td>
<p> A <code>numeric</code> or <code>integer</code> specifying the number of folds to apply in the cross-validation. </p>
</td></tr>
<tr><td><code id="optPenaltyPgen.kCVauto.banded_+3A_target">target</code></td>
<td>
<p> A semi-positive definite target <code>matrix</code> towards which the estimate is shrunken. </p>
</td></tr>
<tr><td><code id="optPenaltyPgen.kCVauto.banded_+3A_zeros">zeros</code></td>
<td>
<p> A two-column <code>matrix</code>, with the first and second column containing the row- and column-index of zero precision elements. </p>
</td></tr>
<tr><td><code id="optPenaltyPgen.kCVauto.banded_+3A_penalize.diag">penalize.diag</code></td>
<td>
<p> A <code>logical</code> indicating whether the diagonal should be penalized. </p>
</td></tr>    
<tr><td><code id="optPenaltyPgen.kCVauto.banded_+3A_ninit">nInit</code></td>
<td>
<p> A <code>numeric</code> specifying the number of iterations. </p>
</td></tr>
<tr><td><code id="optPenaltyPgen.kCVauto.banded_+3A_minsuccdiff">minSuccDiff</code></td>
<td>
<p> A <code>numeric</code>: minimum successive difference (in terms of their penalized loglikelihood) between two succesive estimates to be achieved. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The penalty matrix <code class="reqn">\boldsymbol{\Lambda}</code> is parametrized as follows. The elements of <code class="reqn">\boldsymbol{\Lambda}</code> are <code class="reqn">(\boldsymbol{\Lambda})_{j,j'} = \lambda (| j - j'| + 1)</code> for 
<code class="reqn">j, j' = 1, \ldots, p</code>.
</p>


<h3>Value</h3>

<p>The function returns a <code>numeric</code> containing the cross-validated optimal positive penalty parameters.
</p>


<h3>Author(s)</h3>

<p>W.N. van Wieringen.
</p>


<h3>References</h3>

<p>van Wieringen, W.N. (2019), &quot;The generalized ridge estimator of the inverse covariance matrix&quot;, <em>Journal of Computational and Graphical Statistics</em>, 28(4), 932-942.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ridgePgen">ridgePgen</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># set dimension and sample size
p &lt;- 10
n &lt;- 10

# penalty parameter matrix
lambda       &lt;- matrix(1, p, p)
diag(lambda) &lt;- 0.1

# generate precision matrix
Omega       &lt;- matrix(0.4, p, p)
diag(Omega) &lt;- 1
Sigma       &lt;- solve(Omega)

# data 
Y &lt;- mvtnorm::rmvnorm(n, mean=rep(0,p), sigma=Sigma)
S &lt;- cov(Y)

# find optimal penalty parameters through cross-validation
lambdaOpt &lt;- optPenaltyPgen.kCVauto.banded(Y, 10^(-10), 10^(10),
                          target=matrix(0, p, p),
                          penalize.diag=FALSE, nInit=100, 
                          minSuccDiff=10^(-5)) 

# format the penalty matrix
lambdaOptMat &lt;- matrix(NA, p, p)
for (j1 in 1:p){
    for (j2 in 1:p){
        lambdaOptMat[j1, j2] &lt;- lambdaOpt * (abs(j1-j2)+1)
    }
}

# generalized ridge precision estimate
Phat &lt;- ridgePgen(S, lambdaOptMat, matrix(0, p, p))
</code></pre>

<hr>
<h2 id='optPenaltyPgen.kCVauto.groups'>
Automatic search for optimal penalty parameter (generalized ridge precision).
</h2><span id='topic+optPenaltyPgen.kCVauto.groups'></span>

<h3>Description</h3>

<p>Function that determines the optimal penalty parameters through maximization of the k-fold cross-validated log-likelihood score, assuming that variates are grouped and penalized group-wise.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optPenaltyPgen.kCVauto.groups(Y, lambdaMin, lambdaMax, 
                          lambdaInit=(lambdaMin + lambdaMax)/2,
                          fold=nrow(Y), groups, target, 
                          zeros=matrix(nrow=0, ncol=2), 
                          penalize.diag=TRUE, nInit=100, 
                          minSuccDiff=10^(-5)) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optPenaltyPgen.kCVauto.groups_+3A_y">Y</code></td>
<td>
<p> Data <code>matrix</code> with samples as rows and variates as columns. </p>
</td></tr>
<tr><td><code id="optPenaltyPgen.kCVauto.groups_+3A_lambdamin">lambdaMin</code></td>
<td>
<p> A <code>numeric</code> giving the minimum value for the penalty parameters. One value per group. Values should be specified in the same order as the first appearance of a group representative. </p>
</td></tr>
<tr><td><code id="optPenaltyPgen.kCVauto.groups_+3A_lambdamax">lambdaMax</code></td>
<td>
<p> A <code>numeric</code> giving the maximum value for the penalty parameters. One value per group. Values should be specified in the same order as the first appearance of a group representative. </p>
</td></tr>
<tr><td><code id="optPenaltyPgen.kCVauto.groups_+3A_lambdainit">lambdaInit</code></td>
<td>
<p> A <code>numeric</code> giving the initial (starting) value for the penalty parameters. One value per group. Values should be specified in the same order as the first appearance of a group representative. </p>
</td></tr>
<tr><td><code id="optPenaltyPgen.kCVauto.groups_+3A_fold">fold</code></td>
<td>
<p> A <code>numeric</code> or <code>integer</code> specifying the number of folds to apply in the cross-validation. </p>
</td></tr>
<tr><td><code id="optPenaltyPgen.kCVauto.groups_+3A_groups">groups</code></td>
<td>
<p> A<code>numeric</code> indicating to which group a variate belongs. Same values indicate same group. </p>
</td></tr>
<tr><td><code id="optPenaltyPgen.kCVauto.groups_+3A_target">target</code></td>
<td>
<p> A semi-positive definite target <code>matrix</code> towards which the estimate is shrunken. </p>
</td></tr>
<tr><td><code id="optPenaltyPgen.kCVauto.groups_+3A_zeros">zeros</code></td>
<td>
<p> A two-column <code>matrix</code>, with the first and second column containing the row- and column-index of zero precision elements. </p>
</td></tr>
<tr><td><code id="optPenaltyPgen.kCVauto.groups_+3A_penalize.diag">penalize.diag</code></td>
<td>
<p> A <code>logical</code> indicating whether the diagonal should be penalized. </p>
</td></tr>    
<tr><td><code id="optPenaltyPgen.kCVauto.groups_+3A_ninit">nInit</code></td>
<td>
<p> A <code>numeric</code> specifying the number of iterations. </p>
</td></tr>
<tr><td><code id="optPenaltyPgen.kCVauto.groups_+3A_minsuccdiff">minSuccDiff</code></td>
<td>
<p> A <code>numeric</code>: minimum successive difference (in terms of their penalized loglikelihood) between two succesive estimates to be achieved. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The penalty matrix <code class="reqn">\boldsymbol{\Lambda}</code> is parametrized as follows. The elements of <code class="reqn">\boldsymbol{\Lambda}</code> are <code class="reqn">(\boldsymbol{\Lambda})_{j,j'} = \frac{1}{2} (\lambda_k + \lambda_{k'})</code> for 
<code class="reqn">j, j' = 1, \ldots, p</code> if <code class="reqn">j</code> and <code class="reqn">j'</code> belong to groups <code class="reqn">k</code> and <code class="reqn">k'</code>, respectively, where <code class="reqn">\lambda_k</code> and <code class="reqn">\lambda_{k'}</code> are the corresponding group-specific penalty parameters.
</p>


<h3>Value</h3>

<p>The function returns a <code>numeric</code> containing the cross-validated optimal positive penalty parameters.
</p>


<h3>Author(s)</h3>

<p>W.N. van Wieringen.
</p>


<h3>References</h3>

<p>van Wieringen, W.N. (2019), &quot;The generalized ridge estimator of the inverse covariance matrix&quot;, <em>Journal of Computational and Graphical Statistics</em>, 28(4), 932-942.
</p>


<h3>See Also</h3>

<p><code>ridgePgen</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># set dimension and sample size
p &lt;- 10
n &lt;- 10

# penalty parameter matrix
lambda       &lt;- matrix(1, p, p)
diag(lambda) &lt;- 0.1

# generate precision matrix
Omega       &lt;- matrix(0.4, p, p)
diag(Omega) &lt;- 1
Sigma       &lt;- solve(Omega)

# data 
Y &lt;- mvtnorm::rmvnorm(n, mean=rep(0,p), sigma=Sigma)
S &lt;- cov(Y)

# find optimal penalty parameters through cross-validation
lambdaOpt &lt;- optPenaltyPgen.kCVauto.groups(Y, rep(10^(-10), 2), rep(10^(10), 2), 
                          groups=c(rep(0, p/2), rep(1, p/2)), 
                          target=matrix(0, p, p),
                          penalize.diag=FALSE, nInit=100, 
                          minSuccDiff=10^(-5)) 

# format the penalty matrix
lambdaOptVec &lt;- c(rep(lambdaOpt[1], p/2), rep(lambdaOpt[2], p/2))
lambdaOptMat &lt;- outer(lambdaOptVec, lambdaOptVec, "+")

# generalized ridge precision estimate
Phat &lt;- ridgePgen(S, lambdaOptMat, matrix(0, p, p))
</code></pre>

<hr>
<h2 id='optPenaltyPmultiT.kCVauto'>
Automatic search for optimal penalty parameter (ridge precision with multi-targets).
</h2><span id='topic+optPenaltyPmultiT.kCVauto'></span>

<h3>Description</h3>

<p>Function that determines the optimal penalty parameters through maximization of the k-fold cross-validated log-likelihood score, assuming that variates are grouped and penalized group-wise.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optPenaltyPmultiT.kCVauto(Y, lambdaMin, lambdaMax, 
                          lambdaInit=(lambdaMin+lambdaMax)/2,
                          fold=nrow(Y), targetList) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optPenaltyPmultiT.kCVauto_+3A_y">Y</code></td>
<td>
<p> Data <code>matrix</code> with samples as rows and variates as columns. </p>
</td></tr>
<tr><td><code id="optPenaltyPmultiT.kCVauto_+3A_lambdamin">lambdaMin</code></td>
<td>
<p> A <code>numeric</code> giving the minimum value for the penalty parameters. One value per target. Values should be specified in the same order as the target's appearance in <code>targetList</code>. </p>
</td></tr>
<tr><td><code id="optPenaltyPmultiT.kCVauto_+3A_lambdamax">lambdaMax</code></td>
<td>
<p> A <code>numeric</code> giving the maximum value for the penalty parameters. One value per group. Values should be specified in the same order as the target's appearance in <code>targetList</code>. </p>
</td></tr>
<tr><td><code id="optPenaltyPmultiT.kCVauto_+3A_lambdainit">lambdaInit</code></td>
<td>
<p> A <code>numeric</code> giving the initial (starting) value for the penalty parameters. One value per group. Values should be specified in the same order as the target's appearance in <code>targetList</code>. </p>
</td></tr>
<tr><td><code id="optPenaltyPmultiT.kCVauto_+3A_fold">fold</code></td>
<td>
<p> A <code>numeric</code> or <code>integer</code> specifying the number of folds to apply in the cross-validation. </p>
</td></tr>
<tr><td><code id="optPenaltyPmultiT.kCVauto_+3A_targetlist">targetList</code></td>
<td>
<p> A list of semi-positive definite target matrices towards which the precision matrix is potentially shrunken. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a <code>numeric</code> containing the cross-validated optimal positive penalty parameters.
</p>


<h3>Author(s)</h3>

<p>W.N. van Wieringen.
</p>


<h3>References</h3>

<p>van Wieringen, W.N., Stam, K.A., Peeters, C.F.W., van de Wiel, M.A. (2020), &quot;Updating of the Gaussian graphical model through targeted penalized estimation&quot;, <em>Journal of Multivariate Analysis</em>, 178, Article 104621.
</p>


<h3>See Also</h3>

<p><code>ridgePmultiT</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># set dimension and sample size
p &lt;- 10
n &lt;- 10

# specify vector of penalty parameters
lambda       &lt;- c(2, 1)

# generate precision matrix
T1       &lt;- matrix(0.7, p, p)
diag(T1) &lt;- 1
T2       &lt;- diag(rep(2, p))

# generate precision matrix
Omega       &lt;- matrix(0.4, p, p)
diag(Omega) &lt;- 2
Sigma       &lt;- solve(Omega)

# data 
Y &lt;- mvtnorm::rmvnorm(n, mean=rep(0,p), sigma=Sigma)
S &lt;- cov(Y)

# find optimal penalty parameters through cross-validation
lambdaOpt &lt;- optPenaltyPmultiT.kCVauto(Y, rep(10^(-10), 2), 
                          rep(10^(10), 2), rep(1, 2), 
                          targetList=list(T1=T1, T2=T2)) 

# unpenalized diagonal estimate
Phat &lt;- ridgePmultiT(S, lambdaOpt, list(T1=T1, T2=T2))
</code></pre>

<hr>
<h2 id='optPenaltyPrep.kCVauto'>
Automatic search for optimal penalty parameters (for precision estimation of data with replicates).
</h2><span id='topic+optPenaltyPrep.kCVauto'></span>

<h3>Description</h3>

<p>Function that performs an automatic search of the optimal penalty parameter for the <code>ridgePrep</code> call by employing either the Nelder-Mead or quasi-Newton 
method to calculate the cross-validated (negative) log-likelihood score.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optPenaltyPrep.kCVauto(Y, ids, lambdaInit, 
                       fold=nrow(Y), CVcrit, 
                       splitting="stratified",
                       targetZ=matrix(0, ncol(Y), ncol(Y)),
                       targetE=matrix(0, ncol(Y), ncol(Y)),
                       nInit=100, minSuccDiff=10^(-10))

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optPenaltyPrep.kCVauto_+3A_y">Y</code></td>
<td>
<p> Data <code>matrix</code> with samples (including the repetitions) as rows and variates as columns. </p>
</td></tr>
<tr><td><code id="optPenaltyPrep.kCVauto_+3A_ids">ids</code></td>
<td>
<p> A <code>numeric</code> indicating which rows of <code>Y</code> belong to the same individal.</p>
</td></tr>
<tr><td><code id="optPenaltyPrep.kCVauto_+3A_lambdainit">lambdaInit</code></td>
<td>
<p> A <code>numeric</code> giving the initial (starting) values for the two penalty parameters. </p>
</td></tr>
<tr><td><code id="optPenaltyPrep.kCVauto_+3A_fold">fold</code></td>
<td>
<p> A <code>numeric</code> or <code>integer</code> specifying the number of folds to apply in the cross-validation. </p>
</td></tr>
<tr><td><code id="optPenaltyPrep.kCVauto_+3A_cvcrit">CVcrit</code></td>
<td>
<p> A <code>character</code> with the cross-validation criterion to applied. Either <code>CVcrit="LL"</code> (the loglikelihood) or <code>CVcrit="Qloss"</code> (the quadratic loss). </p>
</td></tr>
<tr><td><code id="optPenaltyPrep.kCVauto_+3A_splitting">splitting</code></td>
<td>
<p> A <code>character</code>, either <code>splitting="replications"</code>, <code>splitting="samples"</code>, or <code>splitting="stratified"</code>, specifying either how the splits are to be formed: either  replications or samples are randomly divided over the <code>fold</code> splits (first two options, respectively), or samples are randomly divided over the <code>fold</code> splits but in a stratified manner such that the total number of replicates in each group is roughly comparable. </p>
</td></tr>
<tr><td><code id="optPenaltyPrep.kCVauto_+3A_targetz">targetZ</code></td>
<td>
<p> A semi-positive definite target <code>matrix</code> towards which the signal precision matrix estimate is shrunken. </p>
</td></tr>
<tr><td><code id="optPenaltyPrep.kCVauto_+3A_targete">targetE</code></td>
<td>
<p> A semi-positive definite target <code>matrix</code> towards which the error precision matrix estimate is shrunken. </p>
</td></tr>
<tr><td><code id="optPenaltyPrep.kCVauto_+3A_ninit">nInit</code></td>
<td>
<p> A <code>numeric</code> specifying the number of iterations. </p>
</td></tr>
<tr><td><code id="optPenaltyPrep.kCVauto_+3A_minsuccdiff">minSuccDiff</code></td>
<td>
<p> A <code>numeric</code>: minimum successive difference (in terms of the relative change in the absolute difference of the penalized loglikelihood) between two succesive estimates to be achieved. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns an all-positive <code>numeric</code>, the cross-validated optimal penalty parameters.
</p>


<h3>Author(s)</h3>

<p>W.N. van Wieringen.
</p>


<h3>References</h3>

<p>van Wieringen, W.N., Chen, Y. (2021), &quot;Penalized estimation of the Gaussian graphical model from data with replicates&quot;, <em>Statistics in Medicine</em>, 40(19), 4279-4293.
</p>


<h3>See Also</h3>

<p><code>ridgePrep</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># set parameters
p        &lt;- 10
Se       &lt;- diag(runif(p))
Sz       &lt;- matrix(3, p, p)
diag(Sz) &lt;- 4

# draw data
n &lt;- 100
ids &lt;- numeric()
Y   &lt;- numeric()
for (i in 1:n){
     Ki &lt;- sample(2:5, 1)
     Zi &lt;- mvtnorm::rmvnorm(1, sigma=Sz)
     for (k in 1:Ki){
          Y   &lt;- rbind(Y, Zi + mvtnorm::rmvnorm(1, sigma=Se))
          ids &lt;- c(ids, i)
     }
}

# find optimal penalty parameters
### optLambdas &lt;- optPenaltyPrep.kCVauto(Y, ids,             
###                                      lambdaInit=c(1,1),  
###                                      fold=nrow(Y),       
###                                      CVcrit="LL")        

# estimate the precision matrices
### Ps &lt;- ridgePrep(Y, ids, optLambdas[1], optLambdas[2])    
</code></pre>

<hr>
<h2 id='optPenaltyPrepEdiag.kCVauto'>
Automatic search for optimal penalty parameters (for precision estimation of data with replicates).
</h2><span id='topic+optPenaltyPrepEdiag.kCVauto'></span>

<h3>Description</h3>

<p>Function that performs an automatic search of the optimal penalty parameter for the <code>ridgePrepEdiag</code> call by employing either the Nelder-Mead or quasi-Newton 
method to calculate of the cross-validated (negative) log-likelihood score.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optPenaltyPrepEdiag.kCVauto(Y, ids, lambdaInit, 
                            fold=nrow(Y), CVcrit, 
                            splitting="stratified",
                            targetZ=matrix(0, ncol(Y), ncol(Y)),
                            nInit=100, minSuccDiff=10^(-10))

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optPenaltyPrepEdiag.kCVauto_+3A_y">Y</code></td>
<td>
<p> Data <code>matrix</code> with samples (including the repetitions) as rows and variates as columns. </p>
</td></tr>
<tr><td><code id="optPenaltyPrepEdiag.kCVauto_+3A_ids">ids</code></td>
<td>
<p> A <code>numeric</code> indicating which rows of <code>Y</code> belong to the same individal.</p>
</td></tr>
<tr><td><code id="optPenaltyPrepEdiag.kCVauto_+3A_lambdainit">lambdaInit</code></td>
<td>
<p> A <code>numeric</code> giving the initial (starting) values for the two penalty parameters. </p>
</td></tr>
<tr><td><code id="optPenaltyPrepEdiag.kCVauto_+3A_fold">fold</code></td>
<td>
<p> A <code>numeric</code> or <code>integer</code> specifying the number of folds to apply in the cross-validation. </p>
</td></tr>
<tr><td><code id="optPenaltyPrepEdiag.kCVauto_+3A_cvcrit">CVcrit</code></td>
<td>
<p> A <code>character</code> with the cross-validation criterion to applied. Either <code>CVcrit="LL"</code> (the loglikelihood) or <code>CVcrit="Qloss"</code> (the quadratic loss). </p>
</td></tr>
<tr><td><code id="optPenaltyPrepEdiag.kCVauto_+3A_splitting">splitting</code></td>
<td>
<p> A <code>character</code>, either <code>splitting="replications"</code>, <code>splitting="samples"</code>, or <code>splitting="stratified"</code>, specifying either how the splits are to be formed: either  replications or samples are randomly divided over the <code>fold</code> splits (first two options, respectively), or samples are randomly divided over the <code>fold</code> splits but in stratified manner such that the total number of replications in each group is roughly comparable. </p>
</td></tr>
<tr><td><code id="optPenaltyPrepEdiag.kCVauto_+3A_targetz">targetZ</code></td>
<td>
<p> A semi-positive definite target <code>matrix</code> towards which the signal precision matrix estimate is shrunken. </p>
</td></tr>
<tr><td><code id="optPenaltyPrepEdiag.kCVauto_+3A_ninit">nInit</code></td>
<td>
<p> A <code>numeric</code> specifying the number of iterations. </p>
</td></tr>
<tr><td><code id="optPenaltyPrepEdiag.kCVauto_+3A_minsuccdiff">minSuccDiff</code></td>
<td>
<p> A <code>numeric</code>: minimum successive difference (in terms of the relative change in the absolute difference of the penalized loglikelihood) between two succesive estimates to be achieved. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns an all-positive <code>numeric</code>, the cross-validated optimal penalty parameters.
</p>


<h3>Author(s)</h3>

<p>W.N. van Wieringen.
</p>


<h3>References</h3>

<p>van Wieringen, W.N., Chen, Y. (2021), &quot;Penalized estimation of the Gaussian graphical model from data with replicates&quot;, <em>Statistics in Medicine</em>, 40(19), 4279-4293.
</p>


<h3>See Also</h3>

<p><code>ridgePrepEdiag</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># set parameters
p        &lt;- 10
Se       &lt;- diag(runif(p))
Sz       &lt;- matrix(3, p, p)
diag(Sz) &lt;- 4

# draw data
n &lt;- 100
ids &lt;- numeric()
Y   &lt;- numeric()
for (i in 1:n){
     Ki &lt;- sample(2:5, 1)
     Zi &lt;- mvtnorm::rmvnorm(1, sigma=Sz)
     for (k in 1:Ki){
          Y   &lt;- rbind(Y, Zi + mvtnorm::rmvnorm(1, sigma=Se))
          ids &lt;- c(ids, i)
     }
}

# find optimal penalty parameters
### optLambdas &lt;- optPenaltyPrepEdiag.kCVauto(Y, ids,             
###                                           lambdaInit=c(1,1),  
###                                           fold=nrow(Y),       
###                                           CVcrit="LL")        

# estimate the precision matrices
### Ps &lt;- ridgePrepEdiag(Y, ids, optLambdas[1], optLambdas[2])    
</code></pre>

<hr>
<h2 id='porridge-package'>Ridge-Type Penalized Estimation of a Potpourri of Models.</h2><span id='topic+porridge-package'></span><span id='topic+porridge'></span>

<h3>Description</h3>

<p>The following functions facilitate the ridge-type penalized estimation of various models. Currently, it includes: 
</p>

<ul>
<li><p> Generalized ridge estimation of the precision matrix of a Gaussian graphical model (van Wieringen, 2019) through the function <code><a href="#topic+ridgePgen">ridgePgen</a></code>. This function is complemented by the functions <code><a href="#topic+ridgePgen.kCV">ridgePgen.kCV</a></code>, <code><a href="#topic+ridgePgen.kCV.banded">ridgePgen.kCV.banded</a></code>, <code><a href="#topic+ridgePgen.kCV.groups">ridgePgen.kCV.groups</a></code>, 
<code><a href="#topic+optPenaltyPgen.kCVauto.banded">optPenaltyPgen.kCVauto.banded</a></code> and <code><a href="#topic+optPenaltyPgen.kCVauto.groups">optPenaltyPgen.kCVauto.groups</a></code> for penalty parameters selection through K-fold cross-validation assuming a particularly structured precision matrix. 
</p>
</li>
<li><p> Multi-targeted ridge estimation of the precision matrix of a Gaussian graphical model (van Wieringen et al., 2020) through the functions <code><a href="#topic+ridgePmultiT">ridgePmultiT</a></code>. This function is complemented by the functions <code><a href="#topic+optPenaltyPmultiT.kCVauto">optPenaltyPmultiT.kCVauto</a></code> for penalty parameters selection through K-fold cross-validation. 
</p>
</li>
<li><p> Gaussian graphical model estimation from data with replicates in ridge penalized fashion (van Wieringen, Chen, 2021) (<code><a href="#topic+ridgePrep">ridgePrep</a></code> and <code><a href="#topic+ridgePrepEdiag">ridgePrepEdiag</a></code>). The two functions 
<code><a href="#topic+optPenaltyPrep.kCVauto">optPenaltyPrep.kCVauto</a></code> and <code><a href="#topic+optPenaltyPrepEdiag.kCVauto">optPenaltyPrepEdiag.kCVauto</a></code> implement the corresponding K-fold cross-validation procedures for an optimal choice of the penalty parameter.
</p>
</li>
<li><p> Ridge penalized estimation of a mixture of Gaussian graphical models: <code><a href="#topic+ridgeGGMmixture">ridgeGGMmixture</a></code> and its penalty selection via K-fold cross-validation 
<code><a href="#topic+optPenaltyGGMmixture.kCVauto">optPenaltyGGMmixture.kCVauto</a></code>. 
</p>
</li>
<li><p> Targeted and multi-targeted ridge estimation of the regression parameter of the generalized linear model (van Wieringen, Binder, 2022; van Wieringen, 2021; Lettink et al., 2022) through the functions <code><a href="#topic+ridgeGLM">ridgeGLM</a></code> and <code><a href="#topic+ridgeGLMmultiT">ridgeGLMmultiT</a></code>. This function is complemented by the functions <code><a href="#topic+optPenaltyGLM.kCVauto">optPenaltyGLM.kCVauto</a></code> and <code><a href="#topic+optPenaltyGLMmultiT.kCVauto">optPenaltyGLMmultiT.kCVauto</a></code> for penalty parameters selection through K-fold cross-validation, and the <code>ridgeGLMdof</code>-function for the evaluation of the fitted model's degrees of freedom.  
</p>
</li></ul>

<p>Future versions aim to include more ridge-type functionality.
</p>
<p>In part the <code>porridge</code>-package extends/builds upon the <code><a href="rags2ridges.html#topic+rags2ridges">rags2ridges</a></code>-packages, in which some or all functionality of the <code>porridge</code>-package may be absorped at some point in the future.
</p>


<h3>Author(s)</h3>

<p>Wessel N. van Wieringen &lt;w.vanwieringen@vumc.ml&gt;
</p>


<h3>References</h3>

<p>Aflakparast, M., de Gunst, M.C.M., van Wieringen, W.N. (2018), &quot;Reconstruction of molecular network evolution from cross-sectional omics data&quot;, <em>Biometrical Journal</em>, 60(3), 547-563.
</p>
<p>Lettink, A., Chinapaw, M.J.M., van Wieringen, W.N. (2022), &quot;Two-dimensional fused targeted ridge regression for health indicator prediction from accelerometer data&quot;, <em>submitted</em>.
</p>
<p>Peeters, C.F.W., Bilgrau, A.E., and van Wieringen, W.N. (2021), &quot;<code>rags2ridges</code>: Ridge Estimation of Precision Matrices from  High-Dimensional Data&quot;, R package version 2.2.5. <a href="https://CRAN.R-project.org/package=rags2ridges">https://CRAN.R-project.org/package=rags2ridges</a>.
</p>
<p>van Wieringen, W.N. (2019), &quot;The generalized ridge estimator of the inverse covariance matrix&quot;, <em>Journal of Computational and Graphical Statistics</em>, 28(4), 932-942.
</p>
<p>van Wieringen, W.N. (2021), &quot;Lecture notes on ridge regression&quot;, <em>Arxiv preprint</em>, arXiv:1509.09169.
</p>
<p>van Wieringen W.N., Chen, Y. (2021), &quot;Penalized estimation of the Gaussian graphical model from data with replicates&quot;, <em>Statistics in Medicine</em>, 40(19), 4279-4293.
</p>
<p>van Wieringen, W.N., Stam, K.A., Peeters, C.F.W., van de Wiel, M.A. (2020), &quot;Updating of the Gaussian graphical model through targeted penalized estimation&quot;, <em>Journal of Multivariate Analysis</em>, 178, Article 104621.
</p>
<p>van Wieringen, W.N. Binder, H. (2022), &quot;Sequential learning of regression models by penalized estimation&quot;, <em>Journal of Computational and Graphical Statistics</em>, accepted.
</p>


<h3>See Also</h3>

<p>The <code>porridge</code>-package.
</p>

<hr>
<h2 id='ridgeGGMmixture'>
Ridge penalized estimation of a mixture of GGMs.
</h2><span id='topic+ridgeGGMmixture'></span>

<h3>Description</h3>

<p>Function that estimates a mixture of GGMs (Gaussian graphical models) through a ridge penalized EM (Expectation-Maximization) algorithm as described in Aflakparast <em>et al</em>. (2018).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ridgeGGMmixture(Y, K, lambda, target,                                    
                iWeights=matrix(sample(seq(0+1/nrow(Y), 
                                1-1/nrow(Y), by=1/(2*nrow(Y))), 
                                nrow(Y)*K, replace=TRUE), 
                                nrow=nrow(Y), ncol=K),
                nInit=100, minSuccDiff=10^(-10),
                minMixProp=0.01)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ridgeGGMmixture_+3A_y">Y</code></td>
<td>
<p> Data <code>matrix</code> with samples as rows and variates as columns. </p>
</td></tr>
<tr><td><code id="ridgeGGMmixture_+3A_k">K</code></td>
<td>
<p> A <code>numeric</code>, specifying the number of mixture components.</p>
</td></tr>
<tr><td><code id="ridgeGGMmixture_+3A_lambda">lambda</code></td>
<td>
<p> A positive <code>numeric</code> representing the ridge penalty parameter. </p>
</td></tr>
<tr><td><code id="ridgeGGMmixture_+3A_target">target</code></td>
<td>
<p> A semi-positive definite target <code>matrix</code> towards which the estimate is shrunken. </p>
</td></tr>
<tr><td><code id="ridgeGGMmixture_+3A_iweights">iWeights</code></td>
<td>
<p> Sample-specific positive component weight <code>matrix</code>. Rows correspond to samples, while columns to components. </p>
</td></tr> 
<tr><td><code id="ridgeGGMmixture_+3A_ninit">nInit</code></td>
<td>
<p> A <code>numeric</code> specifying the number of iterations. </p>
</td></tr>
<tr><td><code id="ridgeGGMmixture_+3A_minsuccdiff">minSuccDiff</code></td>
<td>
<p> A <code>numeric</code>: minimum successive difference (in terms of their penalized loglikelihood) between two succesive estimates to be achieved. </p>
</td></tr>
<tr><td><code id="ridgeGGMmixture_+3A_minmixprop">minMixProp</code></td>
<td>
<p> Smallest mixing probability tolerated. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The data are assumed to follow a mixture of <code class="reqn">K</code> Gaussian graphical models:
</p>
<p style="text-align: center;"><code class="reqn"> \mathbf{Y}_i \sim \sum\nolimits_{k=1}^K \theta_k \mathcal{N}(\boldsymbol{\mu}_k, \boldsymbol{\Omega}_k^{-1}), </code>
</p>

<p>where <code class="reqn">\theta_k = P(Z_i = k)</code> is the probability that the <code class="reqn">i</code>-th sample stems from the <code class="reqn">k</code>-the component. The model parameters are estimated by ridge penalized likelihood maximization:
</p>
<p style="text-align: center;"><code class="reqn"> \sum\nolimits_{i=1}^n \log [ \sum\nolimits_{k=1}^K \theta_k P(\mathbf{Y}_i \, | \, Z_i = k; \boldsymbol{\mu}_k, \boldsymbol{\Omega}_k) ] + \lambda \sum\nolimits_{k=1}^K \| \boldsymbol{\Omega}_k - \mathbf{T}_k \|_F^2, </code>
</p>

<p>where <code class="reqn">\lambda</code> is the penalty parameter and <code class="reqn">\mathbf{T}_k</code> is the shrinkage target of the <code class="reqn">k</code>-th component's precision matrix. This function yields the maximizer of this penalized loglikelihood, which is found by means of a penalized EM algorithm.
</p>


<h3>Value</h3>

<p>The function returns a regularized inverse covariance <code>list</code>-object with slots:
</p>
<table>
<tr><td><code>mu</code></td>
<td>
<p> A <code>matrix</code> with estimated mean vectors are rows. </p>
</td></tr>
<tr><td><code>P</code></td>
<td>
<p> A <code>matrix</code> with estimated mixture precision matrices stacked on top of each other. </p>
</td></tr>
<tr><td><code>pi</code></td>
<td>
<p> A <code>numeric</code> wth estimated mixing probabilities.</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p> A <code>matrix</code> wth estimated component memberships.</p>
</td></tr>
<tr><td><code>penLL</code></td>
<td>
<p> A <code>numeric</code> with the penalized loglikelihood of the estimated model. </p>
</td></tr>
</table>


<h3>Note</h3>

<p>The elements of <code>iWeights</code> may be larger than one as they are rescaled internally to sum to one.
</p>


<h3>Author(s)</h3>

<p>W.N. van Wieringen, M. Aflakparast.
</p>


<h3>References</h3>

<p>Aflakparast, M., de Gunst, M.C.M., van Wieringen, W.N. (2018), &quot;Reconstruction of molecular network evolution from cross-sectional omics data&quot;, <em>Biometrical Journal</em>, 60(3), 547-563.
</p>


<h3>See Also</h3>

<p><code>optPenaltyGGMmixture.kCVauto</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># define mixing proportions
pis &lt;- c(0.2, 0.3, 0.4)

# set dimension and sample size
p &lt;- 5
n &lt;- 100

# define population covariance matrices
diags       &lt;- list(rep(1, p), 
                    rep(0.5, p-1), 
                    rep(0.25, p-2), 
                    rep(0.1, p-3))
Omega       &lt;- as.matrix(Matrix::bandSparse(p, 
                                            k=-c(0:3), 
                                            diag=c(diags), 
                                            symm=TRUE))
Sigma1      &lt;- solve(Omega)
Omega       &lt;- matrix(0.3, p, p)
diag(Omega) &lt;- 1
Sigma2      &lt;- solve(Omega)
Sigma3      &lt;- cov(matrix(rnorm(p*n), ncol=p))

# mean vectors
mean1 &lt;- rep(0,p)
mean2 &lt;- rexp(p)
mean3 &lt;- rnorm(p)

# draw data data from GGM mixture
Z &lt;- sort(sample(c(1:3), n, prob=pis, replace=TRUE))
Y &lt;- rbind(mvtnorm::rmvnorm(sum(Z==1), mean=mean1, sigma=Sigma1),
           mvtnorm::rmvnorm(sum(Z==2), mean=mean2, sigma=Sigma2),
           mvtnorm::rmvnorm(sum(Z==3), mean=mean3, sigma=Sigma3))

# find optimal penalty parameter
optLambda &lt;- optPenaltyGGMmixture.kCVauto(Y,  K=3,          
                                          0.00001, 100,     
                                          10, fold=5,       
                                          target=0*Sigma1)  

# ridge penalized estimation of the GGM mixture
ridgeGGMmixFit &lt;- ridgeGGMmixture(Y, 3, 1, target=0*Sigma1)
</code></pre>

<hr>
<h2 id='ridgeGLM'>
Ridge estimation of generalized linear models.
</h2><span id='topic+ridgeGLM'></span>

<h3>Description</h3>

<p>Function that evaluates the targeted ridge estimator of the regression parameter of generalized linear models. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ridgeGLM(Y, X, U=matrix(ncol=0, nrow=length(Y)), lambda, 
         lambdaG=0, Dg=matrix(0, ncol=ncol(X), nrow=ncol(X)), 
         target=rep(0, ncol(X)), model="linear", 
         minSuccDiff=10^(-10), maxIter=100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ridgeGLM_+3A_y">Y</code></td>
<td>
<p>           A <code>numeric</code> being the response vector. </p>
</td></tr>
<tr><td><code id="ridgeGLM_+3A_x">X</code></td>
<td>
<p>           The design <code>matrix</code> of the penalized covariates. The number of rows should match the number of elements of <code>Y</code>. </p>
</td></tr>
<tr><td><code id="ridgeGLM_+3A_u">U</code></td>
<td>
<p>           The design <code>matrix</code> of the unpenalized covariates. The number of rows should match the number of elements of <code>Y</code>. </p>
</td></tr>
<tr><td><code id="ridgeGLM_+3A_lambda">lambda</code></td>
<td>
<p>      A positive <code>numeric</code> that is the ridge penalty parameter. </p>
</td></tr>
<tr><td><code id="ridgeGLM_+3A_lambdag">lambdaG</code></td>
<td>
<p>     A positive <code>numeric</code> that is the generalized ridge penalty parameter. </p>
</td></tr>
<tr><td><code id="ridgeGLM_+3A_dg">Dg</code></td>
<td>
<p>          A non-negative definite <code>matrix</code> of the unscaled generalized ridge penalty. </p>
</td></tr>
<tr><td><code id="ridgeGLM_+3A_target">target</code></td>
<td>
<p>      A <code>numeric</code> towards which the estimate is shrunken. </p>
</td></tr>
<tr><td><code id="ridgeGLM_+3A_model">model</code></td>
<td>
<p>       A <code>character</code>, either <code>"linear"</code> and <code>"logistic"</code> (a reference to the models currently implemented), indicating which generalized linear model model instance is to be fitted. </p>
</td></tr>
<tr><td><code id="ridgeGLM_+3A_minsuccdiff">minSuccDiff</code></td>
<td>
<p> A <code>numeric</code>, the minimum distance between the loglikelihoods of two successive iterations to be achieved. Used only if <code>model="logistic"</code>. </p>
</td></tr>
<tr><td><code id="ridgeGLM_+3A_maxiter">maxIter</code></td>
<td>
<p>     A <code>numeric</code> specifying the maximum number of iterations. Used only if <code>model="logistic"</code>. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function finds the maximizer of the following penalized loglikelihood: <code class="reqn"> \mathcal{L}( \mathbf{Y}, \mathbf{X}, \mathbf{U}; \boldsymbol{\beta}, \boldsymbol{\gamma}) - \frac{1}{2} \lambda \| \boldsymbol{\beta}  - \boldsymbol{\beta}_0 \|_2^2 - \frac{1}{2} \lambda_g ( \boldsymbol{\beta}  - \boldsymbol{\beta}_0 )^\top \Delta_g ( \boldsymbol{\beta}  - \boldsymbol{\beta}_0 )</code>, with loglikelihood <code class="reqn">\mathcal{L}( \mathbf{Y}, \mathbf{X}; \boldsymbol{\beta})</code>, response <code class="reqn">\mathbf{Y}</code>, design matrices <code class="reqn">\mathbf{X}</code> and <code class="reqn">\mathbf{U}</code>, regression parameters <code class="reqn">\boldsymbol{\beta}</code> and <code class="reqn">\boldsymbol{\gamma}</code>, penalty parameter <code class="reqn">\lambda</code>, shrinkage target <code class="reqn">\boldsymbol{\beta}_0</code>, and generalized ridge penalty matrix <code class="reqn">\Delta_g</code>. For more details, see van Wieringen, Binder (2020) and Lettink et al. (2022).
</p>


<h3>Value</h3>

<p>A <code>numeric</code>, the generalized ridge estimate of the regression parameter. If a nonempty <code class="reqn">\mathbf{U}</code> is supplied, the first few elements are the unpenalized effect estimates of the covariates that comprise this design matrix.
</p>


<h3>Note</h3>

<p>The penalized IRLS (Iterative Reweighted Least Squares) algorithm for the evaluation of the generalized ridge logistic regression estimator may fail to converge for small penalty parameter values in combination with a nonzero shrinkage target. 
</p>


<h3>Author(s)</h3>

<p>W.N. van Wieringen.
</p>


<h3>References</h3>

<p>van Wieringen, W.N. Binder, H. (2022), &quot;Sequential learning of regression models by penalized estimation&quot;, <em>submitted</em>.
</p>
<p>Lettink, A., Chinapaw, M.J.M., van Wieringen, W.N. (2022), &quot;Two-dimensional fused targeted ridge regression for health indicator prediction from accelerometer data&quot;, <em>submitted</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># set the sample size
n &lt;- 50

# set the true parameter
betas &lt;- (c(0:100) - 50) / 20

# generate covariate data
X &lt;- matrix(rnorm(length(betas)*n), nrow=n)

# sample the response
probs &lt;- exp(tcrossprod(betas, X)[1,]) / (1 + exp(tcrossprod(betas, X)[1,]))
Y     &lt;- numeric()
for (i in 1:n){
    Y &lt;- c(Y, sample(c(0,1), 1, prob=c(1-probs[i], probs[i])))
}

# set the penalty parameter
lambda &lt;- 3

# estimate the logistic regression parameter
bHat &lt;- ridgeGLM(Y, X, lambda=lambda, target=betas/2, model="logistic")
</code></pre>

<hr>
<h2 id='ridgeGLMdof'>
Degrees of freedom of the generalized ridge estimator.
</h2><span id='topic+ridgeGLMdof'></span>

<h3>Description</h3>

<p>Function that evaluates the degrees of freedom of the generalized ridge estimator of the regression parameter of generalized linear models. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ridgeGLMdof(X, U=matrix(ncol=0, nrow=nrow(X)), lambda, 
         lambdaG, Dg=matrix(0, ncol=ncol(X), nrow=ncol(X)), 
         model="linear", linPred=rep(0,nrow(X)))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ridgeGLMdof_+3A_x">X</code></td>
<td>
<p>           The design <code>matrix</code> of the penalized covariates. The number of rows should match the number of elements of <code>Y</code>. </p>
</td></tr>
<tr><td><code id="ridgeGLMdof_+3A_u">U</code></td>
<td>
<p>           The design <code>matrix</code> of the unpenalized covariates. The number of rows should match the number of elements of <code>Y</code>. </p>
</td></tr>
<tr><td><code id="ridgeGLMdof_+3A_lambda">lambda</code></td>
<td>
<p>      A positive <code>numeric</code> that is the ridge penalty parameter. </p>
</td></tr>
<tr><td><code id="ridgeGLMdof_+3A_lambdag">lambdaG</code></td>
<td>
<p>     A positive <code>numeric</code> that is the generalized ridge penalty parameter. </p>
</td></tr>
<tr><td><code id="ridgeGLMdof_+3A_dg">Dg</code></td>
<td>
<p>          A non-negative definite <code>matrix</code> of the unscaled generalized ridge penalty. </p>
</td></tr>
<tr><td><code id="ridgeGLMdof_+3A_model">model</code></td>
<td>
<p>       A <code>character</code>, either <code>"linear"</code> and <code>"logistic"</code> (a reference to the models currently implemented), indicating for which generalized linear model model instance the degrees of freedom is to be evaluated. </p>
</td></tr>
<tr><td><code id="ridgeGLMdof_+3A_linpred">linPred</code></td>
<td>
<p> A <code>numeric</code>, the linear predictor associated with the provided <code>X</code>, <code>U</code>, <code>lambda</code>, <code>lambdaG</code>, and <code>Dg</code>. The number of elements of <code>linPred</code> should match the number of rows of <code>X</code> and <code>U</code>. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The degrees of freedom of the regular ridge regression estimator is usually defined the trace of the ridge hat matrix: <code class="reqn">\mbox{tr} [ \mathbf{X} (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I}_{pp})^{-1} \mathbf{X}^{\top}]</code>. That of the regular ridge logistic regression estimator is defined analoguously by Park, Hastie (2008). Lettink et al. (2022) translates these definitions to the generalized ridge (logistic) regression case.
</p>


<h3>Value</h3>

<p>A <code>numeric</code>, the degrees of freedom consumed by the (generalized) ridge (logistic) regression estimator.
</p>


<h3>Author(s)</h3>

<p>W.N. van Wieringen.
</p>


<h3>References</h3>

<p>Park, M. Y., &amp; Hastie, T. (2008). Penalized logistic regression for detecting gene interactions. <em>Biostatistics</em>, 9(1), 30-50.
</p>
<p>Lettink, A., Chinapaw, M.J.M., van Wieringen, W.N. (2022), &quot;Two-dimensional fused targeted ridge regression for health indicator prediction from accelerometer data&quot;, <em>submitted</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># set the sample size
n &lt;- 50

# set the true parameter
betas &lt;- (c(0:100) - 50) / 20

# generate covariate data
X &lt;- matrix(rnorm(length(betas)*n), nrow=n)

# set the penalty parameter
lambda &lt;- 3

# estimate the logistic regression parameter
dofs &lt;- ridgeGLMdof(X, lambda=lambda, lambdaG=0, 
                    model="logistic", 
                    linPred=tcrossprod(X, t(betas)))
</code></pre>

<hr>
<h2 id='ridgeGLMmultiT'>
Multi-targeted ridge estimation of generalized linear models.
</h2><span id='topic+ridgeGLMmultiT'></span>

<h3>Description</h3>

<p>Function that evaluates the multi-targeted ridge estimator of the regression parameter of generalized linear models. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ridgeGLMmultiT(Y, X, U=matrix(ncol=0, nrow=length(Y)), 
               lambdas, targetMat, model="linear", 
               minSuccDiff=10^(-10), maxIter=100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ridgeGLMmultiT_+3A_y">Y</code></td>
<td>
<p>           A <code>numeric</code> being the response vector. </p>
</td></tr>
<tr><td><code id="ridgeGLMmultiT_+3A_x">X</code></td>
<td>
<p>           The design <code>matrix</code> of the penalized covariates. The number of rows should match the number of elements of <code>Y</code>. </p>
</td></tr>
<tr><td><code id="ridgeGLMmultiT_+3A_u">U</code></td>
<td>
<p>           The design <code>matrix</code> of the unpenalized covariates. The number of rows should match the number of elements of <code>Y</code>. </p>
</td></tr>
<tr><td><code id="ridgeGLMmultiT_+3A_lambdas">lambdas</code></td>
<td>
<p>     An all-positive <code>numeric</code>, vector of penalty parameters, one per target. </p>
</td></tr>
<tr><td><code id="ridgeGLMmultiT_+3A_targetmat">targetMat</code></td>
<td>
<p>   A <code>matrix</code> with targets for the regression parameter as columns. </p>
</td></tr>
<tr><td><code id="ridgeGLMmultiT_+3A_model">model</code></td>
<td>
<p>       A <code>character</code>, either <code>"linear"</code> and <code>"logistic"</code> (a reference to the models currently implemented), indicating which generalized linear model model instance is to be fitted. </p>
</td></tr>
<tr><td><code id="ridgeGLMmultiT_+3A_minsuccdiff">minSuccDiff</code></td>
<td>
<p> A <code>numeric</code>, the minimum distance between the loglikelihoods of two successive iterations to be achieved. Used only if <code>model="logistic"</code>. </p>
</td></tr>
<tr><td><code id="ridgeGLMmultiT_+3A_maxiter">maxIter</code></td>
<td>
<p>     A <code>numeric</code> specifying the maximum number of iterations. Used only if <code>model="logistic"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function finds the maximizer of the following penalized loglikelihood: <code class="reqn"> \mathcal{L}( \mathbf{Y}, \mathbf{X}; \boldsymbol{\beta}) - \frac{1}{2} \sum_{k=1}^K \lambda_k \| \boldsymbol{\beta}  - \boldsymbol{\beta}_{k,0} \|_2^2</code>, with loglikelihood <code class="reqn">\mathcal{L}( \mathbf{Y}, \mathbf{X}; \boldsymbol{\beta})</code>, response <code class="reqn">\mathbf{Y}</code>, design matrix <code class="reqn">\mathbf{X}</code>, regression parameter <code class="reqn">\boldsymbol{\beta}</code>, penalty parameter <code class="reqn">\lambda</code>, and the <code class="reqn">k</code>-th shrinkage target <code class="reqn">\boldsymbol{\beta}_{k,0}</code>. For more details, see van Wieringen, Binder (2020).
</p>


<h3>Value</h3>

<p>The ridge estimate of the regression parameter.
</p>


<h3>Author(s)</h3>

<p>W.N. van Wieringen.
</p>


<h3>References</h3>

<p>van Wieringen, W.N. Binder, H. (2020), &quot;Online learning of regression models from a sequence of datasets by penalized estimation&quot;, <em>submitted</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># set the sample size
n &lt;- 50

# set the true parameter
betas &lt;- (c(0:100) - 50) / 20

# generate covariate data
X &lt;- matrix(rnorm(length(betas)*n), nrow=n)

# sample the response
probs &lt;- exp(tcrossprod(betas, X)[1,]) / (1 + exp(tcrossprod(betas, X)[1,]))
Y     &lt;- numeric()
for (i in 1:n){
    Y &lt;- c(Y, sample(c(0,1), 1, prob=c(1-probs[i], probs[i])))
}

# set the penalty parameter
lambdas &lt;- c(1,3)

# estimate the logistic regression parameter
# bHat &lt;- ridgeGLMmultiT(Y, X, lambdas, model="logistic",
#                       targetMat=cbind(betas/2, rnorm(length(betas))))
</code></pre>

<hr>
<h2 id='ridgePgen'>
Ridge estimation of the inverse covariance matrix with element-wise penalization and shrinkage.
</h2><span id='topic+ridgePgen'></span>

<h3>Description</h3>

<p>Function that evaluates the generalized ridge estimator of the inverse covariance matrix with element-wise penalization and shrinkage. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ridgePgen(S, lambda, target, nInit=100, minSuccDiff=10^(-10))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ridgePgen_+3A_s">S</code></td>
<td>
<p>           Sample covariance <code>matrix</code>. </p>
</td></tr>
<tr><td><code id="ridgePgen_+3A_lambda">lambda</code></td>
<td>
<p>      A symmetric <code>matrix</code> with element-wise positive penalty parameters. </p>
</td></tr>
<tr><td><code id="ridgePgen_+3A_target">target</code></td>
<td>
<p>      A semi-positive definite target <code>matrix</code> towards which the estimate is shrunken. </p>
</td></tr>
<tr><td><code id="ridgePgen_+3A_ninit">nInit</code></td>
<td>
<p>       A <code>numeric</code> specifying the number of iteration. </p>
</td></tr>
<tr><td><code id="ridgePgen_+3A_minsuccdiff">minSuccDiff</code></td>
<td>
<p> A <code>numeric</code>: minimum distance between two succesive estimates to be achieved. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function generalizes the <code><a href="rags2ridges.html#topic+ridgeP">ridgeP</a></code>-function in the sense that, besides element-wise shrinkage, it allows for element-wise penalization in the estimation of the precision matrix of a zero-mean multivariate normal distribution. Hence, it assumes that the data stem from <code class="reqn">\mathcal{N}(\mathbf{0}_p, \boldsymbol{\Omega}^{-1})</code>. The estimator maximizes the following penalized loglikelihood:
</p>
<p style="text-align: center;"><code class="reqn">
\log( | \boldsymbol{\Omega} |) - \mbox{tr} ( \boldsymbol{\Omega} \mathbf{S} ) - \| \boldsymbol{\Lambda} \circ (\boldsymbol{\Omega} - \mathbf{T}) \|_F^2,
</code>
</p>

<p>where <code class="reqn">\mathbf{S}</code> the sample covariance matrix, <code class="reqn">\boldsymbol{\Lambda}</code> a symmetric, positive matrix of penalty parameters, the <code class="reqn">\circ</code>-operator represents the Hadamard or element-wise multipication, and <code class="reqn">\mathbf{T}</code> the precision matrix' shrinkage target. For more details see van Wieringen (2019).
</p>


<h3>Value</h3>

<p>The function returns a regularized inverse covariance <code>matrix</code>.
</p>


<h3>Author(s)</h3>

<p>W.N. van Wieringen.
</p>


<h3>References</h3>

<p>van Wieringen, W.N. (2019), &quot;The generalized ridge estimator of the inverse covariance matrix&quot;, <em>Journal of Computational and Graphical Statistics</em>, 28(4), 932-942.
</p>


<h3>See Also</h3>

<p><code><a href="rags2ridges.html#topic+ridgeP">ridgeP</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># set dimension and sample size
p &lt;- 10
n &lt;- 10

# penalty parameter matrix
lambda       &lt;- matrix(1, p, p)
diag(lambda) &lt;- 0.1

# generate precision matrix
Omega       &lt;- matrix(0.4, p, p)
diag(Omega) &lt;- 1
Sigma       &lt;- solve(Omega)

# data 
Y &lt;- mvtnorm::rmvnorm(n, mean=rep(0,p), sigma=Sigma)
S &lt;- cov(Y)

# unpenalized diagonal estimate
Phat &lt;- ridgePgen(S, lambda, 0*S)
</code></pre>

<hr>
<h2 id='ridgePgen.kCV'>
K-fold cross-validated loglikelihood of ridge precision estimator.
</h2><span id='topic+ridgePgen.kCV'></span>

<h3>Description</h3>

<p>Function that calculates of the k-fold cross-validated negative (!) loglikelihood of the generalized ridge precision estimator.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ridgePgen.kCV(lambda, Y, fold=nrow(Y), target, 
              nInit=100, minSuccDiff=10^(-5)) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ridgePgen.kCV_+3A_lambda">lambda</code></td>
<td>
<p> A symmetric <code>matrix</code> with element-wise positive penalty parameters. </p>
</td></tr>
<tr><td><code id="ridgePgen.kCV_+3A_y">Y</code></td>
<td>
<p> Data <code>matrix</code> with samples as rows and variates as columns. </p>
</td></tr>
<tr><td><code id="ridgePgen.kCV_+3A_fold">fold</code></td>
<td>
<p> A <code>numeric</code> or <code>integer</code> specifying the number of folds to apply in the cross-validation. </p>
</td></tr>
<tr><td><code id="ridgePgen.kCV_+3A_target">target</code></td>
<td>
<p> A semi-positive definite target <code>matrix</code> towards which the estimate is shrunken. </p>
</td></tr>
<tr><td><code id="ridgePgen.kCV_+3A_ninit">nInit</code></td>
<td>
<p> A <code>numeric</code> specifying the number of iterations. </p>
</td></tr>
<tr><td><code id="ridgePgen.kCV_+3A_minsuccdiff">minSuccDiff</code></td>
<td>
<p> A <code>numeric</code>: minimum successive difference (in terms of their penalized loglikelihood) between two succesive estimates to be achieved. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a <code>numeric</code> containing the cross-validated negative loglikelihood.
</p>


<h3>Author(s)</h3>

<p>W.N. van Wieringen.
</p>


<h3>References</h3>

<p>van Wieringen, W.N. (2019), &quot;The generalized ridge estimator of the inverse covariance matrix&quot;, <em>Journal of Computational and Graphical Statistics</em>, 28(4), 932-942.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ridgePgen">ridgePgen</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># set dimension and sample size
p &lt;- 10
n &lt;- 10

# penalty parameter matrix
lambda       &lt;- matrix(1, p, p)
diag(lambda) &lt;- 0.1

# generate precision matrix
Omega       &lt;- matrix(0.4, p, p)
diag(Omega) &lt;- 1
Sigma       &lt;- solve(Omega)

# data 
Y &lt;- mvtnorm::rmvnorm(n, mean=rep(0,p), sigma=Sigma)
S &lt;- cov(Y)

# find optimal penalty parameters through cross-validation
lambdaOpt &lt;- optPenaltyPgen.kCVauto.banded(Y, 10^(-10), 10^(10), 
                          target=matrix(0, p, p),
                          penalize.diag=FALSE, nInit=100, 
                          minSuccDiff=10^(-5)) 

# format the penalty matrix
lambdaOptMat &lt;- matrix(NA, p, p)
for (j1 in 1:p){
    for (j2 in 1:p){
        lambdaOptMat[j1, j2] &lt;- lambdaOpt * (abs(j1-j2)+1)
    }
}

# generalized ridge precision estimate
Phat &lt;- ridgePgen(S, lambdaOptMat, matrix(0, p, p))
</code></pre>

<hr>
<h2 id='ridgePgen.kCV.banded'>
K-fold cross-validated loglikelihood of ridge precision estimator for banded precisions.
</h2><span id='topic+ridgePgen.kCV.banded'></span>

<h3>Description</h3>

<p>Function that calculates of the k-fold cross-validated negative (!) loglikelihood of the generalized ridge precision estimator, with a penalization that encourages a banded precision matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ridgePgen.kCV.banded(lambda, Y, fold=nrow(Y), target, 
                     zeros=matrix(nrow=0, ncol=2), 
                     penalize.diag=TRUE, nInit=100, 
                     minSuccDiff=10^(-5)) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ridgePgen.kCV.banded_+3A_lambda">lambda</code></td>
<td>
<p> A <code>numeric</code> with the penalty parameter value. </p>
</td></tr>
<tr><td><code id="ridgePgen.kCV.banded_+3A_y">Y</code></td>
<td>
<p> Data <code>matrix</code> with samples as rows and variates as columns. </p>
</td></tr>
<tr><td><code id="ridgePgen.kCV.banded_+3A_fold">fold</code></td>
<td>
<p> A <code>numeric</code> or <code>integer</code> specifying the number of folds to apply in the cross-validation. </p>
</td></tr>
<tr><td><code id="ridgePgen.kCV.banded_+3A_target">target</code></td>
<td>
<p> A semi-positive definite target <code>matrix</code> towards which the estimate is shrunken. </p>
</td></tr>
<tr><td><code id="ridgePgen.kCV.banded_+3A_zeros">zeros</code></td>
<td>
<p> A two-column <code>matrix</code>, with the first and second column containing the row- and column-index of zero precision elements. </p>
</td></tr>
<tr><td><code id="ridgePgen.kCV.banded_+3A_penalize.diag">penalize.diag</code></td>
<td>
<p> A <code>logical</code> indicating whether the diagonal should be penalized. </p>
</td></tr>    
<tr><td><code id="ridgePgen.kCV.banded_+3A_ninit">nInit</code></td>
<td>
<p> A <code>numeric</code> specifying the number of iterations. </p>
</td></tr>
<tr><td><code id="ridgePgen.kCV.banded_+3A_minsuccdiff">minSuccDiff</code></td>
<td>
<p> A <code>numeric</code>: minimum successive difference (in terms of their penalized loglikelihood) between two succesive estimates to be achieved. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The penalty matrix <code class="reqn">\boldsymbol{\Lambda}</code> is parametrized as follows. The elements of <code class="reqn">\boldsymbol{\Lambda}</code> are <code class="reqn">(\boldsymbol{\Lambda})_{j,j'} = \lambda (| j - j'| + 1)</code> for 
<code class="reqn">j, j' = 1, \ldots, p</code>.
</p>


<h3>Value</h3>

<p>The function returns a <code>numeric</code> containing the cross-validated negative loglikelihood.
</p>


<h3>Author(s)</h3>

<p>W.N. van Wieringen.
</p>


<h3>References</h3>

<p>van Wieringen, W.N. (2019), &quot;The generalized ridge estimator of the inverse covariance matrix&quot;, <em>Journal of Computational and Graphical Statistics</em>, 28(4), 932-942.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ridgePgen">ridgePgen</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># set dimension and sample size
p &lt;- 10
n &lt;- 10

# penalty parameter matrix
lambda       &lt;- matrix(1, p, p)
diag(lambda) &lt;- 0.1

# generate precision matrix
Omega       &lt;- matrix(0.4, p, p)
diag(Omega) &lt;- 1
Sigma       &lt;- solve(Omega)

# data 
Y &lt;- mvtnorm::rmvnorm(n, mean=rep(0,p), sigma=Sigma)
S &lt;- cov(Y)

# find optimal penalty parameters through cross-validation
lambdaOpt &lt;- optPenaltyPgen.kCVauto.banded(Y, 10^(-10), 10^(10),
                          target=matrix(0, p, p),
                          penalize.diag=FALSE, nInit=100, 
                          minSuccDiff=10^(-5)) 

# format the penalty matrix
lambdaOptMat &lt;- matrix(NA, p, p)
for (j1 in 1:p){
    for (j2 in 1:p){
        lambdaOptMat[j1, j2] &lt;- lambdaOpt * (abs(j1-j2)+1)
    }
}

# generalized ridge precision estimate
Phat &lt;- ridgePgen(S, lambdaOptMat, matrix(0, p, p))
</code></pre>

<hr>
<h2 id='ridgePgen.kCV.groups'>
K-fold cross-validated loglikelihood of ridge precision estimator with group-wise penalized variates.
</h2><span id='topic+ridgePgen.kCV.groups'></span>

<h3>Description</h3>

<p>Function that calculates of the k-fold cross-validated negative (!) loglikelihood of the generalized ridge precision estimator, assuming that variates are grouped and penalized group-wise.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ridgePgen.kCV.groups(lambdaGrps, Y, fold=nrow(Y), 
                     groups, target, 
                     zeros=matrix(nrow=0, ncol=2), 
                     penalize.diag=TRUE, nInit=100, 
                     minSuccDiff=10^(-5)) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ridgePgen.kCV.groups_+3A_lambdagrps">lambdaGrps</code></td>
<td>
<p> A <code>numeric</code> with penalty parameter values, one per group. Values should be specified in the same order as the first appearance of a group representative. </p>
</td></tr>
<tr><td><code id="ridgePgen.kCV.groups_+3A_y">Y</code></td>
<td>
<p> Data <code>matrix</code> with samples as rows and variates as columns. </p>
</td></tr>
<tr><td><code id="ridgePgen.kCV.groups_+3A_fold">fold</code></td>
<td>
<p> A <code>numeric</code> or <code>integer</code> specifying the number of folds to apply in the cross-validation. </p>
</td></tr>
<tr><td><code id="ridgePgen.kCV.groups_+3A_groups">groups</code></td>
<td>
<p> A<code>numeric</code> indicating to which group a variate belongs. Same values indicate same group. </p>
</td></tr>
<tr><td><code id="ridgePgen.kCV.groups_+3A_target">target</code></td>
<td>
<p> A semi-positive definite target <code>matrix</code> towards which the estimate is shrunken. </p>
</td></tr>
<tr><td><code id="ridgePgen.kCV.groups_+3A_zeros">zeros</code></td>
<td>
<p> A two-column <code>matrix</code>, with the first and second column containing the row- and column-index of zero precision elements. </p>
</td></tr>
<tr><td><code id="ridgePgen.kCV.groups_+3A_penalize.diag">penalize.diag</code></td>
<td>
<p> A <code>logical</code> indicating whether the diagonal should be penalized. </p>
</td></tr>    
<tr><td><code id="ridgePgen.kCV.groups_+3A_ninit">nInit</code></td>
<td>
<p> A <code>numeric</code> specifying the number of iterations. </p>
</td></tr>
<tr><td><code id="ridgePgen.kCV.groups_+3A_minsuccdiff">minSuccDiff</code></td>
<td>
<p> A <code>numeric</code>: minimum successive difference (in terms of their penalized loglikelihood) between two succesive estimates to be achieved. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The penalty matrix <code class="reqn">\boldsymbol{\Lambda}</code> is parametrized as follows. The elements of <code class="reqn">\boldsymbol{\Lambda}</code> are <code class="reqn">(\boldsymbol{\Lambda})_{j,j'} = \frac{1}{2} (\lambda_k + \lambda_{k'})</code> for 
<code class="reqn">j, j' = 1, \ldots, p</code> if <code class="reqn">j</code> and <code class="reqn">j'</code> belong to groups <code class="reqn">k</code> and <code class="reqn">k'</code>, respectively, where <code class="reqn">\lambda_k</code> and <code class="reqn">\lambda_{k'}</code> are the corresponding group-specific penalty parameters.
</p>


<h3>Value</h3>

<p>The function returns a <code>numeric</code> containing the cross-validated negative loglikelihood.
</p>


<h3>Author(s)</h3>

<p>W.N. van Wieringen.
</p>


<h3>References</h3>

<p>van Wieringen, W.N. (2019), &quot;The generalized ridge estimator of the inverse covariance matrix&quot;, <em>Journal of Computational and Graphical Statistics</em>, 28(4), 932-942.
</p>


<h3>See Also</h3>

<p><code>ridgePgen</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># set dimension and sample size
p &lt;- 10
n &lt;- 10

# penalty parameter matrix
lambda       &lt;- matrix(1, p, p)
diag(lambda) &lt;- 0.1

# generate precision matrix
Omega       &lt;- matrix(0.4, p, p)
diag(Omega) &lt;- 1
Sigma       &lt;- solve(Omega)

# data 
Y &lt;- mvtnorm::rmvnorm(n, mean=rep(0,p), sigma=Sigma)
S &lt;- cov(Y)

# find optimal penalty parameters through cross-validation
lambdaOpt &lt;- optPenaltyPgen.kCVauto.groups(Y, rep(10^(-10), 2), rep(10^(10), 2), 
                          groups=c(rep(0, p/2), rep(1, p/2)), 
                          target=matrix(0, p, p),
                          penalize.diag=FALSE, nInit=100, 
                          minSuccDiff=10^(-5)) 

# format the penalty matrix
lambdaOptVec &lt;- c(rep(lambdaOpt[1], p/2), rep(lambdaOpt[2], p/2))
lambdaOptMat &lt;- outer(lambdaOptVec, lambdaOptVec, "+")

# generalized ridge precision estimate
Phat &lt;- ridgePgen(S, lambdaOptMat, matrix(0, p, p))
</code></pre>

<hr>
<h2 id='ridgePmultiT'>
Ridge estimation of the inverse covariance matrix with multi-target shrinkage.
</h2><span id='topic+ridgePmultiT'></span>

<h3>Description</h3>

<p>Function that evaluates the ridge estimator of the inverse covariance matrix with multi-target shrinkage. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ridgePmultiT(S, lambda, targetList)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ridgePmultiT_+3A_s">S</code></td>
<td>
<p>            Sample covariance <code>matrix</code>. </p>
</td></tr>
<tr><td><code id="ridgePmultiT_+3A_lambda">lambda</code></td>
<td>
<p>     A <code>numeric</code> of positive penalty parameters. Values should be specified in the same order as the target's appearance in <code>targetList</code>. </p>
</td></tr>
<tr><td><code id="ridgePmultiT_+3A_targetlist">targetList</code></td>
<td>
<p> A list of semi-positive definite target matrices towards which the precision matrix is potentially shrunken. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function generalizes the <code><a href="rags2ridges.html#topic+ridgeP">ridgeP</a></code>-function in the sense that multiple shrinkage targets can be provided in the estimation of the precision matrix of a zero-mean multivariate normal distribution. Hence, it assumes that the data stem from <code class="reqn">\mathcal{N}(\mathbf{0}_p, \boldsymbol{\Omega}^{-1})</code>. The estimator maximizes the following penalized loglikelihood:
</p>
<p style="text-align: center;"><code class="reqn">
\log( | \boldsymbol{\Omega} |) - \mbox{tr} ( \boldsymbol{\Omega} \mathbf{S} ) - \sum\nolimits_{g=1}^G \lambda_g \| \boldsymbol{\Omega} - \mathbf{T}_g \|_F^2,
</code>
</p>

<p>where <code class="reqn">\mathbf{S}</code> the sample covariance matrix, <code class="reqn">\{ \lambda_g \}_{g=1}^G</code> the penalty parameters of each target matrix, and the <code class="reqn">\{ \mathbf{T}_g \}_{g=1}^G</code> the precision matrix' shrinkage targets. For more details see van Wieringen <em>et al.</em> (2020).
</p>


<h3>Value</h3>

<p>The function returns a regularized inverse covariance <code>matrix</code>.
</p>


<h3>Author(s)</h3>

<p>W.N. van Wieringen.
</p>


<h3>References</h3>

<p>van Wieringen, W.N., Stam, K.A., Peeters, C.F.W., van de Wiel, M.A. (2020), &quot;Updating of the Gaussian graphical model through targeted penalized estimation&quot;, <em>Journal of Multivariate Analysis</em>, 178, Article 104621.
</p>


<h3>See Also</h3>

<p><code><a href="rags2ridges.html#topic+ridgeP">ridgeP</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># set dimension and sample size
p &lt;- 10
n &lt;- 10

# specify vector of penalty parameters
lambda       &lt;- c(2, 1)

# generate precision matrix
T1       &lt;- matrix(0.7, p, p)
diag(T1) &lt;- 1
T2       &lt;- diag(rep(2, p))

# generate precision matrix
Omega       &lt;- matrix(0.4, p, p)
diag(Omega) &lt;- 2
Sigma       &lt;- solve(Omega)

# data 
Y &lt;- mvtnorm::rmvnorm(n, mean=rep(0,p), sigma=Sigma)
S &lt;- cov(Y)

# unpenalized diagonal estimate
Phat &lt;- ridgePmultiT(S, lambda, list(T1=T1, T2=T2))
</code></pre>

<hr>
<h2 id='ridgePrep'>
Ridge penalized estimation of the precision matrix from data with replicates.
</h2><span id='topic+ridgePrep'></span>

<h3>Description</h3>

<p>Estimation of the precision matrix from data with replicates through a ridge penalized EM (Expectation-Maximization) algorithm. It assumes a simple 'signal+noise' model, both random variables are assumed to be drawn from a multivariate normal distribution with their own unstructured precision matrix. These precision matrices are estimated.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ridgePrep(Y, ids, lambdaZ, lambdaE, 		
          targetZ=matrix(0, ncol(Y), ncol(Y)),
          targetE=matrix(0, ncol(Y), ncol(Y)),
          nInit=100, minSuccDiff=10^(-10))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ridgePrep_+3A_y">Y</code></td>
<td>
<p> Data <code>matrix</code> with samples (including the repetitions) as rows and variates as columns. </p>
</td></tr>
<tr><td><code id="ridgePrep_+3A_ids">ids</code></td>
<td>
<p> A <code>numeric</code> indicating which rows of <code>Y</code> belong to the same individal.</p>
</td></tr>
<tr><td><code id="ridgePrep_+3A_lambdaz">lambdaZ</code></td>
<td>
<p> A positive <code>numeric</code> representing the ridge penalty parameter for the signal precision matrix estimate. </p>
</td></tr>
<tr><td><code id="ridgePrep_+3A_lambdae">lambdaE</code></td>
<td>
<p> A positive <code>numeric</code> representing the ridge penalty parameter for the error precision matrix estimate. </p>
</td></tr>
<tr><td><code id="ridgePrep_+3A_targetz">targetZ</code></td>
<td>
<p> A semi-positive definite target <code>matrix</code> towards which the signal precision matrix estimate is shrunken. </p>
</td></tr>
<tr><td><code id="ridgePrep_+3A_targete">targetE</code></td>
<td>
<p> A semi-positive definite target <code>matrix</code> towards which the error precision matrix estimate is shrunken. </p>
</td></tr>
<tr><td><code id="ridgePrep_+3A_ninit">nInit</code></td>
<td>
<p> A <code>numeric</code> specifying the number of iterations. </p>
</td></tr>
<tr><td><code id="ridgePrep_+3A_minsuccdiff">minSuccDiff</code></td>
<td>
<p> A <code>numeric</code>: minimum successive difference (in terms of the relative change in the absolute difference of the penalized loglikelihood) between two succesive estimates to be achieved. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Data are assumed to originate from a design with replicates. Each observation <code class="reqn">\mathbf{Y}_{i,k_i}</code> with <code class="reqn">k_i</code> (<code class="reqn">k_i = 1, \ldots, K_i</code>) the <code class="reqn">k_i</code>-th replicate of the <code class="reqn">i</code>-th sample, is described by a &lsquo;signal+noise&rsquo; model: <code class="reqn">\mathbf{Y}_{i,k_i} = \mathbf{Z}_i + \boldsymbol{\varepsilon}_{i,k_i}</code>, where <code class="reqn">\mathbf{Z}_i</code> and <code class="reqn">\boldsymbol{\varepsilon}_{i,k_i}</code> represent the signal and noise, respectively. Each observation <code class="reqn">\mathbf{Y}_{i,k_i}</code> follows a multivariate normal law of the form
<code class="reqn">\mathbf{Y}_{i,k_i} \sim \mathcal{N}(\mathbf{0}_p, \boldsymbol{\Omega}_z^{-1} + \boldsymbol{\Omega}_{\varepsilon}^{-1})</code>, which results from the distributional assumptions of the signal and the noise, <code class="reqn">\mathbf{Z}_{i} \sim \mathcal{N}(\mathbf{0}_p, \boldsymbol{\Omega}_z^{-1})</code> and <code class="reqn">\boldsymbol{\varepsilon}_{i, k_i} \sim \mathcal{N}(\mathbf{0}_p, \boldsymbol{\Omega}_{\varepsilon}^{-1})</code>, and their independence. The model parameters are estimated by means of a penalized EM algorithm that maximizes the loglikelihood augmented with the penalty <code class="reqn">\lambda_z \| \boldsymbol{\Omega}_z - \mathbf{T}_z \|_F^2 + \lambda_{\varepsilon} \| \boldsymbol{\Omega}_{\varepsilon} - \mathbf{T}_{\varepsilon} \|_F^2</code>, in which <code class="reqn">\mathbf{T}_z</code> and <code class="reqn">\mathbf{T}_{\varepsilon}</code> are the shrinkage targets of the signal and noise precision matrices, respectively. For more details see van Wieringen and Chen (2019).
</p>


<h3>Value</h3>

<p>The function returns the regularized inverse covariance <code>list</code>-object with slots:
</p>
<table>
<tr><td><code>Pz</code></td>
<td>
<p> The estimated signal precision matrix. </p>
</td></tr>
<tr><td><code>Pz</code></td>
<td>
<p> The estimated error precision matrix. </p>
</td></tr>
<tr><td><code>penLL</code></td>
<td>
<p> The penalized loglikelihood of the estimated model. </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>W.N. van Wieringen.
</p>


<h3>References</h3>

<p>van Wieringen, W.N., Chen, Y. (2021), &quot;Penalized estimation of the Gaussian graphical model from data with replicates&quot;, <em>Statistics in Medicine</em>, 40(19), 4279-4293.
</p>


<h3>See Also</h3>

<p><code>optPenaltyPrep.kCVauto</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># set parameters
p        &lt;- 10
Se       &lt;- diag(runif(p))
Sz       &lt;- matrix(3, p, p)
diag(Sz) &lt;- 4

# draw data
n &lt;- 100
ids &lt;- numeric()
Y   &lt;- numeric()
for (i in 1:n){
     Ki &lt;- sample(2:5, 1)
     Zi &lt;- mvtnorm::rmvnorm(1, sigma=Sz)
     for (k in 1:Ki){
          Y   &lt;- rbind(Y, Zi + mvtnorm::rmvnorm(1, sigma=Se))
          ids &lt;- c(ids, i)
     }
}

# estimate
Ps &lt;- ridgePrep(Y, ids, 1, 1) 
</code></pre>

<hr>
<h2 id='ridgePrepEdiag'>
Ridge penalized estimation of the precision matrix from data with replicates.
</h2><span id='topic+ridgePrepEdiag'></span>

<h3>Description</h3>

<p>Estimation of precision matrices from data with replicates through a ridge penalized EM (Expectation-Maximization) algorithm. It assumes a simple 'signal+noise' model, both random variables are assumed to be drawn from a multivariate normal distribution with their own precision matrix. The signal precision matrix is unstructured, while the former is diagonal. These precision matrices are estimated.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ridgePrepEdiag(Y, ids, lambdaZ,		
               targetZ=matrix(0, ncol(Y), ncol(Y)),
               nInit=100, minSuccDiff=10^(-10))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ridgePrepEdiag_+3A_y">Y</code></td>
<td>
<p> Data <code>matrix</code> with samples (including the repetitions) as rows and variates as columns. </p>
</td></tr>
<tr><td><code id="ridgePrepEdiag_+3A_ids">ids</code></td>
<td>
<p> A <code>numeric</code> indicating which rows of <code>Y</code> belong to the same individal.</p>
</td></tr>
<tr><td><code id="ridgePrepEdiag_+3A_lambdaz">lambdaZ</code></td>
<td>
<p> A positive <code>numeric</code> representing the ridge penalty parameter for the signal precision matrix estimate. </p>
</td></tr>
<tr><td><code id="ridgePrepEdiag_+3A_targetz">targetZ</code></td>
<td>
<p> A semi-positive definite target <code>matrix</code> towards which the signal precision matrix estimate is shrunken. </p>
</td></tr>
<tr><td><code id="ridgePrepEdiag_+3A_ninit">nInit</code></td>
<td>
<p> A <code>numeric</code> specifying the number of iterations. </p>
</td></tr>
<tr><td><code id="ridgePrepEdiag_+3A_minsuccdiff">minSuccDiff</code></td>
<td>
<p> A <code>numeric</code>: minimum successive difference (in terms of the relative change in the absolute difference of the penalized loglikelihood) between two succesive estimates to be achieved. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Data are assumed to originate from a design with replicates. Each observation <code class="reqn">\mathbf{Y}_{i,k_i}</code> with <code class="reqn">k_i</code> (<code class="reqn">k_i = 1, \ldots, K_i</code>) the <code class="reqn">k_i</code>-th replicate of the <code class="reqn">i</code>-th sample, is described by a &lsquo;signal+noise&rsquo; model: <code class="reqn">\mathbf{Y}_{i,k_i} = \mathbf{Z}_i + \boldsymbol{\varepsilon}_{i,k_i}</code>, where <code class="reqn">\mathbf{Z}_i</code> and <code class="reqn">\boldsymbol{\varepsilon}_{i,k_i}</code> represent the signal and noise, respectively. Each observation <code class="reqn">\mathbf{Y}_{i,k_i}</code> follows a multivariate normal law of the form
<code class="reqn">\mathbf{Y}_{i,k_i} \sim \mathcal{N}(\mathbf{0}_p, \boldsymbol{\Omega}_z^{-1} + \boldsymbol{\Omega}_{\varepsilon}^{-1})</code>, which results from the distributional assumptions of the signal and the noise, <code class="reqn">\mathbf{Z}_{i} \sim \mathcal{N}(\mathbf{0}_p, \boldsymbol{\Omega}_z^{-1})</code> and <code class="reqn">\boldsymbol{\varepsilon}_{i, k_i} \sim \mathcal{N}(\mathbf{0}_p, \boldsymbol{\Omega}_{\varepsilon}^{-1})</code> with <code class="reqn">\boldsymbol{\Omega}_{\varepsilon}</code> diagonal, and their independence. The model parameters are estimated by means of a penalized EM algorithm that maximizes the loglikelihood augmented with the penalty <code class="reqn">\lambda_z \| \boldsymbol{\Omega}_z - \mathbf{T}_z \|_F^2</code>, in which <code class="reqn">\mathbf{T}_z</code> is the shrinkage target of the signal precision matrix. For more details see van Wieringen and Chen (2019).
</p>


<h3>Value</h3>

<p>The function returns the regularized inverse covariance <code>list</code>-object with slots:
</p>
<table>
<tr><td><code>Pz</code></td>
<td>
<p> The estimated signal precision matrix. </p>
</td></tr>
<tr><td><code>Pe</code></td>
<td>
<p> The estimated error precision matrix. </p>
</td></tr>
<tr><td><code>penLL</code></td>
<td>
<p> The penalized loglikelihood of the estimated model. </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>W.N. van Wieringen.
</p>


<h3>References</h3>

<p>van Wieringen, W.N., Chen, Y. (2021), &quot;Penalized estimation of the Gaussian graphical model from data with replicates&quot;, <em>Statistics in Medicine</em>, 40(19), 4279-4293.
</p>


<h3>See Also</h3>

<p><code>optPenaltyPrepEdiag.kCVauto</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># set parameters
p        &lt;- 10
Se       &lt;- diag(runif(p))
Sz       &lt;- matrix(3, p, p)
diag(Sz) &lt;- 4

# draw data
n &lt;- 100
ids &lt;- numeric()
Y   &lt;- numeric()
for (i in 1:n){
     Ki &lt;- sample(2:5, 1)
     Zi &lt;- mvtnorm::rmvnorm(1, sigma=Sz)
     for (k in 1:Ki){
          Y   &lt;- rbind(Y, Zi + mvtnorm::rmvnorm(1, sigma=Se))
          ids &lt;- c(ids, i)
     }
}

# estimate
Ps &lt;- ridgePrepEdiag(Y, ids, 1) 
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
