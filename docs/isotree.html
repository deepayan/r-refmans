<!DOCTYPE html><html lang="en"><head><title>Help for package isotree</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {isotree}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#isolation.forest'><p>Create Isolation Forest Model</p></a></li>
<li><a href='#isotree.add.tree'><p>Add additional (single) tree to isolation forest model</p></a></li>
<li><a href='#isotree.append.trees'><p>Append isolation trees from one model into another</p></a></li>
<li><a href='#isotree.build.indexer'><p>Build Indexer for Faster Terminal Node Predictions and/or Distance Calculations</p></a></li>
<li><a href='#isotree.deep.copy'><p>Deep-Copy an Isolation Forest Model Object</p></a></li>
<li><a href='#isotree.drop.imputer'><p>Drop Imputer Sub-Object from Isolation Forest Model Object</p></a></li>
<li><a href='#isotree.drop.indexer'><p>Drop Indexer Sub-Object from Isolation Forest Model Object</p></a></li>
<li><a href='#isotree.drop.reference.points'><p>Drop Reference Points from Isolation Forest Model Object</p></a></li>
<li><a href='#isotree.export.model'><p>Export Isolation Forest model</p></a></li>
<li><a href='#isotree.get.num.nodes'><p>Get Number of Nodes per Tree</p></a></li>
<li><a href='#isotree.import.model'><p>Load an Isolation Forest model exported from Python</p></a></li>
<li><a href='#isotree.is.same'><p>Check if two Isolation Forest Models Share the Same C++ Object</p></a></li>
<li><a href='#isotree.plot.tree'><p>Plot Tree from Isolation Forest Model</p></a></li>
<li><a href='#isotree.restore.handle'><p>Unpack isolation forest model after de-serializing</p></a></li>
<li><a href='#isotree.set.nthreads'><p>Set Number of Threads for Isolation Forest Model Object</p></a></li>
<li><a href='#isotree.set.reference.points'><p>Set Reference Points to Calculate Distances or Kernels With</p></a></li>
<li><a href='#isotree.subset.trees'><p>Subset trees of a given model</p></a></li>
<li><a href='#isotree.to.graphviz'><p>Generate GraphViz Dot Representation of Tree</p></a></li>
<li><a href='#isotree.to.json'><p>Generate JSON representations of model trees</p></a></li>
<li><a href='#isotree.to.sql'><p>Generate SQL statements from Isolation Forest model</p></a></li>
<li><a href='#length.isolation_forest'><p>Get Number of Trees in Model</p></a></li>
<li><a href='#predict.isolation_forest'><p>Predict method for Isolation Forest</p></a></li>
<li><a href='#print.isolation_forest'><p>Print summary information from Isolation Forest model</p></a></li>
<li><a href='#summary.isolation_forest'><p>Print summary information from Isolation Forest model</p></a></li>
<li><a href='#variable.names.isolation_forest'><p>Get Variable Names for Isolation Forest Model</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Isolation-Based Outlier Detection</td>
</tr>
<tr>
<td>Version:</td>
<td>0.6.1-4</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>David Cortes &lt;david.cortes.rivera@gmail.com&gt;</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/david-cortes/isotree">https://github.com/david-cortes/isotree</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/david-cortes/isotree/issues">https://github.com/david-cortes/isotree/issues</a></td>
</tr>
<tr>
<td>Description:</td>
<td>Fast and multi-threaded implementation of
	isolation forest (Liu, Ting, Zhou (2008) &lt;<a href="https://doi.org/10.1109%2FICDM.2008.17">doi:10.1109/ICDM.2008.17</a>&gt;),
	extended isolation forest (Hariri, Kind, Brunner (2018) &lt;<a href="https://doi.org/10.48550%2FarXiv.1811.02141">doi:10.48550/arXiv.1811.02141</a>&gt;),
	SCiForest (Liu, Ting, Zhou (2010) &lt;<a href="https://doi.org/10.1007%2F978-3-642-15883-4_18">doi:10.1007/978-3-642-15883-4_18</a>&gt;),
	fair-cut forest (Cortes (2021) &lt;<a href="https://doi.org/10.48550%2FarXiv.2110.13402">doi:10.48550/arXiv.2110.13402</a>&gt;),
	robust random-cut forest (Guha, Mishra, Roy, Schrijvers (2016) <a href="http://proceedings.mlr.press/v48/guha16.html">http://proceedings.mlr.press/v48/guha16.html</a>),
	and customizable variations of them, for isolation-based outlier detection, clustered outlier detection,
	distance or similarity approximation (Cortes (2019) &lt;<a href="https://doi.org/10.48550%2FarXiv.1910.12362">doi:10.48550/arXiv.1910.12362</a>&gt;),
	isolation kernel calculation (Ting, Zhu, Zhou (2018) &lt;<a href="https://doi.org/10.1145%2F3219819.3219990">doi:10.1145/3219819.3219990</a>&gt;),
	and imputation of missing values (Cortes (2019) &lt;<a href="https://doi.org/10.48550%2FarXiv.1911.06646">doi:10.48550/arXiv.1911.06646</a>&gt;),
	based on random or guided decision tree splitting, and providing different metrics for
	scoring anomalies based on isolation depth or density (Cortes (2021) &lt;<a href="https://doi.org/10.48550%2FarXiv.2111.11639">doi:10.48550/arXiv.2111.11639</a>&gt;).
	Provides simple heuristics for fitting the model to categorical columns and handling missing data,
	and offers options for varying between random and guided splits, and for using different splitting criteria.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/BSD-2-Clause">BSD_2_clause</a> + file LICENSE</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.3.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 1.0.1), jsonlite (&ge; 1.7.3), RhpcBLASctl, methods</td>
</tr>
<tr>
<td>Suggests:</td>
<td>MASS, outliertree, DiagrammeR, mlbench, MLmetrics, kernlab,
knitr, rmarkdown, kableExtra</td>
</tr>
<tr>
<td>Enhances:</td>
<td>Matrix, SparseM</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-01-08 19:31:12 UTC; david</td>
</tr>
<tr>
<td>Author:</td>
<td>David Cortes [aut, cre, cph],
  Thibaut Goetghebuer-Planchon [cph] (Copyright holder of included
    robinmap library),
  David Blackman [cph] (Copyright holder of original xoshiro code),
  Sebastiano Vigna [cph] (Copyright holder of original xoshiro code),
  NumPy Developers [cph] (Copyright holder of formatted ziggurat tables),
  SciPy Developers [cph] (Copyright holder of parts of digamma
    implementation),
  Enthought Inc [cph] (Copyright holder of parts of digamma
    implementation),
  Stephen Moshier [cph] (Copyright holder of parts of digamma
    implementation)</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-01-08 20:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='isolation.forest'>Create Isolation Forest Model</h2><span id='topic+isolation.forest'></span>

<h3>Description</h3>

<p>Isolation Forest is an algorithm originally developed for outlier detection that consists in splitting
sub-samples of the data according to some attribute/feature/column at random. The idea is that, the rarer
the observation, the more likely it is that a random uniform split on some feature would put outliers alone
in one branch, and the fewer splits it will take to isolate an outlier observation like this. The concept
is extended to splitting hyperplanes in the extended model (i.e. splitting by more than one column at a time), and to
guided (not entirely random) splits in the SCiForest and FCF models that aim at isolating outliers faster and/or
finding clustered outliers.
</p>
<p>This version adds heuristics to handle missing data and categorical variables. Can be used to aproximate pairwise
distances by checking the depth after which two observations become separated, and to approximate densities by fitting
trees beyond balanced-tree limit. Offers options to vary between randomized and deterministic splits too.
</p>
<p><b>Important:</b> The default parameters in this software do not correspond to the suggested parameters in
any of the references (see section &quot;Matching models from references&quot;).
In particular, the following default values are likely to cause huge differences when compared to the
defaults in other software: 'ndim', 'sample_size', 'ntrees'. The defaults here are
nevertheless more likely to result in better models. In order to mimic the Python library &quot;scikit-learn&quot; for example, one
would need to pass 'ndim=1', 'sample_size=256', 'ntrees=100', 'missing_action=&quot;fail&quot;', 'nthreads=1'.
</p>
<p>Note that the default parameters will not scale to large datasets. In particular,
if the amount of data is large, it's suggested to set a smaller sample size for each tree (parameter 'sample_size'),
and to fit fewer of them (parameter 'ntrees').
As well, the default option for 'missing_action' might slow things down significantly
(see below for details).
These defaults can also result in very big model sizes in memory and as serialized
files (e.g. models that weight over 10GB) when the number of rows in the data is large.
Using fewer trees, smaller sample sizes, and shallower trees can help to reduce model
sizes if that becomes a problem.
</p>
<p>The model offers many tunable parameters (see reference [11] for a comparison).
The most likely candidate to tune is
'prob_pick_pooled_gain', for which higher values tend to
result in a better ability to flag outliers in multimodal datasets, at the expense of poorer
generalizability to inputs with values outside the variables' ranges to which the model was fit
(see plots generated from the examples for a better idea of the difference). The next candidate to tune is
'sample_size' - the default is to use all rows, but in some datasets introducing sub-sampling can help,
especially for the single-variable model. In smaller datasets, one might also want to experiment
with 'weigh_by_kurtosis' and perhaps lower 'ndim'.If using 'prob_pick_pooled_gain', models
are likely to benefit from deeper trees (controlled by 'max_depth'), but using large samples
and/or deeper trees can result in significantly slower model fitting and predictions - in such cases,
using 'min_gain' (with a value like 0.25) with 'max_depth=NULL' can offer a better speed/performance
trade-off than changing 'max_depth'.
</p>
<p>If the data has categorical variables and these are more important important for determining
outlierness compared to numerical columns, one might want to experiment with 'ndim=1',
'categ_split_type=&quot;single_categ&quot;', and 'scoring_metric=&quot;density&quot;'; while for all-numeric
datasets - especially if there are missing values - one might want to experiment with 'ndim=2' or 'ndim=3'.
</p>
<p>For small datasets, one might also want to experiment with 'ndim=1', 'scoring_metric=&quot;adj_depth&quot;'
and 'penalize_range=TRUE'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isolation.forest(
  data,
  sample_size = min(nrow(data), 10000L),
  ntrees = 500,
  ndim = 1,
  ntry = 1,
  categ_cols = NULL,
  max_depth = ceiling(log2(sample_size)),
  ncols_per_tree = ncol(data),
  prob_pick_pooled_gain = 0,
  prob_pick_avg_gain = 0,
  prob_pick_full_gain = 0,
  prob_pick_dens = 0,
  prob_pick_col_by_range = 0,
  prob_pick_col_by_var = 0,
  prob_pick_col_by_kurt = 0,
  min_gain = 0,
  missing_action = ifelse(ndim &gt; 1, "impute", "divide"),
  new_categ_action = ifelse(ndim &gt; 1, "impute", "weighted"),
  categ_split_type = ifelse(ndim &gt; 1, "subset", "single_categ"),
  all_perm = FALSE,
  coef_by_prop = FALSE,
  recode_categ = FALSE,
  weights_as_sample_prob = TRUE,
  sample_with_replacement = FALSE,
  penalize_range = FALSE,
  standardize_data = TRUE,
  scoring_metric = "depth",
  fast_bratio = TRUE,
  weigh_by_kurtosis = FALSE,
  coefs = "uniform",
  assume_full_distr = TRUE,
  build_imputer = FALSE,
  output_imputations = FALSE,
  min_imp_obs = 3,
  depth_imp = "higher",
  weigh_imp_rows = "inverse",
  output_score = FALSE,
  output_dist = FALSE,
  square_dist = FALSE,
  sample_weights = NULL,
  column_weights = NULL,
  lazy_serialization = TRUE,
  seed = 1,
  use_long_double = FALSE,
  nthreads = parallel::detectCores()
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="isolation.forest_+3A_data">data</code></td>
<td>
<p>Data to which to fit the model. Supported inputs type are:</p>

<ul>
<li><p> A 'data.frame', also accepted as 'data.table' or 'tibble'.
</p>
</li>
<li><p> A 'matrix' object from base R.
</p>
</li>
<li><p> A sparse matrix in CSC format, either from package 'Matrix' (class 'dgCMatrix') or
from package 'SparseM' (class 'matrix.csc').
</p>
</li></ul>

<p>If passing a 'data.frame', will assume that columns are:
</p>

<ul>
<li><p> Numerical, if they are of types 'numeric', 'integer', 'Date', 'POSIXct'.
</p>
</li>
<li><p> Categorical, if they are of type 'character', 'factor', 'bool'. Note that,
if factors are ordered, the order will be ignored here.
</p>
</li></ul>

<p>Other input and column types are not supported.</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_sample_size">sample_size</code></td>
<td>
<p>Sample size of the data sub-samples with which each binary tree will be built.
Recommended value in references [1], [2], [3], [4] is 256, while the default value in the author's code in reference [5] is
'nrow(data)'.
</p>
<p>If passing 'NULL', will take the full number of rows in the data (no sub-sampling).
</p>
<p>If passing a number between zero and one, will assume it means taking a sample size that represents
that proportion of the rows in the data.
</p>
<p>Note that sub-sampling is incompatible with 'output_score', 'output_dist', and 'output_imputations',
and if any of those options is requested, 'sample_size' will be overriden.
</p>
<p>Hint: seeing a distribution of scores which is on average too far below 0.5 could mean that the
model needs more trees and/or bigger samples to reach convergence (unless using non-random
splits, in which case the distribution is likely to be centered around a much lower number),
or that the distributions in the data are too skewed for random uniform splits.</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_ntrees">ntrees</code></td>
<td>
<p>Number of binary trees to build for the model. Recommended value in reference [1] is 100, while the
default value in the author's code in reference [5] is 10. In general, the number of trees required for good results
is higher when (a) there are many columns, (b) there are categorical variables, (c) categorical variables have many
categories, (d) 'ndim' is high, (e) 'prob_pick_pooled_gain' is used, (f) 'scoring_metric=&quot;density&quot;'
or 'scoring_metric=&quot;boxed_density&quot;' are used.
</p>
<p>Hint: seeing a distribution of scores which is on average too far below 0.5 could mean that the
model needs more trees and/or bigger samples to reach convergence (unless using non-random
splits, in which case the distribution is likely to be centered around a much lower number),
or that the distributions in the data are too skewed for random uniform splits.</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_ndim">ndim</code></td>
<td>
<p>Number of columns to combine to produce a split. If passing 1, will produce the single-variable model described
in references [1] and [2], while if passing values greater than 1, will produce the extended model described in
references [3] and [4].
Recommended value in reference [4] is 2, while [3] recommends a low value such as 2 or 3. Models with values higher than 1
are referred hereafter as the extended model (as in reference [3]).
</p>
<p>If passing 'NULL', will assume it means using the full number of columns in the data.
</p>
<p>Note that, when using 'ndim&gt;1' plus 'standardize_data=TRUE', the variables are standardized at each step
as suggested in [4], which makes the models slightly different than in [3].
</p>
<p>In general, when the data has categorical variables, models with 'ndim=1' plus
'categ_split_type=&quot;single_categ&quot;' tend to produce better results, while models 'ndim&gt;1'
tend to produce better results for numerical-only data, especially in the presence of missing values.</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_ntry">ntry</code></td>
<td>
<p>When using any of 'prob_pick_pooled_gain', 'prob_pick_avg_gain', 'prob_pick_full_gain', 'prob_pick_dens', how many variables (with 'ndim=1')
or linear combinations (with 'ndim&gt;1') to try for determining the best one according to gain.
</p>
<p>Recommended value in reference [4] is 10 (with 'prob_pick_avg_gain', for outlier detection), while the
recommended value in reference [11] is 1 (with 'prob_pick_pooled_gain', for outlier detection), and the
recommended value in reference [9] is 10 to 20 (with 'prob_pick_pooled_gain', for missing value imputations).</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_categ_cols">categ_cols</code></td>
<td>
<p>Columns that hold categorical features,
when the data is passed as a matrix (either dense or sparse).
Can be passed as an integer vector (numeration starting at 1)
denoting the indices of the columns that are categorical, or as a character vector denoting the
names of the columns that are categorical, assuming that 'data' has column names.
</p>
<p>Categorical columns should contain only integer values with a continuous numeration starting at <b>zero</b>
(not at one as is typical in R packages), and with negative values and NA/NaN taken as missing.
The maximum categorical value should not exceed '.Machine$integer.max' (typically <code class="reqn">2^{31}-1</code>).
</p>
<p>This is ignored when the input is passed as a 'data.frame' as then it will consider columns as
categorical depending on their type/class (see the documentation for 'data' for details).</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_max_depth">max_depth</code></td>
<td>
<p>Maximum depth of the binary trees to grow. By default, will limit it to the corresponding
depth of a balanced binary tree with number of terminal nodes corresponding to the sub-sample size (the reason
being that, if trying to detect outliers, an outlier will only be so if it turns out to be isolated with shorter average
depth than usual, which corresponds to a balanced tree depth).  When a terminal node has more than 1 observation,
the remaining isolation depth for them is estimated assuming the data and splits are both uniformly random
(separation depth follows a similar process with expected value calculated as in reference [6]). Default setting
for references [1], [2], [3], [4] is the same as the default here, but it's recommended to pass higher values if
using the model for purposes other than outlier detection.
</p>
<p>If passing 'NULL' or zero, will not limit the depth of the trees (that is, will grow them until each
observation is isolated or until no further split is possible).
</p>
<p>Note that models that use 'prob_pick_pooled_gain' or 'prob_pick_avg_gain' are likely to benefit from
deeper trees (larger 'max_depth'), but deeper trees can result in much slower model fitting and
predictions.
</p>
<p>If using pooled gain, one might want to substitute 'max_depth' with 'min_gain'.</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_ncols_per_tree">ncols_per_tree</code></td>
<td>
<p>Number of columns to use (have as potential candidates for splitting at each iteration) in each tree,
somewhat similar to the 'mtry' parameter of random forests.
In general, this is only relevant when using non-random splits and/or weighted column choices.
</p>
<p>If passing a number between zero and one, will assume it means taking a sample size that represents
that proportion of the columns in the data. Note that, if passing exactly 1, will assume it means taking
100% of the columns, rather than taking a single column.
</p>
<p>If passing 'NULL', will use the full number of columns in the data.</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_prob_pick_pooled_gain">prob_pick_pooled_gain</code></td>
<td>
<p>his parameter indicates the probability of choosing the threshold on which to split a variable
(with 'ndim=1') or a linear combination of variables (when using 'ndim&gt;1') as the threshold
that maximizes a pooled standard deviation gain criterion (see references [9] and [11]) on the
same variable or linear combination, similarly to regression trees such as CART.
</p>
<p>If using 'ntry&gt;1', will try several variables or linear combinations thereof and choose the one
in which the largest standardized gain can be achieved.
</p>
<p>For categorical variables with 'ndim=1', will use shannon entropy instead (like in [7]).
</p>
<p>Compared to a simple averaged gain, this tends to result in more evenly-divided splits and more clustered
groups when they are smaller. Recommended to pass higher values when used for imputation of missing values.
When used for outlier detection, datasets with multimodal distributions usually see better performance
under this type of splits.
</p>
<p>Note that, since this makes the trees more even and thus it takes more steps to produce isolated nodes,
the resulting object will be heavier. When splits are not made according to any of 'prob_pick_avg_gain',
'prob_pick_pooled_gain', 'prob_pick_full_gain', 'prob_pick_dens', both the column and the split point are decided at random. Note that, if
passing value 1 (100%) with no sub-sampling and using the single-variable model,
every single tree will have the exact same splits.
</p>
<p>Be aware that 'penalize_range' can also have a large impact when using 'prob_pick_pooled_gain'.
</p>
<p>Under this option, models are likely to produce better results when increasing 'max_depth'.
Alternatively, one can also control the depth through 'min_gain' (for which one might want to
set 'max_depth=NULL').
</p>
<p>Important detail: if using any of 'prob_pick_avg_gain' or 'prob_pick_pooled_gain',
'prob_pick_full_gain', 'prob_pick_dens', the distribution of
outlier scores is unlikely to be centered around 0.5.</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_prob_pick_avg_gain">prob_pick_avg_gain</code></td>
<td>
<p>This parameter indicates the probability of choosing the threshold on which to split a variable
(with 'ndim=1') or a linear combination of variables (when using 'ndim&gt;1') as the threshold
that maximizes an averaged standard deviation gain criterion (see references [4] and [11]) on the
same variable or linear combination.
</p>
<p>If using 'ntry&gt;1', will try several variables or linear combinations thereof and choose the one
in which the largest standardized gain can be achieved.
</p>
<p>For categorical variables with 'ndim=1', will take the expected standard deviation that would be
gotten if the column were converted to numerical by assigning to each category a random
number '~ Unif(0, 1)' and calculate gain with those assumed standard deviations.
</p>
<p>Compared to a pooled gain, this tends to result in more cases in which a single observation or very
few of them are put into one branch. Typically, datasets with outliers defined by extreme values in
some column more or less independently of the rest, usually see better performance under this type
of split. Recommended to use sub-samples (parameter 'sample_size') when
passing this parameter. Note that, since this will create isolated nodes faster, the resulting object
will be lighter (use less memory).
</p>
<p>When splits are
not made according to any of 'prob_pick_avg_gain', 'prob_pick_pooled_gain', 'prob_pick_full_gain', 'prob_pick_dens',
both the column and the split point are decided at random. Default setting for [1], [2], [3] is
zero, and default for [4] is 1. This is the randomization parameter that can be passed to the author's original code in [5],
but note that the code in [5] suffers from a mathematical error in the calculation of running standard deviations,
so the results from it might not match with this library's.
</p>
<p>Be aware that, if passing a value of 1 (100%) with no sub-sampling and using the single-variable model, every single tree will have
the exact same splits.
</p>
<p>Under this option, models are likely to produce better results when increasing 'max_depth'.
</p>
<p>Important detail: if using either 'prob_pick_avg_gain', 'prob_pick_pooled_gain',
'prob_pick_full_gain', 'prob_pick_dens', the distribution of
outlier scores is unlikely to be centered around 0.5.</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_prob_pick_full_gain">prob_pick_full_gain</code></td>
<td>
<p>This parameter indicates the probability of choosing the threshold on which to split a variable
(with 'ndim=1') or a linear combination of variables (when using 'ndim&gt;1') as the threshold
that minimizes the pooled sums of variances of all columns (or a subset of them if using
'ncols_per_tree').
</p>
<p>In general, this is much slower to evaluate than the other gain types, and does not tend to
lead to better results. When using this option, one might want to use a different scoring
metric (particulatly '&quot;density&quot;', '&quot;boxed_density2&quot;' or '&quot;boxed_ratio&quot;'). Note that
the calculations are all done through the (exact) sorted-indices approach, while is much
slower than the (approximate) histogram approach used by other decision tree software.
</p>
<p>Be aware that the data is not standardized in any way for the variance calculations, thus the scales
of features will make a large difference under this option, which might not make it suitable for
all types of data.
</p>
<p>his option is not compatible with categorical data, and 'min_gain' does not apply to it.
</p>
<p>When splits are
not made according to any of 'prob_pick_avg_gain', 'prob_pick_pooled_gain', 'prob_pick_full_gain', 'prob_pick_dens',
both the column and the split point are decided at random. Default setting for references [1], [2], [3], [4] is
zero.</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_prob_pick_dens">prob_pick_dens</code></td>
<td>
<p>This parameter indicates the probability of choosing the threshold on which to split a variable
(with 'ndim=1') or a linear combination of variables (when using 'ndim&gt;1') as the threshold
that maximizes the pooled densities of the branch distributions.
</p>
<p>The 'min_gain' option does not apply to this type of splits.
</p>
<p>When splits are
not made according to any of 'prob_pick_avg_gain', 'prob_pick_pooled_gain', 'prob_pick_full_gain', 'prob_pick_dens',
both the column and the split point are decided at random. Default setting for [1], [2], [3], [4] is
zero.</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_prob_pick_col_by_range">prob_pick_col_by_range</code></td>
<td>
<p>When using 'ndim=1', this denotes the probability of choosing the column to split with a probability
proportional to the range spanned by each column within a node as proposed in reference [12].
</p>
<p>When using 'ndim&gt;1', this denotes the probability of choosing columns to create a hyperplane with a probability proportional to the range spanned by each column within a node.
</p>
<p>This option is not compatible with categorical data. If passing column weights, the
effect will be multiplicative.
</p>
<p>Be aware that the data is not standardized in any way for the range calculations, thus the scales
of features will make a large difference under this option, which might not make it suitable for
all types of data.
</p>
<p>If there are infinite values, all columns having infinite values will be treated as having the
same weight, and will be chosen before every other column with non-infinite values.
</p>
<p>Note that the proposed RRCF model from [12] uses a different scoring metric for producing anomaly
scores, while this library uses isolation depth regardless of how columns are chosen, thus results
are likely to be different from those of other software implementations. Nevertheless, as explored
in [11], isolation depth as a scoring metric typically provides better results than the
&quot;co-displacement&quot; metric from [12] under these split types.</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_prob_pick_col_by_var">prob_pick_col_by_var</code></td>
<td>
<p>When using 'ndim=1', this denotes the probability of choosing the column to split with a probability
proportional to the variance of each column within a node.
</p>
<p>When using 'ndim&gt;1', this denotes the probability of choosing columns to create a hyperplane with a
probability proportional to the variance of each column within a node.
</p>
<p>For categorical data, it will calculate the expected variance if the column were converted to
numerical by assigning to each category a random number '~ Unif(0, 1)', which depending on the number of
categories and their distribution, produces numbers typically a bit smaller than standardized numerical
variables.
</p>
<p>Note that when using sparse matrices, the calculation of variance will rely on a procedure that
uses sums of squares, which has less numerical precision than the
calculation used for dense inputs, and as such, the results might differ slightly.
</p>
<p>Be aware that this calculated variance is not standardized in any way, so the scales of
features will make a large difference under this option.
</p>
<p>If passing column weights, the effect will be multiplicative.
</p>
<p>If passing a 'missing_action' different than &quot;fail&quot;, infinite values will be ignored for the
variance calculation. Otherwise, all columns with infinite values will have the same probability
and will be chosen before columns with non-infinite values.</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_prob_pick_col_by_kurt">prob_pick_col_by_kurt</code></td>
<td>
<p>When using 'ndim=1', this denotes the probability of choosing the column to split with a probability
proportional to the kurtosis of each column <b>within a node</b> (unlike the option 'weigh_by_kurtosis'
which calculates this metric only at the root).
</p>
<p>When using 'ndim&gt;1', this denotes the probability of choosing columns to create a hyperplane with a
probability proportional to the kurtosis of each column within a node.
</p>
<p>For categorical data, it will calculate the expected kurtosis if the column were converted to
numerical by assigning to each category a random number '~ Unif(0, 1)'.
</p>
<p>Note that when using sparse matrices, the calculation of kurtosis will rely on a procedure that
uses sums of squares and higher-power numbers, which has less numerical precision than the
calculation used for dense inputs, and as such, the results might differ slightly.
</p>
<p>If passing column weights, the effect will be multiplicative. This option is not compatible
with 'weigh_by_kurtosis'.
</p>
<p>If passing a 'missing_action' different than &quot;fail&quot;, infinite values will be ignored for the
kurtosis calculation. Otherwise, all columns with infinite values will have the same probability
and will be chosen before columns with non-infinite values.
</p>
<p>If using 'missing_action=&quot;impute&quot;', the calculation of kurtosis will not use imputed values
in order not to favor columns with missing values (which would increase kurtosis by all having
the same central value).
</p>
<p>Be aware that kurtosis can be a rather slow metric to calculate.</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_min_gain">min_gain</code></td>
<td>
<p>Minimum gain that a split threshold needs to produce in order to proceed with a split.
Only used when the splits are decided by a variance gain criterion ('prob_pick_pooled_gain'
or 'prob_pick_avg_gain', but not 'prob_pick_full_gain' nor 'prob_pick_dens').
If the highest possible gain in the evaluated
splits at a node is below this  threshold, that node becomes a terminal node.
</p>
<p>This can be used as a more sophisticated depth control when using pooled gain (note that 'max_depth'
still applies on top of this heuristic).</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_missing_action">missing_action</code></td>
<td>
<p>How to handle missing data at both fitting and prediction time. Options are
</p>

<ul>
<li><p> '&quot;divide&quot;' (for the single-variable model only, recommended), which will follow both branches and combine
the result with the weight given by the fraction of the data that went to each branch when fitting the model.
The weights are determined during fitting by producing a split based on the non-missing values only, and calculating which
fraction of the non-missing values go to each branch (missing values are then sent to both branches with
those weights to continue the fitting procedure).
</p>
</li>
<li><p> '&quot;impute&quot;', which will assign observations to the branch with the most observations in the single-variable model,
or fill in missing values with the median of each column of the sample from which the split was made in the extended
model (recommended for it) (but note that the calculation of medians does not take
into account sample weights when using 'weights_as_sample_prob=FALSE').
When using 'ndim=1', gain calculations will use median-imputed values for missing data under this option.
</p>
</li>
<li><p> '&quot;fail&quot;', which will assume there are no missing values and will trigger undefined behavior if it encounters any.
</p>
</li></ul>

<p>In the extended model, infinite values will be treated as missing.
Passing '&quot;fail&quot;' will produce faster fitting and prediction
times along with decreased model object sizes.
</p>
<p>Models from references [1], [2], [3], [4] correspond to '&quot;fail&quot;' here.
</p>
<p>Typically, models with 'ndim&gt;1' are less affected by missing data that models with 'ndim=1'.</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_new_categ_action">new_categ_action</code></td>
<td>
<p>What to do after splitting a categorical feature when new data that reaches that split has categories that
the sub-sample from which the split was done did not have. Options are
</p>

<ul>
<li><p> '&quot;weighted&quot;' (for the single-variable model only, recommended), which will follow both branches and combine
the result with weight given by the fraction of the data that went to each branch when fitting the model.
</p>
</li>
<li><p> '&quot;impute&quot;' (for the extended model only, recommended) which will assign them the median value for that column
that was added to the linear combination of features (but note that this median calculation does not use sample weights when
using 'weights_as_sample_prob=FALSE').
</p>
</li>
<li><p> '&quot;smallest&quot;', which in the single-variable case will assign all observations with unseen categories in the split
to the branch that had fewer observations when fitting the model, and in the extended case will assign them the coefficient
of the least common category.
</p>
</li>
<li><p> '&quot;random&quot;', which will assing a branch (coefficient in the extended model) at random for
each category beforehand, even if no observations had that category when fitting the model.
Note that this can produce biased results when deciding splits by a gain criterion.
</p>
<p>Important: under this option, if the model is fitted to a 'data.frame', when calling 'predict'
on new data which contains new factor levels (unseen in the data to which the model was fitted),
they will be added to the model's state on-the-fly. This means that, if calling 'predict' on data
which has new categories, there might be inconsistencies in the results if predictions are done in
parallel or if passing the same data in batches or with different row orders.
</p>
</li></ul>

<p>Ignored when passing 'categ_split_type' = '&quot;single_categ&quot;'.</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_categ_split_type">categ_split_type</code></td>
<td>
<p>Whether to split categorical features by assigning sub-sets of them to each branch (by passing '&quot;subset&quot;' there),
or by assigning a single category to a branch and the rest to the other branch (by passing '&quot;single_categ&quot;' here). For the extended model,
whether to give each category a coefficient ('&quot;subset&quot;'), or only one while the rest get zero ('&quot;single_categ&quot;').</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_all_perm">all_perm</code></td>
<td>
<p>When doing categorical variable splits by pooled gain with 'ndim=1' (single-variable model),
whether to consider all possible permutations of variables to assign to each branch or not. If 'FALSE',
will sort the categories by their frequency and make a grouping in this sorted order. Note that the
number of combinations evaluated (if 'TRUE') is the factorial of the number of present categories in
a given column (minus 2). For averaged gain, the best split is always to put the second most-frequent
category in a separate branch, so not evaluating all  permutations (passing 'FALSE') will make it
possible to select other splits that respect the sorted frequency order.
Ignored when not using categorical variables or not doing splits by pooled gain or using 'ndim&gt;1'.</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_coef_by_prop">coef_by_prop</code></td>
<td>
<p>In the extended model, whether to sort the randomly-generated coefficients for categories
according to their relative frequency in the tree node. This might provide better results when using
categorical variables with too many categories, but is not recommended, and not reflective of
real &quot;categorical-ness&quot;. Ignored for the single-variable model ('ndim=1') and/or when not using categorical
variables.</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_recode_categ">recode_categ</code></td>
<td>
<p>Whether to re-encode categorical variables even in case they are already passed
as factors. This is recommended as it will eliminate potentially redundant categorical levels if
they have no observations, but if the categorical variables are already of type 'factor' with only
the levels that are present, it can be skipped for slightly faster fitting times. You'll likely
want to pass 'FALSE' here if merging several models into one through <a href="#topic+isotree.append.trees">isotree.append.trees</a>.</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_weights_as_sample_prob">weights_as_sample_prob</code></td>
<td>
<p>If passing sample (row) weights when fitting the model, whether to consider those weights as row
sampling weights (i.e. the higher the weights, the more likely the observation will end up included
in each tree sub-sample), or as distribution density weights (i.e. putting a weight of two is the same
as if the row appeared twice, thus higher weight makes it less of an outlier, but does not give it a
higher chance of being sampled if the data uses sub-sampling).</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_sample_with_replacement">sample_with_replacement</code></td>
<td>
<p>Whether to sample rows with replacement or not (not recommended).
Note that distance calculations, if desired, don't work when there are duplicate rows.
</p>
<p>This option is not compatible with 'output_score', 'output_dist', 'output_imputations'.</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_penalize_range">penalize_range</code></td>
<td>
<p>Whether to penalize (add -1 to the terminal depth) observations at prediction time that have a value
of the chosen split variable (linear combination in extended model) that falls outside of a pre-determined
reasonable range in the data being split (given by '2 * range' in data and centered around the split point),
as proposed in reference [4] and implemented in the authors' original code in reference [5]. Not used in single-variable model
when splitting by categorical variables.
</p>
<p>This option is not supported when using density-based outlier scoring metrics.
</p>
<p>It's recommended to turn this off for faster predictions on sparse CSC matrices.
</p>
<p>Note that this can make a very large difference in the results when using 'prob_pick_pooled_gain'.
</p>
<p>Be aware that this option can make the distribution of outlier scores a bit different
(i.e. not centered around 0.5).</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_standardize_data">standardize_data</code></td>
<td>
<p>Whether to standardize the features at each node before creating alinear combination of them as suggested
in [4]. This is ignored when using 'ndim=1'.</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_scoring_metric">scoring_metric</code></td>
<td>
<p>Metric to use for determining outlier scores (see reference [13]). Options are:</p>

<ul>
<li><p> &quot;depth&quot;: Will use isolation depth as proposed in reference [1]. This is typically the safest choice
and plays well with all model types offered by this library.
</p>
</li>
<li><p> &quot;density&quot;: Will set scores for each terminal node as the ratio between the fraction of points in the sub-sample
that end up in that node and the fraction of the volume in the feature space which defines
the node according to the splits that lead to it.
If using 'ndim=1', for categorical variables, this is defined in terms
of number of categories that go towards each side of the split divided by number of categories
in the observations that reached that node.
</p>
<p>The standardized outlier score from density for a given observation is calculated as the
negative of the logarithm of the geometric mean from the per-tree densities, which unlike
the standardized score produced from depth, is unbounded, but just like the standardized
score from depth, has a natural threshold for definining outlierness, which in this case
is zero is instead of 0.5. The non-standardized outlier score is calculated as the
geometric mean, while the per-tree scores are calculated as the density values.
</p>
<p>This might lead to better predictions when using 'ndim=1', particularly in the presence
of categorical variables. Note however that using density requires more trees for convergence
of scores (i.e. good results) compared to isolation-based metrics.
</p>
<p>This option is incompatible with 'penalize_range'.
</p>
</li>
<li><p> &quot;adj_depth&quot;: Will use an adjusted isolation depth that takes into account the number of points that
go to each side of a given split vs. the fraction of the range of that feature that each
side of the split occupies, by a metric as follows:
</p>
<p><code class="reqn">d = \frac{2}{(1 + \frac{1}{2 p}}</code>
</p>
<p>Where <code class="reqn">p</code> is defined as:
</p>
<p><code class="reqn">p = \frac{n_s}{n_t} / \frac{r_s}{r_t}</code>
</p>
<p>With <code class="reqn">n_t</code> being the number of points that reach a given node, <code class="reqn">n_s</code> the
number of points that are sent to a given side of the split/branch at that node,
<code class="reqn">r_t</code> being the range (maximum minus minimum) of the splitting feature or
linear combination among the points that reached the node, and <code class="reqn">r_s</code> being the
range of the same feature or linear combination among the points that are sent to this
same side of the split/branch. This makes each split add a number between zero and two
to the isolation depth, with this number's probabilistic distribution being centered
around 1 and thus the expected isolation depth remaing the same as in the original
'&quot;depth&quot;' metric, but having more variability around the extremes.
</p>
<p>Scores (standardized, non-standardized, per-tree) are aggregated in the same way
as for '&quot;depth&quot;'.
</p>
<p>This might lead to better predictions when using 'ndim=1', particularly in the prescence
of categorical variables and for smaller datasets, and for smaller datasets, might make
sense to combine it with 'penalize_range=TRUE'.
</p>
</li>
<li><p> &quot;adj_density&quot;: Will use the same metric from '&quot;adj_depth&quot;', but applied multiplicatively instead
of additively. The expected value for this adjusted density is not strictly the same
as for isolation, but using the expected isolation depth as standardizing criterion
tends to produce similar standardized score distributions (centered around 0.5).
</p>
<p>Scores (standardized, non-standardized, per-tree) are aggregated in the same way
as for '&quot;depth&quot;'.
</p>
<p>This option is incompatible with 'penalize_range'.
</p>
</li>
<li><p> &quot;boxed_ratio&quot;: Will set the scores for each terminal node as the ratio between the volume of the boxed
feature space for the node as defined by the smallest and largest values from the split
conditions for each column (bounded by the variable ranges in the sample) and the
variable ranges in the tree sample.
If using 'ndim=1', for categorical variables this is defined in terms of number of
categories.
If using 'ndim=&gt;1', this is defined in terms of the maximum achievable value for the
splitting linear combination determined from the minimum and maximum values for each
variable among the points in the sample, and as such, it has a rather different meaning
compared to the score obtained with 'ndim=1' - boxed ratio scores with 'ndim&gt;1'
typically provide very poor quality results and this metric is thus not recommended to
use in the extended model. With 'ndim&gt;1', it also has a tendency of producing too small
values which round to zero.
</p>
<p>The standardized outlier score from boxed ratio for a given observation is calculated
simply as the the average from the per-tree boxed ratios. This metric
has a lower bound of zero and a theorical upper bound of one, but in practice the scores
tend to be very small numbers close to zero, and its distribution across
different datasets is rather unpredictable. In order to keep rankings comparable with
the rest of the metrics, the non-standardized outlier scores are calculated as the
negative of the average instead. The per-tree scores are calculated as the ratios.
</p>
<p>This metric can be calculated in a fast-but-not-so-precise way, and in a low-but-precise
way, which is controlled by parameter 'fast_bratio'. Usually, both should give the
same results, but in some fatasets, the fast way can lead to numerical inaccuracies
due to roundoffs very close to zero.
</p>
<p>This metric might lead to better predictions in datasets with many rows when using 'ndim=1'
and a relatively small 'sample_size'. Note that more trees are required for convergence
of scores when using this metric. In some datasets, this metric might result in very bad
predictions, to the point that taking its inverse produces a much better ranking of outliers.
</p>
<p>This option is incompatible with 'penalize_range'.
</p>
</li>
<li><p> &quot;boxed_density2&quot;: Will set the score as the ratio between the fraction of points within the sample that
end up in a given terminal node and the boxed ratio metric.
</p>
<p>Aggregation of scores (standardized, non-standardized, per-tree) is done in the same
way as for density, and it also has a natural threshold at zero for determining
outliers and inliers.
</p>
<p>This metric is typically usable with 'ndim&gt;1', but tends to produce much bigger values
compared to 'ndim=1'.
</p>
<p>Albeit unintuitively, in many datasets, one can usually get better results with metric
'&quot;boxed_density&quot;' instead.
</p>
<p>The calculation of this metric is also controlled by 'fast_bratio'.
</p>
<p>This option is incompatible with 'penalize_range'.
</p>
</li>
<li><p> &quot;boxed_density&quot;: Will set the score as the ratio between the fraction of points within the sample that
end up in a  given terminal node and the ratio between the boxed volume of the feature
space in the sample and the boxed volume of a node given by the split conditions (inverse
as in '&quot;boxed_density2&quot;'). This metric does not have any theoretical or intuitive
justification behind its existence, and it is perhaps ilogical to use it as a
scoring metric, but tends to produce good results in some datasets.
</p>
<p>The standardized outlier scores are defined as the negative of the geometric mean
of this metric, while the non-standardized scores are the geometric mean, and the
per-tree scores are simply the 'density' values.
</p>
<p>The calculation of this metric is also controlled by 'fast_bratio'.
</p>
<p>This option is incompatible with 'penalize_range'.
</p>
</li></ul>
</td></tr>
<tr><td><code id="isolation.forest_+3A_fast_bratio">fast_bratio</code></td>
<td>
<p>When using &quot;boxed&quot; metrics for scoring, whether to calculate them in a fast way through
cumulative sum of logarithms of ratios after each split, or in a slower way as sum of
logarithms of a single ratio per column for each terminal node.
</p>
<p>Usually, both methods should give the same results, but in some datasets, particularly
when variables have too small or too large ranges, the first method can be prone to
numerical inaccuracies due to roundoff close to zero.
</p>
<p>Note that this does not affect calculations for models with 'ndim&gt;1', since given the
split types, the calculation for them is different.</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_weigh_by_kurtosis">weigh_by_kurtosis</code></td>
<td>
<p>Whether to weigh each column according to the kurtosis obtained in the sub-sample that is selected
for each tree as briefly proposed in reference [1]. Note that this is only done at the beginning of each tree
sample. For categorical columns, will calculate expected kurtosis if the column were converted to numerical
by assigning to each category a random number '~ Unif(0, 1)'.
</p>
<p>Note that when using sparse matrices, the calculation of kurtosis will rely on a procedure that
uses sums of squares and higher-power numbers, which has less numerical precision than the
calculation used for dense inputs, and as such, the results might differ slightly.
</p>
<p>Using this option makes the model more likely to pick the columns that have anomalous values
when viewed as a 1-d distribution, and can bring a large improvement in some datasets.
</p>
<p>This is intended as a cheap feature selector, while the parameter 'prob_pick_col_by_kurt'
provides the option to do this at each node in the tree for a different overall type of model.
</p>
<p>If passing column weights or using weighted column choices proportional to some other metric
('prob_pick_col_by_range', 'prob_pick_col_by_var'), the effect will be multiplicative.
</p>
<p>If passing 'missing_action=&quot;fail&quot;' and the data has infinite values, columns with rows
having infinite values will get a weight of zero. If passing a different value for missing
action, infinite values will be ignored in the kurtosis calculation.
</p>
<p>If using 'missing_action=&quot;impute&quot;', the calculation of kurtosis will not use imputed values
in order not to favor columns with missing values (which would increase kurtosis by all having
the same central value).</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_coefs">coefs</code></td>
<td>
<p>For the extended model, whether to sample random coefficients according to a normal distribution '~ N(0, 1)'
(as proposed in reference [4]) or according to a uniform distribution '~ Unif(-1, +1)' as proposed in reference [3].
Ignored for the single-variable model. Note that, for categorical variables, the coefficients will be sampled ~ N (0,1)
regardless - in order for both types of variables to have transformations in similar ranges (which will tend
to boost the importance of categorical variables), pass '&quot;uniform&quot;' here.</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_assume_full_distr">assume_full_distr</code></td>
<td>
<p>When calculating pairwise distances (see reference [8]), whether to assume that the fitted model represents
a full population distribution (will use a standardizing criterion assuming infinite sample as in reference [6],
and the results of the similarity between two points at prediction time will not depend on the
prescence of any third point that is similar to them, but will differ more compared to the pairwise
distances between points from which the model was fit). If passing 'FALSE', will calculate pairwise distances
as if the new observations at prediction time were added to the sample to which each tree was fit, which
will make the distances between two points potentially vary according to other newly introduced points.
This will not be assumed when the distances are calculated as the model is being fit (see documentation
for parameter 'output_dist').
</p>
<p>This was added for experimentation purposes only and it's not recommended to pass 'FALSE'.
Note that when calculating distances using a tree indexer (after calling <a href="#topic+isotree.build.indexer">isotree.build.indexer</a>), there
might be slight discrepancies between the numbers produced with or without the indexer due to what
are considered &quot;additional&quot; observations in this calculation.</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_build_imputer">build_imputer</code></td>
<td>
<p>Whether to construct missing-value imputers so that later this same model could be used to impute
missing values of new (or the same) observations. Be aware that this will significantly increase the memory
requirements and serialized object sizes. Note that this is not related to 'missing_action' as missing values
inside the model are treated differently and follow their own imputation or division strategy.</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_output_imputations">output_imputations</code></td>
<td>
<p>Whether to output imputed missing values for 'data'. Passing 'TRUE' here will force
'build_imputer' to 'TRUE'. Note that, for sparse matrix inputs, even though the output will be sparse, it will
generate a dense representation of each row with missing values.
</p>
<p>This is not supported when using sub-sampling, and if sub-sampling is specified, will override it
using the full number of rows.</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_min_imp_obs">min_imp_obs</code></td>
<td>
<p>Minimum number of observations with which an imputation value can be produced. Ignored if passing
'build_imputer' = 'FALSE'.</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_depth_imp">depth_imp</code></td>
<td>
<p>How to weight observations according to their depth when used for imputing missing values. Passing
'&quot;higher&quot;' will weigh observations higher the further down the tree (away from the root node) the
terminal node is, while '&quot;lower&quot;' will do the opposite, and '&quot;same&quot;' will not modify the weights according
to node depth in the tree. Implemented for testing purposes and not recommended to change
from the default. Ignored when passing 'build_imputer' = 'FALSE'.</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_weigh_imp_rows">weigh_imp_rows</code></td>
<td>
<p>How to weight node sizes when used for imputing missing values. Passing '&quot;inverse&quot;' will weigh
a node inversely proportional to the number of observations that end up there, while '&quot;prop&quot;'
will weight them heavier the more observations there are, and '&quot;flat&quot;' will weigh all nodes the same
in this regard regardless of how many observations end up there. Implemented for testing purposes
and not recommended to change from the default. Ignored when passing 'build_imputer' = 'FALSE'.</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_output_score">output_score</code></td>
<td>
<p>Whether to output outlierness scores for the input data, which will be calculated as
the model is being fit and it's thus faster. Cannot be done when using sub-samples of the data for each tree
(in such case will later need to call the 'predict' function on the same data). If using 'penalize_range', the
results from this might differet a bit from those of 'predict' called after.
</p>
<p>This is not supported when using sub-sampling, and if sub-sampling is specified, will override it
using the full number of rows.</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_output_dist">output_dist</code></td>
<td>
<p>Whether to output pairwise distances for the input data, which will be calculated as
the model is being fit and it's thus faster. Cannot be done when using sub-samples of the data for each tree
(in such case will later need to call the 'predict' function on the same data). If using 'penalize_range', the
results from this might differ a bit from those of 'predict' called after.
</p>
<p>This is not supported when using sub-sampling, and if sub-sampling is specified, will override it
using the full number of rows.
</p>
<p>Note that it might be much faster to calculate distances through a fitted model object with
<a href="#topic+isotree.build.indexer">isotree.build.indexer</a> instead or calculating them while fitting like this.</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_square_dist">square_dist</code></td>
<td>
<p>If passing 'output_dist' = 'TRUE', whether to return a full square matrix or
just the upper-triangular part, in which the entry for pair (i,j) with 1 &lt;= i &lt; j &lt;= n is located at position
p(i, j) = ((i - 1) * (n - i/2) + j - i).</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_sample_weights">sample_weights</code></td>
<td>
<p>Sample observation weights for each row of 'data', with higher weights indicating either higher sampling
probability (i.e. the observation has a larger effect on the fitted model, if using sub-samples), or
distribution density (i.e. if the weight is two, it has the same effect of including the same data
point twice), according to parameter 'weights_as_sample_prob'. Not supported when calculating pairwise
distances while the model is being fit (done by passing 'output_dist' = 'TRUE').
</p>
<p>If 'data' is a 'data.frame' and the variable passed here matches to the name of a column in 'data'
(with or without enclosing 'sample_weights' in quotes), it will assume the weights are to be
taken as that column name.</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_column_weights">column_weights</code></td>
<td>
<p>Sampling weights for each column in 'data'. Ignored when picking columns by deterministic criterion.
If passing 'NULL', each column will have a uniform weight. If used along with kurtosis weights, the
effect is multiplicative.
</p>
<p>Note that, if passing a data.frame with both numeric and categorical columns, the column names must
not be repeated, otherwise the column weights passed here will not end up matching. If passing a 'data.frame'
to 'data', will assume the column order is the same as in there, regardless of whether the entries passed to
'column_weights' are named or not.</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_lazy_serialization">lazy_serialization</code></td>
<td>
<p>Whether to use a lazy serialization mechanism for the model C++ objects through
the ALTREP system, which would only call the serialization and de-serialization methods when needed.
</p>
<p>Passing 'TRUE' here has the following effects:</p>

<ul>
<li><p> Fitting the model will not immediately trigger serialization of the model object, not will it
need to allocate extra memory for the serialized bytes - instead, these serialized bytes will only
get materialized when calling serialization functions such as 'save' or 'saveRDS'.
</p>
</li>
<li><p> The resulting object will not be possible to serialize with library 'qs' ('qs::qsave'), nor
with other serialization libraries that do not work with R's ALTREP system.
</p>
</li>
<li><p> If restoring a session with saved objects or loading serialized models through 'load', if there
is not enough memory to de-serialize the model object, this will be manifested as silent failures
in which the object simply disappears from the environment without leaving any trace or error message.
</p>
</li>
<li><p> When reading a model through &lsquo;readRDS', if there&rsquo;s an error (such as 'insufficient memory'),
it will fail to load the R object at all.
</p>
</li>
<li><p> Since this uses a workaround for which the ALTREP system was not initially designed, calling
methods such as 'str' on the resulting object will result in errors being displayed when it comes
to the external pointer fields.
</p>
</li></ul>

<p>If passing 'FALSE', on the other hand:</p>

<ul>
<li><p> Immediately after fitting the model, this fitted model will be serialized into memory-contiguous
raw bytes from which the C++ object can then be reconstructed.
</p>
</li>
<li><p> If the model gets de-serialized from a saved file (for example through 'load', 'readRDS', 'qs::qread',
or by restarting R sessions), the underlying C++ model object will be lost, and as such will need to be
restored (de-serialized from the serialized bytes) the first time a method like 'predict' gets called on
it (which means the first call will be slower and will result in additional memory allocations).
</p>
</li></ul>
</td></tr>
<tr><td><code id="isolation.forest_+3A_seed">seed</code></td>
<td>
<p>Seed that will be used for random number generation.</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_use_long_double">use_long_double</code></td>
<td>
<p>Whether to use 'long double' (extended precision) type for more precise calculations about
standard deviations, means, ratios, weights, gain, and other potential aggregates. This makes
such calculations accurate to a larger number of decimals (provided that the compiler used has
wider long doubles than doubles) and it is highly recommended to use when the input data has
a number of rows or columns exceeding <code class="reqn">2^{53}</code> (an unlikely scenario), and also highly recommended
to use when the input data has problematic scales (e.g. numbers that differ from each other by
something like <code class="reqn">10^{-100}</code> or columns that include values like <code class="reqn">10^{100}</code>, <code class="reqn">10^{-10}</code>, and <code class="reqn">10^{-100}</code> and still need to
be sensitive to a difference of <code class="reqn">10^{-10}</code>), but will make the calculations slower, the more so in
platforms in which 'long double' is a software-emulated type (e.g. Power8 platforms).
Note that some platforms (most notably windows with the msvc compiler) do not make any difference
between 'double' and 'long double'.
</p>
<p>If 'long double' is not going to be used, the library can be compiled without support for it
(making the library size smaller) by defining an environment variable 'NO_LONG_DOUBLE' before
installing this package (e.g. through 'Sys.setenv(&quot;NO_LONG_DOUBLE&quot; = &quot;1&quot;)' before running the
&lsquo;install.packages' command). If R itself was compiled without &rsquo;long double' support, this library
will follow suit and disable long double too.
</p>
<p>This option is not available on Windows, due to lack of support in some compilers (e.g. msvc)
and lack of thread-safety in the calculations in others (e.g. mingw).</p>
</td></tr>
<tr><td><code id="isolation.forest_+3A_nthreads">nthreads</code></td>
<td>
<p>Number of parallel threads to use. Note that, the more threads,
the more memory will be allocated, even if the thread does not end up being used.
Be aware that most of the operations are bound by memory bandwidth, which means that
adding more threads will not result in a linear speed-up. For some types of data
(e.g. large sparse matrices with small sample sizes), adding more threads might result
in only a very modest speed up (e.g. 1.5x faster with 4x more threads),
even if all threads look fully utilized.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If requesting outlier scores or depths or separation/distance while fitting the
model and using multiple threads, there can be small differences in the predicted
scores/depth/separation/distance between runs due to roundoff error.
</p>


<h3>Value</h3>

<p>If passing 'output_score' = 'FALSE', 'output_dist' = 'FALSE', and 'output_imputations' = 'FALSE' (the defaults),
will output an 'isolation_forest' object from which 'predict' method can then be called on new data.
</p>
<p>If passing 'TRUE' to any of the former options, will output a list with entries:
</p>

<ul>
<li><p> 'model': the 'isolation_forest' object from which new predictions can be made.
</p>
</li>
<li><p> 'scores': a vector with the outlier score for each inpuit observation (if passing 'output_score' = 'TRUE').
</p>
</li>
<li><p> 'dist': the distances (either a 'dist' object or a square matrix), if
passing 'output_dist' = 'TRUE'.
</p>
</li>
<li><p> 'imputed': the input data with missing values imputed according to the model (if passing 'output_imputations' = 'TRUE').
</p>
</li></ul>



<h3>Matching models from references</h3>

<p>Shorthands for parameter combinations that match some of the references:</p>

<ul>
<li><p> 'iForest' (reference [1]): 'ndim=1', 'sample_size=256', 'max_depth=8', 'ntrees=100', 'missing_action=&quot;fail&quot;'.
</p>
</li>
<li><p> 'EIF' (reference [3]): 'ndim=2', 'sample_size=256', 'max_depth=8', 'ntrees=100', 'missing_action=&quot;fail&quot;',
'coefs=&quot;uniform&quot;', 'standardize_data=False' (plus standardizing the data <b>before</b> passing it).
</p>
</li>
<li><p> 'SCiForest' (reference [4]): 'ndim=2', 'sample_size=256', 'max_depth=8', 'ntrees=100', 'missing_action=&quot;fail&quot;',
'coefs=&quot;normal&quot;', 'ntry=10', 'prob_pick_avg_gain=1', 'penalize_range=True'.
Might provide much better results with &lsquo;max_depth=NULL' despite the reference&rsquo;s recommendation.
</p>
</li>
<li><p> 'FCF' (reference [11]): 'ndim=2', 'sample_size=256', 'max_depth=NULL', 'ntrees=200',
'missing_action=&quot;fail&quot;', 'coefs=&quot;normal&quot;', 'ntry=1', 'prob_pick_pooled_gain=1'.
Might provide similar or better results with 'ndim=1'  and/or sample size as low as 32.
For the FCF model aimed at imputing missing values,
might give better results with 'ntry=10' or higher and much larger sample sizes.
</p>
</li>
<li><p> 'RRCF' (reference [12]): 'ndim=1', 'prob_pick_col_by_range=1', 'sample_size=256' or more, 'max_depth=NULL',
'ntrees=100' or more, 'missing_action=&quot;fail&quot;'. Note however that reference [12] proposed a
different method for calculation of anomaly scores, while this library uses isolation depth just
like for 'iForest', so results might differ significantly from those of other libraries.
Nevertheless, experiments in reference [11] suggest that isolation depth might be a better
scoring metric for this model.
</p>
</li></ul>



<h3>Model serving considerations</h3>

<p>If the model is built with 'nthreads&gt;1', the prediction function <a href="#topic+predict.isolation_forest">predict.isolation_forest</a> will
use OpenMP for parallelization. In a linux setup, one usually has GNU's &quot;gomp&quot; as OpenMP as backend, which
will hang when used in a forked process - for example, if one tries to call this prediction function from
'RestRserve', which uses process forking for parallelization, it will cause the whole application to freeze.
A potential fix in these cases is to pass 'nthreads=1' to 'predict', or to set the number of threads to 1 in
the model object (e.g. 'model$nthreads &lt;- 1L' or calling <a href="#topic+isotree.set.nthreads">isotree.set.nthreads</a>), or to compile this library
without OpenMP (requires manually altering the 'Makevars' file), or to use a non-GNU OpenMP backend (such
as LLVM's 'libomp'. This should not be an issue when using this library normally in e.g. an RStudio session.
</p>
<p>The R objects that hold the models contain heap-allocated C++ objects which do not map to R types and
which thus do not survive serializations the same way R objects do.
In order to make model objects serializable (i.e. usable with 'save', 'saveRDS', and similar), the package
offers two mechanisms: (a) a 'lazy_serialization' option which uses the ALTREP system as a workaround,
by defining classes with serialization methods but without datapointer methods (see the docs for
'lazy_serialization' for more info); (b) a more theoretically correct way in which raw bytes are produced
alongside the model and from which the C++ objects can be reconstructed. When using the lazy serialization
system, C++ objects are restored automatically on load and the serialized bytes then discarded, but this is
not the case when using the serialized bytes approach. For model serving, one would usually want to drop these
serialized bytes after having loaded a model through 'readRDS' or similar (note that reconstructing the
C++ object will first require calling <a href="#topic+isotree.restore.handle">isotree.restore.handle</a>, which is done automatically when
calling 'predict' and similar), as they can increase memory usage by a large amount. These redundant raw bytes
can be dropped as follows: 'model$cpp_objects$model$ser &lt;- NULL' (and an additional
'model$cpp_objects$imputer$ser &lt;- NULL' when using 'build_imputer=TRUE', and 'model$cpp_objects$indexer$ser &lt;- NULL'
when building a node indexer). After that, one might want to force garbage
collection through 'gc()'.
</p>
<p>Usually, for serving purposes, one wants a setup as minimalistic as possible (e.g. smaller docker images).
This library can be made smaller and faster to compile by disabling some features - particularly,
the library will by default build with support for calculation of aggregated metrics (such as
standard deviations) in 'long double' precision (an extended precision type), which is a functionality
that's unlikely to get used (default is not to use this type as it is slower, and calculations done in
the &lsquo;predict' function do not use it for anything). Support for &rsquo;long double' can be disable at compile
time by setting up an environment variable 'NO_LONG_DOUBLE' before installing the
package (e.g. by issuing command 'Sys.setenv(&quot;NO_LONG_DOUBLE&quot; = &quot;1&quot;)' before 'install.packages').
</p>


<h3>References</h3>


<ol>
<li><p> Liu, Fei Tony, Kai Ming Ting, and Zhi-Hua Zhou. &quot;Isolation forest.&quot; 2008 Eighth IEEE International Conference on Data Mining. IEEE, 2008.
</p>
</li>
<li><p> Liu, Fei Tony, Kai Ming Ting, and Zhi-Hua Zhou. &quot;Isolation-based anomaly detection.&quot; ACM Transactions on Knowledge Discovery from Data (TKDD) 6.1 (2012): 3.
</p>
</li>
<li><p> Hariri, Sahand, Matias Carrasco Kind, and Robert J. Brunner. &quot;Extended Isolation Forest.&quot; arXiv preprint arXiv:1811.02141 (2018).
</p>
</li>
<li><p> Liu, Fei Tony, Kai Ming Ting, and Zhi-Hua Zhou. &quot;On detecting clustered anomalies using SCiForest.&quot; Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, Berlin, Heidelberg, 2010.
</p>
</li>
<li> <p><a href="https://sourceforge.net/projects/iforest/">https://sourceforge.net/projects/iforest/</a>
</p>
</li>
<li> <p><a href="https://math.stackexchange.com/questions/3388518/expected-number-of-paths-required-to-separate-elements-in-a-binary-tree">https://math.stackexchange.com/questions/3388518/expected-number-of-paths-required-to-separate-elements-in-a-binary-tree</a>
</p>
</li>
<li><p> Quinlan, J. Ross. &quot;C4. 5: programs for machine learning.&quot; Elsevier, 2014.
</p>
</li>
<li><p> Cortes, David. &quot;Distance approximation using Isolation Forests.&quot; arXiv preprint arXiv:1910.12362 (2019).
</p>
</li>
<li><p> Cortes, David. &quot;Imputing missing values with unsupervised random trees.&quot; arXiv preprint arXiv:1911.06646 (2019).
</p>
</li>
<li> <p><a href="https://math.stackexchange.com/questions/3333220/expected-average-depth-in-random-binary-tree-constructed-top-to-bottom">https://math.stackexchange.com/questions/3333220/expected-average-depth-in-random-binary-tree-constructed-top-to-bottom</a>
</p>
</li>
<li><p> Cortes, David. &quot;Revisiting randomized choices in isolation forests.&quot; arXiv preprint arXiv:2110.13402 (2021).
</p>
</li>
<li><p> Guha, Sudipto, et al. &quot;Robust random cut forest based anomaly detection on streams.&quot; International conference on machine learning. PMLR, 2016.
</p>
</li>
<li><p> Cortes, David. &quot;Isolation forests: looking beyond tree depth.&quot; arXiv preprint arXiv:2111.11639 (2021).
</p>
</li>
<li><p> Ting, Kai Ming, Yue Zhu, and Zhi-Hua Zhou. &quot;Isolation kernel and its effect on SVM.&quot; Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. 2018.
</p>
</li></ol>



<h3>See Also</h3>

<p><a href="#topic+predict.isolation_forest">predict.isolation_forest</a>,  <a href="#topic+isotree.add.tree">isotree.add.tree</a> <a href="#topic+isotree.restore.handle">isotree.restore.handle</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Example 1: detect an obvious outlier
### (Random data from a standard normal distribution)
library(isotree)
set.seed(1)
m &lt;- 100
n &lt;- 2
X &lt;- matrix(rnorm(m * n), nrow = m)

### Will now add obvious outlier point (3, 3) to the data
X &lt;- rbind(X, c(3, 3))

### Fit a small isolation forest model
iso &lt;- isolation.forest(X, ntrees = 10, nthreads = 1)

### Check which row has the highest outlier score
pred &lt;- predict(iso, X)
cat("Point with highest outlier score: ",
    X[which.max(pred), ], "\n")


### Example 2: plotting outlier regions
### This example shows predicted outlier score in a small
### grid, with a model fit to a bi-modal distribution. As can
### be seen, the extended model is able to detect high
### outlierness outside of both regions, without having false
### ghost regions of low-outlierness where there isn't any data
library(isotree)
oldpar &lt;- par(mfrow = c(2, 2), mar = c(2.5,2.2,2,2.5))

### Randomly-generated data from different distributions
set.seed(1)
group1 &lt;- data.frame(x = rnorm(1000, -1, .4),
    y = rnorm(1000, -1, .2))
group2 &lt;- data.frame(x = rnorm(1000, +1, .2),
    y = rnorm(1000, +1, .4))
X = rbind(group1, group2)

### Add an obvious outlier which is within the 1d ranges
### (As an interesting test, remove the outlier and see what happens,
###  or check how its score changes when using sub-sampling or
###  changing the scoring metric for 'ndim=1')
X = rbind(X, c(-1, 1))

### Produce heatmaps
pts = seq(-3, 3, .1)
space_d &lt;- expand.grid(x = pts, y = pts)
plot.space &lt;- function(Z, ttl) {
    image(pts, pts, matrix(Z, nrow = length(pts)),
      col = rev(heat.colors(50)),
      main = ttl, cex.main = 1.4,
      xlim = c(-3, 3), ylim = c(-3, 3),
      xlab = "", ylab = "")
    par(new = TRUE)
    plot(X, type = "p", xlim = c(-3, 3), ylim = c(-3, 3),
     col = "#0000801A",
     axes = FALSE, main = "",
     xlab = "", ylab = "")
}

### Now try out different variations of the model

### Single-variable model
iso_simple = isolation.forest(
    X, ndim=1,
    ntrees=100,
    nthreads=1,
    penalize_range=FALSE,
    prob_pick_pooled_gain=0,
    prob_pick_avg_gain=0)
Z1 &lt;- predict(iso_simple, space_d)
plot.space(Z1, "Isolation Forest")

### Extended model
iso_ext = isolation.forest(
     X, ndim=2,
     ntrees=100,
     nthreads=1,
     penalize_range=FALSE,
     prob_pick_pooled_gain=0,
     prob_pick_avg_gain=0)
Z2 &lt;- predict(iso_ext, space_d)
plot.space(Z2, "Extended Isolation Forest")

### SCiForest
iso_sci = isolation.forest(
     X, ndim=2, ntry=1,
     coefs="normal",
     ntrees=100,
     nthreads=1,
     penalize_range=TRUE,
     prob_pick_pooled_gain=0,
     prob_pick_avg_gain=1)
Z3 &lt;- predict(iso_sci, space_d)
plot.space(Z3, "SCiForest")
     
### Fair-cut forest
iso_fcf = isolation.forest(
     X, ndim=2,
     ntrees=100,
     nthreads=1,
     penalize_range=FALSE,
     prob_pick_pooled_gain=1,
     prob_pick_avg_gain=0)
Z4 &lt;- predict(iso_fcf, space_d)
plot.space(Z4, "Fair-Cut Forest")
par(oldpar)

### (As another interesting variation, try setting
###  'penalize_range=TRUE' for the last model)

### Example 3: calculating pairwise distances,
### with a short validation against euclidean dist.
library(isotree)

### Generate random data with 3 dimensions
set.seed(1)
m &lt;- 100
n &lt;- 3
X &lt;- matrix(rnorm(m * n), nrow=m, ncol=n)

### Fit isolation forest model
iso &lt;- isolation.forest(X, ndim=2, ntrees=100, nthreads=1)

### Calculate distances with the model
### (this can be accelerated with 'isotree.build.indexer')
D_iso &lt;- predict(iso, X, type = "dist")

### Check that it correlates with euclidean distance
D_euc &lt;- dist(X, method = "euclidean")

cat(sprintf("Correlation with euclidean distance: %f\n",
    cor(D_euc, D_iso)))
### (Note that euclidean distance will never take
###  any correlations between variables into account,
###  which the isolation forest model can do)


### Example 4: imputing missing values
### (requires package MASS)
library(isotree)

### Generate random data, set some values as NA
if (require("MASS")) {
  set.seed(1)
  S &lt;- crossprod(matrix(rnorm(5 * 5), nrow = 5))
  mu &lt;- rnorm(5)
  X &lt;- MASS::mvrnorm(1000, mu, S)
  X_na &lt;- X
  values_NA &lt;- matrix(runif(1000 * 5) &lt; .15, nrow = 1000)
  X_na[values_NA] = NA
  
  ### Impute missing values with model
  iso &lt;- isolation.forest(
      X_na,
      build_imputer = TRUE,
      prob_pick_pooled_gain = 1,
      ndim = 2,
      ntry = 10,
      nthreads = 1
  )
  X_imputed &lt;- predict(iso, X_na, type = "impute")
  cat(sprintf("MSE for imputed values w/model: %f\n",
      mean((X[values_NA] - X_imputed[values_NA])^2)))
    
  ### Compare against simple mean imputation
  X_means &lt;- apply(X, 2, mean)
  X_imp_mean &lt;- X_na
  for (cl in 1:5)
      X_imp_mean[values_NA[,cl], cl] &lt;- X_means[cl]
  cat(sprintf("MSE for imputed values w/means: %f\n",
      mean((X[values_NA] - X_imp_mean[values_NA])^2)))
}



#### A more interesting example
#### (requires package outliertree)

### Compare outliers returned by these different methods,
### and see why some of the outliers returned by the
### isolation forest could be flagged as outliers
if (require("outliertree")) {
  hypothyroid &lt;- outliertree::hypothyroid
  
  iso &lt;- isolation.forest(hypothyroid, nthreads=1)
  pred_iso &lt;- predict(iso, hypothyroid)
  otree &lt;- outliertree::outlier.tree(
      hypothyroid,
      z_outlier = 6,
      pct_outliers = 0.02,
      outliers_print = 20,
      nthreads = 1)
  
  ### Now compare against the top
  ### outliers from isolation forest
  head(hypothyroid[order(-pred_iso), ], 20)
}

</code></pre>

<hr>
<h2 id='isotree.add.tree'>Add additional (single) tree to isolation forest model</h2><span id='topic+isotree.add.tree'></span>

<h3>Description</h3>

<p>Adds a single tree fit to the full (non-subsampled) data passed here. Must
have the same columns as previously-fitted data. Categorical columns, if any,
may have new categories.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isotree.add.tree(
  model,
  data,
  sample_weights = NULL,
  column_weights = NULL,
  refdata = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="isotree.add.tree_+3A_model">model</code></td>
<td>
<p>An Isolation Forest object as returned by <a href="#topic+isolation.forest">isolation.forest</a>, to which an additional tree will be added.
</p>
<p><b>This object will be modified in-place</b>.</p>
</td></tr>
<tr><td><code id="isotree.add.tree_+3A_data">data</code></td>
<td>
<p>A 'data.frame', 'data.table', 'tibble', 'matrix', or sparse matrix (from package 'Matrix' or 'SparseM', CSC format)
to which to fit the new tree.</p>
</td></tr>
<tr><td><code id="isotree.add.tree_+3A_sample_weights">sample_weights</code></td>
<td>
<p>Sample observation weights for each row of 'X', with higher weights indicating
distribution density (i.e. if the weight is two, it has the same effect of including the same data
point twice). If not 'NULL', model must have been built with 'weights_as_sample_prob' = 'FALSE'.</p>
</td></tr>
<tr><td><code id="isotree.add.tree_+3A_column_weights">column_weights</code></td>
<td>
<p>Sampling weights for each column in 'data'. Ignored when picking columns by deterministic criterion.
If passing 'NULL', each column will have a uniform weight. If used along with kurtosis weights, the
effect is multiplicative.</p>
</td></tr>
<tr><td><code id="isotree.add.tree_+3A_refdata">refdata</code></td>
<td>
<p>Reference points for distance and/or kernel calculations, if these were previously added to
the model object through <a href="#topic+isotree.set.reference.points">isotree.set.reference.points</a>. Must correspond to the same points that
were passed in the call to that function. If sparse, only CSC format is supported.
</p>
<p>This is ignored if the model has no stored reference points.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If constructing trees with different sample sizes, the outlier scores with depth-based metrics
will not be centered around 0.5 and might have a very skewed distribution. The standardizing
constant for the scores will be taken according to the sample size passed in the model construction argument.
</p>
<p>If trees are going to be fit to samples of different sizes, it's strongly recommended to use
density-based scoring metrics instead.
</p>
<p>Be aware that, if an out-of-memory error occurs, the resulting object might be rendered unusable
(might crash when calling certain functions).
</p>
<p>For safety purposes, the model object can be deep copied (including the underlying C++ object)
through function <a href="#topic+isotree.deep.copy">isotree.deep.copy</a> before undergoing an in-place modification like this.
</p>
<p>If this function is going to be called frequently, it's highly recommended to use 'lazy_serialization=TRUE'
as then it will not need to copy over serialized bytes.
</p>


<h3>Value</h3>

<p>The same 'model' object now modified, as invisible.
</p>


<h3>See Also</h3>

<p><a href="#topic+isolation.forest">isolation.forest</a> <a href="#topic+isotree.restore.handle">isotree.restore.handle</a>
</p>

<hr>
<h2 id='isotree.append.trees'>Append isolation trees from one model into another</h2><span id='topic+isotree.append.trees'></span>

<h3>Description</h3>

<p>This function is intended for merging models <b>that use the same hyperparameters</b> but
were fitted to different subsets of data.
</p>
<p>In order for this to work, both models must have been fit to data in the same format - 
that is, same number of columns, same order of the columns, and same column types, although
not necessarily same object classes (e.g. can mix 'base::matrix' and 'Matrix::dgCMatrix').
</p>
<p>If the data has categorical variables, the models should have been built with parameter
'recode_categ=FALSE' in the call to <a href="#topic+isolation.forest">isolation.forest</a>,
and the categorical columns passed as type 'factor' with the same 'levels' -
otherwise different models might be using different encodings for each categorical column,
which will not be preserved as only the trees will be appended without any associated metadata.
</p>
<p>Note that this function will not perform any checks on the inputs, and passing two incompatible
models (e.g. fit to different numbers of columns) will result in wrong results and
potentially crashing the R process when using the resulting object.
</p>
<p>Also be aware that the first input will be modified in-place.
</p>
<p>If using 'lazy_serialization=FALSE', this will trigger a re-serialization so it will be slower
than if using 'lazy_serialization=TRUE'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isotree.append.trees(model, other)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="isotree.append.trees_+3A_model">model</code></td>
<td>
<p>An Isolation Forest model (as returned by function <a href="#topic+isolation.forest">isolation.forest</a>)
to which trees from 'other' (another Isolation Forest model) will be appended into.
</p>
<p><b>Will be modified in-place</b>, and on exit will contain the resulting merged model.</p>
</td></tr>
<tr><td><code id="isotree.append.trees_+3A_other">other</code></td>
<td>
<p>Another Isolation Forest model, from which trees will be appended into
'model'. It will not be modified during the call to this function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Be aware that, if an out-of-memory error occurs, the resulting object might be rendered unusable
(might crash when calling certain functions).
</p>
<p>For safety purposes, the model object can be deep copied (including the underlying C++ object)
through function <a href="#topic+isotree.deep.copy">isotree.deep.copy</a> before undergoing an in-place modification like this.
</p>


<h3>Value</h3>

<p>The same input 'model' object, now with the new trees appended, returned as invisible.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(isotree)

### Generate two random sets of data
m &lt;- 100
n &lt;- 2
set.seed(1)
X1 &lt;- matrix(rnorm(m*n), nrow=m)
X2 &lt;- matrix(rnorm(m*n), nrow=m)

### Fit a model to each dataset
iso1 &lt;- isolation.forest(X1, ntrees=3, ndim=2, nthreads=1)
iso2 &lt;- isolation.forest(X2, ntrees=2, ndim=2, nthreads=1)

### Check the terminal nodes for some observations
nodes1 &lt;- predict(iso1, head(X1, 3), type="tree_num")
nodes2 &lt;- predict(iso2, head(X1, 3), type="tree_num")

### Check also the average isolation depths
nodes1.depths &lt;- predict(iso1, head(X1, 3), type="avg_depth")
nodes2.depths &lt;- predict(iso2, head(X1, 3), type="avg_depth")

### Append the trees from 'iso2' into 'iso1'
iso1 &lt;- isotree.append.trees(iso1, iso2)

### Check that it predicts the same as the two models
nodes.comb &lt;- predict(iso1, head(X1, 3), type="tree_num")
nodes.comb == cbind(nodes1, nodes2)

### The new predicted scores will be a weighted average
### (Be aware that, due to round-off, it will not match with '==')
nodes.comb.depths &lt;- predict(iso1, head(X1, 3), type="avg_depth")
nodes.comb.depths
(3*nodes1.depths + 2*nodes2.depths) / 5
</code></pre>

<hr>
<h2 id='isotree.build.indexer'>Build Indexer for Faster Terminal Node Predictions and/or Distance Calculations</h2><span id='topic+isotree.build.indexer'></span>

<h3>Description</h3>

<p>Builds an index of terminal nodes for faster prediction of terminal node numbers
(calling 'predict' with 'type=&quot;tree_num&quot;').
</p>
<p>Optionally, can also pre-calculate terminal node distances in order to speed up
distance calculations (calling 'predict' with 'type=&quot;dist&quot;' or 'type=&quot;avg_sep&quot;').
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isotree.build.indexer(model, with_distances = FALSE, nthreads = model$nthreads)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="isotree.build.indexer_+3A_model">model</code></td>
<td>
<p>An Isolation Forest model (as returned by function <a href="#topic+isolation.forest">isolation.forest</a>)
for which an indexer for terminal node numbers and/or distances will be added.
</p>
<p><b>The object will be modified in-place</b>.</p>
</td></tr>
<tr><td><code id="isotree.build.indexer_+3A_with_distances">with_distances</code></td>
<td>
<p>Whether to also pre-calculate node distances in order to speed up
'predict' with 'type=&quot;dist&quot;' or 'type=&quot;avg_sep&quot;'.
Note that this will consume a lot more memory and make the resulting object significantly
heavier.</p>
</td></tr>
<tr><td><code id="isotree.build.indexer_+3A_nthreads">nthreads</code></td>
<td>
<p>Number of parallel threads to use.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This feature is not available for models that use 'missing_action=&quot;divide&quot;'
or 'new_categ_action=&quot;weighted&quot;' (which are the defaults when passing 'ndim=1').
</p>


<h3>Value</h3>

<p>The same 'model' object (as invisible), but now with an indexer added to it. Note
the input object is modified in-place regardless.
</p>


<h3>See Also</h3>

<p><a href="#topic+isotree.drop.indexer">isotree.drop.indexer</a>
</p>

<hr>
<h2 id='isotree.deep.copy'>Deep-Copy an Isolation Forest Model Object</h2><span id='topic+isotree.deep.copy'></span>

<h3>Description</h3>

<p>Generates a deep copy of a model object, including the C++ objects inside it.
This function is only meaningful if one intends to call a function that modifies the
internal C++ objects - currently, the only such function are <a href="#topic+isotree.add.tree">isotree.add.tree</a>
and <a href="#topic+isotree.append.trees">isotree.append.trees</a> - as otherwise R's objects follow a copy-on-write logic.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isotree.deep.copy(model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="isotree.deep.copy_+3A_model">model</code></td>
<td>
<p>An 'isolation_forest' model object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A new 'isolation_forest' object, with deep-copied C++ objects.
</p>


<h3>See Also</h3>

<p><a href="#topic+isotree.is.same">isotree.is.same</a>
</p>

<hr>
<h2 id='isotree.drop.imputer'>Drop Imputer Sub-Object from Isolation Forest Model Object</h2><span id='topic+isotree.drop.imputer'></span>

<h3>Description</h3>

<p>Drops the imputer sub-object from an isolation forest model object, if it was fitted with data imputation
capabilities. The imputer, if constructed, is likely to be a very heavy object which might
not be needed for all purposes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isotree.drop.imputer(model, manually_delete_cpp = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="isotree.drop.imputer_+3A_model">model</code></td>
<td>
<p>An 'isolation_forest' model object.</p>
</td></tr>
<tr><td><code id="isotree.drop.imputer_+3A_manually_delete_cpp">manually_delete_cpp</code></td>
<td>
<p>Whether to manually delete the underlying C++ object after calling this function.
</p>
<p>If passing &lsquo;FALSE', memory will not be freed until the underlying R &rsquo;externalptr' object is garbage-collected,
which typically happens after the next call to 'gc()'.
</p>
<p>If passing &lsquo;TRUE', will manually delete the C++ object held in the &rsquo;externalptr' object before nullifying it.
Note that, if somehow one assigned the pointer address to some other R variable through e.g. a deep copy of
the 'externalptr' object (that happened without copying the full model object where this R variable is stored),
then other pointers pointing at the same address might trigger crashes at the moment they are used.
</p>
<p>Note that, unless one starts manually fiddling with the internals of model objects and assigning variables to/from
them, it should not be possible to end up in a situation in which an 'externalptr' object ends up deep-copied,
especially when using 'lazy_serialization=TRUE'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The same 'model' object, but now with the imputer removed. <b>Note that 'model' is modified in-place
in any event</b>.
</p>

<hr>
<h2 id='isotree.drop.indexer'>Drop Indexer Sub-Object from Isolation Forest Model Object</h2><span id='topic+isotree.drop.indexer'></span>

<h3>Description</h3>

<p>Drops the indexer sub-object from an isolation forest model object, if it was constructed.
The indexer, if constructed, is likely to be a very heavy object which might
not be needed for all purposes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isotree.drop.indexer(model, manually_delete_cpp = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="isotree.drop.indexer_+3A_model">model</code></td>
<td>
<p>An 'isolation_forest' model object.</p>
</td></tr>
<tr><td><code id="isotree.drop.indexer_+3A_manually_delete_cpp">manually_delete_cpp</code></td>
<td>
<p>Whether to manually delete the underlying C++ object after calling this function.
</p>
<p>If passing &lsquo;FALSE', memory will not be freed until the underlying R &rsquo;externalptr' object is garbage-collected,
which typically happens after the next call to 'gc()'.
</p>
<p>If passing &lsquo;TRUE', will manually delete the C++ object held in the &rsquo;externalptr' object before nullifying it.
Note that, if somehow one assigned the pointer address to some other R variable through e.g. a deep copy of
the 'externalptr' object (that happened without copying the full model object where this R variable is stored),
then other pointers pointing at the same address might trigger crashes at the moment they are used.
</p>
<p>Note that, unless one starts manually fiddling with the internals of model objects and assigning variables to/from
them, it should not be possible to end up in a situation in which an 'externalptr' object ends up deep-copied,
especially when using 'lazy_serialization=TRUE'.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that reference points as added through <a href="#topic+isotree.set.reference.points">isotree.set.reference.points</a> are
associated with the indexer object and will also be dropped if any were added.
</p>


<h3>Value</h3>

<p>The same 'model' object, but now with the indexer removed. <b>Note that 'model' is modified in-place
in any event</b>.
</p>


<h3>See Also</h3>

<p><a href="#topic+isotree.build.indexer">isotree.build.indexer</a>
</p>

<hr>
<h2 id='isotree.drop.reference.points'>Drop Reference Points from Isolation Forest Model Object</h2><span id='topic+isotree.drop.reference.points'></span>

<h3>Description</h3>

<p>Drops any reference points used for distance and/or kernel calculations
from the model object, if any were set through <a href="#topic+isotree.set.reference.points">isotree.set.reference.points</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isotree.drop.reference.points(model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="isotree.drop.reference.points_+3A_model">model</code></td>
<td>
<p>An 'isolation_forest' model object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The same 'model' object, but now with the reference points removed. <b>Note that 'model' is modified in-place
in any event</b>.
</p>


<h3>See Also</h3>

<p><a href="#topic+isotree.set.reference.points">isotree.set.reference.points</a>
</p>

<hr>
<h2 id='isotree.export.model'>Export Isolation Forest model</h2><span id='topic+isotree.export.model'></span>

<h3>Description</h3>

<p>Save Isolation Forest model to a serialized file along with its
metadata, in order to be used in the Python or the C++ versions of this package.
</p>
<p>This function is not suggested to be used for passing models to and from R -
in such case, one can use 'saveRDS' and 'readRDS' instead, although the function
still works correctly for serializing objects between R sessions.
</p>
<p>Note that, if the model was fitted to a 'data.frame', the column names must be
something exportable as JSON, and must be something that Python's Pandas could
use as column names (e.g. strings/character).
</p>
<p>Can optionally generate a JSON file with metadata such as the column names and the
levels of categorical variables, which can be inspected visually in order to detect
potential issues (e.g. character encoding) or to make sure that the columns are of
the right types.
</p>
<p>Requires the 'jsonlite' package in order to work.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isotree.export.model(model, file, add_metadata_file = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="isotree.export.model_+3A_model">model</code></td>
<td>
<p>An Isolation Forest model as returned by function <a href="#topic+isolation.forest">isolation.forest</a>.</p>
</td></tr>
<tr><td><code id="isotree.export.model_+3A_file">file</code></td>
<td>
<p>File path where to save the model. File connections are not accepted, only
file paths</p>
</td></tr>
<tr><td><code id="isotree.export.model_+3A_add_metadata_file">add_metadata_file</code></td>
<td>
<p>Whether to generate a JSON file with metadata, which will have
the same name as the model but will end in '.metadata'. This file is not used by the
de-serialization function, it's only meant to be inspected manually, since such contents
will already be written in the produced model file.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The metadata file, if produced, will contain, among other things, the encoding that was used for
categorical columns - this is under 'data_info.cat_levels', as an array of arrays by column,
with the first entry for each column corresponding to category 0, second to category 1,
and so on (the C++ version takes them as integers). When passing 'categ_cols', there
will be no encoding but it will save the maximum category integer and the column
numbers instead of names.
</p>
<p>The serialized file can be used in the C++ version by reading it as a binary file
and de-serializing its contents using the C++ function 'deserialize_combined'
(recommended to use 'inspect_serialized_object' beforehand).
</p>
<p>Be aware that this function will write raw bytes from memory as-is without compression,
so the file sizes can end up being much larger than when using 'saveRDS'.
</p>
<p>The metadata is not used in the C++ version, but is necessary for the R and Python versions.
</p>
<p>Note that the model treats boolean/logical variables as categorical. Thus, if the model was fit
to a 'data.frame' with boolean columns, when importing this model into C++, they need to be
encoded in the same order - e.g. the model might encode 'TRUE' as zero and 'FALSE'
as one - you need to look at the metadata for this.
</p>
<p>The files produced by this function will be compatible between:</p>

<ul>
<li><p> Different operating systems.
</p>
</li>
<li><p> Different compilers.
</p>
</li>
<li><p> Different Python/R versions.
</p>
</li>
<li><p> Systems with different 'size_t' width (e.g. 32-bit and 64-bit),
as long as the file was produced on a system that was either 32-bit or 64-bit,
and as long as each saved value fits within the range of the machine's 'size_t' type.
</p>
</li>
<li><p> Systems with different 'int' width,
as long as the file was produced on a system that was 16-bit, 32-bit, or 64-bit,
and as long as each saved value fits within the range of the machine's int type.
</p>
</li>
<li><p> Systems with different bit endianness (e.g. x86 and PPC64 in non-le mode).
</p>
</li>
<li><p> Versions of this package from 0.3.0 onwards, <b>but only forwards compatible</b>
(e.g. a model saved with versions 0.3.0 to 0.3.5 can be loaded under version
0.3.6, but not the other way around, and attempting to do so will cause crashes
and memory curruptions without an informative error message). <b>This last point applies
also to models saved through save, saveRDS, qsave, and similar</b>. Note that loading a
model produced by an earlier version of the library might be slightly slower.
</p>
</li></ul>

<p>But will not be compatible between:</p>

<ul>
<li><p> Systems with different floating point numeric representations
(e.g. standard IEEE754 vs. a base-10 system).
</p>
</li>
<li><p> Versions of this package earlier than 0.3.0.
</p>
</li></ul>

<p>This pretty much guarantees that a given file can be serialized and de-serialized
in the same machine in which it was built, regardless of how the library was compiled.
</p>
<p>Reading a serialized model that was produced in a platform with different
characteristics (e.g. 32-bit vs. 64-bit) will be much slower.
</p>
<p>On Windows, if compiling this library with a compiler other than MSVC or MINGW,
(not currently supported by CRAN's build systems at the moment of writing)
there might be issues exporting models larger than 2GB.
</p>
<p>In non-windows systems, if the file name contains non-ascii characters, the file name
must be in the system's native encoding. In windows, file names with non-ascii
characters are supported as long as the package is compiled with GCC5 or newer.
</p>
<p>Note that, while 'readRDS' and 'load' will not make any changes to the serialized format
of the objects, reading a serialized model from a file will forcibly re-serialize,
using the system's own setup (e.g. 32-bit vs. 64-bit, endianness, etc.), and as such
can be used to convert formats.
</p>


<h3>Value</h3>

<p>The same 'model' object that was passed as input, as invisible.
</p>


<h3>See Also</h3>

<p><a href="#topic+isotree.import.model">isotree.import.model</a> <a href="#topic+isotree.restore.handle">isotree.restore.handle</a>
</p>

<hr>
<h2 id='isotree.get.num.nodes'>Get Number of Nodes per Tree</h2><span id='topic+isotree.get.num.nodes'></span>

<h3>Description</h3>

<p>Get Number of Nodes per Tree
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isotree.get.num.nodes(model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="isotree.get.num.nodes_+3A_model">model</code></td>
<td>
<p>An Isolation Forest model as produced by function 'isolation.forest'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with entries '&quot;total&quot;' and '&quot;terminal&quot;', both of which are integer vectors
with length equal to the number of trees. '&quot;total&quot;' contains the total number of nodes that
each tree has, while '&quot;terminal&quot;' contains the number of terminal nodes per tree.
</p>

<hr>
<h2 id='isotree.import.model'>Load an Isolation Forest model exported from Python</h2><span id='topic+isotree.import.model'></span>

<h3>Description</h3>

<p>Loads a serialized Isolation Forest model as produced and exported
by the Python version of this package. Note that the metadata must be something
importable in R - e.g. column names must be valid for R (numbers are valid for
Python's pandas, but not for R, for example).
</p>
<p>It's recommended to generate a '.metadata' file (passing 'add_metada_file=TRUE') and
to visually inspect said file in any case.
</p>
<p>This function is not meant to be used for passing models to and from R -
in such case, one can use 'saveRDS' and 'readRDS' instead as they will
likely result in smaller file sizes (although this function will still
work correctly for serialization within R).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isotree.import.model(file, lazy_serialization = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="isotree.import.model_+3A_file">file</code></td>
<td>
<p>Path to the saved isolation forest model.
Must be a file path, not a file connection,
and the character encoding should correspond to the system's native encoding.</p>
</td></tr>
<tr><td><code id="isotree.import.model_+3A_lazy_serialization">lazy_serialization</code></td>
<td>
<p>Whether to use lazy serialization through the ALTREP
system for the resulting objects. See the documentation for this same parameter
in <a href="#topic+isolation.forest">isolation.forest</a> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the model was fit to a &lsquo;DataFrame' using Pandas&rsquo; own Boolean types,
take a look at the metadata to check if these columns will be taken as booleans
(R logicals) or as categoricals with string values '&quot;True&quot;' and '&quot;False&quot;'.
</p>
<p>See the documentation for <a href="#topic+isotree.export.model">isotree.export.model</a> for details about compatibility
of the generated files across different machines and versions.
</p>


<h3>Value</h3>

<p>An isolation forest model, as if it had been constructed through
<a href="#topic+isolation.forest">isolation.forest</a>.
</p>


<h3>See Also</h3>

<p><a href="#topic+isotree.export.model">isotree.export.model</a> <a href="#topic+isotree.restore.handle">isotree.restore.handle</a>
</p>

<hr>
<h2 id='isotree.is.same'>Check if two Isolation Forest Models Share the Same C++ Object</h2><span id='topic+isotree.is.same'></span>

<h3>Description</h3>

<p>Checks if two isolation forest models, as produced by functions
like <a href="#topic+isolation.forest">isolation.forest</a>, have a reference to the same underlying C++ object.
</p>
<p>When this is the case, functions that produce in-place modifications, such as
<a href="#topic+isotree.build.indexer">isotree.build.indexer</a>, will produce changes in all of the R variables that
share the same C++ object.
</p>
<p>Two R variables will have the same C++ object when assigning one variable to another,
but will have different C++ objects when these R objects are serialized and
deserialized or when calling <a href="#topic+isotree.deep.copy">isotree.deep.copy</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isotree.is.same(obj1, obj2)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="isotree.is.same_+3A_obj1">obj1</code></td>
<td>
<p>First model to compare (against 'obj2').</p>
</td></tr>
<tr><td><code id="isotree.is.same_+3A_obj2">obj2</code></td>
<td>
<p>Second model to compare (against 'obj1').</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A logical (boolean) value which will be 'TRUE' when both models
have a reference to the same C++ object, or 'FALSE' otherwise.
</p>


<h3>See Also</h3>

<p><a href="#topic+isotree.deep.copy">isotree.deep.copy</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(isotree)
data(mtcars)
model &lt;- isolation.forest(mtcars, ntrees = 10, nthreads = 1, ndim = 1)

model_shallow_copy &lt;- model
isotree.is.same(model, model_shallow_copy)

model_deep_copy &lt;- isotree.deep.copy(model)
isotree.is.same(model, model_deep_copy)

isotree.add.tree(model_shallow_copy, mtcars)
length(isotree.get.num.nodes(model_shallow_copy)$total)
length(isotree.get.num.nodes(model)$total)
length(isotree.get.num.nodes(model_deep_copy)$total)
</code></pre>

<hr>
<h2 id='isotree.plot.tree'>Plot Tree from Isolation Forest Model</h2><span id='topic+isotree.plot.tree'></span>

<h3>Description</h3>

<p>Plots a given tree from an isolation forest model.
</p>
<p>Requires the 'DiagrammeR' library to be installed.
</p>
<p>Note that this is just a wrapper over <a href="#topic+isotree.to.graphviz">isotree.to.graphviz</a> + 'DiagrammeR::grViz'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isotree.plot.tree(
  model,
  output_tree_num = FALSE,
  tree = 1L,
  column_names = NULL,
  column_names_categ = NULL,
  nthreads = model$nthreads,
  width = NULL,
  height = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="isotree.plot.tree_+3A_model">model</code></td>
<td>
<p>An Isolation Forest object as returned by <a href="#topic+isolation.forest">isolation.forest</a>.</p>
</td></tr>
<tr><td><code id="isotree.plot.tree_+3A_output_tree_num">output_tree_num</code></td>
<td>
<p>Whether to make the statements / outputs return the terminal node number
instead of the isolation depth. The numeration will start at one.</p>
</td></tr>
<tr><td><code id="isotree.plot.tree_+3A_tree">tree</code></td>
<td>
<p>Tree for which to generate SQL statements or other outputs. If passed, will generate
the statements only for that single tree. If passing 'NULL', will
generate statements for all trees in the model.</p>
</td></tr>
<tr><td><code id="isotree.plot.tree_+3A_column_names">column_names</code></td>
<td>
<p>Column names to use for the <b>numeric</b> columns.
If not passed and the model was fit to a 'data.frame', will use the column
names from that 'data.frame', which can be found under 'model$metadata$cols_num'.
If not passing it and the model was fit to data in a format other than
'data.frame', the columns will be named 'column_N' in the resulting
SQL statement. Note that the names will be taken verbatim - this function will
not do any checks for e.g. whether they constitute valid SQL or not when exporting to SQL, and will not
escape characters such as double quotation marks when exporting to SQL.</p>
</td></tr>
<tr><td><code id="isotree.plot.tree_+3A_column_names_categ">column_names_categ</code></td>
<td>
<p>Column names to use for the <b>categorical</b> columns.
If not passed, will use the column names from the 'data.frame' to which the
model was fit. These can be found under 'model$metadata$cols_cat'.</p>
</td></tr>
<tr><td><code id="isotree.plot.tree_+3A_nthreads">nthreads</code></td>
<td>
<p>Number of parallel threads to use.</p>
</td></tr>
<tr><td><code id="isotree.plot.tree_+3A_width">width</code></td>
<td>
<p>Width for the plot, to pass to 'DiagrammeR::grViz'.</p>
</td></tr>
<tr><td><code id="isotree.plot.tree_+3A_height">height</code></td>
<td>
<p>Height for the plot, to pass to 'DiagrammeR::grViz'.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In general, isolation forest trees tend to be rather large, and the contents on the
nodes can be very long when using 'ndim&gt;1' - if the idea is to get easily visualizable
trees, one might want to use parameters like 'ndim=1', 'sample_size=256', 'max_depth=8'.
</p>


<h3>Value</h3>

<p>An 'htmlwidget' object that contains the plot.
</p>

<hr>
<h2 id='isotree.restore.handle'>Unpack isolation forest model after de-serializing</h2><span id='topic+isotree.restore.handle'></span>

<h3>Description</h3>

<p>After persisting an isolation forest model object through 'saveRDS', 'save', or restarting a session, the
underlying C++ objects that constitute the isolation forest model and which live only on the C++ heap memory are not saved along,
and depending on parameter 'lazy_serialization', might not get automatically restored after loading a saved model through
'readRDS' or 'load'.
</p>
<p>The model object however keeps serialized versions of the C++ objects as raw bytes, from which the C++ objects can be
reconstructed, and are done so automatically upon de-serialization when using 'lazy_serialization=TRUE', but otherwise,
the C++ objects will only get de-serialized after calling 'predict', 'print', 'summary', or 'isotree.add.tree' on the
freshly-loaded object from 'readRDS' or 'load'.
</p>
<p>This function allows to automatically de-serialize the object (&quot;complete&quot; or &quot;restore&quot; the
handle) without having to call any function that would do extra processing when one uses 'lazy_serialization=FALSE'
(calling the function is <b>not</b> needed when using 'lazy_serialization=TRUE').
</p>
<p>It is an analog to XGBoost's &lsquo;xgb.Booster.complete' and CatBoost&rsquo;s 'catboost.restore_handle' functions.
</p>
<p>If the model was buit with 'lazy_serialization=TRUE', this function will not do anything to the object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isotree.restore.handle(model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="isotree.restore.handle_+3A_model">model</code></td>
<td>
<p>An Isolation Forest object as returned by 'isolation.forest', which has been just loaded from a disk
file through 'readRDS', 'load', or a session restart, and which was constructed with 'lazy_serialization=FALSE'.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If using this function to de-serialize a model in a production system, one might
want to delete the serialized bytes inside the object afterwards in order to free up memory.
These are under 'model$cpp_objects$(model,imputer,indexer)$ser'
- e.g.: 'model$cpp_objects$model$ser = NULL; gc()'.
</p>


<h3>Value</h3>

<p>The same model object that was passed as input. Object is modified in-place
however, so it does not need to be re-assigned.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Warning: this example will generate a temporary .Rds
### file in your temp folder, and will then delete it

### First, create a model from random data
library(isotree)
set.seed(1)
X &lt;- matrix(rnorm(100), nrow = 20)
iso &lt;- isolation.forest(X, ntrees=10, nthreads=1, lazy_serialization=FALSE)

### Now serialize the model
temp_file &lt;- file.path(tempdir(), "iso.Rds")
saveRDS(iso, temp_file)
iso2 &lt;- readRDS(temp_file)
file.remove(temp_file)

cat("Model pointer after loading is this: \n")
print(iso2$cpp_objects$model$ptr)

### now unpack it
isotree.restore.handle(iso2)

cat("Model pointer after unpacking is this: \n")
print(iso2$cpp_objects$model$ptr)

### Note that this function is not needed when using lazy_serialization=TRUE
iso_lazy &lt;- isolation.forest(X, ntrees=10, nthreads=1, lazy_serialization=TRUE)
temp_file_lazy &lt;- file.path(tempdir(), "iso_lazy.Rds")
saveRDS(iso_lazy, temp_file_lazy)
iso_lazy2 &lt;- readRDS(temp_file_lazy)
file.remove(temp_file_lazy)
cat("Model pointer after unpacking lazy-serialized: \n")
print(iso_lazy2$cpp_objects$model$ptr)
</code></pre>

<hr>
<h2 id='isotree.set.nthreads'>Set Number of Threads for Isolation Forest Model Object</h2><span id='topic+isotree.set.nthreads'></span>

<h3>Description</h3>

<p>Changes the number of threads that an isolation forest model object
will use when calling functions such as 'predict'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isotree.set.nthreads(model, nthreads = 1L)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="isotree.set.nthreads_+3A_model">model</code></td>
<td>
<p>An Isolation Forest model (as returned by function <a href="#topic+isolation.forest">isolation.forest</a>)
for which an indexer for terminal node numbers and/or distances will be added.
</p>
<p><b>The object will be modified in-place</b>.</p>
</td></tr>
<tr><td><code id="isotree.set.nthreads_+3A_nthreads">nthreads</code></td>
<td>
<p>Number of threads to set for this model object to use.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The same 'model' object (as invisible), but now with a different configured number of threadst. Note
the input object is modified in-place regardless.
</p>

<hr>
<h2 id='isotree.set.reference.points'>Set Reference Points to Calculate Distances or Kernels With</h2><span id='topic+isotree.set.reference.points'></span>

<h3>Description</h3>

<p>Sets some points as pre-defined landmarks with respect to which distances and/or
isolation kernel values will be calculated for arbitrary new points in calls to
'predict' with types '&quot;dist&quot;', '&quot;avg_sep&quot;', '&quot;kernel&quot;'. If any points have already been set
as references in the model object, they will be overwritten with the new points passed here.
</p>
<p>Be aware that adding reference points requires building a tree indexer.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isotree.set.reference.points(
  model,
  data,
  with_distances = FALSE,
  nthreads = model$nthreads
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="isotree.set.reference.points_+3A_model">model</code></td>
<td>
<p>An Isolation Forest model (as returned by function <a href="#topic+isolation.forest">isolation.forest</a>)
for which reference points for distance and/or kernel calculations will be set.
</p>
<p><b>The object will be modified in-place</b>. If there were any previous references, they will
be overwritten with the new ones passed here.</p>
</td></tr>
<tr><td><code id="isotree.set.reference.points_+3A_data">data</code></td>
<td>
<p>Observations to set as reference points for future distance and/or isolation kernel calculations.
Same format as for <a href="#topic+predict.isolation_forest">predict.isolation_forest</a>.</p>
</td></tr>
<tr><td><code id="isotree.set.reference.points_+3A_with_distances">with_distances</code></td>
<td>
<p>Whether to pre-calculate node distances (this is required to calculate distance
from arbitrary points to the reference points).
</p>
<p>Note that reference points for distances can only be set when using 'assume_full_distr=FALSE'
(which is the default).</p>
</td></tr>
<tr><td><code id="isotree.set.reference.points_+3A_nthreads">nthreads</code></td>
<td>
<p>Number of parallel threads to use.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that points are added in terms of their terminal node indices, but the raw data about
them is not kept - thus, calling <a href="#topic+isotree.add.tree">isotree.add.tree</a> later on a model with reference points
requires passing those reference points again to add their node indices to the new tree.
</p>
<p>If using 'lazy_serialization=TRUE', and the process fails while setting references (e.g.
due to out-of-memory errors), previous references that the model might have had will be lost.
</p>


<h3>Value</h3>

<p>The same 'model' object (as invisible), but now with added reference points that
can be used for new distance and/or kernel calculations with respect to other arbitrary points.
</p>


<h3>See Also</h3>

<p><a href="#topic+isotree.build.indexer">isotree.build.indexer</a>
</p>

<hr>
<h2 id='isotree.subset.trees'>Subset trees of a given model</h2><span id='topic+isotree.subset.trees'></span><span id='topic++5B.isolation_forest'></span>

<h3>Description</h3>

<p>Creates a new isolation forest model containing only selected trees of a
given isolation forest model object. Note that, if using 'lazy_serialization=FALSE',
this will re-trigger serialization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isotree.subset.trees(model, trees_take)

## S3 method for class 'isolation_forest'
x[i]
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="isotree.subset.trees_+3A_model">model</code>, <code id="isotree.subset.trees_+3A_x">x</code></td>
<td>
<p>An 'isolation_forest' model object.</p>
</td></tr>
<tr><td><code id="isotree.subset.trees_+3A_trees_take">trees_take</code>, <code id="isotree.subset.trees_+3A_i">i</code></td>
<td>
<p>Indices of the trees of 'model' to copy over to a new model,
as an integer vector.
Must be integers with numeration starting at one</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A new isolation forest model object, containing only the subset of trees
from this 'model' that was specified under 'trees_take'.
</p>

<hr>
<h2 id='isotree.to.graphviz'>Generate GraphViz Dot Representation of Tree</h2><span id='topic+isotree.to.graphviz'></span>

<h3>Description</h3>

<p>Generate GraphViz representations of model trees in 'dot' format - either
separately per tree (the default), or for a single tree if needed (if passing 'tree')
Can also be made to output terminal node numbers (numeration starting at one).
</p>
<p>These can be loaded as graphs through e.g. 'DiagrammeR::grViz(x)', where 'x' would
be the output of this function for a given tree.
</p>
<p>Graph format is based on XGBoost's.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isotree.to.graphviz(
  model,
  output_tree_num = FALSE,
  tree = NULL,
  column_names = NULL,
  column_names_categ = NULL,
  nthreads = model$nthreads
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="isotree.to.graphviz_+3A_model">model</code></td>
<td>
<p>An Isolation Forest object as returned by <a href="#topic+isolation.forest">isolation.forest</a>.</p>
</td></tr>
<tr><td><code id="isotree.to.graphviz_+3A_output_tree_num">output_tree_num</code></td>
<td>
<p>Whether to make the statements / outputs return the terminal node number
instead of the isolation depth. The numeration will start at one.</p>
</td></tr>
<tr><td><code id="isotree.to.graphviz_+3A_tree">tree</code></td>
<td>
<p>Tree for which to generate SQL statements or other outputs. If passed, will generate
the statements only for that single tree. If passing 'NULL', will
generate statements for all trees in the model.</p>
</td></tr>
<tr><td><code id="isotree.to.graphviz_+3A_column_names">column_names</code></td>
<td>
<p>Column names to use for the <b>numeric</b> columns.
If not passed and the model was fit to a 'data.frame', will use the column
names from that 'data.frame', which can be found under 'model$metadata$cols_num'.
If not passing it and the model was fit to data in a format other than
'data.frame', the columns will be named 'column_N' in the resulting
SQL statement. Note that the names will be taken verbatim - this function will
not do any checks for e.g. whether they constitute valid SQL or not when exporting to SQL, and will not
escape characters such as double quotation marks when exporting to SQL.</p>
</td></tr>
<tr><td><code id="isotree.to.graphviz_+3A_column_names_categ">column_names_categ</code></td>
<td>
<p>Column names to use for the <b>categorical</b> columns.
If not passed, will use the column names from the 'data.frame' to which the
model was fit. These can be found under 'model$metadata$cols_cat'.</p>
</td></tr>
<tr><td><code id="isotree.to.graphviz_+3A_nthreads">nthreads</code></td>
<td>
<p>Number of parallel threads to use.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> The generated graphs will not include range penalizations, thus
predictions might differ from calls to 'predict' when using
'penalize_range=TRUE'.
</p>
</li>
<li><p> The generated graphs will only include handling of missing values
when using 'missing_action=&quot;impute&quot;'. When using the single-variable
model with categorical variables + subset splits, the rule buckets might be
incomplete due to not including categories that were not present in a given
node - this last point can be avoided by using 'new_categ_action=&quot;smallest&quot;',
'new_categ_action=&quot;random&quot;', or 'missing_action=&quot;impute&quot;' (in the latter
case will treat them as missing, but the 'predict' function might treat
them differently).
</p>
</li>
<li><p> If using 'scoring_metric=&quot;density&quot;' or 'scoring_metric=&quot;boxed_ratio&quot;' plus
'output_tree_num=FALSE', the
outputs will correspond to the logarithm of the density rather than the density.
</p>
</li></ul>



<h3>Value</h3>

<p>If passing 'tree=NULL', will return a list with one element per tree in the model,
where each element consists of an R character / string with the 'dot' format representation
of the tree. If passing 'tree', the output will be instead a single character / string element
with the 'dot' representation for that tree.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(isotree)
set.seed(123)
X &lt;- matrix(rnorm(100 * 3), nrow = 100)
model &lt;- isolation.forest(X, ndim=1, max_depth=3, ntrees=2, nthreads=1)
model_as_graphviz &lt;- isotree.to.graphviz(model)

# These can be parsed and plotted with library 'DiagrammeR'
if (require("DiagrammeR")) {
    # first tree
    DiagrammeR::grViz(model_as_graphviz[[1]])

    DiagrammeR::grViz(model_as_graphviz[[1]])
}

## End(Not run)
</code></pre>

<hr>
<h2 id='isotree.to.json'>Generate JSON representations of model trees</h2><span id='topic+isotree.to.json'></span>

<h3>Description</h3>

<p>Generates a JSON representation of either a single tree in the model, or of all
the trees in the model.
</p>
<p>The JSON for a given tree will consist of a sub-json/list for each node, where nodes
are indexed by their number (base-1 indexing) as keys in these JSONs (note that they
are strings, not numbers, in order to conform to JSON format).
</p>
<p>Nodes will in turn consist of another map/list indicating whether they are terminal nodes
or not, their score and terminal node index if terminal, or otherwise the split conditions,
nodes to follow when the condition is or isn't met, and other aspects such as imputation
values if applicable, acceptable ranges when using range penalizations, fraction of the data
that went into the left node if recorded, among others.
</p>
<p>Note that the JSON structure will be very different for models that have 'ndim=1'
than for models that have 'ndim&gt;1'. In the case of 'ndim=1', the conditions are
based on the value of only one variable, but for 'ndim=2', they will consist of
a linear combination of different columns (which is expressed as a list of JSONs
with one entry per column that goes into the calculation) - for numeric columns for example,
these will be expressed in the json by a coefficient for the given column, and a centering
that needs to be applied, with the score from that column being added as
</p>
<p><code class="reqn">\text{coef} \times (x - \text{centering})</code>
</p>
<p>and the imputation value being applied in replacement of this formula in the case of
missing values for that column (depending on the model parameters); while in the case of
categorical columns, might either have a different coefficient for each possible category
('categ_split_type=&quot;subset&quot;'), or a single category that gets a non-zero coefficient
while the others get zeros ('categ_split_type=&quot;single_categ&quot;').
</p>
<p>The JSONs might contain redundant information in order to ease understanding of the model
logic - for example, when using 'ndim&gt;1' and 'categ_split_type=&quot;single_categ&quot;',
the coefficient for the non-chosen categories will always be zero, but is nevertheless
added to every node's JSON, even if not needed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isotree.to.json(
  model,
  output_tree_num = FALSE,
  tree = NULL,
  column_names = NULL,
  column_names_categ = NULL,
  as_str = FALSE,
  nthreads = model$nthreads
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="isotree.to.json_+3A_model">model</code></td>
<td>
<p>An Isolation Forest object as returned by <a href="#topic+isolation.forest">isolation.forest</a>.</p>
</td></tr>
<tr><td><code id="isotree.to.json_+3A_output_tree_num">output_tree_num</code></td>
<td>
<p>Whether to make the statements / outputs return the terminal node number
instead of the isolation depth. The numeration will start at one.</p>
</td></tr>
<tr><td><code id="isotree.to.json_+3A_tree">tree</code></td>
<td>
<p>Tree for which to generate SQL statements or other outputs. If passed, will generate
the statements only for that single tree. If passing 'NULL', will
generate statements for all trees in the model.</p>
</td></tr>
<tr><td><code id="isotree.to.json_+3A_column_names">column_names</code></td>
<td>
<p>Column names to use for the <b>numeric</b> columns.
If not passed and the model was fit to a 'data.frame', will use the column
names from that 'data.frame', which can be found under 'model$metadata$cols_num'.
If not passing it and the model was fit to data in a format other than
'data.frame', the columns will be named 'column_N' in the resulting
SQL statement. Note that the names will be taken verbatim - this function will
not do any checks for e.g. whether they constitute valid SQL or not when exporting to SQL, and will not
escape characters such as double quotation marks when exporting to SQL.</p>
</td></tr>
<tr><td><code id="isotree.to.json_+3A_column_names_categ">column_names_categ</code></td>
<td>
<p>Column names to use for the <b>categorical</b> columns.
If not passed, will use the column names from the 'data.frame' to which the
model was fit. These can be found under 'model$metadata$cols_cat'.</p>
</td></tr>
<tr><td><code id="isotree.to.json_+3A_as_str">as_str</code></td>
<td>
<p>Whether to return the result as raw JSON strings (returned as R's character type)
instead of being parsed into R lists (internally, it uses 'jsonlite::fromJSON').</p>
</td></tr>
<tr><td><code id="isotree.to.json_+3A_nthreads">nthreads</code></td>
<td>
<p>Number of parallel threads to use.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> If using 'scoring_metric=&quot;density&quot;' or 'scoring_metric=&quot;boxed_ratio&quot;' plus
'output_tree_num=FALSE', the
outputs will correspond to the logarithm of the density rather than the density.
</p>
</li></ul>



<h3>Value</h3>

<p>Either a list of lists (when passing 'as_str=FALSE') or a vector of characters (when passing
'as_str=TRUE'), or a single such list or character element if passing 'tree'.
</p>

<hr>
<h2 id='isotree.to.sql'>Generate SQL statements from Isolation Forest model</h2><span id='topic+isotree.to.sql'></span>

<h3>Description</h3>

<p>Generate SQL statements - either separately per tree (the default),
for a single tree if needed (if passing 'tree'), or for all trees
concatenated together (if passing 'table_from'). Can also be made
to output terminal node numbers (numeration starting at one).
</p>
<p>Some important considerations:</p>

<ul>
<li><p> Making predictions through SQL is much less efficient than from the model
itself, as each terminal node will have to check all of the conditions
that lead to it instead of passing observations down a tree.
</p>
</li>
<li><p> If constructed with the default arguments, the model will not perform any
sub-sampling, which can lead to very big trees. If it was fit to a large
dataset, the generated SQL might consist of gigabytes of text, and might
lay well beyond the character limit of commands accepted by SQL vendors.
</p>
</li>
<li><p> The generated SQL statements will not include range penalizations, thus
predictions might differ from calls to 'predict' when using
'penalize_range=TRUE'.
</p>
</li>
<li><p> The generated SQL statements will only include handling of missing values
when using 'missing_action=&quot;impute&quot;'. When using the single-variable
model with categorical variables + subset splits, the rule buckets might be
incomplete due to not including categories that were not present in a given
node - this last point can be avoided by using 'new_categ_action=&quot;smallest&quot;',
'new_categ_action=&quot;random&quot;', or 'missing_action=&quot;impute&quot;' (in the latter
case will treat them as missing, but the 'predict' function might treat
them differently).
</p>
</li>
<li><p> The resulting statements will include all the tree conditions as-is,
with no simplification. Thus, there might be lots of redundant conditions
in a given terminal node (e.g. &quot;X &gt; 2&quot; and &quot;X &gt; 1&quot;, the second of which is
redundant).
</p>
</li>
<li><p> If using 'scoring_metric=&quot;density&quot;' or 'scoring_metric=&quot;boxed_ratio&quot;' plus
'output_tree_num=FALSE', the
outputs will correspond to the logarithm of the density rather than the density.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>isotree.to.sql(
  model,
  enclose = "doublequotes",
  output_tree_num = FALSE,
  tree = NULL,
  table_from = NULL,
  select_as = "outlier_score",
  column_names = NULL,
  column_names_categ = NULL,
  nthreads = model$nthreads
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="isotree.to.sql_+3A_model">model</code></td>
<td>
<p>An Isolation Forest object as returned by <a href="#topic+isolation.forest">isolation.forest</a>.</p>
</td></tr>
<tr><td><code id="isotree.to.sql_+3A_enclose">enclose</code></td>
<td>
<p>With which symbols to enclose the column names in the select statement
so as to make them SQL compatible in case they include characters like dots.
Options are:</p>

<ul>
<li><p> '&quot;doublequotes&quot;', which will enclose them as '&quot;column_name&quot;' - this will
work for e.g. PostgreSQL.
</p>
</li>
<li><p> '&quot;squarebraces&quot;', which will enclose them as '[column_name]' - this will
work for e.g. SQL Server.
</p>
</li>
<li><p> '&quot;none&quot;', which will output the column names as-is (e.g. 'column_name')
</p>
</li></ul>
</td></tr>
<tr><td><code id="isotree.to.sql_+3A_output_tree_num">output_tree_num</code></td>
<td>
<p>Whether to make the statements / outputs return the terminal node number
instead of the isolation depth. The numeration will start at one.</p>
</td></tr>
<tr><td><code id="isotree.to.sql_+3A_tree">tree</code></td>
<td>
<p>Tree for which to generate SQL statements or other outputs. If passed, will generate
the statements only for that single tree. If passing 'NULL', will
generate statements for all trees in the model.</p>
</td></tr>
<tr><td><code id="isotree.to.sql_+3A_table_from">table_from</code></td>
<td>
<p>If passing this, will generate a single select statement for the
outlier score from all trees, selecting the data from the table
name passed here. In this case, will always output the outlier
score, regardless of what is passed under 'output_tree_num'.</p>
</td></tr>
<tr><td><code id="isotree.to.sql_+3A_select_as">select_as</code></td>
<td>
<p>Alias to give to the generated outlier score in the select statement.
Ignored when not passing 'table_from'.</p>
</td></tr>
<tr><td><code id="isotree.to.sql_+3A_column_names">column_names</code></td>
<td>
<p>Column names to use for the <b>numeric</b> columns.
If not passed and the model was fit to a 'data.frame', will use the column
names from that 'data.frame', which can be found under 'model$metadata$cols_num'.
If not passing it and the model was fit to data in a format other than
'data.frame', the columns will be named 'column_N' in the resulting
SQL statement. Note that the names will be taken verbatim - this function will
not do any checks for e.g. whether they constitute valid SQL or not when exporting to SQL, and will not
escape characters such as double quotation marks when exporting to SQL.</p>
</td></tr>
<tr><td><code id="isotree.to.sql_+3A_column_names_categ">column_names_categ</code></td>
<td>
<p>Column names to use for the <b>categorical</b> columns.
If not passed, will use the column names from the 'data.frame' to which the
model was fit. These can be found under 'model$metadata$cols_cat'.</p>
</td></tr>
<tr><td><code id="isotree.to.sql_+3A_nthreads">nthreads</code></td>
<td>
<p>Number of parallel threads to use.</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li><p> If passing neither 'tree' nor 'table_from', will return a list
of 'character' objects, containing at each entry the SQL statement
for the corresponding tree.
</p>
</li>
<li><p> If passing 'tree', will return a single 'character' object with
the SQL statement representing that tree.
</p>
</li>
<li><p> If passing 'table_from', will return a single 'character' object with
the full SQL select statement for the outlier score, selecting the columns
from the table name passed under 'table_from'.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>library(isotree)
data(iris)
set.seed(1)
iso &lt;- isolation.forest(iris, ntrees=2, sample_size=16, ndim=1, nthreads=1)
sql_forest &lt;- isotree.to.sql(iso, table_from="my_iris_table")
cat(sql_forest)
</code></pre>

<hr>
<h2 id='length.isolation_forest'>Get Number of Trees in Model</h2><span id='topic+length.isolation_forest'></span>

<h3>Description</h3>

<p>Returns the number of trees in an isolation forest model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'isolation_forest'
length(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="length.isolation_forest_+3A_x">x</code></td>
<td>
<p>An isolation forest model, as returned by function <a href="#topic+isolation.forest">isolation.forest</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The number of trees in the model, as an integer.
</p>

<hr>
<h2 id='predict.isolation_forest'>Predict method for Isolation Forest</h2><span id='topic+predict.isolation_forest'></span>

<h3>Description</h3>

<p>Predict method for Isolation Forest
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'isolation_forest'
predict(
  object,
  newdata,
  type = "score",
  square_mat = ifelse(type == "kernel", TRUE, FALSE),
  refdata = NULL,
  use_reference_points = TRUE,
  nthreads = min(object$nthreads, RhpcBLASctl::get_num_cores()),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.isolation_forest_+3A_object">object</code></td>
<td>
<p>An Isolation Forest object as returned by <a href="#topic+isolation.forest">isolation.forest</a>.</p>
</td></tr>
<tr><td><code id="predict.isolation_forest_+3A_newdata">newdata</code></td>
<td>
<p>A 'data.frame', 'data.table', 'tibble', 'matrix', or sparse matrix (from package 'Matrix' or 'SparseM',
CSC/dgCMatrix supported for outlierness, distance, kernels; CSR/dgRMatrix supported for outlierness and imputations)
for which to predict outlierness, distance, kernels, or imputations of missing values.
</p>
<p>If 'newdata' is sparse and one wants to obtain the outlier score or average depth or tree
numbers, it's highly recommended to pass it in CSC ('dgCMatrix') format as it will be much faster
when the number of trees or rows is large.</p>
</td></tr>
<tr><td><code id="predict.isolation_forest_+3A_type">type</code></td>
<td>
<p>Type of prediction to output. Options are:
</p>

<ul>
<li><p> '&quot;score&quot;' for the standardized outlier score - for isolation-based metrics (the default), values
closer to 1 indicate more outlierness, while values
closer to 0.5 indicate average outlierness, and close to 0 more averageness (harder to isolate).
For all scoring metrics, higher values indicate more outlierness.
</p>
</li>
<li><p> '&quot;avg_depth&quot;' for  the non-standardized average isolation depth or density or log-density. For 'scoring_metric=&quot;density&quot;',
will output the geometric mean instead. See the documentation for 'scoring_metric' for more details
about the calculations for density-based metrics.
For all scoring metrics, higher values indicate less outlierness.
</p>
</li>
<li><p> '&quot;dist&quot;' for approximate pairwise or between-points distances (must pass more than 1 row) - these are
standardized in the same way as outlierness, values closer to zero indicate nearer points,
closer to one further away points, and closer to 0.5 average distance.
To make this computation faster, it is highly recommended to build a node indexer with
<a href="#topic+isotree.build.indexer">isotree.build.indexer</a> (with 'with_distances=TRUE') before calling this function.
</p>
</li>
<li><p> '&quot;avg_sep&quot;' for the non-standardized average separation depth.
To make this computation faster, it is highly recommended to build a node indexer with
<a href="#topic+isotree.build.indexer">isotree.build.indexer</a> (with 'with_distances=TRUE') before calling this function.
</p>
</li>
<li><p> '&quot;kernel&quot;' for pairwise or between-points isolation kernel calculations (also known as
proximity matrix), which denotes the fraction of trees in which two observations end up in
the same terminal node. This is typically not as good quality as the separation distance, but
it's much faster to calculate, and has other potential uses - for example, this &quot;kernel&quot; can
be used as an estimate of the correlations between residuals for a generalized least-squares
regression, for which distance might not be as appropirate.
Note that building an indexer will not speed up kernel/proximity
calculations unless it has reference points. This calculation can be sped up significantly
by setting reference points in the model object through <a href="#topic+isotree.set.reference.points">isotree.set.reference.points</a>,
and it's highly recommended to do so if this calculation is going to be performed repeatedly.
</p>
</li>
<li><p> '&quot;kernel_raw&quot;' for the isolation kernel or proximity matrix, but having as output the
number of trees instead of the fraction of total trees.
</p>
</li>
<li><p> '&quot;tree_num&quot;' for the terminal node number for each tree - if choosing this option,
will return a list containing both the average isolation depth and the terminal node numbers, under entries
&lsquo;avg_depth' and 'tree_num', respectively. If this calculation is going to be perform frequently, it&rsquo;s recommended to
build node indices through <a href="#topic+isotree.build.indexer">isotree.build.indexer</a>.
</p>
</li>
<li><p> '&quot;tree_depths&quot;' for the non-standardized isolation depth or expected isolation depth or density
or log-density for each tree (note that they will not include range penalties from 'penalize_range=TRUE').
See the documentation for 'scoring_metric' for more details about the calculations for density-based metrics.
</p>
</li>
<li><p> '&quot;impute&quot;' for imputation of missing values in 'newdata'.
</p>
</li></ul>
</td></tr>
<tr><td><code id="predict.isolation_forest_+3A_square_mat">square_mat</code></td>
<td>
<p>When passing 'type' = '&quot;dist' or '&quot;avg_sep&quot;' or '&quot;kernel&quot;' or '&quot;kernel_raw&quot;'
with no 'refdata', whether to return a full square matrix (returned as a numeric 'matrix' object) or
just its upper-triangular part (returned as a 'dist' object and compatible with functions such as 'hclust'),
in which the entry for pair (i,j) with 1 &lt;= i &lt; j &lt;= n is located at position
p(i, j) = ((i - 1) * (n - i/2) + j - i).
</p>
<p>Ignored when not predicting distance/separation/kernels or when passing 'refdata' or 'use_reference_points=TRUE' plus having reference points.</p>
</td></tr>
<tr><td><code id="predict.isolation_forest_+3A_refdata">refdata</code></td>
<td>
<p>If passing this and calculating distances or average separation depths or kernels, will calculate distances
between each point in 'newdata' and each point in 'refdata', outputing a matrix in which points in 'newdata'
correspond to rows and points in 'refdata' correspond to columns. Must be of the same type as 'newdata' (e.g.
'data.frame', 'matrix', 'dgCMatrix', etc.). If this is not passed, and type is '&quot;dist&quot;'
or '&quot;avg_sep&quot;' or '&quot;kernel&quot;' or '&quot;kernel_raw&quot;', will calculate pairwise distances/separation between the points in 'newdata'.
</p>
<p>Note that, if 'refdata' is passed and and the model object has an indexer with reference points
added (through <a href="#topic+isotree.set.reference.points">isotree.set.reference.points</a>), those reference points will be ignored for the
calculation.</p>
</td></tr>
<tr><td><code id="predict.isolation_forest_+3A_use_reference_points">use_reference_points</code></td>
<td>
<p>When the model object has an indexer with reference points (which can be added through
<a href="#topic+isotree.set.reference.points">isotree.set.reference.points</a>) and passing 'type=&quot;dist&quot;' or '&quot;avg_sep&quot;' or '&quot;kernel&quot;' or '&quot;kernel_raw&quot;', whether to calculate the distances/kernels from 'newdata' to those reference
points instead of the pairwise distances between points in 'newdata'.
</p>
<p>This is ignored when passing 'refdata' or when the model object does not contain an indexer
or the indexer does not contain reference points.</p>
</td></tr>
<tr><td><code id="predict.isolation_forest_+3A_nthreads">nthreads</code></td>
<td>
<p>Number of parallel threads to use. <b>Note:</b> for better performance, it's recommended to set the number of
threads to the number of <b>physical</b> CPU cores, which in a typical desktop CPU, corresponds to half the number of threads
(see details for more information).
</p>
<p>Shorthand for best performance: 'nthreads = RhpcBLASctl::get_num_cores()'</p>
</td></tr>
<tr><td><code id="predict.isolation_forest_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The standardized outlier score for isolation-based metrics is calculated according to the
original paper's formula:
<code class="reqn">  2^{ - \frac{\bar{d}}{c(n)}  }  </code>, where
<code class="reqn">\bar{d}</code> is the average depth under each tree at which an observation
becomes isolated (a remainder is extrapolated if the actual terminal node is not isolated),
and <code class="reqn">c(n)</code> is the expected isolation depth if observations were uniformly random
(see references under <a href="#topic+isolation.forest">isolation.forest</a> for details). The actual calculation
of <code class="reqn">c(n)</code> however differs from the paper as this package uses more exact procedures
for calculation of harmonic numbers.
</p>
<p>For density-based matrics, see the documentation for 'scoring_metric' in <a href="#topic+isolation.forest">isolation.forest</a> for
details about the score calculations.
</p>
<p>The distribution of outlier scores for isolation-based metrics should be centered around 0.5, unless
using non-random splits (parameters 'prob_pick_avg_gain', 'prob_pick_pooled_gain', 'prob_pick_full_gain', 'prob_pick_dens')
and/or range penalizations, or having distributions which are too skewed. For 'scoring_metric=&quot;density&quot;',
most of the values should be negative, and while zero can be used as a natural score threshold,
the scores are unlikely to be centered around zero.
</p>
<p>The more threads that are set for the model, the higher the memory requirement will be as each
thread will allocate an array with one entry per row (outlierness) or combination (distance),
with an exception being calculation of distances/kernels to reference points, which do not do this.
</p>
<p>For multi-threaded predictions on many rows, it is recommended to set the number of threads
to the number of physical cores of the CPU rather than the number of logical cores, as it
will typically have better performance that way. Assuming a typical x86-64 desktop CPU,
this typically involves dividing the number of threads by 2 - for example:
'model$nthreads &lt;- RhpcBLASctl::get_num_cores()'
</p>
<p>Outlierness predictions for sparse data will be much slower than for dense data. Not recommended to pass
sparse matrices unless they are too big to fit in memory.
</p>
<p>Note that after loading a serialized object from 'isolation.forest' through 'readRDS' or 'load',
if it was constructed with 'lazy_serialization=FALSE' it will only de-serialize the underlying
C++ object upon running 'predict', 'print', or 'summary', so the
first run will  be slower, while subsequent runs will be faster as the C++ object will already be in-memory.
This does not apply when using 'lazy_serialization=TRUE'.
</p>
<p>In order to save memory when fitting and serializing models, the functionality for outputting
terminal node numbers will generate index mappings on the fly for all tree nodes, even if passing only
1 row, so it's only recommended for batch predictions. If this type of prediction is desired, it can
be sped up by building an index of terminal nodes through <a href="#topic+isotree.build.indexer">isotree.build.indexer</a>, which will avoid
having to recompute these every time.
</p>
<p>The outlier scores/depth predict functionality is optimized for making predictions on one or a
few rows at a time - for making large batches of predictions, it might be faster to use the
option 'output_score=TRUE' in 'isolation.forest'.
</p>
<p>When making predictions on CSC matrices with many rows using multiple threads, there
can be small differences between runs due to roundoff error.
</p>
<p>When imputing missing values, the input may contain new columns (i.e. not present when the model was fitted),
which will be output as-is.
</p>
<p>If passing 'type=&quot;dist&quot;' or 'type=&quot;avg_sep&quot;', by default, it will do the calculation through a procedure
that counts steps as observations are passed down the trees, which is especially slow and
not recommended for more than a few thousand observations. If this calculation is going to be
called repeatedly and/or it is going to be called for a large number of rows, it's highly
recommended to build node distance indexes beforehand through <a href="#topic+isotree.build.indexer">isotree.build.indexer</a> with
option 'with_distances=TRUE', as then the computation will be done based on terminal node
indices instead, which is a much faster procedure. If distance calculations are all going to be performed
with respect to a fixed set of points, it's highly recommended to set those points as references
through <a href="#topic+isotree.set.reference.points">isotree.set.reference.points</a>.
</p>
<p>If using 'assume_full_distr=FALSE' (not recommended to use such option), distance predictions with
and without an indexer will differ slightly due to differences in what they count towards
&quot;additional&quot; observations in the calculation.
</p>


<h3>Value</h3>

<p>The requested prediction type, which can be: </p>

<ul>
<li><p> A numeric vector with one entry per row in 'newdata' (for output types '&quot;score&quot;' and '&quot;avg_depth&quot;').
</p>
</li>
<li><p> An integer matrix with number of rows matching to rows in 'newdata' and number of columns matching
to the number of trees in the model, indicating the terminal node number under each tree for each
observation, with trees as columns, for output type '&quot;tree_num&quot;'.
</p>
</li>
<li><p> A numeric matrix with rows matching to those in 'newdata' and one column per tree in the
model, for output type '&quot;tree_depths&quot;'.
</p>
</li>
<li><p> A numeric square matrix or 'dist' object which consists of a vector with the upper triangular
part of a square matrix,
(for output types '&quot;dist&quot;', '&quot;avg_sep&quot;', '&quot;kernel&quot;', '&quot;kernel_raw&quot;'; with no 'refdata' and no reference points or
'use_reference_points=FALSE').
</p>
</li>
<li><p> A numeric matrix with points in 'newdata' as rows and points in 'refdata' as columns
(for output types '&quot;dist&quot;', '&quot;avg_sep&quot;', '&quot;kernel&quot;', '&quot;kernel_raw&quot;'; with 'refdata').
</p>
</li>
<li><p> A numeric matrix with points in 'newdata' as rows and reference points set through
<a href="#topic+isotree.set.reference.points">isotree.set.reference.points</a> as columns
(for output types '&quot;dist&quot;', '&quot;avg_sep&quot;', '&quot;kernel&quot;', '&quot;kernel_raw&quot;'; with 'use_reference_points=TRUE' and no 'refdata').
</p>
</li>
<li><p> The same type as the input 'newdata' (for output type '&quot;impute&quot;').</p>
</li></ul>



<h3>Model serving considerations</h3>

<p>If the model is built with 'nthreads&gt;1', the prediction function <a href="#topic+predict.isolation_forest">predict.isolation_forest</a> will
use OpenMP for parallelization. In a linux setup, one usually has GNU's &quot;gomp&quot; as OpenMP as backend, which
will hang when used in a forked process - for example, if one tries to call this prediction function from
'RestRserve', which uses process forking for parallelization, it will cause the whole application to freeze.
A potential fix in these cases is to pass 'nthreads=1' to 'predict', or to set the number of threads to 1 in
the model object (e.g. 'model$nthreads &lt;- 1L' or calling <a href="#topic+isotree.set.nthreads">isotree.set.nthreads</a>), or to compile this library
without OpenMP (requires manually altering the 'Makevars' file), or to use a non-GNU OpenMP backend (such
as LLVM's 'libomp'. This should not be an issue when using this library normally in e.g. an RStudio session.
</p>
<p>The R objects that hold the models contain heap-allocated C++ objects which do not map to R types and
which thus do not survive serializations the same way R objects do.
In order to make model objects serializable (i.e. usable with 'save', 'saveRDS', and similar), the package
offers two mechanisms: (a) a 'lazy_serialization' option which uses the ALTREP system as a workaround,
by defining classes with serialization methods but without datapointer methods (see the docs for
'lazy_serialization' for more info); (b) a more theoretically correct way in which raw bytes are produced
alongside the model and from which the C++ objects can be reconstructed. When using the lazy serialization
system, C++ objects are restored automatically on load and the serialized bytes then discarded, but this is
not the case when using the serialized bytes approach. For model serving, one would usually want to drop these
serialized bytes after having loaded a model through 'readRDS' or similar (note that reconstructing the
C++ object will first require calling <a href="#topic+isotree.restore.handle">isotree.restore.handle</a>, which is done automatically when
calling 'predict' and similar), as they can increase memory usage by a large amount. These redundant raw bytes
can be dropped as follows: 'model$cpp_objects$model$ser &lt;- NULL' (and an additional
'model$cpp_objects$imputer$ser &lt;- NULL' when using 'build_imputer=TRUE', and 'model$cpp_objects$indexer$ser &lt;- NULL'
when building a node indexer). After that, one might want to force garbage
collection through 'gc()'.
</p>
<p>Usually, for serving purposes, one wants a setup as minimalistic as possible (e.g. smaller docker images).
This library can be made smaller and faster to compile by disabling some features - particularly,
the library will by default build with support for calculation of aggregated metrics (such as
standard deviations) in 'long double' precision (an extended precision type), which is a functionality
that's unlikely to get used (default is not to use this type as it is slower, and calculations done in
the &lsquo;predict' function do not use it for anything). Support for &rsquo;long double' can be disable at compile
time by setting up an environment variable 'NO_LONG_DOUBLE' before installing the
package (e.g. by issuing command 'Sys.setenv(&quot;NO_LONG_DOUBLE&quot; = &quot;1&quot;)' before 'install.packages').
</p>


<h3>See Also</h3>

<p><a href="#topic+isolation.forest">isolation.forest</a> <a href="#topic+isotree.restore.handle">isotree.restore.handle</a> <a href="#topic+isotree.build.indexer">isotree.build.indexer</a> <a href="#topic+isotree.set.reference.points">isotree.set.reference.points</a>
</p>

<hr>
<h2 id='print.isolation_forest'>Print summary information from Isolation Forest model</h2><span id='topic+print.isolation_forest'></span>

<h3>Description</h3>

<p>Displays the most general characteristics of an isolation forest model (same as 'summary').
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'isolation_forest'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.isolation_forest_+3A_x">x</code></td>
<td>
<p>An Isolation Forest model as produced by function 'isolation.forest'.</p>
</td></tr>
<tr><td><code id="print.isolation_forest_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that after loading a serialized object from 'isolation.forest' through 'readRDS' or 'load',
when using 'lazy_serialization=FALSE', it will only de-serialize the underlying C++ object upon running 'predict',
'print', or 'summary', so the first run will be slower, while subsequent runs will be faster as the C++ object
will already be in-memory. This does not apply when using 'lazy_serialization=TRUE'.
</p>


<h3>Value</h3>

<p>The same model that was passed as input.
</p>


<h3>See Also</h3>

<p><a href="#topic+isolation.forest">isolation.forest</a>
</p>

<hr>
<h2 id='summary.isolation_forest'>Print summary information from Isolation Forest model</h2><span id='topic+summary.isolation_forest'></span>

<h3>Description</h3>

<p>Displays the most general characteristics of an isolation forest model (same as 'print').
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'isolation_forest'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="summary.isolation_forest_+3A_object">object</code></td>
<td>
<p>An Isolation Forest model as produced by function 'isolation.forest'.</p>
</td></tr>
<tr><td><code id="summary.isolation_forest_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that after loading a serialized object from 'isolation.forest' through 'readRDS' or 'load',
when using 'lazy_serialization=FALSE', it will only de-serialize the underlying C++ object upon running 'predict',
'print', or 'summary', so the first run will be slower, while subsequent runs will be faster as the C++ object
will already be in-memory. This does not apply when using 'lazy_serialization=TRUE'.
</p>


<h3>Value</h3>

<p>No return value.
</p>


<h3>See Also</h3>

<p><a href="#topic+isolation.forest">isolation.forest</a>
</p>

<hr>
<h2 id='variable.names.isolation_forest'>Get Variable Names for Isolation Forest Model</h2><span id='topic+variable.names.isolation_forest'></span>

<h3>Description</h3>

<p>Returns the names of the input data columns / variables to which an
isolation forest model was fitted.
</p>
<p>If the data did not have column names, it will make them up as &quot;column_1..N&quot;.
</p>
<p>Note that columns will always be reordered so that numeric columns come first, followed by
categorical columns.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'isolation_forest'
variable.names(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="variable.names.isolation_forest_+3A_object">object</code></td>
<td>
<p>An isolation forest model, as returned by function <a href="#topic+isolation.forest">isolation.forest</a>.</p>
</td></tr>
<tr><td><code id="variable.names.isolation_forest_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector containing the column / variable names.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
