<!DOCTYPE html><html><head><title>Help for package clustrd</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {clustrd}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#clustrd-package'>
<p>Methods for Joint Dimension Reduction and Clustering</p></a></li>
<li><a href='#bribery'><p>Bribery cases in Russia</p></a></li>
<li><a href='#clusmca'>
<p>Joint  dimension  reduction  and  clustering  of  categorical  data.</p></a></li>
<li><a href='#cluspca'>
<p>Joint  dimension  reduction  and  clustering  of  continuous  data.</p></a></li>
<li><a href='#cluspcamix'>
<p>Joint  dimension  reduction  and  clustering  of mixed-type data.</p></a></li>
<li><a href='#cmc'><p>Contraceptive Choice in Indonesia</p></a></li>
<li><a href='#diamond'><p>Diamond Stone Pricing</p></a></li>
<li><a href='#global_bootclus'>
<p>Global stabiliy assessment of Joint Dimension Reduction and Clustering methods by bootstrapping.</p></a></li>
<li><a href='#hsq'><p>Humor Styles</p></a></li>
<li><a href='#local_bootclus'>
<p>Cluster-wise stability assessment of Joint Dimension Reduction and Clustering methods by bootstrapping.</p></a></li>
<li><a href='#macro'><p>Economic Indicators of 20 OECD countries for 1999</p></a></li>
<li><a href='#mybond'><p>James Bond films</p></a></li>
<li><a href='#plot.clusmca'>
<p>Plotting function for <code>clusmca()</code> output.</p></a></li>
<li><a href='#plot.cluspca'>
<p>Plotting function for <code>cluspca()</code> output.</p></a></li>
<li><a href='#plot.cluspcamix'>
<p>Plotting function for <code>cluspcamix()</code> output.</p></a></li>
<li><a href='#tuneclus'>
<p>Cluster quality assessment for a range of clusters and dimensions.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Methods for Joint Dimension Reduction and Clustering</td>
</tr>
<tr>
<td>Description:</td>
<td>A class of methods that combine dimension reduction and clustering of continuous, categorical or mixed-type data (Markos, Iodice D'Enza and van de Velden 2019; &lt;<a href="https://doi.org/10.18637%2Fjss.v091.i10">doi:10.18637/jss.v091.i10</a>&gt;). For continuous data, the package contains implementations of factorial K-means (Vichi and Kiers 2001; &lt;<a href="https://doi.org/10.1016%2FS0167-9473%2800%2900064-5">doi:10.1016/S0167-9473(00)00064-5</a>&gt;) and reduced K-means (De Soete and Carroll 1994; &lt;<a href="https://doi.org/10.1007%2F978-3-642-51175-2_24">doi:10.1007/978-3-642-51175-2_24</a>&gt;); both methods that combine principal component analysis with K-means clustering. For categorical data, the package provides MCA K-means (Hwang, Dillon and Takane 2006; &lt;<a href="https://doi.org/10.1007%2Fs11336-004-1173-x">doi:10.1007/s11336-004-1173-x</a>&gt;), i-FCB (Iodice D'Enza and Palumbo 2013, &lt;<a href="https://doi.org/10.1007%2Fs00180-012-0329-x">doi:10.1007/s00180-012-0329-x</a>&gt;) and Cluster Correspondence Analysis (van de Velden, Iodice D'Enza and Palumbo 2017; &lt;<a href="https://doi.org/10.1007%2Fs11336-016-9514-0">doi:10.1007/s11336-016-9514-0</a>&gt;), which combine multiple correspondence analysis with K-means. For mixed-type data, it provides mixed Reduced K-means and mixed Factorial K-means (van de Velden, Iodice D'Enza and Markos 2019; &lt;<a href="https://doi.org/10.1002%2Fwics.1456">doi:10.1002/wics.1456</a>&gt;), which combine PCA for mixed-type data with K-means.</td>
</tr>
<tr>
<td>Version:</td>
<td>1.4.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-07-16</td>
</tr>
<tr>
<td>Author:</td>
<td>Angelos Markos [aut, cre], Alfonso Iodice D'Enza [aut], Michel van de Velden [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Angelos Markos &lt;amarkos@gmail.com&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>ggplot2, grid</td>
</tr>
<tr>
<td>Imports:</td>
<td>rARPACK, tibble, corpcor, GGally, fpc, cluster, dplyr, plyr,
ggrepel, ca, stats</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-07-16 20:58:12 UTC; amarkos</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-07-16 23:20:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='clustrd-package'>
Methods for Joint Dimension Reduction and Clustering
</h2><span id='topic+clustrd-package'></span><span id='topic+clustrd'></span>

<h3>Description</h3>

<p>A class of methods that combine dimension reduction and clustering of continuous, categorical or mixed-type data (Markos, Iodice D'Enza and van de Velden 2019; &lt;DOI:10.18637/jss.v091.i10&gt;). For continuous data, the package contains implementations of factorial K-means (Vichi and Kiers 2001; &lt;DOI:10.1016/S0167-9473(00)00064-5&gt;) and reduced K-means (De Soete and Carroll 1994; &lt;DOI:10.1007/978-3-642-51175-2_24&gt;); both methods that combine principal component analysis with K-means clustering. For categorical data, the package provides MCA K-means (Hwang, Dillon and Takane 2006; &lt;DOI:10.1007/s11336-004-1173-x&gt;), i-FCB (Iodice D'Enza and Palumbo 2013, &lt;DOI:10.1007/s00180-012-0329-x&gt;) and Cluster Correspondence Analysis (van de Velden, Iodice D'Enza and Palumbo 2017; &lt;DOI:10.1007/s11336-016-9514-0&gt;), which combine multiple correspondence analysis with K-means. For mixed-type data, it provides mixed Reduced K-means and mixed Factorial K-means (van de Velden, Iodice D'Enza and Markos 2019; &lt;DOI:10.1002/wics.1456&gt;), which combine PCA for mixed-type data with K-means.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> clustrd</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.3.6-2</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2019-10-28</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL-3</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Angelos Markos [aut, cre],
Alfonso Iodice D' Enza [aut],
Michel van de Velden [aut]
</p>


<h3>References</h3>

<p>Markos, A., Iodice D'Enza, A., &amp; van de Velden, M. (2019). Beyond Tandem Analysis: Joint Dimension Reduction and Clustering in R. <em>Journal of Statistical Software</em>, <em>91</em>(10), 1&ndash;24. doi:10.18637/jss.v091.i10.
</p>

<hr>
<h2 id='bribery'>Bribery cases in Russia</h2><span id='topic+bribery'></span>

<h3>Description</h3>

<p>The data set refers to a collection of 55 articles on bribery cases from central Russian newspapers 1999-2000 (Mirkin, 2005). The variables reflect the following five-fold structure of bribery situations: two interacting sides - the office and the client, their interaction, the corrupt service rendered, and the environment in which it all occurs. These structural aspects can be characterized by 11 variables that have been manually recovered from the newspaper articles.</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("bribery")</code></pre>


<h3>Format</h3>

<p>A data frame with 55 observations on 11 categorical variables.
</p>

<dl>
<dt><code>Of</code></dt><dd><p>Type of Office</p>
</dd>
<dt><code>Cl</code></dt><dd><p>Level of Client</p>
</dd>
<dt><code>Serv</code></dt><dd><p>Type of service: obstruction of justice, favours, cover-up, change of category, extortion of money for rendering free services</p>
</dd>
<dt><code>Occ</code></dt><dd><p>Frequency of occurrence</p>
</dd>
<dt><code>Init</code></dt><dd><p>Who initiated the bribery act</p>
</dd>
<dt><code>Brib</code></dt><dd><p>Bribe Level in $</p>
</dd>
<dt><code>Typ</code></dt><dd><p>Type of corruption</p>
</dd>
<dt><code>Net</code></dt><dd><p>Corruption network</p>
</dd>
<dt><code>Con</code></dt><dd><p>Condition of corruption</p>
</dd>
<dt><code>Bran</code></dt><dd><p>Branch at which the corrupt service occurred</p>
</dd>
<dt><code>Pun</code></dt><dd><p>Punishment</p>
</dd>
</dl>



<h3>References</h3>

<p>Mirkin, B. (2005). <em>Clustering for data mining: a data recovery approach</em>. Chapman and Hall/CRC.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(bribery)
</code></pre>

<hr>
<h2 id='clusmca'>
Joint  dimension  reduction  and  clustering  of  categorical  data.
</h2><span id='topic+clusmca'></span><span id='topic+print.clusmca'></span><span id='topic+summary.clusmca'></span><span id='topic+fitted.clusmca'></span>

<h3>Description</h3>

<p>This function implements MCA K-means (Hwang, Dillon and Takane, 2006), i-FCB (Iodice D' Enza and Palumbo, 2013) and Cluster Correspondence Analysis (van de Velden, Iodice D' Enza and Palumbo, 2017). The methods combine variants of Correspondence Analysis for dimension reduction with K-means for clustering.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clusmca(data, nclus, ndim, method=c("clusCA","iFCB","MCAk"),
alphak = .5, nstart = 100, smartStart = NULL, gamma = TRUE, 
inboot = FALSE, seed = NULL)

## S3 method for class 'clusmca'
print(x, ...)

## S3 method for class 'clusmca'
summary(object, ...)

## S3 method for class 'clusmca'
fitted(object, mth = c("centers", "classes"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clusmca_+3A_data">data</code></td>
<td>
<p>Dataset with categorical variables</p>
</td></tr>
<tr><td><code id="clusmca_+3A_nclus">nclus</code></td>
<td>
<p>Number of clusters (nclus = 1 returns the MCA solution; see Details)</p>
</td></tr>
<tr><td><code id="clusmca_+3A_ndim">ndim</code></td>
<td>
<p>Dimensionality of the solution</p>
</td></tr>
<tr><td><code id="clusmca_+3A_method">method</code></td>
<td>
<p>Specifies the method. Options are MCAk for MCA K-means, iFCB for Iterative Factorial Clustering of Binary variables and clusCA for Cluster Correspondence Analysis (default = <code>"clusCA")</code></p>
</td></tr>
<tr><td><code id="clusmca_+3A_alphak">alphak</code></td>
<td>
<p>Non-negative scalar to adjust for the relative importance of MCA (<code>alphak = 1</code>) and K-means (<code>alphak = 0</code>) in the solution (default = .5). Works only in combination with <code>method = "MCAk"</code></p>
</td></tr>
<tr><td><code id="clusmca_+3A_nstart">nstart</code></td>
<td>
<p>Number of random starts (default = 100)</p>
</td></tr>
<tr><td><code id="clusmca_+3A_smartstart">smartStart</code></td>
<td>
<p>If <code>NULL</code> then a random cluster membership vector is generated. Alternatively, a cluster membership vector can be provided as a starting solution</p>
</td></tr>
<tr><td><code id="clusmca_+3A_gamma">gamma</code></td>
<td>
<p>Scaling parameter that leads to similar spread in the object and variable scores (default = <code>TRUE</code>)</p>
</td></tr>
<tr><td><code id="clusmca_+3A_seed">seed</code></td>
<td>
<p>An integer that is used as argument by <code>set.seed()</code> for offsetting the random number generator when <code>smartStart = NULL</code>. The default value is NULL.</p>
</td></tr>
<tr><td><code id="clusmca_+3A_inboot">inboot</code></td>
<td>
<p>Used internally in the bootstrap functions to perform bootstrapping on the indicator matrix.</p>
</td></tr>
<tr><td><code id="clusmca_+3A_x">x</code></td>
<td>
<p>For the <code>print</code> method, a class of <code>clusmca</code></p>
</td></tr>
<tr><td><code id="clusmca_+3A_object">object</code></td>
<td>
<p>For the <code>summary</code> method, a class of <code>clusmca</code></p>
</td></tr>
<tr><td><code id="clusmca_+3A_mth">mth</code></td>
<td>
<p>For the <code>fitted</code> method, a character string that specifies the type of fitted value to return: <code>"centers"</code> for the observations center vector, or <code>"class"</code> for the observations cluster membership value</p>
</td></tr>
<tr><td><code id="clusmca_+3A_...">...</code></td>
<td>
<p>Not used</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For the K-means part, the algorithm of Hartigan-Wong is used by default.
</p>
<p>The hidden <code>print</code> and <code>summary</code> methods print out some key components of an object of class <code>clusmca</code>. 
</p>
<p>The hidden <code>fitted</code> method returns cluster fitted values. If method is <code>"classes"</code>, this is a vector of cluster membership (the cluster component of the &quot;clusmca&quot; object). If method is <code>"centers"</code>, this is a matrix where each row is the cluster center for the observation. The rownames of the matrix are the cluster membership values.
</p>
<p>When <code>nclus</code> = 1 the function returns the MCA solution with objects in principal and variables in standard coordinates. <code>plot(object)</code> shows the corresponding asymmetric biplot.
</p>


<h3>Value</h3>

<table>
<tr><td><code>obscoord</code></td>
<td>
<p>Object scores</p>
</td></tr>
<tr><td><code>attcoord</code></td>
<td>
<p>Attribute scores</p>
</td></tr>
<tr><td><code>centroid</code></td>
<td>
<p>Cluster centroids</p>
</td></tr>
<tr><td><code>cluster</code></td>
<td>
<p>Cluster membership</p>
</td></tr>
<tr><td><code>criterion</code></td>
<td>
<p>Optimal value of the objective criterion</p>
</td></tr>
<tr><td><code>size</code></td>
<td>
<p>The number of objects in each cluster</p>
</td></tr>
<tr><td><code>nstart</code></td>
<td>
<p>A copy of <code>nstart</code> in the return object</p>
</td></tr>
<tr><td><code>odata</code></td>
<td>
<p>A copy of <code>data</code> in the return object</p>
</td></tr>
</table>


<h3>References</h3>

<p>Hwang, H., Dillon, W. R., and Takane, Y. (2006). An extension of multiple correspondence analysis for identifying heterogenous subgroups of respondents. <em>Psychometrika</em>, 71, 161-171.
</p>
<p>Iodice D'Enza, A., and Palumbo, F. (2013). Iterative factor clustering of binary data. <em>Computational Statistics</em>, <em>28</em>(2), 789-807.
</p>
<p>van de Velden M., Iodice D' Enza, A., and Palumbo, F. (2017).  Cluster correspondence analysis. <em>Psychometrika</em>, <em>82</em>(1), 158-185.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cluspca">cluspca</a></code>, <code><a href="#topic+cluspcamix">cluspcamix</a></code>, <code><a href="#topic+tuneclus">tuneclus</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(cmc)
# Preprocessing: values of wife's age and number of children were categorized 
# into three groups based on quartiles
cmc$W_AGE = ordered(cut(cmc$W_AGE, c(16,26,39,49), include.lowest = TRUE))
levels(cmc$W_AGE) = c("16-26","27-39","40-49") 
cmc$NCHILD = ordered(cut(cmc$NCHILD, c(0,1,4,17), right = FALSE))
levels(cmc$NCHILD) = c("0","1-4","5 and above")

#Cluster Correspondence Analysis solution with 3 clusters in 2 dimensions 
#after 10 random starts
outclusCA = clusmca(cmc, 3, 2, method = "clusCA", nstart = 10, seed = 1234)
outclusCA
#Scatterplot (dimensions 1 and 2)
plot(outclusCA)

#MCA K-means solution with 3 clusters in 2 dimensions after 10 random starts
outMCAk = clusmca(cmc, 3, 2, method = "MCAk", nstart = 10, seed = 1234)
outMCAk
#Scatterplot (dimensions 1 and 2)
plot(outMCAk)

#nclus = 1 just gives the MCA solution
#outMCA = clusmca(cmc, 1, 2)
#outMCA
#Scatterplot (dimensions 1 and 2) 
#asymmetric biplot with scaling gamma = TRUE
#plot(outMCA)
</code></pre>

<hr>
<h2 id='cluspca'>
Joint  dimension  reduction  and  clustering  of  continuous  data.
</h2><span id='topic+cluspca'></span><span id='topic+print.cluspca'></span><span id='topic+summary.cluspca'></span><span id='topic+fitted.cluspca'></span>

<h3>Description</h3>

<p>This function implements Factorial K-means (Vichi and Kiers, 2001) and Reduced K-means (De Soete and Carroll, 1994), as well as a compromise version of these two methods. The methods combine Principal Component Analysis for dimension reduction with K-means for clustering.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cluspca(data, nclus, ndim, alpha = NULL, method = c("RKM","FKM"), 
center = TRUE, scale = TRUE, rotation = "none", nstart = 100, 
smartStart = NULL, seed = NULL)

## S3 method for class 'cluspca'
print(x, ...)

## S3 method for class 'cluspca'
summary(object, ...)

## S3 method for class 'cluspca'
fitted(object, mth = c("centers", "classes"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cluspca_+3A_data">data</code></td>
<td>
<p>Dataset with metric variables</p>
</td></tr>
<tr><td><code id="cluspca_+3A_nclus">nclus</code></td>
<td>
<p>Number of clusters (nclus = 1 returns the PCA solution</p>
</td></tr>
<tr><td><code id="cluspca_+3A_ndim">ndim</code></td>
<td>
<p>Dimensionality of the solution</p>
</td></tr>
<tr><td><code id="cluspca_+3A_method">method</code></td>
<td>
<p>Specifies the method. Options are RKM for reduced K-means and FKM for factorial K-means (default = <code>"RKM"</code>)</p>
</td></tr>
<tr><td><code id="cluspca_+3A_alpha">alpha</code></td>
<td>
<p>Adjusts for the relative importance of RKM and FKM in the objective function; <code>alpha</code> = 0.5 leads to reduced K-means, <code>alpha</code> = 0 to factorial K-means, and <code>alpha</code> = 1 reduces to the tandem approach (PCA followed by K-means)</p>
</td></tr>
<tr><td><code id="cluspca_+3A_center">center</code></td>
<td>
<p>A logical value indicating whether the variables should be shifted to be zero centered (default = <code>TRUE)</code></p>
</td></tr>
<tr><td><code id="cluspca_+3A_scale">scale</code></td>
<td>
<p>A logical value indicating whether the variables should be scaled to have unit variance before the analysis takes place (default = <code>TRUE)</code></p>
</td></tr>
<tr><td><code id="cluspca_+3A_rotation">rotation</code></td>
<td>
<p>Specifies the method used to rotate the factors. Options are <code>none</code> for no rotation, <code>varimax</code> for varimax rotation with Kaiser normalization and <code>promax</code> for promax rotation (default = <code>"none"</code>)</p>
</td></tr>
<tr><td><code id="cluspca_+3A_nstart">nstart</code></td>
<td>
<p>Number of starts (default = 100)</p>
</td></tr>
<tr><td><code id="cluspca_+3A_smartstart">smartStart</code></td>
<td>
<p>If <code>NULL</code> then a random cluster membership vector is generated. Alternatively, a cluster membership vector can be provided as a starting solution</p>
</td></tr>
<tr><td><code id="cluspca_+3A_seed">seed</code></td>
<td>
<p>An integer that is used as argument by <code>set.seed()</code> for offsetting the random number generator when smartStart = NULL. The default value is NULL.</p>
</td></tr>
<tr><td><code id="cluspca_+3A_x">x</code></td>
<td>
<p>For the <code>print</code> method, a class of <code>clusmca</code></p>
</td></tr>
<tr><td><code id="cluspca_+3A_object">object</code></td>
<td>
<p>For the <code>summary</code> method, a class of <code>clusmca</code></p>
</td></tr>
<tr><td><code id="cluspca_+3A_mth">mth</code></td>
<td>
<p>For the <code>fitted</code> method, a character string that specifies the type of fitted value to return: <code>"centers"</code> for the observations center vector, or <code>"class"</code> for the observations cluster membership value</p>
</td></tr>
<tr><td><code id="cluspca_+3A_...">...</code></td>
<td>
<p>Not used</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For the K-means part, the algorithm of Hartigan-Wong is used by default.
</p>
<p>The hidden <code>print</code> and <code>summary</code> methods print out some key components of an object of class <code>cluspca</code>. 
</p>
<p>The hidden <code>fitted</code> method returns cluster fitted values. If method is <code>"classes"</code>, this is a vector of cluster membership (the cluster component of the &quot;cluspca&quot; object). If method is <code>"centers"</code>, this is a matrix where each row is the cluster center for the observation. The rownames of the matrix are the cluster membership values.
</p>
<p>When <code>nclus</code> = 1 the function returns the PCA solution and <code>plot(object)</code> shows the corresponding biplot.
</p>


<h3>Value</h3>

<table>
<tr><td><code>obscoord</code></td>
<td>
<p>Object scores</p>
</td></tr>
<tr><td><code>attcoord</code></td>
<td>
<p>Variable scores</p>
</td></tr>
<tr><td><code>centroid</code></td>
<td>
<p>Cluster centroids</p>
</td></tr>
<tr><td><code>cluster</code></td>
<td>
<p>Cluster membership</p>
</td></tr>
<tr><td><code>criterion</code></td>
<td>
<p>Optimal value of the objective function</p>
</td></tr>
<tr><td><code>size</code></td>
<td>
<p>The number of objects in each cluster</p>
</td></tr>
<tr><td><code>scale</code></td>
<td>
<p>A copy of <code>scale</code> in the return object</p>
</td></tr>
<tr><td><code>center</code></td>
<td>
<p>A copy of <code>center</code> in the return object</p>
</td></tr>
<tr><td><code>nstart</code></td>
<td>
<p>A copy of <code>nstart</code> in the return object</p>
</td></tr>
<tr><td><code>odata</code></td>
<td>
<p>A copy of <code>data</code> in the return object</p>
</td></tr>
</table>


<h3>References</h3>

<p>De Soete, G., and Carroll, J. D. (1994). K-means clustering in a low-dimensional Euclidean space. In Diday E. et al. (Eds.), <em>New Approaches in Classification and Data Analysis</em>, Heidelberg: Springer, 212-219.
</p>
<p>Vichi, M., and Kiers, H.A.L. (2001). Factorial K-means analysis for two-way data. <em>Computational Statistics and Data Analysis</em>, 37, 49-64.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+clusmca">clusmca</a></code>, <code><a href="#topic+cluspcamix">cluspcamix</a></code>, <code><a href="#topic+tuneclus">tuneclus</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Reduced K-means with 3 clusters in 2 dimensions after 10 random starts
data(macro)
outRKM = cluspca(macro, 3, 2, method = "RKM", rotation = "varimax", scale = FALSE, nstart = 10)
summary(outRKM)
#Scatterplot (dimensions 1 and 2) and cluster description plot
plot(outRKM, cludesc = TRUE)

#Factorial K-means with 3 clusters in 2 dimensions 
#with a Reduced K-means starting solution
data(macro)
outFKM = cluspca(macro, 3, 2, method = "FKM", rotation = "varimax", 
scale = FALSE, smartStart = outRKM$cluster)
outFKM
#Scatterplot (dimensions 1 and 2) and cluster description plot
plot(outFKM, cludesc = TRUE)

#To get the Tandem approach (PCA(SVD) + K-means)
outTandem = cluspca(macro, 3, 2, alpha = 1, seed = 1234)
plot(outTandem)

#nclus = 1 just gives the PCA solution 
#outPCA = cluspca(macro, 1, 2)
#outPCA
#Scatterplot (dimensions 1 and 2) 
#plot(outPCA)
</code></pre>

<hr>
<h2 id='cluspcamix'>
Joint  dimension  reduction  and  clustering  of mixed-type data.
</h2><span id='topic+cluspcamix'></span><span id='topic+print.cluspcamix'></span><span id='topic+summary.cluspcamix'></span><span id='topic+fitted.cluspcamix'></span>

<h3>Description</h3>

<p>This function implements clustering and dimension reduction for mixed-type variables, i.e., categorical and metric (see, Yamamoto &amp; Hwang, 2014; van de Velden, Iodice D'Enza, &amp; Markos 2019; Vichi, Vicari, &amp; Kiers, 2019). This framework includes Mixed Reduced K-means and Mixed Factorial K-means, as well as a compromise of these two methods. The methods combine Principal Component Analysis of mixed-data for dimension reduction with K-means for clustering.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cluspcamix(data, nclus, ndim, method=c("mixedRKM", "mixedFKM"), 
center = TRUE, scale = TRUE, alpha=NULL, rotation="none", 
nstart = 100, smartStart=NULL, seed=NULL, inboot = FALSE)

## S3 method for class 'cluspcamix'
print(x, ...)

## S3 method for class 'cluspcamix'
summary(object, ...)

## S3 method for class 'cluspcamix'
fitted(object, mth = c("centers", "classes"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cluspcamix_+3A_data">data</code></td>
<td>
<p>Dataset with categorical and metric variables</p>
</td></tr>
<tr><td><code id="cluspcamix_+3A_nclus">nclus</code></td>
<td>
<p>Number of clusters (nclus = 1 returns the PCAMIX solution)</p>
</td></tr>
<tr><td><code id="cluspcamix_+3A_ndim">ndim</code></td>
<td>
<p>Dimensionality of the solution</p>
</td></tr>
<tr><td><code id="cluspcamix_+3A_method">method</code></td>
<td>
<p>Specifies the method. Options are mixedRKM for mixed reduced K-means and mixedFKM for mixed factorial K-means (default = <code>"mixedRKM"</code>)</p>
</td></tr>
<tr><td><code id="cluspcamix_+3A_center">center</code></td>
<td>
<p>A logical value indicating whether the variables should be shifted to be zero centered (default = <code>TRUE)</code></p>
</td></tr>
<tr><td><code id="cluspcamix_+3A_scale">scale</code></td>
<td>
<p>A logical value indicating whether the variables should be scaled to have unit variance before the analysis takes place (default = <code>TRUE)</code></p>
</td></tr>
<tr><td><code id="cluspcamix_+3A_alpha">alpha</code></td>
<td>
<p>Adjusts for the relative importance of Mixed RKM and Mixed FKM in the objective function; <code>alpha</code> = 0.5 leads to mixed reduced K-means, <code>alpha</code> = 0 to mixed factorial K-means, and <code>alpha</code> = 1 reduces to the tandem approach (PCAMIX followed by K-means)</p>
</td></tr>
<tr><td><code id="cluspcamix_+3A_rotation">rotation</code></td>
<td>
<p>Specifies the method used to rotate the factors. Options are <code>none</code> for no rotation, <code>varimax</code> for varimax rotation with Kaiser normalization and <code>promax</code> for promax rotation (default = <code>"none"</code>)</p>
</td></tr>
<tr><td><code id="cluspcamix_+3A_nstart">nstart</code></td>
<td>
<p>Number of random starts (default = 100)</p>
</td></tr>
<tr><td><code id="cluspcamix_+3A_smartstart">smartStart</code></td>
<td>
<p>If <code>NULL</code> then a random cluster membership vector is generated. Alternatively, a cluster membership vector can be provided as a starting solution</p>
</td></tr>
<tr><td><code id="cluspcamix_+3A_seed">seed</code></td>
<td>
<p>An integer that is used as argument by <code>set.seed()</code> for offsetting the random number generator when <code>smartStart = NULL</code>. The default value is NULL.</p>
</td></tr>
<tr><td><code id="cluspcamix_+3A_inboot">inboot</code></td>
<td>
<p>Used internally in the bootstrap functions to perform bootstrapping on the indicator matrix.</p>
</td></tr>
<tr><td><code id="cluspcamix_+3A_x">x</code></td>
<td>
<p>For the <code>print</code> method, a class of <code>cluspcamix</code></p>
</td></tr>
<tr><td><code id="cluspcamix_+3A_object">object</code></td>
<td>
<p>For the <code>summary</code> method, a class of <code>cluspcamix</code></p>
</td></tr>
<tr><td><code id="cluspcamix_+3A_mth">mth</code></td>
<td>
<p>For the <code>fitted</code> method, a character string that specifies the type of fitted value to return: <code>"centers"</code> for the observations center vector, or <code>"class"</code> for the observations cluster membership value</p>
</td></tr>
<tr><td><code id="cluspcamix_+3A_...">...</code></td>
<td>
<p>Not used</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For the K-means part, the algorithm of Hartigan-Wong is used by default.
</p>
<p>The hidden <code>print</code> and <code>summary</code> methods print out some key components of an object of class <code>cluspcamix</code>. 
</p>
<p>The hidden <code>fitted</code> method returns cluster fitted values. If method is <code>"classes"</code>, this is a vector of cluster membership (the cluster component of the &quot;cluspcamix&quot; object). If method is <code>"centers"</code>, this is a matrix where each row is the cluster center for the observation. The rownames of the matrix are the cluster membership values.
</p>
<p>When <code>nclus</code> = 1 the function returns the solution of PCAMIX and <code>plot(object)</code> shows the corresponding biplot.
</p>


<h3>Value</h3>

<table>
<tr><td><code>obscoord</code></td>
<td>
<p>Object scores</p>
</td></tr>
<tr><td><code>attcoord</code></td>
<td>
<p>Variable scores</p>
</td></tr>
<tr><td><code>centroid</code></td>
<td>
<p>Cluster centroids</p>
</td></tr>
<tr><td><code>cluster</code></td>
<td>
<p>Cluster membership</p>
</td></tr>
<tr><td><code>criterion</code></td>
<td>
<p>Optimal value of the objective criterion</p>
</td></tr>
<tr><td><code>size</code></td>
<td>
<p>The number of objects in each cluster</p>
</td></tr>
<tr><td><code>scale</code></td>
<td>
<p>A copy of <code>scale</code> in the return object</p>
</td></tr>
<tr><td><code>center</code></td>
<td>
<p>A copy of <code>center</code> in the return object</p>
</td></tr>
<tr><td><code>nstart</code></td>
<td>
<p>A copy of <code>nstart</code> in the return object</p>
</td></tr>
<tr><td><code>odata</code></td>
<td>
<p>A copy of <code>data</code> in the return object</p>
</td></tr>
</table>


<h3>References</h3>

<p>van de Velden, M., Iodice D'Enza, A., &amp; Markos, A. (2019).
Distance-based clustering of mixed data. <em>Wiley Interdisciplinary Reviews: Computational Statistics</em>, e1456.
</p>
<p>Vichi, M., Vicari, D., &amp; Kiers, H.A.L. (2019). Clustering and
dimension reduction for mixed variables. <em>Behaviormetrika</em>. doi:10.1007/s41237-018-0068-6.
</p>
<p>Yamamoto, M., &amp; Hwang, H. (2014). A general formulation of
cluster analysis with dimension reduction and subspace 
separation. <em>Behaviormetrika</em>, <em>41</em>, 115-129.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cluspca">cluspca</a></code>, <code><a href="#topic+clusmca">clusmca</a></code>, <code><a href="#topic+tuneclus">tuneclus</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(diamond)
#Mixed Reduced K-means solution with 3 clusters in 2 dimensions 
#after 10 random starts
outmixedRKM = cluspcamix(diamond, 3, 2, method = "mixedRKM", nstart = 10, seed = 1234)
outmixedRKM 
#A graph with the categories and a biplot of the continuous variables (dimensions 1 and 2)
plot(outmixedRKM)

#Tandem analysis: PCAMIX or FAMD followed by K-means solution 
#with 3 clusters in 2 dimensions after 10 random starts 
outTandem = cluspcamix(diamond, 3, 2, alpha = 1, nstart = 10, seed = 1234)
outTandem
#Scatterplot (dimensions 1 and 2)
plot(outTandem)

#nclus = 1 just gives the PCAMIX or FAMD solution
#outPCAMIX = cluspcamix(diamond, 1, 2)
#outPCAMIX
#Biplot (dimensions 1 and 2) 
#plot(outPCAMIX)
</code></pre>

<hr>
<h2 id='cmc'>Contraceptive Choice in Indonesia
</h2><span id='topic+cmc'></span>

<h3>Description</h3>

<p>Data of married women in Indonesia who were not pregnant (or did not know they were pregnant) at the time of the survey. The dataset contains demographic and socio-economic characteristics of the women along with their preferred method of contraception (no use, long-term methods, short-term methods).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(cmc)</code></pre>


<h3>Format</h3>

<p>A data frame containing 1,437 observations on the following 10 variables.
</p>

<dl>
<dt><code>W_AGE</code></dt><dd><p>wife's age in years.</p>
</dd>
<dt><code>W_EDU</code></dt><dd><p>ordered factor indicating wife's education, with levels <code>"low"</code>, <code>"2"</code>, <code>"3"</code> and <code>"high".</code></p>
</dd>
<dt><code>H_EDU</code></dt><dd><p>ordered factor indicating wife's education, with levels <code>"low"</code>, <code>"2"</code>, <code>"3"</code> and <code>"high".</code></p>
</dd>
<dt><code>NCHILD</code></dt><dd><p>number of children.</p>
</dd>
<dt><code>W_REL</code></dt><dd><p>factor indicating wife's religion, with levels <code>"non-Islam"</code> and <code>"Islam"</code>.</p>
</dd>
<dt><code>W_WORK</code></dt><dd><p>factor indicating if the wife is working.</p>
</dd>
<dt><code>H_OCC</code></dt><dd><p>factor indicating husband's occupation, with levels <code>"1"</code>, <code>"2"</code>, <code>"3"</code> and <code>"4"</code>. The labels are not known.</p>
</dd>
<dt><code>SOL</code></dt><dd><p>ordered factor indicating the standard of living index with levels <code>"low"</code>, <code>"2"</code>, <code>"3"</code> and <code>"high"</code>.</p>
</dd>
<dt><code>MEDEXP</code></dt><dd><p>factor indicating media exposure, with levels <code>"good"</code> and <code>"not good"</code>.</p>
</dd>
<dt><code>CM</code></dt><dd><p>factor indicating the contraceptive method used, with levels <code>"no-use"</code>, <code>"long-term"</code> and <code>"short-term"</code>.</p>
</dd>
</dl>



<h3>Source</h3>

<p>This dataset is part of the 1987 National Indonesia Contraceptive Prevalence Survey and was created by Tjen-Sien Lim. It has been taken from the UCI Machine Learning Repository at <a href="http://archive.ics.uci.edu/ml/">http://archive.ics.uci.edu/ml/</a>.
</p>


<h3>References</h3>

<p>Lim, T.-S., Loh, W.-Y. &amp; Shih, Y.-S. (1999). A Comparison of Prediction Accuracy, Complexity, and Training Time of Thirty-three Old and New Classification Algorithms. <em>Machine Learning</em>, <em>40</em>(3), 203-228.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(cmc)
</code></pre>

<hr>
<h2 id='diamond'>Diamond Stone Pricing</h2><span id='topic+diamond'></span>

<h3>Description</h3>

<p>Data on 308 diamond stones sold in Singapore. The main attributes are diamond weight, colour, clarity, certification body and price in Singapore $. The weight of a diamond stone is indicated in terms of carat units. Since stones may be divided into 3 clusters due to their size, namely small (less than 0.5 carats), medium (0.5 to less than 1 carat) and large (1 carat and over), following Chu (2001), three binary variables have been built representing the three caratage ranges, and three quantitative variables (denoted Small, Medium, Large) have been derived by multiplying such binary variables by carats. So, the &quot;Small&quot; variable has nonzero values (i.e., the carat values) only for the smallest diamonds (less than 0.5 carats), and likewise for the other two variables. Thus, these variables are weighted binary variables. The colour of a diamond is graded from D (completely colourless), E, F, G, ..., to I (almost colorless). Clarity refers to the diamond's internal and external imperfections. Clarity is graded on a scale from IF (internally flawless), to very very slightly imperfect (VVS1 or VVS2), and very slightly imperfect, VS1 or VS2. Three certification bodies were used: New York based Gemmological Institute of America (GIA), Antwerp based International Gemmological Institute (IGI) and Hoge Raad Voor Diamant (HRD).  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(diamond)</code></pre>


<h3>Format</h3>

<p>A data frame with 308 observations on the following 7 variables.
</p>

<dl>
<dt><code>Small</code></dt><dd><p>weighted binary variable with nonzero values (i.e., the carat values) for diamonds with less than 0.5 carats.</p>
</dd>
<dt><code>Medium</code></dt><dd><p>weighted binary variable with nonzero values (i.e., the carat values) for diamonds from 0.5 to less than 1 carat.</p>
</dd>
<dt><code>Large</code></dt><dd><p>weighted binary variable with nonzero values (i.e., the carat values) for diamonds from 1 carat and over.</p>
</dd>
<dt><code>Colour</code></dt><dd><p>the color of the diamond with a factor with levels (D, E, F, G, H, I).</p>
</dd>
<dt><code>Clarity</code></dt><dd><p>the clarity of the diamond with a factor with levels (IF, VVS1, VVS2, VS1, VS2).</p>
</dd>
<dt><code>Certification</code></dt><dd><p>the certification body with a factor with levels (GIA, IGI, HRD).</p>
</dd>
<dt><code>Price</code></dt><dd><p>the price of a diamond in Singapore $.</p>
</dd>
</dl>



<h3>References</h3>

<p>Chu, S. (2001). Pricing the C's of Diamond Stones, <em>Journal of Statistics Education</em>, <em>9</em>(2).</p>

<hr>
<h2 id='global_bootclus'>
Global stabiliy assessment of Joint Dimension Reduction and Clustering methods by bootstrapping.
</h2><span id='topic+global_bootclus'></span>

<h3>Description</h3>

<p>Runs joint dimension and clustering algorithms repeatedly for different numbers of clusters on bootstrap replica of the original data and returns corresponding cluster assignments, and cluster agreement indices comparing pairs of partitions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>global_bootclus(data, nclusrange = 3:4, ndim = NULL, 
method = c("RKM","FKM","mixedRKM","mixedFKM","clusCA","MCAk","iFCB"), 
nboot = 10, alpha = NULL, alphak = NULL, center = TRUE, 
scale = TRUE, nstart = 100, smartStart = NULL, seed = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="global_bootclus_+3A_data">data</code></td>
<td>
<p>Continuous, Categorical ot Mixed data set</p>
</td></tr>
<tr><td><code id="global_bootclus_+3A_nclusrange">nclusrange</code></td>
<td>
<p>An integer or an integer vector with the number of clusters or a range of numbers of clusters (should be greater than one)</p>
</td></tr>
<tr><td><code id="global_bootclus_+3A_ndim">ndim</code></td>
<td>
<p>Dimensionality of the solution; if <code>NULL</code> it is set to nclus - 1</p>
</td></tr>
<tr><td><code id="global_bootclus_+3A_method">method</code></td>
<td>
<p>Specifies the method. Options are RKM for Reduced K-means, FKM for Factorial K-means, mixedRKM for Mixed Reduced K-means, mixedFKM for Mixed Factorial K-means, MCAk for MCA K-means, iFCB for Iterative Factorial Clustering of Binary variables and clusCA for Cluster Correspondence Analysis.</p>
</td></tr>
<tr><td><code id="global_bootclus_+3A_nboot">nboot</code></td>
<td>
<p>Number of bootstrap pairs of partitions</p>
</td></tr>
<tr><td><code id="global_bootclus_+3A_alpha">alpha</code></td>
<td>
<p>Adjusts for the relative importance of (mixed) RKM and FKM in the objective function; <code>alpha = 1</code> reduces to PCA/PCAMIX, <code>alpha = 0.5</code> to (mixed) reduced K-means, and <code>alpha = 0</code> to (mixed) factorial K-means</p>
</td></tr>  
<tr><td><code id="global_bootclus_+3A_alphak">alphak</code></td>
<td>
<p>Non-negative scalar to adjust for the relative importance of MCA (<code>alphak = 1</code>) and K-means (<code>alphak = 0</code>) in the solution (default = .5). Works only in combination with <code>method = "MCAk"</code></p>
</td></tr>
<tr><td><code id="global_bootclus_+3A_center">center</code></td>
<td>
<p>A logical value indicating whether the metric variables should be shifted to be zero centered (default = <code>TRUE)</code></p>
</td></tr>
<tr><td><code id="global_bootclus_+3A_scale">scale</code></td>
<td>
<p>A logical value indicating whether the metric variables should be scaled to have unit variance before the analysis takes place (default = <code>TRUE)</code></p>
</td></tr>
<tr><td><code id="global_bootclus_+3A_nstart">nstart</code></td>
<td>
<p>Number of random starts (default = 100)</p>
</td></tr>
<tr><td><code id="global_bootclus_+3A_smartstart">smartStart</code></td>
<td>
<p>If <code>NULL</code> then a random cluster membership vector is generated. Alternatively, a cluster membership vector can be provided as a starting solution</p>
</td></tr>
<tr><td><code id="global_bootclus_+3A_seed">seed</code></td>
<td>
<p>An integer that is used as argument by <code>set.seed()</code> for offsetting the random number generator when <code>smartStart = NULL</code>. The default value is NULL.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithm for assessing global cluster stability is similar to that in Dolnicar and Leisch (2010) and can be summarized in three steps:
</p>
<p><em>Step 1. Resampling:</em> Draw bootstrap samples S_i and T_i of size <em>n</em> from the data and use the original data, X, as evaluation set E_i = X. Apply the clustering method of choice to S_i and T_i and obtain C^S_i and C^T_i.
</p>
<p><em>Step 2. Mapping:</em> Assign each observation x_i to the closest centers of C^S_i and C^T_i using Euclidean distance, resulting in partitions C^XS_i and C^XT_i, where C^XS_i is the partition of the original data, X, predicted from clustering bootstrap sample S_i (same for T_i and C^XT_i).
</p>
<p><em>Step 3. Evaluation:</em> Use the Adjusted Rand Index (ARI, Hubert &amp; Arabie, 1985) or the Measure of Concordance (MOC, Pfitzner 2008) as measure of agreement and stability.
</p>
<p>Inspect the distributions of ARI/MOC to assess the global reproducibility of the clustering solutions.
</p>
<p>While nboot = 100 is recommended, smaller run numbers could give quite informative results as well, if computation times become too high.
</p>
<p>Note that the stability of a clustering solution is assessed, but stability is not the only important validity criterion - clustering solutions obtained by very inflexible clustering methods may be stable but not valid, as discussed in Hennig (2007).
</p>


<h3>Value</h3>

<table>
<tr><td><code>nclusrange</code></td>
<td>
<p>An integer or an integer vector with the number of clusters or a range of numbers of clusters</p>
</td></tr>
<tr><td><code>clust1</code></td>
<td>
<p>Partitions, C^XS_i of the original data, X, predicted from clustering bootstrap sample S_i (see Details)</p>
</td></tr>
<tr><td><code>clust2</code></td>
<td>
<p>Partitions, C^XT_i of the original data, X, predicted from clustering bootstrap sample T_i (see Details)</p>
</td></tr>
<tr><td><code>index1</code></td>
<td>
<p>Indices of the original data rows in bootstrap sample S_i</p>
</td></tr>
<tr><td><code>index2</code></td>
<td>
<p>Indices of the original data rows in bootstrap sample T_i</p>
</td></tr>
<tr><td><code>rand</code></td>
<td>
<p>Adjusted Rand Index values</p>
</td></tr>
<tr><td><code>moc</code></td>
<td>
<p>Measure of Concordance values</p>
</td></tr>
</table>


<h3>References</h3>

<p>Hennig, C. (2007). Cluster-wise assessment of cluster stability. <em>Computational Statistics and Data Analysis</em>, <em>52</em>, 258-271.
</p>
<p>Pfitzner, D., Leibbrandt, R., &amp; Powers, D. (2009). Characterization and evaluation of similarity measures for pairs of clusterings. <em>Knowledge and Information Systems</em>, <em>19</em>(3), 361-394.
</p>
<p>Dolnicar, S., &amp; Leisch, F. (2010). Evaluation of structure and reproducibility of cluster solutions using the bootstrap. <em>Marketing Letters</em>, <em>21</em>(1), 83-101.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+local_bootclus">local_bootclus</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## 3 bootstrap replicates and nstart = 1 for speed in example,
## use at least 20 replicates for real applications
data(diamond)
boot_mixedRKM = global_bootclus(diamond[,-7], nclusrange = 3:4,
method = "mixedRKM", nboot = 3, nstart = 1, seed = 1234)

boxplot(boot_mixedRKM$rand, xlab = "Number of clusters", ylab =
"adjusted Rand Index")

## 5 bootstrap replicates and nstart = 10 for speed in example,
## use more for real applications
#data(macro)
#boot_RKM = global_bootclus(macro, nclusrange = 2:5,
#method = "RKM", nboot = 5, nstart = 10, seed = 1234)

#boxplot(boot_RKM$rand, xlab = "Number of clusters", ylab =
#"adjusted Rand Index")

## 5 bootstrap replicates and nstart = 1 for speed in example,
## use more for real applications
#data(bribery)
#boot_cluCA = global_bootclus(bribery, nclusrange = 2:5, 
#method = "clusCA", nboot = 5, nstart = 1, seed = 1234)

#boxplot(boot_cluCA$rand, xlab = "Number of clusters", ylab =
#"adjusted Rand Index")
</code></pre>

<hr>
<h2 id='hsq'>Humor Styles</h2><span id='topic+hsq'></span>

<h3>Description</h3>

<p>The dataset was collected with an interactive online version of the Humor Styles Questionnaire (HSQ) which assesses four independent ways in which people express and appreciate humor (Martin et al. 2003): affiliative (items with prefix AF), defined as the benign uses of humor to enhance one's relationships with others; self-enhancing (SE), indicating uses of humor to enhance the self; aggressive (AG), the use of humor to enhance the self at the expense of others; self-defeating (SD), the use of humor to enhance relationships at the expense of oneself. The main part of the questionnaire consisted of 32 statements rated from 1 to 5 according to the respondents' level of agreement. The number of respondents is 993.</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("hsq")</code></pre>


<h3>Format</h3>

<p>A data frame with 993 observations on 32 Likert-type variables (statements) with 5 response categories, ranging from 1 (strong agreement) to 5 (strong disagreement).
</p>

<dl>
<dt><code>AF1</code></dt><dd><p>I usually don't laugh or joke around much with other people</p>
</dd>
<dt><code>SE2</code></dt><dd><p>If I am feeling depressed, I can usually cheer myself up with humor</p>
</dd>
<dt><code>AG3</code></dt><dd><p>If someone makes a mistake, I will often tease them about it</p>
</dd>
<dt><code>SD4</code></dt><dd><p>I let people laugh at me or make fun at my expense more than I should</p>
</dd>
<dt><code>AF5</code></dt><dd><p>I don't have to work very hard at making other people laugh - I seem to be a naturally humorous person</p>
</dd>
<dt><code>SE6</code></dt><dd><p>Even when I'm by myself, I'm often amused by the absurdities of life</p>
</dd>
<dt><code>AG7</code></dt><dd><p>People are never offended or hurt by my sense of humor</p>
</dd>
<dt><code>SD8</code></dt><dd><p>I will often get carried away in putting myself down if it makes my family or friends laugh</p>
</dd>
<dt><code>AF9</code></dt><dd><p>I rarely make other people laugh by telling funny stories about myself</p>
</dd>
<dt><code>SE10</code></dt><dd><p>If I am feeling upset or unhappy I usually try to think of something funny about the situation to make myself feel better</p>
</dd>
<dt><code>AG11</code></dt><dd><p>When telling jokes or saying funny things, I am usually not very concerned about how other people are taking it</p>
</dd>
<dt><code>SD12</code></dt><dd><p>I often try to make people like or accept me more by saying something funny about my own weaknesses, blunders, or faults</p>
</dd>
<dt><code>AF13</code></dt><dd><p>I laugh and joke a lot with my closest friends</p>
</dd>
<dt><code>SE14</code></dt><dd><p>My humorous outlook on life keeps me from getting overly upset or depressed about things</p>
</dd>
<dt><code>AG15</code></dt><dd><p>I do not like it when people use humor as a way of criticizing or putting someone down</p>
</dd>
<dt><code>SD16</code></dt><dd><p>I don't often say funny things to put myself down</p>
</dd>
<dt><code>AF17</code></dt><dd><p>I usually don't like to tell jokes or amuse people</p>
</dd>
<dt><code>SE18</code></dt><dd><p>If I'm by myself and I'm feeling unhappy, I make an effort to think of something funny to cheer myself up</p>
</dd>
<dt><code>AG19</code></dt><dd><p>Sometimes I think of something that is so funny that I can't stop myself from saying it, even if it is not appropriate for the situation</p>
</dd>
<dt><code>SD20</code></dt><dd><p>I often go overboard in putting myself down when I am making jokes or trying to be funny</p>
</dd>
<dt><code>AF21</code></dt><dd><p>I enjoy making people laugh</p>
</dd>
<dt><code>SE22</code></dt><dd><p>If I am feeling sad or upset, I usually lose my sense of humor</p>
</dd>
<dt><code>AG23</code></dt><dd><p>I never participate in laughing at others even if all my friends are doing it</p>
</dd>
<dt><code>SD24</code></dt><dd><p>When I am with friends or family, I often seem to be the one that other people make fun of or joke about</p>
</dd>
<dt><code>AF25</code></dt><dd><p>I don't often joke around with my friends</p>
</dd>
<dt><code>SE26</code></dt><dd><p>It is my experience that thinking about some amusing aspect of a situation is often a very effective way of coping with problems</p>
</dd>
<dt><code>AG27</code></dt><dd><p>If I don't like someone, I often use humor or teasing to put them down</p>
</dd>
<dt><code>SD28</code></dt><dd><p>If I am having problems or feeling unhappy, I often cover it up by joking around, so that even my closest friends don't know how I really feel</p>
</dd>
<dt><code>AF29</code></dt><dd><p>I usually can't think of witty things to say when I'm with other people</p>
</dd>
<dt><code>SE30</code></dt><dd><p>I don't need to be with other people to feel amused - I can usually find things to laugh about even when I'm by myself</p>
</dd>
<dt><code>AG31</code></dt><dd><p>Even if something is really funny to me, I will not laugh or joke about it if someone will be offended</p>
</dd>
<dt><code>SD32</code></dt><dd><p>Letting others laugh at me is my way of keeping my friends and family in good spirits</p>
</dd>
</dl>



<h3>References</h3>

<p>Martin, R. A., Puhlik-Doris, P., Larsen, G., Gray, J., &amp; Weir, K. (2003). Individual differences in uses of humor and their relation to psychological well-being: Development of the Humor Styles Questionnaire. <em>Journal of Research in Personality</em>, <em>37</em>(1), 48-75.</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(hsq)
</code></pre>

<hr>
<h2 id='local_bootclus'>
Cluster-wise stability assessment of Joint Dimension Reduction and Clustering methods by bootstrapping.
</h2><span id='topic+local_bootclus'></span>

<h3>Description</h3>

<p>Assessment of the cluster-wise stability of a joint dimension and clustering method. The data is resampled using bootstrapping and the Jaccard similarities of the original clusters to the most similar clusters in the resampled data are computed. The mean over these similarities is used as an index of the stability of a cluster. The method is similar to the one described in Hennig (2007). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>local_bootclus(data, nclus, ndim = NULL, 
method = c("RKM","FKM","mixedRKM","mixedFKM","clusCA","MCAk","iFCB"), 
scale = TRUE, center= TRUE, alpha = NULL, nstart=100, 
nboot=10, alphak = .5, seed = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="local_bootclus_+3A_data">data</code></td>
<td>
<p>Continuous, Categorical ot Mixed data set</p>
</td></tr>
<tr><td><code id="local_bootclus_+3A_nclus">nclus</code></td>
<td>
<p>Number of clusters</p>
</td></tr>
<tr><td><code id="local_bootclus_+3A_ndim">ndim</code></td>
<td>
<p>Dimensionality of the solution</p>
</td></tr>
<tr><td><code id="local_bootclus_+3A_method">method</code></td>
<td>
<p>Specifies the method. Options are RKM for Reduced K-means, FKM for Factorial K-means, mixedRKM for Mixed Reduced K-means, mixedFKM for Mixed Factorial K-means, MCAk for MCA K-means, iFCB for Iterative Factorial Clustering of Binary variables and clusCA for Cluster Correspondence Analysis.</p>
</td></tr>
<tr><td><code id="local_bootclus_+3A_scale">scale</code></td>
<td>
<p>A logical value indicating whether the metric variables should be scaled to have unit variance before the analysis takes place (default = <code>TRUE)</code></p>
</td></tr>
<tr><td><code id="local_bootclus_+3A_center">center</code></td>
<td>
<p>A logical value indicating whether the metric variables should be shifted to be zero centered (default = <code>TRUE)</code></p>
</td></tr>
<tr><td><code id="local_bootclus_+3A_alpha">alpha</code></td>
<td>
<p>Adjusts for the relative importance of (mixed) RKM and FKM in the objective function; <code>alpha = 1</code> reduces to PCA/PCAMIX, <code>alpha = 0.5</code> to (mixed) reduced K-means, and <code>alpha = 0</code> to (mixed) factorial K-means</p>
</td></tr>
<tr><td><code id="local_bootclus_+3A_nstart">nstart</code></td>
<td>
<p>Number of random starts (default = 100)</p>
</td></tr>
<tr><td><code id="local_bootclus_+3A_nboot">nboot</code></td>
<td>
<p>Number of bootstrap pairs of partitions</p>
</td></tr>
<tr><td><code id="local_bootclus_+3A_alphak">alphak</code></td>
<td>
<p>Non-negative scalar to adjust for the relative importance of MCA (<code>alphak = 1</code>) and K-means (<code>alphak = 0</code>) in the solution (default = .5). Works only in combination with <code>method = "MCAk"</code></p>
</td></tr>
<tr><td><code id="local_bootclus_+3A_seed">seed</code></td>
<td>
<p>An integer that is used as argument by <code>set.seed()</code> for offsetting the random number generator when <code>smartStart = NULL</code>. The default value is NULL.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithm for assessing local cluster stability is similar to that in Hennig (2007) and can be summarized in three steps:
</p>
<p><em>Step 1. Resampling:</em> Draw bootstrap samples S_i and T_i of size n from the data and use the original data as evaluation set E_i = X. Apply a joint dimension reduction and clustering method to S_i and T_i and obtain C^S_i and C^T_i.
</p>
<p><em>Step 2. Mapping</em>: Assign each observation x_i to the closest centers of C^S_i and C^T_i using Euclidean distance, resulting in partitions C^XS_i and C^XT_i.
</p>
<p><em>Step 3. Evaluation</em>: Obtain the maximum Jaccard agreement between each original cluster C_k and each one of the two bootstrap clusters, C_^k'XS_i and C_^k'XT_i as measure of agreement and stability, and take the average of each pair.
</p>
<p>Inspect the distributions of the maximum Jaccard coefficients to assess the cluster level (local) stability of the solution.
</p>
<p>Here are some guidelines for interpretation. Generally, a valid, stable cluster should yield a mean Jaccard similarity value of 0.75 or more. Between 0.6 and 0.75, clusters may be considered as indicating patterns in the data, but which points exactly should belong to these clusters is highly doubtful. Below average Jaccard values of 0.6, clusters should not be trusted. &quot;Highly stable&quot; clusters should yield average Jaccard similarities of 0.85 and above. 
</p>
<p>While B = 100 is recommended, smaller run numbers could give quite informative results as well, if computation times become too high.
</p>
<p>Note that the stability of a cluster is assessed, but stability is not the only important validity criterion - clusters obtained by very inflexible clustering methods may be stable but not valid, as discussed in Hennig (2007).
</p>


<h3>Value</h3>

<table>
<tr><td><code>nclus</code></td>
<td>
<p>An integer with the number of clusters</p>
</td></tr>
<tr><td><code>clust1</code></td>
<td>
<p>Partitions, C^XS_i of the original data, X, predicted from clustering bootstrap sample S_i (see Details)</p>
</td></tr>
<tr><td><code>clust2</code></td>
<td>
<p>Partitions, C^XT_i of the original data, X, predicted from clustering bootstrap sample T_i (see Details)</p>
</td></tr>
<tr><td><code>index1</code></td>
<td>
<p>Indices of the original data rows in bootstrap sample S_i</p>
</td></tr>
<tr><td><code>index2</code></td>
<td>
<p>Indices of the original data rows in bootstrap sample T_i</p>
</td></tr>
<tr><td><code>Jaccard</code></td>
<td>
<p>Mean Jaccard similarity values</p>
</td></tr>
</table>


<h3>References</h3>

<p>Hennig, C. (2007). Cluster-wise assessment of cluster stability. <em>Computational Statistics and Data Analysis</em>, <em>52</em>, 258-271.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+global_bootclus">global_bootclus</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## 5 bootstrap replicates and nstart = 10 for speed in example,
## use more for real applications
data(iris)
bootres = local_bootclus(iris[,-5], nclus = 3, ndim = 2,
method = "RKM", nboot = 5, nstart = 1, seed = 1234)

boxplot(bootres$Jaccard, xlab = "cluster number", ylab =
"Jaccard similarity")

## 5 bootstrap replicates and nstart = 5 for speed in example,
## use more for real applications
#data(diamond)
#bootres = local_bootclus(diamond[,-7], nclus = 4, ndim = 3,
#method = "mixedRKM", nboot = 5, nstart = 10, seed = 1234)

#boxplot(bootres$Jaccard, xlab = "cluster number", ylab =
#"Jaccard similarity")

## 5 bootstrap replicates and nstart = 1 for speed in example,
## use more for real applications
#data(bribery)
#bootres = local_bootclus(bribery, nclus = 5, ndim = 4,
#method = "clusCA", nboot = 10, nstart = 1, seed = 1234)

#boxplot(bootres$Jaccard, xlab = "cluster number", ylab =
#"Jaccard similarity")
</code></pre>

<hr>
<h2 id='macro'>Economic Indicators of 20 OECD countries for 1999</h2><span id='topic+macro'></span>

<h3>Description</h3>

<p>Data on the macroeconomic performance of national economies of 20 countries, members of the OECD (September 1999). The performance of the economies reflects the interaction of six main economic indicators (percentage change from the previous year): gross domestic product (GDP), leading indicator (LI), unemployment rate (UR), interest rate (IR), trade balance (TB), net national savings (NNS).</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(macro)</code></pre>


<h3>Format</h3>

<p>A data frame with 20 observations on the following 6 variables.
</p>

<dl>
<dt><code>GDP</code></dt><dd><p>numeric</p>
</dd>
<dt><code>LI</code></dt><dd><p>numeric</p>
</dd>
<dt><code>UR</code></dt><dd><p>numeric</p>
</dd>
<dt><code>IR</code></dt><dd><p>numeric</p>
</dd>
<dt><code>TB</code></dt><dd><p>numeric</p>
</dd>
<dt><code>NNS</code></dt><dd><p>numeric</p>
</dd>
</dl>



<h3>References</h3>

<p>Vichi, M. &amp; Kiers, H. A. (2001). Factorial k-means analysis for two-way data. <em>Computational Statistics &amp; Data Analysis</em>, <em>37</em>(1), 49-64.</p>

<hr>
<h2 id='mybond'>James Bond films</h2><span id='topic+mybond'></span>

<h3>Description</h3>

<p>The data set refers to 26 James Bond films produced up to 2021, based on 10 film characteristics: 7 continuous (year of release, production budget, box office gross in the USA and worldwide, running time, IMDB average rating, Rotten Tomatoes rating) and 3 categorical (Bond actor, native country of the actor playing the villain, native country of the actor playing the Bond girl). All figures in USD are adjusted for inflation. Most of the data was compiled from the Wikipedia page: <a href="https://en.wikipedia.org/wiki/List_of_James_Bond_films">https://en.wikipedia.org/wiki/List_of_James_Bond_films</a>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("mybond")</code></pre>


<h3>Format</h3>

<p>A data frame with 26 observations on the following 10 variables.
</p>

<dl>
<dt><code>year</code></dt><dd><p>Year of release</p>
</dd>
<dt><code>budget</code></dt><dd><p>Official production budget (in million USD)</p>
</dd>
<dt><code>grossusa</code></dt><dd><p>Box office gross in the USA (in million USD)</p>
</dd>
<dt><code>grosswrld</code></dt><dd><p>Box office gross worldwide (in million USD)</p>
</dd>
<dt><code>rtime</code></dt><dd><p>Running time in minutes</p>
</dd>
<dt><code>IMDB</code></dt><dd><p>IMDB rating</p>
</dd>
<dt><code>rottentomatoes</code></dt><dd><p>Rotten Tomatoes rating</p>
</dd>
<dt><code>actor</code></dt><dd><p>Bond actor</p>
</dd>
<dt><code>villaincnt</code></dt><dd><p>Native country of the actor playing the villain</p>
</dd>
<dt><code>bondgirlcnt</code></dt><dd><p>Native country of the actor playing the Bond girl</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>data(mybond)
</code></pre>

<hr>
<h2 id='plot.clusmca'>
Plotting function for <code>clusmca()</code> output.
</h2><span id='topic+plot.clusmca'></span>

<h3>Description</h3>

<p>Plotting function that creates a scatterplot of the object scores and/or the attribute scores and the cluster centroids. Optionally, the function returns a series of barplots showing the standardized residuals per attribute for each cluster. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'clusmca'
plot(x, dims = c(1,2), what = c(TRUE,TRUE),
cludesc = FALSE, topstdres = 20, objlabs = FALSE, attlabs = NULL, 
subplot = FALSE, max.overlaps=10, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.clusmca_+3A_x">x</code></td>
<td>
<p>Object returned by <code>clusmca()</code></p>
</td></tr>
<tr><td><code id="plot.clusmca_+3A_dims">dims</code></td>
<td>
<p>Numerical vector of length 2 indicating the dimensions to plot on horizontal and vertical axes respectively; default is first dimension horizontal and
second dimension vertical</p>
</td></tr>
<tr><td><code id="plot.clusmca_+3A_what">what</code></td>
<td>
<p>Vector of two logical values specifying the contents of the plots. First entry indicates whether a scatterplot of the objects is displayed in principal coordinates. Second entry indicates whether a scatterplot of the attribute categories is displayed in principal coordinates. Cluster centroids are always displayed. The default is <code>c(TRUE, TRUE)</code> and the resultant plot is a biplot of both objects and attribute categories with gamma-based scaling (see van de Velden et al., 2017)</p>
</td></tr>
<tr><td><code id="plot.clusmca_+3A_cludesc">cludesc</code></td>
<td>
<p>A logical value indicating whether a series of barplots is produced showing the largest (in absolute value) standardized residuals per attribute for each cluster (default = <code>FALSE</code>)</p>
</td></tr>
<tr><td><code id="plot.clusmca_+3A_topstdres">topstdres</code></td>
<td>
<p>Number of largest standardized residuals used to describe each cluster (default = 20). Works only in combination with <code>cludesc = TRUE</code></p>
</td></tr>
<tr><td><code id="plot.clusmca_+3A_objlabs">objlabs</code></td>
<td>
<p>A logical value indicating whether object labels will be plotted; if <code>TRUE</code> row names of the data matrix are used (default = <code>FALSE</code>). Warning: when <code>TRUE</code>, execution time of the plotting function will increase dramatically as the number of objects gets larger</p>
</td></tr>
<tr><td><code id="plot.clusmca_+3A_attlabs">attlabs</code></td>
<td>
<p>Vector of custom attribute labels; if not provided, default labeling is applied</p>
</td></tr>
<tr><td><code id="plot.clusmca_+3A_subplot">subplot</code></td>
<td>
<p>A logical value indicating whether a subplot with the full distribution of the standardized residuals will appear at the bottom left corner of the corresponding plots. Works only in combination with <code>cludesc = TRUE</code></p>
</td></tr>
<tr><td><code id="plot.clusmca_+3A_max.overlaps">max.overlaps</code></td>
<td>
<p>Maximum number of text labels allowed to overlap. Defaults to 10</p>
</td></tr>
<tr><td><code id="plot.clusmca_+3A_...">...</code></td>
<td>
<p>Further arguments to be transferred to <code>clusmca()</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a ggplot2 scatterplot of the solution obtained via <code>clusmca()</code> that can be further customized using the <span class="pkg">ggplot2</span> package. When <code>cludesc = TRUE</code> the function also returns a series of ggplot2 barplots showing the largest (or all) standardized residuals per attribute for each cluster.
</p>


<h3>References</h3>

<p>Hwang, H., Dillon, W. R., and Takane, Y. (2006). An extension of multiple correspondence analysis for identifying heterogenous subgroups of respondents. <em>Psychometrika</em>, 71, 161-171.
</p>
<p>Iodice D'Enza, A., and Palumbo, F. (2013). Iterative factor clustering of binary data. <em>Computational Statistics</em>, <em>28</em>(2), 789-807.
</p>
<p>van de Velden M., Iodice D'Enza, A., and Palumbo, F. (2017).  Cluster correspondence analysis. <em>Psychometrika</em>, <em>82</em>(1), 158-185.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.cluspca">plot.cluspca</a>, <a href="#topic+plot.cluspcamix">plot.cluspcamix</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("mybond")
#Cluster Correspondence Analysis with 3 clusters in 2 dimensions after 10 random starts
outclusCA = clusmca(mybond[,8:10], 3, 2, nstart = 100, seed = 234)
#Save the ggplot2 scatterplot  
map = plot(outclusCA, max.overlaps = 40)$map
#Customization (adding titles)
map + ggtitle(paste("Cluster CA plot of the James bond categorical data: 3 clusters of sizes ", 
                    paste(outclusCA$size, collapse = ", "),sep = "")) + 
    xlab("Dim. 1") + ylab("Dim. 2") + 
    theme(plot.title = element_text(size = 10, face = "bold", hjust = 0.5))
    
data("mybond")
#i-FCB with 3 clusters in 2 dimensions after 10 random starts
outclusCA = clusmca(mybond[,8:10], 3, 2, method = "iFCB", nstart= 10)
#Scatterlot with the observations only (dimensions 1 and 2) 
#and cluster description plots showing the 20 largest std. residuals 
#(with the full distribution showing in subplots)
plot(outclusCA, dim = c(1,2), what = c(TRUE, FALSE), cludesc = TRUE, 
subplot = TRUE)
</code></pre>

<hr>
<h2 id='plot.cluspca'>
Plotting function for <code>cluspca()</code> output.
</h2><span id='topic+plot.cluspca'></span>

<h3>Description</h3>

<p>Plotting function that creates a scatterplot of the objects, a correlation circle of the variables or a biplot of both objects and variables. Optionally, it returns a parallel coordinate plot showing cluster means.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cluspca'
plot(x, dims = c(1, 2), cludesc = FALSE, 
what = c(TRUE,TRUE), attlabs, max.overlaps=10, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.cluspca_+3A_x">x</code></td>
<td>
<p>Object returned by <code>cluspca()</code></p>
</td></tr>
<tr><td><code id="plot.cluspca_+3A_dims">dims</code></td>
<td>
<p>Numerical vector of length 2 indicating the dimensions to plot on horizontal and vertical axes respectively; default is first dimension horizontal and
second dimension vertical</p>
</td></tr>
<tr><td><code id="plot.cluspca_+3A_what">what</code></td>
<td>
<p>Vector of two logical values specifying the contents of the plots. First entry indicates whether a scatterplot of the objects and cluster centroids is displayed and the second entry whether a correlation circle of the variables is displayed. The default is <code>c(TRUE, TRUE)</code> and the resultant plot is a biplot of both objects and variables</p>
</td></tr>
<tr><td><code id="plot.cluspca_+3A_cludesc">cludesc</code></td>
<td>
<p>A logical value indicating if a parallel coordinate plot showing cluster means is produced (default = <code>FALSE)</code></p>
</td></tr>

<tr><td><code id="plot.cluspca_+3A_attlabs">attlabs</code></td>
<td>
<p>Vector of custom attribute labels; if not provided, default labeling is applied</p>
</td></tr>
<tr><td><code id="plot.cluspca_+3A_max.overlaps">max.overlaps</code></td>
<td>
<p>Maximum number of text labels allowed to overlap. Defaults to 10</p>
</td></tr>
<tr><td><code id="plot.cluspca_+3A_...">...</code></td>
<td>
<p>Further arguments to be transferred to <code>cluspca()</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a ggplot2 scatterplot of the solution obtained via <code>cluspca()</code> that can be further customized using the <span class="pkg">ggplot2</span> package. When <code>cludesc = TRUE</code> 
the function also returns a ggplot2 parallel coordinate plot.
</p>


<h3>References</h3>

<p>De Soete, G., and Carroll, J. D. (1994). K-means clustering in a low-dimensional Euclidean space. In Diday E. et al. (Eds.), <em>New Approaches in Classification and Data Analysis</em>, Heidelberg: Springer, 212-219.
</p>
<p>Vichi, M., and Kiers, H.A.L. (2001). Factorial K-means analysis for two-way data. <em>Computational Statistics and Data Analysis</em>, 37, 49-64.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.clusmca">plot.clusmca</a>, <a href="#topic+plot.cluspcamix">plot.cluspcamix</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("macro")
#Factorial K-means (3 clusters in 2 dimensions) after 100 random starts
outFKM = cluspca(macro, 3, 2, method = "FKM", rotation = "varimax")
#Scatterplot (dimensions 1 and 2) and cluster description plot
plot(outFKM, cludesc = TRUE)

data("iris", package = "datasets")
#Compromise solution between PCA and Reduced K-means
#on the iris dataset (3 clusters in 2 dimensions) after 100 random starts
outclusPCA = cluspca(iris[,-5], 3, 2, alpha = 0.3, rotation = "varimax")
table(outclusPCA$cluster,iris[,5])
#Save the ggplot2 scatterplot
map = plot(outclusPCA)$map
#Customization (adding titles)
map + ggtitle(paste("A compromise solution between RKM and FKM on the iris: 
3 clusters of sizes ", paste(outclusPCA$size, 
collapse = ", "),sep = "")) + xlab("Dimension 1") + ylab("Dimension 2") + 
theme(plot.title = element_text(size = 10, face = "bold", hjust = 0.5))
</code></pre>

<hr>
<h2 id='plot.cluspcamix'>
Plotting function for <code>cluspcamix()</code> output.
</h2><span id='topic+plot.cluspcamix'></span>

<h3>Description</h3>

<p>Plotting function that creates a scatterplot of the objects, a correlation circle of the variables or a biplot of both objects and variables. Optionally, for metric variables, it returns a parallel coordinate plot showing cluster means and for categorical variables, a series of barplots showing the standardized residuals per attribute for each cluster.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cluspcamix'
plot(x, dims = c(1, 2), cludesc = FALSE, 
topstdres = 20, objlabs = FALSE, attlabs = NULL, attcatlabs = NULL, 
subplot = FALSE, what = c(TRUE,TRUE), max.overlaps = 10, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.cluspcamix_+3A_x">x</code></td>
<td>
<p>Object returned by <code>cluspcamix()</code></p>
</td></tr>
<tr><td><code id="plot.cluspcamix_+3A_dims">dims</code></td>
<td>
<p>Numerical vector of length 2 indicating the dimensions to plot on horizontal and vertical axes respectively; default is first dimension horizontal and
second dimension vertical</p>
</td></tr>
<tr><td><code id="plot.cluspcamix_+3A_what">what</code></td>
<td>
<p>Vector of two logical values specifying the contents of the plots. First entry indicates whether a scatterplot of the objects and cluster centroids is displayed and the second entry whether a correlation circle of the variables is displayed. The default is <code>c(TRUE, TRUE)</code> and the resultant plot is a biplot of both objects and variables</p>
</td></tr>
<tr><td><code id="plot.cluspcamix_+3A_cludesc">cludesc</code></td>
<td>
<p>A logical value indicating if a parallel coordinate plot showing cluster means is produced (default = <code>FALSE)</code></p>
</td></tr>
<tr><td><code id="plot.cluspcamix_+3A_topstdres">topstdres</code></td>
<td>
<p>Number of largest standardized residuals used to describe each cluster (default = 20). Works only in combination with <code>cludesc = TRUE</code></p>
</td></tr>
<tr><td><code id="plot.cluspcamix_+3A_subplot">subplot</code></td>
<td>
<p>A logical value indicating whether a subplot with the full distribution of the standardized residuals will appear at the bottom left corner of the corresponding plots. Works only in combination with <code>cludesc = TRUE</code></p>
</td></tr>
<tr><td><code id="plot.cluspcamix_+3A_objlabs">objlabs</code></td>
<td>
<p>A logical value indicating whether object labels will be plotted; if <code>TRUE</code> row names of the data matrix are used (default = <code>FALSE</code>). Warning: when <code>TRUE</code>, execution time of the plotting function will increase dramatically as the number of objects gets larger</p>
</td></tr>
<tr><td><code id="plot.cluspcamix_+3A_attlabs">attlabs</code></td>
<td>
<p>Vector of custom labels of continuous attributes; if not provided, default labeling is applied</p>
</td></tr>
<tr><td><code id="plot.cluspcamix_+3A_attcatlabs">attcatlabs</code></td>
<td>
<p>Vector of custom labels of categorical attributes (categories); if not provided, default labeling is applied</p>
</td></tr>
<tr><td><code id="plot.cluspcamix_+3A_max.overlaps">max.overlaps</code></td>
<td>
<p>Maximum number of text labels allowed to overlap. Defaults to 10</p>
</td></tr>
<tr><td><code id="plot.cluspcamix_+3A_...">...</code></td>
<td>
<p>Further arguments to be transferred to <code>cluspcamix()</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a ggplot2 scatterplot of the solution obtained via <code>cluspcamix()</code> that can be further customized using the <span class="pkg">ggplot2</span> package. When <code>cludesc = TRUE</code>, for metric variables, the function also returns a ggplot2 parallel coordinate plot and for categorical variables, a series of ggplot2 barplots showing the largest (or all) standardized residuals per attribute for each cluster.
</p>


<h3>References</h3>

<p>van de Velden, M., Iodice D'Enza, A., &amp; Markos, A. (2019). Distance-based clustering of mixed data. <em>Wiley Interdisciplinary Reviews: Computational Statistics</em>, e1456.
</p>
<p>Vichi, M., Vicari, D., &amp; Kiers, H. A. L. (2019). Clustering and dimension reduction for mixed variables. <em>Behaviormetrika</em>. doi:10.1007/s41237-018-0068-6.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.clusmca">plot.clusmca</a>, <a href="#topic+plot.cluspca">plot.cluspca</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(diamond)
#Mixed Reduced K-means solution with 3 clusters in 2 dimensions 
#after 10 random starts
outmixedRKM = cluspcamix(diamond, 3, 2, method = "mixedRKM", nstart = 10)
#Scatterplot (dimensions 1 and 2)
plot(outmixedRKM, cludesc = TRUE)
</code></pre>

<hr>
<h2 id='tuneclus'>
Cluster quality assessment for a range of clusters and dimensions.
</h2><span id='topic+tuneclus'></span><span id='topic+print.tuneclus'></span><span id='topic+summary.tuneclus'></span><span id='topic+fitted.tuneclus'></span>

<h3>Description</h3>

<p>This function facilitates the selection of the appropriate number of clusters and dimensions for joint dimension reduction and clustering methods. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tuneclus(data, nclusrange = 3:4, ndimrange = 2:3, 
method = c("RKM","FKM","mixedRKM","mixedFKM","clusCA","iFCB","MCAk"), 
criterion = "asw", dst = "full", alpha = NULL, alphak = NULL, 
center = TRUE, scale = TRUE, rotation = "none", nstart = 100, 
smartStart = NULL, seed = NULL)

## S3 method for class 'tuneclus'
print(x, ...)

## S3 method for class 'tuneclus'
summary(object, ...)

## S3 method for class 'tuneclus'
fitted(object, mth = c("centers", "classes"), ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tuneclus_+3A_data">data</code></td>
<td>
<p>Continuous, Categorical ot Mixed data set</p>
</td></tr>
<tr><td><code id="tuneclus_+3A_nclusrange">nclusrange</code></td>
<td>
<p>An integer vector with the range of numbers of clusters which are to be compared by the cluster validity criteria. Note: the number of clusters should be greater than one</p>
</td></tr>
<tr><td><code id="tuneclus_+3A_ndimrange">ndimrange</code></td>
<td>
<p>An integer vector with the range of dimensions which are to be compared by the cluster validity criteria</p>
</td></tr>
<tr><td><code id="tuneclus_+3A_method">method</code></td>
<td>
<p>Specifies the method. Options are <code>RKM</code> for reduced K-means, <code>FKM</code> for factorial K-means, <code>mixedRKM</code> for mixed reduced K-means, <code>mixedFKM</code> for mixed factorial K-means, <code>MCAk</code> for MCA K-means, <code>iFCB</code> for Iterative Factorial Clustering of Binary variables and <code>clusCA</code> for Cluster Correspondence Analysis</p>
</td></tr>
<tr><td><code id="tuneclus_+3A_criterion">criterion</code></td>
<td>
<p>One of <code>asw</code>, <code>ch</code> or <code>crit</code>. Determines whether average silhouette width, Calinski-Harabasz index or objective value of the selected method is used (default = <code>"asw")</code></p>
</td></tr>
<tr><td><code id="tuneclus_+3A_dst">dst</code></td>
<td>
<p>Specifies the data used to compute the distances between objects. Options are <code>full</code> for the original data (after possible scaling) and <code>low</code> for the object scores in the low-dimensional space (default = <code>"full")</code></p>
</td></tr>
<tr><td><code id="tuneclus_+3A_alpha">alpha</code></td>
<td>
<p>Adjusts for the relative importance of (mixed) RKM and FKM in the objective function; <code>alpha = 1</code> reduces to PCA, <code>alpha = 0.5</code> to (mixed) reduced K-means, and <code>alpha = 0</code> to (mixed) factorial K-means</p>
</td></tr>
<tr><td><code id="tuneclus_+3A_alphak">alphak</code></td>
<td>
<p>Non-negative scalar to adjust for the relative importance of MCA (<code>alphak = 1</code>) and K-means (<code>alphak = 0</code>) in the solution (default = .5). Works only in combination with <code>method = "MCAk"</code></p>
</td></tr>
<tr><td><code id="tuneclus_+3A_center">center</code></td>
<td>
<p>A logical value indicating whether the variables should be shifted to be zero centered (default = <code>TRUE)</code></p>
</td></tr>
<tr><td><code id="tuneclus_+3A_scale">scale</code></td>
<td>
<p>A logical value indicating whether the variables should be scaled to have unit variance before the analysis takes place (default = <code>TRUE)</code></p>
</td></tr>
<tr><td><code id="tuneclus_+3A_rotation">rotation</code></td>
<td>
<p>Specifies the method used to rotate the factors. Options are none for no rotation, varimax for varimax rotation with Kaiser normalization and promax for promax rotation (default = <code>"none")</code></p>
</td></tr>
<tr><td><code id="tuneclus_+3A_nstart">nstart</code></td>
<td>
<p>Number of starts (default = 100)</p>
</td></tr>
<tr><td><code id="tuneclus_+3A_smartstart">smartStart</code></td>
<td>
<p>If <code>NULL</code> then a random cluster membership vector is generated. Alternatively, a cluster membership vector can be provided as a starting solution</p>
</td></tr>
<tr><td><code id="tuneclus_+3A_seed">seed</code></td>
<td>
<p>An integer that is used as argument by <code>set.seed()</code> for offsetting the random number generator when smartStart = NULL. The default value is NULL.</p>
</td></tr>
<tr><td><code id="tuneclus_+3A_x">x</code></td>
<td>
<p>For the <code>print</code> method, a class of <code>clusmca</code></p>
</td></tr>
<tr><td><code id="tuneclus_+3A_object">object</code></td>
<td>
<p>For the <code>summary</code> method, a class of <code>clusmca</code></p>
</td></tr>
<tr><td><code id="tuneclus_+3A_mth">mth</code></td>
<td>
<p>For the <code>fitted</code> method, a character string that specifies the type of fitted value to return: <code>"centers"</code> for the observations center vector, or <code>"class"</code> for the observations cluster membership value</p>
</td></tr>
<tr><td><code id="tuneclus_+3A_...">...</code></td>
<td>
<p>Not used</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For the K-means part, the algorithm of Hartigan-Wong is used by default.
</p>
<p>The hidden <code>print</code> and <code>summary</code> methods print out some key components of an object of class <code>tuneclus</code>. 
</p>
<p>The hidden <code>fitted</code> method returns cluster fitted values. If method is <code>"classes"</code>, this is a vector of cluster membership (the cluster component of the &quot;tuneclus&quot; object). If method is <code>"centers"</code>, this is a matrix where each row is the cluster center for the observation. The rownames of the matrix are the cluster membership values.</p>


<h3>Value</h3>

<table>
<tr><td><code>clusobjbest</code></td>
<td>
<p>The output of the optimal run of <code>cluspca()</code> or <code>clusmca()</code></p>
</td></tr>
<tr><td><code>nclusbest</code></td>
<td>
<p>The optimal number of clusters</p>
</td></tr>
<tr><td><code>ndimbest</code></td>
<td>
<p>The optimal number of dimensions</p>
</td></tr>
<tr><td><code>critbest</code></td>
<td>
<p>The optimal criterion value for <code>nclusbest</code> clusters and <code>ndimbest</code> dimensions</p>
</td></tr>
<tr><td><code>critgrid</code></td>
<td>
<p>Matrix of size <code>nclusrange x ndimrange</code> with the criterion values for the specified ranges of clusters and dimensions (values are calculated only when the number of clusters is greater than the number of dimensions; otherwise values in the grid are left blank)</p>
</td></tr>
<tr><td><code>criterion</code></td>
<td>
<p>&quot;asw&quot; for average Silhouette width or &quot;ch&quot; for &quot;Calinski-Harabasz&quot;</p>
</td></tr>
<tr><td><code>cluasw</code></td>
<td>
<p>Average Silhouette width values of each cluster, when criterion = &quot;asw&quot;</p>
</td></tr>
</table>


<h3>References</h3>

<p>Calinski, R.B., and Harabasz, J., (1974). A dendrite method for cluster analysis. <em>Communications in Statistics</em>, 3, 1-27.
</p>
<p>Kaufman, L., and Rousseeuw, P.J., (1990). <em>Finding Groups in Data: An Introduction to Cluster Analysis</em>. Wiley, New York.</p>


<h3>See Also</h3>

<p><code><a href="#topic+global_bootclus">global_bootclus</a></code>, <code><a href="#topic+local_bootclus">local_bootclus</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Reduced K-means for a range of clusters and dimensions
data(macro)
# Cluster quality assessment based on the average silhouette width in the low dimensional space
# nstart = 1 for speed in example
# use more for real applications
bestRKM = tuneclus(macro, 3:4, 2:3, method = "RKM", 
criterion = "asw", dst = "low", nstart = 1, seed = 1234)
bestRKM
#plot(bestRKM)

# Cluster Correspondence Analysis for a range of clusters and dimensions
data(bribery)
# Cluster quality assessment based on the Callinski-Harabasz index in the full dimensional space
bestclusCA = tuneclus(bribery, 4:5, 3:4, method = "clusCA",
criterion = "ch", nstart = 20, seed = 1234)
bestclusCA
#plot(bestclusCA, cludesc = TRUE)

# Mixed reduced K-means for a range of clusters and dimensions
data(diamond)
# Cluster quality assessment based on the average silhouette width in the low dimensional space
# nstart = 5 for speed in example
# use more for real applications
bestmixedRKM = tuneclus(diamond[,-7], 3:4, 2:3, 
method = "mixedRKM", criterion = "asw", dst = "low", 
nstart = 5, seed = 1234)
bestmixedRKM
#plot(bestmixedRKM)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
