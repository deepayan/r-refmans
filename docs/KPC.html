<!DOCTYPE html><html><head><title>Help for package KPC</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {KPC}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ElecData'><p>2017 Korea presidential election data</p></a></li>
<li><a href='#KFOCI'><p>Kernel Feature Ordering by Conditional Independence</p></a></li>
<li><a href='#Klin'><p>A near linear time analogue of KMAc</p></a></li>
<li><a href='#KMAc'><p>KMAc (the unconditional version of graph-based KPC) with geometric graphs.</p></a></li>
<li><a href='#KPCgraph'><p>Kernel partial correlation with geometric graphs</p></a></li>
<li><a href='#KPCRKHS'><p>Kernel partial correlation with RKHS method</p></a></li>
<li><a href='#KPCRKHS_VS'><p>Variable selection with RKHS estimator</p></a></li>
<li><a href='#med'><p>Medical data from 35 patients</p></a></li>
<li><a href='#TnKnn'><p>Tn with geometric graphs</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Kernel Partial Correlation Coefficient</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.2</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Zhen Huang &lt;zh2395@columbia.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Implementations of two empirical versions the kernel partial correlation (KPC) coefficient
    and the associated variable selection algorithms. KPC is a measure of the strength of conditional
    association between Y and Z given X, with X, Y, Z being random variables taking values in
    general topological spaces. As the name suggests, KPC is defined in terms of kernels on
    reproducing kernel Hilbert spaces (RKHSs). The population KPC is a deterministic number
    between 0 and 1; it is 0 if and only if Y is conditionally independent of Z given X, and it is 1 if
    and only if Y is a measurable function of Z and X. One empirical KPC estimator is based on
    geometric graphs, such as K-nearest neighbor graphs and minimum spanning trees, and is
    consistent under very weak conditions. The other empirical estimator, defined using conditional
    mean embeddings (CMEs) as used in the RKHS literature, is also consistent under suitable
    conditions. Using KPC, a stepwise forward variable selection algorithm KFOCI (using the graph
    based estimator of KPC) is provided, as well as a similar stepwise forward selection algorithm
    based on the RKHS based estimator. For more details on KPC, its empirical estimators and its
    application on variable selection, see Huang, Z., N. Deb, and B. Sen (2022). “Kernel partial
    correlation coefficient – a measure of conditional dependence” (URL listed below). When X is
    empty, KPC measures the unconditional dependence between Y and Z, which has been described
    in Deb, N., P. Ghosal, and B. Sen (2020), “Measuring association on topological spaces using
    kernels and geometric graphs” &lt;<a href="https://arxiv.org/abs/2010.01768">arXiv:2010.01768</a>&gt;, and it is implemented in the functions
    KMAc() and Klin() in this package. The latter can be computed in near linear time.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://www.jmlr.org/papers/v23/21-493.html">https://www.jmlr.org/papers/v23/21-493.html</a>,
<a href="https://arxiv.org/abs/2012.14804">https://arxiv.org/abs/2012.14804</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0.0), data.table, kernlab</td>
</tr>
<tr>
<td>Imports:</td>
<td>RANN, proxy, parallel, mlpack</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-10-05 19:55:05 UTC; huangzhen</td>
</tr>
<tr>
<td>Author:</td>
<td>Zhen Huang [aut, cre],
  Nabarun Deb [ctb],
  Bodhisattva Sen [ctb]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-10-05 20:20:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='ElecData'>2017 Korea presidential election data</h2><span id='topic+ElecData'></span>

<h3>Description</h3>

<p>A dataset containing 9 variables, consists of the voting results earned by the top five candidates from 250 electoral districts in Korea.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ElecData
</code></pre>


<h3>Format</h3>

<p>A data frame with 1250 rows and 9 variables:
</p>

<dl>
<dt>PrecinctCode</dt><dd><p>250 precinct codes designated by the election committee (4 digits)</p>
</dd>
<dt>CityCode</dt><dd><p>250 city codes of administrative standard code management system (5 digits)</p>
</dd>
<dt>CandidateName</dt><dd><p>Symbols 1-5, corresponding to Moon Jae-in, Hong Jun-pyo, Ahn Cheol-soo, Yoo Seung-min, Shim Sang-jung</p>
</dd>
<dt>AveAge</dt><dd><p>Average age of voters in 17 years: statistics on resident registration population of the Ministry of Government Administration and Home Affairs</p>
</dd>
<dt>AveYearEdu</dt><dd><p>Average number of years of education for voters</p>
</dd>
<dt>AveHousePrice</dt><dd><p>Average price per square meter in 17 years</p>
</dd>
<dt>AveInsurance</dt><dd><p>The average insurance premium for each city, county, district</p>
</dd>
<dt>VoteRate</dt><dd><p>Vote rate by candidate</p>
</dd>
<dt>NumVote</dt><dd><p>Number of votes by candidate</p>
</dd>
</dl>



<h3>Source</h3>

<p><a href="https://github.com/OhmyNews/2017-Election">https://github.com/OhmyNews/2017-Election</a>
</p>

<hr>
<h2 id='KFOCI'>Kernel Feature Ordering by Conditional Independence</h2><span id='topic+KFOCI'></span>

<h3>Description</h3>

<p>Variable selection with KPC using directed K-NN graph or minimum spanning tree (MST)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KFOCI(
  Y,
  X,
  k = kernlab::rbfdot(1/(2 * stats::median(stats::dist(Y))^2)),
  Knn = min(ceiling(NROW(Y)/20), 20),
  num_features = NULL,
  stop = TRUE,
  numCores = parallel::detectCores(),
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KFOCI_+3A_y">Y</code></td>
<td>
<p>a matrix of responses (n by dy)</p>
</td></tr>
<tr><td><code id="KFOCI_+3A_x">X</code></td>
<td>
<p>a matrix of predictors (n by dx)</p>
</td></tr>
<tr><td><code id="KFOCI_+3A_k">k</code></td>
<td>
<p>a function <code class="reqn">k(y, y')</code> of class <code>kernel</code>. It can be the kernel implemented in <code>kernlab</code> e.g., Gaussian kernel: <code>rbfdot(sigma = 1)</code>, linear kernel: <code>vanilladot()</code>.</p>
</td></tr>
<tr><td><code id="KFOCI_+3A_knn">Knn</code></td>
<td>
<p>a positive integer indicating the number of nearest neighbor; or &quot;MST&quot;. The suggested choice of Knn is 0.05n for samples up to a few hundred observations. For large n, the suggested Knn is sublinear in n. That is, it may grow slower than any linear function of n. The computing time is approximately linear in Knn. A smaller Knn takes less time.</p>
</td></tr>
<tr><td><code id="KFOCI_+3A_num_features">num_features</code></td>
<td>
<p>the number of variables to be selected, cannot be larger than dx. The default value is NULL and in that
case it will be set equal to dx. If <code>stop == TRUE</code> (see below), then num_features is the maximal number of variables to be selected.</p>
</td></tr>
<tr><td><code id="KFOCI_+3A_stop">stop</code></td>
<td>
<p>If <code>stop == TRUE</code>, then the automatic stopping criterion (stops at the first instance of negative Tn, as mentioned in the paper) will be implemented and continued till <code>num_features</code> many variables are selected. If <code>stop == FALSE</code> then exactly <code>num_features</code> many variables are selected.</p>
</td></tr>
<tr><td><code id="KFOCI_+3A_numcores">numCores</code></td>
<td>
<p>number of cores that are going to be used for parallelizing the process.</p>
</td></tr>
<tr><td><code id="KFOCI_+3A_verbose">verbose</code></td>
<td>
<p>whether to print each selected variables during the forward stepwise algorithm</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A stepwise forward selection of variables using KPC. At each step it selects the <code class="reqn">X_j</code> that maximizes
<code class="reqn">\hat{\rho^2}(Y,X_j |</code>selected <code class="reqn">X_i)</code>.
It is suggested to normalize the predictors before applying KFOCI.
Euclidean distance is used for computing the K-NN graph and the MST.
</p>


<h3>Value</h3>

<p>The algorithm returns a vector of the indices from 1,...,dx of the selected variables in the same order that they were selected. The variables at the front are expected to be more informative in predicting Y.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+KPCgraph">KPCgraph</a></code>, <code><a href="#topic+KPCRKHS">KPCRKHS</a></code>, <code><a href="#topic+KPCRKHS_VS">KPCRKHS_VS</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n = 200
p = 10
X = matrix(rnorm(n * p), ncol = p)
Y = X[, 1] * X[, 2] + sin(X[, 1] * X[, 3])
KFOCI(Y, X, kernlab::rbfdot(1), Knn=1, numCores=1)
# 1 2 3
</code></pre>

<hr>
<h2 id='Klin'>A near linear time analogue of KMAc</h2><span id='topic+Klin'></span>

<h3>Description</h3>

<p>Calculate <code class="reqn">\hat{\eta}_n^{\mbox{lin}}</code> (the unconditional version of graph-based KPC) using directed K-NN graph or minimum spanning tree (MST).
The computational complexity is O(nlog(n))
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Klin(
  Y,
  X,
  k = kernlab::rbfdot(1/(2 * stats::median(stats::dist(Y))^2)),
  Knn = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Klin_+3A_y">Y</code></td>
<td>
<p>a matrix of response (n by dy)</p>
</td></tr>
<tr><td><code id="Klin_+3A_x">X</code></td>
<td>
<p>a matrix of predictors (n by dx)</p>
</td></tr>
<tr><td><code id="Klin_+3A_k">k</code></td>
<td>
<p>a function <code class="reqn">k(y, y')</code> of class <code>kernel</code>. It can be the kernel implemented in <code>kernlab</code> e.g. <code>rbfdot(sigma = 1)</code>, <code>vanilladot()</code></p>
</td></tr>
<tr><td><code id="Klin_+3A_knn">Knn</code></td>
<td>
<p>the number of K-nearest neighbor to use; or &quot;MST&quot;. A small Knn (e.g., Knn=1) is recommended.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code class="reqn">\hat{\eta}_n</code> is an estimate of the population kernel measure of association, based on data <code class="reqn">\{(X_i,Y_i)\}_{i=1}^n</code> from <code class="reqn">\mu</code>.
For K-NN graph, <code class="reqn">\hat{\eta}_n</code> can be computed in near linear time (in <code class="reqn">n</code>).
In particular,
</p>
<p style="text-align: center;"><code class="reqn">\hat{\eta}_n^{\mbox{lin}}:=\frac{n^{-1}\sum_{i=1}^n d_i^{-1}\sum_{j:(i,j)\in\mathcal{E}(G_n)} k(Y_i,Y_j)-(n-1)^{-1}\sum_{i=1}^{n-1} k(Y_i,Y_{i+1})}{n^{-1}\sum_{i=1}^n k(Y_i,Y_i)-(n-1)^{-1}\sum_{i=1}^{n-1} k(Y_i,Y_{i+1})}</code>
</p>
<p>,
where all symbols have their usual meanings as in the definition of <code class="reqn">\hat{\eta}_n</code>.
Euclidean distance is used for computing the K-NN graph and the MST.
</p>


<h3>Value</h3>

<p>The algorithm returns a real number &lsquo;Klin&rsquo;: an empirical kernel measure of association which can be computed in near linear time when K-NN graphs are used.
</p>


<h3>References</h3>

<p>Deb, N., P. Ghosal, and B. Sen (2020), “Measuring association on topological spaces using kernels and geometric graphs” &lt;arXiv:2010.01768&gt;.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+KPCgraph">KPCgraph</a></code>, <code><a href="#topic+KMAc">KMAc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(kernlab)
Klin(Y = rnorm(100), X = rnorm(100), k = rbfdot(1), Knn = 1)
</code></pre>

<hr>
<h2 id='KMAc'>KMAc (the unconditional version of graph-based KPC) with geometric graphs.</h2><span id='topic+KMAc'></span>

<h3>Description</h3>

<p>Calculate <code class="reqn">\hat{\eta}_n</code> (the unconditional version of graph-based KPC) using directed K-NN graph or minimum spanning tree (MST).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KMAc(
  Y,
  X,
  k = kernlab::rbfdot(1/(2 * stats::median(stats::dist(Y))^2)),
  Knn = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KMAc_+3A_y">Y</code></td>
<td>
<p>a matrix of response (n by dy)</p>
</td></tr>
<tr><td><code id="KMAc_+3A_x">X</code></td>
<td>
<p>a matrix of predictors (n by dx)</p>
</td></tr>
<tr><td><code id="KMAc_+3A_k">k</code></td>
<td>
<p>a function <code class="reqn">k(y, y')</code> of class <code>kernel</code>. It can be the kernel implemented in <code>kernlab</code> e.g., Gaussian kernel: <code>rbfdot(sigma = 1)</code>, linear kernel: <code>vanilladot()</code></p>
</td></tr>
<tr><td><code id="KMAc_+3A_knn">Knn</code></td>
<td>
<p>the number of K-nearest neighbor to use; or &quot;MST&quot;. A small Knn (e.g., Knn=1) is recommended for an accurate estimate of the population KMAc.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code class="reqn">\hat{\eta}_n</code> is an estimate of the population kernel measure of association, based on data <code class="reqn">\{(X_i,Y_i)\}_{i=1}^n</code> from <code class="reqn">\mu</code>.
For K-NN graph, ties will be broken at random. MST is found using package <code>emstreeR</code>.
In particular,
</p>
<p style="text-align: center;"><code class="reqn">\hat{\eta}_n:=\frac{n^{-1}\sum_{i=1}^n d_i^{-1}\sum_{j:(i,j)\in\mathcal{E}(G_n)} k(Y_i,Y_j)-(n(n-1))^{-1}\sum_{i\neq j}k(Y_i,Y_j)}{n^{-1}\sum_{i=1}^n k(Y_i,Y_i)-(n(n-1))^{-1}\sum_{i\neq j}k(Y_i,Y_j)},</code>
</p>

<p>where <code class="reqn">G_n</code> denotes a MST or K-NN graph on <code class="reqn">X_1,\ldots , X_n</code>, <code class="reqn">\mathcal{E}(G_n)</code> denotes the set of edges of <code class="reqn">G_n</code> and
<code class="reqn">(i,j)\in\mathcal{E}(G_n)</code> implies that there is an edge from <code class="reqn">X_i</code> to <code class="reqn">X_j</code> in <code class="reqn">G_n</code>.
Euclidean distance is used for computing the K-NN graph and the MST.
</p>


<h3>Value</h3>

<p>The algorithm returns a real number &lsquo;KMAc&rsquo;, the empirical kernel measure of association
</p>


<h3>References</h3>

<p>Deb, N., P. Ghosal, and B. Sen (2020), “Measuring association on topological spaces using kernels and geometric graphs” &lt;arXiv:2010.01768&gt;.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+KPCgraph">KPCgraph</a></code>, <code><a href="#topic+Klin">Klin</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(kernlab)
KMAc(Y = rnorm(100), X = rnorm(100), k = rbfdot(1), Knn = 1)
</code></pre>

<hr>
<h2 id='KPCgraph'>Kernel partial correlation with geometric graphs</h2><span id='topic+KPCgraph'></span>

<h3>Description</h3>

<p>Calculate the kernel partial correlation (KPC) coefficient with directed K-nearest neighbor (K-NN) graph or minimum spanning tree (MST).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KPCgraph(
  Y,
  X,
  Z,
  k = kernlab::rbfdot(1/(2 * stats::median(stats::dist(Y))^2)),
  Knn = 1,
  trans_inv = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KPCgraph_+3A_y">Y</code></td>
<td>
<p>a matrix (n by dy)</p>
</td></tr>
<tr><td><code id="KPCgraph_+3A_x">X</code></td>
<td>
<p>a matrix (n by dx) or <code>NULL</code> if <code class="reqn">X</code> is empty</p>
</td></tr>
<tr><td><code id="KPCgraph_+3A_z">Z</code></td>
<td>
<p>a matrix (n by dz)</p>
</td></tr>
<tr><td><code id="KPCgraph_+3A_k">k</code></td>
<td>
<p>a function <code class="reqn">k(y, y')</code> of class <code>kernel</code>. It can be the kernel implemented in <code>kernlab</code> e.g., Gaussian kernel: <code>rbfdot(sigma = 1)</code>, linear kernel: <code>vanilladot()</code>.</p>
</td></tr>
<tr><td><code id="KPCgraph_+3A_knn">Knn</code></td>
<td>
<p>a positive integer indicating the number of nearest neighbor to use; or &quot;MST&quot;. A small Knn (e.g., Knn=1) is recommended for an accurate estimate of the population KPC.</p>
</td></tr>
<tr><td><code id="KPCgraph_+3A_trans_inv">trans_inv</code></td>
<td>
<p>TRUE or FALSE. Is <code class="reqn">k(y, y)</code> free of <code class="reqn">y</code>?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The kernel partial correlation squared (KPC) measures the conditional dependence
between <code class="reqn">Y</code> and <code class="reqn">Z</code> given <code class="reqn">X</code>, based on an i.i.d. sample of <code class="reqn">(Y, Z, X)</code>.
It converges to the population quantity (depending on the kernel) which is between 0 and 1.
A small value indicates low conditional dependence between <code class="reqn">Y</code> and <code class="reqn">Z</code> given <code class="reqn">X</code>, and
a large value indicates stronger conditional dependence.
If <code>X == NULL</code>, it returns the <code>KMAc(Y,Z,k,Knn)</code>, which measures the unconditional dependence between <code class="reqn">Y</code> and <code class="reqn">Z</code>.
Euclidean distance is used for computing the K-NN graph and the MST.
MST in practice often achieves similar performance as the 2-NN graph. A small K is recommended for the K-NN graph for an accurate estimate of the population KPC,
while if KPC is used as a test statistic for conditional independence, a larger K can be beneficial.
</p>


<h3>Value</h3>

<p>The algorithm returns a real number which is the estimated KPC.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+KPCRKHS">KPCRKHS</a></code>, <code><a href="#topic+KMAc">KMAc</a></code>, <code><a href="#topic+Klin">Klin</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(kernlab)
n = 2000
x = rnorm(n)
z = rnorm(n)
y = x + z + rnorm(n,1,1)
KPCgraph(y,x,z,vanilladot(),Knn=1,trans_inv=FALSE)

n = 1000
x = runif(n)
z = runif(n)
y = (x + z) %% 1
KPCgraph(y,x,z,rbfdot(5),Knn="MST",trans_inv=TRUE)

discrete_ker = function(y1,y2) {
    if (y1 == y2) return(1)
    return(0)
}
class(discrete_ker) &lt;- "kernel"
set.seed(1)
n = 2000
x = rnorm(n)
z = rnorm(n)
y = rep(0,n)
for (i in 1:n) y[i] = sample(c(1,0),1,prob = c(exp(-z[i]^2/2),1-exp(-z[i]^2/2)))
KPCgraph(y,x,z,discrete_ker,1)
##0.330413
</code></pre>

<hr>
<h2 id='KPCRKHS'>Kernel partial correlation with RKHS method</h2><span id='topic+KPCRKHS'></span>

<h3>Description</h3>

<p>Compute estimate of Kernel partial correlation (KPC) coefficient using conditional mean embeddings in the reproducing kernel Hilbert spaces (RKHS).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KPCRKHS(
  Y,
  X = NULL,
  Z,
  ky = kernlab::rbfdot(1/(2 * stats::median(stats::dist(Y))^2)),
  kx = kernlab::rbfdot(1/(2 * stats::median(stats::dist(X))^2)),
  kxz = kernlab::rbfdot(1/(2 * stats::median(stats::dist(cbind(X, Z)))^2)),
  eps = 0.001,
  appro = FALSE,
  tol = 1e-05
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KPCRKHS_+3A_y">Y</code></td>
<td>
<p>a matrix (n by dy)</p>
</td></tr>
<tr><td><code id="KPCRKHS_+3A_x">X</code></td>
<td>
<p>a matrix (n by dx) or <code>NULL</code> if <code class="reqn">X</code> is empty</p>
</td></tr>
<tr><td><code id="KPCRKHS_+3A_z">Z</code></td>
<td>
<p>a matrix (n by dz)</p>
</td></tr>
<tr><td><code id="KPCRKHS_+3A_ky">ky</code></td>
<td>
<p>a function <code class="reqn">k(y, y')</code> of class <code>kernel</code>. It can be the kernel implemented in <code>kernlab</code> e.g., Gaussian kernel: <code>rbfdot(sigma = 1)</code>, linear kernel: <code>vanilladot()</code>.</p>
</td></tr>
<tr><td><code id="KPCRKHS_+3A_kx">kx</code></td>
<td>
<p>the kernel function for <code class="reqn">X</code></p>
</td></tr>
<tr><td><code id="KPCRKHS_+3A_kxz">kxz</code></td>
<td>
<p>the kernel function for <code class="reqn">(X, Z)</code> or for <code class="reqn">Z</code> if <code class="reqn">X</code> is empty</p>
</td></tr>
<tr><td><code id="KPCRKHS_+3A_eps">eps</code></td>
<td>
<p>a small positive regularization parameter for inverting the empirical cross-covariance operator</p>
</td></tr>
<tr><td><code id="KPCRKHS_+3A_appro">appro</code></td>
<td>
<p>whether to use incomplete Cholesky decomposition for approximation</p>
</td></tr>
<tr><td><code id="KPCRKHS_+3A_tol">tol</code></td>
<td>
<p>tolerance used for incomplete Cholesky decomposition (implemented by the function <code>inchol</code> in the package <code>kernlab</code>)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The kernel partial correlation (KPC) coefficient measures the conditional dependence
between <code class="reqn">Y</code> and <code class="reqn">Z</code> given <code class="reqn">X</code>, based on an i.i.d. sample of <code class="reqn">(Y, Z, X)</code>.
It converges to the population quantity (depending on the kernel) which is between 0 and 1.
A small value indicates low conditional dependence between <code class="reqn">Y</code> and <code class="reqn">Z</code> given <code class="reqn">X</code>, and
a large value indicates stronger conditional dependence.
If <code>X = NULL</code>, it measures the unconditional dependence between <code class="reqn">Y</code> and <code class="reqn">Z</code>.
</p>


<h3>Value</h3>

<p>The algorithm returns a real number which is the estimated KPC.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+KPCgraph">KPCgraph</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n = 500
set.seed(1)
x = rnorm(n)
z = rnorm(n)
y = x + z + rnorm(n,1,1)
library(kernlab)
k = vanilladot()
KPCRKHS(y, x, z, k, k, k, 1e-3/n^(0.4), appro = FALSE)
# 0.4854383 (Population quantity = 0.5)
KPCRKHS(y, x, z, k, k, k, 1e-3/n^(0.4), appro = TRUE, tol = 1e-5)
# 0.4854383 (Population quantity = 0.5)
</code></pre>

<hr>
<h2 id='KPCRKHS_VS'>Variable selection with RKHS estimator</h2><span id='topic+KPCRKHS_VS'></span>

<h3>Description</h3>

<p>The algorithm performs a forward stepwise variable selection using RKHS estimators.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KPCRKHS_VS(
  Y,
  X,
  num_features,
  ky = kernlab::rbfdot(1/(2 * stats::median(stats::dist(Y))^2)),
  kS = NULL,
  eps = 0.001,
  appro = FALSE,
  tol = 1e-05,
  numCores = parallel::detectCores(),
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KPCRKHS_VS_+3A_y">Y</code></td>
<td>
<p>a matrix of responses (n by dy)</p>
</td></tr>
<tr><td><code id="KPCRKHS_VS_+3A_x">X</code></td>
<td>
<p>a matrix of predictors (n by dx)</p>
</td></tr>
<tr><td><code id="KPCRKHS_VS_+3A_num_features">num_features</code></td>
<td>
<p>the number of variables to be selected, cannot be larger than dx.</p>
</td></tr>
<tr><td><code id="KPCRKHS_VS_+3A_ky">ky</code></td>
<td>
<p>a function <code class="reqn">k(y, y')</code> of class <code>kernel</code>. It can be the kernel implemented in <code>kernlab</code> e.g., Gaussian kernel: <code>rbfdot(sigma = 1)</code>, linear kernel: <code>vanilladot()</code></p>
</td></tr>
<tr><td><code id="KPCRKHS_VS_+3A_ks">kS</code></td>
<td>
<p>a function that takes X and a subset of indices S as inputs, and then outputs the kernel for X_S. The first argument of kS is X, and the second argument is a vector of positive integer. If <code>kS == NULL</code>, Gaussian kernel with empitical bandwidth <code>kernlab::rbfdot(1/(2*stats::median(stats::dist(X[,S]))^2))</code> will be used.</p>
</td></tr>
<tr><td><code id="KPCRKHS_VS_+3A_eps">eps</code></td>
<td>
<p>a positive number; the regularization parameter for the RKHS estimator</p>
</td></tr>
<tr><td><code id="KPCRKHS_VS_+3A_appro">appro</code></td>
<td>
<p>whether to use incomplete Cholesky decomposition for approximation</p>
</td></tr>
<tr><td><code id="KPCRKHS_VS_+3A_tol">tol</code></td>
<td>
<p>tolerance used for incomplete Cholesky decomposition (<code>inchol</code> in package <code>kernlab</code>)</p>
</td></tr>
<tr><td><code id="KPCRKHS_VS_+3A_numcores">numCores</code></td>
<td>
<p>number of cores that are going to be used for parallelizing the process.</p>
</td></tr>
<tr><td><code id="KPCRKHS_VS_+3A_verbose">verbose</code></td>
<td>
<p>whether to print each selected variables during the forward stepwise algorithm</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A stepwise forward selection of variables using KPC. At each step it selects the <code class="reqn">X_j</code> that maximizes <code class="reqn">\tilde{\rho^2}(Y,X_j |</code>selected <code class="reqn">X_i)</code>.
It is suggested to normalize the features before applying the algorithm.
</p>


<h3>Value</h3>

<p>The algorithm returns a vector of the indices from <code>1,...,dx</code> of the selected variables in the same order that they were selected. The variables at the front are expected to be more informative in predicting Y.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+KPCgraph">KPCgraph</a></code>, <code><a href="#topic+KPCRKHS">KPCRKHS</a></code>, <code><a href="#topic+KFOCI">KFOCI</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n = 200
p = 10
X = matrix(rnorm(n * p), ncol = p)
Y = X[, 1] * X[, 2] + sin(X[, 1] * X[, 3])
library(kernlab)
kS = function(X,S) return(rbfdot(1/length(S)))
KPCRKHS_VS(Y, X, num_features = 3, rbfdot(1), kS, eps = 1e-3, appro = FALSE, numCores = 1)
kS = function(X,S) return(rbfdot(1/(2*stats::median(stats::dist(X[,S]))^2)))
KPCRKHS_VS(Y, X, num_features = 3, rbfdot(1), kS, eps = 1e-3, appro = FALSE, numCores = 1)
</code></pre>

<hr>
<h2 id='med'>Medical data from 35 patients</h2><span id='topic+med'></span>

<h3>Description</h3>

<p>A dataset containing three variables (creatinine clearance C; digoxin clearance D; urine flow U) from 35 patients.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>med
</code></pre>


<h3>Format</h3>

<p>A data frame with 35 rows and 3 variables:
</p>

<dl>
<dt>C</dt><dd><p>creatinine clearance, in ml/min/1.73m^2</p>
</dd>
<dt>D</dt><dd><p>digoxin clearance, in ml/min/1.73m^2</p>
</dd>
<dt>U</dt><dd><p>urine flow, in ml/min</p>
</dd>
</dl>



<h3>Source</h3>

<p>Edwards, D. (2012). Introduction to graphical modelling, Section 3.1.4, Springer Science &amp; Business Media.
</p>

<hr>
<h2 id='TnKnn'>Tn with geometric graphs</h2><span id='topic+TnKnn'></span>

<h3>Description</h3>

<p>Calculate <code class="reqn">T_n</code> using directed K-NN graph or minimum spanning tree (MST).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TnKnn(Y, X, k, Knn = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TnKnn_+3A_y">Y</code></td>
<td>
<p>a matrix of response (n by dy)</p>
</td></tr>
<tr><td><code id="TnKnn_+3A_x">X</code></td>
<td>
<p>a matrix of predictors (n by dx)</p>
</td></tr>
<tr><td><code id="TnKnn_+3A_k">k</code></td>
<td>
<p>a function <code class="reqn">k(y, y')</code> of class <code>kernel</code>. It can be the kernel implemented in <code>kernlab</code> e.g. Gaussian kernel: <code>rbfdot(sigma = 1)</code>, linear kernel: <code>vanilladot()</code>.</p>
</td></tr>
<tr><td><code id="TnKnn_+3A_knn">Knn</code></td>
<td>
<p>the number of K-nearest neighbor to use; or &quot;MST&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code class="reqn">T_n</code> is an estimate of <code class="reqn">E[E[k(Y_1,Y_1')|X]]</code>, with <code class="reqn">Y_1</code>, <code class="reqn">Y_1'</code> drawn iid from <code class="reqn">Y|X</code>, given <code class="reqn">X</code>.
For K-NN graph, ties will be broken at random. Algorithm finding the MST is implemented the package <code>emstreeR</code>.
</p>


<h3>Value</h3>

<p>The algorithm returns a real number which is the value of Tn.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
