<!DOCTYPE html><html><head><title>Help for package hdd</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {hdd}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#hdd-package'><p>Easy manipulation of out of memory data sets</p></a></li>
<li><a href='#+5B.hdd'><p>Extraction of HDD data</p></a></li>
<li><a href='#+24.hdd'><p>Extracts a single variable from a HDD object</p></a></li>
<li><a href='#dim.hdd'><p>Dimension of a HDD object</p></a></li>
<li><a href='#guess_col_types'><p>Guesses the columns types of a file</p></a></li>
<li><a href='#guess_delim'><p>Guesses the delimiter of a text file</p></a></li>
<li><a href='#hdd'><p>Hard drive data set</p></a></li>
<li><a href='#hdd_merge'><p>Merges data to a HDD file</p></a></li>
<li><a href='#hdd_setkey'><p>Sorts HDD objects</p></a></li>
<li><a href='#hdd_slice'><p>Applies a function to slices of data to create a HDD data set</p></a></li>
<li><a href='#names.hdd'><p>Variables names of a HDD object</p></a></li>
<li><a href='#origin'><p>Extracts the origin of a HDD object</p></a></li>
<li><a href='#peek'><p>Peek into a text file</p></a></li>
<li><a href='#print.hdd'><p>Print method for HDD objects</p></a></li>
<li><a href='#readfst'><p>Read fst or HDD files as DT</p></a></li>
<li><a href='#setHdd_extract.cap'><p>Sets/gets the size cap when extracting hdd data</p></a></li>
<li><a href='#summary.hdd'><p>Summary information for HDD objects</p></a></li>
<li><a href='#txt2hdd'><p>Transforms text data into a HDD file</p></a></li>
<li><a href='#write_hdd'><p>Saves or appends a data set into a HDD file</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Easy Manipulation of Out of Memory Data Sets</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.1</td>
</tr>
<tr>
<td>Imports:</td>
<td>fst, utils, readr, dreamerr</td>
</tr>
<tr>
<td>Depends:</td>
<td>data.table</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Description:</td>
<td>Hard drive data: Class of data allowing the easy importation/manipulation of out of memory data sets. The data sets are located on disk but look like in-memory, the syntax for manipulation is similar to 'data.table'. Operations are performed "chunk-wise" behind the scene.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-08-24 14:49:57 UTC; lrberge</td>
</tr>
<tr>
<td>Author:</td>
<td>Laurent Berge [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Laurent Berge &lt;laurent.berge@u-bordeaux.fr&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-08-25 10:00:17 UTC</td>
</tr>
</table>
<hr>
<h2 id='hdd-package'>Easy manipulation of out of memory data sets</h2><span id='topic+hdd-package'></span><span id='topic+_PACKAGE'></span>

<h3>Description</h3>

<p><span class="pkg">hdd</span> offers a class of data, hard drive data, allowing the easy importation/manipulation of out of memory data sets. The data sets are located on disk but look like in-memory, the syntax for manipulation is similar to <code><a href="data.table.html#topic+data.table">data.table</a></code>. Operations are performed &quot;chunk-wise&quot; behind the scene.
</p>


<h3>Details</h3>

<p>The functions for importations is <code><a href="#topic+txt2hdd">txt2hdd</a></code>. The loading of a hdd data set is done with <code><a href="#topic+hdd">hdd</a></code> and the data is extracted with <code><a href="#topic+sub-.hdd">sub-.hdd</a></code> which has a <code><a href="data.table.html#topic+data.table">data.table</a></code> syntax. You can alternatively create a <code>hdd</code> data set with <code><a href="#topic+hdd_slice">hdd_slice</a></code>. Other utilities include <code><a href="#topic+hdd_merge">hdd_merge</a></code>, or <code><a href="#topic+peek">peek</a></code> to have a quick look into a text file containing data.
</p>


<h3>Author(s)</h3>

<p>Laurent Berge
</p>

<hr>
<h2 id='+5B.hdd'>Extraction of HDD data</h2><span id='topic++5B.hdd'></span>

<h3>Description</h3>

<p>This function extract data from HDD files, in a similar fashion as data.table but with more arguments.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'hdd'
x[index, ..., file, newfile, replace = FALSE, all.vars = FALSE]
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="+2B5B.hdd_+3A_x">x</code></td>
<td>
<p>A hdd file.</p>
</td></tr>
<tr><td><code id="+2B5B.hdd_+3A_index">index</code></td>
<td>
<p>An index, you can use <code>.N</code> and variable names, like in data.table.</p>
</td></tr>
<tr><td><code id="+2B5B.hdd_+3A_...">...</code></td>
<td>
<p>Other components of the extraction to be passed to <code><a href="data.table.html#topic+data.table">data.table</a></code>.</p>
</td></tr>
<tr><td><code id="+2B5B.hdd_+3A_file">file</code></td>
<td>
<p>Which file to extract from? (Remember hdd data is split in several files.) You can use <code>.N</code>.</p>
</td></tr>
<tr><td><code id="+2B5B.hdd_+3A_newfile">newfile</code></td>
<td>
<p>A destination directory. Default is missing. Should be result of the query be saved into a new HDD directory? Otherwise, it is put in memory.</p>
</td></tr>
<tr><td><code id="+2B5B.hdd_+3A_replace">replace</code></td>
<td>
<p>Only used if argument <code>newfile</code> is not missing: default is <code>FALSE</code>. If the <code>newfile</code> points to an existing HDD data, then to replace it you must have <code>replace = TRUE</code>.</p>
</td></tr>
<tr><td><code id="+2B5B.hdd_+3A_all.vars">all.vars</code></td>
<td>
<p>Logical, default is <code>FALSE</code>. By default, if the first argument of <code>...</code> is provided (i.e. argument <code>j</code>) then only variables appearing in all <code>...</code> plus the variable names found in <code>index</code> are extracted. If <code>TRUE</code> all variables are extracted before any selection is done. (This can be useful when the algorithm getting the variable names gets confused in case of complex queries.)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The extraction of variables look like a regular <code>data.table</code> extraction but in fact all operations are made chunk-by-chunk behind the scene.
</p>
<p>The extra arguments <code>file</code>, <code>newfile</code> and <code>replace</code> are added to a regular <code><a href="data.table.html#topic+data.table">data.table</a></code> call. Argument <code>file</code> is used to select the chunks, you can use the special variable <code>.N</code> to identify the last chunk.
</p>
<p>By default, the operation loads the data in memory. But if the expected size is still too large, you can use the argument <code>newfile</code> to create a new HDD data set without size restriction. If a HDD data set already exists in the <code>newfile</code> destination, you can use the argument <code>replace=TRUE</code> to override it.
</p>


<h3>Value</h3>

<p>Returns a data.table extracted from a HDD file (except if newwfile is not missing).
</p>


<h3>Author(s)</h3>

<p>Laurent Berge
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+hdd">hdd</a></code>, <code><a href="#topic+sub-.hdd">sub-.hdd</a></code> and <code><a href="#topic+cash-.hdd">cash-.hdd</a></code>
for the extraction and manipulation of out of memory data. For importation of
HDD data sets from text files: see <code><a href="#topic+txt2hdd">txt2hdd</a></code>.
</p>
<p>See <code><a href="#topic+hdd_slice">hdd_slice</a></code> to apply functions to chunks of data (and create
HDD objects) and <code><a href="#topic+hdd_merge">hdd_merge</a></code> to merge large files.
</p>
<p>To create/reshape HDD objects from memory or from other HDD objects, see
<code><a href="#topic+write_hdd">write_hdd</a></code>.
</p>
<p>To display general information from HDD objects: <code><a href="#topic+origin">origin</a></code>,
<code><a href="#topic+summary.hdd">summary.hdd</a></code>, <code><a href="#topic+print.hdd">print.hdd</a></code>,
<code><a href="#topic+dim.hdd">dim.hdd</a></code> and <code><a href="#topic+names.hdd">names.hdd</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Toy example with iris data

# First we create a hdd data set to run the example
hdd_path = tempfile()
write_hdd(iris, hdd_path, rowsPerChunk = 40)

# your data set is in the hard drive, in hdd format already.
data_hdd = hdd(hdd_path)

# summary information on the whole file:
summary(data_hdd)

# You can use the argument 'file' to subselect slices.
# Let's have some descriptive statistics of the first slice of HDD
summary(data_hdd[, file = 1])

# It extract the data from the first HDD slice and
# returns a data.table in memory, we then apply summary to it
# You can use the special argument .N, as in data.table.

# the following query shows the first and last lines of
# each slice of the HDD data set:
data_hdd[c(1, .N), file = 1:.N]

# Extraction of observations for which the variable
# Petal.Width is lower than 0.1
data_hdd[Petal.Width &lt; 0.2, ]

# You can apply data.table syntax:
data_hdd[, .(pl = Petal.Length)]

# and create variables
data_hdd[, pl2 := Petal.Length**2]

# You can use the by clause, but then
# the by is applied slice by slice, NOT on the full data set:
data_hdd[, .(mean_pl = mean(Petal.Length)), by = Species]

# If the data you extract does not fit into memory,
# you can create a new HDD file with the argument 'newfile':
hdd_path_new = tempfile()
data_hdd[, pl2 := Petal.Length**2, newfile = hdd_path_new]
# check the result:
data_hdd_bis = hdd(hdd_path_new)
summary(data_hdd_bis)
print(data_hdd_bis)

</code></pre>

<hr>
<h2 id='+24.hdd'>Extracts a single variable from a HDD object</h2><span id='topic++24.hdd'></span>

<h3>Description</h3>

<p>This method extracts a single variable from a hard drive data set (HDD). There is an automatic protection to avoid extracting too large data into memory. The bound is set by the function <code><a href="#topic+setHdd_extract.cap">setHdd_extract.cap</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'hdd'
x$name
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="+2B24.hdd_+3A_x">x</code></td>
<td>
<p>A <code>HDD</code> object.</p>
</td></tr>
<tr><td><code id="+2B24.hdd_+3A_name">name</code></td>
<td>
<p>The variable name to be extracted.Note that there is an automatic protection for not trying to import data that would not fit into memory. The extraction cap is set with the function <code><a href="#topic+setHdd_extract.cap">setHdd_extract.cap</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>By default if the expected size of the variable to extract is greater than the value given by <code><a href="#topic+getHdd_extract.cap">getHdd_extract.cap</a></code> an error is raised.
For numeric variables, the expected size is exact. For non-numeric data, the expected size is a guess that considers all the non-numeric variables being of the same size. This may lead to an over or under estimation depending on the cases.
In any case, if your variable is large and you don't want to change the extraction cap (<code><a href="#topic+setHdd_extract.cap">setHdd_extract.cap</a></code>), you can still extract the variable with <code><a href="#topic+sub-.hdd">sub-.hdd</a></code> for which there is no such protection.
</p>
<p>Note that you cannot create variables with <code>$</code>, e.g. like <code>base_hdd$x_new &lt;- something</code>. To create variables, use the <code>[</code> instead (see <code><a href="#topic+sub-.hdd">sub-.hdd</a></code>).
</p>


<h3>Value</h3>

<p>It returns a vector.
</p>


<h3>Author(s)</h3>

<p>Laurent Berge
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+hdd">hdd</a></code>, <code><a href="#topic+sub-.hdd">sub-.hdd</a></code> and <code><a href="#topic+cash-.hdd">cash-.hdd</a></code>
for the extraction and manipulation of out of memory data. For importation of
HDD data sets from text files: see <code><a href="#topic+txt2hdd">txt2hdd</a></code>.
</p>
<p>See <code><a href="#topic+hdd_slice">hdd_slice</a></code> to apply functions to chunks of data (and create
HDD objects) and <code><a href="#topic+hdd_merge">hdd_merge</a></code> to merge large files.
</p>
<p>To create/reshape HDD objects from memory or from other HDD objects, see
<code><a href="#topic+write_hdd">write_hdd</a></code>.
</p>
<p>To display general information from HDD objects: <code><a href="#topic+origin">origin</a></code>,
<code><a href="#topic+summary.hdd">summary.hdd</a></code>, <code><a href="#topic+print.hdd">print.hdd</a></code>,
<code><a href="#topic+dim.hdd">dim.hdd</a></code> and <code><a href="#topic+names.hdd">names.hdd</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Toy example with iris data
# We first create a hdd dataset with approx. 100KB
hdd_path = tempfile() # =&gt; folder where the data will be saved
write_hdd(iris, hdd_path)
for(i in 1:10) write_hdd(iris, hdd_path, add = TRUE)

base_hdd = hdd(hdd_path)
summary(base_hdd) # =&gt; 11 files

# we can extract the data from the 11 files with '$':
pl = base_hdd$Sepal.Length

#
# Illustration of the protection mechanism:
#

# By default when extracting a variable with '$'
# and the size exceeds the cap (default is greater than 3GB)
# a confirmation is needed.
# You can set the cap with setHdd_extract.cap.

# Following asks for confirmation in interactive mode:
setHdd_extract.cap(sizeMB = 0.005) # new cap of 5KB
pl = base_hdd$Sepal.Length

# To extract the variable without changing the cap:
pl = base_hdd[, Sepal.Length] # =&gt; no size control is performed

# Resetting the default cap
setHdd_extract.cap()

</code></pre>

<hr>
<h2 id='dim.hdd'>Dimension of a HDD object</h2><span id='topic+dim.hdd'></span>

<h3>Description</h3>

<p>Gets the dimension of a hard drive data set (HDD).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'hdd'
dim(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dim.hdd_+3A_x">x</code></td>
<td>
<p>A <code>HDD</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>It returns a vector of length 2 containing the number of rows and the number of columns of the HDD object.
</p>


<h3>Author(s)</h3>

<p>Laurent Berge
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Toy example with iris data
iris_path = tempfile()
fwrite(iris, iris_path)

# destination path
hdd_path = tempfile()

# reading the text file with 50 rows chunks:
txt2hdd(iris_path, dirDest = hdd_path, rowsPerChunk = 50)

# creating a HDD object
base_hdd = hdd(hdd_path)

# Summary information on the whole data set
summary(base_hdd)

# Looking at it like a regular data.frame
print(base_hdd)
dim(base_hdd)
names(base_hdd)



</code></pre>

<hr>
<h2 id='guess_col_types'>Guesses the columns types of a file</h2><span id='topic+guess_col_types'></span>

<h3>Description</h3>

<p>This function is a facility to guess the column types of a text document. It returns columns formatted a la readr.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>guess_col_types(dt_or_path, col_names, n = 10000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="guess_col_types_+3A_dt_or_path">dt_or_path</code></td>
<td>
<p>Either a data frame or a path.</p>
</td></tr>
<tr><td><code id="guess_col_types_+3A_col_names">col_names</code></td>
<td>
<p>Optional: the vector of names of the columns, if not contained in the file. Must match the number of columns in the file.</p>
</td></tr>
<tr><td><code id="guess_col_types_+3A_n">n</code></td>
<td>
<p>Number of observations used to make the guess. By default, <code>n = 100000</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The guessing of the column types is based on the 10,000 (set with argument <code>n</code>) first rows.
</p>
<p>Note that by default, columns that are found to be integers are imported as double (in want of integer64 type in readr). Note that for large data sets, sometimes integer-like identifiers can be larger than 16 digits: in these case you must import them as character not to lose information.
</p>


<h3>Value</h3>

<p>It returns a <code><a href="readr.html#topic+cols">cols</a></code> object a la <code>readr</code>.
</p>


<h3>Author(s)</h3>

<p>Laurent Berge
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+peek">peek</a></code> to have a convenient look at the first lines of a text file. See <code><a href="#topic+guess_delim">guess_delim</a></code> to guess the delimiter of a text data set. See <code><a href="#topic+guess_col_types">guess_col_types</a></code> to guess the column types of a text data set.
</p>
<p>See <code><a href="#topic+hdd">hdd</a></code>, <code><a href="#topic+sub-.hdd">sub-.hdd</a></code> and <code><a href="#topic+cash-.hdd">cash-.hdd</a></code> for the extraction and manipulation of out of memory data. For importation of HDD data sets from text files: see <code><a href="#topic+txt2hdd">txt2hdd</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Example with the iris data set
iris_path = tempfile()
fwrite(iris, iris_path)

# returns a readr columns set:
guess_col_types(iris_path)


</code></pre>

<hr>
<h2 id='guess_delim'>Guesses the delimiter of a text file</h2><span id='topic+guess_delim'></span>

<h3>Description</h3>

<p>This function uses <code><a href="data.table.html#topic+fread">fread</a></code> to guess the delimiter of a text file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>guess_delim(path)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="guess_delim_+3A_path">path</code></td>
<td>
<p>The path to a text file containing a rectangular data set.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>It returns a character string of length 1: the delimiter.
</p>


<h3>Author(s)</h3>

<p>Laurent Berge
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+peek">peek</a></code> to have a convenient look at the first lines of a text file. See <code><a href="#topic+guess_delim">guess_delim</a></code> to guess the delimiter of a text data set. See <code><a href="#topic+guess_col_types">guess_col_types</a></code> to guess the column types of a text data set.
</p>
<p>See <code><a href="#topic+hdd">hdd</a></code>, <code><a href="#topic+sub-.hdd">sub-.hdd</a></code> and <code><a href="#topic+cash-.hdd">cash-.hdd</a></code> for the extraction and manipulation of out of memory data. For importation of HDD data sets from text files: see <code><a href="#topic+txt2hdd">txt2hdd</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Example with the iris data set
iris_path = tempfile()
fwrite(iris, iris_path)

guess_delim(iris_path)

</code></pre>

<hr>
<h2 id='hdd'>Hard drive data set</h2><span id='topic+hdd'></span>

<h3>Description</h3>

<p>This function connects to a hard drive data set (HDD). You can access the hard
drive data in a similar way to a <code>data.table</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hdd(dir)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hdd_+3A_dir">dir</code></td>
<td>
<p>The directory where the hard drive data set is.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>HDD has been created to deal with out of memory data sets. The data set exists
in the hard drive, split in multiple files &ndash; each file being workable in memory.
</p>
<p>You can perform extraction and manipulation operations as with a regular data
set with <code><a href="#topic+sub-.hdd">sub-.hdd</a></code>. Each operation is performed chunk-by-chunk
behind the scene.
</p>
<p>In terms of performance, working with complete data sets in memory will always
be faster. This is because read/write operations on disk are order of magnitude
slower than read/write in memory. However, this might be the only way to deal
with out of memory data.
</p>


<h3>Value</h3>

<p>This function returns an object of class <code>hdd</code> which is linked to
a folder on disk containing the data. The data is not loaded in R.
</p>
<p>This object is not intended to be interacted with directly as a regular list. Please use the methods
<code><a href="#topic+sub-.hdd">sub-.hdd</a></code> and <code><a href="#topic+cash-.hdd">cash-.hdd</a></code> to extract the data.
</p>


<h3>Author(s)</h3>

<p>Laurent Berge
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+hdd">hdd</a></code>, <code><a href="#topic+sub-.hdd">sub-.hdd</a></code> and <code><a href="#topic+cash-.hdd">cash-.hdd</a></code>
for the extraction and manipulation of out of memory data. For importation of
HDD data sets from text files: see <code><a href="#topic+txt2hdd">txt2hdd</a></code>.
</p>
<p>See <code><a href="#topic+hdd_slice">hdd_slice</a></code> to apply functions to chunks of data (and create
HDD objects) and <code><a href="#topic+hdd_merge">hdd_merge</a></code> to merge large files.
</p>
<p>To create/reshape HDD objects from memory or from other HDD objects, see
<code><a href="#topic+write_hdd">write_hdd</a></code>.
</p>
<p>To display general information from HDD objects: <code><a href="#topic+origin">origin</a></code>,
<code><a href="#topic+summary.hdd">summary.hdd</a></code>, <code><a href="#topic+print.hdd">print.hdd</a></code>,
<code><a href="#topic+dim.hdd">dim.hdd</a></code> and <code><a href="#topic+names.hdd">names.hdd</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Toy example with iris data
iris_path = tempfile()
fwrite(iris, iris_path)

# destination path
hdd_path = tempfile()

# reading the text file with 50 rows chunks:
txt2hdd(iris_path, dirDest = hdd_path, rowsPerChunk = 50)

# creating a HDD object
base_hdd = hdd(hdd_path)

# Summary information on the whole data set
summary(base_hdd)

# Looking at it like a regular data.frame
print(base_hdd)
dim(base_hdd)
names(base_hdd)



</code></pre>

<hr>
<h2 id='hdd_merge'>Merges data to a HDD file</h2><span id='topic+hdd_merge'></span>

<h3>Description</h3>

<p>This function merges in-memory/HDD data to a HDD file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hdd_merge(
  x,
  y,
  newfile,
  chunkMB,
  rowsPerChunk,
  all = FALSE,
  all.x = all,
  all.y = all,
  allow.cartesian = FALSE,
  replace = FALSE,
  verbose
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hdd_merge_+3A_x">x</code></td>
<td>
<p>A HDD object or a <code>data.frame</code>.</p>
</td></tr>
<tr><td><code id="hdd_merge_+3A_y">y</code></td>
<td>
<p>A data set either a data.frame of a HDD object.</p>
</td></tr>
<tr><td><code id="hdd_merge_+3A_newfile">newfile</code></td>
<td>
<p>Destination of the result, i.e., a destination folder that will
receive the HDD data.</p>
</td></tr>
<tr><td><code id="hdd_merge_+3A_chunkmb">chunkMB</code></td>
<td>
<p>Numeric, default is missing. If provided, the data 'x' is split
in chunks of 'chunkMB' MB and the merge is applied chunkwise.</p>
</td></tr>
<tr><td><code id="hdd_merge_+3A_rowsperchunk">rowsPerChunk</code></td>
<td>
<p>Integer, default is missing. If provided, the data 'x' is
split in chunks of 'rowsPerChunk' rows and the merge is applied chunkwise.</p>
</td></tr>
<tr><td><code id="hdd_merge_+3A_all">all</code></td>
<td>
<p>Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="hdd_merge_+3A_all.x">all.x</code></td>
<td>
<p>Default is <code>all</code>.</p>
</td></tr>
<tr><td><code id="hdd_merge_+3A_all.y">all.y</code></td>
<td>
<p>Default is <code>all</code>.</p>
</td></tr>
<tr><td><code id="hdd_merge_+3A_allow.cartesian">allow.cartesian</code></td>
<td>
<p>Logical: whether to allow cartesian merge. Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="hdd_merge_+3A_replace">replace</code></td>
<td>
<p>Default is <code>FALSE</code>: if the destination folder already contains
data, whether to replace it.</p>
</td></tr>
<tr><td><code id="hdd_merge_+3A_verbose">verbose</code></td>
<td>
<p>Numeric. Whether information on the advancement should be displayed.
If equal to 0, nothing is displayed. By default it is equal to 1 if the size
of <code>x</code> is greater than 1GB.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>x</code> (resp <code>y</code>) is a HDD object, then the merging will be operated
chunkwise, with the original chunks of the objects. To change the size of the
chunks for <code>x</code>: you can use the argument <code>chunkMB</code> or <code>rowsPerChunk.</code>
</p>
<p>To change the chunk size of <code>y</code>, you can rewrite <code>y</code> with a new chunk
size using <code><a href="#topic+write_hdd">write_hdd</a></code>.
</p>
<p>Note that the merging operation could also be achieved with <code><a href="#topic+hdd_slice">hdd_slice</a></code>
(although it would require setting up an ad hoc function).
</p>


<h3>Value</h3>

<p>This function does not return anything. It applies the merging between
two potentially large (out of memory) data set and saves them on disk at the location
of <code>newfile</code>, the destination folder which will be populated with .fst files
representing chunks of the resulting merge.
</p>
<p>To interact with the data (on disk) newly created, use the function <code><a href="#topic+hdd">hdd()</a></code>.
</p>


<h3>Author(s)</h3>

<p>Laurent Berge
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+hdd">hdd</a></code>, <code><a href="#topic+sub-.hdd">sub-.hdd</a></code> and <code><a href="#topic+cash-.hdd">cash-.hdd</a></code>
for the extraction and manipulation of out of memory data. For importation of
HDD data sets from text files: see <code><a href="#topic+txt2hdd">txt2hdd</a></code>.
</p>
<p>See <code><a href="#topic+hdd_slice">hdd_slice</a></code> to apply functions to chunks of data (and create
HDD objects) and <code><a href="#topic+hdd_merge">hdd_merge</a></code> to merge large files.
</p>
<p>To create/reshape HDD objects from memory or from other HDD objects, see
<code><a href="#topic+write_hdd">write_hdd</a></code>.
</p>
<p>To display general information from HDD objects: <code><a href="#topic+origin">origin</a></code>,
<code><a href="#topic+summary.hdd">summary.hdd</a></code>, <code><a href="#topic+print.hdd">print.hdd</a></code>,
<code><a href="#topic+dim.hdd">dim.hdd</a></code> and <code><a href="#topic+names.hdd">names.hdd</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Toy example with iris data

# Cartesian merge example
iris_bis = iris
names(iris_bis) = c(paste0("x_", 1:4), "species_bis")
# We must have a common key on which to merge
iris_bis$id = iris$id = 1

# merge, we chunk 'x' by 50 rows
hdd_path = tempfile()
hdd_merge(iris, iris_bis, newfile = hdd_path,
		  rowsPerChunk = 50, allow.cartesian = TRUE)

base_merged = hdd(hdd_path)
summary(base_merged)
print(base_merged)

</code></pre>

<hr>
<h2 id='hdd_setkey'>Sorts HDD objects</h2><span id='topic+hdd_setkey'></span>

<h3>Description</h3>

<p>This function sets a key to a HDD file. It creates a copy of the HDD file sorted
by the key. Note that the sorting process is very time consuming.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hdd_setkey(x, key, newfile, chunkMB = 500, replace = FALSE, verbose = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hdd_setkey_+3A_x">x</code></td>
<td>
<p>A hdd file.</p>
</td></tr>
<tr><td><code id="hdd_setkey_+3A_key">key</code></td>
<td>
<p>A character vector of the keys.</p>
</td></tr>
<tr><td><code id="hdd_setkey_+3A_newfile">newfile</code></td>
<td>
<p>Destination of the result, i.e., a destination folder that will
receive the HDD data.</p>
</td></tr>
<tr><td><code id="hdd_setkey_+3A_chunkmb">chunkMB</code></td>
<td>
<p>The size of chunks used to sort the data. Default is 500MB. The
bigger this number the faster the sorting is (depends on your memory available though).</p>
</td></tr>
<tr><td><code id="hdd_setkey_+3A_replace">replace</code></td>
<td>
<p>Default is <code>FALSE</code>: if the destination folder already contains
data, whether to replace it.</p>
</td></tr>
<tr><td><code id="hdd_setkey_+3A_verbose">verbose</code></td>
<td>
<p>Numeric, default is 1. Whether to display information on the advancement
of the algorithm. If equal to 0, nothing is displayed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is provided for convenience reason: it does the job of sorting
the data and ensuring consistency across files, but it is very slow since it
involves copying several times the entire data set. To be used parsimoniously.
</p>


<h3>Value</h3>

<p>This functions does not return anything in R, instead its result is a new
folder populated with <code>.fst</code> files which represent a data set that can be loaded
with the function <code><a href="#topic+hdd">hdd()</a></code>.
</p>


<h3>Author(s)</h3>

<p>Laurent Berge
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+hdd">hdd</a></code>, <code><a href="#topic+sub-.hdd">sub-.hdd</a></code> and <code><a href="#topic+cash-.hdd">cash-.hdd</a></code>
for the extraction and manipulation of out of memory data. For importation of
HDD data sets from text files: see <code><a href="#topic+txt2hdd">txt2hdd</a></code>.
</p>
<p>See <code><a href="#topic+hdd_slice">hdd_slice</a></code> to apply functions to chunks of data (and create
HDD objects) and <code><a href="#topic+hdd_merge">hdd_merge</a></code> to merge large files.
</p>
<p>To create/reshape HDD objects from memory or from other HDD objects, see
<code><a href="#topic+write_hdd">write_hdd</a></code>.
</p>
<p>To display general information from HDD objects: <code><a href="#topic+origin">origin</a></code>,
<code><a href="#topic+summary.hdd">summary.hdd</a></code>, <code><a href="#topic+print.hdd">print.hdd</a></code>,
<code><a href="#topic+dim.hdd">dim.hdd</a></code> and <code><a href="#topic+names.hdd">names.hdd</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Toy example with iris data

# Creating HDD data to be sorted
hdd_path = tempfile() # =&gt; folder where the data will be saved
write_hdd(iris, hdd_path)
# Let's add data to it
for(i in 1:5) write_hdd(iris, hdd_path, add = TRUE)

base_hdd = hdd(hdd_path)
summary(base_hdd)

# Sorting by Sepal.Width
hdd_sorted = tempfile()
# we use a very small chunkMB to show how the function works
hdd_setkey(base_hdd, key = "Sepal.Width",
		   newfile = hdd_sorted, chunkMB = 0.010)


base_hdd_sorted = hdd(hdd_sorted)
summary(base_hdd_sorted) # =&gt; additional line "Sorted by:"
print(base_hdd_sorted)

# Sort with two keys:
hdd_sorted = tempfile()
# we use a very small chunkMB to show how the function works
hdd_setkey(base_hdd, key = c("Species", "Sepal.Width"),
		   newfile = hdd_sorted, chunkMB = 0.010)


base_hdd_sorted = hdd(hdd_sorted)
summary(base_hdd_sorted)
print(base_hdd_sorted)

</code></pre>

<hr>
<h2 id='hdd_slice'>Applies a function to slices of data to create a HDD data set</h2><span id='topic+hdd_slice'></span>

<h3>Description</h3>

<p>This function is useful to apply complex R functions to large data sets (out of memory). It slices the input data, applies the function, then saves each chunk into a hard drive folder. This can then be a HDD data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hdd_slice(
  x,
  fun,
  dir,
  chunkMB = 500,
  rowsPerChunk,
  replace = FALSE,
  verbose = 1,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hdd_slice_+3A_x">x</code></td>
<td>
<p>A data set (data.frame, HDD).</p>
</td></tr>
<tr><td><code id="hdd_slice_+3A_fun">fun</code></td>
<td>
<p>A function to be applied to slices of the data set. The function must return a data frame like object.</p>
</td></tr>
<tr><td><code id="hdd_slice_+3A_dir">dir</code></td>
<td>
<p>The destination directory where the data is saved.</p>
</td></tr>
<tr><td><code id="hdd_slice_+3A_chunkmb">chunkMB</code></td>
<td>
<p>The size of the slices, default is 500MB. That is: the function <code>fun</code> is applied to each 500Mb of data <code>x</code>. If the function creates a lot of additional information, you may want this number to go down. On the other hand, if the function reduces the information you may want this number to go up. In the end it will depend on the amount of memory available.</p>
</td></tr>
<tr><td><code id="hdd_slice_+3A_rowsperchunk">rowsPerChunk</code></td>
<td>
<p>Integer, default is missing. Alternative to the argument <code>chunkMB</code>. If provided, the functions will be applied to chunks of <code>rowsPerChunk</code> of <code>x</code>.</p>
</td></tr>
<tr><td><code id="hdd_slice_+3A_replace">replace</code></td>
<td>
<p>Whether all information on the destination directory should be erased beforehand. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="hdd_slice_+3A_verbose">verbose</code></td>
<td>
<p>Integer, defaults to 1. If greater than 0 then the progress is displayed.</p>
</td></tr>
<tr><td><code id="hdd_slice_+3A_...">...</code></td>
<td>
<p>Other parameters to be passed to <code>fun</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function splits the original data into several slices and then apply a function to each of them, saving the results into a HDD data set.
</p>
<p>You can perform merging operations with <code>hdd_slice</code>, but for regular merges not that you have the function <code><a href="#topic+hdd_merge">hdd_merge</a></code> that may prove more convenient (not need to write a ad hoc function).
</p>


<h3>Value</h3>

<p>It doesn't return anything, the output is a &quot;hard drive data&quot; saved in the hard drive.
</p>


<h3>Author(s)</h3>

<p>Laurent Berge
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+hdd">hdd</a></code>, <code><a href="#topic+sub-.hdd">sub-.hdd</a></code> and <code><a href="#topic+cash-.hdd">cash-.hdd</a></code>
for the extraction and manipulation of out of memory data. For importation of
HDD data sets from text files: see <code><a href="#topic+txt2hdd">txt2hdd</a></code>.
</p>
<p>See <code><a href="#topic+hdd_slice">hdd_slice</a></code> to apply functions to chunks of data (and create
HDD objects) and <code><a href="#topic+hdd_merge">hdd_merge</a></code> to merge large files.
</p>
<p>To create/reshape HDD objects from memory or from other HDD objects, see
<code><a href="#topic+write_hdd">write_hdd</a></code>.
</p>
<p>To display general information from HDD objects: <code><a href="#topic+origin">origin</a></code>,
<code><a href="#topic+summary.hdd">summary.hdd</a></code>, <code><a href="#topic+print.hdd">print.hdd</a></code>,
<code><a href="#topic+dim.hdd">dim.hdd</a></code> and <code><a href="#topic+names.hdd">names.hdd</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Toy example with iris data.
# Say you want to perform a cartesian merge
# If the results of the function is out of memory
# you can use hdd_slice (not the case for this example)

# preparing the cartesian merge
iris_bis = iris
names(iris_bis) = c(paste0("x_", 1:4), "species_bis")


fun_cartesian = function(x){
	# Note that x is treated as a data.table
	# =&gt; we need the argument allow.cartesian
	merge(x, iris_bis, allow.cartesian = TRUE)
}

hdd_result = tempfile() # =&gt; folder where results are saved
hdd_slice(iris, fun_cartesian, dir = hdd_result, rowsPerChunk = 30)

# Let's look at the result
base_hdd = hdd(hdd_result)
summary(base_hdd)
head(base_hdd)



</code></pre>

<hr>
<h2 id='names.hdd'>Variables names of a HDD object</h2><span id='topic+names.hdd'></span>

<h3>Description</h3>

<p>Gets the variable names of a hard drive data set (HDD).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'hdd'
names(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="names.hdd_+3A_x">x</code></td>
<td>
<p>A <code>HDD</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector.
</p>


<h3>Author(s)</h3>

<p>Laurent Berge
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+hdd">hdd</a></code>, <code><a href="#topic+sub-.hdd">sub-.hdd</a></code> and <code><a href="#topic+cash-.hdd">cash-.hdd</a></code>
for the extraction and manipulation of out of memory data. For importation of
HDD data sets from text files: see <code><a href="#topic+txt2hdd">txt2hdd</a></code>.
</p>
<p>See <code><a href="#topic+hdd_slice">hdd_slice</a></code> to apply functions to chunks of data (and create
HDD objects) and <code><a href="#topic+hdd_merge">hdd_merge</a></code> to merge large files.
</p>
<p>To create/reshape HDD objects from memory or from other HDD objects, see
<code><a href="#topic+write_hdd">write_hdd</a></code>.
</p>
<p>To display general information from HDD objects: <code><a href="#topic+origin">origin</a></code>,
<code><a href="#topic+summary.hdd">summary.hdd</a></code>, <code><a href="#topic+print.hdd">print.hdd</a></code>,
<code><a href="#topic+dim.hdd">dim.hdd</a></code> and <code><a href="#topic+names.hdd">names.hdd</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Toy example with iris data
iris_path = tempfile()
fwrite(iris, iris_path)

# destination path
hdd_path = tempfile()

# reading the text file with 50 rows chunks:
txt2hdd(iris_path, dirDest = hdd_path, rowsPerChunk = 50)

# creating a HDD object
base_hdd = hdd(hdd_path)

# Summary information on the whole data set
summary(base_hdd)

# Looking at it like a regular data.frame
print(base_hdd)
dim(base_hdd)
names(base_hdd)



</code></pre>

<hr>
<h2 id='origin'>Extracts the origin of a HDD object</h2><span id='topic+origin'></span>

<h3>Description</h3>

<p>Use this function to extract the information on how the HDD data set was created.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>origin(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="origin_+3A_x">x</code></td>
<td>
<p>A HDD object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Each HDD lives on disk and a &ldquo;_hdd.txt&rdquo; is always present in the folder containing summary information. The function <code>origin</code> extracts the log from this information file.
</p>


<h3>Value</h3>

<p>A character vector, if the HDD data set has been created with several instances of <code><a href="#topic+write_hdd">write_hdd</a></code> its length will be greater than 1.
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+hdd">hdd</a></code>, <code><a href="#topic+sub-.hdd">sub-.hdd</a></code> and <code><a href="#topic+cash-.hdd">cash-.hdd</a></code>
for the extraction and manipulation of out of memory data. For importation of
HDD data sets from text files: see <code><a href="#topic+txt2hdd">txt2hdd</a></code>.
</p>
<p>See <code><a href="#topic+hdd_slice">hdd_slice</a></code> to apply functions to chunks of data (and create
HDD objects) and <code><a href="#topic+hdd_merge">hdd_merge</a></code> to merge large files.
</p>
<p>To create/reshape HDD objects from memory or from other HDD objects, see
<code><a href="#topic+write_hdd">write_hdd</a></code>.
</p>
<p>To display general information from HDD objects: <code><a href="#topic+origin">origin</a></code>,
<code><a href="#topic+summary.hdd">summary.hdd</a></code>, <code><a href="#topic+print.hdd">print.hdd</a></code>,
<code><a href="#topic+dim.hdd">dim.hdd</a></code> and <code><a href="#topic+names.hdd">names.hdd</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Toy example with iris data

hdd_path = tempfile()
write_hdd(iris, hdd_path, rowsPerChunk = 20)

base_hdd = hdd(hdd_path)
origin(base_hdd)

# Let's add something
write_hdd(head(iris), hdd_path, add = TRUE)
write_hdd(iris, hdd_path, add = TRUE, rowsPerChunk = 50)

base_hdd = hdd(hdd_path)
origin(base_hdd)


</code></pre>

<hr>
<h2 id='peek'>Peek into a text file</h2><span id='topic+peek'></span>

<h3>Description</h3>

<p>This function looks at the first elements of a file, format it into a data frame and displays it. It can also just show the first lines of the file without formatting into a DF.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>peek(path, onlyLines = FALSE, n, view = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="peek_+3A_path">path</code></td>
<td>
<p>Path linking to the text file.</p>
</td></tr>
<tr><td><code id="peek_+3A_onlylines">onlyLines</code></td>
<td>
<p>Default is <code>FALSE</code>. If <code>TRUE</code>, then the first <code>n</code> lines are directly displayed without formatting.</p>
</td></tr>
<tr><td><code id="peek_+3A_n">n</code></td>
<td>
<p>Integer. The number of lines to extract from the file. Default is 100 or 5 if <code>onlyLine = TRUE</code>.</p>
</td></tr>
<tr><td><code id="peek_+3A_view">view</code></td>
<td>
<p>Logical, default it <code>TRUE</code>: whether the data should be displayed on the viewer. Only when <code>onlyLines = FALSE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns the data invisibly.
</p>


<h3>Author(s)</h3>

<p>Laurent Berge
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+peek">peek</a></code> to have a convenient look at the first lines of a text file. See <code><a href="#topic+guess_delim">guess_delim</a></code> to guess the delimiter of a text data set. See <code><a href="#topic+guess_col_types">guess_col_types</a></code> to guess the column types of a text data set.
</p>
<p>See <code><a href="#topic+hdd">hdd</a></code>, <code><a href="#topic+sub-.hdd">sub-.hdd</a></code> and <code><a href="#topic+cash-.hdd">cash-.hdd</a></code> for the extraction and manipulation of out of memory data. For importation of HDD data sets from text files: see <code><a href="#topic+txt2hdd">txt2hdd</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Example with the iris data set
iris_path = tempfile()
fwrite(iris, iris_path)

# The first lines of the text file on viewer
peek(iris_path)

# displaying the first lines:
peek(iris_path, onlyLines = TRUE)

# only getting the data from the first observations
base = peek(iris_path, view = FALSE)
head(base)

</code></pre>

<hr>
<h2 id='print.hdd'>Print method for HDD objects</h2><span id='topic+print.hdd'></span>

<h3>Description</h3>

<p>This functions displays the first and last lines of a hard drive data set (HDD).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'hdd'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.hdd_+3A_x">x</code></td>
<td>
<p>A <code>HDD</code> object.</p>
</td></tr>
<tr><td><code id="print.hdd_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Returns the first and last 3 lines of a HDD object. Also formats the values displayed on screen (typically: add commas to increase the readability of large integers).
</p>


<h3>Value</h3>

<p>Nothing is returned.
</p>


<h3>Author(s)</h3>

<p>Laurent Berge
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+hdd">hdd</a></code>, <code><a href="#topic+sub-.hdd">sub-.hdd</a></code> and <code><a href="#topic+cash-.hdd">cash-.hdd</a></code>
for the extraction and manipulation of out of memory data. For importation of
HDD data sets from text files: see <code><a href="#topic+txt2hdd">txt2hdd</a></code>.
</p>
<p>See <code><a href="#topic+hdd_slice">hdd_slice</a></code> to apply functions to chunks of data (and create
HDD objects) and <code><a href="#topic+hdd_merge">hdd_merge</a></code> to merge large files.
</p>
<p>To create/reshape HDD objects from memory or from other HDD objects, see
<code><a href="#topic+write_hdd">write_hdd</a></code>.
</p>
<p>To display general information from HDD objects: <code><a href="#topic+origin">origin</a></code>,
<code><a href="#topic+summary.hdd">summary.hdd</a></code>, <code><a href="#topic+print.hdd">print.hdd</a></code>,
<code><a href="#topic+dim.hdd">dim.hdd</a></code> and <code><a href="#topic+names.hdd">names.hdd</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Toy example with iris data
iris_path = tempfile()
fwrite(iris, iris_path)

# destination path
hdd_path = tempfile()

# reading the text file with 50 rows chunks:
txt2hdd(iris_path, dirDest = hdd_path, rowsPerChunk = 50)

# creating a HDD object
base_hdd = hdd(hdd_path)

# Summary information on the whole data set
summary(base_hdd)

# Looking at it like a regular data.frame
print(base_hdd)
dim(base_hdd)
names(base_hdd)



</code></pre>

<hr>
<h2 id='readfst'>Read fst or HDD files as DT</h2><span id='topic+readfst'></span>

<h3>Description</h3>

<p>This is the function <code><a href="fst.html#topic+read_fst">read_fst</a></code> but with automatic conversion
to data.table. It also allows to read <code>hdd</code> data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>readfst(path, columns = NULL, from = 1, to = NULL, confirm = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="readfst_+3A_path">path</code></td>
<td>
<p>Path to <code>fst</code> file &ndash; or path to <code>hdd</code> data. For hdd files,
there is a</p>
</td></tr>
<tr><td><code id="readfst_+3A_columns">columns</code></td>
<td>
<p>Column names to read. The default is to read all columns. Ignored
for <code>hdd</code> files.</p>
</td></tr>
<tr><td><code id="readfst_+3A_from">from</code></td>
<td>
<p>Read data starting from this row number. Ignored for <code>hdd</code> files.</p>
</td></tr>
<tr><td><code id="readfst_+3A_to">to</code></td>
<td>
<p>Read data up until this row number. The default is to read to the last
row of the stored data set. Ignored for <code>hdd</code> files.</p>
</td></tr>
<tr><td><code id="readfst_+3A_confirm">confirm</code></td>
<td>
<p>If the HDD file is larger than ten times the variable <code>getHdd_extract.cap()</code>,
then by default an error is raised. To anyway read the data, use <code>confirm = TRUE</code>.
You can set the data cap with the function <code><a href="#topic+setHdd_extract.cap">setHdd_extract.cap</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function reads one or several <code>.fst</code> files and place them in a single
data table.
</p>


<h3>Value</h3>

<p>This function returns a data table located in memory. It allows to read in memory
the <code>hdd</code> data saved on disk.
</p>


<h3>Author(s)</h3>

<p>Laurent Berge
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+hdd">hdd</a></code>, <code><a href="#topic+sub-.hdd">sub-.hdd</a></code> and <code><a href="#topic+cash-.hdd">cash-.hdd</a></code>
for the extraction and manipulation of out of memory data. For importation of
HDD data sets from text files: see <code><a href="#topic+txt2hdd">txt2hdd</a></code>.
</p>
<p>See <code><a href="#topic+hdd_slice">hdd_slice</a></code> to apply functions to chunks of data (and create
HDD objects) and <code><a href="#topic+hdd_merge">hdd_merge</a></code> to merge large files.
</p>
<p>To create/reshape HDD objects from memory or from other HDD objects, see
<code><a href="#topic+write_hdd">write_hdd</a></code>.
</p>
<p>To display general information from HDD objects: <code><a href="#topic+origin">origin</a></code>,
<code><a href="#topic+summary.hdd">summary.hdd</a></code>, <code><a href="#topic+print.hdd">print.hdd</a></code>,
<code><a href="#topic+dim.hdd">dim.hdd</a></code> and <code><a href="#topic+names.hdd">names.hdd</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Toy example with the iris data set

# writing a hdd file
hdd_path = tempfile()
write_hdd(iris, hdd_path, rowsPerChunk = 30)

# reading the full data in memory
base_mem = readfst(hdd_path)

# is equivalent to:
base_hdd = hdd(hdd_path)
base_mem_bis = base_hdd[]


</code></pre>

<hr>
<h2 id='setHdd_extract.cap'>Sets/gets the size cap when extracting hdd data</h2><span id='topic+setHdd_extract.cap'></span><span id='topic+getHdd_extract.cap'></span>

<h3>Description</h3>

<p>Sets/gets the default size cap when extracting HDD variables with <code><a href="#topic+cash-.hdd">cash-.hdd</a></code> or when importing full HDD data sets with <code><a href="#topic+readfst">readfst</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setHdd_extract.cap(sizeMB = 3000)

getHdd_extract.cap
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="setHdd_extract.cap_+3A_sizemb">sizeMB</code></td>
<td>
<p>Size cap in MB. Default is 3000.</p>
</td></tr>
</table>


<h3>Format</h3>

<p>An object of class <code>function</code> of length 1.
</p>


<h3>Details</h3>

<p>In <code><a href="#topic+readfst">readfst</a></code>, if the expected size of the data set exceeds the cap then,
in interactive mode, a confirmation is asked. When not in interactive mode, no confirmation is asked.
This can also be bypassed by using the argument <code>confirm</code>.
</p>


<h3>Value</h3>

<p>The size cap, a numeric scalar.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Toy example with iris data
# We first create a hdd dataset with approx. 100KB
hdd_path = tempfile() # =&gt; folder where the data will be saved
write_hdd(iris, hdd_path)
for(i in 1:10) write_hdd(iris, hdd_path, add = TRUE)

base_hdd = hdd(hdd_path)
summary(base_hdd) # =&gt; 11 files

# we can extract the data from the 11 files with '$':
pl = base_hdd$Sepal.Length

#
# Illustration of the protection mechanism:
#

# By default when extracting a variable with '$'
# and the size exceeds the cap (default is greater than 3GB)
# a confirmation is needed.
# You can set the cap with setHdd_extract.cap.

# Following code asks a confirmation:
setHdd_extract.cap(sizeMB = 0.005) # new cap of 5KB
try(pl &lt;- base_hdd$Sepal.Length)

# To extract the variable without changing the cap:
pl = base_hdd[, Sepal.Length] # =&gt; no size control is performed

# Resetting the default cap
setHdd_extract.cap()


</code></pre>

<hr>
<h2 id='summary.hdd'>Summary information for HDD objects</h2><span id='topic+summary.hdd'></span>

<h3>Description</h3>

<p>Provides summary information &ndash; i.e. dimension, size on disk, path, number of
slices &ndash; of hard drive data sets (HDD).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'hdd'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.hdd_+3A_object">object</code></td>
<td>
<p>A HDD object.</p>
</td></tr>
<tr><td><code id="summary.hdd_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Displays concisely general information on the HDD object: its size on disk, the
number of files it is made of, its location on disk and the number of rows and columns.
</p>
<p>Note that each HDD object contain the text file &ldquo;_hdd.txt&rdquo; in their folder
also containing this information.
</p>
<p>To obtain how the HDD object was constructed, use function <code><a href="#topic+origin">origin</a></code>.
</p>


<h3>Value</h3>

<p>This function does not return anything. It only prints general information
on the data set in the console.
</p>


<h3>Author(s)</h3>

<p>Laurent Berge
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+hdd">hdd</a></code>, <code><a href="#topic+sub-.hdd">sub-.hdd</a></code> and <code><a href="#topic+cash-.hdd">cash-.hdd</a></code>
for the extraction and manipulation of out of memory data. For importation of
HDD data sets from text files: see <code><a href="#topic+txt2hdd">txt2hdd</a></code>.
</p>
<p>See <code><a href="#topic+hdd_slice">hdd_slice</a></code> to apply functions to chunks of data (and create
HDD objects) and <code><a href="#topic+hdd_merge">hdd_merge</a></code> to merge large files.
</p>
<p>To create/reshape HDD objects from memory or from other HDD objects, see
<code><a href="#topic+write_hdd">write_hdd</a></code>.
</p>
<p>To display general information from HDD objects: <code><a href="#topic+origin">origin</a></code>,
<code><a href="#topic+summary.hdd">summary.hdd</a></code>, <code><a href="#topic+print.hdd">print.hdd</a></code>,
<code><a href="#topic+dim.hdd">dim.hdd</a></code> and <code><a href="#topic+names.hdd">names.hdd</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Toy example with iris data
iris_path = tempfile()
fwrite(iris, iris_path)

# destination path
hdd_path = tempfile()

# reading the text file with 50 rows chunks:
txt2hdd(iris_path, dirDest = hdd_path, rowsPerChunk = 50)

# creating a HDD object
base_hdd = hdd(hdd_path)

# Summary information on the whole data set
summary(base_hdd)

# Looking at it like a regular data.frame
print(base_hdd)
dim(base_hdd)
names(base_hdd)



</code></pre>

<hr>
<h2 id='txt2hdd'>Transforms text data into a HDD file</h2><span id='topic+txt2hdd'></span>

<h3>Description</h3>

<p>Imports text data and saves it into a HDD file. It uses <code><a href="readr.html#topic+read_delim_chunked">read_delim_chunked</a></code>
to extract the data. It also allows to preprocess the data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>txt2hdd(
  path,
  dirDest,
  chunkMB = 500,
  rowsPerChunk,
  col_names,
  col_types,
  nb_skip,
  delim,
  preprocessfun,
  replace = FALSE,
  encoding = "UTF-8",
  verbose = 0,
  locale = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="txt2hdd_+3A_path">path</code></td>
<td>
<p>Character vector that represents the path to the data. Note that
it can be equal to patterns if multiple files with the same name are to be imported
(if so it must be a fixed pattern, NOT a regular expression).</p>
</td></tr>
<tr><td><code id="txt2hdd_+3A_dirdest">dirDest</code></td>
<td>
<p>The destination directory, where the new HDD data should be saved.</p>
</td></tr>
<tr><td><code id="txt2hdd_+3A_chunkmb">chunkMB</code></td>
<td>
<p>The chunk sizes in MB, defaults to 500MB. Instead of using this
argument, you can alternatively use the argument <code>rowsPerChunk</code> which decides
the size of chunks in terms of lines.</p>
</td></tr>
<tr><td><code id="txt2hdd_+3A_rowsperchunk">rowsPerChunk</code></td>
<td>
<p>Number of rows per chunk. By default it is missing: its value
is deduced from argument <code>chunkMB</code> and the size of the file. If provided,
replaces any value provided in <code>chunkMB</code>.</p>
</td></tr>
<tr><td><code id="txt2hdd_+3A_col_names">col_names</code></td>
<td>
<p>The column names, by default is uses the ones of the data set.
If the data set lacks column names, you must provide them.</p>
</td></tr>
<tr><td><code id="txt2hdd_+3A_col_types">col_types</code></td>
<td>
<p>The column types, in the <code>readr</code> fashion. You can use <code><a href="#topic+guess_col_types">guess_col_types</a></code>
to find them.</p>
</td></tr>
<tr><td><code id="txt2hdd_+3A_nb_skip">nb_skip</code></td>
<td>
<p>Number of lines to skip.</p>
</td></tr>
<tr><td><code id="txt2hdd_+3A_delim">delim</code></td>
<td>
<p>The delimiter. By default the function tries to find the delimiter, but sometimes it fails.</p>
</td></tr>
<tr><td><code id="txt2hdd_+3A_preprocessfun">preprocessfun</code></td>
<td>
<p>A function that is applied to the data before saving. Default
is missing. Note that if a function is provided, it MUST return a data.frame,
anything other than data.frame is ignored.</p>
</td></tr>
<tr><td><code id="txt2hdd_+3A_replace">replace</code></td>
<td>
<p>If the destination directory already exists, you need to set the
argument <code>replace=TRUE</code> to overwrite all the HDD files in it.</p>
</td></tr>
<tr><td><code id="txt2hdd_+3A_encoding">encoding</code></td>
<td>
<p>Character scalar containing the encoding of the file to be read.
By default it is &quot;UTF-8&quot; and is passed to the <code>readr</code> function <code><a href="readr.html#topic+locale">locale</a></code> which is used
in <code><a href="readr.html#topic+read_delim_chunked">read_delim_chunked</a></code> (the reading function). A common encoding in Western Europe is
&quot;ISO-8859-1&quot; (simply use &quot;file filename&quot; in a non-Windows console to get the encoding).
</p>
<p>Note that this argument is ignored if the argument <code>locale</code> is not NULL.</p>
</td></tr>
<tr><td><code id="txt2hdd_+3A_verbose">verbose</code></td>
<td>
<p>Logical scalar or <code>NULL</code> (default). If <code>TRUE</code>, then the evolution of
the importing process as well as the time to import are reported.
If <code>NULL</code>, it becomes <code>TRUE</code> when the data to import is greater than 5GB or there are
more than one chunk.</p>
</td></tr>
<tr><td><code id="txt2hdd_+3A_locale">locale</code></td>
<td>
<p>Either <code>NULL</code> (default), either an object created with <code><a href="readr.html#topic+locale">locale</a></code>.
This object will be passed to the reading function <code><a href="readr.html#topic+read_delim_chunked">read_delim_chunked</a></code> and handles
how the data is imported.</p>
</td></tr>
<tr><td><code id="txt2hdd_+3A_...">...</code></td>
<td>
<p>Other arguments to be passed to <code><a href="readr.html#topic+read_delim_chunked">read_delim_chunked</a></code>,
<code>quote = ""</code> can be interesting sometimes.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function uses <code><a href="readr.html#topic+read_delim_chunked">read_delim_chunked</a></code> from <code>readr</code>
to read a large text file per chunk, and generate a HDD data set.
</p>
<p>Since the main function for importation uses <code>readr</code>, the column specification
must also be in readr's style (namely <code><a href="readr.html#topic+cols">cols</a></code> or <code><a href="readr.html#topic+cols_only">cols_only</a></code>).
</p>
<p>By default a guess of the column types is made on the first 10,000 rows. The
guess is the application of <code><a href="#topic+guess_col_types">guess_col_types</a></code> on these rows.
</p>
<p>Note that by default, columns that are found to be integers are imported as double
(in want of integer64 type in readr). Note that for large data sets, sometimes
integer-like identifiers can be larger than 16 digits: in these case you must
import them as character not to lose information.
</p>
<p>The delimiter is found with the function <code><a href="#topic+guess_delim">guess_delim</a></code>, which
uses the guessing from <code><a href="data.table.html#topic+fread">fread</a></code>. Note that fixed width
delimited files are not supported.
</p>


<h3>Value</h3>

<p>This function does not return anything in R. Instead it creates a folder
on disk containing <code>.fst</code> files. These files represent the data that has been
imported and converted to the <code>hdd</code> format.
</p>
<p>You can then read the created data with the function <code><a href="#topic+hdd">hdd()</a></code>.
</p>


<h3>Author(s)</h3>

<p>Laurent Berge
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+hdd">hdd</a></code>, <code><a href="#topic+sub-.hdd">sub-.hdd</a></code> and <code><a href="#topic+cash-.hdd">cash-.hdd</a></code>
for the extraction and manipulation of out of memory data. For importation of
HDD data sets from text files: see <code><a href="#topic+txt2hdd">txt2hdd</a></code>.
</p>
<p>See <code><a href="#topic+hdd_slice">hdd_slice</a></code> to apply functions to chunks of data (and create
HDD objects) and <code><a href="#topic+hdd_merge">hdd_merge</a></code> to merge large files.
</p>
<p>To create/reshape HDD objects from memory or from other HDD objects, see
<code><a href="#topic+write_hdd">write_hdd</a></code>.
</p>
<p>To display general information from HDD objects: <code><a href="#topic+origin">origin</a></code>,
<code><a href="#topic+summary.hdd">summary.hdd</a></code>, <code><a href="#topic+print.hdd">print.hdd</a></code>,
<code><a href="#topic+dim.hdd">dim.hdd</a></code> and <code><a href="#topic+names.hdd">names.hdd</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Toy example with iris data

# we create a text file on disk
iris_path = tempfile()
fwrite(iris, iris_path)

# destination path
hdd_path = tempfile()
# reading the text file with HDD, with approx. 50 rows per chunk:
txt2hdd(iris_path, dirDest = hdd_path, rowsPerChunk = 50)

base_hdd = hdd(hdd_path)
summary(base_hdd)

# Same example with preprocessing
sl_keep = sort(unique(sample(iris$Sepal.Length, 40)))
fun = function(x){
	# we keep only some observations &amp; vars + renaming
	res = x[Sepal.Length %in% sl_keep, .(sl = Sepal.Length, Species)]
	# we create some variables
	res[, sl2 := sl**2]
	res
}
# reading with preprocessing
hdd_path_preprocess = tempfile()
txt2hdd(iris_path, hdd_path_preprocess,
		preprocessfun = fun, rowsPerChunk = 50)

base_hdd_preprocess = hdd(hdd_path_preprocess)
summary(base_hdd_preprocess)


</code></pre>

<hr>
<h2 id='write_hdd'>Saves or appends a data set into a HDD file</h2><span id='topic+write_hdd'></span>

<h3>Description</h3>

<p>This function saves in-memory/HDD data sets into HDD repositories. Useful to
append several data sets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>write_hdd(
  x,
  dir,
  chunkMB = Inf,
  rowsPerChunk,
  compress = 50,
  add = FALSE,
  replace = FALSE,
  showWarning,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="write_hdd_+3A_x">x</code></td>
<td>
<p>A data set.</p>
</td></tr>
<tr><td><code id="write_hdd_+3A_dir">dir</code></td>
<td>
<p>The HDD repository, i.e. the directory where the HDD data is.</p>
</td></tr>
<tr><td><code id="write_hdd_+3A_chunkmb">chunkMB</code></td>
<td>
<p>If the data has to be split in several files of <code>chunkMB</code>
sizes. Default is <code>Inf</code>.</p>
</td></tr>
<tr><td><code id="write_hdd_+3A_rowsperchunk">rowsPerChunk</code></td>
<td>
<p>Integer, default is missing. Alternative to the argument
<code>chunkMB</code>. If provided, the data will be split in several files of <code>rowsPerChunk</code>
rows.</p>
</td></tr>
<tr><td><code id="write_hdd_+3A_compress">compress</code></td>
<td>
<p>Compression rate to be applied by <code><a href="fst.html#topic+write_fst">write_fst</a></code>.
Default is 50.</p>
</td></tr>
<tr><td><code id="write_hdd_+3A_add">add</code></td>
<td>
<p>Should the file be added to the existing repository? Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="write_hdd_+3A_replace">replace</code></td>
<td>
<p>If <code>add = FALSE</code>, should any existing document be replaced?
Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="write_hdd_+3A_showwarning">showWarning</code></td>
<td>
<p>If the data <code>x</code> has no observation, then a warning is
raised if <code>showWarning = TRUE</code>. By default, it occurs only if <code>write_hdd</code>
is NOT called within a function.</p>
</td></tr>
<tr><td><code id="write_hdd_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Creating a HDD data set with this function always create an additional file named
&ldquo;_hdd.txt&rdquo; in the HDD folder. This file contains summary information on
the data: the number of rows, the number of variables, the first five lines and
a log of how the HDD data set has been created. To access the log directly from
<code>R</code>, use the function <code><a href="#topic+origin">origin</a></code>.
</p>


<h3>Value</h3>

<p>This function does not return anything in R. Instead it creates a folder
on disk containing <code>.fst</code> files. These files represent the data that has been
converted to the <code>hdd</code> format.
</p>
<p>You can then read the created data with the function <code><a href="#topic+hdd">hdd()</a></code>.
</p>


<h3>Author(s)</h3>

<p>Laurent Berge
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+hdd">hdd</a></code>, <code><a href="#topic+sub-.hdd">sub-.hdd</a></code> and <code><a href="#topic+cash-.hdd">cash-.hdd</a></code>
for the extraction and manipulation of out of memory data. For importation of
HDD data sets from text files: see <code><a href="#topic+txt2hdd">txt2hdd</a></code>.
</p>
<p>See <code><a href="#topic+hdd_slice">hdd_slice</a></code> to apply functions to chunks of data (and create
HDD objects) and <code><a href="#topic+hdd_merge">hdd_merge</a></code> to merge large files.
</p>
<p>To create/reshape HDD objects from memory or from other HDD objects, see
<code><a href="#topic+write_hdd">write_hdd</a></code>.
</p>
<p>To display general information from HDD objects: <code><a href="#topic+origin">origin</a></code>,
<code><a href="#topic+summary.hdd">summary.hdd</a></code>, <code><a href="#topic+print.hdd">print.hdd</a></code>,
<code><a href="#topic+dim.hdd">dim.hdd</a></code> and <code><a href="#topic+names.hdd">names.hdd</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Toy example with iris data

# Let's create a HDD data set from iris data
hdd_path = tempfile() # =&gt; folder where the data will be saved
write_hdd(iris, hdd_path)
# Let's add data to it
for(i in 1:10) write_hdd(iris, hdd_path, add = TRUE)

base_hdd = hdd(hdd_path)
summary(base_hdd) # =&gt; 11 files, 1650 lines, 48.7KB on disk

# Let's save the iris data by chunks of 1KB
# we use replace = TRUE to delete the previous data
write_hdd(iris, hdd_path, chunkMB = 0.001, replace = TRUE)

base_hdd = hdd(hdd_path)
summary(base_hdd) # =&gt; 8 files, 150 lines, 10.2KB on disk

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
