<!DOCTYPE html><html lang="en"><head><title>Help for package PEPBVS</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {PEPBVS}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#PEPBVS-package'><p>Bayesian variable selection using power&ndash;expected&ndash;posterior prior</p></a></li>
<li><a href='#comparepriors.lm'><p>Selected models under different choices of prior on the model parameters and</p>
the model space</a></li>
<li><a href='#estimation.pep'><p>Model averaged estimates</p></a></li>
<li><a href='#image.pep'><p>Heatmap for top models</p></a></li>
<li><a href='#pep.lm'><p>Bayesian variable selection for Gaussian linear models using PEP through</p>
exhaustive search or with the MC3 algorithm</a></li>
<li><a href='#PEPBVS-deprecated'><p>Deprecated functions in package <span class="pkg">PEPBVS</span>.</p></a></li>
<li><a href='#peptest'><p>Bayes factor for model comparison</p></a></li>
<li><a href='#plot.pep'><p>Plots for object of class pep</p></a></li>
<li><a href='#posteriorpredictive.pep'><p>Posterior predictive distribution under Bayesian model averaging</p></a></li>
<li><a href='#predict.pep'><p>(Point) Prediction under PEP approach</p></a></li>
<li><a href='#print.pep'><p>Printing object of class pep</p></a></li>
<li><a href='#UScrime_data'><p>US Crime Data</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Bayesian Variable Selection using Power-Expected-Posterior Prior</td>
</tr>
<tr>
<td>Version:</td>
<td>2.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-10-30</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Konstantina Charmpi &lt;xarmpi.kon@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Performs Bayesian variable selection under normal linear
          models for the data with the model parameters following as prior distributions either 
          the power-expected-posterior (PEP) or the intrinsic (a special case of the former)
          (Fouskakis and Ntzoufras (2022) &lt;<a href="https://doi.org/10.1214%2F21-BA1288">doi:10.1214/21-BA1288</a>&gt;,
          Fouskakis and Ntzoufras (2020) &lt;<a href="https://doi.org/10.3390%2Feconometrics8020017">doi:10.3390/econometrics8020017</a>&gt;).          
          The prior distribution on model space is the uniform over all models
          or the uniform on model dimension (a special case of the beta-binomial prior). 
          The selection is performed by either implementing a full enumeration 
          and evaluation of all possible models or using the Markov Chain 
          Monte Carlo Model Composition (MC3) algorithm (Madigan and York (1995) &lt;<a href="https://doi.org/10.2307%2F1403615">doi:10.2307/1403615</a>&gt;). 
          Complementary functions for hypothesis testing, estimation and 
          predictions under Bayesian model averaging, as well as, plotting and 
          printing the results are also provided. The results can be compared to the
          ones obtained under other well-known priors on model parameters and model spaces.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Imports:</td>
<td>BAS, BayesVarSel, Matrix, mcmcse, mvtnorm, Rcpp (&ge; 1.0.9)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo, RcppGSL</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>GNU GSL</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-11-11 16:32:30 UTC; user</td>
</tr>
<tr>
<td>Author:</td>
<td>Konstantina Charmpi [aut, cre],
  Dimitris Fouskakis [aut],
  Ioannis Ntzoufras [aut]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-11-12 09:50:09 UTC</td>
</tr>
</table>
<hr>
<h2 id='PEPBVS-package'>Bayesian variable selection using power&ndash;expected&ndash;posterior prior</h2><span id='topic+PEPBVS-package'></span>

<h3>Description</h3>

<p>Performs Bayesian variable selection under normal linear
models for the data with the model parameters following as prior distributions 
either the PEP or the intrinsic (a special case of the former). 
The prior distribution on model space is the 
uniform over all models or the uniform on model dimension (a special case 
of the beta&ndash;binomial prior). Posterior model probabilities and marginal
likelihoods can be derived in closed&ndash;form expressions under this setup. 
The selection is performed by either implementing a full enumeration 
and evaluation of all possible models (for model spaces of 
small&ndash;to&ndash;moderate dimension) or using the MC3 algorithm 
(for model spaces of large dimension). Complementary functions for hypothesis testing, 
estimation and predictions under Bayesian model averaging, 
as well as plotting and printing the results are also available. Selected
models can be compared to those arising from other well&ndash;known priors.
</p>


<h3>Details</h3>

<p>_PACKAGE
</p>


<h3>References</h3>

<p>Bayarri, M., Berger, J., Forte, A. and Garcia&ndash;Donato, G. (2012) 
Criteria for Bayesian Model Choice with Application to Variable Selection. 
The Annals of Statistics, 40(3): 1550–1577. <a href="https://doi.org/10.1214/12-AOS1013">doi:10.1214/12-AOS1013</a>
</p>
<p>Fouskakis, D. and Ntzoufras, I. (2022) Power&ndash;Expected&ndash;Posterior 
Priors as Mixtures of g&ndash;Priors in Normal Linear Models. 
Bayesian Analysis, 17(4): 1073-1099. <a href="https://doi.org/10.1214/21-BA1288">doi:10.1214/21-BA1288</a>
</p>
<p>Fouskakis, D. and Ntzoufras, I. (2020) Bayesian Model Averaging Using 
Power&ndash;Expected&ndash;Posterior Priors. 
Econometrics, 8(2): 17. <a href="https://doi.org/10.3390/econometrics8020017">doi:10.3390/econometrics8020017</a>
</p>
<p>Garcia&ndash;Donato, G. and Forte, A. (2018) Bayesian Testing, 
Variable Selection and Model Averaging in Linear Models using R with 
BayesVarSel. The R Journal, 10(1): 155–174. 
<a href="https://doi.org/10.32614/RJ-2018-021">doi:10.32614/RJ-2018-021</a>
</p>
<p>Kass, R. and Raftery, A. (1995) Bayes Factors. 
Journal of the American Statistical Association, 90(430): 773–795. 
<a href="https://doi.org/10.1080/01621459.1995.10476572">doi:10.1080/01621459.1995.10476572</a>
</p>
<p>Ley, E. and Steel, M. (2012) Mixtures of g&ndash;Priors for Bayesian Model 
Averaging with Economic Applications. 
Journal of Econometrics, 171(2): 251–266. 
<a href="https://doi.org/10.1016/j.jeconom.2012.06.009">doi:10.1016/j.jeconom.2012.06.009</a>
</p>
<p>Liang, F., Paulo, R., Molina, G., Clyde, M. and Berger, J. (2008) 
Mixtures of g Priors for Bayesian Variable Selection. 
Journal of the American Statistical Association, 103(481): 410–423.
<a href="https://doi.org/10.1198/016214507000001337">doi:10.1198/016214507000001337</a>
</p>
<p>Raftery, A., Madigan, D. and Hoeting, J. (1997) Bayesian Model Averaging 
for Linear Regression Models. 
Journal of the American Statistical Association, 92(437): 179–191.
<a href="https://doi.org/10.1080/01621459.1997.10473615">doi:10.1080/01621459.1997.10473615</a>
</p>
<p>Zellner, A. (1976) Bayesian and Non&ndash;Bayesian Analysis of the Regression 
Model with Multivariate Student&ndash;t Error Terms. 
Journal of the American Statistical Association, 71(354): 400–405. 
<a href="https://doi.org/10.1080/01621459.1976.10480357">doi:10.1080/01621459.1976.10480357</a>
</p>
<p>Zellner, A. and Siow, A. (1980) Posterior Odds Ratios for Selected 
Regression Hypotheses.
Trabajos de Estadistica Y de Investigacion Operativa, 31: 585-603. 
<a href="https://doi.org/10.1007/BF02888369">doi:10.1007/BF02888369</a>
</p>

<hr>
<h2 id='comparepriors.lm'>Selected models under different choices of prior on the model parameters and 
the model space</h2><span id='topic+comparepriors.lm'></span>

<h3>Description</h3>

<p>Given a formula and a data frame, computes the maximum a posteriori (MAP) model  
and median probability model (MPM) 
for different choices of prior on the model parameters and the model space.
Normal linear models are assumed for the data with the prior distribution on the model 
parameters being one or more of the following: PEP, intrinsic, Zellner’s 
<code class="reqn">g</code>&ndash;prior, Zellner and Siow, benchmark, robust, hyper&ndash;<code class="reqn">g</code> and related hyper&ndash;<code class="reqn">g</code>&ndash;<code class="reqn">n</code>.
The prior distribution on the model space can be either the uniform on models
or the uniform on the model dimension (special case of the beta&ndash;binomial prior).
The model space consists of all possible models including an intercept term.
Model selection is performed by using either full enumeration 
and evaluation of all models  
(for model spaces of small&ndash;to&ndash;moderate dimension) or a
Markov chain Monte Carlo (MCMC) scheme (for 
model spaces of large dimension).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>comparepriors.lm(
  formula,
  data,
  algorithmic.choice = "automatic",
  priorbetacoeff = c("PEP", "intrinsic", "Robust", "gZellner", "ZellnerSiow", "FLS",
    "hyper-g", "hyper-g-n"),
  reference.prior = c(TRUE, FALSE),
  priormodels = c("beta-binomial", "uniform"),
  burnin = 1000,
  itermcmc = 11000
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="comparepriors.lm_+3A_formula">formula</code></td>
<td>
<p>A formula, defining the full model.</p>
</td></tr>
<tr><td><code id="comparepriors.lm_+3A_data">data</code></td>
<td>
<p>A data frame (of numeric values), containing the data.</p>
</td></tr>
<tr><td><code id="comparepriors.lm_+3A_algorithmic.choice">algorithmic.choice</code></td>
<td>
<p>A character, the type of algorithm to be used
for selection: full enumeration and evaluation of all models or an MCMC scheme. 
One of &ldquo;automatic&rdquo; (the choice is done automatically based on the number
of explanatory variables in the full model), &ldquo;full enumeration&rdquo; 
or &ldquo;MCMC&rdquo;. Default value=<code>"automatic"</code>.</p>
</td></tr>
<tr><td><code id="comparepriors.lm_+3A_priorbetacoeff">priorbetacoeff</code></td>
<td>
<p>A vector of character containing the different priors on the model
parameters. The character can be one of &ldquo;PEP&rdquo;, &ldquo;intrinsic&rdquo;, &ldquo;Robust&rdquo;, &ldquo;gZellner&rdquo;, 
&ldquo;ZellnerSiow&rdquo;, &ldquo;FLS&rdquo;, &ldquo;hyper&ndash;g&rdquo; and &ldquo;hyper&ndash;g&ndash;n&rdquo;. 
<br /> Default value=<code>
c("PEP","intrinsic","Robust", "gZellner","ZellnerSiow",</code>
<code>"FLS","hyper-g","hyper-g-n")</code>,
i.e., all supported priors are tested.</p>
</td></tr>
<tr><td><code id="comparepriors.lm_+3A_reference.prior">reference.prior</code></td>
<td>
<p>A vector of logical indicating the baseline prior that is used for 
PEP/intrinsic. It can be TRUE (reference prior is used), FALSE (dependence Jeffreys prior 
is used) or both. Default value=<code>c(TRUE,FALSE)</code>, i.e., both baseline priors are tested.</p>
</td></tr>
<tr><td><code id="comparepriors.lm_+3A_priormodels">priormodels</code></td>
<td>
<p>A vector of character containing the different priors on the model
space. The character can be one of &ldquo;beta&ndash;binomial&rdquo; and &ldquo;uniform&rdquo;. 
<br /> Default value=<code>c("beta-binomial","uniform")</code>, i.e., both supported priors are tested.</p>
</td></tr>
<tr><td><code id="comparepriors.lm_+3A_burnin">burnin</code></td>
<td>
<p>Non&ndash;negative integer, the burnin period for the MCMC scheme.
Default value=1000.</p>
</td></tr>
<tr><td><code id="comparepriors.lm_+3A_itermcmc">itermcmc</code></td>
<td>
<p>Positive integer (larger than <code>burnin</code>),
the (total) number of iterations for the MCMC scheme. Default value=11000.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The different priors on the model parameters are implemented using 
different packages: for PEP and intrinsic, the current package is used. 
For hyper&ndash;<code class="reqn">g</code> and related hyper&ndash;<code class="reqn">g</code>&ndash;n priors, the R package <span class="pkg">BAS</span> is used. Finally, 
for the Zellner’s <code class="reqn">g</code>&ndash;prior (&ldquo;gZellner&rdquo;), the Zellner and Siow 
(&ldquo;ZellnerSiow&rdquo;), the robust and the benchmark (&ldquo;FLS&rdquo;) prior,
the results are obtained using <span class="pkg">BayesVarSel</span>.
</p>
<p>The prior distribution on the model space can be either the uniform on models
or the beta&ndash;binomial. For the beta&ndash;binomial prior, 
the following special case is used: uniform prior on model dimension.
</p>
<p>When an MCMC scheme is used, the R package <span class="pkg">BAS</span> uses the birth/death random walk 
in Raftery et al. (1997) combined with a random swap move, <span class="pkg">BayesVarSel</span> 
uses Gibbs sampling while <span class="pkg">PEPBVS</span> implements the MC3 algorithm
described in the Appendix of Fouskakis and Ntzoufras (2022). 
</p>
<p>To assess MCMC convergence, Monte Carlo (MC) standard error is 
computed using batch means estimator (implemented in the R package <span class="pkg">mcmcse</span>).
For computing a standard error, the number (<code>itermcmc</code>-<code>burnin</code>) 
needs to be larger than 100.  
This quantity cannot be computed for the cases treated by <span class="pkg">BAS</span> &mdash; 
since all &lsquo;visited&rsquo; models are not available in the function output &mdash; and thus for those cases 
<code>NA</code> is depicted in the relevant column instead. 
</p>
<p>Similar to <code><a href="#topic+pep.lm">pep.lm</a></code>, if <code>algorithmic.choice</code> equals &ldquo;automatic&rdquo; then 
model selection is implemented as follows: if <code class="reqn">p &lt; 20</code> (where <code class="reqn">p</code> is the 
number of explanatory variables in the full model without the intercept), full enumeration 
and evaluation of all models is performed, otherwise an MCMC scheme is used.
To avoid potential memory or time constraints, if <code>algorithmic.choice</code> 
equals &ldquo;full enumeration&rdquo; but <code class="reqn">p \geq 20</code>, once issuing a warning message,
an MCMC scheme is used instead.
</p>
<p>Similar constraints to <code><a href="#topic+pep.lm">pep.lm</a></code> hold for the data, i.e., 
the case of missing data is not currently supported, the explanatory 
variables need to be quantitative and cannot have an exact linear relationship, 
and <code class="reqn">p\leq n-2</code> (<code class="reqn">n</code> being the sample size).
</p>


<h3>Value</h3>

<p><code>comparepriors.lm</code> returns a list with two elements:
</p>
<table role = "presentation">
<tr><td><code>MAPmodels</code></td>
<td>
<p>A data frame containing the MAP models for all different combinations of 
prior on the model parameters and the model space. In particular, in row <code class="reqn">i</code> the following
information is presented: prior on the model parameters, prior on the model space, 
hyperparameter value, MAP model (corresponding to the specific combination 
of priors on model parameters and model space) 
represented with variable inclusion indicators, and the R package used. When an MCMC scheme
has been used, there are two additional columns: one depicting the specific algorithm 
that has been used and one with the MC standard error (to assess convergence).
With an MCMC scheme, the MAP model output corresponds to the most frequently
&lsquo;visited&rsquo;.</p>
</td></tr>
<tr><td><code>MPMmodels</code></td>
<td>
<p>Same as the first element containing the MPM models instead.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Bayarri, M., Berger, J., Forte, A. and Garcia&ndash;Donato, G. (2012) 
Criteria for Bayesian Model Choice with Application to Variable Selection. 
The Annals of Statistics, 40(3): 1550–1577. <a href="https://doi.org/10.1214/12-AOS1013">doi:10.1214/12-AOS1013</a>
</p>
<p>Fouskakis, D. and Ntzoufras, I. (2022) Power&ndash;Expected&ndash;Posterior 
Priors as Mixtures of g&ndash;Priors in Normal Linear Models. 
Bayesian Analysis, 17(4): 1073-1099. <a href="https://doi.org/10.1214/21-BA1288">doi:10.1214/21-BA1288</a>
</p>
<p>Ley, E. and Steel, M. (2012) Mixtures of g&ndash;Priors for Bayesian Model 
Averaging with Economic Applications. 
Journal of Econometrics, 171(2): 251–266. 
<a href="https://doi.org/10.1016/j.jeconom.2012.06.009">doi:10.1016/j.jeconom.2012.06.009</a>
</p>
<p>Liang, F., Paulo, R., Molina, G., Clyde, M. and Berger, J. (2008) 
Mixtures of g Priors for Bayesian Variable Selection. 
Journal of the American Statistical Association, 103(481): 410–423.
<a href="https://doi.org/10.1198/016214507000001337">doi:10.1198/016214507000001337</a>
</p>
<p>Raftery, A., Madigan, D. and Hoeting, J. (1997) Bayesian Model Averaging 
for Linear Regression Models. 
Journal of the American Statistical Association, 92(437): 179–191.
<a href="https://doi.org/10.1080/01621459.1997.10473615">doi:10.1080/01621459.1997.10473615</a>
</p>
<p>Zellner, A. (1976) Bayesian and Non&ndash;Bayesian Analysis of the Regression 
Model with Multivariate Student&ndash;t Error Terms. 
Journal of the American Statistical Association, 71(354): 400–405. 
<a href="https://doi.org/10.1080/01621459.1976.10480357">doi:10.1080/01621459.1976.10480357</a>
</p>
<p>Zellner, A. and Siow, A. (1980) Posterior Odds Ratios for Selected 
Regression Hypotheses.
Trabajos de Estadistica Y de Investigacion Operativa, 31: 585-603. 
<a href="https://doi.org/10.1007/BF02888369">doi:10.1007/BF02888369</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(UScrime_data)
resc &lt;- comparepriors.lm(y~.,UScrime_data,
                         priorbetacoeff = c("PEP","hyper-g-n"),
                         reference.prior = TRUE,priormodels = "beta-binomial")

</code></pre>

<hr>
<h2 id='estimation.pep'>Model averaged estimates</h2><span id='topic+estimation.pep'></span>

<h3>Description</h3>

<p>Simulates values from the (joint) posterior distribution of the 
beta coefficients under Bayesian model averaging.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimation.pep(
  object,
  ssize = 10000,
  estimator = "BMA",
  n.models = NULL,
  cumul.prob = 0.99
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="estimation.pep_+3A_object">object</code></td>
<td>
<p>An object of class pep (e.g., output of <code>pep.lm</code>).</p>
</td></tr>
<tr><td><code id="estimation.pep_+3A_ssize">ssize</code></td>
<td>
<p>Positive integer, the number of values to be simulated from
the (joint) posterior distribution of the beta coefficients.
Default value=10000.</p>
</td></tr>
<tr><td><code id="estimation.pep_+3A_estimator">estimator</code></td>
<td>
<p>A character, the type of estimation. One of 
&ldquo;BMA&rdquo; (Bayesian model averaging, default), 
&ldquo;MAP&rdquo; (maximum a posteriori model) or &ldquo;MPM&rdquo; (median probability model).
Default value=<code>"BMA"</code>.</p>
</td></tr>
<tr><td><code id="estimation.pep_+3A_n.models">n.models</code></td>
<td>
<p>Positive integer, the number of (top) models where
the average is based on or <code>NULL</code>. Relevant for <code>estimator="BMA"</code>.
Default value=<code>NULL</code>.</p>
</td></tr>
<tr><td><code id="estimation.pep_+3A_cumul.prob">cumul.prob</code></td>
<td>
<p>Numeric between zero and one, cumulative probability of
top models to be used for computing the average. Relevant for <code>estimator="BMA"</code>. 
Default value=0.99.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For the computations, Equation 10 of Garcia&ndash;Donato and Forte (2018) 
is used. That (simplified) formula arises when changing the prior on the
model parameters to the reference prior. This change of prior is
justified in Garcia&ndash;Donato and Forte (2018). The resulting formula is a mixture
distribution and the simulation is implemented as follows: firstly the 
model (component) based on its posterior probability is chosen and 
subsequently the values of the beta coefficients included in the chosen model are
drawn from the corresponding multivariate Student distribution, while the
values of the beta coefficents outside the chosen model are set to zero.
</p>
<p>Let <code class="reqn">k</code> be the number of models with cumulative posterior probability up 
to the given value of <code>cumul.prob</code>. Then, for Bayesian model averaging 
the summation is based on the top <code class="reqn">(k+1)</code> models if they exist, otherwise
on the top <code class="reqn">k</code> models.
</p>
<p>When both <code>n.models</code> and <code>cumul.prob</code> are provided &mdash; once 
specifying the number of models for the given cumulative probability as 
described above &mdash; the minimum between the two numbers is used for estimation.
</p>


<h3>Value</h3>

<p><code>estimation.pep</code> returns a matrix (of dimension 
<code>ssize</code> <code class="reqn">\times \, (p+1)</code>) &mdash; 
where the rows correspond
to the simulations and the columns to the beta coefficients
(including the intercept) &mdash; containing the 
simulated data.
</p>


<h3>References</h3>

<p>Garcia&ndash;Donato, G. and Forte, A. (2018) Bayesian Testing, 
Variable Selection and Model Averaging in Linear Models using R with 
BayesVarSel. The R Journal, 10(1): 155–174. 
<a href="https://doi.org/10.32614/RJ-2018-021">doi:10.32614/RJ-2018-021</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(UScrime_data)
res &lt;- pep.lm(y~.,data=UScrime_data)
set.seed(123)
estM1 &lt;- estimation.pep(res,ssize=2000)
estM2 &lt;- estimation.pep(res,ssize=2000,estimator="MPM")
</code></pre>

<hr>
<h2 id='image.pep'>Heatmap for top models</h2><span id='topic+image.pep'></span>

<h3>Description</h3>

<p>Generates a heatmap where the rows correspond to the (top) models
and the columns to the input/explanatory variables. The value depicted
in cell <code class="reqn">(i,j)</code> corresponds to the posterior inclusion probability 
of variable <code class="reqn">i</code> if this
is included in model <code class="reqn">j</code> and zero otherwise.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pep'
image(x, n.models = 20, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="image.pep_+3A_x">x</code></td>
<td>
<p>An object of class pep (e.g., output of <code>pep.lm</code>).</p>
</td></tr>
<tr><td><code id="image.pep_+3A_n.models">n.models</code></td>
<td>
<p>Positive integer, number of models to be shown on the 
heatmap. Default value=20.</p>
</td></tr>
<tr><td><code id="image.pep_+3A_...">...</code></td>
<td>
<p>Additional parameters to be passed to <code>heatmap</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The number of models to be displayed on the heatmap is computed as the minimum
between the number asked by the user and the number of models present in
the object <code>x</code>.
</p>
<p>The color code is as follows: the darker the blue in the figure, the 
higher the posterior inclusion probability is, while white means that the 
variable is not included in the model.
</p>
<p>In the special case of no explanatory variables, no heatmap is produced
and a message is printed.
</p>


<h3>Value</h3>

<p>No return value, used for heatmap generation.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.pep">plot.pep</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(UScrime_data)
set.seed(123)
resu &lt;- pep.lm(y~.,data=UScrime_data,beta.binom=FALSE,
               algorithmic.choice="MC3",itermc3=5000)
image(resu)
image(resu,n.models=10)

</code></pre>

<hr>
<h2 id='pep.lm'>Bayesian variable selection for Gaussian linear models using PEP through 
exhaustive search or with the MC3 algorithm</h2><span id='topic+pep.lm'></span>

<h3>Description</h3>

<p>Given a formula and a data frame, performs Bayesian 
variable selection using either full enumeration and evaluation of 
all models in the model space (for model spaces of 
small&ndash;to&ndash;moderate dimension) or the MC3 algorithm (for 
model spaces of large dimension). Normal linear models are assumed for the data with the 
prior distribution on the model parameters (beta coefficients and 
error variance) being the PEP or the intrinsic. 
The prior distribution on the model space can be the uniform on models 
or the uniform on the model dimension (special case of the beta&ndash;binomial prior).
The model space consists of all possible models including an intercept term.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pep.lm(
  formula,
  data,
  algorithmic.choice = "automatic",
  intrinsic = FALSE,
  reference.prior = TRUE,
  beta.binom = TRUE,
  ml_constant.term = FALSE,
  burnin = 1000,
  itermc3 = 11000
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pep.lm_+3A_formula">formula</code></td>
<td>
<p>A formula, defining the full model.</p>
</td></tr>
<tr><td><code id="pep.lm_+3A_data">data</code></td>
<td>
<p>A data frame (of numeric values), containing the data.</p>
</td></tr>
<tr><td><code id="pep.lm_+3A_algorithmic.choice">algorithmic.choice</code></td>
<td>
<p>A character, the type of algorithm to be used
for selection: full enumeration and evaluation of all models or the MC3 algorithm. 
One of &ldquo;automatic&rdquo; (the choice is done automatically based on the number
of explanatory variables in the full model), &ldquo;full enumeration&rdquo; 
or &ldquo;MC3&rdquo;. Default value=<code>"automatic"</code>.</p>
</td></tr>
<tr><td><code id="pep.lm_+3A_intrinsic">intrinsic</code></td>
<td>
<p>Logical, indicating whether the PEP 
(<code>FALSE</code>) or the intrinsic &mdash; which   
is a special case of it &mdash; (<code>TRUE</code>) should be used as prior on the  
regression parameters. Default value=<code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="pep.lm_+3A_reference.prior">reference.prior</code></td>
<td>
<p>Logical, indicating whether the reference prior
(<code>TRUE</code>) or the dependence Jeffreys prior (<code>FALSE</code>) is used as 
baseline. Default value=<code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="pep.lm_+3A_beta.binom">beta.binom</code></td>
<td>
<p>Logical, indicating whether the beta&ndash;binomial 
distribution (<code>TRUE</code>) or the uniform distribution (<code>FALSE</code>)    
should be used as prior on the model space. Default value=<code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="pep.lm_+3A_ml_constant.term">ml_constant.term</code></td>
<td>
<p>Logical, indicating whether the constant
(marginal likelihood of the null/intercept&ndash;only model) should be
included in computing the marginal likelihood of a model (<code>TRUE</code>)  
or not (<code>FALSE</code>). Default value=<code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="pep.lm_+3A_burnin">burnin</code></td>
<td>
<p>Non&ndash;negative integer, the burnin period for the MC3 algorithm.
Default value=1000.</p>
</td></tr>
<tr><td><code id="pep.lm_+3A_itermc3">itermc3</code></td>
<td>
<p>Positive integer (larger than <code>burnin</code>),
the (total) number of iterations for the MC3 algorithm. Default value=11000.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function works when <code class="reqn">p\leq n-2</code>, where <code class="reqn">p</code> is the number of explanatory variables
of the full model and <code class="reqn">n</code> is the sample size.
</p>
<p>The reference model is the null model (i.e., intercept&ndash;only model).
</p>
<p>The case of missing data (i.e., presence of <code>NA</code>'s either in the  
response or the explanatory variables) is not currently supported. Further,
the data needs to be quantitative.
</p>
<p>All models considered (i.e., model space) include an intercept term.
</p>
<p>If <code class="reqn">p&gt;1</code>, the explanatory variables cannot have an exact linear relationship 
(perfect multicollinearity).
</p>
<p>The reference prior as baseline corresponds to hyperparameter values
<code class="reqn">d0=0</code> and <code class="reqn">d1=0</code>, while the dependence Jeffreys prior corresponds to 
model&ndash;dependent&ndash;based values for the hyperparameters <code class="reqn">d0</code> and <code class="reqn">d1</code>,
see Fouskakis and Ntzoufras (2022) for more details.
</p>
<p>For computing the marginal likelihood of a model, Equation 16 of 
Fouskakis and Ntzoufras (2022) is used.
</p>
<p>When <code>ml_constant.term=FALSE</code> then the log marginal likelihood of a
model in the output is shifted by -logC1
(logC1: log marginal likelihood of the null model).
</p>
<p>When the prior on the model space is beta&ndash;binomial 
(i.e., <code>beta.binom=TRUE</code>), the following special case is used: uniform 
prior on model dimension. 
</p>
<p>If <code>algorithmic.choice</code> equals &ldquo;automatic&rdquo; then the choice of 
the selection algorithm is as follows: if <code class="reqn">p &lt; 20</code>, full enumeration 
and evaluation of all models in the model space is performed, 
otherwise the MC3 algorithm is used.
To avoid potential memory or time constraints, if <code>algorithmic.choice</code> 
equals &ldquo;full enumeration&rdquo; but <code class="reqn">p \geq 20</code> then the MC3 algorithm is 
used instead (once issuing a warning message).
</p>
<p>The MC3 algorithm was first introduced by Madigan and York (1995)
while its current implementation is described in the Appendix 
of Fouskakis and Ntzoufras (2022).
</p>


<h3>Value</h3>

<p><code>pep.lm</code> returns an object of class <code>pep</code>, 
i.e., a list with the following elements:
</p>
<table role = "presentation">
<tr><td><code>models</code></td>
<td>
<p>A matrix containing information about the models examined. 
In particular, in row <code class="reqn">i</code> after representing model <code class="reqn">i</code> with variable inclusion 
indicators, its marginal likelihood (in log scale), the R2, its dimension  
(including the intercept), the corresponding Bayes factor, 
posterior odds and its posterior probability are contained. The models
are sorted in decreasing order of the posterior probability. For the 
Bayes factor and the posterior odds, the comparison is made with the model 
with the highest posterior probability. The number of rows of this first list element
is <code class="reqn">2^{p}</code> with full enumeration of all possible models, or equal to 
the number of unique models &lsquo;visited&rsquo; by the algorithm, if MC3 was run.
Further, for MC3, the posterior probability of a model corresponds to
the estimated posterior probability as this is computed by the relative
Monte Carlo frequency of the &lsquo;visited&rsquo; models by the MC3 algorithm.</p>
</td></tr>
<tr><td><code>inc.probs</code></td>
<td>
<p>A named vector with the posterior inclusion probabilities of the 
explanatory variables.</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>The input data matrix (of dimension <code class="reqn">n\times p</code>), i.e., matrix containing 
the values of the <code class="reqn">p</code> explanatory variables (without the intercept).</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>The response vector (of length <code class="reqn">n</code>).</p>
</td></tr>
<tr><td><code>fullmodel</code></td>
<td>
<p>Formula, representing the full model.</p>
</td></tr>
<tr><td><code>mapp</code></td>
<td>
<p>For <code class="reqn">p\geq 2</code>, a matrix (of dimension <code class="reqn">p\times 2</code>) containing the mapping between 
the explanatory variables and the Xi's, where the <code class="reqn">i</code>&ndash;th explanatory variable
is denoted by Xi. If <code class="reqn">p&lt;2</code>, <code>NULL</code>.</p>
</td></tr>
<tr><td><code>intrinsic</code></td>
<td>
<p>Whether the prior on the model parameters was PEP or intrinsic.</p>
</td></tr>
<tr><td><code>reference.prior</code></td>
<td>
<p>Whether the baseline prior was the reference prior
or the dependence Jeffreys prior.</p>
</td></tr>
<tr><td><code>beta.binom</code></td>
<td>
<p>Whether the prior on the model space was beta&ndash;binomial or
uniform.</p>
</td></tr>
</table>
<p>When MC3 is run, there is the additional list element <code>allvisitedmodsM</code>, a matrix of 
dimension (<code>itermcmc</code>-<code>burnin</code>) <code class="reqn">\times \,(p+2)</code> containing all &lsquo;visited&rsquo; models 
(as variable inclusion indicators together with their corresponding 
marginal likelihood and R2) by the MC3 algorithm after the burnin period.
</p>


<h3>References</h3>

<p>Fouskakis, D. and Ntzoufras, I. (2022) Power&ndash;Expected&ndash;Posterior 
Priors as Mixtures of g&ndash;Priors in Normal Linear Models. 
Bayesian Analysis, 17(4): 1073-1099. <a href="https://doi.org/10.1214/21-BA1288">doi:10.1214/21-BA1288</a>
</p>
<p>Madigan, D. and York, J. (1995) Bayesian Graphical Models for Discrete Data.
International Statistical Review, 63(2): 215–232. <a href="https://doi.org/10.2307/1403615">doi:10.2307/1403615</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(UScrime_data)
res &lt;- pep.lm(y~.,data=UScrime_data)
resu &lt;- pep.lm(y~.,data=UScrime_data,beta.binom=FALSE)
resi &lt;- pep.lm(y~.,data=UScrime_data,intrinsic=TRUE)
set.seed(123)
res2 &lt;- pep.lm(y~.,data=UScrime_data,algorithmic.choice="MC3",itermc3=2000)
resj2 &lt;- pep.lm(y~.,data=UScrime_data,reference.prior=FALSE,
               algorithmic.choice="MC3",burnin=20,itermc3=1800) 

</code></pre>

<hr>
<h2 id='PEPBVS-deprecated'>Deprecated functions in package <span class="pkg">PEPBVS</span>.</h2><span id='topic+PEPBVS-deprecated'></span><span id='topic+full_enumeration_pep'></span><span id='topic+mc3_pep'></span>

<h3>Description</h3>

<p>The functions listed below are deprecated. 
Alternative functions with similar functionality are mentioned.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>full_enumeration_pep(...)

mc3_pep(...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="PEPBVS-deprecated_+3A_...">...</code></td>
<td>
<p>Deprecated.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Defunct functions are: <code>full_enumeration_pep</code> and <code>mc3_pep</code>.
Please replace both with <code><a href="#topic+pep.lm">pep.lm</a></code>
</p>

<hr>
<h2 id='peptest'>Bayes factor for model comparison</h2><span id='topic+peptest'></span>

<h3>Description</h3>

<p>Given two models to be compared (the one nested to the other), 
computes the corresponding Bayes factor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>peptest(formula1, formula2, data, intrinsic = FALSE, reference.prior = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="peptest_+3A_formula1">formula1</code></td>
<td>
<p>One of the two formulas/models to be compared.</p>
</td></tr>
<tr><td><code id="peptest_+3A_formula2">formula2</code></td>
<td>
<p>The second formula/model. The one model 
needs to be nested to the other.</p>
</td></tr>
<tr><td><code id="peptest_+3A_data">data</code></td>
<td>
<p>A data frame (of numeric values), containing the data.</p>
</td></tr>
<tr><td><code id="peptest_+3A_intrinsic">intrinsic</code></td>
<td>
<p>Logical, indicating whether the PEP 
(<code>FALSE</code>) or the intrinsic &mdash; which   
is a special case of it &mdash; (<code>TRUE</code>) should be used as prior on the  
regression parameters. Default value=<code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="peptest_+3A_reference.prior">reference.prior</code></td>
<td>
<p>Logical, indicating whether the reference prior
(<code>TRUE</code>) or the dependence Jeffreys prior (<code>FALSE</code>) is used as 
baseline. Default value=<code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function can be used to perform hypothesis testing indirectly. More
specifically, for the interpretation of the result (Bayes factor), the table in 
Kass and Raftery (1995) can be used.
</p>
<p>The function works when <code class="reqn">p\leq n-2</code>, where <code class="reqn">p</code> is the number of explanatory 
variables in the more complex model and <code class="reqn">n</code> is the sample size.
</p>
<p>The case of missing data (i.e., presence of <code>NA</code>'s either in the  
data matrix corresponding to the explanatory variables of the more complex 
model or the response vector) is not currently supported. Further, the
explanatory variables of the more complex model need to be quantitative.
</p>
<p>If <code class="reqn">p&gt;1</code>, the explanatory variables of the more complex model 
cannot have an exact linear relationship (perfect multicollinearity).
</p>


<h3>Value</h3>

<p><code>peptest</code> returns the Bayes factor, i.e., a numeric value.
For the ratio, the marginal likelihood of the more complex model (nominator)
with respect to that of the simpler one (denominator) is computed. 
Both marginal likelihoods are computed with respect
to the intercept&ndash;only model (reference model).
</p>


<h3>References</h3>

<p>Kass, R. and Raftery, A. (1995) Bayes Factors. 
Journal of the American Statistical Association, 90(430): 773–795. 
<a href="https://doi.org/10.1080/01621459.1995.10476572">doi:10.1080/01621459.1995.10476572</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(UScrime_data)
resBF1 &lt;- peptest(y~1,y~M+Ed,UScrime_data)
resBF1i &lt;- peptest(y~1,y~M+Ed,UScrime_data, intrinsic=TRUE)
resBF2j &lt;- peptest(y~M+Ed+Po1+Po2,y~M+Ed,UScrime_data,
                   reference.prior=FALSE)
resBF2ij &lt;- peptest(y~M+Ed+Po1+Po2,y~M+Ed,UScrime_data,
                    intrinsic=TRUE, reference.prior=FALSE)

</code></pre>

<hr>
<h2 id='plot.pep'>Plots for object of class pep</h2><span id='topic+plot.pep'></span>

<h3>Description</h3>

<p>Generates four plots related to an object of class pep. In particular,
the first one is a plot of the residuals against fitted values under 
Bayesian model averaging. The second plots the cumulative posterior 
probability of the top models (those with cumulative posterior probability 
larger than 0.99). The third plot depicts the marginal likelihood 
(in log scale) of a model against its dimension while 
the fourth plot shows the posterior inclusion probabilities
of the explanatory variables (with those exceeding 0.5 marked in red).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pep'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.pep_+3A_x">x</code></td>
<td>
<p>An object of class pep (e.g., output of <code>pep.lm</code>).</p>
</td></tr>
<tr><td><code id="plot.pep_+3A_...">...</code></td>
<td>
<p>Additional graphical parameters to be passed to plotting functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Let <code class="reqn">k</code> be the number of models with cumulative posterior probability up 
to 0.99. Then, the second plot depicts the cumulative posterior probability 
of the top <code class="reqn">(k+1)</code> models.
</p>
<p>In the special case of no explanatory variables, the fourth plot with the
posterior inclusion probabilities is not generated.
</p>


<h3>Value</h3>

<p>No return value, used for figure generation.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+image.pep">image.pep</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(UScrime_data)
res &lt;- pep.lm(y~.,data=UScrime_data)
plot(res)

</code></pre>

<hr>
<h2 id='posteriorpredictive.pep'>Posterior predictive distribution under Bayesian model averaging</h2><span id='topic+posteriorpredictive.pep'></span>

<h3>Description</h3>

<p>Simulates values from the posterior predictive distribution under 
Bayesian model averaging.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>posteriorpredictive.pep(
  object,
  xnew,
  ssize = 10000,
  estimator = "BMA",
  n.models = NULL,
  cumul.prob = 0.99
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="posteriorpredictive.pep_+3A_object">object</code></td>
<td>
<p>An object of class pep (e.g., output of <code>pep.lm</code>).</p>
</td></tr>
<tr><td><code id="posteriorpredictive.pep_+3A_xnew">xnew</code></td>
<td>
<p>An optional data frame of numeric, the new data 
on the explanatory variables to be used 
for prediction. The data frame needs to contain information about all 
explanatory variables available in the full model; if not an error message
is output.
If omitted, the data frame employed for fitting the full model is used.</p>
</td></tr>
<tr><td><code id="posteriorpredictive.pep_+3A_ssize">ssize</code></td>
<td>
<p>Positive integer, the number of values to be simulated from
each posterior predictive distribution.
Default value=10000.</p>
</td></tr>
<tr><td><code id="posteriorpredictive.pep_+3A_estimator">estimator</code></td>
<td>
<p>A character, the type of prediction. One of 
&ldquo;BMA&rdquo; (Bayesian model averaging, default), 
&ldquo;MAP&rdquo; (maximum a posteriori model) or &ldquo;MPM&rdquo; (median probability model).
Default value=<code>"BMA"</code>.</p>
</td></tr>
<tr><td><code id="posteriorpredictive.pep_+3A_n.models">n.models</code></td>
<td>
<p>Positive integer, the number of (top) models where
the average is based on or <code>NULL</code>. Relevant for <code>estimator="BMA"</code>.
Default value=<code>NULL</code>.</p>
</td></tr>
<tr><td><code id="posteriorpredictive.pep_+3A_cumul.prob">cumul.prob</code></td>
<td>
<p>Numeric between zero and one, cumulative probability of
top models to be used for computing the average. Relevant for <code>estimator="BMA"</code>. 
Default value=0.99.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For the computations, Equation 11 of Garcia&ndash;Donato and Forte (2018) 
is used. That (simplified) formula arises when changing the prior on the
model parameters to the reference prior. This change of prior is
justified in Garcia&ndash;Donato and Forte (2018). The resulting formula is a mixture
distribution and the simulation is implemented as follows: firstly the 
model (component) based on its posterior probability is chosen and 
subsequently the value for the response is
drawn from the corresponding Student distribution.
</p>
<p>The case of missing data (i.e., presence of NA’s) and non&ndash;quantitative data
in the new data frame
<code>xnew</code> is not currently supported.
</p>
<p>Let <code class="reqn">k</code> be the number of models with cumulative posterior probability up 
to the given value of <code>cumul.prob</code>. Then, for Bayesian model averaging 
the prediction is based on the top <code class="reqn">(k+1)</code> models if they exist, otherwise
on the top <code class="reqn">k</code> models.
</p>
<p>When both <code>n.models</code> and <code>cumul.prob</code> are provided &mdash; once 
specifying the number of models for the given cumulative probability as 
described above &mdash; the minimum between the two numbers is used for prediction.
</p>


<h3>Value</h3>

<p><code>posteriorpredictive.pep</code> returns a matrix (of dimension 
<code>ssize</code> <code class="reqn">\times</code> <code>nrow(xnew)</code>) &mdash; containing the 
simulated data. More specifically, column <code class="reqn">i</code> contains the simulated
values from the posterior predictive corresponding to the <code class="reqn">i</code>&ndash;th new 
observation (i.e., <code class="reqn">i</code>&ndash;th row of <code>xnew</code>).
</p>


<h3>References</h3>

<p>Garcia&ndash;Donato, G. and Forte, A. (2018) Bayesian Testing, 
Variable Selection and Model Averaging in Linear Models using R with 
BayesVarSel. The R Journal, 10(1): 155–174. 
<a href="https://doi.org/10.32614/RJ-2018-021">doi:10.32614/RJ-2018-021</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(UScrime_data)
X &lt;- UScrime_data[,-15]
set.seed(123)
res &lt;- pep.lm(y~.,data=UScrime_data[1:45,],intrinsic=TRUE,
               algorithmic.choice="MC3",itermc3=4000)
resf &lt;- posteriorpredictive.pep(res,ssize=2000,n.models=5)
resf2 &lt;- posteriorpredictive.pep(res,ssize=2000,estimator="MPM")
resp &lt;- posteriorpredictive.pep(res,xnew=X[46:47,],ssize=2000,n.models=5)
</code></pre>

<hr>
<h2 id='predict.pep'>(Point) Prediction under PEP approach</h2><span id='topic+predict.pep'></span>

<h3>Description</h3>

<p>Computes predicted or fitted values under the PEP approach. Predictions
can be based on Bayesian model averaging, maximum a posteriori model or
median probability model. For the Bayesian model averaging, a subset of the 
top models (either based on explicit number or on their cumulative probability) 
can be used for prediction.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pep'
predict(
  object,
  xnew,
  estimator = "BMA",
  n.models = NULL,
  cumul.prob = 0.99,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.pep_+3A_object">object</code></td>
<td>
<p>An object of class pep (e.g., output of <code>pep.lm</code>).</p>
</td></tr>
<tr><td><code id="predict.pep_+3A_xnew">xnew</code></td>
<td>
<p>An optional data frame of numeric, the new data 
on the explanatory variables to be used 
for prediction. The data frame needs to contain information about all 
explanatory variables available in the full model; if not an error message
is output. If omitted, fitted values are 
computed.</p>
</td></tr>
<tr><td><code id="predict.pep_+3A_estimator">estimator</code></td>
<td>
<p>A character, the type of prediction. One of 
&ldquo;BMA&rdquo; (Bayesian model averaging, default), 
&ldquo;MAP&rdquo; (maximum a posteriori model) or &ldquo;MPM&rdquo; (median probability model).
Default value=<code>"BMA"</code>.</p>
</td></tr>
<tr><td><code id="predict.pep_+3A_n.models">n.models</code></td>
<td>
<p>Positive integer, the number of (top) models that
prediction is based on or <code>NULL</code>. Relevant for <code>estimator="BMA"</code>.
Default value=<code>NULL</code>.</p>
</td></tr>
<tr><td><code id="predict.pep_+3A_cumul.prob">cumul.prob</code></td>
<td>
<p>Numeric between zero and one, cumulative probability of
top models to be used for prediction. Relevant for <code>estimator="BMA"</code>. 
Default value=0.99.</p>
</td></tr>
<tr><td><code id="predict.pep_+3A_...">...</code></td>
<td>
<p>Additional parameters to be passed, currently none.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code>xnew</code> is missing or <code>xnew</code> is equal to the initial
data frame used for fitting, then fitted 
values are computed (and returned).
</p>
<p>For prediction, Equation 9 of Fouskakis and Ntzoufras (2020) is used.
</p>
<p>The case of missing data (i.e., presence of NA’s) 
and non&ndash;quantitative data in the new data frame
<code>xnew</code> is not currently supported.
</p>
<p>Let <code class="reqn">k</code> be the number of models with cumulative posterior probability up 
to the given value of <code>cumul.prob</code>. Then, for Bayesian model averaging 
the prediction is based on the top <code class="reqn">(k+1)</code> models if they exist, otherwise
on the top <code class="reqn">k</code> models.
</p>
<p>When both <code>n.models</code> and <code>cumul.prob</code> are provided &mdash; once 
specifying the number of models for the given cumulative probability as 
described above &mdash; the minimum between the two numbers is used for prediction.
</p>


<h3>Value</h3>

<p><code>predict</code> returns a vector with the predicted (or fitted)
values for the different observations.
</p>


<h3>References</h3>

<p>Fouskakis, D. and Ntzoufras, I. (2022) Power&ndash;Expected&ndash;Posterior 
Priors as Mixtures of g&ndash;Priors in Normal Linear Models. 
Bayesian Analysis, 17(4): 1073-1099. <a href="https://doi.org/10.1214/21-BA1288">doi:10.1214/21-BA1288</a>
</p>
<p>Fouskakis, D. and Ntzoufras, I. (2020) Bayesian Model Averaging Using 
Power&ndash;Expected&ndash;Posterior Priors. 
Econometrics, 8(2): 17. <a href="https://doi.org/10.3390/econometrics8020017">doi:10.3390/econometrics8020017</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(UScrime_data)
X &lt;- UScrime_data[,-15]
set.seed(123)
res &lt;- pep.lm(y~.,data=UScrime_data[1:45,],intrinsic=TRUE,
               algorithmic.choice="MC3",itermc3=4000)
resf &lt;- predict(res)
resf2 &lt;- predict(res,estimator="MPM")
resp &lt;- predict(res,xnew=X[46:47,])
</code></pre>

<hr>
<h2 id='print.pep'>Printing object of class pep</h2><span id='topic+print.pep'></span>

<h3>Description</h3>

<p>For each of the top models (shown in columns), the following information is
printed: the model representation using variable inclusion indicators, 
its marginal likelihood (in log scale), the R2, the model dimension, the Bayes 
factor, posterior odds (comparison made with the highest posterior 
probability model) and posterior probability. An additional 
column with the posterior inclusion probabilities of the explanatory variables is 
also printed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pep'
print(
  x,
  n.models = 5,
  actual.PO = FALSE,
  digits = max(3L, getOption("digits") - 3L),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.pep_+3A_x">x</code></td>
<td>
<p>An object of class pep (e.g., output of <code>pep.lm</code>).</p>
</td></tr>
<tr><td><code id="print.pep_+3A_n.models">n.models</code></td>
<td>
<p>Positive integer, the number of top models for which information 
is provided. Default value=5.</p>
</td></tr>
<tr><td><code id="print.pep_+3A_actual.po">actual.PO</code></td>
<td>
<p>Logical, relevant for the MC3 algorithm. If <code>TRUE</code>
then apart from the estimated posterior odds, the actual posterior
odds of the MAP model versus the top models (i.e., ratios based on the marginal likelihood 
times prior probability) are also printed &mdash; which could be used as a 
convergence indicator of the algorithm. Default value=<code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="print.pep_+3A_digits">digits</code></td>
<td>
<p>Positive integer, the number of digits for printing numbers. 
Default value=<code>max(3L, getOption("digits") - 3L)</code>.</p>
</td></tr>
<tr><td><code id="print.pep_+3A_...">...</code></td>
<td>
<p>Additional parameters to be passed to 
<code>print.default</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The number of models for which information is provided, is computed as the minimum
between the number asked by the user and the number of models present in
the object <code>x</code>.
</p>


<h3>Value</h3>

<p>No return value, used for printing the results on the R console.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(UScrime_data)
res &lt;- pep.lm(y~.,data=UScrime_data)
print(res)

</code></pre>

<hr>
<h2 id='UScrime_data'>US Crime Data</h2><span id='topic+UScrime_data'></span>

<h3>Description</h3>

<p>The dataset has been borrowed from the MASS R package and describes 
the effect of punishment regimes on crime rates. One explanatory
variable (indicator variable for a Southern state) was removed since it was binary.
</p>


<h3>Format</h3>

<p>This data frame contains the following columns:
</p>

<dl>
<dt><code>M</code></dt><dd><p>percentage of males aged 14&ndash;24.</p>
</dd>
<dt><code>Ed</code></dt><dd><p>mean years of schooling.</p>
</dd>
<dt><code>Po1</code></dt><dd><p>police expenditure in 1960.</p>
</dd>
<dt><code>Po2</code></dt><dd><p>police expenditure in 1959.</p>
</dd>
<dt><code>LF</code></dt><dd><p>labour force participation rate.</p>
</dd>
<dt><code>M.F</code></dt><dd><p>number of males per 1000 females.</p>
</dd>
<dt><code>Pop</code></dt><dd><p>state population.</p>
</dd>
<dt><code>NW</code></dt><dd><p>number of non-whites per 1000 people.</p>
</dd>
<dt><code>U1</code></dt><dd><p>unemployment rate of urban males 14&ndash;24.</p>
</dd>
<dt><code>U2</code></dt><dd><p>unemployment rate of urban males 35&ndash;39.</p>
</dd>
<dt><code>GDP</code></dt><dd><p>gross domestic product per head.</p>
</dd>
<dt><code>Ineq</code></dt><dd><p>income inequality.</p>
</dd>
<dt><code>Prob</code></dt><dd><p>probability of imprisonment.</p>
</dd>
<dt><code>Time</code></dt><dd><p>average time served in state prisons.</p>
</dd>
<dt><code>y</code></dt><dd><p>rate of crimes in a particular category per head of population.</p>
</dd>
</dl>



<h3>Source</h3>

<p>Data from the R package MASS
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
