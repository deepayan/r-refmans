<!DOCTYPE html><html><head><title>Help for package irr</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {irr}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#agree'><p>Simple and extended percentage agreement</p></a></li>
<li><a href='#anxiety'><p>Anxiety ratings by different raters</p></a></li>
<li><a href='#bhapkar'><p>Bhapkar coefficient of concordance between raters</p></a></li>
<li><a href='#diagnoses'><p>Psychiatric diagnoses provided by different raters</p></a></li>
<li><a href='#finn'><p>Finn coefficient for oneway and twoway models</p></a></li>
<li><a href='#icc'><p>Intraclass correlation coefficient (ICC) for oneway and twoway models</p></a></li>
<li><a href='#iota'><p>iota coefficient for the interrater agreement of multivariate observations</p></a></li>
<li><a href='#kappa2'><p>Cohen's Kappa and weighted Kappa for two raters</p></a></li>
<li><a href='#kappam.fleiss'><p>Fleiss' Kappa for m raters</p></a></li>
<li><a href='#kappam.light'><p>Light's Kappa for m raters</p></a></li>
<li><a href='#kendall'><p>Kendall's coefficient of concordance W</p></a></li>
<li><a href='#kripp.alpha'><p>calculate Krippendorff's alpha reliability coefficient</p></a></li>
<li><a href='#maxwell'><p>Maxwell's RE coefficient for binary data</p></a></li>
<li><a href='#meancor'><p>Mean of bivariate correlations between raters</p></a></li>
<li><a href='#meanrho'><p>Mean of bivariate rank correlations between raters</p></a></li>
<li><a href='#N.cohen.kappa'><p>Sample Size Calculation for Cohen's Kappa Statistic</p></a></li>
<li><a href='#N2.cohen.kappa'><p>Sample Size Calculation for Cohen's Kappa Statistic with more than one category</p></a></li>
<li><a href='#print.icclist'><p>Default printing function for ICC results</p></a></li>
<li><a href='#print.irrlist'><p>Default printing function for various coefficients of interrater reliability</p></a></li>
<li><a href='#rater.bias'><p>Coefficient of rater bias</p></a></li>
<li><a href='#relInterIntra'><p>Inter- and intra-rater reliability</p></a></li>
<li><a href='#robinson'><p>Robinson's A</p></a></li>
<li><a href='#stuart.maxwell.mh'><p>Stuart-Maxwell coefficient of concordance for two raters</p></a></li>
<li><a href='#video'><p>Different raters judging the credibility of videotaped testimonies</p></a></li>
<li><a href='#vision'><p>Eye-testing case records</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>0.84.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2012-01-22</td>
</tr>
<tr>
<td>Title:</td>
<td>Various Coefficients of Interrater Reliability and Agreement</td>
</tr>
<tr>
<td>Author:</td>
<td>Matthias Gamer &lt;m.gamer@uke.uni-hamburg.de&gt;, Jim Lemon
        &lt;jim@bitwrit.com.au&gt;, Ian Fellows &lt;ifellows@uscd.edu&gt; Puspendra
        Singh &lt;puspendra.pusp22@gmail.com&gt;</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Matthias Gamer &lt;m.gamer@uke.uni-hamburg.de&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10), lpSolve</td>
</tr>
<tr>
<td>Description:</td>
<td>Coefficients of Interrater Reliability and Agreement for
        quantitative, ordinal and nominal data: ICC, Finn-Coefficient,
        Robinson's A, Kendall's W, Cohen's Kappa, ...</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://www.r-project.org">https://www.r-project.org</a></td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-01-26 16:15:29 UTC; hornik</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-01-26 17:07:15 UTC</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
</table>
<hr>
<h2 id='agree'>Simple and extended percentage agreement</h2><span id='topic+agree'></span>

<h3>Description</h3>

<p>Computes simple and extended percentage agreement among raters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>agree(ratings, tolerance=0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="agree_+3A_ratings">ratings</code></td>
<td>
<p>n*m matrix or dataframe, n subjects m raters.</p>
</td></tr>
<tr><td><code id="agree_+3A_tolerance">tolerance</code></td>
<td>
<p>number of successive rating categories that should be regarded as rater agreement (see details).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Missing data are omitted in a listwise way.<br />
Using extended percentage agreement (tolerance!=0) is only possible for numerical values. If tolerance equals 1, for example, raters differing by one scale degree are interpreted as agreeing.
</p>


<h3>Value</h3>

<p>A list with class '&quot;irrlist&quot;' containing the following components:
</p>
<table>
<tr><td><code>$method</code></td>
<td>
<p>a character string describing the method applied for the computation of interrater reliability.</p>
</td></tr>
<tr><td><code>$subjects</code></td>
<td>
<p>the number of subjects examined.</p>
</td></tr>
<tr><td><code>$raters</code></td>
<td>
<p>the number of raters.</p>
</td></tr>
<tr><td><code>$irr.name</code></td>
<td>
<p>a character string specifying the name of the coefficient.</p>
</td></tr>
<tr><td><code>$value</code></td>
<td>
<p>coefficient of interrater reliability.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Matthias Gamer</p>


<h3>See Also</h3>

<p><code><a href="#topic+kappa2">kappa2</a></code>, 
<code><a href="#topic+kappam.fleiss">kappam.fleiss</a></code>, 
<code><a href="#topic+kappam.light">kappam.light</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(video)
agree(video)     # Simple percentage agreement
agree(video, 1)  # Extended percentage agreement
</code></pre>

<hr>
<h2 id='anxiety'>Anxiety ratings by different raters</h2><span id='topic+anxiety'></span>

<h3>Description</h3>

<p>The data frame contains the anxiety ratings of 20 subjects, rated by 3 raters.
Values are ranging from 1 (not anxious at all) to 6 (extremely anxious).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(anxiety)</code></pre>


<h3>Format</h3>

<p>A data frame with 20 observations on the following 3 variables.
</p>

<dl>
<dt>rater1</dt><dd><p>ratings of the first rater</p>
</dd>
<dt>rater2</dt><dd><p>ratings of the second rater</p>
</dd>
<dt>rater3</dt><dd><p>ratings of the third rater</p>
</dd>
</dl>



<h3>Source</h3>

<p>artificial data
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(anxiety)
apply(anxiety,2,table)
</code></pre>

<hr>
<h2 id='bhapkar'>Bhapkar coefficient of concordance between raters</h2><span id='topic+bhapkar'></span>

<h3>Description</h3>

<p>Calculates the Bhapkar coefficient of concordance for two raters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bhapkar(ratings)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bhapkar_+3A_ratings">ratings</code></td>
<td>
<p>n*2 matrix or dataframe, n subjects 2 raters.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Missing data are omitted in a listwise way.
The Bhapkar (1966) test is a more powerful alternative to the Stuart-Maxwell test.
Both tests are asymptotically equivalent and will produce comparable chi-squared
values when applied a large sample of rated objects.
</p>


<h3>Value</h3>

<p>A list with class &quot;irrlist&quot; containing the following components: 
</p>
<table>
<tr><td><code>$method</code></td>
<td>
<p>a character string describing the method.</p>
</td></tr>
<tr><td><code>$subjects</code></td>
<td>
<p>the number of data objects.</p>
</td></tr>
<tr><td><code>$raters</code></td>
<td>
<p>the number of raters.</p>
</td></tr>
<tr><td><code>$irr.name</code></td>
<td>
<p>the name of the coefficient (Chisq).</p>
</td></tr>
<tr><td><code>$value</code></td>
<td>
<p>the value of the coefficient.</p>
</td></tr>
<tr><td><code>$stat.name</code></td>
<td>
<p>the name and df of the test statistic.</p>
</td></tr>
<tr><td><code>$statistic</code></td>
<td>
<p>the value of the test statistic.</p>
</td></tr>
<tr><td><code>$p.value</code></td>
<td>
<p>the probability of the test statistic.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Matthias Gamer</p>


<h3>References</h3>

<p>Bhapkar, V.P. (1966). A note on the equivalence of two test criteria for hypotheses in 
categorical data. Journal of the American Statistical Association, 61, 228-235.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+mcnemar.test">mcnemar.test</a></code>, 
<code><a href="#topic+stuart.maxwell.mh">stuart.maxwell.mh</a></code>, 
<code><a href="#topic+rater.bias">rater.bias</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(vision)
bhapkar(vision) # Original example used from Bhapkar (1966)
</code></pre>

<hr>
<h2 id='diagnoses'>Psychiatric diagnoses provided by different raters</h2><span id='topic+diagnoses'></span>

<h3>Description</h3>

<p>Psychiatric diagnoses of n=30 patients provided by different sets of m=6 raters.
Data were used by Fleiss (1971) to illustrate the computation of Kappa for m raters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(diagnoses)</code></pre>


<h3>Format</h3>

<p>A data frame with 30 observations (psychiatric diagnoses with levels 1. Depression, 2. Personality Disorder, 3. Schizophrenia, 4. Neurosis, 5. Other) on 6 variables representing different raters.
</p>

<dl>
<dt>rater1</dt><dd><p>a factor including the diagnoses of rater 1 (levels see above)</p>
</dd>
<dt>rater2</dt><dd><p>a factor including the diagnoses of rater 2 (levels see above)</p>
</dd>
<dt>rater3</dt><dd><p>a factor including the diagnoses of rater 3 (levels see above)</p>
</dd>
<dt>rater4</dt><dd><p>a factor including the diagnoses of rater 4 (levels see above)</p>
</dd>
<dt>rater5</dt><dd><p>a factor including the diagnoses of rater 5 (levels see above)</p>
</dd>
<dt>rater6</dt><dd><p>a factor including the diagnoses of rater 6 (levels see above)</p>
</dd>
</dl>



<h3>Source</h3>

<p>Fleiss, J.L. (1971). Measuring nominal scale agreement among many raters. Psychological Bulletin, 76, 378-382.
</p>


<h3>References</h3>

<p>Fleiss, J.L. (1971). Measuring nominal scale agreement among many raters. Psychological Bulletin, 76, 378-382.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(diagnoses)
table(diagnoses[,1])
</code></pre>

<hr>
<h2 id='finn'>Finn coefficient for oneway and twoway models</h2><span id='topic+finn'></span>

<h3>Description</h3>

<p>Computes the Finn coefficient as an index of the interrater reliability of quantitative data. Additionally, F-test and confidence interval are computed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>finn(ratings, s.levels, model = c("oneway", "twoway"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="finn_+3A_ratings">ratings</code></td>
<td>
<p>n*m matrix or dataframe, n subjects m raters.</p>
</td></tr>
<tr><td><code id="finn_+3A_s.levels">s.levels</code></td>
<td>
<p>the number of different rating categories.</p>
</td></tr>
<tr><td><code id="finn_+3A_model">model</code></td>
<td>
<p>a character string specifying if a '&quot;oneway&quot;' model (default) with row effects random, or a '&quot;twoway&quot;' model with column and row effects random should be applied. You can specify just the initial letter.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Missing data are omitted in a listwise way.<br />
The Finn coefficient is especially useful, when variance between raters is low (i.e. agreement is high).<br />
For the computation it could be specified if only the subjects are considered as random effects ('&quot;oneway&quot;' model) or if subjects and raters are randomly chosen from a bigger pool of persons ('&quot;twoway&quot;' model).
</p>


<h3>Value</h3>

<p>A list with class '&quot;irrlist&quot;' containing the following components:
</p>
<table>
<tr><td><code>$method</code></td>
<td>
<p>a character string describing the method applied for the computation of interrater reliability.</p>
</td></tr>
<tr><td><code>$subjects</code></td>
<td>
<p>the number of subjects examined.</p>
</td></tr>
<tr><td><code>$raters</code></td>
<td>
<p>the number of raters.</p>
</td></tr>
<tr><td><code>$irr.name</code></td>
<td>
<p>a character string specifying the name of the coefficient.</p>
</td></tr>
<tr><td><code>$value</code></td>
<td>
<p>coefficient of interrater reliability.</p>
</td></tr>
<tr><td><code>$stat.name</code></td>
<td>
<p>a character string specifying the name and the df of the corresponding F-statistic.</p>
</td></tr>
<tr><td><code>$statistic</code></td>
<td>
<p>the value of the test statistic.</p>
</td></tr>
<tr><td><code>$p.value</code></td>
<td>
<p>the p-value for the test.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Matthias Gamer</p>


<h3>References</h3>

<p>Finn, R.H. (1970). A note on estimating the reliability of categorical data. Educational and Psychological Measurement, 30, 71-76.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+icc">icc</a></code>, 
<code><a href="#topic+meancor">meancor</a></code>, 
<code><a href="#topic+robinson">robinson</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(video)
finn(video, 6, model="twoway")
</code></pre>

<hr>
<h2 id='icc'>Intraclass correlation coefficient (ICC) for oneway and twoway models</h2><span id='topic+icc'></span>

<h3>Description</h3>

<p>Computes single score or average score ICCs as an index of interrater reliability of quantitative data. Additionally, F-test and confidence interval are computed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>icc(ratings, model = c("oneway", "twoway"), 
    type = c("consistency", "agreement"), 
    unit = c("single", "average"), r0 = 0, conf.level = 0.95)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="icc_+3A_ratings">ratings</code></td>
<td>
<p>n*m matrix or dataframe, n subjects m raters.</p>
</td></tr>
<tr><td><code id="icc_+3A_model">model</code></td>
<td>
<p>a character string specifying if a '&quot;oneway&quot;' model (default) with row effects random, or a '&quot;twoway&quot;' model with column and row effects random should be applied. You can specify just the initial letter.</p>
</td></tr>
<tr><td><code id="icc_+3A_type">type</code></td>
<td>
<p>a character string specifying if '&quot;consistency&quot;' (default) or '&quot;agreement&quot;' between raters should be estimated. If a '&quot;oneway&quot;' model is used, only '&quot;consistency&quot;' could be computed. You can specify just the initial letter.</p>
</td></tr>
<tr><td><code id="icc_+3A_unit">unit</code></td>
<td>
<p>a character string specifying the unit of analysis: Must be one of '&quot;single&quot;' (default) or '&quot;average&quot;'. You can specify just the initial letter.</p>
</td></tr>
<tr><td><code id="icc_+3A_r0">r0</code></td>
<td>
<p>specification of the null hypothesis r = r0. Note that a one sided test (H1: r &gt; r0) is performed.</p>
</td></tr>
<tr><td><code id="icc_+3A_conf.level">conf.level</code></td>
<td>
<p>confidence level of the interval.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Missing data are omitted in a listwise way.<br />
When considering which form of ICC is appropriate for an actual set of data, one has take several decisions (Shrout &amp; Fleiss, 1979):<br /><br />
1. Should only the subjects be considered as random effects ('&quot;oneway&quot;' model) or are subjects and raters randomly chosen from a bigger pool of persons ('&quot;twoway&quot;' model).<br /><br />
2. If differences in judges' mean ratings are of interest, interrater '&quot;agreement&quot;' instead of '&quot;consistency&quot;' should be computed.<br /><br />
3. If the unit of analysis is a mean of several ratings, unit should be changed to '&quot;average&quot;'. In most cases, however, single values (unit='&quot;single&quot;') are regarded.
</p>


<h3>Value</h3>

<p>A list with class '&quot;icclist&quot;' containing the following components:
</p>
<table>
<tr><td><code>$subjects</code></td>
<td>
<p>the number of subjects examined.</p>
</td></tr>
<tr><td><code>$raters</code></td>
<td>
<p>the number of raters.</p>
</td></tr>
<tr><td><code>$model</code></td>
<td>
<p>a character string describing the selected model for the analysis.</p>
</td></tr>
<tr><td><code>$type</code></td>
<td>
<p>a character string describing the selected type of interrater reliability.</p>
</td></tr>
<tr><td><code>$unit</code></td>
<td>
<p>a character string describing the unit of analysis.</p>
</td></tr>
<tr><td><code>$icc.name</code></td>
<td>
<p>a character string specifying the name of ICC according to McGraw &amp; Wong (1996).</p>
</td></tr>
<tr><td><code>$value</code></td>
<td>
<p>the intraclass correlation coefficient.</p>
</td></tr>
<tr><td><code>$r0</code></td>
<td>
<p>the specified null hypothesis.</p>
</td></tr>
<tr><td><code>$Fvalue</code></td>
<td>
<p>the value of the F-statistic.</p>
</td></tr>
<tr><td><code>$df1</code></td>
<td>
<p>the numerator degrees of freedom.</p>
</td></tr>
<tr><td><code>$df2</code></td>
<td>
<p>the denominator degrees of freedom.</p>
</td></tr>
<tr><td><code>$p.value</code></td>
<td>
<p>the p-value for a two-sided test.</p>
</td></tr>
<tr><td><code>$conf.level</code></td>
<td>
<p>the confidence level for the interval.</p>
</td></tr>
<tr><td><code>$lbound</code></td>
<td>
<p>the lower bound of the confidence interval.</p>
</td></tr>
<tr><td><code>$ubound</code></td>
<td>
<p>the upper bound of the confidence interval.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Matthias Gamer</p>


<h3>References</h3>

<p>Bartko, J.J. (1966). The intraclass correlation coefficient as a measure of reliability. Psychological Reports, 19, 3-11.<br /><br />
McGraw, K.O., &amp; Wong, S.P. (1996), Forming inferences about some intraclass correlation coefficients. Psychological Methods, 1, 30-46.<br /><br />
Shrout, P.E., &amp; Fleiss, J.L. (1979), Intraclass correlation: uses in assessing rater reliability. Psychological Bulletin, 86, 420-428.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+finn">finn</a></code>,
<code><a href="#topic+meancor">meancor</a></code>,
<code><a href="#topic+robinson">robinson</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(anxiety)
icc(anxiety, model="twoway", type="agreement")

r1 &lt;- round(rnorm(20, 10, 4))
r2 &lt;- round(r1 + 10 + rnorm(20, 0, 2))
r3 &lt;- round(r1 + 20 + rnorm(20, 0, 2))
icc(cbind(r1, r2, r3), "twoway")              # High consistency
icc(cbind(r1, r2, r3), "twoway", "agreement") # Low agreement
</code></pre>

<hr>
<h2 id='iota'>iota coefficient for the interrater agreement of multivariate observations</h2><span id='topic+iota'></span>

<h3>Description</h3>

<p>Computes iota as an index of interrater agreement of quantitative or nominal multivariate observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>iota(ratings, scaledata = c("quantitative","nominal"),
     standardize = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="iota_+3A_ratings">ratings</code></td>
<td>
<p>list of n*m matrices or dataframes with one list element for each variable, n subjects m raters.</p>
</td></tr>
<tr><td><code id="iota_+3A_scaledata">scaledata</code></td>
<td>
<p>a character string specifying if the data is '&quot;quantitative&quot;' (default) or '&quot;nominal&quot;'. If the data is organized in factors, '&quot;nominal&quot;' is chosen automatically. You can specify just the initial letter.</p>
</td></tr>
<tr><td><code id="iota_+3A_standardize">standardize</code></td>
<td>
<p>a logical indicating whether quantitative data should be z-standardized within each variable before the computation of iota.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Each list element must contain observations for each rater and subject without missing values.<br />
In case of one categorical variable (only one list element), iota reduces to the Fleiss exact kappa coefficient, which was proposed by Conger (1980).
</p>


<h3>Value</h3>

<p>A list with class '&quot;irrlist&quot;' containing the following components:
</p>
<table>
<tr><td><code>$method</code></td>
<td>
<p>a character string describing the method applied for the computation of interrater reliability.</p>
</td></tr>
<tr><td><code>$subjects</code></td>
<td>
<p>the number of subjects examined.</p>
</td></tr>
<tr><td><code>$raters</code></td>
<td>
<p>the number of raters.</p>
</td></tr>
<tr><td><code>$irr.name</code></td>
<td>
<p>a character string specifying the name of the coefficient.</p>
</td></tr>
<tr><td><code>$value</code></td>
<td>
<p>value of iota.</p>
</td></tr>
<tr><td><code>$detail</code></td>
<td>
<p>a character string specifying if the values were z-standardized before the computation of iota.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Matthias Gamer</p>


<h3>References</h3>

<p>Conger, A.J. (1980). Integration and generalisation of Kappas for multiple raters. Psychological Bulletin, 88, 322-328.<br /><br />
Janson, H., &amp; Olsson, U. (2001). A measure of agreement for interval or nominal multivariate observations. Educational and Psychological Measurement, 61, 277-289.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+icc">icc</a></code>,
<code><a href="#topic+kappam.fleiss">kappam.fleiss</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(diagnoses)
iota(list(diagnoses))  # produces the same result as...
kappam.fleiss(diagnoses, exact=TRUE)

# Example from Janson &amp; Olsson (2001), Table 1
photo &lt;- list()
photo[[1]] &lt;- cbind(c( 71, 73, 86, 59, 71),  # weight ratings
                    c( 74, 80,101, 62, 83),
                    c( 76, 80, 93, 66, 77))
photo[[2]] &lt;- cbind(c(166,160,187,161,172),  # height rating
                    c(171,170,174,163,182),
                    c(171,165,185,162,181))
iota(photo)
iota(photo, standardize=TRUE) # iota over standardized values
</code></pre>

<hr>
<h2 id='kappa2'>Cohen's Kappa and weighted Kappa for two raters</h2><span id='topic+kappa2'></span>

<h3>Description</h3>

<p>Calculates Cohen's Kappa and weighted Kappa as an index of interrater agreement between 2 raters on categorical (or ordinal) data. Own weights for the various degrees of disagreement could be specified.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kappa2(ratings, weight = c("unweighted", "equal", "squared"), sort.levels = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kappa2_+3A_ratings">ratings</code></td>
<td>
<p>n*2 matrix or dataframe, n subjects 2 raters.</p>
</td></tr>
<tr><td><code id="kappa2_+3A_weight">weight</code></td>
<td>
<p>either a character string specifying one predifined set of weights or a numeric vector with own weights (see details).</p>
</td></tr>
<tr><td><code id="kappa2_+3A_sort.levels">sort.levels</code></td>
<td>
<p>boolean value describing whether factor levels should be (re-)sorted during the calculation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Missing data are omitted in a listwise way.<br />
During computation, ratings are converted to factors. Therefore, the categories are ordered accordingly. When ratings are numeric, a sorting of factor levels occurs automatically. Otherwise, levels are sorted when the function is called with sort.levels=TRUE. <br />
<code>kappa2</code> allows for calculating weighted Kappa coefficients. Beneath '&quot;unweighted&quot;' (default), predifined sets of weights are '&quot;equal&quot;' (all levels disagreement between raters are weighted equally) and '&quot;squared&quot;' (disagreements are weighted according to their squared distance from perfect agreement). The weighted Kappa coefficient with '&quot;squared&quot;' weights equals the product moment correlation under certain conditions.
Own weights could be specified by supplying the function with a numeric vector of weights, starting from perfect agreement to worst disagreement. The length of this vector must equal the number of rating categories.
</p>


<h3>Value</h3>

<p>A list with class '&quot;irrlist&quot;' containing the following components:
</p>
<table>
<tr><td><code>$method</code></td>
<td>
<p>a character string describing the method and the weights applied for the computation of weighted Kappa.</p>
</td></tr>
<tr><td><code>$subjects</code></td>
<td>
<p>the number of subjects examined.</p>
</td></tr>
<tr><td><code>$raters</code></td>
<td>
<p>the number of raters (=2).</p>
</td></tr>
<tr><td><code>$irr.name</code></td>
<td>
<p>a character string specifying the name of the coefficient.</p>
</td></tr>
<tr><td><code>$value</code></td>
<td>
<p>value of Kappa.</p>
</td></tr>
<tr><td><code>$stat.name</code></td>
<td>
<p>a character string specifying the name of the corresponding test statistic.</p>
</td></tr>
<tr><td><code>$statistic</code></td>
<td>
<p>the value of the test statistic.</p>
</td></tr>
<tr><td><code>$p.value</code></td>
<td>
<p>the p-value for the test.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Matthias Gamer</p>


<h3>References</h3>

<p>Cohen, J. (1960). A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20, 37-46.<br /><br />
Cohen, J. (1968). Weighted kappa: Nominal scale agreement with provision for scaled disagreement or partial credit. Psychological Bulletin, 70, 213-220.<br /><br />
Fleiss, J.L., Cohen, J., &amp; Everitt, B.S. (1969). Large sample standard errors of kappa and weighted kappa. Psychological Bulletin, 72, 323-327.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+cor">cor</a></code>,
<code><a href="#topic+kappa2">kappa2</a></code>,
<code><a href="#topic+kappam.light">kappam.light</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(anxiety)
kappa2(anxiety[,1:2], "squared") # predefined set of squared weights
kappa2(anxiety[,1:2], (0:5)^2)   # same result with own set of squared weights

# own weights increasing gradually with larger distance from perfect agreement
kappa2(anxiety[,1:2], c(0,1,2,4,7,11))

data(diagnoses)
# Unweighted Kappa for categorical data without a logical order
kappa2(diagnoses[,2:3])
</code></pre>

<hr>
<h2 id='kappam.fleiss'>Fleiss' Kappa for m raters</h2><span id='topic+kappam.fleiss'></span>

<h3>Description</h3>

<p>Computes Fleiss' Kappa as an index of interrater agreement between m raters on categorical data. Additionally, category-wise Kappas could be computed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kappam.fleiss(ratings, exact = FALSE, detail = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kappam.fleiss_+3A_ratings">ratings</code></td>
<td>
<p>n*m matrix or dataframe, n subjects m raters.</p>
</td></tr>
<tr><td><code id="kappam.fleiss_+3A_exact">exact</code></td>
<td>
<p>a logical indicating whether the exact Kappa (Conger, 1980) or the Kappa described by Fleiss (1971) should be computed.</p>
</td></tr>
<tr><td><code id="kappam.fleiss_+3A_detail">detail</code></td>
<td>
<p>a logical indicating whether category-wise Kappas should be computed</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Missing data are omitted in a listwise way.<br />
The coefficient described by Fleiss (1971) does not reduce to Cohen's Kappa (unweighted) for m=2 raters. Therefore, the exact Kappa coefficient, which is slightly higher in most cases, was proposed by Conger (1980).<br />
The null hypothesis Kappa=0 could only be tested using Fleiss' formulation of Kappa.
</p>


<h3>Value</h3>

<p>A list with class '&quot;irrlist&quot;' containing the following components:
</p>
<table>
<tr><td><code>$method</code></td>
<td>
<p>a character string describing the method applied for the computation of interrater reliability.</p>
</td></tr>
<tr><td><code>$subjects</code></td>
<td>
<p>the number of subjects examined.</p>
</td></tr>
<tr><td><code>$raters</code></td>
<td>
<p>the number of raters.</p>
</td></tr>
<tr><td><code>$irr.name</code></td>
<td>
<p>a character string specifying the name of the coefficient.</p>
</td></tr>
<tr><td><code>$value</code></td>
<td>
<p>value of Kappa.</p>
</td></tr>
<tr><td><code>$stat.name</code></td>
<td>
<p>a character string specifying the name of the corresponding test statistic.</p>
</td></tr>
<tr><td><code>$statistic</code></td>
<td>
<p>the value of the test statistic.</p>
</td></tr>
<tr><td><code>$p.value</code></td>
<td>
<p>the p-value for the test.</p>
</td></tr>
<tr><td><code>$detail</code></td>
<td>
<p>a table with category-wise kappas and the corresponding test statistics.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Matthias Gamer</p>


<h3>References</h3>

<p>Conger, A.J. (1980). Integration and generalisation of Kappas for multiple raters. Psychological Bulletin, 88, 322-328.<br /><br />
Fleiss, J.L. (1971). Measuring nominal scale agreement among many raters. Psychological Bulletin, 76, 378-382.<br /><br />
Fleiss, J.L., Levin, B., &amp; Paik, M.C. (2003). Statistical Methods for Rates and Proportions, 3rd Edition. New York: John Wiley &amp; Sons.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kappa2">kappa2</a></code>,
<code><a href="#topic+kappam.light">kappam.light</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(diagnoses)
kappam.fleiss(diagnoses)               # Fleiss' Kappa
kappam.fleiss(diagnoses, exact=TRUE)   # Exact Kappa
kappam.fleiss(diagnoses, detail=TRUE)  # Fleiss' and category-wise Kappa

kappam.fleiss(diagnoses[,1:4])         # Fleiss' Kappa of raters 1 to 4
</code></pre>

<hr>
<h2 id='kappam.light'>Light's Kappa for m raters</h2><span id='topic+kappam.light'></span>

<h3>Description</h3>

<p>Computes Light's Kappa as an index of interrater agreement between m raters on categorical data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kappam.light(ratings)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kappam.light_+3A_ratings">ratings</code></td>
<td>
<p>n*m matrix or dataframe, n subjects m raters.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Missing data are omitted in a listwise way.<br />
Light's Kappa equals the average of all possible combinations of bivariate Kappas between raters.
</p>


<h3>Value</h3>

<p>A list with class '&quot;irrlist&quot;' containing the following components:
</p>
<table>
<tr><td><code>$method</code></td>
<td>
<p>a character string describing the method applied for the computation of interrater reliability.</p>
</td></tr>
<tr><td><code>$subjects</code></td>
<td>
<p>the number of subjects examined.</p>
</td></tr>
<tr><td><code>$raters</code></td>
<td>
<p>the number of raters.</p>
</td></tr>
<tr><td><code>$irr.name</code></td>
<td>
<p>a character string specifying the name of the coefficient.</p>
</td></tr>
<tr><td><code>$value</code></td>
<td>
<p>value of Kappa.</p>
</td></tr>
<tr><td><code>$stat.name</code></td>
<td>
<p>a character string specifying the name of the corresponding test statistic.</p>
</td></tr>
<tr><td><code>$statistic</code></td>
<td>
<p>the value of the test statistic.</p>
</td></tr>
<tr><td><code>$p.value</code></td>
<td>
<p>the p-value for the test.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Matthias Gamer</p>


<h3>References</h3>

<p>Conger, A.J. (1980). Integration and generalisation of Kappas for multiple raters. Psychological Bulletin, 88, 322-328.<br /><br />
Light, R.J. (1971). Measures of response agreement for qualitative data: Some generalizations and alternatives. Psychological Bulletin, 76, 365-377.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kappa2">kappa2</a></code>,
<code><a href="#topic+kappam.fleiss">kappam.fleiss</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(diagnoses)
kappam.light(diagnoses)   # Light's Kappa
</code></pre>

<hr>
<h2 id='kendall'>Kendall's coefficient of concordance W</h2><span id='topic+kendall'></span>

<h3>Description</h3>

<p>Computes Kendall's coefficient of concordance as an index of interrater reliability of ordinal data. The coefficient could be corrected for ties within raters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kendall(ratings, correct = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kendall_+3A_ratings">ratings</code></td>
<td>
<p>n*m matrix or dataframe, n subjects m raters.</p>
</td></tr>
<tr><td><code id="kendall_+3A_correct">correct</code></td>
<td>
<p>a logical indicating whether the coefficient should be corrected for ties within raters.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Missing data are omitted in a listwise way.<br />
Kendall's W should be corrected for ties if raters did not use a true ranking order for the subjects.<br />
A test for the significance of Kendall's W is only valid for large samples. 
</p>


<h3>Value</h3>

<p>A list with class '&quot;irrlist&quot;' containing the following components:
</p>
<table>
<tr><td><code>$method</code></td>
<td>
<p>a character string describing the method applied for the computation of interrater reliability.</p>
</td></tr>
<tr><td><code>$subjects</code></td>
<td>
<p>the number of subjects examined.</p>
</td></tr>
<tr><td><code>$raters</code></td>
<td>
<p>the number of raters.</p>
</td></tr>
<tr><td><code>$irr.name</code></td>
<td>
<p>a character string specifying the name of the coefficient.</p>
</td></tr>
<tr><td><code>$value</code></td>
<td>
<p>coefficient of interrater reliability.</p>
</td></tr>
<tr><td><code>$stat.name</code></td>
<td>
<p>a character string specifying the name and the df of the corresponding chi-squared test.</p>
</td></tr>
<tr><td><code>$statistic</code></td>
<td>
<p>the value of the test statistic.</p>
</td></tr>
<tr><td><code>$p.value</code></td>
<td>
<p>the p-value for the test.</p>
</td></tr>
<tr><td><code>$error</code></td>
<td>
<p>the character string of a warning message if ties were found within raters.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Matthias Gamer</p>


<h3>References</h3>

<p>Kendall, M.G. (1948). Rank correlation methods. London: Griffin.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+cor">cor</a></code>,
<code><a href="#topic+meanrho">meanrho</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(anxiety)
kendall(anxiety, TRUE)
</code></pre>

<hr>
<h2 id='kripp.alpha'>calculate Krippendorff's alpha reliability coefficient</h2><span id='topic+kripp.alpha'></span>

<h3>Description</h3>

<p>calculates the alpha coefficient of reliability proposed by Krippendorff
</p>


<h3>Usage</h3>

<pre><code class='language-R'> kripp.alpha(x, method=c("nominal","ordinal","interval","ratio"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kripp.alpha_+3A_x">x</code></td>
<td>
<p>classifier x object matrix of classifications or scores</p>
</td></tr>
<tr><td><code id="kripp.alpha_+3A_method">method</code></td>
<td>
<p>data level of x</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with class '&quot;irrlist&quot;' containing the following components: 
</p>
<table>
<tr><td><code>$method</code></td>
<td>
<p>a character string describing the method.</p>
</td></tr>
<tr><td><code>$subjects</code></td>
<td>
<p>the number of data objects.</p>
</td></tr>
<tr><td><code>$raters</code></td>
<td>
<p>the number of raters.</p>
</td></tr>
<tr><td><code>$irr.name</code></td>
<td>
<p>a character string specifying the name of the coefficient.</p>
</td></tr>
<tr><td><code>$value</code></td>
<td>
<p>value of alpha.</p>
</td></tr>
<tr><td><code>$stat.name</code></td>
<td>
<p>here &quot;nil&quot; as there is no test statistic.</p>
</td></tr>
<tr><td><code>$statistic</code></td>
<td>
<p>the value of the test statistic (NULL).</p>
</td></tr>
<tr><td><code>$p.value</code></td>
<td>
<p>the probability of the test statistic (NULL).</p>
</td></tr>
<tr><td><code>cm</code></td>
<td>
<p>the concordance/discordance matrix used in the calculation of alpha</p>
</td></tr>
<tr><td><code>data.values</code></td>
<td>
<p>a character vector of the unique data values</p>
</td></tr>
<tr><td><code>levx</code></td>
<td>
<p>the unique values of the ratings</p>
</td></tr>
<tr><td><code>nmatchval</code></td>
<td>
<p>the count of matches, used in calculation</p>
</td></tr>
<tr><td><code>data.level</code></td>
<td>
<p>the data level of the ratings (&quot;nominal&quot;,&quot;ordinal&quot;,
&quot;interval&quot;,&quot;ratio&quot;)</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Krippendorff's alpha coefficient is particularly useful where
the level of measurement of classification data is higher than nominal
or ordinal.</p>


<h3>Author(s)</h3>

<p>Jim Lemon</p>


<h3>References</h3>

<p>Krippendorff, K. (1980). Content analysis: An introduction to its methodology. Beverly Hills, CA: Sage.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> # the "C" data from Krippendorff
 nmm&lt;-matrix(c(1,1,NA,1,2,2,3,2,3,3,3,3,3,3,3,3,2,2,2,2,1,2,3,4,4,4,4,4,
 1,1,2,1,2,2,2,2,NA,5,5,5,NA,NA,1,1,NA,NA,3,NA),nrow=4)
 # first assume the default nominal classification
 kripp.alpha(nmm)
 # now use the same data with the other three methods
 kripp.alpha(nmm,"ordinal")
 kripp.alpha(nmm,"interval")
 kripp.alpha(nmm,"ratio") 
</code></pre>

<hr>
<h2 id='maxwell'>Maxwell's RE coefficient for binary data</h2><span id='topic+maxwell'></span>

<h3>Description</h3>

<p>Computes Maxwell's RE as an index of the interrater agreement of binary data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>maxwell(ratings)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="maxwell_+3A_ratings">ratings</code></td>
<td>
<p>n*2 matrix or dataframe, n subjects 2 raters.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Missing data are omitted in a listwise way.
</p>


<h3>Value</h3>

<p>A list with class '&quot;irrlist&quot;' containing the following components:
</p>
<table>
<tr><td><code>$method</code></td>
<td>
<p>a character string describing the method applied for the computation of interrater reliability.</p>
</td></tr>
<tr><td><code>$subjects</code></td>
<td>
<p>the number of subjects examined.</p>
</td></tr>
<tr><td><code>$raters</code></td>
<td>
<p>the number of raters (=2).</p>
</td></tr>
<tr><td><code>$irr.name</code></td>
<td>
<p>a character string specifying the name of the coefficient.</p>
</td></tr>
<tr><td><code>$value</code></td>
<td>
<p>value of RE.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Matthias Gamer</p>


<h3>References</h3>

<p>Maxwell, A.E. (1977). Coefficients of agreement between observers and their interpretation. British Journal of Psychiatry, 130, 79-83.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kappa2">kappa2</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(anxiety)
# Median-split to generate binary data
r1 &lt;- ifelse(anxiety$rater1&lt;median(anxiety$rater1),0,1)
r2 &lt;- ifelse(anxiety$rater2&lt;median(anxiety$rater2),0,1)
maxwell(cbind(r1,r2))
</code></pre>

<hr>
<h2 id='meancor'>Mean of bivariate correlations between raters</h2><span id='topic+meancor'></span>

<h3>Description</h3>

<p>Computes the mean of bivariate Pearson's product moment correlations between raters as an index of the interrater reliability of quantitative data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>meancor(ratings, fisher = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="meancor_+3A_ratings">ratings</code></td>
<td>
<p>n*m matrix or dataframe, n subjects m raters.</p>
</td></tr>
<tr><td><code id="meancor_+3A_fisher">fisher</code></td>
<td>
<p>a logical indicating whether the correlation coefficients should be Fisher z-standardized before averaging.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Missing data are omitted in a listwise way.<br />
The mean of bivariate correlations should not be used as an index of interrater reliability when the variance of ratings differs between raters.<br />
The null hypothesis r=0 could only be tested when Fisher z-standardized values are used for the averaging.<br />
When computing Fisher z-standardized values, perfect correlations are omitted before averaging because z equals +/-Inf in that case.
</p>


<h3>Value</h3>

<p>A list with class '&quot;irrlist&quot;' containing the following components:
</p>
<table>
<tr><td><code>$method</code></td>
<td>
<p>a character string describing the method applied for the computation of interrater reliability.</p>
</td></tr>
<tr><td><code>$subjects</code></td>
<td>
<p>the number of subjects examined.</p>
</td></tr>
<tr><td><code>$raters</code></td>
<td>
<p>the number of raters.</p>
</td></tr>
<tr><td><code>$irr.name</code></td>
<td>
<p>a character string specifying the name of the coefficient.</p>
</td></tr>
<tr><td><code>$value</code></td>
<td>
<p>coefficient of interrater reliability.</p>
</td></tr>
<tr><td><code>$stat.name</code></td>
<td>
<p>a character string specifying the name of the corresponding test statistic.</p>
</td></tr>
<tr><td><code>$statistic</code></td>
<td>
<p>the value of the test statistic.</p>
</td></tr>
<tr><td><code>$p.value</code></td>
<td>
<p>the p-value for the test.</p>
</td></tr>
<tr><td><code>$error</code></td>
<td>
<p>a character string specifying whether correlations were dropped before the computation of the Fisher z-standardized average.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Matthias Gamer</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+cor">cor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(anxiety)
meancor(anxiety)
</code></pre>

<hr>
<h2 id='meanrho'>Mean of bivariate rank correlations between raters</h2><span id='topic+meanrho'></span>

<h3>Description</h3>

<p>Computes the mean of bivariate Spearman's rho rank correlations between raters as an index of the interrater reliability of ordinal data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>meanrho(ratings, fisher = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="meanrho_+3A_ratings">ratings</code></td>
<td>
<p>n*m matrix or dataframe, n subjects m raters.</p>
</td></tr>
<tr><td><code id="meanrho_+3A_fisher">fisher</code></td>
<td>
<p>a logical indicating whether the correlation coefficients should be Fisher z-standardized before averaging.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Missing data are omitted in a listwise way.<br />
The mean of bivariate rank correlations should not be used as an index of interrater reliability when ties within raters occur.<br />
The null hypothesis r=0 could only be tested when Fisher z-standardized values are used for the averaging.<br />
When computing Fisher z-standardized values, perfect correlations are omitted before averaging because z equals +/-Inf in that case.
</p>


<h3>Value</h3>

<p>A list with class '&quot;irrlist&quot;' containing the following components:
</p>
<table>
<tr><td><code>$method</code></td>
<td>
<p>a character string describing the method applied for the computation of interrater reliability.</p>
</td></tr>
<tr><td><code>$subjects</code></td>
<td>
<p>the number of subjects examined.</p>
</td></tr>
<tr><td><code>$raters</code></td>
<td>
<p>the number of raters.</p>
</td></tr>
<tr><td><code>$irr.name</code></td>
<td>
<p>a character string specifying the name of the coefficient.</p>
</td></tr>
<tr><td><code>$value</code></td>
<td>
<p>coefficient of interrater reliability.</p>
</td></tr>
<tr><td><code>$stat.name</code></td>
<td>
<p>a character string specifying the name of the corresponding test statistic.</p>
</td></tr>
<tr><td><code>$statistic</code></td>
<td>
<p>the value of the test statistic.</p>
</td></tr>
<tr><td><code>$p.value</code></td>
<td>
<p>the p-value for the test.</p>
</td></tr>
<tr><td><code>$error</code></td>
<td>
<p>a character specifying whether correlations were dropped before the computation of the Fisher z-standardized average. Additionally, a warning message is created if ties were found within raters.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Matthias Gamer</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+cor">cor</a></code>,
<code><a href="#topic+kendall">kendall</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(anxiety)
meanrho(anxiety, TRUE)
</code></pre>

<hr>
<h2 id='N.cohen.kappa'>Sample Size Calculation for Cohen's Kappa Statistic</h2><span id='topic+N.cohen.kappa'></span>

<h3>Description</h3>

<p>This function is a sample size estimator for the Cohen's Kappa
statistic for a binary outcome. 
Note that any value of &quot;kappa under null&quot; in the interval [0,1] is
acceptable (i.e. k0=0 is a valid null hypothesis).
</p>


<h3>Usage</h3>

<pre><code class='language-R'> N.cohen.kappa(rate1, rate2, k1, k0, alpha=0.05, 
               power=0.8, twosided=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="N.cohen.kappa_+3A_rate1">rate1</code></td>
<td>
<p>the probability that the first rater will record
a positive diagnosis</p>
</td></tr>
<tr><td><code id="N.cohen.kappa_+3A_rate2">rate2</code></td>
<td>
<p>the probability that the second rater will record
a positive diagnosis</p>
</td></tr>
<tr><td><code id="N.cohen.kappa_+3A_k1">k1</code></td>
<td>
<p>the true Cohen's Kappa statistic</p>
</td></tr>
<tr><td><code id="N.cohen.kappa_+3A_k0">k0</code></td>
<td>
<p>the value of kappa under the null hypothesis</p>
</td></tr>
<tr><td><code id="N.cohen.kappa_+3A_alpha">alpha</code></td>
<td>
<p>type I error of test</p>
</td></tr>
<tr><td><code id="N.cohen.kappa_+3A_power">power</code></td>
<td>
<p>the desired power to detect the difference between
true kappa and hypothetical kappa</p>
</td></tr>
<tr><td><code id="N.cohen.kappa_+3A_twosided">twosided</code></td>
<td>
<p>TRUE if test is two-sided</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns required sample size
</p>


<h3>Author(s)</h3>

<p>Ian Fellows</p>


<h3>References</h3>

<p>Cantor, A. B. (1996) Sample-size calculation for Cohen's kappa.
Psychological Methods, 1, 150-153.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kappa2">kappa2</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  # Testing H0: kappa = 0.7 vs. HA: kappa &gt; 0.7 given that
  # kappa = 0.85 and both raters classify 50% of subjects as positive.
  N.cohen.kappa(0.5, 0.5, 0.7, 0.85)
</code></pre>

<hr>
<h2 id='N2.cohen.kappa'>Sample Size Calculation for Cohen's Kappa Statistic with more than one category</h2><span id='topic+N2.cohen.kappa'></span>

<h3>Description</h3>

<p>This function calculates the required sample size for the Cohen's Kappa
statistic when two raters have the same marginal. 
Note that any value of &quot;kappa under null&quot; in the interval [-1,1] is
acceptable (i.e. k0=0 is a valid null hypothesis).
</p>


<h3>Usage</h3>

<pre><code class='language-R'> N2.cohen.kappa(mrg, k1, k0, alpha=0.05, power=0.8, twosided=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="N2.cohen.kappa_+3A_mrg">mrg</code></td>
<td>
<p>a vector of marginal probabilities given by raters</p>
</td></tr>
<tr><td><code id="N2.cohen.kappa_+3A_k1">k1</code></td>
<td>
<p>the true Cohen's Kappa statistic</p>
</td></tr>
<tr><td><code id="N2.cohen.kappa_+3A_k0">k0</code></td>
<td>
<p>the value of kappa under the null hypothesis</p>
</td></tr>
<tr><td><code id="N2.cohen.kappa_+3A_alpha">alpha</code></td>
<td>
<p>type I error of test</p>
</td></tr>
<tr><td><code id="N2.cohen.kappa_+3A_power">power</code></td>
<td>
<p>the desired power to detect the difference between
true kappa and hypothetical kappa</p>
</td></tr>
<tr><td><code id="N2.cohen.kappa_+3A_twosided">twosided</code></td>
<td>
<p>TRUE if test is two-sided</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns required sample size.
</p>


<h3>Author(s)</h3>

<p>Puspendra Singh and Jim Lemon</p>


<h3>References</h3>

<p>Flack, V.F., Afifi, A.A., Lachenbruch, P.A., &amp; Schouten, H.J.A. (1988). Sample size determinations for the two rater kappa statistic. Psychometrika, 53, 321-325.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+N.cohen.kappa">N.cohen.kappa</a></code>,
<code><a href="#topic+kappa2">kappa2</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  require(lpSolve)
  # Testing H0: kappa = 0.4 vs. HA: kappa &gt; 0.4 (=0.6) given that
  # Marginal Probabilities by two raters are (0.2, 0.25, 0.55).
  #
  # one sided test with 80% power:
  N2.cohen.kappa(c(0.2, 0.25, 0.55), k1=0.6, k0=0.4)								
  # one sided test with 90% power:
  N2.cohen.kappa(c(0.2, 0.25, 0.55), k1=0.6, k0=0.4, power=0.9)	  

  # Marginal Probabilities by two raters are (0.2, 0.05, 0.2, 0.05, 0.2, 0.3)
  # Testing H0: kappa = 0.1 vs. HA: kappa &gt; 0.1 (=0.5) given that
  #
  # one sided test with 80% power:
  N2.cohen.kappa(c(0.2, 0.05, 0.2, 0.05, 0.2, 0.3), k1=0.5, k0=0.1)				
</code></pre>

<hr>
<h2 id='print.icclist'>Default printing function for ICC results</h2><span id='topic+print.icclist'></span>

<h3>Description</h3>

<p>Prints the results of the ICC computation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  ## S3 method for class 'icclist'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.icclist_+3A_x">x</code></td>
<td>
<p>a list with class '&quot;icclist&quot;' containing the results of the ICC computation.</p>
</td></tr>
<tr><td><code id="print.icclist_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>'&quot;print.icclist&quot;' is only a printing function and is usually not called directly.
</p>


<h3>Author(s)</h3>

<p>Matthias Gamer</p>


<h3>See Also</h3>

<p><code><a href="#topic+icc">icc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(anxiety)
# "print.icclist" is the default printing function of "icc"
icc(anxiety, model="twoway", type="agreement")
</code></pre>

<hr>
<h2 id='print.irrlist'>Default printing function for various coefficients of interrater reliability</h2><span id='topic+print.irrlist'></span>

<h3>Description</h3>

<p>Prints the results of various functions computing coefficients of interrater reliability.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  ## S3 method for class 'irrlist'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.irrlist_+3A_x">x</code></td>
<td>
<p>a list with class '&quot;irrlist&quot;' containing the results of the interrater reliability computation.</p>
</td></tr>
<tr><td><code id="print.irrlist_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>'&quot;print.irrlist&quot;' is only a printing function and is usually not called directly.
</p>


<h3>Author(s)</h3>

<p>Matthias Gamer</p>


<h3>See Also</h3>

<p><code><a href="#topic+bhapkar">bhapkar</a></code>, 
<code><a href="#topic+finn">finn</a></code>, 
<code><a href="#topic+iota">iota</a></code>, 
<code><a href="#topic+kappa2">kappa2</a></code>, 
<code><a href="#topic+kappam.fleiss">kappam.fleiss</a></code>, 
<code><a href="#topic+kappam.light">kappam.light</a></code>, 
<code><a href="#topic+kripp.alpha">kripp.alpha</a></code>, 
<code><a href="#topic+kendall">kendall</a></code>, 
<code><a href="#topic+maxwell">maxwell</a></code>, 
<code><a href="#topic+meancor">meancor</a></code>, 
<code><a href="#topic+meanrho">meanrho</a></code>, 
<code><a href="#topic+rater.bias">rater.bias</a></code>, 
<code><a href="#topic+robinson">robinson</a></code>, 
<code><a href="#topic+stuart.maxwell">stuart.maxwell</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(anxiety)
# "print.irrlist" is the default printing method of various functions, e.g.
finn(anxiety, 6)
meancor(anxiety)
</code></pre>

<hr>
<h2 id='rater.bias'>Coefficient of rater bias</h2><span id='topic+rater.bias'></span>

<h3>Description</h3>

<p>Calculates a coefficient of systematic bias between two raters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rater.bias(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rater.bias_+3A_x">x</code></td>
<td>
<p>c x c classification matrix or 2 x n or n x 2 matrix of classification
scores into c categories.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>rater.bias</code> calculates a reliability coefficient for two raters
classifying n objects into any number of categories. It will accept either 
a c x c classification matrix of counts of objects falling into c categories
or a 2 x n or n x 2 matrix of classification scores.<br />
The function returns the absolute value of the triangular off-diagnonal 
sum ratio of the cxc classification table and the corresponding test statistic.
A systematic bias between raters can be assumed when the ratio substantially 
deviates from 0.5 while yielding a significant Chi-squared statistic.
</p>


<h3>Value</h3>

<table>
<tr><td><code>method</code></td>
<td>
<p>Name of the method</p>
</td></tr>
<tr><td><code>subjects</code></td>
<td>
<p>Number of subjects</p>
</td></tr>
<tr><td><code>raters</code></td>
<td>
<p>Number of raters (2)</p>
</td></tr>
<tr><td><code>irr.name</code></td>
<td>
<p>Name of the coefficient: ratio of triangular off-diagnonal sums</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>Value of the coefficient</p>
</td></tr>
<tr><td><code>stat.name</code></td>
<td>
<p>Name of the test statistic</p>
</td></tr>
<tr><td><code>statistic</code></td>
<td>
<p>Value of the test statistic</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the probability of the df 1 Chi-square variable</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jim Lemon</p>


<h3>References</h3>

<p>Bishop Y.M.M., Fienberg S.E., &amp; Holland P.W. (1978). Discrete multivariate analysis: theory and practice. Cambridge, Massachusetts: MIT Press.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+mcnemar.test">mcnemar.test</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'> # fake a 2xn matrix of three way classification scores
 ratings &lt;- matrix(sample(1:3,60,TRUE), nrow=2)
 rater.bias(ratings)

 # Example from Bishop, Fienberg &amp; Holland (1978), Table 8.2-1
 data(vision)
 rater.bias(vision)                      
</code></pre>

<hr>
<h2 id='relInterIntra'>Inter- and intra-rater reliability</h2><span id='topic+relInterIntra'></span>

<h3>Description</h3>

<p>&lsquo;<span class="samp">&#8288;relInterIntra&#8288;</span>&rsquo; calculates inter- and intra-rater reliability coefficients.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> relInterIntra(x, nrater=1, raterLabels=NULL, rho0inter=0.6,
               rho0intra=0.8, conf.level=.95)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="relInterIntra_+3A_x">x</code></td>
<td>
<p>Data frame or matrix of rater by object scores</p>
</td></tr>
<tr><td><code id="relInterIntra_+3A_nrater">nrater</code></td>
<td>
<p>Number of raters</p>
</td></tr>
<tr><td><code id="relInterIntra_+3A_raterlabels">raterLabels</code></td>
<td>
<p>Labels for the raters or methods</p>
</td></tr>
<tr><td><code id="relInterIntra_+3A_rho0inter">rho0inter</code></td>
<td>
<p>Null hypothesis value for the inter-rater reliability coefficient</p>
</td></tr>
<tr><td><code id="relInterIntra_+3A_rho0intra">rho0intra</code></td>
<td>
<p>Null hypothesis value for the intra-rater reliability coefficient</p>
</td></tr>
<tr><td><code id="relInterIntra_+3A_conf.level">conf.level</code></td>
<td>
<p>Confidence level for the one-sided confidence interval reported</p>
</td></tr>
</table>


<h3>Value</h3>

<p>nil</p>


<h3>Author(s)</h3>

<p>Tore Wentzel-Larsen</p>


<h3>References</h3>

<p>Eliasziw, M., Young, S.L., Woodbury, M.G., &amp; Fryday-Field, K. (1994). Statistical methodology for the concurrent assessment of interrater and intrarater reliability: Using goniometric measurements as an example. Physical Therapy, 74, 777-788.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># testing code for the Goniometer data from the article:
 table4&lt;-matrix(c(
  -2,16,5,11,7,-7,18,4,0,0,-3,3,7,-6,1,-13,2,4,-10,8,7,-3,-5,5,0,7,-8,1,-3,
  0,16,6,10,8,-8,19,5,-3,0,-2,-1,9,-7,1,-14,1,4,-9,9,6,-2,-5,5,-1,6,-8,1,-3,
  1,15,6,10,6,-8,19,5,-2,-2,-2,1,9,-6,0,-14,0,3,-10,8,7,-4,-7,5,-1,6,-8,2,-3,
  2,12,4,9,5,-9,17,5,-7,1,-4,-1,4,-8,-2,-12,-1,7,-10,2,8,-5,-6,3,-4,4,-10,1,-5,
  1,14,4,7,6,-10,17,5,-6,2,-3,-2,4,-10,-2,-12,0,6,-11,8,7,-5,-8,4,-3,4,-11,-1,-4,
  1,13,4,8,6,-9,17,5,-5,1,-3,1,2,-9,-3,-12,0,4,-10,8,7,-5,-7,4,-4,4,-10,0,-5
  ),ncol=6)
 relInterIntra(x=table4,nrater=2,raterLabels=c('universal','Lamoreux'))
</code></pre>

<hr>
<h2 id='robinson'>Robinson's A</h2><span id='topic+robinson'></span>

<h3>Description</h3>

<p>Computes Robinson's A as an index of the interrater reliability of quantitative data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>robinson(ratings)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="robinson_+3A_ratings">ratings</code></td>
<td>
<p>n*m matrix or dataframe, n subjects m raters.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Missing data are omitted in a listwise way.
</p>


<h3>Value</h3>

<p>A list with class '&quot;irrlist&quot;' containing the following components:
</p>
<table>
<tr><td><code>$method</code></td>
<td>
<p>a character string describing the method applied for the computation of interrater reliability.</p>
</td></tr>
<tr><td><code>$subjects</code></td>
<td>
<p>the number of subjects examined.</p>
</td></tr>
<tr><td><code>$raters</code></td>
<td>
<p>the number of raters.</p>
</td></tr>
<tr><td><code>$irr.name</code></td>
<td>
<p>a character string specifying the name of the coefficient.</p>
</td></tr>
<tr><td><code>$value</code></td>
<td>
<p>coefficient of interrater reliability.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Matthias Gamer</p>


<h3>References</h3>

<p>Robinson, W.S. (1957). The statistical measurement of agreement. American Sociological Review, 22, 17-25.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+finn">finn</a></code>, 
<code><a href="#topic+icc">icc</a></code>, 
<code><a href="#topic+meancor">meancor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(anxiety)
robinson(anxiety)
</code></pre>

<hr>
<h2 id='stuart.maxwell.mh'>Stuart-Maxwell coefficient of concordance for two raters</h2><span id='topic+stuart.maxwell.mh'></span>

<h3>Description</h3>

<p>Calculates the Stuart-Maxwell coefficient of concordance for two raters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stuart.maxwell.mh(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stuart.maxwell.mh_+3A_x">x</code></td>
<td>
<p>c x c classification matrix or matrix of classification scores
into c categories.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>stuart.maxwell.mh</code> calculates a reliability coefficient for two raters
classifying n objects into any number of categories. It will accept either 
a c x c classification matrix of counts of objects falling into c categories
or a c x n or n x c matrix of classification scores.
</p>


<h3>Value</h3>

<p>A list with class &quot;irrlist&quot; containing the following components: 
</p>
<table>
<tr><td><code>$method</code></td>
<td>
<p>a character string describing the method.</p>
</td></tr>
<tr><td><code>$subjects</code></td>
<td>
<p>the number of data objects.</p>
</td></tr>
<tr><td><code>$raters</code></td>
<td>
<p>the number of raters.</p>
</td></tr>
<tr><td><code>$irr.name</code></td>
<td>
<p>the name of the coefficient (Chisq).</p>
</td></tr>
<tr><td><code>$value</code></td>
<td>
<p>the value of the coefficient.</p>
</td></tr>
<tr><td><code>$stat.name</code></td>
<td>
<p>the name and df of the test statistic.</p>
</td></tr>
<tr><td><code>$statistic</code></td>
<td>
<p>the value of the test statistic.</p>
</td></tr>
<tr><td><code>$p.value</code></td>
<td>
<p>the probability of the test statistic.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jim Lemon</p>


<h3>References</h3>

<p>Stuart, A.A. (1955). A test for homogeneity of the marginal distributions in a two-way classification. Biometrika, 42, 412-416.<br /><br />
Maxwell, A.E. (1970) Comparing the classification of subjects by two independent judges. British Journal of Psychiatry, 116, 651-655.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bhapkar">bhapkar</a></code>, 
<code><a href="#topic+rater.bias">rater.bias</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'> # fake a 2xn matrix of three way classification scores
 ratings&lt;-matrix(sample(1:3,60,TRUE), nrow=2)
 stuart.maxwell.mh(ratings)
 
 # Example used from Stuart (1955)
 data(vision)
 stuart.maxwell.mh(vision)
</code></pre>

<hr>
<h2 id='video'>Different raters judging the credibility of videotaped testimonies</h2><span id='topic+video'></span>

<h3>Description</h3>

<p>The data frame contains the credibility ratings of 20 subjects, rated by 4 raters.
Judgements could vary from 1 (not credible) to 6 (highly credible). Variance between and within raters is low.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(video)</code></pre>


<h3>Format</h3>

<p>A data frame with 20 observations on the following 4 variables.
</p>

<dl>
<dt>rater1</dt><dd><p>ratings of rater 1</p>
</dd>
<dt>rater2</dt><dd><p>ratings of rater 2</p>
</dd>
<dt>rater3</dt><dd><p>ratings of rater 3</p>
</dd>
<dt>rater4</dt><dd><p>ratings of rater 4</p>
</dd>
</dl>



<h3>Source</h3>

<p>artificial data
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(video)
apply(video,2,table)
</code></pre>

<hr>
<h2 id='vision'>Eye-testing case records</h2><span id='topic+vision'></span>

<h3>Description</h3>

<p>Case records of the eye-testing of N=7477 female employees in Royal Ordnance factories 
between 1943 and 1946. Data were primarily used by Stuart (1953) to illustrate the 
the estimation and comparison of strengths of association in contingency tables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(anxiety)</code></pre>


<h3>Format</h3>

<p>A data frame with 7477 observations (eye testing results with levels
1st grade, 2nd grade, 3rd grade, 4th Grade) on the following 2 variables.
</p>

<dl>
<dt>r.eye</dt><dd><p>unaided distance vision performance of the right eye</p>
</dd>
<dt>l.eye</dt><dd><p>unaided distance vision performance of the left eye</p>
</dd>
</dl>



<h3>Source</h3>

<p>Stuart, A. (1953). The Estimation and Comparison of Strengths of Association in 
Contingency Tables. Biometrika, 40, 105-110.
</p>


<h3>References</h3>

<p>Stuart, A. (1953). The Estimation and Comparison of Strengths of Association in 
Contingency Tables. Biometrika, 40, 105-110.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(vision)
table(vision$r.eye, vision$l.eye)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
