<!DOCTYPE html><html lang="en"><head><title>Help for package mall</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {mall}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#llm_classify'><p>Categorize data as one of options given</p></a></li>
<li><a href='#llm_custom'><p>Send a custom prompt to the LLM</p></a></li>
<li><a href='#llm_extract'><p>Extract entities from text</p></a></li>
<li><a href='#llm_sentiment'><p>Sentiment analysis</p></a></li>
<li><a href='#llm_summarize'><p>Summarize text</p></a></li>
<li><a href='#llm_translate'><p>Translates text to a specific language</p></a></li>
<li><a href='#llm_use'><p>Specify the model to use</p></a></li>
<li><a href='#llm_verify'><p>Verify if a statement about the text is true or not</p></a></li>
<li><a href='#m_backend_prompt'><p>Functions to integrate different back-ends</p></a></li>
<li><a href='#reviews'><p>Mini reviews data set</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Run Multiple Large Language Model Predictions Against a Table,
or Vectors</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Run multiple 'Large Language Model' predictions against a table. The
    predictions run row-wise over a specified column. It works using a 
    one-shot prompt, along with the current row's content. The prompt that is used
    will depend of the type of analysis needed.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Imports:</td>
<td>cli, dplyr, fs, glue, jsonlite, ollamar, rlang</td>
</tr>
<tr>
<td>Suggests:</td>
<td>dbplyr, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://mlverse.github.io/mall/">https://mlverse.github.io/mall/</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-10-22 12:51:31 UTC; edgar</td>
</tr>
<tr>
<td>Author:</td>
<td>Edgar Ruiz [aut, cre],
  Posit Software, PBC [cph, fnd]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Edgar Ruiz &lt;edgar@posit.co&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-10-24 14:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='llm_classify'>Categorize data as one of options given</h2><span id='topic+llm_classify'></span><span id='topic+llm_vec_classify'></span>

<h3>Description</h3>

<p>Use a Large Language Model (LLM) to classify the provided text as one of the
options provided via the <code>labels</code> argument.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>llm_classify(
  .data,
  col,
  labels,
  pred_name = ".classify",
  additional_prompt = ""
)

llm_vec_classify(x, labels, additional_prompt = "", preview = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="llm_classify_+3A_.data">.data</code></td>
<td>
<p>A <code>data.frame</code> or <code>tbl</code> object that contains the text to be
analyzed</p>
</td></tr>
<tr><td><code id="llm_classify_+3A_col">col</code></td>
<td>
<p>The name of the field to analyze, supports <code>tidy-eval</code></p>
</td></tr>
<tr><td><code id="llm_classify_+3A_labels">labels</code></td>
<td>
<p>A character vector with at least 2 labels to classify the text
as</p>
</td></tr>
<tr><td><code id="llm_classify_+3A_pred_name">pred_name</code></td>
<td>
<p>A character vector with the name of the new column where the
prediction will be placed</p>
</td></tr>
<tr><td><code id="llm_classify_+3A_additional_prompt">additional_prompt</code></td>
<td>
<p>Inserts this text into the prompt sent to the LLM</p>
</td></tr>
<tr><td><code id="llm_classify_+3A_x">x</code></td>
<td>
<p>A vector that contains the text to be analyzed</p>
</td></tr>
<tr><td><code id="llm_classify_+3A_preview">preview</code></td>
<td>
<p>It returns the R call that would have been used to run the
prediction. It only returns the first record in <code>x</code>. Defaults to <code>FALSE</code>
Applies to vector function only.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>llm_classify</code> returns a <code>data.frame</code> or <code>tbl</code> object.
<code>llm_vec_classify</code> returns a vector that is the same length as <code>x</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(mall)

data("reviews")

llm_use("ollama", "llama3.2", seed = 100, .silent = TRUE)

llm_classify(reviews, review, c("appliance", "computer"))

# Use 'pred_name' to customize the new column's name
llm_classify(
  reviews,
  review,
  c("appliance", "computer"),
  pred_name = "prod_type"
)

# Pass custom values for each classification
llm_classify(reviews, review, c("appliance" ~ 1, "computer" ~ 2))

# For character vectors, instead of a data frame, use this function
llm_vec_classify(
  c("this is important!", "just whenever"),
  c("urgent", "not urgent")
)

# To preview the first call that will be made to the downstream R function
llm_vec_classify(
  c("this is important!", "just whenever"),
  c("urgent", "not urgent"),
  preview = TRUE
)

</code></pre>

<hr>
<h2 id='llm_custom'>Send a custom prompt to the LLM</h2><span id='topic+llm_custom'></span><span id='topic+llm_vec_custom'></span>

<h3>Description</h3>

<p>Use a Large Language Model (LLM) to process the provided text using the
instructions from <code>prompt</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>llm_custom(.data, col, prompt = "", pred_name = ".pred", valid_resps = "")

llm_vec_custom(x, prompt = "", valid_resps = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="llm_custom_+3A_.data">.data</code></td>
<td>
<p>A <code>data.frame</code> or <code>tbl</code> object that contains the text to be
analyzed</p>
</td></tr>
<tr><td><code id="llm_custom_+3A_col">col</code></td>
<td>
<p>The name of the field to analyze, supports <code>tidy-eval</code></p>
</td></tr>
<tr><td><code id="llm_custom_+3A_prompt">prompt</code></td>
<td>
<p>The prompt to append to each record sent to the LLM</p>
</td></tr>
<tr><td><code id="llm_custom_+3A_pred_name">pred_name</code></td>
<td>
<p>A character vector with the name of the new column where the
prediction will be placed</p>
</td></tr>
<tr><td><code id="llm_custom_+3A_valid_resps">valid_resps</code></td>
<td>
<p>If the response from the LLM is not open, but
deterministic, provide the options in a vector. This function will set to
<code>NA</code> any response not in the options</p>
</td></tr>
<tr><td><code id="llm_custom_+3A_x">x</code></td>
<td>
<p>A vector that contains the text to be analyzed</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>llm_custom</code> returns a <code>data.frame</code> or <code>tbl</code> object.
<code>llm_vec_custom</code> returns a vector that is the same length as <code>x</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(mall)

data("reviews")

llm_use("ollama", "llama3.2", seed = 100, .silent = TRUE)

my_prompt &lt;- paste(
  "Answer a question.",
  "Return only the answer, no explanation",
  "Acceptable answers are 'yes', 'no'",
  "Answer this about the following text, is this a happy customer?:"
)

reviews |&gt;
  llm_custom(review, my_prompt)

</code></pre>

<hr>
<h2 id='llm_extract'>Extract entities from text</h2><span id='topic+llm_extract'></span><span id='topic+llm_vec_extract'></span>

<h3>Description</h3>

<p>Use a Large Language Model (LLM) to extract specific entity, or entities,
from the provided text
</p>


<h3>Usage</h3>

<pre><code class='language-R'>llm_extract(
  .data,
  col,
  labels,
  expand_cols = FALSE,
  additional_prompt = "",
  pred_name = ".extract"
)

llm_vec_extract(x, labels = c(), additional_prompt = "", preview = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="llm_extract_+3A_.data">.data</code></td>
<td>
<p>A <code>data.frame</code> or <code>tbl</code> object that contains the text to be
analyzed</p>
</td></tr>
<tr><td><code id="llm_extract_+3A_col">col</code></td>
<td>
<p>The name of the field to analyze, supports <code>tidy-eval</code></p>
</td></tr>
<tr><td><code id="llm_extract_+3A_labels">labels</code></td>
<td>
<p>A vector with the entities to extract from the text</p>
</td></tr>
<tr><td><code id="llm_extract_+3A_expand_cols">expand_cols</code></td>
<td>
<p>If multiple <code>labels</code> are passed, this is a flag that
tells the function to create a new column per item in <code>labels</code>. If
<code>labels</code> is a named vector, this function will use those names as the
new column names, if not, the function will use a sanitized version of
the content as the name.</p>
</td></tr>
<tr><td><code id="llm_extract_+3A_additional_prompt">additional_prompt</code></td>
<td>
<p>Inserts this text into the prompt sent to the LLM</p>
</td></tr>
<tr><td><code id="llm_extract_+3A_pred_name">pred_name</code></td>
<td>
<p>A character vector with the name of the new column where the
prediction will be placed</p>
</td></tr>
<tr><td><code id="llm_extract_+3A_x">x</code></td>
<td>
<p>A vector that contains the text to be analyzed</p>
</td></tr>
<tr><td><code id="llm_extract_+3A_preview">preview</code></td>
<td>
<p>It returns the R call that would have been used to run the
prediction. It only returns the first record in <code>x</code>. Defaults to <code>FALSE</code>
Applies to vector function only.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>llm_extract</code> returns a <code>data.frame</code> or <code>tbl</code> object.
<code>llm_vec_extract</code> returns a vector that is the same length as <code>x</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(mall)

data("reviews")

llm_use("ollama", "llama3.2", seed = 100, .silent = TRUE)

# Use 'labels' to let the function know what to extract
llm_extract(reviews, review, labels = "product")

# Use 'pred_name' to customize the new column's name
llm_extract(reviews, review, "product", pred_name = "prod")

# Pass a vector to request multiple things, the results will be pipe delimeted
# in a single column
llm_extract(reviews, review, c("product", "feelings"))

# To get multiple columns, use 'expand_cols'
llm_extract(reviews, review, c("product", "feelings"), expand_cols = TRUE)

# Pass a named vector to set the resulting column names
llm_extract(
  .data = reviews,
  col = review,
  labels = c(prod = "product", feels = "feelings"),
  expand_cols = TRUE
)

# For character vectors, instead of a data frame, use this function
llm_vec_extract("bob smith, 123 3rd street", c("name", "address"))

# To preview the first call that will be made to the downstream R function
llm_vec_extract(
  "bob smith, 123 3rd street",
  c("name", "address"),
  preview = TRUE
)

</code></pre>

<hr>
<h2 id='llm_sentiment'>Sentiment analysis</h2><span id='topic+llm_sentiment'></span><span id='topic+llm_vec_sentiment'></span>

<h3>Description</h3>

<p>Use a Large Language Model (LLM) to perform sentiment analysis
from the provided text
</p>


<h3>Usage</h3>

<pre><code class='language-R'>llm_sentiment(
  .data,
  col,
  options = c("positive", "negative", "neutral"),
  pred_name = ".sentiment",
  additional_prompt = ""
)

llm_vec_sentiment(
  x,
  options = c("positive", "negative", "neutral"),
  additional_prompt = "",
  preview = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="llm_sentiment_+3A_.data">.data</code></td>
<td>
<p>A <code>data.frame</code> or <code>tbl</code> object that contains the text to be
analyzed</p>
</td></tr>
<tr><td><code id="llm_sentiment_+3A_col">col</code></td>
<td>
<p>The name of the field to analyze, supports <code>tidy-eval</code></p>
</td></tr>
<tr><td><code id="llm_sentiment_+3A_options">options</code></td>
<td>
<p>A vector with the options that the LLM should use to assign
a sentiment to the text. Defaults to: 'positive', 'negative', 'neutral'</p>
</td></tr>
<tr><td><code id="llm_sentiment_+3A_pred_name">pred_name</code></td>
<td>
<p>A character vector with the name of the new column where the
prediction will be placed</p>
</td></tr>
<tr><td><code id="llm_sentiment_+3A_additional_prompt">additional_prompt</code></td>
<td>
<p>Inserts this text into the prompt sent to the LLM</p>
</td></tr>
<tr><td><code id="llm_sentiment_+3A_x">x</code></td>
<td>
<p>A vector that contains the text to be analyzed</p>
</td></tr>
<tr><td><code id="llm_sentiment_+3A_preview">preview</code></td>
<td>
<p>It returns the R call that would have been used to run the
prediction. It only returns the first record in <code>x</code>. Defaults to <code>FALSE</code>
Applies to vector function only.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>llm_sentiment</code> returns a <code>data.frame</code> or <code>tbl</code> object.
<code>llm_vec_sentiment</code> returns a vector that is the same length as <code>x</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(mall)

data("reviews")

llm_use("ollama", "llama3.2", seed = 100, .silent = TRUE)

llm_sentiment(reviews, review)

# Use 'pred_name' to customize the new column's name
llm_sentiment(reviews, review, pred_name = "review_sentiment")

# Pass custom sentiment options
llm_sentiment(reviews, review, c("positive", "negative"))

# Specify values to return per sentiment
llm_sentiment(reviews, review, c("positive" ~ 1, "negative" ~ 0))

# For character vectors, instead of a data frame, use this function
llm_vec_sentiment(c("I am happy", "I am sad"))

# To preview the first call that will be made to the downstream R function
llm_vec_sentiment(c("I am happy", "I am sad"), preview = TRUE)

</code></pre>

<hr>
<h2 id='llm_summarize'>Summarize text</h2><span id='topic+llm_summarize'></span><span id='topic+llm_vec_summarize'></span>

<h3>Description</h3>

<p>Use a Large Language Model (LLM) to summarize text
</p>


<h3>Usage</h3>

<pre><code class='language-R'>llm_summarize(
  .data,
  col,
  max_words = 10,
  pred_name = ".summary",
  additional_prompt = ""
)

llm_vec_summarize(x, max_words = 10, additional_prompt = "", preview = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="llm_summarize_+3A_.data">.data</code></td>
<td>
<p>A <code>data.frame</code> or <code>tbl</code> object that contains the text to be
analyzed</p>
</td></tr>
<tr><td><code id="llm_summarize_+3A_col">col</code></td>
<td>
<p>The name of the field to analyze, supports <code>tidy-eval</code></p>
</td></tr>
<tr><td><code id="llm_summarize_+3A_max_words">max_words</code></td>
<td>
<p>The maximum number of words that the LLM should use in the
summary. Defaults to 10.</p>
</td></tr>
<tr><td><code id="llm_summarize_+3A_pred_name">pred_name</code></td>
<td>
<p>A character vector with the name of the new column where the
prediction will be placed</p>
</td></tr>
<tr><td><code id="llm_summarize_+3A_additional_prompt">additional_prompt</code></td>
<td>
<p>Inserts this text into the prompt sent to the LLM</p>
</td></tr>
<tr><td><code id="llm_summarize_+3A_x">x</code></td>
<td>
<p>A vector that contains the text to be analyzed</p>
</td></tr>
<tr><td><code id="llm_summarize_+3A_preview">preview</code></td>
<td>
<p>It returns the R call that would have been used to run the
prediction. It only returns the first record in <code>x</code>. Defaults to <code>FALSE</code>
Applies to vector function only.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>llm_summarize</code> returns a <code>data.frame</code> or <code>tbl</code> object.
<code>llm_vec_summarize</code> returns a vector that is the same length as <code>x</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(mall)

data("reviews")

llm_use("ollama", "llama3.2", seed = 100, .silent = TRUE)

# Use max_words to set the maximum number of words to use for the summary
llm_summarize(reviews, review, max_words = 5)

# Use 'pred_name' to customize the new column's name
llm_summarize(reviews, review, 5, pred_name = "review_summary")

# For character vectors, instead of a data frame, use this function
llm_vec_summarize(
  "This has been the best TV I've ever used. Great screen, and sound.",
  max_words = 5
)

# To preview the first call that will be made to the downstream R function
llm_vec_summarize(
  "This has been the best TV I've ever used. Great screen, and sound.",
  max_words = 5,
  preview = TRUE
)

</code></pre>

<hr>
<h2 id='llm_translate'>Translates text to a specific language</h2><span id='topic+llm_translate'></span><span id='topic+llm_vec_translate'></span>

<h3>Description</h3>

<p>Use a Large Language Model (LLM) to translate a text to a specific
language
</p>


<h3>Usage</h3>

<pre><code class='language-R'>llm_translate(
  .data,
  col,
  language,
  pred_name = ".translation",
  additional_prompt = ""
)

llm_vec_translate(x, language, additional_prompt = "", preview = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="llm_translate_+3A_.data">.data</code></td>
<td>
<p>A <code>data.frame</code> or <code>tbl</code> object that contains the text to be
analyzed</p>
</td></tr>
<tr><td><code id="llm_translate_+3A_col">col</code></td>
<td>
<p>The name of the field to analyze, supports <code>tidy-eval</code></p>
</td></tr>
<tr><td><code id="llm_translate_+3A_language">language</code></td>
<td>
<p>Target language to translate the text to</p>
</td></tr>
<tr><td><code id="llm_translate_+3A_pred_name">pred_name</code></td>
<td>
<p>A character vector with the name of the new column where the
prediction will be placed</p>
</td></tr>
<tr><td><code id="llm_translate_+3A_additional_prompt">additional_prompt</code></td>
<td>
<p>Inserts this text into the prompt sent to the LLM</p>
</td></tr>
<tr><td><code id="llm_translate_+3A_x">x</code></td>
<td>
<p>A vector that contains the text to be analyzed</p>
</td></tr>
<tr><td><code id="llm_translate_+3A_preview">preview</code></td>
<td>
<p>It returns the R call that would have been used to run the
prediction. It only returns the first record in <code>x</code>. Defaults to <code>FALSE</code>
Applies to vector function only.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>llm_translate</code> returns a <code>data.frame</code> or <code>tbl</code> object.
<code>llm_vec_translate</code> returns a vector that is the same length as <code>x</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(mall)

data("reviews")

llm_use("ollama", "llama3.2", seed = 100, .silent = TRUE)

# Pass the desired language to translate to
llm_translate(reviews, review, "spanish")

</code></pre>

<hr>
<h2 id='llm_use'>Specify the model to use</h2><span id='topic+llm_use'></span>

<h3>Description</h3>

<p>Allows us to specify the back-end provider, model to use during the current
R session
</p>


<h3>Usage</h3>

<pre><code class='language-R'>llm_use(
  backend = NULL,
  model = NULL,
  ...,
  .silent = FALSE,
  .cache = NULL,
  .force = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="llm_use_+3A_backend">backend</code></td>
<td>
<p>The name of an supported back-end provider. Currently only
'ollama' is supported.</p>
</td></tr>
<tr><td><code id="llm_use_+3A_model">model</code></td>
<td>
<p>The name of model supported by the back-end provider</p>
</td></tr>
<tr><td><code id="llm_use_+3A_...">...</code></td>
<td>
<p>Additional arguments that this function will pass down to the
integrating function. In the case of Ollama, it will pass those arguments to
<code>ollamar::chat()</code>.</p>
</td></tr>
<tr><td><code id="llm_use_+3A_.silent">.silent</code></td>
<td>
<p>Avoids console output</p>
</td></tr>
<tr><td><code id="llm_use_+3A_.cache">.cache</code></td>
<td>
<p>The path to save model results, so they can be re-used if
the same operation is ran again. To turn off, set this argument to an empty
character: <code>""</code>. It defaults to a temp folder. If this argument is left
<code>NULL</code> when calling this function, no changes to the path will be made.</p>
</td></tr>
<tr><td><code id="llm_use_+3A_.force">.force</code></td>
<td>
<p>Flag that tell the function to reset all of the settings in the
R session</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>mall_session</code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(mall)

llm_use("ollama", "llama3.2")

# Additional arguments will be passed 'as-is' to the
# downstream R function in this example, to ollama::chat()
llm_use("ollama", "llama3.2", seed = 100, temperature = 0.1)

# During the R session, you can change any argument
# individually and it will retain all of previous
# arguments used
llm_use(temperature = 0.3)

# Use .cache to modify the target folder for caching
llm_use(.cache = "_my_cache")

# Leave .cache empty to turn off this functionality
llm_use(.cache = "")

# Use .silent to avoid the print out
llm_use(.silent = TRUE)

</code></pre>

<hr>
<h2 id='llm_verify'>Verify if a statement about the text is true or not</h2><span id='topic+llm_verify'></span><span id='topic+llm_vec_verify'></span>

<h3>Description</h3>

<p>Use a Large Language Model (LLM) to see if something is true or not
based the provided text
</p>


<h3>Usage</h3>

<pre><code class='language-R'>llm_verify(
  .data,
  col,
  what,
  yes_no = factor(c(1, 0)),
  pred_name = ".verify",
  additional_prompt = ""
)

llm_vec_verify(
  x,
  what,
  yes_no = factor(c(1, 0)),
  additional_prompt = "",
  preview = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="llm_verify_+3A_.data">.data</code></td>
<td>
<p>A <code>data.frame</code> or <code>tbl</code> object that contains the text to be
analyzed</p>
</td></tr>
<tr><td><code id="llm_verify_+3A_col">col</code></td>
<td>
<p>The name of the field to analyze, supports <code>tidy-eval</code></p>
</td></tr>
<tr><td><code id="llm_verify_+3A_what">what</code></td>
<td>
<p>The statement or question that needs to be verified against the
provided text</p>
</td></tr>
<tr><td><code id="llm_verify_+3A_yes_no">yes_no</code></td>
<td>
<p>A size 2 vector that specifies the expected output. It is
positional. The first item is expected to be value to return if the
statement about the provided text is true, and the second if it is not. Defaults
to: <code>factor(c(1, 0))</code></p>
</td></tr>
<tr><td><code id="llm_verify_+3A_pred_name">pred_name</code></td>
<td>
<p>A character vector with the name of the new column where the
prediction will be placed</p>
</td></tr>
<tr><td><code id="llm_verify_+3A_additional_prompt">additional_prompt</code></td>
<td>
<p>Inserts this text into the prompt sent to the LLM</p>
</td></tr>
<tr><td><code id="llm_verify_+3A_x">x</code></td>
<td>
<p>A vector that contains the text to be analyzed</p>
</td></tr>
<tr><td><code id="llm_verify_+3A_preview">preview</code></td>
<td>
<p>It returns the R call that would have been used to run the
prediction. It only returns the first record in <code>x</code>. Defaults to <code>FALSE</code>
Applies to vector function only.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>llm_verify</code> returns a <code>data.frame</code> or <code>tbl</code> object.
<code>llm_vec_verify</code> returns a vector that is the same length as <code>x</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(mall)

data("reviews")

llm_use("ollama", "llama3.2", seed = 100, .silent = TRUE)

# By default it will return 1 for 'true', and 0 for 'false',
# the new column will be a factor type
llm_verify(reviews, review, "is the customer happy")

# The yes_no argument can be modified to return a different response
# than 1 or 0. First position will be 'true' and second, 'false'
llm_verify(reviews, review, "is the customer happy", c("y", "n"))

# Number can also be used, this would be in the case that you wish to match
# the output values of existing predictions
llm_verify(reviews, review, "is the customer happy", c(2, 1))


</code></pre>

<hr>
<h2 id='m_backend_prompt'>Functions to integrate different back-ends</h2><span id='topic+m_backend_prompt'></span><span id='topic+m_backend_submit'></span>

<h3>Description</h3>

<p>Functions to integrate different back-ends
</p>


<h3>Usage</h3>

<pre><code class='language-R'>m_backend_prompt(backend, additional)

m_backend_submit(backend, x, prompt, preview = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="m_backend_prompt_+3A_backend">backend</code></td>
<td>
<p>An <code>mall_session</code> object</p>
</td></tr>
<tr><td><code id="m_backend_prompt_+3A_additional">additional</code></td>
<td>
<p>Additional text to insert to the <code>base_prompt</code></p>
</td></tr>
<tr><td><code id="m_backend_prompt_+3A_x">x</code></td>
<td>
<p>The body of the text to be submitted to the LLM</p>
</td></tr>
<tr><td><code id="m_backend_prompt_+3A_prompt">prompt</code></td>
<td>
<p>The additional information to add to the submission</p>
</td></tr>
<tr><td><code id="m_backend_prompt_+3A_preview">preview</code></td>
<td>
<p>If <code>TRUE</code>, it will display the resulting R call of the
first text in <code>x</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>m_backend_submit</code> does not return an object. <code>m_backend_prompt</code>
returns a list of functions that contain the base prompts.
</p>

<hr>
<h2 id='reviews'>Mini reviews data set</h2><span id='topic+reviews'></span>

<h3>Description</h3>

<p>Mini reviews data set
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reviews
</code></pre>


<h3>Format</h3>

<p>A data frame that contains 3 records. The records are of fictitious
product reviews.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mall)
data(reviews)
reviews

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
