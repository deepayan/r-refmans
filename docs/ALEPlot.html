<!DOCTYPE html><html lang="en"><head><title>Help for package ALEPlot</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ALEPlot}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ALEPlot-package'>
<p>Accumulated Local Effects (ALE) Plots and Partial Dependence</p>
(PD) Plots</a></li>
<li><a href='#ALEPlot'>
<p>Accumulated Local Effects (ALE) Plots</p></a></li>
<li><a href='#PDPlot'>
<p>Partial Dependence (PD) Plots</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Accumulated Local Effects (ALE) Plots and Partial Dependence
(PD) Plots</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2018-05-22</td>
</tr>
<tr>
<td>Author:</td>
<td>Dan Apley</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Dan Apley &lt;apley@northwestern.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Visualizes the main effects of individual predictor variables and their second-order interaction effects in black-box supervised learning models. The package creates either Accumulated Local Effects (ALE) plots and/or Partial Dependence (PD) plots, given a fitted supervised learning model.</td>
</tr>
<tr>
<td>Imports:</td>
<td>yaImpute</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Suggests:</td>
<td>R.rsp, nnet</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>R.rsp</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.0.1</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2018-05-24 15:00:22 UTC; thair</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2018-05-24 16:14:07 UTC</td>
</tr>
</table>
<hr>
<h2 id='ALEPlot-package'>
Accumulated Local Effects (ALE) Plots and Partial Dependence
(PD) Plots
</h2><span id='topic+ALEPlot-package'></span>

<h3>Description</h3>

<p>Visualizes the main effects of individual predictor variables and their second-order interaction effects in black-box supervised learning models. The package creates either Accumulated Local Effects (ALE) plots and/or Partial Dependence (PD) plots, given a fitted supervised learning model.
</p>


<h3>Details</h3>

<p>See the two individual functions <code><a href="#topic+ALEPlot">ALEPlot</a></code> and <code><a href="#topic+PDPlot">PDPlot</a></code> that are included in this package.
</p>


<h3>Author(s)</h3>

<p>Dan Apley
</p>
<p>Maintainer: Dan Apley &lt;apley@northwestern.edu&gt;
</p>


<h3>References</h3>

<p>Apley, D. W. (2016), &quot;Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models,&quot; submitted for publication.
</p>

<hr>
<h2 id='ALEPlot'>
Accumulated Local Effects (ALE) Plots
</h2><span id='topic+ALEPlot'></span>

<h3>Description</h3>

<p>Computes and plots accumulated local effects (ALE) plots for a fitted supervised learning model. The effects can be either a main effect for an individual predictor (<code>length(J) = 1</code>) or a second-order interaction effect for a pair of predictors (<code>length(J) = 2</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ALEPlot(X, X.model, pred.fun, J, K = 40, NA.plot = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ALEPlot_+3A_x">X</code></td>
<td>
<p>The data frame of predictor variables to which the supervised learning model was fit. The names of the predictor variables must be the same as when the model was fit. The response variable should not be included in <code>X</code>.</p>
</td></tr>
<tr><td><code id="ALEPlot_+3A_x.model">X.model</code></td>
<td>
<p>The fitted supervised learning model object (e.g., a tree, random forest, neural network, etc.), typically an object to which a built-in <code>predict</code> command associated with that object can be applied.</p>
</td></tr>
<tr><td><code id="ALEPlot_+3A_pred.fun">pred.fun</code></td>
<td>
<p>A user-supplied function that will be used to predict the response for <code>X.model</code> for some specified inputs. <code>pred.fun</code> has two arguments. The first argument is named <code>X.model</code> and must be the same object as the <code>X.model</code> argument to the <code>ALEPlot</code> function. The second argument is named <code>newdata</code> and is a data frame of predictor values at which the object <code>X.model</code> is to be predicted. The output of <code>pred.fun</code> must be a numeric vector of predictions having length equal to the number of rows of <code>newdata</code>. For most <code>X.model</code> objects, <code>pred.fun</code> can simply call the <code>predict</code> function that was written as part of that modeling object package, assuming the package contains a <code>predict</code> function. An example of where a more customized <code>pred.fun</code> would be used is a multi (&gt; 2) class classification problem for which the built-in predict function returns a vector of predicted probabilities, one for each response class. In this case it may make sense to have <code>pred.fun</code> return the predicted probabilities (or its log-odds, etc.) for one particular class of interest.</p>
</td></tr>
<tr><td><code id="ALEPlot_+3A_j">J</code></td>
<td>
<p>A numeric scalar or two-length vector of indices of the predictors for which the ALE plot will be calculated. <code>J</code> is either a single index (for a main effects plot) or a pair of indices (for a second-order interaction plot). For a single index, the corresponding predictor must be either numeric or a factor. For a pair of indices, the corresponding predictors must be either both numeric or the first a factor and the second numeric.</p>
</td></tr>
<tr><td><code id="ALEPlot_+3A_k">K</code></td>
<td>
<p>A numeric scalar that specifies the number of intervals into which the predictor range is divided when calculating the ALE plot effects. If <code>length(J) = 2</code>, the same <code>K</code> will be used for both predictors, resulting in an array of <code>K^2</code> cells over the two-dimensional predictor space. Note that the algorithm may adjust (reduce) <code>K</code> internally if the predictors are discrete and have many repeated values. <code>K</code> is only used if the predictor is numeric. For factor predictors, the equivalent of <code>K</code> is the number of used levels of the factor, which is automatically determined internally.</p>
</td></tr>
<tr><td><code id="ALEPlot_+3A_na.plot">NA.plot</code></td>
<td>
<p>A logical value that is only used if <code>length(J) = 2</code>. If <code>NA.plot = TRUE</code> (the default), the ALE second-order effects are also plotted for empty cells. Empty cells are defined as cells in the <code>(X[,J[1]], X[,J[2]])</code> space into which no training observations fall. If <code>NA.plot = FALSE</code>, the accumulated local second-order effects are only plotted for non-empty cells, and black rectangles are plotted over any empty cells to indicate their locations. Either way, when accumulating the local second-order effects, values are need for the empty cells, and these values are taken to be the local second-order effects for the nearest-neighbor non-empty cell.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See the Apley (2016) reference paper listed below for details. For <code>J = j</code> (i.e., if the index for a single predictor <code class="reqn">x_j</code> is specified), the function calculates and returns the ALE main effect of <code class="reqn">x_j</code>, which is denoted by <code class="reqn">f_{j,ALE}(x_j)</code> in Apley (2016). It also plots <code class="reqn">f_{j,ALE}(x_j)</code>. For <code>J = c(j1,j2)</code> (i.e., if the indices for a pair of predictors <code class="reqn">(x_{j1},x_{j2})</code> are specified), the function calculates and returns the ALE second-order interaction effect of <code class="reqn">(x_{j1},x_{j2})</code>, which is denoted by <code class="reqn">f_{{j1,j2},ALE}(x_{j1},x_{j2})</code> in Apley (2016). It also plots <code class="reqn">f_{{j1,j2},ALE}(x_{j1},x_{j2})</code>. 
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>K</code></td>
<td>
<p>The same as the input argument <code>K</code>, but possibly adjusted internally. For numeric predictors, <code>K</code> is the number of intervals into which the range of each predictor is divided. If the predictor is discrete with many repeated values, <code>K</code> can be reduced internally, as mentioned above. For <code>length(J)=1</code>, <code>K</code> is an integer. For <code>length(J)=2</code>, <code>K = c(K1, K2)</code>, where <code>K1</code> and <code>K2</code> are the numbers of intervals for the <code>X[,J(1)]</code> and <code>X[,J(2)]</code> ranges, respectively. For factor predictors, <code>K</code> is the number of non-empty levels, which is calculated internally.</p>
</td></tr>
<tr><td><code>f.values</code></td>
<td>
<p>If <code>length(J) = 1</code>, a vector of ALE plot function values at the predictor values in <code>x.values</code>. If <code>length(J) = 2</code>, <code>f.values</code> is a matrix of ALE plot function values at the grid of values defined by the <code>X[,J(1)]</code> and <code>X[,J(2)]</code> values in <code>x.values</code>. The rows of <code>f.values</code> correspond to <code>X[,J(1)]</code>, and the columns to <code>X[,J(2)]</code>.</p>
</td></tr>
<tr><td><code>x.values</code></td>
<td>
<p>For numeric predictors, if <code>length(J) = 1</code>, a (<code>K+1</code>)-length vector specifying the ordered predictor values at which the ALE plot function is calculated. These are the break points for the <code>K</code> intervals into which the predictor range is divided, plus the lower boundary of the first interval and the upper boundary of the last interval. If <code>length(J) = 2</code>, a list of two such vectors, the first containing the <code>X[,J(1)]</code> values and the second containing the <code>X[,J(2)]</code> values at which the ALE plot function is calculated. <code>x.values</code> is the same for factor predictors, except it is a <code>K</code>-length character vector containing the ordered levels of the predictor (the ordering is determined internally, based on the similarity of the predictor in question to the other predictors), where <code>K</code> is the number of used levels. The elements of <code>f.values</code> are ordered accordingly.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Dan Apley
</p>


<h3>References</h3>

<p>Apley, D. W. (2016), &quot;Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models,&quot; submitted for publication.
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+PDPlot">PDPlot</a></code> for partial dependence plots.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>########################################################################
## A transparent example in which the supervised learning model is a linear regression \code{lm},
## but we will pretend it is black-box
########################################################################

## Generate some data and fit a \code{lm} supervised learning model
N=500
x1 &lt;- runif(N, min=0, max=1)
x2 &lt;- runif(N, min=0, max=1)
x3 &lt;- runif(N, min=0, max=1)
y = x1 + 2*x2^2 + rnorm(N, 0, 0.1)
DAT = data.frame(y, x1, x2, x3)
lm.DAT = lm(y ~ .^2 + I(x1^2) + I(x2^2) + I(x3^2), DAT)

## Define the predictive function (easy in this case, since \code{lm} has a built-in 
## predict function that suffices)
yhat &lt;- function(X.model, newdata) as.numeric(predict(X.model, newdata))

## Calculate and plot the ALE main and second-order interaction effects of x1, x2, x3
par(mfrow = c(2,3))
ALE.1=ALEPlot(DAT[,2:4], lm.DAT, pred.fun=yhat, J=1, K=50, NA.plot = TRUE)
ALE.2=ALEPlot(DAT[,2:4], lm.DAT, pred.fun=yhat, J=2, K=50, NA.plot = TRUE)
ALE.3=ALEPlot(DAT[,2:4], lm.DAT, pred.fun=yhat, J=3, K=50, NA.plot = TRUE)
ALE.12=ALEPlot(DAT[,2:4], lm.DAT, pred.fun=yhat, J=c(1,2), K=20, NA.plot = TRUE)
ALE.13=ALEPlot(DAT[,2:4], lm.DAT, pred.fun=yhat, J=c(1,3), K=20, NA.plot = TRUE)
ALE.23=ALEPlot(DAT[,2:4], lm.DAT, pred.fun=yhat, J=c(2,3), K=20, NA.plot = TRUE)

## The following manually recreates the same plots produced by the above ALEPlot function calls
par(mfrow = c(2,3))
plot(ALE.1$x.values, ALE.1$f.values, type="l", xlab="x1", ylab="ALE main effect for x1")
plot(ALE.2$x.values, ALE.2$f.values, type="l", xlab="x2", ylab="ALE main effect for x2")
plot(ALE.3$x.values, ALE.3$f.values, type="l", xlab="x3", ylab="ALE main effect for x3")
image(ALE.12$x.values[[1]], ALE.12$x.values[[2]], ALE.12$f.values, xlab = "x1", ylab = "x2")
contour(ALE.12$x.values[[1]], ALE.12$x.values[[2]], ALE.12$f.values, add=TRUE, drawlabels=TRUE)
image(ALE.13$x.values[[1]], ALE.13$x.values[[2]], ALE.13$f.values, xlab = "x1", ylab = "x3")
contour(ALE.13$x.values[[1]], ALE.13$x.values[[2]], ALE.13$f.values, add=TRUE, drawlabels=TRUE)
image(ALE.23$x.values[[1]], ALE.23$x.values[[2]], ALE.23$f.values, xlab = "x2", ylab = "x3")
contour(ALE.23$x.values[[1]], ALE.23$x.values[[2]], ALE.23$f.values, add=TRUE, drawlabels=TRUE)


       
########################################################################
## A larger example in which the supervised learning model is a neural network (\code{nnet})
########################################################################

## Generate some data and fit a \code{nnet} supervised learning model

library(nnet)
N=5000
x1 &lt;- runif(N, min=0, max=1)
x2 &lt;- runif(N, min=0, max=1)
x3 &lt;- runif(N, min=0, max=1)
y = x1 + 2*x2^2 +(x1-0.5)*(x3-0.5) + rnorm(N, 0, 0.1)
DAT = data.frame(y, x1, x2, x3)
nnet.DAT&lt;-nnet(y~., data=DAT, linout=TRUE, skip=FALSE, size=10, decay=0.01, 
	maxit=1000, trace=FALSE)

## Define the predictive function
yhat &lt;- function(X.model, newdata) as.numeric(predict(X.model, newdata, type="raw"))

## Calculate and plot the ALE main and second-order interaction effects of x1, x2, x3
par(mfrow = c(2,3))
ALE.1=ALEPlot(DAT[,2:4], nnet.DAT, pred.fun=yhat, J=1, K=50, NA.plot = TRUE)
ALE.2=ALEPlot(DAT[,2:4], nnet.DAT, pred.fun=yhat, J=2, K=50, NA.plot = TRUE)
ALE.3=ALEPlot(DAT[,2:4], nnet.DAT, pred.fun=yhat, J=3, K=50, NA.plot = TRUE)
ALE.12=ALEPlot(DAT[,2:4], nnet.DAT, pred.fun=yhat, J=c(1,2), K=20, NA.plot = TRUE)
ALE.13=ALEPlot(DAT[,2:4], nnet.DAT, pred.fun=yhat, J=c(1,3), K=20, NA.plot = TRUE)
ALE.23=ALEPlot(DAT[,2:4], nnet.DAT, pred.fun=yhat, J=c(2,3), K=20, NA.plot = TRUE)

########################################################################
## A binary classification example in which the supervised learning model is 
## a neural network (\code{nnet}), and the log-odds of the predicted class probability 
## is the function to be plotted
########################################################################

## Generate some data and fit a \code{nnet} supervised learning model

library(nnet)
N=5000
x1 &lt;- runif(N, min=0, max=1)
x2 &lt;- runif(N, min=0, max=1)
x3 &lt;- runif(N, min=0, max=1)
z = -3.21 + 2.81*x1 + 5.62*x2^2 + 2.81*(x1-0.5)*(x3-0.5) #true log-odds
p = exp(z)/(1+exp(z))
u = runif(N)
y = u &lt; p
DAT = data.frame(y, x1, x2, x3)
nnet.DAT&lt;-nnet(y~., data=DAT, linout=FALSE, skip=FALSE, size=10, decay=0.05, 
	maxit=1000, trace=FALSE)

## Define the ALE function to be the log-odds of the predicted probability that y = TRUE
yhat &lt;- function(X.model, newdata) {
    p.hat = as.numeric(predict(X.model, newdata, type="raw"))
    log(p.hat/(1-p.hat))
}

## Calculate and plot the ALE main and second-order interaction effects of x1, x2, x3
par(mfrow = c(2,3))
ALE.1=ALEPlot(DAT[,2:4], nnet.DAT, pred.fun=yhat, J=1, K=50, NA.plot = TRUE)
ALE.2=ALEPlot(DAT[,2:4], nnet.DAT, pred.fun=yhat, J=2, K=50, NA.plot = TRUE)
ALE.3=ALEPlot(DAT[,2:4], nnet.DAT, pred.fun=yhat, J=3, K=50, NA.plot = TRUE)
ALE.12=ALEPlot(DAT[,2:4], nnet.DAT, pred.fun=yhat, J=c(1,2), K=20, NA.plot = TRUE)
ALE.13=ALEPlot(DAT[,2:4], nnet.DAT, pred.fun=yhat, J=c(1,3), K=20, NA.plot = TRUE)
ALE.23=ALEPlot(DAT[,2:4], nnet.DAT, pred.fun=yhat, J=c(2,3), K=20, NA.plot = TRUE)


</code></pre>

<hr>
<h2 id='PDPlot'>
Partial Dependence (PD) Plots
</h2><span id='topic+PDPlot'></span>

<h3>Description</h3>

<p>Computes and plots partial dependence (PD) plots for a fitted supervised learning model. The effects can be either a main effect for an individual predictor (<code>length(J) = 1</code>) or a second-order interaction effect for a pair of predictors (<code>length(J) = 2</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PDPlot(X, X.model, pred.fun, J, K)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="PDPlot_+3A_x">X</code></td>
<td>
<p>The data frame of predictor variables to which the supervised learning model was fit. The names of the predictor variables must be the same as when the model was fit. The response variable should not be included in <code>X</code>.</p>
</td></tr>
<tr><td><code id="PDPlot_+3A_x.model">X.model</code></td>
<td>
<p>The fitted supervised learning model object (e.g., a tree, random forest, neural network, etc.), typically an object to which a built-in <code>predict</code> command associated with that object can be applied.</p>
</td></tr>
<tr><td><code id="PDPlot_+3A_pred.fun">pred.fun</code></td>
<td>
<p>A user-supplied function that will be used to predict the response for <code>X.model</code> for some specified inputs. <code>pred.fun</code> has two arguments. The first argument is named <code>X.model</code> and must be the same object as the <code>X.model</code> argument to the <code>ALEPlot</code> function. The second argument is named <code>newdata</code> and is a data frame of predictor values at which the object <code>X.model</code> is to be predicted. The output of <code>pred.fun</code> must be a numeric vector of predictions having length equal to the number of rows of <code>newdata</code>. For most <code>X.model</code> objects, <code>pred.fun</code> can simply call the <code>predict</code> function that was written as part of that modeling object package, assuming the package contains a <code>predict</code> function. An example of where a more customized <code>pred.fun</code> would be used is a multi (&gt; 2) class classification problem for which the built-in predict function returns a vector of predicted probabilities, one for each response class. In this case it may make sense to have <code>pred.fun</code> return the predicted probabilities (or its log-odds, etc.) for one particular class of interest.</p>
</td></tr>
<tr><td><code id="PDPlot_+3A_j">J</code></td>
<td>
<p>A numeric scalar or two-length vector of indices of the predictors for which the PD plot will be calculated. <code>J</code> is either a single index (for a main effects plot) or a pair of indices (for a second-order interaction plot). For a single index, the corresponding predictor must be either numeric or a factor. For a pair of indices, the corresponding predictors must be either both numeric or the first a factor and the second numeric.</p>
</td></tr>
<tr><td><code id="PDPlot_+3A_k">K</code></td>
<td>
<p>A numeric scalar that represents the number of discrete points at which the PD plot will be calculated. If <code>length(J) = 2</code>, the same <code>K</code> will be used for both predictors, resulting in a two-dimensional grid of <code>K^2</code> predictor values at with the PD plot will be calculated. <code>K</code> is only used if the predictor is numeric. For factor predictors, the equivalent of <code>K</code> is the number of used levels of the factor, which is automatically determined internally.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates and plots the partial dependence (PD) plots first introduced in Friedman (2001). See the Apley (2016) reference paper listed below for details. For <code>J = j</code> (i.e., if the index for a single predictor <code class="reqn">x_j</code> is specified), the function calculates and returns the PD main effect of <code class="reqn">x_j</code>, which is denoted by <code class="reqn">f_{j,PD}(x_j)</code> in Apley (2016). It also plots <code class="reqn">f_{j,PD}(x_j)</code>. For <code>J = c(j1,j2)</code> (i.e., if the indices for a pair of predictors <code class="reqn">(x_{j1},x_{j2})</code> are specified), the function calculates and returns the PD second-order interaction effect of <code class="reqn">(x_{j1},x_{j2})</code>, which is denoted by <code class="reqn">f_{{j1,j2},PD}(x_{j1},x_{j2})</code> in Apley (2016). It also plots <code class="reqn">f_{{j1,j2},PD}(x_{j1},x_{j2})</code>. 
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>f.values</code></td>
<td>
<p>If <code>length(J) = 1</code>, a vector of PD plot function values at the predictor values in <code>x.values</code>. If <code>length(J) = 2</code>, <code>f.values</code> is a <code>K1xK</code> matrix of the PD plot function values at the grid of predictor values defined by the <code>X[,J[1]]</code> and <code>X[,J[2]]</code> values in <code>x.values</code>. For <code>X[,J[1]]</code> numeric, <code>K1 = K</code>. For <code>X[,J[2]]</code> a factor, <code>K1</code> is the number of used levels (empty levels are dropped). The rows of <code>f.values</code> correspond to <code>X[,J(1)]</code>, and the columns to <code>X[,J(2)]</code>.</p>
</td></tr>
<tr><td><code>x.values</code></td>
<td>
<p>For numeric predictors, if <code>length(J) = 1</code>, a <code>K</code>-length vector specifying the ordered predictor values at which the PD plot function is calculated. If <code>length(J) = 2</code>, a list of two such vectors, the first containing the <code>X[,J(1)]</code> values and the second containing the <code>X[,J(2)]</code> values at which the PD plot function is calculated. <code>x.values</code> is the same for factor predictors, except it is a <code>K1</code>-length character vector of the levels of the predictor, where <code>K1</code> is determined internally as the number of unique levels of the predictor (empty levels are dropped).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Dan Apley
</p>


<h3>References</h3>

<p>Friedman, J. H., (2001), &quot;Greedy function approximation: A gradient boosting machine,&quot; Annals of Statistics, 29(5), pp. 1189-1232.
</p>
<p>Apley, D. W. (2016), &quot;Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models,&quot; submitted for publication.
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+ALEPlot">ALEPlot</a></code> for partial dependence plots.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>########################################################################
## A transparent example in which the supervised learning model is a linear regression \code{lm},
## but we will pretend it is black-box
########################################################################

## Generate some data and fit a \code{lm} supervised learning model
N=500
x1 &lt;- runif(N, min=0, max=1)
x2 &lt;- runif(N, min=0, max=1)
x3 &lt;- runif(N, min=0, max=1)
y = x1 + 2*x2^2 + rnorm(N, 0, 0.1)
DAT = data.frame(y, x1, x2, x3)
lm.DAT = lm(y ~ .^2 + I(x1^2) + I(x2^2) + I(x3^2), DAT)

## Define the predictive function (easy in this case, since \code{lm} has 
## a built-in predict function that suffices)
yhat &lt;- function(X.model, newdata) as.numeric(predict(X.model, newdata))

## Calculate and plot the PD main effects and second-order interaction effects of x1, x2, x3
par(mfrow = c(2,3))
PD.1=PDPlot(DAT[,2:4], lm.DAT, pred.fun=yhat, J=1, K=50)
PD.2=PDPlot(DAT[,2:4], lm.DAT, pred.fun=yhat, J=2, K=50)
PD.3=PDPlot(DAT[,2:4], lm.DAT, pred.fun=yhat, J=3, K=50)
PD.12=PDPlot(DAT[,2:4], lm.DAT, pred.fun=yhat, J=c(1,2), K=30)
PD.13=PDPlot(DAT[,2:4], lm.DAT, pred.fun=yhat, J=c(1,3), K=30)
PD.23=PDPlot(DAT[,2:4], lm.DAT, pred.fun=yhat, J=c(2,3), K=30)

## The following manually recreates the same plots produced by the above PDPlot function calls
par(mfrow = c(2,3))
plot(PD.1$x.values, PD.1$f.values, type="l", xlab="x1", ylab="PD main effect for x1")
plot(PD.2$x.values, PD.2$f.values, type="l", xlab="x2", ylab="PD main effect for x2")
plot(PD.3$x.values, PD.3$f.values, type="l", xlab="x3", ylab="PD main effect for x3")
image(PD.12$x.values[[1]], PD.12$x.values[[2]], PD.12$f.values, xlab = "x1", ylab = "x2")
contour(PD.12$x.values[[1]], PD.12$x.values[[2]], PD.12$f.values, add=TRUE, drawlabels=TRUE)
image(PD.13$x.values[[1]], PD.13$x.values[[2]], PD.13$f.values, xlab = "x1", ylab = "x3")
contour(PD.13$x.values[[1]], PD.13$x.values[[2]], PD.13$f.values, add=TRUE, drawlabels=TRUE)
image(PD.23$x.values[[1]], PD.23$x.values[[2]], PD.23$f.values, xlab = "x2", ylab = "x3")
contour(PD.23$x.values[[1]], PD.23$x.values[[2]], PD.23$f.values, add=TRUE, drawlabels=TRUE)


########################################################################
## A larger example in which the supervised learning model is a neural network (\code{nnet})
########################################################################

## Generate some data and fit a \code{nnet} supervised learning model

library(nnet)
N=5000
x1 &lt;- runif(N, min=0, max=1)
x2 &lt;- runif(N, min=0, max=1)
x3 &lt;- runif(N, min=0, max=1)
y = x1 + 2*x2^2 +(x1-0.5)*(x3-0.5) + rnorm(N, 0, 0.1)
DAT = data.frame(y, x1, x2, x3)
nnet.DAT&lt;-nnet(y~., data=DAT, linout=TRUE, skip=FALSE, size=10, decay=0.01, 
	maxit=1000, trace=FALSE)

## Define the predictive function
yhat &lt;- function(X.model, newdata) as.numeric(predict(X.model, newdata, type="raw"))

## Calculate and plot the PD main and second-order interaction effects of x1, x2, x3
par(mfrow = c(2,3))
PD.1=PDPlot(DAT[,2:4], nnet.DAT, pred.fun=yhat, J=1, K=50)
PD.2=PDPlot(DAT[,2:4], nnet.DAT, pred.fun=yhat, J=2, K=50)
PD.3=PDPlot(DAT[,2:4], nnet.DAT, pred.fun=yhat, J=3, K=50)
PD.12=PDPlot(DAT[,2:4], nnet.DAT, pred.fun=yhat, J=c(1,2), K=20)
PD.13=PDPlot(DAT[,2:4], nnet.DAT, pred.fun=yhat, J=c(1,3), K=20)
PD.23=PDPlot(DAT[,2:4], nnet.DAT, pred.fun=yhat, J=c(2,3), K=20)
       
########################################################################
## A binary classification example in which the supervised learning model is 
## a neural network (\code{nnet}), and the log-odds of the predicted class 
## probability is the function to be plotted
########################################################################

## Generate some data and fit a \code{nnet} supervised learning model

library(nnet)
N=5000
x1 &lt;- runif(N, min=0, max=1)
x2 &lt;- runif(N, min=0, max=1)
x3 &lt;- runif(N, min=0, max=1)
z = -3.21 + 2.81*x1 + 5.62*x2^2 + 2.81*(x1-0.5)*(x3-0.5) #true log-odds
p = exp(z)/(1+exp(z))
u = runif(N)
y = u &lt; p
DAT = data.frame(y, x1, x2, x3)
nnet.DAT&lt;-nnet(y~., data=DAT, linout=FALSE, skip=FALSE, size=10, decay=0.05, 
	maxit=1000, trace=FALSE)

## Define the ALE function to be the log-odds of the predicted probability that y = TRUE
yhat &lt;- function(X.model, newdata) {
    p.hat = as.numeric(predict(X.model, newdata, type="raw"))
    log(p.hat/(1-p.hat))
}

## Calculate and plot the PD main and second-order interaction effects of x1, x2, x3
par(mfrow = c(2,3))
PD.1=PDPlot(DAT[,2:4], nnet.DAT, pred.fun=yhat, J=1, K=50)
PD.2=PDPlot(DAT[,2:4], nnet.DAT, pred.fun=yhat, J=2, K=50)
PD.3=PDPlot(DAT[,2:4], nnet.DAT, pred.fun=yhat, J=3, K=50)
PD.12=PDPlot(DAT[,2:4], nnet.DAT, pred.fun=yhat, J=c(1,2), K=20)
PD.13=PDPlot(DAT[,2:4], nnet.DAT, pred.fun=yhat, J=c(1,3), K=20)
PD.23=PDPlot(DAT[,2:4], nnet.DAT, pred.fun=yhat, J=c(2,3), K=20)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
