<!DOCTYPE html><html><head><title>Help for package Rcrawler</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {Rcrawler}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#browser_path'><p>Return browser (webdriver) location path</p></a></li>
<li><a href='#ContentScraper'><p>ContentScraper</p></a></li>
<li><a href='#Drv_fetchpage'><p>Fetch page using web driver/Session</p></a></li>
<li><a href='#Getencoding'><p>Getencoding</p></a></li>
<li><a href='#install_browser'><p>Install PhantomJS webdriver</p></a></li>
<li><a href='#LinkExtractor'><p>LinkExtractor</p></a></li>
<li><a href='#LinkNormalization'><p>Link Normalization</p></a></li>
<li><a href='#Linkparameters'><p>Get the list of parameters and values from an URL</p></a></li>
<li><a href='#Linkparamsfilter'><p>Link parameters filter</p></a></li>
<li><a href='#ListProjects'><p>ListProjects</p></a></li>
<li><a href='#LoadHTMLFiles'><p>LoadHTMLFiles</p>
@rdname LoadHTMLFiles</a></li>
<li><a href='#LoginSession'><p>Open a logged in Session</p></a></li>
<li><a href='#Rcrawler'><p>Rcrawler</p></a></li>
<li><a href='#RobotParser'><p>RobotParser fetch and parse robots.txt</p></a></li>
<li><a href='#run_browser'><p>Start up web driver process on localhost, with a random port</p></a></li>
<li><a href='#stop_browser'><p>Stop web driver process and Remove its Object</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Web Crawler and Scraper</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.9-1</td>
</tr>
<tr>
<td>Date:</td>
<td>2018-11-11</td>
</tr>
<tr>
<td>Description:</td>
<td>Performs parallel web crawling and web scraping. It is designed to crawl, parse and store web pages to produce data that can be directly used for analysis application. For details see Khalil and Fakir (2017) &lt;<a href="https://doi.org/10.1016%2Fj.softx.2017.04.004">doi:10.1016/j.softx.2017.04.004</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/salimk/Rcrawler/">https://github.com/salimk/Rcrawler/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/salimk/Rcrawler/issues">https://github.com/salimk/Rcrawler/issues</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>Imports:</td>
<td>httr, xml2, data.table, foreach, doParallel, parallel,
selectr, webdriver, callr, jsonlite</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.0</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2018-11-11 22:34:30 UTC; Salim</td>
</tr>
<tr>
<td>Author:</td>
<td>Salim Khalil <a href="https://orcid.org/0000-0002-7804-4041"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Salim Khalil &lt;khalilsalim1@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2018-11-11 22:00:16 UTC</td>
</tr>
</table>
<hr>
<h2 id='browser_path'>Return browser (webdriver) location path</h2><span id='topic+browser_path'></span>

<h3>Description</h3>

<p>After installing webdriver using <code><a href="#topic+install_browser">install_browser</a></code>,
you can check its location path by running this function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>browser_path()
</code></pre>


<h3>Value</h3>

<p>path as character
</p>


<h3>Author(s)</h3>

<p>salim khalil
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

browser_paths()

## End(Not run)
</code></pre>

<hr>
<h2 id='ContentScraper'>ContentScraper</h2><span id='topic+ContentScraper'></span>

<h3>Description</h3>

<p>ContentScraper
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ContentScraper(Url, HTmlText, browser, XpathPatterns, CssPatterns,
  PatternsName, ExcludeXpathPat, ExcludeCSSPat, ManyPerPattern = FALSE,
  astext = TRUE, asDataFrame = FALSE, encod)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ContentScraper_+3A_url">Url</code></td>
<td>
<p>character, one url or a vector of urls of web pages to scrape.</p>
</td></tr>
<tr><td><code id="ContentScraper_+3A_htmltext">HTmlText</code></td>
<td>
<p>character, web page as HTML text to be scraped.use either Url or HtmlText not both.</p>
</td></tr>
<tr><td><code id="ContentScraper_+3A_browser">browser</code></td>
<td>
<p>a web driver session, or a loggedin session of the web driver (see examples)</p>
</td></tr>
<tr><td><code id="ContentScraper_+3A_xpathpatterns">XpathPatterns</code></td>
<td>
<p>character vector, one or more XPath patterns to extract from the web page.</p>
</td></tr>
<tr><td><code id="ContentScraper_+3A_csspatterns">CssPatterns</code></td>
<td>
<p>character vector, one or more CSS selector patterns to extract from the web page.</p>
</td></tr>
<tr><td><code id="ContentScraper_+3A_patternsname">PatternsName</code></td>
<td>
<p>character vector, given names for each xpath pattern to extract, just as an indication .</p>
</td></tr>
<tr><td><code id="ContentScraper_+3A_excludexpathpat">ExcludeXpathPat</code></td>
<td>
<p>character vector, one or more Xpath pattern to exclude from extracted content (like excluding quotes from forum replies or excluding middle ads from Blog post) .</p>
</td></tr>
<tr><td><code id="ContentScraper_+3A_excludecsspat">ExcludeCSSPat</code></td>
<td>
<p>character vector, one or more Css pattern to exclude from extracted content.</p>
</td></tr>
<tr><td><code id="ContentScraper_+3A_manyperpattern">ManyPerPattern</code></td>
<td>
<p>boolean, If False only the first matched element by the pattern is extracted (like in Blogs one page has one article/post and one title). Otherwise if set to True all nodes matching the pattern are extracted (Like in galleries, listing or comments, one page has many elements with the same pattern )</p>
</td></tr>
<tr><td><code id="ContentScraper_+3A_astext">astext</code></td>
<td>
<p>boolean, default is TRUE, HTML and PHP tags is stripped from the extracted piece.</p>
</td></tr>
<tr><td><code id="ContentScraper_+3A_asdataframe">asDataFrame</code></td>
<td>
<p>boolean, transform scraped data into a Dataframe. default is False (data is returned as List)</p>
</td></tr>
<tr><td><code id="ContentScraper_+3A_encod">encod</code></td>
<td>
<p>character, set the weppage character encoding.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>return a named list of scraped content
</p>


<h3>Author(s)</h3>

<p>salim khalil
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

#### Extract title, publishing date and article from the web page using css selectors
#
DATA&lt;-ContentScraper(Url="http://glofile.com/index.php/2017/06/08/taux-nette-detente/",
CssPatterns = c(".entry-title",".published",".entry-content"), astext = TRUE)

#### The web page source can be provided also in HTML text (characters)
#
txthml&lt;-"&lt;html&gt;&lt;title&gt;blah&lt;/title&gt;&lt;div&gt;&lt;p&gt;I m the content&lt;/p&gt;&lt;/div&gt;&lt;/html&gt;"
DATA&lt;-ContentScraper(HTmlText = txthml ,XpathPatterns = "//*/p")

#### Extract post title and bodt from the web page using Xpath patterns,
#  PatternsName can be provided as indication.
#
DATA&lt;-ContentScraper(Url ="http://glofile.com/index.php/2017/06/08/athletisme-m-a-rome/",
XpathPatterns=c("//head/title","//*/article"),PatternsName=c("title", "article"))

#### Extract titles and contents of 3 Urls using CSS selectors, As result DATA variable
# will handle 6 elements.
#
urllist&lt;-c("http://glofile.com/index.php/2017/06/08/sondage-quel-budget/",
"http://glofile.com/index.php/2017/06/08/cyril-hanouna-tire-a-boulets-rouges-sur-le-csa/",
"http://glofile.com/index.php/2017/06/08/placements-quelles-solutions-pour-doper/",
"http://glofile.com/index.php/2017/06/08/paris-un-concentre-de-suspens/")
DATA&lt;-ContentScraper(Url =urllist, CssPatterns = c(".entry-title",".entry-content"),
PatternsName = c("title","content"))

#### Extract post title and list of comments from a set of blog pages,
# ManyPerPattern argument enables extracting many elements having same pattern from each
# page like comments, reviews, quotes and listing.
DATA&lt;-ContentScraper(Url =urllist, CssPatterns = c(".entry-title",".comment-content p"),
PatternsName = c("title","comments"), astext = TRUE, ManyPerPattern = TRUE)
#### From this Forum page  e extract the post title and all replies using CSS selectors
# c("head &gt; title",".post"), However, we know that each reply contain previous Replys
# as quote so we need to exclude To remove inner quotes in each reply we use
# ExcludeCSSPat c(".quote",".quoteheader a")
DATA&lt;-ContentScraper(Url = "https://bitcointalk.org/index.php?topic=2334331.0",
CssPatterns = c("head &gt; title",".post"), ExcludeCSSPat = c(".quote",".quoteheader"),
PatternsName = c("Title","Replys"), ManyPerPattern = TRUE)

#### Scrape data from web page requiring authentification
# replace \@ by @ before running follwing examples
# create a loggedin session
LS&lt;-run_browser()
LS&lt;-LoginSession(Browser = LS, LoginURL = 'https://manager.submittable.com/login',
   LoginCredentials = c('your email','your password'),
   cssLoginFields =c('#email', '#password'),
   XpathLoginButton ='//*[\@type=\"submit\"]' )
#Then scrape data with the session
DATA&lt;-ContentScraper(Url='https://manager.submittable.com/beta/discover/119087',
     XpathPatterns = c('//*[\@id=\"submitter-app\"]/div/div[2]/div/div/div/div/div[3]',
      '//*[\@id=\"submitter-app\"]/div/div[2]/div/div/div/div/div[2]/div[1]/div[1]' ),
      PatternsName = c("Article","Title"), astext = TRUE, browser = LS )
#OR
page&lt;-LinkExtractor(url='https://manager.submittable.com/beta/discover/119087',
                    browser = LS)
DATA&lt;-ContentScraper(HTmlText = page$Info$Source_page,
     XpathPatterns = c("//*[\@id=\"submitter-app\"]/div/div[2]/div/div/div/div/div[3]",
     "//*[\@id=\"submitter-app\"]/div/div[2]/div/div/div/div/div[2]/div[1]/div[1]" ),
      PatternsName = c("Article","Title"),astext = TRUE )

To get all first elements of the lists in one vector (example all titles) :
VecTitle&lt;-unlist(lapply(DATA, `[[`, 1))
To get all second elements of the lists in one vector (example all articles)
VecContent&lt;-unlist(lapply(DATA, `[[`, 2))


## End(Not run)
</code></pre>

<hr>
<h2 id='Drv_fetchpage'>Fetch page using web driver/Session</h2><span id='topic+Drv_fetchpage'></span>

<h3>Description</h3>

<p>Fetch page using web driver/Session
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Drv_fetchpage(url, browser)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Drv_fetchpage_+3A_url">url</code></td>
<td>
<p>character, web page URL to retreive</p>
</td></tr>
<tr><td><code id="Drv_fetchpage_+3A_browser">browser</code></td>
<td>
<p>Object returned by <code><a href="#topic+run_browser">run_browser</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>return a list of three elements, the first is a list containing the web page details (url, encoding-type, content-type, content ... etc), the second is a character-vector containing the list of retreived internal urls and the third is a vetcor of external Urls.
</p>


<h3>Author(s)</h3>

<p>salim khalil
</p>

<hr>
<h2 id='Getencoding'>Getencoding</h2><span id='topic+Getencoding'></span>

<h3>Description</h3>

<p>This function  retreives the encoding charset of web page based on HTML tags and HTTP header
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Getencoding(url)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Getencoding_+3A_url">url</code></td>
<td>
<p>character, the web page url.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>return the encoding charset as character
</p>


<h3>Author(s)</h3>

<p>salim khalil
</p>

<hr>
<h2 id='install_browser'>Install PhantomJS webdriver</h2><span id='topic+install_browser'></span>

<h3>Description</h3>

<p>Download the zip package, unzip it, and copy the executable to a system
directory in which <span class="pkg">webdriver</span> can look for the PhantomJS executable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>install_browser(version = "2.1.1",
  baseURL = "https://github.com/wch/webshot/releases/download/v0.3.1/")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="install_browser_+3A_version">version</code></td>
<td>
<p>The version number of PhantomJS.</p>
</td></tr>
<tr><td><code id="install_browser_+3A_baseurl">baseURL</code></td>
<td>
<p>The base URL for the location of PhantomJS binaries for
download. If the default download site is unavailable, you may specify an
alternative mirror, such as
<code>"https://bitbucket.org/ariya/phantomjs/downloads/"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function was designed primarily to help Windows users since it is
cumbersome to modify the <code>PATH</code> variable. Mac OS X users may install
PhantomJS via Homebrew. If you download the package from the PhantomJS
website instead, please make sure the executable can be found via the
<code>PATH</code> variable.
</p>
<p>On Windows, the directory specified by the environment variable
<code>APPDATA</code> is used to store &lsquo;<span class="file">phantomjs.exe</span>&rsquo;. On OS X, the directory
&lsquo;<span class="file">~/Library/Application Support</span>&rsquo; is used. On other platforms (such as
Linux), the directory &lsquo;<span class="file">~/bin</span>&rsquo; is used. If these directories are not
writable, the directory &lsquo;<span class="file">PhantomJS</span>&rsquo; under the installation directory of
the <span class="pkg">webdriver</span> package will be tried. If this directory still fails, you
will have to install PhantomJS by yourself.
</p>


<h3>Value</h3>

<p><code>NULL</code> (the executable is written to a system directory).
</p>

<hr>
<h2 id='LinkExtractor'>LinkExtractor</h2><span id='topic+LinkExtractor'></span>

<h3>Description</h3>

<p>Fetch and parse a document by URL, to extract page info, HTML source and links (internal/external).
Fetching process can be done by HTTP GET request or through webdriver (phantomjs) which simulate a real browser rendering.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LinkExtractor(url, id, lev, IndexErrPages, Useragent, Timeout = 6,
  use_proxy = NULL, URLlenlimit = 255, urlExtfilter, urlregexfilter,
  encod, urlbotfiler, removeparams, removeAllparams = FALSE,
  ExternalLInks = FALSE, urlsZoneXpath = NULL, Browser,
  RenderingDelay = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LinkExtractor_+3A_url">url</code></td>
<td>
<p>character, url to fetch and parse.</p>
</td></tr>
<tr><td><code id="LinkExtractor_+3A_id">id</code></td>
<td>
<p>numeric, an id to identify a specific web page in a website collection, it's auto-generated byauto-generated by <code><a href="#topic+Rcrawler">Rcrawler</a></code> function.</p>
</td></tr>
<tr><td><code id="LinkExtractor_+3A_lev">lev</code></td>
<td>
<p>numeric, the depth level of the web page, auto-generated by <code><a href="#topic+Rcrawler">Rcrawler</a></code> function.</p>
</td></tr>
<tr><td><code id="LinkExtractor_+3A_indexerrpages">IndexErrPages</code></td>
<td>
<p>character vector, http error code-statut that can be processed, by default, it's <code>IndexErrPages&lt;-c(200)</code> which means only successfull page request should be parsed .Eg, To parse also 404 error pages add, <code>IndexErrPages&lt;-c(200,404)</code>.</p>
</td></tr>
<tr><td><code id="LinkExtractor_+3A_useragent">Useragent</code></td>
<td>
<p>, the name the request sender, default to &quot;Rcrawler&quot;. but we recommand using a regular browser user-agent to avoid being blocked by some server.</p>
</td></tr>
<tr><td><code id="LinkExtractor_+3A_timeout">Timeout</code></td>
<td>
<p>,default to 5s</p>
</td></tr>
<tr><td><code id="LinkExtractor_+3A_use_proxy">use_proxy</code></td>
<td>
<p>object created by httr::use_proxy() function, if you want to use a proxy to retreive web page. (does not work with webdriver).</p>
</td></tr>
<tr><td><code id="LinkExtractor_+3A_urllenlimit">URLlenlimit</code></td>
<td>
<p>interger, Maximum URL length to process, default to 255 characters (Useful to avoid spider traps)</p>
</td></tr>
<tr><td><code id="LinkExtractor_+3A_urlextfilter">urlExtfilter</code></td>
<td>
<p>character vector, the list of file extensions to exclude from parsing, Actualy, only html pages are processed(parsed, scraped); To define your own lis use <code>urlExtfilter&lt;-c(ext1,ext2,ext3)</code></p>
</td></tr>
<tr><td><code id="LinkExtractor_+3A_urlregexfilter">urlregexfilter</code></td>
<td>
<p>character vector, filter out extracted internal urls by one or more regular expression.</p>
</td></tr>
<tr><td><code id="LinkExtractor_+3A_encod">encod</code></td>
<td>
<p>character, web page character encoding</p>
</td></tr>
<tr><td><code id="LinkExtractor_+3A_urlbotfiler">urlbotfiler</code></td>
<td>
<p>character vector , directories/files restricted by robot.txt</p>
</td></tr>
<tr><td><code id="LinkExtractor_+3A_removeparams">removeparams</code></td>
<td>
<p>character vector, list of url parameters to be removed form web page internal links.</p>
</td></tr>
<tr><td><code id="LinkExtractor_+3A_removeallparams">removeAllparams</code></td>
<td>
<p>boolean, IF TRUE the list of scraped urls will have no parameters.</p>
</td></tr>
<tr><td><code id="LinkExtractor_+3A_externallinks">ExternalLInks</code></td>
<td>
<p>boolean, default FALSE, if set to TRUE external links also are returned.</p>
</td></tr>
<tr><td><code id="LinkExtractor_+3A_urlszonexpath">urlsZoneXpath</code></td>
<td>
<p>xpath pattern of the section from where links should be exclusively gathered/collected.</p>
</td></tr>
<tr><td><code id="LinkExtractor_+3A_browser">Browser</code></td>
<td>
<p>the client object of a remote headless web driver(virtual browser), created by <code>br&lt;-run_browser()</code> function, or a logged-in browser session object, created by <a href="#topic+LoginSession">LoginSession</a>, after installing web driver Agent <code>install_browser()</code>. see examples below.</p>
</td></tr>
<tr><td><code id="LinkExtractor_+3A_renderingdelay">RenderingDelay</code></td>
<td>
<p>the time required by a webpage to be fully rendred, in seconds.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>return a list of three elements, the first is a list containing the web page details (url, encoding-type, content-type, content ... etc), the second is a character-vector containing the list of retreived internal urls and the third is a vetcor of external Urls.
</p>


<h3>Author(s)</h3>

<p>salim khalil
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

###### Fetch a URL using GET request :
######################################################
##
## Very Fast, but can't fetch javascript rendred pages or sections

# fetch the page with default config, then returns page info and internal links

page&lt;-LinkExtractor(url="http://www.glofile.com")

# this will return  alse external links

page&lt;-LinkExtractor(url="http://www.glofile.com", ExternalLInks = TRUE)

# Specify Useragent to overcome bots blocking by some websites rules

page&lt;-LinkExtractor(url="http://www.glofile.com", ExternalLInks = TRUE,
       Useragent = "Mozilla/5.0 (Windows NT 6.3; Win64; x64)",)

# By default, only HTTP succeeded page are parsed, therefore, to force
# parse error pages like 404 you need to specify IndexErrPages,

page&lt;-LinkExtractor(url="http://www.glofile.com/404notfoundpage",
      ExternalLInks = TRUE, IndexErrPages = c(200,404))


#### Use GET request with a proxy
#
proxy&lt;-httr::use_proxy("190.90.100.205",41000)
pageinfo&lt;-LinkExtractor(url="http://glofile.com/index.php/2017/06/08/taux-nette-detente/",
use_proxy = proxy)

#' Note : use_proxy arguments can' not't be configured with webdriver

###### Fetch a URL using a web driver (virtual browser)
######################################################
##
## Slow, because a headless browser called phantomjs will simulate
## a user session on a website. It's useful for web page having important
## javascript rendred sections such as menus.
## We recommend that you first try normal previous request, if the function
## returns a forbidden 403 status code or an empty/incomplete source code body,
## then try to set a normal useragent like
## Useragent = "Mozilla/5.0 (Windows NT 6.3; Win64; x64)",
## if you still have issue then you shoud try to set up a virtual browser.

#1 Download and install phantomjs headless browser
install_browser()

#2 start browser process (takes 30 seconds usualy)
br &lt;-run_browser()

#3 call the function
page&lt;-LinkExtractor(url="http://www.master-maroc.com", Browser = br,
      ExternalLInks = TRUE)

#4 dont forget to stop the browser at the end of all your work with it
stop_browser(br)

###### Fetch a web page that requires authentication
#########################################################
## In some case you may need to retreive content from a web page which
## requires authentication via a login page like private forums, platforms..
## In this case you need to run \link{LoginSession} function to establish a
## authenticated browser session; then use \link{LinkExtractor} to fetch
## the URL using the auhenticated session.
## In the example below we will try to fech a private blog post which
## require authentification .

If you retreive the page using regular function LinkExtractor or your browser
page&lt;-LinkExtractor("http://glofile.com/index.php/2017/06/08/jcdecaux/")
The post is not visible because it's private.
Now we will try to login to access this post using folowing creditentials
username : demo and password : rc@pass@r

#1 Download and install phantomjs headless browser (skip if installed)
install_browser()

#2 start browser process
br &lt;-run_browser()

#3 create auhenticated session
#  see \link{LoginSession} for more details

 LS&lt;-LoginSession(Browser = br, LoginURL = 'http://glofile.com/wp-login.php',
                LoginCredentials = c('demo','rc@pass@r'),
                cssLoginFields =c('#user_login', '#user_pass'),
                cssLoginButton='#wp-submit' )

#check if login successful
LS$session$getTitle()
#Or
LS$session$getUrl()
#Or
LS$session$takeScreenshot(file = 'sc.png')

#3 Retreive the target private page using the logged-in session
page&lt;-LinkExtractor(url='http://glofile.com/index.php/2017/06/08/jcdecaux/',Browser = LS)

#4 dont forget to stop the browser at the end of all your work with it
stop_browser(LS)


################### Returned Values #####################
#########################################################

# Returned 'page' variable should include :
# 1- list of page details,
# 2- Internal links
# 3- external links.

#1 Vector of extracted internal links  (in-links)
page$InternalLinks

#2 Vector of extracted external links  (out-links)
page$ExternalLinks

page$Info

# Requested Url
page$Info$Url

# Sum of extracted links
page$Info$SumLinks

# The status code of the HTTP response 200, 401, 300...
page$Info$Status_code

# The MIME type of this content from HTTP response
page$Info$Content_type

# Page text encoding UTF8, ISO-8859-1 , ..
page$Info$Encoding

# Page source code
page$Info$Source_page

Page title
page$Info$Title

Other returned values page$Info$Id, page$Info$Crawl_level,
page$Info$Crawl_status are only used by Rcrawler funtion.



## End(Not run)

</code></pre>

<hr>
<h2 id='LinkNormalization'>Link Normalization</h2><span id='topic+LinkNormalization'></span>

<h3>Description</h3>

<p>To normalize and transform URLs into a canonical form.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LinkNormalization(links, current)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LinkNormalization_+3A_links">links</code></td>
<td>
<p>character, one or more URLs to Normalize.</p>
</td></tr>
<tr><td><code id="LinkNormalization_+3A_current">current</code></td>
<td>
<p>character, The current page URL where links are located</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector of normalized urls
</p>


<h3>Author(s)</h3>

<p>salim khalil
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Normalize a set of links

links&lt;-c("http://www.twitter.com/share?url=http://glofile.com/page.html",
         "/finance/banks/page-2017.html",
         "./section/subscription.php",
         "//section/",
         "www.glofile.com/home/",
         "IndexEn.aspx",
         "glofile.com/sport/foot/page.html",
         "sub.glofile.com/index.php",
         "http://glofile.com/page.html#1",
         "?tags%5B%5D=votingrights&amp;amp;sort=popular"
                   )

links&lt;-LinkNormalization(links,"http://glofile.com" )

links


</code></pre>

<hr>
<h2 id='Linkparameters'>Get the list of parameters and values from an URL</h2><span id='topic+Linkparameters'></span>

<h3>Description</h3>

<p>A function that take a URL _charachter_ as input, and extract the parameters and values from this URL .
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Linkparameters(URL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Linkparameters_+3A_url">URL</code></td>
<td>
<p>character, the URL to extract</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function extract the link parameters and values (Up to 10 parameters)
</p>


<h3>Value</h3>

<p>return the URL paremeters=values
</p>


<h3>Author(s)</h3>

<p>salim khalil
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
Linkparameters("http://www.glogile.com/index.php?name=jake&amp;age=23&amp;template=2&amp;filter=true")
# Extract all URL parameters with values as vector

</code></pre>

<hr>
<h2 id='Linkparamsfilter'>Link parameters filter</h2><span id='topic+Linkparamsfilter'></span>

<h3>Description</h3>

<p>This function remove a given set of parameters from a specific URL
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Linkparamsfilter(URL, params, removeAllparams = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Linkparamsfilter_+3A_url">URL</code></td>
<td>
<p>character, the URL from which params and values have to be removed</p>
</td></tr>
<tr><td><code id="Linkparamsfilter_+3A_params">params</code></td>
<td>
<p>character vector, List of url parameters to be removed</p>
</td></tr>
<tr><td><code id="Linkparamsfilter_+3A_removeallparams">removeAllparams</code></td>
<td>
<p>boolean if true , all url parameters will be removed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function exclude given parameters from the urls,
</p>


<h3>Value</h3>

<p>return a URL wihtout given parameters
</p>


<h3>Author(s)</h3>

<p>salim khalil
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#remove ord and tmp parameters from the URL
url&lt;-"http://www.glogile.com/index.php?name=jake&amp;age=23&amp;tmp=2&amp;ord=1"
url&lt;-Linkparamsfilter(url,c("ord","tmp"))
#remove all URL parameters
Linkparamsfilter(url,removeAllparams = TRUE)

</code></pre>

<hr>
<h2 id='ListProjects'>ListProjects</h2><span id='topic+ListProjects'></span>

<h3>Description</h3>

<p>List all crawling project in your R local directory, or in a custom directory
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ListProjects(DIR)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ListProjects_+3A_dir">DIR</code></td>
<td>
<p>character By default it's your local R workspace, if you set a custom folder for your crawling project then user DIR param to access this folder.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ListProjects</code>, a character vector.
</p>


<h3>Author(s)</h3>

<p>salim khalil
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
ListProjects()

## End(Not run)
</code></pre>

<hr>
<h2 id='LoadHTMLFiles'>LoadHTMLFiles
@rdname LoadHTMLFiles</h2><span id='topic+LoadHTMLFiles'></span>

<h3>Description</h3>

<p>LoadHTMLFiles
@rdname LoadHTMLFiles
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LoadHTMLFiles(ProjectName, type = "vector", max)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LoadHTMLFiles_+3A_projectname">ProjectName</code></td>
<td>
<p>character, the name of the folder holding collected HTML files, use <code>ListProjects</code> fnuction to see all projects.</p>
</td></tr>
<tr><td><code id="LoadHTMLFiles_+3A_type">type</code></td>
<td>
<p>character, the type of returned variable, either vector or list.</p>
</td></tr>
<tr><td><code id="LoadHTMLFiles_+3A_max">max</code></td>
<td>
<p>Integer, maximum number of files to load.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>LoadHTMLFiles</code>, a character vector or a list;
</p>


<h3>Author(s)</h3>

<p>salim khalil
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
ListProjects()
#show all crawling project folders stored in your local R wokspace folder
DataHTML&lt;-LoadHTMLFiles("glofile.com-301010")
#Load all HTML files in DataHTML vector
DataHTML2&lt;-LoadHTMLFiles("glofile.com-301010",max = 10, type = "list")
#Load only 10 first HTMl files in DataHTML2 list

## End(Not run)
</code></pre>

<hr>
<h2 id='LoginSession'>Open a logged in Session</h2><span id='topic+LoginSession'></span>

<h3>Description</h3>

<p>Simulate authentifaction using web driver automation
This function Fetch login page using phantomjs web driver(virtual browser), sets login and password values + other required values then clicks on login button.
You should provide these agruments for the function to work correctly :
- Login page URL
- Login Credentials eg: email &amp; password
- css Or Xpath of Login Credential fields
- css or xpath of Login Button
- If a checkbox is required in the login page then provide provide its css or xpath pattern
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LoginSession(Browser, LoginURL, LoginCredentials, cssLoginFields,
  cssLoginButton, cssRadioToCheck, XpathLoginFields, XpathLoginButton,
  XpathRadioToCheck)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LoginSession_+3A_browser">Browser</code></td>
<td>
<p>object, phatomjs web driver use <code><a href="#topic+run_browser">run_browser</a></code> function to create this object.</p>
</td></tr>
<tr><td><code id="LoginSession_+3A_loginurl">LoginURL</code></td>
<td>
<p>character, login page URL</p>
</td></tr>
<tr><td><code id="LoginSession_+3A_logincredentials">LoginCredentials</code></td>
<td>
<p>login Credentials values eg: email and password</p>
</td></tr>
<tr><td><code id="LoginSession_+3A_cssloginfields">cssLoginFields</code></td>
<td>
<p>vector of login fields css pattern.</p>
</td></tr>
<tr><td><code id="LoginSession_+3A_cssloginbutton">cssLoginButton</code></td>
<td>
<p>the css pattern of the login button that should be clicked to access protected zone.</p>
</td></tr>
<tr><td><code id="LoginSession_+3A_cssradiotocheck">cssRadioToCheck</code></td>
<td>
<p>the radio/checkbox css pattern to be checked(if exist)</p>
</td></tr>
<tr><td><code id="LoginSession_+3A_xpathloginfields">XpathLoginFields</code></td>
<td>
<p>vector of login fields xpath pattern.</p>
</td></tr>
<tr><td><code id="LoginSession_+3A_xpathloginbutton">XpathLoginButton</code></td>
<td>
<p>the xpath pattern of the login button.</p>
</td></tr>
<tr><td><code id="LoginSession_+3A_xpathradiotocheck">XpathRadioToCheck</code></td>
<td>
<p>the radio/checkbox xpath pattern to be checked(if exist)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>return authentified browser session object
</p>


<h3>Author(s)</h3>

<p>salim khalil
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 


 #This function is based on web browser automation, so, before start,
 make sure you have successfully installed web driver (phantomjs).
 install_browser()
 # Run browser process and get its reference object
 br&lt;- run_browser()

  brs&lt;-LoginSession(Browser = br, LoginURL = 'http://glofile.com/wp-login.php',
                LoginCredentials = c('demo','rc@pass@r'),
                cssLoginFields =c('#user_login', '#user_pass'),
                cssLoginButton='#wp-submit' )

 # To make sure that you have been successfully authenticated
 # Check URL of the current page after login redirection
 brs$getUrl()
 # Or Take screenshot of the website dashborad
 brs$takeScreenshot(file = "sc.png")


 brs$delete()
 brs$status()
 brs$go(url)
 brs$getUrl()
 brs$goBack()
 brs$goForward()
 brs$refresh()
 brs$getTitle()
 brs$getSource()
 brs$takeScreenshot(file = NULL)
 brs$findElement(css = NULL, linkText = NULL,
              partialLinkText = NULL, xpath = NULL)
 brs$findElements(css = NULL, linkText = NULL,
               partialLinkText = NULL, xpath = NULL)
 brs$executeScript(script, ...)
 brs$executeScriptAsync(script, ...)
 brs$setTimeout(script = NULL, pageLoad = NULL, implicit = NULL)
 brs$moveMouseTo(xoffset = 0, yoffset = 0)
 brs$click(button = c("left", "middle", "right"))
 brs$doubleClick(button = c("left", "middle", "right"))
 brs$mouseButtonDown(button = c("left", "middle", "right"))
 brs$mouseButtonUp(button = c("left", "middle", "right"))
 brs$readLog(type = c("browser", "har"))
 brs$getLogTypes()


## End(Not run)
</code></pre>

<hr>
<h2 id='Rcrawler'>Rcrawler</h2><span id='topic+Rcrawler'></span>

<h3>Description</h3>

<p>The crawler's main function, by providing only the website URL and the Xpath or CSS selector patterns
this function can crawl the whole website (traverse all web pages) download webpages, and scrape/extract
its contents in an automated manner to produce a structured dataset. The process of a crawling
operation is performed by several concurrent processes or nodes in parallel, so it's recommended to
use 64bit version of R.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Rcrawler(Website, no_cores, no_conn, MaxDepth, DIR, RequestsDelay = 0,
  Obeyrobots = FALSE, Useragent, use_proxy = NULL, Encod,
  Timeout = 5, URLlenlimit = 255, urlExtfilter, dataUrlfilter,
  crawlUrlfilter, crawlZoneCSSPat = NULL, crawlZoneXPath = NULL,
  ignoreUrlParams, ignoreAllUrlParams = FALSE, KeywordsFilter,
  KeywordsAccuracy, FUNPageFilter, ExtractXpathPat, ExtractCSSPat,
  PatternsNames, ExcludeXpathPat, ExcludeCSSPat, ExtractAsText = TRUE,
  ManyPerPattern = FALSE, saveOnDisk = TRUE, NetworkData = FALSE,
  NetwExtLinks = FALSE, statslinks = FALSE, Vbrowser = FALSE,
  LoggedSession)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Rcrawler_+3A_website">Website</code></td>
<td>
<p>character, the root URL of the website to crawl and scrape.</p>
</td></tr>
<tr><td><code id="Rcrawler_+3A_no_cores">no_cores</code></td>
<td>
<p>integer, specify the number of clusters (logical cpu) for parallel crawling, by default it's the numbers of available cores.</p>
</td></tr>
<tr><td><code id="Rcrawler_+3A_no_conn">no_conn</code></td>
<td>
<p>integer, it's the number of concurrent connections per one core, by default it takes the same value of no_cores.</p>
</td></tr>
<tr><td><code id="Rcrawler_+3A_maxdepth">MaxDepth</code></td>
<td>
<p>integer, repsents the max deph level for the crawler, this is not the file depth in a directory structure, but 1+ number of links between this document and root document, default to 10.</p>
</td></tr>
<tr><td><code id="Rcrawler_+3A_dir">DIR</code></td>
<td>
<p>character, correspond to the path of the local repository where all crawled data will be stored ex, &quot;C:/collection&quot; , by default R working directory.</p>
</td></tr>
<tr><td><code id="Rcrawler_+3A_requestsdelay">RequestsDelay</code></td>
<td>
<p>integer, The time interval between each round of parallel http requests, in seconds used to avoid overload the website server. default to 0.</p>
</td></tr>
<tr><td><code id="Rcrawler_+3A_obeyrobots">Obeyrobots</code></td>
<td>
<p>boolean, if TRUE, the crawler will parse the website\'s robots.txt file and obey its rules allowed and disallowed directories.</p>
</td></tr>
<tr><td><code id="Rcrawler_+3A_useragent">Useragent</code></td>
<td>
<p>character, the User-Agent HTTP header that is supplied with any HTTP requests made by this function.it is important to simulate different browser's user-agent to continue crawling without getting banned.</p>
</td></tr>
<tr><td><code id="Rcrawler_+3A_use_proxy">use_proxy</code></td>
<td>
<p>object created by httr::use_proxy() function, if you want to use a proxy (does not work with webdriver).</p>
</td></tr>
<tr><td><code id="Rcrawler_+3A_encod">Encod</code></td>
<td>
<p>character, set the website caharacter encoding, by default the crawler will automatically detect the website defined character encoding.</p>
</td></tr>
<tr><td><code id="Rcrawler_+3A_timeout">Timeout</code></td>
<td>
<p>integer, the maximum request time, the number of seconds to wait for a response until giving up, in order to prevent wasting time waiting for responses from slow servers or huge pages, default to 5 sec.</p>
</td></tr>
<tr><td><code id="Rcrawler_+3A_urllenlimit">URLlenlimit</code></td>
<td>
<p>integer, the maximum URL length limit to crawl, to avoid spider traps; default to 255.</p>
</td></tr>
<tr><td><code id="Rcrawler_+3A_urlextfilter">urlExtfilter</code></td>
<td>
<p>character's vector, by default the crawler avoid irrelevant files for data scraping such us xml,js,css,pdf,zip ...etc, it's not recommanded to change the default value until you can provide all the list of filetypes to be escaped.</p>
</td></tr>
<tr><td><code id="Rcrawler_+3A_dataurlfilter">dataUrlfilter</code></td>
<td>
<p>character's vector, filter Urls to be scraped/collected by one or more regular expression patterns.Useful to control which pages should be collected/scraped, like product, post, detail or category pages if they have a commun URL pattern. without start ^ and end $ regex.</p>
</td></tr>
<tr><td><code id="Rcrawler_+3A_crawlurlfilter">crawlUrlfilter</code></td>
<td>
<p>character's vector, filter Urls to be crawled by one or more regular expression patterns. Useful for large websites to control the crawler behaviour and which URLs should be crawled. For example, In case you want to crawl a website's search resutls (guided/oriented crawling). without start ^ and end $ regex.</p>
</td></tr>
<tr><td><code id="Rcrawler_+3A_crawlzonecsspat">crawlZoneCSSPat</code></td>
<td>
<p>one or more css pattern of page sections from where the crawler should gather links to be followed, to avoid navigating through all visible links and to have more control over the crawler behaviour in target website.</p>
</td></tr>
<tr><td><code id="Rcrawler_+3A_crawlzonexpath">crawlZoneXPath</code></td>
<td>
<p>one or more xpath pattern of page sections from where the crawler should gather links to be followed.</p>
</td></tr>
<tr><td><code id="Rcrawler_+3A_ignoreurlparams">ignoreUrlParams</code></td>
<td>
<p>character's vector, the list of Url paremeter to be ignored during crawling. Some URL parameters are ony related to template view if not ignored will cause duplicate page (many web pages having the same content but have different URLs) .</p>
</td></tr>
<tr><td><code id="Rcrawler_+3A_ignoreallurlparams">ignoreAllUrlParams</code></td>
<td>
<p>boolean, choose to ignore all Url parameter after &quot;?&quot; (Not recommended for Non-SEF CMS websites because only the index.php will be crawled)</p>
</td></tr>
<tr><td><code id="Rcrawler_+3A_keywordsfilter">KeywordsFilter</code></td>
<td>
<p>character vector,  For users who desires to scrape or collect only web pages that contains some keywords one or more. Rcrawler calculate an accuracy score based of the number of founded keywords. This parameter must be a vector with at least one keyword like c(&quot;mykeyword&quot;).</p>
</td></tr>
<tr><td><code id="Rcrawler_+3A_keywordsaccuracy">KeywordsAccuracy</code></td>
<td>
<p>integer value range bewteen 0 and 100, used only with KeywordsFilter parameter to determine the accuracy of web pages to collect. The web page Accuracy value is calculated using the number of matched keywords and their occurence.</p>
</td></tr>
<tr><td><code id="Rcrawler_+3A_funpagefilter">FUNPageFilter</code></td>
<td>
<p>function, filter out pages to be collected/scraped by a custom function (conditions, prediction, calssification model). This function should take a <a href="#topic+LinkExtractor">LinkExtractor</a> object as arument then finally returns TRUE or FALSE.</p>
</td></tr>
<tr><td><code id="Rcrawler_+3A_extractxpathpat">ExtractXpathPat</code></td>
<td>
<p>character's vector, vector of xpath patterns to match for data extraction process.</p>
</td></tr>
<tr><td><code id="Rcrawler_+3A_extractcsspat">ExtractCSSPat</code></td>
<td>
<p>character's vector, vector of CSS selector pattern to match for data extraction process.</p>
</td></tr>
<tr><td><code id="Rcrawler_+3A_patternsnames">PatternsNames</code></td>
<td>
<p>character vector, given names for each xpath pattern to extract.</p>
</td></tr>
<tr><td><code id="Rcrawler_+3A_excludexpathpat">ExcludeXpathPat</code></td>
<td>
<p>character's vector, one or more Xpath pattern to exclude from extracted content ExtractCSSPat or ExtractXpathPat (like excluding quotes from forum replies or excluding middle ads from Blog post) .</p>
</td></tr>
<tr><td><code id="Rcrawler_+3A_excludecsspat">ExcludeCSSPat</code></td>
<td>
<p>character's vector, similar to ExcludeXpathPat but using Css selectors.</p>
</td></tr>
<tr><td><code id="Rcrawler_+3A_extractastext">ExtractAsText</code></td>
<td>
<p>boolean, default is TRUE, HTML and PHP tags is stripped from the extracted piece.</p>
</td></tr>
<tr><td><code id="Rcrawler_+3A_manyperpattern">ManyPerPattern</code></td>
<td>
<p>boolean, ManyPerPattern boolean, If False only the first matched element by the pattern is extracted (like in Blogs one page has one article/post and one title). Otherwise if set to True  all nodes matching the pattern are extracted (Like in galleries, listing or comments, one page has many elements with the same pattern )</p>
</td></tr>
<tr><td><code id="Rcrawler_+3A_saveondisk">saveOnDisk</code></td>
<td>
<p>boolean, By default is true, the crawler will store crawled Html pages and extracted data CSV file on a specific folder. On the other hand you may wish to have DATA only in memory.</p>
</td></tr>
<tr><td><code id="Rcrawler_+3A_networkdata">NetworkData</code></td>
<td>
<p>boolean, If set to TRUE, then the crawler map all the internal hyperlink connections within the given website and return DATA for Network construction using igraph or other tools.(two global variables is returned see details)</p>
</td></tr>
<tr><td><code id="Rcrawler_+3A_netwextlinks">NetwExtLinks</code></td>
<td>
<p>boolean, If TRUE external hyperlinks (outlinks) also will be counted on Network edges and nodes.</p>
</td></tr>
<tr><td><code id="Rcrawler_+3A_statslinks">statslinks</code></td>
<td>
<p>boolean, if TRUE, the crawler counts the number of input and output links of each crawled web page.</p>
</td></tr>
<tr><td><code id="Rcrawler_+3A_vbrowser">Vbrowser</code></td>
<td>
<p>boolean, If TRUE the crawler will use web driver phantomsjs (virtual browser) to fetch and parse web pages instead of GET request</p>
</td></tr>
<tr><td><code id="Rcrawler_+3A_loggedsession">LoggedSession</code></td>
<td>
<p>A loggedin browser session object, created by <a href="#topic+LoginSession">LoginSession</a> function</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To start Rcrawler task you need to provide the root URL of the website you want to scrape, it could be a domain, a subdomain or a website section (eg. http://www.domain.com, http://sub.domain.com or http://www.domain.com/section/).
The crawler then will retreive the web page and go through all its internal links. The crawler continue to follow and parse all website's links automatically on the site until all website's pages have been parsed.
</p>
<p>The process of a crawling is performed by several concurrent processes or nodes in parallel, So, It is recommended to use R 64-bit version.
</p>
<p>For more tutorials check https://github.com/salimk/Rcrawler/
</p>
<p>To scrape content with complex character such as Arabic or Chinese, you need to run Sys.setlocale function then set the appropriate encoding in Rcrawler function.
</p>
<p>If you want to learn more about web scraper/crawler architecture, functional properties and implementation using R language, Follow this link and download the published paper for free .
</p>
<p>Link: http://www.sciencedirect.com/science/article/pii/S2352711017300110
</p>
<p>Dont forget to cite Rcrawler paper:
</p>
<p>Khalil, S., &amp; Fakir, M. (2017). RCrawler: An R package for parallel web crawling and scraping. SoftwareX, 6, 98-106.
</p>


<h3>Value</h3>

<p>The crawling and scraping process may take a long time to finish, therefore, to avoid data loss in the case that a function crashes or stopped in the middle of action, some important data are exported at every iteration to R global environement:
</p>
<p>- INDEX: A data frame in global environement representing the generic URL index,including the list of fetched URLs and page details
(contenttype,HTTP state, number of out-links and in-links, encoding type, and level).
</p>
<p>- A repository in workspace that contains all downloaded pages (.html files)
</p>
<p>Data scraping is enabled by setting ExtractXpathPat or ExtractCSSPat parameter:
</p>
<p>- DATA: A list of lists in global environement holding scraped contents.
</p>
<p>- A csv file 'extracted_contents.csv' holding all extracted data.
</p>
<p>If NetworkData is set to TRUE two additional global variables returned by the function are:
</p>
<p>- NetwIndex : Vector maps alls hyperlinks (nodes) with a unique integer ID
</p>
<p>- NetwEdges : data.frame representing edges of the network, with these column : From, To, Weight (the Depth level where the link connection has been discovered) and Type (1 for internal hyperlinks 2 for external hyperlinks).
</p>


<h3>Author(s)</h3>

<p>salim khalil
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

 ######### Crawl, index, and store all pages of a websites using 4 cores and 4 parallel requests
 #
 Rcrawler(Website ="http://glofile.com/", no_cores = 4, no_conn = 4)

 ######### Crawl and index the website using 8 cores and 8 parallel requests with respect to
 # robot.txt rules using Mozilla string in user agent.

 Rcrawler(Website = "http://www.example.com/", no_cores=8, no_conn=8, Obeyrobots = TRUE,
 Useragent="Mozilla 3.11")

 ######### Crawl the website using the default configuration and scrape specific data from
 # the website, in this case we need all posts (articles and titles) matching two XPath patterns.
 # we know that all blog posts have datesin their URLs like 2017/09/08 so to avoid
 # collecting category or other pages we can tell the crawler that desired page's URLs
 # are like 4-digit/2-digit/2-digit/ using regular expression.
 # Note thatyou can use the excludepattern  parameter to exclude a node from being
 # extracted, e.g., in the case that a desired node includes (is a parent of) an
 # undesired "child" node. (article having inner ads or menu)

 Rcrawler(Website = "http://www.glofile.com/", dataUrlfilter =  "/[0-9]{4}/[0-9]{2}/",
 ExtractXpathPat = c("//*/article","//*/h1"), PatternsNames = c("content","title"))

 ######### Crawl the website. and collect pages having URLs matching this regular expression
 # pattern (/[0-9]{4}/[0-9]{2}/). Collected pages will be stored in a local repository
 # named "myrepo". And The crawler stops After reaching the third level of website depth.

  Rcrawler(Website = "http://www.example.com/", no_cores = 4, no_conn = 4,
  dataUrlfilter =  "/[0-9]{4}/[0-9]{2}/", DIR = "./myrepo", MaxDepth=3)


 ######### Crawl the website and collect/scrape only webpage related to a topic
 # Crawl the website and collect pages containing keyword1 or keyword2 or both.
 # To crawl a website and collect/scrape only some web pages related to a specific topic,
 # like gathering posts related to Donald trump from a news website. Rcrawler function
 # has two useful parameters KeywordsFilter and KeywordsAccuracy.
 #
 # KeywordsFilter : a character vector, here you should provide keywords/terms of the topic
 # you are looking for. Rcrawler will calculate an accuracy score based on matched keywords
 # and their occurrence on the page, then it collects or scrapes only web pages with at
 # least a score of 1% wich mean at least one keyword is founded one time on the page.
 # This parameter must be a vector with at least one keyword like c("mykeyword").
 #
 # KeywordsAccuracy: Integer value range between 0 and 100, used only in combination with
 # KeywordsFilter parameter to determine the minimum accuracy of web pages to be collected
 # /scraped. You can use one or more search terms; the accuracy will be calculated based on
 # how many provided keywords are found on on the page plus their occurrence rate.
 # For example, if only one keyword is provided c("keyword"), 50% means one occurrence of
 # "keyword" in the page 100% means five occurrences of "keyword" in the page

  Rcrawler(Website = "http://www.example.com/", KeywordsFilter = c("keyword1", "keyword2"))

 # Crawl the website and collect webpages that has an accuracy percentage higher than 50%
 # of matching keyword1 and keyword2.

  Rcrawler(Website = "http://www.example.com/", KeywordsFilter = c("keyword1", "keyword2"),
   KeywordsAccuracy = 50)


 ######### Crawl a website search results
 # In the case of scraping web pages specific to a topic of your interest; The methods
 # above has some disadvantages which are complexity and time consuming as the whole
 # website need to be crawled and each page is analyzed to findout desired pages.
 # As result you may want to make use of the search box of the website and then directly
 # crawl only search result pages. To do so, you may use \code{crawlUrlfilter} and
 # \code{dataUrlfilter} arguments or \code{crawlZoneCSSPat}/\code{CrawlZoneXPath} with
 \code{dataUrlfilter}.
 #- \code{crawlUrlfilter}:what urls shoud be crawled (followed).
 #- \code{dataUrlfilter}: what urls should be collected (download HTML or extract data ).
 #- \code{crawlZoneCSSPat} Or \code{CrawlZoneXPath}: the page section where links to be
     crawled are located.

 # Example1
 # the command below will crawl all result pages knowing that result pages are like :
    http://glofile.com/?s=sur
    http://glofile.com/page/2/?s=sur
    http://glofile.com/page/2/?s=sur
 # so they all have "s=sur" in common
 # Post pages should be crawled also, post urls are like
   http://glofile.com/2017/06/08/placements-quelles-solutions-pour-dper/
   http://glofile.com/2017/06/08/taux-nette-detente/
 # which contain a date format march regex "[0-9]{4}/[0-9]{2}/[0-9]{2}

 Rcrawler(Website = "http://glofile.com/?s=sur", no_cores = 4, no_conn = 4,
 crawlUrlfilter = c("[0-9]{4}/[0-9]{2}/[0-9]d{2}","s=sur"))

 # In addition by using dataUrlfilter we specify that :
 #  1- only post pages should be collected/scraped not all crawled result pages
 #  2- additional urls should not be retreived from post page
 #  (like post urls listed in 'related topic' or 'see more' sections)

 Rcrawler(Website = "http://glofile.com/?s=sur", no_cores = 4, no_conn = 4,
 crawlUrlfilter = c("[0-9]{4}/[0-9]{2}/[0-9]d{2}","s=sur"),
 dataUrlfilter = "[0-9]{4}/[0-9]{2}/[0-9]{2}")

 # Example 2
 # collect job pages from indeed search result of "data analyst"

 Rcrawler(Website = "https://www.indeed.com/jobs?q=data+analyst&amp;l=Tampa,+FL",
  no_cores = 4 , no_conn = 4,
  crawlUrlfilter = c("/rc/","start="), dataUrlfilter = "/rc/")
 # To include related post jobs on each collected post remove dataUrlfilter

 # Example 3
 # One other way to control the crawler behaviour, and to avoid fetching
 # unnecessary links is to indicate to crawler the page zone of interest
 # (a page section from where links should be grabed and crawled).
 # The follwing example is similar to the last one,except this time we provide
 # the xpath pattern of results search section to be crawled with all links within.

 Rcrawler(Website = "https://www.indeed.com/jobs?q=data+analyst&amp;l=Tampa,+FL",
  no_cores = 4 , no_conn = 4,MaxDepth = 3,
  crawlZoneXPath = c("//*[\@id='resultsCol']"), dataUrlfilter = "/rc/")


 ######### crawl and scrape a forum posts and replays, each page has a title and
 # a list of replays , ExtractCSSPat = c("head&gt;title","div[class=\"post\"]") .
 # All replays have the same pattern, therfore we set TRUE ManyPerPattern
 # to extract all of them.

 Rcrawler(Website = "https://bitcointalk.org/", ManyPerPattern = TRUE,
 ExtractCSSPat = c("head&gt;title","div[class=\"post\"]"),
 no_cores = 4, no_conn =4, PatternsName = c("Title","Replays"))


 ######### scrape data/collect pages meeting your custom criteria,
 # This is useful when filetring by keyword or urls does not fullfil your needs, for example
 # if you want to detect target pages  by classification/prediction model, or simply by checking
 # a sppecifi text value/field in the web page, you can create a custom filter function for
 # page selection as follow.
 # First will create and test our function and test it with un one page .

 pageinfo&lt;-LinkExtractor(url="http://glofile.com/index.php/2017/06/08/sondage-quel-budget/",
 encod=encod, ExternalLInks = TRUE)

 Customfilterfunc&lt;-function(pageinfo){
  decision&lt;-FALSE
  # put your conditions here
    if(pageinfo$Info$Source_page ... ) ....
  # then return a boolean value TRUE : should be collected / FALSE should be escaped

  return TRUE or FALSE
 }
  # Finally, you just call it inside Rcrawler function, Then the crawler will evaluate each
   page using your set of rules.

 Rcrawler(Website = "http://glofile.com", no_cores=2, FUNPageFilter= Customfilterfunc )

 ######### Website Network
 # Crawl the entire website, and create network edges DATA of internal links.
 # Using Igraph for exmaple you can plot the network by the following commands

   Rcrawler(Website = "http://glofile.com/" , no_cores = 4, no_conn = 4, NetworkData = TRUE)
   library(igraph)
   network&lt;-graph.data.frame(NetwEdges, directed=T)
   plot(network)

  # Crawl the entire website, and create network edges DATA of internal and external links .
  Rcrawler(Website = "http://glofile.com/" , no_cores = 4, no_conn = 4, NetworkData = TRUE,
  NetwExtLinks = TRUE)

###### Crawl a website using a web driver (Vitural browser)
###########################################################################
## In some case you may need to retreive content from a web page which
## requires authentication via a login page like private forums, platforms..
## In this case you need to run \link{LoginSession} function to establish a
## authenticated browser session; then use \link{LinkExtractor} to fetch
## the URL using the auhenticated session.
## In the example below we will try to fech a private blog post which
## require authentification .

If you retreive the page using regular function LinkExtractor or your browser
page&lt;-LinkExtractor("http://glofile.com/index.php/2017/06/08/jcdecaux/")
The post is not visible because it's private.
Now we will try to login to access this post using folowing creditentials
username : demo and password : rc@pass@r

#1 Download and install phantomjs headless browser (skip if installed)
install_browser()

#2 start browser process
br &lt;-run_browser()

#3 create auhenticated session
#  see \link{LoginSession} for more details

 LS&lt;-LoginSession(Browser = br, LoginURL = 'http://glofile.com/wp-login.php',
                LoginCredentials = c('demo','rc@pass@r'),
                cssLoginCredentials =c('#user_login', '#user_pass'),
                cssLoginButton='#wp-submit' )

#check if login successful
LS$session$getTitle()
#Or
LS$session$getUrl()
#Or
LS$session$takeScreenshot(file = 'sc.png')
LS$session$getUrl()
LS&lt;-run_browser()
LS&lt;-LoginSession(Browser = LS, LoginURL = 'https://manager.submittable.com/login',
   LoginCredentials = c('your email','your password'),
   cssLoginFields =c('#email', '#password'),
   XpathLoginButton ='//*[\@type=\"submit\"]' )


# page&lt;-LinkExtractor(url='https://manager.submittable.com/beta/discover/119087',
LoggedSession = LS)
# cont&lt;-ContentScraper(HTmlText = page$Info$Source_page,
XpathPatterns = c("//*[\@id=\"submitter-app\"]/div/div[2]/div/div/div/div/div[3]",
"//*[\@id=\"submitter-app\"]/div/div[2]/div/div/div/div/div[2]/div[1]/div[1]" ),
PatternsName = c("Article","Title"),astext = TRUE )


## End(Not run)


</code></pre>

<hr>
<h2 id='RobotParser'>RobotParser fetch and parse robots.txt</h2><span id='topic+RobotParser'></span>

<h3>Description</h3>

<p>This function fetch and parse robots.txt file of the website which is specified in the first argument and return the list of correspending rules .
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RobotParser(website, useragent)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RobotParser_+3A_website">website</code></td>
<td>
<p>character, url of the website which rules have to be extracted  .</p>
</td></tr>
<tr><td><code id="RobotParser_+3A_useragent">useragent</code></td>
<td>
<p>character, the useragent of the crawler</p>
</td></tr>
</table>


<h3>Value</h3>

<p>return a list of three elements, the first is a character vector of Disallowed directories, the third is a Boolean value which is TRUE if the user agent of the crawler is blocked.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#RobotParser("http://www.glofile.com","AgentX")
#Return robot.txt rules and check whether AgentX is blocked or not.


</code></pre>

<hr>
<h2 id='run_browser'>Start up web driver process on localhost, with a random port</h2><span id='topic+run_browser'></span>

<h3>Description</h3>

<p>Phantomjs is a headless browser, it provide automated control of a web page in
an environment similar to web browsers, but via a command-line. It's able
to render and understand HTML the same way a regular browser would, including
styling elements such as page layout, colors, font selection and execution of
JavaScript and AJAX which are usually not available when using GET request methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>run_browser(debugLevel = "DEBUG", timeout = 5000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="run_browser_+3A_debuglevel">debugLevel</code></td>
<td>
<p>debug level, possible values: 'INFO', 'ERROR', 'WARN', 'DEBUG'</p>
</td></tr>
<tr><td><code id="run_browser_+3A_timeout">timeout</code></td>
<td>
<p>How long to wait (in milliseconds) for the webdriver connection to be established to the phantomjs process.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function will throw an error if webdriver(phantomjs) cannot be found, or cannot be started.
It works with a timeout of five seconds.
</p>
<p>If you got the forllwing error, this means that your operating system or antivirus
is bloking the webdriver (phantom.js) process, try to disable your antivirus temporarily or
adjust your system configuration to allow phantomjs and processx executable (<a href="#topic+browser_path">browser_path</a>
to know where phantomjs is located).
Error in supervisor_start() :  processx supervisor was not ready after 5 seconds.
</p>


<h3>Value</h3>

<p>A list of <code>callr::process</code> object, and
<code>port</code>, the local port where phantom is running.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

#If driver is not installed yet then
install_browser()

br&lt;-run_browser()


## End(Not run)


</code></pre>

<hr>
<h2 id='stop_browser'>Stop web driver process and Remove its Object</h2><span id='topic+stop_browser'></span>

<h3>Description</h3>

<p>At the end of All your operations with the web river, you should stop its process
and remove the driver R object else you may have troubles restarting R normaly.
Throws and error if webdriver phantomjs cannot be found, or cannot be started.
It works with a timeout of five seconds.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stop_browser(browser)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stop_browser_+3A_browser">browser</code></td>
<td>
<p>the web driver object created by <code><a href="#topic+run_browser">run_browser</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of <code>process</code>, the <code>callr::process</code> object, and
<code>port</code>, the local port where phantom is running.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

#Start the browser
br&lt;-run_browser()

#kill the browser process
stop_browser(br)
#remove the object reference
rm(br)

 
## End(Not run)


</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
