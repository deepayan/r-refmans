<!DOCTYPE html><html><head><title>Help for package cvms</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {cvms}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#cvms-package'><p>cvms: A package for cross-validating regression and classification models</p></a></li>
<li><a href='#baseline'><p>Create baseline evaluations</p></a></li>
<li><a href='#baseline_binomial'><p>Create baseline evaluations for binary classification</p></a></li>
<li><a href='#baseline_gaussian'><p>Create baseline evaluations for regression models</p></a></li>
<li><a href='#baseline_multinomial'><p>Create baseline evaluations</p></a></li>
<li><a href='#binomial_metrics'><p>Select metrics for binomial evaluation</p></a></li>
<li><a href='#combine_predictors'><p>Generate model formulas by combining predictors</p></a></li>
<li><a href='#compatible.formula.terms'><p>Compatible formula terms</p></a></li>
<li><a href='#confusion_matrix'><p>Create a confusion matrix</p></a></li>
<li><a href='#cross_validate'><p>Cross-validate regression models for model selection</p></a></li>
<li><a href='#cross_validate_fn'><p>Cross-validate custom model functions for model selection</p></a></li>
<li><a href='#evaluate'><p>Evaluate your model's performance</p></a></li>
<li><a href='#evaluate_residuals'><p>Evaluate residuals from a regression task</p></a></li>
<li><a href='#font'><p>Create a list of font settings for plots</p></a></li>
<li><a href='#gaussian_metrics'><p>Select metrics for Gaussian evaluation</p></a></li>
<li><a href='#model_functions'><p>Examples of model_fn functions</p></a></li>
<li><a href='#most_challenging'><p>Find the data points that were hardest to predict</p></a></li>
<li><a href='#multiclass_probability_tibble'><p>Generate a multiclass probability tibble</p></a></li>
<li><a href='#multinomial_metrics'><p>Select metrics for multinomial evaluation</p></a></li>
<li><a href='#musicians'><p>Musician groups</p></a></li>
<li><a href='#participant.scores'><p>Participant scores</p></a></li>
<li><a href='#plot_confusion_matrix'><p>Plot a confusion matrix</p></a></li>
<li><a href='#plot_metric_density'><p>Density plot for a metric</p></a></li>
<li><a href='#plot_probabilities'><p>Plot predicted probabilities</p></a></li>
<li><a href='#plot_probabilities_ecdf'><p>Plot ECDF for the predicted probabilities</p></a></li>
<li><a href='#precomputed.formulas'><p>Precomputed formulas</p></a></li>
<li><a href='#predict_functions'><p>Examples of predict_fn functions</p></a></li>
<li><a href='#predicted.musicians'><p>Predicted musician groups</p></a></li>
<li><a href='#preprocess_functions'><p>Examples of preprocess_fn functions</p></a></li>
<li><a href='#process_info_binomial'><p>A set of process information object constructors</p></a></li>
<li><a href='#reconstruct_formulas'><p>Reconstruct model formulas from results tibbles</p></a></li>
<li><a href='#render_toc'><p>Render Table of Contents</p></a></li>
<li><a href='#select_definitions'><p>Select model definition columns</p></a></li>
<li><a href='#select_metrics'><p>Select columns with evaluation metrics and model definitions</p></a></li>
<li><a href='#simplify_formula'><p>Simplify formula with inline functions</p></a></li>
<li><a href='#sum_tile_settings'><p>Create a list of settings for the sum tiles in plot_confusion_matrix()</p></a></li>
<li><a href='#summarize_metrics'><p>Summarize metrics with common descriptors</p></a></li>
<li><a href='#update_hyperparameters'><p>Check and update hyperparameters</p></a></li>
<li><a href='#validate'><p>Validate regression models on a test set</p></a></li>
<li><a href='#validate_fn'><p>Validate a custom model function on a test set</p></a></li>
<li><a href='#wines'><p>Wine varieties</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Cross-Validation for Model Selection</td>
</tr>
<tr>
<td>Version:</td>
<td>1.6.1</td>
</tr>
<tr>
<td>Description:</td>
<td>Cross-validate one or multiple regression and classification models
    and get relevant evaluation metrics in a tidy format. Validate the
    best model on a test set and compare it to a baseline evaluation.
    Alternatively, evaluate predictions from an external model. Currently
    supports regression and classification (binary and multiclass).
    Described in chp. 5 of Jeyaraman, B. P., Olsen, L. R., 
    &amp; Wambugu M. (2019, ISBN: 9781838550134).</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/ludvigolsen/cvms">https://github.com/ludvigolsen/cvms</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/ludvigolsen/cvms/issues">https://github.com/ludvigolsen/cvms/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5)</td>
</tr>
<tr>
<td>Imports:</td>
<td>checkmate (&ge; 2.0.0), data.table (&ge; 1.12), dplyr (&ge; 0.8.5),
ggplot2, groupdata2 (&ge; 2.0.2), lifecycle, lme4 (&ge; 1.1-23),
MuMIn (&ge; 1.43.17), parameters (&ge; 0.15.0), plyr, pROC (&ge;
1.16.0), purrr, rearrr (&ge; 0.3.0), recipes (&ge; 0.1.13), rlang
(&ge; 0.4.7), stats, stringr, tibble (&ge; 3.0.3), tidyr (&ge;
1.1.2), utils</td>
</tr>
<tr>
<td>Suggests:</td>
<td>AUC, covr (&ge; 3.3.1), e1071 (&ge; 1.7-2), furrr, ggimage (&ge;
0.3.3), ggnewscale (&ge; 0.4.3), knitr, merDeriv (&ge; 0.2-4), nnet
(&ge; 7.3-12), randomForest (&ge; 4.6-14), rmarkdown, rsvg,
testthat (&ge; 2.3.2), xpectr (&ge; 0.4.1)</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>RdMacros:</td>
<td>lifecycle</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-02-27 17:01:19 UTC; au547627</td>
</tr>
<tr>
<td>Author:</td>
<td>Ludvig Renbo Olsen
    <a href="https://orcid.org/0009-0006-6798-7454"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cre]
    (@ludvigolsen),
  Hugh Benjamin Zachariae [aut],
  Indrajeet Patil <a href="https://orcid.org/0000-0003-1995-6531"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb] (@patilindrajeets),
  Daniel LÃ¼decke <a href="https://orcid.org/0000-0002-8895-3206"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Ludvig Renbo Olsen &lt;r-pkgs@ludvigolsen.dk&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-02-27 20:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='cvms-package'>cvms: A package for cross-validating regression and classification models</h2><span id='topic+cvms'></span><span id='topic+cvms-package'></span>

<h3>Description</h3>

<p>Perform (repeated) cross-validation on a list of model formulas. Validate the best model on a validation set.
Perform baseline evaluations on your test set. Generate model formulas by combining your fixed effects.
Evaluate predictions from an external model.
</p>


<h3>Details</h3>

<p>Returns results in a <code>tibble</code> for easy comparison, reporting and further analysis.
</p>
<p>The main functions are:
<code><a href="#topic+cross_validate">cross_validate()</a></code>,
<code><a href="#topic+cross_validate_fn">cross_validate_fn()</a></code>,
<code><a href="#topic+validate">validate()</a></code>,
<code><a href="#topic+validate_fn">validate_fn()</a></code>,
<code><a href="#topic+baseline">baseline()</a></code>,
and <code><a href="#topic+evaluate">evaluate()</a></code>.
</p>


<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/ludvigolsen/cvms">https://github.com/ludvigolsen/cvms</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/ludvigolsen/cvms/issues">https://github.com/ludvigolsen/cvms/issues</a>
</p>
</li></ul>


<hr>
<h2 id='baseline'>Create baseline evaluations</h2><span id='topic+baseline'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#maturing"><img src="../help/figures/lifecycle-maturing.svg" alt='[Maturing]' /></a>
</p>
<p>Create a baseline evaluation of a test set.
</p>
<p>In modelling, a <em>baseline</em> is a result that
is meaningful to compare the results from our models to. For instance, in
classification, we usually want our results to be better than <em>random guessing</em>.
E.g. if we have three classes, we can expect an accuracy of <code>33.33%</code>, as for every
observation we have <code>1/3</code> chance of guessing the correct class. So our model should achieve
a higher accuracy than <code>33.33%</code> before it is more useful to us than guessing.
</p>
<p>While this expected value is often fairly straightforward to find analytically, it
only represents what we can expect on average. In reality, it's possible to get far better
results than that by guessing.
<strong><code>baseline()</code></strong> (<code>binomial</code>, <code>multinomial</code>)
finds the range of likely values by evaluating multiple sets
of random predictions and summarizing them with a set of useful descriptors.
If random guessing frequently obtains an accuracy of <code>40%</code>, perhaps our model
should have better performance than this, before we declare it better than guessing.
</p>


<h4><strong>How</strong></h4>

<p>When <code>`family`</code> is <code>binomial</code>: evaluates <code>`n`</code> sets of random predictions
against the dependent variable, along with a set of all <code>0</code> predictions and
a set of all <code>1</code> predictions. See also <code><a href="#topic+baseline_binomial">baseline_binomial()</a></code>.
</p>
<p>When <code>`family`</code> is <code>multinomial</code>: creates <em>one-vs-all</em> (binomial)
baseline evaluations for <code>`n`</code> sets of random predictions against the dependent variable,
along with sets of &quot;all class x,y,z,...&quot; predictions.
See also <code><a href="#topic+baseline_multinomial">baseline_multinomial()</a></code>.
</p>
<p>When <code>`family`</code> is <code>gaussian</code>: fits baseline models (<code>y ~ 1</code>) on <code>`n`</code> random
subsets of <code>`train_data`</code> and evaluates each model on <code>`test_data`</code>. Also evaluates a
model fitted on all rows in <code>`train_data`</code>.
See also <code><a href="#topic+baseline_gaussian">baseline_gaussian()</a></code>.
</p>



<h4><strong>Wrapper functions</strong></h4>

<p>Consider using one of the wrappers, as they are simpler to use and understand:
<strong><code><a href="#topic+baseline_gaussian">baseline_gaussian()</a></code></strong>,
<strong><code><a href="#topic+baseline_multinomial">baseline_multinomial()</a></code></strong>, and
<strong><code><a href="#topic+baseline_binomial">baseline_binomial()</a></code></strong>.
</p>



<h3>Usage</h3>

<pre><code class='language-R'>baseline(
  test_data,
  dependent_col,
  family,
  train_data = NULL,
  n = 100,
  metrics = list(),
  positive = 2,
  cutoff = 0.5,
  random_generator_fn = runif,
  random_effects = NULL,
  min_training_rows = 5,
  min_training_rows_left_out = 3,
  REML = FALSE,
  parallel = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="baseline_+3A_test_data">test_data</code></td>
<td>
<p><code>data.frame</code>.</p>
</td></tr>
<tr><td><code id="baseline_+3A_dependent_col">dependent_col</code></td>
<td>
<p>Name of dependent variable in the supplied test and training sets.</p>
</td></tr>
<tr><td><code id="baseline_+3A_family">family</code></td>
<td>
<p>Name of family. (Character)
</p>
<p>Currently supports <code>"gaussian"</code>, <code>"binomial"</code> and <code>"multinomial"</code>.</p>
</td></tr>
<tr><td><code id="baseline_+3A_train_data">train_data</code></td>
<td>
<p><code>data.frame</code>. Only used when <code>`family`</code> is <code>"gaussian"</code>.</p>
</td></tr>
<tr><td><code id="baseline_+3A_n">n</code></td>
<td>
<p>Number of random samplings to perform. (Default is <code>100</code>)
</p>
<p>For <code>gaussian</code>: The number of random samplings of <code>`train_data`</code> to fit baseline models on.
</p>
<p>For <code>binomial</code> and <code>multinomial</code>: The number of sets of random predictions to evaluate.</p>
</td></tr>
<tr><td><code id="baseline_+3A_metrics">metrics</code></td>
<td>
<p><code>list</code> for enabling/disabling metrics.
</p>
<p>E.g. <code>list("RMSE" = FALSE)</code> would remove <code>RMSE</code> from the regression results,
and <code>list("Accuracy" = TRUE)</code> would add the regular <code>Accuracy</code> metric
to the classification results.
Default values (<code>TRUE</code>/<code>FALSE</code>) will be used for the remaining available metrics.
</p>
<p>You can enable/disable all metrics at once by including
<code>"all" = TRUE/FALSE</code> in the <code>list</code>. This is done prior to enabling/disabling
individual metrics, why f.i. <code>list("all" = FALSE, "RMSE" = TRUE)</code>
would return only the <code>RMSE</code> metric.
</p>
<p>The <code>list</code> can be created with
<code><a href="#topic+gaussian_metrics">gaussian_metrics()</a></code>,
<code><a href="#topic+binomial_metrics">binomial_metrics()</a></code>, or
<code><a href="#topic+multinomial_metrics">multinomial_metrics()</a></code>.
</p>
<p>Also accepts the string <code>"all"</code>.</p>
</td></tr>
<tr><td><code id="baseline_+3A_positive">positive</code></td>
<td>
<p>Level from dependent variable to predict.
Either as character (<em>preferable</em>) or level index (<code>1</code> or <code>2</code> - alphabetically).
</p>
<p>E.g. if we have the levels <code>"cat"</code> and <code>"dog"</code> and we want <code>"dog"</code> to be the positive class,
we can either provide <code>"dog"</code> or <code>2</code>, as alphabetically, <code>"dog"</code> comes after <code>"cat"</code>.
</p>
<p><strong>Note:</strong> For <em>reproducibility</em>, it's preferable to <strong>specify the name directly</strong>, as
different <code><a href="base.html#topic+locales">locales</a></code> may sort the levels differently.
</p>
<p>Used when calculating confusion matrix metrics and creating <code>ROC</code> curves.
</p>
<p>N.B. Only affects evaluation metrics, not the returned predictions.
</p>
<p>N.B. <strong>Binomial only</strong>. (Character or Integer)</p>
</td></tr>
<tr><td><code id="baseline_+3A_cutoff">cutoff</code></td>
<td>
<p>Threshold for predicted classes. (Numeric)
</p>
<p>N.B. <strong>Binomial only</strong></p>
</td></tr>
<tr><td><code id="baseline_+3A_random_generator_fn">random_generator_fn</code></td>
<td>
<p>Function for generating random numbers when <code>type</code> is <code>"multinomial"</code>.
The <code>softmax</code> function is applied to the generated numbers to transform them to probabilities.
</p>
<p>The first argument must be the number of random numbers to generate,
as no other arguments are supplied.
</p>
<p>To test the effect of using different functions,
see <code><a href="#topic+multiclass_probability_tibble">multiclass_probability_tibble()</a></code>.
</p>
<p>N.B. <strong>Multinomial only</strong></p>
</td></tr>
<tr><td><code id="baseline_+3A_random_effects">random_effects</code></td>
<td>
<p>Random effects structure for the Gaussian baseline model. (Character)
</p>
<p>E.g. with <code>"(1|ID)"</code>, the model becomes <code>"y ~ 1 + (1|ID)"</code>.
</p>
<p>N.B. <strong>Gaussian only</strong></p>
</td></tr>
<tr><td><code id="baseline_+3A_min_training_rows">min_training_rows</code></td>
<td>
<p>Minimum number of rows in the random subsets of <code>`train_data`</code>.
</p>
<p><strong>Gaussian only</strong>. (Integer)</p>
</td></tr>
<tr><td><code id="baseline_+3A_min_training_rows_left_out">min_training_rows_left_out</code></td>
<td>
<p>Minimum number of rows left out of the random subsets of <code>`train_data`</code>.
</p>
<p>I.e. a subset will maximally have the size:
</p>
<p><code>max_rows_in_subset = nrow(`train_data`) - `min_training_rows_left_out`</code>.
</p>
<p>N.B. <strong>Gaussian only</strong>. (Integer)</p>
</td></tr>
<tr><td><code id="baseline_+3A_reml">REML</code></td>
<td>
<p>Whether to use Restricted Maximum Likelihood. (Logical)
</p>
<p>N.B. <strong>Gaussian only</strong>. (Integer)</p>
</td></tr>
<tr><td><code id="baseline_+3A_parallel">parallel</code></td>
<td>
<p>Whether to run the <code>`n`</code> evaluations in parallel. (Logical)
</p>
<p>Remember to register a parallel backend first.
E.g. with <code>doParallel::registerDoParallel</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Packages used:
</p>


<h4>Models</h4>

<p>Gaussian: <code><a href="stats.html#topic+lm">stats::lm</a></code>, <code><a href="lme4.html#topic+lmer">lme4::lmer</a></code>
</p>



<h4>Results</h4>

<p><strong>Gaussian</strong>:
</p>
<p>r2m : <code><a href="MuMIn.html#topic+r.squaredGLMM">MuMIn::r.squaredGLMM</a></code>
</p>
<p>r2c : <code><a href="MuMIn.html#topic+r.squaredGLMM">MuMIn::r.squaredGLMM</a></code>
</p>
<p>AIC : <code><a href="stats.html#topic+AIC">stats::AIC</a></code>
</p>
<p>AICc : <code><a href="MuMIn.html#topic+AICc">MuMIn::AICc</a></code>
</p>
<p>BIC : <code><a href="stats.html#topic+BIC">stats::BIC</a></code>
</p>
<p><strong>Binomial</strong> and <strong>Multinomial</strong>:
</p>
<p>ROC and related metrics:
</p>
<p>Binomial: <code><a href="pROC.html#topic+roc">pROC::roc</a></code>
</p>
<p>Multinomial: <code><a href="pROC.html#topic+multiclass.roc">pROC::multiclass.roc</a></code>
</p>



<h3>Value</h3>

<p><code>list</code> containing:
</p>

<ol>
<li><p> a <code>tibble</code> with summarized results (called <code>summarized_metrics</code>)
</p>
</li>
<li><p> a <code>tibble</code> with random evaluations (<code>random_evaluations</code>)
</p>
</li>
<li><p> a <code>tibble</code> with the summarized class level results
(<code>summarized_class_level_results</code>)
<strong>(Multinomial only)</strong>
</p>
</li></ol>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>


<h4>Gaussian Results</h4>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>
<p>The <strong>Summarized Results</strong> <code>tibble</code> contains:
</p>
<p>Average <strong><code>RMSE</code></strong>, <strong><code>MAE</code></strong>, <strong><code>NRMSE(IQR)</code></strong>,
<strong><code>RRSE</code></strong>, <strong><code>RAE</code></strong>, <strong><code>RMSLE</code></strong>.
</p>
<p>See the additional metrics (disabled by default) at <code><a href="#topic+gaussian_metrics">?gaussian_metrics</a></code>.
</p>
<p>The <strong>Measure</strong> column indicates the statistical descriptor used on the evaluations.
The row where <code>Measure == All_rows</code> is the evaluation when the baseline model
is trained on all rows in <code>`train_data`</code>.
</p>
<p>The <strong>Training Rows</strong> column contains the aggregated number of rows used from <code>`train_data`</code>,
when fitting the baseline models.
</p>
<p>....................................................................
</p>
<p>The <strong>Random Evaluations</strong> <code>tibble</code> contains:
</p>
<p>The <strong>non-aggregated metrics</strong>.
</p>
<p>A nested <code>tibble</code> with the <strong>predictions</strong> and targets.
</p>
<p>A nested <code>tibble</code> with the <strong>coefficients</strong> of the baseline models.
</p>
<p>Number of <strong>training rows</strong> used when fitting the baseline model on the training set.
</p>
<p>A nested <strong>Process</strong> information object with information
about the evaluation.
</p>
<p>Name of <strong>dependent</strong> variable.
</p>
<p>Name of <strong>fixed</strong> effect (bias term only).
</p>
<p><strong>Random</strong> effects structure (if specified).
</p>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>


<h4>Binomial Results</h4>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>
<p>Based on the generated test set predictions,
a confusion matrix and <code>ROC</code> curve are used to get the following:
</p>
<p><code>ROC</code>:
</p>
<p><strong><code>AUC</code></strong>, <strong><code>Lower CI</code></strong>, and <strong><code>Upper CI</code></strong>
</p>
<p>Note, that the <code>ROC</code> curve is only computed when <code>AUC</code> is enabled.
</p>
<p><code>Confusion Matrix</code>:
</p>
<p><strong><code>Balanced Accuracy</code></strong>,
<strong><code>Accuracy</code></strong>,
<strong><code>F1</code></strong>,
<strong><code>Sensitivity</code></strong>,
<strong><code>Specificity</code></strong>,
<strong><code>Positive Predictive Value</code></strong>,
<strong><code>Negative Predictive Value</code></strong>,
<strong><code>Kappa</code></strong>,
<strong><code>Detection Rate</code></strong>,
<strong><code>Detection Prevalence</code></strong>,
<strong><code>Prevalence</code></strong>, and
<strong><code>MCC</code></strong> (Matthews correlation coefficient).
</p>
<p>....................................................................
</p>
<p>The <strong>Summarized Results</strong> <code>tibble</code> contains:
</p>
<p>The <strong>Measure</strong> column indicates the statistical descriptor used on the evaluations.
The row where <code>Measure == All_0</code> is the evaluation when all predictions are <code>0</code>.
The row where <code>Measure == All_1</code> is the evaluation when all predictions are <code>1</code>.
</p>
<p>The <strong>aggregated metrics</strong>.
</p>
<p>....................................................................
</p>
<p>The <strong>Random Evaluations</strong> <code>tibble</code> contains:
</p>
<p>The <strong>non-aggregated metrics</strong>.
</p>
<p>A nested <code>tibble</code> with the <strong>predictions</strong> and targets.
</p>
<p>A <code>list</code> of <strong>ROC</strong> curve objects (if computed).
</p>
<p>A nested <code>tibble</code> with the <strong>confusion matrix</strong>.
The <code>Pos_</code> columns tells you whether a row is a
True Positive (<code>TP</code>), True Negative (<code>TN</code>), False Positive (<code>FP</code>),
or False Negative (<code>FN</code>), depending on which level is the &quot;positive&quot; class.
I.e. the level you wish to predict.
</p>
<p>A nested <strong>Process</strong> information object with information
about the evaluation.
</p>
<p>Name of <strong>dependent</strong> variable.
</p>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>


<h4>Multinomial Results</h4>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>
<p>Based on the generated test set predictions,
one-vs-all (binomial) evaluations are performed and aggregated
to get the same metrics as in the <code>binomial</code> results
(excluding <code>MCC</code>, <code>AUC</code>, <code>Lower CI</code> and <code>Upper CI</code>),
with the addition of <strong>Overall Accuracy</strong> and <em>multiclass</em>
<strong>MCC</strong> in the summarized results.
It is possible to enable multiclass <strong>AUC</strong> as well, which has been
disabled by default as it is slow to calculate when there's a large set of classes.
</p>
<p>Since we use macro-averaging, <strong><code>Balanced Accuracy</code></strong> is the macro-averaged
metric, <em>not</em> the macro sensitivity as sometimes used.
</p>
<p>Note: we also refer to the <em>one-vs-all evaluations</em> as the <em>class level results</em>.
</p>
<p>....................................................................
</p>
<p>The <strong>Summarized Results</strong> <code>tibble</code> contains:
</p>
<p>Summary of the random evaluations.
</p>
<p><strong>How</strong>: First, the one-vs-all binomial evaluations are aggregated by repetition,
then, these aggregations are summarized. Besides the
metrics from the binomial evaluations (see <em>Binomial Results</em> above), it
also includes <strong><code>Overall Accuracy</code></strong> and <em>multiclass</em> <strong><code>MCC</code></strong>.
</p>
<p>The <strong>Measure</strong> column indicates the statistical descriptor used on the evaluations.
The <strong>Mean</strong>, <strong>Median</strong>, <strong>SD</strong>, <strong>IQR</strong>, <strong>Max</strong>, <strong>Min</strong>,
<strong>NAs</strong>, and <strong>INFs</strong> measures describe the <em>Random Evaluations</em> <code>tibble</code>,
while the <strong>CL_Max</strong>, <strong>CL_Min</strong>, <strong>CL_NAs</strong>, and
<strong>CL_INFs</strong> describe the <strong>C</strong>lass <strong>L</strong>evel results.
</p>
<p>The rows where <code>Measure == All_&lt;&lt;class name&gt;&gt;</code> are the evaluations when all
the observations are predicted to be in that class.
</p>
<p>....................................................................
</p>
<p>The <strong>Summarized Class Level Results</strong> <code>tibble</code> contains:
</p>
<p>The (nested) summarized results for each class, with the same metrics and descriptors as
the <em>Summarized Results</em> <code>tibble</code>. Use <code><a href="tidyr.html#topic+unnest">tidyr::unnest</a></code>
on the <code>tibble</code> to inspect the results.
</p>
<p><strong>How</strong>: The one-vs-all evaluations are summarized by class.
</p>
<p>The rows where <code>Measure == All_0</code> are the evaluations when none of the observations
are predicted to be in that class, while the rows where <code>Measure == All_1</code> are the
evaluations when all of the observations are predicted to be in that class.
</p>
<p>....................................................................
</p>
<p>The <strong>Random Evaluations</strong> <code>tibble</code> contains:
</p>
<p>The repetition results with the same metrics as the <em>Summarized Results</em> <code>tibble</code>.
</p>
<p><strong>How</strong>: The one-vs-all evaluations are aggregated by repetition.
If a metric contains one or more <code>NAs</code> in the one-vs-all evaluations, it
will lead to an <code>NA</code> result for that repetition.
</p>
<p>Also includes:
</p>
<p>A nested <code>tibble</code> with the one-vs-all binomial evaluations (<strong>Class Level Results</strong>),
including nested <strong>Confusion Matrices</strong> and the
<strong>Support</strong> column, which is a count of how many observations from the
class is in the test set.
</p>
<p>A nested <code>tibble</code> with the <strong>predictions</strong> and targets.
</p>
<p>A <code>list</code> of <strong>ROC</strong> curve objects.
</p>
<p>A nested <code>tibble</code> with the multiclass <strong>confusion matrix</strong>.
</p>
<p>A nested <strong>Process</strong> information object with information
about the evaluation.
</p>
<p>Name of <strong>dependent</strong> variable.
</p>



<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>


<h3>See Also</h3>

<p>Other baseline functions: 
<code><a href="#topic+baseline_binomial">baseline_binomial</a>()</code>,
<code><a href="#topic+baseline_gaussian">baseline_gaussian</a>()</code>,
<code><a href="#topic+baseline_multinomial">baseline_multinomial</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Attach packages
library(cvms)
library(groupdata2) # partition()
library(dplyr) # %&gt;% arrange()
library(tibble)

# Data is part of cvms
data &lt;- participant.scores

# Set seed for reproducibility
set.seed(1)

# Partition data
partitions &lt;- partition(data, p = 0.7, list_out = TRUE)
train_set &lt;- partitions[[1]]
test_set &lt;- partitions[[2]]

# Create baseline evaluations
# Note: usually n=100 is a good setting

# Gaussian
baseline(
  test_data = test_set, train_data = train_set,
  dependent_col = "score", random_effects = "(1|session)",
  n = 2, family = "gaussian"
)

# Binomial
baseline(
  test_data = test_set, dependent_col = "diagnosis",
  n = 2, family = "binomial"
)

# Multinomial

# Create some data with multiple classes
multiclass_data &lt;- tibble(
  "target" = rep(paste0("class_", 1:5), each = 10)
) %&gt;%
  dplyr::sample_n(35)

baseline(
  test_data = multiclass_data,
  dependent_col = "target",
  n = 4, family = "multinomial"
)

# Parallelize evaluations

# Attach doParallel and register four cores
# Uncomment:
# library(doParallel)
# registerDoParallel(4)

# Binomial
baseline(
  test_data = test_set, dependent_col = "diagnosis",
  n = 4, family = "binomial"
  #, parallel = TRUE   # Uncomment
)

# Gaussian
baseline(
  test_data = test_set, train_data = train_set,
  dependent_col = "score", random_effects = "(1|session)",
  n = 4, family = "gaussian"
  #, parallel = TRUE   # Uncomment
)

# Multinomial
(mb &lt;- baseline(
  test_data = multiclass_data,
  dependent_col = "target",
  n = 6, family = "multinomial"
  #, parallel = TRUE   # Uncomment
))

# Inspect the summarized class level results
# for class_2
mb$summarized_class_level_results %&gt;%
  dplyr::filter(Class == "class_2") %&gt;%
  tidyr::unnest(Results)

# Multinomial with custom random generator function
# that creates very "certain" predictions
# (once softmax is applied)

rcertain &lt;- function(n) {
  (runif(n, min = 1, max = 100)^1.4) / 100
}

baseline(
  test_data = multiclass_data,
  dependent_col = "target",
  n = 6, family = "multinomial",
  random_generator_fn = rcertain
  #, parallel = TRUE  # Uncomment
)

</code></pre>

<hr>
<h2 id='baseline_binomial'>Create baseline evaluations for binary classification</h2><span id='topic+baseline_binomial'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#maturing"><img src="../help/figures/lifecycle-maturing.svg" alt='[Maturing]' /></a>
</p>
<p>Create a baseline evaluation of a test set.
</p>
<p>In modelling, a <em>baseline</em> is a result that
is meaningful to compare the results from our models to. For instance, in
classification, we usually want our results to be better than <em>random guessing</em>.
E.g. if we have three classes, we can expect an accuracy of <code>33.33%</code>, as for every
observation we have <code>1/3</code> chance of guessing the correct class. So our model should achieve
a higher accuracy than <code>33.33%</code> before it is more useful to us than guessing.
</p>
<p>While this expected value is often fairly straightforward to find analytically, it
only represents what we can expect on average. In reality, it's possible to get far better
results than that by guessing.
<strong><code>baseline_binomial()</code></strong>
finds the range of likely values by evaluating multiple sets
of random predictions and summarizing them with a set of useful descriptors. Additionally,
it evaluates a set of all <code>0</code> predictions and
a set of all <code>1</code> predictions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>baseline_binomial(
  test_data,
  dependent_col,
  n = 100,
  metrics = list(),
  positive = 2,
  cutoff = 0.5,
  parallel = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="baseline_binomial_+3A_test_data">test_data</code></td>
<td>
<p><code>data.frame</code>.</p>
</td></tr>
<tr><td><code id="baseline_binomial_+3A_dependent_col">dependent_col</code></td>
<td>
<p>Name of dependent variable in the supplied test and training sets.</p>
</td></tr>
<tr><td><code id="baseline_binomial_+3A_n">n</code></td>
<td>
<p>The number of sets of random predictions to evaluate. (Default is <code>100</code>)</p>
</td></tr>
<tr><td><code id="baseline_binomial_+3A_metrics">metrics</code></td>
<td>
<p><code>list</code> for enabling/disabling metrics.
</p>
<p>E.g. <code>list("F1" = FALSE)</code> would remove <code>F1</code> from the results,
and <code>list("Accuracy" = TRUE)</code> would add the regular <code>Accuracy</code> metric
to the results.
Default values (<code>TRUE</code>/<code>FALSE</code>) will be used for the remaining available metrics.
</p>
<p>You can enable/disable all metrics at once by including
<code>"all" = TRUE/FALSE</code> in the <code>list</code>. This is done prior to enabling/disabling
individual metrics, why f.i. <code>list("all" = FALSE, "Accuracy" = TRUE)</code>
would return only the <code>Accuracy</code> metric.
</p>
<p>The <code>list</code> can be created with
<code><a href="#topic+binomial_metrics">binomial_metrics()</a></code>.
</p>
<p>Also accepts the string <code>"all"</code>.</p>
</td></tr>
<tr><td><code id="baseline_binomial_+3A_positive">positive</code></td>
<td>
<p>Level from dependent variable to predict.
Either as character (<em>preferable</em>) or level index (<code>1</code> or <code>2</code> - alphabetically).
</p>
<p>E.g. if we have the levels <code>"cat"</code> and <code>"dog"</code> and we want <code>"dog"</code> to be the positive class,
we can either provide <code>"dog"</code> or <code>2</code>, as alphabetically, <code>"dog"</code> comes after <code>"cat"</code>.
</p>
<p><strong>Note:</strong> For <em>reproducibility</em>, it's preferable to <strong>specify the name directly</strong>, as
different <code><a href="base.html#topic+locales">locales</a></code> may sort the levels differently.
</p>
<p>Used when calculating confusion matrix metrics and creating <code>ROC</code> curves.
</p>
<p>N.B. Only affects evaluation metrics, not the returned predictions.</p>
</td></tr>
<tr><td><code id="baseline_binomial_+3A_cutoff">cutoff</code></td>
<td>
<p>Threshold for predicted classes. (Numeric)</p>
</td></tr>
<tr><td><code id="baseline_binomial_+3A_parallel">parallel</code></td>
<td>
<p>Whether to run the <code>`n`</code> evaluations in parallel. (Logical)
</p>
<p>Remember to register a parallel backend first.
E.g. with <code>doParallel::registerDoParallel</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Packages used:
</p>
<p><code>ROC</code> and <code>AUC</code>: <code><a href="pROC.html#topic+roc">pROC::roc</a></code>
</p>


<h3>Value</h3>

<p><code>list</code> containing:
</p>

<ol>
<li><p> a <code>tibble</code> with summarized results (called <code>summarized_metrics</code>)
</p>
</li>
<li><p> a <code>tibble</code> with random evaluations (<code>random_evaluations</code>)
</p>
</li></ol>

<p>....................................................................
</p>
<p>Based on the generated test set predictions,
a confusion matrix and <code>ROC</code> curve are used to get the following:
</p>
<p><code>ROC</code>:
</p>
<p><strong><code>AUC</code></strong>, <strong><code>Lower CI</code></strong>, and <strong><code>Upper CI</code></strong>
</p>
<p>Note, that the <code>ROC</code> curve is only computed when <code>AUC</code> is enabled.
</p>
<p><code>Confusion Matrix</code>:
</p>
<p><strong><code>Balanced Accuracy</code></strong>,
<strong><code>Accuracy</code></strong>,
<strong><code>F1</code></strong>,
<strong><code>Sensitivity</code></strong>, <strong><code>Specificity</code></strong>,
<strong><code>Positive Predictive Value</code></strong>,
<strong><code>Negative Predictive Value</code></strong>,
<strong><code>Kappa</code></strong>,
<strong><code>Detection Rate</code></strong>,
<strong><code>Detection Prevalence</code></strong>,
<strong><code>Prevalence</code></strong>, and
<strong><code>MCC</code></strong> (Matthews correlation coefficient).
</p>
<p>....................................................................
</p>
<p>The <strong>Summarized Results</strong> <code>tibble</code> contains:
</p>
<p>The <strong>Measure</strong> column indicates the statistical descriptor used on the evaluations.
The row where <code>Measure == All_0</code> is the evaluation when all predictions are <code>0</code>.
The row where <code>Measure == All_1</code> is the evaluation when all predictions are <code>1</code>.
</p>
<p>The <strong>aggregated metrics</strong>.
</p>
<p>....................................................................
</p>
<p>The <strong>Random Evaluations</strong> <code>tibble</code> contains:
</p>
<p>The <strong>non-aggregated metrics</strong>.
</p>
<p>A nested <code>tibble</code> with the <strong>predictions</strong> and targets.
</p>
<p>A <code>list</code> of <strong>ROC</strong> curve objects (if computed).
</p>
<p>A nested <code>tibble</code> with the <strong>confusion matrix</strong>.
The <code>Pos_</code> columns tells you whether a row is a
True Positive (<code>TP</code>), True Negative (<code>TN</code>), False Positive (<code>FP</code>),
or False Negative (<code>FN</code>), depending on which level is the &quot;positive&quot; class.
I.e. the level you wish to predict.
</p>
<p>A nested <strong>Process</strong> information object with information
about the evaluation.
</p>
<p>Name of <strong>dependent</strong> variable.
</p>


<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>


<h3>See Also</h3>

<p>Other baseline functions: 
<code><a href="#topic+baseline">baseline</a>()</code>,
<code><a href="#topic+baseline_gaussian">baseline_gaussian</a>()</code>,
<code><a href="#topic+baseline_multinomial">baseline_multinomial</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Attach packages
library(cvms)
library(groupdata2) # partition()
library(dplyr) # %&gt;% arrange()

# Data is part of cvms
data &lt;- participant.scores

# Set seed for reproducibility
set.seed(1)

# Partition data
partitions &lt;- partition(data, p = 0.7, list_out = TRUE)
train_set &lt;- partitions[[1]]
test_set &lt;- partitions[[2]]

# Create baseline evaluations
# Note: usually n=100 is a good setting

baseline_binomial(
  test_data = test_set,
  dependent_col = "diagnosis",
  n = 2
)

# Parallelize evaluations

# Attach doParallel and register four cores
# Uncomment:
# library(doParallel)
# registerDoParallel(4)

# Make sure to uncomment the parallel argument
baseline_binomial(
  test_data = test_set,
  dependent_col = "diagnosis",
  n = 4
  #, parallel = TRUE  # Uncomment
)

</code></pre>

<hr>
<h2 id='baseline_gaussian'>Create baseline evaluations for regression models</h2><span id='topic+baseline_gaussian'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#maturing"><img src="../help/figures/lifecycle-maturing.svg" alt='[Maturing]' /></a>
</p>
<p>Create a baseline evaluation of a test set.
</p>
<p>In modelling, a <em>baseline</em> is a result that
is meaningful to compare the results from our models to. In regression, we
want our model to be better than a model without any predictors. If our
model does not perform better than such a simple model, it's unlikely to
be useful.
</p>
<p><code>baseline_gaussian()</code> fits the intercept-only model (<code>y ~ 1</code>) on <code>`n`</code> random
subsets of <code>`train_data`</code> and evaluates each model on <code>`test_data`</code>. Additionally, it evaluates a
model fitted on all rows in <code>`train_data`</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>baseline_gaussian(
  test_data,
  train_data,
  dependent_col,
  n = 100,
  metrics = list(),
  random_effects = NULL,
  min_training_rows = 5,
  min_training_rows_left_out = 3,
  REML = FALSE,
  parallel = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="baseline_gaussian_+3A_test_data">test_data</code></td>
<td>
<p><code>data.frame</code>.</p>
</td></tr>
<tr><td><code id="baseline_gaussian_+3A_train_data">train_data</code></td>
<td>
<p><code>data.frame</code>.</p>
</td></tr>
<tr><td><code id="baseline_gaussian_+3A_dependent_col">dependent_col</code></td>
<td>
<p>Name of dependent variable in the supplied test and training sets.</p>
</td></tr>
<tr><td><code id="baseline_gaussian_+3A_n">n</code></td>
<td>
<p>The number of random samplings of <code>`train_data`</code> to fit baseline models on. (Default is <code>100</code>)</p>
</td></tr>
<tr><td><code id="baseline_gaussian_+3A_metrics">metrics</code></td>
<td>
<p><code>list</code> for enabling/disabling metrics.
</p>
<p>E.g. <code>list("RMSE" = FALSE)</code> would remove <code>RMSE</code> from the results,
and <code>list("TAE" = TRUE)</code> would add the <code>Total Absolute Error</code> metric
to the results.
Default values (<code>TRUE</code>/<code>FALSE</code>) will be used for the remaining available metrics.
</p>
<p>You can enable/disable all metrics at once by including
<code>"all" = TRUE/FALSE</code> in the <code>list</code>. This is done prior to enabling/disabling
individual metrics, why f.i. <code>list("all" = FALSE, "RMSE" = TRUE)</code>
would return only the <code>RMSE</code> metric.
</p>
<p>The <code>list</code> can be created with
<code><a href="#topic+gaussian_metrics">gaussian_metrics()</a></code>.
</p>
<p>Also accepts the string <code>"all"</code>.</p>
</td></tr>
<tr><td><code id="baseline_gaussian_+3A_random_effects">random_effects</code></td>
<td>
<p>Random effects structure for the baseline model. (Character)
</p>
<p>E.g. with <code>"(1|ID)"</code>, the model becomes <code>"y ~ 1 + (1|ID)"</code>.</p>
</td></tr>
<tr><td><code id="baseline_gaussian_+3A_min_training_rows">min_training_rows</code></td>
<td>
<p>Minimum number of rows in the random subsets of <code>`train_data`</code>.</p>
</td></tr>
<tr><td><code id="baseline_gaussian_+3A_min_training_rows_left_out">min_training_rows_left_out</code></td>
<td>
<p>Minimum number of rows left out of the random subsets of <code>`train_data`</code>.
</p>
<p>I.e. a subset will maximally have the size:
</p>
<p><code>max_rows_in_subset = nrow(`train_data`) - `min_training_rows_left_out`</code>.</p>
</td></tr>
<tr><td><code id="baseline_gaussian_+3A_reml">REML</code></td>
<td>
<p>Whether to use Restricted Maximum Likelihood. (Logical)</p>
</td></tr>
<tr><td><code id="baseline_gaussian_+3A_parallel">parallel</code></td>
<td>
<p>Whether to run the <code>`n`</code> evaluations in parallel. (Logical)
</p>
<p>Remember to register a parallel backend first.
E.g. with <code>doParallel::registerDoParallel</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Packages used:
</p>


<h4>Models</h4>

<p><code><a href="stats.html#topic+lm">stats::lm</a></code>, <code><a href="lme4.html#topic+lmer">lme4::lmer</a></code>
</p>



<h4>Results</h4>

<p>r2m : <code><a href="MuMIn.html#topic+r.squaredGLMM">MuMIn::r.squaredGLMM</a></code>
</p>
<p>r2c : <code><a href="MuMIn.html#topic+r.squaredGLMM">MuMIn::r.squaredGLMM</a></code>
</p>
<p>AIC : <code><a href="stats.html#topic+AIC">stats::AIC</a></code>
</p>
<p>AICc : <code><a href="MuMIn.html#topic+AICc">MuMIn::AICc</a></code>
</p>
<p>BIC : <code><a href="stats.html#topic+BIC">stats::BIC</a></code>
</p>



<h3>Value</h3>

<p><code>list</code> containing:
</p>

<ol>
<li><p> a <code>tibble</code> with summarized results (called <code>summarized_metrics</code>)
</p>
</li>
<li><p> a <code>tibble</code> with random evaluations (<code>random_evaluations</code>)
</p>
</li></ol>

<p>....................................................................
</p>
<p>The <strong>Summarized Results</strong> <code>tibble</code> contains:
</p>
<p>Average <strong><code>RMSE</code></strong>, <strong><code>MAE</code></strong>, <strong><code>NRMSE(IQR)</code></strong>,
<strong><code>RRSE</code></strong>, <strong><code>RAE</code></strong>, <strong><code>RMSLE</code></strong>.
</p>
<p>See the additional metrics (disabled by default) at <code><a href="#topic+gaussian_metrics">?gaussian_metrics</a></code>.
</p>
<p>The <strong>Measure</strong> column indicates the statistical descriptor used on the evaluations.
The row where <code>Measure == All_rows</code> is the evaluation when the baseline model
is trained on all rows in <code>`train_data`</code>.
</p>
<p>The <strong>Training Rows</strong> column contains the aggregated number of rows used from <code>`train_data`</code>,
when fitting the baseline models.
</p>
<p>....................................................................
</p>
<p>The <strong>Random Evaluations</strong> <code>tibble</code> contains:
</p>
<p>The <strong>non-aggregated metrics</strong>.
</p>
<p>A nested <code>tibble</code> with the <strong>predictions</strong> and targets.
</p>
<p>A nested <code>tibble</code> with the <strong>coefficients</strong> of the baseline models.
</p>
<p>Number of <strong>training rows</strong> used when fitting the baseline model on the training set.
</p>
<p>A nested <strong>Process</strong> information object with information
about the evaluation.
</p>
<p>Name of <strong>dependent</strong> variable.
</p>
<p>Name of <strong>fixed</strong> effect (bias term only).
</p>
<p><strong>Random</strong> effects structure (if specified).
</p>


<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>


<h3>See Also</h3>

<p>Other baseline functions: 
<code><a href="#topic+baseline">baseline</a>()</code>,
<code><a href="#topic+baseline_binomial">baseline_binomial</a>()</code>,
<code><a href="#topic+baseline_multinomial">baseline_multinomial</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Attach packages
library(cvms)
library(groupdata2) # partition()
library(dplyr) # %&gt;% arrange()

# Data is part of cvms
data &lt;- participant.scores

# Set seed for reproducibility
set.seed(1)

# Partition data
partitions &lt;- partition(data, p = 0.7, list_out = TRUE)
train_set &lt;- partitions[[1]]
test_set &lt;- partitions[[2]]

# Create baseline evaluations
# Note: usually n=100 is a good setting

baseline_gaussian(
  test_data = test_set,
  train_data = train_set,
  dependent_col = "score",
  random_effects = "(1|session)",
  n = 2
)

# Parallelize evaluations

# Attach doParallel and register four cores
# Uncomment:
# library(doParallel)
# registerDoParallel(4)

# Make sure to uncomment the parallel argument
baseline_gaussian(
  test_data = test_set,
  train_data = train_set,
  dependent_col = "score",
  random_effects = "(1|session)",
  n = 4
  #, parallel = TRUE  # Uncomment
)

</code></pre>

<hr>
<h2 id='baseline_multinomial'>Create baseline evaluations</h2><span id='topic+baseline_multinomial'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#maturing"><img src="../help/figures/lifecycle-maturing.svg" alt='[Maturing]' /></a>
</p>
<p>Create a baseline evaluation of a test set.
</p>
<p>In modelling, a <em>baseline</em> is a result that
is meaningful to compare the results from our models to. For instance, in
classification, we usually want our results to be better than <em>random guessing</em>.
E.g. if we have three classes, we can expect an accuracy of <code>33.33%</code>, as for every
observation we have <code>1/3</code> chance of guessing the correct class. So our model should achieve
a higher accuracy than <code>33.33%</code> before it is more useful to us than guessing.
</p>
<p>While this expected value is often fairly straightforward to find analytically, it
only represents what we can expect on average. In reality, it's possible to get far better
results than that by guessing.
<strong><code>baseline_multinomial()</code></strong>
finds the range of likely values by evaluating multiple sets
of random predictions and summarizing them with a set of useful descriptors.
</p>
<p>Technically, it creates <em>one-vs-all</em> (binomial) baseline evaluations
for the <code>`n`</code> sets of random predictions and summarizes them. Additionally,
sets of &quot;all class x,y,z,...&quot; predictions are evaluated.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>baseline_multinomial(
  test_data,
  dependent_col,
  n = 100,
  metrics = list(),
  random_generator_fn = runif,
  parallel = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="baseline_multinomial_+3A_test_data">test_data</code></td>
<td>
<p><code>data.frame</code>.</p>
</td></tr>
<tr><td><code id="baseline_multinomial_+3A_dependent_col">dependent_col</code></td>
<td>
<p>Name of dependent variable in the supplied test and training sets.</p>
</td></tr>
<tr><td><code id="baseline_multinomial_+3A_n">n</code></td>
<td>
<p>The number of sets of random predictions to evaluate. (Default is <code>100</code>)</p>
</td></tr>
<tr><td><code id="baseline_multinomial_+3A_metrics">metrics</code></td>
<td>
<p><code>list</code> for enabling/disabling metrics.
</p>
<p>E.g. <code>list("F1" = FALSE)</code> would remove <code>F1</code> from the results,
and <code>list("Accuracy" = TRUE)</code> would add the regular <code>Accuracy</code> metric
to the results.
Default values (<code>TRUE</code>/<code>FALSE</code>) will be used for the remaining available metrics.
</p>
<p>You can enable/disable all metrics at once by including
<code>"all" = TRUE/FALSE</code> in the <code>list</code>. This is done prior to enabling/disabling
individual metrics, why f.i. <code>list("all" = FALSE, "Accuracy" = TRUE)</code>
would return only the <code>Accuracy</code> metric.
</p>
<p>The <code>list</code> can be created with
<code><a href="#topic+multinomial_metrics">multinomial_metrics()</a></code>.
</p>
<p>Also accepts the string <code>"all"</code>.</p>
</td></tr>
<tr><td><code id="baseline_multinomial_+3A_random_generator_fn">random_generator_fn</code></td>
<td>
<p>Function for generating random numbers.
The <code>softmax</code> function is applied to the generated numbers to transform them to probabilities.
</p>
<p>The first argument must be the number of random numbers to generate,
as no other arguments are supplied.
</p>
<p>To test the effect of using different functions,
see <code><a href="#topic+multiclass_probability_tibble">multiclass_probability_tibble()</a></code>.</p>
</td></tr>
<tr><td><code id="baseline_multinomial_+3A_parallel">parallel</code></td>
<td>
<p>Whether to run the <code>`n`</code> evaluations in parallel. (Logical)
</p>
<p>Remember to register a parallel backend first.
E.g. with <code>doParallel::registerDoParallel</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Packages used:
</p>
<p>Multiclass <code>ROC</code> curve and <code>AUC</code>:
<code><a href="pROC.html#topic+multiclass.roc">pROC::multiclass.roc</a></code>
</p>


<h3>Value</h3>

<p><code>list</code> containing:
</p>

<ol>
<li><p> a <code>tibble</code> with summarized results (called <code>summarized_metrics</code>)
</p>
</li>
<li><p> a <code>tibble</code> with random evaluations (<code>random_evaluations</code>)
</p>
</li>
<li><p> a <code>tibble</code> with the summarized class level results
(<code>summarized_class_level_results</code>)
</p>
</li></ol>

<p>....................................................................
</p>


<h4>Macro metrics</h4>

<p>Based on the generated predictions,
<em>one-vs-all</em> (binomial) evaluations are performed and aggregated
to get the following <strong>macro</strong> metrics:
</p>
<p><strong><code>Balanced Accuracy</code></strong>,
<strong><code>F1</code></strong>,
<strong><code>Sensitivity</code></strong>,
<strong><code>Specificity</code></strong>,
<strong><code>Positive Predictive Value</code></strong>,
<strong><code>Negative Predictive Value</code></strong>,
<strong><code>Kappa</code></strong>,
<strong><code>Detection Rate</code></strong>,
<strong><code>Detection Prevalence</code></strong>, and
<strong><code>Prevalence</code></strong>.
</p>
<p>In general, the metrics mentioned in
<code><a href="#topic+binomial_metrics">binomial_metrics()</a></code>
can be enabled as macro metrics
(excluding <code>MCC</code>, <code>AUC</code>, <code>Lower CI</code>,
<code>Upper CI</code>, and the <code>AIC/AICc/BIC</code> metrics).
These metrics also has a weighted average
version.
</p>
<p><strong>N.B.</strong> we also refer to the <em>one-vs-all evaluations</em> as the <em>class level results</em>.
</p>



<h4>Multiclass metrics</h4>

<p>In addition, the <strong><code>Overall Accuracy</code></strong> and <em>multiclass</em>
<strong><code>MCC</code></strong> metrics are computed. <em>Multiclass</em> <code>AUC</code> can be enabled but
is slow to calculate with many classes.
</p>

<p>....................................................................
</p>
<p>The <strong>Summarized Results</strong> <code>tibble</code> contains:
</p>
<p>Summary of the random evaluations.
</p>
<p><strong>How</strong>: The one-vs-all binomial evaluations are aggregated by repetition and summarized. Besides the
metrics from the binomial evaluations, it
also includes <strong><code>Overall Accuracy</code></strong> and <em>multiclass</em> <strong><code>MCC</code></strong>.
</p>
<p>The <strong>Measure</strong> column indicates the statistical descriptor used on the evaluations.
The <strong>Mean</strong>, <strong>Median</strong>, <strong>SD</strong>, <strong>IQR</strong>, <strong>Max</strong>, <strong>Min</strong>,
<strong>NAs</strong>, and <strong>INFs</strong> measures describe the <em>Random Evaluations</em> <code>tibble</code>,
while the <strong>CL_Max</strong>, <strong>CL_Min</strong>, <strong>CL_NAs</strong>, and
<strong>CL_INFs</strong> describe the <strong>C</strong>lass <strong>L</strong>evel results.
</p>
<p>The rows where <code>Measure == All_&lt;&lt;class name&gt;&gt;</code> are the evaluations when all
the observations are predicted to be in that class.
</p>
<p>....................................................................
</p>
<p>The <strong>Summarized Class Level Results</strong> <code>tibble</code> contains:
</p>
<p>The (nested) summarized results for each class, with the same metrics and descriptors as
the <em>Summarized Results</em> <code>tibble</code>. Use <code><a href="tidyr.html#topic+unnest">tidyr::unnest</a></code>
on the <code>tibble</code> to inspect the results.
</p>
<p><strong>How</strong>: The one-vs-all evaluations are summarized by class.
</p>
<p>The rows where <code>Measure == All_0</code> are the evaluations when none of the observations
are predicted to be in that class, while the rows where <code>Measure == All_1</code> are the
evaluations when all of the observations are predicted to be in that class.
</p>
<p>....................................................................
</p>
<p>The <strong>Random Evaluations</strong> <code>tibble</code> contains:
</p>
<p>The repetition results with the same metrics as the <em>Summarized Results</em> <code>tibble</code>.
</p>
<p><strong>How</strong>: The one-vs-all evaluations are aggregated by repetition.
If a metric contains one or more <code>NAs</code> in the one-vs-all evaluations, it
will lead to an <code>NA</code> result for that repetition.
</p>
<p>Also includes:
</p>
<p>A nested <code>tibble</code> with the one-vs-all binomial evaluations (<strong>Class Level Results</strong>),
including nested <strong>Confusion Matrices</strong> and the
<strong>Support</strong> column, which is a count of how many observations from the
class is in the test set.
</p>
<p>A nested <code>tibble</code> with the <strong>predictions</strong> and targets.
</p>
<p>A <code>list</code> of <strong>ROC</strong> curve objects.
</p>
<p>A nested <code>tibble</code> with the multiclass <strong>confusion matrix</strong>.
</p>
<p>A nested <strong>Process</strong> information object with information
about the evaluation.
</p>
<p>Name of <strong>dependent</strong> variable.
</p>


<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>


<h3>See Also</h3>

<p>Other baseline functions: 
<code><a href="#topic+baseline">baseline</a>()</code>,
<code><a href="#topic+baseline_binomial">baseline_binomial</a>()</code>,
<code><a href="#topic+baseline_gaussian">baseline_gaussian</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Attach packages
library(cvms)
library(groupdata2) # partition()
library(dplyr) # %&gt;% arrange()
library(tibble)

# Data is part of cvms
data &lt;- participant.scores

# Set seed for reproducibility
set.seed(1)

# Partition data
partitions &lt;- partition(data, p = 0.7, list_out = TRUE)
train_set &lt;- partitions[[1]]
test_set &lt;- partitions[[2]]

# Create baseline evaluations
# Note: usually n=100 is a good setting

# Create some data with multiple classes
multiclass_data &lt;- tibble(
  "target" = rep(paste0("class_", 1:5), each = 10)
) %&gt;%
  dplyr::sample_n(35)

baseline_multinomial(
  test_data = multiclass_data,
  dependent_col = "target",
  n = 4
)

# Parallelize evaluations

# Attach doParallel and register four cores
# Uncomment:
# library(doParallel)
# registerDoParallel(4)

# Make sure to uncomment the parallel argument
(mb &lt;- baseline_multinomial(
  test_data = multiclass_data,
  dependent_col = "target",
  n = 6
  #, parallel = TRUE  # Uncomment
))

# Inspect the summarized class level results
# for class_2
mb$summarized_class_level_results %&gt;%
  dplyr::filter(Class == "class_2") %&gt;%
  tidyr::unnest(Results)

# Multinomial with custom random generator function
# that creates very "certain" predictions
# (once softmax is applied)

rcertain &lt;- function(n) {
  (runif(n, min = 1, max = 100)^1.4) / 100
}

# Make sure to uncomment the parallel argument
baseline_multinomial(
  test_data = multiclass_data,
  dependent_col = "target",
  n = 6,
  random_generator_fn = rcertain
  #, parallel = TRUE  # Uncomment
)

</code></pre>

<hr>
<h2 id='binomial_metrics'>Select metrics for binomial evaluation</h2><span id='topic+binomial_metrics'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#experimental"><img src="../help/figures/lifecycle-experimental.svg" alt='[Experimental]' /></a>
</p>
<p>Enable/disable metrics for binomial evaluation. Can be supplied to the
<code>`metrics`</code> argument in many of the <code>cvms</code> functions.
</p>
<p>Note: Some functions may have slightly different defaults than the ones supplied here.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>binomial_metrics(
  all = NULL,
  balanced_accuracy = NULL,
  accuracy = NULL,
  f1 = NULL,
  sensitivity = NULL,
  specificity = NULL,
  pos_pred_value = NULL,
  neg_pred_value = NULL,
  auc = NULL,
  lower_ci = NULL,
  upper_ci = NULL,
  kappa = NULL,
  mcc = NULL,
  detection_rate = NULL,
  detection_prevalence = NULL,
  prevalence = NULL,
  false_neg_rate = NULL,
  false_pos_rate = NULL,
  false_discovery_rate = NULL,
  false_omission_rate = NULL,
  threat_score = NULL,
  aic = NULL,
  aicc = NULL,
  bic = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="binomial_metrics_+3A_all">all</code></td>
<td>
<p>Enable/disable all arguments at once. (Logical)
</p>
<p>Specifying other metrics will overwrite this, why you can
use (<code>all = FALSE, accuracy = TRUE</code>) to get only the Accuracy metric.</p>
</td></tr>
<tr><td><code id="binomial_metrics_+3A_balanced_accuracy">balanced_accuracy</code></td>
<td>
<p><code>Balanced Accuracy</code> (Default: TRUE)</p>
</td></tr>
<tr><td><code id="binomial_metrics_+3A_accuracy">accuracy</code></td>
<td>
<p><code>Accuracy</code> (Default: FALSE)</p>
</td></tr>
<tr><td><code id="binomial_metrics_+3A_f1">f1</code></td>
<td>
<p><code>F1</code> (Default: TRUE)</p>
</td></tr>
<tr><td><code id="binomial_metrics_+3A_sensitivity">sensitivity</code></td>
<td>
<p><code>Sensitivity</code> (Default: TRUE)</p>
</td></tr>
<tr><td><code id="binomial_metrics_+3A_specificity">specificity</code></td>
<td>
<p><code>Specificity</code> (Default: TRUE)</p>
</td></tr>
<tr><td><code id="binomial_metrics_+3A_pos_pred_value">pos_pred_value</code></td>
<td>
<p><code>Pos Pred Value</code> (Default: TRUE)</p>
</td></tr>
<tr><td><code id="binomial_metrics_+3A_neg_pred_value">neg_pred_value</code></td>
<td>
<p><code>Neg Pred Value</code> (Default: TRUE)</p>
</td></tr>
<tr><td><code id="binomial_metrics_+3A_auc">auc</code></td>
<td>
<p><code>AUC</code> (Default: TRUE)</p>
</td></tr>
<tr><td><code id="binomial_metrics_+3A_lower_ci">lower_ci</code></td>
<td>
<p><code>Lower CI</code> (Default: TRUE)</p>
</td></tr>
<tr><td><code id="binomial_metrics_+3A_upper_ci">upper_ci</code></td>
<td>
<p><code>Upper CI</code> (Default: TRUE)</p>
</td></tr>
<tr><td><code id="binomial_metrics_+3A_kappa">kappa</code></td>
<td>
<p><code>Kappa</code> (Default: TRUE)</p>
</td></tr>
<tr><td><code id="binomial_metrics_+3A_mcc">mcc</code></td>
<td>
<p><code>MCC</code> (Default: TRUE)</p>
</td></tr>
<tr><td><code id="binomial_metrics_+3A_detection_rate">detection_rate</code></td>
<td>
<p><code>Detection Rate</code> (Default: TRUE)</p>
</td></tr>
<tr><td><code id="binomial_metrics_+3A_detection_prevalence">detection_prevalence</code></td>
<td>
<p><code>Detection Prevalence</code> (Default: TRUE)</p>
</td></tr>
<tr><td><code id="binomial_metrics_+3A_prevalence">prevalence</code></td>
<td>
<p><code>Prevalence</code> (Default: TRUE)</p>
</td></tr>
<tr><td><code id="binomial_metrics_+3A_false_neg_rate">false_neg_rate</code></td>
<td>
<p><code>False Neg Rate</code> (Default: FALSE)</p>
</td></tr>
<tr><td><code id="binomial_metrics_+3A_false_pos_rate">false_pos_rate</code></td>
<td>
<p><code>False Pos Rate</code> (Default: FALSE)</p>
</td></tr>
<tr><td><code id="binomial_metrics_+3A_false_discovery_rate">false_discovery_rate</code></td>
<td>
<p><code>False Discovery Rate</code> (Default: FALSE)</p>
</td></tr>
<tr><td><code id="binomial_metrics_+3A_false_omission_rate">false_omission_rate</code></td>
<td>
<p><code>False Omission Rate</code> (Default: FALSE)</p>
</td></tr>
<tr><td><code id="binomial_metrics_+3A_threat_score">threat_score</code></td>
<td>
<p><code>Threat Score</code> (Default: FALSE)</p>
</td></tr>
<tr><td><code id="binomial_metrics_+3A_aic">aic</code></td>
<td>
<p>AIC. (Default: FALSE)</p>
</td></tr>
<tr><td><code id="binomial_metrics_+3A_aicc">aicc</code></td>
<td>
<p>AICc. (Default: FALSE)</p>
</td></tr>
<tr><td><code id="binomial_metrics_+3A_bic">bic</code></td>
<td>
<p>BIC. (Default: FALSE)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>


<h3>See Also</h3>

<p>Other evaluation functions: 
<code><a href="#topic+confusion_matrix">confusion_matrix</a>()</code>,
<code><a href="#topic+evaluate">evaluate</a>()</code>,
<code><a href="#topic+evaluate_residuals">evaluate_residuals</a>()</code>,
<code><a href="#topic+gaussian_metrics">gaussian_metrics</a>()</code>,
<code><a href="#topic+multinomial_metrics">multinomial_metrics</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Attach packages
library(cvms)

# Enable only Balanced Accuracy
binomial_metrics(all = FALSE, balanced_accuracy = TRUE)

# Enable all but Balanced Accuracy
binomial_metrics(all = TRUE, balanced_accuracy = FALSE)

# Disable Balanced Accuracy
binomial_metrics(balanced_accuracy = FALSE)

</code></pre>

<hr>
<h2 id='combine_predictors'>Generate model formulas by combining predictors</h2><span id='topic+combine_predictors'></span><span id='topic+generate_formulas'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#maturing"><img src="../help/figures/lifecycle-maturing.svg" alt='[Maturing]' /></a>
</p>
<p>Create model formulas with every combination
of your fixed effects, along with the dependent variable and random effects.
<code>259,358</code> formulas have been precomputed with two- and three-way interactions
for up to <code>8</code> fixed effects, with up to <code>5</code> included effects per formula.
Uses the <code>+</code> and <code>*</code> operators, so lower order interactions are
automatically included.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>combine_predictors(
  dependent,
  fixed_effects,
  random_effects = NULL,
  max_fixed_effects = 5,
  max_interaction_size = 3,
  max_effect_frequency = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="combine_predictors_+3A_dependent">dependent</code></td>
<td>
<p>Name of dependent variable. (Character)</p>
</td></tr>
<tr><td><code id="combine_predictors_+3A_fixed_effects">fixed_effects</code></td>
<td>
<p><code>list</code> of fixed effects. (Character)
</p>
<p>Max. limit of <code>8</code> effects <strong>when interactions are included</strong>!
</p>
<p>A fixed effect name cannot contain: white spaces, <code>"*"</code> or <code>"+"</code>.
</p>
<p>Effects in sublists will be interchanged. This can be useful, when
we have multiple versions of a predictor (e.g. <code>x1</code> and <code>log(x1)</code>) that we
do not wish to have in the same formula.
</p>
<p>Example of interchangeable effects:
</p>
<p><code>list( list( "x1", "log_x1" ), "x2", "x3" )</code></p>
</td></tr>
<tr><td><code id="combine_predictors_+3A_random_effects">random_effects</code></td>
<td>
<p>The random effects structure. (Character)
</p>
<p>Is appended to the model formulas.</p>
</td></tr>
<tr><td><code id="combine_predictors_+3A_max_fixed_effects">max_fixed_effects</code></td>
<td>
<p>Maximum number of fixed effects in a model formula. (Integer)
</p>
<p>Max. limit of <code>5</code> <strong>when interactions are included</strong>!</p>
</td></tr>
<tr><td><code id="combine_predictors_+3A_max_interaction_size">max_interaction_size</code></td>
<td>
<p>Maximum number of effects in an interaction. (Integer)
</p>
<p>Max. limit of <code>3</code>.
</p>
<p>Use this to limit the <code>n</code>-way interactions allowed.
<code>0</code> or <code>1</code> excludes interactions all together.
</p>
<p>A model formula can contain multiple interactions.</p>
</td></tr>
<tr><td><code id="combine_predictors_+3A_max_effect_frequency">max_effect_frequency</code></td>
<td>
<p>Maximum number of times an effect is included in a formula string.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>list</code> of model formulas.
</p>
<p>E.g.:
</p>
<p><code>c("y ~ x1 + (1|z)", "y ~ x2 + (1|z)",
 "y ~ x1 + x2 + (1|z)", "y ~ x1 * x2 + (1|z)")</code>.
</p>


<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Attach packages
library(cvms)

# Create effect names
dependent &lt;- "y"
fixed_effects &lt;- c("a", "b", "c")
random_effects &lt;- "(1|e)"

# Create model formulas
combine_predictors(
  dependent, fixed_effects,
  random_effects
)

# Create effect names with interchangeable effects in sublists
fixed_effects &lt;- list("a", list("b", "log_b"), "c")

# Create model formulas
combine_predictors(
  dependent, fixed_effects,
  random_effects
)

</code></pre>

<hr>
<h2 id='compatible.formula.terms'>Compatible formula terms</h2><span id='topic+compatible.formula.terms'></span>

<h3>Description</h3>

<p><code>162,660</code> pairs of compatible terms for building model formulas with up to <code>15</code> fixed effects.
</p>


<h3>Format</h3>

<p>A <code>data.frame</code> with <code>162,660</code> rows and <code>5</code> variables:
</p>

<dl>
<dt>left</dt><dd><p>term, fixed effect or interaction, with fixed effects separated by &quot;<code>*</code>&quot;</p>
</dd>
<dt>right</dt><dd><p>term, fixed effect or interaction, with fixed effects separated by &quot;<code>*</code>&quot;</p>
</dd>
<dt>max_interaction_size</dt><dd><p>maximum interaction size in the two terms, up to <code>3</code></p>
</dd>
<dt>num_effects</dt><dd><p>number of unique fixed effects in the two terms, up to <code>5</code></p>
</dd>
<dt>min_num_fixed_effects</dt><dd><p>minimum number of fixed effects required to use a formula with the two terms,
i.e. the index in the alphabet of the last of the alphabetically ordered effects (letters) in the two terms,
so <code>4</code> if <code>left == "A"</code> and <code>right == "D"</code> </p>
</dd>
</dl>



<h3>Details</h3>

<p>A term is either a fixed effect or an interaction between fixed effects (up to three-way), where
the effects are separated by the &quot;<code>*</code>&quot; operator.
</p>
<p>Two terms are compatible if they are not redundant,
meaning that both add a fixed effect to the formula. E.g. as the interaction
<code>"x1 * x2 * x3"</code> expands to <code>"x1 + x2 + x3 + x1 * x2 + x1 * x3 + x2 * x3 + x1 * x2 * x3"</code>,
the higher order interaction makes these &quot;sub terms&quot; redundant. Note: All terms are compatible with <code>NA</code>.
</p>
<p>Effects are represented by the first fifteen capital letters.
</p>
<p>Used to generate the model formulas for <code><a href="#topic+combine_predictors">combine_predictors</a></code>.
</p>


<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>

<hr>
<h2 id='confusion_matrix'>Create a confusion matrix</h2><span id='topic+confusion_matrix'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#experimental"><img src="../help/figures/lifecycle-experimental.svg" alt='[Experimental]' /></a>
</p>
<p>Creates a confusion matrix from targets and predictions.
Calculates associated metrics.
</p>
<p>Multiclass results are based on one-vs-all evaluations.
Both regular averaging and weighted averaging are available. Also calculates the <code>Overall Accuracy</code>.
</p>
<p><strong>Note</strong>: In most cases you should use <code><a href="#topic+evaluate">evaluate()</a></code> instead. It has additional metrics and
works in <code>magrittr</code> pipes (e.g. <code>%&gt;%</code>) and with <code><a href="dplyr.html#topic+group_by">dplyr::group_by()</a></code>.
<code>confusion_matrix()</code> is more lightweight and may be preferred in programming when you don't need the extra stuff
in <code><a href="#topic+evaluate">evaluate()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>confusion_matrix(
  targets,
  predictions,
  metrics = list(),
  positive = 2,
  c_levels = NULL,
  do_one_vs_all = TRUE,
  parallel = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="confusion_matrix_+3A_targets">targets</code></td>
<td>
<p><code>vector</code> with true classes. Either <code>numeric</code> or <code>character</code>.</p>
</td></tr>
<tr><td><code id="confusion_matrix_+3A_predictions">predictions</code></td>
<td>
<p><code>vector</code> with predicted classes. Either <code>numeric</code> or <code>character</code>.</p>
</td></tr>
<tr><td><code id="confusion_matrix_+3A_metrics">metrics</code></td>
<td>
<p><code>list</code> for enabling/disabling metrics.
</p>
<p>E.g. <code>list("Accuracy" = TRUE)</code> would add the regular accuracy metric,
whie <code>list("F1" = FALSE)</code> would remove the <code>F1</code> metric.
Default values (TRUE/FALSE) will be used for the remaining available metrics.
</p>
<p>You can enable/disable all metrics at once by including
<code>"all" = TRUE/FALSE</code> in the <code>list</code>. This is done prior to enabling/disabling
individual metrics, why for instance <code>list("all" = FALSE, "Accuracy" = TRUE)</code>
would return only the <code>Accuracy</code> metric.
</p>
<p>The <code>list</code> can be created with
<code><a href="#topic+binomial_metrics">binomial_metrics()</a></code> or
<code><a href="#topic+multinomial_metrics">multinomial_metrics()</a></code>.
</p>
<p>Also accepts the string <code>"all"</code>.</p>
</td></tr>
<tr><td><code id="confusion_matrix_+3A_positive">positive</code></td>
<td>
<p>Level from <code>`targets`</code> to predict.
Either as character (<em>preferable</em>) or level index (<code>1</code> or <code>2</code> - alphabetically). (<strong>Two-class only</strong>)
</p>
<p>E.g. if we have the levels <code>"cat"</code> and <code>"dog"</code> and we want <code>"dog"</code> to be the positive class,
we can either provide <code>"dog"</code> or <code>2</code>, as alphabetically, <code>"dog"</code> comes after <code>"cat"</code>.
</p>
<p><strong>Note:</strong> For <em>reproducibility</em>, it's preferable to <strong>specify the name directly</strong>, as
different <code><a href="base.html#topic+locales">locales</a></code> may sort the levels differently.</p>
</td></tr>
<tr><td><code id="confusion_matrix_+3A_c_levels">c_levels</code></td>
<td>
<p><code>vector</code> with categorical levels in the targets. Should have same type as <code>`targets`</code>.
If <code>NULL</code>, they are inferred from <code>`targets`</code>.
</p>
<p>N.B. the levels are sorted alphabetically. When <code>`positive`</code> is numeric (i.e. an index),
it therefore still refers to the index of the alphabetically sorted levels.</p>
</td></tr>
<tr><td><code id="confusion_matrix_+3A_do_one_vs_all">do_one_vs_all</code></td>
<td>
<p>Whether to perform <em>one-vs-all</em> evaluations
when working with more than 2 classes (multiclass).
</p>
<p>If you are only interested in the confusion matrix,
this allows you to skip most of the metric calculations.</p>
</td></tr>
<tr><td><code id="confusion_matrix_+3A_parallel">parallel</code></td>
<td>
<p>Whether to perform the one-vs-all evaluations in parallel. (Logical)
</p>
<p>N.B. This only makes sense when you have a lot of classes or a very large dataset.
</p>
<p>Remember to register a parallel backend first.
E.g. with <code>doParallel::registerDoParallel</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The following formulas are used for calculating the metrics:
</p>
<p><code>Sensitivity = TP / (TP + FN)</code>
</p>
<p><code>Specificity = TN / (TN + FP)</code>
</p>
<p><code>Pos Pred Value = TP / (TP + FP)</code>
</p>
<p><code>Neg Pred Value = TN / (TN + FN)</code>
</p>
<p><code>Balanced Accuracy = (Sensitivity + Specificity) / 2</code>
</p>
<p><code>Accuracy = (TP + TN) / (TP + TN + FP + FN)</code>
</p>
<p><code>Overall Accuracy = Correct / (Correct + Incorrect)</code>
</p>
<p><code>F1 = 2 * Pos Pred Value * Sensitivity / (Pos Pred Value + Sensitivity)</code>
</p>
<p><code>MCC = ((TP * TN) - (FP * FN)) / sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))</code>
</p>
<p>Note for <code>MCC</code>: Formula is for the <em>binary</em> case. When the denominator is <code>0</code>,
we set it to <code>1</code> to avoid <code>NaN</code>.
See the <code>metrics</code> vignette for the multiclass version.
</p>
<p><code>Detection Rate = TP / (TP + FN + TN + FP)</code>
</p>
<p><code>Detection Prevalence = (TP + FP) / (TP + FN + TN + FP)</code>
</p>
<p><code>Threat Score = TP / (TP + FN + FP)</code>
</p>
<p><code>False Neg Rate = 1 - Sensitivity</code>
</p>
<p><code>False Pos Rate = 1 - Specificity</code>
</p>
<p><code>False Discovery Rate = 1 - Pos Pred Value</code>
</p>
<p><code>False Omission Rate = 1 - Neg Pred Value</code>
</p>
<p>For <strong>Kappa</strong> the counts (<code>TP</code>, <code>TN</code>, <code>FP</code>, <code>FN</code>) are normalized to percentages (summing to 1).
Then the following is calculated:
</p>
<p><code>p_observed = TP + TN</code>
</p>
<p><code>p_expected = (TN + FP) * (TN + FN) + (FN + TP) * (FP + TP)</code>
</p>
<p><code>Kappa = (p_observed - p_expected) / (1 - p_expected)</code>
</p>


<h3>Value</h3>

<p><code>tibble</code> with:
</p>
<p>Nested <strong>confusion matrix</strong> (tidied version)
</p>
<p>Nested confusion matrix (<strong>table</strong>)
</p>
<p>The <strong>Positive Class</strong>.
</p>
<p>Multiclass only: Nested <strong>Class Level Results</strong> with the two-class metrics,
the nested confusion matrices, and the <strong>Support</strong> metric, which is a
count of the class in the target column and is used for the weighted average metrics.
</p>
<p>The following metrics are available (see <code>`metrics`</code>):
</p>


<h4>Two classes or more</h4>


<table>
<tr>
 <td style="text-align: right;">
  <strong>Metric</strong> </td><td style="text-align: right;"> <strong>Name</strong> </td><td style="text-align: right;"> <strong>Default</strong> </td>
</tr>
<tr>
 <td style="text-align: right;">
  Balanced Accuracy </td><td style="text-align: right;"> "Balanced Accuracy" </td><td style="text-align: right;"> Enabled </td>
</tr>
<tr>
 <td style="text-align: right;">
  Accuracy </td><td style="text-align: right;"> "Accuracy" </td><td style="text-align: right;"> Disabled </td>
</tr>
<tr>
 <td style="text-align: right;">
  F1 </td><td style="text-align: right;"> "F1" </td><td style="text-align: right;"> Enabled </td>
</tr>
<tr>
 <td style="text-align: right;">
  Sensitivity </td><td style="text-align: right;"> "Sensitivity" </td><td style="text-align: right;"> Enabled </td>
</tr>
<tr>
 <td style="text-align: right;">
  Specificity </td><td style="text-align: right;"> "Specificity" </td><td style="text-align: right;"> Enabled </td>
</tr>
<tr>
 <td style="text-align: right;">
  Positive Predictive Value </td><td style="text-align: right;"> "Pos Pred Value" </td><td style="text-align: right;"> Enabled </td>
</tr>
<tr>
 <td style="text-align: right;">
  Negative Predictive Value </td><td style="text-align: right;"> "Neg Pred Value" </td><td style="text-align: right;"> Enabled </td>
</tr>
<tr>
 <td style="text-align: right;">
  Kappa </td><td style="text-align: right;"> "Kappa" </td><td style="text-align: right;"> Enabled </td>
</tr>
<tr>
 <td style="text-align: right;">
  Matthews Correlation Coefficient </td><td style="text-align: right;"> "MCC" </td><td style="text-align: right;"> Enabled </td>
</tr>
<tr>
 <td style="text-align: right;">
  Detection Rate </td><td style="text-align: right;"> "Detection Rate" </td><td style="text-align: right;"> Enabled </td>
</tr>
<tr>
 <td style="text-align: right;">
  Detection Prevalence </td><td style="text-align: right;"> "Detection Prevalence" </td><td style="text-align: right;"> Enabled </td>
</tr>
<tr>
 <td style="text-align: right;">
  Prevalence </td><td style="text-align: right;"> "Prevalence" </td><td style="text-align: right;"> Enabled </td>
</tr>
<tr>
 <td style="text-align: right;">
  False Negative Rate </td><td style="text-align: right;"> "False Neg Rate" </td><td style="text-align: right;"> Disabled </td>
</tr>
<tr>
 <td style="text-align: right;">
  False Positive Rate </td><td style="text-align: right;"> "False Pos Rate" </td><td style="text-align: right;"> Disabled </td>
</tr>
<tr>
 <td style="text-align: right;">
  False Discovery Rate </td><td style="text-align: right;"> "False Discovery Rate" </td><td style="text-align: right;"> Disabled </td>
</tr>
<tr>
 <td style="text-align: right;">
  False Omission Rate </td><td style="text-align: right;"> "False Omission Rate" </td><td style="text-align: right;"> Disabled </td>
</tr>
<tr>
 <td style="text-align: right;">
  Threat Score </td><td style="text-align: right;"> "Threat Score" </td><td style="text-align: right;"> Disabled </td>
</tr>
<tr>
 <td style="text-align: right;">
 </td>
</tr>

</table>

<p>The <strong>Name</strong> column refers to the name used in the package.
This is the name in the output and when enabling/disabling in <code>`metrics`</code>.
</p>



<h4>Three classes or more</h4>

<p>The metrics mentioned above (excluding <code>MCC</code>)
has a weighted average version (disabled by default; weighted by the <strong>Support</strong>).
</p>
<p>In order to enable a weighted metric, prefix the metric name with <code>"Weighted "</code> when specifying <code>`metrics`</code>.
</p>
<p>E.g. <code>metrics = list("Weighted Accuracy" = TRUE)</code>.
</p>

<table>
<tr>
 <td style="text-align: right;">
  <strong>Metric</strong> </td><td style="text-align: right;"> <strong>Name</strong> </td><td style="text-align: right;"> <strong>Default</strong> </td>
</tr>
<tr>
 <td style="text-align: right;">
  Overall Accuracy </td><td style="text-align: right;"> "Overall Accuracy" </td><td style="text-align: right;"> Enabled </td>
</tr>
<tr>
 <td style="text-align: right;">
  Weighted * </td><td style="text-align: right;"> "Weighted *" </td><td style="text-align: right;"> Disabled </td>
</tr>
<tr>
 <td style="text-align: right;">
  Multiclass MCC </td><td style="text-align: right;"> "MCC" </td><td style="text-align: right;"> Enabled </td>
</tr>
<tr>
 <td style="text-align: right;">
 </td>
</tr>

</table>




<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>


<h3>See Also</h3>

<p>Other evaluation functions: 
<code><a href="#topic+binomial_metrics">binomial_metrics</a>()</code>,
<code><a href="#topic+evaluate">evaluate</a>()</code>,
<code><a href="#topic+evaluate_residuals">evaluate_residuals</a>()</code>,
<code><a href="#topic+gaussian_metrics">gaussian_metrics</a>()</code>,
<code><a href="#topic+multinomial_metrics">multinomial_metrics</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Attach cvms
library(cvms)

# Two classes

# Create targets and predictions
targets &lt;- c(0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1)
predictions &lt;- c(1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0)

# Create confusion matrix with default metrics
cm &lt;- confusion_matrix(targets, predictions)
cm
cm[["Confusion Matrix"]]
cm[["Table"]]

# Three classes

# Create targets and predictions
targets &lt;- c(0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0)
predictions &lt;- c(2, 1, 0, 2, 0, 1, 1, 2, 0, 1, 2, 0, 2)

# Create confusion matrix with default metrics
cm &lt;- confusion_matrix(targets, predictions)
cm
cm[["Confusion Matrix"]]
cm[["Table"]]

# Enabling weighted accuracy

# Create confusion matrix with Weighted Accuracy enabled
cm &lt;- confusion_matrix(targets, predictions,
  metrics = list("Weighted Accuracy" = TRUE)
)
cm

</code></pre>

<hr>
<h2 id='cross_validate'>Cross-validate regression models for model selection</h2><span id='topic+cross_validate'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#stable"><img src="../help/figures/lifecycle-stable.svg" alt='[Stable]' /></a>
</p>
<p>Cross-validate one or multiple linear or logistic regression
models at once. Perform repeated cross-validation.
Returns results in a <code>tibble</code> for easy comparison,
reporting and further analysis.
</p>
<p>See <code><a href="#topic+cross_validate_fn">cross_validate_fn()</a></code> for use
with custom model functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cross_validate(
  data,
  formulas,
  family,
  fold_cols = ".folds",
  control = NULL,
  REML = FALSE,
  cutoff = 0.5,
  positive = 2,
  metrics = list(),
  preprocessing = NULL,
  rm_nc = FALSE,
  parallel = FALSE,
  verbose = FALSE,
  link = deprecated(),
  models = deprecated(),
  model_verbose = deprecated()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cross_validate_+3A_data">data</code></td>
<td>
<p><code>data.frame</code>.
</p>
<p>Must include one or more grouping factors for identifying folds
- as made with <code><a href="groupdata2.html#topic+fold">groupdata2::fold()</a></code>.</p>
</td></tr>
<tr><td><code id="cross_validate_+3A_formulas">formulas</code></td>
<td>
<p>Model formulas as strings. (Character)
</p>
<p>E.g. <code>c("y~x", "y~z")</code>.
</p>
<p>Can contain random effects.
</p>
<p>E.g. <code>c("y~x+(1|r)", "y~z+(1|r)")</code>.</p>
</td></tr>
<tr><td><code id="cross_validate_+3A_family">family</code></td>
<td>
<p>Name of the family. (Character)
</p>
<p>Currently supports <strong><code>"gaussian"</code></strong> for linear regression
with <code><a href="stats.html#topic+lm">lm()</a></code> / <code><a href="lme4.html#topic+lmer">lme4::lmer()</a></code>
and <strong><code>"binomial"</code></strong> for binary classification
with <code><a href="stats.html#topic+glm">glm()</a></code> / <code><a href="lme4.html#topic+glmer">lme4::glmer()</a></code>.
</p>
<p>See <code><a href="#topic+cross_validate_fn">cross_validate_fn()</a></code> for use with other model functions.</p>
</td></tr>
<tr><td><code id="cross_validate_+3A_fold_cols">fold_cols</code></td>
<td>
<p>Name(s) of grouping factor(s) for identifying folds. (Character)
</p>
<p>Include names of multiple grouping factors for repeated cross-validation.</p>
</td></tr>
<tr><td><code id="cross_validate_+3A_control">control</code></td>
<td>
<p>Construct control structures for mixed model fitting
(with <code><a href="lme4.html#topic+lmer">lme4::lmer()</a></code> or <code><a href="lme4.html#topic+glmer">lme4::glmer()</a></code>).
See <code><a href="lme4.html#topic+lmerControl">lme4::lmerControl</a></code> and
<code><a href="lme4.html#topic+glmerControl">lme4::glmerControl</a></code>.
</p>
<p>N.B. Ignored if fitting <code><a href="stats.html#topic+lm">lm()</a></code> or <code><a href="stats.html#topic+glm">glm()</a></code> models.</p>
</td></tr>
<tr><td><code id="cross_validate_+3A_reml">REML</code></td>
<td>
<p>Restricted Maximum Likelihood. (Logical)</p>
</td></tr>
<tr><td><code id="cross_validate_+3A_cutoff">cutoff</code></td>
<td>
<p>Threshold for predicted classes. (Numeric)
</p>
<p>N.B. <strong>Binomial models only</strong></p>
</td></tr>
<tr><td><code id="cross_validate_+3A_positive">positive</code></td>
<td>
<p>Level from dependent variable to predict.
Either as character (<em>preferable</em>) or level index (<code>1</code> or <code>2</code> - alphabetically).
</p>
<p>E.g. if we have the levels <code>"cat"</code> and <code>"dog"</code> and we want <code>"dog"</code> to be the positive class,
we can either provide <code>"dog"</code> or <code>2</code>, as alphabetically, <code>"dog"</code> comes after <code>"cat"</code>.
</p>
<p><strong>Note:</strong> For <em>reproducibility</em>, it's preferable to <strong>specify the name directly</strong>, as
different <code><a href="base.html#topic+locales">locales</a></code> may sort the levels differently.
</p>
<p>Used when calculating confusion matrix metrics and creating <code>ROC</code> curves.
</p>
<p>The <code>Process</code> column in the output can be used to verify this setting.
</p>
<p>N.B. Only affects evaluation metrics, not the model training or returned predictions.
</p>
<p>N.B. <strong>Binomial models only</strong>.</p>
</td></tr>
<tr><td><code id="cross_validate_+3A_metrics">metrics</code></td>
<td>
<p><code>list</code> for enabling/disabling metrics.
</p>
<p>E.g. <code>list("RMSE" = FALSE)</code> would remove <code>RMSE</code> from the results,
and <code>list("Accuracy" = TRUE)</code> would add the regular <code>Accuracy</code> metric
to the classification results.
Default values (<code>TRUE</code>/<code>FALSE</code>) will be used for the remaining available metrics.
</p>
<p>You can enable/disable all metrics at once by including
<code>"all" = TRUE/FALSE</code> in the <code>list</code>. This is done prior to enabling/disabling
individual metrics, why <code>list("all" = FALSE, "RMSE" = TRUE)</code>
would return only the <code>RMSE</code> metric.
</p>
<p>The <code>list</code> can be created with
<code><a href="#topic+gaussian_metrics">gaussian_metrics()</a></code> or
<code><a href="#topic+binomial_metrics">binomial_metrics()</a></code>.
</p>
<p>Also accepts the string <code>"all"</code>.</p>
</td></tr>
<tr><td><code id="cross_validate_+3A_preprocessing">preprocessing</code></td>
<td>
<p>Name of preprocessing to apply.
</p>
<p>Available preprocessings are:
</p>

<table>
<tr>
 <td style="text-align: right;">
  <strong>Name</strong> </td><td style="text-align: right;"> <strong>Description</strong> </td>
</tr>
<tr>
 <td style="text-align: right;">
  "standardize" </td><td style="text-align: right;"> Centers and scales the numeric predictors.</td>
</tr>
<tr>
 <td style="text-align: right;">
  "range" </td><td style="text-align: right;"> Normalizes the numeric predictors to the <code>0</code>-<code>1</code> range.
  Values outside the min/max range in the test fold are truncated to <code>0</code>/<code>1</code>.</td>
</tr>
<tr>
 <td style="text-align: right;">
  "scale" </td><td style="text-align: right;"> Scales the numeric predictors to have a standard deviation of one.</td>
</tr>
<tr>
 <td style="text-align: right;">
  "center" </td><td style="text-align: right;"> Centers the numeric predictors to have a mean of zero.</td>
</tr>
<tr>
 <td style="text-align: right;">
 </td>
</tr>

</table>

<p>The preprocessing parameters (<code>mean</code>, <code>SD</code>, etc.) are extracted from the training folds and
applied to both the training folds and the test fold.
They are returned in the <strong>Preprocess</strong> column for inspection.
</p>
<p>N.B. The preprocessings should not affect the results
to a noticeable degree, although <code>"range"</code> might due to the truncation.</p>
</td></tr>
<tr><td><code id="cross_validate_+3A_rm_nc">rm_nc</code></td>
<td>
<p>Remove non-converged models from output. (Logical)</p>
</td></tr>
<tr><td><code id="cross_validate_+3A_parallel">parallel</code></td>
<td>
<p>Whether to cross-validate the <code>list</code> of models in parallel. (Logical)
</p>
<p>Remember to register a parallel backend first.
E.g. with <code>doParallel::registerDoParallel</code>.</p>
</td></tr>
<tr><td><code id="cross_validate_+3A_verbose">verbose</code></td>
<td>
<p>Whether to message process information
like the number of model instances to fit and which model function was applied. (Logical)</p>
</td></tr>
<tr><td><code id="cross_validate_+3A_link">link</code>, <code id="cross_validate_+3A_models">models</code>, <code id="cross_validate_+3A_model_verbose">model_verbose</code></td>
<td>
<p>Deprecated.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Packages used:
</p>


<h4>Models</h4>

<p>Gaussian: <code><a href="stats.html#topic+lm">stats::lm</a></code>, <code><a href="lme4.html#topic+lmer">lme4::lmer</a></code>
</p>
<p>Binomial: <code><a href="stats.html#topic+glm">stats::glm</a></code>, <code><a href="lme4.html#topic+glmer">lme4::glmer</a></code>
</p>



<h4>Results</h4>



<h5>Shared</h5>

<p><code>AIC</code> : <code><a href="stats.html#topic+AIC">stats::AIC</a></code>
</p>
<p><code>AICc</code> : <code><a href="MuMIn.html#topic+AICc">MuMIn::AICc</a></code>
</p>
<p><code>BIC</code> : <code><a href="stats.html#topic+BIC">stats::BIC</a></code>
</p>



<h5>Gaussian</h5>

<p><code>r2m</code> : <code><a href="MuMIn.html#topic+r.squaredGLMM">MuMIn::r.squaredGLMM</a></code>
</p>
<p><code>r2c</code> : <code><a href="MuMIn.html#topic+r.squaredGLMM">MuMIn::r.squaredGLMM</a></code>
</p>



<h5>Binomial</h5>

<p><code>ROC and AUC</code>: <code><a href="pROC.html#topic+roc">pROC::roc</a></code>
</p>




<h3>Value</h3>

<p><code>tibble</code> with results for each model.
</p>


<h4>Shared across families</h4>

<p>A nested <code>tibble</code> with <strong>coefficients</strong> of the models from all iterations.
</p>
<p>Number of <em>total</em> <strong>folds</strong>.
</p>
<p>Number of <strong>fold columns</strong>.
</p>
<p>Count of <strong>convergence warnings</strong>. Consider discarding models that did not converge on all
iterations. Note: you might still see results, but these should be taken with a grain of salt!
</p>
<p>Count of <strong>other warnings</strong>. These are warnings without keywords such as &quot;convergence&quot;.
</p>
<p>Count of <strong>Singular Fit messages</strong>.
See <code><a href="lme4.html#topic+isSingular">lme4::isSingular</a></code> for more information.
</p>
<p>Nested <code>tibble</code> with the <strong>warnings and messages</strong> caught for each model.
</p>
<p>A nested <strong>Process</strong> information object with information
about the evaluation.
</p>
<p>Name of <strong>dependent</strong> variable.
</p>
<p>Names of <strong>fixed</strong> effects.
</p>
<p>Names of <strong>random</strong> effects, if any.
</p>
<p>Nested <code>tibble</code> with <strong>preprocess</strong>ing parameters, if any.
</p>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>


<h4>Gaussian Results</h4>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>
<p>Average <strong><code>RMSE</code></strong>, <strong><code>MAE</code></strong>, <strong><code>NRMSE(IQR)</code></strong>,
<strong><code>RRSE</code></strong>, <strong><code>RAE</code></strong>, <strong><code>RMSLE</code></strong>,
<strong><code>AIC</code></strong>, <strong><code>AICc</code></strong>,
and <strong><code>BIC</code></strong> of all the iterations*,
<em><strong>omitting potential NAs</strong> from non-converged iterations</em>.
Note that the Information Criterion metrics (<code>AIC</code>, <code>AICc</code>, and <code>BIC</code>) are also averages.
</p>
<p>See the additional metrics (disabled by default) at <code><a href="#topic+gaussian_metrics">?gaussian_metrics</a></code>.
</p>
<p>A nested <code>tibble</code> with the <strong>predictions</strong> and targets.
</p>
<p>A nested <code>tibble</code> with the non-averaged <strong>results</strong> from all iterations.
</p>
<p>* In <em>repeated cross-validation</em>,
the metrics are first averaged for each fold column (repetition) and then averaged again.
</p>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>


<h4>Binomial Results</h4>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>
<p>Based on the <strong>collected</strong> predictions from the test folds*,
a confusion matrix and a <code>ROC</code> curve are created to get the following:
</p>
<p><code>ROC</code>:
</p>
<p><strong><code>AUC</code></strong>, <strong><code>Lower CI</code></strong>, and <strong><code>Upper CI</code></strong>
</p>
<p><code>Confusion Matrix</code>:
</p>
<p><strong><code>Balanced Accuracy</code></strong>,
<strong><code>F1</code></strong>,
<strong><code>Sensitivity</code></strong>,
<strong><code>Specificity</code></strong>,
<strong><code>Positive Predictive Value</code></strong>,
<strong><code>Negative Predictive Value</code></strong>,
<strong><code>Kappa</code></strong>,
<strong><code>Detection Rate</code></strong>,
<strong><code>Detection Prevalence</code></strong>,
<strong><code>Prevalence</code></strong>, and
<strong><code>MCC</code></strong> (Matthews correlation coefficient).
</p>
<p>See the additional metrics (disabled by default) at
<code><a href="#topic+binomial_metrics">?binomial_metrics</a></code>.
</p>
<p>Also includes:
</p>
<p>A nested <code>tibble</code> with <strong>predictions</strong>, predicted classes (depends on <code>cutoff</code>), and the targets.
Note, that the predictions are <em>not necessarily</em> of the <em>specified</em> <code>positive</code> class, but of
the <em>model's</em> positive class (second level of dependent variable, alphabetically).
</p>
<p>The <code><a href="pROC.html#topic+roc">pROC::roc</a></code> <strong><code>ROC</code></strong> curve object(s).
</p>
<p>A nested <code>tibble</code> with the <strong>confusion matrix</strong>/matrices.
The <code>Pos_</code> columns tells you whether a row is a
True Positive (<code>TP</code>), True Negative (<code>TN</code>),
False Positive (<code>FP</code>), or False Negative (<code>FN</code>),
depending on which level is the &quot;positive&quot; class. I.e. the level you wish to predict.
</p>
<p>A nested <code>tibble</code> with the <strong>results</strong> from all fold columns.
</p>
<p>The name of the <strong>Positive Class</strong>.
</p>
<p>* In <em>repeated cross-validation</em>, an evaluation is made per fold column (repetition) and averaged.
</p>



<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>
<p>Benjamin Hugh Zachariae
</p>


<h3>See Also</h3>

<p>Other validation functions: 
<code><a href="#topic+cross_validate_fn">cross_validate_fn</a>()</code>,
<code><a href="#topic+validate">validate</a>()</code>,
<code><a href="#topic+validate_fn">validate_fn</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Attach packages
library(cvms)
library(groupdata2) # fold()
library(dplyr) # %&gt;% arrange()

# Data is part of cvms
data &lt;- participant.scores

# Set seed for reproducibility
set.seed(7)

# Fold data
data &lt;- fold(
  data,
  k = 4,
  cat_col = "diagnosis",
  id_col = "participant"
) %&gt;%
  arrange(.folds)

#
# Cross-validate a single model
#

# Gaussian
cross_validate(
  data,
  formulas = "score~diagnosis",
  family = "gaussian",
  REML = FALSE
)

# Binomial
cross_validate(
  data,
  formulas = "diagnosis~score",
  family = "binomial"
)

#
# Cross-validate multiple models
#

formulas &lt;- c(
  "score~diagnosis+(1|session)",
  "score~age+(1|session)"
)

cross_validate(
  data,
  formulas = formulas,
  family = "gaussian",
  REML = FALSE
)

#
# Use parallelization
#

# Attach doParallel and register four cores
# Uncomment:
# library(doParallel)
# registerDoParallel(4)

# Cross-validate a list of model formulas in parallel
# Make sure to uncomment the parallel argument
cross_validate(
  data,
  formulas = formulas,
  family = "gaussian"
  #, parallel = TRUE  # Uncomment
)

</code></pre>

<hr>
<h2 id='cross_validate_fn'>Cross-validate custom model functions for model selection</h2><span id='topic+cross_validate_fn'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#experimental"><img src="../help/figures/lifecycle-experimental.svg" alt='[Experimental]' /></a>
</p>
<p>Cross-validate your model function with one or multiple model formulas at once.
Perform repeated cross-validation. Preprocess the train/test split
within the cross-validation. Perform hyperparameter tuning with grid search.
Returns results in a <code>tibble</code> for easy comparison,
reporting and further analysis.
</p>
<p>Compared to <code><a href="#topic+cross_validate">cross_validate()</a></code>,
this function allows you supply a custom model function, a predict function,
a preprocess function and the hyperparameter values to cross-validate.
</p>
<p>Supports regression and classification (binary and multiclass).
See <code>`type`</code>.
</p>
<p>Note that some metrics may not be computable for some types
of model objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cross_validate_fn(
  data,
  formulas,
  type,
  model_fn,
  predict_fn,
  preprocess_fn = NULL,
  preprocess_once = FALSE,
  hyperparameters = NULL,
  fold_cols = ".folds",
  cutoff = 0.5,
  positive = 2,
  metrics = list(),
  rm_nc = FALSE,
  parallel = FALSE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cross_validate_fn_+3A_data">data</code></td>
<td>
<p><code>data.frame</code>.
</p>
<p>Must include one or more grouping factors for identifying folds
- as made with <code><a href="groupdata2.html#topic+fold">groupdata2::fold()</a></code>.</p>
</td></tr>
<tr><td><code id="cross_validate_fn_+3A_formulas">formulas</code></td>
<td>
<p>Model formulas as strings. (Character)
</p>
<p>Will be converted to <code><a href="stats.html#topic+formula">formula</a></code> objects
before being passed to <code>`model_fn`</code>.
</p>
<p>E.g. <code>c("y~x", "y~z")</code>.
</p>
<p>Can contain random effects.
</p>
<p>E.g. <code>c("y~x+(1|r)", "y~z+(1|r)")</code>.</p>
</td></tr>
<tr><td><code id="cross_validate_fn_+3A_type">type</code></td>
<td>
<p>Type of evaluation to perform:
</p>
<p><code>"gaussian"</code> for regression (like linear regression).
</p>
<p><code>"binomial"</code> for binary classification.
</p>
<p><code>"multinomial"</code> for multiclass classification.</p>
</td></tr>
<tr><td><code id="cross_validate_fn_+3A_model_fn">model_fn</code></td>
<td>
<p>Model function that returns a fitted model object.
Will usually wrap an existing model function like <code><a href="e1071.html#topic+svm">e1071::svm</a></code>
or <code><a href="nnet.html#topic+multinom">nnet::multinom</a></code>.
</p>
<p>Must have the following function arguments:
</p>
<p><code>function(train_data, formula,</code>
</p>
<p><code style="white-space: pre;">&#8288;         &#8288;</code><code>hyperparameters)</code></p>
</td></tr>
<tr><td><code id="cross_validate_fn_+3A_predict_fn">predict_fn</code></td>
<td>
<p>Function for predicting the targets in the test folds/sets using the fitted model object.
Will usually wrap <code><a href="stats.html#topic+predict">stats::predict()</a></code>, but doesn't have to.
</p>
<p>Must have the following function arguments:
</p>
<p><code>function(test_data, model, formula,</code>
</p>
<p><code style="white-space: pre;">&#8288;         &#8288;</code><code>hyperparameters, train_data)</code>
</p>
<p>Must return predictions in the following formats, depending on <code>`type`</code>:
</p>


<h4>Binomial</h4>

<p><code>vector</code> or one-column <code>matrix</code> / <code>data.frame</code> with probabilities (0-1)
<strong>of the second class, alphabetically</strong>.
E.g.:
</p>
<p><code>c(0.3, 0.5, 0.1, 0.5)</code>
</p>
<p>N.B. When unsure whether a model type produces probabilities based off
the alphabetic order of your classes, using 0 and 1 as classes in the
dependent variable instead of the class names should increase the chance of
getting probabilities of the right class.
</p>



<h4>Gaussian</h4>

<p><code>vector</code> or one-column <code>matrix</code> / <code>data.frame</code> with the predicted value.
E.g.:
</p>
<p><code>c(3.7, 0.9, 1.2, 7.3)</code>
</p>



<h4>Multinomial</h4>

<p><code>data.frame</code> with one column per class containing probabilities of the class.
Column names should be identical to how the class names are written in the target column.
E.g.:
</p>

<table>
<tr>
 <td style="text-align: right;">
  <strong>class_1</strong> </td><td style="text-align: right;"> <strong>class_2</strong> </td><td style="text-align: right;">
  <strong>class_3</strong> </td>
</tr>
<tr>
 <td style="text-align: right;">
  0.269 </td><td style="text-align: right;"> 0.528 </td><td style="text-align: right;"> 0.203</td>
</tr>
<tr>
 <td style="text-align: right;">
  0.368 </td><td style="text-align: right;"> 0.322 </td><td style="text-align: right;"> 0.310</td>
</tr>
<tr>
 <td style="text-align: right;">
  0.375 </td><td style="text-align: right;"> 0.371 </td><td style="text-align: right;"> 0.254</td>
</tr>
<tr>
 <td style="text-align: right;">
  ... </td><td style="text-align: right;"> ... </td><td style="text-align: right;"> ...</td>
</tr>

</table>

</td></tr>
<tr><td><code id="cross_validate_fn_+3A_preprocess_fn">preprocess_fn</code></td>
<td>
<p>Function for preprocessing the training and test sets.
</p>
<p>Can, for instance, be used to standardize both the training and test sets
with the scaling and centering parameters from the training set.
</p>
<p>Must have the following function arguments:
</p>
<p><code>function(train_data, test_data,</code>
</p>
<p><code style="white-space: pre;">&#8288;         &#8288;</code><code>formula, hyperparameters)</code>
</p>
<p>Must return a <code>list</code> with the preprocessed <code>`train_data`</code> and <code>`test_data`</code>. It may also contain
a <code>tibble</code> with the <code>parameters</code> used in preprocessing:
</p>
<p><code>list("train" = train_data,</code>
</p>
<p><code style="white-space: pre;">&#8288;     &#8288;</code><code>"test" = test_data,</code>
</p>
<p><code style="white-space: pre;">&#8288;     &#8288;</code><code>"parameters" = preprocess_parameters)</code>
</p>
<p>Additional elements in the returned <code>list</code> will be ignored.
</p>
<p>The optional parameters <code>tibble</code> will be included in the output.
It could have the following format:
</p>

<table>
<tr>
 <td style="text-align: right;">
  <strong>Measure</strong> </td><td style="text-align: right;"> <strong>var_1</strong> </td><td style="text-align: right;"> <strong>var_2</strong> </td>
</tr>
<tr>
 <td style="text-align: right;">
  Mean </td><td style="text-align: right;"> 37.921 </td><td style="text-align: right;"> 88.231</td>
</tr>
<tr>
 <td style="text-align: right;">
  SD </td><td style="text-align: right;"> 12.4 </td><td style="text-align: right;"> 5.986</td>
</tr>
<tr>
 <td style="text-align: right;">
  ... </td><td style="text-align: right;"> ... </td><td style="text-align: right;"> ...</td>
</tr>

</table>

<p>N.B. When <code>`preprocess_once`</code> is <code>FALSE</code>, the current formula and
hyperparameters will be provided. Otherwise,
these arguments will be <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="cross_validate_fn_+3A_preprocess_once">preprocess_once</code></td>
<td>
<p>Whether to apply the preprocessing once
(<strong>ignoring</strong> the formula and hyperparameters arguments in <code>`preprocess_fn`</code>)
or for every model separately. (Logical)
</p>
<p>When preprocessing does not depend on the current formula or hyperparameters,
we can do the preprocessing of each train/test split once, to save time.
This <strong>may require holding a lot more data in memory</strong> though,
why it is not the default setting.</p>
</td></tr>
<tr><td><code id="cross_validate_fn_+3A_hyperparameters">hyperparameters</code></td>
<td>
<p>Either a <code>named list</code> with hyperparameter values to combine in a grid
or a <code>data.frame</code> with one row per hyperparameter combination.
</p>


<h4>Named list for grid search</h4>

<p>Add <code>".n"</code> to sample the combinations. Can be the number of combinations to use,
or a percentage between <code>0</code> and <code>1</code>.
</p>
<p>E.g.
</p>
<p><code>list(".n" = 10,  # sample 10 combinations</code>
</p>
<p><code style="white-space: pre;">&#8288;     &#8288;</code><code>"lrn_rate" = c(0.1, 0.01, 0.001),</code>
</p>
<p><code style="white-space: pre;">&#8288;     &#8288;</code><code>"h_layers" = c(10, 100, 1000),</code>
</p>
<p><code style="white-space: pre;">&#8288;     &#8288;</code><code>"drop_out" = runif(5, 0.3, 0.7))</code>
</p>



<h4><code>data.frame</code> with specific hyperparameter combinations</h4>

<p>One row per combination to test.
</p>
<p>E.g.
</p>

<table>
<tr>
 <td style="text-align: right;">
  <strong>lrn_rate</strong> </td><td style="text-align: right;"> <strong>h_layers</strong> </td><td style="text-align: right;"> <strong>drop_out</strong> </td>
</tr>
<tr>
 <td style="text-align: right;">
  0.1 </td><td style="text-align: right;"> 10 </td><td style="text-align: right;"> 0.65</td>
</tr>
<tr>
 <td style="text-align: right;">
  0.1 </td><td style="text-align: right;"> 1000 </td><td style="text-align: right;"> 0.65</td>
</tr>
<tr>
 <td style="text-align: right;">
  0.01 </td><td style="text-align: right;"> 1000 </td><td style="text-align: right;"> 0.63</td>
</tr>
<tr>
 <td style="text-align: right;">
  ... </td><td style="text-align: right;"> ... </td><td style="text-align: right;"> ...</td>
</tr>

</table>

</td></tr>
<tr><td><code id="cross_validate_fn_+3A_fold_cols">fold_cols</code></td>
<td>
<p>Name(s) of grouping factor(s) for identifying folds. (Character)
</p>
<p>Include names of multiple grouping factors for repeated cross-validation.</p>
</td></tr>
<tr><td><code id="cross_validate_fn_+3A_cutoff">cutoff</code></td>
<td>
<p>Threshold for predicted classes. (Numeric)
</p>
<p>N.B. <strong>Binomial models only</strong></p>
</td></tr>
<tr><td><code id="cross_validate_fn_+3A_positive">positive</code></td>
<td>
<p>Level from dependent variable to predict.
Either as character (<em>preferable</em>) or level index (<code>1</code> or <code>2</code> - alphabetically).
</p>
<p>E.g. if we have the levels <code>"cat"</code> and <code>"dog"</code> and we want <code>"dog"</code> to be the positive class,
we can either provide <code>"dog"</code> or <code>2</code>, as alphabetically, <code>"dog"</code> comes after <code>"cat"</code>.
</p>
<p><strong>Note:</strong> For <em>reproducibility</em>, it's preferable to <strong>specify the name directly</strong>, as
different <code><a href="base.html#topic+locales">locales</a></code> may sort the levels differently.
</p>
<p>Used when calculating confusion matrix metrics and creating <code>ROC</code> curves.
</p>
<p>The <code>Process</code> column in the output can be used to verify this setting.
</p>
<p>N.B. Only affects evaluation metrics, not the model training or returned predictions.
</p>
<p>N.B. <strong>Binomial models only</strong>.</p>
</td></tr>
<tr><td><code id="cross_validate_fn_+3A_metrics">metrics</code></td>
<td>
<p><code>list</code> for enabling/disabling metrics.
</p>
<p>E.g. <code>list("RMSE" = FALSE)</code> would remove <code>RMSE</code> from the regression results,
and <code>list("Accuracy" = TRUE)</code> would add the regular <code>Accuracy</code> metric
to the classification results.
Default values (<code>TRUE</code>/<code>FALSE</code>) will be used for the remaining available metrics.
</p>
<p>You can enable/disable all metrics at once by including
<code>"all" = TRUE/FALSE</code> in the <code>list</code>. This is done prior to enabling/disabling
individual metrics, why f.i. <code>list("all" = FALSE, "RMSE" = TRUE)</code> would return only the <code>RMSE</code> metric.
</p>
<p>The <code>list</code> can be created with
<code><a href="#topic+gaussian_metrics">gaussian_metrics()</a></code>,
<code><a href="#topic+binomial_metrics">binomial_metrics()</a></code>, or
<code><a href="#topic+multinomial_metrics">multinomial_metrics()</a></code>.
</p>
<p>Also accepts the string <code>"all"</code>.</p>
</td></tr>
<tr><td><code id="cross_validate_fn_+3A_rm_nc">rm_nc</code></td>
<td>
<p>Remove non-converged models from output. (Logical)</p>
</td></tr>
<tr><td><code id="cross_validate_fn_+3A_parallel">parallel</code></td>
<td>
<p>Whether to cross-validate the <code>list</code> of models in parallel. (Logical)
</p>
<p>Remember to register a parallel backend first.
E.g. with <code>doParallel::registerDoParallel</code>.</p>
</td></tr>
<tr><td><code id="cross_validate_fn_+3A_verbose">verbose</code></td>
<td>
<p>Whether to message process information
like the number of model instances to fit. (Logical)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Packages used:
</p>


<h4>Results</h4>



<h5>Shared</h5>

<p>AIC : <code><a href="stats.html#topic+AIC">stats::AIC</a></code>
</p>
<p>AICc : <code><a href="MuMIn.html#topic+AICc">MuMIn::AICc</a></code>
</p>
<p>BIC : <code><a href="stats.html#topic+BIC">stats::BIC</a></code>
</p>



<h5>Gaussian</h5>

<p>r2m : <code><a href="MuMIn.html#topic+r.squaredGLMM">MuMIn::r.squaredGLMM</a></code>
</p>
<p>r2c : <code><a href="MuMIn.html#topic+r.squaredGLMM">MuMIn::r.squaredGLMM</a></code>
</p>



<h5>Binomial and Multinomial</h5>

<p>ROC and related metrics:
</p>
<p>Binomial: <code><a href="pROC.html#topic+roc">pROC::roc</a></code>
</p>
<p>Multinomial: <code><a href="pROC.html#topic+multiclass.roc">pROC::multiclass.roc</a></code>
</p>




<h3>Value</h3>

<p><code>tibble</code> with results for each model.
</p>
<p>N.B. The <strong>Fold</strong> column in the nested <code>tibble</code>s contains the test fold in that train/test split.
</p>


<h4>Shared across families</h4>

<p>A nested <code>tibble</code> with <strong>coefficients</strong> of the models from all iterations. The coefficients
are extracted from the model object with <code><a href="parameters.html#topic+model_parameters">parameters::model_parameters()</a></code> or
<code><a href="stats.html#topic+coef">coef()</a></code> (with some restrictions on the output).
If these attempts fail, a default coefficients <code>tibble</code> filled with <code>NA</code>s is returned.
</p>
<p>Nested <code>tibble</code> with the used <strong>preprocessing parameters</strong>,
if a passed <code>preprocess_fn</code> returns the parameters in a <code>tibble</code>.
</p>
<p>Number of <em>total</em> <strong>folds</strong>.
</p>
<p>Number of <strong>fold columns</strong>.
</p>
<p>Count of <strong>convergence warnings</strong>, using a limited set of keywords (e.g. &quot;convergence&quot;). If a
convergence warning does not contain one of these keywords, it will be counted with <strong>other warnings</strong>.
Consider discarding models that did not converge on all iterations.
Note: you might still see results, but these should be taken with a grain of salt!
</p>
<p>Nested <code>tibble</code> with the <strong>warnings and messages</strong> caught for each model.
</p>
<p>A nested <strong>Process</strong> information object with information
about the evaluation.
</p>
<p>Name of <strong>dependent</strong> variable.
</p>
<p>Names of <strong>fixed</strong> effects.
</p>
<p>Names of <strong>random</strong> effects, if any.
</p>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>


<h4>Gaussian Results</h4>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>
<p>Average <strong><code>RMSE</code></strong>, <strong><code>MAE</code></strong>, <strong><code>NRMSE(IQR)</code></strong>,
<strong><code>RRSE</code></strong>, <strong><code>RAE</code></strong>, <strong><code>RMSLE</code></strong> of all the iterations*,
<em><strong>omitting potential NAs</strong> from non-converged iterations</em>.
</p>
<p>See the additional metrics (disabled by default) at <code><a href="#topic+gaussian_metrics">?gaussian_metrics</a></code>.
</p>
<p>A nested <code>tibble</code> with the <strong>predictions</strong> and targets.
</p>
<p>A nested <code>tibble</code> with the non-averaged <strong>results</strong> from all iterations.
</p>
<p>* In <em>repeated cross-validation</em>,
the metrics are first averaged for each fold column (repetition) and then averaged again.
</p>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>


<h4>Binomial Results</h4>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>
<p>Based on the <strong>collected</strong> predictions from the test folds*,
a confusion matrix and a <code>ROC</code> curve are created to get the following:
</p>
<p><code>ROC</code>:
</p>
<p><strong><code>AUC</code></strong>, <strong><code>Lower CI</code></strong>, and <strong><code>Upper CI</code></strong>
</p>
<p><code>Confusion Matrix</code>:
</p>
<p><strong><code>Balanced Accuracy</code></strong>,
<strong><code>F1</code></strong>,
<strong><code>Sensitivity</code></strong>,
<strong><code>Specificity</code></strong>,
<strong><code>Positive Predictive Value</code></strong>,
<strong><code>Negative Predictive Value</code></strong>,
<strong><code>Kappa</code></strong>,
<strong><code>Detection Rate</code></strong>,
<strong><code>Detection Prevalence</code></strong>,
<strong><code>Prevalence</code></strong>, and
<strong><code>MCC</code></strong> (Matthews correlation coefficient).
</p>
<p>See the additional metrics (disabled by default) at
<code><a href="#topic+binomial_metrics">?binomial_metrics</a></code>.
</p>
<p>Also includes:
</p>
<p>A nested <code>tibble</code> with <strong>predictions</strong>, predicted classes (depends on <code>cutoff</code>), and the targets.
Note, that the predictions are <em>not necessarily</em> of the <em>specified</em> <code>positive</code> class, but of
the <em>model's</em> positive class (second level of dependent variable, alphabetically).
</p>
<p>The <code><a href="pROC.html#topic+roc">pROC::roc</a></code> <strong><code>ROC</code></strong> curve object(s).
</p>
<p>A nested <code>tibble</code> with the <strong>confusion matrix</strong>/matrices.
The <code>Pos_</code> columns tells you whether a row is a
True Positive (<code>TP</code>), True Negative (<code>TN</code>),
False Positive (<code>FP</code>), or False Negative (<code>FN</code>),
depending on which level is the &quot;positive&quot; class. I.e. the level you wish to predict.
</p>
<p>A nested <code>tibble</code> with the <strong>results</strong> from all fold columns.
</p>
<p>The name of the <strong>Positive Class</strong>.
</p>
<p>* In <em>repeated cross-validation</em>, an evaluation is made per fold column (repetition) and averaged.
</p>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>


<h4>Multinomial Results</h4>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>
<p>For each class, a <em>one-vs-all</em> binomial evaluation is performed. This creates
a <strong>Class Level Results</strong> <code>tibble</code> containing the same metrics as the binomial results
described above (excluding <code>MCC</code>, <code>AUC</code>, <code>Lower CI</code> and <code>Upper CI</code>),
along with a count of the class in the target column (<strong><code>Support</code></strong>).
These metrics are used to calculate the <strong>macro-averaged</strong> metrics. The nested class level results
<code>tibble</code> is also included in the output <code>tibble</code>,
and could be reported along with the macro and overall metrics.
</p>
<p>The output <code>tibble</code> contains the macro and overall metrics.
The metrics that share their name with the metrics in the nested
class level results <code>tibble</code> are averages of those metrics
(note: does not remove <code>NA</code>s before averaging).
In addition to these, it also includes the <strong><code>Overall Accuracy</code></strong> and
the multiclass <strong><code>MCC</code></strong>.
</p>
<p><strong>Note:</strong> <strong><code>Balanced Accuracy</code></strong> is the macro-averaged metric,
<em>not</em> the macro sensitivity as sometimes used!
</p>
<p>Other available metrics (disabled by default, see <code>metrics</code>):
<strong><code>Accuracy</code></strong>,
<em>multiclass</em> <strong><code>AUC</code></strong>,
<strong><code>Weighted Balanced Accuracy</code></strong>,
<strong><code>Weighted Accuracy</code></strong>,
<strong><code>Weighted F1</code></strong>,
<strong><code>Weighted Sensitivity</code></strong>,
<strong><code>Weighted Sensitivity</code></strong>,
<strong><code>Weighted Specificity</code></strong>,
<strong><code>Weighted Pos Pred Value</code></strong>,
<strong><code>Weighted Neg Pred Value</code></strong>,
<strong><code>Weighted Kappa</code></strong>,
<strong><code>Weighted Detection Rate</code></strong>,
<strong><code>Weighted Detection Prevalence</code></strong>, and
<strong><code>Weighted Prevalence</code></strong>.
</p>
<p>Note that the &quot;Weighted&quot; average metrics are weighted by the <code>Support</code>.
</p>
<p>Also includes:
</p>
<p>A nested <code>tibble</code> with the <strong>predictions</strong>, predicted classes, and targets.
</p>
<p>A <code>list</code> of <strong>ROC</strong> curve objects when <code>AUC</code> is enabled.
</p>
<p>A nested <code>tibble</code> with the multiclass <strong>Confusion Matrix</strong>.
</p>
<p><strong>Class Level Results</strong>
</p>
<p>Besides the binomial evaluation metrics and the <code>Support</code>,
the nested class level results <code>tibble</code> also contains a
nested <code>tibble</code> with the <strong>Confusion Matrix</strong> from the one-vs-all evaluation.
The <code>Pos_</code> columns tells you whether a row is a
True Positive (<code>TP</code>), True Negative (<code>TN</code>),
False Positive (<code>FP</code>), or False Negative (<code>FN</code>),
depending on which level is the &quot;positive&quot; class. In our case, <code>1</code> is the current class
and <code>0</code> represents all the other classes together.
</p>



<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>


<h3>See Also</h3>

<p>Other validation functions: 
<code><a href="#topic+cross_validate">cross_validate</a>()</code>,
<code><a href="#topic+validate">validate</a>()</code>,
<code><a href="#topic+validate_fn">validate_fn</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Attach packages
library(cvms)
library(groupdata2) # fold()
library(dplyr) # %&gt;% arrange() mutate()

# Note: More examples of custom functions can be found at:
# model_fn: model_functions()
# predict_fn: predict_functions()
# preprocess_fn: preprocess_functions()

# Data is part of cvms
data &lt;- participant.scores

# Set seed for reproducibility
set.seed(7)

# Fold data
data &lt;- fold(
  data,
  k = 4,
  cat_col = "diagnosis",
  id_col = "participant"
) %&gt;%
  mutate(diagnosis = as.factor(diagnosis)) %&gt;%
  arrange(.folds)

# Cross-validate multiple formulas

formulas_gaussian &lt;- c(
  "score ~ diagnosis",
  "score ~ age"
)
formulas_binomial &lt;- c(
  "diagnosis ~ score",
  "diagnosis ~ age"
)

#
# Gaussian
#

# Create model function that returns a fitted model object
lm_model_fn &lt;- function(train_data, formula, hyperparameters) {
  lm(formula = formula, data = train_data)
}

# Create predict function that returns the predictions
lm_predict_fn &lt;- function(test_data, model, formula,
                          hyperparameters, train_data) {
  stats::predict(
    object = model,
    newdata = test_data,
    type = "response",
    allow.new.levels = TRUE
  )
}

# Cross-validate the model function
cross_validate_fn(
  data,
  formulas = formulas_gaussian,
  type = "gaussian",
  model_fn = lm_model_fn,
  predict_fn = lm_predict_fn,
  fold_cols = ".folds"
)

#
# Binomial
#

# Create model function that returns a fitted model object
glm_model_fn &lt;- function(train_data, formula, hyperparameters) {
  glm(formula = formula, data = train_data, family = "binomial")
}

# Create predict function that returns the predictions
glm_predict_fn &lt;- function(test_data, model, formula,
                           hyperparameters, train_data) {
  stats::predict(
    object = model,
    newdata = test_data,
    type = "response",
    allow.new.levels = TRUE
  )
}

# Cross-validate the model function
cross_validate_fn(
  data,
  formulas = formulas_binomial,
  type = "binomial",
  model_fn = glm_model_fn,
  predict_fn = glm_predict_fn,
  fold_cols = ".folds"
)

#
# Support Vector Machine (svm)
# with hyperparameter tuning
#

# Only run if the `e1071` package is installed
if (requireNamespace("e1071", quietly = TRUE)){

# Create model function that returns a fitted model object
# We use the hyperparameters arg to pass in the kernel and cost values
svm_model_fn &lt;- function(train_data, formula, hyperparameters) {

  # Expected hyperparameters:
  #  - kernel
  #  - cost
  if (!"kernel" %in% names(hyperparameters))
    stop("'hyperparameters' must include 'kernel'")
  if (!"cost" %in% names(hyperparameters))
    stop("'hyperparameters' must include 'cost'")

  e1071::svm(
    formula = formula,
    data = train_data,
    kernel = hyperparameters[["kernel"]],
    cost = hyperparameters[["cost"]],
    scale = FALSE,
    type = "C-classification",
    probability = TRUE
  )
}

# Create predict function that returns the predictions
svm_predict_fn &lt;- function(test_data, model, formula,
                           hyperparameters, train_data) {
  predictions &lt;- stats::predict(
    object = model,
    newdata = test_data,
    allow.new.levels = TRUE,
    probability = TRUE
  )

  # Extract probabilities
  probabilities &lt;- dplyr::as_tibble(
    attr(predictions, "probabilities")
  )

  # Return second column
  probabilities[[2]]
}

# Specify hyperparameters to try
# The optional ".n" samples 4 combinations
svm_hparams &lt;- list(
  ".n" = 4,
  "kernel" = c("linear", "radial"),
  "cost" = c(1, 5, 10)
)

# Cross-validate the model function
cv &lt;- cross_validate_fn(
  data,
  formulas = formulas_binomial,
  type = "binomial",
  model_fn = svm_model_fn,
  predict_fn = svm_predict_fn,
  hyperparameters = svm_hparams,
  fold_cols = ".folds"
)

cv

# The `HParams` column has the nested hyperparameter values
cv %&gt;%
  select(Dependent, Fixed, HParams, `Balanced Accuracy`, F1, AUC, MCC) %&gt;%
  tidyr::unnest(cols = "HParams") %&gt;%
  arrange(desc(`Balanced Accuracy`), desc(F1))

#
# Use parallelization
# The below examples show the speed gains when running in parallel
#

# Attach doParallel and register four cores
# Uncomment:
# library(doParallel)
# registerDoParallel(4)

# Specify hyperparameters such that we will
# cross-validate 20 models
hparams &lt;- list(
  "kernel" = c("linear", "radial"),
  "cost" = 1:5
)

# Cross-validate a list of 20 models in parallel
# Make sure to uncomment the parallel argument
system.time({
  cross_validate_fn(
    data,
    formulas = formulas_gaussian,
    type = "gaussian",
    model_fn = svm_model_fn,
    predict_fn = svm_predict_fn,
    hyperparameters = hparams,
    fold_cols = ".folds"
    #, parallel = TRUE  # Uncomment
  )
})

# Cross-validate a list of 20 models sequentially
system.time({
  cross_validate_fn(
    data,
    formulas = formulas_gaussian,
    type = "gaussian",
    model_fn = svm_model_fn,
    predict_fn = svm_predict_fn,
    hyperparameters = hparams,
    fold_cols = ".folds"
    #, parallel = TRUE  # Uncomment
  )
})

} # closes `e1071` package check

</code></pre>

<hr>
<h2 id='evaluate'>Evaluate your model's performance</h2><span id='topic+evaluate'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#maturing"><img src="../help/figures/lifecycle-maturing.svg" alt='[Maturing]' /></a>
</p>
<p>Evaluate your model's predictions
on a set of evaluation metrics.
</p>
<p>Create ID-aggregated evaluations by multiple methods.
</p>
<p>Currently supports regression and classification
(binary and multiclass). See <code>`type`</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>evaluate(
  data,
  target_col,
  prediction_cols,
  type,
  id_col = NULL,
  id_method = "mean",
  apply_softmax = FALSE,
  cutoff = 0.5,
  positive = 2,
  metrics = list(),
  include_predictions = TRUE,
  parallel = FALSE,
  models = deprecated()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="evaluate_+3A_data">data</code></td>
<td>
<p><code>data.frame</code> with predictions, targets and (optionally) an ID column.
Can be grouped with <code><a href="dplyr.html#topic+group_by">group_by</a></code>.
</p>


<h4>Multinomial</h4>

<p>When <code>`type`</code> is <code>"multinomial"</code>, the predictions can be passed in one of two formats.
</p>


<h5>Probabilities (Preferable)</h5>

<p>One column per class with the probability of that class.
The columns should have the name of their class,
as they are named in the target column. E.g.:
</p>

<table>
<tr>
 <td style="text-align: right;">
  <strong>class_1</strong> </td><td style="text-align: right;"> <strong>class_2</strong> </td><td style="text-align: right;">
  <strong>class_3</strong> </td><td style="text-align: right;"> <strong>target</strong></td>
</tr>
<tr>
 <td style="text-align: right;">
  0.269 </td><td style="text-align: right;"> 0.528 </td><td style="text-align: right;"> 0.203 </td><td style="text-align: right;"> class_2</td>
</tr>
<tr>
 <td style="text-align: right;">
  0.368 </td><td style="text-align: right;"> 0.322 </td><td style="text-align: right;"> 0.310 </td><td style="text-align: right;"> class_3</td>
</tr>
<tr>
 <td style="text-align: right;">
  0.375 </td><td style="text-align: right;"> 0.371 </td><td style="text-align: right;"> 0.254 </td><td style="text-align: right;"> class_2</td>
</tr>
<tr>
 <td style="text-align: right;">
  ... </td><td style="text-align: right;"> ... </td><td style="text-align: right;"> ... </td><td style="text-align: right;"> ...</td>
</tr>

</table>




<h5>Classes</h5>

<p>A single column of type <code>character</code> with the predicted classes. E.g.:
</p>

<table>
<tr>
 <td style="text-align: right;">
  <strong>prediction</strong> </td><td style="text-align: right;"> <strong>target</strong></td>
</tr>
<tr>
 <td style="text-align: right;">
  class_2 </td><td style="text-align: right;"> class_2</td>
</tr>
<tr>
 <td style="text-align: right;">
  class_1 </td><td style="text-align: right;"> class_3</td>
</tr>
<tr>
 <td style="text-align: right;">
  class_1 </td><td style="text-align: right;"> class_2</td>
</tr>
<tr>
 <td style="text-align: right;">
  ... </td><td style="text-align: right;"> ...</td>
</tr>

</table>





<h4>Binomial</h4>

<p>When <code>`type`</code> is <code>"binomial"</code>, the predictions can be passed in one of two formats.
</p>


<h5>Probabilities (Preferable)</h5>

<p>One column with the <strong>probability of class being
the second class alphabetically</strong>
(1 if classes are 0 and 1). E.g.:
</p>

<table>
<tr>
 <td style="text-align: right;">
  <strong>prediction</strong> </td><td style="text-align: right;"> <strong>target</strong></td>
</tr>
<tr>
 <td style="text-align: right;">
  0.769 </td><td style="text-align: right;"> 1</td>
</tr>
<tr>
 <td style="text-align: right;">
  0.368 </td><td style="text-align: right;"> 1</td>
</tr>
<tr>
 <td style="text-align: right;">
  0.375 </td><td style="text-align: right;"> 0</td>
</tr>
<tr>
 <td style="text-align: right;">
  ... </td><td style="text-align: right;"> ...</td>
</tr>

</table>


<p>Note: At the alphabetical ordering of the class labels, they are of type <code>character</code>,
why e.g. <code>100</code> would come before <code>7</code>.
</p>


<h5>Classes</h5>

<p>A single column of type <code>character</code> with the predicted classes. E.g.:
</p>

<table>
<tr>
 <td style="text-align: right;">
  <strong>prediction</strong> </td><td style="text-align: right;"> <strong>target</strong></td>
</tr>
<tr>
 <td style="text-align: right;">
  class_0 </td><td style="text-align: right;"> class_1</td>
</tr>
<tr>
 <td style="text-align: right;">
  class_1 </td><td style="text-align: right;"> class_1</td>
</tr>
<tr>
 <td style="text-align: right;">
  class_1 </td><td style="text-align: right;"> class_0</td>
</tr>
<tr>
 <td style="text-align: right;">
  ... </td><td style="text-align: right;"> ...</td>
</tr>

</table>


<p>Note: The prediction column will be converted to the probability <code>0.0</code>
for the first class alphabetically and <code>1.0</code> for
the second class alphabetically.
</p>



<h4>Gaussian</h4>

<p>When <code>`type`</code> is <code>"gaussian"</code>, the predictions should be passed as
one column with the predicted values. E.g.:
</p>

<table>
<tr>
 <td style="text-align: right;">
  <strong>prediction</strong> </td><td style="text-align: right;"> <strong>target</strong></td>
</tr>
<tr>
 <td style="text-align: right;">
  28.9 </td><td style="text-align: right;"> 30.2</td>
</tr>
<tr>
 <td style="text-align: right;">
  33.2 </td><td style="text-align: right;"> 27.1</td>
</tr>
<tr>
 <td style="text-align: right;">
  23.4 </td><td style="text-align: right;"> 21.3</td>
</tr>
<tr>
 <td style="text-align: right;">
  ... </td><td style="text-align: right;"> ...</td>
</tr>

</table>

</td></tr>
<tr><td><code id="evaluate_+3A_target_col">target_col</code></td>
<td>
<p>Name of the column with the true classes/values in <code>`data`</code>.
</p>
<p>When <code>`type`</code> is <code>"multinomial"</code>, this column should contain the class names,
not their indices.</p>
</td></tr>
<tr><td><code id="evaluate_+3A_prediction_cols">prediction_cols</code></td>
<td>
<p>Name(s) of column(s) with the predictions.
</p>
<p>Columns can be either numeric or character depending on which format is chosen.
See <code>`data`</code> for the possible formats.</p>
</td></tr>
<tr><td><code id="evaluate_+3A_type">type</code></td>
<td>
<p>Type of evaluation to perform:
</p>
<p><code>"gaussian"</code> for regression (like linear regression).
</p>
<p><code>"binomial"</code> for binary classification.
</p>
<p><code>"multinomial"</code> for multiclass classification.</p>
</td></tr>
<tr><td><code id="evaluate_+3A_id_col">id_col</code></td>
<td>
<p>Name of ID column to aggregate predictions by.
</p>
<p>N.B. Current methods assume that the target class/value is constant within the IDs.
</p>
<p>N.B. When aggregating by ID, some metrics may be disabled.</p>
</td></tr>
<tr><td><code id="evaluate_+3A_id_method">id_method</code></td>
<td>
<p>Method to use when aggregating predictions by ID.
Either <code>"mean"</code> or <code>"majority"</code>.
</p>
<p>When <code>`type`</code> is <code>gaussian</code>, only the <code>"mean"</code> method is available.
</p>


<h4>mean</h4>

<p>The average prediction (value or probability) is calculated per ID and evaluated.
This method assumes that the target class/value is constant within the IDs.
</p>



<h4>majority</h4>

<p>The most predicted class per ID is found and evaluated. In case of a tie,
the winning classes share the probability (e.g. <code>P = 0.5</code> each when two majority classes).
This method assumes that the target class/value is constant within the IDs.
</p>
</td></tr>
<tr><td><code id="evaluate_+3A_apply_softmax">apply_softmax</code></td>
<td>
<p>Whether to apply the softmax function to the
prediction columns when <code>`type`</code> is <code>"multinomial"</code>.
</p>
<p>N.B. <strong>Multinomial models only</strong>.</p>
</td></tr>
<tr><td><code id="evaluate_+3A_cutoff">cutoff</code></td>
<td>
<p>Threshold for predicted classes. (Numeric)
</p>
<p>N.B. <strong>Binomial models only</strong>.</p>
</td></tr>
<tr><td><code id="evaluate_+3A_positive">positive</code></td>
<td>
<p>Level from dependent variable to predict.
Either as character (<em>preferable</em>) or level index (<code>1</code> or <code>2</code> - alphabetically).
</p>
<p>E.g. if we have the levels <code>"cat"</code> and <code>"dog"</code> and we want <code>"dog"</code> to be the positive class,
we can either provide <code>"dog"</code> or <code>2</code>, as alphabetically, <code>"dog"</code> comes after <code>"cat"</code>.
</p>
<p><strong>Note:</strong> For <em>reproducibility</em>, it's preferable to <strong>specify the name directly</strong>, as
different <code><a href="base.html#topic+locales">locales</a></code> may sort the levels differently.
</p>
<p>Used when calculating confusion matrix metrics and creating <code>ROC</code> curves.
</p>
<p>The <code>Process</code> column in the output can be used to verify this setting.
</p>
<p>N.B. Only affects the evaluation metrics. <strong>Does NOT affect what the probabilities are of (always the second class alphabetically).</strong>
</p>
<p>N.B. <strong>Binomial models only</strong>.</p>
</td></tr>
<tr><td><code id="evaluate_+3A_metrics">metrics</code></td>
<td>
<p><code>list</code> for enabling/disabling metrics.
</p>
<p>E.g. <code>list("RMSE" = FALSE)</code> would remove <code>RMSE</code> from the regression results,
and <code>list("Accuracy" = TRUE)</code> would add the regular <code>Accuracy</code> metric
to the classification results.
Default values (<code>TRUE</code>/<code>FALSE</code>) will be used for the remaining available metrics.
</p>
<p>You can enable/disable all metrics at once by including
<code>"all" = TRUE/FALSE</code> in the <code>list</code>. This is done prior to enabling/disabling
individual metrics, why f.i. <code>list("all" = FALSE, "RMSE" = TRUE)</code>
would return only the <code>RMSE</code> metric.
</p>
<p>The <code>list</code> can be created with
<code><a href="#topic+gaussian_metrics">gaussian_metrics()</a></code>,
<code><a href="#topic+binomial_metrics">binomial_metrics()</a></code>, or
<code><a href="#topic+multinomial_metrics">multinomial_metrics()</a></code>.
</p>
<p>Also accepts the string <code>"all"</code>.</p>
</td></tr>
<tr><td><code id="evaluate_+3A_include_predictions">include_predictions</code></td>
<td>
<p>Whether to include the predictions
in the output as a nested <code>tibble</code>. (Logical)</p>
</td></tr>
<tr><td><code id="evaluate_+3A_parallel">parallel</code></td>
<td>
<p>Whether to run evaluations in parallel,
when <code>`data`</code> is grouped with <code><a href="dplyr.html#topic+group_by">group_by</a></code>.</p>
</td></tr>
<tr><td><code id="evaluate_+3A_models">models</code></td>
<td>
<p>Deprecated.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Packages used:
</p>
<p><strong>Binomial</strong> and <strong>Multinomial</strong>:
</p>
<p><code>ROC</code> and <code>AUC</code>:
</p>
<p>Binomial: <code><a href="pROC.html#topic+roc">pROC::roc</a></code>
</p>
<p>Multinomial: <code><a href="pROC.html#topic+multiclass.roc">pROC::multiclass.roc</a></code>
</p>


<h3>Value</h3>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>


<h4>Gaussian Results</h4>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>
<p><code>tibble</code> containing the following metrics by default:
</p>
<p>Average <strong><code>RMSE</code></strong>, <strong><code>MAE</code></strong>, <strong><code>NRMSE(IQR)</code></strong>,
<strong><code>RRSE</code></strong>, <strong><code>RAE</code></strong>, <strong><code>RMSLE</code></strong>.
</p>
<p>See the additional metrics (disabled by default) at
<code><a href="#topic+gaussian_metrics">?gaussian_metrics</a></code>.
</p>
<p>Also includes:
</p>
<p>A nested <code>tibble</code> with the <strong>Predictions</strong> and targets.
</p>
<p>A nested <strong>Process</strong> information object with information
about the evaluation.
</p>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>


<h4>Binomial Results</h4>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>
<p><code>tibble</code> with the following evaluation metrics, based on a
<code>confusion matrix</code> and a <code>ROC</code> curve fitted to the predictions:
</p>
<p><code>Confusion Matrix</code>:
</p>
<p><strong><code>Balanced Accuracy</code></strong>,
<strong><code>Accuracy</code></strong>,
<strong><code>F1</code></strong>,
<strong><code>Sensitivity</code></strong>,
<strong><code>Specificity</code></strong>,
<strong><code>Positive Predictive Value</code></strong>,
<strong><code>Negative Predictive Value</code></strong>,
<strong><code>Kappa</code></strong>,
<strong><code>Detection Rate</code></strong>,
<strong><code>Detection Prevalence</code></strong>,
<strong><code>Prevalence</code></strong>, and
<strong><code>MCC</code></strong> (Matthews correlation coefficient).
</p>
<p><code>ROC</code>:
</p>
<p><strong><code>AUC</code></strong>, <strong><code>Lower CI</code></strong>, and <strong><code>Upper CI</code></strong>
</p>
<p>Note, that the <code>ROC</code> curve is only computed if <code>AUC</code> is enabled. See <code>metrics</code>.
</p>
<p>Also includes:
</p>
<p>A nested <code>tibble</code> with the <strong>predictions</strong> and targets.
</p>
<p>A <code>list</code> of <strong>ROC</strong> curve objects (if computed).
</p>
<p>A nested <code>tibble</code> with the <strong>confusion matrix</strong>.
The <code>Pos_</code> columns tells you whether a row is a
True Positive (<code>TP</code>), True Negative (<code>TN</code>),
False Positive (<code>FP</code>), or False Negative (<code>FN</code>),
depending on which level is the &quot;<code>positive</code>&quot; class.
I.e. the level you wish to predict.
</p>
<p>A nested <strong>Process</strong> information object with information
about the evaluation.
</p>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>


<h4>Multinomial Results</h4>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>
<p>For each class, a <em>one-vs-all</em> binomial evaluation is performed. This creates
a <strong>Class Level Results</strong> <code>tibble</code> containing the same metrics as the binomial results
described above (excluding <code>Accuracy</code>, <code>MCC</code>, <code>AUC</code>, <code>Lower CI</code> and <code>Upper CI</code>),
along with a count of the class in the target column (<strong><code>Support</code></strong>).
These metrics are used to calculate the <strong>macro-averaged</strong> metrics.
The nested class level results <code>tibble</code> is also included in the output <code>tibble</code>,
and could be reported along with the macro and overall metrics.
</p>
<p>The output <code>tibble</code> contains the macro and overall metrics.
The metrics that share their name with the metrics in the nested
class level results <code>tibble</code> are averages of those metrics
(note: does not remove <code>NA</code>s before averaging).
In addition to these, it also includes the <strong><code>Overall Accuracy</code></strong> and
the multiclass <strong><code>MCC</code></strong>.
</p>
<p><strong>Note:</strong> <strong><code>Balanced Accuracy</code></strong> is the macro-averaged metric,
<em>not</em> the macro sensitivity as sometimes used!
</p>
<p>Other available metrics (disabled by default, see <code>metrics</code>):
<strong><code>Accuracy</code></strong>,
<em>multiclass</em> <strong><code>AUC</code></strong>,
<strong><code>Weighted Balanced Accuracy</code></strong>,
<strong><code>Weighted Accuracy</code></strong>,
<strong><code>Weighted F1</code></strong>,
<strong><code>Weighted Sensitivity</code></strong>,
<strong><code>Weighted Sensitivity</code></strong>,
<strong><code>Weighted Specificity</code></strong>,
<strong><code>Weighted Pos Pred Value</code></strong>,
<strong><code>Weighted Neg Pred Value</code></strong>,
<strong><code>Weighted Kappa</code></strong>,
<strong><code>Weighted Detection Rate</code></strong>,
<strong><code>Weighted Detection Prevalence</code></strong>, and
<strong><code>Weighted Prevalence</code></strong>.
</p>
<p>Note that the &quot;Weighted&quot; average metrics are weighted by the <code>Support</code>.
</p>
<p>When having a large set of classes, consider keeping <code>AUC</code> disabled.
</p>
<p>Also includes:
</p>
<p>A nested <code>tibble</code> with the <strong>Predictions</strong> and targets.
</p>
<p>A <code>list</code> of <strong>ROC</strong> curve objects when <code>AUC</code> is enabled.
</p>
<p>A nested <code>tibble</code> with the multiclass <strong>Confusion Matrix</strong>.
</p>
<p>A nested <strong>Process</strong> information object with information
about the evaluation.
</p>


<h5>Class Level Results</h5>

<p>Besides the binomial evaluation metrics and the <code>Support</code>,
the nested class level results <code>tibble</code> also contains a
nested <code>tibble</code> with the <strong>Confusion Matrix</strong> from the one-vs-all evaluation.
The <code>Pos_</code> columns tells you whether a row is a
True Positive (<code>TP</code>), True Negative (<code>TN</code>),
False Positive (<code>FP</code>), or False Negative (<code>FN</code>),
depending on which level is the &quot;positive&quot; class. In our case, <code>1</code> is the current class
and <code>0</code> represents all the other classes together.
</p>




<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>


<h3>See Also</h3>

<p>Other evaluation functions: 
<code><a href="#topic+binomial_metrics">binomial_metrics</a>()</code>,
<code><a href="#topic+confusion_matrix">confusion_matrix</a>()</code>,
<code><a href="#topic+evaluate_residuals">evaluate_residuals</a>()</code>,
<code><a href="#topic+gaussian_metrics">gaussian_metrics</a>()</code>,
<code><a href="#topic+multinomial_metrics">multinomial_metrics</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Attach packages
library(cvms)
library(dplyr)

# Load data
data &lt;- participant.scores

# Fit models
gaussian_model &lt;- lm(age ~ diagnosis, data = data)
binomial_model &lt;- glm(diagnosis ~ score, data = data)

# Add predictions
data[["gaussian_predictions"]] &lt;- predict(gaussian_model, data,
  type = "response",
  allow.new.levels = TRUE
)
data[["binomial_predictions"]] &lt;- predict(binomial_model, data,
  allow.new.levels = TRUE
)

# Gaussian evaluation
evaluate(
  data = data, target_col = "age",
  prediction_cols = "gaussian_predictions",
  type = "gaussian"
)

# Binomial evaluation
evaluate(
  data = data, target_col = "diagnosis",
  prediction_cols = "binomial_predictions",
  type = "binomial"
)

#
# Multinomial
#

# Create a tibble with predicted probabilities and targets
data_mc &lt;- multiclass_probability_tibble(
  num_classes = 3, num_observations = 45,
  apply_softmax = TRUE, FUN = runif,
  class_name = "class_",
  add_targets = TRUE
)

class_names &lt;- paste0("class_", 1:3)

# Multinomial evaluation
evaluate(
  data = data_mc, target_col = "Target",
  prediction_cols = class_names,
  type = "multinomial"
)

#
# ID evaluation
#

# Gaussian ID evaluation
# Note that 'age' is the same for all observations
# of a participant
evaluate(
  data = data, target_col = "age",
  prediction_cols = "gaussian_predictions",
  id_col = "participant",
  type = "gaussian"
)

# Binomial ID evaluation
evaluate(
  data = data, target_col = "diagnosis",
  prediction_cols = "binomial_predictions",
  id_col = "participant",
  id_method = "mean", # alternatively: "majority"
  type = "binomial"
)

# Multinomial ID evaluation

# Add IDs and new targets (must be constant within IDs)
data_mc[["Target"]] &lt;- NULL
data_mc[["ID"]] &lt;- rep(1:9, each = 5)
id_classes &lt;- tibble::tibble(
  "ID" = 1:9,
  "Target" = sample(x = class_names, size = 9, replace = TRUE)
)
data_mc &lt;- data_mc %&gt;%
  dplyr::left_join(id_classes, by = "ID")

# Perform ID evaluation
evaluate(
  data = data_mc, target_col = "Target",
  prediction_cols = class_names,
  id_col = "ID",
  id_method = "mean", # alternatively: "majority"
  type = "multinomial"
)

#
# Training and evaluating a multinomial model with nnet
#

# Only run if `nnet` is installed
if (requireNamespace("nnet", quietly = TRUE)){

# Create a data frame with some predictors and a target column
class_names &lt;- paste0("class_", 1:4)
data_for_nnet &lt;- multiclass_probability_tibble(
  num_classes = 3, # Here, number of predictors
  num_observations = 30,
  apply_softmax = FALSE,
  FUN = rnorm,
  class_name = "predictor_"
) %&gt;%
  dplyr::mutate(Target = sample(
    class_names,
    size = 30,
    replace = TRUE
  ))

# Train multinomial model using the nnet package
mn_model &lt;- nnet::multinom(
  "Target ~ predictor_1 + predictor_2 + predictor_3",
  data = data_for_nnet
)

# Predict the targets in the dataset
# (we would usually use a test set instead)
predictions &lt;- predict(
  mn_model,
  data_for_nnet,
  type = "probs"
) %&gt;%
  dplyr::as_tibble()

# Add the targets
predictions[["Target"]] &lt;- data_for_nnet[["Target"]]

# Evaluate predictions
evaluate(
  data = predictions,
  target_col = "Target",
  prediction_cols = class_names,
  type = "multinomial"
)
}

</code></pre>

<hr>
<h2 id='evaluate_residuals'>Evaluate residuals from a regression task</h2><span id='topic+evaluate_residuals'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#experimental"><img src="../help/figures/lifecycle-experimental.svg" alt='[Experimental]' /></a>
</p>
<p>Calculates a large set of error metrics from regression residuals.
</p>
<p><strong>Note</strong>: In most cases you should use <code><a href="#topic+evaluate">evaluate()</a></code> instead.
It works in <code>magrittr</code> pipes (e.g. <code>%&gt;%</code>) and with
<code><a href="dplyr.html#topic+group_by">dplyr::group_by()</a></code>.
<code>evaluate_residuals()</code> is more lightweight and may be preferred in
programming when you don't need the extra stuff
in <code><a href="#topic+evaluate">evaluate()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>evaluate_residuals(data, target_col, prediction_col, metrics = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="evaluate_residuals_+3A_data">data</code></td>
<td>
<p><code>data.frame</code> with predictions and targets.</p>
</td></tr>
<tr><td><code id="evaluate_residuals_+3A_target_col">target_col</code></td>
<td>
<p>Name of the column with the true values in <code>`data`</code>.</p>
</td></tr>
<tr><td><code id="evaluate_residuals_+3A_prediction_col">prediction_col</code></td>
<td>
<p>Name of column with the predicted values in <code>`data`</code>.</p>
</td></tr>
<tr><td><code id="evaluate_residuals_+3A_metrics">metrics</code></td>
<td>
<p><code>list</code> for enabling/disabling metrics.
</p>
<p>E.g. <code>list("RMSE" = FALSE)</code> would disable <code>RMSE</code>.
Default values (<code>TRUE</code>/<code>FALSE</code>) will be used for the remaining available metrics.
</p>
<p>You can enable/disable all metrics at once by including
<code>"all" = TRUE/FALSE</code> in the <code>list</code>. This is done prior to enabling/disabling
individual metrics, why for instance <code>list("all" = FALSE, "RMSE" = TRUE)</code>
would return only the <code>RMSE</code> metric.
</p>
<p>The <code>list</code> can be created with
<code><a href="#topic+gaussian_metrics">gaussian_metrics()</a></code>.
</p>
<p>Also accepts the string <code>"all"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The metric formulas are listed in <em>'The Available Metrics'</em> vignette.
</p>


<h3>Value</h3>

<p><code>tibble</code> <code>data.frame</code> with the calculated metrics.
</p>
<p>The following metrics are available (see <code>`metrics`</code>):
</p>

<table>
<tr>
 <td style="text-align: right;">
  <strong>Metric</strong> </td><td style="text-align: right;"> <strong>Name</strong> </td><td style="text-align: right;"> <strong>Default</strong> </td>
</tr>
<tr>
 <td style="text-align: right;">
  Mean Absolute Error </td><td style="text-align: right;"> "MAE" </td><td style="text-align: right;"> Enabled </td>
</tr>
<tr>
 <td style="text-align: right;">
  Root Mean Square Error </td><td style="text-align: right;"> "RMSE" </td><td style="text-align: right;"> Enabled </td>
</tr>
<tr>
 <td style="text-align: right;">
  Normalized RMSE (by target range) </td><td style="text-align: right;"> "NRMSE(RNG)" </td><td style="text-align: right;"> Disabled </td>
</tr>
<tr>
 <td style="text-align: right;">
  Normalized RMSE (by target IQR) </td><td style="text-align: right;"> "NRMSE(IQR)" </td><td style="text-align: right;"> Enabled </td>
</tr>
<tr>
 <td style="text-align: right;">
  Normalized RMSE (by target STD) </td><td style="text-align: right;"> "NRMSE(STD)" </td><td style="text-align: right;"> Disabled </td>
</tr>
<tr>
 <td style="text-align: right;">
  Normalized RMSE (by target mean) </td><td style="text-align: right;"> "NRMSE(AVG)" </td><td style="text-align: right;"> Disabled </td>
</tr>
<tr>
 <td style="text-align: right;">
  Relative Squared Error </td><td style="text-align: right;"> "RSE" </td><td style="text-align: right;"> Disabled </td>
</tr>
<tr>
 <td style="text-align: right;">
  Root Relative Squared Error </td><td style="text-align: right;"> "RRSE" </td><td style="text-align: right;"> Enabled </td>
</tr>
<tr>
 <td style="text-align: right;">
  Relative Absolute Error </td><td style="text-align: right;"> "RAE" </td><td style="text-align: right;"> Enabled </td>
</tr>
<tr>
 <td style="text-align: right;">
  Root Mean Squared Log Error </td><td style="text-align: right;"> "RMSLE" </td><td style="text-align: right;"> Enabled </td>
</tr>
<tr>
 <td style="text-align: right;">
  Mean Absolute Log Error </td><td style="text-align: right;"> "MALE" </td><td style="text-align: right;"> Disabled </td>
</tr>
<tr>
 <td style="text-align: right;">
  Mean Absolute Percentage Error </td><td style="text-align: right;"> "MAPE" </td><td style="text-align: right;"> Disabled </td>
</tr>
<tr>
 <td style="text-align: right;">
  Mean Squared Error </td><td style="text-align: right;"> "MSE" </td><td style="text-align: right;"> Disabled </td>
</tr>
<tr>
 <td style="text-align: right;">
  Total Absolute Error </td><td style="text-align: right;"> "TAE" </td><td style="text-align: right;"> Disabled </td>
</tr>
<tr>
 <td style="text-align: right;">
  Total Squared Error </td><td style="text-align: right;"> "TSE" </td><td style="text-align: right;"> Disabled </td>
</tr>
<tr>
 <td style="text-align: right;">
 </td>
</tr>

</table>

<p>The <strong>Name</strong> column refers to the name used in the package.
This is the name in the output and when enabling/disabling in <code>`metrics`</code>.
</p>


<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>


<h3>See Also</h3>

<p>Other evaluation functions: 
<code><a href="#topic+binomial_metrics">binomial_metrics</a>()</code>,
<code><a href="#topic+confusion_matrix">confusion_matrix</a>()</code>,
<code><a href="#topic+evaluate">evaluate</a>()</code>,
<code><a href="#topic+gaussian_metrics">gaussian_metrics</a>()</code>,
<code><a href="#topic+multinomial_metrics">multinomial_metrics</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Attach packages
library(cvms)

data &lt;- data.frame(
  "targets" = rnorm(100, 14.7, 3.6),
  "predictions" = rnorm(100, 13.2, 4.6)
)

evaluate_residuals(
  data = data,
  target_col = "targets",
  prediction_col = "predictions"
)
</code></pre>

<hr>
<h2 id='font'>Create a list of font settings for plots</h2><span id='topic+font'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#experimental"><img src="../help/figures/lifecycle-experimental.svg" alt='[Experimental]' /></a>
</p>
<p>Creates a list of font settings for plotting with cvms plotting functions.
</p>
<p>NOTE: This is very experimental and will likely change.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>font(
  size = NULL,
  color = NULL,
  alpha = NULL,
  nudge_x = NULL,
  nudge_y = NULL,
  angle = NULL,
  family = NULL,
  fontface = NULL,
  hjust = NULL,
  vjust = NULL,
  lineheight = NULL,
  digits = NULL,
  prefix = NULL,
  suffix = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="font_+3A_size">size</code>, <code id="font_+3A_color">color</code>, <code id="font_+3A_alpha">alpha</code>, <code id="font_+3A_nudge_x">nudge_x</code>, <code id="font_+3A_nudge_y">nudge_y</code>, <code id="font_+3A_angle">angle</code>, <code id="font_+3A_family">family</code>, <code id="font_+3A_fontface">fontface</code>, <code id="font_+3A_hjust">hjust</code>, <code id="font_+3A_vjust">vjust</code>, <code id="font_+3A_lineheight">lineheight</code></td>
<td>
<p>As passed to
<code><a href="ggplot2.html#topic+geom_text">ggplot2::geom_text</a></code>.</p>
</td></tr>
<tr><td><code id="font_+3A_digits">digits</code></td>
<td>
<p>Number of digits to round to. If negative, no rounding will take place.</p>
</td></tr>
<tr><td><code id="font_+3A_prefix">prefix</code></td>
<td>
<p>A string prefix.</p>
</td></tr>
<tr><td><code id="font_+3A_suffix">suffix</code></td>
<td>
<p>A string suffix.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of settings.
</p>


<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>


<h3>See Also</h3>

<p>Other plotting functions: 
<code><a href="#topic+plot_confusion_matrix">plot_confusion_matrix</a>()</code>,
<code><a href="#topic+plot_metric_density">plot_metric_density</a>()</code>,
<code><a href="#topic+plot_probabilities">plot_probabilities</a>()</code>,
<code><a href="#topic+plot_probabilities_ecdf">plot_probabilities_ecdf</a>()</code>,
<code><a href="#topic+sum_tile_settings">sum_tile_settings</a>()</code>
</p>

<hr>
<h2 id='gaussian_metrics'>Select metrics for Gaussian evaluation</h2><span id='topic+gaussian_metrics'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#experimental"><img src="../help/figures/lifecycle-experimental.svg" alt='[Experimental]' /></a>
</p>
<p>Enable/disable metrics for Gaussian evaluation. Can be supplied to the
<code>`metrics`</code> argument in many of the <code>cvms</code> functions.
</p>
<p>Note: Some functions may have slightly different defaults than the ones supplied here.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gaussian_metrics(
  all = NULL,
  rmse = NULL,
  mae = NULL,
  nrmse_rng = NULL,
  nrmse_iqr = NULL,
  nrmse_std = NULL,
  nrmse_avg = NULL,
  rae = NULL,
  rse = NULL,
  rrse = NULL,
  rmsle = NULL,
  male = NULL,
  mape = NULL,
  mse = NULL,
  tae = NULL,
  tse = NULL,
  r2m = NULL,
  r2c = NULL,
  aic = NULL,
  aicc = NULL,
  bic = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gaussian_metrics_+3A_all">all</code></td>
<td>
<p>Enable/disable all arguments at once. (Logical)
</p>
<p>Specifying other metrics will overwrite this, why you can
use (<code>all = FALSE, rmse = TRUE</code>) to get only the <code>RMSE</code> metric.</p>
</td></tr>
<tr><td><code id="gaussian_metrics_+3A_rmse">rmse</code></td>
<td>
<p><code>RMSE</code>. (Default: TRUE)
</p>
<p>Root Mean Square Error.</p>
</td></tr>
<tr><td><code id="gaussian_metrics_+3A_mae">mae</code></td>
<td>
<p><code>MAE</code>. (Default: TRUE)
</p>
<p>Mean Absolute Error.</p>
</td></tr>
<tr><td><code id="gaussian_metrics_+3A_nrmse_rng">nrmse_rng</code></td>
<td>
<p><code>NRMSE(RNG)</code>. (Default: FALSE)
</p>
<p>Normalized Root Mean Square Error (by target range).</p>
</td></tr>
<tr><td><code id="gaussian_metrics_+3A_nrmse_iqr">nrmse_iqr</code></td>
<td>
<p><code>NRMSE(IQR)</code>. (Default: TRUE)
</p>
<p>Normalized Root Mean Square Error (by target interquartile range).</p>
</td></tr>
<tr><td><code id="gaussian_metrics_+3A_nrmse_std">nrmse_std</code></td>
<td>
<p><code>NRMSE(STD)</code>. (Default: FALSE)
</p>
<p>Normalized Root Mean Square Error (by target standard deviation).</p>
</td></tr>
<tr><td><code id="gaussian_metrics_+3A_nrmse_avg">nrmse_avg</code></td>
<td>
<p><code>NRMSE(AVG)</code>. (Default: FALSE)
</p>
<p>Normalized Root Mean Square Error (by target mean).</p>
</td></tr>
<tr><td><code id="gaussian_metrics_+3A_rae">rae</code></td>
<td>
<p><code>RAE</code>. (Default: TRUE)
</p>
<p>Relative Absolute Error.</p>
</td></tr>
<tr><td><code id="gaussian_metrics_+3A_rse">rse</code></td>
<td>
<p><code>RSE</code>. (Default: FALSE)
</p>
<p>Relative Squared Error.</p>
</td></tr>
<tr><td><code id="gaussian_metrics_+3A_rrse">rrse</code></td>
<td>
<p><code>RRSE</code>. (Default: TRUE)
</p>
<p>Root Relative Squared Error.</p>
</td></tr>
<tr><td><code id="gaussian_metrics_+3A_rmsle">rmsle</code></td>
<td>
<p><code>RMSLE</code>. (Default: TRUE)
</p>
<p>Root Mean Square Log Error.</p>
</td></tr>
<tr><td><code id="gaussian_metrics_+3A_male">male</code></td>
<td>
<p><code>MALE</code>. (Default: FALSE)
</p>
<p>Mean Absolute Log Error.</p>
</td></tr>
<tr><td><code id="gaussian_metrics_+3A_mape">mape</code></td>
<td>
<p><code>MAPE</code>. (Default: FALSE)
</p>
<p>Mean Absolute Percentage Error.</p>
</td></tr>
<tr><td><code id="gaussian_metrics_+3A_mse">mse</code></td>
<td>
<p><code>MSE</code>. (Default: FALSE)
</p>
<p>Mean Square Error.</p>
</td></tr>
<tr><td><code id="gaussian_metrics_+3A_tae">tae</code></td>
<td>
<p><code>TAE</code>. (Default: FALSE)
</p>
<p>Total Absolute Error</p>
</td></tr>
<tr><td><code id="gaussian_metrics_+3A_tse">tse</code></td>
<td>
<p><code>TSE</code>. (Default: FALSE)
</p>
<p>Total Squared Error.</p>
</td></tr>
<tr><td><code id="gaussian_metrics_+3A_r2m">r2m</code></td>
<td>
<p><code>r2m</code>. (Default: FALSE)
</p>
<p>Marginal R-squared.</p>
</td></tr>
<tr><td><code id="gaussian_metrics_+3A_r2c">r2c</code></td>
<td>
<p><code>r2c</code>. (Default: FALSE)
</p>
<p>Conditional R-squared.</p>
</td></tr>
<tr><td><code id="gaussian_metrics_+3A_aic">aic</code></td>
<td>
<p><code>AIC</code>. (Default: FALSE)
</p>
<p>Akaike Information Criterion.</p>
</td></tr>
<tr><td><code id="gaussian_metrics_+3A_aicc">aicc</code></td>
<td>
<p><code>AICc</code>. (Default: FALSE)
</p>
<p>Corrected Akaike Information Criterion.</p>
</td></tr>
<tr><td><code id="gaussian_metrics_+3A_bic">bic</code></td>
<td>
<p><code>BIC</code>. (Default: FALSE)
</p>
<p>Bayesian Information Criterion.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>


<h3>See Also</h3>

<p>Other evaluation functions: 
<code><a href="#topic+binomial_metrics">binomial_metrics</a>()</code>,
<code><a href="#topic+confusion_matrix">confusion_matrix</a>()</code>,
<code><a href="#topic+evaluate">evaluate</a>()</code>,
<code><a href="#topic+evaluate_residuals">evaluate_residuals</a>()</code>,
<code><a href="#topic+multinomial_metrics">multinomial_metrics</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Attach packages
library(cvms)

# Enable only RMSE
gaussian_metrics(all = FALSE, rmse = TRUE)

# Enable all but RMSE
gaussian_metrics(all = TRUE, rmse = FALSE)

# Disable RMSE
gaussian_metrics(rmse = FALSE)

</code></pre>

<hr>
<h2 id='model_functions'>Examples of model_fn functions</h2><span id='topic+model_functions'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#experimental"><img src="../help/figures/lifecycle-experimental.svg" alt='[Experimental]' /></a>
</p>
<p>Examples of model functions that can be used in
<code><a href="#topic+cross_validate_fn">cross_validate_fn()</a></code>.
They can either be used directly or be starting points.
</p>
<p>The <code><a href="#topic+update_hyperparameters">update_hyperparameters()</a></code> function
updates the list of hyperparameters with default values for missing hyperparameters.
You can also specify required hyperparameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>model_functions(name)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="model_functions_+3A_name">name</code></td>
<td>
<p>Name of model to get model function for,
as it appears in the following list:
</p>

<table>
<tr>
 <td style="text-align: right;">
  <strong>Name</strong> </td><td style="text-align: right;"> <strong>Function</strong> </td><td style="text-align: right;"> <strong>Hyperparameters (default)</strong> </td>
</tr>
<tr>
 <td style="text-align: right;">
  "lm" </td><td style="text-align: right;"> <code><a href="stats.html#topic+lm">stats::lm()</a></code> </td><td style="text-align: right;"> </td>
</tr>
<tr>
 <td style="text-align: right;">
  "lmer" </td><td style="text-align: right;"> <code><a href="lme4.html#topic+lmer">lme4::lmer()</a></code> </td><td style="text-align: right;"> <code>REML (FALSE)</code> </td>
</tr>
<tr>
 <td style="text-align: right;">
  "glm_binomial" </td><td style="text-align: right;"> <code><a href="stats.html#topic+lm">stats::glm()</a></code> </td><td style="text-align: right;"> </td>
</tr>
<tr>
 <td style="text-align: right;">
  "glmer_binomial" </td><td style="text-align: right;"> <code><a href="lme4.html#topic+glmer">lme4::glmer()</a></code> </td><td style="text-align: right;"> </td>
</tr>
<tr>
 <td style="text-align: right;">
  "svm_gaussian" </td><td style="text-align: right;"> <code><a href="e1071.html#topic+svm">e1071::svm()</a></code> </td><td style="text-align: right;"> <code>kernel ("radial")</code>, <code>cost (1)</code></td>
</tr>
<tr>
 <td style="text-align: right;">
  "svm_binomial" </td><td style="text-align: right;"> <code><a href="e1071.html#topic+svm">e1071::svm()</a></code> </td><td style="text-align: right;"> <code>kernel ("radial")</code>, <code>cost (1)</code></td>
</tr>
<tr>
 <td style="text-align: right;">
  "svm_multinomial" </td><td style="text-align: right;"> <code><a href="e1071.html#topic+svm">e1071::svm()</a></code> </td><td style="text-align: right;"> <code>kernel ("radial")</code>, <code>cost (1)</code></td>
</tr>
<tr>
 <td style="text-align: right;">
  "naive_bayes" </td><td style="text-align: right;"> <code><a href="e1071.html#topic+naiveBayes">e1071::naiveBayes()</a></code> </td><td style="text-align: right;"> <code>laplace (0)</code> </td>
</tr>
<tr>
 <td style="text-align: right;">
  </td>
</tr>

</table>
</td></tr>
</table>


<h3>Value</h3>

<p>A function with the following form:
</p>
<p><code>function(train_data, formula, hyperparameters) {</code>
</p>
<p><code style="white-space: pre;">&#8288;    &#8288;</code><code># Return fitted model object</code>
</p>
<p><code>}</code>
</p>


<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>


<h3>See Also</h3>

<p>Other example functions: 
<code><a href="#topic+predict_functions">predict_functions</a>()</code>,
<code><a href="#topic+preprocess_functions">preprocess_functions</a>()</code>,
<code><a href="#topic+update_hyperparameters">update_hyperparameters</a>()</code>
</p>

<hr>
<h2 id='most_challenging'>Find the data points that were hardest to predict</h2><span id='topic+most_challenging'></span><span id='topic+hardest'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#experimental"><img src="../help/figures/lifecycle-experimental.svg" alt='[Experimental]' /></a>
Finds the data points that, overall, were the most challenging to predict,
based on a prediction metric.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>most_challenging(
  data,
  type,
  obs_id_col = "Observation",
  target_col = "Target",
  prediction_cols = ifelse(type == "gaussian", "Prediction", "Predicted Class"),
  threshold = 0.15,
  threshold_is = "percentage",
  metric = NULL,
  cutoff = 0.5
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="most_challenging_+3A_data">data</code></td>
<td>
<p><code>data.frame</code> with predictions, targets and observation IDs.
Can be grouped by <code><a href="dplyr.html#topic+group_by">dplyr::group_by()</a></code>.
</p>
<p>Predictions can be passed as values, predicted classes or predicted probabilities:
</p>
<p><strong>N.B.</strong> Adds <code><a href="base.html#topic+.Machine">.Machine$double.eps</a></code> to all probabilities to avoid <code>log(0)</code>.
</p>


<h4>Multinomial</h4>

<p>When <code>`type`</code> is <code>"multinomial"</code>, the predictions can be passed in one of two formats.
</p>


<h5>Probabilities (Preferable)</h5>

<p>One column per class with the probability of that class.
The columns should have the name of their class,
as they are named in the target column. E.g.:
</p>

<table>
<tr>
 <td style="text-align: right;">
  <strong>class_1</strong> </td><td style="text-align: right;"> <strong>class_2</strong> </td><td style="text-align: right;">
  <strong>class_3</strong> </td><td style="text-align: right;"> <strong>target</strong></td>
</tr>
<tr>
 <td style="text-align: right;">
  0.269 </td><td style="text-align: right;"> 0.528 </td><td style="text-align: right;"> 0.203 </td><td style="text-align: right;"> class_2</td>
</tr>
<tr>
 <td style="text-align: right;">
  0.368 </td><td style="text-align: right;"> 0.322 </td><td style="text-align: right;"> 0.310 </td><td style="text-align: right;"> class_3</td>
</tr>
<tr>
 <td style="text-align: right;">
  0.375 </td><td style="text-align: right;"> 0.371 </td><td style="text-align: right;"> 0.254 </td><td style="text-align: right;"> class_2</td>
</tr>
<tr>
 <td style="text-align: right;">
  ... </td><td style="text-align: right;"> ... </td><td style="text-align: right;"> ... </td><td style="text-align: right;"> ...</td>
</tr>

</table>




<h5>Classes</h5>

<p>A single column of type <code>character</code> with the predicted classes. E.g.:
</p>

<table>
<tr>
 <td style="text-align: right;">
  <strong>prediction</strong> </td><td style="text-align: right;"> <strong>target</strong></td>
</tr>
<tr>
 <td style="text-align: right;">
  class_2 </td><td style="text-align: right;"> class_2</td>
</tr>
<tr>
 <td style="text-align: right;">
  class_1 </td><td style="text-align: right;"> class_3</td>
</tr>
<tr>
 <td style="text-align: right;">
  class_1 </td><td style="text-align: right;"> class_2</td>
</tr>
<tr>
 <td style="text-align: right;">
  ... </td><td style="text-align: right;"> ...</td>
</tr>

</table>





<h4>Binomial</h4>

<p>When <code>`type`</code> is <code>"binomial"</code>, the predictions can be passed in one of two formats.
</p>


<h5>Probabilities (Preferable)</h5>

<p>One column with the <strong>probability of class being
the second class alphabetically</strong>
(&quot;dog&quot; if classes are &quot;cat&quot; and &quot;dog&quot;). E.g.:
</p>

<table>
<tr>
 <td style="text-align: right;">
  <strong>prediction</strong> </td><td style="text-align: right;"> <strong>target</strong></td>
</tr>
<tr>
 <td style="text-align: right;">
  0.769 </td><td style="text-align: right;"> "dog"</td>
</tr>
<tr>
 <td style="text-align: right;">
  0.368 </td><td style="text-align: right;"> "dog"</td>
</tr>
<tr>
 <td style="text-align: right;">
  0.375 </td><td style="text-align: right;"> "cat"</td>
</tr>
<tr>
 <td style="text-align: right;">
  ... </td><td style="text-align: right;"> ...</td>
</tr>

</table>


<p>Note: At the alphabetical ordering of the class labels, they are of type <code>character</code>,
why e.g. <code>100</code> would come before <code>7</code>.
</p>


<h5>Classes</h5>

<p>A single column of type <code>character</code> with the predicted classes. E.g.:
</p>

<table>
<tr>
 <td style="text-align: right;">
  <strong>prediction</strong> </td><td style="text-align: right;"> <strong>target</strong></td>
</tr>
<tr>
 <td style="text-align: right;">
  class_0 </td><td style="text-align: right;"> class_1</td>
</tr>
<tr>
 <td style="text-align: right;">
  class_1 </td><td style="text-align: right;"> class_1</td>
</tr>
<tr>
 <td style="text-align: right;">
  class_1 </td><td style="text-align: right;"> class_0</td>
</tr>
<tr>
 <td style="text-align: right;">
  ... </td><td style="text-align: right;"> ...</td>
</tr>

</table>





<h4>Gaussian</h4>

<p>When <code>`type`</code> is <code>"gaussian"</code>, the predictions should be passed as
one column with the predicted values. E.g.:
</p>

<table>
<tr>
 <td style="text-align: right;">
  <strong>prediction</strong> </td><td style="text-align: right;"> <strong>target</strong></td>
</tr>
<tr>
 <td style="text-align: right;">
  28.9 </td><td style="text-align: right;"> 30.2</td>
</tr>
<tr>
 <td style="text-align: right;">
  33.2 </td><td style="text-align: right;"> 27.1</td>
</tr>
<tr>
 <td style="text-align: right;">
  23.4 </td><td style="text-align: right;"> 21.3</td>
</tr>
<tr>
 <td style="text-align: right;">
  ... </td><td style="text-align: right;"> ...</td>
</tr>

</table>

</td></tr>
<tr><td><code id="most_challenging_+3A_type">type</code></td>
<td>
<p>Type of task used to get the predictions:
</p>
<p><code>"gaussian"</code> for regression (like linear regression).
</p>
<p><code>"binomial"</code> for binary classification.
</p>
<p><code>"multinomial"</code> for multiclass classification.</p>
</td></tr>
<tr><td><code id="most_challenging_+3A_obs_id_col">obs_id_col</code></td>
<td>
<p>Name of column with observation IDs. This will be used to aggregate
the performance of each observation.</p>
</td></tr>
<tr><td><code id="most_challenging_+3A_target_col">target_col</code></td>
<td>
<p>Name of column with the true classes/values in <code>`data`</code>.</p>
</td></tr>
<tr><td><code id="most_challenging_+3A_prediction_cols">prediction_cols</code></td>
<td>
<p>Name(s) of column(s) with the predictions.</p>
</td></tr>
<tr><td><code id="most_challenging_+3A_threshold">threshold</code></td>
<td>
<p>Threshold to filter observations by. Depends on <code>`type`</code> and <code>`threshold_is`</code>.
</p>
<p>The <code>threshold</code> can either be a <strong>percentage</strong> or a <strong>score</strong>.
For percentages, a lower <code>threshold</code>
returns fewer observations. For scores, this depends on <code>`type`</code>.
</p>


<h4>Gaussian</h4>



<h5>threshold_is &quot;percentage&quot;</h5>

<p>(Approximate) percentage of the observations with the largest root mean square errors
to return.
</p>



<h5>threshold_is &quot;score&quot;</h5>

<p>Observations with a root mean square error larger than or equal to the <code>threshold</code> will be returned.
</p>




<h4>Binomial, Multinomial</h4>



<h5>threshold_is &quot;percentage&quot;</h5>

<p>(Approximate) percentage of the observations to return with:
</p>
<p><code>MAE</code>, <code>Cross Entropy</code>: Highest error scores.
</p>
<p><code>Accuracy</code>: Lowest accuracies
</p>



<h5>threshold_is &quot;score&quot;</h5>

<p><code>MAE</code>, <code>Cross Entropy</code>: Observations with an error score above or equal to the threshold will be returned.
</p>
<p><code>Accuracy</code>: Observations with an accuracy below or equal to the threshold will be returned.
</p>

</td></tr>
<tr><td><code id="most_challenging_+3A_threshold_is">threshold_is</code></td>
<td>
<p>Either <code>"score"</code> or <code>"percentage"</code>. See <code>`threshold`</code>.</p>
</td></tr>
<tr><td><code id="most_challenging_+3A_metric">metric</code></td>
<td>
<p>The metric to use. If <code>NULL</code>,
the default metric depends on the format of the prediction columns.
</p>


<h4>Binomial, Multinomial</h4>

<p><code>"Accuracy"</code>, <code>"MAE"</code> or <code>"Cross Entropy"</code>.
</p>
<p>When <em>one</em> prediction column with predicted <em>classes</em> is passed,
the default is <code>"Accuracy"</code>.
In this configuration, the other metrics are not calculated.
</p>
<p>When <em>one or more</em> prediction columns with predicted <em>probabilities</em> are passed,
the default is <code>"MAE"</code>. This is the Mean Absolute Error of the
probability of the target class.
</p>



<h4>Gaussian</h4>

<p>Ignored. Always uses <code>"RMSE"</code>.
</p>
</td></tr>
<tr><td><code id="most_challenging_+3A_cutoff">cutoff</code></td>
<td>
<p>Threshold for predicted classes. (Numeric)
</p>
<p>N.B. <strong>Binomial only</strong>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>data.frame</code> with the most challenging observations and their metrics.
</p>
<p><code>`&gt;=` / `&lt;=`</code> denotes the threshold as score.
</p>


<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Attach packages
library(cvms)
library(dplyr)

##
## Multinomial
##

# Find the most challenging data points (per classifier)
# in the predicted.musicians dataset
# which resembles the "Predictions" tibble from the evaluation results

# Passing predicted probabilities
# Observations with 30% highest MAE scores
most_challenging(
  predicted.musicians,
  obs_id_col = "ID",
  prediction_cols = c("A", "B", "C", "D"),
  type = "multinomial",
  threshold = 0.30
)

# Observations with 25% highest Cross Entropy scores
most_challenging(
  predicted.musicians,
  obs_id_col = "ID",
  prediction_cols = c("A", "B", "C", "D"),
  type = "multinomial",
  threshold = 0.25,
  metric = "Cross Entropy"
)

# Passing predicted classes
# Observations with 30% lowest Accuracy scores
most_challenging(
  predicted.musicians,
  obs_id_col = "ID",
  prediction_cols = "Predicted Class",
  type = "multinomial",
  threshold = 0.30
)

# The 40% lowest-scoring on accuracy per classifier
predicted.musicians %&gt;%
  dplyr::group_by(Classifier) %&gt;%
  most_challenging(
    obs_id_col = "ID",
    prediction_cols = "Predicted Class",
    type = "multinomial",
    threshold = 0.40
  )

# Accuracy scores below 0.05
most_challenging(
  predicted.musicians,
  obs_id_col = "ID",
  type = "multinomial",
  threshold = 0.05,
  threshold_is = "score"
)

##
## Binomial
##

# Subset the predicted.musicians
binom_data &lt;- predicted.musicians %&gt;%
  dplyr::filter(Target %in% c("A","B")) %&gt;%
  dplyr::rename(Prediction = B)

# Passing probabilities
# Observations with 30% highest MAE
most_challenging(
  binom_data,
  obs_id_col = "ID",
  type = "binomial",
  prediction_cols = "Prediction",
  threshold = 0.30
)

# Observations with 30% highest Cross Entropy
most_challenging(
  binom_data,
  obs_id_col = "ID",
  type = "binomial",
  prediction_cols = "Prediction",
  threshold = 0.30,
  metric = "Cross Entropy"
)

# Passing predicted classes
# Observations with 30% lowest Accuracy scores
most_challenging(
  binom_data,
  obs_id_col = "ID",
  type = "binomial",
  prediction_cols = "Predicted Class",
  threshold = 0.30
)

##
## Gaussian
##

set.seed(1)

df &lt;- data.frame(
  "Observation" = rep(1:10, n = 3),
  "Target" = rnorm(n = 30, mean = 25, sd = 5),
  "Prediction" = rnorm(n = 30, mean = 27, sd = 7)
)

# The 20% highest RMSE scores
most_challenging(
  df,
  type = "gaussian",
  threshold = 0.2
)

# RMSE scores above 9
most_challenging(
  df,
  type = "gaussian",
  threshold = 9,
  threshold_is = "score"
)

</code></pre>

<hr>
<h2 id='multiclass_probability_tibble'>Generate a multiclass probability tibble</h2><span id='topic+multiclass_probability_tibble'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#maturing"><img src="../help/figures/lifecycle-maturing.svg" alt='[Maturing]' /></a>
</p>
<p>Generate a <code>tibble</code> with random numbers containing one column per specified class.
When the softmax function is applied, the numbers become probabilities that sum to <code>1</code> row-wise.
Optionally, add columns with targets and predicted classes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multiclass_probability_tibble(
  num_classes,
  num_observations,
  apply_softmax = TRUE,
  FUN = runif,
  class_name = "class_",
  add_predicted_classes = FALSE,
  add_targets = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="multiclass_probability_tibble_+3A_num_classes">num_classes</code></td>
<td>
<p>The number of classes. Also the number of columns in the <code>tibble</code>.</p>
</td></tr>
<tr><td><code id="multiclass_probability_tibble_+3A_num_observations">num_observations</code></td>
<td>
<p>The number of observations. Also the number of rows in the <code>tibble</code>.</p>
</td></tr>
<tr><td><code id="multiclass_probability_tibble_+3A_apply_softmax">apply_softmax</code></td>
<td>
<p>Whether to apply the <code>softmax</code> function row-wise. This will transform the
numbers to probabilities that sum to <code>1</code> row-wise.</p>
</td></tr>
<tr><td><code id="multiclass_probability_tibble_+3A_fun">FUN</code></td>
<td>
<p>Function for generating random numbers.
The first argument must be the number of random numbers to generate,
as no other arguments are supplied.</p>
</td></tr>
<tr><td><code id="multiclass_probability_tibble_+3A_class_name">class_name</code></td>
<td>
<p>The prefix for the column names. The column index is appended.</p>
</td></tr>
<tr><td><code id="multiclass_probability_tibble_+3A_add_predicted_classes">add_predicted_classes</code></td>
<td>
<p>Whether to add a column with the predicted classes. (Logical)
</p>
<p>The class with the highest value is the predicted class.</p>
</td></tr>
<tr><td><code id="multiclass_probability_tibble_+3A_add_targets">add_targets</code></td>
<td>
<p>Whether to add a column with randomly selected target classes. (Logical)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Attach cvms
library(cvms)

# Create a tibble with 5 classes and 10 observations
# Apply softmax to make sure the probabilities sum to 1
multiclass_probability_tibble(
  num_classes = 5,
  num_observations = 10,
  apply_softmax = TRUE
)

# Using the rnorm function to generate the random numbers
multiclass_probability_tibble(
  num_classes = 5,
  num_observations = 10,
  apply_softmax = TRUE,
  FUN = rnorm
)

# Add targets and predicted classes
multiclass_probability_tibble(
  num_classes = 5,
  num_observations = 10,
  apply_softmax = TRUE,
  FUN = rnorm,
  add_predicted_classes = TRUE,
  add_targets = TRUE
)

# Creating a custom generator function that
# exponentiates the numbers to create more "certain" predictions
rcertain &lt;- function(n) {
  (runif(n, min = 1, max = 100)^1.4) / 100
}
multiclass_probability_tibble(
  num_classes = 5,
  num_observations = 10,
  apply_softmax = TRUE,
  FUN = rcertain
)

</code></pre>

<hr>
<h2 id='multinomial_metrics'>Select metrics for multinomial evaluation</h2><span id='topic+multinomial_metrics'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#experimental"><img src="../help/figures/lifecycle-experimental.svg" alt='[Experimental]' /></a>
</p>
<p>Enable/disable metrics for multinomial evaluation. Can be supplied to the
<code>`metrics`</code> argument in many of the <code>cvms</code> functions.
</p>
<p>Note: Some functions may have slightly different defaults than the ones supplied here.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multinomial_metrics(
  all = NULL,
  overall_accuracy = NULL,
  balanced_accuracy = NULL,
  w_balanced_accuracy = NULL,
  accuracy = NULL,
  w_accuracy = NULL,
  f1 = NULL,
  w_f1 = NULL,
  sensitivity = NULL,
  w_sensitivity = NULL,
  specificity = NULL,
  w_specificity = NULL,
  pos_pred_value = NULL,
  w_pos_pred_value = NULL,
  neg_pred_value = NULL,
  w_neg_pred_value = NULL,
  auc = NULL,
  kappa = NULL,
  w_kappa = NULL,
  mcc = NULL,
  detection_rate = NULL,
  w_detection_rate = NULL,
  detection_prevalence = NULL,
  w_detection_prevalence = NULL,
  prevalence = NULL,
  w_prevalence = NULL,
  false_neg_rate = NULL,
  w_false_neg_rate = NULL,
  false_pos_rate = NULL,
  w_false_pos_rate = NULL,
  false_discovery_rate = NULL,
  w_false_discovery_rate = NULL,
  false_omission_rate = NULL,
  w_false_omission_rate = NULL,
  threat_score = NULL,
  w_threat_score = NULL,
  aic = NULL,
  aicc = NULL,
  bic = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="multinomial_metrics_+3A_all">all</code></td>
<td>
<p>Enable/disable all arguments at once. (Logical)
</p>
<p>Specifying other metrics will overwrite this, why you can
use (<code>all = FALSE, accuracy = TRUE</code>) to get only the Accuracy metric.</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_overall_accuracy">overall_accuracy</code></td>
<td>
<p><code>Overall Accuracy</code> (Default: TRUE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_balanced_accuracy">balanced_accuracy</code></td>
<td>
<p><code>Macro Balanced Accuracy</code> (Default: TRUE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_w_balanced_accuracy">w_balanced_accuracy</code></td>
<td>
<p><code>Weighted Balanced Accuracy</code> (Default: FALSE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_accuracy">accuracy</code></td>
<td>
<p><code>Accuracy</code> (Default: FALSE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_w_accuracy">w_accuracy</code></td>
<td>
<p><code>Weighted Accuracy</code> (Default: FALSE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_f1">f1</code></td>
<td>
<p><code>F1</code> (Default: TRUE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_w_f1">w_f1</code></td>
<td>
<p><code>Weighted F1</code> (Default: FALSE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_sensitivity">sensitivity</code></td>
<td>
<p><code>Sensitivity</code> (Default: TRUE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_w_sensitivity">w_sensitivity</code></td>
<td>
<p><code>Weighted Sensitivity</code> (Default: FALSE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_specificity">specificity</code></td>
<td>
<p><code>Specificity</code> (Default: TRUE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_w_specificity">w_specificity</code></td>
<td>
<p><code>Weighted Specificity</code> (Default: FALSE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_pos_pred_value">pos_pred_value</code></td>
<td>
<p><code>Pos Pred Value</code> (Default: TRUE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_w_pos_pred_value">w_pos_pred_value</code></td>
<td>
<p><code>Weighted Pos Pred Value</code> (Default: FALSE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_neg_pred_value">neg_pred_value</code></td>
<td>
<p><code>Neg Pred Value</code> (Default: TRUE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_w_neg_pred_value">w_neg_pred_value</code></td>
<td>
<p><code>Weighted Neg Pred Value</code> (Default: FALSE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_auc">auc</code></td>
<td>
<p><code>AUC</code> (Default: FALSE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_kappa">kappa</code></td>
<td>
<p><code>Kappa</code> (Default: TRUE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_w_kappa">w_kappa</code></td>
<td>
<p><code>Weighted Kappa</code> (Default: FALSE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_mcc">mcc</code></td>
<td>
<p><code>MCC</code> (Default: TRUE)
</p>
<p>Multiclass Matthews Correlation Coefficient.</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_detection_rate">detection_rate</code></td>
<td>
<p><code>Detection Rate</code> (Default: TRUE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_w_detection_rate">w_detection_rate</code></td>
<td>
<p><code>Weighted Detection Rate</code> (Default: FALSE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_detection_prevalence">detection_prevalence</code></td>
<td>
<p><code>Detection Prevalence</code> (Default: TRUE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_w_detection_prevalence">w_detection_prevalence</code></td>
<td>
<p><code>Weighted Detection Prevalence</code> (Default: FALSE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_prevalence">prevalence</code></td>
<td>
<p><code>Prevalence</code> (Default: TRUE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_w_prevalence">w_prevalence</code></td>
<td>
<p><code>Weighted Prevalence</code> (Default: FALSE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_false_neg_rate">false_neg_rate</code></td>
<td>
<p><code>False Neg Rate</code> (Default: FALSE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_w_false_neg_rate">w_false_neg_rate</code></td>
<td>
<p><code>Weighted False Neg Rate</code> (Default: FALSE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_false_pos_rate">false_pos_rate</code></td>
<td>
<p><code>False Pos Rate</code> (Default: FALSE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_w_false_pos_rate">w_false_pos_rate</code></td>
<td>
<p><code>Weighted False Pos Rate</code> (Default: FALSE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_false_discovery_rate">false_discovery_rate</code></td>
<td>
<p><code>False Discovery Rate</code> (Default: FALSE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_w_false_discovery_rate">w_false_discovery_rate</code></td>
<td>
<p><code>Weighted False Discovery Rate</code> (Default: FALSE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_false_omission_rate">false_omission_rate</code></td>
<td>
<p><code>False Omission Rate</code> (Default: FALSE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_w_false_omission_rate">w_false_omission_rate</code></td>
<td>
<p><code>Weighted False Omission Rate</code> (Default: FALSE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_threat_score">threat_score</code></td>
<td>
<p><code>Threat Score</code> (Default: FALSE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_w_threat_score">w_threat_score</code></td>
<td>
<p><code>Weighted Threat Score</code> (Default: FALSE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_aic">aic</code></td>
<td>
<p>AIC. (Default: FALSE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_aicc">aicc</code></td>
<td>
<p>AICc. (Default: FALSE)</p>
</td></tr>
<tr><td><code id="multinomial_metrics_+3A_bic">bic</code></td>
<td>
<p>BIC. (Default: FALSE)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>


<h3>See Also</h3>

<p>Other evaluation functions: 
<code><a href="#topic+binomial_metrics">binomial_metrics</a>()</code>,
<code><a href="#topic+confusion_matrix">confusion_matrix</a>()</code>,
<code><a href="#topic+evaluate">evaluate</a>()</code>,
<code><a href="#topic+evaluate_residuals">evaluate_residuals</a>()</code>,
<code><a href="#topic+gaussian_metrics">gaussian_metrics</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Attach packages
library(cvms)

# Enable only Balanced Accuracy
multinomial_metrics(all = FALSE, balanced_accuracy = TRUE)

# Enable all but Balanced Accuracy
multinomial_metrics(all = TRUE, balanced_accuracy = FALSE)

# Disable Balanced Accuracy
multinomial_metrics(balanced_accuracy = FALSE)

</code></pre>

<hr>
<h2 id='musicians'>Musician groups</h2><span id='topic+musicians'></span>

<h3>Description</h3>

<p>Made-up data on 60 musicians in 4 groups for multiclass classification.
</p>


<h3>Format</h3>

<p>A <code>data.frame</code> with <code>60</code> rows and <code>9</code> variables:
</p>

<dl>
<dt>ID</dt><dd><p>Musician identifier, 60 levels</p>
</dd>
<dt>Age</dt><dd><p>Age of the musician. Between 17 and 66 years.</p>
</dd>
<dt>Class</dt><dd><p>The class of the musician. One of <code>"A"</code>, <code>"B"</code>, <code>"C"</code>, and <code>"D"</code>.</p>
</dd>
<dt>Height</dt><dd><p>Height of the musician. Between <code>146</code> and <code>196</code> centimeters.</p>
</dd>
<dt>Drums</dt><dd><p>Whether the musician plays drums. <code>0</code> = No, <code>1</code> = Yes.</p>
</dd>
<dt>Bass</dt><dd><p>Whether the musician plays bass. <code>0</code> = No, <code>1</code> = Yes.</p>
</dd>
<dt>Guitar</dt><dd><p>Whether the musician plays guitar. <code>0</code> = No, <code>1</code> = Yes.</p>
</dd>
<dt>Keys</dt><dd><p>Whether the musician plays keys. <code>0</code> = No, <code>1</code> = Yes.</p>
</dd>
<dt>Vocals</dt><dd><p>Whether the musician sings. <code>0</code> = No, <code>1</code> = Yes.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>


<h3>See Also</h3>

<p>predicted.musicians
</p>

<hr>
<h2 id='participant.scores'>Participant scores</h2><span id='topic+participant.scores'></span>

<h3>Description</h3>

<p>Made-up experiment data with 10 participants and two diagnoses.
Test scores for 3 sessions per participant, where participants improve their scores each session.
</p>


<h3>Format</h3>

<p>A <code>data.frame</code> with <code>30</code> rows and <code>5</code> variables:
</p>

<dl>
<dt>participant</dt><dd><p>participant identifier, 10 levels</p>
</dd>
<dt>age</dt><dd><p>age of the participant, in years</p>
</dd>
<dt>diagnosis</dt><dd><p>diagnosis of the participant, either 1 or 0</p>
</dd>
<dt>score</dt><dd><p>test score of the participant, on a 0-100 scale</p>
</dd>
<dt>session</dt><dd><p>testing session identifier, 1 to 3</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>

<hr>
<h2 id='plot_confusion_matrix'>Plot a confusion matrix</h2><span id='topic+plot_confusion_matrix'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#experimental"><img src="../help/figures/lifecycle-experimental.svg" alt='[Experimental]' /></a>
</p>
<p>Creates a <code><a href="ggplot2.html#topic+ggplot">ggplot2</a></code> object representing a confusion matrix with counts,
overall percentages, row percentages and column percentages. An extra row and column with sum tiles and the
total count can be added.
</p>
<p>The confusion matrix can be created with <code><a href="#topic+evaluate">evaluate()</a></code>. See <code>`Examples`</code>.
</p>
<p>While this function is intended to be very flexible (hence the large number of arguments),
the defaults should work in most cases for most users. See the <code>Examples</code>.
</p>
<p><strong>NEW</strong>: Our
<a href="https://huggingface.co/spaces/ludvigolsen/plot_confusion_matrix">
<strong>Plot Confusion Matrix</strong> web application</a>
allows using this function without code. Select from multiple design templates
or make your own.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_confusion_matrix(
  conf_matrix,
  target_col = "Target",
  prediction_col = "Prediction",
  counts_col = "N",
  sub_col = NULL,
  class_order = NULL,
  add_sums = FALSE,
  add_counts = TRUE,
  add_normalized = TRUE,
  add_row_percentages = TRUE,
  add_col_percentages = TRUE,
  diag_percentages_only = FALSE,
  rm_zero_percentages = TRUE,
  rm_zero_text = TRUE,
  add_zero_shading = TRUE,
  amount_3d_effect = 1,
  add_arrows = TRUE,
  counts_on_top = FALSE,
  palette = "Blues",
  intensity_by = "counts",
  intensity_lims = NULL,
  intensity_beyond_lims = "truncate",
  theme_fn = ggplot2::theme_minimal,
  place_x_axis_above = TRUE,
  rotate_y_text = TRUE,
  digits = 1,
  font_counts = font(),
  font_normalized = font(),
  font_row_percentages = font(),
  font_col_percentages = font(),
  arrow_size = 0.048,
  arrow_nudge_from_text = 0.065,
  tile_border_color = NA,
  tile_border_size = 0.1,
  tile_border_linetype = "solid",
  sums_settings = sum_tile_settings(),
  darkness = 0.8
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_confusion_matrix_+3A_conf_matrix">conf_matrix</code></td>
<td>
<p>Confusion matrix <code>tibble</code> with each combination of
targets and predictions along with their counts.
</p>
<p>E.g. for a binary classification:
</p>

<table>
<tr>
 <td style="text-align: right;">
  <strong>Target</strong> </td><td style="text-align: right;"> <strong>Prediction</strong> </td><td style="text-align: right;"> <strong>N</strong> </td>
</tr>
<tr>
 <td style="text-align: right;">
  class_1 </td><td style="text-align: right;"> class_1 </td><td style="text-align: right;"> 5 </td>
</tr>
<tr>
 <td style="text-align: right;">
  class_1 </td><td style="text-align: right;"> class_2 </td><td style="text-align: right;"> 9 </td>
</tr>
<tr>
 <td style="text-align: right;">
  class_2 </td><td style="text-align: right;"> class_1 </td><td style="text-align: right;"> 3 </td>
</tr>
<tr>
 <td style="text-align: right;">
  class_2 </td><td style="text-align: right;"> class_2 </td><td style="text-align: right;"> 2 </td>
</tr>
<tr>
 <td style="text-align: right;">
 </td>
</tr>

</table>

<p>As created with the various evaluation functions in <code>cvms</code>, like
<code><a href="#topic+evaluate">evaluate()</a></code>.
</p>
<p>An additional <code>`sub_col`</code> column (<code>character</code>) can be specified
as well. Its content will replace the bottom text ('counts' by default or
'normalized' when <code>`counts_on_top`</code> is enabled).
</p>
<p><strong>Note</strong>: If you supply the results from <code><a href="#topic+evaluate">evaluate()</a></code>
or <code><a href="#topic+confusion_matrix">confusion_matrix()</a></code> directly,
the confusion matrix <code>tibble</code> is extracted automatically, if possible.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_target_col">target_col</code></td>
<td>
<p>Name of column with target levels.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_prediction_col">prediction_col</code></td>
<td>
<p>Name of column with prediction levels.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_counts_col">counts_col</code></td>
<td>
<p>Name of column with a count for each combination
of the target and prediction levels.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_sub_col">sub_col</code></td>
<td>
<p>Name of column with text to replace the bottom text
('counts' by default or 'normalized' when <code>`counts_on_top`</code> is enabled).
</p>
<p>It simply replaces the text, so all settings will still be called
e.g. <code>`font_counts`</code> etc. When other settings make it so, that no
bottom text is displayed (e.g. <code>`add_counts` = FALSE</code>),
this text is not displayed either.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_class_order">class_order</code></td>
<td>
<p>Names of the classes in <code>`conf_matrix`</code> in the desired order.
When <code>NULL</code>, the classes are ordered alphabetically.
Note that the entire set of unique classes from both <code>`target_col`</code>
and <code>`prediction_col`</code> must be specified.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_add_sums">add_sums</code></td>
<td>
<p>Add tiles with the row/column sums. Also adds a total count tile. (Logical)
</p>
<p>The appearance of these tiles can be specified in <code>`sums_settings`</code>.
</p>
<p>Note: Adding the sum tiles with a palette requires the <code>ggnewscale</code> package.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_add_counts">add_counts</code></td>
<td>
<p>Add the counts to the middle of the tiles. (Logical)</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_add_normalized">add_normalized</code></td>
<td>
<p>Normalize the counts to percentages and
add to the middle of the tiles. (Logical)</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_add_row_percentages">add_row_percentages</code></td>
<td>
<p>Add the row percentages,
i.e. how big a part of its row the tile makes up. (Logical)
</p>
<p>By default, the row percentage is placed to the right of the tile, rotated 90 degrees.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_add_col_percentages">add_col_percentages</code></td>
<td>
<p>Add the column percentages,
i.e. how big a part of its column the tile makes up. (Logical)
</p>
<p>By default, the row percentage is placed at the bottom of the tile.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_diag_percentages_only">diag_percentages_only</code></td>
<td>
<p>Whether to only have row and column percentages in the diagonal tiles. (Logical)</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_rm_zero_percentages">rm_zero_percentages</code></td>
<td>
<p>Whether to remove row and column percentages when the count is <code>0</code>. (Logical)</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_rm_zero_text">rm_zero_text</code></td>
<td>
<p>Whether to remove counts and normalized percentages when the count is <code>0</code>. (Logical)</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_add_zero_shading">add_zero_shading</code></td>
<td>
<p>Add image of skewed lines to zero-tiles. (Logical)
</p>
<p>Note: Adding the zero-shading requires the <code>rsvg</code> and <code>ggimage</code> packages.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_amount_3d_effect">amount_3d_effect</code></td>
<td>
<p>Amount of 3D effect (tile overlay) to add.
Passed as whole number from <code>0</code> (no effect) up to <code>6</code> (biggest effect).
This helps separate tiles with the same intensities.
</p>
<p>Note: The overlay may not fit the tiles in many-class cases that haven't been tested.
If the boxes do not overlap properly, simply turn it off.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_add_arrows">add_arrows</code></td>
<td>
<p>Add the arrows to the row and col percentages. (Logical)
</p>
<p>Note: Adding the arrows requires the <code>rsvg</code> and <code>ggimage</code> packages.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_counts_on_top">counts_on_top</code></td>
<td>
<p>Switch the counts and normalized counts,
such that the counts are on top. (Logical)</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_palette">palette</code></td>
<td>
<p>Color scheme. Passed directly to <code>`palette`</code> in
<code><a href="ggplot2.html#topic+scale_fill_distiller">ggplot2::scale_fill_distiller</a></code>.
</p>
<p>Try these palettes: <code>"Greens"</code>, <code>"Oranges"</code>,
<code>"Greys"</code>, <code>"Purples"</code>, <code>"Reds"</code>,
as well as the default <code>"Blues"</code>.
</p>
<p>Alternatively, pass a named list with limits of a custom gradient as e.g.
<code>`list("low"="#B1F9E8", "high"="#239895")`</code>. These are passed to
<code><a href="ggplot2.html#topic+scale_fill_gradient">ggplot2::scale_fill_gradient</a></code>.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_intensity_by">intensity_by</code></td>
<td>
<p>The measure that should control the color intensity of the tiles.
Either <code>`counts`</code>, <code>`normalized`</code> or one of <code>`log counts`,
 `log2 counts`, `log10 counts`, `arcsinh counts`</code>.
</p>
<p>For 'normalized', the color limits become <code>0-100</code> (except when
<code>`intensity_lims`</code> are specified), why the intensities
can better be compared across plots.
</p>
<p>For the 'log*' and 'arcsinh' versions, the log/arcsinh transformed counts are used.
</p>
<p><strong>Note</strong>: In 'log*' transformed counts, 0-counts are set to '0', why they
won't be distinguishable from 1-counts.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_intensity_lims">intensity_lims</code></td>
<td>
<p>A specific range of values for the color intensity of
the tiles. Given as a numeric vector with <code>c(min, max)</code>.
</p>
<p>This allows having the same intensity scale across plots for better comparison
of prediction sets.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_intensity_beyond_lims">intensity_beyond_lims</code></td>
<td>
<p>What to do with values beyond the
<code>`intensity_lims`</code>. One of <code>"truncate", "grey"</code>.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_theme_fn">theme_fn</code></td>
<td>
<p>The <code>ggplot2</code> theme function to apply.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_place_x_axis_above">place_x_axis_above</code></td>
<td>
<p>Move the x-axis text to the top and reverse the levels such that
the &quot;correct&quot; diagonal goes from top left to bottom right. (Logical)</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_rotate_y_text">rotate_y_text</code></td>
<td>
<p>Whether to rotate the y-axis text to
be vertical instead of horizontal. (Logical)</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_digits">digits</code></td>
<td>
<p>Number of digits to round to (percentages only).
Set to a negative number for no rounding.
</p>
<p>Can be set for each font individually via the <code>font_*</code> arguments.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_font_counts">font_counts</code></td>
<td>
<p><code>list</code> of font settings for the counts.
Can be provided with <code><a href="#topic+font">font()</a></code>.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_font_normalized">font_normalized</code></td>
<td>
<p><code>list</code> of font settings for the normalized counts.
Can be provided with <code><a href="#topic+font">font()</a></code>.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_font_row_percentages">font_row_percentages</code></td>
<td>
<p><code>list</code> of font settings for the row percentages.
Can be provided with <code><a href="#topic+font">font()</a></code>.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_font_col_percentages">font_col_percentages</code></td>
<td>
<p><code>list</code> of font settings for the column percentages.
Can be provided with <code><a href="#topic+font">font()</a></code>.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_arrow_size">arrow_size</code></td>
<td>
<p>Size of arrow icons. (Numeric)
</p>
<p>Is divided by <code>sqrt(nrow(conf_matrix))</code> and passed on
to <code><a href="ggimage.html#topic+geom_icon">ggimage::geom_icon()</a></code>.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_arrow_nudge_from_text">arrow_nudge_from_text</code></td>
<td>
<p>Distance from the percentage text to the arrow. (Numeric)</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_tile_border_color">tile_border_color</code></td>
<td>
<p>Color of the tile borders. Passed as <em><code>`colour`</code></em> to
<code><a href="ggplot2.html#topic+geom_tile">ggplot2::geom_tile</a></code>.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_tile_border_size">tile_border_size</code></td>
<td>
<p>Size of the tile borders. Passed as <em><code>`size`</code></em> to
<code><a href="ggplot2.html#topic+geom_tile">ggplot2::geom_tile</a></code>.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_tile_border_linetype">tile_border_linetype</code></td>
<td>
<p>Linetype for the tile borders. Passed as <em><code>`linetype`</code></em> to
<code><a href="ggplot2.html#topic+geom_tile">ggplot2::geom_tile</a></code>.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_sums_settings">sums_settings</code></td>
<td>
<p>A list of settings for the appearance of the sum tiles.
Can be provided with <code><a href="#topic+sum_tile_settings">sum_tile_settings()</a></code>.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_darkness">darkness</code></td>
<td>
<p>How dark the darkest colors should be, between <code>0</code> and <code>1</code>, where <code>1</code> is darkest.
</p>
<p>Technically, a lower value increases the upper limit in
<code><a href="ggplot2.html#topic+scale_fill_distiller">ggplot2::scale_fill_distiller</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Inspired by Antoine Sachet's answer at https://stackoverflow.com/a/53612391/11832955
</p>


<h3>Value</h3>

<p>A <code>ggplot2</code> object representing a confusion matrix.
Color intensity depends on either the counts (default) or the overall percentages.
</p>
<p>By default, each tile has the normalized count
(overall percentage) and count in the middle, the
column percentage at the bottom, and the
row percentage to the right and rotated 90 degrees.
</p>
<p>In the &quot;correct&quot; diagonal (upper left to bottom right, by default),
the column percentages are the class-level sensitivity scores,
while the row percentages are the class-level positive predictive values.
</p>


<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>


<h3>See Also</h3>

<p>Other plotting functions: 
<code><a href="#topic+font">font</a>()</code>,
<code><a href="#topic+plot_metric_density">plot_metric_density</a>()</code>,
<code><a href="#topic+plot_probabilities">plot_probabilities</a>()</code>,
<code><a href="#topic+plot_probabilities_ecdf">plot_probabilities_ecdf</a>()</code>,
<code><a href="#topic+sum_tile_settings">sum_tile_settings</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Attach cvms
library(cvms)
library(ggplot2)

# Two classes

# Create targets and predictions data frame
data &lt;- data.frame(
  "target" = c("A", "B", "A", "B", "A", "B", "A", "B",
               "A", "B", "A", "B", "A", "B", "A", "A"),
  "prediction" = c("B", "B", "A", "A", "A", "B", "B", "B",
                   "B", "B", "A", "B", "A", "A", "A", "A"),
  stringsAsFactors = FALSE
)

# Evaluate predictions and create confusion matrix
evaluation &lt;- evaluate(
  data = data,
  target_col = "target",
  prediction_cols = "prediction",
  type = "binomial"
)

# Inspect confusion matrix tibble
evaluation[["Confusion Matrix"]][[1]]

# Plot confusion matrix
# Supply confusion matrix tibble directly
plot_confusion_matrix(evaluation[["Confusion Matrix"]][[1]])
# Plot first confusion matrix in evaluate() output
plot_confusion_matrix(evaluation)

# Add sum tiles
plot_confusion_matrix(evaluation, add_sums = TRUE)

# Add labels to diagonal row and column percentages
# This example assumes "B" is the positive class
# but you could write anything as prefix to the percentages
plot_confusion_matrix(
    evaluation,
    font_row_percentages = font(prefix=c("NPV = ", "", "", "PPV = ")),
    font_col_percentages = font(prefix=c("Spec = ", "", "", "Sens = "))
)

# Three (or more) classes

# Create targets and predictions data frame
data &lt;- data.frame(
  "target" = c("A", "B", "C", "B", "A", "B", "C",
               "B", "A", "B", "C", "B", "A"),
  "prediction" = c("C", "B", "A", "C", "A", "B", "B",
                   "C", "A", "B", "C", "A", "C"),
  stringsAsFactors = FALSE
)

# Evaluate predictions and create confusion matrix
evaluation &lt;- evaluate(
  data = data,
  target_col = "target",
  prediction_cols = "prediction",
  type = "multinomial"
)

# Inspect confusion matrix tibble
evaluation[["Confusion Matrix"]][[1]]

# Plot confusion matrix
# Supply confusion matrix tibble directly
plot_confusion_matrix(evaluation[["Confusion Matrix"]][[1]])
# Plot first confusion matrix in evaluate() output
plot_confusion_matrix(evaluation)

# Add sum tiles
plot_confusion_matrix(evaluation, add_sums = TRUE)

# Counts only
plot_confusion_matrix(
  evaluation[["Confusion Matrix"]][[1]],
  add_normalized = FALSE,
  add_row_percentages = FALSE,
  add_col_percentages = FALSE
)

# Change color palette to green
# Change theme to `theme_light`.
plot_confusion_matrix(
  evaluation[["Confusion Matrix"]][[1]],
  palette = "Greens",
  theme_fn = ggplot2::theme_light
)

# Change colors palette to custom gradient
# with a different gradient for sum tiles
plot_confusion_matrix(
  evaluation[["Confusion Matrix"]][[1]],
  palette = list("low" = "#B1F9E8", "high" = "#239895"),
  sums_settings = sum_tile_settings(
    palette = list("low" = "#e9e1fc", "high" = "#BE94E6")
  ),
  add_sums = TRUE
)

# The output is a ggplot2 object
# that you can add layers to
# Here we change the axis labels
plot_confusion_matrix(evaluation[["Confusion Matrix"]][[1]]) +
  ggplot2::labs(x = "True", y = "Guess")

# Replace the bottom tile text
# with some information
# First extract confusion matrix
# Then add new column with text
cm &lt;- evaluation[["Confusion Matrix"]][[1]]
cm[["Trials"]] &lt;- c(
  "(8/9)", "(3/9)", "(1/9)",
  "(3/9)", "(7/9)", "(4/9)",
  "(1/9)", "(2/9)", "(8/9)"
 )

# Now plot with the `sub_col` argument specified
plot_confusion_matrix(cm, sub_col="Trials")


</code></pre>

<hr>
<h2 id='plot_metric_density'>Density plot for a metric</h2><span id='topic+plot_metric_density'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#experimental"><img src="../help/figures/lifecycle-experimental.svg" alt='[Experimental]' /></a>
</p>
<p>Creates a <code><a href="ggplot2.html#topic+ggplot">ggplot2</a></code> object with a density plot
for one of the columns in the passed <code>data.frame</code>(s).
</p>
<p>Note: In its current form, it is mainly intended as a quick way to visualize
the results from cross-validations and baselines (random evaluations).
It may change significantly in future versions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_metric_density(
  results = NULL,
  baseline = NULL,
  metric = "",
  fill = c("darkblue", "lightblue"),
  alpha = 0.6,
  theme_fn = ggplot2::theme_minimal,
  xlim = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_metric_density_+3A_results">results</code></td>
<td>
<p><code>data.frame</code> with a metric column to create density plot for.
</p>
<p>To only plot the baseline, set to <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="plot_metric_density_+3A_baseline">baseline</code></td>
<td>
<p><code>data.frame</code> with the random evaluations from <code><a href="#topic+baseline">baseline()</a></code>.
Should contain a column for the <code>metric</code>.
</p>
<p>To only plot the results, set to <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="plot_metric_density_+3A_metric">metric</code></td>
<td>
<p>Name of the metric column in <code>`results`</code> to plot. (Character)</p>
</td></tr>
<tr><td><code id="plot_metric_density_+3A_fill">fill</code></td>
<td>
<p>Colors of the plotted distributions.
The first color is for the <code>`baseline`</code>, the second for the <code>`results`</code>.</p>
</td></tr>
<tr><td><code id="plot_metric_density_+3A_alpha">alpha</code></td>
<td>
<p>Transparency of the distribution (<code>0 - 1</code>).</p>
</td></tr>
<tr><td><code id="plot_metric_density_+3A_theme_fn">theme_fn</code></td>
<td>
<p>The <code>ggplot2</code> theme function to apply.</p>
</td></tr>
<tr><td><code id="plot_metric_density_+3A_xlim">xlim</code></td>
<td>
<p>Limits for the x-axis. Can be set to <code>NULL</code>.
</p>
<p>E.g. <code>c(0, 1)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>ggplot2</code> object with the density of a metric, possibly split
in <em>'Results'</em> and <em>'Baseline'</em>.
</p>


<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>


<h3>See Also</h3>

<p>Other plotting functions: 
<code><a href="#topic+font">font</a>()</code>,
<code><a href="#topic+plot_confusion_matrix">plot_confusion_matrix</a>()</code>,
<code><a href="#topic+plot_probabilities">plot_probabilities</a>()</code>,
<code><a href="#topic+plot_probabilities_ecdf">plot_probabilities_ecdf</a>()</code>,
<code><a href="#topic+sum_tile_settings">sum_tile_settings</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Attach packages
library(cvms)
library(dplyr)

# We will use the musicians and predicted.musicians datasets
musicians
predicted.musicians

# Set seed
set.seed(42)

# Create baseline for targets
bsl &lt;- baseline_multinomial(
  test_data = musicians,
  dependent_col = "Class",
  n = 20  # Normally 100
)

# Evaluate predictions grouped by classifier and fold column
eval &lt;- predicted.musicians %&gt;%
  dplyr::group_by(Classifier, `Fold Column`) %&gt;%
  evaluate(
  target_col = "Target",
  prediction_cols = c("A", "B", "C", "D"),
  type = "multinomial"
)

# Plot density of the Overall Accuracy metric
plot_metric_density(
  results = eval,
  baseline = bsl$random_evaluations,
  metric = "Overall Accuracy",
  xlim = c(0,1)
)

# The bulk of classifier results are much better than
# the baseline results

</code></pre>

<hr>
<h2 id='plot_probabilities'>Plot predicted probabilities</h2><span id='topic+plot_probabilities'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#experimental"><img src="../help/figures/lifecycle-experimental.svg" alt='[Experimental]' /></a>
</p>
<p>Creates a <code><a href="ggplot2.html#topic+ggplot">ggplot2</a></code> line plot object with the probabilities
of either the target classes or the predicted classes.
</p>
<p>The observations are ordered by the highest probability.
</p>
<p>TODO line geom: average probability per observation
</p>
<p>TODO points geom: actual probabilities per observation
</p>
<p>The meaning of the <strong>horizontal lines</strong> depend on the settings.
These are either <em>recall</em> scores, <em>precision</em> scores,
or <em>accuracy</em> scores, depending on the <code>`probability_of`</code>
and <code>`apply_facet`</code> arguments.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_probabilities(
  data,
  target_col,
  probability_cols,
  predicted_class_col = NULL,
  obs_id_col = NULL,
  group_col = NULL,
  probability_of = "target",
  positive = 2,
  order = "centered",
  theme_fn = ggplot2::theme_minimal,
  color_scale = ggplot2::scale_colour_brewer(palette = "Dark2"),
  apply_facet = length(probability_cols) &gt; 1,
  smoothe = FALSE,
  add_points = !is.null(obs_id_col),
  add_hlines = TRUE,
  add_caption = TRUE,
  show_x_scale = FALSE,
  line_settings = list(),
  smoothe_settings = list(),
  point_settings = list(),
  hline_settings = list(),
  facet_settings = list(),
  ylim = c(0, 1)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_probabilities_+3A_data">data</code></td>
<td>
<p><code>data.frame</code> with probabilities, target classes and (optional) predicted classes.
Can also include observation identifiers and a grouping variable.
</p>
<p>Example for binary classification:
</p>

<table>
<tr>
 <td style="text-align: right;">
  <strong>Classifier</strong> </td><td style="text-align: right;"> <strong>Observation</strong> </td><td style="text-align: right;"> <strong>Probability</strong> </td><td style="text-align: right;"> <strong>Target</strong> </td><td style="text-align: right;"> <strong>Prediction</strong>  </td>
</tr>
<tr>
 <td style="text-align: right;">
  SVM </td><td style="text-align: right;"> 1 </td><td style="text-align: right;"> 0.3 </td><td style="text-align: right;"> cl_1 </td><td style="text-align: right;"> cl_1 </td>
</tr>
<tr>
 <td style="text-align: right;">
  SVM </td><td style="text-align: right;"> 2 </td><td style="text-align: right;"> 0.7 </td><td style="text-align: right;"> cl_1 </td><td style="text-align: right;"> cl_2 </td>
</tr>
<tr>
 <td style="text-align: right;">
  NB </td><td style="text-align: right;"> 1 </td><td style="text-align: right;"> 0.2 </td><td style="text-align: right;"> cl_2 </td><td style="text-align: right;"> cl_1 </td>
</tr>
<tr>
 <td style="text-align: right;">
  NB </td><td style="text-align: right;"> 2 </td><td style="text-align: right;"> 0.8 </td><td style="text-align: right;"> cl_2 </td><td style="text-align: right;"> cl_2 </td>
</tr>
<tr>
 <td style="text-align: right;">
  ... </td><td style="text-align: right;"> ... </td><td style="text-align: right;"> ... </td><td style="text-align: right;"> ... </td><td style="text-align: right;"> ... </td>
</tr>
<tr>
 <td style="text-align: right;">
 </td>
</tr>

</table>

<p>Example for multiclass classification:
</p>

<table>
<tr>
 <td style="text-align: right;">
  <strong>Classifier</strong> </td><td style="text-align: right;"> <strong>Observation</strong> </td><td style="text-align: right;"> <strong>cl_1</strong> </td><td style="text-align: right;"> <strong>cl_2</strong> </td><td style="text-align: right;"> <strong>cl_3</strong> </td><td style="text-align: right;"> <strong>Target</strong> </td><td style="text-align: right;"> <strong>Prediction</strong>  </td>
</tr>
<tr>
 <td style="text-align: right;">
  SVM </td><td style="text-align: right;"> 1 </td><td style="text-align: right;"> 0.2 </td><td style="text-align: right;"> 0.1 </td><td style="text-align: right;"> 0.7 </td><td style="text-align: right;"> cl_1 </td><td style="text-align: right;"> cl_3 </td>
</tr>
<tr>
 <td style="text-align: right;">
  SVM </td><td style="text-align: right;"> 2 </td><td style="text-align: right;"> 0.3 </td><td style="text-align: right;"> 0.5 </td><td style="text-align: right;"> 0.2 </td><td style="text-align: right;"> cl_1 </td><td style="text-align: right;"> cl_2 </td>
</tr>
<tr>
 <td style="text-align: right;">
  NB </td><td style="text-align: right;"> 1 </td><td style="text-align: right;"> 0.8 </td><td style="text-align: right;"> 0.1 </td><td style="text-align: right;"> 0.1 </td><td style="text-align: right;"> cl_2 </td><td style="text-align: right;"> cl_1 </td>
</tr>
<tr>
 <td style="text-align: right;">
  NB </td><td style="text-align: right;"> 2 </td><td style="text-align: right;"> 0.1 </td><td style="text-align: right;"> 0.6 </td><td style="text-align: right;"> 0.3 </td><td style="text-align: right;"> cl_3 </td><td style="text-align: right;"> cl_2 </td>
</tr>
<tr>
 <td style="text-align: right;">
  ... </td><td style="text-align: right;"> ... </td><td style="text-align: right;"> ... </td><td style="text-align: right;"> ... </td><td style="text-align: right;"> ... </td><td style="text-align: right;"> ... </td><td style="text-align: right;"> ... </td>
</tr>
<tr>
 <td style="text-align: right;">
 </td>
</tr>

</table>

<p>You can have multiple rows per observation ID per group. If, for instance, we
have run repeated cross-validation of 3 classifiers, we would have one predicted probability
per fold column per classifier.
</p>
<p>As created with the various validation functions in <code>cvms</code>, like
<code><a href="#topic+cross_validate_fn">cross_validate_fn()</a></code>.</p>
</td></tr>
<tr><td><code id="plot_probabilities_+3A_target_col">target_col</code></td>
<td>
<p>Name of column with target levels.</p>
</td></tr>
<tr><td><code id="plot_probabilities_+3A_probability_cols">probability_cols</code></td>
<td>
<p>Name of columns with predicted probabilities.
</p>
<p>For <strong>binary</strong> classification, this should be <strong>one column</strong> with the probability of the
<strong>second class</strong> (alphabetically).
</p>
<p>For <strong>multiclass</strong> classification, this should be <strong>one column per class</strong>.
These probabilities must sum to <code>1</code> row-wise.</p>
</td></tr>
<tr><td><code id="plot_probabilities_+3A_predicted_class_col">predicted_class_col</code></td>
<td>
<p>Name of column with predicted classes.
</p>
<p>This is required when <code>probability_of = "prediction"</code> and/or <code>add_hlines = TRUE</code>.</p>
</td></tr>
<tr><td><code id="plot_probabilities_+3A_obs_id_col">obs_id_col</code></td>
<td>
<p>Name of column with observation identifiers for grouping the <strong>x-axis</strong>.
When <code>NULL</code>, each row is an observation.
</p>
<p>Use case: when you have multiple predicted probabilities per observation by a classifier
(e.g. from repeated cross-validation).
</p>
<p>Can also be a grouping variable that you wish to aggregate.</p>
</td></tr>
<tr><td><code id="plot_probabilities_+3A_group_col">group_col</code></td>
<td>
<p>Name of column with groups. The plot elements
are split by these groups and can be identified by their color.
</p>
<p>E.g. the <em>classifier</em> responsible for the prediction.
</p>
<p><strong>N.B.</strong> With more than <strong><code>8</code></strong> groups,
the default <code>`color_scale`</code> might run out of colors.</p>
</td></tr>
<tr><td><code id="plot_probabilities_+3A_probability_of">probability_of</code></td>
<td>
<p>Whether to plot the probabilities of the
target classes (<code>"target"</code>) or the predicted classes (<code>"prediction"</code>).
</p>
<p>For each row, we extract the probability of either the
<em>target class</em> or the <em>predicted class</em>. Both are useful
to plot, as they show the behavior of the classifier in a way a confusion matrix doesn't.
One classifier might be very certain in its predictions (whether wrong or right), whereas
another might be less certain.</p>
</td></tr>
<tr><td><code id="plot_probabilities_+3A_positive">positive</code></td>
<td>
<p>TODO</p>
</td></tr>
<tr><td><code id="plot_probabilities_+3A_order">order</code></td>
<td>
<p>How to order of the the probabilities. (Character)
</p>
<p>One of: <code>"descending"</code>, <code>"ascending"</code>, and <code>"centered"</code>.</p>
</td></tr>
<tr><td><code id="plot_probabilities_+3A_theme_fn">theme_fn</code></td>
<td>
<p>The <code>ggplot2</code> theme function to apply.</p>
</td></tr>
<tr><td><code id="plot_probabilities_+3A_color_scale">color_scale</code></td>
<td>
<p><code>ggplot2</code> color scale object for adding discrete colors to the plot.
</p>
<p>E.g. the output of
<code><a href="ggplot2.html#topic+scale_colour_brewer">ggplot2::scale_colour_brewer()</a></code> or
<code><a href="ggplot2.html#topic+scale_colour_viridis_d">ggplot2::scale_colour_viridis_d()</a></code>.
</p>
<p><strong>N.B.</strong> The number of colors in the object's palette should be at least the same as
the number of groups in the <code>`group_col`</code> column.</p>
</td></tr>
<tr><td><code id="plot_probabilities_+3A_apply_facet">apply_facet</code></td>
<td>
<p>Whether to use
<code><a href="ggplot2.html#topic+facet_wrap">ggplot2::facet_wrap()</a></code>. (Logical)
</p>
<p>By default, faceting is applied when there are more than one probability column (multiclass).</p>
</td></tr>
<tr><td><code id="plot_probabilities_+3A_smoothe">smoothe</code></td>
<td>
<p>Whether to use <code><a href="ggplot2.html#topic+geom_smooth">ggplot2::geom_smooth()</a></code> instead of
<code><a href="ggplot2.html#topic+geom_line">ggplot2::geom_line()</a></code>.
This also adds a <code>95%</code> confidence interval by default.
</p>
<p>Settings can be passed via the <code>`smoothe_settings`</code> argument.</p>
</td></tr>
<tr><td><code id="plot_probabilities_+3A_add_points">add_points</code></td>
<td>
<p>Add a point for each predicted probability.
These are grouped on the x-axis by the <code>`obs_id_col`</code> column. (Logical)</p>
</td></tr>
<tr><td><code id="plot_probabilities_+3A_add_hlines">add_hlines</code></td>
<td>
<p>Add horizontal lines. (Logical)
</p>
<p>The meaning of these lines depends on the <code>`probability_of`</code>
and <code>`apply_facet`</code> arguments:
</p>

<table>
<tr>
 <td style="text-align: right;">
  <strong><code>apply_facet</code></strong> </td><td style="text-align: right;"> <strong><code>probability_of</code></strong> </td><td style="text-align: right;"> <strong>Metric</strong> </td>
</tr>
<tr>
 <td style="text-align: right;">
  <code>FALSE</code> </td><td style="text-align: right;"> <code>"target"</code> </td><td style="text-align: right;"> <strong>Accuracy</strong> </td>
</tr>
<tr>
 <td style="text-align: right;">
  <code>FALSE</code> </td><td style="text-align: right;"> <code>"prediction"</code> </td><td style="text-align: right;"> <strong>Accuracy</strong> </td>
</tr>
<tr>
 <td style="text-align: right;">
  <code>TRUE</code> </td><td style="text-align: right;"> <code>"target"</code> </td><td style="text-align: right;"> <strong>Recall / Sensitivity</strong> </td>
</tr>
<tr>
 <td style="text-align: right;">
  <code>TRUE</code> </td><td style="text-align: right;"> <code>"prediction"</code> </td><td style="text-align: right;"> <strong>Precision / PPV</strong> </td>
</tr>
<tr>
 <td style="text-align: right;">
 </td>
</tr>

</table>
</td></tr>
<tr><td><code id="plot_probabilities_+3A_add_caption">add_caption</code></td>
<td>
<p>Whether to add a caption explaining the plot. This is dynamically generated
and intended as a starting point. (Logical)
</p>
<p>You can overwrite the text with <code>ggplot2::labs(caption = "...")</code>.</p>
</td></tr>
<tr><td><code id="plot_probabilities_+3A_show_x_scale">show_x_scale</code></td>
<td>
<p>TODO</p>
</td></tr>
<tr><td><code id="plot_probabilities_+3A_line_settings">line_settings</code></td>
<td>
<p>Named list of arguments for <code><a href="ggplot2.html#topic+geom_line">ggplot2::geom_line()</a></code>.
</p>
<p>The <code>mapping</code> argument is set separately.
</p>
<p>Any argument not in the list will use its default value.
</p>
<p>Default: <code>list(size = 0.5)</code>
</p>
<p><strong>N.B.</strong> Ignored when <code>smoothe = TRUE</code>.</p>
</td></tr>
<tr><td><code id="plot_probabilities_+3A_smoothe_settings">smoothe_settings</code></td>
<td>
<p>Named list of arguments for <code><a href="ggplot2.html#topic+geom_smooth">ggplot2::geom_smooth()</a></code>.
</p>
<p>The <code>mapping</code> argument is set separately.
</p>
<p>Any argument not in the list will use its default value.
</p>
<p>Default: <code>list(size = 0.5, alpha = 0.18, level = 0.95, se = TRUE)</code>
</p>
<p><strong>N.B.</strong> Only used when <code>smoothe = TRUE</code>.</p>
</td></tr>
<tr><td><code id="plot_probabilities_+3A_point_settings">point_settings</code></td>
<td>
<p>Named list of arguments for <code><a href="ggplot2.html#topic+geom_point">ggplot2::geom_point()</a></code>.
</p>
<p>The <code>mapping</code> argument is set separately.
</p>
<p>Any argument not in the list will use its default value.
</p>
<p>Default: <code>list(size = 0.1, alpha = 0.4)</code></p>
</td></tr>
<tr><td><code id="plot_probabilities_+3A_hline_settings">hline_settings</code></td>
<td>
<p>Named list of arguments for <code><a href="ggplot2.html#topic+geom_hline">ggplot2::geom_hline()</a></code>.
</p>
<p>The <code>mapping</code> argument is set separately.
</p>
<p>Any argument not in the list will use its default value.
</p>
<p>Default: <code>list(size = 0.35, alpha = 0.5)</code></p>
</td></tr>
<tr><td><code id="plot_probabilities_+3A_facet_settings">facet_settings</code></td>
<td>
<p>Named list of arguments for <code><a href="ggplot2.html#topic+facet_wrap">ggplot2::facet_wrap()</a></code>.
</p>
<p>The <code>facets</code> argument is set separately.
</p>
<p>Any argument not in the list will use its default value.
</p>
<p>Commonly set arguments are <code>nrow</code> and <code>ncol</code>.</p>
</td></tr>
<tr><td><code id="plot_probabilities_+3A_ylim">ylim</code></td>
<td>
<p>Limits for the y-scale.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>TODO
</p>


<h3>Value</h3>

<p>A <code>ggplot2</code> object with a faceted line plot. TODO
</p>


<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>


<h3>See Also</h3>

<p>Other plotting functions: 
<code><a href="#topic+font">font</a>()</code>,
<code><a href="#topic+plot_confusion_matrix">plot_confusion_matrix</a>()</code>,
<code><a href="#topic+plot_metric_density">plot_metric_density</a>()</code>,
<code><a href="#topic+plot_probabilities_ecdf">plot_probabilities_ecdf</a>()</code>,
<code><a href="#topic+sum_tile_settings">sum_tile_settings</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Attach cvms
library(cvms)
library(ggplot2)
library(dplyr)

#
# Multiclass
#

# Plot probabilities of target classes
# From repeated cross-validation of three classifiers

# plot_probabilities(
#   data = predicted.musicians,
#   target_col = "Target",
#   probability_cols = c("A", "B", "C", "D"),
#   predicted_class_col = "Predicted Class",
#   group_col = "Classifier",
#   obs_id_col = "ID",
#   probability_of = "target"
# )

# Plot probabilities of predicted classes
# From repeated cross-validation of three classifiers

# plot_probabilities(
#   data = predicted.musicians,
#   target_col = "Target",
#   probability_cols = c("A", "B", "C", "D"),
#   predicted_class_col = "Predicted Class",
#   group_col = "Classifier",
#   obs_id_col = "ID",
#   probability_of = "prediction"
# )

# Center probabilities

# plot_probabilities(
#   data = predicted.musicians,
#   target_col = "Target",
#   probability_cols = c("A", "B", "C", "D"),
#   predicted_class_col = "Predicted Class",
#   group_col = "Classifier",
#   obs_id_col = "ID",
#   probability_of = "prediction",
#   order = "centered"
# )

#
# Binary
#

# Filter the predicted.musicians dataset
# binom_data &lt;- predicted.musicians %&gt;%
#   dplyr::filter(
#     Target %in% c("A", "B")
#   ) %&gt;%
#   # "B" is the second class alphabetically
#   dplyr::rename(Probability = B) %&gt;%
#   dplyr::mutate(`Predicted Class` = ifelse(
#     Probability &gt; 0.5, "B", "A")) %&gt;%
#   dplyr::select(-dplyr::all_of(c("A","C","D")))

# Plot probabilities of predicted classes
# From repeated cross-validation of three classifiers

# plot_probabilities(
#   data = binom_data,
#   target_col = "Target",
#   probability_cols = "Probability",
#   predicted_class_col = "Predicted Class",
#   group_col = "Classifier",
#   obs_id_col = "ID",
#   probability_of = "target"
# )

# plot_probabilities(
#   data = binom_data,
#   target_col = "Target",
#   probability_cols = "Probability",
#   predicted_class_col = "Predicted Class",
#   group_col = "Classifier",
#   obs_id_col = "ID",
#   probability_of = "prediction",
#   ylim = c(0.5, 1)
# )


</code></pre>

<hr>
<h2 id='plot_probabilities_ecdf'>Plot ECDF for the predicted probabilities</h2><span id='topic+plot_probabilities_ecdf'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#experimental"><img src="../help/figures/lifecycle-experimental.svg" alt='[Experimental]' /></a>
</p>
<p>Plots the empirical cumulative distribution function (ECDF) for the
probabilities of either the target classes or the predicted classes.
</p>
<p>Creates a <code><a href="ggplot2.html#topic+ggplot">ggplot2</a></code> with the <code><a href="ggplot2.html#topic+stat_ecdf">stat_ecdf()</a></code> geom.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_probabilities_ecdf(
  data,
  target_col,
  probability_cols,
  predicted_class_col = NULL,
  obs_id_col = NULL,
  group_col = NULL,
  probability_of = "target",
  positive = 2,
  theme_fn = ggplot2::theme_minimal,
  color_scale = ggplot2::scale_colour_brewer(palette = "Dark2"),
  apply_facet = length(probability_cols) &gt; 1,
  add_caption = TRUE,
  ecdf_settings = list(),
  facet_settings = list(),
  xlim = c(0, 1)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_probabilities_ecdf_+3A_data">data</code></td>
<td>
<p><code>data.frame</code> with probabilities, target classes and (optional) predicted classes.
Can also include observation identifiers and a grouping variable.
</p>
<p>Example for binary classification:
</p>

<table>
<tr>
 <td style="text-align: right;">
  <strong>Classifier</strong> </td><td style="text-align: right;"> <strong>Observation</strong> </td><td style="text-align: right;"> <strong>Probability</strong> </td><td style="text-align: right;"> <strong>Target</strong> </td><td style="text-align: right;"> <strong>Prediction</strong>  </td>
</tr>
<tr>
 <td style="text-align: right;">
  SVM </td><td style="text-align: right;"> 1 </td><td style="text-align: right;"> 0.3 </td><td style="text-align: right;"> cl_1 </td><td style="text-align: right;"> cl_1 </td>
</tr>
<tr>
 <td style="text-align: right;">
  SVM </td><td style="text-align: right;"> 2 </td><td style="text-align: right;"> 0.7 </td><td style="text-align: right;"> cl_1 </td><td style="text-align: right;"> cl_2 </td>
</tr>
<tr>
 <td style="text-align: right;">
  NB </td><td style="text-align: right;"> 1 </td><td style="text-align: right;"> 0.2 </td><td style="text-align: right;"> cl_2 </td><td style="text-align: right;"> cl_1 </td>
</tr>
<tr>
 <td style="text-align: right;">
  NB </td><td style="text-align: right;"> 2 </td><td style="text-align: right;"> 0.8 </td><td style="text-align: right;"> cl_2 </td><td style="text-align: right;"> cl_2 </td>
</tr>
<tr>
 <td style="text-align: right;">
  ... </td><td style="text-align: right;"> ... </td><td style="text-align: right;"> ... </td><td style="text-align: right;"> ... </td><td style="text-align: right;"> ... </td>
</tr>
<tr>
 <td style="text-align: right;">
 </td>
</tr>

</table>

<p>Example for multiclass classification:
</p>

<table>
<tr>
 <td style="text-align: right;">
  <strong>Classifier</strong> </td><td style="text-align: right;"> <strong>Observation</strong> </td><td style="text-align: right;"> <strong>cl_1</strong> </td><td style="text-align: right;"> <strong>cl_2</strong> </td><td style="text-align: right;"> <strong>cl_3</strong> </td><td style="text-align: right;"> <strong>Target</strong> </td><td style="text-align: right;"> <strong>Prediction</strong>  </td>
</tr>
<tr>
 <td style="text-align: right;">
  SVM </td><td style="text-align: right;"> 1 </td><td style="text-align: right;"> 0.2 </td><td style="text-align: right;"> 0.1 </td><td style="text-align: right;"> 0.7 </td><td style="text-align: right;"> cl_1 </td><td style="text-align: right;"> cl_3 </td>
</tr>
<tr>
 <td style="text-align: right;">
  SVM </td><td style="text-align: right;"> 2 </td><td style="text-align: right;"> 0.3 </td><td style="text-align: right;"> 0.5 </td><td style="text-align: right;"> 0.2 </td><td style="text-align: right;"> cl_1 </td><td style="text-align: right;"> cl_2 </td>
</tr>
<tr>
 <td style="text-align: right;">
  NB </td><td style="text-align: right;"> 1 </td><td style="text-align: right;"> 0.8 </td><td style="text-align: right;"> 0.1 </td><td style="text-align: right;"> 0.1 </td><td style="text-align: right;"> cl_2 </td><td style="text-align: right;"> cl_1 </td>
</tr>
<tr>
 <td style="text-align: right;">
  NB </td><td style="text-align: right;"> 2 </td><td style="text-align: right;"> 0.1 </td><td style="text-align: right;"> 0.6 </td><td style="text-align: right;"> 0.3 </td><td style="text-align: right;"> cl_3 </td><td style="text-align: right;"> cl_2 </td>
</tr>
<tr>
 <td style="text-align: right;">
  ... </td><td style="text-align: right;"> ... </td><td style="text-align: right;"> ... </td><td style="text-align: right;"> ... </td><td style="text-align: right;"> ... </td><td style="text-align: right;"> ... </td><td style="text-align: right;"> ... </td>
</tr>
<tr>
 <td style="text-align: right;">
 </td>
</tr>

</table>

<p>As created with the various validation functions in <code>cvms</code>, like
<code><a href="#topic+cross_validate_fn">cross_validate_fn()</a></code>.</p>
</td></tr>
<tr><td><code id="plot_probabilities_ecdf_+3A_target_col">target_col</code></td>
<td>
<p>Name of column with target levels.</p>
</td></tr>
<tr><td><code id="plot_probabilities_ecdf_+3A_probability_cols">probability_cols</code></td>
<td>
<p>Name of columns with predicted probabilities.
</p>
<p>For <strong>binary</strong> classification, this should be <strong>one column</strong> with the probability of the
<strong>second class</strong> (alphabetically).
</p>
<p>For <strong>multiclass</strong> classification, this should be <strong>one column per class</strong>.
These probabilities must sum to <code>1</code> row-wise.</p>
</td></tr>
<tr><td><code id="plot_probabilities_ecdf_+3A_predicted_class_col">predicted_class_col</code></td>
<td>
<p>Name of column with predicted classes.
</p>
<p>This is required when <code>probability_of = "prediction"</code>.</p>
</td></tr>
<tr><td><code id="plot_probabilities_ecdf_+3A_obs_id_col">obs_id_col</code></td>
<td>
<p>Name of column with observation identifiers for averaging the
predicted probabilities per observation before computing the ECDF (<em>when deemed meaningful</em>).
When <code>NULL</code>, each row is an observation.</p>
</td></tr>
<tr><td><code id="plot_probabilities_ecdf_+3A_group_col">group_col</code></td>
<td>
<p>Name of column with groups. The plot elements
are split by these groups and can be identified by their color.
</p>
<p>E.g. the <em>classifier</em> responsible for the prediction.
</p>
<p><strong>N.B.</strong> With more than <strong><code>8</code></strong> groups,
the default <code>`color_scale`</code> might run out of colors.</p>
</td></tr>
<tr><td><code id="plot_probabilities_ecdf_+3A_probability_of">probability_of</code></td>
<td>
<p>Whether to plot the ECDF for the probabilities of the
target classes (<code>"target"</code>) or the predicted classes (<code>"prediction"</code>).
</p>
<p>For each row, we extract the probability of either the
<em>target class</em> or the <em>predicted class</em>. Both are useful
to plot, as they show the behavior of the classifier in a way a confusion matrix doesn't.
One classifier might be very certain in its predictions (whether wrong or right), whereas
another might be less certain.</p>
</td></tr>
<tr><td><code id="plot_probabilities_ecdf_+3A_positive">positive</code></td>
<td>
<p>TODO</p>
</td></tr>
<tr><td><code id="plot_probabilities_ecdf_+3A_theme_fn">theme_fn</code></td>
<td>
<p>The <code>ggplot2</code> theme function to apply.</p>
</td></tr>
<tr><td><code id="plot_probabilities_ecdf_+3A_color_scale">color_scale</code></td>
<td>
<p><code>ggplot2</code> color scale object for adding discrete colors to the plot.
</p>
<p>E.g. the output of
<code><a href="ggplot2.html#topic+scale_colour_brewer">ggplot2::scale_colour_brewer()</a></code> or
<code><a href="ggplot2.html#topic+scale_colour_viridis_d">ggplot2::scale_colour_viridis_d()</a></code>.
</p>
<p><strong>N.B.</strong> The number of colors in the object's palette should be at least the same as
the number of groups in the <code>`group_col`</code> column.</p>
</td></tr>
<tr><td><code id="plot_probabilities_ecdf_+3A_apply_facet">apply_facet</code></td>
<td>
<p>Whether to use
<code><a href="ggplot2.html#topic+facet_wrap">ggplot2::facet_wrap()</a></code>. (Logical)
</p>
<p>By default, faceting is applied when there are more than one probability column (multiclass).</p>
</td></tr>
<tr><td><code id="plot_probabilities_ecdf_+3A_add_caption">add_caption</code></td>
<td>
<p>Whether to add a caption explaining the plot. This is dynamically generated
and intended as a starting point. (Logical)
</p>
<p>You can overwrite the text with <code>ggplot2::labs(caption = "...")</code>.</p>
</td></tr>
<tr><td><code id="plot_probabilities_ecdf_+3A_ecdf_settings">ecdf_settings</code></td>
<td>
<p>Named list of arguments for <code><a href="ggplot2.html#topic+stat_ecdf">ggplot2::stat_ecdf()</a></code>.
</p>
<p>The <code>mapping</code> argument is set separately.
</p>
<p>Any argument not in the list will use the default value set by <code>cvms</code>.
</p>
<p>Defaults: <code>list(geom = "smooth", pad = FALSE)</code>.
</p>
<p>Common changes are to set <code>`geom = "step"`</code> and/or <code>`pad = TRUE`</code>.</p>
</td></tr>
<tr><td><code id="plot_probabilities_ecdf_+3A_facet_settings">facet_settings</code></td>
<td>
<p>Named list of arguments for <code><a href="ggplot2.html#topic+facet_wrap">ggplot2::facet_wrap()</a></code>.
</p>
<p>The <code>facets</code> argument is set separately.
</p>
<p>Any argument not in the list will use its default value.
</p>
<p>Commonly set arguments are <code>nrow</code> and <code>ncol</code>.</p>
</td></tr>
<tr><td><code id="plot_probabilities_ecdf_+3A_xlim">xlim</code></td>
<td>
<p>Limits for the x-scale.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>TODO
</p>


<h3>Value</h3>

<p>A <code>ggplot2</code> object with a faceted line plot. TODO
</p>


<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>


<h3>See Also</h3>

<p>Other plotting functions: 
<code><a href="#topic+font">font</a>()</code>,
<code><a href="#topic+plot_confusion_matrix">plot_confusion_matrix</a>()</code>,
<code><a href="#topic+plot_metric_density">plot_metric_density</a>()</code>,
<code><a href="#topic+plot_probabilities">plot_probabilities</a>()</code>,
<code><a href="#topic+sum_tile_settings">sum_tile_settings</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Attach cvms
library(cvms)
library(ggplot2)
library(dplyr)

#
# Multiclass
#

# TODO: Go through and rewrite comments and code!

# Plot probabilities of target classes
# From repeated cross-validation of three classifiers

# plot_probabilities_ecdf(
#   data = predicted.musicians,
#   target_col = "Target",
#   probability_cols = c("A", "B", "C", "D"),
#   predicted_class_col = "Predicted Class",
#   group_col = "Classifier",
#   probability_of = "target"
# )

# Plot probabilities of predicted classes
# From repeated cross-validation of three classifiers

# plot_probabilities_ecdf(
#   data = predicted.musicians,
#   target_col = "Target",
#   probability_cols = c("A", "B", "C", "D"),
#   predicted_class_col = "Predicted Class",
#   group_col = "Classifier",
#   probability_of = "prediction"
# )

#
# Binary
#

# Filter the predicted.musicians dataset
# binom_data &lt;- predicted.musicians %&gt;%
#   dplyr::filter(
#     Target %in% c("A", "B")
#   ) %&gt;%
#   # "B" is the second class alphabetically
#   dplyr::rename(Probability = B) %&gt;%
#   dplyr::mutate(`Predicted Class` = ifelse(
#     Probability &gt; 0.5, "B", "A")) %&gt;%
#   dplyr::select(-dplyr::all_of(c("A","C","D")))

# Plot probabilities of predicted classes
# From repeated cross-validation of three classifiers

# plot_probabilities_ecdf(
#   data = binom_data,
#   target_col = "Target",
#   probability_cols = "Probability",
#   predicted_class_col = "Predicted Class",
#   group_col = "Classifier",
#   probability_of = "target"
# )

# plot_probabilities_ecdf(
#   data = binom_data,
#   target_col = "Target",
#   probability_cols = "Probability",
#   predicted_class_col = "Predicted Class",
#   group_col = "Classifier",
#   probability_of = "prediction",
#   xlim = c(0.5, 1)
# )


</code></pre>

<hr>
<h2 id='precomputed.formulas'>Precomputed formulas</h2><span id='topic+precomputed.formulas'></span>

<h3>Description</h3>

<p>Fixed effect combinations for model formulas with/without two- and three-way interactions.
Up to eight fixed effects in total with up to five fixed effects per formula.
</p>


<h3>Format</h3>

<p>A <code>data.frame</code> with <code>259,358</code> rows and <code>5</code> variables:
</p>

<dl>
<dt>formula_</dt><dd><p>combination of fixed effects, separated by &quot;<code>+</code>&quot; and &quot;<code>*</code>&quot;</p>
</dd>
<dt>max_interaction_size</dt><dd><p>maximum interaction size in the formula, up to <code>3</code></p>
</dd>
<dt>max_effect_frequency</dt><dd><p>maximum count of an effect in the formula, e.g. the <code>3</code> A's in <code>"A * B + A * C + A * D"</code></p>
</dd>
<dt>num_effects</dt><dd><p>number of unique effects included in the formula</p>
</dd>
<dt>min_num_fixed_effects</dt><dd><p>minimum number of fixed effects required to use the formula,
i.e. the index in the alphabet of the last of the alphabetically ordered effects (letters) in the formula,
so <code>4</code> for the formula: <code>"A + B + D"</code>  </p>
</dd>
</dl>



<h3>Details</h3>

<p>Effects are represented by the first eight capital letters.
</p>
<p>Used by <code><a href="#topic+combine_predictors">combine_predictors</a></code>.
</p>


<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>

<hr>
<h2 id='predict_functions'>Examples of predict_fn functions</h2><span id='topic+predict_functions'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#experimental"><img src="../help/figures/lifecycle-experimental.svg" alt='[Experimental]' /></a>
</p>
<p>Examples of predict functions that can be used in
<code><a href="#topic+cross_validate_fn">cross_validate_fn()</a></code>.
They can either be used directly or be starting points.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_functions(name)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_functions_+3A_name">name</code></td>
<td>
<p>Name of model to get predict function for,
as it appears in the following table.
</p>
<p>The <strong>Model HParams</strong> column lists hyperparameters used
in the respective model function.
</p>

<table>
<tr>
 <td style="text-align: right;">
  <strong>Name</strong> </td><td style="text-align: right;"> <strong>Function</strong> </td><td style="text-align: right;"> <strong>Model HParams</strong> </td>
</tr>
<tr>
 <td style="text-align: right;">
  "lm" </td><td style="text-align: right;"> <code><a href="stats.html#topic+lm">stats::lm()</a></code> </td><td style="text-align: right;"> </td>
</tr>
<tr>
 <td style="text-align: right;">
  "lmer" </td><td style="text-align: right;"> <code><a href="lme4.html#topic+lmer">lme4::lmer()</a></code> </td><td style="text-align: right;"> </td>
</tr>
<tr>
 <td style="text-align: right;">
  "glm_binomial" </td><td style="text-align: right;"> <code><a href="stats.html#topic+lm">stats::glm()</a></code> </td><td style="text-align: right;"> <code>family = "binomial"</code></td>
</tr>
<tr>
 <td style="text-align: right;">
  "glmer_binomial" </td><td style="text-align: right;"> <code><a href="lme4.html#topic+glmer">lme4::glmer()</a></code> </td><td style="text-align: right;"> <code>family = "binomial"</code></td>
</tr>
<tr>
 <td style="text-align: right;">
  "svm_gaussian" </td><td style="text-align: right;"> <code><a href="e1071.html#topic+svm">e1071::svm()</a></code> </td><td style="text-align: right;"> <code>type = "eps-regression"</code></td>
</tr>
<tr>
 <td style="text-align: right;">
  "svm_binomial" </td><td style="text-align: right;"> <code><a href="e1071.html#topic+svm">e1071::svm()</a></code> </td><td style="text-align: right;"> <code>type = "C-classification"</code>, <code>probability = TRUE</code></td>
</tr>
<tr>
 <td style="text-align: right;">
  "svm_multinomial" </td><td style="text-align: right;"> <code><a href="e1071.html#topic+svm">e1071::svm()</a></code> </td><td style="text-align: right;"> <code>type = "C-classification"</code>, <code>probability = TRUE</code></td>
</tr>
<tr>
 <td style="text-align: right;">
  "naive_bayes" </td><td style="text-align: right;"> <code><a href="e1071.html#topic+naiveBayes">e1071::naiveBayes()</a></code> </td><td style="text-align: right;"> </td>
</tr>
<tr>
 <td style="text-align: right;">
  "nnet_multinom" </td><td style="text-align: right;"> <code><a href="nnet.html#topic+multinom">nnet::multinom()</a></code> </td><td style="text-align: right;"> </td>
</tr>
<tr>
 <td style="text-align: right;">
  "nnet_gaussian" </td><td style="text-align: right;"> <code><a href="nnet.html#topic+multinom">nnet::nnet()</a></code> </td><td style="text-align: right;"> <code>linout = TRUE</code> </td>
</tr>
<tr>
 <td style="text-align: right;">
  "nnet_binomial" </td><td style="text-align: right;"> <code><a href="nnet.html#topic+multinom">nnet::nnet()</a></code> </td><td style="text-align: right;"> </td>
</tr>
<tr>
 <td style="text-align: right;">
  "randomForest_gaussian" </td><td style="text-align: right;"> <code><a href="randomForest.html#topic+randomForest">randomForest::randomForest()</a></code> </td><td style="text-align: right;"> </td>
</tr>
<tr>
 <td style="text-align: right;">
  "randomForest_binomial" </td><td style="text-align: right;"> <code><a href="randomForest.html#topic+randomForest">randomForest::randomForest()</a></code> </td><td style="text-align: right;"> </td>
</tr>
<tr>
 <td style="text-align: right;">
  "randomForest_multinomial" </td><td style="text-align: right;"> <code><a href="randomForest.html#topic+randomForest">randomForest::randomForest()</a></code> </td><td style="text-align: right;"> </td>
</tr>
<tr>
 <td style="text-align: right;">
 </td>
</tr>

</table>
</td></tr>
</table>


<h3>Value</h3>

<p>A function with the following form:
</p>
<p><code>function(test_data, model, formula, hyperparameters, train_data) {</code>
</p>
<p><code style="white-space: pre;">&#8288;    &#8288;</code><code># Use model to predict test_data</code>
</p>
<p><code style="white-space: pre;">&#8288;    &#8288;</code><code># Return predictions</code>
</p>
<p><code>}</code>
</p>


<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>


<h3>See Also</h3>

<p>Other example functions: 
<code><a href="#topic+model_functions">model_functions</a>()</code>,
<code><a href="#topic+preprocess_functions">preprocess_functions</a>()</code>,
<code><a href="#topic+update_hyperparameters">update_hyperparameters</a>()</code>
</p>

<hr>
<h2 id='predicted.musicians'>Predicted musician groups</h2><span id='topic+predicted.musicians'></span>

<h3>Description</h3>

<p>Predictions by 3 classifiers of the 4 classes in the
<code><a href="#topic+musicians">musicians</a></code> dataset.
Obtained with 5-fold stratified cross-validation (3 repetitions).
The three classifiers were fit using <code>nnet::multinom</code>,
<code>randomForest::randomForest</code>, and <code>e1071::svm</code>.
</p>


<h3>Format</h3>

<p>A <code>data.frame</code> with <code>540</code> rows and <code>10</code> variables:
</p>

<dl>
<dt>Classifier</dt><dd><p>The applied classifier.
One of <code>"nnet_multinom"</code>, <code>"randomForest"</code>, and <code>"e1071_svm"</code>.</p>
</dd>
<dt>Fold Column</dt><dd><p>The fold column name. Each is a unique 5-fold split.
One of <code>".folds_1"</code>, <code>".folds_2"</code>, and <code>".folds_3"</code>.</p>
</dd>
<dt>Fold</dt><dd><p>The fold. <code>1</code> to <code>5</code>.</p>
</dd>
<dt>ID</dt><dd><p>Musician identifier, 60 levels</p>
</dd>
<dt>Target</dt><dd><p>The actual class of the musician.
One of <code>"A"</code>, <code>"B"</code>, <code>"C"</code>, and <code>"D"</code>.</p>
</dd>
<dt>A</dt><dd><p>The probability of class <code>"A"</code>.</p>
</dd>
<dt>B</dt><dd><p>The probability of class <code>"B"</code>.</p>
</dd>
<dt>C</dt><dd><p>The probability of class <code>"C"</code>.</p>
</dd>
<dt>D</dt><dd><p>The probability of class <code>"D"</code>.</p>
</dd>
<dt>Predicted Class</dt><dd><p>The predicted class. The argmax of the four probability columns.</p>
</dd>
</dl>



<h3>Details</h3>

<p>Used formula: <code>"Class ~ Height + Age + Drums + Bass + Guitar + Keys + Vocals"</code>
</p>


<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>


<h3>See Also</h3>

<p>musicians
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Attach packages
library(cvms)
library(dplyr)


# Evaluate each fold column
predicted.musicians %&gt;%
  dplyr::group_by(Classifier, `Fold Column`) %&gt;%
  evaluate(target_col = "Target",
           prediction_cols = c("A", "B", "C", "D"),
           type = "multinomial")

# Overall ID evaluation
# I.e. if we average all 9 sets of predictions,
# how well did we predict the targets?
overall_id_eval &lt;- predicted.musicians %&gt;%
  evaluate(target_col = "Target",
           prediction_cols = c("A", "B", "C", "D"),
           type = "multinomial",
           id_col = "ID")
overall_id_eval
# Plot the confusion matrix
plot_confusion_matrix(overall_id_eval$`Confusion Matrix`[[1]])

</code></pre>

<hr>
<h2 id='preprocess_functions'>Examples of preprocess_fn functions</h2><span id='topic+preprocess_functions'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#experimental"><img src="../help/figures/lifecycle-experimental.svg" alt='[Experimental]' /></a>
</p>
<p>Examples of preprocess functions that can be used in
<code><a href="#topic+cross_validate_fn">cross_validate_fn()</a></code> and
<code><a href="#topic+validate_fn">validate_fn()</a></code>.
They can either be used directly or be starting points.
</p>
<p>The examples use <code><a href="recipes.html#topic+recipe">recipes</a></code>,
but you can also use <code>caret::preProcess()</code> or
similar functions.
</p>
<p>In these examples, the preprocessing will only affect the numeric predictors.
</p>
<p>You may prefer to hardcode a formula like <code>"y ~ ."</code> (where
<code>y</code> is your dependent variable) as that will allow you to set
<strong>'preprocess_one'</strong> to <code>TRUE</code> in <code><a href="#topic+cross_validate_fn">cross_validate_fn()</a></code>
and <code><a href="#topic+validate_fn">validate_fn()</a></code> and save time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>preprocess_functions(name)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="preprocess_functions_+3A_name">name</code></td>
<td>
<p>Name of preprocessing function
as it appears in the following list:
</p>

<table>
<tr>
 <td style="text-align: right;">
  <strong>Name</strong> </td><td style="text-align: right;"> <strong>Description</strong> </td>
</tr>
<tr>
 <td style="text-align: right;">
  "standardize" </td><td style="text-align: right;"> Centers and scales the numeric predictors</td>
</tr>
<tr>
 <td style="text-align: right;">
  "range" </td><td style="text-align: right;"> Normalizes the numeric predictors to the 0-1 range</td>
</tr>
<tr>
 <td style="text-align: right;">
  "scale" </td><td style="text-align: right;"> Scales the numeric predictors to have a standard deviation of one</td>
</tr>
<tr>
 <td style="text-align: right;">
  "center" </td><td style="text-align: right;"> Centers the numeric predictors to have a mean of zero</td>
</tr>
<tr>
 <td style="text-align: right;">
  "warn" </td><td style="text-align: right;"> Identity function that throws a warning and a message</td>
</tr>
<tr>
 <td style="text-align: right;">
  </td>
</tr>

</table>
</td></tr>
</table>


<h3>Value</h3>

<p>A function with the following form:
</p>
<p><code>function(train_data, test_data, formula, hyperparameters) {</code>
</p>
<p><code style="white-space: pre;">&#8288;    &#8288;</code><code># Preprocess train_data and test_data</code>
</p>
<p><code style="white-space: pre;">&#8288;    &#8288;</code><code># Return a list with the preprocessed datasets</code>
</p>
<p><code style="white-space: pre;">&#8288;    &#8288;</code><code># and optionally a data frame with preprocessing parameters</code>
</p>
<p><code style="white-space: pre;">&#8288;    &#8288;</code><code>list(</code>
</p>
<p><code style="white-space: pre;">&#8288;        &#8288;</code><code>"train" = train_data,</code>
</p>
<p><code style="white-space: pre;">&#8288;        &#8288;</code><code>"test" = test_data,</code>
</p>
<p><code style="white-space: pre;">&#8288;        &#8288;</code><code>"parameters" = tidy_parameters</code>
</p>
<p><code style="white-space: pre;">&#8288;    &#8288;</code><code>)</code>
</p>
<p><code>}</code>
</p>


<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>


<h3>See Also</h3>

<p>Other example functions: 
<code><a href="#topic+model_functions">model_functions</a>()</code>,
<code><a href="#topic+predict_functions">predict_functions</a>()</code>,
<code><a href="#topic+update_hyperparameters">update_hyperparameters</a>()</code>
</p>

<hr>
<h2 id='process_info_binomial'>A set of process information object constructors</h2><span id='topic+process_info_binomial'></span><span id='topic+print.process_info_binomial'></span><span id='topic+as.character.process_info_binomial'></span><span id='topic+process_info_multinomial'></span><span id='topic+print.process_info_multinomial'></span><span id='topic+as.character.process_info_multinomial'></span><span id='topic+process_info_gaussian'></span><span id='topic+print.process_info_gaussian'></span><span id='topic+as.character.process_info_gaussian'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#experimental"><img src="../help/figures/lifecycle-experimental.svg" alt='[Experimental]' /></a>
</p>
<p>Classes for storing process information from prediction evaluations.
</p>
<p>Used internally.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>process_info_binomial(
  data,
  target_col,
  prediction_cols,
  id_col,
  cat_levels,
  positive,
  cutoff,
  locale = NULL
)

## S3 method for class 'process_info_binomial'
print(x, ...)

## S3 method for class 'process_info_binomial'
as.character(x, ...)

process_info_multinomial(
  data,
  target_col,
  prediction_cols,
  pred_class_col,
  id_col,
  cat_levels,
  apply_softmax,
  locale = NULL
)

## S3 method for class 'process_info_multinomial'
print(x, ...)

## S3 method for class 'process_info_multinomial'
as.character(x, ...)

process_info_gaussian(data, target_col, prediction_cols, id_col, locale = NULL)

## S3 method for class 'process_info_gaussian'
print(x, ...)

## S3 method for class 'process_info_gaussian'
as.character(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="process_info_binomial_+3A_data">data</code></td>
<td>
<p>Data frame.</p>
</td></tr>
<tr><td><code id="process_info_binomial_+3A_target_col">target_col</code></td>
<td>
<p>Name of target column.</p>
</td></tr>
<tr><td><code id="process_info_binomial_+3A_prediction_cols">prediction_cols</code></td>
<td>
<p>Names of prediction columns.</p>
</td></tr>
<tr><td><code id="process_info_binomial_+3A_id_col">id_col</code></td>
<td>
<p>Name of ID column.</p>
</td></tr>
<tr><td><code id="process_info_binomial_+3A_cat_levels">cat_levels</code></td>
<td>
<p>Categorical levels (classes).</p>
</td></tr>
<tr><td><code id="process_info_binomial_+3A_positive">positive</code></td>
<td>
<p>Name of the positive class.</p>
</td></tr>
<tr><td><code id="process_info_binomial_+3A_cutoff">cutoff</code></td>
<td>
<p>The cutoff used to get class predictions from probabilities.</p>
</td></tr>
<tr><td><code id="process_info_binomial_+3A_locale">locale</code></td>
<td>
<p>The locale when performing the evaluation.
Relevant when any sorting has been performed.</p>
</td></tr>
<tr><td><code id="process_info_binomial_+3A_x">x</code></td>
<td>
<p>a process info object used to select a method.</p>
</td></tr>
<tr><td><code id="process_info_binomial_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
<tr><td><code id="process_info_binomial_+3A_pred_class_col">pred_class_col</code></td>
<td>
<p>Name of predicted classes column.</p>
</td></tr>
<tr><td><code id="process_info_binomial_+3A_apply_softmax">apply_softmax</code></td>
<td>
<p>Whether softmax has been applied.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with relevant information.
</p>


<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>

<hr>
<h2 id='reconstruct_formulas'>Reconstruct model formulas from results tibbles</h2><span id='topic+reconstruct_formulas'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#maturing"><img src="../help/figures/lifecycle-maturing.svg" alt='[Maturing]' /></a>
</p>
<p>In the (cross-)validation results from functions like
<code><a href="#topic+cross_validate">cross_validate()</a></code>,
the model formulas have been split into the columns
<code>Dependent</code>, <code>Fixed</code> and <code>Random</code>.
Quickly reconstruct the model formulas from these columns.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reconstruct_formulas(results, topn = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reconstruct_formulas_+3A_results">results</code></td>
<td>
<p><code>data.frame</code> with results from
<code><a href="#topic+cross_validate">cross_validate()</a></code>
or <code><a href="#topic+validate">validate()</a></code>. (tbl)
</p>
<p>Must contain at least the columns <code>"Dependent"</code> and <code>"Fixed"</code>. For random effects,
the <code>"Random"</code> column should be included.</p>
</td></tr>
<tr><td><code id="reconstruct_formulas_+3A_topn">topn</code></td>
<td>
<p>Number of top rows to return. Simply applies <code>head()</code> to the results <code>tibble</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>list</code> of model formulas.
</p>


<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>

<hr>
<h2 id='render_toc'>Render Table of Contents</h2><span id='topic+render_toc'></span>

<h3>Description</h3>

<p>From: https://gist.github.com/gadenbuie/c83e078bf8c81b035e32c3fc0cf04ee8
</p>


<h3>Usage</h3>

<pre><code class='language-R'>render_toc(
  filename,
  toc_header_name = "Table of Contents",
  base_level = NULL,
  toc_depth = 3
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="render_toc_+3A_filename">filename</code></td>
<td>
<p>Name of RMarkdown or Markdown document</p>
</td></tr>
<tr><td><code id="render_toc_+3A_toc_header_name">toc_header_name</code></td>
<td>
<p>The table of contents header name. If specified, any
header with this format will not be included in the TOC. Set to 'NULL' to
include the TOC itself in the TOC (but why?).</p>
</td></tr>
<tr><td><code id="render_toc_+3A_base_level">base_level</code></td>
<td>
<p>Starting level of the lowest header level. Any headers
prior to the first header at the base_level are dropped silently.</p>
</td></tr>
<tr><td><code id="render_toc_+3A_toc_depth">toc_depth</code></td>
<td>
<p>Maximum depth for TOC, relative to base_level. Default is
'toc_depth = 3', which results in a TOC of at most 3 levels.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A simple function to extract headers from an RMarkdown or Markdown document
and build a table of contents. Returns a markdown list with links to the
headers using
[pandoc header identifiers](http://pandoc.org/MANUAL.html#header-identifiers).
</p>
<p>WARNING: This function only works with hash-tag headers.
</p>
<p>Because this function returns only the markdown list, the header for the
Table of Contents itself must be manually included in the text. Use
'toc_header_name' to exclude the table of contents header from the TOC, or
set to 'NULL' for it to be included.
</p>


<h3>Usage</h3>

<p>Just drop in a <strong>chunk</strong> where you want the toc to appear (set 'echo=FALSE'):
</p>
<p>render_toc(&quot;/path/to/the/file.Rmd&quot;)
</p>

<hr>
<h2 id='select_definitions'>Select model definition columns</h2><span id='topic+select_definitions'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#experimental"><img src="../help/figures/lifecycle-experimental.svg" alt='[Experimental]' /></a>
</p>
<p>Select the columns that define the models, such as the formula terms
and hyperparameters.
</p>
<p>If an expected column is not in the <code>`results`</code> <code>tibble</code>, it is simply ignored.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>select_definitions(results, unnest_hparams = TRUE, additional_includes = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="select_definitions_+3A_results">results</code></td>
<td>
<p>Results <code>tibble</code>. E.g. from
<code><a href="#topic+cross_validate">cross_validate()</a></code> or <code><a href="#topic+evaluate">evaluate()</a></code>.</p>
</td></tr>
<tr><td><code id="select_definitions_+3A_unnest_hparams">unnest_hparams</code></td>
<td>
<p>Whether to unnest the <code>HParams</code> column. (Logical)</p>
</td></tr>
<tr><td><code id="select_definitions_+3A_additional_includes">additional_includes</code></td>
<td>
<p>Names of additional columns to select. (Character)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The model definition columns from the results <code>tibble</code>.
</p>


<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>

<hr>
<h2 id='select_metrics'>Select columns with evaluation metrics and model definitions</h2><span id='topic+select_metrics'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#maturing"><img src="../help/figures/lifecycle-maturing.svg" alt='[Maturing]' /></a>
</p>
<p>When reporting results, we might not want all
the nested <code>tibble</code>s and process information columns.
This function selects the evaluation metrics and model formulas only.
</p>
<p>If an expected column is not in the <code>`results`</code> <code>tibble</code>, it is simply ignored.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>select_metrics(results, include_definitions = TRUE, additional_includes = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="select_metrics_+3A_results">results</code></td>
<td>
<p>Results <code>tibble</code>. E.g. from
<code><a href="#topic+cross_validate">cross_validate()</a></code> or <code><a href="#topic+evaluate">evaluate()</a></code>.</p>
</td></tr>
<tr><td><code id="select_metrics_+3A_include_definitions">include_definitions</code></td>
<td>
<p>Whether to include the <code>Dependent</code>,
<code>Fixed</code> and (possibly) <code>Random</code> and <code>HParams</code> columns. (Logical)</p>
</td></tr>
<tr><td><code id="select_metrics_+3A_additional_includes">additional_includes</code></td>
<td>
<p>Names of additional columns to select. (Character)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The results <code>tibble</code> with only the metric and model definition columns.
</p>


<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>

<hr>
<h2 id='simplify_formula'>Simplify formula with inline functions</h2><span id='topic+simplify_formula'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#experimental"><img src="../help/figures/lifecycle-experimental.svg" alt='[Experimental]' /></a>
</p>
<p>Extracts all variables from a formula object and creates
a new formula with all predictor variables added together without the
inline functions.
</p>
<p>E.g.:
</p>
<p><code>y ~ x*z + log(a) + (1|b)</code>
</p>
<p>becomes
</p>
<p><code>y ~ x + z + a + b</code>.
</p>
<p>This is useful when passing a formula to <code><a href="recipes.html#topic+recipe">recipes::recipe()</a></code>
for preprocessing a dataset, as used in the
<code><a href="#topic+preprocess_functions">preprocess_functions()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simplify_formula(formula, data = NULL, string_out = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simplify_formula_+3A_formula">formula</code></td>
<td>
<p>Formula object.
</p>
<p>If a string is passed, it will be converted with <code><a href="stats.html#topic+as.formula">as.formula()</a></code>.
</p>
<p>When a side <em>only</em> contains a <code>NULL</code>, it is kept. Otherwise <code>NULL</code>s are removed.
</p>
<p>An intercept (<code>1</code>) will only be kept if there are no variables on that side of the formula.</p>
</td></tr>
<tr><td><code id="simplify_formula_+3A_data">data</code></td>
<td>
<p><code>data.frame</code>. Used to extract variables when the formula contains a &quot;<code>.</code>&quot;.</p>
</td></tr>
<tr><td><code id="simplify_formula_+3A_string_out">string_out</code></td>
<td>
<p>Whether to return as a string. (Logical)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Attach cvms
library(cvms)

# Create formula
f1 &lt;- "y ~ x*z + log(a) + (1|b)"

# Simplify formula (as string)
simplify_formula(f1)

# Simplify formula (as formula)
simplify_formula(as.formula(f1))
</code></pre>

<hr>
<h2 id='sum_tile_settings'>Create a list of settings for the sum tiles in plot_confusion_matrix()</h2><span id='topic+sum_tile_settings'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#experimental"><img src="../help/figures/lifecycle-experimental.svg" alt='[Experimental]' /></a>
</p>
<p>Creates a list of settings for plotting the column/row sums
in <code><a href="#topic+plot_confusion_matrix">plot_confusion_matrix()</a></code>.
</p>
<p>The <code>`tc_`</code> in the arguments refers to the <strong>total count</strong> tile.
</p>
<p>NOTE: This is very experimental and will likely change.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sum_tile_settings(
  palette = NULL,
  label = NULL,
  tile_fill = NULL,
  font_color = NULL,
  tile_border_color = NULL,
  tile_border_size = NULL,
  tile_border_linetype = NULL,
  tc_tile_fill = NULL,
  tc_font_color = NULL,
  tc_tile_border_color = NULL,
  tc_tile_border_size = NULL,
  tc_tile_border_linetype = NULL,
  intensity_by = NULL,
  intensity_lims = NULL,
  intensity_beyond_lims = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sum_tile_settings_+3A_palette">palette</code></td>
<td>
<p>Color scheme to use for sum tiles.
Should be different from the <code>`palette`</code> used for the regular tiles.
</p>
<p>Passed directly to <code>`palette`</code> in
<code><a href="ggplot2.html#topic+scale_fill_distiller">ggplot2::scale_fill_distiller</a></code>.
</p>
<p>Try these palettes: <code>"Greens"</code>, <code>"Oranges"</code>, <code>"Greys"</code>,
<code>"Purples"</code>, <code>"Reds"</code>, and <code>"Blues"</code>.
</p>
<p>Alternatively, pass a named list with limits of a custom gradient as e.g.
<code>`list("low"="#e9e1fc", "high"="#BE94E6")`</code>. These are passed to
<code><a href="ggplot2.html#topic+scale_fill_gradient">ggplot2::scale_fill_gradient</a></code>.
</p>
<p>Note: When <code>`tile_fill`</code> is specified, the <code>`palette`</code> is <strong>ignored</strong>.</p>
</td></tr>
<tr><td><code id="sum_tile_settings_+3A_label">label</code></td>
<td>
<p>The label to use for the sum column and the sum row.</p>
</td></tr>
<tr><td><code id="sum_tile_settings_+3A_tc_tile_fill">tc_tile_fill</code>, <code id="sum_tile_settings_+3A_tile_fill">tile_fill</code></td>
<td>
<p>Specific background color for the tiles. Passed as <em><code>`fill`</code></em> to
<code><a href="ggplot2.html#topic+geom_tile">ggplot2::geom_tile</a></code>.
</p>
<p>If specified, the <code>`palette`</code> is ignored.</p>
</td></tr>
<tr><td><code id="sum_tile_settings_+3A_tc_font_color">tc_font_color</code>, <code id="sum_tile_settings_+3A_font_color">font_color</code></td>
<td>
<p>Color of the text in the tiles with the column and row sums.</p>
</td></tr>
<tr><td><code id="sum_tile_settings_+3A_tc_tile_border_color">tc_tile_border_color</code>, <code id="sum_tile_settings_+3A_tile_border_color">tile_border_color</code></td>
<td>
<p>Color of the tile borders. Passed as <em><code>`colour`</code></em> to
<code><a href="ggplot2.html#topic+geom_tile">ggplot2::geom_tile</a></code>.</p>
</td></tr>
<tr><td><code id="sum_tile_settings_+3A_tc_tile_border_size">tc_tile_border_size</code>, <code id="sum_tile_settings_+3A_tile_border_size">tile_border_size</code></td>
<td>
<p>Size of the tile borders. Passed as <em><code>`size`</code></em> to
<code><a href="ggplot2.html#topic+geom_tile">ggplot2::geom_tile</a></code>.</p>
</td></tr>
<tr><td><code id="sum_tile_settings_+3A_tc_tile_border_linetype">tc_tile_border_linetype</code>, <code id="sum_tile_settings_+3A_tile_border_linetype">tile_border_linetype</code></td>
<td>
<p>Linetype for the tile borders. Passed as <em><code>`linetype`</code></em> to
<code><a href="ggplot2.html#topic+geom_tile">ggplot2::geom_tile</a></code>.</p>
</td></tr>
<tr><td><code id="sum_tile_settings_+3A_intensity_by">intensity_by</code></td>
<td>
<p>The measure that should control the color intensity of the tiles.
Either <code>`counts`</code>, <code>`normalized`</code> or one of <code>`log counts`,
 `log2 counts`, `log10 counts`, `arcsinh counts`</code>.
</p>
<p>For 'normalized', the color limits become <code>0-100</code> (except when
<code>`intensity_lims`</code> are specified), why the intensities
can better be compared across plots.
</p>
<p>For the 'log*' and 'arcsinh' versions, the log/arcsinh transformed counts are used.
</p>
<p><strong>Note</strong>: In 'log*' transformed counts, 0-counts are set to '0', why they
won't be distinguishable from 1-counts.</p>
</td></tr>
<tr><td><code id="sum_tile_settings_+3A_intensity_lims">intensity_lims</code></td>
<td>
<p>A specific range of values for the color intensity of
the tiles. Given as a numeric vector with <code>c(min, max)</code>.
</p>
<p>This allows having the same intensity scale across plots for better comparison
of prediction sets.</p>
</td></tr>
<tr><td><code id="sum_tile_settings_+3A_intensity_beyond_lims">intensity_beyond_lims</code></td>
<td>
<p>What to do with values beyond the
<code>`intensity_lims`</code>. One of <code>"truncate", "grey"</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of settings.
</p>


<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>


<h3>See Also</h3>

<p>Other plotting functions: 
<code><a href="#topic+font">font</a>()</code>,
<code><a href="#topic+plot_confusion_matrix">plot_confusion_matrix</a>()</code>,
<code><a href="#topic+plot_metric_density">plot_metric_density</a>()</code>,
<code><a href="#topic+plot_probabilities">plot_probabilities</a>()</code>,
<code><a href="#topic+plot_probabilities_ecdf">plot_probabilities_ecdf</a>()</code>
</p>

<hr>
<h2 id='summarize_metrics'>Summarize metrics with common descriptors</h2><span id='topic+summarize_metrics'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#experimental"><img src="../help/figures/lifecycle-experimental.svg" alt='[Experimental]' /></a>
</p>
<p>Summarizes all numeric columns. Counts the <code>NA</code>s and <code>Inf</code>s in the columns.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summarize_metrics(data, cols = NULL, na.rm = TRUE, inf.rm = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summarize_metrics_+3A_data">data</code></td>
<td>
<p><code>data.frame</code> with numeric columns to summarize.</p>
</td></tr>
<tr><td><code id="summarize_metrics_+3A_cols">cols</code></td>
<td>
<p>Names of columns to summarize. Non-numeric columns are ignored. (Character)</p>
</td></tr>
<tr><td><code id="summarize_metrics_+3A_na.rm">na.rm</code></td>
<td>
<p>Whether to remove <code>NA</code>s before summarizing. (Logical)</p>
</td></tr>
<tr><td><code id="summarize_metrics_+3A_inf.rm">inf.rm</code></td>
<td>
<p>Whether to remove <code>Inf</code>s before summarizing. (Logical)</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>tibble</code> where each row is a descriptor of the column.
</p>
<p>The <strong>Measure</strong> column contains the name of the descriptor.
</p>
<p>The <strong>NAs</strong> row is a count of the <code>NA</code>s in the column.
</p>
<p>The <strong>INFs</strong> row is a count of the <code>Inf</code>s in the column.
</p>


<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Attach packages
library(cvms)
library(dplyr)

df &lt;- data.frame("a" = c("a", "a", "a", "b", "b", "b", "c", "c", "c"),
                 "b" = c(0.8, 0.6, 0.3, 0.2, 0.4, 0.5, 0.8, 0.1, 0.5),
                 "c" = c(0.2, 0.3, 0.4, 0.6, 0.5, 0.8, 0.1, 0.8, 0.3))

# Summarize all numeric columns
summarize_metrics(df)

# Summarize column "b"
summarize_metrics(df, cols = "b")
</code></pre>

<hr>
<h2 id='update_hyperparameters'>Check and update hyperparameters</h2><span id='topic+update_hyperparameters'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#experimental"><img src="../help/figures/lifecycle-experimental.svg" alt='[Experimental]' /></a>
</p>

<ol>
<li><p> Checks if the required hyperparameters are present and
throws an error when it is not the case.
</p>
</li>
<li><p> Inserts the missing hyperparameters with the supplied
default values.
</p>
</li></ol>

<p>For managing hyperparameters in custom model functions for
<code><a href="#topic+cross_validate_fn">cross_validate_fn()</a></code> or
<code><a href="#topic+validate_fn">validate_fn()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>update_hyperparameters(..., hyperparameters, .required = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="update_hyperparameters_+3A_...">...</code></td>
<td>
<p>Default values for missing hyperparameters.
</p>
<p>E.g.:
</p>
<p><code>kernel = "linear", cost = 10</code></p>
</td></tr>
<tr><td><code id="update_hyperparameters_+3A_hyperparameters">hyperparameters</code></td>
<td>
<p><code>list</code> of hyperparameters as supplied to
<code><a href="#topic+cross_validate_fn">cross_validate_fn()</a></code>.
Can also be a single-row <code>data.frame</code>.</p>
</td></tr>
<tr><td><code id="update_hyperparameters_+3A_.required">.required</code></td>
<td>
<p>Names of required hyperparameters. If any of these
are not present in the hyperparameters, an <code>error</code> is thrown.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>named list</code> with the updated hyperparameters.
</p>


<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>


<h3>See Also</h3>

<p>Other example functions: 
<code><a href="#topic+model_functions">model_functions</a>()</code>,
<code><a href="#topic+predict_functions">predict_functions</a>()</code>,
<code><a href="#topic+preprocess_functions">preprocess_functions</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Attach packages
library(cvms)

# Create a list of hyperparameters
hparams &lt;- list(
  "kernel" = "radial",
  "scale" = TRUE
)

# Update hyperparameters with defaults
# Only 'cost' is changed as it's missing
update_hyperparameters(
  cost = 10,
  kernel = "linear",
  "scale" = FALSE,
  hyperparameters = hparams
)

# 'cost' is required
# throws error
if (requireNamespace("xpectr", quietly = TRUE)){
  xpectr::capture_side_effects(
    update_hyperparameters(
      kernel = "linear",
      "scale" = FALSE,
      hyperparameters = hparams,
      .required = "cost"
    )
  )
}


</code></pre>

<hr>
<h2 id='validate'>Validate regression models on a test set</h2><span id='topic+validate'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#stable"><img src="../help/figures/lifecycle-stable.svg" alt='[Stable]' /></a>
</p>
<p>Train linear or logistic regression models on a training set and validate it by
predicting a test/validation set.
Returns results in a <code>tibble</code> for easy reporting, along with the trained models.
</p>
<p>See <code><a href="#topic+validate_fn">validate_fn()</a></code> for use
with custom model functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>validate(
  train_data,
  formulas,
  family,
  test_data = NULL,
  partitions_col = ".partitions",
  control = NULL,
  REML = FALSE,
  cutoff = 0.5,
  positive = 2,
  metrics = list(),
  preprocessing = NULL,
  err_nc = FALSE,
  rm_nc = FALSE,
  parallel = FALSE,
  verbose = FALSE,
  link = deprecated(),
  models = deprecated(),
  model_verbose = deprecated()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="validate_+3A_train_data">train_data</code></td>
<td>
<p><code>data.frame</code>.
</p>
<p>Can contain a grouping factor for identifying partitions - as made with
<code><a href="groupdata2.html#topic+partition">groupdata2::partition()</a></code>.
See <code>`partitions_col`</code>.</p>
</td></tr>
<tr><td><code id="validate_+3A_formulas">formulas</code></td>
<td>
<p>Model formulas as strings. (Character)
</p>
<p>E.g. <code>c("y~x", "y~z")</code>.
</p>
<p>Can contain random effects.
</p>
<p>E.g. <code>c("y~x+(1|r)", "y~z+(1|r)")</code>.</p>
</td></tr>
<tr><td><code id="validate_+3A_family">family</code></td>
<td>
<p>Name of the family. (Character)
</p>
<p>Currently supports <strong><code>"gaussian"</code></strong> for linear regression
with <code><a href="stats.html#topic+lm">lm()</a></code> / <code><a href="lme4.html#topic+lmer">lme4::lmer()</a></code>
and <strong><code>"binomial"</code></strong> for binary classification
with <code><a href="stats.html#topic+glm">glm()</a></code> / <code><a href="lme4.html#topic+glmer">lme4::glmer()</a></code>.
</p>
<p>See <code><a href="#topic+cross_validate_fn">cross_validate_fn()</a></code> for use with other model functions.</p>
</td></tr>
<tr><td><code id="validate_+3A_test_data">test_data</code></td>
<td>
<p><code>data.frame</code>. If specifying <code>`partitions_col`</code>, this can be <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="validate_+3A_partitions_col">partitions_col</code></td>
<td>
<p>Name of grouping factor for identifying partitions. (Character)
</p>
<p>Rows with the value <code>1</code> in <code>`partitions_col`</code> are used as training set and
rows with the value <code>2</code> are used as test set.
</p>
<p>N.B. <strong>Only used if <code>`test_data`</code> is <code>NULL</code></strong>.</p>
</td></tr>
<tr><td><code id="validate_+3A_control">control</code></td>
<td>
<p>Construct control structures for mixed model fitting
(with <code><a href="lme4.html#topic+lmer">lme4::lmer()</a></code> or <code><a href="lme4.html#topic+glmer">lme4::glmer()</a></code>).
See <code><a href="lme4.html#topic+lmerControl">lme4::lmerControl</a></code> and
<code><a href="lme4.html#topic+glmerControl">lme4::glmerControl</a></code>.
</p>
<p>N.B. Ignored if fitting <code><a href="stats.html#topic+lm">lm()</a></code> or <code><a href="stats.html#topic+glm">glm()</a></code> models.</p>
</td></tr>
<tr><td><code id="validate_+3A_reml">REML</code></td>
<td>
<p>Restricted Maximum Likelihood. (Logical)</p>
</td></tr>
<tr><td><code id="validate_+3A_cutoff">cutoff</code></td>
<td>
<p>Threshold for predicted classes. (Numeric)
</p>
<p>N.B. <strong>Binomial models only</strong></p>
</td></tr>
<tr><td><code id="validate_+3A_positive">positive</code></td>
<td>
<p>Level from dependent variable to predict.
Either as character (<em>preferable</em>) or level index (<code>1</code> or <code>2</code> - alphabetically).
</p>
<p>E.g. if we have the levels <code>"cat"</code> and <code>"dog"</code> and we want <code>"dog"</code> to be the positive class,
we can either provide <code>"dog"</code> or <code>2</code>, as alphabetically, <code>"dog"</code> comes after <code>"cat"</code>.
</p>
<p><strong>Note:</strong> For <em>reproducibility</em>, it's preferable to <strong>specify the name directly</strong>, as
different <code><a href="base.html#topic+locales">locales</a></code> may sort the levels differently.
</p>
<p>Used when calculating confusion matrix metrics and creating <code>ROC</code> curves.
</p>
<p>The <code>Process</code> column in the output can be used to verify this setting.
</p>
<p>N.B. Only affects evaluation metrics, not the model training or returned predictions.
</p>
<p>N.B. <strong>Binomial models only</strong>.</p>
</td></tr>
<tr><td><code id="validate_+3A_metrics">metrics</code></td>
<td>
<p><code>list</code> for enabling/disabling metrics.
</p>
<p>E.g. <code>list("RMSE" = FALSE)</code> would remove <code>RMSE</code> from the results,
and <code>list("Accuracy" = TRUE)</code> would add the regular <code>Accuracy</code> metric
to the classification results.
Default values (<code>TRUE</code>/<code>FALSE</code>) will be used for the remaining available metrics.
</p>
<p>You can enable/disable all metrics at once by including
<code>"all" = TRUE/FALSE</code> in the <code>list</code>. This is done prior to enabling/disabling
individual metrics, why <code>list("all" = FALSE, "RMSE" = TRUE)</code>
would return only the <code>RMSE</code> metric.
</p>
<p>The <code>list</code> can be created with
<code><a href="#topic+gaussian_metrics">gaussian_metrics()</a></code> or
<code><a href="#topic+binomial_metrics">binomial_metrics()</a></code>.
</p>
<p>Also accepts the string <code>"all"</code>.</p>
</td></tr>
<tr><td><code id="validate_+3A_preprocessing">preprocessing</code></td>
<td>
<p>Name of preprocessing to apply.
</p>
<p>Available preprocessings are:
</p>

<table>
<tr>
 <td style="text-align: right;">
  <strong>Name</strong> </td><td style="text-align: right;"> <strong>Description</strong> </td>
</tr>
<tr>
 <td style="text-align: right;">
  "standardize" </td><td style="text-align: right;"> Centers and scales the numeric predictors.</td>
</tr>
<tr>
 <td style="text-align: right;">
  "range" </td><td style="text-align: right;"> Normalizes the numeric predictors to the <code>0</code>-<code>1</code> range.
  Values outside the min/max range in the test fold are truncated to <code>0</code>/<code>1</code>.</td>
</tr>
<tr>
 <td style="text-align: right;">
  "scale" </td><td style="text-align: right;"> Scales the numeric predictors to have a standard deviation of one.</td>
</tr>
<tr>
 <td style="text-align: right;">
  "center" </td><td style="text-align: right;"> Centers the numeric predictors to have a mean of zero.</td>
</tr>
<tr>
 <td style="text-align: right;">
 </td>
</tr>

</table>

<p>The preprocessing parameters (<code>mean</code>, <code>SD</code>, etc.) are extracted from the training folds and
applied to both the training folds and the test fold.
They are returned in the <strong>Preprocess</strong> column for inspection.
</p>
<p>N.B. The preprocessings should not affect the results
to a noticeable degree, although <code>"range"</code> might due to the truncation.</p>
</td></tr>
<tr><td><code id="validate_+3A_err_nc">err_nc</code></td>
<td>
<p>Whether to raise an <code>error</code> if a model does not converge. (Logical)</p>
</td></tr>
<tr><td><code id="validate_+3A_rm_nc">rm_nc</code></td>
<td>
<p>Remove non-converged models from output. (Logical)</p>
</td></tr>
<tr><td><code id="validate_+3A_parallel">parallel</code></td>
<td>
<p>Whether to validate the list of models in parallel. (Logical)
</p>
<p>Remember to register a parallel backend first.
E.g. with <code>doParallel::registerDoParallel</code>.</p>
</td></tr>
<tr><td><code id="validate_+3A_verbose">verbose</code></td>
<td>
<p>Whether to message process information
like the number of model instances to fit and which model function was applied. (Logical)</p>
</td></tr>
<tr><td><code id="validate_+3A_link">link</code>, <code id="validate_+3A_models">models</code>, <code id="validate_+3A_model_verbose">model_verbose</code></td>
<td>
<p>Deprecated.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Packages used:
</p>


<h4>Models</h4>

<p>Gaussian: <code><a href="stats.html#topic+lm">stats::lm</a></code>, <code><a href="lme4.html#topic+lmer">lme4::lmer</a></code>
</p>
<p>Binomial: <code><a href="stats.html#topic+glm">stats::glm</a></code>, <code><a href="lme4.html#topic+glmer">lme4::glmer</a></code>
</p>



<h4>Results</h4>



<h5>Shared</h5>

<p><code>AIC</code> : <code><a href="stats.html#topic+AIC">stats::AIC</a></code>
</p>
<p><code>AICc</code> : <code><a href="MuMIn.html#topic+AICc">MuMIn::AICc</a></code>
</p>
<p><code>BIC</code> : <code><a href="stats.html#topic+BIC">stats::BIC</a></code>
</p>



<h5>Gaussian</h5>

<p><code>r2m</code> : <code><a href="MuMIn.html#topic+r.squaredGLMM">MuMIn::r.squaredGLMM</a></code>
</p>
<p><code>r2c</code> : <code><a href="MuMIn.html#topic+r.squaredGLMM">MuMIn::r.squaredGLMM</a></code>
</p>



<h5>Binomial</h5>

<p><code>ROC and AUC</code>: <code><a href="pROC.html#topic+roc">pROC::roc</a></code>
</p>




<h3>Value</h3>

<p><code>tibble</code> with the results and model objects.
</p>


<h4>Shared across families</h4>

<p>A nested <code>tibble</code> with <strong>coefficients</strong> of the models from all iterations.
</p>
<p>Count of <strong>convergence warnings</strong>. Consider discarding models that did not converge.
</p>
<p>Count of <strong>other warnings</strong>. These are warnings without keywords such as &quot;convergence&quot;.
</p>
<p>Count of <strong>Singular Fit messages</strong>. See
<code><a href="lme4.html#topic+isSingular">lme4::isSingular</a></code> for more information.
</p>
<p>Nested <code>tibble</code> with the <strong>warnings and messages</strong> caught for each model.
</p>
<p>Specified <strong>family</strong>.
</p>
<p>Nested <strong>model</strong> objects.
</p>
<p>Name of <strong>dependent</strong> variable.
</p>
<p>Names of <strong>fixed</strong> effects.
</p>
<p>Names of <strong>random</strong> effects, if any.
</p>
<p>Nested <code>tibble</code> with <strong>preprocess</strong>ing parameters, if any.
</p>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>


<h4>Gaussian Results</h4>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>
<p><strong><code>RMSE</code></strong>, <strong><code>MAE</code></strong>, <strong><code>NRMSE(IQR)</code></strong>,
<strong><code>RRSE</code></strong>, <strong><code>RAE</code></strong>, <strong><code>RMSLE</code></strong>,
<strong><code>AIC</code></strong>, <strong><code>AICc</code></strong>, and <strong><code>BIC</code></strong>.
</p>
<p>See the additional metrics (disabled by default) at <code><a href="#topic+gaussian_metrics">?gaussian_metrics</a></code>.
</p>
<p>A nested <code>tibble</code> with the <strong>predictions</strong> and targets.
</p>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>


<h4>Binomial Results</h4>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>
<p>Based on predictions of the test set,
a confusion matrix and <code>ROC</code> curve are used to get the following:
</p>
<p><code>ROC</code>:
</p>
<p><strong><code>AUC</code></strong>, <strong><code>Lower CI</code></strong>, and <strong><code>Upper CI</code></strong>.
</p>
<p><code>Confusion Matrix</code>:
</p>
<p><strong><code>Balanced Accuracy</code></strong>,
<strong><code>F1</code></strong>,
<strong><code>Sensitivity</code></strong>,
<strong><code>Specificity</code></strong>,
<strong><code>Positive Predictive Value</code></strong>,
<strong><code>Negative Predictive Value</code></strong>,
<strong><code>Kappa</code></strong>,
<strong><code>Detection Rate</code></strong>,
<strong><code>Detection Prevalence</code></strong>,
<strong><code>Prevalence</code></strong>, and
<strong><code>MCC</code></strong> (Matthews correlation coefficient).
</p>
<p>See the additional metrics (disabled by default) at
<code><a href="#topic+binomial_metrics">?binomial_metrics</a></code>.
</p>
<p>Also includes:
</p>
<p>A nested <code>tibble</code> with <strong>predictions</strong>, predicted classes (depends on <code>cutoff</code>), and the targets.
Note, that the predictions are <em>not necessarily</em> of the <em>specified</em> <code>positive</code> class, but of
the <em>model's</em> positive class (second level of dependent variable, alphabetically).
</p>
<p>The <code><a href="pROC.html#topic+roc">pROC::roc</a></code> <strong><code>ROC</code></strong> curve object(s).
</p>
<p>A nested <code>tibble</code> with the <strong>confusion matrix</strong>/matrices.
The <code>Pos_</code> columns tells you whether a row is a
True Positive (<code>TP</code>), True Negative (<code>TN</code>),
False Positive (<code>FP</code>), or False Negative (<code>FN</code>),
depending on which level is the &quot;positive&quot; class. I.e. the level you wish to predict.
</p>
<p>The name of the <strong>Positive Class</strong>.
</p>



<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>


<h3>See Also</h3>

<p>Other validation functions: 
<code><a href="#topic+cross_validate">cross_validate</a>()</code>,
<code><a href="#topic+cross_validate_fn">cross_validate_fn</a>()</code>,
<code><a href="#topic+validate_fn">validate_fn</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Attach packages
library(cvms)
library(groupdata2) # partition()
library(dplyr) # %&gt;% arrange()

# Data is part of cvms
data &lt;- participant.scores

# Set seed for reproducibility
set.seed(7)

# Partition data
# Keep as single data frame
# We could also have fed validate() separate train and test sets.
data_partitioned &lt;- partition(
  data,
  p = 0.7,
  cat_col = "diagnosis",
  id_col = "participant",
  list_out = FALSE
) %&gt;%
  arrange(.partitions)

# Validate a model

# Gaussian
validate(
  data_partitioned,
  formulas = "score~diagnosis",
  partitions_col = ".partitions",
  family = "gaussian",
  REML = FALSE
)

# Binomial
validate(data_partitioned,
  formulas = "diagnosis~score",
  partitions_col = ".partitions",
  family = "binomial"
)

## Feed separate train and test sets

# Partition data to list of data frames
# The first data frame will be train (70% of the data)
# The second will be test (30% of the data)
data_partitioned &lt;- partition(
  data,
  p = 0.7,
  cat_col = "diagnosis",
  id_col = "participant",
  list_out = TRUE
)
train_data &lt;- data_partitioned[[1]]
test_data &lt;- data_partitioned[[2]]

# Validate a model

# Gaussian
validate(
  train_data,
  test_data = test_data,
  formulas = "score~diagnosis",
  family = "gaussian",
  REML = FALSE
)

</code></pre>

<hr>
<h2 id='validate_fn'>Validate a custom model function on a test set</h2><span id='topic+validate_fn'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#experimental"><img src="../help/figures/lifecycle-experimental.svg" alt='[Experimental]' /></a>
</p>
<p>Fit your model function on a training set and validate it by
predicting a test/validation set.
Validate different hyperparameter combinations and formulas at once.
Preprocess the train/test split.
Returns results and fitted models in a <code>tibble</code> for easy reporting and further analysis.
</p>
<p>Compared to <code><a href="#topic+validate">validate()</a></code>,
this function allows you supply a custom model function, a predict function,
a preprocess function and the hyperparameter values to validate.
</p>
<p>Supports regression and classification (binary and multiclass).
See <code>`type`</code>.
</p>
<p>Note that some metrics may not be computable for some types
of model objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>validate_fn(
  train_data,
  formulas,
  type,
  model_fn,
  predict_fn,
  test_data = NULL,
  preprocess_fn = NULL,
  preprocess_once = FALSE,
  hyperparameters = NULL,
  partitions_col = ".partitions",
  cutoff = 0.5,
  positive = 2,
  metrics = list(),
  rm_nc = FALSE,
  parallel = FALSE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="validate_fn_+3A_train_data">train_data</code></td>
<td>
<p><code>data.frame</code>.
</p>
<p>Can contain a grouping factor for identifying partitions - as made with
<code><a href="groupdata2.html#topic+partition">groupdata2::partition()</a></code>.
See <code>`partitions_col`</code>.</p>
</td></tr>
<tr><td><code id="validate_fn_+3A_formulas">formulas</code></td>
<td>
<p>Model formulas as strings. (Character)
</p>
<p>Will be converted to <code><a href="stats.html#topic+formula">formula</a></code> objects
before being passed to <code>`model_fn`</code>.
</p>
<p>E.g. <code>c("y~x", "y~z")</code>.
</p>
<p>Can contain random effects.
</p>
<p>E.g. <code>c("y~x+(1|r)", "y~z+(1|r)")</code>.</p>
</td></tr>
<tr><td><code id="validate_fn_+3A_type">type</code></td>
<td>
<p>Type of evaluation to perform:
</p>
<p><code>"gaussian"</code> for regression (like linear regression).
</p>
<p><code>"binomial"</code> for binary classification.
</p>
<p><code>"multinomial"</code> for multiclass classification.</p>
</td></tr>
<tr><td><code id="validate_fn_+3A_model_fn">model_fn</code></td>
<td>
<p>Model function that returns a fitted model object.
Will usually wrap an existing model function like <code><a href="e1071.html#topic+svm">e1071::svm</a></code>
or <code><a href="nnet.html#topic+multinom">nnet::multinom</a></code>.
</p>
<p>Must have the following function arguments:
</p>
<p><code>function(train_data, formula,</code>
</p>
<p><code style="white-space: pre;">&#8288;         &#8288;</code><code>hyperparameters)</code></p>
</td></tr>
<tr><td><code id="validate_fn_+3A_predict_fn">predict_fn</code></td>
<td>
<p>Function for predicting the targets in the test folds/sets using the fitted model object.
Will usually wrap <code><a href="stats.html#topic+predict">stats::predict()</a></code>, but doesn't have to.
</p>
<p>Must have the following function arguments:
</p>
<p><code>function(test_data, model, formula,</code>
</p>
<p><code style="white-space: pre;">&#8288;         &#8288;</code><code>hyperparameters, train_data)</code>
</p>
<p>Must return predictions in the following formats, depending on <code>`type`</code>:
</p>


<h4>Binomial</h4>

<p><code>vector</code> or one-column <code>matrix</code> / <code>data.frame</code> with probabilities (0-1)
<strong>of the second class, alphabetically</strong>.
E.g.:
</p>
<p><code>c(0.3, 0.5, 0.1, 0.5)</code>
</p>
<p>N.B. When unsure whether a model type produces probabilities based off
the alphabetic order of your classes, using 0 and 1 as classes in the
dependent variable instead of the class names should increase the chance of
getting probabilities of the right class.
</p>



<h4>Gaussian</h4>

<p><code>vector</code> or one-column <code>matrix</code> / <code>data.frame</code> with the predicted value.
E.g.:
</p>
<p><code>c(3.7, 0.9, 1.2, 7.3)</code>
</p>



<h4>Multinomial</h4>

<p><code>data.frame</code> with one column per class containing probabilities of the class.
Column names should be identical to how the class names are written in the target column.
E.g.:
</p>

<table>
<tr>
 <td style="text-align: right;">
  <strong>class_1</strong> </td><td style="text-align: right;"> <strong>class_2</strong> </td><td style="text-align: right;">
  <strong>class_3</strong> </td>
</tr>
<tr>
 <td style="text-align: right;">
  0.269 </td><td style="text-align: right;"> 0.528 </td><td style="text-align: right;"> 0.203</td>
</tr>
<tr>
 <td style="text-align: right;">
  0.368 </td><td style="text-align: right;"> 0.322 </td><td style="text-align: right;"> 0.310</td>
</tr>
<tr>
 <td style="text-align: right;">
  0.375 </td><td style="text-align: right;"> 0.371 </td><td style="text-align: right;"> 0.254</td>
</tr>
<tr>
 <td style="text-align: right;">
  ... </td><td style="text-align: right;"> ... </td><td style="text-align: right;"> ...</td>
</tr>

</table>

</td></tr>
<tr><td><code id="validate_fn_+3A_test_data">test_data</code></td>
<td>
<p><code>data.frame</code>. If specifying <code>`partitions_col`</code>, this can be <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="validate_fn_+3A_preprocess_fn">preprocess_fn</code></td>
<td>
<p>Function for preprocessing the training and test sets.
</p>
<p>Can, for instance, be used to standardize both the training and test sets
with the scaling and centering parameters from the training set.
</p>
<p>Must have the following function arguments:
</p>
<p><code>function(train_data, test_data,</code>
</p>
<p><code style="white-space: pre;">&#8288;         &#8288;</code><code>formula, hyperparameters)</code>
</p>
<p>Must return a <code>list</code> with the preprocessed <code>`train_data`</code> and <code>`test_data`</code>. It may also contain
a <code>tibble</code> with the <code>parameters</code> used in preprocessing:
</p>
<p><code>list("train" = train_data,</code>
</p>
<p><code style="white-space: pre;">&#8288;     &#8288;</code><code>"test" = test_data,</code>
</p>
<p><code style="white-space: pre;">&#8288;     &#8288;</code><code>"parameters" = preprocess_parameters)</code>
</p>
<p>Additional elements in the returned <code>list</code> will be ignored.
</p>
<p>The optional parameters <code>tibble</code> will be included in the output.
It could have the following format:
</p>

<table>
<tr>
 <td style="text-align: right;">
  <strong>Measure</strong> </td><td style="text-align: right;"> <strong>var_1</strong> </td><td style="text-align: right;"> <strong>var_2</strong> </td>
</tr>
<tr>
 <td style="text-align: right;">
  Mean </td><td style="text-align: right;"> 37.921 </td><td style="text-align: right;"> 88.231</td>
</tr>
<tr>
 <td style="text-align: right;">
  SD </td><td style="text-align: right;"> 12.4 </td><td style="text-align: right;"> 5.986</td>
</tr>
<tr>
 <td style="text-align: right;">
  ... </td><td style="text-align: right;"> ... </td><td style="text-align: right;"> ...</td>
</tr>

</table>

<p>N.B. When <code>`preprocess_once`</code> is <code>FALSE</code>, the current formula and
hyperparameters will be provided. Otherwise,
these arguments will be <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="validate_fn_+3A_preprocess_once">preprocess_once</code></td>
<td>
<p>Whether to apply the preprocessing once
(<strong>ignoring</strong> the formula and hyperparameters arguments in <code>`preprocess_fn`</code>)
or for every model separately. (Logical)
</p>
<p>When preprocessing does not depend on the current formula or hyperparameters,
we can do the preprocessing of each train/test split once, to save time.
This <strong>may require holding a lot more data in memory</strong> though,
why it is not the default setting.</p>
</td></tr>
<tr><td><code id="validate_fn_+3A_hyperparameters">hyperparameters</code></td>
<td>
<p>Either a <code>named list</code> with hyperparameter values to combine in a grid
or a <code>data.frame</code> with one row per hyperparameter combination.
</p>


<h4>Named list for grid search</h4>

<p>Add <code>".n"</code> to sample the combinations. Can be the number of combinations to use,
or a percentage between <code>0</code> and <code>1</code>.
</p>
<p>E.g.
</p>
<p><code>list(".n" = 10,  # sample 10 combinations</code>
</p>
<p><code style="white-space: pre;">&#8288;     &#8288;</code><code>"lrn_rate" = c(0.1, 0.01, 0.001),</code>
</p>
<p><code style="white-space: pre;">&#8288;     &#8288;</code><code>"h_layers" = c(10, 100, 1000),</code>
</p>
<p><code style="white-space: pre;">&#8288;     &#8288;</code><code>"drop_out" = runif(5, 0.3, 0.7))</code>
</p>



<h4><code>data.frame</code> with specific hyperparameter combinations</h4>

<p>One row per combination to test.
</p>
<p>E.g.
</p>

<table>
<tr>
 <td style="text-align: right;">
  <strong>lrn_rate</strong> </td><td style="text-align: right;"> <strong>h_layers</strong> </td><td style="text-align: right;"> <strong>drop_out</strong> </td>
</tr>
<tr>
 <td style="text-align: right;">
  0.1 </td><td style="text-align: right;"> 10 </td><td style="text-align: right;"> 0.65</td>
</tr>
<tr>
 <td style="text-align: right;">
  0.1 </td><td style="text-align: right;"> 1000 </td><td style="text-align: right;"> 0.65</td>
</tr>
<tr>
 <td style="text-align: right;">
  0.01 </td><td style="text-align: right;"> 1000 </td><td style="text-align: right;"> 0.63</td>
</tr>
<tr>
 <td style="text-align: right;">
  ... </td><td style="text-align: right;"> ... </td><td style="text-align: right;"> ...</td>
</tr>

</table>

</td></tr>
<tr><td><code id="validate_fn_+3A_partitions_col">partitions_col</code></td>
<td>
<p>Name of grouping factor for identifying partitions. (Character)
</p>
<p>Rows with the value <code>1</code> in <code>`partitions_col`</code> are used as training set and
rows with the value <code>2</code> are used as test set.
</p>
<p>N.B. <strong>Only used if <code>`test_data`</code> is <code>NULL</code></strong>.</p>
</td></tr>
<tr><td><code id="validate_fn_+3A_cutoff">cutoff</code></td>
<td>
<p>Threshold for predicted classes. (Numeric)
</p>
<p>N.B. <strong>Binomial models only</strong></p>
</td></tr>
<tr><td><code id="validate_fn_+3A_positive">positive</code></td>
<td>
<p>Level from dependent variable to predict.
Either as character (<em>preferable</em>) or level index (<code>1</code> or <code>2</code> - alphabetically).
</p>
<p>E.g. if we have the levels <code>"cat"</code> and <code>"dog"</code> and we want <code>"dog"</code> to be the positive class,
we can either provide <code>"dog"</code> or <code>2</code>, as alphabetically, <code>"dog"</code> comes after <code>"cat"</code>.
</p>
<p><strong>Note:</strong> For <em>reproducibility</em>, it's preferable to <strong>specify the name directly</strong>, as
different <code><a href="base.html#topic+locales">locales</a></code> may sort the levels differently.
</p>
<p>Used when calculating confusion matrix metrics and creating <code>ROC</code> curves.
</p>
<p>The <code>Process</code> column in the output can be used to verify this setting.
</p>
<p>N.B. Only affects evaluation metrics, not the model training or returned predictions.
</p>
<p>N.B. <strong>Binomial models only</strong>.</p>
</td></tr>
<tr><td><code id="validate_fn_+3A_metrics">metrics</code></td>
<td>
<p><code>list</code> for enabling/disabling metrics.
</p>
<p>E.g. <code>list("RMSE" = FALSE)</code> would remove <code>RMSE</code> from the regression results,
and <code>list("Accuracy" = TRUE)</code> would add the regular <code>Accuracy</code> metric
to the classification results.
Default values (<code>TRUE</code>/<code>FALSE</code>) will be used for the remaining available metrics.
</p>
<p>You can enable/disable all metrics at once by including
<code>"all" = TRUE/FALSE</code> in the <code>list</code>. This is done prior to enabling/disabling
individual metrics, why f.i. <code>list("all" = FALSE, "RMSE" = TRUE)</code> would return only the <code>RMSE</code> metric.
</p>
<p>The <code>list</code> can be created with
<code><a href="#topic+gaussian_metrics">gaussian_metrics()</a></code>,
<code><a href="#topic+binomial_metrics">binomial_metrics()</a></code>, or
<code><a href="#topic+multinomial_metrics">multinomial_metrics()</a></code>.
</p>
<p>Also accepts the string <code>"all"</code>.</p>
</td></tr>
<tr><td><code id="validate_fn_+3A_rm_nc">rm_nc</code></td>
<td>
<p>Remove non-converged models from output. (Logical)</p>
</td></tr>
<tr><td><code id="validate_fn_+3A_parallel">parallel</code></td>
<td>
<p>Whether to cross-validate the <code>list</code> of models in parallel. (Logical)
</p>
<p>Remember to register a parallel backend first.
E.g. with <code>doParallel::registerDoParallel</code>.</p>
</td></tr>
<tr><td><code id="validate_fn_+3A_verbose">verbose</code></td>
<td>
<p>Whether to message process information
like the number of model instances to fit. (Logical)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Packages used:
</p>


<h4>Results</h4>



<h5>Shared</h5>

<p>AIC : <code><a href="stats.html#topic+AIC">stats::AIC</a></code>
</p>
<p>AICc : <code><a href="MuMIn.html#topic+AICc">MuMIn::AICc</a></code>
</p>
<p>BIC : <code><a href="stats.html#topic+BIC">stats::BIC</a></code>
</p>



<h5>Gaussian</h5>

<p>r2m : <code><a href="MuMIn.html#topic+r.squaredGLMM">MuMIn::r.squaredGLMM</a></code>
</p>
<p>r2c : <code><a href="MuMIn.html#topic+r.squaredGLMM">MuMIn::r.squaredGLMM</a></code>
</p>



<h5>Binomial and Multinomial</h5>

<p>ROC and related metrics:
</p>
<p>Binomial: <code><a href="pROC.html#topic+roc">pROC::roc</a></code>
</p>
<p>Multinomial: <code><a href="pROC.html#topic+multiclass.roc">pROC::multiclass.roc</a></code>
</p>




<h3>Value</h3>

<p><code>tibble</code> with the results and model objects.
</p>


<h4>Shared across families</h4>

<p>A nested <code>tibble</code> with <strong>coefficients</strong> of the models. The coefficients
are extracted from the model object with <code><a href="parameters.html#topic+model_parameters">parameters::model_parameters()</a></code> or
<code><a href="stats.html#topic+coef">coef()</a></code> (with some restrictions on the output).
If these attempts fail, a default coefficients <code>tibble</code> filled with <code>NA</code>s is returned.
</p>
<p>Nested <code>tibble</code> with the used <strong>preprocessing parameters</strong>,
if a passed <code>`preprocess_fn`</code> returns the parameters in a <code>tibble</code>.
</p>
<p>Count of <strong>convergence warnings</strong>, using a limited set of keywords (e.g. &quot;convergence&quot;). If a
convergence warning does not contain one of these keywords, it will be counted with <strong>other warnings</strong>.
Consider discarding models that did not converge on all iterations.
Note: you might still see results, but these should be taken with a grain of salt!
</p>
<p>Nested <code>tibble</code> with the <strong>warnings and messages</strong> caught for each model.
</p>
<p>Specified <strong>family</strong>.
</p>
<p>Nested <strong>model</strong> objects.
</p>
<p>Name of <strong>dependent</strong> variable.
</p>
<p>Names of <strong>fixed</strong> effects.
</p>
<p>Names of <strong>random</strong> effects, if any.
</p>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>


<h4>Gaussian Results</h4>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>
<p><strong><code>RMSE</code></strong>, <strong><code>MAE</code></strong>, <strong><code>NRMSE(IQR)</code></strong>,
<strong><code>RRSE</code></strong>, <strong><code>RAE</code></strong>, and <strong><code>RMSLE</code></strong>.
</p>
<p>See the additional metrics (disabled by default) at <code><a href="#topic+gaussian_metrics">?gaussian_metrics</a></code>.
</p>
<p>A nested <code>tibble</code> with the <strong>predictions</strong> and targets.
</p>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>


<h4>Binomial Results</h4>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>
<p>Based on predictions of the test set,
a confusion matrix and a <code>ROC</code> curve are created to get the following:
</p>
<p><code>ROC</code>:
</p>
<p><strong><code>AUC</code></strong>, <strong><code>Lower CI</code></strong>, and <strong><code>Upper CI</code></strong>
</p>
<p><code>Confusion Matrix</code>:
</p>
<p><strong><code>Balanced Accuracy</code></strong>,
<strong><code>F1</code></strong>,
<strong><code>Sensitivity</code></strong>,
<strong><code>Specificity</code></strong>,
<strong><code>Positive Predictive Value</code></strong>,
<strong><code>Negative Predictive Value</code></strong>,
<strong><code>Kappa</code></strong>,
<strong><code>Detection Rate</code></strong>,
<strong><code>Detection Prevalence</code></strong>,
<strong><code>Prevalence</code></strong>, and
<strong><code>MCC</code></strong> (Matthews correlation coefficient).
</p>
<p>See the additional metrics (disabled by default) at
<code><a href="#topic+binomial_metrics">?binomial_metrics</a></code>.
</p>
<p>Also includes:
</p>
<p>A nested <code>tibble</code> with <strong>predictions</strong>, predicted classes (depends on <code>cutoff</code>), and the targets.
Note, that the predictions are <em>not necessarily</em> of the <em>specified</em> <code>positive</code> class, but of
the <em>model's</em> positive class (second level of dependent variable, alphabetically).
</p>
<p>The <code><a href="pROC.html#topic+roc">pROC::roc</a></code> <strong><code>ROC</code></strong> curve object(s).
</p>
<p>A nested <code>tibble</code> with the <strong>confusion matrix</strong>/matrices.
The <code>Pos_</code> columns tells you whether a row is a
True Positive (<code>TP</code>), True Negative (<code>TN</code>),
False Positive (<code>FP</code>), or False Negative (<code>FN</code>),
depending on which level is the &quot;positive&quot; class. I.e. the level you wish to predict.
</p>
<p>The name of the <strong>Positive Class</strong>.
</p>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>


<h4>Multinomial Results</h4>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>
<p>For each class, a <em>one-vs-all</em> binomial evaluation is performed. This creates
a <strong>Class Level Results</strong> <code>tibble</code> containing the same metrics as the binomial results
described above (excluding <code>MCC</code>, <code>AUC</code>, <code>Lower CI</code> and <code>Upper CI</code>),
along with a count of the class in the target column (<strong><code>Support</code></strong>).
These metrics are used to calculate the <strong>macro-averaged</strong> metrics.
The nested class level results <code>tibble</code> is also included in the output <code>tibble</code>,
and could be reported along with the macro and overall metrics.
</p>
<p>The output <code>tibble</code> contains the macro and overall metrics.
The metrics that share their name with the metrics in the nested
class level results <code>tibble</code> are averages of those metrics
(note: does not remove <code>NA</code>s before averaging).
In addition to these, it also includes the <strong><code>Overall Accuracy</code></strong> and
the multiclass <strong><code>MCC</code></strong>.
</p>
<p><strong>Note:</strong> <strong><code>Balanced Accuracy</code></strong> is the macro-averaged metric,
<em>not</em> the macro sensitivity as sometimes used!
</p>
<p>Other available metrics (disabled by default, see <code>metrics</code>):
<strong><code>Accuracy</code></strong>,
<em>multiclass</em> <strong><code>AUC</code></strong>,
<strong><code>Weighted Balanced Accuracy</code></strong>,
<strong><code>Weighted Accuracy</code></strong>,
<strong><code>Weighted F1</code></strong>,
<strong><code>Weighted Sensitivity</code></strong>,
<strong><code>Weighted Sensitivity</code></strong>,
<strong><code>Weighted Specificity</code></strong>,
<strong><code>Weighted Pos Pred Value</code></strong>,
<strong><code>Weighted Neg Pred Value</code></strong>,
<strong><code>Weighted Kappa</code></strong>,
<strong><code>Weighted Detection Rate</code></strong>,
<strong><code>Weighted Detection Prevalence</code></strong>, and
<strong><code>Weighted Prevalence</code></strong>.
</p>
<p>Note that the &quot;Weighted&quot; average metrics are weighted by the <code>Support</code>.
</p>
<p>Also includes:
</p>
<p>A nested <code>tibble</code> with the <strong>predictions</strong>, predicted classes, and targets.
</p>
<p>A list of <strong>ROC</strong> curve objects when <code>AUC</code> is enabled.
</p>
<p>A nested <code>tibble</code> with the multiclass <strong>Confusion Matrix</strong>.
</p>
<p><strong>Class Level Results</strong>
</p>
<p>Besides the binomial evaluation metrics and the <code>Support</code>,
the nested class level results <code>tibble</code> also contains a
nested <code>tibble</code> with the <strong>Confusion Matrix</strong> from the one-vs-all evaluation.
The <code>Pos_</code> columns tells you whether a row is a
True Positive (<code>TP</code>), True Negative (<code>TN</code>),
False Positive (<code>FP</code>), or False Negative (<code>FN</code>),
depending on which level is the &quot;positive&quot; class. In our case, <code>1</code> is the current class
and <code>0</code> represents all the other classes together.
</p>



<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>


<h3>See Also</h3>

<p>Other validation functions: 
<code><a href="#topic+cross_validate">cross_validate</a>()</code>,
<code><a href="#topic+cross_validate_fn">cross_validate_fn</a>()</code>,
<code><a href="#topic+validate">validate</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Attach packages
library(cvms)
library(groupdata2) # fold()
library(dplyr) # %&gt;% arrange() mutate()

# Note: More examples of custom functions can be found at:
# model_fn: model_functions()
# predict_fn: predict_functions()
# preprocess_fn: preprocess_functions()

# Data is part of cvms
data &lt;- participant.scores

# Set seed for reproducibility
set.seed(7)

# Fold data
data &lt;- partition(
  data,
  p = 0.8,
  cat_col = "diagnosis",
  id_col = "participant",
  list_out = FALSE
) %&gt;%
  mutate(diagnosis = as.factor(diagnosis)) %&gt;%
  arrange(.partitions)

# Formulas to validate

formula_gaussian &lt;- "score ~ diagnosis"
formula_binomial &lt;- "diagnosis ~ score"

#
# Gaussian
#

# Create model function that returns a fitted model object
lm_model_fn &lt;- function(train_data, formula, hyperparameters) {
  lm(formula = formula, data = train_data)
}

# Create predict function that returns the predictions
lm_predict_fn &lt;- function(test_data, model, formula,
                          hyperparameters, train_data) {
  stats::predict(
    object = model,
    newdata = test_data,
    type = "response",
    allow.new.levels = TRUE
  )
}

# Validate the model function
v &lt;- validate_fn(
  data,
  formulas = formula_gaussian,
  type = "gaussian",
  model_fn = lm_model_fn,
  predict_fn = lm_predict_fn,
  partitions_col = ".partitions"
)

v

# Extract model object
v$Model[[1]]

#
# Binomial
#

# Create model function that returns a fitted model object
glm_model_fn &lt;- function(train_data, formula, hyperparameters) {
  glm(formula = formula, data = train_data, family = "binomial")
}

# Create predict function that returns the predictions
glm_predict_fn &lt;- function(test_data, model, formula,
                           hyperparameters, train_data) {
  stats::predict(
    object = model,
    newdata = test_data,
    type = "response",
    allow.new.levels = TRUE
  )
}

# Validate the model function
validate_fn(
  data,
  formulas = formula_binomial,
  type = "binomial",
  model_fn = glm_model_fn,
  predict_fn = glm_predict_fn,
  partitions_col = ".partitions"
)

#
# Support Vector Machine (svm)
# with known hyperparameters
#

# Only run if the `e1071` package is installed
if (requireNamespace("e1071", quietly = TRUE)){

# Create model function that returns a fitted model object
# We use the hyperparameters arg to pass in the kernel and cost values
# These will usually have been found with cross_validate_fn()
svm_model_fn &lt;- function(train_data, formula, hyperparameters) {

  # Expected hyperparameters:
  #  - kernel
  #  - cost
  if (!"kernel" %in% names(hyperparameters))
    stop("'hyperparameters' must include 'kernel'")
  if (!"cost" %in% names(hyperparameters))
    stop("'hyperparameters' must include 'cost'")

  e1071::svm(
    formula = formula,
    data = train_data,
    kernel = hyperparameters[["kernel"]],
    cost = hyperparameters[["cost"]],
    scale = FALSE,
    type = "C-classification",
    probability = TRUE
  )
}

# Create predict function that returns the predictions
svm_predict_fn &lt;- function(test_data, model, formula,
                           hyperparameters, train_data) {
  predictions &lt;- stats::predict(
    object = model,
    newdata = test_data,
    allow.new.levels = TRUE,
    probability = TRUE
  )

  # Extract probabilities
  probabilities &lt;- dplyr::as_tibble(
    attr(predictions, "probabilities")
  )

  # Return second column
  probabilities[[2]]
}

# Specify hyperparameters to use
# We found these in the examples in ?cross_validate_fn()
svm_hparams &lt;- list(
  "kernel" = "linear",
  "cost" = 10
)

# Validate the model function
validate_fn(
  data,
  formulas = formula_binomial,
  type = "binomial",
  model_fn = svm_model_fn,
  predict_fn = svm_predict_fn,
  hyperparameters = svm_hparams,
  partitions_col = ".partitions"
)
}  # closes `e1071` package check

</code></pre>

<hr>
<h2 id='wines'>Wine varieties</h2><span id='topic+wines'></span>

<h3>Description</h3>

<p>A list of wine varieties in an approximately Zipfian distribution, ordered by descending frequencies.
</p>


<h3>Format</h3>

<p>A <code>data.frame</code> with <code>368</code> rows and <code>1</code> variable:
</p>

<dl>
<dt>Variety</dt><dd><p>Wine variety, 10 levels</p>
</dd>
</dl>



<h3>Details</h3>

<p>Based on the wine-reviews (v4) kaggle dataset by Zack Thoutt:
https://www.kaggle.com/zynicide/wine-reviews
</p>


<h3>Author(s)</h3>

<p>Ludvig Renbo Olsen, <a href="mailto:r-pkgs@ludvigolsen.dk">r-pkgs@ludvigolsen.dk</a>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
