<!DOCTYPE html><html lang="en"><head><title>Help for package enmSdmX</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {enmSdmX}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#enmSdmX'><p>enmSdmX: Species distribution modeling and ecological niche modeling</p></a></li>
<li><a href='#.calcWeights'><p>Calculate weights for a model</p></a></li>
<li><a href='#.cardinalDistance'><p>Movement of occupied cells in a given direction of a fixed point</p></a></li>
<li><a href='#.euclid'><p>Euclidean distance between a pair of points</p></a></li>
<li><a href='#.interpCoordFromQuantile'><p>Latitude of quantile(s) of the geographic abundance distribution</p></a></li>
<li><a href='#.scalePredictors'><p>Scales predictors</p></a></li>
<li><a href='#bioticVelocity'><p>Velocity of shifts in densities across a series of rasters</p></a></li>
<li><a href='#canada'><p>Vector outline of Canada</p></a></li>
<li><a href='#compareResponse'><p>Compare two response curves along one or more predictors</p></a></li>
<li><a href='#coordImprecision'><p>Calculate the precision of a geographic coordinate</p></a></li>
<li><a href='#countPoints'><p>Number of points in a &quot;spatial points&quot; object</p></a></li>
<li><a href='#crss'><p>Coordinate reference systems (CRSs) and nicknames</p></a></li>
<li><a href='#customAlbers'><p>Custom coordinate reference system WKT2 string</p></a></li>
<li><a href='#decimalToDms'><p>Convert geographic coordinates in decimal format to degrees-minutes-second</p></a></li>
<li><a href='#dmsToDecimal'><p>Convert geographic coordinates in degrees-minutes-second to decimal format</p></a></li>
<li><a href='#elimCellDuplicates'><p>Thin spatial points so that there is but one per raster cell</p></a></li>
<li><a href='#evalAUC'><p>Weighted AUC</p></a></li>
<li><a href='#evalContBoyce'><p>Continuous Boyce Index (CBI) with weighting</p></a></li>
<li><a href='#evalMultiAUC'><p>Calculate multivariate weighted AUC</p></a></li>
<li><a href='#evalThreshold'><p>Weighted thresholds for predictions</p></a></li>
<li><a href='#evalThresholdStats'><p>Thresholded evaluation statistics</p></a></li>
<li><a href='#evalTjursR2'><p>Weighted Tjur's R2</p></a></li>
<li><a href='#evalTSS'><p>Weighted True Skill Statistic (TSS)</p></a></li>
<li><a href='#extentToVect'><p>Convert extent to a spatial polygon</p></a></li>
<li><a href='#geoFold'><p>Assign geographically-distinct k-folds</p></a></li>
<li><a href='#geoFoldContrast'><p>Assign geographically-distinct k-folds to background/absence sites</p></a></li>
<li><a href='#geoThin'><p>Thin geographic points deterministically or randomly</p></a></li>
<li><a href='#getCRS'><p>WKT string for a named coordinate reference system or a spatial object</p></a></li>
<li><a href='#getValueByCell'><p>Get or assign values to cells in a raster</p></a></li>
<li><a href='#globalx'><p>&quot;Friendly&quot; wrapper for terra::global() for calculating raster statistics</p></a></li>
<li><a href='#interpolateRasts'><p>Interpolate values from a series of rasters</p></a></li>
<li><a href='#lemurs'><p>Lemur occurrences from GBIF</p></a></li>
<li><a href='#longLatRasts'><p>Generate rasters with cell values equal to cell longitude or latitude</p></a></li>
<li><a href='#mad0'><p>Madagascar spatial object</p></a></li>
<li><a href='#mad1'><p>Madagascar spatial object</p></a></li>
<li><a href='#madClim'><p>Present-day climate rasters for Madagascar</p></a></li>
<li><a href='#madClim2030'><p>Future climate rasters for Madagascar</p></a></li>
<li><a href='#madClim2050'><p>Future climate rasters for Madagascar</p></a></li>
<li><a href='#madClim2070'><p>Future climate rasters for Madagascar</p></a></li>
<li><a href='#madClim2090'><p>Future climate rasters for Madagascar</p></a></li>
<li><a href='#modelSize'><p>Number of response data in a model object</p></a></li>
<li><a href='#nearestEnvPoints'><p>Extract &quot;most conservative&quot; environments from points and/or polygons</p></a></li>
<li><a href='#nearestGeogPoints'><p>Minimum convex polygon from a set of spatial polygons and/or points</p></a></li>
<li><a href='#nicheOverlapMetrics'><p>Metrics of niche overlap</p></a></li>
<li><a href='#plotExtent'><p>Create spatial polygon same size as a plot</p></a></li>
<li><a href='#predictEnmSdm'><p>Generic predict function for SDMs/ENMs</p></a></li>
<li><a href='#predictMaxEnt'><p>Predict a MaxEnt model object (with optional feature-level permutation)</p></a></li>
<li><a href='#predictMaxNet'><p>Predictions from a MaxNet model</p></a></li>
<li><a href='#sampleRast'><p>Sample random points from a raster with/out replacement</p></a></li>
<li><a href='#spatVectorToSpatial'><p>Convert SpatVector to Spatial*</p></a></li>
<li><a href='#squareCellRast'><p>Create a raster with square cells</p></a></li>
<li><a href='#summaryByCrossValid'><p>Summarize distribution/niche model cross-validation object</p></a></li>
<li><a href='#trainBRT'><p>Calibrate a boosted regression tree (generalized boosting machine) model</p></a></li>
<li><a href='#trainByCrossValid'><p>Calibrate a distribution/niche model using cross-validation</p></a></li>
<li><a href='#trainESM'><p>Calibrate an ensemble of small models</p></a></li>
<li><a href='#trainGAM'><p>Calibrate a generalized additive model (GAM)</p></a></li>
<li><a href='#trainGLM'><p>Calibrate a generalized linear model (GLM)</p></a></li>
<li><a href='#trainMaxEnt'><p>Calibrate a MaxEnt model using AICc</p></a></li>
<li><a href='#trainMaxNet'><p>Calibrate a MaxNet model using AICc</p></a></li>
<li><a href='#trainNS'><p>Calibrate a natural splines model</p></a></li>
<li><a href='#trainRF'><p>Calibrate a random forest model</p></a></li>
<li><a href='#troubleshooting_parallel_operations'><p>Troubleshooting parallel operations</p></a></li>
<li><a href='#weightByDist'><p>Proximity-based weighting for occurrences to correct for spatial bias</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Species Distribution Modeling and Ecological Niche Modeling</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2.10</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-12-10</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements species distribution modeling and ecological niche
	modeling, including: bias correction, spatial cross-validation, model
	evaluation, raster interpolation, biotic "velocity" (speed and
	direction of movement of a "mass" represented by a raster), interpolating
	across a time series of rasters, and use of spatially imprecise records.
	The heart of the package is a set of "training" functions which
	automatically optimize model complexity based number of available
	occurrences. These algorithms include MaxEnt, MaxNet, boosted regression
	trees/gradient boosting machines, generalized additive models,
	generalized linear models, natural splines, and random forests. To enhance
	interoperability with other modeling packages, no new classes are created.
	The package works with 'PROJ6' geodetic objects and coordinate reference
	systems.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>AICcmodavg, boot, data.table, doParallel, DT, foreach, gbm,
graphics, ks, maxnet, methods, mgcv, omnibus, parallel,
predicts, ranger, rJava, scales, sf, shiny, sp, statisfactory,
stats, terra, utils</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/adamlilith/enmSdmX">https://github.com/adamlilith/enmSdmX</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/adamlilith/enmSdmX/issues">https://github.com/adamlilith/enmSdmX/issues</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-12-11 02:19:19 UTC; adame</td>
</tr>
<tr>
<td>Author:</td>
<td>Adam B. Smith <a href="https://orcid.org/0000-0002-6420-1659"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [cre, aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Adam B. Smith &lt;adam.smith@mobot.org&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-12-11 10:10:08 UTC</td>
</tr>
</table>
<hr>
<h2 id='enmSdmX'>enmSdmX: Species distribution modeling and ecological niche modeling</h2><span id='topic+enmSdmX-package'></span><span id='topic+enmSdmX'></span>

<h3>Description</h3>

<p>Tools for implementing species distribution models and ecological niche models, including: bias correction, spatial cross-validation, model evaluation, raster interpolation, biotic &quot;velocity&quot; (speed and direction of movement of a &quot;mass&quot; represented by a raster), and tools for using spatially imprecise records. The heart of the package is a set of &quot;training&quot; functions which automatically optimize model complexity based number of available occurrences. These algorithms include MaxEnt, MaxNet, boosted regression trees/gradient boosting machines, generalized additive models, generalized linear models,	natural splines, and random forests. To enhance interoperability to and from other packages, the package does not create any new classes. The package works with PROJ6 geodetic objects and coordinate reference systems.<br /> <br />
</p>


<h3>Details</h3>

<p>Create an issue on <a href="https://github.com/adamlilith/enmSdmX/issues">GitHub</a>.
</p>


<h3>Using imprecisely-georeferenced occurrences</h3>

<p><code><a href="#topic+coordImprecision">coordImprecision</a></code>: Coordinate imprecision <br />
<code><a href="#topic+nearestEnvPoints">nearestEnvPoints</a></code>: Extract &quot;most conservative&quot; environments from points and/or polygons <br />
<code><a href="#topic+nearestGeogPoints">nearestGeogPoints</a></code>: Create a minimum convex polygon from a set of spatial polygons and/or points <br />
</p>


<h3>Data preparation</h3>

<p><code><a href="#topic+geoFold">geoFold</a></code>: Assign geographically-distinct k-folds <br />
<code><a href="#topic+geoFoldContrast">geoFoldContrast</a></code>: Assign geographically-distinct k-folds to background or absence sites<br />
<code><a href="#topic+elimCellDuplicates">elimCellDuplicates</a></code>: Eliminate duplicate points in each cell of a raster <br />
</p>


<h3>Bias correction</h3>

<p><code><a href="#topic+geoThin">geoThin</a></code>: Thin geographic points deterministically or randomly <br />
<code><a href="#topic+weightByDist">weightByDist</a></code>: Proximity-based weighting for occurrences (points) to correct for spatial bias <br />
</p>


<h3>Model calibration</h3>

<p><code><a href="#topic+trainByCrossValid">trainByCrossValid</a></code>: and <code><a href="#topic+summaryByCrossValid">summaryByCrossValid</a></code>: Implement a <code>trainXYZ</code> function across calibration folds (which are distinct from evaluation folds). <br />
<code><a href="#topic+trainBRT">trainBRT</a></code>: Boosted regression trees (BRTs) <br />
<code><a href="#topic+trainESM">trainESM</a></code>: Ensembles of small models (ESMs) <br />
<code><a href="#topic+trainGLM">trainGLM</a></code>: Generalized linear models (GLMs) <br />
<code><a href="#topic+trainGLM">trainGLM</a></code>: Generalized linear models (GLMs) <br />
<code><a href="#topic+trainMaxEnt">trainMaxEnt</a></code>: MaxEnt models <br />
<code><a href="#topic+trainMaxNet">trainMaxNet</a></code>: MaxNet models
<code><a href="#topic+trainNS">trainNS</a></code>: Natural spline (NS) models <br />
<code><a href="#topic+trainRF">trainRF</a></code>: Random forest (RF) models <br />
</p>


<h3>Model prediction</h3>

<p><code><a href="#topic+predictEnmSdm">predictEnmSdm</a></code>: Predict most model types using default settings <br />
<code><a href="#topic+predictMaxEnt">predictMaxEnt</a></code>: Predict MaxEnt model <br />
<code><a href="#topic+predictMaxNet">predictMaxNet</a></code>: Predict MaxNet model <br />
</p>


<h3>Model evaluation</h3>

<p><code><a href="#topic+evalAUC">evalAUC</a></code>: AUC (with/out site weights) <br />
<code><a href="#topic+evalMultiAUC">evalMultiAUC</a></code>: Multivariate version of AUC (with/out site weight) <br />
<code><a href="#topic+evalContBoyce">evalContBoyce</a></code>: Continuous Boyce Index (with/out site weights) <br />
<code><a href="#topic+evalThreshold">evalThreshold</a></code>: Thresholds to convert continuous predictions to binary predictions (with/out site weights) <br />
<code><a href="#topic+evalThresholdStats">evalThresholdStats</a></code>: Model performance statistics based on thresholded predictions (with/out site weights) <br />
<code><a href="#topic+evalTjursR2">evalTjursR2</a></code>: Tjur's R2 (with/out site weights) <br />
<code><a href="#topic+evalTSS">evalTSS</a></code>: True Skill Statistic (TSS) (with/out site weights) <br />
<code><a href="#topic+modelSize">modelSize</a></code>: Number of response values in a model object <br />
</p>


<h3>Functions for rasters</h3>

<p><code><a href="#topic+bioticVelocity">bioticVelocity</a></code>: Velocity of movement across a series of rasters <br />
<code><a href="#topic+getValueByCell">getValueByCell</a></code>: Get value(s) in raster cell(s) by cell number <br />
<code><a href="#topic+globalx">globalx</a></code>: &quot;Friendly&quot; wrapper for terra::global() for calculating raster statistics <br />
<code><a href="#topic+interpolateRasts">interpolateRasts</a></code>: Interpolate a stack of rasters <br />
<code><a href="#topic+longLatRasts">longLatRasts</a></code>: Generate rasters with values of longitude/latitude for cell values <br />
<code><a href="#topic+sampleRast">sampleRast</a></code>: Sample raster with/out replacement <br />
<code><a href="#topic+setValueByCell">setValueByCell</a></code>: Set value(s) in raster cell(s) by cell number <br />
<code><a href="#topic+squareCellRast">squareCellRast</a></code>: Create a raster with square cells <br />
</p>


<h3>Niche overlap and similarity</h3>

<p><code><a href="#topic+compareResponse">compareResponse</a></code>: Compare niche model responses to a single variable <br />
<code><a href="#topic+nicheOverlapMetrics">nicheOverlapMetrics</a></code>: Niche overlap metrics <br />
</p>


<h3>Coordinate reference systems</h3>

<p><code><a href="#topic+getCRS">getCRS</a></code>: Return a WKT2 string (coordinate reference system string) using a nickname <br />
<code><a href="#topic+crss">crss</a></code>: Table of coordinate reference systems and nicknames <br />
<code><a href="#topic+customAlbers">customAlbers</a></code>: Create a custom Albers conic equal-area projection <br />
<code><a href="#topic+customLambert">customLambert</a></code>: Create a custom Lambert azimuthal equal-area projection <br />
<code><a href="#topic+customVNS">customVNS</a></code>: Create a custom &quot;vertical near-side&quot; projection <br />
</p>


<h3>Geographic utility functions</h3>

<p><code><a href="#topic+countPoints">countPoints</a></code>: Number of points in a &quot;spatial points&quot; object <br />
<code><a href="#topic+decimalToDms">decimalToDms</a></code>: Convert decimal coordinate to degrees-minutes-seconds <br />
<code><a href="#topic+dmsToDecimal">dmsToDecimal</a></code>: Convert degrees-minutes-seconds coordinate to decimal <br />
<code><a href="#topic+extentToVect">extentToVect</a></code>: Convert extent to polygon <br />
<code><a href="#topic+plotExtent">plotExtent</a></code>: Create a 'SpatialPolygon' the same size as a plot region <br />
<code><a href="#topic+spatVectorToSpatial">spatVectorToSpatial</a></code>: Convert <code>SpatVector</code> object to a <code>Spatial</code>* object. <br />
</p>


<h3>Data</h3>

<p><code><a href="#topic+canada">canada</a></code>: Outline of Canada <br />
<code><a href="#topic+lemurs">lemurs</a></code>: Lemur occurrences <br />
<code><a href="#topic+mad0">mad0</a></code>: Madagascar spatial object <br />
<code><a href="#topic+mad1">mad1</a></code>: Madagascar spatial object <br />
<code><a href="#topic+madClim">madClim</a></code>: Madagascar climate rasters for the present <br />
<code><a href="#topic+madClim2030">madClim2030</a></code>: Madagascar climate rasters for the 2030s <br />
<code><a href="#topic+madClim2050">madClim2050</a></code>: Madagascar climate rasters for the 2050s <br />
<code><a href="#topic+madClim2070">madClim2070</a></code>: Madagascar climate rasters for the 2070s <br />
<code><a href="#topic+madClim2090">madClim2090</a></code>: Madagascar climate rasters for the 2090s <br />
</p>


<h3>Author(s)</h3>

<p>Adam B. Smith
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/adamlilith/enmSdmX">https://github.com/adamlilith/enmSdmX</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/adamlilith/enmSdmX/issues">https://github.com/adamlilith/enmSdmX/issues</a>
</p>
</li></ul>


<hr>
<h2 id='.calcWeights'>Calculate weights for a model</h2><span id='topic+.calcWeights'></span>

<h3>Description</h3>

<p>Calculates weighting for a model. Each record receives a numeric weight.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.calcWeights(w, data, resp, family)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id=".calcWeights_+3A_w">w</code></td>
<td>
<p>Either logical in which case <code>TRUE</code> (default) causes the total weight of presences to equal the total weight of absences (if <code>family='binomial'</code>) <em>or</em> a numeric vector of weights, one per row in <code>data</code> <em>or</em> the name of the column in <code>data</code> that contains site weights. If <code>FALSE</code>, then each datum gets a weight of 1.</p>
</td></tr>
<tr><td><code id=".calcWeights_+3A_data">data</code></td>
<td>
<p>Data frame</p>
</td></tr>
<tr><td><code id=".calcWeights_+3A_resp">resp</code></td>
<td>
<p>Name of response column</p>
</td></tr>
<tr><td><code id=".calcWeights_+3A_family">family</code></td>
<td>
<p>Name of family</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector.
</p>

<hr>
<h2 id='.cardinalDistance'>Movement of occupied cells in a given direction of a fixed point</h2><span id='topic+.cardinalDistance'></span>

<h3>Description</h3>

<p>This function calculates the weighted distance moved by a mass represented by set of cells which fall north, south, east, or west of a given location (i.e., typically the centroid of the starting population). Values &gt;0 confer movement to the north, south, east, or west of this location.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.cardinalDistance(
  direction,
  longOrLat,
  coordVect,
  x1,
  x2,
  refCoord,
  x1weightedLongs,
  x1weightedLats,
  x2weightedLongs,
  x2weightedLats,
  x1weightedElev = NULL,
  x2weightedElev = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id=".cardinalDistance_+3A_direction">direction</code></td>
<td>
<p>Any of: <code>'n'</code> (north), <code>'s'</code> (south), <code>'e'</code> (east), or <code>'w'</code> (west).</p>
</td></tr>
<tr><td><code id=".cardinalDistance_+3A_longorlat">longOrLat</code></td>
<td>
<p>Numeric matrix, latitude or longitudes. If <code>direction</code> is <code>'n'</code> or <code>'s'</code> this must be latitudes. If <code>direction</code> is <code>'e'</code> or <code>'w'</code> this must be longitudes.</p>
</td></tr>
<tr><td><code id=".cardinalDistance_+3A_coordvect">coordVect</code></td>
<td>
<p>Vector of latitude or longitude of cell centers, depending on value of <code>longOrLat</code>. If latitude, these <em>must</em> go from south to north. If <code>longitude</code>, these <em>must</em> go from west to east.</p>
</td></tr>
<tr><td><code id=".cardinalDistance_+3A_x1">x1</code></td>
<td>
<p>Matrix of weights in time 1 (i.e., population size).</p>
</td></tr>
<tr><td><code id=".cardinalDistance_+3A_x2">x2</code></td>
<td>
<p>Matrix of weights in time 2 (i.e., population size).</p>
</td></tr>
<tr><td><code id=".cardinalDistance_+3A_refcoord">refCoord</code></td>
<td>
<p>Numeric, latitude or longitude (depending on <code>longOrLat</code>) of reference point from which to partition the weights into a northern, southern, eastern, or western portion.</p>
</td></tr>
<tr><td><code id=".cardinalDistance_+3A_x1weightedlongs">x1weightedLongs</code></td>
<td>
<p>Matrix of longitudes weighted (i.e., by population size, given by <code>x1</code>).</p>
</td></tr>
<tr><td><code id=".cardinalDistance_+3A_x1weightedlats">x1weightedLats</code></td>
<td>
<p>Matrix of latitudes weighted (i.e., by population size, given by <code>x1</code>).</p>
</td></tr>
<tr><td><code id=".cardinalDistance_+3A_x2weightedlongs">x2weightedLongs</code></td>
<td>
<p>Matrix of longitudes weighted (i.e., by population size, given by <code>x2</code>).</p>
</td></tr>
<tr><td><code id=".cardinalDistance_+3A_x2weightedlats">x2weightedLats</code></td>
<td>
<p>Matrix of latitudes weighted (i.e., by population size, given by <code>x2</code>).</p>
</td></tr>
<tr><td><code id=".cardinalDistance_+3A_x1weightedelev">x1weightedElev</code></td>
<td>
<p>Matrix of elevations weighted by x1 or <code>NULL</code>.</p>
</td></tr>
<tr><td><code id=".cardinalDistance_+3A_x2weightedelev">x2weightedElev</code></td>
<td>
<p>Matrix of elevations weighted by x2 or <code>NULL</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list object with distance moved and abundance of all cells north/south/east/west of reference point.
</p>

<hr>
<h2 id='.euclid'>Euclidean distance between a pair of points</h2><span id='topic+.euclid'></span>

<h3>Description</h3>

<p>Euclidean distance between a pair of points or two points. Note that the output is unsigned if <code>x2</code> and <code>y2</code> are provided, but signed if not.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.euclid(a, b)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id=".euclid_+3A_a">a</code></td>
<td>
<p>Numeric vector from 1 to 3 elements long</p>
</td></tr>
<tr><td><code id=".euclid_+3A_b">b</code></td>
<td>
<p>Numeric vector from 1 to 3 elements long</p>
</td></tr>
</table>

<hr>
<h2 id='.interpCoordFromQuantile'>Latitude of quantile(s) of the geographic abundance distribution</h2><span id='topic+.interpCoordFromQuantile'></span>

<h3>Description</h3>

<p>This function returns the latitude or longitude of quantile(s) of the geographic abundance distribution. The input is derived from a rasterized map of the species abundance distribution. If a quantile would occur somewhere between two cells, the latitude/longitude is linearly interpolated between the cells bracketing its value.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.interpCoordFromQuantile(
  latOrLong,
  quants,
  x,
  coordVect,
  weightedElev = NULL,
  warn = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id=".interpCoordFromQuantile_+3A_latorlong">latOrLong</code></td>
<td>
<p>Either 'latitude' or 'longitude'</p>
</td></tr>
<tr><td><code id=".interpCoordFromQuantile_+3A_quants">quants</code></td>
<td>
<p>Quantile value(s) (i.e., in the range [0, 1])</p>
</td></tr>
<tr><td><code id=".interpCoordFromQuantile_+3A_x">x</code></td>
<td>
<p>Matrix of abundances.</p>
</td></tr>
<tr><td><code id=".interpCoordFromQuantile_+3A_coordvect">coordVect</code></td>
<td>
<p>Vector of latitudes, one per row in <code>x</code> (from south to north!!!) **OR** vector or longitudes, one per column in <code>x</code> (from west to east!!!).</p>
</td></tr>
<tr><td><code id=".interpCoordFromQuantile_+3A_weightedelev">weightedElev</code></td>
<td>
<p>Raster of elevations weighted by x1 or x2 or <code>NULL</code>.</p>
</td></tr>
<tr><td><code id=".interpCoordFromQuantile_+3A_warn">warn</code></td>
<td>
<p>Logical. Show warnings.</p>
</td></tr>
</table>

<hr>
<h2 id='.scalePredictors'>Scales predictors</h2><span id='topic+.scalePredictors'></span>

<h3>Description</h3>

<p>Scales predictors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.scalePredictors(scale, preds, data)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id=".scalePredictors_+3A_scale">scale</code></td>
<td>
<p>Either <code>NA</code> (default), or <code>TRUE</code> or <code>FALSE</code>. If <code>TRUE</code>, the predictors will be centered and scaled by dividing by subtracting their means then dividing by their standard deviations. The means and standard deviations will be returned in the model object under an element named &quot;<code>scales</code>&quot;. For example, if you do something like <code>model &lt;- trainGLM(data, scale=TRUE)</code>, then you can get the means and standard deviations using <code>model$scales$means</code> and <code>model$scales$sds</code>. If <code>FALSE</code>, no scaling is done. If <code>NA</code> (default), then the function will check to see if non-factor predictors have means ~0 and standard deviations ~1. If not, then a warning will be printed, but the function will continue to do it's operations.</p>
</td></tr>
<tr><td><code id=".scalePredictors_+3A_preds">preds</code></td>
<td>
<p>A character vector with names of the predictors in <code>data</code>.</p>
</td></tr>
<tr><td><code id=".scalePredictors_+3A_data">data</code></td>
<td>
<p>A data frame.</p>
</td></tr>
</table>

<hr>
<h2 id='bioticVelocity'>Velocity of shifts in densities across a series of rasters</h2><span id='topic+bioticVelocity'></span>

<h3>Description</h3>

<p>Calculates metrics of &quot;movement&quot; of cell densities across a time series of rasters. Rasters could represent, for example, the probability of presence of a species through time. In this case, velocities would indicate rates and directions of range shift.  The simplest metric is movement of the density-weighted centroid (i.e., range &quot;center&quot;), but many more are available to provide a nuanced indicator of velocity. See <code>Details</code> for the types of metrics that can be calculated.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bioticVelocity(
  x,
  times = NULL,
  atTimes = NULL,
  elevation = NULL,
  metrics = c("centroid", "nsCentroid", "ewCentroid", "nCentroid", "sCentroid",
    "eCentroid", "wCentroid", "nsQuants", "ewQuants", "similarity", "summary"),
  quants = c(0.05, 0.1, 0.5, 0.9, 0.95),
  onlyInSharedCells = FALSE,
  cores = 1,
  warn = TRUE,
  longitude = NULL,
  latitude = NULL,
  paths = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bioticVelocity_+3A_x">x</code></td>
<td>
<p>Either a <code>SpatRaster</code> or a 3-dimensional array. Values <em>should really be</em> be either <code>NA</code> or &gt;= 0.
</p>

<ul>
<li><p> If <code>x</code> is a <code>SpatRaster</code>, then each layer is assumed to represent a time slice. Rasters <em>must</em> be in an equal-area projection. They must also be ordered temporally, with the raster &quot;on top&quot; assumed to represent the starting time.
</p>
</li>
<li><p> If <code>x</code> is an array then each &quot;layer&quot; in the third dimension is assumed to represent a map at a particular time slice in an equal-area projection. Note that if this is an array you should probably also specify the arguments <code>longitude</code> and <code>latitude</code>.
</p>
</li></ul>
</td></tr>
<tr><td><code id="bioticVelocity_+3A_times">times</code></td>
<td>
<p>Numeric vector with the same number of layers in <code>x</code> or <code>NULL</code> (default). This specifies the time represented by each layer in <code>x</code> from beginning of the time series (top layer) to the end (bottom layer). Times <em>must</em> appear in sequential order. For example, if time periods are 24 kybp, 23 kybp, 22 kybp, use <code>c(-24, -23, -22)</code>, not <code>c(24, 23, 22)</code>. If <code>NULL</code> (default), values are assigned starting at 1 and ending at the total number of layers in <code>x</code>.</p>
</td></tr>
<tr><td><code id="bioticVelocity_+3A_attimes">atTimes</code></td>
<td>
<p>Numeric, values of <code>times</code> across which to calculate biotic velocity. You can use this to calculate biotic velocities across selected time periods (e.g., just the first and last time periods). Note that <code>atTimes</code> must be the same as or a subset of <code>times</code>. The default is <code>NULL</code>, in which case velocity is calculated across all time slices (i.e., between <code>times</code> 1 and 2, 2 and 3, 3 and 4, etc.).</p>
</td></tr>
<tr><td><code id="bioticVelocity_+3A_elevation">elevation</code></td>
<td>
<p>Either <code>NULL</code> (default) or a raster or matrix representing elevation. If this is supplied, changes in elevation are incorporated into all velocity and speed metrics. Additionally, you can also calculate the metrics <code>elevCentrioid</code> and <code>elevQuants</code>.</p>
</td></tr>
<tr><td><code id="bioticVelocity_+3A_metrics">metrics</code></td>
<td>
<p>Biotic velocity metrics to calculate (default is to calculate them all). All metrics ignore <code>NA</code> cells in <code>x</code>. Here, &quot;starting time period&quot; represents one layer in <code>x</code> and &quot;end time period&quot; the next layer.
</p>

<ul>
<li> <p><code>centroid</code>: Speed of mass-weighted centroid (directionless).
</p>
</li>
<li> <p><code>nsCentroid</code> or <code>ewCentroid</code>: Velocity in the north-south or east-west directions of the mass-weighted centroid.
</p>
</li>
<li> <p><code>nCentroid</code>, <code>sCentroid</code>, <code>eCentroid</code>, and <code>wCentroid</code>: Speed of mass-weighted centroid of the portion of the raster north/south/east/west of the landscape-wide weighted centroid of the starting time period.
</p>
</li>
<li> <p><code>nsQuants</code> or <code>ewQuants</code>: Velocity of the location of the <em>Q</em>th quantile of mass in the north-south or east-west directions. The quantiles can be specified in <code>quants</code>. For example, this could be the movement of the 5th, 50th, and 95th quantiles of population size going from south to north. The 0th quantile would measure the velocity of the southernmost or easternmost cell(s) with values &gt;0, and the 100th quantile the northernmost or westernmost cell(s) with non-zero values.
</p>
</li>
<li> <p><code>similarity</code>: Metrics of similarity between each time period. Some of these make sense only for cases where values in <code>x</code> are in the range [0, 1], but not if some values are outside this range. See <code><a href="#topic+nicheOverlapMetrics">nicheOverlapMetrics</a></code> for more details. The metrics are:
</p>

<ul>
<li><p> Simple mean difference
</p>
</li>
<li><p> Mean absolute difference
</p>
</li>
<li><p> Root-mean squared difference
</p>
</li>
<li><p> Expected Fraction of Shared Presences or ESP (Godsoe, W. 2014. <em>Ecography</em> 37:130-136 <a href="https://doi.org/10.1111/j.1600-0587.2013.00403.x">doi:10.1111/j.1600-0587.2013.00403.x</a>)
</p>
</li>
<li><p> D statistic (Schoener, T.W. 1968. <em>Ecology</em> 49:704-726. <a href="https://doi.org/10.2307/1935534">doi:10.2307/1935534</a>)
</p>
</li>
<li><p> I statistic (Warren, D.L., et al. 2008. <em>Evolution</em> 62:2868-2883 <a href="https://doi.org/10.1111/j.1558-5646.2008.00482.x">doi:10.1111/j.1558-5646.2008.00482.x</a>)
</p>
</li>
<li><p> Pearson correlation
</p>
</li>
<li><p> Spearman rank correlation
</p>
</li></ul>

</li>
<li> <p><code>summary</code>: This calculates a series of measures for each &quot;starting time period&quot; raster. None of these are measures of velocity:
</p>

<ul>
<li><p> Mean: Mean value across all cells.
</p>
</li>
<li><p> Sum: Total across all cells.
</p>
</li>
<li><p> Quantiles: <em>Q</em>th quantile values across all cells. Quantiles are provided through argument <code>quants</code>.
</p>
</li>
<li><p> Prevalence: Number of cells with values &gt; 0.
</p>
</li></ul>

</li>
<li> <p><code>elevCentroid</code>: Velocity of the centroid of mass in elevation (up or down). A raster or matrix must be supplied to argument <code>elevation</code>.
</p>
</li>
<li> <p><code>elevQuants</code>: Velocity of the <em>Q</em>th quantile of mass in elevation (up or down). The quantiles to be evaluated are given by <code>quants</code>. The lowest elevation with mass &gt;0 is the 0th quantile, and the highest elevation with mass &gt;0 is the 100th. Argument <code>elevation</code> must be supplied.
</p>
</li></ul>
</td></tr>
<tr><td><code id="bioticVelocity_+3A_quants">quants</code></td>
<td>
<p>Numeric vector indicating the quantiles at which biotic velocity is calculated for the &quot;<code>quant</code>&quot; and &quot;<code>Quants</code>&quot; metrics. Default quantiles to calculate are <code>c(0.1, 0.9)</code>.</p>
</td></tr>
<tr><td><code id="bioticVelocity_+3A_onlyinsharedcells">onlyInSharedCells</code></td>
<td>
<p>If <code>TRUE</code>, calculate biotic velocity using only those cells that are not <code>NA</code> in the start <em>and</em> end of each time period. This is useful for controlling for shifting land mass due to sea level rise, for example, when calculating biotic velocity for an ecosystem or a species. The default is <code>FALSE</code>, in which case velocity is calculated using all cells in each time period, regardless of whether some become <code>NA</code> or change from <code>NA</code> to not <code>NA</code>.</p>
</td></tr>
<tr><td><code id="bioticVelocity_+3A_cores">cores</code></td>
<td>
<p>Positive integer. Number of processor cores to use. Note that if the number of time steps at which velocity is calculated is small, using more cores may not always be faster.  If you have issues when <code>cores</code> &gt; 1, please see the <code><a href="#topic+troubleshooting_parallel_operations">troubleshooting_parallel_operations</a></code> guide.</p>
</td></tr>
<tr><td><code id="bioticVelocity_+3A_warn">warn</code></td>
<td>
<p>Logical, if <code>TRUE</code> (default) then display function-specific warnings.</p>
</td></tr>
<tr><td><code id="bioticVelocity_+3A_longitude">longitude</code></td>
<td>
<p>Numeric matrix or <code>NULL</code> (default):
</p>

<ul>
<li><p> If <code>x</code> is a <code>SpatRaster</code>, then this is ignored (longitude is ascertained directly from the rasters, which <em>must</em> be in equal-area projection for velocities to be valid).
</p>
</li>
<li><p> If <code>x</code> is an array and <code>longitude</code> is <code>NULL</code> (default), then longitude will be ascertained from column numbers in <code>x</code> and velocities will be in arbitrary spatial units (versus, for example, meters). Alternatively, this can be a two-dimensional matrix whose elements represent the longitude coordinates of the centers of cells of <code>x</code>. The matrix must have the same number of rows and columns as <code>x</code>. Coordinates must be from an equal-area projection for results to be valid.
</p>
</li></ul>
</td></tr>
<tr><td><code id="bioticVelocity_+3A_latitude">latitude</code></td>
<td>
<p>Numeric matrix or <code>NULL</code> (default):
</p>

<ul>
<li><p> If <code>x</code> is a <code>SpatRaster</code>, then this is ignored (latitude is obtained directly from the rasters, which <em>must</em> be in equal-area projection for velocities to be valid).
</p>
</li>
<li><p> If <code>x</code> is an array and <code>latitude</code> is <code>NULL</code> (default), then latitude will be obtained from row numbers in <code>x</code> and velocities will be in arbitrary spatial units (versus, for example, meters). Alternatively, this can be a two-dimensional matrix whose elements represent the latitude coordinates of the centers of cells of <code>x</code>. The matrix must have the same number of rows and columns as <code>x</code>. Coordinates must be from an equal-area projection for results to be valid.
</p>
</li></ul>
</td></tr>
<tr><td><code id="bioticVelocity_+3A_paths">paths</code></td>
<td>
<p>This is used internally and rarely (never?) needs to be defined by a user (i.e., leave it as <code>NULL</code>). Valid values are a character vector or <code>NULL</code> (default). If a character vector, it should give the values used by <code><a href="base.html#topic+.libPaths">.libPaths</a></code>.</p>
</td></tr>
<tr><td><code id="bioticVelocity_+3A_...">...</code></td>
<td>
<p>Other arguments (not used).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><em>Attention:</em>  
</p>
<p>This function may yield erroneous velocities if the region of interest is near or spans a pole or the international date line. Results using the &quot;Quant&quot; and &quot;quant&quot; metrics may be somewhat counterintuitive if just one cell is &gt;0, or one row or column has the same values with all other values equal to 0 or <code>NA</code> because defining quantiles in these situations is not intuitive. Results may also be counterintuitive if some cells have negative values because they can &quot;push&quot; a centroid away from what would seem to be the center of mass as assessed by visual examination of a map.  
</p>
<p><em>Note:</em>  
</p>
<p>For the <code>nsQuants</code> and <code>ewQuants</code> metrics it is assumed that the latitude/longitude assigned to a cell is at its exact center. For calculating the position of a quantile, density is interpolated linearly from one cell center to the center of the adjacent cell. If a desired quantile does not fall exactly on the cell center, it is calculated from the interpolated values. For quantiles that fall south/westward of the first row/column of cells, the cell border is assumed to be at 0.5 * cell length south/west of the cell center.
</p>


<h3>Value</h3>

<p>A data frame with biotic velocities and related values. Fields are as follows:
</p>

<ul>
<li> <p><code>timeFrom</code>: Start time of interval
</p>
</li>
<li> <p><code>timeTo</code>: End time of interval
</p>
</li>
<li> <p><code>timeMid</code>: Time point between <code>timeFrom</code> and <code>timeTo</code>
</p>
</li>
<li> <p><code>timeSpan</code>: Duration of interval
</p>
</li></ul>

<p>Depending on <code>metrics</code> that are specified, additional fields are as follows. All measurements of velocity are in distance units (typically meters) per time unit (which is the same as the units used for <code>times</code> and <code>atTimes</code>). For example, if the rasters are in an Albers equal-area projection and <code>times</code> are in years, then the output will be meters per year.
</p>

<ul>
<li><p> If <code>metrics</code> has <code>'centroid'</code>: Columns named <code>centroidVelocity</code>, <code>centroidLong</code>, <code>centroidLat</code> &ndash; Speed of weighted centroid, plus its longitude and latitude (in the <code>timeTo</code> period of each time step). Values are always &gt;= 0.
</p>
</li>
<li><p> If <code>metrics</code> has <code>'nsCentroid'</code>: Columns named <code>nsCentroid</code> and <code>nsCentroidLat</code> &ndash; Velocity of weighted centroid in north-south direction, plus its latitude (in the <code>timeTo</code> period of each time step). Positive values connote movement north, and negative values south.
</p>
</li>
<li><p> If <code>metrics</code> has <code>'ewCentroid'</code>: <code>ewCentroid</code> and <code>ewCentroidLong</code> &ndash; Velocity of weighted centroid in east-west direction, plus its longitude (in the <code>timeTo</code> period of each time step).  Positive values connote movement east, and negative values west.
</p>
</li>
<li><p> If <code>metrics</code> has <code>'nCentroid'</code>, <code>'sCentroid'</code>, <code>'eCentroid'</code>, and/or <code>'wCentroid'</code>: Columns named <code>nCentroidVelocity</code> and <code>nCentroidAbund</code>, <code>sCentroid</code> and <code>sCentroidAbund</code>, <code>eCentroid</code> and <code>eCentroidAbund</code>, and/or <code>wCentroid</code> and <code>wCentroidAbund</code> &ndash; Speed of weighted centroid of all cells that fall north, south, east, or west of the landscape-wide centroid, plus a column indicating the total weight (abundance) of all such populations. Values are always &gt;= 0.
</p>
</li>
<li><p> If <code>metrics</code> contains any of <code>nsQuants</code> or <code>ewQuants</code>: Columns named <code>nsQuantVelocity_quant</code><em>Q</em> and <code>nsQuantLat_quant</code><em>Q</em>, or <code>ewQuantVelocity_quant</code><em>Q</em> and <code>ewQuantLat_quant</code><em>Q</em>: Velocity of the <em>Q</em>th quantile weight in the north-south or east-west directions, plus the latitude or longitude thereof (in the <code>timeTo</code> period of each time step). Quantiles are cumulated starting from the south or the west, so the 0.05th quantile, for example, is in the far south or west of the range and the 0.95th in the far north or east. Positive values connote movement north or east, and negative values movement south or west.
</p>
</li>
<li><p> If <code>metrics</code> contains <code>similarity</code>, metrics of similarity are calculated for each pair of successive landscapes, defined below as <code>x1</code> (raster in <code>timeFrom</code>) and <code>x2</code> (raster in <code>timeTo</code>), with the number of shared non-<code>NA</code> cells between them being <code>n</code>:
</p>

<ul>
<li><p> A column named <code>simpleMeanDiff</code>: <code>sum(x2 - x1, na.rm = TRUE) / n</code>
</p>
</li>
<li><p> A column named <code>meanAbsDiff</code>: <code>sum(abs(x2 - x1), na.rm = TRUE) / n</code>
</p>
</li>
<li><p> A column named <code>rmsd</code> (root-mean square difference): <code>sqrt(sum((x2 - x1)^2, na.rm = TRUE)) / n</code>
</p>
</li>
<li><p> A column named <code>godsoeEsp</code>: <code>1 - sum(2 * (x1 * x2), na.rm=TRUE) / sum(x1 + x2, na.rm = TRUE)</code>, values of 1 ==&gt; maximally similar, 0 ==&gt; maximally dissimilar.
</p>
</li>
<li><p> A column named <code>schoenersD</code>: <code>1 - (sum(abs(x1 - x2), na.rm = TRUE) / n)</code>, values of 1 ==&gt; maximally similar, 0 ==&gt; maximally dissimilar.
</p>
</li>
<li><p> A column named <code>warrensI</code>: <code>1 - sqrt(sum((sqrt(x1) - sqrt(x2))^2, na.rm = TRUE) / n)</code>, values of 1 ==&gt; maximally similar, 0 ==&gt; maximally dissimilar.
</p>
</li>
<li><p> A column named <code>cor</code>: Pearson correlation between <code>x1</code> and <code>x2</code>.
</p>
</li>
<li><p> A column named <code>rankCor</code>: Spearman rank correlation between <code>x1</code> and <code>x2</code>.
</p>
</li></ul>

</li>
<li><p> If <code>metrics</code> contains <code>elevCentroid</code>: Columns named <code>elevCentroidVelocity</code> and <code>elevCentroidElev</code> &ndash; Velocity of the centroid in elevation (up or down) and the elevation in the &quot;to&quot; timestep. Positive values of velocity connote movement upward, and negative values downward.
</p>
</li>
<li><p> If <code>metrics</code> contains <code>elevQuants</code>: Columns named <code>elevQuantVelocity_quant</code><em>Q</em> and <code>elevQuantVelocityElev_quant</code><em>Q</em> &ndash; Velocity of the <em>N</em>th quantile of mass in elevation (up or down) and the elevation of this quantile in the &quot;to&quot; timestep. Positive values of velocity connote movement upward, and negative values downward.
</p>
</li>
<li><p> If <code>metrics</code> contains <code>summary</code>:
</p>

<ul>
<li><p> A column named <code>propSharedCellsNotNA</code>: Proportion of cells that are not <code>NA</code> in <em>both</em> the &quot;from&quot; and &quot;to&quot; time steps. The proportion is calculated using the total number of cells in a raster as the denominator (i.e., not total number of cells across two rasters).
</p>
</li>
<li><p> Columns named <code>timeFromPropNotNA</code> and <code>timeToPropNotNA</code>: Proportion of cells in the &quot;from&quot; time and &quot;to&quot; steps that are not <code>NA</code>.
</p>
</li>
<li><p> A column named <code>mean</code>: Mean weight in <code>timeTo</code> time step. In the same units as the values of the cells.
</p>
</li>
<li><p> Columns named <code>quantile_quant</code><em>Q</em>: The <em>Q</em>th quantile(s) of weight in the <code>timeTo</code> time step. In the same units as the values of the cells.
</p>
</li>
<li><p> A column named <code>prevalence</code>: Proportion of non-<code>NA</code> cells with weight &gt;0 in the <code>timeTo</code> time step relative to all non-<code>NA</code> cells. Unitless.
</p>
</li></ul>

</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
# NB These examples can take a few minutes to run.
# To illustrate calculation and interpretation of biotic velocity,
# we will calibrate a SDM for the Red-Bellied Lemur and project
# the model to the present and successive future climates. The time series
# of rasters is then used to calculate biotic velocity.

library(sf)
library(terra)

### process environmental rasters
#################################

# get rasters
rastFile &lt;- system.file('extdata/madClim.tif', package='enmSdmX')
madClim &lt;- rast(rastFile)

rastFile &lt;- system.file('extdata/madClim2030.tif', package='enmSdmX')
madClim2030 &lt;- rast(rastFile)

rastFile &lt;- system.file('extdata/madClim2050.tif', package='enmSdmX')
madClim2050 &lt;- rast(rastFile)

rastFile &lt;- system.file('extdata/madClim2070.tif', package='enmSdmX')
madClim2070 &lt;- rast(rastFile)

rastFile &lt;- system.file('extdata/madClim2090.tif', package='enmSdmX')
madClim2090 &lt;- rast(rastFile)

# The bioticVelocity() function needs rasters to be in equal-area
# projection, so we will project them here.
madAlbers &lt;- getCRS('madAlbers') # Albers projection for Madagascar
madClim &lt;- project(madClim, madAlbers)
madClim2030 &lt;- project(madClim2030, madAlbers)
madClim2050 &lt;- project(madClim2050, madAlbers)
madClim2070 &lt;- project(madClim2070, madAlbers)
madClim2090 &lt;- project(madClim2090, madAlbers)

# Coordinate reference systems:
wgs84 &lt;- getCRS('WGS84') # WGS84
madAlbers &lt;- getCRS('madAlbers') # Madagascar Albers

# lemur occurrence data
data(lemurs)
occs &lt;- lemurs[lemurs$species == 'Eulemur fulvus', ]
occs &lt;- vect(occs, geom=c('longitude', 'latitude'), crs=wgs84)
occs &lt;- project(occs, madAlbers)

# eliminate cell duplicates
occs &lt;- elimCellDuplicates(occs, madClim)

# extract environment at occurrences
occEnv &lt;- extract(madClim, occs, ID = FALSE)
occEnv &lt;- occEnv[complete.cases(occEnv), ]
	
# create background sites (using just 1000 to speed things up!)
bgEnv &lt;- terra::spatSample(madClim, 3000)
bgEnv &lt;- bgEnv[complete.cases(bgEnv), ]
bgEnv &lt;- bgEnv[sample(nrow(bgEnv), 1000), ]

# collate occurrences and background sites
presBg &lt;- data.frame(
   presBg = c(
      rep(1, nrow(occEnv)),
      rep(0, nrow(bgEnv))
   )
)

env &lt;- rbind(occEnv, bgEnv)
env &lt;- cbind(presBg, env)

### calibrate model
###################

predictors &lt;- c('bio1', 'bio12')

# MaxEnt
mx &lt;- trainMaxEnt(
	data = env,
	resp = 'presBg',
	preds = predictors,
	regMult = 1, # too few values for reliable model, but fast
	cores = 2
)

### project to present and future climate
#########################################

predPresent &lt;- predictEnmSdm(mx, madClim)
pred2030 &lt;- predictEnmSdm(mx, madClim2030)
pred2050 &lt;- predictEnmSdm(mx, madClim2050)
pred2070 &lt;- predictEnmSdm(mx, madClim2070)
pred2090 &lt;- predictEnmSdm(mx, madClim2090)

plot(predPresent, main = 'Present Suitability')

# plot change in suitability between present and 2090s
delta &lt;- pred2090 - predPresent
plot(delta, main = 'Change in Suitability')

### calculate biotic velocity
#############################

series &lt;- c(
	predPresent,
	pred2030,
	pred2050,
	pred2070,
	pred2090
)

names(series) &lt;- c('present', 't2030', 't2050', 't2070', 't2090')
plot(series)

times &lt;- c(1985, 2030, 2050, 2070, 2090)
quants &lt;- c(0.10, 0.90)

bv &lt;- bioticVelocity(
	x = series,
	times = times,
	quants = quants,
	cores = 2
)
 
bv

### centroid velocities

# centroid (will always be &gt;= 0)
# fastest centroid movement around 2060
plot(bv$timeMid, bv$centroidVelocity, type = 'l',
  xlab = 'Year', ylab = 'Speed (m / y)', main = 'Centroid Speed')
  
# velocity northward/southward through time
# shows northward shift because always positive, fastest around 2060
plot(bv$timeMid, bv$nsCentroidVelocity, type = 'l',
  xlab = 'Year', ylab = 'Velocity (m / y)', main = 'Centroid N/S Velocity')
  
# velocity eastward (positive)/westward (negative) through time
# movement eastward (positive) first, then westward (negative)
plot(bv$timeMid, bv$ewCentroidVelocity, type = 'l',
  xlab = 'Year', ylab = 'Velocity (m / y)', main = 'Centroid E/W Velocity')

### map of centroid location through time
# shows centroid moves slightly northward through time
plot(delta, main = 'Centroid Location &amp;\nChange in Suitability')
points(bv$centroidLong[1], bv$centroidLat[1], pch = 1)
points(bv$centroidLong[4], bv$centroidLat[4], pch = 16)
lines(bv$centroidLong, bv$centroidLat)
legend(
  'bottomright',
  legend = c(
    'start (~1985)',
	'stop (~2090)',
	'trajectory'
  ),
  pch = c(1, 16, NA),
  lwd = c(NA, NA, 1)
)

### velocities of portions of range north/south/east/west of centroid
# positive ==&gt; northward shift
# negative ==&gt; southward shift
  
# portion of range north of centroid
# shows northward expansion because always positive
plot(bv$timeMid, bv$nCentroidVelocity, type = 'l',
  xlab = 'Year', ylab = 'Velocity (m / y)',
  main = 'Northern Part of Range')
  
# portion of range south of centroid
# shows northward contraction because always positive
plot(bv$timeMid, bv$sCentroidVelocity, type = 'l',
  xlab = 'Year', ylab = 'Velocity (m / y)',
  main = 'Southern Part of Range')
  
# portion of range east of centroid
# shows eastern portion moves farther east
plot(bv$timeMid, bv$eCentroidVelocity, type = 'l',
  xlab = 'Year', ylab = 'Velocity (m / y)',
  main = 'Eastern Part of Range')
  
# portion of range west of centroid
# shows western portion moves east
plot(bv$timeMid, bv$wCentroidVelocity, type = 'l',
  xlab = 'Year', ylab = 'Velocity (m / y)',
  main = 'Western Part of Range')

### velocities of range margins

# from south to north, 10th and 90th quantiles of density
# positive ==&gt; northward shift
# negative ==&gt; southward shift
# shows both northern and southern range margins shift northward
# because always positive... northern margin shift usually slower
ylim &lt;- range(bv$nsQuantVelocity_quant0p1, bv$nsQuantVelocity_quant0p9)

plot(bv$timeMid, bv$nsQuantVelocity_quant0p1, type = 'l', ylim = ylim,
  xlab = 'Year', ylab = 'Velocity (m / y)',
  main = 'Northern/Southern Range Margins')
lines(bv$timeMid, bv$nsQuantVelocity_quant0p9, lty = 'dashed')
legend(
  'bottomright',
  legend = c('Southern Margin', 'Northern Margin'),
  lty = c('solid', 'dashed')
)
  
# from east to west, 10th and 90th quantiles of density
# positive ==&gt; eastward shift
# negative ==&gt; westward shift
ylim &lt;- range(bv$ewQuantVelocity_quant0p1, bv$ewQuantVelocity_quant0p9)

plot(bv$timeMid, bv$ewQuantVelocity_quant0p1, type = 'l', ylim = ylim,
  xlab = 'Year', ylab = 'Velocity (m / y)',
  main = 'Eastern/Western Range Margins')
lines(bv$timeMid, bv$ewQuantVelocity_quant0p9, lty = 'dashed')
legend(
  'bottomright',
  legend = c('Eastern Margin', 'Western Margin'),
  lty = c('solid', 'dashed')
)
  
  
### summary statistics

# mean density across cells through time
plot(bv$timeMid, bv$mean, type = 'l',
  xlab = 'Year', ylab = 'Mean Density',
  main = 'Mean Density')

# sum of density across cells through time
plot(bv$timeMid, bv$sum, type = 'l',
  xlab = 'Year', ylab = 'Sum of Density',
  main = 'Sum of Density')

### change metrics

# average change in suitability from one time period to next
# shows average conditions getting worse
plot(bv$timeMid, bv$simpleMeanDiff, type = 'l',
  xlab = 'Year', ylab = 'Mean Change in Suitability')
  
# average absolute change in suitability from one time period to next
# shows average absolute change declining
plot(bv$timeMid, bv$meanAbsDiff, type = 'l',
  xlab = 'Year', ylab = 'Mean Absolute Change in Suitability')
  
# root-mean square difference from one time period to the next
# shows difference between successive rasters declines through time
plot(bv$timeMid, bv$rmsd, type = 'l',
  xlab = 'Year', ylab = 'RMSD')
  
### raster similarity
# most indicate that successive rasters are similar through time
ylim &lt;- range(bv$godsoeEsp, bv$schoenerD, bv$warrenI, bv$cor, bv$warrenI)
plot(bv$timeMid, bv$godsoeEsp, type = 'l', lty = 1, col = 1,
  xlab = 'Year', ylab = 'Raster similarity', ylim = ylim)
lines(bv$timeMid, bv$schoenerD, lty = 2, col = 2)
lines(bv$timeMid, bv$warrenI, lty = 3, col = 3)
lines(bv$timeMid, bv$cor, lty = 4, col = 4)
lines(bv$timeMid, bv$rankCor, lty = 5, col = 5)

legend(
  'right',
  legend = c(
    'Godsoe\'s ESP',
    'Schoener\'s D',
    'Warren\'s I',
    'Correlation',
    'Rank Correlation'
  ),
  col = 1:5,
  lty = 1:5
)
  

# values of 10th and 90th quantiles across cells through time
# shows most favorable cells becoming less favorable
# least favorable cells remain mainly unchanged
ylim &lt;- range(bv$quantile_quant0p1, bv$quantile_quant0p9)

plot(bv$timeMid, bv$quantile_quant0p1, type = 'l', ylim = ylim,
  xlab = 'Year', ylab = 'Quantile Value',
  main = 'Quantiles across Cells')
lines(bv$timeMid, bv$quantile_quant0p9, lty = 'dashed')

legend(
  'topright',
  legend = c('10th quantile', '90th quantile'),
  lty = c('solid', 'dashed')
)

### map of northern/southern range margins through time

# range of longitude shown in plot
madExtent &lt;- ext(madClim)
xExtent &lt;- as.vector(madExtent)[1:2]

plot(predPresent, main = 'North/South Range Margin Location')
lines(c(xExtent[1], xExtent[2]),
  c(bv$nsQuantLat_quant0p9[1], bv$nsQuantLat_quant0p9[1]))
lines(c(xExtent[1], xExtent[2]),
  c(bv$nsQuantLat_quant0p9[2], bv$nsQuantLat_quant0p9[2]), lty = 'dashed')
lines(c(xExtent[1], xExtent[2]),
  c(bv$nsQuantLat_quant0p9[3], bv$nsQuantLat_quant0p9[3]), lty = 'dotdash')
lines(c(xExtent[1], xExtent[2]),
  c(bv$nsQuantLat_quant0p9[4], bv$nsQuantLat_quant0p9[4]), lty = 'dotted')

lines(c(xExtent[1], xExtent[2]),
  c(bv$nsQuantLat_quant0p1[1], bv$nsQuantLat_quant0p1[1]))
lines(c(xExtent[1], xExtent[2]),
  c(bv$nsQuantLat_quant0p1[2], bv$nsQuantLat_quant0p1[2]), lty = 'dashed')
lines(c(xExtent[1], xExtent[2]),
  c(bv$nsQuantLat_quant0p1[3], bv$nsQuantLat_quant0p1[3]), lty = 'dotdash')
lines(c(xExtent[1], xExtent[2]),
  c(bv$nsQuantLat_quant0p1[4], bv$nsQuantLat_quant0p1[4]), lty = 'dotted')

legend(
  'bottomright',
  legend = c(
    '1980s',
	'2030s',
	'2050s',
	'2070s',
	'2090s'
  ),
  lty = c('solid', 'dashed', 'dotdash', 'dotted')
)

### map of eastern/western range margins through time

# range of longitude shown in plot
madExtent &lt;- ext(madClim)
yExtent &lt;- as.vector(madExtent)[3:4]

plot(predPresent, main = 'North/South Range Margin Location')
lines(c(bv$ewQuantLong_quant0p9[1], bv$ewQuantLong_quant0p9[1]),
  c(yExtent[1], yExtent[2]))
lines(c(bv$ewQuantLong_quant0p9[2], bv$ewQuantLong_quant0p9[2]),
  c(yExtent[1], yExtent[2]), lty = 'dashed')
lines(c(bv$ewQuantLong_quant0p9[3], bv$ewQuantLong_quant0p9[3]),
  c(yExtent[1], yExtent[2]), lty = 'dotdash')
lines(c(bv$ewQuantLong_quant0p9[4], bv$ewQuantLong_quant0p9[4]),
  c(yExtent[1], yExtent[2]), lty = 'dotted')

lines(c(bv$ewQuantLong_quant0p1[1], bv$ewQuantLong_quant0p1[1]),
  c(yExtent[1], yExtent[2]))
lines(c(bv$ewQuantLong_quant0p1[2], bv$ewQuantLong_quant0p1[2]),
  c(yExtent[1], yExtent[2]), lty = 'dashed')
lines(c(bv$ewQuantLong_quant0p1[3], bv$ewQuantLong_quant0p1[3]),
  c(yExtent[1], yExtent[2]), lty = 'dotdash')
lines(c(bv$ewQuantLong_quant0p1[4], bv$ewQuantLong_quant0p1[4]),
  c(yExtent[1], yExtent[2]), lty = 'dotted')

legend(
  'bottomright',
  legend = c(
    '1980s',
	'2030s',
	'2050s',
	'2070s',
	'2090s'
  ),
  lty = c('solid', 'dashed', 'dotdash', 'dotted')
)



</code></pre>

<hr>
<h2 id='canada'>Vector outline of Canada</h2><span id='topic+canada'></span>

<h3>Description</h3>

<p>This <code>SpatVector</code> represents the outline of Canada in WGS84 (unprojected) coordinates. This is the &quot;low resolution&quot; (less accurate) version from GADM.
</p>


<h3>Format</h3>

<p>An object of class <code>'SpatVector'</code>.
</p>


<h3>Source</h3>

<p><a href="https://gadm.org/index.html">Database of Global Administrative Areas (GADM)</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(terra)
canFile &lt;- system.file('extdata', 'canada_level0_gadm41.gpkg', package='enmSdmX')
canada &lt;- vect(canFile)
plot(canada)

</code></pre>

<hr>
<h2 id='compareResponse'>Compare two response curves along one or more predictors</h2><span id='topic+compareResponse'></span>

<h3>Description</h3>

<p>This function calculates a suite of metrics reflecting of niche overlap for two response curves. Response curves are predicted responses of a uni- or multivariate model along a single variable. Depending on the user-specified settings the function calculates these values either at each pair of values of <code>pred1</code> and <code>pred2</code> <em>or</em> along a smoothed version of <code>pred1</code> and <code>pred2</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compareResponse(
  pred1,
  pred2,
  data,
  predictor = names(data),
  adjust = FALSE,
  gap = Inf,
  smooth = FALSE,
  smoothN = 1000,
  smoothRange = c(0, 1),
  graph = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="compareResponse_+3A_pred1">pred1</code></td>
<td>
<p>Numeric vector. Predictions from first model along <code>data</code> (one value per row in <code>data</code>).</p>
</td></tr>
<tr><td><code id="compareResponse_+3A_pred2">pred2</code></td>
<td>
<p>Numeric vector. Predictions from second model along <code>data</code> (one value per row in <code>data</code>).</p>
</td></tr>
<tr><td><code id="compareResponse_+3A_data">data</code></td>
<td>
<p>Data frame or matrix corresponding to <code>pred1</code> and <code>pred2</code>.</p>
</td></tr>
<tr><td><code id="compareResponse_+3A_predictor">predictor</code></td>
<td>
<p>Character vector. Name(s) of predictor(s) for which to calculate comparisons. These must appear as column names in <code>data</code>.</p>
</td></tr>
<tr><td><code id="compareResponse_+3A_adjust">adjust</code></td>
<td>
<p>Logical. If <code>TRUE</code> then subtract the mean of <code>pred1</code> from <code>pred1</code> and the mean of <code>pred2</code> from <code>pred2</code> before analysis. Useful for comparing the shapes of curves while controlling for different elevations (intercepts).</p>
</td></tr>
<tr><td><code id="compareResponse_+3A_gap">gap</code></td>
<td>
<p>Numeric &gt;0. Proportion of range of predictor variable across which to assume a gap exists. Calculation of <code>areaAbsDiff</code> will  ignore gaps wide than this. To ensure the entire range of the data is included set this equal to <code>Inf</code> (default).</p>
</td></tr>
<tr><td><code id="compareResponse_+3A_smooth">smooth</code></td>
<td>
<p>Logical. If <code>TRUE</code> then the responses are first smoothed using loess() then compared at <code>smoothN</code> values along each predictor. If <code>FALSE</code>, then comparisons are conducted at the raw values <code>pred1</code> and <code>pred2</code>.</p>
</td></tr>
<tr><td><code id="compareResponse_+3A_smoothn">smoothN</code></td>
<td>
<p><code>NULL</code> or positive integer. Number of values along &quot;pred&quot; at which to calculate comparisons. Only used if <code>smooth</code> is <code>TRUE</code>. If <code>NULL</code>, then comparisons are calculated at each value in data. If a number, then comparisons are calculated at <code>smoothN</code> values of <code>data[ , pred]</code> that cover the range of <code>data[ , pred]</code>.</p>
</td></tr>
<tr><td><code id="compareResponse_+3A_smoothrange">smoothRange</code></td>
<td>
<p>2-element numeric vector or <code>NULL</code>. If <code>smooth</code> is <code>TRUE</code>, then force loess predictions &lt; <code>smoothRange[1]</code> to equal <code>smoothRange[1]</code> and predictions &gt; <code>smoothRange[2]</code> to equal <code>smoothRange[2]</code>. Ignored if <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="compareResponse_+3A_graph">graph</code></td>
<td>
<p>Logical. If <code>TRUE</code> then plot predictions.</p>
</td></tr>
<tr><td><code id="compareResponse_+3A_...">...</code></td>
<td>
<p>Arguments to pass to functions like <code>sum()</code> (for example, <code>na.rm=TRUE</code>) and to <code>overlap()</code> (for example, <code>w</code> for weights). Note that if <code>smooth = TRUE</code>, then passing an argument called <code>w</code> will likely cause a warning and make results circumspect <em>unless</em> weights are pre-calculated for each of the <code>smoothN</code> points along a particular predictor.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Either a data frame (if <code>smooth = FALSE</code> or a list object with the smooth model plus a data frame (if <code>smooth = TRUE</code>) . The data frame represents metrics comparing response curves of <code>pred1</code> and <code>pred2</code>:
</p>

<ul>
<li> <p><code>predictor</code> Predictor for which comparison was made
</p>
</li>
<li> <p><code>n</code> Number of values of predictor at which comparison was calculated
</p>
</li>
<li> <p><code>adjust</code> <code>adjust</code> argument.
</p>
</li>
<li> <p><code>smooth</code> <code>smooth</code> argument.
</p>
</li>
<li> <p><code>meanDiff</code> Mean difference between predictions of <code>pred1</code> and <code>pred2</code> (higher ==&gt; more different).
</p>
</li>
<li> <p><code>meanAbsDiff</code> Mean absolute value of difference between predictions of <code>pred1</code> and <code>pred2</code> (higher ==&gt; more different).
</p>
</li>
<li> <p><code>areaAbsDiff</code> Sum of the area between curves predicted by <code>pred1</code> and <code>pred2</code>, standardized by total potential area between the two curves (i.e., the area available between the minimum and maximum prediction along the minimum and maximum values of the predictor) (higher ==&gt; more different).
</p>
</li>
<li> <p><code>d</code> Schoener's <em>D</em>
</p>
</li>
<li> <p><code>i</code> Hellinger's <em>I</em> (adjusted to have a range [0, 1])
</p>
</li>
<li> <p><code>esp</code> Godsoe's ESP
</p>
</li>
<li> <p><code>cor</code> Pearson correlation between predictions of <code>pred1</code> and <code>pred2</code>.
</p>
</li>
<li> <p><code>rankCor</code> Spearman rank correlation between predictions of <code>pred1</code> and <code>pred2</code>.
</p>
</li></ul>



<h3>References</h3>

<p>Warren, D.L., Glor, R.E., and Turelli, M.  2008.  Environmental niche equivalency versus conservatism: Quantitative approaches to niche evolution.  Evolution 62:2868-2883.
</p>
<p>Warren, D.L., Glor, R.E., and Turelli, M.  2008.  Erratum.  Evolution 62:2868-2883.
</p>
<p>Godsoe, W.  2014.  Inferring the similarity of species distributions using Species Distribution Models.  Ecography 37:130-136.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nicheOverlapMetrics">nicheOverlapMetrics</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(123)
data &lt;- data.frame(
	x1=seq(-1, 1, length.out=100),
	x2=seq(-1, 1, length.out=100) + rnorm(100, 0, 0.3)
)

pred1 &lt;- 1 / (1 + exp(-(0.3 + 2 * (data$x1 - 0.2) -0.3 * data$x2)))
pred2 &lt;- 1 / (1 + exp(-(-0 + 0.1 * data$x1 - 4 * data$x1^2 + 0.4 * data$x2)))

compareResponse(pred1, pred2, data, graph=TRUE)
compareResponse(pred1, pred2, data, smooth=TRUE, graph=TRUE)
compareResponse(pred1, pred2, data, adjust=TRUE, graph=TRUE)

</code></pre>

<hr>
<h2 id='coordImprecision'>Calculate the precision of a geographic coordinate</h2><span id='topic+coordImprecision'></span>

<h3>Description</h3>

<p>This function calculates the imprecision of geographic coordinates due to rounded coordinate values. See <em>Details</em> for an explanation of how this is calculated.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>coordImprecision(x, dms = FALSE, epsilon = 2)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="coordImprecision_+3A_x">x</code></td>
<td>
<p>Spatial points represented as a <code>SpatVector</code> or <code>sf</code> object. Alternatively, this can also be a data frame or matrix, in which the first two columns must represent longitude and latitude (in that order). If <code>x</code> is a matrix or data frame, the coordinates are assumed to be unprojected (WGS84).</p>
</td></tr>
<tr><td><code id="coordImprecision_+3A_dms">dms</code></td>
<td>
<p>Logical, if <code>FALSE</code> (default), it is assumed that the original format in which coordinate were reported is in decimal notation. If <code>TRUE</code>, then it will be calculated assuming the coordinate were originally in degrees-minutes-seconds format.  If you do not know the original format, the less presumptive approach is to calculate coordinate imprecision twice with or without <code>dm = TRUE</code>, and use the larger of the two values.</p>
</td></tr>
<tr><td><code id="coordImprecision_+3A_epsilon">epsilon</code></td>
<td>
<p>Zero or positive integer, number of digits to which to round seconds value if <code>dms</code> is <code>TRUE</code>. Ignored if <code>dms</code> is <code>FALSE</code>. This is used to accommodate inexact integer values when converting from DMS to decimal. For example, -108.932222 converted to DMS format is 108deg 55min 7.9992sec, but if <code>epsilon</code> = 2 then it would be converted to 108deg 55min 08sec.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For coordinates originally reported in decimal notation, coordinate imprecision is <em>half</em> the distance between the two opposing corners on a bounding box whose size is based on the number of significant digits in the coordinates. This box is defined by 1) finding the maximum number of significant digits after the decimal in the longitude/latitude pair; 2) adding/subtracting 5 to the decimal place that falls just after this; and 3) calculating the distance between these points then dividing by 2. For example, if longitude is 82.37 and latitude 45.8 then the number of significant digits after the decimal place is 2 and 1, respectively so 2 is used on the assumption that latitude is measured to the nearest 100th degree. The precision is then the distance between the point pairs (82.37 - 0.05 = 82.365, 45.8 - 0.05 = 45.795) and (82.37 + 0.05 = 82.375, 45.8 + 0.05 = 45.805). <br /> <br />
</p>
<p>For coordinates originally reported in degree-minus-second (DMS) format, the bounding box is defined by adding/subtracting 0.5 units (degrees, minutes, or seconds, depending on the smallest non-zero unit reported) from the coordinate. For example, if longitude is 90deg 00min 00sec and latitude is 37deg 37min 37sec, then the bounding box will be defined by adding/subtracting 0.5 arcsec to the coordinates.
</p>


<h3>Value</h3>

<p>Numeric values (by default in units of meters).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# coarse-precision cases
long &lt;-	c(45, 45.1, 45.1)
lat &lt;-  c(45, 45.1, 45)
ll &lt;- cbind(long, lat)
precision_m &lt;- coordImprecision(ll)
cbind(ll, precision_m)

# fine-precision cases
long &lt;-	rep(45, 8)
lat &lt;-  c(45, 45.1, 45.11, 45.111, 45.1111, 45.11111, 45.111111, 45.1111111)
ll &lt;- cbind(long, lat)
precision_m &lt;- coordImprecision(ll)
cbind(ll, precision_m)

# precision varies with latitude
long &lt;- rep(45, 181)
lat &lt;- seq(-90, 90)
ll &lt;- cbind(long, lat)
precision_m &lt;- coordImprecision(ll)
cbind(ll, precision_m)
plot(lat, precision_m / 1000, xlab='Latitude', ylab='Precision (km)')

# dateline/polar cases
long &lt;-	c(0, 180, 45, 45)
lat &lt;-  c(45, 45, 90, -90)
ll &lt;- cbind(long, lat)
precision_m &lt;- coordImprecision(ll)
cbind(ll, precision_m)

# original coordinates in degrees-minutes-seconds format
longDD &lt;- c(90, 90, 90, 90, 90, 90)
longMM &lt;- c(0, 0, 0, 11, 11, 0)
longSS &lt;- c(0, 0, 0, 0, 52, 52)
latDD &lt;- c(38, 38, 38, 38, 38, 38)
latMM &lt;- c(0, 37, 37, 37, 37, 0)
latSS &lt;- c(0, 0, 38, 38, 38, 0)
longHemis &lt;- rep('W', 6)
latHemis &lt;- rep('N', 6)
longDec &lt;- dmsToDecimal(longDD, longMM, longSS, longHemis)
latDec &lt;- dmsToDecimal(latDD, latMM, latSS, latHemis)
decimal &lt;- cbind(longDec, latDec)
(decImp &lt;- coordImprecision(decimal))
(dmsImp &lt;- coordImprecision(decimal, dms=TRUE))

# What if we do not know if coordinates were originally reported in
# decimal or degrees-minutes-seconds format? Most conservative option
# is to use maximum:
pmax(decImp, dmsImp)

if (FALSE) {
  # known error when longitude is negative and latitude is -90
  long &lt;- -45
  lat &lt;- -90
  ll &lt;- cbind(long, lat)
  coordImprecision(ll)
}

</code></pre>

<hr>
<h2 id='countPoints'>Number of points in a &quot;spatial points&quot; object</h2><span id='topic+countPoints'></span>

<h3>Description</h3>

<p>Returns the number of points in a <code>sf</code> or <code>SpatVector</code> object. This is typically done using either <code>length(x)</code> or <code>nrow(x)</code>, depending on whether the object in question has rows or not. This function helps in ambiguous cases, so users need not care if <code>nrow</code> or <code>length</code> is needed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>countPoints(x, byFeature = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="countPoints_+3A_x">x</code></td>
<td>
<p>Object of class <code>sf</code> or <code>SpatVector</code></p>
</td></tr>
<tr><td><code id="countPoints_+3A_byfeature">byFeature</code></td>
<td>
<p>If <code>FALSE</code>, return number of points for all features combined. If <code>TRUE</code>, report number of points per feature.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(sf)

# lemur occurrence data
data(lemurs)
wgs84 &lt;- getCRS('WGS84')
occs &lt;- lemurs[lemurs$species == 'Eulemur fulvus', ]
occs &lt;- sf::st_as_sf(occs, coords=c('longitude', 'latitude'), crs=wgs84)

countPoints(occs)

</code></pre>

<hr>
<h2 id='crss'>Coordinate reference systems (CRSs) and nicknames</h2><span id='topic+crss'></span>

<h3>Description</h3>

<p>A table of commonly-used coordinate reference systems, their nicknames, and WKT2 (well-known text) strings
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(crss)
</code></pre>


<h3>Format</h3>

<p>An object of class <code>data.frame</code>. This is a table with &quot;named&quot; coordinate referenbce systems and their well-known-text (WKT2) representation. It can be used as-is, or with <code><a href="#topic+getCRS">getCRS</a></code> to quickly get a WKT for a particular CRS. The fields are as:
</p>

<ul>
<li> <p><code>long</code>: &quot;Long&quot; name of the CRS
</p>
</li>
<li> <p><code>short1</code> and <code>short2</code>: &quot;Short&quot; names of the CRS
</p>
</li>
<li> <p><code>region</code>: Region for which CRS is fit
</p>
</li>
<li> <p><code>projected</code>: Is the CRS projected or not?
</p>
</li>
<li> <p><code>projectionGeometry</code>: Type of projection (<code>NA</code>, 'cylindrical', 'conic', or 'planar')
</p>
</li>
<li> <p><code>datum</code>: Datum
</p>
</li>
<li> <p><code>type</code>: Either 'CRS' or 'data'. The former are proper CRSs, and the latter are those used by popular datasets.
</p>
</li>
<li> <p><code>wkt2</code>: WKT2 string.
</p>
</li>
<li> <p><code>notes</code>: Notes.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
data(crss)
getCRS('North America Albers', nice = TRUE)

</code></pre>

<hr>
<h2 id='customAlbers'>Custom coordinate reference system WKT2 string</h2><span id='topic+customAlbers'></span><span id='topic+customLambert'></span><span id='topic+customVNS'></span>

<h3>Description</h3>

<p>These functions take as input either a spatial object or coordinate pair and a custom WKT2 (well-known text) coordinate reference system string centered on the object or coordinate. Projections include:
</p>

<ul>
<li><p> Albers conic equal-area
</p>
</li>
<li><p> Lambert azimuthal equal-area
</p>
</li>
<li><p> Vertical near-side (i.e., as the world appears from geosynchronous orbit)
</p>
</li></ul>

<p>Please note that these are <em>NOT</em> standard projections, so do not have an EPSG or like code.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>customAlbers(x)

customLambert(x)

customVNS(x, alt = 35800)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="customAlbers_+3A_x">x</code></td>
<td>
<p>Either an object of class <code>SpatRaster</code>, <code>SpatVector</code>, or <code>sf</code>, <em>or</em> a numeric vector with two values (longitude and latitude of the center of the projection), <em>or</em> a two-column matrix/data frame with the centroid of the projection.</p>
</td></tr>
<tr><td><code id="customAlbers_+3A_alt">alt</code></td>
<td>
<p>Altitude in meters of the viewpoint in km. The default (35800 km) is geosynchronous orbit.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A WKT2 (well-known text) string.
</p>


<h3>Functions</h3>


<ul>
<li> <p><code>customLambert()</code>: Custom coordinate reference system WKT2 string
</p>
</li>
<li> <p><code>customVNS()</code>: Custom coordinate reference system WKT2 string
</p>
</li></ul>


<h3>See Also</h3>

<p><code><a href="#topic+getCRS">getCRS</a></code>, <code><a href="#topic+customAlbers">customAlbers</a></code>, <code><a href="#topic+customLambert">customLambert</a></code>, <code><a href="#topic+customVNS">customVNS</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(sf)

### Madagascar
data(mad0)

alb &lt;- customAlbers(mad0)
lamb &lt;- customLambert(mad0)
vert &lt;- customVNS(mad0)

madAlb &lt;- st_transform(mad0, alb)
madLamb &lt;- st_transform(mad0, lamb)
madVert &lt;- st_transform(mad0, vert)

oldPar &lt;- par(mfrow=c(2, 2))

plot(st_geometry(mad0), main='Unprojected (WGS84)')
plot(st_geometry(madAlb), main='Albers')
plot(st_geometry(madLamb), main='Lambert')
plot(st_geometry(madVert), main='Vertical')

par(oldPar)

### Canada
# The effect is more noticeable when plotting large areas,
# especially if they lie near the poles.
# This example can take a few minutes to run and plot.

library(terra)

canFile &lt;- system.file('extdata', 'canada_level0_gadm41.gpkg', package='enmSdmX')
can &lt;- vect(canFile)

alb &lt;- customAlbers(can)
lamb &lt;- customLambert(can)
vert &lt;- customVNS(can)

canAlb &lt;- project(can, alb)
canLamb &lt;- project(can, lamb)
canVert &lt;- project(can, vert)

oldPar &lt;- par(mfrow=c(2, 2))

plot(can, main = 'Unprojected (WGS84)')
plot(canAlb, main = 'Albers')
plot(canLamb, main = 'Lambert')
plot(canVert, main = 'Vertical')
	
par(oldPar)
</code></pre>

<hr>
<h2 id='decimalToDms'>Convert geographic coordinates in decimal format to degrees-minutes-second</h2><span id='topic+decimalToDms'></span>

<h3>Description</h3>

<p>This function converts geographic coordinates in decimal format to degrees-minutes-seconds (DD-MM-SS) format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>decimalToDms(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="decimalToDms_+3A_x">x</code></td>
<td>
<p>Numeric or vector of numeric values, longitude or latitude in decimal format.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric matrix with three columns: degrees, seconds, and seconds. Note that the hemisphere (i.e., indicated by the sign of x) is not returned since it could be either north/south or east/west.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
decimalToDms(38.56123) # latitude of St. Louis, Missouri, USA
decimalToDms(90.06521) # longitude of St. Louis, Missouri, USA

</code></pre>

<hr>
<h2 id='dmsToDecimal'>Convert geographic coordinates in degrees-minutes-second to decimal format</h2><span id='topic+dmsToDecimal'></span>

<h3>Description</h3>

<p>This function converts geographic coordinates in degrees-minutes-seconds (DD-MM-SS) format to decimal format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dmsToDecimal(dd, mm, ss, hemis = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dmsToDecimal_+3A_dd">dd</code></td>
<td>
<p>Numeric. Degrees longitude or latitude. Can be a decimal value.</p>
</td></tr>
<tr><td><code id="dmsToDecimal_+3A_mm">mm</code></td>
<td>
<p>Numeric. Minutes longitude or latitude. Can be a decimal value.</p>
</td></tr>
<tr><td><code id="dmsToDecimal_+3A_ss">ss</code></td>
<td>
<p>Numeric. Second longitude or latitude. Can be a decimal value.</p>
</td></tr>
<tr><td><code id="dmsToDecimal_+3A_hemis">hemis</code></td>
<td>
<p>Character or <code>NULL</code> (default). &quot;N&quot; (north), &quot;S&quot; (south), &quot;E&quot; (east), or &quot;W&quot; (west). If left as <code>NULL</code>, then the value returned will always be positive, even if it is in the western or southern hemisphere.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dmsToDecimal(38, 37, 38) # latitude of St. Louis, Missouri, USA
dmsToDecimal(38, 37, 38, 'N') # latitude of St. Louis, Missouri, USA
dmsToDecimal(90, 11, 52.1) # longitude of St. Louis, Missouri, USA
dmsToDecimal(90, 11, 52.1, 'W') # longitude of St. Louis, Missouri, USA
</code></pre>

<hr>
<h2 id='elimCellDuplicates'>Thin spatial points so that there is but one per raster cell</h2><span id='topic+elimCellDuplicates'></span>

<h3>Description</h3>

<p>This function thins spatial points such that no more than one point falls within each cell of a reference raster. If more than one point falls in a cell, the first point in the input data is retained unless the user specifies a priority for keeping points.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>elimCellDuplicates(x, rast, longLat = NULL, priority = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="elimCellDuplicates_+3A_x">x</code></td>
<td>
<p>Points. This can be either a <code>data.frame</code>, <code>matrix</code>, <code>SpatVector</code>, or <code>sf</code> object.</p>
</td></tr>
<tr><td><code id="elimCellDuplicates_+3A_rast">rast</code></td>
<td>
<p><code>SpatRaster</code> object.</p>
</td></tr>
<tr><td><code id="elimCellDuplicates_+3A_longlat">longLat</code></td>
<td>
<p>Two-element character vector <em>or</em> two-element integer vector. If <code>x</code> is a <code>data.frame</code>, then this should be a character vector specifying the names of the fields in <code>x</code> <em>or</em> a two-element vector of integers that correspond to longitude and latitude (in that order). For example, <code>c('long', 'lat')</code> or <code>c(1, 2)</code>. If <code>x</code> is a <code>matrix</code>, then this is a two-element vector indicating the column numbers in <code>x</code> that represent longitude and latitude. For example, <code>c(1, 2)</code>. If <code>x</code> is an <code>sf</code> object then this is ignored.</p>
</td></tr>
<tr><td><code id="elimCellDuplicates_+3A_priority">priority</code></td>
<td>
<p>Either <code>NULL</code>, in which case for every cell with more than one point the first point in <code>x</code> is chosen, or a numeric or character vector indicating preference for some points over others when points occur in the same cell. There should be the same number of elements in <code>priority</code> as there are points in <code>x</code>. Priority is assigned by the natural sort order of <code>priority</code>. For example, for 3 points in a cell for which <code>priority</code> is <code>c(2, 1, 3)</code>, the script will retain the second point and discard the rest. Similarly, if <code>priority</code> is <code>c('z', 'y', 'x')</code> then the third point will be chosen. Priorities assigned to points in other cells are ignored when thinning points in a particular cell.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class <code>x</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# This example can take &gt;10 second to run.

library(terra)
x &lt;- data.frame(
    long=c(-90.1, -90.1, -90.2, 20),
    lat=c(38, 38, 38, 38), point=letters[1:4]
)
rast &lt;- rast() # empty raster covering entire world with 1-degree resolution
elimCellDuplicates(x, rast, longLat=c(1, 2))
elimCellDuplicates(x, rast, longLat=c(1, 2), priority=c(3, 2, 1, 0))


</code></pre>

<hr>
<h2 id='evalAUC'>Weighted AUC</h2><span id='topic+evalAUC'></span>

<h3>Description</h3>

<p>This function calculates the area under the receiver-operator characteristic curve (AUC) following Mason &amp; Graham (2002). Each case (presence/non-presence) can be assigned a weight, if desired.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>evalAUC(
  pres,
  contrast,
  presWeight = rep(1, length(pres)),
  contrastWeight = rep(1, length(contrast)),
  na.rm = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="evalAUC_+3A_pres">pres</code></td>
<td>
<p>Vector of predictions for &quot;positive&quot; cases (e.g., predictions at presence sites).</p>
</td></tr>
<tr><td><code id="evalAUC_+3A_contrast">contrast</code></td>
<td>
<p>Vector of predictions for &quot;negative&quot; cases (e.g., predictions at absence/background sites).</p>
</td></tr>
<tr><td><code id="evalAUC_+3A_presweight">presWeight</code></td>
<td>
<p>Weights of positive cases. The default is to assign each positive case a weight of 1.</p>
</td></tr>
<tr><td><code id="evalAUC_+3A_contrastweight">contrastWeight</code></td>
<td>
<p>Weights of contrast cases. The default is to assign each case a weight of 1.</p>
</td></tr>
<tr><td><code id="evalAUC_+3A_na.rm">na.rm</code></td>
<td>
<p>Logical. If <code>TRUE</code> then remove any positive cases and associated weights and contrast predictions and associated weights with <code>NA</code>s.</p>
</td></tr>
<tr><td><code id="evalAUC_+3A_...">...</code></td>
<td>
<p>Other arguments (unused).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A Numeric value.
</p>


<h3>References</h3>

<p>Mason, S.J. and N.E. Graham.  2002.  Areas beneath the relative operating characteristics (ROC) and relative operating levels (ROL) curves: Statistical significance and interpretation.  <em>Quarterly Journal of the Royal Meteorological Society</em> 128:2145-2166. <a href="https://doi.org/10.1256/003590002320603584">doi:10.1256/003590002320603584</a>
</p>


<h3>See Also</h3>

<p><code><a href="predicts.html#topic+pa_evaluate">pa_evaluate</a></code>, <code><a href="#topic+evalMultiAUC">evalMultiAUC</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>pres &lt;- seq(0.5, 1, by=0.1)
contrast &lt;- seq(0, 1, by=0.01)

# unweighted
evalAUC(pres, contrast)

# weighted (weight presences with low predictions more)
presWeight &lt;- c(1, 1, 1, 0.5, 0.5, 0.5)
evalAUC(pres, contrast, presWeight=presWeight)

# weighted (weight presences with high predictions more)
presWeight &lt;- c(0.5, 0.5, 0.5, 1, 1, 1)
evalAUC(pres, contrast, presWeight=presWeight)

# weight presences and absences
contrastWeight &lt;- sqrt(contrast)
evalAUC(pres, contrast, presWeight=presWeight, contrastWeight=contrastWeight)
</code></pre>

<hr>
<h2 id='evalContBoyce'>Continuous Boyce Index (CBI) with weighting</h2><span id='topic+evalContBoyce'></span>

<h3>Description</h3>

<p>This function calculates the continuous Boyce index (CBI), a measure of model accuracy for presence-only test data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>evalContBoyce(
  pres,
  contrast,
  numBins = 101,
  binWidth = 0.1,
  presWeight = rep(1, length(pres)),
  contrastWeight = rep(1, length(contrast)),
  autoWindow = TRUE,
  method = "spearman",
  dropZeros = TRUE,
  graph = FALSE,
  table = FALSE,
  na.rm = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="evalContBoyce_+3A_pres">pres</code></td>
<td>
<p>Numeric vector. Predicted values at presence sites.</p>
</td></tr>
<tr><td><code id="evalContBoyce_+3A_contrast">contrast</code></td>
<td>
<p>Numeric vector. Predicted values at background sites.</p>
</td></tr>
<tr><td><code id="evalContBoyce_+3A_numbins">numBins</code></td>
<td>
<p>Positive integer. Number of (overlapping) bins into which to divide predictions.</p>
</td></tr>
<tr><td><code id="evalContBoyce_+3A_binwidth">binWidth</code></td>
<td>
<p>Positive numeric value &lt; 1. Size of a bin. Each bin will be <code>binWidth * (max - min)</code>. If <code>autoWindow</code> is <code>FALSE</code> (the default) then <code>min</code> is 0 and <code>max</code> is 1. If <code>autoWindow</code> is <code>TRUE</code> then <code>min</code> and <code>max</code> are the maximum and minimum value of all predictions in the background and presence sets (i.e., not necessarily 0 and 1).</p>
</td></tr>
<tr><td><code id="evalContBoyce_+3A_presweight">presWeight</code></td>
<td>
<p>Numeric vector same length as <code>pres</code>. Relative weights of presence sites. The default is to assign each presence a weight of 1.</p>
</td></tr>
<tr><td><code id="evalContBoyce_+3A_contrastweight">contrastWeight</code></td>
<td>
<p>Numeric vector same length as <code>contrast</code>. Relative weights of background sites. The default is to assign each presence a weight of 1.</p>
</td></tr>
<tr><td><code id="evalContBoyce_+3A_autowindow">autoWindow</code></td>
<td>
<p>Logical. If <code>FALSE</code> calculate bin boundaries starting at 0 and ending at 1 + epsilon (where epsilon is a very small number to assure inclusion of cases that equal 1 exactly). If <code>TRUE</code> (default) then calculate bin boundaries starting at minimum predicted value and ending at maximum predicted value.</p>
</td></tr>
<tr><td><code id="evalContBoyce_+3A_method">method</code></td>
<td>
<p>Character. Type of correlation to calculate. The default is <code>'spearman'</code>, the Spearman rank correlation coefficient used by Boyce et al. (2002) and Hirzel et al. (2006), which is the &quot;traditional&quot; CBI. In contrast, <code>'pearson'</code> or <code>'kendall'</code> can be used instead.  See <code><a href="stats.html#topic+cor">cor</a></code> for more details.</p>
</td></tr>
<tr><td><code id="evalContBoyce_+3A_dropzeros">dropZeros</code></td>
<td>
<p>Logical. If <code>TRUE</code> then drop all bins in which the frequency of presences is 0.</p>
</td></tr>
<tr><td><code id="evalContBoyce_+3A_graph">graph</code></td>
<td>
<p>Logical. If <code>TRUE</code> then plot P vs E and P/E versus bin.</p>
</td></tr>
<tr><td><code id="evalContBoyce_+3A_table">table</code></td>
<td>
<p>Logical. If <code>TRUE</code>, the function will also return a table of P (proportion of presence weights per bin), E (expected proportion of presence weights per bin&ndash;from contrast sites), and the ratio of the two.</p>
</td></tr>
<tr><td><code id="evalContBoyce_+3A_na.rm">na.rm</code></td>
<td>
<p>Logical. If <code>TRUE</code> then remove any presences and associated weights and background predictions and associated weights with <code>NA</code>s.</p>
</td></tr>
<tr><td><code id="evalContBoyce_+3A_...">...</code></td>
<td>
<p>Other arguments (not used).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>CBI is the Spearman rank correlation coefficient between the proportion of sites in each prediction class and the expected proportion of predictions in each prediction class based on the proportion of the landscape that is in that class.  The index ranges from -1 to 1. Values &gt;0 indicate the model's output is positively correlated with the true probability of presence.  Values &lt;0 indicate it is negatively correlated with the true probability of presence.
</p>


<h3>Value</h3>

<p>Numeric value, or if <code>table</code> is <code>TRUE</code>, then a list object with CBI plus a data frame with P (proportion of presence weights per bin), E (expected proportion of presence weights per bin&ndash;from contrast sites), and the ratio of the two.
</p>


<h3>References</h3>

<p>Boyce, M.S., Vernier, P.R., Nielsen, S.E., and Schmiegelow, F.K.A.  2002.  Evaluating resource selection functions.  <em>Ecological Modeling</em> 157:281-300. <a href="https://doi.org/10.1016/S0304-3800%2802%2900200-4">doi:10.1016/S0304-3800(02)00200-4</a>
</p>
<p>Hirzel, A.H., Le Lay, G., Helfer, V., Randon, C., and Guisan, A.  2006.  Evaluating the ability of habitat suitability models to predict species presences.  <em>Ecological Modeling</em> 199:142-152. <a href="https://doi.org/10.1016/j.ecolmodel.2006.05.017">doi:10.1016/j.ecolmodel.2006.05.017</a>
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+cor">cor</a></code>, <code><a href="predicts.html#topic+pa_evaluate">pa_evaluate</a></code>, <code><a href="#topic+evalAUC">evalAUC</a></code>, <code><a href="#topic+evalMultiAUC">evalMultiAUC</a></code>, <code><a href="#topic+evalContBoyce">evalContBoyce</a></code>, <code><a href="#topic+evalThreshold">evalThreshold</a></code>, <code><a href="#topic+evalThresholdStats">evalThresholdStats</a></code>, <code><a href="#topic+evalTjursR2">evalTjursR2</a></code>, <code><a href="#topic+evalTSS">evalTSS</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(123)
pres &lt;- sqrt(runif(100))
contrast &lt;- runif(1000)
evalContBoyce(pres, contrast)

presWeight &lt;- c(rep(1, 10), rep(0.5, 90))
evalContBoyce(pres, contrast, presWeight=presWeight)

</code></pre>

<hr>
<h2 id='evalMultiAUC'>Calculate multivariate weighted AUC</h2><span id='topic+evalMultiAUC'></span>

<h3>Description</h3>

<p>This function calculates a multivariate version of the area under the receiver-operator characteristic curve (AUC). The multivariate version is simply the mean AUC across all possible pairwise AUCs for all cases (Hand &amp; Till 2001). For example, if we have predictions that can be classified into three groups of expectation, say A, B, and C, where we expect predictions assigned to group A are &gt; those in B and C, and predictions in group B are expected to be &gt; those in group C, the multivariate AUC for this situation is <code>mean(wAB * auc_mean(A, B), wAC * auc_mean(A, C), wBC * auc_mean(B, C))</code>, where <code>auc_mean(X, Y)</code>, is the AUC calculated between cases <code>X</code> and <code>Y</code>, and <code>wXY</code> is a weight for that case-comparison.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>evalMultiAUC(..., weightBySize = FALSE, na.rm = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="evalMultiAUC_+3A_...">...</code></td>
<td>
<p>A set of two or more numeric vectors <em>or</em> two or more 2-column matrices or data frames. The objects must be listed in order of <em>expected</em> probability. For example, you might have a set of predictions for objects you expect to have a low predicted probability (e.g., long-term absences of an animal), a set that you expect to have middle levels of probability (e.g., sites that were recently vacated), and a set for which you expect a high level of predicted probability (e.g., sites that are currently occupied). In this case you should list the cases in order: low, middle, high. If a 2-column matrix or data frame is supplied, then the first column is assumed to represent predictions and the second assumed to represent site-level weights (see <code><a href="#topic+evalAUC">evalAUC</a></code>). Note that site-level weighting is different from case-level weighting.</p>
</td></tr>
<tr><td><code id="evalMultiAUC_+3A_weightbysize">weightBySize</code></td>
<td>
<p>Logical, if <code>FALSE</code> (default) then the multivariate measure of AUC will treat all comparisons as equal (e.g., low versus middle will weigh as much as middle versus high), and so will simply be the mean AUC across all possible comparisons. If <code>TRUE</code> then multivariate AUC is the weighted mean across all possible comparisons where weights are the number of comparisons between each of the two cases. For example, if a set of &quot;low&quot; predictions (&quot;low&quot;) has 10 data points, &quot;middle&quot; has 10, and &quot;high&quot; has 20, then the multivariate AUC will be (10 * low + 10 * middle + 20 * high) / (10 + 10 + 20).</p>
</td></tr>
<tr><td><code id="evalMultiAUC_+3A_na.rm">na.rm</code></td>
<td>
<p>Logical. If <code>TRUE</code> then remove any cases in <code>...</code> that are <code>NA</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Named numeric vector. The names will appear as <code>case2_over_case1</code> (which in this example means the AUC of item #1 in the <code>...</code> when compared to the second item in <code>...</code>), plus <code>multivariate</code> (which is the multivariate AUC).
</p>


<h3>References</h3>

<p>Hand, DJ and Till, RJ. 2001. A simple generalisation of the area under the ROC curve for multiple class classification problems. <em>Machine Learning</em> 45:171-186 <a href="https://doi.org/10.1023/A%3A1010920819831">doi:10.1023/A:1010920819831</a>.
</p>


<h3>See Also</h3>

<p><code><a href="predicts.html#topic+pa_evaluate">pa_evaluate</a></code>, <code><a href="#topic+evalAUC">evalAUC</a></code>, <code><a href="#topic+evalContBoyce">evalContBoyce</a></code>, <code><a href="#topic+evalThreshold">evalThreshold</a></code>, <code><a href="#topic+evalThresholdStats">evalThresholdStats</a></code>, <code><a href="#topic+evalTjursR2">evalTjursR2</a></code>, <code><a href="#topic+evalTSS">evalTSS</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)

# no weights
low &lt;- runif(10)^2
middle &lt;- runif(10)
high &lt;- sqrt(runif(20))

evalMultiAUC(low, middle, high)

# equal weights
low &lt;- matrix(c(low, rep(1, length(low))), ncol=2)
middle &lt;- matrix(c(middle, rep(1, length(middle))), ncol=2)
high &lt;- matrix(c(high, rep(1, length(high))), ncol=2)
evalMultiAUC(low, middle, high)

# equal weights with weighting by number of comparisons
evalMultiAUC(low, middle, high, weightBySize=TRUE)

# unequal weights
middle[ , 2] &lt;- ifelse(middle[ , 1] &gt; 0.5, 0.1, 1)
evalMultiAUC(low, middle, high)

# unequal weights with weighting by number of comparisons
evalMultiAUC(low, middle, high, weightBySize=TRUE)
</code></pre>

<hr>
<h2 id='evalThreshold'>Weighted thresholds for predictions</h2><span id='topic+evalThreshold'></span>

<h3>Description</h3>

<p>This function is similar to the <code><a href="predicts.html#topic+threshold">threshold</a></code> function in the <span class="pkg">predicts</span> package, which calculates thresholds to create binary predictions from continuous values. However, unlike that function, it allows the user to specify weights for presences and absence/background predictions. The output will thus be the threshold that best matches the specified criterion taking into account the relative weights of the input values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>evalThreshold(
  pres,
  contrast,
  presWeight = rep(1, length(pres)),
  contrastWeight = rep(1, length(contrast)),
  at = c("msss", "mdss", "minPres", "prevalence", "sensitivity"),
  sensitivity = 0.9,
  thresholds = seq(0, 1, by = 0.001),
  na.rm = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="evalThreshold_+3A_pres">pres</code></td>
<td>
<p>Numeric vector. Predicted values at test presences.</p>
</td></tr>
<tr><td><code id="evalThreshold_+3A_contrast">contrast</code></td>
<td>
<p>Numeric vector. Predicted values at background/absence sites.</p>
</td></tr>
<tr><td><code id="evalThreshold_+3A_presweight">presWeight</code></td>
<td>
<p>Numeric vector same length as <code>pres</code>. Relative weights of presence sites. The default is to assign each presence a weight of 1.</p>
</td></tr>
<tr><td><code id="evalThreshold_+3A_contrastweight">contrastWeight</code></td>
<td>
<p>Numeric vector same length as <code>contrast</code>. Relative weights of background sites. The default is to assign each presence a weight of 1.</p>
</td></tr>
<tr><td><code id="evalThreshold_+3A_at">at</code></td>
<td>
<p>Character or character vector, name(s) of threshold(s) to calculate. The default is to calculate them all.
</p>

<ul>
<li> <p><code>'msss'</code>: Threshold that the maximizes the sum of sensitivity and specificity.
</p>
</li>
<li> <p><code>'mdss'</code>: Threshold that minimizes the difference between sensitivity and specificity.
</p>
</li>
<li> <p><code>'minPres'</code>: Minimum prediction across presences. This threshold is not weighted.
</p>
</li>
<li> <p><code>'prevalence'</code>: Prevalence of presences (sum(presence weights) / sum(presence weights + background weights))'
</p>
</li>
<li> <p><code>'sensitivity'</code>: Threshold that most closely returns the sensitivity specified by <code>sensitivity</code>.
</p>
</li></ul>
</td></tr>
<tr><td><code id="evalThreshold_+3A_sensitivity">sensitivity</code></td>
<td>
<p>Value of specificity to match (used only if <code>at</code> contains <code>'sensitivity'</code>).</p>
</td></tr>
<tr><td><code id="evalThreshold_+3A_thresholds">thresholds</code></td>
<td>
<p>Numeric vector. Thresholds at which to calculate the sum of sensitivity and specificity. The default evaluates all values from 0 to 1 in steps of 0.01.</p>
</td></tr>
<tr><td><code id="evalThreshold_+3A_na.rm">na.rm</code></td>
<td>
<p>Logical. If <code>TRUE</code> then remove any presences and associated weights and background predictions and associated weights with <code>NA</code>s.</p>
</td></tr>
<tr><td><code id="evalThreshold_+3A_...">...</code></td>
<td>
<p>Other arguments (unused).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Named numeric vector.
Fielding, A.H. and J.F. Bell. 1997. A review of methods for the assessment of prediction errors in conservation presence/absence models. <em>Environmental Conservation</em> 24:38-49. <a href="https://doi.org/10.1017/S0376892997000088">doi:10.1017/S0376892997000088</a>
</p>


<h3>See Also</h3>

<p><code><a href="predicts.html#topic+threshold">threshold</a></code>, <code><a href="predicts.html#topic+pa_evaluate">pa_evaluate</a></code>, <code><a href="#topic+evalAUC">evalAUC</a></code>, <code><a href="#topic+evalMultiAUC">evalMultiAUC</a></code>, <code><a href="#topic+evalContBoyce">evalContBoyce</a></code>, <code><a href="#topic+evalThresholdStats">evalThresholdStats</a></code>, <code><a href="#topic+evalTjursR2">evalTjursR2</a></code>, <code><a href="#topic+evalTSS">evalTSS</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)

# set of bad and good predictions at presences
bad &lt;- runif(100)^2
good &lt;- runif(100)^0.1
hist(good, breaks=seq(0, 1, by=0.1), border='green', main='Presences')
hist(bad, breaks=seq(0, 1, by=0.1), border='red', add=TRUE)
pres &lt;- c(bad, good)
contrast &lt;- runif(1000)
evalThreshold(pres, contrast)

# upweight bad predictions
presWeight &lt;- c(rep(1, 100), rep(0.1, 100))
evalThreshold(pres, contrast, presWeight=presWeight)

# upweight good predictions
presWeight &lt;- c(rep(0.1, 100), rep(1, 100))
evalThreshold(pres, contrast, presWeight=presWeight)
</code></pre>

<hr>
<h2 id='evalThresholdStats'>Thresholded evaluation statistics</h2><span id='topic+evalThresholdStats'></span>

<h3>Description</h3>

<p>This function calculates a series of evaluation statistics based on a threshold or thresholds used to convert continuous predictions to binary predictions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>evalThresholdStats(
  thresholds,
  pres,
  contrast,
  presWeight = rep(1, length(pres)),
  contrastWeight = rep(1, length(contrast)),
  delta = 0.001,
  na.rm = FALSE,
  bg = NULL,
  bgWeight = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="evalThresholdStats_+3A_thresholds">thresholds</code></td>
<td>
<p>Numeric or numeric vector. Threshold(s) at which to calculate sensitivity and specificity.</p>
</td></tr>
<tr><td><code id="evalThresholdStats_+3A_pres">pres</code></td>
<td>
<p>Numeric vector. Predicted values at test presences</p>
</td></tr>
<tr><td><code id="evalThresholdStats_+3A_contrast">contrast</code></td>
<td>
<p>Numeric vector. Predicted values at background/absence sites.</p>
</td></tr>
<tr><td><code id="evalThresholdStats_+3A_presweight">presWeight</code></td>
<td>
<p>Numeric vector same length as <code>pres</code>. Relative weights of presence sites. The default is to assign each presence a weight of 1.</p>
</td></tr>
<tr><td><code id="evalThresholdStats_+3A_contrastweight">contrastWeight</code></td>
<td>
<p>Numeric vector same length as <code>contrast</code>. Relative weights of background sites. The default is to assign each presence a weight of 1.</p>
</td></tr>
<tr><td><code id="evalThresholdStats_+3A_delta">delta</code></td>
<td>
<p>Positive numeric &gt;0 in the range [0, 1] and usually very small. This value is used only if calculating the SEDI threshold when any true positive rate or false negative rate is 0 or the false negative rate is 1. Since SEDI uses log(x) and log(1 - x), values of 0 and 1 will produce <code>NA</code>s. To obviate this, a small amount can be added to rates that equal 0 and subtracted from rates that equal 1.</p>
</td></tr>
<tr><td><code id="evalThresholdStats_+3A_na.rm">na.rm</code></td>
<td>
<p>Logical. If <code>TRUE</code> then remove any presences and associated weights and background predictions and associated weights with <code>NA</code>s.</p>
</td></tr>
<tr><td><code id="evalThresholdStats_+3A_bg">bg</code></td>
<td>
<p>Same as <code>contrast</code>. Included for backwards compatibility. Ignored if <code>contrast</code> is not <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="evalThresholdStats_+3A_bgweight">bgWeight</code></td>
<td>
<p>Same as <code>contrastWeight</code>. Included for backwards compatibility. Ignored if <code>contrastWeight</code> is not <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="evalThresholdStats_+3A_...">...</code></td>
<td>
<p>Other arguments (unused).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>8-column matrix with the following named columns. <em>a</em> = weight of presences &gt;= threshold, <em>b</em> = weight of backgrounds &gt;= threshold, <em>c</em> = weight of presences &lt; threshold, <em>d</em> = weight of backgrounds &lt; threshold, and <em>N</em> = sum of presence and background weights.
</p>

<ul>
<li> <p><code>'threshold'</code>: Threshold
</p>
</li>
<li> <p><code>'sensitivity'</code>: Sensitivity (<em>a</em> / (<em>a</em> + <em>c</em>))
</p>
</li>
<li> <p><code>'specificity'</code>: Specificity (<em>d</em> / (<em>d</em> + <em>b</em>))
</p>
</li>
<li> <p><code>'ccr'</code>: Correct classification rate ((<em>a</em> + <em>d</em>) / <em>N</em>)
</p>
</li>
<li> <p><code>'ppp'</code>: Positive predictive power (<em>a</em> / (<em>a</em> + <em>b</em>))
</p>
</li>
<li> <p><code>'npp'</code>: Negative predictive power (<em>d</em> / (<em>c</em> + <em>d</em>))
</p>
</li>
<li> <p><code>'mr'</code>: Misclassification rate ((<em>b</em> + <em>c</em>) / <em>N</em>)
</p>
</li></ul>

<p>Fielding, A.H. and J.F. Bell. 1997. A review of methods for the assessment of prediction errors in conservation presence/absence models. <em>Environmental Conservation</em> 24:38-49. <a href="https://doi.org/10.1017/S0376892997000088">doi:10.1017/S0376892997000088</a>
</p>


<h3>See Also</h3>

<p><code><a href="predicts.html#topic+threshold">threshold</a></code>, <code><a href="predicts.html#topic+pa_evaluate">pa_evaluate</a></code>, <code><a href="#topic+evalAUC">evalAUC</a></code>, <code><a href="#topic+evalMultiAUC">evalMultiAUC</a></code>, <code><a href="#topic+evalContBoyce">evalContBoyce</a></code>, <code><a href="#topic+evalThreshold">evalThreshold</a></code>, <code><a href="#topic+evalTjursR2">evalTjursR2</a></code>, <code><a href="#topic+evalTSS">evalTSS</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)

# set of bad and good predictions at presences
bad &lt;- runif(100)^2
good &lt;- runif(100)^0.1
hist(good, breaks=seq(0, 1, by=0.1), border='green', main='Presences')
hist(bad, breaks=seq(0, 1, by=0.1), border='red', add=TRUE)
pres &lt;- c(bad, good)
contrast &lt;- runif(1000)
thresholds &lt;- c(0.1, 0.5, 0.9)
evalThresholdStats(thresholds, pres, contrast)

# upweight bad predictions
presWeight &lt;- c(rep(1, 100), rep(0.1, 100))
evalThresholdStats(thresholds, pres, contrast, presWeight=presWeight)

# upweight good predictions
presWeight &lt;- c(rep(0.1, 100), rep(1, 100))
evalThresholdStats(thresholds, pres, contrast, presWeight=presWeight)
</code></pre>

<hr>
<h2 id='evalTjursR2'>Weighted Tjur's R2</h2><span id='topic+evalTjursR2'></span>

<h3>Description</h3>

<p>This function calculates Tjur's R2 metric of model discrimination accuracy. Unweighted R2 is simply the difference between the mean predicted value at presence sites and the mean predicted value at absence/background sites. The weighted version allows for differing weights between presences and between absences/contrast values (i.e., the difference between the weighted mean of predictions at presences and weighted mean predictions at absences/contrast locations).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>evalTjursR2(
  pres,
  contrast,
  presWeight = rep(1, length(pres)),
  contrastWeight = rep(1, length(contrast)),
  na.rm = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="evalTjursR2_+3A_pres">pres</code></td>
<td>
<p>Predictions at presence sites.</p>
</td></tr>
<tr><td><code id="evalTjursR2_+3A_contrast">contrast</code></td>
<td>
<p>Predictions at absence/background sites.</p>
</td></tr>
<tr><td><code id="evalTjursR2_+3A_presweight">presWeight</code></td>
<td>
<p>Weights of presence cases. The default is to assign each presence case a weight of 1.</p>
</td></tr>
<tr><td><code id="evalTjursR2_+3A_contrastweight">contrastWeight</code></td>
<td>
<p>Weights of absence/background cases. The default is to assign each case a weight of 1.</p>
</td></tr>
<tr><td><code id="evalTjursR2_+3A_na.rm">na.rm</code></td>
<td>
<p>Logical. If <code>TRUE</code> then remove any presences and associated weights and background predictions and associated weights with <code>NA</code>s.</p>
</td></tr>
<tr><td><code id="evalTjursR2_+3A_...">...</code></td>
<td>
<p>Other arguments (unused).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric value.
</p>


<h3>References</h3>

<p>Tjur, T. 2009. Coefficients of determination in logistic regression models-A new proposal: The coefficient of discrimination. <em>The American Statistician</em> 63:366-372. <a href="https://doi.org/10.1198/tast.2009.08210">doi:10.1198/tast.2009.08210</a>
</p>


<h3>See Also</h3>

<p><code><a href="predicts.html#topic+pa_evaluate">pa_evaluate</a></code>, <code><a href="#topic+evalAUC">evalAUC</a></code>, <code><a href="#topic+evalMultiAUC">evalMultiAUC</a></code>, <code><a href="#topic+evalContBoyce">evalContBoyce</a></code>, <code><a href="#topic+evalThreshold">evalThreshold</a></code>, <code><a href="#topic+evalThresholdStats">evalThresholdStats</a></code>, <code><a href="#topic+evalTSS">evalTSS</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>pres &lt;- seq(0.5, 1, by=0.1)
contrast &lt;- seq(0, 1, by=0.01)

# unweighted
evalTjursR2(pres, contrast)

# weighted (weight presences with low predictions more)
presWeight &lt;- c(1, 1, 1, 0.5, 0.5, 0.5)
evalTjursR2(pres, contrast, presWeight=presWeight)

# weighted (weight presences with high predictions more)
presWeight &lt;- c(0.5, 0.5, 0.5, 1, 1, 1)
evalTjursR2(pres, contrast, presWeight=presWeight)

# weight presences and absences
contrastWeight &lt;- sqrt(contrast)
evalTjursR2(pres, contrast, presWeight=presWeight, contrastWeight=contrastWeight)
</code></pre>

<hr>
<h2 id='evalTSS'>Weighted True Skill Statistic (TSS)</h2><span id='topic+evalTSS'></span>

<h3>Description</h3>

<p>This function calculates the True Skill Statistic (TSS).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>evalTSS(
  pres,
  contrast,
  presWeight = rep(1, length(pres)),
  contrastWeight = rep(1, length(contrast)),
  thresholds = seq(0, 1, by = 0.001),
  na.rm = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="evalTSS_+3A_pres">pres</code></td>
<td>
<p>Numeric vector. Predicted values at test presences</p>
</td></tr>
<tr><td><code id="evalTSS_+3A_contrast">contrast</code></td>
<td>
<p>Numeric vector. Predicted values at background/absence sites.</p>
</td></tr>
<tr><td><code id="evalTSS_+3A_presweight">presWeight</code></td>
<td>
<p>Numeric vector same length as <code>pres</code>. Relative weights of presence sites. The default is to assign each presence a weight of 1.</p>
</td></tr>
<tr><td><code id="evalTSS_+3A_contrastweight">contrastWeight</code></td>
<td>
<p>Numeric vector same length as <code>contrast</code>. Relative weights of background sites. The default is to assign each presence a weight of 1.</p>
</td></tr>
<tr><td><code id="evalTSS_+3A_thresholds">thresholds</code></td>
<td>
<p>Numeric vector. Thresholds at which to calculate the sum of sensitivity and specificity. The default evaluates all values from 0 to 1 in steps of 0.01.</p>
</td></tr>
<tr><td><code id="evalTSS_+3A_na.rm">na.rm</code></td>
<td>
<p>Logical. If <code>TRUE</code> then remove any presences and associated weights and background predictions and associated weights with <code>NA</code>s.</p>
</td></tr>
<tr><td><code id="evalTSS_+3A_...">...</code></td>
<td>
<p>Other arguments (unused).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates the maximum value of the True Skill Statistic (i.e., across all thresholds, the values that maximizes sensitivity plus specificity).
</p>


<h3>Value</h3>

<p>Numeric value.
</p>


<h3>References</h3>

<p>See Allouche, O., Tsoar, A., and Kadmon, R. 2006. Assessing the accuracy of species distribution models: Prevalence, kappa and the true skill statistic (TSS). <em>Journal of Applied Ecology</em> 43:1223-1232. <a href="https://doi.org/10.1111/j.1365-2664.2006.01214.x">doi:10.1111/j.1365-2664.2006.01214.x</a>
</p>


<h3>See Also</h3>

<p><code><a href="predicts.html#topic+pa_evaluate">pa_evaluate</a></code>, <code><a href="#topic+evalAUC">evalAUC</a></code>, <code><a href="#topic+evalMultiAUC">evalMultiAUC</a></code>, <code><a href="#topic+evalContBoyce">evalContBoyce</a></code>, <code><a href="#topic+evalThreshold">evalThreshold</a></code>, <code><a href="#topic+evalThresholdStats">evalThresholdStats</a></code>, <code><a href="#topic+evalTjursR2">evalTjursR2</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)

# set of bad and good predictions at presences
bad &lt;- runif(30)^2
good &lt;- runif(30)^0.1
hist(good, breaks=seq(0, 1, by=0.1), border='green', main='Presences')
hist(bad, breaks=seq(0, 1, by=0.1), border='red', add=TRUE)
pres &lt;- c(bad, good)
contrast &lt;- runif(1000)
evalTSS(pres, contrast)

# upweight bad predictions
presWeight &lt;- c(rep(1, 30), rep(0.1, 30))
evalTSS(pres, contrast, presWeight=presWeight)

# upweight good predictions
presWeight &lt;- c(rep(0.1, 30), rep(1, 30))
evalTSS(pres, contrast, presWeight=presWeight)

</code></pre>

<hr>
<h2 id='extentToVect'>Convert extent to a spatial polygon</h2><span id='topic+extentToVect'></span>

<h3>Description</h3>

<p>This function returns a <code>SpatVector</code> or <code>sf</code> polygon representing an extent. The input can be a <code>SpatExtent</code> or <code>sf</code> object, or an object from which a <code>SpatExtent</code> (extent) can be obtained.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extentToVect(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="extentToVect_+3A_x">x</code></td>
<td>
<p>A <code>sf</code>, <code>SpatVector</code>, <code>SpatRaster</code>, <code>sf</code>, <em>or</em> a vector of four numeric values representing (in this order): x-coordinate of western side of the extent, x-coordinate of eastern side, y-coordinate of the southern side, and y-coordinate of the northern side. If numeric coordinates are supplied, the output will not have a CRS assigned to it unless supplied in <code>...</code></p>
</td></tr>
<tr><td><code id="extentToVect_+3A_...">...</code></td>
<td>
<p>Arguments to supply to <code>vect</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>SpatVector</code> (usual) or, if the input is an <code>sf</code> object, an <code>sf</code> polygon object.
</p>


<h3>See Also</h3>

<p><a href="#topic+plotExtent">plotExtent</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(mad0)
madExtent &lt;- extentToVect(mad0)
plot(madExtent, border='blue', lty='dotted')
plot(mad0[1], add=TRUE)

# NB This is the same as:
library(terra)
madExtent &lt;- ext(mad0)
madExtent &lt;- as.polygons(madExtent, crs = crs(mad0))

</code></pre>

<hr>
<h2 id='geoFold'>Assign geographically-distinct k-folds</h2><span id='topic+geoFold'></span>

<h3>Description</h3>

<p>This function generates geographically-distinct cross-validation folds, or &quot;geo-folds&quot; (&quot;g-folds&quot; for short). Points are grouped by proximity to one another. Folds can be forced to have at least a minimum number of points in them. Results are deterministic (i.e., the same every time for the same data). <br /> <br />
More specifically, g-folds are created using this process:
</p>

<ul>
<li><p> To start, all pairwise distances between points are calculated. These are used in a clustering algorithm to create a dendrogram of relationships by distance. The dendrogram is then &quot;cut&quot; so it has <code>k</code> groups (folds). If each fold has at least the minimum desired number of points (<code>minIn</code>), then the process stops and fold assignments are returned.
</p>
</li>
<li><p> However, if at least one fold has fewer than the desired number of points, a series of steps is executed.
</p>

<ul>
<li><p> First, the fold with a centroid that is farthest from all others is selected. If it has sufficient points, then the next-most distant fold is selected, and so on.
</p>
</li>
<li><p> Once a fold is identified that has fewer than the desired number of points, it is grown by adding to it the points closest to its centroid, one at a time. Each time a point is added, the fold centroid is calculated again. The fold is grown until it has the desired number of points. Call this &quot;fold #1&quot;. From hereafter, these points are considered &quot;assigned&quot; and not eligible for re-assignment.
</p>
</li>
<li><p> The remaining &quot;unassigned&quot; points are then clustered again, but this time into <code>k - 1</code> folds. And again, the most-distant group found that has fewer than the desired number of points is found. This fold is then grown as before, using only unassigned points. This fold then becomes &quot;fold #2.&quot;
</p>
</li>
<li><p> The process repeats iteratively until there are <code>k</code> folds assigned, each with at least the desired number of points. 
</p>
</li></ul>

</li></ul>

<p>The potential downside of this approach is that the last fold is assigned the remainder of points, so will be the largest. One way to avoid gross imbalance is to select the value of <code>minIn</code> such that it divides the points into nearly equally-sized groups.<br /><br />
</p>


<h3>Usage</h3>

<pre><code class='language-R'>geoFold(x, k, minIn = 1, longLat = 1:2, method = "complete", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="geoFold_+3A_x">x</code></td>
<td>
<p>A &quot;spatial points&quot; object of class <code>SpatVector</code>, <code>sf</code>, <code>data.frame</code>, or <code>matrix</code>. If <code>x</code> is a <code>data.frame</code> or <code>matrix</code>, then the points will be assumed to have the WGS84 coordinate system (i.e., unprojected).</p>
</td></tr>
<tr><td><code id="geoFold_+3A_k">k</code></td>
<td>
<p>Numeric: Number of folds to create.</p>
</td></tr>
<tr><td><code id="geoFold_+3A_minin">minIn</code></td>
<td>
<p>Numeric: Minimum number of points required to be in a fold.</p>
</td></tr>
<tr><td><code id="geoFold_+3A_longlat">longLat</code></td>
<td>
<p>Character or integer vector: This is ignored if <code>x</code> is a <code>SpatVector</code> or <code>sf</code> object. However, if <code>x</code> is a <code>data.frame</code> or <code>matrix</code>, then this should be a character or integer vector specifying the columns in <code>x</code> corresponding to longitude and latitude (in that order). For example, <code>c('long', 'lat')</code> or <code>c(1, 2)</code>. The default is to assume that the first two columns in <code>x</code> represent coordinates.</p>
</td></tr>
<tr><td><code id="geoFold_+3A_method">method</code></td>
<td>
<p>Character: Method used by <code><a href="stats.html#topic+hclust">hclust</a></code> to cluster points. By default, this is <code>'complete'</code>, but other methods may give more reasonable results, depending on the case.</p>
</td></tr>
<tr><td><code id="geoFold_+3A_...">...</code></td>
<td>
<p>Additional arguments (unused)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that in general it is probably mathematically impossible to cluster points in 2-dimensional space into <code>k</code> groups, each with at least <code>minIn</code> points, in a manner that seems &quot;reasonable&quot; to the eye in all cases. In experimentation, &quot;unreasonable&quot; results often appear when the number of groups is high.
</p>


<h3>Value</h3>

<p>A vector of integers the same length as the number of points in <code>x</code>. Each integer indicates which fold a point in <code>x</code> belongs to.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+geoFoldContrast">geoFoldContrast</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(sf)
library(terra)

# lemur occurrence data
data(mad0)
data(lemurs)
crs &lt;- getCRS('WGS84')
ll &lt;- c('longitude', 'latitude')

# use occurrences of all species... easier to see on map
occs &lt;- st_as_sf(lemurs, coords = ll, crs = getCRS('WGS84'))

# create 100 background points
mad0 &lt;- vect(mad0)
bg &lt;- spatSample(mad0, 100)

### assign 3 folds to occurrences and to background sites
k &lt;- 3
minIn &lt;- floor(nrow(occs) / k) # maximally spread between folds

presFolds &lt;- geoFold(occs, k = k, minIn = minIn)
bgFolds &lt;- geoFoldContrast(bg, pres = occs, presFolds = presFolds)

# number of sites per fold
table(presFolds)
table(bgFolds)

# map
plot(mad0, border = 'gray', main = paste(k, 'geo-folds'))
plot(bg, pch = 3, col = bgFolds + 1, add = TRUE)
plot(st_geometry(occs), pch = 20 + presFolds, bg = presFolds + 1, add = TRUE)

legend(
	'bottomright',
	legend = c(
		'presence fold 1',
		'presence fold 2',
		'presence fold 3',
		'background fold 1',
		'background fold 2',
		'background fold 3'
	),
	pch = c(21, 22, 23, 3, 3),
	col = c(rep('black', 3), 2, 3),
	pt.bg = c(2, 3, 4, NA, NA)
)
</code></pre>

<hr>
<h2 id='geoFoldContrast'>Assign geographically-distinct k-folds to background/absence sites</h2><span id='topic+geoFoldContrast'></span>

<h3>Description</h3>

<p>This function generates geographically-distinct cross-validation folds, or &quot;geo-folds&quot; of background or absence sites (i.e., &quot;contrast&quot; sites). Each contrast site is assigned to a fold based on the fold of the presence site that is closest. Typically, this function is run after <code><a href="#topic+geoFold">geoFold</a></code> is run to assign presences to folds.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>geoFoldContrast(
  contrast,
  pres,
  presFolds,
  contrastLongLat = 1:2,
  presLongLat = 1:2,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="geoFoldContrast_+3A_contrast">contrast</code></td>
<td>
<p>A &quot;spatial points&quot; object representing contrast sites:
</p>

<ul>
<li><p> A <code>SpatVector</code> or <code>sf</code> vector with points
</p>
</li>
<li><p> A <code>data.frame</code> or <code>matrix</code>: Points will be assumed to have the WGS84 coordinate system (i.e., unprojected), and <code>contrastLongLat</code> should denote the columns with coordinates.
</p>
</li></ul>
</td></tr>
<tr><td><code id="geoFoldContrast_+3A_pres">pres</code></td>
<td>
<p>A &quot;spatial points&quot; object representing presence sites:
</p>

<ul>
<li><p> A <code>SpatVector</code> or <code>sf</code> vector with points
</p>
</li>
<li><p> A <code>data.frame</code> or <code>matrix</code>: Points will be assumed to have the WGS84 coordinate system (i.e., unprojected), and <code>presLongLat</code> should denote the columns with coordinates.
</p>
</li></ul>
</td></tr>
<tr><td><code id="geoFoldContrast_+3A_presfolds">presFolds</code></td>
<td>
<p>Numeric vector: These provide the folds to which <code>pres</code> are assigned. There must be one value per point in <code>pres</code>.</p>
</td></tr>
<tr><td><code id="geoFoldContrast_+3A_contrastlonglat">contrastLongLat</code>, <code id="geoFoldContrast_+3A_preslonglat">presLongLat</code></td>
<td>
<p>Character or integer vector: A character or integer vector specifying the columns in <code>contrast</code> and <code>pres</code> corresponding to longitude and latitude (in that order). The default is to assume that the first two columns in <code>contrast</code> represent coordinates. These are ignored if <code>contrast</code> or <code>pres</code> are a <code>SpatVector</code> or an <code>sf</code> object.</p>
</td></tr>
<tr><td><code id="geoFoldContrast_+3A_...">...</code></td>
<td>
<p>Additional arguments (unused)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of integers the same length as the number of points in <code>contrast</code>. Each integer indicates which fold a point in <code>contrast</code> belongs to.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+geoFold">geoFold</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(sf)
library(terra)

# lemur occurrence data
data(mad0)
data(lemurs)
crs &lt;- getCRS('WGS84')
ll &lt;- c('longitude', 'latitude')

# use occurrences of all species... easier to see on map
occs &lt;- st_as_sf(lemurs, coords = ll, crs = getCRS('WGS84'))

# create 100 background points
mad0 &lt;- vect(mad0)
bg &lt;- spatSample(mad0, 100)

### assign 3 folds to occurrences and to background sites
k &lt;- 3
minIn &lt;- floor(nrow(occs) / k) # maximally spread between folds

presFolds &lt;- geoFold(occs, k = k, minIn = minIn)
bgFolds &lt;- geoFoldContrast(bg, pres = occs, presFolds = presFolds)

# number of sites per fold
table(presFolds)
table(bgFolds)

# map
plot(mad0, border = 'gray', main = paste(k, 'geo-folds'))
plot(bg, pch = 3, col = bgFolds + 1, add = TRUE)
plot(st_geometry(occs), pch = 20 + presFolds, bg = presFolds + 1, add = TRUE)

legend(
	'bottomright',
	legend = c(
		'presence fold 1',
		'presence fold 2',
		'presence fold 3',
		'background fold 1',
		'background fold 2',
		'background fold 3'
	),
	pch = c(21, 22, 23, 3, 3),
	col = c(rep('black', 3), 2, 3),
	pt.bg = c(2, 3, 4, NA, NA)
)
</code></pre>

<hr>
<h2 id='geoThin'>Thin geographic points deterministically or randomly</h2><span id='topic+geoThin'></span>

<h3>Description</h3>

<p>This function thins geographic points such that none have nearest neighbors closer than some user-specified distance. For a given set of points that fall within this distance, thinning can be conducted in two ways.  Both begin by first calculating all pairwise distances between points. Then, clusters of points are found based on proximity using the &quot;single-linkage&quot; method (i.e., based on minimum distance between groups). Then, either a deterministic or random method is used to select the retained points:
</p>

<ul>
<li><p> Deterministic: For each cluster, distances between each point in the cluster and all points outside of the cluster are calculated. The point retained in each cluster is the one with the greatest minimum pairwise distance to any points in any other cluster. This point will this be maximally isolated from any other point.
</p>
</li>
<li><p> Random: For each cluster, a random point is chosen.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>geoThin(x, minDist, random = FALSE, longLat = 1:2, method = "single", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="geoThin_+3A_x">x</code></td>
<td>
<p>A &quot;spatial points&quot; object of class <code>SpatVector</code>, <code>sf</code>, <code>data.frame</code>, or <code>matrix</code>. If <code>x</code> is a <code>data.frame</code> or <code>matrix</code>, then the points will be assumed to have the WGS84 coordinate system (i.e., unprojected).</p>
</td></tr>
<tr><td><code id="geoThin_+3A_mindist">minDist</code></td>
<td>
<p>Minimum distance (in meters) needed between points to retain them. Points falling closer than this distance will be candidates for being discarded.</p>
</td></tr>
<tr><td><code id="geoThin_+3A_random">random</code></td>
<td>
<p>Logical: If <code>FALSE</code> (default), then use the deterministic method for thinning. If <code>TRUE</code>, then use the random method.</p>
</td></tr>
<tr><td><code id="geoThin_+3A_longlat">longLat</code></td>
<td>
<p>Numeric or integer vector: This is ignored if <code>x</code> is a <code>SpatVector</code> or <code>sf</code> object. However, if <code>x</code> is a <code>data.frame</code> or <code>matrix</code>, then this should be a character or integer vector specifying the columns in <code>x</code> corresponding to longitude and latitude (in that order). For example, <code>c('long', 'lat')</code> or <code>c(1, 2)</code>. The default is to assume that the first two columns in <code>x</code> represent coordinates.</p>
</td></tr>
<tr><td><code id="geoThin_+3A_method">method</code></td>
<td>
<p>Character: Method used by <code><a href="stats.html#topic+hclust">hclust</a></code> to cluster points. By default, this is <code>'single'</code>, but in some cases this may result in strange clustering (especially when there is a large number of points). The <code>'complete'</code> method (or others) may give more reasonable results in these cases.</p>
</td></tr>
<tr><td><code id="geoThin_+3A_...">...</code></td>
<td>
<p>Additional arguments. Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class <code>x</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(sf)

# lemur occurrence data
data(mad0)
data(lemurs)
crs &lt;- getCRS('WGS84')
occs &lt;- lemurs[lemurs$species == 'Eulemur fulvus', ]
ll &lt;- c('longitude', 'latitude')
occs &lt;- st_as_sf(occs, coords = ll, crs = getCRS('WGS84'))

# deterministically thin
det &lt;- geoThin(x = occs, minDist = 30000)

# randomly thin
set.seed(123)
rand &lt;- geoThin(x = occs, minDist = 30000, random = TRUE)

# map
oldPar &lt;- par(mfrow = c(1, 2))

plot(st_geometry(occs), cex = 1.4, main = 'Deterministic')
plot(st_geometry(det), pch = 21, cex = 1.4, bg = 1:nrow(det), add = TRUE)
plot(st_geometry(mad0), add = TRUE)

plot(st_geometry(occs), cex = 1.4, main = 'Random')
plot(st_geometry(rand), pch = 21, cex = 1.4, bg = 1:nrow(rand), add = TRUE)
plot(st_geometry(mad0), add = TRUE)

par(oldPar)

</code></pre>

<hr>
<h2 id='getCRS'>WKT string for a named coordinate reference system or a spatial object</h2><span id='topic+getCRS'></span>

<h3>Description</h3>

<p>Retrieve the Well-Known text string (WKT2) for a coordinate reference system (CRS) by name or from a spatial object. The most common usage of the function is to return the WKT2 string using an easy-to-remember name. For example, <code>getCRS('wgs84')</code> returns the WKT2 string for the WGS84 datum. To get a table of strings, just use <code>getCRS()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getCRS(x = NULL, nice = FALSE, warn = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getCRS_+3A_x">x</code></td>
<td>
<p>This can be any of:
</p>

<ul>
<li><p> Name of CRS: Each CRS has at least one &quot;alias&quot;, which appears in the table returned by <code>getCRS()</code>. You can use any of the aliases to refer to a CRS. Within the function, spaces, case, and dashes in aliases are ignored, but to help make the aliases more memorable, the aliases include them.
</p>
</li>
<li> <p><code>NULL</code> (default): This returns a table of projections with their aliases (nearly the same as <code>data(crss)</code>).
</p>
</li>
<li><p> An object of class <code>SpatVector</code>, <code>SpatRaster</code>, or <code>sf</code>. If this is a &quot;<code>Spat</code>&quot; or &quot;<code>G</code>&quot; object, then a character vector with the CRS in WKT form is returned. If a <code>sf</code> is supplied, then a <code>crs</code> object is returned in WKT format.
</p>
</li></ul>
</td></tr>
<tr><td><code id="getCRS_+3A_nice">nice</code></td>
<td>
<p>If <code>TRUE</code>, then print the CRS in a formatted manner and return it invisibly. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="getCRS_+3A_warn">warn</code></td>
<td>
<p>If <code>TRUE</code> (default), then print a warning if the name of the CRS cannot be found.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A string representing WKT2 (well-known text) object or a <code>data.frame</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# view table of available CRSs
getCRS()

# get specific WKT2 strings
getCRS('WGS84')
getCRS('Mollweide')
getCRS('WorldClim')

# WKT2 strings nice for your eyes
getCRS('WGS84', TRUE)

data(mad0)
getCRS(mad0)

</code></pre>

<hr>
<h2 id='getValueByCell'>Get or assign values to cells in a raster</h2><span id='topic+getValueByCell'></span><span id='topic+setValueByCell'></span>

<h3>Description</h3>

<p>These functions get values from a raster at specific cells, or values to specific cells.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getValueByCell(x, cell, format = "raster")

setValueByCell(x, val, cell, format = "raster")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getValueByCell_+3A_x">x</code></td>
<td>
<p>A <code>SpatRaster</code>.</p>
</td></tr>
<tr><td><code id="getValueByCell_+3A_cell">cell</code></td>
<td>
<p>Cell indices. There must be one per value in <code>val</code>.</p>
</td></tr>
<tr><td><code id="getValueByCell_+3A_format">format</code></td>
<td>
<p>The type of cell indexing used. This can be either &quot;raster&quot; for row indexing (default) or &quot;matrix&quot; for column indexing. Row indexing (the default for rasters), starts with cell &quot;1&quot; in the upper left, cell &quot;2&quot; is to its right, and so on. Numbering then wraps around to the next row. Column indexing (the default for matrices) has the cell &quot;1&quot; in the upper left corner of the matrix. The cell &quot;2&quot; is below it, and so on. The numbering then wraps around to the top of the next column.</p>
</td></tr>
<tr><td><code id="getValueByCell_+3A_val">val</code></td>
<td>
<p>One or more values. If more the number of cells specified is greater than the number of values in <code>val</code>, then values in <code>val</code> will be recycled.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame (<code>getValueByCell</code>) with cell numbers (in row format), or a <code>SpatRaster</code> (<code>setValueByCell</code>).
</p>


<h3>See Also</h3>

<p><code><a href="terra.html#topic+setValues">setValues</a></code>, <code><a href="terra.html#topic+values">values</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(terra)
x &lt;- rast(nrow=10, ncol=10)
x[] &lt;- round(10 * runif(100))

cell &lt;- c(1, 20, 40, 80)
getValueByCell(x, cell = cell)
getValueByCell(x, cell = cell, format = 'matrix')

y &lt;- setValueByCell(x, val = 20, cell = cell)
plot(y)
z &lt;- setValueByCell(x, val = 30, cell = cell, format = 'matrix')

plot(c(x, y, z))

</code></pre>

<hr>
<h2 id='globalx'>&quot;Friendly&quot; wrapper for terra::global() for calculating raster statistics</h2><span id='topic+globalx'></span>

<h3>Description</h3>

<p>Calculate &quot;global&quot; statistics across all the values in a raster. This function is a wrapper for <code><a href="terra.html#topic+global">global</a></code>.  That function, by default, sets <code>na.rm = FALSE</code>, so any cell that is <code>NA</code> can cause the summary statistic to also be <code>NA</code> (usually undesirable). The function also returns a <code>data.frame</code>, so often needs a further line of code to get the actual value(s). This function sets <code>na.rm = TRUE</code> by default, and returns a numeric vector (not a <code>data.frame</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>globalx(x, fun, na.rm = TRUE, ..., weights = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="globalx_+3A_x">x</code></td>
<td>
<p>A <code>SpatRaster</code>.</p>
</td></tr>
<tr><td><code id="globalx_+3A_fun">fun</code></td>
<td>
<p>A function or the name of a function (in quotes). See <code><a href="terra.html#topic+global">global</a></code> for more details.</p>
</td></tr>
<tr><td><code id="globalx_+3A_na.rm">na.rm</code></td>
<td>
<p>If <code>TRUE</code> (default), then the function in <code>fun</code> will ignore <code>NA</code> cells.</p>
</td></tr>
<tr><td><code id="globalx_+3A_...">...</code></td>
<td>
<p>Additional arguments to pass to <code>fun</code>.</p>
</td></tr>
<tr><td><code id="globalx_+3A_weights">weights</code></td>
<td>
<p>Either <code>NULL</code> or a <code>SpatRaster</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector, one value per layer in <code>x</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(terra)

r &lt;- rast(ncols=10, nrows=10)
values(r) &lt;- 1:ncell(r)


global(r, 'sum') # terra
globalx(r, 'sum') # enmSdmX

global(r, "mean", na.rm=TRUE)[1, 1] # terra... same as enmSdmX::globalx

</code></pre>

<hr>
<h2 id='interpolateRasts'>Interpolate values from a series of rasters</h2><span id='topic+interpolateRasts'></span>

<h3>Description</h3>

<p>This function returns a series of rasters interpolated from another series of rasters. For example, the input might represent rasters of a process measured at times t, t + 1, and t + 4. The rasters at t + 2 and t + 3 could be interpolated based on the values in the other rasters. Note that this function can take a lot of time and memory, even for relatively small rasters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>interpolateRasts(
  rasts,
  interpFrom,
  interpTo,
  type = "linear",
  onFail = NA,
  useRasts = FALSE,
  na.rm = TRUE,
  verbose = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="interpolateRasts_+3A_rasts">rasts</code></td>
<td>
<p>A &quot;stack&quot; of <code>SpatRasters</code>s.</p>
</td></tr>
<tr><td><code id="interpolateRasts_+3A_interpfrom">interpFrom</code></td>
<td>
<p>Numeric vector, one value per raster in <code>rasts</code>. Values represent &quot;distance&quot; along the set of rasters rasters (e.g., time).</p>
</td></tr>
<tr><td><code id="interpolateRasts_+3A_interpto">interpTo</code></td>
<td>
<p>Numeric vector, values of &quot;distances&quot; at which to interpolate the rasters.</p>
</td></tr>
<tr><td><code id="interpolateRasts_+3A_type">type</code></td>
<td>
<p>Character. The type of model used to do the interpolation. Note that some of these (the first few) are guaranteed to go through every point being interpolated from. The second set, however, are effectively regressions so are not guaranteed to do through <em>any</em> of the points. Note that some methods cannot handle cases where at least some series of cells have &lt; a given number of non-<code>NA</code> values (e.g., smooth splines will not work if there are &lt; 4 cells with non-<code>NA</code> values).
</p>

<ul>
<li> <p><code>linear</code>: A model based on linear segments &quot;fastened&quot; at each value of <code>interpFrom</code>. The segments will intersect each value being interpolated from.
</p>
</li>
<li> <p><code>spline</code>: A natural splines-based model. Splines will intersect each value being interpolated from.
</p>
</li>
<li> <p><code>gam</code>: A generalized additive model. Note that the GAM is <em>not</em> guaranteed to intersect each value being interpolated from. Arguments to <code><a href="mgcv.html#topic+gam">gam</a></code> can be supplied via <code>...</code>. Especially note the <code>family</code> argument! You can use the <code>onFail</code> argument with this method since in some cases <code><a href="mgcv.html#topic+gam">gam</a></code> if there are too few data points.
</p>
</li>
<li> <p><code>glm</code>: A generalized linear model. Note that the GLM is <em>not</em> guaranteed to intersect each value being interpolated from. Arguments to <code><a href="mgcv.html#topic+gam">gam</a></code> can be supplied via <code>...</code>. Especially note the <code>family</code> argument (the main reason for why you would use a GLM versus just linear interpolation)! You can use the <code>onFail</code> argument with this method since in some cases <code><a href="stats.html#topic+glm">glm</a></code> if there are too few data points.
</p>
</li>
<li> <p><code>ns</code>: A natural splines model. Note that the NS is <em>not</em> guaranteed to intersect each value being interpolated from. Arguments to <code><a href="#topic+trainNS">trainNS</a></code> can be supplied via <code>...</code>. Especially note the <code>family</code> argument and the <code>df</code> argument! If <code>df</code> is not supplied, then the number of splines attempted will be equal to <code>1:(length(interpFrom) - 1)</code>. You can use the <code>onFail</code> argument with this method.
</p>
</li>
<li> <p><code>poly</code>: A polynomial model. This method constructs an <em>n</em>-degree polynomial where <em>n</em> = <code>length(interpFrom) - 1</code>. The most parsimonious model is then selected from all possible subsets of models (including an intercept-only model) using AICc. This method is <em>not</em> guaranteed to intersect each value being interpolated from. Arguments to <code><a href="stats.html#topic+glm">glm</a></code> can be supplied via <code>...</code>. Especially note the <code>family</code> argument! If <code>family</code> is not supplied, then the response is assumed to have a Gaussian distribution. You can use the <code>onFail</code> argument with this method.
</p>
</li>
<li> <p><code>bs</code>: A basis-spline model. This method constructs a series of models with <em>n</em>-degree basis-spline model where <em>n</em> ranges from 3 to <code>length(interpFrom) - 1</code>. The most parsimonious model is then selected from all possible subsets of models (including an intercept-only model) using AICc. This method is <em>not</em> guaranteed to intersect each value being interpolated from. Arguments to <code><a href="stats.html#topic+glm">glm</a></code> can be supplied via <code>...</code>. Especially note the <code>family</code> argument! If <code>family</code> is not supplied, then the response is assumed to have a Gaussian distribution. You can use the <code>onFail</code> argument with this method.
</p>
</li>
<li> <p><code>smooth.spline</code>: A smooth-spline model (see <code><a href="stats.html#topic+smooth.spline">smooth.spline</a></code>). This method is <em>not</em> guaranteed to intersect each value being interpolated from. Arguments to <code><a href="stats.html#topic+smooth.spline">smooth.spline</a></code> can be supplied via <code>...</code>. Unlike some other methods, a <code>family</code> cannot be specified (Gaussian is assumed)! You can use the <code>onFail</code> argument with this method.
</p>
</li></ul>
</td></tr>
<tr><td><code id="interpolateRasts_+3A_onfail">onFail</code></td>
<td>
<p>Either <code>NA</code> (default) or any one of <code>'linear'</code>, <code>'spline'</code>, or <code>'poly'</code>. If a method specified by <code>type</code> fails (i.e., because there are fewer than the required number of values to interpolate from), this method is used in its place. If this is <code>NA</code> and the method fails, then an error occurs.</p>
</td></tr>
<tr><td><code id="interpolateRasts_+3A_userasts">useRasts</code></td>
<td>
<p>Logical. If <code>FALSE</code> (default), then the calculations are done using arrays. This can be substantially faster than using rasters (when <code>useRasts = TRUE</code>), but also run into memory issues.</p>
</td></tr>
<tr><td><code id="interpolateRasts_+3A_na.rm">na.rm</code></td>
<td>
<p>Logical, if <code>TRUE</code> (default) then ignore cases where all values in the same cells across rasters from which interpolations are made are <code>NA</code> (i.e., do not throw an error). If <code>FALSE</code>, then throw an error when this occurs.</p>
</td></tr>
<tr><td><code id="interpolateRasts_+3A_verbose">verbose</code></td>
<td>
<p>Logical. If <code>TRUE</code> (default), display progress.</p>
</td></tr>
<tr><td><code id="interpolateRasts_+3A_...">...</code></td>
<td>
<p>Other arguments passed to <code>approx</code> or <code>spline</code> (<em>do not</em> include any of these arguments: <code>x</code>, <code>y</code>, or <code>xout</code>), or to <code><a href="stats.html#topic+glm">glm</a></code>, <code><a href="mgcv.html#topic+gam">gam</a></code>, or <code><a href="stats.html#topic+smooth.spline">smooth.spline</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function can be very memory-intensive for large rasters.  It may speed things up (and make them possible) to do interpolations piece by piece (e.g., instead of interpolating between times t0, t1, t2, t3, ..., interpolate between t0 and t1, then t1 and t2, etc. This may give results that differ from using the entire set, however, depending on what type of interpolation is used. Note that using linear and splines will often yield very similar results, except that in a small number of cases splines may produce very extreme interpolated values.
</p>


<h3>Value</h3>

<p>A <code>SpatRaster</code> &quot;stack&quot; with one layer per element in <code>interpTo</code>.
</p>


<h3>See Also</h3>

<p><code><a href="terra.html#topic+approximate">approximate</a></code>, <code><a href="stats.html#topic+approxfun">approxfun</a></code>, <code><a href="stats.html#topic+splinefun">splinefun</a></code>, <code><a href="#topic+trainNS">trainNS</a></code>, <code><a href="stats.html#topic+glm">glm</a></code>, , <code><a href="splines.html#topic+bs">bs</a></code>, <code><a href="stats.html#topic+smooth.spline">smooth.spline</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# NB: The example below can take a few minutes to run.

library(terra)

interpFrom &lt;- c(1, 3, 4, 8, 10, 11, 15)
interpTo &lt;- 1:15
rx &lt;- rast(nrows=10, ncols=10)
r1 &lt;- setValues(rx, rnorm(100, 1))
r3 &lt;- setValues(rx, rnorm(100, 3))
r4 &lt;- setValues(rx, rnorm(100, 5))
r8 &lt;- setValues(rx, rnorm(100, 11))
r10 &lt;- setValues(rx, rnorm(100, 3))
r11 &lt;- setValues(rx, rnorm(100, 5))
r15 &lt;- setValues(rx, rnorm(100, 13))
rasts &lt;- c(r1, r3, r4, r8, r10, r11, r15)
names(rasts) &lt;- paste0('rasts', interpFrom)

linear &lt;- interpolateRasts(rasts, interpFrom, interpTo)
spline &lt;- interpolateRasts(rasts, interpFrom, interpTo, type='spline')
gam &lt;- interpolateRasts(rasts, interpFrom, interpTo, type='gam', onFail='linear')
ns &lt;- interpolateRasts(rasts, interpFrom, interpTo, type='ns', onFail='linear', verbose=FALSE)
poly &lt;- interpolateRasts(rasts, interpFrom, interpTo, type='poly', onFail='linear')
bs &lt;- interpolateRasts(rasts, interpFrom, interpTo, type='bs', onFail='linear')
ss &lt;- interpolateRasts(rasts, interpFrom, interpTo, type='smooth.spline', onFail='linear',
verbose=FALSE)

# examine trends for a particular point on the landscape
pts &lt;- matrix(c(-9, 13), ncol = 2)
pts &lt;- vect(pts)
linearExt &lt;- unlist(terra::extract(linear, pts, ID=FALSE))
splineExt &lt;- unlist(terra::extract(spline, pts, ID=FALSE))
gamExt &lt;- unlist(terra::extract(gam, pts, ID=FALSE))
nsExt &lt;- unlist(terra::extract(ns, pts, ID=FALSE))
polyExt &lt;- unlist(terra::extract(poly, pts, ID=FALSE))
bsExt &lt;- unlist(terra::extract(bs, pts, ID=FALSE))
ssExt &lt;- unlist(terra::extract(ss, pts, ID=FALSE))

mins &lt;- min(linearExt, splineExt, gamExt, nsExt, polyExt, bsExt, ssExt)
maxs &lt;- max(linearExt, splineExt, gamExt, nsExt, polyExt, bsExt, ssExt)

plot(interpTo, linearExt, type='l', lwd=2, ylim=c(mins, maxs), ylab='Value')
lines(interpTo, splineExt, col='blue')
lines(interpTo, gamExt, col='green')
lines(interpTo, nsExt, col='orange')
lines(interpTo, polyExt, col='gray')
lines(interpTo, bsExt, col='magenta')
lines(interpTo, ssExt, col='cyan')

ext &lt;- unlist(extract(rasts, pts, ID = FALSE))
points(interpFrom, ext)

legend('topleft', inset=0.01, lty=c(rep(1, 7), NA),
legend=c('linear', 'spline', 'GAM', 'NS', 'polynomial', 'B-spline',
'Smooth spline', 'Observed'), col=c('black', 'blue', 'green',
'orange', 'gray', 'magenta', 'cyan'), pch=c(rep(NA, 7), 1))



</code></pre>

<hr>
<h2 id='lemurs'>Lemur occurrences from GBIF</h2><span id='topic+lemurs'></span>

<h3>Description</h3>

<p>Data frame of lemur occurrences
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(lemurs)
</code></pre>


<h3>Format</h3>

<p>An object of class <code>'data.frame'</code>.
</p>


<h3>Source</h3>

<p><a href="https://gbif.org">GBIF</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(lemurs)
lemurs
</code></pre>

<hr>
<h2 id='longLatRasts'>Generate rasters with cell values equal to cell longitude or latitude</h2><span id='topic+longLatRasts'></span>

<h3>Description</h3>

<p>This function generates a raster stack with two rasters, one with cell values equal to the cell's longitude and the other with cell values equal to the cell's latitude.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>longLatRasts(x, m = TRUE, filePath = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="longLatRasts_+3A_x">x</code></td>
<td>
<p><code>SpatRaster</code> object. The output will have the same resolution, extent, and coordinate projection system as <code>x</code>.</p>
</td></tr>
<tr><td><code id="longLatRasts_+3A_m">m</code></td>
<td>
<p>Any of:
</p>

<ul>
<li> <p><code>TRUE</code> (default): Calculate longitude and latitude only for cells that are not <code>NA</code>.
</p>
</li>
<li> <p><code>FALSE</code>: Calculate longitude and latitude for all cells.
</p>
</li>
<li><p> A <code>SpatRaster</code> object: Force any cells that are <code>NA</code> in this raster to also be <code>NA</code> in the output.
</p>
</li></ul>
</td></tr>
<tr><td><code id="longLatRasts_+3A_filepath">filePath</code></td>
<td>
<p>String or <code>NULL</code>. If a string, then this is the path (not including file name) to which to write the raster stack with longitude/latitude rasters. If <code>NULL</code> then no file is written.</p>
</td></tr>
<tr><td><code id="longLatRasts_+3A_...">...</code></td>
<td>
<p>Arguments to pass to <code>writeRaster</code> (if <code>filePath</code> is not <code>NULL</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class <code>SpatRaster</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(terra)

# generate long/lat rasters for the world
x &lt;- rast() # raster with 1 deg resolution and extent equal to entire world
x[] &lt;- 1:ncell(x)
longLat &lt;- longLatRasts(x)
plot(longLat)

# demonstrate masking
# randomly force some cells to NA
v &lt;- 1:ncell(x)
n &lt;- 10000
v[sample(v, n)] &lt;- NA
x[] &lt;- v
longLatTRUE &lt;- longLatRasts(x, m = TRUE)
longLatFALSE &lt;- longLatRasts(x, m = FALSE)
rasts &lt;- c(x, longLatTRUE, x, longLatFALSE)
names(rasts) &lt;- c('x', 'long_m_TRUE', 'lat_m_TRUE',
	'x', 'long_m_FALSE', 'lat_m_FALSE')
plot(rasts)

</code></pre>

<hr>
<h2 id='mad0'>Madagascar spatial object</h2><span id='topic+mad0'></span>

<h3>Description</h3>

<p>Outline of Madagascar from GADM.  The geometry has been simplified from the version available in GADM, so pleased do not use this for &quot;official&quot; analyses.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(mad0, package='enmSdmX')
</code></pre>


<h3>Format</h3>

<p>An object of class <code>sf</code>.
</p>


<h3>Source</h3>

<p><a href="https://gadm.org">GADM</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(sf)
data(mad0)
mad0
plot(st_geometry(mad0), main='Madagascar')

</code></pre>

<hr>
<h2 id='mad1'>Madagascar spatial object</h2><span id='topic+mad1'></span>

<h3>Description</h3>

<p>Outlines of regions (&quot;Faritra&quot;) of Madagascar from GADM. The geometry has been simplified from the version available in GADM, so pleased do not use this for &quot;official&quot; analyses.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(mad1, package='enmSdmX')
</code></pre>


<h3>Format</h3>

<p>An object of class <code>sf</code>.
</p>


<h3>Source</h3>

<p><a href="https://gadm.org">GADM</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(sf)
data(mad1)
mad1
plot(st_geometry(mad1), main='Madagascar')

</code></pre>

<hr>
<h2 id='madClim'>Present-day climate rasters for Madagascar</h2><span id='topic+madClim'></span>

<h3>Description</h3>

<p>Rasters representing average climate across 1970-2000 for Madagascar from WorldClim version 2.1. Values of these rasters have been rounded to one digit, so <em>please do not use these for &quot;official&quot; work</em>. Please also note that CanESM5 in CMIP6 is known to run &quot;too hot&quot;, but is useful here to aid illustration.
</p>


<h3>Format</h3>

<p>An object of class <code>'SpatRaster'</code>.
</p>


<h3>Source</h3>

<p><a href="https://worldclim.org">WorldClim</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(terra)
rastFile &lt;- system.file('extdata', 'madClim.tif', package='enmSdmX')
madClim &lt;- rast(rastFile)
plot(madClim)

</code></pre>

<hr>
<h2 id='madClim2030'>Future climate rasters for Madagascar</h2><span id='topic+madClim2030'></span>

<h3>Description</h3>

<p>Rasters representing average climate across 2021-2040 modeled with CanESM5 for SSP 585 for Madagascar from WorldClim version 2.1. Values of these rasters have been rounded to one digit, so <em>please do not use these for &quot;official&quot; work</em>. Please also note that CanESM5 in CMIP6 is known to run &quot;too hot&quot;, but is useful here to aid illustration.
</p>


<h3>Format</h3>

<p>An object of class <code>'SpatRaster'</code>.
</p>


<h3>Source</h3>

<p><a href="https://worldclim.org">WorldClim</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(terra)
rastFile &lt;- system.file('extdata', 'madClim2030.tif', package='enmSdmX')
madClimFut &lt;- rast(rastFile)
plot(madClimFut)

</code></pre>

<hr>
<h2 id='madClim2050'>Future climate rasters for Madagascar</h2><span id='topic+madClim2050'></span>

<h3>Description</h3>

<p>Rasters representing average climate across 2041-2060 modeled with CanESM5 for SSP 585 for Madagascar from WorldClim version 2.1. Values of these rasters have been rounded to one digit, so <em>please do not use these for &quot;official&quot; work</em>. Please also note that CanESM5 in CMIP6 is known to run &quot;too hot&quot;, but is useful here to aid illustration.
</p>


<h3>Format</h3>

<p>An object of class <code>'SpatRaster'</code>.
</p>


<h3>Source</h3>

<p><a href="https://worldclim.org">WorldClim</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(terra)
rastFile &lt;- system.file('extdata', 'madClim2050.tif', package='enmSdmX')
madClimFut &lt;- rast(rastFile)
plot(madClimFut)

</code></pre>

<hr>
<h2 id='madClim2070'>Future climate rasters for Madagascar</h2><span id='topic+madClim2070'></span>

<h3>Description</h3>

<p>Rasters representing average climate across 2061-2080 modeled with CanESM5 for SSP 585 for Madagascar from WorldClim version 2.1. Values of these rasters have been rounded to one digit, so <em>please do not use these for &quot;official&quot; work</em>. Please also note that CanESM5 in CMIP6 is known to run &quot;too hot&quot;, but is useful here to aid illustration.
</p>


<h3>Format</h3>

<p>An object of class <code>'SpatRaster'</code>.
</p>


<h3>Source</h3>

<p><a href="https://worldclim.org">WorldClim</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(terra)
rastFile &lt;- system.file('extdata', 'madClim2070.tif', package='enmSdmX')
madClimFut &lt;- rast(rastFile)
plot(madClimFut)

</code></pre>

<hr>
<h2 id='madClim2090'>Future climate rasters for Madagascar</h2><span id='topic+madClim2090'></span>

<h3>Description</h3>

<p>Rasters representing average climate across 2081-2100 modeled with CanESM5 for SSP 585 for Madagascar from WorldClim version 2.1. Values of these rasters have been rounded to one digit, so <em>please do not use these for &quot;official&quot; work</em>. Please also note that CanESM5 in CMIP6 is known to run &quot;too hot&quot;, but is useful here to aid illustration.
</p>


<h3>Format</h3>

<p>An object of class <code>'SpatRaster'</code>.
</p>


<h3>Source</h3>

<p><a href="https://worldclim.org">WorldClim</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(terra)
rastFile &lt;- system.file('extdata', 'madClim2090.tif', package='enmSdmX')
madClimFut &lt;- rast(rastFile)
plot(madClimFut)

</code></pre>

<hr>
<h2 id='modelSize'>Number of response data in a model object</h2><span id='topic+modelSize'></span>

<h3>Description</h3>

<p>This function returns the number of response data used in a model (i.e., the sample size). If the data are binary it can return the number of 1s and 0s.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>modelSize(x, binary = TRUE, graceful = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="modelSize_+3A_x">x</code></td>
<td>
<p>A model object. This can be of many classes, including &quot;gbm&quot;, &quot;glm&quot;, &quot;gam&quot;, &quot;MaxEnt&quot;, and so on.</p>
</td></tr>
<tr><td><code id="modelSize_+3A_binary">binary</code></td>
<td>
<p>If <code>TRUE</code> (default) then the number of 1s and 0s in the response data is returned. If <code>FALSE</code> then the returned values is the total number of response data.</p>
</td></tr>
<tr><td><code id="modelSize_+3A_graceful">graceful</code></td>
<td>
<p>If <code>TRUE</code> (default), then the function returns <code>NA</code> if the function cannot determine the sample size from the model object. If <code>FALSE</code>, then the function exits with an error.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>One or two named integers.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(123)
y &lt;- runif(1:101)^2
yBinary &lt;- as.integer(y &gt; 0.6)
x &lt;- data.frame(x1=1:101, x2=rnorm(101))
model &lt;- lm(y ~ x1 + x2, data=x)
modelBinary &lt;- glm(yBinary ~ x1 + x2, data=x, family='binomial')
modelSize(model, FALSE)
modelSize(model, TRUE) # not binary input... notice warning
modelSize(modelBinary)
modelSize(modelBinary, FALSE)

</code></pre>

<hr>
<h2 id='nearestEnvPoints'>Extract &quot;most conservative&quot; environments from points and/or polygons</h2><span id='topic+nearestEnvPoints'></span>

<h3>Description</h3>

<p>This function implements the &quot;nearest environmental point&quot; method (Smith et al. 2023) to enable the use of occurrence records geolocated only to a general place (e.g., a country or province), along with occurrences georeferenced with little error.  The function returns environments from a set of precisely-geolocated points plus the environment associated with each imprecise record.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nearestEnvPoints(
  rasts,
  pts = NULL,
  polys = NULL,
  centerFrom = "pts",
  pca = TRUE,
  numPcs = 3,
  center = TRUE,
  scale = TRUE,
  rule = "nearest",
  na.rm = TRUE,
  out = "both"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nearestEnvPoints_+3A_rasts">rasts</code></td>
<td>
<p>A <code>SpatRaster</code> or &quot;stack&quot; of <code>SpatRaster</code>s. Please also see argument <code>pca</code>.</p>
</td></tr>
<tr><td><code id="nearestEnvPoints_+3A_pts">pts</code></td>
<td>
<p>A set of spatial points of class <code>SpatVector</code> or <code>sf</code>.</p>
</td></tr>
<tr><td><code id="nearestEnvPoints_+3A_polys">polys</code></td>
<td>
<p>A set of spatial polygons of class <code>SpatVector</code> or <code>sf</code>.</p>
</td></tr>
<tr><td><code id="nearestEnvPoints_+3A_centerfrom">centerFrom</code></td>
<td>
<p>Indicates how to locate the &quot;reference&quot; centroid used to identify single points on each polygon. This is only relevant if both <code>pts</code> and <code>polys</code> are specified.
</p>

<ul>
<li> <p><code>'pts'</code>: The default is to use the environmental centroid of <code>pts</code>, which finds the centroid of <code>pts</code>, then finds the location on the border of each polygon closest to this centroid.
</p>
</li>
<li> <p><code>'polys'</code>: This option will first calculate the environmental centroid of each polygon, then the centroid of these points, and then find the location on the border of each polygon closest to this point.
</p>
</li>
<li> <p><code>'both'</code>: This option first calculates the environmental centroid of each polygon, then finds the joint centroid of these points plus of <code>pts</code>, and lastly locates on the border of each polygon the point closest to this grand centroid.
</p>
</li></ul>
</td></tr>
<tr><td><code id="nearestEnvPoints_+3A_pca">pca</code></td>
<td>
<p>If <code>TRUE</code> (default) and there is more than one raster specified in <code>rasts</code>, then a principal components analysis (PCA) is applied to the values of the rasters before finding the closest points. The returned values are those of the original rasters and the PC scores.</p>
</td></tr>
<tr><td><code id="nearestEnvPoints_+3A_numpcs">numPcs</code></td>
<td>
<p>The number of PC axes used to find environmental centroids. This is only used if <code>pca</code> is <code>TRUE</code>. By default, all axes are used.</p>
</td></tr>
<tr><td><code id="nearestEnvPoints_+3A_center">center</code>, <code id="nearestEnvPoints_+3A_scale">scale</code></td>
<td>
<p>Settings for <code><a href="stats.html#topic+prcomp">prcomp</a></code>. These indicate if, when calculating the PCA, variables should first be centered and scaled (both <code>TRUE</code> by default). If the values in <code>rasts</code> are not of the same units, this should almost always be <code>TRUE</code>. They are ignored if <code>pca</code> is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="nearestEnvPoints_+3A_rule">rule</code></td>
<td>
<p>Determines how to identify the single environmental point to associate with each polygon. Options include:
</p>

<ul>
<li>    <p><code>'nearest'</code> (default): Returns the environmental point <em>closest</em> to the centroid (i.e., the &quot;nearest environmental point&quot;).
</p>
</li>
<li>    <p><code>'farthest'</code>: Returns the environmental point <em>farthest</em> from the centroid (i.e., opposite of the &quot;nearest&quot; point)
</p>
</li></ul>
</td></tr>
<tr><td><code id="nearestEnvPoints_+3A_na.rm">na.rm</code></td>
<td>
<p>If <code>TRUE</code> (default), ignore <code>NA</code>s when extracting from rasters (e.g., if a point or polygon falls onto an <code>NA</code> cell). If <code>FALSE</code>, then any <code>NA</code>s that overlap a point or polygon will result in an error.</p>
</td></tr>
<tr><td><code id="nearestEnvPoints_+3A_out">out</code></td>
<td>
<p>Determines what is returned. Only used if both <code>pts</code> and <code>polys</code> are provided.
</p>

<ul>
<li> <p><code>'both'</code> (default): Returns all environmental points. If <em>n</em> is the number of points in <code>pts</code> and <em>m</em> the number of polygons in <code>polys</code>, then the first <code>n</code> rows in the returned data frame refer to the environments of the <code>pts</code> and the subsequent <em>m</em> to each <code>poly</code>.
</p>
</li>
<li> <p><code>'pts'</code>: Returns the environmental values associated with each point.
</p>
</li>
<li> <p><code>'polys'</code>: Returns the environmental values on each <code>poly</code> polygon closest to the given center.
</p>
</li></ul>
</td></tr>
</table>


<h3>Details</h3>

<p>This function locates a set of points from the environments covered by each polygon using the following procedure, the details of which depend on what arguments are specified:
</p>

<ul>
<li><p> Only <code>pts</code> is specified: Environments are taken directly from the locations of <code>pts</code> in environmental space.
</p>
</li>
<li><p> Only <code>polys</code> is specified: Environments are taken from the closest environment of all the environments associated with each each polygon that is closest to the environmental centroid of the environmental centroids of the polygons (that may be confusing, but it is not a typo).
</p>
</li>
<li> <p><code>pts</code> and <code>polys</code> are specified: Environments are taken from the locations of <code>pts</code> plus the environment from each polygon closest to the environmental centroid of <code>pts</code>. By default, the function uses the environmental centroid of the precise occurrences in step (1), but this can be changed to the environmental centroid of the centroids of the polygons or the environmental centroid of the points defined by the union of precise occurrence points plus the environmental centroids of the polygons.
</p>
</li></ul>

<p>The function can alternatively return the points on the vertices of the MCP, or points on the input polygons closest to the reference centroid.
</p>


<h3>Value</h3>

<p>A data frame.
</p>


<h3>References</h3>

<p>Smith, A.B., Murphy, S.J., Henderson, D., and Erickson, K.D. 2023. Including imprecisely georeferenced specimens improves accuracy of species distribution models and estimates of niche breadth.  <em>Global Ecology and Biogeography</em> In press. Open access pre-print: <a href="https://doi.org/10.1101/2021.06.10.447988">doi:10.1101/2021.06.10.447988</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nearestGeogPoints">nearestGeogPoints</a></code> for the &quot;nearest geographic point&quot; method, a related approach for geographic space.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># This is a contrived example based on red-bellied lemurs in Madagascar.
# Point locations (which are real data) will be assumed to be "precise"
# records. We will designate a set of Faritas ("counties") to represent
# "imprecise" occurrences that can only be georeferenced to a geopolitical
# unit.

library(sf)
library(terra)

# coordinate reference system
wgs84 &lt;- getCRS('WGS84')

# lemur point data
data(lemurs)
precise &lt;- lemurs[lemurs$species == 'Eulemur rubriventer', ]
ll &lt;- c('longitude', 'latitude')
precise &lt;- sf::st_as_sf(precise[ , ll], coords=ll, crs=wgs84)

# *fake* lemur administrative unit-level data
faritras &lt;- c('Vakinankaratra', 'Haute matsiatra', 'Ihorombe',
'Vatovavy Fitovinany', 'Alaotra-Mangoro', 'Analanjirofo', 'Atsinanana',
'Analamanga', 'Itasy')

data(mad1)
imprecise &lt;- mad1[mad1$NAME_2 %in% faritras, ]

# climate predictors
rastFile &lt;- system.file('extdata/madClim.tif', package='enmSdmX')
rasts &lt;- rast(rastFile)

### Plot environment of points and environments of each polygon closest to
### centroid of environments of points. In this example, we use the first two
### principal component axes to characterize the niche.

# apply Nearest Environmental Point method
envPtsPolys &lt;- nearestEnvPoints(rasts, pts = precise, polys = imprecise,
	pca = TRUE,	numPcs = 2)
envPolys &lt;- nearestEnvPoints(rasts, pts = precise, polys = imprecise, numPcs = 2,
	out = 'polys')
envPts &lt;- nearestEnvPoints(rasts, pts = precise, polys = imprecise, numPcs = 2,
	out = 'pts')
	
allPolyEnvs &lt;- extract(rasts, imprecise)

# plot occurrences in environmental space
plot(envPtsPolys$PC1, envPtsPolys$PC2, pch=16, col='black',
	xlab='PC1', ylab='PC2')

points(envPolys$PC1, envPolys$PC2, pch=21, bg='orange')

legend(
	'bottomleft',
	inset = 0.01,
	legend = c('precise', 'imprecise (closest)'),
	pch = c(16, 21),
	col = c('black', 'black'),
	pt.bg = c('orange', 'orange')
)

### compare identified environments to all environments across all polygons
###########################################################################

env &lt;- as.data.frame(rasts)
pca &lt;- stats::prcomp(env, center=TRUE, scale.=TRUE)

allPolyEnvs &lt;- extract(rasts, imprecise, ID = FALSE)
allPolyEnvsPcs &lt;- predict(pca, allPolyEnvs)
allPolyEnvs &lt;- cbind(allPolyEnvs, allPolyEnvsPcs)

# plot in environmental space
plot(allPolyEnvs$PC1, allPolyEnvs$PC2, pch=16, col='orange',
	xlab='PC1', ylab='PC2')
points(envPts$PC1, envPts$PC2, pch=16)
points(envPolys$PC1, envPolys$PC2, pch=1)
legend(
	'bottomleft',
	inset = 0.01,
	legend = c('precise', 'imprecise (closest)', 'imprecise (all)'),
	pch = c(16, 21, 16),
	col = c('black', 'black', 'orange'),
	pt.bg = c(NA, 'orange')
)

### display niches (minimum convex hulls) estimated
### using just precise or precise + imprecise records
#####################################################

pcs &lt;- c('PC1', 'PC2')
preciseIndices &lt;- chull(envPts[ , pcs])
preciseImpreciseIndices &lt;- chull(envPtsPolys[ , pcs])

preciseIndices &lt;- c(preciseIndices, preciseIndices[1])
preciseImpreciseIndices &lt;- c(preciseImpreciseIndices,
	preciseImpreciseIndices[1])

preciseOnlyNiche &lt;- envPts[preciseIndices, pcs]
preciseImpreciseNiche &lt;- envPtsPolys[preciseImpreciseIndices, pcs]

# plot in environmental space
plot(allPolyEnvs$PC1, allPolyEnvs$PC2, pch=16, col='orange',
	xlab='PC1', ylab='PC2')
points(envPts$PC1, envPts$PC2, pch=16)
points(envPolys$PC1, envPolys$PC2, pch=1)
lines(preciseImpreciseNiche, col='coral4', lwd=2)
lines(preciseOnlyNiche, lty='dotted')

legend(
	'bottomleft',
	inset = 0.01,
	legend = c('precise', 'imprecise (closest)', 'imprecise (all)', 
		'MCP imprecise-only', 'MCP precise + imprecise'),
	pch = c(16, 21, 16, NA, NA),
	col = c('black', 'black', 'orange', 'black', 'coral4'),
	pt.bg = c(NA, 'orange', NA, NA, NA),
	lwd = c(NA, NA, NA, 1, 2),
	lty = c(NA, NA, NA, 'dotted', 'solid')
)
</code></pre>

<hr>
<h2 id='nearestGeogPoints'>Minimum convex polygon from a set of spatial polygons and/or points</h2><span id='topic+nearestGeogPoints'></span>

<h3>Description</h3>

<p>This function implements the &quot;nearest geographic point&quot; method (Smith et al. 2023) to enable the use of occurrence records geolocated only to a general place (e.g., a country or province), along with occurrences georeferenced with little error. The function returns a minimum convex polygon (MCP) constructed from a set of spatial polygons and/or points.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nearestGeogPoints(
  pts = NULL,
  polys = NULL,
  centerFrom = "pts",
  return = "mcp",
  terra = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nearestGeogPoints_+3A_pts">pts</code></td>
<td>
<p>Either <code>NULL</code> (default) or a set of spatial points. This can be either a <code>SpatVector</code> (<code>terra</code> package) or <code>POINTS</code> or <code>MULTIPOINTS</code> <code>sf</code> object (<code>sf</code> package). <em>These must be in an equal-area projection!</em> This can also be a <code>Spatial</code> object (e.g., <code>SpatialPoints</code> or <code>SpatialPointsDataFrame</code>) from the <span class="pkg">sp</span> package, but <em>the <code>sp</code> package will be deprecated in 2023</em>.</p>
</td></tr>
<tr><td><code id="nearestGeogPoints_+3A_polys">polys</code></td>
<td>
<p>Either <code>NULL</code> (default), or an object representing spatial polygons of (for example) counties in which a species is known to reside. <em>This must be in an equal-area projection!</em>. This object can be either a <code>SpatVector</code> (<span class="pkg">terra</span> package)), or <code>POLYGON</code>, <code>MULTIPOLYGON</code>, <code>LINESTRING</code>, or <code>MULTILINESTRING</code> <code>sf</code> object (<span class="pkg">sf</span> package). This can also be a <code>Spatial</code> object (e.g., <code>SpatialPolygons</code> or <code>SpatialPolygonsDataFrame</code>) from the <span class="pkg">sp</span> package, <em>the <code>sp</code> package will be deprecated in 2023</em>.</p>
</td></tr>
<tr><td><code id="nearestGeogPoints_+3A_centerfrom">centerFrom</code></td>
<td>
<p>Indicates how to locate the &quot;reference&quot; centroid used to identify points on each polygon. This is only relevant if both <code>pts</code> and <code>polys</code> are not <code>NULL</code>.
</p>

<ul>
<li> <p><code>'pts'</code>: The default is to use the centroid of <code>pts</code>, which finds the centroid of <code>pts</code>, then finds the location on the border of each polygon closest to this centroid.
</p>
</li>
<li> <p><code>'polys'</code>: This option will first calculate the centroid of each polygon, then the centroid of these points, and then find the location on the border of each polygon closest to this point.
</p>
</li>
<li> <p><code>'both'</code>: This option first calculates the centroid of each polygon, then finds the joint centroid of these points plus of <code>pts</code>, and lastly locates on the border of each polygon the point closest to this grand centroid.
</p>
</li></ul>
</td></tr>
<tr><td><code id="nearestGeogPoints_+3A_return">return</code></td>
<td>
<p>Determines what is returned:
</p>

<ul>
<li> <p><code>'mcp'</code> (default): The minimum convex polygon
</p>
</li>
<li> <p><code>'mcpPoints'</code>: Points of the vertices of the minimum convex polygon
</p>
</li>
<li> <p><code>'polyPoints'</code>: The point on each <code>poly</code> polygon closest to the given center
</p>
</li></ul>
</td></tr>
<tr><td><code id="nearestGeogPoints_+3A_terra">terra</code></td>
<td>
<p>If <code>TRUE</code> (default), the return an object of class <code>SpatVector</code>. Otherwise, return an object of class <code>sf</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function constructs a minimum convex polygon (MCP) from a set of spatial points and/or spatial polygons. The manner in which this is done depends on whether <code>polys</code> and/or <code>pts</code> are specified:
</p>

<ul>
<li><p> Only <code>pts</code> is supplied: The MCP is constructed directly from the points.
</p>
</li>
<li><p> Only <code>polys</code> is supplied: The MCP is constructed from the point on each polygon closest to the centroid of the centroids of the polygons.
</p>
</li>
<li><p> Both <code>pts</code> and <code>polys</code> are supplied: The MCP is constructed from the combined set of <code>pts</code> <em>and</em> from the point on each polygon closest to the centroid of <code>pts</code>. By default, the function uses the centroid of the precise occurrences in step (1), but this can be changed to the centroid of the centroids of the polygons or the centroid of the points defined by the union of precise occurrence points plus the centroids of the polygons.
</p>
</li></ul>

<p>The function can alternatively return the points on the vertices of the MCP, or points on the input polygons closest to the reference centroid.
</p>


<h3>Value</h3>

<p><code>SpatVector</code>, or <code>sf POLYGON</code> representing a minimum convex polygon.
</p>


<h3>References</h3>

<p>Smith, A.B., Murphy, S.J., Henderson, D., and Erickson, K.D. 2023. Including imprecisely georeferenced specimens improves accuracy of species distribution models and estimates of niche breadth.  <em>Global Ecology and Biogeography</em> 32:342-355. <a href="https://doi.org/10.1111/geb.13628">doi:10.1111/geb.13628</a> Open access pre-print: <a href="https://doi.org/10.1101/2021.06.10.447988">doi:10.1101/2021.06.10.447988</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nearestEnvPoints">nearestEnvPoints</a></code> for the &quot;nearest environmental point&quot; method, a related application for estimating niche breadth in environmental space.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(sf)
library(terra)

#######################################################
### example using SpatVector inputs (terra package) ###
#######################################################

### prepare data
################

# Get coordinate reference systems:
# * WGS84
# * Tananarive (Paris) / Laborde Grid - EPSG:29701
wgs84 &lt;- getCRS('WGS84')
madProj &lt;- getCRS('Madagascar Albers')

# outline of Madagascar faritras
data(mad1)
mad1 &lt;- vect(mad1)
mad1 &lt;- project(mad1, madProj)

# lemur point data
data(lemurs)
redBelly &lt;- lemurs[lemurs$species == 'Eulemur rubriventer', ]
ll &lt;- c('longitude', 'latitude')
redBelly &lt;- vect(redBelly, geom=ll, crs=wgs84)
redBelly &lt;- project(redBelly, madProj)

# *fake* lemur farita-level data
faritras &lt;- c('Toamasina', 'Atsimo-Atsinana',
'Amoron\'i mania', 'Sava', 'Itasy')
polys &lt;- mad1[mad1$NAME_2 %in% faritras, ]

### apply Nearest Geographic Point method
#########################################

# get three kinds of minimum convex polygons (MCPs):

# MCP using just polygons
mcpPolys &lt;- nearestGeogPoints(polys = polys)

# MCP using just points
mcpPts &lt;- nearestGeogPoints(pts = redBelly)

# MCP using points &amp; polys
mcpPolysPoints &lt;- nearestGeogPoints(pts = redBelly, polys = polys)

# compare extent of occurrence (EOO) in m2
expanse(mcpPolys)
expanse(mcpPts)
expanse(mcpPolysPoints)

### plot minimum convex polygons
################################

# MCP from precise occurrences only
plot(mad1, border='gray', main='MCP points only')
plot(polys, col='gray80', add=TRUE)
plot(mcpPts, col=scales::alpha('red', 0.4), add=TRUE)
plot(redBelly, pch=21, bg='red', add=TRUE)

legend('topleft', 
legend=c('Precise occurrence', 'Imprecise occurrence', 'MCP'),
fill=c(NA, 'gray', scales::alpha('red', 0.4)),
pch=c(21, NA, NA),
pt.bg=c('red', NA, NA),
border=c(NA, 'black', 'black'))

# MCP from imprecise occurrences only
plot(mad1, border='gray', main='MCP polys only')
plot(polys, col='gray80', add=TRUE)
plot(mcpPolys, col=scales::alpha('orange', 0.4), add=TRUE)
plot(redBelly, pch=21, bg='red', add=TRUE)

legend('topleft', 
legend=c('Precise occurrence', 'Imprecise occurrence', 'MCP'),
fill=c(NA, 'gray', scales::alpha('orange', 0.4)),
pch=c(21, NA, NA),
pt.bg=c('red', NA, NA),
border=c(NA, 'black', 'black'))

# MCP from precise and imprecise occurrences
plot(mad1, border='gray', main='MCP polys + points')
plot(polys, col='gray80', add=TRUE)
plot(mcpPolysPoints, col=scales::alpha('green', 0.4), add=TRUE)
plot(redBelly, pch=21, bg='red', add=TRUE)

legend('topleft', 
legend=c('Precise occurrence', 'Imprecise occurrence', 'MCP'),
fill=c(NA, 'gray', scales::alpha('green', 0.4)),
pch=c(21, NA, NA),
pt.bg=c('red', NA, NA),
border=c(NA, 'black', 'black'))

############################################
### example using sf inputs (sf package) ###
############################################

### prepare data
################

# Get coordinate reference systems:
# * WGS84
# * Tananarive (Paris) / Laborde Grid - EPSG:29701
madProj &lt;- sf::st_crs(getCRS('Madagascar Albers'))
wgs84 &lt;- getCRS('WGS84')

# outline of Madagascar faritras
data(mad1)
mad1 &lt;- sf::st_transform(mad1, madProj)

# lemur point occurrence data
data(lemurs)
redBelly &lt;- lemurs[lemurs$species == 'Eulemur rubriventer', ]
ll &lt;- c('longitude', 'latitude')
redBelly &lt;- sf::st_as_sf(redBelly[ , ll], crs=wgs84, coords=ll)
redBelly &lt;- sf::st_transform(redBelly, madProj)

# *fake* farita-level occurrences
faritras &lt;- c('Toamasina', 'Atsimo-Atsinana',
'Amoron\'i mania', 'Sava', 'Itasy')
polys &lt;- mad1[mad1$NAME_2 %in% faritras, ]

### apply Nearest Geographic Point method
#########################################

# get three kinds of minimum convex polygons (MCPs):

# MCP using just polygons
mcpPolys &lt;- nearestGeogPoints(polys = polys, terra = FALSE)

# MCP using just points
mcpPts &lt;- nearestGeogPoints(pts = redBelly, terra = FALSE)

# MCP using points &amp; polys
mcpPolysPoints &lt;- nearestGeogPoints(pts = redBelly, polys = polys,
terra = FALSE)

# extent of occurrence (EOO) in m2
sf::st_area(mcpPolys)
sf::st_area(mcpPts)
sf::st_area(mcpPolysPoints)

### plot minimum convex polygons
################################

# MCP from precise occurrences only
plot(st_geometry(mad1), border='gray', main='MCP points only')
plot(st_geometry(polys), col='gray80', add=TRUE)
plot(st_geometry(mcpPts), col=scales::alpha('red', 0.4), add=TRUE)
plot(st_geometry(redBelly), pch=21, bg='red', add=TRUE)

legend('topleft', 
legend=c('Precise occurrence', 'Imprecise occurrence', 'MCP'),
fill=c(NA, 'gray', scales::alpha('red', 0.4)),
pch=c(21, NA, NA),
pt.bg=c('red', NA, NA),
border=c(NA, 'black', 'black'))

# MCP from imprecise occurrences only
plot(st_geometry(mad1), border='gray', main='MCP points only')
plot(st_geometry(polys), col='gray80', add=TRUE)
plot(st_geometry(mcpPolys), col=scales::alpha('orange', 0.4), add=TRUE)
plot(st_geometry(redBelly), pch=21, bg='red', add=TRUE)

legend('topleft', 
legend=c('Precise occurrence', 'Imprecise occurrence', 'MCP'),
fill=c(NA, 'gray', scales::alpha('orange', 0.4)),
pch=c(21, NA, NA),
pt.bg=c('red', NA, NA),
border=c(NA, 'black', 'black'))

# MCP from precise and imprecise occurrences
plot(st_geometry(mad1), border='gray', main='MCP points only')
plot(st_geometry(polys), col='gray80', add=TRUE)
plot(st_geometry(mcpPolysPoints), col=scales::alpha('green', 0.4), add=TRUE)
plot(st_geometry(redBelly), pch=21, bg='red', add=TRUE)

legend('topleft', 
legend=c('Precise occurrence', 'Imprecise occurrence', 'MCP'),
fill=c(NA, 'gray', scales::alpha('green', 0.4)),
pch=c(21, NA, NA),
pt.bg=c('red', NA, NA),
border=c(NA, 'black', 'black'))

### NOTE
# Using SpatVector input (terra package) yields EOOs that are slightly
# larger than using Spatial* (sp) or sf (sf) objects (by about 0.03-0.07%
# in this example). The difference arises because terra::expanse() yields a
# different value than sf::st_area.
</code></pre>

<hr>
<h2 id='nicheOverlapMetrics'>Metrics of niche overlap</h2><span id='topic+nicheOverlapMetrics'></span>

<h3>Description</h3>

<p>This function calculates several metrics of niche overlap based on predictions for two species (or for the same species but different models) at the same sites.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nicheOverlapMetrics(
  x1,
  x2,
  method = c("meanDiff", "meanAbsDiff", "rmsd", "d", "i", "esp", "cor", "rankCor"),
  w = rep(1, length(x1)),
  na.rm = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nicheOverlapMetrics_+3A_x1">x1</code></td>
<td>
<p>Numeric. Vector of predictions from a model.</p>
</td></tr>
<tr><td><code id="nicheOverlapMetrics_+3A_x2">x2</code></td>
<td>
<p>Numeric. Vector of predictions from another model.</p>
</td></tr>
<tr><td><code id="nicheOverlapMetrics_+3A_method">method</code></td>
<td>
<p>Character vector, indicates type of metric to calculate:
</p>

<ul>
<li> <p><code>meanDiff</code>: Average difference
</p>
</li>
<li> <p><code>meanAbsDiff</code>: Average of absolute values of difference
</p>
</li>
<li> <p><code>rmsd</code>: Root-mean square deviation
</p>
</li>
<li> <p><code>d</code>: Schoener's <em>D</em>
</p>
</li>
<li> <p><code>i</code>: Warren's <em>I</em>
</p>
</li>
<li> <p><code>esp</code>: Godsoe's <em>ESP</em>
</p>
</li>
<li> <p><code>cor</code>: Pearson correlation between <code>x1</code> and <code>x2</code> (will apply <code>logitAdj()</code> first unless logit=FALSE).
</p>
</li>
<li> <p><code>rankCor</code>: Spearman rank correlation.
</p>
</li></ul>
</td></tr>
<tr><td><code id="nicheOverlapMetrics_+3A_w">w</code></td>
<td>
<p>Numeric vector. Weights of predictions in <code>x1</code> and <code>x2</code>.</p>
</td></tr>
<tr><td><code id="nicheOverlapMetrics_+3A_na.rm">na.rm</code></td>
<td>
<p>Logical.  If T<code>TRUE</code> then remove elements in <code>x1</code> and <code>2</code> that are <code>NA</code> in <em>either</em> <code>x1</code> or <code>x2</code>.</p>
</td></tr>
<tr><td><code id="nicheOverlapMetrics_+3A_...">...</code></td>
<td>
<p>Other arguments (not used).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List object with one element per value specified by the argument in <code>method</code>.
</p>


<h3>References</h3>

<p>Warren, D.L., Glor, R.E., and Turelli, M.  2008.  Environmental niche equivalency versus conservatism: Quantitative approaches to niche evolution. <em>Evolution</em> 62:2868-2883. <a href="https://doi.org/10.1111/j.1558-5646.2008.00482.x">doi:10.1111/j.1558-5646.2008.00482.x</a>
</p>
<p>Warren, D.L., Glor, R.E., and Turelli, M.  2008.  Erratum.  <em>Evolution</em> 62:2868-2883. <a href="https://doi.org/10.1111/j.1558-5646.2010.01204.x">doi:10.1111/j.1558-5646.2010.01204.x</a>
</p>
<p>Godsoe, W. 2014. Inferring the similarity of species distributions using Species' Distribution Models.  <em>Ecography</em> 37:130-136. <a href="https://doi.org/10.1111/j.1600-0587.2013.00403.x">doi:10.1111/j.1600-0587.2013.00403.x</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+compareResponse">compareResponse</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x1 &lt;- seq(0, 1, length.out=100)
x2 &lt;- x1^2
nicheOverlapMetrics(x1, x2)

</code></pre>

<hr>
<h2 id='plotExtent'>Create spatial polygon same size as a plot</h2><span id='topic+plotExtent'></span>

<h3>Description</h3>

<p>This function creates a &quot;rectangular&quot; <code>SpatVector</code> object with the same dimensions as a plot window. It is especially useful for cropping subsequent rasters or vector objects to the plot window. A plot must be made before calling this function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotExtent(x = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plotExtent_+3A_x">x</code></td>
<td>
<p>Either <code>NULL</code> (default), an object of class <code>crs</code>, a coordinate reference string (PROJ6 WKT string), or an object with a coordinate reference system. If any of these is provided, the <code>SpatVector</code> object will have this CRS.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>SpatVector</code>
</p>


<h3>See Also</h3>

<p><a href="#topic+extentToVect">extentToVect</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if (FALSE) {

library(sf)

data(mad0)
plot(st_geometry(mad0))
outline &lt;- plotExtent(mad0)
plot(outline, col='cornflowerblue', lty='dotted')
plot(st_geometry(mad0), add=TRUE)

}

</code></pre>

<hr>
<h2 id='predictEnmSdm'>Generic predict function for SDMs/ENMs</h2><span id='topic+predictEnmSdm'></span>

<h3>Description</h3>

<p>This is a generic predict function that automatically uses the model common arguments for predicting models of the following types: linear models, generalized linear models (GLMs), generalized additive models (GAMs), random forests, boosted regression trees (BRTs)/gradient boosting machines (GBMs), conditional random forests, MaxEnt, and more.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predictEnmSdm(
  model,
  newdata,
  maxentFun = "terra",
  scale = TRUE,
  cores = 1,
  nrows = nrow(newdata),
  paths = .libPaths(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predictEnmSdm_+3A_model">model</code></td>
<td>
<p>Object of class <code>lm</code>, <code>glm</code>, <code>gam</code>, <code>randomForest</code>, <code>MaxEnt</code>, <code>maxnet</code>, <code>prcomp</code>, <code>kde</code>, <code>gbm</code>, and possibly others (worth a try!).</p>
</td></tr>
<tr><td><code id="predictEnmSdm_+3A_newdata">newdata</code></td>
<td>
<p>Data frame or matrix, or <code>SpatRaster</code> with data to which to predict.</p>
</td></tr>
<tr><td><code id="predictEnmSdm_+3A_maxentfun">maxentFun</code></td>
<td>
<p>This argument is only used if the <code>model</code> object is a MaxEnt model; otherwise, it is ignored. It takes a value of either <code>'terra'</code>, in which case a MaxEnt model is predicted using the default <code>predict</code> function from the <span class="pkg">terra</span> package, or <code>'enmSdmX'</code> in which case the function <code><a href="#topic+predictMaxEnt">predictMaxEnt</a></code> function from the <span class="pkg">enmSdmX</span> package (this package) is used.</p>
</td></tr>
<tr><td><code id="predictEnmSdm_+3A_scale">scale</code></td>
<td>
<p>Logical. If the model is a GLM trained with <code><a href="#topic+trainGLM">trainGLM</a></code> or <code><a href="#topic+trainNS">trainNS</a></code>, you can use the <code>scale</code> argument in that function to center and scale the predictors. In the <code>predictEnmSdm</code> function, you can set <code>scale</code> to <code>TRUE</code> to scale the rasters or data frame to which you are training using the centers (means) and scales (standard deviations) used in the mode. Otherwise, it is up to you to ensure variables are properly centered and scaled. This argument only has effect if the model is a GLM trained using <code><a href="#topic+trainGLM">trainGLM</a></code> or <code><a href="#topic+trainNS">trainNS</a></code>.</p>
</td></tr>
<tr><td><code id="predictEnmSdm_+3A_cores">cores</code></td>
<td>
<p>Integer &gt;= 1. Number of cores to use when calculating multiple models. Default is 1. This is forced to 1 if <code>newdata</code> is a <code>SpatRaster</code> (i.e., as of now, there is no parallelization when predicting to a raster... sorry!).  If you have issues when <code>cores</code> &gt; 1, please see the <code><a href="#topic+troubleshooting_parallel_operations">troubleshooting_parallel_operations</a></code> guide.</p>
</td></tr>
<tr><td><code id="predictEnmSdm_+3A_nrows">nrows</code></td>
<td>
<p>Number of rows of <code>newdata</code> to predict at a time. This is only used if <code>newdata</code> is a <code>data.frame</code> or <code>matrix</code>. The default is to predict all rows at once, but for very large data frames/matrices this can lead to memory issues in some cases. By setting the number of rows, <code>newdata</code> is divided into chunks, and predictions made to each chunk, which may ease memory limitations. This can be combined with multi-coring (which will increase memory requirements). In this case, all cores combined will get <code>nrows</code> of data. How many rows are too many? You will have to decide depending on the capabilities of your system. For example, predicting the outcome of a GLM on data with 10E6 rows may be fine, but predicting a PCA (with multiple axes) to the data data may require too much memory.</p>
</td></tr>
<tr><td><code id="predictEnmSdm_+3A_paths">paths</code></td>
<td>
<p>Locations where packages are stored. This is typically not useful to the general user, and is only supplied for when the function is called as a functional.</p>
</td></tr>
<tr><td><code id="predictEnmSdm_+3A_...">...</code></td>
<td>
<p>Arguments to pass to the algorithm-specific <code>predict</code> function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric or <code>SpatRaster</code>.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+predict">predict</a></code> from the <span class="pkg">stats</span> package, <code><a href="terra.html#topic+predict">predict</a></code> from the <span class="pkg">terra</span> package, <code><a href="#topic+predictMaxEnt">predictMaxEnt</a></code>, <code><a href="#topic+predictMaxNet">predictMaxNet</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# NB: The examples below show a very basic modeling workflow. They have been 
# designed to work fast, not produce accurate, defensible models. They can
# take a few minutes to run.

library(mgcv)
library(sf)
library(terra)
set.seed(123)

### setup data
##############

# environmental rasters
rastFile &lt;- system.file('extdata/madClim.tif', package='enmSdmX')
madClim &lt;- rast(rastFile)

# coordinate reference system
wgs84 &lt;- getCRS('WGS84')

# lemur occurrence data
data(lemurs)
occs &lt;- lemurs[lemurs$species == 'Eulemur fulvus', ]
occs &lt;- vect(occs, geom=c('longitude', 'latitude'), crs=wgs84)

occs &lt;- elimCellDuplicates(occs, madClim)

occEnv &lt;- extract(madClim, occs, ID = FALSE)
occEnv &lt;- occEnv[complete.cases(occEnv), ]
	
# create 10000 background sites (or as many as raster can support)
bgEnv &lt;- terra::spatSample(madClim, 20000)
bgEnv &lt;- bgEnv[complete.cases(bgEnv), ]
bgEnv &lt;- bgEnv[1:min(10000, nrow(bgEnv)), ]

# collate occurrences and background sites
presBg &lt;- data.frame(
  presBg = c(
    rep(1, nrow(occEnv)),
    rep(0, nrow(bgEnv))
  )
)

env &lt;- rbind(occEnv, bgEnv)
env &lt;- cbind(presBg, env)

predictors &lt;- c('bio1', 'bio12')

### calibrate models
####################

# Note that all of the trainXYZ functions can made to go faster using the
# "cores" argument (set to just 1, by default). The examples below will not
# go too much faster using more cores because they are simplified, but
# you can try!
cores &lt;- 1

# MaxEnt
mx &lt;- trainMaxEnt(
	data = env,
	resp = 'presBg',
	preds = predictors,
	regMult = 1, # too few values for reliable model, but fast
	verbose = TRUE,
	cores = cores
)

# MaxNet
mn &lt;- trainMaxNet(
	data = env,
	resp = 'presBg',
	preds = predictors,
	regMult = 1, # too few values for reliable model, but fast
	verbose = TRUE,
	cores = cores
)

# generalized linear model (GLM)
gl &lt;- trainGLM(
	data = env,
	resp = 'presBg',
	preds = predictors,
	scale = TRUE, # automatic scaling of predictors
	verbose = TRUE,
	cores = cores
)

# generalized additive model (GAM)
ga &lt;- trainGAM(
	data = env,
	resp = 'presBg',
	preds = predictors,
	verbose = TRUE,
	cores = cores
)

# natural splines
ns &lt;- trainNS(
	data = env,
	resp = 'presBg',
	preds = predictors,
	scale = TRUE, # automatic scaling of predictors
	df = 1:2, # too few values for reliable model(?)
	verbose = TRUE,
	cores = cores
)

# boosted regression trees
envSub &lt;- env[1:1049, ] # subsetting data to run faster
brt &lt;- trainBRT(
	data = envSub,
	resp = 'presBg',
	preds = predictors,
	learningRate = 0.001, # too few values for reliable model(?)
	treeComplexity = c(2, 3), # too few values for reliable model, but fast
	minTrees = 1200, # minimum trees for reliable model(?), but fast
	maxTrees = 1200, # too small for reliable model(?), but fast
	tryBy = 'treeComplexity',
	anyway = TRUE, # return models that did not converge
	verbose = TRUE,
	cores = cores
)

# random forests
rf &lt;- trainRF(
	data = env,
	resp = 'presBg',
	preds = predictors,
	numTrees = c(100, 500), # using at least 500 recommended, but fast!
	verbose = TRUE,
	cores = cores
)

### make maps of models
#######################

# NB We do not have to scale rasters before predicting GLMs and NSs because we
# used the `scale = TRUE` argument in trainGLM() and trainNS().

mxMap &lt;- predictEnmSdm(mx, madClim)
mnMap &lt;- predictEnmSdm(mn, madClim) 
glMap &lt;- predictEnmSdm(gl, madClim)
gaMap &lt;- predictEnmSdm(ga, madClim)
nsMap &lt;- predictEnmSdm(ns, madClim)
brtMap &lt;- predictEnmSdm(brt, madClim)
rfMap &lt;- predictEnmSdm(rf, madClim)

maps &lt;- c(
	mxMap,
	mnMap,
	glMap,
	gaMap,
	nsMap,
	brtMap,
	rfMap
)

names(maps) &lt;- c('MaxEnt', 'MaxNet', 'GLM', 'GAM', 'NSs', 'BRTs', 'RFs')
fun &lt;- function() plot(occs, col='black', pch=3, add=TRUE)
plot(maps, fun = fun, nc = 4)

### compare model responses to BIO12 (mean annual precipitation)
################################################################

# make a data frame holding all other variables at mean across occurrences,
# varying only BIO12
occEnvMeans &lt;- colMeans(occEnv, na.rm=TRUE)
occEnvMeans &lt;- rbind(occEnvMeans)
occEnvMeans &lt;- as.data.frame(occEnvMeans)
climFrame &lt;- occEnvMeans[rep(1, 100), ]
rownames(climFrame) &lt;- NULL

minBio12 &lt;- min(env$bio12)
maxBio12 &lt;- max(env$bio12)
climFrame$bio12 &lt;- seq(minBio12, maxBio12, length.out=100)

predMx &lt;- predictEnmSdm(mx, climFrame)
predMn &lt;- predictEnmSdm(mn, climFrame)
predGl &lt;- predictEnmSdm(gl, climFrame)
predGa &lt;- predictEnmSdm(ga, climFrame)
predNat &lt;- predictEnmSdm(ns, climFrame)
predBrt &lt;- predictEnmSdm(brt, climFrame)
predRf &lt;- predictEnmSdm(rf, climFrame)


plot(climFrame$bio12, predMx,
xlab='BIO12', ylab='Prediction', type='l', ylim=c(0, 1))

lines(climFrame$bio12, predMn, lty='solid', col='red')
lines(climFrame$bio12, predGl, lty='dotted', col='blue')
lines(climFrame$bio12, predGa, lty='dashed', col='green')
lines(climFrame$bio12, predNat, lty=4, col='purple')
lines(climFrame$bio12, predBrt, lty=5, col='orange')
lines(climFrame$bio12, predRf, lty=6, col='cyan')

legend(
   'topleft',
   inset = 0.01,
   legend = c(
	'MaxEnt',
	'MaxNet',
	'GLM',
	'GAM',
	'NS',
	'BRT',
	'RF'
   ),
   lty = c(1, 1:6),
   col = c(
	'black',
	'red',
	'blue',
	'green',
	'purple',
	'orange',
	'cyan'
   ),
   bg = 'white'
)


</code></pre>

<hr>
<h2 id='predictMaxEnt'>Predict a MaxEnt model object (with optional feature-level permutation)</h2><span id='topic+predictMaxEnt'></span>

<h3>Description</h3>

<p>Takes a MaxEnt <code>lambda</code> object or a MaxEnt object and returns raw or logistic predictions.  Its output is the same as the <code><a href="terra.html#topic+predict">predict</a></code> function from the <span class="pkg">terra</span> package, and in fact, is slower than the function from <span class="pkg">terra</span>. However, this function does allow custom manipulations that those functions do not allow (e.g., permuting product features while leaving other features with the same variables intact).  This function does <em>not</em> clamp predictions&ndash;beyond the range of the training data, it extends the prediction in the direction it was going (up/down/no change). The function is based on Peter D. Wilson's document &quot;Guidelines for computing MaxEnt model output values from a lambdas file&quot;. The function has a special feature in that it allows you to permute single variables or combinations of variables in specific features before making predictions. This is potentially useful, for example, if you wanted to determine the relative importance of a quadratic feature for a particular variable in a Maxent model relative to the other features in the model.  You can also permute values of a variable regardless of which features they appear in. For product features, you can implement the permutation before or after the values are multiplied together (before often makes for bigger differences in predictions).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predictMaxEnt(
  x,
  data,
  type = "cloglog",
  perm = NULL,
  permLinear = NULL,
  permQuad = NULL,
  permHinge = NULL,
  permThresh = NULL,
  permProd = NULL,
  permProdRule = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predictMaxEnt_+3A_x">x</code></td>
<td>
<p>Either a Maxent lambda object or a Maxent model object</p>
</td></tr>
<tr><td><code id="predictMaxEnt_+3A_data">data</code></td>
<td>
<p>Data frame. Data to which to make predictions</p>
</td></tr>
<tr><td><code id="predictMaxEnt_+3A_type">type</code></td>
<td>
<p>Character.  One of:
</p>

<ul>
<li> <p><code>'raw'</code>: Maxent &quot;raw&quot; values
</p>
</li>
<li> <p><code>'logistic'</code>: Maxent logistic values
</p>
</li>
<li> <p><code>'cloglog'</code> Complementary log-log output (as per version 3.4.0+ of maxent&ndash;called &quot;<code>maxnet()</code>&quot; in the package of the same name)
</p>
</li></ul>
</td></tr>
<tr><td><code id="predictMaxEnt_+3A_perm">perm</code></td>
<td>
<p>Character vector. Name(s) of variable to permute before calculating predictions. This permutes the variables for <em>all</em> features in which they occur.  If a variable is named here, it overrides permutation settings for each feature featType.  Note that for product features the variable is permuted before the product is taken. This permutation is performed before any subsequent permutations (i.e., so if both variables in a product feature are included in <code>perms</code>, then this is equivalent to using the <code>'before'</code> rule for <code>permProdRule</code>). Ignored if <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="predictMaxEnt_+3A_permlinear">permLinear</code></td>
<td>
<p>Character vector. Names(s) of variables to permute in linear features before calculating predictions.  Ignored if <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="predictMaxEnt_+3A_permquad">permQuad</code></td>
<td>
<p>Names(s) of variables to permute in quadratic features before calculating predictions.  Ignored if <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="predictMaxEnt_+3A_permhinge">permHinge</code></td>
<td>
<p>Character vector. Names(s) of variables to permute in forward/reverse hinge features before calculating predictions.  Ignored if <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="predictMaxEnt_+3A_permthresh">permThresh</code></td>
<td>
<p>Character vector. Names(s) of variables to permute in threshold features before calculating predictions.  Ignored if <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="predictMaxEnt_+3A_permprod">permProd</code></td>
<td>
<p>Character list. A list object of <code>n</code> elements, each of which has two character elements naming the variables to permute if they occur in a product feature.  Depending on the value of <code>permProdRule</code>, the function will either permute the individual variables then calculate their product or calculate their product, then permute the product across observations.  Any other features containing the variables will produce values as normal.  Example: <code>permProd=list(c('precipWinter', 'tempWinter'), c('tempSummer', 'precipFall'))</code>.  The order of the variables in each element of <code>permProd</code> doesn't matter, so <code>permProd=list(c('temp', 'precip'))</code> is the same as <code>permProd=list(c('precip', 'temp'))</code>.  Ignored if <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="predictMaxEnt_+3A_permprodrule">permProdRule</code></td>
<td>
<p>Character. Rule for how permutation of product features is applied: <code>'before'</code> ==&gt; Permute individual variable values then calculate product; <code>'after'</code> ==&gt; calculate product then permute across these values. Ignored if <code>permProd</code> is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="predictMaxEnt_+3A_...">...</code></td>
<td>
<p>Extra arguments (not used).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric.
</p>


<h3>See Also</h3>

<p><code><a href="predicts.html#topic+MaxEnt">MaxEnt</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# NB: The examples below show a very basic modeling workflow. They have been 
# designed to work fast, not produce accurate, defensible models. They can
# take a few minutes to run.

library(mgcv)
library(sf)
library(terra)
set.seed(123)

### setup data
##############

# environmental rasters
rastFile &lt;- system.file('extdata/madClim.tif', package='enmSdmX')
madClim &lt;- rast(rastFile)

# coordinate reference system
wgs84 &lt;- getCRS('WGS84')

# lemur occurrence data
data(lemurs)
occs &lt;- lemurs[lemurs$species == 'Eulemur fulvus', ]
occs &lt;- vect(occs, geom=c('longitude', 'latitude'), crs=wgs84)

occs &lt;- elimCellDuplicates(occs, madClim)

occEnv &lt;- extract(madClim, occs, ID = FALSE)
occEnv &lt;- occEnv[complete.cases(occEnv), ]
	
# create 10000 background sites (or as many as raster can support)
bgEnv &lt;- terra::spatSample(madClim, 20000)
bgEnv &lt;- bgEnv[complete.cases(bgEnv), ]
bgEnv &lt;- bgEnv[1:min(10000, nrow(bgEnv)), ]

# collate occurrences and background sites
presBg &lt;- data.frame(
  presBg = c(
    rep(1, nrow(occEnv)),
    rep(0, nrow(bgEnv))
  )
)

env &lt;- rbind(occEnv, bgEnv)
env &lt;- cbind(presBg, env)

predictors &lt;- c('bio1', 'bio12')

### calibrate models
####################

# Note that all of the trainXYZ functions can made to go faster using the
# "cores" argument (set to just 1, by default). The examples below will not
# go too much faster using more cores because they are simplified, but
# you can try!
cores &lt;- 1

# MaxEnt
mx &lt;- trainMaxEnt(
	data = env,
	resp = 'presBg',
	preds = predictors,
	regMult = 1, # too few values for reliable model, but fast
	verbose = TRUE,
	cores = cores
)

# MaxNet
mn &lt;- trainMaxNet(
	data = env,
	resp = 'presBg',
	preds = predictors,
	regMult = 1, # too few values for reliable model, but fast
	verbose = TRUE,
	cores = cores
)

# generalized linear model (GLM)
gl &lt;- trainGLM(
	data = env,
	resp = 'presBg',
	preds = predictors,
	scale = TRUE, # automatic scaling of predictors
	verbose = TRUE,
	cores = cores
)

# generalized additive model (GAM)
ga &lt;- trainGAM(
	data = env,
	resp = 'presBg',
	preds = predictors,
	verbose = TRUE,
	cores = cores
)

# natural splines
ns &lt;- trainNS(
	data = env,
	resp = 'presBg',
	preds = predictors,
	scale = TRUE, # automatic scaling of predictors
	df = 1:2, # too few values for reliable model(?)
	verbose = TRUE,
	cores = cores
)

# boosted regression trees
envSub &lt;- env[1:1049, ] # subsetting data to run faster
brt &lt;- trainBRT(
	data = envSub,
	resp = 'presBg',
	preds = predictors,
	learningRate = 0.001, # too few values for reliable model(?)
	treeComplexity = c(2, 3), # too few values for reliable model, but fast
	minTrees = 1200, # minimum trees for reliable model(?), but fast
	maxTrees = 1200, # too small for reliable model(?), but fast
	tryBy = 'treeComplexity',
	anyway = TRUE, # return models that did not converge
	verbose = TRUE,
	cores = cores
)

# random forests
rf &lt;- trainRF(
	data = env,
	resp = 'presBg',
	preds = predictors,
	numTrees = c(100, 500), # using at least 500 recommended, but fast!
	verbose = TRUE,
	cores = cores
)

### make maps of models
#######################

# NB We do not have to scale rasters before predicting GLMs and NSs because we
# used the `scale = TRUE` argument in trainGLM() and trainNS().

mxMap &lt;- predictEnmSdm(mx, madClim)
mnMap &lt;- predictEnmSdm(mn, madClim) 
glMap &lt;- predictEnmSdm(gl, madClim)
gaMap &lt;- predictEnmSdm(ga, madClim)
nsMap &lt;- predictEnmSdm(ns, madClim)
brtMap &lt;- predictEnmSdm(brt, madClim)
rfMap &lt;- predictEnmSdm(rf, madClim)

maps &lt;- c(
	mxMap,
	mnMap,
	glMap,
	gaMap,
	nsMap,
	brtMap,
	rfMap
)

names(maps) &lt;- c('MaxEnt', 'MaxNet', 'GLM', 'GAM', 'NSs', 'BRTs', 'RFs')
fun &lt;- function() plot(occs, col='black', pch=3, add=TRUE)
plot(maps, fun = fun, nc = 4)

### compare model responses to BIO12 (mean annual precipitation)
################################################################

# make a data frame holding all other variables at mean across occurrences,
# varying only BIO12
occEnvMeans &lt;- colMeans(occEnv, na.rm=TRUE)
occEnvMeans &lt;- rbind(occEnvMeans)
occEnvMeans &lt;- as.data.frame(occEnvMeans)
climFrame &lt;- occEnvMeans[rep(1, 100), ]
rownames(climFrame) &lt;- NULL

minBio12 &lt;- min(env$bio12)
maxBio12 &lt;- max(env$bio12)
climFrame$bio12 &lt;- seq(minBio12, maxBio12, length.out=100)

predMx &lt;- predictEnmSdm(mx, climFrame)
predMn &lt;- predictEnmSdm(mn, climFrame)
predGl &lt;- predictEnmSdm(gl, climFrame)
predGa &lt;- predictEnmSdm(ga, climFrame)
predNat &lt;- predictEnmSdm(ns, climFrame)
predBrt &lt;- predictEnmSdm(brt, climFrame)
predRf &lt;- predictEnmSdm(rf, climFrame)


plot(climFrame$bio12, predMx,
xlab='BIO12', ylab='Prediction', type='l', ylim=c(0, 1))

lines(climFrame$bio12, predMn, lty='solid', col='red')
lines(climFrame$bio12, predGl, lty='dotted', col='blue')
lines(climFrame$bio12, predGa, lty='dashed', col='green')
lines(climFrame$bio12, predNat, lty=4, col='purple')
lines(climFrame$bio12, predBrt, lty=5, col='orange')
lines(climFrame$bio12, predRf, lty=6, col='cyan')

legend(
   'topleft',
   inset = 0.01,
   legend = c(
	'MaxEnt',
	'MaxNet',
	'GLM',
	'GAM',
	'NS',
	'BRT',
	'RF'
   ),
   lty = c(1, 1:6),
   col = c(
	'black',
	'red',
	'blue',
	'green',
	'purple',
	'orange',
	'cyan'
   ),
   bg = 'white'
)


</code></pre>

<hr>
<h2 id='predictMaxNet'>Predictions from a MaxNet model</h2><span id='topic+predictMaxNet'></span>

<h3>Description</h3>

<p>This function is the same as the <code>predict</code> function in the <span class="pkg">maxnet</span> package, except that:
</p>

<ul>
<li><p>	If the input is a data frame, the output is a vector as output (not a single-column matrix);
</p>
</li>
<li><p>	If the input is a <code>SpatRaster</code>, the output is a <code>SpatRaster</code>;
</p>
</li>
<li><p>	The default output is on the cloglog scale;
</p>
</li>
<li><p>   The function can be explicitly called (versus doing, say, <code>maxnet:::predict.maxnet</code>, which does not work even when that would be really useful...).
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>predictMaxNet(model, newdata, clamp = TRUE, type = "cloglog", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predictMaxNet_+3A_model">model</code></td>
<td>
<p>Object of class <code>maxnet</code>.</p>
</td></tr>
<tr><td><code id="predictMaxNet_+3A_newdata">newdata</code></td>
<td>
<p>Object of class <code>data.frame</code> or <code>SpatRaster</code> (<span class="pkg">terra</span> package).</p>
</td></tr>
<tr><td><code id="predictMaxNet_+3A_clamp">clamp</code></td>
<td>
<p>If <code>TRUE</code> (default), predict outside the range of training data by 'clamping' values to the last value.</p>
</td></tr>
<tr><td><code id="predictMaxNet_+3A_type">type</code></td>
<td>
<p>One of:
</p>

<ul>
<li>        <p><code>cloglog</code> (default): Predictions are on a complementary log-log scale.
</p>
</li>
<li>        <p><code>logistic</code>: Predictions are on a logistic scale (and thus technically the same to several decimal places as predictions from MaxEnt &lt;=3.3.3k, except for differences in default features).
</p>
</li>
<li>        <p><code>link</code>: Predictions are on the scale of the predictors.
</p>
</li>
<li>        <p><code>exponential</code>: Predictions are on an exponential ('raw') scale.
</p>
</li></ul>
</td></tr>
<tr><td><code id="predictMaxNet_+3A_...">...</code></td>
<td>
<p>Other arguments (unused).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric vector or <code>SpatRaster</code>
</p>


<h3>See Also</h3>

<p><code><a href="terra.html#topic+predict">predict</a></code> from the <span class="pkg">terra</span> package, and <code><a href="maxnet.html#topic+maxnet">maxnet</a></code> (see the <code>predict</code> function therein)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# NB: The examples below show a very basic modeling workflow. They have been 
# designed to work fast, not produce accurate, defensible models. They can
# take a few minutes to run.

library(mgcv)
library(sf)
library(terra)
set.seed(123)

### setup data
##############

# environmental rasters
rastFile &lt;- system.file('extdata/madClim.tif', package='enmSdmX')
madClim &lt;- rast(rastFile)

# coordinate reference system
wgs84 &lt;- getCRS('WGS84')

# lemur occurrence data
data(lemurs)
occs &lt;- lemurs[lemurs$species == 'Eulemur fulvus', ]
occs &lt;- vect(occs, geom=c('longitude', 'latitude'), crs=wgs84)

occs &lt;- elimCellDuplicates(occs, madClim)

occEnv &lt;- extract(madClim, occs, ID = FALSE)
occEnv &lt;- occEnv[complete.cases(occEnv), ]
	
# create 10000 background sites (or as many as raster can support)
bgEnv &lt;- terra::spatSample(madClim, 20000)
bgEnv &lt;- bgEnv[complete.cases(bgEnv), ]
bgEnv &lt;- bgEnv[1:min(10000, nrow(bgEnv)), ]

# collate occurrences and background sites
presBg &lt;- data.frame(
  presBg = c(
    rep(1, nrow(occEnv)),
    rep(0, nrow(bgEnv))
  )
)

env &lt;- rbind(occEnv, bgEnv)
env &lt;- cbind(presBg, env)

predictors &lt;- c('bio1', 'bio12')

### calibrate models
####################

# Note that all of the trainXYZ functions can made to go faster using the
# "cores" argument (set to just 1, by default). The examples below will not
# go too much faster using more cores because they are simplified, but
# you can try!
cores &lt;- 1

# MaxEnt
mx &lt;- trainMaxEnt(
	data = env,
	resp = 'presBg',
	preds = predictors,
	regMult = 1, # too few values for reliable model, but fast
	verbose = TRUE,
	cores = cores
)

# MaxNet
mn &lt;- trainMaxNet(
	data = env,
	resp = 'presBg',
	preds = predictors,
	regMult = 1, # too few values for reliable model, but fast
	verbose = TRUE,
	cores = cores
)

# generalized linear model (GLM)
gl &lt;- trainGLM(
	data = env,
	resp = 'presBg',
	preds = predictors,
	scale = TRUE, # automatic scaling of predictors
	verbose = TRUE,
	cores = cores
)

# generalized additive model (GAM)
ga &lt;- trainGAM(
	data = env,
	resp = 'presBg',
	preds = predictors,
	verbose = TRUE,
	cores = cores
)

# natural splines
ns &lt;- trainNS(
	data = env,
	resp = 'presBg',
	preds = predictors,
	scale = TRUE, # automatic scaling of predictors
	df = 1:2, # too few values for reliable model(?)
	verbose = TRUE,
	cores = cores
)

# boosted regression trees
envSub &lt;- env[1:1049, ] # subsetting data to run faster
brt &lt;- trainBRT(
	data = envSub,
	resp = 'presBg',
	preds = predictors,
	learningRate = 0.001, # too few values for reliable model(?)
	treeComplexity = c(2, 3), # too few values for reliable model, but fast
	minTrees = 1200, # minimum trees for reliable model(?), but fast
	maxTrees = 1200, # too small for reliable model(?), but fast
	tryBy = 'treeComplexity',
	anyway = TRUE, # return models that did not converge
	verbose = TRUE,
	cores = cores
)

# random forests
rf &lt;- trainRF(
	data = env,
	resp = 'presBg',
	preds = predictors,
	numTrees = c(100, 500), # using at least 500 recommended, but fast!
	verbose = TRUE,
	cores = cores
)

### make maps of models
#######################

# NB We do not have to scale rasters before predicting GLMs and NSs because we
# used the `scale = TRUE` argument in trainGLM() and trainNS().

mxMap &lt;- predictEnmSdm(mx, madClim)
mnMap &lt;- predictEnmSdm(mn, madClim) 
glMap &lt;- predictEnmSdm(gl, madClim)
gaMap &lt;- predictEnmSdm(ga, madClim)
nsMap &lt;- predictEnmSdm(ns, madClim)
brtMap &lt;- predictEnmSdm(brt, madClim)
rfMap &lt;- predictEnmSdm(rf, madClim)

maps &lt;- c(
	mxMap,
	mnMap,
	glMap,
	gaMap,
	nsMap,
	brtMap,
	rfMap
)

names(maps) &lt;- c('MaxEnt', 'MaxNet', 'GLM', 'GAM', 'NSs', 'BRTs', 'RFs')
fun &lt;- function() plot(occs, col='black', pch=3, add=TRUE)
plot(maps, fun = fun, nc = 4)

### compare model responses to BIO12 (mean annual precipitation)
################################################################

# make a data frame holding all other variables at mean across occurrences,
# varying only BIO12
occEnvMeans &lt;- colMeans(occEnv, na.rm=TRUE)
occEnvMeans &lt;- rbind(occEnvMeans)
occEnvMeans &lt;- as.data.frame(occEnvMeans)
climFrame &lt;- occEnvMeans[rep(1, 100), ]
rownames(climFrame) &lt;- NULL

minBio12 &lt;- min(env$bio12)
maxBio12 &lt;- max(env$bio12)
climFrame$bio12 &lt;- seq(minBio12, maxBio12, length.out=100)

predMx &lt;- predictEnmSdm(mx, climFrame)
predMn &lt;- predictEnmSdm(mn, climFrame)
predGl &lt;- predictEnmSdm(gl, climFrame)
predGa &lt;- predictEnmSdm(ga, climFrame)
predNat &lt;- predictEnmSdm(ns, climFrame)
predBrt &lt;- predictEnmSdm(brt, climFrame)
predRf &lt;- predictEnmSdm(rf, climFrame)


plot(climFrame$bio12, predMx,
xlab='BIO12', ylab='Prediction', type='l', ylim=c(0, 1))

lines(climFrame$bio12, predMn, lty='solid', col='red')
lines(climFrame$bio12, predGl, lty='dotted', col='blue')
lines(climFrame$bio12, predGa, lty='dashed', col='green')
lines(climFrame$bio12, predNat, lty=4, col='purple')
lines(climFrame$bio12, predBrt, lty=5, col='orange')
lines(climFrame$bio12, predRf, lty=6, col='cyan')

legend(
   'topleft',
   inset = 0.01,
   legend = c(
	'MaxEnt',
	'MaxNet',
	'GLM',
	'GAM',
	'NS',
	'BRT',
	'RF'
   ),
   lty = c(1, 1:6),
   col = c(
	'black',
	'red',
	'blue',
	'green',
	'purple',
	'orange',
	'cyan'
   ),
   bg = 'white'
)


</code></pre>

<hr>
<h2 id='sampleRast'>Sample random points from a raster with/out replacement</h2><span id='topic+sampleRast'></span>

<h3>Description</h3>

<p>This function returns coordinates randomly located on a raster where cells can be sampled with replacement (if desired) and where the probability of selection is proportionate to the cell value, cell area, or the product of cell value times cell area.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sampleRast(x, n, adjArea = TRUE, replace = TRUE, prob = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sampleRast_+3A_x">x</code></td>
<td>
<p><code>SpatRaster</code> object.</p>
</td></tr>
<tr><td><code id="sampleRast_+3A_n">n</code></td>
<td>
<p>Positive integer. Number of points to draw.</p>
</td></tr>
<tr><td><code id="sampleRast_+3A_adjarea">adjArea</code></td>
<td>
<p>If <code>TRUE</code> (default) then adjust probabilities so sampling accounts for cell area.</p>
</td></tr>
<tr><td><code id="sampleRast_+3A_replace">replace</code></td>
<td>
<p>If <code>TRUE</code> (default) then sample with replacement.</p>
</td></tr>
<tr><td><code id="sampleRast_+3A_prob">prob</code></td>
<td>
<p>If <code>TRUE</code> (default) then sample cells with probabilities proportional to cell values. If <code>adjArea</code> is also <code>TRUE</code> then probabilities are drawn proportional to the product of cell area * the value of the cell.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>2-column matrix with longitude and latitude of random points.
</p>


<h3>See Also</h3>

<p><code><a href="terra.html#topic+spatSample">spatSample</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(terra)
r &lt;- rast()
nc &lt;- ncell(r)
r[] &lt;- 1:nc
rands1 &lt;- sampleRast(r, 10000)
rands2 &lt;- sampleRast(r, 10000, adjArea=FALSE)
rands3 &lt;- sampleRast(r, 10000, prob=FALSE)
rands4 &lt;- sampleRast(r, 10000, adjArea=FALSE, prob=FALSE)

oldPar &lt;- par(mfrow=c(2, 2))

plot(r, main='adjArea = TRUE &amp; prob = TRUE')
points(rands1, pch='.')
plot(r, main='adjArea = FALSE &amp; prob = TRUE')
points(rands2, pch='.')
plot(r, main='adjArea = TRUE &amp; prob = FALSE')
points(rands3, pch='.')
plot(r, main='adjArea = FALSE &amp; prob = FALSE')
points(rands4, pch='.')

par(oldPar)

</code></pre>

<hr>
<h2 id='spatVectorToSpatial'>Convert SpatVector to Spatial*</h2><span id='topic+spatVectorToSpatial'></span>

<h3>Description</h3>

<p>This function converts a <code>SpatVector</code> object from the <span class="pkg">terra</span> package to a <code>Spatial</code> object of the appropriate class (<code>SpatialPoints</code>, <code>SpatialPointsDataFrame</code>, <code>SpatialPolygons</code>, or <code>SpatialPolygonsDataFrame</code>) from the <span class="pkg">sp</span> package. Note that <span class="pkg">sp</span> is to be retired in 2023, so this function is to be come useful only for legacy applications.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spatVectorToSpatial(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spatVectorToSpatial_+3A_x">x</code></td>
<td>
<p><code>SpatVector</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class <code>Spatial</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(terra)
f &lt;- system.file('ex/lux.shp', package='terra')
v &lt;- vect(f)
spat &lt;- spatVectorToSpatial(v)
class(spat)

</code></pre>

<hr>
<h2 id='squareCellRast'>Create a raster with square cells</h2><span id='topic+squareCellRast'></span>

<h3>Description</h3>

<p>This function creates a raster from an object with an extent (i.e., another raster or similar spatial object) with square cells. The user can specify cell resolution (linear dimension) <em>or</em> the approximate number of cells desired.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>squareCellRast(x, numCells = NULL, res = NULL, vals = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="squareCellRast_+3A_x">x</code></td>
<td>
<p>An object with a spatial extent property (e.g., a <code>SpatRaster</code> or a <code>SpatVector</code>).</p>
</td></tr>
<tr><td><code id="squareCellRast_+3A_numcells">numCells</code></td>
<td>
<p>Positive integer, approximate number of cells desired. If this is specified, then <code>res</code> is ignored. If this number of cells cannot be fit into the desired extent exactly, then the actual number of cells will be larger.</p>
</td></tr>
<tr><td><code id="squareCellRast_+3A_res">res</code></td>
<td>
<p>Positive numeric. Size of a cell in the units of the projection of <code>x</code> (typically meters). Ignored if <code>numCells</code> is not <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="squareCellRast_+3A_vals">vals</code></td>
<td>
<p>Numeric, value to assign to cells. Note that if this is shorter than the number of cells in the output, then values will be recycled. If longer, then values will be truncated. The default is to assign all 0s.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>SpatRaster</code> object. The raster will have an extent of the same size or larger than the extent of <code>x</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(sf)
library(terra)

# project outline of Madagascar to equal-area:
data(mad0)
mad0Ea &lt;- st_transform(mad0, getCRS('madAlbers'))

n &lt;- 101
cellSize_meters &lt;- 10E4
byNumCells &lt;- squareCellRast(mad0Ea, numCells=n)
byCellSize &lt;- squareCellRast(mad0Ea, res=cellSize_meters)

oldPar &lt;- par(mfrow=c(1, 2))

main1 &lt;- paste0('Cells: ', n, ' desired, ', ncell(byNumCells), ' actual')
plot(byNumCells, main = main1)
plot(mad0Ea, add = TRUE)

main2 &lt;- paste0('Cells ', cellSize_meters, ' m on a side')
plot(byCellSize, main = main2)
plot(mad0Ea, add = TRUE)

par(oldPar)

# Note that in this example they look the same, but the one on the left
# has one less row than the one on the right.

</code></pre>

<hr>
<h2 id='summaryByCrossValid'>Summarize distribution/niche model cross-validation object</h2><span id='topic+summaryByCrossValid'></span>

<h3>Description</h3>

<p>This function summarizes models calibrated using the <code><a href="#topic+trainByCrossValid">trainByCrossValid</a></code> function. It returns aspects of the best models across k-folds (the particular aspects depends on the kind of models used).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summaryByCrossValid(
  x,
  metric = "cbiTest",
  decreasing = TRUE,
  interceptOnly = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="summaryByCrossValid_+3A_x">x</code></td>
<td>
<p>The output from the <code><a href="#topic+trainByCrossValid">trainByCrossValid</a></code> function (which is a list). Note that the object <em>must</em> include a sublist named <code>tuning</code>.</p>
</td></tr>
<tr><td><code id="summaryByCrossValid_+3A_metric">metric</code></td>
<td>
<p>Metric by which to select the best model in each k-fold. This can be any of the columns that appear in the data frames in <code>x$tuning</code> (or any columns added manually), but typically is one of the following <em>plus</em> either <code>Train</code>, <code>Test</code>, or <code>Delta</code> (e.g., <code>'logLossTrain'</code>, <code>'logLossTest'</code>, or <code>'logLossDelta'</code>):
</p>

<ul>
<li> <p><code>'logLoss'</code>: Log loss.
</p>
</li>
<li> <p><code>'cbi'</code>: Continuous Boyce Index (CBI). Calculated with <code><a href="#topic+evalContBoyce">evalContBoyce</a></code>.
</p>
</li>
<li> <p><code>'auc'</code>: Area under the receiver-operator characteristic curve (AUC). Calculated with <code><a href="#topic+evalAUC">evalAUC</a></code>.
</p>
</li>
<li> <p><code>'tss'</code>: Maximum value of the True Skill Statistic. Calculated with <code><a href="#topic+evalTSS">evalTSS</a></code>.
</p>
</li>
<li> <p><code>'msss'</code>: Sensitivity and specificity calculated at the threshold that maximizes sensitivity (true presence prediction rate) plus specificity (true absence prediction rate).
</p>
</li>
<li> <p><code>'mdss'</code>: Sensitivity (se) and specificity (sp) calculated at the threshold that minimizes the difference between sensitivity and specificity.
</p>
</li>
<li> <p><code>'minTrainPres'</code>: Sensitivity and specificity at the greatest threshold at which all training presences are classified as &quot;present&quot;.
</p>
</li>
<li> <p><code>'trainSe95'</code> and/or <code>'trainSe90'</code>: Sensitivity at the threshold that ensures either 95
</p>
</li></ul>
</td></tr>
<tr><td><code id="summaryByCrossValid_+3A_decreasing">decreasing</code></td>
<td>
<p>Logical, if <code>TRUE</code> (default), for each k-fold sort models by the value listed in <code>metric</code> in decreasing order (highest connotes &quot;best&quot;, lowest &quot;worst&quot;). If <code>FALSE</code> use the lowest value of <code>metric</code>.</p>
</td></tr>
<tr><td><code id="summaryByCrossValid_+3A_interceptonly">interceptOnly</code></td>
<td>
<p>Logical. If <code>TRUE</code> (default) and the top models in each case were intercept-only models, return an emppty data frame (with a warning). If <code>FALSE</code>, return results using the first model in each fold that was not an intercept-only model. This is only used if the training function was a generalized linear model (GLM), natural splines model (NS), or generalized additive model (GAM).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Data frame with statistics on the best set of models across k-folds. Depending on the model algorithm, this could be:
</p>

<ul>
<li><p> BRTs (boosted regression trees): Learning rate, tree complexity, and bag fraction.
</p>
</li>
<li><p> GLMs (generalized linear models): Frequency of use of each term in the best models.
</p>
</li>
<li><p> Maxent: Frequency of times each specific combination of feature classes was used in the best models plus mean master regularization multiplier for each feature set.
</p>
</li>
<li><p> NSs (natural splines): Data frame, one row per fold and one column per predictor, with values representing the maximum degrees of freedom used for each variable in the best model of each fold.
</p>
</li>
<li><p> RFs (random forests): Data frame, one row per fold, with values representing the optimal value of <code>numTrees</code> and <code>mtry</code> (see <code><a href="ranger.html#topic+ranger">ranger</a></code>).
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+trainByCrossValid">trainByCrossValid</a></code>, <code><a href="#topic+trainBRT">trainBRT</a></code>, <code><a href="#topic+trainGAM">trainGAM</a></code>, <code><a href="#topic+trainGLM">trainGLM</a></code>, <code><a href="#topic+trainMaxEnt">trainMaxEnt</a></code>, <code><a href="#topic+trainNS">trainNS</a></code>, <code><a href="#topic+trainRF">trainRF</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># The example below show a very basic modeling workflow. It has been 
# designed to work fast, not produce accurate, defensible models.
# The general idea is to calibrate a series of models and evaluate them
# against a withheld set of data. One can then use the series of models
# of the top models to better select a "final" model.

## Not run: 
# Running the entire set of commands can take a few minutes. This can
# be sped up by increasing the number of cores used. The examples below use
# one core, but you can change that argument according to your machine's
# capabilities.

library(sf)
library(terra)
set.seed(123)

### setup data
##############

# environmental rasters
rastFile &lt;- system.file('extdata/madClim.tif', package='enmSdmX')
madClim &lt;- rast(rastFile)

# coordinate reference system
wgs84 &lt;- getCRS('WGS84')

# lemur occurrence data
data(lemurs)
occs &lt;- lemurs[lemurs$species == 'Eulemur fulvus', ]
occs &lt;- vect(occs, geom=c('longitude', 'latitude'), crs=wgs84)

occs &lt;- elimCellDuplicates(occs, madClim)

occEnv &lt;- extract(madClim, occs, ID = FALSE)
occEnv &lt;- occEnv[complete.cases(occEnv), ]
	
# create background sites (using just 1000 to speed things up!)
bgEnv &lt;- terra::spatSample(madClim, 3000)
bgEnv &lt;- bgEnv[complete.cases(bgEnv), ]
bgEnv &lt;- bgEnv[sample(nrow(bgEnv), 1000), ]

# collate occurrences and background sites
presBg &lt;- data.frame(
   presBg = c(
      rep(1, nrow(occEnv)),
      rep(0, nrow(bgEnv))
   )
)

env &lt;- rbind(occEnv, bgEnv)
env &lt;- cbind(presBg, env)

predictors &lt;- c('bio1', 'bio12')

# using "vector" form of "folds" argument
folds &lt;- dismo::kfold(env, 3) # just 3 folds (for speed)

### calibrate models
####################

cores &lt;- 1 # increase this to go faster, if your computer handles it

## MaxEnt
mxx &lt;- trainByCrossValid(
	data = env,
	resp = 'presBg',
	preds = c('bio1', 'bio12'),
	folds = folds,
	trainFx = trainMaxEnt,
	regMult = 1:2, # too few values for valid model, but fast!
	verbose = 1,
	cores = cores
)

# summarize MaxEnt feature sets and regularization across folds
summaryByCrossValid(mxx)

## MaxNet
mnx &lt;- trainByCrossValid(
	data = env,
	resp = 'presBg',
	preds = c('bio1', 'bio12'),
	folds = folds,
	trainFx = trainMaxNet,
	regMult = 1:2, # too few values for valid model, but fast!
	verbose = 1,
	cores = cores
)

# summarize MaxEnt feature sets and regularization across folds
summaryByCrossValid(mnx)

## generalized linear models
glx &lt;- trainByCrossValid(
	data = env,
	resp = 'presBg',
	preds = c('bio1', 'bio12'),
	folds = folds,
	trainFx = trainGLM,
	verbose = 1,
	cores = cores
)

# summarize GLM terms in best models
summaryByCrossValid(glx)

## generalized additive models
gax &lt;- trainByCrossValid(
	data = env,
	resp = 'presBg',
	preds = c('bio1', 'bio12'),
	folds = folds,
	trainFx = trainGAM,
	verbose = 1,
	cores = cores
)

# summarize GAM terms in best models
summaryByCrossValid(gax)

## natural splines
nsx &lt;- trainByCrossValid(
	data = env,
	resp = 'presBg',
	preds = c('bio1', 'bio12'),
	folds = folds,
	trainFx = trainNS,
	df = 1:2,
	verbose = 1,
	cores = cores
)

# summarize NS terms in best models
summaryByCrossValid(nsx)

## boosted regression trees
brtx &lt;- trainByCrossValid(
	data = env,
	resp = 'presBg',
	preds = c('bio1', 'bio12'),
	folds = folds,
	trainFx = trainBRT,
	learningRate = c(0.001, 0.0001), # too few values for reliable model(?)
	treeComplexity = c(2, 4), # too few values for reliable model, but fast
	minTrees = 1000,
	maxTrees = 1500, # too small for reliable model(?), but fast
	tryBy = 'treeComplexity',
	anyway = TRUE, # return models that did not converge
	verbose = 1,
	cores = cores
)

# summarize BRT parameters across best models
summaryByCrossValid(brtx)

## random forests
rfx &lt;- trainByCrossValid(
	data = env,
	resp = 'presBg',
	preds = c('bio1', 'bio12'),
	folds = folds,
	trainFx = trainRF,
	verbose = 1,
	cores = cores
)

# summarize RF parameters in best models
summaryByCrossValid(rfx)


## End(Not run)
</code></pre>

<hr>
<h2 id='trainBRT'>Calibrate a boosted regression tree (generalized boosting machine) model</h2><span id='topic+trainBRT'></span>

<h3>Description</h3>

<p>This function calibrates a boosted regression tree (or gradient boosting machine) model, and is a wrapper for <code><a href="gbm.html#topic+gbm">gbm</a></code>. The function uses a grid search to assess the best combination of learning rate, tree depth, and bag fraction based on cross-validated deviance. If a particular combination of parameters leads to an unconverged model, the script attempts again using slightly different parameters. Its output is any or all of: a table with deviance of evaluated models; all evaluated models; and/or the single model with the lowest deviance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trainBRT(
  data,
  resp = names(data)[1],
  preds = names(data)[2:ncol(data)],
  learningRate = c(1e-04, 0.001, 0.01),
  treeComplexity = c(5, 3, 1),
  bagFraction = 0.6,
  minTrees = 1000,
  maxTrees = 8000,
  tries = 5,
  tryBy = c("learningRate", "treeComplexity", "maxTrees", "stepSize"),
  w = TRUE,
  anyway = FALSE,
  family = "bernoulli",
  out = "model",
  cores = 1,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="trainBRT_+3A_data">data</code></td>
<td>
<p>Data frame.</p>
</td></tr>
<tr><td><code id="trainBRT_+3A_resp">resp</code></td>
<td>
<p>Response variable. This is either the name of the column in <code>data</code> or an integer indicating the column in <code>data</code> that has the response variable. The default is to use the first column in <code>data</code> as the response.</p>
</td></tr>
<tr><td><code id="trainBRT_+3A_preds">preds</code></td>
<td>
<p>Character vector or integer vector. Names of columns or column indices of predictors. The default is to use the second and subsequent columns in <code>data</code>.</p>
</td></tr>
<tr><td><code id="trainBRT_+3A_learningrate">learningRate</code></td>
<td>
<p>Numeric. Learning rate at which model learns from successive trees (Elith et al. 2008 recommend 0.0001 to 0.1).</p>
</td></tr>
<tr><td><code id="trainBRT_+3A_treecomplexity">treeComplexity</code></td>
<td>
<p>Positive integer. Tree complexity: depth of branches in a single tree (1 to 16).</p>
</td></tr>
<tr><td><code id="trainBRT_+3A_bagfraction">bagFraction</code></td>
<td>
<p>Numeric in the range [0, 1]. Bag fraction: proportion of data used for training in cross-validation (Elith et al. 2008 recommend 0.5 to 0.7).</p>
</td></tr>
<tr><td><code id="trainBRT_+3A_mintrees">minTrees</code></td>
<td>
<p>Positive integer. Minimum number of trees to be scored as a &quot;usable&quot; model (Elith et al. 2008 recommend at least 1000). Default is 1000.</p>
</td></tr>
<tr><td><code id="trainBRT_+3A_maxtrees">maxTrees</code></td>
<td>
<p>Positive integer. Maximum number of trees in model set.</p>
</td></tr>
<tr><td><code id="trainBRT_+3A_tries">tries</code></td>
<td>
<p>Integer &gt; 0. Number of times to try to train a model with a particular set of tuning parameters. The function will stop training the first time a model converges (usually on the first attempt). Non-convergence seems to be related to the number of trees tried in each step.  So if non-convergence occurs then the function automatically increases the number of trees in the step size until <code>tries</code> is reached.</p>
</td></tr>
<tr><td><code id="trainBRT_+3A_tryby">tryBy</code></td>
<td>
<p>Character vector. A list that contains one or more of <code>'learningRate'</code>, <code>'treeComplexity'</code>, <code>numTrees</code>, and/or <code>'stepSize'</code>. If a given combination of <code>learningRate</code>, <code>treeComplexity</code>, <code>numTrees</code>, <code>stepSize</code>, and <code>bagFraction</code> do not allow model convergence then then the function tries again but with alterations to any of the arguments named in <code>tryBy</code>:
* <code>learningRate</code>: Decrease the learning rate by a factor of 10.
* <code>treeComplexity</code>: Randomly increase/decrease tree complexity by 1 (minimum of 1).
* <code>maxTrees</code>: Increase number of trees by 20
* <code>stepSize</code>: Increase step size (argument <code>n.trees</code> in <code>gbm.step()</code>) by 50
If <code>tryBy</code> is NULL then the function attempts to train the model with the same parameters up to <code>tries</code> times.</p>
</td></tr>
<tr><td><code id="trainBRT_+3A_w">w</code></td>
<td>
<p>Weights. Any of:
</p>

<ul>
<li> <p><code>TRUE</code>: Causes the total weight of presences to equal the total weight of absences (if <code>family='binomial'</code>)
</p>
</li>
<li> <p><code>FALSE</code>: Each datum is assigned a weight of 1.
</p>
</li>
<li><p> A numeric vector of weights, one per row in <code>data</code>.
</p>
</li>
<li><p> The name of the column in <code>data</code> that contains site weights.
</p>
</li></ul>
</td></tr>
<tr><td><code id="trainBRT_+3A_anyway">anyway</code></td>
<td>
<p>Logical. If <code>FALSE</code> (default), it is possible for no models to be returned if none converge and/or none had a number of trees is &gt;= <code>minTrees</code>). If <code>TRUE</code> then all models are returned but with a warning.</p>
</td></tr>
<tr><td><code id="trainBRT_+3A_family">family</code></td>
<td>
<p>Character. Name of error family.</p>
</td></tr>
<tr><td><code id="trainBRT_+3A_out">out</code></td>
<td>
<p>Character vector. One or more values:
</p>

<ul>
<li>    <p><code>'model'</code>: Model with the lowest deviance.
</p>
</li>
<li>    <p><code>'models'</code>: All models evaluated, sorted from lowest to highest deviance.
</p>
</li>
<li>    <p><code>'tuning'</code>: Data frame with tuning parameters, one row per model, sorted by deviance.
</p>
</li></ul>
</td></tr>
<tr><td><code id="trainBRT_+3A_cores">cores</code></td>
<td>
<p>Integer &gt;= 1. Number of cores to use when calculating multiple models. Default is 1.  If you have issues when <code>cores</code> &gt; 1, please see the <code><a href="#topic+troubleshooting_parallel_operations">troubleshooting_parallel_operations</a></code> guide.</p>
</td></tr>
<tr><td><code id="trainBRT_+3A_verbose">verbose</code></td>
<td>
<p>Logical. If <code>TRUE</code> display progress.</p>
</td></tr>
<tr><td><code id="trainBRT_+3A_...">...</code></td>
<td>
<p>Additional arguments (not used).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The object that is returned depends on the value of the <code>out</code> argument. It can be a model object, a data frame, a list of models, or a list of two or more of these.
</p>


<h3>References</h3>

<p>Elith, J., J.R. Leathwick, &amp; T. Hastie. 2008. A working guide to boosted regression trees. <em>Journal of Animal Ecology</em> 77:802-813. <a href="https://doi.org/10.1111/j.1365-2656.2008.01390.x">doi:10.1111/j.1365-2656.2008.01390.x</a>
</p>


<h3>See Also</h3>

<p><code><a href="gbm.html#topic+gbm">gbm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# NB: The examples below show a very basic modeling workflow. They have been 
# designed to work fast, not produce accurate, defensible models. They can
# take a few minutes to run.

library(mgcv)
library(sf)
library(terra)
set.seed(123)

### setup data
##############

# environmental rasters
rastFile &lt;- system.file('extdata/madClim.tif', package='enmSdmX')
madClim &lt;- rast(rastFile)

# coordinate reference system
wgs84 &lt;- getCRS('WGS84')

# lemur occurrence data
data(lemurs)
occs &lt;- lemurs[lemurs$species == 'Eulemur fulvus', ]
occs &lt;- vect(occs, geom=c('longitude', 'latitude'), crs=wgs84)

occs &lt;- elimCellDuplicates(occs, madClim)

occEnv &lt;- extract(madClim, occs, ID = FALSE)
occEnv &lt;- occEnv[complete.cases(occEnv), ]
	
# create 10000 background sites (or as many as raster can support)
bgEnv &lt;- terra::spatSample(madClim, 20000)
bgEnv &lt;- bgEnv[complete.cases(bgEnv), ]
bgEnv &lt;- bgEnv[1:min(10000, nrow(bgEnv)), ]

# collate occurrences and background sites
presBg &lt;- data.frame(
  presBg = c(
    rep(1, nrow(occEnv)),
    rep(0, nrow(bgEnv))
  )
)

env &lt;- rbind(occEnv, bgEnv)
env &lt;- cbind(presBg, env)

predictors &lt;- c('bio1', 'bio12')

### calibrate models
####################

# Note that all of the trainXYZ functions can made to go faster using the
# "cores" argument (set to just 1, by default). The examples below will not
# go too much faster using more cores because they are simplified, but
# you can try!
cores &lt;- 1

# MaxEnt
mx &lt;- trainMaxEnt(
	data = env,
	resp = 'presBg',
	preds = predictors,
	regMult = 1, # too few values for reliable model, but fast
	verbose = TRUE,
	cores = cores
)

# MaxNet
mn &lt;- trainMaxNet(
	data = env,
	resp = 'presBg',
	preds = predictors,
	regMult = 1, # too few values for reliable model, but fast
	verbose = TRUE,
	cores = cores
)

# generalized linear model (GLM)
gl &lt;- trainGLM(
	data = env,
	resp = 'presBg',
	preds = predictors,
	scale = TRUE, # automatic scaling of predictors
	verbose = TRUE,
	cores = cores
)

# generalized additive model (GAM)
ga &lt;- trainGAM(
	data = env,
	resp = 'presBg',
	preds = predictors,
	verbose = TRUE,
	cores = cores
)

# natural splines
ns &lt;- trainNS(
	data = env,
	resp = 'presBg',
	preds = predictors,
	scale = TRUE, # automatic scaling of predictors
	df = 1:2, # too few values for reliable model(?)
	verbose = TRUE,
	cores = cores
)

# boosted regression trees
envSub &lt;- env[1:1049, ] # subsetting data to run faster
brt &lt;- trainBRT(
	data = envSub,
	resp = 'presBg',
	preds = predictors,
	learningRate = 0.001, # too few values for reliable model(?)
	treeComplexity = c(2, 3), # too few values for reliable model, but fast
	minTrees = 1200, # minimum trees for reliable model(?), but fast
	maxTrees = 1200, # too small for reliable model(?), but fast
	tryBy = 'treeComplexity',
	anyway = TRUE, # return models that did not converge
	verbose = TRUE,
	cores = cores
)

# random forests
rf &lt;- trainRF(
	data = env,
	resp = 'presBg',
	preds = predictors,
	numTrees = c(100, 500), # using at least 500 recommended, but fast!
	verbose = TRUE,
	cores = cores
)

### make maps of models
#######################

# NB We do not have to scale rasters before predicting GLMs and NSs because we
# used the `scale = TRUE` argument in trainGLM() and trainNS().

mxMap &lt;- predictEnmSdm(mx, madClim)
mnMap &lt;- predictEnmSdm(mn, madClim) 
glMap &lt;- predictEnmSdm(gl, madClim)
gaMap &lt;- predictEnmSdm(ga, madClim)
nsMap &lt;- predictEnmSdm(ns, madClim)
brtMap &lt;- predictEnmSdm(brt, madClim)
rfMap &lt;- predictEnmSdm(rf, madClim)

maps &lt;- c(
	mxMap,
	mnMap,
	glMap,
	gaMap,
	nsMap,
	brtMap,
	rfMap
)

names(maps) &lt;- c('MaxEnt', 'MaxNet', 'GLM', 'GAM', 'NSs', 'BRTs', 'RFs')
fun &lt;- function() plot(occs, col='black', pch=3, add=TRUE)
plot(maps, fun = fun, nc = 4)

### compare model responses to BIO12 (mean annual precipitation)
################################################################

# make a data frame holding all other variables at mean across occurrences,
# varying only BIO12
occEnvMeans &lt;- colMeans(occEnv, na.rm=TRUE)
occEnvMeans &lt;- rbind(occEnvMeans)
occEnvMeans &lt;- as.data.frame(occEnvMeans)
climFrame &lt;- occEnvMeans[rep(1, 100), ]
rownames(climFrame) &lt;- NULL

minBio12 &lt;- min(env$bio12)
maxBio12 &lt;- max(env$bio12)
climFrame$bio12 &lt;- seq(minBio12, maxBio12, length.out=100)

predMx &lt;- predictEnmSdm(mx, climFrame)
predMn &lt;- predictEnmSdm(mn, climFrame)
predGl &lt;- predictEnmSdm(gl, climFrame)
predGa &lt;- predictEnmSdm(ga, climFrame)
predNat &lt;- predictEnmSdm(ns, climFrame)
predBrt &lt;- predictEnmSdm(brt, climFrame)
predRf &lt;- predictEnmSdm(rf, climFrame)


plot(climFrame$bio12, predMx,
xlab='BIO12', ylab='Prediction', type='l', ylim=c(0, 1))

lines(climFrame$bio12, predMn, lty='solid', col='red')
lines(climFrame$bio12, predGl, lty='dotted', col='blue')
lines(climFrame$bio12, predGa, lty='dashed', col='green')
lines(climFrame$bio12, predNat, lty=4, col='purple')
lines(climFrame$bio12, predBrt, lty=5, col='orange')
lines(climFrame$bio12, predRf, lty=6, col='cyan')

legend(
   'topleft',
   inset = 0.01,
   legend = c(
	'MaxEnt',
	'MaxNet',
	'GLM',
	'GAM',
	'NS',
	'BRT',
	'RF'
   ),
   lty = c(1, 1:6),
   col = c(
	'black',
	'red',
	'blue',
	'green',
	'purple',
	'orange',
	'cyan'
   ),
   bg = 'white'
)


</code></pre>

<hr>
<h2 id='trainByCrossValid'>Calibrate a distribution/niche model using cross-validation</h2><span id='topic+trainByCrossValid'></span>

<h3>Description</h3>

<p>This function is an extension of any of the <code>trainXYZ</code> functions for calibrating species distribution and ecological niche models. This function uses the <code>trainXYZ</code> function to calibrate and evaluate a suite of models using cross-validation. The models are evaluated against withheld data to determine the optimal settings for a &quot;final&quot; model using all available data. The function returns a set of models and/or a table with statistics on each model. The statistics represent various measures of model accuracy, and are calculated against training and test sites (separately).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trainByCrossValid(
  data,
  resp = names(data)[1],
  preds = names(data)[2:ncol(data)],
  folds = predicts::folds(data),
  trainFx = enmSdmX::trainGLM,
  ...,
  weightEvalTrain = TRUE,
  weightEvalTest = TRUE,
  na.rm = FALSE,
  outputModels = TRUE,
  verbose = 0
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="trainByCrossValid_+3A_data">data</code></td>
<td>
<p>Data frame or matrix. Response variable and environmental predictors (and no other fields) for presences and non-presence sites.</p>
</td></tr>
<tr><td><code id="trainByCrossValid_+3A_resp">resp</code></td>
<td>
<p>Character or integer. Name or column index of response variable. Default is to use the first column in <code>data</code>.</p>
</td></tr>
<tr><td><code id="trainByCrossValid_+3A_preds">preds</code></td>
<td>
<p>Character vector or integer vector. Names of columns or column indices of predictors. Default is to use the second and subsequent columns in <code>data</code> as predictors.</p>
</td></tr>
<tr><td><code id="trainByCrossValid_+3A_folds">folds</code></td>
<td>
<p>Either a numeric vector, or matrix or data frame which specify which rows in <code>data</code> belong to which folds:
</p>

<ul>
<li><p> If a vector, there must be one value per row in <code>data</code>. If there are <em>K</em> unique values in the vector, then <em>K</em> unique models will be trained. Each model will use all of the data except for rows that match a particular value in the <code>folds</code> vector. For example, if <code>folds = c(1, 1, 1, 2, 2, 2, 3, 3, 3)</code>, then three models will be trained, one with all rows that match the 2s and 3s, one with all rows matching 1s and 2s, and one will all rows matching 1s and 3s. The models will be evaluated against the training data and against the withheld data. Use <code>NA</code> to exclude rows from all testing/training. The default is to construct 5 folds of roughly equal size.
</p>
</li>
<li><p> If a matrix or data frame, there must be one row per row in <code>data</code>. Each column corresponds to a different model to be trained. For a given column there should be only two unique values, plus possibly <code>NA</code>s. Of the two values, the lesser value will be used to identify the calibration data and the greater value the evaluation data. Rows with <code>NA</code>s will be ignored and not used in training or testing.  For example, a particular column could contain 1s, 2, and <code>NA</code>s. Data rows corresponding to 1s will be used as training data, data rows corresponding to 2s as test data, and rows with <code>NA</code> are dropped. The <code>NA</code> flag is useful for creating spatially-structured cross-validation folds where training and test sites are separated (spatially) by censored (ignored) data.
</p>
</li></ul>
</td></tr>
<tr><td><code id="trainByCrossValid_+3A_trainfx">trainFx</code></td>
<td>
<p>Function, name of the <code>trainXYZ</code> function to use. Currently the functions/algorithms supported are <code><a href="#topic+trainBRT">trainBRT</a></code>, <code><a href="#topic+trainGAM">trainGAM</a></code>, <code><a href="#topic+trainGLM">trainGLM</a></code>, <code><a href="#topic+trainMaxEnt">trainMaxEnt</a></code>, <code><a href="#topic+trainRF">trainRF</a></code>, and <code><a href="#topic+trainNS">trainNS</a></code>.</p>
</td></tr>
<tr><td><code id="trainByCrossValid_+3A_...">...</code></td>
<td>
<p>Arguments to pass to the &quot;trainXYZ&quot; function.</p>
</td></tr>
<tr><td><code id="trainByCrossValid_+3A_weightevaltrain">weightEvalTrain</code></td>
<td>
<p>Logical, if <code>TRUE</code> (default) and an argument named <code>w</code> is specified in <code>...</code>, then evaluation statistics that support weighting will use the weights specified by <code>w</code> <em>for the &quot;train&quot; version of evaluation statistics</em>. If <code>FALSE</code>, there will be no weighting of sites. Note that this applies <em>only</em> to the calculation of evaluation statistics, not to model calibration.  If <code>w</code> is supplied, they will be used for model calibration.</p>
</td></tr>
<tr><td><code id="trainByCrossValid_+3A_weightevaltest">weightEvalTest</code></td>
<td>
<p>Logical, if <code>TRUE</code> (default) and an argument named <code>w</code> is specified in <code>...</code>, then evaluation statistics that support weighting will use the weights specified by <code>w</code> <em>for the &quot;test&quot; version of evaluation statistics</em>. If <code>FALSE</code>, there will be no weighting of sites. Note that this applies <em>only</em> to the calculation of evaluation statistics.  If <code>w</code> is supplied, they will be used for model calibration.</p>
</td></tr>
<tr><td><code id="trainByCrossValid_+3A_na.rm">na.rm</code></td>
<td>
<p>Logical, if <code>TRUE</code> then remove <code>NA</code> predictions before calculating evaluation statistics. If <code>FALSE</code> (default), propagate <code>NA</code>s (meaning if predictions contain <code>NA</code>s, then the evaluation statistic will most likely also be <code>NA</code>.)</p>
</td></tr>
<tr><td><code id="trainByCrossValid_+3A_outputmodels">outputModels</code></td>
<td>
<p>If <code>TRUE</code>, then return all models (in addition to tables reporting tuning paramaeters and evaluation metrics). <em>WARNING</em>: Depending on the type of model and amount of data, retuning all models may produce objects that are very large in memory.</p>
</td></tr>
<tr><td><code id="trainByCrossValid_+3A_verbose">verbose</code></td>
<td>
<p>Numeric. If 0 show no progress updates. If &gt; 0 then show minimal progress updates for this function only. If &gt; 1 show detailed progress for this function. If &gt; 2 show detailed progress plus detailed progress for the <code>trainXYZ</code> function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In some cases models do not converge (e.g., boosted regression trees and generalized additive models sometimes suffer from this issue). In this case the model will be skipped, but a data frame with the k-fold and model number in the fold will be returned in the $meta element in the output. If no models converged, then this data frame will be empty.
</p>


<h3>Value</h3>

<p>A list object with several named elements:
</p>

<ul>
<li> <p><code>meta</code>: Meta-data on the model call.
</p>
</li>
<li> <p><code>folds</code>: The <code>folds</code> object.
</p>
</li>
<li> <p><code>models</code> (if <code>outputModels</code> is <code>TRUE</code>): A list of model objects, one per data fold.
</p>
</li>
<li> <p><code>tuning</code>: One data frame per k-fold, each containing evaluation statistics for all candidate models in the fold. In addition to algorithm-specific fields, these consist of:
</p>

<ul>
<li> <p><code>'logLoss'</code>: Log loss. Higher (less negative) values imply better fit.
</p>
</li>
<li> <p><code>'cbi'</code>: Continuous Boyce Index (CBI). Calculated with <code><a href="#topic+evalContBoyce">evalContBoyce</a></code>.
</p>
</li>
<li> <p><code>'auc'</code>: Area under the receiver-operator characteristic curve (AUC). Calculated with <code><a href="#topic+evalAUC">evalAUC</a></code>.
</p>
</li>
<li> <p><code>'tss'</code>: Maximum value of the True Skill Statistic. Calculated with <code><a href="#topic+evalTSS">evalTSS</a></code>.
</p>
</li>
<li> <p><code>'msss'</code>: Sensitivity and specificity calculated at the threshold that maximizes sensitivity (true presence prediction rate) plus specificity (true absence prediction rate).
</p>
</li>
<li> <p><code>'mdss'</code>: Sensitivity (se) and specificity (sp) calculated at the threshold that minimizes the difference between sensitivity and specificity.
</p>
</li>
<li> <p><code>'minTrainPres'</code>: Sensitivity (se) and specificity (sp) at the greatest threshold at which all training presences are classified as &quot;present&quot;.
</p>
</li>
<li> <p><code>'trainSe95'</code> and/or <code>'trainSe90'</code>: Sensitivity (se) and specificity (sp) at the threshold that ensures either 95 or 90 percent of all training presences are classified as &quot;present&quot; (training sensitivity = 0.95 or 0.9).
</p>
</li></ul>

</li></ul>



<h3>References</h3>

<p>Fielding, A.H. and J.F. Bell. 1997. A review of methods for the assessment of prediction errors in conservation presence/absence models. <em>Environmental Conservation</em> 24:38-49. <a href="https://doi.org/10.1017/S0376892997000088">doi:10.1017/S0376892997000088</a>
La Rest, K., Pinaud, D., Monestiez, P., Chadoeuf, J., and Bretagnolle, V.  2014.  Spatial leave-one-out cross-validation for variable selection in the presence of spatial autocorrelation. <em>Global Ecology and Biogeography</em> 23:811-820. <a href="https://doi.org/10.1111/geb.12161">doi:10.1111/geb.12161</a>
Radosavljevic, A. and Anderson, R.P.  2014.  Making better Maxent models of species distributions: complexity, overfitting and evaluation.  <em>Journal of Biogeography</em> 41:629-643. <a href="https://doi.org/10.1111/jbi.12227">doi:10.1111/jbi.12227</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+summaryByCrossValid">summaryByCrossValid</a></code>, <code><a href="#topic+trainBRT">trainBRT</a></code>, <code><a href="#topic+trainGAM">trainGAM</a></code>, <code><a href="#topic+trainGLM">trainGLM</a></code>, <code><a href="#topic+trainMaxEnt">trainMaxEnt</a></code>, <code><a href="#topic+trainMaxNet">trainMaxNet</a></code>, <code><a href="#topic+trainNS">trainNS</a></code>, <code><a href="#topic+trainRF">trainRF</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># The example below show a very basic modeling workflow. It has been 
# designed to work fast, not produce accurate, defensible models.
# The general idea is to calibrate a series of models and evaluate them
# against a withheld set of data. One can then use the series of models
# of the top models to better select a "final" model.

## Not run: 
# Running the entire set of commands can take a few minutes. This can
# be sped up by increasing the number of cores used. The examples below use
# one core, but you can change that argument according to your machine's
# capabilities.

library(sf)
library(terra)
set.seed(123)

### setup data
##############

# environmental rasters
rastFile &lt;- system.file('extdata/madClim.tif', package='enmSdmX')
madClim &lt;- rast(rastFile)

# coordinate reference system
wgs84 &lt;- getCRS('WGS84')

# lemur occurrence data
data(lemurs)
occs &lt;- lemurs[lemurs$species == 'Eulemur fulvus', ]
occs &lt;- vect(occs, geom=c('longitude', 'latitude'), crs=wgs84)

occs &lt;- elimCellDuplicates(occs, madClim)

occEnv &lt;- extract(madClim, occs, ID = FALSE)
occEnv &lt;- occEnv[complete.cases(occEnv), ]
	
# create background sites (using just 1000 to speed things up!)
bgEnv &lt;- terra::spatSample(madClim, 3000)
bgEnv &lt;- bgEnv[complete.cases(bgEnv), ]
bgEnv &lt;- bgEnv[sample(nrow(bgEnv), 1000), ]

# collate occurrences and background sites
presBg &lt;- data.frame(
   presBg = c(
      rep(1, nrow(occEnv)),
      rep(0, nrow(bgEnv))
   )
)

env &lt;- rbind(occEnv, bgEnv)
env &lt;- cbind(presBg, env)

predictors &lt;- c('bio1', 'bio12')

# using "vector" form of "folds" argument
folds &lt;- dismo::kfold(env, 3) # just 3 folds (for speed)

### calibrate models
####################

cores &lt;- 1 # increase this to go faster, if your computer handles it

## MaxEnt
mxx &lt;- trainByCrossValid(
	data = env,
	resp = 'presBg',
	preds = c('bio1', 'bio12'),
	folds = folds,
	trainFx = trainMaxEnt,
	regMult = 1:2, # too few values for valid model, but fast!
	verbose = 1,
	cores = cores
)

# summarize MaxEnt feature sets and regularization across folds
summaryByCrossValid(mxx)

## MaxNet
mnx &lt;- trainByCrossValid(
	data = env,
	resp = 'presBg',
	preds = c('bio1', 'bio12'),
	folds = folds,
	trainFx = trainMaxNet,
	regMult = 1:2, # too few values for valid model, but fast!
	verbose = 1,
	cores = cores
)

# summarize MaxEnt feature sets and regularization across folds
summaryByCrossValid(mnx)

## generalized linear models
glx &lt;- trainByCrossValid(
	data = env,
	resp = 'presBg',
	preds = c('bio1', 'bio12'),
	folds = folds,
	trainFx = trainGLM,
	verbose = 1,
	cores = cores
)

# summarize GLM terms in best models
summaryByCrossValid(glx)

## generalized additive models
gax &lt;- trainByCrossValid(
	data = env,
	resp = 'presBg',
	preds = c('bio1', 'bio12'),
	folds = folds,
	trainFx = trainGAM,
	verbose = 1,
	cores = cores
)

# summarize GAM terms in best models
summaryByCrossValid(gax)

## natural splines
nsx &lt;- trainByCrossValid(
	data = env,
	resp = 'presBg',
	preds = c('bio1', 'bio12'),
	folds = folds,
	trainFx = trainNS,
	df = 1:2,
	verbose = 1,
	cores = cores
)

# summarize NS terms in best models
summaryByCrossValid(nsx)

## boosted regression trees
brtx &lt;- trainByCrossValid(
	data = env,
	resp = 'presBg',
	preds = c('bio1', 'bio12'),
	folds = folds,
	trainFx = trainBRT,
	learningRate = c(0.001, 0.0001), # too few values for reliable model(?)
	treeComplexity = c(2, 4), # too few values for reliable model, but fast
	minTrees = 1000,
	maxTrees = 1500, # too small for reliable model(?), but fast
	tryBy = 'treeComplexity',
	anyway = TRUE, # return models that did not converge
	verbose = 1,
	cores = cores
)

# summarize BRT parameters across best models
summaryByCrossValid(brtx)

## random forests
rfx &lt;- trainByCrossValid(
	data = env,
	resp = 'presBg',
	preds = c('bio1', 'bio12'),
	folds = folds,
	trainFx = trainRF,
	verbose = 1,
	cores = cores
)

# summarize RF parameters in best models
summaryByCrossValid(rfx)


## End(Not run)
</code></pre>

<hr>
<h2 id='trainESM'>Calibrate an ensemble of small models</h2><span id='topic+trainESM'></span>

<h3>Description</h3>

<p>This function calibrates a set of &quot;ensembles of small models&quot; (ESM), which are designed for modeling species with few occurrence records. In the original formulation, each model has two covariates interacting additively. Models are calibrated using all possible combinations of covariates. By default, this function does the same, but can also include univariate models, models with two covariates plus their interaction term, and models with quadratic and corresponding linear terms. This function will <em>only</em> train generalized linear models. Extending the types of algorithms is planned!
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trainESM(
  data,
  resp = names(data)[1],
  preds = names(data)[2:ncol(data)],
  univariate = FALSE,
  quadratic = FALSE,
  interaction = FALSE,
  interceptOnly = FALSE,
  method = "glm.fit",
  scale = NA,
  w = TRUE,
  family = stats::binomial(),
  ...,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="trainESM_+3A_data">data</code></td>
<td>
<p>Data frame or matrix. Response variable and environmental predictors (and no other fields) for presences and non-presence sites.</p>
</td></tr>
<tr><td><code id="trainESM_+3A_resp">resp</code></td>
<td>
<p>Character or integer. Name or column index of response variable. Default is to use the first column in <code>data</code>.</p>
</td></tr>
<tr><td><code id="trainESM_+3A_preds">preds</code></td>
<td>
<p>Character vector or integer vector. Names of columns or column indices of predictors. Default is to use the second and subsequent columns in <code>data</code> as predictors.</p>
</td></tr>
<tr><td><code id="trainESM_+3A_univariate">univariate</code>, <code id="trainESM_+3A_quadratic">quadratic</code>, <code id="trainESM_+3A_interaction">interaction</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code>: Whether or not to include univariate models, quadratic models, and/or models with 2-way interactions (default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="trainESM_+3A_interceptonly">interceptOnly</code></td>
<td>
<p>If <code>TRUE</code>, include an intercept-only model (default is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="trainESM_+3A_method">method</code></td>
<td>
<p>Character: Name of function used to solve the GLM. For &quot;normal&quot; GLMs, this can be <code>'glm.fit'</code> (default), <code>'brglmFit'</code> (from the <span class="pkg">brglm2</span> package), or another function.</p>
</td></tr>
<tr><td><code id="trainESM_+3A_scale">scale</code></td>
<td>
<p>Either <code>NA</code> (default), or <code>TRUE</code> or <code>FALSE</code>. If <code>TRUE</code>, the predictors will be centered and scaled by dividing by subtracting their means then dividing by their standard deviations. The means and standard deviations will be returned in the model object under an element named &quot;<code>scales</code>&quot;. For example, if you do something like <code>model &lt;- trainGLM(data, scale=TRUE)</code>, then you can get the means and standard deviations using <code>model$scales$mean</code> and <code>model$scales$sd</code>. If <code>FALSE</code>, no scaling is done. If <code>NA</code> (default), then the function will check to see if non-factor predictors have means ~0 and standard deviations ~1. If not, then a warning will be printed, but the function will continue to do its operations.</p>
</td></tr>
<tr><td><code id="trainESM_+3A_w">w</code></td>
<td>
<p>Weights. Any of:
</p>

<ul>
<li> <p><code>TRUE</code>: Causes the total weight of presences to equal the total weight of absences (if <code>family='binomial'</code>)
</p>
</li>
<li> <p><code>FALSE</code>: Each datum is assigned a weight of 1.
</p>
</li>
<li><p> A numeric vector of weights, one per row in <code>data</code>.
</p>
</li>
<li><p> The name of the column in <code>data</code> that contains site weights.
</p>
</li></ul>
</td></tr>
<tr><td><code id="trainESM_+3A_family">family</code></td>
<td>
<p>Character or function. Name of family for data error structure (see <code><a href="stats.html#topic+family">family</a></code>). Default is to use the 'binomial' family.</p>
</td></tr>
<tr><td><code id="trainESM_+3A_...">...</code></td>
<td>
<p>Arguments to pass to <code><a href="stats.html#topic+glm">glm</a></code></p>
</td></tr>
<tr><td><code id="trainESM_+3A_verbose">verbose</code></td>
<td>
<p>Logical. If <code>TRUE</code> then display progress.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list object with several named elements:
</p>

<ul>
<li> <p><code>models</code>: A list with each ESM model.
</p>
</li>
<li> <p><code>tuning</code>: A <code>data.frame</code> with one row per model, in the order as they appear in <code>$models</code>.
</p>
</li></ul>



<h3>References</h3>

<p>Breiner, F.T., Guisan, A., Bergamini, A., and Nobis, M.P.  2015.  Overcoming limitations of modeling rare species by using ensembles of small models.  <em>Methods in Ecology and Evolution</em> 6:1210-1218.. <a href="https://doi.org/10.1111/2041-210X.12403">doi:10.1111/2041-210X.12403</a>
Lomba, A., L. Pellissier, C. Randin, J. Vicente, J. Horondo, and A. Guisan.  2010.  Overcoming the rare species modeling complex: A novel hierarchical framework applied to an Iberian endemic plant. <em>Biological Conservation</em> 143:2647-2657. <a href="https://doi.org/10.1016/j.biocon.2010.07.007">doi:10.1016/j.biocon.2010.07.007</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+trainBRT">trainBRT</a></code>, <code><a href="#topic+trainGAM">trainGAM</a></code>, <code><a href="#topic+trainGLM">trainGLM</a></code>, <code><a href="#topic+trainMaxEnt">trainMaxEnt</a></code>, <code><a href="#topic+trainMaxNet">trainMaxNet</a></code>, <code><a href="#topic+trainNS">trainNS</a></code>, <code><a href="#topic+trainRF">trainRF</a></code>, <code><a href="#topic+trainByCrossValid">trainByCrossValid</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># NB: The examples below show a very basic modeling workflow. They have been 
# designed to work fast, not produce accurate, defensible models. They can
# take a few minutes to run.

library(terra)
set.seed(123)

### setup data
##############

# environmental rasters
rastFile &lt;- system.file('extdata/madClim.tif', package='enmSdmX')
madClim &lt;- rast(rastFile)

# coordinate reference system
wgs84 &lt;- getCRS('WGS84')

# lemur occurrence data
data(lemurs)
occs &lt;- lemurs[lemurs$species == 'Eulemur fulvus', ]
occs &lt;- vect(occs, geom=c('longitude', 'latitude'), crs=wgs84)

occs &lt;- elimCellDuplicates(occs, madClim)

occEnv &lt;- extract(madClim, occs, ID = FALSE)
occEnv &lt;- occEnv[complete.cases(occEnv), ]
	
# create 10000 background sites (or as many as raster can support)
bgEnv &lt;- terra::spatSample(madClim, 20000)
bgEnv &lt;- bgEnv[complete.cases(bgEnv), ]
bgEnv &lt;- bgEnv[1:min(10000, nrow(bgEnv)), ]

# collate occurrences and background sites
presBg &lt;- data.frame(
  presBg = c(
    rep(1, nrow(occEnv)),
    rep(0, nrow(bgEnv))
  )
)

env &lt;- rbind(occEnv, bgEnv)
env &lt;- cbind(presBg, env)

predictors &lt;- c('bio1', 'bio12')

### calibrate models
####################

# "traditional" ESMs with just 2 linear predictors
# just one model in this case because we have just 2 predictors
esm1 &lt;- trainESM(
   data = env,
   resp = 'presBg',
   preds = predictors,
   family = stats::binomial(),
   scale = TRUE,
   w = TRUE
)

str(esm1, 1)
esm1$tuning

# extended ESM with other kinds of terms
esm2 &lt;- trainESM(
   data = env,
   resp = 'presBg',
   preds = predictors,
   univariate = TRUE,
   quadratic = TRUE,
   interaction = TRUE,
   interceptOnly = TRUE,
   family = stats::binomial(),
   scale = TRUE,
   w = TRUE,
   verbose = TRUE
)

str(esm2, 1)
esm2$tuning

### make a set of predictions to rasters
########################################

# center environmental rasters and divide by their SD
madClimScaled &lt;- scale(madClim, center = esm2$scale$mean, scale = esm2$scale$sd)

# make one raster per model
predictions &lt;- list()
for (i in 1:length(esm2$models)) {
    predictions[[i]] &lt;- predict(madClimScaled, esm2$models[[i]], type = 'response')
}

# combine into a "stack"
predictions &lt;- do.call(c, predictions)
names(predictions) &lt;- esm2$tuning$model
plot(predictions)

# calculate (unweighted) mean
prediction &lt;- mean(predictions)
plot(prediction)
plot(occs, pch = 1, add = TRUE)
</code></pre>

<hr>
<h2 id='trainGAM'>Calibrate a generalized additive model (GAM)</h2><span id='topic+trainGAM'></span>

<h3>Description</h3>

<p>This function constructs a generalized additive model. By default, the model is constructed in a two-stage process.  First, the &quot;construct&quot; phase generates a series of simple models with univariate and bivariate interaction terms. These simple models are then ranked based on their AICc. Second, the &quot;select&quot; phase creates a &quot;full&quot; model from the simple models such that there is at least <code>presPerTermInitial</code> presences (if the response is binary) or data rows (if not) for each smooth term to be estimated (not counting the intercept). Finally, it selects the best model using AICc from all possible subsets of this &quot;full&quot; model. Its output is any or all of: a table with AICc for all evaluated models; all models evaluated in the &quot;selection&quot; phase; and/or the single model with the lowest AICc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trainGAM(
  data,
  resp = names(data)[1],
  preds = names(data)[2:ncol(data)],
  gamma = 1,
  scale = 0,
  smoothingBasis = "cs",
  interaction = "te",
  interceptOnly = TRUE,
  construct = TRUE,
  select = TRUE,
  presPerTermInitial = 10,
  presPerTermFinal = 10,
  maxTerms = 8,
  w = TRUE,
  family = "binomial",
  out = "model",
  cores = 1,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="trainGAM_+3A_data">data</code></td>
<td>
<p>Data frame.</p>
</td></tr>
<tr><td><code id="trainGAM_+3A_resp">resp</code></td>
<td>
<p>Response variable. This is either the name of the column in <code>data</code> or an integer indicating the column in <code>data</code> that has the response variable. The default is to use the first column in <code>data</code> as the response.</p>
</td></tr>
<tr><td><code id="trainGAM_+3A_preds">preds</code></td>
<td>
<p>Character vector or integer vector. Names of columns or column indices of predictors. The default is to use the second and subsequent columns in <code>data</code>.</p>
</td></tr>
<tr><td><code id="trainGAM_+3A_gamma">gamma</code></td>
<td>
<p>Initial penalty to degrees of freedom to use (larger ==&gt; smoother fits).</p>
</td></tr>
<tr><td><code id="trainGAM_+3A_scale">scale</code></td>
<td>
<p>A numeric value indicating the &quot;scale&quot; parameter (see argument <code>scale</code> in <code><a href="mgcv.html#topic+gam">gam</a></code>). The default is 0 (which allows a single smoother for Poisson and binomial error families and unknown scale for all others.)</p>
</td></tr>
<tr><td><code id="trainGAM_+3A_smoothingbasis">smoothingBasis</code></td>
<td>
<p>Character. Indicates the type of smoothing basis. The default is <code>'cs'</code> (cubic splines), but see <code><a href="mgcv.html#topic+smooth.terms">smooth.terms</a></code> for other options. This is the value of argument <code>bs</code> in a <code><a href="mgcv.html#topic+s">s</a></code> function.</p>
</td></tr>
<tr><td><code id="trainGAM_+3A_interaction">interaction</code></td>
<td>
<p>Character or <code>NULL</code>. Type of interaction term to use (<code>te</code>, <code>ts</code>, <code>s</code>, etc.). See <code>?te</code> (for example) for help on any one of these. If <code>NULL</code>, then interactions are not used.</p>
</td></tr>
<tr><td><code id="trainGAM_+3A_interceptonly">interceptOnly</code></td>
<td>
<p>If <code>TRUE</code> (default) and model selection is enabled, then include an intercept-only model.</p>
</td></tr>
<tr><td><code id="trainGAM_+3A_construct">construct</code></td>
<td>
<p>If <code>TRUE</code> (default), then construct the model by computing AICc for all univariate and bivariate models. Then add terms up to maximum set by <code>presPerTermInitial</code> and <code>maxTerms</code>.</p>
</td></tr>
<tr><td><code id="trainGAM_+3A_select">select</code></td>
<td>
<p>If <code>TRUE</code> (default), then calculate AICc for all possible subsets of models and return the model with the lowest AICc of these. This step if performed <em>after</em> model construction (if <code>construct</code> is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="trainGAM_+3A_presperterminitial">presPerTermInitial</code></td>
<td>
<p>Positive integer. Minimum number of presences needed per model term for a term to be included in the model construction stage. Used only if <code>construct</code> is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="trainGAM_+3A_prespertermfinal">presPerTermFinal</code></td>
<td>
<p>Positive integer. Minimum number of presence sites per term in initial starting model; used only if <code>select</code> is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="trainGAM_+3A_maxterms">maxTerms</code></td>
<td>
<p>Maximum number of terms to be used in any model, not including the intercept (default is 8). Used only if <code>construct</code> is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="trainGAM_+3A_w">w</code></td>
<td>
<p>Weights. Any of:
</p>

<ul>
<li> <p><code>TRUE</code>: Causes the total weight of presences to equal the total weight of absences (if <code>family='binomial'</code>)
</p>
</li>
<li> <p><code>FALSE</code>: Each datum is assigned a weight of 1.
</p>
</li>
<li><p> A numeric vector of weights, one per row in <code>data</code>.
</p>
</li>
<li><p> The name of the column in <code>data</code> that contains site weights.
</p>
</li></ul>
</td></tr>
<tr><td><code id="trainGAM_+3A_family">family</code></td>
<td>
<p>Name of family for data error structure (see <code>?family</code>).</p>
</td></tr>
<tr><td><code id="trainGAM_+3A_out">out</code></td>
<td>
<p>Character vector. One or more values:
</p>

<ul>
<li>    <p><code>'model'</code>: Model with the lowest AICc.
</p>
</li>
<li>    <p><code>'models'</code>: All models evaluated, sorted from lowest to highest AICc (lowest is best).
</p>
</li>
<li>    <p><code>'tuning'</code>: Data frame with tuning parameters, one row per model, sorted by AICc.
</p>
</li></ul>
</td></tr>
<tr><td><code id="trainGAM_+3A_cores">cores</code></td>
<td>
<p>Integer &gt;= 1. Number of cores to use when calculating multiple models. Default is 1. If you have issues when <code>cores</code> &gt; 1, please see the <code><a href="#topic+troubleshooting_parallel_operations">troubleshooting_parallel_operations</a></code> guide.</p>
</td></tr>
<tr><td><code id="trainGAM_+3A_verbose">verbose</code></td>
<td>
<p>Logical. If <code>TRUE</code> then display intermediate results on the display device.</p>
</td></tr>
<tr><td><code id="trainGAM_+3A_...">...</code></td>
<td>
<p>Extra arguments (not used).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The object that is returned depends on the value of the <code>out</code> argument. It can be a model object, a data frame, a list of models, or a list of all two or more of these.
</p>


<h3>See Also</h3>

<p><code><a href="mgcv.html#topic+gam">gam</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# NB: The examples below show a very basic modeling workflow. They have been 
# designed to work fast, not produce accurate, defensible models. They can
# take a few minutes to run.

library(mgcv)
library(sf)
library(terra)
set.seed(123)

### setup data
##############

# environmental rasters
rastFile &lt;- system.file('extdata/madClim.tif', package='enmSdmX')
madClim &lt;- rast(rastFile)

# coordinate reference system
wgs84 &lt;- getCRS('WGS84')

# lemur occurrence data
data(lemurs)
occs &lt;- lemurs[lemurs$species == 'Eulemur fulvus', ]
occs &lt;- vect(occs, geom=c('longitude', 'latitude'), crs=wgs84)

occs &lt;- elimCellDuplicates(occs, madClim)

occEnv &lt;- extract(madClim, occs, ID = FALSE)
occEnv &lt;- occEnv[complete.cases(occEnv), ]
	
# create 10000 background sites (or as many as raster can support)
bgEnv &lt;- terra::spatSample(madClim, 20000)
bgEnv &lt;- bgEnv[complete.cases(bgEnv), ]
bgEnv &lt;- bgEnv[1:min(10000, nrow(bgEnv)), ]

# collate occurrences and background sites
presBg &lt;- data.frame(
  presBg = c(
    rep(1, nrow(occEnv)),
    rep(0, nrow(bgEnv))
  )
)

env &lt;- rbind(occEnv, bgEnv)
env &lt;- cbind(presBg, env)

predictors &lt;- c('bio1', 'bio12')

### calibrate models
####################

# Note that all of the trainXYZ functions can made to go faster using the
# "cores" argument (set to just 1, by default). The examples below will not
# go too much faster using more cores because they are simplified, but
# you can try!
cores &lt;- 1

# MaxEnt
mx &lt;- trainMaxEnt(
	data = env,
	resp = 'presBg',
	preds = predictors,
	regMult = 1, # too few values for reliable model, but fast
	verbose = TRUE,
	cores = cores
)

# MaxNet
mn &lt;- trainMaxNet(
	data = env,
	resp = 'presBg',
	preds = predictors,
	regMult = 1, # too few values for reliable model, but fast
	verbose = TRUE,
	cores = cores
)

# generalized linear model (GLM)
gl &lt;- trainGLM(
	data = env,
	resp = 'presBg',
	preds = predictors,
	scale = TRUE, # automatic scaling of predictors
	verbose = TRUE,
	cores = cores
)

# generalized additive model (GAM)
ga &lt;- trainGAM(
	data = env,
	resp = 'presBg',
	preds = predictors,
	verbose = TRUE,
	cores = cores
)

# natural splines
ns &lt;- trainNS(
	data = env,
	resp = 'presBg',
	preds = predictors,
	scale = TRUE, # automatic scaling of predictors
	df = 1:2, # too few values for reliable model(?)
	verbose = TRUE,
	cores = cores
)

# boosted regression trees
envSub &lt;- env[1:1049, ] # subsetting data to run faster
brt &lt;- trainBRT(
	data = envSub,
	resp = 'presBg',
	preds = predictors,
	learningRate = 0.001, # too few values for reliable model(?)
	treeComplexity = c(2, 3), # too few values for reliable model, but fast
	minTrees = 1200, # minimum trees for reliable model(?), but fast
	maxTrees = 1200, # too small for reliable model(?), but fast
	tryBy = 'treeComplexity',
	anyway = TRUE, # return models that did not converge
	verbose = TRUE,
	cores = cores
)

# random forests
rf &lt;- trainRF(
	data = env,
	resp = 'presBg',
	preds = predictors,
	numTrees = c(100, 500), # using at least 500 recommended, but fast!
	verbose = TRUE,
	cores = cores
)

### make maps of models
#######################

# NB We do not have to scale rasters before predicting GLMs and NSs because we
# used the `scale = TRUE` argument in trainGLM() and trainNS().

mxMap &lt;- predictEnmSdm(mx, madClim)
mnMap &lt;- predictEnmSdm(mn, madClim) 
glMap &lt;- predictEnmSdm(gl, madClim)
gaMap &lt;- predictEnmSdm(ga, madClim)
nsMap &lt;- predictEnmSdm(ns, madClim)
brtMap &lt;- predictEnmSdm(brt, madClim)
rfMap &lt;- predictEnmSdm(rf, madClim)

maps &lt;- c(
	mxMap,
	mnMap,
	glMap,
	gaMap,
	nsMap,
	brtMap,
	rfMap
)

names(maps) &lt;- c('MaxEnt', 'MaxNet', 'GLM', 'GAM', 'NSs', 'BRTs', 'RFs')
fun &lt;- function() plot(occs, col='black', pch=3, add=TRUE)
plot(maps, fun = fun, nc = 4)

### compare model responses to BIO12 (mean annual precipitation)
################################################################

# make a data frame holding all other variables at mean across occurrences,
# varying only BIO12
occEnvMeans &lt;- colMeans(occEnv, na.rm=TRUE)
occEnvMeans &lt;- rbind(occEnvMeans)
occEnvMeans &lt;- as.data.frame(occEnvMeans)
climFrame &lt;- occEnvMeans[rep(1, 100), ]
rownames(climFrame) &lt;- NULL

minBio12 &lt;- min(env$bio12)
maxBio12 &lt;- max(env$bio12)
climFrame$bio12 &lt;- seq(minBio12, maxBio12, length.out=100)

predMx &lt;- predictEnmSdm(mx, climFrame)
predMn &lt;- predictEnmSdm(mn, climFrame)
predGl &lt;- predictEnmSdm(gl, climFrame)
predGa &lt;- predictEnmSdm(ga, climFrame)
predNat &lt;- predictEnmSdm(ns, climFrame)
predBrt &lt;- predictEnmSdm(brt, climFrame)
predRf &lt;- predictEnmSdm(rf, climFrame)


plot(climFrame$bio12, predMx,
xlab='BIO12', ylab='Prediction', type='l', ylim=c(0, 1))

lines(climFrame$bio12, predMn, lty='solid', col='red')
lines(climFrame$bio12, predGl, lty='dotted', col='blue')
lines(climFrame$bio12, predGa, lty='dashed', col='green')
lines(climFrame$bio12, predNat, lty=4, col='purple')
lines(climFrame$bio12, predBrt, lty=5, col='orange')
lines(climFrame$bio12, predRf, lty=6, col='cyan')

legend(
   'topleft',
   inset = 0.01,
   legend = c(
	'MaxEnt',
	'MaxNet',
	'GLM',
	'GAM',
	'NS',
	'BRT',
	'RF'
   ),
   lty = c(1, 1:6),
   col = c(
	'black',
	'red',
	'blue',
	'green',
	'purple',
	'orange',
	'cyan'
   ),
   bg = 'white'
)


</code></pre>

<hr>
<h2 id='trainGLM'>Calibrate a generalized linear model (GLM)</h2><span id='topic+trainGLM'></span>

<h3>Description</h3>

<p>This function constructs a generalized linear model. By default, the model is constructed in a two-stage process.  First, the &quot;construct&quot; phase generates a series of simple models with univariate, quadratic, or 2-way-interaction terms. These simple models are then ranked based on their AICc. Second, the &quot;select&quot; phase creates a &quot;full&quot; model from the simple models such that there is at least <code>presPerTermInitial</code> presences (if the response is binary) or data rows (if not) for each coefficient to be estimated (not counting the intercept). Finally, it selects the best model using AICc from all possible subsets of this &quot;full&quot; model, while respecting marginality (i.e., all lower-order terms of higher-order terms appear in the model).
</p>
<p>The function outputs any or all of: a table with AICc for all evaluated models; all models evaluated in the &quot;selection&quot; phase; and/or the single model with the lowest AICc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trainGLM(
  data,
  resp = names(data)[1],
  preds = names(data)[2:ncol(data)],
  scale = NA,
  construct = TRUE,
  select = TRUE,
  quadratic = TRUE,
  interaction = TRUE,
  interceptOnly = TRUE,
  method = "glm.fit",
  presPerTermInitial = 10,
  presPerTermFinal = 10,
  maxTerms = 8,
  w = TRUE,
  family = stats::binomial(),
  removeInvalid = TRUE,
  failIfNoValid = TRUE,
  out = "model",
  cores = 1,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="trainGLM_+3A_data">data</code></td>
<td>
<p>Data frame.</p>
</td></tr>
<tr><td><code id="trainGLM_+3A_resp">resp</code></td>
<td>
<p>Response variable. This is either the name of the column in <code>data</code> or an integer indicating the column in <code>data</code> that has the response variable. The default is to use the first column in <code>data</code> as the response.</p>
</td></tr>
<tr><td><code id="trainGLM_+3A_preds">preds</code></td>
<td>
<p>Character vector or integer vector. Names of columns or column indices of predictors. The default is to use the second and subsequent columns in <code>data</code>.</p>
</td></tr>
<tr><td><code id="trainGLM_+3A_scale">scale</code></td>
<td>
<p>Either <code>NA</code> (default), or <code>TRUE</code> or <code>FALSE</code>. If <code>TRUE</code>, the predictors will be centered and scaled by dividing by subtracting their means then dividing by their standard deviations. The means and standard deviations will be returned in the model object under an element named &quot;<code>scales</code>&quot;. For example, if you do something like <code>model &lt;- trainGLM(data, scale=TRUE)</code>, then you can get the means and standard deviations using <code>model$scales$mean</code> and <code>model$scales$sd</code>. If <code>FALSE</code>, no scaling is done. If <code>NA</code> (default), then the function will check to see if non-factor predictors have means ~0 and standard deviations ~1. If not, then a warning will be printed, but the function will continue to do its operations.</p>
</td></tr>
<tr><td><code id="trainGLM_+3A_construct">construct</code></td>
<td>
<p>Logical. If <code>TRUE</code> (default) then construct model from individual terms entered in order from lowest to highest AICc up to limits set by <code>presPerTermInitial</code> or <code>maxTerms</code> is met. If <code>FALSE</code> then the &quot;full&quot; model consists of all terms allowed by <code>quadratic</code> and <code>interaction</code>.</p>
</td></tr>
<tr><td><code id="trainGLM_+3A_select">select</code></td>
<td>
<p>Logical. If <code>TRUE</code> (default) then calculate AICc for all possible subsets of models and return the model with the lowest AICc of these. This step if performed <em>after</em> model construction (if any).</p>
</td></tr>
<tr><td><code id="trainGLM_+3A_quadratic">quadratic</code></td>
<td>
<p>Logical. Used only if <code>construct</code> is <code>TRUE</code>. If <code>TRUE</code> (default) then include quadratic terms in model construction stage for non-factor predictors.</p>
</td></tr>
<tr><td><code id="trainGLM_+3A_interaction">interaction</code></td>
<td>
<p>Logical. Used only if <code>construct</code> is <code>TRUE</code>. If <code>TRUE</code> (default) then include 2-way interaction terms (including interactions between factor predictors).</p>
</td></tr>
<tr><td><code id="trainGLM_+3A_interceptonly">interceptOnly</code></td>
<td>
<p>If <code>TRUE</code> (default) and model selection is enabled, then include an intercept-only model.</p>
</td></tr>
<tr><td><code id="trainGLM_+3A_method">method</code></td>
<td>
<p>Character: Name of function used to solve the GLM. For &quot;normal&quot; GLMs, this can be <code>'glm.fit'</code> (default), <code>'brglmFit'</code> (from the <span class="pkg">brglm2</span> package), or another function.</p>
</td></tr>
<tr><td><code id="trainGLM_+3A_presperterminitial">presPerTermInitial</code></td>
<td>
<p>Positive integer. Minimum number of presences needed per model term for a term to be included in the model construction stage. Used only is <code>construct</code> is TRUE.</p>
</td></tr>
<tr><td><code id="trainGLM_+3A_prespertermfinal">presPerTermFinal</code></td>
<td>
<p>Positive integer. Minimum number of presence sites per term in initial starting model. Used only if <code>select</code> is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="trainGLM_+3A_maxterms">maxTerms</code></td>
<td>
<p>Maximum number of terms to be used in any model, not including the intercept (default is 8). Used only if <code>construct</code> is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="trainGLM_+3A_w">w</code></td>
<td>
<p>Weights. Any of:
</p>

<ul>
<li> <p><code>TRUE</code>: Causes the total weight of presences to equal the total weight of absences (if <code>family='binomial'</code>)
</p>
</li>
<li> <p><code>FALSE</code>: Each datum is assigned a weight of 1.
</p>
</li>
<li><p> A numeric vector of weights, one per row in <code>data</code>.
</p>
</li>
<li><p> The name of the column in <code>data</code> that contains site weights.
</p>
</li></ul>
</td></tr>
<tr><td><code id="trainGLM_+3A_family">family</code></td>
<td>
<p>Name of family for data error structure (see <code><a href="stats.html#topic+family">family</a></code>). Default is to use the 'binomial' family.</p>
</td></tr>
<tr><td><code id="trainGLM_+3A_removeinvalid">removeInvalid</code></td>
<td>
<p>Logical. If <code>TRUE</code> (default), remove models that either did not converge or have parameter estimates near the boundaries (usually negative or positive infinity). If you run this function with 'construct = TRUE' (i.e., construct a &quot;full&quot; model from the best &quot;small&quot; models), then any small model that either did not converge or had parameters that are near the boundary (usually negative or positive infinity) are removed from consideration as terms in &quot;full&quot; model.</p>
</td></tr>
<tr><td><code id="trainGLM_+3A_failifnovalid">failIfNoValid</code></td>
<td>
<p>Logical. If <code>TRUE</code> (default), and the &quot;full&quot; model either does not converge or has parameters near the boundary, then the function will fail. If <code>FALSE</code>, then return <code>NULL</code> in this case.</p>
</td></tr>
<tr><td><code id="trainGLM_+3A_out">out</code></td>
<td>
<p>Character vector. One or more values:
</p>

<ul>
<li>    <p><code>'model'</code>: Model with the lowest AICc.
</p>
</li>
<li>    <p><code>'models'</code>: All models evaluated, sorted from lowest to highest AICc (lowest is best).
</p>
</li>
<li>    <p><code>'tuning'</code>: Data frame with tuning parameters, one row per model, sorted by AICc.
</p>
</li></ul>
</td></tr>
<tr><td><code id="trainGLM_+3A_cores">cores</code></td>
<td>
<p>Integer &gt;= 1. Number of cores to use when calculating multiple models. Default is 1. If you have issues when <code>cores</code> &gt; 1, please see the <code><a href="#topic+troubleshooting_parallel_operations">troubleshooting_parallel_operations</a></code> guide.</p>
</td></tr>
<tr><td><code id="trainGLM_+3A_verbose">verbose</code></td>
<td>
<p>Logical. If <code>TRUE</code> then display progress.</p>
</td></tr>
<tr><td><code id="trainGLM_+3A_...">...</code></td>
<td>
<p>Arguments to pass to <code>glm</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is designed to find the most parsimonious model given the amount of calibration data that is available to it. 'trainGLM()' can work with any data, but has been designed to work specifically as a species distribution model where the response is either binary (default) or abundance. Specifically, it 1) identifies the most parsimonious model (lowest AICc) with 2) optimal flexibility (optimal degrees of freedom in splines) and 3) allows for (but does not require) interaction terms between predictors (if desired). If the defaults are used, the following procedure is applied:
</p>

<ul>
<li><p> Constructing a set of simple model terms, each with 1 to 4 degrees of freedom. Terms can be univariate or bilabiate (two-way interactions). Predictors can be continuous or factors. If any simple models has convergence issues or boundary issues (coefficients that approach negative or positive infinity), it is removed.
</p>
</li>
<li><p> Constructing a series of models, each with one of the terms, then using the models to rank terms by AICc.
</p>
</li>
<li><p> From the top set of terms, creating a &quot;full&quot; model. The full model will ensure the maximum number of terms is &lt;= 'maxTerms', and that for each term, there are at least 'presPerTermFinal' data points.
</p>
</li>
<li><p> All possible submodels, plus the full model, are evaluated and ranked by AICc.  If a model has convergence or boundary issues, it is removed from the set. The most parsimonious model (lowest AICc) is returned.
</p>
</li></ul>



<h3>Value</h3>

<p>The object that is returned depends on the value of the <code>out</code> argument. It can be a model object, a data frame, a list of models, or a list of all two or more of these. If <code>scale</code> is <code>TRUE</code>, any model object will also have an element named <code>$scale</code>, which contains the means and standard deviations for predictors that are not factors. The data frame reports the AICc for all of the models evaluated, sorted by best to worst. The <code>converged</code> column indicates whether the model converged (&quot;<code>TRUE</code>&quot; is good), and the <code>boundary</code> column whether the model parameters are near the boundary (usually, negative or positive infinity; &quot;<code>FALSE</code>&quot; is good).
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+glm">glm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# NB: The examples below show a very basic modeling workflow. They have been 
# designed to work fast, not produce accurate, defensible models. They can
# take a few minutes to run.

library(mgcv)
library(sf)
library(terra)
set.seed(123)

### setup data
##############

# environmental rasters
rastFile &lt;- system.file('extdata/madClim.tif', package='enmSdmX')
madClim &lt;- rast(rastFile)

# coordinate reference system
wgs84 &lt;- getCRS('WGS84')

# lemur occurrence data
data(lemurs)
occs &lt;- lemurs[lemurs$species == 'Eulemur fulvus', ]
occs &lt;- vect(occs, geom=c('longitude', 'latitude'), crs=wgs84)

occs &lt;- elimCellDuplicates(occs, madClim)

occEnv &lt;- extract(madClim, occs, ID = FALSE)
occEnv &lt;- occEnv[complete.cases(occEnv), ]
	
# create 10000 background sites (or as many as raster can support)
bgEnv &lt;- terra::spatSample(madClim, 20000)
bgEnv &lt;- bgEnv[complete.cases(bgEnv), ]
bgEnv &lt;- bgEnv[1:min(10000, nrow(bgEnv)), ]

# collate occurrences and background sites
presBg &lt;- data.frame(
  presBg = c(
    rep(1, nrow(occEnv)),
    rep(0, nrow(bgEnv))
  )
)

env &lt;- rbind(occEnv, bgEnv)
env &lt;- cbind(presBg, env)

predictors &lt;- c('bio1', 'bio12')

### calibrate models
####################

# Note that all of the trainXYZ functions can made to go faster using the
# "cores" argument (set to just 1, by default). The examples below will not
# go too much faster using more cores because they are simplified, but
# you can try!
cores &lt;- 1

# MaxEnt
mx &lt;- trainMaxEnt(
	data = env,
	resp = 'presBg',
	preds = predictors,
	regMult = 1, # too few values for reliable model, but fast
	verbose = TRUE,
	cores = cores
)

# MaxNet
mn &lt;- trainMaxNet(
	data = env,
	resp = 'presBg',
	preds = predictors,
	regMult = 1, # too few values for reliable model, but fast
	verbose = TRUE,
	cores = cores
)

# generalized linear model (GLM)
gl &lt;- trainGLM(
	data = env,
	resp = 'presBg',
	preds = predictors,
	scale = TRUE, # automatic scaling of predictors
	verbose = TRUE,
	cores = cores
)

# generalized additive model (GAM)
ga &lt;- trainGAM(
	data = env,
	resp = 'presBg',
	preds = predictors,
	verbose = TRUE,
	cores = cores
)

# natural splines
ns &lt;- trainNS(
	data = env,
	resp = 'presBg',
	preds = predictors,
	scale = TRUE, # automatic scaling of predictors
	df = 1:2, # too few values for reliable model(?)
	verbose = TRUE,
	cores = cores
)

# boosted regression trees
envSub &lt;- env[1:1049, ] # subsetting data to run faster
brt &lt;- trainBRT(
	data = envSub,
	resp = 'presBg',
	preds = predictors,
	learningRate = 0.001, # too few values for reliable model(?)
	treeComplexity = c(2, 3), # too few values for reliable model, but fast
	minTrees = 1200, # minimum trees for reliable model(?), but fast
	maxTrees = 1200, # too small for reliable model(?), but fast
	tryBy = 'treeComplexity',
	anyway = TRUE, # return models that did not converge
	verbose = TRUE,
	cores = cores
)

# random forests
rf &lt;- trainRF(
	data = env,
	resp = 'presBg',
	preds = predictors,
	numTrees = c(100, 500), # using at least 500 recommended, but fast!
	verbose = TRUE,
	cores = cores
)

### make maps of models
#######################

# NB We do not have to scale rasters before predicting GLMs and NSs because we
# used the `scale = TRUE` argument in trainGLM() and trainNS().

mxMap &lt;- predictEnmSdm(mx, madClim)
mnMap &lt;- predictEnmSdm(mn, madClim) 
glMap &lt;- predictEnmSdm(gl, madClim)
gaMap &lt;- predictEnmSdm(ga, madClim)
nsMap &lt;- predictEnmSdm(ns, madClim)
brtMap &lt;- predictEnmSdm(brt, madClim)
rfMap &lt;- predictEnmSdm(rf, madClim)

maps &lt;- c(
	mxMap,
	mnMap,
	glMap,
	gaMap,
	nsMap,
	brtMap,
	rfMap
)

names(maps) &lt;- c('MaxEnt', 'MaxNet', 'GLM', 'GAM', 'NSs', 'BRTs', 'RFs')
fun &lt;- function() plot(occs, col='black', pch=3, add=TRUE)
plot(maps, fun = fun, nc = 4)

### compare model responses to BIO12 (mean annual precipitation)
################################################################

# make a data frame holding all other variables at mean across occurrences,
# varying only BIO12
occEnvMeans &lt;- colMeans(occEnv, na.rm=TRUE)
occEnvMeans &lt;- rbind(occEnvMeans)
occEnvMeans &lt;- as.data.frame(occEnvMeans)
climFrame &lt;- occEnvMeans[rep(1, 100), ]
rownames(climFrame) &lt;- NULL

minBio12 &lt;- min(env$bio12)
maxBio12 &lt;- max(env$bio12)
climFrame$bio12 &lt;- seq(minBio12, maxBio12, length.out=100)

predMx &lt;- predictEnmSdm(mx, climFrame)
predMn &lt;- predictEnmSdm(mn, climFrame)
predGl &lt;- predictEnmSdm(gl, climFrame)
predGa &lt;- predictEnmSdm(ga, climFrame)
predNat &lt;- predictEnmSdm(ns, climFrame)
predBrt &lt;- predictEnmSdm(brt, climFrame)
predRf &lt;- predictEnmSdm(rf, climFrame)


plot(climFrame$bio12, predMx,
xlab='BIO12', ylab='Prediction', type='l', ylim=c(0, 1))

lines(climFrame$bio12, predMn, lty='solid', col='red')
lines(climFrame$bio12, predGl, lty='dotted', col='blue')
lines(climFrame$bio12, predGa, lty='dashed', col='green')
lines(climFrame$bio12, predNat, lty=4, col='purple')
lines(climFrame$bio12, predBrt, lty=5, col='orange')
lines(climFrame$bio12, predRf, lty=6, col='cyan')

legend(
   'topleft',
   inset = 0.01,
   legend = c(
	'MaxEnt',
	'MaxNet',
	'GLM',
	'GAM',
	'NS',
	'BRT',
	'RF'
   ),
   lty = c(1, 1:6),
   col = c(
	'black',
	'red',
	'blue',
	'green',
	'purple',
	'orange',
	'cyan'
   ),
   bg = 'white'
)


</code></pre>

<hr>
<h2 id='trainMaxEnt'>Calibrate a MaxEnt model using AICc</h2><span id='topic+trainMaxEnt'></span>

<h3>Description</h3>

<p>This function calculates the &quot;best&quot; MaxEnt model using AICc across all possible combinations of a set of master regularization parameters and feature classes. The best model has the lowest AICc, with ties broken by number of features (fewer is better), regularization multiplier (higher better), then finally the number of coefficients (fewer better).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trainMaxEnt(
  data,
  resp = names(data)[1],
  preds = names(data)[2:ncol(data)],
  regMult = c(seq(0.5, 5, by = 0.5), 7.5, 10),
  classes = "default",
  testClasses = TRUE,
  dropOverparam = TRUE,
  anyway = TRUE,
  forceLinear = TRUE,
  jackknife = FALSE,
  arguments = NULL,
  scratchDir = NULL,
  out = "model",
  cores = 1,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="trainMaxEnt_+3A_data">data</code></td>
<td>
<p>Data frame.</p>
</td></tr>
<tr><td><code id="trainMaxEnt_+3A_resp">resp</code></td>
<td>
<p>Response variable. This is either the name of the column in <code>data</code> or an integer indicating the column in <code>data</code> that has the response variable. The default is to use the first column in <code>data</code> as the response.</p>
</td></tr>
<tr><td><code id="trainMaxEnt_+3A_preds">preds</code></td>
<td>
<p>Character vector or integer vector. Names of columns or column indices of predictors. The default is to use the second and subsequent columns in <code>data</code>.</p>
</td></tr>
<tr><td><code id="trainMaxEnt_+3A_regmult">regMult</code></td>
<td>
<p>Numeric vector. Values of the master regularization parameters (called <code>beta</code> in some publications) to test.</p>
</td></tr>
<tr><td><code id="trainMaxEnt_+3A_classes">classes</code></td>
<td>
<p>Character vector. Names of feature classes to use (either <code>default</code> to use <code>lpqh</code>) or any combination of <code>lpqht</code>, where <code>l</code> ==&gt; linear features, <code>p</code> ==&gt; product features, <code>q</code> ==&gt; quadratic features, <code>h</code> ==&gt; hinge features, and <code>t</code> ==&gt; threshold features. Example: <code>c('l', 'p', 'q')</code>.</p>
</td></tr>
<tr><td><code id="trainMaxEnt_+3A_testclasses">testClasses</code></td>
<td>
<p>Logical.  If <code>TRUE</code> (default) then test all possible combinations of classes (note that all tested models will at least have linear features). If <code>FALSE</code> then use the classes provided (these will not vary between models).</p>
</td></tr>
<tr><td><code id="trainMaxEnt_+3A_dropoverparam">dropOverparam</code></td>
<td>
<p>Logical, if <code>TRUE</code> (default), drop models if they have more coefficients than training occurrences. It is possible for no models to fulfill this criterion, in which case no models will be returned.</p>
</td></tr>
<tr><td><code id="trainMaxEnt_+3A_anyway">anyway</code></td>
<td>
<p>Logical. Same as <code>dropOverparam</code> (included for backwards compatibility. If <code>NULL</code> (default), then the value of <code>dropOverparam</code> will take precedence. If <code>TRUE</code> or <code>FALSE</code> then <code>anyway</code> will override the value of <code>dropOverparam</code>.</p>
</td></tr>
<tr><td><code id="trainMaxEnt_+3A_forcelinear">forceLinear</code></td>
<td>
<p>Logical. If <code>TRUE</code> (default) then require any tested models to include at least linear features.</p>
</td></tr>
<tr><td><code id="trainMaxEnt_+3A_jackknife">jackknife</code></td>
<td>
<p>Logical. If <code>TRUE</code> (default) the the returned model will be also include jackknife testing of variable importance.</p>
</td></tr>
<tr><td><code id="trainMaxEnt_+3A_arguments">arguments</code></td>
<td>
<p><code>NULL</code> (default) or a character vector. Options to pass to <code>maxent()</code>'s <code>args</code> argument. (Do not include <code>l</code>, <code>p</code>, <code>q</code>, <code>h</code>, <code>t</code>, <code>betamultiplier</code>, or <code>jackknife</code>!)</p>
</td></tr>
<tr><td><code id="trainMaxEnt_+3A_scratchdir">scratchDir</code></td>
<td>
<p>Character. Directory to which to write temporary files. Leave as NULL to create a temporary folder in the current working directory.</p>
</td></tr>
<tr><td><code id="trainMaxEnt_+3A_out">out</code></td>
<td>
<p>Character vector. One or more values:
</p>

<ul>
<li>    <p><code>'model'</code>: Model with the lowest AICc.
</p>
</li>
<li>    <p><code>'models'</code>: All models evaluated, sorted from lowest to highest AICc (lowest is best).
</p>
</li>
<li>    <p><code>'tuning'</code>: Data frame with tuning parameters, one row per model, sorted by AICc.
</p>
</li></ul>
</td></tr>
<tr><td><code id="trainMaxEnt_+3A_cores">cores</code></td>
<td>
<p>Number of cores to use. Default is 1. If you have issues when <code>cores</code> &gt; 1, please see the <code><a href="#topic+troubleshooting_parallel_operations">troubleshooting_parallel_operations</a></code> guide.</p>
</td></tr>
<tr><td><code id="trainMaxEnt_+3A_verbose">verbose</code></td>
<td>
<p>Logical. If <code>TRUE</code> report progress and AICc table.</p>
</td></tr>
<tr><td><code id="trainMaxEnt_+3A_...">...</code></td>
<td>
<p>Extra arguments. Not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function can return the best model (default), a list of models created using all possible combinations of feature classes and regularization multipliers, and/or a data frame with tuning statistics for each model. Models in the list and in the data frame are sorted from best to worst. The function requires the <code>maxent</code> jar file (see <em>Details</em>).  Its output is any or all of: a table with AICc for all evaluated models; all models evaluated in the &quot;selection&quot; phase; and/or the single model with the lowest AICc.
</p>
<p>Note that due to differences in how MaxEnt and MaxNet are implemented in their base packages, the models will not necessarily be the same even for the same training data.
</p>
<p>This function is a wrapper for <code>MaxEnt()</code>. The <code>MaxEnt</code> function creates a series of files on disk for each model. This function assumes you do not want those files, so deletes most of them. However, there is one that cannot be deleted and the normal ways of changing its permissions in <span class="pkg">R</span> do not work. So the function simply writes over that file (which is allowed) to make it smaller. Regardless, if you run many models your temporary directory (argument <code>scratchDir</code>) can fill up and require manual deletion.
</p>


<h3>Value</h3>

<p>The object that is returned depends on the value of the <code>out</code> argument. It can be a model object, a data frame, a list of models, or a list of all two or more of these.
</p>


<h3>References</h3>

<p>Warren, D.L. and S.N. Siefert. 2011. Ecological niche modeling in Maxent: The importance of model complexity and the performance of model selection criteria. <em>Ecological Applications</em> 21:335-342. <a href="https://doi.org/10.1890/10-1171.1">doi:10.1890/10-1171.1</a>
</p>


<h3>See Also</h3>

<p><code><a href="predicts.html#topic+MaxEnt">MaxEnt</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# NB: The examples below show a very basic modeling workflow. They have been 
# designed to work fast, not produce accurate, defensible models. They can
# take a few minutes to run.

library(mgcv)
library(sf)
library(terra)
set.seed(123)

### setup data
##############

# environmental rasters
rastFile &lt;- system.file('extdata/madClim.tif', package='enmSdmX')
madClim &lt;- rast(rastFile)

# coordinate reference system
wgs84 &lt;- getCRS('WGS84')

# lemur occurrence data
data(lemurs)
occs &lt;- lemurs[lemurs$species == 'Eulemur fulvus', ]
occs &lt;- vect(occs, geom=c('longitude', 'latitude'), crs=wgs84)

occs &lt;- elimCellDuplicates(occs, madClim)

occEnv &lt;- extract(madClim, occs, ID = FALSE)
occEnv &lt;- occEnv[complete.cases(occEnv), ]
	
# create 10000 background sites (or as many as raster can support)
bgEnv &lt;- terra::spatSample(madClim, 20000)
bgEnv &lt;- bgEnv[complete.cases(bgEnv), ]
bgEnv &lt;- bgEnv[1:min(10000, nrow(bgEnv)), ]

# collate occurrences and background sites
presBg &lt;- data.frame(
  presBg = c(
    rep(1, nrow(occEnv)),
    rep(0, nrow(bgEnv))
  )
)

env &lt;- rbind(occEnv, bgEnv)
env &lt;- cbind(presBg, env)

predictors &lt;- c('bio1', 'bio12')

### calibrate models
####################

# Note that all of the trainXYZ functions can made to go faster using the
# "cores" argument (set to just 1, by default). The examples below will not
# go too much faster using more cores because they are simplified, but
# you can try!
cores &lt;- 1

# MaxEnt
mx &lt;- trainMaxEnt(
	data = env,
	resp = 'presBg',
	preds = predictors,
	regMult = 1, # too few values for reliable model, but fast
	verbose = TRUE,
	cores = cores
)

# MaxNet
mn &lt;- trainMaxNet(
	data = env,
	resp = 'presBg',
	preds = predictors,
	regMult = 1, # too few values for reliable model, but fast
	verbose = TRUE,
	cores = cores
)

# generalized linear model (GLM)
gl &lt;- trainGLM(
	data = env,
	resp = 'presBg',
	preds = predictors,
	scale = TRUE, # automatic scaling of predictors
	verbose = TRUE,
	cores = cores
)

# generalized additive model (GAM)
ga &lt;- trainGAM(
	data = env,
	resp = 'presBg',
	preds = predictors,
	verbose = TRUE,
	cores = cores
)

# natural splines
ns &lt;- trainNS(
	data = env,
	resp = 'presBg',
	preds = predictors,
	scale = TRUE, # automatic scaling of predictors
	df = 1:2, # too few values for reliable model(?)
	verbose = TRUE,
	cores = cores
)

# boosted regression trees
envSub &lt;- env[1:1049, ] # subsetting data to run faster
brt &lt;- trainBRT(
	data = envSub,
	resp = 'presBg',
	preds = predictors,
	learningRate = 0.001, # too few values for reliable model(?)
	treeComplexity = c(2, 3), # too few values for reliable model, but fast
	minTrees = 1200, # minimum trees for reliable model(?), but fast
	maxTrees = 1200, # too small for reliable model(?), but fast
	tryBy = 'treeComplexity',
	anyway = TRUE, # return models that did not converge
	verbose = TRUE,
	cores = cores
)

# random forests
rf &lt;- trainRF(
	data = env,
	resp = 'presBg',
	preds = predictors,
	numTrees = c(100, 500), # using at least 500 recommended, but fast!
	verbose = TRUE,
	cores = cores
)

### make maps of models
#######################

# NB We do not have to scale rasters before predicting GLMs and NSs because we
# used the `scale = TRUE` argument in trainGLM() and trainNS().

mxMap &lt;- predictEnmSdm(mx, madClim)
mnMap &lt;- predictEnmSdm(mn, madClim) 
glMap &lt;- predictEnmSdm(gl, madClim)
gaMap &lt;- predictEnmSdm(ga, madClim)
nsMap &lt;- predictEnmSdm(ns, madClim)
brtMap &lt;- predictEnmSdm(brt, madClim)
rfMap &lt;- predictEnmSdm(rf, madClim)

maps &lt;- c(
	mxMap,
	mnMap,
	glMap,
	gaMap,
	nsMap,
	brtMap,
	rfMap
)

names(maps) &lt;- c('MaxEnt', 'MaxNet', 'GLM', 'GAM', 'NSs', 'BRTs', 'RFs')
fun &lt;- function() plot(occs, col='black', pch=3, add=TRUE)
plot(maps, fun = fun, nc = 4)

### compare model responses to BIO12 (mean annual precipitation)
################################################################

# make a data frame holding all other variables at mean across occurrences,
# varying only BIO12
occEnvMeans &lt;- colMeans(occEnv, na.rm=TRUE)
occEnvMeans &lt;- rbind(occEnvMeans)
occEnvMeans &lt;- as.data.frame(occEnvMeans)
climFrame &lt;- occEnvMeans[rep(1, 100), ]
rownames(climFrame) &lt;- NULL

minBio12 &lt;- min(env$bio12)
maxBio12 &lt;- max(env$bio12)
climFrame$bio12 &lt;- seq(minBio12, maxBio12, length.out=100)

predMx &lt;- predictEnmSdm(mx, climFrame)
predMn &lt;- predictEnmSdm(mn, climFrame)
predGl &lt;- predictEnmSdm(gl, climFrame)
predGa &lt;- predictEnmSdm(ga, climFrame)
predNat &lt;- predictEnmSdm(ns, climFrame)
predBrt &lt;- predictEnmSdm(brt, climFrame)
predRf &lt;- predictEnmSdm(rf, climFrame)


plot(climFrame$bio12, predMx,
xlab='BIO12', ylab='Prediction', type='l', ylim=c(0, 1))

lines(climFrame$bio12, predMn, lty='solid', col='red')
lines(climFrame$bio12, predGl, lty='dotted', col='blue')
lines(climFrame$bio12, predGa, lty='dashed', col='green')
lines(climFrame$bio12, predNat, lty=4, col='purple')
lines(climFrame$bio12, predBrt, lty=5, col='orange')
lines(climFrame$bio12, predRf, lty=6, col='cyan')

legend(
   'topleft',
   inset = 0.01,
   legend = c(
	'MaxEnt',
	'MaxNet',
	'GLM',
	'GAM',
	'NS',
	'BRT',
	'RF'
   ),
   lty = c(1, 1:6),
   col = c(
	'black',
	'red',
	'blue',
	'green',
	'purple',
	'orange',
	'cyan'
   ),
   bg = 'white'
)


</code></pre>

<hr>
<h2 id='trainMaxNet'>Calibrate a MaxNet model using AICc</h2><span id='topic+trainMaxNet'></span>

<h3>Description</h3>

<p>This function calculates the &quot;best&quot; MaxNet model using AICc across all possible combinations of a set of master regularization parameters and feature classes. The &quot;best&quot; model has the lowest AICc, with ties broken by number of features (fewer is better), regularization multiplier (higher better), then finally the number of coefficients (fewer better).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trainMaxNet(
  data,
  resp = names(data)[1],
  preds = names(data)[2:ncol(data)],
  regMult = c(seq(0.5, 5, by = 0.5), 7.5, 10),
  classes = "default",
  testClasses = TRUE,
  dropOverparam = TRUE,
  forceLinear = TRUE,
  out = "model",
  cores = 1,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="trainMaxNet_+3A_data">data</code></td>
<td>
<p>Data frame or matrix. Contains a column indicating whether each row is a presence (1) or background (0) site, plus columns for environmental predictors.</p>
</td></tr>
<tr><td><code id="trainMaxNet_+3A_resp">resp</code></td>
<td>
<p>Character or integer. Name or column index of response variable. Default is to use the first column in <code>data</code>.</p>
</td></tr>
<tr><td><code id="trainMaxNet_+3A_preds">preds</code></td>
<td>
<p>Character vector or integer vector. Names of columns or column indices of predictors. Default is to use the second and subsequent columns in <code>data</code>.</p>
</td></tr>
<tr><td><code id="trainMaxNet_+3A_regmult">regMult</code></td>
<td>
<p>Numeric vector. Values of the master regularization parameters (called <code>beta</code> in some publications) to test.</p>
</td></tr>
<tr><td><code id="trainMaxNet_+3A_classes">classes</code></td>
<td>
<p>Character vector. Names of feature classes to use (either <code>default</code> to use <code>'lpqh'</code>) or any combination of <code>'lpqht'</code>, where <code>l</code> ==&gt; linear features, <code>p</code> ==&gt; product features, <code>q</code> ==&gt; quadratic features, <code>h</code> ==&gt; hinge features, and <code>t</code> ==&gt; threshold features. Example: <code>c('l', 'p', 'q')</code>.</p>
</td></tr>
<tr><td><code id="trainMaxNet_+3A_testclasses">testClasses</code></td>
<td>
<p>Logical.  If <code>TRUE</code> (default) then test all possible combinations of classes (note that all tested models will at least have linear features). If <code>FALSE</code> then use the classes provided (these will not vary between models).</p>
</td></tr>
<tr><td><code id="trainMaxNet_+3A_dropoverparam">dropOverparam</code></td>
<td>
<p>Logical, if <code>TRUE</code> (default), drop models if they have more coefficients than training occurrences. It is possible for no models to fulfill this criterion, in which case no models will be returned.</p>
</td></tr>
<tr><td><code id="trainMaxNet_+3A_forcelinear">forceLinear</code></td>
<td>
<p>Logical. If <code>TRUE</code> (default) then require any tested models to include at least linear features.</p>
</td></tr>
<tr><td><code id="trainMaxNet_+3A_out">out</code></td>
<td>
<p>Character vector. One or more values:
</p>

<ul>
<li>    <p><code>'model'</code>: Model with the lowest AICc.
</p>
</li>
<li>    <p><code>'models'</code>: All models evaluated, sorted from lowest to highest AICc (lowest is best).
</p>
</li>
<li>    <p><code>'tuning'</code>: Data frame with tuning parameters, one row per model, sorted by AICc.
</p>
</li></ul>
</td></tr>
<tr><td><code id="trainMaxNet_+3A_cores">cores</code></td>
<td>
<p>Number of cores to use. Default is 1. If you have issues when <code>cores</code> &gt; 1, please see the <code><a href="#topic+troubleshooting_parallel_operations">troubleshooting_parallel_operations</a></code> guide.</p>
</td></tr>
<tr><td><code id="trainMaxNet_+3A_verbose">verbose</code></td>
<td>
<p>Logical. If <code>TRUE</code> report the AICc table.</p>
</td></tr>
<tr><td><code id="trainMaxNet_+3A_...">...</code></td>
<td>
<p>Extra arguments. Not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function can return the best model (default), a list of models created using all possible combinations of feature classes and regularization multipliers, and/or a data frame with tuning statistics for each model. Models in the list and in the data frame are sorted from best to worst. Its output is any or all of: a table with AICc for all evaluated models; all models evaluated in the &quot;selection&quot; phase; and/or the single model with the lowest AICc.
</p>
<p>Note that due to differences in how MaxEnt and MaxNet are implemented in their base packages, the models will not necessarily be the same even for the same training data.
</p>


<h3>Value</h3>

<p>If <code>out = 'model'</code> this function returns an object of class <code>MaxEnt</code>. If <code>out = 'tuning'</code> this function returns a data frame with tuning parameters, log likelihood, and AICc for each model tried. If <code>out = c('model', 'tuning'</code> then it returns a list object with the <code>MaxEnt</code> object and the data frame.
</p>


<h3>References</h3>

<p>Phillips, S.J., Anderson, R.P., Dudík, M. Schapire, R.E., and Blair, M.E.  2017.  Opening the black box: An open-source release of Maxent. <em>Ecography</em> 40:887-893. <a href="https://doi.org/10.1111/ecog.03049">doi:10.1111/ecog.03049</a>
Warren, D.L. and S.N. Siefert. 2011. Ecological niche modeling in Maxent: The importance of model complexity and the performance of model selection criteria. <em>Ecological Applications</em> 21:335-342. <a href="https://doi.org/10.1890/10-1171.1">doi:10.1890/10-1171.1</a>
</p>


<h3>See Also</h3>

<p><code><a href="maxnet.html#topic+maxnet">maxnet</a></code>, <code><a href="predicts.html#topic+MaxEnt">MaxEnt</a></code>, <code><a href="#topic+trainMaxEnt">trainMaxEnt</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# NB: The examples below show a very basic modeling workflow. They have been 
# designed to work fast, not produce accurate, defensible models. They can
# take a few minutes to run.

library(mgcv)
library(sf)
library(terra)
set.seed(123)

### setup data
##############

# environmental rasters
rastFile &lt;- system.file('extdata/madClim.tif', package='enmSdmX')
madClim &lt;- rast(rastFile)

# coordinate reference system
wgs84 &lt;- getCRS('WGS84')

# lemur occurrence data
data(lemurs)
occs &lt;- lemurs[lemurs$species == 'Eulemur fulvus', ]
occs &lt;- vect(occs, geom=c('longitude', 'latitude'), crs=wgs84)

occs &lt;- elimCellDuplicates(occs, madClim)

occEnv &lt;- extract(madClim, occs, ID = FALSE)
occEnv &lt;- occEnv[complete.cases(occEnv), ]
	
# create 10000 background sites (or as many as raster can support)
bgEnv &lt;- terra::spatSample(madClim, 20000)
bgEnv &lt;- bgEnv[complete.cases(bgEnv), ]
bgEnv &lt;- bgEnv[1:min(10000, nrow(bgEnv)), ]

# collate occurrences and background sites
presBg &lt;- data.frame(
  presBg = c(
    rep(1, nrow(occEnv)),
    rep(0, nrow(bgEnv))
  )
)

env &lt;- rbind(occEnv, bgEnv)
env &lt;- cbind(presBg, env)

predictors &lt;- c('bio1', 'bio12')

### calibrate models
####################

# Note that all of the trainXYZ functions can made to go faster using the
# "cores" argument (set to just 1, by default). The examples below will not
# go too much faster using more cores because they are simplified, but
# you can try!
cores &lt;- 1

# MaxEnt
mx &lt;- trainMaxEnt(
	data = env,
	resp = 'presBg',
	preds = predictors,
	regMult = 1, # too few values for reliable model, but fast
	verbose = TRUE,
	cores = cores
)

# MaxNet
mn &lt;- trainMaxNet(
	data = env,
	resp = 'presBg',
	preds = predictors,
	regMult = 1, # too few values for reliable model, but fast
	verbose = TRUE,
	cores = cores
)

# generalized linear model (GLM)
gl &lt;- trainGLM(
	data = env,
	resp = 'presBg',
	preds = predictors,
	scale = TRUE, # automatic scaling of predictors
	verbose = TRUE,
	cores = cores
)

# generalized additive model (GAM)
ga &lt;- trainGAM(
	data = env,
	resp = 'presBg',
	preds = predictors,
	verbose = TRUE,
	cores = cores
)

# natural splines
ns &lt;- trainNS(
	data = env,
	resp = 'presBg',
	preds = predictors,
	scale = TRUE, # automatic scaling of predictors
	df = 1:2, # too few values for reliable model(?)
	verbose = TRUE,
	cores = cores
)

# boosted regression trees
envSub &lt;- env[1:1049, ] # subsetting data to run faster
brt &lt;- trainBRT(
	data = envSub,
	resp = 'presBg',
	preds = predictors,
	learningRate = 0.001, # too few values for reliable model(?)
	treeComplexity = c(2, 3), # too few values for reliable model, but fast
	minTrees = 1200, # minimum trees for reliable model(?), but fast
	maxTrees = 1200, # too small for reliable model(?), but fast
	tryBy = 'treeComplexity',
	anyway = TRUE, # return models that did not converge
	verbose = TRUE,
	cores = cores
)

# random forests
rf &lt;- trainRF(
	data = env,
	resp = 'presBg',
	preds = predictors,
	numTrees = c(100, 500), # using at least 500 recommended, but fast!
	verbose = TRUE,
	cores = cores
)

### make maps of models
#######################

# NB We do not have to scale rasters before predicting GLMs and NSs because we
# used the `scale = TRUE` argument in trainGLM() and trainNS().

mxMap &lt;- predictEnmSdm(mx, madClim)
mnMap &lt;- predictEnmSdm(mn, madClim) 
glMap &lt;- predictEnmSdm(gl, madClim)
gaMap &lt;- predictEnmSdm(ga, madClim)
nsMap &lt;- predictEnmSdm(ns, madClim)
brtMap &lt;- predictEnmSdm(brt, madClim)
rfMap &lt;- predictEnmSdm(rf, madClim)

maps &lt;- c(
	mxMap,
	mnMap,
	glMap,
	gaMap,
	nsMap,
	brtMap,
	rfMap
)

names(maps) &lt;- c('MaxEnt', 'MaxNet', 'GLM', 'GAM', 'NSs', 'BRTs', 'RFs')
fun &lt;- function() plot(occs, col='black', pch=3, add=TRUE)
plot(maps, fun = fun, nc = 4)

### compare model responses to BIO12 (mean annual precipitation)
################################################################

# make a data frame holding all other variables at mean across occurrences,
# varying only BIO12
occEnvMeans &lt;- colMeans(occEnv, na.rm=TRUE)
occEnvMeans &lt;- rbind(occEnvMeans)
occEnvMeans &lt;- as.data.frame(occEnvMeans)
climFrame &lt;- occEnvMeans[rep(1, 100), ]
rownames(climFrame) &lt;- NULL

minBio12 &lt;- min(env$bio12)
maxBio12 &lt;- max(env$bio12)
climFrame$bio12 &lt;- seq(minBio12, maxBio12, length.out=100)

predMx &lt;- predictEnmSdm(mx, climFrame)
predMn &lt;- predictEnmSdm(mn, climFrame)
predGl &lt;- predictEnmSdm(gl, climFrame)
predGa &lt;- predictEnmSdm(ga, climFrame)
predNat &lt;- predictEnmSdm(ns, climFrame)
predBrt &lt;- predictEnmSdm(brt, climFrame)
predRf &lt;- predictEnmSdm(rf, climFrame)


plot(climFrame$bio12, predMx,
xlab='BIO12', ylab='Prediction', type='l', ylim=c(0, 1))

lines(climFrame$bio12, predMn, lty='solid', col='red')
lines(climFrame$bio12, predGl, lty='dotted', col='blue')
lines(climFrame$bio12, predGa, lty='dashed', col='green')
lines(climFrame$bio12, predNat, lty=4, col='purple')
lines(climFrame$bio12, predBrt, lty=5, col='orange')
lines(climFrame$bio12, predRf, lty=6, col='cyan')

legend(
   'topleft',
   inset = 0.01,
   legend = c(
	'MaxEnt',
	'MaxNet',
	'GLM',
	'GAM',
	'NS',
	'BRT',
	'RF'
   ),
   lty = c(1, 1:6),
   col = c(
	'black',
	'red',
	'blue',
	'green',
	'purple',
	'orange',
	'cyan'
   ),
   bg = 'white'
)


</code></pre>

<hr>
<h2 id='trainNS'>Calibrate a natural splines model</h2><span id='topic+trainNS'></span>

<h3>Description</h3>

<p>This function constructs a natural-spline model by evaluating all possible models given the available predictors and constraints. &quot;Constraints&quot; in this case include the degrees of freedom for a spline, whether or not interaction terms are included, minimum number of presence sites per model term, and maximum number of terms to include in the model. Its output is any or all of: the most parsimonious model (lowest AICc); all models evaluated; and/or a table with AICc for all evaluated models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trainNS(
  data,
  resp = names(data)[1],
  preds = names(data)[2:ncol(data)],
  scale = NA,
  df = 1:4,
  interaction = TRUE,
  interceptOnly = TRUE,
  method = "glm.fit",
  presPerTermFinal = 10,
  maxTerms = 8,
  w = TRUE,
  family = "binomial",
  removeInvalid = TRUE,
  failIfNoValid = TRUE,
  out = "model",
  cores = 1,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="trainNS_+3A_data">data</code></td>
<td>
<p>Data frame.</p>
</td></tr>
<tr><td><code id="trainNS_+3A_resp">resp</code></td>
<td>
<p>Response variable. This is either the name of the column in <code>data</code> or an integer indicating the column in <code>data</code> that has the response variable. The default is to use the first column in <code>data</code> as the response.</p>
</td></tr>
<tr><td><code id="trainNS_+3A_preds">preds</code></td>
<td>
<p>Character vector or integer vector. Names of columns or column indices of predictors. The default is to use the second and subsequent columns in <code>data</code>.</p>
</td></tr>
<tr><td><code id="trainNS_+3A_scale">scale</code></td>
<td>
<p>Either <code>NA</code> (default), or <code>TRUE</code> or <code>FALSE</code>. If <code>TRUE</code>, the predictors will be centered and scaled by dividing by subtracting their means then dividing by their standard deviations. The means and standard deviations will be returned in the model object under an element named &quot;<code>scales</code>&quot;. For example, if you do something like <code>model &lt;- trainGLM(data, scale=TRUE)</code>, then you can get the means and standard deviations using <code>model$scales$means</code> and <code>model$scales$sds</code>. If <code>FALSE</code>, no scaling is done. If <code>NA</code> (default), then the function will check to see if non-factor predictors have means ~0 and standard deviations ~1. If not, then a warning will be printed, but the function will continue to do it's operations.</p>
</td></tr>
<tr><td><code id="trainNS_+3A_df">df</code></td>
<td>
<p>A vector of integers &gt; 0 or <code>NULL</code>. Sets flexibility of model fit. See documentation for <code><a href="splines.html#topic+ns">ns</a></code>.</p>
</td></tr>
<tr><td><code id="trainNS_+3A_interaction">interaction</code></td>
<td>
<p>If <code>TRUE</code> (default), include two-way interaction terms.</p>
</td></tr>
<tr><td><code id="trainNS_+3A_interceptonly">interceptOnly</code></td>
<td>
<p>If <code>TRUE</code> (default) and model selection is enabled, then include an intercept-only model.</p>
</td></tr>
<tr><td><code id="trainNS_+3A_method">method</code></td>
<td>
<p>Character, name of function used to solve. This can be <code>'glm.fit'</code> (default), <code>'brglmFit'</code> (from the <span class="pkg">brglm2</span> package), or another function.</p>
</td></tr>
<tr><td><code id="trainNS_+3A_prespertermfinal">presPerTermFinal</code></td>
<td>
<p>Minimum number of presence sites per term in initial starting model.</p>
</td></tr>
<tr><td><code id="trainNS_+3A_maxterms">maxTerms</code></td>
<td>
<p>Maximum number of terms to be used in any model, not including the intercept (default is 8).</p>
</td></tr>
<tr><td><code id="trainNS_+3A_w">w</code></td>
<td>
<p>Weights. Any of:
</p>

<ul>
<li> <p><code>TRUE</code>: Causes the total weight of presences to equal the total weight of absences (if <code>family='binomial'</code>)
</p>
</li>
<li> <p><code>FALSE</code>: Each datum is assigned a weight of 1.
</p>
</li>
<li><p> A numeric vector of weights, one per row in <code>data</code>.
</p>
</li>
<li><p> The name of the column in <code>data</code> that contains site weights.
</p>
</li></ul>
</td></tr>
<tr><td><code id="trainNS_+3A_family">family</code></td>
<td>
<p>Name of family for data error structure (see <code><a href="stats.html#topic+family">family</a></code>).</p>
</td></tr>
<tr><td><code id="trainNS_+3A_removeinvalid">removeInvalid</code></td>
<td>
<p>Logical. If <code>TRUE</code> (default), remove models that either did not converge or have parameter estimates near the boundaries (usually negative or positive infinity).</p>
</td></tr>
<tr><td><code id="trainNS_+3A_failifnovalid">failIfNoValid</code></td>
<td>
<p>Logical. If <code>TRUE</code> (default), and the &quot;full&quot; model either does not converge or has parameters near the boundary, then the function will fail. If <code>FALSE</code>, then return <code>NULL</code> in this case.</p>
</td></tr>
<tr><td><code id="trainNS_+3A_out">out</code></td>
<td>
<p>Character vector. One or more values:
</p>

<ul>
<li>    <p><code>'model'</code>: Model with the lowest AICc.
</p>
</li>
<li>    <p><code>'models'</code>: All models evaluated, sorted from lowest to highest AICc (lowest is best).
</p>
</li>
<li>    <p><code>'tuning'</code>: Data frame with tuning parameters, one row per model, sorted by AICc.
</p>
</li></ul>
</td></tr>
<tr><td><code id="trainNS_+3A_cores">cores</code></td>
<td>
<p>Number of cores to use. Default is 1. If you have issues when <code>cores</code> &gt; 1, please see the <code><a href="#topic+troubleshooting_parallel_operations">troubleshooting_parallel_operations</a></code> guide.</p>
</td></tr>
<tr><td><code id="trainNS_+3A_verbose">verbose</code></td>
<td>
<p>Logical. If <code>TRUE</code> then display intermediate results on the display device. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="trainNS_+3A_...">...</code></td>
<td>
<p>Arguments to send to <code><a href="stats.html#topic+glm">glm</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is designed to find the most parsimonious model given the amount of calibration data that is available to it. 'trainNS()' can work with any data, but has been designed to work specifically as a species distribution model where the response is either binary (default) or abundance. Specifically, it 1) identifies the most parsimonious model (lowest AICc) with 2) optimal flexibility (optimal degrees of freedom in splines) and 3) allows for (but does not require) interaction terms between predictors (if desired). If the defaults are used, the following procedure is applied:
</p>

<ul>
<li><p> Constructing a set of simple model terms, each with 1 to 4 degrees of freedom. Terms can be univariate or bilabiate (two-way interactions). Predictors can be continuous or factors. If any simple models has convergence issues or boundary issues (coefficients that approach negative or positive infinity), it is removed.
</p>
</li>
<li><p> Constructing a series of models, each with one of the terms, then using the models to rank terms by AICc.
</p>
</li>
<li><p> From the top set of terms, creating a &quot;full&quot; model. The full model will ensure the maximum number of terms is &lt;= 'maxTerms', and that for each term, there are at least 'presPerTermFinal' data points.
</p>
</li>
<li><p> All possible submodels, plus the full model, are evaluated and ranked by AICc.  If a model has convergence or boundary issues, it is removed from the set. The most parsimonious model (lowest AICc) is returned.
</p>
</li></ul>



<h3>Value</h3>

<p>The object that is returned depends on the value of the <code>out</code> argument. It can be a model object, a data frame, a list of models, or a list of all two or more of these. If <code>scale</code> is <code>TRUE</code>, any model object will also have an element named <code>$scale</code>, which contains the means and standard deviations for predictors that are not factors. The data frame reports the AICc for all of the models evaluated, sorted by best to worst. The <code>converged</code> column indicates whether the model converged (&quot;<code>TRUE</code>&quot; is good), and the <code>boundary</code> column whether the model parameters are near the boundary (usually, negative or positive infinity; &quot;<code>FALSE</code>&quot; is good).
</p>


<h3>See Also</h3>

<p><code><a href="splines.html#topic+ns">ns</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# NB: The examples below show a very basic modeling workflow. They have been 
# designed to work fast, not produce accurate, defensible models. They can
# take a few minutes to run.

library(mgcv)
library(sf)
library(terra)
set.seed(123)

### setup data
##############

# environmental rasters
rastFile &lt;- system.file('extdata/madClim.tif', package='enmSdmX')
madClim &lt;- rast(rastFile)

# coordinate reference system
wgs84 &lt;- getCRS('WGS84')

# lemur occurrence data
data(lemurs)
occs &lt;- lemurs[lemurs$species == 'Eulemur fulvus', ]
occs &lt;- vect(occs, geom=c('longitude', 'latitude'), crs=wgs84)

occs &lt;- elimCellDuplicates(occs, madClim)

occEnv &lt;- extract(madClim, occs, ID = FALSE)
occEnv &lt;- occEnv[complete.cases(occEnv), ]
	
# create 10000 background sites (or as many as raster can support)
bgEnv &lt;- terra::spatSample(madClim, 20000)
bgEnv &lt;- bgEnv[complete.cases(bgEnv), ]
bgEnv &lt;- bgEnv[1:min(10000, nrow(bgEnv)), ]

# collate occurrences and background sites
presBg &lt;- data.frame(
  presBg = c(
    rep(1, nrow(occEnv)),
    rep(0, nrow(bgEnv))
  )
)

env &lt;- rbind(occEnv, bgEnv)
env &lt;- cbind(presBg, env)

predictors &lt;- c('bio1', 'bio12')

### calibrate models
####################

# Note that all of the trainXYZ functions can made to go faster using the
# "cores" argument (set to just 1, by default). The examples below will not
# go too much faster using more cores because they are simplified, but
# you can try!
cores &lt;- 1

# MaxEnt
mx &lt;- trainMaxEnt(
	data = env,
	resp = 'presBg',
	preds = predictors,
	regMult = 1, # too few values for reliable model, but fast
	verbose = TRUE,
	cores = cores
)

# MaxNet
mn &lt;- trainMaxNet(
	data = env,
	resp = 'presBg',
	preds = predictors,
	regMult = 1, # too few values for reliable model, but fast
	verbose = TRUE,
	cores = cores
)

# generalized linear model (GLM)
gl &lt;- trainGLM(
	data = env,
	resp = 'presBg',
	preds = predictors,
	scale = TRUE, # automatic scaling of predictors
	verbose = TRUE,
	cores = cores
)

# generalized additive model (GAM)
ga &lt;- trainGAM(
	data = env,
	resp = 'presBg',
	preds = predictors,
	verbose = TRUE,
	cores = cores
)

# natural splines
ns &lt;- trainNS(
	data = env,
	resp = 'presBg',
	preds = predictors,
	scale = TRUE, # automatic scaling of predictors
	df = 1:2, # too few values for reliable model(?)
	verbose = TRUE,
	cores = cores
)

# boosted regression trees
envSub &lt;- env[1:1049, ] # subsetting data to run faster
brt &lt;- trainBRT(
	data = envSub,
	resp = 'presBg',
	preds = predictors,
	learningRate = 0.001, # too few values for reliable model(?)
	treeComplexity = c(2, 3), # too few values for reliable model, but fast
	minTrees = 1200, # minimum trees for reliable model(?), but fast
	maxTrees = 1200, # too small for reliable model(?), but fast
	tryBy = 'treeComplexity',
	anyway = TRUE, # return models that did not converge
	verbose = TRUE,
	cores = cores
)

# random forests
rf &lt;- trainRF(
	data = env,
	resp = 'presBg',
	preds = predictors,
	numTrees = c(100, 500), # using at least 500 recommended, but fast!
	verbose = TRUE,
	cores = cores
)

### make maps of models
#######################

# NB We do not have to scale rasters before predicting GLMs and NSs because we
# used the `scale = TRUE` argument in trainGLM() and trainNS().

mxMap &lt;- predictEnmSdm(mx, madClim)
mnMap &lt;- predictEnmSdm(mn, madClim) 
glMap &lt;- predictEnmSdm(gl, madClim)
gaMap &lt;- predictEnmSdm(ga, madClim)
nsMap &lt;- predictEnmSdm(ns, madClim)
brtMap &lt;- predictEnmSdm(brt, madClim)
rfMap &lt;- predictEnmSdm(rf, madClim)

maps &lt;- c(
	mxMap,
	mnMap,
	glMap,
	gaMap,
	nsMap,
	brtMap,
	rfMap
)

names(maps) &lt;- c('MaxEnt', 'MaxNet', 'GLM', 'GAM', 'NSs', 'BRTs', 'RFs')
fun &lt;- function() plot(occs, col='black', pch=3, add=TRUE)
plot(maps, fun = fun, nc = 4)

### compare model responses to BIO12 (mean annual precipitation)
################################################################

# make a data frame holding all other variables at mean across occurrences,
# varying only BIO12
occEnvMeans &lt;- colMeans(occEnv, na.rm=TRUE)
occEnvMeans &lt;- rbind(occEnvMeans)
occEnvMeans &lt;- as.data.frame(occEnvMeans)
climFrame &lt;- occEnvMeans[rep(1, 100), ]
rownames(climFrame) &lt;- NULL

minBio12 &lt;- min(env$bio12)
maxBio12 &lt;- max(env$bio12)
climFrame$bio12 &lt;- seq(minBio12, maxBio12, length.out=100)

predMx &lt;- predictEnmSdm(mx, climFrame)
predMn &lt;- predictEnmSdm(mn, climFrame)
predGl &lt;- predictEnmSdm(gl, climFrame)
predGa &lt;- predictEnmSdm(ga, climFrame)
predNat &lt;- predictEnmSdm(ns, climFrame)
predBrt &lt;- predictEnmSdm(brt, climFrame)
predRf &lt;- predictEnmSdm(rf, climFrame)


plot(climFrame$bio12, predMx,
xlab='BIO12', ylab='Prediction', type='l', ylim=c(0, 1))

lines(climFrame$bio12, predMn, lty='solid', col='red')
lines(climFrame$bio12, predGl, lty='dotted', col='blue')
lines(climFrame$bio12, predGa, lty='dashed', col='green')
lines(climFrame$bio12, predNat, lty=4, col='purple')
lines(climFrame$bio12, predBrt, lty=5, col='orange')
lines(climFrame$bio12, predRf, lty=6, col='cyan')

legend(
   'topleft',
   inset = 0.01,
   legend = c(
	'MaxEnt',
	'MaxNet',
	'GLM',
	'GAM',
	'NS',
	'BRT',
	'RF'
   ),
   lty = c(1, 1:6),
   col = c(
	'black',
	'red',
	'blue',
	'green',
	'purple',
	'orange',
	'cyan'
   ),
   bg = 'white'
)


</code></pre>

<hr>
<h2 id='trainRF'>Calibrate a random forest model</h2><span id='topic+trainRF'></span>

<h3>Description</h3>

<p>This function trains a random forest model. It identifies the optimal number of trees and value for <code>mtry</code> (number of variables sampled as candidates at each split) using out-of-bag error (OOB). The number of trees in each candidate model is set by the user with argument <code>numTrees</code>. The number of predictors to test per split, <code>mtry</code>, is found by exploring a range of values. If the response (<code>y</code>) is a factor, the starting value for <code>mtry</code> is <code>max(1, floor(p / 3))</code>, where <code>p</code> is the number of predictors. If the response is not a factor, the starting value is <code>max(1, floor(sqrt(p)))</code>. Values y<code>mtryIncrement</code> argument until the total number of predictors is used.  See <code><a href="ranger.html#topic+ranger">ranger</a></code> for more details.
</p>
<p>The output of the function is any or all of: a table with out-of-bag (OOB) error of evaluated models; all evaluated models; and/or the single model with the lowest OOB error.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trainRF(
  data,
  resp = names(data)[1],
  preds = names(data)[2:ncol(data)],
  numTrees = c(250, 500, 750, 1000),
  mtryIncrement = 2,
  w = TRUE,
  binary = TRUE,
  out = "model",
  cores = 1,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="trainRF_+3A_data">data</code></td>
<td>
<p>Data frame.</p>
</td></tr>
<tr><td><code id="trainRF_+3A_resp">resp</code></td>
<td>
<p>Response variable. This is either the name of the column in <code>data</code> or an integer indicating the column in <code>data</code> that has the response variable. The default is to use the first column in <code>data</code> as the response.</p>
</td></tr>
<tr><td><code id="trainRF_+3A_preds">preds</code></td>
<td>
<p>Character vector or integer vector. Names of columns or column indices of predictors. The default is to use the second and subsequent columns in <code>data</code>.</p>
</td></tr>
<tr><td><code id="trainRF_+3A_numtrees">numTrees</code></td>
<td>
<p>Vector of number of trees to grow. All possible combinations of <code>mtry</code> and <code>numTrees</code> will be assessed.</p>
</td></tr>
<tr><td><code id="trainRF_+3A_mtryincrement">mtryIncrement</code></td>
<td>
<p>Positive integer (default is 2).  Number of predictors to add to <code>mtry</code> until all predictors are in each tree.</p>
</td></tr>
<tr><td><code id="trainRF_+3A_w">w</code></td>
<td>
<p>Weights. For random forests, weights are simply used as relative probabilities of selecting a row in <code>data</code> to be used in a particular tree. This argument takes any of:
</p>

<ul>
<li> <p><code>TRUE</code>: Causes the total weight of presences to equal the total weight of absences (if <code>binary = TRUE</code>)
</p>
</li>
<li> <p><code>FALSE</code>: Each datum is assigned a weight of 1.
</p>
</li>
<li><p> A numeric vector of weights, one per row in <code>data</code>.
</p>
</li>
<li><p> The name of the column in <code>data</code> that contains site weights.
</p>
</li></ul>
</td></tr>
<tr><td><code id="trainRF_+3A_binary">binary</code></td>
<td>
<p>Logical. If <code>TRUE</code> (default) then the response is converted to a binary factor with levels 0 and 1. Otherwise, this argument has no effect and the response will be assumed to be a real number.</p>
</td></tr>
<tr><td><code id="trainRF_+3A_out">out</code></td>
<td>
<p>Character vector. One or more values:
</p>

<ul>
<li>    <p><code>'model'</code>: Model with the lowest out-of-bag (OOB) error rate.
</p>
</li>
<li>    <p><code>'models'</code>: All models evaluated, sorted from lowest to highest OOB.
</p>
</li>
<li>    <p><code>'tuning'</code>: Data frame with tuning parameters, one row per model, sorted by OOB error rate.
</p>
</li></ul>
</td></tr>
<tr><td><code id="trainRF_+3A_cores">cores</code></td>
<td>
<p>Number of cores to use. Default is 1. If you have issues when <code>cores</code> &gt; 1, please see the <code><a href="#topic+troubleshooting_parallel_operations">troubleshooting_parallel_operations</a></code> guide.</p>
</td></tr>
<tr><td><code id="trainRF_+3A_verbose">verbose</code></td>
<td>
<p>Logical. If <code>TRUE</code> then display progress for finding optimal value of <code>mtry</code>.</p>
</td></tr>
<tr><td><code id="trainRF_+3A_...">...</code></td>
<td>
<p>Arguments to pass to <code><a href="ranger.html#topic+ranger">ranger</a></code>. Of note, <code>num.threads</code> will allow for multi-threaded computation of each RF. However, it could be problemmatic to use this when <code>cores</code> &gt; 1. Also of note, <code>save.memory</code> reduces speed but may make larger jobs possible.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The object that is returned depends on the value of the <code>out</code> argument. It can be a model object, a data frame, a list of models, or a list of all two or more of these.
</p>


<h3>See Also</h3>

<p><code><a href="ranger.html#topic+ranger">ranger</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# NB: The examples below show a very basic modeling workflow. They have been 
# designed to work fast, not produce accurate, defensible models. They can
# take a few minutes to run.

library(mgcv)
library(sf)
library(terra)
set.seed(123)

### setup data
##############

# environmental rasters
rastFile &lt;- system.file('extdata/madClim.tif', package='enmSdmX')
madClim &lt;- rast(rastFile)

# coordinate reference system
wgs84 &lt;- getCRS('WGS84')

# lemur occurrence data
data(lemurs)
occs &lt;- lemurs[lemurs$species == 'Eulemur fulvus', ]
occs &lt;- vect(occs, geom=c('longitude', 'latitude'), crs=wgs84)

occs &lt;- elimCellDuplicates(occs, madClim)

occEnv &lt;- extract(madClim, occs, ID = FALSE)
occEnv &lt;- occEnv[complete.cases(occEnv), ]
	
# create 10000 background sites (or as many as raster can support)
bgEnv &lt;- terra::spatSample(madClim, 20000)
bgEnv &lt;- bgEnv[complete.cases(bgEnv), ]
bgEnv &lt;- bgEnv[1:min(10000, nrow(bgEnv)), ]

# collate occurrences and background sites
presBg &lt;- data.frame(
  presBg = c(
    rep(1, nrow(occEnv)),
    rep(0, nrow(bgEnv))
  )
)

env &lt;- rbind(occEnv, bgEnv)
env &lt;- cbind(presBg, env)

predictors &lt;- c('bio1', 'bio12')

### calibrate models
####################

# Note that all of the trainXYZ functions can made to go faster using the
# "cores" argument (set to just 1, by default). The examples below will not
# go too much faster using more cores because they are simplified, but
# you can try!
cores &lt;- 1

# MaxEnt
mx &lt;- trainMaxEnt(
	data = env,
	resp = 'presBg',
	preds = predictors,
	regMult = 1, # too few values for reliable model, but fast
	verbose = TRUE,
	cores = cores
)

# MaxNet
mn &lt;- trainMaxNet(
	data = env,
	resp = 'presBg',
	preds = predictors,
	regMult = 1, # too few values for reliable model, but fast
	verbose = TRUE,
	cores = cores
)

# generalized linear model (GLM)
gl &lt;- trainGLM(
	data = env,
	resp = 'presBg',
	preds = predictors,
	scale = TRUE, # automatic scaling of predictors
	verbose = TRUE,
	cores = cores
)

# generalized additive model (GAM)
ga &lt;- trainGAM(
	data = env,
	resp = 'presBg',
	preds = predictors,
	verbose = TRUE,
	cores = cores
)

# natural splines
ns &lt;- trainNS(
	data = env,
	resp = 'presBg',
	preds = predictors,
	scale = TRUE, # automatic scaling of predictors
	df = 1:2, # too few values for reliable model(?)
	verbose = TRUE,
	cores = cores
)

# boosted regression trees
envSub &lt;- env[1:1049, ] # subsetting data to run faster
brt &lt;- trainBRT(
	data = envSub,
	resp = 'presBg',
	preds = predictors,
	learningRate = 0.001, # too few values for reliable model(?)
	treeComplexity = c(2, 3), # too few values for reliable model, but fast
	minTrees = 1200, # minimum trees for reliable model(?), but fast
	maxTrees = 1200, # too small for reliable model(?), but fast
	tryBy = 'treeComplexity',
	anyway = TRUE, # return models that did not converge
	verbose = TRUE,
	cores = cores
)

# random forests
rf &lt;- trainRF(
	data = env,
	resp = 'presBg',
	preds = predictors,
	numTrees = c(100, 500), # using at least 500 recommended, but fast!
	verbose = TRUE,
	cores = cores
)

### make maps of models
#######################

# NB We do not have to scale rasters before predicting GLMs and NSs because we
# used the `scale = TRUE` argument in trainGLM() and trainNS().

mxMap &lt;- predictEnmSdm(mx, madClim)
mnMap &lt;- predictEnmSdm(mn, madClim) 
glMap &lt;- predictEnmSdm(gl, madClim)
gaMap &lt;- predictEnmSdm(ga, madClim)
nsMap &lt;- predictEnmSdm(ns, madClim)
brtMap &lt;- predictEnmSdm(brt, madClim)
rfMap &lt;- predictEnmSdm(rf, madClim)

maps &lt;- c(
	mxMap,
	mnMap,
	glMap,
	gaMap,
	nsMap,
	brtMap,
	rfMap
)

names(maps) &lt;- c('MaxEnt', 'MaxNet', 'GLM', 'GAM', 'NSs', 'BRTs', 'RFs')
fun &lt;- function() plot(occs, col='black', pch=3, add=TRUE)
plot(maps, fun = fun, nc = 4)

### compare model responses to BIO12 (mean annual precipitation)
################################################################

# make a data frame holding all other variables at mean across occurrences,
# varying only BIO12
occEnvMeans &lt;- colMeans(occEnv, na.rm=TRUE)
occEnvMeans &lt;- rbind(occEnvMeans)
occEnvMeans &lt;- as.data.frame(occEnvMeans)
climFrame &lt;- occEnvMeans[rep(1, 100), ]
rownames(climFrame) &lt;- NULL

minBio12 &lt;- min(env$bio12)
maxBio12 &lt;- max(env$bio12)
climFrame$bio12 &lt;- seq(minBio12, maxBio12, length.out=100)

predMx &lt;- predictEnmSdm(mx, climFrame)
predMn &lt;- predictEnmSdm(mn, climFrame)
predGl &lt;- predictEnmSdm(gl, climFrame)
predGa &lt;- predictEnmSdm(ga, climFrame)
predNat &lt;- predictEnmSdm(ns, climFrame)
predBrt &lt;- predictEnmSdm(brt, climFrame)
predRf &lt;- predictEnmSdm(rf, climFrame)


plot(climFrame$bio12, predMx,
xlab='BIO12', ylab='Prediction', type='l', ylim=c(0, 1))

lines(climFrame$bio12, predMn, lty='solid', col='red')
lines(climFrame$bio12, predGl, lty='dotted', col='blue')
lines(climFrame$bio12, predGa, lty='dashed', col='green')
lines(climFrame$bio12, predNat, lty=4, col='purple')
lines(climFrame$bio12, predBrt, lty=5, col='orange')
lines(climFrame$bio12, predRf, lty=6, col='cyan')

legend(
   'topleft',
   inset = 0.01,
   legend = c(
	'MaxEnt',
	'MaxNet',
	'GLM',
	'GAM',
	'NS',
	'BRT',
	'RF'
   ),
   lty = c(1, 1:6),
   col = c(
	'black',
	'red',
	'blue',
	'green',
	'purple',
	'orange',
	'cyan'
   ),
   bg = 'white'
)


</code></pre>

<hr>
<h2 id='troubleshooting_parallel_operations'>Troubleshooting parallel operations</h2><span id='topic+troubleshooting_parallel_operations'></span>

<h3>Description</h3>

<p>This is a guide to solving issues with running functions that can use more than one core. This includes the <code>train</code><em>XYZ</em> functions, <code><a href="#topic+bioticVelocity">bioticVelocity</a></code>, and <code><a href="#topic+predictEnmSdm">predictEnmSdm</a></code>. Each of these function has the argument <code>cores</code>. By default, the value of <code>cores</code> is 1, so the function will use only one core. By setting this higher, you can use more cores on your machine.  However, occasionally you will run into the error:<br />
<code>Error in checkForRemoteErrors(lapply(cl, recvResult)) :</code><br />
<code>  2 nodes produced errors; first error: object '.doSnowGlobals' not found</code><br />
This means that the worker &quot;nodes&quot; (different instances of <code>R</code> started by the function to run in parallel) cannot find the <span class="pkg">doParallel</span> package, even if it is installed on your system.
</p>
<p>There are several solutions to this issue. One of them may work for you, and none are inherent to <span class="pkg">enmSdmX</span>, as far as I can tell.
</p>


<h3>Anti-virus is blocking R</h3>

<p>Strangely enough, running R in parallel sometimes looks like you are accessing the internet to anti-virus software. So, it may block access to other instances of R. You will have to do some surgery on your anti-virus software settings to find where to change this.
</p>


<h3>Your R packages are not stored in the &quot;traditional&quot; place</h3>

<p>R has a default directory where packages are stored on any system. If your packages are stored in a different place, worker nodes may not be able to find them <em>if</em> you use <code><a href="base.html#topic+setwd">setwd</a></code> to change the working directory. I do not know if you have to set the working directory back to the default for your system, or if you have to change it to the folder that <em>contains</em> the folder where your R packages reside (for me, they are the same directory). You can see what your current working directory is using <code><a href="base.html#topic+getwd">getwd</a></code>. RStudio will often change this directory automatically.
</p>
<p>So, if you get this error, try using <code><a href="base.html#topic+setwd">setwd</a></code> to set your working directory to the default one for your system, or to the folder that contains the folder that contains your packages.
</p>


<h3>Let me know</h3>

<p>I'm always game to help you track down your problems (with this package, not necessarily in general). The best way is to create an issue on <a href="https://github.com/adamlilith/enmSdmX/issues">GitHub</a>.
</p>


<h3>Exorcise your computer</h3>

<p>Not responsible for damage to your computer.
</p>

<hr>
<h2 id='weightByDist'>Proximity-based weighting for occurrences to correct for spatial bias</h2><span id='topic+weightByDist'></span>

<h3>Description</h3>

<p>This function calculates weights for points based on proximity to other points and the distance of spatial autocorrelation.<br /> <br />
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weightByDist(x, maxDist, alpha = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="weightByDist_+3A_x">x</code></td>
<td>
<p>A spatial points object of class <code>SpatVector</code> or <code>sf</code>.</p>
</td></tr>
<tr><td><code id="weightByDist_+3A_maxdist">maxDist</code></td>
<td>
<p>Maximum distance beyond which a two neighboring points are assumed to have no effect on one another for calculation of weights.</p>
</td></tr>
<tr><td><code id="weightByDist_+3A_alpha">alpha</code></td>
<td>
<p>Scaling parameter (see equations above).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Weights can be used, for example, to account for spatial bias in the manner in which the points were observed. Weighting is calculated on the assumption that if two points fell exactly on top of one another, they should each have a weight of 1/2. If three points had the exact same coordinates, then their weights should be 1/3, and so on.  Increasing distance between points should increase their weight, up to the distance at which there is no &quot;significant&quot; spatial autocorrelation, beyond which a point should have a weight of 1. This distance needs to be supplied by the user, as it will depend on the intended use of the weights. The distance can be calculated from &quot;standard&quot; metrics of spatial autocorrelation (e.g., a variogram), or on the basis of knowledge of the system (e.g., maximum dispersal distance of an organism). <br /> <br />
For a given point <code class="reqn">i</code>, the weight is defined as </p>
<p style="text-align: center;"><code class="reqn">w_i = 1 / (1 + \epsilon)</code>
</p>
<p> where </p>
<p style="text-align: center;"><code class="reqn">\epsilon = \sum_{n=1}^{N}((1 - d_n)/d_sac)^\alpha</code>
</p>
<p> in which <code class="reqn">N</code> is the total number of points closer than the maximum distance (<code class="reqn">d_sac</code>) of point <code class="reqn">i</code>, and <code class="reqn">d_n</code> the distance between focal point <code class="reqn">i</code> and point <code class="reqn">n</code>.  <code class="reqn">\alpha</code> is a weighting factor. By default, this is set to 1, but can be changed by the user to augment or diminish the effect that neighboring points have on the weight of a focal cell. When <code class="reqn">\alpha</code> is &lt;1, neighboring points will reduce the weight of the focal point relative to the default, and when <code class="reqn">\alpha</code> is &gt;1, they will have less effect relative to the default. When all neighboring points are at or beyond the maximum distance of spatial autocorrelation, then the focal point gets a weight <code class="reqn">w_i</code> of 1. When at least neighboring one point is less than this distance away, the weight of the focal point will be &gt;0 but &lt;1.
</p>


<h3>Value</h3>

<p>A numeric vector of weights.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(sf)

# lemur occurrence data
data(lemurs)
wgs84 &lt;- getCRS('WGS84')
occs &lt;- lemurs[lemurs$species == 'Eulemur fulvus', ]
occs &lt;- sf::st_as_sf(occs, coords=c('longitude', 'latitude'), crs=wgs84)

# weights
maxDist &lt;- 30000 # in meters, for this example
w &lt;- weightByDist(occs, maxDist)

# plot
plot(st_geometry(occs), cex=5 * w, main='point size ~ weight')
plot(st_geometry(mad0), col='gainsboro', border='gray70', add=TRUE)
plot(st_geometry(occs), cex=5 * w, add=TRUE)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
