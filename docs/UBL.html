<!DOCTYPE html><html lang="en"><head><title>Help for package UBL</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {UBL}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#UBL-package'>
<p>UBL: Utility-Based Learning</p></a></li>
<li><a href='#AdasynClassif'>
<p>ADASYN algorithm for unbalanced classification problems, both binary and multi-class.</p></a></li>
<li><a href='#BaggingRegress'>
<p>Standard Bagging ensemble for regression problems.</p></a></li>
<li><a href='#BagModel-class'><p> Class &quot;BagModel&quot;</p></a></li>
<li><a href='#CNNClassif'>
<p>Condensed Nearest Neighbors strategy for multiclass imbalanced problems</p></a></li>
<li><a href='#distances'>
<p>Distance matrix between all data set examples according to a selected distance metric.</p></a></li>
<li><a href='#ENNClassif'>
<p>Edited Nearest Neighbor for multiclass imbalanced problems</p></a></li>
<li><a href='#EvalClassifMetrics'><p>Utility metrics for assessing the performance of utility-based classification tasks.</p></a></li>
<li><a href='#EvalRegressMetrics'><p>Utility metrics for assessing the performance of utility-based regression tasks.</p></a></li>
<li><a href='#GaussNoiseClassif'>
<p>Introduction of Gaussian Noise for the generation of synthetic examples to handle imbalanced multiclass problems.</p></a></li>
<li><a href='#GaussNoiseRegress'>
<p>Introduction of Gaussian Noise for the generation of synthetic examples to handle imbalanced regression problems</p></a></li>
<li><a href='#ImbC'><p>Synthetic Imbalanced Data Set for a Multi-class Task</p></a></li>
<li><a href='#ImbR'><p>Synthetic Regression Data Set</p></a></li>
<li><a href='#NCLClassif'>
<p>Neighborhood Cleaning Rule (NCL) algorithm for multiclass imbalanced problems</p></a></li>
<li><a href='#neighbours'><p>Computation of nearest neighbours using a selected distance function.</p></a></li>
<li><a href='#OSSClassif'>
<p>One-sided selection strategy for handling multiclass imbalanced problems.</p></a></li>
<li><a href='#phi'><p>Relevance function.</p></a></li>
<li><a href='#phi.control'><p>Estimation of parameters used for obtaining the relevance function.</p></a></li>
<li><a href='#predict+2CBagModel-method'><p>Predicting on new data with a <strong>BagModel</strong> model</p></a></li>
<li><a href='#RandOverClassif'>
<p>Random over-sampling for imbalanced classification problems</p></a></li>
<li><a href='#RandOverRegress'>
<p>Random over-sampling for imbalanced regression problems</p></a></li>
<li><a href='#RandUnderClassif'>
<p>Random under-sampling for imbalanced classification problems</p></a></li>
<li><a href='#RandUnderRegress'>
<p>Random under-sampling for imbalanced regression problems</p></a></li>
<li><a href='#ReBaggRegress'>
<p>REBaggRegress: RE(sampled) BAG(ging), an ensemble method for dealing with imbalanced regression problems.</p></a></li>
<li><a href='#SMOGNClassif'>
<p>SMOGN algorithm for imbalanced classification problems</p></a></li>
<li><a href='#SMOGNRegress'>
<p>SMOGN algorithm for imbalanced regression problems</p></a></li>
<li><a href='#SmoteClassif'>
<p>SMOTE algorithm for unbalanced classification problems</p></a></li>
<li><a href='#SmoteRegress'>
<p>SMOTE algorithm for imbalanced regression problems</p></a></li>
<li><a href='#TomekClassif'>
<p>Tomek links for imbalanced classification problems</p></a></li>
<li><a href='#UtilInterpol'>
<p>Utility surface obtained through methods for spatial interpolation of points.</p></a></li>
<li><a href='#UtilOptimClassif'>
<p>Optimization of predictions utility, cost or benefit for classification problems.</p></a></li>
<li><a href='#UtilOptimRegress'>
<p>Optimization of predictions utility, cost or benefit for regression problems.</p></a></li>
<li><a href='#WERCSClassif'>
<p>WEighted Relevance-based Combination Strategy (WERCS) algorithm for imbalanced classification problems</p></a></li>
<li><a href='#WERCSRegress'>
<p>WEighted Relevance-based Combination Strategy (WERCS) algorithm for imbalanced regression problems</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>An Implementation of Re-Sampling Approaches to Utility-Based
Learning for Both Classification and Regression Tasks</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides a set of functions that can be used to obtain better predictive performance on cost-sensitive and cost/benefits tasks (for both regression and classification). This includes re-sampling approaches that modify the original data set biasing it towards the user preferences.</td>
</tr>
<tr>
<td>Version:</td>
<td>0.0.9</td>
</tr>
<tr>
<td>Depends:</td>
<td>R(&ge; 3.0), methods, grDevices, graphics, stats, MBA, gstat,
automap, sp, randomForest</td>
</tr>
<tr>
<td>Suggests:</td>
<td>MASS, rpart, testthat, DMwR2, ggplot2, e1071</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-10-07</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/paobranco/UBL">https://github.com/paobranco/UBL</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/paobranco/UBL/issues">https://github.com/paobranco/UBL/issues</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>LazyData:</td>
<td>yes</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-10-07 14:14:41 UTC; paulab</td>
</tr>
<tr>
<td>Author:</td>
<td>Paula Branco [aut, cre],
  Rita Ribeiro [aut, ctb],
  Luis Torgo [aut, ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Paula Branco &lt;paobranco@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-10-07 20:40:07 UTC</td>
</tr>
</table>
<hr>
<h2 id='UBL-package'>
UBL: Utility-Based Learning
</h2><span id='topic+UBL-package'></span>

<h3>Description</h3>

<p>The package provides a diversity of pre-processing functions to deal with both classification (binary and multi-class) and regression problems that encompass non-uniform costs and/or benefits. These functions can be used to obtain a better predictive performance on this type of tasks. 
The package also includes two synthetic data sets for regression and classification.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Name: </td><td style="text-align: left;"> UBL</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 0.0.9</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2023-10-07</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL 2 GPL 3</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>The package in focused on utility-based learning, i.e., classification and regression problems with non-uniform benefits and/or costs. The main goal of the implemented functions is to improve the predictive performance of the models obtained. The package provides pre-processing approaches that change the original data set biasing it towards the user preferences.
</p>
<p>All the methods avaliable are suitable for classification (binary and multiclass) and regression tasks. Moreover, several distance functions are also implemented which allows the use of the methods in data sets with categorical and/or numeric features. 
</p>
<p>We also provide two synthetic data sets for classification and regression.
</p>


<h3>Author(s)</h3>

<p> Paula Branco <a href="mailto:paobranco@gmail.com">paobranco@gmail.com</a>, Rita Ribeiro
<a href="mailto:rpribeiro@dcc.fc.up.pt">rpribeiro@dcc.fc.up.pt</a> and Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> 
</p>
<p>Maintainer:
Paula Branco</p>


<h3>References</h3>

<p>Branco, P., Ribeiro, R.P. and Torgo, L. (2016) <em>UBL: an R package for Utility-based Learning.</em> arXiv preprint arXiv:1604.08079.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(UBL)
# an example with the synthetic classification data set provided with the package
data(ImbC)

plot(ImbC$X1, ImbC$X2, col = ImbC$Class, xlab = "X1", ylab = "X2")

summary(ImbC)
# randomly generate a 30-70% test and train partition 
i.train &lt;- sample(1:nrow(ImbC), as.integer(0.7*nrow(ImbC)))
trainD &lt;- ImbC[i.train,]
testD &lt;- ImbC[-i.train,]

model &lt;- rpart(Class~., trainD)
preds &lt;- predict(model, testD, type = "class")
table(preds, testD$Class)

# apply random over-sampling approach to balance the data set:

newTrain &lt;- RandOverClassif(Class~., trainD)

newModel &lt;- rpart(Class~., newTrain)
newPreds &lt;- predict(newModel, testD, type = "class")
table(newPreds, testD$Class)


# an example with the synthetic regression data set provided with the package
data(ImbR)

library(ggplot2)
ggplot(ImbR, aes(x = X1, y = X2)) + geom_point(data = ImbR, aes(colour=Tgt)) +
      scale_color_gradient(low = "red", high="blue")

boxplot(ImbR$Tgt)
#relevance function automatically obtained
phiF.args &lt;- phi.control(ImbR$Tgt, method = "extremes", extr.type = "high")
y.phi &lt;- phi(sort(ImbR$Tgt),control.parms = phiF.args)

plot(sort(ImbR$Tgt), y.phi, type = "l", xlab = "Tgt variable", ylab = "relevance value")

# set the train and test data
i.train &lt;- sample(1:nrow(ImbR), as.integer(0.7*nrow(ImbR)))
trainD &lt;- ImbR[i.train,]
testD &lt;- ImbR[-i.train,]

# train a model on the original train data
  if (requireNamespace("DMwR2", quietly = TRUE)) {
model &lt;- DMwR2::rpartXse(Tgt~., trainD, se = 0)

preds &lt;- DMwR2::predict(model, testD)

plot(testD$Tgt, preds, xlim = c(0,55), ylim = c(0,55))
abline(a = 0, b = 1)

# obtain a new train using random under-sampling strategy
newTrain &lt;- RandUnderRegress(Tgt~., trainD)
newModel &lt;- DMwR2::rpartXse(Tgt~., newTrain, se = 0)
newPreds &lt;- DMwR2::predict(newModel, testD)

# plot the predictions for the model obtained with 
# the original and the modified train data
plot(testD$Tgt, preds, xlim = c(0,55), ylim = c(0,55)) #black for original train
abline(a = 0, b = 1, lty=2, col="grey")
points(testD$Tgt, newPreds, col="blue", pch=2) #blue for changed train
abline(h=30, lty=2, col="grey")
abline(v=30, lty=2, col="grey")
}

## End(Not run)
</code></pre>

<hr>
<h2 id='AdasynClassif'>
ADASYN algorithm for unbalanced classification problems, both binary and multi-class.
</h2><span id='topic+AdasynClassif'></span>

<h3>Description</h3>

<p>This function handles unbalanced classification problems using the ADASYN
algorithm. This algorithm generates synthetic cases using a SMOTE-like approache.
However, the examples of the class(es) where over-sampling is applied are weighted
according to their level of difficulty in learning. This means that more synthetic
data is generated for cases which are hrder to learn compared to the examples of
the same class that are easier to learn. This implementation provides a strategy
suitable for both binary and multi-class problems.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>AdasynClassif(form, dat, baseClass = NULL, beta = 1, dth = 0.95,
                          k = 5, dist = "Euclidean", p = 2)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="AdasynClassif_+3A_form">form</code></td>
<td>

<p>A formula describing the prediction problem
</p>
</td></tr>
<tr><td><code id="AdasynClassif_+3A_dat">dat</code></td>
<td>

<p>A data frame containing the original (unbalanced) data set
</p>
</td></tr>
<tr><td><code id="AdasynClassif_+3A_baseclass">baseClass</code></td>
<td>

<p>Character specifying the reference class, i.e., the class from which all other
classes will be compared to. This can be selected by the user or estimated from the 
classes distribution. If not defined (the default) the majority class is selected.
</p>
</td></tr>
<tr><td><code id="AdasynClassif_+3A_beta">beta</code></td>
<td>

<p>Either a numeric value indicating the desired balance level after synthetic 
examples generation, or a named list specifying the selected classes beta 
value. A beta value of 1 (the default) corresponds to full balancing the 
classes. See examples. 
</p>
</td></tr>
<tr><td><code id="AdasynClassif_+3A_dth">dth</code></td>
<td>
<p>A threshold for the maximum tolerated degree of class imbalance ratio.
Defaults to 0.95, meaning that the strategy is applied if the 
imbalance ratio is more than 5%. This means that the strategy is applied to a class A if, the ratio of this class frequency and the baseClass frequency is less than 95% (|A|/|baseClass| &lt; 0.95).
</p>
</td></tr>
<tr><td><code id="AdasynClassif_+3A_k">k</code></td>
<td>

<p>A number indicating the number of nearest neighbors that are used to
generate the new examples of the minority class(es).
</p>
</td></tr>
<tr><td><code id="AdasynClassif_+3A_dist">dist</code></td>
<td>

<p>A character string indicating which distance metric to use when determining
the k nearest neighbors. See the details. Defaults to &quot;Euclidean&quot;.
</p>
</td></tr>
<tr><td><code id="AdasynClassif_+3A_p">p</code></td>
<td>

<p>A number indicating the value of p if the &quot;p-norm&quot; distance is chosen. Only 
necessary to define if a &quot;p-norm&quot; is chosen in the <code>dist</code> argument. 
See details.
</p>
</td></tr>
</table>


<h3>Details</h3>


<dl>
<dt><code>dist</code> parameter:</dt><dd><p>The parameter <code>dist</code> allows the user to define the distance metric to be used in the neighbors computation. Although the default is the Euclidean distance, other metrics are available. This allows the computation of distances in data sets with, for instance, both nominal and numeric features. The options available for the distance functions are as follows: 
</p>
<p>- for data with only numeric features: &quot;Manhattan&quot;, &quot;Euclidean&quot;, &quot;Canberra&quot;, &quot;Chebyshev&quot;, &quot;p-norm&quot;;
</p>
<p>- for data with only nominal features: &quot;Overlap&quot;;
</p>
<p>- for dealing with both nominal and numeric features: &quot;HEOM&quot;, &quot;HVDM&quot;.
</p>
<p>When the &quot;p-norm&quot; is selected for the <code>dist</code> parameter, it is also necessary to define the value of parameter <code>p</code>. The value of parameter <code>p</code> sets which &quot;p-norm&quot; will be used. For instance, if <code>p</code> is set to 1, the &quot;1-norm&quot; (or Manhattan distance) is used, and if <code>p</code> is set to 2, the &quot;2-norm&quot; (or Euclidean distance) is applied.
For more details regarding the distance functions implemented in UBL package please see the package vignettes.
</p>
</dd>
</dl>



<h3>Value</h3>

<p>The function returns a data frame with
the new data set resulting from the application of ADASYN
algorithm. 
</p>


<h3>Author(s)</h3>

<p> Paula Branco <a href="mailto:paobranco@gmail.com">paobranco@gmail.com</a>, Rita Ribeiro
<a href="mailto:rpribeiro@dcc.fc.up.pt">rpribeiro@dcc.fc.up.pt</a> and Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>He, H., Bai, Y., Garcia, E.A. and Li, S., 2008, June. 
<em>ADASYN: Adaptive synthetic sampling approach for imbalanced learning.</em>
In 2008 IEEE International Joint Conference on Neural Networks 
(IEEE World Congress on Computational Intelligence) (pp. 1322-1328). IEEE.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SmoteClassif">SmoteClassif</a>, <a href="#topic+RandOverClassif">RandOverClassif</a>, <a href="#topic+WERCSClassif">WERCSClassif</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example with an imbalanced multi-class problem
 data(iris)
 dat &lt;- iris[-c(45:75), c(1, 2, 5)]
# checking the class distribution of this artificial data set
 table(dat$Species)
 newdata &lt;- AdasynClassif(Species~., dat, beta=1)
 table(newdata$Species)
 beta &lt;- list("setosa"=1, "versicolor"=0.5)
 newdata &lt;- AdasynClassif(Species~., dat, baseClass="virginica", beta=beta)
 table(newdata$Species)

## Checking visually the created data
par(mfrow = c(1, 2))
plot(dat[, 1], dat[, 2], pch = 19 + as.integer(dat[, 3]),
     col = as.integer(dat[,3]), main = "Original Data",
     xlim=range(newdata[,1]), ylim=range(newdata[,2]))
plot(newdata[, 1], newdata[, 2], pch = 19 + as.integer(newdata[, 3]),
     col = as.integer(newdata[,3]), main = "New Data",
     xlim=range(newdata[,1]), ylim=range(newdata[,2]))

# A binary example
library(MASS)
data(cats)
table(cats$Sex)
Ada1cats &lt;- AdasynClassif(Sex~., cats)
table(Ada1cats$Sex)
Ada2cats &lt;- AdasynClassif(Sex~., cats, beta=5)
table(Ada2cats$Sex)

</code></pre>

<hr>
<h2 id='BaggingRegress'>
Standard Bagging ensemble for regression problems.
</h2><span id='topic+BaggingRegress'></span>

<h3>Description</h3>

<p>This function handles regression problems through ensemble learning. A given number of weak learners selected by the user are trained on bootstrap samples of the training data provided.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BaggingRegress(form, train, nmodels, learner, learner.pars,
               aggregation = "Average", quiet=TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="BaggingRegress_+3A_form">form</code></td>
<td>

<p>A formula describing the prediction problem.
</p>
</td></tr>
<tr><td><code id="BaggingRegress_+3A_train">train</code></td>
<td>

<p>A data frame containing the training (imbalanced) data set.
</p>
</td></tr>
<tr><td><code id="BaggingRegress_+3A_nmodels">nmodels</code></td>
<td>

<p>A numeric indicating the number of models to train. 
</p>
</td></tr>
<tr><td><code id="BaggingRegress_+3A_learner">learner</code></td>
<td>

<p>The learning algorithm to be used as weak learner.
</p>
</td></tr>
<tr><td><code id="BaggingRegress_+3A_learner.pars">learner.pars</code></td>
<td>

<p>A named list with the parameters selected for the learner.
</p>
</td></tr>
<tr><td><code id="BaggingRegress_+3A_aggregation">aggregation</code></td>
<td>
<p>charater specifying the method used for aggregating the results
obtained by the individual learners.
For now, the only method available is by averaging the models predictions.
</p>
</td></tr>
<tr><td><code id="BaggingRegress_+3A_quiet">quiet</code></td>
<td>
<p>logical specifying if development should be shown or not.Defaults to TRUE
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns an object of class BagModel. 
</p>


<h3>Author(s)</h3>

<p> Paula Branco <a href="mailto:paobranco@gmail.com">paobranco@gmail.com</a>, Rita Ribeiro
<a href="mailto:rpribeiro@dcc.fc.up.pt">rpribeiro@dcc.fc.up.pt</a> and Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>

<hr>
<h2 id='BagModel-class'> Class &quot;BagModel&quot;</h2><span id='topic+BagModel'></span><span id='topic+BagModel-class'></span><span id='topic+show+2CBagModel-method'></span>

<h3>Description</h3>

<p><strong>BagModel</strong> is an S4 class that contains the information of a bagging ensemble model.
Besides the base learning algorithms&ndash;<code>baseModels</code> &ndash;
<strong>BagModel</strong> class contains information regarding the
type of weak learners selected, the learned models and the aggregation method to apply in the 
test set for obtaining the final predictions.
</p>


<h3>Objects from BagModel Class</h3>

<p>Objects can be created by calls to the constructor
<code>BagModel(...)</code>. The constructor requires a formula and a training set,
the selected model type, the base models learned and the aggregation method
to use for obtaining the final predictions.
</p>


<h3>Slots</h3>


<dl>
<dt><code>form</code></dt><dd><p>formula</p>
</dd>
<dt><code>train</code></dt><dd><p>training set used to train the <code>baseModels</code></p>
</dd>
<dt><code>learner</code></dt><dd><p>the weak learners type used</p>
</dd>
<dt><code>learner.pars</code></dt><dd><p>the parameters of the used learners</p>
</dd>
<dt><code>baseModels</code></dt><dd><p>a list of base learners</p>
</dd>
<dt><code>aggregation</code></dt><dd><p>the aggregation method
used to obtain the final predictions. For now, only the <code>average</code> method is available.</p>
</dd>
<dt><code>rel</code></dt><dd><p>optional information regarding the relevance function definde for the problem. Should be provided as a matrix. 
</p>
</dd>
<dt><code>thr.rel</code></dt><dd><p>Optional number setting the threshold on the relevance values.
</p>
</dd>
<dt><code>quiet</code></dt><dd><p>logical specifying if development should be shown or not. Defaults to TRUE</p>
</dd>
</dl>


<h3>See Also</h3>

<p><code><a href="#topic+BaggingRegress">BaggingRegress</a></code>
</p>

<hr>
<h2 id='CNNClassif'>
Condensed Nearest Neighbors strategy for multiclass imbalanced problems
</h2><span id='topic+CNNClassif'></span>

<h3>Description</h3>

<p>This function applies the Condensed Nearest Neighbors (CNN) strategy for imbalanced multiclass problems. It constructs a subset of examples which are able to correctly classify the original data set using a one nearest neighbor rule.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CNNClassif(form, dat, dist = "Euclidean", p = 2, Cl = "smaller")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="CNNClassif_+3A_form">form</code></td>
<td>

<p>A formula describing the prediction problem.
</p>
</td></tr>
<tr><td><code id="CNNClassif_+3A_dat">dat</code></td>
<td>

<p>A data frame containing the original imbalanced data set.
</p>
</td></tr>
<tr><td><code id="CNNClassif_+3A_dist">dist</code></td>
<td>

<p>A character string indicating which distance metric to use when determining the k nearest neighbors. See the details. Defaults to &quot;Euclidean&quot;.
</p>
</td></tr>
<tr><td><code id="CNNClassif_+3A_p">p</code></td>
<td>

<p>A number indicating the value of p if the &quot;p-norm&quot; distance is chosen. Only necessary to define if a &quot;p-norm&quot; is chosen in the <code>dist</code> argument. See details.
</p>
</td></tr>
<tr><td><code id="CNNClassif_+3A_cl">Cl</code></td>
<td>

<p>A character vector indicating which are the most important classes. Defaults to &quot;smaller&quot; which means that the smaller classes are automatically determined. In this case, all the smaller classes are those with a frequency below #examples/#classes. With the selection of option &quot;smaller&quot; those classes are the ones considered important for the user.</p>
</td></tr>
</table>


<h3>Details</h3>


<dl>
<dt><code>dist</code> parameter:</dt><dd><p>The parameter <code>dist</code> allows the user to define the distance metric to be used in the neighbors computation. Although the default is the Euclidean distance, other metrics are available. This allows the computation of distances in data sets with, for instance, both nominal and numeric features. The options available for the distance functions are as follows: 
</p>
<p>- for data with only numeric features: &quot;Manhattan&quot;, &quot;Euclidean&quot;, &quot;Canberra&quot;, &quot;Chebyshev&quot;, &quot;p-norm&quot;;
</p>
<p>- for data with only nominal features: &quot;Overlap&quot;;
</p>
<p>- for dealing with both nominal and numeric features: &quot;HEOM&quot;, &quot;HVDM&quot;.
</p>
<p>When the &quot;p-norm&quot; is selected for the <code>dist</code> parameter, it is also necessary to define the value of parameter <code>p</code>. The value of parameter <code>p</code> sets which &quot;p-norm&quot; will be used. For instance, if <code>p</code> is set to 1, the &quot;1-norm&quot; (or Manhattan distance) is used, and if <code>p</code> is set to 2, the &quot;2-norm&quot; (or Euclidean distance) is applied.
For more details regarding the distance functions implemented in UBL package please see the package vignettes.
</p>
</dd>
<dt>CNN algorithm:</dt><dd><p>This function applies the Condensed Nearest Neighbors (CNN) strategy for dealing with imbalanced multiclass problems. The classes selected in <code>Cl</code> are considered the most important ones and all the others are under-sampled. The CNN under-sampling strategy starts with a set composed by all the examples from the important classes and one randomly selected example from the other classes. Then, examples from the other classes are added to the set forming a subset of examples which correctly classifies the original data set using a one nearest neighbor rule.
</p>
</dd>
</dl>



<h3>Value</h3>

<p>The function returns a list with a data frame with
the new data set resulting from the application of the CNN strategy, a character vector with the important classes, and another character vector with the unimportant classes.
</p>


<h3>Author(s)</h3>

<p> Paula Branco <a href="mailto:paobranco@gmail.com">paobranco@gmail.com</a>, Rita Ribeiro
<a href="mailto:rpribeiro@dcc.fc.up.pt">rpribeiro@dcc.fc.up.pt</a> and Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Hart, P. E. (1968). <em>The condensed nearest neighbor rule</em> IEEE Transactions on Information Theory, 14, 515-516
</p>


<h3>See Also</h3>

<p><code><a href="#topic+OSSClassif">OSSClassif</a>, <a href="#topic+TomekClassif">TomekClassif</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  if (requireNamespace("DMwR2", quietly = TRUE)) {
  
  data(algae, package ="DMwR2")
  clean.algae &lt;- data.frame(algae[complete.cases(algae), ])
  myCNN &lt;- CNNClassif(season~., clean.algae, 
                      Cl = c("summer", "spring", "winter"),
                      dist = "HEOM")
  CNN1 &lt;- CNNClassif(season~., clean.algae, Cl = "smaller", dist = "HEOM")
  CNN2&lt;- CNNClassif(season~., clean.algae, Cl = "summer",dist = "HVDM")
  summary(myCNN[[1]]$season)
  summary(CNN1[[1]]$season)
  summary(CNN2[[1]]$season)
  
  
  library(MASS)
  data(cats)
  CNN.catsF &lt;- CNNClassif(Sex~., cats, Cl = "F")
  CNN.cats &lt;- CNNClassif(Sex~., cats, Cl = "smaller")
  
     } else {
     
  library(MASS)
  data(cats)
  CNN.catsF &lt;- CNNClassif(Sex~., cats, Cl = "F")
  CNN.cats &lt;- CNNClassif(Sex~., cats, Cl = "smaller")

   }

</code></pre>

<hr>
<h2 id='distances'>
Distance matrix between all data set examples according to a selected distance metric. 
</h2><span id='topic+distances'></span>

<h3>Description</h3>

<p>This function computes the distances between all examples in a data set using a selected distance metric. The metrics available are suitable for data sets with numeric and/or nominal features and include, among others: Euclidean, Manhattan, HEOM and HVDM.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>distances(tgt, dat, dist, p=2)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="distances_+3A_tgt">tgt</code></td>
<td>

<p>The index of the column of the problem target variable.
</p>
</td></tr>
<tr><td><code id="distances_+3A_dat">dat</code></td>
<td>
<p>A data frame containing the problem data.
</p>
</td></tr>
<tr><td><code id="distances_+3A_dist">dist</code></td>
<td>
<p>A character string specifying the distance function to use in the nearest neighbours evaluation.
</p>
</td></tr>
<tr><td><code id="distances_+3A_p">p</code></td>
<td>
<p>An optional parameter that is only required if the distance function selected in parameter <code>dist</code> is &quot;p-norm&quot;.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Several distance function are implemented in UBL package. The goal of having such a diversity of distance functions is to provide the users more flexibility regarding the distance used and also to provide distance fucntions that are able to deal with nominal and numeric features. The options available for the distance functions are as follows: 
</p>

<dl>
<dt>data with only numeric features:</dt><dd><p> &quot;Manhattan&quot;, &quot;Euclidean&quot;, &quot;Canberra&quot;, &quot;Chebyshev&quot;, &quot;p-norm&quot;;</p>
</dd>
<dt>data with only nominal features:</dt><dd><p> &quot;Overlap&quot;;</p>
</dd>
<dt>data with both nominal and numeric features:</dt><dd><p> &quot;HEOM&quot;, &quot;HVDM&quot;.</p>
</dd>
</dl>

<p>When the &quot;p-norm&quot; is selected for the <code>dist</code> parameter, it is also necessary to define the value of parameter <code>p</code>. The value of parameter <code>p</code> sets which &quot;p-norm&quot; will be used. For instance, if <code>p</code> is set to 1, the &quot;1-norm&quot; (or Manhattan distance) is used, and if <code>p</code> is set to 2, the &quot;2-norm&quot; (or Euclidean distance) is applied.
For more details regarding the distance functions implemented in UBL package please see the package vignettes.
</p>


<h3>Value</h3>

<p>The function returns a matrix with the distances computed between each pair of examples in the data set. 
</p>


<h3>Author(s)</h3>

<p> Paula Branco <a href="mailto:paobranco@gmail.com">paobranco@gmail.com</a>, Rita Ribeiro
<a href="mailto:rpribeiro@dcc.fc.up.pt">rpribeiro@dcc.fc.up.pt</a> and Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> 
</p>


<h3>References</h3>

<p>Wilson, D.R. and Martinez, T.R. (1997). <em>Improved heterogeneous distance functions.</em> Journal of artificial intelligence research, pp.1-34.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+neighbours">neighbours</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data(ImbC)
# determine the distances between each example in ImbC data set
# using the "HVDM" distance function.
dist1 &lt;- distances(3, ImbC, "HVDM")

# now using the "HEOM" distance function
dist2 &lt;- distances(3, ImbC, "HEOM")

# check the differences
head(dist1)
head(dist2)

## End(Not run)
</code></pre>

<hr>
<h2 id='ENNClassif'>
Edited Nearest Neighbor for multiclass imbalanced problems
</h2><span id='topic+ENNClassif'></span>

<h3>Description</h3>

<p>This function handles imbalanced classification problems using the Edited Nearest Neighbor (ENN) algorithm. It removes examples whose class label differs from the class of at least half of its k nearest neighbors. All the existing classes can be under-sampled with this technique. Alternatively a subset of classes to under-sample can be provided by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ENNClassif(form, dat, k = 3, dist = "Euclidean", p = 2, Cl = "all")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ENNClassif_+3A_form">form</code></td>
<td>

<p>A formula describing the prediction problem.
</p>
</td></tr>
<tr><td><code id="ENNClassif_+3A_dat">dat</code></td>
<td>

<p>A data frame containing the original (imbalanced) data set.
</p>
</td></tr>
<tr><td><code id="ENNClassif_+3A_k">k</code></td>
<td>

<p>A number indicating the number of nearest neighbors to use.
</p>
</td></tr>
<tr><td><code id="ENNClassif_+3A_dist">dist</code></td>
<td>

<p>A character string indicating which distance metric to use when determining the k nearest neighbors. See the details. Defaults to &quot;Euclidean&quot;.
</p>
</td></tr>
<tr><td><code id="ENNClassif_+3A_p">p</code></td>
<td>

<p>A number indicating the value of p if the &quot;p-norm&quot; distance is chosen. Only necessary to define if a &quot;p-norm&quot; is chosen in the <code>dist</code> argument. See details.
</p>
</td></tr>
<tr><td><code id="ENNClassif_+3A_cl">Cl</code></td>
<td>

<p>A character vector indicating which classes should be under-sampled. Defaults to &quot;all&quot; meaning that all classes are candidates for having examples removed. The user may define a subset of the existing classes in which this technique will be applied. 
</p>
</td></tr>
</table>


<h3>Details</h3>


<dl>
<dt><code>dist</code> parameter:</dt><dd><p>The parameter <code>dist</code> allows the user to define the distance metric to be used in the neighbors computation. Although the default is the Euclidean distance, other metrics are available. This allows the computation of distances in data sets with, for instance, both nominal and numeric features. The options available for the distance functions are as follows: 
</p>
<p>- for data with only numeric features: &quot;Manhattan&quot;, &quot;Euclidean&quot;, &quot;Canberra&quot;, &quot;Chebyshev&quot;, &quot;p-norm&quot;;
</p>
<p>- for data with only nominal features: &quot;Overlap&quot;;
</p>
<p>- for dealing with both nominal and numeric features: &quot;HEOM&quot;, &quot;HVDM&quot;.
</p>
<p>When the &quot;p-norm&quot; is selected for the <code>dist</code> parameter, it is also necessary to define the value of parameter <code>p</code>. The value of parameter <code>p</code> sets which &quot;p-norm&quot; will be used. For instance, if <code>p</code> is set to 1, the &quot;1-norm&quot; (or Manhattan distance) is used, and if <code>p</code> is set to 2, the &quot;2-norm&quot; (or Euclidean distance) is applied.
For more details regarding the distance functions implemented in UBL package please see the package vignettes.
</p>
</dd>
<dt>ENN algorithm:</dt><dd><p>The ENN algorithm uses a cleaning method to perform under-sampling. For each example with class label in Cl the k nearest neighbors are computed using a selected distance metric. The example is removed from the data set if it is misclassified by at least half of it's k nearest neighbors. Usually this algorithm uses k=3.
</p>
</dd>
</dl>



<h3>Value</h3>

<p>The function returns a list containing a data frame with
the new data set resulting from the application of the ENN
algorithm, and the indexes of the examples removed. 
</p>


<h3>Author(s)</h3>

<p> Paula Branco <a href="mailto:paobranco@gmail.com">paobranco@gmail.com</a>, Rita Ribeiro
<a href="mailto:rpribeiro@dcc.fc.up.pt">rpribeiro@dcc.fc.up.pt</a> and Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>D. Wilson. (1972). <em>Asymptotic properties of nearest neighbor rules using edited data</em>. Systems, Man and Cybernetics, IEEE Transactions on, 408-421.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+NCLClassif">NCLClassif</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# generate an small imbalanced data set
  ir&lt;- iris[-c(95:130), ]
# use ENN technique with different metrics, number of neighbours and classes
  ir1norm &lt;- ENNClassif(Species~., ir, k = 5, dist = "p-norm", 
                        p = 1, Cl = "all")
  irEucl &lt;- ENNClassif(Species~., ir) # defaults to Euclidean distance
  irCheby &lt;- ENNClassif(Species~., ir, k = 7, dist = "Chebyshev",
                       Cl = c("virginica", "setosa"))
  irHVDM &lt;- ENNClassif(Species~., ir, k = 3, dist = "HVDM")
# checking the impact
  summary(ir$Species)
  summary(ir1norm[[1]]$Species)
  summary(irEucl[[1]]$Species)
  summary(irCheby[[1]]$Species)
  summary(irHVDM[[1]]$Species)
# check the removed indexes of the ir1norm data set
  ir1norm[[2]]
</code></pre>

<hr>
<h2 id='EvalClassifMetrics'>Utility metrics for assessing the performance of utility-based classification tasks. 
</h2><span id='topic+EvalClassifMetrics'></span>

<h3>Description</h3>

<p>This function allows to evaluate utility-based metrics in classification problems which have defined a cost, benefit, or utility matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EvalClassifMetrics(trues, preds, mtr, type = "util", metrics = NULL, thr=0.5, beta = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="EvalClassifMetrics_+3A_trues">trues</code></td>
<td>

<p>A vector with the true target variable values of the problem.
</p>
</td></tr>
<tr><td><code id="EvalClassifMetrics_+3A_preds">preds</code></td>
<td>

<p>A vector with the prediction values obtained for the vector of trues.
</p>
</td></tr>
<tr><td><code id="EvalClassifMetrics_+3A_mtr">mtr</code></td>
<td>

<p>A matrix that can be either a cost, a benefit or a utility matrix. 
The matrix must be always provided with the true class in the rows and 
the predicted class in the columns.
</p>
</td></tr>
<tr><td><code id="EvalClassifMetrics_+3A_type">type</code></td>
<td>

<p>A character specifying the type of matrix provided. Can be set to &quot;cost&quot;, 
&quot;benefit&quot; or &quot;utility&quot; (the default).
</p>
</td></tr>
<tr><td><code id="EvalClassifMetrics_+3A_metrics">metrics</code></td>
<td>
<p>A character vector with the metrics names to be evaluated. If not specified (the default), all the metrics avaliable for the type of matrix provided are evaluated.
</p>
</td></tr>
<tr><td><code id="EvalClassifMetrics_+3A_thr">thr</code></td>
<td>
<p>A numeric value between 0 and 1 setting a threshold on the
relevance values for determining which are the important classes to consider.
This threshold is only necessary for the following metrics: precPhi, recPhi
and FPhi. Moreover, these metrics are only available for problems based on 
utility matrices. Defaults to 0.5.
</p>
</td></tr>
<tr><td><code id="EvalClassifMetrics_+3A_beta">beta</code></td>
<td>

<p>The numeric value of the beta parameter for F-score.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a named list with the evaluated metrics results. 
</p>


<h3>Author(s)</h3>

<p> Paula Branco <a href="mailto:paobranco@gmail.com">paobranco@gmail.com</a>, Rita Ribeiro
<a href="mailto:rpribeiro@dcc.fc.up.pt">rpribeiro@dcc.fc.up.pt</a> and Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Ribeiro, R., 2011. Utility-based regression 
(Doctoral dissertation, PhD thesis, 
Dep. Computer Science, Faculty of Sciences - 
University of Porto).
</p>
<p>Branco, P., 2014. Re-sampling Approaches for Regression Tasks under Imbalanced Domains 
(Msc thesis, Dep. Computer Science, Faculty of Sciences - 
University of Porto).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+phi.control">phi.control</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># the synthetic data set provided with UBL package for classification
data(ImbC)
sp &lt;- sample(1:nrow(ImbC), round(0.7*nrow(ImbC)))
train &lt;- ImbC[sp, ]
test &lt;- ImbC[-sp,]

# example with a utility matrix
# define a utility matrix (true class in rows and pred class in columns)
 matU &lt;- matrix(c(0.2, -0.5, -0.3, -1, 1, -0.9, -0.9, -0.8, 0.9), byrow=TRUE, ncol=3)
# determine optimal preds (predictions that maximize utility)
library(e1071) # for the naiveBayes classifier
 resUtil &lt;- UtilOptimClassif(Class~., train, test, mtr = matU, type="util",
                        learner = "naiveBayes", 
                        predictor.pars = list(type="raw", threshold = 0.01))
 
# learning a model without maximizing utility
 model &lt;- naiveBayes(Class~., train)
 resNormal &lt;- predict(model, test, type="class", threshold = 0.01)
#Check the difference in the total utility of the results
 EvalClassifMetrics(test$Class, resNormal, mtr=matU, type= "util")
 EvalClassifMetrics(test$Class, resUtil, mtr=matU, type= "util")
   
# example with a classification task that has a cost matrix associated
# define a cost matrix (true class in rows and pred class in columns)
 matC &lt;- matrix(c(0, 0.5, 0.3, 1, 0, 0.9, 0.9, 0.8, 0), byrow=TRUE, ncol=3)
 resUtil &lt;- UtilOptimClassif(Class~., train, test, mtr = matC, type="cost",
                            learner = "naiveBayes", 
                            predictor.pars = list(type="raw", threshold = 0.01))
 
 # learning a model without maximizing utility
 model &lt;- naiveBayes(Class~., train)
 resNormal &lt;- predict(model, test, type="class")
 #Check the difference in the total utility of the results
 EvalClassifMetrics(test$Class, resNormal, mtr=matC, type= "cost")
 EvalClassifMetrics(test$Class, resUtil, mtr=matC, type= "cost")
 
#example with a benefit matrix
# define a benefit matrix (true class in rows and pred class in columns)
 matB &lt;- matrix(c(0.2, 0, 0, 0, 1, 0, 0, 0, 0.9), byrow=TRUE, ncol=3)
 
 resUtil &lt;- UtilOptimClassif(Class~., train, test, mtr = matB, type="ben",
                            learner = "naiveBayes", 
                            predictor.pars = list(type="raw", threshold = 0.01))
 
# learning a model without maximizing utility
 model &lt;- naiveBayes(Class~., train)
 resNormal &lt;- predict(model, test, type="class", threshold = 0.01)
# Check the difference in the total utility of the results
 EvalClassifMetrics(test$Class, resNormal, mtr=matB, type= "ben")
 EvalClassifMetrics(test$Class, resUtil, mtr=matB, type= "ben")
 
 table(test$Class,resNormal)
 table(test$Class,resUtil)

</code></pre>

<hr>
<h2 id='EvalRegressMetrics'>Utility metrics for assessing the performance of utility-based regression tasks. 
</h2><span id='topic+EvalRegressMetrics'></span>

<h3>Description</h3>

<p>This function allows to evaluate utility-based metrics in regression problems which have defined a cost, benefit, or utility surface.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EvalRegressMetrics(trues, preds, util.vals, type = "util",
metrics = NULL, thr = 0.5, control.parms = NULL, 
beta = 1, maxC = NULL, maxB = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="EvalRegressMetrics_+3A_trues">trues</code></td>
<td>
<p>A vector with the true target variable values of the problem.
</p>
</td></tr>
<tr><td><code id="EvalRegressMetrics_+3A_preds">preds</code></td>
<td>
<p>A vector with the prediction values obtained for the vector of trues.
</p>
</td></tr>
<tr><td><code id="EvalRegressMetrics_+3A_util.vals">util.vals</code></td>
<td>
<p>Either the cost, benefit or utility values corresponding to 
the provided points (trues, preds).
</p>
</td></tr>
<tr><td><code id="EvalRegressMetrics_+3A_type">type</code></td>
<td>

<p>A character specifying the type of surface under consideration. Can be set to &quot;cost&quot;, 
&quot;benefit&quot; or &quot;utility&quot; (the default).
</p>
</td></tr>
<tr><td><code id="EvalRegressMetrics_+3A_metrics">metrics</code></td>
<td>
<p>A character vector with the metrics names to be evaluated. 
If not specified (the default), all the metrics avaliable for the type of
surface provided are evaluated.
</p>
</td></tr>
<tr><td><code id="EvalRegressMetrics_+3A_thr">thr</code></td>
<td>
<p>A numeric value between 0 and 1 setting a threshold on the
relevance values for determining which are the important cases to consider.
This threshold is only necessary for the following metrics: precPhi, recPhi
and FPhi. Moreover, these metrics are only available for problems based on 
utility surfaces. Defaults to 0.5.
</p>
</td></tr>
<tr><td><code id="EvalRegressMetrics_+3A_control.parms">control.parms</code></td>
<td>
<p>the control.parms of the relevance function phi. 
Can be obtained through function <a href="#topic+phi.control">phi.control</a>.
These are only necessary for evaluating the following utility metrics:
recPhi, precPhi and FPhi.
</p>
</td></tr>
<tr><td><code id="EvalRegressMetrics_+3A_beta">beta</code></td>
<td>

<p>The numeric value of the beta parameter for F-score.
</p>
</td></tr>
<tr><td><code id="EvalRegressMetrics_+3A_maxc">maxC</code></td>
<td>
<p>the maximum cost achievable in the cost surface. Parameter 
only required when the problem depends on a cost surface.
</p>
</td></tr>
<tr><td><code id="EvalRegressMetrics_+3A_maxb">maxB</code></td>
<td>
<p>the maximum Benefit achievable in the benefit surface. 
Parameter only required when the problem depends on a benefit surface.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a named list with the evaluated metrics results. 
</p>


<h3>Author(s)</h3>

<p> Paula Branco <a href="mailto:paobranco@gmail.com">paobranco@gmail.com</a>, Rita Ribeiro
<a href="mailto:rpribeiro@dcc.fc.up.pt">rpribeiro@dcc.fc.up.pt</a> and Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Ribeiro, R., 2011. Utility-based regression 
(Doctoral dissertation, PhD thesis, 
Dep. Computer Science, Faculty of Sciences - 
University of Porto).
</p>
<p>Branco, P., 2014. Re-sampling Approaches for Regression Tasks under Imbalanced Domains 
(Msc thesis, Dep. Computer Science, Faculty of Sciences - 
University of Porto).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+phi.control">phi.control</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#Example using a utility surface interpolated and observing the performance of 
# two models: i) a model obtained with a strategy designed for maximizing 
# predictions utility and a model obtained through a standard random Forest.

data(Boston, package = "MASS")

tgt &lt;- which(colnames(Boston) == "medv")
sp &lt;- sample(1:nrow(Boston), as.integer(0.7*nrow(Boston)))
train &lt;- Boston[sp,]
test &lt;- Boston[-sp,]

control.parms &lt;- phi.control(Boston[,tgt], method="extremes", extr.type="both")
# the boundaries of the domain considered
minds &lt;- min(train[,tgt])
maxds &lt;- max(train[,tgt])

# build m.pts to include at least (minds, maxds) and (maxds, minds) points
# m.pts must only contain points in [minds, maxds] range.
m.pts &lt;- matrix(c(minds, maxds, -1, maxds, minds, -1),
                byrow=TRUE, ncol=3)

pred.res &lt;- UtilOptimRegress(medv~., train, test, type = "util", strat = "interpol",
                             strat.parms=list(method = "bilinear"),
                             control.parms = control.parms,
                             m.pts = m.pts, minds = minds, maxds = maxds)

# assess the performance
eval.util &lt;- EvalRegressMetrics(test$medv, pred.res$optim, pred.res$utilRes,
                                thr = 0.8, control.parms = control.parms)

# now train a normal model
model &lt;- randomForest(medv~.,train)
normal.preds &lt;- predict(model, test)

#obtain the utility of this model predictions
NormalUtil &lt;- UtilInterpol(test$medv, normal.preds, type = "util",
                           control.parms = control.parms,
                           minds, maxds, m.pts, method = "bilinear")

#check the performance
eval.normal &lt;- EvalRegressMetrics(test$medv, normal.preds, NormalUtil,
                                  thr=0.8, control.parms = control.parms)

# 3 check visually the utility surface and the predictions of both models 
UtilInterpol(NULL,NULL, type = "util", control.parms = control.parms,
                           minds, maxds, m.pts, method = "bilinear", 
                           visual=TRUE, full.output = TRUE)
points(test$medv, normal.preds) # standard model predition points
points(test$medv, pred.res$optim, col="blue") # model with optimized predictions

## End(Not run)
</code></pre>

<hr>
<h2 id='GaussNoiseClassif'>
Introduction of Gaussian Noise for the generation of synthetic examples to handle imbalanced multiclass problems.
</h2><span id='topic+GaussNoiseClassif'></span>

<h3>Description</h3>

<p>This strategy performs both over-sampling and under-sampling. The under-sampling is randomly performed on the examples of the classes defined by the user through the <code>C.perc</code> parameter. Regarding the over-sampling method, this is based on the generation of new synthetic examples with the introduction of a small perturbation on existing examples through Gaussian noise. A new example from a minority class is obtained by perturbing each feature a percentage of its standard deviation (evaluated on the minority class examples). For nominal features, the new example randomly selects a label according to the frequency of examples belonging to the minority class. The <code>C.perc</code> parameter is also used to express which percentage of over-sampling should be applied and to which classes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GaussNoiseClassif(form, dat, C.perc = "balance", pert = 0.1, repl = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="GaussNoiseClassif_+3A_form">form</code></td>
<td>

<p>A formula describing the prediction problem
</p>
</td></tr>
<tr><td><code id="GaussNoiseClassif_+3A_dat">dat</code></td>
<td>

<p>A data frame containing the original (unbalanced) data set
</p>
</td></tr>
<tr><td><code id="GaussNoiseClassif_+3A_c.perc">C.perc</code></td>
<td>

<p>A named list containing the percentage(s) of under- or/and  over-sampling to apply to each class.         The over-sampling percentage is a number above 1 while the under-sampling percentage should be a number below 1. If the number 1 is provided for a given class then that class remains unchanged. Alternatively it may be &quot;balance&quot; (the default) or &quot;extreme&quot;, cases where the sampling percentages are automatically estimated either to balance the examples between the minority and majority classes or to invert the distribution of examples across the existing classes transforming the majority classes into minority and vice-versa.
</p>
</td></tr>
<tr><td><code id="GaussNoiseClassif_+3A_pert">pert</code></td>
<td>

<p>A number indicating the level of perturbation to introduce when generating synthetic examples. Assuming as center the base example, this parameter defines the radius (based on the standard deviation) where the new example is generated.
</p>
</td></tr>
<tr><td><code id="GaussNoiseClassif_+3A_repl">repl</code></td>
<td>

<p>A boolean value controlling the possibility of having repetition of examples when performing under-sampling by selecting among the majority class(es) examples.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a data frame with
the new data set resulting from the application of random under-sampling and over-sampling through the generation of synthetic examples using Gaussian noise. 
</p>


<h3>Author(s)</h3>

<p> Paula Branco <a href="mailto:paobranco@gmail.com">paobranco@gmail.com</a>, Rita Ribeiro
<a href="mailto:rpribeiro@dcc.fc.up.pt">rpribeiro@dcc.fc.up.pt</a> and Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Sauchi Stephen Lee. (1999) <em>Regularization in skewed binary classification.</em> Computational Statistics Vol.14, Issue 2, 277-292.
</p>
<p>Sauchi Stephen Lee. (2000) <em>Noisy replication in skewed binary classification.</em> Computaional stistics and data analysis Vol.34, Issue 2, 165-191.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SmoteClassif">SmoteClassif</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  if (requireNamespace("DMwR2", quietly = TRUE)) {
data(algae, package ="DMwR2")
clean.algae &lt;- data.frame(algae[complete.cases(algae), ])
# autumn and summer are the most important classes and winter
# is the least important
C.perc = list(autumn = 3, summer = 1.5, winter = 0.2)
gn &lt;- GaussNoiseClassif(season~., clean.algae, C.perc)
table(algae$season)
table(gn$season)
} else {
# another example
data(iris)
dat &lt;- iris[, c(1, 2, 5)]
dat$Species &lt;- factor(ifelse(dat$Species == "setosa", "rare", "common")) 
## checking the class distribution of this artificial data set
table(dat$Species)
## now using gaussian noise to create a more "balanced problem"
new.gn &lt;- GaussNoiseClassif(Species ~ ., dat)
table(new.gn$Species)
## Checking visually the created data
 par(mfrow = c(1, 2))
 plot(dat[, 1], dat[, 2], pch = as.integer(dat[, 3]), 
      col = as.integer(dat[, 3]), main = "Original Data")
 plot(new.gn[, 1], new.gn[, 2], pch = as.integer(new.gn[, 3]),
      col = as.integer(new.gn[, 3]), main = "Data with Gaussian Noise")
      }

</code></pre>

<hr>
<h2 id='GaussNoiseRegress'>
Introduction of Gaussian Noise for the generation of synthetic examples to handle imbalanced regression problems
</h2><span id='topic+GaussNoiseRegress'></span>

<h3>Description</h3>

<p>This strategy performs both over-sampling and under-sampling. The under-sampling is randomly performed on the examples below the relevance threshold defined by the user. Regarding the over-sampling method, this is based on the generation of new synthetic examples with the introduction of a small perturbation on existing examples through Gaussian noise. A new example from a rare &quot;class&quot;&quot; is obtained by perturbing all the features and the target variable a percentage of its standard deviation (evaluated on the rare examples). The value of nominal features of the new example is randomly selected according to the frequency of the values existing in the rare cases of the bump in consideration.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GaussNoiseRegress(form, dat, rel = "auto", thr.rel = 0.5, C.perc = "balance", 
                  pert = 0.1, repl = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="GaussNoiseRegress_+3A_form">form</code></td>
<td>

<p>A formula describing the prediction problem
</p>
</td></tr>
<tr><td><code id="GaussNoiseRegress_+3A_dat">dat</code></td>
<td>

<p>A data frame containing the original (unbalanced) data set
</p>
</td></tr>
<tr><td><code id="GaussNoiseRegress_+3A_rel">rel</code></td>
<td>

<p>The relevance function which can be automatically (&quot;auto&quot;) determined (the default) or may be provided by the user through a matrix with interpolating points.
</p>
</td></tr>
<tr><td><code id="GaussNoiseRegress_+3A_thr.rel">thr.rel</code></td>
<td>

<p>A number indicating the relevance threshold above which a case is considered as belonging to the rare &quot;class&quot;.
</p>
</td></tr>
<tr><td><code id="GaussNoiseRegress_+3A_c.perc">C.perc</code></td>
<td>

<p>A list containing the percentage(s) of under- or/and  over-sampling to apply to each &quot;class&quot; (bump) obtained with the threshold. The <code>C.perc</code> values should be provided in ascending order of target variable values. The over-sampling percentage(s) should be numbers above 1 and represent the increase that is applied to the examples of the bump. The under-sampling percentage(s) should be numbers below 1 and represent the decrease applied to the cases in the corresponding bump. If the value of 1 is provided for a given bump, then the examples in that bump will remain unchanged. Alternatively it may be &quot;balance&quot; (the default) or &quot;extreme&quot;, cases where the sampling percentages are automatically estimated.
</p>
</td></tr>
<tr><td><code id="GaussNoiseRegress_+3A_pert">pert</code></td>
<td>

<p>A number indicating the level of perturbation to introduce when generating synthetic examples. Assuming as center the base example, this parameter defines the radius (based on the standard deviation) where the new example is generated.
</p>
</td></tr>
<tr><td><code id="GaussNoiseRegress_+3A_repl">repl</code></td>
<td>

<p>A boolean value controlling the possibility of having repetition of examples when performing under-sampling by selecting among the &quot;normal&quot; examples.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a data frame with
the new data set resulting from the application of random under-sampling and over-sampling through the generation of synthetic examples using Gaussian noise. 
</p>


<h3>Author(s)</h3>

<p> Paula Branco <a href="mailto:paobranco@gmail.com">paobranco@gmail.com</a>, Rita Ribeiro
<a href="mailto:rpribeiro@dcc.fc.up.pt">rpribeiro@dcc.fc.up.pt</a> and Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Sauchi Stephen Lee. (1999) <em>Regularization in skewed binary classification.</em> Computational Statistics Vol.14, Issue 2, 277-292.
</p>
<p>Sauchi Stephen Lee. (2000) <em>Noisy replication in skewed binary classification.</em> Computaional stistics and data analysis Vol.34, Issue 2, 165-191.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SmoteRegress">SmoteRegress</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  if (requireNamespace("DMwR2", quietly = TRUE)) {
  data(algae, package ="DMwR2")
  clean.algae &lt;- data.frame(algae[complete.cases(algae), ])
  C.perc = list(0.5, 3) 
  mygn.alg &lt;- GaussNoiseRegress(a7~., clean.algae, C.perc = C.perc)
  gnB.alg &lt;- GaussNoiseRegress(a7~., clean.algae, C.perc = "balance", 
                               pert = 0.1)
  gnE.alg &lt;- GaussNoiseRegress(a7~., clean.algae, C.perc = "extreme")
  
  plot(density(clean.algae$a7))
  lines(density(gnE.alg$a7), col = 2)
  lines(density(gnB.alg$a7), col = 3)
  lines(density(mygn.alg$a7), col = 4)


} else {
  ir &lt;- iris[-c(95:130), ]
  mygn1.iris &lt;- GaussNoiseRegress(Sepal.Width~., ir, C.perc = list(0.5, 2.5))
  mygn2.iris &lt;- GaussNoiseRegress(Sepal.Width~., ir, C.perc = list(0.2, 4),
                                  thr.rel = 0.8)
  gnB.iris &lt;- GaussNoiseRegress(Sepal.Width~., ir, C.perc = "balance")
  gnE.iris &lt;- GaussNoiseRegress(Sepal.Width~., ir, C.perc = "extreme")
  
  # defining a relevance function
  rel &lt;- matrix(0, ncol = 3, nrow = 0)
  rel &lt;- rbind(rel, c(2, 1, 0))
  rel &lt;- rbind(rel, c(3, 0, 0))
  rel &lt;- rbind(rel, c(4, 1, 0))

  gn.rel &lt;- GaussNoiseRegress(Sepal.Width~., ir, rel = rel,
                              C.perc = list(5, 0.2, 5))
  plot(density(ir$Sepal.Width), ylim = c(0,1))
  lines(density(gnB.iris$Sepal.Width), col = 3)
  lines(density(gnE.iris$Sepal.Width, bw = 0.3), col = 4)
  # check the impact of a different relevance threshold
  lines(density(gn.rel$Sepal.Width), col = 2)
  }
</code></pre>

<hr>
<h2 id='ImbC'>Synthetic Imbalanced Data Set for a Multi-class Task
</h2><span id='topic+ImbC'></span>

<h3>Description</h3>

<p>Synthetic imbalanced data set for a multi-class task. The data set has a numeric feature (&quot;X1&quot;), a nominal feature (&quot;X2&quot;) and a target class named &quot;Class&quot;. The three classes of the problem (&quot;normal&quot;, &quot;rare1&quot; and &quot;rare2&quot;) are assigned according to the rules described below. These rules depend of the two features (&quot;X1&quot; and &quot;X2&quot;).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(ImbC)
</code></pre>


<h3>Format</h3>

<p>The data set has one continuous feature (<code>X1</code>) and one nominal feature (<code>X2</code>). The target class (denoted as <code>Class</code>) has three possible values (&quot;normal&quot; , &quot;rare1&quot; and &quot;rare2&quot;). Classes &quot;rare1&quot; and &quot;rare2&quot; are the minority classes. Examples of class &quot;rare1&quot; occur in 1% of the data while those of class &quot;rare2&quot; occur in 13.1% of the data. The remaining class, &quot;normal&quot;, is the majority class and occurs in about 85.9% of the data. Data set ImbC has 1000 examples distributed in classes &quot;rare1&quot;, &quot;rare2&quot; and &quot;normal&quot; with 10, 131 and 859 examples respectively.
</p>
<p>ImbC data has been simulated as follows:
</p>

<dl>
<dt>-</dt><dd><p><code>X1</code><code class="reqn">\sim \mathbf{N} \left(0, 4\right)</code></p>
</dd>
<dt>-</dt><dd><p><code>X2</code> labels &quot;cat&quot;, &quot;fish&quot; and &quot;dog&quot; where randomly distributed with the restriction of having a frequency of 30%, 30% and 40% respectively.</p>
</dd>
<dt>-</dt><dd><p>To obtain the target variable <code>Class</code>, we have define the following sets:
</p>

<ul>
<li> <p><code class="reqn">S_1=\{(X1, X2) : X1 &gt; 9 \wedge (X2 \in \{"cat", "dog"\})\}</code>
</p>
</li>
<li> <p><code class="reqn">S_2=\{(X1, X2) : X1 &gt; 7 \wedge X2 = "fish" \}</code>
</p>
</li>
<li> <p><code class="reqn">S_3=\{(X1, X2) :-1  &lt;  X1 &lt; 0.5\}</code>
</p>
</li>
<li> <p><code class="reqn">S_4=\{(X1, X2) : X1 &lt; -7 \wedge X2 = "fish"\}</code>
</p>
</li></ul>

</dd>
<dt>-</dt><dd><p> The following conditions define the target variable distribution of the ImbC synthetic data set:
</p>

<ul>
<li><p> Assign class label &quot;rare1&quot; to: a random sample of 90% of set <code class="reqn">S_1</code> and a random sample of 40% of set <code class="reqn">S_2</code>
</p>
</li>
<li><p> Assign class label &quot;rare2&quot; to: a random sample of 80% of set <code class="reqn">S_3</code> and a random sample of 70% of set <code class="reqn">S_4</code>
</p>
</li>
<li><p> Assign class label &quot;normal&quot; to the remaing examples.
</p>
</li></ul>

</dd>
</dl>



<h3>Author(s)</h3>

<p> Paula Branco <a href="mailto:paobranco@gmail.com">paobranco@gmail.com</a>, Rita Ribeiro
<a href="mailto:rpribeiro@dcc.fc.up.pt">rpribeiro@dcc.fc.up.pt</a> and Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(ggplot2)
data(ImbC)
summary(ImbC)
ggplot(data=ImbC, aes(x=X2, y=X1, color=Class))+geom_jitter()
</code></pre>

<hr>
<h2 id='ImbR'>Synthetic Regression Data Set
</h2><span id='topic+ImbR'></span>

<h3>Description</h3>

<p>Simulated data set for imbalanced domain on regression. The rare cases corresponden to the higher extreme values and are described by a circle with white noise. The normal cases have a normal distribution with the same center of the circunference with elliptical contours.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(ImbR)
</code></pre>


<h3>Format</h3>

<p>The data set has 2 continuous features (<code>X1</code> and <code>X2</code>) and a continuous target variable (denoted as <code>Tgt</code>). The rare examples, i.e, cases with higher values of the target variable  occur in 5% of the data. Data set ImbR has 1000 examples.
</p>
<p>ImbR data has been simulated as follows:
</p>

<dl>
<dt>-</dt><dd><p>lower <code>Tgt</code> values: <code>(X1, X2)</code><code class="reqn">\sim \mathbf{N}_{2} \left(\mathbf{10}_{2}, \mathbf{2.5}_{2}\right)</code></p>
</dd></dl>
<p>  and <code>Tgt</code><code class="reqn">\sim \mathbf{\Gamma} \left( 0.5, 1 \right) +10</code>
</p>
<dl>
<dt>-</dt><dd><p>higher <code>Tgt</code> values: <code>(X1, X2)</code><code class="reqn">\sim \left(\rho * cos(\theta) + 10, \rho * sin(\theta) + 10 \right)</code>, where <code class="reqn">\rho \sim \mathbf{9}_{2}+\mathbf{N}_{2} \left(\mathbf{0}_{2}, \mathbf{I}_{2} \right)</code> and <code class="reqn">\theta \sim \mathbf{U}_{2} \left( \mathbf{0}_{2}, 2\pi \mathbf{I}_{2} \right)</code> <code>Tgt</code><code class="reqn">\sim \mathbf{\Gamma} \left( 1,1 \right) + 20</code> </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p> Paula Branco <a href="mailto:paobranco@gmail.com">paobranco@gmail.com</a>, Rita Ribeiro
<a href="mailto:rpribeiro@dcc.fc.up.pt">rpribeiro@dcc.fc.up.pt</a> and Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ImbR)
summary(ImbR)

boxplot(ImbR$Tgt)
</code></pre>

<hr>
<h2 id='NCLClassif'>
Neighborhood Cleaning Rule (NCL) algorithm for multiclass imbalanced problems
</h2><span id='topic+NCLClassif'></span>

<h3>Description</h3>

<p>This function handles imbalanced classification problems using the Neighborhood Cleaning Rule (NCL) method. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NCLClassif(form, dat, k = 3, dist = "Euclidean", p = 2, Cl = "smaller")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="NCLClassif_+3A_form">form</code></td>
<td>

<p>A formula describing the prediction problem.
</p>
</td></tr>
<tr><td><code id="NCLClassif_+3A_dat">dat</code></td>
<td>

<p>A data frame containing the original imbalanced data set.
</p>
</td></tr>
<tr><td><code id="NCLClassif_+3A_k">k</code></td>
<td>

<p>A number indicating the number of nearest neighbors to use.
</p>
</td></tr>
<tr><td><code id="NCLClassif_+3A_dist">dist</code></td>
<td>

<p>A character string indicating which distance metric to use when determining the k nearest neighbors.  See the details. Defaults to &quot;Euclidean&quot;.
</p>
</td></tr>
<tr><td><code id="NCLClassif_+3A_p">p</code></td>
<td>

<p>A number indicating the value of p if the &quot;p-norm&quot; distance is chosen. Only necessary to define if a &quot;p-norm&quot; is chosen in the <code>dist</code> argument. See details.
</p>
</td></tr>
<tr><td><code id="NCLClassif_+3A_cl">Cl</code></td>
<td>

<p>A character vector indicating which classes should be under-sampled. Defaults to &quot;smaller&quot; meaning that all &quot;smaller&quot;&quot; classes are the most important and therefore only examples from the remaining classes should be removed. The user may define a subset of the existing classes in which this technique will be applied.
</p>
</td></tr>
</table>


<h3>Details</h3>


<dl>
<dt><code>dist</code> parameter:</dt><dd><p>The parameter <code>dist</code> allows the user to define the distance metric to be used in the neighbors computation. Although the default is the Euclidean distance, other metrics are available. This allows the computation of distances in data sets with, for instance, both nominal and numeric features. The options available for the distance functions are as follows: 
</p>
<p>- for data with only numeric features: &quot;Manhattan&quot;, &quot;Euclidean&quot;, &quot;Canberra&quot;, &quot;Chebyshev&quot;, &quot;p-norm&quot;;
</p>
<p>- for data with only nominal features: &quot;Overlap&quot;;
</p>
<p>- for dealing with both nominal and numeric features: &quot;HEOM&quot;, &quot;HVDM&quot;.
</p>
<p>When the &quot;p-norm&quot; is selected for the <code>dist</code> parameter, it is also necessary to define the value of parameter <code>p</code>. The value of parameter <code>p</code> sets which &quot;p-norm&quot; will be used. For instance, if <code>p</code> is set to 1, the &quot;1-norm&quot; (or Manhattan distance) is used, and if <code>p</code> is set to 2, the &quot;2-norm&quot; (or Euclidean distance) is applied.
For more details regarding the distance functions implemented in UBL package please see the package vignettes.
</p>
</dd>
<dt>NCL algorithm:</dt><dd><p>The NCL algorithm includes two phases. In the first phase the ENN algorithm is used to under-sample the examples whose class label is not in Cl. Then, a second step is performed which aims at further clean the neighborhood of the examples in Cl. To achieve this, the k nearest neighbors of examples in Cl are scanned. An example is removed if all the previous neighbors have a class label which is not in Cl, and if the example belongs to a class which is larger than half of the smaller class in Cl. In either steps the examples with class labels in Cl are always maintained.
</p>
</dd>
</dl>



<h3>Value</h3>

<p>The function returns a data frame with
the new data set resulting from the application of the NCL
algorithm. 
</p>


<h3>Author(s)</h3>

<p> Paula Branco <a href="mailto:paobranco@gmail.com">paobranco@gmail.com</a>, Rita Ribeiro
<a href="mailto:rpribeiro@dcc.fc.up.pt">rpribeiro@dcc.fc.up.pt</a> and Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>J. Laurikkala. (2001). <em>Improving identification of difficult small classes by balancing class distribution</em>. Artificial Intelligence in Medicine, pages 63-66.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ENNClassif">ENNClassif</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># generate a small imbalanced data set
ir &lt;- iris[-c(90:135), ]
# apply NCL method with different metrics, number of neighbors and classes
ir.M1 &lt;- NCLClassif(Species~., ir, k = 3, dist = "Manhattan", Cl = "smaller")
ir.Def &lt;- NCLClassif(Species~., ir)
ir.Ch &lt;- NCLClassif(Species~., ir, k = 7, dist = "Chebyshev", Cl = "virginica")
ir.Eu &lt;- NCLClassif(Species~., ir, k = 5, Cl = c("versicolor", "virginica"))
# check the results
summary(ir$Species)
summary(ir.M1$Species)
summary(ir.Def$Species)
summary(ir.Ch$Species)
summary(ir.Eu$Species)
</code></pre>

<hr>
<h2 id='neighbours'>Computation of nearest neighbours using a selected distance function. 
</h2><span id='topic+neighbours'></span>

<h3>Description</h3>

<p>This function allows to obtain the nearest neighbours of each example in a data set using a distance function selected by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>neighbours(tgt, dat, dist, p=2, k)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="neighbours_+3A_tgt">tgt</code></td>
<td>

<p>The column number of the problem target variable.
</p>
</td></tr>
<tr><td><code id="neighbours_+3A_dat">dat</code></td>
<td>
<p>A data frame containing the problem data.
</p>
</td></tr>
<tr><td><code id="neighbours_+3A_dist">dist</code></td>
<td>
<p>A character string specifying the distance function to use in the nearest neighbours evaluation.
</p>
</td></tr>
<tr><td><code id="neighbours_+3A_p">p</code></td>
<td>
<p>An optional parameter that is only required if the distance function selected in parameter <code>dist</code> is &quot;p-norm&quot;.
</p>
</td></tr>
<tr><td><code id="neighbours_+3A_k">k</code></td>
<td>
<p>The number of nearest neighbours to return for each example.  
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Several distance function are implemented in UBL package. The goal of having such a diversity of distance functions is to provide the users more flexibility regarding the distance used and also to provide distance fucntions that are able to deal with nominal and numeric features. The options available for the distance functions are as follows: 
</p>

<dl>
<dt>data with only numeric features:</dt><dd><p> &quot;Manhattan&quot;, &quot;Euclidean&quot;, &quot;Canberra&quot;, &quot;Chebyshev&quot;, &quot;p-norm&quot;;</p>
</dd>
<dt>data with only nominal features:</dt><dd><p> &quot;Overlap&quot;;</p>
</dd>
<dt>data with both nominal and numeric features:</dt><dd><p> &quot;HEOM&quot;, &quot;HVDM&quot;.</p>
</dd>
</dl>

<p>When the &quot;p-norm&quot; is selected for the <code>dist</code> parameter, it is also necessary to define the value of parameter <code>p</code>. The value of parameter <code>p</code> sets which &quot;p-norm&quot; will be used. For instance, if <code>p</code> is set to 1, the &quot;1-norm&quot; (or Manhattan distance) is used, and if <code>p</code> is set to 2, the &quot;2-norm&quot; (or Euclidean distance) is applied.
For more details regarding the distance functions implemented in UBL package please see the package vignettes.
</p>


<h3>Value</h3>

<p>The function returns a matrix with the indexes of the k nearest neighbours for each example in the data set. 
</p>


<h3>Author(s)</h3>

<p> Paula Branco <a href="mailto:paobranco@gmail.com">paobranco@gmail.com</a>, Rita Ribeiro
<a href="mailto:rpribeiro@dcc.fc.up.pt">rpribeiro@dcc.fc.up.pt</a> and Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> 
</p>


<h3>References</h3>

<p>Wilson, D.R. and Martinez, T.R. (1997). <em>Improved heterogeneous distance functions.</em> Journal of artificial intelligence research, pp.1-34.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+distances">distances</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data(ImbC)
# determine the 2 nearest neighbours of each example in ImbC data set
# using the "HVDM" distance function.
neig1 &lt;- neighbours(3, ImbC, "HVDM", k=2)

# now using the "HEOM" distance function
neig2 &lt;- neighbours(3, ImbC, "HEOM", k=2)

# check the differences
head(neig1)
head(neig2)

## End(Not run)
</code></pre>

<hr>
<h2 id='OSSClassif'>
One-sided selection strategy for handling multiclass imbalanced problems.
</h2><span id='topic+OSSClassif'></span>

<h3>Description</h3>

<p>This function performs an adapted one-sided selection strategy for multiclass imbalanced problems.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>OSSClassif(form, dat, dist = "Euclidean", p = 2, Cl = "smaller", start = "CNN")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="OSSClassif_+3A_form">form</code></td>
<td>

<p>A formula describing the prediction problem.
</p>
</td></tr>
<tr><td><code id="OSSClassif_+3A_dat">dat</code></td>
<td>

<p>A data frame containing the original imbalanced data set.
</p>
</td></tr>
<tr><td><code id="OSSClassif_+3A_dist">dist</code></td>
<td>

<p>A character string indicating which distance metric to use when determining the k nearest neighbors. See the details. Defaults to &quot;Euclidean&quot;.
</p>
</td></tr>
<tr><td><code id="OSSClassif_+3A_p">p</code></td>
<td>

<p>A number indicating the value of p if the &quot;p-norm&quot; distance is chosen. Only necessary to define if a &quot;p-norm&quot; is chosen in the <code>dist</code> argument. See details.
</p>
</td></tr>
<tr><td><code id="OSSClassif_+3A_cl">Cl</code></td>
<td>

<p>A character vector indicating which are the most important classes. Defaults to &quot;smaller&quot; which means that the smaller classes are automatically determined. In this case, all the smaller classes are those with a frequency below (nr.examples)/(nr.classes). With the selection of option &quot;smaller&quot; those classes are the ones considered important for the user.
</p>
</td></tr>
<tr><td><code id="OSSClassif_+3A_start">start</code></td>
<td>

<p>A string which determines which strategy (CNN or Tomek links) should be performed first. The existing options are &quot;CNN&quot; and &quot;Tomek&quot;. The first one, &quot;CNN&quot;, which is the default, means that CNN strategy will be performed first and Tomek links are applied after. On the other hand, if <code>start</code> is set to &quot;Tomek&quot; then the reverse order is applied (first Tomek links and after CNN strategy).
</p>
</td></tr>
</table>


<h3>Details</h3>


<dl>
<dt><code>dist</code> parameter:</dt><dd><p>The parameter <code>dist</code> allows the user to define the distance metric to be used in the neighbors computation. Although the default is the Euclidean distance, other metrics are available. This allows the computation of distances in data sets with, for instance, both nominal and numeric features. The options available for the distance functions are as follows: 
</p>
<p>- for data with only numeric features: &quot;Manhattan&quot;, &quot;Euclidean&quot;, &quot;Canberra&quot;, &quot;Chebyshev&quot;, &quot;p-norm&quot;;
</p>
<p>- for data with only nominal features: &quot;Overlap&quot;;
</p>
<p>- for dealing with both nominal and numeric features: &quot;HEOM&quot;, &quot;HVDM&quot;.
</p>
<p>When the &quot;p-norm&quot; is selected for the <code>dist</code> parameter, it is also necessary to define the value of parameter <code>p</code>. The value of parameter <code>p</code> sets which &quot;p-norm&quot; will be used. For instance, if <code>p</code> is set to 1, the &quot;1-norm&quot; (or Manhattan distance) is used, and if <code>p</code> is set to 2, the &quot;2-norm&quot; (or Euclidean distance) is applied.
For more details regarding the distance functions implemented in UBL package please see the package vignettes.
</p>
</dd>
</dl>



<h3>Value</h3>

<p>The function returns a data frame with
the new data set resulting from the application of the selected OSS strategy. 
</p>


<h3>Author(s)</h3>

<p> Paula Branco <a href="mailto:paobranco@gmail.com">paobranco@gmail.com</a>, Rita Ribeiro
<a href="mailto:rpribeiro@dcc.fc.up.pt">rpribeiro@dcc.fc.up.pt</a> and Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Kubat, M. &amp; Matwin, S. (1997). <em>Addressing the Curse of Imbalanced Training Sets: One-Sided Selection</em> Proc. of the 14th Int. Conf. on Machine Learning, Morgan Kaufmann, 179-186.
</p>
<p>Batista, G. E.; Prati, R. C. &amp; Monard, M. C. (2004). <em>A study of the behavior of several methods for balancing machine learning training data</em> ACM SIGKDD Explorations Newsletter, ACM, 6, 20-29
</p>


<h3>See Also</h3>

<p><code><a href="#topic+TomekClassif">TomekClassif</a>, <a href="#topic+CNNClassif">CNNClassif</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  ## Not run: 
    if (requireNamespace("DMwR2", quietly = TRUE)) {
  data(algae, package ="DMwR2")
  clean.algae &lt;- data.frame(algae[complete.cases(algae), ])
  alg1 &lt;- OSSClassif(season~., clean.algae, dist = "HVDM", 
                     Cl = c("spring", "summer"))
  alg2 &lt;- OSSClassif(season~., clean.algae, dist = "HEOM", 
                     Cl = c("spring", "summer"), start = "Tomek")
  alg3 &lt;- OSSClassif(season~., clean.algae, dist = "HVDM", start = "CNN")
  alg4 &lt;- OSSClassif(season~., clean.algae, dist = "HVDM", start = "Tomek")
  alg5 &lt;- OSSClassif(season~., clean.algae, dist = "HEOM", Cl = "winter")
  summary(alg1$season)
  summary(alg2$season)
  summary(alg3$season)
  summary(alg4$season)
  summary(alg5$season)
  }
  
## End(Not run)
</code></pre>

<hr>
<h2 id='phi'>Relevance function. 
</h2><span id='topic+phi'></span>

<h3>Description</h3>

<p>This function allows to obtain the relevance function values on a set of target variable values given the interpolating points.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>phi(y, control.parms)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="phi_+3A_y">y</code></td>
<td>

<p>The target variable values of the problem.
</p>
</td></tr>
<tr><td><code id="phi_+3A_control.parms">control.parms</code></td>
<td>
<p>A named list supplied by the phi.control function with the parameters needed for obtaining the relevance values.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The phi function specifies the regions of interest in the target
variable. It does so by performing a Monotone Cubic Spline
Interpolation over a set of maximum and minimum relevance points. 
The notion of relevance can be associated with rarity.
Nonetheless, this notion may depend on the domain experts knowledge.
</p>


<h3>Value</h3>

<p>The function returns the relevance values. 
</p>


<h3>Author(s)</h3>

<p> Rita Ribeiro
<a href="mailto:rpribeiro@dcc.fc.up.pt">rpribeiro@dcc.fc.up.pt</a>, Paula Branco <a href="mailto:paobranco@gmail.com">paobranco@gmail.com</a>, and Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Ribeiro, R., 2011. Utility-based regression 
(Doctoral dissertation, PhD thesis, 
Dep. Computer Science, Faculty of Sciences - 
University of Porto).
</p>
<p>Fritsch, F.N. and Carlson, R.E., 1980. Monotone piecewise cubic interpolation.
SIAM Journal on Numerical Analysis, 17(2), pp.238-246.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+phi.control">phi.control</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# example of a relevance function where the extremes are the important values.
data(morley)
# the target variable
y &lt;- morley$Speed

phiF.args &lt;- phi.control(y,method="extremes",extr.type="both")
y.phi &lt;- phi(y, control.parms=phiF.args)
plot(y, y.phi)

</code></pre>

<hr>
<h2 id='phi.control'>Estimation of parameters used for obtaining the relevance function.
</h2><span id='topic+phi.control'></span>

<h3>Description</h3>

<p>This function allows to obtain the parameters of the relevance function (<a href="#topic+phi">phi</a>). The parameters can be obtained using one of the following methods: &quot;extremes&quot; or &quot;range&quot;. If the selected method is &quot;extremes&quot;, the distribution of the target variable values is used to assign more importance to the most extreme values according to the boxplot. If the selected method is &quot;range&quot;, a matrix should be provided defining the important and unimportant values (see section details).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>phi.control(y, method="extremes", extr.type="both", coef=1.5, control.pts=NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="phi.control_+3A_y">y</code></td>
<td>
<p>The target variable values.
</p>
</td></tr>
<tr><td><code id="phi.control_+3A_method">method</code></td>
<td>
<p>&quot;extremes&quot; (default) or &quot;range&quot;.
</p>
</td></tr>
<tr><td><code id="phi.control_+3A_extr.type">extr.type</code></td>
<td>
<p>parameter needed for method &quot;extremes&quot; to specify which type of extremes are to be considered relevant: &quot;high&quot;, &quot;low&quot; or &quot;both&quot;(default). 
</p>
</td></tr>
<tr><td><code id="phi.control_+3A_coef">coef</code></td>
<td>
<p>parameter needed for method &quot;extremes&quot; to specify how far the wiskers extend to the most extreme data point in the boxplot. The default is 1.5.
</p>
</td></tr>
<tr><td><code id="phi.control_+3A_control.pts">control.pts</code></td>
<td>
<p>parameter needed for method &quot;range&quot; to specify the interpolating points to the relevance function (phi). It should be a matrix with three columns. The first column represents the y value, the second column represents the corresponding relevance value (phi(y)) in [0,1], and the third optional column represents the corresponding relevance value derivative (phi'(y)).  
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The method &quot;extremes&quot; uses the target variable distribution to automatically determine the most relevant values. This method uses the boxplot to automatically derive a matrix with the interpolating points for the relevance function (phi). According to the <code>extr.type</code> parameter it assigns maximum relevance to: only the &quot;high&quot; extremes, only the &quot;low&quot; extremes or &quot;both&quot;. In the latter case, it assigns maximum relevance to &quot;both&quot; types of extremes if they exist. If &quot;both&quot; is selected and only one type of extremes is present, then only the existing extremes are considered.
</p>
<p>The method &quot;range&quot; uses the <code>control.pts</code> matrix provided by the user to define the interpolating points for the relevance function (phi). The values supplied in the third column (phi derivative) of the matrix are only indicative, meaning that they will be adjusted afterwards by the relevance function (phi) to create a smooth continuous function.
</p>


<h3>Value</h3>

<p>The function returns a list with the parameters needed for obtaining and evaluating the relevance function (phi).
</p>


<h3>Author(s)</h3>

<p> Rita Ribeiro
<a href="mailto:rpribeiro@dcc.fc.up.pt">rpribeiro@dcc.fc.up.pt</a>, Paula Branco <a href="mailto:paobranco@gmail.com">paobranco@gmail.com</a> and Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Ribeiro, R., 2011. Utility-based regression 
(Doctoral dissertation, PhD thesis, 
Dep. Computer Science, Faculty of Sciences - 
University of Porto).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+phi">phi</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(morley)
# the target variable
y &lt;- morley$Speed

# the target variable has "low" and "high"extremes 
boxplot(y)

## using method "extremes" considering that 
## "both" extremes are important
phiF.argsB &lt;- phi.control(y,method="extremes",extr.type="both")
y.phiB &lt;- phi(y, control.parms=phiF.argsB)
plot(y, y.phiB)

## using method "extremes" considering that only the
## "high" extremes are relevant
phiF.argsH &lt;- phi.control(y,method="extremes",extr.type="high")
y.phiH &lt;- phi(y, control.parms=phiF.argsH)
plot(y, y.phiH)


## using method "range" to choose the important values:
rel &lt;- matrix(0,ncol=3,nrow=0)
rel &lt;- rbind(rel,c(700,0,0)) 
rel &lt;- rbind(rel,c(800,1,0))
rel &lt;- rbind(rel,c(900,0,0))
rel &lt;- rbind(rel,c(1000,1,0))
rel
phiF.argsR &lt;- phi.control(y,method="range",control.pts=rel)
y.phiR &lt;- phi(y, control.parms=phiF.argsR)

plot(y, y.phiR)
</code></pre>

<hr>
<h2 id='predict+2CBagModel-method'>Predicting on new data with a <strong>BagModel</strong> model</h2><span id='topic+predict+2CBagModel-method'></span>

<h3>Description</h3>

<p>This is a <code>predict</code> method for predicting new test points using a
<code>BagModel</code> class object - refering to an ensemble
of weak models whose type is selected by the user. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'BagModel'
predict(object, newdata)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict+2B2CBagModel-method_+3A_object">object</code></td>
<td>
<p>A <strong>BagModel-class</strong> object.</p>
</td></tr>
<tr><td><code id="predict+2B2CBagModel-method_+3A_newdata">newdata</code></td>
<td>
<p>New test data to predict using a <code>BagModel</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>predictions produced by a <code>BagModel</code> model.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BagModel-class">BagModel-class</a></code> for details about the bagging model.
</p>

<hr>
<h2 id='RandOverClassif'>
Random over-sampling for imbalanced classification problems
</h2><span id='topic+RandOverClassif'></span>

<h3>Description</h3>

<p>This function performs a random over-sampling strategy for imbalanced multiclass problems. Essentially, a percentage of cases of the class(es) 
selected by the user are randomly over-sampled by the introduction of replicas of examples. Alternatively, the strategy can be applied to either balance all the existing classes or to &quot;smoothly invert&quot; the frequency of the examples in each class.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RandOverClassif(form, dat, C.perc = "balance", repl = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="RandOverClassif_+3A_form">form</code></td>
<td>

<p>A formula describing the prediction problem.
</p>
</td></tr>
<tr><td><code id="RandOverClassif_+3A_dat">dat</code></td>
<td>

<p>A data frame containing the original imbalanced data set.
</p>
</td></tr>
<tr><td><code id="RandOverClassif_+3A_c.perc">C.perc</code></td>
<td>

<p>A named list containing each class name and the corresponding over-sampling percentage, greater than or equal to 1, where 1 means that no over-sampling is to be applied in the corresponding class. The user may indicate only the classes where he wants to apply random over-sampling. For instance, a percenatge of 2 means that, in the changed data set, the number of examples of that class are doubled. Alternatively, this parameter can be set to &quot;balance&quot; (the default) or &quot;extreme&quot;, cases where the over-sampling percentages are automatically estimated. The &quot;balance&quot; option tries to balance all the existing classes while the &quot;extreme&quot; option inverts the classes original frequencies.
</p>
</td></tr>
<tr><td><code id="RandOverClassif_+3A_repl">repl</code></td>
<td>

<p>A boolean value controlling the possibility of having repetition of examples when choosing the examples to repeat in the over-sampled data set. Defaults to TRUE because this is a necessary condition if the selected percentage is greater than 2. This parameter is only important when the over-sampling percentage is between 1 and 2. In this case, it controls if all the new examples selected from a given class can be repeated or not.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function performs a random over-sampling strategy for dealing with imbalanced multiclass problems. The new examples included in the new data set are replicas of the examples already present in the original data set. 
</p>


<h3>Value</h3>

<p>The function returns a data frame with
the new data set resulting from the application of the random over-sampling strategy.
</p>


<h3>Author(s)</h3>

<p> Paula Branco <a href="mailto:paobranco@gmail.com">paobranco@gmail.com</a>, Rita Ribeiro
<a href="mailto:rpribeiro@dcc.fc.up.pt">rpribeiro@dcc.fc.up.pt</a> and Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>See Also</h3>

<p><code><a href="#topic+RandUnderClassif">RandUnderClassif</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  if (requireNamespace("DMwR2", quietly = TRUE)) {
  data(algae, package ="DMwR2")
  clean.algae &lt;- data.frame(algae[complete.cases(algae), ])
  # classes spring and winter remain unchanged
  C.perc = list(autumn = 2, summer = 1.5, spring = 1) 
  myover.algae &lt;- RandOverClassif(season~., clean.algae, C.perc)
  oveBalan.algae &lt;- RandOverClassif(season~., clean.algae, "balance")
  oveInvert.algae &lt;- RandOverClassif(season~., clean.algae, "extreme")
  } else {
  library(MASS)
  data(cats)
  myover.cats &lt;- RandOverClassif(Sex~., cats, list(M = 1.5))
  oveBalan.cats &lt;- RandOverClassif(Sex~., cats, "balance")
  oveInvert.cats &lt;- RandOverClassif(Sex~., cats, "extreme")
  
  # learn a model and check results with original and over-sampled data
  library(rpart)
  idx &lt;- sample(1:nrow(cats), as.integer(0.7 * nrow(cats)))
  tr &lt;- cats[idx, ]
  ts &lt;- cats[-idx, ]
  
  ctO &lt;- rpart(Sex ~ ., tr)
  predsO &lt;- predict(ctO, ts, type = "class")
  new.cats &lt;- RandOverClassif(Sex~., tr, "balance")
  ct1 &lt;- rpart(Sex ~ ., new.cats)
  preds1 &lt;- predict(ct1, ts, type = "class")
  table(predsO, ts$Sex)   
  table(preds1, ts$Sex)
  }
</code></pre>

<hr>
<h2 id='RandOverRegress'>
Random over-sampling for imbalanced regression problems
</h2><span id='topic+RandOverRegress'></span>

<h3>Description</h3>

<p>This function performs a random over-sampling strategy for imbalanced regression problems. 
Basically a percentage of cases of the &quot;class(es)&quot;
(bumps above a relevance threshold defined) selected by the user are randomly over-sampled. 
Alternatively, it can either balance all the existing &quot;classes&quot; 
(the default) or it can &quot;smoothly invert&quot; the frequency
of the examples in each class.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RandOverRegress(form, dat, rel = "auto", thr.rel = 0.5, 
                C.perc = "balance", repl = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="RandOverRegress_+3A_form">form</code></td>
<td>

<p>A formula describing the prediction problem.
</p>
</td></tr>
<tr><td><code id="RandOverRegress_+3A_dat">dat</code></td>
<td>

<p>A data frame containing the original imbalanced data set.
</p>
</td></tr>
<tr><td><code id="RandOverRegress_+3A_rel">rel</code></td>
<td>

<p>The relevance function which can be automatically (&quot;auto&quot;) determined (the default) or may be provided by the user through a matrix with the interpolating points.
</p>
</td></tr>
<tr><td><code id="RandOverRegress_+3A_thr.rel">thr.rel</code></td>
<td>

<p>A number indicating the relevance threshold above which a case is considered as belonging to the rare &quot;class&quot;.
</p>
</td></tr>
<tr><td><code id="RandOverRegress_+3A_c.perc">C.perc</code></td>
<td>

<p>A list containing the over-sampling percentage/s to apply to all/each
&quot;class&quot; (bump) obtained with the relevance threshold. Replicas of the examples are are randomly added in each &quot;class&quot;.
If only one percentage is provided this value is reused in all the &quot;classes&quot; that have values above the relevance threshold. A different percentage can be provided to each &quot;class&quot;. In this case, the percentages should be provided in ascending order of target variable value. The over-sampling percentage(s), should be numbers above 1, meaning that the important cases (cases above the threshold) are over-sampled by the corresponding percentage. If the number 1 is provided then those examples are not changed. 
Alternatively, <code>C.perc</code> parameter may be set to &quot;balance&quot; or &quot;extreme&quot;,
cases where the over-sampling percentages are automatically estimated to either balance or invert the frequencies of the examples in the &quot;classes&quot; (bumps).        
</p>
</td></tr>
<tr><td><code id="RandOverRegress_+3A_repl">repl</code></td>
<td>

<p>A boolean value controlling the possibility of having repetition of examples when choosing the examples to repeat in the over-sampled data set. 
Defaults to TRUE because this is a necessary condition if the selected percentage is greater than 2. This parameter is only important when the over-sampling percentage is between 1 and 2. In this case, it controls if all the new examples selected from a given &quot;class&quot; can be repeated or not.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function performs a random over-sampling strategy for dealing with imbalanced regression problems. The new examples included in the new data set are randomly selected replicas of the examples already present in the original data set. 
</p>


<h3>Value</h3>

<p>The function returns a data frame with
the new data set resulting from the application of the random over-sampling strategy.
</p>


<h3>Author(s)</h3>

<p> Paula Branco <a href="mailto:paobranco@gmail.com">paobranco@gmail.com</a>, Rita Ribeiro
<a href="mailto:rpribeiro@dcc.fc.up.pt">rpribeiro@dcc.fc.up.pt</a> and Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>See Also</h3>

<p><code><a href="#topic+RandUnderRegress">RandUnderRegress</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(morley)

C.perc = list(2, 4)
myover &lt;- RandOverRegress(Speed~., morley, C.perc=C.perc)
Bal &lt;- RandOverRegress(Speed~., morley, C.perc= "balance")
Ext &lt;- RandOverRegress(Speed~., morley, C.perc= "extreme")

  if (requireNamespace("DMwR2", quietly = TRUE)) {
data(algae, package ="DMwR2")
clean.algae &lt;- data.frame(algae[complete.cases(algae), ])
# all automatic
ROB &lt;-RandOverRegress(a7~., clean.algae)
# user defined percentage for the only existing extreme (high)
myRO &lt;-RandOverRegress(a7~., clean.algae, rel = "auto", thr.rel = 0.7,
                        C.perc = list(5))

# check the results
plot(clean.algae[,c(1,ncol(clean.algae))])
plot(ROB[,c(1,ncol(clean.algae))])
plot(myRO[,c(1,ncol(clean.algae))])
}
</code></pre>

<hr>
<h2 id='RandUnderClassif'>
Random under-sampling for imbalanced classification problems
</h2><span id='topic+RandUnderClassif'></span>

<h3>Description</h3>

<p>This function performs a random under-sampling strategy for imbalanced multiclass problems. Essentially, a percentage of cases of the class(es) selected by the user are randomly removed. Alternatively, the strategy can be applied to either balance all the existing classes or to &quot;smoothly invert&quot; the frequency of the examples in each class.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RandUnderClassif(form, dat, C.perc = "balance", repl = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="RandUnderClassif_+3A_form">form</code></td>
<td>

<p>A formula describing the prediction problem.
</p>
</td></tr>
<tr><td><code id="RandUnderClassif_+3A_dat">dat</code></td>
<td>

<p>A data frame containing the original imbalanced data set.
</p>
</td></tr>
<tr><td><code id="RandUnderClassif_+3A_c.perc">C.perc</code></td>
<td>

<p>A named list containing each class name and the corresponding under-sampling percentage, between 0 and 1, where 1 means that no under-sampling is to be applied in the corresponding class. The user may indicate only the classes where he wants to apply random under-sampling. For instance, a percentage of 0.2 means that, in the changed data set, the class is reduced to 20% of its original size. Alternatively, this parameter can be set to &quot;balance&quot; (the defualt) or &quot;extreme&quot;, cases where the under-sampling percentages are automatically estimated. The &quot;balance&quot; option tries to balance all the existing classes while the &quot;extreme&quot; option inverts the classes original frequencies.
</p>
</td></tr>
<tr><td><code id="RandUnderClassif_+3A_repl">repl</code></td>
<td>

<p>A boolean value controlling the possibility of having repetition of examples in the under-sampled data set. Defaults to FALSE.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function performs a random under-sampling strategy for dealing with imbalanced multiclass problems. The examples removed are randomly selected among the examples belonging to each class containing the normal cases. The user can chose one or more classes to be under-sampled. 
</p>


<h3>Value</h3>

<p>The function returns a data frame with
the new data set resulting from the application of the random under-sampling strategy.
</p>


<h3>Author(s)</h3>

<p> Paula Branco <a href="mailto:paobranco@gmail.com">paobranco@gmail.com</a>, Rita Ribeiro
<a href="mailto:rpribeiro@dcc.fc.up.pt">rpribeiro@dcc.fc.up.pt</a> and Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>See Also</h3>

<p><code><a href="#topic+RandOverClassif">RandOverClassif</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  if (requireNamespace("DMwR2", quietly = TRUE)) {

  data(algae, package ="DMwR2")
  clean.algae &lt;- data.frame(algae[complete.cases(algae), ])
  C.perc = list(autumn = 1, summer = 0.9, winter = 0.4) 
  # classes autumn and spring remain unchanged
  
  myunder.algae &lt;- RandUnderClassif(season~., clean.algae, C.perc)
  undBalan.algae &lt;- RandUnderClassif(season~., clean.algae, "balance")
  undInvert.algae &lt;- RandUnderClassif(season~., clean.algae, "extreme")
} else {
  library(MASS)
  data(cats)
  myunder.cats &lt;- RandUnderClassif(Sex~., cats, list(M = 0.8))
  undBalan.cats &lt;- RandUnderClassif(Sex~., cats, "balance")
  undInvert.cats &lt;- RandUnderClassif(Sex~., cats, "extreme")


  # learn a model and check results with original and under-sampled data
  library(rpart)
  idx &lt;- sample(1:nrow(cats), as.integer(0.7*nrow(cats)))
  tr &lt;- cats[idx, ]
  ts &lt;- cats[-idx, ]
  
  idx &lt;- sample(1:nrow(cats), as.integer(0.7*nrow(cats)))
  tr &lt;- cats[idx, ]
  ts &lt;- cats[-idx, ]
  ctO &lt;- rpart(Sex ~ ., tr)
  predsO &lt;- predict(ctO, ts, type = "class")
  new.cats &lt;- RandUnderClassif(Sex~., tr, "balance")
  ct1 &lt;- rpart(Sex ~ ., new.cats)
  preds1 &lt;- predict(ct1, ts, type = "class")
   
  table(predsO, ts$Sex)  
#  predsO  F  M
#      F  9  3
#      M  7 25


  table(preds1, ts$Sex)   
# preds1  F  M
#      F 13  4
#      M  3 24
}
</code></pre>

<hr>
<h2 id='RandUnderRegress'>
Random under-sampling for imbalanced regression problems
</h2><span id='topic+RandUnderRegress'></span>

<h3>Description</h3>

<p>This function performs a random under-sampling strategy for imbalanced regression problems. Essentially, a percentage of cases of the &quot;class(es)&quot; (bumps below a relevance threshold defined) 
selected by the user are randomly removed. Alternatively, the strategy can be applied to either balance all the 
existing &quot;classes&quot;&quot; or to &quot;smoothly invert&quot; the frequency
of the examples in each &quot;class&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RandUnderRegress(form, dat,  rel = "auto", thr.rel = 0.5, 
                 C.perc = "balance", repl = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="RandUnderRegress_+3A_form">form</code></td>
<td>

<p>A formula describing the prediction problem.
</p>
</td></tr>
<tr><td><code id="RandUnderRegress_+3A_dat">dat</code></td>
<td>

<p>A data frame containing the original imbalanced data set.
</p>
</td></tr>
<tr><td><code id="RandUnderRegress_+3A_rel">rel</code></td>
<td>

<p>The relevance function which can be automatically (&quot;auto&quot;) determined (the default) or may be provided by the user through a matrix with interpolating points. 
</p>
</td></tr>
<tr><td><code id="RandUnderRegress_+3A_thr.rel">thr.rel</code></td>
<td>

<p>A number indicating the relevance threshold below which a case is considered as belonging to the normal &quot;class&quot;.
</p>
</td></tr>
<tr><td><code id="RandUnderRegress_+3A_c.perc">C.perc</code></td>
<td>

<p>A list containing the under-sampling percentage/s to apply to all/each
&quot;class&quot; (bump) obtained with the relevance threshold. Examples are randomly removed from the &quot;class(es)&quot;. If only one percentage is provided this value is reused in all the &quot;classes&quot; that have values below the relevance threshold. A different percentage can be provided to each &quot;class&quot;. In this case, the percentages should be provided in ascending order of target variable value. The under-sampling percentage(s), should be a number below 1, meaning that the normal cases (cases below the threshold) are under-sampled by the corresponding percentage. If the number 1 is provided then those examples are not changed. 
Alternatively, <code>C.perc</code> parameter may be set to &quot;balance&quot; or &quot;extreme&quot;,
cases where the under-sampling percentages are automatically estimated to either balance or invert the frequencies of the examples in the &quot;classes&quot; (bumps).
</p>
</td></tr>
<tr><td><code id="RandUnderRegress_+3A_repl">repl</code></td>
<td>

<p>A boolean value controlling the possibility of having repetition of examples in the under-sampled data set. Defaults to FALSE.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function performs a random under-sampling strategy for dealing with imbalanced regression problems. The examples removed are randomly selected among the examples belonging to the normal &quot;class(es)&quot; (bump of relevance below the threshold defined). The user can chose one or more bumps to be under-sampled.
</p>


<h3>Value</h3>

<p>The function returns a data frame with
the new data set resulting from the application of the random under-sampling strategy.
</p>


<h3>Author(s)</h3>

<p> Paula Branco <a href="mailto:paobranco@gmail.com">paobranco@gmail.com</a>, Rita Ribeiro
<a href="mailto:rpribeiro@dcc.fc.up.pt">rpribeiro@dcc.fc.up.pt</a> and Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>See Also</h3>

<p><code><a href="#topic+RandOverRegress">RandOverRegress</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(morley)

C.perc = list(0.5)
myUnd &lt;- RandUnderRegress(Speed~., morley, C.perc=C.perc)
Bal &lt;- RandUnderRegress(Speed~., morley, C.perc= "balance")
Ext &lt;- RandUnderRegress(Speed~., morley, C.perc= "extreme")

</code></pre>

<hr>
<h2 id='ReBaggRegress'>
REBaggRegress: RE(sampled) BAG(ging), an ensemble method for dealing with imbalanced regression problems.
</h2><span id='topic+ReBaggRegress'></span>

<h3>Description</h3>

<p>This function handles imbalanced regression problems by learneing a special purpose bagging ensemble.
A number of weak learners selected by the user are trained on resamples of the training data provided.
The resamples are built taking into consideration the imbalance of the problem. 
Currently, there are 4 different methods for building the resamples used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ReBaggRegress(form, train, rel="auto", thr.rel, learner, learner.pars,
       nmodels, samp.method = "variationSMT", aggregation="Average", quiet=TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ReBaggRegress_+3A_form">form</code></td>
<td>

<p>A formula describing the prediction problem.
</p>
</td></tr>
<tr><td><code id="ReBaggRegress_+3A_train">train</code></td>
<td>

<p>A data frame containing the training (imbalanced) data set.
</p>
</td></tr>
<tr><td><code id="ReBaggRegress_+3A_rel">rel</code></td>
<td>

<p>The relevance function which can be automatically (&quot;auto&quot;) determined (the default) or may be provided by the user through a matrix.
</p>
</td></tr>
<tr><td><code id="ReBaggRegress_+3A_thr.rel">thr.rel</code></td>
<td>

<p>A number indicating the relevance threshold above which a case is considered as belonging to the rare &quot;class&quot;.
</p>
</td></tr>
<tr><td><code id="ReBaggRegress_+3A_learner">learner</code></td>
<td>

<p>The learning algorithm to be used as weak learner.
</p>
</td></tr>
<tr><td><code id="ReBaggRegress_+3A_learner.pars">learner.pars</code></td>
<td>

<p>A named list with the learner parameters.
</p>
</td></tr>
<tr><td><code id="ReBaggRegress_+3A_nmodels">nmodels</code></td>
<td>

<p>A numeric indicating the number of models to train. 
</p>
</td></tr>
<tr><td><code id="ReBaggRegress_+3A_samp.method">samp.method</code></td>
<td>

<p>A character specifying the method used for building the resamples
of the training set provided. 
Possible characters are: &quot;balance&quot;, &quot;variation&quot;, &quot;balanceSMT&quot;, &quot;variationSMT&quot;.
The &quot;balance&quot; methods builds a number (nmodels) of samples that use all 
the rare cases and the same nr of normal cases. The &quot;variation&quot; method
build a number of baggs with all the rare cases and varying percentages of normal cases.
The SMT sufix is used when the SmoteR strategy is used to generate the new examples.
Defaults to &quot;variationSMT&quot;.
</p>
</td></tr>
<tr><td><code id="ReBaggRegress_+3A_aggregation">aggregation</code></td>
<td>
<p>charater specifying the method used for aggregating the results
obtained by the individual learners. 
For now, the only method available is by averaging the models predictions.
</p>
</td></tr>
<tr><td><code id="ReBaggRegress_+3A_quiet">quiet</code></td>
<td>
<p>logical specifying if development should be shown or not. Defaults to TRUE
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns an object of class BagModel. 
</p>


<h3>Author(s)</h3>

<p> Paula Branco <a href="mailto:paobranco@gmail.com">paobranco@gmail.com</a>, Rita Ribeiro
<a href="mailto:rpribeiro@dcc.fc.up.pt">rpribeiro@dcc.fc.up.pt</a> and Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Branco, P. and Torgo, L. and Ribeiro, R.P. (2018) <em>REBAGG: REsampled BAGGing for Imbalanced Regression</em> LIDTA2018: 2nd International Workshop on Learning with Imbalanced Domains: Theory and Applications (Co-located with ECML/PKDD 2018) Dublin, Ireland
</p>

<hr>
<h2 id='SMOGNClassif'>
SMOGN algorithm for imbalanced classification problems
</h2><span id='topic+SMOGNClassif'></span>

<h3>Description</h3>

<p>This function handles imbalanced classification problems using the SMOGN
method. Namely, it can generate a new data set containing synthetic examples
that addresses the problem of imbalanced domains. The new examples are obtained
either using SMOTE method or the introduction of Gaussian Noise
depending on the distance between the two original cases used.
If they are too further apart Gaussian Noise is used, if they are close
then it is safe to use SMOTE method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SMOGNClassif(form, dat, C.perc = "balance",
                         k = 5, repl = FALSE, dist = "Euclidean",
                         p = 2, pert=0.01)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="SMOGNClassif_+3A_form">form</code></td>
<td>

<p>A formula describing the prediction problem
</p>
</td></tr>
<tr><td><code id="SMOGNClassif_+3A_dat">dat</code></td>
<td>

<p>A data frame containing the original (unbalanced) data set
</p>
</td></tr>
<tr><td><code id="SMOGNClassif_+3A_c.perc">C.perc</code></td>
<td>

<p>A named list containing the percentage(s) of under- or/and 
over-sampling to apply to each class.
The over-sampling percentage is a number above 1 while the under-sampling percentage should be a number below 1. If the number 1 is provided for a given class then that class remains unchanged. Alternatively it may be &quot;balance&quot; (the default) or &quot;extreme&quot;, cases where the sampling percentages are automatically estimated either to balance the examples between the minority and majority classes or to invert the distribution of examples across the existing classes transforming the majority classes into minority and vice-versa.
</p>
</td></tr>
<tr><td><code id="SMOGNClassif_+3A_k">k</code></td>
<td>

<p>A number indicating the number of nearest neighbors that are used to
generate the new examples of the minority class(es).
</p>
</td></tr>
<tr><td><code id="SMOGNClassif_+3A_repl">repl</code></td>
<td>

<p>A boolean value controlling the possibility of having repetition of examples when performing under-sampling by selecting among the majority class(es) examples.
</p>
</td></tr>
<tr><td><code id="SMOGNClassif_+3A_dist">dist</code></td>
<td>

<p>A character string indicating which distance metric to use when determining the k nearest neighbors. See the details. Defaults to &quot;Euclidean&quot;.
</p>
</td></tr>
<tr><td><code id="SMOGNClassif_+3A_p">p</code></td>
<td>

<p>A number indicating the value of p if the &quot;p-norm&quot; distance is chosen. Only necessary to define if a &quot;p-norm&quot; is chosen in the <code>dist</code> argument. See details.
</p>
</td></tr>
<tr><td><code id="SMOGNClassif_+3A_pert">pert</code></td>
<td>

<p>A number indicating the level of perturbation to introduce when generating synthetic examples. Assuming as center the base example, this parameter defines the radius (based on the standard deviation) where the new example is generated.
</p>
</td></tr>
</table>


<h3>Details</h3>


<dl>
<dt><code>dist</code> parameter:</dt><dd><p>The parameter <code>dist</code> allows the user to define the distance metric to be used in the neighbors computation. Although the default is the Euclidean distance, other metrics are available. This allows the computation of distances in data sets with, for instance, both nominal and numeric features. The options available for the distance functions are as follows: 
</p>
<p>- for data with only numeric features: &quot;Manhattan&quot;, &quot;Euclidean&quot;, &quot;Canberra&quot;, &quot;Chebyshev&quot;, &quot;p-norm&quot;;
</p>
<p>- for data with only nominal features: &quot;Overlap&quot;;
</p>
<p>- for dealing with both nominal and numeric features: &quot;HEOM&quot;.
</p>
<p>When the &quot;p-norm&quot; is selected for the <code>dist</code> parameter, it is also necessary to define the value of parameter <code>p</code>. The value of parameter <code>p</code> sets which &quot;p-norm&quot; will be used. For instance, if <code>p</code> is set to 1, the &quot;1-norm&quot; (or Manhattan distance) is used, and if <code>p</code> is set to 2, the &quot;2-norm&quot; (or Euclidean distance) is applied.
For more details regarding the distance functions implemented in UBL package please see the package vignettes.
</p>
</dd>
</dl>



<h3>Value</h3>

<p>The function returns a data frame with
the new data set resulting from the application of the SMOGN
algorithm. 
</p>


<h3>Author(s)</h3>

<p> Paula Branco <a href="mailto:paobranco@gmail.com">paobranco@gmail.com</a>, Rita Ribeiro
<a href="mailto:rpribeiro@dcc.fc.up.pt">rpribeiro@dcc.fc.up.pt</a> and Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Paula Branco, Luis Torgo, and Rita Paula Ribeiro. <em>SMOGN: a pre-processing approach for imbalanced regression.</em> First International Workshop on Learning with Imbalanced Domains: Theory and Applications, 36-50(2017).
Torgo, Luis and Ribeiro, Rita P and Pfahringer, Bernhard and Branco, Paula (2013). <em>SMOTE for Regression</em>. Progress in Artificial Intelligence, Springer,378-389.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SmoteClassif">SmoteClassif</a>, <a href="#topic+GaussNoiseClassif">GaussNoiseClassif</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  ir &lt;- iris[-c(95:130), ]
  smogn1 &lt;- SMOGNClassif(Species~., ir)
  smogn2 &lt;- SMOGNClassif(Species~., ir,  C.perc=list(setosa=0.5,versicolor=2.5))

  # checking visually the results 
  plot(sort(ir$Sepal.Width))
  plot(sort(smogn1$Sepal.Width))
  
  # using a relevance function provided by the user
  rel &lt;- matrix(0, ncol = 3, nrow = 0)
  rel &lt;- rbind(rel, c(2, 1, 0))
  rel &lt;- rbind(rel, c(3, 0, 0))
  rel &lt;- rbind(rel, c(4, 1, 0))

  smognRel &lt;- SmoteRegress(Sepal.Width~., ir, rel = rel, dist = "HEOM",
                        C.perc = list(4, 0.5, 4))

  plot(sort(smognRel$Sepal.Width))
</code></pre>

<hr>
<h2 id='SMOGNRegress'>
SMOGN algorithm for imbalanced regression problems
</h2><span id='topic+SMOGNRegress'></span>

<h3>Description</h3>

<p>This function handles imbalanced regression problems using the SMOGN
method. Namely, it can generate a new data set containing synthetic examples
that addresses the problem of imbalanced domains. The new examples are obtained
either using SmoteR method or the introduction of Gaussian Noise
depending on the distance between the two original cases used.
If they are too further apart Gaussian Noise is used, if they are close
then it is safe to use SmoteR method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SMOGNRegress(form, dat, rel = "auto", thr.rel = 0.5,
                        C.perc = "balance", k = 5, repl = FALSE,
                        dist = "Euclidean", p = 2, pert=0.01)

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="SMOGNRegress_+3A_form">form</code></td>
<td>

<p>A formula describing the prediction problem
</p>
</td></tr>
<tr><td><code id="SMOGNRegress_+3A_dat">dat</code></td>
<td>

<p>A data frame containing the original (unbalanced) data set
</p>
</td></tr>
<tr><td><code id="SMOGNRegress_+3A_rel">rel</code></td>
<td>

<p>The relevance function which can be automatically (&quot;auto&quot;) determined (the default) or may be provided by the user through a matrix.
</p>
</td></tr>
<tr><td><code id="SMOGNRegress_+3A_thr.rel">thr.rel</code></td>
<td>

<p>A number indicating the relevance threshold above which a case is considered as belonging to the rare &quot;class&quot;.
</p>
</td></tr>
<tr><td><code id="SMOGNRegress_+3A_c.perc">C.perc</code></td>
<td>

<p>A list containing the percentage(s) of under- or/and 
over-sampling to apply to each &quot;class&quot; (bump) obtained with the threshold. The percentages should be provided in ascending order of target variable value. The percentages are applied in this order to the &quot;classes&quot; (bumps) obtained through the threshold.
The over-sampling percentage, a number above 1, means that the examples in that bump are increased by this percentage. The under-sampling percentage, a number below 1, means that the cases in the corresponding bump are under-sampled by this percentage. If the number 1 is provided then those examples are not changed. Alternatively it may be &quot;balance&quot; (the default) or &quot;extreme&quot;,
cases where the sampling percentages are automatically estimated.
</p>
</td></tr>
<tr><td><code id="SMOGNRegress_+3A_k">k</code></td>
<td>

<p>A number indicating the number of nearest neighbors to consider as the pool from where
the new generated examples are generated.
</p>
</td></tr>
<tr><td><code id="SMOGNRegress_+3A_repl">repl</code></td>
<td>

<p>A boolean value controlling the possibility of having repetition of examples when performing under-sampling by selecting among the &quot;normal&quot; examples.
</p>
</td></tr>
<tr><td><code id="SMOGNRegress_+3A_dist">dist</code></td>
<td>

<p>A character string indicating which distance metric to use when determining the k nearest neighbors. See the details. Defaults to &quot;Euclidean&quot;.
</p>
</td></tr>
<tr><td><code id="SMOGNRegress_+3A_p">p</code></td>
<td>

<p>A number indicating the value of p if the &quot;p-norm&quot; distance is chosen. Only necessary to define if a &quot;p-norm&quot; is chosen in the <code>dist</code> argument. see details.
</p>
</td></tr>
<tr><td><code id="SMOGNRegress_+3A_pert">pert</code></td>
<td>

<p>A number indicating the level of perturbation to introduce when generating synthetic examples through Gaussian Noise. Assuming as center the base example, this parameter defines the radius (based on the standard deviation) where the new example is generated.
</p>
</td></tr>
</table>


<h3>Details</h3>


<dl>
<dt><code>dist</code> parameter:</dt><dd><p>The parameter <code>dist</code> allows the user to define the distance metric to be used in the neighbors computation. Although the default is the Euclidean distance, other metrics are available. This allows the computation of distances in data sets with, for instance, both nominal and numeric features. The options available for the distance functions are as follows: 
</p>
<p>- for data with only numeric features: &quot;Manhattan&quot;, &quot;Euclidean&quot;, &quot;Canberra&quot;, &quot;Chebyshev&quot;, &quot;p-norm&quot;;
</p>
<p>- for data with only nominal features: &quot;Overlap&quot;;
</p>
<p>- for dealing with both nominal and numeric features: &quot;HEOM&quot;.
</p>
<p>When the &quot;p-norm&quot; is selected for the <code>dist</code> parameter, it is also necessary to define the value of parameter <code>p</code>. The value of parameter <code>p</code> sets which &quot;p-norm&quot; will be used. For instance, if <code>p</code> is set to 1, the &quot;1-norm&quot; (or Manhattan distance) is used, and if <code>p</code> is set to 2, the &quot;2-norm&quot; (or Euclidean distance) is applied.
For more details regarding the distance functions implemented in UBL package please see the package vignettes.
</p>
</dd>
</dl>



<h3>Value</h3>

<p>The function returns a data frame with
the new data set resulting from the application of the SMOGN
algorithm. 
</p>


<h3>Author(s)</h3>

<p> Paula Branco <a href="mailto:paobranco@gmail.com">paobranco@gmail.com</a>, Rita Ribeiro
<a href="mailto:rpribeiro@dcc.fc.up.pt">rpribeiro@dcc.fc.up.pt</a> and Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Paula Branco, Luis Torgo, and Rita Paula Ribeiro. <em>SMOGN: a pre-processing approach for imbalanced regression.</em> First International Workshop on Learning with Imbalanced Domains: Theory and Applications, 36-50(2017).
Torgo, Luis and Ribeiro, Rita P and Pfahringer, Bernhard and Branco, Paula (2013). <em>SMOTE for Regression</em>. Progress in Artificial Intelligence, Springer,378-389.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SmoteRegress">SmoteRegress</a>, <a href="#topic+GaussNoiseRegress">GaussNoiseRegress</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  ir &lt;- iris[-c(95:130), ]
  # using automatic relevance
  smogn1 &lt;- SmoteRegress(Sepal.Width~., ir, dist = "HEOM",
                                C.perc=list(0.5,2.5))
  smogn2 &lt;- SmoteRegress(Sepal.Width~., ir, dist = "HEOM",
                                C.perc = list(0.2, 4), thr.rel = 0.8)
  smogn3.iris &lt;- SmoteRegress(Sepal.Width~., ir, dist = "HEOM",
                                C.perc = "balance")

  # checking visually the results 
  plot(sort(ir$Sepal.Width))
  plot(sort(smogn1$Sepal.Width))
  
  # using a relevance function provided by the user
  rel &lt;- matrix(0, ncol = 3, nrow = 0)
  rel &lt;- rbind(rel, c(2, 1, 0))
  rel &lt;- rbind(rel, c(3, 0, 0))
  rel &lt;- rbind(rel, c(4, 1, 0))

  smognRel &lt;- SmoteRegress(Sepal.Width~., ir, rel = rel, dist = "HEOM",
                        C.perc = list(4, 0.5, 4))

  plot(sort(smognRel$Sepal.Width))
</code></pre>

<hr>
<h2 id='SmoteClassif'>
SMOTE algorithm for unbalanced classification problems
</h2><span id='topic+SmoteClassif'></span>

<h3>Description</h3>

<p>This function handles unbalanced classification problems using the SMOTE
method. Namely, it can generate a new &quot;SMOTEd&quot; data set that addresses
the class unbalance problem. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SmoteClassif(form, dat, C.perc = "balance", k = 5, repl = FALSE,
             dist = "Euclidean", p = 2)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="SmoteClassif_+3A_form">form</code></td>
<td>

<p>A formula describing the prediction problem
</p>
</td></tr>
<tr><td><code id="SmoteClassif_+3A_dat">dat</code></td>
<td>

<p>A data frame containing the original (unbalanced) data set
</p>
</td></tr>
<tr><td><code id="SmoteClassif_+3A_c.perc">C.perc</code></td>
<td>

<p>A named list containing the percentage(s) of under- or/and 
over-sampling to apply to each class.
The over-sampling percentage is a number above 1 while the under-sampling percentage should be a number below 1. If the number 1 is provided for a given class then that class remains unchanged. Alternatively it may be &quot;balance&quot; (the default) or &quot;extreme&quot;, cases where the sampling percentages are automatically estimated either to balance the examples between the minority and majority classes or to invert the distribution of examples across the existing classes transforming the majority classes into minority and vice-versa.
</p>
</td></tr>
<tr><td><code id="SmoteClassif_+3A_k">k</code></td>
<td>

<p>A number indicating the number of nearest neighbors that are used to
generate the new examples of the minority class(es).
</p>
</td></tr>
<tr><td><code id="SmoteClassif_+3A_repl">repl</code></td>
<td>

<p>A boolean value controlling the possibility of having repetition of examples when performing under-sampling by selecting among the majority class(es) examples.
</p>
</td></tr>
<tr><td><code id="SmoteClassif_+3A_dist">dist</code></td>
<td>

<p>A character string indicating which distance metric to use when determining the k nearest neighbors. See the details. Defaults to &quot;Euclidean&quot;.
</p>
</td></tr>
<tr><td><code id="SmoteClassif_+3A_p">p</code></td>
<td>

<p>A number indicating the value of p if the &quot;p-norm&quot; distance is chosen. Only necessary to define if a &quot;p-norm&quot; is chosen in the <code>dist</code> argument. See details.
</p>
</td></tr>
</table>


<h3>Details</h3>


<dl>
<dt><code>dist</code> parameter:</dt><dd><p>The parameter <code>dist</code> allows the user to define the distance metric to be used in the neighbors computation. Although the default is the Euclidean distance, other metrics are available. This allows the computation of distances in data sets with, for instance, both nominal and numeric features. The options available for the distance functions are as follows: 
</p>
<p>- for data with only numeric features: &quot;Manhattan&quot;, &quot;Euclidean&quot;, &quot;Canberra&quot;, &quot;Chebyshev&quot;, &quot;p-norm&quot;;
</p>
<p>- for data with only nominal features: &quot;Overlap&quot;;
</p>
<p>- for dealing with both nominal and numeric features: &quot;HEOM&quot;, &quot;HVDM&quot;.
</p>
<p>When the &quot;p-norm&quot; is selected for the <code>dist</code> parameter, it is also necessary to define the value of parameter <code>p</code>. The value of parameter <code>p</code> sets which &quot;p-norm&quot; will be used. For instance, if <code>p</code> is set to 1, the &quot;1-norm&quot; (or Manhattan distance) is used, and if <code>p</code> is set to 2, the &quot;2-norm&quot; (or Euclidean distance) is applied.
For more details regarding the distance functions implemented in UBL package please see the package vignettes.
</p>
</dd>
<dt>Smote algorithm:</dt><dd><p>Unbalanced classification problems cause problems to many learning
algorithms. These problems are characterized by  the uneven proportion
of cases that are available for each class of the problem.
</p>
<p>SMOTE (Chawla et. al. 2002) is a well-known algorithm to fight this
problem. The general idea of this method is to artificially generate
new examples of the minority class using the nearest neighbors of
these cases. Furthermore, the majority class examples are also
under-sampled, leading to a more balanced dataset. 
</p>
<p>The parameter <code>C.perc</code> controls the amount
of over-sampling and under-sampling applied and can be automatically estimated either to balance or invert the distribution of examples across the different classes. 
The parameter <code>k</code> controls the number of neighbors used to generate new synthetic examples.
</p>
</dd>
</dl>



<h3>Value</h3>

<p>The function returns a data frame with
the new data set resulting from the application of the SMOTE
algorithm. 
</p>


<h3>Author(s)</h3>

<p> Paula Branco <a href="mailto:paobranco@gmail.com">paobranco@gmail.com</a>, Rita Ribeiro
<a href="mailto:rpribeiro@dcc.fc.up.pt">rpribeiro@dcc.fc.up.pt</a> and Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Chawla, N. V., Bowyer, K. W., Hall, L. O., and Kegelmeyer, W. P. (2002).
<em>Smote: Synthetic minority over-sampling technique</em>. Journal of Artificial
Intelligence Research, 16:321-357.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+RandUnderClassif">RandUnderClassif</a>, <a href="#topic+RandOverClassif">RandOverClassif</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## A small example with a data set created artificially from the IRIS
## data 
data(iris)
dat &lt;- iris[, c(1, 2, 5)]
dat$Species &lt;- factor(ifelse(dat$Species == "setosa", "rare", "common")) 
## checking the class distribution of this artificial data set
table(dat$Species)

## now using SMOTE to create a more "balanced problem"
newData &lt;- SmoteClassif(Species ~ ., dat, C.perc = list(common = 1,rare = 6))
table(newData$Species)

## Checking visually the created data
par(mfrow = c(1, 2))
plot(dat[, 1], dat[, 2], pch = 19 + as.integer(dat[, 3]),
     main = "Original Data")
plot(newData[, 1], newData[, 2], pch = 19 + as.integer(newData[, 3]),
     main = "SMOTE'd Data")


# automatically balancing the data maintaining the total number of examples
datBal &lt;- SmoteClassif(Species ~ ., dat, C.perc = "balance")
table(datBal$Species)

# automatically inverting the original distribution of examples 
datExt &lt;- SmoteClassif(Species ~ ., dat, C.perc = "extreme")
table(datExt$Species)

  if (requireNamespace("DMwR2", quietly = TRUE)) {
  
  data(algae, package ="DMwR2")
  clean.algae &lt;- data.frame(algae[complete.cases(algae), ])
  C.perc = list(autumn = 2, summer = 1.5, winter = 0.9) 
 # class spring remains unchanged
 # In this case it is necessary to define a distance function that 
 # is able to deal with both nominal and numeric features 
 mysmote.algae &lt;- SmoteClassif(season~., clean.algae, C.perc, dist = "HEOM")
 # the distance function may be HVDM 
 smoteBalan.algae &lt;- SmoteClassif(season~., clean.algae, "balance",
                                  dist = "HVDM")
 smoteExtre.algae &lt;- SmoteClassif(season~., clean.algae, "extreme",
                                  dist = "HVDM")
} else {
  library(MASS)
  data(cats)
  mysmote.cats &lt;- SmoteClassif(Sex~., cats, list(M = 0.8, F = 1.8))
  smoteBalan.cats &lt;- SmoteClassif(Sex~., cats, "balance")
  smoteExtre.cats &lt;- SmoteClassif(Sex~., cats, "extreme")
}
</code></pre>

<hr>
<h2 id='SmoteRegress'>
SMOTE algorithm for imbalanced regression problems
</h2><span id='topic+SmoteRegress'></span>

<h3>Description</h3>

<p>This function handles imbalanced regression problems using the SMOTE
method. Namely, it can generate a new &quot;SMOTEd&quot; data set that addresses
the problem of imbalanced domains. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SmoteRegress(form, dat, rel = "auto", thr.rel = 0.5, C.perc = "balance",
                         k = 5, repl = FALSE, dist = "Euclidean", p = 2)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="SmoteRegress_+3A_form">form</code></td>
<td>

<p>A formula describing the prediction problem
</p>
</td></tr>
<tr><td><code id="SmoteRegress_+3A_dat">dat</code></td>
<td>

<p>A data frame containing the original (unbalanced) data set
</p>
</td></tr>
<tr><td><code id="SmoteRegress_+3A_rel">rel</code></td>
<td>

<p>The relevance function which can be automatically (&quot;auto&quot;) determined (the default) or may be provided by the user through a matrix.
</p>
</td></tr>
<tr><td><code id="SmoteRegress_+3A_thr.rel">thr.rel</code></td>
<td>

<p>A number indicating the relevance threshold above which a case is considered as belonging to the rare &quot;class&quot;.
</p>
</td></tr>
<tr><td><code id="SmoteRegress_+3A_c.perc">C.perc</code></td>
<td>

<p>A list containing the percentage(s) of under- or/and 
over-sampling to apply to each &quot;class&quot; (bump) obtained with the threshold. The percentages should be provided in ascending order of target variable value. The percentages are applied in this order to the &quot;classes&quot; (bumps) obtained through the threshold.
The over-sampling percentage, a number above 1, means that the examples in that bump are increased by this percentage. The under-sampling percentage, a number below 1, means that the cases in the corresponding bump are under-sampled by this percentage. If the number 1 is provided then those examples are not changed. Alternatively it may be &quot;balance&quot; (the default) or &quot;extreme&quot;,
cases where the sampling percentages are automatically estimated.
</p>
</td></tr>
<tr><td><code id="SmoteRegress_+3A_k">k</code></td>
<td>

<p>A number indicating the number of nearest neighbors to consider as the pool from where
the new generated examples are generated.
</p>
</td></tr>
<tr><td><code id="SmoteRegress_+3A_repl">repl</code></td>
<td>

<p>A boolean value controlling the possibility of having repetition of examples when performing under-sampling by selecting among the &quot;normal&quot; examples.
</p>
</td></tr>
<tr><td><code id="SmoteRegress_+3A_dist">dist</code></td>
<td>

<p>A character string indicating which distance metric to use when determining the k nearest neighbors. See the details. Defaults to &quot;Euclidean&quot;.
</p>
</td></tr>
<tr><td><code id="SmoteRegress_+3A_p">p</code></td>
<td>

<p>A number indicating the value of p if the &quot;p-norm&quot; distance is chosen. Only necessary to define if a &quot;p-norm&quot; is chosen in the <code>dist</code> argument. see details.
</p>
</td></tr>
</table>


<h3>Details</h3>


<dl>
<dt><code>dist</code> parameter:</dt><dd><p>The parameter <code>dist</code> allows the user to define the distance metric to be used in the neighbors computation. Although the default is the Euclidean distance, other metrics are available. This allows the computation of distances in data sets with, for instance, both nominal and numeric features. The options available for the distance functions are as follows: 
</p>
<p>- for data with only numeric features: &quot;Manhattan&quot;, &quot;Euclidean&quot;, &quot;Canberra&quot;, &quot;Chebyshev&quot;, &quot;p-norm&quot;;
</p>
<p>- for data with only nominal features: &quot;Overlap&quot;;
</p>
<p>- for dealing with both nominal and numeric features: &quot;HEOM&quot;.
</p>
<p>When the &quot;p-norm&quot; is selected for the <code>dist</code> parameter, it is also necessary to define the value of parameter <code>p</code>. The value of parameter <code>p</code> sets which &quot;p-norm&quot; will be used. For instance, if <code>p</code> is set to 1, the &quot;1-norm&quot; (or Manhattan distance) is used, and if <code>p</code> is set to 2, the &quot;2-norm&quot; (or Euclidean distance) is applied.
For more details regarding the distance functions implemented in UBL package please see the package vignettes.
</p>
</dd>
<dt>SmoteR algorithm:</dt><dd>
<p>Imbalanced domains cause problems to many learning
algorithms. These problems are characterized by the uneven proportion
of cases that are available for certain ranges of the target variable which are the most important to the user.
</p>
<p>SMOTE (Chawla et. al. 2002) is a well-known algorithm for classification tasks to fight this
problem. The general idea of this method is to artificially generate
new examples of the minority class using the nearest neighbors of
these cases. Furthermore, the majority class examples are also
under-sampled, leading to a more balanced data set. SmoteR is a variant of SMOTE algorithm proposed by Torgo et al. (2013) to address the problem of imbalanced domains in regression tasks. This function uses the parameters <code>rel</code> and <code>thr.rel</code>, a relevance function and a relevance threshold for distinguishing between the normal and rare cases.
</p>
<p>The parameter <code>C.perc</code> controls the amount
of over-sampling and under-sampling applied and can be automatically estimated either to balance or invert the distribution of examples across the different bumps. 
The parameter <code>k</code> controls the number of neighbors used to generate new synthetic examples.
</p>
</dd>
</dl>



<h3>Value</h3>

<p>The function returns a data frame with
the new data set resulting from the application of the smoteR
algorithm. 
</p>


<h3>Author(s)</h3>

<p> Paula Branco <a href="mailto:paobranco@gmail.com">paobranco@gmail.com</a>, Rita Ribeiro
<a href="mailto:rpribeiro@dcc.fc.up.pt">rpribeiro@dcc.fc.up.pt</a> and Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Chawla, N. V., Bowyer, K. W., Hall, L. O., and Kegelmeyer, W. P. (2002).
<em>Smote: Synthetic minority over-sampling technique</em>. Journal of Artificial
Intelligence Research, 16:321-357.
</p>
<p>Torgo, Luis and Ribeiro, Rita P and Pfahringer, Bernhard and Branco, Paula (2013). <em>SMOTE for Regression</em>. Progress in Artificial Intelligence, Springer,378-389.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+RandUnderRegress">RandUnderRegress</a>, <a href="#topic+RandOverRegress">RandOverRegress</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  ir &lt;- iris[-c(95:130), ]
  mysmote1.iris &lt;- SmoteRegress(Sepal.Width~., ir, dist = "HEOM",
                                C.perc=list(0.5,2.5))
  mysmote2.iris &lt;- SmoteRegress(Sepal.Width~., ir, dist = "HEOM",
                                C.perc = list(0.2, 4), thr.rel = 0.8)
  smoteBalan.iris &lt;- SmoteRegress(Sepal.Width~., ir, dist = "HEOM",
                                C.perc = "balance")
  smoteExtre.iris &lt;- SmoteRegress(Sepal.Width~., ir, dist = "HEOM",
                                C.perc = "extreme")
  
  # checking visually the results 
  plot(sort(ir$Sepal.Width))
  plot(sort(smoteExtre.iris$Sepal.Width))
  
  # using a relevance function provided by the user
  rel &lt;- matrix(0, ncol = 3, nrow = 0)
  rel &lt;- rbind(rel, c(2, 1, 0))
  rel &lt;- rbind(rel, c(3, 0, 0))
  rel &lt;- rbind(rel, c(4, 1, 0))

  sP.ir &lt;- SmoteRegress(Sepal.Width~., ir, rel = rel, dist = "HEOM",
                        C.perc = list(4, 0.5, 4))

</code></pre>

<hr>
<h2 id='TomekClassif'>
Tomek links for imbalanced classification problems
</h2><span id='topic+TomekClassif'></span>

<h3>Description</h3>

<p>This function uses Tomek links to perform under-sampling for handling imbalanced multiclass problems. Tomek links are broken by removing one or both examples forming the link. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TomekClassif(form, dat, dist = "Euclidean", p = 2, Cl = "all", rem = "both")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="TomekClassif_+3A_form">form</code></td>
<td>

<p>A formula describing the prediction problem.
</p>
</td></tr>
<tr><td><code id="TomekClassif_+3A_dat">dat</code></td>
<td>

<p>A data frame containing the original imbalanced data set.
</p>
</td></tr>
<tr><td><code id="TomekClassif_+3A_dist">dist</code></td>
<td>

<p>A character string indicating which distance metric to use when determining the k nearest neighbors. See the details. Defaults to &quot;Euclidean&quot;.
</p>
</td></tr>
<tr><td><code id="TomekClassif_+3A_p">p</code></td>
<td>

<p>A number indicating the value of p if the &quot;p-norm&quot; distance is chosen. Only necessary to define if a &quot;p-norm&quot; is chosen in the <code>dist</code> argument. See details.
</p>
</td></tr>
<tr><td><code id="TomekClassif_+3A_cl">Cl</code></td>
<td>

<p>A character vector indicating which classes should be under-sampled. Defaults to &quot;all&quot; meaning that examples from all existing classes can be removed. The user may also specify a subset of classes for which tomek links should be removed.</p>
</td></tr>
<tr><td><code id="TomekClassif_+3A_rem">rem</code></td>
<td>

<p>A character string indicating if both examples forming the Tomek link are to be removed, or if only the example from the larger class should be discarded. In the first case this parameter should be set to &quot;both&quot; and in the second case should be set to &quot;maj&quot;.
</p>
</td></tr>
</table>


<h3>Details</h3>


<dl>
<dt><code>dist</code> parameter:</dt><dd><p>The parameter <code>dist</code> allows the user to define the distance metric to be used in the neighbors computation. Although the default is the Euclidean distance, other metrics are available. This allows the computation of distances in data sets with, for instance, both nominal and numeric features. The options available for the distance functions are as follows: 
</p>
<p>- for data with only numeric features: &quot;Manhattan&quot;, &quot;Euclidean&quot;, &quot;Canberra&quot;, &quot;Chebyshev&quot;, &quot;p-norm&quot;;
</p>
<p>- for data with only nominal features: &quot;Overlap&quot;;
</p>
<p>- for dealing with both nominal and numeric features: &quot;HEOM&quot;, &quot;HVDM&quot;.
</p>
<p>When the &quot;p-norm&quot; is selected for the <code>dist</code> parameter, it is also necessary to define the value of parameter <code>p</code>. The value of parameter <code>p</code> sets which &quot;p-norm&quot; will be used. For instance, if <code>p</code> is set to 1, the &quot;1-norm&quot; (or Manhattan distance) is used, and if <code>p</code> is set to 2, the &quot;2-norm&quot; (or Euclidean distance) is applied.
For more details regarding the distance functions implemented in UBL package please see the package vignettes.
</p>
</dd>
<dt>Tomek method:</dt><dd><p>This function performs an under-sampling strategy based on the notion of Tomek links for imbalanced multiclass problems. Two examples form a Tomek link if they are each other closest neighbors and they have different class labels.
</p>
<p>The under-sampling procedure can be performed in two different ways. When detected the Tomek links, the examples of both classes can be removed, or the Tomek link can be broken by removing only one of the examples (traditionally the one belonging to the majority class). This function also includes these two procedures. Moreover, it allows for the user to identify in which classes under-sampling should be applied. These two aspects are controlled by the <code>Cl</code> and <code>rem</code> parameters. The <code>Cl</code> parameter is used to express the classes that can be under-sampled and its default is  &quot;all&quot; (all existing classes are candidates for having examples removed). The parameter <code>rem</code> indicates if the Tomek link is broken by removing both examples (&quot;both&quot;) or by removing only the example belonging to the more populated class between the two existing in the Tomek link.
</p>
<p>Note that the options for <code>Cl</code> and <code>rem</code> may &quot;disagree&quot;. In those cases, the preference is given to the <code>Cl</code> options once the user choose that specific set of classes to under-sample and not the other ones (even if the defined classes are not the larger ones). This means that, when making a decision on how many and which examples will be removed the first criteria used will be the <code>Cl</code> definition .
</p>
<p>For a better clarification of the impact of the options selected for Cl and rem parameters we now provide some possible scenarios and the expected behavior:
</p>
<p>1) <code>Cl</code> is set to one class which is neither the more nor the less frequent, and <code>rem</code> is set to &quot;maj&quot;. The expected behavior is the following:
- if a Tomek link exists connecting the largest class and another class(not included in <code>Cl</code>): no example is removed;
- if a Tomek link exists connecting the larger class and the class defined in <code>Cl</code>: the example from the <code>Cl</code> class is removed (because the user expressly indicates that only examples from class <code>Cl</code> should be removed);
</p>
<p>2) <code>Cl</code> includes two classes and <code>rem</code> is set to &quot;both&quot;. This function will do the following:
- if a Tomek link exists between an example with class in <code>Cl</code> and another example with class not in <code>Cl</code>, then, only the example with class in <code>Cl</code> is removed;
- if the Tomek link exists between two examples with classes in <code>Cl</code>, then, both are removed.
</p>
<p>3) <code>Cl</code> includes two classes and <code>rem</code> is set to &quot;maj&quot;. The behavior of this function is the following:
-if a Tomek link exists connecting two classes included in <code>Cl</code>, then only the example belonging to the more populated class is removed;
-if a Tomek link exists connecting an example from a class included in <code>Cl</code> and another example whose class is not in <code>Cl</code> and is the largest class, then, no example is removed.
</p>
</dd>
</dl>



<h3>Value</h3>

<p>The function returns a list containing a data frame with
the new data set resulting from the application of the Tomek link strategy defined, and the indexes of the examples removed.
</p>


<h3>Author(s)</h3>

<p> Paula Branco <a href="mailto:paobranco@gmail.com">paobranco@gmail.com</a>, Rita Ribeiro
<a href="mailto:rpribeiro@dcc.fc.up.pt">rpribeiro@dcc.fc.up.pt</a> and Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Tomek, I. (1976). <em>Two modifications of CNN</em> IEEE Trans. Syst. Man Cybern.,  769-772
</p>


<h3>See Also</h3>

<p><code><a href="#topic+OSSClassif">OSSClassif</a>, <a href="#topic+CNNClassif">CNNClassif</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  if (requireNamespace("DMwR2", quietly = TRUE)) {

  data(algae, package ="DMwR2")
  clean.algae &lt;- data.frame(algae[complete.cases(algae), ])
  alg.HVDM1 &lt;- TomekClassif(season~., clean.algae, dist = "HVDM", 
                            Cl = c("winter", "spring"), rem = "both")
  alg.HVDM2 &lt;- TomekClassif(season~., clean.algae, dist = "HVDM", rem = "maj")
  
  # removes only examples from class summer which are the 
  # majority class in the link
  alg.EuM &lt;- TomekClassif(season~., clean.algae, dist = "HEOM", 
                          Cl = "summer", rem = "maj")
  
  # removes only examples from class summer in every link they appear
  alg.EuB &lt;- TomekClassif(season~., clean.algae, dist = "HEOM",
                          Cl = "summer", rem = "both")
                          
  summary(clean.algae$season)
  summary(alg.HVDM1[[1]]$season)
  summary(alg.HVDM2[[1]]$season)
  summary(alg.EuM[[1]]$season)
  summary(alg.EuB[[1]]$season)
  
  # check which were the indexes of the examples removed in alg.EuM
  alg.EuM[[2]]
  }
</code></pre>

<hr>
<h2 id='UtilInterpol'>
Utility surface obtained through methods for spatial interpolation of points. 
</h2><span id='topic+UtilInterpol'></span>

<h3>Description</h3>

<p>This function uses spatial interpolation methods for obtaining the utility surface. 
It depends on a set of points provided by the user and on a method selected for 
interpolation. The available interpolation methods are: bilinear, splines, idw and krige. Check the details section for more on these methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>UtilInterpol(trues, preds, type = c("utility", "cost", "benefit"), control.parms,
            minds, maxds, m.pts, method = c("bilinear", "splines", "idw", "krige"),
            visual = FALSE, eps = 0.1, full.output = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="UtilInterpol_+3A_trues">trues</code></td>
<td>

<p>A vector of true target variable values. Can be NULL. See details section.
</p>
</td></tr>
<tr><td><code id="UtilInterpol_+3A_preds">preds</code></td>
<td>

<p>A vector with corresponding predicted values for the trues provided. Can be NULL.
See details section.
</p>
</td></tr>
<tr><td><code id="UtilInterpol_+3A_type">type</code></td>
<td>

<p>A character specifying the type of surface that is being interpolated. 
It can be set to either &quot;utility&quot;, &quot;cost&quot; or &quot;benefit&quot;.
When set to &quot;cost&quot; we assume that the diagonal of the surface (where y=y.pred) is zero.
Therefore, in this case, the user doesn't need to set the control.parms parameter.
</p>
</td></tr>
<tr><td><code id="UtilInterpol_+3A_control.parms">control.parms</code></td>
<td>
 
<p>These parameters are necessary for utility and benefit surfaces. control.parms 
can be obtained with a call to function <a href="#topic+phi.control">phi.control</a>. This provides a list 
with the parameters used for defining the relevance function <a href="#topic+phi">phi</a>.
The points provided through these parameters are used for interpolating the 
utility surface because the relevance function matches the diagonal of the
utility, i.e., the relevance function <a href="#topic+phi">phi</a> corresponds to the utility of
accurate predictions (y = y.pred). Alternatively, the user may build the control.parms list.
When the user selects a cost surface, control.parms can simply be NULL. In this case,
we assume that the surface diagonal is zero. If control.parms are not NULL, then
specified points are used. See examples section.
</p>
</td></tr>
<tr><td><code id="UtilInterpol_+3A_minds">minds</code></td>
<td>

<p>The lower bound of the target variable considered for interpolation. A new minds value may be necessary when trues and/or preds provided have lower values than minds. This is handled by extrapolation and a warning is issued (see details).
</p>
</td></tr>
<tr><td><code id="UtilInterpol_+3A_maxds">maxds</code></td>
<td>

<p>The upper bound of the target variable considered for interpolation. A new maxds value may be necessary when trues and/or preds provided have values higher than maxds. This is handled by extrapolation and a warning is issued (see details).
</p>
</td></tr>
<tr><td><code id="UtilInterpol_+3A_m.pts">m.pts</code></td>
<td>

<p>A 3-column matrix with interpolating points for the off-diagonal cases
(i.e., y != y.pred), provided by the user. The first column
has the y value, the second column the y.pred value and the
third column has the corresponding utility value. At least, the off diagonal
domain boundary points (i.e., points (minds, maxds, util) and 
(maxds, minds, util) ) must be provided in this matrix. Moreover, the points
provided through this parameter must be in [minds, maxds] range.
</p>
</td></tr>
<tr><td><code id="UtilInterpol_+3A_method">method</code></td>
<td>

<p>A character indicating which interpolation method should be used. Can be one of: &quot;bilinear&quot;, &quot;splines&quot;, &quot;idw&quot; or &quot;krige&quot;. See details section for a description of the available methods.
</p>
</td></tr>
<tr><td><code id="UtilInterpol_+3A_visual">visual</code></td>
<td>

<p>Logical. If TRUE a plot of the utility surface isometrics obtained and
the points provided is displayed. If FALSE (the default) no image is plotted.
</p>
</td></tr>
<tr><td><code id="UtilInterpol_+3A_eps">eps</code></td>
<td>

<p>Numeric value for the precision considered during the interpolation. Defaults to 0.1. 
Only relevant if a plot is displayed, or when full.output is set to TRUE. See details section.
</p>
</td></tr>
<tr><td><code id="UtilInterpol_+3A_full.output">full.output</code></td>
<td>
<p>Logical. If FALSE (the default) only the results from points provided through
parameters trues and preds are returned. If TRUE a matrix with the utility
of all points in domain (considering the eps provided) are returned. See details section.
</p>
</td></tr>
</table>


<h3>Details</h3>


<dl>
<dt><code>method</code> parameter:</dt><dd><p>The parameter <code>method</code> allows the user to select from a set of interpolation methods. The available methods are as follows:
</p>
<p>- <code>bilinear</code>: local fitting of a polynomial surface of degree 1 obtained through loess function of stats package.
</p>
<p>- <code>splines</code>: multilevel B-splines interpolation method obtained through MBA R package.
</p>
<p>- <code>idw</code>: inverse distance weighted interpolation obtained through R package gstat.
</p>
<p>- <code>krige</code>: automatic kriging obtained using automap R package. 
</p>
</dd>
<dt>extrapolation:</dt><dd><p> when trues or preds provided are outside the range [minds, maxds] the function   performs an extrapolation of the domain. To achieve this, four new points are added that extend the initial target variable domain ([minds, maxds]). This extrapolation is performed as follows:
</p>
<p>- first: determine inc.fac, the distance necessary to increase (the largest value needed increase the axes to include all trues and preds provided);
</p>
<p>- second: define the new target variable domain ([minds - inc.fac, maxds + inc.fac]);
</p>
<p>- third: add two new diagonal points evaluating the relevance function on these new points
(i.e. add (minds-inc.fac, minds-nc.fac, phi(minds-inc.fac, minds.inc.fac)) and
(maxds+inc.fac, maxds+inc.fac, phi(maxds+inc.fac, maxds+inc.fac)));
</p>
<p>- fourth: add two new off-diagonal points using the new min and max values of
the domain and the utility provided by the user for the two mandatory 
points (minds, maxds) and (maxds, minds).
</p>
<p>In order to avoid this extrapolation, the user must ensure that the values provided in trues and preds vectors are inside the [minds, maxds] range provided.
</p>
</dd>
<dt><code>full.output</code> parameter:</dt><dd><p>This parameter is used to select which 
utility values are returned. There are two options for this parameter:
</p>
<p>- FALSE: This means that the user is only interested in obtaining the utility
surface values of some points (y, y.pred). In this case, the y and y.pred should be
provided through parameters trues and preds and the function returns a vector
with the utility for the corresponding points.
</p>
<p>- TRUE: The user is interested in obtaining the utility surface values on a 
grid of equally spaced values of the target variable domain. In this case, there
is no need for specifying parameters trues and preds, because the goal is not 
to observe the utility of these points. Parameters trues and preds can be set 
to NULL in this case. The function returns a lXl matrix with the utility of
all points in a grid defined as follows. The l equally spaced points are a 
sequence that starts at minds-0.01, ends at maxds+0.01 and are incremented by
eps value.
</p>
</dd>
</dl>



<h3>Value</h3>

<p>The function returns a vector with utility of the points provided through the vectors trues and preds.
</p>


<h3>Author(s)</h3>

<p> Paula Branco <a href="mailto:paobranco@gmail.com">paobranco@gmail.com</a>, Rita Ribeiro
<a href="mailto:rpribeiro@dcc.fc.up.pt">rpribeiro@dcc.fc.up.pt</a> and Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>See Also</h3>

<p><code><a href="#topic+phi.control">phi.control</a>, <a href="#topic+UtilOptimRegress">UtilOptimRegress</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# examples with a utility surface
data(Boston, package = "MASS")

tgt &lt;- which(colnames(Boston) == "medv")
sp &lt;- sample(1:nrow(Boston), as.integer(0.7*nrow(Boston)))
train &lt;- Boston[sp,]
test &lt;- Boston[-sp,]

control.parms &lt;- phi.control(Boston[,tgt], method="extremes", extr.type="both")
# the boundaries of the domain considered
minds &lt;- min(Boston[,tgt])-5
maxds &lt;- max(Boston[,tgt])+5

# build m.pts to include at least the utility of the
# points (minds, maxds) and (maxds, minds)
m.pts &lt;- matrix(c(minds, maxds, -1, maxds, minds, 0),
                byrow=TRUE, ncol=3)

trues &lt;- test[,tgt]
library(randomForest)
model &lt;- randomForest(medv~., train)
preds &lt;- predict(model, test)

resLIN &lt;- UtilInterpol(trues, preds, type="util", control.parms, minds, maxds, m.pts,
                         method = "bilinear", visual=TRUE)
resIDW &lt;- UtilInterpol(trues, preds, type="util", control.parms, minds, maxds, m.pts,
                        method = "idw", visual=TRUE)
resSPL &lt;- UtilInterpol(trues, preds, type="util", control.parms, minds, maxds, m.pts,
                        method = "spl", visual=TRUE)
resKRIGE &lt;- UtilInterpol(trues, preds, type="util", control.parms, minds, maxds, m.pts,
                          method = "krige", visual=TRUE)

# examples with a cost surface
data(Boston, package = "MASS")

tgt &lt;- which(colnames(Boston) == "medv")
sp &lt;- sample(1:nrow(Boston), as.integer(0.7*nrow(Boston)))
train &lt;- Boston[sp,]
test &lt;- Boston[-sp,]

# the boundaries of the domain considered
minds &lt;- min(Boston[,tgt])-5
maxds &lt;- max(Boston[,tgt])+5

# build m.pts to include at least the utility of the
# points (minds, maxds) and (maxds, minds)
m.pts &lt;- matrix(c(minds, maxds, 5, maxds, minds, 20),
                byrow=TRUE, ncol=3)

trues &lt;- test[,tgt]

# train a model and predict on test set
library(randomForest)
model &lt;- randomForest(medv~., train)
preds &lt;- predict(model, test)

costLIN &lt;- UtilInterpol(trues, preds, type="cost", control.parms=NULL, minds, maxds, m.pts,
                         method = "bilinear", visual=TRUE )

costSPL &lt;- UtilInterpol(trues, preds, type="cost", control.parms=NULL, minds, maxds, m.pts,
                        method = "spl", visual=TRUE)

costKRIGE &lt;- UtilInterpol(trues, preds, type="cost", control.parms=NULL, minds, maxds, m.pts,
                          method = "krige", visual=TRUE)

costIDW &lt;- UtilInterpol(trues, preds, type="cost", control.parms=NULL, minds, maxds, m.pts,
                        method = "idw", visual=TRUE)


# if the user has a cost matrix and wants to specify the control.parms:
my.pts &lt;- matrix(c(0, 0, 0, 10, 0, 0, 20, 0, 0, 45, 0, 0), byrow=TRUE, ncol=3)
control.parms &lt;- phi.control(trues, method="range", control.pts = my.pts)

costLIN &lt;- UtilInterpol(trues, preds, type="cost", control.parms=control.parms,
                        minds, maxds, m.pts, method = "bilinear", visual=TRUE )


# first trues and preds
trues[1:5]
preds[1:5]
trues[1:5]-preds[1:5]

# first cost results on these predictions for cost surface costIDW
costIDW[1:5]
# a summary of these prediction costs:
summary(costIDW)

#example with a benefit surface

# define control.parms either by defining a list with 3 named elements 
# or by calling phi.control function with method range and passing 
# the selected control.pts
control.parms &lt;- list(method="range", npts=5,
                      control.pts=c(0,1,0,10,5,0.5,20,10,0.5,30,30,0,50,30,0))
m.pts &lt;- matrix(c(minds, maxds, 0, maxds, minds, 0),
                byrow=TRUE, ncol=3)
                
benLIN &lt;- UtilInterpol(trues, preds, type="ben", control.parms, minds, maxds, m.pts,
                       method = "bilinear", visual=TRUE)
benIDW &lt;- UtilInterpol(trues, preds, type="ben", control.parms, minds, maxds, m.pts,
                       method = "idw", visual=TRUE)
benSPL &lt;- UtilInterpol(trues, preds, type="ben", control.parms, minds, maxds, m.pts,
                       method = "spl", visual=TRUE)
benKRIGE &lt;- UtilInterpol(trues, preds, type="ben", control.parms, minds, maxds, m.pts,
                         method = "krige", visual=TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='UtilOptimClassif'>
Optimization of predictions utility, cost or benefit for classification problems.
</h2><span id='topic+UtilOptimClassif'></span>

<h3>Description</h3>

<p>This function determines the optimal predictions given a utility, cost or benefit matrix for the selected learning algorithm. The learning algorithm must provide probabilities for the problem classes. If the matrix provided is of type utility or benefit a maximization process is carried out. If the user provides a cost matrix, then a minimization process is applied.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>UtilOptimClassif(form, train, test, mtr, type = "util",
                 learner = NULL, learner.pars=NULL, predictor="predict",
                 predictor.pars=NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="UtilOptimClassif_+3A_form">form</code></td>
<td>
    
<p>A formula describing the prediction problem.
</p>
</td></tr>
<tr><td><code id="UtilOptimClassif_+3A_train">train</code></td>
<td>

<p>A data.frame with the training data.
</p>
</td></tr>
<tr><td><code id="UtilOptimClassif_+3A_test">test</code></td>
<td>

<p>A data.frame with the test data.
</p>
</td></tr>
<tr><td><code id="UtilOptimClassif_+3A_mtr">mtr</code></td>
<td>

<p>A matrix, specifying the utility, cost, or benefit values associated to accurate 
predictions and misclassification errors. It can be either a cost matrix, 
a benefit matrix or a utility matrix. The corresponding type should be set in 
parameter type. The matrix must be always provided with the true class in 
the rows and the predicted class in the columns.
</p>
</td></tr>
<tr><td><code id="UtilOptimClassif_+3A_type">type</code></td>
<td>

<p>The type of mtr provided. Can be set to: &quot;utility&quot;(default), &quot;cost&quot; or &quot;benefit&quot;.
</p>
</td></tr>
<tr><td><code id="UtilOptimClassif_+3A_learner">learner</code></td>
<td>

<p>Character specifying the learning algorithm to use. It is required for the selected learner to 
provide class probabilities.
</p>
</td></tr>
<tr><td><code id="UtilOptimClassif_+3A_learner.pars">learner.pars</code></td>
<td>

<p>A named list containing the parameters of the learning algorithm.
</p>
</td></tr>
<tr><td><code id="UtilOptimClassif_+3A_predictor">predictor</code></td>
<td>

<p>Character specifying the predictor selected (defaults to &quot;predict&quot;).
</p>
</td></tr>
<tr><td><code id="UtilOptimClassif_+3A_predictor.pars">predictor.pars</code></td>
<td>

<p>A named list with the predictor selected parameters.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a vector with the predictions for the test data optimized
using the matrix provided.
</p>


<h3>Author(s)</h3>

<p> Paula Branco <a href="mailto:paobranco@gmail.com">paobranco@gmail.com</a>, Rita Ribeiro
<a href="mailto:rpribeiro@dcc.fc.up.pt">rpribeiro@dcc.fc.up.pt</a> and Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Elkan, C., 2001, August. The foundations of cost-sensitive learning. In International joint conference on artificial intelligence (Vol. 17, No. 1, pp. 973-978). LAWRENCE ERLBAUM ASSOCIATES LTD.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+UtilOptimRegress">UtilOptimRegress</a>, <a href="#topic+EvalClassifMetrics">EvalClassifMetrics</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># the synthetic data set provided with UBL package for classification
data(ImbC)
sp &lt;- sample(1:nrow(ImbC), round(0.7*nrow(ImbC)))
train &lt;- ImbC[sp, ]
test &lt;- ImbC[-sp,]

# example with a utility matrix
# define a utility matrix (true class in rows and pred class in columns)
matU &lt;- matrix(c(0.2, -0.5, -0.3, -1, 1, -0.9, -0.9, -0.8, 0.9), byrow=TRUE, ncol=3)

library(e1071) # for the naiveBayes classifier

resUtil &lt;- UtilOptimClassif(Class~., train, test, mtr = matU, type="util",
                       learner = "naiveBayes", 
                       predictor.pars = list(type="raw", threshold = 0.01))

# learning a standard model without maximizing utility
model &lt;- naiveBayes(Class~., train)
resNormal &lt;- predict(model, test, type="class", threshold = 0.01)
# Check the difference in the total utility of the results
EvalClassifMetrics(test$Class, resNormal, mtr=matU, type= "util")
EvalClassifMetrics(test$Class, resUtil, mtr=matU, type= "util")


#example with a cost matrix
# define a cost matrix (true class in rows and pred class in columns)
matC &lt;- matrix(c(0, 0.5, 0.3, 1, 0, 0.9, 0.9, 0.8, 0), byrow=TRUE, ncol=3)
resUtil &lt;- UtilOptimClassif(Class~., train, test, mtr = matC, type="cost",
                           learner = "naiveBayes", 
                           predictor.pars = list(type="raw", threshold = 0.01))

# learning a standard model without minimizing the costs
model &lt;- naiveBayes(Class~., train)
resNormal &lt;- predict(model, test, type="class")
# Check the difference in the total utility of the results
EvalClassifMetrics(test$Class, resNormal, mtr=matC, type= "cost")
EvalClassifMetrics(test$Class, resUtil, mtr=matC, type= "cost")


#example with a benefit matrix
# define a benefit matrix (true class in rows and pred class in columns)
matB &lt;- matrix(c(0.2, 0, 0, 0, 1, 0, 0, 0, 0.9), byrow=TRUE, ncol=3)

resUtil &lt;- UtilOptimClassif(Class~., train, test, mtr = matB, type="ben",
                           learner = "naiveBayes", 
                           predictor.pars = list(type="raw", threshold = 0.01))

# learning a standard model without maximizing benefits
model &lt;- naiveBayes(Class~., train)
resNormal &lt;- predict(model, test, type="class", threshold = 0.01)
# Check the difference in the total utility of the results
EvalClassifMetrics(test$Class, resNormal, mtr=matB, type= "ben")
EvalClassifMetrics(test$Class, resUtil, mtr=matB, type= "ben")

table(test$Class,resNormal)
table(test$Class,resUtil)


</code></pre>

<hr>
<h2 id='UtilOptimRegress'>
Optimization of predictions utility, cost or benefit for regression problems.
</h2><span id='topic+UtilOptimRegress'></span>

<h3>Description</h3>

<p>This function determines the optimal predictions given a utility, cost or benefit surface. This surface is obtained through a specified strategy with some parameters. For determining the optimal predictions an estimation of the conditional probability density function is performed for each test case.
If the surface provided is of type utility or benefit a maximization process is carried out. If the user provides a cost surface, then a minimization is performed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>UtilOptimRegress(form, train, test, type = "util", strat = "interpol", 
                 strat.parms = list(method = "bilinear"), control.parms, m.pts,
                 minds, maxds, eps = 0.1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="UtilOptimRegress_+3A_form">form</code></td>
<td>
    
<p>A formula describing the prediction problem.
</p>
</td></tr>
<tr><td><code id="UtilOptimRegress_+3A_train">train</code></td>
<td>

<p>A data.frame with the training data.
</p>
</td></tr>
<tr><td><code id="UtilOptimRegress_+3A_test">test</code></td>
<td>

<p>A data.frame with the test data.
</p>
</td></tr>
<tr><td><code id="UtilOptimRegress_+3A_type">type</code></td>
<td>

<p>A character specifying the type of surface provided. Can be one of: &quot;utility&quot;,
&quot;cost&quot; or &quot;benefit&quot;. Defaults to &quot;utility&quot;.
</p>
</td></tr>
<tr><td><code id="UtilOptimRegress_+3A_strat">strat</code></td>
<td>

<p>A character determining the strategy for obtaining the surface of the problem. 
For now, only the interpolation strategy is available (the default).  
</p>
</td></tr>
<tr><td><code id="UtilOptimRegress_+3A_strat.parms">strat.parms</code></td>
<td>

<p>A named list containing the parameters necessary for the strategy previously specified.
For the interpolation strategy (the default and only strategy available for now),
it is required that the user specifies wich method sould be used for interpolating the points.
</p>
</td></tr>
<tr><td><code id="UtilOptimRegress_+3A_control.parms">control.parms</code></td>
<td>

<p>A named list with the control.parms defined through the function <a href="#topic+phi.control">phi.control</a>.
These parameters stablish the diagonal of the surface provided. If the type of
surface defined is &quot;cost&quot; this parameter can be set to NULL, because in this 
case we assume that the accurate prediction, i.e., points in the diagonal of 
the surface have zero cost. See examples.
</p>
</td></tr>
<tr><td><code id="UtilOptimRegress_+3A_m.pts">m.pts</code></td>
<td>

<p>A matrix with 3-columns, with interpolation points specifying the utility, cost
or benefit of the surface. The points sould be in the off-diagonal of the 
surface, i.e., the user should provide points where y != y.pred.
The first column must have the true value (y), the second column the corresponding
prediction (y.pred) and the third column sets the utility cost or benefit of that point 
(y, y.pred). The user should define as many points as possible. The minimum number of 
required points are two. More specifically, the user must always set the surface 
values of at least the points (minds, maxds) and (maxds, minds). See minds and 
maxds description.
</p>
</td></tr>
<tr><td><code id="UtilOptimRegress_+3A_maxds">maxds</code></td>
<td>

<p>The numeric upper bound of the target variable considered.
</p>
</td></tr>
<tr><td><code id="UtilOptimRegress_+3A_minds">minds</code></td>
<td>

<p>The numeric lower bound of the target variable considered.
</p>
</td></tr>
<tr><td><code id="UtilOptimRegress_+3A_eps">eps</code></td>
<td>

<p>Numeric value for the precision considered during the interpolation. Defaults to 0.1. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The optimization process carried out by this function uses a method for conditional density estimation proposed by Rau M.M et al.(2015). Code for conditional density estimation (available on github https://github.com/MarkusMichaelRau/OrdinalClassification) kindly contributed by M. M. Rau with changes made by P.Branco.
The optimization is achieved generalizing the method proposed by Elkan (2001) for classification tasks. In regression, this process involves determining, for each test case, the maximum integral (for utility or benefit surfaces, or the minimum if we have a cost surface) of the product of the conditional density function estimated and either the utility, the benefit or the cost surface.
The optimal prediction for a case q is given by:
<code class="reqn">y^{*}(q)=argmax[z] \int pdf(y|q).U(y,z) dy</code>,
where pdf(y|q) is the conditional densitiy estimation for case q, and U(y,z) is the
utility, benefit or cost surface evaluated on the true value y and predictied value z.
</p>


<h3>Value</h3>

<p>The function returns a vector with the predictions for the test data optimized
using the surface provided.
</p>


<h3>Author(s)</h3>

<p> Paula Branco <a href="mailto:paobranco@gmail.com">paobranco@gmail.com</a>, Rita Ribeiro
<a href="mailto:rpribeiro@dcc.fc.up.pt">rpribeiro@dcc.fc.up.pt</a> and Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>References</h3>

<p>Rau, M.M., Seitz, S., Brimioulle, F., Frank, E., Friedrich, O., Gruen, D. and Hoyle, B., 2015. Accurate photometric redshift probability density estimation-method comparison and application. Monthly Notices of the Royal Astronomical Society, 452(4), pp.3710-3725.
</p>
<p>Elkan, C., 2001, August. The foundations of cost-sensitive learning. In International joint conference on artificial intelligence (Vol. 17, No. 1, pp. 973-978). LAWRENCE ERLBAUM ASSOCIATES LTD.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+phi.control">phi.control</a>, <a href="#topic+UtilOptimClassif">UtilOptimClassif</a>, <a href="#topic+UtilInterpol">UtilInterpol</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#Example using a utility surface:
data(Boston, package = "MASS")

tgt &lt;- which(colnames(Boston) == "medv")
sp &lt;- sample(1:nrow(Boston), as.integer(0.7*nrow(Boston)))
train &lt;- Boston[sp,]
test &lt;- Boston[-sp,]

control.parms &lt;- phi.control(Boston[,tgt], method="extremes", extr.type="both")
# the boundaries of the domain considered
minds &lt;- min(train[,tgt])
maxds &lt;- max(train[,tgt])

# build m.pts to include at least (minds, maxds) and (maxds, minds) points
# m.pts must only contain points in [minds, maxds] range.
m.pts &lt;- matrix(c(minds, maxds, -1, maxds, minds, -1),
                byrow=TRUE, ncol=3)

pred.res &lt;- UtilOptimRegress(medv~., train, test, type = "util", strat = "interpol",
                             strat.parms=list(method = "bilinear"),
                             control.parms = control.parms,
                             m.pts = m.pts, minds = minds, maxds = maxds)

eval.util &lt;- EvalRegressMetrics(test$medv, pred.res$optim, pred.res$utilRes,
                                 thr=0.8, control.parms = control.parms)

# train a normal model
model &lt;- randomForest(medv~.,train)
normal.preds &lt;- predict(model, test)

#obtain the utility of the new points (trues, preds)
NormalUtil &lt;- UtilInterpol(test$medv, normal.preds, type="util", 
                           control.parms = control.parms,
                           minds, maxds, m.pts, method = "bilinear")
#check the performance
eval.normal &lt;- EvalRegressMetrics(test$medv, normal.preds, NormalUtil,
                                  thr=0.8, control.parms = control.parms)

#check both results
eval.util
eval.normal


#check visually both predictions and the surface used
UtilInterpol(test$medv, normal.preds, type = "util", control.parms = control.parms,
                           minds, maxds, m.pts, method = "bilinear", visual=TRUE)

points(test$medv, normal.preds, col="green")
points(test$medv, pred.res$optim, col="blue")

# another example now using points interpolation with splines
if (requireNamespace("DMwR2", quietly = TRUE)){

data(algae, package ="DMwR2")
ds &lt;- data.frame(algae[complete.cases(algae[,1:12]), 1:12])
tgt &lt;- which(colnames(ds) == "a1")
sp &lt;- sample(1:nrow(ds), as.integer(0.7*nrow(ds)))
train &lt;- ds[sp,]
test &lt;- ds[-sp,]
  
control.parms &lt;- phi.control(ds[,tgt], method="extremes", extr.type="both")

# the boundaries of the domain considered
minds &lt;- min(train[,tgt])
maxds &lt;- max(train[,tgt])

# build m.pts to include at least (minds, maxds) and (maxds, minds) points
m.pts &lt;- matrix(c(minds, maxds, -1, maxds, minds, -1),
                byrow=TRUE, ncol=3)

pred.res &lt;- UtilOptimRegress(a1~., train, test, type = "util", strat = "interpol",
                             strat.parms=list(method = "splines"),
                             control.parms = control.parms,
                             m.pts = m.pts, minds = minds, maxds = maxds)

# check the predictions
plot(test$a1, pred.res$optim)
# assess the performance
eval.util &lt;- EvalRegressMetrics(test$a1, pred.res$optim, pred.res$utilRes,
                                thr=0.8, control.parms = control.parms)
#
# train a normal model
model &lt;- randomForest(a1~.,train)
normal.preds &lt;- predict(model, test)

#obtain the utility of the new points (trues, preds)
NormalUtil &lt;- UtilInterpol(test$medv, normal.preds, type = "util", 
                           control.parms = control.parms,
                           minds, maxds, m.pts, method="splines")
#check the performance
eval.normal &lt;- EvalRegressMetrics(test$medv, normal.preds, NormalUtil,
                                  thr=0.8, control.parms = control.parms)

eval.util
eval.normal

# observe the utility surface with the normal preds
UtilInterpol(test$a1, normal.preds, type="util", control.parms = control.parms,
             minds, maxds, m.pts, method="splines", visual=TRUE) 
# add the optim preds
points(test$a1, pred.res$optim, col="green")
}

# Example using a cost surface:
data(Boston, package = "MASS")

tgt &lt;- which(colnames(Boston) == "medv")
sp &lt;- sample(1:nrow(Boston), as.integer(0.7*nrow(Boston)))
train &lt;- Boston[sp,]
test &lt;- Boston[-sp,]

# if using interpolation methods for COST surface, the control.parms can be set to NULL
# the boundaries of the domain considered
minds &lt;- min(train[,tgt])
maxds &lt;- max(train[,tgt])

# build m.pts to include at least (minds, maxds) and (maxds, minds) points
m.pts &lt;- matrix(c(minds, maxds, 5, maxds, minds, 20),
                byrow=TRUE, ncol=3)

pred.res &lt;- UtilOptimRegress(medv~., train, test, type = "cost", strat = "interpol",
                             strat.parms = list(method = "bilinear"),
                             control.parms = NULL,
                             m.pts = m.pts, minds = minds, maxds = maxds)

# check the predictions
plot(test$medv, pred.res$optim)

# assess the performance
eval.util &lt;- EvalRegressMetrics(test$medv, pred.res$optim, pred.res$utilRes,
                                type="cost", maxC = 20)
#
# train a normal model
model &lt;- randomForest(medv~.,train)
normal.preds &lt;- predict(model, test)

#obtain the utility of the new points (trues, preds)
NormalUtil &lt;- UtilInterpol(test$medv, normal.preds, type="cost", control.parms = NULL,
                           minds, maxds, m.pts, method="bilinear")
#check the performance
eval.normal &lt;- EvalRegressMetrics(test$medv, normal.preds, NormalUtil,
                                  type="cost", maxC = 20)

eval.normal
eval.util

# check visually the surface and the predictions
UtilInterpol(test$medv, normal.preds, type="cost", control.parms = NULL,
                           minds, maxds, m.pts, method="bilinear",
                           visual=TRUE)

points(test$medv, pred.res$optim, col="blue")


## End(Not run)
</code></pre>

<hr>
<h2 id='WERCSClassif'>
WEighted Relevance-based Combination Strategy (WERCS) algorithm for imbalanced classification problems
</h2><span id='topic+WERCSClassif'></span>

<h3>Description</h3>

<p>This function handles imbalanced classification problems using the importance/relevance provided to re-sample the data set. The relevance is used to introduce replicas of the most important examples and to remove the least important examples. This function combines random over-sampling with random under-sampling which are applied in the problem classes according to the corresponding relevance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>WERCSClassif(form, dat, C.perc = "balance")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="WERCSClassif_+3A_form">form</code></td>
<td>

<p>A formula describing the prediction problem
</p>
</td></tr>
<tr><td><code id="WERCSClassif_+3A_dat">dat</code></td>
<td>

<p>A data frame containing the original (unbalanced) data set
</p>
</td></tr>
<tr><td><code id="WERCSClassif_+3A_c.perc">C.perc</code></td>
<td>

<p>A list containing the percentage(s) of random under- or 
over-sampling to apply to each class. The over-sampling percentage is a number above 1 while the under-sampling percentage should be a number below 1. If the number 1 is provided for a given class then that class remains unchanged. Alternatively it may be &quot;balance&quot; (the default) or &quot;extreme&quot;,
cases where the sampling percentages are automatically estimated. 
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a data frame with
the new data set resulting from the application of the importance sampling strategy. 
</p>


<h3>Author(s)</h3>

<p> Paula Branco <a href="mailto:paobranco@gmail.com">paobranco@gmail.com</a>, Rita Ribeiro
<a href="mailto:rpribeiro@dcc.fc.up.pt">rpribeiro@dcc.fc.up.pt</a> and Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>See Also</h3>

<p><code><a href="#topic+RandUnderClassif">RandUnderClassif</a>, <a href="#topic+RandOverClassif">RandOverClassif</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(iris)
  # generating an artificially imbalanced data set
  ir &lt;- iris[-c(51:70,111:150), ]
  IS.ext &lt;-WERCSClassif(Species~., ir, C.perc = "extreme")
  IS.bal &lt;-WERCSClassif(Species~., ir, C.perc = "balance")
  myIS &lt;-WERCSClassif(Species~., ir, C.perc = list(setosa = 0.2,
                                                    versicolor = 2,
                                                    virginica = 6))
  # check the results
  table(ir$Species)
  table(IS.ext$Species)
  table(IS.bal$Species)
  table(myIS$Species)
</code></pre>

<hr>
<h2 id='WERCSRegress'>
WEighted Relevance-based Combination Strategy (WERCS) algorithm for imbalanced regression problems
</h2><span id='topic+WERCSRegress'></span>

<h3>Description</h3>

<p>This function handles imbalanced regression problems using the relevance function provided to re-sample the data set. The relevance function is used to introduce replicas of the most important examples and to remove the least important examples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>WERCSRegress(form, dat, rel = "auto", thr.rel = NA, 
               C.perc = "balance", O = 0.5, U = 0.5)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="WERCSRegress_+3A_form">form</code></td>
<td>

<p>A formula describing the prediction problem
</p>
</td></tr>
<tr><td><code id="WERCSRegress_+3A_dat">dat</code></td>
<td>

<p>A data frame containing the original (unbalanced) data set
</p>
</td></tr>
<tr><td><code id="WERCSRegress_+3A_rel">rel</code></td>
<td>

<p>The relevance function which can be automatically (&quot;auto&quot;) determined (the default) or may be provided by the user through a matrix with interpolating points.
</p>
</td></tr>
<tr><td><code id="WERCSRegress_+3A_thr.rel">thr.rel</code></td>
<td>

<p>The default is NA which means that no threshold is used when performing the over/under-sampling. In this case, the over-sampling is performed by assigning a higher probability for selecting an example to the examples with higher relevance. On the other hand, the under-sampling is performed by removing the examples with less relevance. The user may chose a number between 0 and 1 indicating the relevance threshold above which a case is considered as belonging to the rare &quot;class&quot;.  
</p>
</td></tr>
<tr><td><code id="WERCSRegress_+3A_c.perc">C.perc</code></td>
<td>

<p>A list containing the percentage(s) of under- or/and 
over-sampling to apply to each &quot;class&quot; obtained with the threshold. This parameter is only used when a relevance threshold (thr.rel) is set. Otherwise it is ignored. The <code>C.perc</code> values should be provided in ascending order of target variable values. The over-sampling percentage(s) must be numbers higher than 1 and represent the increase applied to the examples of the bump. The under-sampling percentage(s) should be numbers below 1 and represent the decrease applyed in the corresponding bump. If the value of 1 is provided for a given bump, then the examples in that bump will remain unchanged. Alternatively, this parameter may be set to &quot;balance&quot; (the default) or &quot;extreme&quot;, cases where the sampling percentages are automatically estimated. 
</p>
</td></tr>
<tr><td><code id="WERCSRegress_+3A_o">O</code></td>
<td>

<p>A number expressing the importance given to over-sampling when the 
thr.rel parameter is NA. When O increases the number of examples to
include during the over-sampling step also increases. Default to 0.5.
</p>
</td></tr>
<tr><td><code id="WERCSRegress_+3A_u">U</code></td>
<td>

<p>A number expressing the importance given to under-sampling when the 
thr.rel parameter is NA. When U increases, the number of examples selected
during the under-sampling step also increases. Defaults to 0.5.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a data frame with
the new data set resulting from the application of the importance sampling strategy. 
</p>


<h3>Author(s)</h3>

<p> Paula Branco <a href="mailto:paobranco@gmail.com">paobranco@gmail.com</a>, Rita Ribeiro
<a href="mailto:rpribeiro@dcc.fc.up.pt">rpribeiro@dcc.fc.up.pt</a> and Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> </p>


<h3>See Also</h3>

<p><code><a href="#topic+RandUnderRegress">RandUnderRegress</a>, <a href="#topic+RandOverRegress">RandOverRegress</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  if (requireNamespace("DMwR2", quietly = TRUE)) {
  data(algae, package ="DMwR2")
  clean.algae &lt;- data.frame(algae[complete.cases(algae), ])
  # defining a threshold on the relevance
  IS.ext &lt;-WERCSRegress(a7~., clean.algae, rel = "auto", 
                          thr.rel = 0.7, C.perc = "extreme")
  IS.bal &lt;-WERCSRegress(a7~., clean.algae, rel = "auto", thr.rel = 0.7,
                          C.perc = "balance")
  myIS &lt;-WERCSRegress(a7~., clean.algae, rel = "auto", thr.rel = 0.7,
                        C.perc = list(0.2, 6))
  # neither threshold nor C.perc defined
  IS.auto &lt;- WERCSRegress(a7~., clean.algae, rel = "auto")
  }

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
