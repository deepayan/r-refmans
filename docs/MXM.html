<!DOCTYPE html><html><head><title>Help for package MXM</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {MXM}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#Ancestors and descendants of a node in a directed graph'>
<p>Returns and plots, if asked, the descendants or ancestors of one or all node(s) (or variable(s))</p></a></li>
<li><a href='#Backward phase of MMPC'>
<p>Backward phase of MMPC</p></a></li>
<li><a href='#Backward selection regression'>
<p>Variable selection in regression models with backward selection</p></a></li>
<li><a href='#Backward selection regression for GLMM'>
<p>Backward selection regression for GLMM</p></a></li>
<li><a href='#Backward selection regression for GLMM using the eBIC'>
<p>Backward selection regression for GLMM using the eBIC</p></a></li>
<li><a href='#Backward selection regression using the eBIC'>
<p>Backward selection regression using the eBIC</p></a></li>
<li><a href='#Backward selection with generalised linear regression models'>
<p>Variable selection in generalised linear regression models with backward selection</p></a></li>
<li><a href='#Bayesian Network construction using a hybrid of MMPC and PC'>
<p>Bayesian Network construction using a hybrid of MMPC and PC</p></a></li>
<li><a href='#Beta regression'>
<p>Beta regression</p></a></li>
<li><a href='#BIC based forward selection'>
<p>Variable selection in regression models with forward selection using BIC</p></a></li>
<li><a href='#BIC based forward selection with generalised linear models'>
<p>Variable selection in generalised linear models with forward selection based on BIC</p>
</p></a></li>
<li><a href='#Bootstrap bias correction for the performance of the cross-validation procedure'>
<p>Bootstrap bias correction for the performance of the cross-validation procedure</p></a></li>
<li><a href='#Calculation of the constant and slope for each subject over time'>
<p>Calculation of the constant and slope for each subject over time</p></a></li>
<li><a href='#Certificate of exclusion from the selected variables set using SES or MMPC'>
<p>Certificate of exclusion from the selected variables set using SES or MMPC</p></a></li>
<li><a href='#Check Markov equivalence of two DAGs'>
<p>Check Markov equivalence of two DAGs</p></a></li>
<li><a href='#Check whether a directed graph is acyclic'>
<p>Check whether a directed graph is acyclic</p></a></li>
<li><a href='#CondInditional independence tests'><p> MXM Conditional independence tests</p></a></li>
<li><a href='#Conditional independence regression based tests'>
<p>Conditional independence regression based tests</p></a></li>
<li><a href='#Conditional independence test for binary, categorical or ordinal data'>
<p>Conditional independence test for binary, categorical or ordinal class variables</p></a></li>
<li><a href='#Conditional independence test for case control data'>
<p>Conditional independence test based on conditional logistic regression for case control studies</p></a></li>
<li><a href='#Conditional independence test for circular data'>
<p>Circular regression conditional independence test for circular class dependent variables and continuous predictors.</p></a></li>
<li><a href='#Conditional independence test for longitudinal and clustered data using GEE'>
<p>Linear mixed models conditional independence test for longitudinal class variables</p></a></li>
<li><a href='#Conditional independence test for longitudinal and clustered data using GLMM'>
<p>Linear mixed models conditional independence test for longitudinal class variables</p></a></li>
<li><a href='#Conditional independence test for proportions/percentages'>
<p>Beta regression conditional independence test for proportions/percentage class dependent variables and mixed predictors</p></a></li>
<li><a href='#Conditional independence test for the static-longitudinal scenario'>
<p>Conditional independence test for the static-longitudinal scenario</p></a></li>
<li><a href='#Conditional independence tests counting the number of times a possible collider d-separates two nodes'>
<p>Many conditional independence tests counting the number of times a possible collider d-separates two nodes</p></a></li>
<li><a href='#Conditional independence tests for continous univariate and multivariate data '>
<p>Linear (and non-linear) regression conditional independence test for continous univariate and multivariate response variables</p></a></li>
<li><a href='#Conditional independence tests for count data '>
<p>Regression conditional independence test for discrete (counts) class dependent variables</p></a></li>
<li><a href='#Conditional independence tests for left censored data '>
<p>Conditional independence test for survival data</p></a></li>
<li><a href='#Conditional independence tests for positive data'>
<p>Regression conditional independence test for positive response variables.</p></a></li>
<li><a href='#Conditional independence tests for sucess rates'>
<p>Binomial regression conditional independence test for success rates (binomial)</p></a></li>
<li><a href='#Conditional independence tests for survival data '>
<p>Conditional independence test for survival data</p></a></li>
<li><a href='#Conditional independence tests with and without permutation p-value'>
<p>Conditional independence test for continuous class variables with and without permutation based p-value</p></a></li>
<li><a href='#Constraint based feature selection algorithms'>
<p>SES: Feature selection algorithm for identifying multiple minimal, statistically-equivalent and equally-predictive feature signatures</p>
</p>
<p>MMPC: Feature selection algorithm for identifying minimal feature subsets</p></a></li>
<li><a href='#Constraint based feature selection algorithms for longitudinal and clustered data'>
<p>SES.glmm/SES.gee: Feature selection algorithm for identifying multiple minimal, statistically-equivalent and equally-predictive feature signatures with correlated data</p></a></li>
<li><a href='#Constraint based feature selection algorithms for multiple datasets'>
<p>ma.ses: Feature selection algorithm for identifying multiple minimal, statistically-equivalent and equally-predictive feature signatures with multiple datasets</p>
</p>
<p>ma.mmpc: Feature selection algorithm for identifying minimal feature subsets with multiple datasets</p></a></li>
<li><a href='#Correlation based conditonal independence tests'>
<p>Fisher and Spearman conditional independence test for continuous class variables</p></a></li>
<li><a href='#Cross-Validation for gOMP'>
<p>Cross-Validation for gOMP</p></a></li>
<li><a href='#Cross-validation for ridge regression'>
<p>Cross validation for the ridge regression</p></a></li>
<li><a href='#Cross-Validation for SES and MMPC'>
<p>Cross-Validation for SES and MMPC</p></a></li>
<li><a href='#Cross-validation of the FBED with LMM'>
<p>Cross-validation of the FBED with LMM</p></a></li>
<li><a href='#Data simulation from a DAG'>
<p>Data simulation from a DAG.</p></a></li>
<li><a href='#Drop all possible single terms from a model using the partial correlation'>
<p>Drop all possible single terms from a model using the partial correlation</p></a></li>
<li><a href='#eBIC for many regression models'>
<p>eBIC for many regression models</p></a></li>
<li><a href='#Effective sample size for G^2 test in BNs with case control data'>
<p>Effective sample size for G^2 test in BNs with case control data</p></a></li>
<li><a href='#Estimation of the percentage of Null p-values'>
<p>Estimation of the percentage of Null p-values</p></a></li>
<li><a href='#Fast MMPC'>
<p>A fast version of MMPC</p></a></li>
<li><a href='#Fast MMPC for longitudinal and clustered data'>
<p>mmpc.glmm2/mmpc.gee2: Fast Feature selection algorithm for identifying minimal feature subsets with correlated data</p></a></li>
<li><a href='#Feature selection using SES and MMPC for classifiication with longitudinal data'>
<p>Feature selection using SES and MMPC for classifiication with longitudinal data</p></a></li>
<li><a href='#Fit a mixture of beta distributions in p-values'>
<p>Fit a mixture of beta distributions in p-values</p></a></li>
<li><a href='#Forward Backward Early Dropping selection regression'>
<p>Forward Backward Early Dropping selection regression</p></a></li>
<li><a href='#Forward Backward Early Dropping selection regression for big data'>
<p>Forward Backward Early Dropping selection regression for big data</p></a></li>
<li><a href='#Forward Backward Early Dropping selection regression with GEE'>
<p>Forward Backward Early Dropping selection regression with GEE</p></a></li>
<li><a href='#Forward Backward Early Dropping selection regression with GLMM'>
<p>Forward Backward Early Dropping selection regression with GLMM</p></a></li>
<li><a href='#Forward selection regression'>
<p>Variable selection in regression models with forward selection</p></a></li>
<li><a href='#Forward selection with generalised linear regression models'>
<p>Variable selection in generalised linear regression models with forward selection</p></a></li>
<li><a href='#Forward selection with linear regression models'>
<p>Variable selection in linear regression models with forward selection</p>
</p></a></li>
<li><a href='#G-square conditional independence test for discrete data'>
<p>G-square conditional independence test for discrete data</p></a></li>
<li><a href='#Generalised linear mixed models based on glmm SES and MMPC outputs'>
<p>Generalised linear mixed model(s) based obtained from glmm SES or MMPC</p></a></li>
<li><a href='#Generalised ordinal regression'>
<p>Generalised ordinal regression</p></a></li>
<li><a href='#Generate random folds for cross-validation'>
<p>Generate random folds for cross-validation</p></a></li>
<li><a href='#Generic orthogonal matching pursuit (gOMP)'>
<p>Generic orthogonal matching pursuit (gOMP)</p></a></li>
<li><a href='#Generic orthogonal matching pursuit(gOMP) for big data'>
<p>Generic orthogonal matching pursuit(gOMP) for big data</p></a></li>
<li><a href='#Graph of unconditional associations'>
<p>Graph of unconditional associations</p></a></li>
<li><a href='#IAMB backward selection phase'>
<p>IAMB backward selection phase</p></a></li>
<li><a href='#IAMB variable selection'>
<p>IAMB variable selection</p></a></li>
<li><a href='#Incremental BIC values and final regression model of the FBED algorithm'>
<p>Incremental BIC values and final regression model of the FBED algorithm</p></a></li>
<li><a href='#Interactive plot of an (un)directed graph'>
<p>Interactive plot of an (un)directed graph</p></a></li>
<li><a href='#Lower limit of the confidence of an edge'>
<p>Lower limit of the confidence of an edge</p></a></li>
<li><a href='#mammpc.output-class'><p>Class <code>"mammpc.output"</code></p></a></li>
<li><a href='#Many approximate simple logistic regressions'>
<p>Many approximate simple logistic regressions.</p></a></li>
<li><a href='#Many simple beta regressions'>
<p>Many simple beta regressions.</p></a></li>
<li><a href='#Many simple quantile regressions using logistic regressions'>
<p>Many simple quantile regressions using logistic regressions.</p></a></li>
<li><a href='#Many simple zero inflated Poisson regressions'>
<p>Many simple zero inflated Poisson regressions.</p></a></li>
<li><a href='#Many Wald based tests for logistic and Poisson regressions with continuous predictors'>
<p>Many Wald based tests for logistic and Poisson regressions with continuous predictors</p></a></li>
<li><a href='#Markov Blanket of a node in a directed graph'>
<p>Returns the Markov blanket of a node (or variable)</p></a></li>
<li><a href='#mases.output-class'><p>Class <code>"mases.output"</code></p></a></li>
<li><a href='#MMPC solution paths for many combinations of hyper-parameters'>
<p>MMPC solution paths for many combinations of hyper-parameters</p></a></li>
<li><a href='#MMPC.gee.output-class'><p>Class <code>"MMPC.gee.output"</code></p></a></li>
<li><a href='#MMPC.glmm.output-class'><p>Class <code>"MMPC.glmm.output"</code></p></a></li>
<li><a href='#MMPCoutput-class'><p>Class <code>"MMPCoutput"</code></p></a></li>
<li><a href='#MXM-internal'><p> Internal MXM Functions</p></a></li>
<li><a href='#MXM-package'>
<p>This is an R package that currently implements feature selection methods for identifying minimal,</p>
statistically-equivalent and equally-predictive feature subsets. Additionally, the package includes two algorithms for
constructing the skeleton of a Bayesian network.</a></li>
<li><a href='#Neighbours of nodes in an undirected graph'>
<p>Returns the node(s) and their neighbour(s), if there are any.</p></a></li>
<li><a href='#Network construction using the partial correlation based forward regression or FBED'>
<p>Network construction using the partial correlation based forward regression of FBED</p></a></li>
<li><a href='#Orientation rules for the PC algorithm'>
<p>The orientations part of the PC algorithm.</p></a></li>
<li><a href='#Partial correlation between two variables'>
<p>Partial correlation</p></a></li>
<li><a href='#Permutation based p-value for the Pearson correlation coefficient'>
<p>Permutation based p-value for the Pearson correlation coefficient</p></a></li>
<li><a href='#Plot of longitudinal data'>
<p>Plot of longitudinal data</p></a></li>
<li><a href='#Probability residual of ordinal logistic regreession'>
<p>Probability residual of ordinal logistic regreession</p></a></li>
<li><a href='#Read big data or a big.matrix object'>
<p>Read big data or a big.matrix object</p></a></li>
<li><a href='#Regression modeler'>
<p>Generic regression modelling function</p></a></li>
<li><a href='#Regression models based on SES and MMPC outputs'>
<p>Regression model(s) obtained from SES or MMPC</p></a></li>
<li><a href='#Regression models based on SES.timeclass and MMPC.timeclass outputs'>
<p>Regression model(s) obtained from SES.timeclass or MMPC.timeclass</p></a></li>
<li><a href='#Regression models fitting'>
<p>Regression modelling</p></a></li>
<li><a href='#Ridge regression'>
<p>Ridge regression</p></a></li>
<li><a href='#Ridge regression coefficients plot'>
<p>Ridge regression</p></a></li>
<li><a href='#ROC and AUC'>
<p>ROC and AUC</p></a></li>
<li><a href='#Search for triangles in an undirected graph'>
<p>Search for triangles in an undirected graph</p></a></li>
<li><a href='#SES.gee.output-class'><p>Class <code>"SES.gee.output"</code></p></a></li>
<li><a href='#SES.glmm.output-class'><p>Class <code>"SES.glmm.output"</code></p></a></li>
<li><a href='#SESoutput-class'><p>Class <code>"SESoutput"</code></p></a></li>
<li><a href='#Skeleton (local) around a node of the max-min hill-climbing (MMHC) algorithm'>
<p>Skeleton (local) around a node of the MMHC algorithm</p></a></li>
<li><a href='#Skeleton of the max-min hill-climbing (MMHC) algorithm'>
<p>The skeleton of a Bayesian network as produced by MMHC</p></a></li>
<li><a href='#Skeleton of the PC algorithm'>
<p>The skeleton of a Bayesian network produced by the PC algorithm</p></a></li>
<li><a href='#Structural Hamming distance between two partially oriented DAGs'>
<p>Structural Hamming distance between two partially oriented DAGs</p></a></li>
<li><a href='#Supervised PCA'>
<p>Supervised PCA</p></a></li>
<li><a href='#Symmetric conditional independence test with clustered data'>
<p>Symmetric conditional independence test with clustered data</p></a></li>
<li><a href='#Symmetric conditional independence test with mixed data'>
<p>Symmetric conditional independence test with mixed data</p></a></li>
<li><a href='#The max-min Markov blanket algorithm'>
<p>Max-min Markov blanket algorithm</p></a></li>
<li><a href='#Topological sort of a DAG'>
<p>Topological sort of a DAG</p></a></li>
<li><a href='#Total causal effect of a node on another node'>
<p>Total causal effect of a node on another node</p></a></li>
<li><a href='#Transformation of a DAG into an essential graph'>
<p>Transforms a DAG into an essential graph</p></a></li>
<li><a href='#Transitive closure of an adjacency matrix'>
<p>Returns the transitive closure of an adjacency matrix</p></a></li>
<li><a href='#Undirected path(s) between two nodes'>
<p>Undirected path(s) between two nodes</p></a></li>
<li><a href='#Univariate regression based tests'>
<p>Univariate regression based tests</p></a></li>
<li><a href='#Utilities for the skeleton of a (Bayesian) Network'>
<p>Utilities for the skeleton of a (Bayesian) Network</p></a></li>
<li><a href='#Variable selection using the PC-simple algorithm'><p>Variable selection using the PC-simple algorithm</p></a></li>
<li><a href='#Zero inflated Poisson and negative binomial regression'>
<p>Zero inflated Poisson and negative binomial regression</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Feature Selection (Including Multiple Solutions) and Bayesian
Networks</td>
</tr>
<tr>
<td>Version:</td>
<td>1.5.5</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="http://mensxmachina.org">http://mensxmachina.org</a></td>
</tr>
<tr>
<td>Date:</td>
<td>2022-08-24</td>
</tr>
<tr>
<td>Author:</td>
<td>Konstantina Biza [aut, cre],
  Ioannis Tsamardinos [aut, cph],
  Vincenzo Lagani [aut, cph],
  Giorgos Athineou [aut],
  Michail Tsagris [aut],
  Giorgos Borboudakis [ctb],
  Anna Roumpelaki [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Konstantina Biza &lt;kbiza@csd.uoc.gr&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Many feature selection methods for a wide range of response variables, including minimal, statistically-equivalent and equally-predictive feature subsets. Bayesian network algorithms and related functions are also included. The package name 'MXM' stands for "Mens eX Machina", meaning "Mind from the Machine" in Latin. References: a) Lagani, V. and Athineou, G. and Farcomeni, A. and Tsagris, M. and Tsamardinos, I. (2017). Feature Selection with the R Package MXM: Discovering Statistically Equivalent Feature Subsets. Journal of Statistical Software, 80(7). &lt;<a href="https://doi.org/10.18637%2Fjss.v080.i07">doi:10.18637/jss.v080.i07</a>&gt;. b) Tsagris, M., Lagani, V. and Tsamardinos, I. (2018). Feature selection for high-dimensional temporal data. BMC Bioinformatics, 19:17. &lt;<a href="https://doi.org/10.1186%2Fs12859-018-2023-7">doi:10.1186/s12859-018-2023-7</a>&gt;. c) Tsagris, M., Borboudakis, G., Lagani, V. and Tsamardinos, I. (2018). Constraint-based causal discovery with mixed data. International Journal of Data Science and Analytics, 6(1): 19-30. &lt;<a href="https://doi.org/10.1007%2Fs41060-018-0097-y">doi:10.1007/s41060-018-0097-y</a>&gt;. d) Tsagris, M., Papadovasilakis, Z., Lakiotaki, K. and Tsamardinos, I. (2018). Efficient feature selection on gene expression data: Which algorithm to use? BioRxiv. &lt;<a href="https://doi.org/10.1101%2F431734">doi:10.1101/431734</a>&gt;. e) Tsagris, M. (2019). Bayesian Network Learning with the PC Algorithm: An Improved and Correct Variation. Applied Artificial Intelligence, 33(2):101-123. &lt;<a href="https://doi.org/10.1080%2F08839514.2018.1526760">doi:10.1080/08839514.2018.1526760</a>&gt;. f) Tsagris, M. and Tsamardinos, I. (2019). Feature selection with the R package MXM. F1000Research 7: 1505. &lt;<a href="https://doi.org/10.12688%2Ff1000research.16216.2">doi:10.12688/f1000research.16216.2</a>&gt;. g) Borboudakis, G. and Tsamardinos, I. (2019). Forward-Backward Selection with Early Dropping. Journal of Machine Learning Research 20: 1-39. h) The gamma-OMP algorithm for feature selection with application to gene expression data. IEEE/ACM Transactions on Computational Biology and Bioinformatics 19(2): 1214-1224. &lt;<a href="https://doi.org/10.1109%2FTCBB.2020.3029952">doi:10.1109/TCBB.2020.3029952</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>markdown, R.rsp</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr, R.rsp</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods, stats, utils, survival, MASS, graphics, ordinal,
nnet, quantreg, lme4, foreach, doParallel, parallel, relations,
Rfast, visNetwork, energy, geepack, knitr, dplyr, bigmemory,
coxme, Rfast2, Hmisc</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-08-24 10:55:48 UTC; Michail</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-08-25 08:52:40 UTC</td>
</tr>
</table>
<hr>
<h2 id='Ancestors+20and+20descendants+20of+20a+20node+20in+20a+20directed+20graph'>
Returns and plots, if asked, the descendants or ancestors of one or all node(s) (or variable(s))
</h2><span id='topic+findDescendants'></span><span id='topic+findAncestors'></span>

<h3>Description</h3>

<p>Returns and plots, if asked, the descendants or ancestors of one or all node(s) (or variable(s))
</p>


<h3>Usage</h3>

<pre><code class='language-R'>findDescendants(G, node = NULL, graph = FALSE)
findAncestors(G, node = NULL, graph = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Ancestors+2B20and+2B20descendants+2B20of+2B20a+2B20node+2B20in+2B20a+2B20directed+2B20graph_+3A_g">G</code></td>
<td>

<p>The graph matrix as produced from <code><a href="#topic+pc.or">pc.or</a></code> or any other algorithm which produces directed graphs. 
</p>
</td></tr>
<tr><td><code id="Ancestors+2B20and+2B20descendants+2B20of+2B20a+2B20node+2B20in+2B20a+2B20directed+2B20graph_+3A_node">node</code></td>
<td>

<p>A numerical value indicating the node (or variable) whose descendants are to be returned. 
</p>
</td></tr>
<tr><td><code id="Ancestors+2B20and+2B20descendants+2B20of+2B20a+2B20node+2B20in+2B20a+2B20directed+2B20graph_+3A_graph">graph</code></td>
<td>

<p>A boolean variable. If TRUE the relevant graph will appear (if there are descendants). 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The functions searches for the descendants of some node. This is an S3 class output. 
</p>


<h3>Value</h3>

<table>
<tr><td><code>isAnc</code></td>
<td>

<p>A matrix of the same dimensions as the original graph matrix with 0s and 1s. isAnc[i, j] = 1 indicates that the i-th node is an ancestor of the j-th node. 
If the argument &quot;node&quot; is NULL, only this matrix will be returned. 
</p>
</td></tr>
<tr><td><code>Ganc</code></td>
<td>

<p>A matrix of dimensions equal to the number of descendants of the node with 0s and 1s, if the argument &quot;node&quot; is not NULL.  
</p>
</td></tr>
<tr><td><code>anc</code></td>
<td>

<p>The descendants of the node if the argument &quot;node&quot; is not NULL.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Anna Roumpelaki
</p>
<p>R implementation and documentation: Anna Roumpelaki &lt;anna.roumpelaki@gmail.com&gt;
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+plotnetwork">plotnetwork</a>, <a href="#topic+nei">nei</a>, <a href="#topic+mb">mb</a>, <a href="#topic+pc.or">pc.or</a> 
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate a dataset with continuous data
y = rdag(1000, 10, 0.3)
tru = y$G 
x = y$x
mod = pc.con(x)
G = pc.or(mod)$G
plotnetwork(G)
findDescendants(G, 4, graph = FALSE)
findAncestors(G, 4, graph = FALSE)
findAncestors(G)
</code></pre>

<hr>
<h2 id='Backward+20phase+20of+20MMPC'>
Backward phase of MMPC
</h2><span id='topic+mmpcbackphase'></span>

<h3>Description</h3>

<p>Backward phase of MMPC.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mmpcbackphase(target, dataset, max_k = 3, threshold = 0.05, test = NULL,
wei = NULL, R = 1) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Backward+2B20phase+2B20of+2B20MMPC_+3A_target">target</code></td>
<td>

<p>The class variable. Provide either a string, an integer, a numeric value, a vector, a factor, an ordered factor 
or a Surv object. See also Details.
</p>
</td></tr>
<tr><td><code id="Backward+2B20phase+2B20of+2B20MMPC_+3A_dataset">dataset</code></td>
<td>

<p>The data-set; provide either a data frame or a matrix (columns = variables , rows = samples).
Alternatively, provide an ExpressionSet (in which case rows are samples and columns are features, 
see bioconductor for details).
</p>
</td></tr>
<tr><td><code id="Backward+2B20phase+2B20of+2B20MMPC_+3A_max_k">max_k</code></td>
<td>

<p>The maximum conditioning set to use in the conditional indepedence test (see Details). Integer, default value is 3.
</p>
</td></tr>
<tr><td><code id="Backward+2B20phase+2B20of+2B20MMPC_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0, 1)) for assessing p-values significance. Default value is 0.05.
</p>
</td></tr>
<tr><td><code id="Backward+2B20phase+2B20of+2B20MMPC_+3A_test">test</code></td>
<td>

<p>The conditional independence test to use. Type the test without &quot; &quot;, e.g. type testIndFisher, Not &quot;testIndFisher&quot;. 
Default value is NULL. See also <code><a href="#topic+CondIndTests">CondIndTests</a></code>.
</p>
</td></tr>
<tr><td><code id="Backward+2B20phase+2B20of+2B20MMPC_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL.
</p>
</td></tr>










<tr><td><code id="Backward+2B20phase+2B20of+2B20MMPC_+3A_r">R</code></td>
<td>

<p>The number of permutations, set to 1 by default (no permutations based test). There is a trick to avoind doing 
all permutations. As soon as the number of times the permuted test statistic is more than the observed test 
statistic is more than 50 (if threshold = 0.05 and R = 999), the p-value has exceeded the signifiance level 
(threshold value) and hence the predictor variable is not significant. There is no need to continue do the extra 
permutations, as a decision has already been made. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For each of the selected variables (dataset) the function performs conditional independence tests where the 
conditioning sets are formed from the other variables. All possible combinations are tried until the variable 
becomes non significant. The maximum size of the conditioning set is equal to max_k. This is called in the 
<code><a href="#topic+MMPC">MMPC</a></code> when the backward phase is requested.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>met</code></td>
<td>
 
<p>A numerical vector of size equal to the number of columns of the dataset.
</p>
</td></tr>
<tr><td><code>counter</code></td>
<td>

<p>The number of tests performed.
</p>
</td></tr>
<tr><td><code>pvalues</code></td>
<td>

<p>The maximum <b>logged p-value</b> for the association of each predictor variable.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ioannis Tsamardinos, Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>  
</p>


<h3>References</h3>

<p>Tsamardinos, Brown and Aliferis (2006). The max-min hill-climbing Bayesian network structure learning algorithm. 
Machine learning, 65(1), 31-78.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+MMPC">MMPC</a>, <a href="#topic+mmhc.skel">mmhc.skel</a>, <a href="#topic+CondIndTests">CondIndTests</a>, <a href="#topic+cv.mmpc">cv.mmpc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)

#simulate a dataset with continuous data
dataset &lt;- matrix(runif(500 * 100, 1, 100), ncol = 100)

#define a simulated class variable 
target &lt;- 3 * dataset[, 10] + 2 * dataset[, 100] + 3 * dataset[, 20] + rnorm(500, 0, 5)

# MMPC algorithm 
m1 &lt;- MMPC(target, dataset, max_k = 3, threshold = 0.05, test="testIndFisher");
m2 &lt;- MMPC(target, dataset, max_k = 3, threshold = 0.05, test="testIndFisher", backward = TRUE);
x &lt;- dataset[, m1@selectedVars]
mmpcbackphase(target, x, test = testIndFisher)
</code></pre>

<hr>
<h2 id='Backward+20selection+20regression'>
Variable selection in regression models with backward selection
</h2><span id='topic+bs.reg'></span>

<h3>Description</h3>

<p>Variable selection in regression models with backward selection
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bs.reg(target, dataset, threshold = 0.05, wei = NULL, test = NULL, user_test = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Backward+2B20selection+2B20regression_+3A_target">target</code></td>
<td>

<p>The class variable. Provide either a string, an integer, a numeric value, a vector, a factor, an ordered factor or a Surv object. See also Details. For the gamma regression this must a vector with strictly positive numbers (no zeros allowed). 
</p>
</td></tr>
<tr><td><code id="Backward+2B20selection+2B20regression_+3A_dataset">dataset</code></td>
<td>

<p>The dataset; provide either a data frame or a matrix (columns = variables, rows = samples). In either case, only two cases are avaialble, either all data are continuous, or categorical. 
</p>
</td></tr>
<tr><td><code id="Backward+2B20selection+2B20regression_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0, 1)) for assessing p-values significance. Default value is 0.05.
</p>
</td></tr>
<tr><td><code id="Backward+2B20selection+2B20regression_+3A_test">test</code></td>
<td>

<p>The regression model to use. Available options are most of the tests for SES and MMPC. 
The ones NOT available are &quot;censIndER&quot;, &quot;testIndMVreg&quot;, &quot;testIndSpearman&quot;. 
If you want to use multinomial or ordinal logistic regression,  make sure your target is factor. 
See also <code><a href="#topic+SES">SES</a></code> and <code><a href="#topic+CondIndTests">CondIndTests</a></code> for the tests.
</p>
</td></tr>
<tr><td><code id="Backward+2B20selection+2B20regression_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. An example where weights are used is surveys when stratified sampling has occured. 
</p>
</td></tr>
<tr><td><code id="Backward+2B20selection+2B20regression_+3A_user_test">user_test</code></td>
<td>

<p>A user-defined conditional independence test (provide a closure type object). Default value is NULL. If this is defined, the &quot;test&quot; argument is ignored.
</p>
</td></tr>



</table>


<h3>Details</h3>

<p>If the sample size is less than the number of variables a meesage will appear and no backward regression is performed. 
</p>


<h3>Value</h3>

<p>The output of the algorithm is S3 object including:
</p>
<table>
<tr><td><code>runtime</code></td>
<td>

<p>The run time of the algorithm. A numeric vector. The first element is the user time, the second element is the system time and the third element is the elapsed time.
</p>
</td></tr>
<tr><td><code>info</code></td>
<td>

<p>A matrix with the non selected variables and their latest test statistics and <b>logged p-values</b>.
</p>
</td></tr>
<tr><td><code>mat</code></td>
<td>

<p>A matrix with the selected variables and their latest statistics and <b>logged p-values</b>.
</p>
</td></tr>
<tr><td><code>ci_test</code></td>
<td>

<p>The conditional independence test used. 
</p>
</td></tr>
<tr><td><code>final</code></td>
<td>

<p>The final regression model.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+glm.fsreg">glm.fsreg</a>, <a href="#topic+lm.fsreg">lm.fsreg</a>, <a href="#topic+bic.fsreg">bic.fsreg</a>, <a href="#topic+bic.glm.fsreg">bic.glm.fsreg</a>, <a href="#topic+CondIndTests">CondIndTests</a>, <a href="#topic+MMPC">MMPC</a>, <a href="#topic+SES">SES</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)
dataset &lt;- matrix( runif(200 * 10, 1, 100), ncol = 10 )
target &lt;- rnorm(200)
a &lt;- bs.reg(target, dataset, threshold = 0.05, test = "testIndRQ") 
b &lt;- bs.reg(target, dataset, threshold = 0.05, test = "testIndReg") 
b2 &lt;- bs.reg(target, dataset, threshold = 0.05, test = "testIndFisher") 
</code></pre>

<hr>
<h2 id='Backward+20selection+20regression+20for+20GLMM'>
Backward selection regression for GLMM
</h2><span id='topic+glmm.bsreg'></span>

<h3>Description</h3>

<p>Backward selection regression for GLMM
</p>


<h3>Usage</h3>

<pre><code class='language-R'>glmm.bsreg(target, dataset, id, threshold = 0.05, wei = NULL, test = "testIndGLMMReg") 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Backward+2B20selection+2B20regression+2B20for+2B20GLMM_+3A_target">target</code></td>
<td>

<p>The class variable. This can be a numerical vector with continuous data, binary or discrete valued data. It can also be a factor variable with two levels only.
</p>
</td></tr>
<tr><td><code id="Backward+2B20selection+2B20regression+2B20for+2B20GLMM_+3A_dataset">dataset</code></td>
<td>

<p>The dataset; provide a numerical a matrix (columns = variables, rows = samples).
</p>
</td></tr>
<tr><td><code id="Backward+2B20selection+2B20regression+2B20for+2B20GLMM_+3A_id">id</code></td>
<td>

<p>This is a numerical vector of the same size as target denoting the groups or the subjects. 
</p>
</td></tr>
<tr><td><code id="Backward+2B20selection+2B20regression+2B20for+2B20GLMM_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0, 1)) for assessing p-values significance. Default value is 0.05.
</p>
</td></tr>
<tr><td><code id="Backward+2B20selection+2B20regression+2B20for+2B20GLMM_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL.
</p>
</td></tr>
<tr><td><code id="Backward+2B20selection+2B20regression+2B20for+2B20GLMM_+3A_test">test</code></td>
<td>

<p>This is for the type of regression to be used, &quot;testIndGLMMReg&quot;, for Gaussian regression, &quot;testIndGLMMLogistic for logistic regression or &quot;testIndGLMMPois&quot; for Poisson regression.
</p>
</td></tr>



</table>


<h3>Details</h3>

<p>If the sample size is less than the number of variables a meesage will appear and no backward regression is performed. 
</p>


<h3>Value</h3>

<p>The output of the algorithm is S3 object including:
</p>
<table>
<tr><td><code>runtime</code></td>
<td>

<p>The run time of the algorithm. A numeric vector. The first element is the user time, the second element is the system time and the third element is the elapsed time.
</p>
</td></tr>
<tr><td><code>info</code></td>
<td>

<p>A matrix with the non selected variables and their latest test statistics and <b>logged p-values</b>.
</p>
</td></tr>
<tr><td><code>mat</code></td>
<td>

<p>A matrix with the selected variables and their latest statistics and <b>logged p-values</b>.
</p>
</td></tr>
<tr><td><code>final</code></td>
<td>

<p>The final regression model.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Eugene Demidenko (2013). Mixed Models: Theory and Applications with R, 2nd Edition. New Jersey: Wiley &amp; Sons.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+fbed.glmm.reg">fbed.glmm.reg</a>, <a href="#topic+ebic.glmm.bsreg">ebic.glmm.bsreg</a>, <a href="#topic+MMPC.glmm">MMPC.glmm</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
require(lme4)
data(sleepstudy)
reaction &lt;- sleepstudy$Reaction
days &lt;- sleepstudy$Days
subject &lt;- sleepstudy$Subject
x &lt;- matrix(rnorm(180 * 200),ncol = 200) ## unrelated predictor variables
m1 &lt;- glmm.bsreg(Reaction, x, subject) 
m2 &lt;- MMPC.glmm(target = reaction, group = subject, dataset = x)

## End(Not run)
</code></pre>

<hr>
<h2 id='Backward+20selection+20regression+20for+20GLMM+20using+20the+20eBIC'>
Backward selection regression for GLMM using the eBIC
</h2><span id='topic+ebic.glmm.bsreg'></span>

<h3>Description</h3>

<p>Backward selection regression for GLMM using the eBIC
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ebic.glmm.bsreg(target, dataset, id, wei = NULL, gam = NULL, test = "testIndGLMMReg") 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Backward+2B20selection+2B20regression+2B20for+2B20GLMM+2B20using+2B20the+2B20eBIC_+3A_target">target</code></td>
<td>

<p>The class variable. This can be a numerical vector with continuous data, binary or discrete valued data. It can also be a factor variable with two levels only.
</p>
</td></tr>
<tr><td><code id="Backward+2B20selection+2B20regression+2B20for+2B20GLMM+2B20using+2B20the+2B20eBIC_+3A_dataset">dataset</code></td>
<td>

<p>The dataset; provide a numerical a matrix (columns = variables, rows = samples).
</p>
</td></tr>
<tr><td><code id="Backward+2B20selection+2B20regression+2B20for+2B20GLMM+2B20using+2B20the+2B20eBIC_+3A_id">id</code></td>
<td>

<p>This is a numerical vector of the same size as target denoting the groups or the subjects. 
</p>
</td></tr>
<tr><td><code id="Backward+2B20selection+2B20regression+2B20for+2B20GLMM+2B20using+2B20the+2B20eBIC_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. An example where weights are used is surveys when stratified sampling has occured. 
</p>
</td></tr>
<tr><td><code id="Backward+2B20selection+2B20regression+2B20for+2B20GLMM+2B20using+2B20the+2B20eBIC_+3A_gam">gam</code></td>
<td>

<p>In case the method is chosen to be &quot;eBIC&quot; one can also specify the <code class="reqn">gamma</code> parameter. The default value is &quot;NULL&quot;, so that the value is automatically calculated.
</p>
</td></tr>
<tr><td><code id="Backward+2B20selection+2B20regression+2B20for+2B20GLMM+2B20using+2B20the+2B20eBIC_+3A_test">test</code></td>
<td>

<p>This is for the type of regression to be used, &quot;testIndGLMMReg&quot;, for Gaussian regression, &quot;testIndGLMMLogistic for logistic regression or &quot;testIndGLMMPois&quot; for Poisson regression.
</p>
</td></tr>



</table>


<h3>Details</h3>

<p>The algorithm is a variation of the usual forward selection. At every step, the most significant variable enters the selected variables set. 
In addition, only the significant variables stay and are further examined. The non signifcant ones are dropped. This goes until no variable 
can enter the set. The user has the option to redo this step 1 or more times (the argument K). In the end, a backward selection is performed
to remove falsely selected variables.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>runtime</code></td>
<td>

<p>The runtime required.
</p>
</td></tr>
<tr><td><code>info</code></td>
<td>

<p>A matrix with the number of variables and the number of tests performed (or models fitted) at each round (value of K). 
</p>
</td></tr>
<tr><td><code>mat</code></td>
<td>

<p>A matrix with the selected variables and their eBIC.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Borboudakis G. and Tsamardinos I. (2019). Forward-backward selection with early dropping. Journal of Machine Learning Research, 20(8): 1-39.
</p>
<p>Eugene Demidenko (2013). Mixed Models: Theory and Applications with R, 2nd Edition. New Jersey: Wiley &amp; Sons.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+fbed.glmm.reg">fbed.glmm.reg</a>, <a href="#topic+glmm.bsreg">glmm.bsreg</a>, <a href="#topic+MMPC.glmm">MMPC.glmm</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
require(lme4)
data(sleepstudy)
reaction &lt;- sleepstudy$Reaction
days &lt;- sleepstudy$Days
subject &lt;- sleepstudy$Subject
x &lt;- matrix(rnorm(180 * 20),ncol = 20) ## unrelated preidctor variables
m1 &lt;- ebic.glmm.bsreg(reaction, x, id = subject) 
m2 &lt;- MMPC.glmm(reaction, group = subject, dataset = x)

## End(Not run)
</code></pre>

<hr>
<h2 id='Backward+20selection+20regression+20using+20the+20eBIC'>
Backward selection regression using the eBIC
</h2><span id='topic+ebic.bsreg'></span>

<h3>Description</h3>

<p>Backward selection regression using the eBIC
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ebic.bsreg(target, dataset, test = NULL, wei = NULL, gam = NULL) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Backward+2B20selection+2B20regression+2B20using+2B20the+2B20eBIC_+3A_target">target</code></td>
<td>

<p>The class variable. Provide either a string, an integer, a numeric value, a vector, a factor, an ordered factor or a Surv object. 
</p>
</td></tr>
<tr><td><code id="Backward+2B20selection+2B20regression+2B20using+2B20the+2B20eBIC_+3A_dataset">dataset</code></td>
<td>

<p>The dataset; provide either a data frame or a matrix (columns = variables, rows = samples). 
</p>
</td></tr>
<tr><td><code id="Backward+2B20selection+2B20regression+2B20using+2B20the+2B20eBIC_+3A_test">test</code></td>
<td>

<p>The available tests: &quot;testIndReg&quot;, &quot;testIndPois&quot;, &quot;testIndNB&quot;, &quot;testIndLogistic&quot;, &quot;testIndMMReg&quot;, 
&quot;testIndBinom&quot;, &quot;censIndCR&quot;, &quot;censIndWR&quot;, &quot;testIndBeta&quot;, &quot;testIndZIP&quot;, &quot;testIndGamma&quot;, &quot;testIndNormLog&quot; 
and &quot;testIndTobit&quot;.
</p>
</td></tr>
<tr><td><code id="Backward+2B20selection+2B20regression+2B20using+2B20the+2B20eBIC_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. It is not suggested when testIndMMReg is used.
An example where weights are used is surveys when stratified sampling has occured.
</p>
</td></tr>
<tr><td><code id="Backward+2B20selection+2B20regression+2B20using+2B20the+2B20eBIC_+3A_gam">gam</code></td>
<td>

<p>In case the method is chosen to be &quot;eBIC&quot; one can also specify the <code class="reqn">gamma</code> parameter. The default value is &quot;NULL&quot;, so that the value is 
automatically calculated.
</p>
</td></tr>



</table>


<h3>Details</h3>

<p>The algorithm is a variation of the usual forward selection. At every step, the most significant variable enters the selected variables set. 
In addition, only the significant variables stay and are further examined. The non signifcant ones are dropped. This goes until no variable 
can enter the set. The user has the option to redo this step 1 or more times (the argument K). In the end, a backward selection is performed
to remove falsely selected variables.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>runtime</code></td>
<td>

<p>The runtime required.
</p>
</td></tr>
<tr><td><code>info</code></td>
<td>

<p>A matrix with the number of variables and the number of tests performed (or models fitted) at each round (value of K). 
</p>
</td></tr>
<tr><td><code>mat</code></td>
<td>

<p>A matrix with the selected variables and their eBIC.
</p>
</td></tr>
<tr><td><code>back.rem</code></td>
<td>

<p>The variables removed in the backward phase.
</p>
</td></tr>
<tr><td><code>back.n.tests</code></td>
<td>

<p>The number of models fitted in the backward phase.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Borboudakis G. and Tsamardinos I. (2019). Forward-backward selection with early dropping. Journal of Machine Learning Research, 20(8): 1-39.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+fs.reg">fs.reg</a>, <a href="#topic+bic.fsreg">bic.fsreg</a>, <a href="#topic+MMPC">MMPC</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dataset &lt;- matrix( runif(100 * 15, 1, 100), ncol = 15 )
target &lt;- rt(100, 10)
a1 &lt;- ebic.bsreg(target, dataset, test = "testIndReg") 
target &lt;- rpois(100, 10)
a2 &lt;- ebic.bsreg(target, dataset, test = "testIndPois") 
</code></pre>

<hr>
<h2 id='Backward+20selection+20with+20generalised+20linear+20regression+20models'>
Variable selection in generalised linear regression models with backward selection
</h2><span id='topic+glm.bsreg'></span><span id='topic+glm.bsreg2'></span>

<h3>Description</h3>

<p>Variable selection in generalised linear regression models with backward selection
</p>


<h3>Usage</h3>

<pre><code class='language-R'>glm.bsreg(target, dataset, threshold = 0.05, wei = NULL, test = NULL)
glm.bsreg2(target, dataset, threshold = 0.05, wei = NULL, test = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Backward+2B20selection+2B20with+2B20generalised+2B20linear+2B20regression+2B20models_+3A_target">target</code></td>
<td>

<p>The class variable. Provide either an integer, a numeric value, or a factor. It can also be a matrix with two columns for the case of binomial regression. In this case, the first column is the nubmer of successes and the second column is the number of trials. See also the Details.
</p>
</td></tr>
<tr><td><code id="Backward+2B20selection+2B20with+2B20generalised+2B20linear+2B20regression+2B20models_+3A_dataset">dataset</code></td>
<td>

<p>The dataset; provide either a data frame or a matrix (columns = variables, rows = observations). In either case, only two cases are avaialble, either all data are continuous, or categorical. 
</p>
</td></tr>
<tr><td><code id="Backward+2B20selection+2B20with+2B20generalised+2B20linear+2B20regression+2B20models_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0, 1)) for assessing p-values significance. Default value is 0.05.
</p>
</td></tr>
<tr><td><code id="Backward+2B20selection+2B20with+2B20generalised+2B20linear+2B20regression+2B20models_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. An example where weights are used is surveys when stratified sampling has occured.
</p>
</td></tr>
<tr><td><code id="Backward+2B20selection+2B20with+2B20generalised+2B20linear+2B20regression+2B20models_+3A_test">test</code></td>
<td>

<p>For &quot;glm.bsreg&quot; this can be &quot;testIndLogistic&quot;, &quot;testIndPois&quot;, &quot;testIndBinom&quot;, testIndReg&quot; or &quot;testIndMMReg&quot;.
For &quot;glm.bsreg2&quot; this can be &quot;testIndGamma&quot;, &quot;testIndNormLog&quot;, &quot;testIndQPois&quot; or &quot;testIndQBinom&quot;.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This functions currently implements only linear, binomial, binary logistic and Poisson regression. If the sample size is less than the number of variables a meesage will appear and no backward regression is performed. 
</p>


<h3>Value</h3>

<p>The output of the algorithm is S3 object including:
</p>
<table>
<tr><td><code>runtime</code></td>
<td>

<p>The run time of the algorithm. A numeric vector. The first element is the user time, the second element is the system time and the third element is the elapsed time.
</p>
</td></tr>
<tr><td><code>info</code></td>
<td>

<p>A matrix with the variables and their latest test statistics and <b>logged p-values</b>.
</p>
</td></tr>
<tr><td><code>mat</code></td>
<td>

<p>A matrix with the selected variables and their latest test statistic and <b>logged p-value</b>.
</p>
</td></tr>
<tr><td><code>ci_test</code></td>
<td>

<p>The conditional independence test used. 
</p>
</td></tr>
<tr><td><code>final</code></td>
<td>

<p>The final regression model.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fs.reg">fs.reg</a>, <a href="#topic+lm.fsreg">lm.fsreg</a>, <a href="#topic+bic.fsreg">bic.fsreg</a>, <a href="#topic+bic.glm.fsreg">bic.glm.fsreg</a>, <a href="#topic+CondIndTests">CondIndTests</a>, <a href="#topic+MMPC">MMPC</a>, <a href="#topic+SES">SES</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)

#simulate a dataset with continuous data
dataset &lt;- matrix( runif(200 * 10, 1, 100), ncol = 10 )

#define a simulated class variable 
target &lt;- rpois(200, 10)
a &lt;- glm.bsreg(target, dataset, threshold = 0.05) 

target &lt;- rbinom(200, 1, 0.6)
b &lt;- glm.bsreg(target, dataset, threshold = 0.05)

target &lt;- rgamma(200, 1, 2)
b1 &lt;- glm.bsreg2(target, dataset, threshold = 0.05, test = "testIndGamma")
b2 &lt;- glm.bsreg2(target, dataset, threshold = 0.05, test = "testIndNormLog")
</code></pre>

<hr>
<h2 id='Bayesian+20Network+20construction+20using+20a+20hybrid+20of+20MMPC+20and+20PC'>
Bayesian Network construction using a hybrid of MMPC and PC
</h2><span id='topic+mmpc.or'></span>

<h3>Description</h3>

<p>Bayesian Network construction using a hybrid of MMPC and PC.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mmpc.or(x, max_k = 5, threshold = 0.01, test = "testIndFisher", backward = TRUE, 
symmetry = TRUE, ini.pvalue = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Bayesian+2B20Network+2B20construction+2B20using+2B20a+2B20hybrid+2B20of+2B20MMPC+2B20and+2B20PC_+3A_x">x</code></td>
<td>

<p>A matrix with the variables. The user must know if they are continuous or if they are categorical. If you have a matrix with categorical data, 
i.e. 0, 1, 2, 3 where each number indicates a category, the minimum number for each variable must be 0. data.frame is also supported, as the 
dataset in this case is converted into a matrix.   
</p>
</td></tr>
<tr><td><code id="Bayesian+2B20Network+2B20construction+2B20using+2B20a+2B20hybrid+2B20of+2B20MMPC+2B20and+2B20PC_+3A_max_k">max_k</code></td>
<td>

<p>The maximum conditioning set to use in the conditional indepedence test (see Details of SES or MMPC). 
</p>
</td></tr>
<tr><td><code id="Bayesian+2B20Network+2B20construction+2B20using+2B20a+2B20hybrid+2B20of+2B20MMPC+2B20and+2B20PC_+3A_threshold">threshold</code></td>
<td>

<p>Threshold ( suitable values in (0, 1) ) for assessing p-values significance. Default value is 0.05.
</p>
</td></tr>
<tr><td><code id="Bayesian+2B20Network+2B20construction+2B20using+2B20a+2B20hybrid+2B20of+2B20MMPC+2B20and+2B20PC_+3A_test">test</code></td>
<td>

<p>The conditional independence test to use. Default value is &quot;testIndFisher&quot;. This procedure allows for &quot;testIndFisher&quot;, &quot;testIndSPearman&quot; for continuous variables and &quot;gSquare&quot; for categorical variables. 
</p>
</td></tr>
<tr><td><code id="Bayesian+2B20Network+2B20construction+2B20using+2B20a+2B20hybrid+2B20of+2B20MMPC+2B20and+2B20PC_+3A_backward">backward</code></td>
<td>

<p>If TRUE, the backward (or symmetry correction) phase will be implemented. This removes any falsely included variables in the parents and children set of the target variable. It calls the <code>link{mmpcbackphase}</code> 
for this purpose. For perm.ses and wald.ses this is not yet applicable.
</p>
</td></tr>
<tr><td><code id="Bayesian+2B20Network+2B20construction+2B20using+2B20a+2B20hybrid+2B20of+2B20MMPC+2B20and+2B20PC_+3A_symmetry">symmetry</code></td>
<td>

<p>In order for an edge to be added, a statistical relationship must have been found from both directions. If you want this symmetry correction to take place, leave this boolean variable to TRUE. If you set it to FALSE, then if 
a relationship between Y and X is detected but not between X and Y, the edge is still added. 
</p>
</td></tr>
<tr><td><code id="Bayesian+2B20Network+2B20construction+2B20using+2B20a+2B20hybrid+2B20of+2B20MMPC+2B20and+2B20PC_+3A_ini.pvalue">ini.pvalue</code></td>
<td>

<p>This is a list with the matrix of the univariate p-values. If you want to run mmhc.skel again, the univariate associations need not be calculated again.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The MMPC is run on every variable. The backward phase (see Tsamardinos et al., 2006) can then take place. After all variables have been used, the matrix is checked for inconsistencies and they are corrected if you want. 
The &quot;symmetry&quot; argument. Do you want the edge to stay if it was discovered from both variables when they were considered as responses? 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>ini.pvalue</code></td>
<td>

<p>A matrix with the p-values of all pairwise univariate assocations.
</p>
</td></tr>
<tr><td><code>kapa</code></td>
<td>

<p>The maximum number of conditioning variables ever observed. 
</p>
</td></tr>
<tr><td><code>ntests</code></td>
<td>

<p>The number of tests MMPC (or SES) performed at each variable.
</p>
</td></tr>
<tr><td><code>info</code></td>
<td>

<p>Some summary statistics about the edges, minimum, maximum, mean, median number of edges.
</p>
</td></tr>
<tr><td><code>density</code></td>
<td>

<p>The number of edges divided by the total possible number of edges, that is #edges / <code class="reqn">n(n-1)/2</code>, where <code class="reqn">n</code> is the number of variables.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The run time of the skeleton phase of the algorithm. A numeric vector. The first element is the user time, the second element is the system time and the third element is the elapsed time.
</p>
</td></tr>
<tr><td><code>runtime.or</code></td>
<td>

<p>The run time of the PC orientation rules. A numeric vector. The first element is the user time, the second element is the system time and the third element is the elapsed time.
</p>
</td></tr>
<tr><td><code>Gini</code></td>
<td>

<p>The adjancency matrix. A value of 1 in G[i, j] appears in G[j, i] also, indicating that i and j have an edge between them.
</p>
</td></tr>
<tr><td><code>G</code></td>
<td>

<p>The final adjaceny matrix with the orientations. If G[i, j] = 2 then G[j, i] = 3. This means that there is an arrow from node i to node j. If G[i, j] = G[j, i] = 0; there is no edge between nodes i and j. 
If G[i, j] = G[j, i] = 1; there is an (undirected) edge between nodes i and j.
</p>
</td></tr>
<tr><td><code>sepset</code></td>
<td>

<p>A list with the separating sets for every value of k.
</p>
</td></tr>
</table>
<p>Bear in mind that the values can be extracted with the $ symbol, i.e. this is an S3 class output. 
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Giorgos Athineou &lt;athineou@csd.uoc.gr&gt; and Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Tsamardinos, Brown and Aliferis (2006). The max-min hill-climbing Bayesian network structure learning algorithm. Machine learning, 65(1), 31-78.
</p>
<p>Spirtes P., Glymour C. and Scheines R. (2001). Causation, Prediction, and Search. The MIT Press, Cambridge, MA, USA, 3nd edition.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+pc.skel">pc.skel</a>, <a href="#topic+pc.or">pc.or</a>, <a href="#topic+corfs.network">corfs.network</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rdag2(500, p = 20, nei = 3)
ind &lt;- sample(1:20, 20)
x &lt;- y$x[, ind]
a1 &lt;- mmpc.or(x, max_k = 3, threshold = 0.01, test = "testIndFisher" ) 
b &lt;- pc.skel( x, alpha = 0.01 ) 
a2 &lt;- pc.or(b)
</code></pre>

<hr>
<h2 id='Beta+20regression'>
Beta regression
</h2><span id='topic+beta.mod'></span>

<h3>Description</h3>

<p>Beta regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>beta.mod(target, dataset, wei = NULL, xnew= NULL) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Beta+2B20regression_+3A_target">target</code></td>
<td>

<p>The target (dependent) variable. It must be a numerical vector with proportions, excluding 0s and 1s. 
</p>
</td></tr>
<tr><td><code id="Beta+2B20regression_+3A_dataset">dataset</code></td>
<td>

<p>The indendent variable(s). It can be a vector, a matrix or a dataframe with continuous only variables, a data frame with mixed or only categorical variables. If this is NULL, a beta distribution is fitted, no covariates are present.
</p>
</td></tr>
<tr><td><code id="Beta+2B20regression_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. An example where weights are used is surveys when stratified sampling has occured. 
</p>
</td></tr>
<tr><td><code id="Beta+2B20regression_+3A_xnew">xnew</code></td>
<td>

<p>If you have new values for the predictor variables (dataset) whose target variable you want to predict insert them here. If you put the &quot;dataset&quot; or leave it NULL.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The beta regression is fitted. The &quot;beta.reg&quot; is an internal wrapper function and is used for speed up purposes. It is not to be called directly by the user unless they know what they are doing. 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>be</code></td>
<td>

<p>The estimated coefficients of the model.
</p>
</td></tr>
<tr><td><code>phi</code></td>
<td>

<p>The estimated precision parameter. 
</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>

<p>The log-likelihood of the regression model. 
</p>
</td></tr>
<tr><td><code>est</code></td>
<td>

<p>The estimated values if xnew is not NULL.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Ferrari S.L.P. and Cribari-Neto F. (2004). Beta Regression for Modelling Rates and Proportions. Journal of Applied Statistics, 31(7): 799-815.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+beta.regs">beta.regs</a>, <a href="#topic+testIndBeta">testIndBeta</a>, <a href="#topic+reg.fit">reg.fit</a>, <a href="#topic+ridge.reg">ridge.reg</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rbeta(300, 3, 5)
x &lt;- matrix( rnorm(300 * 2), ncol = 2)
a1 &lt;- beta.mod(y, x)
w &lt;- runif(300)
a2 &lt;- beta.mod(y, x, w)
</code></pre>

<hr>
<h2 id='BIC+20based+20forward+20selection'>
Variable selection in regression models with forward selection using BIC
</h2><span id='topic+bic.fsreg'></span>

<h3>Description</h3>

<p>Variable selection in regression models with forward selection using BIC
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bic.fsreg(target, dataset, test = NULL, wei = NULL, tol = 2, ncores = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BIC+2B20based+2B20forward+2B20selection_+3A_target">target</code></td>
<td>

<p>The class variable. Provide either a string, an integer, a numeric value, a vector, a factor, an ordered factor or 
a Surv object. See also Details.
</p>
</td></tr>
<tr><td><code id="BIC+2B20based+2B20forward+2B20selection_+3A_dataset">dataset</code></td>
<td>

<p>The dataset; provide either a data frame or a matrix (columns = variables, rows = samples). The data can be either 
euclidean, categorical or both.
</p>
</td></tr>
<tr><td><code id="BIC+2B20based+2B20forward+2B20selection_+3A_test">test</code></td>
<td>

<p>The regression model to use. Available options are 
&quot;testIndReg&quot; for normal linear regression, &quot;testIndBeta&quot; for beta regression, &quot;censIndCR&quot; or &quot;censIndWR&quot; 
for Cox proportional hazards and Weibull regression respectively, &quot;testIndLogistic&quot; for binomial, multinomial 
or ordinal regression, &quot;testIndPois&quot; for poisson regression, &quot;testIndNB&quot; for negative binomial regression, 
&quot;testIndZIP&quot; for zero inflated poisson regression, &quot;testIndRQ&quot; for quantile regression, &quot;testIndGamma&quot; for gamma 
regression, &quot;testIndNormLog&quot; for linear regression with the log-link (non negative data), &quot;testIndTobit&quot; for Tobit 
regression. If you want to use multinomial or ordinal logistic regression, make sure your target is factor.
See also <code><a href="#topic+SES">SES</a></code> and <code><a href="#topic+CondIndTests">CondIndTests</a></code> for the tests.
</p>
</td></tr>
<tr><td><code id="BIC+2B20based+2B20forward+2B20selection_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. It is not suggested when robust is set to TRUE.
An example where weights are used is surveys when stratified sampling has occured.
</p>
</td></tr>
<tr><td><code id="BIC+2B20based+2B20forward+2B20selection_+3A_tol">tol</code></td>
<td>

<p>The difference bewtween two successive values of the stopping rule. By default this is is set to 2. If for example, the BIC difference between two succesive models is less than 2, the process stops and the last variable, even though significant does not enter the model.
</p>
</td></tr>
<tr><td><code id="BIC+2B20based+2B20forward+2B20selection_+3A_ncores">ncores</code></td>
<td>

<p>How many cores to use. This plays an important role if you have tens of thousands of variables or really large sample sizes and tens of thousands of variables and a regression based test which requires numerical optimisation. In other cammmb it will not make a difference in the overall time (in fact it can be slower). The parallel computation is used in the first step of the algorithm, where univariate associations are examined, those take place in parallel. We have seen a reduction in time of 50% with 4 cores in comparison to 1 core. Note also, that the amount of reduction is not linear in the number of cores.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the current 'test' argument is defined as NULL or &quot;auto&quot; and the user_test argument is NULL then the algorithm automatically selects the best test based on the type of the data. Particularly:
</p>

<ul>
<li><p> if target is a factor, the multinomial or the binary logistic regression is used. If the target has two values only, binary logistic regression will be used.
</p>
</li>
<li><p> if target is a ordered factor, the ordinal regression is used.
</p>
</li>
<li><p> if target is a numerical vector or a matrix with at least two columns (multivariate) linear regression is used. 
</p>
</li>
<li><p> if target is discrete numerical (counts), the poisson regression conditional independence test is used. If there are only two values, the binary logistic regression is to be used.
</p>
</li>
<li><p> if target is a Surv object, the Survival conditional independence test (Cox regression) is used.
</p>
</li></ul>



<h3>Value</h3>

<p>The output of the algorithm is S3 object including:
</p>
<table>
<tr><td><code>runtime</code></td>
<td>

<p>The run time of the algorithm. A numeric vector. The first element is the user time, the second element is the system time and the third element is the elapsed time.
</p>
</td></tr>
<tr><td><code>mat</code></td>
<td>

<p>A matrix with the variables and their latest test statistics and p-values.
</p>
</td></tr>
<tr><td><code>info</code></td>
<td>

<p>A matrix with the selected variables, and the BIC of the model with that and all the previous variables.
</p>
</td></tr>
<tr><td><code>ci_test</code></td>
<td>

<p>The conditional independence test used. 
</p>
</td></tr>
<tr><td><code>final</code></td>
<td>

<p>The final regression model.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Giorgos Athineou &lt;athineou@csd.uoc.gr&gt; Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Tsamardinos I., Aliferis C. F. and Statnikov, A. (2003). Time and sample efficient discovery of Markov blankets and direct causal relations. In Proceedings of the 9th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 673-678). 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+glm.fsreg">glm.fsreg</a>, <a href="#topic+lm.fsreg">lm.fsreg</a>, <a href="#topic+bic.glm.fsreg">bic.glm.fsreg</a>, <a href="#topic+CondIndTests">CondIndTests</a>, <a href="#topic+MMPC">MMPC</a>, <a href="#topic+SES">SES</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)
dataset &lt;- matrix( runif(200 * 20, 1, 100), ncol = 20 )
target &lt;- 3 * dataset[, 10] + 2 * dataset[, 15] + 3 * dataset[, 20] + rnorm(200, 0, 5)

a1 &lt;- bic.fsreg(target, dataset, tol = 4, ncores = 1, test = "testIndReg" ) 
a3 &lt;- MMPC(target, dataset, ncores = 1)
target &lt;- round(target)
b1 &lt;- bic.fsreg(target, dataset, tol = 2, ncores = 1, test = "testIndReg" ) 
</code></pre>

<hr>
<h2 id='BIC+20based+20forward+20selection+20with+20generalised+20linear+20models'>
Variable selection in generalised linear models with forward selection based on BIC
</h2><span id='topic+bic.glm.fsreg'></span><span id='topic+bic.mm.fsreg'></span><span id='topic+bic.gammafsreg'></span><span id='topic+bic.normlog.fsreg'></span>

<h3>Description</h3>

<p>Variable selection in generalised linear models with forward selection based on BIC
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bic.glm.fsreg( target, dataset, wei = NULL, tol = 0, ncores = 1) 

bic.mm.fsreg( target, dataset, wei = NULL, tol = 0, ncores = 1) 

bic.gammafsreg(target, dataset, wei = NULL, tol = 0, ncores = 1) 

bic.normlog.fsreg(target, dataset, wei = NULL, tol = 0, ncores = 1) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BIC+2B20based+2B20forward+2B20selection+2B20with+2B20generalised+2B20linear+2B20models_+3A_target">target</code></td>
<td>

<p>The class variable. It can be either a vector with binary data (binomial regression), counts (poisson regression). If none of these is identified, linear regression is used.
</p>
</td></tr>
<tr><td><code id="BIC+2B20based+2B20forward+2B20selection+2B20with+2B20generalised+2B20linear+2B20models_+3A_dataset">dataset</code></td>
<td>

<p>The dataset; provide either a data frame or a matrix (columns = variables, rows = samples). These can be continous and or categorical.
</p>
</td></tr>
<tr><td><code id="BIC+2B20based+2B20forward+2B20selection+2B20with+2B20generalised+2B20linear+2B20models_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. It is not suggested when robust is set to TRUE.
</p>
</td></tr>
<tr><td><code id="BIC+2B20based+2B20forward+2B20selection+2B20with+2B20generalised+2B20linear+2B20models_+3A_tol">tol</code></td>
<td>

<p>The difference bewtween two successive values of BIC. By default this is is set to 2. If for example, the BIC difference between two succesive models is less than 2, the process stops and the last variable, even though significant does not enter the model. 
</p>
</td></tr>
<tr><td><code id="BIC+2B20based+2B20forward+2B20selection+2B20with+2B20generalised+2B20linear+2B20models_+3A_ncores">ncores</code></td>
<td>

<p>How many cores to use. This plays an important role if you have tens of thousands of variables or really large sample sizes and tens of thousands of variables and a regression based test which requires numerical optimisation. In other cammmb it will not make a difference in the overall time (in fact it can be slower). The parallel computation is used in the first step of the algorithm, where univariate associations are examined, those take place in parallel. We have seen a reduction in time of 50% with 4 cores in comparison to 1 core. Note also, that the amount of reduction is not linear in the number of cores.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Forward selection via the BIC is implemented. A variable which results in a reduction of BIC will be included, until the reduction is below a threshold set by the user (argument &quot;tol&quot;).
</p>


<h3>Value</h3>

<p>The output of the algorithm is S3 object including:
</p>
<table>
<tr><td><code>runtime</code></td>
<td>

<p>The run time of the algorithm. A numeric vector. The first element is the user time, the second element is the system time and the third element is the elapsed time.
</p>
</td></tr>
<tr><td><code>mat</code></td>
<td>

<p>A matrix with the variables and their latest test statistics and <b>logged p-values</b>.
</p>
</td></tr>
<tr><td><code>info</code></td>
<td>

<p>A matrix with the selected variables, and the BIC of the model with that and all the previous variables.
</p>
</td></tr>
<tr><td><code>ci_test</code></td>
<td>

<p>The conditional independence test used. 
</p>
</td></tr>
<tr><td><code>final</code></td>
<td>

<p>The final regression model.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Giorgos Aathineou &lt;athineou@csd.uoc.gr&gt; Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fs.reg">fs.reg</a>, <a href="#topic+lm.fsreg">lm.fsreg</a>, <a href="#topic+bic.fsreg">bic.fsreg</a>, <a href="#topic+CondIndTests">CondIndTests</a>, <a href="#topic+MMPC">MMPC</a>, <a href="#topic+SES">SES</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)
dataset &lt;- matrix( runif(200 * 20, 1, 100), ncol = 20 )
target &lt;- 3 * dataset[, 10] + 2 * dataset[, 15] + 3 * dataset[, 20] + rnorm(200, 0, 5)
a1 &lt;- bic.glm.fsreg(target, dataset, tol = 2, ncores = 1 ) 
a2 &lt;- bic.glm.fsreg( round(target), dataset, tol = 2, ncores = 1 ) 
y &lt;- target   ;   me &lt;- median(target)  ;   y[ y &lt; me ] &lt;- 0   ;   y[ y &gt;= me ] &lt;- 1
a3 &lt;- bic.glm.fsreg( y, dataset, tol = 2, ncores = 1 ) 
</code></pre>

<hr>
<h2 id='Bootstrap+20bias+20correction+20for+20the+20performance+20of+20the+20cross-validation+20procedure'>
Bootstrap bias correction for the performance of the cross-validation procedure
</h2><span id='topic+bbc'></span>

<h3>Description</h3>

<p>Bootstrap bias correction for the performance of the cross-validation procedure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bbc(predictions, target, metric = "auc.mxm", conf = 0.95, B = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Bootstrap+2B20bias+2B20correction+2B20for+2B20the+2B20performance+2B20of+2B20the+2B20cross-validation+2B20procedure_+3A_predictions">predictions</code></td>
<td>

<p>A matrix with the predictived values.
</p>
</td></tr>
<tr><td><code id="Bootstrap+2B20bias+2B20correction+2B20for+2B20the+2B20performance+2B20of+2B20the+2B20cross-validation+2B20procedure_+3A_target">target</code></td>
<td>

<p>A vector with the target variable, survival object, factor (ordered or unordered) or a numerical vector.
</p>
</td></tr>
<tr><td><code id="Bootstrap+2B20bias+2B20correction+2B20for+2B20the+2B20performance+2B20of+2B20the+2B20cross-validation+2B20procedure_+3A_metric">metric</code></td>
<td>

<p>The possible values are: 
</p>
<p>a) Binary target: &quot;auc.mxm&quot; (area under the curve), &quot;fscore.mxm&quot; (F-score), &quot;prec.mxm&quot; (precision), &quot;euclid_sens.spec.mxm&quot; (Euclidean distance of sensitivity and specificity), &quot;spec.mxm&quot; (specificity), &quot;sens.mxm&quot; (sensitivity), &quot;acc.mxm&quot; (accuracy, proportion of correct classification).
</p>
<p>b) Multinomial target: &quot;acc_multinom.mxm&quot; (accuracy, proportion of correct classification). 
</p>
<p>c) Ordinal target: &quot;ord_mae.mxm&quot; (mean absolute error).
</p>
<p>d) Continuous target: &quot;mae.mxm&quot; (MAE with continuous target), &quot;mse.mxm&quot; (mean squared error), &quot;pve.mxm&quot; (percentage of variance explained).
</p>
<p>e) Survival target &quot;ci.mxm&quot; (concordance index for Cox regression), &quot;ciwr.mxm&quot; (concordance index for Weibull regression). 
g) Count target &quot;poisdev.mxm&quot;.
</p>
<p>h) Binomial target &quot;binomdev.mxm&quot; (deviance of binomial regression).
</p>
<p>The &quot;nbdev.mxm&quot; (negative binomial deviance) is missing. For more information on these see <code><a href="#topic+cv.ses">cv.ses</a></code>. boldNote that they come with &quot;&quot;.
</p>
</td></tr>
<tr><td><code id="Bootstrap+2B20bias+2B20correction+2B20for+2B20the+2B20performance+2B20of+2B20the+2B20cross-validation+2B20procedure_+3A_conf">conf</code></td>
<td>

<p>A number between 0 and 1, the confidence level.
</p>
</td></tr>
<tr><td><code id="Bootstrap+2B20bias+2B20correction+2B20for+2B20the+2B20performance+2B20of+2B20the+2B20cross-validation+2B20procedure_+3A_b">B</code></td>
<td>

<p>The number of bootstrap replicates. The default number is 1000.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Upon completion of the cross-validation, the predicted values produced by all predictive models across all folds is collected in a matrix <code class="reqn">P</code> of dimensions <code class="reqn">n \times M</code>, where <code class="reqn">n</code> is the number of samples and <code class="reqn">M</code> the number of trained models or configurations. Sampled with replacement a fraction of rows (predictions) from <code class="reqn">P</code> are denoted as the in-sample values. On average, the newly created set will be comprised by 63.2% of the original individuals (The probability of sampling, with replacement, a sample of <code class="reqn">n</code> numbers from a set of <code class="reqn">n</code> numbers is <code class="reqn">1-\left(1-\frac{1}{n} \right)^n \simeq 1-\frac{1}{e}=0.632</code>), whereas the rest 36.8% will be random copies of them. The non re-sampled rows are denoted as out-of-sample values. The performance of each model in the in-sample rows is calculated and the model (or configuration) with the optimal performance is selected, followed by the calculation of performance in the out-of-sample values. This process is repeated B times and the average performance is returned. 
</p>
<p>Note, that the only computational overhead is with the repetitive re-sampling and calculation of the predictive performance, i.e. no model is fitted nor trained. The final estimated performance usually underestimates the true performance, but this negative bias is smaller than the optimistic uncorrected performance. 
</p>
<p>Note, that all metrics are for maximization. For this reason &quot;mse.mxm&quot;, &quot;mae.mxm&quot;, &quot;ord_mae.mxm&quot;, &quot;poisdev.mxm&quot;, &quot;binomdev.mxm&quot; are multiplied by -1.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>out.perf</code></td>
<td>

<p>The B out sampled performances. Their mean is the &quot;bbc.perf&quot; given above.
</p>
</td></tr>
<tr><td><code>bbc.perf</code></td>
<td>

<p>The bootstrap bias corrected performance of the chosen algorithm, model or configuration.
</p>
</td></tr>
<tr><td><code>ci</code></td>
<td>

<p>The (1- conf)% confidence interval of the BBC performance. It is based on the empirical or percentile method for bootstrap samples. The lower and upper 2.5% of the &quot;out.perf&quot;.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Ioannis Tsamardinos, Elissavet Greasidou and Giorgos Borboudakis (2018). Bootstrapping the out-of-sample predictions for efficient and accurate cross-validation. Machine Learning (To appear).  
</p>
<p><a href="https://link.springer.com/article/10.1007/s10994-018-5714-4">https://link.springer.com/article/10.1007/s10994-018-5714-4</a>
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+cv.ses">cv.ses</a>, <a href="#topic+cv.gomp">cv.gomp</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>predictions &lt;- matrix(rbinom(200 * 100, 1, 0.7), ncol = 100) 
target &lt;- rbinom(200, 1, 0.5)
bbc(predictions, target, metric = "auc.mxm")
</code></pre>

<hr>
<h2 id='Calculation+20of+20the+20constant+20and+20slope+20for+20each+20subject+20over+20time'>
Calculation of the constant and slope for each subject over time
</h2><span id='topic+group.mvbetas'></span>

<h3>Description</h3>

<p>Calculation of the constant and slope for each subject over time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>group.mvbetas(x, id, reps)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Calculation+2B20of+2B20the+2B20constant+2B20and+2B20slope+2B20for+2B20each+2B20subject+2B20over+2B20time_+3A_x">x</code></td>
<td>

<p>The dataset; provide a numerical a matrix. This should contain longitudinal data. Each variable is a column, and rows are longitudinal data.
</p>
</td></tr>
<tr><td><code id="Calculation+2B20of+2B20the+2B20constant+2B20and+2B20slope+2B20for+2B20each+2B20subject+2B20over+2B20time_+3A_id">id</code></td>
<td>

<p>This is a numerical vector denoting the subjects. Its length must be equal to the number of rows of the x matrix.  
</p>
</td></tr>
<tr><td><code id="Calculation+2B20of+2B20the+2B20constant+2B20and+2B20slope+2B20for+2B20each+2B20subject+2B20over+2B20time_+3A_reps">reps</code></td>
<td>

<p>This is a numerical vector with the time points. Its length must be equal to the number of rows of the x matrix.  
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is used internally in <code><a href="#topic+SES">SES</a></code> and <code><a href="#topic+MMPC">MMPC</a></code> and does calculations required bys the first step of the <b>Static-Longitudinal</b> scenario of Tsagris, Lagani and Tsamardinos (2018). The measurements of each subject are regressed againt time. 
So, for each subject, we get the constant and interecept over time and this is repated for very feature.
</p>


<h3>Value</h3>

<p>A matrix. The first r ( = length( unique(id) ), the nubmer of subjects ) rows contain the constants and the other r rows contain the slopes.  
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Tsagris M., Lagani V., &amp; Tsamardinos I. (2018). Feature selection for high-dimensional glmm data. BMC bioinformatics, 19(1), 17.
</p>
<p>McCullagh, Peter, and John A. Nelder. Generalized linear models.  CRC press, USA, 2nd edition, 1989.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+fbed.gee.reg">fbed.gee.reg</a>, <a href="#topic+glmm.bsreg">glmm.bsreg</a>, <a href="#topic+MMPC.glmm">MMPC.glmm</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## assume these are longitudinal data, each column is a variable (or feature)
x &lt;- matrix( rnorm(100 * 30), ncol = 30 ) 
id &lt;- rep(1:20, each = 5)  ## 20 subjects
reps &lt;- rep( seq(4, 12, by = 2), 20)  ## 5 time points for each subject
a &lt;- group.mvbetas(x, id, reps)
dim(a)  ## 5  100
## these are the regression coefficients of the first subject's values on the 
## reps (which is assumed to be time in this example)
a[c(1, 21), 1] 
coef( lm( x[id == 1, 1] ~ reps[1:5] ) )
</code></pre>

<hr>
<h2 id='Certificate+20of+20exclusion+20from+20the+20selected+20variables+20set+20using+20SES+20or+20MMPC'>
Certificate of exclusion from the selected variables set using SES or MMPC
</h2><span id='topic+certificate.of.exclusion'></span><span id='topic+certificate.of.exclusion2'></span>

<h3>Description</h3>

<p>Information on why one ore more variables were not selected.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>certificate.of.exclusion(xIndex, sesObject = NULL, mmpcObject = NULL) 
certificate.of.exclusion2(xIndex, mmpc2object) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Certificate+2B20of+2B20exclusion+2B20from+2B20the+2B20selected+2B20variables+2B20set+2B20using+2B20SES+2B20or+2B20MMPC_+3A_xindex">xIndex</code></td>
<td>

<p>A numerical vector with the  indices of the predictor variables.  
</p>
</td></tr>
<tr><td><code id="Certificate+2B20of+2B20exclusion+2B20from+2B20the+2B20selected+2B20variables+2B20set+2B20using+2B20SES+2B20or+2B20MMPC_+3A_sesobject">sesObject</code></td>
<td>

<p>If you ran SES, wald.ses or perm.ses, give the whole SES object here, otherwise leave it NULL.
</p>
</td></tr>
<tr><td><code id="Certificate+2B20of+2B20exclusion+2B20from+2B20the+2B20selected+2B20variables+2B20set+2B20using+2B20SES+2B20or+2B20MMPC_+3A_mmpcobject">mmpcObject</code></td>
<td>

<p>If you ran MMPC, wald.mmpc or prm.mmpc, give the whole MMPC object here, otherwise leave it NULL.
</p>
</td></tr>
<tr><td><code id="Certificate+2B20of+2B20exclusion+2B20from+2B20the+2B20selected+2B20variables+2B20set+2B20using+2B20SES+2B20or+2B20MMPC_+3A_mmpc2object">mmpc2object</code></td>
<td>

<p>If you ran mmpc2, give the whole MMPC object here.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the conditioning variables (if any), the test statistic and the logarithm of the p-value. In case a variable has been selected a message appears.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+MMPC">MMPC</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)
#simulate a dataset with continuous data
dataset &lt;- matrix(runif(100 * 100, 1, 100), ncol = 100)
#define a simulated class variable 
target &lt;- 3 * dataset[, 10] + 2 * dataset[, 100] + 3 * dataset[, 20] + rnorm(100, 0, 5)
# define some simulated equivalences
dataset[, 15] &lt;- dataset[, 10] + rnorm(100, 0, 2)
dataset[, 100] &lt;- dataset[, 100] + rnorm(100, 0, 2) 
dataset[, 20] &lt;- dataset[, 100] + rnorm(100, 0, 2)
# run the SES algorithm
mod1 &lt;- SES(target, dataset, max_k = 5, threshold = 0.05, test = "testIndFisher", 
hash = TRUE, hashObject = NULL);
mod2 &lt;- MMPC(target, dataset, max_k = 5, threshold = 0.05, test = "testIndFisher", 
hash = TRUE, hashObject = NULL);
certificate.of.exclusion(c(10, 15, 30, 45, 20), mod1)
certificate.of.exclusion(c(10, 15, 30, 45, 20), NULL, mod2)
</code></pre>

<hr>
<h2 id='Check+20Markov+20equivalence+20of+20two+20DAGs'>
Check Markov equivalence of two DAGs 
</h2><span id='topic+equivdags'></span>

<h3>Description</h3>

<p>Check Markov equivalence of two DAGs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>equivdags(g1, g2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Check+2B20Markov+2B20equivalence+2B20of+2B20two+2B20DAGs_+3A_g1">g1</code></td>
<td>

<p>The matrix of a DAG or a partially directed graph as produced from <code><a href="#topic+pc.or">pc.or</a></code> or any other algorithm. 
</p>
</td></tr>
<tr><td><code id="Check+2B20Markov+2B20equivalence+2B20of+2B20two+2B20DAGs_+3A_g2">g2</code></td>
<td>

<p>The matrix of a DAG or a partially directed graph as produced from <code><a href="#topic+pc.or">pc.or</a></code> or any other algorithm. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Two DAGs are Markov equivalent if a) they have the same adjancencies (regardlsee of the mark, arrowhead, tail or nothing) and  b) they have the same unshielded colliders.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>apofasi</code></td>
<td>

<p>A boolean variable, TRUE of FALSE.  
</p>
</td></tr>
<tr><td><code>mes</code></td>
<td>

<p>A message specyfying the result, the dimensions of the adjacency matrices do not match for example, or the number of adjancencies 
is not the same, they do not share the same unshilded colliders, or they are Markov equivalent. 
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Giorgos Athineou &lt;athineou@csd.uoc.gr&gt; and Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Tsamardinos, Brown and Aliferis (2006). The max-min hill-climbing Bayesian network structure learning algorithm. Machine learning, 65(1), 31-78.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+pc.or">pc.or</a>, <a href="#topic+mmhc.skel">mmhc.skel</a>, <a href="#topic+pc.con">pc.con</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rdag(1000, 10, 0.3)
tru &lt;- y$G 
x &lt;- y$x
mod &lt;- pc.con(x)

eg &lt;- dag2eg(y$G) ## make it essential graph first 
est &lt;- pc.or(mod)$G

equivdags(est, tru)
</code></pre>

<hr>
<h2 id='Check+20whether+20a+20directed+20graph+20is+20acyclic'>
Check whether a directed graph is acyclic
</h2><span id='topic+is.dag'></span>

<h3>Description</h3>

<p>Check whether a directed graph is acyclic.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.dag(dag)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Check+2B20whether+2B20a+2B20directed+2B20graph+2B20is+2B20acyclic_+3A_dag">dag</code></td>
<td>

<p>A square matrix representing a directed graph which contains either 0, 1 or 0, 2, and 3. In the first canse where G[i, j] = 1, means there is an arrow from node i to node j. 
In the second case G[i, j] = 2 and G[j, i] = 3 means that there is an arrow from node i to node j, where the 2 iindcates the arrohead and the 3 inducates the arrowtail. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The topological sort is performed. If it cannot be performed, NAs are returned. Hence, the functions checks for NAs.
</p>


<h3>Value</h3>

<p>A logical value, TRUE if the matrix represents a DAG and FALSE otherwise.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Chickering, D.M. (1995). A transformational characterization of equivalent Bayesian network structures. Proceedings of the 11th Conference on Uncertainty in Artificial Intelligence, Montreal, Canada, 87-98. 
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+topological_sort">topological_sort</a>, <a href="#topic+dag2eg">dag2eg</a>, <a href="#topic+pc.or">pc.or</a> 
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate a dataset with continuous data
# simulate a dataset with continuous data
G &lt;- rdag(100, 20, 0.3)$G
is.dag(G)  ## TRUE 
</code></pre>

<hr>
<h2 id='CondInditional+20independence+20tests'> MXM Conditional independence tests </h2><span id='topic+CondIndTests'></span>

<h3>Description</h3>

<p>Currently the <span class="pkg">MXM</span> package supports numerous tests for different types of target (dependent) and predictor (independent) variables. The target variable can be of continuous, discrete, categorical and of survival type. As for the predictor variables, they can be continuous, categorical or mixed. 
</p>
<p>The <b>testIndFisher</b> and the <b>gSquare</b> tests have two things in common. They do not use a model implicitly (i.e. estimate some beta coefficients), even though there is an underlying assumed one. Secondly they are pure tests of independence (again, with assumptions required).
</p>
<p>As for the other tests, they share one thing in common. For all of them, two parametric models must be fit. The null model containing the conditioning set of variables alone and the alternative model containing the conditioning set and the candidate variable. The significance of the new variable is assessed via a log-likelihood ratio test with the appropriate degrees of freedom. All of these tests which are available for SES and MMPC are summarized in the below table.
</p>

<table>
<tr>
 <td style="text-align: left;"> 
  <b>Target variable</b> </td><td style="text-align: left;">	<b>Predictor variables</b> </td><td style="text-align: left;"> <b>Available tests</b> </td><td style="text-align: left;">	<b>Short explanation</b> </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  
  Continuous </td><td style="text-align: left;"> Continuous </td><td style="text-align: left;"> testIndFisher </td><td style="text-align: left;"> Partial correlation </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Continuous </td><td style="text-align: left;"> Continuous </td><td style="text-align: left;"> testIndMMFisher </td><td style="text-align: left;"> Robust partial correlation </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Continuous </td><td style="text-align: left;"> Continuous </td><td style="text-align: left;"> testIndSpearman </td><td style="text-align: left;"> Partial correlation </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Continuous </td><td style="text-align: left;"> Mixed </td><td style="text-align: left;"> testIndMMReg </td><td style="text-align: left;"> MM regression </td>
</tr>
<tr>
 <td style="text-align: left;">  
  Continuous </td><td style="text-align: left;"> Mixed </td><td style="text-align: left;"> testIndRQ </td><td style="text-align: left;"> Median regression </td>
</tr>
<tr>
 <td style="text-align: left;">  
  Proportions </td><td style="text-align: left;"> Continuous </td><td style="text-align: left;"> testIndFisher </td><td style="text-align: left;"> Partial correlation </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Proportions </td><td style="text-align: left;"> Continuous </td><td style="text-align: left;"> testIndMMFisher </td><td style="text-align: left;"> Robust partial correlation </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Proportions </td><td style="text-align: left;"> Continuous </td><td style="text-align: left;"> testIndSpearman </td><td style="text-align: left;"> Partial correlation </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Proportions </td><td style="text-align: left;"> Mixed </td><td style="text-align: left;"> testIndReg </td><td style="text-align: left;"> Linear regression </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Proportions </td><td style="text-align: left;"> Mixed </td><td style="text-align: left;"> testIndMMReg </td><td style="text-align: left;"> MM regression </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Proportions </td><td style="text-align: left;"> Mixed </td><td style="text-align: left;"> testIndRQ </td><td style="text-align: left;"> Median regression </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Proportions </td><td style="text-align: left;"> Mixed </td><td style="text-align: left;"> testIndBeta </td><td style="text-align: left;"> Beta regression </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Proportions </td><td style="text-align: left;"> Mixed </td><td style="text-align: left;"> testIndQbinom </td><td style="text-align: left;"> Quasi binomial regression </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Strictly positive </td><td style="text-align: left;"> Mixed </td><td style="text-align: left;"> testIndIGreg </td><td style="text-align: left;"> Inverse Gaussian regression </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Strictly positive </td><td style="text-align: left;"> Mixed </td><td style="text-align: left;"> testIndGamma </td><td style="text-align: left;"> Gamma regression </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Non negative </td><td style="text-align: left;"> Mixed </td><td style="text-align: left;"> testIndNormLog </td><td style="text-align: left;"> Gaussian regression with log link </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Strictly Positive </td><td style="text-align: left;"> Mixed </td><td style="text-align: left;"> censIndWR </td><td style="text-align: left;"> Weibull regression </td>
</tr>
<tr>
 <td style="text-align: left;">
  Strictly Positive </td><td style="text-align: left;"> Mixed </td><td style="text-align: left;"> censIndER </td><td style="text-align: left;"> Exponential regression </td>
</tr>
<tr>
 <td style="text-align: left;">
  Strictly Positive </td><td style="text-align: left;"> Mixed </td><td style="text-align: left;"> censIndLLR </td><td style="text-align: left;"> Log-logistic regression </td>
</tr>
<tr>
 <td style="text-align: left;">
  Successes &amp; totals </td><td style="text-align: left;"> Mixed </td><td style="text-align: left;"> testIndBinom </td><td style="text-align: left;"> Binomial regression </td>
</tr>
<tr>
 <td style="text-align: left;">
  Discrete </td><td style="text-align: left;">	Mixed </td><td style="text-align: left;"> testIndPois </td><td style="text-align: left;"> Poisson regression </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Discrete </td><td style="text-align: left;">	Mixed </td><td style="text-align: left;"> testIndZIP </td><td style="text-align: left;"> Zero Inflated </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  </td><td style="text-align: left;">	</td><td style="text-align: left;">  </td><td style="text-align: left;"> Poisson regression </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Discrete </td><td style="text-align: left;"> Mixed </td><td style="text-align: left;"> testIndNB </td><td style="text-align: left;"> Negative binomial regression </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Discrete </td><td style="text-align: left;"> Mixed </td><td style="text-align: left;"> testIndQPois </td><td style="text-align: left;"> Quasi Poisson regression </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Factor with two </td><td style="text-align: left;"> Mixed </td><td style="text-align: left;"> testIndLogistic </td><td style="text-align: left;"> Binary logistic regression </td>
</tr>
<tr>
 <td style="text-align: left;">
  levels or binary </td><td style="text-align: left;"> </td><td style="text-align: left;"> </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Factor with two </td><td style="text-align: left;"> Mixed </td><td style="text-align: left;"> testIndQBinom </td><td style="text-align: left;"> Quasi binomial regression </td>
</tr>
<tr>
 <td style="text-align: left;">
  levels or binary </td><td style="text-align: left;"> </td><td style="text-align: left;"> </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Factor with more </td><td style="text-align: left;"> Mixed </td><td style="text-align: left;">  testIndMultinom  </td><td style="text-align: left;"> Multinomial logistic regression  </td>
</tr>
<tr>
 <td style="text-align: left;">		 
  than two levels </td><td style="text-align: left;">  </td><td style="text-align: left;">  </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">		 
  (unordered) </td><td style="text-align: left;"> </td><td style="text-align: left;"> </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">		   
  Factor with more than </td><td style="text-align: left;"> Mixed </td><td style="text-align: left;"> testIndOrdinal </td><td style="text-align: left;"> Ordinal logistic regression </td>
</tr>
<tr>
 <td style="text-align: left;">		 
  two levels (ordered) </td><td style="text-align: left;"> </td><td style="text-align: left;"> </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">  
  Categorical </td><td style="text-align: left;"> Categorical </td><td style="text-align: left;"> gSquare </td><td style="text-align: left;"> G-squared test of independence </td>
</tr>
<tr>
 <td style="text-align: left;">  
  Categorical </td><td style="text-align: left;"> Categorical </td><td style="text-align: left;"> testIndMultinom </td><td style="text-align: left;"> Multinomial logistic regression </td>
</tr>
<tr>
 <td style="text-align: left;">  
  Categorical </td><td style="text-align: left;"> Categorical </td><td style="text-align: left;"> testIndOrdinal </td><td style="text-align: left;"> Ordinal logistic regression </td>
</tr>
<tr>
 <td style="text-align: left;">  
  Survival	</td><td style="text-align: left;"> Mixed </td><td style="text-align: left;"> censIndCR </td><td style="text-align: left;"> Cox regression </td>
</tr>
<tr>
 <td style="text-align: left;">
  Survival	</td><td style="text-align: left;"> Mixed </td><td style="text-align: left;"> censIndWR </td><td style="text-align: left;"> Weibull regression </td>
</tr>
<tr>
 <td style="text-align: left;">
  Survival	</td><td style="text-align: left;"> Mixed </td><td style="text-align: left;"> censIndER </td><td style="text-align: left;"> Exponential regression </td>
</tr>
<tr>
 <td style="text-align: left;">
  Survival	</td><td style="text-align: left;"> Mixed </td><td style="text-align: left;"> censIndLLR </td><td style="text-align: left;"> Log-logistic regression </td>
</tr>
<tr>
 <td style="text-align: left;">
  Left censored	</td><td style="text-align: left;"> Mixed </td><td style="text-align: left;"> testIndTobit </td><td style="text-align: left;"> Tobit regression </td>
</tr>
<tr>
 <td style="text-align: left;">
  Case-control  </td><td style="text-align: left;"> Mixed </td><td style="text-align: left;"> testIndClogit </td><td style="text-align: left;"> Conditional logistic regression </td>
</tr>
<tr>
 <td style="text-align: left;">
  Multivariate continuous </td><td style="text-align: left;"> Mixed </td><td style="text-align: left;"> testIndMVreg  </td><td style="text-align: left;"> Multivariate linear regression </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Compositional data </td><td style="text-align: left;"> Mixed </td><td style="text-align: left;"> testIndMVreg </td><td style="text-align: left;"> Multivariate linear regression </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  (no zeros) </td><td style="text-align: left;">  </td><td style="text-align: left;"> after multivariate </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  </td><td style="text-align: left;"> </td><td style="text-align: left;"> logit transformation </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
  Longitudinal/clustered	</td><td style="text-align: left;"> Continuous </td><td style="text-align: left;"> testIndGLMMReg </td><td style="text-align: left;"> Linear mixed models </td>
</tr>
<tr>
 <td style="text-align: left;">
  Clustered	</td><td style="text-align: left;"> Continuous </td><td style="text-align: left;"> testIndLMM   </td><td style="text-align: left;"> Fast linear mixed models </td>
</tr>
<tr>
 <td style="text-align: left;">
  Binary longitudinal	</td><td style="text-align: left;"> Continuous </td><td style="text-align: left;"> testIndGLMMLogistic </td><td style="text-align: left;"> Logistic mixed regression </td>
</tr>
<tr>
 <td style="text-align: left;">
  and clustered	</td><td style="text-align: left;">  </td><td style="text-align: left;">  </td><td style="text-align: left;">   </td>
</tr>
<tr>
 <td style="text-align: left;">
  Count longitudinal	</td><td style="text-align: left;"> Continuous </td><td style="text-align: left;"> testIndGLMMPois </td><td style="text-align: left;"> Poisson mixed regression </td>
</tr>
<tr>
 <td style="text-align: left;">
  and clustered	</td><td style="text-align: left;">  </td><td style="text-align: left;">  </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
  Positive longitudinal	</td><td style="text-align: left;"> Continuous </td><td style="text-align: left;"> testIndGLMMNormLog </td><td style="text-align: left;"> GLMM with Gaussian regression </td>
</tr>
<tr>
 <td style="text-align: left;">
  and clustered	</td><td style="text-align: left;">  </td><td style="text-align: left;">  </td><td style="text-align: left;"> and log link  </td>
</tr>
<tr>
 <td style="text-align: left;">
  Non negative longitudinal	</td><td style="text-align: left;"> Continuous </td><td style="text-align: left;"> testIndGLMMGamma </td><td style="text-align: left;"> GLMM with Gamma regression </td>
</tr>
<tr>
 <td style="text-align: left;">
  and clustered	</td><td style="text-align: left;">  </td><td style="text-align: left;">  </td><td style="text-align: left;"> and log link  </td>
</tr>
<tr>
 <td style="text-align: left;">
  Longitudinal/clustered	</td><td style="text-align: left;"> Continuous </td><td style="text-align: left;"> testIndGEEReg </td><td style="text-align: left;"> GEE with Gaussian regression </td>
</tr>
<tr>
 <td style="text-align: left;">
  Binary longitudinal	</td><td style="text-align: left;"> Continuous </td><td style="text-align: left;"> testIndGEELogistic </td><td style="text-align: left;"> GEE with logistic regression </td>
</tr>
<tr>
 <td style="text-align: left;">
  and clustered	</td><td style="text-align: left;">  </td><td style="text-align: left;">  </td><td style="text-align: left;">   </td>
</tr>
<tr>
 <td style="text-align: left;">
  Count longitudinal	</td><td style="text-align: left;"> Continuous </td><td style="text-align: left;"> testIndGEEPois </td><td style="text-align: left;"> GEE with Poisson regression </td>
</tr>
<tr>
 <td style="text-align: left;">
  and clustered	</td><td style="text-align: left;">  </td><td style="text-align: left;">  </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
  Positive longitudinal	</td><td style="text-align: left;"> Continuous </td><td style="text-align: left;"> testIndGEENormLog </td><td style="text-align: left;"> GEE with Gaussian regression </td>
</tr>
<tr>
 <td style="text-align: left;">
  and clustered	</td><td style="text-align: left;">  </td><td style="text-align: left;">  </td><td style="text-align: left;"> and log link  </td>
</tr>
<tr>
 <td style="text-align: left;">
  Non negative longitudinal	</td><td style="text-align: left;"> Continuous </td><td style="text-align: left;"> testIndGEEGamma </td><td style="text-align: left;"> GEE with Gamma regression </td>
</tr>
<tr>
 <td style="text-align: left;">
  and clustered	</td><td style="text-align: left;">  </td><td style="text-align: left;">  </td><td style="text-align: left;"> and log link  </td>
</tr>
<tr>
 <td style="text-align: left;">
  Clustered survival	</td><td style="text-align: left;"> Contiunous </td><td style="text-align: left;"> testIndGLMMCR </td><td style="text-align: left;"> Mixed effects Cox regression </td>
</tr>
<tr>
 <td style="text-align: left;">
  Circular </td><td style="text-align: left;"> Continuous </td><td style="text-align: left;"> testIndSPML </td><td style="text-align: left;"> Circular-linear regression </td>
</tr>
<tr>
 <td style="text-align: left;"> 

</td>
</tr>

</table>



<h3>Details</h3>

<p>These tests can be called by SES, MMPC, wald.mmpc or individually by the user. In all regression cases, there is an option for weights. 
</p>


<h3>Log-likelihood ratio tests</h3>


<ol>
<li> <p><b>testIndFisher</b>. This is a standard test of independence when both the target and the set of predictor variables are continuous (continuous-continuous). When the joint multivariate normality of all the variables is assumed, we know that if a correlation is zero this means that the two variables are independent. Moving in this spirit, when the partial correlation between the target variable and the new predictor variable conditioning on a set of (predictor) variables is zero, then we have evidence to say they are independent as well. An easy way to calculate the partial correlation between the target and a predictor variable conditioning on some other variables is to regress the both the target and the new variable on the conditioning set. The correlation coefficient of the residuals produced by the two regressions equals the partial correlation coefficient. If the robust option is selected, the two aforementioned regression models are fitted using M estimators (Marona et al., 2006). If the target variable consists of proportions or percentages (within the (0, 1) interval), the logit transformation is applied beforehand.
</p>
</li>
<li> <p><b>testIndSpearman</b>. This is a non-parametric alternative to <b>testIndFisher</b> test. It is a bit slower than its competitor, yet very fast and suggested when normality assumption breaks down or outliers are present. In fact, within SES, what happens is that the ranks of the target and of the dataset (predictor variables) are computed and the <b>testIndSpearman</b> is aplied. This is faster than applying Fisher with M estimators as described above. If the target variable consists of proportions or percentages (within the (0, 1) interval), the logit transformation is applied beforehand.
</p>
</li>
<li> <p><b>testIndReg</b>. In the case of target-predictors being continuous-mixed or continuous-categorical, the suggested test is via the standard linear regression. In this case, two linear regression models are fitted. One with the conditioning set only and one with the conditioning set plus the new variable. The significance of the new variable is assessed via the F test, which calculates the residual sum of squares of the two models. The reason for the F test is because the new variable may be categorical and in this case the t test cannot be used. It makes sense to say, that this test can be used instead of the <b>testIndFisher</b>, but it will be slower. If the robust option is selected, the two models are fitted using M estimators (Marona et al. 2006). If the target variable consists of proportions or percentages (within the (0, 1) interval), the logit transformation is applied beforehand.
</p>
</li>
<li> <p><b>testIndRQ</b>. An alternative to <b>testIndReg</b> for the case of continuous-mixed (or continuous-continuous) variables is the <b>testIndRQ</b>. Instead of fitting two linear regression models, which model the expected value, one can choose to model the median of the distribution (Koenker, 2005). The significance of the new variable is assessed via a rank based test calibrated with an F distribution (Gutenbrunner et al., 1993). The reason for this is that we performed simulation studies and saw that this type of test attains the type I error in contrast to the log-likelihood ratio test. The benefit of this regression is that it is robust, in contrast to the classical linear regression. If the target variable consists of proportions or percentages (within the (0, 1) interval), the logit transformation is applied beforehand.
</p>
</li>
<li> <p><b>testIndBeta</b>. When the target is proportion (or percentage, i.e., between 0 and 1, not inclusive) the user can fit a regression model assuming a beta distribution. The predictor variables can be either continuous, categorical or mixed. The procedure is the same as in the <b>testIndReg</b> case. 
</p>
</li>
<li> <p><b>Alternatives to testIndBeta</b>. Instead of <b>testIndBeta</b> the user has the option to choose all the previous to that mentioned tests by transforming the target variable with the logit transformation. In this way, the support of the target becomes the whole of R^d and then depending on the type of the predictors and whether a robust approach is required or not, there is a variety of alternative to beta regression tests. 
</p>
</li>
<li> <p><b>testIndIGreg</b>. When you have non negative data, i.e. the target variable takes positive values (including 0), a suggested regression is based on the the inverse gaussian distribution. The link function is not the inverse of the square root as expected, but the logarithm. This is to ensure that the fitted values will be always be non negative. The predictor variables can be either continuous, categorical or mixed. The significance between the two models is assessed via the log-likelihood ratio test. Alternatively, the user can use the Weibull regression (<b>censIndWR</b>), gamma regression (<b>testIndGamma</b>) or Gaussian regression with log link (<b>testIndNormLog</b>). 
</p>
</li>
<li> <p><b>testIndGamma</b>. This is an alternative to <b>testIndIGreg</b>.
</p>
</li>
<li> <p><b>testIndNormLog</b>. This is a second alternative to <b>testIndIGreg</b>.
</p>
</li>
<li> <p><b>testIndPois</b>. When the target is discrete, and in specific count data, the default test is via the Poisson regression. The predictor variables can be either continuous, categorical or mixed. The procedure is the same as in all the previously regression model based tests, i.e. the log-likelihood ratio test is used to assess the conditional independence of the variable of interest. 
</p>
</li>
<li> <p><b>testIndNB</b>. As an alternative to the Poisson regression, we have included the Negative binomial regression to capture cases of overdispersion. The predictor variables can be either continuous, categorical or mixed. 
</p>
</li>
<li> <p><b>testIndQPois</b>. This is a better alternative for discrete target, better than the <b>testIndPois</b> and than the <b>testIndNB</b>, because it can capture both cases of overdispersion and undersidpesrion.
</p>
</li>
<li> <p><b>testIndZIP</b>. When the number of zeros is more than expected under a Poisson model, the zero inflated poisson regression is to be employed. The predictor variables can be either continuous, categorical or mixed. 
</p>
</li>
<li> <p><b>testIndLogistic</b>. When the target is categorical with only two outcomes, success or failure for example, then a binary logistic regression is to be used. Whether regression or classification is the task of interest, this method is applicable. The advantage of this over a linear or quadratic discriminant analysis is that it allows for categorical predictor variables as well and for mixed types of predictors.
</p>
</li>
<li> <p><b>testIndQBinom</b>. This is an alternative to either the <b>testIndLogistic</b> or especially the <b>testIndBeta</b>.
</p>
</li>
<li> <p><b>testIndMultinom</b>. If the target has more than two outcomes, but it is of nominal type, there is no ordering of the outcomes, multinomial logistic regression will be employed. Again, this regression is suitable for classification purposes as well and it to allows for categorical predictor variables.
</p>
</li>
<li> <p><b>testIndOrdinal</b>. This is a special case of multinomial regression, in which case the outcomes have an ordering, such as <b>not satisfied</b>, <b>neutral</b>, <b>satisfied</b>. The appropriate method is ordinal logistic regression. 
</p>
</li>
<li> <p><b>testIndBinom</b>. When the target variable is a matrix of two columns, where the first one is the number of successes and the second one is the number of trials, binomial regression is to be used. 
</p>
</li>
<li> <p><b>gSquare</b>. If all variables, both the target and predictors are categorical the default test is the G-square test of independence. It is similar to the chi-squared test of independence, but instead of using the chi-squared metric between the observed and estimated frequencies in contingency tables, the Kullback-Leibler divergence of the observed from the estimated frequencies is used. The asymptotic distribution of the test statistic is a chi-squared distribution on some appropriate degrees of freedom. The target variable can be either ordered or unordered with two or more outcomes. 
</p>
</li>
<li> <p><b>Alternatives to gSquare</b>. An alternative to the <b>gSquare</b> test is the <b>testIndLogistic</b>. Depending on the nature of the target, binary, un-ordered multinomial or ordered multinomial the appropriate regression model is fitted. 
</p>
</li>
<li> <p><b>censIndCR</b>. For the case of time-to-event data, a Cox regression model is employed. The predictor variables can be either continuous, categorical or mixed. Again, the log-likelihood ratio test is used to assess the significance of the new variable. 
</p>
</li>
<li> <p><b>censIndWR</b>. A second model for the case of time-to-event data, a Weibull regression model is employed. The predictor variables can be either continuous, categorical or mixed. Again, the log-likelihood ratio test is used to assess the significance of the new variable. Unlike the semi-parametric Cox model, the Weibull model is fully parametric.
</p>
</li>
<li> <p><b>censIndER</b>. A third model for the case of time-to-event data, an exponential regression model is employed. The predictor variables can be either continuous, categorical or mixed. Again, the log-likelihood ratio test is used to assess the significance of the new variable. This is a special case of the Weibull model.
</p>
</li>
<li> <p><b>testIndClogit</b>. When the data come from a case-control study, the suitable test is via conditional logistic regression. 
</p>
</li>
<li> <p><b>testIndMVreg</b>. In the case of multivariate continuous targets, the suggested test is via a multivariate linear regression. 
The target variable can be compositional data as well. These are positive data, whose vectors sum to 1.  They can sum to any constant, as 
long as it the same, but for convenience reasons we assume that they are normalised to sum to 1.  In this case the additive log-ratio 
transformation (multivariate logit transformation) is applied beforehand. 
</p>
</li>
<li> <p><b>testIndSPML</b>. With a circular target, the projected bivariate normal distribution (Presnell et al., 1998) is used to perform regression. 
</p>
</li></ol>



<h3>Tests for clustered/longitudinal data</h3>


<ol>
<li> <p><b>testIndGLMMReg</b>, <b>testIndGLMM</b>, <b>testIndGLMMPois</b> &amp; <b>testIndGLMMLogistic</b>. In the case of a longitudinal or 
clustered targets (continuous, proportions, binary or counts), the suggested test is via a (generalised) linear mixed model. 
<b>testIndGLMMCR</b> stands for mixed effects Cox regression. 
</p>
</li>
<li> <p><b>testIndGEEReg</b>, <b>testIndGEELogistic</b>, <b>testIndGEEPois</b>, <b>testIndGEENormLog</b> and <b>testIndGEEGamma</b>. 
In the case of a longitudinal or clustered targets (continuous, proportions, binary, counts, positive, strictly positive), 
the suggested test is via GEE (Generalised Estimating Equations).  
</p>
</li></ol>



<h3>Wald based tests</h3>

<p>The available tests for wald.ses and wald.mmpc are listed below. Note, that only continuous predictors are allowed. 
</p>

<table>
<tr>
 <td style="text-align: left;"> 
  <b>Target variable</b> </td><td style="text-align: left;"> <b>Available tests</b> </td><td style="text-align: left;">	<b>Short explanation</b> </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  
  Continuous  </td><td style="text-align: left;"> waldMMReg </td><td style="text-align: left;"> MM regression </td>
</tr>
<tr>
 <td style="text-align: left;">  
  Proportions </td><td style="text-align: left;"> waldMMReg </td><td style="text-align: left;"> MM regression </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  </td><td style="text-align: left;"> </td><td style="text-align: left;"> after logit transformation </td>
</tr>
<tr>
 <td style="text-align: left;">  
  Proportions </td><td style="text-align: left;"> waldBeta </td><td style="text-align: left;"> Beta regression </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Non negative </td><td style="text-align: left;"> waldIGreg </td><td style="text-align: left;"> Inverse Gaussian regression </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Strictly positive </td><td style="text-align: left;"> waldGamma </td><td style="text-align: left;"> Gamma regression </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Non negative </td><td style="text-align: left;"> waldNormLog </td><td style="text-align: left;"> Gaussian regression with log link </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Successes &amp; totals </td><td style="text-align: left;"> testIndBinom </td><td style="text-align: left;"> Binomial regression </td>
</tr>
<tr>
 <td style="text-align: left;">
  Discrete </td><td style="text-align: left;"> waldPois </td><td style="text-align: left;"> Poisson regression </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Discrete </td><td style="text-align: left;"> waldSpeedPois </td><td style="text-align: left;"> Poisson regression </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Discrete </td><td style="text-align: left;"> waldZIP </td><td style="text-align: left;"> Zero Inflated </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  </td><td style="text-align: left;">	</td><td style="text-align: left;">  Poisson regression </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Discrete </td><td style="text-align: left;"> waldNB </td><td style="text-align: left;"> Negative binomial regression </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Factor with two </td><td style="text-align: left;"> waldLogistic </td><td style="text-align: left;"> Logistic regression </td>
</tr>
<tr>
 <td style="text-align: left;">
  levels or binary </td><td style="text-align: left;"> </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">  
  Factor with more than </td><td style="text-align: left;"> waldOrdinal </td><td style="text-align: left;"> Ordinal logistic regression </td>
</tr>
<tr>
 <td style="text-align: left;">		 
  two levels (ordered) </td><td style="text-align: left;"> </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">  
  Left censored </td><td style="text-align: left;"> waldTobit </td><td style="text-align: left;"> Tobit regression </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Case-control  </td><td style="text-align: left;"> Mixed </td><td style="text-align: left;"> testIndClogit </td>
</tr>
<tr>
 <td style="text-align: left;">
  </td><td style="text-align: left;">  </td><td style="text-align: left;">  Conditional logistic regression </td>
</tr>
<tr>
 <td style="text-align: left;">
  Survival </td><td style="text-align: left;"> waldCR </td><td style="text-align: left;"> Cox regression </td>
</tr>
<tr>
 <td style="text-align: left;">
  Survival </td><td style="text-align: left;"> waldWR </td><td style="text-align: left;"> Weibull regression </td>
</tr>
<tr>
 <td style="text-align: left;">
  Survival </td><td style="text-align: left;"> waldER </td><td style="text-align: left;"> Exponential regression </td>
</tr>
<tr>
 <td style="text-align: left;">
  Survival </td><td style="text-align: left;"> waldLLR </td><td style="text-align: left;"> Log-logistic regression </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Permutation based tests</h3>

<p>The available tests for perm.ses and perm.mmpc are listed below. Note, that only continuous predictors are allowed. 
</p>

<table>
<tr>
 <td style="text-align: left;"> 
  <b>Target variable</b> </td><td style="text-align: left;"> <b>Available tests</b> </td><td style="text-align: left;">	<b>Short explanation</b> </td>
</tr>
<tr>
 <td style="text-align: left;"> 

  Continuous  </td><td style="text-align: left;"> permFisher </td><td style="text-align: left;"> Pearson correlation </td>
</tr>
<tr>
 <td style="text-align: left;">  
  Continuous  </td><td style="text-align: left;"> permMMFisher </td><td style="text-align: left;"> Robust Pearson correlation </td>
</tr>
<tr>
 <td style="text-align: left;">  
  Continuous  </td><td style="text-align: left;"> permDcor </td><td style="text-align: left;"> Distance correlation </td>
</tr>
<tr>
 <td style="text-align: left;">  
  Continuous  </td><td style="text-align: left;"> permReg </td><td style="text-align: left;"> Linear regression </td>
</tr>
<tr>
 <td style="text-align: left;">  
  Proportions </td><td style="text-align: left;"> permReg </td><td style="text-align: left;"> Linear regression </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  </td><td style="text-align: left;"> </td><td style="text-align: left;"> after logit transformation </td>
</tr>
<tr>
 <td style="text-align: left;">  
  Proportions </td><td style="text-align: left;"> permBeta </td><td style="text-align: left;"> Beta regression </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Non negative </td><td style="text-align: left;"> permIGreg </td><td style="text-align: left;"> Inverse Gaussian regression </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Strictly positive </td><td style="text-align: left;"> permGamma </td><td style="text-align: left;"> Gamma regression </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Non negative </td><td style="text-align: left;"> permNormLog </td><td style="text-align: left;"> Gaussian regression with log link </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Non negative </td><td style="text-align: left;"> permWR </td><td style="text-align: left;"> Weibull regression </td>
</tr>
<tr>
 <td style="text-align: left;">
  Successes &amp; totals </td><td style="text-align: left;"> permBinom </td><td style="text-align: left;"> Binomial regression </td>
</tr>
<tr>
 <td style="text-align: left;">
  Discrete </td><td style="text-align: left;"> permPois </td><td style="text-align: left;"> Poisson regression </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Discrete </td><td style="text-align: left;"> permZIP </td><td style="text-align: left;"> Zero Inflated </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  </td><td style="text-align: left;">	</td><td style="text-align: left;">  Poisson regression </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Discrete </td><td style="text-align: left;"> permNB </td><td style="text-align: left;"> Negative binomial regression </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Factor with two </td><td style="text-align: left;"> permLogistic </td><td style="text-align: left;"> Binary logistic regression </td>
</tr>
<tr>
 <td style="text-align: left;">
  levels or binary </td><td style="text-align: left;"> </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">  
  Factor with more than </td><td style="text-align: left;"> permMultinom </td><td style="text-align: left;"> Multinomial logistic regression </td>
</tr>
<tr>
 <td style="text-align: left;">		 
  two levels (nominal) </td><td style="text-align: left;"> </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Factor with more than </td><td style="text-align: left;"> permOrdinal </td><td style="text-align: left;"> Ordinal logistic regression </td>
</tr>
<tr>
 <td style="text-align: left;">		 
  two levels (ordered) </td><td style="text-align: left;"> </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">  
  Left censored </td><td style="text-align: left;"> permTobit </td><td style="text-align: left;"> Tobit regression </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  Survival	 </td><td style="text-align: left;"> permCR </td><td style="text-align: left;"> Cox regression </td>
</tr>
<tr>
 <td style="text-align: left;">
  Survival	 </td><td style="text-align: left;"> permWR </td><td style="text-align: left;"> Weibull regression </td>
</tr>
<tr>
 <td style="text-align: left;">
  Survival	 </td><td style="text-align: left;"> permER </td><td style="text-align: left;"> Exponential regression </td>
</tr>
<tr>
 <td style="text-align: left;">
  Survival	 </td><td style="text-align: left;"> permLLR </td><td style="text-align: left;"> Log-logistic regression </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Michail Tsagris &lt;mtsagris@uoc.gr&gt;
</p>


<h3>References</h3>

<p>Aitchison J. (1986). The Statistical Analysis of Compositional Data, Chapman &amp; Hall; reprinted in 2003, with additional material, by The Blackburn Press.
</p>
<p>Brown P.J. (1994). Measurement, Regression and Calibration. Oxford Science Publications.
</p>
<p>Cox D.R. (1972). Regression models and life-tables. J. R. Stat. Soc., 34, 187-220.
</p>
<p>Demidenko E. (2013). Mixed Models: Theory and Applications with R, 2nd Edition. New Jersey: Wiley &amp; Sons.
</p>
<p>Draper, N.R. and Smith H. (1988). Applied regression analysis. New York, Wiley, 3rd edition.
</p>
<p>Fieller E.C. and Pearson E.S. (1961). Tests for rank correlation coefficients: II. Biometrika, 48(1 &amp; 2): 29-40.
</p>
<p>Ferrari S.L.P. and Cribari-Neto F. (2004). Beta Regression for Modelling Rates and Proportions. Journal of Applied Statistics, 31(7): 799-815.
</p>
<p>Gail, M.H., Jay H.L., and Lawrence V.R. (1981). Likelihood calculations for matched case-control studies and survival studies with tied death times. Biometrika 68(3): 703-707.
</p>
<p>Gutenbrunner C., Jureckova J., Koenker R. and Portnoy S. (1993). Tests of Linear Hypothesis based on Regression Rank Scores, Journal of NonParametric Statistics 2, 307-331.
</p>
<p>Hoerl A.E. and Kennard R.W. (1970). Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12(1): 55-67.
</p>
<p>Joseph M.H. (2011). Negative Binomial Regression. Cambridge University Press, 2nd edition.
</p>
<p>Koenker R.W. (2005). Quantile Regression. Cambridge University Press.
</p>
<p>Lagani V., Kortas G. and Tsamardinos I. (2013). Biomarker signature identification in &quot;omics&quot; with multiclass outcome. Computational and Structural Biotechnology Journal, 6(7): 1-7.
</p>
<p>Lagani V. and Tsamardinos I. (2010). Structure-based variable selection for survival data. Bioinformatics Journal 16(15): 1887-1894.
</p>
<p>Lambert D. (1992). Zero-inflated Poisson regression, with an application to defects in manufacturing. Technometrics 34(1)1: 1-14.
</p>
<p>Liang K.Y. and Zeger S.L. (1986). Longitudinal data analysis using generalized linear models. Biometrika, 73(1): 13-22.
</p>
<p>Mardia K.V., Kent J.T. and Bibby J.M. (1979). Multivariate Analysis. Academic Press, New York, USA. 
</p>
<p>Maronna R.D. Yohai M.V. (2006). Robust Statistics, Theory and Methods. Wiley.
</p>
<p>McCullagh P. and Nelder J.A. (1989). Generalized linear models. CRC press, USA, 2nd edition.
</p>
<p>Paik M.C. (1988). Repeated measurement analysis for nonnormal data in small samples. Communications in Statistics-Simulation and Computation, 17(4): 1155-1171.
</p>
<p>Pinheiro J., and D. Bates. Mixed-effects models in S and S-PLUS. Springer Science &amp; Business Media, 2006.
</p>
<p>Prentice R.L. and Zhao L.P. (1991). Estimating equations for parameters in means and covariances of multivariate discrete and continuous responses. Biometrics, 47(3): 825-839.
</p>
<p>Presnell Brett, Morrison Scott P. and Littell Ramon C. (1998). Projected multivariate linear models for directional data. Journal of the American Statistical Association, 93(443): 1068-1077.
</p>
<p>Scholz, F. W. (2001). Maximum likelihood estimation for type I censored Weibull data including covariates. ISSTECH-96-022, Boeing Information &amp; Support Services.
</p>
<p>Smith, R. L. (1991). Weibull regression models for reliability data. Reliability Engineering &amp; System Safety, 34(1), 55-76.
</p>
<p>Spirtes P.,  Glymour C. and Scheines R. (2001). Causation, Prediction, and Search. The MIT Press, Cambridge, MA, USA, 3nd edition.
</p>
<p>Szekely G.J. and Rizzo, M.L. (2014). Partial distance correlation with methods for dissimilarities. The Annals of Statistics, 42(6): 2382&ndash;2412.
</p>
<p>Szekely G.J. and Rizzo M.L. (2013).  Energy statistics: A class of statistics based on distances. Journal of Statistical Planning and Inference 143(8): 1249&ndash;1272.
</p>
<p>Therneau T.M., Grambsch P.M. and Pankratz V.S. (2003). Penalized Survival Models and Frailty, Journal of Computational and Graphical Statistics, 12(1):156-175.
</p>
<p>Yan J. and Fine J. (2004). Estimating equations for association structures. Statistics in medicine, 23(6): 859-874
</p>
<p>Ziegler A., Kastner C., Brunner D. and Blettner M. (2000). Familial associations of lipid proles: A generalised estimating equations approach. Statistics in medicine, 19(24): 3345-3357
</p>

<hr>
<h2 id='Conditional+20independence+20regression+20based+20tests'>
Conditional independence regression based tests
</h2><span id='topic+cond.regs'></span><span id='topic+glmm.condregs'></span><span id='topic+gee.condregs'></span>

<h3>Description</h3>

<p>Conditional independence regression based tests.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cond.regs(target, dataset, xIndex, csIndex, test = NULL, wei = NULL, ncores = 1)

glmm.condregs(target, reps = NULL, id, dataset, xIndex, csIndex, test, wei = NULL, 
slopes = FALSE, ncores = 1)

gee.condregs(target, reps = NULL, id, dataset, xIndex, csIndex, test, wei = NULL, 
correl = "echangeable", se = "jack", ncores = 1)     
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Conditional+2B20independence+2B20regression+2B20based+2B20tests_+3A_target">target</code></td>
<td>

<p>The target (dependent) variable. It must be a numerical vector. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20regression+2B20based+2B20tests_+3A_dataset">dataset</code></td>
<td>

<p>The indendent variable(s). For the &quot;univregs&quot; this can be a matrix or a dataframe with continuous only variables, a data frame with mixed or only categorical variables. For the &quot;wald.univregs&quot;, &quot;perm.univregs&quot;, &quot;score.univregs&quot; and &quot;rint.regs&quot; this can only by a numerical matrix.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20regression+2B20based+2B20tests_+3A_xindex">xIndex</code></td>
<td>

<p>The indices of the variables whose association with the target you want to test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20regression+2B20based+2B20tests_+3A_csindex">csIndex</code></td>
<td>

<p>The index or indices of the variable(s) to condition on. If this is 0, the the function <code><a href="#topic+univregs">univregs</a></code> will be called.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20regression+2B20based+2B20tests_+3A_test">test</code></td>
<td>

<p>For the &quot;cond.regs&quot; one of the following: <b>testIndBeta</b>, <b>testIndReg</b>, <b>testIndLogistic</b>, <b>testIndOrdinal</b>, <b>testIndPois</b>, <b>testIndQPois</b>, <b>testIndZIP</b>, <b>testIndNB</b>, <b>testIndClogit</b>, <b>testIndBinom</b>, <b>testIndQBinom</b>, <b>testIndIGreg</b>, <b>censIndCR</b>, <b>censIndWR</b>, <b>censIndER</b>, <b>censIndLLR</b>, <b>testIndMMReg</b>, <b>testIndMVreg</b>, <b>testIndMultinom</b>, <b>testIndTobit</b>, <b>testIndGamma</b>, <b>testIndNormLog</b> or <b>testIndSPML</b> for a circular target. 
</p>
<p>For the &quot;glmm.condregs&quot; one of the following: <b>testIndLMReg</b>, <b>testIndGLMMLogistic</b>, 
<b>testIndGLMMPois</b>, <b>testIndGLMMOrdinal</b>,  <b>testIndGLMMGamma</b> or <b>testIndGLMMNormLog</b>. 
</p>
<p>For the &quot;gee.condregs&quot; one of the following: <b>testIndGEEReg</b>, <b>testIndGEELogistic</b>, 
<b>testIndGEEPois</b>, <b>testIndGEEGamma</b> or <b>testIndGEENormLog</b>. 
</p>
<p>Note that in all cases you must give the name of the test, without &quot; &quot;. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20regression+2B20based+2B20tests_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. An example where weights are used is surveys when stratified sampling has occured. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20regression+2B20based+2B20tests_+3A_ncores">ncores</code></td>
<td>

<p>How many cores to use. This plays an important role if you have tens of thousands of variables or really large sample 
sizes and tens of thousands of variables and a regression based test which requires numerical optimisation. In other 
cases it will not make a difference in the overall time (in fact it can be slower). The parallel computation is used in the 
first step of the algorithm, where univariate associations are examined, those take place in parallel. We have seen a reduction 
in time of 50% with 4 cores in comparison to 1 core. Note also, that the amount of reduction is not linear in the number of cores.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20regression+2B20based+2B20tests_+3A_id">id</code></td>
<td>

<p>A numerical vector of the same length as target with integer valued numbers, such as 1, 2, 3,... (zeros, negative values and 
factors are not allowed) specifying the clusters or subjects. This argument is for the rint.regs (see details for more information).
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20regression+2B20based+2B20tests_+3A_reps">reps</code></td>
<td>

<p>If you have measurements over time (lognitudinal data) you can put the time here (the length must be equal to the length of the target) 
or set it equal to NULL. (see details for more information).
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20regression+2B20based+2B20tests_+3A_slopes">slopes</code></td>
<td>

<p>Should random slopes for the ime effect be fitted as well? Default value is FALSE. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20regression+2B20based+2B20tests_+3A_correl">correl</code></td>
<td>

<p>The correlation structure. For the Gaussian, Logistic, Poisson and Gamma regression this can be either &quot;exchangeable&quot; 
(compound symmetry, suitable for clustered data) or &quot;ar1&quot; (AR(1) model, suitable for longitudinal data). 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20regression+2B20based+2B20tests_+3A_se">se</code></td>
<td>

<p>The method for estimating standard errors. This is very important and crucial. The available options for Gaussian, Logistic, 
Poisson and Gamma regression are: a)  'san.se', the usual robust estimate. b) 'jack': if approximate jackknife variance estimate 
should be computed. c) 'j1s': if 1-step jackknife variance estimate should be computed and d) 'fij': logical indicating if fully 
iterated jackknife variance estimate should be computed. If you have many clusters (sets of repeated measurements) &quot;san.se&quot; is 
fine as it is astmpotically correct, plus jacknife estimates will take longer. If you have a few clusters, then maybe 
it's better to use jacknife estimates. 
</p>
<p>The jackknife variance estimator was suggested by Paik (1988), which is quite suitable for cases when the number of subjects is 
small (K &lt; 30), as in many biological studies. The simulation studies conducted by Ziegler et al. (2000) and Yan and Fine (2004) 
showed that the approximate jackknife estimates are in many cases in good agreement with the fully iterated ones. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is more as a help function for MMPC, but it can also be called directly by the user. In some, one should specify 
the regression model to use and the function will perform all simple regressions, i.e. all regression models between the target 
and each of the variables in the dataset. The function does not check for zero variance columns, only the &quot;univregs&quot; and 
related functions do. 
</p>
<p>If you want to use the GEE methodology, make sure you load the library geepack first. 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>stat</code></td>
<td>

<p>The value of the test statistic.
</p>
</td></tr>
<tr><td><code>pvalue</code></td>
<td>

<p>The <b>logarithm of the p-value</b> of the test. 
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Chen J. and Chen Z. (2008). Extended Bayesian information criteria for model selection with large model spaces. 
Biometrika, 95(3): 759-771.
</p>
<p>Eugene Demidenko (2013). Mixed Models: Theory and Applications with R, 2nd Edition. New Jersey: Wiley &amp; Sons.
</p>
<p>McCullagh, Peter, and John A. Nelder. Generalized linear models.  CRC press, USA, 2nd edition, 1989.
</p>
<p>Presnell Brett, Morrison Scott P. and Littell Ramon C. (1998). Projected multivariate linear models for directional data. Journal of the American Statistical Association, 93(443): 1068-1077.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+univregs">univregs</a>, <a href="#topic+SES">SES</a>, <a href="#topic+MMPC">MMPC</a>, <a href="#topic+CondIndTests">CondIndTests</a>, <a href="#topic+reg.fit">reg.fit</a>, <a href="#topic+ridge.reg">ridge.reg</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rpois(100, 15)
x &lt;- matrix( rnorm(100 * 10), ncol = 10)
a1 &lt;- univregs(y, x, test = testIndPois)
a4 &lt;- cond.regs(y, as.data.frame(x), xIndex = 1:9, csIndex = 10, test = testIndPois)
</code></pre>

<hr>
<h2 id='Conditional+20independence+20test+20for+20binary+2C+20categorical+20or+20ordinal+20data'>
Conditional independence test for binary, categorical or ordinal class variables
</h2><span id='topic+testIndLogistic'></span><span id='topic+testIndMultinom'></span><span id='topic+testIndOrdinal'></span><span id='topic+testIndQBinom'></span><span id='topic+permLogistic'></span><span id='topic+permMultinom'></span><span id='topic+permOrdinal'></span><span id='topic+waldLogistic'></span><span id='topic+waldQBinom'></span><span id='topic+waldOrdinal'></span>

<h3>Description</h3>

<p>The main task of this test is to provide a p-value PVALUE for the null hypothesis: feature 'X' is independent from 'TARGET' given a conditioning set CS. The pvalue is calculated by comparing a logistic model based on the conditioning set CS against a model whose regressor are both X and CS. The comparison is performed through a chi-square test with the aproprirate degrees of freedom on the difference between the deviances of the two models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>testIndLogistic(target, dataset, xIndex, csIndex, wei = NULL,
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL)

testIndMultinom(target, dataset, xIndex, csIndex, wei = NULL,
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL)

testIndOrdinal(target, dataset, xIndex, csIndex, wei = NULL,
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL)

testIndQBinom(target, dataset, xIndex, csIndex, wei = NULL,  
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL)

permLogistic(target, dataset, xIndex, csIndex, wei = NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL, 
threshold = 0.05, R = 999)

permMultinom(target, dataset, xIndex, csIndex, wei = NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL, 
threshold = 0.05, R = 999)

permOrdinal(target, dataset, xIndex, csIndex, wei = NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL, 
threshold = 0.05, R = 999)

waldLogistic(target, dataset, xIndex, csIndex, wei = NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL)

waldQBinom(target, dataset, xIndex, csIndex, wei = NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL)

waldOrdinal(target, dataset, xIndex, csIndex, wei = NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20binary+2B2C+2B20categorical+2B20or+2B20ordinal+2B20data_+3A_target">target</code></td>
<td>

<p>A numeric vector containing the values of the target variable. For the &quot;testIndLogistic&quot; this can either be a binary numerical variable or a factor variable. The factor variable can have two values (binary logistic regression), more than two values (multinomial logistic regression) or it can be an ordered factor with more than two values (ordinal regression). The last one is for example, factor(x, ordered = TRUE). The &quot;waldBinary&quot; is the Wald test version of the binary logistic regression. The &quot;waldOrdinal&quot; is the Wald test version of the ordinal regression.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20binary+2B2C+2B20categorical+2B20or+2B20ordinal+2B20data_+3A_dataset">dataset</code></td>
<td>

<p>A numeric matrix or data frame, in case of categorical predictors (factors), containing the variables for performing the test. Rows as samples and columns as features. In the cases of &quot;waldBinary&quot;, &quot;waldOrdinal&quot; this is strictly a matrix. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20binary+2B2C+2B20categorical+2B20or+2B20ordinal+2B20data_+3A_xindex">xIndex</code></td>
<td>

<p>The index of the variable whose association with the target we want to test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20binary+2B2C+2B20categorical+2B20or+2B20ordinal+2B20data_+3A_csindex">csIndex</code></td>
<td>

<p>The indices of the variables to condition on. If you have no variables set this equal to 0.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20binary+2B2C+2B20categorical+2B20or+2B20ordinal+2B20data_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL.  An example where weights are used is surveys when stratified sampling has occured.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20binary+2B2C+2B20categorical+2B20or+2B20ordinal+2B20data_+3A_univariatemodels">univariateModels</code></td>
<td>

<p>Fast alternative to the hash object for univariate test. List with vectors &quot;pvalues&quot; (p-values), &quot;stats&quot; (statistics) and &quot;flags&quot; (flag = TRUE if the test was succesful) representing the univariate association of each variable with the target. Default value is NULL.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20binary+2B2C+2B20categorical+2B20or+2B20ordinal+2B20data_+3A_hash">hash</code></td>
<td>

<p>A boolean variable which indicates whether (TRUE) or not (FALSE) to use the hash-based implementation of the statistics of SES. Default value is FALSE. If TRUE you have to specify the stat_hash argument and the pvalue_hash argument.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20binary+2B2C+2B20categorical+2B20or+2B20ordinal+2B20data_+3A_stat_hash">stat_hash</code></td>
<td>

<p>A hash object which contains the cached generated statistics of a SES run in the current dataset, using the current test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20binary+2B2C+2B20categorical+2B20or+2B20ordinal+2B20data_+3A_pvalue_hash">pvalue_hash</code></td>
<td>

<p>A hash object which contains the cached generated p-values of a SES run in the current dataset, using the current test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20binary+2B2C+2B20categorical+2B20or+2B20ordinal+2B20data_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0, 1)) for assessing p-values significance. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20binary+2B2C+2B20categorical+2B20or+2B20ordinal+2B20data_+3A_r">R</code></td>
<td>

<p>The number of permutations, set to 999 by default. There is a trick to avoind doing all permutations. As soon as the number of times the permuted test statistic is more than the observed test statistic is more than 50 (if threshold = 0.05 and R = 999), the p-value has exceeded the signifiance level (threshold value) and hence the predictor variable is not significant. There is no need to continue do the extra permutations, as a decision has already been made. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The testIndQBinom does quasi binomial regression which is suitable for binary targets and proportions including 0 and 1.  
</p>
<p>If hash = TRUE, testIndLogistic requires the arguments 'stat_hash' and 'pvalue_hash' for the hash-based implementation of the statistic test. These hash Objects are produced or updated by each run of SES (if hash == TRUE) and they can be reused in order to speed up next runs of the current statistic test. If &quot;SESoutput&quot; is the output of a SES run, then these objects can be retrieved by SESoutput@hashObject$stat_hash and the SESoutput@hashObject$pvalue_hash.
</p>
<p>Important: Use these arguments only with the same dataset that was used at initialization. For all the available conditional independence tests that are currently included on the package, please see &quot;?CondIndTests&quot;.
</p>
<p>The log-likelihood ratio test used in &quot;testIndLogistic&quot; requires the fitting of two models. The Wald test used in waldBinary&quot; and &quot;waldOrdinal&quot; requires fitting of only one model, the full one. The significance of the variable is examined only. Only continuous (or binary) predictor variables are currently accepted in this test. 
</p>
<p>The <code><a href="#topic+testIndQBinom">testIndQBinom</a></code> can also be used with percentages, see <code><a href="#topic+testIndBeta">testIndBeta</a></code> for example.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>pvalue</code></td>
<td>

<p>A numeric value that represents the logarithm of the generated p-value.
</p>
</td></tr>
<tr><td><code>stat</code></td>
<td>

<p>A numeric value that represents the generated statistic.
</p>
</td></tr>
<tr><td><code>stat_hash</code></td>
<td>

<p>The current hash object used for the statistics. See argument stat_hash and details. If argument hash = FALSE this is NULL.
</p>
</td></tr>
<tr><td><code>pvalue_hash</code></td>
<td>

<p>The current hash object used for the p-values. See argument stat_hash and details. If argument hash = FALSE this is NULL.
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This test uses the function multinom (package nnet) for multinomial logistic regression, the function clm (package ordinal) for ordinal logit regression and the function glm (package stats) for binomial regression.
</p>


<h3>Author(s)</h3>

<p>Vincenzo Lagani and Ioannis Tsamardinos 
</p>
<p>R implementation and documentation: Vincenzo Lagani &lt;vlagani@csd.uoc.gr&gt;, Giorgos Athineou &lt;athineou@csd.uoc.gr&gt; and Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Vincenzo Lagani, George Kortas and Ioannis Tsamardinos (2013), Biomarker signature identification in &quot;omics&quot; with multiclass outcome. Computational and Structural Biotechnology Journal, 6(7):1-7.
</p>
<p>McCullagh, Peter, and John A. Nelder. Generalized linear models.  CRC press, USA, 2nd edition, 1989.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SES">SES</a>, <a href="#topic+gSquare">gSquare</a>, <a href="#topic+CondIndTests">CondIndTests</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#simulate a dataset with categorical data 
dataset_m &lt;- matrix( sample(c(0, 1, 2), 20 * 100, replace = TRUE), ncol = 20)
#initialize categorical target
target_m &lt;- dataset_m[, 20]
#remove target from the dataset
dataset_m &lt;- dataset_m[, -20]

#run the conditional independence test for the nominal class variable
testIndMultinom(target_m, dataset_m, xIndex = 14, csIndex = c(1, 2) )

########################################################################
#run the conditional independence test for the ordinal class variable
testIndOrdinal( factor(target_m, ordered = TRUE), dataset_m, 
xIndex = 14, csIndex = c(1, 2) )

#run the SES algorithm using the testIndLogistic conditional independence test 
#for the ordinal class variable
sesObject &lt;- SES(factor(target_m, ordered=TRUE), dataset_m, max_k = 3, 
threshold = 0.05, test = "testIndOrdinal")

########################################################################
#simulate a dataset with binary data
dataset_b &lt;- matrix(sample(c(0,1), 50 * 20, replace = TRUE), ncol = 20)
#initialize binary target
target_b &lt;- dataset_b[, 20]
#remove target from the dataset
dataset_b &lt;- dataset_b[, -20]

#run the conditional independence test for the binary class variable
testIndLogistic( target_b, dataset_b, xIndex = 14, csIndex = c(1, 2) )

#run the MMPC algorithm using the testIndLogistic conditional independence test
#for the binary class variable
mmpcObject &lt;- MMPC(target_b, dataset_b, max_k = 3, threshold = 0.05, 
test = "testIndLogistic")
</code></pre>

<hr>
<h2 id='Conditional+20independence+20test+20for+20case+20control+20data'>
Conditional independence test based on conditional logistic regression for case control studies 
</h2><span id='topic+testIndClogit'></span><span id='topic+permClogit'></span>

<h3>Description</h3>

<p>The main task of this test is to provide a p-value PVALUE for the null hypothesis: feature 'X' is independent from 'TARGET' given a conditioning set CS. The pvalue is calculated by comparing a conditional logistic regression model based on the conditioning set CS against a model whose regressors are both X and CS. The comparison is performed through a chi-square test with the appropriate degrees of freedom on the difference between the deviances of the two models. This is suitable for a case control design
</p>


<h3>Usage</h3>

<pre><code class='language-R'>testIndClogit(target, dataset, xIndex, csIndex, wei = NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL)

permClogit(target, dataset, xIndex, csIndex, wei = NULL,  
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL, 
threshold = 0.05, R = 999)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20case+2B20control+2B20data_+3A_target">target</code></td>
<td>

<p>A matrix with two columns, the first one must be 0 and 1, standing for 0 = control and 1 = case. The second column is the id of the patients. A numerical variable, for example c(1,2,3,4,5,6,7,1,2,3,4,5,6,7). 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20case+2B20control+2B20data_+3A_dataset">dataset</code></td>
<td>

<p>A numeric matrix or a data.frame in case of categorical predictors (factors), containing the variables for performing the test. Rows as samples and columns as features.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20case+2B20control+2B20data_+3A_xindex">xIndex</code></td>
<td>

<p>The index of the variable whose association with the target we want to test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20case+2B20control+2B20data_+3A_csindex">csIndex</code></td>
<td>

<p>The indices of the variables to condition on. If you have no variables set this equal to 0.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20case+2B20control+2B20data_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL and it should stay NULL as weights are ignored at the conditional logistic regression. See the <b>survival</b> package for more information about conditional logistic regression.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20case+2B20control+2B20data_+3A_univariatemodels">univariateModels</code></td>
<td>

<p>Fast alternative to the hash object for univariate test. List with vectors &quot;pvalues&quot; (p-values), &quot;stats&quot; (statistics) and &quot;flags&quot; (flag = TRUE if the test was succesful) representing the univariate association of each variable with the target. Default value is NULL.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20case+2B20control+2B20data_+3A_hash">hash</code></td>
<td>

<p>A boolean variable which indicates whether (TRUE) or not (FALSE) to use tha hash-based implementation of the statistics of SES. Default value is FALSE. If TRUE you have to specify the stat_hash argument and the pvalue_hash argument.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20case+2B20control+2B20data_+3A_stat_hash">stat_hash</code></td>
<td>

<p>A hash object which contains the cached generated statistics of a SES run in the current dataset, using the current test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20case+2B20control+2B20data_+3A_pvalue_hash">pvalue_hash</code></td>
<td>

<p>A hash object which contains the cached generated p-values of a SES run in the current dataset, using the current test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20case+2B20control+2B20data_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0, 1)) for assessing p-values significance. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20case+2B20control+2B20data_+3A_r">R</code></td>
<td>

<p>The number of permutations, set to 999 by default. There is a trick to avoind doing all permutations. As soon as the number of times the permuted test statistic is more than the observed test statistic is more than 50 (if threshold = 0.05 and R = 999), the p-value has exceeded the signifiance level (threshold value) and hence the predictor variable is not significant. There is no need to continue do the extra permutations, as a decision has already been made. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If hash = TRUE, testIndClogit requires the arguments 'stat_hash' and 'pvalue_hash' for the hash-based implementation of the statistic test. These hash Objects are produced or updated by each run of SES (if hash == TRUE) and they can be reused in order to speed up next runs of the current statistic test. If &quot;SESoutput&quot; is the output of a SES run, then these objects can be retrieved by SESoutput@hashObject$stat_hash and the SESoutput@hashObject$pvalue_hash.
</p>
<p>Important: Use these arguments only with the same dataset that was used at initialization. For all the available conditional independence tests that are currently included on the package, please see &quot;?CondIndTests&quot;.
</p>
<p>This is for case control studies. The log-likelihood for a conditional logistic regression model equals the log-likelihood from a Cox model with a particular data structure. When a well tested Cox model routine is available many packages use this &quot;trick&quot; rather than writing a new software routine from scratch, and this is what the &quot;clogit&quot; function in the &quot;survival&quot; package does. In detail, a stratified Cox model with each case/control group assigned to its own stratum, time set to a constant, status of 1=case 0=control, and using the exact partial likelihood has the same likelihood formula as a conditional logistic regression. The &quot;clogit&quot; routine creates the necessary dummy variable of times (all 1) and the strata, then calls the function &quot;coxph&quot;. 
</p>
<p>Note that this function is a bit sensitive and prone to breaking down for some reason which I have not yet figured out. In case of singular design matrix it will work, it identifies the problem and handles it internally like most regression functions do. However, problems can still occur.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>pvalue</code></td>
<td>

<p>A numeric value that represents the logarithm of the generated p-value due to the conditional logistic regression (see reference below).
</p>
</td></tr>
<tr><td><code>stat</code></td>
<td>

<p>A numeric value that represents the generated statistic due to the conditional logistic regression (see reference below).
</p>
</td></tr>
<tr><td><code>stat_hash</code></td>
<td>

<p>The current hash object used for the statistics. See argument stat_hash and details. If argument hash = FALSE this is NULL.
</p>
</td></tr>
<tr><td><code>pvalue_hash</code></td>
<td>

<p>The current hash object used for the p-values. See argument stat_hash and details. If argument hash = FALSE this is NULL.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Vincenzo Lagani, Ioannis Tsamardinos, Giorgos Athineou and Michail Tsagris
</p>
<p>R implementation and documentation: Giorgos Athineou &lt;athineou@csd.uoc.gr&gt;, Vincenzo Lagani &lt;vlagani@csd.uoc.gr&gt; and Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Michell H. Gail, Jay H. Lubin and Lawrence V. Rubinstein (1980). Likelihood calculations for matched case-control studies and survival studies with tied death times. Biometrika 68:703-707. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SES">SES</a>, <a href="#topic+testIndLogistic">testIndLogistic</a>, <a href="#topic+censIndCR">censIndCR</a>, <a href="#topic+censIndWR">censIndWR</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#simulate a dataset with continuous data
dataset &lt;- matrix( rnorm(100 * 7), nrow = 100 ) 
#the target feature is the last column of the dataset as a vector
case &lt;- rbinom(100, 1, 0.6)
ina &lt;- which(case == 1)
ina &lt;- sample(ina, 50)
case[-ina] = 0 
id &lt;- rep(1:50, 2)
target &lt;- cbind(case, id)

results &lt;- testIndClogit(target, dataset, xIndex = 4, csIndex = 1)
results

#run the SES algorithm using the testIndClogit conditional independence test
a &lt;- MMPC(target, dataset, max_k = 3, threshold = 0.01, test = "testIndClogit")
</code></pre>

<hr>
<h2 id='Conditional+20independence+20test+20for+20circular+20data'>
Circular regression conditional independence test for circular class dependent variables and continuous predictors.
</h2><span id='topic+testIndSPML'></span>

<h3>Description</h3>

<p>The main task of this test is to provide a p-value PVALUE for the null hypothesis: feature 'X' is independent from 'TARGET' given a conditioning set CS. The pvalue is calculated by comparing a Beta regression model based on the conditioning set CS against a model whose regressor are both X and CS. The comparison is performed through a log-likelihood ratio test using circular regression with 2 degrees of freedom.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>testIndSPML(target, dataset, xIndex, csIndex, wei = NULL, univariateModels = NULL, 
hash = FALSE, stat_hash = NULL, pvalue_hash = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20circular+2B20data_+3A_target">target</code></td>
<td>

<p>A numerical vector with the data expressed in radians, or a 2 column matrix with the cosine and sine of the response (i.e. unit vector). See examples for more information on this. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20circular+2B20data_+3A_dataset">dataset</code></td>
<td>

<p>A numeric matrix with the variables for performing the test. Rows as samples and columns as features. currently this test accepts only continuous variables.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20circular+2B20data_+3A_xindex">xIndex</code></td>
<td>

<p>The index of the variable whose association with the target we want to test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20circular+2B20data_+3A_csindex">csIndex</code></td>
<td>

<p>The indices of the variables to condition on. If you have no variables set this equal to 0.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20circular+2B20data_+3A_wei">wei</code></td>
<td>

<p>This is not used in this test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20circular+2B20data_+3A_univariatemodels">univariateModels</code></td>
<td>

<p>Fast alternative to the hash object for univariate test. List with vectors &quot;pvalues&quot; (p-values), &quot;stats&quot; (statistics) representing the univariate association of each variable with the target. Default value is NULL.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20circular+2B20data_+3A_hash">hash</code></td>
<td>

<p>A boolean variable which indicates whether (TRUE) or not (FALSE) to use tha hash-based implementation of the statistics of SES. Default value is FALSE. If TRUE you have to specify the stat_hash argument and the pvalue_hash argument.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20circular+2B20data_+3A_stat_hash">stat_hash</code></td>
<td>

<p>A hash object which contains the cached generated statistics of a SES run in the current dataset, using the current test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20circular+2B20data_+3A_pvalue_hash">pvalue_hash</code></td>
<td>

<p>A hash object which contains the cached generated p-values of a SES run in the current dataset, using the current test.
</p>
</td></tr>






</table>


<h3>Details</h3>

<p>If hash = TRUE, testIndSPML requires the arguments 'stat_hash' and 'pvalue_hash' for the hash-based implementation of the statistic test. These hash Objects are produced or updated by each run of SES (if hash == TRUE) and they can be reused in order to speed up next runs of the current statistic test. If &quot;SESoutput&quot; is the output of a SES run, then these objects can be retrieved by SESoutput@hashObject$stat_hash and the SESoutput@hashObject$pvalue_hash.
</p>
<p>Important: Use these arguments only with the same dataset that was used at initialization. For all the available conditional independence tests that are currently included on the package, please see &quot;?CondIndTests&quot;.
</p>
<p>The log-likelihood ratio test used in &quot;testIndSPML&quot; requires the fitting of two models.  The significance of the variable is examined and Only continuous (or binary) predictor variables are currently accepted in this test. 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>pvalue</code></td>
<td>

<p>A numeric value that represents the logarithm of the generated p-value due to Beta regression (see reference below).
</p>
</td></tr>
<tr><td><code>stat</code></td>
<td>

<p>A numeric value that represents the generated statistic due to Beta regression (see reference below).
</p>
</td></tr>
<tr><td><code>stat_hash</code></td>
<td>

<p>The current hash object used for the statistics. See argument stat_hash and details. If argument hash = FALSE this is NULL.
</p>
</td></tr>
<tr><td><code>pvalue_hash</code></td>
<td>

<p>The current hash object used for the logged p-values. See argument stat_hash and details. If argument hash = FALSE this is NULL.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Presnell Brett, Morrison Scott P. and Littell Ramon C. (1998). Projected multivariate linear models for directional data. Journal of the American Statistical Association, 93(443): 1068-1077.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+univregs">univregs</a>, <a href="#topic+SES">SES</a>, <a href="#topic+testIndReg">testIndReg</a>, <a href="#topic+CondIndTests">CondIndTests</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- runif(100, - pi, pi)  ## suppose these are radians
x &lt;- matrix( rnorm(100 * 3), ncol = 3)
testIndSPML(y1, x, csIndex = 1, xIndex = 2)
## alternatively
y1 &lt;- cbind(cos(y), sin(y)) ## a matrix with two columns
testIndSPML(y1, x, csIndex = 1, xIndex = 2)
</code></pre>

<hr>
<h2 id='Conditional+20independence+20test+20for+20longitudinal+20and+20clustered+20data+20using+20GEE'>
Linear mixed models conditional independence test for longitudinal class variables
</h2><span id='topic+testIndGEEReg'></span><span id='topic+testIndGEELogistic'></span><span id='topic+testIndGEEPois'></span><span id='topic+testIndGEEGamma'></span><span id='topic+testIndGEENormLog'></span>

<h3>Description</h3>

<p>The main task of this test is to provide a p-value PVALUE for the null hypothesis: feature 'X' is independent from 'TARGET' given a conditioning set CS. The pvalue is calculated by comparing a linear model based on the conditioning set CS against a model with both X and CS. The comparison is performed through an F test the appropriate degrees of freedom on the difference between the deviances of the two models. This test accepts a longitudinal target and longitudinal, categorical, continuous or mixed data as predictor variables. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>testIndGEEReg(target, reps = NULL, group, dataset, xIndex, csIndex,  wei =  NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL, 
correl = "exchangeable", se = "jack")

testIndGEELogistic(target, reps = NULL, group, dataset, xIndex, csIndex,  wei =  NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL, 
correl = "exchangeable", se = "jack")

testIndGEEPois(target, reps = NULL, group, dataset, xIndex, csIndex,  wei =  NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL, 
correl = "exchangeable", se = "jack")

testIndGEEGamma(target, reps = NULL, group, dataset, xIndex, csIndex,  wei =  NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL, 
correl = "exchangeable", se = "jack")

testIndGEENormLog(target, reps = NULL, group, dataset, xIndex, csIndex,  wei =  NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL, 
correl = "exchangeable", se = "jack")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data+2B20using+2B20GEE_+3A_target">target</code></td>
<td>

<p>A numeric vector containing the values of the target variable. If the values are proportions or percentages, i.e. strictly within 0 and 1 they are mapped into R using log( target/(1 - target) ). In both cases a linear mixed model is applied. It can also be a binary variable (binary logistic regression) or a discrete, counts (Poisson regression), thus fitting generalised linear mixed models.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data+2B20using+2B20GEE_+3A_reps">reps</code></td>
<td>

<p>A numeric vector containing the time points of the subjects. It's length is equal to the length of the target variable. If you have clustered data, leave this NULL. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data+2B20using+2B20GEE_+3A_group">group</code></td>
<td>

<p>A numeric vector containing the subjects or groups. It must be of the same length as target. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data+2B20using+2B20GEE_+3A_dataset">dataset</code></td>
<td>

<p>A numeric matrix or data frame, in case of categorical predictors (factors), containing the variables for performing the test. Rows as samples and columns as features.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data+2B20using+2B20GEE_+3A_xindex">xIndex</code></td>
<td>

<p>The index of the variable whose association with the target we want to test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data+2B20using+2B20GEE_+3A_csindex">csIndex</code></td>
<td>

<p>The indices of the variables to condition on. If you have no variables set this equal to 0.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data+2B20using+2B20GEE_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. 
It is mentioned in the &quot;geepack&quot; that weights is not (yet) the weight as in sas proc genmod, and hence is not recommended to use.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data+2B20using+2B20GEE_+3A_univariatemodels">univariateModels</code></td>
<td>

<p>Fast alternative to the hash object for univariate test. List with vectors &quot;pvalues&quot; (p-values), &quot;stats&quot; (statistics) and &quot;flags&quot; (flag = TRUE if the test was succesful) representing the univariate association of each variable with the target. Default value is NULL.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data+2B20using+2B20GEE_+3A_hash">hash</code></td>
<td>

<p>A boolean variable which indicates whether (TRUE) or not (FALSE) to use tha hash-based implementation of the statistics of SES. Default value is FALSE. If TRUE you have to specify the stat_hash argument and the pvalue_hash argument.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data+2B20using+2B20GEE_+3A_stat_hash">stat_hash</code></td>
<td>

<p>A hash object which contains the cached generated statistics of a SES run in the current dataset, using the current test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data+2B20using+2B20GEE_+3A_pvalue_hash">pvalue_hash</code></td>
<td>

<p>A hash object which contains the cached generated p-values of a SES run in the current dataset, using the current test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data+2B20using+2B20GEE_+3A_correl">correl</code></td>
<td>

<p>The correlation structure. For the Gaussian, Logistic, Poisson and Gamma regression this can be either &quot;exchangeable&quot; (compound symmetry, suitable for clustered data) or &quot;ar1&quot; (AR(1) model, suitable for longitudinal data). For the ordinal logistic regression its only the &quot;exchangeable&quot; correlation sturcture.  
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data+2B20using+2B20GEE_+3A_se">se</code></td>
<td>

<p>The method for estimating standard errors. This is very important and crucial. The available options for Gaussian, Logistic, Poisson and Gamma regression are: a)  'san.se', the usual robust estimate. b) 'jack': if approximate jackknife variance estimate should be computed. 
c) 'j1s': if 1-step jackknife variance estimate should be computed and d) 'fij': logical indicating if fully iterated jackknife variance estimate should be computed. If you have many clusters (sets of repeated measurements) &quot;san.se&quot; is fine as it is astmpotically correct, plus jacknife estimates will take longer. 
If you have a few clusters, then maybe it's better to use jacknife estimates. 
</p>
<p>The jackknife variance estimator was suggested by Paik (1988), which is quite suitable for cases when the number of subjects is small (K &lt; 30), as in many biological studies. The simulation studies conducted by Ziegler et al. (2000) and Yan and Fine (2004) showed that the approximate jackknife estimates are in many cases in good agreement with the fully iterated ones. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If hash = TRUE, testIndGEE requires the arguments 'stat_hash' and 'pvalue_hash' for the hash-based implementation of the statistic test. These hash Objects are produced or updated by each run of SES (if hash == TRUE) and they can be reused in order to speed up next runs of the current statistic test. If &quot;SESoutput&quot; is the output of a SES.temp run, then these objects can be retrieved by SESoutput@hashObject$stat_hash and the SESoutput@hashObject$pvalue_hash.
</p>
<p>Important: Use these arguments only with the same dataset that was used at initialization. For all the available conditional independence tests that are currently included on the package, please see &quot;?CondIndTests&quot;.
</p>
<p>This test is for longitudinal and clustered data. Bear in mind that the time effect, for the longitudinal data case, is linear. It could be of higer order as well, but this would be a hyper-parameter, increasing the complexity of the models to be tested. 
</p>
<p>Make sure you load the library geepack first. 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>pvalue</code></td>
<td>

<p>A numeric value that represents the logarithm of the generated p-value due to the (generalised) linear mixed model (see reference below).
</p>
</td></tr>
<tr><td><code>stat</code></td>
<td>

<p>A numeric value that represents the generated statistic due to the (generalised) linear mixed model (see reference below).
</p>
</td></tr>
<tr><td><code>stat_hash</code></td>
<td>

<p>The current hash object used for the statistics. See argument stat_hash and details. If argument hash = FALSE this is NULL.
</p>
</td></tr>
<tr><td><code>pvalue_hash</code></td>
<td>

<p>The current hash object used for the p-values. See argument stat_hash and details. If argument hash = FALSE this is NULL.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Liang K.Y. and Zeger S.L. (1986). Longitudinal data analysis using generalized linear models. Biometrika, 73(1): 13-22.
</p>
<p>Prentice R.L. and Zhao L.P. (1991). Estimating equations for parameters in means and covariances of multivariate discrete and continuous responses. Biometrics, 47(3): 825-839.
</p>
<p>Heagerty P.J. and Zeger S.L. (1996) Marginal regression models for clustered ordinal measurements. Journal of the American Statistical Association, 91(435): 1024-1036.
</p>
<p>Paik M.C. (1988). Repeated measurement analysis for nonnormal data in small samples. Communications in Statistics-Simulation and Computation, 17(4): 1155-1171.
</p>
<p>Ziegler A., Kastner C., Brunner D. and Blettner M. (2000). Familial associations of lipid profiles: A generalised estimating equations approach. Statistics in medicine, 19(24): 3345-3357
</p>
<p>Yan J. and Fine J. (2004). Estimating equations for association structures. Statistics in medicine, 23(6): 859-874.
</p>
<p>Eugene Demidenko (2013). Mixed Models: Theory and Applications with R, 2nd Edition. New Jersey: Wiley &amp; Sons.
</p>
<p>Tsagris, M., Lagani, V., &amp; Tsamardinos, I. (2018). Feature selection for high-dimensional glmm data. BMC bioinformatics, 19(1), 17.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SES.glmm">SES.glmm</a>, <a href="#topic+MMPC.glmm">MMPC.glmm</a>, <a href="#topic+CondIndTests">CondIndTests</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("geepack", quietly = TRUE)
y &lt;- rnorm(150)
x &lt;- matrix(rnorm(150 * 5), ncol = 5)
id &lt;- sample(1:20, 150, replace = TRUE)
testIndGEEReg(y, group = id, dataset = x, xIndex = 1, csIndex = 3)
</code></pre>

<hr>
<h2 id='Conditional+20independence+20test+20for+20longitudinal+20and+20clustered+20data+20using+20GLMM'>
Linear mixed models conditional independence test for longitudinal class variables
</h2><span id='topic+testIndGLMMReg'></span><span id='topic+testIndGLMMLogistic'></span><span id='topic+testIndGLMMPois'></span><span id='topic+testIndGLMMNB'></span><span id='topic+testIndGLMMGamma'></span><span id='topic+testIndGLMMNormLog'></span><span id='topic+testIndGLMMOrdinal'></span><span id='topic+testIndGLMMCR'></span><span id='topic+testIndLMM'></span>

<h3>Description</h3>

<p>The main task of this test is to provide a p-value PVALUE for the null hypothesis: feature 'X' is independent from 'TARGET' given a conditioning set CS. The pvalue is calculated by comparing a linear model based on the conditioning set CS against a model with both X and CS. The comparison is performed through an F test the appropriate degrees of freedom on the difference between the deviances of the two models. This test accepts a longitudinal target and longitudinal, categorical, continuous or mixed data as predictor variables. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>testIndGLMMReg(target, reps = NULL, group, dataset, xIndex, csIndex, wei = NULL,
univariateModels = NULL, hash = FALSE, stat_hash = NULL, 
pvalue_hash = NULL, slopes = FALSE)

testIndGLMMLogistic(target, reps = NULL, group, dataset, xIndex, csIndex, wei = NULL,
univariateModels = NULL, hash = FALSE, stat_hash = NULL, 
pvalue_hash = NULL, slopes = FALSE)

testIndGLMMPois(target, reps = NULL, group, dataset, xIndex, csIndex, wei = NULL,
univariateModels = NULL, hash = FALSE, stat_hash = NULL, 
pvalue_hash = NULL, slopes = FALSE)

testIndGLMMNB(target, reps = NULL, group, dataset, xIndex, csIndex, wei = NULL,
univariateModels = NULL, hash = FALSE, stat_hash = NULL, 
pvalue_hash = NULL, slopes = FALSE)

testIndGLMMGamma(target, reps = NULL, group, dataset, xIndex, csIndex, wei = NULL,
univariateModels = NULL, hash = FALSE, stat_hash = NULL, 
pvalue_hash = NULL, slopes = FALSE)

testIndGLMMNormLog(target, reps = NULL, group, dataset, xIndex, csIndex, wei = NULL,
univariateModels = NULL, hash = FALSE, stat_hash = NULL, 
pvalue_hash = NULL, slopes = FALSE)

testIndGLMMOrdinal(target, reps = NULL, group, dataset, xIndex, csIndex, wei = NULL,
univariateModels = NULL, hash = FALSE, stat_hash = NULL, 
pvalue_hash = NULL, slopes = FALSE)

testIndGLMMCR(target, reps = NULL, group, dataset, xIndex, csIndex, wei = NULL,
univariateModels = NULL, hash = FALSE, stat_hash = NULL, 
pvalue_hash = NULL, slopes = FALSE)

testIndLMM(target, reps = NULL, group, dataset, xIndex, csIndex, wei = NULL,
univariateModels = NULL, hash = FALSE, stat_hash = NULL, 
pvalue_hash = NULL, slopes = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data+2B20using+2B20GLMM_+3A_target">target</code></td>
<td>

<p>A numeric vector containing the values of the target variable. If the values are proportions or percentages, i.e. strictly within 0 and 1 they are mapped into R using log( target/(1 - target) ). In both cases a linear mixed model is applied. It can also be a binary variable (binary logistic regression) or a discrete, counts (Poisson regression), thus fitting generalised linear mixed models. In the case of &quot;testIndGLMMOrdinal&quot; this must be an ordered factor.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data+2B20using+2B20GLMM_+3A_reps">reps</code></td>
<td>

<p>A numeric vector containing the time points of the subjects. It's length is equal to the length of the target variable. If you have clustered data, leave this NULL. This is not applied in ordinal and Cox regression. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data+2B20using+2B20GLMM_+3A_group">group</code></td>
<td>

<p>A numeric vector containing the subjects or groups. It must be of the same length as target. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data+2B20using+2B20GLMM_+3A_dataset">dataset</code></td>
<td>

<p>A numeric matrix or data frame, in case of categorical predictors (factors), containing the variables for performing the test. Rows as samples and columns as features.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data+2B20using+2B20GLMM_+3A_xindex">xIndex</code></td>
<td>

<p>The index of the variable whose association with the target we want to test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data+2B20using+2B20GLMM_+3A_csindex">csIndex</code></td>
<td>

<p>The indices of the variables to condition on. If you have no variables set this equal to 0.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data+2B20using+2B20GLMM_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. An example where weights are used is surveys when stratified sampling has occured.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data+2B20using+2B20GLMM_+3A_univariatemodels">univariateModels</code></td>
<td>

<p>Fast alternative to the hash object for univariate test. List with vectors &quot;pvalues&quot; (p-values), &quot;stats&quot; (statistics) and &quot;flags&quot; (flag = TRUE if the test was succesful) representing the univariate association of each variable with the target. Default value is NULL.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data+2B20using+2B20GLMM_+3A_hash">hash</code></td>
<td>

<p>A boolean variable which indicates whether (TRUE) or not (FALSE) to use tha hash-based implementation of the statistics of SES. Default value is FALSE. If TRUE you have to specify the stat_hash argument and the pvalue_hash argument.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data+2B20using+2B20GLMM_+3A_stat_hash">stat_hash</code></td>
<td>

<p>A hash object which contains the cached generated statistics of a SES run in the current dataset, using the current test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data+2B20using+2B20GLMM_+3A_pvalue_hash">pvalue_hash</code></td>
<td>

<p>A hash object which contains the cached generated p-values of a SES run in the current dataset, using the current test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data+2B20using+2B20GLMM_+3A_slopes">slopes</code></td>
<td>

<p>A boolean variable which indicates whether (TRUE) to or not (FALSE) random slopes in the time effect as well. By deault random intercepts are considered. This is not applied in ordinal and Cox regression.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If hash = TRUE, testIndGLMM requires the arguments 'stat_hash' and 'pvalue_hash' for the hash-based implementation of the statistic test. These hash Objects are produced or updated by each run of SES (if hash == TRUE) and they can be reused in order to speed up next runs of the current statistic test. If &quot;SESoutput&quot; is the output of a SES.temp run, then these objects can be retrieved by SESoutput@hashObject$stat_hash and the SESoutput@hashObject$pvalue_hash.
</p>
<p>Important: Use these arguments only with the same dataset that was used at initialization. For all the available conditional independence tests that are currently included on the package, please see &quot;?CondIndTests&quot;.
</p>
<p>This test is for longitudinal and clustered data. Bear in mind that the time effect, for the longitudinal data case, is linear. It could be of higer order as well, but this would be a hyper-parameter, increasing the complexity of the models to be tested. 
</p>
<p>The testIndLMM is used for linear mixed models with no weights, no slopes and no reps. This is a random intercepts model only. 
The advantage of this function is that it is tens of times faster than testIndGLMM. 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>pvalue</code></td>
<td>

<p>A numeric value that represents the logarithm of the generated p-value due to the (generalised) linear mixed model (see reference below).
</p>
</td></tr>
<tr><td><code>stat</code></td>
<td>

<p>A numeric value that represents the generated statistic due to the (generalised) linear mixed model (see reference below).
</p>
</td></tr>
<tr><td><code>stat_hash</code></td>
<td>

<p>The current hash object used for the statistics. See argument stat_hash and details. If argument hash = FALSE this is NULL.
</p>
</td></tr>
<tr><td><code>pvalue_hash</code></td>
<td>

<p>The current hash object used for the p-values. See argument stat_hash and details. If argument hash = FALSE this is NULL.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Vincenzo Lagani, Ioannis Tsamardinos, Michail Tsagris and Giorgos Athineou
</p>
<p>R implementation and documentation: Giorgos Athineou &lt;athineou@csd.gr&gt;, Vincenzo Lagani &lt;vlagani@csd.uoc.gr&gt; and Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Eugene Demidenko (2013). Mixed Models: Theory and Applications with R, 2nd Edition. New Jersey: Wiley &amp; Sons.
</p>
<p>Jose Pinheiro Jose and Douglas Bates. Mixed-effects models in S and S-PLUS. Springer Science &amp; Business Media, 2006.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SES.glmm">SES.glmm</a>, <a href="#topic+MMPC.glmm">MMPC.glmm</a>, <a href="#topic+CondIndTests">CondIndTests</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rnorm(150)
x &lt;- matrix(rnorm(150 * 5), ncol = 5)
id &lt;- sample(1:20, 150, replace = TRUE)
testIndGLMMReg(y, group = id, dataset = x, xIndex = 1, csIndex = 3)
testIndLMM(y, group = id, dataset = x, xIndex = 1, csIndex = 3)
</code></pre>

<hr>
<h2 id='Conditional+20independence+20test+20for+20proportions+2Fpercentages'>
Beta regression conditional independence test for proportions/percentage class dependent variables and mixed predictors
</h2><span id='topic+testIndBeta'></span><span id='topic+permBeta'></span><span id='topic+waldBeta'></span>

<h3>Description</h3>

<p>The main task of this test is to provide a p-value PVALUE for the null hypothesis: feature 'X' is independent from 'TARGET' given a conditioning set CS. The pvalue is calculated by comparing a Beta regression model based on the conditioning set CS against a model whose regressor are both X and CS. The comparison is performed through a chi-square test with the appropriate degrees of freedom on the difference between the deviances of the two models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>testIndBeta(target, dataset, xIndex, csIndex, wei = NULL,  
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL)

permBeta(target, dataset, xIndex, csIndex, wei = NULL,  
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL, 
threshold = 0.05, R = 999)

waldBeta(target, dataset, xIndex, csIndex, wei = NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20proportions+2B2Fpercentages_+3A_target">target</code></td>
<td>

<p>A numeric vector containing the values of the target variable. They must be percentages or proportions, i.e. within the (0, 1) interval. Currently 0 and/or 1 values are not allowed.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20proportions+2B2Fpercentages_+3A_dataset">dataset</code></td>
<td>

<p>A numeric matrix or data frame, in case of categorical predictors (factors), containing the variables for performing the test. Rows as samples and columns as features. In the case of &quot;waldBeta&quot; this is strictly a matrix. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20proportions+2B2Fpercentages_+3A_xindex">xIndex</code></td>
<td>

<p>The index of the variable whose association with the target we want to test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20proportions+2B2Fpercentages_+3A_csindex">csIndex</code></td>
<td>

<p>The indices of the variables to condition on. If you have no variables set this equal to 0.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20proportions+2B2Fpercentages_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. An example where weights are used is surveys when stratified sampling has occured. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20proportions+2B2Fpercentages_+3A_univariatemodels">univariateModels</code></td>
<td>

<p>Fast alternative to the hash object for univariate test. List with vectors &quot;pvalues&quot; (p-values), &quot;stats&quot; (statistics) and &quot;flags&quot; (flag = TRUE if the test was succesful) representing the univariate association of each variable with the target. Default value is NULL.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20proportions+2B2Fpercentages_+3A_hash">hash</code></td>
<td>

<p>A boolean variable which indicates whether (TRUE) or not (FALSE) to use tha hash-based implementation of the statistics of SES. Default value is FALSE. If TRUE you have to specify the stat_hash argument and the pvalue_hash argument.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20proportions+2B2Fpercentages_+3A_stat_hash">stat_hash</code></td>
<td>

<p>A hash object which contains the cached generated statistics of a SES run in the current dataset, using the current test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20proportions+2B2Fpercentages_+3A_pvalue_hash">pvalue_hash</code></td>
<td>

<p>A hash object which contains the cached generated p-values of a SES run in the current dataset, using the current test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20proportions+2B2Fpercentages_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0, 1)) for assessing p-values significance. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20proportions+2B2Fpercentages_+3A_r">R</code></td>
<td>

<p>The number of permutations, set to 999 by default. There is a trick to avoind doing all permutations. As soon as the number of times the permuted test statistic is more than the observed test statistic is more than 50 (if threshold = 0.05 and R = 999), the p-value has exceeded the signifiance level (threshold value) and hence the predictor variable is not significant. There is no need to continue do the extra permutations, as a decision has already been made. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If hash = TRUE, testIndBeta requires the arguments 'stat_hash' and 'pvalue_hash' for the hash-based implementation of the statistic test. These hash Objects are produced or updated by each run of SES (if hash == TRUE) and they can be reused in order to speed up next runs of the current statistic test. If &quot;SESoutput&quot; is the output of a SES run, then these objects can be retrieved by SESoutput@hashObject$stat_hash and the SESoutput@hashObject$pvalue_hash.
</p>
<p>Important: Use these arguments only with the same dataset that was used at initialization. For all the available conditional independence tests that are currently included on the package, please see &quot;?CondIndTests&quot;. An alternative regression to this is &quot;testIndReg&quot; and &quot;testIndRQ&quot; .In these two latter cases, the logit transformation is first applied to the target variable. 
</p>
<p>The log-likelihood ratio test used in &quot;testIndBeta&quot; requires the fitting of two models. The Wald test used in &quot;waldBeta&quot; requires fitting of only one model, the full one. The significance of the variable is examined only. Only continuous (or binary) predictor variables are currently accepted in this test. 
</p>
<p>The <code><a href="#topic+testIndQBinom">testIndQBinom</a></code> is another alternative to use.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>pvalue</code></td>
<td>

<p>A numeric value that represents the logarithm of the generated p-value due to Beta regression (see reference below).
</p>
</td></tr>
<tr><td><code>stat</code></td>
<td>

<p>A numeric value that represents the generated statistic due to Beta regression (see reference below).
</p>
</td></tr>
<tr><td><code>stat_hash</code></td>
<td>

<p>The current hash object used for the statistics. See argument stat_hash and details. If argument hash = FALSE this is NULL.
</p>
</td></tr>
<tr><td><code>pvalue_hash</code></td>
<td>

<p>The current hash object used for the logged p-values. See argument stat_hash and details. If argument hash = FALSE this is NULL.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Vincenzo Lagani, Ioannis Tsamardinos, Michail Tsagris and Giorgos Athineou
</p>
<p>R implementation and documentation: Giorgos Athineou &lt;athineou@csd.uoc.gr&gt;, Vincenzo Lagani &lt;vlagani@csd.uoc.gr&gt; and Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Ferrari S.L.P. and Cribari-Neto F. (2004). Beta Regression for Modelling Rates and Proportions. Journal of Applied Statistics, 31(7): 799-815.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+testIndQBinom">testIndQBinom</a>, <a href="#topic+SES">SES</a>, <a href="#topic+testIndReg">testIndReg</a>, <a href="#topic+testIndRQ">testIndRQ</a>, <a href="#topic+testIndFisher">testIndFisher</a>, <a href="#topic+CondIndTests">CondIndTests</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#simulate a dataset with continuous data
dataset &lt;- matrix(runif(100 * 10, 1, 1000), ncol = 10 ) 
#the target feature is the last column of the dataset as a vector
target &lt;- dataset[, 10]
dataset &lt;- dataset[, -10]
target &lt;- target / (max(target) + 2 )

testIndBeta(target, dataset, xIndex = 4, csIndex = 9)

#run the MMPC algorithm using the testIndBeta conditional independence test
mmpcObject &lt;- MMPC(target, dataset, max_k = 3, threshold = 0.05, test = "testIndBeta")
</code></pre>

<hr>
<h2 id='Conditional+20independence+20test+20for+20the+20static-longitudinal+20scenario'>
Conditional independence test for the static-longitudinal scenario
</h2><span id='topic+testIndTimeLogistic'></span><span id='topic+testIndTimeMultinom'></span>

<h3>Description</h3>

<p>The main task of this test is to provide a p-value PVALUE for the null hypothesis: feature 'X' is independent from 'TARGET' given a conditioning set CS. The pvalue is calculated by comparing a logistic model based on the conditioning set CS against a model whose regressor are both X and CS. The comparison is performed through a chi-square test with the aproprirate degrees of freedom on the difference between the deviances of the two models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>testIndTimeLogistic(target, dataset, xIndex, csIndex, wei = NULL,
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL)

testIndTimeMultinom(target, dataset, xIndex, csIndex, wei = NULL,
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20the+2B20static-longitudinal+2B20scenario_+3A_target">target</code></td>
<td>

<p>A numeric vector containing the values of the target variable. For the &quot;testIndLogistic&quot; this can either be a binary numerical variable or a factor variable. The factor variable can have two values (binary logistic regression), more than two values (multinomial logistic regression) or it can be an ordered factor with more than two values (ordinal regression). The last one is for example, factor(x, ordered = TRUE). The &quot;waldBinary&quot; is the Wald test version of the binary logistic regression. The &quot;waldOrdinal&quot; is the Wald test version of the ordinal regression.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20the+2B20static-longitudinal+2B20scenario_+3A_dataset">dataset</code></td>
<td>

<p>A numeric matrix with the constants and slopes stack one upo the other. The first r rows are the constants and the rest of the rows contains the slopes. In some the matrix can be calculated using the <code><a href="#topic+group.mvbetas">group.mvbetas</a></code> function. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20the+2B20static-longitudinal+2B20scenario_+3A_xindex">xIndex</code></td>
<td>

<p>The index of the variable whose association with the target we want to test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20the+2B20static-longitudinal+2B20scenario_+3A_csindex">csIndex</code></td>
<td>

<p>The indices of the variables to condition on. If you have no variables set this equal to 0.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20the+2B20static-longitudinal+2B20scenario_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. An example where weights are used is surveys when stratified sampling has occured. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20the+2B20static-longitudinal+2B20scenario_+3A_univariatemodels">univariateModels</code></td>
<td>

<p>Fast alternative to the hash object for univariate test. List with vectors &quot;pvalues&quot; (p-values), &quot;stats&quot; (statistics) and &quot;flags&quot; (flag = TRUE if the test was succesful) representing the univariate association of each variable with the target. Default value is NULL.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20the+2B20static-longitudinal+2B20scenario_+3A_hash">hash</code></td>
<td>

<p>A boolean variable which indicates whether (TRUE) or not (FALSE) to use the hash-based implementation of the statistics of SES. Default value is FALSE. If TRUE you have to specify the stat_hash argument and the pvalue_hash argument.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20the+2B20static-longitudinal+2B20scenario_+3A_stat_hash">stat_hash</code></td>
<td>

<p>A hash object which contains the cached generated statistics of a SES run in the current dataset, using the current test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20test+2B20for+2B20the+2B20static-longitudinal+2B20scenario_+3A_pvalue_hash">pvalue_hash</code></td>
<td>

<p>A hash object which contains the cached generated p-values of a SES run in the current dataset, using the current test.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This conditional independence test is devised for the static-longitudinal scenario of Tsagris, Lagani and Tsamardinos (2018). The idea is that you have many features of longitudinal data for many subjects. For each subject you have calculated the coefficients of a simple linear regression over time and this is repeated for each feature. In the end, assuming p features, you have p constants and p slopes for each subject, each constant and slope refers to a feature for a subject. 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>pvalue</code></td>
<td>

<p>A numeric value that represents the logarithm of the generated p-value.
</p>
</td></tr>
<tr><td><code>stat</code></td>
<td>

<p>A numeric value that represents the generated statistic.
</p>
</td></tr>
<tr><td><code>stat_hash</code></td>
<td>

<p>The current hash object used for the statistics. See argument stat_hash and details. If argument hash = FALSE this is NULL.
</p>
</td></tr>
<tr><td><code>pvalue_hash</code></td>
<td>

<p>The current hash object used for the p-values. See argument stat_hash and details. If argument hash = FALSE this is NULL.
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This test uses the function multinom (package nnet) for multinomial logistic regression, the function clm (package ordinal) for ordinal logit regression and the function glm (package stats) for binomial regression.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Tsagris M., Lagani V., &amp; Tsamardinos I. (2018). Feature selection for high-dimensional temporal data. BMC bioinformatics, 19(1), 17.
</p>
<p>Vincenzo Lagani, George Kortas and Ioannis Tsamardinos (2013), Biomarker signature identification in &quot;omics&quot; with multiclass outcome. Computational and Structural Biotechnology Journal, 6(7):1-7.
</p>
<p>McCullagh, Peter, and John A. Nelder. Generalized linear models.  CRC press, USA, 2nd edition, 1989.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SES">SES</a>, <a href="#topic+gSquare">gSquare</a>, <a href="#topic+CondIndTests">CondIndTests</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## assume these are longitudinal data, each column is a variable (or feature)
x &lt;- matrix( rnorm(400 * 50), ncol = 50 ) 
id &lt;- rep(1:80, each = 5)  ## 80 subjects
reps &lt;- rep( seq(4, 12, by = 2), 80)  ## 5 time points for each subject
dataset &lt;- group.mvbetas(x, id, reps)
## these are the regression coefficients of the first subject's values on the 
## reps (which is assumed to be time in this example)
target &lt;- rbinom(80, 1, 0.5)
testIndTimeLogistic(target, dataset, xIndex = 1, csIndex = 0)
testIndTimeLogistic(target, dataset, xIndex = 1, csIndex = 2)
</code></pre>

<hr>
<h2 id='Conditional+20independence+20tests+20counting+20the+20number+20of+20times+20a+20possible+20collider+20d-separates+20two+20nodes'>
Many conditional independence tests counting the number of times a possible collider d-separates two nodes
</h2><span id='topic+condis'></span>

<h3>Description</h3>

<p>Many conditional independence tests counting the number of times a possible collider d-separates two nodes
.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>condis(ind1, ind2, cs1, cs2, Var, dat, type = "pearson", rob = FALSE, max_k = 2, R = 1 )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20counting+2B20the+2B20number+2B20of+2B20times+2B20a+2B20possible+2B20collider+2B20d-separates+2B20two+2B20nodes_+3A_ind1">ind1</code></td>
<td>

<p>The index of the one variable to be considered. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20counting+2B20the+2B20number+2B20of+2B20times+2B20a+2B20possible+2B20collider+2B20d-separates+2B20two+2B20nodes_+3A_ind2">ind2</code></td>
<td>

<p>The index of the other variable to be considered. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20counting+2B20the+2B20number+2B20of+2B20times+2B20a+2B20possible+2B20collider+2B20d-separates+2B20two+2B20nodes_+3A_cs1">cs1</code></td>
<td>

<p>The index or indices of the conditioning set of variable(s). These are the neighbours of node ind1.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20counting+2B20the+2B20number+2B20of+2B20times+2B20a+2B20possible+2B20collider+2B20d-separates+2B20two+2B20nodes_+3A_cs2">cs2</code></td>
<td>

<p>The index or indices of the conditioning set of variable(s). These are the neighbours of node ind2.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20counting+2B20the+2B20number+2B20of+2B20times+2B20a+2B20possible+2B20collider+2B20d-separates+2B20two+2B20nodes_+3A_var">Var</code></td>
<td>

<p>The index of the possible collider. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20counting+2B20the+2B20number+2B20of+2B20times+2B20a+2B20possible+2B20collider+2B20d-separates+2B20two+2B20nodes_+3A_dat">dat</code></td>
<td>

<p>A numerical matrix or a data.frame with numerical, binary, nominal and ordinal variables only. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20counting+2B20the+2B20number+2B20of+2B20times+2B20a+2B20possible+2B20collider+2B20d-separates+2B20two+2B20nodes_+3A_type">type</code></td>
<td>

<p>This is either &quot;pearson&quot;, &quot;spearman&quot;, &quot;cat&quot;, &quot;distcor&quot;, &quot;ci.mm&quot; or &quot;ci.fast&quot;. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20counting+2B20the+2B20number+2B20of+2B20times+2B20a+2B20possible+2B20collider+2B20d-separates+2B20two+2B20nodes_+3A_rob">rob</code></td>
<td>

<p>In case you want robust estimation of the Pearson correlation prior to applying the conditional independence test. This is activated only when type = &quot;pearson&quot;.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20counting+2B20the+2B20number+2B20of+2B20times+2B20a+2B20possible+2B20collider+2B20d-separates+2B20two+2B20nodes_+3A_max_k">max_k</code></td>
<td>

<p>The maximum number of conditioning variables to consider. It can be the case that each node ind1 and ind2 has 10 neighbours. We should try all possible combinations of the neighbours of ind1 and then of ind2. To reduce the computational cost we search the subsets with at most max_k variables. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20counting+2B20the+2B20number+2B20of+2B20times+2B20a+2B20possible+2B20collider+2B20d-separates+2B20two+2B20nodes_+3A_r">R</code></td>
<td>

<p>This is used by most tests, except for type = &quot;ci.mm&quot; and type = &quot;ci.fast&quot;. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is to be used in the conservative version of Rule 0 of the Pc algorithm. When one wants to know whether a variable is a possible collider, Ramsey, Spirtes and Zhang (2005) propose to perform the following action. For every unshilded triple (X, Y, Z) check all subsets of X's possible parents and of Z's poissible parents. 
a) If Y is NOT in any such set conditional on which, X and Z are independent orient X - Y - Z as X -&gt; Y &lt;- Z. 
b) If Y is in ALL sets conditional on which, X and Z are independent, leave X - Y - Z as it is, i.e. a non-collider.
c) Mark the triple X - Y - Z as &quot;unfaitfull&quot; otherwise.
This modification leads to the so called conservative PC (CPC) algorithm.
</p>
<p>A few years later, Colombo and Maathuis (2014) suggested a modification of the previous action, called the majority rule. If Y is less than 50% of thet sets that render X and Z independent orient X - Y - Z as X -&gt; Y &lt;- Z. If Y is found in exactly 50% of sets that render X and Z independent, this triple is marked as &quot;ambiuous&quot;. This modification leads the so called majority rule PC (MPC) algorithm.
</p>
<p>This function we have implemented here, does exactly this. It applies tests to many subsets and returns a matrix with two columns. The first one contains 0 or 1 and the second is the p-value. A value of 0 indicates absenece of the possible collider from the set that produced that p-value, whereas a value of 1 indicates its presence in the set. 
</p>
<p>This way, we can measure the proportion of times the possible collider Y was in a subset that rendered X and Z independent.
</p>


<h3>Value</h3>

<p>A matrix with two columns. The second one is the logarithm of the p-value. The first one contains 0s and 1s. The value of 0 means that the candidate collider was not in that set which produced the relevant p-value, whereas a value of 1 indicates that it was a member of that conditioning set. 
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Ramsey, J., Zhang, J., Spirtes, P., 2006. Adjacency-faithfulness and conservative causal
inference. Proceedings of the 22nd Annual Conference on Uncertainty in Artificial Intelligence (UAI 2006).
<a href="https://arxiv.org/ftp/arxiv/papers/1206/1206.6843.pdf">https://arxiv.org/ftp/arxiv/papers/1206/1206.6843.pdf</a>
</p>
<p>Colombo, Diego, and Marloes H. Maathuis (2014). Order-independent constraint-based causal structure learning. The Journal of Machine Learning Research 15(1): 3741&ndash;3782.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+SES">SES</a>, <a href="#topic+MMPC">MMPC</a>, <a href="#topic+testIndLogistic">testIndLogistic</a> 
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rdag2(1000, p = 10, nei = 5)
G &lt;- x$G
dat &lt;- x$x
cs1 &lt;- which(G[6, ] &gt; 0  |  G[, 6] &gt; 0)
cs2 &lt;- which(G[7, ] &gt; 0  |  G[, 7] &gt; 0)
cs1 &lt;- setdiff( cs1, c(7, 3) )
cs2 &lt;- setdiff( cs2, c(6, 3) ) 
condis(6, 7, cs1, cs2, 3, dat, type = "pearson", rob = FALSE, max_k = 3, R = 1 )
</code></pre>

<hr>
<h2 id='Conditional+20independence+20tests+20for+20continous+20univariate+20and+20multivariate+20data+20'>
Linear (and non-linear) regression conditional independence test for continous univariate and multivariate response variables
</h2><span id='topic+testIndReg'></span><span id='topic+testIndRQ'></span><span id='topic+testIndMVreg'></span><span id='topic+testIndMMReg'></span><span id='topic+permReg'></span><span id='topic+permMMReg'></span><span id='topic+permRQ'></span><span id='topic+permMVreg'></span><span id='topic+waldMMReg'></span>

<h3>Description</h3>

<p>The main task of this test is to provide a p-value PVALUE for the null hypothesis: feature 'X' is independent from 'TARGET' given a conditioning set CS. The pvalue is calculated by comparing a linear regression model based on the conditioning set CS against a model whose regressor are both X and CS. The comparison is performed through an F test the appropriate degrees of freedom on the difference between the deviances of the two models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>testIndReg(target, dataset, xIndex, csIndex, wei = NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL)

testIndRQ(target, dataset, xIndex, csIndex, wei = NULL,
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL)

testIndMVreg(target, dataset, xIndex, csIndex, wei = NULL,
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL)

testIndMMReg(target, dataset, xIndex, csIndex, wei = NULL,  
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL)

waldMMReg(target, dataset, xIndex, csIndex, wei = NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL) 

permReg(target, dataset, xIndex, csIndex, wei = NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL, 
threshold = 0.05, R = 999)

permMMReg(target, dataset, xIndex, csIndex, wei = NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL, 
threshold = 0.05, R = 999)

permRQ(target, dataset, xIndex, csIndex, wei = NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL, 
threshold = 0.05, R = 999)

permMVreg(target, dataset, xIndex, csIndex, wei = NULL,
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL, 
threshold = 0.05, R = 999)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20continous+2B20univariate+2B20and+2B20multivariate+2B20data+2B20_+3A_target">target</code></td>
<td>

<p>A numeric vector containing the values of the target variable. If the values are proportions or percentages, i.e. strictly within 0 and 1 they are mapped into R using log( target/(1 - target) ). In the case of testIndMVreg, the same takes place true. See details for more information.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20continous+2B20univariate+2B20and+2B20multivariate+2B20data+2B20_+3A_dataset">dataset</code></td>
<td>

<p>A numeric matrix or data frame, in case of categorical predictors (factors), containing the variables for performing the test. Rows as samples and columns as features. In the cases of &quot;waldIGreg&quot; and &quot;waldMMreg&quot; this is strictly a matrix. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20continous+2B20univariate+2B20and+2B20multivariate+2B20data+2B20_+3A_xindex">xIndex</code></td>
<td>

<p>The index of the variable whose association with the target we want to test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20continous+2B20univariate+2B20and+2B20multivariate+2B20data+2B20_+3A_csindex">csIndex</code></td>
<td>

<p>The indices of the variables to condition on. If you have no variables set this equal to 0.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20continous+2B20univariate+2B20and+2B20multivariate+2B20data+2B20_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. An example where weights are used is surveys when stratified sampling has occured. 
They are not take into account in the robust regression via MM estimation (testIndMMReg and permMMreg).
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20continous+2B20univariate+2B20and+2B20multivariate+2B20data+2B20_+3A_univariatemodels">univariateModels</code></td>
<td>

<p>Fast alternative to the hash object for univariate test. List with vectors &quot;pvalues&quot; (p-values) and &quot;stats&quot; (statistics) representing the univariate association of each variable with the target. Default value is NULL.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20continous+2B20univariate+2B20and+2B20multivariate+2B20data+2B20_+3A_hash">hash</code></td>
<td>

<p>A boolean variable which indicates whether (TRUE) or not (FALSE) to use tha hash-based implementation of the statistics of SES. Default value is FALSE. If TRUE you have to specify the stat_hash argument and the pvalue_hash argument.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20continous+2B20univariate+2B20and+2B20multivariate+2B20data+2B20_+3A_stat_hash">stat_hash</code></td>
<td>

<p>A hash object which contains the cached generated statistics of a SES run in the current dataset, using the current test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20continous+2B20univariate+2B20and+2B20multivariate+2B20data+2B20_+3A_pvalue_hash">pvalue_hash</code></td>
<td>

<p>A hash object which contains the cached generated p-values of a SES run in the current dataset, using the current test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20continous+2B20univariate+2B20and+2B20multivariate+2B20data+2B20_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0, 1)) for assessing p-values significance. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20continous+2B20univariate+2B20and+2B20multivariate+2B20data+2B20_+3A_r">R</code></td>
<td>

<p>The number of permutations, set to 999 by default. There is a trick to avoind doing all permutations. As soon as the number of times the permuted test statistic is more than the observed test statistic is more than 50 (if threshold = 0.05 and R = 999), the p-value has exceeded the signifiance level (threshold value) and hence the predictor variable is not significant. There is no need to continue do the extra permutations, as a decision has already been made. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If hash = TRUE, all three tests require the arguments 'stat_hash' and 'pvalue_hash' for the hash-based implementation of the statistic test. These hash Objects are produced or updated by each run of SES (if hash == TRUE) and they can be reused in order to speed up next runs of the current statistic test. If &quot;SESoutput&quot; is the output of a SES run, then these objects can be retrieved by SESoutput@hashObject$stat_hash and the SESoutput@hashObject$pvalue_hash.
</p>
<p>Important: Use these arguments only with the same dataset that was used at initialization.
</p>
<p>TestIndReg offers linear regression. 
</p>
<p>testIndMMReg offers robust linear MM estimation regression. 
</p>
<p>TestIndRQ offers quantile (median) regression as a robust alternative to linear regression. 
</p>
<p>In both cases, if the dependent variable consists of proportions (values between 0 and 1) the logit transformation is applied and the tests are applied then. 
</p>
<p>testIndMVreg is for multivariate continuous response variables. Compositional data are positive multivariate data and each vector (observation) sums to the same constant, usually taken 1 for convenience. A check is performed and if such data are found, the additive log-ratio (multivariate logit) transformation (Aitchison, 1986) is applied beforehand. Zeros are not allowed. 
</p>
<p>For all the available conditional independence tests that are currently included on the package, please see &quot;?CondIndTests&quot;.
</p>
<p>The Wald test used in &quot;waldMMReg&quot; requires fitting of only one model, the full one. The significance of the variable is examined only. Only continuous (or binary) predictor variables are currently accepted in this test. 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>pvalue</code></td>
<td>

<p>A numeric value that represents the logarithm of the generated p-value due to linear regression (see reference below).
</p>
</td></tr>
<tr><td><code>stat</code></td>
<td>

<p>A numeric value that represents the generated statistic due to linear regression (see reference below).
</p>
</td></tr>
<tr><td><code>stat_hash</code></td>
<td>

<p>The current hash object used for the statistics. See argument stat_hash and details. If argument hash = FALSE this is NULL.
</p>
</td></tr>
<tr><td><code>pvalue_hash</code></td>
<td>

<p>The current hash object used for the p-values. See argument stat_hash and details. If argument hash = FALSE this is NULL.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Vincenzo Lagani, Ioannis Tsamardinos, Michail Tsagris and Giorgos Athineou
</p>
<p>R implementation and documentation: Giorgos Athineou &lt;athineou@csd.gr&gt;, Vincenzo Lagani &lt;vlagani@csd.uoc.gr&gt; and Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Draper, N.R. and Smith H. (1988). Applied regression analysis. New York, Wiley, 3rd edition.
</p>
<p>Hampel F. R., Ronchetti E. M., Rousseeuw P. J., and Stahel W. A. (1986). Robust statistics: the approach based on influence functions. John Wiley &amp; Sons. 
</p>
<p>Koenker R.W. (2005). Quantile regression. New York, Cambridge University Press.
</p>
<p>Sadovski A. (1974). L1-norm fit of a straight line. Applied Statistics, 23(2):244-248.
</p>
<p>Yohai, V. J. (1987). High breakdown-point and high efficiency robust estimates for regression. The Annals of Statistics, 15(2): 642-656.
</p>
<p>Mardia, Kanti, John T. Kent and John M. Bibby. Multivariate analysis. Academic press, 1979.
</p>
<p>John Aitchison. The Statistical Analysis of Compositional Data, Chapman &amp; Hall; reprinted in 2003, with additional material, by The Blackburn Press.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+testIndRQ">testIndRQ</a>, <a href="#topic+testIndFisher">testIndFisher</a>, <a href="#topic+testIndSpearman">testIndSpearman</a>, <a href="#topic+CondIndTests">CondIndTests</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#simulate a dataset with continuous data
dataset &lt;- matrix(runif(100 * 50, 1, 100), ncol = 50 )
#the target feature is the last column of the dataset as a vector
target &lt;- dataset[, 50]
dataset &lt;- dataset[, -50]

testIndReg(target, dataset, xIndex = 44, csIndex = 10)
testIndMMReg(target, dataset, xIndex = 44, csIndex = 10)
testIndRQ(target, dataset, xIndex = 44, csIndex = 10)
testIndIGreg(target, dataset, xIndex = 44, csIndex = 10)

#run the MMPC algorithm using the testIndReg conditional independence test
m1 &lt;- MMPC(target, dataset, max_k = 3, threshold = 0.05, test = "testIndReg")
m2 &lt;- MMPC(target, dataset, max_k = 3, threshold = 0.05, test = "testIndRQ")
</code></pre>

<hr>
<h2 id='Conditional+20independence+20tests+20for+20count+20data+20'>
Regression conditional independence test for discrete (counts) class dependent variables
</h2><span id='topic+testIndPois'></span><span id='topic+testIndNB'></span><span id='topic+testIndQPois'></span><span id='topic+testIndZIP'></span><span id='topic+permPois'></span><span id='topic+permNB'></span><span id='topic+permZIP'></span><span id='topic+waldPois'></span><span id='topic+waldNB'></span><span id='topic+waldZIP'></span>

<h3>Description</h3>

<p>The main task of this test is to provide a p-value PVALUE for the null hypothesis: feature 'X' is independent from 'TARGET' given a conditioning set CS. The pvalue is calculated by comparing a Poisson regression model based on the conditioning set CS against a model whose regressor are both X and CS. The comparison is performed through a chi-square test with the appropriate degrees of freedom on the difference between the deviances of the two models. The models supported here are poisson, zero inlftaed poisson and negative binomial.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>testIndPois(target, dataset, xIndex, csIndex, wei = NULL,  
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL)

testIndNB(target, dataset, xIndex, csIndex, wei = NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL)

testIndZIP(target, dataset, xIndex, csIndex, wei = NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL)

testIndQPois(target, dataset, xIndex, csIndex, wei = NULL,
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL)

permPois(target, dataset, xIndex, csIndex, wei = NULL,
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL, 
threshold = 0.05, R = 999)

permNB(target, dataset, xIndex, csIndex, wei = NULL,  
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL, 
threshold = 0.05, R = 999)

permZIP(target, dataset, xIndex, csIndex, wei = NULL,  
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL, 
threshold = 0.05, R = 999)

waldPois(target, dataset, xIndex, csIndex, wei = NULL,  
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL) 

waldNB(target, dataset, xIndex, csIndex, wei = NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL) 

waldZIP(target, dataset, xIndex, csIndex, wei = NULL,  
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20count+2B20data+2B20_+3A_target">target</code></td>
<td>

<p>A numeric vector containing the values of the target variable.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20count+2B20data+2B20_+3A_dataset">dataset</code></td>
<td>

<p>A numeric matrix or data frame, in case of categorical predictors (factors), containing the variables for performing the test. Rows as samples and columns as features. In the cases of &quot;waldPois&quot;, &quot;waldNB&quot; and &quot;waldZIP&quot; this is strictly a matrix. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20count+2B20data+2B20_+3A_xindex">xIndex</code></td>
<td>

<p>The index of the variable whose association with the target we want to test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20count+2B20data+2B20_+3A_csindex">csIndex</code></td>
<td>

<p>The indices of the variables to condition on. If you have no variables set this equal to 0.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20count+2B20data+2B20_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. An example where weights are used is surveys when stratified sampling has occured. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20count+2B20data+2B20_+3A_univariatemodels">univariateModels</code></td>
<td>

<p>Fast alternative to the hash object for univariate test. List with vectors &quot;pvalues&quot; (p-values), &quot;stats&quot; (statistics) and &quot;flags&quot; (flag = TRUE if the test was succesful) representing the univariate association of each variable with the target. Default value is NULL.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20count+2B20data+2B20_+3A_hash">hash</code></td>
<td>

<p>A boolean variable which indicates whether (TRUE) or not (FALSE) to use tha hash-based implementation of the statistics of SES. Default value is FALSE. If TRUE you have to specify the stat_hash argument and the pvalue_hash argument.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20count+2B20data+2B20_+3A_stat_hash">stat_hash</code></td>
<td>

<p>A hash object which contains the cached generated statistics of a SES run in the current dataset, using the current test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20count+2B20data+2B20_+3A_pvalue_hash">pvalue_hash</code></td>
<td>

<p>A hash object which contains the cached generated p-values of a SES run in the current dataset, using the current test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20count+2B20data+2B20_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0, 1)) for assessing p-values significance. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20count+2B20data+2B20_+3A_r">R</code></td>
<td>

<p>The number of permutations, set to 999 by default. There is a trick to avoind doing all permutations. As soon as the number of times the permuted test statistic is more than the observed test statistic is more than 50 (if threshold = 0.05 and R = 999), the p-value has exceeded the signifiance level (threshold value) and hence the predictor variable is not significant. There is no need to continue do the extra permutations, as a decision has already been made. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If hash = TRUE, all three tests require the arguments 'stat_hash' and 'pvalue_hash' for the hash-based implementation of the statistic test. These hash Objects are produced or updated by each run of SES (if hash == TRUE) and they can be reused in order to speed up next runs of the current statistic test. If &quot;SESoutput&quot; is the output of a SES run, then these objects can be retrieved by SESoutput@hashObject$stat_hash and the SESoutput@hashObject$pvalue_hash.
</p>
<p>Important: Use these arguments only with the same dataset that was used at initialization. For all the available conditional independence tests that are currently included on the package, please see &quot;?CondIndTests&quot;.
</p>
<p>If you have overdispersion, the variance is higher than the mean, a negative binomial is to be used. If you have more zeros than expected under a Poisson model, not overdispersion, then zero inlfated Poisson is to be used. Bear in mind that if you have a small number of zeros, there is no reason to use this model. If for example you have count data but no, or 1 zeros, this will not work.
</p>
<p>The log-likelihood ratio test used in &quot;testIndPois&quot;, &quot;testIndNB&quot; and &quot;testIndZIP&quot; requires the fitting of two models. The Wald test used in &quot;waldPois&quot;, &quot;waldNB&quot; and &quot;waldZIP&quot; requires fitting of only one model, the full one. The significance of the variable is examined only. Only continuous (or binary) predictor variables are currently accepted in this test. 
</p>
<p>The testIndQPois does quasi Poisson regression. The benefit of this regression is that it works for over and under dispersed data. Negative Binomial works for over dispersed data, but not for under dispersed data. In addition it is fast.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>pvalue</code></td>
<td>

<p>A numeric value that represents the logarithm of the generated p-value due to the count data regression (see references below).
</p>
</td></tr>
<tr><td><code>stat</code></td>
<td>

<p>A numeric value that represents the generated statistic due to Poisson regression(see reference below).
</p>
</td></tr>
<tr><td><code>stat_hash</code></td>
<td>

<p>The current hash object used for the statistics. See argument stat_hash and details. If argument hash = FALSE this is NULL.
</p>
</td></tr>
<tr><td><code>pvalue_hash</code></td>
<td>

<p>The current hash object used for the p-values. See argument stat_hash and details. If argument hash = FALSE this is NULL.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Vincenzo Lagani, Ioannis Tsamardinos, Michail Tsagris and Giorgos Athineou
</p>
<p>R implementation and documentation: Giorgos Athineou &lt;athineou@csd.uoc.gr&gt;, Vincenzo Lagani &lt;vlagani@csd.uoc.gr&gt; and Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>McCullagh P., and Nelder J.A. (1989). Generalized linear models.  CRC press, USA, 2nd edition.
</p>
<p>Lambert D. (1992). Zero-inflated Poisson regression, with an application to defects in manufacturing. Technometrics, 34(1):1-14.
</p>
<p>Joseph M.H. (2011). Negative Binomial Regression. Cambridge University Press, 2nd edition.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+testIndReg">testIndReg</a>, <a href="#topic+testIndNB">testIndNB</a>, <a href="#topic+testIndZIP">testIndZIP</a>, <a href="#topic+gSquare">gSquare</a>, <a href="#topic+CondIndTests">CondIndTests</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#simulate a dataset with continuous data
dataset &lt;- matrix(runif(100 * 20, 1, 50), ncol = 20 ) 
#the target feature is the last column of the dataset as a vector
target &lt;- rpois(100, 10)
results &lt;- testIndPois(target, dataset, xIndex = 14, csIndex = 10)
results

#run the SES algorithm using the testIndPois conditional independence test
m1 &lt;- MMPC(target, dataset, max_k = 3, threshold = 0.05, test = "testIndPois");
m2 &lt;- MMPC(target, dataset, max_k = 3, threshold = 0.05, test = "testIndNB");
</code></pre>

<hr>
<h2 id='Conditional+20independence+20tests+20for+20left+20censored+20data+20'>
Conditional independence test for survival data 
</h2><span id='topic+testIndTobit'></span><span id='topic+waldTobit'></span><span id='topic+permTobit'></span>

<h3>Description</h3>

<p>The main task of this test is to provide a p-value PVALUE for the null hypothesis: feature 'X' is independent from 'TARGET' given a conditioning set CS. This test can based on the Cox (semi-parametric) regression or on the Weibull (parametric) regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>testIndTobit(target, dataset, xIndex, csIndex, wei = NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL)

waldTobit(target, dataset, xIndex, csIndex, wei = NULL,  
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL)

permTobit(target, dataset, xIndex, csIndex, wei = NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL, 
threshold = 0.05, R = 999)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20left+2B20censored+2B20data+2B20_+3A_target">target</code></td>
<td>

<p>A Survival object (class Surv from package survival) with left censored data. This test works with responses that are left censored. See the examples below (the command <a href="survival.html#topic+Surv">Surv</a> or the final example in the <a href="survival.html#topic+survreg">survreg</a> documentation of the &quot;survival&quot; package) for more information on how to create the target variable.  
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20left+2B20censored+2B20data+2B20_+3A_dataset">dataset</code></td>
<td>

<p>A numeric matrix or data frame, in case of categorical predictors (factors), containing the variables for performing the test. Rows as samples and columns as features. In the cases of &quot;waldTobit&quot; and &quot;permTobit&quot; this is strictly a matrix. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20left+2B20censored+2B20data+2B20_+3A_xindex">xIndex</code></td>
<td>

<p>The index of the variable whose association with the target we want to test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20left+2B20censored+2B20data+2B20_+3A_csindex">csIndex</code></td>
<td>

<p>The indices of the variables to condition on. If you have no variables set this equal to 0.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20left+2B20censored+2B20data+2B20_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. An example where weights are used is surveys when stratified sampling has occured. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20left+2B20censored+2B20data+2B20_+3A_univariatemodels">univariateModels</code></td>
<td>

<p>Fast alternative to the hash object for univariate test. List with vectors &quot;pvalues&quot; (p-values), &quot;stats&quot; (statistics) and &quot;flags&quot; (flag = TRUE if the test was succesful) representing the univariate association of each variable with the target. Default value is NULL.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20left+2B20censored+2B20data+2B20_+3A_hash">hash</code></td>
<td>

<p>A boolean variable which indicates whether (TRUE) or not (FALSE) to use the hash-based implementation of the statistics of SES. Default value is FALSE. If TRUE you have to specify the stat_hash argument and the pvalue_hash argument.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20left+2B20censored+2B20data+2B20_+3A_stat_hash">stat_hash</code></td>
<td>

<p>A hash object which contains the cached generated statistics of a SES run in the current dataset, using the current test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20left+2B20censored+2B20data+2B20_+3A_pvalue_hash">pvalue_hash</code></td>
<td>

<p>A hash object which contains the cached generated p-values of a SES run in the current dataset, using the current test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20left+2B20censored+2B20data+2B20_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0, 1)) for assessing p-values significance. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20left+2B20censored+2B20data+2B20_+3A_r">R</code></td>
<td>

<p>The number of permutations, set to 999 by default. There is a trick to avoind doing all permutations. As soon as the number of times the permuted test statistic is more than the observed test statistic is more than 50 (in this example case), the p-value has exceeded the signifiance level (threshold value) and hence the predictor variable is not significant. There is no need to continue do the extra permutations, as a decision has already been made. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Tobit regression is performed. The implied model is Gaussian with left censored data.
</p>
<p>If hash = TRUE, censIndCR, censIndWR and censIndER require the arguments 'stat_hash' and 'pvalue_hash' for the hash-based implementation of the statistic test. 
These hash Objects are produced or updated by each run of SES (if hash == TRUE) and they can be reused in order to speed up next runs of the current statistic test. 
If &quot;SESoutput&quot; is the output of a SES run, then these objects can be retrieved by SESoutput@hashObject$stat_hash and the SESoutput@hashObject$pvalue_hash.
</p>
<p>Important: Use these arguments only with the same dataset that was used at initialization.
</p>
<p>For all the available conditional independence tests that are currently included on the package, please see &quot;?CondIndTests&quot;.
</p>
<p>The log-likelihood ratio test used in &quot;testIndTobit&quot; requires the fitting of two models. The Wald test used in &quot;waldTobit&quot;, requires fitting of only one model, the full one. The significance of the variable is examined only. Only continuous (or binary) predictor variables are currently accepted in this test. 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>pvalue</code></td>
<td>

<p>A numeric value that represents the logarithm of the generated p-value.
</p>
</td></tr>
<tr><td><code>stat</code></td>
<td>

<p>A numeric value that represents the generated statistic.
</p>
</td></tr>
<tr><td><code>stat_hash</code></td>
<td>

<p>The current hash object used for the statistics. See argument stat_hash and details. If argument hash = FALSE this is NULL.
</p>
</td></tr>
<tr><td><code>pvalue_hash</code></td>
<td>

<p>The current hash object used for the p-values. See argument stat_hash and details. If argument hash = FALSE this is NULL.
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This test uses the functions &quot;survreg&quot; and Surv of the package survival and the function anova (analysis of variance) of the package stats.
</p>


<h3>Author(s)</h3>

<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Tobin James (1958). Estimation of relationships for limited dependent variables. Econometrica. 26(1): 24-36. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SES">SES</a>, <a href="#topic+censIndWR">censIndWR</a>, <a href="#topic+testIndFisher">testIndFisher</a>, <a href="#topic+gSquare">gSquare</a>, <a href="#topic+testIndLogistic">testIndLogistic</a>, <a href="survival.html#topic+Surv">Surv</a>, <a href="stats.html#topic+anova">anova</a>, <a href="#topic+CondIndTests">CondIndTests</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(survival, quietly = TRUE)

x &lt;- matrix( rnorm(100 * 30), ncol = 30)
y &lt;- x[, 1] - x[, 2] + rnorm(100, 5)
y[y &lt; 0 ] &lt;- 0
y &lt;- survival::Surv(y, y&gt;0, type = 'left') 
  
#run the censIndCR   conditional independence test
testIndTobit(y, x, xIndex = 12, csIndex = c(5, 7, 4) )
waldTobit(y, x, xIndex = 12, csIndex = c(5, 7, 4) )
permTobit(y, x, xIndex = 12, csIndex = c(5, 7, 4), R = 499 )
  
#run the SES algorithm using the censIndCR conditional independence
#test for the survival class variable
a &lt;- MMPC(y, x, max_k = 2, threshold = 0.05, test = "testIndTobit")
</code></pre>

<hr>
<h2 id='Conditional+20independence+20tests+20for+20positive+20data'>
Regression conditional independence test for positive response variables.
</h2><span id='topic+testIndGamma'></span><span id='topic+testIndNormLog'></span><span id='topic+testIndIGreg'></span><span id='topic+permGamma'></span><span id='topic+permNormLog'></span><span id='topic+permIGreg'></span><span id='topic+waldGamma'></span><span id='topic+waldNormLog'></span><span id='topic+waldIGreg'></span>

<h3>Description</h3>

<p>The main task of this test is to provide a p-value PVALUE for the null hypothesis: feature 'X' is independent from 'TARGET' given a conditioning set CS. The pvalue is calculated by comparing a Poisson regression model based on the conditioning set CS against a model whose regressor are both X and CS. The comparison is performed through a chi-square test with the appropriate degrees of freedom on the difference between the deviances of the two models. The models supported here are poisson, zero inlftaed poisson and negative binomial.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>testIndGamma(target, dataset, xIndex, csIndex, wei = NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL)
  
testIndNormLog(target, dataset, xIndex, csIndex, wei = NULL,  
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL)

testIndIGreg(target, dataset, xIndex, csIndex, wei = NULL,
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL)

permGamma(target, dataset, xIndex, csIndex, wei = NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL, 
threshold = 0.05, R = 999)

permNormLog(target, dataset, xIndex, csIndex, wei = NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL, 
threshold = 0.05, R = 999)

permIGreg(target, dataset, xIndex, csIndex, wei = NULL,  
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL, 
threshold = 0.05, R = 999)

waldGamma(target, dataset, xIndex, csIndex, wei = NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL) 

waldNormLog(target, dataset, xIndex, csIndex, wei = NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL) 

waldIGreg(target, dataset, xIndex, csIndex, wei = NULL,  
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20positive+2B20data_+3A_target">target</code></td>
<td>

<p>A numeric vector containing the values of the target variable. For the Gamma based tests, the values must be strictly greater than zero. For the NormLog case, zeros can be included.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20positive+2B20data_+3A_dataset">dataset</code></td>
<td>

<p>A numeric matrix or data frame, in case of categorical predictors (factors), containing the variables for performing the test. Rows as samples and columns as features. In the cases of &quot;waldPois&quot;, &quot;waldNB&quot; and &quot;waldZIP&quot; this is strictly a matrix. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20positive+2B20data_+3A_xindex">xIndex</code></td>
<td>

<p>The index of the variable whose association with the target we want to test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20positive+2B20data_+3A_csindex">csIndex</code></td>
<td>

<p>The indices of the variables to condition on. If you have no variables set this equal to 0.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20positive+2B20data_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. An example where weights are used is surveys when stratified sampling has occured. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20positive+2B20data_+3A_univariatemodels">univariateModels</code></td>
<td>

<p>Fast alternative to the hash object for univariate test. List with vectors &quot;pvalues&quot; (p-values), &quot;stats&quot; (statistics) and &quot;flags&quot; (flag = TRUE if the test was succesful) representing the univariate association of each variable with the target. Default value is NULL.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20positive+2B20data_+3A_hash">hash</code></td>
<td>

<p>A boolean variable which indicates whether (TRUE) or not (FALSE) to use tha hash-based implementation of the statistics of SES. Default value is FALSE. If TRUE you have to specify the stat_hash argument and the pvalue_hash argument.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20positive+2B20data_+3A_stat_hash">stat_hash</code></td>
<td>

<p>A hash object which contains the cached generated statistics of a SES run in the current dataset, using the current test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20positive+2B20data_+3A_pvalue_hash">pvalue_hash</code></td>
<td>

<p>A hash object which contains the cached generated p-values of a SES run in the current dataset, using the current test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20positive+2B20data_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0, 1)) for assessing p-values significance. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20positive+2B20data_+3A_r">R</code></td>
<td>

<p>The number of permutations, set to 999 by default. There is a trick to avoind doing all permutations. As soon as the number of times the permuted test statistic is more than the observed test statistic is more than 50 (if threshold = 0.05 and R = 999), the p-value has exceeded the signifiance level (threshold value) and hence the predictor variable is not significant. There is no need to continue do the extra permutations, as a decision has already been made. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If hash = TRUE, all three tests require the arguments 'stat_hash' and 'pvalue_hash' for the hash-based implementation of the statistic test. These hash Objects are produced or updated by each run of SES (if hash = TRUE) and they can be reused in order to speed up next runs of the current statistic test. If &quot;SESoutput&quot; is the output of a SES run, then these objects can be retrieved by SESoutput@hashObject$stat_hash and the SESoutput@hashObject$pvalue_hash.
</p>
<p>Important: Use these arguments only with the same dataset that was used at initialization. For all the available conditional independence tests that are currently included on the package, please see &quot;?CondIndTests&quot;.
</p>
<p>For the testIndGamma and testIndNormLog the F test is used and not the log-likelihood ratio test because both of these regression models have a nuisance parameter. The testIndNormLog can be seen as a non linear Gaussian model where the conditional mean is related with the covariate(s) via an exponential function.
</p>
<p>TestIndIGreg fits an inverse gaussian distribution with a log link. The testIndIGreg has some problems due to problems in R's implementation of the inverse gaussian regression with a log link.  
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>pvalue</code></td>
<td>

<p>A numeric value that represents the logarithm of the generated p-value due to the count data regression (see references below).
</p>
</td></tr>
<tr><td><code>stat</code></td>
<td>

<p>A numeric value that represents the generated statistic due to Poisson regression(see reference below).
</p>
</td></tr>
<tr><td><code>stat_hash</code></td>
<td>

<p>The current hash object used for the statistics. See argument stat_hash and details. If argument hash = FALSE this is NULL.
</p>
</td></tr>
<tr><td><code>pvalue_hash</code></td>
<td>

<p>The current hash object used for the p-values. See argument stat_hash and details. If argument hash = FALSE this is NULL.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>McCullagh P., and Nelder J.A. (1989). Generalized linear models.  CRC press, USA, 2nd edition.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+testIndReg">testIndReg</a>, <a href="#topic+testIndNB">testIndNB</a>, <a href="#topic+testIndZIP">testIndZIP</a>, <a href="#topic+gSquare">gSquare</a>, <a href="#topic+CondIndTests">CondIndTests</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#simulate a dataset with continuous data
dataset &lt;- matrix( rnorm(200 * 20, 1, 5), ncol = 20 ) 
#the target feature is the last column of the dataset as a vector
target &lt;- rgamma(200, 1, 3)
testIndGamma(target, dataset, xIndex = 14, csIndex = 10)
testIndNormLog(target, dataset, xIndex = 14, csIndex = 10)
#run the MMPC algorithm using the testIndPois conditional independence test
m1 &lt;- MMPC(target, dataset, max_k = 3, threshold = 0.05, test = "testIndGamma");
m2 &lt;- MMPC(target, dataset, max_k = 3, threshold = 0.05, test = "testIndNormLog");
</code></pre>

<hr>
<h2 id='Conditional+20independence+20tests+20for+20sucess+20rates'>
Binomial regression conditional independence test for success rates (binomial)
</h2><span id='topic+testIndBinom'></span><span id='topic+permBinom'></span><span id='topic+waldBinom'></span>

<h3>Description</h3>

<p>The main task of this test is to provide a p-value PVALUE for the null hypothesis: feature 'X' is independent from 'TARGET' given a conditioning set CS. The pvalue is calculated by comparing a binomial logistic regression model based on the conditioning set CS against a model whose regressor are both X and CS. The comparison is performed through a chi-square test with the appropriate degrees of freedom on the difference between the deviances of the two models. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>testIndBinom(target, dataset, xIndex, csIndex, wei = NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL)

permBinom(target, dataset, xIndex, csIndex, wei = NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL, 
threshold = 0.05, R = 999)

waldBinom(target, dataset, xIndex, csIndex, wei = NULL,  
univariateModels = NULL , hash = FALSE, stat_hash = NULL, pvalue_hash = NULL) 

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20sucess+2B20rates_+3A_target">target</code></td>
<td>

<p>A matrix with two two columns, the first one is the number of successes, the cases (integer) and the second one is the totals (integers).
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20sucess+2B20rates_+3A_dataset">dataset</code></td>
<td>

<p>A numeric matrix or data frame, in case of categorical predictors (factors), containing the variables for performing the test. Rows as samples and columns as features. In the case of &quot;waldBinom&quot; this is strictly a matrix. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20sucess+2B20rates_+3A_xindex">xIndex</code></td>
<td>

<p>The index of the variable whose association with the target we want to test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20sucess+2B20rates_+3A_csindex">csIndex</code></td>
<td>

<p>The indices of the variables to condition on. If you have no variables set this equal to 0.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20sucess+2B20rates_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL and should stay as is, since the totals (second column of the target) is used as weights. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20sucess+2B20rates_+3A_univariatemodels">univariateModels</code></td>
<td>

<p>Fast alternative to the hash object for univariate test. List with vectors &quot;pvalues&quot; (p-values), &quot;stats&quot; (statistics) and &quot;flags&quot; (flag = TRUE if the test was succesful) representing the univariate association of each variable with the target. Default value is NULL.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20sucess+2B20rates_+3A_hash">hash</code></td>
<td>

<p>A boolean variable which indicates whether (TRUE) or not (FALSE) to use tha hash-based implementation of the statistics of SES. Default value is FALSE. If TRUE you have to specify the stat_hash argument and the pvalue_hash argument.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20sucess+2B20rates_+3A_stat_hash">stat_hash</code></td>
<td>

<p>A hash object which contains the cached generated statistics of a SES run in the current dataset, using the current test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20sucess+2B20rates_+3A_pvalue_hash">pvalue_hash</code></td>
<td>

<p>A hash object which contains the cached generated p-values of a SES run in the current dataset, using the current test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20sucess+2B20rates_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0, 1)) for assessing p-values significance. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20sucess+2B20rates_+3A_r">R</code></td>
<td>

<p>The number of permutations, set to 999 by default. There is a trick to avoind doing all permutations. As soon as the number of times the permuted test statistic is more than the observed test statistic is more than 50 (if threshold = 0.05 and R = 999), the p-value has exceeded the signifiance level (threshold value) and hence the predictor variable is not significant. There is no need to continue do the extra permutations, as a decision has already been made. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If hash = TRUE, all three tests require the arguments 'stat_hash' and 'pvalue_hash' for the hash-based implementation of the statistic test. These hash Objects are produced or updated by each run of SES (if hash == TRUE) and they can be reused in order to speed up next runs of the current statistic test. If &quot;SESoutput&quot; is the output of a SES run, then these objects can be retrieved by SESoutput@hashObject$stat_hash and the SESoutput@hashObject$pvalue_hash.
</p>
<p>Important: Use these arguments only with the same dataset that was used at initialization. For all the available conditional independence tests that are currently included on the package, please see &quot;?CondIndTests&quot;.
</p>
<p>If you have overdispersion, the variance is higher than the mean, a negative binomial is to be used. If you have more zeros than expected under a Poisson model, not overdispersion, then zero inlfated Poisson is to be used. This is in fact a logistic reression where the target is the ratio of successes divided by the totals and the weights are the totals. 
</p>
<p>The log-likelihood ratio test used in &quot;testIndBinom&quot; requires the fitting of two models. The Wald test used in &quot;waldBinom&quot; requires fitting of only one model, the full one. The significance of the variable is examined only. Only continuous (or binary) predictor variables are currently accepted in this test. 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>pvalue</code></td>
<td>

<p>A numeric value that represents the logarithm of the generated p-value due to the count data regression (see references below).
</p>
</td></tr>
<tr><td><code>stat</code></td>
<td>

<p>A numeric value that represents the generated statistic due to Poisson regression(see reference below).
</p>
</td></tr>
<tr><td><code>stat_hash</code></td>
<td>

<p>The current hash object used for the statistics. See argument stat_hash and details. If argument hash = FALSE this is NULL.
</p>
</td></tr>
<tr><td><code>pvalue_hash</code></td>
<td>

<p>The current hash object used for the p-values. See argument stat_hash and details. If argument hash = FALSE this is NULL.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Vincenzo Lagani, Ioannis Tsamardinos, Giorgos Athineou and Michail Tsagris.
</p>
<p>R implementation and documentation: Giorgos Athineou &lt;athineou@csd.uoc.gr&gt;, Vincenzo Lagani &lt;vlagani@csd.uoc.gr&gt; and Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>McCullagh P., and Nelder J.A. (1989). Generalized linear models.  CRC press, USA, 2nd edition.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+testIndLogistic">testIndLogistic</a>, <a href="#topic+testIndBeta">testIndBeta</a>, <a href="#topic+testIndReg">testIndReg</a>, <a href="#topic+CondIndTests">CondIndTests</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#simulate a dataset with continuous data
dataset &lt;- matrix(runif(200 * 20, 1, 50), ncol = 20 ) 
#the target feature is the last column of the dataset as a vector
y &lt;- rbinom(200, 10, 0.6)
N &lt;- sample(11:20, 200, replace = TRUE)
target &lt;- cbind(y, N)
testIndBinom(target, dataset, xIndex = 14, csIndex = 10)

#run the MMPC algorithm using the testIndPois conditional independence test
a &lt;- MMPC(target, dataset, max_k = 3, threshold = 0.05, test = "testIndBinom")
</code></pre>

<hr>
<h2 id='Conditional+20independence+20tests+20for+20survival+20data+20'>
Conditional independence test for survival data 
</h2><span id='topic+censIndCR'></span><span id='topic+censIndWR'></span><span id='topic+censIndER'></span><span id='topic+censIndLLR'></span><span id='topic+permCR'></span><span id='topic+permWR'></span><span id='topic+permER'></span><span id='topic+permLLR'></span><span id='topic+waldCR'></span><span id='topic+waldWR'></span><span id='topic+waldER'></span><span id='topic+waldLLR'></span>

<h3>Description</h3>

<p>The main task of this test is to provide a p-value PVALUE for the null hypothesis: feature 'X' is independent from 'TARGET' given a conditioning set CS. This test can based on the Cox (semi-parametric) regression or on the Weibull (parametric) regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>censIndCR(target, dataset, xIndex, csIndex, wei = NULL,  
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL)

censIndWR(target, dataset, xIndex, csIndex, wei = NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL)

censIndER(target, dataset, xIndex, csIndex, wei = NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL)

censIndLLR(target, dataset, xIndex, csIndex, wei = NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL)

permCR(target, dataset, xIndex, csIndex, wei = NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL, 
threshold = 0.05, R = 999)

permWR(target, dataset, xIndex, csIndex, wei = NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL, 
threshold = 0.05, R = 999)

permER(target, dataset, xIndex, csIndex, wei = NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL, 
threshold = 0.05, R = 999)

permLLR(target, dataset, xIndex, csIndex, wei = NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL, 
threshold = 0.05, R = 999)

waldCR(target, dataset, xIndex, csIndex, wei = NULL,
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL)

waldWR(target, dataset, xIndex, csIndex, wei = NULL, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL)

waldER(target, dataset, xIndex, csIndex, wei = NULL,
univariateModels = NULL, hash = FALSE, stat_hash = NULL, pvalue_hash = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20survival+2B20data+2B20_+3A_target">target</code></td>
<td>

<p>A Survival object (class Surv from package survival) containing the time to event data (time) and the status indicator vector (event). View <a href="survival.html#topic+Surv">Surv</a> documentation for more information. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20survival+2B20data+2B20_+3A_dataset">dataset</code></td>
<td>

<p>A numeric matrix or data frame, in case of categorical predictors (factors), containing the variables for performing the test. Rows as samples and columns as features. In the cases of &quot;waldCR&quot;, &quot;waldWR&quot;, &quot;waldER&quot;, &quot;waldCR&quot;, &quot;permWR&quot;, &quot;permER&quot; and &quot;permLLR&quot; this is strictly a matrix. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20survival+2B20data+2B20_+3A_xindex">xIndex</code></td>
<td>

<p>The index of the variable whose association with the target we want to test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20survival+2B20data+2B20_+3A_csindex">csIndex</code></td>
<td>

<p>The indices of the variables to condition on.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20survival+2B20data+2B20_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. An example where weights are used is surveys when stratified sampling has occured. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20survival+2B20data+2B20_+3A_univariatemodels">univariateModels</code></td>
<td>

<p>Fast alternative to the hash object for univariate test. List with vectors &quot;pvalues&quot; (p-values), &quot;stats&quot; (statistics) and &quot;flags&quot; (flag = TRUE if the test was succesful) representing the univariate association of each variable with the target. Default value is NULL.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20survival+2B20data+2B20_+3A_hash">hash</code></td>
<td>

<p>A boolean variable which indicates whether (TRUE) or not (FALSE) to use the hash-based implementation of the statistics of SES. Default value is FALSE. If TRUE you have to specify the stat_hash argument and the pvalue_hash argument.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20survival+2B20data+2B20_+3A_stat_hash">stat_hash</code></td>
<td>

<p>A hash object which contains the cached generated statistics of a SES run in the current dataset, using the current test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20survival+2B20data+2B20_+3A_pvalue_hash">pvalue_hash</code></td>
<td>

<p>A hash object which contains the cached generated p-values of a SES run in the current dataset, using the current test.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20survival+2B20data+2B20_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0, 1)) for assessing p-values significance. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20for+2B20survival+2B20data+2B20_+3A_r">R</code></td>
<td>

<p>The number of permutations, set to 999 by default. There is a trick to avoind doing all permutations. As soon as the number of times the permuted test statistic is more than the observed test statistic is more than 50 (in this example case), the p-value has exceeded the signifiance level (threshold value) and hence the predictor variable is not significant. There is no need to continue do the extra permutations, as a decision has already been made. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The censIndCR implies the Cox (semiparametric) regression, the censIndWR the Weibull (parametric) regression and the censIndER the exponential (parametric) regression, 
which is a special case of the Weibull regression (when shape parameter is 1). <b>Note</b>: When there are observations with zero values (time=0) the Weibull and Exponential
regressions will not work. Only Cox regression will run. The censIndLLR is the log-logistic regression. This is a pure AFT model, unlike Weibull which is either a proportional hazards
model or and AFT.
</p>
<p>If hash = TRUE, censIndCR, censIndWR, censIndER and censIndLLR require the arguments 'stat_hash' and 'pvalue_hash' for the hash-based implementation of the statistic test. 
These hash Objects are produced or updated by each run of SES (if hash == TRUE) and they can be reused in order to speed up next runs of the current statistic test. 
If &quot;SESoutput&quot; is the output of a SES run, then these objects can be retrieved by SESoutput@hashObject$stat_hash and the SESoutput@hashObject$pvalue_hash.
</p>
<p>Important: Use these arguments only with the same dataset that was used at initialization.
</p>
<p>For all the available conditional independence tests that are currently included on the package, please see &quot;?CondIndTests&quot;.
</p>
<p>The log-likelihood ratio test used in &quot;censIndCR&quot;, &quot;censIndWR&quot; and &quot;censIndER&quot; requires the fitting of two models. The Wald tests used in &quot;waldCR&quot;, &quot;waldWR&quot; and &quot;waldER&quot; requires fitting 
of only one model, the full one. The significance of the variable is examined only. Only continuous (or binary) predictor variables are currently accepted in this test. 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>pvalue</code></td>
<td>

<p>A numeric value that represents the logarithm of the generated p-value.
</p>
</td></tr>
<tr><td><code>stat</code></td>
<td>

<p>A numeric value that represents the generated statistic.
</p>
</td></tr>
<tr><td><code>stat_hash</code></td>
<td>

<p>The current hash object used for the statistics. See argument stat_hash and details. If argument hash = FALSE this is NULL.
</p>
</td></tr>
<tr><td><code>pvalue_hash</code></td>
<td>

<p>The current hash object used for the p-values. See argument stat_hash and details. If argument hash = FALSE this is NULL.
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This test uses the functions coxph and Surv of the package survival and the function anova (analysis of variance) of the package stats.
</p>


<h3>Author(s)</h3>

<p>R implementation and documentation: Vincenzo Lagani &lt;vlagani@csd.uoc.gr&gt;, Giorgos Athineou &lt;athineou@csd.uoc.gr&gt;
</p>


<h3>References</h3>

<p>V. Lagani and I. Tsamardinos (2010). Structure-based variable selection for survival data. Bioinformatics Journal 16(15): 1887-1894.
</p>
<p>Cox,D.R. (1972) Regression models and life-tables. J. R. Stat. Soc., 34, 187-220.
</p>
<p>Scholz, F. W. (2001). Maximum likelihood estimation for type I censored Weibull data including covariates. ISSTECH-96-022, Boeing Information &amp; Support Services.
</p>
<p>Smith, R. L. (1991). Weibull regression models for reliability data. Reliability Engineering &amp; System Safety, 34(1), 55-76.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SES">SES</a>, <a href="#topic+censIndWR">censIndWR</a>, <a href="#topic+testIndFisher">testIndFisher</a>, <a href="#topic+gSquare">gSquare</a>, <a href="#topic+testIndLogistic">testIndLogistic</a>, <a href="survival.html#topic+Surv">Surv</a>, <a href="stats.html#topic+anova">anova</a>, <a href="#topic+CondIndTests">CondIndTests</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#create a survival simulated dataset
dataset &lt;- matrix(runif(400 * 20, 1, 100), nrow = 400 , ncol = 20)
dataset &lt;- as.data.frame(dataset);
timeToEvent &lt;- numeric(400)
event &lt;- numeric(400)
ca &lt;- numeric(400)
for(i in 1:400) {
  timeToEvent[i] &lt;- dataset[i, 1] + 0.5 * dataset[i, 10] + 2 * dataset[i, 15] + runif(1, 0, 3);
  event[i] &lt;- sample( c(0, 1), 1)
  ca[i] &lt;- runif(1, 0, timeToEvent[i] - 0.5)
  if(event[i] == 0)   timeToEvent[i] = timeToEvent[i] - ca[i]
}

require(survival, quietly = TRUE)

#init the Surv object class feature
target &lt;- Surv(time = timeToEvent, event = event)
#run the censIndCR   conditional independence test
censIndCR( target, dataset, xIndex = 12, csIndex = c(5, 7, 4) )
# run the SESC algorithm 
## Not run: 
ses1 &lt;- SES(target, dataset, max_k = 1, threshold = 0.05, test = "censIndCR");
ses2 &lt;- SES(target, dataset, max_k = 1, threshold = 0.05, test = "censIndWR");

## End(Not run)
</code></pre>

<hr>
<h2 id='Conditional+20independence+20tests+20with+20and+20without+20permutation+20p-value'>
Conditional independence test for continuous class variables with and without permutation based p-value
</h2><span id='topic+condi'></span><span id='topic+dist.condi'></span><span id='topic+cat.ci'></span>

<h3>Description</h3>

<p>The main task of this test is to provide a permutation based p-value PVALUE for the null hypothesis: feature 'X' is independent from 'TARGET' given a 
conditioning set CS.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>condi(ind1, ind2, cs, dat, type = "pearson", rob = FALSE, R = 1) 
dist.condi(ind1, ind2, cs, dat, type = NULL, rob = FALSE, R = 499) 
cat.ci(ind1, ind2, cs, dat, type, rob = FALSE, R = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20with+2B20and+2B20without+2B20permutation+2B20p-value_+3A_ind1">ind1</code></td>
<td>

<p>The index of the one variable to be considered. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20with+2B20and+2B20without+2B20permutation+2B20p-value_+3A_ind2">ind2</code></td>
<td>

<p>The index of the other variable to be considered. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20with+2B20and+2B20without+2B20permutation+2B20p-value_+3A_cs">cs</code></td>
<td>

<p>The index or indices of the conditioning set of variable(s). If you have no variables set this equal to 0.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20with+2B20and+2B20without+2B20permutation+2B20p-value_+3A_dat">dat</code></td>
<td>

<p>A matrix with the data. In the case of &quot;cat.ci&quot; the minimum must be 0, i.e. the data must be like 0, 1, 2 and NOT 1, 2, 3... There is a C++ code behind and the minimum must be 0. 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20with+2B20and+2B20without+2B20permutation+2B20p-value_+3A_type">type</code></td>
<td>

<p>Do you want the Pearson (type = &quot;pearson&quot;) or the Spearman (type = &quot;spearman&quot;) correlation to be used. For &quot;dist.condi&quot; this is an obsolete argument but it requires to exist when it is used in 
the PC algorithm. 
</p>
<p>For &quot;cat.ci&quot; this should be a vector with the levels, the number of distinct (different) values of each categorical variable. Its length is equal to the number of variables used in the test 
(2 + the number of conditioning variables). 
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20with+2B20and+2B20without+2B20permutation+2B20p-value_+3A_rob">rob</code></td>
<td>

<p>If you choose type=&quot;pearson&quot; then you can sapecify whether you want a robust version of it. For &quot;dist.condi&quot; and &quot;cat.ci&quot; this is an obsolete argument but it requires to exist when it is used in the PC algorithm.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20independence+2B20tests+2B20with+2B20and+2B20without+2B20permutation+2B20p-value_+3A_r">R</code></td>
<td>

<p>If R = 1 then the asymptotic p-value is calculated. If R &gt; 1 a permutation based p-value is returned. For the distance correlation based test, this is set to 499 by default and is used in the partial correlaiton test only. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This test is currently designed for usage by the PC algorithm. The Fisher conditional independence test which is based on the Pearson or Spearman correlation coefficients is much faster than the distance based (partial) correlation test. 
</p>
<p>The distance correlation can handle non linear relationships as well. The p-value for the partial distance correlation is calculated via permutations and is slow. 
</p>


<h3>Value</h3>

<p>A vector including the test statistic, it's associated p-value and the relevant degrees of freedom. In the case of a permutation based p-value, the returned test statistic is the observed test statistic divided by the relevant degrees of freedom (Pearson and Spearman correlation coefficients only). This is for the case of ties between many permutation based p-values. The PC algorithm choose a pair of variables based on the p-values. If they are equal it will use the test statistic. 
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Giorgos Athineou &lt;athineou@csd.uoc.gr&gt; and Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Hampel F. R., Ronchetti E. M., Rousseeuw P. J., and Stahel W. A. (1986). Robust statistics: the approach based on influence functions. John Wiley &amp; Sons. 
</p>
<p>Lee Rodgers J., and Nicewander W.A. (1988). &quot;Thirteen ways to look at the correlation coefficient&quot;. The American Statistician 42(1): 59-66.
</p>
<p>Shevlyakov G. and Smirnov P. (2011). Robust Estimation of the Correlation Coefficient: An Attempt of Survey. Austrian Journal of Statistics, 40(1 &amp; 2): 147-156.
</p>
<p>Spirtes P., Glymour C. and Scheines R. Causation, Prediction, and Search. The MIT Press, Cambridge, MA, USA, second edition, January 2001.
</p>
<p>Szekely G.J. and Rizzo, M.L. (2014). Partial distance correlation with methods for dissimilarities. The Annals of Statistics, 42(6): 2382&ndash;2412.
</p>
<p>Szekely G.J. and Rizzo M.L. (2013).  Energy statistics: A class of statistics based on distances. Journal of Statistical Planning and Inference 143(8): 1249&ndash;1272.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+testIndFisher">testIndFisher</a>, <a href="#topic+testIndSpearman">testIndSpearman</a>, <a href="#topic+pc.skel">pc.skel</a>, <a href="#topic+gSquare">gSquare</a>, <a href="#topic+CondIndTests">CondIndTests</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#simulate a dataset with continuous data
dataset &lt;- matrix(runif(500 * 5, 1, 100), ncol = 5 )
testIndFisher(dataset[, 1], dataset[, -1], xIndex = 1, csIndex = 2)
condi(ind1 = 1, ind2 = 2, cs = 3, dataset, R = 1)
condi(ind1 = 1, ind2 = 2, cs = 3, dataset, R = 999)
dist.condi(ind1 = 1, ind2 = 2, 0, dataset)
dist.condi(ind1 = 1, ind2 = 2, cs = 3, dataset, R = 99)
</code></pre>

<hr>
<h2 id='Constraint+20based+20feature+20selection+20algorithms'>
SES: Feature selection algorithm for identifying multiple minimal, statistically-equivalent and equally-predictive feature signatures
MMPC: Feature selection algorithm for identifying minimal feature subsets
</h2><span id='topic+SES'></span><span id='topic+MMPC'></span><span id='topic+wald.ses'></span><span id='topic+wald.mmpc'></span><span id='topic+perm.ses'></span><span id='topic+perm.mmpc'></span>

<h3>Description</h3>

<p>SES algorithm follows a forward-backward filter approach for feature selection in order to provide minimal, highly-predictive, statistically-equivalent, multiple feature subsets of a high dimensional dataset. See also Details. MMPC algorithm follows the same approach without generating multiple feature subsets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SES(target, dataset, max_k = 3, threshold = 0.05, test = NULL, ini = NULL, 
wei = NULL, user_test = NULL, hash = FALSE, hashObject = NULL,
ncores = 1, backward = FALSE)
 
MMPC(target, dataset, max_k = 3, threshold = 0.05, test = NULL, ini = NULL, 
wei = NULL, user_test = NULL, hash = FALSE, hashObject = NULL, 
ncores = 1, backward = FALSE)

wald.ses(target, dataset, max_k = 3, threshold = 0.05, test = NULL, ini = NULL, 
wei = NULL, user_test = NULL, hash = FALSE, hashObject = NULL, 
ncores = 1, backward = FALSE)

wald.mmpc(target, dataset, max_k = 3, threshold = 0.05, test = NULL, ini = NULL, 
wei = NULL, user_test = NULL, hash = FALSE, hashObject = NULL,  
ncores = 1, backward = FALSE)

perm.ses(target, dataset, max_k = 3, threshold = 0.05, test = NULL, ini = NULL, 
wei = NULL, user_test = NULL, hash=FALSE, hashObject = NULL, 
R = 999, ncores = 1, backward = FALSE)

perm.mmpc(target, dataset, max_k = 3, threshold = 0.05, test = NULL, ini = NULL, 
wei = NULL, user_test = NULL, hash=FALSE, hashObject = NULL,
R = 999, ncores = 1, backward = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms_+3A_target">target</code></td>
<td>

<p>The class variable. Provide either a string, an integer, a numeric value, a vector, a factor, an ordered factor or a Surv object. See also Details.
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms_+3A_dataset">dataset</code></td>
<td>

<p>The data-set; provide either a data frame or a matrix (columns = variables, rows = samples).
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms_+3A_max_k">max_k</code></td>
<td>

<p>The maximum conditioning set to use in the conditional indepedence test (see Details). Integer, default value is 3.
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0, 1)) for assessing p-values significance. Default value is 0.05.
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms_+3A_test">test</code></td>
<td>

<p>The conditional independence test to use. Default value is NULL. See also <code><a href="#topic+CondIndTests">CondIndTests</a></code>.
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms_+3A_ini">ini</code></td>
<td>

<p>This is a supposed to be a list. After running SES or MMPC with some hyper-parameters you might want to run SES again with different hyper-parameters. To avoid calculating the univariate associations (first step of SES and of MMPC) again, you can extract them from the first run of SES and plug them here. This can speed up the second run (and subequent runs of course) by 50%. See the details and the argument &quot;univ&quot; in the output values. 
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. An example where weights are used is surveys when stratified sampling has occured. 
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms_+3A_user_test">user_test</code></td>
<td>

<p>A user-defined conditional independence test (provide a closure type object). Default value is NULL. If this is defined, the &quot;test&quot; argument is ignored.
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms_+3A_hash">hash</code></td>
<td>

<p>A boolean variable which indicates whether (TRUE) or not (FALSE) to store the statistics calculated during SES execution in a hash-type object. Default value is FALSE. If TRUE a hashObject is produced.
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms_+3A_hashobject">hashObject</code></td>
<td>

<p>A List with the hash objects generated in a previous run of SES or MMPC. 
Each time SES runs with &quot;hash=TRUE&quot; it produces a list of hashObjects that can be re-used in order to speed up next runs of SES or MMPC.
</p>
<p>Important: the generated hashObjects should be used only when the same dataset is re-analyzed, possibly with different values of max_k and threshold.
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms_+3A_r">R</code></td>
<td>

<p>The number of permutations, set to 999 by default. There is a trick to avoind doing all permutations. As soon as the number of times the permuted test statistic is more than the observed test statistic is more than 50 (if threshold = 0.05 and R = 999), the p-value has exceeded the signifiance level (threshold value) and hence the predictor variable is not significant. There is no need to continue do the extra permutations, as a decision has already been made. 
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms_+3A_ncores">ncores</code></td>
<td>

<p>How many cores to use. This plays an important role if you have tens of thousands of variables or really large sample sizes and tens of thousands of variables and a regression based test which requires numerical optimisation. In other cases it will not make a difference in the overall time (in fact it can be slower). The parallel computation is used in the first step of the algorithm, where univariate associations are examined, those take place in parallel. We have seen a reduction in time of 50% with 4 cores in comparison to 1 core. Note also, that the amount of reduction is not linear in the number of cores.
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms_+3A_backward">backward</code></td>
<td>

<p>If TRUE, the backward (or symmetry correction) phase will be implemented. This removes any falsely included variables in the parents and children set of the target variable. It calls the <code>link{mmpcbackphase}</code> for this purpose. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The SES function implements the Statistically Equivalent Signature (SES) algorithm as presented in &quot;Tsamardinos, Lagani and Pappas, HSCBB 2012&quot;.
</p>
<p>The MMPC function implements the MMPC algorithm as presented in &quot;Tsamardinos, Brown and Aliferis. The max-min hill-climbing Bayesian network structure learning algorithm&quot;.
</p>
<p>he output value &quot;univ&quot; along with the output value &quot;hashObject&quot; 
can speed up the computations of subsequent runs of SES and MMPC. The first run with a specific pair of hyper-parameters (threshold and max_k) the univariate associations tests 
and the conditional independence tests (test statistic and <b>natural logarithm of their corresponding p-values</b>) are stored and returned. In the next run(s) with different pair(s) of 
hyper-parameters you can use this information to save time. With a few thousands of variables you will see the difference, which can be up to 50%. For the non robust correlation based tests, 
the difference may not be significant though, because the unconditional correlation coefficients are calculated very efficiently.s. 
</p>
<p>The max_k option: the maximum size of the conditioning set to use in the conditioning independence test. Larger values provide more accurate results, at the cost of higher 
computational times. When the sample size is small (e.g., <code class="reqn">&lt;50</code> observations) the max_k parameter should be say 3, otherwise the conditional independence test may 
not be able to provide reliable results.
</p>
<p>If the dataset (predictor variables) contains missing (NA) values, they will automatically be replaced by the current variable (column) mean value with an appropriate warning 
to the user after the execution.
</p>
<p>If the target is a single integer value or a string, it has to corresponds to the column number or to the name of the target feature in the datase. In any other case the target is a variable that is not contained in the dataset.
</p>
<p>If the current 'test' argument is defined as NULL or &quot;auto&quot; and the user_test argument is NULL then the algorithm automatically selects the best test based on the type of the data. 
Particularly:
</p>

<ul>
<li><p> if the target is a factor, the multinomial or the binary logistic regression is used. If the target has two values only, binary logistic regression will be used. 
</p>
</li>
<li><p> if target is a ordered factor, ordinal regression is used in the logistic test. Hence, if you want to use multinomial or ordinal logistic regression, make sure your 
target is factor. 
</p>
</li>
<li><p> if target is a numerical vector and the dataset is a matrix or a data.frame with continuous variables, the Fisher conditional independence test is used. If the dataset is a 
data.frame and there are categorical variables, linear regression is used.
</p>
</li>
<li><p> if target is discrete numerical (counts), the Poisson regression conditional independence test is used. If there are only two values, the binary logistic regression is to 
be used.
</p>
</li>
<li><p> if target is a Surv object, a Survival conditional independence test is used.
</p>
</li>
<li><p> if target is a matrix with at least 2 columns, the multivariate linear regression is used. 
</p>
</li>
<li><p> if target is a 2 column matrix whose columns are the number of successes and the number of trials (first and second column respectively) the testIndBinom should be used.
</p>
</li></ul>

<p>Conditional independence test functions to be pass through the user_test argument should have the same signature of the included test. See <code><a href="#topic+testIndFisher">testIndFisher</a></code> for an example.
</p>
<p>For all the available conditional independence tests that are currently included on the package, please see <code><a href="#topic+CondIndTests">CondIndTests</a></code>. 
If two or more p-values are below the machine epsilon (.Machine$double.eps which is equal to 2.220446e-16), all of them are set to 0. To make the comparison or the ordering feasible 
we use the logarithm of the p-value. The max-min heuristic though, requires comparison and an ordering of the p-values. Hence, all conditional independence tests calculate the logarithm 
of the p-value.
</p>
<p>If there are missing values in the dataset (predictor variables) columnwise imputation takes place. The median is used for the continuous variables and the mode for categorical variables. It is a naive and not so clever method. For this reason the user is encouraged to make sure his data contain no missing values. 
</p>
<p>If you have percentages, in the (0, 1) interval, they are automatically mapped into <code class="reqn">R</code> by using the logit transformation. If you set the test to <code><a href="#topic+testIndBeta">testIndBeta</a></code>, 
beta regression is used. If you have compositional data, positive multivariate data where each vector sums to 1, with NO zeros, they are also mapped into the Euclidean space using the additive log-ratio (multivariate logit) transformation (Aitchison, 1986).
</p>
<p>If you use testIndSpearman (argument &quot;test&quot;), the ranks of the data calculated and those are used in the caclulations. This speeds up the whole procedure.
</p>
<p>As a rule of thumb you can try this. If for example you have counts and want to see which model fits best, there are two ways. Calculate the mean and the variance. If they are similar, 
use the Poisson instead of the negative binomial as it is much faster. If you are not convinced, you can either use the negative binomial or do the following simulation study.
</p>
<p># x &lt;- matrix(rnorm(n * 1000), ncol = 1000)
# a &lt;- Rfast::univglms(y, x)
# hist(a[, 2])  ## histogram of the p-values
</p>
<p>If the histogram shows a uniform distribution, use the Poisson regression. If the histogram is not uniform, then repeat the simluation but with a negative binomial distribution. 
If the histogram is again not flat, then another model is necessary. If the data come from a Poisson or negative binomial, the histogram with a negative binomial regressino will be flat.
If the data come a from a negative binomial, the histogram with a Poisson will not be uniform. 
</p>
<p>On the same page, if you have many zeros, try Rfast::zip.mle and see whether there are grounds to to facilitate the use of a zero inflated Poisson model. Otherwise, do a simulation 
study like before.  
</p>


<h3>Value</h3>

<p>The output of the algorithm is an object of the class 'SESoutput' for SES or 'MMPCoutput' for MMPC including:
</p>
<table>
<tr><td><code>selectedVars</code></td>
<td>

<p>The selected variables, i.e., the signature of the target variable.
</p>
</td></tr>
<tr><td><code>selectedVarsOrder</code></td>
<td>

<p>The order of the selected variables according to increasing pvalues.
</p>
</td></tr>
<tr><td><code>queues</code></td>
<td>

<p>A list containing a list (queue) of equivalent features for each variable included in selectedVars. An equivalent signature can be built by selecting a single feature from each queue. 
Featured only in SES.
</p>
</td></tr>
<tr><td><code>signatures</code></td>
<td>

<p>A matrix reporting all equivalent signatures (one signature for each row). Featured only in SES.
</p>
</td></tr>
<tr><td><code>hashObject</code></td>
<td>

<p>The hashObject caching the statistic calculated in the current run.
</p>
</td></tr>
<tr><td><code>pvalues</code></td>
<td>

<p>For each feature included in the dataset, this vector reports the strength of its association with the target in the context of all other variable. 
Particularly, this vector reports the max p-values found when the association of each variable with the target is tested against different conditional sets. 
Lower values indicate higher association. <b>Note that these are the logged p-values, natural logarithm of the pvalues, and not the p-values</b>.
</p>
</td></tr>
<tr><td><code>stats</code></td>
<td>

<p>The statistics corresponding to &quot;pvalues&quot; (higher values indicates higher association).
</p>
</td></tr>
<tr><td><code>univ</code></td>
<td>

<p>This is a list with the univariate associations; the test statistics and their corresponding logged p-values. This list is very important for subsequent runs of SES with different hyper-parameters. After running SES with some hyper-parameters you might want to run SES again with different hyper-parameters. To avoid calculating the univariate associations (first step of SES or MMPC) again, you can take this list from the first run of SES and plug it in the argument &quot;ini&quot; in the next run(s) of SES or MMPC. This can speed up the second run (and subequent runs of course) by 50%. See the argument &quot;univ&quot; in the output values. 
</p>
</td></tr>
<tr><td><code>max_k</code></td>
<td>

<p>The max_k option used in the current run.
</p>
</td></tr>
<tr><td><code>threshold</code></td>
<td>

<p>The threshold option used in the current run.
</p>
</td></tr>
<tr><td><code>n.tests</code></td>
<td>

<p>If you have set hash = TRUE, then the number of tests performed by SES or MMPC will be returned. If you have not set this to TRUE, the number of univariate associations will be returned. So be careful with this number.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The run time of the algorithm. A numeric vector. The first element is the user time, the second element is the system time and the third element is the elapsed time.
</p>
</td></tr>
<tr><td><code>test</code></td>
<td>

<p>The character name of the statistic test used.
</p>
</td></tr>
</table>
<p>Generic Functions implemented for SESoutput Object:
</p>
<table>
<tr><td><code>plot(object=SESoutput</code>, <code>mode="all")</code></td>
<td>

<p>Plots the generated pvalues (using barplot) of the current SESoutput object in comparison to the threshold.
</p>
<p>Argument mode can be either &quot;all&quot; or &quot;partial&quot; for the first 500 pvalues of the object.
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The packages required by the SES and MMPC algorithm operations are: 
</p>
<p><b>quantreg</b>: for the quantile (median) regression
</p>
<p><b>MASS</b>: for negative binomial regression and simple ordinal regression
</p>
<p><b>nnet</b> : also require(stats) and require(MASS) for the testIndLogistic test
</p>
<p><b>survival</b> : for the censIndCR, censIndWR and the censIndER tests
</p>
<p><b>doParallel</b>: for parallel computations
</p>
<p><b>lme4</b>: for (generalised) linear mixed models
</p>
<p><b>Rfast</b>: for many fast functions.
</p>


<h3>Author(s)</h3>

<p>Ioannis Tsamardinos, Vincenzo Lagani
</p>
<p>R implementation and documentation: Giorgos Athineou &lt;athineou@csd.uoc.gr&gt; Vincenzo Lagani &lt;vlagani@csd.uoc.gr&gt;  
</p>


<h3>References</h3>

<p>Feature Selection with the R Package MXM: Discovering Statistically Equivalent Feature Subsets, Lagani, V. and Athineou, 
G. and Farcomeni, A. and Tsagris, M. and Tsamardinos, I. (2017). Journal of Statistical Software, 80(7).
</p>
<p>I. Tsamardinos, V. Lagani and D. Pappas (2012). Discovering multiple, equivalent biomarker signatures. In proceedings of the 
7th conference of the Hellenic Society for Computational Biology &amp; Bioinformatics - HSCBB12.
</p>
<p>Tsamardinos, I., Aliferis, C. F., &amp; Statnikov, A. (2003). Time and sample efficient discovery of Markov blankets and direct causal 
relations. In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 673-678). ACM.
</p>
<p>Brown, L. E., Tsamardinos, I., &amp; Aliferis, C. F. (2004). A novel algorithm for scalable and accurate Bayesian network learning. 
Medinfo, 711-715.
</p>
<p>Tsamardinos, Brown and Aliferis (2006). The max-min hill-climbing Bayesian network structure learning algorithm. Machine learning, 
65(1), 31-78.
</p>




<h3>See Also</h3>

<p><code><a href="#topic+CondIndTests">CondIndTests</a>, <a href="#topic+cv.ses">cv.ses</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)

#simulate a dataset with continuous data
dataset &lt;- matrix(runif(100 * 50, 1, 100), ncol = 50)

#define a simulated class variable 
target &lt;- 3 * dataset[, 10] + 2 * dataset[, 15] + 3 * dataset[, 20] + rnorm(100, 0, 5)

# define some simulated equivalences
dataset[, 16] &lt;- dataset[, 10] + rnorm(100, 0, 2)
dataset[, 17] &lt;- dataset[, 15] + rnorm(100, 0, 2)

# run the SES algorithm
sesObject &lt;- SES(target , dataset, max_k = 5, threshold = 0.05, test = "testIndFisher", 
hash = TRUE, hashObject = NULL);

# get the queues with the equivalences for each selected variable
sesObject@queues
#get the generated signatures
sesObject@signatures;

# re-run the SES algorithm with the same or different configuration 
# under the hash-based implementation of retrieving the statistics
# in the SAME dataset (!important)
hashObj &lt;- sesObject@hashObject;
sesObject2 &lt;- SES(target, dataset, max_k = 2, threshold = 0.01, test = "testIndFisher", 
hash = TRUE, hashObject = hashObj);

# get the run time
sesObject@runtime;
sesObject2@runtime;

# MMPC algorithm 
mmpcObject &lt;- MMPC(target, dataset, max_k = 3, threshold = 0.05, test="testIndFisher");
mmpcObject@selectedVars
mmpcObject@runtime
</code></pre>

<hr>
<h2 id='Constraint+20based+20feature+20selection+20algorithms+20for+20longitudinal+20and+20clustered+20data'>
SES.glmm/SES.gee: Feature selection algorithm for identifying multiple minimal, statistically-equivalent and equally-predictive feature signatures with correlated data
</h2><span id='topic+SES.glmm'></span><span id='topic+MMPC.glmm'></span><span id='topic+MMPC.gee'></span><span id='topic+SES.gee'></span>

<h3>Description</h3>

<p>SES.glmm algorithm follows a forward-backward filter approach for feature selection in order to provide minimal, highly-predictive, statistically-equivalent, multiple feature subsets of a high dimensional dataset. See also Details. MMPC.glmm algorithm follows the same approach without generating multiple feature subsets. They are both adapted to longitudinal target variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SES.glmm(target, reps = NULL, group, dataset, max_k = 3, threshold = 0.05, 
test = NULL, ini = NULL, wei = NULL, user_test = NULL, hash = FALSE, 
hashObject = NULL, slopes = FALSE, ncores = 1)

MMPC.glmm(target, reps = NULL, group, dataset, max_k = 3, threshold = 0.05, 
test = NULL, ini = NULL, wei = NULL, user_test = NULL, hash = FALSE, 
hashObject = NULL, slopes = FALSE, ncores = 1)

MMPC.gee(target, reps = NULL, group, dataset, max_k = 3, threshold = 0.05, 
test = NULL, ini = NULL, wei = NULL, user_test = NULL, hash = FALSE, 
hashObject = NULL, correl = "exchangeable", se = "jack", ncores = 1)

SES.gee(target, reps = NULL, group, dataset, max_k = 3, threshold = 0.05, 
test = NULL, ini = NULL, wei = NULL, user_test = NULL, hash = FALSE, 
hashObject = NULL, correl = "exchangeable", se = "jack", ncores = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data_+3A_target">target</code></td>
<td>

<p>The class variable. Provide a vector with continuous (normal), binary (binomial) or discrete (Poisson) data.
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data_+3A_reps">reps</code></td>
<td>

<p>A numeric vector containing the time points of the subjects. It's length is equal to the length of the target variable. If you have 
clustered data, leave this NULL. 
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data_+3A_group">group</code></td>
<td>

<p>A numeric vector containing the subjects or groups. It must be of the same legnth as target. 
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data_+3A_dataset">dataset</code></td>
<td>

<p>The dataset; provide either a data frame or a matrix (columns = variables , rows = samples). Currently, only continuous datasets are supported. 
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data_+3A_max_k">max_k</code></td>
<td>

<p>The maximum conditioning set to use in the conditional indepedence test (see Details). Integer, default value is 3.
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0, 1)) for assessing p-values significance. Default value is 0.05.
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data_+3A_test">test</code></td>
<td>

<p>The conditional independence test to use. Default value is NULL. Currently, the only available conditional independence tests are the 
&quot;testIndGLMMLogistic&quot;, &quot;testIndGLMMPois&quot;, &quot;testIndGLMMGamma&quot;, &quot;testIndGLMMNormLog&quot;, &quot;testIndGLMMOrdinal&quot;, &quot;testIndGLMMReg&quot;, &quot;testIndLMM&quot; 
and &quot;testIndGLMMCR&quot; for generalised linear mixed models. For the GEE, the available tests are &quot;testIndGEEReg&quot;, &quot;testIndGEEPois&quot;, 
&quot;testIndGEELogistic&quot;, &quot;testIndGEEGamma&quot; and &quot;testIndGEENormLog&quot;. 
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data_+3A_ini">ini</code></td>
<td>

<p>This is a supposed to be a list. After running SES or MMPC with some hyper-parameters you might want to run SES again with different 
hyper-parameters. To avoid calculating the univariate associations (first step of SES and of MPPC) again, you can extract them 
from the first run of SES and plug them here. This can speed up the 
second run (and subequent runs of course) by 50%. See the details and the argument &quot;univ&quot; in the output values. 
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. 
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data_+3A_user_test">user_test</code></td>
<td>

<p>A user-defined conditional independence test (provide a closure type object). Default value is NULL. If this is defined, the 
&quot;test&quot; argument is ignored.
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data_+3A_hash">hash</code></td>
<td>

<p>A boolean variable which indicates whether (TRUE) or not (FALSE) to store the statistics calculated during SES execution in a 
hash-type object. Default value is FALSE. If TRUE a hashObject is produced.
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data_+3A_hashobject">hashObject</code></td>
<td>

<p>A List with the hash objects generated in a previous run of SES.glmm. 
Each time SES runs with &quot;hash=TRUE&quot; it produces a list of hashObjects that can be re-used in order to speed up next runs of SES.
</p>
<p>Important: the generated hashObjects should be used only when the same dataset is re-analyzed, possibly with different values of max_k and threshold.
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data_+3A_slopes">slopes</code></td>
<td>

<p>Should random slopes for the ime effect be fitted as well? Default value is FALSE. 
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data_+3A_correl">correl</code></td>
<td>

<p>The correlation structure. For the Gaussian, Logistic, Poisson and Gamma regression this can be either &quot;exchangeable&quot; (compound symmetry, 
suitable for clustered data) or &quot;ar1&quot; (AR(1) model, suitable for longitudinal data). For the ordinal logistic regression its only the 
&quot;exchangeable&quot; correlation sturcture. 
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data_+3A_se">se</code></td>
<td>

<p>The method for estimating standard errors. This is very important and crucial. The available options for Gaussian, Logistic, Poisson and 
Gamma regression are: a) 'san.se': the usual robust estimate. b) 'jack': approximate jackknife variance estimate. c) 'j1s': if 1-step 
jackknife variance estimate and d) 'fij': fully iterated jackknife variance estimate. If you have many clusters (sets of repeated measurements) 
&quot;san.se&quot; is fine as it is asympotically correct, plus jacknife estimates will take longer. If you have a few clusters, then maybe it's better 
to use jacknife estimates. 
</p>
<p>The jackknife variance estimator was suggested by Paik (1988), which is quite suitable for cases when the number of subjects is small 
(K &lt; 30), as in many biological studies. The simulation studies conducted by Ziegler et al. (2000) and Yan and Fine (2004) showed that 
the approximate jackknife estimates are in many cases in good agreement with the fully iterated ones. 
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data_+3A_ncores">ncores</code></td>
<td>

<p>How many cores to use. This plays an important role if you have tens of thousands of variables or really large sample sizes and tens of 
thousands of variables and a regression based test which requires numerical optimisation. In other cases it will not make a difference 
in the overall time (in fact it can be slower). The parallel computation is used in the first step of the algorithm, where univariate 
associations are examined, those take place in parallel. We have seen a reduction in time of 50% with 4 cores in comparison to 1 core. 
Note also, that the amount of reduction is definetely not linear in the number of cores.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The SES.glmm function implements the Statistically Equivalent Signature (SES) algorithm as presented in &quot;Tsamardinos, Lagani and Pappas, 
HSCBB 2012&quot; adapted to longitudinal data. The citation for this is &quot;Tsagris, Lagani and tsamardinos, (2018)&quot;. These functions presented 
here are for the <b>temporal-lonitudinal scenario</b>. 
</p>
<p>The MMPC function mplements the MMPC algorithm as presented in &quot;Tsamardinos, Brown and Aliferis. The max-min hill-climbing Bayesian network structure learning algorithm&quot; adapted to longitudinal data. 
</p>
<p>The output value &quot;univ&quot; along with the output value &quot;hashObject&quot; can speed up the computations of subesequent runs of SES and MMPC. 
The first run with a specific pair of hyper-parameters (threshold and max_k) the univariate associations tests and the conditional 
independence tests (test statistic and logarithm of their corresponding p-values) are stored and returned. In the next run(s) with 
different pair(s) of hyper-parameters you can use this information to save time. With a few thousands of variables you will see the 
difference, which can be up to 50%. 
</p>
<p>The max_k option: the maximum size of the conditioning set to use in the conditioning independence test. Larger values provide more 
accurate results, at the cost of higher computational times. When the sample size is small (e.g., <code class="reqn">&lt;50</code> observations) the max_k 
parameter should be <code class="reqn">\leq 5</code>, otherwise the conditional independence test may not be able to provide reliable results.
</p>
<p>If the dataset contains missing (NA) values, they will automatically be replaced by the current variable (column) mean value with an 
appropriate warning to the user after the execution.
</p>
<p>If the target is a single integer value or a string, it has to corresponds to the column number or to the name of the target feature in 
the dataset. In any other case the target is a variable that is not contained in the dataset.
</p>
<p>If the current 'test' argument is defined as NULL or &quot;auto&quot; and the user_test argument is NULL then the algorithm automatically selects 
only available, which is <code><a href="#topic+testIndGLMMReg">testIndGLMMReg</a></code>.
</p>
<p>Conditional independence test functions to be pass through the user_test argument should have the same signature of the included test. 
See &quot;?testIndFisher&quot; for an example.
</p>
<p>For all the available conditional independence tests that are currently included on the package, please see &quot;?CondIndTests&quot;. 
</p>
<p>If two or more p-values are below the machine epsilon (.Machine$double.eps which is equal to 2.220446e-16), all of them are set to 0. 
To make the comparison or the ordering feasible we use the logarithm of the p-value. The max-min heuristic though, requires comparison 
and an ordering of the p-values. Hence, all conditional independence tests calculate the logarithm of the p-value.
</p>
<p>If there are missing values in the dataset (predictor variables) columnwise imputation takes place. The median is used for the continuous 
variables and the mode for categorical variables. It is a naive and not so clever method. For this reason the user is encouraged to make 
sure his data contain no missing values. 
</p>
<p>If you have percentages, in the (0, 1) interval, they are automatically mapped into <code class="reqn">R</code> by using the logit transformation and a 
linear mixed model is fitted. If you have binary data, logistic mixed regression is applied and if you have discrete data (counts), 
Poisson mixed regression is applied. 
</p>
<p>If you want to use the GEE methodology, make sure you load the library geepack first. 
</p>


<h3>Value</h3>

<p>The output of the algorithm is an object of the class 'SES.glmm.output' for SES.glmm or 'MMPC.glmm.output' for MMPC.glmm including:
</p>
<table>
<tr><td><code>selectedVars</code></td>
<td>

<p>The selected variables, i.e., the signature of the target variable.
</p>
</td></tr>
<tr><td><code>selectedVarsOrder</code></td>
<td>

<p>The order of the selected variables according to increasing pvalues.
</p>
</td></tr>
<tr><td><code>queues</code></td>
<td>

<p>A list containing a list (queue) of equivalent features for each variable included in selectedVars. An equivalent signature can 
be built by selecting a single feature from each queue. Featured only in SES.
</p>
</td></tr>
<tr><td><code>signatures</code></td>
<td>

<p>A matrix reporting all equivalent signatures (one signature for each row). Featured only in SES.
</p>
</td></tr>
<tr><td><code>hashObject</code></td>
<td>

<p>The hashObject caching the statistic calculted in the current run.
</p>
</td></tr>
<tr><td><code>pvalues</code></td>
<td>

<p>For each feature included in the dataset, this vector reports the strength of its association with the target in the context of all other variables. Particularly, this vector reports the max p-values found when the association of each variable with the target is tested against different conditional sets. Lower values indicate higher association. <b>Note that these are the logarithm of the p-values</b>.
</p>
</td></tr>
<tr><td><code>stats</code></td>
<td>

<p>The statistics corresponding to &quot;pvalues&quot; (higher values indicates higher association).
</p>
</td></tr>
<tr><td><code>univ</code></td>
<td>

<p>This is a list with the univariate associations. The test statistics and their corresponding <b>logged p-values</b>. This list is very 
important for subsequent runs of SES with different hyper-parameters. After running SES with some hyper-parameters you might want 
to run SES again with different hyper-parameters. To avoid calculating the univariate associations (first step of SES or MMPC) 
again, you can take this list from the first run of SES and plug it in the argument &quot;ini&quot; in the next run(s) of SES or MMPC. 
This can speed up the second run (and subequent runs of course) by 50%. See the argument &quot;univ&quot; in the output values. 
</p>
</td></tr>
<tr><td><code>max_k</code></td>
<td>

<p>The max_k option used in the current run.
</p>
</td></tr>
<tr><td><code>threshold</code></td>
<td>

<p>The threshold option used in the current run.
</p>
</td></tr>
<tr><td><code>n.tests</code></td>
<td>

<p>If you have set hash = TRUE, then the number of tests performed by SES or MMPC will be returned. If you have not set this to TRUE,
the number of univariate associations will be returned. So be careful with this number.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The run time of the algorithm. A numeric vector. The first element is the user time, the second element is the system time and the 
third element is the elapsed time.
</p>
</td></tr>
<tr><td><code>slope</code></td>
<td>

<p>Whether random slopes for the time effects were used or not, TRUE or FALSE.
</p>
</td></tr>
</table>
<p>Generic Functions implemented for SESoutput Object:
</p>
<table>
<tr><td><code>plot(object=SES.glmm.output</code>, <code>mode="all")</code></td>
<td>

<p>Plots the generated pvalues (using barplot) of the current SESoutput object in comparison to the threshold.
Argument mode can be either &quot;all&quot; or &quot;partial&quot; for the first 500 pvalues of the object.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ioannis Tsamardinos, Vincenzo Lagani
</p>
<p>R implementation and documentation: Giorgos Athineou &lt;athineou@csd.uoc.gr&gt; Vincenzo Lagani &lt;vlagani@csd.uoc.gr&gt;  
</p>


<h3>References</h3>

<p>Tsagris, M., Lagani, V., &amp; Tsamardinos, I. (2018). Feature selection for high-dimensional glmm data. BMC bioinformatics, 19(1), 17.
</p>
<p>I. Tsamardinos, M. Tsagris and V. Lagani (2015). Feature selection for longitudinal data. Proceedings of the 10th conference of 
the Hellenic Society for Computational Biology &amp; Bioinformatics (HSCBB15).
</p>
<p>I. Tsamardinos, V. Lagani and D. Pappas (2012). Discovering multiple, equivalent biomarker signatures. In proceedings of the 7th 
conference of the Hellenic Society for Computational Biology &amp; Bioinformatics - HSCBB12.
</p>
<p>Tsamardinos, Brown and Aliferis (2006). The max-min hill-climbing Bayesian network structure learning algorithm. Machine learning, 65(1), 31-78.
</p>
<p>Eugene Demidenko (2013). Mixed Models: Theory and Applications with R, 2nd Edition. New Jersey: Wiley &amp; Sons.
</p>
<p>J. Pinheiro and D. Bates. Mixed-effects models in S and S-PLUS. Springer Science &amp; Business Media, 2006.
</p>
<p>Liang K.Y. and Zeger S.L. (1986). Longitudinal data analysis using generalized linear models. Biometrika, 73(1): 13-22.
</p>
<p>Prentice R.L. and Zhao L.P. (1991). Estimating equations for parameters in means and covariances of multivariate discrete and 
continuous responses. Biometrics, 47(3): 825-839.
</p>
<p>Heagerty P.J. and Zeger S.L. (1996) Marginal regression models for clustered ordinal measurements. Journal of the American Statistical 
Association, 91(435): 1024-1036.
</p>
<p>Paik M.C. (1988). Repeated measurement analysis for nonnormal data in small samples. Communications in Statistics-Simulation and 
Computation, 17(4): 1155-1171.
</p>
<p>Ziegler A., Kastner C., Brunner D. and Blettner M. (2000). Familial associations of lipid profiles: A generalised estimating 
equations approach. Statistics in medicine, 19(24): 3345-3357
</p>
<p>Yan J. and Fine J. (2004). Estimating equations for association structures. Statistics in medicine, 23(6): 859-874.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CondIndTests">CondIndTests</a>, <a href="#topic+testIndGLMMReg">testIndGLMMReg</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
require(lme4)
data(sleepstudy)
reaction &lt;- sleepstudy$Reaction
days &lt;- sleepstudy$Days
subject &lt;- sleepstudy$Subject
x &lt;- matrix(rnorm(180 * 200),ncol = 200) ## unrelated predictor variables
m1 &lt;- SES.glmm(target = reaction, reps = days, group = subject, dataset = x)
m2 &lt;- MMPC.glmm(target = reaction, reps = days, group = subject, dataset = x)

## End(Not run)
</code></pre>

<hr>
<h2 id='Constraint+20based+20feature+20selection+20algorithms+20for+20multiple+20datasets'>
ma.ses: Feature selection algorithm for identifying multiple minimal, statistically-equivalent and equally-predictive feature signatures with multiple datasets
ma.mmpc: Feature selection algorithm for identifying minimal feature subsets with multiple datasets
</h2><span id='topic+ma.ses'></span><span id='topic+ma.mmpc'></span>

<h3>Description</h3>

<p>SES algorithm follows a forward-backward filter approach for feature selection in order to provide minimal, highly-predictive, statistically-equivalent, multiple feature subsets of two or more high dimensional datasets. 
See also Details. MMPC algorithm follows the same approach without generating multiple feature subsets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ma.ses(target, dataset, ina, statistic = FALSE, max_k = 3, threshold = 0.05, 
test = NULL, ini = NULL, user_test = NULL, hash = FALSE, hashObject = NULL, 
ncores = 1)
 
ma.mmpc(target, dataset, ina, statistic = FALSE, max_k = 3, threshold = 0.05, 
test = NULL, ini = NULL, user_test = NULL, hash = FALSE, hashObject = NULL, 
ncores = 1, backward = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms+2B20for+2B20multiple+2B20datasets_+3A_target">target</code></td>
<td>

<p>The class variable. Provide either a string, an integer, a numeric value, a vector, a factor, an ordered factor or a Surv object. See also Details.
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms+2B20for+2B20multiple+2B20datasets_+3A_dataset">dataset</code></td>
<td>

<p>The data-set; provide either a data frame or a matrix (columns = variables , rows = samples).
Alternatively, provide an ExpressionSet (in which case rows are samples and columns are features, see bioconductor for details).
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms+2B20for+2B20multiple+2B20datasets_+3A_ina">ina</code></td>
<td>

<p>A numerical vector indicating the dataset. The numbers must be 1, 2, 3,...
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms+2B20for+2B20multiple+2B20datasets_+3A_statistic">statistic</code></td>
<td>
 
<p>A boolean variable indicating whether the test statistics (TRUE) or the p-values should be combined (FALSE). See the details about this. 
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms+2B20for+2B20multiple+2B20datasets_+3A_max_k">max_k</code></td>
<td>

<p>The maximum conditioning set to use in the conditional indepedence test (see Details). Integer, default value is 3.
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms+2B20for+2B20multiple+2B20datasets_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0, 1)) for assessing p-values significance. Default value is 0.05.
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms+2B20for+2B20multiple+2B20datasets_+3A_test">test</code></td>
<td>

<p>The conditional independence test to use. Default value is NULL. See also <code><a href="#topic+CondIndTests">CondIndTests</a></code>.
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms+2B20for+2B20multiple+2B20datasets_+3A_ini">ini</code></td>
<td>

<p>This is a supposed to be a list. After running SES or MMPC with some hyper-parameters you might want to run SES again with different hyper-parameters. To avoid calculating the univariate associations (first step of SES and of MPPC) again, you can extract them from the first run of SES and plug them here. This can speed up the second run (and subequent runs of course) by 50%. See the details and the argument &quot;univ&quot; in the output values. 
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms+2B20for+2B20multiple+2B20datasets_+3A_user_test">user_test</code></td>
<td>

<p>A user-defined conditional independence test (provide a closure type object). Default value is NULL. If this is defined, the &quot;test&quot; argument is ignored.
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms+2B20for+2B20multiple+2B20datasets_+3A_hash">hash</code></td>
<td>

<p>A boolean variable which indicates whether (TRUE) or not (FALSE) to store the statistics calculated during SES execution in a hash-type object. Default value is FALSE. If TRUE a hashObject is produced.
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms+2B20for+2B20multiple+2B20datasets_+3A_hashobject">hashObject</code></td>
<td>

<p>A List with the hash objects generated in a previous run of SES or MMPC. 
Each time SES runs with &quot;hash=TRUE&quot; it produces a list of hashObjects that can be re-used in order to speed up next runs of SES or MMPC.
</p>
<p>Important: the generated hashObjects should be used only when the same dataset is re-analyzed, possibly with different values of max_k and threshold.
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms+2B20for+2B20multiple+2B20datasets_+3A_ncores">ncores</code></td>
<td>

<p>How many cores to use. This plays an important role if you have tens of thousands of variables or really large sample sizes and tens of thousands of variables and a regression based test which requires numerical optimisation. In other cases it will not make a difference in the overall time (in fact it can be slower). The parallel computation is used in the first step of the algorithm, where univariate associations are examined, those take place in parallel. We have seen a reduction in time of 50% with 4 cores in comparison to 1 core. Note also, that the amount of reduction is not linear in the number of cores.
</p>
</td></tr>
<tr><td><code id="Constraint+2B20based+2B20feature+2B20selection+2B20algorithms+2B20for+2B20multiple+2B20datasets_+3A_backward">backward</code></td>
<td>

<p>If TRUE, the backward (or symmetry correction) phase will be implemented. This removes any falsely included variables in the parents and children set of the target variable. This is an experimental stage, so do not trust too much or ven better do not use. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is more at an experimental stage at the present. 
</p>
<p>The SES function implements the Statistically Equivalent Signature (SES) algorithm as presented in &quot;Tsamardinos, Lagani and Pappas, HSCBB 2012&quot;. 
</p>
<p>The MMPC function mplements the MMPC algorithm as presented in &quot;Tsamardinos, Brown and Aliferis. The max-min hill-climbing Bayesian network structure learning algorithm&quot;.
</p>
<p>The output value &quot;univ&quot; along with the output value &quot;hashObject&quot; can speed up the computations of subesequent runs of SES and MMPC. 
The first run with a specific pair of hyper-parameters (threshold and max_k) the univariate associations tests and the conditional independence tests (test statistic and logarithm of their corresponding p-values) 
are stored and returned. In the next run(s) with different pair(s) of hyper-parameters you can use this information to save time. With a few thousands of variables you will see the difference, 
which can be up to 50%. For the non robust correlation based tests, the difference may not be significant though, because the unconditional correlation coefficients are calcualted very efficiently. 
</p>
<p>The max_k option: the maximum size of the conditioning set to use in the conditioning independence test. Larger values provide more accurate results, at the cost of higher computational times. When the sample size is small (e.g., <code class="reqn">&lt;50 </code> observations) the max_k parameter should be <code class="reqn"> \leq 5 </code>, otherwise the conditional independence test may not be able to provide reliable results.
</p>
<p>If the dataset (predictor variables) contains missing (NA) values, they will automatically be replaced by the current variable (column) mean value with an appropriate warning to the user after the execution.
</p>










<p>Conditional independence test functions to be pass through the user_test argument should have the same signature of the included test. See <code> <a href="#topic+testIndFisher">testIndFisher</a> </code> for an example.
</p>
<p>For all the available conditional independence tests that are currently included on the package, please see <code><a href="#topic+CondIndTests">CondIndTests</a> </code>. 
</p>
<p>If two or more p-values are below the machine epsilon (.Machine$double.eps which is equal to 2.220446e-16), all of them are set to 0. To make the comparison or the ordering feasible we use the logarithm of the p-value. The max-min heuristic though, requires comparison and an ordering of the p-values. Hence, all conditional independence tests calculate the logarithm of the p-value.
</p>
<p>If there are missing values in the dataset (predictor variables) columnwise imputation takes place. The median is used for the continuous variables and the mode for categorical variables. It is a naive and not so clever method. For this reason the user is encouraged to make sure his data contain no missing values. 
</p>
<p>If you have percentages, in the (0, 1) interval, they are automatically mapped into <code class="reqn">R </code> by using the logit transformation. 

</p>
<p>If you use testIndSpearman (argument &quot;test&quot;), the ranks of the data calculated and those are used in the caclulations. This speeds up the whole procedure.
</p>
<p>Currently only the testIndFisher and testIndSpearman tests are supported for use in the algorithm. 
</p>
<p>If the argument <b>statistic </b> is set to FALSE, the p-values from the hypothesis test of each dataset are combined via Fisher's meta-analytic approach, that is <code class="reqn"> T=-2\sum_{i=1}^k\log{p_i} </code>
and <code class="reqn"> T~\chi^2_{2k} </code>. If <b>statistic </b> is TRUE, the test statistics are combined as 
<code class="reqn"> T = \frac{{\sum_{i=1}^kt_i/se(t_i)}}{\sum_{i=1}^k 1/se(t_i)}</code> and <code class="reqn">T~N(0,1)</code>.
</p>


<h3>Value</h3>

<p>The output of the algorithm is an object of the class 'SESoutput' for SES or 'MMPCoutput' for MMPC including:
</p>
<table>
<tr><td><code>selectedVars</code></td>
<td>

<p>The selected variables, i.e., the signature of the target variable.
</p>
</td></tr>
<tr><td><code>selectedVarsOrder</code></td>
<td>

<p>The order of the selected variables according to increasing pvalues.
</p>
</td></tr>
<tr><td><code>queues</code></td>
<td>

<p>A list containing a list (queue) of equivalent features for each variable included in selectedVars. An equivalent signature can be built by selecting a single feature from each queue. Featured only in SES.
</p>
</td></tr>
<tr><td><code>signatures</code></td>
<td>

<p>A matrix reporting all equivalent signatures (one signature for each row). Featured only in SES.
</p>
</td></tr>
<tr><td><code>hashObject</code></td>
<td>

<p>The hashObject caching the statistic calculated in the current run.
</p>
</td></tr>
<tr><td><code>pvalues</code></td>
<td>

<p>For each feature included in the dataset, this vector reports the strength of its association with the target in the context of all other variables. 
Particularly, this vector reports the max p-values (on a logarithmic scale) found when the association of each variable with the target is tested against different conditional sets. Lower values indicate higher association.
</p>
</td></tr>
<tr><td><code>stats</code></td>
<td>

<p>The statistics corresponding to &quot;pvalues&quot; (higher values indicates higher association).
</p>
</td></tr>
<tr><td><code>univ</code></td>
<td>

<p>This is a list with the univariate associations. The test statistics and their corresponding logged p-values, along with their flag (1 if the test was perfromed and 0 otherwise). This list is very important for subsequent runs of SES with different hyper-parameters. After running SES with some hyper-parameters you might want to run SES again with different hyper-parameters. To avoid calculating the univariate associations (first step of SES or MMPC) again, you can take this list from the first run of SES and plug it in the argument &quot;ini&quot; in the next run(s) of SES or MMPC. This can speed up the second run (and subequent runs of course) by 50%. See the argument &quot;univ&quot; in the output values. 
</p>
</td></tr>
<tr><td><code>max_k</code></td>
<td>

<p>The max_k option used in the current run.
</p>
</td></tr>
<tr><td><code>threshold</code></td>
<td>

<p>The threshold option used in the current run.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The run time of the algorithm. A numeric vector. The first element is the user time, the second element is the system time and the third element is the elapsed time.
</p>
</td></tr>
<tr><td><code>test</code></td>
<td>

<p>The character name of the statistic test used.
</p>
</td></tr>
</table>
<p>Generic Function implemented for SESoutput Object:
</p>
<table>
<tr><td><code>plot(object=SESoutput</code>, <code>mode="all")</code></td>
<td>

<p>Plots the generated pvalues (using barplot) of the current SESoutput object in comparison to the threshold.
</p>
<p>Argument mode can be either &quot;all&quot; or &quot;partial&quot; for the first 500 pvalues of the object.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ioannis Tsamardinos, Vincenzo Lagani
</p>
<p>R implementation and documentation: Giorgos Athineou &lt;athineou@csd.uoc.gr&gt;, Vincenzo Lagani &lt;vlagani@csd.uoc.gr&gt;  and Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>V. Lagani, A.D. Karozou, D. Gomez-Cabrero, G. Silberberg and I. Tsamardinos (2016). A comparative evaluation of data-merging and meta-analysis methods for reconstructing gene-gene interactions, BMC Bioinformatics 17(Supplementary 5): 287-305. 
</p>
<p>I. Tsamardinos, V. Lagani and D. Pappas (2012). Discovering multiple, equivalent biomarker signatures. In proceedings of the 7th conference of the Hellenic Society for Computational Biology &amp; Bioinformatics - HSCBB12.
</p>
<p>Tsamardinos, Brown and Aliferis (2006). The max-min hill-climbing Bayesian network structure learning algorithm. Machine learning, 65(1), 31-78.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SES">SES</a>, <a href="#topic+CondIndTests">CondIndTests</a>, <a href="#topic+cv.ses">cv.ses</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)

#simulate a dataset with continuous data
dataset &lt;- matrix(runif(400 * 100, 1, 100), ncol = 100)

#define a simulated class variable 
target &lt;- 3 * dataset[, 10] + 2 * dataset[, 20] + 3 * dataset[, 30] + rnorm(400, 0, 5)

#define some simulated equivalences
dataset[, 15] &lt;- dataset[, 10] + rnorm(400, 0, 2)
dataset[, 10] &lt;- dataset[ , 10] + rnorm(400, 0, 2)
dataset[, 25] &lt;- dataset[, 20] + rnorm(400, 0, 2) 
dataset[, 23] &lt;- dataset[, 20] + rnorm(400, 0, 2)

#run the SES algorithm
a1 &lt;- SES(target , dataset, max_k = 5, threshold = 0.05, test = "testIndFisher", 
hash = TRUE, hashObject = NULL)

ina &lt;- rbinom(400, 2, 0.5) + 1
a2 &lt;- ma.ses(target , dataset, ina = ina, max_k = 5, threshold = 0.05, test = "testIndFisher", 
hash = TRUE, hashObject = NULL)
a3 &lt;- ma.mmpc(target , dataset, ina = ina, max_k = 5, threshold = 0.05, test = "testIndFisher", 
hash = TRUE, hashObject = NULL)

#get the generated signatures
a1@signatures
a2@signatures
a3@selectedVars

</code></pre>

<hr>
<h2 id='Correlation+20based+20conditonal+20independence+20tests'>
Fisher and Spearman conditional independence test for continuous class variables
</h2><span id='topic+testIndFisher'></span><span id='topic+testIndSpearman'></span><span id='topic+testIndMMFisher'></span><span id='topic+permFisher'></span><span id='topic+permMMFisher'></span><span id='topic+permDcor'></span>

<h3>Description</h3>

<p>The main task of this test is to provide a p-value PVALUE for the null hypothesis: feature 'X' is independent from 'TARGET' given a conditioning set CS.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>testIndFisher(target, dataset, xIndex, csIndex, wei = NULL, statistic = FALSE, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, 
pvalue_hash = NULL)

testIndMMFisher(target, dataset, xIndex, csIndex, wei = NULL, statistic = FALSE, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, 
pvalue_hash = NULL)

testIndSpearman(target, dataset, xIndex, csIndex, wei = NULL, statistic = FALSE, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, 
pvalue_hash = NULL)

permFisher(target, dataset, xIndex, csIndex, wei = NULL, statistic = FALSE, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, 
pvalue_hash = NULL, threshold = 0.05, R = 999)

permMMFisher(target, dataset, xIndex, csIndex, wei = NULL, statistic = FALSE, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, 
pvalue_hash = NULL, threshold = 0.05, R = 999)

permDcor(target, dataset, xIndex, csIndex, wei = NULL, statistic = FALSE, 
univariateModels = NULL, hash = FALSE, stat_hash = NULL, 
pvalue_hash = NULL, threshold = 0.05, R = 499)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Correlation+2B20based+2B20conditonal+2B20independence+2B20tests_+3A_target">target</code></td>
<td>

<p>A numeric vector containing the values of the target variable. If the values are proportions or percentages, i.e. strictly within 0 and 1 they are mapped into R using log( target/(1 - target) ). 
This can also be a list of vectors as well. In this case, the metanalytic approach is used. 
</p>
</td></tr>
<tr><td><code id="Correlation+2B20based+2B20conditonal+2B20independence+2B20tests_+3A_dataset">dataset</code></td>
<td>

<p>A numeric matrix containing the variables for performing the test. Rows as samples and columns as features.
</p>
</td></tr>
<tr><td><code id="Correlation+2B20based+2B20conditonal+2B20independence+2B20tests_+3A_xindex">xIndex</code></td>
<td>

<p>The index of the variable whose association with the target we want to test.
</p>
</td></tr>
<tr><td><code id="Correlation+2B20based+2B20conditonal+2B20independence+2B20tests_+3A_csindex">csIndex</code></td>
<td>

<p>The indices of the variables to condition on. If you have no variables set this equal to 0.
</p>
</td></tr>
<tr><td><code id="Correlation+2B20based+2B20conditonal+2B20independence+2B20tests_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. This is not used with &quot;permDcor&quot;.
</p>
</td></tr>
<tr><td><code id="Correlation+2B20based+2B20conditonal+2B20independence+2B20tests_+3A_statistic">statistic</code></td>
<td>
 
<p>A boolean variable indicating whether the test statistics (TRUE) or the p-values should be combined (FALSE). See the details about this.
For the permFisher test this is not taken into account.
</p>
</td></tr>
<tr><td><code id="Correlation+2B20based+2B20conditonal+2B20independence+2B20tests_+3A_univariatemodels">univariateModels</code></td>
<td>

<p>Fast alternative to the hash object for univariate test. List with vectors &quot;pvalues&quot; (p-values), &quot;stats&quot; (statistics) and &quot;flags&quot; (flag = TRUE if the test was succesful) representing the univariate association of each variable with the target. Default value is NULL.
</p>
</td></tr>
<tr><td><code id="Correlation+2B20based+2B20conditonal+2B20independence+2B20tests_+3A_hash">hash</code></td>
<td>

<p>A boolean variable which indicates whether (TRUE) or not (FALSE) to use the hash-based implementation of the statistics of SES. Default value is FALSE. If TRUE you have to specify the stat_hash argument and the pvalue_hash argument.
</p>
</td></tr>
<tr><td><code id="Correlation+2B20based+2B20conditonal+2B20independence+2B20tests_+3A_stat_hash">stat_hash</code></td>
<td>

<p>A hash object which contains the cached generated statistics of a SES run in the current dataset, using the current test.
</p>
</td></tr>
<tr><td><code id="Correlation+2B20based+2B20conditonal+2B20independence+2B20tests_+3A_pvalue_hash">pvalue_hash</code></td>
<td>

<p>A hash object which contains the cached generated p-values of a SES run in the current dataset, using the current test.
</p>
</td></tr>
<tr><td><code id="Correlation+2B20based+2B20conditonal+2B20independence+2B20tests_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0, 1)) for assessing p-values significance. Default value is 0.05. This is actually obsolete here, but has to be in order tyo have a concise list of input arguments across the same family of functions.
</p>
</td></tr>
<tr><td><code id="Correlation+2B20based+2B20conditonal+2B20independence+2B20tests_+3A_r">R</code></td>
<td>

<p>The number of permutations to use. The default value is 999. For the &quot;permDcor&quot; this is set to 499. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If hash = TRUE, testIndFisher requires the arguments 'stat_hash' and 'pvalue_hash' for the hash-based implementation of the statistic test. These hash Objects are produced or updated by each run of SES (if hash == TRUE) and they can be reused in order to speed up next runs of the current statistic test. If &quot;SESoutput&quot; is the output of a SES run, then these objects can be retrieved by SESoutput@hashObject$stat_hash and the SESoutput@hashObject$pvalue_hash.
</p>
<p>Important: Use these arguments only with the same dataset that was used at initialization.
</p>
<p>For all the available conditional independence tests that are currently included on the package, please see &quot;?CondIndTests&quot;.
</p>
<p>Note that if the <code><a href="#topic+testIndReg">testIndReg</a></code> is used instead the results will not be be the same, unless the sample size is very large. This is because the Fisher test uses the t distribution stemming from the Fisher's z transform and not the t distribution of the correlation coefficient.
</p>
<p>BE CAREFUL with testIndSpearman. The Pearson's correlation coefficient is actually calculated. So, you must have transformed the data into their ranks before plugging them here. The reason for this is to speed up the computation time, as this test can be used in SES, MMPC and mmhc.skel. The variance of the Fisher transformed Spearman's correlation is <code class="reqn">\frac{1.06}{n-3}</code> and the variance of the Fisher transformed Pearson's correlation coefficient is <code class="reqn">\frac{1}{n-3}</code>.
</p>
<p>When performing the above tests with multiple datasets, the test statistic and the p-values are combined in a meta-analytic way. Is up to the user to decide whether to use the fixed effects model approach and combine the test statistics (statistic = TRUE), or combine the p-values as Fisher suggested (statistic = FALSE). 
</p>
<p>The argument R is useful only for the permFisher and permDcor tests. The permDcor test uses the distance correlation instead of the usual Pearson or Spearman correlations.
</p>
<p>TestIndMMFisher does a robust estimation of the correlation via MM regression.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>pvalue</code></td>
<td>

<p>A numeric value that represents the logarithm of the generated p-value due to Fisher's method (see reference below).
</p>
</td></tr>
<tr><td><code>stat</code></td>
<td>

<p>A numeric value that represents the generated statistic due to Fisher's method (see reference below).
</p>
</td></tr>
<tr><td><code>stat_hash</code></td>
<td>

<p>The current hash object used for the statistics. See argument stat_hash and details. If argument hash = FALSE this is NULL.
</p>
</td></tr>
<tr><td><code>pvalue_hash</code></td>
<td>

<p>The current hash object used for the p-values. See argument stat_hash and details. If argument hash = FALSE this is NULL.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Vincenzo Lagani and Ioannis Tsamardinos
</p>
<p>R implementation and documentation: Giorgos Athineou &lt;athineou@csd.uoc.gr&gt; Vincenzo Lagani &lt;vlagani@csd.uoc.gr&gt;.
</p>


<h3>References</h3>

<p>Fisher R. A. (1925). Statistical methods for research workers. Genesis Publishing Pvt Ltd.
</p>
<p>Fisher R. A. (1948). Combining independent tests of significance. American Statistician, 2(5), 30&ndash;31
</p>
<p>Fisher R. A. (1915). Frequency distribution of the values of the correlation coefficient in samples from an indefinitely large population. Biometrika, 10(4): 507&ndash;521.
</p>
<p>Fieller E. C., Hartley H. O. and Pearson E. S. (1957). Tests for rank correlation coefficients. I. Biometrika, 44(3/4): 470&ndash;481.
</p>
<p>Fieller E. C. and Pearson E. S. (1961). Tests for rank correlation coefficients. II. Biometrika, 48(1/2): 29&ndash;40.
</p>
<p>Hampel F. R., Ronchetti E. M., Rousseeuw P. J., and Stahel W. A. (1986). Robust statistics: the approach based on influence functions. John Wiley &amp; Sons. 
</p>
<p>Pearson, K. (1895). Note on regression and inheritance in the case of two parents. Proceedings of the Royal Society of London, 58, 240&ndash;242.
</p>
<p>Peter Spirtes, Clark Glymour, and Richard Scheines. Causation, Prediction, and Search. The MIT Press, Cambridge, MA, USA, second edition, January 2001.
</p>
<p>Lee Rodgers J., and Nicewander W.A. (1988). &quot;Thirteen ways to look at the correlation coefficient.&quot; The American Statistician 42(1): 59&ndash;66.
</p>
<p>Shevlyakov G. and Smirnov P. (2011). Robust Estimation of the Correlation Coefficient: An Attempt of Survey. Austrian Journal of Statistics, 40(1 &amp; 2): 147&ndash;156.
</p>
<p>Szekely G.J. and Rizzo, M.L. (2014). Partial distance correlation with methods for dissimilarities. The Annals of Statistics, 42(6): 2382&ndash;2412.
</p>
<p>Szekely G.J. and Rizzo M.L. (2013).  Energy statistics: A class of statistics based on distances. Journal of Statistical Planning and Inference 143(8): 1249&ndash;1272.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+testIndSpearman">testIndSpearman</a>, <a href="#topic+testIndReg">testIndReg</a>, <a href="#topic+SES">SES</a>, <a href="#topic+testIndLogistic">testIndLogistic</a>, <a href="#topic+gSquare">gSquare</a>, <a href="#topic+CondIndTests">CondIndTests</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#simulate a dataset with continuous data
dataset &lt;- matrix(runif(300 * 50, 1, 1000), nrow = 50 )
#the target feature is the last column of the dataset as a vector
target &lt;- dataset[, 50]
res1 &lt;- testIndFisher(target, dataset, xIndex = 44, csIndex = 10)
res2 &lt;- testIndSpearman(target, dataset, xIndex = 44, csIndex = 10)
res3 &lt;- permFisher(target, dataset, xIndex = 44, csIndex = 10, R = 999)
res4 &lt;- permDcor(target, dataset, xIndex = 44, csIndex = 10, R = 99)

#define class variable (here tha last column of the dataset)
dataset &lt;- dataset[, -50]
#run the MMPC algorithm using the testIndFisher conditional independence test
mmpcObject &lt;- MMPC(target, dataset, max_k = 3, threshold = 0.05, test = "testIndFisher")
</code></pre>

<hr>
<h2 id='Cross-Validation+20for+20gOMP'>
Cross-Validation for gOMP
</h2><span id='topic+cv.gomp'></span>

<h3>Description</h3>

<p>The function performs a k-fold cross-validation for identifying the best tolerance values for gOMP.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.gomp(target, dataset, kfolds = 10, folds = NULL, tol = seq(4, 9, by = 1), 
task = "C", metric = NULL, metricbbc = NULL, modeler = NULL, test = NULL, 
method = "ar2", B = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Cross-Validation+2B20for+2B20gOMP_+3A_target">target</code></td>
<td>

<p>The target or class variable as in SES and MMPC. The difference is that it cannot accept a single numeric value, an integer indicating the column in the dataset. 
</p>
</td></tr>
<tr><td><code id="Cross-Validation+2B20for+2B20gOMP_+3A_dataset">dataset</code></td>
<td>

<p>The dataset object as in SES and MMPC.
</p>
</td></tr>
<tr><td><code id="Cross-Validation+2B20for+2B20gOMP_+3A_kfolds">kfolds</code></td>
<td>

<p>The number of the folds in the k-fold Cross Validation (integer).
</p>
</td></tr>
<tr><td><code id="Cross-Validation+2B20for+2B20gOMP_+3A_folds">folds</code></td>
<td>

<p>The folds of the data to use (a list generated by the function generateCVRuns TunePareto). If NULL the folds are created internally with the same function.
</p>
</td></tr>
<tr><td><code id="Cross-Validation+2B20for+2B20gOMP_+3A_tol">tol</code></td>
<td>

<p>A vector of tolerance values. 
</p>
</td></tr>
<tr><td><code id="Cross-Validation+2B20for+2B20gOMP_+3A_task">task</code></td>
<td>

<p>A character (&quot;C&quot;, &quot;R&quot; or &quot;S&quot;). It can be &quot;C&quot; for classification (logistic, multinomial or ordinal regression), &quot;R&quot; for regression (robust and non robust linear regression, median regression, 
(zero inflated) poisson and negative binomial regression, beta regression), &quot;S&quot; for survival regresion (Cox, Weibull or exponential regression).
</p>
</td></tr>
<tr><td><code id="Cross-Validation+2B20for+2B20gOMP_+3A_metric">metric</code></td>
<td>

<p>A metric function provided by the user. If NULL the following functions will be used: auc.mxm, mse.mxm, ci.mxm for classification, regression and 
survival analysis tasks, respectively. See details for more. If you know what you have put it here to avoid the function choosing somehting else. 
<b>Note</b> that you put these words as they are, without &quot;&quot;.  
</p>
</td></tr>
<tr><td><code id="Cross-Validation+2B20for+2B20gOMP_+3A_metricbbc">metricbbc</code></td>
<td>

<p>This is the same argument as &quot;metric&quot; with the difference that &quot; &quot; must be placed. If for example, metric = auc.mxm, here metricbbc = &quot;auc.mxm&quot;. The same value must be given here. This argument is to be used with the function <code><a href="#topic+bbc">bbc</a></code> which does bootstrap bias correction of the estimated performance (Tsamardinos, Greasidou and Borboudakis, 2018). This argument is valid if the last argument (B) is more than 1. 
</p>
</td></tr>
<tr><td><code id="Cross-Validation+2B20for+2B20gOMP_+3A_modeler">modeler</code></td>
<td>

<p>A modeling function provided by the user. If NULL the following functions will be used: glm.mxm, lm.mxm, coxph.mxm for classification, regression and survival analysis tasks, respectively. See details for more.
If you know what you have put it here to avoid the function choosing somehting else. <b>Note</b> that you put these words as they are, without &quot;&quot;. 
</p>
</td></tr>
<tr><td><code id="Cross-Validation+2B20for+2B20gOMP_+3A_test">test</code></td>
<td>

<p>A function object that defines the conditional independence test used in the SES function (see also SES help page). If NULL, &quot;testIndFisher&quot;, &quot;testIndLogistic&quot; and &quot;censIndCR&quot; are used for classification, regression and survival analysis tasks, respectively. If you know what you have put it here to avoid the function choosing somehting else. Not all tests can be included here. &quot;testIndClogit&quot;, &quot;testIndMVreg&quot;, &quot;testIndIG&quot;, &quot;testIndGamma&quot;, &quot;testIndZIP&quot; and &quot;testIndTobit&quot; are anot available at the moment.  
</p>
</td></tr>
<tr><td><code id="Cross-Validation+2B20for+2B20gOMP_+3A_method">method</code></td>
<td>

<p>This is only for the &quot;testIndFisher&quot;. You can either specify, &quot;ar2&quot; for the adjusted R-square or &quot;sse&quot; for the sum of squares of errors. The tolerance value in both cases must a number between 0 and 1. That will denote a percentage. If the percentage increase or decrease is less than the nubmer the algorithm stops. An alternative is &quot;BIC&quot; for BIC and the tolerance values are like in all other regression models.
</p>
</td></tr>
<tr><td><code id="Cross-Validation+2B20for+2B20gOMP_+3A_b">B</code></td>
<td>

<p>How many bootstrap re-samples to draw. This argument is to be used with the function <code><a href="#topic+bbc">bbc</a></code> which does bootstrap bias correction of the estimated performance (Tsamardinos, Greasidou and Borboudakis, 2018). If you have thousands of samples (observations) then this might not be necessary, as there is no optimistic bias to be corrected. What is the lower limit cannot be told beforehand however. SES and MMPC however were designed for the low sample cases, hence, bootstrap bias correction is perhaps a must thing to do.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For more details see also <code><a href="#topic+cv.ses">cv.ses</a></code>.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>cv_results_all</code></td>
<td>

<p>A list with predictions, performances and selected variables for each fold and each tolerance value. The elements are called
&quot;preds&quot;, &quot;performances&quot; and &quot;selectedVars&quot;.
</p>
</td></tr>
<tr><td><code>best_performance</code></td>
<td>

<p>A numeric value that represents the best average performance.
</p>
</td></tr>
<tr><td><code>best_configuration</code></td>
<td>

<p>A numeric value that represents the best tolerance value.
</p>
</td></tr>
<tr><td><code>bbc_best_performance</code></td>
<td>

<p>The bootstrap bias corrected best performance if B was more than 1, othwerwise this is NULL.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The runtime of the cross-validation procedure.
</p>
</td></tr>
</table>
<p>Bear in mind that the values can be extracted with the $ symbol, i.e. this is an S3 class output. 
</p>


<h3>Author(s)</h3>

<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Tsamardinos I., Greasidou E. and Borboudakis G. (2018).  
Bootstrapping the out-of-sample predictions for efficient and accurate cross-validation. 
Machine Learning 107(12): 1895-1922.  
<a href="https://link.springer.com/article/10.1007/s10994-018-5714-4">https://link.springer.com/article/10.1007/s10994-018-5714-4</a>
</p>
<p>Tsagris, M., Papadovasilakis, Z., Lakiotaki, K., &amp; Tsamardinos, I. (2022). 
The <code class="reqn">\gamma</code>-OMP algorithm for feature selection with application to gene expression data. 
IEEE/ACM Transactions on Computational Biology and Bioinformatics, 19(2): 1214-1224.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+cv.mmpc">cv.mmpc</a>, <a href="#topic+gomp.path">gomp.path</a>, <a href="#topic+bbc">bbc</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
set.seed(1234)
# simulate a dataset with continuous data
dataset &lt;- matrix( rnorm(200 * 50), ncol = 50 )
# the target feature is the last column of the dataset as a vector
target &lt;- dataset[, 50]
dataset &lt;- dataset[, -50]
# run a 10 fold CV for the regression task
best_model &lt;- cv.gomp(target, dataset, kfolds = 5, task = "R", 
tol = seq(0.001, 0.01,by=0.001), method = "ar2" )

## End(Not run)
</code></pre>

<hr>
<h2 id='Cross-validation+20for+20ridge+20regression'>
Cross validation for the ridge regression
</h2><span id='topic+ridgereg.cv'></span>

<h3>Description</h3>

<p>Cross validation for the ridge regression is performed using the TT estimate of bias (Tibshirani and Tibshirani, 2009).
There is an option for the GCV criterion which is automatic. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ridgereg.cv( target, dataset, K = 10, lambda = seq(0, 2, by = 0.1), auto = FALSE, 
seed = FALSE, ncores = 1, mat = NULL )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Cross-validation+2B20for+2B20ridge+2B20regression_+3A_target">target</code></td>
<td>

<p>A numeric vector containing the values of the target variable. If the values are proportions or percentages, 
i.e. strictly within 0 and 1 they are mapped into R using log( target/(1 - target) ).
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20ridge+2B20regression_+3A_dataset">dataset</code></td>
<td>

<p>A numeric matrix containing the variables. Rows are samples and columns are features.
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20ridge+2B20regression_+3A_k">K</code></td>
<td>

<p>The number of folds. Set to 10 by default. 
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20ridge+2B20regression_+3A_lambda">lambda</code></td>
<td>

<p>A vector with the a grid of values of <code class="reqn">\lambda</code> to be used. 
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20ridge+2B20regression_+3A_auto">auto</code></td>
<td>

<p>A boolean variable. If it is TRUE the GCV criterion will provide an automatic answer for the best $lambda$. Otherwise k-fold cross validation is performed.
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20ridge+2B20regression_+3A_seed">seed</code></td>
<td>

<p>A boolean variable. If it is TRUE the results will always be the same. 
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20ridge+2B20regression_+3A_ncores">ncores</code></td>
<td>

<p>The number of cores to use. If it is more than 1 parallel computing is performed.  
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20ridge+2B20regression_+3A_mat">mat</code></td>
<td>

<p>If the user has its own matrix with the folds, he can put it here. It must be a matrix with K columns, each column is a fold and it contains the positions of the data,
i.e. numbers, not the data. For example the first column is c(1,10,4,25,30), the second is c(21, 23,2, 19, 9) and so on.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The lm.ridge command in MASS library is a wrapper for this function. If you want a fast choice of <code class="reqn">\lambda</code>, then specify auto = TRUE and the <code class="reqn">\lambda</code> which minimizes the generalised cross-validation criterion will be returned. Otherise a k-fold cross validation is performed and the estimated performance is bias corrected as suggested by Tibshirani and Tibshirani (2009). 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>mspe</code></td>
<td>

<p>If auto is FALSE the values of the mean prediction error for each value of <code class="reqn">\lambda</code>. 
</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>

<p>If auto is FALSE the <code class="reqn">\lambda</code> which minimizes the MSPE. 
</p>
</td></tr>
<tr><td><code>performance</code></td>
<td>

<p>If auto is FALSE the minimum bias corrected MSPE along with the estimate of bias. 
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The run time of the algorithm. A numeric vector. The first element is the user time, the second element is the system time and the third element is the elapsed time.
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The values can be extracted with the $ symbol, i.e. this is an S3 class output. 
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Hoerl A.E. and R.W. Kennard (1970). Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12(1):55-67.
</p>
<p>Brown P. J. (1994). Measurement, Regression and Calibration. Oxford Science Publications.
</p>
<p>Tibshirani R.J., and Tibshirani R. (2009). A bias correction for the minimum error rate in cross-validation. The Annals of Applied Statistics 3(2): 822-829.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+ridge.reg">ridge.reg</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#simulate a dataset with continuous data
dataset &lt;- matrix(runif(200 * 40, 1, 100), nrow = 200 ) 
#the target feature is the last column of the dataset as a vector
target &lt;- dataset[, 40]
a1 &lt;- ridgereg.cv(target, dataset, auto = TRUE)
a2 &lt;- ridgereg.cv( target, dataset, K = 10, lambda = seq(0, 1, by = 0.1) )
</code></pre>

<hr>
<h2 id='Cross-Validation+20for+20SES+20and+20MMPC'>
Cross-Validation for SES and MMPC
</h2><span id='topic+cv.ses'></span><span id='topic+cv.mmpc'></span><span id='topic+cv.waldses'></span><span id='topic+cv.waldmmpc'></span><span id='topic+cv.permses'></span><span id='topic+cv.permmmpc'></span><span id='topic+auc.mxm'></span><span id='topic+acc.mxm'></span><span id='topic+ord_mae.mxm'></span><span id='topic+mae.mxm'></span><span id='topic+mse.mxm'></span><span id='topic+pve.mxm'></span><span id='topic+ci.mxm'></span><span id='topic+ciwr.mxm'></span><span id='topic+fscore.mxm'></span><span id='topic+prec.mxm'></span><span id='topic+euclid_sens.spec.mxm'></span><span id='topic+spec.mxm'></span><span id='topic+sens.mxm'></span><span id='topic+glm.mxm'></span><span id='topic+lm.mxm'></span><span id='topic+rq.mxm'></span><span id='topic+acc_multinom.mxm'></span><span id='topic+lmrob.mxm'></span><span id='topic+weibreg.mxm'></span><span id='topic+coxph.mxm'></span><span id='topic+poisdev.mxm'></span><span id='topic+nbdev.mxm'></span><span id='topic+pois.mxm'></span><span id='topic+nb.mxm'></span><span id='topic+multinom.mxm'></span><span id='topic+ordinal.mxm'></span><span id='topic+beta.mxm'></span><span id='topic+clogit.mxm'></span><span id='topic+exporeg.mxm'></span><span id='topic+mci.mxm'></span><span id='topic+llrreg.mxm'></span>

<h3>Description</h3>

<p>The function performs a k-fold cross-validation for identifying the best values for the SES and MMPC 'max_k' and 'threshold' hyper-parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.ses(target, dataset, wei = NULL, kfolds = 10, folds = NULL, 
alphas = c(0.1, 0.05, 0.01), max_ks = c(3, 2), task = NULL, 
metric = NULL, metricbbc = NULL, modeler = NULL, ses_test = NULL, 
ncores = 1, B = 1)

cv.mmpc(target, dataset, wei = NULL, kfolds = 10, folds = NULL, 
alphas = c(0.1, 0.05, 0.01), max_ks = c(3, 2), task = NULL, 
metric = NULL, metricbbc = NULL, modeler = NULL, mmpc_test = NULL, 
ncores = 1, B = 1)

cv.waldses(target, dataset, wei = NULL, kfolds = 10, folds = NULL, 
alphas = c(0.1, 0.05, 0.01), max_ks = c(3, 2), task = NULL, 
metric = NULL, metricbbc = NULL, modeler = NULL, ses_test = NULL,
ncores = 1, B = 1)

cv.waldmmpc(target, dataset, wei = NULL, kfolds = 10, folds = NULL, 
alphas = c(0.1, 0.05, 0.01), max_ks = c(3, 2), task = NULL, 
metric = NULL, metricbbc = NULL, modeler = NULL, mmpc_test = NULL, 
ncores = 1, B = 1)

cv.permses(target, dataset, wei = NULL, kfolds = 10, folds = NULL, 
alphas = c(0.1, 0.05, 0.01), max_ks = c(3, 2), task = NULL, 
metric = NULL, metricbbc = NULL, modeler = NULL, ses_test = NULL, R = 999, 
ncores = 1, B = 1)

cv.permmmpc(target, dataset, wei = NULL, kfolds = 10, folds = NULL, 
alphas = c(0.1, 0.05, 0.01), max_ks = c(3, 2), task = NULL, 
metric = NULL, metricbbc = NULL, modeler = NULL, mmpc_test = NULL, R = 999, 
ncores = 1, B = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Cross-Validation+2B20for+2B20SES+2B20and+2B20MMPC_+3A_target">target</code></td>
<td>

<p>The target or class variable as in SES and MMPC. The difference is that it cannot accept a single numeric value, an integer indicating the column in the dataset. 
</p>
</td></tr>
<tr><td><code id="Cross-Validation+2B20for+2B20SES+2B20and+2B20MMPC_+3A_dataset">dataset</code></td>
<td>

<p>The dataset object as in SES and MMPC.
</p>
</td></tr>
<tr><td><code id="Cross-Validation+2B20for+2B20SES+2B20and+2B20MMPC_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. 
</p>
</td></tr>
<tr><td><code id="Cross-Validation+2B20for+2B20SES+2B20and+2B20MMPC_+3A_kfolds">kfolds</code></td>
<td>

<p>The number of the folds in the k-fold Cross Validation (integer).
</p>
</td></tr>
<tr><td><code id="Cross-Validation+2B20for+2B20SES+2B20and+2B20MMPC_+3A_folds">folds</code></td>
<td>

<p>The folds of the data to use (a list generated by the function generateCVRuns TunePareto). If NULL the folds are created internally with the same function.
</p>
</td></tr>
<tr><td><code id="Cross-Validation+2B20for+2B20SES+2B20and+2B20MMPC_+3A_alphas">alphas</code></td>
<td>

<p>A vector of SES or MMPC thresholds hyper parameters used in CV. 
</p>
</td></tr>
<tr><td><code id="Cross-Validation+2B20for+2B20SES+2B20and+2B20MMPC_+3A_max_ks">max_ks</code></td>
<td>

<p>A vector of SES or MMPC max_ks parameters used in CV. 
</p>
</td></tr>
<tr><td><code id="Cross-Validation+2B20for+2B20SES+2B20and+2B20MMPC_+3A_task">task</code></td>
<td>

<p>A character (&quot;C&quot;, &quot;R&quot; or &quot;S&quot;). It can be &quot;C&quot; for classification (logistic, multinomial or ordinal regression), &quot;R&quot; for regression (robust and non robust linear regression, median regression, 
(zero inflated) poisson and negative binomial regression, beta regression), &quot;S&quot; for survival regresion (Cox, Weibull or exponential regression).
</p>
</td></tr>
<tr><td><code id="Cross-Validation+2B20for+2B20SES+2B20and+2B20MMPC_+3A_metric">metric</code></td>
<td>

<p>A metric function provided by the user. If NULL the following functions will be used: auc.mxm, mse.mxm, ci.mxm for classification, regression and survival analysis tasks, respectively. See details for more.
If you know what you have put it here to avoid the function choosing somehting else. <b>Note</b> that you put these words as they are, without &quot;&quot;.  
</p>
</td></tr>
<tr><td><code id="Cross-Validation+2B20for+2B20SES+2B20and+2B20MMPC_+3A_metricbbc">metricbbc</code></td>
<td>

<p>This is the same argument as &quot;metric&quot; with the difference that &quot; &quot; must be placed. If for example, metric = auc.mxm, here metricbbc = &quot;auc.mxm&quot;. The same value must be given here. This argument is to be used with the function <code><a href="#topic+bbc">bbc</a></code> which does bootstrap bias correction of the estimated performance (Tsamardinos, Greasidou and Borboudakis, 2018). This argument is valid if the last argument (B) is more than 1. 
</p>
</td></tr>
<tr><td><code id="Cross-Validation+2B20for+2B20SES+2B20and+2B20MMPC_+3A_modeler">modeler</code></td>
<td>

<p>A modeling function provided by the user. If NULL the following functions will be used: glm.mxm, lm.mxm, coxph.mxm for classification, regression and survival analysis tasks, respectively. See details for more.
If you know what you have put it here to avoid the function choosing somehting else. <b>Note</b> that you put these words as they are, without &quot;&quot;. 
</p>
</td></tr>
<tr><td><code id="Cross-Validation+2B20for+2B20SES+2B20and+2B20MMPC_+3A_ses_test">ses_test</code></td>
<td>

<p>A function object that defines the conditional independence test used in the SES function (see also SES help page). If NULL, &quot;testIndFisher&quot;, &quot;testIndLogistic&quot; and &quot;censIndCR&quot; 
are used for classification, regression and survival analysis tasks, respectively. If you know what you have put it here to avoid the function choosing somehting else. 
Not all tests can be included here. &quot;testIndClogit&quot;, &quot;testIndMVreg&quot;, &quot;testIndIG&quot;, &quot;testIndGamma&quot;, &quot;testIndZIP&quot; and &quot;testIndTobit&quot; are anot available at the moment.  
</p>
</td></tr>
<tr><td><code id="Cross-Validation+2B20for+2B20SES+2B20and+2B20MMPC_+3A_mmpc_test">mmpc_test</code></td>
<td>

<p>A function object that defines the conditional independence test used in the MMPC function (see also SES help page). If NULL, &quot;testIndFisher&quot;, &quot;testIndLogistic&quot; and &quot;censIndCR&quot;  are used for classification, 
regression and survival analysis tasks, respectively.
</p>
</td></tr>
<tr><td><code id="Cross-Validation+2B20for+2B20SES+2B20and+2B20MMPC_+3A_r">R</code></td>
<td>

<p>The number of permutations, set to 999 by default. There is a trick to avoind doing all permutations. As soon as the number of times the permuted test statistic is more than the observed test statistic is more 
than 50 (if threshold = 0.05 and R = 999), the p-value has exceeded the signifiance level (threshold value) and hence the predictor variable is not significant. There is no need to continue do the extra permutations, 
as a decision has already been made. 
</p>
</td></tr>
<tr><td><code id="Cross-Validation+2B20for+2B20SES+2B20and+2B20MMPC_+3A_ncores">ncores</code></td>
<td>

<p>This argument is valid only if you have a multi-threaded machine. 
</p>
</td></tr>
<tr><td><code id="Cross-Validation+2B20for+2B20SES+2B20and+2B20MMPC_+3A_b">B</code></td>
<td>

<p>How many bootstrap re-samples to draw. This argument is to be used with the function <code><a href="#topic+bbc">bbc</a></code> which does bootstrap bias correction of the estimated performance (Tsamardinos, Greasidou and Borboudakis, 2018). If you have thousands of samples (observations) then this might not be necessary, as there is no optimistic bias to be corrected. What is the lower limit cannot be told beforehand however. SES and MMPC however were designed for the low sample cases, hence, bootstrap bias correction is perhaps a must thing to do.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Input for metric functions:
predictions: A vector of predictions to be tested.
test_target: target variable actual values to be compared with the predictions.
</p>
<p>The output of a metric function is a single numeric value. <b>Higher values indicate better performance</b>. Metric based on error measures should be modified accordingly (e.g., multiplying the error for -1)
</p>
<p>The metric functions that are currently supported are:
</p>

<ul>
<li><p> auc.mxm: &quot;area under the receiver operator characteristic curve&quot; metric for binary logistic regression.
</p>
</li>
<li><p> acc.mxm: accuracy for binary logistic regression.
</p>
</li>
<li><p> fscore.mxm: F score for binary logistic regression.
</p>
</li>
<li><p> prec.mxm: precision for binary logistic regression.
</p>
</li>
<li><p> euclid_sens.spec.mxm: Euclidean norm of 1 - sensititivy and 1 - specificity for binary logistic regression.
</p>
</li>
<li><p> spec.mxm: specificity for logistic regression.
</p>
</li>
<li><p> sens.mxm: sensitivity for logistic regression.
</p>
</li>
<li><p> acc_multinom.mxm: accuracy for multinomial logistic regression.
</p>
</li>
<li><p> mse.mxm: mean squared error, for robust and non robust linear regression and median (quantile) regression (multiplied by -1). 
</p>
</li>
<li><p> pve.mxm:  1 - (mean squared error)/( (n - 1) * var(y_out) ), for non robust linear regression. It is basically the proportion of variance explained in the test set. 
</p>
</li>
<li><p> ci.mxm: 1 - concordance index as provided in the rcorr.cens function from the suvriva package. This is to be used with the Cox proportional hazards model only.
</p>
</li>
<li><p> ciwr.mxm concordance index as provided in the rcorr.cens function from the survival package. This is to be used with the Weibull regression model only.
</p>
</li>
<li><p> poisdev.mxm: Poisson regression deviance (multiplied by -1).
</p>
</li>
<li><p> nbdev.mxm: Negative binomial regression deviance (multiplied by -1). 
</p>
</li>
<li><p> binomdev.mxm: Negative binomial regression deviance (multiplied by -1). 
</p>
</li>
<li><p> ord_mae.mxm: Ordinal regression mean absolute error (multiplied by -1). 
</p>
</li>
<li><p> mae.mxm: Mean absolute error (multiplied by -1). 
</p>
</li>
<li><p> mci.mxm: Matched concordance index (for conditonal logistic regression). 
</p>
</li></ul>

<p>Usage: metric(predictions, test_target)
</p>
<p>Input of modelling functions:
train_target: target variable used in the training procedure.
sign_data: training set.
sign_test: test set.
</p>
<p>Modelling functions provide a single vector of predictions obtained by applying the model fit on sign_data and train_target on the sign_test
</p>
<p>The modelling functions that are currently supported are:
</p>

<ul>
<li><p> glm.mxm: fits a glm for a binomial family (classification task).
</p>
</li>
<li><p> multinom.mxm: fits a multinomial regression model (classification task).
</p>
</li>
<li><p> lm.mxm: fits a linear model (regression task).
</p>
</li>
<li><p> coxph.mxm: fits a cox proportional hazards regression model (survival task).
</p>
</li>
<li><p> weibreg.mxm: fits a Weibull regression model (survival task).
</p>
</li>
<li><p> rq.mxm: fits a quantile (median) regression model (regression task).
</p>
</li>
<li><p> lmrob.mxm: fits a robust linear model (regression task).
</p>
</li>
<li><p> pois.mxm: fits a poisson regression model (regression task).  
</p>
</li>
<li><p> nb.mxm: fits a negative binomial regression model (regression task).  
</p>
</li>
<li><p> ordinal.mxm: fits an ordinal regression model (regression task).
</p>
</li>
<li><p> beta.mxm: fits a beta regression model (regression task). The predicted values are transformed into <code class="reqn">R</code> using
the logit transformation. This is so that the &quot;mse.mxm&quot; metric function can be used. In addition, this way the performance can be
compared with the regression scenario, where the logit is applied and then a regression model is employed. 
</p>
</li>
<li><p> clogit: fits a conditonalo logistic regression model.
</p>
</li></ul>

<p>Usage: modeler(train_target, sign_data, sign_test)
</p>
<p>The procedure will be more automated in the future and more functions will be added. 
The multithreaded functions have been tested and no error has been detected. However, if you spot any suspicious results please let us know. 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>cv_results_all</code></td>
<td>

<p>A list with predictions, performances and signatures for each fold and each SES or MMPC configuration (e.g cv_results_all[[3]]$performances[1] indicates the performance of the 1st fold with the 
3d configuration of SES or MMPC). In the case of the multi-threaded functions (cvses.par and cvmmpc.par) this is a list with a matrix. The rows correspond to the folds and the columns to the 
configurations (pairs of threshold and max_k).
</p>
</td></tr>
<tr><td><code>best_performance</code></td>
<td>

<p>A numeric value that represents the best average performance.
</p>
</td></tr>
<tr><td><code>best_configuration</code></td>
<td>

<p>A list that corresponds to the best configuration of SES or MMPC including id, threshold (named 'a') and max_k.
</p>
</td></tr>
<tr><td><code>bbc_best_performance</code></td>
<td>

<p>The bootstrap bias corrected best performance if B was more than 1, othwerwise this is NULL.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The runtime of the cross-validation procedure.
</p>
</td></tr>
</table>
<p>Bear in mind that the values can be extracted with the $ symbol, i.e. this is an S3 class output. 
</p>


<h3>Author(s)</h3>

<p>R implementation and documentation: Giorgos Athineou &lt;athineou@csd.uoc.gr&gt; and Vincenzo Lagani &lt;vlagani@csd.uoc.gr&gt;
</p>


<h3>References</h3>

<p>Ioannis Tsamardinos, Elissavet Greasidou and Giorgos Borboudakis (2018). Bootstrapping the out-of-sample predictions for efficient and accurate cross-validation. Machine Learning (To appear).  
https://link.springer.com/article/10.1007/s10994-018-5714-4
</p>
<p>Harrell F. E., Lee K. L. and Mark D. B. (1996). Multivariable prognostic models: issues in developing models, evaluating assumptions and adequacy, and measuring and reducing errors. Statistics in medicine, 15(4), 361-387.
</p>
<p>Hanley J. A. and McNeil B. J. (1982). The meaning and use of the area under a receiver operating characteristic (ROC) curve. Radiology, 143(1), 29-36.
</p>
<p>Brentnall A. R., Cuzick J., Field J. and Duffy S. W. (2015). A concordance index for matched case-control studies with applications in cancer risk. Statistics in medicine, 34(3), 396-405.
</p>
<p>Pedregosa F., Bach F. &amp; Gramfort A. (2017). On the consistency of ordinal regression methods. The Journal of Machine Learning Research, 18(1), 1769-1803.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SES">SES</a>, <a href="#topic+CondIndTests">CondIndTests</a>, <a href="#topic+cv.gomp">cv.gomp</a>, <a href="#topic+bbc">bbc</a>, <a href="#topic+testIndFisher">testIndFisher</a>, <a href="#topic+testIndLogistic">testIndLogistic</a>, <a href="#topic+gSquare">gSquare</a>, <a href="#topic+censIndCR">censIndCR</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1234)

# simulate a dataset with continuous data
dataset &lt;- matrix( rnorm(100 * 50), ncol = 50 )
# the target feature is the last column of the dataset as a vector
target &lt;- dataset[, 50]
dataset &lt;- dataset[, -50]

# get 50 percent of the dataset as a train set
train_set &lt;- dataset[1:100, ]
train_target &lt;- target[1:100]

# run a 10 fold CV for the regression task
best_model &lt;- cv.ses(target = train_target, dataset = train_set, kfolds = 5, task = "R")

# get the results
best_model$best_configuration
best_model$best_performance

# summary elements of the process. Press tab after each $ to view all the elements and
# choose the one you are intresting in.
# best_model$cv_results_all[[...]]$...
#i.e.
# mse value for the 1st configuration of SES of the 5 fold
abs( best_model$cv_results_all[[ 1 ]]$performances[5] )

best_a &lt;- best_model$best_configuration$a
best_max_k &lt;- best_model$best_configuration$max_k
</code></pre>

<hr>
<h2 id='Cross-validation+20of+20the+20FBED+20with+20LMM'>
Cross-validation of the FBED with LMM
</h2><span id='topic+cv.fbed.lmm.reg'></span>

<h3>Description</h3>

<p>Cross-validation of the FBED with LMM.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.fbed.lmm.reg(target, dataset, id, prior = NULL, kfolds = 10, 
                folds = NULL, alphas = c(0.01, 0.05), ks = 0:2) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Cross-validation+2B20of+2B20the+2B20FBED+2B20with+2B20LMM_+3A_target">target</code></td>
<td>

<p>The class variable. This must be a numerical vector with continuous data.
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20of+2B20the+2B20FBED+2B20with+2B20LMM_+3A_dataset">dataset</code></td>
<td>

<p>The dataset; provide a numerical a matrix (columns = variables, rows = observations).
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20of+2B20the+2B20FBED+2B20with+2B20LMM_+3A_id">id</code></td>
<td>

<p>This is a numerical vector of the same size as target denoting the groups or the subjects. 
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20of+2B20the+2B20FBED+2B20with+2B20LMM_+3A_prior">prior</code></td>
<td>

<p>If you have prior knowledge of some variables that must be in the variable selection phase add them here. This an be a vector (if you have one variable) or a matrix (if you more variables). This does not work during the backward phase at the moment.
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20of+2B20the+2B20FBED+2B20with+2B20LMM_+3A_kfolds">kfolds</code></td>
<td>

<p>The number of the folds in the k-fold Cross Validation (integer).
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20of+2B20the+2B20FBED+2B20with+2B20LMM_+3A_folds">folds</code></td>
<td>

<p>The folds of the data to use (a list generated by the function generateCVRuns TunePareto). If NULL the folds are created internally with the same function.
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20of+2B20the+2B20FBED+2B20with+2B20LMM_+3A_alphas">alphas</code></td>
<td>

<p>A vector of significance levels to be tested.
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20of+2B20the+2B20FBED+2B20with+2B20LMM_+3A_ks">ks</code></td>
<td>

<p>A vector of K values to be tested.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function performs cross-validation for the FBED agortihm with clustered data using the linear mixed model. The k-folds cross-validation is on clusters. Instead of leaving observations, clusters are left aside each time. 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<p>list(vars = vars, cv = cv, perf = perf, best = best, runtime = runtime)
</p>
<table>
<tr><td><code>vars</code></td>
<td>

<p>An array with the number of selected variables for each combination of significance level and value of K.
</p>
</td></tr>
<tr><td><code>cv</code></td>
<td>

<p>An array with the number of selected variables for each combination of significance level and value of K.
</p>
</td></tr>
<tr><td><code>perf</code></td>
<td>

<p>A matrix with the average performance each combination of significance level and value of K.
</p>
</td></tr>
<tr><td><code>best</code></td>
<td>

<p>The best significance level and value of K.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The runtime required.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Fang Y. (2011). Asymptotic equivalence between cross-validations and Akaike information criteria in mixed-effects models. Journal of data science, 9(1), 15-21.
</p>
<p>Eugene Demidenko (2013). Mixed Models: Theory and Applications with R, 2nd Edition. New Jersey: Wiley &amp; Sons.
</p>
<p>Borboudakis G. and Tsamardinos I. (2019). Forward-backward selection with early dropping. Journal of Machine Learning Research, 20(8): 1-39.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+fbed.glmm.reg">fbed.glmm.reg</a>, <a href="#topic+fbed.gee.reg">fbed.gee.reg</a>, <a href="#topic+MMPC.glmm">MMPC.glmm</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
require(lme4)
data(sleepstudy)
reaction &lt;- sleepstudy$Reaction
subject &lt;- sleepstudy$Subject
x1 &lt;- sleepstudy$Days
x &lt;- matrix(rnorm(180 * 200),ncol = 200) ## unrelated predictor variables
x &lt;- cbind(x1, x)
m &lt;- cv.fbed.lmm.reg(reaction, x, subject) 

## End(Not run)
</code></pre>

<hr>
<h2 id='Data+20simulation+20from+20a+20DAG'>
Data simulation from a DAG.
</h2><span id='topic+rdag'></span><span id='topic+rdag2'></span><span id='topic+rmdag'></span>

<h3>Description</h3>

<p>Data simulation from a DAG.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rdag(n, p, s, a = 0, m, A = NULL, seed = FALSE) 
rdag2(n, A = NULL, p, nei, low = 0.1, up = 1) 
rmdag(n, A = NULL, p, nei, low = 0.1, up = 1) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Data+2B20simulation+2B20from+2B20a+2B20DAG_+3A_n">n</code></td>
<td>

<p>A number indicating the sample size. 
</p>
</td></tr>
<tr><td><code id="Data+2B20simulation+2B20from+2B20a+2B20DAG_+3A_p">p</code></td>
<td>

<p>A number indicating the number of nodes (or vectices, or variables).
</p>
</td></tr>
<tr><td><code id="Data+2B20simulation+2B20from+2B20a+2B20DAG_+3A_nei">nei</code></td>
<td>

<p>The average number of neighbours.
</p>
</td></tr>
<tr><td><code id="Data+2B20simulation+2B20from+2B20a+2B20DAG_+3A_s">s</code></td>
<td>

<p>A number in <code class="reqn">(0, 1)</code>. This defines somehow the sparseness of the model. It is the probability that a node has an edge. 
</p>
</td></tr>
<tr><td><code id="Data+2B20simulation+2B20from+2B20a+2B20DAG_+3A_a">a</code></td>
<td>

<p>A number in <code class="reqn">(0, 1)</code>. The defines the percentage of outliers to be included in the simulated data. If <code class="reqn">a=0</code>, no outliers are generated. 
</p>
</td></tr>
<tr><td><code id="Data+2B20simulation+2B20from+2B20a+2B20DAG_+3A_m">m</code></td>
<td>

<p>A vector equal to the number of nodes. This is the mean vector of the normal distribution from which the data are to be generated. This is used only when <code class="reqn">a&gt;0</code> 
so as to define the mena vector of the multivariate normal from which the outliers will be generated. 
</p>
</td></tr>
<tr><td><code id="Data+2B20simulation+2B20from+2B20a+2B20DAG_+3A_a">A</code></td>
<td>

<p>If you already have an an adjacency matrix in mind, plug it in here, otherwise, leave it NULL. 
</p>
</td></tr>
<tr><td><code id="Data+2B20simulation+2B20from+2B20a+2B20DAG_+3A_seed">seed</code></td>
<td>

<p>If seed is TRUE, the simulated data will always be the same. 
</p>
</td></tr>
<tr><td><code id="Data+2B20simulation+2B20from+2B20a+2B20DAG_+3A_low">low</code></td>
<td>

<p>Every child will be a function of some parents. The beta coefficients of the parents will be drawn uniformly from two numbers, low and up. See details for more information on this.
</p>
</td></tr>
<tr><td><code id="Data+2B20simulation+2B20from+2B20a+2B20DAG_+3A_up">up</code></td>
<td>

<p>Every child will be a function of some parents. The beta coefficients of the parents will be drawn uniformly from two numbers, low and up. See details for more information on this. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the case where no adjacency matrix is given, an <code class="reqn">p \times p</code> matrix with zeros everywhere is created. 
Every element below the diagonal is is replaced by random values from a Bernoulli distribution with probability of success equal to s. 
This is the matrix B. Every value of 1 is replaced by a uniform value in <code class="reqn">0.1, 1</code>. This final matrix is called A. 
The data are generated from a multivariate normal distribution with a zero mean vector and covariance matrix equal to 
<code class="reqn">\left({\bf I}_p- A\right)^{-1}\left({\bf I}_p- A\right)</code>, where <code class="reqn">{\bf I}_p</code> is the <code class="reqn">p \times p</code> identiy matrix. 
If a is greater than zero, the outliers are generated from a multivariate normal with the same covariance matrix and mean vector the one 
specified by the user, the argument &quot;m&quot;. The flexibility of the outliers is that you cna specifiy outliers in some variables only or in all of them. For example, m = c(0,0,5) introduces outliers in the third variable only, whereas m = c(5,5,5) introduces outliers in all variables. 
The user is free to decide on the type of outliers to include in the data.
</p>
<p>For the &quot;rdag2&quot;, this is a different way of simulating data from DAGs. The first variable is normally generated. Every other variable can be a function of some previous ones. Suppose now that the i-th variable is a child of 4 previous variables. We need for coefficients <code class="reqn">b_j</code> to multiply the 4 variables and then generate the i-th variable from a normal with mean <code class="reqn">\sum_{j=1}b_j X_j</code> and variance 1. The <code class="reqn">b_j</code> will be either positive or negative values with equal probability. Their absolute values ranges between &quot;low&quot; and &quot;up&quot;. The code is accessible and you can see in detail what is going on. In addition, every generated data, are standardised to avoid numerical overflow.
</p>
<p>The &quot;rmdag&quot; generates data from a BN with continous, ordinal and binary data in proportions 50%, 25% and 25% resepctively on average. This was used in the experiments run by Tsagris et al. (2017). If you want to generate data and then use them in the &quot;pcalg&quot; package with the function &quot;ci.fast2&quot; or &quot;ci.mm2&quot; you should transform the resulting data into a matrix. The factor variables must becomw numeric starting from 0. See the examples for more on this. 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>nout</code></td>
<td>

<p>The number of outliers.
</p>
</td></tr>
<tr><td><code>G</code></td>
<td>

<p>The adcacency matrix used. For the &quot;rdag&quot; if G[i, j] = 2, then G[j, i] = 3 and this means that there is an arrow from j to i. For the &quot;rdag2&quot; and &quot;rmdag&quot; the entries are either G[i, j] = G[j, i] = 0 (no edge) or G[i, j] = 1 and G[j, i] = 0 (indicating i -&gt; j). 
</p>
</td></tr>
<tr><td><code>A</code></td>
<td>

<p>The matrix with the with the uniform values in the interval <code class="reqn">0.1, 1</code>. This is returned only by &quot;rdag&quot;. 
</p>
</td></tr>
<tr><td><code>x</code></td>
<td>

<p>The simulated data. 
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Tsagris M. (2019). Bayesian network learning with the PC algorithm: an improved and correct variation. 
Applied Artificial Intelligence, 33(2): 101-123.
</p>
<p>Tsagris M., Borboudakis G., Lagani V. and Tsamardinos I. (2018). Constraint-based Causal Discovery with Mixed Data. 
International Journal of Data Science and Analytics. 
</p>
<p>Spirtes P.,  Glymour C. and Scheines R. (2001). Causation, Prediction, and Search. The MIT Press, Cambridge, MA, USA, 3nd edition.
</p>
<p>Colombo, Diego, and Marloes H. Maathuis (2014). Order-independent constraint-based causal structure learning. The Journal of Machine Learning Research 15(1): 3741&ndash;3782.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pc.skel">pc.skel</a>, <a href="#topic+pc.or">pc.or</a>, <a href="#topic+ci.mm">ci.mm</a>, <a href="#topic+mmhc.skel">mmhc.skel</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rdag(100, 20, 0.2)
x &lt;- y$x
tru &lt;- y$G 

mod &lt;- pc.con(x)
b &lt;- pc.or(mod)
plotnetwork(tru) 
dev.new()
plotnetwork(b$G)

</code></pre>

<hr>
<h2 id='Drop+20all+20possible+20single+20terms+20from+20a+20model+20using+20the+20partial+20correlation'>
Drop all possible single terms from a model using the partial correlation
</h2><span id='topic+cor.drop1'></span>

<h3>Description</h3>

<p>Drop all possible single terms from a model using the partial correlation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cor.drop1(y, x, logged = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Drop+2B20all+2B20possible+2B20single+2B20terms+2B20from+2B20a+2B20model+2B20using+2B20the+2B20partial+2B20correlation_+3A_y">y</code></td>
<td>

<p>A numerical vector with the response variable.
</p>
</td></tr>
<tr><td><code id="Drop+2B20all+2B20possible+2B20single+2B20terms+2B20from+2B20a+2B20model+2B20using+2B20the+2B20partial+2B20correlation_+3A_x">x</code></td>
<td>

<p>A numerical matrix or a data.frame with the predictor variables. If is is a matrix it is internally transformed into a data.frame form, hence the user is advised to supply a data.frame in order to save some time. If the number of columns (variables) is higher than the number of rows (observations) the function will simply not work.
</p>
</td></tr>
<tr><td><code id="Drop+2B20all+2B20possible+2B20single+2B20terms+2B20from+2B20a+2B20model+2B20using+2B20the+2B20partial+2B20correlation_+3A_logged">logged</code></td>
<td>

<p>If you want the p-values be returned leave this FALSE. If it is TRUE their logarithm is returned.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This uses R's command <code><a href="stats.html#topic+drop1">drop1</a></code> and modifies it so as to calculate the p-value using Fisher's conditional independence test.
</p>


<h3>Value</h3>

<p>A matrix with two columns, the test statistic values and its associated p-value.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+glm.bsreg">glm.bsreg</a>, <a href="#topic+fbed.reg">fbed.reg</a>, <a href="#topic+mmpcbackphase">mmpcbackphase</a> 
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rnorm(200)
x &lt;- matrix( rnorm(200 * 10), ncol = 10)
cor.drop1(y, x)
</code></pre>

<hr>
<h2 id='eBIC+20for+20many+20regression+20models'>
eBIC for many regression models
</h2><span id='topic+ebic.regs'></span>

<h3>Description</h3>

<p>eBIC for many regression models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ebic.regs(target, dataset, xIndex, csIndex,  gam = NULL, test = NULL, wei = NULL, 
ncores = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="eBIC+2B20for+2B20many+2B20regression+2B20models_+3A_target">target</code></td>
<td>

<p>The target (dependent) variable. It must be a numerical vector, a factor or a Surv object. 
</p>
</td></tr>
<tr><td><code id="eBIC+2B20for+2B20many+2B20regression+2B20models_+3A_dataset">dataset</code></td>
<td>

<p>The indendent variable(s). This can be a matrix or a dataframe with continuous only variables, a data frame with mixed or only categorical variables. 
</p>
</td></tr>
<tr><td><code id="eBIC+2B20for+2B20many+2B20regression+2B20models_+3A_xindex">xIndex</code></td>
<td>

<p>The indices of the variables whose association with the target you want to test.
</p>
</td></tr>
<tr><td><code id="eBIC+2B20for+2B20many+2B20regression+2B20models_+3A_csindex">csIndex</code></td>
<td>

<p>The index or indices of the variable(s) to condition on. If this is 0, the the function <code><a href="#topic+univregs">univregs</a></code> will be called.
</p>
</td></tr>
<tr><td><code id="eBIC+2B20for+2B20many+2B20regression+2B20models_+3A_gam">gam</code></td>
<td>

<p>In case the method is chosen to be &quot;eBIC&quot; one can also specify the <code class="reqn">gamma</code> parameter. The default value is &quot;NULL&quot;, so that the value is automatically calculated.
</p>
</td></tr>
<tr><td><code id="eBIC+2B20for+2B20many+2B20regression+2B20models_+3A_test">test</code></td>
<td>

<p>One of the following: <b>testIndBeta</b>, <b>testIndReg</b>, <b>testIndLogistic</b>, <b>testIndOrdinal</b>, 
<b>testIndPois</b>, <b>testIndZIP</b>, <b>testIndNB</b>, <b>testIndClogit</b>, <b>testIndBinom</b>, <b>testIndIGreg</b>, 
<b>censIndCR</b>, <b>censIndWR</b>, <b>censIndER</b>, <b>censIndLLR</b> <b>testIndMultinom</b>, <b>testIndTobit</b>, <b>testIndSPML</b>, <b>testIndGamma</b> or <b>testIndNormLog</b>. 
</p>
<p>Note that in all cases you must give the name of the test, without &quot; &quot;. 
</p>
</td></tr>
<tr><td><code id="eBIC+2B20for+2B20many+2B20regression+2B20models_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. An example where weights are used is surveys when stratified sampling has occured. 
</p>
</td></tr>
<tr><td><code id="eBIC+2B20for+2B20many+2B20regression+2B20models_+3A_ncores">ncores</code></td>
<td>

<p>How many cores to use. This plays an important role if you have tens of thousands of variables or really large sample sizes and tens of thousands of variables and a regression based test which requires numerical optimisation. In other cases it will not make a difference in the overall time (in fact it can be slower). The parallel computation is used in the first step of the algorithm, where univariate associations are examined, those take place in parallel. We have seen a reduction in time of 50% with 4 cores in comparison to 1 core. Note also, that the amount of reduction is not linear in the number of cores.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is more as a help function for MMPC, but it can also be called directly by the user. In some, one should specify the regression model to use and the function will perform all simple regressions, i.e. all regression models between the target and each of the variables in the dataset. The function does not check for zero variance columns, only the &quot;univregs&quot; and related functions do. 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>stat</code></td>
<td>

<p>The value of the test statistic.
</p>
</td></tr>
<tr><td><code>pvalue</code></td>
<td>

<p>The logarithm of the p-value of the test. 
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Chen J. and Chen Z. (2008). Extended Bayesian information criteria for model selection with large model spaces. 
Biometrika, 95(3): 759-771.
</p>
<p>Eugene Demidenko (2013). Mixed Models: Theory and Applications with R, 2nd Edition. New Jersey: Wiley &amp; Sons.
</p>
<p>McCullagh, Peter, and John A. Nelder. Generalized linear models.  CRC press, USA, 2nd edition, 1989.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+univregs">univregs</a>, <a href="#topic+SES">SES</a>, <a href="#topic+MMPC">MMPC</a>, <a href="#topic+CondIndTests">CondIndTests</a>, <a href="#topic+reg.fit">reg.fit</a>, <a href="#topic+ridge.reg">ridge.reg</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rpois(100, 15)
x &lt;- matrix( rnorm(100 * 10), ncol = 10)
a1 &lt;- univregs(y, x, test = testIndPois)
a2 &lt;- perm.univregs(y, x, test = permPois)
a3 &lt;- wald.univregs(y, x, test = waldPois)
a4 &lt;- cond.regs(y, as.data.frame(x), xIndex = 1:4, csIndex = 5, test = testIndPois)
</code></pre>

<hr>
<h2 id='Effective+20sample+20size+20for+20G+5E2+20test+20in+20BNs+20with+20case+20control+20data'>
Effective sample size for G^2 test in BNs with case control data
</h2><span id='topic+Ness'></span>

<h3>Description</h3>

<p>Effective sample size for G^2 test in BNs with case control data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Ness(propNt, N, K = 10000) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Effective+2B20sample+2B20size+2B20for+2B20G+2B5E2+2B20test+2B20in+2B20BNs+2B20with+2B20case+2B20control+2B20data_+3A_propnt">propNt</code></td>
<td>

<p>A numerical vector with the proportions (distribution) of the (single) selection variable.   
</p>
</td></tr>
<tr><td><code id="Effective+2B20sample+2B20size+2B20for+2B20G+2B5E2+2B20test+2B20in+2B20BNs+2B20with+2B20case+2B20control+2B20data_+3A_n">N</code></td>
<td>

<p>The sample size of the data. 
</p>
</td></tr>
<tr><td><code id="Effective+2B20sample+2B20size+2B20for+2B20G+2B5E2+2B20test+2B20in+2B20BNs+2B20with+2B20case+2B20control+2B20data_+3A_k">K</code></td>
<td>

<p>The number of repetitions to be used for estimating the effective sample size.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When dealing with case control data, spurious correlations or relationships arise. To deal with this one way is to adjust the sample size used in the G^2 test statistic. This function does exactly this, estimates the effective sample size as per the Borboudakis and Tsamardinos (2012) suggestion. The idea is that after learning the skeleton with the usual G^2 test, one should go to the edges and perform a conditional G^2
</p>


<h3>Value</h3>

<p>The estimated effective sample size.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Borboudakis G. and Tsamardinos I. (2015). Bayesian Network Learning with Discrete Case-Control Data. 31st Conference on Uncertainty in Artificial Intelligence (UAI), 151-160.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+SES">SES</a>, <a href="#topic+MMPC">MMPC</a>, <a href="#topic+testIndLogistic">testIndLogistic</a> 
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Ness(c(0.3, 0.7), N = 1000, K = 10000) 
</code></pre>

<hr>
<h2 id='Estimation+20of+20the+20percentage+20of+20Null+20p-values'>
Estimation of the percentage of Null p-values
</h2><span id='topic+pi0est'></span>

<h3>Description</h3>

<p>Estimation of the percentage of Null p-values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pi0est(p, lambda = seq(0.05, 0.95, by = 0.01), dof = 3) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Estimation+2B20of+2B20the+2B20percentage+2B20of+2B20Null+2B20p-values_+3A_p">p</code></td>
<td>

<p>A vector of p-values.
</p>
</td></tr>
<tr><td><code id="Estimation+2B20of+2B20the+2B20percentage+2B20of+2B20Null+2B20p-values_+3A_lambda">lambda</code></td>
<td>

<p>A vector of values of the tuning parameter lambda.
</p>
</td></tr>
<tr><td><code id="Estimation+2B20of+2B20the+2B20percentage+2B20of+2B20Null+2B20p-values_+3A_dof">dof</code></td>
<td>

<p>Number of degrees of freedom to use when estimating pi_0 with smoothing splines.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The estimated proporiton of null p-values is estimated the algorithm by Storey and Tibshirani (2003). 
</p>


<h3>Value</h3>

<p>The estimated proportion of non significant (null) p-values. In the paper Storey and Tibshirani mention that the estimate of pi0 is with lambda=1, but in their R code they use the highest value of lambda and thus we do the same here.  
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Storey J.D. and Tibshirani R. (2003). Statistical significance for genome-wide experiments. Proceedings of the National Academy of Sciences, 100: 9440-9445. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+conf.edge.lower">conf.edge.lower</a>, <a href="#topic+bn.skel.utils">bn.skel.utils</a>, <a href="#topic+mmhc.skel">mmhc.skel</a>, <a href="#topic+pc.skel">pc.skel</a>, <a href="#topic+corfs.network">corfs.network</a>, <a href="#topic+local.mmhc.skel">local.mmhc.skel</a>  </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## simulate a dataset with continuous data
y &lt;- rdag2(1000, p = 20, nei = 3)
ind &lt;- sample(1:20, 20)
x &lt;- y$x[, ind]
mod &lt;- pc.skel( x, method = "comb.fast", alpha = 0.01 ) 
pval &lt;- exp(mod$pvalue)
pval &lt;- lower.tri(pval)
pi0est(pval)
</code></pre>

<hr>
<h2 id='Fast+20MMPC'>
A fast version of MMPC
</h2><span id='topic+mmpc2'></span>

<h3>Description</h3>

<p>A fast version of MMPC
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mmpc2(target, dataset, prior = NULL, max_k = 3, threshold = 0.05, 
test = "testIndLogistic", ini = NULL, wei = NULL, ncores = 1, backward = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Fast+2B20MMPC_+3A_target">target</code></td>
<td>

<p>The class variable. Provide either a string, an integer, a numeric value, a vector, a factor, an ordered factor or a Surv object. See also Details.
</p>
</td></tr>
<tr><td><code id="Fast+2B20MMPC_+3A_dataset">dataset</code></td>
<td>

<p>The data-set; provide either a data frame or a matrix (columns = variables , rows = samples).
</p>
</td></tr>
<tr><td><code id="Fast+2B20MMPC_+3A_prior">prior</code></td>
<td>

<p>If you have prior knowledge of some variables that must be in the variable selection phase add them here. This an be a vector (if you have one variable) or a matrix (if you more variables). This does not work during the backward phase at the moment.
</p>
</td></tr>
<tr><td><code id="Fast+2B20MMPC_+3A_max_k">max_k</code></td>
<td>

<p>The maximum conditioning set to use in the conditional indepedence test (see Details). Integer, default value is 3.
</p>
</td></tr>
<tr><td><code id="Fast+2B20MMPC_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0, 1)) for assessing p-values significance. Default value is 0.05.
</p>
</td></tr>
<tr><td><code id="Fast+2B20MMPC_+3A_test">test</code></td>
<td>

<p>One of the following: &quot;testIndBeta&quot;, &quot;testIndReg&quot;, &quot;testIndRQ&quot;, &quot;testIndLogistic&quot;, &quot;testIndMultinom&quot;, 
&quot;testIndOrdinal&quot;, &quot;testIndPois&quot;, &quot;testIndQPois&quot;, &quot;testIndZIP&quot;, &quot;testIndNB&quot;, &quot;testIndClogit&quot;, 
&quot;testIndBinom&quot;, &quot;testIndQBinom&quot;, &quot;testIndIGreg&quot;, &quot;censIndCR&quot;, &quot;censIndWR&quot;, &quot;censIndER&quot;, &quot;testIndMMReg&quot;,
&quot;testIndMVreg&quot;, &quot;testIndMultinom&quot;, &quot;testIndOrdinal&quot;, &quot;testIndTobit&quot;, &quot;testIndGamma&quot;, &quot;testIndNormLog&quot; or &quot;testIndSPML&quot;. 
</p>
</td></tr>
<tr><td><code id="Fast+2B20MMPC_+3A_ini">ini</code></td>
<td>

<p>This is a supposed to be a list. To avoid calculating the univariate associations (first step of SES, MMPC and of FBED) again, you can extract them from the first run of omne of these algorithms and plug them here. 
This can speed up the second run (and subequent runs of course) by 50%. See the details and the argument &quot;univ&quot; in the output values. 
</p>
</td></tr>
<tr><td><code id="Fast+2B20MMPC_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. An example where weights are used is surveys when stratified sampling has occured. 
</p>
</td></tr>



<tr><td><code id="Fast+2B20MMPC_+3A_ncores">ncores</code></td>
<td>

<p>How many cores to use. This plays an important role if you have tens of thousands of variables or really large sample sizes and tens of thousands of 
variables and a regression based test which requires numerical optimisation. In other cases it will not make a difference in the overall time (in fact it can be slower). 
The parallel computation is used in the first step of the algorithm, where univariate associations are examined, those take place in parallel. We have seen a reduction in time of 50% with 4 cores in comparison to 1 core. Note also, that the amount of reduction is not linear in the number of cores.
</p>
</td></tr>
<tr><td><code id="Fast+2B20MMPC_+3A_backward">backward</code></td>
<td>

<p>If TRUE, the backward (or symmetry correction) phase will be implemented. This removes any falsely included variables in the parents and children set of the target variable. It calls the <code>link{mmpcbackphase}</code> for this purpose.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>MMPC tests each feature for inclusion (selection). For each featurer it performsa conditional independence tets. Each test requires fitting two regression models, one without the feature and one with the feature included. In this version, we have changed the order of the tests. We find all possible subsets of the already selected features and for each of them we test each feature. This way, only half of the regression models the usual MMPC fits, are fitted. Also, less tests will be performed. It is the same algorithm, with a change in the sequence. 
</p>
<p>We have seen a 50% in the computational time, but the drawback is that if you want to run MMPC with different value of &quot;max$_$k&quot; and &quot;alpha&quot;, this is not possible from here. This function is for oa signle pair of &quot;max$_$k&quot; and &quot;alpha&quot; values. It saves no test statistics, only p-values, no hashing and hence is memory efficient, but contains less information than <code><a href="#topic+MMPC">MMPC</a></code>.
</p>


<h3>Value</h3>

<p>The output of the algorithm is an S3 object including:
</p>
<table>
<tr><td><code>selectedVars</code></td>
<td>

<p>The selected variables, i.e., the signature of the target variable.
</p>
</td></tr>
<tr><td><code>pvalues</code></td>
<td>

<p>For each feature included in the dataset, this vector reports the strength of its association with the target in the context of all other variable. Particularly, this vector reports the max p-values found when the association of each variable with the target is tested against different conditional sets. Lower values indicate higher association. <b>Note that these are the logarithm of the p-values</b>
</p>
</td></tr>
<tr><td><code>univ</code></td>
<td>

<p>A vector with the <b>logged p-values</b> of the univariate associations. This vector is very important for subsequent runs of MMPC with different hyper-parameters. After running SES with some hyper-parameters you might want to run MMPCagain with different hyper-parameters. To avoid calculating the univariate associations (first step) again, you can take this list from the first run of SES and plug it in the argument &quot;ini&quot; in the next run(s) of MMPC. This can speed up the second run (and subequent runs of course) by 50%. See the argument &quot;univ&quot; in the output values. 
</p>
</td></tr>
<tr><td><code>kapa_pval</code></td>
<td>

<p>A list with the same number of elements as the max$_k$. Every element in the list is a matrix. The first row is the <b>logged p-values</b>, the second row is the variable whose conditional association with the target variable was tests and the other rows are the conditioning variables. 
</p>
</td></tr>
<tr><td><code>max_k</code></td>
<td>

<p>The max_k option used in the current run.
</p>
</td></tr>
<tr><td><code>threshold</code></td>
<td>

<p>The threshold option used in the current run.
</p>
</td></tr>
<tr><td><code>n.tests</code></td>
<td>

<p>The number of tests performed by MMPC will be returned. 
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The run time of the algorithm. A numeric vector. The first element is the user time, the second element is the system time and the third element is the elapsed time.
</p>
</td></tr>
<tr><td><code>test</code></td>
<td>

<p>The character name of the statistic test used.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ioannis Tsamardinos, Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>. 
</p>


<h3>References</h3>

<p>Feature Selection with the R Package MXM: Discovering Statistically Equivalent Feature Subsets, Lagani, V. and Athineou, 
G. and Farcomeni, A. and Tsagris, M. and Tsamardinos, I. (2017). Journal of Statistical Software, 80(7).
</p>
<p>Tsamardinos, I., Aliferis, C. F., &amp; Statnikov, A. (2003). Time and sample efficient discovery of Markov blankets and direct causal 
relations. In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 673-678). ACM.
</p>
<p>Brown, L. E., Tsamardinos, I., &amp; Aliferis, C. F. (2004). A novel algorithm for scalable and accurate Bayesian network learning. 
Medinfo, 711-715.
</p>
<p>Tsamardinos, Brown and Aliferis (2006). The max-min hill-climbing Bayesian network structure learning algorithm. Machine learning, 
65(1), 31-78.
</p>




<h3>See Also</h3>

<p><code><a href="#topic+MMPC">MMPC</a>, <a href="#topic+certificate.of.exclusion2">certificate.of.exclusion2</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)

#simulate a dataset with continuous data
dataset &lt;- matrix(runif(100 * 40, 1, 100), ncol = 40)

#define a simulated class variable 
target &lt;- 3 * dataset[, 10] + 2 * dataset[, 15] + 3 * dataset[, 20] + rnorm(100, 0, 5)
m &lt;- median(target)
target[target &lt; m] &lt;- 0
target[abs(target) &gt; 0 ] &lt;- 1

m1 &lt;- mmpc2(target, dataset, max_k = 3, threshold = 0.05, test="testIndLogistic")
m1$selectedVars  ## S3 class, $, not @
m1$runtime

m2 &lt;- MMPC(target, dataset, max_k = 3, threshold = 0.05, test="testIndLogistic")
m2@selectedVars  ## S3 class, @, not $
m2@runtime
</code></pre>

<hr>
<h2 id='Fast+20MMPC+20for+20longitudinal+20and+20clustered+20data'>
mmpc.glmm2/mmpc.gee2: Fast Feature selection algorithm for identifying minimal feature subsets with correlated data
</h2><span id='topic+mmpc.glmm2'></span><span id='topic+mmpc.gee2'></span>

<h3>Description</h3>

<p>SES.glmm algorithm follows a forward-backward filter approach for feature selection in order to provide minimal, 
highly-predictive, statistically-equivalent, multiple feature subsets of a high dimensional dataset. 
See also Details. MMPC.glmm algorithm follows the same approach without generating multiple feature subsets. 
They are both adapted to longitudinal target variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mmpc.glmm2(target, reps = NULL, group, dataset, prior = NULL, max_k = 3, 
threshold = 0.05, test = NULL, ini = NULL, wei = NULL, slopes = FALSE, ncores = 1)

mmpc.gee2(target, reps = NULL, group, dataset, prior = NULL, max_k = 3, 
threshold = 0.05, test = NULL, ini = NULL, wei = NULL, correl = "exchangeable", 
se = "jack", ncores = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Fast+2B20MMPC+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data_+3A_target">target</code></td>
<td>

<p>The class variable. Provide a vector with continuous (normal), binary (binomial) or discrete (Poisson) data. For the GEE the data must be sorted so that observations on a cluster are contiguous rows for all entities in the formula.
</p>
</td></tr>
<tr><td><code id="Fast+2B20MMPC+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data_+3A_reps">reps</code></td>
<td>

<p>A numeric vector containing the time points of the subjects. It's length is equal to the length of the target variable. If you have clustered data, leave this NULL. For the GEE the data must be sorted so that observations on a cluster are contiguous rows for all entities in the formula.
</p>
</td></tr>
<tr><td><code id="Fast+2B20MMPC+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data_+3A_group">group</code></td>
<td>

<p>A numeric vector containing the subjects or groups. It must be of the same length as target. For the GEE the data must be sorted so that observations on a cluster are contiguous rows for all entities in the formula.
</p>
</td></tr>
<tr><td><code id="Fast+2B20MMPC+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data_+3A_dataset">dataset</code></td>
<td>

<p>The dataset; provide either a data frame or a matrix (columns = variables, rows = samples). Currently, only 
continuous datasets are supported. For the GEE the data must be sorted so that observations on a cluster are contiguous rows for all entities in the formula.
</p>
</td></tr>
<tr><td><code id="Fast+2B20MMPC+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data_+3A_prior">prior</code></td>
<td>

<p>If you have prior knowledge of some variables that must be in the variable selection phase add them here. This an be a vector (if you have one variable) or a matrix (if you more variables). 
</p>
</td></tr>
<tr><td><code id="Fast+2B20MMPC+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data_+3A_max_k">max_k</code></td>
<td>

<p>The maximum conditioning set to use in the conditional indepedence test (see Details). Integer, default value is 3.
</p>
</td></tr>
<tr><td><code id="Fast+2B20MMPC+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0, 1)) for assessing p-values significance. Default value is 0.05.
</p>
</td></tr>
<tr><td><code id="Fast+2B20MMPC+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data_+3A_test">test</code></td>
<td>

<p>The conditional independence test to use. Default value is NULL. Currently, the only available conditional independence test are the <code><a href="#topic+testIndGLMMLogistic">testIndGLMMLogistic</a></code>, <code><a href="#topic+testIndGLMMPois">testIndGLMMPois</a></code>, <code><a href="#topic+testIndGLMMGamma">testIndGLMMGamma</a></code>, 
<code><a href="#topic+testIndGLMMNormLog">testIndGLMMNormLog</a></code> and <code><a href="#topic+testIndGLMMReg">testIndGLMMReg</a></code> which fit linear mixed models. For the GEE the options are  <code><a href="#topic+testIndGEELogistic">testIndGEELogistic</a></code>, <code><a href="#topic+testIndGEEPois">testIndGEEPois</a></code>, <code><a href="#topic+testIndGEEGamma">testIndGEEGamma</a></code>, 
<code><a href="#topic+testIndGEENormLog">testIndGEENormLog</a></code> and <code><a href="#topic+testIndGEENormLog">testIndGEENormLog</a></code>.
</p>
</td></tr>
<tr><td><code id="Fast+2B20MMPC+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data_+3A_ini">ini</code></td>
<td>

<p>This is a supposed to be a list. After running SES or MMPC with some hyper-parameters you might want to run SES again with different hyper-parameters. To avoid calculating the univariate associations (first step of SES and of MPPC) again, you can extract them from the first run of SES and plug them here. This can speed up the second run (and subequent runs of course) by 50%. See the details and the argument &quot;univ&quot; in the output values. 
</p>
</td></tr>
<tr><td><code id="Fast+2B20MMPC+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. 
</p>
</td></tr>
<tr><td><code id="Fast+2B20MMPC+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data_+3A_slopes">slopes</code></td>
<td>

<p>Should random slopes for the ime effect be fitted as well? Default value is FALSE. 
</p>
</td></tr>
<tr><td><code id="Fast+2B20MMPC+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data_+3A_correl">correl</code></td>
<td>

<p>The correlation structure of the GEE. For the Gaussian, Logistic, Poisson and Gamma regression this can be either 
&quot;exchangeable&quot; (compound symmetry, suitable for clustered data) or &quot;ar1&quot; (AR(1) model, 
suitable for longitudinal data). All observations must be ordered according to time within each subject. See the vignette of <b>geepack</b> to understand how.
</p>
</td></tr>
<tr><td><code id="Fast+2B20MMPC+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data_+3A_se">se</code></td>
<td>

<p>The method for estimating standard errors. This is very important and crucial. The available options for Gaussian, Logistic, Poisson and Gamma regression are: a) 'san.se': the usual robust estimate. b) 'jack': approximate jackknife variance estimate. c) 'j1s': if 1-step jackknife variance estimate and d) 'fij': fully iterated jackknife variance estimate. If you have many clusters (sets of repeated measurements) &quot;san.se&quot; is fine as it is asympotically correct, plus jacknife estimates will take longer. If you have a few clusters, then maybe it's better to use jacknife estimates. 
</p>
<p>The jackknife variance estimator was suggested by Paik (1988), which is quite suitable for cases when the number of subjects is small (K &lt; 30), as in many biological studies. The simulation studies conducted by Ziegler et al. (2000) and Yan and Fine (2004) showed that the approximate jackknife estimates are in many cases in good agreement with the fully iterated ones. 
</p>
</td></tr>
<tr><td><code id="Fast+2B20MMPC+2B20for+2B20longitudinal+2B20and+2B20clustered+2B20data_+3A_ncores">ncores</code></td>
<td>

<p>How many cores to use. This plays an important role if you have tens of thousands of variables or really large sample sizes and tens of thousands of variables and a regression based test which requires numerical optimisation. In other cases it will not make a difference in the overall time (in fact it can be slower). The parallel computation is used in the first step of the algorithm, where univariate associations are examined, those take place in parallel. We have seen a reduction in time of 50% with 4 cores in comparison to 1 core. Note also, that the amount of reduction is definetely not linear in the number of cores.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These are faster versions of MMPC using GLMM or GEE methodologies. See also <code><a href="#topic+mmpc2">mmpc2</a></code> for more details on the algorithm.
</p>
<p>If you want to use the GEE methodology, make sure you load the library geepack first. 
</p>


<h3>Value</h3>

<p>The output of the algorithm is an object of the class 'SES.glmm.output' for SES.glmm or 'MMPC.glmm.output' for MMPC.glmm including:
</p>
<table>
<tr><td><code>selectedVars</code></td>
<td>

<p>The selected variables, i.e., the signature of the target variable.
</p>
</td></tr>
<tr><td><code>pvalues</code></td>
<td>

<p>For each feature included in the dataset, this vector reports the strength of its association with the target in the context of 
all other variables. Particularly, this vector reports the max p-values foudn when the association of each variable with the 
target is tested against different conditional sets. Lower values indicate higher association. <b>Note that these are the logarithm of the p-values</b>.
</p>
</td></tr>
<tr><td><code>univ</code></td>
<td>

<p>A vector with the <b>logged p-values</b> of the univariate associations. This vector is very important for subsequent runs of MMPC with 
different hyper-parameters. After running SES with some hyper-parameters you might want to run MMPCagain with different hyper-parameters. 
To avoid calculating the univariate associations (first step) again, you can take this list from the first run of SES and plug it in 
the argument &quot;ini&quot; in the next run(s) of MMPC. This can speed up the second run (and subequent runs of course) by 50%. See the argument 
&quot;univ&quot; in the output values. 
</p>
</td></tr>
<tr><td><code>kapa_pval</code></td>
<td>

<p>A list with the same number of elements as the max$_k$. Every element in the list is a matrix. The first row is the logged p-values, 
the second row is the variable whose conditional association with the target variable was tests and the other rows are the 
conditioning variables. 
</p>
</td></tr>
<tr><td><code>max_k</code></td>
<td>

<p>The max_k option used in the current run.
</p>
</td></tr>
<tr><td><code>threshold</code></td>
<td>

<p>The threshold option used in the current run.
</p>
</td></tr>
<tr><td><code>n.tests</code></td>
<td>

<p>If you have set hash = TRUE, then the number of tests performed by SES or MMPC will be returned. If you have not set this to TRUE, 
the number of univariate associations will be returned. So be careful with this number.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The run time of the algorithm. A numeric vector. The first element is the user time, the second element is the system time and the 
third element is the elapsed time.
</p>
</td></tr>
<tr><td><code>test</code></td>
<td>

<p>The character name of the statistic test used.
</p>
</td></tr>
<tr><td><code>slope</code></td>
<td>

<p>Whether random slopes for the time effects were used or not, TRUE or FALSE.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ioannis Tsamardinos, Vincenzo Lagani
</p>
<p>R implementation and documentation: Giorgos Athineou &lt;athineou@csd.uoc.gr&gt; Vincenzo Lagani &lt;vlagani@csd.uoc.gr&gt;  
</p>


<h3>References</h3>

<p>Tsagris, M., Lagani, V., &amp; Tsamardinos, I. (2018). Feature selection for high-dimensional glmm data. BMC bioinformatics, 19(1), 17.
</p>
<p>I. Tsamardinos, M. Tsagris and V. Lagani (2015). Feature selection for longitudinal data. Proceedings of the 10th conference 
of the Hellenic Society for Computational Biology &amp; Bioinformatics (HSCBB15).
</p>
<p>I. Tsamardinos, V. Lagani and D. Pappas (2012). Discovering multiple, equivalent biomarker signatures. In proceedings of the 7th 
conference of the Hellenic Society for Computational Biology &amp; Bioinformatics - HSCBB12.
</p>
<p>Tsamardinos, Brown and Aliferis (2006). The max-min hill-climbing Bayesian network structure learning algorithm. 
Machine learning, 65(1), 31-78.
</p>
<p>Eugene Demidenko (2013). Mixed Models: Theory and Applications with R, 2nd Edition. New Jersey: Wiley &amp; Sons.
</p>
<p>J. Pinheiro and D. Bates. Mixed-effects models in S and S-PLUS. Springer Science &amp; Business Media, 2006.
</p>
<p>Liang K.Y. and Zeger S.L. (1986). Longitudinal data analysis using generalized linear models. Biometrika, 73(1): 13-22.
</p>
<p>Prentice R.L. and Zhao L.P. (1991). Estimating equations for parameters in means and covariances of multivariate discrete and 
continuous responses. Biometrics, 47(3): 825-839.
</p>
<p>Heagerty P.J. and Zeger S.L. (1996) Marginal regression models for clustered ordinal measurements. Journal of the American 
Statistical Association, 91(435): 1024-1036.
</p>
<p>Paik M.C. (1988). Repeated measurement analysis for nonnormal data in small samples. Communications in Statistics-Simulation and 
Computation, 17(4): 1155-1171.
</p>
<p>Ziegler A., Kastner C., Brunner D. and Blettner M. (2000). Familial associations of lipid profiles: A generalised estimating 
equations approach. Statistics in medicine, 19(24): 3345-3357
</p>
<p>Yan J. and Fine J. (2004). Estimating equations for association structures. Statistics in medicine, 23(6): 859-874.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CondIndTests">CondIndTests</a>, <a href="#topic+testIndGLMMReg">testIndGLMMReg</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
require(lme4)
data(sleepstudy)
reaction &lt;- sleepstudy$Reaction
days &lt;- sleepstudy$Days
subject &lt;- sleepstudy$Subject
x &lt;- matrix(rnorm(180 * 100), ncol = 100) ## unrelated predictor variables
m1 &lt;- mmpc.glmm2(target = reaction, reps = days, group = subject, dataset = x)
m2 &lt;- MMPC.glmm(target = reaction, reps = days, group = subject, dataset = x)

## End(Not run)
</code></pre>

<hr>
<h2 id='Feature+20selection+20using+20SES+20and+20MMPC+20for+20classifiication+20with+20longitudinal+20data'>
Feature selection using SES and MMPC for classifiication with longitudinal data
</h2><span id='topic+SES.timeclass'></span><span id='topic+MMPC.timeclass'></span>

<h3>Description</h3>

<p>SES algorithm follows a forward-backward filter approach for feature selection in order to provide minimal, highly-predictive, statistically-equivalent, multiple feature subsets of a high dimensional dataset. See also Details. MMPC algorithm follows the same approach without generating multiple feature subsets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SES.timeclass(target, reps, id, dataset, max_k = 3, threshold = 0.05, 
ini = NULL, wei = NULL, hash = FALSE, hashObject = NULL, ncores = 1) 

MMPC.timeclass(target, reps, id, dataset, max_k = 3, threshold = 0.05, 
ini = NULL, wei = NULL, hash = FALSE, hashObject = NULL, ncores = 1) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Feature+2B20selection+2B20using+2B20SES+2B20and+2B20MMPC+2B20for+2B20classifiication+2B20with+2B20longitudinal+2B20data_+3A_target">target</code></td>
<td>

<p>The class variable. Provide a vector or a factor with discrete numbers indicating the class. Its length is equal to the number of rows of the dataset. 
</p>
</td></tr>
<tr><td><code id="Feature+2B20selection+2B20using+2B20SES+2B20and+2B20MMPC+2B20for+2B20classifiication+2B20with+2B20longitudinal+2B20data_+3A_reps">reps</code></td>
<td>

<p>A numeric vector containing the time points of the subjects. Its length is equal to the number of rows of the dataset.
</p>
</td></tr>
<tr><td><code id="Feature+2B20selection+2B20using+2B20SES+2B20and+2B20MMPC+2B20for+2B20classifiication+2B20with+2B20longitudinal+2B20data_+3A_id">id</code></td>
<td>

<p>A numeric vector containing the subjects or groups. Its length is equal to the number of rows of the dataset.
</p>
</td></tr>
<tr><td><code id="Feature+2B20selection+2B20using+2B20SES+2B20and+2B20MMPC+2B20for+2B20classifiication+2B20with+2B20longitudinal+2B20data_+3A_dataset">dataset</code></td>
<td>

<p>The dataset; provide a matrix. Currently, only continuous datasets are supported. The dataset
contains longitudinal data, where each column is a variable. The repeated measurements are the samples. 
</p>
</td></tr>
<tr><td><code id="Feature+2B20selection+2B20using+2B20SES+2B20and+2B20MMPC+2B20for+2B20classifiication+2B20with+2B20longitudinal+2B20data_+3A_max_k">max_k</code></td>
<td>

<p>The maximum conditioning set to use in the conditional indepedence test (see Details). Integer, default value is 3.
</p>
</td></tr>
<tr><td><code id="Feature+2B20selection+2B20using+2B20SES+2B20and+2B20MMPC+2B20for+2B20classifiication+2B20with+2B20longitudinal+2B20data_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0, 1)) for assessing p-values significance. Default value is 0.05.
</p>
</td></tr>
<tr><td><code id="Feature+2B20selection+2B20using+2B20SES+2B20and+2B20MMPC+2B20for+2B20classifiication+2B20with+2B20longitudinal+2B20data_+3A_ini">ini</code></td>
<td>

<p>This is a supposed to be a list. After running SES or MMPC with some hyper-parameters you might want to run SES again with different hyper-parameters. To avoid calculating the univariate associations (first step of SES and of MPPC) again, you can extract them from the first run of SES and plug them here. This can speed up the second run (and subequent runs of course) by 50%. See the details and the argument &quot;univ&quot; in the output values. 
</p>
</td></tr>
<tr><td><code id="Feature+2B20selection+2B20using+2B20SES+2B20and+2B20MMPC+2B20for+2B20classifiication+2B20with+2B20longitudinal+2B20data_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. 
</p>
</td></tr>
<tr><td><code id="Feature+2B20selection+2B20using+2B20SES+2B20and+2B20MMPC+2B20for+2B20classifiication+2B20with+2B20longitudinal+2B20data_+3A_hash">hash</code></td>
<td>

<p>A boolean variable which indicates whether (TRUE) or not (FALSE) to store the statistics calculated during SES execution in a hash-type object. Default value is FALSE. If TRUE a hashObject is produced.
</p>
</td></tr>
<tr><td><code id="Feature+2B20selection+2B20using+2B20SES+2B20and+2B20MMPC+2B20for+2B20classifiication+2B20with+2B20longitudinal+2B20data_+3A_hashobject">hashObject</code></td>
<td>

<p>A List with the hash objects generated in a previous run of SES.glmm. 
Each time SES runs with &quot;hash=TRUE&quot; it produces a list of hashObjects that can be re-used in order to speed up next runs of SES.
</p>
<p>Important: the generated hashObjects should be used only when the same dataset is re-analyzed, possibly with different values of max_k and threshold.
</p>
</td></tr>
<tr><td><code id="Feature+2B20selection+2B20using+2B20SES+2B20and+2B20MMPC+2B20for+2B20classifiication+2B20with+2B20longitudinal+2B20data_+3A_ncores">ncores</code></td>
<td>

<p>How many cores to use. This plays an important role if you have tens of thousands of variables or really large sample sizes and tens of thousands of variables and a regression based test which requires numerical optimisation. In other cases it will not make a difference in the overall time (in fact it can be slower). The parallel computation is used in the first step of the algorithm, where univariate associations are examined, those take place in parallel. We have seen a reduction in time of 50% with 4 cores in comparison to 1 core. Note also, that the amount of reduction is definetely not linear in the number of cores.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is SES and MMPC used in the static-longitudinal scenario of Tsagris, Lagani and Tsamardinos (2018). The idea is that you have many features of longitudinal data for many subjects. For each subject you have calculated the coefficients of a simple linear regression over time and this is repeated for each feature. In the end, assuming p features, you have p constants and p slopes for each subject, each constant and slope refers to a feature for a subject. Hence, each new feature consists of two vectors, the constants and the slopes and the feature selection takes place there.
</p>


<h3>Value</h3>

<p>The output of the algorithm is an object of the class 'SESoutput' for SES or 'MMPCoutput' for MMPC including:
</p>
<table>
<tr><td><code>selectedVars</code></td>
<td>

<p>The selected variables, i.e., the signature of the target variable.
</p>
</td></tr>
<tr><td><code>selectedVarsOrder</code></td>
<td>

<p>The order of the selected variables according to increasing pvalues.
</p>
</td></tr>
<tr><td><code>queues</code></td>
<td>

<p>A list containing a list (queue) of equivalent features for each variable included in selectedVars. An equivalent signature can be built by selecting a single feature from each queue. 
Featured only in SES.
</p>
</td></tr>
<tr><td><code>signatures</code></td>
<td>

<p>A matrix reporting all equivalent signatures (one signature for each row). Featured only in SES.
</p>
</td></tr>
<tr><td><code>hashObject</code></td>
<td>

<p>The hashObject caching the statistic calculated in the current run.
</p>
</td></tr>
<tr><td><code>pvalues</code></td>
<td>

<p>For each feature included in the dataset, this vector reports the strength of its association with the target in the context of all other variables. 
Particularly, this vector reports the max p-values found when the association of each variable with the target is tested against different conditional sets. 
Lower values indicate higher association. <b>Note that these are the logged p-values, natural logarithm of the pvalues, and not the p-values</b>.
</p>
</td></tr>
<tr><td><code>stats</code></td>
<td>

<p>The statistics corresponding to &quot;pvalues&quot; (higher values indicates higher association).
</p>
</td></tr>
<tr><td><code>univ</code></td>
<td>

<p>This is a list with the univariate associations. The test statistics and their corresponding <b>logged p-values</b>, along with their flag (1 if the test was perfromed and 0 otherwise). 
This list is very important for subsequent runs of SES with different hyper-parameters. After running SES with some hyper-parameters you might want to run SES again with different 
hyper-parameters. To avoid calculating the univariate associations (first step of SES or MMPC) again, you can take this list from the first run of SES and plug it in the argument &quot;ini&quot; 
in the next run(s) of SES or MMPC. This can speed up the second run (and subequent runs of course) by 50%. See the argument &quot;univ&quot; in the output values. 
</p>
</td></tr>
<tr><td><code>max_k</code></td>
<td>

<p>The max_k option used in the current run.
</p>
</td></tr>
<tr><td><code>threshold</code></td>
<td>

<p>The threshold option used in the current run.
</p>
</td></tr>
<tr><td><code>n.tests</code></td>
<td>

<p>If you have set hash = TRUE, then the number of tests performed by SES or MMPC will be returned. If you have not set this to TRUE, the number of univariate associations will be returned. 
So be careful with this number.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The run time of the algorithm. A numeric vector. The first element is the user time, the second element is the system time and the third element is the elapsed time.
</p>
</td></tr>
<tr><td><code>test</code></td>
<td>

<p>The character name of the statistic test used.
</p>
</td></tr>
</table>
<p>Generic Functions implemented for SESoutput Object:
</p>
<table>
<tr><td><code>plot(object=SESoutput</code>, <code>mode="all")</code></td>
<td>

<p>Plots the generated pvalues (using barplot) of the current SESoutput object in comparison to the threshold.
</p>
<p>Argument mode can be either &quot;all&quot; or &quot;partial&quot; for the first 500 pvalues of the object.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ioannis Tsamardinos, Vincenzo Lagani
</p>
<p>R implementation and documentation: Giorgos Athineou &lt;athineou@csd.uoc.gr&gt; Vincenzo Lagani &lt;vlagani@csd.uoc.gr&gt;  
</p>


<h3>References</h3>

<p>Tsagris M., Lagani V., &amp; Tsamardinos I. (2018). Feature selection for high-dimensional glmm data. BMC bioinformatics, 19(1), 17.
</p>
<p>Vincenzo Lagani, George Kortas and Ioannis Tsamardinos (2013), Biomarker signature identification in &quot;omics&quot; with multiclass outcome. Computational and Structural Biotechnology Journal, 6(7):1-7.
</p>
<p>McCullagh, Peter, and John A. Nelder. Generalized linear models.  CRC press, USA, 2nd edition, 1989.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mmpc.timeclass.model">mmpc.timeclass.model</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## assume these are longitudinal data, each column is a variable (or feature)
dataset &lt;- matrix( rnorm(400 * 30), ncol = 30 ) 
id &lt;- rep(1:80, each = 5)  ## 80 subjects
reps &lt;- rep( seq(4, 12, by = 2), 80)  ## 5 time points for each subject
## dataset contains are the regression coefficients of each subject's values on the 
## reps (which is assumed to be time in this example)
target &lt;- rep(0:1, each = 200)
a &lt;- MMPC.timeclass(target, reps, id, dataset)
</code></pre>

<hr>
<h2 id='Fit+20a+20mixture+20of+20beta+20distributions+20in+20p-values'>
Fit a mixture of beta distributions in p-values
</h2><span id='topic+pval.mixbeta'></span>

<h3>Description</h3>

<p>Fit a mixture of beta distributions in p-values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pval.mixbeta(p)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Fit+2B20a+2B20mixture+2B20of+2B20beta+2B20distributions+2B20in+2B20p-values_+3A_p">p</code></td>
<td>

<p>A vector of p-values.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The p-values are assumed to follow a mixture of two beta distributions. The null p-values follow Be(1, 1) and the non-null p-values follow Be(<code class="reqn">\xi</code>, 1). 
In the first step, the proportion of true null values using Storey and Tibshirani (2003) is calculated and then MLE is adopted to obtain <code class="reqn">\xi</code>. 
For more information on this see Triantafillou (2014).
</p>


<h3>Value</h3>

<p>A vector with the estimated <code class="reqn">\pi_0</code> and <code class="reqn">\xi</code> values. 
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Triantafillou S., Tsamardinos I. and Roumpelaki A. (2014). Learning neighborhoods of high confidence in constraint-based causal discovery. 
In European Workshop on Probabilistic Graphical Models, pp. 487-502.
</p>
<p>Storey J.D. and Tibshirani R. (2003). Statistical significance for genome-wide experiments. Proceedings of the National Academy of Sciences, 100: 9440-9445. 
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+pc.skel">pc.skel</a>, <a href="#topic+mmhc.skel">mmhc.skel</a>, <a href="#topic+corfs.network">corfs.network</a>, <a href="#topic+local.mmhc.skel">local.mmhc.skel</a>, <a href="#topic+conf.edge.lower">conf.edge.lower</a>  </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## simulate a dataset with continuous data
y &lt;- rdag2(400, p = 25, nei = 3)
ind &lt;- sample(1:25, 25)
x &lt;- y$x[, ind]
mod &lt;- pc.skel( x, method = "comb.fast", alpha = 0.01 ) 
pval &lt;- as.vector( mod$pvalue[lower.tri(mod$pvalue)] )
pval.mixbeta(pval)
</code></pre>

<hr>
<h2 id='Forward+20Backward+20Early+20Dropping+20selection+20regression'>
Forward Backward Early Dropping selection regression
</h2><span id='topic+fbed.reg'></span>

<h3>Description</h3>

<p>Forward Backward Early Dropping selection regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fbed.reg(target, dataset, prior = NULL, ini = NULL, test = NULL, threshold = 0.05, 
wei = NULL, K = 0, method = "LR", gam = NULL, backward = TRUE) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression_+3A_target">target</code></td>
<td>

<p>The class variable. Provide either a string, an integer, a numeric value, a vector, a factor, an ordered factor or a Surv object. 
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression_+3A_dataset">dataset</code></td>
<td>

<p>The dataset; provide either a data frame or a matrix (columns = variables, rows = samples). 
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression_+3A_prior">prior</code></td>
<td>

<p>If you have prior knowledge of some variables that must be in the variable selection phase add them here. This an be a vector (if you have one variable) or a matrix (if you more variables). This does not work during the backward phase at the moment.
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression_+3A_ini">ini</code></td>
<td>

<p>If you already have the test statistics and the p-values of the univariate associations (the first step of FBED) supply them as a list with the names &quot;stat&quot; and &quot;pvalue&quot; respectively. If you have used the EBIc this list contains the eBIC of the univariate associations. Note, that the &quot;gam&quot; argument must be the same though.
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression_+3A_test">test</code></td>
<td>

<p>The available tests: &quot;testIndReg&quot;, &quot;testIndPois&quot;, &quot;testIndNB&quot;, &quot;testIndLogistic&quot;, 
&quot;testIndMMReg&quot;, &quot;testIndBinom&quot;, &quot;censIndCR&quot;, &quot;censIndWR&quot;, &quot;testIndBeta&quot;, &quot;testIndZIP&quot;, 
&quot;testIndGamma, &quot;testIndNormLog&quot;, &quot;testIndTobit&quot;, &quot;testIndQPois&quot;,  &quot;testIndQBinom&quot;, 
&quot;testIndFisher&quot; &quot;testIndMultinom&quot;, &quot;testIndOrdinal&quot;, &quot;testIndClogit&quot; and &quot;testIndSPML&quot;. Note that not all 
of them work with eBIC. The only test that does not have &quot;eBIC&quot; and no weights (input argument &quot;wei&quot;) 
is the test &quot;gSquare&quot;. The tests &quot;testIndFisher&quot; and &quot;testIndSPML&quot; have no weights.
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0, 1)) for assessing p-values significance. Default value is 0.05.
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. It is not suggested when robust is set 
to TRUE. If you want to use the &quot;testIndBinom&quot;, then supply the successes in the y and the trials here.
An example where weights are used is surveys when stratified sampling has occured.
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression_+3A_k">K</code></td>
<td>

<p>How many times should the process be repeated? The default value is 0. You can also specify a range of values of K, say 0:4 for example.
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression_+3A_method">method</code></td>
<td>

<p>Do you want the likelihood ratio test to be performed (&quot;LR&quot; is the default value) or perform the selection using the &quot;eBIC&quot; criterion (BIC is a special case). 
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression_+3A_gam">gam</code></td>
<td>

<p>In case the method is chosen to be &quot;eBIC&quot; one can also specify the <code class="reqn">gamma</code> parameter. The default value is &quot;NULL&quot;, so that the value is automatically calculated.
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression_+3A_backward">backward</code></td>
<td>

<p>After the Forward Early Dropping phase, the algorithm proceeds witha the usual Backward Selection phase. The default value is set to TRUE. It is advised to perform this step as maybe some variables are false positives, they were wrongly selected. 
</p>
<p>The backward phase using likelihood ratio test and eBIc are two different functions and can be called directly by the user. SO, if you want for example to perform a backard regression with a different threshold value, just use these two functions separately. 
</p>
</td></tr>



</table>


<h3>Details</h3>

<p>The algorithm is a variation of the usual forward selection. At every step, the most significant variable enters the selected variables set. In addition, only the significant variables stay and are further examined. The non signifcant ones are dropped. This goes until no variable can enter the set. The user has the option to re-do this step 1 or more times (the argument K). In the end, a backward selection is performed to remove falsely selected variables. Note that you may have specified, for example, K=10, but the maximum value FBED used can be 4 for example. 
</p>
<p>The &quot;testIndQPois&quot; and &quot;testIndQBinom&quot; do not work for method = &quot;eBIC&quot; as there is no BIC associated with quasi Poisson and quasi binomial
regression models.
</p>
<p>If you specify a range of values of K it returns the results of fbed.reg for this range of values of K. For example, instead of runnning fbed.reg with K=0, K=1, K=2 and so on, you can run fbed.reg with K=0:2 and the selected variables found at K=2, K=1 and K=0 are returned. Note also, that you may specify maximum value of K to be 10, but the maximum value FBED used was 4 (for example). 
</p>


<h3>Value</h3>

<p>If K is a single number a list including:
</p>
<table>
<tr><td><code>univ</code></td>
<td>

<p>If you have used the log-likelihood ratio test this list contains the test statistics and the associated <b>logged p-values</b> of the univariate associations tests. If you have used the EBIc this list contains the eBIC of the univariate associations. Note, that the &quot;gam&quot; argument must be the same though.
</p>
</td></tr>
<tr><td><code>res</code></td>
<td>

<p>A matrix with the selected variables, their test statistic and the associated <b>logged p-value</b>.
</p>
</td></tr>
<tr><td><code>info</code></td>
<td>

<p>A matrix with the number of variables and the number of tests performed (or models fitted) at each round (value of K). This refers to the
forward phase only.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The runtime required.
</p>
</td></tr>
<tr><td><code>back.rem</code></td>
<td>

<p>The variables removed in the backward phase.
</p>
</td></tr>
<tr><td><code>back.n.tests</code></td>
<td>

<p>The number of models fitted in the backward phase.
</p>
</td></tr>
</table>
<p>If K is a sequence of numbers a list tincluding:
</p>
<table>
<tr><td><code>univ</code></td>
<td>

<p>If you have used the log-likelihood ratio test this list contains the test statistics and the associated p-values of the univariate associations tests. If you have used the EBIc this list contains the eBIC of the univariate associations.
</p>
</td></tr>
<tr><td><code>res</code></td>
<td>

<p>The results of fbed.reg with the maximum value of K. 
</p>
</td></tr>
<tr><td><code>mod</code></td>
<td>

<p>A list where each element refers to a K value. If you run FBEd with method = &quot;LR&quot; the selected variables, their test statistic and their p-value is returned. If you run it with method = &quot;eBIC&quot;, the selected variables and the eBIC of the model with those variables are returned.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Borboudakis G. and Tsamardinos I. (2019). Forward-backward selection with early dropping. Journal of Machine Learning Research, 20(8): 1-39.
</p>
<p>Tsagris, M. and Tsamardinos, I. (2019). Feature selection with the R package MXM. F1000Research 7: 1505
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+fs.reg">fs.reg</a>, <a href="#topic+ebic.bsreg">ebic.bsreg</a>, <a href="#topic+bic.fsreg">bic.fsreg</a>, <a href="#topic+MMPC">MMPC</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#simulate a dataset with continuous data
dataset &lt;- matrix( runif(100 * 20, 1, 100), ncol = 20 )
#define a simulated class variable 
target &lt;- rt(100, 10)

a1 &lt;- fbed.reg(target, dataset, test = "testIndReg") 
y &lt;- rpois(100, 10)
a2 &lt;- fbed.reg(y, dataset, test = "testIndPois") 
a3 &lt;- MMPC(y, dataset)
</code></pre>

<hr>
<h2 id='Forward+20Backward+20Early+20Dropping+20selection+20regression+20for+20big+20data'>
Forward Backward Early Dropping selection regression for big data
</h2><span id='topic+big.fbed.reg'></span>

<h3>Description</h3>

<p>Forward Backward Early Dropping selection regression for big data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>big.fbed.reg(target = NULL, dataset, threshold = 0.01, ini = NULL,
test = "testIndLogistic", K = 0, backward = FALSE) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression+2B20for+2B20big+2B20data_+3A_target">target</code></td>
<td>

<p>The class variable. Provide either a string, an integer, a numeric value, a vector, a factor, an ordered factor or a Surv object. This can also be NULL and will be extracted from the big.matrix object &quot;dataset&quot;. If you want to use the test &quot;censIndWR&quot;, for survival data for example, the target must not contain any censored values. 
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression+2B20for+2B20big+2B20data_+3A_dataset">dataset</code></td>
<td>

<p>The dataset; this is abig.matrix object, where rows denote the samples and columns the features. If &quot;target&quot; is NULL, the first column must be the target. Only continuous variables are allowed. <b>Note:</b> In the case of thest being &quot;gSquare&quot;, the dataset should contain the target variable in the last line. 
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression+2B20for+2B20big+2B20data_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0, 1)) for assessing p-values significance. Default value is 0.05.
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression+2B20for+2B20big+2B20data_+3A_ini">ini</code></td>
<td>

<p>If you already have the test statistics and the p-values of the univariate associations (the first step of FBED) supply them as a list with the names &quot;stat&quot; and &quot;pvalue&quot; respectively. 
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression+2B20for+2B20big+2B20data_+3A_test">test</code></td>
<td>

<p>The available tests: &quot;testIndFisher&quot;, &quot;testIndPois&quot;, &quot;testIndLogistic&quot;, &quot;censIndWR&quot;, &quot;testIndQPois&quot;, &quot;testIndMultinom&quot;, 
&quot;gSquare&quot;. 
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression+2B20for+2B20big+2B20data_+3A_k">K</code></td>
<td>

<p>How many times should the process be repeated? The default value is 0. You can also specify a range of values of K, say 0:4 for example.
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression+2B20for+2B20big+2B20data_+3A_backward">backward</code></td>
<td>

<p>After the Forward Early Dropping phase, the algorithm proceeds witha the usual Backward Selection phase. The default value is set to TRUE. It is advised to perform this step as maybe some variables are false positives, they were wrongly selected. Pay attention to this, as it will convert the big.matrix object with the selected features into a matrix object in R. 
</p>
<p>The backward phase using likelihood ratio test is a different functions and can be called directly by the user. So, if you want for example to perform a backard regression with a different threshold value, just use that functions separately. 
</p>
</td></tr>



</table>


<h3>Details</h3>

<p>The algorithm is a variation of the usual forward selection. At every step, the most significant variable enters the selected variables set. In addition, only the significant variables stay and are further examined. The non signifcant ones are dropped. This goes until no variable can enter the set. The user has the option to re-do this step 1 or more times (the argument K). In the end, a backward selection is performed to remove falsely selected variables. Note that you may have specified, for example, K=10, but the maximum value FBED used can be 4 for example. 
</p>
<p><b>Notes:</b> The backward phase needs caution, because the big.matrix object with the selected features is turned into a matrix and then the backward selection takes place. In general, this algorithm is to be used with a few tens ,or hundreds of features and millions of rows. It is designed for thin matrices only. The <code><a href="#topic+big.gomp">big.gomp</a></code> on the other hand is designed for thin, fat and matrices with many rows and many columns.
</p>



<h3>Value</h3>

<p>If K is a single number a list including:
</p>
<table>
<tr><td><code>univ</code></td>
<td>

<p>If you have used the log-likelihood ratio test this list contains the test statistics and the associated p-values of the univariate associations tests. If you have used the EBIc this list contains the eBIC of the univariate associations. Note, that the &quot;gam&quot; argument must be the same though.
</p>
</td></tr>
<tr><td><code>res</code></td>
<td>

<p>A matrix with the selected variables, their test statistic and the associated <b>logged p-value</b>.
</p>
</td></tr>
<tr><td><code>info</code></td>
<td>

<p>A matrix with the number of variables and the number of tests performed (or models fitted) at each round (value of K). This refers to the
forward phase only.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The runtime required.
</p>
</td></tr>
<tr><td><code>back.rem</code></td>
<td>

<p>The variables removed in the backward phase.
</p>
</td></tr>
<tr><td><code>back.n.tests</code></td>
<td>

<p>The number of models fitted in the backward phase.
</p>
</td></tr>










</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Borboudakis G. and Tsamardinos I. (2019). Forward-backward selection with early dropping. Journal of Machine Learning Research, 20(8): 1-39.
</p>




<h3>See Also</h3>

<p><code> <a href="#topic+fs.reg">fs.reg</a>, <a href="#topic+ebic.bsreg">ebic.bsreg</a>, <a href="#topic+bic.fsreg">bic.fsreg</a>, <a href="#topic+MMPC">MMPC</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#simulate a dataset with continuous data
x &lt;- matrix( runif(10^6 * 50, 1, 100), ncol = 50 )
require(bigmemory)
dataset &lt;- bigmemory::as.big.matrix(x)
#define a simulated class variable 
target &lt;- rt(10^6, 10)
a1 &lt;- big.fbed.reg(target, dataset, test = "testIndFisher") 
y &lt;- rpois(10^6, 10)
a2 &lt;- big.fbed.reg(y, dataset, test = "testIndPois") 

## End(Not run)
</code></pre>

<hr>
<h2 id='Forward+20Backward+20Early+20Dropping+20selection+20regression+20with+20GEE'>
Forward Backward Early Dropping selection regression with GEE
</h2><span id='topic+fbed.gee.reg'></span>

<h3>Description</h3>

<p>Forward Backward Early Dropping selection regression with GEE.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fbed.gee.reg(target, dataset, id, prior = NULL, reps = NULL, ini = NULL, 
threshold = 0.05, wei = NULL, K = 0, test = "testIndGEEReg", 
correl = "exchangeable", se = "jack") 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression+2B20with+2B20GEE_+3A_target">target</code></td>
<td>

<p>The class variable. This can be a numerical vector with continuous data, binary, discrete or an ordered factor variable. It can also be a factor variable with two levels only. Pay attention to this. If you have ordinal data, then supply an ordered factor variable. The data must be sorted so that observations on a cluster are contiguous rows for all entities in the formula.
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression+2B20with+2B20GEE_+3A_dataset">dataset</code></td>
<td>

<p>The set of candidate predictor variables provide a numerical matrix or a data.frame in case of categorical variables (columns = variables, rows = samples). In the case of ordinal regression, this can only be a numerical matrix, i.e. only continuous predictor variables. The data must be sorted so that observations on a cluster are contiguous rows for all entities in the formula.
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression+2B20with+2B20GEE_+3A_id">id</code></td>
<td>

<p>This is a numerical vector of the same size as target denoting the groups or the subjects. The data must be sorted so that observations on a cluster are contiguous rows for all entities in the formula.
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression+2B20with+2B20GEE_+3A_prior">prior</code></td>
<td>

<p>If you have prior knowledge of some variables that must be in the variable selection phase add them here. This an be a vector (if you have one variable) or a matrix (if you more variables). 
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression+2B20with+2B20GEE_+3A_reps">reps</code></td>
<td>

<p>This is a numerical vector of the same size as target denoting the groups or the subjects. If you have longitudinal data and you know the time points, then supply them here. The data must be sorted so that observations on a cluster are contiguous rows for all entities in the formula.
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression+2B20with+2B20GEE_+3A_ini">ini</code></td>
<td>

<p>If you already have the test statistics and the logged p-values of the univariate associations (the first step of FBED) supply them as a list with the names &quot;stat&quot; and &quot;pvalue&quot; respectively.
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression+2B20with+2B20GEE_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0, 1)) for asmmmbsing p-values significance. Default value is 0.05.
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression+2B20with+2B20GEE_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. 
It is mentioned in the &quot;geepack&quot; that weights is not (yet) the weight as in sas proc genmod, and hence is not recommended to use.
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression+2B20with+2B20GEE_+3A_k">K</code></td>
<td>

<p>How many times should the process be repated? The default value is 0. 
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression+2B20with+2B20GEE_+3A_test">test</code></td>
<td>

<p>This is for the type of regression to be used, &quot;testIndGEEReg&quot;, for Gaussian regression, 
&quot;testIndGEEGamma&quot; for Gamma regression, &quot;testIndGEELogistic for logistic regression, &quot;testIndGEENormLog&quot; for linear regression witha log link, &quot;testIndGEEPois&quot; for Poisson regression or &quot;testIndGEEOrdinal&quot; for ordinal regression.
</p>
</td></tr>



<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression+2B20with+2B20GEE_+3A_correl">correl</code></td>
<td>

<p>The correlation structure. For the Gaussian, Logistic, Poisson and Gamma regression this can be either &quot;exchangeable&quot; (compound symmetry, suitable for clustered data) or &quot;ar1&quot; (AR(1) model, suitable for longitudinal data). If you want to use the AR(1) correlation structure you must be careful with the input dat you give. All data (target and dataset) must be sorted according to the id and time points. All observations must be ordered according to time within each subject. See the vignette of <b>geepack</b> to understand how. For the ordinal logistic regression its only the &quot;exchangeable&quot; correlation sturcture. 
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression+2B20with+2B20GEE_+3A_se">se</code></td>
<td>

<p>The method for estimating standard errors. This is very important and crucial. The available options for Gaussian, Logistic, Poisson and Gamma regression are: a) 'san.se', the usual robust estimate. b) 'jack': if approximate jackknife variance estimate should be computed. c) 'j1s': if 1-step jackknife variance estimate should be computed and d) 'fij': logical indicating if fully iterated jackknife variance estimate should be computed. If you have many clusters (sets of repeated measurements) &quot;san.se&quot; is fine as it is astmpotically correct, plus jacknife estimates will take longer. If you have a few clusters, then maybe it's better to use jacknife estimates. 
</p>
<p>The jackknife variance estimator was suggested by Paik (1988), which is quite suitable for cases when the number of subjects is small (K &lt; 30), as in many biological studies. The simulation studies conducted by Ziegler et al. (2000) and Yan and Fine (2004) showed that the approximate jackknife estimates are in many cases in good agreement with the fully iterated ones. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithm is a variation of the usual forward selection. At every step, the most significant variable enters the selected variables set. In addition, only the significant variables stay and are further examined. The non signifcant ones are dropped. This goes until no variable can enter the set. The user has the option to redo this step 1 or more times (the argument K). In the end, a backward selection is performed to remove falsely selected variables. 
</p>
<p>Since GEE are likelihood free, all significance tests take place using the Wald test, hence we decided not to have a backward phase. This algorithm is suitable for both clustered and longitudinal (glmm) data. 
</p>
<p>If you specify a range of values of K it returns the results of fbed.reg for this range of values of K. For example, instead of runnning fbed.reg with K=0, K=1, K=2 and so on, you can run fbed.reg with K=0:2 and the selected variables found at K=2, K=1 and K=0 are returned. Note also, that you may specify maximum value of K to be 10, but the maximum value FBED used was 4 (for example). 
</p>
<p><b>For GEE make sure you load the library geepack first</b>. 
</p>


<h3>Value</h3>

<p>If K is a single number a list including:
</p>
<table>
<tr><td><code>res</code></td>
<td>

<p>A matrix with the selected variables, their test statistic and the associated <b>logged p-value</b>.
</p>
</td></tr>
<tr><td><code>info</code></td>
<td>

<p>A matrix with the number of variables and the number of tests performed (or models fitted) at each round (value of K).
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The runtime required.
</p>
</td></tr>
</table>
<p>If K is a sequence of numbers a list tincluding:
</p>
<table>
<tr><td><code>univ</code></td>
<td>

<p>If you have used the log-likelihood ratio test this list contains the test statistics and the associated logged p-values of the univariate associations tests. If you have used the EBIc this list contains the eBIC of the univariate associations.
</p>
</td></tr>
<tr><td><code>res</code></td>
<td>

<p>The results of fbed.gee.reg with the maximum value of K. 
</p>
</td></tr>
<tr><td><code>mod</code></td>
<td>

<p>A list where each element refers to a K value. If you run FBEd with method = &quot;LR&quot; the selected variables, their test statistic and their p-value is returned. If you run it with method = &quot;eBIC&quot;, the selected variables and the eBIC of the model with those variables are returned.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Borboudakis G. and Tsamardinos I. (2019). Forward-backward selection with early dropping. Journal of Machine Learning Research, 20(8): 1-39.
</p>
<p>Liang  K.Y.  and  Zeger  S.L. (1986). Longitudinal data analysis using generalized linear models. Biometrika, 73(1): 13-22.
</p>
<p>Prentice R.L. and Zhao L.P. (1991). Estimating equations for parameters in means and covariances
of multivariate discrete and continuous responses. Biometrics, 47(3): 825-839.
</p>
<p>Paik M.C. (1988). Repeated measurement analysis for nonnormal data in small samples. Communications in
Statistics-Simulation and Computation, 17(4): 1155-1171.
</p>
<p>Ziegler A., Kastner C., Brunner D. and Blettner M. (2000). Familial associations of lipid profiles: A generalised
estimating equations approach. Statistics in medicine, 19(24): 3345-3357
</p>
<p>Yan J. and Fine J. (2004). Estimating equations for association structures. Statistics in medicine, 23(6): 859-874
</p>
<p>Eugene Demidenko (2013). Mixed Models: Theory and Applications with R, 2nd Edition. New Jersey: Wiley &amp; Sons.
</p>
<p>The R package <b>geepack </b> vignette.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+fbed.glmm.reg">fbed.glmm.reg</a>, <a href="#topic+glmm.bsreg">glmm.bsreg</a>, <a href="#topic+MMPC.glmm">MMPC.glmm</a>, <a href="#topic+fbed.reg">fbed.reg</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
require(lme4)
data(sleepstudy)
reaction &lt;- sleepstudy$Reaction
days &lt;- sleepstudy$Days
subject &lt;- sleepstudy$Subject
x &lt;- matrix(rnorm(180 * 200),ncol = 200) ## unrelated predictor variables
m1 &lt;- fbed.gee.reg(reaction, x, subject) 
m2 &lt;- fbed.glmm.reg(reaction, x, subject, backward = FALSE) 

## End(Not run)
</code></pre>

<hr>
<h2 id='Forward+20Backward+20Early+20Dropping+20selection+20regression+20with+20GLMM'>
Forward Backward Early Dropping selection regression with GLMM
</h2><span id='topic+fbed.glmm.reg'></span>

<h3>Description</h3>

<p>Forward Backward Early Dropping selection regression with GLMM.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fbed.glmm.reg(target, dataset, id, prior = NULL, reps = NULL, ini = NULL, 
threshold = 0.05, wei = NULL, K = 0, method = "LR", gam = NULL, 
backward = TRUE, test = "testIndGLMMReg")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression+2B20with+2B20GLMM_+3A_target">target</code></td>
<td>

<p>The class variable. This can be a numerical vector with continuous data, binary or discrete valued data. It can also be a 
factor variable with two levels only.
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression+2B20with+2B20GLMM_+3A_dataset">dataset</code></td>
<td>

<p>The dataset; provide a numerical a matrix (columns = variables, rows = observations).
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression+2B20with+2B20GLMM_+3A_id">id</code></td>
<td>

<p>This is a numerical vector of the same size as target denoting the groups or the subjects. 
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression+2B20with+2B20GLMM_+3A_prior">prior</code></td>
<td>

<p>If you have prior knowledge of some variables that must be in the variable selection phase add them here. This an be a vector (if you have one variable) or a matrix (if you more variables). This does not work during the backward phase at the moment.
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression+2B20with+2B20GLMM_+3A_reps">reps</code></td>
<td>

<p>This is a numerical vector of the same size as target denoting the groups or the subjects. If you have longitudinal data and you know the time points, then supply them here. 
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression+2B20with+2B20GLMM_+3A_ini">ini</code></td>
<td>

<p>If you already have the test statistics and the p-values of the univariate associations (the first step of FBED) supply them as a 
list with the names &quot;stat&quot; and &quot;pvalue&quot; respectively in the case of method = &quot;LR&quot;. In the case of method = &quot;eBIC&quot; this is a list with an 
element called &quot;bic&quot;.
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression+2B20with+2B20GLMM_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0, 1)) for asmmmbsing p-values significance. Default value is 0.05.
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression+2B20with+2B20GLMM_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. 
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression+2B20with+2B20GLMM_+3A_k">K</code></td>
<td>

<p>How many times should the process be repated? The default value is 0. 
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression+2B20with+2B20GLMM_+3A_method">method</code></td>
<td>

<p>Do you want the likelihood ratio test to be performed (&quot;LR&quot; is the default value) or perform the selection using the &quot;eBIC&quot; criterion (BIC is a special case). 
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression+2B20with+2B20GLMM_+3A_gam">gam</code></td>
<td>

<p>In case the method is chosen to be &quot;eBIC&quot; one can also specify the <code class="reqn">gamma</code> parameter. The default value is &quot;NULL&quot;, so that the value is 
automatically calculated.
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression+2B20with+2B20GLMM_+3A_backward">backward</code></td>
<td>

<p>After the Forward Early Dropping phase, the algorithm proceeds witha the usual Backward Selection phase. The default value is set to TRUE. It is advised to perform this step as maybe some variables are false positives, they were wrongly selected. 
</p>
<p>The backward phase using likelihood ratio test and eBIc are two different functions and can be called directly by the user. SO, if you want for example to perform a backard regression with a different threshold value, just use these two functions separately. 
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression+2B20with+2B20GLMM_+3A_test">test</code></td>
<td>

<p>This is for the type of regression to be used, &quot;testIndGLMMReg&quot;, for Gaussian regression, &quot;testIndGLMMLogistic for logistic regression or &quot;testIndGLMMPois&quot; for Poisson regression, &quot;testIndGLMMGamma&quot; for Gamma regression and &quot;testIndGLMMNormLog&quot; for Gaussian regression with log link and &quot;testIndGLMMOrdinal&quot; for ordinal regression.
</p>
</td></tr>



</table>


<h3>Details</h3>

<p>The algorithm is a variation of the usual forward selection. At every step, the most significant variable enters the selected variables set. In addition, only the significant variables stay and are further examined. The non signifcant ones are dropped. This goes until no variable can enter the set. The user has the option to redo this step 1 or more times (the argument K). In the end, a backward selection is performed to remove falsely selected variables. 
</p>
<p>Bear in mind that for the &quot;gaussian&quot; case, the forward phase takes place using the F test (Wald statistic calibrated against an F distribution). The backward phase though takes place using the log-likelihood ratio test.
</p>
<p>If you specify a range of values of K it returns the results of fbed.reg for this range of values of K. For example, instead of runnning fbed.reg with K=0, K=1, K=2 and so on, you can run fbed.reg with K=0:2 and the selected variables found at K=2, K=1 and K=0 are returned. Note also, that you may specify maximum value of K to be 10, but the maximum value FBED used was 4 (for example). 
</p>


<h3>Value</h3>

<p>If K is a single number a list including:
</p>
<table>
<tr><td><code>res</code></td>
<td>

<p>A matrix with the selected variables, their test statistic and the associated <b>logged p-value</b>.
</p>
</td></tr>
<tr><td><code>info</code></td>
<td>

<p>A matrix with the number of variables and the number of tests performed (or models fitted) at each round (value of K). This refers to the
forward phase only.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The runtime required.
</p>
</td></tr>
<tr><td><code>back.rem</code></td>
<td>

<p>The variables removed in the backward phase.
</p>
</td></tr>
<tr><td><code>back.n.tests</code></td>
<td>

<p>The number of models fitted in the backward phase.
</p>
</td></tr>
</table>
<p>If K is a sequence of numbers a list tincluding:
</p>
<table>
<tr><td><code>univ</code></td>
<td>

<p>If you have used the log-likelihood ratio test this list contains the test statistics and the associated p-values of the univariate associations tests. If you have used the EBIc this list contains the eBIC of the univariate associations.
</p>
</td></tr>
<tr><td><code>res</code></td>
<td>

<p>The results of fbed.glmm.reg with the maximum value of K. 
</p>
</td></tr>
<tr><td><code>mod</code></td>
<td>

<p>A list where each element refers to a K value. If you run FBED with method = &quot;LR&quot; the selected variables, their test statistic and their p-value is returned. If you run it with method = &quot;eBIC&quot;, the selected variables and the eBIC of the model with those variables are returned.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Borboudakis G. and Tsamardinos I. (2019). Forward-backward selection with early dropping. Journal of Machine Learning Research, 20(8): 1-39.
</p>
<p>Eugene Demidenko (2013). Mixed Models: Theory and Applications with R, 2nd Edition. New Jersey: Wiley &amp; Sons.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+fbed.gee.reg">fbed.gee.reg</a>, <a href="#topic+glmm.bsreg">glmm.bsreg</a>, <a href="#topic+MMPC.glmm">MMPC.glmm</a>, <a href="#topic+cv.fbed.lmm.reg">cv.fbed.lmm.reg</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
require(lme4)
data(sleepstudy)
reaction &lt;- sleepstudy$Reaction
subject &lt;- sleepstudy$Subject
x &lt;- matrix(rnorm(180 * 200),ncol = 200) ## unrelated predictor variables
m1 &lt;- fbed.glmm.reg(reaction, cbind(x1, x), subject) 
m2 &lt;- MMPC.glmm(target = reaction, group = subject, dataset = x)

## End(Not run)
</code></pre>

<hr>
<h2 id='Forward+20selection+20regression'>
Variable selection in regression models with forward selection
</h2><span id='topic+fs.reg'></span>

<h3>Description</h3>

<p>Variable selection in regression models with forward selection
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fs.reg(target, dataset, ini = NULL, threshold = 0.05, wei = NULL, test = NULL, 
user_test = NULL, stopping = "BIC", tol = 2, ncores = 1) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Forward+2B20selection+2B20regression_+3A_target">target</code></td>
<td>

<p>The class variable. Provide either a string, an integer, a numeric value, a vector, a factor, an ordered factor or a Surv object. See also Details.
</p>
</td></tr>
<tr><td><code id="Forward+2B20selection+2B20regression_+3A_dataset">dataset</code></td>
<td>

<p>The dataset; provide either a data frame or a matrix (columns = variables, rows = samples). In either case, only two cases are avaialble, either all data are continuous, or categorical. 
</p>
</td></tr>
<tr><td><code id="Forward+2B20selection+2B20regression_+3A_ini">ini</code></td>
<td>

<p>If you have a set of variables already start with this one. Otherwise leave it NULL. 
</p>
</td></tr>
<tr><td><code id="Forward+2B20selection+2B20regression_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0, 1)) for asmmmbsing p-values significance. Default value is 0.05.
</p>
</td></tr>
<tr><td><code id="Forward+2B20selection+2B20regression_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. An example where weights are used is surveys when stratified sampling has occured. 
</p>
</td></tr>
<tr><td><code id="Forward+2B20selection+2B20regression_+3A_test">test</code></td>
<td>

<p>The regression model to use. Available options are most of the tests for SES and MMPC. The ones NOT available are &quot;gSquare&quot;, &quot;censIndER&quot;, &quot;testIndMVreg&quot;, &quot;testIndClogit&quot; and &quot;testIndSpearman&quot;. 
If you have continuous predictor variables in matrix form, you can put &quot;testIndFisher&quot; and this is pretty fast. Instead of calcualting partial F-tests, which requires liear regression models to be fit, 
it calcualtes partial correlation coefficients and this is much more efficient. 
</p>
</td></tr>
<tr><td><code id="Forward+2B20selection+2B20regression_+3A_user_test">user_test</code></td>
<td>

<p>A user-defined conditional independence test (provide a closure type object). Default value is NULL. If this is defined, 
the &quot;test&quot; argument is ignored.
</p>
</td></tr>
<tr><td><code id="Forward+2B20selection+2B20regression_+3A_stopping">stopping</code></td>
<td>

<p>The stopping rule. The BIC is always used for all methods. If you have linear regression though you can change this to &quot;adjrsq&quot; and in this case the adjusted R qaured is used.
</p>
</td></tr>
<tr><td><code id="Forward+2B20selection+2B20regression_+3A_tol">tol</code></td>
<td>

<p>The difference bewtween two successive values of the stopping rule. By default this is is set to 2. If for example, the BIC difference between two succesive models is less than 2, the process stops and the last variable, even though significant does not enter the model.
</p>
</td></tr>
<tr><td><code id="Forward+2B20selection+2B20regression_+3A_ncores">ncores</code></td>
<td>

<p>How many cores to use. This plays an important role if you have tens of thousands of variables or really large sample sizes and tens of thousands of variables and a regression based test which requires numerical optimisation. In other cases it will not make a difference in the overall time (in fact it can be slower). The parallel computation is used in the first step of the algorithm, where univariate associations are examined, those take place in parallel. We have seen a reduction in time of 50% with 4 cores in comparison to 1 core. Note also, that the amount of reduction is not linear in the number of cores.
</p>
</td></tr>



</table>


<h3>Details</h3>

<p>If the current 'test' argument is defined as NULL or &quot;auto&quot; and the user_test argument is NULL then the algorithm automatically selects the best test based on the type of the data. Particularly:
</p>

<ul>
<li><p> if target is a factor, the multinomial or the binary logistic regression is used. If the target has two values only,   binary logistic regression will be used.
</p>
</li>
<li><p> if target is a ordered factor, the ordered logit regression is used. Hence, if you want to use multinomial or ordinal logistic regression, make sure your target is factor. 
</p>
</li>
<li><p> if target is a numerical vector and the dataset is not a matrix, but a data.frame linear regression is used. If however, the dataset is a matrix, the correlation based forward selection is used. That is, instead of partial F-tests, we do partial correlation tests.
</p>
</li>
<li><p> if target is discrete numerical (counts), the poisson regression conditional independence test is used. If there are only two values, the binary logistic regression is to be used.
</p>
</li>
<li><p> if target is a Surv object, the Survival conditional independence test is used.
</p>
</li></ul>



<h3>Value</h3>

<p>In the case of test=&quot;testIndMMReg&quot; and class(dataset) = matrix, just one matrix is returned with the index of the selected variable(s), the p-value, the test statistic and the BIC value of each model. For all other cases, the output of the algorithm is S3 object including:
</p>
<table>
<tr><td><code>runtime</code></td>
<td>

<p>The run time of the algorithm. A numeric vector. The first element is the user time, the second element is the system time and the third element is the elapsed time.
</p>
</td></tr>
<tr><td><code>mat</code></td>
<td>

<p>A matrix with the variables and their latest test statistics and <b>logged p-values</b>.
</p>
</td></tr>
<tr><td><code>info</code></td>
<td>

<p>A matrix with the selected variables, their <b>logged p-values</b> and test statistics. Each row corresponds to a model which contains the variables up to that line. The BIC in the last column is the BIC of that model.
</p>
</td></tr>
<tr><td><code>ci_test</code></td>
<td>

<p>The conditional independence test used. 
</p>
</td></tr>
<tr><td><code>final</code></td>
<td>

<p>The final regression model.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+glm.fsreg">glm.fsreg</a>, <a href="#topic+lm.fsreg">lm.fsreg</a>, <a href="#topic+bic.fsreg">bic.fsreg</a>, <a href="#topic+bic.glm.fsreg">bic.glm.fsreg</a>, <a href="#topic+CondIndTests">CondIndTests</a>, <a href="#topic+MMPC">MMPC</a>, <a href="#topic+SES">SES</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)

#simulate a dataset with continuous data
dataset &lt;- matrix( runif(500 * 20, 1, 100), ncol = 20 )

#define a simulated class variable 
target &lt;- rt(500, 10)

a0 &lt;- fs.reg(target, dataset, threshold = 0.05, stopping = "BIC", tol = 2) 

a1 &lt;- fs.reg(target, dataset, threshold = 0.05, test = "testIndRQ", stopping = "BIC", 
tol = 2) 

require(survival, quietly = TRUE)
y &lt;- survival::Surv(rexp(500), rep(1, 500) )
a2 &lt;- fs.reg(y, dataset, threshold = 0.05, test = "censIndWR", stopping = "BIC", tol = 2) 
a3 &lt;- MMPC(target, dataset)

target &lt;- factor( rbinom(500, 1, 0.6) )
b2 &lt;- fs.reg(target, dataset, threshold = 0.05, test = NULL, stopping = "BIC", tol = 2) 
</code></pre>

<hr>
<h2 id='Forward+20selection+20with+20generalised+20linear+20regression+20models'>
Variable selection in generalised linear regression models with forward selection
</h2><span id='topic+glm.fsreg'></span><span id='topic+gammafsreg'></span><span id='topic+normlog.fsreg'></span>

<h3>Description</h3>

<p>Variable selection in generalised linear regression models with forward selection
</p>


<h3>Usage</h3>

<pre><code class='language-R'>glm.fsreg(target, dataset, ini = NULL, threshold = 0.05, wei = NULL, tol = 2, 
ncores = 1) 

gammafsreg(target, dataset, ini = NULL, threshold = 0.05, wei = NULL, tol = 2, 
ncores = 1) 

normlog.fsreg(target, dataset, ini = NULL, threshold = 0.05, wei = NULL, tol = 2, 
ncores = 1) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Forward+2B20selection+2B20with+2B20generalised+2B20linear+2B20regression+2B20models_+3A_target">target</code></td>
<td>

<p>A numerical vector or a factor variable with two levels. The Gamma regression reuqires strictly positive numbers, wehreas the normlog requires positive numbes, zero included.
</p>
</td></tr>
<tr><td><code id="Forward+2B20selection+2B20with+2B20generalised+2B20linear+2B20regression+2B20models_+3A_dataset">dataset</code></td>
<td>

<p>The dataset; provide either a data frame or a matrix (columns = variables, rows = samples). In either case, only two cases are available, either all data are continuous, or categorical. 
</p>
</td></tr>
<tr><td><code id="Forward+2B20selection+2B20with+2B20generalised+2B20linear+2B20regression+2B20models_+3A_ini">ini</code></td>
<td>

<p>If you have a set of variables already start with this one. Otherwise leave it NULL. 
</p>
</td></tr>
<tr><td><code id="Forward+2B20selection+2B20with+2B20generalised+2B20linear+2B20regression+2B20models_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0, 1)) for assessing the p-values significance. Default value is 0.05.
</p>
</td></tr>
<tr><td><code id="Forward+2B20selection+2B20with+2B20generalised+2B20linear+2B20regression+2B20models_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. An example where weights are used is surveys when stratified sampling has occured.
</p>
</td></tr>
<tr><td><code id="Forward+2B20selection+2B20with+2B20generalised+2B20linear+2B20regression+2B20models_+3A_tol">tol</code></td>
<td>

<p>The difference bewtween two successive values of the stopping rule. By default this is is set to 2. If for example, the BIC difference between two succesive models is less than 2, the process stops and the last variable, even though significant does not enter the model.
</p>
</td></tr>
<tr><td><code id="Forward+2B20selection+2B20with+2B20generalised+2B20linear+2B20regression+2B20models_+3A_ncores">ncores</code></td>
<td>

<p>How many cores to use. This plays an important role if you have tens of thousands of variables or really large sample sizes and tens of thousands of variables and a regression based test which requires numerical optimisation. In other cammmb it will not make a difference in the overall time (in fact it can be slower). The parallel computation is used in the first step of the algorithm, where univariate associations are examined, those take place in parallel. We have seen a reduction in time of 50% with 4 cores in comparison to 1 core. Note also, that the amount of reduction is not linear in the number of cores.
</p>
</td></tr>



</table>


<h3>Value</h3>

<p>The output of the algorithm is S3 object including:
</p>
<table>
<tr><td><code>runtime</code></td>
<td>

<p>The run time of the algorithm. A numeric vector. The first element is the user time, the second element is the system time and the third element is the elapsed time.
</p>
</td></tr>
<tr><td><code>mat</code></td>
<td>

<p>A matrix with the variables and their latest test statistics and <b>logged p-values</b>. If you have logistic or Poisson regression with continuous predictor variables in a matrix form and no weights, this will not appear. In this case, a C++ code is called and the output is less.
</p>
</td></tr>
<tr><td><code>info</code></td>
<td>

<p>A matrix with the selected variables, their <b>logged p-values</b> and test statistics. Each row corresponds to a model which contains the variables up to that line. The BIC in the last column is the BIC of that model.
</p>
</td></tr>
<tr><td><code>ci_test</code></td>
<td>

<p>The conditional independence test used. 
</p>
</td></tr>
<tr><td><code>final</code></td>
<td>

<p>The final regression model.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fs.reg">fs.reg</a>, <a href="#topic+lm.fsreg">lm.fsreg</a>, <a href="#topic+bic.fsreg">bic.fsreg</a>, <a href="#topic+bic.glm.fsreg">bic.glm.fsreg</a>, <a href="#topic+CondIndTests">CondIndTests</a>, <a href="#topic+MMPC">MMPC</a>, <a href="#topic+SES">SES</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)

#simulate a dataset with continuous data
dataset &lt;- matrix( runif(200 * 30, 1, 100), ncol = 30 )

#define a simulated class variable 
target &lt;- rpois(200, 10)

a &lt;- glm.fsreg(target, dataset, threshold = 0.05, tol = 2, ncores = 1 ) 
b &lt;- MMPC(target, dataset, max_k = 3, threshold = 0.05, test = "testIndPois")
</code></pre>

<hr>
<h2 id='Forward+20selection+20with+20linear+20regression+20models'>
Variable selection in linear regression models with forward selection
</h2><span id='topic+lm.fsreg'></span>

<h3>Description</h3>

<p>Variable selection in linear regression models with forward selection
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lm.fsreg(target, dataset, ini = NULL, threshold = 0.05, wei = NULL, stopping = "BIC", 
tol = 2, ncores = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Forward+2B20selection+2B20with+2B20linear+2B20regression+2B20models_+3A_target">target</code></td>
<td>

<p>The class variable. Provide either a string, an integer, a numeric value, a vector, a factor, an ordered factor or a Surv object. See also Details.
</p>
</td></tr>
<tr><td><code id="Forward+2B20selection+2B20with+2B20linear+2B20regression+2B20models_+3A_dataset">dataset</code></td>
<td>

<p>The dataset; provide either a data frame or a matrix (columns = variables, rows = samples). In either case, only two cases are avaialble, either all data are continuous, or categorical. 
</p>
</td></tr>
<tr><td><code id="Forward+2B20selection+2B20with+2B20linear+2B20regression+2B20models_+3A_ini">ini</code></td>
<td>

<p>If you have a set of variables already start with this one. Currently this can only be a matrix with continuous variables. In such cases, the dataset must also contain continuous variables only. Otherwise leave it NULL. 
</p>
</td></tr>
<tr><td><code id="Forward+2B20selection+2B20with+2B20linear+2B20regression+2B20models_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0, 1)) for assessing the p-values significance. Default value is 0.05.
</p>
</td></tr>
<tr><td><code id="Forward+2B20selection+2B20with+2B20linear+2B20regression+2B20models_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. An example where weights are used is surveys when stratified sampling has occured.
</p>
</td></tr>
<tr><td><code id="Forward+2B20selection+2B20with+2B20linear+2B20regression+2B20models_+3A_stopping">stopping</code></td>
<td>

<p>The stopping rule. The BIC (&quot;BIC&quot;) or the adjusted <code class="reqn">R^2</code> (&quot;adjrsq&quot;) can be used. 
</p>
</td></tr>
<tr><td><code id="Forward+2B20selection+2B20with+2B20linear+2B20regression+2B20models_+3A_tol">tol</code></td>
<td>

<p>The difference bewtween two successive values of the stopping rule. By default this is is set to 2. If for example, the BIC difference between two succesive models is less than 2, the process stops and the last variable, even though significant does not enter the model. If the adjusted <code class="reqn">R^2</code> is used, the tol should be something like 0.01 or 0.02. 
</p>
</td></tr>
<tr><td><code id="Forward+2B20selection+2B20with+2B20linear+2B20regression+2B20models_+3A_ncores">ncores</code></td>
<td>

<p>How many cores to use. This plays an important role if you have tens of thousands of variables or really large sample sizes and tens of thousands of variables and a regression based test which requires numerical optimisation. In other cammmb it will not make a difference in the overall time (in fact it can be slower). The parallel computation is used in the first step of the algorithm, where univariate associations are examined, those take place in parallel. We have seen a reduction in time of 50% with 4 cores in comparison to 1 core. Note also, that the amount of reduction is not linear in the number of cores.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Only linear regression (robust and non robust) is supported from this function.  
</p>


<h3>Value</h3>

<p>The output of the algorithm is S3 object including:
</p>
<table>
<tr><td><code>runtime</code></td>
<td>

<p>The run time of the algorithm. A numeric vector. The first element is the user time, the second element is the system time and the third element is the elapsed time.
</p>
</td></tr>
<tr><td><code>mat</code></td>
<td>

<p>A matrix with the variables and their latest test statistics and <b>logged p-values</b>.
</p>
</td></tr>
<tr><td><code>info</code></td>
<td>

<p>A matrix with the selected variables, their <b>logged p-values</b> and test statistics. Each row corresponds to a model which contains the variables up to that line. The BIC in the last column is the BIC of that model.
</p>
</td></tr>
<tr><td><code>models</code></td>
<td>

<p>The regression models, one at each step.
</p>
</td></tr>
<tr><td><code>ci_test</code></td>
<td>

<p>The conditional independence test used, &quot;testIndReg&quot;.  
</p>
</td></tr>
<tr><td><code>final</code></td>
<td>

<p>The final regression model.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fs.reg">fs.reg</a>, <a href="#topic+lm.fsreg">lm.fsreg</a>, <a href="#topic+bic.fsreg">bic.fsreg</a>, <a href="#topic+bic.glm.fsreg">bic.glm.fsreg</a>. <a href="#topic+CondIndTests">CondIndTests</a>, <a href="#topic+MMPC">MMPC</a>, <a href="#topic+SES">SES</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)

#simulate a dataset with continuous data
dataset &lt;- matrix( runif(200 * 20, 1, 100), ncol = 20 )

#define a simulated class variable 
target &lt;- 3 * dataset[, 10] + 2 * dataset[, 20] + rnorm(200, 0, 5)
a1 &lt;- lm.fsreg(target, dataset, threshold = 0.05, stopping = "BIC", tol = 2) 
</code></pre>

<hr>
<h2 id='G-square+20conditional+20independence+20test+20for+20discrete+20data'>
G-square conditional independence test for discrete data
</h2><span id='topic+gSquare'></span><span id='topic+permgSquare'></span>

<h3>Description</h3>

<p>The main task of this test is to provide a p-value PVALUE for the null hypothesis: feature 'X' is independent 
from 'TARGET' given a conditioning set CS. This test is based on the log likelihood ratio test.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gSquare(target, dataset, xIndex, csIndex, wei = NULL,  
univariateModels = NULL, hash = FALSE, stat_hash = NULL, 
pvalue_hash = NULL)

permgSquare(target, dataset, xIndex, csIndex, wei = NULL,
univariateModels = NULL, hash = FALSE, stat_hash = NULL, 
pvalue_hash = NULL, threshold = 0.05, R = 999)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="G-square+2B20conditional+2B20independence+2B20test+2B20for+2B20discrete+2B20data_+3A_target">target</code></td>
<td>

<p>A numeric vector containing the values of the target variable. <b>The minimum value must be 0</b>.
</p>
</td></tr>
<tr><td><code id="G-square+2B20conditional+2B20independence+2B20test+2B20for+2B20discrete+2B20data_+3A_dataset">dataset</code></td>
<td>

<p>A numeric matrix containing the variables for performing the test. Rows as samples and columns as features. <b>The minimum value must be 0</b>.
</p>
</td></tr>
<tr><td><code id="G-square+2B20conditional+2B20independence+2B20test+2B20for+2B20discrete+2B20data_+3A_xindex">xIndex</code></td>
<td>

<p>The index of the variable whose association with the target we want to test.
</p>
</td></tr>
<tr><td><code id="G-square+2B20conditional+2B20independence+2B20test+2B20for+2B20discrete+2B20data_+3A_csindex">csIndex</code></td>
<td>

<p>The indices of the variables to condition on.
</p>
</td></tr>
<tr><td><code id="G-square+2B20conditional+2B20independence+2B20test+2B20for+2B20discrete+2B20data_+3A_wei">wei</code></td>
<td>

<p>This argument is not used in this test. 
</p>
</td></tr>
<tr><td><code id="G-square+2B20conditional+2B20independence+2B20test+2B20for+2B20discrete+2B20data_+3A_univariatemodels">univariateModels</code></td>
<td>

<p>Fast alternative to the hash object for univariate tests. List with vectors &quot;pvalues&quot; (p-values), &quot;stats&quot; (statistics) and 
&quot;flags&quot; (flag = TRUE if the test was succesful) representing the univariate association of each variable with the target. 
Default value is NULL.
</p>
</td></tr>
<tr><td><code id="G-square+2B20conditional+2B20independence+2B20test+2B20for+2B20discrete+2B20data_+3A_hash">hash</code></td>
<td>

<p>A boolean variable which indicates whether (TRUE) or not (FALSE) to use the hash-based implementation of the statistics of SES. 
Default value is FALSE. If TRUE you have to specify the stat_hash argument and the pvalue_hash argument.
</p>
</td></tr>
<tr><td><code id="G-square+2B20conditional+2B20independence+2B20test+2B20for+2B20discrete+2B20data_+3A_stat_hash">stat_hash</code></td>
<td>

<p>A hash object which contains the cached generated statistics of a SES run in the current dataset, 
using the current test.
</p>
</td></tr>
<tr><td><code id="G-square+2B20conditional+2B20independence+2B20test+2B20for+2B20discrete+2B20data_+3A_pvalue_hash">pvalue_hash</code></td>
<td>

<p>A hash object which contains the cached generated p-values of a SES run in the current dataset, 
using the current test.
</p>
</td></tr>
<tr><td><code id="G-square+2B20conditional+2B20independence+2B20test+2B20for+2B20discrete+2B20data_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0, 1)) for assessing p-values significance. Default value is 0.05. This is actually obsolete here, but has to be in order to have a concise list of input arguments across the same family of functions.
</p>
</td></tr>
<tr><td><code id="G-square+2B20conditional+2B20independence+2B20test+2B20for+2B20discrete+2B20data_+3A_r">R</code></td>
<td>

<p>The number of permutations to use. The default value is 999.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the number of samples is at least 5 times the number of the parameters to be estimated, the test is performed, otherwise, 
independence is not rejected (see Tsmardinos et al., 2006, pg. 43) 
</p>
<p>If hash = TRUE, testIndLogistic requires the arguments 'stat_hash' and 'pvalue_hash' for the hash-based implementation of the 
statistical test. These hash Objects are produced or updated by each run of SES (if hash == TRUE) and they can be reused in order 
to speed up next runs of the current statistic test. If &quot;SESoutput&quot; is the output of a SES run, then these objects can be 
retrieved by SESoutput@hashObject$stat_hash and the SESoutput@hashObject$pvalue_hash.
</p>
<p>Important: Use these arguments only with the same dataset that was used at initialization.
</p>
<p>For all the available conditional independence tests that are currently included on the package, please see &quot;?CondIndTests&quot;.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>pvalue</code></td>
<td>

<p>A numeric value that represents the logarithm of the generated p-value of the <code class="reqn">G^2</code> test (see reference below).
</p>
</td></tr>
<tr><td><code>stat</code></td>
<td>

<p>A numeric value that represents the generated statistic of the <code class="reqn">G^2</code> test (see reference below).
</p>
</td></tr>
<tr><td><code>stat_hash</code></td>
<td>

<p>The current hash object used for the statistics. See argument stat_hash and details. If argument hash = FALSE this is NULL.
</p>
</td></tr>
<tr><td><code>pvalue_hash</code></td>
<td>

<p>The current hash object used for the p-values. See argument stat_hash and details. If argument hash = FALSE this is NULL.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>R implementation and documentation: Giorgos Athineou &lt;athineou@csd.uoc.gr&gt;
</p>


<h3>References</h3>

<p>Tsamardinos, Ioannis, Laura E. Brown, and Constantin F. Aliferis. The max-min hill-climbing Bayesian network structure learning algorithm. 
Machine learning, 2006 65(1): 31&ndash;78. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SES">SES</a>, <a href="#topic+testIndFisher">testIndFisher</a>, <a href="#topic+testIndLogistic">testIndLogistic</a>, <a href="#topic+censIndCR">censIndCR</a>, <a href="#topic+CondIndTests">CondIndTests</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#simulate a dataset with binary data
dataset &lt;- matrix(rbinom(500 * 51, 1, 0.6), ncol = 51)
#initialize binary target
target &lt;- dataset[, 51]
#remove target from the dataset
dataset &lt;- dataset[, -51]

#run the gSquare conditional independence test for the binary class variable
results &lt;- gSquare(target, dataset, xIndex = 44, csIndex = c(10,20) )
results

#run SES algorithm using the gSquare conditional independence test for the binary class variable
sesObject &lt;- SES(target, dataset, max_k = 3, threshold = 0.05, test = "gSquare");
target &lt;- as.factor(target)
sesObject2 &lt;- SES(target, dataset, max_k = 3, threshold = 0.05, test = "testIndLogistic");
</code></pre>

<hr>
<h2 id='Generalised+20linear+20mixed+20models+20based+20on+20glmm+20SES+20and+20MMPC+20outputs'>
Generalised linear mixed model(s) based obtained from glmm SES or MMPC
</h2><span id='topic+mmpc.glmm.model'></span><span id='topic+ses.glmm.model'></span><span id='topic+mmpc.gee.model'></span><span id='topic+ses.gee.model'></span>

<h3>Description</h3>

<p>One or more regression models obtained from SES or MMPC, are returned. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mmpc.glmm.model(target, dataset, reps = NULL, group, slopes = FALSE, wei = NULL,
mmpcglmm.Object, test = NULL) 

ses.glmm.model(target, dataset, reps = NULL, group, slopes = FALSE, wei = NULL,
sesglmm.Object, nsignat = 1, test = NULL) 

mmpc.gee.model(target, dataset, reps = NULL, group, correl = "exchangeable", 
se = "jack", wei = NULL, mmpcgee.Object, test = NULL)

ses.gee.model(target, dataset, reps = NULL, group, correl = "exchangeable", 
se = "jack", wei = NULL, sesgee.Object, nsignat = 1, test = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Generalised+2B20linear+2B20mixed+2B20models+2B20based+2B20on+2B20glmm+2B20SES+2B20and+2B20MMPC+2B20outputs_+3A_target">target</code></td>
<td>

<p>The class variable. Provide a vector with continuous (normal), binary (binomial) or discrete (Poisson) data.
</p>
</td></tr>
<tr><td><code id="Generalised+2B20linear+2B20mixed+2B20models+2B20based+2B20on+2B20glmm+2B20SES+2B20and+2B20MMPC+2B20outputs_+3A_dataset">dataset</code></td>
<td>

<p>The dataset; provide either a data frame or a matrix (columns = variables, rows = observations). Currently, only continuous 
datasets are supported. 
</p>
</td></tr>
<tr><td><code id="Generalised+2B20linear+2B20mixed+2B20models+2B20based+2B20on+2B20glmm+2B20SES+2B20and+2B20MMPC+2B20outputs_+3A_reps">reps</code></td>
<td>

<p>A numeric vector containing the time points of the subjects. Its length is equal to the length of the target variable. 
If you have clustered data, leave this NULL. 
</p>
</td></tr>
<tr><td><code id="Generalised+2B20linear+2B20mixed+2B20models+2B20based+2B20on+2B20glmm+2B20SES+2B20and+2B20MMPC+2B20outputs_+3A_group">group</code></td>
<td>

<p>A numeric vector containing the subjects or groups. It must be of the same legnth as target. 
</p>
</td></tr>
<tr><td><code id="Generalised+2B20linear+2B20mixed+2B20models+2B20based+2B20on+2B20glmm+2B20SES+2B20and+2B20MMPC+2B20outputs_+3A_slopes">slopes</code></td>
<td>

<p>Should random slopes for the ime effect be fitted as well? Default value is FALSE. 
</p>
</td></tr>
<tr><td><code id="Generalised+2B20linear+2B20mixed+2B20models+2B20based+2B20on+2B20glmm+2B20SES+2B20and+2B20MMPC+2B20outputs_+3A_correl">correl</code></td>
<td>

<p>The correlation structure. For the Gaussian, Logistic, Poisson and Gamma regression this can be either &quot;exchangeable&quot; (compound symmetry, 
suitable for clustered data) or &quot;ar1&quot; (AR(1) model, suitable for longitudinal data). For the ordinal logistic regression its only the 
&quot;exchangeable&quot; correlation sturcture.  
</p>
</td></tr>
<tr><td><code id="Generalised+2B20linear+2B20mixed+2B20models+2B20based+2B20on+2B20glmm+2B20SES+2B20and+2B20MMPC+2B20outputs_+3A_se">se</code></td>
<td>

<p>The method for estimating standard errors. This is very important and crucial. The available options for Gaussian, Logistic, Poisson and 
Gamma regression are: a)  'san.se', the usual robust estimate. b) 'jack': if approximate jackknife variance estimate should be computed. 
c) 'j1s': if 1-step jackknife variance estimate should be computed and d) 'fij': logical indicating if fully iterated jackknife variance 
estimate should be computed. If you have many clusters (sets of repeated measurements) &quot;san.se&quot; is fine as it is astmpotically correct, 
plus jacknife estimates will take longer. If you have a few clusters, then maybe it's better to use jacknife estimates. 
</p>
<p>The jackknife variance estimator was suggested by Paik (1988), which is quite suitable for cases when the number of subjects is small 
(K &lt; 30), as in many biological studies. The simulation studies conducted by Ziegler et al. (2000) and Yan and Fine (2004) showed that 
the approximate jackknife estimates are in many cases in good agreement with the fully iterated ones. 
</p>
</td></tr>
<tr><td><code id="Generalised+2B20linear+2B20mixed+2B20models+2B20based+2B20on+2B20glmm+2B20SES+2B20and+2B20MMPC+2B20outputs_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. 
</p>
</td></tr>
<tr><td><code id="Generalised+2B20linear+2B20mixed+2B20models+2B20based+2B20on+2B20glmm+2B20SES+2B20and+2B20MMPC+2B20outputs_+3A_mmpcglmm.object">mmpcglmm.Object</code></td>
<td>

<p>An object with the results of an MMPC.glmm run.
</p>
</td></tr>
<tr><td><code id="Generalised+2B20linear+2B20mixed+2B20models+2B20based+2B20on+2B20glmm+2B20SES+2B20and+2B20MMPC+2B20outputs_+3A_sesglmm.object">sesglmm.Object</code></td>
<td>

<p>An object with the results of a SES.glmm run.
</p>
</td></tr>
<tr><td><code id="Generalised+2B20linear+2B20mixed+2B20models+2B20based+2B20on+2B20glmm+2B20SES+2B20and+2B20MMPC+2B20outputs_+3A_mmpcgee.object">mmpcgee.Object</code></td>
<td>

<p>An object with the results of an MMPC.gee run.
</p>
</td></tr>
<tr><td><code id="Generalised+2B20linear+2B20mixed+2B20models+2B20based+2B20on+2B20glmm+2B20SES+2B20and+2B20MMPC+2B20outputs_+3A_sesgee.object">sesgee.Object</code></td>
<td>

<p>An object with the results of an SES.gee run.
</p>
</td></tr>
<tr><td><code id="Generalised+2B20linear+2B20mixed+2B20models+2B20based+2B20on+2B20glmm+2B20SES+2B20and+2B20MMPC+2B20outputs_+3A_nsignat">nsignat</code></td>
<td>

<p>How many signatures to use. If nsignat = 1 (default value) the first set of variables will be used for the model. If you want more, 
then specify the nubmer of signatures you want. If you want the models based on all signatures, specify &quot;all&quot;. If you put a number 
which is higher than the number of signatures, all models will be returned.
</p>
</td></tr>
<tr><td><code id="Generalised+2B20linear+2B20mixed+2B20models+2B20based+2B20on+2B20glmm+2B20SES+2B20and+2B20MMPC+2B20outputs_+3A_test">test</code></td>
<td>

<p>The conditional independence test to use. Default value is NULL. Currently, the only available conditional independence test are 
the <code><a href="#topic+testIndGLMMReg">testIndGLMMReg</a></code>, <code><a href="#topic+testIndGLMMLogistic">testIndGLMMLogistic</a></code>, <code><a href="#topic+testIndGLMMPois">testIndGLMMPois</a></code> and <code><a href="#topic+testIndLMM">testIndLMM</a></code> which fit 
linear mixed models. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This command is useful if you want to see all models and check for example their fitting ability, MSE in linear models for exmaple.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>mod</code></td>
<td>

<p>Depending on the number of signatures requested, one or models will be returned.
</p>
</td></tr>
<tr><td><code>signature</code></td>
<td>

<p>A matrix (or just one vector if one signature only) with the variables of each signature, along with the BIC of the corresponding regression model.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Tsagris, M., Lagani, V., &amp; Tsamardinos, I. (2018). Feature selection for high-dimensional glmm data. BMC bioinformatics, 19(1), 17.
</p>
<p>I. Tsamardinos, M. Tsagris and V. Lagani (2015). Feature selection for longitudinal data. Proceedings of the 10th conference of 
the Hellenic Society for Computational Biology &amp; Bioinformatics (HSCBB15)
</p>
<p>Eugene Demidenko (2013). Mixed Models: Theory and Applications with R, 2nd Edition. New Jersey: Wiley &amp; Sons.
</p>
<p>Pinheiro J. and D. Bates. Mixed-effects models in S and S-PLUS. Springer Science &amp; Business Media, 2006.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+SES">SES</a>, <a href="#topic+MMPC">MMPC</a>, <a href="#topic+cv.ses">cv.ses</a>, <a href="#topic+cv.mmpc">cv.mmpc</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
require(lme4)
data(sleepstudy)
reaction &lt;- sleepstudy$Reaction
days &lt;- sleepstudy$Days
subject &lt;- sleepstudy$Subject
x &lt;- matrix(rnorm(180 * 50),ncol = 50) ## unrelated predictor variables
m1 &lt;- SES.glmm(reaction, days, subject, x)
m2 &lt;- MMPC.glmm(reaction, days, subject, x)
mod &lt;- mmpc.glmm.model(reaction, dataset = x, reps = days, group = subject, slopes = FALSE, 
mmpcglmm.Object = m2) 

## End(Not run)
</code></pre>

<hr>
<h2 id='Generalised+20ordinal+20regression'>
Generalised ordinal regression
</h2><span id='topic+ordinal.reg'></span>

<h3>Description</h3>

<p>Generalised ordinal regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ordinal.reg(formula, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Generalised+2B20ordinal+2B20regression_+3A_formula">formula</code></td>
<td>

<p>An object of class &quot;formula&quot; (or one that can be coerced to that class): a symbolic description of the model to be fitted. This is the usual formula use by many regression
models in R and other packages. Type &quot;glm&quot; or &quot;formula&quot; in R for more information. 
</p>
</td></tr>
<tr><td><code id="Generalised+2B20ordinal+2B20regression_+3A_data">data</code></td>
<td>

<p>A data.frame object carrying the relevant data.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Generalised ordinal regression is fitted. This means the instead of having the same coefficient for each predictor variable, they are allowed 
to vary. The usual, proportional odds, ordinal regression specifies that the lines do not cross. This one does not need the proportional odds assumption. The proportional odds assumption does not always hold in practice and is a rather restrictive model. Be careful though, you may 
end up qith negative probabilities. We do a tick to fix them, but in that case, you may have not found the optimal model. 
This is a problematic case unfortunately. Williams (2006) explains in a very nice way how one can fit this model by using many 
logistic regressions in an incremental way. The number of logistic regression models to be fit is the number of categories of the 
response variables - 1.  
</p>
<p>It may be the case that the message says &quot;problematic region&quot;. In this case the optimization was not succesful and perhaps the deviance is not 
at the global minimum. For example, with the addition of one extra variable the deviance might increase. I know, this type of ordinal regression is hard. In these difficult situations other packages return &quot;error&quot;. Another difficult I have seen is when &quot;NA&quot; appear in the coefficients. In this case I do not consider these coefficients, not their corresponding variables in the calculation of the deviance. 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>message</code></td>
<td>

<p>If you hit negative probabilities, the message &quot;problematic region&quot; will be printed. Otherwise, this is NULL.
</p>
</td></tr>
<tr><td><code>be</code></td>
<td>

<p>The regression coefficients.
</p>
</td></tr>
<tr><td><code>devi</code></td>
<td>

<p>The deviance of the model. 
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Williams, R. (2006). Generalized ordered logit/partial proportional odds models for ordinal dependent variables. Stata Journal, 6(1), 58-82
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+pc.skel">pc.skel</a>, <a href="#topic+ridge.reg">ridge.reg</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- factor( rbinom(100, 3, 0.5) )
x &lt;- matrix( rnorm(100 * 3), ncol = 3)
ordinal.reg(y ~ x, data = data.frame(x) )
ordinal.reg(y ~ 1, data = data.frame(x) )
</code></pre>

<hr>
<h2 id='Generate+20random+20folds+20for+20cross-validation'>
Generate random folds for cross-validation
</h2><span id='topic+generatefolds'></span>

<h3>Description</h3>

<p>Random folds for use in a cross validation are generated. There is the option for stratified splitting as well. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generatefolds(target, nfolds = 10, stratified = TRUE, seed = FALSE) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Generate+2B20random+2B20folds+2B20for+2B20cross-validation_+3A_target">target</code></td>
<td>

<p>A vector with some data, either continuous or categorical. 
</p>
</td></tr>
<tr><td><code id="Generate+2B20random+2B20folds+2B20for+2B20cross-validation_+3A_nfolds">nfolds</code></td>
<td>

<p>The number of folds to produce.
</p>
</td></tr>
<tr><td><code id="Generate+2B20random+2B20folds+2B20for+2B20cross-validation_+3A_stratified">stratified</code></td>
<td>

<p>A boolean variable specifying whether stratified random (TRUE) or simple random (FALSE) sampling is to be used when producing the folds.
</p>
</td></tr>
<tr><td><code id="Generate+2B20random+2B20folds+2B20for+2B20cross-validation_+3A_seed">seed</code></td>
<td>

<p>A boolean variable. If set to TRUE, the folds will always be the same. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>I was inspired by the sam command in the package <b>TunePareto</b> in order to do the stratified version. 
</p>


<h3>Value</h3>

<p>A list with nfolds elements where each elements is a fold containing the indices of the data.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cv.ses">cv.ses</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>a &lt;- generatefolds(iris[, 5], nfolds = 5, stratified = TRUE)
table(iris[a[[1]], 5])  ## 10 values from each group
</code></pre>

<hr>
<h2 id='Generic+20orthogonal+20matching+20pursuit+20+28gOMP+29'>
Generic orthogonal matching pursuit (gOMP)
</h2><span id='topic+gomp'></span><span id='topic+gomp.path'></span><span id='topic+boot.gomp'></span>

<h3>Description</h3>

<p>Generic orthogonal matching pursuit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gomp(target, dataset, xstand = TRUE, tol = qchisq(0.95, 1), 
test = "testIndLogistic", method = "ar2" ) 

gomp.path(target, dataset, xstand = TRUE, tol = c(4, 5, 6), 
test = "testIndLogistic", method = "ar2" ) 

boot.gomp(target, dataset, tol = qchisq(0.95, 1), 
test = "testIndLogistic", method = "ar2", B = 500, ncores = 1) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Generic+2B20orthogonal+2B20matching+2B20pursuit+2B20+2B28gOMP+2B29_+3A_target">target</code></td>
<td>

<p>The response variable, a numeric vector, a matrix or a Surv object.
</p>
</td></tr>
<tr><td><code id="Generic+2B20orthogonal+2B20matching+2B20pursuit+2B20+2B28gOMP+2B29_+3A_dataset">dataset</code></td>
<td>

<p>A matrix with continuous data, where the rows denote the samples and the columns are the variables. 
</p>
</td></tr>
<tr><td><code id="Generic+2B20orthogonal+2B20matching+2B20pursuit+2B20+2B28gOMP+2B29_+3A_xstand">xstand</code></td>
<td>

<p>If this is TRUE the independent variables are standardised. 
</p>
</td></tr>
<tr><td><code id="Generic+2B20orthogonal+2B20matching+2B20pursuit+2B20+2B28gOMP+2B29_+3A_tol">tol</code></td>
<td>

<p>The tolerance value to terminate the algorithm. This is the change in the criterion value 
between two successive steps. The default value is the 95% quantile of the <code class="reqn">\chi^2</code> distribution
with 1 degree of freedom. For test = &quot;testIndFisher&quot; the BIC is already calculated. 
</p>
<p>In the case of &quot;gomp.path&quot; this is a vector of values. For each tolerance value the result of the gOMP is returned. 
It returns the whole path of solutions. 
</p>
</td></tr>
<tr><td><code id="Generic+2B20orthogonal+2B20matching+2B20pursuit+2B20+2B28gOMP+2B29_+3A_test">test</code></td>
<td>

<p>This denotes the parametric model to be used each time. It depends upon the nature of the target variable. 
The possible values are &quot;testIndFisher&quot; (or &quot;testIndReg&quot; for the same purpose), &quot;testIndLogistic&quot;, 
&quot;testIndPois&quot;, &quot;testIndQPois&quot;, &quot;testIndQbinom&quot;, &quot;testIndNormLog&quot;, &quot;testIndMVreg&quot;, &quot;testIndNB&quot;, &quot;testIndBeta&quot;, 
&quot;testIndGamma&quot;, &quot;testIndMMReg&quot;, &quot;testIndRQ&quot;, &quot;testIndOrdinal&quot;, &quot;testIndTobit&quot;, &quot;censIndCR&quot;, &quot;censIndWR&quot;, &quot;censIndLLR&quot; and 
&quot;testIndMultinom&quot;.
</p>
</td></tr>
<tr><td><code id="Generic+2B20orthogonal+2B20matching+2B20pursuit+2B20+2B28gOMP+2B29_+3A_method">method</code></td>
<td>

<p>This is only for the &quot;testIndFisher&quot;. You can either specify, &quot;ar2&quot; for the adjusted R-square or &quot;sse&quot; for the sum of squares 
of errors. The tolerance value in both cases must a number between 0 and 1. That will denote a percentage. If the percentage 
increase or decrease is less than the nubmer the algorithm stops. An alternative is &quot;BIC&quot; for BIC and the tolerance values 
are like in all other regression models.
</p>
</td></tr>
<tr><td><code id="Generic+2B20orthogonal+2B20matching+2B20pursuit+2B20+2B28gOMP+2B29_+3A_b">B</code></td>
<td>

<p>How many bootstrap samples to generate. The gOMP will be performed for each of these samples. 
</p>
</td></tr>
<tr><td><code id="Generic+2B20orthogonal+2B20matching+2B20pursuit+2B20+2B28gOMP+2B29_+3A_ncores">ncores</code></td>
<td>

<p>How many cores to use. This argument is valid only if you have a multi-threaded machine.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>runtime</code></td>
<td>

<p>The runtime of the algorithm
</p>
</td></tr>
<tr><td><code>phi</code></td>
<td>

<p>The <code class="reqn">phi</code> coefficient, returned in the quasi binomial (testIndQBinom), quasi Poisson (testIndQPois), Gamma (testIndGamma) 
and Gaussian with log link (testIndNormLog). In all other cases this is NULL. 
</p>
</td></tr>
<tr><td><code>res</code></td>
<td>

<p>For the case of &quot;gomp&quot; a matrix with two columns. The selected variable(s) and the criterion value at every step. 
For the case of &quot;gomp.path&quot; a matrix with many columns. Every column contains the selected variables for each tolerance calue, 
starting from the smallest value (which selected most variables). The final column is the deviance of the model at each step. 
For the &quot;boot.gomp&quot; this is a matrix with two columns. The first one is the selected variables and the second column is their
proportion of selection.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Pati Y. C., Rezaiifar R. &amp; Krishnaprasad P. S. (1993). Orthogonal matching pursuit: Recursive function approximation with 
applications to wavelet decomposition. In Signals, Systems and Computers. 1993 Conference Record of The Twenty-Seventh 
Asilomar Conference on. IEEE.
</p>
<p>Davis G. (1994). Adaptive Nonlinear Approximations. PhD thesis. 
http://www.geoffdavis.net/papers/dissertation.pdf
</p>
<p>Mallat S. G. &amp; Zhang Z. (1993). Matching pursuits with time-frequency dictionaries. IEEE Transactions on signal processing, 
41(12), 3397-3415.
https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf
</p>
<p>Gharavi-Alkhansari M., &amp; Huang T. S. (1998, May). A fast orthogonal matching pursuit algorithm. In Acoustics, Speech and Signal 
Processing, 1998. 
Proceedings of the 1998 IEEE International Conference on (Vol. 3, pp. 1389-1392). IEEE.
</p>
<p>Chen S., Billings S. A., &amp; Luo W. (1989). Orthogonal least squares methods and their application to non-linear system identification. 
International Journal of control, 50(5), 1873-1896.
</p>
<p>Lozano A., Swirszcz G., &amp; Abe N. (2011). Group orthogonal matching pursuit for logistic regression. In Proceedings of the Fourteenth 
International Conference on Artificial Intelligence and Statistics.
</p>
<p>Razavi S. A. Ollila E., &amp; Koivunen V. (2012). Robust greedy algorithms for compressed sensing. In Signal Processing 
Conference (EUSIPCO), 2012 Proceedings of the 20th European. IEEE.
</p>
<p>Mazin Abdulrasool Hameed (2012). Comparative analysis of orthogonal matching pursuit and least angle regression. 
MSc thesis, Michigan State University.
https://www.google.gr/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0ahUKEwik9P3Yto7XAhUiCZoKHQ8XDr8QFgglMAA&amp;url=https
</p>
<p>Tsagris, M., Papadovasilakis, Z., Lakiotaki, K., &amp; Tsamardinos, I. (2022). 
The <code class="reqn">\gamma</code>-OMP algorithm for feature selection with application to gene expression data. 
IEEE/ACM Transactions on Computational Biology and Bioinformatics, 19(2): 1214-1224.
</p>


<h3>See Also</h3>

<p><code> <a href="Rfast.html#topic+cor.fbed">cor.fbed</a>, <a href="Rfast.html#topic+cor.fsreg">cor.fsreg</a>, <a href="Rfast.html#topic+correls">correls</a>, <a href="#topic+fs.reg">fs.reg</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix( rnorm(500 * 50), ncol = 50)
y &lt;- rnorm(500)
b &lt;- MXM::gomp(y, x, test = "testIndFisher")
</code></pre>

<hr>
<h2 id='Generic+20orthogonal+20matching+20pursuit+28gOMP+29+20for+20big+20data'>
Generic orthogonal matching pursuit(gOMP) for big data
</h2><span id='topic+big.gomp'></span><span id='topic+big.gomp.path'></span>

<h3>Description</h3>

<p>Generic orthogonal matching pursuit(gOMP) for big data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>big.gomp(target = NULL, dataset, tol = qchisq(0.95, 1) + log(dim(x)[1]), 
test = "testIndFisher", method = "ar2")

big.gomp.path(target = NULL, dataset, tol = c(8, 9, 10), 
test = "testIndFisher", method = "ar2")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Generic+2B20orthogonal+2B20matching+2B20pursuit+2B28gOMP+2B29+2B20for+2B20big+2B20data_+3A_target">target</code></td>
<td>

<p>The target (response) variable. If NULL, then it must inside the dataset. You might have the target variable though outside the 
big data file. This is like in the case of the regular gomp, a Surv object, a factor or a continuous numerical vector for all 
other cases. The default value is NULL.
</p>
</td></tr>
<tr><td><code id="Generic+2B20orthogonal+2B20matching+2B20pursuit+2B28gOMP+2B29+2B20for+2B20big+2B20data_+3A_dataset">dataset</code></td>
<td>

<p>The big.matrix oject. If target is NULL, the first column must be the target, the response variable and all the others are the 
predictor variables. In the case of survival data, the first two columns are used to form the response variable.
</p>
</td></tr>
<tr><td><code id="Generic+2B20orthogonal+2B20matching+2B20pursuit+2B28gOMP+2B29+2B20for+2B20big+2B20data_+3A_tol">tol</code></td>
<td>

<p>The tolerance value to terminate the algorithm. This is the change in the criterion value between two successive steps. 
The default value is the 95% quantile of the <code class="reqn">\chi^2</code> distribution with 1 degree of freedom. For test = &quot;testIndFisher&quot; 
the BIC is already calculated. 
</p>
<p>In the case of &quot;big.gomp.path&quot; this is a vector of values. For each tolerance value the result of the gOMP is returned. 
It returns the whole path of solutions. 
</p>
</td></tr>
<tr><td><code id="Generic+2B20orthogonal+2B20matching+2B20pursuit+2B28gOMP+2B29+2B20for+2B20big+2B20data_+3A_test">test</code></td>
<td>

<p>This denotes the parametric model to be used each time. It depends upon the nature of the target variable. 
The possible values are &quot;testIndFisher&quot; (or &quot;testIndReg&quot; for the same purpose), &quot;testIndLogistic&quot;, 
&quot;testIndPois&quot;, &quot;testIndQPois&quot;, &quot;testIndQbinom&quot;, &quot;testIndNormLog&quot;, &quot;testIndNB&quot;, &quot;testIndGamma&quot;, &quot;testIndMMReg&quot;, 
&quot;testIndRQ&quot;, &quot;testIndOrdinal&quot;, &quot;testIndTobit&quot;, &quot;censIndCR&quot; and &quot;censIndWR&quot;. 
</p>
</td></tr>
<tr><td><code id="Generic+2B20orthogonal+2B20matching+2B20pursuit+2B28gOMP+2B29+2B20for+2B20big+2B20data_+3A_method">method</code></td>
<td>

<p>This is only for the &quot;testIndFisher&quot;. You can either specify, &quot;ar2&quot; for the adjusted R-square or &quot;sse&quot; for the sum of squares 
of errors. The tolerance value in both cases must a number between 0 and 1. That will denote a percentage. If the percentage 
increase or decrease is less than the nubmer the algorithm stops. An alternative is &quot;BIC&quot; for BIC and the tolerance values are 
like in all other regression models.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The data (matrix) which will be read and compressed into a big.matrix object must be of type &quot;numeric&quot;. We tested it and it 
works with &quot;integer&quot; as well. But, in general, bear in mind that only matrices will be read. We have not tested with data.frame 
for example. Whatsoever, in the help page of the package &quot;bigmemory&quot; this is mentioned: Any non-numeric entry will be ignored 
and replaced with NA, so reading something that traditionally would be a data.frame won't cause an error. A warning is issued. 
In all cases, the object size is always 696 bytes!
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>runtime</code></td>
<td>

<p>The runtime of the algorithm
</p>
</td></tr>
<tr><td><code>phi</code></td>
<td>

<p>The <code class="reqn">phi</code> coefficient, returned in the quasi binomial (testIndQBinom), quasi Poisson (testIndQPois), Gamma (testIndGamma) 
and Gaussian with log link (testIndNormLog). In all other cases this is NULL.
</p>
</td></tr>
<tr><td><code>res</code></td>
<td>

<p>For the case of &quot;big.gomp&quot; a matrix with two columns. The selected variable(s) and the criterion value at every step. 
For the case of &quot;gomp.path&quot; a matrix with many columns. Every column contains the selected variables for each tolerance value, 
starting from the smallest value (which selected most variables). The final column is the deviance of the model at each step.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>. For more information see the &quot;bigmemory&quot; package.
</p>


<h3>References</h3>

<p>Pati Y. C., Rezaiifar R. &amp; Krishnaprasad P. S. (1993). Orthogonal matching pursuit: Recursive function approximation with 
applications to wavelet decomposition. In Signals, Systems and Computers. 1993 Conference Record of The Twenty-Seventh 
Asilomar Conference on. IEEE.
</p>
<p>Davis G. (1994). Adaptive Nonlinear Approximations. PhD thesis. 
http://www.geoffdavis.net/papers/dissertation.pdf
</p>
<p>Mallat S. G. &amp; Zhang Z. (1993). Matching pursuits with time-frequency dictionaries. IEEE Transactions on signal processing, 
41(12), 3397-3415.
https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf
</p>
<p>Gharavi-Alkhansari M., &amp; Huang T. S. (1998, May). A fast orthogonal matching pursuit algorithm. In Acoustics, Speech and Signal 
Processing, 1998. 
Proceedings of the 1998 IEEE International Conference on (Vol. 3, pp. 1389-1392). IEEE.
</p>
<p>Chen S., Billings S. A., &amp; Luo W. (1989). Orthogonal least squares methods and their application to non-linear system identification. 
International Journal of control, 50(5), 1873-1896.
</p>
<p>Lozano A., Swirszcz G., &amp; Abe N. (2011). Group orthogonal matching pursuit for logistic regression. In Proceedings of the Fourteenth 
International Conference on Artificial Intelligence and Statistics.
</p>
<p>Razavi S. A. Ollila E., &amp; Koivunen V. (2012). Robust greedy algorithms for compressed sensing. In Signal Processing 
Conference (EUSIPCO), 2012 Proceedings of the 20th European. IEEE.
</p>
<p>Mazin Abdulrasool Hameed (2012). Comparative analysis of orthogonal matching pursuit and least angle regression. 
MSc thesis, Michigan State University.
https://www.google.gr/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0ahUKEwik9P3Yto7XAhUiCZoKHQ8XDr8QFgglMAA&amp;url=https
</p>


<p>Tsagris, M., Papadovasilakis, Z., Lakiotaki, K., &amp; Tsamardinos, I. (2022). 
The <code class="reqn">\gamma</code>-OMP algorithm for feature selection with application to gene expression data. 
IEEE/ACM Transactions on Computational Biology and Bioinformatics, 19(2): 1214-1224.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+gomp">gomp</a>, <a href="#topic+read.big.data">read.big.data</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
dataset &lt;- matrix( runif(10^6 * 50, 1, 100), ncol = 50 )
write.csv(data, "dataset.csv", sep = ",")
a &lt;- read.big.data("dataset.csv")
mod &lt;- big.gomp(dataset = a, test = "testIndFisher", tol = 0.01)

## End(Not run)
</code></pre>

<hr>
<h2 id='Graph+20of+20unconditional+20associations'>
Graph of unconditional associations
</h2><span id='topic+corgraph'></span>

<h3>Description</h3>

<p>Calcualtes the graph of unconditional associations. If the correlation (Pearson, Spearman) or the <code class="reqn">G^2</code> test of independence, between pairs of continuous or categorical variables respectively is not statistically significant, there is no edge between the two respective nodes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>corgraph(dataset, test = "testIndFisher", threshold = 0.01)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Graph+2B20of+2B20unconditional+2B20associations_+3A_dataset">dataset</code></td>
<td>

<p>A matrix with the variables. The user must know if they are continuous or if they are categorical. If you have a matrix with categorical data, i.e. 0, 1, 2, 3 where each number indicates a category, the minimum number for each variable must be 0.
</p>
</td></tr>
<tr><td><code id="Graph+2B20of+2B20unconditional+2B20associations_+3A_test">test</code></td>
<td>

<p>The conditional independence test to use. Default value is &quot;testIndFisher&quot;. This procedure allows for &quot;testIndFisher&quot;, &quot;testIndSPearman&quot; for continuous variables and &quot;gSquare&quot; for categorical variables. 
</p>
</td></tr>
<tr><td><code id="Graph+2B20of+2B20unconditional+2B20associations_+3A_threshold">threshold</code></td>
<td>

<p>Threshold ( suitable values in (0, 1) ) for assessing p-values significance. Default value is 0.05.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>runtime</code></td>
<td>

<p>The run time of the algorithm. A numeric vector. The first element is the user time, the second element is the system time and the third element is the elapsed time.
</p>
</td></tr>
<tr><td><code>stat</code></td>
<td>

<p>A matrix with the test statistics. 
</p>
</td></tr>
<tr><td><code>pvalue</code></td>
<td>

<p>A matrix with the p-values. 
</p>
</td></tr>
<tr><td><code>G</code></td>
<td>

<p>The adjancency matrix. A value of 1 in G[i, j] appears in G[j, i] also, indicating that i and j have an edge between them.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Giorgos Athineou &lt;athineou@csd.uoc.gr&gt; and Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+pc.skel">pc.skel</a>, <a href="#topic+mmhc.skel">mmhc.skel</a>, <a href="#topic+corfs.network">corfs.network</a>, <a href="#topic+bn.skel.utils">bn.skel.utils</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate a dataset with continuous data
y &lt;- rdag2(500, p = 20, nei = 3)
x &lt;- y$x
a &lt;- mmhc.skel(x, max_k = 5, threshold = 0.01, test = "testIndFisher" ) 
b &lt;- pc.skel( x, alpha = 0.01 ) 
d &lt;- corgraph(x, test = "testIndFisher", threshold = 0.01) 
</code></pre>

<hr>
<h2 id='IAMB+20backward+20selection+20phase'>
IAMB backward selection phase
</h2><span id='topic+iamb.bs'></span>

<h3>Description</h3>

<p>IAMB backward selection phase.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>iamb.bs(target, dataset, threshold = 0.05, wei = NULL, test = NULL, user_test = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="IAMB+2B20backward+2B20selection+2B20phase_+3A_target">target</code></td>
<td>

<p>The class variable. Provide either a string, an integer, a numeric value, a vector, a factor, an ordered factor or a Surv object. 
</p>
</td></tr>
<tr><td><code id="IAMB+2B20backward+2B20selection+2B20phase_+3A_dataset">dataset</code></td>
<td>

<p>The dataset; provide either a data frame or a matrix (columns = variables, rows = observations). In either case, only two cases are avaialble, either all data are continuous, or categorical. 
</p>
</td></tr>
<tr><td><code id="IAMB+2B20backward+2B20selection+2B20phase_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0,1)) for assessing p-values significance. Default value is 0.05.
</p>
</td></tr>
<tr><td><code id="IAMB+2B20backward+2B20selection+2B20phase_+3A_test">test</code></td>
<td>

<p>The regression model to use. Available options are most of the tests for SES and MMPC. The ones NOT available are &quot;gSquare&quot;, &quot;censIndER&quot;, &quot;testIndMVreg&quot;, &quot;testIndClogit&quot;, &quot;testIndSpearman&quot; and &quot;testIndFisher&quot;.
</p>
</td></tr>
<tr><td><code id="IAMB+2B20backward+2B20selection+2B20phase_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. An example where weights are used is surveys when stratified sampling has occured.
</p>
</td></tr>
<tr><td><code id="IAMB+2B20backward+2B20selection+2B20phase_+3A_user_test">user_test</code></td>
<td>

<p>A user-defined conditional independence test (provide a closure type object). Default value is NULL. If this is defined, the &quot;test&quot; argument is ignored.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>IAMB stands for Incremental Association Markov Blanket. The algorithm comprises of a forward selection and a modified backward selection process. This functions does the modified backward selection process. In the usual backward selection, among the non singificant variabels, the one with the maximum p-value is dropped. So, one variable is removed at every step. In the IAMB backward phase, at aevery step, all non significant variables are removed. This makes it a lot faster. 
</p>


<h3>Value</h3>

<p>The output of the algorithm is a list of an S3 object including:
</p>
<table>
<tr><td><code>runtime</code></td>
<td>

<p>The run time of the algorithm. A numeric vector. The first element is the user time, the second element is the system time and the third element is the elapsed time.
</p>
</td></tr>
<tr><td><code>ci_test</code></td>
<td>

<p>The conditional independence test used. 
</p>
</td></tr>
<tr><td><code>vars</code></td>
<td>

<p>The selected variables.
</p>
</td></tr>
<tr><td><code>mat</code></td>
<td>

<p>A matrix with the selected variables and their latest test statistic and <b>logged p-value</b>. If no variable is selected this is NULL.
</p>
</td></tr>
<tr><td><code>final</code></td>
<td>

<p>The final regression model.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Tsamardinos, I., Aliferis, C. F., Statnikov, A. R., &amp; Statnikov, E. (2003). Algorithms for Large Scale Markov Blanket Discovery. In FLAIRS conference, pp. 376-380.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+glm.fsreg">glm.fsreg</a>, <a href="#topic+lm.fsreg">lm.fsreg</a>, <a href="#topic+bic.fsreg">bic.fsreg</a>, <a href="#topic+bic.glm.fsreg">bic.glm.fsreg</a>, <a href="#topic+CondIndTests">CondIndTests</a>, <a href="#topic+MMPC">MMPC</a>, <a href="#topic+SES">SES</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)
dataset &lt;- matrix( runif(500 * 10, 1, 100), ncol = 10 )
target &lt;- rnorm(500)

a1 &lt;- iamb.bs(target, dataset, threshold = 0.05, test = "testIndRQ") 
a2 &lt;- bs.reg(target, dataset, threshold = 0.05, test = "testIndRQ") 
</code></pre>

<hr>
<h2 id='IAMB+20variable+20selection'>
IAMB variable selection
</h2><span id='topic+iamb'></span>

<h3>Description</h3>

<p>IAMB variable selection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>iamb(target, dataset, threshold = 0.05, wei = NULL, test = NULL, user_test = NULL, 
stopping = "BIC", tol = 2, ncores = 1, back = "iambbs")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="IAMB+2B20variable+2B20selection_+3A_target">target</code></td>
<td>

<p>The class variable. Provide either a string, an integer, a numeric value, a vector, a factor, an ordered factor or a Surv object. 
</p>
</td></tr>
<tr><td><code id="IAMB+2B20variable+2B20selection_+3A_dataset">dataset</code></td>
<td>

<p>The dataset; provide either a data frame or a matrix (columns = variables, rows = observations). In either case, only two cases are avaialble, either all data are continuous, or categorical. 
</p>
</td></tr>
<tr><td><code id="IAMB+2B20variable+2B20selection_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0,1)) for assessing p-values significance. Default value is 0.05.
</p>
</td></tr>
<tr><td><code id="IAMB+2B20variable+2B20selection_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. An example where weights are used is surveys when stratified sampling has occured.
</p>
</td></tr>
<tr><td><code id="IAMB+2B20variable+2B20selection_+3A_test">test</code></td>
<td>

<p>The regression model to use. Available options are most of the tests for SES and MMPC. The ones NOT available are &quot;gSquare&quot;, &quot;censIndER&quot;, &quot;testIndMVreg&quot;, &quot;testIndClogit&quot;, &quot;testIndSpearman&quot; and &quot;testIndFisher&quot; and &quot;testIndIGreg&quot;. 
</p>
</td></tr>
<tr><td><code id="IAMB+2B20variable+2B20selection_+3A_user_test">user_test</code></td>
<td>

<p>A user-defined conditional independence test (provide a closure type object). Default value is NULL. If this is defined, the &quot;test&quot; argument is ignored.
</p>
</td></tr>
<tr><td><code id="IAMB+2B20variable+2B20selection_+3A_stopping">stopping</code></td>
<td>

<p>The stopping rule. The BIC is always used for all methods. If you have linear regression though you can change this to &quot;adjrsq&quot; and in this case the adjusted R qaured is used.
</p>
</td></tr>
<tr><td><code id="IAMB+2B20variable+2B20selection_+3A_tol">tol</code></td>
<td>

<p>The difference bewtween two successive values of the stopping rule. By default this is is set to 2. If for example, the BIC difference between two succesive models is less than 2, the process stops and the last variable, even though significant does not enter the model.
</p>
</td></tr>
<tr><td><code id="IAMB+2B20variable+2B20selection_+3A_ncores">ncores</code></td>
<td>

<p>How many cores to use. This plays an important role if you have tens of thousands of variables or really large sample sizes and tens of thousands of variables and a regression based test which requires numerical optimisation. In other cases it will not make a difference in the overall time (in fact it can be slower). The parallel computation is used in the first step of the algorithm, where univariate associations are examined, those take place in parallel. We have seen a reduction in time of 50% with 4 cores in comparison to 1 core. Note also, that the amount of reduction is not linear in the number of cores.
</p>
</td></tr>
<tr><td><code id="IAMB+2B20variable+2B20selection_+3A_back">back</code></td>
<td>

<p>The backward phase. If this &quot;iambbs&quot; (default value) the IAMB backward phase is performed and hence the IAMB algorithm is completed. If &quot;bs&quot;, a simple backward selection phase is performed. This way, the IAMB algorithm is slightly more general. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>IAMB stands for Incremental Association Markov Blanket. The algorithm comprises of a forward selection and a modified backward selection process. This functions does the modified 
backward selection process. In the usual backward selection, among the non singificant variabels, the one with the maximum p-value is dropped. 
So, one variable is removed at every step. In the IAMB backward phase, at aevery step, all non significant variables are removed. This makes it a lot faster. 
</p>


<h3>Value</h3>

<p>The output of the algorithm is a list of an S3 object including:
</p>
<table>
<tr><td><code>vars</code></td>
<td>

<p>A vector with the selected variables.
</p>
</td></tr>
<tr><td><code>mod</code></td>
<td>

<p>The output of the backward phase. In the case of no backward procedure this is the output of the forward phase. 
</p>
</td></tr>
<tr><td><code>mess</code></td>
<td>

<p>If the forward regression returned at most one variable, no backward procedure takes place and a message appears informing the user about this.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Tsamardinos, I., Aliferis, C. F., Statnikov, A. R., &amp; Statnikov, E. (2003). Algorithms for Large Scale Markov Blanket Discovery. In FLAIRS conference, pp. 376-380.
</p>




<h3>See Also</h3>

<p><code><a href="#topic+glm.fsreg">glm.fsreg</a>, <a href="#topic+lm.fsreg">lm.fsreg</a>, <a href="#topic+bic.fsreg">bic.fsreg</a>, <a href="#topic+bic.glm.fsreg">bic.glm.fsreg</a>, <a href="#topic+CondIndTests">CondIndTests</a>, <a href="#topic+MMPC">MMPC</a>, <a href="#topic+SES">SES</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)
dataset &lt;- matrix( runif(100 * 50, 1, 100), ncol = 50 )

target &lt;- rpois(100, 10)
a1 &lt;- iamb(target, dataset, threshold = 0.05, stopping = "BIC", tol = 0, back = "iambbs")
a2 &lt;- iamb(target, dataset, threshold = 0.05, stopping = "BIC", tol = 0, back = "bs")
</code></pre>

<hr>
<h2 id='Incremental+20BIC+20values+20and+20final+20regression+20model+20of+20the+20FBED+20algorithm'>
Incremental BIC values and final regression model of the FBED algorithm
</h2><span id='topic+fbedreg.bic'></span>

<h3>Description</h3>

<p>Incremental BIC values and final regression model of the FBED algorithm. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fbedreg.bic(target, dataset, wei = NULL, fbedreg.object, test = NULL, graph = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Incremental+2B20BIC+2B20values+2B20and+2B20final+2B20regression+2B20model+2B20of+2B20the+2B20FBED+2B20algorithm_+3A_target">target</code></td>
<td>

<p>A numeric vector containing the values of the target variable. It can also discrete data, binary data (as factor), nominal or ordinal data (as factor). In contrast to SES, no position of the target variable in the dataset is accepted. The target must be a numerical vector.
</p>
</td></tr>
<tr><td><code id="Incremental+2B20BIC+2B20values+2B20and+2B20final+2B20regression+2B20model+2B20of+2B20the+2B20FBED+2B20algorithm_+3A_dataset">dataset</code></td>
<td>

<p>A numeric matrix or data.frame containing the variables. Rows are samples and columns are features. If you have categorical variables, this should be a data frame.
</p>
</td></tr>
<tr><td><code id="Incremental+2B20BIC+2B20values+2B20and+2B20final+2B20regression+2B20model+2B20of+2B20the+2B20FBED+2B20algorithm_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL.  An example where weights are used is surveys when stratified sampling has occured.
We suggest not to use weights if you choose &quot;testIndMMReg&quot; as weights are already being used there.
</p>
</td></tr>
<tr><td><code id="Incremental+2B20BIC+2B20values+2B20and+2B20final+2B20regression+2B20model+2B20of+2B20the+2B20FBED+2B20algorithm_+3A_fbedreg.object">fbedreg.object</code></td>
<td>

<p>An object with the results of an FBED run.
</p>
</td></tr>
<tr><td><code id="Incremental+2B20BIC+2B20values+2B20and+2B20final+2B20regression+2B20model+2B20of+2B20the+2B20FBED+2B20algorithm_+3A_test">test</code></td>
<td>

<p>If you know the test used in SES put it here, otherwise leave it NULL. It will take this information from the SES object. 
</p>
</td></tr>
<tr><td><code id="Incremental+2B20BIC+2B20values+2B20and+2B20final+2B20regression+2B20model+2B20of+2B20the+2B20FBED+2B20algorithm_+3A_graph">graph</code></td>
<td>

<p>If you want a graphical representation of the drop in the BIC values set this to TRUE.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function takes the output of the FBED (<code><a href="#topic+fbed.reg">fbed.reg</a></code>) and fits succesive models calculating the BIC for each of them. A graph can also be returned. 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>res</code></td>
<td>

<p>A matrix with the selected variables, their test statistic and p-value (taken from the fbedreg.object) along with the BIC. Each row contains a BIC, that is the BIC of the model with the variables up to that row.
</p>
</td></tr>
<tr><td><code>mod</code></td>
<td>

<p>The final model with all selected variables.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Borboudakis G. and Tsamardinos I. (2019). Forward-backward selection with early dropping. Journal of Machine Learning Research, 20(8): 1-39.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+reg.fit">reg.fit</a>, <a href="#topic+mmpc.model">mmpc.model</a>, <a href="#topic+SES">SES</a>, <a href="#topic+MMPC">MMPC</a>, <a href="#topic+cv.ses">cv.ses</a>, <a href="#topic+cv.mmpc">cv.mmpc</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dataset &lt;- matrix( runif(100 * 20, 1, 100), ncol = 20 )
#define a simulated class variable 
target &lt;- rt(100, 10)
a &lt;- fbed.reg(target, dataset, K = 10, test = "testIndFisher", method = "eBIC") 
fbedreg.bic(target, dataset, fbedreg.object = a, test = "testIndFisher")
</code></pre>

<hr>
<h2 id='Interactive+20plot+20of+20an+20+28un+29directed+20graph'>
Interactive plot of an (un)directed graph
</h2><span id='topic+plotnetwork'></span>

<h3>Description</h3>

<p>Interactive plot of an (un)directed graph.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotnetwork(G, titlos)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Interactive+2B20plot+2B20of+2B20an+2B20+2B28un+2B29directed+2B20graph_+3A_g">G</code></td>
<td>

<p>The adjacency matrix as produced from <code><a href="#topic+mmhc.skel">mmhc.skel</a></code>, <code><a href="#topic+pc.skel">pc.skel</a></code>, <code><a href="#topic+pc.con">pc.con</a></code> or any other algorithm. 
This can correspond to an undirected, partially directed or a completely directed graph. 
</p>
</td></tr>
<tr><td><code id="Interactive+2B20plot+2B20of+2B20an+2B20+2B28un+2B29directed+2B20graph_+3A_titlos">titlos</code></td>
<td>
<p>A character argument specifying the title of the graph, for example &quot;PC network&quot;. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This visualises the directed graph. 
</p>


<h3>Value</h3>

<p>The plot of the directed graph. This is interactive, in the sense that the user can &quot;play&quot; with it. Move the nodes, zoom it, strectch it etc.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Giorgos Athineou &lt;athineou@csd.uoc.gr&gt; and Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+mmhc.skel">mmhc.skel</a>, <a href="#topic+nei">nei</a>, <a href="#topic+pc.skel">pc.skel</a>, <a href="#topic+mb">mb</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# simulate a dataset with continuous data
dataset &lt;- matrix( runif(200 * 20, 1, 100), nrow = 200 ) 
a &lt;- mmhc.skel(dataset, max_k = 3, threshold = 0.05, test = "testIndFisher", 
nc = 1) 
plotnetwork(a$G)
plotnetwork(a$G, titlos = "DAG skeleton")

## End(Not run)
</code></pre>

<hr>
<h2 id='Lower+20limit+20of+20the+20confidence+20of+20an+20edge'>
Lower limit of the confidence of an edge
</h2><span id='topic+conf.edge.lower'></span>

<h3>Description</h3>

<p>Lower limit of the confidence of an edge.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>conf.edge.lower(p)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Lower+2B20limit+2B20of+2B20the+2B20confidence+2B20of+2B20an+2B20edge_+3A_p">p</code></td>
<td>

<p>A numerical vector with the proportion of times an edge was found in the bootstrapped PC algorithm or the confidence of the edge returned by <code><a href="#topic+bn.skel.utils2">bn.skel.utils2</a></code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>After having performed PC algorithm many times in the bootstrap samples (using <code><a href="#topic+pc.skel.boot">pc.skel.boot</a></code> for example) you get a symmetric matrix with the proportion of times an edge was discovered. Take the lower (or upper) triangular elements of that matrix and pass them as input in this function. This will tell you the minimum proportion required to be confident that an edge is trully significant.
</p>


<h3>Value</h3>

<p>The estimated cutoff limit above which an edge can be deemed significant. 
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Scutari M. and Nagarajan R. (2013). Identifying significant edges in graphical models of molecular networks. Artifficial Intelligence in Medicine, 57: 207-217.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+pc.skel.boot">pc.skel.boot</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rdag2(200, p = 40, nei = 3)
x &lt;- y$x
g &lt;- pc.skel.boot(x, R = 199)$Gboot
a &lt;- g[ lower.tri(g) ]
conf.edge.lower(a)
</code></pre>

<hr>
<h2 id='mammpc.output-class'>Class <code>"mammpc.output"</code></h2><span id='topic+mammpc.output-class'></span><span id='topic+mammpc.output'></span><span id='topic+plot+2Cmammpc.output-method'></span><span id='topic+mammpc.output-method'></span><span id='topic+plot+2Cmammpc.output+2CANY-method'></span>

<h3>Description</h3>

<p>mammpc. output object class.
</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("mammpc.output", ...)</code>.
</p>


<h3>Slots</h3>


<dl>
<dt><code>selectedVars</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>selectedVarsOrder</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>hashObject</code>:</dt><dd><p>Object of class <code>"list"</code></p>
</dd>
<dt><code>pvalues</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>stats</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>univ</code>:</dt><dd><p>Object of class <code>"list"</code></p>
</dd>
<dt><code>max_k</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>threshold</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>test</code>:</dt><dd><p>Object of class <code>"character"</code></p>
</dd>
<dt><code>runtime</code>:</dt><dd><p>Object of class <code>"proc_time"</code></p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>plot</dt><dd><p><code>plot(x = "mammpc.output", mode = "all")</code>: Generic function for plotting the generated pvalues of the MMPCoutput object. Argument mode = &quot;all&quot; for plotting all the pvalues or mode=&quot;partial&quot; for partial plotting the first 500 pvalues </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Giorgos Athineou &lt;athineou@csd.uoc.gr&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ma.mmpc">ma.mmpc</a>, <a href="#topic+ma.ses">ma.ses</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("mammpc.output")
</code></pre>

<hr>
<h2 id='Many+20approximate+20simple+20logistic+20regressions'>
Many approximate simple logistic regressions.
</h2><span id='topic+sp.logiregs'></span>

<h3>Description</h3>

<p>Many approximate simple logistic regressions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sp.logiregs(target, dataset, logged = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Many+2B20approximate+2B20simple+2B20logistic+2B20regressions_+3A_target">target</code></td>
<td>

<p>The dependent variable, a numerical vector with 0s or 1s.
</p>
</td></tr>
<tr><td><code id="Many+2B20approximate+2B20simple+2B20logistic+2B20regressions_+3A_dataset">dataset</code></td>
<td>

<p>A matrix with the indendent variables.
</p>
</td></tr>
<tr><td><code id="Many+2B20approximate+2B20simple+2B20logistic+2B20regressions_+3A_logged">logged</code></td>
<td>

<p>Should the p-values be returned (FALSE) or their logarithm (TRUE)?
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Many simple approximate logistic regressions are performed and hypothesis testing
for the singificance of each coefficient is returned. The code is available in the paper by
Sikorska et al. (2013). We simply took the code and made some minor modifications. The explanation
and the motivation can be found in their paper. They call it semi-parallel logistic regressions, hence 
we named the function sp.logiregs. 
</p>


<h3>Value</h3>

<p>A two-column matrix with the test statistics (Wald statistic) and their 
associated p-values (or their loggarithm).
</p>


<h3>Author(s)</h3>

<p>Initial author Karolina Sikorska in the above reference paper. Modifications by Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Karolina Sikorska, Emmanuel Lesaffre, Patrick FJ Groenen and Paul HC Eilers (2013), 14:166.
GWAS on your notebook: fast semi-parallel linear and logistic regression for genome-wide
association studies. 
<a href="https://bmcbioinformatics.biomedcentral.com/track/pdf/10.1186/1471-2105-14-166.pdf">https://bmcbioinformatics.biomedcentral.com/track/pdf/10.1186/1471-2105-14-166.pdf</a>
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+logiquant.regs">logiquant.regs</a>, <a href="Rfast2.html#topic+bic.regs">bic.regs</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rbinom(200, 1, 0.5)
x &lt;- matrix( rnorm(200 * 30), ncol = 30 )
a &lt;- MXM::sp.logiregs(y, x)
</code></pre>

<hr>
<h2 id='Many+20simple+20beta+20regressions'>
Many simple beta regressions.
</h2><span id='topic+beta.regs'></span><span id='topic+perm.betaregs'></span><span id='topic+wald.betaregs'></span>

<h3>Description</h3>

<p>Many simple beta regressions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>beta.regs(target, dataset, wei = NULL, check = FALSE, logged = FALSE, ncores = 1) 

perm.betaregs(target, dataset, wei = NULL, check = FALSE, logged = FALSE, 
threshold = 0.05, R = 999, ncores = 1)

wald.betaregs(target, dataset, wei = NULL, check = FALSE, logged = FALSE, ncores = 1) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Many+2B20simple+2B20beta+2B20regressions_+3A_target">target</code></td>
<td>

<p>The target (dependent) variable. It must be a numerical vector with integers. 
</p>
</td></tr>
<tr><td><code id="Many+2B20simple+2B20beta+2B20regressions_+3A_dataset">dataset</code></td>
<td>

<p>The indendent variable(s). It can be a vector, a matrix or a dataframe with continuous only variables, a data frame with mixed or only categorical variables.
</p>
</td></tr>
<tr><td><code id="Many+2B20simple+2B20beta+2B20regressions_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. An example where weights are used is surveys when stratified sampling has occured. 
</p>
</td></tr>
<tr><td><code id="Many+2B20simple+2B20beta+2B20regressions_+3A_check">check</code></td>
<td>

<p>A boolean variable indicating whether to check for variables with identical values. The default is FALSE.
</p>
</td></tr>
<tr><td><code id="Many+2B20simple+2B20beta+2B20regressions_+3A_logged">logged</code></td>
<td>

<p>A boolean variable; it will return the logarithm of the p-value if set to TRUE.
</p>
</td></tr>
<tr><td><code id="Many+2B20simple+2B20beta+2B20regressions_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0, 1)) for assessing p-values significance. Default value is 0.05.
</p>
</td></tr>
<tr><td><code id="Many+2B20simple+2B20beta+2B20regressions_+3A_r">R</code></td>
<td>

<p>The number of permutations, set to 999 by default. There is a trick to avoind doing all permutations. As soon as the number of times the permuted test statistic is more than the observed test statistic is more than 50 (in this example case), the p-value has exceeded the signifiance level (threshold value) and hence the predictor variable is not significant. There is no need to continue do the extra permutations, as a decision has already been made. 
</p>
</td></tr>
<tr><td><code id="Many+2B20simple+2B20beta+2B20regressions_+3A_ncores">ncores</code></td>
<td>

<p>The number of cores to use. The default value is 1.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Many simple beta regressions are fitted. 
</p>


<h3>Value</h3>

<p>A matrix with the test statistic values, their relevant (<b>logged</b>) p-values and the BIC values. 
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Ferrari S.L.P. and Cribari-Neto F. (2004). Beta Regression for Modelling Rates and Proportions. Journal of Applied Statistics, 31(7): 799-815.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+beta.mod">beta.mod</a>, <a href="#topic+testIndBeta">testIndBeta</a>, <a href="#topic+reg.fit">reg.fit</a>, <a href="#topic+ridge.reg">ridge.reg</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rbeta(80, 5, 3)
x &lt;- matrix( rnorm(80 * 7), ncol = 7)
a1 &lt;- beta.regs(y, x)
a2 &lt;- perm.betaregs(y, x[, 1:4], R = 299)
</code></pre>

<hr>
<h2 id='Many+20simple+20quantile+20regressions+20using+20logistic+20regressions'>
Many simple quantile regressions using logistic regressions.
</h2><span id='topic+logiquant.regs'></span>

<h3>Description</h3>

<p>Many simple quantile regressions using logistic regressions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>logiquant.regs(target, dataset, logged = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Many+2B20simple+2B20quantile+2B20regressions+2B20using+2B20logistic+2B20regressions_+3A_target">target</code></td>
<td>

<p>The dependent variable, a numerical vector.
</p>
</td></tr>
<tr><td><code id="Many+2B20simple+2B20quantile+2B20regressions+2B20using+2B20logistic+2B20regressions_+3A_dataset">dataset</code></td>
<td>

<p>A matrix with the indendent variables.
</p>
</td></tr>
<tr><td><code id="Many+2B20simple+2B20quantile+2B20regressions+2B20using+2B20logistic+2B20regressions_+3A_logged">logged</code></td>
<td>

<p>Should the p-values be returned (FALSE) or their logarithm (TRUE)?
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Instead of fitting quantile regression models, one for each predictor variable and trying to assess its significance, 
Redden et al. (2004) proposed a simple singificance test based on logistic regression. 
Create an indicator variable I where 1 indicates a response value above its median and 0 elsewhere. 
Since I is binary, perform logistic regression for the predictor and assess its significance using the likelihood 
ratio test. We perform many logistic regression models since we have many predictors whose univariate association with the 
response variable we want to test.
</p>


<h3>Value</h3>

<p>A two-column matrix with the test statistics (likelihood ratio test statistic) and their 
associated p-values (or their loggarithm).
</p>


<h3>Author(s)</h3>

<p>Author: Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>David T. Redden, Jose R. Fernandez and David B. Allison (2004). A simple significance test for quantile regression.
Statistics in Medicine, 23(16): 2587-2597 
</p>


<h3>See Also</h3>

<p><code> <a href="Rfast2.html#topic+bic.regs">bic.regs</a>, <a href="#topic+sp.logiregs">sp.logiregs</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rcauchy(100, 3, 2)
x &lt;- matrix( rnorm(100 * 50), ncol = 50 )
a &lt;- MXM::logiquant.regs(y, x)
</code></pre>

<hr>
<h2 id='Many+20simple+20zero+20inflated+20Poisson+20regressions'>
Many simple zero inflated Poisson regressions.
</h2><span id='topic+zip.regs'></span><span id='topic+perm.zipregs'></span><span id='topic+wald.zipregs'></span>

<h3>Description</h3>

<p>Many simple zero inflated Poisson regressions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>zip.regs(target, dataset, wei = NULL, check = FALSE, logged = FALSE, ncores = 1) 

perm.zipregs(target, dataset, wei = NULL, check = FALSE, logged = FALSE, R = 999, 
threshold = 0.05, ncores = 1)

wald.zipregs(target, dataset, wei = NULL, check = FALSE, logged = FALSE, ncores = 1) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Many+2B20simple+2B20zero+2B20inflated+2B20Poisson+2B20regressions_+3A_target">target</code></td>
<td>

<p>The target (dependent) variable. It must be a numerical vector with integers. 
</p>
</td></tr>
<tr><td><code id="Many+2B20simple+2B20zero+2B20inflated+2B20Poisson+2B20regressions_+3A_dataset">dataset</code></td>
<td>

<p>The indendent variable(s). It can be a vector, a matrix or a dataframe with continuous only variables, a data frame with mixed or only categorical variables.
</p>
</td></tr>
<tr><td><code id="Many+2B20simple+2B20zero+2B20inflated+2B20Poisson+2B20regressions_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. An example where weights are used is surveys when stratified sampling has occured. 
</p>
</td></tr>
<tr><td><code id="Many+2B20simple+2B20zero+2B20inflated+2B20Poisson+2B20regressions_+3A_check">check</code></td>
<td>

<p>A boolean variable indicating whether to chekc for variables with identical values. The defauls is FALSE.
</p>
</td></tr>
<tr><td><code id="Many+2B20simple+2B20zero+2B20inflated+2B20Poisson+2B20regressions_+3A_logged">logged</code></td>
<td>

<p>A boolean variable; it will return the logarithm of the pvalue if set to TRUE.
</p>
</td></tr>
<tr><td><code id="Many+2B20simple+2B20zero+2B20inflated+2B20Poisson+2B20regressions_+3A_r">R</code></td>
<td>

<p>The number of permutations, set to 999 by default. There is a trick to avoind doing all permutations. As soon as the number of times the permuted test statistic is more than the observed test statistic is more than 50 (in this example case), the p-value has exceeded the signifiance level (threshold value) and hence the predictor variable is not significant. There is no need to continue do the extra permutations, as a decision has already been made. 
</p>
</td></tr>
<tr><td><code id="Many+2B20simple+2B20zero+2B20inflated+2B20Poisson+2B20regressions_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0, 1)) for assessing p-values significance. Default value is 0.05.
</p>
</td></tr>
<tr><td><code id="Many+2B20simple+2B20zero+2B20inflated+2B20Poisson+2B20regressions_+3A_ncores">ncores</code></td>
<td>

<p>The number of cores to use. The default value is 1.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Many simple zero inflated Poisson regressions are fitted. The permutations based approach may cause some errors sometimes, and this is due to the nature of the distribution and its maximisation process. &quot;nlm&quot; and &quot;optim&quot; are used internally. 
</p>


<h3>Value</h3>

<p>A matrix with the test statistic values, their relevant (<b>logged</b>) p-values and the BIC values. 
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Lambert D. (1992). Zero-inflated Poisson regression, with an application to defects in manufacturing. Technometrics, 34(1):1-14.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+zip.mod">zip.mod</a>, <a href="#topic+testIndZIP">testIndZIP</a>, <a href="#topic+reg.fit">reg.fit</a>, <a href="#topic+ridge.reg">ridge.reg</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rpois(50, 3)
x &lt;- matrix( rnorm(50 * 7), ncol = 7)
y[1:10] &lt;- 0
a1 &lt;- zip.regs(y, x)
a2 &lt;- perm.zipregs(y, x[, 1:3], R = 299)
</code></pre>

<hr>
<h2 id='Many+20Wald+20based+20tests+20for+20logistic+20and+20Poisson+20regressions+20with+20continuous+20predictors'>
Many Wald based tests for logistic and Poisson regressions with continuous predictors
</h2><span id='topic+wald.logisticregs'></span><span id='topic+wald.poissonregs'></span>

<h3>Description</h3>

<p>Many Wald based tests for logistic and Poisson regressions with continuous predictors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wald.logisticregs(y, x, tol = 1e-09, wei = NULL, check = FALSE, logged = FALSE, 
ncores = 1)  
wald.poissonregs(y, x, tol = 1e-09, wei = NULL, check = FALSE, logged = FALSE, 
ncores = 1)  
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Many+2B20Wald+2B20based+2B20tests+2B20for+2B20logistic+2B20and+2B20Poisson+2B20regressions+2B20with+2B20continuous+2B20predictors_+3A_y">y</code></td>
<td>

<p>A vector with either 0s and 1 (logistic regression) or discrete data, counts (Poisson regression).
</p>
</td></tr>
<tr><td><code id="Many+2B20Wald+2B20based+2B20tests+2B20for+2B20logistic+2B20and+2B20Poisson+2B20regressions+2B20with+2B20continuous+2B20predictors_+3A_x">x</code></td>
<td>

<p>A data.frame, the predictor variables. If you have no categorical variables, the fucntion will still work but it's better to use the <code><a href="Rfast.html#topic+score.glms">score.glms</a></code> because it is faster. 
</p>
</td></tr>
<tr><td><code id="Many+2B20Wald+2B20based+2B20tests+2B20for+2B20logistic+2B20and+2B20Poisson+2B20regressions+2B20with+2B20continuous+2B20predictors_+3A_tol">tol</code></td>
<td>

<p>The tolerance value to stop the Newton-Raphson iterations. It is set to 1e-09 by default. 
</p>
</td></tr>
<tr><td><code id="Many+2B20Wald+2B20based+2B20tests+2B20for+2B20logistic+2B20and+2B20Poisson+2B20regressions+2B20with+2B20continuous+2B20predictors_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. An example where weights are used is surveys when stratified sampling has occured. 
</p>
</td></tr>
<tr><td><code id="Many+2B20Wald+2B20based+2B20tests+2B20for+2B20logistic+2B20and+2B20Poisson+2B20regressions+2B20with+2B20continuous+2B20predictors_+3A_check">check</code></td>
<td>

<p>A boolean variable indicating whether to chekc for variables with identical values. The defauls is FALSE.
</p>
</td></tr>
<tr><td><code id="Many+2B20Wald+2B20based+2B20tests+2B20for+2B20logistic+2B20and+2B20Poisson+2B20regressions+2B20with+2B20continuous+2B20predictors_+3A_logged">logged</code></td>
<td>

<p>A boolean variable; it will return the logarithm of the pvalue if set to TRUE.
</p>
</td></tr>
<tr><td><code id="Many+2B20Wald+2B20based+2B20tests+2B20for+2B20logistic+2B20and+2B20Poisson+2B20regressions+2B20with+2B20continuous+2B20predictors_+3A_ncores">ncores</code></td>
<td>

<p>How many to cores to useq the default value is 1. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Instead of using R built-in function <code><a href="stats.html#topic+glm">glm</a></code> we implemented the newton-Raphson in order to avoid unnecessary calculations. The functions are much faster.  
</p>


<h3>Value</h3>

<p>A matrix with three columns, the test statistic, its associated (<b>logged</b>) p-value and the BIC of each model.   
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a> and Manos Papadakis &lt;papadakm95@gmail.com&gt;.
</p>


<h3>References</h3>

<p>Draper, N.R. and Smith H. (1988). Applied regression analysis. New York, Wiley, 3rd edition.
</p>
<p>McCullagh, Peter, and John A. Nelder. Generalized linear models.  CRC press, USA, 2nd edition, 1989.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+univregs">univregs</a>, <a href="#topic+perm.univregs">perm.univregs</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## 20 variables, hence 20 univariate regressions are to be fitted
x &lt;- matrix( rnorm(200 * 20), ncol = 20 )
y &lt;- rpois(200, 4)
a &lt;- wald.poissonregs(y, x)
b &lt;- univregs(y, x, test = testIndPois)
cor(exp(a[, 2]), exp(b$pvalue) )
</code></pre>

<hr>
<h2 id='Markov+20Blanket+20of+20a+20node+20in+20a+20directed+20graph'>
Returns the Markov blanket of a node (or variable)
</h2><span id='topic+mb'></span>

<h3>Description</h3>

<p>Returns the Markov blanket of a node (or variable).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mb(G, node)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Markov+2B20Blanket+2B20of+2B20a+2B20node+2B20in+2B20a+2B20directed+2B20graph_+3A_g">G</code></td>
<td>

<p>The graph matrix as produced from <code><a href="#topic+pc.or">pc.or</a></code> or any other algorithm which produces directed graphs. 
</p>
</td></tr>
<tr><td><code id="Markov+2B20Blanket+2B20of+2B20a+2B20node+2B20in+2B20a+2B20directed+2B20graph_+3A_node">node</code></td>
<td>

<p>A vector with one or more numbers indicating the seleted node(s) (or variable(s)). 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a way to see the network for some given nodes. It is useful if you have many nodes and the whole network is a bit difficult to see clearly. 
Bear in mind that the values can be extracted with the $ symbol, i.e. this is an S3 class output. 
</p>


<h3>Value</h3>

<table>
<tr><td><code>parents</code></td>
<td>

<p>The parents of the node of interest.
</p>
</td></tr>
<tr><td><code>children</code></td>
<td>

<p>The children of the node of interest.
</p>
</td></tr>
<tr><td><code>spouses</code></td>
<td>

<p>The spouses of the node of interest. These are the other parents of the children of the node of interest. 
</p>
</td></tr>
<tr><td><code>relatives</code></td>
<td>

<p>Nodes which are connected with the node of interest, but it is not known whether they are parents or children. The edge between them is undirected.
</p>
</td></tr>
<tr><td><code>markov.blanket</code></td>
<td>

<p>The Markov blanket of the node of interest. The collection of all the previous.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+plotnetwork">plotnetwork</a>, <a href="#topic+nei">nei</a>, <a href="#topic+pc.or">pc.or</a> 
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate a dataset with continuous data
# simulate a dataset with continuous data
y &lt;- rdag(1000, 10, 0.3)
tru &lt;- y$G 
x &lt;- y$x
mod &lt;- pc.con(x)
G &lt;- pc.or(mod)$G
plotnetwork(G)
dev.new()
mb(G, 8)
</code></pre>

<hr>
<h2 id='mases.output-class'>Class <code>"mases.output"</code></h2><span id='topic+mases.output-class'></span><span id='topic+mases.output'></span><span id='topic+plot+2Cmases.output-method'></span><span id='topic+mases.output-method'></span><span id='topic+plot+2Cmases.output+2CANY-method'></span>

<h3>Description</h3>

<p>Meta analytic SES output object class.
</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("mases.output", ...)</code>.
</p>


<h3>Slots</h3>


<dl>
<dt><code>selectedVars</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>selectedVarsOrder</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>queues</code>:</dt><dd><p>Object of class <code>"list"</code></p>
</dd>
<dt><code>signatures</code>:</dt><dd><p>Object of class <code>"matrix"</code></p>
</dd>
<dt><code>hashObject</code>:</dt><dd><p>Object of class <code>"list"</code></p>
</dd>
<dt><code>pvalues</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>stats</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>univ</code>:</dt><dd><p>Object of class <code>"list"</code></p>
</dd>
<dt><code>max_k</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>threshold</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>runtime</code>:</dt><dd><p>Object of class <code>"proc_time"</code></p>
</dd>
<dt><code>test</code>:</dt><dd><p>Object of class <code>"character"</code></p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>plot</dt><dd><p><code>plot(x = "mases.output", mode = "all")</code>: Generic function for plotting the generated pvalues of the mases.output object. Argument mode = &quot;all&quot; for plotting all the pvalues or mode=&quot;partial&quot; for partial plotting the first 500 pvalues </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Giorgos Athineou &lt;athineou@csd.uoc.gr&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ma.ses">ma.ses</a>, <a href="#topic+ma.mmpc">ma.mmpc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("mases.output")
</code></pre>

<hr>
<h2 id='MMPC+20solution+20paths+20for+20many+20combinations+20of+20hyper-parameters'>
MMPC solution paths for many combinations of hyper-parameters
</h2><span id='topic+mmpc.path'></span><span id='topic+wald.mmpc.path'></span><span id='topic+perm.mmpc.path'></span>

<h3>Description</h3>

<p>MMPC solution paths for many combinations of hyper-parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mmpc.path(target, dataset, wei = NULL, max_ks = NULL, alphas = NULL, test = NULL,
user_test = NULL, ncores = 1)

wald.mmpc.path(target, dataset, wei = NULL, max_ks = NULL, alphas = NULL, test = NULL,
user_test = NULL, ncores = 1)

perm.mmpc.path(target, dataset, wei = NULL, max_ks = NULL, alphas = NULL, test = NULL,
user_test = NULL, R = 999, ncores = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MMPC+2B20solution+2B20paths+2B20for+2B20many+2B20combinations+2B20of+2B20hyper-parameters_+3A_target">target</code></td>
<td>

<p>The class variable. Provide either a string, an integer, a numeric value, a vector, a factor, an ordered factor or a Surv object. See also Details.
</p>
</td></tr>
<tr><td><code id="MMPC+2B20solution+2B20paths+2B20for+2B20many+2B20combinations+2B20of+2B20hyper-parameters_+3A_dataset">dataset</code></td>
<td>

<p>The dataset; provide either a data frame or a matrix (columns = variables , rows = samples).
Alternatively, provide an ExpressionSet (in which case rows are samples and columns are features, see bioconductor for details).
</p>
</td></tr>
<tr><td><code id="MMPC+2B20solution+2B20paths+2B20for+2B20many+2B20combinations+2B20of+2B20hyper-parameters_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. An example where weights are used is surveys when stratified sampling has occured. 
</p>
</td></tr>
<tr><td><code id="MMPC+2B20solution+2B20paths+2B20for+2B20many+2B20combinations+2B20of+2B20hyper-parameters_+3A_max_ks">max_ks</code></td>
<td>

<p>A vector of possible max_k values. Can be a number as well, but this does not really make sense to do. If nothing is given, the values max_k = (4,3,2) are used by default. 
</p>
</td></tr>
<tr><td><code id="MMPC+2B20solution+2B20paths+2B20for+2B20many+2B20combinations+2B20of+2B20hyper-parameters_+3A_alphas">alphas</code></td>
<td>

<p>A vector of possible threshold values. Can be a number as well, but this does not really make sense to do. If nothing is given, the values (0.1, 0.05, 0.01) are used by default.  
</p>
</td></tr>
<tr><td><code id="MMPC+2B20solution+2B20paths+2B20for+2B20many+2B20combinations+2B20of+2B20hyper-parameters_+3A_test">test</code></td>
<td>

<p>The conditional independence test to use. Default value is NULL. See also <code><a href="#topic+CondIndTests">CondIndTests</a></code>.
</p>
</td></tr>
<tr><td><code id="MMPC+2B20solution+2B20paths+2B20for+2B20many+2B20combinations+2B20of+2B20hyper-parameters_+3A_user_test">user_test</code></td>
<td>

<p>A user-defined conditional independence test (provide a closure type object). Default value is NULL. If this is defined, the &quot;test&quot; argument is ignored.
</p>
</td></tr>
<tr><td><code id="MMPC+2B20solution+2B20paths+2B20for+2B20many+2B20combinations+2B20of+2B20hyper-parameters_+3A_r">R</code></td>
<td>

<p>The number of permutations, set to 999 by default. There is a trick to avoind doing all permutations. As soon as the number of times the permuted test statistic is more than the observed test statistic is more than 50 (if threshold = 0.05 and R = 999), the p-value has exceeded the signifiance level (threshold value) and hence the predictor variable is not significant. There is no need to continue do the extra permutations, as a decision has already been made. 
</p>
</td></tr>
<tr><td><code id="MMPC+2B20solution+2B20paths+2B20for+2B20many+2B20combinations+2B20of+2B20hyper-parameters_+3A_ncores">ncores</code></td>
<td>

<p>How many cores to use. This plays an important role if you have tens of thousands of variables or really large sample sizes and tens of thousands of variables and a regression based test which requires numerical optimisation. In other cases it will not make a difference in the overall time (in fact it can be slower). The parallel computation is used in the first step of the algorithm, where univariate associations are examined, those take place in parallel. We have seen a reduction in time of 50% with 4 cores in comparison to 1 core. Note also, that the amount of reduction is not linear in the number of cores. This argument is used only in the first run of MMPC and for the univariate associations only and the results are stored (hashed). In the enxt runs of MMPC the results are used (cashed) and so the process is faster. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For different combinations of the hyper-parameters, max_k and the significance level (threshold or alpha) the MMPC algorith is run. 
</p>


<h3>Value</h3>

<p>The output of the algorithm is an object of the class 'SESoutput' for SES or 'MMPCoutput' for MMPC including:
</p>
<table>
<tr><td><code>bic</code></td>
<td>

<p>A matrix with the BIC values of the final fitted model based on the selected variables identified by each configuration, combination of the hyper-parameters.
</p>
</td></tr>
<tr><td><code>size</code></td>
<td>

<p>A matrix with the legnth of the selected variables identified by each configuration of MMPC.
</p>
</td></tr>
<tr><td><code>variables</code></td>
<td>

<p>A list containing the variables from each configuration of MMPC
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The run time of the algorithm. A numeric vector. The first element is the user time, the second element is the system time and the third element is the elapsed time.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ioannis Tsamardinos, Vincenzo Lagani
</p>
<p>R implementation and documentation: Giorgos Athineou &lt;athineou@csd.uoc.gr&gt; Vincenzo Lagani &lt;vlagani@csd.uoc.gr&gt;  
</p>


<h3>References</h3>

<p>Tsamardinos, Brown and Aliferis (2006). The max-min hill-climbing Bayesian network structure learning algorithm. 
Machine learning, 65(1): 31-78.
</p>




<h3>See Also</h3>

<p><code><a href="#topic+CondIndTests">CondIndTests</a>, <a href="#topic+cv.ses">cv.ses</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)
# simulate a dataset with continuous data
dataset &lt;- matrix(runif(500 * 51, 1, 100), nrow = 500 ) 
#the target feature is the last column of the dataset as a vector
target &lt;- dataset[, 51]
dataset &lt;- dataset[, -51]

a &lt;- mmpc.path(target, dataset, max_ks = NULL, alphas = NULL, test = NULL, 
user_test = NULL, ncores = 1)
</code></pre>

<hr>
<h2 id='MMPC.gee.output-class'>Class <code>"MMPC.gee.output"</code></h2><span id='topic+MMPC.gee.output-class'></span><span id='topic+MMPC.gee.output'></span><span id='topic+plot+2CMMPC.gee.output-method'></span><span id='topic+MMPC.gee.output-method'></span><span id='topic+plot+2CMMPC.gee.output+2CANY-method'></span>

<h3>Description</h3>

<p>MMPC.glmm output object class.
</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("MMPC.gee.output", ...)</code>.
</p>


<h3>Slots</h3>


<dl>
<dt><code>selectedVars</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>selectedVarsOrder</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>hashObject</code>:</dt><dd><p>Object of class <code>"list"</code></p>
</dd>
<dt><code>pvalues</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>stats</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>univ</code>:</dt><dd><p>Object of class <code>"list"</code></p>
</dd>
<dt><code>max_k</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>threshold</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>n.tests</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>runtime</code>:</dt><dd><p>Object of class <code>"proc_time"</code></p>
</dd>
<dt><code>test</code>:</dt><dd><p>Object of class <code>"character"</code></p>
</dd>
<dt><code>correl</code>:</dt><dd><p>Object of class <code>"character"</code></p>
</dd>  
<dt><code>se</code>:</dt><dd><p>Object of class <code>"character"</code></p>
</dd>  </dl>



<h3>Methods</h3>


<dl>
<dt>plot</dt><dd><p><code>plot(x = "MMPC.gee.output", mode = "all")</code>: Generic function for plotting the generated pvalues of the MMPC.glmm.output object. Argument mode = &quot;all&quot; for plotting all the pvalues or mode=&quot;partial&quot; for partial plotting the first 500 pvalues </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Giorgos Athineou and Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+MMPC.gee">MMPC.gee</a>, <a href="#topic+SES.gee">SES.gee</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("MMPC.gee.output")
</code></pre>

<hr>
<h2 id='MMPC.glmm.output-class'>Class <code>"MMPC.glmm.output"</code></h2><span id='topic+MMPC.glmm.output-class'></span><span id='topic+MMPC.glmm.output'></span><span id='topic+plot+2CMMPC.glmm.output-method'></span><span id='topic+MMPC.glmm.output-method'></span><span id='topic+plot+2CMMPC.glmm.output+2CANY-method'></span>

<h3>Description</h3>

<p>MMPC.glmm output object class.
</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("MMPC.glmm.output", ...)</code>.
</p>


<h3>Slots</h3>


<dl>
<dt><code>selectedVars</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>selectedVarsOrder</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>hashObject</code>:</dt><dd><p>Object of class <code>"list"</code></p>
</dd>
<dt><code>pvalues</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>stats</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>univ</code>:</dt><dd><p>Object of class <code>"list"</code></p>
</dd>
<dt><code>max_k</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>threshold</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>n.tests</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>runtime</code>:</dt><dd><p>Object of class <code>"proc_time"</code></p>
</dd>
<dt><code>test</code>:</dt><dd><p>Object of class <code>"character"</code></p>
</dd>
<dt><code>slope</code>:</dt><dd><p>Object of class <code>"logical"</code></p>
</dd>  </dl>



<h3>Methods</h3>


<dl>
<dt>plot</dt><dd><p><code>plot(x = "MMPC.glmm.output", mode = "all")</code>: Generic function for plotting the generated pvalues of the MMPC.glmm.output object. Argument mode = &quot;all&quot; for plotting all the pvalues or mode=&quot;partial&quot; for partial plotting the first 500 pvalues </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Giorgos Athineou &lt;athineou@csd.uoc.gr&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+MMPC.glmm">MMPC.glmm</a>, <a href="#topic+SES.glmm">SES.glmm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("MMPC.glmm.output")
</code></pre>

<hr>
<h2 id='MMPCoutput-class'>Class <code>"MMPCoutput"</code></h2><span id='topic+MMPCoutput-class'></span><span id='topic+MMPCoutput'></span><span id='topic+plot+2CMMPCoutput-method'></span><span id='topic+MMPCoutput-method'></span><span id='topic+plot+2CMMPCoutput+2CANY-method'></span>

<h3>Description</h3>

<p>MMPC output object class.
</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("MMPCoutput", ...)</code>.
</p>


<h3>Slots</h3>


<dl>
<dt><code>selectedVars</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>selectedVarsOrder</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>hashObject</code>:</dt><dd><p>Object of class <code>"list"</code></p>
</dd>
<dt><code>pvalues</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>stats</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>univ</code>:</dt><dd><p>Object of class <code>"list"</code></p>
</dd>
<dt><code>max_k</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>threshold</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>n.tests</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>runtime</code>:</dt><dd><p>Object of class <code>"proc_time"</code></p>
</dd>
<dt><code>test</code>:</dt><dd><p>Object of class <code>"character"</code></p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>plot</dt><dd><p><code>plot(x = "MMPCoutput", mode = "all")</code>: Generic function for plotting the generated pvalues of the MMPCoutput object. Argument mode = &quot;all&quot; for plotting all the pvalues or mode=&quot;partial&quot; for partial plotting the first 500 pvalues </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Giorgos Athineou &lt;athineou@csd.uoc.gr&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+MMPC">MMPC</a>, <a href="#topic+SES">SES</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("MMPCoutput")
</code></pre>

<hr>
<h2 id='MXM-internal'> Internal MXM Functions </h2><span id='topic+InternalSES'></span><span id='topic+InternalMMPC'></span><span id='topic+Internalmases'></span><span id='topic+Internalmammpc'></span><span id='topic+IdentifyEquivalence'></span><span id='topic+IdentifyEquivalence.ma'></span><span id='topic+apply_ideq'></span><span id='topic+apply_ideq.ma'></span><span id='topic+compare_p_values'></span><span id='topic+identifyTheEquivalent'></span><span id='topic+identifyTheEquivalent.ma'></span><span id='topic+max_min_assoc'></span><span id='topic+max_min_assoc.ma'></span><span id='topic+min_assoc'></span><span id='topic+min_assoc.ma'></span><span id='topic+univariateScore'></span><span id='topic+univariateScore.ma'></span><span id='topic+condi.perm'></span><span id='topic+InternalSES.glmm'></span><span id='topic+InternalMMPC.glmm'></span><span id='topic+IdentifyEquivalence.glmm'></span><span id='topic+apply_ideq.glmm'></span><span id='topic+identifyTheEquivalent.glmm'></span><span id='topic+max_min_assoc.glmm'></span><span id='topic+min_assoc.glmm'></span><span id='topic+univariateScore.glmm'></span><span id='topic+is.sepset'></span><span id='topic+lm.fsreg_2'></span><span id='topic+glm.fsreg_2'></span><span id='topic+dag_to_eg'></span><span id='topic+nchoosek'></span><span id='topic+proc_time-class'></span><span id='topic+R0'></span><span id='topic+R1'></span><span id='topic+R2'></span><span id='topic+R3'></span><span id='topic+is.sepset'></span><span id='topic+regbeta'></span><span id='topic+regbetawei'></span><span id='topic+betamle.wei'></span><span id='topic+regzip'></span><span id='topic+regzipwei'></span><span id='topic+zipmle.wei'></span><span id='topic+zipwei'></span><span id='topic+bic.betafsreg'></span><span id='topic+bic.zipfsreg'></span><span id='topic+beta.fsreg'></span><span id='topic+zip.fsreg'></span><span id='topic+beta.bsreg'></span><span id='topic+zip.bsreg'></span><span id='topic+iamb.betabs'></span><span id='topic+iamb.zipbs'></span><span id='topic+iamb.glmbs'></span><span id='topic+internaliamb.binombs'></span><span id='topic+internaliamb.poisbs'></span><span id='topic+internaliamb.lmbs'></span><span id='topic+InternalMMPC.glmm'></span><span id='topic+InternalSES.glmm'></span><span id='topic+InternalSES'></span><span id='topic+InternalMMPC'></span><span id='topic+univariateScore'></span><span id='topic+perm.univariateScore'></span><span id='topic+max_min_assoc'></span><span id='topic+min_assoc'></span><span id='topic+nchoosek'></span><span id='topic+Internalmases'></span><span id='topic+compare_p_values'></span><span id='topic+perm.Internalmmpc'></span><span id='topic+wald.Internalmmpc'></span><span id='topic+wald.Internalses'></span><span id='topic+perm.IdentifyEquivalence'></span><span id='topic+perm.identifyTheEquivalent'></span><span id='topic+perm.apply_ideq'></span><span id='topic+IdentifyEquivalence'></span><span id='topic+apply_ideq'></span><span id='topic+identifyTheEquivalent'></span><span id='topic+cvses.par'></span><span id='topic+cvmmpc.par'></span><span id='topic+cvwaldses.par'></span><span id='topic+cvwaldmmpc.par'></span><span id='topic+cvpermses.par'></span><span id='topic+cvpermmmpc.par'></span><span id='topic+wald.univariateScore'></span><span id='topic+univariateScore.ma'></span><span id='topic+IdentifyEquivalence.ma'></span><span id='topic+identifyTheEquivalent.ma'></span><span id='topic+apply_ideq.ma'></span><span id='topic+max_min_assoc.ma'></span><span id='topic+min_assoc.ma'></span><span id='topic+fs.reg_2'></span><span id='topic+gammafsreg_2'></span><span id='topic+beta.fsreg_2'></span><span id='topic+zip.fsreg_2'></span><span id='topic+internaliamb.zipbs'></span><span id='topic+internaliamb.betabs'></span><span id='topic+iamb.gammabs'></span><span id='topic+iamb.normlogbs'></span><span id='topic+internaliamb.gammabs'></span><span id='topic+internaliamb.normlogbs'></span><span id='topic+bic.tobit.fsreg'></span><span id='topic+iamb.tobitbs'></span><span id='topic+internaliamb.tobitbs'></span><span id='topic+ebic.fbed.beta'></span><span id='topic+ebic.fbed.cr'></span><span id='topic+ebic.fbed.glm'></span><span id='topic+ebic.fbed.lm'></span><span id='topic+ebic.fbed.mmreg'></span><span id='topic+ebic.fbed.multinom'></span><span id='topic+ebic.fbed.nb'></span><span id='topic+ebic.fbed.ordinal'></span><span id='topic+ebic.fbed.tobit'></span><span id='topic+ebic.fbed.wr'></span><span id='topic+ebic.fbed.zip'></span><span id='topic+ebic.beta.bsreg'></span><span id='topic+ebic.cr.bsreg'></span><span id='topic+ebic.glm.bsreg'></span><span id='topic+ebic.lm.bsreg'></span><span id='topic+ebic.mm.bsreg'></span><span id='topic+ebic.multinom.bsreg'></span><span id='topic+ebic.ordinal.bsreg'></span><span id='topic+ebic.tobit.bsreg'></span><span id='topic+ebic.wr.bsreg'></span><span id='topic+ebic.zip.bsreg'></span><span id='topic+cat_condis'></span><span id='topic+pearson_condis'></span><span id='topic+pearson_condis.rob'></span><span id='topic+disctor_condis'></span><span id='topic+comb_condis'></span><span id='topic+fbed.glmm'></span><span id='topic+fbed.lmm'></span><span id='topic+ebic.fbed.glmm'></span><span id='topic+ebic.fbed.lmm'></span><span id='topic+lmm.bsreg'></span><span id='topic+clogit.fsreg'></span><span id='topic+clogit.fsreg_2'></span><span id='topic+bic.clogit.fsreg'></span><span id='topic+quasibinom.fsreg'></span><span id='topic+quasipois.fsreg'></span><span id='topic+quasibinom.fsreg_2'></span><span id='topic+quasipois.fsreg_2'></span><span id='topic+fbed.geeglm'></span><span id='topic+fbed.geeglm.reps'></span><span id='topic+fbed.ordgee'></span><span id='topic+internaliamb.mmbs'></span><span id='topic+iamb.gammabs'></span><span id='topic+iamb.normlogbs'></span><span id='topic+internaliamb.gammabs'></span><span id='topic+internaliamb.normlogbs'></span><span id='topic+bic.wr.fsreg'></span><span id='topic+wr.fsreg'></span><span id='topic+wr.fsreg_2'></span><span id='topic+ebicScore'></span><span id='topic+fbed.glmm.reps'></span><span id='topic+fbed.lmm.reps'></span><span id='topic+ebic.fbed.lmm.reps'></span><span id='topic+ebic.fbed.glmm.reps'></span><span id='topic+ebic.glmm.reps.bsreg'></span><span id='topic+fbed.ordgee.reps'></span><span id='topic+fbed.geelm'></span><span id='topic+fbed.geelm.reps'></span><span id='topic+univariateScore.gee'></span><span id='topic+InternalMMPC.gee'></span><span id='topic+InternalSES.gee'></span><span id='topic+max_min_assoc.gee'></span><span id='topic+min_assoc.gee'></span><span id='topic+IdentifyEquivalence.gee'></span><span id='topic+identifyTheEquivalent.gee'></span><span id='topic+univariateScore.timeclass'></span><span id='topic+InternalMMPC.timeclass'></span><span id='topic+gomp2'></span><span id='topic+kfbed.reg'></span><span id='topic+kfbed.glmm.reg'></span><span id='topic+kfbed.gee.reg'></span><span id='topic+bs.g2'></span><span id='topic+fbed.g2'></span><span id='topic+fbed.glmm.ordinal'></span><span id='topic+fbed.glmm.ordinal.reps'></span><span id='topic+ebic.fbed.glmm.ordinal'></span><span id='topic+ebic.fbed.glmm.ordinal.reps'></span><span id='topic+glmm.ordinal.bsreg'></span><span id='topic+glmm.ordinal.reps.bsreg'></span><span id='topic+ebic.glmm.reps.bsreg'></span><span id='topic+ebic.glmm.ordinal.reps.bsreg'></span><span id='topic+big.model'></span><span id='topic+big.fbed.g2'></span><span id='topic+big.bs.g2'></span><span id='topic+cvlogit.cv.ses'></span><span id='topic+bsreg.big'></span><span id='topic+fbed.glmm.cr'></span><span id='topic+ebic.fbed.glmm.cr'></span><span id='topic+glmm.cr.bsreg'></span><span id='topic+ebic.glmm.cr.bsreg'></span><span id='topic+test.maker'></span><span id='topic+ebic.model'></span><span id='topic+fbed.lr'></span><span id='topic+fbed.ebic'></span><span id='topic+spml.bsreg'></span><span id='topic+bic.llr.fsreg'></span><span id='topic+ebic.llr.bsreg'></span><span id='topic+llr.bsreg'></span><span id='topic+beta.reg'></span><span id='topic+ebic.spml.bsreg'></span><span id='topic+regzinb'></span><span id='topic+zinb.mle'></span><span id='topic+fbed.glmm.nb'></span><span id='topic+fbed.glmm.nb.reps'></span><span id='topic+glmm.nb.bsreg'></span><span id='topic+glmm.nb.reps.bsreg'></span>

<h3>Description</h3>

<p>Internal functions of Package <span class="pkg">MXM</span>
</p>


<h3>Details</h3>

<p>These functions are only for internal usage of the MXM package - NOT to be called by the user.
</p>


<h3>Functions</h3>


<ul>
<li><p> InternalSES( ... )
</p>
</li>
<li><p> InternalMMPC( ... )
</p>
</li>
<li><p> Internalmases( ... )
</p>
</li>
<li><p> Internalmammpc( ... )
</p>
</li>
<li><p> IdentifyEquivalence( ... )
</p>
</li>
<li><p> IdentifyEquivalence.ma( ... )
</p>
</li>
<li><p> apply_ideq( ... )
</p>
</li>
<li><p> apply_ideq.ma( ... )
</p>
</li>
<li><p> compare_p_values( ... )
</p>
</li>
<li><p> identifyTheEquivalent( ... )
</p>
</li>
<li><p> identifyTheEquivalent.ma( ... )
</p>
</li>
<li><p> max_min_assoc( ... )
</p>
</li>
<li><p> max_min_assoc.ma( ... )
</p>
</li>
<li><p> min_assoc( ... )
</p>
</li>
<li><p> min_assoc.ma( ... )
</p>
</li>
<li><p> univariateScore( ... )
</p>
</li>
<li><p> univariateScore.ma( ... )
</p>
</li>
<li><p> condi.perm( ... )
</p>
</li>
<li><p> InternalSES.glmm( ... )
</p>
</li>
<li><p> InternalMMPC.glmm( ... )
</p>
</li>
<li><p> IdentifyEquivalence.glmm( ... )
</p>
</li>
<li><p> apply_ideq.glmm( ... )
</p>
</li>
<li><p> identifyTheEquivalent.glmm( ... )
</p>
</li>
<li><p> max_min_assoc.glmm( ... )
</p>
</li>
<li><p> min_assoc.glmm( ... )
</p>
</li>
<li><p> univariateScore.glmm( ... )
</p>
</li>
<li><p> is.sepset( ... ) 
</p>
</li>
<li><p> lm.fsreg_2( ... )
</p>
</li>
<li><p> glm.fsreg_2( ... )
</p>
</li>
<li><p> dag_to_eg( ... )
</p>
</li>
<li><p> nchoosek( ... )
</p>
</li>
<li><p> R0( ... )
</p>
</li>
<li><p> R1( ... )
</p>
</li>
<li><p> R2( ... )
</p>
</li>
<li><p> R3( ... )
</p>
</li>
<li><p> is.sepset( ... )
</p>
</li>
<li><p> regbeta( ... )
</p>
</li>
<li><p> regbetawei( ... )
</p>
</li>
<li><p> betamle.wei( ... )
</p>
</li>
<li><p> regzip( ... )
</p>
</li>
<li><p> regzipawei( ... )
</p>
</li>
<li><p> zipmle.wei( ... )
</p>
</li>
<li><p> zipwei( ... )
</p>
</li>
<li><p> bic.betafsreg( ... )
</p>
</li>
<li><p> bic.zipfsreg( ... )
</p>
</li>
<li><p> beta.fsreg( ... )
</p>
</li>
<li><p> zip.fsreg( ... )
</p>
</li>
<li><p> beta.bsreg( ... )
</p>
</li>
<li><p> zip.bsreg( ... )
</p>
</li>
<li><p> iamb.betabs( ... )
</p>
</li>
<li><p> iamb.zipbs( ... )
</p>
</li>
<li><p> iamb.glmbs( ... )
</p>
</li>
<li><p> internaliamb.binombs( ... )
</p>
</li>
<li><p> internaliamb.poisbs( ... )
</p>
</li>
<li><p> internaliamb.lmbs( ... )
</p>
</li>
<li><p> InternalMMPC.glmm( ... )
</p>
</li>
<li><p> InternalSES.glmm( ... )
</p>
</li>
<li><p> InternalSES( ... )
</p>
</li>
<li><p> InternalMMPC( ... )
</p>
</li>
<li><p> univariateScore( ... )
</p>
</li>
<li><p> perm.univariateScore( ... )
</p>
</li>
<li><p> max_min_assoc( ... )
</p>
</li>
<li><p> min_assoc( ... )
</p>
</li>
<li><p> nchoosek( ... )
</p>
</li>
<li><p> Internalmases( ... )
</p>
</li>
<li><p> compare_p_values( ... )
</p>
</li>
<li><p> perm.Internalmmpc
</p>
</li>
<li><p> wald.Internalmmpc
</p>
</li>
<li><p> wald.Internalses
</p>
</li>
<li><p> perm.IdentifyEquivalence( ... )
</p>
</li>
<li><p> perm.identifyTheEquivalent( ... )
</p>
</li>
<li><p> perm.apply_ideq( ... )
</p>
</li>
<li><p> IdentifyEquivalence( ... )
</p>
</li>
<li><p> apply_ideq( ... )
</p>
</li>
<li><p> identifyTheEquivalent( ... )
</p>
</li>
<li><p> cvses.par( ... )
</p>
</li>
<li><p> cvmmpc.par( ... )
</p>
</li>
<li><p> cvwaldses.par( ... )
</p>
</li>
<li><p> cvwaldmmpc.par( ... )
</p>
</li>
<li><p> cvpermses.par( ... )
</p>
</li>
<li><p> cvpermmmpc.par( ... )
</p>
</li>
<li><p> wald.univariateScore( ... )
</p>
</li>
<li><p> univariateScore.ma( ... )
</p>
</li>
<li><p> IdentifyEquivalence.ma( ... )
</p>
</li>
<li><p> identifyTheEquivalent.ma( ... )
</p>
</li>
<li><p> apply_ideq.ma( ... )
</p>
</li>
<li><p> max_min_assoc.ma( ... )
</p>
</li>
<li><p> min_assoc.ma( ... )
</p>
</li>
<li><p> fs.reg_2( ... )
</p>
</li>
<li><p> gammafsreg_2( ... )
</p>
</li>
<li><p> beta.fsreg_2( ... )
</p>
</li>
<li><p> zip.fsreg_2( ... )
</p>
</li>
<li><p> internaliamb.zipbs( ... )
</p>
</li>
<li><p> internaliamb.betabs( ... )
</p>
</li>
<li><p> iamb.gammabs( ... )
</p>
</li>
<li><p> iamb.normlogbs( ... )
</p>
</li>
<li><p> internaliamb.gammabs( ... )
</p>
</li>
<li><p> internaliamb.normlogbs( ... )
</p>
</li>
<li><p> bic.tobit.fsreg( ... )
</p>
</li>
<li><p> iamb.tobitbs( ... )
</p>
</li>
<li><p> internaliamb.tobitbs( ... )
</p>
</li>
<li><p> ebic.fbed.beta( ... )
</p>
</li>
<li><p> ebic.fbed.cr( ... )
</p>
</li>
<li><p> ebic.fbed.glm( ... )
</p>
</li>
<li><p> ebic.fbed.lm( ... )
</p>
</li>
<li><p> ebic.fbed.mmreg( ... )
</p>
</li>
<li><p> ebic.fbed.multinom( ... )
</p>
</li>
<li><p> ebic.fbed.nb( ... )
</p>
</li>
<li><p> ebic.fbed.ordinal( ... )
</p>
</li>
<li><p> ebic.fbed.tobit( ... )
</p>
</li>
<li><p> ebic.fbed.wr( ... )
</p>
</li>
<li><p> ebic.fbed.zip( ... )
</p>
</li>
<li><p> ebic.beta.bsreg( ... )
</p>
</li>
<li><p> ebic.cr.bsreg( ... )
</p>
</li>
<li><p> ebic.glm.bsreg( ... )
</p>
</li>
<li><p> ebic.lm.bsreg( ... )
</p>
</li>
<li><p> ebic.mm.bsreg( ... )
</p>
</li>
<li><p> ebic.multinom.bsreg( ... )
</p>
</li>
<li><p> ebic.ordinal.bsreg( ... )
</p>
</li>
<li><p> ebic.tobit.bsreg( ... )
</p>
</li>
<li><p> ebic.wr.bsreg( ... )
</p>
</li>
<li><p> ebic.zip.bsreg( ... )
</p>
</li>
<li><p> cat_condis( ... )
</p>
</li>
<li><p> pearson_condis( ... )
</p>
</li>
<li><p> pearson_condis.rob( ... )
</p>
</li>
<li><p> disctor_condis( ... )
</p>
</li>
<li><p> comb_condis( ... )
</p>
</li>
<li><p> fbed.glmm( ... )
</p>
</li>
<li><p> fbed.lmm( ... )
</p>
</li>
<li><p> ebic.fbed.glmm( ... )
</p>
</li>
<li><p> ebic.fbed.lmm( ... )
</p>
</li>
<li><p> lmm.bsreg( ... )
</p>
</li>
<li><p> clogit.fsreg( ... )
</p>
</li>
<li><p> clogit.fsreg_2( ... )
</p>
</li>
<li><p> bic.clogit.fsreg( ... )
</p>
</li>
<li><p> quasibinom.fsreg( ... )
</p>
</li>
<li><p> quasipois.fsreg( ... )
</p>
</li>
<li><p> quasibinom.fsreg_2( ... )
</p>
</li>
<li><p> quasipois.fsreg_2( ... )
</p>
</li>
<li><p> fbed.geeglm( ... )
</p>
</li>
<li><p> fbed.geeglm.reps( ... )
</p>
</li>
<li><p> fbed.ordgee( ... )
</p>
</li>
<li><p> internaliamb.mmbs( ... )
</p>
</li>
<li><p> iamb.gammabs( ... )
</p>
</li>
<li><p> iamb.normlogbs( ... )
</p>
</li>
<li><p> internaliamb.gammabs( ... )
</p>
</li>
<li><p> internaliamb.normlogbs( ... )
</p>
</li>
<li><p> bic.wr.fsreg( ... )
</p>
</li>
<li><p> wr.fsreg( ... )
</p>
</li>
<li><p> wr.fsreg_2( ... )
</p>
</li>
<li><p> ebicScore( ... )
</p>
</li>
<li><p> fbed.glmm.reps( ... )
</p>
</li>
<li><p> fbed.lmm.reps( ... )
</p>
</li>
<li><p> ebic.fbed.lmm.reps( ... )
</p>
</li>
<li><p> ebic.fbed.glmm.reps( ... )
</p>
</li>
<li><p> ebic.glmm.reps.bsreg( ... )
</p>
</li>
<li><p> fbed.ordgee.reps( ... )
</p>
</li>
<li><p> fbed.geelm( ... )
</p>
</li>
<li><p> fbed.geelm.reps( ... )
</p>
</li>
<li><p> univariateScore.gee( ... )
</p>
</li>
<li><p> InternalMMPC.gee( .. )
</p>
</li>
<li><p> InternalSES.gee( .. )
</p>
</li>
<li><p> max_min_assoc.gee( ... )
</p>
</li>
<li><p> min_assoc.gee( ... )
</p>
</li>
<li><p> IdentifyEquivalence.gee( ... )
</p>
</li>
<li><p> identifyTheEquivalent.gee( ... )
</p>
</li>
<li><p> univariateScore.timeclass( ... )
</p>
</li>
<li><p> InternalMMPC.timeclass( ... )
</p>
</li>
<li><p> gomp2( ... )
</p>
</li>
<li><p> kfbed.reg( ... )
</p>
</li>
<li><p> kfbed.glmm.reg( ... )
</p>
</li>
<li><p> kfbed.gee.reg( ... )
</p>
</li>
<li><p> bs.g2( ... )
</p>
</li>
<li><p> fbed.g2( ... )
</p>
</li>
<li><p> fbed.glmm.ordinal( ... )
</p>
</li>
<li><p> fbed.glmm.ordinal.reps( ... )
</p>
</li>
<li><p> ebic.fbed.glmm.ordinal( ... )
</p>
</li>
<li><p> ebic.fbed.glmm.ordinal.reps( ... )
</p>
</li>
<li><p> glmm.ordinal.bsreg( ... )
</p>
</li>
<li><p> glmm.ordinal.reps.bsreg( ... )
</p>
</li>
<li><p> ebic.glmm.reps.bsreg( ... )
</p>
</li>
<li><p> ebic.glmm.ordinal.reps.bsreg( ... )
</p>
</li>
<li><p> big.model( ... )
</p>
</li>
<li><p> big.fbed.g2( ... )
</p>
</li>
<li><p> big.bs.g2( ... )
</p>
</li>
<li><p> clogit.cv.ses( ... )
</p>
</li>
<li><p> bsreg.big( ... )
</p>
</li>
<li><p> fbed.glmm.cr( ... )
</p>
</li>
<li><p> ebic.fbed.glmm.cr( ... )
</p>
</li>
<li><p> glmm.cr.bsreg( ... )
</p>
</li>
<li><p> ebic.glmm.cr.bsreg( ... )
</p>
</li>
<li><p> test.maker( ... )
</p>
</li>
<li><p> ebic.model( ... )
</p>
</li>
<li><p> fbed.lr( ... )
</p>
</li>
<li><p> fbed.ebic( ... )
</p>
</li>
<li><p> spml.bsreg( ... )
</p>
</li>
<li><p> bic.llr.fsreg( ... )
</p>
</li>
<li><p> ebic.llr.bsreg( ... )
</p>
</li>
<li><p> llr.bsreg( ... )
</p>
</li>
<li><p> beta.reg( ... )
</p>
</li>
<li><p> ebic.spml.bsreg( ... )
</p>
</li>
<li><p> regzinb( ... )
</p>
</li>
<li><p> zinb.mle( ... )
</p>
</li>
<li><p> fbed.glmm.nb( ... )
</p>
</li>
<li><p> fbed.glmm.nb.reps( ... )
</p>
</li>
<li><p> glmm.nb.bsreg( ... )
</p>
</li>
<li><p> glmm.nb.reps.bsreg( ... )
</p>
</li></ul>


<hr>
<h2 id='MXM-package'>
This is an R package that currently implements feature selection methods for identifying minimal, 
statistically-equivalent and equally-predictive feature subsets. Additionally, the package includes two algorithms for 
constructing the skeleton of a Bayesian network.
</h2><span id='topic+MXM-package'></span>

<h3>Description</h3>

<p>'MXM' stands for Mens eX Machina, meaning 'Mind from the Machine' in Latin. The package provides source code for the SES algorithm and for some appropriate statistical conditional independence tests. (Fisher and Spearman correlation, 
G-square test are some examples. Currently the response variable can be univariate or multivariate Euclidean, 
proportions within 0 and 1, compositional data without zeros and  ones, binary, nominal or ordinal multinomial, 
count data (handling also overdispersed and with more zeros than expected), longitudinal, clustered data, survival 
and case-control. Robust versions are also available in some cases and a K-fold cross validation is offered. 
Bayesian network related algorithms and ridge reression are also included. Read the package's help pages for more details.
</p>
<p>MMPC and SES can handle even thousands of variables and for some tests, even many sample sizes of tens of thousands. 
The user is best advised to check his variables in the beginning. For some regressions, logistic and Poisson for example, we have used C++ codes for speed reasons. 
</p>
<p>For more information the reader is addressed to 
</p>
<p>Lagani V., Athineou G., Farcomeni A., Tsagris M. and Tsamardinos I. (2017). Feature Selection with the R Package MXM: Discovering Statistically Equivalent Feature Subsets. Journal of Statistical Software, 80(7), doi:10.18637/jss.v080.i07 
and 
</p>
<p>Tsagris, M. and Tsamardinos, I. (2019). Feature selection with the R package MXM. F1000Research 7: 1505.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> MXM</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.5.5</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2022-08-24</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL-2</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Maintainer</h3>

<p>Konstantina Biza <a href="mailto:kbiza@csd.uoc.gr">kbiza@csd.uoc.gr</a>.
</p>


<h3>Note</h3>

<p>Acknowledgments:
The research leading to these results has received funding from the European Research Council under the 
European Union's Seventh Framework Programme (FP/2007-2013) / ERC Grant Agreement n. 617393.
</p>
<p>Michail Tsagris would like to express his acknowledgments to Marios Dimitriadis and Manos Papadakis, undergraduate students in the department of computer science, university of Crete, for their programming tips and advice. Their help has been very valuable. Dr Uwe Ligges and Prof Kurt Hornik from the CRAN team are greatly acknowledged for their assistance. Prof Achim Zeileis is greatly acknowledged for this help with the quasi Poisson and quasi binomial regression models. Christina Chatzipantsiou and Kleio Maria Verrou are acknowledged for their suggestions. Nikolaos Pandis from the University of Bern is acknowledged for his suggestion of the AFT (regression) models and for his suggestions. Michail is grateful to James Dalgleish from Columbia University who suggested that we mention, in various places, that most algorithms return the logarithm of the p-values and not the p-values. Stavros Lymperiadis provided a very useful example where weights are used in a regression model; in surveys when stratified random sampling has been applied. Dr David Gomez Cabrero Lopez is also acknowledged. Margarita Rebolledo is acknowledged for spotting a bug. Zurab Khasidashvili from Intel Israel is acknowledged for spotting a bug in the function mmmb(). Teny Handhayani (PhD student at the University of York) spotted a bug in the conditional independence tests with mixed data and she is acknowledged for that. Dr. Kjell Jorner (Postdoctoral Fellow at the Department of Chemistry, University of Toronto) spotted a bug in two performance metrics of the bbc() function and he is acknowledged for that.
</p>
<p><b>Disclaimer:</b> Professor Tsamardinos is the creator of this package and Dr Lagani supervised Mr Athineou build it. Dr Tsagris is the current maintainer.
</p>


<h3>Author(s)</h3>

<p>Ioannis Tsamardinos &lt;tsamard@csd.uoc.gr&gt;, Vincenzo Lagani &lt;vlagani@csd.uoc.gr&gt;, Giorgos Athineou &lt;athineou@csd.uoc.gr&gt;, Michail Tsagris &lt;mtsagris@uoc.gr&gt;, Giorgos Borboudakis &lt;borbudak@csd.uoc.gr&gt;, Anna Roumpelaki &lt;anna.roumpelaki@gmail.com&gt;, Konstantina Biza &lt;kbiza@csd.uoc.gr&gt;.
</p>


<h3>References</h3>

<p>Tsagris, M., Papadovasilakis, Z., Lakiotaki, K., &amp; Tsamardinos, I. (2022). The <code class="reqn">\gamma</code>-OMP algorithm for feature selection with application to gene expression data. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 19(2): 1214-1224.
</p>
<p>Tsagris, M. and Tsamardinos, I. (2019). Feature selection with the R package MXM. F1000Research 7: 1505
</p>
<p>Borboudakis G. and Tsamardinos I. (2019). Forward-backward selection with early dropping. Journal of Machine Learning Research, 20(8): 1-39.
</p>
<p>Tsagris, M. (2019). Bayesian Network Learning with the PC Algorithm: An Improved and Correct Variation. Applied Artificial Intelligence, 33(2):101-123.
</p>
<p>Tsagris, M., Lagani, V. and Tsamardinos, I. (2018). Feature selection for high-dimensional temporal data. 
BMC Bioinformatics, 19:17. 
</p>
<p>Tsagris, M., Borboudakis, G., Lagani, V. and Tsamardinos, I. (2018). Constraint-based causal discovery with mixed data. International Journal of Data Science and Analytics, 6(1): 19-30. 
</p>
<p>Tsagris, M., Papadovasilakis, Z., Lakiotaki, K. and Tsamardinos, I. (2018). Efficient feature selection on gene expression data: Which algorithm to use? BioRxiv preprint. 
</p>
<p>Lagani V., Athineou G., Farcomeni A., Tsagris M. and Tsamardinos I. (2017). Feature Selection with the R Package MXM: Discovering Statistically Equivalent Feature Subsets. Journal of Statistical Software, 80(7), doi:10.18637/jss.v080.i07.
</p>
<p>Chen S., Billings S. A., and Luo W. (1989). Orthogonal least squares methods and their application to non-linear system identification. International Journal of control, 50(5), 1873-1896.
http://eprints.whiterose.ac.uk/78100/1/acse
</p>
<p>Davis G. (1994). Adaptive Nonlinear Approximations. PhD thesis. 
http://www.geoffdavis.net/papers/dissertation.pdf
</p>
<p>Demidenko E. (2013). Mixed Models: Theory and Applications with R, 2nd Edition. New Jersey: Wiley &amp; Sons.
</p>
<p>Gharavi-Alkhansari M., anz Huang T. S. (1998, May). A fast orthogonal matching pursuit algorithm. In Acoustics, Speech and Signal Processing, 1998. 
Proceedings of the 1998 IEEE International Conference on (Vol. 3, pp. 1389-1392). 
</p>
<p>Lagani V., Kortas G. and Tsamardinos I. (2013), Biomarker signature identification in &quot;omics&quot; with multiclass outcome. Computational and Structural Biotechnology Journal, 6(7):1-7.
</p>
<p>Liang  K.Y.  and  Zeger  S.L. (1986). Longitudinal data analysis using generalized linear models. 
Biometrika, 73(1): 13-22.
</p>
<p>Mallat S. G. &amp; Zhang Z. (1993). Matching pursuits with time-frequency dictionaries. IEEE Transactions on signal processing, 41(12), 3397-3415.
https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf
</p>
<p>Paik M.C. (1988). Repeated measurement analysis for nonnormal data in small samples. Communications in
Statistics-Simulation and Computation, 17(4): 1155-1171.
</p>
<p>Pati Y. C., Rezaiifar R. and Krishnaprasad P. S. (1993). Orthogonal matching pursuit: Recursive function approximation with applications to wavelet 
decomposition. In Signals, Systems and Computers. 1993 Conference Record of The Twenty-Seventh Asilomar Conference on. IEEE.
</p>
<p>Prentice R.L. and Zhao L.P. (1991). Estimating equations for parameters in means and covariances
of multivariate discrete and continuous responses. Biometrics, 47(3): 825-839.
</p>
<p>Spirtes P.,  Glymour C. and Scheines R. (2001). Causation, Prediction, and Search. The MIT Press, Cambridge, MA, USA, 3nd edition.
</p>
<p>Tsamardinos I., Greasidou E. and Borboudakis G. (2018). Bootstrapping the out-of-sample predictions for efficient and accurate cross-validation. Machine Learning 107(12): 1895-1922.  
</p>
<p>Tsamardinos I., Lagani V. and Pappas D. (2012) Discovering multiple, equivalent biomarker signatures. In proceedings of the 7th conference of the Hellenic Society for Computational Biology &amp; Bioinformatics - HSCBB12.
</p>
<p>Tsamardinos, Brown and Aliferis (2006). The max-min hill-climbing Bayesian network structure learning algorithm. Machine learning, 65(1), 31-78.
</p>
<p>Tsamardinos I., Aliferis C. F. and Statnikov, A. (2003). Time and sample efficient discovery of Markov blankets and direct causal relations. In Proceedings of the 9th ACM SIGKDD international conference on Knowledge discovery and data mining p. 673-678. 
</p>
<p>Yan J. and Fine J. (2004). Estimating equations for association structures. Statistics in medicine, 23(6): 859-874
</p>
<p>Zhang, Jiji. (2008). On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias. Artificial Intelligence 172(16): 1873&ndash;1896.
</p>
<p>Ziegler A., Kastner C., Brunner D. and Blettner M. (2000). Familial associations of lipid profiles: A generalised estimating equations approach. Statistics in medicine, 19(24): 3345-3357
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SES">SES</a>, <a href="#topic+MMPC">MMPC</a>, <a href="#topic+fbed.reg">fbed.reg</a>, <a href="#topic+gomp">gomp</a>, <a href="#topic+pc.sel">pc.sel</a>, <a href="#topic+censIndCR">censIndCR</a>,<a href="#topic+testIndFisher">testIndFisher</a>, <a href="#topic+testIndLogistic">testIndLogistic</a>, <a href="#topic+gSquare">gSquare</a>, <a href="#topic+testIndRQ">testIndRQ</a></code>
</p>

<hr>
<h2 id='Neighbours+20of+20nodes+20in+20an+20undirected+20graph'>
Returns the node(s) and their neighbour(s), if there are any.
</h2><span id='topic+nei'></span>

<h3>Description</h3>

<p>Returns the node(s) and their neighbour(s) of one or more nodes (if there are any).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nei(G, node)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Neighbours+2B20of+2B20nodes+2B20in+2B20an+2B20undirected+2B20graph_+3A_g">G</code></td>
<td>

<p>The adjacency matrix of an undirected graph as produced by <code><a href="#topic+mmhc.skel">mmhc.skel</a></code>, <code><a href="#topic+pc.skel">pc.skel</a></code> or any other algorithm. 
</p>
</td></tr>
<tr><td><code id="Neighbours+2B20of+2B20nodes+2B20in+2B20an+2B20undirected+2B20graph_+3A_node">node</code></td>
<td>

<p>A vector with one or more numbers indicating the seleted node(s) (or variable(s)). 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a way to see the network for some given nodes. It is useful if you have many nodes and the whole network is a bit difficult to see clearly. 
</p>


<h3>Value</h3>

<p>A list object called &quot;geit&quot; containing the neighbours of the node(s). If there are no neighbours a message appears and no plot is presented. If the &quot;graph&quot; argument is set to TRUE and there are neighbours, a plot will appear. 
</p>
<p>Bear in mind that the values can be extracted with the $ symbol, i.e. this is an S3 class output. 
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+mmhc.skel">mmhc.skel</a>, <a href="#topic+SES">SES</a>, <a href="#topic+MMPC">MMPC</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate a dataset with continuous data
set.seed(1234)
dataset &lt;- matrix(runif(500 * 20, 1, 100), nrow = 500 ) 
G &lt;- pc.con(dataset)$G
plotnetwork(G)
dev.new()
nei( G, c(3, 4) )
nei( G, c(1, 3) )
</code></pre>

<hr>
<h2 id='Network+20construction+20using+20the+20partial+20correlation+20based+20forward+20regression+20or+20FBED'>
Network construction using the partial correlation based forward regression of FBED
</h2><span id='topic+corfs.network'></span><span id='topic+corfbed.network'></span>

<h3>Description</h3>

<p>Network construction using the partial correlation based forward regression or FBED. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>corfs.network(x, threshold = 0.05, tolb = 2, tolr = 0.02, stopping = "BIC", 
symmetry = TRUE, nc = 1) 

corfbed.network(x, threshold = 0.05, symmetry = TRUE, nc = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Network+2B20construction+2B20using+2B20the+2B20partial+2B20correlation+2B20based+2B20forward+2B20regression+2B20or+2B20FBED_+3A_x">x</code></td>
<td>

<p>A matrix with continuous data.
</p>
</td></tr>
<tr><td><code id="Network+2B20construction+2B20using+2B20the+2B20partial+2B20correlation+2B20based+2B20forward+2B20regression+2B20or+2B20FBED_+3A_threshold">threshold</code></td>
<td>

<p>Threshold ( suitable values in (0, 1) ) for assessing p-values significance. Default value is 0.05.
</p>
</td></tr>
<tr><td><code id="Network+2B20construction+2B20using+2B20the+2B20partial+2B20correlation+2B20based+2B20forward+2B20regression+2B20or+2B20FBED_+3A_tolb">tolb</code></td>
<td>

<p>The difference in the BIC bewtween two successive values. By default this is is set to 2. If for example, the BIC difference between two succesive models is less than 2, the process stops and the last variable, 
even though significant does not enter the model.
</p>
</td></tr>
<tr><td><code id="Network+2B20construction+2B20using+2B20the+2B20partial+2B20correlation+2B20based+2B20forward+2B20regression+2B20or+2B20FBED_+3A_tolr">tolr</code></td>
<td>

<p>The difference in the adjusted <code class="reqn">R^2</code> bewtween two successive values. By default this is is set to 0.02. If for example, the difference between the adjusted <code class="reqn">R^2</code> of two succesive models is less than 0.02, 
the process stops and the last variable, even though significant does not enter the model.
</p>
</td></tr>
<tr><td><code id="Network+2B20construction+2B20using+2B20the+2B20partial+2B20correlation+2B20based+2B20forward+2B20regression+2B20or+2B20FBED_+3A_stopping">stopping</code></td>
<td>

<p>The stopping rule. The &quot;BIC&quot; is the default value, but can change to &quot;ar2&quot; and in this case the adjusted <code class="reqn">R^2</code> is used. If you want both of these criteria to be satisified, type &quot;BICR2&quot;. 
</p>
</td></tr>
<tr><td><code id="Network+2B20construction+2B20using+2B20the+2B20partial+2B20correlation+2B20based+2B20forward+2B20regression+2B20or+2B20FBED_+3A_symmetry">symmetry</code></td>
<td>

<p>In order for an edge to be added, a statistical relationship must have been found from both directions. If you want this symmetry correction to take place, leave this boolean variable to TRUE. If you set it to FALSE, then if 
a relationship between Y and X is detected but not between X and Y, the edge is still added. 
</p>
</td></tr>
<tr><td><code id="Network+2B20construction+2B20using+2B20the+2B20partial+2B20correlation+2B20based+2B20forward+2B20regression+2B20or+2B20FBED_+3A_nc">nc</code></td>
<td>

<p>How many cores to use. This plays an important role if you have many variables, say thousands or so. You can try with nc = 1 and with nc = 4 for example to see the differences. If you have a multicore machine, this is a must 
option. There was an extra argument for plotting the skeleton but it does not work with the current visualisation 
packages, hence we removed the argument. Use <code><a href="#topic+plotnetwork">plotnetwork</a></code> to plot the skeleton.   
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the MMHC algorithm (see <code><a href="#topic+mmhc.skel">mmhc.skel</a></code>), the MMPC or SES algorithms are run for every variable. Hence, one can use forward regression for each variable and this is what we are doing here. Partial correlation 
forward regression is very efficient, since only correlations are being calculated. 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>runtime</code></td>
<td>

<p>The run time of the algorithm. A numeric vector. The first element is the user time, the second element is the system time and the third element is the elapsed time.
</p>
</td></tr>
<tr><td><code>density</code></td>
<td>

<p>The number of edges divided by the total possible number of edges, that is #edges / <code class="reqn">n(n-1)/2</code>, where <code class="reqn">n</code> is the number of variables.
</p>
</td></tr>
<tr><td><code>info</code></td>
<td>

<p>Some summary statistics about the edges, minimum, maximum, mean, median number of edges.
</p>
</td></tr>
<tr><td><code>ntests</code></td>
<td>

<p>The number of tests MMPC (or SES) performed at each variable.
</p>
</td></tr>
<tr><td><code>G</code></td>
<td>

<p>The adjancency matrix. A value of 1 in G[i, j] appears in G[j, i] also, indicating that i and j have an edge between them.
</p>
</td></tr>
</table>
<p>Bear in mind that the values can be extracted with the $ symbol, i.e. this is an S3 class output. 
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Sanford Weisberg (2014). Applied Linear Regression. Hoboken NJ: John Wiley, 4th edition.
</p>
<p>Draper N.R. and Smith H. (1988). Applied regression analysis. New York, Wiley, 3rd edition. 
</p>
<p>Tsamardinos, Brown and Aliferis (2006). The max-min hill-climbing Bayesian network structure learning algorithm. Machine learning, 65(1), 31-78.
</p>
<p>Borboudakis G. and Tsamardinos I. (2019). Forward-backward selection with early dropping. Journal of Machine Learning Research, 20(8): 1-39.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+mmhc.skel">mmhc.skel</a>, <a href="#topic+pc.skel">pc.skel</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate a dataset with continuous data
dataset &lt;- matrix(runif(400 * 20, 1, 100), ncol = 20 ) 
a1 &lt;- mmhc.skel(dataset, max_k = 3, threshold = 0.05, test = "testIndFisher", 
nc = 1) 
a2 &lt;- corfs.network(dataset, threshold = 0.05, tolb = 2, tolr = 0.02, stopping = "BIC", 
symmetry = TRUE, nc = 1) 
a1$runtime  
a2$runtime 
</code></pre>

<hr>
<h2 id='Orientation+20rules+20for+20the+20PC+20algorithm'>
The orientations part of the PC algorithm.
</h2><span id='topic+pc.or'></span>

<h3>Description</h3>

<p>The function takes the outcome of the PC algorithm, as produced by <code><a href="#topic+pc.skel">pc.skel</a></code> or <code><a href="#topic+pc.con">pc.con</a></code> and performes 
the 4 orientation rules. A graph is also possible to visualize.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pc.or(mod) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Orientation+2B20rules+2B20for+2B20the+2B20PC+2B20algorithm_+3A_mod">mod</code></td>
<td>

<p>An object with the results of the PC algorithm, as produced by <code><a href="#topic+pc.skel">pc.skel</a></code> or <code><a href="#topic+pc.con">pc.con</a></code>. There was an extra 
argument for plotting the skeleton but it does not work with the current visualisation packages, hence we removed the argument. 
Use <code><a href="#topic+plotnetwork">plotnetwork</a></code> to plot the skeleton.   
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>After having calculated the skeleton of the PC algorithm one may wants to perform orientations, leading to causal relationships. 
The rules as stated in Spirtes, Glymour and Scheines (2001) are
</p>

<ol>
<li> <p><b>Rule 0</b>. For each triple of vertices X, Y, Z such that the pair X, Y and the pair Y, Z are each adjacent in C but the pair X, Z 
are not adjacent in C, orient X - Y - Z as X -&gt; Y &lt;- Z if and only if Y is not in Sepset(X, Z). 
</p>
</li>
<li> <p><b>Rule 1</b>. If A -&gt; B, B and C are adjacent, A and C are not adjacent, and there is no arrowhead at B, then orient B - C as B -&gt; C.
</p>
</li>
<li> <p><b>Rule 2</b>. If there is a directed path from A to B, and an edge between A and B, then orient A - B as A -&gt; B.
</p>
</li>
<li> <p><b>Rule 3</b>. If A -&gt; B &lt;- C, A - D - C, A and C are not adjacent, and D - B, then orient D - B as D -&gt; B. 
</p>
</li></ol>

<p>The first rule is applied once. Rules 2-4 are applied repeatedly until no more edges can be oriented. If when a rule is applied and a cycle is 
detected, the rule is cancelled. Also, when applying Rules 1-3 we try to avoid the creation of new v-structures (X -&gt; Y &lt;- Z). 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>Gini</code></td>
<td>

<p>The initial adjacency matrix, no orientations. This is the matrix produced by <code><a href="#topic+pc.skel">pc.skel</a></code> or <code><a href="#topic+pc.con">pc.con</a></code>. 
</p>
</td></tr>
<tr><td><code>G</code></td>
<td>

<p>The final adjaceny matrix with the orientations. If G[i, j] = 2 then G[j, i] = 3. This means that there is an arrow from node i to node j. 
If G[i, j] = G[j, i] = 0; 
there is no edge between nodes i and j. If G[i, j] = G[j, i] = 1; there is an (undirected) edge between nodes i and j.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The run time of the algorithm. A numeric vector. The first element is the user time, the second element is the system time and the 
third element is the elapsed time.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Spirtes P.,  Glymour C. and Scheines R. (2001). Causation, Prediction, and Search. The MIT Press, Cambridge, MA, USA, 3nd edition.
</p>
<p>Zhang, Jiji. (2008). On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias. 
Artificial Intelligence 172(16): 1873&ndash;1896.
</p>
<p>Tsagris M. (2019). Bayesian network learning with the PC algorithm: an improved and correct variation. 
Applied Artificial Intelligence 33(2): 101-123.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+pc.con">pc.con</a>, <a href="#topic+pc.skel">pc.skel</a>, <a href="#topic+mmhc.skel">mmhc.skel</a>, <a href="#topic+is.dag">is.dag</a>, <a href="#topic+mb">mb</a> 
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate a dataset with continuous data
y &lt;- rdag2(2000, p = 20, nei = 3)
ind &lt;- sample(1:20, 20)
tru &lt;- y$G[ind, ind] 
x &lt;- y$x[, ind]
mod &lt;- pc.con(x)
mod$runtime

plotnetwork(tru) 

b &lt;- pc.or(mod)
plotnetwork(b$G)

plotnetwork( dag2eg(tru) )  ## essential graph
plotnetwork(b$G)
</code></pre>

<hr>
<h2 id='Partial+20correlation+20between+20two+20variables'>
Partial correlation
</h2><span id='topic+partialcor'></span>

<h3>Description</h3>

<p>Partial correlation between two variables when a correlation matrix is given.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>partialcor(R, indx, indy, indz, n) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Partial+2B20correlation+2B20between+2B20two+2B20variables_+3A_r">R</code></td>
<td>

<p>A correlation matrix.
</p>
</td></tr>
<tr><td><code id="Partial+2B20correlation+2B20between+2B20two+2B20variables_+3A_indx">indx</code></td>
<td>

<p>The index of the first variable whose conditional correlation is to estimated.
</p>
</td></tr>
<tr><td><code id="Partial+2B20correlation+2B20between+2B20two+2B20variables_+3A_indy">indy</code></td>
<td>

<p>The index of the second variable whose conditional correlation is to estimated.
</p>
</td></tr>
<tr><td><code id="Partial+2B20correlation+2B20between+2B20two+2B20variables_+3A_indz">indz</code></td>
<td>

<p>The index of the conditioning variables.
</p>
</td></tr>
<tr><td><code id="Partial+2B20correlation+2B20between+2B20two+2B20variables_+3A_n">n</code></td>
<td>

<p>The sample size of the data from which the correlation matrix was computed.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given a correlation matrix the function will caclulate the partial correlation between variables indx and indy conditioning on variable(s) indz. The logarithm of the p-value is also returned.  
</p>


<h3>Value</h3>

<p>The partial correlation coefficient and the logged p-value for the test of no association.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+testIndFisher">testIndFisher</a>, <a href="#topic+testIndSpearman">testIndSpearman</a>, <a href="#topic+permcor">permcor</a>, <a href="#topic+pc.con">pc.con</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>r &lt;- cor( iris[, 1:4] )
partialcor(r, 1, 2, 0, 150) 
r[1, 2]  ## the same as above

y &lt;- as.vector( iris[, 1] )
x &lt;- as.vector( iris[, 2] )
z &lt;- as.vector( iris[, 3] )
e1 &lt;- resid( lm(y ~ z) )
e2 &lt;- resid( lm(x ~ z) )
cor(e1, e2)
partialcor(r, 1, 2, 3, 150)
</code></pre>

<hr>
<h2 id='Permutation+20based+20p-value+20for+20the+20Pearson+20correlation+20coefficient'>
Permutation based p-value for the Pearson correlation coefficient
</h2><span id='topic+permcor'></span><span id='topic+permcorrels'></span>

<h3>Description</h3>

<p>The main task of this test is to provide a p-value PVALUE for the null hypothesis: feature 'X' is independent from 'TARGET' given a conditioning set CS.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>permcor(x1, x2, R = 999) 
permcorrels(y, x, R = 999)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Permutation+2B20based+2B20p-value+2B20for+2B20the+2B20Pearson+2B20correlation+2B20coefficient_+3A_x1">x1</code></td>
<td>

<p>A numerical vector.
</p>
</td></tr>
<tr><td><code id="Permutation+2B20based+2B20p-value+2B20for+2B20the+2B20Pearson+2B20correlation+2B20coefficient_+3A_x2">x2</code></td>
<td>

<p>A numerical vector of the same size as x1.
</p>
</td></tr>
<tr><td><code id="Permutation+2B20based+2B20p-value+2B20for+2B20the+2B20Pearson+2B20correlation+2B20coefficient_+3A_y">y</code></td>
<td>

<p>A vector whose length is equal to the number of rows of x.
</p>
</td></tr>
<tr><td><code id="Permutation+2B20based+2B20p-value+2B20for+2B20the+2B20Pearson+2B20correlation+2B20coefficient_+3A_x">x</code></td>
<td>

<p>This is a matrix with many variables. 
</p>
</td></tr>
<tr><td><code id="Permutation+2B20based+2B20p-value+2B20for+2B20the+2B20Pearson+2B20correlation+2B20coefficient_+3A_r">R</code></td>
<td>

<p>The number of permutations to be conducted; set to 999 by default. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a computational non parametric (permutation based) correlation coefficient test and is advised to be used when a small sample size is available. If you want to use the Spearman correlation instead, simply provide the ranks of x or of y and x. 
</p>


<h3>Value</h3>

<p>For the case of &quot;permcor&quot; a vector consisting of two values, the Pearson correlation and the permutation based p-value. 
For the &quot;permcorrels&quot; a vector with three values, the Pearson correlation, the test statistic value and the permutation based logged p-value.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Legendre Pierre (2000). Comparison of permutation methods for the partial correlation and partial Mantel tests. Journal of Statistical Computation and Simulation 67(1):37-73.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pc.skel">pc.skel</a>, <a href="#topic+testIndSpearman">testIndSpearman</a>, <a href="#topic+testIndFisher">testIndFisher</a>, <a href="#topic+SES">SES</a>, <a href="#topic+CondIndTests">CondIndTests</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>MXM::permcor(iris[, 1], iris[, 2], R = 999)
x &lt;- matrix( rnorm(50 * 100), ncol = 100)
a &lt;- permcorrels(iris[1:50, 1], x)
</code></pre>

<hr>
<h2 id='Plot+20of+20longitudinal+20data'>
Plot of longitudinal data
</h2><span id='topic+tc.plot'></span>

<h3>Description</h3>

<p>Plot of longitudinal data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tc.plot(target, tp, id, type = "l", ylab = "Values", xlab = "Time points",
       col = 2, lwd = 1, lty = 2, pch = 1, main = "Spaghetti plot")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Plot+2B20of+2B20longitudinal+2B20data_+3A_target">target</code></td>
<td>

<p>A numerical vector with the longitudinal data. 
</p>
</td></tr>
<tr><td><code id="Plot+2B20of+2B20longitudinal+2B20data_+3A_tp">tp</code></td>
<td>

<p>The time points. It can either be a vector with length either equal to the number of time points or equal to the legnth of the target.
</p>
</td></tr>
<tr><td><code id="Plot+2B20of+2B20longitudinal+2B20data_+3A_id">id</code></td>
<td>

<p>A numerical vector specifying the subjects. It can either be a vector with length either equal to the number of subjects or equal to the legnth of the target.
</p>
</td></tr>
<tr><td><code id="Plot+2B20of+2B20longitudinal+2B20data_+3A_type">type</code></td>
<td>

<p>This is a graphical parameter. You can have lines &quot;l&quot; everywhere or lines with points at each time point &quot;p&quot;. 
</p>
</td></tr>
<tr><td><code id="Plot+2B20of+2B20longitudinal+2B20data_+3A_ylab">ylab</code></td>
<td>

<p>This is a graphical parameter. The label on the y axis.
</p>
</td></tr>
<tr><td><code id="Plot+2B20of+2B20longitudinal+2B20data_+3A_xlab">xlab</code></td>
<td>

<p>This is a graphical parameter. The label on the x axis.
</p>
</td></tr>
<tr><td><code id="Plot+2B20of+2B20longitudinal+2B20data_+3A_col">col</code></td>
<td>

<p>This is a graphical parameter. The color of the lines. 
</p>
</td></tr>
<tr><td><code id="Plot+2B20of+2B20longitudinal+2B20data_+3A_lwd">lwd</code></td>
<td>

<p>This is a graphical parameter. The thickness of the lines.
</p>
</td></tr>
<tr><td><code id="Plot+2B20of+2B20longitudinal+2B20data_+3A_lty">lty</code></td>
<td>

<p>This is a graphical parameter. The type of line, e.g. dashed, dotted, etc.
</p>
</td></tr>
<tr><td><code id="Plot+2B20of+2B20longitudinal+2B20data_+3A_pch">pch</code></td>
<td>

<p>This is a graphical parameter. If the type is &quot;b&quot;, then you can specify if you want different signs, for example circles, crosses, diamonds etc. 
</p>
</td></tr>
<tr><td><code id="Plot+2B20of+2B20longitudinal+2B20data_+3A_main">main</code></td>
<td>

<p>This is a graphical parameter. The title of the graph. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The data must be longitudinal (the same subject measured multiple times at different time points) and for one variable only. For the graphical parameters see <code><a href="graphics.html#topic+plot">plot</a></code> or <code><a href="graphics.html#topic+par">par</a></code>.
</p>


<h3>Value</h3>

<p>A plot with the longitudinal data over time.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Giorgos Athineou &lt;athineou@csd.uoc.gr&gt; Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Tsamardinos I., Aliferis C. F. and Statnikov, A. (2003). Time and sample efficient discovery of Markov blankets and direct causal relations. In Proceedings of the 9th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 673-678). 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+testIndGLMMReg">testIndGLMMReg</a>, <a href="#topic+SES.glmm">SES.glmm</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rnorm(70)
Days &lt;- rep(1:7, each = 10)
id &lt;- rep(1:10, 7)
tc.plot(y, Days, id)
tc.plot(y, Days, id, type = "b")
</code></pre>

<hr>
<h2 id='Probability+20residual+20of+20ordinal+20logistic+20regreession'>
Probability residual of ordinal logistic regreession
</h2><span id='topic+ord.resid'></span>

<h3>Description</h3>

<p>Probability residual of ordinal logistic regreession.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ord.resid(y, est)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Probability+2B20residual+2B20of+2B20ordinal+2B20logistic+2B20regreession_+3A_y">y</code></td>
<td>

<p>An ordered factor variable or a numerical vector.
</p>
</td></tr>
<tr><td><code id="Probability+2B20residual+2B20of+2B20ordinal+2B20logistic+2B20regreession_+3A_est">est</code></td>
<td>

<p>A matrix with the fitted values of an ordinal logistic regression model. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The probability residual of an ordinal logistic regression model is calculated (Li and Shepherd, 2012). 
It is a vector, irrespective of how many categories there are.
</p>


<h3>Value</h3>

<p>A vector with the probability residuals.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Li C., &amp; Shepherd B. E. (2012). A new residual for ordinal outcomes. Biometrika, 99(2): 473&ndash;480.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+testIndOrdinal">testIndOrdinal</a>, <a href="#topic+ordinal.reg">ordinal.reg</a> 
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(MASS)
y &lt;- factor( rbinom(400, 3, 0.6), ordered = TRUE )
x &lt;- rnorm(400)
mod &lt;- MASS::polr(y ~ x) 
res &lt;- ord.resid(y, mod$fitted.values)
</code></pre>

<hr>
<h2 id='Read+20big+20data+20or+20a+20big.matrix+20object'>
Read big data or a big.matrix object
</h2><span id='topic+read.big.data'></span>

<h3>Description</h3>

<p>Read big data or a big.matrix object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read.big.data(path, sep = ",", header = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Read+2B20big+2B20data+2B20or+2B20a+2B20big.matrix+2B20object_+3A_path">path</code></td>
<td>

<p>The path where the big.matrix object is.  
</p>
</td></tr>
<tr><td><code id="Read+2B20big+2B20data+2B20or+2B20a+2B20big.matrix+2B20object_+3A_sep">sep</code></td>
<td>

<p>A field delimiter, for example &quot; &quot; (tab separated) or &quot;,&quot; (comma separated). 
</p>
</td></tr>
<tr><td><code id="Read+2B20big+2B20data+2B20or+2B20a+2B20big.matrix+2B20object_+3A_header">header</code></td>
<td>

<p>If there are column names, then this should be TRUE.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The data (matrix) which will be read and compressed into a big.matrix object must be of type &quot;numeric&quot;. We tested it and it works with &quot;integer&quot; as well. But, in general, bear in mind that only matrices will be read. We have not tested with data.frame for example. Woever, in the help page of &quot;bigmemory&quot; this is mentioned: Any non-numeric entry will be ignored and replaced with NA, so reading something that traditionally would be a data.frame won't cause an error. A warning is issued. In all cases, the object size is alwasy 696 bytes!
</p>


<h3>Value</h3>

<p>A big.matrix object.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>. For more information see the &quot;bigmemory&quot; package.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+big.gomp">big.gomp</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
dataset &lt;- matrix( runif(100 * 50, 1, 100), ncol = 50 )
write.csv(dataset, "dataset.csv", sep = ",")
a &lt;- read.big.data("dataset.csv", header = TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='Regression+20modeler'>
Generic regression modelling function
</h2><span id='topic+modeler'></span>

<h3>Description</h3>

<p>Generic regression modelling function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>modeler(target, dataset = NULL, test = "testIndFisher")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Regression+2B20modeler_+3A_target">target</code></td>
<td>

<p>The target (dependent) variable. It can be a numerical variable, factor, ordinal factor, percentages, or time to event. 
</p>
</td></tr>
<tr><td><code id="Regression+2B20modeler_+3A_dataset">dataset</code></td>
<td>

<p>The predictor variable(s). It can be a vector, a matrix with continuous only variables. If there are no predictor variables leave this NULL.
</p>
</td></tr>
<tr><td><code id="Regression+2B20modeler_+3A_test">test</code></td>
<td>

<p>Unlike <code><a href="#topic+reg.fit">reg.fit</a></code> this accepts the test. The test argument is exactly like in all feature selection methods. 
This function accepts the following:  &quot;testIndReg&quot;, &quot;testIndPois&quot;, &quot;testIndNB&quot;, &quot;testIndLogistic&quot;, 
&quot;testIndMMReg&quot;, &quot;testIndRQ&quot;, &quot;testIndBinom&quot;, &quot;censIndCR&quot;, &quot;censIndWR&quot;, &quot;censIndLLR&quot;, &quot;testIndBeta&quot;, &quot;testIndGamma, &quot;testIndNormLog&quot;, 
&quot;testIndTobit&quot;, &quot;testIndQPois&quot;, &quot;testIndQBinom&quot;, &quot;testIndFisher&quot;, &quot;testIndMultinom&quot; and &quot;testIndOrdinal&quot;. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a generic regression function designed for continuous predictor variables only. It was useful for me so I decided to epxort it.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>mod</code></td>
<td>

<p>The fitted model.
</p>
</td></tr>
<tr><td><code>dev</code></td>
<td>

<p>The deviance. For some models though (&quot;testIndMMReg&quot;, &quot;testIndRQ&quot;, &quot;censIndCR&quot;, &quot;censIndWR&quot;, &quot;testIndTobit&quot;, &quot;testIndBeta&quot;, &quot;testIndNB&quot;,
&quot;&quot;testIndQPois&quot;, &quot;testIndQBinom&quot;) this contains twice the log-likelihood.
</p>
</td></tr>
<tr><td><code>bic</code></td>
<td>

<p>The BIC of the model. This is NA for the &quot;testIndQPois&quot; and &quot;testIndQBinom&quot; because they are quasi likhelidood models and hence have no BIC. 
</p>
</td></tr>
<tr><td><code>res</code></td>
<td>

<p>The residuals of the fitted model.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Almost the same as in <code><a href="#topic+CondIndTests">CondIndTests</a></code>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+reg.fit">reg.fit</a>, <a href="#topic+fbedreg.bic">fbedreg.bic</a>, <a href="#topic+mmpc.model">mmpc.model</a>, <a href="#topic+ridge.reg">ridge.reg</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#simulate a dataset with continuous data
dataset &lt;- matrix(runif(100 * 5, 1, 100), nrow = 100 ) 
#the target feature is the last column of the dataset as a vector
target &lt;- dataset[, 1]
dataset &lt;- dataset[, -1]
a &lt;- modeler(target, dataset)
</code></pre>

<hr>
<h2 id='Regression+20models+20based+20on+20SES+20and+20MMPC+20outputs'>
Regression model(s) obtained from SES or MMPC
</h2><span id='topic+ses.model'></span><span id='topic+mmpc.model'></span><span id='topic+waldses.model'></span><span id='topic+waldmmpc.model'></span>

<h3>Description</h3>

<p>One or more regression models obtained from SES or MMPC, are returned. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ses.model(target, dataset, wei = NULL, sesObject, nsignat = 1, test = NULL)

mmpc.model(target, dataset, wei = NULL, mmpcObject, test = NULL)

waldses.model(target, dataset, wei = NULL, wald.sesObject, nsignat = 1, test = NULL)

waldmmpc.model(target, dataset, wei = NULL, wald.mmpcObject, test = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Regression+2B20models+2B20based+2B20on+2B20SES+2B20and+2B20MMPC+2B20outputs_+3A_target">target</code></td>
<td>

<p>A numeric vector containing the values of the target variable. If the values are proportions or percentages, 
i.e. strictly within 0 and 1 they are mapped into R using log( target/(1 - target) ). It can also discrete data, binary data (as factor), nominal or ordinal data (as factor). In contrast to SES, no position of the target variable in the dataset is accepted. The target must be a numerical vector.
</p>
</td></tr>
<tr><td><code id="Regression+2B20models+2B20based+2B20on+2B20SES+2B20and+2B20MMPC+2B20outputs_+3A_dataset">dataset</code></td>
<td>

<p>A numeric matrix or data.frame containing the variables. Rows are samples and columns are features. If you have categorical variables, this should be a data frame.
</p>
</td></tr>
<tr><td><code id="Regression+2B20models+2B20based+2B20on+2B20SES+2B20and+2B20MMPC+2B20outputs_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. An example where weights are used is surveys when stratified sampling has occured. 
</p>
</td></tr>
<tr><td><code id="Regression+2B20models+2B20based+2B20on+2B20SES+2B20and+2B20MMPC+2B20outputs_+3A_sesobject">sesObject</code></td>
<td>

<p>An object with the results of a SES run.
</p>
</td></tr>
<tr><td><code id="Regression+2B20models+2B20based+2B20on+2B20SES+2B20and+2B20MMPC+2B20outputs_+3A_mmpcobject">mmpcObject</code></td>
<td>

<p>An object with the results of an MMPC run.
</p>
</td></tr>
<tr><td><code id="Regression+2B20models+2B20based+2B20on+2B20SES+2B20and+2B20MMPC+2B20outputs_+3A_wald.sesobject">wald.sesObject</code></td>
<td>

<p>An object with the results of a wald.ses run.
</p>
</td></tr>
<tr><td><code id="Regression+2B20models+2B20based+2B20on+2B20SES+2B20and+2B20MMPC+2B20outputs_+3A_wald.mmpcobject">wald.mmpcObject</code></td>
<td>

<p>An object with the results of an wald.mmpc run.
</p>
</td></tr>
<tr><td><code id="Regression+2B20models+2B20based+2B20on+2B20SES+2B20and+2B20MMPC+2B20outputs_+3A_nsignat">nsignat</code></td>
<td>

<p>How many signatures to use. If nsignat = 1 (default value) the first set of variables will be used for the model. If you want more, then specify the nubmer of signatures you want. If you want the models based on all signatures, specify &quot;all&quot;. If you put a number which is higher than the number of signatures, all models will be returned.
</p>
</td></tr>
<tr><td><code id="Regression+2B20models+2B20based+2B20on+2B20SES+2B20and+2B20MMPC+2B20outputs_+3A_test">test</code></td>
<td>

<p>If you know the test used in SES put it here, otherwise leave it NULL. It will take this information from the SEs object. If you used a robust version of a test (wherever possible), robust model(s) will be created.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This command is useful if you want to see all models and check for example their fitting ability, MSE in linear models for example.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>mod</code></td>
<td>

<p>Depending on the number of signatures requested, one or models will be returned.
</p>
</td></tr>
<tr><td><code>signature</code></td>
<td>

<p>A matrix (or just one vector if one signature only) with the variables of each signature, along with the BIC of the corresponding regression model.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Giorgos Athineou &lt;athineou@csd.uoc.gr&gt; and Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Aitchison J. (1986). The Statistical Analysis of Compositional Data, Chapman &amp; Hall; reprinted in 2003, with additional material, by The Blackburn Press.
</p>
<p>Cox D.R. (1972). Regression models and life-tables. J. R. Stat. Soc., 34, 187-220.
</p>
<p>Draper, N.R. and Smith H. (1988). Applied regression analysis. New York, Wiley, 3rd edition.
</p>
<p>Ferrari S.L.P. and Cribari-Neto F. (2004). Beta Regression for Modelling Rates and Proportions. Journal of Applied Statistics, 31(7): 799-815.
</p>
<p>Gutenbrunner C., Jureckova J., Koenker R. and Portnoy S. (1993). Tests of Linear Hypothesis based on Regression Rank Scores, Journal of NonParametric Statistics 2, 307-331.
</p>
<p>Joseph M.H. (2011). Negative Binomial Regression. Cambridge University Press, 2nd edition.
</p>
<p>Koenker R.W. (2005). Quantile Regression, Cambridge University Press.
</p>
<p>Lagani V., Kortas G. and Tsamardinos I. (2013). Biomarker signature identification in &quot;omics&quot; with multiclass outcome. Computational and Structural Biotechnology Journal, 6(7): 1-7.
</p>
<p>Lagani V. and Tsamardinos I. (2010). Structure-based variable selection for survival data. Bioinformatics Journal 16(15): 1887-1894.
</p>
<p>Lambert, Diane (1992). Zero-inflated Poisson regression, with an application to defects in manufacturing. Technometrics 34(1)1: 1-14.
</p>
<p>Mardia K.V., Kent J.T. and Bibby J.M. (1979). Multivariate Analysis, Academic Press, New York, USA. 
</p>
<p>Maronna R.D. Yohai M.V. (2006). Robust Statistics, Theory and Methods. Wiley.
</p>
<p>McCullagh P., and Nelder J.A. (1989). Generalized linear models. CRC press, USA, 2nd edition.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+SES">SES</a>, <a href="#topic+MMPC">MMPC</a>, <a href="#topic+cv.ses">cv.ses</a>, <a href="#topic+cv.mmpc">cv.mmpc</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate a dataset with continuous data
dataset &lt;- matrix( runif(500 * 20, 1, 20), nrow = 500 ) 
#the target feature is the last column of the dataset as a vector
target &lt;- dataset[, 20]
dataset &lt;- dataset[, -20]
sesObject &lt;- SES(target , dataset , max_k=3 , threshold = 0.05)
ses.model(target, dataset, sesObject = sesObject, nsignat = 1, test = NULL) 
ses.model(target, dataset, sesObject = sesObject, nsignat = 40, test = NULL)
mmpcObject &lt;- MMPC(target, dataset, max_k=3, threshold = 0.05)
mmpc.model(target, dataset, mmpcObject = mmpcObject, test = NULL) 
</code></pre>

<hr>
<h2 id='Regression+20models+20based+20on+20SES.timeclass+20and+20MMPC.timeclass+20outputs'>
Regression model(s) obtained from SES.timeclass or MMPC.timeclass
</h2><span id='topic+mmpc.timeclass.model'></span><span id='topic+ses.timeclass.model'></span>

<h3>Description</h3>

<p>One or more regression models obtained from SES.timeclass or MMPC.timeclass, are returned. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mmpc.timeclass.model(target, dataset, id, reps, wei = NULL, mmpctimeclass.Object)
ses.timeclass.model(target, dataset, id, reps, wei = NULL, sestimeclass.Object, 
nsignat = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Regression+2B20models+2B20based+2B20on+2B20SES.timeclass+2B20and+2B20MMPC.timeclass+2B20outputs_+3A_target">target</code></td>
<td>

<p>The class variable. Provide a vector or a factor with discrete numbers indicating the class. Its length is equal to the number of rows of the dataset. 
</p>
</td></tr>
<tr><td><code id="Regression+2B20models+2B20based+2B20on+2B20SES.timeclass+2B20and+2B20MMPC.timeclass+2B20outputs_+3A_dataset">dataset</code></td>
<td>

<p>The dataset; provide a matrix. Currently, only continuous datasets are supported. The dataset
contains longitudinal data, where each column is a variable. The repeated measurements are the samples. 
</p>
</td></tr>
<tr><td><code id="Regression+2B20models+2B20based+2B20on+2B20SES.timeclass+2B20and+2B20MMPC.timeclass+2B20outputs_+3A_id">id</code></td>
<td>

<p>A numeric vector containing the subjects or groups. Its length is equal to the number of rows of the dataset.
</p>
</td></tr>
<tr><td><code id="Regression+2B20models+2B20based+2B20on+2B20SES.timeclass+2B20and+2B20MMPC.timeclass+2B20outputs_+3A_reps">reps</code></td>
<td>

<p>A numeric vector containing the time points of the subjects. Its length is equal to the number of rows of the dataset.
</p>
</td></tr>
<tr><td><code id="Regression+2B20models+2B20based+2B20on+2B20SES.timeclass+2B20and+2B20MMPC.timeclass+2B20outputs_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. 
</p>
</td></tr>
<tr><td><code id="Regression+2B20models+2B20based+2B20on+2B20SES.timeclass+2B20and+2B20MMPC.timeclass+2B20outputs_+3A_mmpctimeclass.object">mmpctimeclass.Object</code></td>
<td>

<p>An object with the results of an MMPC.timeclass run.
</p>
</td></tr>
<tr><td><code id="Regression+2B20models+2B20based+2B20on+2B20SES.timeclass+2B20and+2B20MMPC.timeclass+2B20outputs_+3A_sestimeclass.object">sestimeclass.Object</code></td>
<td>

<p>An object with the results of a SES.timeclass run.
</p>
</td></tr>
<tr><td><code id="Regression+2B20models+2B20based+2B20on+2B20SES.timeclass+2B20and+2B20MMPC.timeclass+2B20outputs_+3A_nsignat">nsignat</code></td>
<td>

<p>How many signatures to use. If nsignat = 1 (default value) the first set of variables will be used for the model. If you want more, then specify the nubmer of signatures you want. If you want the models based on all signatures, specify &quot;all&quot;. If you put a number which is higher than the number of signatures, all models will be returned.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This command is useful if you want to see all models and check for example their fitting ability.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>mod</code></td>
<td>

<p>Depending on the number of signatures requested, one or models will be returned.
</p>
</td></tr>
<tr><td><code>signature</code></td>
<td>

<p>A matrix (or just one vector if one signature only) with the variables of each signature, along with the BIC of the corresponding regression model.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Tsagris, M., Lagani, V., &amp; Tsamardinos, I. (2018). Feature selection for high-dimensional glmm data. BMC bioinformatics, 19(1), 17.
</p>
<p>McCullagh P., and Nelder J.A. (1989). Generalized linear models. CRC press, USA, 2nd edition.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+MMPC.timeclass">MMPC.timeclass</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## assume these are longitudinal data, each column is a variable (or feature)
dataset &lt;- matrix( rnorm(400 * 50), ncol = 50 ) 
id &lt;- rep(1:80, each = 5)  ## 80 subjects
reps &lt;- rep( seq(4, 12, by = 2), 80)  ## 5 time points for each subject
## dataset contains are the regression coefficients of each subject's values on the 
## reps (which is assumed to be time in this example)
target &lt;- rep(0:1, each = 200)
a &lt;- MMPC.timeclass(target, reps, id, dataset)
mmpc.timeclass.model(target, dataset, id, reps, mmpctimeclass.Object = a)
</code></pre>

<hr>
<h2 id='Regression+20models+20fitting'>
Regression modelling
</h2><span id='topic+reg.fit'></span>

<h3>Description</h3>

<p>Generic regression modelling function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reg.fit(y, dataset, event = NULL, reps = NULL, group = NULL, slopes = FALSE, 
reml = FALSE, model = NULL, wei = NULL, xnew = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Regression+2B20models+2B20fitting_+3A_y">y</code></td>
<td>

<p>The target (dependent) variable. It can be a numerical variable, factor, ordinal factor, percentages, matrix, or time to event. 
If the values are proportions or percentages, i.e. strictly within 0 and 1 they are mapped into R using log( target/(1 - target) ).
If they are compositional data the additive log-ratio (multivariate logit) transformation is aplied beforehand. If the model is &quot;clogit&quot; 
this must be a matrix with two columns. The first column must be 0 and 1, standing for 0 = control and 1 = case. The second column is the id of the patients. A numerical variable, for example c(1,2,3,4,5,6,7,1,2,3,4,5,6,7).
</p>
</td></tr>
<tr><td><code id="Regression+2B20models+2B20fitting_+3A_dataset">dataset</code></td>
<td>

<p>The indendent variable(s). It can be a vector, a matrix or a dataframe with continuous only variables, a data frame with mixed or only categorical variables.
</p>
</td></tr>
<tr><td><code id="Regression+2B20models+2B20fitting_+3A_event">event</code></td>
<td>

<p>This is NULL unless you have time to event data (survival regression).
</p>
</td></tr>
<tr><td><code id="Regression+2B20models+2B20fitting_+3A_reps">reps</code></td>
<td>

<p>This is NULL unless you have time measurements (longitudinal data).
</p>
</td></tr>
<tr><td><code id="Regression+2B20models+2B20fitting_+3A_group">group</code></td>
<td>

<p>This is NULL unless you have grouped (or clustered) data or longitudinal data (is the latter case the arugment reps is required). 
</p>
</td></tr>
<tr><td><code id="Regression+2B20models+2B20fitting_+3A_slopes">slopes</code></td>
<td>

<p>This is for the longitudinal data only, TRUE or FALSE. Should random slopes be added or not?
</p>
</td></tr>
<tr><td><code id="Regression+2B20models+2B20fitting_+3A_reml">reml</code></td>
<td>

<p>This is for the longitudinal or grouped data only, TRUE or FALSE. If TRUE, REML will be used, otherwise ML will be used.
</p>
</td></tr>
<tr><td><code id="Regression+2B20models+2B20fitting_+3A_model">model</code></td>
<td>

<p>The type of model you want to use. It can be specified by the user or left NULL, if other correct arguments have been passed. 
Poissible values (apart from NULL) are: &quot;gaussian&quot; (default), &quot;binary&quot;, &quot;binomial&quot;, &quot;multinomial&quot;, &quot;poisson&quot;, &quot;ordinal&quot;, &quot;gamma&quot;, &quot;normlog&quot;, &quot;tobit&quot;, &quot;cox&quot;, &quot;weibull&quot;, &quot;exponential&quot;, &quot;zip&quot;, &quot;beta&quot;, &quot;median&quot;, &quot;negbin&quot;, &quot;longitudinal&quot;, &quot;grouped&quot;, &quot;qpois&quot; and &quot;qbinom&quot;. The &quot;zip&quot; means 
that the zero part is constant, the variables are not associated with the excessive zeros. The value &quot;grouped&quot; refers to grouped data, but this does not have to be given if the argument &quot;group&quot; is given, but not the argument &quot;reps. The &quot;binomial&quot; is when you have the number of successes 
and also the number of trials. &quot;MM&quot; stands for robust regression using MM estimation and &quot;clogit&quot; stands for conditional logistic regression.
</p>
</td></tr>
<tr><td><code id="Regression+2B20models+2B20fitting_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. An example where weights are used is surveys when stratified sampling has occured.
</p>
</td></tr>
<tr><td><code id="Regression+2B20models+2B20fitting_+3A_xnew">xnew</code></td>
<td>

<p>If you have new data whose target values you want to predict put it here, otherwise leave it blank.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a generic regression function, which offers prediction as well. It is important that you pass the arguments with their names, for example if you have time to event data, write &quot;event = ...&quot; and not just put your event variable. This will avoid confusion. For the mixed models you need to specify the relevant arguments, &quot;slopes&quot;, &quot;reps&quot;, &quot;reml&quot; and &quot;group&quot; 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>mod</code></td>
<td>

<p>The fitted model.
</p>
</td></tr>
<tr><td><code>pred</code></td>
<td>

<p>If you have new data the predicted values of the target (dependent) variable.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Almost the same as in <code><a href="#topic+CondIndTests">CondIndTests</a></code>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+modeler">modeler</a>, <a href="#topic+fbedreg.bic">fbedreg.bic</a>, <a href="#topic+mmpc.model">mmpc.model</a>, <a href="#topic+ridge.reg">ridge.reg</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#simulate a dataset with continuous data
dataset &lt;- matrix(runif(100 * 5, 1, 100), nrow = 100 ) 
#the target feature is the last column of the dataset as a vector
target &lt;- dataset[, 1]
dataset &lt;- dataset[, -1]
a &lt;- reg.fit(target, dataset)
</code></pre>

<hr>
<h2 id='Ridge+20regression'>
Ridge regression
</h2><span id='topic+ridge.reg'></span>

<h3>Description</h3>

<p>Regularisation via ridge regression is performed. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ridge.reg(target, dataset, lambda, B = 1, newdata = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Ridge+2B20regression_+3A_target">target</code></td>
<td>

<p>A numeric vector containing the values of the target variable. If the values are proportions or percentages, 
i.e. strictly within 0 and 1 they are mapped into R using log( target/(1 - target) ).
</p>
</td></tr>
<tr><td><code id="Ridge+2B20regression_+3A_dataset">dataset</code></td>
<td>

<p>A numeric matrix containing the variables. Rows are samples and columns are features.
</p>
</td></tr>
<tr><td><code id="Ridge+2B20regression_+3A_lambda">lambda</code></td>
<td>

<p>The value of the regularisation parameter <code class="reqn">\lambda</code>.
</p>
</td></tr>
<tr><td><code id="Ridge+2B20regression_+3A_b">B</code></td>
<td>

<p>Number of bootstraps. If B = 1 no bootstrap is performed and no standard error for the regression coefficients is returned. 
</p>
</td></tr>
<tr><td><code id="Ridge+2B20regression_+3A_newdata">newdata</code></td>
<td>

<p>If you have new data and want to predict the value of the target put them here, otherwise, leave it NULL.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There is also the lm.ridge command in MASS library if you are interested in ridge regression. 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>beta</code></td>
<td>

<p>The regression coefficients if no bootstrap is performed. If bootstrap is performed their standard error appears as well.
</p>
</td></tr>
<tr><td><code>seb</code></td>
<td>

<p>The standard erorr of the regression coefficients. If bootstrap is performed their bootstrap estimated standard error appears.
</p>
</td></tr>
<tr><td><code>est</code></td>
<td>

<p>The fitted values if no new data are available. If you have used new data these will be the predicted target values. 
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Hoerl A.E. and R.W. Kennard (1970). Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12(1): 55-67.
</p>
<p>Brown P. J. (1994). Measurement, Regression and Calibration. Oxford Science Publications.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+ridgereg.cv">ridgereg.cv</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#simulate a dataset with continuous data
dataset &lt;- matrix(runif(100 * 30, 1, 100), nrow = 100 ) 
#the target feature is the last column of the dataset as a vector
target &lt;- dataset[, 10]
dataset &lt;- dataset[, -10]
a1 &lt;- ridge.reg(target, dataset, lambda = 0.5, B = 1, newdata = NULL)
a2 &lt;- ridge.reg(target, dataset, lambda = 0.5, B = 100, newdata = NULL) 
</code></pre>

<hr>
<h2 id='Ridge+20regression+20coefficients+20plot'>
Ridge regression
</h2><span id='topic+ridge.plot'></span>

<h3>Description</h3>

<p>A plot of the regularised parameters is shown. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ridge.plot(target, dataset, lambda = seq(0, 5, by = 0.1) ) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Ridge+2B20regression+2B20coefficients+2B20plot_+3A_target">target</code></td>
<td>

<p>A numeric vector containing the values of the target variable. If the values are proportions or percentages, 
i.e. strictly within 0 and 1 they are mapped into R using log( target/(1 - target) ). In any case, they must be continuous only.
</p>
</td></tr>
<tr><td><code id="Ridge+2B20regression+2B20coefficients+2B20plot_+3A_dataset">dataset</code></td>
<td>

<p>A numeric matrix containing the continuous variables. Rows are samples and columns are features.
</p>
</td></tr>
<tr><td><code id="Ridge+2B20regression+2B20coefficients+2B20plot_+3A_lambda">lambda</code></td>
<td>

<p>A grid of values of the regularisation parameter <code class="reqn">\lambda</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For every value of <code class="reqn">\lambda</code> the coefficients are obtained. They are plotted versus the <code class="reqn">\lambda</code> values. 
</p>


<h3>Value</h3>

<p>A plot with the values of the coefficients as a function of <code class="reqn">\lambda</code>.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Giorgos Athineou &lt;athineou@csd.uoc.gr&gt;, Vincenzo Lagani &lt;vlagani@csd.uoc.gr&gt; and Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Hoerl A.E. and R.W. Kennard (1970). Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12(1): 55-67.
</p>
<p>Brown P. J. (1994). Measurement, Regression and Calibration. Oxford Science Publications.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+ridge.reg">ridge.reg</a>, <a href="#topic+ridgereg.cv">ridgereg.cv</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#simulate a dataset with continuous data
dataset &lt;- matrix( runif(300 * 20, 1, 20), nrow = 300 ) 
#the target feature is the last column of the dataset as a vector
target &lt;- dataset[, 20]
dataset &lt;- dataset[, -20]
ridge.plot(target, dataset)
</code></pre>

<hr>
<h2 id='ROC+20and+20AUC'>
ROC and AUC
</h2><span id='topic+auc'></span>

<h3>Description</h3>

<p>Receiver operating curve and area under the curve.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>auc(group, preds, roc = FALSE, cutoffs = NULL) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ROC+2B20and+2B20AUC_+3A_group">group</code></td>
<td>

<p>A numerical vector with the predicted values of each group as 0 and 1.
</p>
</td></tr>
<tr><td><code id="ROC+2B20and+2B20AUC_+3A_preds">preds</code></td>
<td>

<p>The predicted values of each group. 
</p>
</td></tr>
<tr><td><code id="ROC+2B20and+2B20AUC_+3A_roc">roc</code></td>
<td>

<p>If you want the ROC to appear set it to TRUE.
</p>
</td></tr>
<tr><td><code id="ROC+2B20and+2B20AUC_+3A_cutoffs">cutoffs</code></td>
<td>

<p>If you provide a vector with decreasing numbers from 1 to 0 that will be used for the ROC, otherwise, the values from 1 to 0 with a step equal to -0.01 will be used.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The ara under the curve is returned. The user has the option of getting the receiver operating curve as well.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>cutoffs</code></td>
<td>

<p>The cutoff values.
</p>
</td></tr>
<tr><td><code>sensitivity</code></td>
<td>

<p>The sensitivity values for each cutoff value.
</p>
</td></tr>
<tr><td><code>specificity</code></td>
<td>

<p>The specificity value for each cutoff value.
</p>
</td></tr>
<tr><td><code>youden</code></td>
<td>

<p>The pair of of 1- specificity and sensitivity where the Youden's J appears on the graph and the Youden index which is defined as the maximum value of sensitivity - specificity + 1. 
</p>
</td></tr>
<tr><td><code>auc</code></td>
<td>

<p>The area under the curve, plus a circle with the point where Youden's J is located. If &quot;roc&quot; is set to FALSE, this is the only item in the list to be returned.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+bbc">bbc</a>, <a href="#topic+testIndLogistic">testIndLogistic</a> 
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>g &lt;- rbinom(150, 1, 0.6)
f &lt;- rnorm(150)
auc(g, f, roc = FALSE)
</code></pre>

<hr>
<h2 id='Search+20for+20triangles+20in+20an+20undirected+20graph'>
Search for triangles in an undirected graph
</h2><span id='topic+triangles.search'></span>

<h3>Description</h3>

<p>Search for triangles in an undirected graph.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>triangles.search(G) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Search+2B20for+2B20triangles+2B20in+2B20an+2B20undirected+2B20graph_+3A_g">G</code></td>
<td>

<p>The adjacency matrix of an undirected graph. G[i, j] = G[j, i] = 1 means there is an edge between modes i and j. Zero values indicate the absence of edges. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The functions searches for triangles, that is for tripletes of nodes (or variables) for which X-Y, Y-Z and X-Z.
</p>


<h3>Value</h3>

<p>A matrix with thre columns. If there are no triangles, the matrix is empty. If there is at least one triangle, then each row contains three numbers, one for each node. See the examples below.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+plotnetwork">plotnetwork</a>, <a href="#topic+pc.skel">pc.skel</a>, <a href="#topic+mmhc.skel">mmhc.skel</a> 
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)
x &lt;- rdag2(1000, p = 20, nei = 4)$x
a &lt;- pc.skel(x, alpha = 0.05)
plotnetwork(a$G)
triangles.search(a$G)
</code></pre>

<hr>
<h2 id='SES.gee.output-class'>Class <code>"SES.gee.output"</code></h2><span id='topic+SES.gee.output-class'></span><span id='topic+SES.gee.output'></span><span id='topic+plot+2CSES.gee.output-method'></span><span id='topic+SES.gee.output-method'></span><span id='topic+plot+2CSES.gee.output+2CANY-method'></span>

<h3>Description</h3>

<p>SES.gee output object class.
</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("SES.glmm.output", ...)</code>.
</p>


<h3>Slots</h3>


<dl>
<dt><code>selectedVars</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>selectedVarsOrder</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>queues</code>:</dt><dd><p>Object of class <code>"list"</code></p>
</dd>
<dt><code>signatures</code>:</dt><dd><p>Object of class <code>"matrix"</code></p>
</dd>
<dt><code>hashObject</code>:</dt><dd><p>Object of class <code>"list"</code></p>
</dd>
<dt><code>pvalues</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>stats</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>univ</code>:</dt><dd><p>Object of class <code>"list"</code></p>
</dd>
<dt><code>max_k</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>threshold</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>n.tests</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>runtime</code>:</dt><dd><p>Object of class <code>"proc_time"</code></p>
</dd>
<dt><code>test</code>:</dt><dd><p>Object of class <code>"character"</code></p>
</dd>
<dt><code>correl</code>:</dt><dd><p>Object of class <code>"character"</code></p>
</dd>  
<dt><code>se</code>:</dt><dd><p>Object of class <code>"character"</code></p>
</dd>  </dl>



<h3>Methods</h3>


<dl>
<dt>plot</dt><dd><p><code>plot(x = "SES.gee.output", mode = "all")</code>: Generic function for plotting the generated pvalues of the SES.glmm.output object. Argument mode = &quot;all&quot; for plotting all the pvalues or mode=&quot;partial&quot; for partial plotting the first 500 pvalues </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Giorgos Athineou &lt;athineou@csd.uoc.gr&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+MMPC.gee">MMPC.gee</a>, <a href="#topic+SES.gee">SES.gee</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("SES.glmm.output")
</code></pre>

<hr>
<h2 id='SES.glmm.output-class'>Class <code>"SES.glmm.output"</code></h2><span id='topic+SES.glmm.output-class'></span><span id='topic+SES.glmm.output'></span><span id='topic+plot+2CSES.glmm.output-method'></span><span id='topic+SES.glmm.output-method'></span><span id='topic+plot+2CSES.glmm.output+2CANY-method'></span>

<h3>Description</h3>

<p>SES.glmm output object class.
</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("SES.glmm.output", ...)</code>.
</p>


<h3>Slots</h3>


<dl>
<dt><code>selectedVars</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>selectedVarsOrder</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>queues</code>:</dt><dd><p>Object of class <code>"list"</code></p>
</dd>
<dt><code>signatures</code>:</dt><dd><p>Object of class <code>"matrix"</code></p>
</dd>
<dt><code>hashObject</code>:</dt><dd><p>Object of class <code>"list"</code></p>
</dd>
<dt><code>pvalues</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>stats</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>univ</code>:</dt><dd><p>Object of class <code>"list"</code></p>
</dd>
<dt><code>max_k</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>threshold</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>n.tests</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>runtime</code>:</dt><dd><p>Object of class <code>"proc_time"</code></p>
</dd>
<dt><code>test</code>:</dt><dd><p>Object of class <code>"character"</code></p>
</dd>
<dt><code>slope</code>:</dt><dd><p>Object of class <code>"logical"</code></p>
</dd>  </dl>



<h3>Methods</h3>


<dl>
<dt>plot</dt><dd><p><code>plot(x = "SES.glmm.output", mode = "all")</code>: Generic function for plotting the generated pvalues of the SES.glmm.output object. Argument mode = &quot;all&quot; for plotting all the pvalues or mode=&quot;partial&quot; for partial plotting the first 500 pvalues </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Giorgos Athineou &lt;athineou@csd.uoc.gr&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+MMPC.glmm">MMPC.glmm</a>, <a href="#topic+SES.glmm">SES.glmm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("SES.glmm.output")
</code></pre>

<hr>
<h2 id='SESoutput-class'>Class <code>"SESoutput"</code></h2><span id='topic+SESoutput-class'></span><span id='topic+SESoutput'></span><span id='topic+plot+2CSESoutput-method'></span><span id='topic+SESoutput-method'></span><span id='topic+plot+2CSESoutput+2CANY-method'></span>

<h3>Description</h3>

<p>SES output object class.
</p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("SESoutput", ...)</code>.
</p>


<h3>Slots</h3>


<dl>
<dt><code>selectedVars</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>selectedVarsOrder</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>queues</code>:</dt><dd><p>Object of class <code>"list"</code></p>
</dd>
<dt><code>signatures</code>:</dt><dd><p>Object of class <code>"matrix"</code></p>
</dd>
<dt><code>hashObject</code>:</dt><dd><p>Object of class <code>"list"</code></p>
</dd>
<dt><code>pvalues</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>stats</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>univ</code>:</dt><dd><p>Object of class <code>"list"</code></p>
</dd>
<dt><code>max_k</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>threshold</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>n.tests</code>:</dt><dd><p>Object of class <code>"numeric"</code></p>
</dd>
<dt><code>runtime</code>:</dt><dd><p>Object of class <code>"proc_time"</code></p>
</dd>
<dt><code>test</code>:</dt><dd><p>Object of class <code>"character"</code></p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>plot</dt><dd><p><code>plot(x = "SESoutput", mode = "all")</code>: Generic function for plotting the generated pvalues of the SESoutput object. Argument mode = &quot;all&quot; for plotting all the pvalues or mode=&quot;partial&quot; for partial plotting the first 500 pvalues </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Giorgos Athineou &lt;athineou@csd.uoc.gr&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SES">SES</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("SESoutput")
</code></pre>

<hr>
<h2 id='Skeleton+20+28local+29+20around+20a+20node+20of+20the+20max-min+20hill-climbing+20+28MMHC+29+20algorithm'>
Skeleton (local) around a node of the MMHC algorithm
</h2><span id='topic+local.mmhc.skel'></span>

<h3>Description</h3>

<p>The local skeleton of a Bayesian network around a node produced by MMHC. No orientations are involved.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>local.mmhc.skel(dataset, node, max_k = 3, threshold = 0.05, test = "testIndFisher") 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Skeleton+2B20+2B28local+2B29+2B20around+2B20a+2B20node+2B20of+2B20the+2B20max-min+2B20hill-climbing+2B20+2B28MMHC+2B29+2B20algorithm_+3A_dataset">dataset</code></td>
<td>

<p>A matrix with the variables. The user must know if they are continuous or if they are categorical. If you have a matrix with categorical data, i.e. 0, 1, 2, 3 where each number indicates a category, the minimum number for each variable must be 0. data.frame is also supported, as the dataset in this case is converted into a matrix.   
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20+2B28local+2B29+2B20around+2B20a+2B20node+2B20of+2B20the+2B20max-min+2B20hill-climbing+2B20+2B28MMHC+2B29+2B20algorithm_+3A_node">node</code></td>
<td>

<p>A number between 1 and the number of columns of the dataset. The local network (edges only) will be built around this node. At first the parents and children of this node are identified and then their parents and children. No inconsistencies correction whastsoever is attempted. A variable detected by the node, but the node was not detected by that variable.   
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20+2B28local+2B29+2B20around+2B20a+2B20node+2B20of+2B20the+2B20max-min+2B20hill-climbing+2B20+2B28MMHC+2B29+2B20algorithm_+3A_max_k">max_k</code></td>
<td>

<p>The maximum conditioning set to use in the conditional indepedence test (see Details of SES or MMPC). 
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20+2B28local+2B29+2B20around+2B20a+2B20node+2B20of+2B20the+2B20max-min+2B20hill-climbing+2B20+2B28MMHC+2B29+2B20algorithm_+3A_threshold">threshold</code></td>
<td>

<p>Threshold ( suitable values in (0, 1) ) for assessing p-values significance. Default value is 0.05.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20+2B28local+2B29+2B20around+2B20a+2B20node+2B20of+2B20the+2B20max-min+2B20hill-climbing+2B20+2B28MMHC+2B29+2B20algorithm_+3A_test">test</code></td>
<td>

<p>The conditional independence test to use. Default value is &quot;testIndFisher&quot;. This procedure allows for &quot;testIndFisher&quot;, &quot;testIndSPearman&quot; for continuous variables and &quot;gSquare&quot; for categorical variables. Or in general, if the dataset is a data.frame with different types of data, leave this NULL. See also <code>link{MMPC}</code> for the automatic choice of tests.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The MMPC is run on the user specific variable. The backward phase (see Tsamardinos et al., 2006) takes place automatically. Then, the MMPC is run on the parents and children of that variable. If the node variable is not detected by a variable, this variable is not removed though. 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>runtime</code></td>
<td>

<p>The run time of the algorithm. A numeric vector. The first element is the user time, the second element is the system time and the third element is the elapsed time.
</p>
</td></tr>
<tr><td><code>ntests</code></td>
<td>

<p>The number of tests MMPC (or SES) performed at each variable.
</p>
</td></tr>
<tr><td><code>res</code></td>
<td>

<p>A list with the parents and children of each variable. The first element is the parents and children of the node variable.
</p>
</td></tr>
<tr><td><code>Gloc</code></td>
<td>

<p>The local adjancency matrix. A value of 1 in G[i, j] may not appear appear in G[j, i] also, indicating that variable j was discovered as a possible parent or child of node i, but not the covnerse. The usual MMHC (<code><a href="#topic+mmhc.skel">mmhc.skel</a></code>) removes the edge between them as this is an inconsistency. 
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Tsamardinos, Brown and Aliferis (2006). The max-min hill-climbing Bayesian network structure learning algorithm. Machine learning, 65(1), 31-78.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+mmhc.skel">mmhc.skel</a>, <a href="#topic+pc.skel">pc.skel</a>, <a href="#topic+pc.or">pc.or</a>, <a href="#topic+corfs.network">corfs.network</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate a dataset with continuous data
dataset &lt;- matrix(runif(500 * 30, 1, 100), nrow = 500 ) 
a1 &lt;- mmhc.skel(dataset, max_k = 3, threshold = 0.05, test = "testIndFisher") 
a2 &lt;- local.mmhc.skel(dataset, 4)
a1$runtime  
a2$runtime 

dataset &lt;- rdag2(500, p = 20, nei = 3)$x
a1 &lt;- mmhc.skel(dataset, max_k = 3, threshold = 0.05, test = "testIndFisher") 
a2 &lt;- local.mmhc.skel(dataset, 5)
a1$runtime  
a2$runtime 

</code></pre>

<hr>
<h2 id='Skeleton+20of+20the+20max-min+20hill-climbing+20+28MMHC+29+20algorithm'>
The skeleton of a Bayesian network as produced by MMHC
</h2><span id='topic+mmhc.skel'></span><span id='topic+glmm.mmhc.skel'></span><span id='topic+gee.mmhc.skel'></span>

<h3>Description</h3>

<p>The skeleton of a Bayesian network produced by MMHC. No orientations are involved.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mmhc.skel(dataset, max_k = 3, threshold = 0.05, test = "testIndFisher", type = "MMPC",
hash = FALSE, backward = TRUE, symmetry = TRUE, nc = 1, ini.pvalue = NULL) 

glmm.mmhc.skel(dataset, group, max_k = 3, threshold = 0.05, test = "testIndLMM",
type = "MMPC.glmm", hash = FALSE, symmetry = TRUE, nc = 1, ini.pvalue = NULL)

gee.mmhc.skel(dataset, group, max_k = 3, threshold = 0.05, test = "testIndGEEReg",
type = "MMPC.gee", se = "jack", hash = FALSE, symmetry = TRUE, nc = 1, 
ini.pvalue = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20max-min+2B20hill-climbing+2B20+2B28MMHC+2B29+2B20algorithm_+3A_dataset">dataset</code></td>
<td>

<p>A matrix with the variables. The user must know if they are continuous or if they are categorical. If you have a matrix with categorical data, i.e. 0, 1, 2, 3 where each number indicates a category, the minimum number for each variable must be 0. For the &quot;glmm.mmhc.skel&quot; this must be only a matrix.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20max-min+2B20hill-climbing+2B20+2B28MMHC+2B29+2B20algorithm_+3A_group">group</code></td>
<td>

<p>This is to be used in the &quot;glmm.mmhc.skel&quot; and &quot;gee.mmhc.skel&quot; only. It is a vector for identifying the grouped data, the correlated observations, the subjects.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20max-min+2B20hill-climbing+2B20+2B28MMHC+2B29+2B20algorithm_+3A_max_k">max_k</code></td>
<td>

<p>The maximum conditioning set to use in the conditional indepedence test (see Details of SES or MMPC). 
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20max-min+2B20hill-climbing+2B20+2B28MMHC+2B29+2B20algorithm_+3A_threshold">threshold</code></td>
<td>

<p>Threshold ( suitable values in (0, 1) ) for assessing p-values significance. Default value is 0.05.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20max-min+2B20hill-climbing+2B20+2B28MMHC+2B29+2B20algorithm_+3A_test">test</code></td>
<td>

<p>The conditional independence test to use. Default value is &quot;testIndFisher&quot;. This procedure allows for &quot;testIndFisher&quot;, &quot;testIndSPearman&quot; for continuous variables and &quot;gSquare&quot; for categorical variables. In case the dataset is a data.frame with mixed types of data leave this NULL and an appropriate test will be selected. See <code><a href="#topic+MMPC">MMPC</a></code> for the automatic choice of tests.
</p>
<p>For the &quot;glmm.mmhc.skel&quot; the available tests in <code><a href="#topic+MMPC.glmm">MMPC.glmm</a></code>.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20max-min+2B20hill-climbing+2B20+2B28MMHC+2B29+2B20algorithm_+3A_type">type</code></td>
<td>

<p>The type of variable selection to take place for each variable (or node). The default (and standard) is &quot;MMPC&quot; and &quot;MMPC.glmm&quot;. You can also choose to run it via &quot;SES&quot; and &quot;SES.glmm&quot; and thus allow for multiple signatures of variables to be connected to a variable.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20max-min+2B20hill-climbing+2B20+2B28MMHC+2B29+2B20algorithm_+3A_se">se</code></td>
<td>

<p>The method for estimating standard errors. This is very important and crucial. The available options for Gaussian, Logistic, Poisson and Gamma regression are: a) 'san.se': the usual robust estimate. b) 'jack': approximate jackknife variance estimate. c) 'j1s': if 1-step jackknife variance estimate and d) 'fij': fully iterated jackknife variance estimate. If you have many clusters (sets of repeated measurements) &quot;san.se&quot; is fine as it is asympotically correct, plus jacknife estimates will take longer. If you have a few clusters, then maybe it's better to use jacknife estimates. 
</p>
<p>The jackknife variance estimator was suggested by Paik (1988), which is quite suitable for cases when the number of subjects is small (K &lt; 30), as in many biological studies. The simulation studies conducted by Ziegler et al. (2000) and Yan and Fine (2004) showed that the approximate jackknife estimates are in many cases in good agreement with the fully iterated ones. 
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20max-min+2B20hill-climbing+2B20+2B28MMHC+2B29+2B20algorithm_+3A_hash">hash</code></td>
<td>

<p>A boolean variable which indicates whether (TRUE) or not (FALSE). Default value is FALSE. If TRUE a hashObject is produced and hence more memory is required. If you want to know the number of tests executed for each variable then make it TRUE.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20max-min+2B20hill-climbing+2B20+2B28MMHC+2B29+2B20algorithm_+3A_backward">backward</code></td>
<td>

<p>If TRUE, the backward (or symmetry correction) phase will be implemented. This removes any falsely included variables in the parents and children set of the target variable. It call the <code><a href="#topic+mmpcbackphase">mmpcbackphase</a></code> for this purpose. For perm.ses and wald.ses this is not yet applicable.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20max-min+2B20hill-climbing+2B20+2B28MMHC+2B29+2B20algorithm_+3A_symmetry">symmetry</code></td>
<td>

<p>In order for an edge to be added, a statistical relationship must have been found from both directions. If you want this symmetry correction to take place, leave this boolean variable to TRUE. If you set it to FALSE, then if 
a relationship between Y and X is detected but not between X and Y, the edge is still added. 
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20max-min+2B20hill-climbing+2B20+2B28MMHC+2B29+2B20algorithm_+3A_nc">nc</code></td>
<td>

<p>How many cores to use. This plays an important role if you have many variables, say thousands or so. You can try with nc = 1 and with nc = 4 for example to see the differences. If you have a multicore machine, this is a must 
option. There was an extra argument for plotting the skeleton but it does not work with the current visualisation 
packages, hence we removed the argument. Use <code><a href="#topic+plotnetwork">plotnetwork</a></code> to plot the skeleton.   
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20max-min+2B20hill-climbing+2B20+2B28MMHC+2B29+2B20algorithm_+3A_ini.pvalue">ini.pvalue</code></td>
<td>

<p>This is a list with the matrix of the univariate p-values. If you want to run mmhc.skel again, the univariate associations need not be calculated again.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The MMPC is run on every variable. The backward phase (see Tsamardinos et al., 2006) takes place automatically. After all variables have been used, the matrix is checked for inconsistencies and they are corrected. 
</p>
<p>A trick mentioned in that paper to make the procedure faster is the following. In the k-th variable, the algorithm checks how many previously scanned variables have an edge with the this variable and keeps them (it discards the other variables with no edge) along with the next (unscanned) variables. 
</p>
<p>This trick reduces time, but can lead to different results. For example, if the i-th variable is removed, the k-th node might not remove an edge between the j-th variable, simply because the i-th variable that could d-sepate them is missing. 
</p>
<p>The user is given this option via the argument &quot;fast&quot;, which can be either TRUE or FALSE. Parallel computation is also available. 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>runtime</code></td>
<td>

<p>The run time of the algorithm. A numeric vector. The first element is the user time, the second element is the system time and the third element is the elapsed time.
</p>
</td></tr>
<tr><td><code>density</code></td>
<td>

<p>The number of edges divided by the total possible number of edges, that is #edges / <code class="reqn">n(n-1)/2</code>, where <code class="reqn">n</code> is the number of variables.
</p>
</td></tr>
<tr><td><code>info</code></td>
<td>

<p>Some summary statistics about the edges, minimum, maximum, mean, median number of edges.
</p>
</td></tr>
<tr><td><code>ms</code></td>
<td>

<p>If you run &quot;MMPC&quot; for each variable this is NULL. If you run &quot;SES&quot; is a vector denoting which variables had more than one signature, i.e. more than one set of variables associated with them.
</p>
</td></tr>
<tr><td><code>ntests</code></td>
<td>

<p>The number of tests MMPC (or SES) performed at each variable.
</p>
</td></tr>
<tr><td><code>ini.pvalue</code></td>
<td>

<p>A matrix with the p-values of all pairwise univariate assocations.
</p>
</td></tr>
<tr><td><code>pvalue</code></td>
<td>

<p>A matrix with the final p-values. These are the maximum p-values calculated during the process. When the process finishes, the matrix is not symmetric. 
It becomes symmetric though by keeping the maximum between any two off-diagonal elements. These p-values now can be used to sort the strength of the edges. 
If you know the true adjacency matrix you can use them and create a ROC curve (see <code><a href="#topic+bn.skel.utils">bn.skel.utils</a></code> for more information).
</p>
</td></tr>
<tr><td><code>G</code></td>
<td>

<p>The adjancency matrix. A value of 1 in G[i, j] appears in G[j, i] also, indicating that i and j have an edge between them.
</p>
</td></tr>
</table>
<p>Bear in mind that the values can be extracted with the $ symbol, i.e. this is an S3 class output. 
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Tsamardinos, Brown and Aliferis (2006). The max-min hill-climbing Bayesian network structure learning algorithm. Machine learning, 65(1), 31-78.
</p>
<p>Brown L. E., Tsamardinos I., and Aliferis C. F. (2004). A novel algorithm for scalable and accurate Bayesian network learning. Medinfo, 711-715.
</p>
<p>Tsamardinos, Ioannis, and Laura E. Brown. Bounding the False Discovery Rate in Local Bayesian Network Learning. AAAI, 2008.
</p>
<p>Eugene Demidenko (2013). Mixed Models: Theory and Applications with R, 2nd Edition. New Jersey: Wiley &amp; Sons.
</p>
<p>Paik M.C. (1988). Repeated measurement analysis for nonnormal data in small samples. Communications in Statistics-Simulation and Computation, 17(4): 1155-1171.
</p>
<p>Ziegler A., Kastner C., Brunner D. and Blettner M. (2000). Familial associations of lipid profiles: A generalised estimating equations approach. Statistics in medicine, 19(24): 3345-3357
</p>
<p>Yan J. and Fine J. (2004). Estimating equations for association structures. Statistics in medicine, 23(6): 859-874.
</p>
<p>Liang K.Y. and Zeger S.L. (1986). Longitudinal data analysis using generalized linear models. Biometrika, 73(1): 13-22.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+pc.skel">pc.skel</a>, <a href="#topic+pc.or">pc.or</a>, <a href="#topic+corfs.network">corfs.network</a>, <a href="#topic+bn.skel.utils">bn.skel.utils</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate a dataset with continuous data
y &lt;- rdag2(500, p = 20, nei = 3)
x &lt;- y$x
a &lt;- mmhc.skel(x, max_k = 5, threshold = 0.01, test = "testIndFisher" ) 
b &lt;- pc.skel( x, alpha = 0.01 ) 
a$runtime  
b$runtime 
</code></pre>

<hr>
<h2 id='Skeleton+20of+20the+20PC+20algorithm'>
The skeleton of a Bayesian network produced by the PC algorithm
</h2><span id='topic+pc.skel'></span><span id='topic+pc.con'></span><span id='topic+pc.skel.boot'></span><span id='topic+glmm.pc.skel'></span><span id='topic+gee.pc.skel'></span>

<h3>Description</h3>

<p>The skeleton of a Bayesian network produced by the PC algorithm. No orientations are involved. 
The pc.con is for continuous data only and it calls the same C++ as pc.skel. Hence, you are advised to use pc.skel. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pc.skel(dataset, method = "pearson", alpha = 0.01, rob = FALSE, R = 1, stat = NULL, 
ini.pvalue = NULL) 

pc.con(dataset, method = "pearson", alpha = 0.01) 

pc.skel.boot(dataset, method = "pearson", alpha = 0.01, R = 199, ncores = 1)

glmm.pc.skel(dataset, group, method = "comb.mm", alpha = 0.01, stat = NULL, 
ini.pvalue = NULL)

gee.pc.skel(dataset, group, se = "jack", method = "comb.mm", alpha = 0.01, stat = NULL, 
ini.pvalue = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20PC+2B20algorithm_+3A_dataset">dataset</code></td>
<td>

<p>A matrix with the variables. The user must know if they are continuous or if they are categorical. 
<b>If you have categorical data though, the user must transform the data.frame into a matrix. 
In addition, the numerical matrix must have values starting from 0. For example, 0, 1, 2, instead of &quot;A&quot;, &quot;B&quot; and &quot;C&quot;</b>. 
In the case of mixed variables, continuous, binary and ordinal this must a data.frame and the non continuous variables 
must be ordered factors, even the binary variables. 
</p>
<p>For the pc.con, pc.skel.boot and glmm.pc.skel, the dataset can only be a matrix.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20PC+2B20algorithm_+3A_method">method</code></td>
<td>

<p>If you have continuous data, you can choose either &quot;pearson&quot;, &quot;spearman&quot; or &quot;distcor&quot;. The latter uses the distance correlation and 
should not be used with lots of observations as it is by default really slow. If you have categorical data, this must be &quot;cat&quot;. 
If you have a mix of continuous, binary and ordinal data (we will expand the available dataset in the future) then choose 
&quot;comb.fast&quot; or &quot;comb.mm&quot;. These two methods perform the symmetric test for mixed data (Tsagris et al., 2018). See details for 
more information on this. For the mixed models PC algorithm, this is the same argument, but currently only &quot;comb.mm&quot; is acecpted.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20PC+2B20algorithm_+3A_group">group</code></td>
<td>

<p>This is to be used in the &quot;glmm.pc.skel&quot; and &quot;gee.pc.skel&quot; only. It is a vector for identifying the grouped data, the correlated 
observations, the subjects.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20PC+2B20algorithm_+3A_se">se</code></td>
<td>

<p>The method for estimating standard errors. This is very important and crucial. The available options for Gaussian, Logistic, Poisson 
and Gamma regression are: a) 'san.se': the usual robust estimate. b) 'jack': approximate jackknife variance estimate. c) 'j1s': if 
1-step jackknife variance estimate and d) 'fij': fully iterated jackknife variance estimate. If you have many clusters (sets of 
repeated measurements) &quot;san.se&quot; is fine as it is asympotically correct, plus jacknife estimates will take longer. If you have a 
few clusters, then maybe it's better to use jacknife estimates. 
</p>
<p>The jackknife variance estimator was suggested by Paik (1988), which is quite suitable for cases when the number of subjects is 
small (K &lt; 30), as in many biological studies. The simulation studies conducted by Ziegler et al. (2000) and Yan and Fine (2004) 
showed that the approximate jackknife estimates are in many cases in good agreement with the fully iterated ones. 
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20PC+2B20algorithm_+3A_alpha">alpha</code></td>
<td>

<p>The significance level ( suitable values in (0, 1) ) for assessing the p-values. Default value is 0.01.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20PC+2B20algorithm_+3A_rob">rob</code></td>
<td>

<p>This is for robust estimation of the Pearson correlation coefficient. Default value is FALSE.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20PC+2B20algorithm_+3A_r">R</code></td>
<td>

<p>The number of permutations to be conducted. This is taken into consideration for the &quot;pc.skel&quot; only. The Pearson correlation coefficient 
is calculated and the p-value is assessed via permutations. There was an extra argument for plotting the skeleton but it does not work 
with the current visualisation packages, hence we removed the argument. Use <code><a href="#topic+plotnetwork">plotnetwork</a></code> to plot the skeleton.  
</p>
<p>In the pc.skel.boot this is the number of bootstrap resamples to draw. The PC algorithm is performed in each bootstrap sample. In the end, 
the adjacency matrix on the observed data is returned, along with another adjacency matrix produced by the bootstrap. The latter one contains 
values from 0 to 1 indicating the proportion of times an edge between two nodes was present.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20PC+2B20algorithm_+3A_ncores">ncores</code></td>
<td>

<p>The number of cores to use. By default this is set to 1.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20PC+2B20algorithm_+3A_stat">stat</code></td>
<td>

<p>If you have the initial test statistics (univariate associations) values supply them here.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20PC+2B20algorithm_+3A_ini.pvalue">ini.pvalue</code></td>
<td>

<p>If you have the initial p-values of the univariate associations supply them here.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The PC algorithm as proposed by Spirtes et al. (2000) is implemented. The variables must be either continuous or categorical, only. 
The skeleton of the PC algorithm is order independent, since we are using the third heuristic (Spirte et al., 2000, pg. 90). 
At every ste of the alogirithm use the pairs which are least statistically associated. The conditioning set consists of variables 
which are most statistically associated with each either of the pair of variables. 
</p>
<p>For example, for the pair (X, Y) there can be two coniditoning sets for example (Z1, Z2) and (W1, W2). All p-values and test statistics 
and degrees of freedom have been computed at the first step of the algorithm. Take the p-values between (Z1, Z2) and (X, Y) and between 
(Z1, Z2) and (X, Y). The conditioning set with the minimum p-value is used first. If the minimum p-values are the same, use the second 
lowest p-value. In the event of 2 or more p-values being the same (with permutations for example), the test statistic divided by the 
degrees of freedom is used as a means of choosing which conditioning set is to be used first. If two or more p-values are below the 
machine epsilon (.Machine$double.eps which is equal to 2.220446e-16), all of them are set to 0. To make the comparison or the ordering 
feasible we use the logarithm of the p-value. Hence, the logarithm of the p-values is always calculated and used.
</p>
<p>In the case of the <code class="reqn">G^2</code> test of independence (for categorical data) we have incorporated a rule of thumb. I the number of samples 
is at least 5 times the number of the parameters to be estimated, the test is performed, otherwise, independence is not rejected 
(see Tsamardinos et al., 2006).
</p>
<p>The &quot;comb.fast&quot; and &quot;comb.mm&quot; methods are used with mixed variables, continuous, binary and ordinal. The &quot;comb.mm&quot; performs two 
log-likelihood ratio tests. For every pair of variables each of the two variables is treated as response and the suitable regression model 
is fitted. Then, two likelihood ratio tests are performed and the 2 p-values are combined in a meta-analytic way. In the case of 
&quot;comb.fast&quot; one regression model is fitted, the easiest (between the two) to implement. The ordering of the &quot;easiness&quot; is as follows: 
linear regression &gt; logistic regression &gt; ordinal regression.  
</p>
<p>The &quot;pc.con&quot; is a faster implementation of the PC algorithm but for continuous data only, without the robust option, unlike pc.skel 
which is more general and even for the continuous datasets slower. pc.con accepts only &quot;pearson&quot; and &quot;spearman&quot; as correlations. 
</p>
<p>If there are missing values they are placed by their median in case of continuous data and by their mode (most frequent value) if they 
are categorical. 
</p>
<p>The &quot;glmm.pc.skel&quot; and &quot;gee.pc.skel&quot; are designed for clustered or grouped data. It uses linear mixed models and works in the same way 
as the PC with mixed data. For each variable, a random intercepts model is fitted and the significance of the other variable is assessed. 
The two p-values are meta-analytically combined. 
</p>
<p>For all cases, we return the maximum logged p-value of the conditional independence tests between the pairs. This can be used to order 
the strength of association between pairs of variables. In addition, one can use it to estimate the AUC. See the example in <code><a href="#topic+bn.skel.utils">bn.skel.utils</a></code>.
</p>
<p>If you want to use the GEE methodology, make sure you load the library geepack first. 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>stat</code></td>
<td>

<p>The test statistics of the univariate associations. 
</p>
</td></tr>
<tr><td><code>ini.pvalue</code></td>
<td>

<p>The initival univariate associations p-values.
</p>
</td></tr>
<tr><td><code>pvalue</code></td>
<td>

<p>The logarithm of the maximum p-values from every conditional independence test. Note also, that if there is a log p-value smaller that 
the log of the threshold value that means there is an edge. This can be used to estimate the FDR (Tsamardinos and Brown, 2008). See 
details for more information. This is also an estimate of the strength of the association between pairs of variables. 
</p>
<p>At the moment this is not possible for method = c(&quot;pearson&quot;, &quot;spearman&quot;, &quot;cat&quot;) with R = 1, or R &gt; 1 because these cases are handled in 
C++. We will add those cases in the future. 
</p>
</td></tr>
<tr><td><code>info</code></td>
<td>

<p>Some summary statistics about the edges, minimum, maximum, mean, median number of edges.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The run time of the algorithm. A numeric vector. The first element is the user time, the second element is the system time and the 
third element is the elapsed time.
</p>
</td></tr>
<tr><td><code>kappa</code></td>
<td>

<p>The maximum value of k, the maximum cardinality of the conditioning set at which the algorithm stopped.
</p>
</td></tr>
<tr><td><code>density</code></td>
<td>

<p>The number of edges divided by the total possible number of edges, that is #edges / <code class="reqn">n(n-1)/2</code>, where <code class="reqn">n</code> is the number of variables.
</p>
</td></tr>
<tr><td><code>info</code></td>
<td>

<p>Some summary statistics about the edges, minimum, maximum, mean, median number of edges.
</p>
</td></tr>
<tr><td><code>G</code></td>
<td>

<p>The adjancency matrix. A value of 1 in G[i, j] appears in G[j, i] also, indicating that i and j have an edge between them.
</p>
</td></tr>
<tr><td><code>sepset</code></td>
<td>

<p>A list with the separating sets for every value of k.
</p>
</td></tr>
<tr><td><code>title</code></td>
<td>

<p>The name of the dataset. 
</p>
</td></tr>
</table>
<p>Bear in mind that the values can be extracted with the $ symbol, i.e. this is an S3 class output. 
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Spirtes P.,  Glymour C. and Scheines R. (2001). Causation, Prediction, and Search. The MIT Press, Cambridge, MA, USA, 3nd edition.
</p>
<p>Tsagris M. (2019). Bayesian network learning with the PC algorithm: an improved and correct variation. 
Applied Artificial Intelligence, 33(2): 101-123.
</p>
<p>Tsagris M., Borboudakis G., Lagani V. and Tsamardinos I. (2018). Constraint-based Causal Discovery with Mixed Data. 
International Journal of Data Science and Analytics, 6: 19-30. 
</p>
<p>Sedgewick, A. J., Ramsey, J. D., Spirtes, P., Glymour, C., &amp; Benos, P. V. (2017). Mixed Graphical Models for Causal Analysis of 
Multi-modal Variables. arXiv preprint arXiv:1704.02621.
</p>
<p>Szekely G.J. and Rizzo, M.L. (2014). Partial distance correlation with methods for dissimilarities. The Annals of Statistics, 42(6): 
2382&ndash;2412.
</p>
<p>Szekely G.J. and Rizzo M.L. (2013). Energy statistics: A class of statistics based on distances. Journal of Statistical Planning 
and Inference 143(8): 1249&ndash;1272.
</p>
<p>Paik M.C. (1988). Repeated measurement analysis for nonnormal data in small samples. Communications in Statistics-Simulation and 
Computation, 17(4): 1155-1171.
</p>
<p>Ziegler A., Kastner C., Brunner D. and Blettner M. (2000). Familial associations of lipid profiles: A generalised estimating 
equations approach. Statistics in Medicine, 19(24): 3345-3357
</p>
<p>Yan J. and Fine J. (2004). Estimating equations for association structures. Statistics in Medicine, 23(6): 859-874.
</p>
<p>Liang K.Y. and Zeger S.L. (1986). Longitudinal data analysis using generalized linear models. Biometrika, 73(1): 13-22.
</p>
<p>Eugene Demidenko (2013). Mixed Models: Theory and Applications with R, 2nd Edition. New Jersey: Wiley &amp; Sons.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+bn.skel.utils">bn.skel.utils</a>, <a href="#topic+mmhc.skel">mmhc.skel</a>, <a href="#topic+corfs.network">corfs.network</a>, <a href="#topic+local.mmhc.skel">local.mmhc.skel</a> 
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate a dataset with continuous data
y &lt;- rdag2(300, p = 20, nei = 3)
ind &lt;- sample(1:20, 20)
x &lt;- y$x[, ind]
a &lt;- mmhc.skel(x, max_k = 3, threshold = 0.05, test = "testIndFisher" ) 
b &lt;- pc.skel( x, method = "pearson", alpha = 0.05 ) 
a$runtime  
b$runtime  

</code></pre>

<hr>
<h2 id='Structural+20Hamming+20distance+20between+20two+20partially+20oriented+20DAGs'>
Structural Hamming distance between two partially oriented DAGs
</h2><span id='topic+shd'></span>

<h3>Description</h3>

<p>Structural Hamming distance between two partially oriented DAGs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shd(est, true) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Structural+2B20Hamming+2B20distance+2B20between+2B20two+2B20partially+2B20oriented+2B20DAGs_+3A_est">est</code></td>
<td>

<p>The first (partially oriented) DAG. This could also be the estimated DAG.
</p>
</td></tr>
<tr><td><code id="Structural+2B20Hamming+2B20distance+2B20between+2B20two+2B20partially+2B20oriented+2B20DAGs_+3A_true">true</code></td>
<td>

<p>The second (partially oriented) DAG. This could also be the equivalence class of the true DAG.  
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The structural Hamming distance as proposed by Tsamardinos et al. (2006) is calculated and returned. The cases are listed below
</p>

<table>
<tr>
 <td style="text-align: left;"> 
<b>True</b> </td><td style="text-align: left;">  <b>Estimated</b>  </td><td style="text-align: left;">  <b>Penalty</b>  </td>
</tr>
<tr>
 <td style="text-align: left;"> 
   -   </td><td style="text-align: left;">      </td><td style="text-align: left;">   1  </td>
</tr>
<tr>
 <td style="text-align: left;">   
       </td><td style="text-align: left;">  -   </td><td style="text-align: left;">   1  </td>
</tr>
<tr>
 <td style="text-align: left;">   
   -&gt;  </td><td style="text-align: left;">      </td><td style="text-align: left;">   1  </td>
</tr>
<tr>
 <td style="text-align: left;">   
       </td><td style="text-align: left;">  &lt;-  </td><td style="text-align: left;">   1  </td>
</tr>
<tr>
 <td style="text-align: left;">   
   -&gt;  </td><td style="text-align: left;">  -   </td><td style="text-align: left;">   1  </td>
</tr>
<tr>
 <td style="text-align: left;">   
   -   </td><td style="text-align: left;">  &lt;-  </td><td style="text-align: left;">   1  </td>
</tr>
<tr>
 <td style="text-align: left;">   
   -&gt;  </td><td style="text-align: left;">  &lt;-  </td><td style="text-align: left;">   1  </td>
</tr>
<tr>
 <td style="text-align: left;">        
</td>
</tr>

</table>



<h3>Value</h3>

<p>A list including
</p>
<table>
<tr><td><code>mat</code></td>
<td>

<p>A table with the agreements and disagreements between the two DAGs.
</p>
</td></tr>
<tr><td><code>shd</code></td>
<td>

<p>The structural Hamming distance.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Tsamardinos, Brown and Aliferis (2006). The max-min hill-climbing Bayesian network structure learning algorithm. Machine learning, 65(1), 31-78.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+pc.skel">pc.skel</a>, <a href="#topic+pc.or">pc.or</a>, <a href="#topic+mmhc.skel">mmhc.skel</a>, <a href="#topic+plotnetwork">plotnetwork</a> 
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rdag(1000, 20, 0.2)
tru &lt;- y$G 
mod &lt;- pc.skel(y$x)
a &lt;- pc.or(mod)
shd( a$G, dag2eg(tru) )
</code></pre>

<hr>
<h2 id='Supervised+20PCA'>
Supervised PCA
</h2><span id='topic+supervised.pca'></span>

<h3>Description</h3>

<p>Supervised PCA.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>supervised.pca(target, dataset, indices, center = TRUE, scale = TRUE, 
colours = NULL, graph = TRUE) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Supervised+2B20PCA_+3A_target">target</code></td>
<td>

<p>A numerical vector or a factor denoting the class of each sample, the response variable.
</p>
</td></tr>
<tr><td><code id="Supervised+2B20PCA_+3A_dataset">dataset</code></td>
<td>

<p>A matrix with numerical data (the predictor variables).
</p>
</td></tr>
<tr><td><code id="Supervised+2B20PCA_+3A_indices">indices</code></td>
<td>

<p>A vector with indices denoting whcih variables have been selected.
</p>
</td></tr>
<tr><td><code id="Supervised+2B20PCA_+3A_center">center</code></td>
<td>

<p>In the calculation of the PCA, should the data be centered? Default value is TRUE.
</p>
</td></tr>
<tr><td><code id="Supervised+2B20PCA_+3A_scale">scale</code></td>
<td>

<p>In the calculation of the PCA, should the data be scaled to unity variance? Default value is TRUE.
</p>
</td></tr>
<tr><td><code id="Supervised+2B20PCA_+3A_colours">colours</code></td>
<td>

<p>Should the colour of the points be defined by the target variable or do you want to pass your own colours? This must be a vector whose length is equal to the length of the target.
</p>
</td></tr>
<tr><td><code id="Supervised+2B20PCA_+3A_graph">graph</code></td>
<td>

<p>Should two graphs be returned? The scores of the frist two principal components based on all the data and based on the selected variables.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is not exactly the standard supervised PCA as suggested by Bair et al (2006). What we do here essentially is the following: PCA on all variables and on the variables selected by a variable selection algortihm. 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>mod.all</code></td>
<td>

<p>The output returned by <code><a href="stats.html#topic+prcomp">prcomp</a></code> applied to all variables.
</p>
</td></tr>
<tr><td><code>mode.sel</code></td>
<td>

<p>The output returned by <code><a href="stats.html#topic+prcomp">prcomp</a></code> applied to the selected variables.
</p>
</td></tr>
<tr><td><code>var.percent</code></td>
<td>

<p>The percentage of variance explained by the selected variables.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Bair E., Hastie T., Debashis P. and Tibshirani R. (2006). Prediction by supervised principal components. Journal of the American Statistical Association 101(473): 119&ndash;137.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+gomp">gomp</a>, <a href="#topic+fbed.reg">fbed.reg</a>, <a href="#topic+MMPC">MMPC</a> 
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix(iris[, 1:4])
target &lt;- iris[, 5]
supervised.pca(target, x, indices = 1:2)
</code></pre>

<hr>
<h2 id='Symmetric+20conditional+20independence+20test+20with+20clustered+20data'>
Symmetric conditional independence test with clustered data
</h2><span id='topic+glmm.ci.mm'></span><span id='topic+gee.ci.mm'></span>

<h3>Description</h3>

<p>Symmetric conditional independence test with clustered data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>glmm.ci.mm(ind1, ind2, cs = NULL, dat, group) 
gee.ci.mm(ind1, ind2, cs = NULL, dat, group, se = "jack") 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Symmetric+2B20conditional+2B20independence+2B20test+2B20with+2B20clustered+2B20data_+3A_ind1">ind1</code></td>
<td>

<p>The index of the one variable to be considered. 
</p>
</td></tr>
<tr><td><code id="Symmetric+2B20conditional+2B20independence+2B20test+2B20with+2B20clustered+2B20data_+3A_ind2">ind2</code></td>
<td>

<p>The index of the other variable to be considered. 
</p>
</td></tr>
<tr><td><code id="Symmetric+2B20conditional+2B20independence+2B20test+2B20with+2B20clustered+2B20data_+3A_cs">cs</code></td>
<td>

<p>The index or indices of the conditioning set of variable(s). If you have no variables set this equal to 0.
</p>
</td></tr>
<tr><td><code id="Symmetric+2B20conditional+2B20independence+2B20test+2B20with+2B20clustered+2B20data_+3A_dat">dat</code></td>
<td>

<p>A numerical matrix with data. 
</p>
</td></tr>
<tr><td><code id="Symmetric+2B20conditional+2B20independence+2B20test+2B20with+2B20clustered+2B20data_+3A_group">group</code></td>
<td>

<p>This is a numerical vector denoting the groups or the subjects.  
</p>
</td></tr>
<tr><td><code id="Symmetric+2B20conditional+2B20independence+2B20test+2B20with+2B20clustered+2B20data_+3A_se">se</code></td>
<td>

<p>The method for estimating standard errors. This is very important and crucial. The available options for Gaussian, Logistic and Poisson regression are: a) 'san.se': the usual robust estimate. b) 'jack': approximate jackknife variance estimate. c) 'j1s': if 1-step jackknife variance estimate and d) 'fij': fully iterated jackknife variance estimate. If you have many clusters (sets of repeated measurements) &quot;san.se&quot; is fine as it is asympotically correct, plus jacknife estimates will take longer. If you have a few clusters, then maybe it's better to use jacknife estimates. 
</p>
<p>The jackknife variance estimator was suggested by Paik (1988), which is quite suitable for cases when the number of subjects is small (K &lt; 30), as in many biological studies. The simulation studies conducted by Ziegler et al. (2000) and Yan and Fine (2004) showed that the approximate jackknife estimates are in many cases in good agreement with the fully iterated ones. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Two linear random intercept models are fitted, one for each variable and the p-value of the hypothesis test that the other variable is significant 
is calculated. These two p-values are combined in a meta-analytic way. The models fitted are either linear, logistic and Poisson regression.
</p>


<h3>Value</h3>

<p>A vector including the test statistic, it's associated p-value and the relevant degrees of freedom. 
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Tsagris M. (2019). Bayesian network learning with the PC algorithm: an improved and correct variation. 
Applied Artificial Intelligence, 33(2): 101-123.
</p>
<p>Tsagris M., Borboudakis G., Lagani V. and Tsamardinos I. (2018). Constraint-based Causal Discovery with Mixed Data. 
International Journal of Data Science and Analytics. 
</p>
<p>Paik M.C. (1988). Repeated measurement analysis for nonnormal data in small samples. Communications in Statistics-Simulation and Computation, 17(4): 1155-1171.
</p>
<p>Ziegler A., Kastner C., Brunner D. and Blettner M. (2000). Familial associations of lipid profiles: A generalised estimating equations approach. Statistics in medicine, 19(24): 3345-3357
</p>
<p>Yan J. and Fine J. (2004). Estimating equations for association structures. Statistics in medicine, 23(6): 859-874.
</p>
<p>Eugene Demidenko (2013). Mixed Models: Theory and Applications with R, 2nd Edition. New Jersey: Wiley &amp; Sons.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+pc.skel">pc.skel</a>, <a href="#topic+condi">condi</a>, <a href="#topic+testIndGLMMReg">testIndGLMMReg</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## we generate two independent vectors of clustered data
s1 &lt;- matrix(1.5, 4, 4)
diag(s1) &lt;- 2.5
s2 &lt;- matrix(1.5, 4, 4)
diag(s2) &lt;- 2
x1 &lt;- MASS::mvrnorm(10, rnorm(4), s1)  
x1 &lt;- as.vector( t(x1) )
x2 &lt;- MASS::mvrnorm(10, rnorm(4), s2)  
x2 &lt;- as.vector( t(x2) )
id &lt;- rep(1:10, each = 4)
glmm.ci.mm(1, 2, dat = cbind(x1,x2), group = id)
gee.ci.mm(1, 2, dat = cbind(x1,x2), group = id)
</code></pre>

<hr>
<h2 id='Symmetric+20conditional+20independence+20test+20with+20mixed+20data'>
Symmetric conditional independence test with mixed data
</h2><span id='topic+ci.mm'></span><span id='topic+ci.fast'></span><span id='topic+ci.mm2'></span><span id='topic+ci.fast2'></span>

<h3>Description</h3>

<p>Symmetric conditional independence test with mixed data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ci.mm(ind1, ind2, cs = NULL, dat, type, rob = FALSE, R = 1) 
ci.fast(ind1, ind2, cs = NULL, dat, type, rob = FALSE, R = 1)
ci.mm2(ind1, ind2, cs = NULL, suffStat) 
ci.fast2(ind1, ind2, cs = NULL, suffStat) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Symmetric+2B20conditional+2B20independence+2B20test+2B20with+2B20mixed+2B20data_+3A_ind1">ind1</code></td>
<td>

<p>The index of the one variable to be considered. 
</p>
</td></tr>
<tr><td><code id="Symmetric+2B20conditional+2B20independence+2B20test+2B20with+2B20mixed+2B20data_+3A_ind2">ind2</code></td>
<td>

<p>The index of the other variable to be considered. 
</p>
</td></tr>
<tr><td><code id="Symmetric+2B20conditional+2B20independence+2B20test+2B20with+2B20mixed+2B20data_+3A_cs">cs</code></td>
<td>

<p>The index or indices of the conditioning set of variable(s). If you have no variables set this equal to 0.
</p>
</td></tr>
<tr><td><code id="Symmetric+2B20conditional+2B20independence+2B20test+2B20with+2B20mixed+2B20data_+3A_dat">dat</code></td>
<td>

<p>A data.frame with numerical, binary, nominal and ordinal variables only. 
</p>
</td></tr>
<tr><td><code id="Symmetric+2B20conditional+2B20independence+2B20test+2B20with+2B20mixed+2B20data_+3A_type">type</code></td>
<td>

<p>This is obsolete basically, but we need it here, so that the functions ci.mm and ci.fast have the same signatures as in <code><a href="#topic+cat.ci">cat.ci</a></code>, <code><a href="#topic+condi">condi</a></code> and <code><a href="#topic+dist.condi">dist.condi</a></code>.
</p>
</td></tr>
<tr><td><code id="Symmetric+2B20conditional+2B20independence+2B20test+2B20with+2B20mixed+2B20data_+3A_rob">rob</code></td>
<td>

<p>This is obsolete basically, but we need it here, so that the functions ci.mm and ci.fast have the same signatures as in <code><a href="#topic+cat.ci">cat.ci</a></code>, <code><a href="#topic+condi">condi</a></code> and <code><a href="#topic+dist.condi">dist.condi</a></code>.
</p>
</td></tr>
<tr><td><code id="Symmetric+2B20conditional+2B20independence+2B20test+2B20with+2B20mixed+2B20data_+3A_r">R</code></td>
<td>

<p>This is obsolete basically, but we need it here, so that the functions ci.mm and ci.fast have the same signatures as in <code><a href="#topic+cat.ci">cat.ci</a></code>, <code><a href="#topic+condi">condi</a></code> and <code><a href="#topic+dist.condi">dist.condi</a></code>.
</p>
</td></tr>
<tr><td><code id="Symmetric+2B20conditional+2B20independence+2B20test+2B20with+2B20mixed+2B20data_+3A_suffstat">suffStat</code></td>
<td>

<p>This is a list with only the dataset and the name must be &quot;dataset&quot;. A data.frame with numerical, binary, nominal and ordinal variables only. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The functions &quot;ci.mm&quot; and &quot;ci.fast&quot; are general functions to be used anywhere. The functions &quot;ci.mm2&quot; and &quot;ci.fast2&quot; are designed to be accepted by the command &quot;pc&quot; in the package &quot;pcalg&quot;. The functions &quot;ci.mm2&quot; and &quot;ci.fast2&quot; can be fed in the &quot;pc&quot; function of the &quot;pcalg&quot; package in order to produce a PDAG with mixed data using the PC algorithm. For more information see the relevant paper in the references.
</p>
<p>The &quot;ci.mm&quot; and &quot;ci.fast&quot; work with linear, logistic, multinomial and ordinal regression, whereas the &quot;ci.mm2&quot; and &quot;ci.fast2&quot; work with linear, logistic and ordinal regression only.
</p>


<h3>Value</h3>

<p>A vector including the test statistic, it's associated p-value and the relevant degrees of freedom. 
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Tsagris M. (2019). Bayesian network learning with the PC algorithm: an improved and correct variation. 
Applied Artificial Intelligence, 33(2): 101-123.
</p>
<p>Tsagris M., Borboudakis G., Lagani V. and Tsamardinos I. (2018). Constraint-based Causal Discovery with Mixed Data. International Journal of Data Science and Analytics. 
</p>
<p>Spirtes P.,  Glymour C. and Scheines R. (2001). Causation, Prediction, and Search. The MIT Press, Cambridge, MA, USA, 3nd edition.
</p>
<p>Sedgewick, A. J., Ramsey, J. D., Spirtes, P., Glymour, C., &amp; Benos, P. V. (2017). Mixed Graphical Models for Causal Analysis of Multi-modal Variables. arXiv preprint arXiv:1704.02621.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+pc.skel">pc.skel</a>, <a href="#topic+mmhc.skel">mmhc.skel</a>, <a href="#topic+cat.ci">cat.ci</a>, <a href="#topic+condi">condi</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ci.mm(1, 2, dat = iris)
ci.mm(1, 5, dat = iris)
ci.fast(1, 5, dat = iris)
x &lt;- iris
x[, 5] &lt;- as.numeric(x[ ,5])  ## Caution:: this will be treated as ordered variable. 
x[, 5] &lt;- factor(x[, 5], ordered = TRUE)
## ci.mm2 and ci.fast2 do not perform multinomial regression.
ci.mm2(1, 5, suffStat = list(dataset = x) )
ci.fast2(1, 5, suffStat = list(dataset = x) )
</code></pre>

<hr>
<h2 id='The+20max-min+20Markov+20blanket+20algorithm'>
Max-min Markov blanket algorithm
</h2><span id='topic+mmmb'></span>

<h3>Description</h3>

<p>The MMMB algorithm follows a forward-backward filter approach for feature selection in order to provide a minimal, highly-predictive, feature subset of a high dimensional dataset. See also Details. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mmmb(target , dataset , max_k = 3 , threshold = 0.05 , test = "testIndFisher", 
user_test = NULL, ncores = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="The+2B20max-min+2B20Markov+2B20blanket+2B20algorithm_+3A_target">target</code></td>
<td>

<p>The class variable. Provide either a string, an integer, a numeric value, a vector, a factor, an ordered factor or a Surv object. See also Details.
</p>
</td></tr>
<tr><td><code id="The+2B20max-min+2B20Markov+2B20blanket+2B20algorithm_+3A_dataset">dataset</code></td>
<td>

<p>The dataset; provide either a data frame or a matrix (columns = variables, rows = samples). In either case, only two cases are avaialble, either all data are continuous, or categorical. 
</p>
</td></tr>
<tr><td><code id="The+2B20max-min+2B20Markov+2B20blanket+2B20algorithm_+3A_max_k">max_k</code></td>
<td>

<p>The maximum conditioning set to use in the conditional indepedence test (see Details). Integer, default value is 3.
</p>
</td></tr>
<tr><td><code id="The+2B20max-min+2B20Markov+2B20blanket+2B20algorithm_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0, 1)) for assessing the p-values. Default value is 0.05.
</p>
</td></tr>
<tr><td><code id="The+2B20max-min+2B20Markov+2B20blanket+2B20algorithm_+3A_test">test</code></td>
<td>

<p>The conditional independence test to use. Default value is &quot;testIndFisher&quot;. See also <code>link{CondIndTests}</code>.
</p>
</td></tr>
<tr><td><code id="The+2B20max-min+2B20Markov+2B20blanket+2B20algorithm_+3A_user_test">user_test</code></td>
<td>

<p>A user-defined conditional independence test (provide a closure type object). Default value is NULL. If this is defined, the &quot;test&quot; argument is ignored.
</p>
</td></tr>
<tr><td><code id="The+2B20max-min+2B20Markov+2B20blanket+2B20algorithm_+3A_ncores">ncores</code></td>
<td>

<p>How many cores to use. This plays an important role if you have tens of thousands of variables or really large sample sizes and tens of thousands of variables and a regression based test which requires numerical optimisation. In other cammmb it will not make a difference in the overall time (in fact it can be slower). The parallel computation is used in the first step of the algorithm, where univariate associations are examined, those take place in parallel. We have seen a reduction in time of 50% with 4 cores in comparison to 1 core. Note also, that the amount of reduction is not linear in the number of cores.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The idea is to run the MMPC algorithm at first and identify the parents and children (PC) of the target variable. As a second step, the MMPC algorithm is run on the discovered variables to return PC. The parents of the children of the target are the spouses of the target. Every variable in PCi is checked to see if it is a spouse of the target. If yes, it is included in the Markov Blanket of the target, otherwise it is thrown. If the data are continous, the Fisher correlation test is used or the Spearman correlation (more robust). If the data are categorical, the <code class="reqn">G^2</code> test is used. 
</p>


<h3>Value</h3>

<p>The output of the algorithm is an S3 object including:
</p>
<table>
<tr><td><code>mb</code></td>
<td>

<p>The Markov Blanket of the target variable. The parents and children of the target variable, along with the spouses of the target, which are the parents of the children of the target variable.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The run time of the algorithm. A numeric vector. The first element is the user time, the second element is the system time and the third element is the elapsed time.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Tsamardinos I., Aliferis C. F. and Statnikov, A. (2003). Time and sample efficient discovery of Markov blankets and direct causal relations. In Proceedings of the 9th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 673-678.
</p>




<h3>See Also</h3>

<p><code><a href="#topic+CondIndTests">CondIndTests</a>, <a href="#topic+MMPC">MMPC</a>, <a href="#topic+SES">SES</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)
#simulate a dataset with continuous data
dataset &lt;- matrix( runif(200 * 50, 1, 100), ncol = 50 )
#define a simulated class variable 
target &lt;- 3 * dataset[, 10] + 2 * dataset[, 50] + 3 * dataset[, 20] + rnorm(200, 0, 5)
a1 &lt;- mmmb(target , dataset , max_k = 3 , threshold = 0.05, test= "testIndFisher", 
ncores = 1,)
a2 &lt;- MMPC(target, dataset, test="testIndFisher")
</code></pre>

<hr>
<h2 id='Topological+20sort+20of+20a+20DAG'>
Topological sort of a DAG
</h2><span id='topic+topological_sort'></span>

<h3>Description</h3>

<p>Topological sort of a DAG.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>topological_sort(dag)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Topological+2B20sort+2B20of+2B20a+2B20DAG_+3A_dag">dag</code></td>
<td>

<p>A square matrix representing a directed graph which contains 0s and 1s. If G[i, j] = 1 it 
means there is an arrow from node i to node j. When there is no edge between nodes i and j if G[i, j] = 0.  
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function is an R translation from an old matlab code.
</p>


<h3>Value</h3>

<p>A vector with numbers indicating the sorting. If the matrix does not correspond to a DAG, NA will be returned. 
</p>


<h3>Author(s)</h3>

<p>Ioannis Tsamardinos and Michail Tsagris
</p>
<p>R implementation and documentation:  Ioannis Tsamardinos &lt;tsamard@csd.uoc.gr&gt; and Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Chickering, D.M. (1995). A transformational characterization of equivalent Bayesian network structures. 
Proceedings of the 11th Conference on Uncertainty in Artificial Intelligence, Montreal, Canada, 87-98. 
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+plotnetwork">plotnetwork</a>, <a href="#topic+nei">nei</a>, <a href="#topic+pc.or">pc.or</a> 
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate a dataset with continuous data
# simulate a dataset with continuous data
G = rdag(100, 10, 0.3)$G
G[G == 2] &lt;- 1
G[G == 3] &lt;- 0
topological_sort(G)
</code></pre>

<hr>
<h2 id='Total+20causal+20effect+20of+20a+20node+20on+20another+20node'>
Total causal effect of a node on another node
</h2><span id='topic+ida'></span>

<h3>Description</h3>

<p>Total causal effect of a node on another node.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ida(x, y, G, dataset)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Total+2B20causal+2B20effect+2B20of+2B20a+2B20node+2B20on+2B20another+2B20node_+3A_x">x</code></td>
<td>

<p>A number between 1 and the number of variables (nodes) indicating the node whose total causal effect we want to estimate. This is the independent variable. See details for more on this.
</p>
</td></tr>
<tr><td><code id="Total+2B20causal+2B20effect+2B20of+2B20a+2B20node+2B20on+2B20another+2B20node_+3A_y">y</code></td>
<td>

<p>A number between 1 and the number of variables (nodes) indicating the node who is the dependent variable. The goal is to estimate the total 
causal effect of x on y.
</p>
</td></tr>
<tr><td><code id="Total+2B20causal+2B20effect+2B20of+2B20a+2B20node+2B20on+2B20another+2B20node_+3A_g">G</code></td>
<td>

<p>A square matrix representing a (partially) directed graph which contains 0s and 1s. If G[i, j] = 2 it 
means there is an arrow from node i to node j. If G[i, j] = 1, there is an undirected edge between nodes i and j and there is no edge between nodes i and j if G[i, j] = 0.  
</p>
</td></tr>
<tr><td><code id="Total+2B20causal+2B20effect+2B20of+2B20a+2B20node+2B20on+2B20another+2B20node_+3A_dataset">dataset</code></td>
<td>

<p>The dataset. This is a numerical matrix with data.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The total causal effect defined in Pearl's do-calculus is E(Y|do(X=z+1)) - E(Y|do(X=z)). As Pearl described it, he used linear regression, hence this funciton works for continuous data which are assumed to be Gaussian.
</p>
<p>We estimate a set of possible total causal effects using linear regression. if y is a parent, or a descendant, of x in G, the estimated causal effect of x on y is zero. If y is not a parent of x, we take the regression coefficient of x in the regression lm(y ~ x + pa(x)), where pa(x) denotes the parents of x. This is repeated using all parents of x (including the empty set) and all possible parents values of x and their combinations.
</p>
<p>One restriction to bear in mind. If a collider is created that combination of nodes is not used in the regression. 
</p>


<h3>Value</h3>

<p>A lis tincluding:
</p>
<table>
<tr><td><code>tc</code></td>
<td>

<p>A matrix with 4 elements. The first columns one is the estimated beta coefficient, the second is its standard error, its t-value and the p-value for testing whether this is equal to 0.
</p>
</td></tr>
<tr><td><code>mess</code></td>
<td>

<p>If the x node has no parents a message about this appears. Otherwise this is NULL.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>M.H. Maathuis, M. Kalisch and P. Buehlmann (2009). Estimating high-dimensional intervention effects from observational data. Annals of Statistics 37, 3133-3164.
</p>
<p>Pearl (2005). Causality. Models, reasoning and inference. Cambridge University Press, New York.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+plotnetwork">plotnetwork</a>, <a href="#topic+nei">nei</a>, <a href="#topic+pc.or">pc.or</a> 
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dataset &lt;- rdag2(1000, p = 20, nei = 3)$x
mod &lt;- pc.skel(dataset ,alpha = 0.01)
G &lt;- pc.or(mod)$G
ida(10, 15, G, dataset)
</code></pre>

<hr>
<h2 id='Transformation+20of+20a+20DAG+20into+20an+20essential+20graph'>
Transforms a DAG into an essential graph
</h2><span id='topic+dag2eg'></span>

<h3>Description</h3>

<p>Transforms a DAG into an essential graph.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dag2eg(dag, type = NULL) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Transformation+2B20of+2B20a+2B20DAG+2B20into+2B20an+2B20essential+2B20graph_+3A_dag">dag</code></td>
<td>

<p>The graph matrix as produced from <code><a href="#topic+pc.or">pc.or</a></code> or any other algorithm which produces directed graphs. A DAG in general. 
</p>
</td></tr>
<tr><td><code id="Transformation+2B20of+2B20a+2B20DAG+2B20into+2B20an+2B20essential+2B20graph_+3A_type">type</code></td>
<td>

<p>This can be either NULL or 1 or 2. type = 1 means that the matrix contains 0, 1, 2, 3 where G[i, j] = g[j, i] = 0, means there is no edge between nodes 
i and j, G[i, j] = g[j, i] = 1, there is an edge between nodes i and j and G[i, j] = 2 and G[j, i] = 3 means that there is an arrow from node i to node j. 
If type 2, the matrix contains 0 for no edge and 1 for a directed edge. In this case, G[i,j]=1 and G[j,i]=0 means that there is an arrow from node i to 
node j. If you are not sure of what you have, just leave it NULL, the function will check to which case your matrix belongs.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function is an R translation from an old matlab code.
</p>


<h3>Value</h3>

<p>The matrix of the essential graph.
</p>


<h3>Author(s)</h3>

<p>Ioannis Tsamardinos and Michail Tsagris
</p>
<p>R implementation and documentation: Ioannis Tsamardinos &lt;tsamard@csd.uoc.gr&gt; and  and Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Chickering, D.M. (1995). A transformational characterization of equivalent Bayesian network structures. Proceedings of the 11th Conference on Uncertainty in Artificial Intelligence, Montreal, Canada, 87-98. 
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+plotnetwork">plotnetwork</a>, <a href="#topic+is.dag">is.dag</a>, <a href="#topic+topological_sort">topological_sort</a> 
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate a dataset with continuous data
y &lt;- rdag(1000, 10, 0.3)
tru &lt;- y$G 
eg &lt;- dag2eg(tru)
par( mfrow = c(1, 2) )
plotnetwork(tru)
plotnetwork(eg)
</code></pre>

<hr>
<h2 id='Transitive+20closure+20of+20an+20adjacency+20matrix'>
Returns the transitive closure of an adjacency matrix
</h2><span id='topic+transitiveClosure'></span>

<h3>Description</h3>

<p>Returns the transitive closure of an adjacency matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>transitiveClosure(amat)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Transitive+2B20closure+2B20of+2B20an+2B20adjacency+2B20matrix_+3A_amat">amat</code></td>
<td>

<p>The adjacency matrix of a graph. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A function that computes the transitive closure of a graph. The transitive closure C(G) of a graph is a graph which contains an edge between nodes u and v whenever there is a directed path from u to v (Skiena 1990, p. 203).
http://mathworld.wolfram.com/TransitiveClosure.html
</p>


<h3>Value</h3>

<table>
<tr><td><code>closure</code></td>
<td>

<p>The transititve closure of the adjacency matrix representing a graph.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Anna Roumpelaki
</p>
<p>R implementation and documentation: Anna Roumpelaki &lt;anna.roumpelaki@gmail.com&gt;
</p>


<h3>References</h3>

<p>Skiena S. (1990). Implementing Discrete Mathematics: Combinatorics and Graph Theory with Mathematica. Reading, MA: Addison-Wesley 
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example adjacency matrix
# simulate a dataset with continuous data
dataset &lt;- matrix( runif(300 * 20, 1, 100), nrow = 300 ) 
test &lt;- pc.con( dataset, method = "pearson", alpha = 0.05 )$G 
transitiveClosure(test)
</code></pre>

<hr>
<h2 id='Undirected+20path+28s+29+20between+20two+20nodes'>
Undirected path(s) between two nodes
</h2><span id='topic+undir.path'></span>

<h3>Description</h3>

<p>Undirected path(s) between two nodes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>undir.path(G, y, x) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Undirected+2B20path+2B28s+2B29+2B20between+2B20two+2B20nodes_+3A_g">G</code></td>
<td>

<p>An adjacency matrix where G[i,j] = G[j, i] = 1 means there is an edge between nodes i and j. If G[i, j] = G[j, i] = 0 there is no edge between them.     
</p>
</td></tr>
<tr><td><code id="Undirected+2B20path+2B28s+2B29+2B20between+2B20two+2B20nodes_+3A_y">y</code></td>
<td>

<p>A numerical value indicating the first node, it has to be a number between 1 and the maximum number of variables.
</p>
</td></tr>
<tr><td><code id="Undirected+2B20path+2B28s+2B29+2B20between+2B20two+2B20nodes_+3A_x">x</code></td>
<td>

<p>A numerical value indicating the second node, it has to be a number between 1 and the maximum number of variables. The order of the nodes does not make a difference.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithm finds all the nodes between the two nodes. It finds all paths between the two chosen nodes.
</p>


<h3>Value</h3>

<p>A vector with the two nodes and all nodes between them in the case of connecting nodes. Otherwise, a matrix with the neighbours of each node.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Tsamardinos, Brown and Aliferis (2006). The max-min hill-climbing Bayesian network structure learning algorithm. Machine learning, 65(1), 31-78.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+SES">SES</a>, <a href="#topic+MMPC">MMPC</a>, <a href="#topic+pc.skel">pc.skel</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate a dataset with continuous data
set.seed(1234)
dataset &lt;- matrix(runif(1000 * 10, 1, 100), nrow = 1000 ) 
G &lt;- pc.con(dataset)$G
plotnetwork(G)
undir.path(G, 3, 4)
undir.path(G, 1, 3)
</code></pre>

<hr>
<h2 id='Univariate+20regression+20based+20tests'>
Univariate regression based tests
</h2><span id='topic+univregs'></span><span id='topic+ebic.univregs'></span><span id='topic+perm.univregs'></span><span id='topic+wald.univregs'></span><span id='topic+score.univregs'></span><span id='topic+big.score.univregs'></span><span id='topic+rint.regs'></span><span id='topic+glmm.univregs'></span><span id='topic+gee.univregs'></span>

<h3>Description</h3>

<p>Univariate regression based tests.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>univregs(target, dataset, targetID = -1, test = NULL, user_test = NULL, 
wei = NULL, ncores = 1) 

ebic.univregs(target, dataset, targetID = -1, test = NULL, user_test = NULL, 
wei = NULL, ncores = 1, gam = NULL) 

wald.univregs(target, dataset, targetID = - 1, test = NULL, user_test = NULL, 
wei = NULL, ncores = 1)

perm.univregs(target, dataset, targetID = -1, test = NULL, user_test = NULL, 
wei = NULL, threshold = 0.05, R = 999, ncores = 1) 

score.univregs(target, dataset, test) 

big.score.univregs(target = NULL, dataset, test) 

rint.regs(target, dataset, targetID = -1, id, reps = NULL, tol = 1e-07)

glmm.univregs(target, reps = NULL, id, dataset, targetID = -1, test, wei = NULL, 
slopes = FALSE, ncores = 1)

gee.univregs(target, reps = NULL, id, dataset, targetID = -1, test, wei = NULL, 
correl = "echangeable", se = "jack", ncores = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Univariate+2B20regression+2B20based+2B20tests_+3A_target">target</code></td>
<td>

<p>The target (dependent) variable. It must be a numerical vector. In the case of &quot;big.score.univregs&quot; this can also be NULL if it is included in the first column of the dataset.
</p>
</td></tr>
<tr><td><code id="Univariate+2B20regression+2B20based+2B20tests_+3A_dataset">dataset</code></td>
<td>

<p>The indendent variable(s). For the &quot;univregs&quot; this can be a matrix or a dataframe with continuous only variables, a data frame with mixed or only categorical variables. For the &quot;wald.univregs&quot;, &quot;perm.univregs&quot;, &quot;score.univregs&quot; and &quot;rint.regs&quot; this can only by a numerical matrix. For the &quot;big.score.univregs&quot; this is a big.matrix object with continuous data only.
</p>
</td></tr>
<tr><td><code id="Univariate+2B20regression+2B20based+2B20tests_+3A_targetid">targetID</code></td>
<td>

<p>This is by default -1. If the target is not a variable but is in included in the dataset, you can specify the column of dataset playing the role of the target. 
</p>
</td></tr>
<tr><td><code id="Univariate+2B20regression+2B20based+2B20tests_+3A_test">test</code></td>
<td>

<p>For the &quot;univregs&quot; this can only be one of the following: <b>testIndFisher</b>, <b>testIndSpearman</b>, <b>gSquare</b>, <b>testIndBeta</b>, <b>testIndReg</b>, <b>testIndLogistic</b>, <b>testIndMultinom</b>, <b>testIndOrdinal</b>, 
<b>testIndPois</b>, <b>testIndZIP</b>, <b>testIndNB</b>, <b>testIndClogit</b>, <b>testIndBinom</b>, <b>testIndIGreg</b>, <b>censIndCR</b>, <b>censIndWR</b>, <b>censIndER</b>, <b>censIndLLR</b>, <b>testIndTobit</b>, <b>testIndGamma</b>, <b>testIndNormLog</b> or <b>testIndSPML</b> for a circular target. For the <b>testIndSpearman</b> the user must supply the ranked target and 
ranked dataset. The reason for this, is because this function is called internally by SES and MMPC, the ranks have 
already been applied and there is no reason to re-rank the data. 
</p>
<p>For the &quot;ebic.univregs&quot; this can only be one of the following: <b>testIndFisher</b>, <b>testIndBeta</b>, <b>testIndReg</b>, <b>testIndLogistic</b>, <b>testIndMultinom</b>, <b>testIndOrdinal</b>, <b>testIndPois</b>, <b>testIndZIP</b>, <b>testIndNB</b>, <b>testIndClogit</b>, <b>testIndBinom</b>, <b>censIndCR</b>, <b>censIndWR</b>, <b>censIndER</b>, <b>censIndLLR</b>, <b>testIndTobit</b>, <b>testIndGamma</b>, <b>testIndNormLog</b> or <b>testIndSPML</b> for a circular target.
</p>
<p>For the &quot;wald.univregs&quot; this can only be one of the following: <b>waldBeta</b>, <b>waldCR</b>, <b>waldWR</b>, 
<b>waldER</b>, <b>waldLLR</b>, <b>waldTobit</b>, <b>waldClogit</b>, <b>waldLogistic</b>, <b>waldPois</b>, <b>waldNB</b>, <b>waldBinom</b>, <b>waldZIP</b>, <b>waldMMReg</b>, <b>waldIGreg</b>, <b>waldOrdinal</b>, <b>waldGamma</b> or <b>waldNormLog</b>.
</p>
<p>For the &quot;perm.univregs&quot; this can only be one of the following: <b>permFisher</b>, <b>permReg</b>, <b>permRQ</b>, 
<b>permBeta</b>, <b>permCR</b>, <b>permWR</b>, <b>permER</b>, <b>permLLR</b>, <b>permTobit</b>, <b>permClogit</b>, <b>permLogistic</b>, <b>permPois</b>, <b>permNB</b>, <b>permBinom</b>, <b>permgSquare</b>, <b>permZIP</b>, <b>permMVreg</b>, <b>permIGreg</b>, <b>permGamma</b> or <b>permNormLog</b>.
</p>
<p>For the &quot;score.univregs&quot; and &quot;big.score.univregs&quot; this can only be one of the following: <b>testIndBeta</b>, 
<b>testIndLogistic</b>, <b>testIndMultinom</b>, <b>testIndPois</b>, <b>testIndNB</b>, <b>testIndGamma</b> or 
<b>censIndWR</b> but with no censored values and simply the vector, 
not a Surv object. Ordinal regression is not supported. 
</p>
<p>For the &quot;glmm.univregs&quot; this can only be one of the following: <b>testIndGLMMReg</b>, <b>testIndLMM</b>, 
<b>testIndGLMMLogistic</b>, <b>testIndGLMMOrdinal</b>, <b>testIndGLMMPois</b>, <b>testIndGLMMNB</b>, <b>testIndGLMMGamma</b>, 
<b>testIndGLMMNormLog</b> or <b>testIndGLMMCR</b>. 
</p>
<p>For the &quot;gee.univregs&quot; this can only be one of the following: <b>testIndGLMMReg</b>, <b>testIndGLMMLogistic</b>, 
<b>testIndGLMMOrdinal</b>, <b>testIndGLMMPois</b>, <b>testIndGLMMGamma</b> or <b>testIndGLMMNormLog</b>. 
</p>
<p><b>Note that in all cases you must give the name of the test, without &quot; &quot;</b>. 
</p>
</td></tr>
<tr><td><code id="Univariate+2B20regression+2B20based+2B20tests_+3A_user_test">user_test</code></td>
<td>

<p>A user-defined conditional independence test (provide a closure type object). Default value is NULL. If this is defined, the &quot;test&quot; 
argument is ignored.
</p>
</td></tr>
<tr><td><code id="Univariate+2B20regression+2B20based+2B20tests_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. An example where weights are used is surveys when stratified sampling has occured. 
</p>
</td></tr>
<tr><td><code id="Univariate+2B20regression+2B20based+2B20tests_+3A_gam">gam</code></td>
<td>

<p>This is for ebic.univregs only and it refers to the <code class="reqn">gamma</code> value of the eBIC. If it is NULL, the default value is calculated and if gam is zero, the usual BIC is returned. see Chen and Chen (2008) for more information.
</p>
</td></tr>
<tr><td><code id="Univariate+2B20regression+2B20based+2B20tests_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0, 1)) for assessing p-values significance. Default value is 0.05. The reason for this is to speed up the computations. 
The permutation p-value is calculated as the proportion of times the permuted test statistic is higher than the observed test statistic. When running the
permutations, if the proportion is more than 0.05, the process stops. A decision must be made fast, and if the non rejection decision has been made, there 
is no need to perform the rest permutations; the decision cannot change.
</p>
</td></tr>
<tr><td><code id="Univariate+2B20regression+2B20based+2B20tests_+3A_r">R</code></td>
<td>

<p>This is for the permutations based regression models. It is the number of permutations to apply. Note, that not all the number of permutations need be performed. If the number of times the test statistic is greater than the observed test statistic is more than threshold * R, the iterations stop, as a decision has already been made. 
</p>
</td></tr>
<tr><td><code id="Univariate+2B20regression+2B20based+2B20tests_+3A_ncores">ncores</code></td>
<td>

<p>How many cores to use. This plays an important role if you have tens of thousands of variables or really large sample sizes and tens of thousands of variables and a regression based test which requires numerical optimisation. In other cases it will not make a difference in the overall time (in fact it can be slower). The parallel computation is used in the first step of the algorithm, where univariate associations are examined, those take place in parallel. We have seen a reduction in time of 50% with 4 cores in comparison to 1 core. Note also, that the amount of reduction is not linear in the number of cores.
</p>
</td></tr>
<tr><td><code id="Univariate+2B20regression+2B20based+2B20tests_+3A_id">id</code></td>
<td>

<p>A numerical vector of the same length as target with integer valued numbers, such as 1, 2, 3,... (zeros, negative values and factors are not allowed) specifying the clusters or subjects. This argument is for the rint.regs (see details for more information).
</p>
</td></tr>
<tr><td><code id="Univariate+2B20regression+2B20based+2B20tests_+3A_reps">reps</code></td>
<td>

<p>If you have measurements over time (lognitudinal data) you can put the time here (the length must be equal to the length of the target) or set it equal to NULL. (see details for more information).
</p>
</td></tr>
<tr><td><code id="Univariate+2B20regression+2B20based+2B20tests_+3A_tol">tol</code></td>
<td>

<p>The tolerance value to terminate the Newton-Raphson algorithm in the random effects models.
</p>
</td></tr>
<tr><td><code id="Univariate+2B20regression+2B20based+2B20tests_+3A_slopes">slopes</code></td>
<td>

<p>Should random slopes for the ime effect be fitted as well? Default value is FALSE. 
</p>
</td></tr>
<tr><td><code id="Univariate+2B20regression+2B20based+2B20tests_+3A_correl">correl</code></td>
<td>

<p>The correlation structure. For the Gaussian, Logistic, Poisson and Gamma regression this can be either &quot;exchangeable&quot; (compound symmetry, suitable for clustered data) or &quot;ar1&quot; (AR(1) model, suitable for longitudinal data). For the ordinal logistic regression its only the &quot;exchangeable&quot; correlation sturcture.  
</p>
</td></tr>
<tr><td><code id="Univariate+2B20regression+2B20based+2B20tests_+3A_se">se</code></td>
<td>

<p>The method for estimating standard errors. This is very important and crucial. The available options for Gaussian, Logistic, Poisson and Gamma regression are: a)  'san.se', the usual robust estimate. b) 'jack': if approximate jackknife variance estimate should be computed. 
c) 'j1s': if 1-step jackknife variance estimate should be computed and d) 'fij': logical indicating if fully iterated jackknife variance estimate should be computed. If you have many clusters (sets of repeated measurements) &quot;san.se&quot; is fine as it is astmpotically correct, plus jacknife estimates will take longer. If you have a few clusters, then maybe it's better to use jacknife estimates. 
</p>
<p>The jackknife variance estimator was suggested by Paik (1988), which is quite suitable for cases when the number of subjects is small (K &lt; 30), as in many biological studies. The simulation studies conducted by Ziegler et al. (2000) and Yan and Fine (2004) showed that the approximate jackknife estimates are in many cases in good agreement with the fully iterated ones. This is obsolete for &quot;testIndGEEOrdinal&quot;, but is here for compatibility reasons.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is more as a help function for SES and MMPC, but it can also be called directly by the user. In some, one should specify the regression model to use and the function will perform all simple regressions, i.e. all regression models between the target and each of the variables in the dataset. 
</p>
<p>For the score.univregs, score based tests are used which are extremely fast. 
</p>
<p>For the rint.regs, we perform linear mixed models (weights are not allowed) with random intercepts only (no ranodm slopes). This function works for clustered or longitudinal data. The covariance structure we impose is compound symmetry, hence for longitudinal data, this may not be the best option, yet it will work. 
</p>
<p>If you want to use the GEE methodology, make sure you load the library geepack first. 
</p>


<h3>Value</h3>

<p>In the case of &quot;ebic.univregs&quot; a list with one element 
</p>
<table>
<tr><td><code>ebic</code></td>
<td>
 
<p>The eBIc of every model. If the i-th value is &quot;Inf&quot; it means that the i-th variable had zero variance. It had the same value everywhere.
</p>
</td></tr>
</table>
<p>For all other cases a list including:
</p>
<table>
<tr><td><code>stat</code></td>
<td>

<p>The value of the test statistic. If the i-th value is zero (0) it means that the i-th variable had zero variance. It had the same value everywhere.
</p>
</td></tr>
<tr><td><code>pvalue</code></td>
<td>

<p>The <b>logarithm of the p-value</b> of the test. If the i-th value is zero (0) it means that the i-th variable had zero variance. It had the same value everywhere.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Chen J. and Chen Z. (2008). Extended Bayesian information criteria for model selection with large model spaces. 
Biometrika, 95(3): 759-771.
</p>
<p>Eugene Demidenko (2013). Mixed Models: Theory and Applications with R, 2nd Edition. New Jersey: Wiley &amp; Sons.
</p>
<p>McCullagh, Peter, and John A. Nelder. Generalized linear models.  CRC press, USA, 2nd edition, 1989.
</p>
<p>Presnell Brett, Morrison Scott P. and Littell Ramon C. (1998). Projected multivariate linear models for directional data. Journal of the American Statistical Association, 93(443): 1068-1077.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+cond.regs">cond.regs</a>, <a href="#topic+SES">SES</a>, <a href="#topic+MMPC">MMPC</a>, <a href="#topic+CondIndTests">CondIndTests</a>, <a href="#topic+reg.fit">reg.fit</a>, <a href="#topic+ridge.reg">ridge.reg</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rpois(50, 15)
x &lt;- matrix( rnorm(50 * 7), ncol = 7)
a1 &lt;- univregs(y, x, test = testIndPois)
a2 &lt;- perm.univregs(y, x, test = permPois)
a3 &lt;- wald.univregs(y, x, test = waldPois)
</code></pre>

<hr>
<h2 id='Utilities+20for+20the+20skeleton+20of+20a+20+28Bayesian+29+20Network'>
Utilities for the skeleton of a (Bayesian) Network
</h2><span id='topic+bn.skel.utils'></span><span id='topic+bn.skel.utils2'></span>

<h3>Description</h3>

<p>Utilities for the skeleton of a (Bayesian) Network
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bn.skel.utils(mod, G = NULL, roc = TRUE, alpha = 0.01) 
bn.skel.utils2(mod, G = NULL, roc = TRUE, alpha = 0.01) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Utilities+2B20for+2B20the+2B20skeleton+2B20of+2B20a+2B20+2B28Bayesian+2B29+2B20Network_+3A_mod">mod</code></td>
<td>

<p>An object as retured by pc.skel, glmm.pc.skel or mmhc.skel. 
</p>
</td></tr>
<tr><td><code id="Utilities+2B20for+2B20the+2B20skeleton+2B20of+2B20a+2B20+2B28Bayesian+2B29+2B20Network_+3A_g">G</code></td>
<td>

<p>The true adjacency matrix with 1 indicating an edge and zero its absence. Symmetric or not is not important. If this is not available, 
leave it NULL.
</p>
</td></tr>
<tr><td><code id="Utilities+2B20for+2B20the+2B20skeleton+2B20of+2B20a+2B20+2B28Bayesian+2B29+2B20Network_+3A_roc">roc</code></td>
<td>

<p>Do you want a graph with the ROC curve be returned? Default value is TRUE.
</p>
</td></tr>
<tr><td><code id="Utilities+2B20for+2B20the+2B20skeleton+2B20of+2B20a+2B20+2B28Bayesian+2B29+2B20Network_+3A_alpha">alpha</code></td>
<td>

<p>The significance level ( suitable values in (0, 1) ) for assessing the p-values. Default value is 0.01.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given the true adjaceny matrix one can evaluate the estimated adjacency matrix, skeleton, of the PC or the MMHC algorithm. 
</p>
<p>The bn.skels.utils give you the area under the curve, false discovery rate and sorting of the edges based on their p-values. 
</p>
<p>The bn.skel.utils2 estimates the confidence of each edge. The estimated proportion of null p-values is estimated the algorithm by 
Storey and Tibshirani (2003). 
</p>


<h3>Value</h3>

<p>For the &quot;bn.skel.utils&quot; a list including:
</p>
<table>
<tr><td><code>fdr</code></td>
<td>

<p>The false discovery rate as estimated using the Benjamini-Hochberg correction. 
</p>
</td></tr>
<tr><td><code>area</code></td>
<td>

<p>This is a list with the elements of the <code><a href="#topic+auc">auc</a></code> function. The area under the curve, the sensitivy and specificity for a range of values, the Youden index, etc.
</p>
</td></tr>
<tr><td><code>sig.pvalues</code></td>
<td>

<p>A matrix with the row and column of each significant p-value sorted in asending order. As we move down the matrix, the p-values increase and hence the strength of the associations decreases.
</p>
</td></tr>
</table>
<p>For the &quot;bn.skel.utils2&quot; a list including:
</p>
<table>
<tr><td><code>area</code></td>
<td>

<p>This is a list with the elements of the <code><a href="#topic+auc">auc</a></code> function. The area under the curve, the sensitivy and specificity for a range of values, the Youden index, etc.
</p>
</td></tr>
<tr><td><code>pxy</code></td>
<td>

<p>A matrix with the row and column of the confidence of each p-value sorted in asending order. As we move down the matrix, the confidences decrease.
</p>
</td></tr>
<tr><td><code>lower</code></td>
<td>

<p>The lower confidcence limit of an edge as estimated by <code><a href="#topic+conf.edge.lower">conf.edge.lower</a></code>. 
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Tsamardinos I. and Brown L.E. Bounding the False Discovery Rate in Local Bayesian Network Learning. AAAI, 2008.
</p>
<p>Triantafillou S., Tsamardinos I. and Roumpelaki A. (2014). Learning neighborhoods of high confidence in constraint-based causal discovery. In European Workshop on Probabilistic Graphical Models, pp. 487-502.
</p>
<p>Storey J.D. and Tibshirani R. (2003). Statistical significance for genome-wide experiments. Proceedings of the National Academy of Sciences, 100: 9440-9445. 
</p>
<p>Benjamini Y. and Hochberg Y. (1995). Controlling the false discovery rate: a practical and powerful approach to multiple testing. 
Journal of the Royal Statistical Society Series B, 57(1), 289-300.
</p>
<p>Spirtes P.,  Glymour C. and Scheines R. (2001). Causation, Prediction, and Search. The MIT Press, Cambridge, MA, USA, 3nd edition.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+pc.skel">pc.skel</a>, <a href="#topic+mmhc.skel">mmhc.skel</a>, <a href="#topic+corfs.network">corfs.network</a>, <a href="#topic+local.mmhc.skel">local.mmhc.skel</a>, <a href="#topic+conf.edge.lower">conf.edge.lower</a>  </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## simulate a dataset with continuous data
y &lt;- rdag2(500, p = 25, nei = 3)
ind &lt;- sample(1:25, 25)
x &lt;- y$x[, ind]
mod &lt;- pc.skel( x, method = "comb.fast", alpha = 0.01 ) 
G &lt;- y$G[ind, ind]
G &lt;- G + t(G)
bn.skel.utils(mod, G, roc = FALSE, alpha = 0.01) 
bn.skel.utils2(mod, G, roc = FALSE, alpha = 0.01) 
</code></pre>

<hr>
<h2 id='Variable+20selection+20using+20the+20PC-simple+20algorithm'>Variable selection using the PC-simple algorithm
</h2><span id='topic+pc.sel'></span>

<h3>Description</h3>

<p>Variable selection using the PC-simple algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pc.sel(target, dataset, threshold = 0.05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Variable+2B20selection+2B20using+2B20the+2B20PC-simple+2B20algorithm_+3A_target">target</code></td>
<td>

<p>A numerical vector with continuous data.  
</p>
</td></tr>
<tr><td><code id="Variable+2B20selection+2B20using+2B20the+2B20PC-simple+2B20algorithm_+3A_dataset">dataset</code></td>
<td>

<p>A matrix with numerical data; the independent variables, of which some will probably be selected.
</p>
</td></tr>
<tr><td><code id="Variable+2B20selection+2B20using+2B20the+2B20PC-simple+2B20algorithm_+3A_threshold">threshold</code></td>
<td>

<p>The significance level.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Variable selection for continuous data only is performed using the PC-simple algorithm 
(Buhlmann, Kalisch and Maathuis, 2010). The PC algorithm used to infer the skeleton of a Bayesian
Network has been adopted in the context of variable selection. In other words, the PC algorithm
is used for a single node.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>vars</code></td>
<td>

<p>A vector with the selected variables.
</p>
</td></tr>
<tr><td><code>n.tests</code></td>
<td>

<p>The number of tests performed.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The runtime of the algorithm.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a> 
</p>


<h3>References</h3>

<p>Buhlmann P., Kalisch M. and Maathuis M. H. (2010). Variable selection in high-dimensional linear models:
partially faithful distributions and the PC-simple algorithm. Biometrika, 97(2), 261-278.
<a href="https://arxiv.org/pdf/0906.3204.pdf">https://arxiv.org/pdf/0906.3204.pdf</a>
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+pc.skel">pc.skel</a>, <a href="Rfast.html#topic+omp">omp</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rnorm(100)
x &lt;- matrix( rnorm(100 * 30), ncol = 30)
a &lt;- MXM::pc.sel(y, x)
b &lt;- MMPC(y, x)
</code></pre>

<hr>
<h2 id='Zero+20inflated+20Poisson+20and+20negative+20binomial+20regression'>
Zero inflated Poisson and negative binomial regression
</h2><span id='topic+zip.mod'></span><span id='topic+zip.reg'></span><span id='topic+zinb.mod'></span><span id='topic+zinb.reg'></span>

<h3>Description</h3>

<p>Zero inflated Poisson and negative binomial regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>zip.mod(target, dataset, wei = NULL, xnew = NULL) 
zip.reg(target, dataset, wei = NULL, lgy = NULL) 
zinb.mod(target, dataset, xnew = NULL)
zinb.reg(target, dataset, lgy = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Zero+2B20inflated+2B20Poisson+2B20and+2B20negative+2B20binomial+2B20regression_+3A_target">target</code></td>
<td>

<p>The target (dependent) variable. It must be a numerical vector with integers. 
</p>
</td></tr>
<tr><td><code id="Zero+2B20inflated+2B20Poisson+2B20and+2B20negative+2B20binomial+2B20regression_+3A_dataset">dataset</code></td>
<td>

<p>The indendent variable(s). It can be a vector, a matrix or a dataframe with continuous only variables, a data frame 
with mixed or only categorical variables. If this is NULL, a zero inflated Poisson distribution is fitted, no covariates 
are present.
</p>
</td></tr>
<tr><td><code id="Zero+2B20inflated+2B20Poisson+2B20and+2B20negative+2B20binomial+2B20regression_+3A_wei">wei</code></td>
<td>

<p>A vector of weights to be used for weighted regression. The default value is NULL. An example where weights are used is surveys when stratified sampling has occured.
This is applicable only in the zero inflated Poisson distribution.
</p>
</td></tr>
<tr><td><code id="Zero+2B20inflated+2B20Poisson+2B20and+2B20negative+2B20binomial+2B20regression_+3A_xnew">xnew</code></td>
<td>

<p>If you have new values for the predictor variables (dataset) whose target variable you want to predict insert them here. 
If you put the &quot;dataset&quot; or leave it NULL it will calculate the regression fitted values.
</p>
</td></tr>
<tr><td><code id="Zero+2B20inflated+2B20Poisson+2B20and+2B20negative+2B20binomial+2B20regression_+3A_lgy">lgy</code></td>
<td>

<p>If you have already calculated the constant term of the ZIP regression plug it here. This is the sum of the logarithm of 
the factorial of the values. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The zero inflated Poisson regression as suggested by Lambert (1992) is fitted. Unless you have a sufficient number of zeros, 
there is no reason to use this model. The &quot;zip.reg&quot; is an internal wrapper function and is used for speed up purposes. 
It is not to be called directly by the user unless they know what they are doing. The zero inflated negative binomial 
regression does not accept weights though.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>be</code></td>
<td>

<p>The estimated coefficients of the model and for the zip.mod and zinb.mod the standard errors, 
Wald test statistics and p-values are included.
</p>
</td></tr>
<tr><td><code>prop</code></td>
<td>

<p>The estimated proportion of zeros. 
</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>

<p>The log-likelihood of the regression model. 
</p>
</td></tr>
<tr><td><code>est</code></td>
<td>

<p>The estimated values if &quot;xnew&quot; is not NULL.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Lambert D. (1992). Zero-inflated Poisson regression, with an application to defects in manufacturing. 
Technometrics, 34(1):1-14.
</p>
<p>Rui Fang (2013). Zero-inflated neagative binomial (ZINB) regression model for over-dispersed count data with excess zeros and 
repeated measures, an application to human microbiota sequence data. MSc thesis, University of Colorado. 
https://mountainscholar.org/bitstream/handle/10968/244/FANG_ucdenveramc_1639M_10037.pdf?sequence=1&amp;isAllowed=y
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+testIndZIP">testIndZIP</a>, <a href="#topic+zip.regs">zip.regs</a>, <a href="#topic+reg.fit">reg.fit</a>, <a href="#topic+ridge.reg">ridge.reg</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rpois(100, 2)
x &lt;- matrix( rnorm(100 * 2), ncol = 2)
a1 &lt;- glm(y ~ x, poisson)
a2 &lt;- zip.mod(y, x) 
summary(a1)
logLik(a1)
a2  ## a ZIP is not really necessary
y[1:20] &lt;- 0
a1 &lt;- glm(y ~ x, poisson)
a2 &lt;- zip.mod(y, x) 
summary(a1)
logLik(a1)
a2  ## a ZIP is probably more necessary
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
