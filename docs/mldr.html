<!DOCTYPE html><html><head><title>Help for package mldr</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {mldr}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#[.mldr'><p>Filter rows in a<code>mldr</code> returning a new <code>mldr</code></p></a></li>
<li><a href='#+.mldr'><p>Generates a new mldr object joining the rows</p>
in the two mldrs given as input</a></li>
<li><a href='#==.mldr'><p>Checks if two mldr objects have the same structure</p></a></li>
<li><a href='#Averaged metrics'><p>Multi-label averaged evaluation metrics</p></a></li>
<li><a href='#Basic metrics'><p>Multi-label evaluation metrics</p></a></li>
<li><a href='#birds'><p>birds</p></a></li>
<li><a href='#concurrenceReport'><p>Generates a label concurrence report</p></a></li>
<li><a href='#emotions'><p>emotions</p></a></li>
<li><a href='#genbase'><p>genbase</p></a></li>
<li><a href='#labelInteractions'><p>Provides data about interactions between labels</p></a></li>
<li><a href='#mldr'><p>Creates an object representing a multilabel dataset</p></a></li>
<li><a href='#mldr_evaluate'><p>Evaluate predictions made by a multilabel classifier</p></a></li>
<li><a href='#mldr_from_dataframe'><p>Generates an mldr object from a data.frame and a vector with label indices</p></a></li>
<li><a href='#mldr_to_labels'><p>Label matrix of an MLD</p></a></li>
<li><a href='#mldr_transform'><p>Transformns an MLDR into binary or multiclass datasets</p></a></li>
<li><a href='#mldrGUI'><p>Launches the web-based GUI for mldr</p></a></li>
<li><a href='#plot.mldr'><p>Generates graphic representations of an mldr object</p></a></li>
<li><a href='#print.mldr'><p>Prints the mldr content</p></a></li>
<li><a href='#Ranking-based metrics'><p>Multi-label ranking-based evaluation metrics</p></a></li>
<li><a href='#read.arff'><p>Read an ARFF file</p></a></li>
<li><a href='#remedial'><p>Decouples highly imbalanced labels</p></a></li>
<li><a href='#roc'><p>ROC curve</p></a></li>
<li><a href='#summary.mldr'><p>Provides a summary of measures about the mldr</p></a></li>
<li><a href='#write_arff'><p>Write an <code>"mldr"</code> object to a file</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Exploratory Data Analysis and Manipulation of Multi-Label Data
Sets</td>
</tr>
<tr>
<td>Version:</td>
<td>0.4.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2019-12-19</td>
</tr>
<tr>
<td>Description:</td>
<td>Exploratory data analysis and manipulation functions for multi-
    label data sets along with an interactive Shiny application to ease their use.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/fcharte/mldr">https://github.com/fcharte/mldr</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>shiny (&ge; 0.11), XML, circlize, graphics, grDevices, stats,
methods</td>
</tr>
<tr>
<td>Suggests:</td>
<td>pROC, knitr, mldr.datasets, testthat</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/LGPL-3">LGPL (&ge; 3)</a> | file LICENSE</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.1</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-12-19 15:49:26 UTC; fdavidcl</td>
</tr>
<tr>
<td>Author:</td>
<td>David Charte <a href="https://orcid.org/0000-0002-4830-9512"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [cre],
  Francisco Charte <a href="https://orcid.org/0000-0002-3083-8942"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Antonio J. Rivera [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>David Charte &lt;fdavidcl@ugr.es&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-12-19 17:50:10 UTC</td>
</tr>
</table>
<hr>
<h2 id='+5B.mldr'>Filter rows in a<code>mldr</code> returning a new <code>mldr</code></h2><span id='topic++5B.mldr'></span>

<h3>Description</h3>

<p>Generates a new <code>mldr</code> object containing the selected
rows from an existent <code>mldr</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mldr'
mldrObject[rowFilter = T]
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="+2B5B.mldr_+3A_mldrobject">mldrObject</code></td>
<td>
<p>Original <code>mldr</code> object from which some rows are going to be selected</p>
</td></tr>
<tr><td><code id="+2B5B.mldr_+3A_rowfilter">rowFilter</code></td>
<td>
<p>Expression to filter the rows</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A new <code>mldr</code> object with the selected rows
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mldr_from_dataframe">mldr_from_dataframe</a></code>, <code><a href="#topic++3D+3D.mldr">==.mldr</a></code>, <code><a href="#topic++2B.mldr">+.mldr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(mldr)

highlycoupled &lt;- genbase[.SCUMBLE &gt; 0.05] # Select instances with highly imbalanced coupled labels
summary(highlycoupled)   # Compare the selected instances
summary(genbase)         # with the traits of the original MLD

</code></pre>

<hr>
<h2 id='+2B.mldr'>Generates a new mldr object joining the rows
in the two mldrs given as input</h2><span id='topic++2B.mldr'></span>

<h3>Description</h3>

<p>Generates a new mldr object joining the rows
in the two mldrs given as input
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mldr'
mldr1 + mldr2
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="+2B2B.mldr_+3A_mldr1">mldr1</code></td>
<td>
<p>First <code>mldr</code> object to join</p>
</td></tr>
<tr><td><code id="+2B2B.mldr_+3A_mldr2">mldr2</code></td>
<td>
<p>Second <code>mldr</code> object to join</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a new <code>mldr</code> object with all rows in the two parameters
</p>

<hr>
<h2 id='+3D+3D.mldr'>Checks if two mldr objects have the same structure</h2><span id='topic++3D+3D.mldr'></span>

<h3>Description</h3>

<p>Checks if two mldr objects have the same structure
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mldr'
mldr1 == mldr2
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="+2B3D+2B3D.mldr_+3A_mldr1">mldr1</code></td>
<td>
<p>First <code>mldr</code> object to compare</p>
</td></tr>
<tr><td><code id="+2B3D+2B3D.mldr_+3A_mldr2">mldr2</code></td>
<td>
<p>Second <code>mldr</code> object to compare</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if the two mldr objects have the same structure, <code>FALSE</code> otherwise
</p>

<hr>
<h2 id='Averaged+20metrics'>Multi-label averaged evaluation metrics</h2><span id='topic+Averaged+20metrics'></span><span id='topic+accuracy'></span><span id='topic+precision'></span><span id='topic+micro_precision'></span><span id='topic+macro_precision'></span><span id='topic+recall'></span><span id='topic+micro_recall'></span><span id='topic+macro_recall'></span><span id='topic+fmeasure'></span><span id='topic+micro_fmeasure'></span><span id='topic+macro_fmeasure'></span>

<h3>Description</h3>

<p>Evaluation metrics based on simple metrics for the confusion
matrix, averaged through several criteria.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>accuracy(true_labels, predicted_labels, undefined_value = "diagnose")

precision(true_labels, predicted_labels, undefined_value = "diagnose")

micro_precision(true_labels, predicted_labels, ...)

macro_precision(true_labels, predicted_labels,
  undefined_value = "diagnose")

recall(true_labels, predicted_labels, undefined_value = "diagnose")

micro_recall(true_labels, predicted_labels, ...)

macro_recall(true_labels, predicted_labels, undefined_value = "diagnose")

fmeasure(true_labels, predicted_labels, undefined_value = "diagnose")

micro_fmeasure(true_labels, predicted_labels, ...)

macro_fmeasure(true_labels, predicted_labels,
  undefined_value = "diagnose")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Averaged+2B20metrics_+3A_true_labels">true_labels</code></td>
<td>
<p>Matrix of true labels, columns corresponding to labels and
rows to instances.</p>
</td></tr>
<tr><td><code id="Averaged+2B20metrics_+3A_predicted_labels">predicted_labels</code></td>
<td>
<p>Matrix of predicted labels, columns corresponding to
labels and rows to instances.</p>
</td></tr>
<tr><td><code id="Averaged+2B20metrics_+3A_undefined_value">undefined_value</code></td>
<td>
<p>The value to be returned when a computation results in
an undefined value due to a division by zero. See details.</p>
</td></tr>
<tr><td><code id="Averaged+2B20metrics_+3A_...">...</code></td>
<td>
<p>Additional parameters for precision, recall and Fmeasure.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><strong>Available metrics in this category</strong>
</p>

<ul>
<li> <p><code>accuracy</code>: Bipartition based accuracy
</p>
</li>
<li> <p><code>fmeasure</code>:  Example and binary partition F_1 measure (harmonic mean between precision and recall, averaged by instance)
</p>
</li>
<li> <p><code>macro_fmeasure</code>: Label and bipartition based F_1 measure (harmonic mean between precision and recall, macro-averaged by label)
</p>
</li>
<li> <p><code>macro_precision</code>: Label and bipartition based precision (macro-averaged by label)
</p>
</li>
<li> <p><code>macro_recall</code>: Label and bipartition based recall (macro-averaged by label)
</p>
</li>
<li> <p><code>micro_fmeasure</code>: Label and bipartition based F_1 measure (micro-averaged)
</p>
</li>
<li> <p><code>micro_precision</code>: Label and bipartition based precision (micro-averaged)
</p>
</li>
<li> <p><code>micro_recall</code>: Label and bipartition based recall (micro-averaged)
</p>
</li>
<li> <p><code>precision</code>: Example and bipartition based precision (averaged by instance)
</p>
</li>
<li> <p><code>recall</code>: Example and bipartition based recall (averaged by instance)
</p>
</li></ul>

<p><strong>Deciding a value when denominators are zero</strong>
</p>
<p>Parameter <code>undefined_value</code>: The value to be returned when a computation
results in an undefined value due to a division by zero. Can be a single
value (e.g. NA, 0), a function with the following signature:
</p>
<p><code>function(tp, fp, tn, fn)</code>
</p>
<p>or a string corresponding to one of the predefined strategies. These are:
</p>

<ul>
<li> <p><code>"diagnose"</code>: This strategy performs the following decision:
</p>

<ul>
<li><p> Returns 1 if there are no true labels and none were predicted
</p>
</li>
<li><p> Returns 0 otherwise
</p>
</li></ul>

<p>This is the default strategy, and the one followed by MULAN.
</p>
</li>
<li> <p><code>"ignore"</code>: Occurrences of undefined values will be ignored when
averaging (averages will be computed with potentially less values than
instances/labels). Undefined values in micro-averaged metrics cannot be
ignored (will return <code>NA</code>).
</p>
</li>
<li> <p><code>"na"</code>: Will return <code>NA</code> (with class <code>numeric</code>) and it
will be propagated when averaging (averaged metrics will potentially return
<code>NA</code>).
</p>
</li></ul>



<h3>Value</h3>

<p>Atomical numeric vector containing the resulting value in the range
[0, 1].
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mldr_evaluate">mldr_evaluate</a></code>, <code><a href="#topic+mldr_to_labels">mldr_to_labels</a></code>
</p>
<p>Other evaluation metrics: <code><a href="#topic+Basic+20metrics">Basic metrics</a></code>,
<code><a href="#topic+Ranking-based+20metrics">Ranking-based metrics</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>true_labels &lt;- matrix(c(
1,1,1,
0,0,0,
1,0,0,
1,1,1,
0,0,0,
1,0,0
), ncol = 3, byrow = TRUE)
predicted_labels &lt;- matrix(c(
1,1,1,
0,0,0,
1,0,0,
1,1,0,
1,0,0,
0,1,0
), ncol = 3, byrow = TRUE)

precision(true_labels, predicted_labels, undefined_value = "diagnose")
macro_recall(true_labels, predicted_labels, undefined_value = 0)
macro_fmeasure(
  true_labels, predicted_labels,
  undefined_value = function(tp, fp, tn, fn) as.numeric(fp == 0 &amp;&amp; fn == 0)
)
</code></pre>

<hr>
<h2 id='Basic+20metrics'>Multi-label evaluation metrics</h2><span id='topic+Basic+20metrics'></span><span id='topic+hamming_loss'></span><span id='topic+subset_accuracy'></span>

<h3>Description</h3>

<p>Several evaluation metrics designed for multi-label problems.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hamming_loss(true_labels, predicted_labels)

subset_accuracy(true_labels, predicted_labels)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Basic+2B20metrics_+3A_true_labels">true_labels</code></td>
<td>
<p>Matrix of true labels, columns corresponding to labels and
rows to instances.</p>
</td></tr>
<tr><td><code id="Basic+2B20metrics_+3A_predicted_labels">predicted_labels</code></td>
<td>
<p>Matrix of predicted labels, columns corresponding to
labels and rows to instances.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><strong>Available metrics in this category</strong>
</p>

<ul>
<li> <p><code>hamming_loss</code>: describes
the average absolute distance between a predicted label and its true value.
</p>
</li>
<li> <p><code>subset_accuracy</code>: the ratio of correctly predicted labelsets.
</p>
</li></ul>



<h3>Value</h3>

<p>Resulting value in the range [0, 1]
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mldr_evaluate">mldr_evaluate</a></code>, <code><a href="#topic+mldr_to_labels">mldr_to_labels</a></code>
</p>
<p>Other evaluation metrics: <code><a href="#topic+Averaged+20metrics">Averaged metrics</a></code>,
<code><a href="#topic+Ranking-based+20metrics">Ranking-based metrics</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>true_labels &lt;- matrix(c(
1,1,1,
0,0,0,
1,0,0,
1,1,1,
0,0,0,
1,0,0
), ncol = 3, byrow = TRUE)
predicted_labels &lt;- matrix(c(
1,1,1,
0,0,0,
1,0,0,
1,1,0,
1,0,0,
0,1,0
), ncol = 3, byrow = TRUE)

hamming_loss(true_labels, predicted_labels)
subset_accuracy(true_labels, predicted_labels)
</code></pre>

<hr>
<h2 id='birds'>birds</h2><span id='topic+birds'></span>

<h3>Description</h3>

<p>birds dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>birds
</code></pre>


<h3>Format</h3>

<p>An mldr object with 645 instances, 279 attributes and 19 labels</p>


<h3>Source</h3>

<p>F. Briggs, Yonghong Huang, R. Raich, K. Eftaxias, Zhong Lei, W. Cukierski, S. Hadley, A. Hadley, M. Betts, X. Fern, J. Irvine, L. Neal, A. Thomas, G. Fodor, G. Tsoumakas, Hong Wei Ng, Thi Ngoc Tho Nguyen, H. Huttunen, P. Ruusuvuori, T. Manninen, A. Diment, T. Virtanen, J. Marzat, J. Defretin, D. Callender, C. Hurlburt, K. Larrey, M. Milakov. &quot;The 9th annual MLSP competition: New methods for acoustic classification of multiple simultaneous bird species in a noisy environment&quot;, in proc. 2013 IEEE International Workshop on Machine Learning for Signal Processing (MLSP)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>summary(birds)
birds$labels
</code></pre>

<hr>
<h2 id='concurrenceReport'>Generates a label concurrence report</h2><span id='topic+concurrenceReport'></span>

<h3>Description</h3>

<p>This function produces a label concurrence report, providing the average SCUMBLE, SCUMBLE by label, a list with the minority labels
most affected by this problem indicating which majority labels they appear with, and a concurrence plot. The report output is written in the
standard output by default, but it could be redirected to a PDF file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>concurrenceReport(mld, pdfOutput = FALSE, file = "Rconcurrence.pdf")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="concurrenceReport_+3A_mld">mld</code></td>
<td>
<p><code>mldr</code> object to analyze</p>
</td></tr>
<tr><td><code id="concurrenceReport_+3A_pdfoutput">pdfOutput</code></td>
<td>
<p>Boolean value indicating if the output has to be sent to a PDF file. Defaults to true, so the output is shown in the console.</p>
</td></tr>
<tr><td><code id="concurrenceReport_+3A_file">file</code></td>
<td>
<p>If the <code>pdfOutput</code> parameter is <code>true</code> the output will be written in the file specified by this parameter. The default file name is <code>Rocurrence.pdf</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>


<h3>See Also</h3>

<p><code><a href="#topic+remedial">remedial</a></code>, <code><a href="#topic+labelInteractions">labelInteractions</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(mldr)
## Not run: 
concurrenceReport(birds)

## End(Not run)
</code></pre>

<hr>
<h2 id='emotions'>emotions</h2><span id='topic+emotions'></span>

<h3>Description</h3>

<p>emotions dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>emotions
</code></pre>


<h3>Format</h3>

<p>An mldr object with 593 instances, 78 attributes and 6 labels</p>


<h3>Source</h3>

<p>K. Trohidis, G. Tsoumakas, G. Kalliris, I. Vlahavas. &quot;Multilabel Classification of Music into Emotions&quot;. Proc. 2008 International Conference on Music Information Retrieval (ISMIR 2008), pp. 325-330, Philadelphia, PA, USA, 2008
</p>


<h3>Examples</h3>

<pre><code class='language-R'>summary(emotions)
emotions$labels
</code></pre>

<hr>
<h2 id='genbase'>genbase</h2><span id='topic+genbase'></span>

<h3>Description</h3>

<p>genbase dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>genbase
</code></pre>


<h3>Format</h3>

<p>An mldr object with 662 instances, 1213 attributes and 27 labels</p>


<h3>Source</h3>

<p>S. Diplaris, G. Tsoumakas, P. Mitkas and I. Vlahavas. Protein Classification with Multiple Algorithms, Proc. 10th Panhellenic Conference on Informatics (PCI 2005), pp. 448-456, Volos, Greece, November 2005
</p>


<h3>Examples</h3>

<pre><code class='language-R'>summary(genbase)
genbase$labels
</code></pre>

<hr>
<h2 id='labelInteractions'>Provides data about interactions between labels</h2><span id='topic+labelInteractions'></span>

<h3>Description</h3>

<p>This function facilitates a list with the minority labels most affected by the problem of concurrence with majority labels,
provinding the indexes of the majority labels interacting with each minority and also the number of instances in which they appear together.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>labelInteractions(mld, labelProportion)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="labelInteractions_+3A_mld">mld</code></td>
<td>
<p><code>mldr</code> object to analyze</p>
</td></tr>
<tr><td><code id="labelInteractions_+3A_labelproportion">labelProportion</code></td>
<td>
<p>A value in the (0,1] range establishing the proportion of minority labels to be included as result. By default at least
3 or 10% of minority labels are included, or all of them if there are fewer than 3.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with two slots, <code>indexes</code> and <code>interactions</code>. The former contains the indexes of the minority labels, sorted from
higher to lower SCUMBLE metric. The latter will provide an element for each of the previous labels, communicating the indexes of the majority
labels it interacts with and the number of samples in which they appear together.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+remedial">remedial</a></code>, <code><a href="#topic+concurrenceReport">concurrenceReport</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(mldr)
labelInteractions(birds)
</code></pre>

<hr>
<h2 id='mldr'>Creates an object representing a multilabel dataset</h2><span id='topic+mldr'></span>

<h3>Description</h3>

<p>Reads a multilabel dataset from a file and returns an <code>mldr</code> object
containing the data and additional measures. The file has to be in ARFF format.
The label information could be in a separate XML file (MULAN style) or in the
the arff header (MEKA style)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mldr(filename, use_xml = TRUE, auto_extension = TRUE, xml_file,
  label_indices, label_names, label_amount,
  force_read_from_file = !all(c(missing(xml_file),
  missing(label_indices), missing(label_names), missing(label_amount),
  use_xml, auto_extension)), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mldr_+3A_filename">filename</code></td>
<td>
<p>Name of the dataset</p>
</td></tr>
<tr><td><code id="mldr_+3A_use_xml">use_xml</code></td>
<td>
<p>Specifies whether to use an
associated XML file to identify the labels. Defaults to TRUE</p>
</td></tr>
<tr><td><code id="mldr_+3A_auto_extension">auto_extension</code></td>
<td>
<p>Specifies whether to add
the '.arff' and '.xml' extensions to the filename
where appropriate. Defaults to TRUE</p>
</td></tr>
<tr><td><code id="mldr_+3A_xml_file">xml_file</code></td>
<td>
<p>Path to the XML file. If not
provided, the filename ending in &quot;.xml&quot; will be
assumed</p>
</td></tr>
<tr><td><code id="mldr_+3A_label_indices">label_indices</code></td>
<td>
<p>Optional vector containing the indices of the attributes
that should be read as labels</p>
</td></tr>
<tr><td><code id="mldr_+3A_label_names">label_names</code></td>
<td>
<p>Optional vector containing the names of the attributes
that should be read as labels</p>
</td></tr>
<tr><td><code id="mldr_+3A_label_amount">label_amount</code></td>
<td>
<p>Optional parameter indicating the number of labels in the
dataset, which will be taken from the last attributes of the dataset</p>
</td></tr>
<tr><td><code id="mldr_+3A_force_read_from_file">force_read_from_file</code></td>
<td>
<p>Set this parameter to TRUE to always read from a local file,
or set it to FALSE to look for the dataset within the 'mldr.datasets' package</p>
</td></tr>
<tr><td><code id="mldr_+3A_...">...</code></td>
<td>
<p>Extra parameters to be passed to 'read_arff'</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An mldr object containing the multilabel dataset
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mldr_from_dataframe">mldr_from_dataframe</a></code>, <code><a href="#topic+read.arff">read.arff</a></code>, <code><a href="#topic+summary.mldr">summary.mldr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(mldr)
## Not run: 
# Read "yeast.arff" and labels from "yeast.xml"
mymld &lt;- mldr("yeast")

# Read "yeast.arff" and labels from "yeast.xml", converting categorical
# attributes to factors
mymld &lt;- mldr("yeast", stringsAsFactors = TRUE)

# Read "yeast-tra.arff" and labels from "yeast.xml"
mymld &lt;- mldr("yeast-tra", xml_file = "yeast.xml")

# Read "yeast.arff" specifying the amount of attributes to be used as labels
mymld &lt;- mldr("yeast", label_amount = 14)

# Read MEKA style dataset, without XML file and giving extension
mymld &lt;- mldr("IMDB.arff", use_xml = FALSE, auto_extension = FALSE)

## End(Not run)
</code></pre>

<hr>
<h2 id='mldr_evaluate'>Evaluate predictions made by a multilabel classifier</h2><span id='topic+mldr_evaluate'></span>

<h3>Description</h3>

<p>Taking as input an <code>mldr</code> object and a matrix with the predictions
given by a classifier, this function evaluates the classifier performance through
several multilabel metrics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mldr_evaluate(mldr, predictions, threshold = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mldr_evaluate_+3A_mldr">mldr</code></td>
<td>
<p>Object of <code>"mldr"</code> class containing the instances to evaluate</p>
</td></tr>
<tr><td><code id="mldr_evaluate_+3A_predictions">predictions</code></td>
<td>
<p>Matrix with the labels predicted for each instance in the <code>mldr</code> parameter. Each element
should be a value into [0,1] range</p>
</td></tr>
<tr><td><code id="mldr_evaluate_+3A_threshold">threshold</code></td>
<td>
<p>Threshold to use to generate bipartition of labels. By default the value 0.5 is used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with multilabel predictive performance measures. The items in the list will be </p>

<ul>
<li> <p><code>accuracy</code>
</p>
</li>
<li> <p><code>example_auc</code>
</p>
</li>
<li> <p><code>average_precision</code>
</p>
</li>
<li> <p><code>coverage</code>
</p>
</li>
<li> <p><code>fmeasure</code>
</p>
</li>
<li> <p><code>hamming_loss</code>
</p>
</li>
<li> <p><code>macro_auc</code>
</p>
</li>
<li> <p><code>macro_fmeasure</code>
</p>
</li>
<li> <p><code>macro_precision</code>
</p>
</li>
<li> <p><code>macro_recall</code>
</p>
</li>
<li> <p><code>micro_auc</code>
</p>
</li>
<li> <p><code>micro_fmeasure</code>
</p>
</li>
<li> <p><code>micro_precision</code>
</p>
</li>
<li> <p><code>micro_recall</code>
</p>
</li>
<li> <p><code>one_error</code>
</p>
</li>
<li> <p><code>precision</code>
</p>
</li>
<li> <p><code>ranking_loss</code>
</p>
</li>
<li> <p><code>recall</code>
</p>
</li>
<li> <p><code>subset_accuracy</code>
</p>
</li>
<li> <p><code>roc</code>
</p>
</li></ul>

<p>The <code>roc</code> element corresponds to a <code>roc</code> object associated to the <code>MicroAUC</code> value. This object can be given as input to <code>plot</code> for plotting the ROC curve
The <code>example_auc</code>, <code>macro_auc</code>, <code>micro_auc</code> and <code>roc</code> members will be <code>NULL</code> if the <code>pROC</code> package is not installed.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mldr">mldr</a></code>, <a href="#topic+Basic+20metrics">Basic metrics</a>, <a href="#topic+Averaged+20metrics">Averaged metrics</a>, <a href="#topic+Ranking-based+20metrics">Ranking-based metrics</a>, <code><a href="#topic+roc.mldr">roc.mldr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(mldr)

# Get the true labels in emotions
predictions &lt;- as.matrix(emotions$dataset[, emotions$labels$index])
# and introduce some noise (alternatively get the predictions from some classifier)
noised_labels &lt;- cbind(sample(1:593, 200, replace = TRUE), sample(1:6, 200, replace = TRUE))
predictions[noised_labels] &lt;- sample(0:1, 100, replace = TRUE)
# then evaluate predictive performance
res &lt;- mldr_evaluate(emotions, predictions)
str(res)
plot(res$roc, main = "ROC curve for emotions")

## End(Not run)
</code></pre>

<hr>
<h2 id='mldr_from_dataframe'>Generates an mldr object from a data.frame and a vector with label indices</h2><span id='topic+mldr_from_dataframe'></span>

<h3>Description</h3>

<p>This function creates a new <code>mldr</code> object from the data
stored in a <code>data.frame</code>, taking as labels the columns pointed by the
indexes given in a vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mldr_from_dataframe(dataframe, labelIndices, attributes, name)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mldr_from_dataframe_+3A_dataframe">dataframe</code></td>
<td>
<p>The <code>data.frame</code> containing the dataset attributes and labels.</p>
</td></tr>
<tr><td><code id="mldr_from_dataframe_+3A_labelindices">labelIndices</code></td>
<td>
<p>Vector containing the indices of attributes acting as labels. Usually the
labels will be at the end (right-most columns) or the beginning (left-most columns) of the <code>data.frame</code></p>
</td></tr>
<tr><td><code id="mldr_from_dataframe_+3A_attributes">attributes</code></td>
<td>
<p>Vector with the attributes type, as returned by the  <code>attributes</code> member of an <code>mldr</code>
object. By default the type of the data.frame columns will be used.</p>
</td></tr>
<tr><td><code id="mldr_from_dataframe_+3A_name">name</code></td>
<td>
<p>Name of the dataset. The name of the dataset given as first parameter will be used by default</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An mldr object containing the multilabel dataset
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mldr">mldr</a></code>, <code><a href="#topic+summary.mldr">summary.mldr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(mldr)

df &lt;- data.frame(matrix(rnorm(1000), ncol = 10))
df$Label1 &lt;- c(sample(c(0,1), 100, replace = TRUE))
df$Label2 &lt;- c(sample(c(0,1), 100, replace = TRUE))
mymldr &lt;- mldr_from_dataframe(df, labelIndices = c(11, 12), name = "testMLDR")

summary(mymldr)

</code></pre>

<hr>
<h2 id='mldr_to_labels'>Label matrix of an MLD</h2><span id='topic+mldr_to_labels'></span>

<h3>Description</h3>

<p>Extracts a matrix with the true 0-1 assignment of labels of an
<code>"mldr"</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mldr_to_labels(mldr)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mldr_to_labels_+3A_mldr">mldr</code></td>
<td>
<p><code>"mldr"</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric matrix of labels.
</p>


<h3>See Also</h3>

<p><a href="#topic+Basic+20metrics">Basic metrics</a>, <a href="#topic+Averaged+20metrics">Averaged metrics</a>, <a href="#topic+Ranking-based+20metrics">Ranking-based metrics</a>.
</p>

<hr>
<h2 id='mldr_transform'>Transformns an MLDR into binary or multiclass datasets</h2><span id='topic+mldr_transform'></span>

<h3>Description</h3>

<p>Transforms an <code>mldr</code> object into one or serveral binary or multiclass datasets, returning them as <code>data.frame</code> objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mldr_transform(mldr, type = "BR", labels)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mldr_transform_+3A_mldr">mldr</code></td>
<td>
<p>The mldr object to transform</p>
</td></tr>
<tr><td><code id="mldr_transform_+3A_type">type</code></td>
<td>
<p>Indicates the type of transformation to apply. Possible types are:</p>

<ul>
<li> <p><code>"BR"</code> Produces one or more binary datasets, each one with one label
</p>
</li>
<li> <p><code>"LP"</code> Produces a multiclass dataset using each labelset as class label
</p>
</li></ul>
</td></tr>
<tr><td><code id="mldr_transform_+3A_labels">labels</code></td>
<td>
<p>Vector with the label indexes to include in the transformation. All labels will be used if not specified</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of data.frames containing the resulting datasets (for BR) or a data.frame with the dataset (for LP).
The result is no longer an <code>mldr</code> object, but a plain <code>data.frame</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mldr)
emotionsbr &lt;- mldr_transform(emotions, type = "BR")
emotionslp &lt;- mldr_transform(emotions, type = "LP")
</code></pre>

<hr>
<h2 id='mldrGUI'>Launches the web-based GUI for mldr</h2><span id='topic+mldrGUI'></span>

<h3>Description</h3>

<p>Loads an interactive user interface in the web browser, built using R shiny.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mldrGUI()
</code></pre>


<h3>Details</h3>

<p>The <span class="pkg">mldr</span> package provides a basic, Shiny-based GUI to work with multilabel datasets.
You have to install the <span class="pkg">shiny</span> package to be able to use this GUI.
</p>
<p>The user interface allows working with any of the previous loaded datasets, as well as loading
new ones. The GUI is structured into the following pages:
</p>

<ul>
<li><p><strong>Main:</strong> This page is divided into two sections.
The one at the left can be used to choose a previously loaded dataset,
as well as to load datasets from files. The right part shows some basic
statistics about the selected multilabel dataset.
</p>
</li>
<li><p><strong>Labels:</strong> This page shows a table containing for each label its name, index, count, relative frequency
and imbalance ratio (IRLbl). The page also includes a bar plot
of the label frequency. The range of labels in the plot can be customized.
</p>
</li>
<li><p><strong>Labelsets:</strong> This page shows a table containing for each labelset its representation and a counter.
</p>
</li>
<li><p><strong>Attributes:</strong> This page shows a table containing for each attribute its name, type and a summary
of its values.
</p>
</li>
<li><p><strong>Concurrence:</strong> This page shows for each label the number of instances in which it appears and its
mean SCUMBLE measure, along with a plot that shows the level of concurrence among the selected labels. Clicking the
labels in the table makes it possible to add/remove them from the plot.
</p>
</li></ul>

<p>The tables shown in these pages can be sorted by any of its fields, as well as filtered. The content of the tables
can be copied to clipboard, printed and saved in CSV and Microsoft Excel format.
</p>


<h3>Value</h3>

<p>Nothing
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(mldr)
mldrGUI()

## End(Not run)
</code></pre>

<hr>
<h2 id='plot.mldr'>Generates graphic representations of an mldr object</h2><span id='topic+plot.mldr'></span>

<h3>Description</h3>

<p>Generates graphic representations of an <code>mldr</code> object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mldr'
plot(x, type = "LC", labelCount, labelIndices, title,
  ask = length(type) &gt; prod(par("mfcol")), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.mldr_+3A_x">x</code></td>
<td>
<p>The mldr object whose features are to be drawn</p>
</td></tr>
<tr><td><code id="plot.mldr_+3A_type">type</code></td>
<td>
<p>Indicates the type(s) of plot to be produced. Possible types are:</p>

<ul>
<li> <p><code>"LC"</code> Draws a circular plot with sectors representing each label
and links between them depicting label co-occurrences
</p>
</li>
<li> <p><code>"LH"</code> for label histogram
</p>
</li>
<li> <p><code>"LB"</code> for label bar plot
</p>
</li>
<li> <p><code>"CH"</code> for cardinality histogram
</p>
</li>
<li> <p><code>"AT"</code> for attributes by type pie chart
</p>
</li>
<li> <p><code>"LSH"</code> for labelset histogram
</p>
</li>
<li> <p><code>"LSB"</code> for labelset bar plot
</p>
</li></ul>
</td></tr>
<tr><td><code id="plot.mldr_+3A_labelcount">labelCount</code></td>
<td>
<p>Samples the labels in the dataset to show information of only <code>labelCount</code> of them</p>
</td></tr>
<tr><td><code id="plot.mldr_+3A_labelindices">labelIndices</code></td>
<td>
<p>Establishes the labels to be shown in the plot</p>
</td></tr>
<tr><td><code id="plot.mldr_+3A_title">title</code></td>
<td>
<p>A title to be shown above the plot. Defaults to the name of the dataset passed as first argument</p>
</td></tr>
<tr><td><code id="plot.mldr_+3A_ask">ask</code></td>
<td>
<p>Specifies whether to pause the generation of plots after each one</p>
</td></tr>
<tr><td><code id="plot.mldr_+3A_...">...</code></td>
<td>
<p>Additional parameters to be given to barplot, hist, etc.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
library(mldr)
## Not run: 
# Label concurrence plot
plot(genbase, type = "LC") # Plots all labels
plot(genbase) # Same as above
plot(genbase, title = "genbase dataset", color.function = heat.colors) # Changes the title and color
plot(genbase, labelCount = 10) # Randomly selects 10 labels to plot
plot(genbase, labelIndices = genbase$labels$index[1:10]) # Plots info of first 10 labels

# Label bar plot
plot(emotions, type = "LB", col = terrain.colors(emotions$measures$num.labels))

# Label histogram plot
plot(emotions, type = "LH")

# Cardinality histogram plot
plot(emotions, type = "CH")

# Attributes by type
plot(emotions, type = "AT", cex = 0.85)

# Labelset histogram
plot(emotions, type = "LSH")

## End(Not run)
</code></pre>

<hr>
<h2 id='print.mldr'>Prints the mldr content</h2><span id='topic+print.mldr'></span>

<h3>Description</h3>

<p>Prints the <code>mldr</code> object data, including input attributs and output labels
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mldr'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.mldr_+3A_x">x</code></td>
<td>
<p>Object whose data are to be printed</p>
</td></tr>
<tr><td><code id="print.mldr_+3A_...">...</code></td>
<td>
<p>Additional parameters to be given to print</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+mldr">mldr</a></code>, <code><a href="#topic+summary.mldr">summary.mldr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(mldr)

emotions
print(emotions) # Same as above

</code></pre>

<hr>
<h2 id='Ranking-based+20metrics'>Multi-label ranking-based evaluation metrics</h2><span id='topic+Ranking-based+20metrics'></span><span id='topic+average_precision'></span><span id='topic+one_error'></span><span id='topic+coverage'></span><span id='topic+ranking_loss'></span><span id='topic+macro_auc'></span><span id='topic+micro_auc'></span><span id='topic+example_auc'></span>

<h3>Description</h3>

<p>Functions that compute ranking-based metrics, given a matrix
of true labels and a matrix of predicted probabilities.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>average_precision(true_labels, predictions, ...)

one_error(true_labels, predictions)

coverage(true_labels, predictions, ...)

ranking_loss(true_labels, predictions)

macro_auc(true_labels, predictions, undefined_value = 0.5,
  na.rm = FALSE)

micro_auc(true_labels, predictions)

example_auc(true_labels, predictions, undefined_value = 0.5,
  na.rm = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Ranking-based+2B20metrics_+3A_true_labels">true_labels</code></td>
<td>
<p>Matrix of true labels, columns corresponding to labels and
rows to instances.</p>
</td></tr>
<tr><td><code id="Ranking-based+2B20metrics_+3A_predictions">predictions</code></td>
<td>
<p>Matrix of probabilities predicted by a classifier.</p>
</td></tr>
<tr><td><code id="Ranking-based+2B20metrics_+3A_...">...</code></td>
<td>
<p>Additional parameters to be passed to the ranking function.</p>
</td></tr>
<tr><td><code id="Ranking-based+2B20metrics_+3A_undefined_value">undefined_value</code></td>
<td>
<p>A default value for the cases when macro-averaged
and example-averaged AUC encounter undefined (not computable) values, e.g.
<code>0</code>, <code>0.5</code>, or <code>NA</code>.</p>
</td></tr>
<tr><td><code id="Ranking-based+2B20metrics_+3A_na.rm">na.rm</code></td>
<td>
<p>Logical specifying whether to ignore undefined values when
<code>undefined_value</code> is set to <code>NA</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><strong>Available metrics in this category</strong>
</p>

<ul>
<li> <p><code>average_precision</code>: Example and ranking based average precision (how many steps have to be made in the ranking to reach a certain relevant label, averaged by instance)
</p>
</li>
<li> <p><code>coverage</code>: Example and ranking based coverage (how many steps have to be made in the ranking to cover all the relevant labels, averaged by instance)
</p>
</li>
<li> <p><code>example_auc</code>: Example based Area Under the Curve ROC (averaged by instance)
</p>
</li>
<li> <p><code>macro_auc</code>: Label and ranking based Area Under the Curve ROC (macro-averaged by label)
</p>
</li>
<li> <p><code>micro_auc</code>: Label and ranking based Area Under the Curve ROC (micro-averaged)
</p>
</li>
<li> <p><code>one_error</code>: Example and ranking based one-error (how many times the top-ranked label is not a relevant label, averaged by instance)
</p>
</li>
<li> <p><code>ranking_loss</code>: Example and ranking based ranking-loss (how many times a non-relevant label is ranked above a relevant one, evaluated for all label pairs and averaged by instance)
</p>
</li></ul>

<p><strong>Breaking ties in rankings</strong>
</p>
<p>The additional <code>ties_method</code> parameter for the ranking
function is passed to R's own <code>rank</code>. It accepts the following values:
</p>

<ul>
<li> <p><code>"average"</code>
</p>
</li>
<li> <p><code>"first"</code>
</p>
</li>
<li> <p><code>"last"</code>
</p>
</li>
<li> <p><code>"random"</code>
</p>
</li>
<li> <p><code>"max"</code>
</p>
</li>
<li> <p><code>"min"</code>
</p>
</li></ul>

<p>See <code><a href="base.html#topic+rank">rank</a></code> for information on the effect of each
parameter.
The default behavior in mldr corresponds to value <code>"last"</code>, since this
is the behavior of the ranking method in MULAN, in order to facilitate fair
comparisons among classifiers over both platforms.
</p>


<h3>Value</h3>

<p>Atomical numeric vector specifying the resulting performance metric
value.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mldr_evaluate">mldr_evaluate</a></code>, <code><a href="#topic+mldr_to_labels">mldr_to_labels</a></code>
</p>
<p>Other evaluation metrics: <code><a href="#topic+Averaged+20metrics">Averaged metrics</a></code>,
<code><a href="#topic+Basic+20metrics">Basic metrics</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>true_labels &lt;- matrix(c(
1,1,1,
0,0,0,
1,0,0,
1,1,1,
0,0,0,
1,0,0
), ncol = 3, byrow = TRUE)
predicted_probs &lt;- matrix(c(
.6,.5,.9,
.0,.1,.2,
.8,.3,.2,
.7,.9,.1,
.7,.3,.2,
.1,.8,.3
), ncol = 3, byrow = TRUE)

# by default, labels with same ranking are assigned ascending rankings
# in the order they are encountered
coverage(true_labels, predicted_probs)
# in the following, labels with same ranking will receive the same,
# averaged ranking
average_precision(true_labels, predicted_probs, ties_method = "average")

# the following will treat all undefined values as 0 (counting them
# for the average)
example_auc(true_labels, predicted_probs, undefined_value = 0)
# the following will ignore undefined values (not counting them for
# the average)
example_auc(true_labels, predicted_probs, undefined_value = NA, na.rm = TRUE)
</code></pre>

<hr>
<h2 id='read.arff'>Read an ARFF file</h2><span id='topic+read.arff'></span>

<h3>Description</h3>

<p>Reads a multilabel dataset from an ARFF file in Mulan or MEKA
and retrieves instances distinguishing attributes corresponding to labels
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read.arff(filename, use_xml = TRUE, auto_extension = TRUE, xml_file,
  label_indices, label_names, label_amount, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="read.arff_+3A_filename">filename</code></td>
<td>
<p>Name of the dataset</p>
</td></tr>
<tr><td><code id="read.arff_+3A_use_xml">use_xml</code></td>
<td>
<p>Specifies whether to use an
associated XML file to identify the labels. Defaults to TRUE</p>
</td></tr>
<tr><td><code id="read.arff_+3A_auto_extension">auto_extension</code></td>
<td>
<p>Specifies whether to add
the '.arff' and '.xml' extensions to the filename
where appropriate. Defaults to TRUE</p>
</td></tr>
<tr><td><code id="read.arff_+3A_xml_file">xml_file</code></td>
<td>
<p>Path to the XML file. If not
provided, the filename ending in &quot;.xml&quot; will be
assumed</p>
</td></tr>
<tr><td><code id="read.arff_+3A_label_indices">label_indices</code></td>
<td>
<p>Optional vector containing the indices of the attributes
that should be read as labels</p>
</td></tr>
<tr><td><code id="read.arff_+3A_label_names">label_names</code></td>
<td>
<p>Optional vector containing the names of the attributes
that should be read as labels</p>
</td></tr>
<tr><td><code id="read.arff_+3A_label_amount">label_amount</code></td>
<td>
<p>Optional parameter indicating the number of labels in the
dataset, which will be taken from the last attributes of the dataset</p>
</td></tr>
<tr><td><code id="read.arff_+3A_...">...</code></td>
<td>
<p>Extra parameters that will be passed to the parsers. Currently
only the option <code>stringsAsFactors</code> is available</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing four members: <code>dataframe</code> containing the dataset,
<code>labelIndices</code> specifying the indices of the attributes that correspond to
labels, <code>attributes</code> containing name and type of each attribute and <code>name</code> of
the dataset.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mldr_from_dataframe">mldr_from_dataframe</a></code>, <code><a href="#topic+mldr">mldr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(mldr)
## Not run: 
# Read "yeast.arff" and labels from "yeast.xml"
mymld &lt;- read.arff("yeast")

## End(Not run)
</code></pre>

<hr>
<h2 id='remedial'>Decouples highly imbalanced labels</h2><span id='topic+remedial'></span>

<h3>Description</h3>

<p>This function implements the REMEDIAL algorithm. It is a preprocessing algorithm for imbalanced multilabel datasets,
whose aim is to decouple frequent and rare classes appearing in the same instance. For doing so, it aggregates new instances to the dataset
and edit the labels present in them.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>remedial(mld)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="remedial_+3A_mld">mld</code></td>
<td>
<p><code>mldr</code> object with the multilabel dataset to preprocess</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An mldr object containing the preprocessed multilabel dataset
</p>


<h3>Source</h3>

<p>F. Charte, A. J. Rivera, M. J. del Jesus, F. Herrera. &quot;Resampling Multilabel Datasets by Decoupling Highly Imbalanced Labels&quot;. Proc. 2015 International Conference on Hybrid Artificial Intelligent Systems (HAIS 2015), pp. 489-501, Bilbao, Spain, 2015
</p>


<h3>See Also</h3>

<p><code><a href="#topic+concurrenceReport">concurrenceReport</a></code>, <code><a href="#topic+labelInteractions">labelInteractions</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(mldr)
## Not run: 
summary(birds)
summary(remedial(birds))

## End(Not run)
</code></pre>

<hr>
<h2 id='roc'>ROC curve</h2><span id='topic+roc'></span><span id='topic+roc.mldr'></span>

<h3>Description</h3>

<p>Calculates the ROC (Receiver Operating Characteristic) curve
for given true labels and predicted ones. The pROC package is needed for
this functionality.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>roc(...)

## S3 method for class 'mldr'
roc(mldr, predictions, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="roc_+3A_...">...</code></td>
<td>
<p>Additional parameters to be passed to the <code>pROC::roc</code> function.
See <code><a href="pROC.html#topic+roc">roc</a></code> for more information.</p>
</td></tr>
<tr><td><code id="roc_+3A_mldr">mldr</code></td>
<td>
<p>An <code>"mldr"</code> object. Its labels will be extracted via
<code><a href="#topic+mldr_to_labels">mldr_to_labels</a></code>.</p>
</td></tr>
<tr><td><code id="roc_+3A_predictions">predictions</code></td>
<td>
<p>Matrix of predicted labels or probabilities, columns
corresponding to labels and rows to instances.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ROC object from pROC package.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mldr_evaluate">mldr_evaluate</a></code> <code><a href="pROC.html#topic+roc">roc</a></code>
</p>

<hr>
<h2 id='summary.mldr'>Provides a summary of measures about the mldr</h2><span id='topic+summary.mldr'></span>

<h3>Description</h3>

<p>Prints a summary of the measures obtained from the <code>mldr</code> object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mldr'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.mldr_+3A_object">object</code></td>
<td>
<p>Object whose measures are to be printed</p>
</td></tr>
<tr><td><code id="summary.mldr_+3A_...">...</code></td>
<td>
<p>Additional parameters to be given to print</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+mldr">mldr</a></code>, <code><a href="#topic+print.mldr">print.mldr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(mldr)

summary(emotions)

</code></pre>

<hr>
<h2 id='write_arff'>Write an <code>"mldr"</code> object to a file</h2><span id='topic+write_arff'></span>

<h3>Description</h3>

<p>Save the <code>mldr</code> content to an ARFF file and the label data to an XML file.
If you need <strong>faster write, more options and support for other formats</strong>, please
refer to the <code><a href="mldr.datasets.html#topic+write.mldr">write.mldr</a></code> function in package mldr.datasets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>write_arff(obj, filename, write.xml = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="write_arff_+3A_obj">obj</code></td>
<td>
<p>The <code>"mldr"</code> object whose content is going to be written</p>
</td></tr>
<tr><td><code id="write_arff_+3A_filename">filename</code></td>
<td>
<p>Base name for the files (without extension)</p>
</td></tr>
<tr><td><code id="write_arff_+3A_write.xml">write.xml</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code>, stating if the XML file has to be written</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+mldr_from_dataframe">mldr_from_dataframe</a></code>, <code><a href="#topic+mldr">mldr</a></code>
</p>
<p>In mldr.datasets: <code><a href="mldr.datasets.html#topic+write.mldr">write.mldr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

dir &lt;- tempdir()
write_arff(emotions, file.path(dir, "myemotions"))
file.remove(file.path(dir, "myemotions.arff"))

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
