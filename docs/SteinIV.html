<!DOCTYPE html><html lang="en"><head><title>Help for package SteinIV</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {SteinIV}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#jive.est'><p>The Jackknife Instrumental Variable Estimator (JIVE).</p></a></li>
<li><a href='#jive.internal'><p>Internal function for the Jackknife Instrumental Variable Estimator (JIVE).</p></a></li>
<li><a href='#ols.est'><p>The Ordinary Least Squares (OLS) Estimator.</p></a></li>
<li><a href='#sps.est'><p>Semi-parametric Stein-like (SPS) estimator.</p></a></li>
<li><a href='#sps.internal'><p>Internal function for Semi-parametric Stein-like (SPS) estimator.</p></a></li>
<li><a href='#tr'><p>Trace of a matrix.</p></a></li>
<li><a href='#tsls.est'><p>The Two-Stage Least Squares (TSLS) estimator.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Version:</td>
<td>0.1-1</td>
</tr>
<tr>
<td>Date:</td>
<td>2016-01-01</td>
</tr>
<tr>
<td>Title:</td>
<td>Semi-Parametric Stein-Like Estimator with Instrumental Variables</td>
</tr>
<tr>
<td>Author:</td>
<td>Cedric E Ginestet &lt;cedric.ginestet@kcl.ac.uk&gt;</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Cedric E Ginestet &lt;cedric.ginestet@kcl.ac.uk&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10.1)</td>
</tr>
<tr>
<td>Description:</td>
<td>Routines for computing different types of linear estimators, based on instrumental variables (IVs), including the semi-parametric Stein-like (SPS) estimator, originally introduced by Judge and Mittelhammer (2004)  &lt;<a href="https://doi.org/10.1198%2F016214504000000430">doi:10.1198/016214504000000430</a>&gt;. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://cran.r-project.org/package=SteinIV">https://cran.r-project.org/package=SteinIV</a></td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2016-01-26 14:03:42 UTC; cgineste</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2016-01-26 15:19:33</td>
</tr>
</table>
<hr>
<h2 id='jive.est'>The Jackknife Instrumental Variable Estimator (JIVE).</h2><span id='topic+jive.est'></span>

<h3>Description</h3>

<p>Compute the JIVE for a multiple regression, as well
as the set of standard errors for the individual vector entries, and
the estimate of the asymptotic variance/covariance matrix.</p>


<h3>Usage</h3>

<pre><code class='language-R'>  jive.est(y,X,Z,SE=FALSE,n.bt=100)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="jive.est_+3A_y">y</code></td>
<td>
<p>Numeric: A vector of observations, representing the outcome variable.</p>
</td></tr>
<tr><td><code id="jive.est_+3A_x">X</code></td>
<td>
<p>Numeric: A matrix of observations, whose number of columns
corresponds to the number of predictors in the model, and the number
of rows should be conformal with the number of entries in <code class="reqn">y</code>. This
matrix may contain both endogenous and exogenous variables.</p>
</td></tr>
<tr><td><code id="jive.est_+3A_z">Z</code></td>
<td>
<p>Numeric: A matrix of observations representing the
intrumental variables (IVs) in the first-stage structural equation. The
number of IVs should be at least as large as the number of
endogenous variables in <code class="reqn">X</code>.</p>
</td></tr>
<tr><td><code id="jive.est_+3A_se">SE</code></td>
<td>
<p>Logical: If TRUE, then the function also returns the
standard errors of the individual JIVE estimators, and a bootstrap
estimate of its asymptotic variance/covariance matrix.</p>
</td></tr>
<tr><td><code id="jive.est_+3A_n.bt">n.bt</code></td>
<td>
<p>Numeric: The number of bootstrap samples performed for
estimating the variance/covariance matrix.
This automatically occurs, whenever the user selects the
SE to be true.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The JIVE was originally introduced by Angrist et al. (1995), in order to reduce the finite-sample
bias of the TSLS estimator, when applied to a large number of
instruments. Indeed, the TSLS estimator tends to behave poorly as the
number of instruments increases. We briefly outline this method. See
Angrist et al. (1999) for an exhaustive description.
</p>
<p>The model is identical to the one used in the rest of this package. That is,
the second-stage equation is modelled as <code class="reqn">y = X\beta + \epsilon,</code>
in which <code class="reqn">y</code> is a vector of <code class="reqn">n</code> observations representing the
outcome variable, <code class="reqn">X</code> is a matrix of order <code class="reqn">n\times k</code>
denoting the predictors of the model, and comprised of both exogenous
and endogenous variables, <code class="reqn">\beta</code> is the <code class="reqn">k</code>-dimensional
vector of parameters of interest; whereas <code class="reqn">\epsilon</code> is an unknown
vector of error terms.
Moreover, the first-stage level of the model is given by a multivariate
multiple regression. That is, this is a linear modle with a
<em>multivariate</em> outcome variable, as well as <em>multiple</em>
predictors. This first-stage model is represented in this manner, 
<code class="reqn">X = Z\Gamma + \Delta</code>,
where <code class="reqn">X</code> is the matrix of predictors from the second-stage
equation, <code class="reqn">Z</code> is a matrix of instrumental variables (IVs) of order
<code class="reqn">n \times l</code>, <code class="reqn">\Gamma</code> is a matrix of unknown parameters of
order <code class="reqn">l\times k</code>; whereas <code class="reqn">\Delta</code> denotes an unknown matrix of order
<code class="reqn">n\times k</code> of error terms. 
</p>
<p>For computing the JIVE, we first consider the estimator of the
regression parameter in the first-stage equation, which is denoted by  
</p>
<p style="text-align: center;"><code class="reqn">\hat\Gamma := ({Z}^{T}{Z})^{-1}({Z}^{T}{X}).</code>
</p>

<p>This matrix is of order <code class="reqn">l\times k</code>. The matrix of predictors, <code class="reqn">{X}</code>, projected
onto the column space of the instruments is then given by
<code class="reqn">\hat{X}={Z}\hat\Gamma</code>. The JIVE proceeds by
estimating each row of <code class="reqn">\hat{X}</code> without using the corresponding data
point. That is, the <code class="reqn">i</code>th row in the jackknife matrix, <code class="reqn">\hat{X}_{J}</code>,
is estimated without using the <code class="reqn">i</code>th row of <code class="reqn">{X}</code>.
This is conducted as follows. For every <code class="reqn">i=1,\ldots,n</code>, we first compute
</p>
<p style="text-align: center;"><code class="reqn">\hat\Gamma_{(i)} :=
    ({Z}_{(i)}^{T}{Z}_{(i)})^{-1}({Z}_{(i)}^{T}{X}_{(i)}),</code>
</p>
  
<p>where <code class="reqn">{Z}_{(i)}</code> and <code class="reqn">{X}_{(i)}</code> denote matrices <code class="reqn">{Z}</code> and <code class="reqn">{X}</code> after
removal of the <code class="reqn">i</code>th row, such that these two matrices are of order
<code class="reqn">(n-1)\times l</code> and <code class="reqn">(n-1)\times k</code>, respectively. Then, the
matrix <code class="reqn">\hat{X}_{J}</code> is constructed by stacking these jackknife
estimates of <code class="reqn">\hat\Gamma</code>, after they have been pre-multiplied by the
corresponding rows of <code class="reqn">{Z}</code>, 
</p>
<p style="text-align: center;"><code class="reqn">\hat{X}_{J} :=
	({z}_{1}\hat\Gamma_{(1)},\ldots,{z}_{n}\hat\Gamma_{(n)})^{T},</code>
</p>
     
<p>where each <code class="reqn">{z}_{i}</code> is an <code class="reqn">l</code>-dimensional row vector. The JIVE
estimator is then obtained by replacing <code class="reqn">\hat{X}</code> with
<code class="reqn">\hat{X}_{J}</code> in the standard formula of the TSLS, such that 
</p>
<p style="text-align: center;"><code class="reqn">\hat\beta_{J} := (\hat{X}_{J}{}^{T}{X})^{-1}(\hat{X}_{J}{}^{T}{y}).</code>
</p>

<p>In this package, we have additionally made use of the computational
formula suggested by Angrist et al. (1999), in which each row of
<code class="reqn">\hat{X}_{J}</code> is calculated using 
</p>
<p style="text-align: center;"><code class="reqn">{z}_{i}\hat\Gamma_{(i)} = \frac{{z}_{i}\hat\Gamma -
    h_{i}{x}_{i}}{1-h_{i}},</code>
</p>

<p>where <code class="reqn">{z}_{i}\hat\Gamma_{(i)}</code>, <code class="reqn">{z}_{i}\hat\Gamma</code> and
<code class="reqn">{x}_{i}</code> are <code class="reqn">k</code>-dimensional row vectors; and with <code class="reqn">h_{i}</code> denoting
the leverage of the corresponding data point in the first-level
equation of our model, such that each <code class="reqn">h_{i}</code> is defined as
<code class="reqn">{z}_{i}({Z}^{T}{Z})^{-1}{z}_{i}^{T}</code>.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>list</code></td>
<td>
<p>A list with one or three arguments, depending on whether
the user has activated the SE flag. The first element (est) in the list
is the TSLS estimate of the model in vector format. The second
element (se) is the vector of standard errors; and the third
element (var) is the sample estimate of the asymptotic
variance/covariance matrix.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Cedric E. Ginestet &lt;cedric.ginestet@kcl.ac.uk&gt;
</p>


<h3>References</h3>

<p>Angrist, J., Imbens, G., and Krueger, A.B. (1995). Jackknife instrumental variables esti-
mation. Technical Working Paper 172, National Bureau of Economic
Research.
</p>
<p>Angrist, J.D., Imbens, G.W., and Krueger, A.B. (1999). Jackknife instrumental variables
estimation. Journal of Applied Econometrics, 14(1), 57&ndash;67.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### Generate a simple example with synthetic data, and no intercept. 
n &lt;- 100; k &lt;- 3; l &lt;- 3;
Ga&lt;- diag(rep(1,l)); be &lt;- rep(1,k);
Z &lt;- matrix(0,n,l); for(j in 1:l) Z[,j] &lt;- rnorm(n); 
X &lt;- matrix(0,n,k); for(j in 1:k) X[,j] &lt;- Z[,j]*Ga[j,j] + rnorm(n); 
y &lt;- X%*%be + rnorm(n);

### Compute JIVE estimator with SEs and variance/covariance matrix.
print(jive.est(y,X,Z))
print(jive.est(y,X,Z,SE=TRUE));

</code></pre>

<hr>
<h2 id='jive.internal'>Internal function for the Jackknife Instrumental Variable Estimator (JIVE).</h2><span id='topic+jive.internal'></span>

<h3>Description</h3>

<p>Compute the JIVE for a multiple regression</p>


<h3>Usage</h3>

<pre><code class='language-R'>  jive.internal(y,X,Z)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="jive.internal_+3A_y">y</code></td>
<td>
<p>Numeric: A vector of observations, representing the outcome variable.</p>
</td></tr>
<tr><td><code id="jive.internal_+3A_x">X</code></td>
<td>
<p>Numeric: A matrix of observations, whose number of columns
corresponds to the number of predictors in the model, and the number
of rows should be conformal with the number of entries in <code class="reqn">y</code>. This
matrix may contain both endogenous and exogenous variables.</p>
</td></tr>
<tr><td><code id="jive.internal_+3A_z">Z</code></td>
<td>
<p>Numeric: A matrix of observations representing the
intrumental variables (IVs) in the first-stage structural equation. The
number of IVs should be at least as large as the number of
endogenous variables in <code class="reqn">X</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See documentaion for the jive.est function. Users should use the
jive.est function, instead. 
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>B</code></td>
<td>
<p>A vector of estimates for the coefficients of interest.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Cedric E. Ginestet &lt;cedric.ginestet@kcl.ac.uk&gt;
</p>


<h3>References</h3>

<p>Angrist, J., Imbens, G., and Krueger, A.B. (1995). Jackknife instrumental variables esti-
mation. Technical Working Paper 172, National Bureau of Economic
Research.
</p>
<p>Angrist, J.D., Imbens, G.W., and Krueger, A.B. (1999). Jackknife instrumental variables
estimation. Journal of Applied Econometrics, 14(1), 57&ndash;67.
</p>

<hr>
<h2 id='ols.est'>The Ordinary Least Squares (OLS) Estimator.</h2><span id='topic+ols.est'></span>

<h3>Description</h3>

<p>Compute the OLS estimator of a multiple regression, as well
as the set of standard errors for the individual vector entries, and
the estimate of the asymptotic variance/covariance matrix.</p>


<h3>Usage</h3>

<pre><code class='language-R'>  ols.est(y,X,SE=FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ols.est_+3A_y">y</code></td>
<td>
<p>Numeric: A vector of observations, representing the outcome variable.</p>
</td></tr>
<tr><td><code id="ols.est_+3A_x">X</code></td>
<td>
<p>Numeric: A matrix of observations, whose number of columns
corresponds to the number of predictors in the model, and the number
of rows should be conformal with the number of entries in <code class="reqn">y</code>. This
matrix may contain both endogenous and exogenous variables.</p>
</td></tr>
<tr><td><code id="ols.est_+3A_se">SE</code></td>
<td>
<p>Logical: If TRUE, then the function also returns the
standard errors of the individual TSLS estimator, and a sample
estimate of its asymptotic variance/covariance matrix.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The OLS estimator is computed for a standard one-stage structural
model. We here adopt the terminology commonly used in
econometrics. See, for example, the references below for Cameron 
and Trivedi (2005), Davidson and MacKinnon (1993), as well as
Wooldridge (2002). The second-stage equation is thus modelled as follows,
</p>
<p style="text-align: center;"><code class="reqn">y = X\beta + \epsilon,</code>
</p>

<p>in which <code class="reqn">y</code> is a vector of <code class="reqn">n</code> observations representing the
outcome variable, <code class="reqn">X</code> is a matrix of order <code class="reqn">n\times k</code>
denoting the predictors of the model, and comprised of both exogenous
and endogenous variables, <code class="reqn">\beta</code> is the <code class="reqn">k</code>-dimensional
vector of parameters of interest; whereas <code class="reqn">\epsilon</code> is an unknown
vector of error terms. The formula for the OLS estimator is then
obtained in the standard fashion by the following equation,
</p>
<p style="text-align: center;"><code class="reqn">\hat\beta_{OLS} := (X^TX)^{-1}(X^{T}y),</code>
</p>

<p>with variance/covariance matrix given by
</p>
<p style="text-align: center;"><code class="reqn">\hat\Sigma_{OLS} := \hat\sigma^{2}(X^{T}X)^{-1},</code>
</p>

<p>in which the sample residual sum of squares is
<code class="reqn">\hat\sigma^{2}:=(y - X\hat\beta_{OLS})^{T}(y -
    X\hat\beta_{OLS})/(n-k)</code>.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>list</code></td>
<td>
<p>A list with one or three arguments, depending on whether
the user has activated the SE flag. The first element (est) in the list
is the TSLS estimate of the model in vector format. The second
element (se) is the vector of standard errors; and the third
element (var) is the sample estimate of the asymptotic
variance/covariance matrix.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Cedric E. Ginestet &lt;cedric.ginestet@kcl.ac.uk&gt;
</p>


<h3>References</h3>

<p>Cameron, A. and Trivedi, P. (2005). Microeconometrics: Methods and Applications. Cam-
bridge University press, Cambridge.
</p>
<p>Davidson, R. and MacKinnon, J.G. (1993). Estimation and inference in econometrics. OUP
Catalogue.
</p>
<p>Wooldridge, J. (2002). Econometric analysis of cross-section and panel data. MIT press,
London.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### Generate a simple example with synthetic data, and no intercept. 
n &lt;- 100; k &lt;- 3; l &lt;- 3;
Ga&lt;- diag(rep(1,l)); be &lt;- rep(1,k);
Z &lt;- matrix(0,n,l); for(j in 1:l) Z[,j] &lt;- rnorm(n); 
X &lt;- matrix(0,n,k); for(j in 1:k) X[,j] &lt;- Z[,j]*Ga[j,j] + rnorm(n); 
y &lt;- X%*%be + rnorm(n);

### Compute OLS estimator with SEs and variance/covariance matrix.
print(ols.est(y,X))
print(ols.est(y,X,SE=TRUE))

</code></pre>

<hr>
<h2 id='sps.est'>Semi-parametric Stein-like (SPS) estimator.</h2><span id='topic+sps.est'></span>

<h3>Description</h3>

<p>Computes the SPS estimator for a two-stage structural model, as well
as the set of standard errors for each individual estimator, and
the sample estimate of the asymptotic variance/covariance matrix.</p>


<h3>Usage</h3>

<pre><code class='language-R'>  sps.est(y,X,Z,SE=FALSE,ALPHA=TRUE,REF="TSLS",n.bt=100,n.btj=10)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sps.est_+3A_y">y</code></td>
<td>
<p>Numeric: A vector of observations, representing the outcome variable.</p>
</td></tr>
<tr><td><code id="sps.est_+3A_x">X</code></td>
<td>
<p>Numeric: A matrix of observations, whose number of columns
corresponds to the number of predictors in the model, and the number
of rows should be conformal with the number of entries in <code class="reqn">y</code>. This
matrix may contain both endogenous and exogenous variables.</p>
</td></tr>
<tr><td><code id="sps.est_+3A_z">Z</code></td>
<td>
<p>Numeric: A matrix of observations representing the
intrumental variables (IVs) in the first-stage structural equation. The
number of IVs should be at least as large as the number of
endogenous variables in <code class="reqn">X</code>.</p>
</td></tr>
<tr><td><code id="sps.est_+3A_se">SE</code></td>
<td>
<p>Logical: If TRUE, then the function also returns the
standard errors of the individual SPS estimators, and a sample (or
bootstrap, if JIVE is selected as a reference estimator)
estimate of its asymptotic variance/covariance matrix.</p>
</td></tr>
<tr><td><code id="sps.est_+3A_alpha">ALPHA</code></td>
<td>
<p>Logical: If TRUE, the function returns the value of the
sample estimate of the parameter controlling the respective
contribution of the <em>reference</em> estimator (by default, this is the TSLS
estimator), and the one of the <em>alternative</em> estimator (by
default, this is the OLS estimator).</p>
</td></tr>
<tr><td><code id="sps.est_+3A_ref">REF</code></td>
<td>
<p>Character: Controls the choice of the <em>reference</em>
estimator in the SPS framework. This can accept two values: &quot;TSLS&quot; or &quot;JIVE&quot;,
with the former being the default option. The <em>alternative</em>
estimator is always the OLS estimator.</p>
</td></tr>   
<tr><td><code id="sps.est_+3A_n.bt">n.bt</code></td>
<td>
<p>Numeric: The number of bootstrap samples performed, when
the sample variance/covariance matrix is estimated using the
bootstrap. This automatically occurs, whenever the user selects the
JIVE as the reference estimator.</p>
</td></tr>
<tr><td><code id="sps.est_+3A_n.btj">n.btj</code></td>
<td>
<p>Numeric: The number of boostrap iterations performed, when
computing the SPS estimator, when using the JIVE as reference
estimator. This option is only relevant, when JIVE has   
been selected as the reference estimator. These iterations are used
to compute the various components entering in the calculation of the
SPS estimator.</p>
</td></tr>  
</table>


<h3>Details</h3>

<p>The SPS estimator is applied to a two-stage structural
model. We here adopt the terminology commonly used in
econometrics. See, for example, the references below for Cameron 
and Trivedi (2005), Davidson and MacKinnon (1993), as well as
Wooldridge (2002). The second-stage equation is thus modelled as follows,
</p>
<p style="text-align: center;"><code class="reqn">y = X\beta + \epsilon,</code>
</p>

<p>in which <code class="reqn">y</code> is a vector of <code class="reqn">n</code> observations representing the
outcome variable, <code class="reqn">X</code> is a matrix of order <code class="reqn">n\times k</code>
denoting the predictors of the model, and comprised of both exogenous
and endogenous variables, <code class="reqn">\beta</code> is the <code class="reqn">k</code>-dimensional
vector of parameters of interest; whereas <code class="reqn">\epsilon</code> is an unknown
vector of error terms.
The first-stage level of the model is given by a multivariate
multiple regression. That is, this is a linear modle with a
<em>multivariate</em> outcome variable, as well as <em>multiple</em>
predictors. This first-stage model is represented in this manner, 
</p>
<p style="text-align: center;"><code class="reqn">X = Z\Gamma + \Delta,</code>
</p>

<p>where <code class="reqn">X</code> is the matrix of predictors from the second-stage
equation, <code class="reqn">Z</code> is a matrix of instrumental variables (IVs) of order
<code class="reqn">n \times l</code>, <code class="reqn">\Gamma</code> is a matrix of unknown parameters of
order <code class="reqn">l\times k</code>; whereas <code class="reqn">\Delta</code> denotes an unknown matrix of order
<code class="reqn">n\times k</code> of error terms. 
</p>
<p>As for the TSLS estimator, whenever certain variables in <code class="reqn">X</code> are
assumed to be exogenous, these variables should be incorporated into 
<code class="reqn">Z</code>. That is, all the exogneous variables are their own
instruments. Moreover, it is also assumed that the model contains at
least as many instruments as predictors, in the sense that <code class="reqn">l\geq
    k</code>, as commonly donein practice (Wooldridge, 2002). Also, the matrices,
<code class="reqn">X^TX</code>, <code class="reqn">Z^TX</code>, and <code class="reqn">Z^TZ</code> are all assumed to be full
rank. Finally, both <code class="reqn">X</code> and <code class="reqn">Z</code> should comprise a column of
one's, representing the intercept in each structural equation. 
</p>
<p>The formula for the SPS estimator is then obtained as a weigthed
combination of the OLS and TSLS estimators (using the default
options), such that 
</p>
<p style="text-align: center;"><code class="reqn">\hat\beta_{SPS}(\alpha) :=
      \alpha\hat\beta_{OLS} + (1-\alpha)\hat\beta_{TSLS},</code>
</p>

<p>for every <code class="reqn">\alpha</code>. The <em>proportion parameter</em>, <code class="reqn">\alpha</code>,
controls the respective contributions of the OLS and TSLS estimators. 
(Despite our choice of name, however, note that <code class="reqn">\alpha</code> needs not be bounded
between 0 and 1.) This parameter is selected in order to minimize the
trace of the theoretical MSE of the corresponding SPS estimator,
</p>
<p style="text-align: center;"><code class="reqn">MSE(\hat\beta_{SPS}(\alpha)) 
      = E[(\bar\beta(\alpha)-\beta)(\hat\beta(\alpha)-\beta)^{T}] 
      = Var(\hat\beta(\alpha)) + Bias^{2}(\hat\beta(\alpha)),</code>
</p>

<p>where <code class="reqn">\beta\in R^{k}</code> is the true parameter of interest and the MSE is
a <code class="reqn">k\times k</code> matrix. It is particularly appealing to combine these
two estimators, because the asymptotic unbiasedness of the TSLS
estimator guarantees that the resulting SPS is asymptotically
unbiased. Thus, the MSE automatically strikes a trade-off between the
unbiasedness of the TSLS estimator and the efficiency of the OLS
estimator.   
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>list</code></td>
<td>
<p>A list with one or four arguments, depending on whether
the user has activated the SE flag, and the ALPHA flag. The first
element (est) in the list    
is the SPS estimate of the model in vector format. The second
element (se) is the vector of standard errors; the third
element (var) is the sample estimate of the asymptotic
variance/covariance matrix; the fourth element (alpha) is a real
number representing the estimate of the contribution of the OLS to
the combined SPS estimator.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Cedric E. Ginestet &lt;cedric.ginestet@kcl.ac.uk&gt;
</p>


<h3>References</h3>

<p>Judge, G.G. and Mittelhammer, R.C. (2004). A semiparametric basis for combining esti-
mation problems under quadratic loss. Journal of the American Statistical Association,
99(466), 479&ndash;487.
</p>
<p>Judge, G.G. and Mittelhammer, R.C. (2012a). An information theoretic approach to econo-
metrics. Cambridge University Press.
</p>
<p>Judge, G. and Mittelhammer, R. (2012b). A risk superior semiparametric estimator for
over-identified linear models. Advances in Econometrics, 237&ndash;255.
</p>
<p>Judge, G. and Mittelhammer, R. (2013). A minimum mean squared error semiparametric
combining estimator. Advances in Econometrics, 55&ndash;85.
</p>
<p>Mittelhammer, R.C. and Judge, G.G. (2005). Combining estimators to improve structural
model estimation and inference under quadratic loss. Journal of econometrics, 128(1),
1&ndash;29.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### Generate a simple example with synthetic data, and no intercept. 
n &lt;- 100; k &lt;- 3; l &lt;- 3;
Ga&lt;- diag(rep(1,l)); be &lt;- rep(1,k);
Z &lt;- matrix(0,n,l); for(j in 1:l) Z[,j] &lt;- rnorm(n); 
X &lt;- matrix(0,n,k); for(j in 1:k) X[,j] &lt;- Z[,j]*Ga[j,j] + rnorm(n); 
y &lt;- X%*%be + rnorm(n);

### Compute SPS estimator with SEs and variance/covariance matrix.
print(sps.est(y,X,Z))
print(sps.est(y,X,Z,SE=TRUE));

</code></pre>

<hr>
<h2 id='sps.internal'>Internal function for Semi-parametric Stein-like (SPS) estimator.</h2><span id='topic+sps.internal'></span>

<h3>Description</h3>

<p>Computes the SPS estimator for a two-stage structural model, as well
as a sample estimate of the alpha parameter controlling the degree
of combination between the OLS and TSLS estimators.</p>


<h3>Usage</h3>

<pre><code class='language-R'>  sps.internal(y,X,Z,REF="TSLS",ALPHA=FALSE,n.btj=10)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sps.internal_+3A_y">y</code></td>
<td>
<p>Numeric: A vector of observations, representing the outcome variable.</p>
</td></tr>
<tr><td><code id="sps.internal_+3A_x">X</code></td>
<td>
<p>Numeric: A matrix of observations, whose number of columns
corresponds to the number of predictors in the model, and the number
of rows should be conformal with the number of entries in <code class="reqn">y</code>. This
matrix may contain both endogenous and exogenous variables.</p>
</td></tr>
<tr><td><code id="sps.internal_+3A_z">Z</code></td>
<td>
<p>Numeric: A matrix of observations representing the
intrumental variables (IVs) in the first-stage structural equation. The
number of IVs should be at least as large as the number of
endogenous variables in <code class="reqn">X</code>.</p>
</td></tr>
<tr><td><code id="sps.internal_+3A_ref">REF</code></td>
<td>
<p>Character: Controls the choice of the <em>reference</em>
estimator in the SPS framework. This can accept two values: &quot;TSLS&quot; or &quot;JIVE&quot;,
with the former being the default option. The <em>alternative</em>
estimator is always the OLS estimator.</p>
</td></tr>   
<tr><td><code id="sps.internal_+3A_alpha">ALPHA</code></td>
<td>
<p>Logical: If TRUE, the function returns the value of the
sample estimate of the parameter controlling the respective
contribution of the <em>reference</em> estimator (by default, this is the TSLS
estimator), and the one of the <em>alternative</em> estimator (by
default, this is the OLS estimator).</p>
</td></tr>
<tr><td><code id="sps.internal_+3A_n.btj">n.btj</code></td>
<td>
<p>Numeric: The number of boostrap iterations performed, when
computing the SPS estimator, when using the JIVE as reference
estimator. This option is only relevant, when JIVE has   
been selected as the reference estimator. These iterations are used
to compute the various components entering in the calculation of the
SPS estimator.</p>
</td></tr>  
</table>


<h3>Details</h3>

<p>See documentaion for the sps.est function. Users should use the
sps.est function, instead. 
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>list</code></td>
<td>
<p>The first term (est) is a vector of estimates for the
coefficients of interest, and the second term (alpha) representing
the estimate of the contribution of the OLS to the combined SPS
estimator.</p>
</td></tr> 
</table>


<h3>Author(s)</h3>

<p>Cedric E. Ginestet &lt;cedric.ginestet@kcl.ac.uk&gt;
</p>


<h3>References</h3>

<p>Judge, G.G. and Mittelhammer, R.C. (2004). A semiparametric basis for combining esti-
mation problems under quadratic loss. Journal of the American Statistical Association,
99(466), 479&ndash;487.
</p>
<p>Judge, G.G. and Mittelhammer, R.C. (2012a). An information theoretic approach to econo-
metrics. Cambridge University Press.
</p>
<p>Judge, G. and Mittelhammer, R. (2012b). A risk superior semiparametric estimator for
over-identified linear models. Advances in Econometrics, 237&ndash;255.
</p>
<p>Judge, G. and Mittelhammer, R. (2013). A minimum mean squared error semiparametric
combining estimator. Advances in Econometrics, 55&ndash;85.
</p>
<p>Mittelhammer, R.C. and Judge, G.G. (2005). Combining estimators to improve structural
model estimation and inference under quadratic loss. Journal of econometrics, 128(1),
1&ndash;29.
</p>

<hr>
<h2 id='tr'>Trace of a matrix.</h2><span id='topic+tr'></span>

<h3>Description</h3>

<p>Compute the trace of a square matrix.</p>


<h3>Usage</h3>

<pre><code class='language-R'>  tr(X)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tr_+3A_x">X</code></td>
<td>
<p>Numeric: A square matrix.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This computes the sum of the diagonal elements of a square matrix.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>numeric</code></td>
<td>
<p>A real number.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Cedric E. Ginestet &lt;cedric.ginestet@kcl.ac.uk&gt;
</p>

<hr>
<h2 id='tsls.est'>The Two-Stage Least Squares (TSLS) estimator.</h2><span id='topic+tsls.est'></span>

<h3>Description</h3>

<p>Computes the TSLS estimator for a two-stage structural model, as well
as the set of standard errors for each individual estimator, and
the sample estimate of the asymptotic variance/covariance matrix.</p>


<h3>Usage</h3>

<pre><code class='language-R'>  tsls.est(y,X,Z,SE=FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tsls.est_+3A_y">y</code></td>
<td>
<p>Numeric: A vector of observations, representing the outcome variable.</p>
</td></tr>
<tr><td><code id="tsls.est_+3A_x">X</code></td>
<td>
<p>Numeric: A matrix of observations, whose number of columns
corresponds to the number of predictors in the model, and the number
of rows should be conformal with the number of entries in <code class="reqn">y</code>. This
matrix may contain both endogenous and exogenous variables.</p>
</td></tr>
<tr><td><code id="tsls.est_+3A_z">Z</code></td>
<td>
<p>Numeric: A matrix of observations representing the
intrumental variables (IVs) in the first-stage structural equation. The
number of IVs should be at least as large as the number of
endogenous variables in <code class="reqn">X</code>.</p>
</td></tr>
<tr><td><code id="tsls.est_+3A_se">SE</code></td>
<td>
<p>Logical: If TRUE, then the function also returns the
standard errors of the individual TSLS estimator, and a sample
estimate of its asymptotic variance/covariance matrix.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The TSLS estimator is applied to a two-stage structural
model. We here adopt the terminology commonly used in
econometrics. See, for example, the references below for Cameron 
and Trivedi (2005), Davidson and MacKinnon (1993), as well as
Wooldridge (2002). The second-stage equation is thus modelled as follows,
</p>
<p style="text-align: center;"><code class="reqn">y = X\beta + \epsilon,</code>
</p>

<p>in which <code class="reqn">y</code> is a vector of <code class="reqn">n</code> observations representing the
outcome variable, <code class="reqn">X</code> is a matrix of order <code class="reqn">n\times k</code>
denoting the predictors of the model, and comprised of both exogenous
and endogenous variables, <code class="reqn">\beta</code> is the <code class="reqn">k</code>-dimensional
vector of parameters of interest; whereas <code class="reqn">\epsilon</code> is an unknown
vector of error terms.
</p>
<p>The first-stage level of the model is given by a multivariate
multiple regression. That is, this is a linear modle with a
<em>multivariate</em> outcome variable, as well as <em>multiple</em>
predictors. This first-stage model is represented in this manner, 
</p>
<p style="text-align: center;"><code class="reqn">X = Z\Gamma + \Delta</code>
</p>
<p>,
where <code class="reqn">X</code> is the matrix of predictors from the second-stage
equation, <code class="reqn">Z</code> is a matrix of instrumental variables (IVs) of order
<code class="reqn">n \times l</code>, <code class="reqn">\Gamma</code> is a matrix of unknown parameters of
order <code class="reqn">l\times k</code>; whereas <code class="reqn">\Delta</code> denotes an unknown matrix of order
<code class="reqn">n\times k</code> of error terms. 
</p>
<p>Whenever certain variables in <code class="reqn">X</code> are assumed to be exogenous,
these variables should be incorporated into 
<code class="reqn">Z</code>. That is, all the exogneous variables are their own
instruments. Moreover, it is also assumed that the model contains at
least as many instruments as predictors, in the sense that <code class="reqn">l\geq
    k</code>, as commonly donein practice (Wooldridge, 2002). Also, the matrices,
<code class="reqn">X^TX</code>, <code class="reqn">Z^TX</code>, and <code class="reqn">Z^TZ</code> are all assumed to be full
rank. Finally, both <code class="reqn">X</code> and <code class="reqn">Z</code> should comprise a column of
one's, representing the intercept in each structural equation. 
</p>
<p>The formula for the TSLS estimator is then obtained in the standard
fashion by the following equation,
</p>
<p style="text-align: center;"><code class="reqn">\hat\beta_{TSLS} := (\hat{X}^T\hat{X})^{-1}(\hat{X}^{T}y),</code>
</p>

<p>where <code class="reqn">\hat{X}:=H_zX</code>, is the orthogonal projection of the matrix
<code class="reqn">X</code>, onto the vector space spanned by the columns of <code class="reqn">Z</code>; and
<code class="reqn">H_{z}:=Z(Z^{T}Z)^{-1}Z^{T}</code> is the hat matrix of the first-stage
multivariate regression.
</p>
<p>When requested by the user, the standard errors of each entry in
<code class="reqn">\hat\beta_{TSLS}</code> are also provided, as a vector. These are
computed by taking the squareroot of the diagonal entries of the sample asymptotic
variance/covariance matrix, which is given by the following equation,
</p>
<p style="text-align: center;"><code class="reqn">\hat\Sigma_{TSLS} := \hat\sigma^{2}(\hat{X}^{T}\hat{X})^{-1},</code>
</p>

<p>in which the sample residual sum of squares is
<code class="reqn">\hat\sigma^{2}:=(y - X\hat\beta_{TSLS})^{T}(y - X\hat\beta_{TSLS})/(n-k)</code>.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>list</code></td>
<td>
<p>A list with one or three arguments, depending on whether
the user has activated the SE flag. The first element (est) in the list
is the TSLS estimate of the model in vector format. The second
element (se) is the vector of standard errors; and the third
element (var) is the sample estimate of the asymptotic
variance/covariance matrix.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Cedric E. Ginestet &lt;cedric.ginestet@kcl.ac.uk&gt;
</p>


<h3>References</h3>

<p>Cameron, A. and Trivedi, P. (2005). Microeconometrics: Methods and Applications. Cam-
bridge University press, Cambridge.
</p>
<p>Davidson, R. and MacKinnon, J.G. (1993). Estimation and inference in econometrics. OUP
Catalogue.
</p>
<p>Wooldridge, J. (2002). Econometric analysis of cross-section and panel data. MIT press,
London.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### Generate a simple example with synthetic data, and no intercept. 
n &lt;- 100; k &lt;- 3; l &lt;- 3;
Ga&lt;- diag(rep(1,l)); be &lt;- rep(1,k);
Z &lt;- matrix(0,n,l); for(j in 1:l) Z[,j] &lt;- rnorm(n); 
X &lt;- matrix(0,n,k); for(j in 1:k) X[,j] &lt;- Z[,j]*Ga[j,j] + rnorm(n); 
y &lt;- X%*%be + rnorm(n);

### Compute TSLS estimator with SEs and variance/covariance matrix.
print(tsls.est(y,X,Z));
print(tsls.est(y,X,Z,SE=TRUE));

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
