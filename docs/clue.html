<!DOCTYPE html><html lang="en"><head><title>Help for package clue</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {clue}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#addtree'><p>Additive Tree Distances</p></a></li>
<li><a href='#Cassini'><p>Cassini Data</p></a></li>
<li><a href='#CKME'><p>Cassini Data Partitions Obtained by K-Means</p></a></li>
<li><a href='#cl_agreement'><p>Agreement Between Partitions or Hierarchies</p></a></li>
<li><a href='#cl_bag'><p>Bagging for Clustering</p></a></li>
<li><a href='#cl_boot'><p>Bootstrap Resampling of Clustering Algorithms</p></a></li>
<li><a href='#cl_classes'><p>Cluster Classes</p></a></li>
<li><a href='#cl_consensus'><p>Consensus Partitions and Hierarchies</p></a></li>
<li><a href='#cl_dissimilarity'><p>Dissimilarity Between Partitions or Hierarchies</p></a></li>
<li><a href='#cl_ensemble'><p>Cluster Ensembles</p></a></li>
<li><a href='#cl_fuzziness'><p>Partition Fuzziness</p></a></li>
<li><a href='#cl_margin'><p>Membership Margins</p></a></li>
<li><a href='#cl_medoid'><p>Medoid Partitions and Hierarchies</p></a></li>
<li><a href='#cl_membership'><p>Memberships of Partitions</p></a></li>
<li><a href='#cl_object_names'><p>Find Object Names</p></a></li>
<li><a href='#cl_pam'><p>K-Medoids Partitions of Clusterings</p></a></li>
<li><a href='#cl_pclust'><p>Prototype-Based Partitions of Clusterings</p></a></li>
<li><a href='#cl_predict'><p>Predict Memberships</p></a></li>
<li><a href='#cl_prototypes'><p>Partition Prototypes</p></a></li>
<li><a href='#cl_tabulate'><p>Tabulate Vector Objects</p></a></li>
<li><a href='#cl_ultrametric'><p>Ultrametrics of Hierarchies</p></a></li>
<li><a href='#cl_validity'><p>Validity Measures for Partitions and Hierarchies</p></a></li>
<li><a href='#fit_ultrametric_target'><p>Fit Dissimilarities to a Hierarchy</p></a></li>
<li><a href='#GVME'><p>Gordon-Vichi Macroeconomic Partition Ensemble Data</p></a></li>
<li><a href='#GVME_Consensus'><p>Gordon-Vichi Macroeconomic Consensus Partition Data</p></a></li>
<li><a href='#hierarchy'><p>Hierarchies</p></a></li>
<li><a href='#Kinship82'><p>Rosenberg-Kim Kinship Terms Partition Data</p></a></li>
<li><a href='#Kinship82_Consensus'><p>Gordon-Vichi Kinship82 Consensus Partition Data</p></a></li>
<li><a href='#kmedoids'><p>K-Medoids Clustering</p></a></li>
<li><a href='#l1_fit_ultrametric'><p>Least Absolute Deviation Fit of Ultrametrics to Dissimilarities</p></a></li>
<li><a href='#lattice'><p>Cluster Lattices</p></a></li>
<li><a href='#ls_fit_addtree'><p>Least Squares Fit of Additive Tree Distances to Dissimilarities</p></a></li>
<li><a href='#ls_fit_sum_of_ultrametrics'><p>Least Squares Fit of Sums of Ultrametrics to Dissimilarities</p></a></li>
<li><a href='#ls_fit_ultrametric'><p>Least Squares Fit of Ultrametrics to Dissimilarities</p></a></li>
<li><a href='#n_of_classes'><p>Classes in a Partition</p></a></li>
<li><a href='#n_of_objects'><p>Number of Objects in a Partition or Hierarchy</p></a></li>
<li><a href='#partition'><p>Partitions</p></a></li>
<li><a href='#pclust'><p>Prototype-Based Partitioning</p></a></li>
<li><a href='#Phonemes'><p>Miller-Nicely Consonant Phoneme Confusion Data</p></a></li>
<li><a href='#solve_LSAP'><p>Solve Linear Sum Assignment Problem</p></a></li>
<li><a href='#sumt'><p>Sequential Unconstrained Minimization Technique</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Version:</td>
<td>0.3-66</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Title:</td>
<td>Cluster Ensembles</td>
</tr>
<tr>
<td>Description:</td>
<td>CLUster Ensembles.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.2.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, cluster, graphics, methods</td>
</tr>
<tr>
<td>Suggests:</td>
<td>e1071, lpSolve (&ge; 5.5.7), quadprog (&ge; 1.4-8), relations</td>
</tr>
<tr>
<td>Enhances:</td>
<td>RWeka, ape, cba, cclust, flexclust, flexmix, kernlab, mclust,
movMF, modeltools</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-11-13 06:15:44 UTC; hornik</td>
</tr>
<tr>
<td>Author:</td>
<td>Kurt Hornik <a href="https://orcid.org/0000-0003-4198-9911"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre],
  Walter Böhm [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Kurt Hornik &lt;Kurt.Hornik@R-project.org&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-11-13 07:13:18 UTC</td>
</tr>
</table>
<hr>
<h2 id='addtree'>Additive Tree Distances</h2><span id='topic+as.cl_addtree'></span>

<h3>Description</h3>

<p>Objects representing additive tree distances.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.cl_addtree(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="addtree_+3A_x">x</code></td>
<td>
<p>an R object representing additive tree distances.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Additive tree distances are object dissimilarities <code class="reqn">d</code> satisfying
the so-called <em>additive tree conditions</em>, also known as
<em>four-point conditions</em> <code class="reqn">d_{ij} + d_{kl} \le \max(d_{ik} +
    d_{jl}, d_{il} + d_{jk})</code> for all quadruples <code class="reqn">i, j, k, l</code>.
Equivalently, for each such quadruple, the largest two values of the
sums <code class="reqn">d_{ij} + d_{kl}</code>, <code class="reqn">d_{ik} + d_{jl}</code>, and <code class="reqn">d_{il} +
    d_{jk}</code> must be equal.
Centroid distances are additive tree distances where the inequalities
in the four-point conditions are strengthened to equalities (such that
all three sums are equal), and can be represented as <code class="reqn">d_{ij} = g_i
    + g_j</code>, i.e., as sums of distances from a &ldquo;centroid&rdquo;.
See, e.g., Barthélémy and Guénoche (1991) for more details on additive
tree distances.
</p>
<p><code>as.cl_addtree</code> is a generic function.  Its default method can
handle objects representing ultrametric distances and raw additive
distance matrices.  In addition, there is a method for coercing
objects of class <code>"<a href="ape.html#topic+as.phylo">phylo</a>"</code> from package
<span class="pkg">ape</span>.
</p>
<p>Functions <code><a href="#topic+ls_fit_addtree">ls_fit_addtree</a></code> and
<code><a href="#topic+ls_fit_centroid">ls_fit_centroid</a></code> can be used to find the additive tree
distance or centroid distance minimizing least squares distance
(Euclidean dissimilarity) to a given dissimilarity object.
</p>
<p>There is a <code><a href="base.html#topic+plot">plot</a></code> method for additive tree distances.
</p>


<h3>Value</h3>

<p>An object of class <code>"cl_addtree"</code> containing the additive
tree distances.
</p>


<h3>References</h3>

<p>J.-P. Barthélémy and A. Guénoche (1991).
<em>Trees and proximity representations</em>.
Chichester: John Wiley &amp; Sons.
ISBN 0-471-92263-3.
</p>

<hr>
<h2 id='Cassini'>Cassini Data</h2><span id='topic+Cassini'></span>

<h3>Description</h3>

<p>A Cassini data set with 1000 points in 2-dimensional space which are
drawn from the uniform distribution on 3 structures.  The two outer
structures are banana-shaped; the &ldquo;middle&rdquo; structure in between
them is a circle.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("Cassini")</code></pre>


<h3>Format</h3>

<p>A classed list with components
</p>

<dl>
<dt><code>x</code></dt><dd><p>a matrix with 1000 rows and 2 columns giving the
coordinates of the points.</p>
</dd>
<dt><code>classes</code></dt><dd><p>a factor indicating which structure the
respective points belong to.</p>
</dd>
</dl>



<h3>Details</h3>

<p>Instances of Cassini data sets can be created using function
<code><a href="mlbench.html#topic+mlbench.cassini">mlbench.cassini</a></code> in package <span class="pkg">mlbench</span>.
The data set at hand was obtained using
</p>
<pre>
    library("mlbench")
    set.seed(1234)
    Cassini &lt;- mlbench.cassini(1000)
  </pre>


<h3>Examples</h3>

<pre><code class='language-R'>data("Cassini")
op &lt;- par(mfcol = c(1, 2))
## Plot the data set:
plot(Cassini$x, col = as.integer(Cassini$classes),
     xlab = "", ylab = "")
## Create a "random" k-means partition of the data:
set.seed(1234)
party &lt;- kmeans(Cassini$x, 3)
## And plot that.
plot(Cassini$x, col = cl_class_ids(party),
     xlab = "", ylab = "")
## (We can see the problem ...)
par(op)
</code></pre>

<hr>
<h2 id='CKME'>Cassini Data Partitions Obtained by K-Means</h2><span id='topic+CKME'></span>

<h3>Description</h3>

<p>A cluster ensemble of 50 <code class="reqn">k</code>-means partitions of the Cassini data
into three classes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("CKME")</code></pre>


<h3>Format</h3>

<p>A cluster ensemble of 50 (<code class="reqn">k</code>-means) partitions.
</p>


<h3>Details</h3>

<p>The ensemble was generated via
</p>
<pre>
    require("clue")
    data("Cassini")
    set.seed(1234)
    CKME &lt;- cl_boot(Cassini$x, 50, 3)
  </pre>

<hr>
<h2 id='cl_agreement'>Agreement Between Partitions or Hierarchies</h2><span id='topic+cl_agreement'></span>

<h3>Description</h3>

<p>Compute the agreement between (ensembles) of partitions or
hierarchies.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cl_agreement(x, y = NULL, method = "euclidean", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cl_agreement_+3A_x">x</code></td>
<td>
<p>an ensemble of partitions or hierarchies and dissimilarities,
or something coercible to that (see <code><a href="#topic+cl_ensemble">cl_ensemble</a></code>).</p>
</td></tr>
<tr><td><code id="cl_agreement_+3A_y">y</code></td>
<td>
<p><code>NULL</code> (default), or as for <code>x</code>.</p>
</td></tr>
<tr><td><code id="cl_agreement_+3A_method">method</code></td>
<td>
<p>a character string specifying one of the built-in
methods for computing agreement, or a function to be taken as
a user-defined method.  If a character string, its lower-cased
version is matched against the lower-cased names of the available
built-in methods using <code><a href="base.html#topic+pmatch">pmatch</a></code>.  See <b>Details</b> for
available built-in methods.</p>
</td></tr>
<tr><td><code id="cl_agreement_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>y</code> is given, its components must be of the same kind as those
of <code>x</code> (i.e., components must either all be partitions, or all be
hierarchies or dissimilarities).
</p>
<p>If all components are partitions, the following built-in methods for
measuring agreement between two partitions with respective membership
matrices <code class="reqn">u</code> and <code class="reqn">v</code> (brought to a common number of columns)
are available:
</p>

<dl>
<dt><code>"euclidean"</code></dt><dd><p><code class="reqn">1 - d / m</code>, where <code class="reqn">d</code> is the
Euclidean dissimilarity of the memberships, i.e., the square root
of the minimal sum of the squared differences of <code class="reqn">u</code> and all
column permutations of <code class="reqn">v</code>, and <code class="reqn">m</code> is an upper bound for
the maximal Euclidean dissimilarity.  See Dimitriadou, Weingessel
and Hornik (2002).</p>
</dd>
<dt><code>"manhattan"</code></dt><dd><p><code class="reqn">1 - d / m</code>, where <code class="reqn">d</code> is the
Manhattan dissimilarity of the memberships, i.e., the minimal
sum of the absolute differences of <code class="reqn">u</code> and all column
permutations of <code class="reqn">v</code>, and <code class="reqn">m</code> is an upper bound for the
maximal Manhattan dissimilarity.</p>
</dd>
<dt><code>"Rand"</code></dt><dd><p>the Rand index (the rate of distinct pairs of
objects both in the same class or both in different classes in
both partitions), see Rand (1971) or Gordon (1999), page 198.
For soft partitions, (currently) the Rand index of the
corresponding nearest hard partitions is used.</p>
</dd>
<dt><code>"cRand"</code></dt><dd><p>the Rand index corrected for agreement by
chance, see Hubert and Arabie (1985) or Gordon (1999), page 198.
Can only be used for hard partitions.</p>
</dd>
<dt><code>"NMI"</code></dt><dd><p>Normalized Mutual Information, see Strehl and
Ghosh (2002).  For soft partitions, (currently) the NMI of the
corresponding nearest hard partitions is used.</p>
</dd> 
<dt><code>"KP"</code></dt><dd><p>the Katz-Powell index, i.e., the product-moment
correlation coefficient between the elements of the co-membership
matrices <code class="reqn">C(u) = u u'</code> and <code class="reqn">C(v)</code>, respectively, see Katz
and Powell (1953).  For soft partitions, (currently) the
Katz-Powell index of the corresponding nearest hard partitions is
used.  (Note that for hard partitions, the <code class="reqn">(i,j)</code> entry of
<code class="reqn">C(u)</code> is one iff objects <code class="reqn">i</code> and <code class="reqn">j</code> are in the same
class.)</p>
</dd>
<dt><code>"angle"</code></dt><dd><p>the maximal cosine of the angle between the
elements of <code class="reqn">u</code> and all column permutations of <code class="reqn">v</code>.</p>
</dd>
<dt><code>"diag"</code></dt><dd><p>the maximal co-classification rate, i.e., the
maximal rate of objects with the same class ids in both
partitions after arbitrarily permuting the ids.</p>
</dd>
<dt><code>"FM"</code></dt><dd><p>the index of Fowlkes and Mallows (1983), i.e.,
the ratio <code class="reqn">N_{xy} / \sqrt{N_x N_y}</code> of
the number <code class="reqn">N_{xy}</code> of distinct pairs of objects in the
same class in both partitions and the geometric mean of the
numbers <code class="reqn">N_x</code> and <code class="reqn">N_y</code> of distinct pairs of objects in
the same class in partition <code class="reqn">x</code> and partition <code class="reqn">y</code>,
respectively.
For soft partitions, (currently) the Fowlkes-Mallows index of the
corresponding nearest hard partitions is used.</p>
</dd>
<dt><code>"Jaccard"</code></dt><dd><p>the Jaccard index, i.e., the ratio of the
numbers of distinct pairs of objects in the same class in both
partitions and in at least one partition, respectively.
For soft partitions, (currently) the Jaccard index of the
corresponding nearest hard partitions is used.</p>
</dd>
<dt><code>"purity"</code></dt><dd><p>the purity of the classes of <code>x</code> with
respect to those of <code>y</code>, i.e.,
<code class="reqn">\sum_j \max_i n_{ij} / n</code>,
where <code class="reqn">n_{ij}</code> is the joint frequency of objects in class
<code class="reqn">i</code> for <code>x</code> and in class <code class="reqn">j</code> for <code>y</code>, and
<code class="reqn">n</code> is the total number of objects.</p>
</dd>
<dt><code>"PS"</code></dt><dd><p>Prediction Strength, see Tibshirani and Walter
(2005): the minimum, over all classes <code class="reqn">j</code> of <code>y</code>, of the
maximal rate of objects in the same class for <code>x</code> and in
class <code class="reqn">j</code> for <code>y</code>.</p>
</dd>
</dl>

<p>If all components are hierarchies, available built-in methods for
measuring agreement between two hierarchies with respective
ultrametrics <code class="reqn">u</code> and <code class="reqn">v</code> are as follows.
</p>

<dl>
<dt><code>"euclidean"</code></dt><dd><p><code class="reqn">1 / (1 + d)</code>, where <code class="reqn">d</code> is the
Euclidean dissimilarity of the ultrametrics (i.e., the square root
of the sum of the squared differences of <code class="reqn">u</code> and <code class="reqn">v</code>).</p>
</dd>
<dt><code>"manhattan"</code></dt><dd><p><code class="reqn">1 / (1 + d)</code>, where <code class="reqn">d</code> is the
Manhattan dissimilarity of the ultrametrics (i.e., the sum of the
absolute differences of <code class="reqn">u</code> and <code class="reqn">v</code>).</p>
</dd>
<dt><code>"cophenetic"</code></dt><dd><p>The cophenetic correlation coefficient.
(I.e., the product-moment correlation of the ultrametrics.)</p>
</dd>
<dt><code>"angle"</code></dt><dd><p>the cosine of the angle between the
ultrametrics.</p>
</dd>
<dt><code>"gamma"</code></dt><dd><p><code class="reqn">1 - d</code>, where <code class="reqn">d</code> is the rate of
inversions between the associated ultrametrics (i.e., the rate of
pairs <code class="reqn">(i,j)</code> and <code class="reqn">(k,l)</code> for which <code class="reqn">u_{ij} &lt; u_{kl}</code>
and <code class="reqn">v_{ij} &gt; v_{kl}</code>).  (This agreement measure is a linear
transformation of Kruskal's <code class="reqn">\gamma</code>.)</p>
</dd>
</dl>

<p>The measures based on ultrametrics also allow computing agreement with
&ldquo;raw&rdquo; dissimilarities on the underlying objects (R objects
inheriting from class <code>"dist"</code>).
</p>
<p>If a user-defined agreement method is to be employed, it must be a
function taking two clusterings as its arguments.
</p>
<p>Symmetric agreement objects of class <code>"cl_agreement"</code> are
implemented as symmetric proximity objects with self-proximities
identical to one, and inherit from class <code>"cl_proximity"</code>.  They
can be coerced to dense square matrices using <code>as.matrix</code>.  It is
possible to use 2-index matrix-style subscripting for such objects;
unless this uses identical row and column indices, this results in a
(non-symmetric agreement) object of class <code>"cl_cross_agreement"</code>.
</p>


<h3>Value</h3>

<p>If <code>y</code> is <code>NULL</code>, an object of class <code>"cl_agreement"</code>
containing the agreements between the all pairs of components of
<code>x</code>.  Otherwise, an object of class <code>"cl_cross_agreement"</code>
with the agreements between the components of <code>x</code> and the
components of <code>y</code>.
</p>


<h3>References</h3>

<p>E. Dimitriadou, A. Weingessel and K. Hornik (2002).
A combination scheme for fuzzy clustering.
<em>International Journal of Pattern Recognition and Artificial
Intelligence</em>, <b>16</b>, 901&ndash;912. <br />
<a href="https://doi.org/10.1142/S0218001402002052">doi:10.1142/S0218001402002052</a>.
</p>
<p>E. B. Fowlkes and C. L. Mallows (1983).
A method for comparing two hierarchical clusterings.
<em>Journal of the American Statistical Association</em>, <b>78</b>,
553&ndash;569. <br />
<a href="https://doi.org/10.1080/01621459.1983.10478008">doi:10.1080/01621459.1983.10478008</a>.
</p>
<p>A. D. Gordon (1999).
<em>Classification</em> (2nd edition).
Boca Raton, FL: Chapman &amp; Hall/CRC.
</p>
<p>L. Hubert and P. Arabie (1985).
Comparing partitions.
<em>Journal of Classification</em>, <b>2</b>, 193&ndash;218.
<a href="https://doi.org/10.1007/bf01908075">doi:10.1007/bf01908075</a>.
</p>
<p>W. M. Rand (1971).
Objective criteria for the evaluation of clustering methods.
<em>Journal of the American Statistical Association</em>, <b>66</b>,
846&ndash;850.
<a href="https://doi.org/10.2307/2284239">doi:10.2307/2284239</a>.
</p>
<p>L. Katz and J. H. Powell (1953).
A proposed index of the conformity of one sociometric measurement to
another.
<em>Psychometrika</em>, <b>18</b>, 249&ndash;256.
<a href="https://doi.org/10.1007/BF02289063">doi:10.1007/BF02289063</a>.
</p>
<p>A. Strehl and J. Ghosh (2002).
Cluster ensembles &mdash; A knowledge reuse framework for combining
multiple partitions.
<em>Journal of Machine Learning Research</em>, <b>3</b>, 583&ndash;617. <br />
<a href="https://www.jmlr.org/papers/volume3/strehl02a/strehl02a.pdf">https://www.jmlr.org/papers/volume3/strehl02a/strehl02a.pdf</a>.
</p>
<p>R. Tibshirani and G. Walter (2005).
Cluster validation by Prediction Strength.
<em>Journal of Computational and Graphical Statistics</em>, <b>14</b>/3,
511&ndash;528.
<a href="https://doi.org/10.1198/106186005X59243">doi:10.1198/106186005X59243</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cl_dissimilarity">cl_dissimilarity</a></code>;
<code><a href="e1071.html#topic+classAgreement">classAgreement</a></code> in package <span class="pkg">e1071</span>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## An ensemble of partitions.
data("CKME")
pens &lt;- CKME[1 : 20]		# for saving precious time ...
summary(c(cl_agreement(pens)))
summary(c(cl_agreement(pens, method = "Rand")))
summary(c(cl_agreement(pens, method = "diag")))
cl_agreement(pens[1:5], pens[6:7], method = "NMI")
## Equivalently, using subscripting.
cl_agreement(pens, method = "NMI")[1:5, 6:7]

## An ensemble of hierarchies.
d &lt;- dist(USArrests)
hclust_methods &lt;-
    c("ward", "single", "complete", "average", "mcquitty")
hclust_results &lt;- lapply(hclust_methods, function(m) hclust(d, m))
names(hclust_results) &lt;- hclust_methods 
hens &lt;- cl_ensemble(list = hclust_results)
summary(c(cl_agreement(hens)))
## Note that the Euclidean agreements are *very* small.
## This is because the ultrametrics differ substantially in height:
u &lt;- lapply(hens, cl_ultrametric)
round(sapply(u, max), 3)
## Rescaling the ultrametrics to [0, 1] gives:
u &lt;- lapply(u, function(x) (x - min(x)) / (max(x) - min(x)))
shens &lt;- cl_ensemble(list = lapply(u, as.cl_dendrogram))
summary(c(cl_agreement(shens)))
## Au contraire ...
summary(c(cl_agreement(hens, method = "cophenetic")))
cl_agreement(hens[1:3], hens[4:5], method = "gamma")
</code></pre>

<hr>
<h2 id='cl_bag'>Bagging for Clustering</h2><span id='topic+cl_bag'></span>

<h3>Description</h3>

<p>Construct partitions of objects by running a base clustering algorithm
on bootstrap samples from a given data set, and &ldquo;suitably&rdquo;
aggregating these primary partitions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cl_bag(x, B, k = NULL, algorithm = "kmeans", parameters = NULL, 
       method = "DFBC1", control = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cl_bag_+3A_x">x</code></td>
<td>
<p>the data set of objects to be clustered, as appropriate for
the base clustering algorithm.</p>
</td></tr>
<tr><td><code id="cl_bag_+3A_b">B</code></td>
<td>
<p>an integer giving the number of bootstrap replicates.</p>
</td></tr>
<tr><td><code id="cl_bag_+3A_k">k</code></td>
<td>
<p><code>NULL</code> (default), or an integer giving the number of
classes to be used for a partitioning base algorithm.</p>
</td></tr>
<tr><td><code id="cl_bag_+3A_algorithm">algorithm</code></td>
<td>
<p>a character string or function specifying the base
clustering algorithm.</p>
</td></tr>
<tr><td><code id="cl_bag_+3A_parameters">parameters</code></td>
<td>
<p>a named list of additional arguments to be passed to
the base algorithm.</p>
</td></tr>
<tr><td><code id="cl_bag_+3A_method">method</code></td>
<td>
<p>a character string indicating the bagging method to
use.  Currently, only method <code>"DFBC1"</code> is available, which
implements algorithm <em>BagClust1</em> in Dudoit &amp; Fridlyand (2003).</p>
</td></tr>
<tr><td><code id="cl_bag_+3A_control">control</code></td>
<td>
<p>a list of control parameters for the aggregation.
Currently, not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Bagging for clustering is really a rather general conceptual framework
than a specific algorithm.  If the primary partitions generated in the
bootstrap stage form a cluster ensemble (so that class memberships of
the objects in <code>x</code> can be obtained), consensus methods for
cluster ensembles (as implemented, e.g., in <code><a href="#topic+cl_consensus">cl_consensus</a></code>
and <code><a href="#topic+cl_medoid">cl_medoid</a></code>) can be employed for the aggregation
stage.  In particular, (possibly new) bagging algorithms can easily be
realized by directly running <code><a href="#topic+cl_consensus">cl_consensus</a></code> on the results
of <code><a href="#topic+cl_boot">cl_boot</a></code>.
</p>
<p>In BagClust1, aggregation proceeds by generating a reference partition
by running the base clustering algorithm on the whole given data set,
and averaging the ensemble memberships after optimally matching them
to the reference partition (in fact, by minimizing Euclidean
dissimilarity, see <code><a href="#topic+cl_dissimilarity">cl_dissimilarity</a></code>).
</p>
<p>If the base clustering algorithm yields prototypes, aggregation can be
based on clustering these.  This is the idea underlying the
&ldquo;Bagged Clustering&rdquo; algorithm introduced in Leisch (1999) and
implemented by function <code><a href="e1071.html#topic+bclust">bclust</a></code> in package
<span class="pkg">e1071</span>.
</p>


<h3>Value</h3>

<p>An R object representing a partition of the objects given in <code>x</code>.
</p>


<h3>References</h3>

<p>S. Dudoit and J. Fridlyand (2003).
Bagging to improve the accuracy of a clustering procedure.
<em>Bioinformatics</em>, <b>19</b>/9, 1090&ndash;1099.
<a href="https://doi.org/10.1093/bioinformatics/btg038">doi:10.1093/bioinformatics/btg038</a>.
</p>
<p>F. Leisch (1999).
<em>Bagged Clustering</em>.
Working Paper 51, SFB &ldquo;Adaptive Information Systems and
Modeling in Economics and Management Science&rdquo;.
<a href="https://doi.org/10.57938/9b129f95-b53b-44ce-a129-5b7a1168d832">doi:10.57938/9b129f95-b53b-44ce-a129-5b7a1168d832</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1234)
## Run BagClust1 on the Cassini data.
data("Cassini")
party &lt;- cl_bag(Cassini$x, 50, 3)
plot(Cassini$x, col = cl_class_ids(party), xlab = "", ylab = "")
## Actually, using fuzzy c-means as a base learner works much better:
if(require("e1071", quietly = TRUE)) {
    party &lt;- cl_bag(Cassini$x, 20, 3, algorithm = "cmeans")
    plot(Cassini$x, col = cl_class_ids(party), xlab = "", ylab = "")
}
</code></pre>

<hr>
<h2 id='cl_boot'>Bootstrap Resampling of Clustering Algorithms</h2><span id='topic+cl_boot'></span>

<h3>Description</h3>

<p>Generate bootstrap replicates of the results of applying a
&ldquo;base&rdquo; clustering algorithm to a given data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cl_boot(x, B, k = NULL,
        algorithm = if (is.null(k)) "hclust" else "kmeans", 
        parameters = list(), resample = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cl_boot_+3A_x">x</code></td>
<td>
<p>the data set of objects to be clustered, as appropriate for
the base clustering algorithm.</p>
</td></tr>
<tr><td><code id="cl_boot_+3A_b">B</code></td>
<td>
<p>an integer giving the number of bootstrap replicates.</p>
</td></tr>
<tr><td><code id="cl_boot_+3A_k">k</code></td>
<td>
<p><code>NULL</code> (default), or an integer giving the number of
classes to be used for a partitioning base algorithm.</p>
</td></tr>
<tr><td><code id="cl_boot_+3A_algorithm">algorithm</code></td>
<td>
<p>a character string or function specifying the base
clustering algorithm.</p>
</td></tr>
<tr><td><code id="cl_boot_+3A_parameters">parameters</code></td>
<td>
<p>a named list of additional arguments to be passed to
the base algorithm.</p>
</td></tr>
<tr><td><code id="cl_boot_+3A_resample">resample</code></td>
<td>
<p>a logical indicating whether the data should be
resampled in addition to &ldquo;sampling from the algorithm&rdquo;.
If resampling is used, the class memberships of the objects given in
<code>x</code> are predicted from the results of running the base
algorithm on bootstrap samples of <code>x</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a rather simple-minded function with limited applicability,
and mostly useful for studying the effect of (uncontrolled) random
initializations of fixed-point partitioning algorithms such as
<code><a href="stats.html#topic+kmeans">kmeans</a></code> or <code><a href="e1071.html#topic+cmeans">cmeans</a></code>, see the
examples.  To study the effect of varying control parameters or
explicitly providing random starting values, the respective cluster
ensemble has to be generated explicitly (most conveniently by using
<code><a href="base.html#topic+replicate">replicate</a></code> to create a list <code>lst</code> of suitable
instances of clusterings obtained by the base algorithm, and using
<code>cl_ensemble(list = lst)</code> to create the ensemble).
</p>


<h3>Value</h3>

<p>A cluster ensemble of length <code class="reqn">B</code>, with either (if resampling is
not used, default) the results of running the base algorithm on the
given data set, or (if resampling is used) the memberships for the
given data predicted from the results of running the base algorithm on
bootstrap samples of the data.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Study e.g. the effect of random kmeans() initializations.
data("Cassini")
pens &lt;- cl_boot(Cassini$x, 15, 3)
diss &lt;- cl_dissimilarity(pens)
summary(c(diss))
plot(hclust(diss))
</code></pre>

<hr>
<h2 id='cl_classes'>Cluster Classes</h2><span id='topic+cl_classes'></span>

<h3>Description</h3>

<p>Extract the classes in a partition or hierarchy.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cl_classes(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cl_classes_+3A_x">x</code></td>
<td>
<p>an R object representing a partition or hierarchy of
objects.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For partitions, the classes are the equivalence classes
(&ldquo;clusters&rdquo;) of the partition; for soft partitions, the classes
of the nearest hard partition are used.
</p>
<p>For hierarchies represented by trees, the classes are the sets of
objects corresponding to (joined at or split by) the nodes of the
tree.
</p>


<h3>Value</h3>

<p>A list inheriting from <code>"cl_classes_of_objects"</code> of vectors
indicating the classes.
</p>

<hr>
<h2 id='cl_consensus'>Consensus Partitions and Hierarchies</h2><span id='topic+cl_consensus'></span>

<h3>Description</h3>

<p>Compute the consensus clustering of an ensemble of partitions or
hierarchies.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cl_consensus(x, method = NULL, weights = 1, control = list())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cl_consensus_+3A_x">x</code></td>
<td>
<p>an ensemble of partitions or hierarchies, or something
coercible to that (see <code><a href="#topic+cl_ensemble">cl_ensemble</a></code>).</p>
</td></tr>
<tr><td><code id="cl_consensus_+3A_method">method</code></td>
<td>
<p>a character string specifying one of the built-in
methods for computing consensus clusterings, or a function to be
taken as a user-defined method, or <code>NULL</code> (default value).  If
a character string, its lower-cased version is matched against the
lower-cased names of the available built-in methods using
<code><a href="base.html#topic+pmatch">pmatch</a></code>.  See <b>Details</b> for available built-in
methods and defaults.</p>
</td></tr>
<tr><td><code id="cl_consensus_+3A_weights">weights</code></td>
<td>
<p>a numeric vector with non-negative case weights.
Recycled to the number of elements in the ensemble given by <code>x</code>
if necessary.</p>
</td></tr>
<tr><td><code id="cl_consensus_+3A_control">control</code></td>
<td>
<p>a list of control parameters.  See <b>Details</b>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Consensus clusterings &ldquo;synthesize&rdquo; the information in the
elements of a cluster ensemble into a single clustering, often by
minimizing a criterion function measuring how dissimilar consensus
candidates are from the (elements of) the ensemble (the so-called
&ldquo;optimization approach&rdquo; to consensus clustering).  
</p>
<p>The most popular criterion functions are of the form <code class="reqn">L(x) = \sum
    w_b d(x_b, x)^p</code>, where <code class="reqn">d</code> is a suitable dissimilarity measure
(see <code><a href="#topic+cl_dissimilarity">cl_dissimilarity</a></code>), <code class="reqn">w_b</code> is the case weight
given to element <code class="reqn">x_b</code> of the ensemble, and <code class="reqn">p \ge 1</code>.  If
<code class="reqn">p = 1</code> and minimization is over all possible base clusterings, a
consensus solution is called a <em>median</em> of the ensemble; if
minimization is restricted to the elements of the ensemble, a
consensus solution is called a <em>medoid</em> (see
<code><a href="#topic+cl_medoid">cl_medoid</a></code>).  For <code class="reqn">p = 2</code>, we obtain <em>least
squares</em> consensus partitions and hierarchies (generalized means).
See also Gordon (1999) for more information.
</p>
<p>If all elements of the ensemble are partitions, the built-in consensus
methods compute consensus partitions by minimizing a criterion of the
form <code class="reqn">L(x) = \sum w_b d(x_b, x)^p</code> over all hard or soft
partitions <code class="reqn">x</code> with a given (maximal) number <code class="reqn">k</code> of classes.
Available built-in methods are as follows.
</p>

<dl>
<dt><code>"SE"</code></dt><dd><p>a fixed-point algorithm for obtaining <em>soft</em>
least squares Euclidean consensus partitions (i.e., for minimizing
<code class="reqn">L</code> with Euclidean dissimilarity <code class="reqn">d</code> and <code class="reqn">p =	2</code> over
all soft partitions with a given maximal number of classes).
</p>
<p>This iterates between individually matching all partitions to the
current approximation to the consensus partition, and computing
the next approximation as the membership matrix closest to a
suitable weighted average of the memberships of all partitions
after permuting their columns for the optimal matchings of class
ids.
</p>
<p>The following control parameters are available for this method.
</p>

<dl>
<dt><code>k</code></dt><dd><p>an integer giving the number of classes to be
used for the least squares consensus partition.
By default, the maximal number of classes in the ensemble is
used.</p>
</dd>
<dt><code>maxiter</code></dt><dd><p>an integer giving the maximal number of
iterations to be performed.
Defaults to 100.</p>
</dd>
<dt><code>nruns</code></dt><dd><p>an integer giving the number of runs to be
performed.  Defaults to 1.</p>
</dd>
<dt><code>reltol</code></dt><dd><p>the relative convergence tolerance.
Defaults to <code>sqrt(.Machine$double.eps)</code>.</p>
</dd>
<dt><code>start</code></dt><dd><p>a matrix with number of rows equal to the
number of objects of the cluster ensemble, and <code class="reqn">k</code>
columns, to be used as a starting value, or a list of such
matrices.  By default, suitable random membership matrices are
used.</p>
</dd>
<dt><code>verbose</code></dt><dd><p>a logical indicating whether to provide
some output on minimization progress.
Defaults to <code>getOption("verbose")</code>.</p>
</dd>
</dl>

<p>In the case of multiple runs, the first optimum found is returned.
</p>
<p>This method can also be referred to as <code>"soft/euclidean"</code>.
</p>
</dd>
<dt><code>"GV1"</code></dt><dd><p>the fixed-point algorithm for the &ldquo;first
model&rdquo; in Gordon and Vichi (2001) for minimizing <code class="reqn">L</code> with
<code class="reqn">d</code> being GV1 dissimilarity and <code class="reqn">p = 2</code> over all soft
partitions with a given maximal number of classes.
</p>
<p>This is similar to <code>"SE"</code>, but uses GV1 rather than Euclidean
dissimilarity.
</p>
<p>Available control parameters are the same as for <code>"SE"</code>.
</p>
</dd>
<dt><code>"DWH"</code></dt><dd><p>an extension of the greedy algorithm in
Dimitriadou, Weingessel and Hornik (2002) for (approximately)
obtaining soft least squares Euclidean consensus partitions.
The reference provides some structure theory relating finding
the consensus partition to an instance of the multiple assignment
problem, which is known to be NP-hard, and suggests a simple
heuristic based on successively matching an individual partition
<code class="reqn">x_b</code> to the current approximation to the consensus partition,
and compute the memberships of the next approximation as a
weighted average of those of the current one and of <code class="reqn">x_b</code>
after permuting its columns for the optimal matching of class
ids.
</p>
<p>The following control parameters are available for this method.
</p>

<dl>
<dt><code>k</code></dt><dd><p>an integer giving the number of classes to be
used for the least squares consensus partition.  By default,
the maximal number of classes in the ensemble is used.</p>
</dd>
<dt><code>order</code></dt><dd><p>a permutation of the integers from 1 to the
size of the ensemble, specifying the order in which the
partitions in the ensemble should be aggregated.  Defaults to
using a random permutation (unlike the reference, which does
not permute at all).</p>
</dd>
</dl>

</dd>
<dt><code>"HE"</code></dt><dd><p>a fixed-point algorithm for obtaining <em>hard</em>
least squares Euclidean consensus partitions (i.e., for minimizing
<code class="reqn">L</code> with Euclidean dissimilarity <code class="reqn">d</code> and <code class="reqn">p =	2</code> over
all hard partitions with a given maximal number of classes.)
</p>
<p>Available control parameters are the same as for <code>"SE"</code>.
</p>
<p>This method can also be referred to as <code>"hard/euclidean"</code>.
</p>
</dd>
<dt><code>"SM"</code></dt><dd><p>a fixed-point algorithm for obtaining <em>soft</em>
median Manhattan consensus partitions (i.e., for minimizing
<code class="reqn">L</code> with Manhattan dissimilarity <code class="reqn">d</code> and <code class="reqn">p =	1</code> over 
all soft partitions with a given maximal number of classes).
</p>
<p>Available control parameters are the same as for <code>"SE"</code>.
</p>
<p>This method can also be referred to as <code>"soft/manhattan"</code>.
</p>
</dd>
<dt><code>"HM"</code></dt><dd><p>a fixed-point algorithm for obtaining <em>hard</em>
median Manhattan consensus partitions (i.e., for minimizing
<code class="reqn">L</code> with Manhattan dissimilarity <code class="reqn">d</code> and <code class="reqn">p =	1</code> over 
all hard partitions with a given maximal number of classes).
</p>
<p>Available control parameters are the same as for <code>"SE"</code>.
</p>
<p>This method can also be referred to as <code>"hard/manhattan"</code>.
</p>
</dd>
<dt><code>"GV3"</code></dt><dd><p>a <abbr><span class="acronym">SUMT</span></abbr> algorithm for the &ldquo;third
model&rdquo; in Gordon and Vichi (2001) for minimizing <code class="reqn">L</code> with
<code class="reqn">d</code> being co-membership dissimilarity and <code class="reqn">p = 2</code>.  (See
<code><a href="#topic+sumt">sumt</a></code> for more information on the <abbr><span class="acronym">SUMT</span></abbr>
approach.)  This optimization problem is equivalent to finding the
membership matrix <code class="reqn">m</code> for which the sum of the squared
differences between <code class="reqn">C(m) = m m'</code> and the weighted average
co-membership matrix <code class="reqn">\sum_b w_b C(m_b)</code> of the partitions is
minimal.
</p>
<p>Available control parameters are <code>method</code>, <code>control</code>,
<code>eps</code>, <code>q</code>, and <code>verbose</code>, which have the same
roles as for <code><a href="#topic+sumt">sumt</a></code>, and the following.
</p>

<dl>
<dt><code>k</code></dt><dd><p>an integer giving the number of classes to be
used for the least squares consensus partition.  By default,
the maximal number of classes in the ensemble is used.</p>
</dd>
<dt><code>nruns</code></dt><dd><p>an integer giving the number of runs to be
performed.  Defaults to 1.</p>
</dd>
<dt><code>start</code></dt><dd><p>a matrix with number of rows equal to the
size of the cluster ensemble, and <code class="reqn">k</code> columns, to be used
as a starting value, or a list of such matrices.  By default,
a membership based on a rank <code class="reqn">k</code> approximation to the
weighted average co-membership matrix is used.</p>
</dd>
</dl>

<p>In the case of multiple runs, the first optimum found is returned.
</p>
</dd>
<dt><code>"soft/symdiff"</code></dt><dd><p>a <abbr><span class="acronym">SUMT</span></abbr> approach for
minimizing <code class="reqn">L = \sum w_b d(x_b, x)</code> over all soft partitions
with a given maximal number of classes, where <code class="reqn">d</code> is the
Manhattan dissimilarity of the co-membership matrices (coinciding
with symdiff partition dissimilarity in the case of hard
partitions).
</p>
<p>Available control parameters are the same as for <code>"GV3"</code>.
</p>
</dd>
<dt><code>"hard/symdiff"</code></dt><dd><p>an exact solver for minimizing
<code class="reqn">L = \sum w_b d(x_b, x)</code> over all hard partitions (possibly
with a given maximal number of classes as specified by the control
parameter <code>k</code>), where <code class="reqn">d</code> is symdiff partition
dissimilarity (so that soft partitions in the ensemble are
replaced by their closest hard partitions), or equivalently, Rand
distance or pair-bonds (Boorman-Arabie <code class="reqn">D</code>) distance.  The
consensus solution is found via mixed linear or quadratic
programming.
</p>
</dd>
</dl>

<p>By default, method <code>"SE"</code> is used for ensembles of partitions.
</p>
<p>If all elements of the ensemble are hierarchies, the following
built-in methods for computing consensus hierarchies are available.
</p>

<dl>
<dt><code>"euclidean"</code></dt><dd><p>an algorithm for minimizing
<code class="reqn">L(x) = \sum w_b d(x_b, x) ^ 2</code> over all dendrograms, where
<code class="reqn">d</code> is Euclidean dissimilarity.  This is equivalent to finding
the best least squares ultrametric approximation of the weighted
average <code class="reqn">d = \sum w_b u_b</code> of the ultrametrics <code class="reqn">u_b</code> of
the hierarchies <code class="reqn">x_b</code>, which is attempted by calling
<code><a href="#topic+ls_fit_ultrametric">ls_fit_ultrametric</a></code> on <code class="reqn">d</code> with appropriate
control parameters.
</p>
<p>This method can also be referred to as <code>"cophenetic"</code>.
</p>
</dd>
<dt><code>"manhattan"</code></dt><dd><p>a <abbr><span class="acronym">SUMT</span></abbr> for minimizing
<code class="reqn">L = \sum w_b d(x_b, x)</code> over all dendrograms, where <code class="reqn">d</code>
is Manhattan dissimilarity.
</p>
<p>Available control parameters are the same as for
<code>"euclidean"</code>.
</p>
</dd>
<dt><code>"majority"</code></dt><dd><p>a hierarchy obtained from an extension of
the majority consensus tree of Margush and McMorris (1981), which
minimizes <code class="reqn">L(x) = \sum w_b d(x_b, x)</code> over all dendrograms,
where <code class="reqn">d</code> is the symmetric difference dissimilarity.  The
unweighted <code class="reqn">p</code>-majority tree is the <code class="reqn">n</code>-tree (hierarchy in
the strict sense) consisting of all subsets of objects contained
in more than <code class="reqn">100 p</code> percent of the <code class="reqn">n</code>-trees <code class="reqn">T_b</code>
induced by the dendrograms, where <code class="reqn">1/2 \le p &lt; 1</code> and
<code class="reqn">p = 1/2</code> (default) corresponds to the standard majority tree.
In the weighted case, it consists of all subsets <code class="reqn">A</code> for which
<code class="reqn">\sum_{b: A \in T_b} w_b &gt; W p</code>, where <code class="reqn">W = \sum_b w_b</code>.
We also allow for <code class="reqn">p = 1</code>, which gives the <em>strict
consensus tree</em> consisting of all subsets contained in each of
the <code class="reqn">n</code>-trees.  The majority dendrogram returned is a
representation of the majority tree where all splits have height
one.
</p>
<p>The fraction <code class="reqn">p</code> can be specified via the control parameter
<code>p</code>.
</p>
</dd>
</dl>

<p>By default, method <code>"euclidean"</code> is used for ensembles of
hierarchies.
</p>
<p>If a user-defined consensus method is to be employed, it must be a
function taking the cluster ensemble, the case weights, and a list of
control parameters as its arguments, with formals named <code>x</code>,
<code>weights</code>, and <code>control</code>, respectively.
</p>
<p>Most built-in methods use heuristics for solving hard optimization
problems, and cannot be guaranteed to find a global minimum.  Standard
practice would recommend to use the best solution found in
&ldquo;sufficiently many&rdquo; replications of the methods.
</p>


<h3>Value</h3>

<p>The consensus partition or hierarchy.
</p>


<h3>References</h3>

<p>E. Dimitriadou, A. Weingessel and K. Hornik (2002).
A combination scheme for fuzzy clustering.
<em>International Journal of Pattern Recognition and Artificial
Intelligence</em>, <b>16</b>, 901&ndash;912. <br />
<a href="https://doi.org/10.1142/S0218001402002052">doi:10.1142/S0218001402002052</a>.
</p>
<p>A. D. Gordon and M. Vichi (2001).
Fuzzy partition models for fitting a set of partitions.
<em>Psychometrika</em>, <b>66</b>, 229&ndash;248.
<a href="https://doi.org/10.1007/BF02294837">doi:10.1007/BF02294837</a>.
</p>
<p>A. D. Gordon (1999).
<em>Classification</em> (2nd edition).
Boca Raton, FL: Chapman &amp; Hall/CRC.
</p>
<p>T. Margush and F. R. McMorris (1981).
Consensus <code class="reqn">n</code>-trees.
<em>Bulletin of Mathematical Biology</em>, <b>43</b>, 239&ndash;244.
<a href="https://doi.org/10.1007/BF02459446">doi:10.1007/BF02459446</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cl_medoid">cl_medoid</a></code>,
<code><a href="ape.html#topic+consensus">consensus</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Consensus partition for the Rosenberg-Kim kinship terms partition
## data based on co-membership dissimilarities.
data("Kinship82")
m1 &lt;- cl_consensus(Kinship82, method = "GV3",
                   control = list(k = 3, verbose = TRUE))
## (Note that one should really use several replicates of this.)
## Value for criterion function to be minimized:
sum(cl_dissimilarity(Kinship82, m1, "comem") ^ 2)
## Compare to the consensus solution given in Gordon &amp; Vichi (2001).
data("Kinship82_Consensus")
m2 &lt;- Kinship82_Consensus[["JMF"]]
sum(cl_dissimilarity(Kinship82, m2, "comem") ^ 2)
## Seems we get a better solution ...
## How dissimilar are these solutions?
cl_dissimilarity(m1, m2, "comem")
## How "fuzzy" are they?
cl_fuzziness(cl_ensemble(m1, m2))
## Do the "nearest" hard partitions fully agree?
cl_dissimilarity(as.cl_hard_partition(m1),
                 as.cl_hard_partition(m2))

## Consensus partition for the Gordon and Vichi (2001) macroeconomic
## partition data based on Euclidean dissimilarities.
data("GVME")
set.seed(1)
## First, using k = 2 classes.
m1 &lt;- cl_consensus(GVME, method = "GV1",
                   control = list(k = 2, verbose = TRUE))
## (Note that one should really use several replicates of this.)
## Value of criterion function to be minimized:
sum(cl_dissimilarity(GVME, m1, "GV1") ^ 2)
## Compare to the consensus solution given in Gordon &amp; Vichi (2001).
data("GVME_Consensus")
m2 &lt;- GVME_Consensus[["MF1/2"]]
sum(cl_dissimilarity(GVME, m2, "GV1") ^ 2)
## Seems we get a slightly  better solution ...
## But note that
cl_dissimilarity(m1, m2, "GV1")
## and that the maximal deviation of the memberships is
max(abs(cl_membership(m1) - cl_membership(m2)))
## so the differences seem to be due to rounding.
## Do the "nearest" hard partitions fully agree?
table(cl_class_ids(m1), cl_class_ids(m2))

## And now for k = 3 classes.
m1 &lt;- cl_consensus(GVME, method = "GV1",
                   control = list(k = 3, verbose = TRUE))
sum(cl_dissimilarity(GVME, m1, "GV1") ^ 2)
## Compare to the consensus solution given in Gordon &amp; Vichi (2001).
m2 &lt;- GVME_Consensus[["MF1/3"]]
sum(cl_dissimilarity(GVME, m2, "GV1") ^ 2)
## This time we look much better ...
## How dissimilar are these solutions?
cl_dissimilarity(m1, m2, "GV1")
## Do the "nearest" hard partitions fully agree?
table(cl_class_ids(m1), cl_class_ids(m2))
</code></pre>

<hr>
<h2 id='cl_dissimilarity'>Dissimilarity Between Partitions or Hierarchies</h2><span id='topic+cl_dissimilarity'></span>

<h3>Description</h3>

<p>Compute the dissimilarity between (ensembles) of partitions
or hierarchies.</p>


<h3>Usage</h3>

<pre><code class='language-R'>cl_dissimilarity(x, y = NULL, method = "euclidean", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cl_dissimilarity_+3A_x">x</code></td>
<td>
<p>an ensemble of partitions or hierarchies and dissimilarities,
or something coercible to that (see <code><a href="#topic+cl_ensemble">cl_ensemble</a></code>).</p>
</td></tr>
<tr><td><code id="cl_dissimilarity_+3A_y">y</code></td>
<td>
<p><code>NULL</code> (default), or as for <code>x</code>.</p>
</td></tr>
<tr><td><code id="cl_dissimilarity_+3A_method">method</code></td>
<td>
<p>a character string specifying one of the built-in
methods for computing dissimilarity, or a function to be taken as
a user-defined method.  If a character string, its lower-cased
version is matched against the lower-cased names of the available
built-in methods using <code><a href="base.html#topic+pmatch">pmatch</a></code>.  See <b>Details</b> for
available built-in methods.</p>
</td></tr>
<tr><td><code id="cl_dissimilarity_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>y</code> is given, its components must be of the same kind as those
of <code>x</code> (i.e., components must either all be partitions, or all be
hierarchies or dissimilarities).
</p>
<p>If all components are partitions, the following built-in methods for
measuring dissimilarity between two partitions with respective
membership matrices <code class="reqn">u</code> and <code class="reqn">v</code> (brought to a common number of
columns) are available:
</p>

<dl>
<dt><code>"euclidean"</code></dt><dd><p>the Euclidean dissimilarity of the
memberships, i.e., the square root of the minimal sum of the
squared differences of <code class="reqn">u</code> and all column permutations of
<code class="reqn">v</code>.  See Dimitriadou, Weingessel and Hornik (2002).</p>
</dd>
<dt><code>"manhattan"</code></dt><dd><p>the Manhattan dissimilarity of the
memberships, i.e., the minimal sum of the absolute differences of
<code class="reqn">u</code> and all column permutations of <code class="reqn">v</code>.</p>
</dd>
<dt><code>"comemberships"</code></dt><dd><p>the Euclidean dissimilarity of the
elements of the co-membership matrices <code class="reqn">C(u) = u u'</code> and
<code class="reqn">C(v)</code>, i.e., the square root of the sum of the squared
differences of <code class="reqn">C(u)</code> and <code class="reqn">C(v)</code>.</p>
</dd>
<dt><code>"symdiff"</code></dt><dd><p>the cardinality of the symmetric set
difference of the sets of co-classified pairs of distinct objects
in the partitions.  I.e., the number of distinct pairs of objects
in the same class in exactly one of the partitions.
(Alternatively, the cardinality of the symmetric set difference
between the (binary) equivalence relations corresponding to the
partitions.)  For soft partitions, (currently) the symmetric set
difference of the corresponding nearest hard partitions is used.</p>
</dd>
<dt><code>"Rand"</code></dt><dd><p>the Rand distance, i.e., the rate of distinct
pairs of objects in the same class in exactly one of the
partitions.  (Related to the Rand index <code class="reqn">a</code> via the linear
transformation <code class="reqn">d = (1 - a) / 2</code>.)  For soft partitions,
(currently) the Rand distance of the corresponding nearest hard
partitions is used.</p>
</dd>
<dt><code>"GV1"</code></dt><dd><p>the square root of the dissimilarity
<code class="reqn">\Delta_1</code> used for the first model in Gordon and
Vichi (2001), i.e., the square root of the minimal sum of the
squared differences of the <em>matched</em> non-zero columns of
<code class="reqn">u</code> and <code class="reqn">v</code>.</p>
</dd>
<dt><code>"BA/<var>d</var>"</code></dt><dd><p>distance measures for hard partitions
discussed in Boorman and Arabie (1972), with <var>d</var> one of
&lsquo;<span class="samp">&#8288;A&#8288;</span>&rsquo;, &lsquo;<span class="samp">&#8288;C&#8288;</span>&rsquo;, &lsquo;<span class="samp">&#8288;D&#8288;</span>&rsquo;, or &lsquo;<span class="samp">&#8288;E&#8288;</span>&rsquo;.  For soft partitions,
the distances of the corresponding nearest hard partitions are
used.
</p>
<p><code>"BA/A"</code> is the minimum number of single element moves (move
from one class to another or a new one) needed to transform one
partition into the other.  Introduced in Rubin (1967).
</p>
<p><code>"BA/C"</code> is the minimum number of lattice moves for
transforming one partition into the other, where partitions are
said to be connected by a lattice move if one is <em>just</em> finer
than the other (i.e., there is no other partition between them) in
the partition lattice (see <code><a href="#topic+cl_meet">cl_meet</a></code>).  Equivalently,
with <code class="reqn">z</code> the join of <code>x</code> and <code>y</code> and <code class="reqn">S</code> giving
the number of classes, this can be written as <code class="reqn">S(x) + S(y) - 2
	S(z)</code>.  Attributed to David Pavy.
</p>
<p><code>"BA/D"</code> is the &ldquo;pair-bonds&rdquo; distance, which can be
defined as <code class="reqn">S(x) + S(y) - 2 S(z)</code>, with <code class="reqn">z</code> the meet of
<code>x</code> and <code>y</code> and <code class="reqn">S</code> the <em>supervaluation</em> (i.e.,
non-decreasing with respect to the partial order on the partition
lattice) function <code class="reqn">\sum_i (n_i (n_i - 1)) / (n (n - 1))</code>,
where the <code class="reqn">n_i</code> are the numbers of objects in the respective
classes of the partition (such that <code class="reqn">n_i (n_i - 1) / 2</code> are the
numbers of pair bonds in the classes), and <code class="reqn">n</code> the total
number of objects.
</p>
<p><code>"BA/E"</code> is the normalized information distance, defined as
<code class="reqn">1 - I / H</code>, where <code class="reqn">I</code> is the average mutual information
between the partitions, and <code class="reqn">H</code> is the average entropy of the
meet <code class="reqn">z</code> of the partitions.  Introduced in Rajski (1961).
</p>
<p>(Boorman and Arabie also discuss a distance measure (<code class="reqn">B</code>)
based on the minimum number of set moves needed to transform one
partition into the other, which, differently from the <code class="reqn">A</code> and
<code class="reqn">C</code> distance measures is hard to compute (Day, 1981) and
(currently) not provided.)</p>
</dd>
<dt><code>"VI"</code></dt><dd><p>Variation of Information, see Meila (2003).  If
<code>...</code> has an argument named <code>weights</code>, it is taken to
specify case weights.</p>
</dd>
<dt><code>"Mallows"</code></dt><dd><p>the Mallows-type distance by Zhou, Li and
Zha (2005), which is related to the Monge-Kantorovich mass
transfer problem, and given as the <code class="reqn">p</code>-th root of the minimal
value of the transportation problem <code class="reqn">\sum w_{jk} \sum_i
      |u_{ij} - v_{ik}| ^ p</code> with constraints <code class="reqn">w_{jk} \ge 0</code>,
<code class="reqn">\sum_j w_{jk} = \alpha_j</code>, <code class="reqn">\sum_k w_{jk} = \beta_k</code>,
where <code class="reqn">\sum_j \alpha_j = \sum_k \beta_k</code>.  The parameters
<code class="reqn">p</code>, <code class="reqn">\alpha</code> and <code class="reqn">\beta</code> all default to one (in this
case, the Mallows distance coincides with the Manhattan
dissimilarity), and can be specified via additional arguments
named <code>p</code>, <code>alpha</code>, and <code>beta</code>, respectively.</p>
</dd>
<dt><code>"CSSD"</code></dt><dd><p>the Cluster Similarity Sensitive Distance of
Zhou, Li and Zha (2005), which is given as the minimal value of
<code class="reqn">\sum_{k,l} (1 - 2 w_{kl} / (\alpha_k + \beta_l)) L_{kl}</code>,
where <code class="reqn">L_{kl} = \sum_i u_{ik} v_{il} d(p_{x;k}, p_{y;l})</code> with
<code class="reqn">p_{x;k}</code> and <code class="reqn">p_{y;l}</code> the prototype of the <code class="reqn">k</code>-th
class of <code>x</code> and the <code class="reqn">l</code>-th class of <code>y</code>,
respectively, <code class="reqn">d</code> is the distance between these, and the
<code class="reqn">w_{kl}</code> as for Mallows distance.  If prototypes are matrices,
the Euclidean distance between these is used as default.  Using
the additional argument <code>L</code>, one can give a matrix of
<code class="reqn">L_{kl}</code> values, or the function <code class="reqn">d</code>.  Parameters
<code class="reqn">\alpha</code> and <code class="reqn">\beta</code> all default to one, and can be
specified via additional arguments named <code>alpha</code> and
<code>beta</code>, respectively.</p>
</dd>
</dl>

<p>For hard partitions, both Manhattan and squared Euclidean
dissimilarity give twice the <em>transfer distance</em> (Charon et al.,
2005), which is the minimum number of objects that must be removed so
that the implied partitions (restrictions to the remaining objects)
are identical.  This is also known as the <em><code class="reqn">R</code>-metric</em> in Day
(1981), i.e., the number of augmentations and removals of single
objects needed to transform one partition into the other, and the
<em>partition-distance</em> in Gusfield (2002), and equals twice the
number of single element moves distance of Boorman and Arabie.
</p>
<p>For hard partitions, the pair-bonds (Boorman-Arabie <code class="reqn">D</code>) distance
is identical to the Rand distance, and can also be written as the
Manhattan distance between the co-membership matrices corresponding to
the partitions, or equivalently, their symdiff distance, normalized by
<code class="reqn">n (n - 1)</code>.
</p>
<p>If all components are hierarchies, available built-in methods for
measuring dissimilarity between two hierarchies with respective
ultrametrics <code class="reqn">u</code> and <code class="reqn">v</code> are as follows.
</p>

<dl>
<dt><code>"euclidean"</code></dt><dd><p>the Euclidean dissimilarity of the
ultrametrics (i.e., the square root of the sum of the squared
differences of <code class="reqn">u</code> and <code class="reqn">v</code>).</p>
</dd>
<dt><code>"manhattan"</code></dt><dd><p>the Manhattan dissimilarity of the
ultrametrics (i.e., the sum of the absolute differences of <code class="reqn">u</code>
and <code class="reqn">v</code>).</p>
</dd>
<dt><code>"cophenetic"</code></dt><dd><p><code class="reqn">1 - c^2</code>, where <code class="reqn">c</code> is the
cophenetic correlation coefficient (i.e., the product-moment
correlation of the ultrametrics).</p>
</dd>
<dt><code>"gamma"</code></dt><dd><p>the rate of inversions between the
ultrametrics (i.e., the rate of pairs <code class="reqn">(i,j)</code> and <code class="reqn">(k,l)</code>
for which <code class="reqn">u_{ij} &lt; u_{kl}</code> and <code class="reqn">v_{ij} &gt; v_{kl}</code>).</p>
</dd>
<dt><code>"symdiff"</code></dt><dd><p>the cardinality of the symmetric set
difference of the sets of classes (hierarchies in the strict
sense) induced by the dendrograms.  I.e., the number of sets of
objects obtained by a split in exactly one of the hierarchies.</p>
</dd>
<dt><code>"Chebyshev"</code></dt><dd><p>the Chebyshev (maximal) dissimilarity of
the ultrametrics (i.e., the maximum of the absolute differences of
<code class="reqn">u</code> and <code class="reqn">v</code>).</p>
</dd>
<dt><code>"Lyapunov"</code></dt><dd><p>the logarithm of the product of the
maximal and minimal ratios of the ultrametrics.  This is also
known as the &ldquo;Hilbert projective metric&rdquo; on the cone
represented by the ultrametrics (e.g., Jardine &amp; Sibson (1971),
page 107), and only defined for <em>strict</em> ultrametrics (which
are strictly positive for distinct objects).</p>
</dd>
<dt><code>"BO"</code></dt><dd><p>the <code class="reqn">m_\delta</code> family of tree metrics by
Boorman and Olivier (1973), which are of the form <code class="reqn">m_\delta =
	\int_0^\infty \delta(p(h), q(h)) dh</code>, where <code class="reqn">p(h)</code> and
<code class="reqn">q(h)</code> are the hard partitions obtaining by cutting the trees
(dendrograms) at height <code class="reqn">h</code>, and <code class="reqn">\delta</code> is a suitably
dissimilarity measure for partitions.  In particular, when taking
<code class="reqn">\delta</code> as symdiff or Rand dissimilarity, <code class="reqn">m_\delta</code> is
the Manhattan dissimilarity of the hierarchies.
</p>
<p>If <code>...</code> has an argument named <code>delta</code> it is taken to
specify the partition dissimilarity <code class="reqn">\delta</code> to be employed.</p>
</dd>
<dt><code>"spectral"</code></dt><dd><p>the spectral norm (2-norm) of the
differences of the ultrametrics, suggested in Mérigot, Durbec, and
Gaertner (2010).</p>
</dd>
</dl>

<p>The measures based on ultrametrics also allow computing dissimilarity
with &ldquo;raw&rdquo; dissimilarities on the underlying objects (R objects
inheriting from class <code>"dist"</code>).
</p>
<p>If a user-defined dissimilarity method is to be employed, it must be a
function taking two clusterings as its arguments.
</p>
<p>Symmetric dissimilarity objects of class <code>"cl_dissimilarity"</code> are
implemented as symmetric proximity objects with self-proximities
identical to zero, and inherit from class <code>"cl_proximity"</code>.  They
can be coerced to dense square matrices using <code>as.matrix</code>.  It
is possible to use 2-index matrix-style subscripting for such objects;
unless this uses identical row and column indices, this results in a
(non-symmetric dissimilarity) object of class
<code>"cl_cross_dissimilarity"</code>.
</p>
<p>Symmetric dissimilarity objects also inherit from class
<code>"<a href="stats.html#topic+dist">dist</a>"</code> (although they currently do not &ldquo;strictly&rdquo;
extend this class), thus making it possible to use them directly for
clustering algorithms based on dissimilarity matrices of this class,
see the examples.
</p>


<h3>Value</h3>

<p>If <code>y</code> is <code>NULL</code>, an object of class
<code>"cl_dissimilarity"</code> containing the dissimilarities between all
pairs of components of <code>x</code>.  Otherwise, an object of class
<code>"cl_cross_dissimilarity"</code> with the dissimilarities between the
components of <code>x</code> and the components of <code>y</code>.
</p>


<h3>References</h3>

<p>S. A. Boorman and P. Arabie (1972).
Structural measures and the method of sorting.
In R. N. Shepard, A. K. Romney, &amp; S. B. Nerlove (eds.),
<em>Multidimensional Scaling: Theory and Applications in the
Behavioral Sciences, 1: Theory</em> (pages 225&ndash;249).
New York: Seminar Press.
</p>
<p>S. A. Boorman and D. C. Olivier (1973).
Metrics on spaces of finite trees.
<em>Journal of Mathematical Psychology</em>, <b>10</b>, 26&ndash;59.
<a href="https://doi.org/10.1016/0022-2496%2873%2990003-5">doi:10.1016/0022-2496(73)90003-5</a>.
</p>
<p>I. Charon, L. Denoeud, A. Guénoche and O. Hudry (2006).
<em>Maximum Transfer Distance Between Partitions</em>.
<em>Journal of Classification</em>, <b>23</b>, 103&ndash;121.
<a href="https://doi.org/10.1007/s00357-006-0006-2">doi:10.1007/s00357-006-0006-2</a>.
</p>
<p>W. E. H. Day (1981).
The complexity of computing metric distances between partitions.
<em>Mathematical Social Sciences</em>, <b>1</b>, 269&ndash;287.
<a href="https://doi.org/10.1016/0165-4896%2881%2990042-1">doi:10.1016/0165-4896(81)90042-1</a>.
</p>
<p>E. Dimitriadou, A. Weingessel and K. Hornik (2002).
A combination scheme for fuzzy clustering.
<em>International Journal of Pattern Recognition and Artificial
Intelligence</em>, <b>16</b>, 901&ndash;912. <br />
<a href="https://doi.org/10.1142/S0218001402002052">doi:10.1142/S0218001402002052</a>.
</p>
<p>A. D. Gordon and M. Vichi (2001).
Fuzzy partition models for fitting a set of partitions.
<em>Psychometrika</em>, <b>66</b>, 229&ndash;248.
<a href="https://doi.org/10.1007/BF02294837">doi:10.1007/BF02294837</a>.
</p>
<p>D. Gusfield (2002).
Partition-distance: A problem and class of perfect graphs arising in
clustering.
<em>Information Processing Letters</em>, <b>82</b>, 159&ndash;164.
<a href="https://doi.org/10.1016/S0020-0190%2801%2900263-0">doi:10.1016/S0020-0190(01)00263-0</a>.
</p>
<p>N. Jardine and E. Sibson (1971).
<em>Mathematical Taxonomy</em>.
London: Wiley.
</p>
<p>M. Meila (2003).
Comparing clusterings by the variation of information.
In B. Schölkopf and M. K. Warmuth (eds.), <em>Learning Theory and
Kernel Machines</em>, pages 173&ndash;187.
Springer-Verlag: Lecture Notes in Computer Science 2777.
</p>
<p>B. Mérigot, J.-P. Durbec and J.-C. Gaertner (2010).
On goodness-of-fit measure for dendrogram-based analyses.
<em>Ecology</em>, <b>91</b>, 1850—-1859.
<a href="https://doi.org/10.1890/09-1387.1">doi:10.1890/09-1387.1</a>.
</p>
<p>C. Rajski (1961).
A metric space of discrete probability distributions,
<em>Information and Control</em>, <b>4</b>, 371&ndash;377.
<a href="https://doi.org/10.1016/S0019-9958%2861%2980055-7">doi:10.1016/S0019-9958(61)80055-7</a>.
</p>
<p>J. Rubin (1967).
Optimal classification into groups: An approach for solving the
taxonomy problem.
<em>Journal of Theoretical Biology</em>, <b>15</b>, 103&ndash;144.
<a href="https://doi.org/10.1016/0022-5193%2867%2990046-X">doi:10.1016/0022-5193(67)90046-X</a>.
</p>
<p>D. Zhou, J. Li and H. Zha (2005).
A new Mallows distance based metric for comparing clusterings.
In <em>Proceedings of the 22nd international Conference on Machine
Learning</em> (Bonn, Germany, August 07&ndash;11, 2005), pages 1028&ndash;1035.
ICML '05, volume 119.
ACM Press, New York, NY.
<a href="https://doi.org/10.1145/1102351.1102481">doi:10.1145/1102351.1102481</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cl_agreement">cl_agreement</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## An ensemble of partitions.
data("CKME")
pens &lt;- CKME[1 : 30]
diss &lt;- cl_dissimilarity(pens)
summary(c(diss))
cl_dissimilarity(pens[1:5], pens[6:7])
## Equivalently, using subscripting.
diss[1:5, 6:7]
## Can use the dissimilarities for "secondary" clustering
## (e.g. obtaining hierarchies of partitions):
hc &lt;- hclust(diss)
plot(hc)

## Example from Boorman and Arabie (1972).
P1 &lt;- as.cl_partition(c(1, 2, 2, 2, 3, 3, 2, 2))
P2 &lt;- as.cl_partition(c(1, 1, 2, 2, 3, 3, 4, 4))
cl_dissimilarity(P1, P2, "BA/A")
cl_dissimilarity(P1, P2, "BA/C")

## Hierarchical clustering.
d &lt;- dist(USArrests)
x &lt;- hclust(d)
cl_dissimilarity(x, d, "cophenetic")
cl_dissimilarity(x, d, "gamma")
</code></pre>

<hr>
<h2 id='cl_ensemble'>Cluster Ensembles</h2><span id='topic+cl_ensemble'></span><span id='topic+as.cl_ensemble'></span><span id='topic+is.cl_ensemble'></span>

<h3>Description</h3>

<p>Creation and manipulation of cluster ensembles.</p>


<h3>Usage</h3>

<pre><code class='language-R'>cl_ensemble(..., list = NULL)
as.cl_ensemble(x)
is.cl_ensemble(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cl_ensemble_+3A_...">...</code></td>
<td>
<p>R objects representing clusterings of or dissimilarities
between the same objects.</p>
</td></tr>
<tr><td><code id="cl_ensemble_+3A_list">list</code></td>
<td>
<p>a list of R objects as in <code>...</code>.</p>
</td></tr>
<tr><td><code id="cl_ensemble_+3A_x">x</code></td>
<td>
<p>for <code>as.cl_ensemble</code>, an R object as in <code>...</code>;
for <code>is.cl_ensemble</code>, an arbitrary R object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>cl_ensemble</code> creates &ldquo;cluster ensembles&rdquo;, which are
realized as lists of clusterings (or dissimilarities) with additional
class information, always inheriting from <code>"cl_ensemble"</code>.  All
elements of the ensemble must have the same number of objects.
</p>
<p>If all elements are partitions, the ensemble has class
<code>"cl_partition_ensemble"</code>;
if all elements are dendrograms, it has class
<code>"cl_dendrogram_ensemble"</code> and inherits from
<code>"cl_hierarchy_ensemble"</code>;
if all elements are hierarchies (but not always dendrograms), it has
class <code>"cl_hierarchy_ensemble"</code>.
Note that empty or &ldquo;mixed&rdquo; ensembles cannot be categorized
according to the kind of elements they contain, and hence only have
class <code>"cl_ensemble"</code>.
</p>
<p>The list representation makes it possible to use <code>lapply</code> for
computations on the individual clusterings in (i.e., the components
of) a cluster ensemble.
</p>
<p>Available methods for cluster ensembles include those for
subscripting, <code>c</code>, <code>rep</code>, and <code>print</code>.  There is also a
<code>plot</code> method for ensembles for which all elements can be plotted
(currently, additive trees, dendrograms and ultrametrics).
</p>


<h3>Value</h3>

<p><code>cl_ensemble</code> returns a list of the given clusterings or
dissimilarities, with additional class information (see
<b>Details</b>).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d &lt;- dist(USArrests)
hclust_methods &lt;-
    c("ward", "single", "complete", "average", "mcquitty")
hclust_results &lt;- lapply(hclust_methods, function(m) hclust(d, m))
names(hclust_results) &lt;- hclust_methods 
## Now create an ensemble from the results.
hens &lt;- cl_ensemble(list = hclust_results)
hens
## Subscripting.
hens[1 : 3]
## Replication.
rep(hens, 3)
## Plotting.
plot(hens, main = names(hens))
## And continue to analyze the ensemble, e.g.
round(cl_dissimilarity(hens, method = "gamma"), 4)
</code></pre>

<hr>
<h2 id='cl_fuzziness'>Partition Fuzziness</h2><span id='topic+cl_fuzziness'></span>

<h3>Description</h3>

<p>Compute the fuzziness of partitions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cl_fuzziness(x, method = NULL, normalize = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cl_fuzziness_+3A_x">x</code></td>
<td>
<p>a cluster ensemble of partitions, or an R object coercible to
such.</p>
</td></tr>
<tr><td><code id="cl_fuzziness_+3A_method">method</code></td>
<td>
<p>a character string indicating the fuzziness measure to
be employed, or <code>NULL</code> (default), or a function to be taken as
a user-defined method.  Currently available built-in methods are
<code>"PC"</code> (Partition Coefficient) and <code>"PE"</code> (Partition
Entropy), with the default corresponding to the first one.  If
<code>method</code> is a character string, its lower-cased version is
matched against the lower-cased names of the available built-in
methods using <code><a href="base.html#topic+pmatch">pmatch</a></code>.</p>
</td></tr>
<tr><td><code id="cl_fuzziness_+3A_normalize">normalize</code></td>
<td>
<p>a logical indicating whether the fuzziness measure
should be normalized in a way that hard partitions have value 0, and
&ldquo;completely fuzzy&rdquo; partitions (where for all objects, all
classes get the same membership) have value 1.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code class="reqn">m</code> contains the membership values of a partition, the
(unnormalized) Partition Coefficient and Partition Entropy are given
by <code class="reqn">\sum_{n,i} m_{n,i}^2</code> and <code class="reqn">\sum_{n,i} H(m_{n,i})</code>,
respectively, where <code class="reqn">H(u) = u \log u - (1-u) \log(1-u)</code>.
</p>
<p>Note that the normalization used here is different from the
normalizations typically found in the literature.
</p>
<p>If a user-defined fuzziness method is to be employed, is must be a
function taking a matrix of membership values and a logical to
indicate whether normalization is to be performed as its arguments (in
that order; argument names are not used).
</p>


<h3>Value</h3>

<p>An object of class <code>"cl_fuzziness"</code> giving the fuzziness
values.
</p>


<h3>References</h3>

<p>J. C. Bezdek (1981).
<em>Pattern Recognition with Fuzzy Objective Function Algorithms</em>.
New York: Plenum.
</p>


<h3>See Also</h3>

<p>Function <code><a href="e1071.html#topic+fclustIndex">fclustIndex</a></code> in package <span class="pkg">e1071</span>,
which also computes several other &ldquo;fuzzy cluster indexes&rdquo;
(typically based on more information than just the membership
values).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if(require("e1071", quietly = TRUE)) {
    ## Use an on-line version of fuzzy c-means from package e1071 if
    ## available.
    data("Cassini")
    pens &lt;- cl_boot(Cassini$x, B = 15, k = 3, algorithm = "cmeans",
                    parameters = list(method = "ufcl"))
    pens
    summary(cl_fuzziness(pens, "PC"))
    summary(cl_fuzziness(pens, "PE"))
}
</code></pre>

<hr>
<h2 id='cl_margin'>Membership Margins</h2><span id='topic+cl_margin'></span>

<h3>Description</h3>

<p>Compute the <em>margin</em> of the memberships of a partition, i.e., the
difference between the largest and second largest membership values of
the respective objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cl_margin(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cl_margin_+3A_x">x</code></td>
<td>
<p>an <span class="rlang"><b>R</b></span> object representing a partition of objects.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For hard partitions, the margins are always 1.
</p>
<p>For soft partitions, the margins may be taken as an indication of the
&ldquo;sureness&rdquo; of classifying an object to the class with maximum
membership value.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("GVME")
## Look at the classes obtained for 1980:
split(cl_object_names(GVME[["1980"]]), cl_class_ids(GVME[["1980"]]))
## Margins:
x &lt;- cl_margin(GVME[["1980"]])
## Add names, and sort:
names(x) &lt;- cl_object_names(GVME[["1980"]])
sort(x)
## Note the "uncertainty" of assigning Egypt to the "intermediate" class
## of nations.
</code></pre>

<hr>
<h2 id='cl_medoid'>Medoid Partitions and Hierarchies</h2><span id='topic+cl_medoid'></span>

<h3>Description</h3>

<p>Compute the medoid of an ensemble of partitions or hierarchies, i.e.,
the element of the ensemble minimizing the sum of dissimilarities to
all other elements.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cl_medoid(x, method = "euclidean")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cl_medoid_+3A_x">x</code></td>
<td>
<p>an ensemble of partitions or hierarchies, or something
coercible to that (see <code><a href="#topic+cl_ensemble">cl_ensemble</a></code>).</p>
</td></tr>
<tr><td><code id="cl_medoid_+3A_method">method</code></td>
<td>
<p>a character string or a function, as for argument
<code>method</code> of function <code><a href="#topic+cl_dissimilarity">cl_dissimilarity</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Medoid clusterings are special cases of &ldquo;consensus&rdquo; clusterings
characterized as the solutions of an optimization problem.  See Gordon
(2001) for more information.
</p>
<p>The dissimilarities <code>d</code> for determining the medoid are obtained
by calling <code>cl_dissimilarity</code> with arguments <code>x</code> and
<code>method</code>.  The medoid can then be found as the (first) row index
for which the row sum of <code>as.matrix(d)</code> is minimal.  Modulo
possible differences in the case of ties, this gives the same results
as (the medoid obtained by) <code><a href="cluster.html#topic+pam">pam</a></code> in package
<span class="pkg">cluster</span>.
</p>


<h3>Value</h3>

<p>The medoid partition or hierarchy.
</p>


<h3>References</h3>

<p>A. D. Gordon (1999).
<em>Classification</em> (2nd edition).
Boca Raton, FL: Chapman &amp; Hall/CRC.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cl_consensus">cl_consensus</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## An ensemble of partitions.
data("CKME")
pens &lt;- CKME[1 : 20]
m1 &lt;- cl_medoid(pens)
diss &lt;- cl_dissimilarity(pens)
require("cluster")
m2 &lt;- pens[[pam(diss, 1)$medoids]]
## Agreement of medoid consensus partitions.
cl_agreement(m1, m2)
## Or, more straightforwardly:
table(cl_class_ids(m1), cl_class_ids(m2))
</code></pre>

<hr>
<h2 id='cl_membership'>Memberships of Partitions</h2><span id='topic+cl_membership'></span><span id='topic+as.cl_membership'></span>

<h3>Description</h3>

<p>Compute the memberships values for objects representing partitions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cl_membership(x, k = n_of_classes(x))
as.cl_membership(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cl_membership_+3A_x">x</code></td>
<td>
<p>an R object representing a partition of objects (for
<code>cl_membership</code>) or raw memberships or class ids (for
<code>as.cl_membership</code>).</p>
</td></tr>
<tr><td><code id="cl_membership_+3A_k">k</code></td>
<td>
<p>an integer giving the number of columns (corresponding to
class ids) to be used in the membership matrix.  Must not be less,
and default to, the number of classes in the partition.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>cl_membership</code> is a generic function.
</p>
<p>The methods provided in package <span class="pkg">clue</span> handle the partitions
obtained from clustering functions in the base R distribution, as well
as packages <span class="pkg">RWeka</span>, <span class="pkg">cba</span>, <span class="pkg">cclust</span>, <span class="pkg">cluster</span>,
<span class="pkg">e1071</span>, <span class="pkg">flexclust</span>, <span class="pkg">flexmix</span>, <span class="pkg">kernlab</span>,
<span class="pkg">mclust</span>, <span class="pkg">movMF</span> and <span class="pkg">skmeans</span> (and of course, <span class="pkg">clue</span>
itself).
</p>
<p><code>as.cl_membership</code> can be used for coercing &ldquo;raw&rdquo; class
ids (given as atomic vectors) or membership values (given as numeric
matrices) to membership objects.
</p>


<h3>Value</h3>

<p>An object of class <code>"cl_membership"</code> with the matrix of
membership values.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+is.cl_partition">is.cl_partition</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Getting the memberships of a single soft partition.
d &lt;- dist(USArrests)
hclust_methods &lt;-
    c("ward", "single", "complete", "average", "mcquitty")
hclust_results &lt;- lapply(hclust_methods, function(m) hclust(d, m))
names(hclust_results) &lt;- hclust_methods 
## Now create an ensemble from the results.
hens &lt;- cl_ensemble(list = hclust_results)
## And add the results of agnes and diana.
require("cluster")
hens &lt;- c(hens, list(agnes = agnes(d), diana = diana(d)))
## Create a dissimilarity object from this.
d1 &lt;- cl_dissimilarity(hens)
## And compute a soft partition.
party &lt;- fanny(d1, 2)
round(cl_membership(party), 5)
## The "nearest" hard partition to this:
as.cl_hard_partition(party)
## (which has the same class ids as cl_class_ids(party)).

## Extracting the memberships from the elements of an ensemble of
## partitions.
pens &lt;- cl_boot(USArrests, 30, 3)
pens
mems &lt;- lapply(pens, cl_membership)
## And turning these raw memberships into an ensemble of partitions.
pens &lt;- cl_ensemble(list = lapply(mems, as.cl_partition))
pens
pens[[length(pens)]]
</code></pre>

<hr>
<h2 id='cl_object_names'>Find Object Names</h2><span id='topic+cl_object_names'></span>

<h3>Description</h3>

<p>Find the names of the objects from which a taxonomy (partition or
hierarchy) or proximity was obtained.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cl_object_names(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cl_object_names_+3A_x">x</code></td>
<td>
<p>an <span class="rlang"><b>R</b></span> object representing a taxonomy or proximity.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a generic function.
</p>
<p>The methods provided in package <span class="pkg">clue</span> handle the partitions and
hierarchies obtained from clustering functions in the base R
distribution, as well as packages <span class="pkg">RWeka</span>, <span class="pkg">ape</span>, <span class="pkg">cba</span>,
<span class="pkg">cclust</span>, <span class="pkg">cluster</span>, <span class="pkg">e1071</span>, <span class="pkg">flexclust</span>,
<span class="pkg">flexmix</span>, <span class="pkg">kernlab</span>, <span class="pkg">mclust</span>, <span class="pkg">movMF</span> and
<span class="pkg">skmeans</span> (and of course, <span class="pkg">clue</span> itself), in as much as
possible.
</p>
<p>There is also a method for object dissimilarities which inherit from
class <code>"<a href="stats.html#topic+dist">dist</a>"</code>.
</p>


<h3>Value</h3>

<p>A character vector of length <code><a href="#topic+n_of_objects">n_of_objects</a>(x)</code> in case the
names of the objects could be determined, or <code>NULL</code>.
</p>

<hr>
<h2 id='cl_pam'>K-Medoids Partitions of Clusterings</h2><span id='topic+cl_pam'></span>

<h3>Description</h3>

<p>Compute <code class="reqn">k</code>-medoids partitions of clusterings.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cl_pam(x, k, method = "euclidean", solver = c("pam", "kmedoids"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cl_pam_+3A_x">x</code></td>
<td>
<p>an ensemble of partitions or hierarchies, or something
coercible to that (see <code><a href="#topic+cl_ensemble">cl_ensemble</a></code>).</p>
</td></tr>
<tr><td><code id="cl_pam_+3A_k">k</code></td>
<td>
<p>an integer giving the number of classes to be used in the
partition.</p>
</td></tr>
<tr><td><code id="cl_pam_+3A_method">method</code></td>
<td>
<p>a character string or a function, as for argument
<code>method</code> of function <code><a href="#topic+cl_dissimilarity">cl_dissimilarity</a></code>.</p>
</td></tr>
<tr><td><code id="cl_pam_+3A_solver">solver</code></td>
<td>
<p>a character string indicating the <code class="reqn">k</code>-medoids solver
to be employed.  May be abbreviated.  If <code>"pam"</code> (default), the
Partitioning Around Medoids (Kaufman &amp; Rousseeuw (1990), Chapter 2)
heuristic <code><a href="cluster.html#topic+pam">pam</a></code> of package <span class="pkg">cluster</span> is
used.  Otherwise, the exact algorithm of <code><a href="#topic+kmedoids">kmedoids</a></code> is
employed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An optimal <code class="reqn">k</code>-medoids partition of the given cluster ensemble is
defined as a partition of the objects <code class="reqn">x_i</code> (the elements of the
ensemble) into <code class="reqn">k</code> classes <code class="reqn">C_1, \ldots, C_k</code> such that the
criterion function
<code class="reqn">L = \sum_{l=1}^k \min_{j \in C_l} \sum_{i \in C_l} d(x_i, x_j)</code> 
is minimized.
</p>
<p>Such secondary partitions (e.g., Gordon &amp; Vichi, 1998) are obtained by
computing the dissimilarities <code class="reqn">d</code> of the objects in the ensemble
for the given dissimilarity method, and applying a dissimilarity-based
<code class="reqn">k</code>-medoids solver to <code class="reqn">d</code>.
</p>


<h3>Value</h3>

<p>An object of class <code>"cl_pam"</code> representing the obtained
&ldquo;secondary&rdquo; partition, which is a list with the following
components.
</p>
<table role = "presentation">
<tr><td><code>cluster</code></td>
<td>
<p>the class ids of the partition.</p>
</td></tr>
<tr><td><code>medoid_ids</code></td>
<td>
<p>the indices of the medoids.</p>
</td></tr>
<tr><td><code>prototypes</code></td>
<td>
<p>a cluster ensemble with the <code class="reqn">k</code> prototypes
(medoids).</p>
</td></tr>
<tr><td><code>criterion</code></td>
<td>
<p>the value of the criterion function of the
partition.</p>
</td></tr>
<tr><td><code>description</code></td>
<td>
<p>a character string indicating the dissimilarity
method employed.</p>
</td></tr>
</table>


<h3>References</h3>

<p>L. Kaufman and P. J. Rousseeuw (1990).
<em>Finding Groups in Data: An Introduction to Cluster Analysis</em>.
Wiley, New York.
</p>
<p>A. D. Gordon and M. Vichi (1998).
Partitions of partitions.
<em>Journal of Classification</em>, <b>15</b>, 265&ndash;285.
<a href="https://doi.org/10.1007/s003579900034">doi:10.1007/s003579900034</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cl_pclust">cl_pclust</a></code> for more general prototype-based partitions of
clusterings.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Kinship82")
party &lt;- cl_pam(Kinship82, 3, "symdiff")
## Compare results with tables 5 and 6 in Gordon &amp; Vichi (1998).
party
lapply(cl_prototypes(party), cl_classes)
table(cl_class_ids(party))
</code></pre>

<hr>
<h2 id='cl_pclust'>Prototype-Based Partitions of Clusterings</h2><span id='topic+cl_pclust'></span>

<h3>Description</h3>

<p>Compute prototype-based partitions of a cluster ensemble by minimizing
<code class="reqn">\sum w_b u_{bj}^m d(x_b, p_j)^e</code>, the sum of the case-weighted and
membership-weighted <code class="reqn">e</code>-th powers of the dissimilarities between
the elements <code class="reqn">x_b</code> of the ensemble and the prototypes <code class="reqn">p_j</code>,
for suitable dissimilarities <code class="reqn">d</code> and exponents <code class="reqn">e</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cl_pclust(x, k, method = NULL, m = 1, weights = 1,
          control = list())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cl_pclust_+3A_x">x</code></td>
<td>
<p>an ensemble of partitions or hierarchies, or something
coercible to that (see <code><a href="#topic+cl_ensemble">cl_ensemble</a></code>).</p>
</td></tr>
<tr><td><code id="cl_pclust_+3A_k">k</code></td>
<td>
<p>an integer giving the number of classes to be used in the
partition.</p>
</td></tr>
<tr><td><code id="cl_pclust_+3A_method">method</code></td>
<td>
<p>the consensus method to be employed, see
<code><a href="#topic+cl_consensus">cl_consensus</a></code>.</p>
</td></tr>
<tr><td><code id="cl_pclust_+3A_m">m</code></td>
<td>
<p>a number not less than 1 controlling the softness of the
partition (as the &ldquo;fuzzification parameter&rdquo; of the fuzzy
<code class="reqn">c</code>-means algorithm).  The default value of 1 corresponds to
hard partitions obtained from a generalized <code class="reqn">k</code>-means problem;
values greater than one give partitions of increasing softness
obtained from a generalized fuzzy <code class="reqn">c</code>-means problem.</p>
</td></tr>
<tr><td><code id="cl_pclust_+3A_weights">weights</code></td>
<td>
<p>a numeric vector of non-negative case weights.
Recycled to the number of elements in the ensemble given by <code>x</code>
if necessary.</p>
</td></tr>
<tr><td><code id="cl_pclust_+3A_control">control</code></td>
<td>
<p>a list of control parameters.  See <b>Details</b>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Partitioning is performed using <code><a href="#topic+pclust">pclust</a></code> via a family
constructed from <code>method</code>.  The dissimilarities <code class="reqn">d</code> and
exponent <code class="reqn">e</code> are implied by the consensus method employed, and
inferred via a registration mechanism currently only made available to
built-in consensus methods.  The default methods compute Least Squares
Euclidean consensus clusterings, i.e., use Euclidean dissimilarity
<code class="reqn">d</code> and <code class="reqn">e = 2</code>.
</p>
<p>For <code class="reqn">m = 1</code>, the partitioning procedure was introduced by Gaul and
Schader (1988) for &ldquo;Clusterwise Aggregation of Relations&rdquo; (with
the same domains), containing equivalence relations, i.e., hard
partitions, as a special case.
</p>
<p>Available control parameters are as for <code><a href="#topic+pclust">pclust</a></code>.
</p>
<p>The fixed point approach employed is a heuristic which cannot be
guaranteed to find the global minimum (as this is already true for the
computation of consensus clusterings).  Standard practice would
recommend to use the best solution found in &ldquo;sufficiently many&rdquo;
replications of the base algorithm.
</p>


<h3>Value</h3>

<p>An object of class <code>"cl_partition"</code> representing the obtained 
&ldquo;secondary&rdquo; partition by an object of class <code>"cl_pclust"</code>,
which is a list containing at least the following components.
</p>
<table role = "presentation">
<tr><td><code>prototypes</code></td>
<td>
<p>a cluster ensemble with the <code class="reqn">k</code> prototypes.</p>
</td></tr>
<tr><td><code>membership</code></td>
<td>
<p>an object of class <code>"<a href="#topic+cl_membership">cl_membership</a>"</code>
with the membership values <code class="reqn">u_{bj}</code>.</p>
</td></tr>
<tr><td><code>cluster</code></td>
<td>
<p>the class ids of the nearest hard partition.</p>
</td></tr>
<tr><td><code>silhouette</code></td>
<td>
<p>Silhouette information for the partition, see
<code><a href="cluster.html#topic+silhouette">silhouette</a></code>.</p>
</td></tr> 
<tr><td><code>validity</code></td>
<td>
<p>precomputed validity measures for the partition.</p>
</td></tr>
<tr><td><code>m</code></td>
<td>
<p>the softness control argument.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>d</code></td>
<td>
<p>the dissimilarity function <code class="reqn">d = d(x, p)</code> employed.</p>
</td></tr>
<tr><td><code>e</code></td>
<td>
<p>the exponent <code class="reqn">e</code> employed.</p>
</td></tr>
</table>


<h3>References</h3>

<p>J. C. Bezdek (1981).
<em>Pattern recognition with fuzzy objective function algorithms</em>.
New York: Plenum.
</p>
<p>W. Gaul and M. Schader (1988).
Clusterwise aggregation of relations.
<em>Applied Stochastic Models and Data Analysis</em>, <b>4</b>:273&ndash;282.
<a href="https://doi.org/10.1002/asm.3150040406">doi:10.1002/asm.3150040406</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Use a precomputed ensemble of 50 k-means partitions of the
## Cassini data.
data("CKME")
CKME &lt;- CKME[1 : 30]		# for saving precious time ...
diss &lt;- cl_dissimilarity(CKME)
hc &lt;- hclust(diss)
plot(hc)
## This suggests using a partition with three classes, which can be
## obtained using cutree(hc, 3).  Could use cl_consensus() to compute
## prototypes as the least squares consensus clusterings of the classes,
## or alternatively:
set.seed(123)
x1 &lt;- cl_pclust(CKME, 3, m = 1)
x2 &lt;- cl_pclust(CKME, 3, m = 2)
## Agreement of solutions.
cl_dissimilarity(x1, x2)
table(cl_class_ids(x1), cl_class_ids(x2))
</code></pre>

<hr>
<h2 id='cl_predict'>Predict Memberships</h2><span id='topic+cl_predict'></span>

<h3>Description</h3>

<p>Predict class ids or memberships from R objects representing
partitions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cl_predict(object, newdata = NULL,
           type = c("class_ids", "memberships"), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cl_predict_+3A_object">object</code></td>
<td>
<p>an R object representing a partition of objects.</p>
</td></tr>
<tr><td><code id="cl_predict_+3A_newdata">newdata</code></td>
<td>
<p>an optional data set giving the objects to make
predictions for.  This must be of the same &ldquo;kind&rdquo; as the data
set employed for obtaining the partition.  If omitted, the original
data are used.</p>
</td></tr>
<tr><td><code id="cl_predict_+3A_type">type</code></td>
<td>
<p>a character string indicating whether class ids or
memberships should be returned. May be abbreviated.</p>
</td></tr>
<tr><td><code id="cl_predict_+3A_...">...</code></td>
<td>
<p>arguments to be passed to and from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Many algorithms resulting in partitions of a given set of objects can
be taken to induce a partition of the underlying feature space for the
measurements on the objects, so that class memberships for
&ldquo;new&rdquo; objects can be obtained from the induced partition.
Examples include partitions based on assigning objects to their
&ldquo;closest&rdquo; prototypes, or providing mixture models for the
distribution of objects in feature space.
</p>
<p>This is a generic function.  The methods provided in package
<span class="pkg">clue</span> handle the partitions obtained from clustering functions in
the base R distribution, as well as packages <span class="pkg">RWeka</span>, <span class="pkg">cba</span>,
<span class="pkg">cclust</span>, <span class="pkg">cluster</span>, <span class="pkg">e1071</span>, <span class="pkg">flexclust</span>,
<span class="pkg">flexmix</span>, <span class="pkg">kernlab</span>, <span class="pkg">mclust</span>, <span class="pkg">movMF</span> and
<span class="pkg">skmeans</span> (and of course, <span class="pkg">clue</span> itself).
</p>


<h3>Value</h3>

<p>Depending on <code>type</code>, an object of class <code>"cl_class_ids"</code>
with the predicted class ids, or of class <code>"cl_membership"</code> with
the matrix of predicted membership values.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Run kmeans on a random subset of the Cassini data, and predict the
## memberships for the "test" data set.
data("Cassini")
nr &lt;- NROW(Cassini$x)
ind &lt;- sample(nr, 0.9 * nr, replace = FALSE)
party &lt;- kmeans(Cassini$x[ind, ], 3)
table(cl_predict(party, Cassini$x[-ind, ]),
      Cassini$classes[-ind])
</code></pre>

<hr>
<h2 id='cl_prototypes'>Partition Prototypes</h2><span id='topic+cl_prototypes'></span>

<h3>Description</h3>

<p>Determine prototypes for the classes of an R object representing a
partition.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cl_prototypes(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cl_prototypes_+3A_x">x</code></td>
<td>
<p>an R object representing a partition of objects.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Many partitioning methods are based on prototypes (&ldquo;centers&rdquo;,
&ldquo;centroids&rdquo;, &ldquo;medoids&rdquo;, ...).  In typical cases, these
are points in the feature space for the measurements on the objects to
be partitioned, such that one can quantify the distance between the
objects and the prototypes, and, e.g., classify objects to their
closest prototype.
</p>
<p>This is a generic function.  The methods provided in package
<span class="pkg">clue</span> handle the partitions obtained from clustering functions in
the base R distribution, as well as packages <span class="pkg">cba</span>, <span class="pkg">cclust</span>,
<span class="pkg">cluster</span>, <span class="pkg">e1071</span>, <span class="pkg">flexclust</span>, <span class="pkg">kernlab</span>, and
<span class="pkg">mclust</span> (and of course, <span class="pkg">clue</span> itself).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Show how prototypes ("centers") vary across k-means runs on
## bootstrap samples from the Cassini data.
data("Cassini")
nr &lt;- NROW(Cassini$x)
out &lt;- replicate(50,
                 { kmeans(Cassini$x[sample(nr, replace = TRUE), ], 3) },
                 simplify = FALSE)
## Plot the data points in light gray, and the prototypes found.
plot(Cassini$x, col = gray(0.8))
points(do.call("rbind", lapply(out, cl_prototypes)), pch = 19)
</code></pre>

<hr>
<h2 id='cl_tabulate'>Tabulate Vector Objects</h2><span id='topic+cl_tabulate'></span>

<h3>Description</h3>

<p>Tabulate the unique values in vector objects.</p>


<h3>Usage</h3>

<pre><code class='language-R'>cl_tabulate(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cl_tabulate_+3A_x">x</code></td>
<td>
<p>a vector.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with components:
</p>
<table role = "presentation">
<tr><td><code>values</code></td>
<td>
<p>the unique values.</p>
</td></tr>
<tr><td><code>counts</code></td>
<td>
<p>an integer vector with the number of times each of the
unique values occurs in <code>x</code>.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data("Kinship82")
tab &lt;- cl_tabulate(Kinship82)
## The counts:
tab$counts
## The most frequent partition:
tab$values[[which.max(tab$counts)]]
</code></pre>

<hr>
<h2 id='cl_ultrametric'>Ultrametrics of Hierarchies</h2><span id='topic+cl_ultrametric'></span><span id='topic+as.cl_ultrametric'></span>

<h3>Description</h3>

<p>Compute the ultrametric distances for objects representing (total
indexed) hierarchies.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cl_ultrametric(x, size = NULL, labels = NULL)
as.cl_ultrametric(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cl_ultrametric_+3A_x">x</code></td>
<td>
<p>an R object representing a (total indexed) hierarchy of
objects.</p>
</td></tr>
<tr><td><code id="cl_ultrametric_+3A_size">size</code></td>
<td>
<p>an integer giving the number of objects in the hierarchy.</p>
</td></tr>
<tr><td><code id="cl_ultrametric_+3A_labels">labels</code></td>
<td>
<p>a character vector giving the names of the objects in
the hierarchy.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>x</code> is not an ultrametric or a hierarchy with an ultrametric
representation, <code>cl_ultrametric</code> uses
<code><a href="stats.html#topic+cophenetic">cophenetic</a></code> to obtain the ultrametric (also known
as cophenetic) distances from the hierarchy, which in turn by default
calls the S3 generic <code><a href="stats.html#topic+as.hclust">as.hclust</a></code> on the hierarchy.
Support for a class which represents hierarchies can thus be added by 
providing <code>as.hclust</code> methods for this class.  In R 2.1.0 or
better, <code>cophenetic</code> is an S3 generic as well, and one can also
more directly provide methods for this if necessary.
</p>
<p><code>as.cl_ultrametric</code> is a generic function which can be used for
coercing <em>raw</em> (non-classed) ultrametrics, represented as numeric
vectors (of the lower-half entries) or numeric matrices, to
ultrametric objects.
</p>
<p>Ultrametric objects are implemented as symmetric proximity objects
with a dissimilarity interpretation so that self-proximities are zero,
and inherit from classes <code>"<a href="#topic+cl_dissimilarity">cl_dissimilarity</a>"</code> and
<code>"cl_proximity"</code>.  See section <b>Details</b> in the
documentation for <code><a href="#topic+cl_dissimilarity">cl_dissimilarity</a></code> for implications.
</p>
<p>Ultrametric objects can also be coerced to classes
<code>"<a href="stats.html#topic+dendrogram">dendrogram</a>"</code> and
<code>"<a href="stats.html#topic+hclust">hclust</a>"</code>, and hence in particular use the
<code>plot</code> methods for these classes.  By default, plotting an
ultrametric object uses the plot method for dendrograms.
</p>


<h3>Value</h3>

<p>An object of class <code>"cl_ultrametric"</code> containing the ultrametric
distances.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+is.cl_hierarchy">is.cl_hierarchy</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>hc &lt;- hclust(dist(USArrests))
u &lt;- cl_ultrametric(hc)
## Subscripting.
u[1 : 5, 1 : 5]
u[1 : 5, 6 : 7]
## Plotting.
plot(u)
</code></pre>

<hr>
<h2 id='cl_validity'>Validity Measures for Partitions and Hierarchies</h2><span id='topic+cl_validity'></span><span id='topic+cl_validity.default'></span>

<h3>Description</h3>

<p>Compute validity measures for partitions and hierarchies, attempting
to measure how well these clusterings capture the underlying structure
in the data they were obtained from.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cl_validity(x, ...)
## Default S3 method:
cl_validity(x, d, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cl_validity_+3A_x">x</code></td>
<td>
<p>an object representing a partition or hierarchy.</p>
</td></tr>
<tr><td><code id="cl_validity_+3A_d">d</code></td>
<td>
<p>a dissimilarity object from which <code>x</code> was obtained.</p>
</td></tr>
<tr><td><code id="cl_validity_+3A_...">...</code></td>
<td>
<p>arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>cl_validity</code> is a generic function.
</p>
<p>For partitions, its default method gives the &ldquo;dissimilarity
accounted for&rdquo;, defined as <code class="reqn">1 - a_w / a_t</code>, where <code class="reqn">a_t</code> is
the average total dissimilarity, and the &ldquo;average within
dissimilarity&rdquo; <code class="reqn">a_w</code> is given by
</p>
<p style="text-align: center;"><code class="reqn">\frac{\sum_{i,j} \sum_k m_{ik}m_{jk} d_{ij}}{
        \sum_{i,j} \sum_k m_{ik}m_{jk}}</code>
</p>

<p>where <code class="reqn">d</code> and <code class="reqn">m</code> are the dissimilarities and memberships,
respectively, and the sums are over all pairs of objects and all
classes.
</p>
<p>For hierarchies, the validity measures computed by default are
&ldquo;variance accounted for&rdquo; (VAF, e.g., Hubert, Arabie &amp; Meulman,
2006) and &ldquo;deviance accounted for&rdquo; (DEV, e.g., Smith, 2001).
If <code>u</code> is the ultrametric corresponding to the hierarchy <code>x</code>
and <code>d</code> the dissimilarity <code>x</code> was obtained from, these
validity measures are given by
</p>
<p style="text-align: center;"><code class="reqn">\mathrm{VAF} =
    \max\left(0, 1 - \frac{\sum_{i,j} (d_{ij} - u_{ij})^2}{
    \sum_{i,j} (d_{ij} - \mathrm{mean}(d)) ^ 2}\right)</code>
</p>

<p>and
</p>
<p style="text-align: center;"><code class="reqn">\mathrm{DEV} =
    \max\left(0, 1 - \frac{\sum_{i,j} |d_{ij} - u_{ij}|}{
    \sum_{i,j} |d_{ij} - \mathrm{median}(d)|}\right)</code>
</p>

<p>respectively.  Note that VAF and DEV are not invariant under rescaling
<code>u</code>, and may be &ldquo;arbitrarily small&rdquo; (i.e., 0 using the
above definitions) even though <code>u</code> and <code>d</code> are
&ldquo;structurally close&rdquo; in some sense.
</p>
<p>For the results of using <code><a href="cluster.html#topic+agnes">agnes</a></code> and
<code><a href="cluster.html#topic+diana">diana</a></code>, the agglomerative and divisive
coefficients are provided in addition to the default ones.
</p>


<h3>Value</h3>

<p>A list of class <code>"cl_validity"</code> with the computed validity
measures.
</p>


<h3>References</h3>

<p>L. Hubert, P. Arabie and J. Meulman (2006).
<em>The structural representation of proximity matrices with
MATLAB</em>.
Philadelphia, PA: SIAM.
</p>
<p>T. J. Smith (2001).
Constructing ultrametric and additive trees based on the <code class="reqn">L_1</code>
norm.
<em>Journal of Classification</em>, <b>18</b>/2, 185&ndash;207.
<a href="https://link.springer.com/article/10.1007/s00357-001-0015-0">https://link.springer.com/article/10.1007/s00357-001-0015-0</a>.




</p>


<h3>See Also</h3>

<p><code><a href="fpc.html#topic+cluster.stats">cluster.stats</a></code> in package <span class="pkg">fpc</span> for a variety of
cluster validation statistics;
<code><a href="e1071.html#topic+fclustIndex">fclustIndex</a></code> in package <span class="pkg">e1071</span> for several
fuzzy cluster indexes;
<code><a href="cclust.html#topic+clustIndex">clustIndex</a></code> in package <span class="pkg">cclust</span>;
<code><a href="cluster.html#topic+silhouette">silhouette</a></code> in package <span class="pkg">cluster</span>.
</p>

<hr>
<h2 id='fit_ultrametric_target'>Fit Dissimilarities to a Hierarchy</h2><span id='topic+ls_fit_ultrametric_target'></span><span id='topic+l1_fit_ultrametric_target'></span>

<h3>Description</h3>

<p>Find the ultrametric from a target equivalence class of hierarchies
which minimizes weighted Euclidean or Manhattan dissimilarity to a
given dissimilarity object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ls_fit_ultrametric_target(x, y, weights = 1)
l1_fit_ultrametric_target(x, y, weights = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fit_ultrametric_target_+3A_x">x</code></td>
<td>
<p>a dissimilarity object inheriting from class
<code>"<a href="stats.html#topic+dist">dist</a>"</code>.</p>
</td></tr>
<tr><td><code id="fit_ultrametric_target_+3A_y">y</code></td>
<td>
<p>a target hierarchy.</p>
</td></tr>
<tr><td><code id="fit_ultrametric_target_+3A_weights">weights</code></td>
<td>
<p>a numeric vector or matrix with non-negative weights
for obtaining a weighted fit.  If a matrix, its numbers of rows and
columns must be the same as the number of objects in <code>x</code>.
Otherwise, it is recycled to the number of elements in <code>x</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The target equivalence class consists of all dendrograms for which the
corresponding <code class="reqn">n</code>-trees are the same as the one corresponding to
<code>y</code>.  I.e., all splits are the same as for <code>y</code>, and
optimization is over the height of the splits.
</p>
<p>The criterion function to be optimized over all ultrametrics from the
equivalence class is <code class="reqn">\sum w_{ij} |x_{ij} - u_{ij}|^p</code>, where
<code class="reqn">p = 2</code> in the Euclidean and <code class="reqn">p = 1</code> in the Manhattan case,
respectively.
</p>
<p>The optimum can be computed as follows.  Suppose split <code class="reqn">s</code> joins
object classes <code class="reqn">A</code> and <code class="reqn">B</code>.  As the ultrametric
dissimilarities of all objects in <code class="reqn">A</code> to all objects in <code class="reqn">B</code>
must be the same value, say, <code class="reqn">u_{A,B} = u_s</code>, the contribution
from the split to the criterion function is of the form
<code class="reqn">f_s(u_s) = \sum_{i \in A, j \in B} w_{ij} |x_{ij} - u_s|^p</code>.
We need to minimize <code class="reqn">\sum_s f_s(u_s)</code> under the constraint that
the <code class="reqn">u_s</code> form a non-decreasing sequence, which is accomplished by
using the Pool Adjacent Violator Algorithm (<abbr><span class="acronym">PAVA</span></abbr>) using the
weighted mean (<code class="reqn">p = 2</code>) or weighted median (<code class="reqn">p = 1</code>) for
solving the blockwise optimization problems.
</p>


<h3>Value</h3>

<p>An object of class <code>"<a href="#topic+cl_ultrametric">cl_ultrametric</a>"</code> containing the
optimal ultrametric distances.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ls_fit_ultrametric">ls_fit_ultrametric</a></code> for finding the ultrametric
minimizing Euclidean dissimilarity (without fixing the splits).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Phonemes")
## Note that the Phonemes data set has the consonant misclassification
## probabilities, i.e., the similarities between the phonemes.
d &lt;- as.dist(1 - Phonemes)
## Find the maximal dominated and miminal dominating ultrametrics by
## hclust() with single and complete linkage:
y1 &lt;- hclust(d, "single")
y2 &lt;- hclust(d, "complete")
## Note that these are quite different:
cl_dissimilarity(y1, y2, "gamma")
## Now find the L2 optimal members of the respective dendrogram
## equivalence classes.
u1 &lt;- ls_fit_ultrametric_target(d, y1)
u2 &lt;- ls_fit_ultrametric_target(d, y2)
## Compute the L2 optimal ultrametric approximation to d.
u &lt;- ls_fit_ultrametric(d)
## And compare ...
cl_dissimilarity(cl_ensemble(Opt = u, Single = u1, Complete = u2), d)
## The solution obtained via complete linkage is quite close:
cl_agreement(u2, u, "cophenetic")
</code></pre>

<hr>
<h2 id='GVME'>Gordon-Vichi Macroeconomic Partition Ensemble Data</h2><span id='topic+GVME'></span>

<h3>Description</h3>

<p>Soft partitions of 21 countries based on macroeconomic data for the
years 1975, 1980, 1985, 1990, and 1995.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("GVME")</code></pre>


<h3>Format</h3>

<p>A named cluster ensemble of 5 soft partitions of 21 countries into 2
or 3 classes.  The names are the years to which the partitions
correspond.
</p>


<h3>Details</h3>

<p>The partitions were obtained using fuzzy <code class="reqn">c</code>-means on measurements
of the following variables: the annual per capita gross domestic
product (GDP) in USD (converted to 1987 prices); the percentage of GDP
provided by agriculture; the percentage of employees who worked in
agriculture; and gross domestic investment, expressed as a percentage
of the GDP.  See Gordon and Vichi (2001), page 230, for more details.
</p>


<h3>Source</h3>

<p>Table 1 in Gordon and Vichi (2001).
</p>


<h3>References</h3>

<p>A. D. Gordon and M. Vichi (2001).  
Fuzzy partition models for fitting a set of partitions.
<em>Psychometrika</em>, <b>66</b>, 229&ndash;248.
<a href="https://doi.org/10.1007/BF02294837">doi:10.1007/BF02294837</a>.
</p>

<hr>
<h2 id='GVME_Consensus'>Gordon-Vichi Macroeconomic Consensus Partition Data</h2><span id='topic+GVME_Consensus'></span>

<h3>Description</h3>

<p>The soft (&ldquo;fuzzy&rdquo;) consensus partitions for the macroeconomic
partition data given in Gordon and Vichi (2001).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("GVME_Consensus")</code></pre>


<h3>Format</h3>

<p>A named cluster ensemble of eight soft partitions of 21 countries
terms into two or three classes.
</p>


<h3>Details</h3>

<p>The elements of the ensemble are consensus partitions for the
macroeconomic partition data in Gordon and Vichi (2001), which are
available as data set <code><a href="#topic+GVME">GVME</a></code>.  Element names are of the
form <code>"<var>m</var>/<var>k</var>"</code>, where <var>m</var> indicates the consensus
method employed (one of &lsquo;<span class="samp">&#8288;MF1&#8288;</span>&rsquo;, &lsquo;<span class="samp">&#8288;MF2&#8288;</span>&rsquo;, &lsquo;<span class="samp">&#8288;JMF&#8288;</span>&rsquo;, and
&lsquo;<span class="samp">&#8288;S&amp;S&#8288;</span>&rsquo;, corresponding to the application of models 1, 2, and 3
in Gordon and Vichi (2001) and the approach in Sato and Sato (1994),
respectively), and <var>k</var> denotes the number classes (2 or 3).
</p>


<h3>Source</h3>

<p>Tables 4 and 5 in Gordon and Vichi (2001).
</p>


<h3>References</h3>

<p>A. D. Gordon and M. Vichi (2001).
Fuzzy partition models for fitting a set of partitions.
<em>Psychometrika</em>, <b>66</b>, 229&ndash;248.
<a href="https://doi.org/10.1007/BF02294837">doi:10.1007/BF02294837</a>.
</p>
<p>M. Sato and Y. Sato (1994).
On a multicriteria fuzzy clustering method for 3-way data.
<em>International Journal of Uncertainty, Fuzziness and
Knowledge-Based Systems</em>, <b>2</b>, 127&ndash;142. <br />
<a href="https://doi.org/10.1142/S0218488594000122">doi:10.1142/S0218488594000122</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Load the consensus partitions.
data("GVME_Consensus")
## Pick the partitions into 2 classes.
GVME_Consensus_2 &lt;- GVME_Consensus[1 : 4]
## Fuzziness using the Partition Coefficient.
cl_fuzziness(GVME_Consensus_2)
## (Corresponds to 1 - F in the source.)
## Dissimilarities:
cl_dissimilarity(GVME_Consensus_2)
cl_dissimilarity(GVME_Consensus_2, method = "comem")
</code></pre>

<hr>
<h2 id='hierarchy'>Hierarchies</h2><span id='topic+cl_hierarchy'></span><span id='topic+is.cl_hierarchy'></span><span id='topic+as.cl_hierarchy'></span><span id='topic+cl_dendrogram'></span><span id='topic+is.cl_dendrogram'></span><span id='topic+as.cl_dendrogram'></span><span id='topic+plot.cl_dendrogram'></span>

<h3>Description</h3>

<p>Determine whether an R object represents a hierarchy of objects, or
coerce to an R object representing such.</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.cl_hierarchy(x)
is.cl_dendrogram(x)

as.cl_hierarchy(x)
as.cl_dendrogram(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hierarchy_+3A_x">x</code></td>
<td>
<p>an R object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions are generic functions.
</p>
<p>The methods provided in package <span class="pkg">clue</span> handle the partitions and
hierarchies obtained from clustering functions in the base R
distribution, as well as packages <span class="pkg">RWeka</span>, <span class="pkg">ape</span>, <span class="pkg">cba</span>,
<span class="pkg">cclust</span>, <span class="pkg">cluster</span>, <span class="pkg">e1071</span>, <span class="pkg">flexclust</span>,
<span class="pkg">flexmix</span>, <span class="pkg">kernlab</span>, <span class="pkg">mclust</span>, <span class="pkg">movMF</span> and
<span class="pkg">skmeans</span> (and of course, <span class="pkg">clue</span> itself).
</p>
<p>The hierarchies considered by <span class="pkg">clue</span> are <em><code class="reqn">n</code>-trees</em>
(hierarchies in the strict sense) and <em>dendrograms</em> (also known
as valued <code class="reqn">n</code>-trees or total indexed hierarchies), which are
represented by the virtual classes <code>"cl_hierarchy"</code> and
<code>"cl_dendrogram"</code> (which inherits from the former),
respectively.
</p>
<p><code class="reqn">n</code>-trees on a set <code class="reqn">X</code> of objects correspond to collections
<code class="reqn">H</code> of subsets of <code class="reqn">X</code>, usually called <em>classes</em> of the
hierarchy, which satisfy the following properties:
</p>

<ul>
<li> <p><code class="reqn">H</code> contains all singletons with objects of <code class="reqn">X</code>,
<code class="reqn">X</code> itself, but not the empty set;
</p>
</li>
<li><p> The intersection of two sets <code class="reqn">A</code> and <code class="reqn">B</code> in <code class="reqn">H</code> is
either empty or one of the sets.
</p>
</li></ul>

<p>The classes of a hierarchy can be obtained by
<code><a href="#topic+cl_classes">cl_classes</a></code>.
</p>
<p>Dendrograms are <code class="reqn">n</code>-trees where additionally a height <code class="reqn">h</code> is
associated with each of the classes, so that for two classes <code class="reqn">A</code>
and <code class="reqn">B</code> with non-empty intersection we have <code class="reqn">h(A) \le h(B)</code>
iff <code class="reqn">A</code> is a subset of <code class="reqn">B</code>.  For each pair of objects one can
then define <code class="reqn">u_{ij}</code> as the height of the smallest class
containing both <code class="reqn">i</code> and <code class="reqn">j</code>: this results in a dissimilarity
on <code class="reqn">X</code> which satisfies the ultrametric (3-point) conditions
<code class="reqn">u_{ij} \le \max(u_{ik}, u_{jk})</code> for all triples <code class="reqn">(i, j, k)</code>
of objects.  Conversely, an ultrametric dissimilarity induces a unique
dendrogram.
</p>
<p>The ultrametric dissimilarities of a dendrogram can be obtained by
<code><a href="#topic+cl_ultrametric">cl_ultrametric</a></code>.
</p>
<p><code>as.cl_hierarchy</code> returns an object of class
<code>"cl_hierarchy"</code> &ldquo;containing&rdquo; the given object <code>x</code> if
this already represents a hierarchy (i.e., <code>is.cl_hierarchy(x)</code>
is true), or the ultrametric obtained from <code>x</code> via
<code><a href="#topic+as.cl_ultrametric">as.cl_ultrametric</a></code>.
</p>
<p><code>as.cl_dendrogram</code> returns an object which has class
<code>"cl_dendrogram"</code> and inherits from <code>"cl_hierarchy"</code>,
and contains <code>x</code> if it represents a dendrogram (i.e.,
<code>is.cl_dendrogram(x)</code> is true), or the ultrametric obtained from
<code>x</code>.
</p>
<p>Conceptually, hierarchies and dendrograms are <em>virtual</em> classes,
allowing for a variety of representations.
</p>
<p>There are group methods for comparing dendrograms and computing their
minimum, maximum, and range based on the meet and join operations, see
<code><a href="#topic+cl_meet">cl_meet</a></code>.  There is also a <code>plot</code> method.
</p>


<h3>Value</h3>

<p>For the testing functions, a logical indicating whether the given
object represents a clustering of objects of the respective kind.
</p>
<p>For the coercion functions, a container object inheriting from
<code>"cl_hierarchy"</code>, with a suitable representation of the hierarchy
given by <code>x</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>hcl &lt;- hclust(dist(USArrests))
is.cl_dendrogram(hcl)
is.cl_hierarchy(hcl)
</code></pre>

<hr>
<h2 id='Kinship82'>Rosenberg-Kim Kinship Terms Partition Data</h2><span id='topic+Kinship82'></span>

<h3>Description</h3>

<p>Partitions of 15 kinship terms given by 85 female undergraduates at
Rutgers University who were asked to sort the terms into classes
&ldquo;on the basis of some aspect of meaning&rdquo;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("Kinship82")</code></pre>


<h3>Format</h3>

<p>A cluster ensemble of 85 hard partitions of the 15 kinship terms.
</p>


<h3>Details</h3>

<p>Rosenberg and Kim (1975) describe an experiment where perceived
similarities of the kinship terms were obtained from six different
&ldquo;sorting&rdquo; experiments.  These &ldquo;original&rdquo; Rosenberg-Kim
kinship terms data were published in Arabie, Carroll and de Sarbo
(1987), and are also contained in file &lsquo;<span class="file">indclus.data</span>&rsquo; in the
shell archive <a href="https://netlib.org/mds/indclus.shar">https://netlib.org/mds/indclus.shar</a>.
</p>
<p>For one of the experiments, partitions of the terms were printed in
Rosenberg (1982).  Comparison with the original data indicates that
the partition data have the &ldquo;nephew&rdquo; and &ldquo;niece&rdquo; columns
interchanged, which is corrected in the data set at hand.
</p>


<h3>Source</h3>

<p>Table 7.1 in Rosenberg (1982), with the &ldquo;nephew&rdquo; and
&ldquo;niece&rdquo; columns interchanged.
</p>


<h3>References</h3>

<p>P. Arabie, J. D. Carroll and W. S. de Sarbo (1987).
<em>Three-way scaling and clustering</em>.
Newbury Park, CA: Sage.
</p>
<p>S. Rosenberg and M. P. Kim (1975).
The method of sorting as a data-gathering procedure in multivariate
research.
<em>Multivariate Behavioral Research</em>, <b>10</b>, 489&ndash;502. <br />
<a href="https://doi.org/10.1207/s15327906mbr1004_7">doi:10.1207/s15327906mbr1004_7</a>.
</p>
<p>S. Rosenberg (1982).
The method of sorting in multivariate research with applications
selected from cognitive psychology and person perception.
In N. Hirschberg and L. G. Humphreys (eds.),
<em>Multivariate Applications in the Social Sciences</em>, 117&ndash;142.
Hillsdale, NJ: Erlbaum.
</p>

<hr>
<h2 id='Kinship82_Consensus'>Gordon-Vichi Kinship82 Consensus Partition Data</h2><span id='topic+Kinship82_Consensus'></span>

<h3>Description</h3>

<p>The soft (&ldquo;fuzzy&rdquo;) consensus partitions for the Rosenberg-Kim
kinship terms partition data given in Gordon and Vichi (2001).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("Kinship82_Consensus")</code></pre>


<h3>Format</h3>

<p>A named cluster ensemble of three soft partitions of the 15 kinship
terms into three classes.
</p>


<h3>Details</h3>

<p>The elements of the ensemble are named <code>"MF1"</code>, <code>"MF2"</code>, and
<code>"JMF"</code>, and correspond to the consensus partitions obtained by
applying models 1, 2, and 3 in Gordon and Vichi (2001) to the kinship
terms partition data in Rosenberg (1982), which are available as data
set <code><a href="#topic+Kinship82">Kinship82</a></code>.
</p>


<h3>Source</h3>

<p>Table 6 in Gordon and Vichi (2001).
</p>


<h3>References</h3>

<p>A. D. Gordon and M. Vichi (2001).  
Fuzzy partition models for fitting a set of partitions.
<em>Psychometrika</em>, <b>66</b>, 229&ndash;248.
<a href="https://doi.org/10.1007/BF02294837">doi:10.1007/BF02294837</a>.
</p>
<p>S. Rosenberg (1982).
The method of sorting in multivariate research with applications
selected from cognitive psychology and person perception.
In N. Hirschberg and L. G. Humphreys (eds.),
<em>Multivariate Applications in the Social Sciences</em>, 117&ndash;142.
Hillsdale, NJ: Erlbaum.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Load the consensus partitions.
data("Kinship82_Consensus")
## Fuzziness using the Partition Coefficient.
cl_fuzziness(Kinship82_Consensus)
## (Corresponds to 1 - F in the source.)
## Dissimilarities:
cl_dissimilarity(Kinship82_Consensus)
cl_dissimilarity(Kinship82_Consensus, method = "comem")
</code></pre>

<hr>
<h2 id='kmedoids'>K-Medoids Clustering</h2><span id='topic+kmedoids'></span>

<h3>Description</h3>

<p>Compute a <code class="reqn">k</code>-medoids partition of a dissimilarity object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kmedoids(x, k)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kmedoids_+3A_x">x</code></td>
<td>
<p>a dissimilarity object inheriting from class
<code>"<a href="stats.html#topic+dist">dist</a>"</code>, or a square matrix of pairwise
object-to-object dissimilarity values.</p>
</td></tr>
<tr><td><code id="kmedoids_+3A_k">k</code></td>
<td>
<p>an integer giving the number of classes to be used in the
partition.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Let <code class="reqn">d</code> denote the pairwise object-to-object dissimilarity matrix
corresponding to <code>x</code>.  A <code class="reqn">k</code>-medoids partition of <code>x</code> is
defined as a partition of the numbers from 1 to <code class="reqn">n</code>, the number of
objects in <code>x</code>, into <code class="reqn">k</code> classes <code class="reqn">C_1, \ldots, C_k</code> such
that the criterion function
<code class="reqn">L = \sum_l \min_{j \in C_l} \sum_{i \in C_l} d_{ij}</code>
is minimized.
</p>
<p>This is an NP-hard optimization problem.  PAM (Partitioning Around
Medoids, see Kaufman &amp; Rousseeuw (1990), Chapter 2) is a very popular
heuristic for obtaining optimal <code class="reqn">k</code>-medoids partitions, and
provided by <code><a href="cluster.html#topic+pam">pam</a></code> in package <span class="pkg">cluster</span>.
</p>
<p><code>kmedoids</code> is an exact algorithm based on a binary linear
programming formulation of the optimization problem (e.g., Gordon &amp;
Vichi (1998), [P4']), using <code><a href="lpSolve.html#topic+lp">lp</a></code> from package
<span class="pkg">lpSolve</span> as solver.  Depending on available hardware resources
(the number of constraints of the program is of the order <code class="reqn">n^2</code>),
it may not be possible to obtain a solution.
</p>


<h3>Value</h3>

<p>An object of class <code>"kmedoids"</code> representing the obtained
partition, which is a list with the following components.
</p>
<table role = "presentation">
<tr><td><code>cluster</code></td>
<td>
<p>the class ids of the partition.</p>
</td></tr>
<tr><td><code>medoid_ids</code></td>
<td>
<p>the indices of the medoids.</p>
</td></tr>
<tr><td><code>criterion</code></td>
<td>
<p>the value of the criterion function of the
partition.</p>
</td></tr>
</table>


<h3>References</h3>

<p>L. Kaufman and P. J. Rousseeuw (1990).
<em>Finding Groups in Data: An Introduction to Cluster Analysis</em>.
Wiley, New York.
</p>
<p>A. D. Gordon and M. Vichi (1998).
Partitions of partitions.
<em>Journal of Classification</em>, <b>15</b>, 265&ndash;285.
<a href="https://doi.org/10.1007/s003579900034">doi:10.1007/s003579900034</a>.
</p>

<hr>
<h2 id='l1_fit_ultrametric'>Least Absolute Deviation Fit of Ultrametrics to Dissimilarities</h2><span id='topic+l1_fit_ultrametric'></span>

<h3>Description</h3>

<p>Find the ultrametric with minimal absolute distance (Manhattan
dissimilarity) to a given dissimilarity object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>l1_fit_ultrametric(x, method = c("SUMT", "IRIP"), weights = 1,
                   control = list())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="l1_fit_ultrametric_+3A_x">x</code></td>
<td>
<p>a dissimilarity object inheriting from or coercible to class
<code>"<a href="stats.html#topic+dist">dist</a>"</code>.</p>
</td></tr>
<tr><td><code id="l1_fit_ultrametric_+3A_method">method</code></td>
<td>
<p>a character string indicating the fitting method to be
employed.  Must be one of <code>"SUMT"</code> (default) or <code>"IRIP"</code>,
or a unique abbreviation thereof.</p>
</td></tr>
<tr><td><code id="l1_fit_ultrametric_+3A_weights">weights</code></td>
<td>
<p>a numeric vector or matrix with non-negative weights
for obtaining a weighted least squares fit.  If a matrix, its
numbers of rows and columns must be the same as the number of
objects in <code>x</code>, and the lower diagonal part is used.
Otherwise, it is recycled to the number of elements in <code>x</code>.</p>
</td></tr>
<tr><td><code id="l1_fit_ultrametric_+3A_control">control</code></td>
<td>
<p>a list of control parameters.  See <b>Details</b>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The problem to be solved is minimizing
</p>
<p style="text-align: center;"><code class="reqn">L(u) = \sum_{i,j} w_{ij} |x_{ij} - u_{ij}|</code>
</p>

<p>over all <code class="reqn">u</code> satisfying the ultrametric constraints (i.e., for all
<code class="reqn">i, j, k</code>, <code class="reqn">u_{ij} \le \max(u_{ik}, u_{jk})</code>).  This problem
is known to be NP hard (Krivanek and Moravek, 1986).
</p>
<p>We provide two heuristics for solving this problem.
</p>
<p>Method <code>"SUMT"</code> implements a <abbr><span class="acronym">SUMT</span></abbr> (Sequential
Unconstrained Minimization Technique, see <code><a href="#topic+sumt">sumt</a></code>) approach
using the sign function for the gradients of the absolute value
function.
</p>
<p>Available control parameters are <code>method</code>, <code>control</code>,
<code>eps</code>, <code>q</code>, and <code>verbose</code>, which have the same roles as
for <code><a href="#topic+sumt">sumt</a></code>, and the following.
</p>

<dl>
<dt><code>nruns</code></dt><dd><p>an integer giving the number of runs to be
performed.  Defaults to 1.</p>
</dd>
<dt><code>start</code></dt><dd><p>a single dissimilarity, or a list of
dissimilarities to be employed as starting values.</p>
</dd>
</dl>

<p>Method <code>"IRIP"</code> implements a variant of the Iteratively
Reweighted Iterative Projection approach of Smith (2001), which
attempts to solve the <code class="reqn">L_1</code> problem via a sequence of weighted
<code class="reqn">L_2</code> problems, determining <code class="reqn">u(t+1)</code> by minimizing the
criterion function
</p>
<p style="text-align: center;"><code class="reqn">\sum_{i,j} w_{ij}
    (x_{ij} - u_{ij})^2 / \max(|x_{ij} - u_{ij}(t)|, m)</code>
</p>

<p>with <code class="reqn">m</code> a &ldquo;small&rdquo; non-zero value to avoid zero divisors.
We use the <abbr><span class="acronym">SUMT</span></abbr> method of <code><a href="#topic+ls_fit_ultrametric">ls_fit_ultrametric</a></code>
for solving the weighted least squares problems.
</p>
<p>Available control parameters are as follows.
</p>

<dl>
<dt><code>maxiter</code></dt><dd><p>an integer giving the maximal number of
iteration steps to be performed.
Defaults to 100.</p>
</dd>
<dt><code>eps</code></dt><dd><p>a nonnegative number controlling the iteration,
which stops when the maximal change in <code class="reqn">u</code> is less than
<code>eps</code>.
Defaults to <code class="reqn">10^{-6}</code>.</p>
</dd>
<dt><code>reltol</code></dt><dd><p>the relative convergence tolerance.  Iteration
stops when the relative change in the <code class="reqn">L_1</code> criterion is less
than <code>reltol</code>.
Defaults to <code class="reqn">10^{-6}</code>.</p>
</dd>
<dt><code>MIN</code></dt><dd><p>the cutoff <code class="reqn">m</code>.  Defaults to <code class="reqn">10^{-3}</code>.</p>
</dd>
<dt><code>start</code></dt><dd><p>a dissimilarity object to be used as the
starting value for <code class="reqn">u</code>.</p>
</dd>
<dt><code>control</code></dt><dd><p>a list of control parameters to be used by the
method of <code><a href="#topic+ls_fit_ultrametric">ls_fit_ultrametric</a></code> employed for solving
the weighted <code class="reqn">L_2</code> problems.</p>
</dd>
</dl>

<p>One may need to adjust the default control parameters to achieve
convergence.
</p>
<p>It should be noted that all methods are heuristics which can not be
guaranteed to find the global minimum.  
</p>


<h3>Value</h3>

<p>An object of class <code>"<a href="#topic+cl_ultrametric">cl_ultrametric</a>"</code> containing the
fitted ultrametric distances.
</p>


<h3>References</h3>

<p>M. Krivanek and J. Moravek (1986).
NP-hard problems in hierarchical tree clustering.
<em>Acta Informatica</em>, <b>23</b>, 311&ndash;323.
<a href="https://doi.org/10.1007/BF00289116">doi:10.1007/BF00289116</a>.
</p>
<p>T. J. Smith (2001).
Constructing ultrametric and additive trees based on the <code class="reqn">L_1</code>
norm.
<em>Journal of Classification</em>, <b>18</b>, 185&ndash;207.
<a href="https://link.springer.com/article/10.1007/s00357-001-0015-0">https://link.springer.com/article/10.1007/s00357-001-0015-0</a>.




</p>


<h3>See Also</h3>

<p><code><a href="#topic+cl_consensus">cl_consensus</a></code> for computing least absolute deviation
(Manhattan) consensus hierarchies;
<code><a href="#topic+ls_fit_ultrametric">ls_fit_ultrametric</a></code>.
</p>

<hr>
<h2 id='lattice'>Cluster Lattices</h2><span id='topic+cl_meet'></span><span id='topic+cl_join'></span><span id='topic+Ops.cl_partition'></span><span id='topic+Summary.cl_partition'></span><span id='topic+Ops.cl_dendrogram'></span><span id='topic+Ops.cl_hierarchy'></span><span id='topic+Summary.cl_hierarchy'></span>

<h3>Description</h3>

<p>Computations on the lattice of all (hard) partitions, or the lattice
of all dendrograms, or the meet semilattice of all hierarchies
(<code class="reqn">n</code>-trees) of/on a set of objects: meet, join, and comparisons.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cl_meet(x, y)
cl_join(x, y)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lattice_+3A_x">x</code></td>
<td>
<p>an ensemble of partitions or dendrograms or hierarchies, or
an R object representing a partition or dendrogram or hierarchy.</p>
</td></tr>
<tr><td><code id="lattice_+3A_y">y</code></td>
<td>
<p>an R object representing a partition or dendrogram or
hierarchy.  Ignored if <code>x</code> is an ensemble.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For a given finite set of objects <code class="reqn">X</code>, the set <code class="reqn">H(X)</code> of all
(hard) partitions of <code class="reqn">X</code> can be partially ordered by defining a
partition <code class="reqn">P</code> to be &ldquo;finer&rdquo; than a partition <code class="reqn">Q</code>, i.e.,
<code class="reqn">P \le Q</code>, if each class of <code class="reqn">P</code> is contained in some class of
<code class="reqn">Q</code>.  With this partial order, <code class="reqn">H(X)</code> becomes a bounded
<dfn>lattice</dfn>, with intersection and union of two elements given by
their greatest lower bound (<dfn>meet</dfn>) and their least upper bound
(<dfn>join</dfn>), respectively.
</p>
<p>Specifically, the meet of two partitions computed by <code>cl_meet</code> is
the partition obtained by intersecting the classes of the partitions;
the classes of the join computed by <code>cl_join</code> are obtained by
joining all elements in the same class in at least one of the
partitions.  Obviously, the least and greatest elements of the
partition lattice are the partitions where each object is in a single
class (sometimes referred to as the &ldquo;splitter&rdquo; partition) or in
the same class (the &ldquo;lumper&rdquo; partition), respectively.  Meet
and join of an arbitrary number of partitions can be defined
recursively.
</p>
<p>In addition to computing the meet and join, the comparison operations
corresponding to the above partial order as well as <code>min</code>,
<code>max</code>, and <code>range</code> are available at least for R objects
representing partitions inheriting from <code>"<a href="#topic+cl_partition">cl_partition</a>"</code>.
The summary methods give the meet and join of the given partitions
(for <code>min</code> and <code>max</code>), or a partition ensemble with the meet
and join (for <code>range</code>).
</p>
<p>If the partitions specified by <code>x</code> and <code>y</code> are soft
partitions, the corresponding nearest hard partitions are used.
Future versions may optionally provide suitable &ldquo;soft&rdquo; (fuzzy)
extensions for computing meets and joins.
</p>
<p>The set of all dendrograms on <code class="reqn">X</code> can be ordered using pointwise
inequality of the associated ultrametric dissimilarities: i.e., if
<code class="reqn">D</code> and <code class="reqn">E</code> are the dendrograms with ultrametrics <code class="reqn">u</code> and
<code class="reqn">v</code>, respectively, then <code class="reqn">D \le E</code> if <code class="reqn">u_{ij} \le v_{ij}</code>
for all pairs <code class="reqn">(i, j)</code> of objects.  This again yields a lattice
(of dendrograms).  The join of <code class="reqn">D</code> and <code class="reqn">E</code> is the dendrogram
with ultrametrics given by <code class="reqn">\max(u_{ij}, v_{ij})</code> (as this gives
an ultrametric); the meet is the dendrogram with the maximal
ultrametric dominated by <code class="reqn">\min(u_{ij}, v_{ij})</code>, and can be
obtained by applying single linkage hierarchical clustering to the
minima.
</p>
<p>The set of all hierarchies on <code class="reqn">X</code> can be ordered by set-wise
inclusion of the classes: i.e., if <code class="reqn">H</code> and <code class="reqn">G</code> are two
hierarchies, then <code class="reqn">H \le G</code> if all classes of <code class="reqn">H</code> are also
classes of <code class="reqn">G</code>.  This yields a meet semilattice, with meet given
by the classes contained in both hierarchies.  The join only exists if
the union of the classes is a hierarchy.
</p>
<p>In each case, a modular semilattice is obtained, which allows for a
natural metrization via least element (semi)lattice move distances,
see Barthélémy, Leclerc and Monjardet (1981).  These latticial metrics
are given by the BA/C (partitions), Manhattan (dendrograms), and
symdiff (hierarchies) dissimilarities, respectively (see
<code><a href="#topic+cl_dissimilarity">cl_dissimilarity</a></code>).
</p>


<h3>Value</h3>

<p>For <code>cl_meet</code> and <code>cl_join</code>, an object of class
<code>"<a href="#topic+cl_partition">cl_partition</a>"</code> or <code>"<a href="#topic+cl_dendrogram">cl_dendrogram</a>"</code> with the
class ids or ultrametric dissimilarities of the meet and join of the
partitions or dendrograms, respectively.
</p>


<h3>References</h3>

<p>J.-P. Barthélémy, B. Leclerc and B. Monjardet (1981).
On the use of ordered sets in problems of comparison and consensus of
classification.
<em>Journal of Classification</em>, <b>3</b>, 187&ndash;224.
<a href="https://doi.org/10.1007/BF01894188">doi:10.1007/BF01894188</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Two simple partitions of 7 objects.
A &lt;- as.cl_partition(c(1, 1, 2, 3, 3, 5, 5))
B &lt;- as.cl_partition(c(1, 2, 2, 3, 4, 5, 5))
## These disagree on objects 1-3, A splits objects 4 and 5 into
## separate classes.  Objects 6 and 7 are always in the same class.
(A &lt;= B) || (B &lt;= A)
## (Neither partition is finer than the other.)
cl_meet(A, B)
cl_join(A, B)
## Meeting with the lumper (greatest) or joining with the splitter
## (least) partition does not make a difference: 
C_lumper &lt;- as.cl_partition(rep(1, n_of_objects(A)))
cl_meet(cl_ensemble(A, B, C_lumper))
C_splitter &lt;- as.cl_partition(seq_len(n_of_objects(A)))
cl_join(cl_ensemble(A, B, C_splitter))
## Another way of computing the join:
range(A, B, C_splitter)$max
</code></pre>

<hr>
<h2 id='ls_fit_addtree'>Least Squares Fit of Additive Tree Distances to Dissimilarities</h2><span id='topic+ls_fit_addtree'></span><span id='topic+ls_fit_centroid'></span>

<h3>Description</h3>

<p>Find the additive tree distance or centroid distance minimizing least
squares distance (Euclidean dissimilarity) to a given dissimilarity
object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ls_fit_addtree(x, method = c("SUMT", "IP", "IR"), weights = 1,
               control = list())
ls_fit_centroid(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ls_fit_addtree_+3A_x">x</code></td>
<td>
<p>a dissimilarity object inheriting from class
<code>"<a href="stats.html#topic+dist">dist</a>"</code>.</p>
</td></tr>
<tr><td><code id="ls_fit_addtree_+3A_method">method</code></td>
<td>
<p>a character string indicating the fitting method to be
employed.  Must be one of <code>"SUMT"</code> (default), <code>"IP"</code>, or
<code>"IR"</code>, or a unique abbreviation thereof.</p>
</td></tr>
<tr><td><code id="ls_fit_addtree_+3A_weights">weights</code></td>
<td>
<p>a numeric vector or matrix with non-negative weights
for obtaining a weighted least squares fit.  If a matrix, its
numbers of rows and columns must be the same as the number of
objects in <code>x</code>, and the lower diagonal part is used.
Otherwise, it is recycled to the number of elements in <code>x</code>.</p>
</td></tr>
<tr><td><code id="ls_fit_addtree_+3A_control">control</code></td>
<td>
<p>a list of control parameters.  See <b>Details</b>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <code><a href="#topic+as.cl_addtree">as.cl_addtree</a></code> for details on additive tree distances
and centroid distances.
</p>
<p>With <code class="reqn">L(d) = \sum w_{ij} (x_{ij} - d_{ij})^2</code>, the problem to be
solved by <code>ls_fit_addtree</code> is minimizing <code class="reqn">L</code> over all
additive tree distances <code class="reqn">d</code>.  This problem is known to be NP
hard.
</p>
<p>We provide three heuristics for solving this problem.
</p>
<p>Method <code>"SUMT"</code> implements the <abbr><span class="acronym">SUMT</span></abbr> (Sequential
Unconstrained Minimization Technique, Fiacco and McCormick, 1968)
approach of de Soete (1983).  Incomplete dissimilarities are currently
not supported.
</p>
<p>Methods <code>"IP"</code> and <code>"IR"</code> implement the Iterative
Projection and Iterative Reduction approaches of Hubert and Arabie
(1995) and Roux (1988), respectively.  Non-identical weights and
incomplete dissimilarities are currently not supported.
</p>
<p>See <code><a href="#topic+ls_fit_ultrametric">ls_fit_ultrametric</a></code> for details on these methods and
available control parameters.
</p>
<p>It should be noted that all methods are heuristics which can not be
guaranteed to find the global minimum.  Standard practice would
recommend to use the best solution found in &ldquo;sufficiently many&rdquo;
replications of the base algorithm.
</p>
<p><code>ls_fit_centroid</code> finds the centroid distance <code class="reqn">d</code> minimizing
<code class="reqn">L(d)</code> (currently, only for the case of identical weights).  This
optimization problem has a closed-form solution.
</p>


<h3>Value</h3>

<p>An object of class <code>"cl_addtree"</code> containing the optimal additive
tree distances.
</p>


<h3>References</h3>

<p>A. V. Fiacco and G. P. McCormick (1968).
<em>Nonlinear programming: Sequential unconstrained minimization
techniques</em>.
New York: John Wiley &amp; Sons.
</p>
<p>L. Hubert and P. Arabie (1995).
Iterative projection strategies for the least squares fitting of tree
structures to proximity data.
<em>British Journal of Mathematical and Statistical Psychology</em>,
<b>48</b>, 281&ndash;317.
<a href="https://doi.org/10.1111/j.2044-8317.1995.tb01065.x">doi:10.1111/j.2044-8317.1995.tb01065.x</a>.
</p>
<p>M. Roux (1988).
Techniques of approximation for building two tree structures.
In C. Hayashi and E. Diday and M. Jambu and N. Ohsumi (Eds.),
<em>Recent Developments in Clustering and Data Analysis</em>, pages
151&ndash;170.
New York: Academic Press.
</p>
<p>G. de Soete (1983).
A least squares algorithm for fitting additive trees to proximity
data.
<em>Psychometrika</em>, <b>48</b>, 621&ndash;626.
<a href="https://doi.org/10.1007/BF02293884">doi:10.1007/BF02293884</a>.
</p>

<hr>
<h2 id='ls_fit_sum_of_ultrametrics'>Least Squares Fit of Sums of Ultrametrics to Dissimilarities</h2><span id='topic+ls_fit_sum_of_ultrametrics'></span>

<h3>Description</h3>

<p>Find a sequence of ultrametrics with sum minimizing square distance
(Euclidean dissimilarity) to a given dissimilarity object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ls_fit_sum_of_ultrametrics(x, nterms = 1, weights = 1,
                           control = list())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ls_fit_sum_of_ultrametrics_+3A_x">x</code></td>
<td>
<p>a dissimilarity object inheriting from or coercible to class
<code>"<a href="stats.html#topic+dist">dist</a>"</code>.</p>
</td></tr>
<tr><td><code id="ls_fit_sum_of_ultrametrics_+3A_nterms">nterms</code></td>
<td>
<p>an integer giving the number of ultrametrics to be
fitted.</p>
</td></tr>
<tr><td><code id="ls_fit_sum_of_ultrametrics_+3A_weights">weights</code></td>
<td>
<p>a numeric vector or matrix with non-negative weights
for obtaining a weighted least squares fit.  If a matrix, its
numbers of rows and columns must be the same as the number of
objects in <code>x</code>, and the lower diagonal part is used.
Otherwise, it is recycled to the number of elements in <code>x</code>.</p>
</td></tr>
<tr><td><code id="ls_fit_sum_of_ultrametrics_+3A_control">control</code></td>
<td>
<p>a list of control parameters.  See <b>Details</b>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The problem to be solved is minimizing the criterion function
</p>
<p style="text-align: center;"><code class="reqn">L(u(1), \dots, u(n)) =
    \sum_{i,j} w_{ij} \left(x_{ij} - \sum_{k=1}^n u_{ij}(k)\right)^2</code>
</p>

<p>over all <code class="reqn">u(1), \ldots, u(n)</code> satisfying the ultrametric
constraints.
</p>
<p>We provide an implementation of the iterative heuristic suggested in
Carroll &amp; Pruzansky (1980) which in each step <code class="reqn">t</code> sequentially
refits the <code class="reqn">u(k)</code> as the least squares ultrametric fit to the
&ldquo;residuals&rdquo; <code class="reqn">x - \sum_{l \ne k} u(l)</code> using
<code><a href="#topic+ls_fit_ultrametric">ls_fit_ultrametric</a></code>.
</p>
<p>Available control parameters include
</p>

<dl>
<dt><code>maxiter</code></dt><dd><p>an integer giving the maximal number of
iteration steps to be performed.
Defaults to 100.</p>
</dd>
<dt><code>eps</code></dt><dd><p>a nonnegative number controlling the iteration,
which stops when the maximal change in all <code class="reqn">u(k)</code> is less than
<code>eps</code>.
Defaults to <code class="reqn">10^{-6}</code>.</p>
</dd>
<dt><code>reltol</code></dt><dd><p>the relative convergence tolerance.  Iteration
stops when the relative change in the criterion function is less
than <code>reltol</code>.
Defaults to <code class="reqn">10^{-6}</code>.</p>
</dd>
<dt><code>method</code></dt><dd><p>a character string indicating the fitting
method to be employed by the individual least squares fits.</p>
</dd>
<dt><code>control</code></dt><dd><p>a list of control parameters to be used by the
method of <code><a href="#topic+ls_fit_ultrametric">ls_fit_ultrametric</a></code> employed.  By default,
if the <abbr><span class="acronym">SUMT</span></abbr> method method is used, 10 inner
<abbr><span class="acronym">SUMT</span></abbr> runs are performed for each refitting.</p>
</dd>
</dl>

<p>It should be noted that the method used is a heuristic which can not
be guaranteed to find the global minimum.
</p>


<h3>Value</h3>

<p>A list of objects of class <code>"<a href="#topic+cl_ultrametric">cl_ultrametric</a>"</code> containing
the fitted ultrametric distances.
</p>


<h3>References</h3>

<p>J. D. Carroll and S. Pruzansky (1980).
Discrete and hybrid scaling models.
In E. D. Lantermann and H. Feger (eds.), <em>Similarity and Choice</em>.
Bern (Switzerland): Huber.
</p>

<hr>
<h2 id='ls_fit_ultrametric'>Least Squares Fit of Ultrametrics to Dissimilarities</h2><span id='topic+ls_fit_ultrametric'></span>

<h3>Description</h3>

<p>Find the ultrametric with minimal square distance (Euclidean
dissimilarity) to given dissimilarity objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ls_fit_ultrametric(x, method = c("SUMT", "IP", "IR"), weights = 1,
                   control = list())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ls_fit_ultrametric_+3A_x">x</code></td>
<td>
<p>a dissimilarity object inheriting from or coercible to class
<code>"<a href="stats.html#topic+dist">dist</a>"</code>, or an ensemble of such objects.</p>
</td></tr> 
<tr><td><code id="ls_fit_ultrametric_+3A_method">method</code></td>
<td>
<p>a character string indicating the fitting method to be
employed.  Must be one of <code>"SUMT"</code> (default), <code>"IP"</code>, or
<code>"IR"</code>, or a unique abbreviation thereof.</p>
</td></tr>
<tr><td><code id="ls_fit_ultrametric_+3A_weights">weights</code></td>
<td>
<p>a numeric vector or matrix with non-negative weights
for obtaining a weighted least squares fit.  If a matrix, its
numbers of rows and columns must be the same as the number of
objects in <code>x</code>, and the lower diagonal part is used.
Otherwise, it is recycled to the number of elements in <code>x</code>.</p>
</td></tr>
<tr><td><code id="ls_fit_ultrametric_+3A_control">control</code></td>
<td>
<p>a list of control parameters.  See <b>Details</b>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For a single dissimilarity object <code>x</code>, the problem to be solved
is minimizing
</p>
<p style="text-align: center;"><code class="reqn">L(u) = \sum_{i,j} w_{ij} (x_{ij} - u_{ij})^2</code>
</p>

<p>over all <code class="reqn">u</code> satisfying the ultrametric constraints (i.e., for all
<code class="reqn">i, j, k</code>, <code class="reqn">u_{ij} \le \max(u_{ik}, u_{jk})</code>).  This problem
is known to be NP hard (Krivanek and Moravek, 1986).
</p>
<p>For an ensemble of dissimilarity objects, the criterion function is
</p>
<p style="text-align: center;"><code class="reqn">L(u) = \sum_b w_b \sum_{i,j} w_{ij} (x_{ij}(b) - u_{ij})^2,</code>
</p>

<p>where <code class="reqn">w_b</code> is the weight given to element <code class="reqn">x_b</code> of the
ensemble and can be specified via control parameter <code>weights</code>
(default: all ones).  This problem reduces to the above basic problem
with <code class="reqn">x</code> as the <code class="reqn">w_b</code>-weighted mean of the <code class="reqn">x_b</code>.
</p>
<p>We provide three heuristics for solving the basic problem.
</p>
<p>Method <code>"SUMT"</code> implements the <abbr><span class="acronym">SUMT</span></abbr> (Sequential
Unconstrained Minimization Technique, Fiacco and McCormick, 1968)
approach of de Soete (1986) which in turn simplifies the suggestions
in Carroll and Pruzansky (1980).  (See <code><a href="#topic+sumt">sumt</a></code> for more
information on the <abbr><span class="acronym">SUMT</span></abbr> approach.)  We then use a final
single linkage hierarchical clustering step to ensure that the
returned object exactly satisfies the ultrametric constraints.  The
starting value <code class="reqn">u_0</code> is obtained by &ldquo;random shaking&rdquo; of the
given dissimilarity object (if not given).  If there are missing
values in <code>x</code>, i.e., the given dissimilarities are
<em>incomplete</em>, we follow a suggestion of de Soete (1984), imputing
the missing values by the weighted mean of the non-missing ones, and
setting the corresponding weights to zero.
</p>
<p>Available control parameters are <code>method</code>, <code>control</code>,
<code>eps</code>, <code>q</code>, and <code>verbose</code>, which have the same roles as
for <code><a href="#topic+sumt">sumt</a></code>, and the following.
</p>

<dl>
<dt><code>nruns</code></dt><dd><p>an integer giving the number of runs to be
performed.  Defaults to 1.</p>
</dd>
<dt><code>start</code></dt><dd><p>a single dissimilarity, or a list of
dissimilarities to be employed as starting values.</p>
</dd>
</dl>

<p>The default optimization using conjugate gradients should work
reasonably well for medium to large size problems.  For &ldquo;small&rdquo;
ones, using <code>nlm</code> is usually faster.  Note that the number of
ultrametric constraints is of the order <code class="reqn">n^3</code>, where <code class="reqn">n</code> is
the number of objects in the dissimilarity object, suggesting to use
the <abbr><span class="acronym">SUMT</span></abbr> approach in favor of
<code><a href="stats.html#topic+constrOptim">constrOptim</a></code>.
</p>
<p>If starting values for the <abbr><span class="acronym">SUMT</span></abbr> are provided via
<code>start</code>, the number of starting values gives the number of runs
to be performed, and control option <code>nruns</code> is ignored.
Otherwise, <code>nruns</code> starting values are obtained by random shaking
of the dissimilarity to be fitted.  In the case of multiple
<abbr><span class="acronym">SUMT</span></abbr> runs, the (first) best solution found is returned.
</p>
<p>Method <code>"IP"</code> implements the Iterative Projection approach of
Hubert and Arabie (1995).  This iteratively projects the current
dissimilarities to the closed convex set given by the ultrametric
constraints (3-point conditions) for a single index triple <code class="reqn">(i, j,
    k)</code>, in fact replacing the two largest values among <code class="reqn">d_{ij},
    d_{ik}, d_{jk}</code> by their mean.  The following control parameters can
be provided via the <code>control</code> argument.
</p>

<dl>
<dt><code>nruns</code></dt><dd><p>an integer giving the number of runs to be
performed.  Defaults to 1.</p>
</dd>
<dt><code>order</code></dt><dd><p>a permutation of the numbers from 1 to the
number of objects in <code>x</code>, specifying the order in which the
ultrametric constraints are considered, or a list of such
permutations.</p>
</dd>
<dt><code>maxiter</code></dt><dd><p>an integer giving the maximal number of
iterations to be employed.</p>
</dd>
<dt><code>tol</code></dt><dd><p>a double indicating the maximal convergence
tolerance.  The algorithm stops if the total absolute change in
the dissimilarities in an iteration is less than <code>tol</code>.</p>
</dd>
<dt><code>verbose</code></dt><dd><p>a logical indicating whether to provide some
output on minimization progress.  Defaults to
<code>getOption("verbose")</code>.</p>
</dd>
</dl>

<p>If permutations are provided via <code>order</code>, the number of these
gives the number of runs to be performed, and control option
<code>nruns</code> is ignored.  Otherwise, <code>nruns</code> randomly generated
orders are tried.  In the case of multiple runs, the (first) best
solution found is returned.
</p>
<p>Non-identical weights and incomplete dissimilarities are currently not
supported.
</p>
<p>Method <code>"IR"</code> implements the Iterative Reduction approach
suggested by Roux (1988), see also Barthélémy and Guénoche (1991).
This is similar to the Iterative Projection method, but modifies the
dissimilarities between objects proportionally to the aggregated
change incurred from the ultrametric projections.  Available control
parameters are identical to those of method <code>"IP"</code>.
</p>
<p>Non-identical weights and incomplete dissimilarities are currently not
supported.
</p>
<p>It should be noted that all methods are heuristics which can not be
guaranteed to find the global minimum.  Standard practice would
recommend to use the best solution found in &ldquo;sufficiently many&rdquo;
replications of the base algorithm.
</p>


<h3>Value</h3>

<p>An object of class <code>"<a href="#topic+cl_ultrametric">cl_ultrametric</a>"</code> containing the
fitted ultrametric distances.
</p>


<h3>References</h3>

<p>J.-P. Barthélémy and A. Guénoche (1991).
<em>Trees and proximity representations</em>.
Chichester: John Wiley &amp; Sons.
ISBN 0-471-92263-3.
</p>
<p>J. D. Carroll and S. Pruzansky (1980).
Discrete and hybrid scaling models.
In E. D. Lantermann and H. Feger (eds.), <em>Similarity and Choice</em>.
Bern (Switzerland): Huber.
</p>
<p>L. Hubert and P. Arabie (1995).
Iterative projection strategies for the least squares fitting of tree
structures to proximity data.
<em>British Journal of Mathematical and Statistical Psychology</em>,
<b>48</b>, 281&ndash;317.
<a href="https://doi.org/10.1111/j.2044-8317.1995.tb01065.x">doi:10.1111/j.2044-8317.1995.tb01065.x</a>.
</p>
<p>M. Krivanek and J. Moravek (1986).
NP-hard problems in hierarchical tree clustering.
<em>Acta Informatica</em>, <b>23</b>, 311&ndash;323.
<a href="https://doi.org/10.1007/BF00289116">doi:10.1007/BF00289116</a>.
</p>
<p>M. Roux (1988).
Techniques of approximation for building two tree structures.
In C. Hayashi and E. Diday and M. Jambu and N. Ohsumi (Eds.),
<em>Recent Developments in Clustering and Data Analysis</em>, pages
151&ndash;170.
New York: Academic Press.
</p>
<p>G. de Soete (1984).
Ultrametric tree representations of incomplete dissimilarity data.
<em>Journal of Classification</em>, <b>1</b>, 235&ndash;242.
<a href="https://doi.org/10.1007/BF01890124">doi:10.1007/BF01890124</a>.
</p>
<p>G. de Soete (1986).
A least squares algorithm for fitting an ultrametric tree to a
dissimilarity matrix.
<em>Pattern Recognition Letters</em>, <b>2</b>, 133&ndash;137.
<a href="https://doi.org/10.1016/0167-8655%2884%2990036-9">doi:10.1016/0167-8655(84)90036-9</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cl_consensus">cl_consensus</a></code> for computing least squares (Euclidean)
consensus hierarchies by least squares fitting of average ultrametric
distances;
<code><a href="#topic+l1_fit_ultrametric">l1_fit_ultrametric</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Least squares fit of an ultrametric to the Miller-Nicely consonant
## phoneme confusion data.
data("Phonemes")
## Note that the Phonemes data set has the consonant misclassification
## probabilities, i.e., the similarities between the phonemes.
d &lt;- as.dist(1 - Phonemes)
u &lt;- ls_fit_ultrametric(d, control = list(verbose = TRUE))
## Cophenetic correlation:
cor(d, u)
## Plot:
plot(u)
## ("Basically" the same as Figure 1 in de Soete (1986).)
</code></pre>

<hr>
<h2 id='n_of_classes'>Classes in a Partition</h2><span id='topic+n_of_classes'></span><span id='topic+cl_class_ids'></span><span id='topic+as.cl_class_ids'></span>

<h3>Description</h3>

<p>Determine the number of classes and the class ids in a
partition of objects.</p>


<h3>Usage</h3>

<pre><code class='language-R'>n_of_classes(x)
cl_class_ids(x)
as.cl_class_ids(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="n_of_classes_+3A_x">x</code></td>
<td>
<p>an object representing a (hard or soft) partition (for
<code>n_of_classes</code> and <code>cl_class_ids</code>), or raw class ids (for
<code>as.cl_class_ids</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These function are generic functions.
</p>
<p>The methods provided in package <span class="pkg">clue</span> handle the partitions
obtained from clustering functions in the base R distribution, as well
as packages <span class="pkg">RWeka</span>, <span class="pkg">cba</span>, <span class="pkg">cclust</span>, <span class="pkg">cluster</span>,
<span class="pkg">e1071</span>, <span class="pkg">flexclust</span>, <span class="pkg">flexmix</span>, <span class="pkg">kernlab</span>,
<span class="pkg">mclust</span>, <span class="pkg">movMF</span> and <span class="pkg">skmeans</span> (and of course, <span class="pkg">clue</span>
itself).
</p>
<p>Note that the number of classes is taken as the number of distinct
class ids actually used in the partition, and may differ from the
number of columns in a membership matrix representing the partition.
</p>
<p><code>as.cl_class_ids</code> can be used for coercing &ldquo;raw&rdquo; class
ids (given as atomic vectors) to class id objects.
</p>


<h3>Value</h3>

<p>For <code>n_of_classes</code>, an integer giving the number of classes in
the partition.
</p>
<p>For <code>cl_class_ids</code>, a vector of integers with the corresponding
class ids.  For soft partitions, the class ids returned are those of
the <em>nearest hard partition</em> obtained by taking the class ids of
the (first) maximal membership values.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+is.cl_partition">is.cl_partition</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Cassini")
party &lt;- kmeans(Cassini$x, 3)
n_of_classes(party)
## A simple confusion matrix:
table(cl_class_ids(party), Cassini$classes)
## For an "oversize" membership matrix representation:
n_of_classes(cl_membership(party, 6))
</code></pre>

<hr>
<h2 id='n_of_objects'>Number of Objects in a Partition or Hierarchy</h2><span id='topic+n_of_objects'></span>

<h3>Description</h3>

<p>Determine the number of objects from which a partition or
hierarchy was obtained.</p>


<h3>Usage</h3>

<pre><code class='language-R'>n_of_objects(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="n_of_objects_+3A_x">x</code></td>
<td>
<p>an <span class="rlang"><b>R</b></span> object representing a (hard of soft) partition or a
hierarchy of objects, or dissimilarities between objects.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a generic function.
</p>
<p>The methods provided in package <span class="pkg">clue</span> handle the partitions and
hierarchies obtained from clustering functions in the base R
distribution, as well as packages <span class="pkg">RWeka</span>, <span class="pkg">ape</span>, <span class="pkg">cba</span>,
<span class="pkg">cclust</span>, <span class="pkg">cluster</span>, <span class="pkg">e1071</span>, <span class="pkg">flexclust</span>,
<span class="pkg">flexmix</span>, <span class="pkg">kernlab</span>, <span class="pkg">mclust</span>, <span class="pkg">movMF</span> and
<span class="pkg">skmeans</span> (and of course, <span class="pkg">clue</span> itself).
</p>
<p>There is also a method for object dissimilarities which inherit from
class <code>"<a href="stats.html#topic+dist">dist</a>"</code>.
</p>


<h3>Value</h3>

<p>An integer giving the number of objects.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+is.cl_partition">is.cl_partition</a></code>,
<code><a href="#topic+is.cl_hierarchy">is.cl_hierarchy</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Cassini")
pcl &lt;- kmeans(Cassini$x, 3)
n_of_objects(pcl)
hcl &lt;- hclust(dist(USArrests))
n_of_objects(hcl)
</code></pre>

<hr>
<h2 id='partition'>Partitions</h2><span id='topic+cl_partition'></span><span id='topic+is.cl_partition'></span><span id='topic+is.cl_hard_partition'></span><span id='topic+is.cl_soft_partition'></span><span id='topic+cl_hard_partition'></span><span id='topic+as.cl_partition'></span><span id='topic+as.cl_hard_partition'></span>

<h3>Description</h3>

<p>Determine whether an R object represents a partition of objects, or
coerce to an R object representing such.</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.cl_partition(x)
is.cl_hard_partition(x)
is.cl_soft_partition(x)

as.cl_partition(x)
as.cl_hard_partition(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="partition_+3A_x">x</code></td>
<td>
<p>an R object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>is.cl_partition</code> and <code>is.cl_hard_partition</code> are generic
functions.
</p>
<p>The methods provided in package <span class="pkg">clue</span> handle the partitions
obtained from clustering functions in the base R distribution, as well
as packages <span class="pkg">RWeka</span>, <span class="pkg">cba</span>, <span class="pkg">cclust</span>, <span class="pkg">cluster</span>,
<span class="pkg">e1071</span>, <span class="pkg">flexclust</span>, <span class="pkg">flexmix</span>, <span class="pkg">kernlab</span>,
<span class="pkg">mclust</span>, <span class="pkg">movMF</span> and <span class="pkg">skmeans</span> (and of course, <span class="pkg">clue</span>
itself).
</p>
<p><code>is.cl_soft_partition</code> gives true iff <code>is.cl_partition</code> is
true and <code>is.cl_hard_partition</code> is false.
</p>
<p><code>as.cl_partition</code> returns an object of class
<code>"cl_partition"</code> &ldquo;containing&rdquo; the given object <code>x</code> if
this already represents a partition (i.e., <code>is.cl_partition(x)</code>
is true), or the memberships obtained from <code>x</code> via
<code><a href="#topic+as.cl_membership">as.cl_membership</a></code>.
</p>
<p><code>as.cl_hard_partition(x)</code> returns an object which has class
<code>"cl_hard_partition"</code> and inherits from <code>"cl_partition"</code>,
and contains <code>x</code> if it already represents a hard partition (i.e.,
provided that <code>is.cl_hard_partition(x)</code> is true), or the class
ids obtained from <code>x</code>, using <code>x</code> if this is an atomic vector
of raw class ids, or, if <code>x</code> represents a soft partition or is a
raw matrix of membership values, using the class ids of the
<em>nearest hard partition</em>, defined by taking the class ids of the
(first) maximal membership values.
</p>
<p>Conceptually, partitions and hard partitions are <em>virtual</em>
classes, allowing for a variety of representations.
</p>
<p>There are group methods for comparing partitions and computing their
minimum, maximum, and range based on the meet and join operations, see
<code><a href="#topic+cl_meet">cl_meet</a></code>.
</p>


<h3>Value</h3>

<p>For the testing functions, a logical indicating whether the given
object represents a clustering of objects of the respective kind.
</p>
<p>For the coercion functions, a container object inheriting from
<code>"cl_partition"</code>, with a suitable representation of the partition
given by <code>x</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Cassini")
pcl &lt;- kmeans(Cassini$x, 3)
is.cl_partition(pcl)
is.cl_hard_partition(pcl)
is.cl_soft_partition(pcl)
</code></pre>

<hr>
<h2 id='pclust'>Prototype-Based Partitioning</h2><span id='topic+pclust'></span><span id='topic+pclust_family'></span><span id='topic+pclust_object'></span>

<h3>Description</h3>

<p>Obtain prototype-based partitions of objects by minimizing the criterion
<code class="reqn">\sum w_b u_{bj}^m d(x_b, p_j)^e</code>, the sum of the case-weighted and
membership-weighted <code class="reqn">e</code>-th powers of the dissimilarities between
the objects <code class="reqn">x_b</code> and the prototypes <code class="reqn">p_j</code>, for suitable
dissimilarities <code class="reqn">d</code> and exponents <code class="reqn">e</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pclust(x, k, family, m = 1, weights = 1, control = list())
pclust_family(D, C, init = NULL, description = NULL, e = 1,
              .modify = NULL, .subset = NULL)
pclust_object(prototypes, membership, cluster, family, m = 1,
              value, ..., classes = NULL, attributes = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pclust_+3A_x">x</code></td>
<td>
<p>the object to be partitioned.</p>
</td></tr>
<tr><td><code id="pclust_+3A_k">k</code></td>
<td>
<p>an integer giving the number of classes to be used in the
partition.</p>
</td></tr>
<tr><td><code id="pclust_+3A_family">family</code></td>
<td>
<p>an object of class <code>"pclust_family"</code> as generated
by <code>pclust_family</code>, containing the information about <code class="reqn">d</code>
and <code class="reqn">e</code>.</p>
</td></tr>
<tr><td><code id="pclust_+3A_m">m</code></td>
<td>
<p>a number not less than 1 controlling the softness of the
partition (as the &ldquo;fuzzification parameter&rdquo; of the fuzzy
<code class="reqn">c</code>-means algorithm).  The default value of 1 corresponds to
hard partitions obtained from a generalized <code class="reqn">k</code>-means problem;
values greater than one give partitions of increasing softness
obtained from a generalized fuzzy <code class="reqn">c</code>-means problem.</p>
</td></tr>
<tr><td><code id="pclust_+3A_weights">weights</code></td>
<td>
<p>a numeric vector of non-negative case weights.
Recycled to the number of elements given by <code>x</code> if necessary.</p>
</td></tr>
<tr><td><code id="pclust_+3A_control">control</code></td>
<td>
<p>a list of control parameters.  See <b>Details</b>.</p>
</td></tr>
<tr><td><code id="pclust_+3A_d">D</code></td>
<td>
<p>a function for computing dissimilarities <code class="reqn">d</code> between
objects and prototypes.</p>
</td></tr>
<tr><td><code id="pclust_+3A_c">C</code></td>
<td>
<p>a &lsquo;consensus&rsquo; function with formals <code>x</code>,
<code>weights</code> and <code>control</code> for computing a consensus
prototype <code class="reqn">p</code> minimizing <code class="reqn">\sum_b w_b d(x_b, p) ^ e</code>.</p>
</td></tr>
<tr><td><code id="pclust_+3A_init">init</code></td>
<td>
<p>a function with formals <code>x</code> and <code>k</code> initializing
an object with <code class="reqn">k</code> prototypes from the object <code>x</code> to be
partitioned.</p>
</td></tr>
<tr><td><code id="pclust_+3A_description">description</code></td>
<td>
<p>a character string describing the family.</p>
</td></tr>
<tr><td><code id="pclust_+3A_e">e</code></td>
<td>
<p>a number giving the exponent <code class="reqn">e</code> of the criterion.</p>
</td></tr>
<tr><td><code id="pclust_+3A_.modify">.modify</code></td>
<td>
<p>a function with formals <code>x</code>, <code>i</code> and
<code>value</code> for modifying a single prototype,
or <code>NULL</code> (default).</p>
</td></tr>
<tr><td><code id="pclust_+3A_.subset">.subset</code></td>
<td>
<p>a function with formals <code>x</code> and <code>i</code> for
subsetting prototypes,
or <code>NULL</code> (default).</p>
</td></tr>
<tr><td><code id="pclust_+3A_prototypes">prototypes</code></td>
<td>
<p>an object representing the prototypes of the
partition.</p>
</td></tr>
<tr><td><code id="pclust_+3A_membership">membership</code></td>
<td>
<p>an object of class <code>"<a href="#topic+cl_membership">cl_membership</a>"</code>
with the membership values <code class="reqn">u_{bj}</code>.</p>
</td></tr>
<tr><td><code id="pclust_+3A_cluster">cluster</code></td>
<td>
<p>the class ids of the nearest hard partition.</p>
</td></tr>
<tr><td><code id="pclust_+3A_value">value</code></td>
<td>
<p>the value of the criterion to be minimized.</p>
</td></tr>
<tr><td><code id="pclust_+3A_...">...</code></td>
<td>
<p>further elements to be included in the generated pclust
object.</p>
</td></tr>
<tr><td><code id="pclust_+3A_classes">classes</code></td>
<td>
<p>a character vector giving further classes to be given
to the generated pclust object in addition to <code>"pclust"</code>, or
<code>NULL</code> (default).</p>
</td></tr>
<tr><td><code id="pclust_+3A_attributes">attributes</code></td>
<td>
<p>a list of attributes, or <code>NULL</code> (default).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code class="reqn">m = 1</code>, a generalization of the Lloyd-Forgy variant of the
<code class="reqn">k</code>-means algorithm is used, which iterates between reclassifying
objects to their closest prototypes (according to the dissimilarities
given by <code>D</code>), and computing new prototypes as the consensus for
the classes (using <code>C</code>).
</p>
<p>For <code class="reqn">m &gt; 1</code>, a generalization of the fuzzy <code class="reqn">c</code>-means recipe
(e.g., Bezdek (1981)) is used, which alternates between computing
optimal memberships for fixed prototypes, and computing new prototypes
as the suitably weighted consensus clusterings for the classes.
</p>
<p>This procedure is repeated until convergence occurs, or the maximal
number of iterations is reached.
</p>
<p>Currently, no local improvement heuristics are provided.
</p>
<p>It is possible to perform several runs of the procedure via control
arguments <code>nruns</code> or <code>start</code> (the default is to perform a
single run), in which case the first partition with the smallest
value of the criterion is returned.
</p>
<p>The dissimilarity and consensus functions as well as the exponent
<code class="reqn">e</code> are specified via <code>family</code>.  In principle, arbitrary
representations of objects to be partitioned and prototypes (which do
not necessarily have to be &ldquo;of the same kind&rdquo;) can be employed.
In addition to <code>D</code> and <code>C</code>, what is needed are means to
obtain an initial collection of <code class="reqn">k</code> prototypes (<code>init</code>), to
modify a single prototype (<code>.modify</code>), and subset the prototypes
(<code>.subset</code>).  By default, list and (currently, only dense) matrix
(with the usual convention that the rows correspond to the objects)
are supported.  Otherwise, the family has to provide the functions
needed. 
</p>
<p>Available control parameters are as follows.
</p>

<dl>
<dt><code>maxiter</code></dt><dd><p>an integer giving the maximal number of
iterations to be performed.
Defaults to 100.</p>
</dd>
<dt><code>nruns</code></dt><dd><p>an integer giving the number of runs to be
performed.
Defaults to 1.</p>
</dd> 
<dt><code>reltol</code></dt><dd><p>the relative convergence tolerance.
Defaults to <code>sqrt(.Machine$double.eps)</code>.</p>
</dd>
<dt><code>start</code></dt><dd><p>a list of prototype objects to be used as
starting values.</p>
</dd>
<dt><code>verbose</code></dt><dd><p>a logical indicating whether to provide
some output on minimization progress.
Defaults to <code>getOption("verbose")</code>.</p>
</dd>
<dt><code>control</code></dt><dd><p>control parameters to be used in the consensus
function.</p>
</dd>
</dl>

<p>The fixed point approach employed is a heuristic which cannot be
guaranteed to find the global minimum, in particular if <code>C</code> is
not an exact minimizer.  Standard practice would recommend to use the
best solution found in &ldquo;sufficiently many&rdquo; replications of the
base algorithm.
</p>


<h3>Value</h3>

<p><code>pclust</code> returns the partition found as an object of class
<code>"pclust"</code> (as obtained by calling <code>pclust_object</code>) which in
addition to the <em>default</em> components contains <code>call</code> (the 
matched call) and a <code>converged</code> attribute indicating convergence
status (i.e., whether the maximal number of iterations was reached).
</p>
<p><code>pclust_family</code> returns an object of class
<code>"pclust_family"</code>, which is a list with components corresponding
to the formals of <code>pclust_family</code>.
</p>
<p><code>pclust_object</code> returns an object inheriting from class
<code>"pclust"</code>, which is a list with components corresponding
to the formals (up to and including <code>...</code>) of
<code>pclust_object</code>, and additional classes and attributes specified
by <code>classes</code> and <code>attributes</code>, respectively.
</p>


<h3>References</h3>

<p>J. C. Bezdek (1981).
<em>Pattern recognition with fuzzy objective function algorithms</em>.
New York: Plenum.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+kmeans">kmeans</a></code>,
<code><a href="e1071.html#topic+cmeans">cmeans</a></code>.
</p>

<hr>
<h2 id='Phonemes'>Miller-Nicely Consonant Phoneme Confusion Data</h2><span id='topic+Phonemes'></span>

<h3>Description</h3>

<p>Miller-Nicely data on the auditory confusion of 16 consonant
phonemes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("Phonemes")</code></pre>


<h3>Format</h3>

<p>A symmetric matrix of the misclassification probabilities of 16
English consonant phonemes.
</p>


<h3>Details</h3>

<p>Miller and Nicely (1955) obtained the confusions by exposing female
subjects to a series of syllables consisting of one of the 16
consonants followed by the vowel &lsquo;<span class="samp">&#8288;a&#8288;</span>&rsquo; under 17 different
experimental conditions.  The data provided are obtained from
aggregating the six so-called flat-noise conditions in which only the
speech-to-noise ratio was varied into a single matrix of
misclassification frequencies.
</p>


<h3>Source</h3>

<p>The data set is also contained in file &lsquo;<span class="file">mapclus.data</span>&rsquo; in the
shell archive <a href="https://netlib.org/mds/mapclus.shar">https://netlib.org/mds/mapclus.shar</a>.
</p>


<h3>References</h3>

<p>G. A. Miller and P. E. Nicely (1955).
An analysis of perceptual confusions among some English consonants.
<em>Journal of the Acoustical Society of America</em>, <b>27</b>,
338&ndash;352.
<a href="https://doi.org/10.1121/1.1907526">doi:10.1121/1.1907526</a>.
</p>

<hr>
<h2 id='solve_LSAP'>Solve Linear Sum Assignment Problem</h2><span id='topic+solve_LSAP'></span>

<h3>Description</h3>

<p>Solve the linear sum assignment problem using the Hungarian method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>solve_LSAP(x, maximum = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="solve_LSAP_+3A_x">x</code></td>
<td>
<p>a matrix with nonnegative entries and at least as many
columns as rows.</p>
</td></tr>
<tr><td><code id="solve_LSAP_+3A_maximum">maximum</code></td>
<td>
<p>a logical indicating whether to minimize of maximize
the sum of assigned costs.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code class="reqn">nr</code> and <code class="reqn">nc</code> are the numbers of rows and columns of
<code>x</code>, <code>solve_LSAP</code> finds an optimal <em>assignment</em> of rows
to columns, i.e., a one-to-one map <code>p</code> of the numbers from 1 to
<code class="reqn">nr</code> to the numbers from 1 to <code class="reqn">nc</code> (a permutation of these
numbers in case <code>x</code> is a square matrix) such that
<code class="reqn">\sum_{i=1}^{nr} x[i, p[i]]</code> is minimized or maximized.
</p>
<p>This assignment can be found using a linear program (and package
<span class="pkg">lpSolve</span> provides a function <code>lp.assign</code> for doing so), but
typically more efficiently and provably in polynomial time
<code class="reqn">O(n^3)</code> using primal-dual methods such as the so-called Hungarian
method (see the references).
</p>


<h3>Value</h3>

<p>An object of class <code>"solve_LSAP"</code> with the optimal assignment of
rows to columns.
</p>


<h3>Author(s)</h3>

<p>Walter Böhm <a href="mailto:Walter.Boehm@wu.ac.at">Walter.Boehm@wu.ac.at</a> kindly provided C code
implementing the Hungarian method.
</p>


<h3>References</h3>

<p>C. Papadimitriou and K. Steiglitz (1982),
<em>Combinatorial Optimization: Algorithms and Complexity</em>.
Englewood Cliffs: Prentice Hall.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix(c(5, 1, 4, 3, 5, 2, 2, 4, 4), nrow = 3)
solve_LSAP(x)
solve_LSAP(x, maximum = TRUE)
## To get the optimal value (for now):
y &lt;- solve_LSAP(x)
sum(x[cbind(seq_along(y), y)])
</code></pre>

<hr>
<h2 id='sumt'>Sequential Unconstrained Minimization Technique</h2><span id='topic+sumt'></span>

<h3>Description</h3>

<p>Solve constrained optimization problems via the Sequential
Unconstrained Minimization Technique (<abbr><span class="acronym">SUMT</span></abbr>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sumt(x0, L, P, grad_L = NULL, grad_P = NULL, method = NULL,
     eps = NULL, q = NULL, verbose = NULL, control = list())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sumt_+3A_x0">x0</code></td>
<td>
<p>a list of starting values, or a single starting value.</p>
</td></tr>
<tr><td><code id="sumt_+3A_l">L</code></td>
<td>
<p>a function to minimize.</p>
</td></tr>
<tr><td><code id="sumt_+3A_p">P</code></td>
<td>
<p>a non-negative penalty function such that <code class="reqn">P(x)</code> is zero
iff the constraints are satisfied.</p>
</td></tr>
<tr><td><code id="sumt_+3A_grad_l">grad_L</code></td>
<td>
<p>a function giving the gradient of <code>L</code>, or
<code>NULL</code> (default).</p>
</td></tr>
<tr><td><code id="sumt_+3A_grad_p">grad_P</code></td>
<td>
<p>a function giving the gradient of <code>P</code>, or
<code>NULL</code> (default).</p>
</td></tr>
<tr><td><code id="sumt_+3A_method">method</code></td>
<td>
<p>a character string, or <code>NULL</code>.  If not given,
<code>"CG"</code> is used.  If equal to <code>"nlm"</code>, minimization is
carried out using <code><a href="stats.html#topic+nlm">nlm</a></code>.  Otherwise,
<code><a href="stats.html#topic+optim">optim</a></code> is used with <code>method</code> as the given
method.</p>
</td></tr>
<tr><td><code id="sumt_+3A_eps">eps</code></td>
<td>
<p>the absolute convergence tolerance.  The algorithm stops if
the (maximum) distance between successive <code>x</code> values is
less than <code>eps</code>.
</p>
<p>Defaults to <code>sqrt(.Machine$double.eps)</code>.</p>
</td></tr>
<tr><td><code id="sumt_+3A_q">q</code></td>
<td>
<p>a double greater than one controlling the growth of the
<code class="reqn">\rho_k</code> as described in <b>Details</b>.
</p>
<p>Defaults to 10.</p>
</td></tr>
<tr><td><code id="sumt_+3A_verbose">verbose</code></td>
<td>
<p>a logical indicating whether to provide some output on
minimization progress.
</p>
<p>Defaults to <code>getOption("verbose")</code>.</p>
</td></tr>
<tr><td><code id="sumt_+3A_control">control</code></td>
<td>
<p>a list of control parameters to be passed to the
minimization routine in case <code>optim</code> is used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Sequential Unconstrained Minimization Technique is a heuristic for
constrained optimization.  To minimize a function <code class="reqn">L</code> subject to
constraints, one employs a non-negative function <code class="reqn">P</code> penalizing
violations of the constraints, such that <code class="reqn">P(x)</code> is zero iff <code class="reqn">x</code>
satisfies the constraints.  One iteratively minimizes <code class="reqn">L(x) +
    \rho_k P(x)</code>, where the <code class="reqn">\rho</code> values are increased according to
the rule <code class="reqn">\rho_{k+1} = q \rho_k</code> for some constant <code class="reqn">q &gt; 1</code>,
until convergence is obtained in the sense that the Euclidean distance
between successive solutions <code class="reqn">x_k</code> and <code class="reqn">x_{k+1}</code> is small
enough.  Note that the &ldquo;solution&rdquo; <code class="reqn">x</code> obtained does not
necessarily satisfy the constraints, i.e., has zero <code class="reqn">P(x)</code>.  Note
also that there is no guarantee that global (approximately)
constrained optima are found.  Standard practice would recommend to
use the best solution found in &ldquo;sufficiently many&rdquo; replications
of the algorithm.
</p>
<p>The unconstrained minimizations are carried out by either
<code><a href="stats.html#topic+optim">optim</a></code> or <code><a href="stats.html#topic+nlm">nlm</a></code>, using analytic
gradients if both <code>grad_L</code> and <code>grad_P</code> are given, and
numeric ones otherwise.
</p>
<p>If more than one starting value is given, the solution with the
minimal augmented criterion function value is returned.
</p>


<h3>Value</h3>

<p>A list inheriting from class <code>"sumt"</code>, with components <code>x</code>,
<code>L</code>, <code>P</code>, and <code>rho</code> giving the solution obtained, the
value of the criterion and penalty function at <code>x</code>, and the final
<code class="reqn">\rho</code> value used in the augmented criterion function. 
</p>


<h3>References</h3>

<p>A. V. Fiacco and G. P. McCormick (1968).
<em>Nonlinear programming: Sequential unconstrained minimization
techniques</em>.
New York: John Wiley &amp; Sons.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
