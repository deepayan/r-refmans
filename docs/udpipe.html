<!DOCTYPE html><html lang="en"><head><title>Help for package udpipe</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {udpipe}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#as_conllu'><p>Convert a data.frame to CONLL-U format</p></a></li>
<li><a href='#as_cooccurrence'><p>Convert a matrix to a co-occurrence data.frame</p></a></li>
<li><a href='#as_fasttext'><p>Combine labels and text as used in fasttext</p></a></li>
<li><a href='#as_phrasemachine'><p>Convert Parts of Speech tags to one-letter tags which can be used to identify phrases based on regular expressions</p></a></li>
<li><a href='#as_word2vec'><p>Convert a matrix of word vectors to word2vec format</p></a></li>
<li><a href='#as.data.frame.udpipe_connlu'><p>Convert the result of udpipe_annotate to a tidy data frame</p></a></li>
<li><a href='#as.matrix.cooccurrence'><p>Convert the result of cooccurrence to a sparse matrix</p></a></li>
<li><a href='#brussels_listings'><p>Brussels AirBnB address locations available at www.insideairbnb.com</p></a></li>
<li><a href='#brussels_reviews'><p>Reviews of AirBnB customers on Brussels address locations available at www.insideairbnb.com</p></a></li>
<li><a href='#brussels_reviews_anno'><p>Reviews of the AirBnB customers which are tokenised, POS tagged and lemmatised</p></a></li>
<li><a href='#brussels_reviews_w2v_embeddings_lemma_nl'><p>An example matrix of word embeddings</p></a></li>
<li><a href='#cbind_dependencies'><p>Add the dependency parsing information to an annotated dataset</p></a></li>
<li><a href='#cbind_morphological'><p>Add morphological features to an annotated dataset</p></a></li>
<li><a href='#cooccurrence'><p>Create a cooccurence data.frame</p></a></li>
<li><a href='#document_term_frequencies'><p>Aggregate a data.frame to the document/term level by calculating how many times a term occurs per document</p></a></li>
<li><a href='#document_term_frequencies_statistics'><p>Add Term Frequency, Inverse Document Frequency and Okapi BM25 statistics to the output of document_term_frequencies</p></a></li>
<li><a href='#document_term_matrix'><p>Create a document/term matrix</p></a></li>
<li><a href='#dtm_align'><p>Reorder a Document-Term-Matrix alongside a vector or data.frame</p></a></li>
<li><a href='#dtm_bind'><p>Combine 2 document term matrices either by rows or by columns</p></a></li>
<li><a href='#dtm_chisq'><p>Compare term usage across 2 document groups using the Chi-square Test for Count Data</p></a></li>
<li><a href='#dtm_colsums'><p>Column sums and Row sums for document term matrices</p></a></li>
<li><a href='#dtm_conform'><p>Make sure a document term matrix has exactly the specified rows and columns</p></a></li>
<li><a href='#dtm_cor'><p>Pearson Correlation for Sparse Matrices</p></a></li>
<li><a href='#dtm_remove_lowfreq'><p>Remove terms occurring with low frequency from a Document-Term-Matrix and documents with no terms</p></a></li>
<li><a href='#dtm_remove_sparseterms'><p>Remove terms with high sparsity from a Document-Term-Matrix</p></a></li>
<li><a href='#dtm_remove_terms'><p>Remove terms from a Document-Term-Matrix and keep only documents which have a least some terms</p></a></li>
<li><a href='#dtm_remove_tfidf'><p>Remove terms from a Document-Term-Matrix and documents with no terms based on the term frequency inverse document frequency</p></a></li>
<li><a href='#dtm_reverse'><p>Inverse operation of the document_term_matrix function</p></a></li>
<li><a href='#dtm_sample'><p>Random samples and permutations from a Document-Term-Matrix</p></a></li>
<li><a href='#dtm_svd_similarity'><p>Semantic Similarity to a Singular Value Decomposition</p></a></li>
<li><a href='#dtm_tfidf'><p>Term Frequency - Inverse Document Frequency calculation</p></a></li>
<li><a href='#keywords_collocation'><p>Extract collocations - a sequence of terms which follow each other</p></a></li>
<li><a href='#keywords_phrases'><p>Extract phrases - a sequence of terms which follow each other based on a sequence of Parts of Speech tags</p></a></li>
<li><a href='#keywords_rake'><p>Keyword identification using Rapid Automatic Keyword Extraction (RAKE)</p></a></li>
<li><a href='#paste.data.frame'><p>Concatenate text of each group of data together</p></a></li>
<li><a href='#predict.LDA_VEM'><p>Predict method for an object of class LDA_VEM or class LDA_Gibbs</p></a></li>
<li><a href='#strsplit.data.frame'><p>Obtain a tokenised data frame by splitting text alongside a regular expression</p></a></li>
<li><a href='#syntaxpatterns-class'><p>Experimental and undocumented querying of syntax patterns</p></a></li>
<li><a href='#syntaxrelation-class'><p>Experimental and undocumented querying of syntax relationships</p></a></li>
<li><a href='#txt_collapse'><p>Collapse a character vector while removing missing data.</p></a></li>
<li><a href='#txt_contains'><p>Check if text contains a certain pattern</p></a></li>
<li><a href='#txt_context'><p>Based on a vector with a word sequence, get n-grams (looking forward + backward)</p></a></li>
<li><a href='#txt_count'><p>Count the number of times a pattern is occurring in text</p></a></li>
<li><a href='#txt_freq'><p>Frequency statistics of elements in a vector</p></a></li>
<li><a href='#txt_grepl'><p>Look up a multiple patterns and indicate their presence in text</p></a></li>
<li><a href='#txt_highlight'><p>Highlight words in a character vector</p></a></li>
<li><a href='#txt_next'><p>Get the n-th next element of a vector</p></a></li>
<li><a href='#txt_nextgram'><p>Based on a vector with a word sequence, get n-grams (looking forward)</p></a></li>
<li><a href='#txt_overlap'><p>Get the overlap between 2 vectors</p></a></li>
<li><a href='#txt_paste'><p>Concatenate strings with options how to handle missing data</p></a></li>
<li><a href='#txt_previous'><p>Get the n-th previous element of a vector</p></a></li>
<li><a href='#txt_previousgram'><p>Based on a vector with a word sequence, get n-grams (looking backward)</p></a></li>
<li><a href='#txt_recode'><p>Recode text to other categories</p></a></li>
<li><a href='#txt_recode_ngram'><p>Recode words with compound multi-word expressions</p></a></li>
<li><a href='#txt_sample'><p>Boilerplate function to sample one element from a vector.</p></a></li>
<li><a href='#txt_sentiment'><p>Perform dictionary-based sentiment analysis on a tokenised data frame</p></a></li>
<li><a href='#txt_show'><p>Boilerplate function to cat only 1 element of a character vector.</p></a></li>
<li><a href='#txt_tagsequence'><p>Identify a contiguous sequence of tags as 1 being entity</p></a></li>
<li><a href='#udpipe'><p>Tokenising, Lemmatising, Tagging and Dependency Parsing of raw text in TIF format</p></a></li>
<li><a href='#udpipe_accuracy'><p>Evaluate the accuracy of your UDPipe model on holdout data</p></a></li>
<li><a href='#udpipe_annotate'><p>Tokenising, Lemmatising, Tagging and Dependency Parsing Annotation of raw text</p></a></li>
<li><a href='#udpipe_annotation_params'><p>List with training options set by the UDPipe community when building models based on the Universal Dependencies data</p></a></li>
<li><a href='#udpipe_download_model'><p>Download an UDPipe model provided by the UDPipe community for a specific language of choice</p></a></li>
<li><a href='#udpipe_load_model'><p>Load an UDPipe model</p></a></li>
<li><a href='#udpipe_read_conllu'><p>Read in a CONLL-U file as a data.frame</p></a></li>
<li><a href='#udpipe_train'><p>Train a UDPipe model</p></a></li>
<li><a href='#unique_identifier'><p>Create a unique identifier for each combination of fields in a data frame</p></a></li>
<li><a href='#unlist_tokens'><p>Create a data.frame from a list of tokens</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Tokenization, Parts of Speech Tagging, Lemmatization and
Dependency Parsing with the 'UDPipe' 'NLP' Toolkit</td>
</tr>
<tr>
<td>Version:</td>
<td>0.8.11</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jan Wijffels &lt;jwijffels@bnosac.be&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>This natural language processing toolkit provides language-agnostic
    'tokenization', 'parts of speech tagging', 'lemmatization' and 'dependency
    parsing' of raw text. Next to text parsing, the package also allows you to train
    annotation models based on data of 'treebanks' in 'CoNLL-U' format as provided
    at <a href="https://universaldependencies.org/format.html">https://universaldependencies.org/format.html</a>. The techniques are explained
    in detail in the paper: 'Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0
    with UDPipe', available at &lt;<a href="https://doi.org/10.18653%2Fv1%2FK17-3009">doi:10.18653/v1/K17-3009</a>&gt;. 
    The toolkit also contains functionalities for commonly used data manipulations on texts 
    which are enriched with the output of the parser. Namely functionalities and algorithms 
    for collocations, token co-occurrence, document term matrix handling, 
    term frequency inverse document frequency calculations,
    information retrieval metrics (Okapi BM25), handling of multi-word expressions,
    keyword detection (Rapid Automatic Keyword Extraction, noun phrase extraction, syntactical patterns) 
    sentiment scoring and semantic similarity analysis.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.mozilla.org/en-US/MPL/2.0/">MPL-2.0</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://bnosac.github.io/udpipe/en/index.html">https://bnosac.github.io/udpipe/en/index.html</a>,
<a href="https://github.com/bnosac/udpipe">https://github.com/bnosac/udpipe</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.11.5), data.table (&ge; 1.9.6), Matrix, methods,
stats</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, topicmodels, lattice, parallel</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-01-04 16:20:39 UTC; jwijffels</td>
</tr>
<tr>
<td>Author:</td>
<td>Jan Wijffels [aut, cre, cph],
  BNOSAC [cph],
  Institute of Formal and Applied Linguistics, Faculty of Mathematics and
    Physics, Charles University in Prague, Czech Republic [cph],
  Milan Straka [ctb, cph],
  Jana Strakov√° [ctb, cph]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-01-06 11:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='as_conllu'>Convert a data.frame to CONLL-U format</h2><span id='topic+as_conllu'></span>

<h3>Description</h3>

<p>If you have a data.frame with annotations containing 1 row per token, you can convert it to CONLL-U format with this function.
The data frame is required to have the following columns: doc_id, sentence_id, sentence, token_id, token and
optionally has the following columns: lemma, upos, xpos, feats, head_token_id, dep_rel, deps, misc. Where these fields
have the following meaning
</p>

<ul>
<li><p> doc_id: the identifier of the document
</p>
</li>
<li><p> sentence_id: the identifier of the sentence
</p>
</li>
<li><p> sentence: the text of the sentence for which this token is part of
</p>
</li>
<li><p> token_id: Word index, integer starting at 1 for each new sentence; may be a range for multiword tokens; may be a decimal number for empty nodes.
</p>
</li>
<li><p> token: Word form or punctuation symbol.
</p>
</li>
<li><p> lemma: Lemma or stem of word form.
</p>
</li>
<li><p> upos: Universal part-of-speech tag.
</p>
</li>
<li><p> xpos: Language-specific part-of-speech tag; underscore if not available.
</p>
</li>
<li><p> feats: List of morphological features from the universal feature inventory or from a defined language-specific extension; underscore if not available.
</p>
</li>
<li><p> head_token_id: Head of the current word, which is either a value of token_id or zero (0).
</p>
</li>
<li><p> dep_rel: Universal dependency relation to the HEAD (root iff HEAD = 0) or a defined language-specific subtype of one.
</p>
</li>
<li><p> deps: Enhanced dependency graph in the form of a list of head-deprel pairs.
</p>
</li>
<li><p> misc: Any other annotation.
</p>
</li></ul>

<p>The tokens in the data.frame should be ordered as they appear in the sentence.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_conllu(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="as_conllu_+3A_x">x</code></td>
<td>
<p>a data.frame with columns doc_id, sentence_id, sentence, 
token_id, token, lemma, upos, xpos, feats, head_token_id, deprel, dep_rel, misc</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a character string of length 1 containing the data.frame in CONLL-U format. See the example. You can easily save this to disk for processing in other applications.
</p>


<h3>References</h3>

<p><a href="https://universaldependencies.org/format.html">https://universaldependencies.org/format.html</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>file_conllu &lt;- system.file(package = "udpipe", "dummydata", "traindata.conllu")
x &lt;- udpipe_read_conllu(file_conllu)
str(x)
conllu &lt;- as_conllu(x)
cat(conllu)
## Not run: 
## Write it to file, making sure it is in UTF-8
cat(as_conllu(x), file = file("annotations.conllu", encoding = "UTF-8"))

## End(Not run)

## Some fields are not mandatory, they will assummed to be NA
conllu &lt;- as_conllu(x[, c('doc_id', 'sentence_id', 'sentence', 
                          'token_id', 'token', 'upos')])
cat(conllu)
</code></pre>

<hr>
<h2 id='as_cooccurrence'>Convert a matrix to a co-occurrence data.frame</h2><span id='topic+as_cooccurrence'></span>

<h3>Description</h3>

<p>Use this function to convert the cells of a matrix to a 
co-occurrence data.frame containing fields term1, term2 and cooc where each row of the resulting
data.frame contains the value of a cell in the matrix if the cell is not empty.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_cooccurrence(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="as_cooccurrence_+3A_x">x</code></td>
<td>
<p>a matrix or sparseMatrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame with columns term1, term2 and cooc where the data in cooc contain 
the content of the cells in the matrix for the combination of term1 and term2
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(brussels_reviews_anno)
x &lt;- subset(brussels_reviews_anno, language == "nl")
dtm &lt;- document_term_frequencies(x = x, document = "doc_id", term = "token")
dtm &lt;- document_term_matrix(dtm)

correlation &lt;- dtm_cor(dtm)
cooc &lt;- as_cooccurrence(correlation)
head(cooc)
</code></pre>

<hr>
<h2 id='as_fasttext'>Combine labels and text as used in fasttext</h2><span id='topic+as_fasttext'></span>

<h3>Description</h3>

<p>Fasttext prepends a label or different labels to text using a special string (__label__).
This function takes a character vector of text and prepends the labels alongside the special string.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_fasttext(x, y, label = "__label__")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="as_fasttext_+3A_x">x</code></td>
<td>
<p>a character vector</p>
</td></tr>
<tr><td><code id="as_fasttext_+3A_y">y</code></td>
<td>
<p>a character vector of labels or a list of labels. <code>y</code> should be of the same length as <code>x</code></p>
</td></tr>
<tr><td><code id="as_fasttext_+3A_label">label</code></td>
<td>
<p>the string to use to prepend to the label. Defaults to __label__</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a character vector of text where <code>x</code> and <code>y</code> are combined
</p>


<h3>Examples</h3>

<pre><code class='language-R'>as_fasttext(x = c("just a bit of txt", "example2", "more txt please", "more"), 
            y = c("pos", "neg", "neg", NA))
as_fasttext(x = c("just a bit of txt", "example2", "more txt please", "more"), 
            y = list(c("ok", "pos"), c("neg", "topic2"), "", NA))
</code></pre>

<hr>
<h2 id='as_phrasemachine'>Convert Parts of Speech tags to one-letter tags which can be used to identify phrases based on regular expressions</h2><span id='topic+as_phrasemachine'></span>

<h3>Description</h3>

<p>Noun phrases are of common interest when doing natural language processing. Extracting noun phrases
from text can be done easily by defining a sequence of Parts of Speech tags. For example this sequence of POS tags
can be seen as a noun phrase: Adjective, Noun, Preposition, Noun.<br />
This function recodes Universal POS tags to one of the following 1-letter tags, in order to simplify writing regular expressions
to find Parts of Speech sequences:
</p>

<ul>
<li><p> A: adjective
</p>
</li>
<li><p> C: coordinating conjuction
</p>
</li>
<li><p> D: determiner
</p>
</li>
<li><p> M: modifier of verb
</p>
</li>
<li><p> N: noun or proper noun
</p>
</li>
<li><p> P: preposition
</p>
</li>
<li><p> O: other elements
</p>
</li></ul>

<p>After which identifying a simple noun phrase can be just expressed by using the following 
regular expression (A|N)*N(P+D*(A|N)*N)* which basically says
start with adjective or noun, another noun, a preposition, determiner adjective or noun and next a noun again.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_phrasemachine(x, type = c("upos", "penn-treebank"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="as_phrasemachine_+3A_x">x</code></td>
<td>
<p>a character vector of POS tags for example by using <code><a href="#topic+udpipe_annotate">udpipe_annotate</a></code></p>
</td></tr>
<tr><td><code id="as_phrasemachine_+3A_type">type</code></td>
<td>
<p>either 'upos' or 'penn-treebank' indicating to recode Universal Parts of Speech tags to the counterparts
as described in the description, or to recode Parts of Speech tags as known in the Penn Treebank to the counterparts
as described in the description</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For more information on extracting phrases see <a href="http://brenocon.com/handler2016phrases.pdf">http://brenocon.com/handler2016phrases.pdf</a>
</p>


<h3>Value</h3>

<p>the character vector <code>x</code> where the respective POS tags are replaced with one-letter tags
</p>


<h3>See Also</h3>

<p><code><a href="#topic+phrases">phrases</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c("PROPN", "SCONJ", "ADJ", "NOUN", "VERB", "INTJ", "DET", "VERB", 
       "PROPN", "AUX", "NUM", "NUM", "X", "SCONJ", "PRON", "PUNCT", "ADP", 
       "X", "PUNCT", "AUX", "PROPN", "ADP", "X", "PROPN", "ADP", "DET", 
       "CCONJ", "INTJ", "NOUN", "PROPN")
as_phrasemachine(x)
</code></pre>

<hr>
<h2 id='as_word2vec'>Convert a matrix of word vectors to word2vec format</h2><span id='topic+as_word2vec'></span>

<h3>Description</h3>

<p>The word2vec format provides in the first line the dimension of the word vectors and in the following lines one
has the elements of the wordvector where each line covers one word or token.<br />
</p>
<p>The function is basically a utility function which allows one to write wordvectors created with other R packages in 
the well-known word2vec format which is used by <code>udpipe_train</code> to train the dependency parser.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_word2vec(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="as_word2vec_+3A_x">x</code></td>
<td>
<p>a matrix with word vectors where the rownames indicate the word or token and the number of columns
of the matrix indicate the side of the word vector</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a character string of length 1 containing the word vectors in word2vec format which can be written to a file on disk
</p>


<h3>Examples</h3>

<pre><code class='language-R'>wordvectors &lt;- matrix(rnorm(1000), nrow = 100, ncol = 10)
rownames(wordvectors) &lt;- sprintf("word%s", seq_len(nrow(wordvectors)))
wv &lt;- as_word2vec(wordvectors)
cat(wv)

f &lt;- file(tempfile(fileext = ".txt"), encoding = "UTF-8")
cat(wv, file = f)
close(f)
</code></pre>

<hr>
<h2 id='as.data.frame.udpipe_connlu'>Convert the result of udpipe_annotate to a tidy data frame</h2><span id='topic+as.data.frame.udpipe_connlu'></span>

<h3>Description</h3>

<p>Convert the result of <code><a href="#topic+udpipe_annotate">udpipe_annotate</a></code> to a tidy data frame
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'udpipe_connlu'
as.data.frame(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="as.data.frame.udpipe_connlu_+3A_x">x</code></td>
<td>
<p>an object of class <code>udpipe_connlu</code> as returned by <code><a href="#topic+udpipe_annotate">udpipe_annotate</a></code></p>
</td></tr>
<tr><td><code id="as.data.frame.udpipe_connlu_+3A_...">...</code></td>
<td>
<p>currently not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame with columns doc_id, paragraph_id, sentence_id, sentence, 
token_id, token, lemma, upos, xpos, feats, head_token_id, dep_rel, deps, misc <br />
</p>
<p>The columns paragraph_id, sentence_id are integers, the other fields
are character data in UTF-8 encoding. <br />
</p>
<p>To get more information on these fields, visit <a href="https://universaldependencies.org/format.html">https://universaldependencies.org/format.html</a> 
or look at <code><a href="#topic+udpipe">udpipe</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+udpipe_annotate">udpipe_annotate</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model    &lt;- udpipe_download_model(language = "dutch-lassysmall")

if(!model$download_failed){

ud_dutch &lt;- udpipe_load_model(model$file_model)
txt &lt;- c("Ik ben de weg kwijt, kunt u me zeggen waar de Lange Wapper ligt? Jazeker meneer", 
         "Het gaat vooruit, het gaat verbazend goed vooruit")
x &lt;- udpipe_annotate(ud_dutch, x = txt)
x &lt;- as.data.frame(x)
head(x)

}

## cleanup for CRAN only - you probably want to keep your model if you have downloaded it
if(file.exists(model$file_model)) file.remove(model$file_model)
</code></pre>

<hr>
<h2 id='as.matrix.cooccurrence'>Convert the result of cooccurrence to a sparse matrix</h2><span id='topic+as.matrix.cooccurrence'></span>

<h3>Description</h3>

<p>Convert the result of <code><a href="#topic+cooccurrence">cooccurrence</a></code> to a sparse matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cooccurrence'
as.matrix(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="as.matrix.cooccurrence_+3A_x">x</code></td>
<td>
<p>an object of class <code>cooccurrence</code> as returned by  <code><a href="#topic+cooccurrence">cooccurrence</a></code></p>
</td></tr>
<tr><td><code id="as.matrix.cooccurrence_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a sparse matrix with in the rows and columns the terms and in the cells how many times
the cooccurrence occurred
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cooccurrence">cooccurrence</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(brussels_reviews_anno)
## By document, which lemma's co-occur
x &lt;- subset(brussels_reviews_anno, xpos %in% c("NN", "JJ") &amp; language %in% "fr")
x &lt;- cooccurrence(x, group = "doc_id", term = "lemma")
x &lt;- as.matrix(x)
dim(x)
x[1:3, 1:3]
</code></pre>

<hr>
<h2 id='brussels_listings'>Brussels AirBnB address locations available at www.insideairbnb.com</h2><span id='topic+brussels_listings'></span>

<h3>Description</h3>

<p>Brussels AirBnB address locations available at www.insideairbnb.com
More information: http://insideairbnb.com/get-the-data.html <br />
Data has been converted from UTF-8 to ASCII as in <code>iconv(x, from = "UTF-8", to = "ASCII//TRANSLIT")</code> in order
to be able to comply to CRAN policies.
</p>


<h3>Source</h3>

<p><a href="http://insideairbnb.com/brussels">http://insideairbnb.com/brussels</a>: information of 2015-10-03
</p>


<h3>See Also</h3>

<p><code><a href="#topic+brussels_reviews">brussels_reviews</a></code>, <code><a href="#topic+brussels_reviews_anno">brussels_reviews_anno</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(brussels_listings)
head(brussels_listings)
</code></pre>

<hr>
<h2 id='brussels_reviews'>Reviews of AirBnB customers on Brussels address locations available at www.insideairbnb.com</h2><span id='topic+brussels_reviews'></span>

<h3>Description</h3>

<p>Reviews of AirBnB customers on Brussels address locations available at www.insideairbnb.com
More information: http://insideairbnb.com/get-the-data.html.
The data contains 500 reviews in Spanish, 500 reviews in French and 500 reviews in Dutch.<br />
The data frame contains the field id (unique), listing_id which corresponds to the listing_id of
the <code><a href="#topic+brussels_listings">brussels_listings</a></code> dataset and text fields feedback and language (identified with package cld2) <br />
Data has been converted from UTF-8 to ASCII as in <code>iconv(x, from = "UTF-8", to = "ASCII//TRANSLIT")</code> in order
to be able to comply to CRAN policies.
</p>


<h3>Source</h3>

<p><a href="http://insideairbnb.com/brussels">http://insideairbnb.com/brussels</a>: information of 2015-10-03
</p>


<h3>See Also</h3>

<p><code><a href="#topic+brussels_listings">brussels_listings</a></code>, <code><a href="#topic+brussels_reviews_anno">brussels_reviews_anno</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(brussels_reviews)
str(brussels_reviews)
head(brussels_reviews)
</code></pre>

<hr>
<h2 id='brussels_reviews_anno'>Reviews of the AirBnB customers which are tokenised, POS tagged and lemmatised</h2><span id='topic+brussels_reviews_anno'></span>

<h3>Description</h3>

<p>Reviews of the AirBnB customerswhich are tokenised, POS tagged and lemmatised.
The data contains 1 row per document/token and contains the fields
doc_id, language, sentence_id, token_id, token, lemma, xpos. <br />
Data has been converted from UTF-8 to ASCII as in <code>iconv(x, from = "UTF-8", to = "ASCII//TRANSLIT")</code> in order
to be able to comply to CRAN policies.
</p>


<h3>Source</h3>

<p><a href="http://insideairbnb.com/brussels">http://insideairbnb.com/brussels</a>: information of 2015-10-03
</p>


<h3>See Also</h3>

<p><code><a href="#topic+brussels_reviews">brussels_reviews</a></code>, <code><a href="#topic+brussels_listings">brussels_listings</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## brussels_reviews_anno
data(brussels_reviews_anno)
head(brussels_reviews_anno)
sort(table(brussels_reviews_anno$xpos))

## Not run: 

##
## If you want to construct a similar dataset as the 
## brussels_reviews_anno dataset based on the udpipe library, do as follows
##

library(udpipe)
library(data.table)
data(brussels_reviews)

## The brussels_reviews contains comments on Airbnb sites in 3 languages: es, fr and nl
table(brussels_reviews$language)
bxl_anno &lt;- split(brussels_reviews, brussels_reviews$language)

## Annotate the Spanish comments
m &lt;- udpipe_download_model(language = "spanish-ancora")
m &lt;- udpipe_load_model(file = m$file_model)
bxl_anno$es &lt;- udpipe_annotate(object = m, x = bxl_anno$es$feedback, doc_id = bxl_anno$es$id)

## Annotate the French comments
m &lt;- udpipe_download_model(language = "french-partut")
m &lt;- udpipe_load_model(file = m$file_model)
bxl_anno$fr &lt;- udpipe_annotate(object = m, x = bxl_anno$fr$feedback, doc_id = bxl_anno$fr$id)

## Annotate the Dutch comments
m &lt;- udpipe_download_model(language = "dutch-lassysmall")
m &lt;- udpipe_load_model(file = m$file_model)
bxl_anno$nl &lt;- udpipe_annotate(object = m, x = bxl_anno$nl$feedback, doc_id = bxl_anno$nl$id)

brussels_reviews_anno &lt;- lapply(bxl_anno, as.data.frame)
brussels_reviews_anno &lt;- rbindlist(brussels_reviews_anno)
str(brussels_reviews_anno)

## End(Not run)
</code></pre>

<hr>
<h2 id='brussels_reviews_w2v_embeddings_lemma_nl'>An example matrix of word embeddings</h2><span id='topic+brussels_reviews_w2v_embeddings_lemma_nl'></span>

<h3>Description</h3>

<p>An simple 10-dimensional example matrix of word embeddings trained on the Dutch lemma's 
of the dataset <code><a href="#topic+brussels_reviews_anno">brussels_reviews_anno</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(brussels_reviews_w2v_embeddings_lemma_nl)
head(brussels_reviews_w2v_embeddings_lemma_nl)
</code></pre>

<hr>
<h2 id='cbind_dependencies'>Add the dependency parsing information to an annotated dataset</h2><span id='topic+cbind_dependencies'></span>

<h3>Description</h3>

<p>Annotated results of <code>udpipe_annotate</code> contain dependency parsing results which indicate
how each word is linked to another word and the relation between these 2 words.<br />
This information is available in the fields token_id, head_token_id and dep_rel which indicates how each token
is linked to the parent. The type of relation (dep_rel) is defined at 
<a href="https://universaldependencies.org/u/dep/index.html">https://universaldependencies.org/u/dep/index.html</a>. <br />
For example in the text 'The economy is weak but the outlook is bright', the term economy is linked to weak
as the term economy is the nominal subject of weak. <br /><br />
This function adds the parent or child information to the annotated data.frame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cbind_dependencies(
  x,
  type = c("parent", "child", "parent_rowid", "child_rowid"),
  recursive = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cbind_dependencies_+3A_x">x</code></td>
<td>
<p>a data.frame or data.table as returned by <code>as.data.frame(udpipe_annotate(...))</code></p>
</td></tr>
<tr><td><code id="cbind_dependencies_+3A_type">type</code></td>
<td>
<p>either one of 'parent', 'child', 'parent_rowid', 'child_rowid'. 
Look to the return value section for more information on the difference in logic. 
Defaults to  'parent', indicating to add the information of the head_token_id to the dataset</p>
</td></tr>
<tr><td><code id="cbind_dependencies_+3A_recursive">recursive</code></td>
<td>
<p>in case when <code>type</code> is set to 'parent_rowid' or 'child_rowid', do you want the parent of the parent of the parent, ... or the child of the child of the child ... included. Defaults to FALSE indicating to only have the direct parent or children.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mark that the output which this function provides might possibly change in subsequent releases and is experimental.
</p>


<h3>Value</h3>

<p>a data.frame/data.table in the same order of <code>x</code> where extra information is added on top namely:
</p>

<ul>
<li><p> In case <code>type</code> is set to <code>'parent'</code>: the token/lemma/upos/xpos/feats information of the parent (head dependency) is added to the data.frame. See the examples.
</p>
</li>
<li><p> In case <code>type</code> is set to <code>'child'</code>: the token/lemma/upos/xpos/feats/dep_rel information of all the children is put into a column called 'children' which is added to the data.frame. This is a list column where each list element is a data.table with these 
columns: token/lemma/upos/xpos/dep_rel. See the examples.
</p>
</li>
<li><p> In case <code>type</code> is set to <code>'parent_rowid'</code>: a new list column is added to <code>x</code> containing the row numbers within each combination of <code>doc_id, paragraph_id, sentence_id</code> which are parents of the token. <br />
In case recursive is set to <code>TRUE</code> the new column which is added to the data.frame is called <code>parent_rowids</code>, otherwise it is called <code>parent_rowid</code>. See the examples.
</p>
</li>
<li><p> In case <code>type</code> is set to <code>'child_rowid'</code>: a new list column is added to <code>x</code> containing the row numbers  within each combination of <code>doc_id, paragraph_id, sentence_id</code> which are children of the token. <br />
In case recursive is set to <code>TRUE</code> the new column which is added to the data.frame is called <code>child_rowids</code>, otherwise it is called <code>child_rowid</code>. See the examples.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
udmodel &lt;- udpipe_download_model(language = "english-ewt")
udmodel &lt;- udpipe_load_model(file = udmodel$file_model)
x &lt;- udpipe_annotate(udmodel, 
                     x = "The economy is weak but the outlook is bright")
x &lt;- as.data.frame(x)
x[, c("token_id", "token", "head_token_id", "dep_rel")]
x &lt;- cbind_dependencies(x, type = "parent")
nominalsubject &lt;- subset(x, dep_rel %in% c("nsubj"))
nominalsubject &lt;- nominalsubject[, c("dep_rel", "token", "token_parent")]
nominalsubject

x &lt;- cbind_dependencies(x, type = "child")
x &lt;- cbind_dependencies(x, type = "parent_rowid")
x &lt;- cbind_dependencies(x, type = "parent_rowid", recursive = TRUE)
x &lt;- cbind_dependencies(x, type = "child_rowid")
x &lt;- cbind_dependencies(x, type = "child_rowid", recursive = TRUE)
x
lapply(x$child_rowid, FUN=function(i) x[sort(i), ])

## End(Not run)
</code></pre>

<hr>
<h2 id='cbind_morphological'>Add morphological features to an annotated dataset</h2><span id='topic+cbind_morphological'></span>

<h3>Description</h3>

<p>The result of <code><a href="#topic+udpipe_annotate">udpipe_annotate</a></code> which is put into a data.frame 
returns a field called <code>feats</code> containing morphological features as defined at 
<a href="https://universaldependencies.org/u/feat/index.html">https://universaldependencies.org/u/feat/index.html</a>. If there are several of these features,
these are concatenated with the <code>|</code> symbol. This function extracts each of these morphological 
features separately and adds these as extra columns to the data.frame
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cbind_morphological(x, term = "feats", which)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cbind_morphological_+3A_x">x</code></td>
<td>
<p>a data.frame or data.table as returned by <code>as.data.frame(udpipe_annotate(...))</code></p>
</td></tr>
<tr><td><code id="cbind_morphological_+3A_term">term</code></td>
<td>
<p>the name of the field in <code>x</code> which contains the morphological features. Defaults to 'feats'.</p>
</td></tr>
<tr><td><code id="cbind_morphological_+3A_which">which</code></td>
<td>
<p>a character vector with names of morphological features to uniquely parse out. These 
features are one of the 24 lexical and grammatical properties of words defined at <a href="https://universaldependencies.org/u/feat/index.html">https://universaldependencies.org/u/feat/index.html</a>.
Possible values are:
</p>

<ul>
<li><p>&quot;lexical&quot;: &quot;PronType&quot;, &quot;NumType&quot;, &quot;Poss&quot;, &quot;Reflex&quot;, &quot;Foreign&quot;, &quot;Abbr&quot;, &quot;Typo&quot;
</p>
</li>
<li><p>&quot;inflectional_noun&quot;: &quot;Gender&quot;, &quot;Animacy&quot;, &quot;NounClass&quot;, &quot;Number&quot;, &quot;Case&quot;, &quot;Definite&quot;, &quot;Degree&quot;
</p>
</li>
<li><p>&quot;inflectional_verb&quot;: &quot;VerbForm&quot;, &quot;Mood&quot;, &quot;Tense&quot;, &quot;Aspect&quot;, &quot;Voice&quot;, &quot;Evident&quot;, &quot;Polarity&quot;, &quot;Person&quot;, &quot;Polite&quot;, &quot;Clusivity&quot;
</p>
</li></ul>

<p>See the examples.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>x</code> in the same order with extra columns added (at least the column has_morph is added indicating
if any morphological features are present and as well extra columns for each possible morphological feature in the data)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
udmodel &lt;- udpipe_download_model(language = "english-ewt")
udmodel &lt;- udpipe_load_model(file = udmodel$file_model)
x &lt;- udpipe_annotate(udmodel, 
                     x = "The economy is weak but the outlook is bright")
x &lt;- as.data.frame(x)
x &lt;- cbind_morphological(x, term = "feats")

## End(Not run)

f &lt;- system.file(package = "udpipe", "dummydata", "traindata.conllu")
x &lt;- udpipe_read_conllu(f)
x &lt;- cbind_morphological(x, term = "feats")

f &lt;- system.file(package = "udpipe", "dummydata", "traindata.conllu")
x &lt;- udpipe_read_conllu(f)
x &lt;- cbind_morphological(x, term = "feats", 
                         which = c("Mood", "Gender", "VerbForm", "Polarity", "Polite"))

# extract all features from the feats column even if not present in the data
f &lt;- system.file(package = "udpipe", "dummydata", "traindata.conllu")
x &lt;- udpipe_read_conllu(f)
x &lt;- cbind_morphological(x, term = "feats", 
                         which = c("lexical", "inflectional_noun", "inflectional_verb"))
</code></pre>

<hr>
<h2 id='cooccurrence'>Create a cooccurence data.frame</h2><span id='topic+cooccurrence'></span><span id='topic+cooccurrence.character'></span><span id='topic+cooccurrence.cooccurrence'></span><span id='topic+cooccurrence.data.frame'></span>

<h3>Description</h3>

<p>A cooccurence data.frame indicates how many times each term co-occurs with another term.<br />
</p>
<p>There are 3 types of cooccurrences:
</p>

<ul>
<li><p> Looking at which words are located in the same document/sentence/paragraph.
</p>
</li>
<li><p> Looking at which words are followed by another word
</p>
</li>
<li><p> Looking at which words are in the neighbourhood of the word as in follows the word within <code>skipgram</code> number of words
</p>
</li></ul>

<p>The output of the function gives a cooccurrence data.frame which contains the fields term1, term2 and cooc where cooc indicates how many times
term1 and term2 co-occurred. This dataset can be constructed 
</p>

<ul>
<li><p> based upon a data frame where you look within a group (column of the data.frame) if 2 terms occurred in that group.
</p>
</li>
<li><p> based upon a vector of words in which case we look how many times each word is followed by another word.
</p>
</li>
<li><p> based upon a vector of words in which case we look how many times each word is followed by another word or is followed by another word if we skip a number of words in between.
</p>
</li></ul>

<p>Note that 
</p>

<ul>
<li><p> For cooccurrence.data.frame no ordering is assumed which implies that the function does not return self-occurrences if a word occurs several times in the same group of text and term1 is always smaller than term2 in the output
</p>
</li>
<li><p> For cooccurrence.character we assume text is ordered from left to right, the function as well returns self-occurrences
</p>
</li></ul>

<p>You can also aggregate cooccurrences if you decide to do any of these 3 by a certain group and next want to obtain an overall aggregate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cooccurrence(x, order = TRUE, ...)

## S3 method for class 'character'
cooccurrence(
  x,
  order = TRUE,
  ...,
  relevant = rep(TRUE, length(x)),
  skipgram = 0
)

## S3 method for class 'cooccurrence'
cooccurrence(x, order = TRUE, ...)

## S3 method for class 'data.frame'
cooccurrence(x, order = TRUE, ..., group, term)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cooccurrence_+3A_x">x</code></td>
<td>
<p>either
</p>

<ul>
<li><p> a data.frame where the data.frame contains 1 row per document/term,
in which case you need to provide <code>group</code> and <code>term</code> where <code>term</code> is the column containing 1 term per row
and <code>group</code> indicates something like a document id or document + sentence id. This uses cooccurrence.data.frame.
</p>
</li>
<li><p> a character vector with terms where one element contains 1 term. This uses cooccurrence.character.
</p>
</li>
<li><p> an object of class <code>cooccurrence</code>. This uses cooccurrence.cooccurrence.
</p>
</li></ul>
</td></tr>
<tr><td><code id="cooccurrence_+3A_order">order</code></td>
<td>
<p>logical indicating if we need to sort the output from high cooccurrences to low coccurrences. Defaults to TRUE.</p>
</td></tr>
<tr><td><code id="cooccurrence_+3A_...">...</code></td>
<td>
<p>other arguments passed on to the methods</p>
</td></tr>
<tr><td><code id="cooccurrence_+3A_relevant">relevant</code></td>
<td>
<p>a logical vector of the same length as <code>x</code>, indicating if the word in <code>x</code> is relevant or not.
This can be used to exclude stopwords from the cooccurrence calculation or selecting only nouns and adjectives to 
find cooccurrences along with each other 
(for example based on the Parts of Speech <code>upos</code> output from <code>udpipe_annotate</code>).<br />
Only used if calculating cooccurrences on <code>x</code> which is a character vector of words.</p>
</td></tr>
<tr><td><code id="cooccurrence_+3A_skipgram">skipgram</code></td>
<td>
<p>integer of length 1, indicating how far in the neighbourhood to look for words.<br />
<code>skipgram</code> is considered the maximum skip distance between words to calculate co-occurrences 
(where co-occurrences are of type skipgram-bigram, where a skipgram-bigram are 2 words which occur at a distance of at most <code>skipgram + 1</code> from each other). <br />
Only used if calculating cooccurrences on <code>x</code> which is a character vector of words.</p>
</td></tr>
<tr><td><code id="cooccurrence_+3A_group">group</code></td>
<td>
<p>character vector of columns in the data frame <code>x</code> indicating to calculate cooccurrences within these columns. <br />
This is typically a field like document id or a sentence identifier. To be used if <code>x</code> is a data.frame.</p>
</td></tr>
<tr><td><code id="cooccurrence_+3A_term">term</code></td>
<td>
<p>character string of a column in the data frame <code>x</code>, containing 1 term per row. To be used if <code>x</code> is a data.frame.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame with columns term1, term2 and cooc indicating
for the combination of term1 and term2 how many times this combination occurred
</p>


<h3>Methods (by class)</h3>


<ul>
<li> <p><code>character</code>: Create a cooccurence data.frame based on a vector of terms
</p>
</li>
<li> <p><code>cooccurrence</code>: Aggregate co-occurrence statistics by summing the cooc by term/term2
</p>
</li>
<li> <p><code>data.frame</code>: Create a cooccurence data.frame based on a data.frame where you look within a document / sentence / paragraph / group 
if terms co-occur
</p>
</li></ul>


<h3>Examples</h3>

<pre><code class='language-R'>
data(brussels_reviews_anno)

## By document, which lemma's co-occur
x &lt;- subset(brussels_reviews_anno, xpos %in% c("NN", "JJ") &amp; language %in% "fr")
x &lt;- cooccurrence(x, group = "doc_id", term = "lemma")
head(x)

## Which words follow each other
x &lt;- c("A", "B", "A", "A", "B", "c")
cooccurrence(x)

data(brussels_reviews_anno)
x &lt;- subset(brussels_reviews_anno, language == "es")
x &lt;- cooccurrence(x$lemma)
head(x)
x &lt;- subset(brussels_reviews_anno, language == "es")
x &lt;- cooccurrence(x$lemma, relevant = x$xpos %in% c("NN", "JJ"), skipgram = 4)
head(x)

## Which nouns follow each other in the same document
library(data.table)
x &lt;- as.data.table(brussels_reviews_anno)
x &lt;- subset(x, language == "nl" &amp; xpos %in% c("NN"))
x &lt;- x[, cooccurrence(lemma, order = FALSE), by = list(doc_id)]
head(x)

x_nodoc &lt;- cooccurrence(x)
x_nodoc &lt;- subset(x_nodoc, term1 != "appartement" &amp; term2 != "appartement")
head(x_nodoc)
</code></pre>

<hr>
<h2 id='document_term_frequencies'>Aggregate a data.frame to the document/term level by calculating how many times a term occurs per document</h2><span id='topic+document_term_frequencies'></span><span id='topic+document_term_frequencies.data.frame'></span><span id='topic+document_term_frequencies.character'></span>

<h3>Description</h3>

<p>Aggregate a data.frame to the document/term level by calculating how many times a term occurs per document
</p>


<h3>Usage</h3>

<pre><code class='language-R'>document_term_frequencies(x, document, ...)

## S3 method for class 'data.frame'
document_term_frequencies(
  x,
  document = colnames(x)[1],
  term = colnames(x)[2],
  ...
)

## S3 method for class 'character'
document_term_frequencies(
  x,
  document = paste("doc", seq_along(x), sep = ""),
  split = "[[:space:][:punct:][:digit:]]+",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="document_term_frequencies_+3A_x">x</code></td>
<td>
<p>a data.frame or data.table containing a field which can be considered 
as a document (defaults to the first column in <code>x</code>) and a field which can be considered as a term 
(defaults to the second column in <code>x</code>). If the dataset also contains a column called 'freq', this will be summed over instead of counting the number 
of rows occur by document/term combination.<br />
If <code>x</code> is a character vector containing several terms, the text will be split by the argument <code>split</code>
before doing the agregation at the document/term level.</p>
</td></tr>
<tr><td><code id="document_term_frequencies_+3A_document">document</code></td>
<td>
<p>If <code>x</code> is a data.frame, the column in <code>x</code> which identifies a document. If <code>x</code>
is a character vector then <code>document</code> is a vector of the same length as <code>x</code> where <code>document[i]</code> is the
document id which corresponds to the text in <code>x[i]</code>.</p>
</td></tr>
<tr><td><code id="document_term_frequencies_+3A_...">...</code></td>
<td>
<p>further arguments passed on to the methods</p>
</td></tr>
<tr><td><code id="document_term_frequencies_+3A_term">term</code></td>
<td>
<p>If <code>x</code> is a data.frame, the column in <code>x</code> which identifies a term. Defaults to the second column
in <code>x</code>.</p>
</td></tr>
<tr><td><code id="document_term_frequencies_+3A_split">split</code></td>
<td>
<p>The regular expression to be used if <code>x</code> is a character vector. 
This will split the character vector <code>x</code> in pieces by the provides split argument. 
Defaults to splitting according to spaces/punctuations/digits.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.table with columns doc_id, term, freq indicating how many times a term occurred in each document.
If freq occurred in the input dataset the resulting data will have summed the freq. If freq is not in the dataset,
will assume that freq is 1 for each row in the input dataset <code>x</code>.
</p>


<h3>Methods (by class)</h3>


<ul>
<li> <p><code>data.frame</code>: Create a data.frame with one row per document/term combination indicating the frequency of the term in the document
</p>
</li>
<li> <p><code>character</code>: Create a data.frame with one row per document/term combination indicating the frequency of the term in the document
</p>
</li></ul>


<h3>Examples</h3>

<pre><code class='language-R'>
##
## Calculate document_term_frequencies on a data.frame
##
data(brussels_reviews_anno)

x &lt;- document_term_frequencies(brussels_reviews_anno[, c("doc_id", "token")])
x &lt;- document_term_frequencies(brussels_reviews_anno[, c("doc_id", "lemma")])
str(x)

brussels_reviews_anno$my_doc_id &lt;- paste(brussels_reviews_anno$doc_id, 
                                         brussels_reviews_anno$sentence_id)
x &lt;- document_term_frequencies(brussels_reviews_anno[, c("my_doc_id", "lemma")])

##
## Calculate document_term_frequencies on a character vector
##
data(brussels_reviews)
x &lt;- document_term_frequencies(x = brussels_reviews$feedback, document = brussels_reviews$id, 
                               split = " ")
x &lt;- document_term_frequencies(x = brussels_reviews$feedback, document = brussels_reviews$id, 
                               split = "[[:space:][:punct:][:digit:]]+")
                               
##
## document-term-frequencies on several fields to easily include bigram and trigrams
##
library(data.table)
x &lt;- as.data.table(brussels_reviews_anno)
x &lt;- x[, token_bigram  := txt_nextgram(token, n = 2), by = list(doc_id, sentence_id)]
x &lt;- x[, token_trigram := txt_nextgram(token, n = 3), by = list(doc_id, sentence_id)]
x &lt;- document_term_frequencies(x = x, 
                               document = "doc_id", 
                               term = c("token", "token_bigram", "token_trigram"))
head(x)
</code></pre>

<hr>
<h2 id='document_term_frequencies_statistics'>Add Term Frequency, Inverse Document Frequency and Okapi BM25 statistics to the output of document_term_frequencies</h2><span id='topic+document_term_frequencies_statistics'></span>

<h3>Description</h3>

<p>Term frequency Inverse Document Frequency (tfidf) is calculated as the multiplication of
</p>

<ul>
<li><p> Term Frequency (tf): how many times the word occurs in the document / how many words are in the document
</p>
</li>
<li><p> Inverse Document Frequency (idf): log(number of documents / number of documents where the term appears)
</p>
</li></ul>

<p>The Okapi BM25 statistic is calculated as the multiplication of the inverse document frequency
and the weighted term frequency as defined at <a href="https://en.wikipedia.org/wiki/Okapi_BM25">https://en.wikipedia.org/wiki/Okapi_BM25</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>document_term_frequencies_statistics(x, k = 1.2, b = 0.75)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="document_term_frequencies_statistics_+3A_x">x</code></td>
<td>
<p>a data.table as returned by <code>document_term_frequencies</code> containing the columns doc_id, term and freq.</p>
</td></tr>
<tr><td><code id="document_term_frequencies_statistics_+3A_k">k</code></td>
<td>
<p>parameter k1 of the Okapi BM25 ranking function as defined at <a href="https://en.wikipedia.org/wiki/Okapi_BM25">https://en.wikipedia.org/wiki/Okapi_BM25</a>. Defaults to 1.2.</p>
</td></tr>
<tr><td><code id="document_term_frequencies_statistics_+3A_b">b</code></td>
<td>
<p>parameter b of the Okapi BM25 ranking function as defined at <a href="https://en.wikipedia.org/wiki/Okapi_BM25">https://en.wikipedia.org/wiki/Okapi_BM25</a>. Defaults to 0.5.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.table with columns doc_id, term, freq and added to that the computed statistics
tf, idf, tfidf, tf_bm25 and bm25.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(brussels_reviews_anno)

x &lt;- document_term_frequencies(brussels_reviews_anno[, c("doc_id", "token")])
x &lt;- document_term_frequencies_statistics(x)
head(x)
</code></pre>

<hr>
<h2 id='document_term_matrix'>Create a document/term matrix</h2><span id='topic+document_term_matrix'></span><span id='topic+document_term_matrix.data.frame'></span><span id='topic+document_term_matrix.matrix'></span><span id='topic+document_term_matrix.integer'></span><span id='topic+document_term_matrix.numeric'></span><span id='topic+document_term_matrix.default'></span><span id='topic+document_term_matrix.DocumentTermMatrix'></span><span id='topic+document_term_matrix.TermDocumentMatrix'></span><span id='topic+document_term_matrix.simple_triplet_matrix'></span>

<h3>Description</h3>

<p>Create a document/term matrix from either
</p>

<ul>
<li><p> a data.frame with 1 row per document/term as returned by <code><a href="#topic+document_term_frequencies">document_term_frequencies</a></code>
</p>
</li>
<li><p> a list of tokens from e.g. from package sentencepiece, tokenizers.bpe or just by using strsplit
</p>
</li>
<li><p> an object of class DocumentTermMatrix or TermDocumentMatrix from the tm package
</p>
</li>
<li><p> an object of class simple_triplet_matrix from the slam package
</p>
</li>
<li><p> a regular dense matrix
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>document_term_matrix(x, vocabulary, weight = "freq", ...)

## S3 method for class 'data.frame'
document_term_matrix(x, vocabulary, weight = "freq", ...)

## S3 method for class 'matrix'
document_term_matrix(x, ...)

## S3 method for class 'integer'
document_term_matrix(x, ...)

## S3 method for class 'numeric'
document_term_matrix(x, ...)

## Default S3 method:
document_term_matrix(x, vocabulary, ...)

## S3 method for class 'DocumentTermMatrix'
document_term_matrix(x, ...)

## S3 method for class 'TermDocumentMatrix'
document_term_matrix(x, ...)

## S3 method for class 'simple_triplet_matrix'
document_term_matrix(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="document_term_matrix_+3A_x">x</code></td>
<td>
<p>a data.frame with columns doc_id, term and freq indicating how many times a term occurred in that specific document. This is what <code><a href="#topic+document_term_frequencies">document_term_frequencies</a></code> returns.<br />
This data.frame will be reshaped to a matrix with 1 row per doc_id, the terms will be put 
in the columns and the freq in the matrix cells. Note that the column name to use for freq can be set in the <code>weight</code> argument.</p>
</td></tr>
<tr><td><code id="document_term_matrix_+3A_vocabulary">vocabulary</code></td>
<td>
<p>a character vector of terms which should be present in the document term matrix even if they did not occur in <code>x</code></p>
</td></tr>
<tr><td><code id="document_term_matrix_+3A_weight">weight</code></td>
<td>
<p>a column of <code>x</code> indicating what to put in the matrix cells. Defaults to 'freq' indicating to use column <code>freq</code> from <code>x</code> to put into the matrix cells</p>
</td></tr>
<tr><td><code id="document_term_matrix_+3A_...">...</code></td>
<td>
<p>further arguments currently not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an sparse object of class dgCMatrix with in the rows the documents and in the columns the terms containing the frequencies
provided in <code>x</code> extended with terms which were not in <code>x</code> but were provided in <code>vocabulary</code>.
The rownames of this resulting object contain the doc_id from <code>x</code>
</p>


<h3>Methods (by class)</h3>


<ul>
<li> <p><code>data.frame</code>: Construct a document term matrix from a data.frame with columns doc_id, term, freq
</p>
</li>
<li> <p><code>matrix</code>: Construct a sparse document term matrix from a matrix
</p>
</li>
<li> <p><code>integer</code>: Construct a sparse document term matrix from an named integer vector
</p>
</li>
<li> <p><code>numeric</code>: Construct a sparse document term matrix from a named numeric vector
</p>
</li>
<li> <p><code>default</code>: Construct a document term matrix from a list of tokens
</p>
</li>
<li> <p><code>DocumentTermMatrix</code>: Convert an object of class <code>DocumentTermMatrix</code> from the tm package to a sparseMatrix
</p>
</li>
<li> <p><code>TermDocumentMatrix</code>: Convert an object of class <code>TermDocumentMatrix</code> from the tm package to a sparseMatrix with
the documents in the rows and the terms in the columns
</p>
</li>
<li> <p><code>simple_triplet_matrix</code>: Convert an object of class <code>simple_triplet_matrix</code> from the slam package to a sparseMatrix
</p>
</li></ul>


<h3>See Also</h3>

<p><code><a href="Matrix.html#topic+sparseMatrix">sparseMatrix</a></code>, <code><a href="#topic+document_term_frequencies">document_term_frequencies</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- data.frame(doc_id = c(1, 1, 2, 3, 4), 
 term = c("A", "C", "Z", "X", "G"), 
 freq = c(1, 5, 7, 10, 0))
document_term_matrix(x)
document_term_matrix(x, vocabulary = LETTERS)

## Example on larger dataset
data(brussels_reviews_anno)

x   &lt;- document_term_frequencies(brussels_reviews_anno[, c("doc_id", "lemma")])
dtm &lt;- document_term_matrix(x)
dim(dtm)
x   &lt;- document_term_frequencies(brussels_reviews_anno[, c("doc_id", "lemma")])
x   &lt;- document_term_frequencies_statistics(x)
dtm &lt;- document_term_matrix(x)
dtm &lt;- document_term_matrix(x, weight = "freq")
dtm &lt;- document_term_matrix(x, weight = "tf_idf")
dtm &lt;- document_term_matrix(x, weight = "bm25")
x   &lt;- split(brussels_reviews_anno$lemma, brussels_reviews_anno$doc_id)
dtm &lt;- document_term_matrix(x)
## example showing the vocubulary argument
## allowing you to making sure terms which are not in the data are provided in the resulting dtm
allterms &lt;- unique(x$term)
dtm &lt;- document_term_matrix(head(x, 1000), vocabulary = allterms)

## example for a list of tokens
x &lt;- list(doc1 = c("aa", "bb", "cc", "aa", "b"), 
          doc2 = c("bb", "bb", "dd", ""), 
          doc3 = character(),
          doc4 = c("cc", NA), 
          doc5 = character())
document_term_matrix(x)
dtm &lt;- document_term_matrix(x, vocabulary = c("a", "bb", "cc"))
dtm &lt;- dtm_conform(dtm, rows = c("doc1", "doc2", "doc7"), columns = c("a", "bb", "cc"))
data(brussels_reviews)
x   &lt;- strsplit(setNames(brussels_reviews$feedback, brussels_reviews$id), split = " +")
x   &lt;- document_term_matrix(x)


##
## Example adding bigrams/trigrams to the document term matrix
## Mark that this can also be done using ?dtm_cbind
##
library(data.table)
x &lt;- as.data.table(brussels_reviews_anno)
x &lt;- x[, token_bigram  := txt_nextgram(token, n = 2), by = list(doc_id, sentence_id)]
x &lt;- x[, token_trigram := txt_nextgram(token, n = 3), by = list(doc_id, sentence_id)]
x &lt;- document_term_frequencies(x = x, 
                               document = "doc_id", 
                               term = c("token", "token_bigram", "token_trigram"))
dtm &lt;- document_term_matrix(x)

##
## Convert dense matrix to sparse matrix
##
x &lt;- matrix(c(0, 0, 0, 1, NA, 3, 4, 5, 6, 7), nrow = 2)
x
dtm &lt;- document_term_matrix(x)
dtm
x &lt;- matrix(c(0, 0, 0, 0.1, NA, 0.3, 0.4, 0.5, 0.6, 0.7), nrow = 2)
x
dtm &lt;- document_term_matrix(x)
dtm
x   &lt;- setNames(c(TRUE, NA, FALSE, FALSE), c("a", "b", "c", "d"))
x   &lt;- as.matrix(x)
dtm &lt;- document_term_matrix(x)
dtm

##
## Convert vectors to sparse matrices
##
x   &lt;- setNames(-3:3, c("a", "b", "c", "d", "e", "f"))
dtm &lt;- document_term_matrix(x)
dtm
x   &lt;- setNames(runif(6), c("a", "b", "c", "d", "e", "f"))
dtm &lt;- document_term_matrix(x)
dtm

##
## Convert lists to sparse matrices
##
x   &lt;- list(a = c("some", "set", "of", "words"), 
            b1 = NA,
            b2 = NA,  
            c1 = character(),
            c2 = 0,  
            d = c("words", "words", "words"))
dtm &lt;- document_term_matrix(x)
dtm
</code></pre>

<hr>
<h2 id='dtm_align'>Reorder a Document-Term-Matrix alongside a vector or data.frame</h2><span id='topic+dtm_align'></span>

<h3>Description</h3>

<p>This utility function is useful to align a Document-Term-Matrix with 
information in a data.frame or a vector to predict, such that both the predictive information as well as the target 
is available in the same order. <br />
Matching is done based on the identifiers in the rownames of <code>x</code> and either the names of the <code>y</code> vector 
or the first column of <code>y</code> in case it is a data.frame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dtm_align(x, y, FUN, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dtm_align_+3A_x">x</code></td>
<td>
<p>a Document-Term-Matrix of class dgCMatrix (which can be an object returned by <code><a href="#topic+document_term_matrix">document_term_matrix</a></code>)</p>
</td></tr>
<tr><td><code id="dtm_align_+3A_y">y</code></td>
<td>
<p>either a vector or data.frame containing something to align with <code>x</code> (e.g. for predictive purposes).
</p>

<ul>
<li><p>In case <code>y</code> is a vector, it should have names which are available in the rownames of <code>x</code>.
</p>
</li>
<li><p>In case <code>y</code> is a data.frame, it's first column should contain identifiers which are available in the rownames of <code>x</code>.
</p>
</li></ul>
</td></tr>
<tr><td><code id="dtm_align_+3A_fun">FUN</code></td>
<td>
<p>a function to be applied on <code>x</code> before aligning it to <code>y</code>. See the examples</p>
</td></tr>
<tr><td><code id="dtm_align_+3A_...">...</code></td>
<td>
<p>further arguments passed on to FUN</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with elements <code>x</code> and <code>y</code> containing the document term matrix <code>x</code> in the same order as <code>y</code>.
</p>

<ul>
<li><p>If in <code>y</code> a vector was passed, the returned <code>y</code> element will be a vector
</p>
</li>
<li><p>If in <code>y</code> a data.frame was passed with more than 2 columns, the returned <code>y</code> element will be a data.frame
</p>
</li>
<li><p>If in <code>y</code> a data.frame was passed with exactly 2 columns, the returned <code>y</code> element will be a vector
</p>
</li></ul>

<p>Only returns data of <code>x</code> with overlapping identifiers in <code>y</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+document_term_matrix">document_term_matrix</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix(1:9, nrow = 3, dimnames = list(c("a", "b", "c")))
x
dtm_align(x = x, 
          y = c(b = 1, a = 2, c = 6, d = 6))
dtm_align(x = x, 
          y = c(b = 1, a = 2, c = 6, d = 6, d = 7, a = -1))
          
data(brussels_reviews)
data(brussels_listings)
x &lt;- brussels_reviews
x &lt;- strsplit.data.frame(x, term = "feedback", group = "listing_id")
x &lt;- document_term_frequencies(x)
x &lt;- document_term_matrix(x)
y &lt;- brussels_listings$price
names(y) &lt;- brussels_listings$listing_id

## align a matrix of predictors with a vector to predict
trainset &lt;- dtm_align(x = x, y = y)
trainset &lt;- dtm_align(x = x, y = y, FUN = function(dtm){
  dtm &lt;- dtm_remove_lowfreq(dtm, minfreq = 5)
  dtm &lt;- dtm_sample(dtm)
  dtm
})
head(names(y))
head(rownames(x))
head(names(trainset$y))
head(rownames(trainset$x))

## align a matrix of predictors with a data.frame
trainset &lt;- dtm_align(x = x, y = brussels_listings[, c("listing_id", "price")])
trainset &lt;- dtm_align(x = x, 
                y = brussels_listings[, c("listing_id", "price", "room_type")])
head(trainset$y$listing_id)
head(rownames(trainset$x))

## example with duplicate data in case of data balancing
dtm_align(x = matrix(1:30, nrow = 3, dimnames = list(c("a", "b", "c"))), 
          y = c(a = 1, a = 2, b = 3, d = 6, b = 6))
target   &lt;- subset(brussels_listings, listing_id %in% brussels_reviews$listing_id)
target   &lt;- rbind(target[1:3, ], target[c(2, 3), ], target[c(1, 4), ])
trainset &lt;- dtm_align(x = x, y = target[, c("listing_id", "price")])
trainset &lt;- dtm_align(x = x, y = setNames(target$price, target$listing_id))
names(trainset$y)
rownames(trainset$x)
</code></pre>

<hr>
<h2 id='dtm_bind'>Combine 2 document term matrices either by rows or by columns</h2><span id='topic+dtm_bind'></span><span id='topic+dtm_cbind'></span><span id='topic+dtm_rbind'></span>

<h3>Description</h3>

<p>These 2 methods provide <code><a href="base.html#topic+cbind">cbind</a></code> and <code><a href="base.html#topic+rbind">rbind</a></code> functionality 
for sparse matrix objects which are returned by <code><a href="#topic+document_term_matrix">document_term_matrix</a></code>. <br />
</p>
<p>In case of <code>dtm_cbind</code>, if the rows are not ordered in the same way in x and y, it will order them based on the rownames. 
If there are missing rows these will be filled with NA values. <br />
In case of <code>dtm_rbind</code>, if the columns are not ordered in the same way in x and y, it will order them based on the colnames. 
If there are missing columns these will be filled with NA values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dtm_cbind(x, y, ...)

dtm_rbind(x, y, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dtm_bind_+3A_x">x</code></td>
<td>
<p>a sparse matrix such as a &quot;dgCMatrix&quot; object which is returned by <code><a href="#topic+document_term_matrix">document_term_matrix</a></code></p>
</td></tr>
<tr><td><code id="dtm_bind_+3A_y">y</code></td>
<td>
<p>a sparse matrix such as a &quot;dgCMatrix&quot; object which is returned by <code><a href="#topic+document_term_matrix">document_term_matrix</a></code></p>
</td></tr>
<tr><td><code id="dtm_bind_+3A_...">...</code></td>
<td>
<p>more sparse matrices</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a sparse matrix where either rows are put below each other in case of <code>dtm_rbind</code>
or columns are put next to each other in case of <code>dtm_cbind</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+document_term_matrix">document_term_matrix</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(brussels_reviews_anno)
x &lt;- brussels_reviews_anno

## rbind
dtm1 &lt;- document_term_frequencies(x = subset(x, doc_id %in% c("10049756", "10284782")),
                                  document = "doc_id", term = "token")
dtm1 &lt;- document_term_matrix(dtm1)
dtm2 &lt;- document_term_frequencies(x = subset(x, doc_id %in% c("10789408", "12285061", "35509091")),
                                  document = "doc_id", term = "token")
dtm2 &lt;- document_term_matrix(dtm2)
dtm3 &lt;- document_term_frequencies(x = subset(x, doc_id %in% c("31133394", "36224131")),
                                  document = "doc_id", term = "token")
dtm3 &lt;- document_term_matrix(dtm3)
m &lt;- dtm_rbind(dtm1, dtm2)
dim(m)
m &lt;- dtm_rbind(dtm1, dtm2, dtm3)
dim(m)

## cbind
library(data.table)
x &lt;- subset(brussels_reviews_anno, language %in% c("nl", "fr"))
x &lt;- as.data.table(x)
x &lt;- x[, token_bigram  := txt_nextgram(token, n = 2), by = list(doc_id, sentence_id)]
x &lt;- x[, lemma_upos    := sprintf("%s//%s", lemma, upos)]
dtm1 &lt;- document_term_frequencies(x = x, document = "doc_id", term = c("token"))
dtm1 &lt;- document_term_matrix(dtm1)
dtm2 &lt;- document_term_frequencies(x = x, document = "doc_id", term = c("token_bigram"))
dtm2 &lt;- document_term_matrix(dtm2)
dtm3 &lt;- document_term_frequencies(x = x, document = "doc_id", term = c("upos"))
dtm3 &lt;- document_term_matrix(dtm3)
dtm4 &lt;- document_term_frequencies(x = x, document = "doc_id", term = c("lemma_upos"))
dtm4 &lt;- document_term_matrix(dtm4)
m &lt;- dtm_cbind(dtm1, dtm2)
dim(m)
m &lt;- dtm_cbind(dtm1, dtm2, dtm3, dtm4)
dim(m)
m &lt;- dtm_cbind(dtm1[-c(100, 999), ], dtm2[-1000,])
dim(m)
</code></pre>

<hr>
<h2 id='dtm_chisq'>Compare term usage across 2 document groups using the Chi-square Test for Count Data</h2><span id='topic+dtm_chisq'></span>

<h3>Description</h3>

<p>Perform a <code><a href="stats.html#topic+chisq.test">chisq.test</a></code> to compare if groups of documents have more prevalence of specific terms.<br />
The function looks to each term in the document term matrix and applies a <code><a href="stats.html#topic+chisq.test">chisq.test</a></code> comparing the frequency 
of occurrence of each term compared to the other terms in the document group.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dtm_chisq(dtm, groups, correct = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dtm_chisq_+3A_dtm">dtm</code></td>
<td>
<p>a document term matrix: an object returned by <code><a href="#topic+document_term_matrix">document_term_matrix</a></code></p>
</td></tr>
<tr><td><code id="dtm_chisq_+3A_groups">groups</code></td>
<td>
<p>a logical vector with 2 groups (TRUE / FALSE) where the size of the <code>groups</code> vector 
is the same as the number of rows of <code>dtm</code> and where element i corresponds row i of <code>dtm</code></p>
</td></tr>
<tr><td><code id="dtm_chisq_+3A_correct">correct</code></td>
<td>
<p>passed on to <code><a href="stats.html#topic+chisq.test">chisq.test</a></code></p>
</td></tr>
<tr><td><code id="dtm_chisq_+3A_...">...</code></td>
<td>
<p>further arguments passed on to <code><a href="stats.html#topic+chisq.test">chisq.test</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame with columns term, chisq, p.value, freq, freq_true, freq_false indicating for each term in the <code>dtm</code>,
how frequently it occurs in each group, the Chi-Square value and it's corresponding p-value.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(brussels_reviews_anno)
##
## Which nouns occur in text containing the term 'centre'
##
x &lt;- subset(brussels_reviews_anno, xpos == "NN" &amp; language == "fr")
x &lt;- x[, c("doc_id", "lemma")]
x &lt;- document_term_frequencies(x)
dtm &lt;- document_term_matrix(x)
relevant &lt;- dtm_chisq(dtm, groups = dtm[, "centre"] &gt; 0)
head(relevant, 10)

##
## Which adjectives occur in text containing the term 'hote'
##
x &lt;- subset(brussels_reviews_anno, xpos == "JJ" &amp; language == "fr")
x &lt;- x[, c("doc_id", "lemma")]
x &lt;- document_term_frequencies(x)
dtm &lt;- document_term_matrix(x)

group &lt;- subset(brussels_reviews_anno, lemma %in% "hote")
group &lt;- rownames(dtm) %in% group$doc_id
relevant &lt;- dtm_chisq(dtm, groups = group)
head(relevant, 10)


## Not run: 
# do not show scientific notation of the p-values
options(scipen = 100)
head(relevant, 10)

## End(Not run)
</code></pre>

<hr>
<h2 id='dtm_colsums'>Column sums and Row sums for document term matrices</h2><span id='topic+dtm_colsums'></span><span id='topic+dtm_rowsums'></span>

<h3>Description</h3>

<p>Column sums and Row sums for document term matrices
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dtm_colsums(dtm, groups)

dtm_rowsums(dtm, groups)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dtm_colsums_+3A_dtm">dtm</code></td>
<td>
<p>an object returned by <code><a href="#topic+document_term_matrix">document_term_matrix</a></code></p>
</td></tr>
<tr><td><code id="dtm_colsums_+3A_groups">groups</code></td>
<td>
<p>optionally, a list with column/row names or column/row indexes of the <code>dtm</code> which should be combined by 
taking the sum over the rows or columns of these. See the examples</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns either a vector in case argument <code>groups</code> is not provided or a sparse matrix of class <code>dgCMatrix</code>
in case argument <code>groups</code> is provided
</p>

<ul>
<li><p>in case <code>groups</code> is not provided: a vector of row/column sums with corresponding names
</p>
</li>
<li><p>in case <code>groups</code> is provided: a sparse matrix containing summed information over the groups of rows/columns
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- data.frame(
 doc_id = c(1, 1, 2, 3, 4), 
 term = c("A", "C", "Z", "X", "G"), 
 freq = c(1, 5, 7, 10, 0))
dtm &lt;- document_term_matrix(x)
x &lt;- dtm_colsums(dtm)
x
x &lt;- dtm_rowsums(dtm)
head(x)

## 
## Grouped column summation
## 
x &lt;- list(doc1 = c("aa", "bb", "aa", "b"), doc2 = c("bb", "bb", "BB"))
dtm &lt;- document_term_matrix(x)
dtm
dtm_colsums(dtm, groups = list(combinedB = c("b", "bb"), combinedA = c("aa", "A")))
dtm_colsums(dtm, groups = list(combinedA = c("aa", "A")))
dtm_colsums(dtm, groups = list(
  combinedB = grep(pattern = "b", colnames(dtm), ignore.case = TRUE, value = TRUE), 
  combinedA = c("aa", "A", "ZZZ"),
  test      = character()))
dtm_colsums(dtm, groups = list())

## 
## Grouped row summation
## 
x &lt;- list(doc1 = c("aa", "bb", "aa", "b"), 
          doc2 = c("bb", "bb", "BB"),
          doc3 = c("bb", "bb", "BB"),
          doc4 = c("bb", "bb", "BB", "b"))
dtm &lt;- document_term_matrix(x)
dtm
dtm_rowsums(dtm, groups = list(doc1 = "doc1", combi = c("doc2", "doc3", "doc4")))
dtm_rowsums(dtm, groups = list(unknown = "docUnknown", combi = c("doc2", "doc3", "doc4")))
dtm_rowsums(dtm, groups = list())
</code></pre>

<hr>
<h2 id='dtm_conform'>Make sure a document term matrix has exactly the specified rows and columns</h2><span id='topic+dtm_conform'></span>

<h3>Description</h3>

<p>Makes sure the document term matrix has exactly the rows and columns which you specify. If missing rows or columns
are occurring, the function fills these up either with empty cells or with the value that you provide. See the examples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dtm_conform(dtm, rows, columns, fill)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dtm_conform_+3A_dtm">dtm</code></td>
<td>
<p>a document term matrix: an object returned by <code><a href="#topic+document_term_matrix">document_term_matrix</a></code></p>
</td></tr>
<tr><td><code id="dtm_conform_+3A_rows">rows</code></td>
<td>
<p>a character vector of row names which <code>dtm</code> should have</p>
</td></tr>
<tr><td><code id="dtm_conform_+3A_columns">columns</code></td>
<td>
<p>a character vector of column names which <code>dtm</code> should have</p>
</td></tr>
<tr><td><code id="dtm_conform_+3A_fill">fill</code></td>
<td>
<p>a value to use to fill up missing rows / columns. Defaults to using an empty cell.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the sparse matrix <code>dtm</code> with exactly the specified rows and columns
</p>


<h3>See Also</h3>

<p><code><a href="#topic+document_term_matrix">document_term_matrix</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- data.frame(doc_id = c("doc_1", "doc_1", "doc_1", "doc_2"), 
                text = c("a", "a", "b", "c"), 
                stringsAsFactors = FALSE)
dtm &lt;- document_term_frequencies(x)
dtm &lt;- document_term_matrix(dtm)
dtm
dtm_conform(dtm, 
            rows = c("doc_1", "doc_2", "doc_3"), columns = c("a", "b", "c", "Z", "Y"))
dtm_conform(dtm, 
            rows = c("doc_1", "doc_2", "doc_3"), columns = c("a", "b", "c", "Z", "Y"), 
            fill = 1)
dtm_conform(dtm, rows = c("doc_1", "doc_3"), columns = c("a", "b", "c", "Z", "Y"))
dtm_conform(dtm, columns = c("a", "b", "Z"))
dtm_conform(dtm, rows = c("doc_1"))
dtm_conform(dtm, rows = character())
dtm_conform(dtm, columns = character())
dtm_conform(dtm, rows = character(), columns = character())

##
## Some examples on border line cases
##
special1 &lt;- dtm[, character()]
special2 &lt;- dtm[character(), character()]
special3 &lt;- dtm[character(), ]

dtm_conform(special1, 
            rows = c("doc_1", "doc_2", "doc_3"), columns = c("a", "b", "c", "Z", "Y"))
dtm_conform(special1, 
            rows = c("doc_1", "doc_2", "doc_3"), columns = c("a", "b", "c", "Z", "Y"), 
            fill = 1)
dtm_conform(special1, rows = c("doc_1", "doc_3"), columns = c("a", "b", "c", "Z", "Y"))
dtm_conform(special1, columns = c("a", "b", "Z"))
dtm_conform(special1, rows = c("doc_1"))
dtm_conform(special1, rows = character())
dtm_conform(special1, columns = character())
dtm_conform(special1, rows = character(), columns = character())

dtm_conform(special2, 
            rows = c("doc_1", "doc_2", "doc_3"), columns = c("a", "b", "c", "Z", "Y"))
dtm_conform(special2, 
            rows = c("doc_1", "doc_2", "doc_3"), columns = c("a", "b", "c", "Z", "Y"), 
            fill = 1)
dtm_conform(special2, rows = c("doc_1", "doc_3"), columns = c("a", "b", "c", "Z", "Y"))
dtm_conform(special2, columns = c("a", "b", "Z"))
dtm_conform(special2, rows = c("doc_1"))
dtm_conform(special2, rows = character())
dtm_conform(special2, columns = character())
dtm_conform(special2, rows = character(), columns = character())

dtm_conform(special3, 
            rows = c("doc_1", "doc_2", "doc_3"), columns = c("a", "b", "c", "Z", "Y"))
dtm_conform(special3, 
            rows = c("doc_1", "doc_2", "doc_3"), columns = c("a", "b", "c", "Z", "Y"), 
            fill = 1)
dtm_conform(special3, rows = c("doc_1", "doc_3"), columns = c("a", "b", "c", "Z", "Y"))
dtm_conform(special3, columns = c("a", "b", "Z"))
dtm_conform(special3, rows = c("doc_1"))
dtm_conform(special3, rows = character())
dtm_conform(special3, columns = character())
dtm_conform(special3, rows = character(), columns = character())
</code></pre>

<hr>
<h2 id='dtm_cor'>Pearson Correlation for Sparse Matrices</h2><span id='topic+dtm_cor'></span>

<h3>Description</h3>

<p>Pearson Correlation for Sparse Matrices. 
More memory and time-efficient than <code>cor(as.matrix(x))</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dtm_cor(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dtm_cor_+3A_x">x</code></td>
<td>
<p>A matrix, potentially a sparse matrix such as a &quot;dgCMatrix&quot; object 
which is returned by <code><a href="#topic+document_term_matrix">document_term_matrix</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>a correlation matrix
</p>


<h3>See Also</h3>

<p><code><a href="#topic+document_term_matrix">document_term_matrix</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- data.frame(
 doc_id = c(1, 1, 2, 3, 4), 
 term = c("A", "C", "Z", "X", "G"), 
 freq = c(1, 5, 7, 10, 0))
dtm &lt;- document_term_matrix(x)
dtm_cor(dtm)
</code></pre>

<hr>
<h2 id='dtm_remove_lowfreq'>Remove terms occurring with low frequency from a Document-Term-Matrix and documents with no terms</h2><span id='topic+dtm_remove_lowfreq'></span>

<h3>Description</h3>

<p>Remove terms occurring with low frequency from a Document-Term-Matrix and documents with no terms
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dtm_remove_lowfreq(dtm, minfreq = 5, maxterms, remove_emptydocs = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dtm_remove_lowfreq_+3A_dtm">dtm</code></td>
<td>
<p>an object returned by <code><a href="#topic+document_term_matrix">document_term_matrix</a></code></p>
</td></tr>
<tr><td><code id="dtm_remove_lowfreq_+3A_minfreq">minfreq</code></td>
<td>
<p>integer with the minimum number of times the term should occur in order to keep the term</p>
</td></tr>
<tr><td><code id="dtm_remove_lowfreq_+3A_maxterms">maxterms</code></td>
<td>
<p>integer indicating the maximum number of terms which should be kept in the <code>dtm</code>. The argument is optional.</p>
</td></tr>
<tr><td><code id="dtm_remove_lowfreq_+3A_remove_emptydocs">remove_emptydocs</code></td>
<td>
<p>logical indicating to remove documents containing no more terms after the term removal is executed. Defaults to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a sparse Matrix as returned by <code>sparseMatrix</code> 
where terms with low occurrence are removed and documents without any terms are also removed
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(brussels_reviews_anno)
x &lt;- subset(brussels_reviews_anno, xpos == "NN")
x &lt;- x[, c("doc_id", "lemma")]
x &lt;- document_term_frequencies(x)
dtm &lt;- document_term_matrix(x)


## Remove terms with low frequencies and documents with no terms
x &lt;- dtm_remove_lowfreq(dtm, minfreq = 10)
dim(x)
x &lt;- dtm_remove_lowfreq(dtm, minfreq = 10, maxterms = 25)
dim(x)
x &lt;- dtm_remove_lowfreq(dtm, minfreq = 10, maxterms = 25, remove_emptydocs = FALSE)
dim(x)
</code></pre>

<hr>
<h2 id='dtm_remove_sparseterms'>Remove terms with high sparsity from a Document-Term-Matrix</h2><span id='topic+dtm_remove_sparseterms'></span>

<h3>Description</h3>

<p>Remove terms with high sparsity from a Document-Term-Matrix and remove documents with no terms.<br />
Sparsity indicates in how many documents the term is not occurring.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dtm_remove_sparseterms(dtm, sparsity = 0.99, remove_emptydocs = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dtm_remove_sparseterms_+3A_dtm">dtm</code></td>
<td>
<p>an object returned by <code><a href="#topic+document_term_matrix">document_term_matrix</a></code></p>
</td></tr>
<tr><td><code id="dtm_remove_sparseterms_+3A_sparsity">sparsity</code></td>
<td>
<p>numeric in 0-1 range indicating the sparsity percent. Defaults to 0.99 meaning drop terms which occur in less than 1 percent of the documents.</p>
</td></tr>
<tr><td><code id="dtm_remove_sparseterms_+3A_remove_emptydocs">remove_emptydocs</code></td>
<td>
<p>logical indicating to remove documents containing no more terms after the term removal is executed. Defaults to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a sparse Matrix as returned by <code>sparseMatrix</code> 
where terms with high sparsity are removed and documents without any terms are also removed
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(brussels_reviews_anno)
x &lt;- subset(brussels_reviews_anno, xpos == "NN")
x &lt;- x[, c("doc_id", "lemma")]
x &lt;- document_term_frequencies(x)
dtm &lt;- document_term_matrix(x)


## Remove terms with low frequencies and documents with no terms
x &lt;- dtm_remove_sparseterms(dtm, sparsity = 0.99)
dim(x)
x &lt;- dtm_remove_sparseterms(dtm, sparsity = 0.99, remove_emptydocs = FALSE)
dim(x)
</code></pre>

<hr>
<h2 id='dtm_remove_terms'>Remove terms from a Document-Term-Matrix and keep only documents which have a least some terms</h2><span id='topic+dtm_remove_terms'></span>

<h3>Description</h3>

<p>Remove terms from a Document-Term-Matrix and keep only documents which have a least some terms
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dtm_remove_terms(dtm, terms, remove_emptydocs = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dtm_remove_terms_+3A_dtm">dtm</code></td>
<td>
<p>an object returned by <code><a href="#topic+document_term_matrix">document_term_matrix</a></code></p>
</td></tr>
<tr><td><code id="dtm_remove_terms_+3A_terms">terms</code></td>
<td>
<p>a character vector of terms which are in <code>colnames(dtm)</code> and which should be removed</p>
</td></tr>
<tr><td><code id="dtm_remove_terms_+3A_remove_emptydocs">remove_emptydocs</code></td>
<td>
<p>logical indicating to remove documents containing no more terms after the term removal is executed. Defaults to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a sparse Matrix as returned by <code>sparseMatrix</code> 
where the indicated terms are removed as well as documents with no terms whatsoever
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(brussels_reviews_anno)
x &lt;- subset(brussels_reviews_anno, xpos == "NN")
x &lt;- x[, c("doc_id", "lemma")]
x &lt;- document_term_frequencies(x)
dtm &lt;- document_term_matrix(x)
dim(dtm)
x &lt;- dtm_remove_terms(dtm, terms = c("appartement", "casa", "centrum", "ciudad"))
dim(x)
x &lt;- dtm_remove_terms(dtm, terms = c("appartement", "casa", "centrum", "ciudad"), 
                      remove_emptydocs = FALSE)
dim(x)
</code></pre>

<hr>
<h2 id='dtm_remove_tfidf'>Remove terms from a Document-Term-Matrix and documents with no terms based on the term frequency inverse document frequency</h2><span id='topic+dtm_remove_tfidf'></span>

<h3>Description</h3>

<p>Remove terms from a Document-Term-Matrix and documents with no terms based on the term frequency inverse document frequency.
Either giving in the maximum number of terms (argument <code>top</code>), the tfidf cutoff (argument <code>cutoff</code>)
or a quantile (argument <code>prob</code>)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dtm_remove_tfidf(dtm, top, cutoff, prob, remove_emptydocs = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dtm_remove_tfidf_+3A_dtm">dtm</code></td>
<td>
<p>an object returned by <code><a href="#topic+document_term_matrix">document_term_matrix</a></code></p>
</td></tr>
<tr><td><code id="dtm_remove_tfidf_+3A_top">top</code></td>
<td>
<p>integer with the number of terms which should be kept as defined by the highest mean tfidf</p>
</td></tr>
<tr><td><code id="dtm_remove_tfidf_+3A_cutoff">cutoff</code></td>
<td>
<p>numeric cutoff value to keep only terms in <code>dtm</code> where the tfidf obtained by <code>dtm_tfidf</code> is higher than this value</p>
</td></tr>
<tr><td><code id="dtm_remove_tfidf_+3A_prob">prob</code></td>
<td>
<p>numeric quantile indicating to keep only terms in <code>dtm</code> where the tfidf obtained by <code>dtm_tfidf</code> is higher than 
the <code>prob</code> percent quantile</p>
</td></tr>
<tr><td><code id="dtm_remove_tfidf_+3A_remove_emptydocs">remove_emptydocs</code></td>
<td>
<p>logical indicating to remove documents containing no more terms after the term removal is executed. Defaults to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a sparse Matrix as returned by <code>sparseMatrix</code> 
where terms with high tfidf are kept and documents without any remaining terms are removed
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(brussels_reviews_anno)
x &lt;- subset(brussels_reviews_anno, xpos == "NN")
x &lt;- x[, c("doc_id", "lemma")]
x &lt;- document_term_frequencies(x)
dtm &lt;- document_term_matrix(x)
dtm &lt;- dtm_remove_lowfreq(dtm, minfreq = 10)
dim(dtm)

## Keep only terms with high tfidf
x &lt;- dtm_remove_tfidf(dtm, top=50)
dim(x)
x &lt;- dtm_remove_tfidf(dtm, top=50, remove_emptydocs = FALSE)
dim(x)

## Keep only terms with tfidf above 1.1
x &lt;- dtm_remove_tfidf(dtm, cutoff=1.1)
dim(x)

## Keep only terms with tfidf above the 60 percent quantile
x &lt;- dtm_remove_tfidf(dtm, prob=0.6)
dim(x)
</code></pre>

<hr>
<h2 id='dtm_reverse'>Inverse operation of the document_term_matrix function</h2><span id='topic+dtm_reverse'></span>

<h3>Description</h3>

<p>Inverse operation of the <code><a href="#topic+document_term_matrix">document_term_matrix</a></code> function. 
Creates frequency table which contains 1 row per document/term
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dtm_reverse(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dtm_reverse_+3A_x">x</code></td>
<td>
<p>an object as returned by <code><a href="#topic+document_term_matrix">document_term_matrix</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame with columns doc_id, term and freq where freq is just the value in each
cell of the <code>x</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+document_term_matrix">document_term_matrix</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- data.frame(
 doc_id = c(1, 1, 2, 3, 4), 
 term = c("A", "C", "Z", "X", "G"), 
 freq = c(1, 5, 7, 10, 0))
dtm &lt;- document_term_matrix(x)
dtm_reverse(dtm)
</code></pre>

<hr>
<h2 id='dtm_sample'>Random samples and permutations from a Document-Term-Matrix</h2><span id='topic+dtm_sample'></span>

<h3>Description</h3>

<p>Sample the specified number of rows from the Document-Term-Matrix using either with or without replacement.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dtm_sample(dtm, size = nrow(dtm), replace = FALSE, prob = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dtm_sample_+3A_dtm">dtm</code></td>
<td>
<p>a document term matrix of class dgCMatrix (which can be an object returned by <code><a href="#topic+document_term_matrix">document_term_matrix</a></code>)</p>
</td></tr>
<tr><td><code id="dtm_sample_+3A_size">size</code></td>
<td>
<p>a positive number, the number of rows to sample</p>
</td></tr>
<tr><td><code id="dtm_sample_+3A_replace">replace</code></td>
<td>
<p>should sampling be with replacement</p>
</td></tr>
<tr><td><code id="dtm_sample_+3A_prob">prob</code></td>
<td>
<p>a vector of probability weights, one for each row of <code>x</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>dtm</code> with as many rows as specified in <code>size</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- list(doc1 = c("aa", "bb", "cc", "aa", "b"), 
          doc2 = c("bb", "bb", "dd", ""), 
          doc3 = character(),
          doc4 = c("cc", NA), 
          doc5 = character())
dtm &lt;- document_term_matrix(x)
dtm_sample(dtm, size = 2)
dtm_sample(dtm, size = 3)
dtm_sample(dtm, size = 2)
dtm_sample(dtm, size = 8, replace = TRUE)
dtm_sample(dtm, size = 8, replace = TRUE, prob = c(1, 1, 0.01, 0.5, 0.01))
</code></pre>

<hr>
<h2 id='dtm_svd_similarity'>Semantic Similarity to a Singular Value Decomposition</h2><span id='topic+dtm_svd_similarity'></span>

<h3>Description</h3>

<p>Calculate the similarity of a document term matrix to a set of terms based on 
a Singular Value Decomposition (SVD) embedding matrix.<br />
This can be used to easily construct a sentiment score based on the latent scale defined by a set of positive or negative terms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dtm_svd_similarity(
  dtm,
  embedding,
  weights,
  terminology = rownames(embedding),
  type = c("cosine", "dot")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dtm_svd_similarity_+3A_dtm">dtm</code></td>
<td>
<p>a sparse matrix such as a &quot;dgCMatrix&quot; object which is returned by <code><a href="#topic+document_term_matrix">document_term_matrix</a></code> containing frequencies of terms for each document</p>
</td></tr>
<tr><td><code id="dtm_svd_similarity_+3A_embedding">embedding</code></td>
<td>
<p>a matrix containing the <code>v</code> element from an singular value decomposition with the right singular vectors. 
The rownames of that matrix should contain terms which are available in the <code>colnames(dtm)</code>. See the examples.</p>
</td></tr>
<tr><td><code id="dtm_svd_similarity_+3A_weights">weights</code></td>
<td>
<p>a numeric vector with weights giving your definition of which terms are positive or negative, 
The names of this vector should be terms available in the rownames of the embedding matrix. See the examples.</p>
</td></tr>
<tr><td><code id="dtm_svd_similarity_+3A_terminology">terminology</code></td>
<td>
<p>a character vector of terms to limit the calculation of the similarity for the <code>dtm</code> to the linear combination of the weights. 
Defaults to all terms from the <code>embedding</code> matrix.</p>
</td></tr>
<tr><td><code id="dtm_svd_similarity_+3A_type">type</code></td>
<td>
<p>either 'cosine' or 'dot' indicating to respectively calculate cosine similarities or inner product similarities between the <code>dtm</code> and the SVD embedding space. Defaults to 'cosine'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an object of class 'svd_similarity' which is a list with elements
</p>

<ul>
<li><p> weights: The weights used. These are scaled to sum up to 1 as well on the positive as the negative side
</p>
</li>
<li><p> type: The type of similarity calculated (either 'cosine' or 'dot')
</p>
</li>
<li><p> terminology: A data.frame with columns term, freq and similarity where similarity indicates 
the similarity between the term and the SVD embedding space of the weights and freq is how frequently the term occurs in the <code>dtm</code>. 
This dataset is sorted in descending order by similarity.
</p>
</li>
<li><p> similarity: A data.frame with columns doc_id and similarity indicating the similarity between
the <code>dtm</code> and the SVD embedding space of the weights. The doc_id is the identifier taken from the rownames of <code>dtm</code>.
</p>
</li>
<li><p> scale: A list with elements terminology and weights 
indicating respectively the similarity in the SVD embedding space
between the <code>terminology</code> and each of the weights and between the weight terms itself
</p>
</li></ul>



<h3>See Also</h3>

<p><a href="https://en.wikipedia.org/wiki/Latent_semantic_analysis">https://en.wikipedia.org/wiki/Latent_semantic_analysis</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("brussels_reviews_anno", package = "udpipe")
x &lt;- subset(brussels_reviews_anno, language %in% "nl" &amp; (upos %in% "ADJ" | lemma %in% "niet"))
dtm &lt;- document_term_frequencies(x, document = "doc_id", term = "lemma")
dtm &lt;- document_term_matrix(dtm)
dtm &lt;- dtm_remove_lowfreq(dtm, minfreq = 3)

## Function performing Singular Value Decomposition on sparse/dense data
dtm_svd &lt;- function(dtm, dim = 5, type = c("RSpectra", "svd"), ...){
  type &lt;- match.arg(type)
  if(type == "svd"){
    SVD &lt;- svd(dtm, nu = 0, nv = dim, ...)
  }else if(type == "RSpectra"){
    #Uncomment this if you want to use the faster sparse SVD by RSpectra
    #SVD &lt;- RSpectra::svds(dtm, nu = 0, k = dim, ...)
  }
  rownames(SVD$v) &lt;- colnames(dtm)
  SVD$v
}
#embedding &lt;- dtm_svd(dtm, dim = 5)
embedding &lt;- dtm_svd(dtm, dim = 5, type = "svd")

## Define positive / negative terms and calculate the similarity to these
weights &lt;- setNames(c(1, 1, 1, 1, -1, -1, -1, -1),
                    c("fantastisch", "schoon", "vriendelijk", "net",
                      "lawaaiig", "lastig", "niet", "slecht"))
scores &lt;- dtm_svd_similarity(dtm, embedding = embedding, weights = weights)
scores
str(scores$similarity)
hist(scores$similarity$similarity)

plot(scores$terminology$similarity_weight, log(scores$terminology$freq), 
     type = "n")
text(scores$terminology$similarity_weight, log(scores$terminology$freq), 
     labels = scores$terminology$term)
     
## Not run: 
## More elaborate example using word2vec
## building word2vec model on all Dutch texts, 
## finding similarity of dtm to adjectives only
set.seed(123)
library(word2vec)
text      &lt;- subset(brussels_reviews_anno, language == "nl")
text      &lt;- paste.data.frame(text, term = "lemma", group = "doc_id")
text      &lt;- text$lemma
model     &lt;- word2vec(text, dim = 10, iter = 20, type = "cbow", min_count = 1)
predict(model, newdata = names(weights), type = "nearest", top_n = 3)
embedding &lt;- as.matrix(model)

## End(Not run)
data(brussels_reviews_w2v_embeddings_lemma_nl)
embedding &lt;- brussels_reviews_w2v_embeddings_lemma_nl
adjective &lt;- subset(brussels_reviews_anno, language %in% "nl" &amp; upos %in% "ADJ")
adjective &lt;- txt_freq(adjective$lemma)
adjective &lt;- subset(adjective, freq &gt;= 5 &amp; nchar(key) &gt; 1)
adjective &lt;- adjective$key

scores    &lt;- dtm_svd_similarity(dtm, embedding, weights = weights, type = "dot", 
                                terminology = adjective)
scores
plot(scores$terminology$similarity_weight, log(scores$terminology$freq), 
     type = "n")
text(scores$terminology$similarity_weight, log(scores$terminology$freq), 
     labels = scores$terminology$term, cex = 0.8)
</code></pre>

<hr>
<h2 id='dtm_tfidf'>Term Frequency - Inverse Document Frequency calculation</h2><span id='topic+dtm_tfidf'></span>

<h3>Description</h3>

<p>Term Frequency - Inverse Document Frequency calculation.
Averaged by each term.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dtm_tfidf(dtm)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dtm_tfidf_+3A_dtm">dtm</code></td>
<td>
<p>an object returned by <code><a href="#topic+document_term_matrix">document_term_matrix</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector with tfidf values, one for each term in the <code>dtm</code> matrix
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(brussels_reviews_anno)
x &lt;- subset(brussels_reviews_anno, xpos == "NN")
x &lt;- x[, c("doc_id", "lemma")]
x &lt;- document_term_frequencies(x)
dtm &lt;- document_term_matrix(x)

## Calculate tfidf
tfidf &lt;- dtm_tfidf(dtm)
hist(tfidf, breaks = "scott")
head(sort(tfidf, decreasing = TRUE))
head(sort(tfidf, decreasing = FALSE))
</code></pre>

<hr>
<h2 id='keywords_collocation'>Extract collocations - a sequence of terms which follow each other</h2><span id='topic+keywords_collocation'></span><span id='topic+collocation'></span>

<h3>Description</h3>

<p>Collocations are a sequence of words or terms that co-occur more often than would be expected by chance.
Common collocation are adjectives + nouns, nouns followed by nouns, verbs and nouns, adverbs and adjectives,
verbs and prepositional phrases or verbs and adverbs.<br />
This function extracts relevant collocations and computes the following statistics on them
which are indicators of how likely two terms are collocated compared to being independent.
</p>

<ul>
<li><p> PMI (pointwise mutual information): log2(P(w1w2) / P(w1) P(w2))
</p>
</li>
<li><p> MD (mutual dependency): log2(P(w1w2)^2 / P(w1) P(w2))
</p>
</li>
<li><p> LFMD (log-frequency biased mutual dependency): MD + log2(P(w1w2))
</p>
</li></ul>

<p>As natural language is non random - otherwise you wouldn't understand what I'm saying, 
most of the combinations of terms are significant. That's why these indicators of collocation
are merely used to order the collocations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>keywords_collocation(x, term, group, ngram_max = 2, n_min = 2, sep = " ")

collocation(x, term, group, ngram_max = 2, n_min = 2, sep = " ")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="keywords_collocation_+3A_x">x</code></td>
<td>
<p>a data.frame with one row per term where the sequence of the terms correspond to 
the natural order of a text. The data frame <code>x</code> should also contain 
the columns provided in <code>term</code> and <code>group</code></p>
</td></tr>
<tr><td><code id="keywords_collocation_+3A_term">term</code></td>
<td>
<p>a character vector with 1 column from <code>x</code> which indicates the term</p>
</td></tr>
<tr><td><code id="keywords_collocation_+3A_group">group</code></td>
<td>
<p>a character vector with 1 or several columns from <code>x</code> which indicates 
for example a document id or a sentence id. Collocations will be computed within this 
group in order not to find collocations across sentences or documents for example.</p>
</td></tr>
<tr><td><code id="keywords_collocation_+3A_ngram_max">ngram_max</code></td>
<td>
<p>integer indicating the size of the collocations. Defaults to 2, indicating
to compute bigrams. If set to 3, will find collocations of bigrams and trigrams.</p>
</td></tr>
<tr><td><code id="keywords_collocation_+3A_n_min">n_min</code></td>
<td>
<p>integer indicating the frequency of how many times a collocation should
at least occur in the data in order to be returned. Defaults to 2.</p>
</td></tr>
<tr><td><code id="keywords_collocation_+3A_sep">sep</code></td>
<td>
<p>character string with the separator which will be used to <code>paste</code> together
terms which are collocated. Defaults to a space: ' '.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame with columns 
</p>

<ul>
<li><p> keyword: the terms which are combined as a collocation
</p>
</li>
<li><p> ngram: the number of terms which are combined
</p>
</li>
<li><p> left: the left term of the collocation
</p>
</li>
<li><p> right: the right term of the collocation
</p>
</li>
<li><p> freq: the number of times the collocation occurred in the data
</p>
</li>
<li><p> freq_left: the number of times the left element of the collocation occurred in the data
</p>
</li>
<li><p> freq_right: the number of times the right element of the collocation occurred in the data
</p>
</li>
<li><p> pmi: the pointwise mutual information
</p>
</li>
<li><p> md: mutual dependency
</p>
</li>
<li><p> lfmd: log-frequency biased mutual dependency
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
data(brussels_reviews_anno)
x      &lt;- subset(brussels_reviews_anno, language %in% "fr")
colloc &lt;- keywords_collocation(x, term = "lemma", group = c("doc_id", "sentence_id"), 
                               ngram_max = 3, n_min = 10)
head(colloc, 10)

## Example on finding collocations of nouns preceded by an adjective
library(data.table)
x &lt;- as.data.table(x)
x &lt;- x[, xpos_previous := txt_previous(xpos, n = 1), by = list(doc_id, sentence_id)]
x &lt;- x[, xpos_next     := txt_next(xpos, n = 1),     by = list(doc_id, sentence_id)]
x &lt;- subset(x, (xpos %in% c("NN") &amp; xpos_previous %in% c("JJ")) | 
               (xpos %in% c("JJ") &amp; xpos_next %in% c("NN")))
colloc &lt;- keywords_collocation(x, term = "lemma", group = c("doc_id", "sentence_id"), 
                               ngram_max = 2, n_min = 2)
head(colloc)
</code></pre>

<hr>
<h2 id='keywords_phrases'>Extract phrases - a sequence of terms which follow each other based on a sequence of Parts of Speech tags</h2><span id='topic+keywords_phrases'></span><span id='topic+phrases'></span>

<h3>Description</h3>

<p>This function allows to extract phrases, like simple noun phrases, complex noun phrases
or any exact sequence of parts of speech tag patterns.<br />
An example use case of this is to get all text where an adjective is followed by a noun or
for example to get all phrases consisting of a preposition which is followed by a noun which is next followed by a verb.
More complex patterns are shown in the details below.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>keywords_phrases(
  x,
  term = x,
  pattern,
  is_regex = FALSE,
  sep = " ",
  ngram_max = 8,
  detailed = TRUE
)

phrases(
  x,
  term = x,
  pattern,
  is_regex = FALSE,
  sep = " ",
  ngram_max = 8,
  detailed = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="keywords_phrases_+3A_x">x</code></td>
<td>
<p>a character vector of Parts of Speech tags where we want to locate a relevant sequence of POS tags as defined in <code>pattern</code></p>
</td></tr>
<tr><td><code id="keywords_phrases_+3A_term">term</code></td>
<td>
<p>a character vector of the same length as <code>x</code> with the words or terms corresponding to the tags in <code>x</code></p>
</td></tr>
<tr><td><code id="keywords_phrases_+3A_pattern">pattern</code></td>
<td>
<p>In case <code>is_regex</code> is set to FALSE, <code>pattern</code> should be a character vector with a sequence of POS tags 
to identify in <code>x</code>. The length of the character vector should be bigger than 1.<br />
In case <code>is_regex</code> is set to TRUE, this should be a regular expressions which will be used on a concatenated version 
of <code>x</code> to identify the locations where these regular expression occur. See the examples below.</p>
</td></tr>
<tr><td><code id="keywords_phrases_+3A_is_regex">is_regex</code></td>
<td>
<p>logical indicating if <code>pattern</code> can be considered as a regular expression or if it is just
a character vector of POS tags. Defaults to FALSE, indicating <code>pattern</code> is not a regular expression.</p>
</td></tr>
<tr><td><code id="keywords_phrases_+3A_sep">sep</code></td>
<td>
<p>character indicating how to collapse the phrase of terms which are found. Defaults to using a space.</p>
</td></tr>
<tr><td><code id="keywords_phrases_+3A_ngram_max">ngram_max</code></td>
<td>
<p>an integer indicating to allow phrases to be found up to <code>ngram</code> maximum number of terms following each other. Only 
used if is_regex is set to TRUE. Defaults to 8.</p>
</td></tr>
<tr><td><code id="keywords_phrases_+3A_detailed">detailed</code></td>
<td>
<p>logical indicating to return the exact positions where the phrase was found (set to <code>TRUE</code>) or just how many times each phrase is occurring (set to <code>FALSE</code>). 
Defaults to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Common phrases which you might be interested in and which can be supplied to <code>pattern</code> are
</p>

<ul>
<li><p> Simple noun phrase: &quot;(A|N)*N(P+D*(A|N)*N)*&quot;
</p>
</li>
<li><p> Simple verb Phrase: &quot;((A|N)*N(P+D*(A|N)*N)*P*(M|V)*V(M|V)*|(M|V)*V(M|V)*D*(A|N)*N(P+D*(A|N)*N)*|(M|V)*V(M|V)*(P+D*(A|N)*N)+|(A|N)*N(P+D*(A|N)*N)*P*((M|V)*V(M|V)*D*(A|N)*N(P+D*(A|N)*N)*|(M|V)*V(M|V)*(P+D*(A|N)*N)+))&quot;
</p>
</li>
<li><p> Noun hrase with coordination conjuction: &quot;((A(CA)*|N)*N((P(CP)*)+(D(CD)*)*(A(CA)*|N)*N)*(C(D(CD)*)*(A(CA)*|N)*N((P(CP)*)+(D(CD)*)*(A(CA)*|N)*N)*)*)&quot;
</p>
</li>
<li><p> Verb phrase with coordination conjuction: &quot;(((A(CA)*|N)*N((P(CP)*)+(D(CD)*)*(A(CA)*|N)*N)*(C(D(CD)*)*(A(CA)*|N)*N((P(CP)*)+(D(CD)*)*(A(CA)*|N)*N)*)*)(P(CP)*)*(M(CM)*|V)*V(M(CM)*|V)*(C(M(CM)*|V)*V(M(CM)*|V)*)*|(M(CM)*|V)*V(M(CM)*|V)*(C(M(CM)*|V)*V(M(CM)*|V)*)*(D(CD)*)*((A(CA)*|N)*N((P(CP)*)+(D(CD)*)*(A(CA)*|N)*N)*(C(D(CD)*)*(A(CA)*|N)*N((P(CP)*)+(D(CD)*)*(A(CA)*|N)*N)*)*)|(M(CM)*|V)*V(M(CM)*|V)*(C(M(CM)*|V)*V(M(CM)*|V)*)*((P(CP)*)+(D(CD)*)*(A(CA)*|N)*N)+|((A(CA)*|N)*N((P(CP)*)+(D(CD)*)*(A(CA)*|N)*N)*(C(D(CD)*)*(A(CA)*|N)*N((P(CP)*)+(D(CD)*)*(A(CA)*|N)*N)*)*)(P(CP)*)*((M(CM)*|V)*V(M(CM)*|V)*(C(M(CM)*|V)*V(M(CM)*|V)*)*(D(CD)*)*((A(CA)*|N)*N((P(CP)*)+(D(CD)*)*(A(CA)*|N)*N)*(C(D(CD)*)*(A(CA)*|N)*N((P(CP)*)+(D(CD)*)*(A(CA)*|N)*N)*)*)|(M(CM)*|V)*V(M(CM)*|V)*(C(M(CM)*|V)*V(M(CM)*|V)*)*((P(CP)*)+(D(CD)*)*(A(CA)*|N)*N)+))&quot;
</p>
</li></ul>

<p>See the examples.<br />
Mark that this functionality is also implemented in the phrasemachine package where it is implemented using plain R code, 
while the implementation in this package uses a more quick Rcpp implementation for 
extracting these kind of regular expression like phrases.
</p>


<h3>Value</h3>

<p>If argument <code>detailed</code> is set to <code>TRUE</code> a data.frame with columns 
</p>

<ul>
<li><p> keyword: the phrase which corresponds to the collapsed terms of where the pattern was found
</p>
</li>
<li><p> ngram: the length of the phrase
</p>
</li>
<li><p> pattern: the pattern which was found
</p>
</li>
<li><p> start: the starting index of <code>x</code> where the pattern was found
</p>
</li>
<li><p> end: the ending index of <code>x</code> where the pattern was found
</p>
</li></ul>

<p>If argument <code>detailed</code> is set to <code>FALSE</code> will return aggregate frequency statistics in a data.frame containing the columns keyword, 
ngram and freq (how many time it is occurring)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+as_phrasemachine">as_phrasemachine</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(brussels_reviews_anno, package = "udpipe")
x &lt;- subset(brussels_reviews_anno, language %in% "fr")

## Find exactly this sequence of POS tags
np &lt;- keywords_phrases(x$xpos, pattern = c("DT", "NN", "VB", "RB", "JJ"), sep = "-")
head(np)
np &lt;- keywords_phrases(x$xpos, pattern = c("DT", "NN", "VB", "RB", "JJ"), term = x$token)
head(np)

## Find noun phrases with the following regular expression: (A|N)+N(P+D*(A|N)*N)*
x$phrase_tag &lt;- as_phrasemachine(x$xpos, type = "penn-treebank")
nounphrases &lt;- keywords_phrases(x$phrase_tag, term = x$token, 
                                pattern = "(A|N)+N(P+D*(A|N)*N)*", is_regex = TRUE, 
                                ngram_max = 4, 
                                detailed = TRUE)
head(nounphrases, 10)
head(sort(table(nounphrases$keyword), decreasing=TRUE), 20)

## Find frequent sequences of POS tags
library(data.table)
x &lt;- as.data.table(x)
x &lt;- x[, pos_sequence := txt_nextgram(x = xpos, n = 3), by = list(doc_id, sentence_id)]
tail(sort(table(x$pos_sequence)))
np &lt;- keywords_phrases(x$xpos, term = x$token, pattern = c("IN", "DT", "NN"))
head(np)
</code></pre>

<hr>
<h2 id='keywords_rake'>Keyword identification using Rapid Automatic Keyword Extraction (RAKE)</h2><span id='topic+keywords_rake'></span>

<h3>Description</h3>

<p>RAKE is a basic algorithm which tries to identify keywords in text. Keywords are 
defined as a sequence of words following one another.<br />
The algorithm goes as follows.
</p>

<ul>
<li><p> candidate keywords are extracted by looking to a contiguous sequence of words which do not contain irrelevant words
</p>
</li>
<li><p> a score is being calculated for each word which is part of any candidate keyword, this is done by
</p>

<ul>
<li><p> among the words of the candidate keywords, the algorithm looks how many times each word is occurring and how many times it co-occurs with other words
</p>
</li>
<li><p> each word gets a score which is the ratio of the word degree (how many times it co-occurs with other words) to the word frequency
</p>
</li></ul>

</li>
<li><p> a RAKE score for the full candidate keyword is calculated by summing up the scores of each of the words which define the candidate keyword
</p>
</li></ul>

<p>The resulting keywords are returned as a data.frame together with their RAKE score.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>keywords_rake(
  x,
  term,
  group,
  relevant = rep(TRUE, nrow(x)),
  ngram_max = 2,
  n_min = 2,
  sep = " "
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="keywords_rake_+3A_x">x</code></td>
<td>
<p>a data.frame with one row per term as returned by <code>as.data.frame(udpipe_annotate(...))</code></p>
</td></tr>
<tr><td><code id="keywords_rake_+3A_term">term</code></td>
<td>
<p>character string with a column in the data frame <code>x</code>, containing 1 term per row. To be used if <code>x</code> is a data.frame.</p>
</td></tr>
<tr><td><code id="keywords_rake_+3A_group">group</code></td>
<td>
<p>a character vector with 1 or several columns from <code>x</code> which indicates for example a document id or a sentence id. 
Keywords will be computed within this group in order not to find keywords across sentences or documents for example.</p>
</td></tr>
<tr><td><code id="keywords_rake_+3A_relevant">relevant</code></td>
<td>
<p>a logical vector of the same length as <code>nrow(x)</code>, indicating if the word in the corresponding row of <code>x</code> is relevant or not.
This can be used to exclude stopwords from the keywords calculation or for selecting only nouns and adjectives to 
find keywords (for example based on the Parts of Speech <code>upos</code> output from <code>udpipe_annotate</code>).<br /></p>
</td></tr>
<tr><td><code id="keywords_rake_+3A_ngram_max">ngram_max</code></td>
<td>
<p>integer indicating the maximum number of words that there should be in each keyword</p>
</td></tr>
<tr><td><code id="keywords_rake_+3A_n_min">n_min</code></td>
<td>
<p>integer indicating the frequency of how many times a keywords should
at least occur in the data in order to be returned. Defaults to 2.</p>
</td></tr>
<tr><td><code id="keywords_rake_+3A_sep">sep</code></td>
<td>
<p>character string with the separator which will be used to <code>paste</code> together
the terms which define the keywords. Defaults to a space: ' '.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame with columns keyword, ngram and rake which is ordered from low to high rake
</p>

<ul>
<li><p> keyword: the keyword
</p>
</li>
<li><p> ngram: how many terms are in the keyword
</p>
</li>
<li><p> freq: how many times did the keyword occur
</p>
</li>
<li><p> rake: the ratio of the degree to the frequency as explained in the description, summed up for all words from the keyword
</p>
</li></ul>



<h3>References</h3>

<p>Rose, Stuart &amp; Engel, Dave &amp; Cramer, Nick &amp; Cowley, Wendy. (2010). Automatic Keyword Extraction from Individual Documents. Text Mining: Applications and Theory. 1 - 20. 10.1002/9780470689646.ch1.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(brussels_reviews_anno)
x &lt;- subset(brussels_reviews_anno, language == "nl")
keywords &lt;- keywords_rake(x = x, term = "lemma", group = "doc_id", 
                          relevant = x$xpos %in% c("NN", "JJ"))
head(keywords)

x &lt;- subset(brussels_reviews_anno, language == "fr")
keywords &lt;- keywords_rake(x = x, term = "lemma", group = c("doc_id", "sentence_id"), 
                          relevant = x$xpos %in% c("NN", "JJ"), 
                          ngram_max = 10, n_min = 2, sep = "-")
head(keywords)
</code></pre>

<hr>
<h2 id='paste.data.frame'>Concatenate text of each group of data together</h2><span id='topic+paste.data.frame'></span>

<h3>Description</h3>

<p>This function is similar to <code><a href="base.html#topic+paste">paste</a></code>
but works on a data.frame, hence paste.data.frame. 
It concatenates text belonging to groups of data together in one string. 
The function is the inverse operation of <code><a href="#topic+strsplit.data.frame">strsplit.data.frame</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>paste.data.frame(data, term, group, collapse = " ")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="paste.data.frame_+3A_data">data</code></td>
<td>
<p>a data.frame or data.table</p>
</td></tr>
<tr><td><code id="paste.data.frame_+3A_term">term</code></td>
<td>
<p>a string with a column name or a character vector of column names from <code>data</code> which you want to concatenate together using <code><a href="base.html#topic+paste">paste</a></code></p>
</td></tr>
<tr><td><code id="paste.data.frame_+3A_group">group</code></td>
<td>
<p>a string with a column name or a character vector of column names from <code>data</code> indicating identifiers of groups. 
The text in <code>term</code> will be concatenated by group.</p>
</td></tr>
<tr><td><code id="paste.data.frame_+3A_collapse">collapse</code></td>
<td>
<p>a character string that you want to use to collapse the text data together. 
Defaults to a single space.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.frame with 1 row per group containing the columns from <code>group</code> and <code>term</code> 
where all the text in <code>term</code> for each group will be <code><a href="base.html#topic+paste">paste</a>-d</code> together, separated by the <code>collapse</code> argument.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+strsplit.data.frame">strsplit.data.frame</a></code>, <code><a href="base.html#topic+paste">paste</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(brussels_reviews_anno, package = "udpipe")
head(brussels_reviews_anno)
x &lt;- paste.data.frame(brussels_reviews_anno, 
                      term = "lemma", 
                      group = c("doc_id", "sentence_id"))
str(x)
x &lt;- paste.data.frame(brussels_reviews_anno, 
                      term = c("lemma", "token"), 
                      group = c("doc_id", "sentence_id"), 
                      collapse = "-")
str(x)                       
</code></pre>

<hr>
<h2 id='predict.LDA_VEM'>Predict method for an object of class LDA_VEM or class LDA_Gibbs</h2><span id='topic+predict.LDA_VEM'></span><span id='topic+predict.LDA'></span><span id='topic+predict.LDA_Gibbs'></span>

<h3>Description</h3>

<p>Gives either the predictions to which topic a document belongs or 
the term posteriors by topic indicating which terms are emitted by each topic.<br />
If you provide in <code>newdata</code> a document term matrix 
for which a document does not contain any text and hence does not have any terms with nonzero entries,
the prediction will give as topic prediction NA values (see the examples).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'LDA_VEM'
predict(
  object,
  newdata,
  type = c("topics", "terms"),
  min_posterior = -1,
  min_terms = 0,
  labels,
  ...
)

## S3 method for class 'LDA_Gibbs'
predict(
  object,
  newdata,
  type = c("topics", "terms"),
  min_posterior = -1,
  min_terms = 0,
  labels,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.LDA_VEM_+3A_object">object</code></td>
<td>
<p>an object of class LDA_VEM or LDA_Gibbs as returned by <code>LDA</code> from the topicmodels package</p>
</td></tr>
<tr><td><code id="predict.LDA_VEM_+3A_newdata">newdata</code></td>
<td>
<p>a document/term matrix containing data for which to make a prediction</p>
</td></tr>
<tr><td><code id="predict.LDA_VEM_+3A_type">type</code></td>
<td>
<p>either 'topic' or 'terms' for the topic predictions or the term posteriors</p>
</td></tr>
<tr><td><code id="predict.LDA_VEM_+3A_min_posterior">min_posterior</code></td>
<td>
<p>numeric in 0-1 range to output only terms emitted by each 
topic which have a posterior probability equal or higher than <code>min_posterior</code>. 
Only used if <code>type</code> is 'terms'. Provide -1 if you want to keep all values.</p>
</td></tr>
<tr><td><code id="predict.LDA_VEM_+3A_min_terms">min_terms</code></td>
<td>
<p>integer indicating the minimum number of terms to keep in the output when <code>type</code> is 'terms'. Defaults to 0.</p>
</td></tr>
<tr><td><code id="predict.LDA_VEM_+3A_labels">labels</code></td>
<td>
<p>a character vector of the same length as the number of topics in the topic model. 
Indicating how to label the topics. Only valid for type = 'topic'. 
Defaults to topic_prob_001 up to topic_prob_999.</p>
</td></tr>
<tr><td><code id="predict.LDA_VEM_+3A_...">...</code></td>
<td>
<p>further arguments passed on to topicmodels::posterior</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li><p>in case of type = 'topic': a data.table with columns 
doc_id, 
topic (the topic number to which the document is assigned to), 
topic_label (the topic label)
topic_prob (the posterior probability score for that topic), 
topic_probdiff_2nd (the probability score for that topic - the probability score for the 2nd highest topic) 
and the probability scores for each topic as indicated by topic_labelyourownlabel
</p>
</li>
<li><p>n case of type = 'terms': a list of data.frames with columns term and prob, 
giving the posterior probability that each term is emitted by the topic
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="topicmodels.html#topic+posterior-methods">posterior-methods</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Build document/term matrix on dutch nouns
data(brussels_reviews_anno)
data(brussels_reviews)
x &lt;- subset(brussels_reviews_anno, language == "nl")
x &lt;- subset(x, xpos %in% c("JJ"))
x &lt;- x[, c("doc_id", "lemma")]
x &lt;- document_term_frequencies(x)
dtm &lt;- document_term_matrix(x)
dtm &lt;- dtm_remove_lowfreq(dtm, minfreq = 10)
dtm &lt;- dtm_remove_tfidf(dtm, top = 100)


## Fit a topicmodel using VEM
library(topicmodels)
mymodel &lt;- LDA(x = dtm, k = 4, method = "VEM")

## Get topic terminology
terminology &lt;- predict(mymodel, type = "terms", min_posterior = 0.05, min_terms = 3)
terminology

## Get scores alongside the topic model
dtm &lt;- document_term_matrix(x, vocabulary = mymodel@terms)
scores &lt;- predict(mymodel, newdata = dtm, type = "topics")
scores &lt;- predict(mymodel, newdata = dtm, type = "topics", 
                  labels = c("mylabel1", "xyz", "app-location", "newlabel"))
head(scores)
table(scores$topic)
table(scores$topic_label)
table(scores$topic, exclude = c())
table(scores$topic_label, exclude = c())

## Fit a topicmodel using Gibbs
library(topicmodels)
mymodel &lt;- LDA(x = dtm, k = 4, method = "Gibbs")
terminology &lt;- predict(mymodel, type = "terms", min_posterior = 0.05, min_terms = 3)
scores &lt;- predict(mymodel, type = "topics", newdata = dtm)

</code></pre>

<hr>
<h2 id='strsplit.data.frame'>Obtain a tokenised data frame by splitting text alongside a regular expression</h2><span id='topic+strsplit.data.frame'></span>

<h3>Description</h3>

<p>Obtain a tokenised data frame by splitting text alongside a regular expression. 
This is the inverse operation of <code><a href="#topic+paste.data.frame">paste.data.frame</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>strsplit.data.frame(
  data,
  term,
  group,
  split = "[[:space:][:punct:][:digit:]]+",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="strsplit.data.frame_+3A_data">data</code></td>
<td>
<p>a data.frame or data.table</p>
</td></tr>
<tr><td><code id="strsplit.data.frame_+3A_term">term</code></td>
<td>
<p>a character with a column name from <code>data</code> which you want to split into tokens</p>
</td></tr>
<tr><td><code id="strsplit.data.frame_+3A_group">group</code></td>
<td>
<p>a string with a column name or a character vector of column names from <code>data</code> indicating identifiers of groups. 
The text in <code>term</code> will be split into tokens by group.</p>
</td></tr>
<tr><td><code id="strsplit.data.frame_+3A_split">split</code></td>
<td>
<p>a regular expression indicating how to split the <code>term</code> column. 
Defaults to splitting by spaces, punctuation symbols or digits. This will be passed on to <code><a href="base.html#topic+strsplit">strsplit</a></code>.</p>
</td></tr>
<tr><td><code id="strsplit.data.frame_+3A_...">...</code></td>
<td>
<p>further arguments passed on to <code><a href="base.html#topic+strsplit">strsplit</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tokenised data frame containing one row per token.<br />
This data.frame has the columns from <code>group</code> and <code>term</code> where the text in column <code>term</code>
will be split by the provided regular expression into tokens.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+paste.data.frame">paste.data.frame</a></code>, <code><a href="base.html#topic+strsplit">strsplit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(brussels_reviews, package = "udpipe")
x &lt;- strsplit.data.frame(brussels_reviews, term = "feedback", group = "id")
head(x)
x &lt;- strsplit.data.frame(brussels_reviews, 
                         term = c("feedback"), 
                         group = c("listing_id", "language"))
head(x)  
x &lt;- strsplit.data.frame(brussels_reviews, term = "feedback", group = "id", 
                         split = " ", fixed = TRUE)
head(x)                          
</code></pre>

<hr>
<h2 id='syntaxpatterns-class'>Experimental and undocumented querying of syntax patterns</h2><span id='topic+syntaxpatterns-class'></span><span id='topic+syntaxpatterns'></span>

<h3>Description</h3>

<p>Currently undocumented
</p>

<hr>
<h2 id='syntaxrelation-class'>Experimental and undocumented querying of syntax relationships</h2><span id='topic+syntaxrelation-class'></span><span id='topic+syntaxrelation'></span><span id='topic++7C+2Csyntaxrelation+2Clogical-method'></span><span id='topic++7C+2Clogical+2Csyntaxrelation-method'></span><span id='topic++26+2Csyntaxrelation+2Clogical-method'></span><span id='topic++26+2Clogical+2Csyntaxrelation-method'></span>

<h3>Description</h3>

<p>Currently undocumented
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'syntaxrelation,logical'
e1 | e2

## S4 method for signature 'logical,syntaxrelation'
e1 | e2

## S4 method for signature 'syntaxrelation,logical'
e1 &amp; e2

## S4 method for signature 'logical,syntaxrelation'
e1 &amp; e2
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="syntaxrelation-class_+3A_e1">e1</code></td>
<td>
<p>Currently undocumented</p>
</td></tr>
<tr><td><code id="syntaxrelation-class_+3A_e2">e2</code></td>
<td>
<p>Currently undocumented</p>
</td></tr>
</table>

<hr>
<h2 id='txt_collapse'>Collapse a character vector while removing missing data.</h2><span id='topic+txt_collapse'></span>

<h3>Description</h3>

<p>Collapse a character vector while removing missing data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>txt_collapse(x, collapse = " ")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="txt_collapse_+3A_x">x</code></td>
<td>
<p>a character vector or a list of character vectors</p>
</td></tr>
<tr><td><code id="txt_collapse_+3A_collapse">collapse</code></td>
<td>
<p>a character string to be used to collapse the vector. Defaults to a space: ' '.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a character vector of length 1 with the content of x collapsed using paste
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+paste">paste</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>txt_collapse(c(NA, "hello", "world", NA))

x &lt;- list(a = c("h", "i"), b = c("some", "more", "text"), 
          c = character(), d = NA)
txt_collapse(x, collapse = " ")
</code></pre>

<hr>
<h2 id='txt_contains'>Check if text contains a certain pattern</h2><span id='topic+txt_contains'></span>

<h3>Description</h3>

<p>Look up text which has a certain pattern. This pattern lookup is performed by executing a regular expression using <code><a href="base.html#topic+grepl">grepl</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>txt_contains(x, patterns, value = FALSE, ignore.case = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="txt_contains_+3A_x">x</code></td>
<td>
<p>a character vector with text</p>
</td></tr>
<tr><td><code id="txt_contains_+3A_patterns">patterns</code></td>
<td>
<p>a regular expression which might be contained in <code>x</code>, a vector of these
or a list of pattern elements where the list elements <code>include</code> and <code>exclude</code> indicate to find a pattern in <code>x</code>
while excluding elements which have another pattern</p>
</td></tr>
<tr><td><code id="txt_contains_+3A_value">value</code></td>
<td>
<p>logical, indicating to return the elements of <code>x</code> where the pattern was found or just a logical vector. Defaults to FALSE indicating to return a logical.</p>
</td></tr>
<tr><td><code id="txt_contains_+3A_ignore.case">ignore.case</code></td>
<td>
<p>logical, if set to <code>FALSE</code>, the pattern matching is case sensitive and if TRUE, case is ignored during matching. Passed on to <code><a href="base.html#topic+grepl">grepl</a></code></p>
</td></tr>
<tr><td><code id="txt_contains_+3A_...">...</code></td>
<td>
<p>other parameters which can be passed on to <code><a href="base.html#topic+grepl">grepl</a></code> e.g. fixed/perl/useBytes</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a logical vector of the same length as <code>x</code> indicating if one of the patterns was found in <code>x</code>.<br /> 
Or the vector of elements of <code>x</code> where the pattern was found in case argument <code>value</code> is set to <code>TRUE</code>
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+grepl">grepl</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c("The cats are eating catfood", 
       "Our cat is eating the catfood", 
       "the dog eats catfood, he likes it", 
       NA)
txt_contains(x, patterns = c("cat", "dog")) 
txt_contains(x, patterns = c("cat", "dog"), value = TRUE) 
txt_contains(x, patterns = c("eats"), value = TRUE) 
txt_contains(x, patterns = c("^The"), ignore.case = FALSE, value = TRUE) 
txt_contains(x, patterns = list(include = c("cat"), exclude = c("dog")), 
             value = TRUE) 
txt_contains(x, "cat") &amp; txt_contains(x, "dog")
</code></pre>

<hr>
<h2 id='txt_context'>Based on a vector with a word sequence, get n-grams (looking forward + backward)</h2><span id='topic+txt_context'></span>

<h3>Description</h3>

<p>If you have annotated your text using <code><a href="#topic+udpipe_annotate">udpipe_annotate</a></code>,
your text is tokenised in a sequence of words. Based on this vector of words in sequence
getting n-grams comes down to looking at the previous/next word and the subsequent previous/next word andsoforth.
These words can be <code>pasted</code> together to form an n-gram.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>txt_context(x, n = c(-1, 0, 1), sep = " ", na.rm = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="txt_context_+3A_x">x</code></td>
<td>
<p>a character vector where each element is just 1 term or word</p>
</td></tr>
<tr><td><code id="txt_context_+3A_n">n</code></td>
<td>
<p>an integer vector indicating how many terms to look back and ahead</p>
</td></tr>
<tr><td><code id="txt_context_+3A_sep">sep</code></td>
<td>
<p>a character element indicating how to <code><a href="base.html#topic+paste">paste</a></code> the subsequent words together</p>
</td></tr>
<tr><td><code id="txt_context_+3A_na.rm">na.rm</code></td>
<td>
<p>logical, if set to <code>TRUE</code>, will keep all text even if it can not look back/ahead the amount specified by <code>n</code>. 
If set to <code>FALSE</code>, will have a resulting value of <code>NA</code>
if at least one element is <code>NA</code> or it can not look back/ahead the amount specified by <code>n</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a character vector of the same length of <code>x</code> with the n-grams
</p>


<h3>See Also</h3>

<p><code><a href="#topic+txt_paste">txt_paste</a></code>, <code><a href="#topic+txt_next">txt_next</a></code>, <code><a href="#topic+txt_previous">txt_previous</a></code>, <code><a href="data.table.html#topic+shift">shift</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c("We", "walked", "anxiously", "to", "the", "doctor", "!")

## Look 1 word before + word itself
y &lt;- txt_context(x, n = c(-1, 0), na.rm = FALSE)
data.frame(x, y)
## Look 1 word before + word itself + 1 word after
y &lt;- txt_context(x, n = c(-1, 0, 1), na.rm = FALSE)
data.frame(x, y)
y &lt;- txt_context(x, n = c(-1, 0, 1), na.rm = TRUE)
data.frame(x, y)

## Look 2 words before + word itself + 1 word after 
## even if not all words are there
y &lt;- txt_context(x, n = c(-2, -1, 0, 1), na.rm = TRUE, sep = "_")
data.frame(x, y)
y &lt;- txt_context(x, n = c(-2, -1, 1, 2), na.rm = FALSE, sep = "_")
data.frame(x, y)

x &lt;- c("We", NA, NA, "to", "the", "doctor", "!")
y &lt;- txt_context(x, n = c(-1, 0), na.rm = FALSE)
data.frame(x, y)
y &lt;- txt_context(x, n = c(-1, 0), na.rm = TRUE)
data.frame(x, y)

library(data.table)
data(brussels_reviews_anno, package = "udpipe")
x      &lt;- as.data.table(brussels_reviews_anno)
x      &lt;- subset(x, doc_id %in% txt_sample(unique(x$doc_id), n = 10))
x      &lt;- x[, context := txt_context(lemma), by = list(doc_id, sentence_id)]
head(x, 20)
x$term &lt;- sprintf("%s/%s", x$lemma, x$upos)
x      &lt;- x[, context := txt_context(term), by = list(doc_id, sentence_id)]
head(x, 20)
</code></pre>

<hr>
<h2 id='txt_count'>Count the number of times a pattern is occurring in text</h2><span id='topic+txt_count'></span>

<h3>Description</h3>

<p>Count the number of times a pattern is occurring in text. 
Pattern counting is performed by executing a regular expression using <code><a href="base.html#topic+gregexpr">gregexpr</a></code> and 
checking how many times the regular expression occurs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>txt_count(x, pattern, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="txt_count_+3A_x">x</code></td>
<td>
<p>a character vector with text</p>
</td></tr>
<tr><td><code id="txt_count_+3A_pattern">pattern</code></td>
<td>
<p>a text pattern which might be contained in <code>x</code></p>
</td></tr>
<tr><td><code id="txt_count_+3A_...">...</code></td>
<td>
<p>other arguments, passed on to <code><a href="base.html#topic+gregexpr">gregexpr</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>an integer vector of the same length as <code>x</code> indicating how many times the pattern is occurring in <code>x</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c("abracadabra", "ababcdab", NA)
txt_count(x, pattern = "ab")
txt_count(x, pattern = "AB", ignore.case = TRUE)
txt_count(x, pattern = "AB", ignore.case = FALSE)
</code></pre>

<hr>
<h2 id='txt_freq'>Frequency statistics of elements in a vector</h2><span id='topic+txt_freq'></span>

<h3>Description</h3>

<p>Frequency statistics of elements in a vector
</p>


<h3>Usage</h3>

<pre><code class='language-R'>txt_freq(x, exclude = c(NA, NaN), order = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="txt_freq_+3A_x">x</code></td>
<td>
<p>a vector</p>
</td></tr>
<tr><td><code id="txt_freq_+3A_exclude">exclude</code></td>
<td>
<p>logical indicating to exclude values from the table. Defaults to NA and NaN.</p>
</td></tr>
<tr><td><code id="txt_freq_+3A_order">order</code></td>
<td>
<p>logical indicating to order the resulting dataset in order of frequency. Defaults to TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame with columns key, freq and freq_pct indicating the how 
many times each value in the vector <code>x</code> is occurring
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- sample(LETTERS, 1000, replace = TRUE)
txt_freq(x)
x &lt;- factor(x, levels = LETTERS)
txt_freq(x, order = FALSE)
</code></pre>

<hr>
<h2 id='txt_grepl'>Look up a multiple patterns and indicate their presence in text</h2><span id='topic+txt_grepl'></span>

<h3>Description</h3>

<p>A variant of <code><a href="base.html#topic+grepl">grepl</a></code> which allows to specify multiple regular expressions
and allows to combine the result of these into one logical vector.<br /> 
You can specify how to combine the results of the regular expressions by specifying
an aggregate function like <code><a href="base.html#topic+all">all</a></code>, <code><a href="base.html#topic+any">any</a></code>, <code><a href="base.html#topic+sum">sum</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>txt_grepl(
  x,
  pattern,
  FUN = all,
  ignore.case = FALSE,
  perl = FALSE,
  fixed = FALSE,
  useBytes = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="txt_grepl_+3A_x">x</code></td>
<td>
<p>a character vector</p>
</td></tr>
<tr><td><code id="txt_grepl_+3A_pattern">pattern</code></td>
<td>
<p>a character vector containing one or several regular expressions</p>
</td></tr>
<tr><td><code id="txt_grepl_+3A_fun">FUN</code></td>
<td>
<p>a function to apply to combine the results ot the different regular expressions for each element of <code>x</code>. 
Defaults to <code><a href="base.html#topic+all">all</a></code>.</p>
</td></tr>
<tr><td><code id="txt_grepl_+3A_ignore.case">ignore.case</code></td>
<td>
<p>passed on to <code><a href="base.html#topic+grepl">grepl</a></code></p>
</td></tr>
<tr><td><code id="txt_grepl_+3A_perl">perl</code></td>
<td>
<p>passed on to <code><a href="base.html#topic+grepl">grepl</a></code></p>
</td></tr>
<tr><td><code id="txt_grepl_+3A_fixed">fixed</code></td>
<td>
<p>passed on to <code><a href="base.html#topic+grepl">grepl</a></code></p>
</td></tr>
<tr><td><code id="txt_grepl_+3A_usebytes">useBytes</code></td>
<td>
<p>passed on to <code><a href="base.html#topic+grepl">grepl</a></code></p>
</td></tr>
<tr><td><code id="txt_grepl_+3A_...">...</code></td>
<td>
<p>further arguments passed on to <code>FUN</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>a logical vector with the same length as <code>x</code> with the result of the call to <code>FUN</code> applied elementwise to each result of grepl for each pattern
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+grepl">grepl</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c("--A--", "--B--", "--ABC--", "--AC--", "Z")
txt_grepl(x, pattern = c("A", "C"), FUN = all)
txt_grepl(x, pattern = c("A", "C"), FUN = any)
txt_grepl(x, pattern = c("A", "C"), FUN = sum)
data.frame(x = x, 
           A_and_C = txt_grepl(x, pattern = c("A", "C"), FUN = all),
           A_or_C  = txt_grepl(x, pattern = c("A", "C"), FUN = any),
           A_C_n   = txt_grepl(x, pattern = c("A", "C"), FUN = sum))
txt_grepl(x, pattern = "A|C")
</code></pre>

<hr>
<h2 id='txt_highlight'>Highlight words in a character vector</h2><span id='topic+txt_highlight'></span>

<h3>Description</h3>

<p>Highlight words in a character vector. The words provided in <code>terms</code> are 
highlighted in the text by wrapping it around the following charater: |.
So 'I like milk and sugar in my coffee' would give 'I like |milk| and sugar in my coffee' if you 
want to highlight the word milk
</p>


<h3>Usage</h3>

<pre><code class='language-R'>txt_highlight(x, terms)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="txt_highlight_+3A_x">x</code></td>
<td>
<p>a character vector with text</p>
</td></tr>
<tr><td><code id="txt_highlight_+3A_terms">terms</code></td>
<td>
<p>a vector of words to highlight which appear in <code>x</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector with the same length of <code>x</code> where the terms provided in <code>terms</code>
are put in between || to highlight them
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- "I like milk and sugar in my coffee."
txt_highlight(x, terms = "sugar")
txt_highlight(x, terms = c("milk", "my"))
</code></pre>

<hr>
<h2 id='txt_next'>Get the n-th next element of a vector</h2><span id='topic+txt_next'></span>

<h3>Description</h3>

<p>Get the n-th next element of a vector
</p>


<h3>Usage</h3>

<pre><code class='language-R'>txt_next(x, n = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="txt_next_+3A_x">x</code></td>
<td>
<p>a character vector where each element is just 1 term or word</p>
</td></tr>
<tr><td><code id="txt_next_+3A_n">n</code></td>
<td>
<p>an integer indicating how far to look next. Defaults to 1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a character vector of the same length of <code>x</code> with the next element
</p>


<h3>See Also</h3>

<p><code><a href="data.table.html#topic+shift">shift</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- sprintf("%s%s", LETTERS, 1:26)
txt_next(x, n = 1)

data.frame(word = x,
           word_next1 = txt_next(x, n = 1),
           word_next2 = txt_next(x, n = 2),
           stringsAsFactors = FALSE)
</code></pre>

<hr>
<h2 id='txt_nextgram'>Based on a vector with a word sequence, get n-grams (looking forward)</h2><span id='topic+txt_nextgram'></span>

<h3>Description</h3>

<p>If you have annotated your text using <code><a href="#topic+udpipe_annotate">udpipe_annotate</a></code>,
your text is tokenised in a sequence of words. Based on this vector of words in sequence
getting n-grams comes down to looking at the next word and the subsequent word andsoforth.
These words can be <code>pasted</code> together to form an n-gram containing
the current word, the next word up, the subsequent word, ...
</p>


<h3>Usage</h3>

<pre><code class='language-R'>txt_nextgram(x, n = 2, sep = " ")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="txt_nextgram_+3A_x">x</code></td>
<td>
<p>a character vector where each element is just 1 term or word</p>
</td></tr>
<tr><td><code id="txt_nextgram_+3A_n">n</code></td>
<td>
<p>an integer indicating the ngram. Values of 1 will keep the x, a value of 2 will
append the next term to the current term, a value of 3 will append the subsequent
term and the term following that term to the current term</p>
</td></tr>
<tr><td><code id="txt_nextgram_+3A_sep">sep</code></td>
<td>
<p>a character element indicating how to <code><a href="base.html#topic+paste">paste</a></code> the subsequent words together</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a character vector of the same length of <code>x</code> with the n-grams
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+paste">paste</a></code>, <code><a href="data.table.html#topic+shift">shift</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- sprintf("%s%s", LETTERS, 1:26)
txt_nextgram(x, n = 2)

data.frame(words = x,
           bigram = txt_nextgram(x, n = 2),
           trigram = txt_nextgram(x, n = 3, sep = "-"),
           quatrogram = txt_nextgram(x, n = 4, sep = ""),
           stringsAsFactors = FALSE)

x &lt;- c("A1", "A2", "A3", NA, "A4", "A5")
data.frame(x, 
           bigram = txt_nextgram(x, n = 2, sep = "_"),
           stringsAsFactors = FALSE)
</code></pre>

<hr>
<h2 id='txt_overlap'>Get the overlap between 2 vectors</h2><span id='topic+txt_overlap'></span>

<h3>Description</h3>

<p>Get the overlap between 2 vectors
</p>


<h3>Usage</h3>

<pre><code class='language-R'>txt_overlap(x, y)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="txt_overlap_+3A_x">x</code></td>
<td>
<p>a vector</p>
</td></tr>
<tr><td><code id="txt_overlap_+3A_y">y</code></td>
<td>
<p>a vector</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector with elements of <code>x</code> which are also found in <code>y</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c("a", "b", "c")
y &lt;- c("b", "c", "e", "z")
txt_overlap(x, y)
txt_overlap(y, x)
</code></pre>

<hr>
<h2 id='txt_paste'>Concatenate strings with options how to handle missing data</h2><span id='topic+txt_paste'></span>

<h3>Description</h3>

<p>NA friendly version for concatenating string
</p>


<h3>Usage</h3>

<pre><code class='language-R'>txt_paste(..., collapse = " ", na.rm = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="txt_paste_+3A_...">...</code></td>
<td>
<p>character vectors</p>
</td></tr>
<tr><td><code id="txt_paste_+3A_collapse">collapse</code></td>
<td>
<p>a character string to be used to paste the vectors together. Defaults to a space: ' '.</p>
</td></tr>
<tr><td><code id="txt_paste_+3A_na.rm">na.rm</code></td>
<td>
<p>logical, if set to <code>TRUE</code>, will replace NA with &rdquo;. If set to <code>FALSE</code>, will have a resulting value of NA
if at least one element is <code>NA</code>, in a similar spirit as <code>mean</code>. Defaults to <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a character vector
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+paste">paste</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c(1, 2, 3, NA, NA)
y &lt;- c("a", "b", "c", NA, "OK")
paste(x, y, sep = "-")
txt_paste(x, y, collapse = "-", na.rm = TRUE)
txt_paste(x, y, collapse = "-", na.rm = FALSE)

x &lt;- c(NA, "a", "b")
y &lt;- c("1", "2", NA)
z &lt;- c("-", "*", NA)
txt_paste(x, y, z, collapse = "", na.rm = TRUE)
txt_paste(x, y, z, "_____", collapse = "", na.rm = TRUE)
txt_paste(x, y, z, "_____", collapse = "", na.rm = FALSE)
</code></pre>

<hr>
<h2 id='txt_previous'>Get the n-th previous element of a vector</h2><span id='topic+txt_previous'></span>

<h3>Description</h3>

<p>Get the n-th previous element of a vector
</p>


<h3>Usage</h3>

<pre><code class='language-R'>txt_previous(x, n = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="txt_previous_+3A_x">x</code></td>
<td>
<p>a character vector where each element is just 1 term or word</p>
</td></tr>
<tr><td><code id="txt_previous_+3A_n">n</code></td>
<td>
<p>an integer indicating how far to look back. Defaults to 1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a character vector of the same length of <code>x</code> with the previous element
</p>


<h3>See Also</h3>

<p><code><a href="data.table.html#topic+shift">shift</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- sprintf("%s%s", LETTERS, 1:26)
txt_previous(x, n = 1)

data.frame(word = x,
           word_previous1 = txt_previous(x, n = 1),
           word_previous2 = txt_previous(x, n = 2),
           stringsAsFactors = FALSE)
</code></pre>

<hr>
<h2 id='txt_previousgram'>Based on a vector with a word sequence, get n-grams (looking backward)</h2><span id='topic+txt_previousgram'></span>

<h3>Description</h3>

<p>If you have annotated your text using <code><a href="#topic+udpipe_annotate">udpipe_annotate</a></code>,
your text is tokenised in a sequence of words. Based on this vector of words in sequence
getting n-grams comes down to looking at the previous word and the subsequent previous word andsoforth.
These words can be <code>pasted</code> together to form an n-gram containing
the second previous word, the previous word, the current word ...
</p>


<h3>Usage</h3>

<pre><code class='language-R'>txt_previousgram(x, n = 2, sep = " ")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="txt_previousgram_+3A_x">x</code></td>
<td>
<p>a character vector where each element is just 1 term or word</p>
</td></tr>
<tr><td><code id="txt_previousgram_+3A_n">n</code></td>
<td>
<p>an integer indicating the ngram. Values of 1 will keep the x, a value of 2 will
append the previous term to the current term, a value of 3 will append the second previous term
term and the previous term preceding the current term to the current term</p>
</td></tr>
<tr><td><code id="txt_previousgram_+3A_sep">sep</code></td>
<td>
<p>a character element indicating how to <code><a href="base.html#topic+paste">paste</a></code> the subsequent words together</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a character vector of the same length of <code>x</code> with the n-grams
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+paste">paste</a></code>, <code><a href="data.table.html#topic+shift">shift</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- sprintf("%s%s", LETTERS, 1:26)
txt_previousgram(x, n = 2)

data.frame(words = x,
           bigram = txt_previousgram(x, n = 2),
           trigram = txt_previousgram(x, n = 3, sep = "-"),
           quatrogram = txt_previousgram(x, n = 4, sep = ""),
           stringsAsFactors = FALSE)

x &lt;- c("A1", "A2", "A3", NA, "A4", "A5")
data.frame(x, 
           bigram = txt_previousgram(x, n = 2, sep = "_"),
           stringsAsFactors = FALSE)
</code></pre>

<hr>
<h2 id='txt_recode'>Recode text to other categories</h2><span id='topic+txt_recode'></span>

<h3>Description</h3>

<p>Recode text to other categories. 
Values of <code>x</code> which correspond to <code>from[i]</code> will be recoded to <code>to[i]</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>txt_recode(x, from = c(), to = c(), na.rm = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="txt_recode_+3A_x">x</code></td>
<td>
<p>a character vector</p>
</td></tr>
<tr><td><code id="txt_recode_+3A_from">from</code></td>
<td>
<p>a character vector with values of <code>x</code> which you want to recode</p>
</td></tr>
<tr><td><code id="txt_recode_+3A_to">to</code></td>
<td>
<p>a character vector with values of you want to use to recode to where you
want to replace values of <code>x</code> which correspond to <code>from[i]</code> to <code>to[i]</code></p>
</td></tr>
<tr><td><code id="txt_recode_+3A_na.rm">na.rm</code></td>
<td>
<p>logical, if set to TRUE, will put all values of <code>x</code> which have no
matching value in <code>from</code> to NA. Defaults to <code>FALSE</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>a character vector of the same length of <code>x</code> where values of <code>x</code>
which are given in <code>from</code> will be replaced by the corresponding element in <code>to</code>
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+match">match</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c("NOUN", "VERB", "NOUN", "ADV")
txt_recode(x = x,
           from = c("VERB", "ADV"),
           to = c("conjugated verb", "adverb"))
txt_recode(x = x,
           from = c("VERB", "ADV"),
           to = c("conjugated verb", "adverb"),
           na.rm = TRUE)
txt_recode(x = x,
           from = c("VERB", "ADV", "NOUN"),
           to = c("conjugated verb", "adverb", "noun"),
           na.rm = TRUE)
</code></pre>

<hr>
<h2 id='txt_recode_ngram'>Recode words with compound multi-word expressions</h2><span id='topic+txt_recode_ngram'></span>

<h3>Description</h3>

<p>Replace in a character vector of tokens, tokens with compound multi-word expressions.
So that <code>c("New", "York")</code> will be <code>c("New York", NA)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>txt_recode_ngram(x, compound, ngram, sep = " ")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="txt_recode_ngram_+3A_x">x</code></td>
<td>
<p>a character vector of words where you want to replace tokens with compound multi-word expressions.
This is generally a character vector as returned by the token column of <code>as.data.frame(udpipe_annotate(txt))</code></p>
</td></tr>
<tr><td><code id="txt_recode_ngram_+3A_compound">compound</code></td>
<td>
<p>a character vector of compound words multi-word expressions indicating terms which can be considered as one word. 
For example <code>c('New York', 'Brussels Hoofdstedelijk Gewest')</code>.</p>
</td></tr>
<tr><td><code id="txt_recode_ngram_+3A_ngram">ngram</code></td>
<td>
<p>a integer vector of the same length as <code>compound</code> indicating how many terms there are in the specific compound multi-word expressions
given by <code>compound</code>, where <code>compound[i]</code> contains <code>ngram[i]</code> words. 
So if <code>x</code> is <code>c('New York', 'Brussels Hoofdstedelijk Gewest')</code>, the ngram would be <code>c(2, 3)</code></p>
</td></tr>
<tr><td><code id="txt_recode_ngram_+3A_sep">sep</code></td>
<td>
<p>separator used when the compounds were constructed by combining the words together into a compound multi-word expression. Defaults to a space: ' '.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the same character vector <code>x</code> where elements in <code>x</code> will be replaced by compound multi-word expression. 
If will give preference to replacing with compounds with higher ngrams if these occur. See the examples.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+txt_nextgram">txt_nextgram</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c("I", "went", "to", "New", "York", "City", "on", "holiday", ".")
y &lt;- txt_recode_ngram(x, compound = "New York", ngram = 2, sep = " ")
data.frame(x, y)

keyw &lt;- data.frame(keyword = c("New-York", "New-York-City"), ngram = c(2, 3))
y &lt;- txt_recode_ngram(x, compound = keyw$keyword, ngram = keyw$ngram, sep = "-")
data.frame(x, y)

## Example replacing adjectives followed by a noun with the full compound word
data(brussels_reviews_anno)
x &lt;- subset(brussels_reviews_anno, language == "nl")
keyw &lt;- keywords_phrases(x$xpos, term = x$token, pattern = "JJNN", 
                         is_regex = TRUE, detailed = FALSE)
head(keyw)
x$term &lt;- txt_recode_ngram(x$token, compound = keyw$keyword, ngram = keyw$ngram)
head(x[, c("token", "term", "xpos")], 12)
</code></pre>

<hr>
<h2 id='txt_sample'>Boilerplate function to sample one element from a vector.</h2><span id='topic+txt_sample'></span>

<h3>Description</h3>

<p>Boilerplate function to sample one element from a vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>txt_sample(x, na.exclude = TRUE, n = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="txt_sample_+3A_x">x</code></td>
<td>
<p>a vector</p>
</td></tr>
<tr><td><code id="txt_sample_+3A_na.exclude">na.exclude</code></td>
<td>
<p>logical indicating to remove NA values before taking a sample</p>
</td></tr>
<tr><td><code id="txt_sample_+3A_n">n</code></td>
<td>
<p>integer indicating the number of items to sample from <code>x</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>one element sampled from the vector x
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+sample.int">sample.int</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>txt_sample(c(NA, "hello", "world", NA))
</code></pre>

<hr>
<h2 id='txt_sentiment'>Perform dictionary-based sentiment analysis on a tokenised data frame</h2><span id='topic+txt_sentiment'></span>

<h3>Description</h3>

<p>This function identifies words which have a positive/negative meaning, with the addition of some basic logic regarding occurrences of amplifiers/deamplifiers and negators
in the neighbourhood of the word which has a positive/negative meaning.
</p>

<ul>
<li><p>If a negator is occurring in the neigbourhood, positive becomes negative or vice versa.
</p>
</li>
<li><p>If amplifiers/deamplifiers occur in the neigbourhood, these amplifier weight is added to the sentiment polarity score.
</p>
</li></ul>

<p>This function took inspiration from qdap::polarity but was completely re-engineered to allow to calculate similar things on 
a udpipe-tokenised dataset. It works on a sentence level and the negator/amplification logic can not surpass a boundary defined
by the PUNCT upos parts of speech tag.<br />
</p>
<p>Note that if you prefer to build a supervised model to perform sentiment scoring 
you might be interested in looking at the ruimtehol R package <a href="https://github.com/bnosac/ruimtehol">https://github.com/bnosac/ruimtehol</a> instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>txt_sentiment(
  x,
  term = "lemma",
  polarity_terms,
  polarity_negators = character(),
  polarity_amplifiers = character(),
  polarity_deamplifiers = character(),
  amplifier_weight = 0.8,
  n_before = 4,
  n_after = 2,
  constrain = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="txt_sentiment_+3A_x">x</code></td>
<td>
<p>a data.frame with the columns doc_id, paragraph_id, sentence_id, upos and the column as indicated in <code>term</code>. This is exactly what <code><a href="#topic+udpipe">udpipe</a></code> returns.</p>
</td></tr>
<tr><td><code id="txt_sentiment_+3A_term">term</code></td>
<td>
<p>a character string with the name of a column of <code>x</code> where you want to apply to sentiment scoring upon</p>
</td></tr>
<tr><td><code id="txt_sentiment_+3A_polarity_terms">polarity_terms</code></td>
<td>
<p>data.frame containing terms which have positive or negative meaning. This data frame should contain the columns
term and polarity where term is of type character and polarity can either be 1 or -1.</p>
</td></tr>
<tr><td><code id="txt_sentiment_+3A_polarity_negators">polarity_negators</code></td>
<td>
<p>a character vector of words which will invert the meaning of the <code>polarity_terms</code> such that -1 becomes 1 and vice versa</p>
</td></tr>
<tr><td><code id="txt_sentiment_+3A_polarity_amplifiers">polarity_amplifiers</code></td>
<td>
<p>a character vector of words which amplify the <code>polarity_terms</code></p>
</td></tr>
<tr><td><code id="txt_sentiment_+3A_polarity_deamplifiers">polarity_deamplifiers</code></td>
<td>
<p>a character vector of words which deamplify the <code>polarity_terms</code></p>
</td></tr>
<tr><td><code id="txt_sentiment_+3A_amplifier_weight">amplifier_weight</code></td>
<td>
<p>weight which is added to the polarity score if an amplifier occurs in the neighbourhood</p>
</td></tr>
<tr><td><code id="txt_sentiment_+3A_n_before">n_before</code></td>
<td>
<p>integer indicating how many words before the <code>polarity_terms</code> word one has to look to find negators/amplifiers/deamplifiers to apply its logic</p>
</td></tr>
<tr><td><code id="txt_sentiment_+3A_n_after">n_after</code></td>
<td>
<p>integer indicating how many words after the <code>polarity_terms</code> word one has to look to find negators/amplifiers/deamplifiers to apply its logic</p>
</td></tr>
<tr><td><code id="txt_sentiment_+3A_constrain">constrain</code></td>
<td>
<p>logical indicating to make sure the aggregated sentiment scores is between -1 and 1</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing 
</p>

<ul>
<li><p>data: the <code>x</code> data.frame with 2 columns added: polarity and sentiment_polarity. 
</p>

<ul>
<li><p>The column polarity being just the polarity column of the <code>polarity_terms</code> dataset corresponding to the polarity of the <code>term</code> you apply the sentiment scoring 
</p>
</li>
<li><p>The colummn sentiment_polarity is the value where the amplifier/de-amplifier/negator logic is applied on. 
</p>
</li></ul>


</li>
<li><p>overall: a data.frame with one row per doc_id containing the columns doc_id, sentences,
terms, sentiment_polarity, terms_positive, terms_negative, terms_negation and terms_amplification 
providing the aggregate sentiment_polarity score of the dataset <code>x</code> by doc_id as well as 
the terminology causing the sentiment, the number of sentences and the number of non punctuation terms in the document.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c("I do not like whatsoever when an R package has soo many dependencies.",
       "Making other people install java is annoying, 
        as it is a really painful experience in classrooms.")
## Not run: 
## Do the annotation to get the data.frame needed as input to txt_sentiment
anno &lt;- udpipe(x, "english-gum")

## End(Not run)
anno &lt;- data.frame(doc_id = c(rep("doc1", 14), rep("doc2", 18)), 
                   paragraph_id = 1,
                   sentence_id = 1,
                   lemma = c("I", "do", "not", "like", "whatsoever", 
                             "when", "an", "R", "package", 
                             "has", "soo", "many", "dependencies", ".", 
                             "Making", "other", "people", "install", 
                             "java", "is", "annoying", ",", "as", 
                             "it", "is", "a", "really", "painful", 
                             "experience", "in", "classrooms", "."),
                   upos = c("PRON", "AUX", "PART", "VERB", "PRON", 
                            "SCONJ", "DET", "PROPN", "NOUN", "VERB", 
                             "ADV", "ADJ", "NOUN", "PUNCT", 
                             "VERB", "ADJ", "NOUN", "ADJ", "NOUN", 
                             "AUX", "VERB", "PUNCT", "SCONJ", "PRON", 
                             "AUX", "DET", "ADV", "ADJ", "NOUN", 
                             "ADP", "NOUN", "PUNCT"),
                   stringsasFactors = FALSE)
scores &lt;- txt_sentiment(x = anno, 
              term = "lemma",
              polarity_terms = data.frame(term = c("annoy", "like", "painful"), 
                                          polarity = c(-1, 1, -1)), 
              polarity_negators = c("not", "neither"),
              polarity_amplifiers = c("pretty", "many", "really", "whatsoever"), 
              polarity_deamplifiers = c("slightly", "somewhat"))
scores$overall
scores$data
scores &lt;- txt_sentiment(x = anno, 
              term = "lemma",
              polarity_terms = data.frame(term = c("annoy", "like", "painful"), 
                                          polarity = c(-1, 1, -1)), 
              polarity_negators = c("not", "neither"),
              polarity_amplifiers = c("pretty", "many", "really", "whatsoever"), 
              polarity_deamplifiers = c("slightly", "somewhat"),
              constrain = TRUE, n_before = 4,
              n_after = 2, amplifier_weight = .8)
scores$overall
scores$data
</code></pre>

<hr>
<h2 id='txt_show'>Boilerplate function to cat only 1 element of a character vector.</h2><span id='topic+txt_show'></span>

<h3>Description</h3>

<p>Boilerplate function to cat only 1 element of a character vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>txt_show(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="txt_show_+3A_x">x</code></td>
<td>
<p>a character vector</p>
</td></tr>
</table>


<h3>Value</h3>

<p>invisible
</p>


<h3>See Also</h3>

<p><code><a href="#topic+txt_sample">txt_sample</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>txt_show(c("hello \n\n\n world", "world \n\n\n hello"))
</code></pre>

<hr>
<h2 id='txt_tagsequence'>Identify a contiguous sequence of tags as 1 being entity</h2><span id='topic+txt_tagsequence'></span>

<h3>Description</h3>

<p>This function allows to identify contiguous sequences of text which have the same label or 
which follow the IOB scheme.<br /> 
Named Entity Recognition or Chunking frequently follows the IOB tagging scheme 
where &quot;B&quot; means the token begins an entity, &quot;I&quot; means it is inside an entity,
&quot;E&quot; means it is the end of an entity and &quot;O&quot; means it is not part of an entity. 
An example of such an annotation would be 'New', 'York', 'City', 'District' which can be tagged as 
'B-LOC', 'I-LOC', 'I-LOC', 'E-LOC'.<br />
The function looks for such sequences which start with 'B-LOC' and combines all subsequent 
labels of the same tagging group into 1 category. This sequence of words also gets a unique identifier such 
that the terms 'New', 'York', 'City', 'District' would get the same sequence identifier.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>txt_tagsequence(x, entities)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="txt_tagsequence_+3A_x">x</code></td>
<td>
<p>a character vector of categories in the sequence of occurring (e.g. B-LOC, I-LOC, I-PER, B-PER, O, O, B-PER)</p>
</td></tr>
<tr><td><code id="txt_tagsequence_+3A_entities">entities</code></td>
<td>
<p>a list of groups, where each list element contains
</p>

<ul>
<li><p>start: A length 1 character string with the start element identifying a sequence start. E.g. 'B-LOC'
</p>
</li>
<li><p>labels: A character vector containing all the elements which are considered being part of a same labelling sequence, including the starting element. 
E.g. <code>c('B-LOC', 'I-LOC', 'E-LOC')</code>
</p>
</li></ul>

<p>The list name of the group defines the label that will be assigned to the entity. If <code>entities</code> is not provided each possible value of <code>x</code>
is considered an entity. See the examples.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with elements <code>entity_id</code> and <code>entity</code> where 
</p>

<ul>
<li><p>entity is a character vector of the same length as <code>x</code> containing entities , 
constructed by recoding <code>x</code> to the names of <code>names(entities</code>)
</p>
</li>
<li><p>entity_id is an integer vector of the same length as <code>x</code> containing unique identifiers identfying the compound label sequence such that 
e.g. the sequence 'B-LOC', 'I-LOC', 'I-LOC', 'E-LOC' (New York City District) would get the same <code>entity_id</code> identifier.
</p>
</li></ul>

<p>See the examples.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- data.frame(
  token = c("The", "chairman", "of", "the", "Nakitoma", "Corporation", 
           "Donald", "Duck", "went", "skiing", 
            "in", "the", "Niagara", "Falls"),
  upos = c("DET", "NOUN", "ADP", "DET", "PROPN", "PROPN", 
           "PROPN", "PROPN", "VERB", "VERB", 
           "ADP", "DET", "PROPN", "PROPN"),
  label = c("O", "O", "O", "O", "B-ORG", "I-ORG", 
            "B-PERSON", "I-PERSON", "O", "O", 
            "O", "O", "B-LOCATION", "I-LOCATION"), stringsAsFactors = FALSE)
x[, c("sequence_id", "group")] &lt;- txt_tagsequence(x$upos)
x

##
## Define entity groups following the IOB scheme
## and combine B-LOC I-LOC I-LOC sequences as 1 group (e.g. New York City) 
groups &lt;- list(
 Location = list(start = "B-LOC", labels = c("B-LOC", "I-LOC", "E-LOC")),
 Organisation =  list(start = "B-ORG", labels = c("B-ORG", "I-ORG", "E-ORG")),
 Person = list(start = "B-PER", labels = c("B-PER", "I-PER", "E-PER")), 
 Misc = list(start = "B-MISC", labels = c("B-MISC", "I-MISC", "E-MISC")))
x[, c("entity_id", "entity")] &lt;- txt_tagsequence(x$label, groups)
x
</code></pre>

<hr>
<h2 id='udpipe'>Tokenising, Lemmatising, Tagging and Dependency Parsing of raw text in TIF format</h2><span id='topic+udpipe'></span>

<h3>Description</h3>

<p>Tokenising, Lemmatising, Tagging and Dependency Parsing of raw text in TIF format
</p>


<h3>Usage</h3>

<pre><code class='language-R'>udpipe(x, object, parallel.cores = 1L, parallel.chunksize, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="udpipe_+3A_x">x</code></td>
<td>
<p>either
</p>

<ul>
<li><p>a character vector: The character vector contains the text you want to tokenize, lemmatise, tag and perform dependency parsing. The names of the character vector indicate the document identifier.
</p>
</li>
<li><p>a data.frame with columns doc_id and text: The text column contains the text you want to tokenize, lemmatise, tag and perform dependency parsing. The doc_id column indicate the document identifier.
</p>
</li>
<li><p>a list of tokens: If you have already a tokenised list of tokens and you want to enrich it by lemmatising, tagging and performing dependency parsing. The names of the list indicate the document identifier.
</p>
</li></ul>

<p>All text data should be in UTF-8 encoding</p>
</td></tr>
<tr><td><code id="udpipe_+3A_object">object</code></td>
<td>
<p>either an object of class <code>udpipe_model</code> as returned by <code><a href="#topic+udpipe_load_model">udpipe_load_model</a></code>,
the path to the file on disk containing the udpipe model or the language as defined by <code><a href="#topic+udpipe_download_model">udpipe_download_model</a></code>. 
If the language is provided, it will download the model using <code><a href="#topic+udpipe_download_model">udpipe_download_model</a></code>.</p>
</td></tr>
<tr><td><code id="udpipe_+3A_parallel.cores">parallel.cores</code></td>
<td>
<p>integer indicating the number of parallel cores to use to speed up the annotation. Defaults to 1 (use only 1 single thread). <br />
If more than 1 is specified, it uses parallel::mclapply (unix) or parallel::clusterApply (windows) to run annotation in parallel. In order to do this on Windows it runs first parallel::makeCluster to set up a local socket cluster, on unix it just uses forking to parallelise the annotation.<br />
Only set this if you have more than 1 CPU at disposal and you have large amount of data to annotate as setting up a parallel backend also takes some time plus 
annotations will run in chunks set by <code>parallel.chunksize</code> and for each parallel chunk the udpipe model will be loaded which takes also some time.<br />
If <code>parallel.cores</code> is bigger than 1 and <code>object</code> is of class <code>udpipe_model</code>, it will load the corresponding file from the model again in each parallel chunk.</p>
</td></tr>
<tr><td><code id="udpipe_+3A_parallel.chunksize">parallel.chunksize</code></td>
<td>
<p>integer with the size of the chunks of text to be annotated in parallel. If not provided, defaults to the size of <code>x</code> divided by <code>parallel.cores</code>. Only used in case <code>parallel.cores</code> is bigger than 1.</p>
</td></tr>
<tr><td><code id="udpipe_+3A_...">...</code></td>
<td>
<p>other elements to pass on to <code><a href="#topic+udpipe_annotate">udpipe_annotate</a></code> and <code><a href="#topic+udpipe_download_model">udpipe_download_model</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame with one row per doc_id and term_id containing all the tokens in the data, the lemma, the part of speech tags,
the morphological features and the dependency relationship along the tokens. The data.frame has the following fields:
</p>

<ul>
<li><p>doc_id: The document identifier.
</p>
</li>
<li><p>paragraph_id: The paragraph identifier which is unique within each document.
</p>
</li>
<li><p>sentence_id: The sentence identifier which is unique within each document.
</p>
</li>
<li><p>sentence: The text of the sentence of the sentence_id.
</p>
</li>
<li><p>start: Integer index indicating in the original text where the token starts. Missing in case of tokens part of multi-word tokens which are not in the text.
</p>
</li>
<li><p>end: Integer index indicating in the original text where the token ends. Missing in case of tokens part of multi-word tokens which are not in the text.
</p>
</li>
<li><p>term_id: A row identifier which is unique within the doc_id identifier.
</p>
</li>
<li><p>token_id: Token index, integer starting at 1 for each new sentence. May be a range for multiword tokens or a decimal number for empty nodes.
</p>
</li>
<li><p>token: The token.
</p>
</li>
<li><p>lemma: The lemma of the token.
</p>
</li>
<li><p>upos: The universal parts of speech tag of the token. See <a href="https://universaldependencies.org/format.html">https://universaldependencies.org/format.html</a>
</p>
</li>
<li><p>xpos: The treebank-specific parts of speech tag of the token. See <a href="https://universaldependencies.org/format.html">https://universaldependencies.org/format.html</a>
</p>
</li>
<li><p>feats: The morphological features of the token, separated by |. See <a href="https://universaldependencies.org/format.html">https://universaldependencies.org/format.html</a>
</p>
</li>
<li><p>head_token_id: Indicating what is the token_id of the head of the token, indicating to which other token in the sentence it is related. See <a href="https://universaldependencies.org/format.html">https://universaldependencies.org/format.html</a>
</p>
</li>
<li><p>dep_rel: The type of relation the token has with the head_token_id. See <a href="https://universaldependencies.org/format.html">https://universaldependencies.org/format.html</a>
</p>
</li>
<li><p>deps: Enhanced dependency graph in the form of a list of head-deprel pairs. See <a href="https://universaldependencies.org/format.html">https://universaldependencies.org/format.html</a>
</p>
</li>
<li><p>misc: SpacesBefore/SpacesAfter/SpacesInToken spaces before/after/inside the token. Used to reconstruct the original text. See <a href="https://ufal.mff.cuni.cz/udpipe/1/users-manual">https://ufal.mff.cuni.cz/udpipe/1/users-manual</a>
</p>
</li></ul>

<p>The columns paragraph_id, sentence_id, term_id, start, end are integers, the other fields
are character data in UTF-8 encoding. <br />
</p>


<h3>References</h3>

<p><a href="https://ufal.mff.cuni.cz/udpipe">https://ufal.mff.cuni.cz/udpipe</a>, <a href="https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-2364">https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-2364</a>, 
<a href="https://universaldependencies.org/format.html">https://universaldependencies.org/format.html</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+udpipe_load_model">udpipe_load_model</a></code>, <code><a href="#topic+as.data.frame.udpipe_connlu">as.data.frame.udpipe_connlu</a></code>, <code><a href="#topic+udpipe_download_model">udpipe_download_model</a></code>, <code><a href="#topic+udpipe_annotate">udpipe_annotate</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model    &lt;- udpipe_download_model(language = "dutch-lassysmall")
if(!model$download_failed){
ud_dutch &lt;- udpipe_load_model(model)

## Tokenise, Tag and Dependency Parsing Annotation. Output is in CONLL-U format.
txt &lt;- c("Dus. Godvermehoeren met pus in alle puisten, 
  zei die schele van Van Bukburg en hij had nog gelijk ook. 
  Er was toen dat liedje van tietenkonttieten kont tieten kontkontkont, 
  maar dat hoefden we geenseens niet te zingen. 
  Je kunt zeggen wat je wil van al die gesluierde poezenpas maar d'r kwam wel 
  een vleeswarenwinkel onder te voorschijn van heb je me daar nou.
  
  En zo gaat het maar door.",
  "Wat die ransaap van een academici nou weer in z'n botte pan heb gehaald mag 
  Joost in m'n schoen gooien, maar feit staat boven water dat het een gore 
  vieze vuile ransaap is.")
names(txt) &lt;- c("document_identifier_1", "we-like-ilya-leonard-pfeiffer")

##
## TIF tagging: tag if x is a character vector, a data frame or a token sequence
##
x &lt;- udpipe(txt, object = ud_dutch)
x &lt;- udpipe(data.frame(doc_id = names(txt), text = txt, stringsAsFactors = FALSE), 
            object = ud_dutch)
x &lt;- udpipe(strsplit(txt, "[[:space:][:punct:][:digit:]]+"), 
            object = ud_dutch)
       
## You can also directly pass on the language in the call to udpipe
x &lt;- udpipe("Dit werkt ook.", object = "dutch-lassysmall")
x &lt;- udpipe(txt, object = "dutch-lassysmall")
x &lt;- udpipe(data.frame(doc_id = names(txt), text = txt, stringsAsFactors = FALSE), 
            object = "dutch-lassysmall")
x &lt;- udpipe(strsplit(txt, "[[:space:][:punct:][:digit:]]+"), 
            object = "dutch-lassysmall")
}
            
## cleanup for CRAN only - you probably want to keep your model if you have downloaded it
if(file.exists(model$file_model)) file.remove(model$file_model)
</code></pre>

<hr>
<h2 id='udpipe_accuracy'>Evaluate the accuracy of your UDPipe model on holdout data</h2><span id='topic+udpipe_accuracy'></span>

<h3>Description</h3>

<p>Get precision, recall and F1 measures on finding words / sentences / upos / xpos / features
annotation as well as UAS and LAS dependency scores on holdout data in conllu format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>udpipe_accuracy(
  object,
  file_conllu,
  tokenizer = c("default", "none"),
  tagger = c("default", "none"),
  parser = c("default", "none")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="udpipe_accuracy_+3A_object">object</code></td>
<td>
<p>an object of class <code>udpipe_model</code> as returned by <code><a href="#topic+udpipe_load_model">udpipe_load_model</a></code></p>
</td></tr>
<tr><td><code id="udpipe_accuracy_+3A_file_conllu">file_conllu</code></td>
<td>
<p>the full path to a file on disk containing holdout data in conllu format</p>
</td></tr>
<tr><td><code id="udpipe_accuracy_+3A_tokenizer">tokenizer</code></td>
<td>
<p>a character string of length 1, which is either 'default' or 'none'</p>
</td></tr>
<tr><td><code id="udpipe_accuracy_+3A_tagger">tagger</code></td>
<td>
<p>a character string of length 1, which is either 'default' or 'none'</p>
</td></tr>
<tr><td><code id="udpipe_accuracy_+3A_parser">parser</code></td>
<td>
<p>a character string of length 1, which is either 'default' or 'none'</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with 3 elements
</p>

<ul>
<li><p>accuracy: A character vector with accuracy metrics.
</p>
</li>
<li><p>error: A character string with possible errors when calculating the accuracy metrics
</p>
</li></ul>



<h3>References</h3>

<p><a href="https://ufal.mff.cuni.cz/udpipe">https://ufal.mff.cuni.cz/udpipe</a>, 
<a href="https://universaldependencies.org/format.html">https://universaldependencies.org/format.html</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+udpipe_load_model">udpipe_load_model</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- udpipe_download_model(language = "dutch-lassysmall")
if(!model$download_failed){
ud_dutch &lt;- udpipe_load_model(model$file_model)

file_conllu &lt;- system.file(package = "udpipe", "dummydata", "traindata.conllu")
metrics &lt;- udpipe_accuracy(ud_dutch, file_conllu)
metrics$accuracy
metrics &lt;- udpipe_accuracy(ud_dutch, file_conllu, 
                           tokenizer = "none", tagger = "default", parser = "default")
metrics$accuracy
metrics &lt;- udpipe_accuracy(ud_dutch, file_conllu, 
                           tokenizer = "none", tagger = "none", parser = "default")
metrics$accuracy
metrics &lt;- udpipe_accuracy(ud_dutch, file_conllu, 
                           tokenizer = "default", tagger = "none", parser = "none")
metrics$accuracy
}


## cleanup for CRAN only - you probably want to keep your model if you have downloaded it
if(file.exists(model$file_model)) file.remove(model$file_model)
</code></pre>

<hr>
<h2 id='udpipe_annotate'>Tokenising, Lemmatising, Tagging and Dependency Parsing Annotation of raw text</h2><span id='topic+udpipe_annotate'></span>

<h3>Description</h3>

<p>Tokenising, Lemmatising, Tagging and Dependency Parsing Annotation of raw text
</p>


<h3>Usage</h3>

<pre><code class='language-R'>udpipe_annotate(
  object,
  x,
  doc_id = paste("doc", seq_along(x), sep = ""),
  tokenizer = "tokenizer",
  tagger = c("default", "none"),
  parser = c("default", "none"),
  trace = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="udpipe_annotate_+3A_object">object</code></td>
<td>
<p>an object of class <code>udpipe_model</code> as returned by <code><a href="#topic+udpipe_load_model">udpipe_load_model</a></code></p>
</td></tr>
<tr><td><code id="udpipe_annotate_+3A_x">x</code></td>
<td>
<p>a character vector in UTF-8 encoding where each element of the character vector 
contains text which you like to tokenize, tag and perform dependency parsing.</p>
</td></tr>
<tr><td><code id="udpipe_annotate_+3A_doc_id">doc_id</code></td>
<td>
<p>an identifier of a document with the same length as <code>x</code>. This should be a character vector.
<code>doc_id[i]</code> corresponds to <code>x[i]</code>.</p>
</td></tr>
<tr><td><code id="udpipe_annotate_+3A_tokenizer">tokenizer</code></td>
<td>
<p>a character string of length 1, which is either 'tokenizer' (default udpipe tokenisation)
or a character string with more complex tokenisation options 
as specified in <a href="https://ufal.mff.cuni.cz/udpipe/1/users-manual">https://ufal.mff.cuni.cz/udpipe/1/users-manual</a> in which case <code>tokenizer</code> should be a character string where the options
are put after each other using the semicolon as separation.</p>
</td></tr>
<tr><td><code id="udpipe_annotate_+3A_tagger">tagger</code></td>
<td>
<p>a character string of length 1, which is either 'default' (default udpipe POS tagging and lemmatisation)
or 'none' (no POS tagging and lemmatisation needed) or a character string with more complex tagging options 
as specified in <a href="https://ufal.mff.cuni.cz/udpipe/1/users-manual">https://ufal.mff.cuni.cz/udpipe/1/users-manual</a> in which case <code>tagger</code> should be a character string where the options
are put after each other using the semicolon as separation.</p>
</td></tr>
<tr><td><code id="udpipe_annotate_+3A_parser">parser</code></td>
<td>
<p>a character string of length 1, which is either 'default' (default udpipe dependency parsing) or
'none' (no dependency parsing needed) or a character string with more complex parsing options 
as specified in <a href="https://ufal.mff.cuni.cz/udpipe/1/users-manual">https://ufal.mff.cuni.cz/udpipe/1/users-manual</a> in which case <code>parser</code> should be a character string where the options
are put after each other using the semicolon as separation.</p>
</td></tr>
<tr><td><code id="udpipe_annotate_+3A_trace">trace</code></td>
<td>
<p>A non-negative integer indicating to show progress on the annotation. 
If positive it prints out a message before each <code>trace</code> number of elements of <code>x</code> for which annotation is to be executed,
allowing you to see how much of the text is already annotated. Defaults to FALSE (no progress shown).</p>
</td></tr>
<tr><td><code id="udpipe_annotate_+3A_...">...</code></td>
<td>
<p>currently not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with 3 elements
</p>

<ul>
<li><p>x: The <code>x</code> character vector with text.
</p>
</li>
<li><p>conllu: A character vector of length 1 containing the annotated result of the annotation flow in CONLL-U format.
This format is explained at <a href="https://universaldependencies.org/format.html">https://universaldependencies.org/format.html</a>
</p>
</li>
<li><p>error: A vector with the same length of <code>x</code> containing possible errors when annotating <code>x</code>
</p>
</li></ul>



<h3>References</h3>

<p><a href="https://ufal.mff.cuni.cz/udpipe">https://ufal.mff.cuni.cz/udpipe</a>, <a href="https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-2364">https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-2364</a>, 
<a href="https://universaldependencies.org/format.html">https://universaldependencies.org/format.html</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+udpipe_load_model">udpipe_load_model</a></code>, <code><a href="#topic+as.data.frame.udpipe_connlu">as.data.frame.udpipe_connlu</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model    &lt;- udpipe_download_model(language = "dutch-lassysmall")
if(!model$download_failed){
ud_dutch &lt;- udpipe_load_model(model$file_model)

## Tokenise, Tag and Dependency Parsing Annotation. Output is in CONLL-U format.
txt &lt;- c("Dus. Godvermehoeren met pus in alle puisten, 
  zei die schele van Van Bukburg en hij had nog gelijk ook. 
  Er was toen dat liedje van tietenkonttieten kont tieten kontkontkont, 
  maar dat hoefden we geenseens niet te zingen. 
  Je kunt zeggen wat je wil van al die gesluierde poezenpas maar d'r kwam wel 
  een vleeswarenwinkel onder te voorschijn van heb je me daar nou.
  
  En zo gaat het maar door.",
  "Wat die ransaap van een academici nou weer in z'n botte pan heb gehaald mag 
  Joost in m'n schoen gooien, maar feit staat boven water dat het een gore 
  vieze vuile ransaap is.")
x &lt;- udpipe_annotate(ud_dutch, x = txt)
cat(x$conllu)
as.data.frame(x)

## Only tokenisation
x &lt;- udpipe_annotate(ud_dutch, x = txt, tagger = "none", parser = "none")
as.data.frame(x)

## Only tokenisation and POS tagging + lemmatisation, no dependency parsing
x &lt;- udpipe_annotate(ud_dutch, x = txt, tagger = "default", parser = "none")
as.data.frame(x)

## Only tokenisation and dependency parsing, no POS tagging nor lemmatisation
x &lt;- udpipe_annotate(ud_dutch, x = txt, tagger = "none", parser = "default")
as.data.frame(x)

## Provide doc_id for joining and identification purpose
x &lt;- udpipe_annotate(ud_dutch, x = txt, doc_id = c("id1", "feedbackabc"),
                     tagger = "none", parser = "none", trace = TRUE)
as.data.frame(x)

## Mark on encodings: if your data is not in UTF-8 encoding, make sure you convert it to UTF-8 
## This can be done using iconv as follows for example
udpipe_annotate(ud_dutch, x = iconv('Ik drink melk bij mijn koffie.', to = "UTF-8"))
}



## cleanup for CRAN only - you probably want to keep your model if you have downloaded it
if(file.exists(model$file_model)) file.remove(model$file_model)
</code></pre>

<hr>
<h2 id='udpipe_annotation_params'>List with training options set by the UDPipe community when building models based on the Universal Dependencies data</h2><span id='topic+udpipe_annotation_params'></span>

<h3>Description</h3>

<p>In order to show the settings which were used by the UDPipe community when building 
the models made available when using <code><a href="#topic+udpipe_download_model">udpipe_download_model</a></code>, 
the tokenizer settings used for the different treebanks are shown below, 
so that you can easily use this to retrain your model directly on the corresponding 
UD treebank which you can download at <code>http://universaldependencies.org/#ud-treebanks</code>. <br />
</p>
<p>More information on how the models provided by the UDPipe community have been built are available
at <a href="https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-2364">https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-2364</a>
</p>


<h3>References</h3>

<p><a href="https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-2364">https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-2364</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(udpipe_annotation_params)
str(udpipe_annotation_params)

## settings of the tokenizer
head(udpipe_annotation_params$tokenizer)

## settings of the tagger
subset(udpipe_annotation_params$tagger, language_treebank == "nl")

## settings of the parser
udpipe_annotation_params$parser
</code></pre>

<hr>
<h2 id='udpipe_download_model'>Download an UDPipe model provided by the UDPipe community for a specific language of choice</h2><span id='topic+udpipe_download_model'></span>

<h3>Description</h3>

<p>Ready-made models for 65 languages trained on 101 treebanks from <a href="https://universaldependencies.org/">https://universaldependencies.org/</a> are provided to you.
Some of these models were provided by the UDPipe community. Other models were build using this R package.
You can either download these models manually in order to use it for annotation purposes 
or use <code>udpipe_download_model</code> to download these models for a specific language of choice. You have the following options: <br />
</p>


<h3>Usage</h3>

<pre><code class='language-R'>udpipe_download_model(
  language = c("afrikaans-afribooms", "ancient_greek-perseus", "ancient_greek-proiel",
    "arabic-padt", "armenian-armtdp", "basque-bdt", "belarusian-hse", "bulgarian-btb",
    "buryat-bdt", "catalan-ancora", "chinese-gsd", "chinese-gsdsimp",
    "classical_chinese-kyoto", "coptic-scriptorium", "croatian-set", "czech-cac",
    "czech-cltt", "czech-fictree", "czech-pdt", "danish-ddt", "dutch-alpino",
    "dutch-lassysmall", "english-ewt", "english-gum", "english-lines", "english-partut",
    "estonian-edt", "estonian-ewt", "finnish-ftb",      "finnish-tdt", "french-gsd",
    "french-partut", "french-sequoia", "french-spoken", "galician-ctg",
    "galician-treegal", "german-gsd", "german-hdt", "gothic-proiel", "greek-gdt",
    "hebrew-htb", "hindi-hdtb", "hungarian-szeged", "indonesian-gsd", "irish-idt",
    "italian-isdt", "italian-partut", "italian-postwita", "italian-twittiro",
    "italian-vit", "japanese-gsd", "kazakh-ktb", "korean-gsd", "korean-kaist",
    "kurmanji-mg", "latin-ittb", "latin-perseus", "latin-proiel", "latvian-lvtb",
    "lithuanian-alksnis",      "lithuanian-hse", "maltese-mudt", "marathi-ufal",
    "north_sami-giella", "norwegian-bokmaal", "norwegian-nynorsk",
    "norwegian-nynorsklia", "old_church_slavonic-proiel", "old_french-srcmf",
    "old_russian-torot", "persian-seraji", "polish-lfg", "polish-pdb", "polish-sz",
    "portuguese-bosque", "portuguese-br", "portuguese-gsd", "romanian-nonstandard",
    "romanian-rrt", "russian-gsd", "russian-syntagrus", "russian-taiga", "sanskrit-ufal",
    "scottish_gaelic-arcosg", "serbian-set", "slovak-snk", "slovenian-ssj",     
    "slovenian-sst", "spanish-ancora", "spanish-gsd", "swedish-lines",
    "swedish-talbanken", "tamil-ttb", "telugu-mtg", "turkish-imst", "ukrainian-iu",
    "upper_sorbian-ufal", "urdu-udtb", "uyghur-udt", "vietnamese-vtb", "wolof-wtb"),
  model_dir = getwd(),
  udpipe_model_repo = c("jwijffels/udpipe.models.ud.2.5",
    "jwijffels/udpipe.models.ud.2.4", "jwijffels/udpipe.models.ud.2.3",
    "jwijffels/udpipe.models.ud.2.0", "jwijffels/udpipe.models.conll18.baseline",
    "bnosac/udpipe.models.ud"),
  overwrite = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="udpipe_download_model_+3A_language">language</code></td>
<td>
<p>a character string with a Universal Dependencies treebank which was used to build the model. Possible values are:<br />
afrikaans-afribooms, ancient_greek-perseus, ancient_greek-proiel, arabic-padt, armenian-armtdp, basque-bdt, belarusian-hse, 
bulgarian-btb, buryat-bdt, catalan-ancora, chinese-gsd, chinese-gsdsimp, coptic-scriptorium, croatian-set, czech-cac, czech-cltt, 
czech-fictree, czech-pdt, danish-ddt, dutch-alpino, dutch-lassysmall, english-ewt, english-gum, english-lines, 
english-partut, estonian-edt, finnish-ftb, finnish-tdt, french-gsd, french-partut, french-sequoia, french-spoken, 
galician-ctg, galician-treegal, german-gsd, german-hdt, gothic-proiel, greek-gdt, hebrew-htb, hindi-hdtb, hungarian-szeged, 
indonesian-gsd, irish-idt, italian-isdt, italian-partut, italian-postwita, italian-twittiro, japanese-gsd, kazakh-ktb, korean-gsd, 
korean-kaist, kurmanji-mg, latin-ittb, latin-perseus, latin-proiel, latvian-lvtb, lithuanian-hse, maltese-mudt, 
marathi-ufal, north_sami-giella, norwegian-bokmaal, norwegian-nynorsk, norwegian-nynorsklia, old_church_slavonic-proiel, 
old_french-srcmf, persian-seraji, polish-lfg, polish-sz, portuguese-bosque, portuguese-br, portuguese-gsd, 
romanian-nonstandard, romanian-rrt, russian-gsd, russian-syntagrus, russian-taiga, sanskrit-ufal, scottish_gaelic-arcosg, serbian-set, 
slovak-snk, slovenian-ssj, slovenian-sst, spanish-ancora, spanish-gsd, swedish-lines, swedish-talbanken, tamil-ttb, 
telugu-mtg, turkish-imst, ukrainian-iu, upper_sorbian-ufal, urdu-udtb, uyghur-udt, vietnamese-vtb <br /> 
</p>
<p>Each language should have a treebank extension (e.g. english-ewt, russian-syntagrus, dutch-alpino, ...). 
If you do not provide a treebank extension (e.g. only english, russian, dutch), 
the function will use the default treebank of that language as was used in Universal Dependencies up to version 2.1.</p>
</td></tr>
<tr><td><code id="udpipe_download_model_+3A_model_dir">model_dir</code></td>
<td>
<p>a path where the model will be downloaded to. Defaults to the current working directory</p>
</td></tr>
<tr><td><code id="udpipe_download_model_+3A_udpipe_model_repo">udpipe_model_repo</code></td>
<td>
<p>location where the models will be downloaded from. 
Either 'jwijffels/udpipe.models.ud.2.5', 'jwijffels/udpipe.models.ud.2.4', 'jwijffels/udpipe.models.ud.2.3', 'jwijffels/udpipe.models.ud.2.0', 'jwijffels/udpipe.models.conll18.baseline' or 'bnosac/udpipe.models.ud'. <br />
Defaults to 'jwijffels/udpipe.models.ud.2.5'. <br />
</p>

<ul>
<li><p>'bnosac/udpipe.models.ud' contains models mainly released under the CC-BY-SA license constructed on Universal Dependencies 2.1 data, and some models released under the GPL-3 and LGPL-LR license
</p>
</li>
<li><p>'jwijffels/udpipe.models.ud.2.5' contains models released under the CC-BY-NC-SA license constructed on Universal Dependencies 2.5 data
</p>
</li>
<li><p>'jwijffels/udpipe.models.ud.2.4' contains models released under the CC-BY-NC-SA license constructed on Universal Dependencies 2.4 data
</p>
</li>
<li><p>'jwijffels/udpipe.models.ud.2.3' contains models released under the CC-BY-NC-SA license constructed on Universal Dependencies 2.3 data
</p>
</li>
<li><p>'jwijffels/udpipe.models.ud.2.0' contains models released under the CC-BY-NC-SA license constructed on Universal Dependencies 2.0 data
</p>
</li>
<li><p>'jwijffels/udpipe.models.conll18.baseline' contains models released under the CC-BY-NC-SA license constructed on Universal Dependencies 2.2 data for the 2018 conll shared task
</p>
</li></ul>

<p>See the Details section for further information on which languages are available in each of these repositories.</p>
</td></tr>
<tr><td><code id="udpipe_download_model_+3A_overwrite">overwrite</code></td>
<td>
<p>logical indicating to overwrite the file if the file was already downloaded. Defaults to <code>TRUE</code> indicating 
it will download the model and overwrite the file if the file already existed. If set to <code>FALSE</code>,
the model will only be downloaded if it does not exist on disk yet in the <code>model_dir</code> folder.</p>
</td></tr>
<tr><td><code id="udpipe_download_model_+3A_...">...</code></td>
<td>
<p>currently not used</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function allows you to download the following language models based on your setting of argument <code>udpipe_model_repo</code>:
</p>

<ul>
<li><p> 'jwijffels/udpipe.models.ud.2.5': <a href="https://github.com/jwijffels/udpipe.models.ud.2.5">https://github.com/jwijffels/udpipe.models.ud.2.5</a>
</p>

<ul>
<li><p>UDPipe models constructed on data from Universal Dependencies 2.5
</p>
</li>
<li><p>languages-treebanks: afrikaans-afribooms, ancient_greek-perseus, ancient_greek-proiel, arabic-padt, armenian-armtdp, basque-bdt, belarusian-hse, bulgarian-btb, catalan-ancora, chinese-gsd, chinese-gsdsimp, classical_chinese-kyoto, coptic-scriptorium, croatian-set, czech-cac, czech-cltt, czech-fictree, czech-pdt, danish-ddt, dutch-alpino, dutch-lassysmall, english-ewt, english-gum, english-lines, english-partut, estonian-edt, estonian-ewt, finnish-ftb, finnish-tdt, french-gsd, french-partut, french-sequoia, french-spoken, galician-ctg, galician-treegal, german-gsd, german-hdt, gothic-proiel, greek-gdt, hebrew-htb, hindi-hdtb, hungarian-szeged, indonesian-gsd, irish-idt, italian-isdt, italian-partut, italian-postwita, italian-twittiro, italian-vit, japanese-gsd, korean-gsd, korean-kaist, latin-ittb, latin-perseus, latin-proiel, latvian-lvtb, lithuanian-alksnis, lithuanian-hse, maltese-mudt, marathi-ufal, north_sami-giella, norwegian-bokmaal, norwegian-nynorsk, norwegian-nynorsklia, old_church_slavonic-proiel, old_french-srcmf, old_russian-torot, persian-seraji, polish-lfg, polish-pdb, portuguese-bosque, portuguese-gsd, romanian-nonstandard, romanian-rrt, russian-gsd, russian-syntagrus, russian-taiga, scottish_gaelic-arcosg, serbian-set, slovak-snk, slovenian-ssj, slovenian-sst, spanish-ancora, spanish-gsd, swedish-lines, swedish-talbanken, tamil-ttb, telugu-mtg, turkish-imst, ukrainian-iu, urdu-udtb, uyghur-udt, vietnamese-vtb, wolof-wtb
</p>
</li>
<li><p>license: CC-BY-SA-NC
</p>
</li></ul>
 
</li>
<li><p> 'jwijffels/udpipe.models.ud.2.4': <a href="https://github.com/jwijffels/udpipe.models.ud.2.4">https://github.com/jwijffels/udpipe.models.ud.2.4</a>
</p>

<ul>
<li><p>UDPipe models constructed on data from Universal Dependencies 2.4
</p>
</li>
<li><p>languages-treebanks: afrikaans-afribooms, ancient_greek-perseus, ancient_greek-proiel, arabic-padt, armenian-armtdp, basque-bdt, belarusian-hse, bulgarian-btb, catalan-ancora, chinese-gsd, classical_chinese-kyoto, coptic-scriptorium, croatian-set, czech-cac, czech-cltt, czech-fictree, czech-pdt, danish-ddt, dutch-alpino, dutch-lassysmall, english-ewt, english-gum, english-lines, english-partut, estonian-edt, estonian-ewt, finnish-ftb, finnish-tdt, french-gsd, french-partut, french-sequoia, french-spoken, galician-ctg, galician-treegal, german-gsd, gothic-proiel, greek-gdt, hebrew-htb, hindi-hdtb, hungarian-szeged, indonesian-gsd, irish-idt, italian-isdt, italian-partut, italian-postwita, italian-vit, japanese-gsd, korean-gsd, korean-kaist, latin-ittb, latin-perseus, latin-proiel, latvian-lvtb, lithuanian-alksnis, lithuanian-hse, maltese-mudt, marathi-ufal, north_sami-giella, norwegian-bokmaal, norwegian-nynorsk, norwegian-nynorsklia, old_church_slavonic-proiel, old_french-srcmf, old_russian-torot, persian-seraji, polish-lfg, polish-pdb, portuguese-bosque, portuguese-gsd, romanian-nonstandard, romanian-rrt, russian-gsd, russian-syntagrus, russian-taiga, serbian-set, slovak-snk, slovenian-ssj, slovenian-sst, spanish-ancora, spanish-gsd, swedish-lines, swedish-talbanken, tamil-ttb, telugu-mtg, turkish-imst, ukrainian-iu, urdu-udtb, uyghur-udt, vietnamese-vtb, wolof-wtb
</p>
</li>
<li><p>license: CC-BY-SA-NC
</p>
</li></ul>
 
</li>
<li><p> 'jwijffels/udpipe.models.ud.2.3': <a href="https://github.com/jwijffels/udpipe.models.ud.2.3">https://github.com/jwijffels/udpipe.models.ud.2.3</a>
</p>

<ul>
<li><p>UDPipe models constructed on data from Universal Dependencies 2.3
</p>
</li>
<li><p>languages-treebanks: afrikaans-afribooms, ancient_greek-perseus, ancient_greek-proiel, arabic-padt, armenian-armtdp, basque-bdt, belarusian-hse, bulgarian-btb, catalan-ancora, chinese-gsd, coptic-scriptorium, croatian-set, czech-cac, czech-cltt, czech-fictree, czech-pdt, danish-ddt, dutch-alpino, dutch-lassysmall, english-ewt, english-gum, english-lines, english-partut, estonian-edt, finnish-ftb, finnish-tdt, french-gsd, french-partut, french-sequoia, french-spoken, galician-ctg, galician-treegal, german-gsd, gothic-proiel, greek-gdt, hebrew-htb, hindi-hdtb, hungarian-szeged, indonesian-gsd, irish-idt, italian-isdt, italian-partut, italian-postwita, japanese-gsd, korean-gsd, korean-kaist, latin-ittb, latin-perseus, latin-proiel, latvian-lvtb, lithuanian-hse, maltese-mudt, marathi-ufal, north_sami-giella, norwegian-bokmaal, norwegian-nynorsk, norwegian-nynorsklia, old_church_slavonic-proiel, old_french-srcmf, persian-seraji, polish-lfg, polish-sz, portuguese-bosque, portuguese-gsd, romanian-nonstandard, romanian-rrt, russian-gsd, russian-syntagrus, russian-taiga, serbian-set, slovak-snk, slovenian-ssj, slovenian-sst, spanish-ancora, spanish-gsd, swedish-lines, swedish-talbanken, tamil-ttb, telugu-mtg, turkish-imst, ukrainian-iu, urdu-udtb, uyghur-udt, vietnamese-vtb
</p>
</li>
<li><p>license: CC-BY-SA-NC
</p>
</li></ul>
 
</li>
<li><p> 'jwijffels/udpipe.models.ud.2.0': <a href="https://github.com/jwijffels/udpipe.models.ud.2.0">https://github.com/jwijffels/udpipe.models.ud.2.0</a>
</p>

<ul>
<li><p>UDPipe models constructed on data from Universal Dependencies 2.0
</p>
</li>
<li><p>languages-treebanks: ancient_greek-proiel, ancient_greek, arabic, basque, belarusian, bulgarian, catalan, chinese, coptic, croatian, czech-cac, czech-cltt, czech, danish, dutch-lassysmall, dutch, english-lines, english-partut, english, estonian, finnish-ftb, finnish, french-partut, french-sequoia, french, galician-treegal, galician, german, gothic, greek, hebrew, hindi, hungarian, indonesian, irish, italian, japanese, kazakh, korean, latin-ittb, latin-proiel, latin, latvian, lithuanian, norwegian-bokmaal, norwegian-nynorsk, old_church_slavonic, persian, polish, portuguese-br, portuguese, romanian, russian-syntagrus, russian, sanskrit, slovak, slovenian-sst, slovenian, spanish-ancora, spanish, swedish-lines, swedish, tamil, turkish, ukrainian, urdu, uyghur, vietnamese
</p>
</li>
<li><p>license: CC-BY-SA-NC
</p>
</li></ul>
 
</li>
<li><p> 'jwijffels/udpipe.models.conll18.baseline': <a href="https://github.com/jwijffels/udpipe.models.conll18.baseline">https://github.com/jwijffels/udpipe.models.conll18.baseline</a>
</p>

<ul>
<li><p>UDPipe models constructed on data from Universal Dependencies 2.2
</p>
</li>
<li><p>languages-treebanks: afrikaans-afribooms, ancient_greek-perseus, ancient_greek-proiel, arabic-padt, armenian-armtdp, basque-bdt, bulgarian-btb, buryat-bdt, catalan-ancora, chinese-gsd, croatian-set, czech-cac, czech-fictree, czech-pdt, danish-ddt, dutch-alpino, dutch-lassysmall, english-ewt, english-gum, english-lines, estonian-edt, finnish-ftb, finnish-tdt, french-gsd, french-sequoia, french-spoken, galician-ctg, galician-treegal, german-gsd, gothic-proiel, greek-gdt, hebrew-htb, hindi-hdtb, hungarian-szeged, indonesian-gsd, irish-idt, italian-isdt, italian-postwita, japanese-gsd, kazakh-ktb, korean-gsd, korean-kaist, kurmanji-mg, latin-ittb, latin-perseus, latin-proiel, latvian-lvtb, mixed, north_sami-giella, norwegian-bokmaal, norwegian-nynorsk, norwegian-nynorsklia, old_church_slavonic-proiel, old_french-srcmf, persian-seraji, polish-lfg, polish-sz, portuguese-bosque, romanian-rrt, russian-syntagrus, russian-taiga, serbian-set, slovak-snk, slovenian-ssj, slovenian-sst, spanish-ancora, swedish-lines, swedish-talbanken, turkish-imst, ukrainian-iu, upper_sorbian-ufal, urdu-udtb, uyghur-udt, vietnamese-vtb
</p>
</li>
<li><p>license: CC-BY-SA-NC
</p>
</li></ul>
 
</li>
<li><p> 'bnosac/udpipe.models.ud': <a href="https://github.com/bnosac/udpipe.models.ud">https://github.com/bnosac/udpipe.models.ud</a>
</p>

<ul>
<li><p>UDPipe models constructed on data from Universal Dependencies 2.1
</p>
</li>
<li><p>This repository contains models build with this R package on open data from Universal Dependencies 2.1 which allows for commercial usage. The license of these models is mostly CC-BY-SA. Visit that github repository for details on the licenses of the language of your choice. And contact www.bnosac.be if you need support on these models or require models tuned to your needs.
</p>
</li>
<li><p>languages-treebanks: afrikaans, croatian, czech-cac, dutch, english, finnish, french-sequoia, irish, norwegian-bokmaal, persian, polish, portuguese, romanian, serbian, slovak, spanish-ancora, swedish
</p>
</li>
<li><p>license: license is treebank-specific but mainly CC-BY-SA and GPL-3 and LGPL-LR
</p>
</li></ul>
 
</li>
<li><p>If you need to train models yourself for commercial purposes or if you want to improve models, you can easily do this with <code><a href="#topic+udpipe_train">udpipe_train</a></code> which is explained in detail in the package vignette.
</p>
</li></ul>
 
<p>Note that when you download these models, you comply to the license of your specific language model.
</p>


<h3>Value</h3>

<p>A data.frame with 1 row and the following columns: 
</p>

<ul>
<li><p>language: The language as provided by the input parameter <code>language</code>
</p>
</li>
<li><p>file_model: The path to the file on disk where the model was downloaded to
</p>
</li>
<li><p>url: The URL where the model was downloaded from
</p>
</li>
<li><p>download_failed: A logical indicating if the download has failed or not due to internet connectivity issues
</p>
</li>
<li><p>download_message: A character string with the error message in case the downloading of the model failed
</p>
</li></ul>



<h3>References</h3>

<p><a href="https://ufal.mff.cuni.cz/udpipe">https://ufal.mff.cuni.cz/udpipe</a>, 
<a href="https://github.com/jwijffels/udpipe.models.ud.2.5">https://github.com/jwijffels/udpipe.models.ud.2.5</a>, 
<a href="https://github.com/jwijffels/udpipe.models.ud.2.4">https://github.com/jwijffels/udpipe.models.ud.2.4</a>, 
<a href="https://github.com/jwijffels/udpipe.models.ud.2.3">https://github.com/jwijffels/udpipe.models.ud.2.3</a>, 
<a href="https://github.com/jwijffels/udpipe.models.conll18.baseline">https://github.com/jwijffels/udpipe.models.conll18.baseline</a>
<a href="https://github.com/jwijffels/udpipe.models.ud.2.0">https://github.com/jwijffels/udpipe.models.ud.2.0</a>, 
<a href="https://github.com/bnosac/udpipe.models.ud">https://github.com/bnosac/udpipe.models.ud</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+udpipe_load_model">udpipe_load_model</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
x &lt;- udpipe_download_model(language = "dutch-alpino")
x &lt;- udpipe_download_model(language = "dutch-lassysmall")
x &lt;- udpipe_download_model(language = "russian")
x &lt;- udpipe_download_model(language = "french")
x &lt;- udpipe_download_model(language = "english-partut")
x &lt;- udpipe_download_model(language = "english-ewt")
x &lt;- udpipe_download_model(language = "german-gsd")
x &lt;- udpipe_download_model(language = "spanish-gsd")
x &lt;- udpipe_download_model(language = "spanish-gsd", overwrite = FALSE)

x &lt;- udpipe_download_model(language = "dutch-alpino", 
                           udpipe_model_repo = "jwijffels/udpipe.models.ud.2.5")
x &lt;- udpipe_download_model(language = "dutch-alpino", 
                           udpipe_model_repo = "jwijffels/udpipe.models.ud.2.4")
x &lt;- udpipe_download_model(language = "dutch-alpino", 
                           udpipe_model_repo = "jwijffels/udpipe.models.ud.2.3")
x &lt;- udpipe_download_model(language = "dutch-alpino", 
                           udpipe_model_repo = "jwijffels/udpipe.models.ud.2.0")
x &lt;- udpipe_download_model(language = "english", udpipe_model_repo = "bnosac/udpipe.models.ud")
x &lt;- udpipe_download_model(language = "dutch", udpipe_model_repo = "bnosac/udpipe.models.ud")
x &lt;- udpipe_download_model(language = "afrikaans", udpipe_model_repo = "bnosac/udpipe.models.ud")
x &lt;- udpipe_download_model(language = "spanish-ancora", 
                           udpipe_model_repo = "bnosac/udpipe.models.ud")
x &lt;- udpipe_download_model(language = "dutch-ud-2.1-20180111.udpipe", 
                           udpipe_model_repo = "bnosac/udpipe.models.ud")                           
x &lt;- udpipe_download_model(language = "english", 
                           udpipe_model_repo = "jwijffels/udpipe.models.conll18.baseline")

## End(Not run)

x &lt;- udpipe_download_model(language = "sanskrit", 
                           udpipe_model_repo = "jwijffels/udpipe.models.ud.2.0", 
                           model_dir = tempdir())
x
## cleanup for CRAN
if(file.exists(x$file_model)) file.remove(x$file_model)
</code></pre>

<hr>
<h2 id='udpipe_load_model'>Load an UDPipe model</h2><span id='topic+udpipe_load_model'></span>

<h3>Description</h3>

<p>Load an UDPipe model so that it can be use in <code><a href="#topic+udpipe_annotate">udpipe_annotate</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>udpipe_load_model(file)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="udpipe_load_model_+3A_file">file</code></td>
<td>
<p>full path to the model or the value returned by a call to <code><a href="#topic+udpipe_download_model">udpipe_download_model</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>udpipe_model</code> which is a list with 2 elements
</p>

<ul>
<li><p>file: The path to the model as provided by <code>file</code>
</p>
</li>
<li><p>model: An Rcpp-generated pointer to the loaded model which can be used in <code><a href="#topic+udpipe_annotate">udpipe_annotate</a></code>
</p>
</li></ul>



<h3>References</h3>

<p><a href="https://ufal.mff.cuni.cz/udpipe">https://ufal.mff.cuni.cz/udpipe</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+udpipe_annotate">udpipe_annotate</a></code>, <code><a href="#topic+udpipe_download_model">udpipe_download_model</a></code>, <code><a href="#topic+udpipe_train">udpipe_train</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
x &lt;- udpipe_download_model(language = "dutch-lassysmall")
x$file_model
ud_english &lt;- udpipe_load_model(x$file_model)

x &lt;- udpipe_download_model(language = "english")
x$file_model
ud_english &lt;- udpipe_load_model(x$file_model)

x &lt;- udpipe_download_model(language = "hebrew")
x$file_model
ud_hebrew &lt;- udpipe_load_model(x$file_model)

## End(Not run)


x &lt;- udpipe_download_model(language = "dutch-lassysmall", model_dir = tempdir())
x$file_model
if(!x$download_failed){
  ud_dutch &lt;- udpipe_load_model(x$file_model)
}

## cleanup for CRAN
if(file.exists(x$file_model)) file.remove(x$file_model)
</code></pre>

<hr>
<h2 id='udpipe_read_conllu'>Read in a CONLL-U file as a data.frame</h2><span id='topic+udpipe_read_conllu'></span>

<h3>Description</h3>

<p>Read in a CONLL-U file as a data.frame
</p>


<h3>Usage</h3>

<pre><code class='language-R'>udpipe_read_conllu(file)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="udpipe_read_conllu_+3A_file">file</code></td>
<td>
<p>a connection object or a character string with the location of the file</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame with columns doc_id, paragraph_id, sentence_id, sentence, 
token_id, token, lemma, upos, xpos, feats, head_token_id, deprel, dep_rel, misc
</p>


<h3>Examples</h3>

<pre><code class='language-R'>file_conllu &lt;- system.file(package = "udpipe", "dummydata", "traindata.conllu")
x &lt;- udpipe_read_conllu(file_conllu)
head(x)
</code></pre>

<hr>
<h2 id='udpipe_train'>Train a UDPipe model</h2><span id='topic+udpipe_train'></span>

<h3>Description</h3>

<p>Train a UDPipe model which allows to do 
Tokenization, Parts of Speech Tagging, Lemmatization and Dependency Parsing or a combination of those. <br />
</p>
<p>This function allows you to build models based on data in in CONLL-U format
as described at <a href="https://universaldependencies.org/format.html">https://universaldependencies.org/format.html</a>. At the time of writing open data in CONLL-U
format for more than 50 languages are available at <a href="https://universaldependencies.org">https://universaldependencies.org</a>. 
Most of these are distributed under the CC-BY-SA licence or the CC-BY-NC-SA license. <br />
</p>
<p>This function allows to build annotation tagger models based on these data in CONLL-U format, allowing you 
to have your own tagger model. This is relevant if you want to tune the tagger to your needs
or if you don't want to use ready-made models provided under the CC-BY-NC-SA license as shown at <code><a href="#topic+udpipe_load_model">udpipe_load_model</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>udpipe_train(
  file = file.path(getwd(), "my_annotator.udpipe"),
  files_conllu_training,
  files_conllu_holdout = character(),
  annotation_tokenizer = "default",
  annotation_tagger = "default",
  annotation_parser = "default"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="udpipe_train_+3A_file">file</code></td>
<td>
<p>full path where the model will be saved. The model will be stored as a binary file which <code><a href="#topic+udpipe_load_model">udpipe_load_model</a></code>
can handle. Defaults to 'my_annotator.udpipe' in the current working directory.</p>
</td></tr>
<tr><td><code id="udpipe_train_+3A_files_conllu_training">files_conllu_training</code></td>
<td>
<p>a character vector of files in CONLL-U format used for training the model</p>
</td></tr>
<tr><td><code id="udpipe_train_+3A_files_conllu_holdout">files_conllu_holdout</code></td>
<td>
<p>a character vector of files in CONLL-U format used for holdout evalution of the model. This argument is optional.</p>
</td></tr>
<tr><td><code id="udpipe_train_+3A_annotation_tokenizer">annotation_tokenizer</code></td>
<td>
<p>a string containing options for the tokenizer. This can be either 'none' or 'default' or a list 
of options as mentioned in the UDPipe manual. See the vignette <code>vignette("udpipe-train", package = "udpipe")</code> or
go directly to <a href="https://ufal.mff.cuni.cz/udpipe/1/users-manual#model_training_tokenizer">https://ufal.mff.cuni.cz/udpipe/1/users-manual#model_training_tokenizer</a> for a full description of the options
or see the examples below.
Defaults to 'default'. If you specify 'none', the model will not be able to perform tokenization.</p>
</td></tr>
<tr><td><code id="udpipe_train_+3A_annotation_tagger">annotation_tagger</code></td>
<td>
<p>a string containing options for the pos tagger and lemmatiser. This can be either 'none' or 'default' or a list 
of options as mentioned in the UDPipe manual. See the vignette <code>vignette("udpipe-train", package = "udpipe")</code> or
go directly to <a href="https://ufal.mff.cuni.cz/udpipe/1/users-manual#model_training_tagger">https://ufal.mff.cuni.cz/udpipe/1/users-manual#model_training_tagger</a> for a full description of the options
or see the examples below.
Defaults to 'default'. If you specify 'none', the model will not be able to perform POS tagging or lemmatization.</p>
</td></tr>
<tr><td><code id="udpipe_train_+3A_annotation_parser">annotation_parser</code></td>
<td>
<p>a string containing options for the dependency parser.  This can be either 'none' or 'default' or a list 
of options as mentioned in the UDPipe manual. See the vignette <code>vignette("udpipe-train", package = "udpipe")</code> or
go directly to <a href="https://ufal.mff.cuni.cz/udpipe/1/users-manual#model_training_parser">https://ufal.mff.cuni.cz/udpipe/1/users-manual#model_training_parser</a> for a full description of the options
or see the examples below.
Defaults to 'default'. If you specify 'none', the model will not be able to perform dependency parsing.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In order to train a model, you need to provide files which are in CONLL-U format in argument <code>files_conllu_training</code>. 
This can be a vector of files or just one file. If you do not have your own CONLL-U files, you can download files for your language of 
choice at <a href="https://universaldependencies.org">https://universaldependencies.org</a>. <br />
</p>
<p>At the time of writing open data in CONLL-U
format for 50 languages are available at <a href="https://universaldependencies.org">https://universaldependencies.org</a>, namely for: 
ancient_greek, arabic, basque, belarusian, bulgarian, catalan, chinese, coptic, croatian, 
czech, danish, dutch, english, estonian, finnish, french, galician, german, gothic, greek, hebrew, hindi, hungarian, 
indonesian, irish, italian, japanese, kazakh, korean, latin, latvian, lithuanian, norwegian, 
old_church_slavonic, persian, polish, portuguese, romanian, russian, sanskrit, slovak, 
slovenian, spanish, swedish, tamil, turkish, ukrainian, urdu, uyghur, vietnamese.
</p>


<h3>Value</h3>

<p>A list with elements 
</p>

<ul>
<li><p>file: The path to the model, which can be used in <code>udpipe_load_model</code>
</p>
</li>
<li><p>annotation_tokenizer: The input argument <code>annotation_tokenizer</code>
</p>
</li>
<li><p>annotation_tagger: The input argument <code>annotation_tagger</code>
</p>
</li>
<li><p>annotation_parser: The input argument <code>annotation_parser</code>
</p>
</li>
<li><p>errors: Messages from the UDPipe process indicating possible errors for example when passing the wrong arguments to the 
annotation_tokenizer, annotation_tagger or annotation_parser
</p>
</li></ul>



<h3>References</h3>

<p><a href="https://ufal.mff.cuni.cz/udpipe/1/users-manual">https://ufal.mff.cuni.cz/udpipe/1/users-manual</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+udpipe_annotation_params">udpipe_annotation_params</a></code>, <code><a href="#topic+udpipe_annotate">udpipe_annotate</a></code>, <code><a href="#topic+udpipe_load_model">udpipe_load_model</a></code>,
<code><a href="#topic+udpipe_accuracy">udpipe_accuracy</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## You need to have a file on disk in CONLL-U format, taking the toy example file put in the package
file_conllu &lt;- system.file(package = "udpipe", "dummydata", "traindata.conllu")
file_conllu
cat(head(readLines(file_conllu), 3), sep="\n")

## Not run: 
##
## This is a toy example showing how to build a model, it is not a good model whatsoever, 
##   because model building takes more than 5 seconds this model is saved also in 
##   the file at system.file(package = "udpipe", "dummydata", "toymodel.udpipe")
##
m &lt;- udpipe_train(file = "toymodel.udpipe", files_conllu_training = file_conllu, 
  annotation_tokenizer = list(dimension = 16, epochs = 1, batch_size = 100, dropout = 0.7), 
  annotation_tagger = list(iterations = 1, models = 1, 
     provide_xpostag = 1, provide_lemma = 0, provide_feats = 0, 
     guesser_suffix_rules = 2, guesser_prefix_min_count = 2), 
  annotation_parser = list(iterations = 2, 
     embedding_upostag = 20, embedding_feats = 20, embedding_xpostag = 0, embedding_form = 50, 
     embedding_lemma = 0, embedding_deprel = 20, learning_rate = 0.01, 
     learning_rate_final = 0.001, l2 = 0.5, hidden_layer = 200, 
     batch_size = 10, transition_system = "projective", transition_oracle = "dynamic", 
     structured_interval = 10))

## End(Not run)

file_model &lt;- system.file(package = "udpipe", "dummydata", "toymodel.udpipe")
ud_toymodel &lt;- udpipe_load_model(file_model)
x &lt;- udpipe_annotate(object = ud_toymodel, x = "Ik ging deze morgen naar de bakker brood halen.")
x &lt;- as.data.frame(x)

##
## The above was a toy example showing how to build a model, if you want real-life scenario's
## look at the training parameter examples given below and train it on your CONLL-U file
##
## Example training arguments used for the models available at udpipe_download_model
data(udpipe_annotation_params)
head(udpipe_annotation_params$tokenizer)
head(udpipe_annotation_params$tagger)
head(udpipe_annotation_params$parser)
## Not run: 
## More details in the package vignette:
vignette("udpipe-train", package = "udpipe")

## End(Not run)
</code></pre>

<hr>
<h2 id='unique_identifier'>Create a unique identifier for each combination of fields in a data frame</h2><span id='topic+unique_identifier'></span>

<h3>Description</h3>

<p>Create a unique identifier for each combination of fields in a data frame. 
This unique identifier is unique for each combination of the elements of the fields. 
The generated identifier is like a primary key or a secondary key on a table.
This is just a small wrapper around <code><a href="data.table.html#topic+frank">frank</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unique_identifier(x, fields, start_from = 1L)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="unique_identifier_+3A_x">x</code></td>
<td>
<p>a data.frame</p>
</td></tr>
<tr><td><code id="unique_identifier_+3A_fields">fields</code></td>
<td>
<p>a character vector of columns from <code>x</code></p>
</td></tr>
<tr><td><code id="unique_identifier_+3A_start_from">start_from</code></td>
<td>
<p>integer number indicating to start from that number onwards</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an integer vector of the same length as the number of rows in <code>x</code> 
containing the unique identifier
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(brussels_reviews_anno)
x &lt;- brussels_reviews_anno
x$doc_sent_id &lt;- unique_identifier(x, fields = c("doc_id", "sentence_id"))
head(x, 15)
range(x$doc_sent_id)
x$doc_sent_id &lt;- unique_identifier(x, fields = c("doc_id", "sentence_id"), start_from = 10)
head(x, 15)
range(x$doc_sent_id)
</code></pre>

<hr>
<h2 id='unlist_tokens'>Create a data.frame from a list of tokens</h2><span id='topic+unlist_tokens'></span>

<h3>Description</h3>

<p>Create a data.frame from a list of tokens.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unlist_tokens(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="unlist_tokens_+3A_x">x</code></td>
<td>
<p>a list where the list elements are character vectors of tokens</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the data of <code>x</code> converted to a data.frame.
This data.frame has columns doc_id and token where the doc_id is taken from the list names of x
and token contains the data of <code>x</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- setNames(c("some text here", "hi  there understand this?"), c("a", "b"))
x &lt;- strsplit(x, split = " ")
x
unlist_tokens(x)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
