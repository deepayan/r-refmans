<!DOCTYPE html><html><head><title>Help for package mlr3tuning</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {mlr3tuning}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ArchiveTuning'><p>Class for Logging Evaluated Hyperparameter Configurations</p></a></li>
<li><a href='#as_search_space'><p>Convert to a Search Space</p></a></li>
<li><a href='#auto_tuner'><p>Function for Automatic Tuning</p></a></li>
<li><a href='#AutoTuner'><p>Class for Automatic Tuning</p></a></li>
<li><a href='#callback_tuning'><p>Create Tuning Callback</p></a></li>
<li><a href='#CallbackTuning'><p>Create Tuning Callback</p></a></li>
<li><a href='#ContextEval'><p>Evaluation Context</p></a></li>
<li><a href='#extract_inner_tuning_archives'><p>Extract Inner Tuning Archives</p></a></li>
<li><a href='#extract_inner_tuning_results'><p>Extract Inner Tuning Results</p></a></li>
<li><a href='#mlr_tuners'><p>Dictionary of Tuners</p></a></li>
<li><a href='#mlr_tuners_cmaes'><p>Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy</p></a></li>
<li><a href='#mlr_tuners_design_points'><p>Hyperparameter Tuning with Design Points</p></a></li>
<li><a href='#mlr_tuners_gensa'><p>Hyperparameter Tuning with Generalized Simulated Annealing</p></a></li>
<li><a href='#mlr_tuners_grid_search'><p>Hyperparameter Tuning with Grid Search</p></a></li>
<li><a href='#mlr_tuners_irace'><p>Hyperparameter Tuning with Iterated Racing.</p></a></li>
<li><a href='#mlr_tuners_nloptr'><p>Hyperparameter Tuning with Non-linear Optimization</p></a></li>
<li><a href='#mlr_tuners_random_search'><p>Hyperparameter Tuning with Random Search</p></a></li>
<li><a href='#mlr3tuning-package'><p>mlr3tuning: Hyperparameter Optimization for 'mlr3'</p></a></li>
<li><a href='#mlr3tuning.backup'><p>Backup Benchmark Result Callback</p></a></li>
<li><a href='#mlr3tuning.early_stopping'><p>Early Stopping Callback</p></a></li>
<li><a href='#mlr3tuning.measures'><p>Measure Callback</p></a></li>
<li><a href='#ObjectiveTuning'><p>Class for Tuning Objective</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#ti'><p>Syntactic Sugar for Tuning Instance Construction</p></a></li>
<li><a href='#tnr'><p>Syntactic Sugar for Tuning Objects Construction</p></a></li>
<li><a href='#tune'><p>Function for Tuning a Learner</p></a></li>
<li><a href='#tune_nested'><p>Function for Nested Resampling</p></a></li>
<li><a href='#Tuner'><p>Class for Tuning Algorithms</p></a></li>
<li><a href='#TunerFromOptimizer'><p>TunerFromOptimizer</p></a></li>
<li><a href='#TuningInstanceMultiCrit'><p>Class for Multi Criteria Tuning</p></a></li>
<li><a href='#TuningInstanceSingleCrit'><p>Class for Single Criterion Tuning</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Hyperparameter Optimization for 'mlr3'</td>
</tr>
<tr>
<td>Version:</td>
<td>0.20.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Hyperparameter optimization package of the 'mlr3' ecosystem. It
    features highly configurable search spaces via the 'paradox' package and
    finds optimal hyperparameter configurations for any 'mlr3' learner.
    'mlr3tuning' works with several optimization algorithms e.g. Random
    Search, Iterated Racing, Bayesian Optimization (in 'mlr3mbo') and
    Hyperband (in 'mlr3hyperband'). Moreover, it can automatically optimize
    learners and estimate the performance of optimized models with nested
    resampling.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/LGPL-3">LGPL-3</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://mlr3tuning.mlr-org.com">https://mlr3tuning.mlr-org.com</a>,
<a href="https://github.com/mlr-org/mlr3tuning">https://github.com/mlr-org/mlr3tuning</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/mlr-org/mlr3tuning/issues">https://github.com/mlr-org/mlr3tuning/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>mlr3 (&ge; 0.17.0), paradox (&ge; 0.10.0), R (&ge; 3.1.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>bbotk (&ge; 0.7.3), checkmate (&ge; 2.0.0), data.table, lgr,
mlr3misc (&ge; 0.13.0), R6</td>
</tr>
<tr>
<td>Suggests:</td>
<td>adagio, GenSA, irace, knitr, mlr3learners (&ge; 0.5.5),
mlr3pipelines, nloptr, rmarkdown, rpart, testthat (&ge; 3.0.0),
xgboost</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Config/testthat/parallel:</td>
<td>true</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>Collate:</td>
<td>'ArchiveTuning.R' 'AutoTuner.R' 'CallbackTuning.R'
'ContextEval.R' 'ObjectiveTuning.R' 'mlr_tuners.R' 'Tuner.R'
'TunerCmaes.R' 'TunerDesignPoints.R' 'TunerFromOptimizer.R'
'TunerGenSA.R' 'TunerGridSearch.R' 'TunerIrace.R'
'TunerNLoptr.R' 'TunerRandomSearch.R'
'TuningInstanceSingleCrit.R' 'TuningInstanceMulticrit.R'
'as_search_space.R' 'assertions.R' 'auto_tuner.R'
'bibentries.R' 'extract_inner_tuning_archives.R'
'extract_inner_tuning_results.R' 'helper.R' 'mlr_callbacks.R'
'reexport.R' 'sugar.R' 'tune.R' 'tune_nested.R' 'zzz.R'</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-04 15:40:08 UTC; marc</td>
</tr>
<tr>
<td>Author:</td>
<td>Marc Becker <a href="https://orcid.org/0000-0002-8115-0400"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [cre,
    aut],
  Michel Lang <a href="https://orcid.org/0000-0001-9754-0393"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Jakob Richter <a href="https://orcid.org/0000-0003-4481-5554"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Bernd Bischl <a href="https://orcid.org/0000-0001-6002-6980"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Daniel Schalk <a href="https://orcid.org/0000-0003-0950-1947"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Marc Becker &lt;marcbecker@posteo.de&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-05 05:30:14 UTC</td>
</tr>
</table>
<hr>
<h2 id='ArchiveTuning'>Class for Logging Evaluated Hyperparameter Configurations</h2><span id='topic+ArchiveTuning'></span>

<h3>Description</h3>

<p>The <a href="#topic+ArchiveTuning">ArchiveTuning</a> stores all evaluated hyperparameter configurations and performance scores.
</p>


<h3>Details</h3>

<p>The <a href="#topic+ArchiveTuning">ArchiveTuning</a> is a container around a <code><a href="data.table.html#topic+data.table">data.table::data.table()</a></code>.
Each row corresponds to a single evaluation of a hyperparameter configuration.
See the section on Data Structure for more information.
The archive stores additionally a <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a> (<code style="white-space: pre;">&#8288;$benchmark_result&#8288;</code>) that records the resampling experiments.
Each experiment corresponds to to a single evaluation of a hyperparameter configuration.
The table (<code style="white-space: pre;">&#8288;$data&#8288;</code>) and the benchmark result (<code style="white-space: pre;">&#8288;$benchmark_result&#8288;</code>) are linked by the <code>uhash</code> column.
If the archive is passed to <code>as.data.table()</code>, both are joined automatically.
</p>


<h3>Data Structure</h3>

<p>The table (<code style="white-space: pre;">&#8288;$data&#8288;</code>) has the following columns:
</p>

<ul>
<li><p> One column for each hyperparameter of the search space (<code style="white-space: pre;">&#8288;$search_space&#8288;</code>).
</p>
</li>
<li><p> One column for each performance measure (<code style="white-space: pre;">&#8288;$codomain&#8288;</code>).
</p>
</li>
<li> <p><code>x_domain</code> (<code>list()</code>)<br />
Lists of (transformed) hyperparameter values that are passed to the learner.
</p>
</li>
<li> <p><code>runtime_learners</code> (<code>numeric(1)</code>)<br />
Sum of training and predict times logged in learners per <a href="mlr3.html#topic+ResampleResult">mlr3::ResampleResult</a> / evaluation.
This does not include potential overhead time.
</p>
</li>
<li> <p><code>timestamp</code> (<code>POSIXct</code>)<br />
Time stamp when the evaluation was logged into the archive.
</p>
</li>
<li> <p><code>batch_nr</code> (<code>integer(1)</code>)<br />
Hyperparameters are evaluated in batches.
Each batch has a unique batch number.
</p>
</li>
<li> <p><code>uhash</code> (<code>character(1)</code>)<br />
Connects each hyperparameter configuration to the resampling experiment stored in the <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>.
</p>
</li></ul>



<h3>Analysis</h3>

<p>For analyzing the tuning results, it is recommended to pass the <a href="#topic+ArchiveTuning">ArchiveTuning</a> to <code>as.data.table()</code>.
The returned data table is joined with the benchmark result which adds the <a href="mlr3.html#topic+ResampleResult">mlr3::ResampleResult</a> for each hyperparameter evaluation.
</p>
<p>The archive provides various getters (e.g. <code style="white-space: pre;">&#8288;$learners()&#8288;</code>) to ease the access.
All getters extract by position (<code>i</code>) or unique hash (<code>uhash</code>).
For a complete list of all getters see the methods section.
</p>
<p>The benchmark result (<code style="white-space: pre;">&#8288;$benchmark_result&#8288;</code>) allows to score the hyperparameter configurations again on a different measure.
Alternatively, measures can be supplied to <code>as.data.table()</code>.
</p>
<p>The <a href="https://CRAN.R-project.org/package=mlr3viz"><span class="pkg">mlr3viz</span></a> package provides visualizations for tuning results.
</p>


<h3>S3 Methods</h3>


<ul>
<li> <p><code>as.data.table.ArchiveTuning(x, unnest = "x_domain", exclude_columns = "uhash", measures = NULL)</code><br />
Returns a tabular view of all evaluated hyperparameter configurations.<br />
<a href="#topic+ArchiveTuning">ArchiveTuning</a> -&gt; <code><a href="data.table.html#topic+data.table">data.table::data.table()</a></code><br />
</p>

<ul>
<li> <p><code>x</code> (<a href="#topic+ArchiveTuning">ArchiveTuning</a>)
</p>
</li>
<li> <p><code>unnest</code> (<code>character()</code>)<br />
Transforms list columns to separate columns. Set to <code>NULL</code> if no column should be unnested.
</p>
</li>
<li> <p><code>exclude_columns</code> (<code>character()</code>)<br />
Exclude columns from table. Set to <code>NULL</code> if no column should be excluded.
</p>
</li>
<li> <p><code>measures</code> (List of <a href="mlr3.html#topic+Measure">mlr3::Measure</a>)<br />
Score hyperparameter configurations on additional measures.
</p>
</li></ul>

</li></ul>



<h3>Super class</h3>

<p><code><a href="bbotk.html#topic+Archive">bbotk::Archive</a></code> -&gt; <code>ArchiveTuning</code>
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>benchmark_result</code></dt><dd><p>(<a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>)<br />
Benchmark result.</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-ArchiveTuning-new"><code>ArchiveTuning$new()</code></a>
</p>
</li>
<li> <p><a href="#method-ArchiveTuning-learner"><code>ArchiveTuning$learner()</code></a>
</p>
</li>
<li> <p><a href="#method-ArchiveTuning-learners"><code>ArchiveTuning$learners()</code></a>
</p>
</li>
<li> <p><a href="#method-ArchiveTuning-learner_param_vals"><code>ArchiveTuning$learner_param_vals()</code></a>
</p>
</li>
<li> <p><a href="#method-ArchiveTuning-predictions"><code>ArchiveTuning$predictions()</code></a>
</p>
</li>
<li> <p><a href="#method-ArchiveTuning-resample_result"><code>ArchiveTuning$resample_result()</code></a>
</p>
</li>
<li> <p><a href="#method-ArchiveTuning-print"><code>ArchiveTuning$print()</code></a>
</p>
</li>
<li> <p><a href="#method-ArchiveTuning-clone"><code>ArchiveTuning$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Archive" data-id="add_evals"><a href='../../bbotk/html/Archive.html#method-Archive-add_evals'><code>bbotk::Archive$add_evals()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Archive" data-id="best"><a href='../../bbotk/html/Archive.html#method-Archive-best'><code>bbotk::Archive$best()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Archive" data-id="clear"><a href='../../bbotk/html/Archive.html#method-Archive-clear'><code>bbotk::Archive$clear()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Archive" data-id="format"><a href='../../bbotk/html/Archive.html#method-Archive-format'><code>bbotk::Archive$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Archive" data-id="nds_selection"><a href='../../bbotk/html/Archive.html#method-Archive-nds_selection'><code>bbotk::Archive$nds_selection()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-ArchiveTuning-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>ArchiveTuning$new(search_space, codomain, check_values = TRUE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>search_space</code></dt><dd><p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>)<br />
Hyperparameter search space. If <code>NULL</code> (default), the search space is
constructed from the <a href="paradox.html#topic+TuneToken">TuneToken</a> of the learner's parameter set
(learner$param_set).</p>
</dd>
<dt><code>codomain</code></dt><dd><p>(<a href="bbotk.html#topic+Codomain">bbotk::Codomain</a>)<br />
Specifies codomain of objective function i.e. a set of performance measures.
Internally created from provided <a href="mlr3.html#topic+Measure">mlr3::Measure</a>s.</p>
</dd>
<dt><code>check_values</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default), hyperparameter configurations are check for validity.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-ArchiveTuning-learner"></a>



<h4>Method <code>learner()</code></h4>

<p>Retrieve <a href="mlr3.html#topic+Learner">mlr3::Learner</a> of the i-th evaluation, by position or by unique hash <code>uhash</code>.
<code>i</code> and <code>uhash</code> are mutually exclusive.
Learner does not contain a model. Use <code style="white-space: pre;">&#8288;$learners()&#8288;</code> to get learners with models.
</p>


<h5>Usage</h5>

<div class="r"><pre>ArchiveTuning$learner(i = NULL, uhash = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>i</code></dt><dd><p>(<code>integer(1)</code>)<br />
The iteration value to filter for.</p>
</dd>
<dt><code>uhash</code></dt><dd><p>(<code>logical(1)</code>)<br />
The <code>uhash</code> value to filter for.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-ArchiveTuning-learners"></a>



<h4>Method <code>learners()</code></h4>

<p>Retrieve list of trained <a href="mlr3.html#topic+Learner">mlr3::Learner</a> objects of the i-th evaluation, by position or by unique hash <code>uhash</code>.
<code>i</code> and <code>uhash</code> are mutually exclusive.
</p>


<h5>Usage</h5>

<div class="r"><pre>ArchiveTuning$learners(i = NULL, uhash = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>i</code></dt><dd><p>(<code>integer(1)</code>)<br />
The iteration value to filter for.</p>
</dd>
<dt><code>uhash</code></dt><dd><p>(<code>logical(1)</code>)<br />
The <code>uhash</code> value to filter for.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-ArchiveTuning-learner_param_vals"></a>



<h4>Method <code>learner_param_vals()</code></h4>

<p>Retrieve param values of the i-th evaluation, by position or by unique hash <code>uhash</code>.
<code>i</code> and <code>uhash</code> are mutually exclusive.
</p>


<h5>Usage</h5>

<div class="r"><pre>ArchiveTuning$learner_param_vals(i = NULL, uhash = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>i</code></dt><dd><p>(<code>integer(1)</code>)<br />
The iteration value to filter for.</p>
</dd>
<dt><code>uhash</code></dt><dd><p>(<code>logical(1)</code>)<br />
The <code>uhash</code> value to filter for.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-ArchiveTuning-predictions"></a>



<h4>Method <code>predictions()</code></h4>

<p>Retrieve list of <a href="mlr3.html#topic+Prediction">mlr3::Prediction</a> objects of the i-th evaluation, by position or by unique hash <code>uhash</code>.
<code>i</code> and <code>uhash</code> are mutually exclusive.
</p>


<h5>Usage</h5>

<div class="r"><pre>ArchiveTuning$predictions(i = NULL, uhash = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>i</code></dt><dd><p>(<code>integer(1)</code>)<br />
The iteration value to filter for.</p>
</dd>
<dt><code>uhash</code></dt><dd><p>(<code>logical(1)</code>)<br />
The <code>uhash</code> value to filter for.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-ArchiveTuning-resample_result"></a>



<h4>Method <code>resample_result()</code></h4>

<p>Retrieve <a href="mlr3.html#topic+ResampleResult">mlr3::ResampleResult</a> of the i-th evaluation, by position or by unique hash <code>uhash</code>.
<code>i</code> and <code>uhash</code> are mutually exclusive.
</p>


<h5>Usage</h5>

<div class="r"><pre>ArchiveTuning$resample_result(i = NULL, uhash = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>i</code></dt><dd><p>(<code>integer(1)</code>)<br />
The iteration value to filter for.</p>
</dd>
<dt><code>uhash</code></dt><dd><p>(<code>logical(1)</code>)<br />
The <code>uhash</code> value to filter for.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-ArchiveTuning-print"></a>



<h4>Method <code>print()</code></h4>

<p>Printer.
</p>


<h5>Usage</h5>

<div class="r"><pre>ArchiveTuning$print()</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>...</code></dt><dd><p>(ignored).</p>
</dd>
</dl>

</div>


<hr>
<a id="method-ArchiveTuning-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>ArchiveTuning$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='as_search_space'>Convert to a Search Space</h2><span id='topic+as_search_space'></span><span id='topic+as_search_space.Learner'></span><span id='topic+as_search_space.ParamSet'></span>

<h3>Description</h3>

<p>Convert object to a search space.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_search_space(x, ...)

## S3 method for class 'Learner'
as_search_space(x, ...)

## S3 method for class 'ParamSet'
as_search_space(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as_search_space_+3A_x">x</code></td>
<td>
<p>(<code>any</code>)<br />
Object to convert to search space.</p>
</td></tr>
<tr><td><code id="as_search_space_+3A_...">...</code></td>
<td>
<p>(any)<br />
Additional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>.
</p>

<hr>
<h2 id='auto_tuner'>Function for Automatic Tuning</h2><span id='topic+auto_tuner'></span>

<h3>Description</h3>

<p>The <a href="#topic+AutoTuner">AutoTuner</a> wraps a <a href="mlr3.html#topic+Learner">mlr3::Learner</a> and augments it with an automatic tuning process for a given set of hyperparameters.
The <code><a href="#topic+auto_tuner">auto_tuner()</a></code> function creates an <a href="#topic+AutoTuner">AutoTuner</a> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>auto_tuner(
  tuner,
  learner,
  resampling,
  measure = NULL,
  term_evals = NULL,
  term_time = NULL,
  terminator = NULL,
  search_space = NULL,
  store_tuning_instance = TRUE,
  store_benchmark_result = TRUE,
  store_models = FALSE,
  check_values = FALSE,
  allow_hotstart = FALSE,
  keep_hotstart_stack = FALSE,
  evaluate_default = FALSE,
  callbacks = list(),
  method
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="auto_tuner_+3A_tuner">tuner</code></td>
<td>
<p>(<a href="#topic+Tuner">Tuner</a>)<br />
Optimization algorithm.</p>
</td></tr>
<tr><td><code id="auto_tuner_+3A_learner">learner</code></td>
<td>
<p>(<a href="mlr3.html#topic+Learner">mlr3::Learner</a>)<br />
Learner to tune.</p>
</td></tr>
<tr><td><code id="auto_tuner_+3A_resampling">resampling</code></td>
<td>
<p>(<a href="mlr3.html#topic+Resampling">mlr3::Resampling</a>)<br />
Resampling that is used to evaluate the performance of the hyperparameter configurations.
Uninstantiated resamplings are instantiated during construction so that all configurations are evaluated on the same data splits.
Already instantiated resamplings are kept unchanged.
Specialized <a href="#topic+Tuner">Tuner</a> change the resampling e.g. to evaluate a hyperparameter configuration on different data splits.
This field, however, always returns the resampling passed in construction.</p>
</td></tr>
<tr><td><code id="auto_tuner_+3A_measure">measure</code></td>
<td>
<p>(<a href="mlr3.html#topic+Measure">mlr3::Measure</a>)<br />
Measure to optimize. If <code>NULL</code>, default measure is used.</p>
</td></tr>
<tr><td><code id="auto_tuner_+3A_term_evals">term_evals</code></td>
<td>
<p>(<code>integer(1)</code>)<br />
Number of allowed evaluations.
Ignored if <code>terminator</code> is passed.</p>
</td></tr>
<tr><td><code id="auto_tuner_+3A_term_time">term_time</code></td>
<td>
<p>(<code>integer(1)</code>)<br />
Maximum allowed time in seconds.
Ignored if <code>terminator</code> is passed.</p>
</td></tr>
<tr><td><code id="auto_tuner_+3A_terminator">terminator</code></td>
<td>
<p>(<a href="bbotk.html#topic+Terminator">Terminator</a>)<br />
Stop criterion of the tuning process.</p>
</td></tr>
<tr><td><code id="auto_tuner_+3A_search_space">search_space</code></td>
<td>
<p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>)<br />
Hyperparameter search space. If <code>NULL</code> (default), the search space is
constructed from the <a href="paradox.html#topic+TuneToken">TuneToken</a> of the learner's parameter set
(learner$param_set).</p>
</td></tr>
<tr><td><code id="auto_tuner_+3A_store_tuning_instance">store_tuning_instance</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default), stores the internally created <a href="#topic+TuningInstanceSingleCrit">TuningInstanceSingleCrit</a> with all intermediate results in slot <code style="white-space: pre;">&#8288;$tuning_instance&#8288;</code>.</p>
</td></tr>
<tr><td><code id="auto_tuner_+3A_store_benchmark_result">store_benchmark_result</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default), store resample result of evaluated hyperparameter
configurations in archive as <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>.</p>
</td></tr>
<tr><td><code id="auto_tuner_+3A_store_models">store_models</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, fitted models are stored in the benchmark result
(<code>archive$benchmark_result</code>). If <code>store_benchmark_result = FALSE</code>, models
are only stored temporarily and not accessible after the tuning. This
combination is needed for measures that require a model.</p>
</td></tr>
<tr><td><code id="auto_tuner_+3A_check_values">check_values</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, hyperparameter values are checked before evaluation and
performance scores after. If <code>FALSE</code> (default), values are unchecked but
computational overhead is reduced.</p>
</td></tr>
<tr><td><code id="auto_tuner_+3A_allow_hotstart">allow_hotstart</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
Allow to hotstart learners with previously fitted models. See also
<a href="mlr3.html#topic+HotstartStack">mlr3::HotstartStack</a>. The learner must support hotstarting. Sets
<code>store_models = TRUE</code>.</p>
</td></tr>
<tr><td><code id="auto_tuner_+3A_keep_hotstart_stack">keep_hotstart_stack</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, <a href="mlr3.html#topic+HotstartStack">mlr3::HotstartStack</a> is kept in <code style="white-space: pre;">&#8288;$objective$hotstart_stack&#8288;</code>
after tuning.</p>
</td></tr>
<tr><td><code id="auto_tuner_+3A_evaluate_default">evaluate_default</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, learner is evaluated with hyperparameters set to their default
values at the start of the optimization.</p>
</td></tr>
<tr><td><code id="auto_tuner_+3A_callbacks">callbacks</code></td>
<td>
<p>(list of <a href="#topic+CallbackTuning">CallbackTuning</a>)<br />
List of callbacks.</p>
</td></tr>
<tr><td><code id="auto_tuner_+3A_method">method</code></td>
<td>
<p>(<code>character(1)</code>)<br />
Deprecated. Use <code>tuner</code> instead.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <a href="#topic+AutoTuner">AutoTuner</a> is a <a href="mlr3.html#topic+Learner">mlr3::Learner</a> which wraps another <a href="mlr3.html#topic+Learner">mlr3::Learner</a> and performs the following steps during <code style="white-space: pre;">&#8288;$train()&#8288;</code>:
</p>

<ol>
<li><p> The hyperparameters of the wrapped (inner) learner are trained on the training data via resampling.
The tuning can be specified by providing a <a href="#topic+Tuner">Tuner</a>, a <a href="bbotk.html#topic+Terminator">bbotk::Terminator</a>, a search space as <a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>, a <a href="mlr3.html#topic+Resampling">mlr3::Resampling</a> and a <a href="mlr3.html#topic+Measure">mlr3::Measure</a>.
</p>
</li>
<li><p> The best found hyperparameter configuration is set as hyperparameters for the wrapped (inner) learner stored in <code>at$learner</code>.
Access the tuned hyperparameters via <code>at$tuning_result</code>.
</p>
</li>
<li><p> A final model is fit on the complete training data using the now parametrized wrapped learner.
The respective model is available via field <code>at$learner$model</code>.
</p>
</li></ol>

<p>During <code style="white-space: pre;">&#8288;$predict()&#8288;</code> the <code>AutoTuner</code> just calls the predict method of the wrapped (inner) learner.
A set timeout is disabled while fitting the final model.
</p>


<h3>Value</h3>

<p><a href="#topic+AutoTuner">AutoTuner</a>.
</p>


<h3>Default Measures</h3>

<p>If no measure is passed, the default measure is used.
The default measure depends on the task type.</p>

<table>
<tr>
 <td style="text-align: left;">
   Task </td><td style="text-align: left;"> Default Measure </td><td style="text-align: left;"> Package </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"classif"</code> </td><td style="text-align: left;"> <code>"classif.ce"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3"><span class="pkg">mlr3</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"regr"</code> </td><td style="text-align: left;"> <code>"regr.mse"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3"><span class="pkg">mlr3</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"surv"</code> </td><td style="text-align: left;"> <code>"surv.cindex"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3proba"><span class="pkg">mlr3proba</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"dens"</code> </td><td style="text-align: left;"> <code>"dens.logloss"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3proba"><span class="pkg">mlr3proba</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"classif_st"</code> </td><td style="text-align: left;"> <code>"classif.ce"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3spatial"><span class="pkg">mlr3spatial</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"regr_st"</code> </td><td style="text-align: left;"> <code>"regr.mse"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3spatial"><span class="pkg">mlr3spatial</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"clust"</code> </td><td style="text-align: left;"> <code>"clust.dunn"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3cluster"><span class="pkg">mlr3cluster</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li> <p><a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-autotuner">Automate</a> the tuning.
</p>
</li>
<li><p> Estimate the model performance with <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-nested-resampling">nested resampling</a>.
</p>
</li></ul>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>


<h3>Nested Resampling</h3>

<p>Nested resampling is performed by passing an <a href="#topic+AutoTuner">AutoTuner</a> to <code><a href="mlr3.html#topic+resample">mlr3::resample()</a></code> or <code><a href="mlr3.html#topic+benchmark">mlr3::benchmark()</a></code>.
To access the inner resampling results, set <code>store_tuning_instance = TRUE</code> and execute <code><a href="mlr3.html#topic+resample">mlr3::resample()</a></code> or <code><a href="mlr3.html#topic+benchmark">mlr3::benchmark()</a></code> with <code>store_models = TRUE</code> (see examples).
The <a href="mlr3.html#topic+Resampling">mlr3::Resampling</a> passed to the <a href="#topic+AutoTuner">AutoTuner</a> is meant to be the inner resampling, operating on the training set of an arbitrary outer resampling.
For this reason, the inner resampling should be not instantiated.
If an instantiated resampling is passed, the <a href="#topic+AutoTuner">AutoTuner</a> fails when a row id of the inner resampling is not present in the training set of the outer resampling.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>at = auto_tuner(
  tuner = tnr("random_search"),
  learner = lrn("classif.rpart", cp = to_tune(1e-04, 1e-1, logscale = TRUE)),
  resampling = rsmp ("holdout"),
  measure = msr("classif.ce"),
  term_evals = 4)

at$train(tsk("pima"))
</code></pre>

<hr>
<h2 id='AutoTuner'>Class for Automatic Tuning</h2><span id='topic+AutoTuner'></span>

<h3>Description</h3>

<p>The <a href="#topic+AutoTuner">AutoTuner</a> wraps a <a href="mlr3.html#topic+Learner">mlr3::Learner</a> and augments it with an automatic tuning process for a given set of hyperparameters.
The <code><a href="#topic+auto_tuner">auto_tuner()</a></code> function creates an <a href="#topic+AutoTuner">AutoTuner</a> object.
</p>


<h3>Details</h3>

<p>The <a href="#topic+AutoTuner">AutoTuner</a> is a <a href="mlr3.html#topic+Learner">mlr3::Learner</a> which wraps another <a href="mlr3.html#topic+Learner">mlr3::Learner</a> and performs the following steps during <code style="white-space: pre;">&#8288;$train()&#8288;</code>:
</p>

<ol>
<li><p> The hyperparameters of the wrapped (inner) learner are trained on the training data via resampling.
The tuning can be specified by providing a <a href="#topic+Tuner">Tuner</a>, a <a href="bbotk.html#topic+Terminator">bbotk::Terminator</a>, a search space as <a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>, a <a href="mlr3.html#topic+Resampling">mlr3::Resampling</a> and a <a href="mlr3.html#topic+Measure">mlr3::Measure</a>.
</p>
</li>
<li><p> The best found hyperparameter configuration is set as hyperparameters for the wrapped (inner) learner stored in <code>at$learner</code>.
Access the tuned hyperparameters via <code>at$tuning_result</code>.
</p>
</li>
<li><p> A final model is fit on the complete training data using the now parametrized wrapped learner.
The respective model is available via field <code>at$learner$model</code>.
</p>
</li></ol>

<p>During <code style="white-space: pre;">&#8288;$predict()&#8288;</code> the <code>AutoTuner</code> just calls the predict method of the wrapped (inner) learner.
A set timeout is disabled while fitting the final model.
</p>


<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li> <p><a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-autotuner">Automate</a> the tuning.
</p>
</li>
<li><p> Estimate the model performance with <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-nested-resampling">nested resampling</a>.
</p>
</li></ul>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>


<h3>Nested Resampling</h3>

<p>Nested resampling is performed by passing an <a href="#topic+AutoTuner">AutoTuner</a> to <code><a href="mlr3.html#topic+resample">mlr3::resample()</a></code> or <code><a href="mlr3.html#topic+benchmark">mlr3::benchmark()</a></code>.
To access the inner resampling results, set <code>store_tuning_instance = TRUE</code> and execute <code><a href="mlr3.html#topic+resample">mlr3::resample()</a></code> or <code><a href="mlr3.html#topic+benchmark">mlr3::benchmark()</a></code> with <code>store_models = TRUE</code> (see examples).
The <a href="mlr3.html#topic+Resampling">mlr3::Resampling</a> passed to the <a href="#topic+AutoTuner">AutoTuner</a> is meant to be the inner resampling, operating on the training set of an arbitrary outer resampling.
For this reason, the inner resampling should be not instantiated.
If an instantiated resampling is passed, the <a href="#topic+AutoTuner">AutoTuner</a> fails when a row id of the inner resampling is not present in the training set of the outer resampling.
</p>


<h3>Default Measures</h3>

<p>If no measure is passed, the default measure is used.
The default measure depends on the task type.</p>

<table>
<tr>
 <td style="text-align: left;">
   Task </td><td style="text-align: left;"> Default Measure </td><td style="text-align: left;"> Package </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"classif"</code> </td><td style="text-align: left;"> <code>"classif.ce"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3"><span class="pkg">mlr3</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"regr"</code> </td><td style="text-align: left;"> <code>"regr.mse"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3"><span class="pkg">mlr3</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"surv"</code> </td><td style="text-align: left;"> <code>"surv.cindex"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3proba"><span class="pkg">mlr3proba</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"dens"</code> </td><td style="text-align: left;"> <code>"dens.logloss"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3proba"><span class="pkg">mlr3proba</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"classif_st"</code> </td><td style="text-align: left;"> <code>"classif.ce"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3spatial"><span class="pkg">mlr3spatial</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"regr_st"</code> </td><td style="text-align: left;"> <code>"regr.mse"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3spatial"><span class="pkg">mlr3spatial</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"clust"</code> </td><td style="text-align: left;"> <code>"clust.dunn"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3cluster"><span class="pkg">mlr3cluster</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Super class</h3>

<p><code><a href="mlr3.html#topic+Learner">mlr3::Learner</a></code> -&gt; <code>AutoTuner</code>
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>instance_args</code></dt><dd><p>(<code>list()</code>)<br />
All arguments from construction to create the <a href="#topic+TuningInstanceSingleCrit">TuningInstanceSingleCrit</a>.</p>
</dd>
<dt><code>tuner</code></dt><dd><p>(<a href="#topic+Tuner">Tuner</a>)<br />
Optimization algorithm.</p>
</dd>
</dl>

</div>


<h3>Active bindings</h3>

<div class="r6-active-bindings">

<dl>
<dt><code>archive</code></dt><dd><p><a href="#topic+ArchiveTuning">ArchiveTuning</a><br />
Archive of the <a href="#topic+TuningInstanceSingleCrit">TuningInstanceSingleCrit</a>.</p>
</dd>
<dt><code>learner</code></dt><dd><p>(<a href="mlr3.html#topic+Learner">mlr3::Learner</a>)<br />
Trained learner</p>
</dd>
<dt><code>tuning_instance</code></dt><dd><p>(<a href="#topic+TuningInstanceSingleCrit">TuningInstanceSingleCrit</a>)<br />
Internally created tuning instance with all intermediate results.</p>
</dd>
<dt><code>tuning_result</code></dt><dd><p>(<a href="data.table.html#topic+data.table">data.table::data.table</a>)<br />
Short-cut to <code>result</code> from <a href="#topic+TuningInstanceSingleCrit">TuningInstanceSingleCrit</a>.</p>
</dd>
<dt><code>predict_type</code></dt><dd><p>(<code>character(1)</code>)<br />
Stores the currently active predict type, e.g. <code>"response"</code>.
Must be an element of <code style="white-space: pre;">&#8288;$predict_types&#8288;</code>.</p>
</dd>
<dt><code>hash</code></dt><dd><p>(<code>character(1)</code>)<br />
Hash (unique identifier) for this object.</p>
</dd>
<dt><code>phash</code></dt><dd><p>(<code>character(1)</code>)<br />
Hash (unique identifier) for this partial object, excluding some components which are varied systematically during tuning (parameter values) or feature selection (feature names).</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-AutoTuner-new"><code>AutoTuner$new()</code></a>
</p>
</li>
<li> <p><a href="#method-AutoTuner-base_learner"><code>AutoTuner$base_learner()</code></a>
</p>
</li>
<li> <p><a href="#method-AutoTuner-importance"><code>AutoTuner$importance()</code></a>
</p>
</li>
<li> <p><a href="#method-AutoTuner-selected_features"><code>AutoTuner$selected_features()</code></a>
</p>
</li>
<li> <p><a href="#method-AutoTuner-oob_error"><code>AutoTuner$oob_error()</code></a>
</p>
</li>
<li> <p><a href="#method-AutoTuner-loglik"><code>AutoTuner$loglik()</code></a>
</p>
</li>
<li> <p><a href="#method-AutoTuner-print"><code>AutoTuner$print()</code></a>
</p>
</li>
<li> <p><a href="#method-AutoTuner-clone"><code>AutoTuner$clone()</code></a>
</p>
</li></ul>



<details><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="format"><a href='../../mlr3/html/Learner.html#method-Learner-format'><code>mlr3::Learner$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="help"><a href='../../mlr3/html/Learner.html#method-Learner-help'><code>mlr3::Learner$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="predict"><a href='../../mlr3/html/Learner.html#method-Learner-predict'><code>mlr3::Learner$predict()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="predict_newdata"><a href='../../mlr3/html/Learner.html#method-Learner-predict_newdata'><code>mlr3::Learner$predict_newdata()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="reset"><a href='../../mlr3/html/Learner.html#method-Learner-reset'><code>mlr3::Learner$reset()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="train"><a href='../../mlr3/html/Learner.html#method-Learner-train'><code>mlr3::Learner$train()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-AutoTuner-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>AutoTuner$new(
  tuner,
  learner,
  resampling,
  measure = NULL,
  terminator,
  search_space = NULL,
  store_tuning_instance = TRUE,
  store_benchmark_result = TRUE,
  store_models = FALSE,
  check_values = FALSE,
  allow_hotstart = FALSE,
  keep_hotstart_stack = FALSE,
  evaluate_default = FALSE,
  callbacks = list()
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>tuner</code></dt><dd><p>(<a href="#topic+Tuner">Tuner</a>)<br />
Optimization algorithm.</p>
</dd>
<dt><code>learner</code></dt><dd><p>(<a href="mlr3.html#topic+Learner">mlr3::Learner</a>)<br />
Learner to tune.</p>
</dd>
<dt><code>resampling</code></dt><dd><p>(<a href="mlr3.html#topic+Resampling">mlr3::Resampling</a>)<br />
Resampling that is used to evaluate the performance of the hyperparameter configurations.
Uninstantiated resamplings are instantiated during construction so that all configurations are evaluated on the same data splits.
Already instantiated resamplings are kept unchanged.
Specialized <a href="#topic+Tuner">Tuner</a> change the resampling e.g. to evaluate a hyperparameter configuration on different data splits.
This field, however, always returns the resampling passed in construction.</p>
</dd>
<dt><code>measure</code></dt><dd><p>(<a href="mlr3.html#topic+Measure">mlr3::Measure</a>)<br />
Measure to optimize. If <code>NULL</code>, default measure is used.</p>
</dd>
<dt><code>terminator</code></dt><dd><p>(<a href="bbotk.html#topic+Terminator">Terminator</a>)<br />
Stop criterion of the tuning process.</p>
</dd>
<dt><code>search_space</code></dt><dd><p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>)<br />
Hyperparameter search space. If <code>NULL</code> (default), the search space is
constructed from the <a href="paradox.html#topic+TuneToken">TuneToken</a> of the learner's parameter set
(learner$param_set).</p>
</dd>
<dt><code>store_tuning_instance</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default), stores the internally created <a href="#topic+TuningInstanceSingleCrit">TuningInstanceSingleCrit</a> with all intermediate results in slot <code style="white-space: pre;">&#8288;$tuning_instance&#8288;</code>.</p>
</dd>
<dt><code>store_benchmark_result</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default), store resample result of evaluated hyperparameter
configurations in archive as <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>.</p>
</dd>
<dt><code>store_models</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, fitted models are stored in the benchmark result
(<code>archive$benchmark_result</code>). If <code>store_benchmark_result = FALSE</code>, models
are only stored temporarily and not accessible after the tuning. This
combination is needed for measures that require a model.</p>
</dd>
<dt><code>check_values</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, hyperparameter values are checked before evaluation and
performance scores after. If <code>FALSE</code> (default), values are unchecked but
computational overhead is reduced.</p>
</dd>
<dt><code>allow_hotstart</code></dt><dd><p>(<code>logical(1)</code>)<br />
Allow to hotstart learners with previously fitted models. See also
<a href="mlr3.html#topic+HotstartStack">mlr3::HotstartStack</a>. The learner must support hotstarting. Sets
<code>store_models = TRUE</code>.</p>
</dd>
<dt><code>keep_hotstart_stack</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, <a href="mlr3.html#topic+HotstartStack">mlr3::HotstartStack</a> is kept in <code style="white-space: pre;">&#8288;$objective$hotstart_stack&#8288;</code>
after tuning.</p>
</dd>
<dt><code>evaluate_default</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, learner is evaluated with hyperparameters set to their default
values at the start of the optimization.</p>
</dd>
<dt><code>callbacks</code></dt><dd><p>(list of <a href="#topic+CallbackTuning">CallbackTuning</a>)<br />
List of callbacks.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-AutoTuner-base_learner"></a>



<h4>Method <code>base_learner()</code></h4>

<p>Extracts the base learner from nested learner objects like <code>GraphLearner</code> in <a href="https://CRAN.R-project.org/package=mlr3pipelines"><span class="pkg">mlr3pipelines</span></a>.
If <code>recursive = 0</code>, the (tuned) learner is returned.
</p>


<h5>Usage</h5>

<div class="r"><pre>AutoTuner$base_learner(recursive = Inf)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>recursive</code></dt><dd><p>(<code>integer(1)</code>)<br />
Depth of recursion for multiple nested objects.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p><a href="mlr3.html#topic+Learner">Learner</a>.
</p>


<hr>
<a id="method-AutoTuner-importance"></a>



<h4>Method <code>importance()</code></h4>

<p>The importance scores of the final model.
</p>


<h5>Usage</h5>

<div class="r"><pre>AutoTuner$importance()</pre></div>



<h5>Returns</h5>

<p>Named <code>numeric()</code>.
</p>


<hr>
<a id="method-AutoTuner-selected_features"></a>



<h4>Method <code>selected_features()</code></h4>

<p>The selected features of the final model.
</p>


<h5>Usage</h5>

<div class="r"><pre>AutoTuner$selected_features()</pre></div>



<h5>Returns</h5>

<p><code>character()</code>.
</p>


<hr>
<a id="method-AutoTuner-oob_error"></a>



<h4>Method <code>oob_error()</code></h4>

<p>The out-of-bag error of the final model.
</p>


<h5>Usage</h5>

<div class="r"><pre>AutoTuner$oob_error()</pre></div>



<h5>Returns</h5>

<p><code>numeric(1)</code>.
</p>


<hr>
<a id="method-AutoTuner-loglik"></a>



<h4>Method <code>loglik()</code></h4>

<p>The log-likelihood of the final model.
</p>


<h5>Usage</h5>

<div class="r"><pre>AutoTuner$loglik()</pre></div>



<h5>Returns</h5>

<p><code>logLik</code>.
Printer.
</p>


<hr>
<a id="method-AutoTuner-print"></a>



<h4>Method <code>print()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>AutoTuner$print()</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>...</code></dt><dd><p>(ignored).</p>
</dd>
</dl>

</div>


<hr>
<a id="method-AutoTuner-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>AutoTuner$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'># Automatic Tuning

# split to train and external set
task = tsk("penguins")
split = partition(task, ratio = 0.8)

# load learner and set search space
learner = lrn("classif.rpart",
  cp = to_tune(1e-04, 1e-1, logscale = TRUE)
)

# create auto tuner
at = auto_tuner(
  tuner = tnr("random_search"),
  learner = learner,
  resampling = rsmp ("holdout"),
  measure = msr("classif.ce"),
  term_evals = 4)

# tune hyperparameters and fit final model
at$train(task, row_ids = split$train)

# predict with final model
at$predict(task, row_ids = split$test)

# show tuning result
at$tuning_result

# model slot contains trained learner and tuning instance
at$model

# shortcut trained learner
at$learner

# shortcut tuning instance
at$tuning_instance


# Nested Resampling

at = auto_tuner(
  tuner = tnr("random_search"),
  learner = learner,
  resampling = rsmp ("holdout"),
  measure = msr("classif.ce"),
  term_evals = 4)

resampling_outer = rsmp("cv", folds = 3)
rr = resample(task, at, resampling_outer, store_models = TRUE)

# retrieve inner tuning results.
extract_inner_tuning_results(rr)

# performance scores estimated on the outer resampling
rr$score()

# unbiased performance of the final model trained on the full data set
rr$aggregate()
</code></pre>

<hr>
<h2 id='callback_tuning'>Create Tuning Callback</h2><span id='topic+callback_tuning'></span>

<h3>Description</h3>

<p>Function to create a <a href="#topic+CallbackTuning">CallbackTuning</a>.
Predefined callbacks are stored in the <a href="mlr3misc.html#topic+Dictionary">dictionary</a> <a href="#topic+mlr_callbacks">mlr_callbacks</a> and can be retrieved with <code><a href="#topic+clbk">clbk()</a></code>.
</p>
<p>Tuning callbacks can be called from different stages of tuning process.
The stages are prefixed with <code style="white-space: pre;">&#8288;on_*&#8288;</code>.
</p>
<div class="sourceCode"><pre>Start Tuning
     - on_optimization_begin
    Start Tuner Batch
         - on_optimizer_before_eval
        Start Evaluation
             - on_eval_after_design
             - on_eval_after_benchmark
             - on_eval_before_archive
        End Evaluation
         - on_optimizer_after_eval
    End Tuner Batch
     - on_result
     - on_optimization_end
End Tuning
</pre></div>
<p>See also the section on parameters for more information on the stages.
A tuning callback works with <a href="bbotk.html#topic+ContextOptimization">bbotk::ContextOptimization</a> and <a href="#topic+ContextEval">ContextEval</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>callback_tuning(
  id,
  label = NA_character_,
  man = NA_character_,
  on_optimization_begin = NULL,
  on_optimizer_before_eval = NULL,
  on_eval_after_design = NULL,
  on_eval_after_benchmark = NULL,
  on_eval_before_archive = NULL,
  on_optimizer_after_eval = NULL,
  on_result = NULL,
  on_optimization_end = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="callback_tuning_+3A_id">id</code></td>
<td>
<p>(<code>character(1)</code>)<br />
Identifier for the new instance.</p>
</td></tr>
<tr><td><code id="callback_tuning_+3A_label">label</code></td>
<td>
<p>(<code>character(1)</code>)<br />
Label for the new instance.</p>
</td></tr>
<tr><td><code id="callback_tuning_+3A_man">man</code></td>
<td>
<p>(<code>character(1)</code>)<br />
String in the format <code style="white-space: pre;">&#8288;[pkg]::[topic]&#8288;</code> pointing to a manual page for this object.
The referenced help package can be opened via method <code style="white-space: pre;">&#8288;$help()&#8288;</code>.</p>
</td></tr>
<tr><td><code id="callback_tuning_+3A_on_optimization_begin">on_optimization_begin</code></td>
<td>
<p>(<code style="white-space: pre;">&#8288;function()&#8288;</code>)<br />
Stage called at the beginning of the optimization.
Called in <code>Optimizer$optimize()</code>.
The context available is <a href="bbotk.html#topic+ContextOptimization">bbotk::ContextOptimization</a>.</p>
</td></tr>
<tr><td><code id="callback_tuning_+3A_on_optimizer_before_eval">on_optimizer_before_eval</code></td>
<td>
<p>(<code style="white-space: pre;">&#8288;function()&#8288;</code>)<br />
Stage called after the optimizer proposes points.
Called in <code>OptimInstance$eval_batch()</code>.
The context available is <a href="bbotk.html#topic+ContextOptimization">bbotk::ContextOptimization</a>.</p>
</td></tr>
<tr><td><code id="callback_tuning_+3A_on_eval_after_design">on_eval_after_design</code></td>
<td>
<p>(<code style="white-space: pre;">&#8288;function()&#8288;</code>)<br />
Stage called after design is created.
Called in <code>ObjectiveTuning$eval_many()</code>.
The context available is <a href="#topic+ContextEval">ContextEval</a>.</p>
</td></tr>
<tr><td><code id="callback_tuning_+3A_on_eval_after_benchmark">on_eval_after_benchmark</code></td>
<td>
<p>(<code style="white-space: pre;">&#8288;function()&#8288;</code>)<br />
Stage called after hyperparameter configurations are evaluated.
Called in <code>ObjectiveTuning$eval_many()</code>.
The context available is <a href="#topic+ContextEval">ContextEval</a>.</p>
</td></tr>
<tr><td><code id="callback_tuning_+3A_on_eval_before_archive">on_eval_before_archive</code></td>
<td>
<p>(<code style="white-space: pre;">&#8288;function()&#8288;</code>)<br />
Stage called before performance values are written to the archive.
Called in <code>ObjectiveTuning$eval_many()</code>.
The context available is <a href="#topic+ContextEval">ContextEval</a>.</p>
</td></tr>
<tr><td><code id="callback_tuning_+3A_on_optimizer_after_eval">on_optimizer_after_eval</code></td>
<td>
<p>(<code style="white-space: pre;">&#8288;function()&#8288;</code>)<br />
Stage called after points are evaluated.
Called in <code>OptimInstance$eval_batch()</code>.
The context available is <a href="bbotk.html#topic+ContextOptimization">bbotk::ContextOptimization</a>.</p>
</td></tr>
<tr><td><code id="callback_tuning_+3A_on_result">on_result</code></td>
<td>
<p>(<code style="white-space: pre;">&#8288;function()&#8288;</code>)<br />
Stage called after result are written.
Called in <code>OptimInstance$assign_result()</code>.
The context available is <a href="bbotk.html#topic+ContextOptimization">bbotk::ContextOptimization</a>.</p>
</td></tr>
<tr><td><code id="callback_tuning_+3A_on_optimization_end">on_optimization_end</code></td>
<td>
<p>(<code style="white-space: pre;">&#8288;function()&#8288;</code>)<br />
Stage called at the end of the optimization.
Called in <code>Optimizer$optimize()</code>.
The context available is <a href="bbotk.html#topic+ContextOptimization">bbotk::ContextOptimization</a>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When implementing a callback, each functions must have two arguments named <code>callback</code> and <code>context</code>.
</p>
<p>A callback can write data to the state (<code style="white-space: pre;">&#8288;$state&#8288;</code>), e.g. settings that affect the callback itself.
Avoid writing large data the state.
This can slow down the tuning process when the evaluation of configurations is parallelized.
</p>
<p>Tuning callbacks access two different contexts depending on the stage.
The stages <code>on_eval_after_design</code>, <code>on_eval_after_benchmark</code>, <code>on_eval_before_archive</code> access <a href="#topic+ContextEval">ContextEval</a>.
This context can be used to customize the evaluation of a batch of hyperparameter configurations.
Changes to the state of callback are lost after the evaluation of a batch and changes to the tuning instance or the tuner are not possible.
Persistent data should be written to the archive via <code style="white-space: pre;">&#8288;$aggregated_performance&#8288;</code> (see <a href="#topic+ContextEval">ContextEval</a>).
The other stages access <a href="bbotk.html#topic+ContextOptimization">ContextOptimization</a>.
This context can be used to modify the tuning instance, archive, tuner and final result.
There are two different contexts because the evaluation can be parallelized i.e. multiple instances of <a href="#topic+ContextEval">ContextEval</a> exists on different workers at the same time.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># write archive to disk
callback_tuning("mlr3tuning.backup",
  on_optimization_end = function(callback, context) {
    saveRDS(context$instance$archive, "archive.rds")
  }
)
</code></pre>

<hr>
<h2 id='CallbackTuning'>Create Tuning Callback</h2><span id='topic+CallbackTuning'></span>

<h3>Description</h3>

<p>Specialized <a href="bbotk.html#topic+CallbackOptimization">bbotk::CallbackOptimization</a> for tuning.
Callbacks allow to customize the behavior of processes in mlr3tuning.
The <code><a href="#topic+callback_tuning">callback_tuning()</a></code> function creates a <a href="#topic+CallbackTuning">CallbackTuning</a>.
Predefined callbacks are stored in the <a href="mlr3misc.html#topic+Dictionary">dictionary</a> <a href="#topic+mlr_callbacks">mlr_callbacks</a> and can be retrieved with <code><a href="#topic+clbk">clbk()</a></code>.
For more information on tuning callbacks see <code><a href="#topic+callback_tuning">callback_tuning()</a></code>.
</p>


<h3>Super classes</h3>

<p><code><a href="mlr3misc.html#topic+Callback">mlr3misc::Callback</a></code> -&gt; <code><a href="bbotk.html#topic+CallbackOptimization">bbotk::CallbackOptimization</a></code> -&gt; <code>CallbackTuning</code>
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>on_eval_after_design</code></dt><dd><p>(<code style="white-space: pre;">&#8288;function()&#8288;</code>)<br />
Stage called after design is created.
Called in <code>ObjectiveTuning$eval_many()</code>.</p>
</dd>
<dt><code>on_eval_after_benchmark</code></dt><dd><p>(<code style="white-space: pre;">&#8288;function()&#8288;</code>)<br />
Stage called after hyperparameter configurations are evaluated.
Called in <code>ObjectiveTuning$eval_many()</code>.</p>
</dd>
<dt><code>on_eval_before_archive</code></dt><dd><p>(<code style="white-space: pre;">&#8288;function()&#8288;</code>)<br />
Stage called before performance values are written to the archive.
Called in <code>ObjectiveTuning$eval_many()</code>.</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-CallbackTuning-clone"><code>CallbackTuning$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3misc" data-topic="Callback" data-id="call"><a href='../../mlr3misc/html/Callback.html#method-Callback-call'><code>mlr3misc::Callback$call()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3misc" data-topic="Callback" data-id="format"><a href='../../mlr3misc/html/Callback.html#method-Callback-format'><code>mlr3misc::Callback$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3misc" data-topic="Callback" data-id="help"><a href='../../mlr3misc/html/Callback.html#method-Callback-help'><code>mlr3misc::Callback$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3misc" data-topic="Callback" data-id="initialize"><a href='../../mlr3misc/html/Callback.html#method-Callback-initialize'><code>mlr3misc::Callback$initialize()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3misc" data-topic="Callback" data-id="print"><a href='../../mlr3misc/html/Callback.html#method-Callback-print'><code>mlr3misc::Callback$print()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-CallbackTuning-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>CallbackTuning$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'># write archive to disk
callback_tuning("mlr3tuning.backup",
  on_optimization_end = function(callback, context) {
    saveRDS(context$instance$archive, "archive.rds")
  }
)
</code></pre>

<hr>
<h2 id='ContextEval'>Evaluation Context</h2><span id='topic+ContextEval'></span>

<h3>Description</h3>

<p>The <a href="#topic+ContextEval">ContextEval</a> allows <a href="#topic+CallbackTuning">CallbackTuning</a>s to access and modify data while a batch of hyperparameter configurations is evaluated.
See section on active bindings for a list of modifiable objects.
See <code><a href="#topic+callback_tuning">callback_tuning()</a></code> for a list of stages which access <a href="#topic+ContextEval">ContextEval</a>.
</p>


<h3>Details</h3>

<p>This context is re-created each time a new batch of hyperparameter configurations is evaluated.
Changes to <code style="white-space: pre;">&#8288;$objective_tuning&#8288;</code>, <code style="white-space: pre;">&#8288;$design&#8288;</code> <code style="white-space: pre;">&#8288;$benchmark_result&#8288;</code> are discarded after the function is finished.
Modification on the data table in <code style="white-space: pre;">&#8288;$aggregated_performance&#8288;</code> are written to the archive.
Any number of columns can be added.
</p>


<h3>Super class</h3>

<p><code><a href="mlr3misc.html#topic+Context">mlr3misc::Context</a></code> -&gt; <code>ContextEval</code>
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>objective_tuning</code></dt><dd><p><a href="#topic+ObjectiveTuning">ObjectiveTuning</a>.</p>
</dd>
</dl>

</div>


<h3>Active bindings</h3>

<div class="r6-active-bindings">

<dl>
<dt><code>xss</code></dt><dd><p>(list())<br />
The hyperparameter configurations of the latest batch.
Contains the values on the learner scale i.e. transformations are applied.
See <code style="white-space: pre;">&#8288;$xdt&#8288;</code> in <a href="bbotk.html#topic+ContextOptimization">bbotk::ContextOptimization</a> for the untransformed values.</p>
</dd>
<dt><code>design</code></dt><dd><p>(<a href="data.table.html#topic+data.table">data.table::data.table</a>)<br />
The benchmark design of the latest batch.</p>
</dd>
<dt><code>benchmark_result</code></dt><dd><p>(<a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>)<br />
The benchmark result of the latest batch.</p>
</dd>
<dt><code>aggregated_performance</code></dt><dd><p>(<a href="data.table.html#topic+data.table">data.table::data.table</a>)<br />
Aggregated performance scores and training time of the latest batch.
This data table is passed to the archive.
A callback can add additional columns which are also written to the archive.</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-ContextEval-new"><code>ContextEval$new()</code></a>
</p>
</li>
<li> <p><a href="#method-ContextEval-clone"><code>ContextEval$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3misc" data-topic="Context" data-id="format"><a href='../../mlr3misc/html/Context.html#method-Context-format'><code>mlr3misc::Context$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3misc" data-topic="Context" data-id="print"><a href='../../mlr3misc/html/Context.html#method-Context-print'><code>mlr3misc::Context$print()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-ContextEval-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>ContextEval$new(objective_tuning)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>objective_tuning</code></dt><dd><p><a href="#topic+ObjectiveTuning">ObjectiveTuning</a>.</p>
</dd>
<dt><code>id</code></dt><dd><p>(<code>character(1)</code>)<br />
Identifier for the new callback.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-ContextEval-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>ContextEval$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='extract_inner_tuning_archives'>Extract Inner Tuning Archives</h2><span id='topic+extract_inner_tuning_archives'></span>

<h3>Description</h3>

<p>Extract inner tuning archives of nested resampling.
Implemented for <a href="mlr3.html#topic+ResampleResult">mlr3::ResampleResult</a> and <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>.
The function iterates over the <a href="#topic+AutoTuner">AutoTuner</a> objects and binds the tuning archives to a <code><a href="data.table.html#topic+data.table">data.table::data.table()</a></code>.
<a href="#topic+AutoTuner">AutoTuner</a> must be initialized with <code>store_tuning_instance = TRUE</code> and <code><a href="mlr3.html#topic+resample">mlr3::resample()</a></code> or <code><a href="mlr3.html#topic+benchmark">mlr3::benchmark()</a></code> must be called with <code>store_models = TRUE</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_inner_tuning_archives(
  x,
  unnest = "x_domain",
  exclude_columns = "uhash"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_inner_tuning_archives_+3A_x">x</code></td>
<td>
<p>(<a href="mlr3.html#topic+ResampleResult">mlr3::ResampleResult</a> | <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>).</p>
</td></tr>
<tr><td><code id="extract_inner_tuning_archives_+3A_unnest">unnest</code></td>
<td>
<p>(<code>character()</code>)<br />
Transforms list columns to separate columns.
By default, <code>x_domain</code> is unnested.
Set to <code>NULL</code> if no column should be unnested.</p>
</td></tr>
<tr><td><code id="extract_inner_tuning_archives_+3A_exclude_columns">exclude_columns</code></td>
<td>
<p>(<code>character()</code>)<br />
Exclude columns from result table.
Set to <code>NULL</code> if no column should be excluded.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code><a href="data.table.html#topic+data.table">data.table::data.table()</a></code>.
</p>


<h3>Data structure</h3>

<p>The returned data table has the following columns:
</p>

<ul>
<li> <p><code>experiment</code> (integer(1))<br />
Index, giving the according row number in the original benchmark grid.
</p>
</li>
<li> <p><code>iteration</code> (integer(1))<br />
Iteration of the outer resampling.
</p>
</li>
<li><p> One column for each hyperparameter of the search spaces.
</p>
</li>
<li><p> One column for each performance measure.
</p>
</li>
<li> <p><code>runtime_learners</code> (<code>numeric(1)</code>)<br />
Sum of training and predict times logged in learners per <a href="mlr3.html#topic+ResampleResult">mlr3::ResampleResult</a> / evaluation.
This does not include potential overhead time.
</p>
</li>
<li> <p><code>timestamp</code> (<code>POSIXct</code>)<br />
Time stamp when the evaluation was logged into the archive.
</p>
</li>
<li> <p><code>batch_nr</code> (<code>integer(1)</code>)<br />
Hyperparameters are evaluated in batches.
Each batch has a unique batch number.
</p>
</li>
<li> <p><code>x_domain</code> (<code>list()</code>)<br />
List of transformed hyperparameter values.
By default this column is unnested.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;x_domain_*&#8288;</code> (<code>any</code>)<br />
Separate column for each transformed hyperparameter.
</p>
</li>
<li> <p><code>resample_result</code> (<a href="mlr3.html#topic+ResampleResult">mlr3::ResampleResult</a>)<br />
Resample result of the inner resampling.
</p>
</li>
<li> <p><code>task_id</code> (<code>character(1)</code>).
</p>
</li>
<li> <p><code>learner_id</code> (<code>character(1)</code>).
</p>
</li>
<li> <p><code>resampling_id</code> (<code>character(1)</code>).
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Nested Resampling on Palmer Penguins Data Set

learner = lrn("classif.rpart",
  cp = to_tune(1e-04, 1e-1, logscale = TRUE))

# create auto tuner
at = auto_tuner(
  tuner = tnr("random_search"),
  learner = learner,
  resampling = rsmp ("holdout"),
  measure = msr("classif.ce"),
  term_evals = 4)

resampling_outer = rsmp("cv", folds = 2)
rr = resample(tsk("iris"), at, resampling_outer, store_models = TRUE)

# extract inner archives
extract_inner_tuning_archives(rr)
</code></pre>

<hr>
<h2 id='extract_inner_tuning_results'>Extract Inner Tuning Results</h2><span id='topic+extract_inner_tuning_results'></span><span id='topic+extract_inner_tuning_results.ResampleResult'></span><span id='topic+extract_inner_tuning_results.BenchmarkResult'></span>

<h3>Description</h3>

<p>Extract inner tuning results of nested resampling.
Implemented for <a href="mlr3.html#topic+ResampleResult">mlr3::ResampleResult</a> and <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_inner_tuning_results(x, tuning_instance, ...)

## S3 method for class 'ResampleResult'
extract_inner_tuning_results(x, tuning_instance = FALSE, ...)

## S3 method for class 'BenchmarkResult'
extract_inner_tuning_results(x, tuning_instance = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_inner_tuning_results_+3A_x">x</code></td>
<td>
<p>(<a href="mlr3.html#topic+ResampleResult">mlr3::ResampleResult</a> | <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>).</p>
</td></tr>
<tr><td><code id="extract_inner_tuning_results_+3A_tuning_instance">tuning_instance</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, tuning instances are added to the table.</p>
</td></tr>
<tr><td><code id="extract_inner_tuning_results_+3A_...">...</code></td>
<td>
<p>(any)<br />
Additional arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function iterates over the <a href="#topic+AutoTuner">AutoTuner</a> objects and binds the tuning results to a <code><a href="data.table.html#topic+data.table">data.table::data.table()</a></code>.
The <a href="#topic+AutoTuner">AutoTuner</a> must be initialized with <code>store_tuning_instance = TRUE</code> and <code><a href="mlr3.html#topic+resample">mlr3::resample()</a></code> or <code><a href="mlr3.html#topic+benchmark">mlr3::benchmark()</a></code> must be called with <code>store_models = TRUE</code>.
Optionally, the tuning instance can be added for each iteration.
</p>


<h3>Value</h3>

<p><code><a href="data.table.html#topic+data.table">data.table::data.table()</a></code>.
</p>


<h3>Data structure</h3>

<p>The returned data table has the following columns:
</p>

<ul>
<li> <p><code>experiment</code> (integer(1))<br />
Index, giving the according row number in the original benchmark grid.
</p>
</li>
<li> <p><code>iteration</code> (integer(1))<br />
Iteration of the outer resampling.
</p>
</li>
<li><p> One column for each hyperparameter of the search spaces.
</p>
</li>
<li><p> One column for each performance measure.
</p>
</li>
<li> <p><code>learner_param_vals</code> (<code>list()</code>)<br />
Hyperparameter values used by the learner.
Includes fixed and proposed hyperparameter values.
</p>
</li>
<li> <p><code>x_domain</code> (<code>list()</code>)<br />
List of transformed hyperparameter values.
</p>
</li>
<li> <p><code>tuning_instance</code> (<a href="#topic+TuningInstanceSingleCrit">TuningInstanceSingleCrit</a> | <a href="#topic+TuningInstanceMultiCrit">TuningInstanceMultiCrit</a>)<br />
Optionally, tuning instances.
</p>
</li>
<li> <p><code>task_id</code> (<code>character(1)</code>).
</p>
</li>
<li> <p><code>learner_id</code> (<code>character(1)</code>).
</p>
</li>
<li> <p><code>resampling_id</code> (<code>character(1)</code>).
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Nested Resampling on Palmer Penguins Data Set

learner = lrn("classif.rpart",
  cp = to_tune(1e-04, 1e-1, logscale = TRUE))

# create auto tuner
at = auto_tuner(
  tuner = tnr("random_search"),
  learner = learner,
  resampling = rsmp ("holdout"),
  measure = msr("classif.ce"),
  term_evals = 4)

resampling_outer = rsmp("cv", folds = 2)
rr = resample(tsk("iris"), at, resampling_outer, store_models = TRUE)

# extract inner results
extract_inner_tuning_results(rr)
</code></pre>

<hr>
<h2 id='mlr_tuners'>Dictionary of Tuners</h2><span id='topic+mlr_tuners'></span>

<h3>Description</h3>

<p>A simple <a href="mlr3misc.html#topic+Dictionary">mlr3misc::Dictionary</a> storing objects of class <a href="#topic+Tuner">Tuner</a>.
Each tuner has an associated help page, see <code>mlr_tuners_[id]</code>.
</p>
<p>This dictionary can get populated with additional tuners by add-on packages.
</p>
<p>For a more convenient way to retrieve and construct tuner, see <code><a href="#topic+tnr">tnr()</a></code>/<code><a href="#topic+tnrs">tnrs()</a></code>.
</p>


<h3>Format</h3>

<p><a href="R6.html#topic+R6Class">R6::R6Class</a> object inheriting from <a href="mlr3misc.html#topic+Dictionary">mlr3misc::Dictionary</a>.
</p>


<h3>Methods</h3>

<p>See <a href="mlr3misc.html#topic+Dictionary">mlr3misc::Dictionary</a>.
</p>


<h3>S3 methods</h3>


<ul>
<li> <p><code>as.data.table(dict, ..., objects = FALSE)</code><br />
<a href="mlr3misc.html#topic+Dictionary">mlr3misc::Dictionary</a> -&gt; <code><a href="data.table.html#topic+data.table">data.table::data.table()</a></code><br />
Returns a <code><a href="data.table.html#topic+data.table">data.table::data.table()</a></code> with fields &quot;key&quot;, &quot;label&quot;, &quot;param_classes&quot;, &quot;properties&quot; and &quot;packages&quot; as columns.
If <code>objects</code> is set to <code>TRUE</code>, the constructed objects are returned in the list column named <code>object</code>.
</p>
</li></ul>



<h3>See Also</h3>

<p>Sugar functions: <code><a href="#topic+tnr">tnr()</a></code>, <code><a href="#topic+tnrs">tnrs()</a></code>
</p>
<p>Other Tuner: 
<code><a href="#topic+mlr_tuners_cmaes">mlr_tuners_cmaes</a></code>,
<code><a href="#topic+mlr_tuners_design_points">mlr_tuners_design_points</a></code>,
<code><a href="#topic+mlr_tuners_gensa">mlr_tuners_gensa</a></code>,
<code><a href="#topic+mlr_tuners_grid_search">mlr_tuners_grid_search</a></code>,
<code><a href="#topic+mlr_tuners_irace">mlr_tuners_irace</a></code>,
<code><a href="#topic+mlr_tuners_nloptr">mlr_tuners_nloptr</a></code>,
<code><a href="#topic+mlr_tuners_random_search">mlr_tuners_random_search</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>as.data.table(mlr_tuners)
mlr_tuners$get("random_search")
tnr("random_search")
</code></pre>

<hr>
<h2 id='mlr_tuners_cmaes'>Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy</h2><span id='topic+mlr_tuners_cmaes'></span><span id='topic+TunerCmaes'></span>

<h3>Description</h3>

<p>Subclass for Covariance Matrix Adaptation Evolution Strategy (CMA-ES).
Calls <code><a href="adagio.html#topic+cmaes">adagio::pureCMAES()</a></code> from package <a href="https://CRAN.R-project.org/package=adagio"><span class="pkg">adagio</span></a>.
</p>


<h3>Dictionary</h3>

<p>This <a href="#topic+Tuner">Tuner</a> can be instantiated with the associated sugar function <code><a href="#topic+tnr">tnr()</a></code>:
</p>
<div class="sourceCode"><pre>tnr("cmaes")
</pre></div>


<h3>Control Parameters</h3>


<dl>
<dt><code>start_values</code></dt><dd><p><code>character(1)</code><br />
Create <code>random</code> start values or based on <code>center</code> of search space?
In the latter case, it is the center of the parameters before a trafo is applied.</p>
</dd>
</dl>

<p>For the meaning of the control parameters, see <code><a href="adagio.html#topic+cmaes">adagio::pureCMAES()</a></code>.
Note that we have removed all control parameters which refer to the termination of the algorithm and where our terminators allow to obtain the same behavior.
</p>


<h3>Progress Bars</h3>

<p><code style="white-space: pre;">&#8288;$optimize()&#8288;</code> supports progress bars via the package <a href="https://CRAN.R-project.org/package=progressr"><span class="pkg">progressr</span></a>
combined with a <a href="bbotk.html#topic+Terminator">Terminator</a>. Simply wrap the function in
<code>progressr::with_progress()</code> to enable them. We recommend to use package
<a href="https://CRAN.R-project.org/package=progress"><span class="pkg">progress</span></a> as backend; enable with <code>progressr::handlers("progress")</code>.
</p>


<h3>Logging</h3>

<p>All <a href="#topic+Tuner">Tuner</a>s use a logger (as implemented in <a href="https://CRAN.R-project.org/package=lgr"><span class="pkg">lgr</span></a>) from package
<a href="https://CRAN.R-project.org/package=bbotk"><span class="pkg">bbotk</span></a>.
Use <code>lgr::get_logger("bbotk")</code> to access and control the logger.
</p>


<h3>Optimizer</h3>

<p>This <a href="#topic+Tuner">Tuner</a> is based on <a href="bbotk.html#topic+mlr_optimizers_cmaes">bbotk::OptimizerCmaes</a> which can be applied on any black box optimization problem.
See also the documentation of <a href="https://bbotk.mlr-org.com/">bbotk</a>.
</p>


<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li><p> Learn more about <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-tuner">tuners</a>.
</p>
</li></ul>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>

<ul>
<li><p> Use the <a href="https://mlr-org.com/gallery/series/2023-01-15-hyperband-xgboost/">Hyperband</a> optimizer with different budget parameters.
</p>
</li></ul>



<h3>Super classes</h3>

<p><code><a href="#topic+Tuner">mlr3tuning::Tuner</a></code> -&gt; <code><a href="#topic+TunerFromOptimizer">mlr3tuning::TunerFromOptimizer</a></code> -&gt; <code>TunerCmaes</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TunerCmaes-new"><code>TunerCmaes$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TunerCmaes-clone"><code>TunerCmaes$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="format"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-format'><code>mlr3tuning::Tuner$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="help"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-help'><code>mlr3tuning::Tuner$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="print"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-print'><code>mlr3tuning::Tuner$print()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="TunerFromOptimizer" data-id="optimize"><a href='../../mlr3tuning/html/TunerFromOptimizer.html#method-TunerFromOptimizer-optimize'><code>mlr3tuning::TunerFromOptimizer$optimize()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-TunerCmaes-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerCmaes$new()</pre></div>


<hr>
<a id="method-TunerCmaes-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerCmaes$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Source</h3>

<p>Hansen N (2016).
&ldquo;The CMA Evolution Strategy: A Tutorial.&rdquo;
1604.00772.
</p>


<h3>See Also</h3>

<p>Other Tuner: 
<code><a href="#topic+mlr_tuners">mlr_tuners</a></code>,
<code><a href="#topic+mlr_tuners_design_points">mlr_tuners_design_points</a></code>,
<code><a href="#topic+mlr_tuners_gensa">mlr_tuners_gensa</a></code>,
<code><a href="#topic+mlr_tuners_grid_search">mlr_tuners_grid_search</a></code>,
<code><a href="#topic+mlr_tuners_irace">mlr_tuners_irace</a></code>,
<code><a href="#topic+mlr_tuners_nloptr">mlr_tuners_nloptr</a></code>,
<code><a href="#topic+mlr_tuners_random_search">mlr_tuners_random_search</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Hyperparameter Optimization

# load learner and set search space
learner = lrn("classif.rpart",
  cp = to_tune(1e-04, 1e-1, logscale = TRUE),
  minsplit = to_tune(p_dbl(2, 128, trafo = as.integer)),
  minbucket = to_tune(p_dbl(1, 64, trafo = as.integer))
)

# run hyperparameter tuning on the Palmer Penguins data set
instance = tune(
  tuner = tnr("cmaes"),
  task = tsk("penguins"),
  learner = learner,
  resampling = rsmp("holdout"),
  measure = msr("classif.ce"),
  term_evals = 10)

# best performing hyperparameter configuration
instance$result

# all evaluated hyperparameter configuration
as.data.table(instance$archive)

# fit final model on complete data set
learner$param_set$values = instance$result_learner_param_vals
learner$train(tsk("penguins"))
</code></pre>

<hr>
<h2 id='mlr_tuners_design_points'>Hyperparameter Tuning with Design Points</h2><span id='topic+mlr_tuners_design_points'></span><span id='topic+TunerDesignPoints'></span>

<h3>Description</h3>

<p>Subclass for tuning w.r.t. fixed design points.
</p>
<p>We simply search over a set of points fully specified by the user.
The points in the design are evaluated in order as given.
</p>


<h3>Dictionary</h3>

<p>This <a href="#topic+Tuner">Tuner</a> can be instantiated with the associated sugar function <code><a href="#topic+tnr">tnr()</a></code>:
</p>
<div class="sourceCode"><pre>tnr("design_points")
</pre></div>


<h3>Parallelization</h3>

<p>In order to support general termination criteria and parallelization, we
evaluate points in a batch-fashion of size <code>batch_size</code>. Larger batches mean
we can parallelize more, smaller batches imply a more fine-grained checking
of termination criteria. A batch contains of <code>batch_size</code> times <code>resampling$iters</code> jobs.
E.g., if you set a batch size of 10 points and do a 5-fold cross validation, you can
utilize up to 50 cores.
</p>
<p>Parallelization is supported via package <a href="https://CRAN.R-project.org/package=future"><span class="pkg">future</span></a> (see <code><a href="mlr3.html#topic+benchmark">mlr3::benchmark()</a></code>'s
section on parallelization for more details).
</p>


<h3>Logging</h3>

<p>All <a href="#topic+Tuner">Tuner</a>s use a logger (as implemented in <a href="https://CRAN.R-project.org/package=lgr"><span class="pkg">lgr</span></a>) from package
<a href="https://CRAN.R-project.org/package=bbotk"><span class="pkg">bbotk</span></a>.
Use <code>lgr::get_logger("bbotk")</code> to access and control the logger.
</p>


<h3>Optimizer</h3>

<p>This <a href="#topic+Tuner">Tuner</a> is based on <a href="bbotk.html#topic+mlr_optimizers_design_points">bbotk::OptimizerDesignPoints</a> which can be applied on any black box optimization problem.
See also the documentation of <a href="https://bbotk.mlr-org.com/">bbotk</a>.
</p>


<h3>Parameters</h3>


<dl>
<dt><code>batch_size</code></dt><dd><p><code>integer(1)</code><br />
Maximum number of configurations to try in a batch.</p>
</dd>
<dt><code>design</code></dt><dd><p><a href="data.table.html#topic+data.table">data.table::data.table</a><br />
Design points to try in search, one per row.</p>
</dd>
</dl>



<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li><p> Learn more about <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-tuner">tuners</a>.
</p>
</li></ul>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>

<ul>
<li><p> Use the <a href="https://mlr-org.com/gallery/series/2023-01-15-hyperband-xgboost/">Hyperband</a> optimizer with different budget parameters.
</p>
</li></ul>



<h3>Progress Bars</h3>

<p><code style="white-space: pre;">&#8288;$optimize()&#8288;</code> supports progress bars via the package <a href="https://CRAN.R-project.org/package=progressr"><span class="pkg">progressr</span></a>
combined with a <a href="bbotk.html#topic+Terminator">Terminator</a>. Simply wrap the function in
<code>progressr::with_progress()</code> to enable them. We recommend to use package
<a href="https://CRAN.R-project.org/package=progress"><span class="pkg">progress</span></a> as backend; enable with <code>progressr::handlers("progress")</code>.
</p>


<h3>Super classes</h3>

<p><code><a href="#topic+Tuner">mlr3tuning::Tuner</a></code> -&gt; <code><a href="#topic+TunerFromOptimizer">mlr3tuning::TunerFromOptimizer</a></code> -&gt; <code>TunerDesignPoints</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TunerDesignPoints-new"><code>TunerDesignPoints$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TunerDesignPoints-clone"><code>TunerDesignPoints$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="format"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-format'><code>mlr3tuning::Tuner$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="help"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-help'><code>mlr3tuning::Tuner$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="print"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-print'><code>mlr3tuning::Tuner$print()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="TunerFromOptimizer" data-id="optimize"><a href='../../mlr3tuning/html/TunerFromOptimizer.html#method-TunerFromOptimizer-optimize'><code>mlr3tuning::TunerFromOptimizer$optimize()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-TunerDesignPoints-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerDesignPoints$new()</pre></div>


<hr>
<a id="method-TunerDesignPoints-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerDesignPoints$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Package <a href="https://CRAN.R-project.org/package=mlr3hyperband"><span class="pkg">mlr3hyperband</span></a> for hyperband tuning.
</p>
<p>Other Tuner: 
<code><a href="#topic+mlr_tuners">mlr_tuners</a></code>,
<code><a href="#topic+mlr_tuners_cmaes">mlr_tuners_cmaes</a></code>,
<code><a href="#topic+mlr_tuners_gensa">mlr_tuners_gensa</a></code>,
<code><a href="#topic+mlr_tuners_grid_search">mlr_tuners_grid_search</a></code>,
<code><a href="#topic+mlr_tuners_irace">mlr_tuners_irace</a></code>,
<code><a href="#topic+mlr_tuners_nloptr">mlr_tuners_nloptr</a></code>,
<code><a href="#topic+mlr_tuners_random_search">mlr_tuners_random_search</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Hyperparameter Optimization

# load learner and set search space
learner = lrn("classif.rpart",
  cp = to_tune(1e-04, 1e-1),
  minsplit = to_tune(2, 128),
  minbucket = to_tune(1, 64)
)

# create design
design = mlr3misc::rowwise_table(
  ~cp,   ~minsplit,  ~minbucket,
  0.1,   2,          64,
  0.01,  64,         32,
  0.001, 128,        1
)

# run hyperparameter tuning on the Palmer Penguins data set
instance = tune(
  tuner = tnr("design_points", design = design),
  task = tsk("penguins"),
  learner = learner,
  resampling = rsmp("holdout"),
  measure = msr("classif.ce")
)

# best performing hyperparameter configuration
instance$result

# all evaluated hyperparameter configuration
as.data.table(instance$archive)

# fit final model on complete data set
learner$param_set$values = instance$result_learner_param_vals
learner$train(tsk("penguins"))
</code></pre>

<hr>
<h2 id='mlr_tuners_gensa'>Hyperparameter Tuning with Generalized Simulated Annealing</h2><span id='topic+mlr_tuners_gensa'></span><span id='topic+TunerGenSA'></span>

<h3>Description</h3>

<p>Subclass for generalized simulated annealing tuning.
Calls <code><a href="GenSA.html#topic+GenSA">GenSA::GenSA()</a></code> from package <a href="https://CRAN.R-project.org/package=GenSA"><span class="pkg">GenSA</span></a>.
</p>


<h3>Details</h3>

<p>In contrast to the <code><a href="GenSA.html#topic+GenSA">GenSA::GenSA()</a></code> defaults, we set <code>smooth = FALSE</code> as a default.
</p>


<h3>Dictionary</h3>

<p>This <a href="#topic+Tuner">Tuner</a> can be instantiated with the associated sugar function <code><a href="#topic+tnr">tnr()</a></code>:
</p>
<div class="sourceCode"><pre>tnr("gensa")
</pre></div>


<h3>Parallelization</h3>

<p>In order to support general termination criteria and parallelization, we
evaluate points in a batch-fashion of size <code>batch_size</code>. Larger batches mean
we can parallelize more, smaller batches imply a more fine-grained checking
of termination criteria. A batch contains of <code>batch_size</code> times <code>resampling$iters</code> jobs.
E.g., if you set a batch size of 10 points and do a 5-fold cross validation, you can
utilize up to 50 cores.
</p>
<p>Parallelization is supported via package <a href="https://CRAN.R-project.org/package=future"><span class="pkg">future</span></a> (see <code><a href="mlr3.html#topic+benchmark">mlr3::benchmark()</a></code>'s
section on parallelization for more details).
</p>


<h3>Logging</h3>

<p>All <a href="#topic+Tuner">Tuner</a>s use a logger (as implemented in <a href="https://CRAN.R-project.org/package=lgr"><span class="pkg">lgr</span></a>) from package
<a href="https://CRAN.R-project.org/package=bbotk"><span class="pkg">bbotk</span></a>.
Use <code>lgr::get_logger("bbotk")</code> to access and control the logger.
</p>


<h3>Optimizer</h3>

<p>This <a href="#topic+Tuner">Tuner</a> is based on <a href="bbotk.html#topic+mlr_optimizers_gensa">bbotk::OptimizerGenSA</a> which can be applied on any black box optimization problem.
See also the documentation of <a href="https://bbotk.mlr-org.com/">bbotk</a>.
</p>


<h3>Parameters</h3>


<dl>
<dt><code>smooth</code></dt><dd><p><code>logical(1)</code></p>
</dd>
<dt><code>temperature</code></dt><dd><p><code>numeric(1)</code></p>
</dd>
<dt><code>acceptance.param</code></dt><dd><p><code>numeric(1)</code></p>
</dd>
<dt><code>verbose</code></dt><dd><p><code>logical(1)</code></p>
</dd>
<dt><code>trace.mat</code></dt><dd><p><code>logical(1)</code></p>
</dd>
</dl>

<p>For the meaning of the control parameters, see <code><a href="GenSA.html#topic+GenSA">GenSA::GenSA()</a></code>. Note that we
have removed all control parameters which refer to the termination of the
algorithm and where our terminators allow to obtain the same behavior.
</p>
<p>In contrast to the <code><a href="GenSA.html#topic+GenSA">GenSA::GenSA()</a></code> defaults, we set <code>trace.mat = FALSE</code>.
Note that <code><a href="GenSA.html#topic+GenSA">GenSA::GenSA()</a></code> uses <code>smooth = TRUE</code> as a default.
In the case of using this optimizer for Hyperparameter Optimization you may
want to set <code>smooth = FALSE</code>.
</p>


<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li><p> Learn more about <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-tuner">tuners</a>.
</p>
</li></ul>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>

<ul>
<li><p> Use the <a href="https://mlr-org.com/gallery/series/2023-01-15-hyperband-xgboost/">Hyperband</a> optimizer with different budget parameters.
</p>
</li></ul>



<h3>Progress Bars</h3>

<p><code style="white-space: pre;">&#8288;$optimize()&#8288;</code> supports progress bars via the package <a href="https://CRAN.R-project.org/package=progressr"><span class="pkg">progressr</span></a>
combined with a <a href="bbotk.html#topic+Terminator">Terminator</a>. Simply wrap the function in
<code>progressr::with_progress()</code> to enable them. We recommend to use package
<a href="https://CRAN.R-project.org/package=progress"><span class="pkg">progress</span></a> as backend; enable with <code>progressr::handlers("progress")</code>.
</p>


<h3>Super classes</h3>

<p><code><a href="#topic+Tuner">mlr3tuning::Tuner</a></code> -&gt; <code><a href="#topic+TunerFromOptimizer">mlr3tuning::TunerFromOptimizer</a></code> -&gt; <code>TunerGenSA</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TunerGenSA-new"><code>TunerGenSA$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TunerGenSA-clone"><code>TunerGenSA$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="format"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-format'><code>mlr3tuning::Tuner$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="help"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-help'><code>mlr3tuning::Tuner$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="print"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-print'><code>mlr3tuning::Tuner$print()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="TunerFromOptimizer" data-id="optimize"><a href='../../mlr3tuning/html/TunerFromOptimizer.html#method-TunerFromOptimizer-optimize'><code>mlr3tuning::TunerFromOptimizer$optimize()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-TunerGenSA-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerGenSA$new()</pre></div>


<hr>
<a id="method-TunerGenSA-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerGenSA$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Source</h3>

<p>Tsallis C, Stariolo DA (1996).
&ldquo;Generalized simulated annealing.&rdquo;
<em>Physica A: Statistical Mechanics and its Applications</em>, <b>233</b>(1-2), 395&ndash;406.
<a href="https://doi.org/10.1016/s0378-4371%2896%2900271-3">doi:10.1016/s0378-4371(96)00271-3</a>.
</p>
<p>Xiang Y, Gubian S, Suomela B, Hoeng J (2013).
&ldquo;Generalized Simulated Annealing for Global Optimization: The GenSA Package.&rdquo;
<em>The R Journal</em>, <b>5</b>(1), 13.
<a href="https://doi.org/10.32614/rj-2013-002">doi:10.32614/rj-2013-002</a>.
</p>


<h3>See Also</h3>

<p>Other Tuner: 
<code><a href="#topic+mlr_tuners">mlr_tuners</a></code>,
<code><a href="#topic+mlr_tuners_cmaes">mlr_tuners_cmaes</a></code>,
<code><a href="#topic+mlr_tuners_design_points">mlr_tuners_design_points</a></code>,
<code><a href="#topic+mlr_tuners_grid_search">mlr_tuners_grid_search</a></code>,
<code><a href="#topic+mlr_tuners_irace">mlr_tuners_irace</a></code>,
<code><a href="#topic+mlr_tuners_nloptr">mlr_tuners_nloptr</a></code>,
<code><a href="#topic+mlr_tuners_random_search">mlr_tuners_random_search</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Hyperparameter Optimization

# load learner and set search space
learner = lrn("classif.rpart",
  cp = to_tune(1e-04, 1e-1, logscale = TRUE)
)

# run hyperparameter tuning on the Palmer Penguins data set
instance = tune(
  tuner = tnr("gensa"),
  task = tsk("penguins"),
  learner = learner,
  resampling = rsmp("holdout"),
  measure = msr("classif.ce"),
  term_evals = 10
)

# best performing hyperparameter configuration
instance$result

# all evaluated hyperparameter configuration
as.data.table(instance$archive)

# fit final model on complete data set
learner$param_set$values = instance$result_learner_param_vals
learner$train(tsk("penguins"))
</code></pre>

<hr>
<h2 id='mlr_tuners_grid_search'>Hyperparameter Tuning with Grid Search</h2><span id='topic+mlr_tuners_grid_search'></span><span id='topic+TunerGridSearch'></span>

<h3>Description</h3>

<p>Subclass for grid search tuning.
</p>


<h3>Details</h3>

<p>The grid is constructed as a Cartesian product over discretized values per parameter, see <code><a href="paradox.html#topic+generate_design_grid">paradox::generate_design_grid()</a></code>.
If the learner supports hotstarting, the grid is sorted by the hotstart parameter (see also <a href="mlr3.html#topic+HotstartStack">mlr3::HotstartStack</a>).
If not, the points of the grid are evaluated in a  random order.
</p>


<h3>Dictionary</h3>

<p>This <a href="#topic+Tuner">Tuner</a> can be instantiated with the associated sugar function <code><a href="#topic+tnr">tnr()</a></code>:
</p>
<div class="sourceCode"><pre>tnr("grid_search")
</pre></div>


<h3>Control Parameters</h3>


<dl>
<dt><code>resolution</code></dt><dd><p><code>integer(1)</code><br />
Resolution of the grid, see <code><a href="paradox.html#topic+generate_design_grid">paradox::generate_design_grid()</a></code>.</p>
</dd>
<dt><code>param_resolutions</code></dt><dd><p>named <code>integer()</code><br />
Resolution per parameter, named by parameter ID, see <code><a href="paradox.html#topic+generate_design_grid">paradox::generate_design_grid()</a></code>.</p>
</dd>
<dt><code>batch_size</code></dt><dd><p><code>integer(1)</code><br />
Maximum number of points to try in a batch.</p>
</dd>
</dl>



<h3>Progress Bars</h3>

<p><code style="white-space: pre;">&#8288;$optimize()&#8288;</code> supports progress bars via the package <a href="https://CRAN.R-project.org/package=progressr"><span class="pkg">progressr</span></a>
combined with a <a href="bbotk.html#topic+Terminator">Terminator</a>. Simply wrap the function in
<code>progressr::with_progress()</code> to enable them. We recommend to use package
<a href="https://CRAN.R-project.org/package=progress"><span class="pkg">progress</span></a> as backend; enable with <code>progressr::handlers("progress")</code>.
</p>


<h3>Parallelization</h3>

<p>In order to support general termination criteria and parallelization, we
evaluate points in a batch-fashion of size <code>batch_size</code>. Larger batches mean
we can parallelize more, smaller batches imply a more fine-grained checking
of termination criteria. A batch contains of <code>batch_size</code> times <code>resampling$iters</code> jobs.
E.g., if you set a batch size of 10 points and do a 5-fold cross validation, you can
utilize up to 50 cores.
</p>
<p>Parallelization is supported via package <a href="https://CRAN.R-project.org/package=future"><span class="pkg">future</span></a> (see <code><a href="mlr3.html#topic+benchmark">mlr3::benchmark()</a></code>'s
section on parallelization for more details).
</p>


<h3>Logging</h3>

<p>All <a href="#topic+Tuner">Tuner</a>s use a logger (as implemented in <a href="https://CRAN.R-project.org/package=lgr"><span class="pkg">lgr</span></a>) from package
<a href="https://CRAN.R-project.org/package=bbotk"><span class="pkg">bbotk</span></a>.
Use <code>lgr::get_logger("bbotk")</code> to access and control the logger.
</p>


<h3>Optimizer</h3>

<p>This <a href="#topic+Tuner">Tuner</a> is based on <a href="bbotk.html#topic+mlr_optimizers_grid_search">bbotk::OptimizerGridSearch</a> which can be applied on any black box optimization problem.
See also the documentation of <a href="https://bbotk.mlr-org.com/">bbotk</a>.
</p>


<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li><p> Learn more about <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-tuner">tuners</a>.
</p>
</li></ul>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>

<ul>
<li><p> Use the <a href="https://mlr-org.com/gallery/series/2023-01-15-hyperband-xgboost/">Hyperband</a> optimizer with different budget parameters.
</p>
</li></ul>



<h3>Super class</h3>

<p><code><a href="#topic+Tuner">mlr3tuning::Tuner</a></code> -&gt; <code>TunerGridSearch</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TunerGridSearch-new"><code>TunerGridSearch$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TunerGridSearch-clone"><code>TunerGridSearch$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="format"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-format'><code>mlr3tuning::Tuner$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="help"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-help'><code>mlr3tuning::Tuner$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="optimize"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-optimize'><code>mlr3tuning::Tuner$optimize()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="print"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-print'><code>mlr3tuning::Tuner$print()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-TunerGridSearch-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerGridSearch$new()</pre></div>


<hr>
<a id="method-TunerGridSearch-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerGridSearch$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other Tuner: 
<code><a href="#topic+mlr_tuners">mlr_tuners</a></code>,
<code><a href="#topic+mlr_tuners_cmaes">mlr_tuners_cmaes</a></code>,
<code><a href="#topic+mlr_tuners_design_points">mlr_tuners_design_points</a></code>,
<code><a href="#topic+mlr_tuners_gensa">mlr_tuners_gensa</a></code>,
<code><a href="#topic+mlr_tuners_irace">mlr_tuners_irace</a></code>,
<code><a href="#topic+mlr_tuners_nloptr">mlr_tuners_nloptr</a></code>,
<code><a href="#topic+mlr_tuners_random_search">mlr_tuners_random_search</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Hyperparameter Optimization

# load learner and set search space
learner = lrn("classif.rpart",
  cp = to_tune(1e-04, 1e-1, logscale = TRUE)
)

# run hyperparameter tuning on the Palmer Penguins data set
instance = tune(
  tuner = tnr("grid_search"),
  task = tsk("penguins"),
  learner = learner,
  resampling = rsmp("holdout"),
  measure = msr("classif.ce"),
  term_evals = 10
)

# best performing hyperparameter configuration
instance$result

# all evaluated hyperparameter configuration
as.data.table(instance$archive)

# fit final model on complete data set
learner$param_set$values = instance$result_learner_param_vals
learner$train(tsk("penguins"))
</code></pre>

<hr>
<h2 id='mlr_tuners_irace'>Hyperparameter Tuning with Iterated Racing.</h2><span id='topic+mlr_tuners_irace'></span><span id='topic+TunerIrace'></span>

<h3>Description</h3>

<p>Subclass for iterated racing.
Calls <code><a href="irace.html#topic+irace">irace::irace()</a></code> from package <a href="https://CRAN.R-project.org/package=irace"><span class="pkg">irace</span></a>.
</p>


<h3>Dictionary</h3>

<p>This <a href="#topic+Tuner">Tuner</a> can be instantiated with the associated sugar function <code><a href="#topic+tnr">tnr()</a></code>:
</p>
<div class="sourceCode"><pre>tnr("irace")
</pre></div>


<h3>Control Parameters</h3>


<dl>
<dt><code>n_instances</code></dt><dd><p><code>integer(1)</code><br />
Number of resampling instances.</p>
</dd>
</dl>

<p>For the meaning of all other parameters, see <code><a href="irace.html#topic+defaultScenario">irace::defaultScenario()</a></code>. Note
that we have removed all control parameters which refer to the termination of
the algorithm. Use <a href="bbotk.html#topic+TerminatorEvals">TerminatorEvals</a> instead. Other terminators do not work
with <code>TunerIrace</code>.
</p>


<h3>Archive</h3>

<p>The <a href="#topic+ArchiveTuning">ArchiveTuning</a> holds the following additional columns:
</p>

<ul>
<li> <p><code>"race"</code> (<code>integer(1)</code>)<br />
Race iteration.
</p>
</li>
<li> <p><code>"step"</code> (<code>integer(1)</code>)<br />
Step number of race.
</p>
</li>
<li> <p><code>"instance"</code> (<code>integer(1)</code>)<br />
Identifies resampling instances across races and steps.
</p>
</li>
<li> <p><code>"configuration"</code> (<code>integer(1)</code>)<br />
Identifies configurations across races and steps.
</p>
</li></ul>



<h3>Result</h3>

<p>The tuning result (<code>instance$result</code>) is the best performing elite of the final race.
The reported performance is the average performance estimated  on all used instances.
</p>


<h3>Progress Bars</h3>

<p><code style="white-space: pre;">&#8288;$optimize()&#8288;</code> supports progress bars via the package <a href="https://CRAN.R-project.org/package=progressr"><span class="pkg">progressr</span></a>
combined with a <a href="bbotk.html#topic+Terminator">Terminator</a>. Simply wrap the function in
<code>progressr::with_progress()</code> to enable them. We recommend to use package
<a href="https://CRAN.R-project.org/package=progress"><span class="pkg">progress</span></a> as backend; enable with <code>progressr::handlers("progress")</code>.
</p>


<h3>Logging</h3>

<p>All <a href="#topic+Tuner">Tuner</a>s use a logger (as implemented in <a href="https://CRAN.R-project.org/package=lgr"><span class="pkg">lgr</span></a>) from package
<a href="https://CRAN.R-project.org/package=bbotk"><span class="pkg">bbotk</span></a>.
Use <code>lgr::get_logger("bbotk")</code> to access and control the logger.
</p>


<h3>Optimizer</h3>

<p>This <a href="#topic+Tuner">Tuner</a> is based on <a href="bbotk.html#topic+mlr_optimizers_irace">bbotk::OptimizerIrace</a> which can be applied on any black box optimization problem.
See also the documentation of <a href="https://bbotk.mlr-org.com/">bbotk</a>.
</p>


<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li><p> Learn more about <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-tuner">tuners</a>.
</p>
</li></ul>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>

<ul>
<li><p> Use the <a href="https://mlr-org.com/gallery/series/2023-01-15-hyperband-xgboost/">Hyperband</a> optimizer with different budget parameters.
</p>
</li></ul>



<h3>Super classes</h3>

<p><code><a href="#topic+Tuner">mlr3tuning::Tuner</a></code> -&gt; <code><a href="#topic+TunerFromOptimizer">mlr3tuning::TunerFromOptimizer</a></code> -&gt; <code>TunerIrace</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TunerIrace-new"><code>TunerIrace$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TunerIrace-optimize"><code>TunerIrace$optimize()</code></a>
</p>
</li>
<li> <p><a href="#method-TunerIrace-clone"><code>TunerIrace$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="format"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-format'><code>mlr3tuning::Tuner$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="help"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-help'><code>mlr3tuning::Tuner$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="print"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-print'><code>mlr3tuning::Tuner$print()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-TunerIrace-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerIrace$new()</pre></div>


<hr>
<a id="method-TunerIrace-optimize"></a>



<h4>Method <code>optimize()</code></h4>

<p>Performs the tuning on a <a href="#topic+TuningInstanceSingleCrit">TuningInstanceSingleCrit</a> until termination.
The single evaluations and the final results will be written into the
<a href="#topic+ArchiveTuning">ArchiveTuning</a> that resides in the <a href="#topic+TuningInstanceSingleCrit">TuningInstanceSingleCrit</a>. The final
result is returned.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerIrace$optimize(inst)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>inst</code></dt><dd><p>(<a href="#topic+TuningInstanceSingleCrit">TuningInstanceSingleCrit</a>).</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p><a href="data.table.html#topic+data.table">data.table::data.table</a>.
</p>


<hr>
<a id="method-TunerIrace-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerIrace$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Source</h3>

<p>Lopez-Ibanez M, Dubois-Lacoste J, Caceres LP, Birattari M, Stuetzle T (2016).
&ldquo;The irace package: Iterated racing for automatic algorithm configuration.&rdquo;
<em>Operations Research Perspectives</em>, <b>3</b>, 43&ndash;58.
<a href="https://doi.org/10.1016/j.orp.2016.09.002">doi:10.1016/j.orp.2016.09.002</a>.
</p>


<h3>See Also</h3>

<p>Other Tuner: 
<code><a href="#topic+mlr_tuners">mlr_tuners</a></code>,
<code><a href="#topic+mlr_tuners_cmaes">mlr_tuners_cmaes</a></code>,
<code><a href="#topic+mlr_tuners_design_points">mlr_tuners_design_points</a></code>,
<code><a href="#topic+mlr_tuners_gensa">mlr_tuners_gensa</a></code>,
<code><a href="#topic+mlr_tuners_grid_search">mlr_tuners_grid_search</a></code>,
<code><a href="#topic+mlr_tuners_nloptr">mlr_tuners_nloptr</a></code>,
<code><a href="#topic+mlr_tuners_random_search">mlr_tuners_random_search</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># retrieve task
task = tsk("pima")

# load learner and set search space
learner = lrn("classif.rpart", cp = to_tune(1e-04, 1e-1, logscale = TRUE))

# hyperparameter tuning on the pima indians diabetes data set
instance = tune(
  tuner = tnr("irace"),
  task = task,
  learner = learner,
  resampling = rsmp("holdout"),
  measure = msr("classif.ce"),
  term_evals = 42
)

# best performing hyperparameter configuration
instance$result

# all evaluated hyperparameter configuration
as.data.table(instance$archive)

# fit final model on complete data set
learner$param_set$values = instance$result_learner_param_vals
learner$train(task)

</code></pre>

<hr>
<h2 id='mlr_tuners_nloptr'>Hyperparameter Tuning with Non-linear Optimization</h2><span id='topic+mlr_tuners_nloptr'></span><span id='topic+TunerNLoptr'></span>

<h3>Description</h3>

<p>Subclass for non-linear optimization (NLopt).
Calls <a href="nloptr.html#topic+nloptr">nloptr::nloptr</a> from package <a href="https://CRAN.R-project.org/package=nloptr"><span class="pkg">nloptr</span></a>.
</p>


<h3>Details</h3>

<p>The termination conditions <code>stopval</code>, <code>maxtime</code> and <code>maxeval</code> of <code><a href="nloptr.html#topic+nloptr">nloptr::nloptr()</a></code> are deactivated and replaced by the <a href="bbotk.html#topic+Terminator">bbotk::Terminator</a> subclasses.
The x and function value tolerance termination conditions (<code>xtol_rel = 10^-4</code>, <code>xtol_abs = rep(0.0, length(x0))</code>, <code>ftol_rel = 0.0</code> and <code>ftol_abs = 0.0</code>) are still available and implemented with their package defaults.
To deactivate these conditions, set them to <code>-1</code>.
</p>


<h3>Dictionary</h3>

<p>This <a href="#topic+Tuner">Tuner</a> can be instantiated with the associated sugar function <code><a href="#topic+tnr">tnr()</a></code>:
</p>
<div class="sourceCode"><pre>tnr("nloptr")
</pre></div>


<h3>Logging</h3>

<p>All <a href="#topic+Tuner">Tuner</a>s use a logger (as implemented in <a href="https://CRAN.R-project.org/package=lgr"><span class="pkg">lgr</span></a>) from package
<a href="https://CRAN.R-project.org/package=bbotk"><span class="pkg">bbotk</span></a>.
Use <code>lgr::get_logger("bbotk")</code> to access and control the logger.
</p>


<h3>Optimizer</h3>

<p>This <a href="#topic+Tuner">Tuner</a> is based on <a href="bbotk.html#topic+mlr_optimizers_nloptr">bbotk::OptimizerNLoptr</a> which can be applied on any black box optimization problem.
See also the documentation of <a href="https://bbotk.mlr-org.com/">bbotk</a>.
</p>


<h3>Parameters</h3>


<dl>
<dt><code>algorithm</code></dt><dd><p><code>character(1)</code></p>
</dd>
<dt><code>eval_g_ineq</code></dt><dd><p><code style="white-space: pre;">&#8288;function()&#8288;</code></p>
</dd>
<dt><code>xtol_rel</code></dt><dd><p><code>numeric(1)</code></p>
</dd>
<dt><code>xtol_abs</code></dt><dd><p><code>numeric(1)</code></p>
</dd>
<dt><code>ftol_rel</code></dt><dd><p><code>numeric(1)</code></p>
</dd>
<dt><code>ftol_abs</code></dt><dd><p><code>numeric(1)</code></p>
</dd>
<dt><code>start_values</code></dt><dd><p><code>character(1)</code><br />
Create <code>random</code> start values or based on <code>center</code> of search space? In the
latter case, it is the center of the parameters before a trafo is applied.</p>
</dd>
</dl>

<p>For the meaning of the control parameters, see <code><a href="nloptr.html#topic+nloptr">nloptr::nloptr()</a></code> and
<code><a href="nloptr.html#topic+nloptr.print.options">nloptr::nloptr.print.options()</a></code>.
</p>
<p>The termination conditions <code>stopval</code>, <code>maxtime</code> and <code>maxeval</code> of
<code><a href="nloptr.html#topic+nloptr">nloptr::nloptr()</a></code> are deactivated and replaced by the <a href="bbotk.html#topic+Terminator">Terminator</a>
subclasses. The x and function value tolerance termination conditions
(<code>xtol_rel = 10^-4</code>, <code>xtol_abs = rep(0.0, length(x0))</code>, <code>ftol_rel = 0.0</code> and
<code>ftol_abs = 0.0</code>) are still available and implemented with their package
defaults. To deactivate these conditions, set them to <code>-1</code>.
</p>


<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li><p> Learn more about <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-tuner">tuners</a>.
</p>
</li></ul>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>

<ul>
<li><p> Use the <a href="https://mlr-org.com/gallery/series/2023-01-15-hyperband-xgboost/">Hyperband</a> optimizer with different budget parameters.
</p>
</li></ul>



<h3>Progress Bars</h3>

<p><code style="white-space: pre;">&#8288;$optimize()&#8288;</code> supports progress bars via the package <a href="https://CRAN.R-project.org/package=progressr"><span class="pkg">progressr</span></a>
combined with a <a href="bbotk.html#topic+Terminator">Terminator</a>. Simply wrap the function in
<code>progressr::with_progress()</code> to enable them. We recommend to use package
<a href="https://CRAN.R-project.org/package=progress"><span class="pkg">progress</span></a> as backend; enable with <code>progressr::handlers("progress")</code>.
</p>


<h3>Super classes</h3>

<p><code><a href="#topic+Tuner">mlr3tuning::Tuner</a></code> -&gt; <code><a href="#topic+TunerFromOptimizer">mlr3tuning::TunerFromOptimizer</a></code> -&gt; <code>TunerNLoptr</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TunerNLoptr-new"><code>TunerNLoptr$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TunerNLoptr-clone"><code>TunerNLoptr$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="format"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-format'><code>mlr3tuning::Tuner$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="help"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-help'><code>mlr3tuning::Tuner$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="print"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-print'><code>mlr3tuning::Tuner$print()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="TunerFromOptimizer" data-id="optimize"><a href='../../mlr3tuning/html/TunerFromOptimizer.html#method-TunerFromOptimizer-optimize'><code>mlr3tuning::TunerFromOptimizer$optimize()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-TunerNLoptr-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerNLoptr$new()</pre></div>


<hr>
<a id="method-TunerNLoptr-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerNLoptr$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Source</h3>

<p>Johnson, G S (2020).
&ldquo;The NLopt nonlinear-optimization package.&rdquo;
<a href="https://github.com/stevengj/nlopt">https://github.com/stevengj/nlopt</a>.
</p>


<h3>See Also</h3>

<p>Other Tuner: 
<code><a href="#topic+mlr_tuners">mlr_tuners</a></code>,
<code><a href="#topic+mlr_tuners_cmaes">mlr_tuners_cmaes</a></code>,
<code><a href="#topic+mlr_tuners_design_points">mlr_tuners_design_points</a></code>,
<code><a href="#topic+mlr_tuners_gensa">mlr_tuners_gensa</a></code>,
<code><a href="#topic+mlr_tuners_grid_search">mlr_tuners_grid_search</a></code>,
<code><a href="#topic+mlr_tuners_irace">mlr_tuners_irace</a></code>,
<code><a href="#topic+mlr_tuners_random_search">mlr_tuners_random_search</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Hyperparameter Optimization


# load learner and set search space
learner = lrn("classif.rpart",
  cp = to_tune(1e-04, 1e-1, logscale = TRUE)
)

# run hyperparameter tuning on the Palmer Penguins data set
instance = tune(
  tuner = tnr("nloptr", algorithm = "NLOPT_LN_BOBYQA"),
  task = tsk("penguins"),
  learner = learner,
  resampling = rsmp("holdout"),
  measure = msr("classif.ce")
)

# best performing hyperparameter configuration
instance$result

# all evaluated hyperparameter configuration
as.data.table(instance$archive)

# fit final model on complete data set
learner$param_set$values = instance$result_learner_param_vals
learner$train(tsk("penguins"))

</code></pre>

<hr>
<h2 id='mlr_tuners_random_search'>Hyperparameter Tuning with Random Search</h2><span id='topic+mlr_tuners_random_search'></span><span id='topic+TunerRandomSearch'></span>

<h3>Description</h3>

<p>Subclass for random search tuning.
</p>


<h3>Details</h3>

<p>The random points are sampled by <code><a href="paradox.html#topic+generate_design_random">paradox::generate_design_random()</a></code>.
</p>


<h3>Dictionary</h3>

<p>This <a href="#topic+Tuner">Tuner</a> can be instantiated with the associated sugar function <code><a href="#topic+tnr">tnr()</a></code>:
</p>
<div class="sourceCode"><pre>tnr("random_search")
</pre></div>


<h3>Parallelization</h3>

<p>In order to support general termination criteria and parallelization, we
evaluate points in a batch-fashion of size <code>batch_size</code>. Larger batches mean
we can parallelize more, smaller batches imply a more fine-grained checking
of termination criteria. A batch contains of <code>batch_size</code> times <code>resampling$iters</code> jobs.
E.g., if you set a batch size of 10 points and do a 5-fold cross validation, you can
utilize up to 50 cores.
</p>
<p>Parallelization is supported via package <a href="https://CRAN.R-project.org/package=future"><span class="pkg">future</span></a> (see <code><a href="mlr3.html#topic+benchmark">mlr3::benchmark()</a></code>'s
section on parallelization for more details).
</p>


<h3>Logging</h3>

<p>All <a href="#topic+Tuner">Tuner</a>s use a logger (as implemented in <a href="https://CRAN.R-project.org/package=lgr"><span class="pkg">lgr</span></a>) from package
<a href="https://CRAN.R-project.org/package=bbotk"><span class="pkg">bbotk</span></a>.
Use <code>lgr::get_logger("bbotk")</code> to access and control the logger.
</p>


<h3>Optimizer</h3>

<p>This <a href="#topic+Tuner">Tuner</a> is based on <a href="bbotk.html#topic+mlr_optimizers_random_search">bbotk::OptimizerRandomSearch</a> which can be applied on any black box optimization problem.
See also the documentation of <a href="https://bbotk.mlr-org.com/">bbotk</a>.
</p>


<h3>Parameters</h3>


<dl>
<dt><code>batch_size</code></dt><dd><p><code>integer(1)</code><br />
Maximum number of points to try in a batch.</p>
</dd>
</dl>



<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li><p> Learn more about <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-tuner">tuners</a>.
</p>
</li></ul>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>

<ul>
<li><p> Use the <a href="https://mlr-org.com/gallery/series/2023-01-15-hyperband-xgboost/">Hyperband</a> optimizer with different budget parameters.
</p>
</li></ul>



<h3>Progress Bars</h3>

<p><code style="white-space: pre;">&#8288;$optimize()&#8288;</code> supports progress bars via the package <a href="https://CRAN.R-project.org/package=progressr"><span class="pkg">progressr</span></a>
combined with a <a href="bbotk.html#topic+Terminator">Terminator</a>. Simply wrap the function in
<code>progressr::with_progress()</code> to enable them. We recommend to use package
<a href="https://CRAN.R-project.org/package=progress"><span class="pkg">progress</span></a> as backend; enable with <code>progressr::handlers("progress")</code>.
</p>


<h3>Super classes</h3>

<p><code><a href="#topic+Tuner">mlr3tuning::Tuner</a></code> -&gt; <code><a href="#topic+TunerFromOptimizer">mlr3tuning::TunerFromOptimizer</a></code> -&gt; <code>TunerRandomSearch</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TunerRandomSearch-new"><code>TunerRandomSearch$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TunerRandomSearch-clone"><code>TunerRandomSearch$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="format"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-format'><code>mlr3tuning::Tuner$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="help"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-help'><code>mlr3tuning::Tuner$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="print"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-print'><code>mlr3tuning::Tuner$print()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="TunerFromOptimizer" data-id="optimize"><a href='../../mlr3tuning/html/TunerFromOptimizer.html#method-TunerFromOptimizer-optimize'><code>mlr3tuning::TunerFromOptimizer$optimize()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-TunerRandomSearch-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerRandomSearch$new()</pre></div>


<hr>
<a id="method-TunerRandomSearch-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerRandomSearch$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Source</h3>

<p>Bergstra J, Bengio Y (2012).
&ldquo;Random Search for Hyper-Parameter Optimization.&rdquo;
<em>Journal of Machine Learning Research</em>, <b>13</b>(10), 281&ndash;305.
<a href="https://jmlr.csail.mit.edu/papers/v13/bergstra12a.html">https://jmlr.csail.mit.edu/papers/v13/bergstra12a.html</a>.
</p>


<h3>See Also</h3>

<p>Package <a href="https://CRAN.R-project.org/package=mlr3hyperband"><span class="pkg">mlr3hyperband</span></a> for hyperband tuning.
</p>
<p>Other Tuner: 
<code><a href="#topic+mlr_tuners">mlr_tuners</a></code>,
<code><a href="#topic+mlr_tuners_cmaes">mlr_tuners_cmaes</a></code>,
<code><a href="#topic+mlr_tuners_design_points">mlr_tuners_design_points</a></code>,
<code><a href="#topic+mlr_tuners_gensa">mlr_tuners_gensa</a></code>,
<code><a href="#topic+mlr_tuners_grid_search">mlr_tuners_grid_search</a></code>,
<code><a href="#topic+mlr_tuners_irace">mlr_tuners_irace</a></code>,
<code><a href="#topic+mlr_tuners_nloptr">mlr_tuners_nloptr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Hyperparameter Optimization

# load learner and set search space
learner = lrn("classif.rpart",
  cp = to_tune(1e-04, 1e-1, logscale = TRUE)
)

# run hyperparameter tuning on the Palmer Penguins data set
instance = tune(
  tuner = tnr("random_search"),
  task = tsk("penguins"),
  learner = learner,
  resampling = rsmp("holdout"),
  measure = msr("classif.ce"),
  term_evals = 10
)

# best performing hyperparameter configuration
instance$result

# all evaluated hyperparameter configuration
as.data.table(instance$archive)

# fit final model on complete data set
learner$param_set$values = instance$result_learner_param_vals
learner$train(tsk("penguins"))
</code></pre>

<hr>
<h2 id='mlr3tuning-package'>mlr3tuning: Hyperparameter Optimization for 'mlr3'</h2><span id='topic+mlr3tuning'></span><span id='topic+mlr3tuning-package'></span>

<h3>Description</h3>

<p><img src="../help/figures/logo.png" style='float: right' alt='logo' width='120' />
</p>
<p>Hyperparameter optimization package of the 'mlr3' ecosystem. It features highly configurable search spaces via the 'paradox' package and finds optimal hyperparameter configurations for any 'mlr3' learner. 'mlr3tuning' works with several optimization algorithms e.g. Random Search, Iterated Racing, Bayesian Optimization (in 'mlr3mbo') and Hyperband (in 'mlr3hyperband'). Moreover, it can automatically optimize learners and estimate the performance of optimized models with nested resampling.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Marc Becker <a href="mailto:marcbecker@posteo.de">marcbecker@posteo.de</a> (<a href="https://orcid.org/0000-0002-8115-0400">ORCID</a>)
</p>
<p>Authors:
</p>

<ul>
<li><p> Michel Lang <a href="mailto:michellang@gmail.com">michellang@gmail.com</a> (<a href="https://orcid.org/0000-0001-9754-0393">ORCID</a>)
</p>
</li>
<li><p> Jakob Richter <a href="mailto:jakob1richter@gmail.com">jakob1richter@gmail.com</a> (<a href="https://orcid.org/0000-0003-4481-5554">ORCID</a>)
</p>
</li>
<li><p> Bernd Bischl <a href="mailto:bernd_bischl@gmx.net">bernd_bischl@gmx.net</a> (<a href="https://orcid.org/0000-0001-6002-6980">ORCID</a>)
</p>
</li>
<li><p> Daniel Schalk <a href="mailto:daniel.schalk@stat.uni-muenchen.de">daniel.schalk@stat.uni-muenchen.de</a> (<a href="https://orcid.org/0000-0003-0950-1947">ORCID</a>)
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://mlr3tuning.mlr-org.com">https://mlr3tuning.mlr-org.com</a>
</p>
</li>
<li> <p><a href="https://github.com/mlr-org/mlr3tuning">https://github.com/mlr-org/mlr3tuning</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/mlr-org/mlr3tuning/issues">https://github.com/mlr-org/mlr3tuning/issues</a>
</p>
</li></ul>


<hr>
<h2 id='mlr3tuning.backup'>Backup Benchmark Result Callback</h2><span id='topic+mlr3tuning.backup'></span>

<h3>Description</h3>

<p>This <a href="#topic+CallbackTuning">CallbackTuning</a> writes the <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a> after each batch to disk.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>clbk("mlr3tuning.backup", path = "backup.rds")

# tune classification tree on the pima data set
instance = tune(
  tuner = tnr("random_search", batch_size = 2),
  task = tsk("pima"),
  learner = lrn("classif.rpart", cp = to_tune(1e-04, 1e-1, logscale = TRUE)),
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  term_evals = 4,
  callbacks = clbk("mlr3tuning.backup", path = tempfile(fileext = ".rds"))
)
</code></pre>

<hr>
<h2 id='mlr3tuning.early_stopping'>Early Stopping Callback</h2><span id='topic+mlr3tuning.early_stopping'></span>

<h3>Description</h3>

<p>This <a href="#topic+CallbackTuning">CallbackTuning</a> integrates early stopping into the hyperparameter tuning of an XGBoost learner.
Early stopping estimates the optimal number of trees (<code>nrounds</code>) for a given hyperparameter configuration.
Since early stopping is performed in each resampling iteration, there are several optimal <code>nrounds</code> values.
The callback writes the maximum value to the archive in the <code>max_nrounds</code> column.
In the best hyperparameter configuration (<code>instance$result_learner_param_vals</code>), the value of <code>nrounds</code> is replaced by <code>max_nrounds</code> and early stopping is deactivated.
</p>


<h3>Details</h3>

<p>Currently, the callback does not work with <code>GraphLearner</code>s from the package <a href="https://CRAN.R-project.org/package=mlr3pipelines"><span class="pkg">mlr3pipelines</span></a>.
The callback is compatible with the <a href="#topic+AutoTuner">AutoTuner</a>.
The final model is fitted with the best hyperparameter configuration and <code>max_nrounds</code> i.e. early stopping is not performed.
</p>


<h3>Resources</h3>


<ul>
<li> <p><a href="https://mlr-org.com/gallery/optimization/2022-11-04-early-stopping-with-xgboost/">gallery post</a> on early stopping with XGBoost.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>clbk("mlr3tuning.early_stopping")

if (requireNamespace("mlr3learners") &amp;&amp; requireNamespace("xgboost") ) {
  library(mlr3learners)

  # activate early stopping on the test set and set search space
  learner = lrn("classif.xgboost",
    eta = to_tune(1e-02, 1e-1, logscale = TRUE),
    early_stopping_rounds = 5,
    nrounds = 100,
    early_stopping_set = "test")

  # tune xgboost on the pima data set
  instance = tune(
    tuner = tnr("random_search"),
    task = tsk("pima"),
    learner = learner,
    resampling = rsmp("cv", folds = 3),
    measures = msr("classif.ce"),
    term_evals = 10,
    callbacks = clbk("mlr3tuning.early_stopping")
  )
}

</code></pre>

<hr>
<h2 id='mlr3tuning.measures'>Measure Callback</h2><span id='topic+mlr3tuning.measures'></span>

<h3>Description</h3>

<p>This <a href="#topic+CallbackTuning">CallbackTuning</a> scores the hyperparameter configurations on additional measures while tuning.
Usually, the configurations can be scored on additional measures after tuning (see <a href="#topic+ArchiveTuning">ArchiveTuning</a>).
However, if the memory is not sufficient to store the <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>, it is necessary to score the additional measures while tuning.
The measures are not taken into account by the tuner.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>clbk("mlr3tuning.measures")

# additionally score the configurations on the accuracy measure
instance = tune(
  tuner = tnr("random_search", batch_size = 2),
  task = tsk("pima"),
  learner = lrn("classif.rpart", cp = to_tune(1e-04, 1e-1, logscale = TRUE)),
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  term_evals = 4,
  callbacks = clbk("mlr3tuning.measures", measures = msr("classif.acc"))
)

# score the configurations on the holdout set
task = tsk("pima")
splits = partition(task, ratio = 0.8)
task$row_roles$use = splits$train
task$row_roles$holdout = splits$test

learner = lrn("classif.rpart", cp = to_tune(1e-04, 1e-1, logscale = TRUE))
learner$predict_sets = c("test", "holdout")

instance = tune(
  tuner = tnr("random_search", batch_size = 2),
  task = task,
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  term_evals = 4,
  callbacks = clbk("mlr3tuning.measures", measures = msr("classif.ce",
    predict_sets = "holdout", id = "classif.ce_holdout"))
)
</code></pre>

<hr>
<h2 id='ObjectiveTuning'>Class for Tuning Objective</h2><span id='topic+ObjectiveTuning'></span>

<h3>Description</h3>

<p>Stores the objective function that estimates the performance of hyperparameter configurations.
This class is usually constructed internally by the <a href="#topic+TuningInstanceSingleCrit">TuningInstanceSingleCrit</a> or <a href="#topic+TuningInstanceMultiCrit">TuningInstanceMultiCrit</a>.
</p>


<h3>Super class</h3>

<p><code><a href="bbotk.html#topic+Objective">bbotk::Objective</a></code> -&gt; <code>ObjectiveTuning</code>
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>task</code></dt><dd><p>(<a href="mlr3.html#topic+Task">mlr3::Task</a>).</p>
</dd>
<dt><code>learner</code></dt><dd><p>(<a href="mlr3.html#topic+Learner">mlr3::Learner</a>).</p>
</dd>
<dt><code>default_values</code></dt><dd><p>(named list).
Default hyperparameter values of the learner.</p>
</dd>
<dt><code>resampling</code></dt><dd><p>(<a href="mlr3.html#topic+Resampling">mlr3::Resampling</a>).</p>
</dd>
<dt><code>measures</code></dt><dd><p>(list of <a href="mlr3.html#topic+Measure">mlr3::Measure</a>).</p>
</dd>
<dt><code>store_models</code></dt><dd><p>(<code>logical(1)</code>).</p>
</dd>
<dt><code>store_benchmark_result</code></dt><dd><p>(<code>logical(1)</code>).</p>
</dd>
<dt><code>archive</code></dt><dd><p>(<a href="#topic+ArchiveTuning">ArchiveTuning</a>).</p>
</dd>
<dt><code>hotstart_stack</code></dt><dd><p>(<a href="mlr3.html#topic+HotstartStack">mlr3::HotstartStack</a>).</p>
</dd>
<dt><code>allow_hotstart</code></dt><dd><p>(<code>logical(1)</code>).</p>
</dd>
<dt><code>keep_hotstart_stack</code></dt><dd><p>(<code>logical(1)</code>).</p>
</dd>
<dt><code>callbacks</code></dt><dd><p>(List of <a href="#topic+CallbackTuning">CallbackTuning</a>s).</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-ObjectiveTuning-new"><code>ObjectiveTuning$new()</code></a>
</p>
</li>
<li> <p><a href="#method-ObjectiveTuning-clone"><code>ObjectiveTuning$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Objective" data-id="eval"><a href='../../bbotk/html/Objective.html#method-Objective-eval'><code>bbotk::Objective$eval()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Objective" data-id="eval_dt"><a href='../../bbotk/html/Objective.html#method-Objective-eval_dt'><code>bbotk::Objective$eval_dt()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Objective" data-id="eval_many"><a href='../../bbotk/html/Objective.html#method-Objective-eval_many'><code>bbotk::Objective$eval_many()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Objective" data-id="format"><a href='../../bbotk/html/Objective.html#method-Objective-format'><code>bbotk::Objective$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Objective" data-id="print"><a href='../../bbotk/html/Objective.html#method-Objective-print'><code>bbotk::Objective$print()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-ObjectiveTuning-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>ObjectiveTuning$new(
  task,
  learner,
  resampling,
  measures,
  store_benchmark_result = TRUE,
  store_models = FALSE,
  check_values = TRUE,
  allow_hotstart = FALSE,
  keep_hotstart_stack = FALSE,
  archive = NULL,
  callbacks = list()
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>task</code></dt><dd><p>(<a href="mlr3.html#topic+Task">mlr3::Task</a>)<br />
Task to operate on.</p>
</dd>
<dt><code>learner</code></dt><dd><p>(<a href="mlr3.html#topic+Learner">mlr3::Learner</a>)<br />
Learner to tune.</p>
</dd>
<dt><code>resampling</code></dt><dd><p>(<a href="mlr3.html#topic+Resampling">mlr3::Resampling</a>)<br />
Resampling that is used to evaluate the performance of the hyperparameter configurations.
Uninstantiated resamplings are instantiated during construction so that all configurations are evaluated on the same data splits.
Already instantiated resamplings are kept unchanged.
Specialized <a href="#topic+Tuner">Tuner</a> change the resampling e.g. to evaluate a hyperparameter configuration on different data splits.
This field, however, always returns the resampling passed in construction.</p>
</dd>
<dt><code>measures</code></dt><dd><p>(list of <a href="mlr3.html#topic+Measure">mlr3::Measure</a>)<br />
Measures to optimize.</p>
</dd>
<dt><code>store_benchmark_result</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default), store resample result of evaluated hyperparameter
configurations in archive as <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>.</p>
</dd>
<dt><code>store_models</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, fitted models are stored in the benchmark result
(<code>archive$benchmark_result</code>). If <code>store_benchmark_result = FALSE</code>, models
are only stored temporarily and not accessible after the tuning. This
combination is needed for measures that require a model.</p>
</dd>
<dt><code>check_values</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, hyperparameter values are checked before evaluation and
performance scores after. If <code>FALSE</code> (default), values are unchecked but
computational overhead is reduced.</p>
</dd>
<dt><code>allow_hotstart</code></dt><dd><p>(<code>logical(1)</code>)<br />
Allow to hotstart learners with previously fitted models. See also
<a href="mlr3.html#topic+HotstartStack">mlr3::HotstartStack</a>. The learner must support hotstarting. Sets
<code>store_models = TRUE</code>.</p>
</dd>
<dt><code>keep_hotstart_stack</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, <a href="mlr3.html#topic+HotstartStack">mlr3::HotstartStack</a> is kept in <code style="white-space: pre;">&#8288;$objective$hotstart_stack&#8288;</code>
after tuning.</p>
</dd>
<dt><code>archive</code></dt><dd><p>(<a href="#topic+ArchiveTuning">ArchiveTuning</a>)<br />
Reference to archive of <a href="#topic+TuningInstanceSingleCrit">TuningInstanceSingleCrit</a> | <a href="#topic+TuningInstanceMultiCrit">TuningInstanceMultiCrit</a>.
If <code>NULL</code> (default), benchmark result and models cannot be stored.</p>
</dd>
<dt><code>callbacks</code></dt><dd><p>(list of <a href="#topic+CallbackTuning">CallbackTuning</a>)<br />
List of callbacks.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-ObjectiveTuning-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>ObjectiveTuning$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic+mlr_terminators'></span><span id='topic+trm'></span><span id='topic+trms'></span><span id='topic+mlr_callbacks'></span><span id='topic+clbk'></span><span id='topic+clbks'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>bbotk</dt><dd><p><code><a href="bbotk.html#topic+mlr_terminators">mlr_terminators</a></code>, <code><a href="bbotk.html#topic+trm">trm</a></code>, <code><a href="bbotk.html#topic+trm">trms</a></code></p>
</dd>
<dt>mlr3misc</dt><dd><p><code><a href="mlr3misc.html#topic+clbk">clbk</a></code>, <code><a href="mlr3misc.html#topic+clbk">clbks</a></code>, <code><a href="mlr3misc.html#topic+mlr_callbacks">mlr_callbacks</a></code></p>
</dd>
</dl>

<hr>
<h2 id='ti'>Syntactic Sugar for Tuning Instance Construction</h2><span id='topic+ti'></span>

<h3>Description</h3>

<p>Function to construct a <a href="#topic+TuningInstanceSingleCrit">TuningInstanceSingleCrit</a> or <a href="#topic+TuningInstanceMultiCrit">TuningInstanceMultiCrit</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ti(
  task,
  learner,
  resampling,
  measures = NULL,
  terminator,
  search_space = NULL,
  store_benchmark_result = TRUE,
  store_models = FALSE,
  check_values = FALSE,
  allow_hotstart = FALSE,
  keep_hotstart_stack = FALSE,
  evaluate_default = FALSE,
  callbacks = list()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ti_+3A_task">task</code></td>
<td>
<p>(<a href="mlr3.html#topic+Task">mlr3::Task</a>)<br />
Task to operate on.</p>
</td></tr>
<tr><td><code id="ti_+3A_learner">learner</code></td>
<td>
<p>(<a href="mlr3.html#topic+Learner">mlr3::Learner</a>)<br />
Learner to tune.</p>
</td></tr>
<tr><td><code id="ti_+3A_resampling">resampling</code></td>
<td>
<p>(<a href="mlr3.html#topic+Resampling">mlr3::Resampling</a>)<br />
Resampling that is used to evaluate the performance of the hyperparameter configurations.
Uninstantiated resamplings are instantiated during construction so that all configurations are evaluated on the same data splits.
Already instantiated resamplings are kept unchanged.
Specialized <a href="#topic+Tuner">Tuner</a> change the resampling e.g. to evaluate a hyperparameter configuration on different data splits.
This field, however, always returns the resampling passed in construction.</p>
</td></tr>
<tr><td><code id="ti_+3A_measures">measures</code></td>
<td>
<p>(<a href="mlr3.html#topic+Measure">mlr3::Measure</a> or list of <a href="mlr3.html#topic+Measure">mlr3::Measure</a>)<br />
A single measure creates a <a href="#topic+TuningInstanceSingleCrit">TuningInstanceSingleCrit</a> and multiple measures a <a href="#topic+TuningInstanceMultiCrit">TuningInstanceMultiCrit</a>.
If <code>NULL</code>, default measure is used.</p>
</td></tr>
<tr><td><code id="ti_+3A_terminator">terminator</code></td>
<td>
<p>(<a href="bbotk.html#topic+Terminator">Terminator</a>)<br />
Stop criterion of the tuning process.</p>
</td></tr>
<tr><td><code id="ti_+3A_search_space">search_space</code></td>
<td>
<p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>)<br />
Hyperparameter search space. If <code>NULL</code> (default), the search space is
constructed from the <a href="paradox.html#topic+TuneToken">TuneToken</a> of the learner's parameter set
(learner$param_set).</p>
</td></tr>
<tr><td><code id="ti_+3A_store_benchmark_result">store_benchmark_result</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default), store resample result of evaluated hyperparameter
configurations in archive as <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>.</p>
</td></tr>
<tr><td><code id="ti_+3A_store_models">store_models</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, fitted models are stored in the benchmark result
(<code>archive$benchmark_result</code>). If <code>store_benchmark_result = FALSE</code>, models
are only stored temporarily and not accessible after the tuning. This
combination is needed for measures that require a model.</p>
</td></tr>
<tr><td><code id="ti_+3A_check_values">check_values</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, hyperparameter values are checked before evaluation and
performance scores after. If <code>FALSE</code> (default), values are unchecked but
computational overhead is reduced.</p>
</td></tr>
<tr><td><code id="ti_+3A_allow_hotstart">allow_hotstart</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
Allow to hotstart learners with previously fitted models. See also
<a href="mlr3.html#topic+HotstartStack">mlr3::HotstartStack</a>. The learner must support hotstarting. Sets
<code>store_models = TRUE</code>.</p>
</td></tr>
<tr><td><code id="ti_+3A_keep_hotstart_stack">keep_hotstart_stack</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, <a href="mlr3.html#topic+HotstartStack">mlr3::HotstartStack</a> is kept in <code style="white-space: pre;">&#8288;$objective$hotstart_stack&#8288;</code>
after tuning.</p>
</td></tr>
<tr><td><code id="ti_+3A_evaluate_default">evaluate_default</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, learner is evaluated with hyperparameters set to their default
values at the start of the optimization.</p>
</td></tr>
<tr><td><code id="ti_+3A_callbacks">callbacks</code></td>
<td>
<p>(list of <a href="#topic+CallbackTuning">CallbackTuning</a>)<br />
List of callbacks.</p>
</td></tr>
</table>


<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li><p> Getting started with <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html">hyperparameter optimization</a>.
</p>
</li>
<li> <p><a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-model-tuning">Tune</a> a simple classification tree on the Sonar data set.
</p>
</li>
<li><p> Learn about <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-defining-search-spaces">tuning spaces</a>.
</p>
</li></ul>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>

<ul>
<li><p> Learn more advanced methods with the <a href="https://mlr-org.com/gallery/series/2021-03-09-practical-tuning-series-tune-a-support-vector-machine/">practical tuning series</a>.
</p>
</li>
<li><p> Simultaneously optimize hyperparameters and use <a href="https://mlr-org.com/gallery/optimization/2022-11-04-early-stopping-with-xgboost/">early stopping</a> with XGBoost.
</p>
</li>
<li><p> Make us of proven <a href="https://mlr-org.com/gallery/optimization/2021-07-06-introduction-to-mlr3tuningspaces/">search space</a>.
</p>
</li>
<li><p> Learn about <a href="https://mlr-org.com/gallery/optimization/2023-01-16-hotstart/">hotstarting</a> models.
</p>
</li>
<li><p> Run the <a href="https://mlr-org.com/gallery/optimization/2023-01-31-default-configuration/">default hyperparameter configuration</a> of learners as a baseline.
</p>
</li></ul>



<h3>Default Measures</h3>

<p>If no measure is passed, the default measure is used.
The default measure depends on the task type.</p>

<table>
<tr>
 <td style="text-align: left;">
   Task </td><td style="text-align: left;"> Default Measure </td><td style="text-align: left;"> Package </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"classif"</code> </td><td style="text-align: left;"> <code>"classif.ce"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3"><span class="pkg">mlr3</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"regr"</code> </td><td style="text-align: left;"> <code>"regr.mse"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3"><span class="pkg">mlr3</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"surv"</code> </td><td style="text-align: left;"> <code>"surv.cindex"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3proba"><span class="pkg">mlr3proba</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"dens"</code> </td><td style="text-align: left;"> <code>"dens.logloss"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3proba"><span class="pkg">mlr3proba</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"classif_st"</code> </td><td style="text-align: left;"> <code>"classif.ce"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3spatial"><span class="pkg">mlr3spatial</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"regr_st"</code> </td><td style="text-align: left;"> <code>"regr.mse"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3spatial"><span class="pkg">mlr3spatial</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"clust"</code> </td><td style="text-align: left;"> <code>"clust.dunn"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3cluster"><span class="pkg">mlr3cluster</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Examples</h3>

<pre><code class='language-R'># Hyperparameter optimization on the Palmer Penguins data set
task = tsk("penguins")

# Load learner and set search space
learner = lrn("classif.rpart",
  cp = to_tune(1e-04, 1e-1, logscale = TRUE)
)

# Construct tuning instance
instance = ti(
  task = task,
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  terminator = trm("evals", n_evals = 4)
)

# Choose optimization algorithm
tuner = tnr("random_search", batch_size = 2)

# Run tuning
tuner$optimize(instance)

# Set optimal hyperparameter configuration to learner
learner$param_set$values = instance$result_learner_param_vals

# Train the learner on the full data set
learner$train(task)

# Inspect all evaluated configurations
as.data.table(instance$archive)
</code></pre>

<hr>
<h2 id='tnr'>Syntactic Sugar for Tuning Objects Construction</h2><span id='topic+tnr'></span><span id='topic+tnrs'></span>

<h3>Description</h3>

<p>Functions to retrieve objects, set parameters and assign to fields in one go.
Relies on <code><a href="mlr3misc.html#topic+dictionary_sugar_get">mlr3misc::dictionary_sugar_get()</a></code> to extract objects from the respective <a href="mlr3misc.html#topic+Dictionary">mlr3misc::Dictionary</a>:
</p>

<ul>
<li> <p><code>tnr()</code> for a <a href="#topic+Tuner">Tuner</a> from <a href="#topic+mlr_tuners">mlr_tuners</a>.
</p>
</li>
<li> <p><code>tnrs()</code> for a list of <a href="#topic+Tuner">Tuners</a> from <a href="#topic+mlr_tuners">mlr_tuners</a>.
</p>
</li>
<li> <p><code>trm()</code> for a <a href="bbotk.html#topic+Terminator">Terminator</a> from <a href="#topic+mlr_terminators">mlr_terminators</a>.
</p>
</li>
<li> <p><code>trms()</code> for a list of <a href="bbotk.html#topic+Terminator">Terminators</a> from <a href="#topic+mlr_terminators">mlr_terminators</a>.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>tnr(.key, ...)

tnrs(.keys, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tnr_+3A_.key">.key</code></td>
<td>
<p>(<code>character(1)</code>)<br />
Key passed to the respective <a href="mlr3misc.html#topic+Dictionary">dictionary</a> to retrieve the object.</p>
</td></tr>
<tr><td><code id="tnr_+3A_...">...</code></td>
<td>
<p>(named <code>list()</code>)<br />
Named arguments passed to the constructor, to be set as parameters in the <a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>, or to be set as public field.
See <code><a href="mlr3misc.html#topic+dictionary_sugar_get">mlr3misc::dictionary_sugar_get()</a></code> for more details.</p>
</td></tr>
<tr><td><code id="tnr_+3A_.keys">.keys</code></td>
<td>
<p>(<code>character()</code>)<br />
Keys passed to the respective <a href="mlr3misc.html#topic+Dictionary">dictionary</a> to retrieve multiple objects.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><a href="R6.html#topic+R6Class">R6::R6Class</a> object of the respective type, or a list of <a href="R6.html#topic+R6Class">R6::R6Class</a> objects for the plural versions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># random search tuner with batch size of 5
tnr("random_search", batch_size = 5)

# run time terminator with 20 seconds
trm("run_time", secs = 20)
</code></pre>

<hr>
<h2 id='tune'>Function for Tuning a Learner</h2><span id='topic+tune'></span>

<h3>Description</h3>

<p>Function to tune a <a href="mlr3.html#topic+Learner">mlr3::Learner</a>.
The function internally creates a <a href="#topic+TuningInstanceSingleCrit">TuningInstanceSingleCrit</a> or <a href="#topic+TuningInstanceMultiCrit">TuningInstanceMultiCrit</a> which describe the tuning problem.
It executes the tuning with the <a href="#topic+Tuner">Tuner</a> (<code>tuner</code>) and returns the result with the tuning instance (<code style="white-space: pre;">&#8288;$result&#8288;</code>).
The <a href="#topic+ArchiveTuning">ArchiveTuning</a> (<code style="white-space: pre;">&#8288;$archive&#8288;</code>) stores all evaluated hyperparameter configurations and performance scores.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tune(
  tuner,
  task,
  learner,
  resampling,
  measures = NULL,
  term_evals = NULL,
  term_time = NULL,
  terminator = NULL,
  search_space = NULL,
  store_benchmark_result = TRUE,
  store_models = FALSE,
  check_values = FALSE,
  allow_hotstart = FALSE,
  keep_hotstart_stack = FALSE,
  evaluate_default = FALSE,
  callbacks = list(),
  method
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tune_+3A_tuner">tuner</code></td>
<td>
<p>(<a href="#topic+Tuner">Tuner</a>)<br />
Optimization algorithm.</p>
</td></tr>
<tr><td><code id="tune_+3A_task">task</code></td>
<td>
<p>(<a href="mlr3.html#topic+Task">mlr3::Task</a>)<br />
Task to operate on.</p>
</td></tr>
<tr><td><code id="tune_+3A_learner">learner</code></td>
<td>
<p>(<a href="mlr3.html#topic+Learner">mlr3::Learner</a>)<br />
Learner to tune.</p>
</td></tr>
<tr><td><code id="tune_+3A_resampling">resampling</code></td>
<td>
<p>(<a href="mlr3.html#topic+Resampling">mlr3::Resampling</a>)<br />
Resampling that is used to evaluate the performance of the hyperparameter configurations.
Uninstantiated resamplings are instantiated during construction so that all configurations are evaluated on the same data splits.
Already instantiated resamplings are kept unchanged.
Specialized <a href="#topic+Tuner">Tuner</a> change the resampling e.g. to evaluate a hyperparameter configuration on different data splits.
This field, however, always returns the resampling passed in construction.</p>
</td></tr>
<tr><td><code id="tune_+3A_measures">measures</code></td>
<td>
<p>(<a href="mlr3.html#topic+Measure">mlr3::Measure</a> or list of <a href="mlr3.html#topic+Measure">mlr3::Measure</a>)<br />
A single measure creates a <a href="#topic+TuningInstanceSingleCrit">TuningInstanceSingleCrit</a> and multiple measures a <a href="#topic+TuningInstanceMultiCrit">TuningInstanceMultiCrit</a>.
If <code>NULL</code>, default measure is used.</p>
</td></tr>
<tr><td><code id="tune_+3A_term_evals">term_evals</code></td>
<td>
<p>(<code>integer(1)</code>)<br />
Number of allowed evaluations.
Ignored if <code>terminator</code> is passed.</p>
</td></tr>
<tr><td><code id="tune_+3A_term_time">term_time</code></td>
<td>
<p>(<code>integer(1)</code>)<br />
Maximum allowed time in seconds.
Ignored if <code>terminator</code> is passed.</p>
</td></tr>
<tr><td><code id="tune_+3A_terminator">terminator</code></td>
<td>
<p>(<a href="bbotk.html#topic+Terminator">Terminator</a>)<br />
Stop criterion of the tuning process.</p>
</td></tr>
<tr><td><code id="tune_+3A_search_space">search_space</code></td>
<td>
<p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>)<br />
Hyperparameter search space. If <code>NULL</code> (default), the search space is
constructed from the <a href="paradox.html#topic+TuneToken">TuneToken</a> of the learner's parameter set
(learner$param_set).</p>
</td></tr>
<tr><td><code id="tune_+3A_store_benchmark_result">store_benchmark_result</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default), store resample result of evaluated hyperparameter
configurations in archive as <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>.</p>
</td></tr>
<tr><td><code id="tune_+3A_store_models">store_models</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, fitted models are stored in the benchmark result
(<code>archive$benchmark_result</code>). If <code>store_benchmark_result = FALSE</code>, models
are only stored temporarily and not accessible after the tuning. This
combination is needed for measures that require a model.</p>
</td></tr>
<tr><td><code id="tune_+3A_check_values">check_values</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, hyperparameter values are checked before evaluation and
performance scores after. If <code>FALSE</code> (default), values are unchecked but
computational overhead is reduced.</p>
</td></tr>
<tr><td><code id="tune_+3A_allow_hotstart">allow_hotstart</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
Allow to hotstart learners with previously fitted models. See also
<a href="mlr3.html#topic+HotstartStack">mlr3::HotstartStack</a>. The learner must support hotstarting. Sets
<code>store_models = TRUE</code>.</p>
</td></tr>
<tr><td><code id="tune_+3A_keep_hotstart_stack">keep_hotstart_stack</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, <a href="mlr3.html#topic+HotstartStack">mlr3::HotstartStack</a> is kept in <code style="white-space: pre;">&#8288;$objective$hotstart_stack&#8288;</code>
after tuning.</p>
</td></tr>
<tr><td><code id="tune_+3A_evaluate_default">evaluate_default</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, learner is evaluated with hyperparameters set to their default
values at the start of the optimization.</p>
</td></tr>
<tr><td><code id="tune_+3A_callbacks">callbacks</code></td>
<td>
<p>(list of <a href="#topic+CallbackTuning">CallbackTuning</a>)<br />
List of callbacks.</p>
</td></tr>
<tr><td><code id="tune_+3A_method">method</code></td>
<td>
<p>(<code>character(1)</code>)<br />
Deprecated. Use <code>tuner</code> instead.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <a href="mlr3.html#topic+Task">mlr3::Task</a>, <a href="mlr3.html#topic+Learner">mlr3::Learner</a>, <a href="mlr3.html#topic+Resampling">mlr3::Resampling</a>, <a href="mlr3.html#topic+Measure">mlr3::Measure</a> and <a href="bbotk.html#topic+Terminator">Terminator</a> are used to construct a <a href="#topic+TuningInstanceSingleCrit">TuningInstanceSingleCrit</a>.
If multiple performance <a href="mlr3.html#topic+Measure">Measures</a> are supplied, a <a href="#topic+TuningInstanceMultiCrit">TuningInstanceMultiCrit</a> is created.
The parameter <code>term_evals</code> and <code>term_time</code> are shortcuts to create a <a href="bbotk.html#topic+Terminator">Terminator</a>.
If both parameters are passed, a <a href="bbotk.html#topic+TerminatorCombo">TerminatorCombo</a> is constructed.
For other <a href="bbotk.html#topic+Terminator">Terminators</a>, pass one with <code>terminator</code>.
If no termination criterion is needed, set <code>term_evals</code>, <code>term_time</code> and <code>terminator</code> to <code>NULL</code>.
The search space is created from <a href="paradox.html#topic+to_tune">paradox::TuneToken</a> or is supplied by <code>search_space</code>.
</p>


<h3>Value</h3>

<p><a href="#topic+TuningInstanceSingleCrit">TuningInstanceSingleCrit</a> | <a href="#topic+TuningInstanceMultiCrit">TuningInstanceMultiCrit</a>
</p>


<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li><p> Simplify tuning with the <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-autotuner"><code>tune()</code></a> function.
</p>
</li>
<li><p> Learn about <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-defining-search-spaces">tuning spaces</a>.
</p>
</li></ul>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>

<ul>
<li><p> Optimize an rpart classification tree with only a <a href="https://mlr-org.com/gallery/optimization/2022-11-10-hyperparameter-optimization-on-the-palmer-penguins/">few lines of code</a>.
</p>
</li>
<li><p> Tune an XGBoost model with <a href="https://mlr-org.com/gallery/optimization/2022-11-04-early-stopping-with-xgboost/">early stopping</a>.
</p>
</li>
<li><p> Make us of proven <a href="https://mlr-org.com/gallery/optimization/2021-07-06-introduction-to-mlr3tuningspaces/">search space</a>.
</p>
</li>
<li><p> Learn about <a href="https://mlr-org.com/gallery/optimization/2023-01-16-hotstart/">hotstarting</a> models.
</p>
</li></ul>



<h3>Default Measures</h3>

<p>If no measure is passed, the default measure is used.
The default measure depends on the task type.</p>

<table>
<tr>
 <td style="text-align: left;">
   Task </td><td style="text-align: left;"> Default Measure </td><td style="text-align: left;"> Package </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"classif"</code> </td><td style="text-align: left;"> <code>"classif.ce"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3"><span class="pkg">mlr3</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"regr"</code> </td><td style="text-align: left;"> <code>"regr.mse"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3"><span class="pkg">mlr3</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"surv"</code> </td><td style="text-align: left;"> <code>"surv.cindex"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3proba"><span class="pkg">mlr3proba</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"dens"</code> </td><td style="text-align: left;"> <code>"dens.logloss"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3proba"><span class="pkg">mlr3proba</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"classif_st"</code> </td><td style="text-align: left;"> <code>"classif.ce"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3spatial"><span class="pkg">mlr3spatial</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"regr_st"</code> </td><td style="text-align: left;"> <code>"regr.mse"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3spatial"><span class="pkg">mlr3spatial</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"clust"</code> </td><td style="text-align: left;"> <code>"clust.dunn"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3cluster"><span class="pkg">mlr3cluster</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Analysis</h3>

<p>For analyzing the tuning results, it is recommended to pass the <a href="#topic+ArchiveTuning">ArchiveTuning</a> to <code>as.data.table()</code>.
The returned data table is joined with the benchmark result which adds the <a href="mlr3.html#topic+ResampleResult">mlr3::ResampleResult</a> for each hyperparameter evaluation.
</p>
<p>The archive provides various getters (e.g. <code style="white-space: pre;">&#8288;$learners()&#8288;</code>) to ease the access.
All getters extract by position (<code>i</code>) or unique hash (<code>uhash</code>).
For a complete list of all getters see the methods section.
</p>
<p>The benchmark result (<code style="white-space: pre;">&#8288;$benchmark_result&#8288;</code>) allows to score the hyperparameter configurations again on a different measure.
Alternatively, measures can be supplied to <code>as.data.table()</code>.
</p>
<p>The <a href="https://CRAN.R-project.org/package=mlr3viz"><span class="pkg">mlr3viz</span></a> package provides visualizations for tuning results.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Hyperparameter optimization on the Palmer Penguins data set
task = tsk("pima")

# Load learner and set search space
learner = lrn("classif.rpart",
  cp = to_tune(1e-04, 1e-1, logscale = TRUE)
)

# Run tuning
instance = tune(
  tuner = tnr("random_search", batch_size = 2),
  task = tsk("pima"),
  learner = learner,
  resampling = rsmp ("holdout"),
  measures = msr("classif.ce"),
  terminator = trm("evals", n_evals = 4)
)

# Set optimal hyperparameter configuration to learner
learner$param_set$values = instance$result_learner_param_vals

# Train the learner on the full data set
learner$train(task)

# Inspect all evaluated configurations
as.data.table(instance$archive)
</code></pre>

<hr>
<h2 id='tune_nested'>Function for Nested Resampling</h2><span id='topic+tune_nested'></span>

<h3>Description</h3>

<p>Function to conduct nested resampling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tune_nested(
  tuner,
  task,
  learner,
  inner_resampling,
  outer_resampling,
  measure = NULL,
  term_evals = NULL,
  term_time = NULL,
  terminator = NULL,
  search_space = NULL,
  store_tuning_instance = TRUE,
  store_benchmark_result = TRUE,
  store_models = FALSE,
  check_values = FALSE,
  allow_hotstart = FALSE,
  keep_hotstart_stack = FALSE,
  evaluate_default = FALSE,
  callbacks = list(),
  method
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tune_nested_+3A_tuner">tuner</code></td>
<td>
<p>(<a href="#topic+Tuner">Tuner</a>)<br />
Optimization algorithm.</p>
</td></tr>
<tr><td><code id="tune_nested_+3A_task">task</code></td>
<td>
<p>(<a href="mlr3.html#topic+Task">mlr3::Task</a>)<br />
Task to operate on.</p>
</td></tr>
<tr><td><code id="tune_nested_+3A_learner">learner</code></td>
<td>
<p>(<a href="mlr3.html#topic+Learner">mlr3::Learner</a>)<br />
Learner to tune.</p>
</td></tr>
<tr><td><code id="tune_nested_+3A_inner_resampling">inner_resampling</code></td>
<td>
<p>(<a href="mlr3.html#topic+Resampling">mlr3::Resampling</a>)<br />
Resampling used for the inner loop.</p>
</td></tr>
<tr><td><code id="tune_nested_+3A_outer_resampling">outer_resampling</code></td>
<td>
<p><a href="mlr3.html#topic+Resampling">mlr3::Resampling</a>)<br />
Resampling used for the outer loop.</p>
</td></tr>
<tr><td><code id="tune_nested_+3A_measure">measure</code></td>
<td>
<p>(<a href="mlr3.html#topic+Measure">mlr3::Measure</a>)<br />
Measure to optimize. If <code>NULL</code>, default measure is used.</p>
</td></tr>
<tr><td><code id="tune_nested_+3A_term_evals">term_evals</code></td>
<td>
<p>(<code>integer(1)</code>)<br />
Number of allowed evaluations.
Ignored if <code>terminator</code> is passed.</p>
</td></tr>
<tr><td><code id="tune_nested_+3A_term_time">term_time</code></td>
<td>
<p>(<code>integer(1)</code>)<br />
Maximum allowed time in seconds.
Ignored if <code>terminator</code> is passed.</p>
</td></tr>
<tr><td><code id="tune_nested_+3A_terminator">terminator</code></td>
<td>
<p>(<a href="bbotk.html#topic+Terminator">Terminator</a>)<br />
Stop criterion of the tuning process.</p>
</td></tr>
<tr><td><code id="tune_nested_+3A_search_space">search_space</code></td>
<td>
<p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>)<br />
Hyperparameter search space. If <code>NULL</code> (default), the search space is
constructed from the <a href="paradox.html#topic+TuneToken">TuneToken</a> of the learner's parameter set
(learner$param_set).</p>
</td></tr>
<tr><td><code id="tune_nested_+3A_store_tuning_instance">store_tuning_instance</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default), stores the internally created <a href="#topic+TuningInstanceSingleCrit">TuningInstanceSingleCrit</a> with all intermediate results in slot <code style="white-space: pre;">&#8288;$tuning_instance&#8288;</code>.</p>
</td></tr>
<tr><td><code id="tune_nested_+3A_store_benchmark_result">store_benchmark_result</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default), store resample result of evaluated hyperparameter
configurations in archive as <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>.</p>
</td></tr>
<tr><td><code id="tune_nested_+3A_store_models">store_models</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, fitted models are stored in the benchmark result
(<code>archive$benchmark_result</code>). If <code>store_benchmark_result = FALSE</code>, models
are only stored temporarily and not accessible after the tuning. This
combination is needed for measures that require a model.</p>
</td></tr>
<tr><td><code id="tune_nested_+3A_check_values">check_values</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, hyperparameter values are checked before evaluation and
performance scores after. If <code>FALSE</code> (default), values are unchecked but
computational overhead is reduced.</p>
</td></tr>
<tr><td><code id="tune_nested_+3A_allow_hotstart">allow_hotstart</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
Allow to hotstart learners with previously fitted models. See also
<a href="mlr3.html#topic+HotstartStack">mlr3::HotstartStack</a>. The learner must support hotstarting. Sets
<code>store_models = TRUE</code>.</p>
</td></tr>
<tr><td><code id="tune_nested_+3A_keep_hotstart_stack">keep_hotstart_stack</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, <a href="mlr3.html#topic+HotstartStack">mlr3::HotstartStack</a> is kept in <code style="white-space: pre;">&#8288;$objective$hotstart_stack&#8288;</code>
after tuning.</p>
</td></tr>
<tr><td><code id="tune_nested_+3A_evaluate_default">evaluate_default</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, learner is evaluated with hyperparameters set to their default
values at the start of the optimization.</p>
</td></tr>
<tr><td><code id="tune_nested_+3A_callbacks">callbacks</code></td>
<td>
<p>(list of <a href="#topic+CallbackTuning">CallbackTuning</a>)<br />
List of callbacks.</p>
</td></tr>
<tr><td><code id="tune_nested_+3A_method">method</code></td>
<td>
<p>(<code>character(1)</code>)<br />
Deprecated. Use <code>tuner</code> instead.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><a href="mlr3.html#topic+ResampleResult">mlr3::ResampleResult</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Nested resampling on Palmer Penguins data set
rr = tune_nested(
  tuner = tnr("random_search", batch_size = 2),
  task = tsk("penguins"),
  learner = lrn("classif.rpart", cp = to_tune(1e-04, 1e-1, logscale = TRUE)),
  inner_resampling = rsmp ("holdout"),
  outer_resampling = rsmp("cv", folds = 2),
  measure = msr("classif.ce"),
  term_evals = 2)

# Performance scores estimated on the outer resampling
rr$score()

# Unbiased performance of the final model trained on the full data set
rr$aggregate()
</code></pre>

<hr>
<h2 id='Tuner'>Class for Tuning Algorithms</h2><span id='topic+Tuner'></span>

<h3>Description</h3>

<p>The <a href="#topic+Tuner">Tuner</a> implements the optimization algorithm.
</p>


<h3>Details</h3>

<p><a href="#topic+Tuner">Tuner</a> is a abstract base class that implements the base functionality each tuner must provide.
A subclass is implemented in the following way:
</p>

<ul>
<li><p> Inherit from Tuner.
</p>
</li>
<li><p> Specify the private abstract method <code style="white-space: pre;">&#8288;$.optimize()&#8288;</code> and use it to call into your optimizer.
</p>
</li>
<li><p> You need to call <code>instance$eval_batch()</code> to evaluate design points.
</p>
</li>
<li><p> The batch evaluation is requested at the <a href="#topic+TuningInstanceSingleCrit">TuningInstanceSingleCrit</a>/<a href="#topic+TuningInstanceMultiCrit">TuningInstanceMultiCrit</a> object <code>instance</code>, so each batch is possibly executed in parallel via <code><a href="mlr3.html#topic+benchmark">mlr3::benchmark()</a></code>, and all evaluations are stored inside of <code>instance$archive</code>.
</p>
</li>
<li><p> Before the batch evaluation, the <a href="bbotk.html#topic+Terminator">bbotk::Terminator</a> is checked, and if it is positive, an exception of class <code>"terminated_error"</code> is generated.
In the  later case the current batch of evaluations is still stored in <code>instance</code>, but the numeric scores are not sent back to the handling optimizer as it has lost execution control.
</p>
</li>
<li><p> After such an exception was caught we select the best configuration from <code>instance$archive</code> and return it.
</p>
</li>
<li><p> Note that therefore more points than specified by the <a href="bbotk.html#topic+Terminator">bbotk::Terminator</a> may be evaluated, as the Terminator is only checked before a batch evaluation, and not in-between evaluation in a batch.
How many more depends on the setting of the batch size.
</p>
</li>
<li><p> Overwrite the private super-method <code>.assign_result()</code> if you want to decide yourself how to estimate the final configuration in the instance and its estimated performance.
The default behavior is: We pick the best resample-experiment, regarding the given measure, then assign its configuration and aggregated performance to the instance.
</p>
</li></ul>



<h3>Private Methods</h3>


<ul>
<li> <p><code>.optimize(instance)</code> -&gt; <code>NULL</code><br />
Abstract base method. Implement to specify tuning of your subclass.
See details sections.
</p>
</li>
<li> <p><code>.assign_result(instance)</code> -&gt; <code>NULL</code><br />
Abstract base method. Implement to specify how the final configuration is selected.
See details sections.
</p>
</li></ul>



<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li><p> Learn more about <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-tuner">tuners</a>.
</p>
</li></ul>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>

<ul>
<li><p> Use the <a href="https://mlr-org.com/gallery/series/2023-01-15-hyperband-xgboost/">Hyperband</a> optimizer with different budget parameters.
</p>
</li></ul>



<h3>Extension Packages</h3>

<p>Additional tuners are provided by the following packages.
</p>

<ul>
<li> <p><a href="https://github.com/mlr-org/mlr3hyperband">mlr3hyperband</a> adds the Hyperband and Successive Halving algorithm.
</p>
</li>
<li> <p><a href="https://github.com/mlr-org/mlr3mbo">mlr3mbo</a> adds Bayesian optimization methods.
</p>
</li></ul>



<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>id</code></dt><dd><p>(<code>character(1)</code>)<br />
Identifier of the object.
Used in tables, plot and text output.</p>
</dd>
</dl>

</div>


<h3>Active bindings</h3>

<div class="r6-active-bindings">

<dl>
<dt><code>param_set</code></dt><dd><p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>)<br />
Set of control parameters.</p>
</dd>
<dt><code>param_classes</code></dt><dd><p>(<code>character()</code>)<br />
Supported parameter classes for learner hyperparameters that the tuner can optimize, as given in the <a href="paradox.html#topic+ParamSet">paradox::ParamSet</a> <code style="white-space: pre;">&#8288;$class&#8288;</code> field.</p>
</dd>
<dt><code>properties</code></dt><dd><p>(<code>character()</code>)<br />
Set of properties of the tuner.
Must be a subset of <code><a href="mlr3.html#topic+mlr_reflections">mlr_reflections$tuner_properties</a></code>.</p>
</dd>
<dt><code>packages</code></dt><dd><p>(<code>character()</code>)<br />
Set of required packages.
Note that these packages will be loaded via <code><a href="base.html#topic+requireNamespace">requireNamespace()</a></code>, and are not attached.</p>
</dd>
<dt><code>label</code></dt><dd><p>(<code>character(1)</code>)<br />
Label for this object.
Can be used in tables, plot and text output instead of the ID.</p>
</dd>
<dt><code>man</code></dt><dd><p>(<code>character(1)</code>)<br />
String in the format <code style="white-space: pre;">&#8288;[pkg]::[topic]&#8288;</code> pointing to a manual page for this object.
The referenced help package can be opened via method <code style="white-space: pre;">&#8288;$help()&#8288;</code>.</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-Tuner-new"><code>Tuner$new()</code></a>
</p>
</li>
<li> <p><a href="#method-Tuner-format"><code>Tuner$format()</code></a>
</p>
</li>
<li> <p><a href="#method-Tuner-print"><code>Tuner$print()</code></a>
</p>
</li>
<li> <p><a href="#method-Tuner-help"><code>Tuner$help()</code></a>
</p>
</li>
<li> <p><a href="#method-Tuner-optimize"><code>Tuner$optimize()</code></a>
</p>
</li>
<li> <p><a href="#method-Tuner-clone"><code>Tuner$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-Tuner-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>Tuner$new(
  id = "tuner",
  param_set,
  param_classes,
  properties,
  packages = character(),
  label = NA_character_,
  man = NA_character_
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>id</code></dt><dd><p>(<code>character(1)</code>)<br />
Identifier for the new instance.</p>
</dd>
<dt><code>param_set</code></dt><dd><p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>)<br />
Set of control parameters.</p>
</dd>
<dt><code>param_classes</code></dt><dd><p>(<code>character()</code>)<br />
Supported parameter classes for learner hyperparameters that the tuner can optimize, as given in the <a href="paradox.html#topic+ParamSet">paradox::ParamSet</a> <code style="white-space: pre;">&#8288;$class&#8288;</code> field.</p>
</dd>
<dt><code>properties</code></dt><dd><p>(<code>character()</code>)<br />
Set of properties of the tuner.
Must be a subset of <code><a href="mlr3.html#topic+mlr_reflections">mlr_reflections$tuner_properties</a></code>.</p>
</dd>
<dt><code>packages</code></dt><dd><p>(<code>character()</code>)<br />
Set of required packages.
Note that these packages will be loaded via <code><a href="base.html#topic+requireNamespace">requireNamespace()</a></code>, and are not attached.</p>
</dd>
<dt><code>label</code></dt><dd><p>(<code>character(1)</code>)<br />
Label for this object.
Can be used in tables, plot and text output instead of the ID.</p>
</dd>
<dt><code>man</code></dt><dd><p>(<code>character(1)</code>)<br />
String in the format <code style="white-space: pre;">&#8288;[pkg]::[topic]&#8288;</code> pointing to a manual page for this object.
The referenced help package can be opened via method <code style="white-space: pre;">&#8288;$help()&#8288;</code>.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-Tuner-format"></a>



<h4>Method <code>format()</code></h4>

<p>Helper for print outputs.
</p>


<h5>Usage</h5>

<div class="r"><pre>Tuner$format(...)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>...</code></dt><dd><p>(ignored).</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>(<code>character()</code>).
</p>


<hr>
<a id="method-Tuner-print"></a>



<h4>Method <code>print()</code></h4>

<p>Print method.
</p>


<h5>Usage</h5>

<div class="r"><pre>Tuner$print()</pre></div>



<h5>Returns</h5>

<p>(<code>character()</code>).
</p>


<hr>
<a id="method-Tuner-help"></a>



<h4>Method <code>help()</code></h4>

<p>Opens the corresponding help page referenced by field <code style="white-space: pre;">&#8288;$man&#8288;</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>Tuner$help()</pre></div>


<hr>
<a id="method-Tuner-optimize"></a>



<h4>Method <code>optimize()</code></h4>

<p>Performs the tuning on a <a href="#topic+TuningInstanceSingleCrit">TuningInstanceSingleCrit</a> or <a href="#topic+TuningInstanceMultiCrit">TuningInstanceMultiCrit</a> until termination.
The single evaluations will be written into the <a href="#topic+ArchiveTuning">ArchiveTuning</a> that resides in the <a href="#topic+TuningInstanceSingleCrit">TuningInstanceSingleCrit</a>/<a href="#topic+TuningInstanceMultiCrit">TuningInstanceMultiCrit</a>.
The result will be written into the instance object.
</p>


<h5>Usage</h5>

<div class="r"><pre>Tuner$optimize(inst)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>inst</code></dt><dd><p>(<a href="#topic+TuningInstanceSingleCrit">TuningInstanceSingleCrit</a> | <a href="#topic+TuningInstanceMultiCrit">TuningInstanceMultiCrit</a>).</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p><code><a href="data.table.html#topic+data.table">data.table::data.table()</a></code>
</p>


<hr>
<a id="method-Tuner-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>Tuner$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='TunerFromOptimizer'>TunerFromOptimizer</h2><span id='topic+TunerFromOptimizer'></span>

<h3>Description</h3>

<p>Internally used to transform <a href="bbotk.html#topic+Optimizer">bbotk::Optimizer</a> to <a href="#topic+Tuner">Tuner</a>.
</p>


<h3>Super class</h3>

<p><code><a href="#topic+Tuner">mlr3tuning::Tuner</a></code> -&gt; <code>TunerFromOptimizer</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TunerFromOptimizer-new"><code>TunerFromOptimizer$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TunerFromOptimizer-optimize"><code>TunerFromOptimizer$optimize()</code></a>
</p>
</li>
<li> <p><a href="#method-TunerFromOptimizer-clone"><code>TunerFromOptimizer$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="format"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-format'><code>mlr3tuning::Tuner$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="help"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-help'><code>mlr3tuning::Tuner$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="print"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-print'><code>mlr3tuning::Tuner$print()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-TunerFromOptimizer-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerFromOptimizer$new(optimizer, man = NA_character_)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>optimizer</code></dt><dd><p><a href="bbotk.html#topic+Optimizer">bbotk::Optimizer</a><br />
Optimizer that is called.</p>
</dd>
<dt><code>man</code></dt><dd><p>(<code>character(1)</code>)<br />
String in the format <code style="white-space: pre;">&#8288;[pkg]::[topic]&#8288;</code> pointing to a manual page for this object.
The referenced help package can be opened via method <code style="white-space: pre;">&#8288;$help()&#8288;</code>.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-TunerFromOptimizer-optimize"></a>



<h4>Method <code>optimize()</code></h4>

<p>Performs the tuning on a <a href="#topic+TuningInstanceSingleCrit">TuningInstanceSingleCrit</a> /
<a href="#topic+TuningInstanceMultiCrit">TuningInstanceMultiCrit</a> until termination. The single evaluations and
the final results will be written into the <a href="#topic+ArchiveTuning">ArchiveTuning</a> that
resides in the <a href="#topic+TuningInstanceSingleCrit">TuningInstanceSingleCrit</a>/<a href="#topic+TuningInstanceMultiCrit">TuningInstanceMultiCrit</a>.
The final result is returned.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerFromOptimizer$optimize(inst)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>inst</code></dt><dd><p>(<a href="#topic+TuningInstanceSingleCrit">TuningInstanceSingleCrit</a> | <a href="#topic+TuningInstanceMultiCrit">TuningInstanceMultiCrit</a>).</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p><a href="data.table.html#topic+data.table">data.table::data.table</a>.
</p>


<hr>
<a id="method-TunerFromOptimizer-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerFromOptimizer$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='TuningInstanceMultiCrit'>Class for Multi Criteria Tuning</h2><span id='topic+TuningInstanceMultiCrit'></span>

<h3>Description</h3>

<p>The <a href="#topic+TuningInstanceMultiCrit">TuningInstanceMultiCrit</a> specifies a tuning problem for <a href="#topic+Tuner">Tuners</a>.
The function <code><a href="#topic+ti">ti()</a></code> creates a <a href="#topic+TuningInstanceMultiCrit">TuningInstanceMultiCrit</a> and the function <code><a href="#topic+tune">tune()</a></code> creates an instance internally.
</p>


<h3>Details</h3>

<p>The instance contains an <a href="#topic+ObjectiveTuning">ObjectiveTuning</a> object that encodes the black box objective function a <a href="#topic+Tuner">Tuner</a> has to optimize.
The instance allows the basic operations of querying the objective at design points (<code style="white-space: pre;">&#8288;$eval_batch()&#8288;</code>).
This operation is usually done by the <a href="#topic+Tuner">Tuner</a>.
Evaluations of hyperparameter configurations are performed in batches by calling <code><a href="mlr3.html#topic+benchmark">mlr3::benchmark()</a></code> internally.
The evaluated hyperparameter configurations are stored in the <a href="#topic+ArchiveTuning">Archive</a> (<code style="white-space: pre;">&#8288;$archive&#8288;</code>).
Before a batch is evaluated, the <a href="bbotk.html#topic+Terminator">bbotk::Terminator</a> is queried for the remaining budget.
If the available budget is exhausted, an exception is raised, and no further evaluations can be performed from this point on.
The tuner is also supposed to store its final result, consisting of a  selected hyperparameter configuration and associated estimated performance values, by calling the method <code>instance$assign_result</code>.
</p>


<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li><p> Learn about <a href="https://mlr3book.mlr-org.com/chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html#sec-multi-metrics-tuning">multi-objective optimization</a>.
</p>
</li></ul>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>


<h3>Analysis</h3>

<p>For analyzing the tuning results, it is recommended to pass the <a href="#topic+ArchiveTuning">ArchiveTuning</a> to <code>as.data.table()</code>.
The returned data table is joined with the benchmark result which adds the <a href="mlr3.html#topic+ResampleResult">mlr3::ResampleResult</a> for each hyperparameter evaluation.
</p>
<p>The archive provides various getters (e.g. <code style="white-space: pre;">&#8288;$learners()&#8288;</code>) to ease the access.
All getters extract by position (<code>i</code>) or unique hash (<code>uhash</code>).
For a complete list of all getters see the methods section.
</p>
<p>The benchmark result (<code style="white-space: pre;">&#8288;$benchmark_result&#8288;</code>) allows to score the hyperparameter configurations again on a different measure.
Alternatively, measures can be supplied to <code>as.data.table()</code>.
</p>
<p>The <a href="https://CRAN.R-project.org/package=mlr3viz"><span class="pkg">mlr3viz</span></a> package provides visualizations for tuning results.
</p>


<h3>Super classes</h3>

<p><code><a href="bbotk.html#topic+OptimInstance">bbotk::OptimInstance</a></code> -&gt; <code><a href="bbotk.html#topic+OptimInstanceMultiCrit">bbotk::OptimInstanceMultiCrit</a></code> -&gt; <code>TuningInstanceMultiCrit</code>
</p>


<h3>Active bindings</h3>

<div class="r6-active-bindings">

<dl>
<dt><code>result_learner_param_vals</code></dt><dd><p>(<code>list()</code>)<br />
List of param values for the optimal learner call.</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TuningInstanceMultiCrit-new"><code>TuningInstanceMultiCrit$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TuningInstanceMultiCrit-assign_result"><code>TuningInstanceMultiCrit$assign_result()</code></a>
</p>
</li>
<li> <p><a href="#method-TuningInstanceMultiCrit-clone"><code>TuningInstanceMultiCrit$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstance" data-id="clear"><a href='../../bbotk/html/OptimInstance.html#method-OptimInstance-clear'><code>bbotk::OptimInstance$clear()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstance" data-id="eval_batch"><a href='../../bbotk/html/OptimInstance.html#method-OptimInstance-eval_batch'><code>bbotk::OptimInstance$eval_batch()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstance" data-id="format"><a href='../../bbotk/html/OptimInstance.html#method-OptimInstance-format'><code>bbotk::OptimInstance$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstance" data-id="objective_function"><a href='../../bbotk/html/OptimInstance.html#method-OptimInstance-objective_function'><code>bbotk::OptimInstance$objective_function()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstance" data-id="print"><a href='../../bbotk/html/OptimInstance.html#method-OptimInstance-print'><code>bbotk::OptimInstance$print()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-TuningInstanceMultiCrit-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TuningInstanceMultiCrit$new(
  task,
  learner,
  resampling,
  measures,
  terminator,
  search_space = NULL,
  store_benchmark_result = TRUE,
  store_models = FALSE,
  check_values = FALSE,
  allow_hotstart = FALSE,
  keep_hotstart_stack = FALSE,
  evaluate_default = FALSE,
  callbacks = list()
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>task</code></dt><dd><p>(<a href="mlr3.html#topic+Task">mlr3::Task</a>)<br />
Task to operate on.</p>
</dd>
<dt><code>learner</code></dt><dd><p>(<a href="mlr3.html#topic+Learner">mlr3::Learner</a>)<br />
Learner to tune.</p>
</dd>
<dt><code>resampling</code></dt><dd><p>(<a href="mlr3.html#topic+Resampling">mlr3::Resampling</a>)<br />
Resampling that is used to evaluate the performance of the hyperparameter configurations.
Uninstantiated resamplings are instantiated during construction so that all configurations are evaluated on the same data splits.
Already instantiated resamplings are kept unchanged.
Specialized <a href="#topic+Tuner">Tuner</a> change the resampling e.g. to evaluate a hyperparameter configuration on different data splits.
This field, however, always returns the resampling passed in construction.</p>
</dd>
<dt><code>measures</code></dt><dd><p>(list of <a href="mlr3.html#topic+Measure">mlr3::Measure</a>)<br />
Measures to optimize.</p>
</dd>
<dt><code>terminator</code></dt><dd><p>(<a href="bbotk.html#topic+Terminator">Terminator</a>)<br />
Stop criterion of the tuning process.</p>
</dd>
<dt><code>search_space</code></dt><dd><p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>)<br />
Hyperparameter search space. If <code>NULL</code> (default), the search space is
constructed from the <a href="paradox.html#topic+TuneToken">TuneToken</a> of the learner's parameter set
(learner$param_set).</p>
</dd>
<dt><code>store_benchmark_result</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default), store resample result of evaluated hyperparameter
configurations in archive as <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>.</p>
</dd>
<dt><code>store_models</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, fitted models are stored in the benchmark result
(<code>archive$benchmark_result</code>). If <code>store_benchmark_result = FALSE</code>, models
are only stored temporarily and not accessible after the tuning. This
combination is needed for measures that require a model.</p>
</dd>
<dt><code>check_values</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, hyperparameter values are checked before evaluation and
performance scores after. If <code>FALSE</code> (default), values are unchecked but
computational overhead is reduced.</p>
</dd>
<dt><code>allow_hotstart</code></dt><dd><p>(<code>logical(1)</code>)<br />
Allow to hotstart learners with previously fitted models. See also
<a href="mlr3.html#topic+HotstartStack">mlr3::HotstartStack</a>. The learner must support hotstarting. Sets
<code>store_models = TRUE</code>.</p>
</dd>
<dt><code>keep_hotstart_stack</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, <a href="mlr3.html#topic+HotstartStack">mlr3::HotstartStack</a> is kept in <code style="white-space: pre;">&#8288;$objective$hotstart_stack&#8288;</code>
after tuning.</p>
</dd>
<dt><code>evaluate_default</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, learner is evaluated with hyperparameters set to their default
values at the start of the optimization.</p>
</dd>
<dt><code>callbacks</code></dt><dd><p>(list of <a href="#topic+CallbackTuning">CallbackTuning</a>)<br />
List of callbacks.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-TuningInstanceMultiCrit-assign_result"></a>



<h4>Method <code>assign_result()</code></h4>

<p>The <a href="#topic+Tuner">Tuner</a> object writes the best found points and estimated performance values here.
For internal use.
</p>


<h5>Usage</h5>

<div class="r"><pre>TuningInstanceMultiCrit$assign_result(xdt, ydt, learner_param_vals = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>xdt</code></dt><dd><p>(<code>data.table::data.table()</code>)<br />
Hyperparameter values as <code>data.table::data.table()</code>. Each row is one
configuration. Contains values in the search space. Can contain additional
columns for extra information.</p>
</dd>
<dt><code>ydt</code></dt><dd><p>(<code>data.table::data.table()</code>)<br />
Optimal outcomes, e.g. the Pareto front.</p>
</dd>
<dt><code>learner_param_vals</code></dt><dd><p>(List of named <code style="white-space: pre;">&#8288;list()s&#8288;</code>)<br />
Fixed parameter values of the learner that are neither part of the</p>
</dd>
</dl>

</div>


<hr>
<a id="method-TuningInstanceMultiCrit-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TuningInstanceMultiCrit$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'># Hyperparameter optimization on the Palmer Penguins data set
task = tsk("penguins")

# Load learner and set search space
learner = lrn("classif.rpart",
  cp = to_tune(1e-04, 1e-1, logscale = TRUE)
)

# Construct tuning instance
instance = ti(
  task = task,
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msrs(c("classif.ce", "time_train")),
  terminator = trm("evals", n_evals = 4)
)

# Choose optimization algorithm
tuner = tnr("random_search", batch_size = 2)

# Run tuning
tuner$optimize(instance)

# Optimal hyperparameter configurations
instance$result

# Inspect all evaluated configurations
as.data.table(instance$archive)
</code></pre>

<hr>
<h2 id='TuningInstanceSingleCrit'>Class for Single Criterion Tuning</h2><span id='topic+TuningInstanceSingleCrit'></span>

<h3>Description</h3>

<p>The <a href="#topic+TuningInstanceSingleCrit">TuningInstanceSingleCrit</a> specifies a tuning problem for <a href="#topic+Tuner">Tuners</a>.
The function <code><a href="#topic+ti">ti()</a></code> creates a <a href="#topic+TuningInstanceSingleCrit">TuningInstanceSingleCrit</a> and the function <code><a href="#topic+tune">tune()</a></code> creates an instance internally.
</p>


<h3>Details</h3>

<p>The instance contains an <a href="#topic+ObjectiveTuning">ObjectiveTuning</a> object that encodes the black box objective function a <a href="#topic+Tuner">Tuner</a> has to optimize.
The instance allows the basic operations of querying the objective at design points (<code style="white-space: pre;">&#8288;$eval_batch()&#8288;</code>).
This operation is usually done by the <a href="#topic+Tuner">Tuner</a>.
Evaluations of hyperparameter configurations are performed in batches by calling <code><a href="mlr3.html#topic+benchmark">mlr3::benchmark()</a></code> internally.
The evaluated hyperparameter configurations are stored in the <a href="#topic+ArchiveTuning">Archive</a> (<code style="white-space: pre;">&#8288;$archive&#8288;</code>).
Before a batch is evaluated, the <a href="bbotk.html#topic+Terminator">bbotk::Terminator</a> is queried for the remaining budget.
If the available budget is exhausted, an exception is raised, and no further evaluations can be performed from this point on.
The tuner is also supposed to store its final result, consisting of a  selected hyperparameter configuration and associated estimated performance values, by calling the method <code>instance$assign_result</code>.
</p>


<h3>Default Measures</h3>

<p>If no measure is passed, the default measure is used.
The default measure depends on the task type.</p>

<table>
<tr>
 <td style="text-align: left;">
   Task </td><td style="text-align: left;"> Default Measure </td><td style="text-align: left;"> Package </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"classif"</code> </td><td style="text-align: left;"> <code>"classif.ce"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3"><span class="pkg">mlr3</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"regr"</code> </td><td style="text-align: left;"> <code>"regr.mse"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3"><span class="pkg">mlr3</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"surv"</code> </td><td style="text-align: left;"> <code>"surv.cindex"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3proba"><span class="pkg">mlr3proba</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"dens"</code> </td><td style="text-align: left;"> <code>"dens.logloss"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3proba"><span class="pkg">mlr3proba</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"classif_st"</code> </td><td style="text-align: left;"> <code>"classif.ce"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3spatial"><span class="pkg">mlr3spatial</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"regr_st"</code> </td><td style="text-align: left;"> <code>"regr.mse"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3spatial"><span class="pkg">mlr3spatial</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"clust"</code> </td><td style="text-align: left;"> <code>"clust.dunn"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3cluster"><span class="pkg">mlr3cluster</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li><p> Getting started with <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html">hyperparameter optimization</a>.
</p>
</li>
<li> <p><a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-model-tuning">Tune</a> a simple classification tree on the Sonar data set.
</p>
</li>
<li><p> Learn about <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-defining-search-spaces">tuning spaces</a>.
</p>
</li></ul>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>

<ul>
<li><p> Learn more advanced methods with the <a href="https://mlr-org.com/gallery/series/2021-03-09-practical-tuning-series-tune-a-support-vector-machine/">practical tuning series</a>.
</p>
</li>
<li><p> Simultaneously optimize hyperparameters and use <a href="https://mlr-org.com/gallery/optimization/2022-11-04-early-stopping-with-xgboost/">early stopping</a> with XGBoost.
</p>
</li>
<li><p> Make us of proven <a href="https://mlr-org.com/gallery/optimization/2021-07-06-introduction-to-mlr3tuningspaces/">search space</a>.
</p>
</li>
<li><p> Learn about <a href="https://mlr-org.com/gallery/optimization/2023-01-16-hotstart/">hotstarting</a> models.
</p>
</li>
<li><p> Run the <a href="https://mlr-org.com/gallery/optimization/2023-01-31-default-configuration/">default hyperparameter configuration</a> of learners as a baseline.
</p>
</li></ul>



<h3>Extension Packages</h3>

<p>mlr3tuning is extended by the following packages.
</p>

<ul>
<li> <p><a href="https://github.com/mlr-org/mlr3tuningspaces">mlr3tuningspaces</a> is a collection of search spaces from scientific articles for commonly used learners.
</p>
</li>
<li> <p><a href="https://github.com/mlr-org/mlr3hyperband">mlr3hyperband</a> adds the Hyperband and Successive Halving algorithm.
</p>
</li>
<li> <p><a href="https://github.com/mlr-org/mlr3mbo">mlr3mbo</a> adds Bayesian optimization methods.
</p>
</li></ul>



<h3>Analysis</h3>

<p>For analyzing the tuning results, it is recommended to pass the <a href="#topic+ArchiveTuning">ArchiveTuning</a> to <code>as.data.table()</code>.
The returned data table is joined with the benchmark result which adds the <a href="mlr3.html#topic+ResampleResult">mlr3::ResampleResult</a> for each hyperparameter evaluation.
</p>
<p>The archive provides various getters (e.g. <code style="white-space: pre;">&#8288;$learners()&#8288;</code>) to ease the access.
All getters extract by position (<code>i</code>) or unique hash (<code>uhash</code>).
For a complete list of all getters see the methods section.
</p>
<p>The benchmark result (<code style="white-space: pre;">&#8288;$benchmark_result&#8288;</code>) allows to score the hyperparameter configurations again on a different measure.
Alternatively, measures can be supplied to <code>as.data.table()</code>.
</p>
<p>The <a href="https://CRAN.R-project.org/package=mlr3viz"><span class="pkg">mlr3viz</span></a> package provides visualizations for tuning results.
</p>


<h3>Super classes</h3>

<p><code><a href="bbotk.html#topic+OptimInstance">bbotk::OptimInstance</a></code> -&gt; <code><a href="bbotk.html#topic+OptimInstanceSingleCrit">bbotk::OptimInstanceSingleCrit</a></code> -&gt; <code>TuningInstanceSingleCrit</code>
</p>


<h3>Active bindings</h3>

<div class="r6-active-bindings">

<dl>
<dt><code>result_learner_param_vals</code></dt><dd><p>(<code>list()</code>)<br />
Param values for the optimal learner call.</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TuningInstanceSingleCrit-new"><code>TuningInstanceSingleCrit$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TuningInstanceSingleCrit-assign_result"><code>TuningInstanceSingleCrit$assign_result()</code></a>
</p>
</li>
<li> <p><a href="#method-TuningInstanceSingleCrit-clone"><code>TuningInstanceSingleCrit$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstance" data-id="clear"><a href='../../bbotk/html/OptimInstance.html#method-OptimInstance-clear'><code>bbotk::OptimInstance$clear()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstance" data-id="eval_batch"><a href='../../bbotk/html/OptimInstance.html#method-OptimInstance-eval_batch'><code>bbotk::OptimInstance$eval_batch()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstance" data-id="format"><a href='../../bbotk/html/OptimInstance.html#method-OptimInstance-format'><code>bbotk::OptimInstance$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstance" data-id="objective_function"><a href='../../bbotk/html/OptimInstance.html#method-OptimInstance-objective_function'><code>bbotk::OptimInstance$objective_function()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstance" data-id="print"><a href='../../bbotk/html/OptimInstance.html#method-OptimInstance-print'><code>bbotk::OptimInstance$print()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-TuningInstanceSingleCrit-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TuningInstanceSingleCrit$new(
  task,
  learner,
  resampling,
  measure = NULL,
  terminator,
  search_space = NULL,
  store_benchmark_result = TRUE,
  store_models = FALSE,
  check_values = FALSE,
  allow_hotstart = FALSE,
  keep_hotstart_stack = FALSE,
  evaluate_default = FALSE,
  callbacks = list()
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>task</code></dt><dd><p>(<a href="mlr3.html#topic+Task">mlr3::Task</a>)<br />
Task to operate on.</p>
</dd>
<dt><code>learner</code></dt><dd><p>(<a href="mlr3.html#topic+Learner">mlr3::Learner</a>)<br />
Learner to tune.</p>
</dd>
<dt><code>resampling</code></dt><dd><p>(<a href="mlr3.html#topic+Resampling">mlr3::Resampling</a>)<br />
Resampling that is used to evaluate the performance of the hyperparameter configurations.
Uninstantiated resamplings are instantiated during construction so that all configurations are evaluated on the same data splits.
Already instantiated resamplings are kept unchanged.
Specialized <a href="#topic+Tuner">Tuner</a> change the resampling e.g. to evaluate a hyperparameter configuration on different data splits.
This field, however, always returns the resampling passed in construction.</p>
</dd>
<dt><code>measure</code></dt><dd><p>(<a href="mlr3.html#topic+Measure">mlr3::Measure</a>)<br />
Measure to optimize. If <code>NULL</code>, default measure is used.</p>
</dd>
<dt><code>terminator</code></dt><dd><p>(<a href="bbotk.html#topic+Terminator">Terminator</a>)<br />
Stop criterion of the tuning process.</p>
</dd>
<dt><code>search_space</code></dt><dd><p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>)<br />
Hyperparameter search space. If <code>NULL</code> (default), the search space is
constructed from the <a href="paradox.html#topic+TuneToken">TuneToken</a> of the learner's parameter set
(learner$param_set).</p>
</dd>
<dt><code>store_benchmark_result</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default), store resample result of evaluated hyperparameter
configurations in archive as <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>.</p>
</dd>
<dt><code>store_models</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, fitted models are stored in the benchmark result
(<code>archive$benchmark_result</code>). If <code>store_benchmark_result = FALSE</code>, models
are only stored temporarily and not accessible after the tuning. This
combination is needed for measures that require a model.</p>
</dd>
<dt><code>check_values</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, hyperparameter values are checked before evaluation and
performance scores after. If <code>FALSE</code> (default), values are unchecked but
computational overhead is reduced.</p>
</dd>
<dt><code>allow_hotstart</code></dt><dd><p>(<code>logical(1)</code>)<br />
Allow to hotstart learners with previously fitted models. See also
<a href="mlr3.html#topic+HotstartStack">mlr3::HotstartStack</a>. The learner must support hotstarting. Sets
<code>store_models = TRUE</code>.</p>
</dd>
<dt><code>keep_hotstart_stack</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, <a href="mlr3.html#topic+HotstartStack">mlr3::HotstartStack</a> is kept in <code style="white-space: pre;">&#8288;$objective$hotstart_stack&#8288;</code>
after tuning.</p>
</dd>
<dt><code>evaluate_default</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, learner is evaluated with hyperparameters set to their default
values at the start of the optimization.</p>
</dd>
<dt><code>callbacks</code></dt><dd><p>(list of <a href="#topic+CallbackTuning">CallbackTuning</a>)<br />
List of callbacks.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-TuningInstanceSingleCrit-assign_result"></a>



<h4>Method <code>assign_result()</code></h4>

<p>The <a href="#topic+Tuner">Tuner</a> object writes the best found point and estimated performance value here.
For internal use.
</p>


<h5>Usage</h5>

<div class="r"><pre>TuningInstanceSingleCrit$assign_result(xdt, y, learner_param_vals = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>xdt</code></dt><dd><p>(<code>data.table::data.table()</code>)<br />
Hyperparameter values as <code>data.table::data.table()</code>. Each row is one
configuration. Contains values in the search space. Can contain additional
columns for extra information.</p>
</dd>
<dt><code>y</code></dt><dd><p>(<code>numeric(1)</code>)<br />
Optimal outcome.</p>
</dd>
<dt><code>learner_param_vals</code></dt><dd><p>(List of named <code style="white-space: pre;">&#8288;list()s&#8288;</code>)<br />
Fixed parameter values of the learner that are neither part of the</p>
</dd>
</dl>

</div>


<hr>
<a id="method-TuningInstanceSingleCrit-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TuningInstanceSingleCrit$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'># Hyperparameter optimization on the Palmer Penguins data set
task = tsk("penguins")

# Load learner and set search space
learner = lrn("classif.rpart",
  cp = to_tune(1e-04, 1e-1, logscale = TRUE)
)

# Construct tuning instance
instance = ti(
  task = task,
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  terminator = trm("evals", n_evals = 4)
)

# Choose optimization algorithm
tuner = tnr("random_search", batch_size = 2)

# Run tuning
tuner$optimize(instance)

# Set optimal hyperparameter configuration to learner
learner$param_set$values = instance$result_learner_param_vals

# Train the learner on the full data set
learner$train(task)

# Inspect all evaluated configurations
as.data.table(instance$archive)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
