<!DOCTYPE html><html><head><title>Help for package mlr3tuning</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {mlr3tuning}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#mlr3tuning-package'><p>mlr3tuning: Hyperparameter Optimization for 'mlr3'</p></a></li>
<li><a href='#ArchiveAsyncTuning'><p>Rush Data Storage</p></a></li>
<li><a href='#ArchiveBatchTuning'><p>Class for Logging Evaluated Hyperparameter Configurations</p></a></li>
<li><a href='#as_search_space'><p>Convert to a Search Space</p></a></li>
<li><a href='#as_tuner'><p>Convert to a Tuner</p></a></li>
<li><a href='#auto_tuner'><p>Function for Automatic Tuning</p></a></li>
<li><a href='#AutoTuner'><p>Class for Automatic Tuning</p></a></li>
<li><a href='#callback_async_tuning'><p>Create Asynchronous Tuning Callback</p></a></li>
<li><a href='#callback_batch_tuning'><p>Create Batch Tuning Callback</p></a></li>
<li><a href='#CallbackAsyncTuning'><p>Create Asynchronous Tuning Callback</p></a></li>
<li><a href='#CallbackBatchTuning'><p>Create Batch Tuning Callback</p></a></li>
<li><a href='#ContextAsyncTuning'><p>Asynchronous Tuning Context</p></a></li>
<li><a href='#ContextBatchTuning'><p>Batch Tuning Context</p></a></li>
<li><a href='#extract_inner_tuning_archives'><p>Extract Inner Tuning Archives</p></a></li>
<li><a href='#extract_inner_tuning_results'><p>Extract Inner Tuning Results</p></a></li>
<li><a href='#mlr_tuners'><p>Dictionary of Tuners</p></a></li>
<li><a href='#mlr_tuners_async_design_points'><p>Hyperparameter Tuning with Asynchronous Design Points</p></a></li>
<li><a href='#mlr_tuners_async_grid_search'><p>Hyperparameter Tuning with Asynchronous Grid Search</p></a></li>
<li><a href='#mlr_tuners_async_random_search'><p>Hyperparameter Tuning with Asynchronous Random Search</p></a></li>
<li><a href='#mlr_tuners_cmaes'><p>Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy</p></a></li>
<li><a href='#mlr_tuners_design_points'><p>Hyperparameter Tuning with Design Points</p></a></li>
<li><a href='#mlr_tuners_gensa'><p>Hyperparameter Tuning with Generalized Simulated Annealing</p></a></li>
<li><a href='#mlr_tuners_grid_search'><p>Hyperparameter Tuning with Grid Search</p></a></li>
<li><a href='#mlr_tuners_internal'><p>Hyperparameter Tuning with Internal Tuning</p></a></li>
<li><a href='#mlr_tuners_irace'><p>Hyperparameter Tuning with Iterated Racing.</p></a></li>
<li><a href='#mlr_tuners_nloptr'><p>Hyperparameter Tuning with Non-linear Optimization</p></a></li>
<li><a href='#mlr_tuners_random_search'><p>Hyperparameter Tuning with Random Search</p></a></li>
<li><a href='#mlr3tuning_assertions'><p>Assertion for mlr3tuning objects</p></a></li>
<li><a href='#mlr3tuning.asnyc_mlflow'><p>MLflow Connector Callback</p></a></li>
<li><a href='#mlr3tuning.async_default_configuration'><p>Default Configuration Callback</p></a></li>
<li><a href='#mlr3tuning.async_save_logs'><p>Save Logs Callback</p></a></li>
<li><a href='#mlr3tuning.backup'><p>Backup Benchmark Result Callback</p></a></li>
<li><a href='#mlr3tuning.measures'><p>Measure Callback</p></a></li>
<li><a href='#ObjectiveTuning'><p>Class for Tuning Objective</p></a></li>
<li><a href='#ObjectiveTuningAsync'><p>Class for Tuning Objective</p></a></li>
<li><a href='#ObjectiveTuningBatch'><p>Class for Tuning Objective</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#set_validate.AutoTuner'><p>Configure Validation for AutoTuner</p></a></li>
<li><a href='#ti'><p>Syntactic Sugar for Tuning Instance Construction</p></a></li>
<li><a href='#ti_async'><p>Syntactic Sugar for Asynchronous Tuning Instance Construction</p></a></li>
<li><a href='#tnr'><p>Syntactic Sugar for Tuning Objects Construction</p></a></li>
<li><a href='#tune'><p>Function for Tuning a Learner</p></a></li>
<li><a href='#tune_nested'><p>Function for Nested Resampling</p></a></li>
<li><a href='#Tuner'><p>Tuner</p></a></li>
<li><a href='#TunerAsync'><p>Class for Asynchronous Tuning Algorithms</p></a></li>
<li><a href='#TunerAsyncFromOptimizerAsync'><p>TunerAsyncFromOptimizerAsync</p></a></li>
<li><a href='#TunerBatch'><p>Class for Batch Tuning Algorithms</p></a></li>
<li><a href='#TunerBatchFromOptimizerBatch'><p>TunerBatchFromOptimizerBatch</p></a></li>
<li><a href='#TuningInstanceAsyncMultiCrit'><p>Multi-Criteria Tuning with Rush</p></a></li>
<li><a href='#TuningInstanceAsyncSingleCrit'><p>Single Criterion Tuning with Rush</p></a></li>
<li><a href='#TuningInstanceBatchMultiCrit'><p>Class for Multi Criteria Tuning</p></a></li>
<li><a href='#TuningInstanceBatchSingleCrit'><p>Class for Single Criterion Tuning</p></a></li>
<li><a href='#TuningInstanceMultiCrit'><p>Multi Criteria Tuning Instance for Batch Tuning</p></a></li>
<li><a href='#TuningInstanceSingleCrit'><p>Single Criterion Tuning Instance for Batch Tuning</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Hyperparameter Optimization for 'mlr3'</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Hyperparameter optimization package of the 'mlr3' ecosystem.
    It features highly configurable search spaces via the 'paradox'
    package and finds optimal hyperparameter configurations for any 'mlr3'
    learner.  'mlr3tuning' works with several optimization algorithms e.g.
    Random Search, Iterated Racing, Bayesian Optimization (in 'mlr3mbo')
    and Hyperband (in 'mlr3hyperband'). Moreover, it can automatically
    optimize learners and estimate the performance of optimized models
    with nested resampling.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/LGPL-3">LGPL-3</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://mlr3tuning.mlr-org.com">https://mlr3tuning.mlr-org.com</a>,
<a href="https://github.com/mlr-org/mlr3tuning">https://github.com/mlr-org/mlr3tuning</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/mlr-org/mlr3tuning/issues">https://github.com/mlr-org/mlr3tuning/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>mlr3 (&ge; 0.20.0), paradox (&ge; 1.0.0), R (&ge; 3.1.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>bbotk (&ge; 1.0.0), checkmate (&ge; 2.0.0), data.table, lgr,
mlr3misc (&ge; 0.15.1), R6</td>
</tr>
<tr>
<td>Suggests:</td>
<td>adagio, future, GenSA, irace, knitr, mlflow, mlr3learners (&ge;
0.7.0), mlr3pipelines (&ge; 0.5.2), nloptr, rush, rmarkdown,
rpart, testthat (&ge; 3.0.0), xgboost</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Config/testthat/parallel:</td>
<td>false</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>Collate:</td>
<td>'ArchiveAsyncTuning.R' 'ArchiveBatchTuning.R' 'AutoTuner.R'
'CallbackAsyncTuning.R' 'CallbackBatchTuning.R'
'ContextAsyncTuning.R' 'ContextBatchTuning.R'
'ObjectiveTuning.R' 'ObjectiveTuningAsync.R'
'ObjectiveTuningBatch.R' 'mlr_tuners.R' 'Tuner.R'
'TunerAsync.R' 'TunerAsyncDesignPoints.R'
'TunerAsyncFromOptimizerAsync.R' 'TunerAsyncGridSearch.R'
'TunerAsyncRandomSearch.R' 'TunerBatch.R' 'TunerBatchCmaes.R'
'TunerBatchDesignPoints.R' 'TunerBatchFromBatchOptimizer.R'
'TunerBatchGenSA.R' 'TunerBatchGridSearch.R'
'TunerBatchInternal.R' 'TunerBatchIrace.R' 'TunerBatchNLoptr.R'
'TunerBatchRandomSearch.R' 'TuningInstanceBatchSingleCrit.R'
'TuningInstanceAsyncMulticrit.R'
'TuningInstanceAsyncSingleCrit.R'
'TuningInstanceBatchMulticrit.R' 'TuningInstanceMultiCrit.R'
'TuningInstanceSingleCrit.R' 'as_search_space.R' 'as_tuner.R'
'assertions.R' 'auto_tuner.R' 'bibentries.R'
'extract_inner_tuning_archives.R'
'extract_inner_tuning_results.R' 'helper.R' 'mlr_callbacks.R'
'reexport.R' 'sugar.R' 'tune.R' 'tune_nested.R' 'zzz.R'</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-06-28 17:32:04 UTC; marc</td>
</tr>
<tr>
<td>Author:</td>
<td>Marc Becker <a href="https://orcid.org/0000-0002-8115-0400"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [cre,
    aut],
  Michel Lang <a href="https://orcid.org/0000-0001-9754-0393"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Jakob Richter <a href="https://orcid.org/0000-0003-4481-5554"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Bernd Bischl <a href="https://orcid.org/0000-0001-6002-6980"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Daniel Schalk <a href="https://orcid.org/0000-0003-0950-1947"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Marc Becker &lt;marcbecker@posteo.de&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-06-29 06:40:11 UTC</td>
</tr>
</table>
<hr>
<h2 id='mlr3tuning-package'>mlr3tuning: Hyperparameter Optimization for 'mlr3'</h2><span id='topic+mlr3tuning'></span><span id='topic+mlr3tuning-package'></span>

<h3>Description</h3>

<p><img src="../help/figures/logo.png" style='float: right' alt='logo' width='120' />
</p>
<p>Hyperparameter optimization package of the 'mlr3' ecosystem. It features highly configurable search spaces via the 'paradox' package and finds optimal hyperparameter configurations for any 'mlr3' learner. 'mlr3tuning' works with several optimization algorithms e.g. Random Search, Iterated Racing, Bayesian Optimization (in 'mlr3mbo') and Hyperband (in 'mlr3hyperband'). Moreover, it can automatically optimize learners and estimate the performance of optimized models with nested resampling.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Marc Becker <a href="mailto:marcbecker@posteo.de">marcbecker@posteo.de</a> (<a href="https://orcid.org/0000-0002-8115-0400">ORCID</a>)
</p>
<p>Authors:
</p>

<ul>
<li><p> Michel Lang <a href="mailto:michellang@gmail.com">michellang@gmail.com</a> (<a href="https://orcid.org/0000-0001-9754-0393">ORCID</a>)
</p>
</li>
<li><p> Jakob Richter <a href="mailto:jakob1richter@gmail.com">jakob1richter@gmail.com</a> (<a href="https://orcid.org/0000-0003-4481-5554">ORCID</a>)
</p>
</li>
<li><p> Bernd Bischl <a href="mailto:bernd_bischl@gmx.net">bernd_bischl@gmx.net</a> (<a href="https://orcid.org/0000-0001-6002-6980">ORCID</a>)
</p>
</li>
<li><p> Daniel Schalk <a href="mailto:daniel.schalk@stat.uni-muenchen.de">daniel.schalk@stat.uni-muenchen.de</a> (<a href="https://orcid.org/0000-0003-0950-1947">ORCID</a>)
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://mlr3tuning.mlr-org.com">https://mlr3tuning.mlr-org.com</a>
</p>
</li>
<li> <p><a href="https://github.com/mlr-org/mlr3tuning">https://github.com/mlr-org/mlr3tuning</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/mlr-org/mlr3tuning/issues">https://github.com/mlr-org/mlr3tuning/issues</a>
</p>
</li></ul>


<hr>
<h2 id='ArchiveAsyncTuning'>Rush Data Storage</h2><span id='topic+ArchiveAsyncTuning'></span>

<h3>Description</h3>

<p>The 'ArchiveAsyncTuning&ldquo; stores all evaluated hyperparameter configurations and performance scores in a <a href="rush.html#topic+Rush">rush::Rush</a> database.
</p>


<h3>Details</h3>

<p>The <a href="#topic+ArchiveAsyncTuning">ArchiveAsyncTuning</a> is a connector to a <a href="rush.html#topic+Rush">rush::Rush</a> database.
</p>


<h3>Data Structure</h3>

<p>The table (<code style="white-space: pre;">&#8288;$data&#8288;</code>) has the following columns:
</p>

<ul>
<li><p> One column for each hyperparameter of the search space (<code style="white-space: pre;">&#8288;$search_space&#8288;</code>).
</p>
</li>
<li><p> One (list-)column for the <code>internal_tuned_values</code>
</p>
</li>
<li><p> One column for each performance measure (<code style="white-space: pre;">&#8288;$codomain&#8288;</code>).
</p>
</li>
<li> <p><code>x_domain</code> (<code>list()</code>)<br />
Lists of (transformed) hyperparameter values that are passed to the learner.
</p>
</li>
<li> <p><code>runtime_learners</code> (<code>numeric(1)</code>)<br />
Sum of training and predict times logged in learners per <a href="mlr3.html#topic+ResampleResult">mlr3::ResampleResult</a> / evaluation.
This does not include potential overhead time.
</p>
</li>
<li> <p><code>timestamp</code> (<code>POSIXct</code>)<br />
Time stamp when the evaluation was logged into the archive.
</p>
</li>
<li> <p><code>batch_nr</code> (<code>integer(1)</code>)<br />
Hyperparameters are evaluated in batches.
Each batch has a unique batch number.
</p>
</li></ul>



<h3>Analysis</h3>

<p>For analyzing the tuning results, it is recommended to pass the <a href="#topic+ArchiveAsyncTuning">ArchiveAsyncTuning</a> to <code>as.data.table()</code>.
The returned data table contains the <a href="mlr3.html#topic+ResampleResult">mlr3::ResampleResult</a> for each hyperparameter evaluation.
</p>


<h3>S3 Methods</h3>


<ul>
<li> <p><code>as.data.table.ArchiveTuning(x, unnest = "x_domain", exclude_columns = "uhash", measures = NULL)</code><br />
Returns a tabular view of all evaluated hyperparameter configurations.<br />
<a href="#topic+ArchiveAsyncTuning">ArchiveAsyncTuning</a> -&gt; <code><a href="data.table.html#topic+data.table">data.table::data.table()</a></code><br />
</p>

<ul>
<li> <p><code>x</code> (<a href="#topic+ArchiveAsyncTuning">ArchiveAsyncTuning</a>)
</p>
</li>
<li> <p><code>unnest</code> (<code>character()</code>)<br />
Transforms list columns to separate columns. Set to <code>NULL</code> if no column should be unnested.
</p>
</li>
<li> <p><code>exclude_columns</code> (<code>character()</code>)<br />
Exclude columns from table. Set to <code>NULL</code> if no column should be excluded.
</p>
</li>
<li> <p><code>measures</code> (List of <a href="mlr3.html#topic+Measure">mlr3::Measure</a>)<br />
Score hyperparameter configurations on additional measures.
</p>
</li></ul>

</li></ul>



<h3>Super classes</h3>

<p><code><a href="bbotk.html#topic+Archive">bbotk::Archive</a></code> -&gt; <code><a href="bbotk.html#topic+ArchiveAsync">bbotk::ArchiveAsync</a></code> -&gt; <code>ArchiveAsyncTuning</code>
</p>


<h3>Active bindings</h3>

<div class="r6-active-bindings">

<dl>
<dt><code>internal_search_space</code></dt><dd><p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>)<br />
The search space containing those parameters that are internally optimized by the <code><a href="mlr3.html#topic+Learner">mlr3::Learner</a></code>.</p>
</dd>
<dt><code>benchmark_result</code></dt><dd><p>(<a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>)<br />
Benchmark result.</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-ArchiveAsyncTuning-new"><code>ArchiveAsyncTuning$new()</code></a>
</p>
</li>
<li> <p><a href="#method-ArchiveAsyncTuning-learner"><code>ArchiveAsyncTuning$learner()</code></a>
</p>
</li>
<li> <p><a href="#method-ArchiveAsyncTuning-learners"><code>ArchiveAsyncTuning$learners()</code></a>
</p>
</li>
<li> <p><a href="#method-ArchiveAsyncTuning-learner_param_vals"><code>ArchiveAsyncTuning$learner_param_vals()</code></a>
</p>
</li>
<li> <p><a href="#method-ArchiveAsyncTuning-predictions"><code>ArchiveAsyncTuning$predictions()</code></a>
</p>
</li>
<li> <p><a href="#method-ArchiveAsyncTuning-resample_result"><code>ArchiveAsyncTuning$resample_result()</code></a>
</p>
</li>
<li> <p><a href="#method-ArchiveAsyncTuning-print"><code>ArchiveAsyncTuning$print()</code></a>
</p>
</li>
<li> <p><a href="#method-ArchiveAsyncTuning-clone"><code>ArchiveAsyncTuning$clone()</code></a>
</p>
</li></ul>



<details><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Archive" data-id="format"><a href='../../bbotk/html/Archive.html#method-Archive-format'><code>bbotk::Archive$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Archive" data-id="help"><a href='../../bbotk/html/Archive.html#method-Archive-help'><code>bbotk::Archive$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="ArchiveAsync" data-id="best"><a href='../../bbotk/html/ArchiveAsync.html#method-ArchiveAsync-best'><code>bbotk::ArchiveAsync$best()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="ArchiveAsync" data-id="clear"><a href='../../bbotk/html/ArchiveAsync.html#method-ArchiveAsync-clear'><code>bbotk::ArchiveAsync$clear()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="ArchiveAsync" data-id="data_with_state"><a href='../../bbotk/html/ArchiveAsync.html#method-ArchiveAsync-data_with_state'><code>bbotk::ArchiveAsync$data_with_state()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="ArchiveAsync" data-id="nds_selection"><a href='../../bbotk/html/ArchiveAsync.html#method-ArchiveAsync-nds_selection'><code>bbotk::ArchiveAsync$nds_selection()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="ArchiveAsync" data-id="pop_point"><a href='../../bbotk/html/ArchiveAsync.html#method-ArchiveAsync-pop_point'><code>bbotk::ArchiveAsync$pop_point()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="ArchiveAsync" data-id="push_failed_point"><a href='../../bbotk/html/ArchiveAsync.html#method-ArchiveAsync-push_failed_point'><code>bbotk::ArchiveAsync$push_failed_point()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="ArchiveAsync" data-id="push_points"><a href='../../bbotk/html/ArchiveAsync.html#method-ArchiveAsync-push_points'><code>bbotk::ArchiveAsync$push_points()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="ArchiveAsync" data-id="push_result"><a href='../../bbotk/html/ArchiveAsync.html#method-ArchiveAsync-push_result'><code>bbotk::ArchiveAsync$push_result()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="ArchiveAsync" data-id="push_running_point"><a href='../../bbotk/html/ArchiveAsync.html#method-ArchiveAsync-push_running_point'><code>bbotk::ArchiveAsync$push_running_point()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-ArchiveAsyncTuning-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>ArchiveAsyncTuning$new(
  search_space,
  codomain,
  rush,
  internal_search_space = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>search_space</code></dt><dd><p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>)<br />
Hyperparameter search space. If <code>NULL</code> (default), the search space is
constructed from the <a href="paradox.html#topic+to_tune">paradox::TuneToken</a> of the learner's parameter set
(learner$param_set).</p>
</dd>
<dt><code>codomain</code></dt><dd><p>(<a href="bbotk.html#topic+Codomain">bbotk::Codomain</a>)<br />
Specifies codomain of objective function i.e. a set of performance measures.
Internally created from provided <a href="mlr3.html#topic+Measure">mlr3::Measure</a>s.</p>
</dd>
<dt><code>rush</code></dt><dd><p>(<code>Rush</code>)<br />
If a rush instance is supplied, the tuning runs without batches.</p>
</dd>
<dt><code>internal_search_space</code></dt><dd><p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a> or <code>NULL</code>)<br />
The internal search space of the tuner. This includes parameters that the learner can optimize internally
durign <code style="white-space: pre;">&#8288;$train()&#8288;</code>, such as the number of epochs via early stopping.</p>
</dd>
<dt><code>check_values</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default), hyperparameter configurations are check for validity.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-ArchiveAsyncTuning-learner"></a>



<h4>Method <code>learner()</code></h4>

<p>Retrieve <a href="mlr3.html#topic+Learner">mlr3::Learner</a> of the i-th evaluation, by position or by unique hash <code>uhash</code>.
<code>i</code> and <code>uhash</code> are mutually exclusive.
Learner does not contain a model. Use <code style="white-space: pre;">&#8288;$learners()&#8288;</code> to get learners with models.
</p>


<h5>Usage</h5>

<div class="r"><pre>ArchiveAsyncTuning$learner(i = NULL, uhash = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>i</code></dt><dd><p>(<code>integer(1)</code>)<br />
The iteration value to filter for.</p>
</dd>
<dt><code>uhash</code></dt><dd><p>(<code>logical(1)</code>)<br />
The <code>uhash</code> value to filter for.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-ArchiveAsyncTuning-learners"></a>



<h4>Method <code>learners()</code></h4>

<p>Retrieve list of trained <a href="mlr3.html#topic+Learner">mlr3::Learner</a> objects of the i-th evaluation, by position or by unique hash <code>uhash</code>.
<code>i</code> and <code>uhash</code> are mutually exclusive.
</p>


<h5>Usage</h5>

<div class="r"><pre>ArchiveAsyncTuning$learners(i = NULL, uhash = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>i</code></dt><dd><p>(<code>integer(1)</code>)<br />
The iteration value to filter for.</p>
</dd>
<dt><code>uhash</code></dt><dd><p>(<code>logical(1)</code>)<br />
The <code>uhash</code> value to filter for.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-ArchiveAsyncTuning-learner_param_vals"></a>



<h4>Method <code>learner_param_vals()</code></h4>

<p>Retrieve param values of the i-th evaluation, by position or by unique hash <code>uhash</code>.
<code>i</code> and <code>uhash</code> are mutually exclusive.
</p>


<h5>Usage</h5>

<div class="r"><pre>ArchiveAsyncTuning$learner_param_vals(i = NULL, uhash = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>i</code></dt><dd><p>(<code>integer(1)</code>)<br />
The iteration value to filter for.</p>
</dd>
<dt><code>uhash</code></dt><dd><p>(<code>logical(1)</code>)<br />
The <code>uhash</code> value to filter for.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-ArchiveAsyncTuning-predictions"></a>



<h4>Method <code>predictions()</code></h4>

<p>Retrieve list of <a href="mlr3.html#topic+Prediction">mlr3::Prediction</a> objects of the i-th evaluation, by position or by unique hash <code>uhash</code>.
<code>i</code> and <code>uhash</code> are mutually exclusive.
</p>


<h5>Usage</h5>

<div class="r"><pre>ArchiveAsyncTuning$predictions(i = NULL, uhash = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>i</code></dt><dd><p>(<code>integer(1)</code>)<br />
The iteration value to filter for.</p>
</dd>
<dt><code>uhash</code></dt><dd><p>(<code>logical(1)</code>)<br />
The <code>uhash</code> value to filter for.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-ArchiveAsyncTuning-resample_result"></a>



<h4>Method <code>resample_result()</code></h4>

<p>Retrieve <a href="mlr3.html#topic+ResampleResult">mlr3::ResampleResult</a> of the i-th evaluation, by position or by unique hash <code>uhash</code>.
<code>i</code> and <code>uhash</code> are mutually exclusive.
</p>


<h5>Usage</h5>

<div class="r"><pre>ArchiveAsyncTuning$resample_result(i = NULL, uhash = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>i</code></dt><dd><p>(<code>integer(1)</code>)<br />
The iteration value to filter for.</p>
</dd>
<dt><code>uhash</code></dt><dd><p>(<code>logical(1)</code>)<br />
The <code>uhash</code> value to filter for.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-ArchiveAsyncTuning-print"></a>



<h4>Method <code>print()</code></h4>

<p>Printer.
</p>


<h5>Usage</h5>

<div class="r"><pre>ArchiveAsyncTuning$print()</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>...</code></dt><dd><p>(ignored).</p>
</dd>
</dl>

</div>


<hr>
<a id="method-ArchiveAsyncTuning-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>ArchiveAsyncTuning$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='ArchiveBatchTuning'>Class for Logging Evaluated Hyperparameter Configurations</h2><span id='topic+ArchiveBatchTuning'></span>

<h3>Description</h3>

<p>The <code>ArchiveBatchTuning</code> stores all evaluated hyperparameter configurations and performance scores in a <code><a href="data.table.html#topic+data.table">data.table::data.table()</a></code>.
</p>


<h3>Details</h3>

<p>The <a href="#topic+ArchiveBatchTuning">ArchiveBatchTuning</a> is a container around a <code><a href="data.table.html#topic+data.table">data.table::data.table()</a></code>.
Each row corresponds to a single evaluation of a hyperparameter configuration.
See the section on Data Structure for more information.
The archive stores additionally a <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a> (<code style="white-space: pre;">&#8288;$benchmark_result&#8288;</code>) that records the resampling experiments.
Each experiment corresponds to to a single evaluation of a hyperparameter configuration.
The table (<code style="white-space: pre;">&#8288;$data&#8288;</code>) and the benchmark result (<code style="white-space: pre;">&#8288;$benchmark_result&#8288;</code>) are linked by the <code>uhash</code> column.
If the archive is passed to <code>as.data.table()</code>, both are joined automatically.
</p>


<h3>Data Structure</h3>

<p>The table (<code style="white-space: pre;">&#8288;$data&#8288;</code>) has the following columns:
</p>

<ul>
<li><p> One column for each hyperparameter of the search space (<code style="white-space: pre;">&#8288;$search_space&#8288;</code>).
</p>
</li>
<li><p> One (list-)column for the <code>internal_tuned_values</code>
</p>
</li>
<li><p> One column for each performance measure (<code style="white-space: pre;">&#8288;$codomain&#8288;</code>).
</p>
</li>
<li> <p><code>x_domain</code> (<code>list()</code>)<br />
Lists of (transformed) hyperparameter values that are passed to the learner.
</p>
</li>
<li> <p><code>runtime_learners</code> (<code>numeric(1)</code>)<br />
Sum of training and predict times logged in learners per <a href="mlr3.html#topic+ResampleResult">mlr3::ResampleResult</a> / evaluation.
This does not include potential overhead time.
</p>
</li>
<li> <p><code>timestamp</code> (<code>POSIXct</code>)<br />
Time stamp when the evaluation was logged into the archive.
</p>
</li>
<li> <p><code>batch_nr</code> (<code>integer(1)</code>)<br />
Hyperparameters are evaluated in batches.
Each batch has a unique batch number.
</p>
</li>
<li> <p><code>uhash</code> (<code>character(1)</code>)<br />
Connects each hyperparameter configuration to the resampling experiment stored in the <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>.
</p>
</li></ul>



<h3>Analysis</h3>

<p>For analyzing the tuning results, it is recommended to pass the <a href="#topic+ArchiveBatchTuning">ArchiveBatchTuning</a> to <code>as.data.table()</code>.
The returned data table is joined with the benchmark result which adds the <a href="mlr3.html#topic+ResampleResult">mlr3::ResampleResult</a> for each hyperparameter evaluation.
</p>
<p>The archive provides various getters (e.g. <code style="white-space: pre;">&#8288;$learners()&#8288;</code>) to ease the access.
All getters extract by position (<code>i</code>) or unique hash (<code>uhash</code>).
For a complete list of all getters see the methods section.
</p>
<p>The benchmark result (<code style="white-space: pre;">&#8288;$benchmark_result&#8288;</code>) allows to score the hyperparameter configurations again on a different measure.
Alternatively, measures can be supplied to <code>as.data.table()</code>.
</p>
<p>The <a href="https://CRAN.R-project.org/package=mlr3viz"><span class="pkg">mlr3viz</span></a> package provides visualizations for tuning results.
</p>


<h3>S3 Methods</h3>


<ul>
<li> <p><code>as.data.table.ArchiveTuning(x, unnest = "x_domain", exclude_columns = "uhash", measures = NULL)</code><br />
Returns a tabular view of all evaluated hyperparameter configurations.<br />
<a href="#topic+ArchiveBatchTuning">ArchiveBatchTuning</a> -&gt; <code><a href="data.table.html#topic+data.table">data.table::data.table()</a></code><br />
</p>

<ul>
<li> <p><code>x</code> (<a href="#topic+ArchiveBatchTuning">ArchiveBatchTuning</a>)
</p>
</li>
<li> <p><code>unnest</code> (<code>character()</code>)<br />
Transforms list columns to separate columns. Set to <code>NULL</code> if no column should be unnested.
</p>
</li>
<li> <p><code>exclude_columns</code> (<code>character()</code>)<br />
Exclude columns from table. Set to <code>NULL</code> if no column should be excluded.
</p>
</li>
<li> <p><code>measures</code> (List of <a href="mlr3.html#topic+Measure">mlr3::Measure</a>)<br />
Score hyperparameter configurations on additional measures.
</p>
</li></ul>

</li></ul>



<h3>Super classes</h3>

<p><code><a href="bbotk.html#topic+Archive">bbotk::Archive</a></code> -&gt; <code><a href="bbotk.html#topic+ArchiveBatch">bbotk::ArchiveBatch</a></code> -&gt; <code>ArchiveBatchTuning</code>
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>benchmark_result</code></dt><dd><p>(<a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>)<br />
Benchmark result.</p>
</dd>
</dl>

</div>


<h3>Active bindings</h3>

<div class="r6-active-bindings">

<dl>
<dt><code>internal_search_space</code></dt><dd><p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>)<br />
The search space containing those parameters that are internally optimized by the <code><a href="mlr3.html#topic+Learner">mlr3::Learner</a></code>.</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-ArchiveBatchTuning-new"><code>ArchiveBatchTuning$new()</code></a>
</p>
</li>
<li> <p><a href="#method-ArchiveBatchTuning-learner"><code>ArchiveBatchTuning$learner()</code></a>
</p>
</li>
<li> <p><a href="#method-ArchiveBatchTuning-learners"><code>ArchiveBatchTuning$learners()</code></a>
</p>
</li>
<li> <p><a href="#method-ArchiveBatchTuning-learner_param_vals"><code>ArchiveBatchTuning$learner_param_vals()</code></a>
</p>
</li>
<li> <p><a href="#method-ArchiveBatchTuning-predictions"><code>ArchiveBatchTuning$predictions()</code></a>
</p>
</li>
<li> <p><a href="#method-ArchiveBatchTuning-resample_result"><code>ArchiveBatchTuning$resample_result()</code></a>
</p>
</li>
<li> <p><a href="#method-ArchiveBatchTuning-print"><code>ArchiveBatchTuning$print()</code></a>
</p>
</li>
<li> <p><a href="#method-ArchiveBatchTuning-clone"><code>ArchiveBatchTuning$clone()</code></a>
</p>
</li></ul>



<details><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Archive" data-id="format"><a href='../../bbotk/html/Archive.html#method-Archive-format'><code>bbotk::Archive$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Archive" data-id="help"><a href='../../bbotk/html/Archive.html#method-Archive-help'><code>bbotk::Archive$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="ArchiveBatch" data-id="add_evals"><a href='../../bbotk/html/ArchiveBatch.html#method-ArchiveBatch-add_evals'><code>bbotk::ArchiveBatch$add_evals()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="ArchiveBatch" data-id="best"><a href='../../bbotk/html/ArchiveBatch.html#method-ArchiveBatch-best'><code>bbotk::ArchiveBatch$best()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="ArchiveBatch" data-id="clear"><a href='../../bbotk/html/ArchiveBatch.html#method-ArchiveBatch-clear'><code>bbotk::ArchiveBatch$clear()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="ArchiveBatch" data-id="nds_selection"><a href='../../bbotk/html/ArchiveBatch.html#method-ArchiveBatch-nds_selection'><code>bbotk::ArchiveBatch$nds_selection()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-ArchiveBatchTuning-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>ArchiveBatchTuning$new(
  search_space,
  codomain,
  check_values = FALSE,
  internal_search_space = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>search_space</code></dt><dd><p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>)<br />
Hyperparameter search space. If <code>NULL</code> (default), the search space is
constructed from the <a href="paradox.html#topic+to_tune">paradox::TuneToken</a> of the learner's parameter set
(learner$param_set).</p>
</dd>
<dt><code>codomain</code></dt><dd><p>(<a href="bbotk.html#topic+Codomain">bbotk::Codomain</a>)<br />
Specifies codomain of objective function i.e. a set of performance measures.
Internally created from provided <a href="mlr3.html#topic+Measure">mlr3::Measure</a>s.</p>
</dd>
<dt><code>check_values</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default), hyperparameter configurations are check for validity.</p>
</dd>
<dt><code>internal_search_space</code></dt><dd><p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a> or <code>NULL</code>)<br />
The internal search space of the tuner. This includes parameters that the learner can optimize internally
durign <code style="white-space: pre;">&#8288;$train()&#8288;</code>, such as the number of epochs via early stopping.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-ArchiveBatchTuning-learner"></a>



<h4>Method <code>learner()</code></h4>

<p>Retrieve <a href="mlr3.html#topic+Learner">mlr3::Learner</a> of the i-th evaluation, by position or by unique hash <code>uhash</code>.
<code>i</code> and <code>uhash</code> are mutually exclusive.
Learner does not contain a model. Use <code style="white-space: pre;">&#8288;$learners()&#8288;</code> to get learners with models.
</p>


<h5>Usage</h5>

<div class="r"><pre>ArchiveBatchTuning$learner(i = NULL, uhash = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>i</code></dt><dd><p>(<code>integer(1)</code>)<br />
The iteration value to filter for.</p>
</dd>
<dt><code>uhash</code></dt><dd><p>(<code>logical(1)</code>)<br />
The <code>uhash</code> value to filter for.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-ArchiveBatchTuning-learners"></a>



<h4>Method <code>learners()</code></h4>

<p>Retrieve list of trained <a href="mlr3.html#topic+Learner">mlr3::Learner</a> objects of the i-th evaluation, by position or by unique hash <code>uhash</code>.
<code>i</code> and <code>uhash</code> are mutually exclusive.
</p>


<h5>Usage</h5>

<div class="r"><pre>ArchiveBatchTuning$learners(i = NULL, uhash = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>i</code></dt><dd><p>(<code>integer(1)</code>)<br />
The iteration value to filter for.</p>
</dd>
<dt><code>uhash</code></dt><dd><p>(<code>logical(1)</code>)<br />
The <code>uhash</code> value to filter for.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-ArchiveBatchTuning-learner_param_vals"></a>



<h4>Method <code>learner_param_vals()</code></h4>

<p>Retrieve param values of the i-th evaluation, by position or by unique hash <code>uhash</code>.
<code>i</code> and <code>uhash</code> are mutually exclusive.
</p>


<h5>Usage</h5>

<div class="r"><pre>ArchiveBatchTuning$learner_param_vals(i = NULL, uhash = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>i</code></dt><dd><p>(<code>integer(1)</code>)<br />
The iteration value to filter for.</p>
</dd>
<dt><code>uhash</code></dt><dd><p>(<code>logical(1)</code>)<br />
The <code>uhash</code> value to filter for.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-ArchiveBatchTuning-predictions"></a>



<h4>Method <code>predictions()</code></h4>

<p>Retrieve list of <a href="mlr3.html#topic+Prediction">mlr3::Prediction</a> objects of the i-th evaluation, by position or by unique hash <code>uhash</code>.
<code>i</code> and <code>uhash</code> are mutually exclusive.
</p>


<h5>Usage</h5>

<div class="r"><pre>ArchiveBatchTuning$predictions(i = NULL, uhash = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>i</code></dt><dd><p>(<code>integer(1)</code>)<br />
The iteration value to filter for.</p>
</dd>
<dt><code>uhash</code></dt><dd><p>(<code>logical(1)</code>)<br />
The <code>uhash</code> value to filter for.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-ArchiveBatchTuning-resample_result"></a>



<h4>Method <code>resample_result()</code></h4>

<p>Retrieve <a href="mlr3.html#topic+ResampleResult">mlr3::ResampleResult</a> of the i-th evaluation, by position or by unique hash <code>uhash</code>.
<code>i</code> and <code>uhash</code> are mutually exclusive.
</p>


<h5>Usage</h5>

<div class="r"><pre>ArchiveBatchTuning$resample_result(i = NULL, uhash = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>i</code></dt><dd><p>(<code>integer(1)</code>)<br />
The iteration value to filter for.</p>
</dd>
<dt><code>uhash</code></dt><dd><p>(<code>logical(1)</code>)<br />
The <code>uhash</code> value to filter for.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-ArchiveBatchTuning-print"></a>



<h4>Method <code>print()</code></h4>

<p>Printer.
</p>


<h5>Usage</h5>

<div class="r"><pre>ArchiveBatchTuning$print()</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>...</code></dt><dd><p>(ignored).</p>
</dd>
</dl>

</div>


<hr>
<a id="method-ArchiveBatchTuning-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>ArchiveBatchTuning$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='as_search_space'>Convert to a Search Space</h2><span id='topic+as_search_space'></span><span id='topic+as_search_space.Learner'></span><span id='topic+as_search_space.ParamSet'></span>

<h3>Description</h3>

<p>Convert object to a search space.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_search_space(x, ...)

## S3 method for class 'Learner'
as_search_space(x, ...)

## S3 method for class 'ParamSet'
as_search_space(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as_search_space_+3A_x">x</code></td>
<td>
<p>(<code>any</code>)<br />
Object to convert to search space.</p>
</td></tr>
<tr><td><code id="as_search_space_+3A_...">...</code></td>
<td>
<p>(any)<br />
Additional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>.
</p>

<hr>
<h2 id='as_tuner'>Convert to a Tuner</h2><span id='topic+as_tuner'></span><span id='topic+as_tuner.Tuner'></span><span id='topic+as_tuners'></span><span id='topic+as_tuners.default'></span><span id='topic+as_tuners.list'></span>

<h3>Description</h3>

<p>Convert object to a <a href="#topic+Tuner">Tuner</a> or a list of <a href="#topic+Tuner">Tuner</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_tuner(x, ...)

## S3 method for class 'Tuner'
as_tuner(x, clone = FALSE, ...)

as_tuners(x, ...)

## Default S3 method:
as_tuners(x, ...)

## S3 method for class 'list'
as_tuners(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as_tuner_+3A_x">x</code></td>
<td>
<p>(any)<br />
Object to convert.</p>
</td></tr>
<tr><td><code id="as_tuner_+3A_...">...</code></td>
<td>
<p>(any)<br />
Additional arguments.</p>
</td></tr>
<tr><td><code id="as_tuner_+3A_clone">clone</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
Whether to clone the object.</p>
</td></tr>
</table>

<hr>
<h2 id='auto_tuner'>Function for Automatic Tuning</h2><span id='topic+auto_tuner'></span>

<h3>Description</h3>

<p>The <a href="#topic+AutoTuner">AutoTuner</a> wraps a <a href="mlr3.html#topic+Learner">mlr3::Learner</a> and augments it with an automatic tuning process for a given set of hyperparameters.
The <code><a href="#topic+auto_tuner">auto_tuner()</a></code> function creates an <a href="#topic+AutoTuner">AutoTuner</a> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>auto_tuner(
  tuner,
  learner,
  resampling,
  measure = NULL,
  term_evals = NULL,
  term_time = NULL,
  terminator = NULL,
  search_space = NULL,
  store_tuning_instance = TRUE,
  store_benchmark_result = TRUE,
  store_models = FALSE,
  check_values = FALSE,
  callbacks = NULL,
  validate = NULL,
  rush = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="auto_tuner_+3A_tuner">tuner</code></td>
<td>
<p>(<a href="#topic+Tuner">Tuner</a>)<br />
Optimization algorithm.</p>
</td></tr>
<tr><td><code id="auto_tuner_+3A_learner">learner</code></td>
<td>
<p>(<a href="mlr3.html#topic+Learner">mlr3::Learner</a>)<br />
Learner to tune.</p>
</td></tr>
<tr><td><code id="auto_tuner_+3A_resampling">resampling</code></td>
<td>
<p>(<a href="mlr3.html#topic+Resampling">mlr3::Resampling</a>)<br />
Resampling that is used to evaluate the performance of the hyperparameter configurations.
Uninstantiated resamplings are instantiated during construction so that all configurations are evaluated on the same data splits.
Already instantiated resamplings are kept unchanged.
Specialized <a href="#topic+Tuner">Tuner</a> change the resampling e.g. to evaluate a hyperparameter configuration on different data splits.
This field, however, always returns the resampling passed in construction.</p>
</td></tr>
<tr><td><code id="auto_tuner_+3A_measure">measure</code></td>
<td>
<p>(<a href="mlr3.html#topic+Measure">mlr3::Measure</a>)<br />
Measure to optimize. If <code>NULL</code>, default measure is used.</p>
</td></tr>
<tr><td><code id="auto_tuner_+3A_term_evals">term_evals</code></td>
<td>
<p>(<code>integer(1)</code>)<br />
Number of allowed evaluations.
Ignored if <code>terminator</code> is passed.</p>
</td></tr>
<tr><td><code id="auto_tuner_+3A_term_time">term_time</code></td>
<td>
<p>(<code>integer(1)</code>)<br />
Maximum allowed time in seconds.
Ignored if <code>terminator</code> is passed.</p>
</td></tr>
<tr><td><code id="auto_tuner_+3A_terminator">terminator</code></td>
<td>
<p>(<a href="bbotk.html#topic+Terminator">bbotk::Terminator</a>)<br />
Stop criterion of the tuning process.</p>
</td></tr>
<tr><td><code id="auto_tuner_+3A_search_space">search_space</code></td>
<td>
<p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>)<br />
Hyperparameter search space. If <code>NULL</code> (default), the search space is
constructed from the <a href="paradox.html#topic+to_tune">paradox::TuneToken</a> of the learner's parameter set
(learner$param_set).</p>
</td></tr>
<tr><td><code id="auto_tuner_+3A_store_tuning_instance">store_tuning_instance</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default), stores the internally created <a href="#topic+TuningInstanceBatchSingleCrit">TuningInstanceBatchSingleCrit</a> with all intermediate results in slot <code style="white-space: pre;">&#8288;$tuning_instance&#8288;</code>.</p>
</td></tr>
<tr><td><code id="auto_tuner_+3A_store_benchmark_result">store_benchmark_result</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default), store resample result of evaluated hyperparameter
configurations in archive as <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>.</p>
</td></tr>
<tr><td><code id="auto_tuner_+3A_store_models">store_models</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, fitted models are stored in the benchmark result
(<code>archive$benchmark_result</code>). If <code>store_benchmark_result = FALSE</code>, models
are only stored temporarily and not accessible after the tuning. This
combination is needed for measures that require a model.</p>
</td></tr>
<tr><td><code id="auto_tuner_+3A_check_values">check_values</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, hyperparameter values are checked before evaluation and
performance scores after. If <code>FALSE</code> (default), values are unchecked but
computational overhead is reduced.</p>
</td></tr>
<tr><td><code id="auto_tuner_+3A_callbacks">callbacks</code></td>
<td>
<p>(list of <a href="mlr3misc.html#topic+Callback">mlr3misc::Callback</a>)<br />
List of callbacks.</p>
</td></tr>
<tr><td><code id="auto_tuner_+3A_validate">validate</code></td>
<td>
<p>(<code>numeric(1)</code>, <code>"test"</code>, <code>"predefined"</code> or <code>NULL</code>)<br />
How to construct the internal validation data.</p>
</td></tr>
<tr><td><code id="auto_tuner_+3A_rush">rush</code></td>
<td>
<p>(<code>Rush</code>)<br />
If a rush instance is supplied, the tuning runs without batches.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <a href="#topic+AutoTuner">AutoTuner</a> is a <a href="mlr3.html#topic+Learner">mlr3::Learner</a> which wraps another <a href="mlr3.html#topic+Learner">mlr3::Learner</a> and performs the following steps during <code style="white-space: pre;">&#8288;$train()&#8288;</code>:
</p>

<ol>
<li><p> The hyperparameters of the wrapped (inner) learner are trained on the training data via resampling.
The tuning can be specified by providing a <a href="#topic+Tuner">Tuner</a>, a <a href="bbotk.html#topic+Terminator">bbotk::Terminator</a>, a search space as <a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>, a <a href="mlr3.html#topic+Resampling">mlr3::Resampling</a> and a <a href="mlr3.html#topic+Measure">mlr3::Measure</a>.
</p>
</li>
<li><p> The best found hyperparameter configuration is set as hyperparameters for the wrapped (inner) learner stored in <code>at$learner</code>.
Access the tuned hyperparameters via <code>at$tuning_result</code>.
</p>
</li>
<li><p> A final model is fit on the complete training data using the now parametrized wrapped learner.
The respective model is available via field <code>at$learner$model</code>.
</p>
</li></ol>

<p>During <code style="white-space: pre;">&#8288;$predict()&#8288;</code> the <code>AutoTuner</code> just calls the predict method of the wrapped (inner) learner.
A set timeout is disabled while fitting the final model.
</p>


<h3>Value</h3>

<p><a href="#topic+AutoTuner">AutoTuner</a>.
</p>


<h3>Default Measures</h3>

<p>If no measure is passed, the default measure is used.
The default measure depends on the task type.</p>

<table>
<tr>
 <td style="text-align: left;">
   Task </td><td style="text-align: left;"> Default Measure </td><td style="text-align: left;"> Package </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"classif"</code> </td><td style="text-align: left;"> <code>"classif.ce"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3"><span class="pkg">mlr3</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"regr"</code> </td><td style="text-align: left;"> <code>"regr.mse"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3"><span class="pkg">mlr3</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"surv"</code> </td><td style="text-align: left;"> <code>"surv.cindex"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3proba"><span class="pkg">mlr3proba</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"dens"</code> </td><td style="text-align: left;"> <code>"dens.logloss"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3proba"><span class="pkg">mlr3proba</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"classif_st"</code> </td><td style="text-align: left;"> <code>"classif.ce"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3spatial"><span class="pkg">mlr3spatial</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"regr_st"</code> </td><td style="text-align: left;"> <code>"regr.mse"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3spatial"><span class="pkg">mlr3spatial</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"clust"</code> </td><td style="text-align: left;"> <code>"clust.dunn"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3cluster"><span class="pkg">mlr3cluster</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li> <p><a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-autotuner">Automate</a> the tuning.
</p>
</li>
<li><p> Estimate the model performance with <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-nested-resampling">nested resampling</a>.
</p>
</li></ul>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>


<h3>Nested Resampling</h3>

<p>Nested resampling is performed by passing an <a href="#topic+AutoTuner">AutoTuner</a> to <code><a href="mlr3.html#topic+resample">mlr3::resample()</a></code> or <code><a href="mlr3.html#topic+benchmark">mlr3::benchmark()</a></code>.
To access the inner resampling results, set <code>store_tuning_instance = TRUE</code> and execute <code><a href="mlr3.html#topic+resample">mlr3::resample()</a></code> or <code><a href="mlr3.html#topic+benchmark">mlr3::benchmark()</a></code> with <code>store_models = TRUE</code> (see examples).
The <a href="mlr3.html#topic+Resampling">mlr3::Resampling</a> passed to the <a href="#topic+AutoTuner">AutoTuner</a> is meant to be the inner resampling, operating on the training set of an arbitrary outer resampling.
For this reason, the inner resampling should be not instantiated.
If an instantiated resampling is passed, the <a href="#topic+AutoTuner">AutoTuner</a> fails when a row id of the inner resampling is not present in the training set of the outer resampling.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>at = auto_tuner(
  tuner = tnr("random_search"),
  learner = lrn("classif.rpart", cp = to_tune(1e-04, 1e-1, logscale = TRUE)),
  resampling = rsmp ("holdout"),
  measure = msr("classif.ce"),
  term_evals = 4)

at$train(tsk("pima"))
</code></pre>

<hr>
<h2 id='AutoTuner'>Class for Automatic Tuning</h2><span id='topic+AutoTuner'></span>

<h3>Description</h3>

<p>The <a href="#topic+AutoTuner">AutoTuner</a> wraps a <a href="mlr3.html#topic+Learner">mlr3::Learner</a> and augments it with an automatic tuning process for a given set of hyperparameters.
The <code><a href="#topic+auto_tuner">auto_tuner()</a></code> function creates an <a href="#topic+AutoTuner">AutoTuner</a> object.
</p>


<h3>Details</h3>

<p>The <a href="#topic+AutoTuner">AutoTuner</a> is a <a href="mlr3.html#topic+Learner">mlr3::Learner</a> which wraps another <a href="mlr3.html#topic+Learner">mlr3::Learner</a> and performs the following steps during <code style="white-space: pre;">&#8288;$train()&#8288;</code>:
</p>

<ol>
<li><p> The hyperparameters of the wrapped (inner) learner are trained on the training data via resampling.
The tuning can be specified by providing a <a href="#topic+Tuner">Tuner</a>, a <a href="bbotk.html#topic+Terminator">bbotk::Terminator</a>, a search space as <a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>, a <a href="mlr3.html#topic+Resampling">mlr3::Resampling</a> and a <a href="mlr3.html#topic+Measure">mlr3::Measure</a>.
</p>
</li>
<li><p> The best found hyperparameter configuration is set as hyperparameters for the wrapped (inner) learner stored in <code>at$learner</code>.
Access the tuned hyperparameters via <code>at$tuning_result</code>.
</p>
</li>
<li><p> A final model is fit on the complete training data using the now parametrized wrapped learner.
The respective model is available via field <code>at$learner$model</code>.
</p>
</li></ol>

<p>During <code style="white-space: pre;">&#8288;$predict()&#8288;</code> the <code>AutoTuner</code> just calls the predict method of the wrapped (inner) learner.
A set timeout is disabled while fitting the final model.
</p>


<h3>Validation</h3>

<p>Both, the tuned <a href="mlr3.html#topic+Learner">mlr3::Learner</a> and the <code>AutoTuner</code> itself can make use of validation data.
the <code style="white-space: pre;">&#8288;$validate&#8288;</code> field of the <code>AutoTuner</code> determines how validation is done during the final model fit.
In most cases, this should be left as <code>NULL</code>.
The <code style="white-space: pre;">&#8288;$validate&#8288;</code> field of the tuned <a href="mlr3.html#topic+Learner">mlr3::Learner</a> specifies how the validation data is constructed
during the hyperparameter optimization.
</p>


<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li> <p><a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-autotuner">Automate</a> the tuning.
</p>
</li>
<li><p> Estimate the model performance with <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-nested-resampling">nested resampling</a>.
</p>
</li></ul>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>


<h3>Nested Resampling</h3>

<p>Nested resampling is performed by passing an <a href="#topic+AutoTuner">AutoTuner</a> to <code><a href="mlr3.html#topic+resample">mlr3::resample()</a></code> or <code><a href="mlr3.html#topic+benchmark">mlr3::benchmark()</a></code>.
To access the inner resampling results, set <code>store_tuning_instance = TRUE</code> and execute <code><a href="mlr3.html#topic+resample">mlr3::resample()</a></code> or <code><a href="mlr3.html#topic+benchmark">mlr3::benchmark()</a></code> with <code>store_models = TRUE</code> (see examples).
The <a href="mlr3.html#topic+Resampling">mlr3::Resampling</a> passed to the <a href="#topic+AutoTuner">AutoTuner</a> is meant to be the inner resampling, operating on the training set of an arbitrary outer resampling.
For this reason, the inner resampling should be not instantiated.
If an instantiated resampling is passed, the <a href="#topic+AutoTuner">AutoTuner</a> fails when a row id of the inner resampling is not present in the training set of the outer resampling.
</p>


<h3>Default Measures</h3>

<p>If no measure is passed, the default measure is used.
The default measure depends on the task type.</p>

<table>
<tr>
 <td style="text-align: left;">
   Task </td><td style="text-align: left;"> Default Measure </td><td style="text-align: left;"> Package </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"classif"</code> </td><td style="text-align: left;"> <code>"classif.ce"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3"><span class="pkg">mlr3</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"regr"</code> </td><td style="text-align: left;"> <code>"regr.mse"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3"><span class="pkg">mlr3</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"surv"</code> </td><td style="text-align: left;"> <code>"surv.cindex"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3proba"><span class="pkg">mlr3proba</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"dens"</code> </td><td style="text-align: left;"> <code>"dens.logloss"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3proba"><span class="pkg">mlr3proba</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"classif_st"</code> </td><td style="text-align: left;"> <code>"classif.ce"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3spatial"><span class="pkg">mlr3spatial</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"regr_st"</code> </td><td style="text-align: left;"> <code>"regr.mse"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3spatial"><span class="pkg">mlr3spatial</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"clust"</code> </td><td style="text-align: left;"> <code>"clust.dunn"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3cluster"><span class="pkg">mlr3cluster</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Super class</h3>

<p><code><a href="mlr3.html#topic+Learner">mlr3::Learner</a></code> -&gt; <code>AutoTuner</code>
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>instance_args</code></dt><dd><p>(<code>list()</code>)<br />
All arguments from construction to create the <a href="#topic+TuningInstanceBatchSingleCrit">TuningInstanceBatchSingleCrit</a>.</p>
</dd>
<dt><code>tuner</code></dt><dd><p>(<a href="#topic+Tuner">Tuner</a>)<br />
Optimization algorithm.</p>
</dd>
</dl>

</div>


<h3>Active bindings</h3>

<div class="r6-active-bindings">

<dl>
<dt><code>internal_valid_scores</code></dt><dd><p>Retrieves the inner validation scores as a named <code>list()</code>.
Returns <code>NULL</code> if learner is not trained yet.</p>
</dd>
<dt><code>validate</code></dt><dd><p>How to construct the internal validation data. This parameter can be either <code>NULL</code>,
a ratio in $(0, 1)$, <code>"test"</code>, or <code>"predefined"</code>.</p>
</dd>
<dt><code>archive</code></dt><dd><p><a href="#topic+ArchiveBatchTuning">ArchiveBatchTuning</a><br />
Archive of the <a href="#topic+TuningInstanceBatchSingleCrit">TuningInstanceBatchSingleCrit</a>.</p>
</dd>
<dt><code>learner</code></dt><dd><p>(<a href="mlr3.html#topic+Learner">mlr3::Learner</a>)<br />
Trained learner</p>
</dd>
<dt><code>tuning_instance</code></dt><dd><p>(<a href="#topic+TuningInstanceAsyncSingleCrit">TuningInstanceAsyncSingleCrit</a> | <a href="#topic+TuningInstanceBatchSingleCrit">TuningInstanceBatchSingleCrit</a>)<br />
Internally created tuning instance with all intermediate results.</p>
</dd>
<dt><code>tuning_result</code></dt><dd><p>(<a href="data.table.html#topic+data.table">data.table::data.table</a>)<br />
Short-cut to <code>result</code> from  tuning instance.</p>
</dd>
<dt><code>predict_type</code></dt><dd><p>(<code>character(1)</code>)<br />
Stores the currently active predict type, e.g. <code>"response"</code>.
Must be an element of <code style="white-space: pre;">&#8288;$predict_types&#8288;</code>.</p>
</dd>
<dt><code>hash</code></dt><dd><p>(<code>character(1)</code>)<br />
Hash (unique identifier) for this object.</p>
</dd>
<dt><code>phash</code></dt><dd><p>(<code>character(1)</code>)<br />
Hash (unique identifier) for this partial object, excluding some components which are varied systematically during tuning (parameter values) or feature selection (feature names).</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-AutoTuner-new"><code>AutoTuner$new()</code></a>
</p>
</li>
<li> <p><a href="#method-AutoTuner-base_learner"><code>AutoTuner$base_learner()</code></a>
</p>
</li>
<li> <p><a href="#method-AutoTuner-importance"><code>AutoTuner$importance()</code></a>
</p>
</li>
<li> <p><a href="#method-AutoTuner-selected_features"><code>AutoTuner$selected_features()</code></a>
</p>
</li>
<li> <p><a href="#method-AutoTuner-oob_error"><code>AutoTuner$oob_error()</code></a>
</p>
</li>
<li> <p><a href="#method-AutoTuner-loglik"><code>AutoTuner$loglik()</code></a>
</p>
</li>
<li> <p><a href="#method-AutoTuner-print"><code>AutoTuner$print()</code></a>
</p>
</li>
<li> <p><a href="#method-AutoTuner-marshal"><code>AutoTuner$marshal()</code></a>
</p>
</li>
<li> <p><a href="#method-AutoTuner-unmarshal"><code>AutoTuner$unmarshal()</code></a>
</p>
</li>
<li> <p><a href="#method-AutoTuner-marshaled"><code>AutoTuner$marshaled()</code></a>
</p>
</li>
<li> <p><a href="#method-AutoTuner-clone"><code>AutoTuner$clone()</code></a>
</p>
</li></ul>



<details><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="format"><a href='../../mlr3/html/Learner.html#method-Learner-format'><code>mlr3::Learner$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="help"><a href='../../mlr3/html/Learner.html#method-Learner-help'><code>mlr3::Learner$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="predict"><a href='../../mlr3/html/Learner.html#method-Learner-predict'><code>mlr3::Learner$predict()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="predict_newdata"><a href='../../mlr3/html/Learner.html#method-Learner-predict_newdata'><code>mlr3::Learner$predict_newdata()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="reset"><a href='../../mlr3/html/Learner.html#method-Learner-reset'><code>mlr3::Learner$reset()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="train"><a href='../../mlr3/html/Learner.html#method-Learner-train'><code>mlr3::Learner$train()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-AutoTuner-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>AutoTuner$new(
  tuner,
  learner,
  resampling,
  measure = NULL,
  terminator,
  search_space = NULL,
  store_tuning_instance = TRUE,
  store_benchmark_result = TRUE,
  store_models = FALSE,
  check_values = FALSE,
  callbacks = NULL,
  rush = NULL,
  validate = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>tuner</code></dt><dd><p>(<a href="#topic+Tuner">Tuner</a>)<br />
Optimization algorithm.</p>
</dd>
<dt><code>learner</code></dt><dd><p>(<a href="mlr3.html#topic+Learner">mlr3::Learner</a>)<br />
Learner to tune.</p>
</dd>
<dt><code>resampling</code></dt><dd><p>(<a href="mlr3.html#topic+Resampling">mlr3::Resampling</a>)<br />
Resampling that is used to evaluate the performance of the hyperparameter configurations.
Uninstantiated resamplings are instantiated during construction so that all configurations are evaluated on the same data splits.
Already instantiated resamplings are kept unchanged.
Specialized <a href="#topic+Tuner">Tuner</a> change the resampling e.g. to evaluate a hyperparameter configuration on different data splits.
This field, however, always returns the resampling passed in construction.</p>
</dd>
<dt><code>measure</code></dt><dd><p>(<a href="mlr3.html#topic+Measure">mlr3::Measure</a>)<br />
Measure to optimize. If <code>NULL</code>, default measure is used.</p>
</dd>
<dt><code>terminator</code></dt><dd><p>(<a href="bbotk.html#topic+Terminator">bbotk::Terminator</a>)<br />
Stop criterion of the tuning process.</p>
</dd>
<dt><code>search_space</code></dt><dd><p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>)<br />
Hyperparameter search space. If <code>NULL</code> (default), the search space is
constructed from the <a href="paradox.html#topic+to_tune">paradox::TuneToken</a> of the learner's parameter set
(learner$param_set).</p>
</dd>
<dt><code>store_tuning_instance</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default), stores the internally created <a href="#topic+TuningInstanceBatchSingleCrit">TuningInstanceBatchSingleCrit</a> with all intermediate results in slot <code style="white-space: pre;">&#8288;$tuning_instance&#8288;</code>.</p>
</dd>
<dt><code>store_benchmark_result</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default), store resample result of evaluated hyperparameter
configurations in archive as <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>.</p>
</dd>
<dt><code>store_models</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, fitted models are stored in the benchmark result
(<code>archive$benchmark_result</code>). If <code>store_benchmark_result = FALSE</code>, models
are only stored temporarily and not accessible after the tuning. This
combination is needed for measures that require a model.</p>
</dd>
<dt><code>check_values</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, hyperparameter values are checked before evaluation and
performance scores after. If <code>FALSE</code> (default), values are unchecked but
computational overhead is reduced.</p>
</dd>
<dt><code>callbacks</code></dt><dd><p>(list of <a href="mlr3misc.html#topic+Callback">mlr3misc::Callback</a>)<br />
List of callbacks.</p>
</dd>
<dt><code>rush</code></dt><dd><p>(<code>Rush</code>)<br />
If a rush instance is supplied, the tuning runs without batches.</p>
</dd>
<dt><code>validate</code></dt><dd><p>(<code>numeric(1)</code>, <code>"test"</code>, <code>"predefined"</code> or <code>NULL</code>)<br />
How to construct the internal validation data.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-AutoTuner-base_learner"></a>



<h4>Method <code>base_learner()</code></h4>

<p>Extracts the base learner from nested learner objects like <code>GraphLearner</code> in <a href="https://CRAN.R-project.org/package=mlr3pipelines"><span class="pkg">mlr3pipelines</span></a>.
If <code>recursive = 0</code>, the (tuned) learner is returned.
</p>


<h5>Usage</h5>

<div class="r"><pre>AutoTuner$base_learner(recursive = Inf)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>recursive</code></dt><dd><p>(<code>integer(1)</code>)<br />
Depth of recursion for multiple nested objects.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p><a href="mlr3.html#topic+Learner">mlr3::Learner</a>.
</p>


<hr>
<a id="method-AutoTuner-importance"></a>



<h4>Method <code>importance()</code></h4>

<p>The importance scores of the final model.
</p>


<h5>Usage</h5>

<div class="r"><pre>AutoTuner$importance()</pre></div>



<h5>Returns</h5>

<p>Named <code>numeric()</code>.
</p>


<hr>
<a id="method-AutoTuner-selected_features"></a>



<h4>Method <code>selected_features()</code></h4>

<p>The selected features of the final model.
</p>


<h5>Usage</h5>

<div class="r"><pre>AutoTuner$selected_features()</pre></div>



<h5>Returns</h5>

<p><code>character()</code>.
</p>


<hr>
<a id="method-AutoTuner-oob_error"></a>



<h4>Method <code>oob_error()</code></h4>

<p>The out-of-bag error of the final model.
</p>


<h5>Usage</h5>

<div class="r"><pre>AutoTuner$oob_error()</pre></div>



<h5>Returns</h5>

<p><code>numeric(1)</code>.
</p>


<hr>
<a id="method-AutoTuner-loglik"></a>



<h4>Method <code>loglik()</code></h4>

<p>The log-likelihood of the final model.
</p>


<h5>Usage</h5>

<div class="r"><pre>AutoTuner$loglik()</pre></div>



<h5>Returns</h5>

<p><code>logLik</code>.
Printer.
</p>


<hr>
<a id="method-AutoTuner-print"></a>



<h4>Method <code>print()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>AutoTuner$print()</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>...</code></dt><dd><p>(ignored).</p>
</dd>
</dl>

</div>


<hr>
<a id="method-AutoTuner-marshal"></a>



<h4>Method <code>marshal()</code></h4>

<p>Marshal the learner.
</p>


<h5>Usage</h5>

<div class="r"><pre>AutoTuner$marshal(...)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>...</code></dt><dd><p>(any)<br />
Additional parameters.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>self
</p>


<hr>
<a id="method-AutoTuner-unmarshal"></a>



<h4>Method <code>unmarshal()</code></h4>

<p>Unmarshal the learner.
</p>


<h5>Usage</h5>

<div class="r"><pre>AutoTuner$unmarshal(...)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>...</code></dt><dd><p>(any)<br />
Additional parameters.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>self
</p>


<hr>
<a id="method-AutoTuner-marshaled"></a>



<h4>Method <code>marshaled()</code></h4>

<p>Whether the learner is marshaled.
</p>


<h5>Usage</h5>

<div class="r"><pre>AutoTuner$marshaled()</pre></div>


<hr>
<a id="method-AutoTuner-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>AutoTuner$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'># Automatic Tuning

# split to train and external set
task = tsk("penguins")
split = partition(task, ratio = 0.8)

# load learner and set search space
learner = lrn("classif.rpart",
  cp = to_tune(1e-04, 1e-1, logscale = TRUE)
)

# create auto tuner
at = auto_tuner(
  tuner = tnr("random_search"),
  learner = learner,
  resampling = rsmp ("holdout"),
  measure = msr("classif.ce"),
  term_evals = 4)

# tune hyperparameters and fit final model
at$train(task, row_ids = split$train)

# predict with final model
at$predict(task, row_ids = split$test)

# show tuning result
at$tuning_result

# model slot contains trained learner and tuning instance
at$model

# shortcut trained learner
at$learner

# shortcut tuning instance
at$tuning_instance


# Nested Resampling

at = auto_tuner(
  tuner = tnr("random_search"),
  learner = learner,
  resampling = rsmp ("holdout"),
  measure = msr("classif.ce"),
  term_evals = 4)

resampling_outer = rsmp("cv", folds = 3)
rr = resample(task, at, resampling_outer, store_models = TRUE)

# retrieve inner tuning results.
extract_inner_tuning_results(rr)

# performance scores estimated on the outer resampling
rr$score()

# unbiased performance of the final model trained on the full data set
rr$aggregate()
</code></pre>

<hr>
<h2 id='callback_async_tuning'>Create Asynchronous Tuning Callback</h2><span id='topic+callback_async_tuning'></span>

<h3>Description</h3>

<p>Function to create a <a href="#topic+CallbackAsyncTuning">CallbackAsyncTuning</a>.
Predefined callbacks are stored in the <a href="mlr3misc.html#topic+Dictionary">dictionary</a> <a href="#topic+mlr_callbacks">mlr_callbacks</a> and can be retrieved with <code><a href="#topic+clbk">clbk()</a></code>.
</p>
<p>Tuning callbacks can be called from different stages of the tuning process.
The stages are prefixed with <code style="white-space: pre;">&#8288;on_*&#8288;</code>.
</p>
<div class="sourceCode"><pre>Start Tuning
     - on_optimization_begin
    Start Worker
         - on_worker_begin
        Start Evaluation
             - on_eval_after_xs
             - on_eval_after_resample
             - on_eval_before_archive
        End Evaluation
         - on_worker_end
    End Worker
     - on_result
     - on_optimization_end
End Tuning
</pre></div>
<p>See also the section on parameters for more information on the stages.
A tuning callback works with <a href="#topic+ContextAsyncTuning">ContextAsyncTuning</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>callback_async_tuning(
  id,
  label = NA_character_,
  man = NA_character_,
  on_optimization_begin = NULL,
  on_worker_begin = NULL,
  on_eval_after_xs = NULL,
  on_eval_after_resample = NULL,
  on_eval_before_archive = NULL,
  on_worker_end = NULL,
  on_result = NULL,
  on_optimization_end = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="callback_async_tuning_+3A_id">id</code></td>
<td>
<p>(<code>character(1)</code>)<br />
Identifier for the new instance.</p>
</td></tr>
<tr><td><code id="callback_async_tuning_+3A_label">label</code></td>
<td>
<p>(<code>character(1)</code>)<br />
Label for the new instance.</p>
</td></tr>
<tr><td><code id="callback_async_tuning_+3A_man">man</code></td>
<td>
<p>(<code>character(1)</code>)<br />
String in the format <code style="white-space: pre;">&#8288;[pkg]::[topic]&#8288;</code> pointing to a manual page for this object.
The referenced help package can be opened via method <code style="white-space: pre;">&#8288;$help()&#8288;</code>.</p>
</td></tr>
<tr><td><code id="callback_async_tuning_+3A_on_optimization_begin">on_optimization_begin</code></td>
<td>
<p>(<code style="white-space: pre;">&#8288;function()&#8288;</code>)<br />
Stage called at the beginning of the optimization.
Called in <code>Optimizer$optimize()</code>.</p>
</td></tr>
<tr><td><code id="callback_async_tuning_+3A_on_worker_begin">on_worker_begin</code></td>
<td>
<p>(<code style="white-space: pre;">&#8288;function()&#8288;</code>)<br />
Stage called at the beginning of the optimization on the worker.
Called in the worker loop.</p>
</td></tr>
<tr><td><code id="callback_async_tuning_+3A_on_eval_after_xs">on_eval_after_xs</code></td>
<td>
<p>(<code style="white-space: pre;">&#8288;function()&#8288;</code>)<br />
Stage called after xs is passed.
Called in <code>ObjectiveTuning$eval()</code>.</p>
</td></tr>
<tr><td><code id="callback_async_tuning_+3A_on_eval_after_resample">on_eval_after_resample</code></td>
<td>
<p>(<code style="white-space: pre;">&#8288;function()&#8288;</code>)<br />
Stage called after a hyperparameter configuration is evaluated.
Called in <code>ObjectiveTuning$eval()</code>.</p>
</td></tr>
<tr><td><code id="callback_async_tuning_+3A_on_eval_before_archive">on_eval_before_archive</code></td>
<td>
<p>(<code style="white-space: pre;">&#8288;function()&#8288;</code>)<br />
Stage called before performance values are written to the archive.
Called in <code>ObjectiveTuning$eval()</code>.</p>
</td></tr>
<tr><td><code id="callback_async_tuning_+3A_on_worker_end">on_worker_end</code></td>
<td>
<p>(<code style="white-space: pre;">&#8288;function()&#8288;</code>)<br />
Stage called at the end of the optimization on the worker.
Called in the worker loop.</p>
</td></tr>
<tr><td><code id="callback_async_tuning_+3A_on_result">on_result</code></td>
<td>
<p>(<code style="white-space: pre;">&#8288;function()&#8288;</code>)<br />
Stage called after the result is written.
Called in <code>OptimInstance$assign_result()</code>.</p>
</td></tr>
<tr><td><code id="callback_async_tuning_+3A_on_optimization_end">on_optimization_end</code></td>
<td>
<p>(<code style="white-space: pre;">&#8288;function()&#8288;</code>)<br />
Stage called at the end of the optimization.
Called in <code>Optimizer$optimize()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When implementing a callback, each function must have two arguments named <code>callback</code> and <code>context</code>.
A callback can write data to the state (<code style="white-space: pre;">&#8288;$state&#8288;</code>), e.g. settings that affect the callback itself.
Tuning callbacks access <a href="#topic+ContextAsyncTuning">ContextAsyncTuning</a>.
</p>

<hr>
<h2 id='callback_batch_tuning'>Create Batch Tuning Callback</h2><span id='topic+callback_batch_tuning'></span>

<h3>Description</h3>

<p>Function to create a <a href="#topic+CallbackBatchTuning">CallbackBatchTuning</a>.
Predefined callbacks are stored in the <a href="mlr3misc.html#topic+Dictionary">dictionary</a> <a href="#topic+mlr_callbacks">mlr_callbacks</a> and can be retrieved with <code><a href="#topic+clbk">clbk()</a></code>.
</p>
<p>Tuning callbacks can be called from different stages of the tuning process.
The stages are prefixed with <code style="white-space: pre;">&#8288;on_*&#8288;</code>.
</p>
<div class="sourceCode"><pre>Start Tuning
     - on_optimization_begin
    Start Tuner Batch
         - on_optimizer_before_eval
        Start Evaluation
             - on_eval_after_design
             - on_eval_after_benchmark
             - on_eval_before_archive
        End Evaluation
         - on_optimizer_after_eval
    End Tuner Batch
     - on_result
     - on_optimization_end
End Tuning
</pre></div>
<p>See also the section on parameters for more information on the stages.
A tuning callback works with <a href="#topic+ContextBatchTuning">ContextBatchTuning</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>callback_batch_tuning(
  id,
  label = NA_character_,
  man = NA_character_,
  on_optimization_begin = NULL,
  on_optimizer_before_eval = NULL,
  on_eval_after_design = NULL,
  on_eval_after_benchmark = NULL,
  on_eval_before_archive = NULL,
  on_optimizer_after_eval = NULL,
  on_result = NULL,
  on_optimization_end = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="callback_batch_tuning_+3A_id">id</code></td>
<td>
<p>(<code>character(1)</code>)<br />
Identifier for the new instance.</p>
</td></tr>
<tr><td><code id="callback_batch_tuning_+3A_label">label</code></td>
<td>
<p>(<code>character(1)</code>)<br />
Label for the new instance.</p>
</td></tr>
<tr><td><code id="callback_batch_tuning_+3A_man">man</code></td>
<td>
<p>(<code>character(1)</code>)<br />
String in the format <code style="white-space: pre;">&#8288;[pkg]::[topic]&#8288;</code> pointing to a manual page for this object.
The referenced help package can be opened via method <code style="white-space: pre;">&#8288;$help()&#8288;</code>.</p>
</td></tr>
<tr><td><code id="callback_batch_tuning_+3A_on_optimization_begin">on_optimization_begin</code></td>
<td>
<p>(<code style="white-space: pre;">&#8288;function()&#8288;</code>)<br />
Stage called at the beginning of the optimization.
Called in <code>Optimizer$optimize()</code>.</p>
</td></tr>
<tr><td><code id="callback_batch_tuning_+3A_on_optimizer_before_eval">on_optimizer_before_eval</code></td>
<td>
<p>(<code style="white-space: pre;">&#8288;function()&#8288;</code>)<br />
Stage called after the optimizer proposes points.
Called in <code>OptimInstance$eval_batch()</code>.</p>
</td></tr>
<tr><td><code id="callback_batch_tuning_+3A_on_eval_after_design">on_eval_after_design</code></td>
<td>
<p>(<code style="white-space: pre;">&#8288;function()&#8288;</code>)<br />
Stage called after the design is created.
Called in <code>ObjectiveTuning$eval_many()</code>.
The context available is <a href="#topic+ContextBatchTuning">ContextBatchTuning</a>.</p>
</td></tr>
<tr><td><code id="callback_batch_tuning_+3A_on_eval_after_benchmark">on_eval_after_benchmark</code></td>
<td>
<p>(<code style="white-space: pre;">&#8288;function()&#8288;</code>)<br />
Stage called after hyperparameter configurations are evaluated.
Called in <code>ObjectiveTuning$eval_many()</code>.
The context available is <a href="#topic+ContextBatchTuning">ContextBatchTuning</a>.</p>
</td></tr>
<tr><td><code id="callback_batch_tuning_+3A_on_eval_before_archive">on_eval_before_archive</code></td>
<td>
<p>(<code style="white-space: pre;">&#8288;function()&#8288;</code>)<br />
Stage called before performance values are written to the archive.
Called in <code>ObjectiveTuning$eval_many()</code>.
The context available is <a href="#topic+ContextBatchTuning">ContextBatchTuning</a>.</p>
</td></tr>
<tr><td><code id="callback_batch_tuning_+3A_on_optimizer_after_eval">on_optimizer_after_eval</code></td>
<td>
<p>(<code style="white-space: pre;">&#8288;function()&#8288;</code>)<br />
Stage called after points are evaluated.
Called in <code>OptimInstance$eval_batch()</code>.</p>
</td></tr>
<tr><td><code id="callback_batch_tuning_+3A_on_result">on_result</code></td>
<td>
<p>(<code style="white-space: pre;">&#8288;function()&#8288;</code>)<br />
Stage called after the result is written.
Called in <code>OptimInstance$assign_result()</code>.</p>
</td></tr>
<tr><td><code id="callback_batch_tuning_+3A_on_optimization_end">on_optimization_end</code></td>
<td>
<p>(<code style="white-space: pre;">&#8288;function()&#8288;</code>)<br />
Stage called at the end of the optimization.
Called in <code>Optimizer$optimize()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When implementing a callback, each function must have two arguments named <code>callback</code> and <code>context</code>.
A callback can write data to the state (<code style="white-space: pre;">&#8288;$state&#8288;</code>), e.g. settings that affect the callback itself.
Tuning callbacks access <a href="#topic+ContextBatchTuning">ContextBatchTuning</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># write archive to disk
callback_batch_tuning("mlr3tuning.backup",
  on_optimization_end = function(callback, context) {
    saveRDS(context$instance$archive, "archive.rds")
  }
)
</code></pre>

<hr>
<h2 id='CallbackAsyncTuning'>Create Asynchronous Tuning Callback</h2><span id='topic+CallbackAsyncTuning'></span>

<h3>Description</h3>

<p>Specialized <a href="bbotk.html#topic+CallbackAsync">bbotk::CallbackAsync</a> for asynchronous tuning.
Callbacks allow to customize the behavior of processes in mlr3tuning.
The <code><a href="#topic+callback_async_tuning">callback_async_tuning()</a></code> function creates a <a href="#topic+CallbackAsyncTuning">CallbackAsyncTuning</a>.
Predefined callbacks are stored in the <a href="mlr3misc.html#topic+Dictionary">dictionary</a> <a href="#topic+mlr_callbacks">mlr_callbacks</a> and can be retrieved with <code><a href="#topic+clbk">clbk()</a></code>.
For more information on tuning callbacks see <code><a href="#topic+callback_async_tuning">callback_async_tuning()</a></code>.
</p>


<h3>Super classes</h3>

<p><code><a href="mlr3misc.html#topic+Callback">mlr3misc::Callback</a></code> -&gt; <code><a href="bbotk.html#topic+CallbackAsync">bbotk::CallbackAsync</a></code> -&gt; <code>CallbackAsyncTuning</code>
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>on_eval_after_xs</code></dt><dd><p>(<code style="white-space: pre;">&#8288;function()&#8288;</code>)<br />
Stage called after xs is passed.
Called in <code>ObjectiveTuning$eval()</code>.</p>
</dd>
<dt><code>on_eval_after_resample</code></dt><dd><p>(<code style="white-space: pre;">&#8288;function()&#8288;</code>)<br />
Stage called after hyperparameter configurations are evaluated.
Called in <code>ObjectiveTuning$eval()</code>.</p>
</dd>
<dt><code>on_eval_before_archive</code></dt><dd><p>(<code style="white-space: pre;">&#8288;function()&#8288;</code>)<br />
Stage called before performance values are written to the archive.
Called in <code>ObjectiveTuning$eval()</code>.</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-CallbackAsyncTuning-clone"><code>CallbackAsyncTuning$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3misc" data-topic="Callback" data-id="call"><a href='../../mlr3misc/html/Callback.html#method-Callback-call'><code>mlr3misc::Callback$call()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3misc" data-topic="Callback" data-id="format"><a href='../../mlr3misc/html/Callback.html#method-Callback-format'><code>mlr3misc::Callback$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3misc" data-topic="Callback" data-id="help"><a href='../../mlr3misc/html/Callback.html#method-Callback-help'><code>mlr3misc::Callback$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3misc" data-topic="Callback" data-id="initialize"><a href='../../mlr3misc/html/Callback.html#method-Callback-initialize'><code>mlr3misc::Callback$initialize()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3misc" data-topic="Callback" data-id="print"><a href='../../mlr3misc/html/Callback.html#method-Callback-print'><code>mlr3misc::Callback$print()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-CallbackAsyncTuning-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>CallbackAsyncTuning$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='CallbackBatchTuning'>Create Batch Tuning Callback</h2><span id='topic+CallbackBatchTuning'></span>

<h3>Description</h3>

<p>Specialized <a href="bbotk.html#topic+CallbackBatch">bbotk::CallbackBatch</a> for batch tuning.
Callbacks allow to customize the behavior of processes in mlr3tuning.
The <code><a href="#topic+callback_batch_tuning">callback_batch_tuning()</a></code> function creates a <a href="#topic+CallbackBatchTuning">CallbackBatchTuning</a>.
Predefined callbacks are stored in the <a href="mlr3misc.html#topic+Dictionary">dictionary</a> <a href="#topic+mlr_callbacks">mlr_callbacks</a> and can be retrieved with <code><a href="#topic+clbk">clbk()</a></code>.
For more information on tuning callbacks see <code><a href="#topic+callback_batch_tuning">callback_batch_tuning()</a></code>.
</p>


<h3>Super classes</h3>

<p><code><a href="mlr3misc.html#topic+Callback">mlr3misc::Callback</a></code> -&gt; <code><a href="bbotk.html#topic+CallbackBatch">bbotk::CallbackBatch</a></code> -&gt; <code>CallbackBatchTuning</code>
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>on_eval_after_design</code></dt><dd><p>(<code style="white-space: pre;">&#8288;function()&#8288;</code>)<br />
Stage called after design is created.
Called in <code>ObjectiveTuning$eval_many()</code>.</p>
</dd>
<dt><code>on_eval_after_benchmark</code></dt><dd><p>(<code style="white-space: pre;">&#8288;function()&#8288;</code>)<br />
Stage called after hyperparameter configurations are evaluated.
Called in <code>ObjectiveTuning$eval_many()</code>.</p>
</dd>
<dt><code>on_eval_before_archive</code></dt><dd><p>(<code style="white-space: pre;">&#8288;function()&#8288;</code>)<br />
Stage called before performance values are written to the archive.
Called in <code>ObjectiveTuning$eval_many()</code>.</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-CallbackBatchTuning-clone"><code>CallbackBatchTuning$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3misc" data-topic="Callback" data-id="call"><a href='../../mlr3misc/html/Callback.html#method-Callback-call'><code>mlr3misc::Callback$call()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3misc" data-topic="Callback" data-id="format"><a href='../../mlr3misc/html/Callback.html#method-Callback-format'><code>mlr3misc::Callback$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3misc" data-topic="Callback" data-id="help"><a href='../../mlr3misc/html/Callback.html#method-Callback-help'><code>mlr3misc::Callback$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3misc" data-topic="Callback" data-id="initialize"><a href='../../mlr3misc/html/Callback.html#method-Callback-initialize'><code>mlr3misc::Callback$initialize()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3misc" data-topic="Callback" data-id="print"><a href='../../mlr3misc/html/Callback.html#method-Callback-print'><code>mlr3misc::Callback$print()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-CallbackBatchTuning-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>CallbackBatchTuning$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'># write archive to disk
callback_batch_tuning("mlr3tuning.backup",
  on_optimization_end = function(callback, context) {
    saveRDS(context$instance$archive, "archive.rds")
  }
)
</code></pre>

<hr>
<h2 id='ContextAsyncTuning'>Asynchronous Tuning Context</h2><span id='topic+ContextAsyncTuning'></span>

<h3>Description</h3>

<p>A <a href="#topic+CallbackAsyncTuning">CallbackAsyncTuning</a> accesses and modifies data during the optimization via the <code>ContextAsyncTuning</code>.
See the section on active bindings for a list of modifiable objects.
See <code><a href="#topic+callback_async_tuning">callback_async_tuning()</a></code> for a list of stages that access <code>ContextAsyncTuning</code>.
</p>


<h3>Details</h3>

<p>Changes to <code style="white-space: pre;">&#8288;$instance&#8288;</code> and <code style="white-space: pre;">&#8288;$optimizer&#8288;</code> in the stages executed on the workers are not reflected in the main process.
</p>


<h3>Super classes</h3>

<p><code><a href="mlr3misc.html#topic+Context">mlr3misc::Context</a></code> -&gt; <code><a href="bbotk.html#topic+ContextAsync">bbotk::ContextAsync</a></code> -&gt; <code>ContextAsyncTuning</code>
</p>


<h3>Active bindings</h3>

<div class="r6-active-bindings">

<dl>
<dt><code>xs</code></dt><dd><p>(list())<br />
The hyperparameter configuration currently evaluated.
Contains the values on the learner scale i.e. transformations are applied.</p>
</dd>
<dt><code>resample_result</code></dt><dd><p>(<a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>)<br />
The resample result of the hyperparameter configuration currently evaluated.</p>
</dd>
<dt><code>aggregated_performance</code></dt><dd><p>(<code>list()</code>)<br />
Aggregated performance scores and training time of the evaluated hyperparameter configuration.
This list is passed to the archive.
A callback can add additional elements which are also written to the archive.</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-ContextAsyncTuning-clone"><code>ContextAsyncTuning$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3misc" data-topic="Context" data-id="format"><a href='../../mlr3misc/html/Context.html#method-Context-format'><code>mlr3misc::Context$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3misc" data-topic="Context" data-id="print"><a href='../../mlr3misc/html/Context.html#method-Context-print'><code>mlr3misc::Context$print()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="ContextAsync" data-id="initialize"><a href='../../bbotk/html/ContextAsync.html#method-ContextAsync-initialize'><code>bbotk::ContextAsync$initialize()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-ContextAsyncTuning-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>ContextAsyncTuning$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='ContextBatchTuning'>Batch Tuning Context</h2><span id='topic+ContextBatchTuning'></span>

<h3>Description</h3>

<p>A <a href="#topic+CallbackBatchTuning">CallbackBatchTuning</a> accesses and modifies data during the optimization via the <code>ContextBatchTuning</code>.
See the section on active bindings for a list of modifiable objects.
See <code><a href="#topic+callback_batch_tuning">callback_batch_tuning()</a></code> for a list of stages that access <code>ContextBatchTuning</code>.
</p>


<h3>Super classes</h3>

<p><code><a href="mlr3misc.html#topic+Context">mlr3misc::Context</a></code> -&gt; <code><a href="bbotk.html#topic+ContextBatch">bbotk::ContextBatch</a></code> -&gt; <code>ContextBatchTuning</code>
</p>


<h3>Active bindings</h3>

<div class="r6-active-bindings">

<dl>
<dt><code>xss</code></dt><dd><p>(list())<br />
The hyperparameter configurations of the latest batch.
Contains the values on the learner scale i.e. transformations are applied.
See <code style="white-space: pre;">&#8288;$xdt&#8288;</code> for the untransformed values.</p>
</dd>
<dt><code>design</code></dt><dd><p>(<a href="data.table.html#topic+data.table">data.table::data.table</a>)<br />
The benchmark design of the latest batch.</p>
</dd>
<dt><code>benchmark_result</code></dt><dd><p>(<a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>)<br />
The benchmark result of the latest batch.</p>
</dd>
<dt><code>aggregated_performance</code></dt><dd><p>(<a href="data.table.html#topic+data.table">data.table::data.table</a>)<br />
Aggregated performance scores and training time of the latest batch.
This data table is passed to the archive.
A callback can add additional columns which are also written to the archive.</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-ContextBatchTuning-clone"><code>ContextBatchTuning$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3misc" data-topic="Context" data-id="format"><a href='../../mlr3misc/html/Context.html#method-Context-format'><code>mlr3misc::Context$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3misc" data-topic="Context" data-id="print"><a href='../../mlr3misc/html/Context.html#method-Context-print'><code>mlr3misc::Context$print()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="ContextBatch" data-id="initialize"><a href='../../bbotk/html/ContextBatch.html#method-ContextBatch-initialize'><code>bbotk::ContextBatch$initialize()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-ContextBatchTuning-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>ContextBatchTuning$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='extract_inner_tuning_archives'>Extract Inner Tuning Archives</h2><span id='topic+extract_inner_tuning_archives'></span>

<h3>Description</h3>

<p>Extract inner tuning archives of nested resampling.
Implemented for <a href="mlr3.html#topic+ResampleResult">mlr3::ResampleResult</a> and <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>.
The function iterates over the <a href="#topic+AutoTuner">AutoTuner</a> objects and binds the tuning archives to a <code><a href="data.table.html#topic+data.table">data.table::data.table()</a></code>.
<a href="#topic+AutoTuner">AutoTuner</a> must be initialized with <code>store_tuning_instance = TRUE</code> and <code><a href="mlr3.html#topic+resample">mlr3::resample()</a></code> or <code><a href="mlr3.html#topic+benchmark">mlr3::benchmark()</a></code> must be called with <code>store_models = TRUE</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_inner_tuning_archives(
  x,
  unnest = "x_domain",
  exclude_columns = "uhash"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_inner_tuning_archives_+3A_x">x</code></td>
<td>
<p>(<a href="mlr3.html#topic+ResampleResult">mlr3::ResampleResult</a> | <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>).</p>
</td></tr>
<tr><td><code id="extract_inner_tuning_archives_+3A_unnest">unnest</code></td>
<td>
<p>(<code>character()</code>)<br />
Transforms list columns to separate columns.
By default, <code>x_domain</code> is unnested.
Set to <code>NULL</code> if no column should be unnested.</p>
</td></tr>
<tr><td><code id="extract_inner_tuning_archives_+3A_exclude_columns">exclude_columns</code></td>
<td>
<p>(<code>character()</code>)<br />
Exclude columns from result table.
Set to <code>NULL</code> if no column should be excluded.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code><a href="data.table.html#topic+data.table">data.table::data.table()</a></code>.
</p>


<h3>Data structure</h3>

<p>The returned data table has the following columns:
</p>

<ul>
<li> <p><code>experiment</code> (integer(1))<br />
Index, giving the according row number in the original benchmark grid.
</p>
</li>
<li> <p><code>iteration</code> (integer(1))<br />
Iteration of the outer resampling.
</p>
</li>
<li><p> One column for each hyperparameter of the search spaces.
</p>
</li>
<li><p> One column for each performance measure.
</p>
</li>
<li> <p><code>runtime_learners</code> (<code>numeric(1)</code>)<br />
Sum of training and predict times logged in learners per <a href="mlr3.html#topic+ResampleResult">mlr3::ResampleResult</a> / evaluation.
This does not include potential overhead time.
</p>
</li>
<li> <p><code>timestamp</code> (<code>POSIXct</code>)<br />
Time stamp when the evaluation was logged into the archive.
</p>
</li>
<li> <p><code>batch_nr</code> (<code>integer(1)</code>)<br />
Hyperparameters are evaluated in batches.
Each batch has a unique batch number.
</p>
</li>
<li> <p><code>x_domain</code> (<code>list()</code>)<br />
List of transformed hyperparameter values.
By default this column is unnested.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;x_domain_*&#8288;</code> (<code>any</code>)<br />
Separate column for each transformed hyperparameter.
</p>
</li>
<li> <p><code>resample_result</code> (<a href="mlr3.html#topic+ResampleResult">mlr3::ResampleResult</a>)<br />
Resample result of the inner resampling.
</p>
</li>
<li> <p><code>task_id</code> (<code>character(1)</code>).
</p>
</li>
<li> <p><code>learner_id</code> (<code>character(1)</code>).
</p>
</li>
<li> <p><code>resampling_id</code> (<code>character(1)</code>).
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Nested Resampling on Palmer Penguins Data Set

learner = lrn("classif.rpart",
  cp = to_tune(1e-04, 1e-1, logscale = TRUE))

# create auto tuner
at = auto_tuner(
  tuner = tnr("random_search"),
  learner = learner,
  resampling = rsmp ("holdout"),
  measure = msr("classif.ce"),
  term_evals = 4)

resampling_outer = rsmp("cv", folds = 2)
rr = resample(tsk("iris"), at, resampling_outer, store_models = TRUE)

# extract inner archives
extract_inner_tuning_archives(rr)
</code></pre>

<hr>
<h2 id='extract_inner_tuning_results'>Extract Inner Tuning Results</h2><span id='topic+extract_inner_tuning_results'></span><span id='topic+extract_inner_tuning_results.ResampleResult'></span><span id='topic+extract_inner_tuning_results.BenchmarkResult'></span>

<h3>Description</h3>

<p>Extract inner tuning results of nested resampling.
Implemented for <a href="mlr3.html#topic+ResampleResult">mlr3::ResampleResult</a> and <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_inner_tuning_results(x, tuning_instance, ...)

## S3 method for class 'ResampleResult'
extract_inner_tuning_results(x, tuning_instance = FALSE, ...)

## S3 method for class 'BenchmarkResult'
extract_inner_tuning_results(x, tuning_instance = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_inner_tuning_results_+3A_x">x</code></td>
<td>
<p>(<a href="mlr3.html#topic+ResampleResult">mlr3::ResampleResult</a> | <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>).</p>
</td></tr>
<tr><td><code id="extract_inner_tuning_results_+3A_tuning_instance">tuning_instance</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, tuning instances are added to the table.</p>
</td></tr>
<tr><td><code id="extract_inner_tuning_results_+3A_...">...</code></td>
<td>
<p>(any)<br />
Additional arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function iterates over the <a href="#topic+AutoTuner">AutoTuner</a> objects and binds the tuning results to a <code><a href="data.table.html#topic+data.table">data.table::data.table()</a></code>.
The <a href="#topic+AutoTuner">AutoTuner</a> must be initialized with <code>store_tuning_instance = TRUE</code> and <code><a href="mlr3.html#topic+resample">mlr3::resample()</a></code> or <code><a href="mlr3.html#topic+benchmark">mlr3::benchmark()</a></code> must be called with <code>store_models = TRUE</code>.
Optionally, the tuning instance can be added for each iteration.
</p>


<h3>Value</h3>

<p><code><a href="data.table.html#topic+data.table">data.table::data.table()</a></code>.
</p>


<h3>Data structure</h3>

<p>The returned data table has the following columns:
</p>

<ul>
<li> <p><code>experiment</code> (integer(1))<br />
Index, giving the according row number in the original benchmark grid.
</p>
</li>
<li> <p><code>iteration</code> (integer(1))<br />
Iteration of the outer resampling.
</p>
</li>
<li><p> One column for each hyperparameter of the search spaces.
</p>
</li>
<li><p> One column for each performance measure.
</p>
</li>
<li> <p><code>learner_param_vals</code> (<code>list()</code>)<br />
Hyperparameter values used by the learner.
Includes fixed and proposed hyperparameter values.
</p>
</li>
<li> <p><code>x_domain</code> (<code>list()</code>)<br />
List of transformed hyperparameter values.
</p>
</li>
<li> <p><code>tuning_instance</code> (<a href="#topic+TuningInstanceBatchSingleCrit">TuningInstanceBatchSingleCrit</a> | <a href="#topic+TuningInstanceBatchMultiCrit">TuningInstanceBatchMultiCrit</a>)<br />
Optionally, tuning instances.
</p>
</li>
<li> <p><code>task_id</code> (<code>character(1)</code>).
</p>
</li>
<li> <p><code>learner_id</code> (<code>character(1)</code>).
</p>
</li>
<li> <p><code>resampling_id</code> (<code>character(1)</code>).
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Nested Resampling on Palmer Penguins Data Set

learner = lrn("classif.rpart",
  cp = to_tune(1e-04, 1e-1, logscale = TRUE))

# create auto tuner
at = auto_tuner(
  tuner = tnr("random_search"),
  learner = learner,
  resampling = rsmp ("holdout"),
  measure = msr("classif.ce"),
  term_evals = 4)

resampling_outer = rsmp("cv", folds = 2)
rr = resample(tsk("iris"), at, resampling_outer, store_models = TRUE)

# extract inner results
extract_inner_tuning_results(rr)
</code></pre>

<hr>
<h2 id='mlr_tuners'>Dictionary of Tuners</h2><span id='topic+mlr_tuners'></span>

<h3>Description</h3>

<p>A simple <a href="mlr3misc.html#topic+Dictionary">mlr3misc::Dictionary</a> storing objects of class <a href="#topic+Tuner">Tuner</a>.
Each tuner has an associated help page, see <code>mlr_tuners_[id]</code>.
</p>
<p>This dictionary can get populated with additional tuners by add-on packages.
</p>
<p>For a more convenient way to retrieve and construct tuner, see <code><a href="#topic+tnr">tnr()</a></code>/<code><a href="#topic+tnrs">tnrs()</a></code>.
</p>


<h3>Format</h3>

<p><a href="R6.html#topic+R6Class">R6::R6Class</a> object inheriting from <a href="mlr3misc.html#topic+Dictionary">mlr3misc::Dictionary</a>.
</p>


<h3>Methods</h3>

<p>See <a href="mlr3misc.html#topic+Dictionary">mlr3misc::Dictionary</a>.
</p>


<h3>S3 methods</h3>


<ul>
<li> <p><code>as.data.table(dict, ..., objects = FALSE)</code><br />
<a href="mlr3misc.html#topic+Dictionary">mlr3misc::Dictionary</a> -&gt; <code><a href="data.table.html#topic+data.table">data.table::data.table()</a></code><br />
Returns a <code><a href="data.table.html#topic+data.table">data.table::data.table()</a></code> with fields &quot;key&quot;, &quot;label&quot;, &quot;param_classes&quot;, &quot;properties&quot; and &quot;packages&quot; as columns.
If <code>objects</code> is set to <code>TRUE</code>, the constructed objects are returned in the list column named <code>object</code>.
</p>
</li></ul>



<h3>See Also</h3>

<p>Sugar functions: <code><a href="#topic+tnr">tnr()</a></code>, <code><a href="#topic+tnrs">tnrs()</a></code>
</p>
<p>Other Tuner: 
<code><a href="#topic+Tuner">Tuner</a></code>,
<code><a href="#topic+mlr_tuners_cmaes">mlr_tuners_cmaes</a></code>,
<code><a href="#topic+mlr_tuners_design_points">mlr_tuners_design_points</a></code>,
<code><a href="#topic+mlr_tuners_gensa">mlr_tuners_gensa</a></code>,
<code><a href="#topic+mlr_tuners_grid_search">mlr_tuners_grid_search</a></code>,
<code><a href="#topic+mlr_tuners_internal">mlr_tuners_internal</a></code>,
<code><a href="#topic+mlr_tuners_irace">mlr_tuners_irace</a></code>,
<code><a href="#topic+mlr_tuners_nloptr">mlr_tuners_nloptr</a></code>,
<code><a href="#topic+mlr_tuners_random_search">mlr_tuners_random_search</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>as.data.table(mlr_tuners)
mlr_tuners$get("random_search")
tnr("random_search")
</code></pre>

<hr>
<h2 id='mlr_tuners_async_design_points'>Hyperparameter Tuning with Asynchronous Design Points</h2><span id='topic+mlr_tuners_async_design_points'></span><span id='topic+TunerAsyncDesignPoints'></span>

<h3>Description</h3>

<p>Subclass for asynchronous design points tuning.
</p>


<h3>Dictionary</h3>

<p>This <a href="#topic+Tuner">Tuner</a> can be instantiated with the associated sugar function <code><a href="#topic+tnr">tnr()</a></code>:
</p>
<div class="sourceCode"><pre>tnr("async_design_points")
</pre></div>


<h3>Parameters</h3>


<dl>
<dt><code>design</code></dt><dd><p><a href="data.table.html#topic+data.table">data.table::data.table</a><br />
Design points to try in search, one per row.</p>
</dd>
</dl>



<h3>Super classes</h3>

<p><code><a href="#topic+Tuner">mlr3tuning::Tuner</a></code> -&gt; <code><a href="#topic+TunerAsync">mlr3tuning::TunerAsync</a></code> -&gt; <code><a href="#topic+TunerAsyncFromOptimizerAsync">mlr3tuning::TunerAsyncFromOptimizerAsync</a></code> -&gt; <code>TunerAsyncDesignPoints</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TunerAsyncDesignPoints-new"><code>TunerAsyncDesignPoints$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TunerAsyncDesignPoints-clone"><code>TunerAsyncDesignPoints$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="format"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-format'><code>mlr3tuning::Tuner$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="help"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-help'><code>mlr3tuning::Tuner$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="print"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-print'><code>mlr3tuning::Tuner$print()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="TunerAsyncFromOptimizerAsync" data-id="optimize"><a href='../../mlr3tuning/html/TunerAsyncFromOptimizerAsync.html#method-TunerAsyncFromOptimizerAsync-optimize'><code>mlr3tuning::TunerAsyncFromOptimizerAsync$optimize()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-TunerAsyncDesignPoints-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerAsyncDesignPoints$new()</pre></div>


<hr>
<a id="method-TunerAsyncDesignPoints-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerAsyncDesignPoints$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other TunerAsync: 
<code><a href="#topic+mlr_tuners_async_grid_search">mlr_tuners_async_grid_search</a></code>,
<code><a href="#topic+mlr_tuners_async_random_search">mlr_tuners_async_random_search</a></code>
</p>

<hr>
<h2 id='mlr_tuners_async_grid_search'>Hyperparameter Tuning with Asynchronous Grid Search</h2><span id='topic+mlr_tuners_async_grid_search'></span><span id='topic+TunerAsyncGridSearch'></span>

<h3>Description</h3>

<p>Subclass for asynchronous grid search tuning.
</p>


<h3>Dictionary</h3>

<p>This <a href="#topic+Tuner">Tuner</a> can be instantiated with the associated sugar function <code><a href="#topic+tnr">tnr()</a></code>:
</p>
<div class="sourceCode"><pre>tnr("async_design_points")
</pre></div>


<h3>Parameters</h3>


<dl>
<dt><code>batch_size</code></dt><dd><p><code>integer(1)</code><br />
Maximum number of points to try in a batch.</p>
</dd>
</dl>



<h3>Super classes</h3>

<p><code><a href="#topic+Tuner">mlr3tuning::Tuner</a></code> -&gt; <code><a href="#topic+TunerAsync">mlr3tuning::TunerAsync</a></code> -&gt; <code><a href="#topic+TunerAsyncFromOptimizerAsync">mlr3tuning::TunerAsyncFromOptimizerAsync</a></code> -&gt; <code>TunerAsyncGridSearch</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TunerAsyncGridSearch-new"><code>TunerAsyncGridSearch$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TunerAsyncGridSearch-clone"><code>TunerAsyncGridSearch$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="format"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-format'><code>mlr3tuning::Tuner$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="help"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-help'><code>mlr3tuning::Tuner$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="print"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-print'><code>mlr3tuning::Tuner$print()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="TunerAsyncFromOptimizerAsync" data-id="optimize"><a href='../../mlr3tuning/html/TunerAsyncFromOptimizerAsync.html#method-TunerAsyncFromOptimizerAsync-optimize'><code>mlr3tuning::TunerAsyncFromOptimizerAsync$optimize()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-TunerAsyncGridSearch-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerAsyncGridSearch$new()</pre></div>


<hr>
<a id="method-TunerAsyncGridSearch-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerAsyncGridSearch$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other TunerAsync: 
<code><a href="#topic+mlr_tuners_async_design_points">mlr_tuners_async_design_points</a></code>,
<code><a href="#topic+mlr_tuners_async_random_search">mlr_tuners_async_random_search</a></code>
</p>

<hr>
<h2 id='mlr_tuners_async_random_search'>Hyperparameter Tuning with Asynchronous Random Search</h2><span id='topic+mlr_tuners_async_random_search'></span><span id='topic+TunerAsyncRandomSearch'></span>

<h3>Description</h3>

<p>Subclass for asynchronous random search tuning.
</p>


<h3>Details</h3>

<p>The random points are sampled by <code><a href="paradox.html#topic+generate_design_random">paradox::generate_design_random()</a></code>.
</p>


<h3>Dictionary</h3>

<p>This <a href="#topic+Tuner">Tuner</a> can be instantiated with the associated sugar function <code><a href="#topic+tnr">tnr()</a></code>:
</p>
<div class="sourceCode"><pre>tnr("async_random_search")
</pre></div>


<h3>Super classes</h3>

<p><code><a href="#topic+Tuner">mlr3tuning::Tuner</a></code> -&gt; <code><a href="#topic+TunerAsync">mlr3tuning::TunerAsync</a></code> -&gt; <code><a href="#topic+TunerAsyncFromOptimizerAsync">mlr3tuning::TunerAsyncFromOptimizerAsync</a></code> -&gt; <code>TunerAsyncRandomSearch</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TunerAsyncRandomSearch-new"><code>TunerAsyncRandomSearch$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TunerAsyncRandomSearch-clone"><code>TunerAsyncRandomSearch$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="format"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-format'><code>mlr3tuning::Tuner$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="help"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-help'><code>mlr3tuning::Tuner$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="print"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-print'><code>mlr3tuning::Tuner$print()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="TunerAsyncFromOptimizerAsync" data-id="optimize"><a href='../../mlr3tuning/html/TunerAsyncFromOptimizerAsync.html#method-TunerAsyncFromOptimizerAsync-optimize'><code>mlr3tuning::TunerAsyncFromOptimizerAsync$optimize()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-TunerAsyncRandomSearch-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerAsyncRandomSearch$new()</pre></div>


<hr>
<a id="method-TunerAsyncRandomSearch-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerAsyncRandomSearch$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Source</h3>

<p>Bergstra J, Bengio Y (2012).
&ldquo;Random Search for Hyper-Parameter Optimization.&rdquo;
<em>Journal of Machine Learning Research</em>, <b>13</b>(10), 281&ndash;305.
<a href="https://jmlr.csail.mit.edu/papers/v13/bergstra12a.html">https://jmlr.csail.mit.edu/papers/v13/bergstra12a.html</a>.
</p>


<h3>See Also</h3>

<p>Other TunerAsync: 
<code><a href="#topic+mlr_tuners_async_design_points">mlr_tuners_async_design_points</a></code>,
<code><a href="#topic+mlr_tuners_async_grid_search">mlr_tuners_async_grid_search</a></code>
</p>

<hr>
<h2 id='mlr_tuners_cmaes'>Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy</h2><span id='topic+mlr_tuners_cmaes'></span><span id='topic+TunerBatchCmaes'></span>

<h3>Description</h3>

<p>Subclass for Covariance Matrix Adaptation Evolution Strategy (CMA-ES).
Calls <code><a href="adagio.html#topic+cmaes">adagio::pureCMAES()</a></code> from package <a href="https://CRAN.R-project.org/package=adagio"><span class="pkg">adagio</span></a>.
</p>


<h3>Dictionary</h3>

<p>This <a href="#topic+Tuner">Tuner</a> can be instantiated with the associated sugar function <code><a href="#topic+tnr">tnr()</a></code>:
</p>
<div class="sourceCode"><pre>tnr("cmaes")
</pre></div>


<h3>Control Parameters</h3>


<dl>
<dt><code>start_values</code></dt><dd><p><code>character(1)</code><br />
Create <code>random</code> start values or based on <code>center</code> of search space?
In the latter case, it is the center of the parameters before a trafo is applied.</p>
</dd>
</dl>

<p>For the meaning of the control parameters, see <code><a href="adagio.html#topic+cmaes">adagio::pureCMAES()</a></code>.
Note that we have removed all control parameters which refer to the termination of the algorithm and where our terminators allow to obtain the same behavior.
</p>


<h3>Progress Bars</h3>

<p><code style="white-space: pre;">&#8288;$optimize()&#8288;</code> supports progress bars via the package <a href="https://CRAN.R-project.org/package=progressr"><span class="pkg">progressr</span></a>
combined with a <a href="bbotk.html#topic+Terminator">bbotk::Terminator</a>. Simply wrap the function in
<code>progressr::with_progress()</code> to enable them. We recommend to use package
<a href="https://CRAN.R-project.org/package=progress"><span class="pkg">progress</span></a> as backend; enable with <code>progressr::handlers("progress")</code>.
</p>


<h3>Logging</h3>

<p>All <a href="#topic+Tuner">Tuner</a>s use a logger (as implemented in <a href="https://CRAN.R-project.org/package=lgr"><span class="pkg">lgr</span></a>) from package
<a href="https://CRAN.R-project.org/package=bbotk"><span class="pkg">bbotk</span></a>.
Use <code>lgr::get_logger("bbotk")</code> to access and control the logger.
</p>


<h3>Optimizer</h3>

<p>This <a href="#topic+Tuner">Tuner</a> is based on <a href="bbotk.html#topic+mlr_optimizers_cmaes">bbotk::OptimizerBatchCmaes</a> which can be applied on any black box optimization problem.
See also the documentation of <a href="https://bbotk.mlr-org.com/">bbotk</a>.
</p>


<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li><p> An overview of all tuners can be found on our <a href="https://mlr-org.com/tuners.html">website</a>.
</p>
</li>
<li><p> Learn more about <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-tuner">tuners</a>.
</p>
</li></ul>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>

<ul>
<li><p> Use the <a href="https://mlr-org.com/gallery/series/2023-01-15-hyperband-xgboost/">Hyperband</a> optimizer with different budget parameters.
</p>
</li></ul>



<h3>Super classes</h3>

<p><code><a href="#topic+Tuner">mlr3tuning::Tuner</a></code> -&gt; <code><a href="#topic+TunerBatch">mlr3tuning::TunerBatch</a></code> -&gt; <code><a href="#topic+TunerBatchFromOptimizerBatch">mlr3tuning::TunerBatchFromOptimizerBatch</a></code> -&gt; <code>TunerBatchCmaes</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TunerBatchCmaes-new"><code>TunerBatchCmaes$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TunerBatchCmaes-clone"><code>TunerBatchCmaes$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="format"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-format'><code>mlr3tuning::Tuner$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="help"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-help'><code>mlr3tuning::Tuner$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="print"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-print'><code>mlr3tuning::Tuner$print()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="TunerBatchFromOptimizerBatch" data-id="optimize"><a href='../../mlr3tuning/html/TunerBatchFromOptimizerBatch.html#method-TunerBatchFromOptimizerBatch-optimize'><code>mlr3tuning::TunerBatchFromOptimizerBatch$optimize()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-TunerBatchCmaes-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerBatchCmaes$new()</pre></div>


<hr>
<a id="method-TunerBatchCmaes-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerBatchCmaes$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Source</h3>

<p>Hansen N (2016).
&ldquo;The CMA Evolution Strategy: A Tutorial.&rdquo;
1604.00772.
</p>


<h3>See Also</h3>

<p>Other Tuner: 
<code><a href="#topic+Tuner">Tuner</a></code>,
<code><a href="#topic+mlr_tuners">mlr_tuners</a></code>,
<code><a href="#topic+mlr_tuners_design_points">mlr_tuners_design_points</a></code>,
<code><a href="#topic+mlr_tuners_gensa">mlr_tuners_gensa</a></code>,
<code><a href="#topic+mlr_tuners_grid_search">mlr_tuners_grid_search</a></code>,
<code><a href="#topic+mlr_tuners_internal">mlr_tuners_internal</a></code>,
<code><a href="#topic+mlr_tuners_irace">mlr_tuners_irace</a></code>,
<code><a href="#topic+mlr_tuners_nloptr">mlr_tuners_nloptr</a></code>,
<code><a href="#topic+mlr_tuners_random_search">mlr_tuners_random_search</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Hyperparameter Optimization

# load learner and set search space
learner = lrn("classif.rpart",
  cp = to_tune(1e-04, 1e-1, logscale = TRUE),
  minsplit = to_tune(p_dbl(2, 128, trafo = as.integer)),
  minbucket = to_tune(p_dbl(1, 64, trafo = as.integer))
)

# run hyperparameter tuning on the Palmer Penguins data set
instance = tune(
  tuner = tnr("cmaes"),
  task = tsk("penguins"),
  learner = learner,
  resampling = rsmp("holdout"),
  measure = msr("classif.ce"),
  term_evals = 10)

# best performing hyperparameter configuration
instance$result

# all evaluated hyperparameter configuration
as.data.table(instance$archive)

# fit final model on complete data set
learner$param_set$values = instance$result_learner_param_vals
learner$train(tsk("penguins"))
</code></pre>

<hr>
<h2 id='mlr_tuners_design_points'>Hyperparameter Tuning with Design Points</h2><span id='topic+mlr_tuners_design_points'></span><span id='topic+TunerBatchDesignPoints'></span>

<h3>Description</h3>

<p>Subclass for tuning w.r.t. fixed design points.
</p>
<p>We simply search over a set of points fully specified by the user.
The points in the design are evaluated in order as given.
</p>


<h3>Dictionary</h3>

<p>This <a href="#topic+Tuner">Tuner</a> can be instantiated with the associated sugar function <code><a href="#topic+tnr">tnr()</a></code>:
</p>
<div class="sourceCode"><pre>tnr("design_points")
</pre></div>


<h3>Parallelization</h3>

<p>In order to support general termination criteria and parallelization, we
evaluate points in a batch-fashion of size <code>batch_size</code>. Larger batches mean
we can parallelize more, smaller batches imply a more fine-grained checking
of termination criteria. A batch contains of <code>batch_size</code> times <code>resampling$iters</code> jobs.
E.g., if you set a batch size of 10 points and do a 5-fold cross validation, you can
utilize up to 50 cores.
</p>
<p>Parallelization is supported via package <a href="https://CRAN.R-project.org/package=future"><span class="pkg">future</span></a> (see <code><a href="mlr3.html#topic+benchmark">mlr3::benchmark()</a></code>'s
section on parallelization for more details).
</p>


<h3>Logging</h3>

<p>All <a href="#topic+Tuner">Tuner</a>s use a logger (as implemented in <a href="https://CRAN.R-project.org/package=lgr"><span class="pkg">lgr</span></a>) from package
<a href="https://CRAN.R-project.org/package=bbotk"><span class="pkg">bbotk</span></a>.
Use <code>lgr::get_logger("bbotk")</code> to access and control the logger.
</p>


<h3>Optimizer</h3>

<p>This <a href="#topic+Tuner">Tuner</a> is based on <a href="bbotk.html#topic+mlr_optimizers_design_points">bbotk::OptimizerBatchDesignPoints</a> which can be applied on any black box optimization problem.
See also the documentation of <a href="https://bbotk.mlr-org.com/">bbotk</a>.
</p>


<h3>Parameters</h3>


<dl>
<dt><code>batch_size</code></dt><dd><p><code>integer(1)</code><br />
Maximum number of configurations to try in a batch.</p>
</dd>
<dt><code>design</code></dt><dd><p><a href="data.table.html#topic+data.table">data.table::data.table</a><br />
Design points to try in search, one per row.</p>
</dd>
</dl>



<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li><p> An overview of all tuners can be found on our <a href="https://mlr-org.com/tuners.html">website</a>.
</p>
</li>
<li><p> Learn more about <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-tuner">tuners</a>.
</p>
</li></ul>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>

<ul>
<li><p> Use the <a href="https://mlr-org.com/gallery/series/2023-01-15-hyperband-xgboost/">Hyperband</a> optimizer with different budget parameters.
</p>
</li></ul>



<h3>Progress Bars</h3>

<p><code style="white-space: pre;">&#8288;$optimize()&#8288;</code> supports progress bars via the package <a href="https://CRAN.R-project.org/package=progressr"><span class="pkg">progressr</span></a>
combined with a <a href="bbotk.html#topic+Terminator">Terminator</a>. Simply wrap the function in
<code>progressr::with_progress()</code> to enable them. We recommend to use package
<a href="https://CRAN.R-project.org/package=progress"><span class="pkg">progress</span></a> as backend; enable with <code>progressr::handlers("progress")</code>.
</p>


<h3>Super classes</h3>

<p><code><a href="#topic+Tuner">mlr3tuning::Tuner</a></code> -&gt; <code><a href="#topic+TunerBatch">mlr3tuning::TunerBatch</a></code> -&gt; <code><a href="#topic+TunerBatchFromOptimizerBatch">mlr3tuning::TunerBatchFromOptimizerBatch</a></code> -&gt; <code>TunerBatchDesignPoints</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TunerBatchDesignPoints-new"><code>TunerBatchDesignPoints$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TunerBatchDesignPoints-clone"><code>TunerBatchDesignPoints$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="format"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-format'><code>mlr3tuning::Tuner$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="help"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-help'><code>mlr3tuning::Tuner$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="print"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-print'><code>mlr3tuning::Tuner$print()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="TunerBatchFromOptimizerBatch" data-id="optimize"><a href='../../mlr3tuning/html/TunerBatchFromOptimizerBatch.html#method-TunerBatchFromOptimizerBatch-optimize'><code>mlr3tuning::TunerBatchFromOptimizerBatch$optimize()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-TunerBatchDesignPoints-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerBatchDesignPoints$new()</pre></div>


<hr>
<a id="method-TunerBatchDesignPoints-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerBatchDesignPoints$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Package <a href="https://CRAN.R-project.org/package=mlr3hyperband"><span class="pkg">mlr3hyperband</span></a> for hyperband tuning.
</p>
<p>Other Tuner: 
<code><a href="#topic+Tuner">Tuner</a></code>,
<code><a href="#topic+mlr_tuners">mlr_tuners</a></code>,
<code><a href="#topic+mlr_tuners_cmaes">mlr_tuners_cmaes</a></code>,
<code><a href="#topic+mlr_tuners_gensa">mlr_tuners_gensa</a></code>,
<code><a href="#topic+mlr_tuners_grid_search">mlr_tuners_grid_search</a></code>,
<code><a href="#topic+mlr_tuners_internal">mlr_tuners_internal</a></code>,
<code><a href="#topic+mlr_tuners_irace">mlr_tuners_irace</a></code>,
<code><a href="#topic+mlr_tuners_nloptr">mlr_tuners_nloptr</a></code>,
<code><a href="#topic+mlr_tuners_random_search">mlr_tuners_random_search</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Hyperparameter Optimization

# load learner and set search space
learner = lrn("classif.rpart",
  cp = to_tune(1e-04, 1e-1),
  minsplit = to_tune(2, 128),
  minbucket = to_tune(1, 64)
)

# create design
design = mlr3misc::rowwise_table(
  ~cp,   ~minsplit,  ~minbucket,
  0.1,   2,          64,
  0.01,  64,         32,
  0.001, 128,        1
)

# run hyperparameter tuning on the Palmer Penguins data set
instance = tune(
  tuner = tnr("design_points", design = design),
  task = tsk("penguins"),
  learner = learner,
  resampling = rsmp("holdout"),
  measure = msr("classif.ce")
)

# best performing hyperparameter configuration
instance$result

# all evaluated hyperparameter configuration
as.data.table(instance$archive)

# fit final model on complete data set
learner$param_set$values = instance$result_learner_param_vals
learner$train(tsk("penguins"))
</code></pre>

<hr>
<h2 id='mlr_tuners_gensa'>Hyperparameter Tuning with Generalized Simulated Annealing</h2><span id='topic+mlr_tuners_gensa'></span><span id='topic+TunerBatchGenSA'></span>

<h3>Description</h3>

<p>Subclass for generalized simulated annealing tuning.
Calls <code><a href="GenSA.html#topic+GenSA">GenSA::GenSA()</a></code> from package <a href="https://CRAN.R-project.org/package=GenSA"><span class="pkg">GenSA</span></a>.
</p>


<h3>Details</h3>

<p>In contrast to the <code><a href="GenSA.html#topic+GenSA">GenSA::GenSA()</a></code> defaults, we set <code>smooth = FALSE</code> as a default.
</p>


<h3>Dictionary</h3>

<p>This <a href="#topic+Tuner">Tuner</a> can be instantiated with the associated sugar function <code><a href="#topic+tnr">tnr()</a></code>:
</p>
<div class="sourceCode"><pre>tnr("gensa")
</pre></div>


<h3>Parallelization</h3>

<p>In order to support general termination criteria and parallelization, we
evaluate points in a batch-fashion of size <code>batch_size</code>. Larger batches mean
we can parallelize more, smaller batches imply a more fine-grained checking
of termination criteria. A batch contains of <code>batch_size</code> times <code>resampling$iters</code> jobs.
E.g., if you set a batch size of 10 points and do a 5-fold cross validation, you can
utilize up to 50 cores.
</p>
<p>Parallelization is supported via package <a href="https://CRAN.R-project.org/package=future"><span class="pkg">future</span></a> (see <code><a href="mlr3.html#topic+benchmark">mlr3::benchmark()</a></code>'s
section on parallelization for more details).
</p>


<h3>Logging</h3>

<p>All <a href="#topic+Tuner">Tuner</a>s use a logger (as implemented in <a href="https://CRAN.R-project.org/package=lgr"><span class="pkg">lgr</span></a>) from package
<a href="https://CRAN.R-project.org/package=bbotk"><span class="pkg">bbotk</span></a>.
Use <code>lgr::get_logger("bbotk")</code> to access and control the logger.
</p>


<h3>Optimizer</h3>

<p>This <a href="#topic+Tuner">Tuner</a> is based on <a href="bbotk.html#topic+mlr_optimizers_gensa">bbotk::OptimizerBatchGenSA</a> which can be applied on any black box optimization problem.
See also the documentation of <a href="https://bbotk.mlr-org.com/">bbotk</a>.
</p>


<h3>Parameters</h3>


<dl>
<dt><code>smooth</code></dt><dd><p><code>logical(1)</code></p>
</dd>
<dt><code>temperature</code></dt><dd><p><code>numeric(1)</code></p>
</dd>
<dt><code>acceptance.param</code></dt><dd><p><code>numeric(1)</code></p>
</dd>
<dt><code>verbose</code></dt><dd><p><code>logical(1)</code></p>
</dd>
<dt><code>trace.mat</code></dt><dd><p><code>logical(1)</code></p>
</dd>
</dl>

<p>For the meaning of the control parameters, see <code><a href="GenSA.html#topic+GenSA">GenSA::GenSA()</a></code>. Note that we
have removed all control parameters which refer to the termination of the
algorithm and where our terminators allow to obtain the same behavior.
</p>
<p>In contrast to the <code><a href="GenSA.html#topic+GenSA">GenSA::GenSA()</a></code> defaults, we set <code>trace.mat = FALSE</code>.
Note that <code><a href="GenSA.html#topic+GenSA">GenSA::GenSA()</a></code> uses <code>smooth = TRUE</code> as a default.
In the case of using this optimizer for Hyperparameter Optimization you may
want to set <code>smooth = FALSE</code>.
</p>


<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li><p> An overview of all tuners can be found on our <a href="https://mlr-org.com/tuners.html">website</a>.
</p>
</li>
<li><p> Learn more about <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-tuner">tuners</a>.
</p>
</li></ul>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>

<ul>
<li><p> Use the <a href="https://mlr-org.com/gallery/series/2023-01-15-hyperband-xgboost/">Hyperband</a> optimizer with different budget parameters.
</p>
</li></ul>



<h3>Progress Bars</h3>

<p><code style="white-space: pre;">&#8288;$optimize()&#8288;</code> supports progress bars via the package <a href="https://CRAN.R-project.org/package=progressr"><span class="pkg">progressr</span></a>
combined with a <a href="bbotk.html#topic+Terminator">Terminator</a>. Simply wrap the function in
<code>progressr::with_progress()</code> to enable them. We recommend to use package
<a href="https://CRAN.R-project.org/package=progress"><span class="pkg">progress</span></a> as backend; enable with <code>progressr::handlers("progress")</code>.
</p>


<h3>Super classes</h3>

<p><code><a href="#topic+Tuner">mlr3tuning::Tuner</a></code> -&gt; <code><a href="#topic+TunerBatch">mlr3tuning::TunerBatch</a></code> -&gt; <code><a href="#topic+TunerBatchFromOptimizerBatch">mlr3tuning::TunerBatchFromOptimizerBatch</a></code> -&gt; <code>TunerBatchGenSA</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TunerBatchGenSA-new"><code>TunerBatchGenSA$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TunerBatchGenSA-clone"><code>TunerBatchGenSA$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="format"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-format'><code>mlr3tuning::Tuner$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="help"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-help'><code>mlr3tuning::Tuner$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="print"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-print'><code>mlr3tuning::Tuner$print()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="TunerBatchFromOptimizerBatch" data-id="optimize"><a href='../../mlr3tuning/html/TunerBatchFromOptimizerBatch.html#method-TunerBatchFromOptimizerBatch-optimize'><code>mlr3tuning::TunerBatchFromOptimizerBatch$optimize()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-TunerBatchGenSA-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerBatchGenSA$new()</pre></div>


<hr>
<a id="method-TunerBatchGenSA-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerBatchGenSA$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Source</h3>

<p>Tsallis C, Stariolo DA (1996).
&ldquo;Generalized simulated annealing.&rdquo;
<em>Physica A: Statistical Mechanics and its Applications</em>, <b>233</b>(1-2), 395&ndash;406.
<a href="https://doi.org/10.1016/s0378-4371%2896%2900271-3">doi:10.1016/s0378-4371(96)00271-3</a>.
</p>
<p>Xiang Y, Gubian S, Suomela B, Hoeng J (2013).
&ldquo;Generalized Simulated Annealing for Global Optimization: The GenSA Package.&rdquo;
<em>The R Journal</em>, <b>5</b>(1), 13.
<a href="https://doi.org/10.32614/rj-2013-002">doi:10.32614/rj-2013-002</a>.
</p>


<h3>See Also</h3>

<p>Other Tuner: 
<code><a href="#topic+Tuner">Tuner</a></code>,
<code><a href="#topic+mlr_tuners">mlr_tuners</a></code>,
<code><a href="#topic+mlr_tuners_cmaes">mlr_tuners_cmaes</a></code>,
<code><a href="#topic+mlr_tuners_design_points">mlr_tuners_design_points</a></code>,
<code><a href="#topic+mlr_tuners_grid_search">mlr_tuners_grid_search</a></code>,
<code><a href="#topic+mlr_tuners_internal">mlr_tuners_internal</a></code>,
<code><a href="#topic+mlr_tuners_irace">mlr_tuners_irace</a></code>,
<code><a href="#topic+mlr_tuners_nloptr">mlr_tuners_nloptr</a></code>,
<code><a href="#topic+mlr_tuners_random_search">mlr_tuners_random_search</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Hyperparameter Optimization

# load learner and set search space
learner = lrn("classif.rpart",
  cp = to_tune(1e-04, 1e-1, logscale = TRUE)
)

# run hyperparameter tuning on the Palmer Penguins data set
instance = tune(
  tuner = tnr("gensa"),
  task = tsk("penguins"),
  learner = learner,
  resampling = rsmp("holdout"),
  measure = msr("classif.ce"),
  term_evals = 10
)

# best performing hyperparameter configuration
instance$result

# all evaluated hyperparameter configuration
as.data.table(instance$archive)

# fit final model on complete data set
learner$param_set$values = instance$result_learner_param_vals
learner$train(tsk("penguins"))
</code></pre>

<hr>
<h2 id='mlr_tuners_grid_search'>Hyperparameter Tuning with Grid Search</h2><span id='topic+mlr_tuners_grid_search'></span><span id='topic+TunerBatchGridSearch'></span>

<h3>Description</h3>

<p>Subclass for grid search tuning.
</p>


<h3>Details</h3>

<p>The grid is constructed as a Cartesian product over discretized values per parameter, see <code><a href="paradox.html#topic+generate_design_grid">paradox::generate_design_grid()</a></code>.
If the learner supports hotstarting, the grid is sorted by the hotstart parameter (see also <a href="mlr3.html#topic+HotstartStack">mlr3::HotstartStack</a>).
If not, the points of the grid are evaluated in a  random order.
</p>


<h3>Dictionary</h3>

<p>This <a href="#topic+Tuner">Tuner</a> can be instantiated with the associated sugar function <code><a href="#topic+tnr">tnr()</a></code>:
</p>
<div class="sourceCode"><pre>tnr("grid_search")
</pre></div>


<h3>Control Parameters</h3>


<dl>
<dt><code>resolution</code></dt><dd><p><code>integer(1)</code><br />
Resolution of the grid, see <code><a href="paradox.html#topic+generate_design_grid">paradox::generate_design_grid()</a></code>.</p>
</dd>
<dt><code>param_resolutions</code></dt><dd><p>named <code>integer()</code><br />
Resolution per parameter, named by parameter ID, see <code><a href="paradox.html#topic+generate_design_grid">paradox::generate_design_grid()</a></code>.</p>
</dd>
<dt><code>batch_size</code></dt><dd><p><code>integer(1)</code><br />
Maximum number of points to try in a batch.</p>
</dd>
</dl>



<h3>Progress Bars</h3>

<p><code style="white-space: pre;">&#8288;$optimize()&#8288;</code> supports progress bars via the package <a href="https://CRAN.R-project.org/package=progressr"><span class="pkg">progressr</span></a>
combined with a <a href="bbotk.html#topic+Terminator">bbotk::Terminator</a>. Simply wrap the function in
<code>progressr::with_progress()</code> to enable them. We recommend to use package
<a href="https://CRAN.R-project.org/package=progress"><span class="pkg">progress</span></a> as backend; enable with <code>progressr::handlers("progress")</code>.
</p>


<h3>Parallelization</h3>

<p>In order to support general termination criteria and parallelization, we
evaluate points in a batch-fashion of size <code>batch_size</code>. Larger batches mean
we can parallelize more, smaller batches imply a more fine-grained checking
of termination criteria. A batch contains of <code>batch_size</code> times <code>resampling$iters</code> jobs.
E.g., if you set a batch size of 10 points and do a 5-fold cross validation, you can
utilize up to 50 cores.
</p>
<p>Parallelization is supported via package <a href="https://CRAN.R-project.org/package=future"><span class="pkg">future</span></a> (see <code><a href="mlr3.html#topic+benchmark">mlr3::benchmark()</a></code>'s
section on parallelization for more details).
</p>


<h3>Logging</h3>

<p>All <a href="#topic+Tuner">Tuner</a>s use a logger (as implemented in <a href="https://CRAN.R-project.org/package=lgr"><span class="pkg">lgr</span></a>) from package
<a href="https://CRAN.R-project.org/package=bbotk"><span class="pkg">bbotk</span></a>.
Use <code>lgr::get_logger("bbotk")</code> to access and control the logger.
</p>


<h3>Optimizer</h3>

<p>This <a href="#topic+Tuner">Tuner</a> is based on <a href="bbotk.html#topic+mlr_optimizers_grid_search">bbotk::OptimizerBatchGridSearch</a> which can be applied on any black box optimization problem.
See also the documentation of <a href="https://bbotk.mlr-org.com/">bbotk</a>.
</p>


<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li><p> An overview of all tuners can be found on our <a href="https://mlr-org.com/tuners.html">website</a>.
</p>
</li>
<li><p> Learn more about <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-tuner">tuners</a>.
</p>
</li></ul>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>

<ul>
<li><p> Use the <a href="https://mlr-org.com/gallery/series/2023-01-15-hyperband-xgboost/">Hyperband</a> optimizer with different budget parameters.
</p>
</li></ul>



<h3>Super classes</h3>

<p><code><a href="#topic+Tuner">mlr3tuning::Tuner</a></code> -&gt; <code><a href="#topic+TunerBatch">mlr3tuning::TunerBatch</a></code> -&gt; <code><a href="#topic+TunerBatchFromOptimizerBatch">mlr3tuning::TunerBatchFromOptimizerBatch</a></code> -&gt; <code>TunerBatchGridSearch</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TunerBatchGridSearch-new"><code>TunerBatchGridSearch$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TunerBatchGridSearch-clone"><code>TunerBatchGridSearch$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="format"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-format'><code>mlr3tuning::Tuner$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="help"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-help'><code>mlr3tuning::Tuner$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="print"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-print'><code>mlr3tuning::Tuner$print()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="TunerBatchFromOptimizerBatch" data-id="optimize"><a href='../../mlr3tuning/html/TunerBatchFromOptimizerBatch.html#method-TunerBatchFromOptimizerBatch-optimize'><code>mlr3tuning::TunerBatchFromOptimizerBatch$optimize()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-TunerBatchGridSearch-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerBatchGridSearch$new()</pre></div>


<hr>
<a id="method-TunerBatchGridSearch-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerBatchGridSearch$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other Tuner: 
<code><a href="#topic+Tuner">Tuner</a></code>,
<code><a href="#topic+mlr_tuners">mlr_tuners</a></code>,
<code><a href="#topic+mlr_tuners_cmaes">mlr_tuners_cmaes</a></code>,
<code><a href="#topic+mlr_tuners_design_points">mlr_tuners_design_points</a></code>,
<code><a href="#topic+mlr_tuners_gensa">mlr_tuners_gensa</a></code>,
<code><a href="#topic+mlr_tuners_internal">mlr_tuners_internal</a></code>,
<code><a href="#topic+mlr_tuners_irace">mlr_tuners_irace</a></code>,
<code><a href="#topic+mlr_tuners_nloptr">mlr_tuners_nloptr</a></code>,
<code><a href="#topic+mlr_tuners_random_search">mlr_tuners_random_search</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Hyperparameter Optimization

# load learner and set search space
learner = lrn("classif.rpart",
  cp = to_tune(1e-04, 1e-1, logscale = TRUE)
)

# run hyperparameter tuning on the Palmer Penguins data set
instance = tune(
  tuner = tnr("grid_search"),
  task = tsk("penguins"),
  learner = learner,
  resampling = rsmp("holdout"),
  measure = msr("classif.ce"),
  term_evals = 10
)

# best performing hyperparameter configuration
instance$result

# all evaluated hyperparameter configuration
as.data.table(instance$archive)

# fit final model on complete data set
learner$param_set$values = instance$result_learner_param_vals
learner$train(tsk("penguins"))
</code></pre>

<hr>
<h2 id='mlr_tuners_internal'>Hyperparameter Tuning with Internal Tuning</h2><span id='topic+mlr_tuners_internal'></span><span id='topic+TunerBatchInternal'></span>

<h3>Description</h3>

<p>Subclass to conduct only internal hyperparameter tuning for a <a href="mlr3.html#topic+Learner">mlr3::Learner</a>.
</p>


<h3>Dictionary</h3>

<p>This <a href="#topic+Tuner">Tuner</a> can be instantiated with the associated sugar function <code><a href="#topic+tnr">tnr()</a></code>:
</p>
<div class="sourceCode"><pre>tnr("internal")
</pre></div>


<h3>Progress Bars</h3>

<p><code style="white-space: pre;">&#8288;$optimize()&#8288;</code> supports progress bars via the package <a href="https://CRAN.R-project.org/package=progressr"><span class="pkg">progressr</span></a>
combined with a <a href="bbotk.html#topic+Terminator">bbotk::Terminator</a>. Simply wrap the function in
<code>progressr::with_progress()</code> to enable them. We recommend to use package
<a href="https://CRAN.R-project.org/package=progress"><span class="pkg">progress</span></a> as backend; enable with <code>progressr::handlers("progress")</code>.
</p>


<h3>Logging</h3>

<p>All <a href="#topic+Tuner">Tuner</a>s use a logger (as implemented in <a href="https://CRAN.R-project.org/package=lgr"><span class="pkg">lgr</span></a>) from package
<a href="https://CRAN.R-project.org/package=bbotk"><span class="pkg">bbotk</span></a>.
Use <code>lgr::get_logger("bbotk")</code> to access and control the logger.
</p>


<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li><p> An overview of all tuners can be found on our <a href="https://mlr-org.com/tuners.html">website</a>.
</p>
</li>
<li><p> Learn more about <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-tuner">tuners</a>.
</p>
</li></ul>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>

<ul>
<li><p> Use the <a href="https://mlr-org.com/gallery/series/2023-01-15-hyperband-xgboost/">Hyperband</a> optimizer with different budget parameters.
</p>
</li></ul>



<h3>Super classes</h3>

<p><code><a href="#topic+Tuner">mlr3tuning::Tuner</a></code> -&gt; <code><a href="#topic+TunerBatch">mlr3tuning::TunerBatch</a></code> -&gt; <code>TunerBatchInternal</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TunerBatchInternal-new"><code>TunerBatchInternal$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TunerBatchInternal-clone"><code>TunerBatchInternal$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="format"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-format'><code>mlr3tuning::Tuner$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="help"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-help'><code>mlr3tuning::Tuner$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="print"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-print'><code>mlr3tuning::Tuner$print()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="TunerBatch" data-id="optimize"><a href='../../mlr3tuning/html/TunerBatch.html#method-TunerBatch-optimize'><code>mlr3tuning::TunerBatch$optimize()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-TunerBatchInternal-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerBatchInternal$new()</pre></div>


<hr>
<a id="method-TunerBatchInternal-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerBatchInternal$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Note</h3>

<p>The selected <a href="mlr3.html#topic+Measure">mlr3::Measure</a> does not influence the tuning result.
To change the loss-function for the internal tuning, consult the hyperparameter documentation of the tuned <a href="mlr3.html#topic+Learner">mlr3::Learner</a>.
</p>


<h3>See Also</h3>

<p>Other Tuner: 
<code><a href="#topic+Tuner">Tuner</a></code>,
<code><a href="#topic+mlr_tuners">mlr_tuners</a></code>,
<code><a href="#topic+mlr_tuners_cmaes">mlr_tuners_cmaes</a></code>,
<code><a href="#topic+mlr_tuners_design_points">mlr_tuners_design_points</a></code>,
<code><a href="#topic+mlr_tuners_gensa">mlr_tuners_gensa</a></code>,
<code><a href="#topic+mlr_tuners_grid_search">mlr_tuners_grid_search</a></code>,
<code><a href="#topic+mlr_tuners_irace">mlr_tuners_irace</a></code>,
<code><a href="#topic+mlr_tuners_nloptr">mlr_tuners_nloptr</a></code>,
<code><a href="#topic+mlr_tuners_random_search">mlr_tuners_random_search</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(mlr3learners)

# Retrieve task
task = tsk("pima")

# Load learner and set search space
learner = lrn("classif.xgboost",
  nrounds = to_tune(upper = 1000, internal = TRUE),
  early_stopping_rounds = 10,
  validate = "test"
)

# Internal hyperparameter tuning on the pima indians diabetes data set
instance = tune(
  tnr("internal"),
  tsk("iris"),
  learner,
  rsmp("cv", folds = 3),
  msr("classif.ce")
)

# best performing hyperparameter configuration
instance$result_learner_param_vals

instance$result_learner_param_vals$internal_tuned_values

</code></pre>

<hr>
<h2 id='mlr_tuners_irace'>Hyperparameter Tuning with Iterated Racing.</h2><span id='topic+mlr_tuners_irace'></span><span id='topic+TunerBatchIrace'></span>

<h3>Description</h3>

<p>Subclass for iterated racing.
Calls <code><a href="irace.html#topic+irace">irace::irace()</a></code> from package <a href="https://CRAN.R-project.org/package=irace"><span class="pkg">irace</span></a>.
</p>


<h3>Dictionary</h3>

<p>This <a href="#topic+Tuner">Tuner</a> can be instantiated with the associated sugar function <code><a href="#topic+tnr">tnr()</a></code>:
</p>
<div class="sourceCode"><pre>tnr("irace")
</pre></div>


<h3>Control Parameters</h3>


<dl>
<dt><code>n_instances</code></dt><dd><p><code>integer(1)</code><br />
Number of resampling instances.</p>
</dd>
</dl>

<p>For the meaning of all other parameters, see <code><a href="irace.html#topic+defaultScenario">irace::defaultScenario()</a></code>. Note
that we have removed all control parameters which refer to the termination of
the algorithm. Use <a href="bbotk.html#topic+mlr_terminators_evals">bbotk::TerminatorEvals</a> instead. Other terminators do not work
with <code>TunerIrace</code>.
</p>


<h3>Archive</h3>

<p>The <a href="#topic+ArchiveBatchTuning">ArchiveBatchTuning</a> holds the following additional columns:
</p>

<ul>
<li> <p><code>"race"</code> (<code>integer(1)</code>)<br />
Race iteration.
</p>
</li>
<li> <p><code>"step"</code> (<code>integer(1)</code>)<br />
Step number of race.
</p>
</li>
<li> <p><code>"instance"</code> (<code>integer(1)</code>)<br />
Identifies resampling instances across races and steps.
</p>
</li>
<li> <p><code>"configuration"</code> (<code>integer(1)</code>)<br />
Identifies configurations across races and steps.
</p>
</li></ul>



<h3>Result</h3>

<p>The tuning result (<code>instance$result</code>) is the best-performing elite of the final race.
The reported performance is the average performance estimated on all used instances.
</p>


<h3>Progress Bars</h3>

<p><code style="white-space: pre;">&#8288;$optimize()&#8288;</code> supports progress bars via the package <a href="https://CRAN.R-project.org/package=progressr"><span class="pkg">progressr</span></a>
combined with a <a href="bbotk.html#topic+Terminator">bbotk::Terminator</a>. Simply wrap the function in
<code>progressr::with_progress()</code> to enable them. We recommend to use package
<a href="https://CRAN.R-project.org/package=progress"><span class="pkg">progress</span></a> as backend; enable with <code>progressr::handlers("progress")</code>.
</p>


<h3>Logging</h3>

<p>All <a href="#topic+Tuner">Tuner</a>s use a logger (as implemented in <a href="https://CRAN.R-project.org/package=lgr"><span class="pkg">lgr</span></a>) from package
<a href="https://CRAN.R-project.org/package=bbotk"><span class="pkg">bbotk</span></a>.
Use <code>lgr::get_logger("bbotk")</code> to access and control the logger.
</p>


<h3>Optimizer</h3>

<p>This <a href="#topic+Tuner">Tuner</a> is based on <a href="bbotk.html#topic+mlr_optimizers_irace">bbotk::OptimizerBatchIrace</a> which can be applied on any black box optimization problem.
See also the documentation of <a href="https://bbotk.mlr-org.com/">bbotk</a>.
</p>


<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li><p> An overview of all tuners can be found on our <a href="https://mlr-org.com/tuners.html">website</a>.
</p>
</li>
<li><p> Learn more about <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-tuner">tuners</a>.
</p>
</li></ul>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>

<ul>
<li><p> Use the <a href="https://mlr-org.com/gallery/series/2023-01-15-hyperband-xgboost/">Hyperband</a> optimizer with different budget parameters.
</p>
</li></ul>



<h3>Super classes</h3>

<p><code><a href="#topic+Tuner">mlr3tuning::Tuner</a></code> -&gt; <code><a href="#topic+TunerBatch">mlr3tuning::TunerBatch</a></code> -&gt; <code><a href="#topic+TunerBatchFromOptimizerBatch">mlr3tuning::TunerBatchFromOptimizerBatch</a></code> -&gt; <code>TunerBatchIrace</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TunerBatchIrace-new"><code>TunerBatchIrace$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TunerBatchIrace-optimize"><code>TunerBatchIrace$optimize()</code></a>
</p>
</li>
<li> <p><a href="#method-TunerBatchIrace-clone"><code>TunerBatchIrace$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="format"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-format'><code>mlr3tuning::Tuner$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="help"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-help'><code>mlr3tuning::Tuner$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="print"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-print'><code>mlr3tuning::Tuner$print()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-TunerBatchIrace-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerBatchIrace$new()</pre></div>


<hr>
<a id="method-TunerBatchIrace-optimize"></a>



<h4>Method <code>optimize()</code></h4>

<p>Performs the tuning on a <a href="#topic+TuningInstanceBatchSingleCrit">TuningInstanceBatchSingleCrit</a> until termination.
The single evaluations and the final results will be written into the
<a href="#topic+ArchiveBatchTuning">ArchiveBatchTuning</a> that resides in the <a href="#topic+TuningInstanceBatchSingleCrit">TuningInstanceBatchSingleCrit</a>. The final
result is returned.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerBatchIrace$optimize(inst)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>inst</code></dt><dd><p>(<a href="#topic+TuningInstanceBatchSingleCrit">TuningInstanceBatchSingleCrit</a>).</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p><a href="data.table.html#topic+data.table">data.table::data.table</a>.
</p>


<hr>
<a id="method-TunerBatchIrace-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerBatchIrace$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Source</h3>

<p>Lopez-Ibanez M, Dubois-Lacoste J, Caceres LP, Birattari M, Stuetzle T (2016).
&ldquo;The irace package: Iterated racing for automatic algorithm configuration.&rdquo;
<em>Operations Research Perspectives</em>, <b>3</b>, 43&ndash;58.
<a href="https://doi.org/10.1016/j.orp.2016.09.002">doi:10.1016/j.orp.2016.09.002</a>.
</p>


<h3>See Also</h3>

<p>Other Tuner: 
<code><a href="#topic+Tuner">Tuner</a></code>,
<code><a href="#topic+mlr_tuners">mlr_tuners</a></code>,
<code><a href="#topic+mlr_tuners_cmaes">mlr_tuners_cmaes</a></code>,
<code><a href="#topic+mlr_tuners_design_points">mlr_tuners_design_points</a></code>,
<code><a href="#topic+mlr_tuners_gensa">mlr_tuners_gensa</a></code>,
<code><a href="#topic+mlr_tuners_grid_search">mlr_tuners_grid_search</a></code>,
<code><a href="#topic+mlr_tuners_internal">mlr_tuners_internal</a></code>,
<code><a href="#topic+mlr_tuners_nloptr">mlr_tuners_nloptr</a></code>,
<code><a href="#topic+mlr_tuners_random_search">mlr_tuners_random_search</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># retrieve task
task = tsk("pima")

# load learner and set search space
learner = lrn("classif.rpart", cp = to_tune(1e-04, 1e-1, logscale = TRUE))

# hyperparameter tuning on the pima indians diabetes data set
instance = tune(
  tuner = tnr("irace"),
  task = task,
  learner = learner,
  resampling = rsmp("holdout"),
  measure = msr("classif.ce"),
  term_evals = 42
)

# best performing hyperparameter configuration
instance$result

# all evaluated hyperparameter configuration
as.data.table(instance$archive)

# fit final model on complete data set
learner$param_set$values = instance$result_learner_param_vals
learner$train(task)

</code></pre>

<hr>
<h2 id='mlr_tuners_nloptr'>Hyperparameter Tuning with Non-linear Optimization</h2><span id='topic+mlr_tuners_nloptr'></span><span id='topic+TunerBatchNLoptr'></span>

<h3>Description</h3>

<p>Subclass for non-linear optimization (NLopt).
Calls <a href="nloptr.html#topic+nloptr">nloptr::nloptr</a> from package <a href="https://CRAN.R-project.org/package=nloptr"><span class="pkg">nloptr</span></a>.
</p>


<h3>Details</h3>

<p>The termination conditions <code>stopval</code>, <code>maxtime</code> and <code>maxeval</code> of <code><a href="nloptr.html#topic+nloptr">nloptr::nloptr()</a></code> are deactivated and replaced by the <a href="bbotk.html#topic+Terminator">bbotk::Terminator</a> subclasses.
The x and function value tolerance termination conditions (<code>xtol_rel = 10^-4</code>, <code>xtol_abs = rep(0.0, length(x0))</code>, <code>ftol_rel = 0.0</code> and <code>ftol_abs = 0.0</code>) are still available and implemented with their package defaults.
To deactivate these conditions, set them to <code>-1</code>.
</p>


<h3>Dictionary</h3>

<p>This <a href="#topic+Tuner">Tuner</a> can be instantiated with the associated sugar function <code><a href="#topic+tnr">tnr()</a></code>:
</p>
<div class="sourceCode"><pre>tnr("nloptr")
</pre></div>


<h3>Logging</h3>

<p>All <a href="#topic+Tuner">Tuner</a>s use a logger (as implemented in <a href="https://CRAN.R-project.org/package=lgr"><span class="pkg">lgr</span></a>) from package
<a href="https://CRAN.R-project.org/package=bbotk"><span class="pkg">bbotk</span></a>.
Use <code>lgr::get_logger("bbotk")</code> to access and control the logger.
</p>


<h3>Optimizer</h3>

<p>This <a href="#topic+Tuner">Tuner</a> is based on <a href="bbotk.html#topic+mlr_optimizers_nloptr">bbotk::OptimizerBatchNLoptr</a> which can be applied on any black box optimization problem.
See also the documentation of <a href="https://bbotk.mlr-org.com/">bbotk</a>.
</p>


<h3>Parameters</h3>


<dl>
<dt><code>algorithm</code></dt><dd><p><code>character(1)</code></p>
</dd>
<dt><code>eval_g_ineq</code></dt><dd><p><code style="white-space: pre;">&#8288;function()&#8288;</code></p>
</dd>
<dt><code>xtol_rel</code></dt><dd><p><code>numeric(1)</code></p>
</dd>
<dt><code>xtol_abs</code></dt><dd><p><code>numeric(1)</code></p>
</dd>
<dt><code>ftol_rel</code></dt><dd><p><code>numeric(1)</code></p>
</dd>
<dt><code>ftol_abs</code></dt><dd><p><code>numeric(1)</code></p>
</dd>
<dt><code>start_values</code></dt><dd><p><code>character(1)</code><br />
Create <code>random</code> start values or based on <code>center</code> of search space? In the
latter case, it is the center of the parameters before a trafo is applied.</p>
</dd>
</dl>

<p>For the meaning of the control parameters, see <code><a href="nloptr.html#topic+nloptr">nloptr::nloptr()</a></code> and
<code><a href="nloptr.html#topic+nloptr.print.options">nloptr::nloptr.print.options()</a></code>.
</p>
<p>The termination conditions <code>stopval</code>, <code>maxtime</code> and <code>maxeval</code> of
<code><a href="nloptr.html#topic+nloptr">nloptr::nloptr()</a></code> are deactivated and replaced by the <a href="bbotk.html#topic+Terminator">Terminator</a>
subclasses. The x and function value tolerance termination conditions
(<code>xtol_rel = 10^-4</code>, <code>xtol_abs = rep(0.0, length(x0))</code>, <code>ftol_rel = 0.0</code> and
<code>ftol_abs = 0.0</code>) are still available and implemented with their package
defaults. To deactivate these conditions, set them to <code>-1</code>.
</p>


<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li><p> An overview of all tuners can be found on our <a href="https://mlr-org.com/tuners.html">website</a>.
</p>
</li>
<li><p> Learn more about <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-tuner">tuners</a>.
</p>
</li></ul>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>

<ul>
<li><p> Use the <a href="https://mlr-org.com/gallery/series/2023-01-15-hyperband-xgboost/">Hyperband</a> optimizer with different budget parameters.
</p>
</li></ul>



<h3>Progress Bars</h3>

<p><code style="white-space: pre;">&#8288;$optimize()&#8288;</code> supports progress bars via the package <a href="https://CRAN.R-project.org/package=progressr"><span class="pkg">progressr</span></a>
combined with a <a href="bbotk.html#topic+Terminator">Terminator</a>. Simply wrap the function in
<code>progressr::with_progress()</code> to enable them. We recommend to use package
<a href="https://CRAN.R-project.org/package=progress"><span class="pkg">progress</span></a> as backend; enable with <code>progressr::handlers("progress")</code>.
</p>


<h3>Super classes</h3>

<p><code><a href="#topic+Tuner">mlr3tuning::Tuner</a></code> -&gt; <code><a href="#topic+TunerBatch">mlr3tuning::TunerBatch</a></code> -&gt; <code><a href="#topic+TunerBatchFromOptimizerBatch">mlr3tuning::TunerBatchFromOptimizerBatch</a></code> -&gt; <code>TunerBatchNLoptr</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TunerBatchNLoptr-new"><code>TunerBatchNLoptr$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TunerBatchNLoptr-clone"><code>TunerBatchNLoptr$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="format"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-format'><code>mlr3tuning::Tuner$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="help"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-help'><code>mlr3tuning::Tuner$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="print"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-print'><code>mlr3tuning::Tuner$print()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="TunerBatchFromOptimizerBatch" data-id="optimize"><a href='../../mlr3tuning/html/TunerBatchFromOptimizerBatch.html#method-TunerBatchFromOptimizerBatch-optimize'><code>mlr3tuning::TunerBatchFromOptimizerBatch$optimize()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-TunerBatchNLoptr-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerBatchNLoptr$new()</pre></div>


<hr>
<a id="method-TunerBatchNLoptr-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerBatchNLoptr$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Source</h3>

<p>Johnson, G S (2020).
&ldquo;The NLopt nonlinear-optimization package.&rdquo;
<a href="https://github.com/stevengj/nlopt">https://github.com/stevengj/nlopt</a>.
</p>


<h3>See Also</h3>

<p>Other Tuner: 
<code><a href="#topic+Tuner">Tuner</a></code>,
<code><a href="#topic+mlr_tuners">mlr_tuners</a></code>,
<code><a href="#topic+mlr_tuners_cmaes">mlr_tuners_cmaes</a></code>,
<code><a href="#topic+mlr_tuners_design_points">mlr_tuners_design_points</a></code>,
<code><a href="#topic+mlr_tuners_gensa">mlr_tuners_gensa</a></code>,
<code><a href="#topic+mlr_tuners_grid_search">mlr_tuners_grid_search</a></code>,
<code><a href="#topic+mlr_tuners_internal">mlr_tuners_internal</a></code>,
<code><a href="#topic+mlr_tuners_irace">mlr_tuners_irace</a></code>,
<code><a href="#topic+mlr_tuners_random_search">mlr_tuners_random_search</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Hyperparameter Optimization


# load learner and set search space
learner = lrn("classif.rpart",
  cp = to_tune(1e-04, 1e-1, logscale = TRUE)
)

# run hyperparameter tuning on the Palmer Penguins data set
instance = tune(
  tuner = tnr("nloptr", algorithm = "NLOPT_LN_BOBYQA"),
  task = tsk("penguins"),
  learner = learner,
  resampling = rsmp("holdout"),
  measure = msr("classif.ce")
)

# best performing hyperparameter configuration
instance$result

# all evaluated hyperparameter configuration
as.data.table(instance$archive)

# fit final model on complete data set
learner$param_set$values = instance$result_learner_param_vals
learner$train(tsk("penguins"))

</code></pre>

<hr>
<h2 id='mlr_tuners_random_search'>Hyperparameter Tuning with Random Search</h2><span id='topic+mlr_tuners_random_search'></span><span id='topic+TunerBatchRandomSearch'></span>

<h3>Description</h3>

<p>Subclass for random search tuning.
</p>


<h3>Details</h3>

<p>The random points are sampled by <code><a href="paradox.html#topic+generate_design_random">paradox::generate_design_random()</a></code>.
</p>


<h3>Dictionary</h3>

<p>This <a href="#topic+Tuner">Tuner</a> can be instantiated with the associated sugar function <code><a href="#topic+tnr">tnr()</a></code>:
</p>
<div class="sourceCode"><pre>tnr("random_search")
</pre></div>


<h3>Parallelization</h3>

<p>In order to support general termination criteria and parallelization, we
evaluate points in a batch-fashion of size <code>batch_size</code>. Larger batches mean
we can parallelize more, smaller batches imply a more fine-grained checking
of termination criteria. A batch contains of <code>batch_size</code> times <code>resampling$iters</code> jobs.
E.g., if you set a batch size of 10 points and do a 5-fold cross validation, you can
utilize up to 50 cores.
</p>
<p>Parallelization is supported via package <a href="https://CRAN.R-project.org/package=future"><span class="pkg">future</span></a> (see <code><a href="mlr3.html#topic+benchmark">mlr3::benchmark()</a></code>'s
section on parallelization for more details).
</p>


<h3>Logging</h3>

<p>All <a href="#topic+Tuner">Tuner</a>s use a logger (as implemented in <a href="https://CRAN.R-project.org/package=lgr"><span class="pkg">lgr</span></a>) from package
<a href="https://CRAN.R-project.org/package=bbotk"><span class="pkg">bbotk</span></a>.
Use <code>lgr::get_logger("bbotk")</code> to access and control the logger.
</p>


<h3>Optimizer</h3>

<p>This <a href="#topic+Tuner">Tuner</a> is based on <a href="bbotk.html#topic+mlr_optimizers_random_search">bbotk::OptimizerBatchRandomSearch</a> which can be applied on any black box optimization problem.
See also the documentation of <a href="https://bbotk.mlr-org.com/">bbotk</a>.
</p>


<h3>Parameters</h3>


<dl>
<dt><code>batch_size</code></dt><dd><p><code>integer(1)</code><br />
Maximum number of points to try in a batch.</p>
</dd>
</dl>



<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li><p> An overview of all tuners can be found on our <a href="https://mlr-org.com/tuners.html">website</a>.
</p>
</li>
<li><p> Learn more about <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-tuner">tuners</a>.
</p>
</li></ul>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>

<ul>
<li><p> Use the <a href="https://mlr-org.com/gallery/series/2023-01-15-hyperband-xgboost/">Hyperband</a> optimizer with different budget parameters.
</p>
</li></ul>



<h3>Progress Bars</h3>

<p><code style="white-space: pre;">&#8288;$optimize()&#8288;</code> supports progress bars via the package <a href="https://CRAN.R-project.org/package=progressr"><span class="pkg">progressr</span></a>
combined with a <a href="bbotk.html#topic+Terminator">Terminator</a>. Simply wrap the function in
<code>progressr::with_progress()</code> to enable them. We recommend to use package
<a href="https://CRAN.R-project.org/package=progress"><span class="pkg">progress</span></a> as backend; enable with <code>progressr::handlers("progress")</code>.
</p>


<h3>Super classes</h3>

<p><code><a href="#topic+Tuner">mlr3tuning::Tuner</a></code> -&gt; <code><a href="#topic+TunerBatch">mlr3tuning::TunerBatch</a></code> -&gt; <code><a href="#topic+TunerBatchFromOptimizerBatch">mlr3tuning::TunerBatchFromOptimizerBatch</a></code> -&gt; <code>TunerBatchRandomSearch</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TunerBatchRandomSearch-new"><code>TunerBatchRandomSearch$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TunerBatchRandomSearch-clone"><code>TunerBatchRandomSearch$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="format"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-format'><code>mlr3tuning::Tuner$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="help"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-help'><code>mlr3tuning::Tuner$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="print"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-print'><code>mlr3tuning::Tuner$print()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="TunerBatchFromOptimizerBatch" data-id="optimize"><a href='../../mlr3tuning/html/TunerBatchFromOptimizerBatch.html#method-TunerBatchFromOptimizerBatch-optimize'><code>mlr3tuning::TunerBatchFromOptimizerBatch$optimize()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-TunerBatchRandomSearch-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerBatchRandomSearch$new()</pre></div>


<hr>
<a id="method-TunerBatchRandomSearch-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerBatchRandomSearch$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Source</h3>

<p>Bergstra J, Bengio Y (2012).
&ldquo;Random Search for Hyper-Parameter Optimization.&rdquo;
<em>Journal of Machine Learning Research</em>, <b>13</b>(10), 281&ndash;305.
<a href="https://jmlr.csail.mit.edu/papers/v13/bergstra12a.html">https://jmlr.csail.mit.edu/papers/v13/bergstra12a.html</a>.
</p>


<h3>See Also</h3>

<p>Package <a href="https://CRAN.R-project.org/package=mlr3hyperband"><span class="pkg">mlr3hyperband</span></a> for hyperband tuning.
</p>
<p>Other Tuner: 
<code><a href="#topic+Tuner">Tuner</a></code>,
<code><a href="#topic+mlr_tuners">mlr_tuners</a></code>,
<code><a href="#topic+mlr_tuners_cmaes">mlr_tuners_cmaes</a></code>,
<code><a href="#topic+mlr_tuners_design_points">mlr_tuners_design_points</a></code>,
<code><a href="#topic+mlr_tuners_gensa">mlr_tuners_gensa</a></code>,
<code><a href="#topic+mlr_tuners_grid_search">mlr_tuners_grid_search</a></code>,
<code><a href="#topic+mlr_tuners_internal">mlr_tuners_internal</a></code>,
<code><a href="#topic+mlr_tuners_irace">mlr_tuners_irace</a></code>,
<code><a href="#topic+mlr_tuners_nloptr">mlr_tuners_nloptr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Hyperparameter Optimization

# load learner and set search space
learner = lrn("classif.rpart",
  cp = to_tune(1e-04, 1e-1, logscale = TRUE)
)

# run hyperparameter tuning on the Palmer Penguins data set
instance = tune(
  tuner = tnr("random_search"),
  task = tsk("penguins"),
  learner = learner,
  resampling = rsmp("holdout"),
  measure = msr("classif.ce"),
  term_evals = 10
)

# best performing hyperparameter configuration
instance$result

# all evaluated hyperparameter configuration
as.data.table(instance$archive)

# fit final model on complete data set
learner$param_set$values = instance$result_learner_param_vals
learner$train(tsk("penguins"))
</code></pre>

<hr>
<h2 id='mlr3tuning_assertions'>Assertion for mlr3tuning objects</h2><span id='topic+mlr3tuning_assertions'></span><span id='topic+assert_tuner'></span><span id='topic+assert_tuners'></span><span id='topic+assert_tuner_async'></span><span id='topic+assert_tuner_batch'></span><span id='topic+assert_tuning_instance'></span><span id='topic+assert_tuning_instance_async'></span><span id='topic+assert_tuning_instance_batch'></span>

<h3>Description</h3>

<p>Most assertion functions ensure the right class attribute, and optionally additional properties.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>assert_tuner(tuner)

assert_tuners(tuners)

assert_tuner_async(tuner)

assert_tuner_batch(tuner)

assert_tuning_instance(inst)

assert_tuning_instance_async(inst)

assert_tuning_instance_batch(inst)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mlr3tuning_assertions_+3A_tuner">tuner</code></td>
<td>
<p>(<a href="#topic+TunerBatch">TunerBatch</a>).</p>
</td></tr>
<tr><td><code id="mlr3tuning_assertions_+3A_tuners">tuners</code></td>
<td>
<p>(list of <a href="#topic+Tuner">Tuner</a>).</p>
</td></tr>
<tr><td><code id="mlr3tuning_assertions_+3A_inst">inst</code></td>
<td>
<p>(<a href="#topic+TuningInstanceBatchSingleCrit">TuningInstanceBatchSingleCrit</a> | <a href="#topic+TuningInstanceBatchMultiCrit">TuningInstanceBatchMultiCrit</a>).</p>
</td></tr>
</table>

<hr>
<h2 id='mlr3tuning.asnyc_mlflow'>MLflow Connector Callback</h2><span id='topic+mlr3tuning.asnyc_mlflow'></span>

<h3>Description</h3>

<p>This <a href="mlr3misc.html#topic+Callback">mlr3misc::Callback</a> logs the hyperparameter configurations and the performance of the configurations to MLflow.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

clbk("mlr3tuning.async_mlflow", tracking_uri = "http://localhost:5000")

## Not run: 
rush::rush_plan(n_workers = 4)

learner = lrn("classif.rpart",
  minsplit = to_tune(2, 128),
  cp = to_tune(1e-04, 1e-1))

instance = TuningInstanceAsyncSingleCrit$new(
  task = tsk("pima"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.ce"),
  terminator = trm("evals", n_evals = 20),
  store_benchmark_result = FALSE,
  callbacks = clbk("mlr3tuning.rush_mlflow", tracking_uri = "http://localhost:8080")
)

tuner = tnr("random_search_v2")
tuner$optimize(instance)

## End(Not run)
</code></pre>

<hr>
<h2 id='mlr3tuning.async_default_configuration'>Default Configuration Callback</h2><span id='topic+mlr3tuning.async_default_configuration'></span>

<h3>Description</h3>

<p>These <a href="#topic+CallbackAsyncTuning">CallbackAsyncTuning</a> and <a href="#topic+CallbackBatchTuning">CallbackBatchTuning</a> evaluate the default hyperparameter values of a learner.
</p>

<hr>
<h2 id='mlr3tuning.async_save_logs'>Save Logs Callback</h2><span id='topic+mlr3tuning.async_save_logs'></span>

<h3>Description</h3>

<p>This <a href="#topic+CallbackAsyncTuning">CallbackAsyncTuning</a> saves the logs of the learners to the archive.
</p>

<hr>
<h2 id='mlr3tuning.backup'>Backup Benchmark Result Callback</h2><span id='topic+mlr3tuning.backup'></span>

<h3>Description</h3>

<p>This <a href="mlr3misc.html#topic+Callback">mlr3misc::Callback</a> writes the <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a> after each batch to disk.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>clbk("mlr3tuning.backup", path = "backup.rds")

# tune classification tree on the pima data set
instance = tune(
  tuner = tnr("random_search", batch_size = 2),
  task = tsk("pima"),
  learner = lrn("classif.rpart", cp = to_tune(1e-04, 1e-1, logscale = TRUE)),
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  term_evals = 4,
  callbacks = clbk("mlr3tuning.backup", path = tempfile(fileext = ".rds"))
)
</code></pre>

<hr>
<h2 id='mlr3tuning.measures'>Measure Callback</h2><span id='topic+mlr3tuning.measures'></span><span id='topic+mlr3tuning.async_measures'></span>

<h3>Description</h3>

<p>This <a href="mlr3misc.html#topic+Callback">mlr3misc::Callback</a> scores the hyperparameter configurations on additional measures while tuning.
Usually, the configurations can be scored on additional measures after tuning (see <a href="#topic+ArchiveBatchTuning">ArchiveBatchTuning</a>).
However, if the memory is not sufficient to store the <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>, it is necessary to score the additional measures while tuning.
The measures are not taken into account by the tuner.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>clbk("mlr3tuning.measures")

# additionally score the configurations on the accuracy measure
instance = tune(
  tuner = tnr("random_search", batch_size = 2),
  task = tsk("pima"),
  learner = lrn("classif.rpart", cp = to_tune(1e-04, 1e-1, logscale = TRUE)),
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  term_evals = 4,
  callbacks = clbk("mlr3tuning.measures", measures = msr("classif.acc"))
)

</code></pre>

<hr>
<h2 id='ObjectiveTuning'>Class for Tuning Objective</h2><span id='topic+ObjectiveTuning'></span>

<h3>Description</h3>

<p>Stores the objective function that estimates the performance of hyperparameter configurations.
This class is usually constructed internally by the <a href="#topic+TuningInstanceBatchSingleCrit">TuningInstanceBatchSingleCrit</a> or <a href="#topic+TuningInstanceBatchMultiCrit">TuningInstanceBatchMultiCrit</a>.
</p>


<h3>Super class</h3>

<p><code><a href="bbotk.html#topic+Objective">bbotk::Objective</a></code> -&gt; <code>ObjectiveTuning</code>
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>task</code></dt><dd><p>(<a href="mlr3.html#topic+Task">mlr3::Task</a>).</p>
</dd>
<dt><code>learner</code></dt><dd><p>(<a href="mlr3.html#topic+Learner">mlr3::Learner</a>).</p>
</dd>
<dt><code>resampling</code></dt><dd><p>(<a href="mlr3.html#topic+Resampling">mlr3::Resampling</a>).</p>
</dd>
<dt><code>measures</code></dt><dd><p>(list of <a href="mlr3.html#topic+Measure">mlr3::Measure</a>).</p>
</dd>
<dt><code>store_models</code></dt><dd><p>(<code>logical(1)</code>).</p>
</dd>
<dt><code>store_benchmark_result</code></dt><dd><p>(<code>logical(1)</code>).</p>
</dd>
<dt><code>callbacks</code></dt><dd><p>(List of <a href="mlr3misc.html#topic+Callback">mlr3misc::Callback</a>).</p>
</dd>
<dt><code>default_values</code></dt><dd><p>(named <code>list()</code>).</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-ObjectiveTuning-new"><code>ObjectiveTuning$new()</code></a>
</p>
</li>
<li> <p><a href="#method-ObjectiveTuning-clone"><code>ObjectiveTuning$clone()</code></a>
</p>
</li></ul>



<details><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Objective" data-id="eval"><a href='../../bbotk/html/Objective.html#method-Objective-eval'><code>bbotk::Objective$eval()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Objective" data-id="eval_dt"><a href='../../bbotk/html/Objective.html#method-Objective-eval_dt'><code>bbotk::Objective$eval_dt()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Objective" data-id="eval_many"><a href='../../bbotk/html/Objective.html#method-Objective-eval_many'><code>bbotk::Objective$eval_many()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Objective" data-id="format"><a href='../../bbotk/html/Objective.html#method-Objective-format'><code>bbotk::Objective$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Objective" data-id="help"><a href='../../bbotk/html/Objective.html#method-Objective-help'><code>bbotk::Objective$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Objective" data-id="print"><a href='../../bbotk/html/Objective.html#method-Objective-print'><code>bbotk::Objective$print()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-ObjectiveTuning-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>ObjectiveTuning$new(
  task,
  learner,
  resampling,
  measures,
  store_benchmark_result = TRUE,
  store_models = FALSE,
  check_values = FALSE,
  callbacks = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>task</code></dt><dd><p>(<a href="mlr3.html#topic+Task">mlr3::Task</a>)<br />
Task to operate on.</p>
</dd>
<dt><code>learner</code></dt><dd><p>(<a href="mlr3.html#topic+Learner">mlr3::Learner</a>)<br />
Learner to tune.</p>
</dd>
<dt><code>resampling</code></dt><dd><p>(<a href="mlr3.html#topic+Resampling">mlr3::Resampling</a>)<br />
Resampling that is used to evaluate the performance of the hyperparameter configurations.
Uninstantiated resamplings are instantiated during construction so that all configurations are evaluated on the same data splits.
Already instantiated resamplings are kept unchanged.
Specialized <a href="#topic+Tuner">Tuner</a> change the resampling e.g. to evaluate a hyperparameter configuration on different data splits.
This field, however, always returns the resampling passed in construction.</p>
</dd>
<dt><code>measures</code></dt><dd><p>(list of <a href="mlr3.html#topic+Measure">mlr3::Measure</a>)<br />
Measures to optimize.</p>
</dd>
<dt><code>store_benchmark_result</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default), store resample result of evaluated hyperparameter
configurations in archive as <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>.</p>
</dd>
<dt><code>store_models</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, fitted models are stored in the benchmark result
(<code>archive$benchmark_result</code>). If <code>store_benchmark_result = FALSE</code>, models
are only stored temporarily and not accessible after the tuning. This
combination is needed for measures that require a model.</p>
</dd>
<dt><code>check_values</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, hyperparameter values are checked before evaluation and
performance scores after. If <code>FALSE</code> (default), values are unchecked but
computational overhead is reduced.</p>
</dd>
<dt><code>callbacks</code></dt><dd><p>(list of <a href="mlr3misc.html#topic+Callback">mlr3misc::Callback</a>)<br />
List of callbacks.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-ObjectiveTuning-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>ObjectiveTuning$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='ObjectiveTuningAsync'>Class for Tuning Objective</h2><span id='topic+ObjectiveTuningAsync'></span>

<h3>Description</h3>

<p>Stores the objective function that estimates the performance of hyperparameter configurations.
This class is usually constructed internally by the <a href="#topic+TuningInstanceBatchSingleCrit">TuningInstanceBatchSingleCrit</a> or <a href="#topic+TuningInstanceBatchMultiCrit">TuningInstanceBatchMultiCrit</a>.
</p>


<h3>Super classes</h3>

<p><code><a href="bbotk.html#topic+Objective">bbotk::Objective</a></code> -&gt; <code><a href="#topic+ObjectiveTuning">mlr3tuning::ObjectiveTuning</a></code> -&gt; <code>ObjectiveTuningAsync</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-ObjectiveTuningAsync-clone"><code>ObjectiveTuningAsync$clone()</code></a>
</p>
</li></ul>



<details><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Objective" data-id="eval"><a href='../../bbotk/html/Objective.html#method-Objective-eval'><code>bbotk::Objective$eval()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Objective" data-id="eval_dt"><a href='../../bbotk/html/Objective.html#method-Objective-eval_dt'><code>bbotk::Objective$eval_dt()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Objective" data-id="eval_many"><a href='../../bbotk/html/Objective.html#method-Objective-eval_many'><code>bbotk::Objective$eval_many()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Objective" data-id="format"><a href='../../bbotk/html/Objective.html#method-Objective-format'><code>bbotk::Objective$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Objective" data-id="help"><a href='../../bbotk/html/Objective.html#method-Objective-help'><code>bbotk::Objective$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Objective" data-id="print"><a href='../../bbotk/html/Objective.html#method-Objective-print'><code>bbotk::Objective$print()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="ObjectiveTuning" data-id="initialize"><a href='../../mlr3tuning/html/ObjectiveTuning.html#method-ObjectiveTuning-initialize'><code>mlr3tuning::ObjectiveTuning$initialize()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-ObjectiveTuningAsync-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>ObjectiveTuningAsync$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='ObjectiveTuningBatch'>Class for Tuning Objective</h2><span id='topic+ObjectiveTuningBatch'></span>

<h3>Description</h3>

<p>Stores the objective function that estimates the performance of hyperparameter configurations.
This class is usually constructed internally by the <a href="#topic+TuningInstanceBatchSingleCrit">TuningInstanceBatchSingleCrit</a> or <a href="#topic+TuningInstanceBatchMultiCrit">TuningInstanceBatchMultiCrit</a>.
</p>


<h3>Super classes</h3>

<p><code><a href="bbotk.html#topic+Objective">bbotk::Objective</a></code> -&gt; <code><a href="#topic+ObjectiveTuning">mlr3tuning::ObjectiveTuning</a></code> -&gt; <code>ObjectiveTuningBatch</code>
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>archive</code></dt><dd><p>(<a href="#topic+ArchiveBatchTuning">ArchiveBatchTuning</a>).</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-ObjectiveTuningBatch-new"><code>ObjectiveTuningBatch$new()</code></a>
</p>
</li>
<li> <p><a href="#method-ObjectiveTuningBatch-clone"><code>ObjectiveTuningBatch$clone()</code></a>
</p>
</li></ul>



<details><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Objective" data-id="eval"><a href='../../bbotk/html/Objective.html#method-Objective-eval'><code>bbotk::Objective$eval()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Objective" data-id="eval_dt"><a href='../../bbotk/html/Objective.html#method-Objective-eval_dt'><code>bbotk::Objective$eval_dt()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Objective" data-id="eval_many"><a href='../../bbotk/html/Objective.html#method-Objective-eval_many'><code>bbotk::Objective$eval_many()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Objective" data-id="format"><a href='../../bbotk/html/Objective.html#method-Objective-format'><code>bbotk::Objective$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Objective" data-id="help"><a href='../../bbotk/html/Objective.html#method-Objective-help'><code>bbotk::Objective$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Objective" data-id="print"><a href='../../bbotk/html/Objective.html#method-Objective-print'><code>bbotk::Objective$print()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-ObjectiveTuningBatch-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>ObjectiveTuningBatch$new(
  task,
  learner,
  resampling,
  measures,
  store_benchmark_result = TRUE,
  store_models = FALSE,
  check_values = FALSE,
  archive = NULL,
  callbacks = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>task</code></dt><dd><p>(<a href="mlr3.html#topic+Task">mlr3::Task</a>)<br />
Task to operate on.</p>
</dd>
<dt><code>learner</code></dt><dd><p>(<a href="mlr3.html#topic+Learner">mlr3::Learner</a>)<br />
Learner to tune.</p>
</dd>
<dt><code>resampling</code></dt><dd><p>(<a href="mlr3.html#topic+Resampling">mlr3::Resampling</a>)<br />
Resampling that is used to evaluate the performance of the hyperparameter configurations.
Uninstantiated resamplings are instantiated during construction so that all configurations are evaluated on the same data splits.
Already instantiated resamplings are kept unchanged.
Specialized <a href="#topic+Tuner">Tuner</a> change the resampling e.g. to evaluate a hyperparameter configuration on different data splits.
This field, however, always returns the resampling passed in construction.</p>
</dd>
<dt><code>measures</code></dt><dd><p>(list of <a href="mlr3.html#topic+Measure">mlr3::Measure</a>)<br />
Measures to optimize.</p>
</dd>
<dt><code>store_benchmark_result</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default), store resample result of evaluated hyperparameter
configurations in archive as <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>.</p>
</dd>
<dt><code>store_models</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, fitted models are stored in the benchmark result
(<code>archive$benchmark_result</code>). If <code>store_benchmark_result = FALSE</code>, models
are only stored temporarily and not accessible after the tuning. This
combination is needed for measures that require a model.</p>
</dd>
<dt><code>check_values</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, hyperparameter values are checked before evaluation and
performance scores after. If <code>FALSE</code> (default), values are unchecked but
computational overhead is reduced.</p>
</dd>
<dt><code>archive</code></dt><dd><p>(<a href="#topic+ArchiveBatchTuning">ArchiveBatchTuning</a>)<br />
Reference to archive of <a href="#topic+TuningInstanceBatchSingleCrit">TuningInstanceBatchSingleCrit</a> | <a href="#topic+TuningInstanceBatchMultiCrit">TuningInstanceBatchMultiCrit</a>.
If <code>NULL</code> (default), benchmark result and models cannot be stored.</p>
</dd>
<dt><code>callbacks</code></dt><dd><p>(list of <a href="mlr3misc.html#topic+Callback">mlr3misc::Callback</a>)<br />
List of callbacks.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-ObjectiveTuningBatch-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>ObjectiveTuningBatch$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic+mlr_terminators'></span><span id='topic+trm'></span><span id='topic+trms'></span><span id='topic+mlr_callbacks'></span><span id='topic+clbk'></span><span id='topic+clbks'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>bbotk</dt><dd><p><code><a href="bbotk.html#topic+mlr_terminators">mlr_terminators</a></code>, <code><a href="bbotk.html#topic+trm">trm</a></code>, <code><a href="bbotk.html#topic+trm">trms</a></code></p>
</dd>
<dt>mlr3misc</dt><dd><p><code><a href="mlr3misc.html#topic+clbk">clbk</a></code>, <code><a href="mlr3misc.html#topic+clbk">clbks</a></code>, <code><a href="mlr3misc.html#topic+mlr_callbacks">mlr_callbacks</a></code></p>
</dd>
</dl>

<hr>
<h2 id='set_validate.AutoTuner'>Configure Validation for AutoTuner</h2><span id='topic+set_validate.AutoTuner'></span>

<h3>Description</h3>

<p>Configure validation for the final model fit (<code>final_validate</code>), as well as
during the tuning (<code>validate</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'AutoTuner'
set_validate(learner, validate, final_validate, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_validate.AutoTuner_+3A_learner">learner</code></td>
<td>
<p>(<code><a href="#topic+AutoTuner">AutoTuner</a></code>)<br />
The autotuner for which to enable validation.</p>
</td></tr>
<tr><td><code id="set_validate.AutoTuner_+3A_validate">validate</code></td>
<td>
<p>(<code>numeric(1)</code>, <code>"predefined"</code>, <code>"test"</code>, or <code>NULL</code>)<br />
How to configure the validation during the hyperparameter tuning.</p>
</td></tr>
<tr><td><code id="set_validate.AutoTuner_+3A_final_validate">final_validate</code></td>
<td>
<p>(<code>numeric(1)</code>, <code>"predefined"</code>, <code>"test"</code> or <code>NULL</code>)<br />
How to configure the validation during the final model fit.
The default behavior is to not change the value.
Rarely needed.</p>
</td></tr>
<tr><td><code id="set_validate.AutoTuner_+3A_...">...</code></td>
<td>
<p>(any)<br />
Passed when calling <code>set_validate()</code> on the wrapped leaerner.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>at = auto_tuner(
  tuner = tnr("random_search"),
  learner = lrn("classif.debug", early_stopping = TRUE,
    iter = to_tune(upper = 1000L, internal = TRUE), validate = 0.2),
  resampling = rsmp("holdout")
)
# use the test set as validation data during tuning
set_validate(at, validate = "test")
at$learner$validate
</code></pre>

<hr>
<h2 id='ti'>Syntactic Sugar for Tuning Instance Construction</h2><span id='topic+ti'></span>

<h3>Description</h3>

<p>Function to construct a <a href="#topic+TuningInstanceBatchSingleCrit">TuningInstanceBatchSingleCrit</a> or <a href="#topic+TuningInstanceBatchMultiCrit">TuningInstanceBatchMultiCrit</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ti(
  task,
  learner,
  resampling,
  measures = NULL,
  terminator,
  search_space = NULL,
  store_benchmark_result = TRUE,
  store_models = FALSE,
  check_values = FALSE,
  callbacks = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ti_+3A_task">task</code></td>
<td>
<p>(<a href="mlr3.html#topic+Task">mlr3::Task</a>)<br />
Task to operate on.</p>
</td></tr>
<tr><td><code id="ti_+3A_learner">learner</code></td>
<td>
<p>(<a href="mlr3.html#topic+Learner">mlr3::Learner</a>)<br />
Learner to tune.</p>
</td></tr>
<tr><td><code id="ti_+3A_resampling">resampling</code></td>
<td>
<p>(<a href="mlr3.html#topic+Resampling">mlr3::Resampling</a>)<br />
Resampling that is used to evaluate the performance of the hyperparameter configurations.
Uninstantiated resamplings are instantiated during construction so that all configurations are evaluated on the same data splits.
Already instantiated resamplings are kept unchanged.
Specialized <a href="#topic+Tuner">Tuner</a> change the resampling e.g. to evaluate a hyperparameter configuration on different data splits.
This field, however, always returns the resampling passed in construction.</p>
</td></tr>
<tr><td><code id="ti_+3A_measures">measures</code></td>
<td>
<p>(<a href="mlr3.html#topic+Measure">mlr3::Measure</a> or list of <a href="mlr3.html#topic+Measure">mlr3::Measure</a>)<br />
A single measure creates a <a href="#topic+TuningInstanceBatchSingleCrit">TuningInstanceBatchSingleCrit</a> and multiple measures a <a href="#topic+TuningInstanceBatchMultiCrit">TuningInstanceBatchMultiCrit</a>.
If <code>NULL</code>, default measure is used.</p>
</td></tr>
<tr><td><code id="ti_+3A_terminator">terminator</code></td>
<td>
<p>(<a href="bbotk.html#topic+Terminator">bbotk::Terminator</a>)<br />
Stop criterion of the tuning process.</p>
</td></tr>
<tr><td><code id="ti_+3A_search_space">search_space</code></td>
<td>
<p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>)<br />
Hyperparameter search space. If <code>NULL</code> (default), the search space is
constructed from the <a href="paradox.html#topic+to_tune">paradox::TuneToken</a> of the learner's parameter set
(learner$param_set).</p>
</td></tr>
<tr><td><code id="ti_+3A_store_benchmark_result">store_benchmark_result</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default), store resample result of evaluated hyperparameter
configurations in archive as <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>.</p>
</td></tr>
<tr><td><code id="ti_+3A_store_models">store_models</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, fitted models are stored in the benchmark result
(<code>archive$benchmark_result</code>). If <code>store_benchmark_result = FALSE</code>, models
are only stored temporarily and not accessible after the tuning. This
combination is needed for measures that require a model.</p>
</td></tr>
<tr><td><code id="ti_+3A_check_values">check_values</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, hyperparameter values are checked before evaluation and
performance scores after. If <code>FALSE</code> (default), values are unchecked but
computational overhead is reduced.</p>
</td></tr>
<tr><td><code id="ti_+3A_callbacks">callbacks</code></td>
<td>
<p>(list of <a href="mlr3misc.html#topic+Callback">mlr3misc::Callback</a>)<br />
List of callbacks.</p>
</td></tr>
</table>


<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li><p> Getting started with <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html">hyperparameter optimization</a>.
</p>
</li>
<li> <p><a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-model-tuning">Tune</a> a simple classification tree on the Sonar data set.
</p>
</li>
<li><p> Learn about <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-defining-search-spaces">tuning spaces</a>.
</p>
</li></ul>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>

<ul>
<li><p> Learn more advanced methods with the <a href="https://mlr-org.com/gallery/series/2021-03-09-practical-tuning-series-tune-a-support-vector-machine/">practical tuning series</a>.
</p>
</li>
<li><p> Simultaneously optimize hyperparameters and use <a href="https://mlr-org.com/gallery/optimization/2022-11-04-early-stopping-with-xgboost/">early stopping</a> with XGBoost.
</p>
</li>
<li><p> Make us of proven <a href="https://mlr-org.com/gallery/optimization/2021-07-06-introduction-to-mlr3tuningspaces/">search space</a>.
</p>
</li>
<li><p> Learn about <a href="https://mlr-org.com/gallery/optimization/2023-01-16-hotstart/">hotstarting</a> models.
</p>
</li>
<li><p> Run the <a href="https://mlr-org.com/gallery/optimization/2023-01-31-default-configuration/">default hyperparameter configuration</a> of learners as a baseline.
</p>
</li></ul>



<h3>Default Measures</h3>

<p>If no measure is passed, the default measure is used.
The default measure depends on the task type.</p>

<table>
<tr>
 <td style="text-align: left;">
   Task </td><td style="text-align: left;"> Default Measure </td><td style="text-align: left;"> Package </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"classif"</code> </td><td style="text-align: left;"> <code>"classif.ce"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3"><span class="pkg">mlr3</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"regr"</code> </td><td style="text-align: left;"> <code>"regr.mse"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3"><span class="pkg">mlr3</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"surv"</code> </td><td style="text-align: left;"> <code>"surv.cindex"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3proba"><span class="pkg">mlr3proba</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"dens"</code> </td><td style="text-align: left;"> <code>"dens.logloss"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3proba"><span class="pkg">mlr3proba</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"classif_st"</code> </td><td style="text-align: left;"> <code>"classif.ce"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3spatial"><span class="pkg">mlr3spatial</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"regr_st"</code> </td><td style="text-align: left;"> <code>"regr.mse"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3spatial"><span class="pkg">mlr3spatial</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"clust"</code> </td><td style="text-align: left;"> <code>"clust.dunn"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3cluster"><span class="pkg">mlr3cluster</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Examples</h3>

<pre><code class='language-R'># Hyperparameter optimization on the Palmer Penguins data set
task = tsk("penguins")

# Load learner and set search space
learner = lrn("classif.rpart",
  cp = to_tune(1e-04, 1e-1, logscale = TRUE)
)

# Construct tuning instance
instance = ti(
  task = task,
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  terminator = trm("evals", n_evals = 4)
)

# Choose optimization algorithm
tuner = tnr("random_search", batch_size = 2)

# Run tuning
tuner$optimize(instance)

# Set optimal hyperparameter configuration to learner
learner$param_set$values = instance$result_learner_param_vals

# Train the learner on the full data set
learner$train(task)

# Inspect all evaluated configurations
as.data.table(instance$archive)
</code></pre>

<hr>
<h2 id='ti_async'>Syntactic Sugar for Asynchronous Tuning Instance Construction</h2><span id='topic+ti_async'></span>

<h3>Description</h3>

<p>Function to construct a <a href="#topic+TuningInstanceAsyncSingleCrit">TuningInstanceAsyncSingleCrit</a> or <a href="#topic+TuningInstanceAsyncMultiCrit">TuningInstanceAsyncMultiCrit</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ti_async(
  task,
  learner,
  resampling,
  measures = NULL,
  terminator,
  search_space = NULL,
  store_benchmark_result = TRUE,
  store_models = FALSE,
  check_values = FALSE,
  callbacks = NULL,
  rush = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ti_async_+3A_task">task</code></td>
<td>
<p>(<a href="mlr3.html#topic+Task">mlr3::Task</a>)<br />
Task to operate on.</p>
</td></tr>
<tr><td><code id="ti_async_+3A_learner">learner</code></td>
<td>
<p>(<a href="mlr3.html#topic+Learner">mlr3::Learner</a>)<br />
Learner to tune.</p>
</td></tr>
<tr><td><code id="ti_async_+3A_resampling">resampling</code></td>
<td>
<p>(<a href="mlr3.html#topic+Resampling">mlr3::Resampling</a>)<br />
Resampling that is used to evaluate the performance of the hyperparameter configurations.
Uninstantiated resamplings are instantiated during construction so that all configurations are evaluated on the same data splits.
Already instantiated resamplings are kept unchanged.
Specialized <a href="#topic+Tuner">Tuner</a> change the resampling e.g. to evaluate a hyperparameter configuration on different data splits.
This field, however, always returns the resampling passed in construction.</p>
</td></tr>
<tr><td><code id="ti_async_+3A_measures">measures</code></td>
<td>
<p>(<a href="mlr3.html#topic+Measure">mlr3::Measure</a> or list of <a href="mlr3.html#topic+Measure">mlr3::Measure</a>)<br />
A single measure creates a <a href="#topic+TuningInstanceAsyncSingleCrit">TuningInstanceAsyncSingleCrit</a> and multiple measures a <a href="#topic+TuningInstanceAsyncMultiCrit">TuningInstanceAsyncMultiCrit</a>.
If <code>NULL</code>, default measure is used.</p>
</td></tr>
<tr><td><code id="ti_async_+3A_terminator">terminator</code></td>
<td>
<p>(<a href="bbotk.html#topic+Terminator">bbotk::Terminator</a>)<br />
Stop criterion of the tuning process.</p>
</td></tr>
<tr><td><code id="ti_async_+3A_search_space">search_space</code></td>
<td>
<p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>)<br />
Hyperparameter search space. If <code>NULL</code> (default), the search space is
constructed from the <a href="paradox.html#topic+to_tune">paradox::TuneToken</a> of the learner's parameter set
(learner$param_set).</p>
</td></tr>
<tr><td><code id="ti_async_+3A_store_benchmark_result">store_benchmark_result</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default), store resample result of evaluated hyperparameter
configurations in archive as <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>.</p>
</td></tr>
<tr><td><code id="ti_async_+3A_store_models">store_models</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, fitted models are stored in the benchmark result
(<code>archive$benchmark_result</code>). If <code>store_benchmark_result = FALSE</code>, models
are only stored temporarily and not accessible after the tuning. This
combination is needed for measures that require a model.</p>
</td></tr>
<tr><td><code id="ti_async_+3A_check_values">check_values</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, hyperparameter values are checked before evaluation and
performance scores after. If <code>FALSE</code> (default), values are unchecked but
computational overhead is reduced.</p>
</td></tr>
<tr><td><code id="ti_async_+3A_callbacks">callbacks</code></td>
<td>
<p>(list of <a href="mlr3misc.html#topic+Callback">mlr3misc::Callback</a>)<br />
List of callbacks.</p>
</td></tr>
<tr><td><code id="ti_async_+3A_rush">rush</code></td>
<td>
<p>(<code>Rush</code>)<br />
If a rush instance is supplied, the tuning runs without batches.</p>
</td></tr>
</table>


<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li><p> Getting started with <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html">hyperparameter optimization</a>.
</p>
</li>
<li> <p><a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-model-tuning">Tune</a> a simple classification tree on the Sonar data set.
</p>
</li>
<li><p> Learn about <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-defining-search-spaces">tuning spaces</a>.
</p>
</li></ul>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>

<ul>
<li><p> Learn more advanced methods with the <a href="https://mlr-org.com/gallery/series/2021-03-09-practical-tuning-series-tune-a-support-vector-machine/">practical tuning series</a>.
</p>
</li>
<li><p> Simultaneously optimize hyperparameters and use <a href="https://mlr-org.com/gallery/optimization/2022-11-04-early-stopping-with-xgboost/">early stopping</a> with XGBoost.
</p>
</li>
<li><p> Make us of proven <a href="https://mlr-org.com/gallery/optimization/2021-07-06-introduction-to-mlr3tuningspaces/">search space</a>.
</p>
</li>
<li><p> Learn about <a href="https://mlr-org.com/gallery/optimization/2023-01-16-hotstart/">hotstarting</a> models.
</p>
</li>
<li><p> Run the <a href="https://mlr-org.com/gallery/optimization/2023-01-31-default-configuration/">default hyperparameter configuration</a> of learners as a baseline.
</p>
</li></ul>



<h3>Default Measures</h3>

<p>If no measure is passed, the default measure is used.
The default measure depends on the task type.</p>

<table>
<tr>
 <td style="text-align: left;">
   Task </td><td style="text-align: left;"> Default Measure </td><td style="text-align: left;"> Package </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"classif"</code> </td><td style="text-align: left;"> <code>"classif.ce"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3"><span class="pkg">mlr3</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"regr"</code> </td><td style="text-align: left;"> <code>"regr.mse"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3"><span class="pkg">mlr3</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"surv"</code> </td><td style="text-align: left;"> <code>"surv.cindex"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3proba"><span class="pkg">mlr3proba</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"dens"</code> </td><td style="text-align: left;"> <code>"dens.logloss"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3proba"><span class="pkg">mlr3proba</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"classif_st"</code> </td><td style="text-align: left;"> <code>"classif.ce"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3spatial"><span class="pkg">mlr3spatial</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"regr_st"</code> </td><td style="text-align: left;"> <code>"regr.mse"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3spatial"><span class="pkg">mlr3spatial</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"clust"</code> </td><td style="text-align: left;"> <code>"clust.dunn"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3cluster"><span class="pkg">mlr3cluster</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Examples</h3>

<pre><code class='language-R'># Hyperparameter optimization on the Palmer Penguins data set
task = tsk("penguins")

# Load learner and set search space
learner = lrn("classif.rpart",
  cp = to_tune(1e-04, 1e-1, logscale = TRUE)
)

# Construct tuning instance
instance = ti(
  task = task,
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  terminator = trm("evals", n_evals = 4)
)

# Choose optimization algorithm
tuner = tnr("random_search", batch_size = 2)

# Run tuning
tuner$optimize(instance)

# Set optimal hyperparameter configuration to learner
learner$param_set$values = instance$result_learner_param_vals

# Train the learner on the full data set
learner$train(task)

# Inspect all evaluated configurations
as.data.table(instance$archive)
</code></pre>

<hr>
<h2 id='tnr'>Syntactic Sugar for Tuning Objects Construction</h2><span id='topic+tnr'></span><span id='topic+tnrs'></span>

<h3>Description</h3>

<p>Functions to retrieve objects, set parameters and assign to fields in one go.
Relies on <code><a href="mlr3misc.html#topic+dictionary_sugar_get">mlr3misc::dictionary_sugar_get()</a></code> to extract objects from the respective <a href="mlr3misc.html#topic+Dictionary">mlr3misc::Dictionary</a>:
</p>

<ul>
<li> <p><code>tnr()</code> for a <a href="#topic+Tuner">Tuner</a> from <a href="#topic+mlr_tuners">mlr_tuners</a>.
</p>
</li>
<li> <p><code>tnrs()</code> for a list of <a href="#topic+Tuner">Tuners</a> from <a href="#topic+mlr_tuners">mlr_tuners</a>.
</p>
</li>
<li> <p><code>trm()</code> for a <a href="bbotk.html#topic+Terminator">bbotk::Terminator</a> from <a href="#topic+mlr_terminators">mlr_terminators</a>.
</p>
</li>
<li> <p><code>trms()</code> for a list of <a href="bbotk.html#topic+Terminator">Terminators</a> from <a href="#topic+mlr_terminators">mlr_terminators</a>.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>tnr(.key, ...)

tnrs(.keys, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tnr_+3A_.key">.key</code></td>
<td>
<p>(<code>character(1)</code>)<br />
Key passed to the respective <a href="mlr3misc.html#topic+Dictionary">dictionary</a> to retrieve the object.</p>
</td></tr>
<tr><td><code id="tnr_+3A_...">...</code></td>
<td>
<p>(any)<br />
Additional arguments.</p>
</td></tr>
<tr><td><code id="tnr_+3A_.keys">.keys</code></td>
<td>
<p>(<code>character()</code>)<br />
Keys passed to the respective <a href="mlr3misc.html#topic+Dictionary">dictionary</a> to retrieve multiple objects.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><a href="R6.html#topic+R6Class">R6::R6Class</a> object of the respective type, or a list of <a href="R6.html#topic+R6Class">R6::R6Class</a> objects for the plural versions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># random search tuner with batch size of 5
tnr("random_search", batch_size = 5)

# run time terminator with 20 seconds
trm("run_time", secs = 20)
</code></pre>

<hr>
<h2 id='tune'>Function for Tuning a Learner</h2><span id='topic+tune'></span>

<h3>Description</h3>

<p>Function to tune a <a href="mlr3.html#topic+Learner">mlr3::Learner</a>.
The function internally creates a <a href="#topic+TuningInstanceBatchSingleCrit">TuningInstanceBatchSingleCrit</a> or <a href="#topic+TuningInstanceBatchMultiCrit">TuningInstanceBatchMultiCrit</a> which describes the tuning problem.
It executes the tuning with the <a href="#topic+Tuner">Tuner</a> (<code>tuner</code>) and returns the result with the tuning instance (<code style="white-space: pre;">&#8288;$result&#8288;</code>).
The <a href="#topic+ArchiveBatchTuning">ArchiveBatchTuning</a> and <a href="#topic+ArchiveAsyncTuning">ArchiveAsyncTuning</a> (<code style="white-space: pre;">&#8288;$archive&#8288;</code>) stores all evaluated hyperparameter configurations and performance scores.
</p>
<p>You can find an overview of all tuners on our <a href="https://mlr-org.com/tuners.html">website</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tune(
  tuner,
  task,
  learner,
  resampling,
  measures = NULL,
  term_evals = NULL,
  term_time = NULL,
  terminator = NULL,
  search_space = NULL,
  store_benchmark_result = TRUE,
  store_models = FALSE,
  check_values = FALSE,
  callbacks = NULL,
  rush = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tune_+3A_tuner">tuner</code></td>
<td>
<p>(<a href="#topic+Tuner">Tuner</a>)<br />
Optimization algorithm.</p>
</td></tr>
<tr><td><code id="tune_+3A_task">task</code></td>
<td>
<p>(<a href="mlr3.html#topic+Task">mlr3::Task</a>)<br />
Task to operate on.</p>
</td></tr>
<tr><td><code id="tune_+3A_learner">learner</code></td>
<td>
<p>(<a href="mlr3.html#topic+Learner">mlr3::Learner</a>)<br />
Learner to tune.</p>
</td></tr>
<tr><td><code id="tune_+3A_resampling">resampling</code></td>
<td>
<p>(<a href="mlr3.html#topic+Resampling">mlr3::Resampling</a>)<br />
Resampling that is used to evaluate the performance of the hyperparameter configurations.
Uninstantiated resamplings are instantiated during construction so that all configurations are evaluated on the same data splits.
Already instantiated resamplings are kept unchanged.
Specialized <a href="#topic+Tuner">Tuner</a> change the resampling e.g. to evaluate a hyperparameter configuration on different data splits.
This field, however, always returns the resampling passed in construction.</p>
</td></tr>
<tr><td><code id="tune_+3A_measures">measures</code></td>
<td>
<p>(<a href="mlr3.html#topic+Measure">mlr3::Measure</a> or list of <a href="mlr3.html#topic+Measure">mlr3::Measure</a>)<br />
A single measure creates a <a href="#topic+TuningInstanceBatchSingleCrit">TuningInstanceBatchSingleCrit</a> and multiple measures a <a href="#topic+TuningInstanceBatchMultiCrit">TuningInstanceBatchMultiCrit</a>.
If <code>NULL</code>, default measure is used.</p>
</td></tr>
<tr><td><code id="tune_+3A_term_evals">term_evals</code></td>
<td>
<p>(<code>integer(1)</code>)<br />
Number of allowed evaluations.
Ignored if <code>terminator</code> is passed.</p>
</td></tr>
<tr><td><code id="tune_+3A_term_time">term_time</code></td>
<td>
<p>(<code>integer(1)</code>)<br />
Maximum allowed time in seconds.
Ignored if <code>terminator</code> is passed.</p>
</td></tr>
<tr><td><code id="tune_+3A_terminator">terminator</code></td>
<td>
<p>(<a href="bbotk.html#topic+Terminator">bbotk::Terminator</a>)<br />
Stop criterion of the tuning process.</p>
</td></tr>
<tr><td><code id="tune_+3A_search_space">search_space</code></td>
<td>
<p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>)<br />
Hyperparameter search space. If <code>NULL</code> (default), the search space is
constructed from the <a href="paradox.html#topic+to_tune">paradox::TuneToken</a> of the learner's parameter set
(learner$param_set).</p>
</td></tr>
<tr><td><code id="tune_+3A_store_benchmark_result">store_benchmark_result</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default), store resample result of evaluated hyperparameter
configurations in archive as <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>.</p>
</td></tr>
<tr><td><code id="tune_+3A_store_models">store_models</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, fitted models are stored in the benchmark result
(<code>archive$benchmark_result</code>). If <code>store_benchmark_result = FALSE</code>, models
are only stored temporarily and not accessible after the tuning. This
combination is needed for measures that require a model.</p>
</td></tr>
<tr><td><code id="tune_+3A_check_values">check_values</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, hyperparameter values are checked before evaluation and
performance scores after. If <code>FALSE</code> (default), values are unchecked but
computational overhead is reduced.</p>
</td></tr>
<tr><td><code id="tune_+3A_callbacks">callbacks</code></td>
<td>
<p>(list of <a href="mlr3misc.html#topic+Callback">mlr3misc::Callback</a>)<br />
List of callbacks.</p>
</td></tr>
<tr><td><code id="tune_+3A_rush">rush</code></td>
<td>
<p>(<code>Rush</code>)<br />
If a rush instance is supplied, the tuning runs without batches.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <a href="mlr3.html#topic+Task">mlr3::Task</a>, <a href="mlr3.html#topic+Learner">mlr3::Learner</a>, <a href="mlr3.html#topic+Resampling">mlr3::Resampling</a>, <a href="mlr3.html#topic+Measure">mlr3::Measure</a> and <a href="bbotk.html#topic+Terminator">bbotk::Terminator</a> are used to construct a <a href="#topic+TuningInstanceBatchSingleCrit">TuningInstanceBatchSingleCrit</a>.
If multiple performance <a href="mlr3.html#topic+Measure">mlr3::Measure</a>s are supplied, a <a href="#topic+TuningInstanceBatchMultiCrit">TuningInstanceBatchMultiCrit</a> is created.
The parameter <code>term_evals</code> and <code>term_time</code> are shortcuts to create a <a href="bbotk.html#topic+Terminator">bbotk::Terminator</a>.
If both parameters are passed, a <a href="bbotk.html#topic+mlr_terminators_combo">bbotk::TerminatorCombo</a> is constructed.
For other <a href="bbotk.html#topic+Terminator">Terminators</a>, pass one with <code>terminator</code>.
If no termination criterion is needed, set <code>term_evals</code>, <code>term_time</code> and <code>terminator</code> to <code>NULL</code>.
The search space is created from <a href="paradox.html#topic+to_tune">paradox::TuneToken</a> or is supplied by <code>search_space</code>.
</p>


<h3>Value</h3>

<p><a href="#topic+TuningInstanceBatchSingleCrit">TuningInstanceBatchSingleCrit</a> | <a href="#topic+TuningInstanceBatchMultiCrit">TuningInstanceBatchMultiCrit</a>
</p>


<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li><p> Simplify tuning with the <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-autotuner"><code>tune()</code></a> function.
</p>
</li>
<li><p> Learn about <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-defining-search-spaces">tuning spaces</a>.
</p>
</li></ul>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>

<ul>
<li><p> Optimize an rpart classification tree with only a <a href="https://mlr-org.com/gallery/optimization/2022-11-10-hyperparameter-optimization-on-the-palmer-penguins/">few lines of code</a>.
</p>
</li>
<li><p> Tune an XGBoost model with <a href="https://mlr-org.com/gallery/optimization/2022-11-04-early-stopping-with-xgboost/">early stopping</a>.
</p>
</li>
<li><p> Make us of proven <a href="https://mlr-org.com/gallery/optimization/2021-07-06-introduction-to-mlr3tuningspaces/">search space</a>.
</p>
</li>
<li><p> Learn about <a href="https://mlr-org.com/gallery/optimization/2023-01-16-hotstart/">hotstarting</a> models.
</p>
</li></ul>



<h3>Default Measures</h3>

<p>If no measure is passed, the default measure is used.
The default measure depends on the task type.</p>

<table>
<tr>
 <td style="text-align: left;">
   Task </td><td style="text-align: left;"> Default Measure </td><td style="text-align: left;"> Package </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"classif"</code> </td><td style="text-align: left;"> <code>"classif.ce"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3"><span class="pkg">mlr3</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"regr"</code> </td><td style="text-align: left;"> <code>"regr.mse"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3"><span class="pkg">mlr3</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"surv"</code> </td><td style="text-align: left;"> <code>"surv.cindex"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3proba"><span class="pkg">mlr3proba</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"dens"</code> </td><td style="text-align: left;"> <code>"dens.logloss"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3proba"><span class="pkg">mlr3proba</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"classif_st"</code> </td><td style="text-align: left;"> <code>"classif.ce"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3spatial"><span class="pkg">mlr3spatial</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"regr_st"</code> </td><td style="text-align: left;"> <code>"regr.mse"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3spatial"><span class="pkg">mlr3spatial</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"clust"</code> </td><td style="text-align: left;"> <code>"clust.dunn"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3cluster"><span class="pkg">mlr3cluster</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Analysis</h3>

<p>For analyzing the tuning results, it is recommended to pass the <a href="#topic+ArchiveBatchTuning">ArchiveBatchTuning</a> to <code>as.data.table()</code>.
The returned data table is joined with the benchmark result which adds the <a href="mlr3.html#topic+ResampleResult">mlr3::ResampleResult</a> for each hyperparameter evaluation.
</p>
<p>The archive provides various getters (e.g. <code style="white-space: pre;">&#8288;$learners()&#8288;</code>) to ease the access.
All getters extract by position (<code>i</code>) or unique hash (<code>uhash</code>).
For a complete list of all getters see the methods section.
</p>
<p>The benchmark result (<code style="white-space: pre;">&#8288;$benchmark_result&#8288;</code>) allows to score the hyperparameter configurations again on a different measure.
Alternatively, measures can be supplied to <code>as.data.table()</code>.
</p>
<p>The <a href="https://CRAN.R-project.org/package=mlr3viz"><span class="pkg">mlr3viz</span></a> package provides visualizations for tuning results.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Hyperparameter optimization on the Palmer Penguins data set
task = tsk("pima")

# Load learner and set search space
learner = lrn("classif.rpart",
  cp = to_tune(1e-04, 1e-1, logscale = TRUE)
)

# Run tuning
instance = tune(
  tuner = tnr("random_search", batch_size = 2),
  task = tsk("pima"),
  learner = learner,
  resampling = rsmp ("holdout"),
  measures = msr("classif.ce"),
  terminator = trm("evals", n_evals = 4)
)

# Set optimal hyperparameter configuration to learner
learner$param_set$values = instance$result_learner_param_vals

# Train the learner on the full data set
learner$train(task)

# Inspect all evaluated configurations
as.data.table(instance$archive)
</code></pre>

<hr>
<h2 id='tune_nested'>Function for Nested Resampling</h2><span id='topic+tune_nested'></span>

<h3>Description</h3>

<p>Function to conduct nested resampling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tune_nested(
  tuner,
  task,
  learner,
  inner_resampling,
  outer_resampling,
  measure = NULL,
  term_evals = NULL,
  term_time = NULL,
  terminator = NULL,
  search_space = NULL,
  store_tuning_instance = TRUE,
  store_benchmark_result = TRUE,
  store_models = FALSE,
  check_values = FALSE,
  callbacks = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tune_nested_+3A_tuner">tuner</code></td>
<td>
<p>(<a href="#topic+Tuner">Tuner</a>)<br />
Optimization algorithm.</p>
</td></tr>
<tr><td><code id="tune_nested_+3A_task">task</code></td>
<td>
<p>(<a href="mlr3.html#topic+Task">mlr3::Task</a>)<br />
Task to operate on.</p>
</td></tr>
<tr><td><code id="tune_nested_+3A_learner">learner</code></td>
<td>
<p>(<a href="mlr3.html#topic+Learner">mlr3::Learner</a>)<br />
Learner to tune.</p>
</td></tr>
<tr><td><code id="tune_nested_+3A_inner_resampling">inner_resampling</code></td>
<td>
<p>(<a href="mlr3.html#topic+Resampling">mlr3::Resampling</a>)<br />
Resampling used for the inner loop.</p>
</td></tr>
<tr><td><code id="tune_nested_+3A_outer_resampling">outer_resampling</code></td>
<td>
<p><a href="mlr3.html#topic+Resampling">mlr3::Resampling</a>)<br />
Resampling used for the outer loop.</p>
</td></tr>
<tr><td><code id="tune_nested_+3A_measure">measure</code></td>
<td>
<p>(<a href="mlr3.html#topic+Measure">mlr3::Measure</a>)<br />
Measure to optimize. If <code>NULL</code>, default measure is used.</p>
</td></tr>
<tr><td><code id="tune_nested_+3A_term_evals">term_evals</code></td>
<td>
<p>(<code>integer(1)</code>)<br />
Number of allowed evaluations.
Ignored if <code>terminator</code> is passed.</p>
</td></tr>
<tr><td><code id="tune_nested_+3A_term_time">term_time</code></td>
<td>
<p>(<code>integer(1)</code>)<br />
Maximum allowed time in seconds.
Ignored if <code>terminator</code> is passed.</p>
</td></tr>
<tr><td><code id="tune_nested_+3A_terminator">terminator</code></td>
<td>
<p>(<a href="bbotk.html#topic+Terminator">bbotk::Terminator</a>)<br />
Stop criterion of the tuning process.</p>
</td></tr>
<tr><td><code id="tune_nested_+3A_search_space">search_space</code></td>
<td>
<p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>)<br />
Hyperparameter search space. If <code>NULL</code> (default), the search space is
constructed from the <a href="paradox.html#topic+to_tune">paradox::TuneToken</a> of the learner's parameter set
(learner$param_set).</p>
</td></tr>
<tr><td><code id="tune_nested_+3A_store_tuning_instance">store_tuning_instance</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default), stores the internally created <a href="#topic+TuningInstanceBatchSingleCrit">TuningInstanceBatchSingleCrit</a> with all intermediate results in slot <code style="white-space: pre;">&#8288;$tuning_instance&#8288;</code>.</p>
</td></tr>
<tr><td><code id="tune_nested_+3A_store_benchmark_result">store_benchmark_result</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default), store resample result of evaluated hyperparameter
configurations in archive as <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>.</p>
</td></tr>
<tr><td><code id="tune_nested_+3A_store_models">store_models</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, fitted models are stored in the benchmark result
(<code>archive$benchmark_result</code>). If <code>store_benchmark_result = FALSE</code>, models
are only stored temporarily and not accessible after the tuning. This
combination is needed for measures that require a model.</p>
</td></tr>
<tr><td><code id="tune_nested_+3A_check_values">check_values</code></td>
<td>
<p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, hyperparameter values are checked before evaluation and
performance scores after. If <code>FALSE</code> (default), values are unchecked but
computational overhead is reduced.</p>
</td></tr>
<tr><td><code id="tune_nested_+3A_callbacks">callbacks</code></td>
<td>
<p>(list of <a href="mlr3misc.html#topic+Callback">mlr3misc::Callback</a>)<br />
List of callbacks.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><a href="mlr3.html#topic+ResampleResult">mlr3::ResampleResult</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Nested resampling on Palmer Penguins data set
rr = tune_nested(
  tuner = tnr("random_search", batch_size = 2),
  task = tsk("penguins"),
  learner = lrn("classif.rpart", cp = to_tune(1e-04, 1e-1, logscale = TRUE)),
  inner_resampling = rsmp ("holdout"),
  outer_resampling = rsmp("cv", folds = 2),
  measure = msr("classif.ce"),
  term_evals = 2)

# Performance scores estimated on the outer resampling
rr$score()

# Unbiased performance of the final model trained on the full data set
rr$aggregate()
</code></pre>

<hr>
<h2 id='Tuner'>Tuner</h2><span id='topic+Tuner'></span>

<h3>Description</h3>

<p>The <code>Tuner</code> implements the optimization algorithm.
</p>


<h3>Details</h3>

<p><code>Tuner</code> is an abstract base class that implements the base functionality each tuner must provide.
</p>


<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li><p> An overview of all tuners can be found on our <a href="https://mlr-org.com/tuners.html">website</a>.
</p>
</li>
<li><p> Learn more about <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-tuner">tuners</a>.
</p>
</li></ul>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>

<ul>
<li><p> Use the <a href="https://mlr-org.com/gallery/series/2023-01-15-hyperband-xgboost/">Hyperband</a> optimizer with different budget parameters.
</p>
</li></ul>



<h3>Extension Packages</h3>

<p>Additional tuners are provided by the following packages.
</p>

<ul>
<li> <p><a href="https://github.com/mlr-org/mlr3hyperband">mlr3hyperband</a> adds the Hyperband and Successive Halving algorithm.
</p>
</li>
<li> <p><a href="https://github.com/mlr-org/mlr3mbo">mlr3mbo</a> adds Bayesian optimization methods.
</p>
</li></ul>



<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>id</code></dt><dd><p>(<code>character(1)</code>)<br />
Identifier of the object.
Used in tables, plot and text output.</p>
</dd>
</dl>

</div>


<h3>Active bindings</h3>

<div class="r6-active-bindings">

<dl>
<dt><code>param_set</code></dt><dd><p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>)<br />
Set of control parameters.</p>
</dd>
<dt><code>param_classes</code></dt><dd><p>(<code>character()</code>)<br />
Supported parameter classes for learner hyperparameters that the tuner can optimize, as given in the <a href="paradox.html#topic+ParamSet">paradox::ParamSet</a> <code style="white-space: pre;">&#8288;$class&#8288;</code> field.</p>
</dd>
<dt><code>properties</code></dt><dd><p>(<code>character()</code>)<br />
Set of properties of the tuner.
Must be a subset of <code><a href="mlr3.html#topic+mlr_reflections">mlr_reflections$tuner_properties</a></code>.</p>
</dd>
<dt><code>packages</code></dt><dd><p>(<code>character()</code>)<br />
Set of required packages.
Note that these packages will be loaded via <code><a href="base.html#topic+requireNamespace">requireNamespace()</a></code>, and are not attached.</p>
</dd>
<dt><code>label</code></dt><dd><p>(<code>character(1)</code>)<br />
Label for this object.
Can be used in tables, plot and text output instead of the ID.</p>
</dd>
<dt><code>man</code></dt><dd><p>(<code>character(1)</code>)<br />
String in the format <code style="white-space: pre;">&#8288;[pkg]::[topic]&#8288;</code> pointing to a manual page for this object.
The referenced help package can be opened via method <code style="white-space: pre;">&#8288;$help()&#8288;</code>.</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-Tuner-new"><code>Tuner$new()</code></a>
</p>
</li>
<li> <p><a href="#method-Tuner-format"><code>Tuner$format()</code></a>
</p>
</li>
<li> <p><a href="#method-Tuner-print"><code>Tuner$print()</code></a>
</p>
</li>
<li> <p><a href="#method-Tuner-help"><code>Tuner$help()</code></a>
</p>
</li>
<li> <p><a href="#method-Tuner-clone"><code>Tuner$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-Tuner-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>Tuner$new(
  id = "tuner",
  param_set,
  param_classes,
  properties,
  packages = character(),
  label = NA_character_,
  man = NA_character_
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>id</code></dt><dd><p>(<code>character(1)</code>)<br />
Identifier for the new instance.</p>
</dd>
<dt><code>param_set</code></dt><dd><p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>)<br />
Set of control parameters.</p>
</dd>
<dt><code>param_classes</code></dt><dd><p>(<code>character()</code>)<br />
Supported parameter classes for learner hyperparameters that the tuner can optimize, as given in the <a href="paradox.html#topic+ParamSet">paradox::ParamSet</a> <code style="white-space: pre;">&#8288;$class&#8288;</code> field.</p>
</dd>
<dt><code>properties</code></dt><dd><p>(<code>character()</code>)<br />
Set of properties of the tuner.
Must be a subset of <code><a href="mlr3.html#topic+mlr_reflections">mlr_reflections$tuner_properties</a></code>.</p>
</dd>
<dt><code>packages</code></dt><dd><p>(<code>character()</code>)<br />
Set of required packages.
Note that these packages will be loaded via <code><a href="base.html#topic+requireNamespace">requireNamespace()</a></code>, and are not attached.</p>
</dd>
<dt><code>label</code></dt><dd><p>(<code>character(1)</code>)<br />
Label for this object.
Can be used in tables, plot and text output instead of the ID.</p>
</dd>
<dt><code>man</code></dt><dd><p>(<code>character(1)</code>)<br />
String in the format <code style="white-space: pre;">&#8288;[pkg]::[topic]&#8288;</code> pointing to a manual page for this object.
The referenced help package can be opened via method <code style="white-space: pre;">&#8288;$help()&#8288;</code>.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-Tuner-format"></a>



<h4>Method <code>format()</code></h4>

<p>Helper for print outputs.
</p>


<h5>Usage</h5>

<div class="r"><pre>Tuner$format(...)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>...</code></dt><dd><p>(ignored).</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>(<code>character()</code>).
</p>


<hr>
<a id="method-Tuner-print"></a>



<h4>Method <code>print()</code></h4>

<p>Print method.
</p>


<h5>Usage</h5>

<div class="r"><pre>Tuner$print()</pre></div>



<h5>Returns</h5>

<p>(<code>character()</code>).
</p>


<hr>
<a id="method-Tuner-help"></a>



<h4>Method <code>help()</code></h4>

<p>Opens the corresponding help page referenced by field <code style="white-space: pre;">&#8288;$man&#8288;</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>Tuner$help()</pre></div>


<hr>
<a id="method-Tuner-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>Tuner$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other Tuner: 
<code><a href="#topic+mlr_tuners">mlr_tuners</a></code>,
<code><a href="#topic+mlr_tuners_cmaes">mlr_tuners_cmaes</a></code>,
<code><a href="#topic+mlr_tuners_design_points">mlr_tuners_design_points</a></code>,
<code><a href="#topic+mlr_tuners_gensa">mlr_tuners_gensa</a></code>,
<code><a href="#topic+mlr_tuners_grid_search">mlr_tuners_grid_search</a></code>,
<code><a href="#topic+mlr_tuners_internal">mlr_tuners_internal</a></code>,
<code><a href="#topic+mlr_tuners_irace">mlr_tuners_irace</a></code>,
<code><a href="#topic+mlr_tuners_nloptr">mlr_tuners_nloptr</a></code>,
<code><a href="#topic+mlr_tuners_random_search">mlr_tuners_random_search</a></code>
</p>

<hr>
<h2 id='TunerAsync'>Class for Asynchronous Tuning Algorithms</h2><span id='topic+TunerAsync'></span>

<h3>Description</h3>

<p>The <a href="#topic+TunerAsync">TunerAsync</a> implements the asynchronous optimization algorithm.
</p>


<h3>Details</h3>

<p><a href="#topic+TunerAsync">TunerAsync</a> is an abstract base class that implements the base functionality each asynchronous tuner must provide.
</p>


<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li><p> An overview of all tuners can be found on our <a href="https://mlr-org.com/tuners.html">website</a>.
</p>
</li>
<li><p> Learn more about <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-tuner">tuners</a>.
</p>
</li></ul>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>

<ul>
<li><p> Use the <a href="https://mlr-org.com/gallery/series/2023-01-15-hyperband-xgboost/">Hyperband</a> optimizer with different budget parameters.
</p>
</li></ul>



<h3>Super class</h3>

<p><code><a href="#topic+Tuner">mlr3tuning::Tuner</a></code> -&gt; <code>TunerAsync</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TunerAsync-optimize"><code>TunerAsync$optimize()</code></a>
</p>
</li>
<li> <p><a href="#method-TunerAsync-clone"><code>TunerAsync$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="format"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-format'><code>mlr3tuning::Tuner$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="help"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-help'><code>mlr3tuning::Tuner$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="initialize"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-initialize'><code>mlr3tuning::Tuner$initialize()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="print"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-print'><code>mlr3tuning::Tuner$print()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-TunerAsync-optimize"></a>



<h4>Method <code>optimize()</code></h4>

<p>Performs the tuning on a <a href="#topic+TuningInstanceAsyncSingleCrit">TuningInstanceAsyncSingleCrit</a> or <a href="#topic+TuningInstanceAsyncMultiCrit">TuningInstanceAsyncMultiCrit</a> until termination.
The single evaluations will be written into the <a href="#topic+ArchiveAsyncTuning">ArchiveAsyncTuning</a> that resides in the <a href="#topic+TuningInstanceAsyncSingleCrit">TuningInstanceAsyncSingleCrit</a>/<a href="#topic+TuningInstanceAsyncMultiCrit">TuningInstanceAsyncMultiCrit</a>.
The result will be written into the instance object.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerAsync$optimize(inst)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>inst</code></dt><dd><p>(<a href="#topic+TuningInstanceAsyncSingleCrit">TuningInstanceAsyncSingleCrit</a> | <a href="#topic+TuningInstanceAsyncMultiCrit">TuningInstanceAsyncMultiCrit</a>).</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p><code><a href="data.table.html#topic+data.table">data.table::data.table()</a></code>
</p>


<hr>
<a id="method-TunerAsync-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerAsync$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='TunerAsyncFromOptimizerAsync'>TunerAsyncFromOptimizerAsync</h2><span id='topic+TunerAsyncFromOptimizerAsync'></span>

<h3>Description</h3>

<p>Internally used to transform <a href="bbotk.html#topic+Optimizer">bbotk::Optimizer</a> to <a href="#topic+Tuner">Tuner</a>.
</p>


<h3>Super classes</h3>

<p><code><a href="#topic+Tuner">mlr3tuning::Tuner</a></code> -&gt; <code><a href="#topic+TunerAsync">mlr3tuning::TunerAsync</a></code> -&gt; <code>TunerAsyncFromOptimizerAsync</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TunerAsyncFromOptimizerAsync-new"><code>TunerAsyncFromOptimizerAsync$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TunerAsyncFromOptimizerAsync-optimize"><code>TunerAsyncFromOptimizerAsync$optimize()</code></a>
</p>
</li>
<li> <p><a href="#method-TunerAsyncFromOptimizerAsync-clone"><code>TunerAsyncFromOptimizerAsync$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="format"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-format'><code>mlr3tuning::Tuner$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="help"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-help'><code>mlr3tuning::Tuner$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="print"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-print'><code>mlr3tuning::Tuner$print()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-TunerAsyncFromOptimizerAsync-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerAsyncFromOptimizerAsync$new(optimizer, man = NA_character_)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>optimizer</code></dt><dd><p><a href="bbotk.html#topic+Optimizer">bbotk::Optimizer</a><br />
Optimizer that is called.</p>
</dd>
<dt><code>man</code></dt><dd><p>(<code>character(1)</code>)<br />
String in the format <code style="white-space: pre;">&#8288;[pkg]::[topic]&#8288;</code> pointing to a manual page for this object.
The referenced help package can be opened via method <code style="white-space: pre;">&#8288;$help()&#8288;</code>.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-TunerAsyncFromOptimizerAsync-optimize"></a>



<h4>Method <code>optimize()</code></h4>

<p>Performs the tuning on a <a href="#topic+TuningInstanceBatchSingleCrit">TuningInstanceBatchSingleCrit</a> /
<a href="#topic+TuningInstanceBatchMultiCrit">TuningInstanceBatchMultiCrit</a> until termination. The single evaluations and
the final results will be written into the <a href="#topic+ArchiveAsyncTuning">ArchiveAsyncTuning</a> that
resides in the <a href="#topic+TuningInstanceBatchSingleCrit">TuningInstanceBatchSingleCrit</a>/<a href="#topic+TuningInstanceBatchMultiCrit">TuningInstanceBatchMultiCrit</a>.
The final result is returned.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerAsyncFromOptimizerAsync$optimize(inst)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>inst</code></dt><dd><p>(<a href="#topic+TuningInstanceBatchSingleCrit">TuningInstanceBatchSingleCrit</a> | <a href="#topic+TuningInstanceBatchMultiCrit">TuningInstanceBatchMultiCrit</a>).</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p><a href="data.table.html#topic+data.table">data.table::data.table</a>.
</p>


<hr>
<a id="method-TunerAsyncFromOptimizerAsync-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerAsyncFromOptimizerAsync$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='TunerBatch'>Class for Batch Tuning Algorithms</h2><span id='topic+TunerBatch'></span>

<h3>Description</h3>

<p>The <a href="#topic+TunerBatch">TunerBatch</a> implements the optimization algorithm.
</p>


<h3>Details</h3>

<p><a href="#topic+TunerBatch">TunerBatch</a> is an abstract base class that implements the base functionality each tuner must provide.
A subclass is implemented in the following way:
</p>

<ul>
<li><p> Inherit from Tuner.
</p>
</li>
<li><p> Specify the private abstract method <code style="white-space: pre;">&#8288;$.optimize()&#8288;</code> and use it to call into your optimizer.
</p>
</li>
<li><p> You need to call <code>instance$eval_batch()</code> to evaluate design points.
</p>
</li>
<li><p> The batch evaluation is requested at the <a href="#topic+TuningInstanceBatchSingleCrit">TuningInstanceBatchSingleCrit</a>/<a href="#topic+TuningInstanceBatchMultiCrit">TuningInstanceBatchMultiCrit</a> object <code>instance</code>, so each batch is possibly executed in parallel via <code><a href="mlr3.html#topic+benchmark">mlr3::benchmark()</a></code>, and all evaluations are stored inside of <code>instance$archive</code>.
</p>
</li>
<li><p> Before the batch evaluation, the <a href="bbotk.html#topic+Terminator">bbotk::Terminator</a> is checked, and if it is positive, an exception of class <code>"terminated_error"</code> is generated.
In the  later case the current batch of evaluations is still stored in <code>instance</code>, but the numeric scores are not sent back to the handling optimizer as it has lost execution control.
</p>
</li>
<li><p> After such an exception was caught we select the best configuration from <code>instance$archive</code> and return it.
</p>
</li>
<li><p> Note that therefore more points than specified by the <a href="bbotk.html#topic+Terminator">bbotk::Terminator</a> may be evaluated, as the Terminator is only checked before a batch evaluation, and not in-between evaluation in a batch.
How many more depends on the setting of the batch size.
</p>
</li>
<li><p> Overwrite the private super-method <code>.assign_result()</code> if you want to decide yourself how to estimate the final configuration in the instance and its estimated performance.
The default behavior is: We pick the best resample-experiment, regarding the given measure, then assign its configuration and aggregated performance to the instance.
</p>
</li></ul>



<h3>Private Methods</h3>


<ul>
<li> <p><code>.optimize(instance)</code> -&gt; <code>NULL</code><br />
Abstract base method. Implement to specify tuning of your subclass.
See details sections.
</p>
</li>
<li> <p><code>.assign_result(instance)</code> -&gt; <code>NULL</code><br />
Abstract base method. Implement to specify how the final configuration is selected.
See details sections.
</p>
</li></ul>



<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li><p> An overview of all tuners can be found on our <a href="https://mlr-org.com/tuners.html">website</a>.
</p>
</li>
<li><p> Learn more about <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-tuner">tuners</a>.
</p>
</li></ul>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>

<ul>
<li><p> Use the <a href="https://mlr-org.com/gallery/series/2023-01-15-hyperband-xgboost/">Hyperband</a> optimizer with different budget parameters.
</p>
</li></ul>



<h3>Super class</h3>

<p><code><a href="#topic+Tuner">mlr3tuning::Tuner</a></code> -&gt; <code>TunerBatch</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TunerBatch-new"><code>TunerBatch$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TunerBatch-optimize"><code>TunerBatch$optimize()</code></a>
</p>
</li>
<li> <p><a href="#method-TunerBatch-clone"><code>TunerBatch$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="format"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-format'><code>mlr3tuning::Tuner$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="help"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-help'><code>mlr3tuning::Tuner$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="print"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-print'><code>mlr3tuning::Tuner$print()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-TunerBatch-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerBatch$new(
  id = "tuner_batch",
  param_set,
  param_classes,
  properties,
  packages = character(),
  label = NA_character_,
  man = NA_character_
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>id</code></dt><dd><p>(<code>character(1)</code>)<br />
Identifier for the new instance.</p>
</dd>
<dt><code>param_set</code></dt><dd><p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>)<br />
Set of control parameters.</p>
</dd>
<dt><code>param_classes</code></dt><dd><p>(<code>character()</code>)<br />
Supported parameter classes for learner hyperparameters that the tuner can optimize, as given in the <a href="paradox.html#topic+ParamSet">paradox::ParamSet</a> <code style="white-space: pre;">&#8288;$class&#8288;</code> field.</p>
</dd>
<dt><code>properties</code></dt><dd><p>(<code>character()</code>)<br />
Set of properties of the tuner.
Must be a subset of <code><a href="mlr3.html#topic+mlr_reflections">mlr_reflections$tuner_properties</a></code>.</p>
</dd>
<dt><code>packages</code></dt><dd><p>(<code>character()</code>)<br />
Set of required packages.
Note that these packages will be loaded via <code><a href="base.html#topic+requireNamespace">requireNamespace()</a></code>, and are not attached.</p>
</dd>
<dt><code>label</code></dt><dd><p>(<code>character(1)</code>)<br />
Label for this object.
Can be used in tables, plot and text output instead of the ID.</p>
</dd>
<dt><code>man</code></dt><dd><p>(<code>character(1)</code>)<br />
String in the format <code style="white-space: pre;">&#8288;[pkg]::[topic]&#8288;</code> pointing to a manual page for this object.
The referenced help package can be opened via method <code style="white-space: pre;">&#8288;$help()&#8288;</code>.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-TunerBatch-optimize"></a>



<h4>Method <code>optimize()</code></h4>

<p>Performs the tuning on a <a href="#topic+TuningInstanceBatchSingleCrit">TuningInstanceBatchSingleCrit</a> or <a href="#topic+TuningInstanceBatchMultiCrit">TuningInstanceBatchMultiCrit</a> until termination.
The single evaluations will be written into the <a href="#topic+ArchiveBatchTuning">ArchiveBatchTuning</a> that resides in the <a href="#topic+TuningInstanceBatchSingleCrit">TuningInstanceBatchSingleCrit</a>/<a href="#topic+TuningInstanceBatchMultiCrit">TuningInstanceBatchMultiCrit</a>.
The result will be written into the instance object.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerBatch$optimize(inst)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>inst</code></dt><dd><p>(<a href="#topic+TuningInstanceBatchSingleCrit">TuningInstanceBatchSingleCrit</a> | <a href="#topic+TuningInstanceBatchMultiCrit">TuningInstanceBatchMultiCrit</a>).</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p><code><a href="data.table.html#topic+data.table">data.table::data.table()</a></code>
</p>


<hr>
<a id="method-TunerBatch-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerBatch$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='TunerBatchFromOptimizerBatch'>TunerBatchFromOptimizerBatch</h2><span id='topic+TunerBatchFromOptimizerBatch'></span>

<h3>Description</h3>

<p>Internally used to transform <a href="bbotk.html#topic+Optimizer">bbotk::Optimizer</a> to <a href="#topic+Tuner">Tuner</a>.
</p>


<h3>Super classes</h3>

<p><code><a href="#topic+Tuner">mlr3tuning::Tuner</a></code> -&gt; <code><a href="#topic+TunerBatch">mlr3tuning::TunerBatch</a></code> -&gt; <code>TunerBatchFromOptimizerBatch</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TunerBatchFromOptimizerBatch-new"><code>TunerBatchFromOptimizerBatch$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TunerBatchFromOptimizerBatch-optimize"><code>TunerBatchFromOptimizerBatch$optimize()</code></a>
</p>
</li>
<li> <p><a href="#method-TunerBatchFromOptimizerBatch-clone"><code>TunerBatchFromOptimizerBatch$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="format"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-format'><code>mlr3tuning::Tuner$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="help"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-help'><code>mlr3tuning::Tuner$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="print"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-print'><code>mlr3tuning::Tuner$print()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-TunerBatchFromOptimizerBatch-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerBatchFromOptimizerBatch$new(optimizer, man = NA_character_)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>optimizer</code></dt><dd><p><a href="bbotk.html#topic+Optimizer">bbotk::Optimizer</a><br />
Optimizer that is called.</p>
</dd>
<dt><code>man</code></dt><dd><p>(<code>character(1)</code>)<br />
String in the format <code style="white-space: pre;">&#8288;[pkg]::[topic]&#8288;</code> pointing to a manual page for this object.
The referenced help package can be opened via method <code style="white-space: pre;">&#8288;$help()&#8288;</code>.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-TunerBatchFromOptimizerBatch-optimize"></a>



<h4>Method <code>optimize()</code></h4>

<p>Performs the tuning on a <a href="#topic+TuningInstanceBatchSingleCrit">TuningInstanceBatchSingleCrit</a> / <a href="#topic+TuningInstanceBatchMultiCrit">TuningInstanceBatchMultiCrit</a> until termination.
The single evaluations and the final results will be written into the <a href="#topic+ArchiveBatchTuning">ArchiveBatchTuning</a> that resides in the <a href="#topic+TuningInstanceBatchSingleCrit">TuningInstanceBatchSingleCrit</a>/<a href="#topic+TuningInstanceBatchMultiCrit">TuningInstanceBatchMultiCrit</a>.
The final result is returned.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerBatchFromOptimizerBatch$optimize(inst)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>inst</code></dt><dd><p>(<a href="#topic+TuningInstanceBatchSingleCrit">TuningInstanceBatchSingleCrit</a> | <a href="#topic+TuningInstanceBatchMultiCrit">TuningInstanceBatchMultiCrit</a>).</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p><a href="data.table.html#topic+data.table">data.table::data.table</a>.
</p>


<hr>
<a id="method-TunerBatchFromOptimizerBatch-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerBatchFromOptimizerBatch$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='TuningInstanceAsyncMultiCrit'>Multi-Criteria Tuning with Rush</h2><span id='topic+TuningInstanceAsyncMultiCrit'></span>

<h3>Description</h3>

<p>The <a href="#topic+TuningInstanceAsyncMultiCrit">TuningInstanceAsyncMultiCrit</a> specifies a tuning problem for a <a href="#topic+Tuner">Tuner</a>.
The function <code><a href="#topic+ti_async">ti_async()</a></code> creates a <a href="#topic+TuningInstanceAsyncMultiCrit">TuningInstanceAsyncMultiCrit</a> and the function <code><a href="#topic+tune">tune()</a></code> creates an instance internally.
</p>


<h3>Details</h3>

<p>The instance contains an <a href="#topic+ObjectiveTuningAsync">ObjectiveTuningAsync</a> object that encodes the black box objective function a <a href="#topic+Tuner">Tuner</a> has to optimize.
The instance allows the basic operations of querying the objective at design points (<code style="white-space: pre;">&#8288;$eval_async()&#8288;</code>).
This operation is usually done by the <a href="#topic+Tuner">Tuner</a>.
Hyperparameter configurations are asynchronously sent to workers and evaluated by calling <code><a href="mlr3.html#topic+resample">mlr3::resample()</a></code>.
The evaluated hyperparameter configurations are stored in the <a href="#topic+ArchiveAsyncTuning">ArchiveAsyncTuning</a> (<code style="white-space: pre;">&#8288;$archive&#8288;</code>).
Before a batch is evaluated, the <a href="bbotk.html#topic+Terminator">bbotk::Terminator</a> is queried for the remaining budget.
If the available budget is exhausted, an exception is raised, and no further evaluations can be performed from this point on.
The tuner is also supposed to store its final result, consisting of a selected hyperparameter configuration and associated estimated performance values, by calling the method <code>instance$.assign_result</code>.
</p>


<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li><p> Learn about <a href="https://mlr3book.mlr-org.com/chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html#sec-multi-metrics-tuning">multi-objective optimization</a>.
</p>
</li></ul>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>


<h3>Analysis</h3>

<p>For analyzing the tuning results, it is recommended to pass the <a href="#topic+ArchiveAsyncTuning">ArchiveAsyncTuning</a> to <code>as.data.table()</code>.
The returned data table contains the <a href="mlr3.html#topic+ResampleResult">mlr3::ResampleResult</a> for each hyperparameter evaluation.
</p>


<h3>Super classes</h3>

<p><code><a href="bbotk.html#topic+OptimInstance">bbotk::OptimInstance</a></code> -&gt; <code><a href="bbotk.html#topic+OptimInstanceAsync">bbotk::OptimInstanceAsync</a></code> -&gt; <code><a href="bbotk.html#topic+OptimInstanceAsyncMultiCrit">bbotk::OptimInstanceAsyncMultiCrit</a></code> -&gt; <code>TuningInstanceAsyncMultiCrit</code>
</p>


<h3>Active bindings</h3>

<div class="r6-active-bindings">

<dl>
<dt><code>result_learner_param_vals</code></dt><dd><p>(<code>list()</code>)<br />
List of param values for the optimal learner call.</p>
</dd>
<dt><code>internal_search_space</code></dt><dd><p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>)<br />
The search space containing those parameters that are internally optimized by the <code><a href="mlr3.html#topic+Learner">mlr3::Learner</a></code>.</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TuningInstanceAsyncMultiCrit-new"><code>TuningInstanceAsyncMultiCrit$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TuningInstanceAsyncMultiCrit-assign_result"><code>TuningInstanceAsyncMultiCrit$assign_result()</code></a>
</p>
</li>
<li> <p><a href="#method-TuningInstanceAsyncMultiCrit-clone"><code>TuningInstanceAsyncMultiCrit$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstance" data-id="format"><a href='../../bbotk/html/OptimInstance.html#method-OptimInstance-format'><code>bbotk::OptimInstance$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstanceAsync" data-id="clear"><a href='../../bbotk/html/OptimInstanceAsync.html#method-OptimInstanceAsync-clear'><code>bbotk::OptimInstanceAsync$clear()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstanceAsync" data-id="print"><a href='../../bbotk/html/OptimInstanceAsync.html#method-OptimInstanceAsync-print'><code>bbotk::OptimInstanceAsync$print()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-TuningInstanceAsyncMultiCrit-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TuningInstanceAsyncMultiCrit$new(
  task,
  learner,
  resampling,
  measures,
  terminator,
  search_space = NULL,
  store_benchmark_result = TRUE,
  store_models = FALSE,
  check_values = FALSE,
  callbacks = NULL,
  rush = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>task</code></dt><dd><p>(<a href="mlr3.html#topic+Task">mlr3::Task</a>)<br />
Task to operate on.</p>
</dd>
<dt><code>learner</code></dt><dd><p>(<a href="mlr3.html#topic+Learner">mlr3::Learner</a>)<br />
Learner to tune.</p>
</dd>
<dt><code>resampling</code></dt><dd><p>(<a href="mlr3.html#topic+Resampling">mlr3::Resampling</a>)<br />
Resampling that is used to evaluate the performance of the hyperparameter configurations.
Uninstantiated resamplings are instantiated during construction so that all configurations are evaluated on the same data splits.
Already instantiated resamplings are kept unchanged.
Specialized <a href="#topic+Tuner">Tuner</a> change the resampling e.g. to evaluate a hyperparameter configuration on different data splits.
This field, however, always returns the resampling passed in construction.</p>
</dd>
<dt><code>measures</code></dt><dd><p>(list of <a href="mlr3.html#topic+Measure">mlr3::Measure</a>)<br />
Measures to optimize.</p>
</dd>
<dt><code>terminator</code></dt><dd><p>(<a href="bbotk.html#topic+Terminator">bbotk::Terminator</a>)<br />
Stop criterion of the tuning process.</p>
</dd>
<dt><code>search_space</code></dt><dd><p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>)<br />
Hyperparameter search space. If <code>NULL</code> (default), the search space is
constructed from the <a href="paradox.html#topic+to_tune">paradox::TuneToken</a> of the learner's parameter set
(learner$param_set).</p>
</dd>
<dt><code>store_benchmark_result</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default), store resample result of evaluated hyperparameter
configurations in archive as <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>.</p>
</dd>
<dt><code>store_models</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, fitted models are stored in the benchmark result
(<code>archive$benchmark_result</code>). If <code>store_benchmark_result = FALSE</code>, models
are only stored temporarily and not accessible after the tuning. This
combination is needed for measures that require a model.</p>
</dd>
<dt><code>check_values</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, hyperparameter values are checked before evaluation and
performance scores after. If <code>FALSE</code> (default), values are unchecked but
computational overhead is reduced.</p>
</dd>
<dt><code>callbacks</code></dt><dd><p>(list of <a href="mlr3misc.html#topic+Callback">mlr3misc::Callback</a>)<br />
List of callbacks.</p>
</dd>
<dt><code>rush</code></dt><dd><p>(<code>Rush</code>)<br />
If a rush instance is supplied, the tuning runs without batches.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-TuningInstanceAsyncMultiCrit-assign_result"></a>



<h4>Method <code>assign_result()</code></h4>

<p>The <a href="#topic+TunerAsync">TunerAsync</a> writes the best found points and estimated performance values here (probably the Pareto set / front).
For internal use.
</p>


<h5>Usage</h5>

<div class="r"><pre>TuningInstanceAsyncMultiCrit$assign_result(xdt, ydt, learner_param_vals = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>xdt</code></dt><dd><p>(<code>data.table::data.table()</code>)<br />
Hyperparameter values as <code>data.table::data.table()</code>. Each row is one
configuration. Contains values in the search space. Can contain additional
columns for extra information.</p>
</dd>
<dt><code>ydt</code></dt><dd><p>(<code>numeric(1)</code>)<br />
Optimal outcomes, e.g. the Pareto front.</p>
</dd>
<dt><code>learner_param_vals</code></dt><dd><p>(List of named <code style="white-space: pre;">&#8288;list()s&#8288;</code>)<br />
Fixed parameter values of the learner that are neither part of the</p>
</dd>
</dl>

</div>


<hr>
<a id="method-TuningInstanceAsyncMultiCrit-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TuningInstanceAsyncMultiCrit$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='TuningInstanceAsyncSingleCrit'>Single Criterion Tuning with Rush</h2><span id='topic+TuningInstanceAsyncSingleCrit'></span>

<h3>Description</h3>

<p>The <code>TuningInstanceAsyncSingleCrit</code> specifies a tuning problem for a <a href="#topic+TunerAsync">TunerAsync</a>.
The function <code><a href="#topic+ti_async">ti_async()</a></code> creates a <a href="#topic+TuningInstanceAsyncSingleCrit">TuningInstanceAsyncSingleCrit</a> and the function <code><a href="#topic+tune">tune()</a></code> creates an instance internally.
</p>


<h3>Details</h3>

<p>The instance contains an <a href="#topic+ObjectiveTuningAsync">ObjectiveTuningAsync</a> object that encodes the black box objective function a <a href="#topic+Tuner">Tuner</a> has to optimize.
The instance allows the basic operations of querying the objective at design points (<code style="white-space: pre;">&#8288;$eval_async()&#8288;</code>).
This operation is usually done by the <a href="#topic+Tuner">Tuner</a>.
Hyperparameter configurations are asynchronously sent to workers and evaluated by calling <code><a href="mlr3.html#topic+resample">mlr3::resample()</a></code>.
The evaluated hyperparameter configurations are stored in the <a href="#topic+ArchiveAsyncTuning">ArchiveAsyncTuning</a> (<code style="white-space: pre;">&#8288;$archive&#8288;</code>).
Before a batch is evaluated, the <a href="bbotk.html#topic+Terminator">bbotk::Terminator</a> is queried for the remaining budget.
If the available budget is exhausted, an exception is raised, and no further evaluations can be performed from this point on.
The tuner is also supposed to store its final result, consisting of a selected hyperparameter configuration and associated estimated performance values, by calling the method <code>instance$.assign_result</code>.
</p>


<h3>Default Measures</h3>

<p>If no measure is passed, the default measure is used.
The default measure depends on the task type.</p>

<table>
<tr>
 <td style="text-align: left;">
   Task </td><td style="text-align: left;"> Default Measure </td><td style="text-align: left;"> Package </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"classif"</code> </td><td style="text-align: left;"> <code>"classif.ce"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3"><span class="pkg">mlr3</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"regr"</code> </td><td style="text-align: left;"> <code>"regr.mse"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3"><span class="pkg">mlr3</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"surv"</code> </td><td style="text-align: left;"> <code>"surv.cindex"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3proba"><span class="pkg">mlr3proba</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"dens"</code> </td><td style="text-align: left;"> <code>"dens.logloss"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3proba"><span class="pkg">mlr3proba</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"classif_st"</code> </td><td style="text-align: left;"> <code>"classif.ce"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3spatial"><span class="pkg">mlr3spatial</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"regr_st"</code> </td><td style="text-align: left;"> <code>"regr.mse"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3spatial"><span class="pkg">mlr3spatial</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"clust"</code> </td><td style="text-align: left;"> <code>"clust.dunn"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3cluster"><span class="pkg">mlr3cluster</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Analysis</h3>

<p>For analyzing the tuning results, it is recommended to pass the <a href="#topic+ArchiveAsyncTuning">ArchiveAsyncTuning</a> to <code>as.data.table()</code>.
The returned data table contains the <a href="mlr3.html#topic+ResampleResult">mlr3::ResampleResult</a> for each hyperparameter evaluation.
</p>


<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li><p> Getting started with <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html">hyperparameter optimization</a>.
</p>
</li>
<li> <p><a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-model-tuning">Tune</a> a simple classification tree on the Sonar data set.
</p>
</li>
<li><p> Learn about <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-defining-search-spaces">tuning spaces</a>.
</p>
</li></ul>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>

<ul>
<li><p> Learn more advanced methods with the <a href="https://mlr-org.com/gallery/series/2021-03-09-practical-tuning-series-tune-a-support-vector-machine/">practical tuning series</a>.
</p>
</li>
<li><p> Simultaneously optimize hyperparameters and use <a href="https://mlr-org.com/gallery/optimization/2022-11-04-early-stopping-with-xgboost/">early stopping</a> with XGBoost.
</p>
</li>
<li><p> Make us of proven <a href="https://mlr-org.com/gallery/optimization/2021-07-06-introduction-to-mlr3tuningspaces/">search space</a>.
</p>
</li>
<li><p> Learn about <a href="https://mlr-org.com/gallery/optimization/2023-01-16-hotstart/">hotstarting</a> models.
</p>
</li>
<li><p> Run the <a href="https://mlr-org.com/gallery/optimization/2023-01-31-default-configuration/">default hyperparameter configuration</a> of learners as a baseline.
</p>
</li></ul>



<h3>Extension Packages</h3>

<p>mlr3tuning is extended by the following packages.
</p>

<ul>
<li> <p><a href="https://github.com/mlr-org/mlr3tuningspaces">mlr3tuningspaces</a> is a collection of search spaces from scientific articles for commonly used learners.
</p>
</li>
<li> <p><a href="https://github.com/mlr-org/mlr3hyperband">mlr3hyperband</a> adds the Hyperband and Successive Halving algorithm.
</p>
</li>
<li> <p><a href="https://github.com/mlr-org/mlr3mbo">mlr3mbo</a> adds Bayesian optimization methods.
</p>
</li></ul>



<h3>Super classes</h3>

<p><code><a href="bbotk.html#topic+OptimInstance">bbotk::OptimInstance</a></code> -&gt; <code><a href="bbotk.html#topic+OptimInstanceAsync">bbotk::OptimInstanceAsync</a></code> -&gt; <code><a href="bbotk.html#topic+OptimInstanceAsyncSingleCrit">bbotk::OptimInstanceAsyncSingleCrit</a></code> -&gt; <code>TuningInstanceAsyncSingleCrit</code>
</p>


<h3>Active bindings</h3>

<div class="r6-active-bindings">

<dl>
<dt><code>result_learner_param_vals</code></dt><dd><p>(<code>list()</code>)<br />
Param values for the optimal learner call.</p>
</dd>
<dt><code>internal_search_space</code></dt><dd><p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>)<br />
The search space containing those parameters that are internally optimized by the <code><a href="mlr3.html#topic+Learner">mlr3::Learner</a></code>.</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TuningInstanceAsyncSingleCrit-new"><code>TuningInstanceAsyncSingleCrit$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TuningInstanceAsyncSingleCrit-assign_result"><code>TuningInstanceAsyncSingleCrit$assign_result()</code></a>
</p>
</li>
<li> <p><a href="#method-TuningInstanceAsyncSingleCrit-clone"><code>TuningInstanceAsyncSingleCrit$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstance" data-id="format"><a href='../../bbotk/html/OptimInstance.html#method-OptimInstance-format'><code>bbotk::OptimInstance$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstanceAsync" data-id="clear"><a href='../../bbotk/html/OptimInstanceAsync.html#method-OptimInstanceAsync-clear'><code>bbotk::OptimInstanceAsync$clear()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstanceAsync" data-id="print"><a href='../../bbotk/html/OptimInstanceAsync.html#method-OptimInstanceAsync-print'><code>bbotk::OptimInstanceAsync$print()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-TuningInstanceAsyncSingleCrit-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TuningInstanceAsyncSingleCrit$new(
  task,
  learner,
  resampling,
  measure = NULL,
  terminator,
  search_space = NULL,
  store_benchmark_result = TRUE,
  store_models = FALSE,
  check_values = FALSE,
  callbacks = NULL,
  rush = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>task</code></dt><dd><p>(<a href="mlr3.html#topic+Task">mlr3::Task</a>)<br />
Task to operate on.</p>
</dd>
<dt><code>learner</code></dt><dd><p>(<a href="mlr3.html#topic+Learner">mlr3::Learner</a>)<br />
Learner to tune.</p>
</dd>
<dt><code>resampling</code></dt><dd><p>(<a href="mlr3.html#topic+Resampling">mlr3::Resampling</a>)<br />
Resampling that is used to evaluate the performance of the hyperparameter configurations.
Uninstantiated resamplings are instantiated during construction so that all configurations are evaluated on the same data splits.
Already instantiated resamplings are kept unchanged.
Specialized <a href="#topic+Tuner">Tuner</a> change the resampling e.g. to evaluate a hyperparameter configuration on different data splits.
This field, however, always returns the resampling passed in construction.</p>
</dd>
<dt><code>measure</code></dt><dd><p>(<a href="mlr3.html#topic+Measure">mlr3::Measure</a>)<br />
Measure to optimize. If <code>NULL</code>, default measure is used.</p>
</dd>
<dt><code>terminator</code></dt><dd><p>(<a href="bbotk.html#topic+Terminator">bbotk::Terminator</a>)<br />
Stop criterion of the tuning process.</p>
</dd>
<dt><code>search_space</code></dt><dd><p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>)<br />
Hyperparameter search space. If <code>NULL</code> (default), the search space is
constructed from the <a href="paradox.html#topic+to_tune">paradox::TuneToken</a> of the learner's parameter set
(learner$param_set).</p>
</dd>
<dt><code>store_benchmark_result</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default), store resample result of evaluated hyperparameter
configurations in archive as <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>.</p>
</dd>
<dt><code>store_models</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, fitted models are stored in the benchmark result
(<code>archive$benchmark_result</code>). If <code>store_benchmark_result = FALSE</code>, models
are only stored temporarily and not accessible after the tuning. This
combination is needed for measures that require a model.</p>
</dd>
<dt><code>check_values</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, hyperparameter values are checked before evaluation and
performance scores after. If <code>FALSE</code> (default), values are unchecked but
computational overhead is reduced.</p>
</dd>
<dt><code>callbacks</code></dt><dd><p>(list of <a href="mlr3misc.html#topic+Callback">mlr3misc::Callback</a>)<br />
List of callbacks.</p>
</dd>
<dt><code>rush</code></dt><dd><p>(<code>Rush</code>)<br />
If a rush instance is supplied, the tuning runs without batches.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-TuningInstanceAsyncSingleCrit-assign_result"></a>



<h4>Method <code>assign_result()</code></h4>

<p>The <a href="#topic+TunerAsync">TunerAsync</a> object writes the best found point and estimated performance value here.
For internal use.
</p>


<h5>Usage</h5>

<div class="r"><pre>TuningInstanceAsyncSingleCrit$assign_result(xdt, y, learner_param_vals = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>xdt</code></dt><dd><p>(<code>data.table::data.table()</code>)<br />
Hyperparameter values as <code>data.table::data.table()</code>. Each row is one
configuration. Contains values in the search space. Can contain additional
columns for extra information.</p>
</dd>
<dt><code>y</code></dt><dd><p>(<code>numeric(1)</code>)<br />
Optimal outcome.</p>
</dd>
<dt><code>learner_param_vals</code></dt><dd><p>(List of named <code style="white-space: pre;">&#8288;list()s&#8288;</code>)<br />
Fixed parameter values of the learner that are neither part of the</p>
</dd>
</dl>

</div>


<hr>
<a id="method-TuningInstanceAsyncSingleCrit-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TuningInstanceAsyncSingleCrit$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='TuningInstanceBatchMultiCrit'>Class for Multi Criteria Tuning</h2><span id='topic+TuningInstanceBatchMultiCrit'></span>

<h3>Description</h3>

<p>The <a href="#topic+TuningInstanceBatchMultiCrit">TuningInstanceBatchMultiCrit</a> specifies a tuning problem for a <a href="#topic+Tuner">Tuner</a>.
The function <code><a href="#topic+ti">ti()</a></code> creates a <a href="#topic+TuningInstanceBatchMultiCrit">TuningInstanceBatchMultiCrit</a> and the function <code><a href="#topic+tune">tune()</a></code> creates an instance internally.
</p>


<h3>Details</h3>

<p>The instance contains an <a href="#topic+ObjectiveTuningBatch">ObjectiveTuningBatch</a> object that encodes the black box objective function a <a href="#topic+Tuner">Tuner</a> has to optimize.
The instance allows the basic operations of querying the objective at design points (<code style="white-space: pre;">&#8288;$eval_batch()&#8288;</code>).
This operation is usually done by the <a href="#topic+Tuner">Tuner</a>.
Evaluations of hyperparameter configurations are performed in batches by calling <code><a href="mlr3.html#topic+benchmark">mlr3::benchmark()</a></code> internally.
The evaluated hyperparameter configurations are stored in the <a href="#topic+ArchiveBatchTuning">ArchiveBatchTuning</a> (<code style="white-space: pre;">&#8288;$archive&#8288;</code>).
Before a batch is evaluated, the <a href="bbotk.html#topic+Terminator">bbotk::Terminator</a> is queried for the remaining budget.
If the available budget is exhausted, an exception is raised, and no further evaluations can be performed from this point on.
The tuner is also supposed to store its final result, consisting of a selected hyperparameter configuration and associated estimated performance values, by calling the method <code>instance$assign_result</code>.
</p>


<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li><p> Learn about <a href="https://mlr3book.mlr-org.com/chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html#sec-multi-metrics-tuning">multi-objective optimization</a>.
</p>
</li></ul>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>


<h3>Analysis</h3>

<p>For analyzing the tuning results, it is recommended to pass the <a href="#topic+ArchiveBatchTuning">ArchiveBatchTuning</a> to <code>as.data.table()</code>.
The returned data table is joined with the benchmark result which adds the <a href="mlr3.html#topic+ResampleResult">mlr3::ResampleResult</a> for each hyperparameter evaluation.
</p>
<p>The archive provides various getters (e.g. <code style="white-space: pre;">&#8288;$learners()&#8288;</code>) to ease the access.
All getters extract by position (<code>i</code>) or unique hash (<code>uhash</code>).
For a complete list of all getters see the methods section.
</p>
<p>The benchmark result (<code style="white-space: pre;">&#8288;$benchmark_result&#8288;</code>) allows to score the hyperparameter configurations again on a different measure.
Alternatively, measures can be supplied to <code>as.data.table()</code>.
</p>
<p>The <a href="https://CRAN.R-project.org/package=mlr3viz"><span class="pkg">mlr3viz</span></a> package provides visualizations for tuning results.
</p>


<h3>Super classes</h3>

<p><code><a href="bbotk.html#topic+OptimInstance">bbotk::OptimInstance</a></code> -&gt; <code><a href="bbotk.html#topic+OptimInstanceBatch">bbotk::OptimInstanceBatch</a></code> -&gt; <code><a href="bbotk.html#topic+OptimInstanceBatchMultiCrit">bbotk::OptimInstanceBatchMultiCrit</a></code> -&gt; <code>TuningInstanceBatchMultiCrit</code>
</p>


<h3>Active bindings</h3>

<div class="r6-active-bindings">

<dl>
<dt><code>result_learner_param_vals</code></dt><dd><p>(<code>list()</code>)<br />
List of param values for the optimal learner call.</p>
</dd>
<dt><code>internal_search_space</code></dt><dd><p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>)<br />
The search space containing those parameters that are internally optimized by the <code><a href="mlr3.html#topic+Learner">mlr3::Learner</a></code>.</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TuningInstanceBatchMultiCrit-new"><code>TuningInstanceBatchMultiCrit$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TuningInstanceBatchMultiCrit-assign_result"><code>TuningInstanceBatchMultiCrit$assign_result()</code></a>
</p>
</li>
<li> <p><a href="#method-TuningInstanceBatchMultiCrit-clone"><code>TuningInstanceBatchMultiCrit$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstance" data-id="clear"><a href='../../bbotk/html/OptimInstance.html#method-OptimInstance-clear'><code>bbotk::OptimInstance$clear()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstance" data-id="format"><a href='../../bbotk/html/OptimInstance.html#method-OptimInstance-format'><code>bbotk::OptimInstance$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstance" data-id="print"><a href='../../bbotk/html/OptimInstance.html#method-OptimInstance-print'><code>bbotk::OptimInstance$print()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstanceBatch" data-id="eval_batch"><a href='../../bbotk/html/OptimInstanceBatch.html#method-OptimInstanceBatch-eval_batch'><code>bbotk::OptimInstanceBatch$eval_batch()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstanceBatch" data-id="objective_function"><a href='../../bbotk/html/OptimInstanceBatch.html#method-OptimInstanceBatch-objective_function'><code>bbotk::OptimInstanceBatch$objective_function()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-TuningInstanceBatchMultiCrit-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TuningInstanceBatchMultiCrit$new(
  task,
  learner,
  resampling,
  measures,
  terminator,
  search_space = NULL,
  store_benchmark_result = TRUE,
  store_models = FALSE,
  check_values = FALSE,
  callbacks = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>task</code></dt><dd><p>(<a href="mlr3.html#topic+Task">mlr3::Task</a>)<br />
Task to operate on.</p>
</dd>
<dt><code>learner</code></dt><dd><p>(<a href="mlr3.html#topic+Learner">mlr3::Learner</a>)<br />
Learner to tune.</p>
</dd>
<dt><code>resampling</code></dt><dd><p>(<a href="mlr3.html#topic+Resampling">mlr3::Resampling</a>)<br />
Resampling that is used to evaluate the performance of the hyperparameter configurations.
Uninstantiated resamplings are instantiated during construction so that all configurations are evaluated on the same data splits.
Already instantiated resamplings are kept unchanged.
Specialized <a href="#topic+Tuner">Tuner</a> change the resampling e.g. to evaluate a hyperparameter configuration on different data splits.
This field, however, always returns the resampling passed in construction.</p>
</dd>
<dt><code>measures</code></dt><dd><p>(list of <a href="mlr3.html#topic+Measure">mlr3::Measure</a>)<br />
Measures to optimize.</p>
</dd>
<dt><code>terminator</code></dt><dd><p>(<a href="bbotk.html#topic+Terminator">bbotk::Terminator</a>)<br />
Stop criterion of the tuning process.</p>
</dd>
<dt><code>search_space</code></dt><dd><p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>)<br />
Hyperparameter search space. If <code>NULL</code> (default), the search space is
constructed from the <a href="paradox.html#topic+to_tune">paradox::TuneToken</a> of the learner's parameter set
(learner$param_set).</p>
</dd>
<dt><code>store_benchmark_result</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default), store resample result of evaluated hyperparameter
configurations in archive as <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>.</p>
</dd>
<dt><code>store_models</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, fitted models are stored in the benchmark result
(<code>archive$benchmark_result</code>). If <code>store_benchmark_result = FALSE</code>, models
are only stored temporarily and not accessible after the tuning. This
combination is needed for measures that require a model.</p>
</dd>
<dt><code>check_values</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, hyperparameter values are checked before evaluation and
performance scores after. If <code>FALSE</code> (default), values are unchecked but
computational overhead is reduced.</p>
</dd>
<dt><code>callbacks</code></dt><dd><p>(list of <a href="mlr3misc.html#topic+Callback">mlr3misc::Callback</a>)<br />
List of callbacks.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-TuningInstanceBatchMultiCrit-assign_result"></a>



<h4>Method <code>assign_result()</code></h4>

<p>The <a href="#topic+Tuner">Tuner</a> object writes the best found points and estimated performance values here.
For internal use.
</p>


<h5>Usage</h5>

<div class="r"><pre>TuningInstanceBatchMultiCrit$assign_result(xdt, ydt, learner_param_vals = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>xdt</code></dt><dd><p>(<code>data.table::data.table()</code>)<br />
Hyperparameter values as <code>data.table::data.table()</code>. Each row is one
configuration. Contains values in the search space. Can contain additional
columns for extra information.</p>
</dd>
<dt><code>ydt</code></dt><dd><p>(<code>data.table::data.table()</code>)<br />
Optimal outcomes, e.g. the Pareto front.</p>
</dd>
<dt><code>learner_param_vals</code></dt><dd><p>(List of named <code style="white-space: pre;">&#8288;list()s&#8288;</code>)<br />
Fixed parameter values of the learner that are neither part of the</p>
</dd>
</dl>

</div>


<hr>
<a id="method-TuningInstanceBatchMultiCrit-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TuningInstanceBatchMultiCrit$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'># Hyperparameter optimization on the Palmer Penguins data set
task = tsk("penguins")

# Load learner and set search space
learner = lrn("classif.rpart",
  cp = to_tune(1e-04, 1e-1, logscale = TRUE)
)

# Construct tuning instance
instance = ti(
  task = task,
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msrs(c("classif.ce", "time_train")),
  terminator = trm("evals", n_evals = 4)
)

# Choose optimization algorithm
tuner = tnr("random_search", batch_size = 2)

# Run tuning
tuner$optimize(instance)

# Optimal hyperparameter configurations
instance$result

# Inspect all evaluated configurations
as.data.table(instance$archive)
</code></pre>

<hr>
<h2 id='TuningInstanceBatchSingleCrit'>Class for Single Criterion Tuning</h2><span id='topic+TuningInstanceBatchSingleCrit'></span>

<h3>Description</h3>

<p>The <a href="#topic+TuningInstanceBatchSingleCrit">TuningInstanceBatchSingleCrit</a> specifies a tuning problem for a <a href="#topic+Tuner">Tuner</a>.
The function <code><a href="#topic+ti">ti()</a></code> creates a <a href="#topic+TuningInstanceBatchSingleCrit">TuningInstanceBatchSingleCrit</a> and the function <code><a href="#topic+tune">tune()</a></code> creates an instance internally.
</p>


<h3>Details</h3>

<p>The instance contains an <a href="#topic+ObjectiveTuningBatch">ObjectiveTuningBatch</a> object that encodes the black box objective function a <a href="#topic+Tuner">Tuner</a> has to optimize.
The instance allows the basic operations of querying the objective at design points (<code style="white-space: pre;">&#8288;$eval_batch()&#8288;</code>).
This operation is usually done by the <a href="#topic+Tuner">Tuner</a>.
Evaluations of hyperparameter configurations are performed in batches by calling <code><a href="mlr3.html#topic+benchmark">mlr3::benchmark()</a></code> internally.
The evaluated hyperparameter configurations are stored in the <a href="#topic+ArchiveBatchTuning">ArchiveBatchTuning</a> (<code style="white-space: pre;">&#8288;$archive&#8288;</code>).
Before a batch is evaluated, the <a href="bbotk.html#topic+Terminator">bbotk::Terminator</a> is queried for the remaining budget.
If the available budget is exhausted, an exception is raised, and no further evaluations can be performed from this point on.
The tuner is also supposed to store its final result, consisting of a selected hyperparameter configuration and associated estimated performance values, by calling the method <code>instance$assign_result</code>.
</p>


<h3>Default Measures</h3>

<p>If no measure is passed, the default measure is used.
The default measure depends on the task type.</p>

<table>
<tr>
 <td style="text-align: left;">
   Task </td><td style="text-align: left;"> Default Measure </td><td style="text-align: left;"> Package </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"classif"</code> </td><td style="text-align: left;"> <code>"classif.ce"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3"><span class="pkg">mlr3</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"regr"</code> </td><td style="text-align: left;"> <code>"regr.mse"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3"><span class="pkg">mlr3</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"surv"</code> </td><td style="text-align: left;"> <code>"surv.cindex"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3proba"><span class="pkg">mlr3proba</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"dens"</code> </td><td style="text-align: left;"> <code>"dens.logloss"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3proba"><span class="pkg">mlr3proba</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"classif_st"</code> </td><td style="text-align: left;"> <code>"classif.ce"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3spatial"><span class="pkg">mlr3spatial</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"regr_st"</code> </td><td style="text-align: left;"> <code>"regr.mse"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3spatial"><span class="pkg">mlr3spatial</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code>"clust"</code> </td><td style="text-align: left;"> <code>"clust.dunn"</code> </td><td style="text-align: left;"> <a href="https://CRAN.R-project.org/package=mlr3cluster"><span class="pkg">mlr3cluster</span></a> </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li><p> Getting started with <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html">hyperparameter optimization</a>.
</p>
</li>
<li> <p><a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-model-tuning">Tune</a> a simple classification tree on the Sonar data set.
</p>
</li>
<li><p> Learn about <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-defining-search-spaces">tuning spaces</a>.
</p>
</li></ul>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>

<ul>
<li><p> Learn more advanced methods with the <a href="https://mlr-org.com/gallery/series/2021-03-09-practical-tuning-series-tune-a-support-vector-machine/">practical tuning series</a>.
</p>
</li>
<li><p> Simultaneously optimize hyperparameters and use <a href="https://mlr-org.com/gallery/optimization/2022-11-04-early-stopping-with-xgboost/">early stopping</a> with XGBoost.
</p>
</li>
<li><p> Make us of proven <a href="https://mlr-org.com/gallery/optimization/2021-07-06-introduction-to-mlr3tuningspaces/">search space</a>.
</p>
</li>
<li><p> Learn about <a href="https://mlr-org.com/gallery/optimization/2023-01-16-hotstart/">hotstarting</a> models.
</p>
</li>
<li><p> Run the <a href="https://mlr-org.com/gallery/optimization/2023-01-31-default-configuration/">default hyperparameter configuration</a> of learners as a baseline.
</p>
</li></ul>



<h3>Extension Packages</h3>

<p>mlr3tuning is extended by the following packages.
</p>

<ul>
<li> <p><a href="https://github.com/mlr-org/mlr3tuningspaces">mlr3tuningspaces</a> is a collection of search spaces from scientific articles for commonly used learners.
</p>
</li>
<li> <p><a href="https://github.com/mlr-org/mlr3hyperband">mlr3hyperband</a> adds the Hyperband and Successive Halving algorithm.
</p>
</li>
<li> <p><a href="https://github.com/mlr-org/mlr3mbo">mlr3mbo</a> adds Bayesian optimization methods.
</p>
</li></ul>



<h3>Analysis</h3>

<p>For analyzing the tuning results, it is recommended to pass the <a href="#topic+ArchiveBatchTuning">ArchiveBatchTuning</a> to <code>as.data.table()</code>.
The returned data table is joined with the benchmark result which adds the <a href="mlr3.html#topic+ResampleResult">mlr3::ResampleResult</a> for each hyperparameter evaluation.
</p>
<p>The archive provides various getters (e.g. <code style="white-space: pre;">&#8288;$learners()&#8288;</code>) to ease the access.
All getters extract by position (<code>i</code>) or unique hash (<code>uhash</code>).
For a complete list of all getters see the methods section.
</p>
<p>The benchmark result (<code style="white-space: pre;">&#8288;$benchmark_result&#8288;</code>) allows to score the hyperparameter configurations again on a different measure.
Alternatively, measures can be supplied to <code>as.data.table()</code>.
</p>
<p>The <a href="https://CRAN.R-project.org/package=mlr3viz"><span class="pkg">mlr3viz</span></a> package provides visualizations for tuning results.
</p>


<h3>Super classes</h3>

<p><code><a href="bbotk.html#topic+OptimInstance">bbotk::OptimInstance</a></code> -&gt; <code><a href="bbotk.html#topic+OptimInstanceBatch">bbotk::OptimInstanceBatch</a></code> -&gt; <code><a href="bbotk.html#topic+OptimInstanceBatchSingleCrit">bbotk::OptimInstanceBatchSingleCrit</a></code> -&gt; <code>TuningInstanceBatchSingleCrit</code>
</p>


<h3>Active bindings</h3>

<div class="r6-active-bindings">

<dl>
<dt><code>result_learner_param_vals</code></dt><dd><p>(<code>list()</code>)<br />
Param values for the optimal learner call.</p>
</dd>
<dt><code>internal_search_space</code></dt><dd><p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>)<br />
The search space containing those parameters that are internally optimized by the <code><a href="mlr3.html#topic+Learner">mlr3::Learner</a></code>.</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TuningInstanceBatchSingleCrit-new"><code>TuningInstanceBatchSingleCrit$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TuningInstanceBatchSingleCrit-assign_result"><code>TuningInstanceBatchSingleCrit$assign_result()</code></a>
</p>
</li>
<li> <p><a href="#method-TuningInstanceBatchSingleCrit-clone"><code>TuningInstanceBatchSingleCrit$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstance" data-id="clear"><a href='../../bbotk/html/OptimInstance.html#method-OptimInstance-clear'><code>bbotk::OptimInstance$clear()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstance" data-id="format"><a href='../../bbotk/html/OptimInstance.html#method-OptimInstance-format'><code>bbotk::OptimInstance$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstance" data-id="print"><a href='../../bbotk/html/OptimInstance.html#method-OptimInstance-print'><code>bbotk::OptimInstance$print()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstanceBatch" data-id="eval_batch"><a href='../../bbotk/html/OptimInstanceBatch.html#method-OptimInstanceBatch-eval_batch'><code>bbotk::OptimInstanceBatch$eval_batch()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstanceBatch" data-id="objective_function"><a href='../../bbotk/html/OptimInstanceBatch.html#method-OptimInstanceBatch-objective_function'><code>bbotk::OptimInstanceBatch$objective_function()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-TuningInstanceBatchSingleCrit-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TuningInstanceBatchSingleCrit$new(
  task,
  learner,
  resampling,
  measure = NULL,
  terminator,
  search_space = NULL,
  store_benchmark_result = TRUE,
  store_models = FALSE,
  check_values = FALSE,
  callbacks = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>task</code></dt><dd><p>(<a href="mlr3.html#topic+Task">mlr3::Task</a>)<br />
Task to operate on.</p>
</dd>
<dt><code>learner</code></dt><dd><p>(<a href="mlr3.html#topic+Learner">mlr3::Learner</a>)<br />
Learner to tune.</p>
</dd>
<dt><code>resampling</code></dt><dd><p>(<a href="mlr3.html#topic+Resampling">mlr3::Resampling</a>)<br />
Resampling that is used to evaluate the performance of the hyperparameter configurations.
Uninstantiated resamplings are instantiated during construction so that all configurations are evaluated on the same data splits.
Already instantiated resamplings are kept unchanged.
Specialized <a href="#topic+Tuner">Tuner</a> change the resampling e.g. to evaluate a hyperparameter configuration on different data splits.
This field, however, always returns the resampling passed in construction.</p>
</dd>
<dt><code>measure</code></dt><dd><p>(<a href="mlr3.html#topic+Measure">mlr3::Measure</a>)<br />
Measure to optimize. If <code>NULL</code>, default measure is used.</p>
</dd>
<dt><code>terminator</code></dt><dd><p>(<a href="bbotk.html#topic+Terminator">bbotk::Terminator</a>)<br />
Stop criterion of the tuning process.</p>
</dd>
<dt><code>search_space</code></dt><dd><p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>)<br />
Hyperparameter search space. If <code>NULL</code> (default), the search space is
constructed from the <a href="paradox.html#topic+to_tune">paradox::TuneToken</a> of the learner's parameter set
(learner$param_set).</p>
</dd>
<dt><code>store_benchmark_result</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default), store resample result of evaluated hyperparameter
configurations in archive as <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>.</p>
</dd>
<dt><code>store_models</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, fitted models are stored in the benchmark result
(<code>archive$benchmark_result</code>). If <code>store_benchmark_result = FALSE</code>, models
are only stored temporarily and not accessible after the tuning. This
combination is needed for measures that require a model.</p>
</dd>
<dt><code>check_values</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, hyperparameter values are checked before evaluation and
performance scores after. If <code>FALSE</code> (default), values are unchecked but
computational overhead is reduced.</p>
</dd>
<dt><code>callbacks</code></dt><dd><p>(list of <a href="mlr3misc.html#topic+Callback">mlr3misc::Callback</a>)<br />
List of callbacks.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-TuningInstanceBatchSingleCrit-assign_result"></a>



<h4>Method <code>assign_result()</code></h4>

<p>The <a href="#topic+Tuner">Tuner</a> object writes the best found point and estimated performance value here.
For internal use.
</p>


<h5>Usage</h5>

<div class="r"><pre>TuningInstanceBatchSingleCrit$assign_result(xdt, y, learner_param_vals = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>xdt</code></dt><dd><p>(<code>data.table::data.table()</code>)<br />
Hyperparameter values as <code>data.table::data.table()</code>. Each row is one
configuration. Contains values in the search space. Can contain additional
columns for extra information.</p>
</dd>
<dt><code>y</code></dt><dd><p>(<code>numeric(1)</code>)<br />
Optimal outcome.</p>
</dd>
<dt><code>learner_param_vals</code></dt><dd><p>(List of named <code style="white-space: pre;">&#8288;list()s&#8288;</code>)<br />
Fixed parameter values of the learner that are neither part of the</p>
</dd>
</dl>

</div>


<hr>
<a id="method-TuningInstanceBatchSingleCrit-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TuningInstanceBatchSingleCrit$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'># Hyperparameter optimization on the Palmer Penguins data set
task = tsk("penguins")

# Load learner and set search space
learner = lrn("classif.rpart",
  cp = to_tune(1e-04, 1e-1, logscale = TRUE)
)

# Construct tuning instance
instance = ti(
  task = task,
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  terminator = trm("evals", n_evals = 4)
)

# Choose optimization algorithm
tuner = tnr("random_search", batch_size = 2)

# Run tuning
tuner$optimize(instance)

# Set optimal hyperparameter configuration to learner
learner$param_set$values = instance$result_learner_param_vals

# Train the learner on the full data set
learner$train(task)

# Inspect all evaluated configurations
as.data.table(instance$archive)
</code></pre>

<hr>
<h2 id='TuningInstanceMultiCrit'>Multi Criteria Tuning Instance for Batch Tuning</h2><span id='topic+TuningInstanceMultiCrit'></span>

<h3>Description</h3>

<p><code>TuningInstanceMultiCrit</code> is a deprecated class that is now a wrapper around <a href="#topic+TuningInstanceBatchMultiCrit">TuningInstanceBatchMultiCrit</a>.
</p>


<h3>Super classes</h3>

<p><code><a href="bbotk.html#topic+OptimInstance">bbotk::OptimInstance</a></code> -&gt; <code><a href="bbotk.html#topic+OptimInstanceBatch">bbotk::OptimInstanceBatch</a></code> -&gt; <code><a href="bbotk.html#topic+OptimInstanceBatchMultiCrit">bbotk::OptimInstanceBatchMultiCrit</a></code> -&gt; <code><a href="#topic+TuningInstanceBatchMultiCrit">mlr3tuning::TuningInstanceBatchMultiCrit</a></code> -&gt; <code>TuningInstanceMultiCrit</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TuningInstanceMultiCrit-new"><code>TuningInstanceMultiCrit$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TuningInstanceMultiCrit-clone"><code>TuningInstanceMultiCrit$clone()</code></a>
</p>
</li></ul>



<details><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstance" data-id="clear"><a href='../../bbotk/html/OptimInstance.html#method-OptimInstance-clear'><code>bbotk::OptimInstance$clear()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstance" data-id="format"><a href='../../bbotk/html/OptimInstance.html#method-OptimInstance-format'><code>bbotk::OptimInstance$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstance" data-id="print"><a href='../../bbotk/html/OptimInstance.html#method-OptimInstance-print'><code>bbotk::OptimInstance$print()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstanceBatch" data-id="eval_batch"><a href='../../bbotk/html/OptimInstanceBatch.html#method-OptimInstanceBatch-eval_batch'><code>bbotk::OptimInstanceBatch$eval_batch()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstanceBatch" data-id="objective_function"><a href='../../bbotk/html/OptimInstanceBatch.html#method-OptimInstanceBatch-objective_function'><code>bbotk::OptimInstanceBatch$objective_function()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="TuningInstanceBatchMultiCrit" data-id="assign_result"><a href='../../mlr3tuning/html/TuningInstanceBatchMultiCrit.html#method-TuningInstanceBatchMultiCrit-assign_result'><code>mlr3tuning::TuningInstanceBatchMultiCrit$assign_result()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-TuningInstanceMultiCrit-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TuningInstanceMultiCrit$new(
  task,
  learner,
  resampling,
  measures,
  terminator,
  search_space = NULL,
  store_benchmark_result = TRUE,
  store_models = FALSE,
  check_values = FALSE,
  callbacks = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>task</code></dt><dd><p>(<a href="mlr3.html#topic+Task">mlr3::Task</a>)<br />
Task to operate on.</p>
</dd>
<dt><code>learner</code></dt><dd><p>(<a href="mlr3.html#topic+Learner">mlr3::Learner</a>)<br />
Learner to tune.</p>
</dd>
<dt><code>resampling</code></dt><dd><p>(<a href="mlr3.html#topic+Resampling">mlr3::Resampling</a>)<br />
Resampling that is used to evaluate the performance of the hyperparameter configurations.
Uninstantiated resamplings are instantiated during construction so that all configurations are evaluated on the same data splits.
Already instantiated resamplings are kept unchanged.
Specialized <a href="#topic+Tuner">Tuner</a> change the resampling e.g. to evaluate a hyperparameter configuration on different data splits.
This field, however, always returns the resampling passed in construction.</p>
</dd>
<dt><code>measures</code></dt><dd><p>(list of <a href="mlr3.html#topic+Measure">mlr3::Measure</a>)<br />
Measures to optimize.</p>
</dd>
<dt><code>terminator</code></dt><dd><p>(<a href="bbotk.html#topic+Terminator">bbotk::Terminator</a>)<br />
Stop criterion of the tuning process.</p>
</dd>
<dt><code>search_space</code></dt><dd><p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>)<br />
Hyperparameter search space. If <code>NULL</code> (default), the search space is
constructed from the <a href="paradox.html#topic+to_tune">paradox::TuneToken</a> of the learner's parameter set
(learner$param_set).</p>
</dd>
<dt><code>store_benchmark_result</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default), store resample result of evaluated hyperparameter
configurations in archive as <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>.</p>
</dd>
<dt><code>store_models</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, fitted models are stored in the benchmark result
(<code>archive$benchmark_result</code>). If <code>store_benchmark_result = FALSE</code>, models
are only stored temporarily and not accessible after the tuning. This
combination is needed for measures that require a model.</p>
</dd>
<dt><code>check_values</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, hyperparameter values are checked before evaluation and
performance scores after. If <code>FALSE</code> (default), values are unchecked but
computational overhead is reduced.</p>
</dd>
<dt><code>callbacks</code></dt><dd><p>(list of <a href="mlr3misc.html#topic+Callback">mlr3misc::Callback</a>)<br />
List of callbacks.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-TuningInstanceMultiCrit-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TuningInstanceMultiCrit$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='TuningInstanceSingleCrit'>Single Criterion Tuning Instance for Batch Tuning</h2><span id='topic+TuningInstanceSingleCrit'></span>

<h3>Description</h3>

<p><code>TuningInstanceSingleCrit</code> is a deprecated class that is now a wrapper around <a href="#topic+TuningInstanceBatchSingleCrit">TuningInstanceBatchSingleCrit</a>.
</p>


<h3>Super classes</h3>

<p><code><a href="bbotk.html#topic+OptimInstance">bbotk::OptimInstance</a></code> -&gt; <code><a href="bbotk.html#topic+OptimInstanceBatch">bbotk::OptimInstanceBatch</a></code> -&gt; <code><a href="bbotk.html#topic+OptimInstanceBatchSingleCrit">bbotk::OptimInstanceBatchSingleCrit</a></code> -&gt; <code><a href="#topic+TuningInstanceBatchSingleCrit">mlr3tuning::TuningInstanceBatchSingleCrit</a></code> -&gt; <code>TuningInstanceSingleCrit</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TuningInstanceSingleCrit-new"><code>TuningInstanceSingleCrit$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TuningInstanceSingleCrit-clone"><code>TuningInstanceSingleCrit$clone()</code></a>
</p>
</li></ul>



<details><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstance" data-id="clear"><a href='../../bbotk/html/OptimInstance.html#method-OptimInstance-clear'><code>bbotk::OptimInstance$clear()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstance" data-id="format"><a href='../../bbotk/html/OptimInstance.html#method-OptimInstance-format'><code>bbotk::OptimInstance$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstance" data-id="print"><a href='../../bbotk/html/OptimInstance.html#method-OptimInstance-print'><code>bbotk::OptimInstance$print()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstanceBatch" data-id="eval_batch"><a href='../../bbotk/html/OptimInstanceBatch.html#method-OptimInstanceBatch-eval_batch'><code>bbotk::OptimInstanceBatch$eval_batch()</code></a></span></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="OptimInstanceBatch" data-id="objective_function"><a href='../../bbotk/html/OptimInstanceBatch.html#method-OptimInstanceBatch-objective_function'><code>bbotk::OptimInstanceBatch$objective_function()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="TuningInstanceBatchSingleCrit" data-id="assign_result"><a href='../../mlr3tuning/html/TuningInstanceBatchSingleCrit.html#method-TuningInstanceBatchSingleCrit-assign_result'><code>mlr3tuning::TuningInstanceBatchSingleCrit$assign_result()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-TuningInstanceSingleCrit-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new instance of this <a href="R6.html#topic+R6Class">R6</a> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TuningInstanceSingleCrit$new(
  task,
  learner,
  resampling,
  measure = NULL,
  terminator,
  search_space = NULL,
  store_benchmark_result = TRUE,
  store_models = FALSE,
  check_values = FALSE,
  callbacks = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>task</code></dt><dd><p>(<a href="mlr3.html#topic+Task">mlr3::Task</a>)<br />
Task to operate on.</p>
</dd>
<dt><code>learner</code></dt><dd><p>(<a href="mlr3.html#topic+Learner">mlr3::Learner</a>)<br />
Learner to tune.</p>
</dd>
<dt><code>resampling</code></dt><dd><p>(<a href="mlr3.html#topic+Resampling">mlr3::Resampling</a>)<br />
Resampling that is used to evaluate the performance of the hyperparameter configurations.
Uninstantiated resamplings are instantiated during construction so that all configurations are evaluated on the same data splits.
Already instantiated resamplings are kept unchanged.
Specialized <a href="#topic+Tuner">Tuner</a> change the resampling e.g. to evaluate a hyperparameter configuration on different data splits.
This field, however, always returns the resampling passed in construction.</p>
</dd>
<dt><code>measure</code></dt><dd><p>(<a href="mlr3.html#topic+Measure">mlr3::Measure</a>)<br />
Measure to optimize. If <code>NULL</code>, default measure is used.</p>
</dd>
<dt><code>terminator</code></dt><dd><p>(<a href="bbotk.html#topic+Terminator">bbotk::Terminator</a>)<br />
Stop criterion of the tuning process.</p>
</dd>
<dt><code>search_space</code></dt><dd><p>(<a href="paradox.html#topic+ParamSet">paradox::ParamSet</a>)<br />
Hyperparameter search space. If <code>NULL</code> (default), the search space is
constructed from the <a href="paradox.html#topic+to_tune">paradox::TuneToken</a> of the learner's parameter set
(learner$param_set).</p>
</dd>
<dt><code>store_benchmark_result</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code> (default), store resample result of evaluated hyperparameter
configurations in archive as <a href="mlr3.html#topic+BenchmarkResult">mlr3::BenchmarkResult</a>.</p>
</dd>
<dt><code>store_models</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, fitted models are stored in the benchmark result
(<code>archive$benchmark_result</code>). If <code>store_benchmark_result = FALSE</code>, models
are only stored temporarily and not accessible after the tuning. This
combination is needed for measures that require a model.</p>
</dd>
<dt><code>check_values</code></dt><dd><p>(<code>logical(1)</code>)<br />
If <code>TRUE</code>, hyperparameter values are checked before evaluation and
performance scores after. If <code>FALSE</code> (default), values are unchecked but
computational overhead is reduced.</p>
</dd>
<dt><code>callbacks</code></dt><dd><p>(list of <a href="mlr3misc.html#topic+Callback">mlr3misc::Callback</a>)<br />
List of callbacks.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-TuningInstanceSingleCrit-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TuningInstanceSingleCrit$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
