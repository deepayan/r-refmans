<!DOCTYPE html><html><head><title>Help for package PracTools</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {PracTools}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#BW2stagePPS'>
<p>Relvariance components for 2-stage sample</p></a></li>
<li><a href='#BW2stagePPSe'>
<p>Estimated relvariance components for 2-stage sample</p></a></li>
<li><a href='#BW2stageSRS'>
<p>Relvariance components for 2-stage sample</p></a></li>
<li><a href='#BW3stagePPS'>
<p>Relvariance components for 3-stage sample</p></a></li>
<li><a href='#BW3stagePPSe'>
<p>Estimated relvariance components for 3-stage sample</p></a></li>
<li><a href='#clusOpt2'>
<p>Compute optimal sample sizes for a two-stage sample</p></a></li>
<li><a href='#clusOpt2fixedPSU'>
<p>Optimal number of sample elements per PSU in a two-stage sample when the sample of PSUs is fixed</p></a></li>
<li><a href='#clusOpt3'>
<p>Compute optimal sample sizes for a three-stage sample</p></a></li>
<li><a href='#clusOpt3fixedPSU'>
<p>Compute optimal number of sample secondary sampling units (SSUs) and elements per SSU for a fixed set of primary sampling units (PSUs) in a three-stage sample</p></a></li>
<li><a href='#CompMOS'>
<p>Compute a composite measure of size for domain-based two-stage sampling</p></a></li>
<li><a href='#CVcalc2'>
<p>Coefficient of variation of an estimated total in a 2-stage sample</p></a></li>
<li><a href='#CVcalc3'>
<p>Coefficient of variation of an estimated total in a 3-stage sample</p></a></li>
<li><a href='#deff'>
<p>Design effects of various types</p></a></li>
<li><a href='#deffCR'>
<p>Chen-Rust design effect</p></a></li>
<li><a href='#deffH'>
<p>Henry design effect for <em>pps</em> sampling and GREG estimation of totals</p></a></li>
<li><a href='#deffK'>
<p>Kish design effect</p></a></li>
<li><a href='#deffS'>
<p>Spencer design effect for an estimated total from a <em>pps</em> sample</p></a></li>
<li><a href='#Domainy1y2'>
<p>Domain data</p></a></li>
<li><a href='#dub'>
<p>Sample sizes for a double sampling design</p></a></li>
<li><a href='#gamEst'>
<p>Estimate variance model parameter <code class="reqn">\gamma</code></p></a></li>
<li><a href='#gammaFit'>
<p>Iteratively estimate variance model parameter <code class="reqn">\gamma</code></p></a></li>
<li><a href='#GeoDistMOS'>
<p>Split geographic PSUs based on a measure of size threshold</p></a></li>
<li><a href='#GeoDistPSU'>
<p>Form PSUs based on geographic distances</p></a></li>
<li><a href='#GeoMinMOS'>
<p>Check geographic PSUs to determine whether any are less than minimum measure of size threshold</p></a></li>
<li><a href='#HMT'>
<p>Generate an HMT population</p></a></li>
<li><a href='#hospital'>
<p>Hospital Data</p></a></li>
<li><a href='#labor'>
<p>Labor force population</p></a></li>
<li><a href='#MDarea.popA'>
<p>Maryland area population</p></a></li>
<li><a href='#mibrfss'>
<p>Michigan Behavioral Risk Factor Surveillance Survey</p></a></li>
<li><a href='#nAuditAttr'>
<p>Sample sizes for an attribute sample in an audit</p></a></li>
<li><a href='#nAuditMUS'>
<p>Sample sizes for a Monetary Unit Sample in an audit</p></a></li>
<li><a href='#nCont'>
<p>Compute a simple random sample size for an estimated mean</p></a></li>
<li><a href='#nContMoe'>
<p>Compute a simple random sample size for an estimated mean of a continuous variable based on margin of error</p></a></li>
<li><a href='#nContOpt'>
<p>Compute the sample size required to estimate the mean of a continuous variable by optimizing the numbers of take-alls and non-take-all units selected by probability sampling</p></a></li>
<li><a href='#nDep2sam'>
<p>Simple random sample size for difference in means</p></a></li>
<li><a href='#nDomain'>
<p>Compute a simple random sample size for an estimated mean or total for a domain</p></a></li>
<li><a href='#nEdge'>
<p>Compute the total sample size for a stratified, simple random sample based on an Edgeworth approximation</p></a></li>
<li><a href='#nEdgeSRS'>
<p>Compute the total sample size for a simple random sample based on an Edgeworth approximation</p></a></li>
<li><a href='#nhis'>
<p>National Health Interview Survey: Demographic variables</p></a></li>
<li><a href='#nhis.large'>
<p>National Health Interview Survey: Demographic and health variables</p></a></li>
<li><a href='#nhispart'>
<p>National Health Interview Survey data from 2003: socioeconomic variables</p></a></li>
<li><a href='#nLogOdds'>
<p>Calculate simple random sample size for estimating a proportion</p></a></li>
<li><a href='#nPPS'>
<p>Calculate the sample size for a probability proportional to size (PPS) sample</p></a></li>
<li><a href='#nProp'>
<p>Compute simple random sample size for estimating a proportion</p></a></li>
<li><a href='#nProp2sam'>
<p>Simple random sample size for difference in proportions</p></a></li>
<li><a href='#nPropMoe'>
<p>Simple random sample size for a proportion based on margin of error</p></a></li>
<li><a href='#NRadjClass'>
<p>Class-based nonresponse adjustments</p></a></li>
<li><a href='#NRFUopt'>
<p>Sample sizes for a nonresponse follow-up study</p></a></li>
<li><a href='#nWilson'>
<p>Calculate a simple random sample size for estimating a proportion</p></a></li>
<li><a href='#pclass'>
<p>Form nonresponse adjustment classes based on propensity scores</p></a></li>
<li><a href='#quad_roots'>
<p>Compute the roots of a quadratic equation</p></a></li>
<li><a href='#smho.N874'>
<p>Survey of Mental Health Organizations Data</p></a></li>
<li><a href='#smho98'>
<p>Survey of Mental Health Organizations Data</p></a></li>
<li><a href='#strAlloc'>
<p>Allocate a sample to strata</p></a></li>
<li><a href='#Test_Data_US'>
<p>Accounting data for some US cities with latitude and longitude of the city centroids</p></a></li>
<li><a href='#ThirdGrade'>
<p>Third grade population</p></a></li>
<li><a href='#TPV'>
<p>TPV Data</p></a></li>
<li><a href='#unitVar'>
<p>Compute the unit (population) variance for a variable</p></a></li>
<li><a href='#wtd.moments'>
<p>Compute moments of a variable from either a population or sample</p></a></li>
<li><a href='#wtdvar'>
<p>Compute weighted variance</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Designing and Weighting Survey Samples</td>
</tr>
<tr>
<td>Version:</td>
<td>1.5</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-06-24</td>
</tr>
<tr>
<td>Author:</td>
<td>Richard Valliant [aut, cre],
  Jill A. Dever [ctb],
  Frauke Kreuter [ctb],
  George Zipf [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Richard Valliant &lt;valliant@umich.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Functions and datasets to support Valliant, Dever, and Kreuter (2018), &lt;<a href="https://doi.org/10.1007%2F978-3-319-93632-1">doi:10.1007/978-3-319-93632-1</a>&gt;, "Practical Tools for Designing and Weighting Survey Samples". Contains functions for sample size calculation for survey samples using stratified or clustered one-, two-, and three-stage sample designs, and single-stage audit sample designs. Functions are included that will group geographic units accounting for distances apart and measures of size. Other functions compute variance components for multistage designs and sample sizes in two-phase designs. A number of example data sets are included.</td>
</tr>
<tr>
<td>Suggests:</td>
<td>doBy, foreign, lpSolve, markdown, plyr, pps, Rcpp, reshape,
roxygen2, sampling, samplingbook, sp, survey, knitr, rmarkdown</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>dplyr, geosphere, ggplot2, graphics, usmap</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-06-24 19:56:26 UTC; rv</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-06-24 20:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='BW2stagePPS'>
Relvariance components for 2-stage sample
</h2><span id='topic+BW2stagePPS'></span>

<h3>Description</h3>

<p>Compute components of relvariance for a sample design where primary sampling units (PSUs) are selected with probability proportional to size (<em>pps</em>) and elements are selected via simple random sampling (<em>srs</em>). The input is an entire sampling frame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BW2stagePPS(X, pp, psuID, lonely.SSU = "mean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BW2stagePPS_+3A_x">X</code></td>
<td>

<p>data vector; length is the number of elements in the population.
</p>
</td></tr>
<tr><td><code id="BW2stagePPS_+3A_pp">pp</code></td>
<td>

<p>vector of one-draw probabilities for the PSUs; length is number of PSUs in population.
</p>
</td></tr>
<tr><td><code id="BW2stagePPS_+3A_psuid">psuID</code></td>
<td>

<p>vector of PSU identification numbers.  This vector must be as long as <code>X</code>. Each element in a given PSU should have the same value in <code>psuID</code>. PSUs must be in the same order as in <code>X</code>.
</p>
</td></tr>
<tr><td><code id="BW2stagePPS_+3A_lonely.ssu">lonely.SSU</code></td>
<td>

<p>indicator for how singleton SSUs should be handled when computing the within PSU unit relvariance. Allowable values are <code>"mean"</code> and <code>"zero"</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>BW2stagePPS</code> computes the between and within population relvariance components
appropriate for a two-stage sample in which PSUs are selected with varying probabilities
and with replacement. Elements within PSUs are selected by simple random sampling.
The components are appropriate for approximating the relvariance of the probability-with-replacement (<em>pwr</em>)-estimator of a total when the same number of elements are selected within each sample PSU.
The function requires that an entire frame of PSUs and elements be input.
</p>
<p>If a PSU contains multiple SSUs, some of which have missing data, or contains only one SSU, a value is imputed. If <code>lonely.SSU = "mean"</code>, the mean of the non-missing PSU contributions is imputed. If <code>lonely.SSU = "zero"</code>, a 0 is imputed. The former would be appropriate if a PSU contains multiple SSUs but one or more of them has missing data in which case R will normally calculate
an NA. The latter would be appropriate if the PSU contains only one SSU which would be selected with certainty in any sample.
</p>
<p>If any PSUs have one-draw probabilities of 1 (pp=1), they will be excluded from all computations.
</p>
<p>(Use <code><a href="#topic+BW2stagePPSe">BW2stagePPSe</a></code> if only a sample of PSUs and elements is available.)
</p>


<h3>Value</h3>

<p>List object with values:
</p>
<table>
<tr><td><code>B2</code></td>
<td>
<p>between PSU unit relvariance</p>
</td></tr>
<tr><td><code>W2</code></td>
<td>
<p>within PSU unit relvariance</p>
</td></tr>
<tr><td><code>unit relvar</code></td>
<td>
<p>unit relvariance for population</p>
</td></tr>
<tr><td><code>B2+W2</code></td>
<td>
<p>sum of between and within relvariance estimates</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>ratio of <code class="reqn">B^2 + W^2</code> to unit relvariance</p>
</td></tr>
<tr><td><code>delta</code></td>
<td>
<p>measure of homogeneity with PSUs estimated as <code class="reqn">B^2/(B^2 + W^2)</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Cochran, W.G. (1977, pp.308-310). <em>Sampling Techniques</em>. New York: John Wiley &amp; Sons.
</p>
<p>Saerndal, C.E., Swensson, B., and Wretman, J. (1992). <em>Model Assisted Survey Sampling</em>. New York: Springer.
</p>
<p>Valliant, R., Dever, J., Kreuter, F. (2018, sect. 9.2.3). <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BW2stagePPSe">BW2stagePPSe</a></code>, <code><a href="#topic+BW2stageSRS">BW2stageSRS</a></code>, <code><a href="#topic+BW3stagePPS">BW3stagePPS</a></code>, <code><a href="#topic+BW3stagePPSe">BW3stagePPSe</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(MDarea.popA)
MDsub &lt;- MDarea.popA[1:100000,]
    # Use PSU and SSU variables to define psu's
pp.PSU &lt;- table(MDsub$PSU) / nrow(MDsub)
pp.SSU &lt;- table(MDsub$SSU) / nrow(MDsub)
    # components with psu's defined by the PSU variable
BW2stagePPS(MDsub$y1, pp=pp.PSU, psuID=MDsub$PSU, lonely.SSU="mean")
    # components with psu's defined by the SSU variable
BW2stagePPS(MDsub$y1, pp=pp.SSU, psuID=MDsub$SSU, lonely.SSU="mean")

    # Use census tracts and block groups to define psu's
trtBG &lt;- 10*MDsub$TRACT + MDsub$BLKGROUP
pp.trt &lt;- table(MDsub$TRACT) / nrow(MDsub)
pp.BG &lt;- table(trtBG) / nrow(MDsub)
    # components with psu's defined by tracts
BW2stagePPS(MDsub$ins.cov, pp=pp.trt, psuID=MDsub$TRACT, lonely.SSU="mean")
    # components with psu's defined by block groups
BW2stagePPS(MDsub$ins.cov, pp=pp.BG, psuID=trtBG, lonely.SSU="mean")
</code></pre>

<hr>
<h2 id='BW2stagePPSe'>
Estimated relvariance components for 2-stage sample
</h2><span id='topic+BW2stagePPSe'></span>

<h3>Description</h3>

<p>Estimate components of relvariance for a sample design where primary sampling units (PSUs) are selected with <em>pps</em> and elements are selected via <em>srs</em>. The input is a sample selected in this way.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BW2stagePPSe(Ni, ni, X, psuID, w, m, pp, lonely.SSU = "mean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BW2stagePPSe_+3A_ni">Ni</code></td>
<td>

<p>vector of number of elements in the population of each sample PSU; length is the number of PSUs in the sample.
</p>
</td></tr>
<tr><td><code id="BW2stagePPSe_+3A_ni">ni</code></td>
<td>

<p>vector of number of sample elements in each sample PSU; length is the number of PSUs in the sample. PSUs must be in the same order in <code>ni</code> and in <code>X</code>.
</p>
</td></tr>
<tr><td><code id="BW2stagePPSe_+3A_x">X</code></td>
<td>

<p>data vector for sample elements; length is the number of elements in the sample. These must be in PSU order. PSUs must be in the same order in <code>ni</code> and in <code>X</code>.
</p>
</td></tr>
<tr><td><code id="BW2stagePPSe_+3A_psuid">psuID</code></td>
<td>

<p>vector of PSU identification numbers.  This vector must be as long as <code>X</code>. Each element in a given PSU should have the same value in <code>psuID</code>.
</p>
</td></tr>
<tr><td><code id="BW2stagePPSe_+3A_w">w</code></td>
<td>

<p>vector of full sample weights. This vector must be as long as <code>X</code>. Vector must be in the same order as <code>X</code>.
</p>
</td></tr>
<tr><td><code id="BW2stagePPSe_+3A_m">m</code></td>
<td>
<p>number of sample PSUs</p>
</td></tr>
<tr><td><code id="BW2stagePPSe_+3A_pp">pp</code></td>
<td>

<p>vector of 1-draw probabilities for the PSUs. The length of this vector is the number of PSUs in the sample. Vector must be in the same order as <code>Ni</code> and <code>ni</code>.
</p>
</td></tr>
<tr><td><code id="BW2stagePPSe_+3A_lonely.ssu">lonely.SSU</code></td>
<td>

<p>indicator for how singleton SSUs should be handled when computing the within PSU unit relvariance. Allowable values are <code>"mean"</code> and <code>"zero"</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>BW2stagePPSe</code> computes the between and within population variance and relvariance
components appropriate for a two-stage sample in which PSUs are selected with varying
probabilities and with replacement. Elements within PSUs are selected by simple random sampling.
The number of elements selected within each sample PSU can vary but must be at least two.
The estimated components are appropriate for approximating the relvariance of the <em>pwr</em>-estimator
of a total when the same number of elements are selected within each sample PSU.
This function can also be used if PSUs are selected by <em>srswr</em> by appropriate definition of <code>pp</code>.
</p>
<p>If a PSU contains multiple SSUs, some of which have missing data, or contains only one SSU, a value is imputed. If <code>lonely.SSU = "mean"</code>, the mean of the non-missing PSU contributions is imputed. If <code>lonely.SSU = "zero"</code>, a 0 is imputed. The former would be appropriate if a PSU contains multiple SSUs but one or more of them has missing data in which case R will normally calculate an NA. The latter would be appropriate if the PSU contains only one SSU which would be selected with certainty in any sample.
</p>
<p>If any PSUs have one-draw probabilities of 1 (pp=1), the function will be halted. Any such PSUs should be removed before calling the function.
</p>


<h3>Value</h3>

<p>List with values:
</p>
<table>
<tr><td><code>Vpsu</code></td>
<td>
<p>estimated between PSU unit variance</p>
</td></tr>
<tr><td><code>Vssu</code></td>
<td>
<p>estimated within PSU unit variance</p>
</td></tr>
<tr><td><code>B</code></td>
<td>
<p>estimated between PSU unit relvariance</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>estimated within PSU unit relvariance</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>estimated ratio of <code>B+W</code> to estimated unit relvariance of the analysis variable</p>
</td></tr>
<tr><td><code>delta</code></td>
<td>
<p>intraclass correlation estimated as <code>B/(B+W)</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Cochran, W.G. (1977, pp.308-310). <em>Sampling Techniques</em>. New York: John Wiley &amp; Sons.
</p>
<p>Valliant, R., Dever, J., Kreuter, F. (2018, sect. 9.4.1). <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BW2stagePPS">BW2stagePPS</a></code>, <code><a href="#topic+BW2stageSRS">BW2stageSRS</a></code>, <code><a href="#topic+BW3stagePPS">BW3stagePPS</a></code>, <code><a href="#topic+BW3stagePPSe">BW3stagePPSe</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
require(sampling)
require(plyr)      # has function that allows renaming variables
data(MDarea.popA)
Ni &lt;- table(MDarea.popA$TRACT)
m &lt;- 20
probi &lt;- m*Ni / sum(Ni)
    # select sample of clusters
sam &lt;- cluster(data=MDarea.popA, clustername="TRACT", size=m, method="systematic",
                pik=probi, description=TRUE)
    # extract data for the sample clusters
samclus &lt;- getdata(MDarea.popA, sam)
samclus &lt;- rename(samclus, c("Prob" = "pi1"))


    # treat sample clusters as strata and select srswor from each
s &lt;- strata(data = as.data.frame(samclus), stratanames = "TRACT",
            size = rep(50,m), method="srswor")
# extracts the observed data
samdat &lt;- getdata(samclus,s)
samdat &lt;- rename(samdat, c("Prob" = "pi2"))

    # extract pop counts for PSUs in sample
pick &lt;- names(Ni) %in% sort(unique(samdat$TRACT))
Ni.sam &lt;- Ni[pick]
pp &lt;- Ni.sam / sum(Ni)
wt &lt;- 1/samdat$pi1/samdat$pi2

BW2stagePPSe(Ni = Ni.sam, ni = rep(50,20), X = samdat$y1,
            psuID = samdat$TRACT, w = wt,
            m = 20, pp = pp, lonely.SSU="mean")

</code></pre>

<hr>
<h2 id='BW2stageSRS'>
Relvariance components for 2-stage sample
</h2><span id='topic+BW2stageSRS'></span>

<h3>Description</h3>

<p>Compute components of relvariance for a sample design where primary sampling units (PSUs) and elements are selected via <em>srs</em>. The input is an entire sampling frame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BW2stageSRS(X, psuID, lonely.SSU)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BW2stageSRS_+3A_x">X</code></td>
<td>

<p>data vector; length is the number of elements in the population.
</p>
</td></tr>
<tr><td><code id="BW2stageSRS_+3A_psuid">psuID</code></td>
<td>

<p>vector of PSU identification numbers.  This vector must be as long as <code>X</code>. Each element in a given PSU should have the same value in <code>psuID</code>. PSUs must be in the same order as in <code>X</code>.
</p>
</td></tr>
<tr><td><code id="BW2stageSRS_+3A_lonely.ssu">lonely.SSU</code></td>
<td>

<p>indicator for how singleton SSUs should be handled when computing the within PSU unit relvariance. Allowable values are <code>"mean"</code> and <code>"zero"</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>BW2stageSRS</code> computes the between and within population relvariance components
appropriate for a two-stage sample in which PSUs are selected via <em>srs</em> (either with or without replacement). Elements within PSUs are assumed to be selected by <em>srswor</em>. The same number of elements is assumed to be selected within each sample PSU. The function requires that an entire frame of PSUs and elements be input.
</p>
<p>If a PSU contains multiple SSUs, some of which have missing data, or contains only one SSU, a value is imputed. If <code>lonely.SSU = "mean"</code>, the mean of the non-missing PSU contributions is imputed. If <code>lonely.SSU = "zero"</code>, a 0 is imputed. The former would be appropriate if a PSU contains multiple SSUs but one or more of them has missing data in which case R will normally calculate
an NA. The latter would be appropriate if the PSU contains only one SSU which would be selected with certainty in any sample.
</p>
<p>(Use <code><a href="#topic+BW2stagePPSe">BW2stagePPSe</a></code> if only a sample of PSUs and elements are available.)
</p>


<h3>Value</h3>

<p>List with values:
</p>
<table>
<tr><td><code>B2</code></td>
<td>
<p>between PSU unit relvariance</p>
</td></tr>
<tr><td><code>W2</code></td>
<td>
<p>within PSU unit relvariance</p>
</td></tr>
<tr><td><code>unit relvar</code></td>
<td>
<p>unit relvariance for population</p>
</td></tr>
<tr><td><code>B2+W2</code></td>
<td>
<p><code class="reqn">B^2 + W^2</code></p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>ratio of <code class="reqn">B^2 + W^2</code> to unit relvariance</p>
</td></tr>
<tr><td><code>delta full</code></td>
<td>
<p>intraclass correlation estimated as <code class="reqn">B^2/(B^2 + W^2)</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Cochran, W.G. (1977, chap. 11). <em>Sampling Techniques</em>. New York: John Wiley &amp; Sons.
</p>
<p>Valliant, R., Dever, J., Kreuter, F. (2018, sect. 9.2.1). <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BW2stagePPS">BW2stagePPS</a></code>, <code><a href="#topic+BW2stagePPSe">BW2stagePPSe</a></code>, <code><a href="#topic+BW3stagePPS">BW3stagePPS</a></code>, <code><a href="#topic+BW3stagePPSe">BW3stagePPSe</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(MDarea.popA)
MDsub &lt;- MDarea.popA[1:100000,]
    # psu's are defined by PSU variable
BW2stageSRS(abs(MDsub$Hispanic-2), psuID=MDsub$PSU, lonely.SSU="mean")
    # psu's are defined by SSU variable
BW2stageSRS(abs(MDsub$Hispanic-2), psuID=MDsub$SSU, lonely.SSU="mean")
</code></pre>

<hr>
<h2 id='BW3stagePPS'>
Relvariance components for 3-stage sample
</h2><span id='topic+BW3stagePPS'></span>

<h3>Description</h3>

<p>Compute components of relvariance for a sample design where primary sampling units (PSUs) are selected with <em>ppswr</em> and secondary sampling units (SSUs) and elements within SSUs are selected via <em>srs</em>. The input is an entire sampling frame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BW3stagePPS(X, pp, psuID, ssuID, lonely.SSU = "mean", lonely.TSU = "mean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BW3stagePPS_+3A_x">X</code></td>
<td>

<p>data vector; length is the number of elements in the population.
</p>
</td></tr>
<tr><td><code id="BW3stagePPS_+3A_pp">pp</code></td>
<td>

<p>vector of one-draw probabilities for the PSUs; length is number of PSUs in population.
</p>
</td></tr>
<tr><td><code id="BW3stagePPS_+3A_psuid">psuID</code></td>
<td>

<p>vector of PSU identification numbers.  This vector must be as long as <code>X</code>. Each element in a given PSU should have the same value in <code>psuID</code>. PSUs must be in the same order as in <code>X</code>.
</p>
</td></tr>
<tr><td><code id="BW3stagePPS_+3A_ssuid">ssuID</code></td>
<td>

<p>vector of SSU identification numbers.  This vector must be as long as <code>X</code>. Each element in a given SSU should have the same value in <code>ssuID</code>. PSUs and SSUs must be in the same order as in <code>X</code>. <code>ssuID</code> should have the form <code>psuID</code>||(<code>ssuID</code> within PSU).
</p>
</td></tr>
<tr><td><code id="BW3stagePPS_+3A_lonely.ssu">lonely.SSU</code></td>
<td>

<p>indicator for how singleton SSUs should be handled when computing the within PSU unit relvariance. Allowable values are <code>"mean"</code> and <code>"zero"</code>.
</p>
</td></tr>
<tr><td><code id="BW3stagePPS_+3A_lonely.tsu">lonely.TSU</code></td>
<td>

<p>indicator for how singleton third-stage units (TSUs) should be handled when computing the within SSU unit relvariance. Allowable values are <code>"mean"</code> and <code>"zero"</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>BW3stagePPS</code> computes the between and within population relvariance components
appropriate for a three-stage sample in which PSUs are selected with varying probabilities
and with replacement. SSUs and elements within SSUs are selected by simple random sampling.
The components are appropriate for approximating the relvariance of the <em>pwr</em>-estimator of a total
when the same number of SSUs are selected within each PSU, and the same number of elements are
selected within each sample SSU. The function requires that an entire sampling frame of PSUs
and elements be input.
</p>
<p>If a PSU contains multiple SSUs, some of which have missing data, or contains only one SSU, a value is imputed. If <code>lonely.SSU = "mean"</code>, the mean of the non-missing PSU contributions is imputed. If <code>lonely.SSU = "zero"</code>, a 0 is imputed. The former would be appropriate if a PSU contains multiple SSUs but one or more of them has missing data in which case R will normally calculate an NA. The latter would be appropriate if the PSU contains only one SSU which would be selected with certainty in any sample. <code>lonely.TSU</code> has a similar purpose for third-stage units.
</p>
<p>(Use <code><a href="#topic+BW2stagePPSe">BW2stagePPSe</a></code> if only a sample of PSUs, SSUs, and elements is available.)
</p>


<h3>Value</h3>

<p>List with values:
</p>
<table>
<tr><td><code>B</code></td>
<td>
<p>between PSU unit relvariance</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>within PSU unit relvariance computed as if the sample were two-stage</p>
</td></tr>
<tr><td><code>W2</code></td>
<td>
<p>unit relvariance among SSU totals</p>
</td></tr>
<tr><td><code>W3</code></td>
<td>
<p>unit relvariance among elements within PSU/SSUs</p>
</td></tr>
<tr><td><code>unit relvar</code></td>
<td>
<p>unit relvariance for population</p>
</td></tr>
<tr><td><code>k1</code></td>
<td>
<p>ratio of <code class="reqn">B^2 + W^2</code> to unit relvariance</p>
</td></tr>
<tr><td><code>k2</code></td>
<td>
<p>ratio of <code class="reqn">W_{2}^2 + W_{3}^2</code> to unit relvariance</p>
</td></tr>
<tr><td><code>delta1</code></td>
<td>
<p>homogeneity measure among elements within PSUs estimated as <code class="reqn">B^2/(B^2+W^2)</code></p>
</td></tr>
<tr><td><code>delta2</code></td>
<td>
<p>homogeneity measure among elements within SSUs estimated as <code class="reqn">W_{2}^2/(W_{2}^2 + W_{3}^2)</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Hansen,M.H., Hurwitz,W.N., and Madow,W.G. (1953, chap. 9, p.211). <em>Sample Survey Methods and Theory</em>, Vol.I. John Wiley &amp; Sons.
</p>
<p>Saerndal, C.E., Swensson, B., and Wretman, J. (1992, p.149). <em>Model Assisted Survey Sampling</em>. Springer.
</p>
<p>Valliant, R., Dever, J., Kreuter, F. (2018, sect. 9.2.4). <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BW2stagePPS">BW2stagePPS</a></code>, <code><a href="#topic+BW2stagePPSe">BW2stagePPSe</a></code>, <code><a href="#topic+BW2stageSRS">BW2stageSRS</a></code>, <code><a href="#topic+BW3stagePPSe">BW3stagePPSe</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(MDarea.popA)
MDsub &lt;- MDarea.popA[1:100000,]
M &lt;- length(unique(MDsub$PSU))
    # srs/srs/srs design
pp.PSU &lt;- rep(1/M,M)
BW3stagePPS(X=MDsub$y1, pp=pp.PSU, psuID=MDsub$PSU, ssuID=MDsub$SSU,
    lonely.SSU = "mean", lonely.TSU = "mean")
    # ppswr/srs/srs design
pp.PSU &lt;- table(MDsub$PSU) / nrow(MDsub)
BW3stagePPS(X=MDsub$y1, pp=pp.PSU, psuID=MDsub$PSU, ssuID=MDsub$SSU,
    lonely.SSU = "mean", lonely.TSU = "mean")
</code></pre>

<hr>
<h2 id='BW3stagePPSe'>
Estimated relvariance components for 3-stage sample
</h2><span id='topic+BW3stagePPSe'></span>

<h3>Description</h3>

<p>Estimate components of relvariance for a sample design where primary sampling units (PSUs) are selected with probability proportional to size with replacement (<em>ppswr</em>) and secondary sampling units (SSUs) and elements within SSUs are selected via simple random sampling (<em>srs</em>). The input is a sample selected in this way.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BW3stagePPSe(dat, v, Ni, Qi, Qij, m, lonely.SSU = "mean", lonely.TSU = "mean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BW3stagePPSe_+3A_dat">dat</code></td>
<td>

<p>data frame for sample elements with PSU and SSU identifiers, weights, and analysis variable(s).  The data frame should be sorted in hierarchical order: by PSU and SSU within PSU.
Required names for columns:
<code>psuID</code> = PSU identifier;
<code>ssuID</code> = SSU identifier. These must be unique, i.e., numbering should not restart within each PSU. Setting <code>ssuID</code> = <code>psuID</code>||(<code>ssuID</code> within PSU) is a method of doing this.
<code>w1i</code>  = vector of weights for PSUs;
<code>w2ij</code> = vector of weights for SSUs (PSU weight*SSU weight within PSU);
<code>w</code> = full sample weight
</p>
</td></tr>
<tr><td><code id="BW3stagePPSe_+3A_v">v</code></td>
<td>

<p>Name or number of column in data frame <code>dat</code> with variable to be analyzed.
</p>
</td></tr>
<tr><td><code id="BW3stagePPSe_+3A_ni">Ni</code></td>
<td>

<p><code>m</code>-vector of number of SSUs in the population in the sample PSUs; <code>m</code> is number of sample PSUs.
</p>
</td></tr>
<tr><td><code id="BW3stagePPSe_+3A_qi">Qi</code></td>
<td>

<p><code>m</code>-vector of number of elements in the population in the sample PSUs
</p>
</td></tr>
<tr><td><code id="BW3stagePPSe_+3A_qij">Qij</code></td>
<td>

<p>vector of numbers of elements in the population in the sample SSUs
</p>
</td></tr>
<tr><td><code id="BW3stagePPSe_+3A_m">m</code></td>
<td>
<p>number of sample PSUs</p>
</td></tr>
<tr><td><code id="BW3stagePPSe_+3A_lonely.ssu">lonely.SSU</code></td>
<td>

<p>indicator for how singleton SSUs should be handled when computing the within PSU unit relvariance. Allowable values are <code>"mean"</code> and <code>"zero"</code>.
</p>
</td></tr>
<tr><td><code id="BW3stagePPSe_+3A_lonely.tsu">lonely.TSU</code></td>
<td>

<p>indicator for how singleton third-stage units (TSUs) should be handled when computing the within SSU unit relvariance. Allowable values are <code>"mean"</code> and <code>"zero"</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>BW3stagePPSe</code> computes the between and within population relvariance components appropriate
for a three-stage sample in which PSUs are selected with varying probabilities and with replacement.
SSUs and elements within SSUs are selected by simple random sampling.
The estimated components are appropriate for approximating the relvariance of the
<em>pwr</em>-estimator of a total when the same number of SSUs are selected within each PSU,
and the same number of elements are selected within each sample SSU.
</p>
<p>If a PSU contains multiple SSUs, some of which have missing data, or contains only one SSU, a value is imputed. If <code>lonely.SSU = "mean"</code>, the mean of the non-missing PSU contributions is imputed. If <code>lonely.SSU = "zero"</code>, a 0 is imputed. The former would be appropriate if a PSU contains multiple SSUs but one or more of them has missing data in which case R will normally calculate an NA. The latter would be appropriate if the PSU contains only one SSU which would be selected with certainty in any sample. <code>lonely.TSU</code> has a similar purpose for third-stage units.
</p>


<h3>Value</h3>

<p>List with values:
</p>
<table>
<tr><td><code>Vpsu</code></td>
<td>
<p>estimated between PSU unit variance</p>
</td></tr>
<tr><td><code>Vssu</code></td>
<td>
<p>estimated second-stage unit variance among SSU totals</p>
</td></tr>
<tr><td><code>Vtsu</code></td>
<td>
<p>estimated third-stage unit variance</p>
</td></tr>
<tr><td><code>B</code></td>
<td>
<p>estimated between PSU unit relvariance</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>estimated within PSU unit relvariance computed as if the sample were two-stage</p>
</td></tr>
<tr><td><code>k1</code></td>
<td>
<p>estimated ratio of <code>B+W</code> to estimated unit relvariance of the analysis variable</p>
</td></tr>
<tr><td><code>W2</code></td>
<td>
<p>estimated unit relvariance among SSU totals</p>
</td></tr>
<tr><td><code>W3</code></td>
<td>
<p>estimated third-stage unit relvariance among elements within PSU/SSUs</p>
</td></tr>
<tr><td><code>k2</code></td>
<td>
<p>estimated ratio of <code>W2+W3</code> to estimated unit relvariance of the analysis variable</p>
</td></tr>
<tr><td><code>delta1</code></td>
<td>
<p>homogeneity measure among elements within PSUs estimated as <code class="reqn">B^2/(B^2+W^2)</code></p>
</td></tr>
<tr><td><code>delta2</code></td>
<td>
<p>homogeneity measure among elements within SSUs estimated as <code class="reqn">W_{2}^2/(W_{2}^2 + W_{3}^2)</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Hansen, M.H., Hurwitz, W.N., and Madow, W.G. (1953, chap. 9, sect. 10). <em>Sample Survey Methods and Theory</em>, Vol.II. New York: John Wiley &amp; Sons.
</p>
<p>Valliant, R., Dever, J., Kreuter, F. (2018, sect. 9.4.2). <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BW2stagePPS">BW2stagePPS</a></code>, <code><a href="#topic+BW2stagePPSe">BW2stagePPSe</a></code>, <code><a href="#topic+BW2stageSRS">BW2stageSRS</a></code>, <code><a href="#topic+BW3stagePPS">BW3stagePPS</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
    # select 3-stage sample from Maryland population
set.seed(-780087528)
data(MDarea.popA)
MDpop &lt;- MDarea.popA
require(sampling)
require(plyr)      # has function that allows renaming variables
    # make counts of SSUs and elements per PSU
xx &lt;- do.call("rbind",list(by(1:nrow(MDpop),MDpop$SSU,head,1)))
pop.tmp &lt;- MDpop[xx,]
Ni &lt;- table(pop.tmp$PSU)
Qi &lt;- table(MDarea.popA$PSU)
Qij &lt;- table(MDpop$SSU)
m &lt;- 30         # no. of PSUs to select
probi &lt;- m*Qi / sum(Qi)
    # select sample of clusters
sam &lt;- cluster(data=MDpop, clustername="PSU", size=m, method="systematic",
               pik=probi, description=TRUE)
    # extract data for the sample clusters
samclus &lt;- getdata(MDpop, sam)
samclus &lt;- rename(samclus, c("Prob" = "p1i"))
samclus &lt;- samclus[order(samclus$PSU),]
    # treat sample clusters as strata and select srswor of block groups from each
    # identify psu IDs for 1st instance of each ssuID
xx &lt;- do.call("rbind",list(by(1:nrow(samclus),samclus$SSU,head,1)))
SSUs &lt;- cbind(PSU=samclus$PSU[xx], SSU=samclus$SSU[xx])
    # select 2 SSUs per tract
n &lt;- 2
s &lt;- strata(data = as.data.frame(SSUs), stratanames = "PSU",
            size = rep(n,m), method="srswor")
s &lt;- rename(s, c("Prob" = "p2i"))
    # extract the SSU data
    # s contains selection probs of SSUs, need to get those onto data file
SSUsam &lt;- SSUs[s$ID_unit, ]
SSUsam &lt;- cbind(SSUsam, s[, 2:3])
    # identify rows in PSU sample that correspond to sample SSUs
tmp &lt;- samclus$SSU %in% SSUsam$SSU
SSUdat &lt;- samclus[tmp,]
SSUdat &lt;- merge(SSUdat, SSUsam[, c("p2i","SSU")], by="SSU")
    # select srswor from each sample SSU
n.SSU &lt;- m*n
s &lt;- strata(data = as.data.frame(SSUdat), stratanames = "SSU",
            size = rep(50,n.SSU), method="srswor")
s &lt;- rename(s, c("Prob" = "p3i"))
samclus &lt;- getdata(SSUdat, s)
del &lt;- (1:ncol(samclus))[dimnames(samclus)[[2]] %in% c("ID_unit","Stratum")]
samclus &lt;- samclus[, -del]
    # extract pop counts for PSUs in sample
pick &lt;- names(Qi) %in% sort(unique(samclus$PSU))
Qi.sam &lt;- Qi[pick]
    # extract pop counts of SSUs for PSUs in sample
pick &lt;- names(Ni) %in% sort(unique(samclus$PSU))
Ni.sam &lt;- Ni[pick]
    # extract pop counts for SSUs in sample
pick &lt;- names(Qij) %in% sort(unique(samclus$SSU))
Qij.sam &lt;- Qij[pick]
    # compute full sample weight and wts for PSUs and SSUs
wt &lt;- 1 / samclus$p1i / samclus$p2i / samclus$p3i
w1i &lt;- 1 / samclus$p1i
w2ij &lt;- 1 / samclus$p1i / samclus$p2i
samdat &lt;- data.frame(psuID = samclus$PSU, ssuID = samclus$SSU,
                     w1i = w1i, w2ij = w2ij, w = wt,
                     samclus[, c("y1","y2","y3","ins.cov", "hosp.stay")])
BW3stagePPSe(dat=samdat, v="y1", Ni=Ni.sam, Qi=Qi.sam, Qij=Qij.sam, m,
    lonely.SSU = "mean", lonely.TSU = "mean")

</code></pre>

<hr>
<h2 id='clusOpt2'>
Compute optimal sample sizes for a two-stage sample
</h2><span id='topic+clusOpt2'></span>

<h3>Description</h3>

<p>Compute the sample sizes that minimize the variance of the <em>pwr</em>-estimator of a total in a two-stage sample.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clusOpt2(C1, C2, delta, unit.rv, k=1, CV0=NULL, tot.cost=NULL, cal.sw)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clusOpt2_+3A_c1">C1</code></td>
<td>
<p>unit cost per primary sampling unit (PSU)</p>
</td></tr>
<tr><td><code id="clusOpt2_+3A_c2">C2</code></td>
<td>
<p>unit cost per element</p>
</td></tr>
<tr><td><code id="clusOpt2_+3A_delta">delta</code></td>
<td>
<p>homogeneity measure <code class="reqn">\delta</code></p>
</td></tr>
<tr><td><code id="clusOpt2_+3A_unit.rv">unit.rv</code></td>
<td>
<p>unit relvariance</p>
</td></tr>
<tr><td><code id="clusOpt2_+3A_k">k</code></td>
<td>
<p>ratio of <code class="reqn">B^2 + W^2</code> to unit relvariance</p>
</td></tr>
<tr><td><code id="clusOpt2_+3A_cv0">CV0</code></td>
<td>
<p>target CV</p>
</td></tr>
<tr><td><code id="clusOpt2_+3A_tot.cost">tot.cost</code></td>
<td>
<p>total budget for variable costs</p>
</td></tr>
<tr><td><code id="clusOpt2_+3A_cal.sw">cal.sw</code></td>
<td>
<p>specify type of optimum:
1 = find optimal <code>m.opt</code> for fixed total budget;
2 = find optimal <code>m.opt</code> for target <code>CV0</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>clusOpt2</code> will compute <code class="reqn">m_{opt}</code> and  <code class="reqn">\bar{n}_{opt}</code> for a two-stage sample
which uses simple random sampling at each stage or <em>ppswr</em> at the first stage and <em>srs</em> at the second.
</p>


<h3>Value</h3>

<p>List with values:
</p>
<table>
<tr><td><code>C1</code></td>
<td>
<p>unit cost per PSU</p>
</td></tr>
<tr><td><code>C2</code></td>
<td>
<p>unit cost per element</p>
</td></tr>
<tr><td><code>delta</code></td>
<td>
<p>homogeneity measure</p>
</td></tr>
<tr><td><code>unit relvar</code></td>
<td>
<p>unit relvariance</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>ratio of <code class="reqn">B^2 + W^2</code> to unit relvariance</p>
</td></tr>
<tr><td><code>cost</code></td>
<td>
<p>total budget for variable costs, <code class="reqn">C-C_{0}</code> if <code>cal.sw</code>=1; or computed cost if <code>cal.sw</code>=2</p>
</td></tr>
<tr><td><code>m.opt</code></td>
<td>
<p>optimum number of sample PSUs</p>
</td></tr>
<tr><td><code>n.opt</code></td>
<td>
<p>optimum number of sample elements per PSU</p>
</td></tr>
<tr><td><code>CV</code></td>
<td>
<p>computed CV if <code>cal.sw</code>=1; or target CV if <code>cal.sw</code>=2</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Hansen,M.H., Hurwitz,W.N., and Madow,W.G. (1953, chap. 6, sect. 16).  <em>Sample Survey Methods and Theory</em>, Vol.I. John Wiley &amp; Sons.
</p>
<p>Valliant, R., Dever, J., Kreuter, F. (2018, sect. 9.3.1).  <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+clusOpt2fixedPSU">clusOpt2fixedPSU</a></code>, <code><a href="#topic+clusOpt3">clusOpt3</a></code>, <code><a href="#topic+clusOpt3fixedPSU">clusOpt3fixedPSU</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>    # optimum for a fixed total budget
clusOpt2(C1=750, C2=100, delta=0.05, unit.rv=1, k=1, tot.cost=100000, cal.sw=1)
clusOpt2(C1=750, C2=100, delta=seq(0.05,0.25,0.05), unit.rv=1, k=1, tot.cost=100000, cal.sw=1)
    # optimum for a target CV
clusOpt2(C1=750, C2=100, delta=0.01, unit.rv=1, k=1, CV0=0.05, cal.sw=2)

</code></pre>

<hr>
<h2 id='clusOpt2fixedPSU'>
Optimal number of sample elements per PSU in a two-stage sample when the sample of PSUs is fixed
</h2><span id='topic+clusOpt2fixedPSU'></span>

<h3>Description</h3>

<p>Compute the optimum number of sample elements per primary sampling unit (PSU) for a fixed set of PSUs
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clusOpt2fixedPSU(C1, C2, m, delta, unit.rv, k=1, CV0=NULL, tot.cost, cal.sw)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clusOpt2fixedPSU_+3A_c1">C1</code></td>
<td>
<p>unit cost per PSU</p>
</td></tr>
<tr><td><code id="clusOpt2fixedPSU_+3A_c2">C2</code></td>
<td>
<p>unit cost per element</p>
</td></tr>
<tr><td><code id="clusOpt2fixedPSU_+3A_m">m</code></td>
<td>
<p>number of sample PSU's (fixed)</p>
</td></tr>
<tr><td><code id="clusOpt2fixedPSU_+3A_delta">delta</code></td>
<td>
<p>homogeneity measure</p>
</td></tr>
<tr><td><code id="clusOpt2fixedPSU_+3A_unit.rv">unit.rv</code></td>
<td>
<p>unit relvariance</p>
</td></tr>
<tr><td><code id="clusOpt2fixedPSU_+3A_k">k</code></td>
<td>
<p>ratio of <code class="reqn">B^2 + W^2</code> to unit relvariance</p>
</td></tr>
<tr><td><code id="clusOpt2fixedPSU_+3A_cv0">CV0</code></td>
<td>
<p>target CV</p>
</td></tr>
<tr><td><code id="clusOpt2fixedPSU_+3A_tot.cost">tot.cost</code></td>
<td>
<p>total budget for variable costs</p>
</td></tr>
<tr><td><code id="clusOpt2fixedPSU_+3A_cal.sw">cal.sw</code></td>
<td>
<p>specify type of optimum:
1 = find optimal <code class="reqn">\bar{n}</code> for fixed total budget;
2 = find optimal <code class="reqn">\bar{n}</code> for target CV0</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>clusOpt2fixedPSU</code> will compute <code class="reqn">\bar{n}_{opt}</code> for a two-stage sample which uses simple
random sampling at each stage or <em>ppswr</em> at the first stage and <em>srs</em> at the second. The PSU sample is fixed.
</p>


<h3>Value</h3>

<p>List with values:
</p>
<table>
<tr><td><code>C1</code></td>
<td>
<p>unit cost per PSU</p>
</td></tr>
<tr><td><code>C2</code></td>
<td>
<p>unit cost per element</p>
</td></tr>
<tr><td><code>m</code></td>
<td>
<p>number of (fixed) sample PSUs</p>
</td></tr>
<tr><td><code>delta</code></td>
<td>
<p>homogeneity measure</p>
</td></tr>
<tr><td><code>unit relvar</code></td>
<td>
<p>unit relvariance</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>ratio of <code class="reqn">B^2 + W^2</code> to unit relvariance</p>
</td></tr>
<tr><td><code>cost</code></td>
<td>
<p>total budget for variable costs, <code class="reqn">C-C_{0}</code> if <code>cal.sw</code>=1; or computed cost if <code>cal.sw</code>=2</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>optimum number of sample elements per PSU</p>
</td></tr>
<tr><td><code>CV</code></td>
<td>
<p>computed CV if <code>cal.sw</code>=1; or target CV if <code>cal.sw</code>=2</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Valliant, R., Dever, J., Kreuter, F. (2018, sect. 9.3.3).  <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+clusOpt2">clusOpt2</a></code>, <code><a href="#topic+clusOpt3">clusOpt3</a></code>, <code><a href="#topic+clusOpt3fixedPSU">clusOpt3fixedPSU</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>    # optima for a vector of budgets
clusOpt2fixedPSU(C1=500, C2=100, m=100, delta=0.05, unit.rv=2, k=1, CV0=NULL,
       tot.cost=c(100000, 500000, 10^6), cal.sw=1)
    # optima for a target CV and vector of PSU costs
clusOpt2fixedPSU(C1=c(500,1000,5000), C2=100, m=100, delta=0.05, unit.rv=2, k=1,
       CV0=0.05, tot.cost=NULL, cal.sw=2)
</code></pre>

<hr>
<h2 id='clusOpt3'>
Compute optimal sample sizes for a three-stage sample
</h2><span id='topic+clusOpt3'></span>

<h3>Description</h3>

<p>Compute the sample sizes that minimize the variance of the <em>pwr</em>-estimator of a total in a three-stage sample.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clusOpt3(unit.cost, delta1, delta2, unit.rv, k1=1, k2=1, CV0=NULL, tot.cost=NULL, cal.sw)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clusOpt3_+3A_unit.cost">unit.cost</code></td>
<td>
<p>vector with three components for unit costs:
<code>C1</code> = unit cost per primary sampling unit (PSU);
<code>C2</code> = unit cost per  secondary sampling unit (SSU);
<code>C3</code> = unit cost per element
</p>
</td></tr>
<tr><td><code id="clusOpt3_+3A_delta1">delta1</code></td>
<td>
<p>homogeneity measure among elements within PSUs</p>
</td></tr>
<tr><td><code id="clusOpt3_+3A_delta2">delta2</code></td>
<td>
<p>homogeneity measure among elements within SSUs</p>
</td></tr>
<tr><td><code id="clusOpt3_+3A_unit.rv">unit.rv</code></td>
<td>
<p>population unit relvariance</p>
</td></tr>
<tr><td><code id="clusOpt3_+3A_k1">k1</code></td>
<td>
<p>ratio of <code class="reqn">B^2 + W^2</code> to the population unit relvariance</p>
</td></tr>
<tr><td><code id="clusOpt3_+3A_k2">k2</code></td>
<td>
<p>ratio of <code class="reqn">W_{2}^2 + W_{3}^2</code> to the population unit relvariance</p>
</td></tr>
<tr><td><code id="clusOpt3_+3A_cv0">CV0</code></td>
<td>
<p>target CV</p>
</td></tr>
<tr><td><code id="clusOpt3_+3A_tot.cost">tot.cost</code></td>
<td>
<p>total budget for variable costs</p>
</td></tr>
<tr><td><code id="clusOpt3_+3A_cal.sw">cal.sw</code></td>
<td>
<p>specify type of optimum:
1 = find optimal <code>m.opt</code> for fixed total budget;
2 = find optimal <code>m.opt</code> for target CV0</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>clusOpt3</code> will compute <code class="reqn">m_{opt}</code>, <code class="reqn">\bar{n}_{opt}</code>, and <code class="reqn">\bar{\bar{q}}_{opt}</code>
for a three-stage sample which uses simple random sampling at each stage or <em>ppswr</em>
at the first stage and <em>srs</em> at the second and third stages.
</p>


<h3>Value</h3>

<p>List with values:
</p>
<table>
<tr><td><code>C1</code></td>
<td>
<p>unit cost per PSU</p>
</td></tr>
<tr><td><code>C2</code></td>
<td>
<p>unit cost per SSU</p>
</td></tr>
<tr><td><code>C3</code></td>
<td>
<p>unit cost per element</p>
</td></tr>
<tr><td><code>delta1</code></td>
<td>
<p>homogeneity measure among elements within PSUs</p>
</td></tr>
<tr><td><code>delta2</code></td>
<td>
<p>homogeneity measure among elements within SSUs</p>
</td></tr>
<tr><td><code>unit relvar</code></td>
<td>
<p>unit relvariance</p>
</td></tr>
<tr><td><code>k1</code></td>
<td>
<p>ratio of <code class="reqn">B^2 + W^2</code> to the population unit relvariance</p>
</td></tr>
<tr><td><code>k2</code></td>
<td>
<p>ratio of <code class="reqn">W_{2}^2 + W_{3}^2</code> to the population unit relvariance</p>
</td></tr>
<tr><td><code>cost</code></td>
<td>
<p>total budget for variable costs if <code>cal.sw</code>=1; or computed cost if <code>cal.sw</code>=2</p>
</td></tr>
<tr><td><code>m.opt</code></td>
<td>
<p>optimum number of sample PSUs</p>
</td></tr>
<tr><td><code>n.opt</code></td>
<td>
<p>optimum number of sample SSUs per PSU</p>
</td></tr>
<tr><td><code>q.opt</code></td>
<td>
<p>optimum number of sample elements per SSU</p>
</td></tr>
<tr><td><code>CV</code></td>
<td>
<p>achieved CV if <code>cal.sw</code>=1 or target CV if <code>cal.sw</code>=2</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Hansen,M.H., Hurwitz,W.N., and Madow,W.G. (1953, p. 225).  <em>Sample Survey Methods and Theory</em>, Vol.II. John Wiley &amp; Sons.
</p>
<p>Valliant, R., Dever, J., Kreuter, F. (2018, sect. 9.3.2).  <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+clusOpt2">clusOpt2</a></code>, <code><a href="#topic+clusOpt2fixedPSU">clusOpt2fixedPSU</a></code>, <code><a href="#topic+clusOpt3fixedPSU">clusOpt3fixedPSU</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>    # optima for a fixed total budget
clusOpt3(unit.cost=c(500, 100, 120), delta1=0.01, delta2=0.10, unit.rv=1,
       k1=1, k2=1, tot.cost=100000,cal.sw=1)
    # optima for a target CV
clusOpt3(unit.cost=c(500, 100, 120), delta1=0.01, delta2=0.10, unit.rv=1,
       k1=1, k2=1, CV0=0.01,cal.sw=2)
</code></pre>

<hr>
<h2 id='clusOpt3fixedPSU'>
Compute optimal number of sample secondary sampling units (SSUs) and elements per SSU for a fixed set of primary sampling units (PSUs) in a three-stage sample
</h2><span id='topic+clusOpt3fixedPSU'></span>

<h3>Description</h3>

<p>Compute the sample sizes that minimize the variance of the <em>pwr</em>-estimator of a total in a three-stage sample when the PSU sample is fixed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clusOpt3fixedPSU(unit.cost, m, delta1, delta2, unit.rv, k1=1, k2=1, CV0=NULL,
         tot.cost=NULL, cal.sw)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clusOpt3fixedPSU_+3A_unit.cost">unit.cost</code></td>
<td>
<p>3-vector of unit costs:
<code>C1</code> = unit cost per PSU;
<code>C2</code> = unit cost per SSU;
<code>C3</code> = unit cost per element
</p>
</td></tr>
<tr><td><code id="clusOpt3fixedPSU_+3A_m">m</code></td>
<td>
<p>number of sample PSUs (fixed)</p>
</td></tr>
<tr><td><code id="clusOpt3fixedPSU_+3A_delta1">delta1</code></td>
<td>
<p>homogeneity measure among elements within PSUs</p>
</td></tr>
<tr><td><code id="clusOpt3fixedPSU_+3A_delta2">delta2</code></td>
<td>
<p>homogeneity measure among elements within SSUs</p>
</td></tr>
<tr><td><code id="clusOpt3fixedPSU_+3A_unit.rv">unit.rv</code></td>
<td>
<p>unit relvariance</p>
</td></tr>
<tr><td><code id="clusOpt3fixedPSU_+3A_k1">k1</code></td>
<td>
<p>ratio of <code class="reqn">B^2 + W^2</code> to unit relvariance</p>
</td></tr>
<tr><td><code id="clusOpt3fixedPSU_+3A_k2">k2</code></td>
<td>
<p>ratio of <code class="reqn">W_{2}^2 + W_{3}^2</code> to unit relvariance</p>
</td></tr>
<tr><td><code id="clusOpt3fixedPSU_+3A_cv0">CV0</code></td>
<td>
<p>target CV</p>
</td></tr>
<tr><td><code id="clusOpt3fixedPSU_+3A_tot.cost">tot.cost</code></td>
<td>
<p>total budget for variable costs, including PSU costs</p>
</td></tr>
<tr><td><code id="clusOpt3fixedPSU_+3A_cal.sw">cal.sw</code></td>
<td>
<p>specify type of optimum:
1 = find optimal <code>m.opt</code> for fixed total budget;
2 = find optimal <code>m.opt</code> for target CV0</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>clusOpt3</code> will compute <code class="reqn">\bar{n}_{opt}</code> and <code class="reqn">\bar{\bar{q}}_{opt}</code> for a three-stage
sample which uses simple random sampling at each stage or <em>ppswr</em> at the first stage and <em>srs</em> at the second and third stages. The set of sample PSUs is assumed to be fixed.
&quot;Variable costs&quot; in <code>tot.cost</code> includes the budget for all costs that vary with the number
of sample PSUs, SSUs, and elements, i.e., <code class="reqn">C_{1}m + C_{2}m\bar{n} + C_{3}m\bar{n}\bar{\bar{q}}</code>.
</p>


<h3>Value</h3>

<p>List with values:
</p>
<table>
<tr><td><code>C1</code></td>
<td>
<p>unit cost per PSU</p>
</td></tr>
<tr><td><code>C2</code></td>
<td>
<p>unit cost per SSU</p>
</td></tr>
<tr><td><code>C3</code></td>
<td>
<p>unit cost per element</p>
</td></tr>
<tr><td><code>m</code></td>
<td>
<p> number of sample PSUs (fixed)</p>
</td></tr>
<tr><td><code>delta1</code></td>
<td>
<p>homogeneity measure among elements within PSUs</p>
</td></tr>
<tr><td><code>delta2</code></td>
<td>
<p>homogeneity measure among elements within SSUs</p>
</td></tr>
<tr><td><code>unit relvar</code></td>
<td>
<p>unit relvariance</p>
</td></tr>
<tr><td><code>k1</code></td>
<td>
<p>ratio of <code class="reqn">B^2 + W^2</code> to unit relvariance</p>
</td></tr>
<tr><td><code>k2</code></td>
<td>
<p>ratio of <code class="reqn">W_{2}^2 + W_{3}^2</code> to unit relvariance</p>
</td></tr>
<tr><td><code>cost</code></td>
<td>
<p>budget constraint, <code>tot.cost</code> if <code>cal.sw</code>=1; computed cost if <code>cal.sw</code>=2</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>optimum number of sample SSUs per PSU</p>
</td></tr>
<tr><td><code>q</code></td>
<td>
<p>optimum number of sample elements per SSU</p>
</td></tr>
<tr><td><code>CV</code></td>
<td>
<p>achieved CV, used if <code>cal.sw</code>=1; or target CV, used if <code>cal.sw</code>=2</p>
</td></tr>
<tr><td><code>CV check</code></td>
<td>
<p>computed CV based on optimal sample sizes; used only if <code>cal.sw</code>=2</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Hansen,M.H., Hurwitz,W.N., and Madow,W.G. (1953, p. 225).  <em>Sample Survey Methods and Theory</em>, Vol.II. John Wiley &amp; Sons.
</p>
<p>Valliant, R., Dever, J., Kreuter, F. (2018, sect. 9.3.2).  <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+clusOpt2">clusOpt2</a></code>, <code><a href="#topic+clusOpt2fixedPSU">clusOpt2fixedPSU</a></code>, <code><a href="#topic+clusOpt3">clusOpt3</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>    # optima for a fixed total budget
clusOpt3fixedPSU(unit.cost=c(500, 100, 120), m=100, delta1=0.01, delta2=0.05, unit.rv=1,
     k1=1, k2=1, tot.cost=500000,cal.sw=1)
    # optima for a target CV
clusOpt3fixedPSU(unit.cost=c(500, 100, 120), m=100, delta1=0.01, delta2=0.05, unit.rv=1,
     k1=1, k2=1, CV0=0.05,cal.sw=2)
</code></pre>

<hr>
<h2 id='CompMOS'>
Compute a composite measure of size for domain-based two-stage sampling
</h2><span id='topic+CompMOS'></span>

<h3>Description</h3>

<p>Compute a composite measure of size variable for domain-based sampling that accounts for desired sampling rates of domain units.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   CompMOS(dsn = NULL,  psuID = NULL, n.PSU = NULL, domain = NULL, domain.req.n = NULL,
   exp.domain.rr = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CompMOS_+3A_dsn">dsn</code></td>
<td>
<p>Data (sampling) frame used for Composite MOS calculations</p>
</td></tr>
<tr><td><code id="CompMOS_+3A_psuid">psuID</code></td>
<td>
<p>PSU Cluster ID</p>
</td></tr>
<tr><td><code id="CompMOS_+3A_n.psu">n.PSU</code></td>
<td>
<p>PSU sample size</p>
</td></tr>
<tr><td><code id="CompMOS_+3A_domain">domain</code></td>
<td>
<p>Vector of domain variable names</p>
</td></tr>
<tr><td><code id="CompMOS_+3A_domain.req.n">domain.req.n</code></td>
<td>
<p>Vector of required sample size from each domain</p>
</td></tr>
<tr><td><code id="CompMOS_+3A_exp.domain.rr">exp.domain.rr</code></td>
<td>
<p>Vector of expected response rate for each domain as a percentage between 0 and 1</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Two-stage samples are often selected from populations for which separate estimates are required for domains, i.e., subpopulations. Composite measures of size for selecting PSU samples with probability proportional to that size can accomplish three things:
</p>

<ol>
<li><p>Self-weighting samples from each of several domains
</p>
</li>
<li><p>Equal workload in each PSU, i.e., same total sample size in each PSU (across all domains)
</p>
</li>
<li><p>PSU selection probabilities that give &quot;credit&quot; for containing domains that are relatively rare in the population
</p>
</li></ol>

<p><code>CompMOS</code> computes a single composite measure of size, probability of inclusion for each PSU in the sampling frame, and within-PSU sampling rates for each domain. Additional variables regarding survey operations at the PSU domain level are also provided (see Value section below).
</p>


<h3>Value</h3>

<p>A list with four components:
</p>
<table>
<tr><td><code>warning</code></td>
<td>
<p>If domain sampling at the desired rate is not feasible in one of more PSUs (i.e. the domain population in the PSU is too small to meet the domain sampling requirements), a warning is included. Review <code>CompMOS.psuID</code> to see where the sampling is not feasible. If all PSUs pass the feasibility test, warning = &quot;None&quot;.</p>
</td></tr>
<tr><td><code>CompMOS.psuID</code></td>
<td>
<p>A data frame containing the input psuID and domain variables, the composite measure of size, the probability of inclusion, and the PSU/domain sampling fractions,
PSU/domain sample sizes, and a feasibility check on each PSU/domain to ensure that the PSU/domain population size is sufficient for sampling.</p>
</td></tr>
<tr><td><code>CompMOS.design</code></td>
<td>
<p>A data frame containing domain level  survey design and sample information from the input data frame and input domain requirements.</p>
</td></tr>
<tr><td><code>CompMOS.Ops</code></td>
<td>
<p>A data frame containing the number of PSUs, the sample workload, and the PSU workload.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>George Zipf, Richard Valliant</p>


<h3>References</h3>

<p>Aldworth J., Hirsch E. L., Martin P. C., Shook-Sa B. E. (2015). 2014 National Survey on Drug Use and Health sample design report. Tech. Rep. Prepared under contract no. HHSS283201300001C by RTI International, Substance Abuse and Mental Health Services Administration, <code>https://www.
samhsa.gov/data/sites/default/files/NSDUHmrbSampleDesign2014v1.pdf</code>
</p>
<p>Singh, A.C. and Harter, R. (2015). Domain sample allocation within primary sampling units in designing domain-level equal probability selection methods. <em>Survey Methodology</em>, 41(2), 297-314.
</p>
<p>Valliant, R., Dever, J., Kreuter, F. (2018, sec. 10.5). <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>psuID &lt;- c(1:10)
D1 &lt;- c(50, 50, 50, 50, 50, 70, 50, 50, 50, 50)
D2 &lt;- c(50, 30, 90, 40, 25, 40, 80, 65, 30, 50)
dsn &lt;- cbind.data.frame(psuID, D1, D2)
n.PSU &lt;- 4
domain &lt;- c("D1", "D2")
domain.req.n &lt;- c(130, 50)
exp.domain.rr &lt;- c(1, 1)

CompMOS(dsn = dsn, psuID  = psuID, n.PSU  = n.PSU, domain = domain,
        domain.req.n = domain.req.n, exp.domain.rr = exp.domain.rr)

  # MDarea.popA has multiple rows for each TRACT; need to summarize TRACT/Age totals
  # for input to CompMOS
data(MDarea.popA)
MDpop &lt;- MDarea.popA[,1:8]
MDpop$AgeGrp &lt;- cut(MDpop$Age, breaks = c(0, 12, 17, 23),
                    labels = c("Age.44.or.under", "Age.45-64", "Age.65+"))

xx &lt;- by(MDpop$TRACT, INDICES=MDpop$AgeGrp, table)
  # All tracts do not contain every age group; merge tract/domain count tables, retaining all tracts
xx1 &lt;- cbind(tract=rownames(xx$Age.44.or.under), as.data.frame(unname(xx$Age.44.or.under)))
colnames(xx1)[3] &lt;- 'Age.44.or.under'
xx2 &lt;- cbind(tract=rownames(xx$`Age.45-64`), as.data.frame(unname(xx$`Age.45-64`)))
colnames(xx2)[3] &lt;- 'Age.45-64'
xx3 &lt;- cbind(tract=rownames(xx$`Age.65+`), as.data.frame(unname(xx$`Age.65+`)))
colnames(xx3)[3] &lt;- 'Age.65+'
pop &lt;- merge(xx1,xx2,by='tract', all=TRUE)
pop &lt;- merge(pop,xx3,by='tract', all=TRUE)
pop &lt;- pop[, -c(2,4,6)]
  # recode counts for missing tract/age-groups to 0
pop[is.na(pop)] &lt;- 0

   # Note that one tract cannot be sampled at the desired rate for the 'Age.65+' domain
MDmos &lt;- CompMOS(dsn  = pop, psuID  = pop$tract, n.PSU  = 15,
              domain = c("Age.44.or.under", "Age.45-64", "Age.65+"),
              exp.domain.rr = c(0.60, 0.70, 0.85),
              domain.req.n  = c(100, 100, 100))
</code></pre>

<hr>
<h2 id='CVcalc2'>
Coefficient of variation of an estimated total in a 2-stage sample
</h2><span id='topic+CVcalc2'></span>

<h3>Description</h3>

<p>Compute the coefficient of variation of an estimated total in a two-stage design. Primary sampling units (PSUs) can be selected either with probability proportional to size (<em>pps</em>) or with equal probability. Elements are selected via simple random sampling (<em>srs</em>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CVcalc2(V=NULL, m=NULL , nbar=NULL, k=1, delta=NULL, Bsq=NULL, Wsq=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CVcalc2_+3A_v">V</code></td>
<td>

<p>unit relvariance of analysis variable in the population
</p>
</td></tr>
<tr><td><code id="CVcalc2_+3A_m">m</code></td>
<td>

<p>number of sample PSUs
</p>
</td></tr>
<tr><td><code id="CVcalc2_+3A_nbar">nbar</code></td>
<td>

<p>number of sample elements per PSU
</p>
</td></tr>
<tr><td><code id="CVcalc2_+3A_k">k</code></td>
<td>

<p>ratio of <code class="reqn">B^2 + W^2</code> to <code class="reqn">V</code>. Default value is 1.
</p>
</td></tr>
<tr><td><code id="CVcalc2_+3A_delta">delta</code></td>
<td>

<p>measure of homogeneity equal to <code class="reqn">B^2/(B^2 + W^2)</code>
</p>
</td></tr>
<tr><td><code id="CVcalc2_+3A_bsq">Bsq</code></td>
<td>

<p>unit relvariance of PSU totals
</p>
</td></tr>
<tr><td><code id="CVcalc2_+3A_wsq">Wsq</code></td>
<td>

<p>within PSU relvariance
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>CVcalc2</code> computes the coefficient of variation of an estimated total for a two-stage sample.  PSUs can be selected either with varying probabilities
and with replacement or with equal probabilities and with replacement. Elements within PSUs are selected by simple random sampling.   The <code class="reqn">CV</code> formula is appropriate for approximating the relvariance of the probability-with-replacement (<em>pwr</em>)-estimator of a total when the same number of elements is selected within each sample PSU. See Sections 9.2.1&ndash;9.2.3 of Valliant, Dever, and Kreuter (2013) for details of formulas.
</p>


<h3>Value</h3>

<p>Value of the coefficient of variation of an estimated total
</p>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Cochran, W.G. (1977, pp.308-310). <em>Sampling Techniques</em>. New York: John Wiley &amp; Sons.
</p>
<p>Saerndal, C.E., Swensson, B., and Wretman, J. (1992). <em>Model Assisted Survey Sampling</em>. New York: Springer.
</p>
<p>Valliant, R., Dever, J., Kreuter, F. (2018, sect. 9.2.1). <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CVcalc3">CVcalc3</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>CVcalc2(V=1, m=20 , nbar=5, k=1, delta=0.05)
CVcalc2(V=10, m=20 , nbar=5, k=1, delta=0.5)
CVcalc2(V=2.5, m=20 , nbar=5, k=2, Bsq=1, Wsq=4)
</code></pre>

<hr>
<h2 id='CVcalc3'>
Coefficient of variation of an estimated total in a 3-stage sample
</h2><span id='topic+CVcalc3'></span>

<h3>Description</h3>

<p>Compute the coefficient of variation of an estimated total in a three-stage design. Primary sampling units (PSUs) can be selected either with probability proportional to size (<em>pps</em>) or with equal probability. Secondary units and elements within SSUs are selected via simple random sampling (<em>srs</em>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CVcalc3(V=NULL, m=NULL , nbar=NULL, qbar=NULL, k1=1, k2=1, delta1=NULL, delta2=NULL,
            Bsq=NULL, Wsq=NULL, W2sq=NULL, W3sq=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CVcalc3_+3A_v">V</code></td>
<td>

<p>unit relvariance of analysis variable in the population
</p>
</td></tr>
<tr><td><code id="CVcalc3_+3A_m">m</code></td>
<td>

<p>number of sample PSUs
</p>
</td></tr>
<tr><td><code id="CVcalc3_+3A_nbar">nbar</code></td>
<td>

<p>number of sample secondary units per PSU
</p>
</td></tr>
<tr><td><code id="CVcalc3_+3A_qbar">qbar</code></td>
<td>

<p>number of sample elements per SSU
</p>
</td></tr>
<tr><td><code id="CVcalc3_+3A_k1">k1</code></td>
<td>

<p>ratio of <code class="reqn">B^2 + W^2</code> to <code class="reqn">V</code>. Default value is 1.
</p>
</td></tr>
<tr><td><code id="CVcalc3_+3A_k2">k2</code></td>
<td>

<p>ratio of <code class="reqn">W_{2}^{2} + W_{3}^{2}</code> to <code class="reqn">V</code>. Default value is 1.
</p>
</td></tr>
<tr><td><code id="CVcalc3_+3A_delta1">delta1</code></td>
<td>

<p>measure of homogeneity between PSUs equal to <code class="reqn">B^2/(B^2 + W^2)</code>
</p>
</td></tr>
<tr><td><code id="CVcalc3_+3A_delta2">delta2</code></td>
<td>

<p>measure of homogeneity between SSUs within PSUs, equal to <code class="reqn">W_{2}^{2}/(W_{2}^{2} + W_{3}^{2})</code>
</p>
</td></tr>
<tr><td><code id="CVcalc3_+3A_bsq">Bsq</code></td>
<td>

<p>unit relvariance of PSU totals, equal to population variance of totals divided by <code class="reqn">\bar{t}_{U}^2</code>
</p>
</td></tr>
<tr><td><code id="CVcalc3_+3A_wsq">Wsq</code></td>
<td>

<p>within PSU relvariance of elements
</p>
</td></tr>
<tr><td><code id="CVcalc3_+3A_w2sq">W2sq</code></td>
<td>

<p>unit SSU relvariance
</p>
</td></tr>
<tr><td><code id="CVcalc3_+3A_w3sq">W3sq</code></td>
<td>

<p>unit element relvariance
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>CVcalc3</code> computes the coefficient of variation of an estimated total for a three-stage sample.  PSUs can be selected either with varying probabilities
and with replacement or with equal probabilities and with replacement. SSUs and elements within SSUs are selected by simple random sampling.
The <code class="reqn">CV</code> formula is appropriate for approximating the relvariance of the probability-with-replacement (<em>pwr</em>)-estimator of a total when
the same number of SSUs is selected in each PSU and the same number of elements is selected within each sample SSU. See Sect. 9.2.4 of Valliant, Dever, and Kreuter (2018) for details of formulas.
</p>


<h3>Value</h3>

<p>Value of the coefficient of variation of an estimated total
</p>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Cochran, W.G. (1977, pp.308-310). <em>Sampling Techniques</em>. New York: John Wiley &amp; Sons.
</p>
<p>Saerndal, C.E., Swensson, B., and Wretman, J. (1992). <em>Model Assisted Survey Sampling</em>. New York: Springer.
</p>
<p>Valliant, R., Dever, J., Kreuter, F. (2018, sect. 9.2.4). <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CVcalc3">CVcalc3</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>CVcalc3(V=1, m=20 , nbar=5, qbar=10, delta1=0.02, delta2=0.10)
CVcalc3(V=1, m=20 , nbar=5, qbar=10, delta1=0.02, delta2=0.09)
CVcalc3(V=2, m=20 , nbar=5, qbar=10, k1=5, k2=10, Bsq=1, Wsq=9, W2sq=2 , W3sq=18 )
</code></pre>

<hr>
<h2 id='deff'>
Design effects of various types
</h2><span id='topic+deff'></span>

<h3>Description</h3>

<p>Compute the Kish, Henry, Spencer, or Chen-Rust design effects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>deff(w, x=NULL, y=NULL, p=NULL, strvar=NULL, clvar=NULL, Wh=NULL, nest=FALSE, type)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="deff_+3A_w">w</code></td>
<td>
<p>vector of weights for a sample</p>
</td></tr>
<tr><td><code id="deff_+3A_x">x</code></td>
<td>
<p>matrix of covariates used to construct a GREG estimator of the total of <code class="reqn">y</code>.  This matrix does not include the intercept. Used only for Henry <em>deff</em>.</p>
</td></tr>
<tr><td><code id="deff_+3A_y">y</code></td>
<td>
<p>vector of the sample values of an analysis variable</p>
</td></tr>
<tr><td><code id="deff_+3A_p">p</code></td>
<td>
<p>vector of 1-draw selection probabilities, i.e., the probability that each unit would be selected in a sample of size 1. Used only for Spencer <em>deff</em>.</p>
</td></tr>
<tr><td><code id="deff_+3A_strvar">strvar</code></td>
<td>
<p>vector of stratum identifiers; equal in length to that of <code>w</code>. Used only for Chen-Rust <em>deff</em>. </p>
</td></tr>
<tr><td><code id="deff_+3A_clvar">clvar</code></td>
<td>
<p>vector of cluster identifiers; equal in length to that of <code>w</code>. Used only for Chen-Rust <em>deff</em>. </p>
</td></tr>
<tr><td><code id="deff_+3A_wh">Wh</code></td>
<td>
<p>vector of the proportions of elements that are in each stratum; length is number of strata. Used only for Chen-Rust <em>deff</em>.</p>
</td></tr>
<tr><td><code id="deff_+3A_nest">nest</code></td>
<td>
<p>Are cluster IDs numbered within strata (<code>TRUE</code> or <code>FALSE</code>)? If <code>TRUE</code>, cluster IDs can be restarted within strata, e.g., 1,2,3,1,2,3,...</p>
</td></tr>
<tr><td><code id="deff_+3A_type">type</code></td>
<td>
<p>type of allocation; must be one of <code>"kish"</code>, <code>"henry"</code>, <code>"spencer"</code>, <code>"cr"</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>deff</code> calls one of <code>deffK</code>, <code>deffH</code>, <code>deffS</code>, or <code>deffCR</code> depending on the value of the <code>type</code> parameter. The Kish design effect is the ratio of the variance of an estimated mean in stratified simple random sampling without replacement (<em>stsrswor</em>) to the variance of the estimated mean in <em>srswor</em>, assuming that all stratum unit variances are equal. In that case, proportional allocation with equal weighting is optimal. deffK equals 1 + relvar(w) where relvar is relvariance of the vector of survey weights. This measure is not appropriate in samples where unequal weighting is more efficient than equal weighting.
</p>
<p>The Henry design effect is the ratio of the variance of the general regression (GREG) estimator of a total of <code class="reqn">y</code> to the variance of the estimated total in <em>srswr</em>. Calculations for the Henry <em>deff</em> are done as if the sample is selected in a single-stage and with replacement.  Varying selection probabilities can be used. The model for the GREG is assumed to be <code class="reqn">y = \alpha + \beta x + \epsilon</code>, i.e., the model has an intercept.
</p>
<p>The Spencer design effect is the ratio of the variance of the <em>pwr</em>-estimator of the total of <em>y</em>, assuming that a single-stage sample is selected with replacement, to the variance of the total estimated in <em>srswr</em>. Varying selection probabilities can be used.
</p>
<p>The Chen-Rust <em>deff</em> accounts for stratification, clustering, and unequal weights, but does not account for the use of any auxiliary data in the estimator of a mean. The Chen-Rust <em>deff</em> returned here is appropriate for stratified, two-stage sampling.</p>


<h3>Value</h3>

<p>Numeric design effect for types <code>kish</code>, <code>henry</code>, <code>spencer</code>. For type <code>cr</code> a list with components:
</p>
<table>
<tr><td><code>strata components</code></td>
<td>
<p>Matrix with <em>deff</em>'s due to weighting, clustering, and stratification for each stratum</p>
</td></tr>
<tr><td><code>overall deff</code></td>
<td>
<p>Design effect for full sample accounting for weighting, clustering, and stratification</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Chen, S. and Rust, K. (2017). An Extension of Kish's Formula for Design Effects to Two- and Three-Stage Designs with Stratification. <em>Journal of Survey Statistics and Methodology</em>, 5(2), 111-130.
</p>
<p>Henry, K.A., and Valliant, R. (2015). A Design Effect Measure for Calibration Weighting in Single-stage Samples. <em>Survey Methodology</em>, 41, 315-331.
</p>
<p>Kish, L. (1965). <em>Survey Sampling</em>. New York: John Wiley &amp; Sons.
</p>
<p>Kish, L. (1992). Weighting for unequal Pi. <em>Journal of Official Statistics</em>, 8, 183-200.
</p>
<p>Park, I., and Lee, H. (2004). Design Effects for the Weighted Mean and Total Estimators under Complex Survey Sampling. <em>Survey Methodology</em>, 30, 183-193.
</p>
<p>Spencer, B. D. (2000). An Approximate Design Effect for Unequal Weighting When Measurements May Correlate With Selection Probabilities. Survey Methodology, 26, 137-138.
</p>
<p>Valliant, R., Dever, J., Kreuter, F. (2018, chap. 14). <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+deffK">deffK</a></code>, <code><a href="#topic+deffH">deffH</a></code>, <code><a href="#topic+deffS">deffS</a></code>, <code><a href="#topic+deffCR">deffCR</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(reshape)      # has function that allows renaming variables
require(sampling)

set.seed(-500398777)
    # generate population using HMT function
pop.dat &lt;- as.data.frame(HMT())
mos &lt;- pop.dat$x
pop.dat$prbs.1d &lt;- mos / sum(mos)
    # select pps sample
n &lt;- 80
pk &lt;- pop.dat$prbs.1d
sam &lt;- UPrandomsystematic(pk)
sam &lt;- sam==1

sam.dat &lt;- pop.dat[sam, ]
dsgn.wts &lt;- 1/pk[sam]
deff(w=dsgn.wts, type="kish")
deff(w=dsgn.wts, y=sam.dat$y, p=sam.dat$prbs.1d, type="spencer")
deff(w=dsgn.wts, x=sam.dat$x, y=sam.dat$y, type="henry")


data(MDarea.popA)
Ni &lt;- table(MDarea.popA$TRACT)
m &lt;- 10
probi &lt;- m*Ni / sum(Ni)
    # select sample of clusters
set.seed(-780087528)
sam &lt;- cluster(data=MDarea.popA, clustername="TRACT", size=m, method="systematic",
                pik=probi, description=TRUE)
    # extract data for the sample clusters
samclus &lt;- getdata(MDarea.popA, sam)
samclus &lt;- rename(samclus, c("Prob" = "pi1"))
    # treat sample clusters as strata and select srswor from each
nbar &lt;- 4
s &lt;- strata(data = as.data.frame(samclus), stratanames = "TRACT",
            size = rep(nbar,m), method="srswor")
    # extracts the observed data
samdat &lt;- getdata(samclus,s)
samdat &lt;- rename(samdat, c("Prob" = "pi2"))
    # add a fake stratum ID
H &lt;- 2
nh &lt;- m * nbar / H
stratum &lt;- NULL
for (h in 1:H){
    stratum &lt;- c(stratum, rep(h,nh))
}
wt &lt;- 1/(samdat$pi1*samdat$pi2) * runif(m*nbar)
samdat &lt;- cbind(subset(samdat, select = -c(Stratum)), stratum, wt)
deff(w = samdat$wt, y=samdat$y2, strvar = samdat$stratum, clvar = samdat$TRACT, Wh=NULL, type="cr")
</code></pre>

<hr>
<h2 id='deffCR'>
Chen-Rust design effect
</h2><span id='topic+deffCR'></span>

<h3>Description</h3>

<p>Chen-Rust design effect for an estimated mean from a stratified, clustered, two-stage samples
</p>


<h3>Usage</h3>

<pre><code class='language-R'>deffCR(w, strvar=NULL, clvar=NULL, Wh=NULL, nest=FALSE, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="deffCR_+3A_w">w</code></td>
<td>
<p>vector of weights for a sample</p>
</td></tr>
<tr><td><code id="deffCR_+3A_strvar">strvar</code></td>
<td>
<p>vector of stratum identifiers; equal in length to that of <code>w</code>. </p>
</td></tr>
<tr><td><code id="deffCR_+3A_clvar">clvar</code></td>
<td>
<p>vector of cluster identifiers; equal in length to that of <code>w</code>. </p>
</td></tr>
<tr><td><code id="deffCR_+3A_wh">Wh</code></td>
<td>
<p>vector of the proportions of elements that are in each stratum; length is number of strata.</p>
</td></tr>
<tr><td><code id="deffCR_+3A_nest">nest</code></td>
<td>
<p>Are cluster IDs numbered within strata (<code>TRUE</code> or <code>FALSE</code>)? If <code>TRUE</code>, cluster IDs can be restarted within strata, e.g., 1,2,3,1,2,3,...</p>
</td></tr>
<tr><td><code id="deffCR_+3A_y">y</code></td>
<td>
<p>vector of the sample values of an analysis variable</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Chen-Rust <em>deff</em> for an estimated mean accounts for stratification, clustering, and unequal weights, but does not account for the use of any auxiliary data in the estimator of a mean. The Chen-Rust <em>deff</em> returned here is appropriate for stratified, two-stage sampling. Note that separate <em>deff</em>'s are produced for weighting, clustering, and stratification within each stratum. These cannot be added across strata unless the stratum values of the coefficient of variation of the weights, the sample size of clusters, and the intracluster correlation of <code>y</code> are equal across all strata (see Chen and Rust 2017, p.117).
</p>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>strata components</code></td>
<td>
<p>Matrix with number of sample first-stage units, intracluster correlation, coefficient of variation of the weights, and <em>deff</em>'s due to weighting (<code>deff.w</code>), clustering (<code>deff.c</code>), and stratification (<code>deff.s</code>)for each stratum. When <code>strvar</code> or <code>clvar</code> are <code>NULL</code> appropriate subsets of these are output.</p>
</td></tr>
<tr><td><code>overall deff</code></td>
<td>
<p>Design effect for full sample accounting for weighting, clustering, and stratification</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Chen, S. and Rust, K. (2017). An Extension of Kish's Formula for Design Effects to Two- and Three-Stage Designs with Stratification. <em>Journal of Survey Statistics and Methodology</em>, 5(2), 111-130.
</p>
<p>Valliant, R., Dever, J., Kreuter, F. (2018, chap. 14). <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+deff">deff</a></code>, <code><a href="#topic+deffH">deffH</a></code>, <code><a href="#topic+deffK">deffK</a></code>, <code><a href="#topic+deffS">deffS</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(sampling)
require(reshape)
data(MDarea.popA)
Ni &lt;- table(MDarea.popA$TRACT)
m &lt;- 20
probi &lt;- m*Ni / sum(Ni)
    # select sample of clusters
set.seed(-780087528)
sam &lt;- sampling::cluster(data=MDarea.popA, clustername="TRACT", size=m, method="systematic",
                pik=probi, description=TRUE)
    # extract data for the sample clusters
samclus &lt;- getdata(MDarea.popA, sam)
samclus &lt;- rename(samclus, c("Prob" = "pi1"))
    # treat sample clusters as strata and select srswor from each
nbar &lt;- 8
s &lt;- sampling::strata(data = as.data.frame(samclus), stratanames = "TRACT",
            size = rep(nbar,m), method="srswor")
    # extracts the observed data
samdat &lt;- getdata(samclus,s)
samdat &lt;- rename(samdat, c("Prob" = "pi2"))
    # add a fake stratum ID
H &lt;- 2
nh &lt;- m * nbar / H
stratum &lt;- NULL
for (h in 1:H){
    stratum &lt;- c(stratum, rep(h,nh))
}
wt &lt;- 1/(samdat$pi1*samdat$pi2) * runif(m*nbar)
samdat &lt;- cbind(subset(samdat, select = -c(stratum)), stratum, wt)
deffCR(w = samdat$wt, strvar = samdat$stratum, clvar = samdat$TRACT, Wh=NULL, y=samdat$y2)
</code></pre>

<hr>
<h2 id='deffH'>
Henry design effect for <em>pps</em> sampling and GREG estimation of totals
</h2><span id='topic+deffH'></span>

<h3>Description</h3>

<p>Compute the Henry design effect for single-stage samples when a general regression estimator is used for a total.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>deffH(w, y, x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="deffH_+3A_w">w</code></td>
<td>
<p>vector of inverses of selection probabilities for a sample</p>
</td></tr>
<tr><td><code id="deffH_+3A_y">y</code></td>
<td>
<p>vector of the sample values of an analysis variable</p>
</td></tr>
<tr><td><code id="deffH_+3A_x">x</code></td>
<td>
<p>matrix of covariates used to construct a GREG estimator of the total of <code class="reqn">y</code>.  This matrix does not include the intercept.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Henry design effect is the ratio of the variance of the general regression (GREG) estimator of a total of <code class="reqn">y</code> to the variance of the estimated total in <em>srswr</em>. Calculations for the Henry <em>deff</em> are done as if the sample is selected in a single-stage and with replacement.  Varying selection probabilities can be used. The model for the GREG is assumed to be <code class="reqn">y = \alpha + \beta x + \epsilon</code>, i.e., the model has an intercept.  </p>


<h3>Value</h3>

<p>numeric design effect
</p>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Henry, K.A., and Valliant, R. (2015). A Design Effect Measure for Calibration Weighting in Single-stage Samples. <em>Survey Methodology</em>, 41, 315-331.
</p>
<p>Valliant, R., Dever, J., Kreuter, F. (2018, chap. 14). <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+deff">deff</a></code>, <code><a href="#topic+deffCR">deffCR</a></code>, <code><a href="#topic+deffK">deffK</a></code>, <code><a href="#topic+deffS">deffS</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(-500398777)
    # generate population using HMT function
pop.dat &lt;- as.data.frame(HMT())
mos &lt;- pop.dat$x
pop.dat$prbs.1d &lt;- mos / sum(mos)
    # select pps sample
require(sampling)
n &lt;- 80
pk &lt;- n * pop.dat$prbs.1d
sam &lt;- UPrandomsystematic(pk)
sam &lt;- sam==1
sam.dat &lt;- pop.dat[sam, ]
dsgn.wts &lt;- 1/pk[sam]
deffH(w=dsgn.wts, y=sam.dat$y, x=sam.dat$x)
</code></pre>

<hr>
<h2 id='deffK'>
Kish design effect
</h2><span id='topic+deffK'></span>

<h3>Description</h3>

<p>Compute the Kish design effect due to having unequal weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>deffK(w)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="deffK_+3A_w">w</code></td>
<td>
<p>vector of inverses of selection probabilities for a sample</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Kish design effect is the ratio of the variance of an estimated mean in stratified simple random sampling without replacement (<em>stsrswor</em>) to the variance of the estimated mean in <em>srswor</em>, assuming that all stratum unit variances are equal. In that case, proportional allocation with equal weighting is optimal. <code>deffK</code> equals <code class="reqn">1 + relvar(w)</code> where <code class="reqn">relvar</code> is relvariance of the vector of survey weights. This measure is not appropriate in samples where unequal weighting is more efficient than equal weighting.
</p>


<h3>Value</h3>

<p>numeric design effect
</p>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Kish, L. (1965). <em>Survey Sampling</em>. New York: John Wiley &amp; Sons.
</p>
<p>Kish, L. (1992). Weighting for unequal Pi.  <em>Journal of Official Statistics</em>, 8, 183-200.
</p>
<p>Valliant, R., Dever, J., Kreuter, F. (2018, chap. 14). <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+deff">deff</a></code>, <code><a href="#topic+deffCR">deffCR</a></code>, <code><a href="#topic+deffH">deffH</a></code>, <code><a href="#topic+deffS">deffS</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(nhis)
w &lt;- nhis$svywt
deffK(w)
</code></pre>

<hr>
<h2 id='deffS'>
Spencer design effect for an estimated total from a <em>pps</em> sample
</h2><span id='topic+deffS'></span>

<h3>Description</h3>

<p>Compute the Spencer design effect for single-stage samples selected with probability proportional to a measure of size.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>deffS(p, w, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="deffS_+3A_p">p</code></td>
<td>
<p>vector of 1-draw selection probabilities, i.e., the probability that each unit would be selected in a sample of size 1.</p>
</td></tr>
<tr><td><code id="deffS_+3A_w">w</code></td>
<td>
<p>vector of inverses of selection probabilities for a sample</p>
</td></tr>
<tr><td><code id="deffS_+3A_y">y</code></td>
<td>
<p>vector of the sample values of an analysis variable</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Spencer design effect is the ratio of the variance of the <em>pwr-</em>estimator of the total of <code class="reqn">y</code>, assuming that a single-stage sample is selected with replacement, to the variance of the total estimated in <em>srswr</em>. Varying selection probabilities can be used.
</p>


<h3>Value</h3>

<p>numeric design effect
</p>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Park, I., and Lee, H. (2004). Design Effects for the Weighted Mean and Total Estimators under Complex Survey Sampling. <em>Survey Methodology</em>, 30, 183-193.
</p>
<p>Spencer, B. D. (2000). An Approximate Design Effect for Unequal Weighting When Measurements May Correlate With Selection Probabilities. <em>Survey Methodology</em>, 26, 137-138.
</p>
<p>Valliant, R., Dever, J., Kreuter, F. (2018, chap. 14). <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+deff">deff</a></code>, <code><a href="#topic+deffCR">deffCR</a></code>, <code><a href="#topic+deffH">deffH</a></code>, <code><a href="#topic+deffK">deffK</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(-500398777)
    # generate population using HMT function
pop.dat &lt;- as.data.frame(HMT())
mos &lt;- pop.dat$x
pop.dat$prbs.1d &lt;- mos / sum(mos)
    # select pps sample
require(sampling)
n &lt;- 80
pk &lt;- pop.dat$prbs.1d
sam &lt;- UPrandomsystematic(pk)
sam &lt;- sam==1
sam.dat &lt;- pop.dat[sam, ]
dsgn.wts &lt;- 1/pk[sam]
deffS(p=sam.dat$prbs.1d, w=dsgn.wts, y=sam.dat$y)
</code></pre>

<hr>
<h2 id='Domainy1y2'>
Domain data
</h2><span id='topic+Domainy1y2'></span>

<h3>Description</h3>

<p><code>Domainy1y2</code> is a small data file used for an exercise in sample size calculations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Domainy1y2)</code></pre>


<h3>Format</h3>

<p>A data frame with 30 observations on 2 variables.
</p>

<dl>
<dt><code>y1</code></dt><dd><p>an artificial variable</p>
</dd>
<dt><code>y2</code></dt><dd><p>an artificial variable</p>
</dd>
</dl>



<h3>References</h3>

<p>Valliant, R., Dever, J., Kreuter, F. (2018, chap. 3). <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Domainy1y2)
str(Domainy1y2)
summary(Domainy1y2)
</code></pre>

<hr>
<h2 id='dub'>
Sample sizes for a double sampling design
</h2><span id='topic+dub'></span>

<h3>Description</h3>

<p>Compute samples sizes at each phase of a two-phase design where strata are created using the first phase.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dub(c1, c2, Ctot, Nh, Sh, Yh.bar)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dub_+3A_c1">c1</code></td>
<td>
<p>cost per unit in phase-1</p>
</td></tr>
<tr><td><code id="dub_+3A_c2">c2</code></td>
<td>
<p>cost per unit in phase-2</p>
</td></tr>
<tr><td><code id="dub_+3A_ctot">Ctot</code></td>
<td>
<p>Total variable cost</p>
</td></tr>
<tr><td><code id="dub_+3A_nh">Nh</code></td>
<td>
<p>Vector of stratum population counts or proportions</p>
</td></tr>
<tr><td><code id="dub_+3A_sh">Sh</code></td>
<td>
<p>Vector of stratum population standard deviations</p>
</td></tr>
<tr><td><code id="dub_+3A_yh.bar">Yh.bar</code></td>
<td>
<p>Vector of stratum population means</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Compute the first and second phase sample sizes for a double sampling design. A first phase sample is selected by simple random sampling (<em>srs</em>). Strata are formed based on information collected in the first phase. The Neyman allocation to strata of the second phase sample is computed ignoring costs. Optimal total sample sizes are computed for the first and second phases, given per-unit costs for the first and second phases and a fixed total budget for both phases combined.</p>


<h3>Value</h3>

<p>A list object with elements:
</p>
<table>
<tr><td><code>V1</code></td>
<td>
<p>Variance component associated with phase-1</p>
</td></tr>
<tr><td><code>V2</code></td>
<td>
<p>Variance component associated with phase-2</p>
</td></tr>
<tr><td><code>n1</code></td>
<td>
<p>Phase-1 sample size</p>
</td></tr>
<tr><td><code>n2</code></td>
<td>
<p>Total phase-2 sample across all strata</p>
</td></tr>
<tr><td><code>"n2/n1"</code></td>
<td>
<p>Fraction that phase-2 is of phase-1</p>
</td></tr>
<tr><td><code>ney.alloc</code></td>
<td>
<p>Vector of stratum sample sizes for phase-2 sample</p>
</td></tr>
<tr><td><code>Vopt</code></td>
<td>
<p>Variance of mean with the calculated phase-1 and phase-2 sample sizes</p>
</td></tr>
<tr><td><code>nsrs</code></td>
<td>
<p>Size of an <em>srs</em> that has cost <code>Ctot</code>, assuming each unit costs <code>c2</code></p>
</td></tr>
<tr><td><code>Vsrs</code></td>
<td>
<p>Variance of mean in an <em>srs</em> of cost <code>Ctot</code>, assuming each unit costs <code>c2</code></p>
</td></tr>
<tr><td><code>Vratio</code></td>
<td>
<p>Ratio of <code>Vopt</code> to <code>Vsrs</code></p>
</td></tr>
<tr><td><code>Ctot</code></td>
<td>
<p>Input value of total cost</p>
</td></tr>
<tr><td><code>cost.chk</code></td>
<td>
<p>Computed value of phase-1 plus phase-2 sample with optimal sample sizes; should agree with Ctot</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Cochran W (1977, sect. 12.3) <em>Sampling Techniques</em>. New York: John Wiley &amp; Sons, Inc.
</p>
<p>Neyman J (1938) Contribution to the theory of sampling human populations. <em>Journal
of the American Statistical Association</em>, 33(201), 101-116.
</p>
<p>Valliant, R., Dever, J., Kreuter, F. (2018, sect. 17.5.2).  <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Wh &lt;- rep(0.25,4)
Ph &lt;- c(0.02,0.12,0.37,0.54)
Sh &lt;- sqrt(Ph*(1-Ph))
c1 &lt;- 10
c2 &lt;- 50
Ctot &lt;- 20000
dub(c1, c2, Ctot, Nh=Wh, Sh, Yh.bar=Ph)
</code></pre>

<hr>
<h2 id='gamEst'>
Estimate variance model parameter <code class="reqn">\gamma</code>
</h2><span id='topic+gamEst'></span>

<h3>Description</h3>

<p>Regresses a <em>y</em> on a set of covariates X where <code class="reqn">Var_M(y)=\sigma^2x^\gamma</code> and then
regresses the squared residuals on <code class="reqn">log(x)</code> to estimate <code class="reqn">\gamma</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gamEst(X1, x1, y1, v1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gamEst_+3A_x1">X1</code></td>
<td>

<p>matrix of predictors in the linear model for <code>y1</code>
</p>
</td></tr>
<tr><td><code id="gamEst_+3A_x1">x1</code></td>
<td>

<p>vector of <em>x</em>'s for individual units in the assumed specification of <code class="reqn">Var_M(y)</code>
</p>
</td></tr>
<tr><td><code id="gamEst_+3A_y1">y1</code></td>
<td>

<p>vector of dependent variables for individual units
</p>
</td></tr>
<tr><td><code id="gamEst_+3A_v1">v1</code></td>
<td>

<p>vector proportional to <code class="reqn">Var_M(y)</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>gamEst</code> estimates the power <code class="reqn">\gamma</code> in a model where the variance
of the errors is proportional to <code class="reqn">x^\gamma</code> for some covariate x.
Values of <code class="reqn">\gamma</code> are typically in [0,2].  The function is iteratively called by <code><a href="#topic+gammaFit">gammaFit</a></code>, which is normally the function that an analyst should use.
</p>


<h3>Value</h3>

<p>The estimate of <code class="reqn">\gamma</code>.
</p>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Valliant, R., Dever, J., Kreuter, F. (2018, chap. 3).  <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gammaFit">gammaFit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(hospital)
x &lt;- hospital$x
y &lt;- hospital$y

X &lt;- cbind(sqrt(x), x)
gamEst(X1 = X, x1 = x, y1 = y, v1 = x)
</code></pre>

<hr>
<h2 id='gammaFit'>
Iteratively estimate variance model parameter <code class="reqn">\gamma</code>
</h2><span id='topic+gammaFit'></span>

<h3>Description</h3>

<p>Iteratively computes estimate of <code class="reqn">\gamma</code> in a model with <code class="reqn">E_M(y)=x^T\beta</code> and
<code class="reqn">Var_M(y)=\sigma^2x^\gamma</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gammaFit(X, x, y, maxiter = 100, show.iter = FALSE, tol = 0.001)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gammaFit_+3A_x">X</code></td>
<td>

<p>matrix of predictors in the linear model for <em>y</em>
</p>
</td></tr>
<tr><td><code id="gammaFit_+3A_x">x</code></td>
<td>

<p>vector of <em>x</em>'s for individual units in the assumed specification of <code class="reqn">Var_M(y)</code>
</p>
</td></tr>
<tr><td><code id="gammaFit_+3A_y">y</code></td>
<td>

<p>vector of dependent variables for individual units
</p>
</td></tr>
<tr><td><code id="gammaFit_+3A_maxiter">maxiter</code></td>
<td>

<p>maximum number of iterations allowed
</p>
</td></tr>
<tr><td><code id="gammaFit_+3A_show.iter">show.iter</code></td>
<td>

<p>should values of <code class="reqn">\gamma</code> be printed of each iteration? <code>TRUE</code> or <code>FALSE</code>
</p>
</td></tr>
<tr><td><code id="gammaFit_+3A_tol">tol</code></td>
<td>

<p>size of relative difference in <code class="reqn">\hat{\gamma}</code>'s between consecutive iterations
used to determine convergence. Algorithm terminates when relative difference
is less than <code>tol</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>gammaFit</code> estimates the power <code class="reqn">\gamma</code> in a model where the variance
of the errors is proportional to <code class="reqn">x^\gamma</code> for some covariate <em>x</em>.
Values of <code class="reqn">\gamma</code> are typically in [0,2]. The function calls <code><a href="#topic+gamEst">gamEst</a></code>.
</p>


<h3>Value</h3>

<p>A list with the components:
</p>
<table>
<tr><td><code>g.hat</code></td>
<td>
<p>estimate of <code class="reqn">\gamma</code> when iterative procedure stopped</p>
</td></tr>
<tr><td><code>converged</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code> depending on whether convergence was obtained</p>
</td></tr>
<tr><td><code>steps</code></td>
<td>
<p>number of steps used by the algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Valliant, R., Dever, J., Kreuter, F. (2018, chap. 3).  <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gamEst">gamEst</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(hospital)
x &lt;- hospital$x
y &lt;- hospital$y

X &lt;- cbind(sqrt(x), x)
gammaFit(X = X, x = x, y = y, maxiter=100, tol=0.002)
</code></pre>

<hr>
<h2 id='GeoDistMOS'>
Split geographic PSUs based on a measure of size threshold
</h2><span id='topic+GeoDistMOS'></span>

<h3>Description</h3>

<p>Split geographic PSUs into new geographically contiguous PSUs based on a maximum measure of size for each PSU
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   GeoDistMOS(lat, long, psuID, n, MOS.var, MOS.takeall = 1, Input.ID = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GeoDistMOS_+3A_lat">lat</code></td>
<td>
<p>latitude variable in an input file. Must be in decimal format.</p>
</td></tr>
<tr><td><code id="GeoDistMOS_+3A_long">long</code></td>
<td>
<p>longitude variable in an input file. Must be in decimal format.</p>
</td></tr>
<tr><td><code id="GeoDistMOS_+3A_psuid">psuID</code></td>
<td>
<p>PSU Cluster ID from an input file.</p>
</td></tr>
<tr><td><code id="GeoDistMOS_+3A_n">n</code></td>
<td>
<p>Sample size of PSUs; may be a preliminary value used in the computation to identify certainty PSUs</p>
</td></tr>
<tr><td><code id="GeoDistMOS_+3A_mos.var">MOS.var</code></td>
<td>
<p>Variable used for probability proportional to size sampling</p>
</td></tr>
<tr><td><code id="GeoDistMOS_+3A_mos.takeall">MOS.takeall</code></td>
<td>
<p>Threshold relative measure of size value for certainties; must satisfy 0 &lt; <code>MOS.takeall</code> &lt;= 1</p>
</td></tr>
<tr><td><code id="GeoDistMOS_+3A_input.id">Input.ID</code></td>
<td>
<p>ID variable from the input file</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>GeoDistMOS</code> splits geographic primary sampling units (PSUs) in the input object based on a variable which is used to create the measure of size for each PSU (<code>MOS.var</code>). The goal is to create PSUs of similarly sized MOS. The input file should have one row for each geographic unit, i.e. secondary sampling unit (SSU), with a PSU ID assigned. The latitude and longitude input vectors define the centroid of each input SSU. The complete linkage method for clustering is used. Accordingly, PSUs are split on a distance metric and not on the MOS threshold value. <code>GeoDistMOS</code> calls the function <code>inclusionprobabilities</code> from the <code>sampling</code> package to calculate the inclusion probability for each SSU within a PSU and <code>distHaversine</code> from the <code>geosphere</code> package to calculate the distances between centroids.
</p>


<h3>Value</h3>

<p>A list with two components:
</p>
<table>
<tr><td><code>PSU.ID.Max.MOS</code></td>
<td>
<p>A data frame containing the SSU ID value in character format (<code>Input.ID</code>), the original PSU ID (<code>psuID.orig</code>), and the new PSU ID after splitting for the maximum measure of size (<code>psuID.new</code>).</p>
</td></tr>
<tr><td><code>PSU.Max.MOS.Info</code></td>
<td>
<p>A data frame containing the new PSU ID (<code>psuID.new</code>) after splitting for the maximum Measure of Size, the inclusion probability of the PSU ID given the input sample size n (<code>psuID.prob</code>), the measure of size of the new PSU (<code>MOS</code>), the number of SSUs in the new PSU ID (<code>Number.SSUs</code>), and the means of the SSUs latitudes and longitudes that were combined to form the new PSU (<code>PSU.Mean.Latitude</code> and <code>PSU.Mean.Longitude</code>).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>George Zipf, Richard Valliant
</p>


<h3>See Also</h3>

<p><code><a href="#topic+GeoDistPSU">GeoDistPSU</a></code>, <code><a href="#topic+GeoMinMOS">GeoMinMOS</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Test_Data_US)

   # Create PSU ID with GeoDistPSU
g &lt;- GeoDistPSU(Test_Data_US$lat,
                Test_Data_US$long,
                "miles",
                100,
                Input.ID = Test_Data_US$ID)
   # Append PSU ID to input file
library(dplyr)
Test_Data_US &lt;- dplyr::inner_join(Test_Data_US, g$PSU.ID, by=c("ID" = "Input.file.ID"))

   # Split PSUs with MOS above 0.80
m &lt;- GeoDistMOS(lat         = Test_Data_US$lat,
                long        = Test_Data_US$long,
                psuID       = Test_Data_US$psuID,
                n           = 15,
                MOS.var     = Test_Data_US$Amount,
                MOS.takeall = 0.80,
                Input.ID    = Test_Data_US$ID)

   # Create histogram of Measure of Size Values
hist(m$PSU.Max.MOS.Info$psuID.prob,
     breaks = seq(0, 1, 0.1),
     main = "Histogram of PSU Inclusion Probabilities (Certainties = 1)",
     xlab = "Inclusion Probability",
     ylab = "Frequency")
</code></pre>

<hr>
<h2 id='GeoDistPSU'>
Form PSUs based on geographic distances
</h2><span id='topic+GeoDistPSU'></span>

<h3>Description</h3>

<p>Combine geographic areas into primary sampling units to limit travel distances
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GeoDistPSU(lat, long, dist.sw, max.dist, Input.ID = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GeoDistPSU_+3A_lat">lat</code></td>
<td>
<p>latitude variable in an input file. Must be in decimal format.</p>
</td></tr>
<tr><td><code id="GeoDistPSU_+3A_long">long</code></td>
<td>
<p>longitude variable in an input file. Must be in decimal format.</p>
</td></tr>
<tr><td><code id="GeoDistPSU_+3A_dist.sw">dist.sw</code></td>
<td>
<p>units for distance; either <code>"miles"</code> or <code>"kms"</code> (for kilometers)</p>
</td></tr>
<tr><td><code id="GeoDistPSU_+3A_max.dist">max.dist</code></td>
<td>
<p>maximum distance allowed within a PSU between centroids of geographic units</p>
</td></tr>
<tr><td><code id="GeoDistPSU_+3A_input.id">Input.ID</code></td>
<td>
<p>ID field in the input file if present</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>GeoDistPSU</code> combines geographic secondary sampling units (SSUs), like cities or census block groups, into primary sampling units (PSUs) given a maximum distance allowed between the centroids of the SSUs within each grouped PSU. The input file must have one row for each geographic unit. If the input file does not have an ID field, the function will create a sequential ID that is appended to the output. The latitude and longitude input vectors define the centroid of each input SSU. The complete linkage method for clustering is used. <code>GeoDistPSU</code> calls the functions <code>distm</code> and <code>distHaversine</code> from the <code>geosphere</code> package to calculate the distances between centroids.
</p>


<h3>Value</h3>

<p>A list with two components:
</p>
<table>
<tr><td><code>PSU.ID</code></td>
<td>
<p>A data frame with the same number of rows as the input file. Column names are <code>Input.file.ID</code> and <code>psuID</code>. The <code>psuID column</code> contains the PSU number assigned to each geographic unit in the input file; multiple rows of the input file will typically be assigned to the same PSU.</p>
</td></tr>
<tr><td><code>PSU.Info</code></td>
<td>
<p>A data frame with the number of rows equal to the number of PSUs that are created. Column names are <code>Num.SSUs</code>, number of SSUs assigned to each PSU; <code>PSU.Mean.Latitude</code>, mean of the latitudes of the units assigned to a PSU; <code>PSU.Mean.Longitude</code>, mean of the longitudes of the units assigned to a PSU; <code>PSU.Max.Dist</code>, maximum distance among the SSUs in a PSU</p>
</td></tr></table>
<p>.
</p>


<h3>Author(s)</h3>

<p>George Zipf, Richard Valliant
</p>


<h3>See Also</h3>

<p><code><a href="#topic+GeoDistMOS">GeoDistMOS</a></code>, <code><a href="#topic+GeoMinMOS">GeoMinMOS</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Test_Data_US)
g &lt;- GeoDistPSU(Test_Data_US$lat,
                Test_Data_US$long,
                "miles", 100,
                Input.ID = Test_Data_US$ID)
    # Plot GeoDistPSU output
plot(g$PSU.Info$PSU.Mean.Longitude,
     g$PSU.Info$PSU.Mean.Latitude,
     col  = 1:nrow(g$PSU.Info),
     pch  = 19,
     main = "Plot of PSU Centers",
     xlab = "Longitude",
     ylab = "Latitude")
grid(col = "grey40")

    # Plot GeoDistPSU output with map
## Not run: 
  # install package sf to run usmap_transform
library(ggplot2)
library(sp)
library(usmap)
    # Transform PSUs into usmap projection
g.map  &lt;- cbind(long = g$PSU.Info$PSU.Mean.Longitude,
                lat  = g$PSU.Info$PSU.Mean.Latitude)
g.map  &lt;- as.data.frame(g.map)
g.proj &lt;- usmap::usmap_transform(g.map,
                          input_names  = c("long", "lat"),
                          output_names = c("Long", "Lat"))
usmap::plot_usmap(color = "gray") +
  geom_point(data = g.proj,
             aes(x = Long,
                 y = Lat))
    # Create histogram of maximum distance
hist(g$PSU.Info$PSU.Max.Dist,
     main = "Histogram of Maximum Within-PSU Distance",
     xlab = "Distance",
     ylab = "Frequency")

## End(Not run)
</code></pre>

<hr>
<h2 id='GeoMinMOS'>
Check geographic PSUs to determine whether any are less than minimum measure of size threshold
</h2><span id='topic+GeoMinMOS'></span>

<h3>Description</h3>

<p>Identify geographic PSUs whose measure of size is below a specified minimum and combine those PSUs with others
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   GeoMinMOS(lat, long, geo.var, MOS.var, MOS.min)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GeoMinMOS_+3A_lat">lat</code></td>
<td>
<p>latitude variable in an input file. Must be in decimal format.</p>
</td></tr>
<tr><td><code id="GeoMinMOS_+3A_long">long</code></td>
<td>
<p>longitude variable in an input file. Must be in decimal format.</p>
</td></tr>
<tr><td><code id="GeoMinMOS_+3A_geo.var">geo.var</code></td>
<td>
<p>Geographic variable ID for grouping</p>
</td></tr>
<tr><td><code id="GeoMinMOS_+3A_mos.var">MOS.var</code></td>
<td>
<p>Measure of size (MOS) for each PSU</p>
</td></tr>
<tr><td><code id="GeoMinMOS_+3A_mos.min">MOS.min</code></td>
<td>
<p>Minimum allowed MOS value</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>GeoMinMOS</code> is a utility function that should be run after using <code>GeoDistMOS</code> or <code>GeoDistPSU</code>. <code>GeoMinMOS</code> identifies each PSU whose measure of size, (MOS.var), is below the minimum specified by <code>MOS.min</code>. Distances to the latitude/longitude centroids of other PSUs are calculated. The undersized PSUs are then combined with the nearest PSUs in proximity order until the minimum MOS is met or exceeded. In some cases, <em>this can result in the same input PSU being assigned to more than one combined PSU</em>. Also, the distances between the centroids of the PSUs in a combination may be impractically large. Thus, the new combinations generated by <code>GeoMinMOS</code> should be treated as suggestions that should be manually reviewed and adjusted if desired.
</p>


<h3>Value</h3>

<p>A list with four components:
</p>
<table>
<tr><td><code>Parameter.Information</code></td>
<td>
<p>A data frame with three elements: <code>Minimum.MOS</code> = value of <code>MOS.min</code>; <code>Geo.vars.start</code> = total number of PSUs in the input data set; <code>Geo.Vars.lt.min.MOS</code> = number of PSUs whose MOS was less than the minimum.
</p>
</td></tr>
<tr><td><code>Input.Information</code></td>
<td>
<p>A data frame containing <code>Geo.Var</code> = geographic variable ID in the input data set used for grouping; <code>Geo.MOS</code> = MOS of each PSU in the input data set; <code>Below.min.MOS</code> = TRUE/FALSE indicator for whether a PSU's MOS was below the minimum in <code>MOS.min</code>.
</p>
</td></tr>
<tr><td><code>Geo.var.MOS.output</code></td>
<td>
<p>A data frame with PSUs that were formed by combining undersized PSUs with adequately-sized PSUs. Columns in the data frame are: <code>Geo.Var</code> = new geographic variable ID for a combined PSU. This is equal to <code>geo.var</code> for the undersized PSU used in a combination;  <code>New.Geo.MOS</code> = input MOS for each PSU; <code>Geo.Cum.MOS</code> = cumulative MOS for a combined PSU. The last PSU in a combination will have the total size of the combined PSU; <code>Geo.Var.ID</code> = geographic variable ID for a PSU in the input data set;  <code>Geo.Var.Num</code> = sequential number (1, 2, etc.) for the PSUs in a combination; <code>Geo.Var.Kms</code> = distance in kilometers of a PSU's centroid to the centroid of the undersized PSU in a combination. The undersized PSU will have a distance of 0; <code>Geo.Var.Miles</code> = distance in miles of a PSU's centroid to the centroid of the undersized PSU in a combination. The undersized PSU will have a distance of 0; <code>Geo.Var.Lat</code> = latitude of the PSU centroid; <code>Geo.Var.Long</code> = longitude of the PSU centroid.
</p>
</td></tr>
<tr><td><code>For.Review</code></td>
<td>
<p>A list of the <code>geo.var</code>'s of PSUs that are used in more than one combination; these should be manually reviewed to determine which combination is preferred. The distances between PSU centroids in <code>Geo.var.MOS.output</code> can be helpful in the review.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>George Zipf, Richard Valliant
</p>


<h3>See Also</h3>

<p><code><a href="#topic+GeoDistPSU">GeoDistPSU</a></code>, <code><a href="#topic+GeoDistMOS">GeoDistMOS</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(PracTools)
library(dplyr)
g &lt;- GeoDistPSU(Test_Data_US$lat,
                Test_Data_US$long,
                "miles",
                100,
                Input.ID = Test_Data_US$ID)
Test_Data_US &lt;- inner_join(Test_Data_US, g$PSU.ID, by=c("ID" = "Input.file.ID"))
GeoMinMOS(lat      = Test_Data_US$lat,
          long     = Test_Data_US$long,
          geo.var  = Test_Data_US$psuID,
          MOS.var  = Test_Data_US$Amount,
          MOS.min  = 200000)
</code></pre>

<hr>
<h2 id='HMT'>
Generate an HMT population
</h2><span id='topic+HMT'></span>

<h3>Description</h3>

<p>Generate a population that follows the model in Hansen, Madow, and Tepping (1983)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HMT(N=5000, H=10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="HMT_+3A_n">N</code></td>
<td>
<p>population size</p>
</td></tr>
<tr><td><code id="HMT_+3A_h">H</code></td>
<td>
<p>number of strata</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>HMT</code> generates a population based on the model: <code class="reqn">E(y)= \alpha + \beta x</code>, <code class="reqn">Var(y)=\sigma^2x^{3/2}</code>. Both <em>x</em> and <em>y</em> have gamma distributions. Strata are formed to have approximately the same total of <code class="reqn">x</code>.</p>


<h3>Value</h3>

<p>N x 3 matrix with columns:
</p>
<table>
<tr><td><code>strat</code></td>
<td>
<p>stratum ID</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>auxiliary variable <em>x</em></p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>analysis variable <em>y</em></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Hansen,M.H., Madow,W.G., and Tepping,B.J. (1983). An evaluation of model-dependent and probability sampling inferences in sample surveys. <em>Journal of the American Statistical Association</em>, 78, 776-793.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>    # generate HMT population with 1000 units and 5 strata and plot results
pop &lt;- HMT(N=1000, H=5)
plot(pop[,"x"],pop[,"y"])
</code></pre>

<hr>
<h2 id='hospital'>
Hospital Data
</h2><span id='topic+hospital'></span>

<h3>Description</h3>

<p>The <code>hospital</code> data file is a national sample of short-stay hospitals with fewer than 1000 beds.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(hospital)</code></pre>


<h3>Format</h3>

<p>A data frame with 393 observations on the following 2 variables.
</p>

<dl>
<dt><code>y</code></dt><dd><p>Number of patients discharged by the hospital in January 1968</p>
</dd>
<dt><code>x</code></dt><dd><p>Number of inpatient beds in the hospital</p>
</dd>
</dl>



<h3>Details</h3>

<p>The <code>hospital</code> data are from the National Hospital Discharge Survey conducted by the U.S. National Center for Health Statistics.
The survey collects characteristics of inpatients discharged from non-Federal short-stay hospitals in the United States.
This population is from the January 1968 survey and contains observations on 393 hospitals.
</p>


<h3>Source</h3>

<p>National Center for Health Statistics Hospital Discharge Survey of 1968.
</p>


<h3>References</h3>

<p>Herson, J. (1976). An Investigation of Relative Efficiency of Least-Squares Prediction to Conventional Probability Sampling Plans.  <em>Journal of the American Statistical Association</em>, 71, 700-703.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(hospital)
str(hospital)
</code></pre>

<hr>
<h2 id='labor'>
Labor force population
</h2><span id='topic+labor'></span>

<h3>Description</h3>

<p>A clustered population of persons extracted from the September 1976 Current Population Survey (CPS)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(labor)</code></pre>


<h3>Format</h3>

<p>A data frame with 478 persons on the following variables:
</p>

<dl>
<dt><code>h</code></dt><dd><p>stratum</p>
</dd>
<dt><code>cluster</code></dt><dd><p>cluster (or segment) number</p>
</dd>
<dt><code>person</code></dt><dd><p>person number</p>
</dd>
<dt><code>age</code></dt><dd><p>age of person</p>
</dd>
<dt><code>agecat</code></dt><dd><p>age category
(1 = 19 years and under;
2 = 20-24;
3 = 25-34;
4 = 35-64;
5 = 65 years and over)
</p>
</dd>
<dt><code>race</code></dt><dd><p>race	
(1 = non-Black;
2 = Black)
</p>
</dd>
<dt><code>sex</code></dt><dd><p>Gender (1=Male; 2=Female)</p>
</dd>
<dt><code>HoursPerWk</code></dt><dd><p>Usual number of hours worked per week</p>
</dd>
<dt><code>WklyWage</code></dt><dd><p>Usual amount of weekly wages (in 1976 U.S. dollars)</p>
</dd>	
<dt><code>y</code></dt><dd><p>An artificial variable generated to follow a model with a common mean. Persons in the same cluster are correlated.  Persons in different clusters are uncorrelated under the model.
</p>
</dd>	
</dl>



<h3>Details</h3>

<p>This population is a clustered population of 478 persons extracted from the
September 1976 Current Population Survey (CPS) in the United States.
The clusters are compact geographic areas used as one of the stages of sampling in the
CPS and are typically composed of about 4 nearby households.
The elements within clusters for this illustrative population are individual persons.
</p>


<h3>Source</h3>

<p>Current Population Survey, <code>https://www.census.gov/programs-surveys/cps.html</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(labor)
str(labor)
table(labor$h)
hist(labor$WklyWage)
</code></pre>

<hr>
<h2 id='MDarea.popA'>
Maryland area population
</h2><span id='topic+MDarea.popA'></span>

<h3>Description</h3>

<p>An artificial population of census tracts, block groups, and persons
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(MDarea.popA)</code></pre>


<h3>Format</h3>

<p>A data frame with 343,398 persons on the following variables:
</p>

<dl>
<dt><code>PSU</code></dt><dd><p>A grouping of block groups (<code>BLKGROUP</code>) which has about 4290 persons</p>
</dd>
<dt><code>SSU</code></dt><dd><p>A grouping of block groups which has about 1010 persons</p>
</dd>
<dt><code>TRACT</code></dt><dd><p>A geographic area defined by the Census Bureau.  Tracts generally have between 1,500 and 8,000 people but have a much wider range in Anne Arundel county.</p>
</dd>
<dt><code>BLKGROUP</code></dt><dd><p>Block group. A geographic area defined by the Census Bureau.  Block groups generally have between 600 and 3,000 people.</p>
</dd>
<dt><code>Hispanic</code></dt><dd><p>Hispanic ethnicity (1=Hispanic; 2=Non-Hispanic)</p>
</dd>
<dt><code>Gender</code></dt><dd><p> Gender (1=Male; 2=Female)</p>
</dd>
<dt><code>Age</code></dt><dd><p>23 level age category
(1 = Under 5 years;
2 = 5 to 9 years;
3 = 10 to 14 years;
4 = 15 to 17 years;
5 = 18 and 19 years;
6 = 20 years;
7 = 21 years;
8 = 22 to 24 years;
9 = 25 to 29 years;
10 = 30 to 34 years;
11 = 35 to 39 years;
12 = 40 to 44 years;
13 = 45 to 49 years;
14 = 50 to 54 years;
15 = 55 to 59 years;
16 = 60 and 61 years;
17 = 62 to 64 years;
18 = 65 and 66 years;
19 = 67 to 69 years;
20 = 70 to 74 years;
21 = 75 to 79 years;
22 = 80 to 84 years;
23 = 85 years and over)
</p>
</dd>
<dt><code>person</code></dt><dd><p>Counter for person within tract/block group/Hispanic/Gender/Age combination</p>
</dd>
<dt><code>y1</code></dt><dd><p>Artificial continuous variable</p>
</dd>
<dt><code>y2</code></dt><dd><p>Artificial continuous variable</p>
</dd>
<dt><code>y3</code></dt><dd><p>Artificial continuous variable</p>
</dd>
<dt><code>ins.cov</code></dt><dd><p>Medical coverage
(0 = person does not have medical insurance coverage;
1 = person has medical insurance coverage)
</p>
</dd>
<dt><code>hosp.stay</code></dt><dd><p>Overnight hospital stay
(0 = person did not have an overnight hospital stay in last 12 months;
1 = person did have an overnight hospital stay in last 12 months)
</p>
</dd>
</dl>



<h3>Details</h3>

<p>A dataset of 343,398 persons based on the 2000 decennial U.S. Census for Anne Arundel County
in the US state of Maryland. Person records were generated based on counts from the 2000 census.
Individual values for each person were generated using models.
Groupings to form the variables <code>PSU</code> and <code>SSU</code> were done after sorting the census file by tract and block group within tract.
</p>
<p>Note that <code>MDarea.popA</code> is different from the dataset, <code>MDarea.pop</code>, that is used in the book by Valliant, Dever, and Kreuter (2018).  <code>MDarea.pop</code> is larger with 403,997 persons. <code>MDarea.popA</code> was created by taking an equal probability, systematic subsample from <code>MDarea.pop</code>. <code>MDarea.popA</code> does have the same numbers of <code>TRACT</code>s, <code>PSU</code>s, and <code>SSU</code>s as <code>MDarea.pop</code>. The smaller data set was created to meet the CRAN size limit on installed packages. The full population, <code>MDarea.pop</code>, can be downloaded from <code>https://umd.app.box.com/v/PracTools2ndEdition</code>.
</p>


<h3>Source</h3>

<p>2000 U.S. decennial census, <code>http://www.census.gov/main/www/cen2000.html</code>
</p>


<h3>References</h3>

<p>Valliant, R., Dever, J., Kreuter, F. (2018). <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(MDarea.popA)
str(MDarea.popA)
table(MDarea.popA$TRACT)
table(MDarea.popA$TRACT, MDarea.popA$Hispanic)
</code></pre>

<hr>
<h2 id='mibrfss'>
Michigan Behavioral Risk Factor Surveillance Survey
</h2><span id='topic+mibrfss'></span>

<h3>Description</h3>

<p>Demographic and health related variables from a U.S. household survey in the state of Michigan
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(mibrfss)</code></pre>


<h3>Format</h3>

<p>A data frame with 2485 observations on persons for the following 21 variables.
</p>

<dl>
<dt><code>SMOKE100</code></dt><dd><p>Smoked 100 or more cigarettes in lifetime
(1 = Yes;
2 = No)
</p>
</dd>
<dt><code>BMICAT3</code></dt><dd><p>Body mass index category
(1 = Neither overweight nor obese (BMI &lt; 25);
2 = Overweight (25 &lt;= BMI &lt;= 30);
3 = Obese (BMI &gt; 30) )
</p>
</dd>
<dt><code>AGECAT</code></dt><dd><p>Age group
(1 = 18-24 years;
2 = 25-34 years;
3 = 35-44 years;
4 = 45-54 years;
5 = 55-64 years;
6 = 65+)
</p>
</dd>
<dt><code>GENHLTH</code></dt><dd><p>General health (self-reported)
(1 = Excellent;
2 = Very good;
3 = Good;
4 = Fair;
5 = Poor)
</p>
</dd>
<dt><code>PHYSACT</code></dt><dd><p>Physical activity: In last month participated in activities such as running, calisthenics, golf, gardening, or walking for exercise
(1 = Yes;
2 = No)
</p>
</dd>
<dt><code>HIGHBP</code></dt><dd><p>High blood pressure: Have you ever been told by a doctor, nurse, or other health professional that you have high blood pressure?
(1 = Yes;
2 = No)
</p>
</dd>
<dt><code>ASTHMA</code></dt><dd><p>Asthma: Have you ever been told by a doctor, nurse, or other health professional that you have asthma?
(1 = Yes;
2 = No)
</p>
</dd>
<dt><code>HISPANIC</code></dt><dd><p>Hispanic ethnicity
(1 = Yes;
2 = No;
7 = Missing)
</p>
</dd>
<dt><code>WEIGHT</code></dt><dd><p>Body weight in pounds
</p>
</dd>
<dt><code>GENDER</code></dt><dd><p>Gender
(1 = Male;
2 = Female)
</p>
</dd>
<dt><code>CELLPHON</code></dt><dd><p>Has a wireless phone
(1 = Yes;
2 = No)
</p>
</dd>
<dt><code>INETHOME</code></dt><dd><p>Has access to the Internet at home
(1 = Yes;
2 = No)
</p>
</dd>
<dt><code>WEBUSE</code></dt><dd><p>How often do you use the Internet at home? Would you say, at least once a day, five to six times a week, two to four times a week, about once a week, less than once a week, or have you not used the Internet in the last month?
(1	= At least once a day;
2 = 5-6 times a week;
3	= 2-4 times a week;
4	= About once a week;
5	= Less than once a week;
6	= Not in the last month)
</p>
</dd>
<dt><code>RACECAT</code></dt><dd><p>Race
(1 = White;
2 = African American;
3 = Other)
</p>
</dd>
<dt><code>EDCAT</code></dt><dd><p>Education level
(1 = Did not graduate high school;
2 = Graduated high school;
3 = Attended college or technical school;
4 = Graduated from college or technical school)
</p>
</dd>
<dt><code>INCOMC3</code></dt><dd><p>Income category
(1 = Less than $15000;
2 = $15000 to less than $25000;
3 = $25000 to less than $35000;
4 = $35000 to less than $50000;
5 = $50000 or more)
</p>
</dd>
<dt><code>DIABETE2</code></dt><dd><p>Diabetes: Have you ever been told by a doctor, nurse, or other health professional that you have diabetes?
(1 = Yes;
2 = No)
</p>
</dd>
<dt><code>CHOLCHK</code></dt><dd><p>Cholesterol check: Blood cholesterol is a fatty substance found in the blood. Have you ever had your blood cholesterol checked?
(1 = Yes;
2 = No)
</p>
</dd>
<dt><code>BMI</code></dt><dd><p>Body mass index (continuous)
</p>
</dd>
<dt><code>BINGE2</code></dt><dd><p>Binge drinking: At risk for binge drinking based on alcohol consumption responses
(1 = Yes;
2 = No)
</p>
</dd>
<dt><code>ARTHRIT</code></dt><dd><p>Arthritis: Have you ever been told by a doctor, nurse, or other health professional that you have some form of arthritis, rheumatoid arthritis, gout, lupus, or fibromyalgia, or have joint symptoms of arthritis?
(1 = Yes;
2 = No;
3 = Don't know, not sure, or refused)
</p>
</dd>
</dl>



<h3>Details</h3>

<p>The Michigan Behavioral Risk Factor Surveillance Survey (MIBRFSS) is part of a national state-by-state system of surveys used to monitor health conditions in the U.S. Data are collected through telephone household interviews. Demographic variables and a few health related variables are included in this subset. The <code>mibrfss</code> data set contains observations on 2845 persons and is  extracted from the 2003 U.S. survey. The file contains only persons 18 years and older.
</p>


<h3>Source</h3>

<p>Michigan Behavioral Risk Factor Surveillance Survey of 2003 sponsored by the U.S. Center for Disease Control.
<a href="https://www.cdc.gov/brfss/">https://www.cdc.gov/brfss/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nhis">nhis</a></code>, <code><a href="#topic+nhis.large">nhis.large</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(mibrfss)
str(mibrfss)
table(mibrfss$SMOKE100, useNA = "always")
table(mibrfss$BMICAT3, useNA="always")</code></pre>

<hr>
<h2 id='nAuditAttr'>
Sample sizes for an attribute sample in an audit
</h2><span id='topic+nAuditAttr'></span>

<h3>Description</h3>

<p>Compute a sample size for an audit where the goal is to control the probability of observing only a small number of errors given an underlying error rate in the population. Auditors refer to this as an attribute sample.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nAuditAttr(TolRate=0.05, AccDev, CL, N=5000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nAuditAttr_+3A_tolrate">TolRate</code></td>
<td>

<p>Proportion of units in the population with an attribute, e.g., errors in an audit. Auditors term this the 'tolerable rate of deviation' in the population to be tested.
</p>
</td></tr>
<tr><td><code id="nAuditAttr_+3A_accdev">AccDev</code></td>
<td>

<p>Acceptable deviation, which is the number of units with the attribute (i.e., the number of errors) that would be acceptable in the sample. The largest proportion of errors that would be deemed to be acceptable in an audit would be <code>AccDev/N</code>.
</p>
</td></tr>
<tr><td><code id="nAuditAttr_+3A_cl">CL</code></td>
<td>

<p>Probability that the sample will contain an acceptable number of errors. Auditors refer to this as 'confidence level'. The probability that the sample will contain <code>AccDev</code> errors or fewer is <code>1-CL</code>.
</p>
</td></tr>
<tr><td><code id="nAuditAttr_+3A_n">N</code></td>
<td>

<p>Size of the population of records to be audited.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>nAuditAttr</code> computes the minimum sample size required so that the probability, <code>1-CL</code> of detecting less than or equal to a specified number of errors in the sample, is controlled. The sample is assumed to be selected with equal probabilities. <code>AccDev</code> is the largest number of errors in the sample that will be considered as meeting the audit standards. <code>TolRate</code> is the underlying population error rate, which is typically set to be larger than <code>AccDev/N</code>. The sample size is computed in two ways: (1) using the hypergeometric distribution, which accounts for the size of the population and (2) with the binomial distribution, which will be appropriate if the population is very large. When <code>N</code> is large and the sampling fraction is small, both sample sizes will be approximately the same.
</p>


<h3>Value</h3>

<p>List object with values:
</p>
<table>
<tr><td><code>Pop.Size</code></td>
<td>
<p>population size</p>
</td></tr>
<tr><td><code>Tol.Dev.Rate</code></td>
<td>
<p>proportion of records with errors in population</p>
</td></tr>
<tr><td><code>Acceptable.Errors</code></td>
<td>
<p>largest number of errors, found in the sample, that will meet audit standards</p>
</td></tr>
<tr><td><code>Sample.Size.Hypergeometric</code></td>
<td>
<p>minimum sample size needed to detect <code>AccDev</code> errors in the sample computed via the hypergeometric distribution</p>
</td></tr>
<tr><td><code>Sample.Size.Binomial</code></td>
<td>
<p>minimum sample size needed to detect <code>AccDev</code> errors in the sample computed via the binomial distribution</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>George Zipf, Richard Valliant
</p>


<h3>References</h3>

<p>GAO (2020). Financial Audit Manual, Volume 1, section 450.08. Washington DC; <a href="https://www.gao.gov/assets/gao-18-601g.pdf">https://www.gao.gov/assets/gao-18-601g.pdf</a>
</p>
<p>Stewart, Trevor R. (2012). <em>Technical Notes on the AICPA Audit Guide: Audit Sampling</em>. American Institute of Certified Public Accountants, Inc. New York, NY 10036-8775; <a href="https://us.aicpa.org/content/dam/aicpa/publications/accountingauditing/keytopics/downloadabledocuments/sampling_guide_technical_notes.pdf">https://us.aicpa.org/content/dam/aicpa/publications/accountingauditing/keytopics/downloadabledocuments/sampling_guide_technical_notes.pdf</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Examples from the US GAO Financial Audit Manual (2020), Figure 450.1, Table 1
nAuditAttr(AccDev = 0, CL = .90)
nAuditAttr(AccDev = 1, CL = .90)
nAuditAttr(AccDev = 2, CL = .90)
nAuditAttr(AccDev = 3, CL = .90)
nAuditAttr(AccDev = 4, CL = .90)
</code></pre>

<hr>
<h2 id='nAuditMUS'>
Sample sizes for a Monetary Unit Sample in an audit
</h2><span id='topic+nAuditMUS'></span>

<h3>Description</h3>

<p>Compute a sample size for an audit where the goal is to control the probability of observing only a small number of errors given an underlying error rate in the population. The sample will be selected with probabilities proportional to a measure of size (MOS). When the MOS of each record is a monetary unit, auditors refer to this as an monetary unit sampling or dollar unit sampling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nAuditMUS(MUSVar, Value.sw, CL = 0.90, Error.sw, Tol.Error, Exp.Error = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nAuditMUS_+3A_musvar">MUSVar</code></td>
<td>

<p>The measure of size for monetary unit sampling (MUS)
</p>
</td></tr>
<tr><td><code id="nAuditMUS_+3A_value.sw">Value.sw</code></td>
<td>

<p>Determines whether the monetary unit sample is based on positive values, negative values, or absolute values. If <code>Value.sw</code> = &quot;Positive&quot; or &quot;Pos&quot;, only positive values of <code>MUSVar</code> are used; if &quot;Negative&quot; or &quot;Neg&quot; only negative values are used; if &quot;Absolute&quot; or &quot;Abs&quot;, all values of <code>MUSVar</code> are used after taking their absolute values.
</p>
</td></tr>
<tr><td><code id="nAuditMUS_+3A_cl">CL</code></td>
<td>

<p>Probability that the sample will contain an acceptable number of errors. Auditors refer to this as 'confidence level'. The probability that the sample will contain the tolerable number of  errors or fewer is <code>1-CL</code>. The range of <code>CL</code> is 0 to 1.
</p>
</td></tr>
<tr><td><code id="nAuditMUS_+3A_error.sw">Error.sw</code></td>
<td>

<p>Determines whether errors are based on monetary amounts or percentages, i.e., whether <code>Tol.error</code> is interpreted as a dollar amount (<code>Error.sw</code> = &quot;Absolute&quot; or &quot;Amt&quot;) or as a percent (<code>Error.sw</code> = &quot;Percent&quot; or &quot;Pct&quot;).
</p>
</td></tr>
<tr><td><code id="nAuditMUS_+3A_tol.error">Tol.Error</code></td>
<td>

<p>The amount of error expressed as a value or a percentage that the auditor considers tolerable. If <code>Error.sw</code> is &quot;Percent&quot; or &quot;Pct&quot;, <code>Tol.Error</code> is a percent between 0 and 100. If <code>Error.sw</code> = &quot;Absolute&quot; or &quot;Amt&quot;, <code>Tol.Error</code> is interpreted as a dollar amount.
</p>
</td></tr>
<tr><td><code id="nAuditMUS_+3A_exp.error">Exp.Error</code></td>
<td>

<p>The amount of error, expressed as a value or a percentage, that the auditor expects in the population. If <code>Error.sw</code> is &quot;Percent&quot; or &quot;Pct&quot;, <code>Exp.Error</code> is a percent between 0 and 100. If <code>Error.sw</code> = &quot;Absolute&quot; or &quot;Amt&quot;, <code>Exp.Error</code> is interpreted as a dollar amount.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>nAuditMUS</code> computes the minimum sample size required for a given population, tolerable error rate or misstatement, and desired confidence level. If the expected error or misstatement is 0, (i.e. <code>Exp.Error</code> = 0), then the sample size is computed using the hypergeometric distribution where the acceptable number of deviations is 0. If the expected error is greater than 0, then sample size is computed by first calculating the maximum sample size where the number of deviations divided by the sample size is less than the expected error, then calculating the minimum sample size where the number of deviations divided by the sample size is greater than the expected error, and finally performing a straight line interpolation between these two values where the interpolated value is the specified expected error. The returned sample size calculation is the ceiling of that interpolated sample size.
</p>


<h3>Value</h3>

<p>List object with values:
</p>
<table>
<tr><td><code>Value.Range</code></td>
<td>
<p>Whether the MUS variable is for positive, negative, or absolute values as defined by <code>Value.sw</code></p>
</td></tr>
<tr><td><code>Error.Type</code></td>
<td>
<p>Amount or Percent as defined by <code>Error.sw</code></p>
</td></tr>
<tr><td><code>Tol.Error.Rate</code></td>
<td>
<p>The tolerable error expressed as a percentage of items if <code>Error.sw</code> = &quot;Percent&quot; or &quot;Pct&quot; or as a percentage of total monetary value otherwise</p>
</td></tr>
<tr><td><code>Exp.Error.Rate</code></td>
<td>
<p>The expected error expressed as a percentage of items if <code>Error.sw</code> = &quot;Percent&quot; or &quot;Pct&quot; or as a percentage of total monetary value otherwise</p>
</td></tr>
<tr><td><code>Number.Records</code></td>
<td>
<p>The population count of records in the value range based on selecting ones with positive, negative or absolute value of <code>MUSVar</code></p>
</td></tr>
<tr><td><code>Sample.Size</code></td>
<td>
<p>Minimum sample size needed to meet tolerable and expected error rate requirements</p>
</td></tr>
<tr><td><code>Number.HighVal</code></td>
<td>
<p>Number of records that are high value (exceed the interval used for systematic sampling) and will be certainties in the sample</p>
</td></tr>
<tr><td><code>Positive.Pop.Dollars</code></td>
<td>
<p>The absolute value of the total dollar (or other monetary unit) amount in the population in the value range</p>
</td></tr>
<tr><td><code>Conf.level</code></td>
<td>
<p>Probability that the sample will meet MUS requirements</p>
</td></tr>
<tr><td><code>Sampling.Interval</code></td>
<td>
<p>Spacing or skip interval that would be used in a systematic probability proportional to monetary unit sampling</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>George Zipf, Richard Valliant
</p>


<h3>References</h3>

<p>GAO (2020). Financial Audit Manual, Volume 1, section 480.21-480.26. Washington DC; <a href="https://www.gao.gov/assets/gao-18-601g.pdf">https://www.gao.gov/assets/gao-18-601g.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nAuditAttr">nAuditAttr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>    # generate an artificial population with some negative monetary amounts
EX &lt;- 1000
relvar &lt;- 2
alpha &lt;- 1/relvar
sigma &lt;- EX * relvar
lowval &lt;- 100                   # minimum positive X's allowed
prop.neg &lt;- 0.05                # proportion of pop with negative values
N.neg &lt;- floor(1000 * prop.neg)    # number of negative X's allowed
X &lt;- rgamma(n=1000, shape=alpha, scale=sigma)
Xlow &lt;- sort(X)[1:N.neg]
xneg &lt;- -Xlow - lowval
xpos &lt;- X[N.neg:length(X)]
X &lt;- c(xneg, xpos)

nAuditMUS(X, Value.sw = "Pos", Error.sw = "Amount", Tol.Error = 180000, Exp.Error = 10000)
nAuditMUS(X, Value.sw = "Pos", Error.sw = "Pct", Tol.Error = 18, Exp.Error = 3)
nAuditMUS(X, Value.sw = "Abs", Error.sw = "Amount", Tol.Error = 180000, Exp.Error = 10000)
</code></pre>

<hr>
<h2 id='nCont'>
Compute a simple random sample size for an estimated mean
</h2><span id='topic+nCont'></span>

<h3>Description</h3>

<p>Compute a simple random sample size using either a target coefficient of variation, <code class="reqn">CV_0</code>, or target variance, <code class="reqn">V_0</code>, for an estimated mean.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nCont(CV0=NULL, V0=NULL, S2=NULL, ybarU=NULL, N=Inf, CVpop=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nCont_+3A_cv0">CV0</code></td>
<td>

<p>target value of coefficient of variation of <code class="reqn">\bar{y_s}</code>
</p>
</td></tr>
<tr><td><code id="nCont_+3A_v0">V0</code></td>
<td>

<p>target value of variance of <code class="reqn">\bar{y_s}</code>
</p>
</td></tr>
<tr><td><code id="nCont_+3A_s2">S2</code></td>
<td>

<p>unit (population) variance
</p>
</td></tr>
<tr><td><code id="nCont_+3A_ybaru">ybarU</code></td>
<td>

<p>population mean of target variable
</p>
</td></tr>
<tr><td><code id="nCont_+3A_n">N</code></td>
<td>

<p>number of units in finite population
</p>
</td></tr>
<tr><td><code id="nCont_+3A_cvpop">CVpop</code></td>
<td>

<p>unit (population) coefficient of variation
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code class="reqn">CV_0</code> is the desired target, then the unit CV,  <code>CVpop</code>,
or the population mean and variance, <code>ybarU</code> and <code>S2</code>, must also be provided.
If <code class="reqn">V_0</code> is the constrained value, then <code>S2</code> must be also be included in the function call.
</p>


<h3>Value</h3>

<p>numeric sample size
</p>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Valliant, R., Dever, J., Kreuter, F. (2018, chap. 3). <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nLogOdds">nLogOdds</a></code>, <code><a href="#topic+nProp">nProp</a></code>, <code><a href="#topic+nPropMoe">nPropMoe</a></code>, <code><a href="#topic+nWilson">nWilson</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>nCont(CV0=0.05, CVpop=2)
nCont(CV0=0.05, CVpop=2, N=500)
nCont(CV0=0.10/1.645, CVpop=1)

    # Compute sample size for a ratio estimator in smho98 population
    # Identify large units to select with certainty first
data(smho98)
cert &lt;- smho98[,"BEDS"] &gt; 2000
tmp &lt;- smho98[!cert, ]
tmp &lt;- tmp[tmp[, "BEDS"] &gt; 0, ]

x &lt;- tmp[,"BEDS"]
y &lt;- tmp[, "EXPTOTAL"]
m &lt;- lm(y ~ 0 + x, weights = 1/x)
ybarU &lt;- mean(y)
S2R &lt;- sum(m$residuals^2/(length(x)-1))
nCont(CV0=0.15, S2=S2R, ybarU=ybarU)
</code></pre>

<hr>
<h2 id='nContMoe'>
Compute a simple random sample size for an estimated mean of a continuous variable based on margin of error
</h2><span id='topic+nContMoe'></span>

<h3>Description</h3>

<p>Compute a simple random sample size using a margin of error specified as the half-width of a normal approximation confidence interval or the half-width relative to the population mean.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nContMoe(moe.sw, e, alpha=0.05, CVpop=NULL, S2=NULL, ybarU=NULL, N=Inf)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nContMoe_+3A_moe.sw">moe.sw</code></td>
<td>

<p>switch for setting desired margin of error (1 = CI half-width on the mean;
2 = CI half-width on the mean divided by <code class="reqn">\bar{y}_U</code>)
</p>
</td></tr>
<tr><td><code id="nContMoe_+3A_e">e</code></td>
<td>

<p>desired margin of error; either <code class="reqn">e=z_{1-\alpha/2}\sqrt{V(\bar{y}_s)}</code> or
<code class="reqn">e=z_{1-\alpha/2}CV(\bar{y}_s)</code>
</p>
</td></tr>
<tr><td><code id="nContMoe_+3A_alpha">alpha</code></td>
<td>
<p>1 - (confidence level)</p>
</td></tr>
<tr><td><code id="nContMoe_+3A_cvpop">CVpop</code></td>
<td>
<p>unit (population) coefficient of variation</p>
</td></tr>
<tr><td><code id="nContMoe_+3A_s2">S2</code></td>
<td>
<p>population variance of the target variable</p>
</td></tr>
<tr><td><code id="nContMoe_+3A_ybaru">ybarU</code></td>
<td>
<p>population mean of target variable</p>
</td></tr>
<tr><td><code id="nContMoe_+3A_n">N</code></td>
<td>
<p>number of units in finite population</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>moe.sw</code>=1, then <code>S2</code> must be provided.  If <code>moe.sw</code>=2, then either (i) <code>CVpop</code> or (ii) <code>S2</code> and <code>ybarU</code> must be provided.
</p>


<h3>Value</h3>

<p>numeric sample size
</p>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Valliant, R., Dever, J., Kreuter, F. (2018, chap. 3). <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nCont">nCont</a></code>, <code><a href="#topic+nLogOdds">nLogOdds</a></code>, <code><a href="#topic+nProp">nProp</a></code>, <code><a href="#topic+nPropMoe">nPropMoe</a></code>, <code><a href="#topic+nWilson">nWilson</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>nContMoe(moe.sw=1, e=0.05, alpha=0.05, S2=2)
nContMoe(moe.sw=1, e=0.05, alpha=0.05, S2=2, N=200)
nContMoe(moe.sw=2, e=0.05, alpha=0.05, CVpop=2)
nContMoe(moe.sw=2, e=0.05, alpha=0.05, CVpop=2, N=200)
nContMoe(moe.sw=2, e=0.05, alpha=0.05, S2=4, ybarU=2)
</code></pre>

<hr>
<h2 id='nContOpt'>      
Compute the sample size required to estimate the mean of a continuous variable by optimizing the numbers of take-alls and non-take-all units selected by probability sampling
</h2><span id='topic+nContOpt'></span>

<h3>Description</h3>

<p>Compute a sample size required to achieve a precision target for an estimated mean, <code class="reqn">\hat{\bar{y}}_s</code>, based on splitting the sample between take-alls and non-take-alls. The sample design for non-take-alls can be either simple random sampling or probability proportional to size sampling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nContOpt(X, Y = NULL, CV0 = NULL, V0 = NULL, design = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nContOpt_+3A_x">X</code></td>
<td>

<p>population variable used for determining take-all cutoff and for selecting a probability proportional to size sample if <code>design = "PPS"</code>. <code>X</code> is a vector that contains a value for every unit in the population.
</p>
</td></tr>
<tr><td><code id="nContOpt_+3A_y">Y</code></td>
<td>

<p>variable used for computing a population variance; required if <code>design = "PPS"</code>. <code>Y</code> is ignored if  <code>design = "SRS"</code>. <code>X</code> is a vector that contains a value for every unit in the population and is the same length as <code>Y</code>.
</p>
</td></tr>  
<tr><td><code id="nContOpt_+3A_cv0">CV0</code></td>
<td>

<p>target value of coefficient of variation of <code class="reqn">\hat{\bar{y}}_s</code>
</p>
</td></tr>
<tr><td><code id="nContOpt_+3A_v0">V0</code></td>
<td>

<p>target value of variance of <code class="reqn">\hat{\bar{y}}_s</code>
</p>
</td></tr>
<tr><td><code id="nContOpt_+3A_design">design</code></td>
<td>

<p>Sample design to be used for non-take-alls; must be either <code>"SRS"</code> or <code>"PPS"</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Compute a sample size based on splitting the sample between take-alls and non-take-alls in a way that achieves either a target coefficient of variation or a target variance for an estimated mean. The function sorts the file in descending order by <code>X</code> and then systematically designates units as  take-alls (certainty selections) starting from largest to smallest, and computes the sample size of non-take-alls needed to achieve the precision target. Initially, no unit in the ordered list is a certainty, and if <code>design = "SRS"</code>, the first value in <code>nContOpt.curve</code> is the same as <code>nCont</code> produces under identical inputs. In each pass, the algorithm increases the number of certainties. In the second pass, the first value is taken as a certainty and the non-take-all sample size is based on units 2:N, where N is the population size. On the third pass, the first two values are taken as certainties and the non-take-all sample size is based on units 3:N. The function cycles through units 1:(N-1) with take-alls increasing by 1 each cycle, and determines the minimum total sample size needed to achieve the specified precision target. The optimum sample size <code>nContOpt.n</code> combines certainties and non-certainties for its value.
</p>
<p>The sample design can be either simple random sampling or probability proportional to size sampling. When <code>design = "SRS"</code>, calculations are based only on <code>X</code>. The SRS variance formula is for without replacement sampling so that a finite population correction factor (<em>fpc</em>) is included. When <code>design = "PPS"</code>, <code>X</code> is used for the measure of size and <code>Y</code> is the variable for computing the variance used to determine the sample size. The PPS variance is computed for a with-replacement design, but an ad hoc <em>fpc</em> is included. Either <code>CV0</code> or <code>V0</code> must be provided but not both.
</p>


<h3>Value</h3>

<p>A list with five components:
</p>
<table>
<tr><td><code>nContOpt.Curve</code></td>
<td>
<p>The sample size for the given inputs based on the number of take-alls incrementing from 1 to N-1</p>
</td></tr>
<tr><td><code>Take.alls</code></td>
<td>
<p>A TRUE/FALSE vector for whether the element in the <code>X</code> vector is a take-all</p>
</td></tr>
<tr><td><code>nContOpt.n</code></td>
<td>
<p>The minimum sample size (take-alls + non-take-alls) required for the given inputs, rounded to 4 decimal places</p>
</td></tr>
<tr><td><code>Min.Takeall.Val</code></td>
<td>
<p>The minimum value of <code>X</code> for the take-alls</p>
</td></tr>
<tr><td><code>n.Take.all</code></td>
<td>
<p>The number of take-all units in the optimal sample</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>George Zipf, Richard Valliant</p>


<h3>See Also</h3>

<p><code><a href="#topic+nCont">nCont</a></code>, <code><a href="#topic+nContMoe">nContMoe</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>nContOpt(X = TPV$Total.Pot.Value, CV0 = 0.05, design = "SRS")
nContOpt(X = TPV$Total.Pot.Value, V0  = 5e+14, design = "SRS")
g &lt;- nContOpt(X = TPV$Total.Pot.Value, CV0 = 0.05, design = "SRS")
plot(g$nContOpt.Curve,
     type = "o",
     main = "Sample Size Curve",
     xlab = "Take-all / Sample Split Starting Value",
     ylab = "Total sample size (take-alls + non-tale-alls)" )
nContOpt(X = TPV$Total.Pot.Value, Y = TPV$Y, CV0 = 0.05, design = "PPS")
</code></pre>

<hr>
<h2 id='nDep2sam'>
Simple random sample size for difference in means
</h2><span id='topic+nDep2sam'></span>

<h3>Description</h3>

<p>Compute a simple random sample size for estimating the difference in means when samples overlap
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nDep2sam(S2x, S2y, g, r, rho, alt, del, sig.level=0.05, pow=0.80)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nDep2sam_+3A_s2x">S2x</code></td>
<td>

<p>unit variance of analysis variable <em>x</em> in sample 1
</p>
</td></tr>
<tr><td><code id="nDep2sam_+3A_s2y">S2y</code></td>
<td>

<p>unit variance of analysis variable <em>y</em> in sample 2
</p>
</td></tr>
<tr><td><code id="nDep2sam_+3A_g">g</code></td>
<td>

<p>proportion of sample 1 that is in the overlap with sample 2
</p>
</td></tr>
<tr><td><code id="nDep2sam_+3A_r">r</code></td>
<td>

<p>ratio of the size of sample 1 to that of sample 2
</p>
</td></tr>
<tr><td><code id="nDep2sam_+3A_rho">rho</code></td>
<td>

<p>unit-level correlation between <em>x</em> and <em>y</em>
</p>
</td></tr>
<tr><td><code id="nDep2sam_+3A_alt">alt</code></td>
<td>

<p>should the test be 1-sided or 2-sided; allowable values are <code>alt="one.sided"</code> or  <code>alt="two.sided"</code>.
</p>
</td></tr>
<tr><td><code id="nDep2sam_+3A_del">del</code></td>
<td>

<p>size of the difference between the means to be detected
</p>
</td></tr>
<tr><td><code id="nDep2sam_+3A_sig.level">sig.level</code></td>
<td>

<p>significance level of the hypothesis test
</p>
</td></tr>
<tr><td><code id="nDep2sam_+3A_pow">pow</code></td>
<td>

<p>desired power of the test
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>nDep2sam</code> computes sample sizes in two groups that are required for testing whether the difference in group means is significant.  The power of the test is one of the input parameters. The samples have a specified proportion of units in common. Both samples are assumed to be selected via simple random sampling.
</p>


<h3>Value</h3>

<p>List with values:
</p>
<table>
<tr><td><code>n1</code></td>
<td>
<p>sample size in group 1</p>
</td></tr>
<tr><td><code>n2</code></td>
<td>
<p>sample size in group 2</p>
</td></tr>
<tr><td><code>S2x.S2y</code></td>
<td>
<p>unit variances in groups 1 and 2</p>
</td></tr>
<tr><td><code>delta</code></td>
<td>
<p>difference in group means to be detected</p>
</td></tr>
<tr><td><code>gamma</code></td>
<td>
<p>proportion of sample 1 that is in the overlap with sample 2</p>
</td></tr>
<tr><td><code>r</code></td>
<td>
<p>ratio of the size of sample 1 to that of sample 2</p>
</td></tr>
<tr><td><code>rho</code></td>
<td>
<p>unit-level correlation between analysis variables in groups 1 and 2</p>
</td></tr>
<tr><td><code>alt</code></td>
<td>
<p>type of test: one-sided or two-sided</p>
</td></tr>
<tr><td><code>sig.level</code></td>
<td>
<p>significance level of test</p>
</td></tr>
<tr><td><code>power</code></td>
<td>
<p>power of the test</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Valliant, R., Dever, J., Kreuter, F. (2018, chap. 4).  <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>
<p>Woodward, M. (1992). Formulas for Sample Size, Power, and Minimum Detectable Relative Risk in Medical Studies.  <em>The Statistician</em>, 41, 185-196.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nProp2sam">nProp2sam</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>nDep2sam(S2x=200, S2y=200,
            g=0.75, r=1, rho=0.9,
            alt="one.sided", del=5,
            sig.level=0.05, pow=0.80)
</code></pre>

<hr>
<h2 id='nDomain'>
Compute a simple random sample size for an estimated mean or total for a domain
</h2><span id='topic+nDomain'></span>

<h3>Description</h3>

<p>Compute a simple random sample size using either a target coefficient of variation, <code class="reqn">CV_{0}(d)</code>, or target variance, <code class="reqn">V_{0}(d)</code>, for an estimated mean or total for a domain.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nDomain(CV0d=NULL, V0d=NULL, S2d=NULL, ybarUd=NULL, N=Inf, CVpopd=NULL, Pd, est.type)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nDomain_+3A_cv0d">CV0d</code></td>
<td>

<p>target value of coefficient of variation of estimated domain mean or total
</p>
</td></tr>
<tr><td><code id="nDomain_+3A_v0d">V0d</code></td>
<td>

<p>target value of variance of estimated domain mean or total
</p>
</td></tr>
<tr><td><code id="nDomain_+3A_s2d">S2d</code></td>
<td>

<p>unit (population) variance for domain units
</p>
</td></tr>
<tr><td><code id="nDomain_+3A_ybarud">ybarUd</code></td>
<td>

<p>population mean of target variable for domain units
</p>
</td></tr>
<tr><td><code id="nDomain_+3A_n">N</code></td>
<td>

<p>number of units in full finite population (not just the domain population)
</p>
</td></tr>
<tr><td><code id="nDomain_+3A_cvpopd">CVpopd</code></td>
<td>

<p>unit (population) coefficient of variation for domain units
</p>
</td></tr>
<tr><td><code id="nDomain_+3A_pd">Pd</code></td>
<td>

<p>proportion of units in the population that are in the domain
</p>
</td></tr>
<tr><td><code id="nDomain_+3A_est.type">est.type</code></td>
<td>

<p>type of estimate; allowable values are <code>"mean"</code> or  <code>"total"</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>CV0d</code> is the desired target, then the unit CV, <code>CVpopd</code>,
or the domain population mean and variance, <code>ybarUd</code> and <code>S2d</code>, must also be provided.
If <code>V0d</code> is the constrained value, then <code>ybarUd</code> must be also be included in the function call. <code>CV0d</code>
will then be computed as <code>sqrt(V0d)/ybarUd</code>.
</p>


<h3>Value</h3>

<p>numeric sample size
</p>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Valliant, R., Dever, J., Kreuter, F. (2018, sec. 3.5.2). <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nCont">nCont</a></code>, <code><a href="#topic+nLogOdds">nLogOdds</a></code>, <code><a href="#topic+nProp">nProp</a></code>, <code><a href="#topic+nPropMoe">nPropMoe</a></code>, <code><a href="#topic+nWilson">nWilson</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>nDomain(CV0d=0.05, N=Inf, CVpopd=1, Pd=0.5, est.type="total")
nDomain(CV0d=0.05, N=Inf, CVpopd=1, Pd=0.5, est.type="mean")
nDomain(V0d=50, ybarUd=50, S2d=100, N=Inf, Pd=0.5, est.type="total")
nDomain(CV0d=0.05, ybarUd=50, S2d=100, N=Inf, Pd=0.5, est.type="total")
nDomain(CV0d=0.05, ybarUd=50, S2d=100, N=Inf, Pd=0.5, est.type="mean")
</code></pre>

<hr>
<h2 id='nEdge'>
Compute the total sample size for a stratified, simple random sample based on an Edgeworth approximation
</h2><span id='topic+nEdge'></span>

<h3>Description</h3>

<p>Compute the total stratified, simple random sample size for various allocations that is large enough to insure adequate coverage of a normal approximation confidence interval (CI) for a population mean.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nEdge(ci.lev, side, epsilon = 0.005, dat, pop.sw = TRUE, wts = NULL, hcol=NULL, ycol,
      alloc = NULL, Ch = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nEdge_+3A_ci.lev">ci.lev</code></td>
<td>

<p>desired confidence level for a 1- or 2-sided normal approximation confidence interval based on an estimated mean; must be in the interval (0,1)
</p>
</td></tr>
<tr><td><code id="nEdge_+3A_side">side</code></td>
<td>

<p>either <code>"two.sided"</code> or <code>"one.sided"</code> for type of confidence interval
</p>
</td></tr>
<tr><td><code id="nEdge_+3A_epsilon">epsilon</code></td>
<td>

<p>tolerance on coverage probability; the sample should be large enough that CI coverage is within <code class="reqn">\pm</code> <code>epsilon</code> of <code>ci.lev</code>; must be in the interval (0,1)
</p>
</td></tr>
<tr><td><code id="nEdge_+3A_dat">dat</code></td>
<td>

<p>either a population or sample data frame
</p>
</td></tr>
<tr><td><code id="nEdge_+3A_pop.sw">pop.sw</code></td>
<td>

<p>TRUE if <code>dat</code> is for a full population; FALSE if <code>dat</code> is for a sample
</p>
</td></tr>
<tr><td><code id="nEdge_+3A_wts">wts</code></td>
<td>

<p>vector of weights if <code>dat</code> is a sample; if <code>dat</code> is for a population, <code>wts = NULL</code>
</p>
</td></tr>
<tr><td><code id="nEdge_+3A_hcol">hcol</code></td>
<td>

<p>column of <code>dat</code> that contains the stratum ID; strata can be character or numeric
</p>
</td></tr>
<tr><td><code id="nEdge_+3A_ycol">ycol</code></td>
<td>

<p>column of <code>dat</code> that contains the analysis variable; must be numeric
</p>
</td></tr>
<tr><td><code id="nEdge_+3A_alloc">alloc</code></td>
<td>

<p>allocation to the strata; must be one of <code>prop</code>, <code>equal</code>, <code>neyman</code>, <code>totcost</code>, <code>totvar</code>, or <code>NULL</code>
</p>
</td></tr>
<tr><td><code id="nEdge_+3A_ch">Ch</code></td>
<td>

<p>vector of costs per unit in each stratum; these exclude fixed costs that do not vary with the sample size
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>nEdge</code> computes the total sample size needed in either a stratified or unstratified simple random sample so that the coverage probability of a confidence interval is within a specified tolerance (<code>epsilon</code>) of a nominal confidence level (<code>ci.lev</code>). The calculation assumes that there is a single estimated mean or total of the variable <code>ycol</code> that is of key importance in a sample. Confidence intervals for the finite population mean are usually computed using the normal approximation whose accuracy depends on the underlying structure of the analytic variable and the total sample size. In some applications, assuring that CIs have near nominal coverage is critical. For example, for some items on business tax returns the US Internal Revenue Service allows sample estimates to be used but sets precision standards based on the lower (or upper) limit of a 1-sided CI.
</p>
<p>Using an Edgeworth approximation to the distribution of the estimated overall mean in Qing &amp; Valliant (2024), <code>nEdge</code> computes the total sample size needed so that a CI will have coverage equal to the nominal value in <code>ci.lev</code> plus or minus the tolerance <code>epsilon</code>. The calculation assumes that the sampling fraction in each stratum is negligible. The total sample size returned by <code>nEdge</code> is based on the overall Edgeworth criterion; the resulting stratum sample sizes may not be large enough so that the normal approximation is adequate for each stratum estimator. When <code>dat</code> is a sample, the weights (<code>wts</code>) used in the estimator of the mean (or total) are assumed to be scaled for estimating population totals. They can be inverse selection probabilities, i.e. ones used in the <code class="reqn">\pi</code>-estimator, or weights that have been adjusted to account for nonresponse or coverage errors.
</p>
<p>The remainder term in the approximation used in <code>nEdge</code> is <code class="reqn">O(n^{-1/2})</code>. In contrast, the function <code>nEdgeSRS</code> uses a <code class="reqn">O(n^{-1})</code> approximation but applies only to simple random sampling.
</p>


<h3>Value</h3>

<p>List with values:
</p>
<table>
<tr><td><code>CI type</code></td>
<td>
<p>one-sided or two-sided</p>
</td></tr>
<tr><td><code>epsilon</code></td>
<td>
<p>tolerance on CI coverage</p>
</td></tr>
<tr><td><code>Total sample size</code></td>
<td>
<p>numeric sample size</p>
</td></tr>
<tr><td><code>allocation</code></td>
<td>
<p>type of allocation to strata or NULL if no strata are used</p>
</td></tr>
<tr><td><code>Stratum values</code></td>
<td>
<p>Data frame with columns for stratum, number of sample units allocated to each stratum (<code>nh</code>), proportion of sample allocated to each stratum (<code>ph</code>), and skewness in each stratum (<code>g1h</code>); if no strata are used, only <code>g1</code>, the overall skewness is returned</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Richard Valliant, Siyu Qing
</p>


<h3>References</h3>

<p>Qing, S. and Valliant, R. (2024). Extending Cochran's Sample Size Rule to Stratified Simple Random Sampling with Applications to Audit Sampling. <em>Journal of Official Statistics</em>, accepted.
</p>
<p>U.S. Internal Revenue Service (2011). 26 CFR 601.105: Examination of returns and claims for refund, credit or abatement: determination of correct tax liability. Washington DC. <a href="https://www.irs.gov/pub/irs-drop/rp-11-42.pdf">https://www.irs.gov/pub/irs-drop/rp-11-42.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nCont">nCont</a></code>, <code><a href="#topic+nEdgeSRS">nEdgeSRS</a></code>, <code><a href="#topic+nLogOdds">nLogOdds</a></code>, <code><a href="#topic+nProp">nProp</a></code>, <code><a href="#topic+nPropMoe">nPropMoe</a></code>, <code><a href="#topic+nWilson">nWilson</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(PracTools)
set.seed(1289129963)
pop &lt;- HMT(N=10000, H=5)
    # run for full population
nEdge(ci.lev=0.95, side="one.sided", dat=pop, pop.sw=TRUE, wts=NULL, hcol="strat", ycol="y",
       alloc="neyman")
    # run for a stratified sample
require(sampling)
sam &lt;- strata(data=pop, stratanames="strat", size=c(30, 40, 50, 60, 70), method=c("srswor"),
              description=TRUE)
samdat &lt;- pop[sam$ID_unit,]
w = 1/sam$Prob
nEdge(ci.lev=0.95, side="two.sided", epsilon=0.02, dat=samdat, pop.sw=FALSE, wts=w,
       hcol="strat", ycol="y", alloc="equal")
</code></pre>

<hr>
<h2 id='nEdgeSRS'>
Compute the total sample size for a simple random sample based on an Edgeworth approximation
</h2><span id='topic+nEdgeSRS'></span>

<h3>Description</h3>

<p>Compute the total simple random sample size that is large enough to insure adequate coverage of a normal approximation confidence interval (CI) for a population mean.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nEdgeSRS(ci.lev, side, epsilon = 0.005, dat, pop.sw = TRUE, wts = NULL, hcol=NULL, ycol)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nEdgeSRS_+3A_ci.lev">ci.lev</code></td>
<td>

<p>desired confidence level for a 1- or 2-sided normal approximation confidence interval based on an estimated mean; must be in the interval (0,1)
</p>
</td></tr>
<tr><td><code id="nEdgeSRS_+3A_side">side</code></td>
<td>

<p>either <code>"two.sided"</code> or <code>"one.sided"</code> for type of confidence interval
</p>
</td></tr>
<tr><td><code id="nEdgeSRS_+3A_epsilon">epsilon</code></td>
<td>

<p>tolerance on coverage probability; the sample should be large enough that CI coverage is within <code class="reqn">\pm</code> <code>epsilon</code> of <code>ci.lev</code>; must be in the interval (0,1)
</p>
</td></tr>
<tr><td><code id="nEdgeSRS_+3A_dat">dat</code></td>
<td>

<p>either a population or sample data frame
</p>
</td></tr>
<tr><td><code id="nEdgeSRS_+3A_pop.sw">pop.sw</code></td>
<td>

<p>TRUE if <code>dat</code> is for a full population; FALSE if <code>dat</code> is for a sample
</p>
</td></tr>
<tr><td><code id="nEdgeSRS_+3A_wts">wts</code></td>
<td>

<p>vector of weights if <code>dat</code> is a sample; if <code>dat</code> is for a population, <code>wts = NULL</code>
</p>
</td></tr>
<tr><td><code id="nEdgeSRS_+3A_hcol">hcol</code></td>
<td>

<p>column of <code>dat</code> that contains the stratum ID; strata can be character or numeric
</p>
</td></tr>
<tr><td><code id="nEdgeSRS_+3A_ycol">ycol</code></td>
<td>

<p>column of <code>dat</code> that contains the analysis variable; must be numeric
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>nEdgeSRS</code> computes the total sample size needed in a simple random sample so that the coverage probability of a confidence interval is within a specified tolerance (<code>epsilon</code>) of a nominal confidence level (<code>ci.lev</code>). Confidence intervals for the finite population mean are usually computed using the normal approximation whose accuracy depends on the sample size and the underlying structure of the analytic variable. In some applications, assuring that CIs have near nominal coverage is critical. For example, for some items on business tax returns the US Internal Revenue Service allows sample estimates to be used but sets precision standards based on the lower (or upper) limit of a 1-sided CI.
</p>
<p>Using an Edgeworth approximation in Sugden, Smith, and Jones (SSJ, 2000) to the distribution of the estimated mean, <code>nEdgeSRS</code> computes the total sample size needed so that a CI will have coverage equal to the nominal value in <code>ci.lev</code> plus or minus the tolerance <code>epsilon</code>. Two alternatives are given: (1) a sample size from solving quadratic equation (4.4) in SSJ and (2) a modification of a rule from Cochran (1977) given in expression (4.3) of SSJ. If <code>hcol</code> is specified, a separate calculation is made in each stratum of the required stratum simple random sample size; thus, each stratum sample size should be adequate so that the normal approximation for each stratum estimator holds. The calculation assumes that the overall or stratum sampling fractions are negligible.
</p>
<p>When <code>dat</code> is a sample, the weights (<code>wts</code>) used in the estimator of the mean (or total) are assumed to be scaled for estimating population totals. They can be inverse selection probabilities, i.e. ones used in the <code class="reqn">\pi</code>-estimator, or weights that have been adjusted to account for nonresponse or coverage errors.
</p>
<p>The remainder term in the approximation used in <code>nEdgeSRS</code> is <code class="reqn">O(n^{-1})</code>. In contrast, the function <code>nEdge</code> uses a <code class="reqn">O(n^{-1/2})</code> approximation but applies to an overall mean from a stratified simple random sample for which several different allocations can be specified. The total sample size returned by <code>nEdge</code> is based on the overall Edgeworth approximation for the distribution of the population mean estimator; the resulting stratum sample sizes may not be large enough so that the normal approximation is adequate for each stratum estimator.
</p>


<h3>Value</h3>

<p>List with values:
</p>
<table>
<tr><td><code>CI type</code></td>
<td>
<p>one-sided or two-sided</p>
</td></tr>
<tr><td><code>epsilon</code></td>
<td>
<p>tolerance on CI coverage</p>
</td></tr>
<tr><td><code>Total sample size</code></td>
<td>
<p>vector of numeric sample sizes from (1) solving SSJ (2000) quadratic equation and (2) SSJ's modified Cochran rule</p>
</td></tr>
<tr><td><code>g1</code></td>
<td>
<p>overall skewness and kurtosis; returned if no strata are used</p>
</td></tr>
<tr><td><code>Stratum values</code></td>
<td>
<p>data frame with columns for stratum, number of sample units allocated to each stratum (<code>nh</code>) based on the SSJ quadratic rule, proportion that each quadratic-rule stratum sample is of the total sample (<code>ph</code>), modified Cochran sample size (<code>nh.cochran</code>), skewness in each stratum (<code>stratum.skewness</code>), and kurtosis in each stratum (<code>stratum.kurtosis</code>); returned if strata are used</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Richard Valliant
</p>


<h3>References</h3>

<p>Cochran, W.G. (1977). <em>Sampling Techniques</em>, 3rd edition. New York: Wiley.
</p>
<p>Sugden, R. A., Smith, T. M. F., and Jones, R. P. (2000). Cochran's Rule for Simple Random Sampling. <em>Journal of the Royal Statistical Society. Series B</em>, Vol. 62, No.4, 787-793.
doi:https://doi.org/10.1111/1467-9868.00264
</p>
<p>U.S. Internal Revenue Service (2011). 26 CFR 601.105: Examination of returns and claims for refund, credit or abatement: determination of correct tax liability. Washington DC.
<a href="https://www.irs.gov/pub/irs-drop/rp-11-42.pdf">https://www.irs.gov/pub/irs-drop/rp-11-42.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nCont">nCont</a></code>, <code><a href="#topic+nEdge">nEdge</a></code>, <code><a href="#topic+nLogOdds">nLogOdds</a></code>, <code><a href="#topic+nProp">nProp</a></code>, <code><a href="#topic+nPropMoe">nPropMoe</a></code>, <code><a href="#topic+nWilson">nWilson</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(PracTools)
#   test using HMT pop
require(PracTools)
set.seed(1289129963)
pop &lt;- HMT(N=10000, H=5)
    # using pop with no strata
nEdgeSRS(ci.lev=0.95, side="one.sided", dat=pop, pop.sw=TRUE, hcol=NULL, ycol="y")
    # using a sample as input
require(sampling)
sam &lt;- strata(data=pop, stratanames="strat", size=c(30, 40, 50, 60, 70), method=c("srswor"),
              description=TRUE)
samdat &lt;- pop[sam$ID_unit,]
w = 1/sam$Prob
nEdgeSRS(ci.lev=0.95, side="one.sided", epsilon=0.005, dat=samdat, pop.sw=FALSE, wts=w,
         hcol="strat", ycol="y")
</code></pre>

<hr>
<h2 id='nhis'>
National Health Interview Survey: Demographic variables
</h2><span id='topic+nhis'></span>

<h3>Description</h3>

<p>Demographic variables from a U.S. national household survey
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(nhis)</code></pre>


<h3>Format</h3>

<p>A data frame with 3,911 observations on the following 16 variables.
</p>

<dl>
<dt><code>ID</code></dt><dd><p>Identification variable</p>
</dd>
<dt><code>stratum</code></dt><dd><p>Sample design stratum</p>
</dd>
<dt><code>psu</code></dt><dd><p>Primary sampling unit, numbered within each stratum (1,2)</p>
</dd>
<dt><code>svywt</code></dt><dd><p>survey weight</p>
</dd>
<dt><code>sex</code></dt><dd><p>Gender (1 = male; 2 = female)</p>
</dd>
<dt><code>age</code></dt><dd><p>Age, continuous</p>
</dd>
<dt><code>age_r</code></dt><dd><p>Recoded age
(3 = 18-24 years;
4 = 25-44 years;
5 = 45-64 years;
6 = 65-69 years;
7 = 70-74 years;
8 = 75 years and older)
</p>
</dd>
<dt><code>hisp</code></dt><dd><p>Hispanic ethnicity
(1 = Hispanic;
2 = Non-Hispanic)
</p>
</dd>
<dt><code>marital</code></dt><dd><p>Marital status
(1 = Separated;
2 = Divorced;
3 = Married;
4 = Single/never married;
5 = Widowed;
9 = Unknown marital status)
</p>
</dd>
<dt><code>parents</code></dt><dd><p>Parent(s) of sample person present in the family
(1 = Mother, no father;
2 = Father, no mother;
3 = Mother and father;
4 = Neither mother nor father)
</p>
</dd>
<dt><code>parents_r</code></dt><dd><p>Parent(s) of sample person present in the family recode (1 = Yes; 2 = No)</p>
</dd>
<dt><code>educ</code></dt><dd><p>Education
(1 = 8th grade or less;
2 = 9-12th grade, no high school diploma;
3 = High school graduate;
4 = General education development (GED) degree recipient;
5 = Some college, no degree;
6 = Associate's degree, technical or vocational;
7 = Associate's degree, academic program;
8 = Bachelor's degree (BA, BS, AB, BBA);
9 = Master's, professional, or doctoral degree)
</p>
</dd>
<dt><code>educ_r</code></dt><dd><p>Education recode
(1 = High school, general education development degree (GED), or less;
2 = Some college;
3 = Bachelor's or associate's degree;
4 = Master's degree &amp; higher)
</p>
</dd>
<dt><code>race</code></dt><dd><p>Race (1 = White; 2 = Black; 3 = Other)</p>
</dd>
<dt><code>resp</code></dt><dd><p>Respondent (0 = nonrespondent; 1 = respondent)</p>
</dd>
</dl>



<h3>Details</h3>

<p>The National Health Interview Survey (NHIS) is used to monitor health conditions in the U.S.
Data are collected through personal household interviews. Only demographic variables are included in this subset which was collected in 2003. The <code>nhis</code> data set contains observations for 3,911 persons. The file contains only persons 18 years and older.
</p>


<h3>Source</h3>

<p>National Health Interview Survey of 2003 conducted by the U.S. National Center for Health Statistics.
<a href="https://www.cdc.gov/nchs/nhis.htm">https://www.cdc.gov/nchs/nhis.htm</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nhis.large">nhis.large</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(nhis)
str(nhis)
table(nhis$sex,nhis$age_r)
</code></pre>

<hr>
<h2 id='nhis.large'>
National Health Interview Survey: Demographic and health variables
</h2><span id='topic+nhis.large'></span>

<h3>Description</h3>

<p>Demographic and health related variables from a U.S. national household survey
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(nhis.large)</code></pre>


<h3>Format</h3>

<p>A data frame with 21,588 observations on the following 18 variables.
</p>

<dl>
<dt><code>ID</code></dt><dd><p>Identification variable</p>
</dd>
<dt><code>stratum</code></dt><dd><p>Sample design stratum</p>
</dd>
<dt><code>psu</code></dt><dd><p>Primary sampling unit, numbered within each stratum (1,2)</p>
</dd>
<dt><code>svywt</code></dt><dd><p>survey weight</p>
</dd>
<dt><code>sex</code></dt><dd><p>Gender (1 = male; 2 = female)</p>
</dd>
<dt><code>age.grp</code></dt><dd><p>Age group
(1 = &lt; 18 years;
2 = 18-24 years;
3 = 25-44 years;
4 = 45-64 years;
5 = 65+)
</p>
</dd>
<dt><code>hisp</code></dt><dd><p>Hispanic ethnicity
(1 = Hispanic;
2 = Non-Hispanic White;
3 = Non-Hispanic Black;
4 = Non-Hispanic All other race groups)
</p>
</dd>
<dt><code>parents</code></dt><dd><p>Parents present in the household
(1 = mother, father, or both present;
2 = neither present)
</p>
</dd>
<dt><code>educ</code></dt><dd><p>Highest level of education attained
(1 = High school graduate, graduate equivalence degree, or less;
2 = Some college;
3 = Bachelor's or associate's degree;
4 = Master's degree or higher;
NA = missing)
</p>
</dd>
<dt><code>race</code></dt><dd><p>Race
(1 = White;
2 = Black;
3 = All other race groups)
</p>
</dd>
<dt><code>inc.grp</code></dt><dd><p>Family income group
(1 = &lt; $20K;
2 = $20000-$24999;
3 = $25000-$34999;
4 = $35000-$44999;
5 = $45000-$54999;
6 = $55000-$64999;
7 = $65000-$74999;
8 = $75K+;
NA = missing)
</p>
</dd>
<dt><code>delay.med</code></dt><dd><p>Delayed medical care in last 12 months because of cost
(1 = Yes;
2 = No;
NA = missing)
</p>
</dd>
<dt><code>hosp.stay</code></dt><dd><p>Had an overnight hospital stay in last 12 months
(1 = Yes;
2 = No;
NA = missing)
</p>
</dd>
<dt><code>doc.visit</code></dt><dd><p>During 2 WEEKS before interview, did (person) see a doctor or
other health care professional at a doctor's office, a clinic,
an emergency room, or some other place? (excluding overnight hospital stay)?
(1 = Yes;
2 = No)
</p>
</dd>
<dt><code>medicaid</code></dt><dd><p>Covered by medicaid, a governmental subsidy program for the poor
(1 = Yes;
2 = No;
NA = missing)
</p>
</dd>
<dt><code>notcov</code></dt><dd><p>Not covered by any type of health insurance
(1 = Yes;
2 = No;
NA = missing)
</p>
</dd>
<dt><code>doing.lw</code></dt><dd><p>What was person doing last week?
(1 = Working for pay at a job or business;
2 = With a job or business but not at work;
3 = Looking for work;
4 = Working, but not for pay, at a job or business;
5 = Not working and not looking for work;
NA = missing)
</p>
</dd>
<dt><code>limited</code></dt><dd><p>Is the person limited in any way in any activities because of physical, mental or emotional problems?
(1 = Limited in some way;
2 = Not limited in any way;
NA = missing)
</p>
</dd>
</dl>



<h3>Details</h3>

<p>The National Health Interview Survey (NHIS) is used to monitor health conditions in the U.S. Data are collected through personal household interviews. Demographic variables and a few health related variables are included in this subset. The <code>nhis.large</code> data set contains observations on 21,588 persons extracted from the 2003 U.S. NHIS survey. The file contains only persons 18 years and older.
</p>


<h3>Source</h3>

<p>National Health Interview Survey of 2003 conducted by the U.S. National Center for Health Statistics.
<a href="https://www.cdc.gov/nchs/nhis.htm">https://www.cdc.gov/nchs/nhis.htm</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nhis">nhis</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(nhis.large)
str(nhis.large)
table(nhis.large$stratum, nhis.large$psu)
table(nhis.large$delay.med, useNA="always")</code></pre>

<hr>
<h2 id='nhispart'>
National Health Interview Survey data from 2003: socioeconomic variables
</h2><span id='topic+nhispart'></span>

<h3>Description</h3>

<p>Socioeconomic variables from a U.S. national household survey
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(nhispart)</code></pre>


<h3>Format</h3>

<p>A data frame with 3,924 observations on the following variables.
</p>

<dl>
<dt><code>HHX</code></dt><dd><p>Household identification variable</p>
</dd>
<dt><code>PX</code></dt><dd><p>Person identifier within household</p>
</dd>
<dt><code>STRATUM</code></dt><dd><p>Sample design stratum</p>
</dd>
<dt><code>PSU</code></dt><dd><p>Primary sampling unit, numbered within each stratum (1,2)</p>
</dd>
<dt><code>WTFA</code></dt><dd><p>survey weight</p>
</dd>
<dt><code>SEX</code></dt><dd><p>Gender (1 = male; 2 = female)</p>
</dd>
<dt><code>AGE_P</code></dt><dd><p>Age of persons; values are 18-85 (85 includes age 85 and older)</p>
</dd>
<dt><code>R_AGE1</code></dt><dd><p>Age group
(3 = 18-24 years;
4 = 25-44 years;
5 = 45-64 years;
6 = 65-69 years;
7 = 70-74 years;
8 = 75 years and over)
</p>
</dd>
<dt><code>ORIGIN_I</code></dt><dd><p>Hispanic ethnicity
(1 = Hispanic;
2 = Non-Hispanic)
</p>
</dd>
<dt><code>RACERPI2</code></dt><dd><p>Race grouped
(1 = White only;
2 = Black/African American only;
3 = American Indian or Alaska native (AIAN) only;
4 = Asian only;
5 = Race group not releasable;
6 = Multiple race)
</p>
</dd>
<dt><code>MRACRPI2</code></dt><dd><p>Race detailed
(1  = White;
2  = Black/African American;
3  = Indian (American), Alaska Native;
9  = Asian Indian;
10 = Chinese;
11 = Filipino;
15 = Other Asian;
16 = Primary race not releasable;
17 = Multiple race, no primary race selected)
</p>
</dd>
<dt><code>RACRECI2</code></dt><dd><p>White/Black
(1 = White;
2 Black;
3 All other race groups)
</p>
</dd>
<dt><code>R_MARITL</code></dt><dd><p>Marital status
(1 = Married - spouse in household;
2 = Married - spouse not in household;
3 = Married - unknown whether spouse in household;
4 = Widowed;
5 = Divorced;
6 = Separated;
7 = Never married;
8 = Living with partner;
9 = Unknown marital status)
</p>
</dd>
<dt><code>CDCMSTAT</code></dt><dd><p>CDC marital status
(1 = Mother, no father;
2 = Father, no mother;
3 = Mother and father;
4 = Neither mother nor father)
</p>
</dd>
<dt><code>INCGRP</code></dt><dd><p>Total combined family income group
(1 = 0-$4999;
2 = $5000-$9999;
3 = $10000-$14999;
4 = $15000-$19999;
5 = $20000-$24999;
6 = $25000-$34999;
7 = $35000-$44999;
8 = $45000-$54999;
9 = $55000-$64999;
10 = $65000-$74999;
11 = $75000 and over;
12 = $20000 or more (no detail);
13 = Less than $20000 (no detail);
97 = Refused;
98 = Not ascertained;
99 = Don't know)
</p>
</dd>
<dt><code>PARENTS</code></dt><dd><p>Parent(s) present in the family
(1 = Mother, no father;
2 = Father, no mother;
3 = Mother and father;
4 = Neither mother nor father)
</p>
</dd>
<dt><code>EDUC_R1</code></dt><dd><p>Highest level of education attained
(1 = Less than high school graduate;
3 = High school graduate or general education development degree (GED);
5 = Some college, no degree;
6 = AA degree, technical or vocational or AA degree, academic program or Bachelor's degree (BA, BS, AB, BBA);
9 = Master's, professional, or doctoral degree)
</p>
</dd>
<dt><code>RAT_CAT</code></dt><dd><p>Ratio of family income to poverty level
(1 = Under 0.50;
2 = 0.50 to 0.74;
3 = 0.75 to 0.99;
4 = 1.00 to 1.24;
5 = 1.25 to 1.49;
6 = 1.50 to 1.74;
7 = 1.75 to 1.99;
8 = 2.00 to 2.49;
9 = 2.50 to 2.99;
10 = 3.00 to 3.49;
11 = 3.50 to 3.99;
12 = 4.00 to 4.49;
13 = 4.50 to 4.99;
14 = 5.00 and over;
99 = Unknown)
</p>
</dd>
</dl>



<h3>Details</h3>

<p>The National Health Interview Survey (NHIS) is used to monitor health conditions in the U.S. Data are collected through personal household interviews. Socioeconomic variables are included in this subset along with household and person codes. The <code>nhispart</code> data set contains observations on 3,924 persons extracted from the 2003 U.S. survey. The file contains only persons 18 years and older.
</p>


<h3>Source</h3>

<p>National Health Interview Survey of 2003 conducted by the U.S. National Center for Health Statistics.
<a href="https://www.cdc.gov/nchs/nhis.htm">https://www.cdc.gov/nchs/nhis.htm</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(nhispart)
str(nhispart)
table(nhispart$STRATUM, nhispart$PSU)
table(nhispart$RACERPI2, nhispart$RACRECI2, useNA="always")
</code></pre>

<hr>
<h2 id='nLogOdds'>
Calculate simple random sample size for estimating a proportion</h2><span id='topic+nLogOdds'></span>

<h3>Description</h3>

<p>Calculate the simple random sample size for estimating a proportion using the log-odds transformation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>    nLogOdds(moe.sw, e, alpha=0.05, pU, N=Inf)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nLogOdds_+3A_moe.sw">moe.sw</code></td>
<td>

<p>switch for setting desired margin of error
(1 = CI half-width on the proportion;
2 = CI half-width on a proportion divided by <code>pU</code>)
</p>
</td></tr>
<tr><td><code id="nLogOdds_+3A_e">e</code></td>
<td>

<p>desired margin of error
</p>
</td></tr>
<tr><td><code id="nLogOdds_+3A_alpha">alpha</code></td>
<td>

<p>1 - (confidence level)
</p>
</td></tr>
<tr><td><code id="nLogOdds_+3A_pu">pU</code></td>
<td>

<p>population proportion
</p>
</td></tr>
<tr><td><code id="nLogOdds_+3A_n">N</code></td>
<td>

<p>number of units in finite population
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function accepts five parameters, which are the same ones as accepted by <code><a href="#topic+nPropMoe">nPropMoe</a></code>.
The desired margin of error can be specified as the CI half-width on the proportion (<code>moe.sw=1</code>)
or as the CI half-width as a proportion of the population value <code>pU</code> (<code>moe.sw=2</code>).
</p>


<h3>Value</h3>

<p>numeric sample size
</p>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Valliant, R., Dever, J., Kreuter, F. (2018, chap. 3). <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nProp">nProp</a></code>, <code><a href="#topic+nPropMoe">nPropMoe</a></code>, <code><a href="#topic+nWilson">nWilson</a></code>, <code><a href="#topic+nCont">nCont</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>nLogOdds(moe.sw=1, e=0.05, alpha=0.05, pU=0.2, N=Inf)
nLogOdds(moe.sw=2, e=0.05, alpha=0.05, pU=0.2, N=Inf)
</code></pre>

<hr>
<h2 id='nPPS'>
Calculate the sample size for a probability proportional to size (PPS) sample
</h2><span id='topic+nPPS'></span>

<h3>Description</h3>

<p>Calculate the sample size for a probability proportional to size (PPS) sample, assuming the sample is selected with replacement.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nPPS(X = NULL, Y = NULL, CV0 = NULL, V0 = NULL, N = NULL, V1 = NULL, ybarU = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nPPS_+3A_x">X</code></td>
<td>

<p>variable used for computing 1-draw probabilities; length is <code class="reqn">N</code>, the population size; must be numeric
</p>
</td></tr>
<tr><td><code id="nPPS_+3A_y">Y</code></td>
<td>

<p>variable used for variance calculation; length is <code class="reqn">N</code>, the population size; must be numeric
</p>
</td></tr>
<tr><td><code id="nPPS_+3A_cv0">CV0</code></td>
<td>

<p>target value of the coefficient of variation of the estimated total of <code>Y</code>
</p>
</td></tr>
<tr><td><code id="nPPS_+3A_v0">V0</code></td>
<td>

<p>target value of the variance of the estimated total of <code>Y</code>; only one of <code>CV0</code> and <code>V0</code> can be specified
</p>
</td></tr>
<tr><td><code id="nPPS_+3A_n">N</code></td>
<td>

<p>population size; required if <code>X</code> or <code>Y</code> is NULL
</p>
</td></tr>
<tr><td><code id="nPPS_+3A_v1">V1</code></td>
<td>

<p>unit variance for PPS calculation
</p>
</td></tr>
<tr><td><code id="nPPS_+3A_ybaru">ybarU</code></td>
<td>

<p>population mean of <code class="reqn">Y</code> (or an estimate of it)
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>nPPS</code> computes the sample size needed for a probability proportional to size sample or, more generally, a sample selected with varying probabilities, assuming the sample is selected with replacement (WR). Although these samples are rarely selected WR, the variance formula for WR samples is simple and convenient for sample size calculations. Population vectors can be input of <code>X</code>, a measure of size for selecting the sample, and <code>Y</code>, an analysis variable. Alternatively, the population size, <code>N</code>, the unit variance, <code>V1</code>, and the population mean of <code class="reqn">Y</code>, <code>ybarU</code> can be inputs.
</p>


<h3>Value</h3>

<p>A list with four components:
</p>
<table>
<tr><td><code>N</code></td>
<td>
<p>Size of the population</p>
</td></tr>
<tr><td><code>V1</code></td>
<td>
<p>Population variance of <code class="reqn">Y</code> appropriate for a sample selected with varying probabilities and with replacement; see Valliant, Dever, and Kreuter (2018, sec. 3.4).</p>
</td></tr>
<tr><td><code>ybarU</code></td>
<td>
<p>Population mean of <code class="reqn">Y</code></p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>Calculated sample size</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>George Zipf, Richard Valliant
</p>


<h3>References</h3>

<p>Valliant, R., Dever, J., Kreuter, F. (2018, chap. 3). <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nCont">nCont</a></code>, <code><a href="#topic+nContMoe">nContMoe</a></code>, <code><a href="#topic+nContOpt">nContOpt</a></code>, <code><a href="#topic+unitVar">unitVar</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(PracTools)
data("smho.N874")
y &lt;- smho.N874[,"EXPTOTAL"]
x &lt;- smho.N874[, "BEDS"]
y &lt;- y[x&gt;0]
x &lt;- x[x&gt;0]
nPPS(X = x, Y = y, CV0 = 0.15)
nPPS(X = x, Y = y,  V0 = 2000000^2)
nPPS(CV0 = 0.15, N = length(y), V1 = (10^21), ybarU = mean(y))
</code></pre>

<hr>
<h2 id='nProp'>
Compute simple random sample size for estimating a proportion
</h2><span id='topic+nProp'></span>

<h3>Description</h3>

<p>Compute the simple random sample size for estimating a proportion based on different precision requirements.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nProp(CV0 = NULL, V0 = NULL, pU = NULL, N = Inf)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nProp_+3A_cv0">CV0</code></td>
<td>

<p>target value of coefficient of variation of the estimated proportion
</p>
</td></tr>
<tr><td><code id="nProp_+3A_v0">V0</code></td>
<td>

<p>target value of variance of the estimated proportion
</p>
</td></tr>
<tr><td><code id="nProp_+3A_pu">pU</code></td>
<td>

<p>population proportion
</p>
</td></tr>
<tr><td><code id="nProp_+3A_n">N</code></td>
<td>

<p>number of units in finite population
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The precision requirement of <code class="reqn">p_s</code> can be set based on either a target coefficient of variation,
<code class="reqn">CV_0</code>, or a target variance, <code class="reqn">V_0</code>. In either case, a value of <code class="reqn">p_U</code> must be supplied.
</p>


<h3>Value</h3>

<p>numeric sample size
</p>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Valliant, R., Dever, J., Kreuter, F. (2018, chap. 3).  <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nCont">nCont</a></code>, <code><a href="#topic+nLogOdds">nLogOdds</a></code>, <code><a href="#topic+nPropMoe">nPropMoe</a></code>, <code><a href="#topic+nWilson">nWilson</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># srs sample size so that CV of estimated proportion is 0.05
# assuming the population is large and pU=0.01
# Both examples below are equivalent
nProp(V0=0.0005^2, N=Inf, pU=0.01) #or
nProp(CV0=0.05, N=Inf, pU=0.01)

# srswor sample size so that half-width of 2-sided 95% CI is 0.005
nProp(V0=(0.005/1.96)^2, N=Inf, pU=0.01)
</code></pre>

<hr>
<h2 id='nProp2sam'>
Simple random sample size for difference in proportions
</h2><span id='topic+nProp2sam'></span>

<h3>Description</h3>

<p>Compute a simple random sample size for estimating the difference in proportions when samples overlap
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nProp2sam(px, py, pxy, g, r, alt, sig.level=0.05, pow=0.80)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nProp2sam_+3A_px">px</code></td>
<td>

<p>proportion in group 1
</p>
</td></tr>
<tr><td><code id="nProp2sam_+3A_py">py</code></td>
<td>

<p>proportion in group 2
</p>
</td></tr>
<tr><td><code id="nProp2sam_+3A_pxy">pxy</code></td>
<td>

<p>proportion in the overlap has the characteristic in both samples
</p>
</td></tr>
<tr><td><code id="nProp2sam_+3A_g">g</code></td>
<td>

<p>proportion of sample 1 that is in the overlap with sample 2
</p>
</td></tr>
<tr><td><code id="nProp2sam_+3A_r">r</code></td>
<td>

<p>ratio of the size of sample 1 to that of sample 2
</p>
</td></tr>
<tr><td><code id="nProp2sam_+3A_alt">alt</code></td>
<td>

<p>should the test be 1-sided or 2-sided; allowable values are <code>alt="one.sided"</code> or  <code>alt="two.sided"</code>.
</p>
</td></tr>
<tr><td><code id="nProp2sam_+3A_sig.level">sig.level</code></td>
<td>

<p>significance level of the hypothesis test
</p>
</td></tr>
<tr><td><code id="nProp2sam_+3A_pow">pow</code></td>
<td>

<p>desired power of the test
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>nProp2sam</code> computes sample sizes in two groups that are required for testing whether
the difference in group proportions is significant.
The power of the test is one of the input parameters.
The samples have a specified proportion of units in common.
</p>


<h3>Value</h3>

<p>List with values:
</p>
<table>
<tr><td><code>n1</code></td>
<td>
<p>sample size in group 1</p>
</td></tr>
<tr><td><code>n2</code></td>
<td>
<p>sample size in group 2</p>
</td></tr>
<tr><td><code>px.py.pxy</code></td>
<td>
<p>input values of the <code>px</code>, <code>py</code>, <code>pxy</code> parameters</p>
</td></tr>
<tr><td><code>gamma</code></td>
<td>
<p>proportion of sample 1 that is in the overlap with sample 2</p>
</td></tr>
<tr><td><code>r</code></td>
<td>
<p>ratio of the size of sample 1 to that of sample 2</p>
</td></tr>
<tr><td><code>alt</code></td>
<td>
<p>type of test: one-sided or two-sided</p>
</td></tr>
<tr><td><code>sig.level</code></td>
<td>
<p>significance level of test</p>
</td></tr>
<tr><td><code>power</code></td>
<td>
<p>power of the test</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Valliant, R., Dever, J., Kreuter, F. (2018, chap. 4). <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>
<p>Woodward, M. (1992). Formulas for Sample Size, Power, and Minimum Detectable Relative Risk in Medical Studies.  <em>The Statistician</em>, 41, 185-196.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nDep2sam">nDep2sam</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>nProp2sam(px=0.5, py=0.55, pxy=0.45, g=0.5, r=1, alt="two.sided")
</code></pre>

<hr>
<h2 id='nPropMoe'>
Simple random sample size for a proportion based on margin of error
</h2><span id='topic+nPropMoe'></span>

<h3>Description</h3>

<p>Calculates a simple random sample size based on a specified margin of error.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nPropMoe(moe.sw, e, alpha = 0.05, pU, N = Inf)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nPropMoe_+3A_moe.sw">moe.sw</code></td>
<td>

<p>switch for setting desired margin of error (1 = CI half-width on the proportion;
2 = CI half-width on a proportion divided by <code class="reqn">p_U</code>)
</p>
</td></tr>
<tr><td><code id="nPropMoe_+3A_e">e</code></td>
<td>

<p>desired margin of error; either <code class="reqn">e=z_{1-\alpha/2}\sqrt{V(p_s)}</code> or
<code class="reqn">e=z_{1-\alpha/2}CV(p_s)</code>
</p>
</td></tr>
<tr><td><code id="nPropMoe_+3A_alpha">alpha</code></td>
<td>

<p>1 - (confidence level)
</p>
</td></tr>
<tr><td><code id="nPropMoe_+3A_pu">pU</code></td>
<td>

<p>population proportion
</p>
</td></tr>
<tr><td><code id="nPropMoe_+3A_n">N</code></td>
<td>

<p>number of units in finite population
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The margin of error can be set as the half-width of a normal approximation
confidence interval, <code class="reqn">e=z_{1-\alpha/2}\sqrt{V(p_s)}</code>, or as the half-width
of a normal approximation confidence interval divided by the population proportion,
<code class="reqn">e=z_{1-\alpha/2}CV(p_s)</code>. The type of margin of error is selected by the
parameter <code>moe.sw</code> where <code>moe.sw=1</code> sets <code class="reqn">e=z_{1-\alpha/2}\sqrt{V(p_s)}</code> and <code>moe.sw=2</code> sets i.e., <code class="reqn">e=\frac{z_{1-\alpha/2}\sqrt{V(p_s)}}{p_U}</code>.
</p>


<h3>Value</h3>

<p>numeric sample size
</p>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Valliant, R., Dever, J., Kreuter, F. (2018, chap. 3).  <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nCont">nCont</a></code>, <code><a href="#topic+nLogOdds">nLogOdds</a></code>, <code><a href="#topic+nProp">nProp</a></code>, <code><a href="#topic+nWilson">nWilson</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># srs sample size so that half-width of a 95% CI is 0.01
# population is large and population proportion is 0.04
nPropMoe(moe.sw=1, e=0.01, alpha=0.05, pU=0.04, N=Inf)

# srswor sample size for a range of margins of error defined as
# half-width of a 95% CI
nPropMoe(moe.sw=1, e=seq(0.01,0.08,0.01), alpha=0.05, pU=0.5)

# srswor sample size for a range of margins of error defined as
# the proportion that the half-width of a 95% CI is of pU
nPropMoe(moe.sw=2, e=seq(0.05,0.1,0.2), alpha=0.05, pU=0.5)
</code></pre>

<hr>
<h2 id='NRadjClass'>
Class-based nonresponse adjustments
</h2><span id='topic+NRadjClass'></span>

<h3>Description</h3>

<p>Compute separate nonresponse adjustments in a set of classes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NRadjClass(ID, NRclass, resp, preds=NULL, wts=NULL, type)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="NRadjClass_+3A_id">ID</code></td>
<td>
<p>identification value for a unit</p>
</td></tr>
<tr><td><code id="NRadjClass_+3A_nrclass">NRclass</code></td>
<td>
<p>vector of classes to use for nonresponse adjustment. Length is number of respondents plus nonrespondents</p>
</td></tr>
<tr><td><code id="NRadjClass_+3A_resp">resp</code></td>
<td>
<p>indicator for whether unit is a nonrespondent (must be coded 0) or respondent (must be coded 1)</p>
</td></tr>
<tr><td><code id="NRadjClass_+3A_preds">preds</code></td>
<td>
<p>response probabilities, typically estimated from a binary regression model as in <code>pclass</code></p>
</td></tr>
<tr><td><code id="NRadjClass_+3A_wts">wts</code></td>
<td>
<p>vector of survey weights, typically base weights or base weights adjusted for unknown eligibility</p>
</td></tr>
<tr><td><code id="NRadjClass_+3A_type">type</code></td>
<td>
<p>type of adjustment computed within each value of <code>NRclass</code>. Allowable codes are 1, 2, 3, 4, or 5.
(1 = unweighted average of response propensities, i.e., <code>preds</code>;
2 = weighted average response propensity;
3 = unweighted response rate;
4 = weighted response rate;
5 = median response propensity)
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The input vectors should include both respondents and nonrespondents in a sample. A single value between 0 and 1 is computed in each nonresponse adjustment class to be used as a nonresponse adjustment. Five alternatives are available for computing the adjustment based on the value of <code>type</code>. The value of the adjustment is merged with individual unit data and stored in the <code>RR</code> field of the output data frame.
</p>


<h3>Value</h3>

<p>A data frame of respondents only with four columns:
</p>
<table>
<tr><td><code>NRcl.no</code></td>
<td>
<p>number of the nonresponse adjustment class for each unit</p>
</td></tr>
<tr><td><code>ID</code></td>
<td>
<p>identification value for a unit</p>
</td></tr>
<tr><td><code>resp</code></td>
<td>
<p>value of the <code>resp</code> variable (always 1)</p>
</td></tr>
<tr><td><code>RR</code></td>
<td>
<p>nonresponse adjustment for each unit</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Valliant, R., Dever, J., Kreuter, F. (2018, chap. 13).  <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pclass">pclass</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(PracTools)
data(nhis)
out &lt;- pclass(formula = resp ~ age + as.factor(sex) + as.factor(hisp) + as.factor(race),
         data = nhis, type = "unwtd", link="logit", numcl=5)
    # unweighted average of response propensities within each class
zz &lt;- NRadjClass(ID=nhis[,"ID"], NRclass = as.numeric(out$p.class), resp=nhis[,"resp"],
            preds=out$propensities, wts=NULL, type=1)
</code></pre>

<hr>
<h2 id='NRFUopt'>
Sample sizes for a nonresponse follow-up study
</h2><span id='topic+NRFUopt'></span>

<h3>Description</h3>

<p>Compute optimal values of the first-phase sample size and the second-phase sampling fraction in a two-phase sample.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NRFUopt(Ctot=NULL, c1, c2, theta, CV0=NULL, CVpop=NULL, N=Inf, type.sw)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="NRFUopt_+3A_ctot">Ctot</code></td>
<td>
<p>total variable cost</p>
</td></tr>
<tr><td><code id="NRFUopt_+3A_c1">c1</code></td>
<td>
<p>cost per unit in phase-1</p>
</td></tr>
<tr><td><code id="NRFUopt_+3A_c2">c2</code></td>
<td>
<p>cost per unit in phase-2</p>
</td></tr>
<tr><td><code id="NRFUopt_+3A_theta">theta</code></td>
<td>
<p>probability of response for each unit</p>
</td></tr>
<tr><td><code id="NRFUopt_+3A_cv0">CV0</code></td>
<td>
<p>target coefficient of variation for the estimated total or mean</p>
</td></tr>
<tr><td><code id="NRFUopt_+3A_cvpop">CVpop</code></td>
<td>
<p>Unit coefficient of variation</p>
</td></tr>
<tr><td><code id="NRFUopt_+3A_n">N</code></td>
<td>
<p>Population size; default is <code>Inf</code></p>
</td></tr>
<tr><td><code id="NRFUopt_+3A_type.sw">type.sw</code></td>
<td>
<p>type of allocation; <code>"cost"</code> = target total variable cost, <code>"cv"</code> = target coefficient of variation</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>NRFUopt</code> computes the optimal values of the first-phase sample size and the second-phase sampling fraction in a two-phase sample. Both stages are assumed to be selected using simple random sampling without replacement. If <code>type.sw="cost"</code>, the optima are computed for a target total, expected cost across both phases. If <code>type.sw="cv"</code>, the optima are computed for a target coefficient of variation for an estimated mean.
</p>


<h3>Value</h3>

<p>List object with values:
</p>
<table>
<tr><td><code>allocation</code></td>
<td>
<p>type of allocation: either &quot;fixed cost&quot; or &quot;fixed CV&quot;</p>
</td></tr>
<tr><td><code>"Total variable cost"</code></td>
<td>
<p>expected total cost: fixed cost if <code>type.sw="cost"</code> or computed cost if <code>type.sw="cv"</code>; unrounded sample sizes are used in calculation</p>
</td></tr>
<tr><td><code>"Response rate"</code></td>
<td>
<p>first-phase response rate</p>
</td></tr>
<tr><td><code>CV</code></td>
<td>
<p>anticipated coefficient of variation (CV) if <code>type.sw="cost"</code> or target CV if <code>type.sw="cv"</code></p>
</td></tr>
<tr><td><code>v.opt</code></td>
<td>
<p>optimal fraction of first-phase nonrespondents to select for second-phase follow-up</p>
</td></tr>
<tr><td><code>n1.opt</code></td>
<td>
<p>optimal number of units to sample at first-phase</p>
</td></tr>
<tr><td><code>"Expected n2"</code></td>
<td>
<p>expected number of respondents obtained at second-phase</p>
</td></tr>
<tr><td><code>"srs sample for same cv"</code></td>
<td>
<p>size of single-phase simple random sample (<em>srs</em>) needed to obtain same CV as the two-phase sample</p>
</td></tr>
<tr><td><code>"Cost Ratio: Two phase to srs"</code></td>
<td>
<p>ratio of expected cost for two-phase sample to cost of single-phase <em>srs</em></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Saerndal, C.E., Swensson, B., and Wretman, J. (1992, examples 15.4.4 and 15.4.5). <em>Model Assisted Survey Sampling</em>. New York: Springer.
</p>
<p>Valliant, R., Dever, J., Kreuter, F. (2018, chap.17). <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>    # optima for fixed target CV
NRFUopt(Ctot=NULL, c1=50, c2=200, theta=0.5, CV0=0.05, CVpop=1, type.sw = "cv")
    # optima for fixed total cost
NRFUopt(Ctot=100000, c1=50, c2=200, theta=0.5, CV0=NULL, CVpop=1, type.sw = "cost")

</code></pre>

<hr>
<h2 id='nWilson'>
Calculate a simple random sample size for estimating a proportion
</h2><span id='topic+nWilson'></span>

<h3>Description</h3>

<p>Calculate a simple random sample size for estimating a proportion using the Wilson method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nWilson(moe.sw, alpha = 0.05, pU, e)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nWilson_+3A_moe.sw">moe.sw</code></td>
<td>

<p>switch for setting desired margin of error (1 = CI half-width on the proportion;
2 = CI half-width on a proportion divided by <code>pU</code>)
</p>
</td></tr>
<tr><td><code id="nWilson_+3A_alpha">alpha</code></td>
<td>

<p>1 - (confidence level)
</p>
</td></tr>
<tr><td><code id="nWilson_+3A_pu">pU</code></td>
<td>

<p>population proportion
</p>
</td></tr>
<tr><td><code id="nWilson_+3A_e">e</code></td>
<td>

<p>desired margin of error; either the value of CI half-width or the value of the half-width divided by <code>pU</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculate a simple random sample size using the Wilson (1927) method. A margin of error
can be set as the CI half-width on the proportion (<code>moe.sw=1</code>) or as the CI
half-width as a proportion of the population value <code class="reqn">p_U</code> (<code>moe.sw=2</code>).
</p>


<h3>Value</h3>

<table>
<tr><td><code>n.sam</code></td>
<td>
<p>numeric sample size</p>
</td></tr>
<tr><td><code>"CI lower limit"</code></td>
<td>
<p>lower limit of Wilson confidence interval with computed sample size</p>
</td></tr>
<tr><td><code>"CI upper limit"</code></td>
<td>
<p>upper limit of Wilson confidence interval with computed sample size</p>
</td></tr>
<tr><td><code>"length of CI"</code></td>
<td>
<p>length of Wilson confidence interval with computed sample size</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Valliant, R., Dever, J., Kreuter,F. (2018, chap. 3).  <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>
<p>Wilson, E.B. (1927). Probable inference, the law of succession, and statistical inference.  <em>Journal of the American Statistical Association</em>, 22, 209-212.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nCont">nCont</a></code>, <code><a href="#topic+nLogOdds">nLogOdds</a></code>, <code><a href="#topic+nProp">nProp</a></code>, <code><a href="#topic+nPropMoe">nPropMoe</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># srs sample size using Wilson method so that half-width of a 95% CI
# is 0.01. Population proportion is 0.04
nWilson(moe.sw = 1, pU = 0.04, e = 0.01)
</code></pre>

<hr>
<h2 id='pclass'>
Form nonresponse adjustment classes based on propensity scores
</h2><span id='topic+pclass'></span>

<h3>Description</h3>

<p>Fit a binary regression model for response probabilities and divide units into a specified number of classes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pclass(formula, data, link="logit", numcl=5, type, design=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pclass_+3A_formula">formula</code></td>
<td>
<p>symbolic description of the binary regression model to be fitted as used in <code>glm</code></p>
</td></tr>
<tr><td><code id="pclass_+3A_data">data</code></td>
<td>
<p>an optional data frame; must be specified if <code>type="unwtd"</code></p>
</td></tr>
<tr><td><code id="pclass_+3A_link">link</code></td>
<td>
<p>a specification for the model link function; allowable values are <code>"logit"</code>, <code>"probit"</code>, or <code>"cloglog"</code></p>
</td></tr>
<tr><td><code id="pclass_+3A_numcl">numcl</code></td>
<td>
<p>number of classes into which units are split based on estimated propensities</p>
</td></tr>
<tr><td><code id="pclass_+3A_type">type</code></td>
<td>
<p>whether an unweighted or weighted binary regression should be fit; allowable values are <code>"unwtd"</code> or <code>"wtd"</code></p>
</td></tr>
<tr><td><code id="pclass_+3A_design">design</code></td>
<td>
<p>sample design object; required if <code>type="wtd"</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>A typical <code>formula</code> has the form <code>response ~ terms</code> where response is a two-level variable coded as 0 or 1, or is a factor where the first level denotes nonresponse and the second level is response. If <code>type="unwtd"</code>, <code>glm</code> is used to fit an unweighted regression. If <code>type="wtd"</code>, <code>svyglm</code> in the <code>survey</code> package is used to fit a survey-weighted regression.
</p>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>p.class</code></td>
<td>
<p>propensity class for each unit</p>
</td></tr>
<tr><td><code>propensities</code></td>
<td>
<p>estimated response probability for each unit</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Valliant, R., Dever, J., Kreuter, F. (2018, chap. 13). <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+NRadjClass">NRadjClass</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>    # classes based on unweighted logistic regression
require(PracTools)
data(nhis)
out &lt;- pclass(formula = resp ~ age + as.factor(sex) + as.factor(hisp) + as.factor(race),
           data = nhis, type = "unwtd", link="logit", numcl=5)
table(out$p.class, useNA="always")
summary(out$propensities)
    # classes based on survey-weighted logistic regression
require(survey)
nhis.dsgn &lt;- svydesign(ids = ~psu, strata = ~stratum, data = nhis, nest = TRUE, weights = ~svywt)
out &lt;- pclass(formula = resp ~ age + as.factor(sex) + as.factor(hisp) + as.factor(race),
           type = "wtd", design = nhis.dsgn, link="logit", numcl=5)
table(out$p.class, useNA="always")
summary(out$propensities)
</code></pre>

<hr>
<h2 id='quad_roots'>
Compute the roots of a quadratic equation   
</h2><span id='topic+quad_roots'></span>

<h3>Description</h3>

<p>Compute the roots of a quadratic equation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>quad_roots(a, b, c)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="quad_roots_+3A_a">a</code></td>
<td>
<p>coefficient of the quadratic term</p>
</td></tr>
<tr><td><code id="quad_roots_+3A_b">b</code></td>
<td>
<p>coefficient of the linear term</p>
</td></tr>
<tr><td><code id="quad_roots_+3A_c">c</code></td>
<td>
<p>coefficient of the constant term</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>quad_roots</code> computes the roots of a quadratic equation of the form <code class="reqn">ax^2 + bx + c = 0</code>.
</p>


<h3>Value</h3>

<p>vector with the two roots</p>


<h3>Examples</h3>

<pre><code class='language-R'>    quad_roots(1, -8, 12)
</code></pre>

<hr>
<h2 id='smho.N874'>
Survey of Mental Health Organizations Data
</h2><span id='topic+smho.N874'></span>

<h3>Description</h3>

<p>Data from the 1998 Survey of Mental Health Organizations (SMHO)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(smho.N874)</code></pre>


<h3>Format</h3>

<p>A data frame with 874 observations on the following 6 variables.
</p>

<dl>
<dt><code>EXPTOTAL</code></dt><dd><p>Total expenditures in 1998</p>
</dd>
<dt><code>BEDS</code></dt><dd><p>Total inpatient beds</p>
</dd>
<dt><code>SEENCNT</code></dt><dd><p>Unduplicated client/patient count seen during year</p>
</dd>
<dt><code>EOYCNT</code></dt><dd><p>End of year count of patients on the role</p>
</dd>
<dt><code>FINDIRCT</code></dt><dd><p>Hospital receives money from the state mental health agency (1=Yes; 2=No)</p>
</dd>
<dt><code>hosp.type</code></dt><dd><p>Hospital type
(1 = Psychiatric;
2 = Residential or veterans;
3 = General;
4 = Outpatient, partial care;
5 = Multi-service, substance abuse)
</p>
</dd>
</dl>



<h3>Details</h3>

<p>The 1998 SMHO was conducted by the U.S. Substance Abuse and Mental Health Services Administration.
It collected data on mental health care organizations and general hospitals that provide mental health care services, with an objective to develop national and state level estimates for total expenditure,
full time equivalent staff, bed count, and total caseload by type of organization.
The population omits one extreme observation in the <code>smho98</code> population and
has fewer variables than <code>smho98</code>.  <code>smho.N874</code> contains observations on 874 facilities.
</p>


<h3>Source</h3>

<p>Substance Abuse and Mental Health Services Administration
</p>


<h3>References</h3>

<p>Manderscheid, R.W. and Henderson, M.J. (2002). Mental Health, United States, 2002.
DHHS Publication No. SMA04-3938. Rockville MD USA: Substance Abuse and
Mental Health Services Administration.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+smho98">smho98</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(smho.N874)
str(smho.N874)
</code></pre>

<hr>
<h2 id='smho98'>
Survey of Mental Health Organizations Data
</h2><span id='topic+smho98'></span>

<h3>Description</h3>

<p>Data from the 1998 Survey of Mental Health Organizations (SMHO)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(smho98)</code></pre>


<h3>Format</h3>

<p>A data frame with 875 observations on the following variables.
</p>

<dl>
<dt><code>STRATUM</code></dt><dd><p>Sample design stratum
(1 = Psychiatric Hospital, private;
2 = Psychiatric Hospital, public;
3 = Residential, children;
4 = Residential, adults;
5 = General Hospital, public, inpatient or residential care;
6 = General Hospital, public, outpatient care only;
7 = General Hospital, private, inpatient or residential care;
8 = General Hospital, private, outpatient care only;
9 = Military Veterans, inpatient or residential care;
10 = Military Veterans, outpatient care only;
11 = Partial Care
12 = Outpatient care, private;
13 = Outpatient care, public;
14 = Multi-service, private;
15 = Multi-service, public;
16 = Substance Abuse)
</p>
</dd>
<dt><code>BEDS</code></dt><dd><p>Total inpatient beds</p>
</dd>
<dt><code>EXPTOTAL</code></dt><dd><p>Total expenditures in 1998</p>
</dd>
<dt><code>SEENCNT</code></dt><dd><p>Unduplicated client/patient count seen during year</p>
</dd>
<dt><code>EOYCNT</code></dt><dd><p>End of year count of patients on the role</p>
</dd>
<dt><code>Y_IP</code></dt><dd><p>Number of inpatient visits during year</p>
</dd>
<dt><code>OPCSFRST</code></dt><dd><p>Number of outpatients on the rolls on the first day of the reporting year</p>
</dd>
<dt><code>OPCSADDS</code></dt><dd><p>Number of outpatients admitted, readmitted, or transferred to the organization during the reporting year for less than a 24 hour period and not overnight</p>
</dd>
<dt><code>OPCSVIST</code></dt><dd><p>Number of outpatient visits during the reporting year for less than a 24 hour period and not overnight</p>
</dd>
<dt><code>EMGWALK</code></dt><dd><p>Number of emergency walk-ins during the reporting year</p>
</dd>
<dt><code>PSYREHAB</code></dt><dd><p>Number of visits for psychiatric rehabilitation services</p>
</dd>
<dt><code>IPCSADDS</code></dt><dd><p>Number of residential patients added during the reporting year or patients admitted for more than a 24 hour period</p>
</dd>
</dl>



<h3>Details</h3>

<p>The 1998 SMHO was conducted by the U.S. Substance Abuse and Mental Health Services Administration.
It collected data on mental health care organizations and general hospitals that provide mental health care services,
with an objective to develop national and state level estimates for total expenditure,
full time equivalent staff, bed count, and total caseload by type of organization.
</p>


<h3>Source</h3>

<p>Substance Abuse and Mental Health Services Administration
</p>


<h3>References</h3>

<p>Manderscheid, R.W. and Henderson, M.J. (2002). Mental Health, United States, 2002.
DHHS Publication No. SMA04-3938. Rockville MD USA: Substance Abuse and
Mental Health Services Administration.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+smho.N874">smho.N874</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(smho98)
str(smho98)
summary(smho98)
</code></pre>

<hr>
<h2 id='strAlloc'>
Allocate a sample to strata
</h2><span id='topic+strAlloc'></span>

<h3>Description</h3>

<p>Compute the proportional, Neyman, cost-constrained, and variance-constrained
allocations in a stratified simple random sample.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>strAlloc(n.tot = NULL, Nh = NULL, Sh = NULL, cost = NULL, ch = NULL,
     V0 = NULL, CV0 = NULL, ybarU = NULL, alloc)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="strAlloc_+3A_n.tot">n.tot</code></td>
<td>

<p>fixed total sample size
</p>
</td></tr>
<tr><td><code id="strAlloc_+3A_nh">Nh</code></td>
<td>

<p>vector of population stratum sizes (<code class="reqn">N_h</code>) or pop stratum proportions (<code class="reqn">W_h</code>)
</p>
</td></tr>
<tr><td><code id="strAlloc_+3A_sh">Sh</code></td>
<td>

<p>stratum unit standard deviations (<code class="reqn">S_h</code>), required unless <code>alloc = "prop"</code>
</p>
</td></tr>
<tr><td><code id="strAlloc_+3A_cost">cost</code></td>
<td>

<p>total variable cost
</p>
</td></tr>
<tr><td><code id="strAlloc_+3A_ch">ch</code></td>
<td>

<p>vector of costs per unit in stratum <em>h</em> <code class="reqn">(c_h)</code>
</p>
</td></tr>
<tr><td><code id="strAlloc_+3A_v0">V0</code></td>
<td>

<p>fixed variance target for estimated mean
</p>
</td></tr>
<tr><td><code id="strAlloc_+3A_cv0">CV0</code></td>
<td>

<p>fixed CV target for estimated mean
</p>
</td></tr>
<tr><td><code id="strAlloc_+3A_ybaru">ybarU</code></td>
<td>

<p>population mean of <em>y</em> (<code class="reqn">\bar{y}_U</code>)
</p>
</td></tr>
<tr><td><code id="strAlloc_+3A_alloc">alloc</code></td>
<td>

<p>type of allocation; must be one of <code>"prop"</code>, <code>"neyman"</code>, <code>"totcost"</code>, <code>"totvar"</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>alloc="prop"</code> computes the proportional allocation of a fixed total sample size, <code>n.tot</code>, to the strata. <code>alloc="neyman"</code> computes the allocation of a fixed total sample size, <code>n.tot</code>, to the strata that minimizes the variance of an estimated mean. <code>alloc="totcost"</code> computes the allocation of a fixed total sample size, <code>n.tot</code>, to the strata that minimizes the variance of an estimated mean subject to the fixed total <code>cost</code>.  <code>alloc="totvar"</code> computes the allocation that minimizes total cost subject to the target coefficient of variation, <code>CV0</code>, or the target variance, <code>V0</code>, of the estimated mean.
</p>


<h3>Value</h3>

<p>For proportional allocation, a list with values:
</p>
<table>
<tr><td><code>alloc</code></td>
<td>
<p>type of allocation: <code>"prop"</code>, <code>"neyman"</code>, <code>"totcost"</code>, <code>"totvar"</code></p>
</td></tr>
<tr><td><code>Nh</code></td>
<td>
<p>vector of population sizes (<code class="reqn">N_h</code>) or pop stratum proportions (<code class="reqn">W_h</code>)</p>
</td></tr>
<tr><td><code>nh</code></td>
<td>
<p>vector of stratum sample sizes</p>
</td></tr>
<tr><td><code>"nh/n"</code></td>
<td>
<p>proportion of sample allocated to each stratum</p>
</td></tr>
</table>
<p>For other allocations, the three components above plus:
</p>
<table>
<tr><td><code>Sh</code></td>
<td>
<p>stratum unit standard deviations (<code class="reqn">S_h</code>)</p>
</td></tr>
<tr><td><code>"anticipated SE of estimated mean"</code></td>
<td>
<p>Anticipated SE of the estimated mean for the computed allocation</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Cochran, W.G. (1977). <em>Sampling Techniques</em>. John Wiley &amp; Sons.
</p>
<p>Valliant, R., Dever, J., Kreuter, F. (2018, chap. 3). <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nCont">nCont</a></code>, <code><a href="#topic+nLogOdds">nLogOdds</a></code>, <code><a href="#topic+nProp">nProp</a></code>, <code><a href="#topic+nPropMoe">nPropMoe</a></code>, <code><a href="#topic+nWilson">nWilson</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Neyman allocation
Nh &lt;- c(215, 65, 252, 50, 149, 144)
Sh &lt;- c(26787207, 10645109, 6909676, 11085034, 9817762, 44553355)
strAlloc(n.tot = 100, Nh = Nh, Sh = Sh, alloc = "neyman")

# cost constrained allocation
ch &lt;- c(1400, 200, 300, 600, 450, 1000)
strAlloc(Nh = Nh, Sh = Sh, cost = 100000, ch = ch, alloc = "totcost")

# allocation with CV target of 0.05
strAlloc(Nh = Nh, Sh = Sh, CV0 = 0.05, ch = ch, ybarU = 11664181, alloc = "totvar")
</code></pre>

<hr>
<h2 id='Test_Data_US'>
Accounting data for some US cities with latitude and longitude of the city centroids
</h2><span id='topic+Test_Data_US'></span>

<h3>Description</h3>

<p>A list of US cities with their latitude and longitude centroids and other data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Test_Data_US)</code></pre>


<h3>Format</h3>

<p>A data frame with 381 cities with the following variables:
</p>

<dl>
<dt><code>ID</code></dt><dd><p>Sequential ID field</p>
</dd>
<dt><code>State</code></dt><dd><p>State name</p>
</dd>
<dt><code>City</code></dt><dd><p>City name</p>
</dd>
<dt><code>Count</code></dt><dd><p>Number of records in city</p>
</dd>
<dt><code>Amount</code></dt><dd><p>Total dollar amount of records</p>
</dd>
<dt><code>lat</code></dt><dd><p>latitude of the city center</p>
</dd>
<dt><code>long</code></dt><dd><p>longitude of the city center</p>
</dd>
<dt><code>Y</code></dt><dd><p>Artificial analysis variable</p>
</dd>
</dl>
	


<h3>Details</h3>

<p>This population has 381 US cities with the latitude and longitude of the city center. It is used to illustrate the use of the <code>GeoDistPSU</code> and <code>GeoDistMOS</code> functions.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+GeoDistPSU">GeoDistPSU</a></code>,<code><a href="#topic+GeoDistMOS">GeoDistMOS</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Test_Data_US)
str(Test_Data_US)
</code></pre>

<hr>
<h2 id='ThirdGrade'>
Third grade population
</h2><span id='topic+ThirdGrade'></span>

<h3>Description</h3>

<p>The <code>ThirdGrade</code> data file is a population of students who participated in the Third International Mathematics and Science Study (TIMSS).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(ThirdGrade)</code></pre>


<h3>Format</h3>

<p>A data frame with 2,427 students on the following variables:
</p>

<dl>
<dt><code>region</code></dt><dd><p>Geographic region of the U.S.
(1 = Northeast;
2 = South;
3 = Central;
4 = West)
</p>
</dd>
<dt><code>school.id</code></dt><dd><p>School identifier (1 - 135)</p>
</dd>
<dt><code>student.id</code></dt><dd><p>Student identifier (1 - 2427)</p>
</dd>
<dt><code>sex</code></dt><dd><p>Sex of student
(1 = female;
2 = male)
</p>
</dd>
<dt><code>language</code></dt><dd><p>Is language of test spoken at home?
(1 = always;
2 = sometimes;
3 = never)
</p>
</dd>
<dt><code>math</code></dt><dd><p>Mathematics test score</p>
</dd>
<dt><code>ethnicity</code></dt><dd><p>Ethnicity of student
(1 = White, non-Hispanic;
2 = Black;
3 = Hispanic;
4 = Asian;
5 = Native American;
6 = Other)
</p>
</dd>
<dt><code>science</code></dt><dd><p>Science test score</p>
</dd>
<dt><code>community</code></dt><dd><p>Type of location of school
(2 = village or rural area;
3 = outskirts of a town or city; 
4 = close to center of a town or city)
</p>
</dd>
<dt><code>enrollment</code></dt><dd><p>Number of students in entire school</p>
</dd>
</dl>



<h3>Details</h3>

<p>The Third Grade population consists of 2,427 students in the U.S. who participated in the Third International Mathematics and Science Study (Caslyn, Gonzales, Frase 1999).  The methods used in conducting the original study are given in TIMSS International Study Center (1996). Clusters are schools while units within clusters are the students.  </p>


<h3>Source</h3>

<p>TIMSS International Study Center 1996.
</p>


<h3>References</h3>

<p>Caslyn, C., Gonzales, P., Frase, M. (1999). <em>Highlights from TIMSS</em>. National Center for Education Statistics, Washington DC.
</p>
<p>TIMSS International Study Center (1996). <em>Third International Mathematics and Science Study: Technical Report, Volume 1 Design and Development</em>. Boston College: Chestnut Hill MA.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ThirdGrade)
str(ThirdGrade)
</code></pre>

<hr>
<h2 id='TPV'>
TPV Data
</h2><span id='topic+TPV'></span>

<h3>Description</h3>

<p><code>TPV</code> is an example data file for illustrating the use of certainty (take-all) units in sampling
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(TPV)</code></pre>


<h3>Format</h3>

<p>A data frame with 67 observations on the following 2 variables:
</p>

<dl>
<dt><code>Total.Pot.Value</code></dt><dd><p>a measure of size for each unit; for example, maximum potential amount spent on a contract, i.e. base price plus all options.</p>
</dd>
<dt><code>Y</code></dt><dd><p>an analytic variable for each unit</p>
</dd>
</dl>



<h3>Details</h3>

<p>The <code>TPV</code> data are used as an example for <code>nContOpt</code> which determines the optimal split of a sample between take-all and non-take-all units.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nContOpt">nContOpt</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(TPV)
str(TPV)
</code></pre>

<hr>
<h2 id='unitVar'>
Compute the unit (population) variance for a variable
</h2><span id='topic+unitVar'></span>

<h3>Description</h3>

<p>Compute the unit (population) variance for a variable based on either a full population file or a sample from a finite population.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unitVar(pop.sw = NULL, w = NULL, p = NULL, y = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="unitVar_+3A_pop.sw">pop.sw</code></td>
<td>

<p>TRUE if the full population is input; FALSE if a sample is input
</p>
</td></tr>
<tr><td><code id="unitVar_+3A_w">w</code></td>
<td>

<p>vector of sample weights if <code>y</code> is a sample; used only if <code>pop.sw = FALSE</code>
</p>
</td></tr>
<tr><td><code id="unitVar_+3A_p">p</code></td>
<td>

<p>vector of 1-draw selection probabilities; optionally provided if <code>pop.sw = TRUE</code>
</p>
</td></tr>
<tr><td><code id="unitVar_+3A_y">y</code></td>
<td>

<p>vector of values of an analysis variable; must be numeric
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>unitVar</code> computes unit (population) variances of an analysis variable <code class="reqn">y</code> from either a population or a sample. <code>S2</code> is the unweighted population variance, <code class="reqn">S^2 = \sum_{i \in U}(y_i - \bar{y}_U)^2/(N-1)</code> where <code class="reqn">U</code> is the universe of elements, <code class="reqn">N</code> is the population size, and <code class="reqn">\bar{y}_U</code> is the population mean. If the input is a sample, <code>S2</code> is estimated as <code class="reqn">\hat{S}^2 = (n/(n-1))\sum_{i \in s} w_i(y_i - \bar{y}_w)^2/(\sum_{i \in s} w_i)</code> where <code class="reqn">s</code> is the set of sample elements, <code class="reqn">n</code> is the sample size, and <code class="reqn">\bar{y}_w</code> is the weighted sample mean.
</p>
<p><code>V1</code> is a weighted population variance used in calculations for samples where elements are selected with varying probabilities. If the <code class="reqn">y</code> is a population vector, <code class="reqn">V_1 = \sum_U p_i(y_i/p_i - t_U)^2</code> where <code class="reqn">p_i</code> is the 1-draw probability for element <code class="reqn">i</code> and <code class="reqn">t_U</code> is the population total of <code class="reqn">y</code>. If <code class="reqn">y</code> is for a sample, <code class="reqn">\hat{V}_1 = \sum_s (y_i/p_i - n^{-1}\sum_k y_k/p_k)^2 / (n-1)</code> with <code class="reqn">p_i</code> computed as <code class="reqn">1/(n w_i)</code>.
</p>


<h3>Value</h3>

<p>A list with three or four components:
</p>
<table>
<tr><td><code>Note</code></td>
<td>
<p>Describes whether output was computed from a full population or estimated from a sample.</p>
</td></tr>
<tr><td><code>Pop size N</code></td>
<td>
<p>Size of the population; included if <code>y</code> is for the full population.</p>
</td></tr>
<tr><td><code>S2</code></td>
<td>
<p>Unit variance of <code>y</code>; if <code>pop.sw = TRUE</code>, <code>S2</code> is computed from the full population; if <code>pop.sw = FALSE</code>, <code>S2</code> is estimated from the sample using the <code>w</code> weights.</p>
</td></tr>
<tr><td><code>V1</code></td>
<td>
<p>Population variance of <code>y</code> appropriate for a sample selected with varying probabilities; see Valliant, Dever, and Kreuter (VDK; 2018, sec. 3.4). If <code>pop.sw = TRUE</code> and <code>p</code> is provided, <code>V1</code> is computed with equation (3.32) in VDK. If <code>pop.sw = FALSE</code>, <code>V1</code> is estimated with equation (3.41) in VDK.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Richard Valliant
</p>


<h3>References</h3>

<p>Valliant, R., Dever, J., Kreuter, F. (2018, chap. 3). <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nCont">nCont</a></code>, <code><a href="#topic+nContMoe">nContMoe</a></code>, <code><a href="#topic+nContOpt">nContOpt</a></code>, <code><a href="#topic+nPPS">nPPS</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(PracTools)
data("smho.N874")
y &lt;- smho.N874[,"EXPTOTAL"]
x &lt;- smho.N874[, "BEDS"]
y &lt;- y[x&gt;0]
x &lt;- x[x&gt;0]
pik &lt;- x/sum(x)
require(sampling)
n &lt;- 50
sam &lt;- UPrandomsystematic(n * pik)
wts &lt;- 1/(n*pik[sam==1])
unitVar(pop.sw = TRUE, w = NULL, p = pik, y=y)
unitVar(pop.sw = FALSE, w = wts, p = NULL, y=y[sam==1])
</code></pre>

<hr>
<h2 id='wtd.moments'>
Compute moments of a variable from either a population or sample
</h2><span id='topic+wtd.moments'></span>

<h3>Description</h3>

<p>Compute the 2nd, 3rd, 4th moments, skewness, and kurtosis of a variable from either population or sample input
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wtd.moments(y, w=NULL, pop.sw=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wtd.moments_+3A_y">y</code></td>
<td>
<p>variable to be analyzed</p>
</td></tr>
<tr><td><code id="wtd.moments_+3A_w">w</code></td>
<td>
<p>vector of weights if the input is a sample</p>
</td></tr>
<tr><td><code id="wtd.moments_+3A_pop.sw">pop.sw</code></td>
<td>
<p>is the input for a population (<code>pop.sw=TRUE</code>) or for a sample (<code>pop.sw=FALSE</code>)?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code class="reqn">r^{th}</code> population moment is defined as <code class="reqn">m_r = (1/N) \sum_{k \in U} (y_k - \bar{y}_U)^r</code> where <em>U</em> is the set of population units, <em>N</em> is the population size, and <code class="reqn">\bar{y}_U</code> is the population mean. When the input is for the whole population, <code>wtd.moments</code> evaluates this directly for <code class="reqn">r=2, 3, 4</code>. When the input is for a sample, the <code class="reqn">r^{th}</code> moment is estimated as <code class="reqn">\hat{m}_r = (K/\hat{N}) \sum_{k \in s} ( w_k (y_k - \hat{\bar{y}}_U)^r ), r=2, 3, 4</code> where <code class="reqn">s</code> is the set of sample units, <code class="reqn">w_k</code> is the weight for sample unit <code class="reqn">k</code>, <code class="reqn">\hat{N} = \sum_s w_k</code>, and <code class="reqn">\hat{\bar{y}}_U = \sum_{k \in s} w_k y_k / \hat{N}</code>. When <code class="reqn">r=2</code>, <code class="reqn">K=n/(n-1)</code> so that the estimator equals the unbiased variance estimator if the sample is a simple random sample; if <code class="reqn">r=3,4</code>, then <code class="reqn">K=1</code>. The function also computes or estimates the population skewness, defined as <code class="reqn">m_3/m_2^{3/2}</code> and the population kurtosis, <code class="reqn">m_4/m_2^2</code>.
</p>
<p>The weights should be scaled for estimating population totals. The sample can be obtained from any complex design.
</p>


<h3>Value</h3>

<p>Vector with values:
</p>
<table>
<tr><td><code>m2</code></td>
<td>
<p>2nd moment</p>
</td></tr>
<tr><td><code>m3</code></td>
<td>
<p>3rd moment</p>
</td></tr>
<tr><td><code>m4</code></td>
<td>
<p>4th moment</p>
</td></tr>
<tr><td><code>skewness</code></td>
<td>
<p>skewness</p>
</td></tr>
<tr><td><code>kurtosis</code></td>
<td>
<p>kurtosis</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Valliant, R., Dever, J., Kreuter, F. (2018, sect. 3.4).  <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+wtdvar">wtdvar</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(PracTools)
wtd.moments(y = hospital$y, w = NULL)
require(sampling)
sam &lt;- strata(data = labor, stratanames = "h", size = c(30, 20, 10), method = c("srswor"),
              description=TRUE)
samdat &lt;- labor[sam$ID_unit,]
wtd.moments(y = samdat$WklyWage, w = 1/sam$Prob, pop.sw=FALSE)
</code></pre>

<hr>
<h2 id='wtdvar'>
Compute weighted variance
</h2><span id='topic+wtdvar'></span>

<h3>Description</h3>

<p>Compute an estimate of a population unit variance from a complex sample with survey weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wtdvar(x,w,na.rm=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wtdvar_+3A_x">x</code></td>
<td>

<p>data vector
</p>
</td></tr>
<tr><td><code id="wtdvar_+3A_w">w</code></td>
<td>

<p>vector of survey weights; must be same length as <code>x</code>
</p>
</td></tr>
<tr><td><code id="wtdvar_+3A_na.rm">na.rm</code></td>
<td>

<p>remove missing values (TRUE or FALSE)
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>wtdvar</code> is also used by <code><a href="#topic+BW3stagePPSe">BW3stagePPSe</a></code> in estimating relvariance components.
</p>


<h3>Value</h3>

<p>numeric estimate of population unit variance
</p>


<h3>Author(s)</h3>

<p>Richard Valliant, Jill A. Dever, Frauke Kreuter
</p>


<h3>References</h3>

<p>Valliant, R., Dever, J., Kreuter, F. (2018, chap. 9). <em>Practical Tools for Designing and Weighting Survey Samples, 2nd edition</em>. Springer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c(1:3)
wts &lt;- c(4, 6, 8)
wtdvar(x=x, w=wts)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
