<!DOCTYPE html><html><head><title>Help for package guidedPLS</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {guidedPLS}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#guidedPLS-package'>
<p>Supervised Dimensional Reduction by Guided Partial Least Squares</p></a></li>
<li><a href='#dummyMatrix'>
<p>Toy model data for using dNMF, dSVD, dsiNMF, djNMF, dPLS, dNTF, and dNTD</p></a></li>
<li><a href='#guidedPLS'>
<p>Guided Partial Least Squares (guied-PLS)</p></a></li>
<li><a href='#PLSSVD'>
<p>Partial Least Squares by Singular Value Decomposition (PLS-SVD)</p></a></li>
<li><a href='#softThr'>
<p>Soft-thresholding to make a sparse vector sparse</p></a></li>
<li><a href='#sPLSDA'>
<p>Sparse Partial Least Squares Discriminant Analysis (sPLS-DA)</p></a></li>
<li><a href='#toyModel'>
<p>Toy model data for using PLSSVD, sPLSDA, and guidedPLS</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Supervised Dimensional Reduction by Guided Partial Least Squares</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.0</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.4.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>irlba</td>
</tr>
<tr>
<td>Suggests:</td>
<td>fields, knitr, rmarkdown, testthat</td>
</tr>
<tr>
<td>Description:</td>
<td>Guided partial least squares (guided-PLS) is the combination of partial least squares by singular value decomposition (PLS-SVD) and guided principal component analysis (guided-PCA). For the details of the methods, see the reference section of GitHub README.md <a href="https://github.com/rikenbit/guidedPLS">https://github.com/rikenbit/guidedPLS</a>.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/rikenbit/guidedPLS">https://github.com/rikenbit/guidedPLS</a></td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-05-12 01:26:10 UTC; root</td>
</tr>
<tr>
<td>Author:</td>
<td>Koki Tsuyuzaki [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Koki Tsuyuzaki &lt;k.t.the-answer@hotmail.co.jp&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-05-12 03:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='guidedPLS-package'>
Supervised Dimensional Reduction by Guided Partial Least Squares
</h2><span id='topic+guidedPLS-package'></span>

<h3>Description</h3>

<p>Guided partial least squares (guided-PLS) is the combination of partial least squares by singular value decomposition (PLS-SVD) and guided principal component analysis (guided-PCA). For the details of the methods, see the reference section of GitHub README.md &lt;https://github.com/rikenbit/guidedPLS&gt;.
</p>


<h3>Details</h3>

<p>The DESCRIPTION file:
</p>

<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> guidedPLS</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Title: </td><td style="text-align: left;"> Supervised Dimensional Reduction by Guided Partial Least Squares</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.0.0</td>
</tr>
<tr>
 <td style="text-align: left;">
Authors@R: </td><td style="text-align: left;"> c(person("Koki", "Tsuyuzaki", role = c("aut", "cre"),
                      email = "k.t.the-answer@hotmail.co.jp"))</td>
</tr>
<tr>
 <td style="text-align: left;">
Depends: </td><td style="text-align: left;"> R (&gt;= 3.4.0)</td>
</tr>
<tr>
 <td style="text-align: left;">
Imports: </td><td style="text-align: left;"> irlba</td>
</tr>
<tr>
 <td style="text-align: left;">
Suggests: </td><td style="text-align: left;"> fields,
knitr,
rmarkdown,
testthat</td>
</tr>
<tr>
 <td style="text-align: left;">
Description: </td><td style="text-align: left;"> Guided partial least squares (guided-PLS) is the combination of partial least squares by singular value decomposition (PLS-SVD) and guided principal component analysis (guided-PCA). For the details of the methods, see the reference section of GitHub README.md &lt;https://github.com/rikenbit/guidedPLS&gt;.</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> MIT + file LICENSE</td>
</tr>
<tr>
 <td style="text-align: left;">
URL: </td><td style="text-align: left;"> https://github.com/rikenbit/guidedPLS</td>
</tr>
<tr>
 <td style="text-align: left;">
VignetteBuilder: </td><td style="text-align: left;"> knitr</td>
</tr>
<tr>
 <td style="text-align: left;">
Author: </td><td style="text-align: left;"> Koki Tsuyuzaki [aut, cre]</td>
</tr>
<tr>
 <td style="text-align: left;">
Maintainer: </td><td style="text-align: left;"> Koki Tsuyuzaki &lt;k.t.the-answer@hotmail.co.jp&gt;</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>


<p>Index of help topics:
</p>
<pre>
dummyMatrix             Toy model data for using dNMF, dSVD, dsiNMF,
                        djNMF, dPLS, dNTF, and dNTD
guidedPLS               Guided Partial Least Squares (guied-PLS)
guidedPLS-package       Supervised Dimensional Reduction by Guided
                        Partial Least Squares
PLSSVD                  Partial Least Squares by Singular Value
                        Decomposition (PLS-SVD)
softThr                 Soft-thresholding to make a sparse vector
                        sparse
sPLSDA                  Sparse Partial Least Squares Discriminant
                        Analysis (sPLS-DA)
toyModel                Toy model data for using PLSSVD, sPLSDA, and
                        guidedPLS
</pre>


<h3>Author(s)</h3>

<p>NA
</p>
<p>Maintainer: NA
</p>


<h3>References</h3>

<p>Le Cao, et al. (2008). A Sparse PLS for Variable Selection when Integrating Omics Data. <em>Statistical Applications in Genetics and Molecular Biology</em>, 7(1)
</p>
<p>Reese S E, et al. (2013). A new statistic for identifying batch effects in high-throughput genomic data that uses guided principal component analysis. <em>Bioinformatics</em>, 29(22), 2877-2883
</p>


<h3>See Also</h3>

<p><code><a href="#topic+toyModel">toyModel</a></code>,<code><a href="#topic+PLSSVD">PLSSVD</a></code>,<code><a href="#topic+sPLSDA">sPLSDA</a></code>,<code><a href="#topic+guidedPLS">guidedPLS</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>ls("package:guidedPLS")
</code></pre>

<hr>
<h2 id='dummyMatrix'>
Toy model data for using dNMF, dSVD, dsiNMF, djNMF, dPLS, dNTF, and dNTD
</h2><span id='topic+dummyMatrix'></span>

<h3>Description</h3>

<p>A label vector is converted to a dummy matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dummyMatrix(y, center=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dummyMatrix_+3A_y">y</code></td>
<td>

<p>A label vector to specify the group of data.
</p>
</td></tr>
<tr><td><code id="dummyMatrix_+3A_center">center</code></td>
<td>

<p>An option to center the rows of matrix (Default: TRUE).
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix is generated. The number of row is equal to the length of y and the number of columns is the number of unique elements of y.
</p>


<h3>Author(s)</h3>

<p>Koki Tsuyuzaki</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- c(1, 3, 2, 1, 4, 2)
dummyMatrix(y)
</code></pre>

<hr>
<h2 id='guidedPLS'>
Guided Partial Least Squares (guied-PLS)
</h2><span id='topic+guidedPLS'></span>

<h3>Description</h3>

<p>Four matrices X1, X2, Y1, and Y2 are required.
X1 and Y1 are supposed to share the rows,
X2 and Y2 are supposed to share the rows,
and Y1 and Y2 are supposed to share the columns.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>guidedPLS(X1, X2, Y1, Y2, k=.minDim(X1, X2, Y1, Y2),
    cortest=FALSE, fullrank=TRUE, verbose=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="guidedPLS_+3A_x1">X1</code></td>
<td>

<p>The input matrix which has N-rows and M-columns.
</p>
</td></tr>
<tr><td><code id="guidedPLS_+3A_y1">Y1</code></td>
<td>

<p>The input matrix which has N-rows and L-columns.
</p>
</td></tr>
<tr><td><code id="guidedPLS_+3A_x2">X2</code></td>
<td>

<p>The input matrix which has O-rows and P-columns.
</p>
</td></tr>
<tr><td><code id="guidedPLS_+3A_y2">Y2</code></td>
<td>

<p>The input matrix which has O-rows and L-columns.
</p>
</td></tr>
<tr><td><code id="guidedPLS_+3A_k">k</code></td>
<td>

<p>The number of low-dimension (k &lt; N, M, L, O, Default: .minDim(X1, X2, Y1, Y2))
</p>
</td></tr>
<tr><td><code id="guidedPLS_+3A_cortest">cortest</code></td>
<td>

<p>If cortest is set as TRUE, t-test of correlation coefficient is performed (Default: FALSE)
</p>
</td></tr>
<tr><td><code id="guidedPLS_+3A_fullrank">fullrank</code></td>
<td>

<p>If fullrank is set as TRUE, irlba is used, otherwise fullrank SVD is used (Default: TRUE)
</p>
</td></tr>
<tr><td><code id="guidedPLS_+3A_verbose">verbose</code></td>
<td>

<p>Verbose option (Default: FALSE)
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>res: object of svd()
loadingYX1: Loading vector to project X1 to lower dimension via Y1 (M times k).
loadingYX2: Loading vector to project X2 to lower dimension via Y2 (P times k).
scoreX1: Projected X1 (N times k)
scoreX2: Projected X2 (O times k)
scoreYX1: Projected YX1 (L times k)
scoreYX2: Projected YX2 (L times k)
corYX1: Correlation Coefficient (Default: NULL)
corYX2: Correlation Coefficient (Default: NULL)
pvalYX1: P-value vector of corYX1 (Default: NULL)
pvalYX2: P-value vector of corYX2 (Default: NULL)
qvalYX1: Q-value vector of BH method against pvalYX1 (Default: NULL)
qvalYX2: Q-value vector of BH method against pvalYX2 (Default: NULL)
</p>


<h3>Author(s)</h3>

<p>Koki Tsuyuzaki</p>


<h3>References</h3>

<p>Le Cao, et al. (2008). A Sparse PLS for Variable Selection when Integrating Omics Data. <em>Statistical Applications in Genetics and Molecular Biology</em>, 7(1)
</p>
<p>Reese S E, et al. (2013). A new statistic for identifying batch effects in high-throughput genomic data that uses guided principal component analysis. <em>Bioinformatics</em>, 29(22), 2877-2883
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Test data
data &lt;- toyModel()

# Simple usage
out &lt;- guidedPLS(X1=data$X1, X2=data$X2, Y1=data$Y1, Y2=data$Y2, k=4)
</code></pre>

<hr>
<h2 id='PLSSVD'>
Partial Least Squares by Singular Value Decomposition (PLS-SVD)
</h2><span id='topic+PLSSVD'></span>

<h3>Description</h3>

<p>Two matrices X and Y sharing a row are required
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PLSSVD(X, Y, k=.minDim(X, Y), cortest=FALSE,
  deflation=FALSE, fullrank=TRUE, verbose=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PLSSVD_+3A_x">X</code></td>
<td>

<p>The input matrix which has N-rows and M-columns.
</p>
</td></tr>
<tr><td><code id="PLSSVD_+3A_y">Y</code></td>
<td>

<p>The input matrix which has N-rows and L-columns.
</p>
</td></tr>
<tr><td><code id="PLSSVD_+3A_k">k</code></td>
<td>

<p>The number of low-dimension (k &lt; N, M, L, Default: .minDim(X, Y))
</p>
</td></tr>
<tr><td><code id="PLSSVD_+3A_cortest">cortest</code></td>
<td>

<p>If cortest is set as TRUE, t-test of correlation coefficient is performed (Default: FALSE)
</p>
</td></tr>
<tr><td><code id="PLSSVD_+3A_deflation">deflation</code></td>
<td>

<p>If deflation is set as TRUE, the score vectors are made orthogonal, otherwise the loading vectors are made orthogonal (Default: FALSE)
</p>
</td></tr>
<tr><td><code id="PLSSVD_+3A_fullrank">fullrank</code></td>
<td>

<p>If fullrank is set as TRUE, irlba is used, otherwise fullrank SVD is used (Default: TRUE)
</p>
</td></tr>
<tr><td><code id="PLSSVD_+3A_verbose">verbose</code></td>
<td>

<p>Verbose option (Default: FALSE)
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>scoreX : Score matrix which has M-rows and K-columns.
loadingX : Loading matrix which has N-rows and K-columns.
scoreY : Score matrix which has L-rows and K-columns.
loadingY : Loading matrix which has N-rows and K-columns.
d : K-length singular value vector of the cross-product matrix X'Y.
corX: Correlation Coefficient (Default: NULL)
corY: Correlation Coefficient (Default: NULL)
pvalX: P-value vector of corX (Default: NULL)
pvalY: P-value vector of corY (Default: NULL)
qvalX: Q-value vector of BH method against pvalX (Default: NULL)
qvalY: Q-value vector of BH method against pvalY (Default: NULL)
</p>


<h3>Author(s)</h3>

<p>Koki Tsuyuzaki</p>


<h3>References</h3>

<p>Le Cao, et al. (2008). A Sparse PLS for Variable Selection when Integrating Omics Data. <em>Statistical Applications in Genetics and Molecular Biology</em>, 7(1)
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Test data
data &lt;- toyModel()

# Simple usage
out &lt;- PLSSVD(X=data$X1, Y=data$Y1, k=4)
</code></pre>

<hr>
<h2 id='softThr'>
Soft-thresholding to make a sparse vector sparse
</h2><span id='topic+softThr'></span>

<h3>Description</h3>

<p>The degree of the sparseness of vector is controlled by the lambda parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>softThr(y, lambda=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="softThr_+3A_y">y</code></td>
<td>

<p>A numerical vector.
</p>
</td></tr>
<tr><td><code id="softThr_+3A_lambda">lambda</code></td>
<td>

<p>Threshold value to convert a value 0. If the absolute value of an element of vector is less than lambda, the value is converted to 0 (Default: 1).
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numerical vector, whose length is the same as that of y.
</p>


<h3>Author(s)</h3>

<p>Koki Tsuyuzaki</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- seq(-2, 2, 0.1)
softThr(y)
</code></pre>

<hr>
<h2 id='sPLSDA'>
Sparse Partial Least Squares Discriminant Analysis (sPLS-DA)
</h2><span id='topic+sPLSDA'></span>

<h3>Description</h3>

<p>Two matrices X and Y sharing a row are required
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sPLSDA(X, Y, k=.minDim(X, Y), cortest=FALSE, lambda=1, thr=1e-10, fullrank=TRUE,
    num.iter=10, verbose=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sPLSDA_+3A_x">X</code></td>
<td>

<p>The input matrix which has N-rows and M-columns.
</p>
</td></tr>
<tr><td><code id="sPLSDA_+3A_y">Y</code></td>
<td>

<p>The input matrix which has N-rows and L-columns.
</p>
</td></tr>
<tr><td><code id="sPLSDA_+3A_k">k</code></td>
<td>

<p>The number of low-dimension (k &lt; N, M, L, Default: .minDim(X, Y))
</p>
</td></tr>
<tr><td><code id="sPLSDA_+3A_cortest">cortest</code></td>
<td>

<p>If cortest is set as TRUE, t-test of correlation coefficient is performed (Default: FALSE)
</p>
</td></tr>
<tr><td><code id="sPLSDA_+3A_lambda">lambda</code></td>
<td>

<p>Penalty parameter to control the sparseness of u and v.
The larger the value, the sparser the solution (Default: 1).
</p>
</td></tr>
<tr><td><code id="sPLSDA_+3A_thr">thr</code></td>
<td>

<p>Threshold to stop the iteration (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="sPLSDA_+3A_fullrank">fullrank</code></td>
<td>

<p>If fullrank is set as TRUE, irlba is used, otherwise fullrank SVD is used (Default: TRUE)
</p>
</td></tr>
<tr><td><code id="sPLSDA_+3A_num.iter">num.iter</code></td>
<td>

<p>The number of iterations in each rank (Default: 10)
</p>
</td></tr>
<tr><td><code id="sPLSDA_+3A_verbose">verbose</code></td>
<td>

<p>Verbose option (Default: FALSE)
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>scoreX : Score matrix which has M-rows and K-columns.
loadingX : Loading matrix which has N-rows and K-columns.
scoreY : Score matrix which has L-rows and K-columns.
loadingY : Loading matrix which has N-rows and K-columns.
d : K-length singular value vector of the cross-product matrix X'Y.
corX: Correlation Coefficient (Default: NULL)
corY: Correlation Coefficient (Default: NULL)
pvalX: P-value vector of corX (Default: NULL)
pvalY: P-value vector of corY (Default: NULL)
qvalX: Q-value vector of BH method against pvalX (Default: NULL)
qvalY: Q-value vector of BH method against pvalY (Default: NULL)
</p>


<h3>Author(s)</h3>

<p>Koki Tsuyuzaki</p>


<h3>References</h3>

<p>Le Cao, et al. (2008). A Sparse PLS for Variable Selection when Integrating Omics Data. <em>Statistical Applications in Genetics and Molecular Biology</em>, 7(1)
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Test data
data &lt;- toyModel()

# Simple usage
out &lt;- sPLSDA(X=data$X1, Y=data$Y1, k=4)
</code></pre>

<hr>
<h2 id='toyModel'>
Toy model data for using PLSSVD, sPLSDA, and guidedPLS
</h2><span id='topic+toyModel'></span>

<h3>Description</h3>

<p>The data is used for confirming the algorithm are properly working.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>toyModel(model="Easy", seeds=123)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="toyModel_+3A_model">model</code></td>
<td>

<p>&quot;Easy&quot; and &quot;Hard&quot; are available (Default: &quot;Easy&quot;).
</p>
</td></tr>
<tr><td><code id="toyModel_+3A_seeds">seeds</code></td>
<td>

<p>Random number for setting set.seeds in the function (Default: 123).
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list object containing a set of matrices X1, X2, Y1, Y1_dummy, Y2, Y1_dummy.
</p>


<h3>Author(s)</h3>

<p>Koki Tsuyuzaki</p>


<h3>See Also</h3>

<p><code><a href="#topic+PLSSVD">PLSSVD</a></code>,<code><a href="#topic+sPLSDA">sPLSDA</a></code>,<code><a href="#topic+guidedPLS">guidedPLS</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- toyModel(seeds=123)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
