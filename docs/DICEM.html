<!DOCTYPE html><html lang="en"><head><title>Help for package DICEM</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {DICEM}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#basicSet'><p>Basic Features</p></a></li>
<li><a href='#DICE'><p>DICE Model Scores</p></a></li>
<li><a href='#diceNGrams'><p>Pre-trained advice concreteness features</p></a></li>
<li><a href='#featureSet'><p>DICE Features</p></a></li>
<li><a href='#phone_offers'><p>Purchase offers for phone</p></a></li>
<li><a href='#polymodel'><p>Polynomial pre-trained fit</p></a></li>
<li><a href='#uk2us'><p>UK to US Conversion dictionary</p></a></li>
<li><a href='#usWords'><p>UK to US conversion</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Directness and Intensity of Conflict Expression</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.0</td>
</tr>
<tr>
<td>Description:</td>
<td>A Natural Language Processing Model trained to detect directness and intensity during conflict. See <a href="https://www.mikeyeomans.info">https://www.mikeyeomans.info</a>.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>politeness, stringr, doc2concrete, vader, Matrix, quanteda,
xgboost</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, spacyr, rmarkdown, testthat</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-08-16 13:30:39 UTC; myeomans</td>
</tr>
<tr>
<td>Author:</td>
<td>Michael Yeomans [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Michael Yeomans &lt;mk.yeomans@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-08-21 12:50:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='basicSet'>Basic Features</h2><span id='topic+basicSet'></span>

<h3>Description</h3>

<p>Simple features as inputs to the DICE model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>basicSet(text)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="basicSet_+3A_text">text</code></td>
<td>
<p>character A vector of texts, each of which will be tallied for DICE features.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The DICE models use, as features, linear and quadratic terms for sentiment, emotion, and word count.
</p>


<h3>Value</h3>

<p>a data.frame of feature scores for the pre-trained models.
</p>

<hr>
<h2 id='DICE'>DICE Model Scores</h2><span id='topic+DICE'></span>

<h3>Description</h3>

<p>Detects linguistic markers of politeness in natural language.
Takes an N-length vector of text documents and returns an N-row data.frame of scores on the two DICE dimensions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DICE(text, parser = c("none", "spacy"), uk_english = FALSE, num_mc_cores = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="DICE_+3A_text">text</code></td>
<td>
<p>character A vector of texts, each of which will be tallied for DICE features.</p>
</td></tr>
<tr><td><code id="DICE_+3A_parser">parser</code></td>
<td>
<p>character Name of dependency parser to use (see details). Without a dependency parser, some features will be approximated, while others cannot be calculated at all.</p>
</td></tr>
<tr><td><code id="DICE_+3A_uk_english">uk_english</code></td>
<td>
<p>logical Does the text contain any British English spelling? Including variants (e.g. Canadian). Default is FALSE</p>
</td></tr>
<tr><td><code id="DICE_+3A_num_mc_cores">num_mc_cores</code></td>
<td>
<p>integer Number of cores for parallelization. Default is 1, but we encourage users to try parallel::detectCores() if possible.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The best intensity model uses politeness features, which depend on part-of-speech tagged sentences (e.g. &quot;bare commands&quot; are a particular verb class).
To include these features in the analysis, a POS tagger must be initialized beforehand - we currently support SpaCy which must
be installed separately in Python (see example for implementation). If not, a simpler model can be used, though it is somewhat less accruate.
</p>


<h3>Value</h3>

<p>a data.frame of scores on directness and intensity.
</p>


<h3>References</h3>

<p>Weingart et al., 2015
Yeomans et al., 2024
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("phone_offers")

DICE(phone_offers$message[1:10], parser="none")

## Not run: 

# Detect multiple cores automatically for parallel processing
DICE(phone_offers$message, num_mc_cores=parallel::detectCores())

# Connect to SpaCy installation for part-of-speech features
# THIS REQUIRES SPACY INSTALLATION OUTSIDE OF R
# For some machines, spacyr::spacy_install() will work, but please confirm before running
spacyr::spacy_initialize(python_executable = PYTHON_PATH)
DICE(phone_offers$message, parser="spacy")

## End(Not run)


</code></pre>

<hr>
<h2 id='diceNGrams'>Pre-trained advice concreteness features</h2><span id='topic+diceNGrams'></span>

<h3>Description</h3>

<p>For internal use only. This dataset demonstrates the ngram features that are used for the pre-trained models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>diceNGrams
</code></pre>


<h3>Format</h3>

<p>A (truncated) matrix of ngram feature counts for alignment to the pre-trained glmnet models.
</p>


<h3>Source</h3>

<p>Yeomans et al., (2024). A Natural Language Processing Model for Conflict Expression in Conversation
</p>

<hr>
<h2 id='featureSet'>DICE Features</h2><span id='topic+featureSet'></span>

<h3>Description</h3>

<p>Extracts feature sets to match pre-trained models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>featureSet(text, parser = c("none", "spacy"), num_mc_cores = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="featureSet_+3A_text">text</code></td>
<td>
<p>character A vector of texts, each of which will be tallied for politeness features.</p>
</td></tr>
<tr><td><code id="featureSet_+3A_parser">parser</code></td>
<td>
<p>character Name of dependency parser to use (see details). Without a dependency parser, the politeness features are excluded from the model.</p>
</td></tr>
<tr><td><code id="featureSet_+3A_num_mc_cores">num_mc_cores</code></td>
<td>
<p>integer Number of cores for parallelization. Default is 1, but we encourage users to try parallel::detectCores() if possible.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The politeness features depend on part-of-speech tagged sentences (e.g. &quot;bare commands&quot; are a particular verb class).
To include these features in the analysis, a POS tagger must be initialized beforehand - we currently support SpaCy which must
be installed separately in Python (see example for implementation).
</p>


<h3>Value</h3>

<p>a data.frame of  features, matching the pre-trained model set
</p>

<hr>
<h2 id='phone_offers'>Purchase offers for phone</h2><span id='topic+phone_offers'></span>

<h3>Description</h3>

<p>A dataset containing the purchase offer message and a
label indicating if the writer was assigned to be warm (1) or tough (0)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>phone_offers
</code></pre>


<h3>Format</h3>

<p>A data frame with 355 rows and 2 variables:
</p>

<dl>
<dt>message</dt><dd><p>character of purchase offer message</p>
</dd>
<dt>condition</dt><dd><p>binary label indicating if message is warm or tough</p>
</dd>
</dl>



<h3>Source</h3>

<p>Jeong, M., Minson, J., Yeomans, M. &amp; Gino, F. (2019).
</p>
<p>&quot;Communicating Warmth in Distributed Negotiations is Surprisingly Ineffective.&quot;
</p>
<p>Study 1. <a href="https://osf.io/t7sd6/">https://osf.io/t7sd6/</a>
</p>

<hr>
<h2 id='polymodel'>Polynomial pre-trained fit</h2><span id='topic+polymodel'></span>

<h3>Description</h3>

<p>This calculates the polynomial projection of the simple features used during model training
</p>


<h3>Usage</h3>

<pre><code class='language-R'>polymodel
</code></pre>


<h3>Format</h3>

<p>A pre-trained polynomial equation
</p>

<hr>
<h2 id='uk2us'>UK to US Conversion dictionary</h2><span id='topic+uk2us'></span>

<h3>Description</h3>

<p>For internal use only. This dataset contains a quanteda dictionary for converting UK words to US words. The models in this package were all trained on US English.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>uk2us
</code></pre>


<h3>Format</h3>

<p>A quanteda dictionary with named entries. Names are the US version, and entries are the UK version.
</p>


<h3>Source</h3>

<p>Borrowed from the quanteda.dictionaries package on github (from user kbenoit)
</p>

<hr>
<h2 id='usWords'>UK to US conversion</h2><span id='topic+usWords'></span>

<h3>Description</h3>

<p>background function to load.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>usWords(text)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="usWords_+3A_text">text</code></td>
<td>
<p>character Vector of strings to convert to US spelling.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>character Vector of Americanized strings.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
