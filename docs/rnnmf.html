<!DOCTYPE html><html lang="en"><head><title>Help for package rnnmf</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {rnnmf}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#rnnmf'><p>regularized non-negative matrix factorization</p></a></li>
<li><a href='#aurnmf'><p>nmf .</p></a></li>
<li><a href='#gaurnmf'><p>gaurnmf .</p></a></li>
<li><a href='#giqpm'><p>giqpm .</p></a></li>
<li><a href='#murnmf'><p>murnmf .</p></a></li>
<li><a href='#rnnmf-NEWS'><p>News for package 'rnnmf':</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Maintainer:</td>
<td>Steven E. Pav &lt;shabbychef@gmail.com&gt;</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-10-30</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/LGPL-3">LGPL-3</a></td>
</tr>
<tr>
<td>Title:</td>
<td>Regularized Non-Negative Matrix Factorization</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/shabbychef/rnnmf/issues">https://github.com/shabbychef/rnnmf/issues</a></td>
</tr>
<tr>
<td>Description:</td>
<td>A proof of concept implementation of regularized non-negative matrix factorization optimization.
    A non-negative matrix factorization factors non-negative matrix Y approximately as L R, for non-negative
    matrices L and R of reduced rank. This package supports such factorizations with weighted objective and
    regularization penalties. Allowable regularization penalties include L1 and L2 penalties on L and R,
    as well as non-orthogonality penalties. This package provides multiplicative update algorithms, which are
    a modification of the algorithm of Lee and Seung (2001)
    <a href="http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf">http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf</a>, as well
    as an additive update derived from that multiplicative update.  See also Pav (2004) &lt;<a href="https://doi.org/10.48550%2FarXiv.2410.22698">doi:10.48550/arXiv.2410.22698</a>&gt;.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0.2)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Matrix</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, dplyr, ggplot2, scales, viridis, knitr</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/shabbychef/rnnmf">https://github.com/shabbychef/rnnmf</a></td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Collate:</td>
<td>'aurnmf.r' 'gaurnmf.r' 'giqpm.r' 'murnmf.r' 'rnnmf-package.r'</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-10-31 03:07:08 UTC; spav</td>
</tr>
<tr>
<td>Author:</td>
<td>Steven E. Pav <a href="https://orcid.org/0000-0002-4197-6195"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-11-04 10:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='rnnmf'>regularized non-negative matrix factorization</h2><span id='topic+rnnmf-package'></span><span id='topic+rnnmf'></span>

<h3>Description</h3>

<p>Regularized Non-negative Matrix Factorization.
</p>


<h3>Legal Mumbo Jumbo</h3>

<p>rnnmf is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU Lesser General Public License for more details.
</p>


<h3>Note</h3>

<p>This package provides proof of concept code which is unlikely to be fast
or robust, and may not solve the optimization problem at hand. User assumes
all risk.
</p>
<p>This package is maintained as a hobby.
</p>


<h3>Author(s)</h3>

<p>Steven E. Pav <a href="mailto:shabbychef@gmail.com">shabbychef@gmail.com</a>
</p>
<p><strong>Maintainer</strong>: Steven E. Pav <a href="mailto:shabbychef@gmail.com">shabbychef@gmail.com</a> (<a href="https://orcid.org/0000-0002-4197-6195">ORCID</a>)
</p>


<h3>References</h3>

<p>Lee, Daniel D. and Seung, H. Sebastian. &quot;Algorithms for Non-negative Matrix 
Factorization.&quot; Advances in Neural Information Processing Systems 13 (2001):
556&ndash;562.
<a href="http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf">http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf</a>
</p>
<p>Pav, S. E. &quot;An Iterative Algorithm for Regularized Non-negative Matrix Factorizations.&quot;
Forthcoming. (2024)
</p>
<p>Pav, Steven E. &quot;System and method for unmixing spectroscopic observations with nonnegative 
matrix factorization.&quot; US Patent 8140272, 2012.
<a href="https://patentscope.wipo.int/search/en/detail.jsf?docId=US42758160">https://patentscope.wipo.int/search/en/detail.jsf?docId=US42758160</a>
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/shabbychef/rnnmf">https://github.com/shabbychef/rnnmf</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/shabbychef/rnnmf/issues">https://github.com/shabbychef/rnnmf/issues</a>
</p>
</li></ul>


<hr>
<h2 id='aurnmf'>nmf .</h2><span id='topic+aurnmf'></span>

<h3>Description</h3>

<p>Additive update Non-negative matrix factorization with regularization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aurnmf(
  Y,
  L,
  R,
  W_0R = NULL,
  W_0C = NULL,
  lambda_1L = 0,
  lambda_1R = 0,
  lambda_2L = 0,
  lambda_2R = 0,
  gamma_2L = 0,
  gamma_2R = 0,
  tau = 0.1,
  annealing_rate = 0.01,
  check_optimal_step = TRUE,
  zero_tolerance = 1e-12,
  max_iterations = 1000L,
  min_xstep = 1e-09,
  on_iteration_end = NULL,
  verbosity = 0
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="aurnmf_+3A_y">Y</code></td>
<td>
<p>an <code class="reqn">r \times c</code> matrix to be decomposed.
Should have non-negative elements; an error is thrown otherwise.</p>
</td></tr>
<tr><td><code id="aurnmf_+3A_l">L</code></td>
<td>
<p>an <code class="reqn">r \times d</code> matrix of the initial estimate of L.
Should have non-negative elements; an error is thrown otherwise.</p>
</td></tr>
<tr><td><code id="aurnmf_+3A_r">R</code></td>
<td>
<p>an <code class="reqn">d \times c</code> matrix of the initial estimate of R.
Should have non-negative elements; an error is thrown otherwise.</p>
</td></tr>
<tr><td><code id="aurnmf_+3A_w_0r">W_0R</code></td>
<td>
<p>the row space weighting matrix.
This should be a positive definite non-negative symmetric <code class="reqn">r \times r</code> matrix.
If omitted, it defaults to the properly sized identity matrix.</p>
</td></tr>
<tr><td><code id="aurnmf_+3A_w_0c">W_0C</code></td>
<td>
<p>the column space weighting matrix.
This should be a positive definite non-negative symmetric <code class="reqn">c \times c</code> matrix.
If omitted, it defaults to the properly sized identity matrix.</p>
</td></tr>
<tr><td><code id="aurnmf_+3A_lambda_1l">lambda_1L</code></td>
<td>
<p>the scalar <code class="reqn">\ell_1</code> penalty for the matrix <code class="reqn">L</code>.
Defaults to zero.</p>
</td></tr>
<tr><td><code id="aurnmf_+3A_lambda_1r">lambda_1R</code></td>
<td>
<p>the scalar <code class="reqn">\ell_1</code> penalty for the matrix <code class="reqn">R</code>.
Defaults to zero.</p>
</td></tr>
<tr><td><code id="aurnmf_+3A_lambda_2l">lambda_2L</code></td>
<td>
<p>the scalar <code class="reqn">\ell_2</code> penalty for the matrix <code class="reqn">L</code>.
Defaults to zero.</p>
</td></tr>
<tr><td><code id="aurnmf_+3A_lambda_2r">lambda_2R</code></td>
<td>
<p>the scalar <code class="reqn">\ell_2</code> penalty for the matrix <code class="reqn">R</code>.
Defaults to zero.</p>
</td></tr>
<tr><td><code id="aurnmf_+3A_gamma_2l">gamma_2L</code></td>
<td>
<p>the scalar <code class="reqn">\ell_2</code> penalty for non-orthogonality of the matrix <code class="reqn">L</code>.
Defaults to zero.</p>
</td></tr>
<tr><td><code id="aurnmf_+3A_gamma_2r">gamma_2R</code></td>
<td>
<p>the scalar <code class="reqn">\ell_2</code> penalty for non-orthogonality of the matrix <code class="reqn">R</code>.
Defaults to zero.</p>
</td></tr>
<tr><td><code id="aurnmf_+3A_tau">tau</code></td>
<td>
<p>the starting shrinkage factor applied to the step length.
Should be a value in <code class="reqn">(0,1)</code>.</p>
</td></tr>
<tr><td><code id="aurnmf_+3A_annealing_rate">annealing_rate</code></td>
<td>
<p>the rate at which we scale the shrinkage factor towards 1.
Should be a value in <code class="reqn">[0,1)</code>.</p>
</td></tr>
<tr><td><code id="aurnmf_+3A_check_optimal_step">check_optimal_step</code></td>
<td>
<p>if TRUE, we attempt to take the optimal step
length in the given direction. If not, we merely take the longest feasible
step in the step direction.</p>
</td></tr>
<tr><td><code id="aurnmf_+3A_zero_tolerance">zero_tolerance</code></td>
<td>
<p>values of <code class="reqn">x</code> less than this will be &lsquo;snapped&rsquo; to zero.
This happens at the end of the iteration and does not affect the measurement
of convergence.</p>
</td></tr>
<tr><td><code id="aurnmf_+3A_max_iterations">max_iterations</code></td>
<td>
<p>the maximum number of iterations to perform.</p>
</td></tr>
<tr><td><code id="aurnmf_+3A_min_xstep">min_xstep</code></td>
<td>
<p>the minimum L-infinity norm of the step taken.
Once the step falls under this value, we terminate.</p>
</td></tr>
<tr><td><code id="aurnmf_+3A_on_iteration_end">on_iteration_end</code></td>
<td>
<p>an optional function that is called at the end of
each iteration. The function is called as 
<code>on_iteration_end(iteration=iteration, Y=Y, L=L, R=R, Lstep=Lstep, Rstep=Rstep, ...)</code></p>
</td></tr>
<tr><td><code id="aurnmf_+3A_verbosity">verbosity</code></td>
<td>
<p>controls whether we print information to the console.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Attempts to factor given non-negative matrix <code class="reqn">Y</code> as the product <code class="reqn">LR</code>
of two non-negative matrices. The objective function is Frobenius norm
with <code class="reqn">\ell_1</code> and <code class="reqn">\ell_2</code> regularization terms.
We seek to minimize the objective
</p>
<p style="text-align: center;"><code class="reqn">\frac{1}{2}tr((Y-LR)' W_{0R} (Y-LR) W_{0C}) + \lambda_{1L} |L| + \lambda_{1R} |R| + \frac{\lambda_{2L}}{2} tr(L'L) + \frac{\lambda_{2R}}{2} tr(R'R) + \frac{\gamma_{2L}}{2} tr((L'L) (11' - I)) + \frac{\gamma_{2R}}{2} tr((R'R) (11' - I)),</code>
</p>

<p>subject to <code class="reqn">L \ge 0</code> and <code class="reqn">R \ge 0</code> elementwise, 
where <code class="reqn">|A|</code> is the sum of the elements of <code class="reqn">A</code> and 
<code class="reqn">tr(A)</code> is the trace of <code class="reqn">A</code>.
</p>
<p>The code starts from initial estimates and iteratively 
improves them, maintaining non-negativity.
This implementation uses the Lee and Seung step direction,
with a correction to avoid divide-by-zero.
The iterative step is optionally re-scaled to take the steepest 
descent in the step direction.
</p>


<h3>Value</h3>

<p>a list with the elements
</p>

<dl>
<dt>L</dt><dd><p>The final estimate of L.</p>
</dd>
<dt>R</dt><dd><p>The final estimate of R.</p>
</dd>
<dt>Lstep</dt><dd><p>The infinity norm of the final step in L.</p>
</dd>
<dt>Rstep</dt><dd><p>The infinity norm of the final step in R.</p>
</dd>
<dt>iterations</dt><dd><p>The number of iterations taken.</p>
</dd>
<dt>converged</dt><dd><p>Whether convergence was detected.</p>
</dd>
</dl>



<h3>Note</h3>

<p>This package provides proof of concept code which is unlikely to be fast
or robust, and may not solve the optimization problem at hand. User assumes
all risk.
</p>


<h3>Author(s)</h3>

<p>Steven E. Pav <a href="mailto:shabbychef@gmail.com">shabbychef@gmail.com</a>
</p>


<h3>References</h3>

<p>Merritt, Michael, and Zhang, Yin. &quot;Interior-point Gradient Method for Large-Scale Totally 
Nonnegative Least Squares Problems.&quot; Journal of Optimization Theory and Applications 126, 
no 1 (2005): 191&ndash;202. <a href="https://scholarship.rice.edu/bitstream/handle/1911/102020/TR04-08.pdf">https://scholarship.rice.edu/bitstream/handle/1911/102020/TR04-08.pdf</a>
</p>
<p>Pav, S. E. &quot;An Iterative Algorithm for Regularized Non-negative Matrix Factorizations.&quot;
Forthcoming. (2024)
</p>
<p>Lee, Daniel D. and Seung, H. Sebastian. &quot;Algorithms for Non-negative Matrix 
Factorization.&quot; Advances in Neural Information Processing Systems 13 (2001):
556&ndash;562.
<a href="http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf">http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gaurnmf">gaurnmf</a></code>, <code><a href="#topic+murnmf">murnmf</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 nr &lt;- 100
 nc &lt;- 20
 dm &lt;- 4

 randmat &lt;- function(nr,nc,...) { matrix(pmax(0,runif(nr*nc,...)),nrow=nr) }
 set.seed(1234)
 real_L &lt;- randmat(nr,dm)
 real_R &lt;- randmat(dm,nc)
 Y &lt;- real_L %*% real_R
# without regularization
 objective &lt;- function(Y, L, R) { sum((Y - L %*% R)^2) }
 objective(Y,real_L,real_R)

 L_0 &lt;- randmat(nr,dm)
 R_0 &lt;- randmat(dm,nc)
 objective(Y,L_0,R_0)
 out1 &lt;- aurnmf(Y, L_0, R_0, max_iterations=5e3L,check_optimal_step=FALSE)
 objective(Y,out1$L,out1$R)
# with L1 regularization on one side
 out2 &lt;- aurnmf(Y, L_0, R_0, lambda_1L=0.1, max_iterations=5e3L,check_optimal_step=FALSE)
# objective does not suffer because all mass is shifted to R
 objective(Y,out2$L,out2$R)
list(L1=sum(out1$L),R1=sum(out1$R),L2=sum(out2$L),R2=sum(out2$R))
sum(out2$L)
# with L1 regularization on both sides
 out3 &lt;- aurnmf(Y, L_0, R_0, lambda_1L=0.1,lambda_1R=0.1,
     max_iterations=5e3L,check_optimal_step=FALSE)
# with L1 regularization on both sides, raw objective suffers
 objective(Y,out3$L,out3$R)
list(L1=sum(out1$L),R1=sum(out1$R),L3=sum(out3$L),R3=sum(out3$R))


# example showing how to use the on_iteration_end callback to save iterates.
max_iterations &lt;- 5e3L
it_history &lt;&lt;- rep(NA_real_, max_iterations)
quadratic_objective &lt;- function(Y, L, R) { sum((Y - L %*% R)^2) }
on_iteration_end &lt;- function(iteration, Y, L, R, ...) {
  it_history[iteration] &lt;&lt;- quadratic_objective(Y,L,R)
}
out1b &lt;- aurnmf(Y, L_0, R_0, max_iterations=max_iterations, on_iteration_end=on_iteration_end)


# should work on sparse matrices too.
if (require(Matrix)) { 
 real_L &lt;- randmat(nr,dm,min=-1)
 real_R &lt;- randmat(dm,nc,min=-1)
 Y &lt;- as(real_L %*% real_R, "sparseMatrix")
 L_0 &lt;- as(randmat(nr,dm,min=-0.5), "sparseMatrix")
 R_0 &lt;- as(randmat(dm,nc,min=-0.5), "sparseMatrix")
 out1 &lt;- aurnmf(Y, L_0, R_0, max_iterations=1e2L,check_optimal_step=TRUE)
}

</code></pre>

<hr>
<h2 id='gaurnmf'>gaurnmf .</h2><span id='topic+gaurnmf'></span>

<h3>Description</h3>

<p>Additive update Non-negative matrix factorization with regularization, general form.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gaurnmf(
  Y,
  L,
  R,
  W_0R = NULL,
  W_0C = NULL,
  W_1L = 0,
  W_1R = 0,
  W_2RL = 0,
  W_2CL = 0,
  W_2RR = 0,
  W_2CR = 0,
  tau = 0.1,
  annealing_rate = 0.01,
  check_optimal_step = TRUE,
  zero_tolerance = 1e-12,
  max_iterations = 1000L,
  min_xstep = 1e-09,
  on_iteration_end = NULL,
  verbosity = 0
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gaurnmf_+3A_y">Y</code></td>
<td>
<p>an <code class="reqn">r \times c</code> matrix to be decomposed.
Should have non-negative elements; an error is thrown otherwise.</p>
</td></tr>
<tr><td><code id="gaurnmf_+3A_l">L</code></td>
<td>
<p>an <code class="reqn">r \times d</code> matrix of the initial estimate of L.
Should have non-negative elements; an error is thrown otherwise.</p>
</td></tr>
<tr><td><code id="gaurnmf_+3A_r">R</code></td>
<td>
<p>an <code class="reqn">d \times c</code> matrix of the initial estimate of R.
Should have non-negative elements; an error is thrown otherwise.</p>
</td></tr>
<tr><td><code id="gaurnmf_+3A_w_0r">W_0R</code></td>
<td>
<p>the row space weighting matrix.
This should be a positive definite non-negative symmetric <code class="reqn">r \times r</code> matrix.
If omitted, it defaults to the properly sized identity matrix.</p>
</td></tr>
<tr><td><code id="gaurnmf_+3A_w_0c">W_0C</code></td>
<td>
<p>the column space weighting matrix.
This should be a positive definite non-negative symmetric <code class="reqn">c \times c</code> matrix.
If omitted, it defaults to the properly sized identity matrix.</p>
</td></tr>
<tr><td><code id="gaurnmf_+3A_w_1l">W_1L</code></td>
<td>
<p>the <code class="reqn">\ell_1</code> penalty matrix for the matrix <code class="reqn">R</code>.
If a scalar, corresponds to that scalar times the all-ones matrix.
Defaults to all-zeroes matrix, which is no penalty term.</p>
</td></tr>
<tr><td><code id="gaurnmf_+3A_w_1r">W_1R</code></td>
<td>
<p>the <code class="reqn">\ell_1</code> penalty matrix for the matrix <code class="reqn">L</code>.
If a scalar, corresponds to that scalar times the all-ones matrix.
Defaults to all-zeroes matrix, which is no penalty term.</p>
</td></tr>
<tr><td><code id="gaurnmf_+3A_w_2rl">W_2RL</code></td>
<td>
<p>the <code class="reqn">\ell_2</code> row penalty matrix for the matrix <code class="reqn">L</code>.
If a scalar, corresponds to that scalar times the identity matrix.
Can also be a list, in which case <code>W_2CL</code> must be a list of the same
length. The list should consist of <code class="reqn">\ell_2</code> row penalty matrices.
Defaults to all-zeroes matrix, which is no penalty term.</p>
</td></tr>
<tr><td><code id="gaurnmf_+3A_w_2cl">W_2CL</code></td>
<td>
<p>the <code class="reqn">\ell_2</code> column penalty matrix for the matrix <code class="reqn">L</code>.
If a scalar, corresponds to that scalar times the identity matrix.
Can also be a list, in which case <code>W_2RL</code> must be a list of the same
length. The list should consist of <code class="reqn">\ell_2</code> column penalty matrices.
Defaults to all-zeroes matrix, which is no penalty term.</p>
</td></tr>
<tr><td><code id="gaurnmf_+3A_w_2rr">W_2RR</code></td>
<td>
<p>the <code class="reqn">\ell_2</code> row penalty matrix for the matrix <code class="reqn">R</code>.
If a scalar, corresponds to that scalar times the identity matrix.
Can also be a list, in which case <code>W_2CR</code> must be a list of the same
length. The list should consist of <code class="reqn">\ell_2</code> row penalty matrices.
Defaults to all-zeroes matrix, which is no penalty term.</p>
</td></tr>
<tr><td><code id="gaurnmf_+3A_w_2cr">W_2CR</code></td>
<td>
<p>the <code class="reqn">\ell_2</code> column penalty matrix for the matrix <code class="reqn">R</code>.
If a scalar, corresponds to that scalar times the identity matrix.
Can also be a list, in which case <code>W_2RR</code> must be a list of the same
length. The list should consist of <code class="reqn">\ell_2</code> column penalty matrices.
Defaults to all-zeroes matrix, which is no penalty term.</p>
</td></tr>
<tr><td><code id="gaurnmf_+3A_tau">tau</code></td>
<td>
<p>the starting shrinkage factor applied to the step length.
Should be a value in <code class="reqn">(0,1)</code>.</p>
</td></tr>
<tr><td><code id="gaurnmf_+3A_annealing_rate">annealing_rate</code></td>
<td>
<p>the rate at which we scale the shrinkage factor towards 1.
Should be a value in <code class="reqn">[0,1)</code>.</p>
</td></tr>
<tr><td><code id="gaurnmf_+3A_check_optimal_step">check_optimal_step</code></td>
<td>
<p>if TRUE, we attempt to take the optimal step
length in the given direction. If not, we merely take the longest feasible
step in the step direction.</p>
</td></tr>
<tr><td><code id="gaurnmf_+3A_zero_tolerance">zero_tolerance</code></td>
<td>
<p>values of <code class="reqn">x</code> less than this will be &lsquo;snapped&rsquo; to zero.
This happens at the end of the iteration and does not affect the measurement
of convergence.</p>
</td></tr>
<tr><td><code id="gaurnmf_+3A_max_iterations">max_iterations</code></td>
<td>
<p>the maximum number of iterations to perform.</p>
</td></tr>
<tr><td><code id="gaurnmf_+3A_min_xstep">min_xstep</code></td>
<td>
<p>the minimum L-infinity norm of the step taken.
Once the step falls under this value, we terminate.</p>
</td></tr>
<tr><td><code id="gaurnmf_+3A_on_iteration_end">on_iteration_end</code></td>
<td>
<p>an optional function that is called at the end of
each iteration. The function is called as 
<code>on_iteration_end(iteration=iteration, Y=Y, L=L, R=R, Lstep=Lstep, Rstep=Rstep, ...)</code></p>
</td></tr>
<tr><td><code id="gaurnmf_+3A_verbosity">verbosity</code></td>
<td>
<p>controls whether we print information to the console.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Attempts to factor given non-negative matrix <code class="reqn">Y</code> as the product <code class="reqn">LR</code>
of two non-negative matrices. The objective function is Frobenius norm
with <code class="reqn">\ell_1</code> and <code class="reqn">\ell_2</code> regularization terms.
We seek to minimize the objective
</p>
<p style="text-align: center;"><code class="reqn">\frac{1}{2}tr((Y-LR)' W_{0R} (Y-LR) W_{0C}) + tr(W_{1L}'L) + tr(W_{1R}'R) + \frac{1}{2} \sum_j tr(L'W_{2RLj}LW_{2CLj}) + tr(R'W_{2RRj}RW_{2CRj}),</code>
</p>

<p>subject to <code class="reqn">L \ge 0</code> and <code class="reqn">R \ge 0</code> elementwise, 
where <code class="reqn">tr(A)</code> is the trace of <code class="reqn">A</code>.
</p>
<p>The code starts from initial estimates and iteratively 
improves them, maintaining non-negativity.
This implementation uses the Lee and Seung step direction,
with a correction to avoid divide-by-zero.
The iterative step is optionally re-scaled to take the steepest 
descent in the step direction.
</p>


<h3>Value</h3>

<p>a list with the elements
</p>

<dl>
<dt>L</dt><dd><p>The final estimate of L.</p>
</dd>
<dt>R</dt><dd><p>The final estimate of R.</p>
</dd>
<dt>Lstep</dt><dd><p>The infinity norm of the final step in L</p>
</dd></dl>
<p>.
</p>
<dl>
<dt>Rstep</dt><dd><p>The infinity norm of the final step in R</p>
</dd></dl>
<p>.
</p>
<dl>
<dt>iterations</dt><dd><p>The number of iterations taken.</p>
</dd>
<dt>converged</dt><dd><p>Whether convergence was detected.</p>
</dd>
</dl>



<h3>Note</h3>

<p>This package provides proof of concept code which is unlikely to be fast
or robust, and may not solve the optimization problem at hand. User assumes
all risk.
</p>


<h3>Author(s)</h3>

<p>Steven E. Pav <a href="mailto:shabbychef@gmail.com">shabbychef@gmail.com</a>
</p>


<h3>References</h3>

<p>Merritt, Michael, and Zhang, Yin. &quot;Interior-point Gradient Method for Large-Scale Totally 
Nonnegative Least Squares Problems.&quot; Journal of Optimization Theory and Applications 126, 
no 1 (2005): 191&ndash;202. <a href="https://scholarship.rice.edu/bitstream/handle/1911/102020/TR04-08.pdf">https://scholarship.rice.edu/bitstream/handle/1911/102020/TR04-08.pdf</a>
</p>
<p>Pav, S. E. &quot;An Iterative Algorithm for Regularized Non-negative Matrix Factorizations.&quot;
Forthcoming. (2024)
</p>
<p>Lee, Daniel D. and Seung, H. Sebastian. &quot;Algorithms for Non-negative Matrix 
Factorization.&quot; Advances in Neural Information Processing Systems 13 (2001):
556&ndash;562.
<a href="http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf">http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+aurnmf">aurnmf</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 nr &lt;- 20
 nc &lt;- 5
 dm &lt;- 2
 
 randmat &lt;- function(nr,nc,...) { matrix(pmax(0,runif(nr*nc,...)),nrow=nr) }
 set.seed(1234)
 real_L &lt;- randmat(nr,dm+2)
 real_R &lt;- randmat(ncol(real_L),nc)
 Y &lt;- real_L %*% real_R
 gram_it &lt;- function(G) { t(G) %*% G }
 W_0R &lt;- gram_it(randmat(nr+5,nr))
 W_0C &lt;- gram_it(randmat(nc+5,nc))
 
 wt_objective &lt;- function(Y, L, R, W_0R, W_0C) { 
   err &lt;- Y - L %*% R
   0.5 * sum((err %*% W_0C) * (t(W_0R) %*% err))
 }
 matrix_trace &lt;- function(G) {
   sum(diag(G))
 }
 wt_objective(Y,real_L,real_R,W_0R,W_0C)
 
 L_0 &lt;- randmat(nr,dm)
 R_0 &lt;- randmat(dm,nc)
 wt_objective(Y,L_0,R_0,W_0R,W_0C)
 out1 &lt;- gaurnmf(Y, L_0, R_0, W_0R=W_0R, W_0C=W_0C, 
         max_iterations=1e4L,check_optimal_step=FALSE)
 wt_objective(Y,out1$L,out1$R,W_0R,W_0C)
 
 W_1L &lt;- randmat(nr,dm)
 out2 &lt;- gaurnmf(Y, out1$L, out1$R, W_0R=W_0R, W_0C=W_0C, W_1L=W_1L, 
         max_iterations=1e4L,check_optimal_step=FALSE)
 wt_objective(Y,out2$L,out2$R,W_0R,W_0C)
 
 W_1R &lt;- randmat(dm,nc)
 out3 &lt;- gaurnmf(Y, out2$L, out2$R, W_0R=W_0R, W_0C=W_0C, W_1R=W_1R, 
         max_iterations=1e4L,check_optimal_step=FALSE)
 wt_objective(Y,out3$L,out3$R,W_0R,W_0C)


# example showing how to use the on_iteration_end callback to save iterates.
 max_iterations &lt;- 1e3L
 it_history &lt;&lt;- rep(NA_real_, max_iterations)
 on_iteration_end &lt;- function(iteration, Y, L, R, ...) {
   it_history[iteration] &lt;&lt;- wt_objective(Y,L,R,W_0R,W_0C)
 }
 out1b &lt;- gaurnmf(Y, L_0, R_0, W_0R=W_0R, W_0C=W_0C, 
   max_iterations=max_iterations, on_iteration_end=on_iteration_end, check_optimal_step=FALSE)


# should work on sparse matrices too.
if (require(Matrix)) { 
 real_L &lt;- randmat(nr,dm,min=-1)
 real_R &lt;- randmat(dm,nc,min=-1)
 Y &lt;- as(real_L %*% real_R, "sparseMatrix")
 L_0 &lt;- as(randmat(nr,dm,min=-0.5), "sparseMatrix")
 R_0 &lt;- as(randmat(dm,nc,min=-0.5), "sparseMatrix")
 out1 &lt;- gaurnmf(Y, L_0, R_0, max_iterations=1e2L,check_optimal_step=TRUE)
}

</code></pre>

<hr>
<h2 id='giqpm'>giqpm .</h2><span id='topic+giqpm'></span>

<h3>Description</h3>

<p>Generalized Iterative Quadratic Programming Method for non-negative quadratic optimization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>giqpm(
  Gmat,
  dvec,
  x0 = NULL,
  tau = 0.5,
  annealing_rate = 0.25,
  check_optimal_step = TRUE,
  mult_func = NULL,
  grad_func = NULL,
  step_func = NULL,
  zero_tolerance = 1e-09,
  max_iterations = 1000L,
  min_xstep = 1e-09,
  verbosity = 0
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="giqpm_+3A_gmat">Gmat</code></td>
<td>
<p>a representation of the matrix <code class="reqn">G</code>.</p>
</td></tr>
<tr><td><code id="giqpm_+3A_dvec">dvec</code></td>
<td>
<p>a representation of the vector <code class="reqn">d</code>.</p>
</td></tr>
<tr><td><code id="giqpm_+3A_x0">x0</code></td>
<td>
<p>the initial iterate. If none given, we spawn one of the same
size as <code>dvec</code>.</p>
</td></tr>
<tr><td><code id="giqpm_+3A_tau">tau</code></td>
<td>
<p>the starting shrinkage factor applied to the step length.
Should be a value in <code class="reqn">(0,1)</code>.</p>
</td></tr>
<tr><td><code id="giqpm_+3A_annealing_rate">annealing_rate</code></td>
<td>
<p>the rate at which we scale the shrinkage factor towards 1.
Should be a value in <code class="reqn">[0,1)</code>.</p>
</td></tr>
<tr><td><code id="giqpm_+3A_check_optimal_step">check_optimal_step</code></td>
<td>
<p>if TRUE, we attempt to take the optimal step
length in the given direction. If not, we merely take the longest feasible
step in the step direction.</p>
</td></tr>
<tr><td><code id="giqpm_+3A_mult_func">mult_func</code></td>
<td>
<p>a function which takes matrix and vector and performs
matrix multiplication. 
The default does this on matrix and vector input,
but the user can implement this for some implicit versions of the problem.</p>
</td></tr>
<tr><td><code id="giqpm_+3A_grad_func">grad_func</code></td>
<td>
<p>a function which takes matrix <code class="reqn">G</code>, vector <code class="reqn">d</code>, 
the current iterate <code class="reqn">x</code> and the product <code class="reqn">Gx</code> and is supposed to
compute <code class="reqn">Gx + d</code>.
The default does this on matrix and vector input,
but the user can implement this for some implicit versions of the problem.</p>
</td></tr>
<tr><td><code id="giqpm_+3A_step_func">step_func</code></td>
<td>
<p>a function which takes the vector gradient, the product 
<code class="reqn">Gx</code>, the matrix <code class="reqn">G</code>, vector <code class="reqn">d</code>, vector <code class="reqn">x</code> and the
<code>mult_func</code> and produces a step vector.
By default this step vector is the Lee-Seung step vector, namely
<code class="reqn">-(Gx + d) * x / d</code>, with Hadamard product and division.</p>
</td></tr>
<tr><td><code id="giqpm_+3A_zero_tolerance">zero_tolerance</code></td>
<td>
<p>values of <code class="reqn">x</code> less than this will be &lsquo;snapped&rsquo; to zero.
This happens at the end of the iteration and does not affect the measurement
of convergence.</p>
</td></tr>
<tr><td><code id="giqpm_+3A_max_iterations">max_iterations</code></td>
<td>
<p>the maximum number of iterations to perform.</p>
</td></tr>
<tr><td><code id="giqpm_+3A_min_xstep">min_xstep</code></td>
<td>
<p>the minimum L-infinity norm of the step taken.
Once the step falls under this value, we terminate.</p>
</td></tr>
<tr><td><code id="giqpm_+3A_verbosity">verbosity</code></td>
<td>
<p>controls whether we print information to the console.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Iteratively solves the problem
</p>
<p style="text-align: center;"><code class="reqn">\min_x \frac{1}{2}x^{\top}G x + d^{\top}x</code>
</p>

<p>subject to the elementwise constraint <code class="reqn">x \ge 0</code>.
</p>
<p>This implementation allows the user to specify methods to perform matrix by
vector multiplication, computation of the gradient (which should be
<code class="reqn">G x + d</code>), and computation of the step direction.
By default we compute the optimal step in the given step direction.
</p>


<h3>Value</h3>

<p>a list with the elements
</p>

<dl>
<dt>x</dt><dd><p>The final iterate.</p>
</dd>
<dt>iterations</dt><dd><p>The number of iterations taken.</p>
</dd>
<dt>converged</dt><dd><p>Whether convergence was detected.</p>
</dd>
</dl>



<h3>Note</h3>

<p>This package provides proof of concept code which is unlikely to be fast
or robust, and may not solve the optimization problem at hand. User assumes
all risk.
</p>


<h3>Author(s)</h3>

<p>Steven E. Pav <a href="mailto:shabbychef@gmail.com">shabbychef@gmail.com</a>
</p>


<h3>References</h3>

<p>Pav, S. E. &quot;An Iterative Algorithm for Regularized Non-negative Matrix Factorizations.&quot;
Forthcoming. (2024)
</p>
<p>Merritt, Michael, and Zhang, Yin. &quot;Interior-point Gradient Method for Large-Scale Totally 
Nonnegative Least Squares Problems.&quot; Journal of Optimization Theory and Applications 126, 
no 1 (2005): 191&ndash;202. <a href="https://scholarship.rice.edu/bitstream/handle/1911/102020/TR04-08.pdf">https://scholarship.rice.edu/bitstream/handle/1911/102020/TR04-08.pdf</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(1234)
ssiz &lt;- 100
preG &lt;- matrix(runif(ssiz*(ssiz+20)),nrow=ssiz)
G &lt;- preG %*% t(preG)
d &lt;- - runif(ssiz)
y1 &lt;- giqpm(G, d)
objective &lt;- function(G, d, x) { as.numeric(0.5 * t(x) %*% (G %*% x) + t(x) %*% d) }

# this does not converge to an actual solution!
steepest_step_func &lt;- function(gradf, ...) { return(-gradf) }
y2 &lt;- giqpm(G, d, step_func = steepest_step_func)

scaled_step_func &lt;- function(gradf, Gx, Gmat, dvec, x0, ...) { return(-gradf * abs(x0)) }
y3 &lt;- giqpm(G, d, step_func = scaled_step_func)

sqrt_step_func &lt;- function(gradf, Gx, Gmat, dvec, x0, ...) { return(-gradf * abs(sqrt(x0))) }
y4 &lt;- giqpm(G, d, step_func = sqrt_step_func)

complementarity_stepfunc &lt;- function(gradf, Gx, Gmat, dvec, x0, ...) { return(-gradf * x0) }
y5 &lt;- giqpm(G, d, step_func = complementarity_stepfunc)

objective(G, d, y1$x)
objective(G, d, y2$x)
objective(G, d, y3$x)
objective(G, d, y4$x)
objective(G, d, y5$x)

</code></pre>

<hr>
<h2 id='murnmf'>murnmf .</h2><span id='topic+murnmf'></span>

<h3>Description</h3>

<p>Multiplicative update Non-negative matrix factorization with regularization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>murnmf(
  Y,
  L,
  R,
  W_0R = NULL,
  W_0C = NULL,
  lambda_1L = 0,
  lambda_1R = 0,
  lambda_2L = 0,
  lambda_2R = 0,
  gamma_2L = 0,
  gamma_2R = 0,
  epsilon = 1e-07,
  max_iterations = 1000L,
  min_xstep = 1e-09,
  on_iteration_end = NULL,
  verbosity = 0
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="murnmf_+3A_y">Y</code></td>
<td>
<p>an <code class="reqn">r \times c</code> matrix to be decomposed.
Should have non-negative elements; an error is thrown otherwise.</p>
</td></tr>
<tr><td><code id="murnmf_+3A_l">L</code></td>
<td>
<p>an <code class="reqn">r \times d</code> matrix of the initial estimate of L.
Should have non-negative elements; an error is thrown otherwise.</p>
</td></tr>
<tr><td><code id="murnmf_+3A_r">R</code></td>
<td>
<p>an <code class="reqn">d \times c</code> matrix of the initial estimate of R.
Should have non-negative elements; an error is thrown otherwise.</p>
</td></tr>
<tr><td><code id="murnmf_+3A_w_0r">W_0R</code></td>
<td>
<p>the row space weighting matrix.
This should be a positive definite non-negative symmetric <code class="reqn">r \times r</code> matrix.
If omitted, it defaults to the properly sized identity matrix.</p>
</td></tr>
<tr><td><code id="murnmf_+3A_w_0c">W_0C</code></td>
<td>
<p>the column space weighting matrix.
This should be a positive definite non-negative symmetric <code class="reqn">c \times c</code> matrix.
If omitted, it defaults to the properly sized identity matrix.</p>
</td></tr>
<tr><td><code id="murnmf_+3A_lambda_1l">lambda_1L</code></td>
<td>
<p>the scalar <code class="reqn">\ell_1</code> penalty for the matrix <code class="reqn">L</code>.
Defaults to zero.</p>
</td></tr>
<tr><td><code id="murnmf_+3A_lambda_1r">lambda_1R</code></td>
<td>
<p>the scalar <code class="reqn">\ell_1</code> penalty for the matrix <code class="reqn">R</code>.
Defaults to zero.</p>
</td></tr>
<tr><td><code id="murnmf_+3A_lambda_2l">lambda_2L</code></td>
<td>
<p>the scalar <code class="reqn">\ell_2</code> penalty for the matrix <code class="reqn">L</code>.
Defaults to zero.</p>
</td></tr>
<tr><td><code id="murnmf_+3A_lambda_2r">lambda_2R</code></td>
<td>
<p>the scalar <code class="reqn">\ell_2</code> penalty for the matrix <code class="reqn">R</code>.
Defaults to zero.</p>
</td></tr>
<tr><td><code id="murnmf_+3A_gamma_2l">gamma_2L</code></td>
<td>
<p>the scalar <code class="reqn">\ell_2</code> penalty for non-orthogonality of the matrix <code class="reqn">L</code>.
Defaults to zero.</p>
</td></tr>
<tr><td><code id="murnmf_+3A_gamma_2r">gamma_2R</code></td>
<td>
<p>the scalar <code class="reqn">\ell_2</code> penalty for non-orthogonality of the matrix <code class="reqn">R</code>.
Defaults to zero.</p>
</td></tr>
<tr><td><code id="murnmf_+3A_epsilon">epsilon</code></td>
<td>
<p>the numerator clipping value.</p>
</td></tr>
<tr><td><code id="murnmf_+3A_max_iterations">max_iterations</code></td>
<td>
<p>the maximum number of iterations to perform.</p>
</td></tr>
<tr><td><code id="murnmf_+3A_min_xstep">min_xstep</code></td>
<td>
<p>the minimum L-infinity norm of the step taken.
Once the step falls under this value, we terminate.</p>
</td></tr>
<tr><td><code id="murnmf_+3A_on_iteration_end">on_iteration_end</code></td>
<td>
<p>an optional function that is called at the end of
each iteration. The function is called as 
<code>on_iteration_end(iteration=iteration, Y=Y, L=L, R=R, Lstep=Lstep, Rstep=Rstep, ...)</code></p>
</td></tr>
<tr><td><code id="murnmf_+3A_verbosity">verbosity</code></td>
<td>
<p>controls whether we print information to the console.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function uses multiplicative updates only, and may not optimize the
nominal objective. It is also unlikely to achieve optimality.
This code is for reference purposes and is not suited for usage other
than research and experimentation.
</p>


<h3>Value</h3>

<p>a list with the elements
</p>

<dl>
<dt>L</dt><dd><p>The final estimate of L.</p>
</dd>
<dt>R</dt><dd><p>The final estimate of R.</p>
</dd>
<dt>Lstep</dt><dd><p>The infinity norm of the final step in L.</p>
</dd>
<dt>Rstep</dt><dd><p>The infinity norm of the final step in R.</p>
</dd>
<dt>iterations</dt><dd><p>The number of iterations taken.</p>
</dd>
<dt>converged</dt><dd><p>Whether convergence was detected.</p>
</dd>
</dl>



<h3>Note</h3>

<p>This package provides proof of concept code which is unlikely to be fast
or robust, and may not solve the optimization problem at hand. User assumes
all risk.
</p>


<h3>Author(s)</h3>

<p>Steven E. Pav <a href="mailto:shabbychef@gmail.com">shabbychef@gmail.com</a>
</p>


<h3>References</h3>

<p>Merritt, Michael, and Zhang, Yin. &quot;Interior-point Gradient Method for Large-Scale Totally 
Nonnegative Least Squares Problems.&quot; Journal of Optimization Theory and Applications 126, 
no 1 (2005): 191&ndash;202. <a href="https://scholarship.rice.edu/bitstream/handle/1911/102020/TR04-08.pdf">https://scholarship.rice.edu/bitstream/handle/1911/102020/TR04-08.pdf</a>
</p>
<p>Pav, S. E. &quot;An Iterative Algorithm for Regularized Non-negative Matrix Factorizations.&quot;
Forthcoming. (2024)
</p>
<p>Lee, Daniel D. and Seung, H. Sebastian. &quot;Algorithms for Non-negative Matrix 
Factorization.&quot; Advances in Neural Information Processing Systems 13 (2001):
556&ndash;562.
<a href="http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf">http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+aurnmf">aurnmf</a></code>, <code><a href="#topic+gaurnmf">gaurnmf</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 nr &lt;- 100
 nc &lt;- 20
 dm &lt;- 4

 randmat &lt;- function(nr,nc,...) { matrix(pmax(0,runif(nr*nc,...)),nrow=nr) }
 set.seed(1234)
 real_L &lt;- randmat(nr,dm)
 real_R &lt;- randmat(dm,nc)
 Y &lt;- real_L %*% real_R
# without regularization
 objective &lt;- function(Y, L, R) { sum((Y - L %*% R)^2) }
 objective(Y,real_L,real_R)

 L_0 &lt;- randmat(nr,dm)
 R_0 &lt;- randmat(dm,nc)
 objective(Y,L_0,R_0)
 out1 &lt;- murnmf(Y, L_0, R_0, max_iterations=5e3L)
 objective(Y,out1$L,out1$R)
# with L1 regularization on one side
 out2 &lt;- murnmf(Y, L_0, R_0, max_iterations=5e3L,lambda_1L=0.1)
# objective does not suffer because all mass is shifted to R
 objective(Y,out2$L,out2$R)
list(L1=sum(out1$L),R1=sum(out1$R),L2=sum(out2$L),R2=sum(out2$R))
sum(out2$L)
# with L1 regularization on both sides
 out3 &lt;- murnmf(Y, L_0, R_0, max_iterations=5e3L,lambda_1L=0.1,lambda_1R=0.1)
# with L1 regularization on both sides, raw objective suffers
 objective(Y,out3$L,out3$R)
list(L1=sum(out1$L),R1=sum(out1$R),L3=sum(out3$L),R3=sum(out3$R))


# example showing how to use the on_iteration_end callback to save iterates.
max_iterations &lt;- 1e3L
it_history &lt;&lt;- rep(NA_real_, max_iterations)
quadratic_objective &lt;- function(Y, L, R) { sum((Y - L %*% R)^2) }
on_iteration_end &lt;- function(iteration, Y, L, R, ...) {
  it_history[iteration] &lt;&lt;- quadratic_objective(Y,L,R)
}
out1b &lt;- murnmf(Y, L_0, R_0, max_iterations=max_iterations, on_iteration_end=on_iteration_end)


# should work on sparse matrices too, but beware zeros in the initial estimates
if (require(Matrix)) { 
 real_L &lt;- randmat(nr,dm,min=-1)
 real_R &lt;- randmat(dm,nc,min=-1)
 Y &lt;- as(real_L %*% real_R, "sparseMatrix")
 L_0 &lt;- randmat(nr,dm)
 R_0 &lt;- randmat(dm,nc)
 out1 &lt;- murnmf(Y, L_0, R_0, max_iterations=1e2L)
}

</code></pre>

<hr>
<h2 id='rnnmf-NEWS'>News for package 'rnnmf':</h2><span id='topic+rnnmf-NEWS'></span>

<h3>Description</h3>

<p>News for package &lsquo;rnnmf&rsquo;
</p>




<h3><a href="https://cran.r-project.org/package=rnnmf"><span class="pkg">rnnmf</span></a> Initial Version 0.3.0 (2024-10-30) </h3>


<ul>
<li><p> first CRAN release.
</p>
</li>
<li><p> changed name from rnmf to rnnmf.
</p>
</li></ul>


</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
