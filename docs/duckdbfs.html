<!DOCTYPE html><html lang="en-US"><head><title>Help for package duckdbfs</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {duckdbfs}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#as_dataset'><p>as_dataset</p></a></li>
<li><a href='#as_view'><p>as_view</p></a></li>
<li><a href='#cached_connection'><p>create a cachable duckdb connection</p></a></li>
<li><a href='#close_connection'><p>close connection</p></a></li>
<li><a href='#duckdb_s3_config'><p>Configure S3 settings for database connection</p></a></li>
<li><a href='#load_spatial'><p>load the duckdb geospatial data plugin</p></a></li>
<li><a href='#open_dataset'><p>Open a dataset from a variety of sources</p></a></li>
<li><a href='#spatial_join'><p>spatial_join</p></a></li>
<li><a href='#st_read_meta'><p>read spatial metadata</p></a></li>
<li><a href='#to_sf'><p>Convert output to sf object</p></a></li>
<li><a href='#write_dataset'><p>write_dataset</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>High Performance Remote File System, Database and 'Geospatial'
Access Using 'duckdb'</td>
</tr>
<tr>
<td>Version:</td>
<td>0.0.9</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides friendly wrappers for creating 'duckdb'-backed connections
  to tabular datasets ('csv', parquet, etc) on local or remote file systems.
  This mimics the behaviour of "open_dataset" in the 'arrow' package, 
  but in addition to 'S3' file system also generalizes to any list of 'http' URLs.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/cboettig/duckdbfs">https://github.com/cboettig/duckdbfs</a>,
<a href="https://cboettig.github.io/duckdbfs/">https://cboettig.github.io/duckdbfs/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/cboettig/duckdbfs/issues">https://github.com/cboettig/duckdbfs/issues</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>DBI, dbplyr, dplyr, duckdb (&ge; 1.1), fs, glue</td>
</tr>
<tr>
<td>Suggests:</td>
<td>curl, sf, jsonlite, spelling, minioclient, testthat (&ge;
3.0.0)</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-12-16 23:24:38 UTC; jovyan</td>
</tr>
<tr>
<td>Author:</td>
<td>Carl Boettiger <a href="https://orcid.org/0000-0002-1642-628X"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Michael D. Sumner <a href="https://orcid.org/0000-0002-2471-7511"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Carl Boettiger &lt;cboettig@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-12-17 00:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='as_dataset'>as_dataset</h2><span id='topic+as_dataset'></span>

<h3>Description</h3>

<p>Push a local (in-memory) dataset into a the duckdb database as a table.
This enables it to share the connection source with other data.
This is equivalent to the behavior of copy=TRUE on many (but not all) of the two-table verbs in dplyr.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_dataset(df, conn = cached_connection())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="as_dataset_+3A_df">df</code></td>
<td>
<p>a local data frame.  Otherwise will be passed back without side effects</p>
</td></tr>
<tr><td><code id="as_dataset_+3A_conn">conn</code></td>
<td>
<p>A connection to a database.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a remote <code>dplyr::tbl</code> connection to the table.
</p>

<hr>
<h2 id='as_view'>as_view</h2><span id='topic+as_view'></span>

<h3>Description</h3>

<p>Create a View of the current query.  This can be an effective way to allow
a query chain to remain lazy
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_view(x, tblname = tmp_tbl_name(), conn = cached_connection())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="as_view_+3A_x">x</code></td>
<td>
<p>a duckdb spatial dataset</p>
</td></tr>
<tr><td><code id="as_view_+3A_tblname">tblname</code></td>
<td>
<p>The name of the table to create in the database.</p>
</td></tr>
<tr><td><code id="as_view_+3A_conn">conn</code></td>
<td>
<p>A connection to a database.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
path &lt;- system.file("extdata/spatial-test.csv", package="duckdbfs")
df &lt;- open_dataset(path)
library(dplyr)

df |&gt; filter(latitude &gt; 5) |&gt; as_view()

</code></pre>

<hr>
<h2 id='cached_connection'>create a cachable duckdb connection</h2><span id='topic+cached_connection'></span>

<h3>Description</h3>

<p>This function is primarily intended for internal use by other
<code>duckdbfs</code> functions.  However, it can be called directly by
the user whenever it is desirable to have direct access to the
connection object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cached_connection(
  dbdir = ":memory:",
  read_only = FALSE,
  bigint = "numeric",
  config = list(temp_directory = tempfile())
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cached_connection_+3A_dbdir">dbdir</code></td>
<td>
<p>Location for database files. Should be a path to an existing
directory in the file system. With the default (or <code>""</code>), all
data is kept in RAM.</p>
</td></tr>
<tr><td><code id="cached_connection_+3A_read_only">read_only</code></td>
<td>
<p>Set to <code>TRUE</code> for read-only operation.
For file-based databases, this is only applied when the database file is opened for the first time.
Subsequent connections (via the same <code>drv</code> object or a <code>drv</code> object pointing to the same path)
will silently ignore this flag.</p>
</td></tr>
<tr><td><code id="cached_connection_+3A_bigint">bigint</code></td>
<td>
<p>How 64-bit integers should be returned. There are two options: <code>"numeric"</code> and <code>"integer64"</code>.
If <code>"numeric"</code> is selected, bigint integers will be treated as double/numeric.
If <code>"integer64"</code> is selected, bigint integers will be set to bit64 encoding.</p>
</td></tr>
<tr><td><code id="cached_connection_+3A_config">config</code></td>
<td>
<p>Named list with DuckDB configuration flags, see
<a href="https://duckdb.org/docs/configuration/overview#configuration-reference">https://duckdb.org/docs/configuration/overview#configuration-reference</a> for the possible options.
These flags are only applied when the database object is instantiated.
Subsequent connections will silently ignore these flags.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When first called (by a user or internal function),
this function both creates a duckdb connection and places
that connection into a cache (<code>duckdbfs_conn</code> option).
On subsequent calls, this function returns the cached connection,
rather than recreating a fresh connection.
</p>
<p>This frees the user from the responsibility of managing a
connection object, because functions needing access to the
connection can use this to create or access the existing connection.
At the close of the global environment, this function's finalizer
should gracefully shutdown the connection before removing the cache.
</p>
<p>By default, this function creates an in-memory connection. When reading
from on-disk or remote files (parquet or csv), this option can still
effectively support most operations on much-larger-than-RAM data.
However, some operations require additional working space, so by default
we set a temporary storage location in configuration as well.
</p>


<h3>Value</h3>

<p>a <code><a href="duckdb.html#topic+duckdb">duckdb::duckdb()</a></code> connection object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
con &lt;- cached_connection()
close_connection(con)

</code></pre>

<hr>
<h2 id='close_connection'>close connection</h2><span id='topic+close_connection'></span>

<h3>Description</h3>

<p>close connection
</p>


<h3>Usage</h3>

<pre><code class='language-R'>close_connection(conn = cached_connection())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="close_connection_+3A_conn">conn</code></td>
<td>
<p>a duckdb connection (leave blank)
Closes the invisible cached connection to duckdb</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Shuts down connection before gc removes it.
Then clear cached reference to avoid using a stale connection
This avoids complaint about connection being garbage collected.
</p>


<h3>Value</h3>

<p>returns nothing.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
close_connection()

</code></pre>

<hr>
<h2 id='duckdb_s3_config'>Configure S3 settings for database connection</h2><span id='topic+duckdb_s3_config'></span>

<h3>Description</h3>

<p>This function is used to configure S3 settings for a database connection.
It allows you to set various S3-related parameters such as access key,
secret access key, endpoint, region, session token, uploader settings,
URL compatibility mode, URL style, and SSL usage.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>duckdb_s3_config(
  conn = cached_connection(),
  s3_access_key_id = NULL,
  s3_secret_access_key = NULL,
  s3_endpoint = NULL,
  s3_region = NULL,
  s3_session_token = NULL,
  s3_uploader_max_filesize = NULL,
  s3_uploader_max_parts_per_file = NULL,
  s3_uploader_thread_limit = NULL,
  s3_url_compatibility_mode = NULL,
  s3_url_style = NULL,
  s3_use_ssl = NULL,
  anonymous = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="duckdb_s3_config_+3A_conn">conn</code></td>
<td>
<p>A database connection object created using the
<code>cache_connection</code> function (default: <code>cache_connection()</code>).</p>
</td></tr>
<tr><td><code id="duckdb_s3_config_+3A_s3_access_key_id">s3_access_key_id</code></td>
<td>
<p>The S3 access key ID (default: <code>NULL</code>).</p>
</td></tr>
<tr><td><code id="duckdb_s3_config_+3A_s3_secret_access_key">s3_secret_access_key</code></td>
<td>
<p>The S3 secret access key (default: <code>NULL</code>).</p>
</td></tr>
<tr><td><code id="duckdb_s3_config_+3A_s3_endpoint">s3_endpoint</code></td>
<td>
<p>The S3 endpoint (default: <code>NULL</code>).</p>
</td></tr>
<tr><td><code id="duckdb_s3_config_+3A_s3_region">s3_region</code></td>
<td>
<p>The S3 region (default: <code>NULL</code>).</p>
</td></tr>
<tr><td><code id="duckdb_s3_config_+3A_s3_session_token">s3_session_token</code></td>
<td>
<p>The S3 session token (default: <code>NULL</code>).</p>
</td></tr>
<tr><td><code id="duckdb_s3_config_+3A_s3_uploader_max_filesize">s3_uploader_max_filesize</code></td>
<td>
<p>The maximum filesize for S3 uploader
(between 50GB and 5TB, default 800GB).</p>
</td></tr>
<tr><td><code id="duckdb_s3_config_+3A_s3_uploader_max_parts_per_file">s3_uploader_max_parts_per_file</code></td>
<td>
<p>The maximum number of parts per file
for S3 uploader (between 1 and 10000, default 10000).</p>
</td></tr>
<tr><td><code id="duckdb_s3_config_+3A_s3_uploader_thread_limit">s3_uploader_thread_limit</code></td>
<td>
<p>The thread limit for S3 uploader
(default: 50).</p>
</td></tr>
<tr><td><code id="duckdb_s3_config_+3A_s3_url_compatibility_mode">s3_url_compatibility_mode</code></td>
<td>
<p>Disable Globs and Query Parameters on
S3 URLs (default: 0, allows globs/queries).</p>
</td></tr>
<tr><td><code id="duckdb_s3_config_+3A_s3_url_style">s3_url_style</code></td>
<td>
<p>The style of S3 URLs to use. Default is
&quot;vhost&quot; unless s3_endpoint is set, which makes default &quot;path&quot;
(i.e. MINIO systems).</p>
</td></tr>
<tr><td><code id="duckdb_s3_config_+3A_s3_use_ssl">s3_use_ssl</code></td>
<td>
<p>Enable or disable SSL for S3 connections
(default: 1 (TRUE)).</p>
</td></tr>
<tr><td><code id="duckdb_s3_config_+3A_anonymous">anonymous</code></td>
<td>
<p>request anonymous access (sets <code>s3_access_key_id</code> and
<code>s3_secret_access_key</code> to <code>""</code>, allowing anonymous access to public buckets).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>see <a href="https://duckdb.org/docs/sql/configuration.html">https://duckdb.org/docs/sql/configuration.html</a>
</p>


<h3>Value</h3>

<p>Returns silently (NULL) if successful.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Configure S3 settings
duckdb_s3_config(
           s3_access_key_id = "YOUR_ACCESS_KEY_ID",
           s3_secret_access_key = "YOUR_SECRET_ACCESS_KEY",
           s3_endpoint = "YOUR_S3_ENDPOINT",
           s3_region = "YOUR_S3_REGION",
           s3_uploader_max_filesize = "800GB",
           s3_uploader_max_parts_per_file = 100,
           s3_uploader_thread_limit = 8,
           s3_url_compatibility_mode = FALSE,
           s3_url_style = "vhost",
           s3_use_ssl = TRUE,
           anonymous = TRUE)

</code></pre>

<hr>
<h2 id='load_spatial'>load the duckdb geospatial data plugin</h2><span id='topic+load_spatial'></span>

<h3>Description</h3>

<p>load the duckdb geospatial data plugin
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load_spatial(
  conn = cached_connection(),
  nightly = getOption("duckdbfs_use_nightly", FALSE),
  force = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="load_spatial_+3A_conn">conn</code></td>
<td>
<p>A database connection object created using the
<code>cache_connection</code> function (default: <code>cache_connection()</code>).</p>
</td></tr>
<tr><td><code id="load_spatial_+3A_nightly">nightly</code></td>
<td>
<p>should we use the nightly version or not?
default FALSE, configurable as <code>duckdbfs_use_nightly</code> option.</p>
</td></tr>
<tr><td><code id="load_spatial_+3A_force">force</code></td>
<td>
<p>force re-install?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>loads the extension and returns status invisibly.
</p>


<h3>References</h3>

<p><a href="https://duckdb.org/docs/extensions/spatial.html">https://duckdb.org/docs/extensions/spatial.html</a>
</p>

<hr>
<h2 id='open_dataset'>Open a dataset from a variety of sources</h2><span id='topic+open_dataset'></span>

<h3>Description</h3>

<p>This function opens a dataset from a variety of sources, including Parquet,
CSV, etc, using either local file system paths, URLs, or S3 bucket URI
notation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>open_dataset(
  sources,
  schema = NULL,
  hive_style = TRUE,
  unify_schemas = FALSE,
  format = c("parquet", "csv", "tsv", "sf"),
  conn = cached_connection(),
  tblname = tmp_tbl_name(),
  mode = "VIEW",
  filename = FALSE,
  recursive = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="open_dataset_+3A_sources">sources</code></td>
<td>
<p>A character vector of paths to the dataset files.</p>
</td></tr>
<tr><td><code id="open_dataset_+3A_schema">schema</code></td>
<td>
<p>The schema for the dataset. If NULL, the schema will be
inferred from the dataset files.</p>
</td></tr>
<tr><td><code id="open_dataset_+3A_hive_style">hive_style</code></td>
<td>
<p>A logical value indicating whether to the dataset uses
Hive-style partitioning.</p>
</td></tr>
<tr><td><code id="open_dataset_+3A_unify_schemas">unify_schemas</code></td>
<td>
<p>A logical value indicating whether to unify the schemas
of the dataset files (union_by_name). If TRUE, will execute a UNION by
column name across all files (NOTE: this can add considerably to
the initial execution time)</p>
</td></tr>
<tr><td><code id="open_dataset_+3A_format">format</code></td>
<td>
<p>The format of the dataset files. One of <code>"parquet"</code>, <code>"csv"</code>,
<code>"tsv"</code>, or <code>"sf"</code> (spatial vector files supported by the sf package / GDAL).
if no argument is provided, the function will try to guess the type based
on minimal heuristics.</p>
</td></tr>
<tr><td><code id="open_dataset_+3A_conn">conn</code></td>
<td>
<p>A connection to a database.</p>
</td></tr>
<tr><td><code id="open_dataset_+3A_tblname">tblname</code></td>
<td>
<p>The name of the table to create in the database.</p>
</td></tr>
<tr><td><code id="open_dataset_+3A_mode">mode</code></td>
<td>
<p>The mode to create the table in. One of <code>"VIEW"</code> or <code>"TABLE"</code>.
Creating a <code>VIEW</code>, the default, will execute more quickly because it
does not create a local copy of the dataset.  <code>TABLE</code> will create a local
copy in duckdb's native format, downloading the full dataset if necessary.
When using <code>TABLE</code> mode with large data, please be sure to use a <code>conn</code>
connections with disk-based storage, e.g. by calling <code><a href="#topic+cached_connection">cached_connection()</a></code>,
e.g. <code>cached_connection("storage_path")</code>, otherwise the full data must fit
into RAM.  Using <code>TABLE</code> assumes familiarity with R's DBI-based interface.</p>
</td></tr>
<tr><td><code id="open_dataset_+3A_filename">filename</code></td>
<td>
<p>A logical value indicating whether to include the filename in
the table name.</p>
</td></tr>
<tr><td><code id="open_dataset_+3A_recursive">recursive</code></td>
<td>
<p>should we assume recursive path? default TRUE. Set to FALSE
if trying to open a single, un-partitioned file.</p>
</td></tr>
<tr><td><code id="open_dataset_+3A_...">...</code></td>
<td>
<p>optional additional arguments passed to <code><a href="#topic+duckdb_s3_config">duckdb_s3_config()</a></code>.
Note these apply after those set by the URI notation and thus may be used
to override or provide settings not supported in that format.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A lazy <code>dplyr::tbl</code> object representing the opened dataset backed
by a duckdb SQL connection.  Most <code>dplyr</code> (and some <code>tidyr</code>) verbs can be
used directly on this object, as they can be translated into SQL commands
automatically via <code>dbplyr</code>.  Generic R commands require using
<code><a href="dplyr.html#topic+compute">dplyr::collect()</a></code> on the table, which forces evaluation and reading the
resulting data into memory.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# A remote, hive-partitioned Parquet dataset
base &lt;- paste0("https://github.com/duckdb/duckdb/raw/main/",
             "data/parquet-testing/hive-partitioning/union_by_name/")
f1 &lt;- paste0(base, "x=1/f1.parquet")
f2 &lt;- paste0(base, "x=1/f2.parquet")
f3 &lt;- paste0(base, "x=2/f2.parquet")

open_dataset(c(f1,f2,f3), unify_schemas = TRUE)

# Access an S3 database specifying an independently-hosted (MINIO) endpoint
efi &lt;- open_dataset("s3://neon4cast-scores/parquet/aquatics",
                    s3_access_key_id="",
                    s3_endpoint="data.ecoforecast.org")

</code></pre>

<hr>
<h2 id='spatial_join'>spatial_join</h2><span id='topic+spatial_join'></span>

<h3>Description</h3>

<p>spatial_join
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spatial_join(
  x,
  y,
  by = c("st_intersects", "st_within", "st_dwithin", "st_touches", "st_contains",
    "st_containsproperly", "st_covers", "st_overlaps", "st_crosses", "st_equals",
    "st_disjoint"),
  args = "",
  join = "left",
  tblname = tmp_tbl_name(),
  conn = cached_connection()
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spatial_join_+3A_x">x</code></td>
<td>
<p>a duckdb table with a spatial geometry column called &quot;geom&quot;</p>
</td></tr>
<tr><td><code id="spatial_join_+3A_y">y</code></td>
<td>
<p>a duckdb table with a spatial geometry column called &quot;geom&quot;</p>
</td></tr>
<tr><td><code id="spatial_join_+3A_by">by</code></td>
<td>
<p>A spatial join function, see details.</p>
</td></tr>
<tr><td><code id="spatial_join_+3A_args">args</code></td>
<td>
<p>additional arguments to join function (e.g. distance for st_dwithin)</p>
</td></tr>
<tr><td><code id="spatial_join_+3A_join">join</code></td>
<td>
<p>JOIN type (left, right, inner, full)</p>
</td></tr>
<tr><td><code id="spatial_join_+3A_tblname">tblname</code></td>
<td>
<p>name for the temporary view</p>
</td></tr>
<tr><td><code id="spatial_join_+3A_conn">conn</code></td>
<td>
<p>the duckdb connection (imputed by duckdbfs by default,
must be shared across both tables)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Possible <a href="https://postgis.net/workshops/postgis-intro/spatial_relationships.html">spatial joins</a> include:</p>

<table>
<tr>
 <td style="text-align: left;">
   Function </td><td style="text-align: left;"> Description </td>
</tr>
<tr>
 <td style="text-align: left;">
   st_intersects </td><td style="text-align: left;"> Geometry A intersects with geometry B </td>
</tr>
<tr>
 <td style="text-align: left;">
   st_disjoint </td><td style="text-align: left;"> The complement of intersects </td>
</tr>
<tr>
 <td style="text-align: left;">
   st_within </td><td style="text-align: left;"> Geometry A is within geometry B (complement of contains) </td>
</tr>
<tr>
 <td style="text-align: left;">
   st_dwithin </td><td style="text-align: left;"> Geometries are within a specified distance, expressed in the same units as the coordinate reference system. </td>
</tr>
<tr>
 <td style="text-align: left;">
   st_touches </td><td style="text-align: left;"> Two polygons touch if the that have at least one point in common, even if their interiors do not touch. </td>
</tr>
<tr>
 <td style="text-align: left;">
   st_contains </td><td style="text-align: left;"> Geometry A entirely contains to geometry B. (complement of within) </td>
</tr>
<tr>
 <td style="text-align: left;">
   st_containsproperly </td><td style="text-align: left;"> stricter version of <code>st_contains</code> (boundary counts as external) </td>
</tr>
<tr>
 <td style="text-align: left;">
   st_covers </td><td style="text-align: left;"> geometry B is inside or on boundary of A. (A polygon covers a point on its boundary but does not contain it.) </td>
</tr>
<tr>
 <td style="text-align: left;">
   st_overlaps </td><td style="text-align: left;"> geometry A intersects but does not completely contain geometry B </td>
</tr>
<tr>
 <td style="text-align: left;">
   st_equals </td><td style="text-align: left;"> geometry A is equal to geometry B </td>
</tr>
<tr>
 <td style="text-align: left;">
   st_crosses </td><td style="text-align: left;"> Lines or points in geometry A cross geometry B. </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>All though SQL is not case sensitive, this function expects only
lower case names for &quot;by&quot; functions.
</p>


<h3>Value</h3>

<p>a (lazy) view of the resulting table. Users can continue to operate
on using dplyr operations and call to_st() to collect this as an sf object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# note we can read in remote data in a variety of vector formats:
countries &lt;-
paste0("/vsicurl/",
       "https://github.com/cboettig/duckdbfs/",
       "raw/spatial-read/inst/extdata/world.gpkg") |&gt;
open_dataset(format = "sf")

cities &lt;-
 paste0("/vsicurl/https://github.com/cboettig/duckdbfs/raw/",
        "spatial-read/inst/extdata/metro.fgb") |&gt;
 open_dataset(format = "sf")

countries |&gt;
  dplyr::filter(iso_a3 == "AUS") |&gt;
  spatial_join(cities)

</code></pre>

<hr>
<h2 id='st_read_meta'>read spatial metadata</h2><span id='topic+st_read_meta'></span>

<h3>Description</h3>

<p>At this time, reads a subset of spatial metadata.
This is similar to what is reported by <code>ogrinfo -json</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>st_read_meta(
  path,
  layer = 1L,
  tblname = tbl_name(path),
  conn = cached_connection(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="st_read_meta_+3A_path">path</code></td>
<td>
<p>URL or path to spatial data file</p>
</td></tr>
<tr><td><code id="st_read_meta_+3A_layer">layer</code></td>
<td>
<p>layer number to read metadata for, defaults to first layer.</p>
</td></tr>
<tr><td><code id="st_read_meta_+3A_tblname">tblname</code></td>
<td>
<p>metadata will be stored as a view with this name,
by default this is based on the name of the file.</p>
</td></tr>
<tr><td><code id="st_read_meta_+3A_conn">conn</code></td>
<td>
<p>A connection to a database.</p>
</td></tr>
<tr><td><code id="st_read_meta_+3A_...">...</code></td>
<td>
<p>optional additional arguments passed to <code><a href="#topic+duckdb_s3_config">duckdb_s3_config()</a></code>.
Note these apply after those set by the URI notation and thus may be used
to override or provide settings not supported in that format.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A lazy <code>dplyr::tbl</code> object containing core spatial metadata such
as projection information.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
st_read_meta("https://github.com/duckdb/duckdb_spatial/raw/main/test/data/amsterdam_roads.fgb")

</code></pre>

<hr>
<h2 id='to_sf'>Convert output to sf object</h2><span id='topic+to_sf'></span>

<h3>Description</h3>

<p>Convert output to sf object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>to_sf(x, crs = NA, conn = cached_connection())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="to_sf_+3A_x">x</code></td>
<td>
<p>a remote duckdb <code>tbl</code> (from <code>open_dataset</code>) or dplyr-pipeline thereof.</p>
</td></tr>
<tr><td><code id="to_sf_+3A_crs">crs</code></td>
<td>
<p>The coordinate reference system, any format understood by <code>sf::st_crs</code>.</p>
</td></tr>
<tr><td><code id="to_sf_+3A_conn">conn</code></td>
<td>
<p>the connection object from the tbl.
Takes a duckdb table (from <code>open_dataset</code>) or a dataset or dplyr
pipline and returns an sf object. <strong>Important</strong>: the table must have
a <code>geometry</code> column, which you will almost always have to create
first.
</p>
<p>Note: <code>to_sf()</code> triggers collection into R.  This function is suitable
to use at the end of a dplyr pipeline that will subset the data.
Using this function on a large dataset without filtering first may
exceed available memory.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an <code>sf</code> class object (in memory).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

library(dplyr)
csv_file &lt;- system.file("extdata/spatial-test.csv", package="duckdbfs")

# Note that we almost always must first create a `geometry` column, e.g.
# from lat/long columns using the `st_point` method.
sf &lt;-
  open_dataset(csv_file, format = "csv") |&gt;
  mutate(geom = ST_Point(longitude, latitude)) |&gt;
  to_sf()

# We can use the full space of spatial operations, including spatial
# and normal dplyr filters.  All operations are translated into a
# spatial SQL query by `to_sf`:
open_dataset(csv_file, format = "csv") |&gt;
  mutate(geom = ST_Point(longitude, latitude)) |&gt;
  mutate(dist = ST_Distance(geom, ST_Point(0,0))) |&gt;
  filter(site %in% c("a", "b", "e")) |&gt;
  to_sf()


</code></pre>

<hr>
<h2 id='write_dataset'>write_dataset</h2><span id='topic+write_dataset'></span>

<h3>Description</h3>

<p>write_dataset
</p>


<h3>Usage</h3>

<pre><code class='language-R'>write_dataset(
  dataset,
  path,
  conn = cached_connection(),
  format = c("parquet", "csv"),
  partitioning = dplyr::group_vars(dataset),
  overwrite = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="write_dataset_+3A_dataset">dataset</code></td>
<td>
<p>a remote tbl object from <code>open_dataset</code>,
or an in-memory data.frame.</p>
</td></tr>
<tr><td><code id="write_dataset_+3A_path">path</code></td>
<td>
<p>a local file path or S3 path with write credentials</p>
</td></tr>
<tr><td><code id="write_dataset_+3A_conn">conn</code></td>
<td>
<p>duckdbfs database connection</p>
</td></tr>
<tr><td><code id="write_dataset_+3A_format">format</code></td>
<td>
<p>export format</p>
</td></tr>
<tr><td><code id="write_dataset_+3A_partitioning">partitioning</code></td>
<td>
<p>names of columns to use as partition variables</p>
</td></tr>
<tr><td><code id="write_dataset_+3A_overwrite">overwrite</code></td>
<td>
<p>allow overwriting of existing files?</p>
</td></tr>
<tr><td><code id="write_dataset_+3A_...">...</code></td>
<td>
<p>additional arguments to <code><a href="#topic+duckdb_s3_config">duckdb_s3_config()</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns the path, invisibly.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  write_dataset(mtcars, tempfile())


write_dataset(mtcars, tempdir())

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
