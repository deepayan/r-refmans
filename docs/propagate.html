<!DOCTYPE html><html><head><title>Help for package propagate</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {propagate}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bigcor'><p>Creating very large correlation/covariance matrices</p></a></li>
<li><a href='#cor2cov'><p>Converting a correlation matrix into a covariance matrix</p></a></li>
<li><a href='#datasets'><p>Datasets from the GUM &quot;Guide to the expression of uncertainties in measurement&quot; (2008)</p></a></li>
<li><a href='#fitDistr'><p>Fitting distributions to observations/Monte Carlo simulations</p></a></li>
<li><a href='#interval'><p>Uncertainty propagation based on interval arithmetics</p></a></li>
<li><a href='#makeDat'><p>Create a dataframe from the variables defined in an expression</p></a></li>
<li><a href='#makeDerivs'><p>Utility functions for creating Gradient- and Hessian-like matrices with symbolic derivatives and evaluating them in an environment</p></a></li>
<li><a href='#matrixStats'><p>Fast column- and row-wise versions of variance coded in C++</p></a></li>
<li><a href='#mixCov'><p>Aggregating covariances matrices and/or error vectors into a single covariance matrix</p></a></li>
<li><a href='#moments'><p>Skewness and (excess) Kurtosis of a vector of values</p></a></li>
<li><a href='#numDerivs'><p>Functions for creating Gradient and Hessian matrices by numerical differentiation (Richardson's method) of the partial derivatives</p></a></li>
<li><a href='#plot.propagate'><p>Plotting function for 'propagate' objects</p></a></li>
<li><a href='#predictNLS'><p>Confidence/prediction intervals for (weighted) nonlinear models based on uncertainty propagation</p></a></li>
<li><a href='#propagate'><p>Propagation of uncertainty using higher-order Taylor expansion and Monte Carlo simulation</p></a></li>
<li><a href='#rDistr'><p>Creating random samples from a variety of useful distributions</p></a></li>
<li><a href='#statVec'><p>Transform an input vector into one with defined mean and standard deviation</p></a></li>
<li><a href='#stochContr'><p>Stochastic contribution analysis of Monte Carlo simulation-derived propagated uncertainty</p></a></li>
<li><a href='#summary.propagate'><p>Summary function for 'propagate' objects</p></a></li>
<li><a href='#WelchSatter'><p>Welch-Satterthwaite approximation to the 'effective degrees of freedom'</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>no</td>
</tr>
<tr>
<td>LazyData:</td>
<td>no</td>
</tr>
<tr>
<td>Title:</td>
<td>Propagation of Uncertainty</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0-6</td>
</tr>
<tr>
<td>Date:</td>
<td>2018-05-02</td>
</tr>
<tr>
<td>Author:</td>
<td>Andrej-Nikolai Spiess &lt;a.spiess@uke.uni-hamburg.de&gt;        </td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Andrej-Nikolai Spiess &lt;a.spiess@uke.uni-hamburg.de&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Propagation of uncertainty using higher-order Taylor expansion and Monte Carlo simulation.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.13.0), MASS, tmvtnorm, Rcpp (&ge; 0.10.1), ff,
minpack.lm</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2018-05-06 09:00:48 UTC; ans</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2018-05-06 17:08:12 UTC</td>
</tr>
</table>
<hr>
<h2 id='bigcor'>Creating very large correlation/covariance matrices</h2><span id='topic+bigcor'></span>

<h3>Description</h3>

<p>The storage of a value in double format needs 8 bytes. When creating large correlation matrices, the amount of RAM might not suffice, giving rise to the dreaded <em>&quot;cannot allocate vector of size ...&quot;</em> error. For example, an input matrix with 50000 columns/100 rows will result in a correlation matrix with a size of 50000 x 50000 x 8 Byte / (1024 x 1024 x 1024) = 18.63 GByte, which is still more than most standard PCs. <code>bigcor</code> uses the framework of the 'ff' package to store the correlation/covariance matrix in a file. The complete matrix is created by filling a large preallocated empty matrix with sub-matrices at the corresponding positions. See 'Details'. Calculation time is ~ 20s for an input matrix of 10000 x 100 (cols x rows).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bigcor(x, y = NULL, fun = c("cor", "cov"), size = 2000, 
       verbose = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bigcor_+3A_x">x</code></td>
<td>
<p>the input matrix.</p>
</td></tr>
<tr><td><code id="bigcor_+3A_y">y</code></td>
<td>
<p><code>NULL</code> (default) or a vector, matrix or data frame with compatible dimensions to <code>x</code>.</p>
</td></tr>
<tr><td><code id="bigcor_+3A_fun">fun</code></td>
<td>
<p>create either a <code><a href="stats.html#topic+cor">cor</a></code>elation or <code><a href="stats.html#topic+cov">cov</a></code>ariance matrix.</p>
</td></tr> 
<tr><td><code id="bigcor_+3A_size">size</code></td>
<td>
<p>the n x n block size of the submatrices. 2000 has shown to be time-effective.</p>
</td></tr>
<tr><td><code id="bigcor_+3A_verbose">verbose</code></td>
<td>
<p>logical. If <code>TRUE</code>, information is printed in the console when running.</p>
</td></tr>
<tr><td><code id="bigcor_+3A_...">...</code></td>
<td>
<p>other parameters to be passed to <code><a href="stats.html#topic+cor">cor</a></code> or <code><a href="stats.html#topic+cor">cor</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates a correlation matrix <code class="reqn">\mathbf{C}</code> or covariance matrix <code class="reqn">\mathbf{\Sigma}</code> using the following steps:<br />
1) An input matrix <code>x</code> with <code class="reqn">N</code> columns is split into <code class="reqn">k</code> equal size blocks (+ a possible remainder block) <code class="reqn">A_1, A_2, \ldots,  A_k</code> of size <code class="reqn">n</code>. The block size can be defined by the user, <code>size = 2000</code> is a good value because <code><a href="stats.html#topic+cor">cor</a></code> can handle this quite quickly (~ 400 ms). For example, if the matrix has 13796 columns, the split will be <code class="reqn">A_1 = 1 \ldots 2000; A_2 = 2001 \ldots 4000; A_3 = 4001 \ldots 6000; A_4 = 6000 \ldots 8000 ; A_5 = 8001 \ldots 10000; A_6 = 10001 \ldots 12000; A_7 = 12001 \ldots 13796</code>.<br />
2) For all pairwise combinations of blocks <code class="reqn">k \choose 2</code>, the <code class="reqn">n \times n</code> correlation sub-matrix is calculated. If <code>y = NULL</code>, <code class="reqn">\mathrm{cor}(A_1, A_1), \mathrm{cor}(A_1, A_2), \ldots, \mathrm{cor}(A_k, A_k)</code>, otherwise <code class="reqn">\mathrm{cor}(A_1, y), \mathrm{cor}(A_2, y), \ldots, \mathrm{cor}(A_k, y)</code>.<br />
3) The sub-matrices are transferred into a preallocated <code class="reqn">N \times N</code> empty matrix at the corresponding position (where the correlations would usually reside). To ensure symmetry around the diagonal, this is done twice in the upper and lower triangle. If <code>y</code> was supplied, a <code class="reqn">N \times M</code> matrix is filled, with <code class="reqn">M</code> = number of columns in <code>y</code>. <br />
</p>
<p>Since the resulting matrix is in 'ff' format, one has to subset to extract regions into normal <code><a href="base.html#topic+matrix">matrix</a></code>-like objects. See 'Examples'.
</p>


<h3>Value</h3>

<p>The corresponding correlation/covariance matrix in 'ff' format.
</p>


<h3>Author(s)</h3>

<p>Andrej-Nikolai Spiess
</p>


<h3>References</h3>

<p><a href="http://rmazing.wordpress.com/2013/02/22/bigcor-large-correlation-matrices-in-r/">http://rmazing.wordpress.com/2013/02/22/bigcor-large-correlation-matrices-in-r/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Small example to prove similarity
## to standard 'cor'. We create a matrix
## by subsetting the complete 'ff' matrix.
MAT &lt;- matrix(rnorm(70000), ncol = 700)
COR &lt;- bigcor(MAT, size= 500, fun = "cor")
COR &lt;- COR[1:nrow(COR), 1:ncol(COR)]
all.equal(COR, cor(MAT)) # =&gt; TRUE

## Example for cor(x, y) with 
## y = small matrix.
MAT1 &lt;- matrix(rnorm(50000), nrow = 10)
MAT2 &lt;- MAT1[, 4950:5000]
COR &lt;- cor(MAT1, MAT2)
BCOR &lt;- bigcor(MAT1, MAT2)
BCOR &lt;- BCOR[1:5000, 1:ncol(BCOR)] # =&gt; convert 'ff' to 'matrix'
all.equal(COR, BCOR)

## Not run: 
## Create large matrix.
MAT &lt;- matrix(rnorm(57500), ncol = 5750)
COR &lt;- bigcor(MAT, size= 2000, fun = "cor")

## Extract submatrix.
SUB &lt;- COR[1:3000, 1:3000]
all.equal(SUB, cor(MAT[, 1:3000]))

## End(Not run)
</code></pre>

<hr>
<h2 id='cor2cov'>Converting a correlation matrix into a covariance matrix</h2><span id='topic+cor2cov'></span>

<h3>Description</h3>

<p>Converts a correlation matrix into a covariance matrix using variance information. It is therefore the opposite of <code><a href="stats.html#topic+cov2cor">cov2cor</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cor2cov(C, var)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cor2cov_+3A_c">C</code></td>
<td>
<p>a symmetric numeric correlation matrix <code class="reqn">\mathbf{C}</code>.</p>
</td></tr>
<tr><td><code id="cor2cov_+3A_var">var</code></td>
<td>
<p>a vector of variances <code class="reqn">\sigma_n^2</code>.</p>
</td></tr>   
</table>


<h3>Details</h3>

<p>Calculates the covariance matrix <code class="reqn">\mathbf{\Sigma}</code> using a correlation matrix <code class="reqn">\mathbf{C}</code> and outer products of the standard deviations <code class="reqn">\sigma_n</code>:
</p>
<p style="text-align: center;"><code class="reqn">\mathbf{\Sigma} = \mathbf{C} \cdot \sigma_n \otimes \sigma_n</code>
</p>



<h3>Value</h3>

<p>The corresponding covariance matrix.
</p>


<h3>Author(s)</h3>

<p>Andrej-Nikolai Spiess
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Example in Annex H.2 from the GUM 2008 manual
## (see 'References'), simultaneous resistance
## and reactance measurement.
data(H.2)
attach(H.2)

## Original covariance matrix.
COV &lt;- cov(H.2)
## extract variances
VAR &lt;- diag(COV)

## cor2cov covariance matrix.
COV2 &lt;- cor2cov(cor(H.2), VAR) 

## Equal to original covariance matrix.
all.equal(COV2, COV)
</code></pre>

<hr>
<h2 id='datasets'>Datasets from the GUM &quot;Guide to the expression of uncertainties in measurement&quot; (2008)</h2><span id='topic+H.2'></span><span id='topic+H.3'></span><span id='topic+H.4'></span>

<h3>Description</h3>

<p>Several datasets found in &quot;Annex H&quot; of the GUM that are used in illustrating the different approaches to quantifying measurement uncertainty.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>H.2
H.3
H.4
</code></pre>


<h3>Details</h3>

<p><b>H.2: Simultaneous resistance and reactance measurement, Table H.2</b><br />
This example demonstrates the treatment of multiple measurands or output quantities determined simultaneously in the same measurement and the correlation of their estimates. It considers only the random variations of the observations; in actual practice, the uncertainties of corrections for systematic effects would also contribute to the uncertainty of the measurement results. The data are analysed in two different ways with each yielding essentially the same numerical values.<br /> 
H.2.1  The measurement problem:<br /> 
The resistance <em>R</em> and the reactance <em>X</em> of a circuit element are determined by measuring the amplitude <em>V</em> of a sinusoidally-alternating  potential  difference  across  its  terminals,  the  amplitude  <em>I</em>  of  the  alternating  current passing through it, and the phase-shift angle <code class="reqn">\phi</code> of the alternating potential difference relative to the alternating current. Thus the three input quantities are <em>V</em>, <em>I</em>, and <code class="reqn">\phi</code> and the three output quantities -the measurands- are the three impedance components <em>R</em>, <em>X</em>, and <em>Z</em>. Since <code class="reqn">Z^2 = R^2 + X^2</code>, there are only two independent output 
quantities.<br />
H.2.2  Mathematical model and data:<br />
The measurands are related to the input quantities by Ohm's law: 
</p>
<p style="text-align: center;"><code class="reqn">R = \frac{V}{I}\cos\phi;\quad X = \frac{V}{I}\sin\phi;\quad Z = \frac{V}{I} \qquad (\mathrm{H.7})</code>
</p>

<p><b>H.3: Calibration of a thermometer, Table H.6</b><br />
This example illustrates the use of the method of least squares to obtain a linear calibration curve and how the parameters of the fit, the intercept and slope, and their estimated variances and covariance, are used to 
obtain from the curve the value and standard uncertainty of a predicted correction.<br /> 
H.3.1 The measurement problem:<br />
A thermometer is calibrated by comparing <em>n</em> = 11 temperature readings <code class="reqn">t_k</code> of the thermometer, each having negligible uncertainty, with corresponding known reference temperatures <code class="reqn">t_{R,k}</code> in the temperature range 21°C to 27°C to obtain the corrections <code class="reqn">b_k = t_{R,k} - t_k</code> to the readings. The <em>measured</em> corrections <code class="reqn">b_k</code> and <em>measured</em> temperatures <code class="reqn">t_k</code> are the input quantities of the evaluation. A linear calibration curve </p>
<p style="text-align: center;"><code class="reqn">b(t) = y_1 + y_2(t-t_0) \qquad (\mathrm{H.12})</code>
</p>
<p> is fitted to the measured corrections and temperatures by the method of least squares. The parameters <code class="reqn">y_1</code> and <code class="reqn">y_2</code>, which are respectively the intercept and slope of the calibration curve, are the two measurands or output 
quantities to be determined. The temperature <code class="reqn">t_0</code> is a conveniently chosen exact reference temperature; it is not an independent parameter to be determined by the least-squares fit. Once <code class="reqn">y_1</code> and <code class="reqn">y_2</code> are found, along with their  estimated  variances  and  covariance, Equation (H.12)  can  be  used  to  predict  the  value  and  standard uncertainty of the correction to be applied to the thermometer for any value <code class="reqn">t</code> of the temperature. 
</p>
<p><b>H.4: Measurement of activity, Table H.7</b><br />
This example is similar to example H.2, the simultaneous measurement of resistance and reactance, in that 
the data can be analysed in two different ways but each yields essentially the same numerical result. The first approach  illustrates  once  again  the  need  to  take  the  observed  correlations  between  input  quantities  into account.<br /> 
H.4.1  The measurement problem:<br /> 
The  unknown  radon (<code class="reqn">{}^{222}\mathrm{Rn}</code>) activity concentration in a water sample is determined by liquid-scintillation counting against a radon-in-water standard sample having a known activity concentration. The unknown activity concentration is obtained by measuring three counting sources consisting of approximately 5g of water and 12g of organic emulsion scintillator in vials of volume 22ml:<br />
Source (a) a  <em>standard</em>  consisting  of  a  mass <code class="reqn">m_S</code> of  the  standard  solution  with  a  known  activity concentration;<br />
Source (b) a  matched  <em>blank</em>  water  sample  containing  no  radioactive  material,  used  to  obtain  the background counting rate;<br />
Source (c) the <em>sample</em> consisting of an aliquot of mass <code class="reqn">m_x</code> with unknown activity concentration.<br />
Six cycles of measurement of the three counting sources are made in the order standard - blank - sample; and  each  dead-time-corrected counting  interval <code class="reqn">T_0</code> for each source  during  all six cycles is 60 minutes. Although the background counting rate cannot be assumed to be constant over the entire counting interval (65 hours), it is assumed that the number of counts obtained for each blank may be used as representative of the background counting rate during the measurements of the standard and sample in the same cycle. The 
data are given in Table H.7, where<br />
<code class="reqn">t_S, t_B, t_x</code> are  the  times  from  the  reference  time  <code class="reqn">t</code> = 0  to  the  midpoint  of  the  dead-time-corrected counting intervals <code class="reqn">T_0</code> = 60 min for the standard, blank, and sample vials, respectively; although <code class="reqn">t_B</code> is given for completeness, it is not needed in the analysis;<br />
<code class="reqn">C_S, C_B, C_x</code> are the number of counts recorded in the dead-time-corrected counting intervals <code class="reqn">T_0</code> = 60 min for the standard, blank, and sample vials, respectively.<br />
The observed counts may be expressed as<br />
</p>
<p style="text-align: center;"><code class="reqn">C_S = C_B + \varepsilon A_S T_0 m_S e^{-\lambda t_S} \qquad (\mathrm{H.18a})</code>
</p>

<p style="text-align: center;"><code class="reqn">C_x = C_B + \varepsilon A_x T_0 m_x e^{-\lambda t_x} \qquad (\mathrm{H.18b})</code>
</p>

<p>where<br />
<code class="reqn">\varepsilon</code> is the liquid scintillation detection efficiency for <code class="reqn">{}^{222}\mathrm{Rn}</code> for a given source composition, assumed to be independent of the activity level;<br />
<code class="reqn">A_S</code> is the activity concentration of the standard at the reference time <code class="reqn">t</code> = 0;<br />
<code class="reqn">A_x</code> is the measurand and is defined as the unknown activity concentration of the sample at the reference time <code class="reqn">t</code> = 0;<br />
<code class="reqn">m_S</code> is the mass of the standard solution;<br />
<code class="reqn">m_x</code> is the mass of the sample aliquot;<br />
<code class="reqn">\lambda</code> is the decay constant for <code class="reqn">{}^{222}\mathrm{Rn}</code>: <code class="reqn">\lambda = (ln 2)/T_{1/2} = 1.25894 \cdot 10^{-4}\ \mathrm{min}^{-1} (T_{1/2} = 5505.8\ \mathrm{min})</code>.<br />
(...) This suggests combining Equations (H.18a) and (H.18b) to obtain the following expression for the unknown concentration in terms of the known quantities:<br />
</p>
<p style="text-align: center;"><code class="reqn">... = A_S \frac{m_S}{m_x}\frac{C_x - C_B}{C_S - C_B}e^{\lambda(t_x - t_S)} \qquad (\mathrm{H.19})</code>
</p>

<p>where <code class="reqn">(C_x - C_B)e^{\lambda t_x}</code> and <code class="reqn">(C_S - C_B)e^{\lambda t_S}</code> are, respectively, the background-corrected counts of the sample and the standard  at the reference time <code class="reqn">t</code> = 0 and for the time interval <code class="reqn">T_0</code> = 60 min.
</p>


<h3>Author(s)</h3>

<p>Andrej-Nikolai Spiess, taken mainly from the GUM 2008 manual.
</p>


<h3>References</h3>

<p>Evaluation of measurement data - Guide to the expression of uncertainty in measurement.<br />
JCGM 100:2008 (GUM 1995 with minor corrections).<br />
<a href="http://www.bipm.org/utils/common/documents/jcgm/JCGM_100_2008_E.pdf">http://www.bipm.org/utils/common/documents/jcgm/JCGM_100_2008_E.pdf</a>.
</p>
<p>Evaluation of measurement data - Supplement 1 to the Guide to the expression of uncertainty in measurement - Propagation of distributions using a Monte Carlo Method.<br />
JCGM 101:2008.<br />
<a href="http://www.bipm.org/utils/common/documents/jcgm/JCGM_101_2008_E.pdf">http://www.bipm.org/utils/common/documents/jcgm/JCGM_101_2008_E.pdf</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## See "Examples" in 'propagate'.
</code></pre>

<hr>
<h2 id='fitDistr'>Fitting distributions to observations/Monte Carlo simulations</h2><span id='topic+fitDistr'></span>

<h3>Description</h3>

<p>This function fits 32 different continuous distributions by (weighted) NLS to the histogram of Monte Carlo simulation results as obtained by <code><a href="#topic+propagate">propagate</a></code> or any other vector containing large-scale observations. Finally, the fits are sorted by ascending <code><a href="stats.html#topic+BIC">BIC</a></code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fitDistr(object, nbin = 100, weights = FALSE, verbose = TRUE, 
         brute = c("fast", "slow"), plot = c("hist", "qq"),  distsel = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fitDistr_+3A_object">object</code></td>
<td>
<p>an object of class 'propagate' or a vector containing observations.</p>
</td></tr>
<tr><td><code id="fitDistr_+3A_nbin">nbin</code></td>
<td>
<p>the number of bins in the histogram.</p>
</td></tr>
<tr><td><code id="fitDistr_+3A_weights">weights</code></td>
<td>
<p>numeric or logical. Either a numeric vector of weights, or if <code>TRUE</code>, the distributions are fitted with weights = 1/(counts per bin).</p>
</td></tr>
<tr><td><code id="fitDistr_+3A_verbose">verbose</code></td>
<td>
<p>logical. If <code>TRUE</code>, steps of the analysis are printed to the console.</p>
</td></tr>
<tr><td><code id="fitDistr_+3A_brute">brute</code></td>
<td>
<p>complexity of the brute force approach. See 'Details'.</p>
</td></tr>
<tr><td><code id="fitDistr_+3A_plot">plot</code></td>
<td>
<p>if <code>"hist"</code>, a plot with the &quot;best&quot; distribution (in terms of lowest BIC) on top of the histogram is displayed. If <code>"qq"</code>, a QQ-Plot will display the difference between the observed and fitted quantiles.</p>
</td></tr>
<tr><td><code id="fitDistr_+3A_distsel">distsel</code></td>
<td>
<p>a vector of distribution numbers to select from the complete cohort as listed below, e.g. <code>c(1:10, 15)</code>.</p>
</td></tr>
<tr><td><code id="fitDistr_+3A_...">...</code></td>
<td>
<p>other parameters to be passed to the plots.</p>
</td></tr>  
</table>


<h3>Details</h3>

<p>Fits the following 32 distributions using (weighted) residual sum-of-squares as the minimization criterion for <code><a href="minpack.lm.html#topic+nls.lm">nls.lm</a></code>:<br />
1) Normal distribution (<code>dnorm</code>) =&gt; <a href="https://en.wikipedia.org/wiki/Normal_distribution">https://en.wikipedia.org/wiki/Normal_distribution</a><br />
2) Skewed-normal distribution (<code>propagate:::dsn</code>) =&gt; <a href="https://en.wikipedia.org/wiki/Skew_normal_distribution">https://en.wikipedia.org/wiki/Skew_normal_distribution</a><br />
3) Generalized normal distribution (<code>propagate:::dgnorm</code>) =&gt; <a href="https://en.wikipedia.org/wiki/Generalized_normal_distribution">https://en.wikipedia.org/wiki/Generalized_normal_distribution</a><br /> 
4) Log-normal distribution (<code>dlnorm</code>) =&gt; <a href="https://en.wikipedia.org/wiki/Log-normal_distribution">https://en.wikipedia.org/wiki/Log-normal_distribution</a><br /> 
5) Scaled and shifted t-distribution (<code>propagate:::dst</code>) =&gt; GUM 2008, Chapter 6.4.9.2.<br />
6) Logistic distribution (<code>dlogis</code>) =&gt; <a href="https://en.wikipedia.org/wiki/Logistic_distribution">https://en.wikipedia.org/wiki/Logistic_distribution</a><br /> 
7) Uniform distribution (<code>dunif</code>) =&gt; <a href="https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)">https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)</a><br /> 
8) Triangular distribution (<code>propagate:::dtriang</code>) =&gt; <a href="https://en.wikipedia.org/wiki/Triangular_distribution">https://en.wikipedia.org/wiki/Triangular_distribution</a><br /> 
9) Trapezoidal distribution (<code>propagate:::dtrap</code>) =&gt; <a href="https://en.wikipedia.org/wiki/Trapezoidal_distribution">https://en.wikipedia.org/wiki/Trapezoidal_distribution</a><br /> 
10) Curvilinear Trapezoidal distribution (<code>propagate:::dctrap</code>) =&gt; GUM 2008, Chapter 6.4.3.1<br />
11) Gamma distribution (<code>dgamma</code>) =&gt; <a href="https://en.wikipedia.org/wiki/Gamma_distribution">https://en.wikipedia.org/wiki/Gamma_distribution</a><br /> 
12) Inverse Gamma distribution (<code>propagate:::dinvgamma</code>) =&gt; <a href="https://en.wikipedia.org/wiki/Inverse-gamma_distribution">https://en.wikipedia.org/wiki/Inverse-gamma_distribution</a><br /> 
13) Cauchy distribution (<code>dcauchy</code>) =&gt; <a href="https://en.wikipedia.org/wiki/Cauchy_distribution">https://en.wikipedia.org/wiki/Cauchy_distribution</a><br /> 
14) Laplace distribution (<code>propagate:::dlaplace</code>) =&gt; <a href="https://en.wikipedia.org/wiki/Laplace_distribution">https://en.wikipedia.org/wiki/Laplace_distribution</a><br /> 
15) Gumbel distribution (<code>propagate:::dgumbel</code>) =&gt; <a href="https://en.wikipedia.org/wiki/Gumbel_distribution">https://en.wikipedia.org/wiki/Gumbel_distribution</a><br /> 
16) Johnson SU distribution (<code>propagate:::dJSU</code>) =&gt; <a href="https://en.wikipedia.org/wiki/Johnson_SU_distribution">https://en.wikipedia.org/wiki/Johnson_SU_distribution</a><br /> 
17) Johnson SB distribution (<code>propagate:::dJSB</code>) =&gt; <a href="https://www.mathwave.com/articles/johnson_sb_distribution.html">https://www.mathwave.com/articles/johnson_sb_distribution.html</a><br /> 
18) Three-parameter Weibull distribution (<code>propagate:::dweibull2</code>) =&gt; <a href="https://en.wikipedia.org/wiki/Weibull_distribution">https://en.wikipedia.org/wiki/Weibull_distribution</a><br /> 
19) Two-parameter beta distribution (<code>dbeta2</code>) =&gt; <a href="https://en.wikipedia.org/wiki/Beta_distribution#Two_parameters_2">https://en.wikipedia.org/wiki/Beta_distribution#Two_parameters_2</a><br />
20) Four-parameter beta distribution (<code>propagate:::dbeta2</code>) =&gt; <a href="https://en.wikipedia.org/wiki/Beta_distribution#Four_parameters_2">https://en.wikipedia.org/wiki/Beta_distribution#Four_parameters_2</a><br /> 
21) Arcsine distribution (<code>propagate:::darcsin</code>) =&gt; <a href="https://en.wikipedia.org/wiki/Arcsine_distribution">https://en.wikipedia.org/wiki/Arcsine_distribution</a><br /> 
22) Von Mises distribution (<code>propagate:::dmises</code>) =&gt; <a href="https://en.wikipedia.org/wiki/Von_Mises_distribution">https://en.wikipedia.org/wiki/Von_Mises_distribution</a><br /> 
23) Inverse Gaussian distribution (<code>propagate:::dinvgauss</code>) =&gt; <a href="https://en.wikipedia.org/wiki/Inverse_Gaussian_distribution">https://en.wikipedia.org/wiki/Inverse_Gaussian_distribution</a><br /> 
24) Generalized Extreme Value distribution (<code>propagate:::dgevd</code>) =&gt; <a href="https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution">https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution</a><br /> 
25) Rayleigh distribution (<code>propagate:::drayleigh</code>) =&gt; <a href="https://en.wikipedia.org/wiki/Rayleigh_distribution">https://en.wikipedia.org/wiki/Rayleigh_distribution</a><br />
26) Chi-square distribution (<code>dchisq</code>) =&gt; <a href="https://en.wikipedia.org/wiki/Chi-squared_distribution">https://en.wikipedia.org/wiki/Chi-squared_distribution</a><br />
27) Exponential distribution (<code>dexp</code>) =&gt; <a href="https://en.wikipedia.org/wiki/Exponential_distribution">https://en.wikipedia.org/wiki/Exponential_distribution</a><br />
28) F-distribution (<code>df</code>) =&gt; <a href="https://en.wikipedia.org/wiki/F-distribution">https://en.wikipedia.org/wiki/F-distribution</a><br />
29) Burr distribution (<code>dburr</code>) =&gt; <a href="https://en.wikipedia.org/wiki/Burr_distribution">https://en.wikipedia.org/wiki/Burr_distribution</a><br />
30) Chi distribution (<code>dchi</code>) =&gt; <a href="https://en.wikipedia.org/wiki/Chi_distribution">https://en.wikipedia.org/wiki/Chi_distribution</a><br />
31) Inverse Chi-square distribution (<code>dinvchisq</code>) =&gt; <a href="https://en.wikipedia.org/wiki/Inverse-chi-squared_distribution">https://en.wikipedia.org/wiki/Inverse-chi-squared_distribution</a><br />
32) Cosine distribution (<code>dcosine</code>) =&gt; <a href="https://en.wikipedia.org/wiki/Raised_cosine_distribution">https://en.wikipedia.org/wiki/Raised_cosine_distribution</a><br />
</p>
<p>All distributions are fitted with a brute force approach, in which the parameter space is extended over three orders of magnitude <code class="reqn">(0.1, 1, 10)\times \beta_i</code> when <code>brute = "fast"</code>, or five orders <code class="reqn">(0.01, 0.1, 1, 10, 100)\times \beta_i</code> when <code>brute = "slow"</code>. Approx. 20-90s are needed to fit for the fast version, depending mainly on the number of bins. 
</p>
<p>The goodness-of-fit (GOF) is calculated with <code><a href="stats.html#topic+BIC">BIC</a></code> from the (weighted) log-likelihood of the fit:
</p>
<p style="text-align: center;"><code class="reqn">\rm{ln}(L) = 0.5 \cdot \left(-N \cdot \left(\rm{ln}(2\pi) + 1 + \rm{ln}(N) - \sum_{i=1}^n log(w_i) + \rm{ln}\left(\sum_{i=1}^n w_i \cdot x_i^2\right) \right) \right)</code>
</p>
 
<p style="text-align: center;"><code class="reqn">\rm{BIC} = - 2\rm{ln}(L) + (N - k)ln(N)</code>
</p>

<p>with <code class="reqn">x_i</code> = the residuals from the NLS fit, <code class="reqn">N</code> = the length of the residual vector, <code class="reqn">k</code> = the number of parameters of the fitted model and <code class="reqn">w_i</code> = the weights.
</p>
<p>In contrast to some other distribution fitting softwares (i.e. Easyfit, Mathwave) that use residual sum-of-squares/Anderson-Darling/Kolmogorov-Smirnov statistics as GOF measures, the application of BIC accounts for increasing number of parameters in the distribution fit and therefore compensates for overfitting. Hence, this approach is more similar to ModelRisk (Vose Software) and as employed in <code>fitdistr</code> of the 'MASS' package.
Another application is to identify a possible distribution for the raw data prior to using Monte Carlo simulations from this distribution. However, a decent number of observations should be at hand in order to obtain a realistic estimate of the proper distribution. See 'Examples'.<br />
The code for the density functions can be found in file &quot;distr-densities.R&quot;.
</p>
<p>IMPORTANT: It can be feasible to set <code>weights = TRUE</code> in order to give more weight to bins with low counts. See 'Examples'.
ALSO: Distribution fitting is highly sensitive to the number of defined histogram bins, so it is advisable to change this parameter and inspect if the order of fitted distributions remains stable.
</p>


<h3>Value</h3>

<p>A list with the following items:
</p>
<p><code>stat</code>: the by BIC value ascendingly sorted distribution names, including RSS and MSE.<br />
<code>fit</code>: a list of the results from <code><a href="minpack.lm.html#topic+nls.lm">nls.lm</a></code> for each distribution model, also sorted ascendingly by BIC values.<br />
<code>par</code>: a list of the estimated parameters of the models in <code>fit</code>.<br />
<code>se</code>: a list of the parameters' standard errors, calculated from the square root of the covariance matrices diagonals.<br />
<code>dens</code>: a list with all density function used for fitting, sorted as in <code>fit</code>.<br />
<code>bestfit</code>: the best model in terms of lowest BIC.<br />
<code>bestpar</code>: the parameters of <code>bestfit</code>.<br />
<code>bestse</code>: the parameters' standard errors of <code>bestfit</code>.<br />
<code>fitted</code>: the fitted values of <code>bestfit</code>.<br />
<code>residuals</code>: the residuals of <code>bestfit</code>.<br />
</p>


<h3>Author(s)</h3>

<p>Andrej-Nikolai Spiess
</p>


<h3>References</h3>

<p>Continuous univariate distributions, Volume 1.<br />
Johnson NL, Kotz S and Balakrishnan N.<br />
<em>Wiley Series in Probability and Statistics, 2.ed</em> (2004).
</p>
<p>A guide on probability distributions.<br />
R-forge distributions core team.<br />
<a href="http://dutangc.free.fr/pub/prob/probdistr-main.pdf">http://dutangc.free.fr/pub/prob/probdistr-main.pdf</a>.
</p>
<p>Univariate distribution relationships.<br />
Leemis LM and McQueston JT.<br />
<em>The American Statistician</em> (2008), <b>62</b>: 45-53.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Linear example, small error
## =&gt; Normal distribution.
EXPR1 &lt;- expression(x + 2 * y)
x &lt;- c(5, 0.01)
y &lt;- c(1, 0.01)
DF1 &lt;- cbind(x, y)
RES1 &lt;- propagate(expr = EXPR1, data = DF1, type = "stat", 
                  do.sim = TRUE, verbose = TRUE)
fitDistr(RES1)

## Ratio example, larger error
## =&gt; Gamma distribution.
EXPR2 &lt;- expression(x/2 * y)
x &lt;- c(5, 0.1)
y &lt;- c(1, 0.02)
DF2 &lt;- cbind(x, y)
RES2 &lt;- propagate(expr = EXPR2, data = DF2, type = "stat", 
                  do.sim = TRUE, verbose = TRUE)
fitDistr(RES2)

## Exponential example, large error
## =&gt; Log-Normal distribution.
EXPR3 &lt;- expression(x^(2 * y))
x &lt;- c(5, 0.1)
y &lt;- c(1, 0.1)
DF3 &lt;- cbind(x, y)
RES3 &lt;- propagate(expr = EXPR3, data = DF3, type = "stat", 
                  do.sim = TRUE, verbose = TRUE)
fitDistr(RES3)

## Rectangular input distributions result
## in Curvilinear Trapezoidal output distribution.
A &lt;- runif(100000, 20, 25)
B &lt;- runif(100000, 3, 3.5)
DF4 &lt;- cbind(A, B)
EXPR4 &lt;- expression(A + B)
RES4 &lt;- propagate(EXPR4, data = DF4, type = "sim", 
                 use.cov = FALSE, do.sim = TRUE)
fitDistr(RES4)        

## Fitting with 1/counts as weights.
EXPR5 &lt;- expression(x + 2 * y)
x &lt;- c(5, 0.05)
y &lt;- c(1, 0.05)
DF5 &lt;- cbind(x, y)
RES5 &lt;- propagate(expr = EXPR5, data = DF5, type = "stat", 
                  do.sim = TRUE, verbose = TRUE, weights = TRUE)
fitDistr(RES5)

## Using only selected distributions.
EXPR6 &lt;- expression(x + sin(y))
x &lt;- c(5, 0.1)
y &lt;- c(1, 0.2)
DF6 &lt;- cbind(x, y)
RES6 &lt;- propagate(expr = EXPR6, data = DF6, type = "stat", 
                  do.sim = TRUE)
fitDistr(RES6, distsel = c(1:10, 15, 28))

## End(Not run)
</code></pre>

<hr>
<h2 id='interval'>Uncertainty propagation based on interval arithmetics</h2><span id='topic+interval'></span>

<h3>Description</h3>

<p>Calculates the uncertainty of a model by using interval arithmetics based on a &quot;combinatorial sequence grid evaluation&quot; approach, thereby avoiding the classical dependency problem that inflates the result interval.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>interval(df, expr, seq = 10, plot = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="interval_+3A_df">df</code></td>
<td>
<p>a 2-row dataframe/matrix with lower border values <code class="reqn">A_i</code> in the first row and upper border values <code class="reqn">B_i</code> in the second row. Column names must correspond to the variable names in <code>expr</code>.</p>
</td></tr>
<tr><td><code id="interval_+3A_expr">expr</code></td>
<td>
<p>an expression, such as <code>expression(x/y)</code>.</p>
</td></tr>   
<tr><td><code id="interval_+3A_seq">seq</code></td>
<td>
<p>the sequence length from <code class="reqn">A_i</code> to <code class="reqn">B_i</code> in <code class="reqn">[A_i, B_i]</code>.</p>
</td></tr>   
<tr><td><code id="interval_+3A_plot">plot</code></td>
<td>
<p>logical. If <code>TRUE</code>, plots the evaluations and min/max values as blue border lines.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For two variables <code class="reqn">{\color{red}x}, {\color{blue}y}</code> with intervals <code class="reqn">[{\color{red}x_1}, {\color{red}x_2}]</code> and <code class="reqn">[{\color{blue}y_1}, {\color{blue}y_2}]</code>, the four basic arithmetic operations <code class="reqn">\langle \mathrm{op} \rangle \in \{+, -, \cdot, /\}</code> are
</p>
<p style="text-align: center;"><code class="reqn">[{\color{red}x_1}, {\color{red}x_2}] \,\langle\!\mathrm{op}\!\rangle\, [{\color{blue}y_1}, {\color{blue}y_2}] = </code>
</p>

<p style="text-align: center;"><code class="reqn">\left[ \min({\color{red}x_1} {\langle\!\mathrm{op}\!\rangle} {\color{blue}y_1}, {\color{red}x_1} \langle\!\mathrm{op}\!\rangle {\color{blue}y_2}, {\color{red}x_2} \langle\!\mathrm{op}\!\rangle {\color{blue}y_1}, {\color{red}x_2} \langle\!\mathrm{op}\!\rangle {\color{blue}y_2}),  \max({\color{red}x_1} {\langle\!\mathrm{op}\!\rangle} {\color{blue}y_1}, {\color{red}x_1} {\langle\!\mathrm{op}\!\rangle} {\color{blue}y_2}, {\color{red}x_2} {\langle\!\mathrm{op}\!\rangle} {\color{blue}y_1}, {\color{red}x_2} {\langle\!\mathrm{op}\!\rangle} {\color{blue}y_2})\right]
</code>
</p>

<p>So for a function <code class="reqn">f([{\color{red}x_1}, {\color{red}x_2}], [{\color{blue}y_1}, {\color{blue}y_2}], [{\color{green}z_1}, {\color{green}z_2}], ...)</code> with <code class="reqn">k</code> variables, we have to create all combinations <code class="reqn">C_i = {\{\{{\color{red}x_1}, {\color{red}x_2}\}, \{{\color{blue}y_1}, {\color{blue}y2}\}, \{{\color{green}z_1}, {\color{green}z_2}\}, ...\} \choose k}</code>, evaluate their function values <code class="reqn">R_i = f(C_i)</code> and select <code class="reqn">R = [\min R_i, \max R_i]</code>.<br /> 
The so-called <em>dependency problem</em> is a major obstacle to the application of interval arithmetic and arises when the same variable exists in several terms of a complicated and often nonlinear function. In these cases, over-estimation can cover a range that is significantly larger, i.e. <code class="reqn">\min R_i \ll \min f(x, y, z, ...) , \max R_i \gg \max f(x, y, z, ...)</code>. For an example, see <a href="http://en.wikipedia.org/w/index.php?title=Interval_arithmetic">http://en.wikipedia.org/w/index.php?title=Interval_arithmetic</a> under &quot;Dependency problem&quot;. A partial solution to this problem is to refine <code class="reqn">R_i</code> by dividing <code class="reqn">[{\color{red}x_1}, {\color{red}x_2}]</code> into <code class="reqn">i</code> smaller subranges to obtain sequence <code class="reqn">({\color{red}x_1}, x_{1.1}, x_{1.2}, x_{1.i}, {\color{red}x_2})</code>. Again, all combinations are evaluated as described above, resulting in a larger number of <code class="reqn">R_i</code> in which <code class="reqn">\min R_i</code> and <code class="reqn">\max R_i</code> may be closer to <code class="reqn">\min f(x, y, z, ...)</code> and <code class="reqn">\max f(x, y, z, ...)</code>, respectively. This is the &quot;combinatorial sequence grid evaluation&quot; approach which works quite well in scenarios where monotonicity changes direction (see 'Examples'), obviating the need to create multivariate derivatives (Hessians) or use some multivariate minimization algorithm.<br />
If intervals are of type <code class="reqn">[{\color{red}x_1} &lt; 0, {\color{red}x_2} &gt; 0]</code>, a zero is included into the middle of the sequence to avoid wrong results in case of even powers, i.e. <code class="reqn">[-1, 1]^2 = [-1, 1][-1, 1] = [-1, 1]</code> when actually the right interval is <code class="reqn">[0, 1]</code>, see <code>curve(x^2, -1, 1)</code>.
</p>


<h3>Value</h3>

<p>A 2-element vector with the resulting interval and an (optional) plot of all evaluations.
</p>


<h3>Author(s)</h3>

<p>Andrej-Nikolai Spiess
</p>


<h3>References</h3>

<p><b>Wikipedia entry is quite good, especially the section
on the 'dependency problem':</b><br />
<a href="http://en.wikipedia.org/wiki/Interval_arithmetic">http://en.wikipedia.org/wiki/Interval_arithmetic</a>
</p>
<p><b>Comparison to Monte Carlo and error propagation:</b><br />
Interval Arithmetic in Power Flow Analysis.<br />
Wang Z &amp; Alvarado FL.<br />
Power Industry Computer Application Conference (1991): 156-162.
</p>
<p><b>Computer implementation</b><br />
Interval arithmetic: From principles to implementation.<br />
Hickey T, Ju Q, Van Emden MH.<br />
<em>JACM</em> (2001), <b>48</b>: 1038-1068.
</p>
<p>Complete Interval Arithmetic and its Implementation on the Computer.<br />
Kulisch UW.<br />
In: Numerical Validation in Current Hardware Architectures.
Lecture Notes in Computer Science <b>5492</b> (2009): 7-26. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Example 1: even squaring of negative interval.
EXPR1 &lt;- expression(x^2)
DAT1 &lt;- data.frame(x = c(-1, 1))
interval(DAT1, EXPR1)

## Example 2: A complicated nonlinear model.
## Reduce sequence length to 2 =&gt; original interval
## for quicker evaluation.
EXPR2 &lt;- expression(C * sqrt((520 * H * P)/(M *(t + 460))))
H &lt;- c(64, 65)
M &lt;- c(16, 16.2)
P &lt;- c(361, 365)
t &lt;- c(165, 170)
C &lt;- c(38.4, 38.5)
DAT2 &lt;- makeDat(EXPR2)
interval(DAT2, EXPR2, seq = 2)

## Example 3: Body Mass Index taken from
## http://en.wikipedia.org/w/index.php?title=Interval_arithmetic
EXPR3 &lt;- expression(m/h^2)
m &lt;- c(79.5, 80.5)
h &lt;- c(1.795, 1.805)
DAT3 &lt;- makeDat(EXPR3)
interval(DAT3, EXPR3)

## Example 4: Linear model.
EXPR4 &lt;- expression(a * x + b)
a &lt;- c(1, 2)
b &lt;- c(5, 7)
x &lt;- c(2, 3)
DAT4 &lt;- makeDat(EXPR4)
interval(DAT4, EXPR4)

## Example 5: Overestimation from dependency problem.
# Original interval with seq = 2 =&gt; [1, 7]
EXPR5 &lt;- expression(x^2 - x + 1)
x &lt;- c(-2, 1)
DAT5 &lt;- makeDat(EXPR5)
interval(DAT5, EXPR5, seq = 2)

# Refine with large sequence =&gt; [0.75, 7]
interval(DAT5, EXPR5, seq = 100)
# Tallies with curve function.
curve(x^2 - x + 1, -2, 1)

## Example 6: Underestimation from dependency problem.
# Original interval with seq = 2 =&gt; [0, 0]
EXPR6 &lt;- expression(x - x^2)
x &lt;- c(0, 1)
DAT6 &lt;- makeDat(EXPR6)
interval(DAT6, EXPR6, seq = 2)

# Refine with large sequence =&gt; [0, 0.25]
interval(DAT6, EXPR6, seq = 100)
# Tallies with curve function.
curve(x - x^2, 0, 1)
</code></pre>

<hr>
<h2 id='makeDat'>Create a dataframe from the variables defined in an expression</h2><span id='topic+makeDat'></span>

<h3>Description</h3>

<p>Creates a dataframe from the variables defined in an expression by <code><a href="base.html#topic+cbind">cbind</a></code>ing the corresponding data found in the workspace. This is a convenience function for creating a dataframe to be passed to <code><a href="#topic+propagate">propagate</a></code>, when starting with data which was simulated from distributions, i.e. when <code>type = "sim"</code>. Will throw an error if a variable is defined in the expression but is not available from the workspace.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>makeDat(expr)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="makeDat_+3A_expr">expr</code></td>
<td>
<p>an expression to be use for <code><a href="#topic+propagate">propagate</a></code>.</p>
</td></tr>  
</table>


<h3>Value</h3>

<p>A dataframe containing the data defined in <code>expr</code> in columns.
</p>


<h3>Author(s)</h3>

<p>Andrej-Nikolai Spiess
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Simulating from uniform
## and normal distribution,
## run 'propagate'.
EXPR1 &lt;- expression(a + b^c)
a &lt;- rnorm(100000, 12, 1)
b &lt;- rnorm(100000, 5, 0.1)
c &lt;- runif(100000, 6, 7)

DAT1 &lt;- makeDat(EXPR1)
propagate(EXPR1, DAT1, type = "sim", cov = FALSE)
</code></pre>

<hr>
<h2 id='makeDerivs'>Utility functions for creating Gradient- and Hessian-like matrices with symbolic derivatives and evaluating them in an environment</h2><span id='topic+makeGrad'></span><span id='topic+makeHess'></span><span id='topic+evalDerivs'></span>

<h3>Description</h3>

<p>These are three different utility functions that create matrices containing the symbolic partial derivatives of first (<code>makeGrad</code>) and second (<code>makeHess</code>) order and a function for evaluating these matrices in an environment. The evaluations of the symbolic derivatives are used within the <code><a href="#topic+propagate">propagate</a></code> function to calculate the propagated uncertainty, but these functions may also be useful for other applications. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>makeGrad(expr, order = NULL)
makeHess(expr, order = NULL)
evalDerivs(deriv, envir)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="makeDerivs_+3A_expr">expr</code></td>
<td>
<p>an expression, such as <code>expression(x/y)</code>.</p>
</td></tr>
<tr><td><code id="makeDerivs_+3A_order">order</code></td>
<td>
<p>order of creating partial derivatives, i.e. <code>c(2, 1)</code>. See 'Examples'.</p>
</td></tr> 
<tr><td><code id="makeDerivs_+3A_deriv">deriv</code></td>
<td>
<p>a matrix returned from <code>makeGrad</code> or <code>makeHess</code>.</p>
</td></tr>
<tr><td><code id="makeDerivs_+3A_envir">envir</code></td>
<td>
<p>an environment to evaluate in. By default the workspace.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given a function <code class="reqn">f(x_1, x_2, \ldots, x_n)</code>, the following matrices containing symbolic derivatives of <code class="reqn">f</code> are returned:<br />
</p>
<p><b>makeGrad:</b><br />
</p>
<p style="text-align: center;"><code class="reqn">\nabla(f) = \left[\frac{\partial f}{\partial x_1}, \ldots, \frac{\partial f}{\partial x_n}\right]</code>
</p>
 
<p><b>makeHess:</b><br />
</p>
<p style="text-align: center;"><code class="reqn">H(f) = \left[ \begin{array}{cccc} \frac{\partial^2 f}{\partial x_1^2} &amp; \frac{\partial^2 f}{\partial x_1\,\partial x_2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_1\,\partial x_n} \\ \frac{\partial^2 f}{\partial x_2\,\partial x_1} &amp; \frac{\partial^2 f}{\partial x_2^2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_2\,\partial x_n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \frac{\partial^2 f}{\partial x_n\,\partial x_1} &amp; \frac{\partial^2 f}{\partial x_n\,\partial x_2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_n^2} \end{array} \right]
</code>
</p>



<h3>Value</h3>

<p>The symbolic or evaluated Gradient/Hessian matrices.</p>


<h3>Author(s)</h3>

<p>Andrej-Nikolai Spiess
</p>


<h3>References</h3>

<p>The Matrix Cookbook (Version November 2012).<br />
Petersen KB &amp; Pedersen MS.<br />
<a href="http://ais.informatik.uni-freiburg.de/teaching/ws12/mapping/pdf/imm3274.pdf">http://ais.informatik.uni-freiburg.de/teaching/ws12/mapping/pdf/imm3274.pdf</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>EXPR &lt;- expression(a^b + sin(c))
ENVIR &lt;- list(a = 2, b = 3, c = 4)

## First-order partial derivatives: Gradient.
GRAD &lt;- makeGrad(EXPR) 

## This will evaluate the Gradient.
evalDerivs(GRAD, ENVIR)

## Second-order partial derivatives: Hessian.
HESS &lt;- makeHess(EXPR) 

## This will evaluate the Hessian.
evalDerivs(HESS, ENVIR)

## Change derivatives order.
GRAD &lt;- makeGrad(EXPR, order = c(2,1,3)) 
evalDerivs(GRAD, ENVIR)
</code></pre>

<hr>
<h2 id='matrixStats'>Fast column- and row-wise versions of variance coded in C++</h2><span id='topic+colVarsC'></span><span id='topic+rowVarsC'></span>

<h3>Description</h3>

<p>These two functions are fast C++ versions for column- and row-wise <code><a href="stats.html#topic+var">var</a></code>iance calculation on matrices/data.frames and are meant to substitute the classical <code>apply(mat, 1, var)</code> approach. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  colVarsC(x)
  rowVarsC(x)  
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="matrixStats_+3A_x">x</code></td>
<td>
<p>a matrix or data.frame</p>
</td></tr> 
</table>


<h3>Details</h3>

<p>They are coded in a way that they automatically remove <code>NA</code> values, so they behave like <code>na.rm = TRUE</code>.
</p>


<h3>Value</h3>

<p>A vector with the variance values.
</p>


<h3>Author(s)</h3>

<p>Andrej-Nikolai Spiess
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Speed comparison on large matrix.
## ~ 110x speed increase!
## Not run: 
MAT &lt;- matrix(rnorm(10 * 500000), ncol = 10)
system.time(RES1 &lt;- apply(MAT, 1, var))
system.time(RES2 &lt;- rowVarsC(MAT))
all.equal(RES1, RES2)

## End(Not run)
</code></pre>

<hr>
<h2 id='mixCov'>Aggregating covariances matrices and/or error vectors into a single covariance matrix</h2><span id='topic+mixCov'></span>

<h3>Description</h3>

<p>This function aggregates covariances matrices, single variance values or a vector of multiple variance values into one final covariance matrix suitable for <code><a href="#topic+propagate">propagate</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mixCov(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mixCov_+3A_...">...</code></td>
<td>
<p>either covariance matrices, or a vector of single/multiple variance values.</p>
</td></tr>  
</table>


<h3>Details</h3>

<p>'Mixes' (aggregates) data of the following types into a final covariance matrix:<br />
1) covariance matrices <code class="reqn">\mathbf{\Sigma}</code> that are already available.<br />
2) single variance values <code class="reqn">\sigma^2</code>.<br />
3) a vector of variance values <code class="reqn">\sigma^2_1, \sigma^2_2, ..., \sigma^2_n</code>.<br />
</p>
<p>This is accomplished by filling a <code class="reqn">m_1 + m_2 + \ldots + m_n</code> sized square matrix <code class="reqn">\mathbf{C}</code> succesively with elements <code class="reqn">1 \ldots m_1, m_1 + 1 \ldots m1 + m2, \ldots, m_n + 1 \ldots m_n + m_{n + 1}</code> with either covariance matrices at <code class="reqn">C_{m_n + 1 \ldots m_n + m_{n + 1}, m_n + 1 \ldots m_n + m_{n + 1}}</code> or single variance values on the diagonals at <code class="reqn">C_{m_n, m_n}</code>.
</p>


<h3>Value</h3>

<p>The aggregated covariance matrix.</p>


<h3>Author(s)</h3>

<p>Andrej-Nikolai Spiess
</p>


<h3>References</h3>

<p>Evaluation of measurement data - Guide to the expression of uncertainty in measurement.<br />
JCGM 100:2008 (GUM 1995 with minor corrections).<br />
<a href="http://www.bipm.org/utils/common/documents/jcgm/JCGM_100_2008_E.pdf">http://www.bipm.org/utils/common/documents/jcgm/JCGM_100_2008_E.pdf</a>.
</p>
<p>Evaluation of measurement data - Supplement 1 to the Guide to the expression of uncertainty in measurement - Propagation of distributions using a Monte Carlo Method.<br />
JCGM 101:2008.<br />
<a href="http://www.bipm.org/utils/common/documents/jcgm/JCGM_101_2008_E.pdf">http://www.bipm.org/utils/common/documents/jcgm/JCGM_101_2008_E.pdf</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#######################################################
## Example in Annex H.4.1 from the GUM 2008 manual
## (see 'References'), measurement of activity.
## This will give exactly the same values as Table H.8.
data(H.4)
attach(H.4)
T0 &lt;- 60
lambda &lt;- 1.25894E-4
Rx &lt;- ((Cx - Cb)/60) * exp(lambda * tx)
Rs &lt;- ((Cs - Cb)/60) * exp(lambda * ts)

mRx &lt;- mean(Rx)
sRx &lt;- sd(Rx)/sqrt(6)
mRx
sRx

mRs &lt;- mean(Rs)
sRs &lt;- sd(Rs)/sqrt(6)
mRs
sRs

R &lt;- Rx/Rs
mR &lt;- mean(R)
sR &lt;- sd(R)/sqrt(6)
mR
sR

cor(Rx, Rs)

## Definition as in H.4.3.
As &lt;- c(0.1368, 0.0018)
ms &lt;- c(5.0192, 0.005)
mx &lt;- c(5.0571, 0.001)

## We have to scale Rs/Rx by sqrt(6) to get the 
## corresponding covariances.
Rs &lt;- Rs/sqrt(6)
Rx &lt;- Rx/sqrt(6)

## Here we create an aggregated covariance matrix
## from the raw and summary data.
COV1 &lt;- cov(cbind(Rs, Rx))
COV &lt;- mixCov(COV1, As[2]^2, ms[2]^2, mx[2]^2)
COV

## Prepare the data for 'propagate'.
MEANS &lt;- c(mRs, mRx, As[1], ms[1], mx[1])
SDS &lt;- c(sRs, sRx, As[2], ms[2], mx[2])
DAT &lt;- rbind(MEANS, SDS)
colnames(DAT) &lt;- c("Rs", "Rx", "As", "ms", "mx")

## This will give exactly the same values as 
## in H.4.3/H.4.3.1.
EXPR &lt;- expression(As * (ms/mx) * (Rx/Rs))
RES &lt;- propagate(EXPR, data = DAT, cov = COV, nsim = 100000)
RES
</code></pre>

<hr>
<h2 id='moments'>Skewness and (excess) Kurtosis of a vector of values</h2><span id='topic+skewness'></span><span id='topic+kurtosis'></span>

<h3>Description</h3>

<p>These functions calculate skewness and excess kurtosis of a vector of values. They were taken from the package 'moments'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>skewness(x, na.rm = FALSE) 
kurtosis(x, na.rm = FALSE) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="moments_+3A_x">x</code></td>
<td>
<p>a numeric vector, matrix or data frame.</p>
</td></tr>
<tr><td><code id="moments_+3A_na.rm">na.rm</code></td>
<td>
<p>logical. Should missing values be removed?</p>
</td></tr>  
</table>


<h3>Details</h3>

<p>Skewness:
</p>
<p style="text-align: center;"><code class="reqn">\frac{\frac{1}{n} \sum_{i=1}^n (x_i-\overline{x})^3}{\left(\frac{1}{n} \sum_{i=1}^n (x_i-\overline{x})^2\right)^{3/2}}</code>
</p>

<p>(excess) Kurtosis:
</p>
<p style="text-align: center;"><code class="reqn">\frac{\frac{1}{n} \sum_{i=1}^n (x_i - \overline{x})^4}{\left(\frac{1}{n} \sum_{i=1}^n (x_i - \overline{x})^2\right)^2} - 3</code>
</p>



<h3>Value</h3>

<p>The skewness/kurtosis values.</p>


<h3>Author(s)</h3>

<p>Andrej-Nikolai Spiess
</p>


<h3>Examples</h3>

<pre><code class='language-R'>X &lt;- rnorm(100, 20, 2)
skewness(X)
kurtosis(X)
</code></pre>

<hr>
<h2 id='numDerivs'>Functions for creating Gradient and Hessian matrices by numerical differentiation (Richardson's method) of the partial derivatives</h2><span id='topic+numGrad'></span><span id='topic+numHess'></span>

<h3>Description</h3>

<p>These two functions create Gradient and Hessian matrices by Richardson's central finite difference method of the partial derivatives for any expression. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>numGrad(expr, envir = .GlobalEnv)
numHess(expr, envir = .GlobalEnv)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="numDerivs_+3A_expr">expr</code></td>
<td>
<p>an expression, such as <code>expression(x/y)</code>.</p>
</td></tr>
<tr><td><code id="numDerivs_+3A_envir">envir</code></td>
<td>
<p>the <code><a href="base.html#topic+environment">environment</a></code> to evaluate in.</p>
</td></tr>  
</table>


<h3>Details</h3>

<p>Calculates first- and second-order numerical approximation using Richardson's <b>central difference formula</b>:<br />
</p>
<p style="text-align: center;"><code class="reqn">f'_i(x) \approx \frac{f(x_1, \ldots, x_i + d, \ldots, x_n) - f(x_1, \ldots, x_i - d, \ldots, x_n)}{2d}</code>
</p>

<p style="text-align: center;"><code class="reqn">f''_i(x) \approx \frac{f(x_1, \ldots, x_i + d, \ldots, x_n) - 2f(x_1, \ldots, x_n) + f(x_1, \ldots, x_i - d, \ldots, x_n)}{d^2}</code>
</p>



<h3>Value</h3>

<p>The numeric Gradient/Hessian matrices.</p>


<h3>Note</h3>

<p>The two functions are modified versions of the <code>genD</code> function in the 'numDeriv' package, but a bit more easy to handle because they use expressions and the function's <code>x</code> value must not be defined as splitted scalar values <code>x[1], x[2], ... x[n]</code> in the body of the function.</p>


<h3>Author(s)</h3>

<p>Andrej-Nikolai Spiess
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Check for equality of symbolic  
## and numerical derivatives.
EXPR &lt;- expression(2^x + sin(2 * y) - cos(z))
x &lt;- 5
y &lt;- 10
z &lt;- 20

symGRAD &lt;- evalDerivs(makeGrad(EXPR))
numGRAD &lt;- numGrad(EXPR)
all.equal(symGRAD, numGRAD)

symHESS &lt;- evalDerivs(makeHess(EXPR))
numHESS &lt;- numHess(EXPR)
all.equal(symHESS, numHESS)
</code></pre>

<hr>
<h2 id='plot.propagate'>Plotting function for 'propagate' objects</h2><span id='topic+plot.propagate'></span>

<h3>Description</h3>

<p>Creates a histogram of the evaluated results from the multivariate simulated data, along with a density curve, <code>alpha</code>-based confidence intervals, median and mean.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'propagate'
plot(x, logx = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.propagate_+3A_x">x</code></td>
<td>
<p>an object returned from <code><a href="#topic+propagate">propagate</a></code>.</p>
</td></tr> 
<tr><td><code id="plot.propagate_+3A_logx">logx</code></td>
<td>
<p>logical. Should the data be displayed on a logarithmic abscissa?</p>
</td></tr>
<tr><td><code id="plot.propagate_+3A_...">...</code></td>
<td>
<p>other parameters to <code><a href="graphics.html#topic+hist">hist</a></code>.</p>
</td></tr>  
</table>


<h3>Value</h3>

<p>A plot as described above.
</p>


<h3>Author(s)</h3>

<p>Andrej-Nikolai Spiess
</p>


<h3>Examples</h3>

<pre><code class='language-R'>EXPR1 &lt;- expression(x^2 * sin(y))
x &lt;- c(5, 0.01)
y &lt;- c(1, 0.01)
DF1 &lt;- cbind(x, y)
RES1 &lt;- propagate(expr = EXPR1, data = DF1, type = "stat", 
                  nsim = 100000, alpha = 0.01)
plot(RES1)
</code></pre>

<hr>
<h2 id='predictNLS'>Confidence/prediction intervals for (weighted) nonlinear models based on uncertainty propagation</h2><span id='topic+predictNLS'></span>

<h3>Description</h3>

<p>A function for calculating confidence/prediction intervals of (weighted) nonlinear models for the supplied or new predictor values, by using first-/second-order Taylor expansion and Monte Carlo simulation. This approach can be used to construct more realistic error estimates and confidence/prediction intervals for nonlinear models than what is possible with only a simple linearization (first-order Taylor expansion) approach. Another application is when there is an &quot;error in x&quot; setup with uncertainties in the predictor variable (See 'Examples'). This function will also work in the presence of multiple predictors with/without errors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predictNLS(model, newdata, newerror, interval = c("confidence", "prediction", "none"),
           alpha = 0.05, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predictNLS_+3A_model">model</code></td>
<td>
<p>a model obtained from <code><a href="stats.html#topic+nls">nls</a></code> or <code>nlsLM</code> (package 'minpack.lm').</p>
</td></tr>
<tr><td><code id="predictNLS_+3A_newdata">newdata</code></td>
<td>
<p>a data frame with new predictor values, having the same column names as in <code>model</code>. See <code><a href="stats.html#topic+predict.nls">predict.nls</a></code> and 'Examples'. If omitted, the model's predictor values are employed.</p>
</td></tr>
<tr><td><code id="predictNLS_+3A_newerror">newerror</code></td>
<td>
<p>a data frame with optional error values, having the same column names as in <code>model</code> and in the same order as in <code>newdata</code>. See 'Examples'.</p>
</td></tr>
<tr><td><code id="predictNLS_+3A_interval">interval</code></td>
<td>
<p>A character string indicating if confidence/prediction intervals are to be calculated or not.</p>
</td></tr>
<tr><td><code id="predictNLS_+3A_alpha">alpha</code></td>
<td>
<p>the <code class="reqn">\alpha</code> level.</p>
</td></tr>
<tr><td><code id="predictNLS_+3A_...">...</code></td>
<td>
<p>other parameters to be supplied to <code><a href="#topic+propagate">propagate</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculation of the propagated uncertainty <code class="reqn">\sigma_y</code> using <code class="reqn">\nabla \Sigma \nabla^T</code> is called the &quot;Delta Method&quot; and is widely applied in NLS fitting. However, this method is based on first-order Taylor expansion and thus assummes linearity around <code class="reqn">f(x)</code>. The second-order approach as implemented in the <code><a href="#topic+propagate">propagate</a></code> function can partially correct for this restriction by using a second-order polynomial around <code class="reqn">f(x)</code>.<br />
Confidence and prediction intervals are calculated in a usual way using <code class="reqn">t(1 - \frac{\alpha}{2}, \nu) \cdot \sigma_y</code> (1) or <code class="reqn">t(1 - \frac{\alpha}{2}, \nu) \cdot \sqrt{\sigma_y^2 + \textcolor{red}{\sigma_r^2}}</code> (2), respectively, where the residual variance <code class="reqn">\textcolor{red}{\sigma_r^2} = \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{n - \nu}</code> (3).
The inclusion of <code class="reqn">\textcolor{red}{\sigma_r^2}</code> in the prediction interval is implemented as an extended gradient and &quot;augmented&quot; covariance matrix. So instead of using <code class="reqn">y = f(x, \beta)</code> (4) we take <code class="reqn">y = f(x, \beta) + \textcolor{red}{\sigma_r^2}</code> (5) as the expression and augment the <code class="reqn">n \times n</code> covariance matrix <code class="reqn">C</code> to an <code class="reqn">n+1 \times n+1</code> covariance matrix, where <code class="reqn">C_{n+1, n+1} = \textcolor{red}{\sigma_r^2}</code>. Partial differentiation and matrix multiplication will then yield, for example with two coefficients <code class="reqn">\beta_1</code> and <code class="reqn">\beta_2</code> and their corresponding covariance matrix <code class="reqn">\Sigma</code>:     
</p>
<p style="text-align: center;"><code class="reqn">\left[\frac{\partial f}{\partial \beta_1}\; \frac{\partial f}{\partial \beta_2}\; \textcolor{red}{1}\right] \left[ \begin{array}{ccc} \sigma_1^2 &amp; \sigma_1\sigma_2 &amp; 0 \\ \sigma_2\sigma_1 &amp; \sigma_2^2 &amp; 0 \\ 0 &amp; 0 &amp; \textcolor{red}{\sigma_r^2} \end{array} \right] \left[ \begin{array}{c} \frac{\partial f}{\partial \beta_1} \\ \frac{\partial f}{\partial \beta_2} \\ \textcolor{red}{1} \end{array} \right] = \left(\frac{\partial f}{\partial \beta_1}\right)^2\sigma_1^2 + 2 \frac{\partial f}{\partial \beta_1} \frac{\partial f}{\partial \beta_2} \sigma_1 \sigma_2 + \left(\frac{\partial f}{\partial \beta_2}\right)^2\sigma_2^2 + \textcolor{red}{\sigma_r^2}</code>
</p>

<p><code class="reqn">\equiv \sigma_y^2 + \textcolor{red}{\sigma_r^2}</code>, where <code class="reqn">\sigma_y^2 + \textcolor{red}{\sigma_r^2}</code> then goes into (2).<br />
The advantage of the augmented covariance matrix is that it can be exploited for creating Monte Carlo simulation-based prediction intervals. This is new from version 1.0-6 and is based on the paradigm that we simply add another dimension with <code class="reqn">\mu = 0</code> and <code class="reqn">\sigma^2 = \textcolor{red}{\sigma_r^2}</code> to the multivariate t-distribution random number generator (in our case <code><a href="tmvtnorm.html#topic+rtmvt">rtmvt</a></code>). All <code class="reqn">n</code> simulations are then evaluated with (5) and the usual <code class="reqn">[1 - \frac{\alpha}{2}, \frac{\alpha}{2}]</code> quantiles calculated.<br /> 
If errors are supplied to the predictor values in <code>newerror</code>, they need to have the same column names and order than the new predictor values. 
</p>


<h3>Value</h3>

<p>A list with the following items:<br />
<code>summary</code>: The mean/error estimates obtained from first-/second-order Taylor expansion and Monte Carlo simulation, together with calculated confidence/prediction intervals based on asymptotic normality.<br />
<code>prop</code>: the complete output from <code><a href="#topic+propagate">propagate</a></code> for each value in <code>newdata</code>.
</p>


<h3>Author(s)</h3>

<p>Andrej-Nikolai Spiess
</p>


<h3>References</h3>

<p>Nonlinear Regression.<br />
Seber GAF &amp; Wild CJ.<br />
John Wiley &amp; Sons; 1ed, 2003.
</p>
<p>Nonlinear Regression Analysis and its Applications.<br />
Bates DM &amp; Watts DG.<br />
Wiley-Interscience; 1ed, 2007.
</p>
<p>Statistical Error Propagation.<br />
Tellinghuisen J.<br />
<em>J. Phys. Chem. A</em> (2001), <b>47</b>: 3917-3921.
</p>
<p>Least-squares analysis of data with uncertainty in x and y: A Monte Carlo
methods comparison.<br />
Tellinghuisen J.<br />
<em>Chemometr Intell Lab</em> (2010), <b>47</b>: 160-169.
</p>
<p>From the author's blog:<br />
http://rmazing.wordpress.com/2013/08/14/predictnls-part-1-monte-carlo-simulation-confidence-intervals-for-nls-models/<br />
http://rmazing.wordpress.com/2013/08/26/predictnls-part-2-taylor-approximation-confidence-intervals-for-nls-models/
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## In these examples, 'nsim = 100000' to save
## Rcmd check time (CRAN). It is advocated
## to use at least 'nsim = 1000000' though...

## Example from ?nls.
DNase1 &lt;- subset(DNase, Run == 1)
fm3DNase1 &lt;- nls(density ~ Asym/(1 + exp((xmid - log(conc))/scal)),
                 data = DNase1, start = list(Asym = 3, xmid = 0, scal = 1))

## Using a single predictor value without error.
PROP1 &lt;- predictNLS(fm3DNase1, newdata = data.frame(conc = 2), nsim = 100000)
PRED1 &lt;- predict(fm3DNase1, newdata = data.frame(conc = 2), nsim = 100000)
PROP1$summary
PRED1
## =&gt; Prop.Mean.1 equal to PRED1

## Using a single predictor value with error.
PROP2 &lt;- predictNLS(fm3DNase1, newdata = data.frame(conc = 2), 
                    newerror = data.frame(conc = 0.5), nsim = 100000)
PROP2$summary

## Not run: 
## Using a sequence of predictor values without error.
CONC &lt;- seq(1, 12, by = 1)
PROP3 &lt;- predictNLS(fm3DNase1, newdata = data.frame(conc = CONC))
PRED3 &lt;- predict(fm3DNase1, newdata = data.frame(conc = CONC))
PROP3$summary
PRED3
## =&gt; Prop.Mean.1 equal to PRED3

## Plot mean and confidence values from first-/second-order 
## Taylor expansion and Monte Carlo simulation.
plot(DNase1$conc, DNase1$density)
lines(DNase1$conc, fitted(fm3DNase1), lwd = 2, col = 1)
points(CONC, PROP3$summary[, 1], col = 2, pch = 16)
lines(CONC, PROP3$summary[, 5], col = 2)
lines(CONC, PROP3$summary[, 6], col = 2)
lines(CONC, PROP3$summary[, 11], col = 4)
lines(CONC, PROP3$summary[, 12], col = 4)

## Using a sequence of predictor values with error.
PROP4 &lt;- predictNLS(fm3DNase1, newdata = data.frame(conc = 1:5), 
                    newerror = data.frame(conc = (1:5)/10))
PROP4$summary

## Using multiple predictor values.
## 1: Setup of response values with gaussian error of 10%.
x &lt;- seq(1, 10, by = 0.01)
y &lt;- seq(10, 1, by = -0.01)
a &lt;- 2
b &lt;- 5
c &lt;- 10
z &lt;- a * exp(b * x)^sin(y/c)
z &lt;- z + sapply(z, function(x) rnorm(1, x, 0.10 * x))

## 2: Fit 'nls' model.
MOD &lt;- nls(z ~ a * exp(b * x)^sin(y/c), 
           start = list(a = 2, b = 5, c = 10))

## 3: Single newdata without errors.
DAT1 &lt;- data.frame(x = 4, y = 3)
PROP5 &lt;- predictNLS(MOD, newdata = DAT1)
PROP5$summary

## 4: Single newdata with errors.
DAT2 &lt;- data.frame(x = 4, y = 3)
ERR2 &lt;- data.frame(x = 0.2, y = 0.1)
PROP6 &lt;- predictNLS(MOD, newdata = DAT2, newerror = ERR2)
PROP6$summary

## 5: Multiple newdata with errors.
DAT3 &lt;- data.frame(x = 1:4, y = 3)
ERR3 &lt;- data.frame(x = rep(0.2, 4), y = seq(1:4)/10)
PROP7 &lt;- predictNLS(MOD, newdata = DAT3, newerror = ERR3)
PROP7$summary

## 6: Linear model to compare conf/pred intervals.
set.seed(123)
X &lt;- 1:20
Y &lt;- 3 + 2 * X + rnorm(20, 0, 2)
plot(X, Y)
LM &lt;- lm(Y ~ X)
NLS &lt;- nlsLM(Y ~ a + b * X, start = list(a = 3, b = 2)) 
predict(LM, newdata = data.frame(X = 14.5), interval = "conf") 
predictNLS(NLS, newdata = data.frame(X = 14.5), interval = "conf")$summary
predict(LM, newdata = data.frame(X = 14.5), interval = "pred")
predictNLS(NLS, newdata = data.frame(X = 14.5), interval = "pred")$summary

## 7: compare to 'predFit' function of 'investr' package.
## Same results when using only first-order Taylor expansion.
require(investr)
data(Puromycin, package = "datasets")
Puromycin2 &lt;- Puromycin[Puromycin$state == "treated", ][, 1:2]
Puro.nls &lt;- nls(rate ~ Vm * conc/(K + conc), data = Puromycin2,
                start = c(Vm = 200, K = 0.05))
PRED1 &lt;- predFit(Puro.nls, interval = "prediction")
PRED2 &lt;- predictNLS(Puro.nls, interval = "prediction", second.order = FALSE, do.sim = FALSE)
all.equal(PRED1[, "lwr"], PRED2$summary[, 5]) # =&gt; TRUE

## End(Not run) 
</code></pre>

<hr>
<h2 id='propagate'>Propagation of uncertainty using higher-order Taylor expansion and Monte Carlo simulation</h2><span id='topic+propagate'></span>

<h3>Description</h3>

<p>A general function for the calculation of uncertainty propagation by first-/second-order Taylor expansion and Monte Carlo simulation including covariances. Input data can be any symbolic/numeric differentiable expression and data based on summaries (mean &amp; s.d.) or sampled from distributions. Uncertainty propagation is based completely on matrix calculus accounting for full covariance structure. Monte Carlo simulation is conducted using a multivariate t-distribution with covariance structure. Propagation confidence intervals are calculated from the expanded uncertainties by means of the degrees of freedom obtained from <code><a href="#topic+WelchSatter">WelchSatter</a></code>, or from the [<code class="reqn">\frac{\alpha}{2}, 1-\frac{\alpha}{2}</code>] quantiles of the MC evaluations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>propagate(expr, data, second.order = TRUE, do.sim = TRUE, cov = TRUE, 
          df = NULL, nsim = 1000000, alpha = 0.05, ...)  
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="propagate_+3A_expr">expr</code></td>
<td>
<p>an expression, such as <code>expression(x/y)</code>.</p>
</td></tr>
<tr><td><code id="propagate_+3A_data">data</code></td>
<td>
<p>a dataframe or matrix containing either a) the means <code class="reqn">\mu_i</code>, standard deviations <code class="reqn">\sigma_i</code> and degrees of freedom <code class="reqn">\nu_i</code> (optionally) in the first, second and third (optionally) row, or b) sampled data generated from any of <span class="rlang"><b>R</b></span>'s <code><a href="stats.html#topic+distributions">distributions</a></code> or those implemented in this package (<code><a href="#topic+rDistr">rDistr</a></code>). If <code>nrow(data)</code> &gt; 3, sampled data is assumed. The column names must match the variable names.</p>
</td></tr>
<tr><td><code id="propagate_+3A_second.order">second.order</code></td>
<td>
<p>logical. If <code>TRUE</code>, error propagation will be calculated with first- and second-order Taylor expansion. See 'Details'.</p>
</td></tr>  
<tr><td><code id="propagate_+3A_do.sim">do.sim</code></td>
<td>
<p>logical. Should Monte Carlo simulation be applied?</p>
</td></tr>
<tr><td><code id="propagate_+3A_cov">cov</code></td>
<td>
<p>logical or variance-covariance matrix with the same column names as <code>data</code>. See 'Details'.</p>
</td></tr>
<tr><td><code id="propagate_+3A_df">df</code></td>
<td>
<p>an optional scalar with the total degrees of freedom <code class="reqn">\nu_{\mathrm{tot}}</code> of the system.</p>
</td></tr>
<tr><td><code id="propagate_+3A_nsim">nsim</code></td>
<td>
<p>the number of Monte Carlo simulations to be performed, minimum is 10000.</p>
</td></tr>  
<tr><td><code id="propagate_+3A_alpha">alpha</code></td>
<td>
<p>the 1 - confidence level.</p>
</td></tr>
<tr><td><code id="propagate_+3A_...">...</code></td>
<td>
<p>other parameters to be supplied to future methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The implemented methods are:<br /><br />
1) <b>Monte Carlo simulation:</b><br />
For each variable <code class="reqn">m</code> in <code>data</code>, simulated data <code class="reqn">X = [x_1, x_2, \ldots, x_n]</code> with <code class="reqn">n</code> = <code>nsim</code> samples is generated from a multivariate t-distribution <code class="reqn">X_{m, n} \sim t(\mu, \Sigma, \nu)</code> using means <code class="reqn">\mu_i</code> and covariance matrix <code class="reqn">\boldsymbol{\Sigma}</code> constructed from the standard deviations <code class="reqn">\sigma_i</code> of each variable. All data is coerced into a new dataframe that has the same covariance structure as the initial <code>data</code>: <code class="reqn">\boldsymbol{\Sigma}(\mathtt{data}) = \boldsymbol{\Sigma}(X_{m, n})</code>. Each row <code class="reqn">i = 1, \ldots, n</code> of the simulated dataset <code class="reqn">X_{m, n}</code> is evaluated with <code>expr</code>, <code class="reqn">y_i = f(x_{m, i})</code>, and summary statistics (mean, sd, median, mad, quantile-based confidence interval based on [<code class="reqn">\frac{\alpha}{2}, 1-\frac{\alpha}{2}</code>]) are calculated on <code class="reqn">y</code>. 
</p>
<p>2) <b>Error propagation:</b><br />
The propagated error is calculated by first-/second-order Taylor expansion accounting for full covariance structure using matrix algebra.<br />
The following transformations based on two variables <code class="reqn">x_1, x_2</code> illustrate the equivalence of the matrix-based approach with well-known classical notations:<br />
<b>First-order mean:</b> <code class="reqn">\rm{E[y]} = f(\bar{x}_i)</code><br />
<b>First-order variance:</b> <code class="reqn">\sigma_y^2 = {\color{red} \nabla \mathbf{\Sigma} \nabla^T}</code>:<br />
</p>
<p style="text-align: center;"><code class="reqn">{ \color{red}[\rm{j_1}\; \rm{j_2}] \left[ \begin{array}{cc} \sigma_1^2 &amp; \sigma_1\sigma_2 \\ \sigma_2\sigma_1 &amp; \sigma_2^2 \end{array} \right] \left[ \begin{array}{c} \rm{j_1} \\ \rm{j_2} \end{array} \right]} = \rm{j_1}^2 \sigma_1^2 + \rm{2 j_1 j_2} \sigma_1 \sigma_2 + \rm{j_2}^2 \sigma_2^2</code>
</p>

<p style="text-align: center;"><code class="reqn">= \underbrace{\sum_{i=1}^2 \rm{j_i}^2 \sigma_i^2 + 2\sum_{i=1\atop i \neq k}^2\sum_{k=1\atop k \neq i}^2 \rm{j_i j_k} \sigma_{ik}}_{\rm{classical\;notation}} = \frac{1}{1!} \left(\sum_{i=1}^2 \frac{\partial f}{\partial x_i} \sigma_i \right)^2</code>
</p>

<p><br />
<b>Second-order mean:</b> <code class="reqn">\rm{E}[y] = f(\bar{x}_i) + {\color{blue} \frac{1}{2}\rm{tr}(\mathbf{H\Sigma)}}</code>:<br />
</p>
<p style="text-align: center;"><code class="reqn">{ \color{blue} \frac{1}{2} \rm{tr} \left[ \begin{array}{cc} \rm{h_1} &amp; \rm{h_2} \\ \rm{h_3} &amp; \rm{h_4} \end{array} \right] \left[ \begin{array}{cc} \sigma_1^2 &amp; \sigma_1\sigma_2 \\ \sigma_2\sigma_1 &amp; \sigma_2^2 \end{array} \right]} = \frac{1}{2} \rm{tr} \left[ \begin{array}{cc} \rm{h_1} \sigma_1^2 + \rm{h_2}\sigma_1\sigma_2 &amp; \rm{h_1}\sigma_1\sigma_2 + \rm{h_2}\sigma_2^2 \\ \rm{h_3} \sigma_1^2 + \rm{h_4} \sigma_1\sigma_2 &amp; \rm{h_3} \sigma_1\sigma_2 + \rm{h_4} \sigma_2^2 \end{array} \right]</code>
</p>

<p style="text-align: center;"><code class="reqn"> = \frac{1}{2}(\rm{h_1}\sigma_1^2 + \rm{h_2}\sigma_1\sigma_2 + \rm{h_3}\sigma_1\sigma_2 + \rm{h_4}\sigma_2^2) = \frac{1}{2!} \left(\sum_{i=1}^2 \frac{\partial}{\partial x_i} \sigma_i \right)^2 \it f</code>
</p>
 
<p><br />
<b>Second-order variance:</b> <code class="reqn">\sigma_y^2 = {\color{red} \nabla\mathbf{\Sigma}\nabla^T} + {\color{blue} \frac{1}{2}\rm{tr}(\mathbf{H\Sigma H\Sigma)}}</code>:<br />
</p>
<p style="text-align: center;"><code class="reqn">{\color{blue}\frac{1}{2} \rm{tr} \left[ \begin{array}{cc} \rm{h_1} &amp; \rm{h_2} \\ \rm{h_3} &amp; \rm{h_4} \end{array} \right] \left[ \begin{array}{cc} \rm{\sigma_1^2} &amp; \rm{\sigma_1\sigma_2} \\ \rm{\sigma_2\sigma_1} &amp; \rm{\sigma_2^2} \end{array} \right] \left[ \begin{array}{cc} \rm{h_1} &amp; \rm{h_2} \\ \rm{h_3} &amp; \rm{h_4} \end{array} \right] \left[ \begin{array}{cc} \rm{\sigma_1^2} &amp; \rm{\sigma_1\sigma_2} \\ \rm{\sigma_2\sigma_1} &amp; \rm{\sigma_2^2} \end{array} \right]} = \ldots</code>
</p>

<p style="text-align: center;"><code class="reqn">= \frac{1}{2} (\rm{h_1}^2\sigma_1^4 + \rm{2h_1h_2}\sigma_1^3\sigma_2 + \rm{2h_1h_3}\sigma_1^3\sigma_2 + \rm{h_2}^2\sigma_1^2\sigma_2^2 + \rm{2h_2h_3}\sigma_1^2\sigma_2^2 + \rm{h_3}^2\sigma_1^2\sigma_2^2 + \rm{2h_1h_4}\sigma_1^2\sigma_2^2</code>
</p>

<p style="text-align: center;"><code class="reqn">+ \rm{2h_2h_4}\sigma_1\sigma_2^3 + \rm{2h_3h_4}\sigma_1\sigma_2^3 + \rm{h_4}^2\sigma_2^4 = \frac{1}{2} (\rm{h_1}\sigma_1^2 + \rm{h_2}\sigma_1\sigma_2 + \rm{h_3}\sigma_1\sigma_2 + \rm{h_4}\sigma_2^2)^2</code>
</p>

<p style="text-align: center;"><code class="reqn">= \frac{1}{2!} \left( \left(\sum_{i=1}^2 \frac{\partial}{\partial x_i} \sigma_i \right)^2 \it f \right)^2</code>
</p>

<p><br />
with <code class="reqn">\mathrm{E}(y)</code> = expectation of <code class="reqn">y</code>, <code class="reqn">\mathbf{\sigma_y^2}</code> = variance of <code class="reqn">y</code>, <code class="reqn">{\color{red} \nabla}</code> = the p x n gradient matrix with all partial first derivatives <code class="reqn">{\color{red} \rm{j_i}}</code>, <code class="reqn">\mathbf{\Sigma}</code> = the p x p covariance matrix, <code class="reqn">{\color{blue}\mathbf{H}}</code> the Hessian matrix with all partial second derivatives <code class="reqn">{\color{blue} \rm{h_i}}</code>, <code class="reqn">\sigma_i</code> = the uncertainties and <code class="reqn">\rm{tr}(\cdot)</code> = the trace (sum of diagonal) of a matrix. Note that because the Hessian matrices are symmetric, <code class="reqn">{\color{blue} \rm{h_2}} = {\color{blue} \rm{h_3}}</code>. For a detailed derivation, see 'References'.<br />
The second-order Taylor expansion corrects for bias in nonlinear expressions as the first-order Taylor expansion assumes linearity around <code class="reqn">\bar{x}_i</code>. There is also a Python library available for second-order error propagation ('soerp', <a href="https://pypi.python.org/pypi/soerp">https://pypi.python.org/pypi/soerp</a>). The 'propagate' package gives <b>exactly</b> the same results, see last example under &quot;Examples&quot;.<br />
Depending on the input expression, the uncertainty propagation may result in an error that is not normally distributed. The Monte Carlo simulation, starting with a symmetric t-distributions of the variables, can clarify this. For instance, a high tendency from deviation of normality is encountered in formulas in which the error of the denominator is relatively large or in exponential models with a large error in the exponent.<br /> 
</p>
<p>For setups in which there is no symbolic derivation possible (i.e. <code>e &lt;- expression(abs(x))</code> =&gt; &quot;Function 'abs' is not in the derivatives table&quot;) the function automatically switches from symbolic (using <code><a href="#topic+makeGrad">makeGrad</a></code> or <code><a href="#topic+makeHess">makeHess</a></code>) to numeric (<code><a href="#topic+numGrad">numGrad</a></code> or <code><a href="#topic+numHess">numHess</a></code>) differentiation.<br />
</p>
<p>The function will try to evaluate the expression in an environment using <code><a href="base.html#topic+eval">eval</a></code> which results in a significant speed enhancement (~ 10-fold). If that fails, evaluation is done over the rows of the simulated data using <code><a href="base.html#topic+apply">apply</a></code>.
</p>
<p><code>cov</code> is used in the following ways:<br />
1) If <code class="reqn">\mu_i, \sigma_i</code> are supplied, a covariance matrix is built with diagonals <code class="reqn">\sigma_i^2</code>, independent of <code>cov = TRUE, FALSE</code>.<br />
2) When simulated data is supplied, a covariance matrix is constructed that either has (<code>cov = TRUE</code>) or has not (<code>cov = FALSE</code>) off-diagonal covariances.<br />
3) The user can supply an own covariance matrix <code class="reqn">\Sigma</code>, with the same column/row names as in <code>data</code>.
</p>
<p>The expanded uncertainty used for constructing the confidence interval is calculated from the Welch-Satterthwaite degrees of freedom <code class="reqn">\nu_{\mathrm{WS}}</code> of the <code><a href="#topic+WelchSatter">WelchSatter</a></code> function.
</p>


<h3>Value</h3>

<p>A list with the following components:   
</p>
<table>
<tr><td><code>gradient</code></td>
<td>
<p>the symbolic gradient vector <code class="reqn">\nabla</code> of partial first-order derivatives.</p>
</td></tr>
<tr><td><code>evalGrad</code></td>
<td>
<p>the evaluated gradient vector <code class="reqn">\nabla</code> of partial first-order derivatives, also known as the &quot;sensitivity&quot;. See <code><a href="#topic+summary.propagate">summary.propagate</a></code>.</p>
</td></tr>
<tr><td><code>hessian</code></td>
<td>
<p>the symbolic Hessian matrix <code class="reqn">\mathbf{H}</code> of partial second-order derivatives.</p>
</td></tr>
<tr><td><code>evalHess</code></td>
<td>
<p>the evaluated Hessian matrix <code class="reqn">\mathbf{H}</code> of partial second-order derivatives.</p>
</td></tr>
<tr><td><code>rel.contr</code></td>
<td>
<p>the relative contribution matrix, see <code><a href="#topic+summary.propagate">summary.propagate</a></code>.</p>
</td></tr>
<tr><td><code>covMat</code></td>
<td>
<p>the covariance matrix <code class="reqn">\mathbf{\Sigma}</code> used for Monte Carlo simulation and uncertainty propagation.</p>
</td></tr> 
<tr><td><code>ws.df</code></td>
<td>
<p>the Welch-Satterthwaite degrees of freedom <code class="reqn">\nu_{\mathrm{ws}}</code>, as obtained from <code><a href="#topic+WelchSatter">WelchSatter</a></code>.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>the coverage factor <code class="reqn">k</code>, as calculated by <code class="reqn">t(1-(\alpha/2), \nu_{\mathrm{ws}})</code>.</p>
</td></tr>
<tr><td><code>u.exp</code></td>
<td>
<p>the expanded uncertainty, <code class="reqn">k\sigma(y)</code>, where <code class="reqn">\sigma(y)</code> is derived either from the second-order uncertainty, if successfully calculated, or first-order otherwise.</p>
</td></tr>
<tr><td><code>resSIM</code></td>
<td>
<p>a vector containing the <code>nsim</code> values obtained from the row-wise expression evaluations <code class="reqn">f(x_{m, i})</code> of the simulated data in <code>datSIM</code>.</p>
</td></tr>
<tr><td><code>datSIM</code></td>
<td>
<p>a vector containing the <code>nsim</code> simulated multivariate values for each variable in column format.</p>
</td></tr>  
<tr><td><code>prop</code></td>
<td>
<p>a summary vector containing first-/second-order expectations and uncertainties as well as the confidence interval based on <code>alpha</code>.</p>
</td></tr> 
<tr><td><code>sim</code></td>
<td>
<p>a summary vector containing the mean, standard deviation, median, MAD as well as the confidence interval based on <code>alpha</code>.</p>
</td></tr>
<tr><td><code>expr</code></td>
<td>
<p>the original expression <code>expr</code>.</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>the original data <code>data</code>.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>the otiginal <code>alpha</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Andrej-Nikolai Spiess
</p>


<h3>References</h3>

<p><b>Error propagation (in general):</b><br />
An Introduction to error analysis.<br />
Taylor JR.<br />
University Science Books (1996), New York.
</p>
<p>Evaluation of measurement data - Guide to the expression of uncertainty in measurement.<br />
JCGM 100:2008 (GUM 1995 with minor corrections).<br />
<a href="http://www.bipm.org/utils/common/documents/jcgm/JCGM_100_2008_E.pdf">http://www.bipm.org/utils/common/documents/jcgm/JCGM_100_2008_E.pdf</a>.
</p>
<p>Evaluation of measurement data - Supplement 1 to the Guide to the expression of uncertainty in measurement - Propagation of distributions using a Monte Carlo Method.<br />
JCGM 101:2008.<br />
<a href="http://www.bipm.org/utils/common/documents/jcgm/JCGM_101_2008_E.pdf">http://www.bipm.org/utils/common/documents/jcgm/JCGM_101_2008_E.pdf</a>.
</p>
<p><b>Higher-order Taylor expansion:</b><br />
On higher-order corrections for propagating uncertainties.<br />
Wang CM &amp; Iyer HK.<br />
<em>Metrologia</em> (2005), <b>42</b>: 406-410.
</p>
<p>Propagation of uncertainty: Expressions of second and third order uncertainty with third and fourth moments.<br />
Mekid S &amp; Vaja D.<br />
<em>Measurement</em> (2008), <b>41</b>: 600-609.
</p>
<p><b>Matrix algebra for error propagation:</b><br />
An Introduction to Error Propagation: Derivation, Meaning and Examples of Equation Cy = FxCxFx^t.<br />
<a href="www.nada.kth.se/~kai-a/papers/arrasTR-9801-R3.pdf">www.nada.kth.se/~kai-a/papers/arrasTR-9801-R3.pdf</a>.
</p>
<p>Second order nonlinear uncertainty modeling in strapdown integration using MEMS IMUs.<br />
Zhang M, Hol JD, Slot L, Luinge H.<br />
2011 Proceedings of the 14th International Conference on Information Fusion (FUSION) (2011).
</p>
<p>Uncertainty propagation in non-linear measurement equations.<br />
Mana G &amp; Pennecchi F.<br />
<em>Metrologia</em> (2007), <b>44</b>: 246-251.
</p>
<p>A compact tensor algebra expression of the law of propagation of uncertainty.<br />
Bouchot C, Quilantan JLC, Ochoa JCS.<br />
<em>Metrologia</em> (2011), <b>48</b>: L22-L28.
</p>
<p>Nonlinear error propagation law.<br />
Kubacek L.<br />
<em>Appl Math</em> (1996), <b>41</b>: 329-345.
</p>
<p><b>Monte Carlo simulation (normal- and t-distribution):</b><br />
MUSE: computational aspects of a GUM supplement 1 implementation.<br />
Mueller M, Wolf M, Roesslein M.<br />
<em>Metrologia</em> (2008), <b>45</b>: 586-594.
</p>
<p>Copulas for uncertainty analysis.<br />
Possolo A.<br />
<em>Metrologia</em> (2010), <b>47</b>: 262-271.
</p>
<p><b>Multivariate normal distribution:</b><br />
Stochastic Simulation.<br />
Ripley BD.<br />
Stochastic Simulation (1987). Wiley. Page 98.
</p>
<p><b>Testing for normal distribution:</b><br />
Testing for  Normality.<br />
Thode Jr. HC.<br />
Marcel Dekker (2002), New York.
</p>
<p>Approximating the Shapiro-Wilk W-test for non-normality.<br />
Royston P.<br /> 
<em>Stat Comp</em> (1992), <b>2</b>: 117-119.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## In these examples, 'nsim = 100000' to save
## Rcmd check time (CRAN). It is advocated
## to use at least 'nsim = 1000000' though...

## Example without given degrees-of-freedom.
EXPR1 &lt;- expression(x/y)
x &lt;- c(5, 0.01)
y &lt;- c(1, 0.01)
DF1 &lt;- cbind(x, y)
RES1 &lt;- propagate(expr = EXPR1, data = DF1, type = "stat", 
                  do.sim = TRUE, verbose = TRUE, 
                  nsim = 100000)
RES1

## Same example with given degrees-of-freedom
## =&gt; third row in input 'data'.
EXPR2 &lt;- expression(x/y)
x &lt;- c(5, 0.01, 12)
y &lt;- c(1, 0.01, 5)
DF2 &lt;- cbind(x, y)
RES2 &lt;- propagate(expr = EXPR2, data = DF2, type = "stat", 
                  do.sim = TRUE, verbose = TRUE,
                  nsim = 100000)
RES2

## With the 'summary' function, we can get the
## Welch-Satterthwaite DF's, coverage, expanded uncertainty,
## Gradient and Hessian matrix etc.
summary(RES2)

## Example using a recursive function:
## no Taylor expansion possible, only Monte-Carlo.
a &lt;- c(5, 0.1)
b &lt;- c(100, 2)
DAT &lt;- cbind(a, b)

f &lt;- function(a, b) {
  N &lt;- 0
  for (i in 1:100) {
    N &lt;- N + i * log(a) + b^(1/i)
  }
  return(N)
}

propagate(f, DAT, nsim = 100000)

## Not run: 
################# GUM 2008 (1) ########################
## Example in Annex H.1 from the GUM 2008 manual
## (see 'References'), an end gauge calibration
## study. We use only first-order error propagation,
## with total df = 16 and alpha = 0.01, 
## as detailed in GUM H.1.6.
EXPR3 &lt;- expression(ls + d - ls * (da * the + as * dt))
ls &lt;- c(50000623, 25)
d &lt;- c(215, 9.7)
da &lt;- c(0, 0.58E-6)
the &lt;- c(-0.1, 0.41)
as &lt;- c(11.5E-6, 1.2E-6)
dt &lt;- c(0, 0.029)
DF3 &lt;- cbind(ls, d, da, the, as, dt)
RES3 &lt;- propagate(expr = EXPR3, data = DF3, second.order = FALSE,
                  df = 16, alpha = 0.01)
RES3
## propagate: sd.1 = 31.71 
## GUM H.1.4/H.6c: u = 32  

## Expanded uncertainty, from summary function.
summary(RES3)
## propagate: 92.62
## GUM H.1.6: 93

## Proof that covariance of Monte-Carlo
## simulated dataset is "fairly"" the same 
## as from initial data.
RES3$covMat
cov(RES3$datSIM)
all.equal(RES3$covMat, cov(RES3$datSIM))

## Now using second-order Taylor expansion.
RES4 &lt;- propagate(expr = EXPR3, data = DF3)
RES4
## propagate: sd.2 = 33.91115
## GUM H.1.7: u = 34.
## Also similar to the non-matrix-based approach
## in Wang et al. (2005, page 408): u1 = 33.91115.
## NOTE: After second-order correction ("sd.2"), 
## uncertainty is more similar to the uncertainty
## obtained from Monte Carlo simulation!

#################### GUM 2008 (2) #################
## Example in Annex H.2 from the GUM 2008 manual
## (see 'References'), simultaneous resistance
## and reactance measurement.
data(H.2)

## This gives exactly the means, uncertainties and
## correlations as given in Table H.2:
colMeans(H.2)
sqrt(colVarsC(H.2))/sqrt(5)
cor(H.2)

## H.2.3 Approach 1 using mean values and
## standard uncertainties:
EXPR6a &lt;- expression((V/I) *  cos(phi)) ## R
EXPR6b &lt;- expression((V/I) *  sin(phi)) ## X
EXPR6c &lt;- expression(V/I) ## Z
MEAN6 &lt;- colMeans(H.2)
SD6 &lt;- sqrt(colVarsC(H.2))
DF6 &lt;- rbind(MEAN6, SD6)
COV6ab &lt;- cov(H.2) ## covariance matrix of V, I, phi
COV6c &lt;- cov(H.2[, 1:2])  ## covariance matrix of V, I

RES6a &lt;- propagate(expr = EXPR6a, data = DF6, cov = COV6ab)
RES6b &lt;- propagate(expr = EXPR6b, data = DF6, cov = COV6ab)
RES6c &lt;- propagate(expr = EXPR6c, data = DF6[, 1:2], 
                   cov = COV6c)

## This gives exactly the same values of mean and sd/sqrt(5)
## as given in Table H.4.
RES6a$prop # 0.15892/sqrt(5) = 0.071
RES6b$prop # 0.66094/sqrt(5) = 0.296
RES6c$prop # 0.52846/sqrt(5) = 0.236

######### GUM 2008 Supplement 1 (1) #######################
## Example from 9.2.2 of the GUM 2008 Supplement 1
## (see 'References'), normally distributed input
## quantities. Assign values as in 9.2.2.1.
EXPR7 &lt;- expression(X1 + X2 + X3 + X4)
X1 &lt;- c(0, 1)
X2 &lt;- c(0, 1)
X3 &lt;- c(0, 1)
X4 &lt;- c(0, 1)
DF7 &lt;- cbind(X1, X2, X3, X4)
RES7 &lt;- propagate(expr = EXPR7, data = DF7, nsim = 1E5)
## This will give exactly the same results as in 
## 9.2.2.6, Table 2.
RES7

######### GUM 2008 Supplement 1 (2) #######################
## Example from 9.3 of the GUM 2008 Supplement 1
## (see 'References'), mass calibration.
## Formula 24 in 9.3.1.3 and values as in 9.3.1.4, Table 5.
EXPR8 &lt;- expression((Mrc + dMrc) * (1 + (Pa - Pa0) * ((1/Pw) - (1/Pr))) - Mnom)
Mrc &lt;- rnorm(1E5, 100000, 0.050)
dMrc &lt;- rnorm(1E5, 1.234, 0.020)
Pa &lt;- runif(1E5, 1.10, 1.30)  ## E(Pa) = 1.2, (b-a)/2 = 0.1 
Pw &lt;- runif(1E5, 7000, 9000)  ## E(Pw) = 8000, (b-a)/2 = 1000
Pr &lt;- runif(1E5, 7950, 8050) ## E(Pr) = 8000, (b-a)/2 = 50
Pa0 &lt;- 1.2 
Mnom &lt;- 100000
DF8 &lt;- cbind(Mrc, dMrc, Pa, Pw, Pr, Pa0, Mnom)
RES8 &lt;- propagate(expr = EXPR8, data = DF8, nsim = 1E5)
## This will give exactly the same results as in 
## 9.3.2.3, Table 6
RES8
RES8
 
######### GUM 2008 Supplement 1 (3) #######################
## Example from 9.4 of the GUM 2008 Supplement 1
## (see 'References'), comparioson loss in microwave
## power meter calibration, zero covariance.
## Formula 28 in 9.4.1.5 and values as in 9.4.1.7.
EXPR9 &lt;- expression(X1^2 - X2^2)
X1 &lt;- c(0.050, 0.005)
X2 &lt;- c(0, 0.005)
DF9 &lt;- cbind(X1, X2)
RES9a &lt;- propagate(expr = EXPR9, data = DF9, nsim = 1E5)
## This will give exactly the same results as in 
## 9.4.2.2.7, Table 8, x1 = 0.050.
RES9a

## Using covariance matrix with r(x1, x2) = 0.9
## We convert to covariances using cor2cov.
COR9 &lt;- matrix(c(1, 0.9, 0.9, 1), nrow = 2)
COV9 &lt;- cor2cov(COR9, c(0.005^2, 0.005^2))
colnames(COV9) &lt;- c("X1", "X2")
rownames(COV9) &lt;- c("X1", "X2")
RES9b &lt;- propagate(expr = EXPR9, data = DF9, cov = COV9)
## This will give exactly the same results as in 
## 9.4.3.2.1, Table 9, x1 = 0.050.
RES9b

######### GUM 2008 Supplement 1 (4) #######################
## Example from 9.5 of the GUM 2008 Supplement 1
## (see 'References'), gauge block calibration.
## Assignment of PDF's as in Table 10 of 9.5.2.1.
EXPR10 &lt;- expression(Ls + D + d1 + d2 - Ls *(da *(t0 + Delta) + as * dt) - Lnom)
Lnom &lt;- 50000000
Ls &lt;- propagate:::rst(1000000, mean = 50000623, sd  = 25, df = 18)
D &lt;- propagate:::rst(1000000, mean = 215, sd = 6, df = 25)
d1 &lt;- propagate:::rst(1000000, mean = 0, sd = 4, df = 5)
d2 &lt;- propagate:::rst(1000000, mean = 0, sd = 7, df = 8)
as &lt;- runif(1000000, 9.5E-6, 13.5E-6)
t0 &lt;- rnorm(1000000, -0.1, 0.2)
Delta &lt;- propagate:::rarcsin(1000000, -0.5, 0.5)
da &lt;- propagate:::rctrap(1000000, -1E-6, 1E-6, 0.1E-6)
dt &lt;- propagate:::rctrap(1000000, -0.050, 0.050, 0.025)
DF10 &lt;- cbind(Ls, D, d1, d2, as, t0, Delta, da, dt, Lnom)
RES10 &lt;- propagate(expr = EXPR10, data = DF10, cov = FALSE, alpha = 0.01)
RES10
## This gives the same results as in 9.5.4.2, Table 11.
## However: results are exacter than in the GUM 2008
## manual, especially when comparing sd(Monte Carlo) with sd.2!
## GUM 2008 gives 32 and 36, respectively.
RES10

########## Comparison to Pythons 'soerp' ###################
## Exactly the same results as under 
## https://pypi.python.org/pypi/soerp ! 
EXPR11 &lt;- expression(C * sqrt((520 * H * P)/(M *(t + 460))))
H &lt;- c(64, 0.5)
M &lt;- c(16, 0.1)
P &lt;- c(361, 2)
t &lt;- c(165, 0.5)
C &lt;- c(38.4, 0) 
DAT11 &lt;- makeDat(EXPR11)
RES11 &lt;- propagate(expr = EXPR11, data = DAT11) 
RES11

## End(Not run)   
</code></pre>

<hr>
<h2 id='rDistr'>Creating random samples from a variety of useful distributions</h2><span id='topic+rDistr'></span>

<h3>Description</h3>

<p>These are random sample generators for 22 different continuous distributions which are not readily available as <code><a href="stats.html#topic+Distributions">Distributions</a></code> in <span class="rlang"><b>R</b></span>. Some of them are implemented in other specialized packages (i.e. <code>rsn</code> in package 'sn' or <code>rtrapezoid</code> in package 'trapezoid'), but here they are collated in a way that makes them easily accessible for Monte Carlo-based uncertainty propagation.
</p>


<h3>Details</h3>

<p>Random samples can be drawn from the following distributions:<br />
1) Skewed-normal distribution: <code>propagate:::rsn(n, location = 0, scale = 1, shape = 0)</code><br />
2) Generalized normal distribution: <code>propagate:::rgnorm(n, alpha = 1, xi = 1, kappa = -0.1)</code><br /> 
3) Scaled and shifted t-distribution: <code>propagate:::rst(n, mean = 0, sd = 1, df = 2)</code><br /> 
4) Gumbel distribution: <code>propagate:::rgumbel(n, location = 0, scale = 1)</code><br />
5) Johnson SU distribution: <code>propagate:::rJSU(n, xi = 0, lambda = 1, gamma = 1, delta = 1)</code><br />
6) Johnson SB distribution: <code>propagate:::rJSB(n, xi = 0, lambda = 1, gamma = 1, delta = 1)</code><br />
7) 3P Weibull distribution: <code>propagate:::rweibull2(n, location = 0, shape = 1, scale = 1)</code><br />
8) 4P Beta distribution: <code>propagate:::rbeta2(n, alpha1 = 1, alpha2 = 1, a = 0, b = 0)</code><br />
9) Triangular distribution: <code>propagate:::rtriang(n, a = 0, b = 1, c = 0.5)</code><br />
10) Trapezoidal distribution: <code>propagate:::rtrap(n, a = 0, b = 1, c = 2, d = 3)</code><br />
11) Laplacian distribution: <code>propagate:::rlaplace(n, mean = 0, sigma = 1)</code><br />
12) Arcsine distribution: <code>propagate:::rarcsin(n, a = 2, b = 1)</code><br />
13) von Mises distribution: <code>propagate:::rmises(n, mu = 1, kappa = 3)</code><br />
14) Curvilinear Trapezoidal distribution: <code>propagate:::rctrap(n, a = 0, b = 1, d = 0.1)</code><br />
15) Generalized trapezoidal distribution:<br />
<code>propagate:::rgtrap(n, min = 0, mode1 = 1/3, mode2 = 2/3, max = 1, n1 = 2, n3 = 2, alpha = 1)</code><br />
16) Inverse Gaussian distribution: <code>propagate:::rinvgauss(n, mean = 1, dispersion = 1)</code>
17) Generalized Extreme Value distribution: <code>propagate:::rgevd(n, loc = 0, scale = 1, shape = 0)</code>
with <code>n</code> = number of samples.<br />
18) Inverse Gamma distribution: <code>propagate:::rinvgamma(n, shape = 1, scale = 5)</code><br />
19) Rayleigh distribution: <code>propagate:::rrayleigh(n, mu = 1, sigma = 1)</code><br />
20) Burr distribution: <code>propagate:::rburr(n, k = 1)</code><br />
21) Chi distribution: <code>propagate:::rchi(n, nu = 5)</code><br />
22) Inverse Chi-Square distribution: <code>propagate:::rinvchisq(n, nu = 5)</code><br />
23) Cosine distribution:  <code>propagate:::rcosine(n, mu = 5, sigma = 1)</code><br />
</p>
<p>1) - 12), 17) - 22) use the inverse cumulative distribution function as mapping functions for <code><a href="stats.html#topic+runif">runif</a></code> (<b>Inverse Transform Method</b>):<br />
(1) <code class="reqn">U \sim \mathcal{U}(0, 1)</code><br />
(2) <code class="reqn">Y = F^{-1}(U, \beta)</code>
</p>
<p>16) uses binomial selection from a <code class="reqn">\chi^2</code>-distribution.
</p>
<p>13) - 15), 23) employ &quot;Rejection Sampling&quot; using a uniform envelope distribution (<b>Acceptance Rejection Method</b>):<br />
(1) Find <code class="reqn">F_{max} = \max(F([x_{min}, x_{max}], \beta)</code><br />
(2) <code class="reqn">U_{max} = 1/(x_{max} - x_{min})</code><br />
(3) <code class="reqn">A = F_{max}/U_{max}</code><br />
(4) <code class="reqn">U \sim \mathcal{U}(0, 1)</code><br />
(5) <code class="reqn">X \sim \mathcal{U}(x_{min}, x_{max})</code><br />
(6) <code class="reqn">Y \iff U \le A \cdot \mathcal{U}(X, x_{min}, x_{max})/F(X, \beta)</code><br />
</p>
<p>These four distributions are coded in a vectorized approach and are hence not much slower than implementations in C/C++ (0.2 - 0.5 sec for 100000 samples; 3 GHz Quadcore processor, 4 GByte RAM). The code for the random generators is in file &quot;distr-samplers.R&quot;.
</p>


<h3>Value</h3>

<p>A vector with <code>n</code> samples from the corresponding distribution.
</p>


<h3>Author(s)</h3>

<p>Andrej-Nikolai Spiess
</p>


<h3>References</h3>

<p><b>Inverse CDFs were taken from:</b><br />
&quot;The Ultimate Univariate Probability Distribution Explorer&quot;<br />
<a href="http://blog.wolfram.com/data/uploads/2013/02/ProbabilityDistributionExplorer.zip">http://blog.wolfram.com/data/uploads/2013/02/ProbabilityDistributionExplorer.zip</a>.
</p>
<p><b>Rejection Sampling in R:</b><br />
Rejection Sampling.<br />
<a href="https://www.r-bloggers.com/rejection-sampling/">https://www.r-bloggers.com/rejection-sampling/</a>.
</p>
<p>An example of rejection sampling.<br />
<a href="http://www.mas.ncl.ac.uk/~ndjw1/teaching/sim/reject/circ.html">http://www.mas.ncl.ac.uk/~ndjw1/teaching/sim/reject/circ.html</a>.
</p>
<p><b>Rejection Sampling in general:</b><br />
Non-uniform random variate generation.<br />
Devroye L.<br />
Springer-Verlag, New York (1986).
</p>
<p><b>Distributions:</b><br />
Continuous univariate distributions, Volume 1.<br />
Johnson NL, Kotz S and Balakrishnan N.<br />
<em>Wiley Series in Probability and Statistics, 2.ed</em> (2004).
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+propagate">propagate</a></code>, in which GUM 2008 Supplement 1 examples use these distributions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## First we create random samples from the
## von Mises distribution.
X &lt;- propagate:::rmises(1000000, mu = 1, kappa = 2)

## then we fit all available distributions
## with 'fitDistr'.
fitDistr(X, nbin = 200)
## =&gt; von Mises wins! (lowest BIC)

## End(Not run)
</code></pre>

<hr>
<h2 id='statVec'>Transform an input vector into one with defined mean and standard deviation</h2><span id='topic+statVec'></span>

<h3>Description</h3>

<p>Transforms an input vector into one with defined <code class="reqn">\mu</code> and <code class="reqn">\sigma</code> by using a scaled-and-shifted Z-transformation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>statVec(x, mean, sd)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="statVec_+3A_x">x</code></td>
<td>
<p>the input vector to be transformed.</p>
</td></tr>
<tr><td><code id="statVec_+3A_mean">mean</code></td>
<td>
<p>the desired mean of the created vector.</p>
</td></tr>   
<tr><td><code id="statVec_+3A_sd">sd</code></td>
<td>
<p>the desired standard deviation of the created vector.</p>
</td></tr>   
</table>


<h3>Details</h3>

<p>Calculates vector <code class="reqn">V</code> using a Z-transformation of the input vector <code class="reqn">X</code> and subsequent scaling by <code>sd</code> and shifting by <code>mean</code>:
</p>
<p style="text-align: center;"><code class="reqn">V = \frac{X - \mu_X}{\sigma_X} \cdot \rm{sd} + \rm{mean}</code>
</p>



<h3>Value</h3>

<p>A vector with defined <code class="reqn">\mu</code> and <code class="reqn">\sigma</code>.
</p>


<h3>Author(s)</h3>

<p>Andrej-Nikolai Spiess
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Create a 10-sized vector with mean = 10 and s.d. = 1.
x &lt;- rnorm(10, 5, 2)
mean(x) ## =&gt; mean is not 5!
sd(x) ## =&gt; s.d. is not 2!

z &lt;- statVec(x, 5, 2)
mean(z) ## =&gt; mean is 5!
sd(z) ## =&gt; s.d. is 2!
</code></pre>

<hr>
<h2 id='stochContr'>Stochastic contribution analysis of Monte Carlo simulation-derived propagated uncertainty</h2><span id='topic+stochContr'></span>

<h3>Description</h3>

<p>Conducts a &quot;stochastic contribution analysis&quot; by calculating the change in propagated uncertainty when each of the simulated variables is kept constant at its mean, i.e. the uncertainty is removed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stochContr(prop, plot = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stochContr_+3A_prop">prop</code></td>
<td>
<p>a <code>propagate</code> object.</p>
</td></tr>
<tr><td><code id="stochContr_+3A_plot">plot</code></td>
<td>
<p>logical. If <code>TRUE</code>, a boxplot with the original and mean-value propagated distribution.</p>
</td></tr>  
</table>


<h3>Details</h3>

<p>This function takes the Monte Carlo simulated data <code class="reqn">X_n</code> from a <code><a href="#topic+propagate">propagate</a></code> object (<code>...$datSIM</code>), sequentially substitutes each variable <code class="reqn">\beta_i</code> by its mean <code class="reqn">\bar{\beta_i}</code> and then re-evaluates the output distribution <code class="reqn">Y_n = f(\beta, X_n)</code>. Optional boxplots are displayed that compare the original <code class="reqn">Y_n\mathrm{(orig)}</code> to those obtained from removing <code class="reqn">\sigma</code> from each <code class="reqn">\beta_i</code>. Finally, the relative contribution <code class="reqn">C_i</code> for all <code class="reqn">\beta_i</code> is calculated by <code class="reqn">C_i = \sigma(Y_n\mathrm{(orig)})-\sigma(Y_n)</code>, and divided by its sum so that <code class="reqn">\sum_{i=1}^n C_i = 1</code>.
</p>


<h3>Value</h3>

<p>The relative contribution <code class="reqn">C_i</code> for all variables.
</p>


<h3>Author(s)</h3>

<p>Andrej-Nikolai Spiess
</p>


<h3>Examples</h3>

<pre><code class='language-R'>a &lt;- c(15, 1)
b &lt;- c(100, 5)
c &lt;- c(0.5, 0.02)
DAT &lt;- cbind(a, b, c)
EXPR &lt;- expression(a * b^sin(c))
RES &lt;- propagate(EXPR, DAT, nsim = 100000)
stochContr(RES)
</code></pre>

<hr>
<h2 id='summary.propagate'>Summary function for 'propagate' objects</h2><span id='topic+summary.propagate'></span>

<h3>Description</h3>

<p>Provides a printed summary of the results obtained by <code><a href="#topic+propagate">propagate</a></code>, such as statistics of the first/second-order uncertainty propagation, Monte Carlo simulation, the covariance matrix, symbolic as well as evaluated versions of the Gradient (&quot;sensitivity&quot;) and Hessian matrices, relative contributions, the coverage factor and the Welch-Satterthwaite degrees of freedom. If <code>do.sim = TRUE</code> was set in <code>propagate</code>, skewness/kurtosis and Shapiro-Wilks/Kolmogorov-Smirnov tests for normality are calculated on the Monte-Carlo evaluations. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'propagate'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.propagate_+3A_object">object</code></td>
<td>
<p>an object returned from <code><a href="#topic+propagate">propagate</a></code>.</p>
</td></tr>     
<tr><td><code id="summary.propagate_+3A_...">...</code></td>
<td>
<p>other parameters for future methods.</p>
</td></tr>       
</table>


<h3>Details</h3>

<p>Calculates the &quot;sensitivity&quot;&quot; <code class="reqn">S_i</code> of each variable <code class="reqn">x_i</code> to the propagated uncertainty, as defined in the <em>Expression of the Uncertainty of Measurement in Calibration, Eqn 4.2, page 9</em> (see 'References'):<br />
</p>
<p style="text-align: center;"><code class="reqn">S_i = \mathrm{eval}\left(\frac{\partial f}{\partial x_i}\right)</code>
</p>
<p><br /> 
The &quot;contribution&quot; matrix is then <code class="reqn">\mathbf{C} = \mathbf{SS}^T\mathbf{\Sigma}</code>, where <code class="reqn">\mathbf{\Sigma}</code> is the covariance matrix. In the implementation here, the &quot;relative contribution&quot; matrix <code class="reqn">\mathbf{C}_{\mathrm{rel}}</code> is rescaled to sum up to 1.
</p>


<h3>Value</h3>

<p>A printed output with the items listed in 'Description'.
</p>


<h3>Author(s)</h3>

<p>Andrej-Nikolai Spiess
</p>


<h3>References</h3>

<p>Expression of the Uncertainty of Measurement in Calibration.<br />
European Cooperation for Accreditation (EA-4/02), 1999.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>EXPR1 &lt;- expression(x^2 * sin(y))
x &lt;- c(5, 0.01)
y &lt;- c(1, 0.01)
DF1 &lt;- cbind(x, y)
RES1 &lt;- propagate(expr = EXPR1, data = DF1, type = "stat", 
                  do.sim = TRUE, verbose = TRUE, nsim = 100000)
summary(RES1)
</code></pre>

<hr>
<h2 id='WelchSatter'>Welch-Satterthwaite approximation to the 'effective degrees of freedom'</h2><span id='topic+WelchSatter'></span>

<h3>Description</h3>

<p>Calculates the Welch-Satterthwaite approximation to the 'effective degrees of freedom' by using the samples' uncertainties and degrees of freedoms, as described in Welch (1947) and Satterthwaite (1946). External sensitivity coefficients can be supplied optionally.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>WelchSatter(ui, ci = NULL, df = NULL, dftot = NULL, uc = NULL, alpha = 0.05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="WelchSatter_+3A_ui">ui</code></td>
<td>
<p>the uncertainties <code class="reqn">u_i</code> for each variable <code class="reqn">x_i</code>.</p>
</td></tr>
<tr><td><code id="WelchSatter_+3A_ci">ci</code></td>
<td>
<p>the sensitivity coefficients <code class="reqn">c_i = \partial y/\partial x_i</code>.</p>
</td></tr>   
<tr><td><code id="WelchSatter_+3A_df">df</code></td>
<td>
<p>the degrees of freedom for the samples, <code class="reqn">\nu_i</code>.</p>
</td></tr>
<tr><td><code id="WelchSatter_+3A_dftot">dftot</code></td>
<td>
<p>an optional known total degrees of freedom for the system, <code class="reqn">\nu_{\mathrm{tot}}</code>. Overrides the internal calculation of <code class="reqn">\nu_{\mathrm{ws}}</code>.</p>
</td></tr> 
<tr><td><code id="WelchSatter_+3A_uc">uc</code></td>
<td>
<p>the combined uncertainty, u(y).</p>
</td></tr>
<tr><td><code id="WelchSatter_+3A_alpha">alpha</code></td>
<td>
<p>the significance level for the t-statistic. See 'Details'.</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">\nu_{\rm{eff}} \approx \frac{u(y)^4}{\sum_{i = 1}^n \frac{(c_iu_i)^4}{\nu_i}},  \quad k = t(1 - (\alpha/2), \nu_{\rm{eff}}), \quad u_{\rm{exp}} = ku(y)</code>
</p>



<h3>Value</h3>

<p>A list with the following items:<br />
</p>
<table>
<tr><td><code>ws.df</code></td>
<td>
<p>the 'effective degrees of freedom'.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>the coverage factor for calculating the expanded uncertainty.</p>
</td></tr>
<tr><td><code>u.exp</code></td>
<td>
<p>the expanded uncertainty <code class="reqn">u_{\rm{exp}}</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Andrej-Nikolai Spiess
</p>


<h3>References</h3>

<p>An Approximate Distribution of Estimates of Variance Components.<br />
Satterthwaite FE.<br />
<em>Biometrics Bulletin</em> (1946), <b>2</b>: 110-114.
</p>
<p>The generalization of &quot;Student's&quot; problem when several different population variances are involved.<br />
Welch BL.<br />
<em>Biometrika</em> (1947), <b>34</b>: 28-35.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Taken from GUM H.1.6, 4).
WelchSatter(ui = c(25, 9.7, 2.9, 16.6), df = c(18, 25.6, 50, 2), uc = 32, alpha = 0.01)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
