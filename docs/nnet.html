<!DOCTYPE html><html lang="en"><head><title>Help for package nnet</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {nnet}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#class.ind'>
<p>Generates Class Indicator Matrix from a Factor</p></a></li>
<li><a href='#multinom'>
<p>Fit Multinomial Log-linear Models</p></a></li>
<li><a href='#nnet'>
<p>Fit Neural Networks</p></a></li>
<li><a href='#nnetHess'>
<p>Evaluates Hessian for a Neural Network</p></a></li>
<li><a href='#predict.nnet'>
<p>Predict New Examples by a Trained Neural Net</p></a></li>
<li><a href='#which.is.max'>
<p>Find Maximum Position in Vector</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Priority:</td>
<td>recommended</td>
</tr>
<tr>
<td>Version:</td>
<td>7.3-20</td>
</tr>
<tr>
<td>Date:</td>
<td>2025-01-01</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0.0), stats, utils</td>
</tr>
<tr>
<td>Suggests:</td>
<td>MASS</td>
</tr>
<tr>
<td>Description:</td>
<td>Software for feed-forward neural networks with a single
  hidden layer, and for multinomial log-linear models.</td>
</tr>
<tr>
<td>Title:</td>
<td>Feed-Forward Neural Networks and Multinomial Log-Linear Models</td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>yes</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="http://www.stats.ox.ac.uk/pub/MASS4/">http://www.stats.ox.ac.uk/pub/MASS4/</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-01-01 07:07:13 UTC; ripley</td>
</tr>
<tr>
<td>Author:</td>
<td>Brian Ripley [aut, cre, cph],
  William Venables [cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Brian Ripley &lt;Brian.Ripley@R-project.org&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-01-01 10:25:43 UTC</td>
</tr>
</table>
<hr>
<h2 id='class.ind'>
Generates Class Indicator Matrix from a Factor
</h2><span id='topic+class.ind'></span>

<h3>Description</h3>

<p>Generates a class indicator function from a given factor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>class.ind(cl)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="class.ind_+3A_cl">cl</code></td>
<td>

<p>factor or vector of classes for cases.
</p>
</td></tr></table>


<h3>Value</h3>

<p>a matrix which is zero except for the column corresponding to the class.
</p>


<h3>References</h3>

<p>Venables, W. N. and Ripley, B. D. (2002)
<em>Modern Applied Statistics with S.</em> Fourth edition.  Springer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># The function is currently defined as
class.ind &lt;- function(cl)
{
  n &lt;- length(cl)
  cl &lt;- as.factor(cl)
  x &lt;- matrix(0, n, length(levels(cl)) )
  x[(1:n) + n*(unclass(cl)-1)] &lt;- 1
  dimnames(x) &lt;- list(names(cl), levels(cl))
  x
}
</code></pre>

<hr>
<h2 id='multinom'>
Fit Multinomial Log-linear Models
</h2><span id='topic+multinom'></span><span id='topic+add1.multinom'></span><span id='topic+anova.multinom'></span><span id='topic+coef.multinom'></span><span id='topic+drop1.multinom'></span><span id='topic+extractAIC.multinom'></span><span id='topic+predict.multinom'></span><span id='topic+print.multinom'></span><span id='topic+summary.multinom'></span><span id='topic+print.summary.multinom'></span><span id='topic+vcov.multinom'></span><span id='topic+model.frame.multinom'></span><span id='topic+logLik.multinom'></span>

<h3>Description</h3>

<p>Fits multinomial log-linear models via neural networks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multinom(formula, data, weights, subset, na.action,
         contrasts = NULL, Hess = FALSE, summ = 0, censored = FALSE,
         model = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="multinom_+3A_formula">formula</code></td>
<td>

<p>a formula expression as for regression models, of the form
<code>response ~ predictors</code>. The response should be a factor or a
matrix with K columns, which will be interpreted as counts for each of
K classes.
A log-linear model is fitted, with coefficients zero for the first
class. An offset can be included: it should be a numeric matrix with K columns
if the response is either a matrix with K columns or a factor with K &gt;= 2
classes, or a numeric vector for a response factor with 2 levels.
See the documentation of <code><a href="stats.html#topic+formula">formula</a>()</code> for other details.
</p>
</td></tr>
<tr><td><code id="multinom_+3A_data">data</code></td>
<td>

<p>an optional data frame in which to interpret the variables occurring
in <code>formula</code>.
</p>
</td></tr>
<tr><td><code id="multinom_+3A_weights">weights</code></td>
<td>

<p>optional case weights in fitting.
</p>
</td></tr>
<tr><td><code id="multinom_+3A_subset">subset</code></td>
<td>

<p>expression saying which subset of the rows of the data should  be used
in the fit. All observations are included by default.
</p>
</td></tr>
<tr><td><code id="multinom_+3A_na.action">na.action</code></td>
<td>

<p>a function to filter missing data.
</p>
</td></tr>
<tr><td><code id="multinom_+3A_contrasts">contrasts</code></td>
<td>

<p>a list of contrasts to be used for some or all of
the factors appearing as variables in the model formula.
</p>
</td></tr>
<tr><td><code id="multinom_+3A_hess">Hess</code></td>
<td>

<p>logical for whether the Hessian (the observed/expected information matrix)
should be returned.
</p>
</td></tr>
<tr><td><code id="multinom_+3A_summ">summ</code></td>
<td>

<p>integer; if non-zero summarize by deleting duplicate rows and adjust weights.
Methods 1 and 2 differ in speed (2 uses <code>C</code>); method 3 also combines rows
with the same X and different Y, which changes the baseline for the
deviance.
</p>
</td></tr>
<tr><td><code id="multinom_+3A_censored">censored</code></td>
<td>

<p>If Y is a matrix with <code>K</code> columns, interpret the entries as one
for possible classes, zero for impossible classes, rather than as
counts.
</p>
</td></tr>
<tr><td><code id="multinom_+3A_model">model</code></td>
<td>

<p>logical. If true, the model frame is saved as component <code>model</code>
of the returned object.
</p>
</td></tr>
<tr><td><code id="multinom_+3A_...">...</code></td>
<td>

<p>additional arguments for <code>nnet</code>
</p>
</td></tr></table>


<h3>Details</h3>

<p><code>multinom</code> calls <code><a href="#topic+nnet">nnet</a></code>.  The variables on the rhs of
the formula should be roughly scaled to [0,1] or the fit will be slow
or may not converge at all.
</p>


<h3>Value</h3>

<p>A <code>nnet</code> object with additional components:
</p>
<table role = "presentation">
<tr><td><code>deviance</code></td>
<td>

<p>the residual deviance, compared to the full saturated model (that
explains individual observations exactly).  Also, minus twice log-likelihood.
</p>
</td></tr>
<tr><td><code>edf</code></td>
<td>

<p>the (effective) number of degrees of freedom used by the model
</p>
</td></tr>
<tr><td><code>AIC</code></td>
<td>

<p>the AIC for this fit.
</p>
</td></tr>
<tr><td><code>Hessian</code></td>
<td>

<p>(if <code>Hess</code> is true).
</p>
</td></tr>
<tr><td><code>model</code></td>
<td>

<p>(if <code>model</code> is true).
</p>
</td></tr></table>


<h3>References</h3>

<p>Venables, W. N. and Ripley, B. D. (2002)
<em>Modern Applied Statistics with S.</em> Fourth edition.  Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nnet">nnet</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>oc &lt;- options(contrasts = c("contr.treatment", "contr.poly"))
library(MASS)
example(birthwt)
(bwt.mu &lt;- multinom(low ~ ., bwt))
options(oc)
</code></pre>

<hr>
<h2 id='nnet'>
Fit Neural Networks
</h2><span id='topic+nnet'></span><span id='topic+nnet.default'></span><span id='topic+nnet.formula'></span><span id='topic+add.net'></span><span id='topic+norm.net'></span><span id='topic+eval.nn'></span><span id='topic+coef.nnet'></span><span id='topic+print.nnet'></span><span id='topic+summary.nnet'></span><span id='topic+print.summary.nnet'></span>

<h3>Description</h3>

<p>Fit single-hidden-layer neural network, possibly with skip-layer connections.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnet(x, ...)

## S3 method for class 'formula'
nnet(formula, data, weights, ...,
     subset, na.action, contrasts = NULL)

## Default S3 method:
nnet(x, y, weights, size, Wts, mask,
     linout = FALSE, entropy = FALSE, softmax = FALSE,
     censored = FALSE, skip = FALSE, rang = 0.7, decay = 0,
     maxit = 100, Hess = FALSE, trace = TRUE, MaxNWts = 1000,
     abstol = 1.0e-4, reltol = 1.0e-8, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nnet_+3A_formula">formula</code></td>
<td>

<p>A formula of the form <code>class ~ x1 + x2 + ...</code>
</p>
</td></tr>
<tr><td><code id="nnet_+3A_x">x</code></td>
<td>

<p>matrix or data frame of <code>x</code> values for examples.
</p>
</td></tr>
<tr><td><code id="nnet_+3A_y">y</code></td>
<td>

<p>matrix or data frame of target values for examples.
</p>
</td></tr>
<tr><td><code id="nnet_+3A_weights">weights</code></td>
<td>

<p>(case) weights for each example &ndash; if missing defaults to 1.
</p>
</td></tr>
<tr><td><code id="nnet_+3A_size">size</code></td>
<td>

<p>number of units in the hidden layer. Can be zero if there are skip-layer units.
</p>
</td></tr>
<tr><td><code id="nnet_+3A_data">data</code></td>
<td>

<p>Data frame from which variables specified in  <code>formula</code> are
preferentially to be taken.
</p>
</td></tr>
<tr><td><code id="nnet_+3A_subset">subset</code></td>
<td>

<p>An index vector specifying the cases to be used in the training
sample.  (NOTE: If given, this argument must be named.)
</p>
</td></tr>
<tr><td><code id="nnet_+3A_na.action">na.action</code></td>
<td>

<p>A function to specify the action to be taken if <code>NA</code>s are found.
The default action is for the procedure to fail.  An alternative is
na.omit, which leads to rejection of cases with missing values on
any required variable.  (NOTE: If given, this argument must be named.)
</p>
</td></tr>
<tr><td><code id="nnet_+3A_contrasts">contrasts</code></td>
<td>

<p>a list of contrasts to be used for some or all  of
the  factors  appearing as variables in the model formula.
</p>
</td></tr>
<tr><td><code id="nnet_+3A_wts">Wts</code></td>
<td>

<p>initial parameter vector. If missing chosen at random.
</p>
</td></tr>
<tr><td><code id="nnet_+3A_mask">mask</code></td>
<td>

<p>logical vector indicating which parameters should be optimized (default all).
</p>
</td></tr>
<tr><td><code id="nnet_+3A_linout">linout</code></td>
<td>

<p>switch for linear output units. Default logistic output units.
</p>
</td></tr>
<tr><td><code id="nnet_+3A_entropy">entropy</code></td>
<td>

<p>switch for entropy (= maximum conditional likelihood) fitting.
Default by least-squares.
</p>
</td></tr>
<tr><td><code id="nnet_+3A_softmax">softmax</code></td>
<td>

<p>switch for softmax (log-linear model) and maximum conditional
likelihood fitting. <code>linout</code>, <code>entropy</code>, <code>softmax</code> and <code>censored</code> are mutually
exclusive.
</p>
</td></tr>
<tr><td><code id="nnet_+3A_censored">censored</code></td>
<td>

<p>A variant on <code>softmax</code>, in which non-zero targets mean possible
classes. Thus for <code>softmax</code> a row of <code>(0, 1, 1)</code> means one example
each of classes 2 and 3, but for <code>censored</code> it means one example whose
class is only known to be 2 or 3.
</p>
</td></tr>
<tr><td><code id="nnet_+3A_skip">skip</code></td>
<td>

<p>switch to add skip-layer connections from input to output.
</p>
</td></tr>
<tr><td><code id="nnet_+3A_rang">rang</code></td>
<td>

<p>Initial random weights on [-<code>rang</code>, <code>rang</code>].  Value about 0.5 unless the
inputs are large, in which case it should be chosen so that
<code>rang</code> * max(<code>|x|</code>) is about 1.
</p>
</td></tr>
<tr><td><code id="nnet_+3A_decay">decay</code></td>
<td>

<p>parameter for weight decay.  Default 0.
</p>
</td></tr>
<tr><td><code id="nnet_+3A_maxit">maxit</code></td>
<td>

<p>maximum number of iterations. Default 100.
</p>
</td></tr>
<tr><td><code id="nnet_+3A_hess">Hess</code></td>
<td>

<p>If true, the Hessian of the measure of fit at the best set of weights
found is returned as component <code>Hessian</code>.
</p>
</td></tr>
<tr><td><code id="nnet_+3A_trace">trace</code></td>
<td>

<p>switch for tracing optimization. Default <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="nnet_+3A_maxnwts">MaxNWts</code></td>
<td>

<p>The maximum allowable number of weights.  There is no intrinsic limit
in the code, but increasing <code>MaxNWts</code> will probably allow fits that
are very slow and time-consuming.
</p>
</td></tr>
<tr><td><code id="nnet_+3A_abstol">abstol</code></td>
<td>

<p>Stop if the fit criterion falls below <code>abstol</code>, indicating an
essentially perfect fit.
</p>
</td></tr>
<tr><td><code id="nnet_+3A_reltol">reltol</code></td>
<td>

<p>Stop if the optimizer is unable to reduce the fit criterion by a
factor of at least <code>1 - reltol</code>.
</p>
</td></tr>
<tr><td><code id="nnet_+3A_...">...</code></td>
<td>

<p>arguments passed to or from other methods.
</p>
</td></tr></table>


<h3>Details</h3>

<p>If the response in <code>formula</code> is a factor, an appropriate classification
network is constructed; this has one output and entropy fit if the
number of levels is two, and a number of outputs equal to the number
of classes and a softmax output stage for more levels.  If the
response is not a factor, it is passed on unchanged to <code>nnet.default</code>.
</p>
<p>Optimization is done via the BFGS method of <code><a href="stats.html#topic+optim">optim</a></code>.
</p>


<h3>Value</h3>

<p>object of class <code>"nnet"</code> or <code>"nnet.formula"</code>.
Mostly internal structure, but has components
</p>
<table role = "presentation">
<tr><td><code>wts</code></td>
<td>

<p>the best set of weights found
</p>
</td></tr>
<tr><td><code>value</code></td>
<td>

<p>value of fitting criterion plus weight decay term.
</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>

<p>the fitted values for the training data.
</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>

<p>the residuals for the training data.
</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>

<p><code>1</code> if the maximum number of iterations was reached, otherwise <code>0</code>.
</p>
</td></tr></table>


<h3>References</h3>

<p>Ripley, B. D. (1996)
<em>Pattern Recognition and Neural Networks.</em> Cambridge.
</p>
<p>Venables, W. N. and Ripley, B. D. (2002)
<em>Modern Applied Statistics with S.</em> Fourth edition.  Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.nnet">predict.nnet</a></code>, <code><a href="#topic+nnetHess">nnetHess</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># use half the iris data
ir &lt;- rbind(iris3[,,1],iris3[,,2],iris3[,,3])
targets &lt;- class.ind( c(rep("s", 50), rep("c", 50), rep("v", 50)) )
samp &lt;- c(sample(1:50,25), sample(51:100,25), sample(101:150,25))
ir1 &lt;- nnet(ir[samp,], targets[samp,], size = 2, rang = 0.1,
            decay = 5e-4, maxit = 200)
test.cl &lt;- function(true, pred) {
    true &lt;- max.col(true)
    cres &lt;- max.col(pred)
    table(true, cres)
}
test.cl(targets[-samp,], predict(ir1, ir[-samp,]))


# or
ird &lt;- data.frame(rbind(iris3[,,1], iris3[,,2], iris3[,,3]),
        species = factor(c(rep("s",50), rep("c", 50), rep("v", 50))))
ir.nn2 &lt;- nnet(species ~ ., data = ird, subset = samp, size = 2, rang = 0.1,
               decay = 5e-4, maxit = 200)
table(ird$species[-samp], predict(ir.nn2, ird[-samp,], type = "class"))
</code></pre>

<hr>
<h2 id='nnetHess'>
Evaluates Hessian for a Neural Network
</h2><span id='topic+nnetHess'></span>

<h3>Description</h3>

<p>Evaluates the Hessian (matrix of second derivatives) of the specified
neural network. Normally called via argument <code>Hess=TRUE</code> to <code>nnet</code> or via
<code>vcov.multinom</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnetHess(net, x, y, weights)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nnetHess_+3A_net">net</code></td>
<td>

<p>object of class <code>nnet</code> as returned by <code>nnet</code>.
</p>
</td></tr>
<tr><td><code id="nnetHess_+3A_x">x</code></td>
<td>

<p>training data.
</p>
</td></tr>
<tr><td><code id="nnetHess_+3A_y">y</code></td>
<td>

<p>classes for training data.
</p>
</td></tr>
<tr><td><code id="nnetHess_+3A_weights">weights</code></td>
<td>

<p>the (case) weights used in the <code>nnet</code> fit.
</p>
</td></tr></table>


<h3>Value</h3>

<p>square symmetric matrix of the Hessian evaluated at the weights stored
in the net.
</p>


<h3>References</h3>

<p>Ripley, B. D. (1996)
<em>Pattern Recognition and Neural Networks.</em> Cambridge.
</p>
<p>Venables, W. N. and Ripley, B. D. (2002)
<em>Modern Applied Statistics with S.</em> Fourth edition.  Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nnet">nnet</a></code>, <code><a href="#topic+predict.nnet">predict.nnet</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># use half the iris data
ir &lt;- rbind(iris3[,,1], iris3[,,2], iris3[,,3])
targets &lt;- matrix(c(rep(c(1,0,0),50), rep(c(0,1,0),50), rep(c(0,0,1),50)),
150, 3, byrow=TRUE)
samp &lt;- c(sample(1:50,25), sample(51:100,25), sample(101:150,25))
ir1 &lt;- nnet(ir[samp,], targets[samp,], size=2, rang=0.1, decay=5e-4, maxit=200)
eigen(nnetHess(ir1, ir[samp,], targets[samp,]), TRUE)$values
</code></pre>

<hr>
<h2 id='predict.nnet'>
Predict New Examples by a Trained Neural Net
</h2><span id='topic+predict.nnet'></span>

<h3>Description</h3>

<p>Predict new examples by a trained neural net.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'nnet'
predict(object, newdata, type = c("raw","class"), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.nnet_+3A_object">object</code></td>
<td>

<p>an object of class <code>nnet</code> as  returned by <code>nnet</code>.
</p>
</td></tr>
<tr><td><code id="predict.nnet_+3A_newdata">newdata</code></td>
<td>

<p>matrix or data frame of test examples. A vector is considered to be
a row vector comprising a single case.
</p>
</td></tr>
<tr><td><code id="predict.nnet_+3A_type">type</code></td>
<td>

<p>Type of output
</p>
</td></tr>
<tr><td><code id="predict.nnet_+3A_...">...</code></td>
<td>

<p>arguments passed to or from other methods.
</p>
</td></tr></table>


<h3>Details</h3>

<p>This function is a method for the generic function
<code>predict()</code> for class <code>"nnet"</code>.
It can be invoked by calling <code>predict(x)</code> for an
object <code>x</code> of the appropriate class, or directly by
calling <code>predict.nnet(x)</code> regardless of the
class of the object.
</p>


<h3>Value</h3>

<p>If <code>type = "raw"</code>, the matrix of values returned by the trained network;
if <code>type = "class"</code>, the corresponding class (which is probably only
useful if the net was generated by <code>nnet.formula</code>).
</p>


<h3>References</h3>

<p>Ripley, B. D. (1996)
<em>Pattern Recognition and Neural Networks.</em> Cambridge.
</p>
<p>Venables, W. N. and Ripley, B. D. (2002)
<em>Modern Applied Statistics with S.</em> Fourth edition.  Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nnet">nnet</a></code>, <code><a href="#topic+which.is.max">which.is.max</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># use half the iris data
ir &lt;- rbind(iris3[,,1], iris3[,,2], iris3[,,3])
targets &lt;- class.ind( c(rep("s", 50), rep("c", 50), rep("v", 50)) )
samp &lt;- c(sample(1:50,25), sample(51:100,25), sample(101:150,25))
ir1 &lt;- nnet(ir[samp,], targets[samp,],size = 2, rang = 0.1,
            decay = 5e-4, maxit = 200)
test.cl &lt;- function(true, pred){
        true &lt;- max.col(true)
        cres &lt;- max.col(pred)
        table(true, cres)
}
test.cl(targets[-samp,], predict(ir1, ir[-samp,]))

# or
ird &lt;- data.frame(rbind(iris3[,,1], iris3[,,2], iris3[,,3]),
        species = factor(c(rep("s",50), rep("c", 50), rep("v", 50))))
ir.nn2 &lt;- nnet(species ~ ., data = ird, subset = samp, size = 2, rang = 0.1,
               decay = 5e-4, maxit = 200)
table(ird$species[-samp], predict(ir.nn2, ird[-samp,], type = "class"))
</code></pre>

<hr>
<h2 id='which.is.max'>
Find Maximum Position in Vector
</h2><span id='topic+which.is.max'></span>

<h3>Description</h3>

<p>Find the maximum position in a vector, breaking ties at random.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>which.is.max(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="which.is.max_+3A_x">x</code></td>
<td>

<p>a vector
</p>
</td></tr></table>


<h3>Details</h3>

<p>Ties are broken at random.
</p>


<h3>Value</h3>

<p>index of a maximal value.
</p>


<h3>References</h3>

<p>Venables, W. N. and Ripley, B. D. (2002)
<em>Modern Applied Statistics with S.</em> Fourth edition.  Springer.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+max.col">max.col</a></code>, <code><a href="base.html#topic+which.max">which.max</a></code> which takes the first of ties.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: ## this is incomplete
pred &lt;- predict(nnet, test)
table(true, apply(pred, 1, which.is.max))

## End(Not run)</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
