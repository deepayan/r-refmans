<!DOCTYPE html><html><head><title>Help for package FeatureHashing</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {FeatureHashing}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#CSCMatrix-class'><p>CSCMatrix</p></a></li>
<li><a href='#hash.mapping'><p>Extract mapping between hash and original values</p></a></li>
<li><a href='#hash.size'><p>Compute minimum hash size to reduce collision rate</p></a></li>
<li><a href='#hashed.model.matrix'><p>Create a model matrix with feature hashing</p></a></li>
<li><a href='#intToRaw'><p>Convert the integer to raw vector with endian correction</p></a></li>
<li><a href='#ipinyou'><p>iPinYou Real-Time Bidding Dataset for Computational Advertising Research</p></a></li>
<li><a href='#simulate.split'><p>Simulate how <code>split</code> work in <code>hashed.model.matrix</code> to split the string into</p>
tokens</a></li>
<li><a href='#test.tag'><p>test.tag</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Creates a Model Matrix via Feature Hashing with a Formula
Interface</td>
</tr>
<tr>
<td>Version:</td>
<td>0.9.2</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-01-10</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Wush Wu &lt;wush978@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Feature hashing, also called as the hashing trick, is a method to transform 
  features of a instance to a vector. Thus, it is a method to transform a real dataset to a matrix. 
  Without looking up the indices in an associative array, 
  it applies a hash function to the features and uses their hash values as indices directly.
  The method of feature hashing in this package was proposed in Weinberger et al. (2009) &lt;<a href="https://arxiv.org/abs/0902.2206">arXiv:0902.2206</a>&gt;. 
  The hashing algorithm is the murmurhash3 from the 'digest' package. 
  Please see the README in <a href="https://github.com/wush978/FeatureHashing">https://github.com/wush978/FeatureHashing</a> for more information.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a> | file LICENSE</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0), methods</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.11), Matrix, digest(&ge; 0.6.8), magrittr (&ge; 1.5)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, digest(&ge; 0.6.8), BH(&ge; 1.54.0-1)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>RUnit, glmnet, knitr, xgboost, rmarkdown, pROC</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/wush978/FeatureHashing/issues">https://github.com/wush978/FeatureHashing/issues</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/wush978/FeatureHashing">https://github.com/wush978/FeatureHashing</a></td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-01-10 13:51:31 UTC; wush</td>
</tr>
<tr>
<td>Author:</td>
<td>Wush Wu [aut, cre],
  Michael Benesty [aut, ctb]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-01-10 15:33:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='CSCMatrix-class'>CSCMatrix</h2><span id='topic+CSCMatrix-class'></span><span id='topic+dim+3C-+2CCSCMatrix-method'></span><span id='topic+dim+2CCSCMatrix-method'></span><span id='topic++25+2A+25+2CCSCMatrix+2Cnumeric-method'></span><span id='topic++25+2A+25+2Cnumeric+2CCSCMatrix-method'></span><span id='topic++5B+2CCSCMatrix+2Cmissing+2Cnumeric+2CANY-method'></span><span id='topic++5B+2CCSCMatrix+2Cnumeric+2Cmissing+2CANY-method'></span><span id='topic++5B+2CCSCMatrix+2Cnumeric+2Cnumeric+2CANY-method'></span>

<h3>Description</h3>

<p>The structure of <code>CSCMatrix</code> is the same
as the structure of <code>dgCMatrix</code>. However, the
<code>CSCMatrix</code> has weaker constraints compared to <code>dgCMatrix</code>.
</p>
<p><code>CSCMatrix</code> onlysupports limited operators. The users can convert it to
<code>dgCMatrix</code> for compatibility of existed algorithms.
</p>


<h3>Details</h3>

<p>The <code>CSCMatrix</code> violates two constraints used in <code>dgCMatrix</code>:
</p>

<ul>
<li><p> The row indices should be sorted with columns.
</p>
</li>
<li><p> The row indices should be unique with columns.
</p>
</li></ul>

<p>The result of matrix-vector multiplication should be the same.
</p>


<h3>Methods</h3>


<ul>
<li> <p><code>dim</code> The dimension of the matrix object <code>CSCMatrix</code>.
</p>
</li>
<li> <p><code>dim&lt;-</code> The assignment of dimension of the matrix object <code>CSCMatrix</code>.
</p>
</li>
<li> <p><code>[</code> The subsetting operator of the matrix object <code>CSCMatrix</code>.
</p>
</li>
<li> <p><code>%*%</code> The matrix-vector multiplication of the matrix object <code>CSCMatrix</code>.
The returned object is a numeric vector.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="Matrix.html#topic+dgCMatrix-class">dgCMatrix-class</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># construct a CSCMatrix
m &lt;- hashed.model.matrix(~ ., CO2, 8)
# convert it to dgCMatrix
m2 &lt;- as(m, "dgCMatrix")
</code></pre>

<hr>
<h2 id='hash.mapping'>Extract mapping between hash and original values</h2><span id='topic+hash.mapping'></span>

<h3>Description</h3>

<p>Extract mapping between hash and original values
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hash.mapping(matrix)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hash.mapping_+3A_matrix">matrix</code></td>
<td>
<p>Matrix returned by <code>hashed.model.matrix</code> function</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Generate a mapping between original values and hashes. 
</p>
<p>Option <code>create.mapping = T</code> needs to be used in function <code>hashed.model.matrix</code>.
</p>
<p>Original values are stores in the names of the vector.
</p>


<h3>Value</h3>

<p>a named <code>numeric</code> vector
</p>


<h3>Author(s)</h3>

<p>Michael Benesty
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ipinyou)

m &lt;- hashed.model.matrix(~., ipinyou.train, 2^10, create.mapping = TRUE)
mapping &lt;- hash.mapping(m)

</code></pre>

<hr>
<h2 id='hash.size'>Compute minimum hash size to reduce collision rate</h2><span id='topic+hash.size'></span>

<h3>Description</h3>

<p>Compute minimum hash size to reduce collision rate
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hash.size(df)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hash.size_+3A_df">df</code></td>
<td>
<p><code>data.frame</code> with data to hash</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To reduce collision rate, the hash size should be 
equal or superior to the nearest power of two to the number
of unique values in the input <code>data.frame</code>.
</p>
<p>The value computed is a theorical minimum hash size.
It just means that in the best situation it may be 
possible that all computed hash can be stored with
this hash size.
</p>
<p>Intuitively, if the distribution of hash generated by the algorithm
was perfect, when the computed size is used, each permutation of bits
of the hash vector would correspond to one unique original value of
your <code>data.frame</code>. 
</p>
<p>Because a bit value is <code>{0,1}</code>, the computed size is <code>2^x</code>
with a <code>x</code> big enough to have a hash vector containing each
original value.
</p>
<p>In real life, there will be some collisions if the computed
size is used because the distribution of hash is not
perfect. However, the hashing algorithm <code>Murmur3</code> is known to
minimize the number of collisions and is also very performant.
</p>
<p>The only known solution to have zero collision is to build a
dictionnary of values, and for each new value to hash, check in the
dictionnary if the hash value already exists. It is not performant at all.
</p>
<p>If you increase the computed size (by multiplying it by <code>2^x</code>, 
it is up to you to choose a <code>x</code>), you will reduce the collision rate.
If you use a value under the computed size,
there is a 100
</p>
<p>There is a trade-off between collision rate and memory
used to store hash. Machine learning algorithms usually deal
well with collisions when the rate is reasonable.
</p>


<h3>Value</h3>

<p>The hash size of feature hashing as a positive integer.
</p>


<h3>Author(s)</h3>

<p>Michael Benesty
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ipinyou)

#First try with a size of 2^10
mat1 &lt;- hashed.model.matrix(~., ipinyou.train, 2^10, create.mapping = TRUE)

#Extract mapping
mapping1 &lt;- hash.mapping(mat1)
#Rate of collision
mean(duplicated(mapping1))

#Second try, the size is computed
size &lt;- hash.size(ipinyou.train)
mat2 &lt;- hashed.model.matrix(~., ipinyou.train, size, create.mapping = TRUE)

#Extract mapping
mapping2 &lt;- hash.mapping(mat2)
#Rate of collision
mean(duplicated(mapping2))

</code></pre>

<hr>
<h2 id='hashed.model.matrix'>Create a model matrix with feature hashing</h2><span id='topic+hashed.model.matrix'></span><span id='topic+hashed.value'></span><span id='topic+hash.sign'></span><span id='topic+hashed.interaction.value'></span>

<h3>Description</h3>

<p>Create a model matrix with feature hashing
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hashed.model.matrix(
  formula,
  data,
  hash.size = 2^18,
  transpose = FALSE,
  create.mapping = FALSE,
  is.dgCMatrix = TRUE,
  signed.hash = FALSE,
  progress = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hashed.model.matrix_+3A_formula">formula</code></td>
<td>
<p><code>formula</code> or a <code>character</code> vector of column names (will be expanded to a <code>formula</code>)</p>
</td></tr>
<tr><td><code id="hashed.model.matrix_+3A_data">data</code></td>
<td>
<p>data.frame. The original data.</p>
</td></tr>
<tr><td><code id="hashed.model.matrix_+3A_hash.size">hash.size</code></td>
<td>
<p>positive integer. The hash size of feature hashing.</p>
</td></tr>
<tr><td><code id="hashed.model.matrix_+3A_transpose">transpose</code></td>
<td>
<p>logical value. Indicating if the transpose should be returned. It affects the space
of the returned object when the dimension is imbalanced. Please see the details.</p>
</td></tr>
<tr><td><code id="hashed.model.matrix_+3A_create.mapping">create.mapping</code></td>
<td>
<p>logical value. The indicator of whether storing the hash mapping or not. 
The mapping might miss some interaction terms which involves <code>split</code>ed features. 
Please see the details.</p>
</td></tr>
<tr><td><code id="hashed.model.matrix_+3A_is.dgcmatrix">is.dgCMatrix</code></td>
<td>
<p>logical value. Indicating if the result is <code>dgCMatrix</code> or <code>CSCMatrix</code></p>
</td></tr>
<tr><td><code id="hashed.model.matrix_+3A_signed.hash">signed.hash</code></td>
<td>
<p>logical value. Indicating if the hashed value is multipled by random sign.
This will reduce the impact of collision. Disable it will enhance the speed.</p>
</td></tr>
<tr><td><code id="hashed.model.matrix_+3A_progress">progress</code></td>
<td>
<p>logical value. Indicating if the progress bar is displayed or not.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>hashed.model.matrix</code> hashes the feature during
the construction of the model matrix. It uses the 32-bit variant of MurmurHash3 
<a href="https://github.com/aappleby/smhasher">https://github.com/aappleby/smhasher</a>. Weinberger 
et. al. (2009) used two separate hashing function <code class="reqn">h</code>(<code>hashed.value</code>) and 
<code class="reqn">\xi</code>(<code>hash.sign</code>) to determine the indices and the sign of the values
respectively. Different seeds are used to implement the hashing function 
<code class="reqn">h</code> and <code class="reqn">\xi</code> with MurmurHash3.
</p>
<p>The formula is parsed via <code><a href="stats.html#topic+terms.formula">terms.formula</a></code> with &quot;split&quot; as special
keyword. The interaction term is hashed (the reader can try  to expl)in different ways. Please see example for 
the detailed implementation. We provide a helper function: <code><a href="#topic+hashed.interaction.value">hashed.interaction.value</a></code> to show show the index after interaction.
The &quot;<code>split</code>&quot; is used to expand the concatenated feature
such as &quot;10129,10024,13866,10111,10146,10120,10115,10063&quot; which represents the occurrence of 
multiple categorical variable: &quot;10129&quot;, &quot;10024&quot;, &quot;13866&quot;, &quot;10111&quot;, &quot;10146&quot;, &quot;10120&quot;, &quot;10115&quot;, and
&quot;10063&quot;. The <code>hashed.model.matrix</code> will expand the concatenated feature and produce
the related model matrix.
</p>
<p>The &quot;<code>split</code>&quot; accepts two parameters:
</p>

<ul>
<li> <p><code>delim</code>, character value to use as delimiter for splitting;
</p>
</li>
<li> <p><code>type</code>, one of <code>existence</code>, <code>count</code> or <code>tf-idf</code>.
</p>
</li></ul>

<p>If <code>type</code> is set to <code>tf-idf</code>, then <code>signed.hash</code> should be set to <code>FALSE</code>.
</p>
<p>The user could explore the behavior via function <code><a href="#topic+simulate.split">simulate.split</a></code>.
</p>
<p>The argument <code>transpose</code> affects the size of the returned object in the following way.
For a <code class="reqn">m \times n</code> matrix with <code class="reqn">k</code> non-zero elements, the returned <code>dgCMatrix</code> requires
<code class="reqn">O(n) + O(k)</code> space. For details, please check the documentation of 
the <code><a href="Matrix.html#topic+dgCMatrix-class">dgCMatrix-class</a></code>. Note that the <code>rownames</code> of the returned <code>dgCMatrix</code>
is <code>character(0)</code> so the space complexity does not contain the term <code class="reqn">O(m)</code>.
</p>
<p>The <code>mapping</code> created by enabling <code>create.mapping</code> might miss the interaction term which
involves <code>split</code>ed features. For example, suppose there are two columns <code>a</code> and <code>b</code>
while the value are 1 and 1,2,3 respectively. The user marks the column <code>b</code> with 
<code>split</code>. If the hashed value of <code>b1</code> and <code>b2</code> are collided, then the interaction 
<code>a1:b1</code> will not appear in the returned mapping table. Because this package is originally 
designed for predictive analysis and the mapping should not play an 
important role of predictive analysis. If you have a test case and want to ask us to fix this, 
please provide us a test case in <a href="https://github.com/wush978/FeatureHashing/issues/67">https://github.com/wush978/FeatureHashing/issues/67</a>.
</p>


<h3>References</h3>

<p>H. B. McMahan, G. Holt, D. Sculley, et al. &quot;Ad click
prediction: a view from the trenches&quot;. In: _The 19th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining,
KDD 2013, Chicago, IL, USA, August 11-14, 2013_. Ed. by I. S.
Dhillon, Y. Koren, R. Ghani, T. E. Senator, P. Bradley, R. Parekh,
J. He, R. L. Grossman and R. Uthurusamy. ACM, 2013, pp. 1222-1230.
DOI: 10.1145/2487575.2488200. &lt;URL:
<a href="https://doi.acm.org/10.1145/2487575.2488200">https://doi.acm.org/10.1145/2487575.2488200</a>&gt;.
</p>
<p>Kilian Q. Weinberger, Anirban Dasgupta, John Langford, 
Alexander J. Smola, and Josh Attenberg. ICML, volume 382 of ACM 
International Conference Proceeding Series, page 140. ACM, (2009)
</p>
<p>W. Zhang, S. Yuan, J. Wang, et al. &quot;Real-Time Bidding
Benchmarking with iPinYou Dataset&quot;. In: _arXiv preprint
arXiv:1407.7073_ (2014).
</p>


<h3>Examples</h3>

<pre><code class='language-R'># The following scripts show how to fit a logistic regression 
# after feature hashing
## Not run: 
data(ipinyou)
f &lt;- ~ IP + Region + City + AdExchange + Domain +
 URL + AdSlotId + AdSlotWidth + AdSlotHeight +
 AdSlotVisibility + AdSlotFormat + CreativeID +
 Adid + split(UserTag, delim = ",")
# if the version of FeatureHashing is 0.8, please use the following command:
# m.train &lt;- as(hashed.model.matrix(f, ipinyou.train, 2^16, transpose = FALSE), "dgCMatrix")
m.train &lt;- hashed.model.matrix(f, ipinyou.train, 2^16)
m.test &lt;- hashed.model.matrix(f, ipinyou.test, 2^16)

# logistic regression with glmnet

library(glmnet)

cv.g.lr &lt;- cv.glmnet(m.train, ipinyou.train$IsClick,
 family = "binomial")#, type.measure = "auc")
p.lr &lt;- predict(cv.g.lr, m.test, s="lambda.min")
auc(ipinyou.test$IsClick, p.lr)

## Per-Coordinate FTRL-Proximal with $L_1$ and $L_2$ Regularization for Logistic Regression

# The following scripts use an implementation of the FTRL-Proximal for Logistic Regresion, 
# which is published in McMahan, Holt and Sculley et al. (2013), to predict the probability 
# (1-step prediction) and update the model simultaneously.


source(system.file("ftprl.R", package = "FeatureHashing"))
m.train &lt;- hashed.model.matrix(f, ipinyou.train, 2^16, transpose = TRUE)
ftprl &lt;- initialize.ftprl(0.1, 1, 0.1, 0.1, 2^16)
ftprl &lt;- update.ftprl(ftprl, m.train, ipinyou.train$IsClick, predict = TRUE)
auc(ipinyou.train$IsClick, attr(ftprl, "predict"))

# If we use the same algorithm to predict the click through rate of the 3rd season of iPinYou, 
# the overall AUC will be 0.77 which is comparable to the overall AUC of the 
# 3rd season 0.76 reported in Zhang, Yuan, Wang, et al. (2014).

## End(Not run)

# The following scripts show the implementation of the FeatureHashing.

# Below the original values will be project in a space of 2^6 dimensions
m &lt;- hashed.model.matrix(~ ., CO2, 2^6, create.mapping = TRUE, 
 transpose = TRUE, is.dgCMatrix = FALSE)
 
# Print the matrix via dgCMatrix
as(m, "dgCMatrix")

# Extraction of the dictionary: values with their hash
mapping &lt;- hash.mapping(m)

# To check the rate of collisions, we will extract the indices of the hash
# values through the modulo-division method, count how many duplicates 
# we have (in best case it should be zero) and perform a mean.
mean(duplicated(mapping))

# The type of the result produced by the function `hashed.model.matrix` 
# is a CSCMatrix. It supports simple subsetting 
# and matrix-vector multiplication
rnorm(2^6) %*% m

# Detail of the hashing
# To hash one specific value, we can use the `hashed.value` function
# Below we will apply this function to the feature names
vectHash &lt;- hashed.value(names(mapping))

# Now we will check that the result is the same than the one got with 
# the more generation `hashed.model.matrix` function.
# We will use the Modulo-division method (that's the [%% 2^6] below) 
# to find the address in hash table easily.
stopifnot(all(vectHash %% 2^6 + 1 == mapping))

# The sign is corrected by `hash.sign`
hash.sign(names(mapping))

## The interaction term is implemented as follow:
m2 &lt;- hashed.model.matrix(~ .^2, CO2, 2^6, create.mapping = TRUE, 
 transpose = TRUE, is.dgCMatrix = FALSE)
# The ^ operator indicates crossing to the specified degree. 
# For example (a+b+c)^2 is identical to (a+b+c)*(a+b+c) 
# which in turn expands to a formula containing the main effects
# for a, b and c together with their second-order interactions. 
     
# Extract the mapping
mapping2 &lt;- hash.mapping(m2)

# Get the hash of combination of two items, PlantQn2 and uptake 
mapping2["PlantQn2:uptake"] 

# Extract hash of each item
h1 &lt;- hashed.value("PlantQn2")
h2 &lt;- hashed.value("uptake")

# Computation of hash of both items combined
h3 &lt;- hashed.value(rawToChar(c(intToRaw(h1), intToRaw(h2)))) 
stopifnot(h3 %% 2^6 + 1 == mapping2["PlantQn2:uptake"])

# The concatenated feature, i.e. the array&lt;string&gt; type in hive
data(test.tag)
df &lt;- data.frame(a = test.tag, b = rnorm(length(test.tag)))
m &lt;- hashed.model.matrix(~ split(a, delim = ",", type = "existence"):b, df, 2^6,
 create.mapping = TRUE)
# The column `a` is splitted by "," and have an interaction with "b":
mapping &lt;- hash.mapping(m)
names(mapping)

</code></pre>

<hr>
<h2 id='intToRaw'>Convert the integer to raw vector with endian correction</h2><span id='topic+intToRaw'></span>

<h3>Description</h3>

<p>Convert the integer to raw vector with endian correction
</p>


<h3>Usage</h3>

<pre><code class='language-R'>intToRaw(src)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="intToRaw_+3A_src">src</code></td>
<td>
<p>integer value.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>raw vector with length 4
</p>

<hr>
<h2 id='ipinyou'>iPinYou Real-Time Bidding Dataset for Computational Advertising Research</h2><span id='topic+ipinyou'></span><span id='topic+ipinyou.train'></span><span id='topic+ipinyou.test'></span>

<h3>Description</h3>

<p>This is a sample from the iPinYou Real-Time Bidding dataset.
The data.frame named <code>ipinyou.train</code> is a sample from the data of 2013-10-19 and 
the data.frame named <code>ipinyou.test</code> is a sample from the  data of 2013-10-20.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(ipinyou)
</code></pre>


<h3>Format</h3>

<p>The column name of the data is the description of the data in Zhang, Yuan, Wang, et al. (2014). 
Most of the columns should be clearly described by their column names. 
For the details of the dataset, please read the Zhang, Yuan, Wang, et al. (2014).
</p>
<p><code>BidID</code>, the id of the RTB which is the unique identifier of the events.
</p>
<p><code>Adid</code>, the advertiser id.
</p>
<p><code>UserTag</code>, the user tags (segments) in iPinYou's proprietary audience database. 
This is also a real example of the concatenated feature.
</p>


<h3>Source</h3>

<p><a href="http://data.computational-advertising.org/">http://data.computational-advertising.org/</a>
</p>


<h3>References</h3>

<p>W. Zhang, S. Yuan, J. Wang, et al. 
&quot;Real-Time Bidding Benchmarking with iPinYou Dataset&quot;. 
In: arXiv preprint arXiv:1407.7073 (2014).
</p>

<hr>
<h2 id='simulate.split'>Simulate how <code>split</code> work in <code>hashed.model.matrix</code> to split the string into 
tokens</h2><span id='topic+simulate.split'></span>

<h3>Description</h3>

<p>Simulate how <code>split</code> work in <code>hashed.model.matrix</code> to split the string into 
tokens
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simulate.split(x, delim = ",", type = c("existence", "count"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simulate.split_+3A_x">x</code></td>
<td>
<p>character vector or factor. The source of concatenated feature.</p>
</td></tr>
<tr><td><code id="simulate.split_+3A_delim">delim</code></td>
<td>
<p>character value. The string to use for splitting.</p>
</td></tr>
<tr><td><code id="simulate.split_+3A_type">type</code></td>
<td>
<p>character value. Either &quot;<code>count</code>&quot; or &quot;<code>existence</code>&quot;. 
&quot;<code>count</code>&quot; indicates the number of occurrence of the token. 
&quot;<code>existence</code>&quot; indicates the boolean that whether the token exist or not.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>integer vector for <code>type = "count"</code> and logical vector for <code>type = "existence"</code>.
</p>

<hr>
<h2 id='test.tag'>test.tag</h2><span id='topic+test.tag'></span>

<h3>Description</h3>

<p>This is a vector to demo the concatenated feature.
</p>


<h3>Format</h3>

<p>For each element, the string represents the occurrence
of different tags. For example, the string &quot;1,27,19,25,tp,tw&quot;
of the first instance represents that the feature '1' is TRUE, the feature '27' is 
TRUE, et. al. On the contrary, the missing feature such as '2' 
is FALSE.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
