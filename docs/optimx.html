<!DOCTYPE html><html><head><title>Help for package optimx</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {optimx}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#optimx-package'>
<p>A replacement and extension of the optim() function, plus various</p>
optimization tools</a></li>
<li><a href='#axsearch'><p>Perform axial search around a supposed MINIMUM and provide diagnostics</p></a></li>
<li><a href='#bmchk'><p>Check bounds and masks for parameter constraints used in nonlinear optimization</p></a></li>
<li><a href='#bmstep'><p>Compute the maximum step along a search direction.</p></a></li>
<li><a href='#checksolver'><p>Test if requested solver is present</p></a></li>
<li><a href='#coef'><p>Summarize opm object</p></a></li>
<li><a href='#ctrldefault'><p>set control defaults</p></a></li>
<li><a href='#fnchk'><p>Run tests, where possible, on user objective function</p></a></li>
<li><a href='#gHgen'><p>Generate gradient and Hessian for a function at given parameters.</p></a></li>
<li><a href='#gHgenb'><p>Generate gradient and Hessian for a function at given parameters.</p></a></li>
<li><a href='#grback'><p>Backward difference numerical gradient approximation.</p></a></li>
<li><a href='#grcentral'><p>Central difference numerical gradient approximation.</p></a></li>
<li><a href='#grchk'><p>Run tests, where possible, on user objective function and (optionally) gradient and hessian</p></a></li>
<li><a href='#grfwd'><p>Forward difference numerical gradient approximation.</p></a></li>
<li><a href='#grnd'><p>A reorganization of the call to numDeriv grad() function.</p></a></li>
<li><a href='#grpracma'><p>A reorganization of the call to numDeriv grad() function.</p></a></li>
<li><a href='#hesschk'><p>Run tests, where possible, on user objective function and (optionally) gradient and hessian</p></a></li>
<li><a href='#hjn'><p>Compact R Implementation of Hooke and Jeeves Pattern Search Optimization</p></a></li>
<li><a href='#kktchk'><p>Check Kuhn Karush Tucker conditions for a supposed function minimum</p></a></li>
<li><a href='#multistart'><p>General-purpose optimization - multiple starts</p></a></li>
<li><a href='#opm'><p>General-purpose optimization</p></a></li>
<li><a href='#optchk'><p>General-purpose optimization</p></a></li>
<li><a href='#optimr'><p>General-purpose optimization</p></a></li>
<li><a href='#optimx'><p>General-purpose optimization</p></a></li>
<li><a href='#polyopt'><p>General-purpose optimization - sequential application of methods</p></a></li>
<li><a href='#proptimr'><p>Compact display of an <code>optimr()</code> result object</p></a></li>
<li><a href='#Rcgmin'><p>An R implementation of a nonlinear conjugate gradient algorithm with the Dai / Yuan</p>
update and restart. Based on Nash (1979) Algorithm 22 for its main structure.</a></li>
<li><a href='#Rcgminb'><p>An R implementation of a bounded nonlinear conjugate gradient algorithm</p>
with the Dai / Yuan update and restart. Based on Nash (1979) Algorithm 22
for its main structure. CALL THIS VIA <code>Rcgmin</code> AND DO NOT USE DIRECTLY.</a></li>
<li><a href='#Rcgminu'><p>An R implementation of an unconstrained nonlinear conjugate gradient algorithm</p>
with the Dai / Yuan update and restart. Based on Nash (1979) Algorithm 22
for its main structure.  CALL THIS VIA <code>Rcgmin</code> AND DO NOT USE DIRECTLY.</a></li>
<li><a href='#Rvmmin'><p>Variable metric nonlinear function minimization, driver.</p></a></li>
<li><a href='#Rvmminb'><p>Variable metric nonlinear function minimization with bounds constraints</p></a></li>
<li><a href='#Rvmminu'><p>Variable metric nonlinear function minimization, unconstrained</p></a></li>
<li><a href='#scalechk'><p>Check the scale of the initial parameters and bounds input to an optimization code</p>
used in nonlinear optimization</a></li>
<li><a href='#snewton'><p>Safeguarded Newton methods for function minimization using R functions.</p></a></li>
<li><a href='#summary.optimx'><p>Summarize optimx object</p></a></li>
<li><a href='#tn'><p>Truncated Newton minimization of an unconstrained function.</p></a></li>
<li><a href='#tnbc'><p>Truncated Newton function minimization with bounds constraints</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>2023-10.21</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-10-21</td>
</tr>
<tr>
<td>Title:</td>
<td>Expanded Replacement and Extension of the 'optim' Function</td>
</tr>
<tr>
<td>Author:</td>
<td>John C Nash [aut, cre],
  Ravi Varadhan [aut],
  Gabor Grothendieck [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>John C Nash &lt;nashjc@uottawa.ca&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides a replacement and extension of the optim()
    function to call to several function minimization codes in R in a single
    statement. These methods handle smooth, possibly box constrained functions 
    of several or many parameters. Note that function 'optimr()' was prepared to
    simplify the incorporation of minimization codes going forward. Also implements some
    utility codes and some extra solvers, including safeguarded Newton methods.
    Many methods previously separate are now included here.
    This is the version for CRAN.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>Yes</td>
</tr>
<tr>
<td>Imports:</td>
<td>numDeriv, nloptr, pracma</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, setRNG, BB, ucminf, minqa, dfoptim,
lbfgsb3c, lbfgs, subplex, marqLevAlg, testthat (&ge; 3.0.0),
R.rsp</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>R.rsp</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-10-24 12:58:46 UTC; john</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-10-24 13:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='optimx-package'>
A replacement and extension of the optim() function, plus various 
optimization tools
</h2><span id='topic+optimx-package'></span>

<h3>Description</h3>

<p><code><a href="#topic+optimx">optimx</a></code>
provides a replacement and extension of the <code>link{optim()}</code> function to 
unify and streamline optimization capabilities in R for smooth, possibly box
constrained functions of several or many parameters
</p>
<p>The three functions ufn, ugr and uhess wrap corresponding user functions fn, gr, and 
hess so that these functions can be executed safely (via try()) and also so parameter or
function scaling can be applied. The wrapper
functions also allow for maximization of functions (via minimization of the negative of
the function) using the logical parameter <code>maximize</code>.
</p>
<p>There are three test functions, fnchk, grchk, and hesschk, to allow the user 
function to be tested for validity and correctness. However, no set of tests is 
exhaustive, and extensions and improvements are welcome. The package 
<code>numDeriv</code> is used for generation of numerical approximations to 
derivatives.
</p>


<h3>Details</h3>

<p>Index:
</p>
<pre>
axsearch            Perform an axial search optimality check
bmchk               Check bounds and masks for parameter constraints
bmstep              Compute the maximum step along a search direction.
checksolver         Checks if method is available in allmeth
ctrldefault         Sets the default values of elements of the control() list
dispdefault         To display default control settings
fnchk               Test validity of user function
gHgen               Compute gradient and Hessian as a given
                    set of parameters
gHgenb              Compute gradient and Hessian as a given 
                    set of parameters appying bounds and masks
grback              Backward numerical gradient approximation
grcentral           Central numerical gradient approximation
grchk               Check that gradient function evaluation 
                    matches numerical gradient
grfwd               Forward numerical gradient approximation
grnd                Gradient approximation using \code{numDeriv}
grpracma            Gradient approximation using \code{pracma}
hesschk             Check that Hessian function evaluation
                    matches numerical approximation
hjn                 A didactic example code of the Hooke and Jeeves algorithm
kktchk              Check the Karush-Kuhn-Tucker optimality conditions
multistart          Try a single method with multiple starting parameter sets
ncg                 Revised CG solver
nvm                 Revised Variable Metric solver
opm                 Wrapper that allows multiple minimizers to be applied to a
                    given objective function 
optchk              Check supplied objective function
optimr              Wrapper that allows different (single) minimizers to be
                    applied to a given objective function using a common syntax
                    like that of optim()
optimx              Wrapper that allows multiple minimizers to be applied to a
                    given objective function. Complexity of the code maked this
                    function difficult to maintain, and opm() is the suggested
                    replacement, but optimx() is retained for backward 
                    compatibility.
optimx.check        a component of optimx()
optimx-package      a component of optimx()
optimx.run          a component of optimx()
optimx.setup        a component of optimx()
optsp               An environment to hold some globally useful items
                    used by optimization programs. Created on loading package
                    with zzz.R
polyopt             Allows sequential application of methods to a given problem.
proptimr            compact output of optimr() result object             
Rcgmin              Conjugate gradients minimization
Rcgminb             Bounds constrained conjugate gradients minimization
Rcgminu             Unconstrained conjugate gradients minimization
Rtnmin-package      Internal functions for the S.G.Nash truncated newton method 
Rvmmin              Variable metric minimization method
Rvmminb             Bounds constrained variable metric minimization method
Rvmminu             Unconstrained variable metric minimization method
scalechk            Check scale of initial parameters and bounds
snewtm              Demonstration Newton-Marquardt minimization method
snewton             Demonstration safeguarded Newton minimization method
snewtonmb           Bounds constrained safeguarded Newton method
tnbc                Bounds constrained truncated Newton method
tn                  Unconstrained truncated Newton method
</pre>


<h3>Author(s)</h3>

<p>John C Nash &lt;nashjc@uottawa.ca&gt; and Ravi Varadhan &lt;RVaradhan@jhmi.edu&gt;
</p>
<p>Maintainer: John C Nash &lt;nashjc@uottawa.ca&gt;
</p>


<h3>References</h3>

<p>Nash, John C. and Varadhan, Ravi (2011) Unifying Optimization Algorithms 
to Aid Software System Users: optimx for R, Journal of Statistical
Software, publication pending.
</p>

<hr>
<h2 id='axsearch'>Perform axial search around a supposed MINIMUM and provide diagnostics</h2><span id='topic+axsearch'></span>

<h3>Description</h3>

<p>Nonlinear optimization problems often terminate at points in the 
parameter space that are not satisfactory optima. This routine conducts an axial
search, stepping forward and backward along each parameter and computing the objective
function. This allows us to compute the <code>tilt</code> and <code>radius of curvature</code> or
<code>roc</code> along that parameter axis. 
</p>
<p><code>axsearch</code> assumes that one is MINIMIZING the function <code>fn</code>. If you are
working with a maximization, it is suggested that you write your own function that 
is to be minimized, that is, (-1)*(function to be maximized). All discussion here is in
terms of minimization.
</p>
<p>Axial search may find parameters with a function value lower than that at the 
supposed minimum, i.e., lower than <code>fmin</code>. 
</p>
<p>In this case <code>axsearch</code> exits immediately with the new function value and
parameters. This can be used to restart an optimizer, as in the optimx wrapper.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   axsearch(par, fn=NULL, fmin=NULL, lower=NULL, upper=NULL, bdmsk=NULL, 
              control=list(), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="axsearch_+3A_par">par</code></td>
<td>
<p>A numeric vector of values of the optimization function parameters that are
at a supposed minimum.</p>
</td></tr>
<tr><td><code id="axsearch_+3A_fn">fn</code></td>
<td>
<p>The user objective function</p>
</td></tr>
<tr><td><code id="axsearch_+3A_fmin">fmin</code></td>
<td>
<p>The presumed value of the objective function at the parameters <code>par</code>.
NOTE: This is NOT cheched. Caution!. However, if fmin==NULL on call, axsearch will
compute the value.</p>
</td></tr>
<tr><td><code id="axsearch_+3A_lower">lower</code></td>
<td>
<p>A vector of lower bounds on the parameters.</p>
</td></tr>
<tr><td><code id="axsearch_+3A_upper">upper</code></td>
<td>
<p>A vector of upper bounds on the parameters.</p>
</td></tr>
<tr><td><code id="axsearch_+3A_bdmsk">bdmsk</code></td>
<td>
<p>An indicator vector, having 1 for each parameter that is &quot;free&quot; or
unconstrained, and 0 for any parameter that is fixed or MASKED for the
duration of the optimization. Partly for historical reasons, we use the 
same array during the progress of optimization as an indicator that a 
parameter is at a lower bound (bdmsk element set to -3) or upper bound (-1).</p>
</td></tr>
<tr><td><code id="axsearch_+3A_control">control</code></td>
<td>
<p>Algorithm controls as per <code>ctrldefault</code>. See <code>details</code>.</p>
</td></tr>
<tr><td><code id="axsearch_+3A_...">...</code></td>
<td>
<p>Extra arguments for the user function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The axial search MAY give a lower function value, in which case, one can restart
an optimization. However, it is left to the user to do this.
Its primary use is in presenting some features of the function surface in the
tilt and radius of curvature measures returned. However, better measures should
be possible, and this function should be regarded as largely experimental.
</p>
<p>Note: As of December 2021, the calling syntax has changed from 
<code>
       axsearch(par, fn=NULL, fmin=NULL, lower=NULL, upper=NULL, bdmsk=NULL, trace=0, ...)
  </code>
</p>
<p>In case any user has code employing the older function, it is to be found in 
<code>inst/doc/replaced2021/axsearch2018.R</code>.
</p>
<p>The new syntax has <code>trace</code> replaced with <code>control=list{}</code>, where the defaults
are found from the function <code>ctrldefault()</code>. This routine uses three particular
elements:
</p>
<p><code>trace</code> is 0 if no intermediate output is desired, non-zero otherwise.
</p>
<p><code>bigval</code> is a large number used to provide a value for the objective function
when the parameters are inadmissible.
</p>
<p><code>reltest</code> is used to test for equality of small numbers by comparing their
sums with <code>reltest</code>.
</p>
<p><code>grtesttol</code> is a small quantity, but it is used when multiplied by <code>reltest</code>
to give <code>epst</code>, the axial step control. Each parameter is stepped by an amount
<code>epst*(abs(parameter_value)+epst)</code>. Note that the author has never found it 
necessary to adjust these values from the defaults generated by <code>ctrldefault()</code>.
</p>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>bestfn</code></td>
<td>
<p>The lowest (best) function value found during the axial search, 
else the original fmin value. (This is actively set in that case.)</p>
</td></tr>
<tr><td><code>par</code></td>
<td>
<p>The vector of parameters at the best function value. </p>
</td></tr>
<tr><td><code>details</code></td>
<td>
<p>A data frame reporting the original parameters, the forward step and backward
step function values, the size of the step taken for a particular parameter, the tilt and
the roc (radius of curvature). Some elements will be NA if we find a lower function
value during the axial search.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>#####################
# require(optimx)
# Simple bounds test for n=4
bt.f&lt;-function(x){
  sum(x*x)
}

bt.g&lt;-function(x){
  gg&lt;-2.0*x
}

n&lt;-4
lower&lt;-rep(0,n)
upper&lt;-lower # to get arrays set
bdmsk&lt;-rep(1,n)
# bdmsk[(trunc(n/2)+1)]&lt;-0
for (i in 1:n) { 
  lower[i]&lt;-1.0*(i-1)*(n-1)/n
  upper[i]&lt;-1.0*i*(n+1)/n
}
xx&lt;-0.5*(lower+upper)

cat("lower bounds:")
print(lower)
cat("start:       ")
print(xx)
cat("upper bounds:")
print(upper)

abtrvm &lt;- list() # ensure we have the structure

cat("Rvmmin \n\n")
# Note: trace set to 0 below. Change as needed to view progress. 

# Following can be executed if package optimx available
# abtrvm &lt;- optimr(xx, bt.f, bt.g, lower=lower, upper=upper, method="Rvmmin", 
#                 control=list(trace=0))
# Note: use lower=lower etc. because there is a missing hess= argument
# print(abtrvm)

abtrvm$par &lt;- c(0.00, 0.75, 1.50, 2.25)
abtrvm$value &lt;- 7.875
cat("Axial search")
axabtrvm &lt;- axsearch(abtrvm$par, fn=bt.f, fmin=abtrvm$value, lower, upper, bdmsk=NULL)
print(axabtrvm)

abtrvm1 &lt;- optimr(xx, bt.f, bt.g, lower=lower, upper=upper, method="Rvmmin", 
                   control=list(maxit=1, trace=0))
proptimr(abtrvm1)

abtrvm1$value &lt;- 8.884958
abtrvm1$par &lt;- c(0.625, 1.625, 2.625, 3.625)

cat("Axial search")
axabtrvm1 &lt;- axsearch(abtrvm1$par, fn=bt.f, fmin=abtrvm1$value, lower, upper, bdmsk=NULL)
print(axabtrvm1)

cat("Do NOT try axsearch() with maximize\n")

</code></pre>

<hr>
<h2 id='bmchk'>Check bounds and masks for parameter constraints used in nonlinear optimization</h2><span id='topic+bmchk'></span>

<h3>Description</h3>

<p>Nonlinear optimization problems often have explicit or implicit upper and
lower bounds on the parameters of the function to be miminized or maximized. These are 
called bounds or box constraints. Some of the parameters may be fixed for a given problem
or for a temporary trial. These fixed, or masked, paramters are held at one value during 
a specific 'run' of the optimization.
</p>
<p>It is possible that the bounds are inadmissible, that is, that at least one lower bound
exceeds an upper bound. In this case we set the flag <code>admissible</code> to FALSE.
</p>
<p>Parameters that are outside the bounds are moved to the nearest bound and the flag
<code>parchanged</code> is set TRUE. However, we DO NOT change masked parameters, and they
may be outside the bounds. This is an implementation choice, since it may be useful
to test objective functions at point outside the bounds.
</p>
<p>The package bmchk is essentially a test of the R function bmchk(), which is likely to be 
incorporated within optimization codes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   bmchk(par, lower=NULL, upper=NULL, bdmsk=NULL, trace=0, offset=100, shift2bound=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bmchk_+3A_par">par</code></td>
<td>
<p>A numeric vector of starting values of the optimization function parameters.</p>
</td></tr>
<tr><td><code id="bmchk_+3A_lower">lower</code></td>
<td>
<p>A vector of lower bounds on the parameters.</p>
</td></tr>
<tr><td><code id="bmchk_+3A_upper">upper</code></td>
<td>
<p>A vector of upper bounds on the parameters.</p>
</td></tr>
<tr><td><code id="bmchk_+3A_bdmsk">bdmsk</code></td>
<td>
<p>An indicator vector, having 1 for each parameter that is &quot;free&quot; or
unconstrained, and 0 for any parameter that is fixed or MASKED for the
duration of the optimization. Partly for historical reasons, we use the 
same array during the progress of optimization as an indicator that a 
parameter is at a lower bound (bdmsk element set to -3) or upper bound (-1).</p>
</td></tr>
<tr><td><code id="bmchk_+3A_trace">trace</code></td>
<td>
<p>An integer that controls whether diagnostic information is displayed.
A positive value displays information, 0 (default) does not.</p>
</td></tr>
<tr><td><code id="bmchk_+3A_offset">offset</code></td>
<td>
<p>If provided, is used to detect equality of numbers. That is, two 
values <code>a</code> and <code>b</code> are taken as equal if <code>a + offset</code> is 
equal to <code>b + offset</code>. Default value is 100. Note that in previous versions
of this code a tolerance <code>tol</code> was used.</p>
</td></tr>
<tr><td><code id="bmchk_+3A_shift2bound">shift2bound</code></td>
<td>
<p>If TRUE, non-masked paramters outside bounds are adjusted 
to the nearest bound. We then set parchanged = TRUE which implies the 
original parameters were infeasible.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The bmchk function will check that the bounds exist and are admissible, 
that is, that there are no lower bounds that exceed upper bounds. 
</p>
<p>There is a check if lower and upper bounds are very close together, in 
which case a mask is imposed and maskadded is set TRUE. NOTE: it is 
generally a VERY BAD IDEA to have bounds close together in optimization,
but here we use a tolerance based on the double precision machine 
epsilon. Thus it is not a good idea to rely on bmchk() to test if 
bounds constraints are well-posed.
</p>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>bvec</code></td>
<td>
<p>The vector of parameters, possibly adjusted to bounds. Parameters 
outside bounds are adjusted to the nearest bound. We let <code>n</code> be the
length of this vector.</p>
</td></tr>
<tr><td><code>bdmsk</code></td>
<td>
<p>adjusted input masks</p>
</td></tr>
<tr><td><code>bchar</code></td>
<td>
<p>a set of indicators that give information about the parameters, that is,
if they are out-of-bounds-low (&quot;-&quot;), at a lower bound (&quot;L&quot;), 
free (&quot;F&quot;), at an upper bound (&quot;U&quot;), out-of-bounds-high (&quot;+&quot;), 
masked (fixed) (&quot;M&quot;), of unknown characteristics (&quot;?&quot;&quot;), or 
inadmissible (&quot;!&quot;&quot;).</p>
</td></tr>
<tr><td><code>lower</code></td>
<td>
<p>(adjusted) lower bounds. If there are no lower bounds, a vector
of values equal to -Inf is substituted. Similarly, a single value is
expanded to a complete vector.
If any upper and lower bounds are equal (as mediated by <code>offset</code>),
we create a mask.</p>
</td></tr>
<tr><td><code>upper</code></td>
<td>
<p>(adjusted) upper bounds</p>
</td></tr>
<tr><td><code>nolower</code></td>
<td>
<p>TRUE if no lower bounds, FALSE otherwise</p>
</td></tr>
<tr><td><code>noupper</code></td>
<td>
<p>TRUE if no upper bounds, FALSE otherwise</p>
</td></tr>
<tr><td><code>bounds</code></td>
<td>
<p>TRUE if there are any bounds, FALSE otherwise</p>
</td></tr>
<tr><td><code>admissible</code></td>
<td>
<p>TRUE if bounds are admissible, FALSE otherwise
This means no lower bound exceeds an upper bound. That is the bounds 
themselves are sensible. This condition has nothing to do with the 
starting parameters.</p>
</td></tr>
<tr><td><code>maskadded</code></td>
<td>
<p>TRUE when a mask has been added because bounds are very close
or equal, FALSE otherwise. See the code for the implementation.</p>
</td></tr>
<tr><td><code>parchanged</code></td>
<td>
<p>TRUE if parameters are changed by bounds, FALSE otherswise.
Note that parchanged = TRUE implies the input parameter values were infeasible, 
that is, violated the bounds constraints.</p>
</td></tr>
<tr><td><code>feasible</code></td>
<td>
<p>TRUE if parameters are within or on bounds, FALSE otherswise.</p>
</td></tr>
<tr><td><code>onbound</code></td>
<td>
<p>TRUE if any parameter is on a bound, FALSE otherswise.
Note that parchanged = TRUE implies onbound = TRUE, but this is not used inside
the function. This output value may be important, for example, in using the
optimization function <code>nmkb</code> from package <code>dfoptim</code>.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>#####################

## cat("25-dimensional box constrained function\n")
## flb &lt;- function(x)
##     { p &lt;- length(x); sum(c(1, rep(4, p-1)) * (x - c(1, x[-p])^2)^2) }

start&lt;-rep(2, 25)
cat("\n start:")
print(start)
lo&lt;-rep(2,25)
cat("\n lo:")
print(lo)
hi&lt;-rep(4,25)
cat("\n hi:")
print(hi)
bt&lt;-bmchk(start, lower=lo, upper=hi, trace=1)
print(bt)

</code></pre>

<hr>
<h2 id='bmstep'>Compute the maximum step along a search direction.</h2><span id='topic+bmstep'></span>

<h3>Description</h3>

<p>Nonlinear optimization problems often have explicit or implicit upper and
lower bounds on the parameters of the function to be miminized or maximized. These are 
called bounds or box constraints. Some of the parameters may be fixed for a given problem
or for a temporary trial. These fixed, or masked, paramters are held at one value during 
a specific 'run' of the optimization.
</p>
<p>The bmstep() function computes the maximum step possible (which could be infinite) 
along a particular search direction from current parameters to bounds.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   bmstep(par, srchdirn, lower=NULL, upper=NULL, bdmsk=NULL, trace=0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bmstep_+3A_par">par</code></td>
<td>
<p>A numeric vector of starting values of the optimization function parameters.</p>
</td></tr>
<tr><td><code id="bmstep_+3A_srchdirn">srchdirn</code></td>
<td>
<p>A numeric vector giving the search direction.</p>
</td></tr>
<tr><td><code id="bmstep_+3A_lower">lower</code></td>
<td>
<p>A vector of lower bounds on the parameters.</p>
</td></tr>
<tr><td><code id="bmstep_+3A_upper">upper</code></td>
<td>
<p>A vector of upper bounds on the parameters.</p>
</td></tr>
<tr><td><code id="bmstep_+3A_bdmsk">bdmsk</code></td>
<td>
<p>An indicator vector, having 1 for each parameter that is &quot;free&quot; or
unconstrained, and 0 for any parameter that is fixed or MASKED for the
duration of the optimization. Partly for historical reasons, we use the 
same array during the progress of optimization as an indicator that a 
parameter is at a lower bound (bdmsk element set to -3) or upper bound (-1).</p>
</td></tr>
<tr><td><code id="bmstep_+3A_trace">trace</code></td>
<td>
<p>An integer that controls whether diagnostic information is displayed.
A positive value displays information, 0 (default) does not.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The bmstep function will compute and return (as a double or Inf) the 
maximum step to the bounds.
</p>


<h3>Value</h3>

<p>A double precision value or Inf giving the 
maximum step to the bounds.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#####################
xx &lt;- c(1, 1)
lo &lt;- c(0, 0)
up &lt;- c(100, 40)
sdir &lt;- c(4,1)
bm &lt;- c(1,1) # both free
ans &lt;- bmstep(xx, sdir, lo, up, bm, trace=1)
# stepsize
print(ans)
# distance
print(ans*sdir)
# New parameters
print(xx+ans*sdir)

</code></pre>

<hr>
<h2 id='checksolver'>Test if requested solver is present</h2><span id='topic+checksolver'></span><span id='topic+checkallsolvers'></span>

<h3>Description</h3>

<p>Test if requested solver is present.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   checksolver(method, allmeth, allpkg)
   checkallsolvers()
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="checksolver_+3A_method">method</code></td>
<td>
<p>Character string giving the name of the solver requested.</p>
</td></tr>
<tr><td><code id="checksolver_+3A_allmeth">allmeth</code></td>
<td>
<p>Character vector giving the names of the methods optimr can use.</p>
</td></tr>
<tr><td><code id="checksolver_+3A_allpkg">allpkg</code></td>
<td>
<p>Character vector giving the names of the packages where
solvers are found.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If the solver defined by character string in <code>method</code> is
available, then <code>checksolver</code> returns this string, else NULL.
</p>
<p><code>checkallsolvers()</code> returns a vector of strings that are the
names of missing solvers, else NULL if all solvers specified in <code>allmeth</code>
are present where <code>allmeth</code> is returned from a call to <code>ctrldefault(n)</code> 
where <code>n</code> is some integer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>   allmeth &lt;- c("Rvmmin", "nlminb","ipopttest")
   allpkg &lt;- c("Rvmmin", "stats","ipoptr")
   
   print(checksolver("nlminb", allmeth, allpkg))
   # If Rvmmin NOT available, get msg that PACKAGE not available.
   print(checksolver("Rvmmin", allmeth, allpkg))
   # Get message that SOLVER not found
   print(checksolver("notasolver", allmeth, allpkg))

</code></pre>

<hr>
<h2 id='coef'>Summarize opm object</h2><span id='topic+coef+3C-'></span><span id='topic+coef.opm'></span><span id='topic+coef+3C-.opm'></span><span id='topic+coef.optimx'></span><span id='topic+coef+3C-.optimx'></span>

<h3>Description</h3>

<p>Summarize an <code>"opm"</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  ## S3 method for class 'opm'
coef(object, ...) 
  ## S3 replacement method for class 'opm'
coef(x) &lt;- value 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef_+3A_object">object</code></td>
<td>
<p>Object returned by <code>opm</code>.</p>
</td></tr>
<tr><td><code id="coef_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to the function. Currently not
used.</p>
</td></tr>
<tr><td><code id="coef_+3A_x">x</code></td>
<td>
<p>An <code>opm</code> object.</p>
</td></tr>
<tr><td><code id="coef_+3A_value">value</code></td>
<td>
<p>Set parameters equal to this value.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>coef.opm</code> returns the best parameters found by each method that
returned such parameters. The returned coefficients are in the form of a
matrix with the rows named by the relevant methods and the columns named
according to parameter names provided by the user in the vector of starting
values, or else by &quot;p1&quot;, &quot;p2&quot;, ..., if names are not provided.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ans &lt;- opm(fn = function(x) sum(x*x), par = 1:2, method="ALL", control=list(trace=0))
print(coef(ans))

ansx &lt;- optimx(fn = function(x) sum(x*x), par = 1:2, control=list(all.methods=TRUE, trace=0))
print(coef(ansx))


## Not run: 
proj &lt;- function(x) x/sum(x)
f &lt;- function(x) -prod(proj(x))
ans &lt;- opm(1:2, f)
print(ans)
coef(ans) &lt;- apply(coef(ans), 1, proj)
print(ans)

## End(Not run)

</code></pre>

<hr>
<h2 id='ctrldefault'>set control defaults</h2><span id='topic+ctrldefault'></span><span id='topic+dispdefault'></span>

<h3>Description</h3>

<p>Set control defaults.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   ctrldefault(npar)

   dispdefault(ctrl)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ctrldefault_+3A_npar">npar</code></td>
<td>
<p>Number of parameters to optimize.</p>
</td></tr>
<tr><td><code id="ctrldefault_+3A_ctrl">ctrl</code></td>
<td>
<p>A list (likely generated by 'ctrldefault') of default settings to 'optimx'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ctrldefault</code> returns the default control settings for optimization tools.
</p>
<p><code>dispdefault</code> provides a compact display of the contents of a control settings list.
</p>

<hr>
<h2 id='fnchk'>Run tests, where possible, on user objective function</h2><span id='topic+fnchk'></span>

<h3>Description</h3>

<p><code>fnchk</code> checks a user-provided R function, <code>ffn</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   fnchk(xpar, ffn, trace=0, ... )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fnchk_+3A_xpar">xpar</code></td>
<td>

<p>the (double) vector of parameters to the objective funcion
</p>
</td></tr>
<tr><td><code id="fnchk_+3A_ffn">ffn</code></td>
<td>

<p>a user-provided function to compute the objective function
</p>
</td></tr>
<tr><td><code id="fnchk_+3A_trace">trace</code></td>
<td>

<p>set &gt;0 to provide output from fnchk to the console, 0 otherwise
</p>
</td></tr>
<tr><td><code id="fnchk_+3A_...">...</code></td>
<td>

<p>optional arguments passed to the objective function.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>fnchk</code> attempts to discover various errors in function setup in user-supplied
functions primarily intended for use in optimization calculations. There are always 
more conditions that could be tested!    
</p>


<h3>Value</h3>

<p>The output is a list consisting of 
list(fval=fval, infeasible=infeasible, excode=excode, msg=msg)
</p>
<table>
<tr><td><code>fval</code></td>
<td>
<p>The calculated value of the function at parameters <code>xpar</code> if the function
can be evaluated.</p>
</td></tr>
<tr><td><code>infeasible</code></td>
<td>
<p>FALSE if the function can be evaluated, TRUE if not.</p>
</td></tr>
<tr><td><code>excode</code></td>
<td>
<p>An exit code, which has a relationship to </p>
</td></tr>
<tr><td><code>msg</code></td>
<td>
<p>A text string giving information about the result of the function check: Messages and
the corresponding values of <code>excode</code> are:
</p>

<dl>
<dt>fnchk OK;</dt><dd> <p><code>excode</code> = 0; 
<code>infeasible</code> = FALSE</p>
</dd>
<dt>Function returns INADMISSIBLE;</dt><dd>
<p><code>excode</code> = -1; <code>infeasible</code> = TRUE</p>
</dd>
<dt>Function returns a vector not a scalar;</dt><dd>
<p><code>excode</code> = -4; <code>infeasible</code> = TRUE</p>
</dd>
<dt>Function returns a list not a scalar;</dt><dd>
<p><code>excode</code> = -4; <code>infeasible</code> = TRUE</p>
</dd>
<dt>Function returns a matrix list not a scalar;</dt><dd>
<p><code>excode</code> = -4; <code>infeasible</code> = TRUE</p>
</dd>
<dt>Function returns an array not a scalar;</dt><dd>
<p><code>excode</code> = -4; <code>infeasible</code> = TRUE</p>
</dd>
<dt>Function returned not length 1, despite not vector, matrix or array;</dt><dd>
<p><code>excode</code> = -4; <code>infeasible</code> = TRUE</p>
</dd>
<dt>Function returned non-numeric value; <code>excode</code> = 0;</dt><dd>
<p><code>excode</code> = -1; <code>infeasible</code> = TRUE</p>
</dd>
<dt>Function returned Inf or NA (non-computable);</dt><dd>
<p><code>excode</code> = -1; <code>infeasible</code> = TRUE</p>
</dd>
</dl>

</td></tr>
</table>


<h3>Author(s)</h3>

<p>John C. Nash &lt;nashjc@uottawa.ca&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Want to illustrate each case.
# Ben Bolker idea for a function that is NOT scalar
# rm(list=ls())
# library(optimx)
sessionInfo()
benbad&lt;-function(x, y){
  # y may be provided with different structures
  f&lt;-(x-y)^2
} # very simple, but ...

y&lt;-1:10
x&lt;-c(1)
cat("fc01: test benbad() with y=1:10, x=c(1)\n")
fc01&lt;-fnchk(x, benbad, trace=4, y)
print(fc01)

y&lt;-as.vector(y)
cat("fc02: test benbad() with y=as.vector(1:10), x=c(1)\n")
fc02&lt;-fnchk(x, benbad, trace=1, y)
print(fc02)

y&lt;-as.matrix(y)
cat("fc03: test benbad() with y=as.matrix(1:10), x=c(1)\n")
fc03&lt;-fnchk(x, benbad, trace=1, y)
print(fc03)

y&lt;-as.array(y)
cat("fc04: test benbad() with y=as.array(1:10), x=c(1)\n")
fc04&lt;-fnchk(x, benbad, trace=1, y)
print(fc04)

y&lt;-"This is a string"
cat("test benbad() with y a string, x=c(1)\n")
fc05&lt;-fnchk(x, benbad, trace=1, y)
print(fc05)

cat("fnchk with Rosenbrock\n")
fr &lt;- function(x) {   ## Rosenbrock Banana function
  x1 &lt;- x[1]
  x2 &lt;- x[2]
  100 * (x2 - x1 * x1)^2 + (1 - x1)^2
}
xtrad&lt;-c(-1.2,1)
ros1&lt;-fnchk(xtrad, fr, trace=1)
print(ros1)
npar&lt;-2
opros&lt;-list2env(list(fn=fr, gr=NULL, hess=NULL, MAXIMIZE=FALSE, PARSCALE=rep(1,npar), FNSCALE=1,
                     KFN=0, KGR=0, KHESS=0, dots=NULL))
uros1&lt;-fnchk(xtrad, fr, trace=1)
print(uros1)


</code></pre>

<hr>
<h2 id='gHgen'>Generate gradient and Hessian for a function at given parameters.</h2><span id='topic+gHgen'></span>

<h3>Description</h3>

<p><code>gHgen</code> is used to generate the gradient and Hessian of an objective
function used for optimization. If a user-provided gradient function 
<code>gr</code> is available it is used to compute the gradient, otherwise 
package <code>numDeriv</code> is used. If a user-provided Hessian function
<code>hess</code> is available, it is used to compute a Hessian. Otherwise, if
<code>gr</code> is available, we use the function <code>jacobian()</code> from
package <code>numDeriv</code> to compute the Hessian. In both these cases we
check for symmetry of the Hessian. Computational Hessians are commonly
NOT symmetric. If only the objective function <code>fn</code> is provided, then
the Hessian is approximated with the function <code>hessian</code> from 
package <code>numDeriv</code> which guarantees a symmetric matrix. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  gHgen(par, fn, gr=NULL, hess=NULL,
      control=list(ktrace=0), ...) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gHgen_+3A_par">par</code></td>
<td>
<p>Set of parameters, assumed to be at a minimum of the function <code>fn</code>.</p>
</td></tr>
<tr><td><code id="gHgen_+3A_fn">fn</code></td>
<td>
<p>Name of the objective function.</p>
</td></tr>
<tr><td><code id="gHgen_+3A_gr">gr</code></td>
<td>
<p>(Optional) function to compute the gradient of the objective function. If present,
we use the Jacobian of the gradient as the Hessian and avoid one layer of numerical
approximation to the Hessian.</p>
</td></tr> 
<tr><td><code id="gHgen_+3A_hess">hess</code></td>
<td>
<p>(Optional) function to compute the Hessian of the objective function. This
is rarely available, but is included for completeness.</p>
</td></tr>
<tr><td><code id="gHgen_+3A_control">control</code></td>
<td>
<p>A list of controls to the function. Currently 
asymptol (default of 1.0e-7 which tests for asymmetry of Hessian approximation
(see code for details of the test); 
ktrace, a logical flag which, if TRUE, monitors the progress 
of gHgen (default FALSE), and 
stoponerror, defaulting to FALSE to NOT stop when there is
an error or asymmetry of Hessian. Set TRUE to stop.</p>
</td></tr>
<tr><td><code id="gHgen_+3A_...">...</code></td>
<td>
<p>Extra data needed to compute the function, gradient and Hessian.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>None
</p>


<h3>Value</h3>

<p><code>ansout</code> a list of four items, 
</p>

<dl>
<dt><code>gn</code></dt><dd><p>  The approximation to the gradient vector.</p>
</dd>
<dt><code>Hn</code></dt><dd><p>  The approximation to the Hessian matrix.</p>
</dd>
<dt><code>gradOK</code></dt><dd><p>  TRUE if the gradient has been computed acceptably. FALSE otherwise.</p>
</dd>
<dt><code>hessOK</code></dt><dd><p>  TRUE if the gradient has been computed acceptably and passes the
symmetry test. FALSE otherwise.</p>
</dd>
<dt><code>nbm</code></dt><dd><p>  Always 0. The number of active bounds and masks.
Present to make function consistent with <code>gHgenb</code>.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'># genrose function code
genrose.f&lt;- function(x, gs=NULL){ # objective function
## One generalization of the Rosenbrock banana valley function (n parameters)
	n &lt;- length(x)
        if(is.null(gs)) { gs=100.0 }
	fval&lt;-1.0 + sum (gs*(x[1:(n-1)]^2 - x[2:n])^2 + (x[2:n] - 1)^2)
        return(fval)
}

genrose.g &lt;- function(x, gs=NULL){
# vectorized gradient for genrose.f
# Ravi Varadhan 2009-04-03
	n &lt;- length(x)
        if(is.null(gs)) { gs=100.0 }
	gg &lt;- as.vector(rep(0, n))
	tn &lt;- 2:n
	tn1 &lt;- tn - 1
	z1 &lt;- x[tn] - x[tn1]^2
	z2 &lt;- 1 - x[tn]
	gg[tn] &lt;- 2 * (gs * z1 - z2)
	gg[tn1] &lt;- gg[tn1] - 4 * gs * x[tn1] * z1
	return(gg)
}

genrose.h &lt;- function(x, gs=NULL) { ## compute Hessian
   if(is.null(gs)) { gs=100.0 }
	n &lt;- length(x)
	hh&lt;-matrix(rep(0, n*n),n,n)
	for (i in 2:n) {
		z1&lt;-x[i]-x[i-1]*x[i-1]
#		z2&lt;-1.0-x[i]
                hh[i,i]&lt;-hh[i,i]+2.0*(gs+1.0)
                hh[i-1,i-1]&lt;-hh[i-1,i-1]-4.0*gs*z1-4.0*gs*x[i-1]*(-2.0*x[i-1])
                hh[i,i-1]&lt;-hh[i,i-1]-4.0*gs*x[i-1]
                hh[i-1,i]&lt;-hh[i-1,i]-4.0*gs*x[i-1]
	}
        return(hh)
}

trad&lt;-c(-1.2,1)
ans100fgh&lt;-  gHgen(trad, genrose.f, gr=genrose.g, hess=genrose.h,
      control=list(ktrace=1)) 
print(ans100fgh)
ans100fg&lt;-  gHgen(trad, genrose.f, gr=genrose.g, 
      control=list(ktrace=1)) 
print(ans100fg)
ans100f&lt;-  gHgen(trad, genrose.f, control=list(ktrace=1)) 
print(ans100f)
ans10fgh&lt;-   gHgen(trad, genrose.f, gr=genrose.g, hess=genrose.h,
      control=list(ktrace=1), gs=10) 
print(ans10fgh)
ans10fg&lt;-   gHgen(trad, genrose.f, gr=genrose.g, 
      control=list(ktrace=1), gs=10) 
print(ans10fg)
ans10f&lt;-   gHgen(trad, genrose.f, control=list(ktrace=1), gs=10) 
print(ans10f)

</code></pre>

<hr>
<h2 id='gHgenb'>Generate gradient and Hessian for a function at given parameters.</h2><span id='topic+gHgenb'></span>

<h3>Description</h3>

<p><code>gHgenb</code> is used to generate the gradient and Hessian of an objective
function used for optimization. If a user-provided gradient function 
<code>gr</code> is available it is used to compute the gradient, otherwise 
package <code>numDeriv</code> is used. If a user-provided Hessian function
<code>hess</code> is available, it is used to compute a Hessian. Otherwise, if
<code>gr</code> is available, we use the function <code>jacobian()</code> from
package <code>numDeriv</code> to compute the Hessian. In both these cases we
check for symmetry of the Hessian. Computational Hessians are commonly
NOT symmetric. If only the objective function <code>fn</code> is provided, then
the Hessian is approximated with the function <code>hessian</code> from 
package <code>numDeriv</code> which guarantees a symmetric matrix. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  gHgenb(par, fn, gr=NULL, hess=NULL, bdmsk=NULL, lower=NULL, upper=NULL,
      control=list(ktrace=0), ...) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gHgenb_+3A_par">par</code></td>
<td>
<p>Set of parameters, assumed to be at a minimum of the function <code>fn</code>.</p>
</td></tr>
<tr><td><code id="gHgenb_+3A_fn">fn</code></td>
<td>
<p>Name of the objective function.</p>
</td></tr>
<tr><td><code id="gHgenb_+3A_gr">gr</code></td>
<td>
<p>(Optional) function to compute the gradient of the objective function. If present,
we use the Jacobian of the gradient as the Hessian and avoid one layer of numerical
approximation to the Hessian.</p>
</td></tr> 
<tr><td><code id="gHgenb_+3A_hess">hess</code></td>
<td>
<p>(Optional) function to compute the Hessian of the objective function. This
is rarely available, but is included for completeness.</p>
</td></tr>
<tr><td><code id="gHgenb_+3A_bdmsk">bdmsk</code></td>
<td>
<p>An integer vector of the same length as <code>par</code>. When an element
of this vector is 0, the corresponding parameter value is fixed (masked) 
during an optimization. Non-zero values indicate a parameter is free (1),
at a lower bound (-3) or at an upper bound (-1), but this routine only
uses 0 values.</p>
</td></tr>
<tr><td><code id="gHgenb_+3A_lower">lower</code></td>
<td>
<p>Lower bounds for parameters in <code>par</code>.</p>
</td></tr>
<tr><td><code id="gHgenb_+3A_upper">upper</code></td>
<td>
<p>Upper bounds for parameters in <code>par</code>.</p>
</td></tr>
<tr><td><code id="gHgenb_+3A_control">control</code></td>
<td>
<p>A list of controls to the function. Currently 
asymptol (default of 1.0e-7 which tests for asymmetry of Hessian approximation
(see code for details of the test); 
ktrace, a logical flag which, if TRUE, monitors the progress 
of gHgenb (default FALSE), and 
stoponerror, defaulting to FALSE to NOT stop when there is
an error or asymmetry of Hessian. Set TRUE to stop.</p>
</td></tr>
<tr><td><code id="gHgenb_+3A_...">...</code></td>
<td>
<p>Extra data needed to compute the function, gradient and Hessian.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>None
</p>


<h3>Value</h3>

<p><code>ansout</code> a list of four items, 
</p>

<dl>
<dt><code>gn</code></dt><dd><p>  The approximation to the gradient vector.</p>
</dd>
<dt><code>Hn</code></dt><dd><p>  The approximation to the Hessian matrix.</p>
</dd>
<dt><code>gradOK</code></dt><dd><p>  TRUE if the gradient has been computed acceptably. FALSE otherwise.</p>
</dd>
<dt><code>hessOK</code></dt><dd><p>  TRUE if the gradient has been computed acceptably and passes the
symmetry test. FALSE otherwise.</p>
</dd>
<dt><code>nbm</code></dt><dd><p>  The number of active bounds and masks.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>require(numDeriv)
# genrose function code
genrose.f&lt;- function(x, gs=NULL){ # objective function
## One generalization of the Rosenbrock banana valley function (n parameters)
	n &lt;- length(x)
        if(is.null(gs)) { gs=100.0 }
	fval&lt;-1.0 + sum (gs*(x[1:(n-1)]^2 - x[2:n])^2 + (x[2:n] - 1)^2)
        return(fval)
}

genrose.g &lt;- function(x, gs=NULL){
# vectorized gradient for genrose.f
# Ravi Varadhan 2009-04-03
	n &lt;- length(x)
        if(is.null(gs)) { gs=100.0 }
	gg &lt;- as.vector(rep(0, n))
	tn &lt;- 2:n
	tn1 &lt;- tn - 1
	z1 &lt;- x[tn] - x[tn1]^2
	z2 &lt;- 1 - x[tn]
	gg[tn] &lt;- 2 * (gs * z1 - z2)
	gg[tn1] &lt;- gg[tn1] - 4 * gs * x[tn1] * z1
	return(gg)
}

genrose.h &lt;- function(x, gs=NULL) { ## compute Hessian
   if(is.null(gs)) { gs=100.0 }
	n &lt;- length(x)
	hh&lt;-matrix(rep(0, n*n),n,n)
	for (i in 2:n) {
		z1&lt;-x[i]-x[i-1]*x[i-1]
		z2&lt;-1.0-x[i]
                hh[i,i]&lt;-hh[i,i]+2.0*(gs+1.0)
                hh[i-1,i-1]&lt;-hh[i-1,i-1]-4.0*gs*z1-4.0*gs*x[i-1]*(-2.0*x[i-1])
                hh[i,i-1]&lt;-hh[i,i-1]-4.0*gs*x[i-1]
                hh[i-1,i]&lt;-hh[i-1,i]-4.0*gs*x[i-1]
	}
        return(hh)
}


maxfn&lt;-function(x, top=10) {
      	n&lt;-length(x)
	ss&lt;-seq(1,n)
	f&lt;-top-(crossprod(x-ss))^2
	f&lt;-as.numeric(f)
	return(f)
}

negmaxfn&lt;-function(x) {
	f&lt;-(-1)*maxfn(x)
	return(f)
}

parx&lt;-rep(1,4)
lower&lt;-rep(-10,4)
upper&lt;-rep(10,4)
bdmsk&lt;-c(1,1,0,1) # masked parameter 3
fval&lt;-genrose.f(parx)
gval&lt;-genrose.g(parx)
Ahess&lt;-genrose.h(parx)
gennog&lt;-gHgenb(parx,genrose.f)
cat("results of gHgenb for genrose without gradient code at ")
print(parx)
print(gennog)
cat("compare to g =")
print(gval)
cat("and Hess\n")
print(Ahess)
cat("\n\n")
geng&lt;-gHgenb(parx,genrose.f,genrose.g)
cat("results of gHgenb for genrose at ")
print(parx)
print(gennog)
cat("compare to g =")
print(gval)
cat("and Hess\n")
print(Ahess)
cat("*****************************************\n")
parx&lt;-rep(0.9,4)
fval&lt;-genrose.f(parx)
gval&lt;-genrose.g(parx)
Ahess&lt;-genrose.h(parx)
gennog&lt;-gHgenb(parx,genrose.f,control=list(ktrace=TRUE), gs=9.4)
cat("results of gHgenb with gs=",9.4," for genrose without gradient code at ")
print(parx)
print(gennog)
cat("compare to g =")
print(gval)
cat("and Hess\n")
print(Ahess)
cat("\n\n")
geng&lt;-gHgenb(parx,genrose.f,genrose.g, control=list(ktrace=TRUE))
cat("results of gHgenb for genrose at ")
print(parx)
print(gennog)
cat("compare to g =")
print(gval)
cat("and Hess\n")
print(Ahess)
gst&lt;-5
cat("\n\nTest with full calling sequence and gs=",gst,"\n")
gengall&lt;-gHgenb(parx,genrose.f,genrose.g,genrose.h, control=list(ktrace=TRUE),gs=gst)
print(gengall)


top&lt;-25
x0&lt;-rep(2,4)
cat("\n\nTest for maximization and top=",top,"\n")
cat("Gradient and Hessian will have sign inverted")
maxt&lt;-gHgen(x0, maxfn, control=list(ktrace=TRUE), top=top)
print(maxt)

cat("test against negmaxfn\n")
gneg &lt;- grad(negmaxfn, x0)
Hneg&lt;-hessian(negmaxfn, x0)
# gdiff&lt;-max(abs(gneg-maxt$gn))/max(abs(maxt$gn))
# Hdiff&lt;-max(abs(Hneg-maxt$Hn))/max(abs(maxt$Hn))
# explicitly change sign 
gdiff&lt;-max(abs(gneg-(-1)*maxt$gn))/max(abs(maxt$gn))
Hdiff&lt;-max(abs(Hneg-(-1)*maxt$Hn))/max(abs(maxt$Hn))
cat("gdiff = ",gdiff,"  Hdiff=",Hdiff,"\n")



</code></pre>

<hr>
<h2 id='grback'>Backward difference numerical gradient approximation.</h2><span id='topic+grback'></span>

<h3>Description</h3>

<p><code>grback</code> computes the backward difference approximation to the gradient of 
user function <code>userfn</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   grback(par, userfn, fbase=NULL, env=optsp, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="grback_+3A_par">par</code></td>
<td>

<p>parameters to the user objective function userfn
</p>
</td></tr>
<tr><td><code id="grback_+3A_userfn">userfn</code></td>
<td>

<p>User-supplied objective function
</p>
</td></tr>
<tr><td><code id="grback_+3A_fbase">fbase</code></td>
<td>

<p>The value of the function at the parameters, else NULL. This is to save
recomputing the function at this point.
</p>
</td></tr>
<tr><td><code id="grback_+3A_env">env</code></td>
<td>

<p>Environment for scratchpad items (like <code>deps</code> for approximation 
control in this routine). Default <code>optsp</code>.
</p>
</td></tr>
<tr><td><code id="grback_+3A_...">...</code></td>
<td>

<p>optional arguments passed to the objective function.
</p>
</td></tr>
</table>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> grback</td>
</tr>
<tr>
 <td style="text-align: left;">
Depends: </td><td style="text-align: left;"> R (&gt;= 2.6.1)</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL Version 2.</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>
  


<h3>Value</h3>

<p><code>grback</code> returns a single vector object <code>df</code> which approximates the 
gradient of userfn at the parameters par. The approximation is controlled by a
global value <code>optderiveps</code> that is set when the package is attached.
</p>


<h3>Author(s)</h3>

<p>John C. Nash
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cat("Example of use of grback\n")

myfn&lt;-function(xx, shift=100){
    ii&lt;-1:length(xx)
    result&lt;-shift+sum(xx^ii)
}

xx&lt;-c(1,2,3,4)
ii&lt;-1:length(xx)
print(xx)
gn&lt;-grback(xx,myfn, shift=0)
print(gn)
ga&lt;-ii*xx^(ii-1)
cat("compare to analytic gradient:\n")
print(ga)

cat("change the step parameter to 1e-4\n")
optsp$deps &lt;- 1e-4
gn2&lt;-grback(xx,myfn, shift=0)
print(gn2)

</code></pre>

<hr>
<h2 id='grcentral'>Central difference numerical gradient approximation.</h2><span id='topic+grcentral'></span>

<h3>Description</h3>

<p><code>grcentral</code> computes the central difference approximation to the gradient of 
user function <code>userfn</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   grcentral(par, userfn, fbase=NULL, env=optsp, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="grcentral_+3A_par">par</code></td>
<td>

<p>parameters to the user objective function userfn
</p>
</td></tr>
<tr><td><code id="grcentral_+3A_userfn">userfn</code></td>
<td>

<p>User-supplied objective function
</p>
</td></tr>
<tr><td><code id="grcentral_+3A_fbase">fbase</code></td>
<td>

<p>The value of the function at the parameters, else NULL. This is to save
recomputing the function at this point.
</p>
</td></tr>
<tr><td><code id="grcentral_+3A_env">env</code></td>
<td>

<p>Environment for scratchpad items (like <code>deps</code> for approximation 
control in this routine). Default <code>optsp</code>.
</p>
</td></tr>
<tr><td><code id="grcentral_+3A_...">...</code></td>
<td>

<p>optional arguments passed to the objective function.
</p>
</td></tr>
</table>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> grcentral</td>
</tr>
<tr>
 <td style="text-align: left;">
Depends: </td><td style="text-align: left;"> R (&gt;= 2.6.1)</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL Version 2.</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>
  


<h3>Value</h3>

<p><code>grcentral</code> returns a single vector object <code>df</code> which approximates the 
gradient of userfn at the parameters par. The approximation is controlled by a
global value <code>optderiveps</code> that is set when the package is attached.
</p>


<h3>Author(s)</h3>

<p>John C. Nash
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cat("Example of use of grcentral\n")

myfn&lt;-function(xx, shift=100){
    ii&lt;-1:length(xx)
    result&lt;-shift+sum(xx^ii)
}
xx&lt;-c(1,2,3,4)
ii&lt;-1:length(xx)
print(xx)
gn&lt;-grcentral(xx,myfn, shift=0)
print(gn)
ga&lt;-ii*xx^(ii-1)
cat("compare to\n")
print(ga)

</code></pre>

<hr>
<h2 id='grchk'>Run tests, where possible, on user objective function and (optionally) gradient and hessian</h2><span id='topic+grchk'></span>

<h3>Description</h3>

<p><code>grchk</code> checks a user-provided R function, <code>ffn</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   grchk(xpar, ffn, ggr, trace=0, testtol=(.Machine$double.eps)^(1/3), ...) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="grchk_+3A_xpar">xpar</code></td>
<td>

<p>parameters to the user objective and gradient functions ffn and ggr
</p>
</td></tr>
<tr><td><code id="grchk_+3A_ffn">ffn</code></td>
<td>

<p>User-supplied objective function
</p>
</td></tr>
<tr><td><code id="grchk_+3A_ggr">ggr</code></td>
<td>

<p>User-supplied gradient function
</p>
</td></tr>
<tr><td><code id="grchk_+3A_trace">trace</code></td>
<td>

<p>set &gt;0 to provide output from grchk to the console, 0 otherwise
</p>
</td></tr>
<tr><td><code id="grchk_+3A_testtol">testtol</code></td>
<td>

<p>tolerance for equality tests
</p>
</td></tr>
<tr><td><code id="grchk_+3A_...">...</code></td>
<td>

<p>optional arguments passed to the objective function.
</p>
</td></tr>
</table>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> grchk</td>
</tr>
<tr>
 <td style="text-align: left;">
Depends: </td><td style="text-align: left;"> R (&gt;= 2.6.1)</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL Version 2.</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>
  
<p><code>numDeriv</code> is used to numerically approximate the gradient of function <code>ffn</code>
and compare this to the result of function <code>ggr</code>.
</p>


<h3>Value</h3>

<p><code>grchk</code> returns a single object <code>gradOK</code> which is TRUE if the differences 
between analytic and approximated gradient are small as measured by the tolerance 
<code>testtol</code>.
</p>
<p>This has attributes &quot;ga&quot; and &quot;gn&quot; for the analytic and numerically approximated gradients,
and &quot;maxdiff&quot; for the maximum absolute difference between these vectors.
</p>
<p>At the time of preparation, there are no checks for validity of the gradient code in
<code>ggr</code> as in the function <code>fnchk</code>.
</p>


<h3>Author(s)</h3>

<p>John C. Nash
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Would like examples of success and failure. What about "near misses"?
cat("Show how grchk works\n")
require(numDeriv)
# require(optimx)

jones&lt;-function(xx){
  x&lt;-xx[1]
  y&lt;-xx[2]
  ff&lt;-sin(x*x/2 - y*y/4)*cos(2*x-exp(y))
  ff&lt;- -ff
}

jonesg &lt;- function(xx) {
  x&lt;-xx[1]
  y&lt;-xx[2]
  gx &lt;-  cos(x * x/2 - y * y/4) * ((x + x)/2) * cos(2 * x - exp(y)) - 
    sin(x * x/2 - y * y/4) * (sin(2 * x - exp(y)) * 2)
  gy &lt;- sin(x * x/2 - y * y/4) * (sin(2 * x - exp(y)) * exp(y)) - cos(x * 
              x/2 - y * y/4) * ((y + y)/4) * cos(2 * x - exp(y))
  gg &lt;- - c(gx, gy)
}

jonesg2 &lt;- function(xx) {
  gx &lt;- 1
  gy &lt;- 2
  gg &lt;- - c(gx, gy)
}


xx &lt;- c(1, 2)

gcans &lt;- grchk(xx, jones, jonesg, trace=1, testtol=(.Machine$double.eps)^(1/3))
gcans

gcans2 &lt;- grchk(xx, jones, jonesg2, trace=1, testtol=(.Machine$double.eps)^(1/3))
gcans2




</code></pre>

<hr>
<h2 id='grfwd'>Forward difference numerical gradient approximation.</h2><span id='topic+grfwd'></span><span id='topic+optsp'></span>

<h3>Description</h3>

<p><code>grfwd</code> computes the forward difference approximation to the gradient of 
user function <code>userfn</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   grfwd(par, userfn, fbase=NULL, env=optsp, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="grfwd_+3A_par">par</code></td>
<td>

<p>parameters to the user objective function userfn
</p>
</td></tr>
<tr><td><code id="grfwd_+3A_userfn">userfn</code></td>
<td>

<p>User-supplied objective function
</p>
</td></tr>
<tr><td><code id="grfwd_+3A_fbase">fbase</code></td>
<td>

<p>The value of the function at the parameters, else NULL. This is to save
recomputing the function at this point.
</p>
</td></tr>
<tr><td><code id="grfwd_+3A_env">env</code></td>
<td>

<p>Environment for scratchpad items (like <code>deps</code> for approximation 
control in this routine). Default <code>optsp</code>.
</p>
</td></tr>
<tr><td><code id="grfwd_+3A_...">...</code></td>
<td>

<p>optional arguments passed to the objective function.
</p>
</td></tr>
</table>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> grfwd</td>
</tr>
<tr>
 <td style="text-align: left;">
Depends: </td><td style="text-align: left;"> R (&gt;= 2.6.1)</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL Version 2.</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>
  


<h3>Value</h3>

<p><code>grfwd</code> returns a single vector object <code>df</code> which approximates the 
gradient of userfn at the parameters par. The approximation is controlled by a
global value <code>optderiveps</code> that is set when the package is attached.
</p>


<h3>Author(s)</h3>

<p>John C. Nash
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cat("Example of use of grfwd\n")

myfn&lt;-function(xx, shift=100){
    ii&lt;-1:length(xx)
    result&lt;-shift+sum(xx^ii)
}
xx&lt;-c(1,2,3,4)
ii&lt;-1:length(xx)
print(xx)
gn&lt;-grfwd(xx,myfn, shift=0)
print(gn)
ga&lt;-ii*xx^(ii-1)
cat("compare to\n")
print(ga)
</code></pre>

<hr>
<h2 id='grnd'>A reorganization of the call to numDeriv grad() function.</h2><span id='topic+grnd'></span>

<h3>Description</h3>

<p>Provides a wrapper for the numDeriv approximation to the
gradient of a user supplied objective function <code>userfn</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>       grnd(par, userfn, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="grnd_+3A_par">par</code></td>
<td>
<p>A vector of parameters to the user-supplied function <code>fn</code></p>
</td></tr>
<tr><td><code id="grnd_+3A_userfn">userfn</code></td>
<td>
<p>A user-supplied function </p>
</td></tr>
<tr><td><code id="grnd_+3A_...">...</code></td>
<td>
<p>Other data needed to evaluate the user function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Richardson method is used in this routine.
</p>


<h3>Value</h3>

<p><code>grnd</code> returns an approximation to the gradient of the function userfn
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cat("Example of use of grnd\n")
require(numDeriv)
myfn&lt;-function(xx, shift=100){
    ii&lt;-1:length(xx)
    result&lt;-shift+sum(xx^ii)
}
xx&lt;-c(1,2,3,4)
ii&lt;-1:length(xx)
print(xx)
gn&lt;-grnd(xx,myfn, shift=0)
print(gn)
ga&lt;-ii*xx^(ii-1)
cat("compare to\n")
print(ga)
</code></pre>

<hr>
<h2 id='grpracma'>A reorganization of the call to numDeriv grad() function.</h2><span id='topic+grpracma'></span>

<h3>Description</h3>

<p>Provides a wrapper for the numDeriv approximation to the
gradient of a user supplied objective function <code>userfn</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>       grpracma(par, userfn, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="grpracma_+3A_par">par</code></td>
<td>
<p>A vector of parameters to the user-supplied function <code>fn</code></p>
</td></tr>
<tr><td><code id="grpracma_+3A_userfn">userfn</code></td>
<td>
<p>A user-supplied function </p>
</td></tr>
<tr><td><code id="grpracma_+3A_...">...</code></td>
<td>
<p>Other data needed to evaluate the user function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Richardson method is used in this routine.
</p>


<h3>Value</h3>

<p><code>grpracma</code> returns an approximation to the gradient of the function userfn
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cat("Example of use of grpracma\n")
require(numDeriv)
myfn&lt;-function(xx, shift=100){
    ii&lt;-1:length(xx)
    result&lt;-shift+sum(xx^ii)
}
xx&lt;-c(1,2,3,4)
ii&lt;-1:length(xx)
print(xx)
gn&lt;-grpracma(xx,myfn, shift=0)
print(gn)
ga&lt;-ii*xx^(ii-1)
cat("compare to\n")
print(ga)
</code></pre>

<hr>
<h2 id='hesschk'>Run tests, where possible, on user objective function and (optionally) gradient and hessian</h2><span id='topic+hesschk'></span>

<h3>Description</h3>

<p><code>hesschk</code> checks a user-provided R function, <code>ffn</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   hesschk(xpar, ffn, ggr, hhess, trace=0, testtol=(.Machine$double.eps)^(1/3), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hesschk_+3A_xpar">xpar</code></td>
<td>

<p>parameters to the user objective and gradient functions ffn and ggr
</p>
</td></tr>
<tr><td><code id="hesschk_+3A_ffn">ffn</code></td>
<td>

<p>User-supplied objective function
</p>
</td></tr>
<tr><td><code id="hesschk_+3A_ggr">ggr</code></td>
<td>

<p>User-supplied gradient function
</p>
</td></tr>
<tr><td><code id="hesschk_+3A_hhess">hhess</code></td>
<td>

<p>User-supplied Hessian function
</p>
</td></tr>
<tr><td><code id="hesschk_+3A_trace">trace</code></td>
<td>

<p>set &gt;0 to provide output from hesschk to the console, 0 otherwise
</p>
</td></tr>
<tr><td><code id="hesschk_+3A_testtol">testtol</code></td>
<td>

<p>tolerance for equality tests
</p>
</td></tr>
<tr><td><code id="hesschk_+3A_...">...</code></td>
<td>

<p>optional arguments passed to the objective function.
</p>
</td></tr>
</table>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> hesschk</td>
</tr>
<tr>
 <td style="text-align: left;">
Depends: </td><td style="text-align: left;"> R (&gt;= 2.6.1)</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL Version 2.</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>
  
<p><code>numDeriv</code> is used to compute a numerical approximation to the Hessian
matrix. If there is no analytic gradient, then the <code>hessian()</code> function 
from <code>numDeriv</code> is applied to the user function <code>ffn</code>. Otherwise, 
the <code>jacobian()</code> function of <code>numDeriv</code> is applied to the <code>ggr</code>
function so that only one level of differencing is used.
</p>


<h3>Value</h3>

<p>The function returns a single object <code>hessOK</code> which is TRUE if the 
analytic Hessian code returns a Hessian matrix that is &quot;close&quot; to the 
numerical approximation obtained via <code>numDeriv</code>; FALSE otherwise.
</p>
<p><code>hessOK</code> is returned with the following attributes:
</p>

<dl>
<dt>&quot;nullhess&quot;</dt><dd><p>Set TRUE if the user does not supply a function to compute the Hessian.</p>
</dd>
<dt>&quot;asym&quot;</dt><dd><p>Set TRUE if the Hessian does not satisfy symmetry conditions to
within a tolerance. See the <code>hesschk</code> for details.</p>
</dd>
<dt>&quot;ha&quot;</dt><dd><p>The analytic Hessian computed at paramters <code>xpar</code> using <code>hhess</code>.</p>
</dd>
<dt>&quot;hn&quot;</dt><dd><p>The numerical approximation to the Hessian computed at paramters <code>xpar</code>.</p>
</dd>
<dt>&quot;msg&quot;</dt><dd><p>A text comment on the outcome of the tests.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>John C. Nash
</p>


<h3>Examples</h3>

<pre><code class='language-R'># genrose function code
genrose.f&lt;- function(x, gs=NULL){ # objective function
## One generalization of the Rosenbrock banana valley function (n parameters)
	n &lt;- length(x)
        if(is.null(gs)) { gs=100.0 }
	fval&lt;-1.0 + sum (gs*(x[1:(n-1)]^2 - x[2:n])^2 + (x[2:n] - 1)^2)
        return(fval)
}

genrose.g &lt;- function(x, gs=NULL){
# vectorized gradient for genrose.f
# Ravi Varadhan 2009-04-03
	n &lt;- length(x)
        if(is.null(gs)) { gs=100.0 }
	gg &lt;- as.vector(rep(0, n))
	tn &lt;- 2:n
	tn1 &lt;- tn - 1
	z1 &lt;- x[tn] - x[tn1]^2
	z2 &lt;- 1 - x[tn]
	gg[tn] &lt;- 2 * (gs * z1 - z2)
	gg[tn1] &lt;- gg[tn1] - 4 * gs * x[tn1] * z1
	return(gg)
}

genrose.h &lt;- function(x, gs=NULL) { ## compute Hessian
   if(is.null(gs)) { gs=100.0 }
	n &lt;- length(x)
	hh&lt;-matrix(rep(0, n*n),n,n)
	for (i in 2:n) {
		z1&lt;-x[i]-x[i-1]*x[i-1]
#		z2&lt;-1.0-x[i]
                hh[i,i]&lt;-hh[i,i]+2.0*(gs+1.0)
                hh[i-1,i-1]&lt;-hh[i-1,i-1]-4.0*gs*z1-4.0*gs*x[i-1]*(-2.0*x[i-1])
                hh[i,i-1]&lt;-hh[i,i-1]-4.0*gs*x[i-1]
                hh[i-1,i]&lt;-hh[i-1,i]-4.0*gs*x[i-1]
	}
        return(hh)
}

trad&lt;-c(-1.2,1)
ans100&lt;-hesschk(trad, genrose.f, genrose.g, genrose.h, trace=1)
print(ans100)
ans10&lt;-hesschk(trad, genrose.f, genrose.g, genrose.h, trace=1, gs=10)
print(ans10)


</code></pre>

<hr>
<h2 id='hjn'>Compact R Implementation of Hooke and Jeeves Pattern Search Optimization</h2><span id='topic+hjn'></span>

<h3>Description</h3>

<p>The purpose of <code>hjn</code> is to minimize an unconstrained or bounds
(box) and mask constrained function 
of several parameters by a Hooke and Jeeves pattern search. This 
didactic code is
entirely in R to allow users to explore and understand the method. It also
allows bounds (or box) constraints and masks (equality constraints) to be
imposed on parameters. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   hjn(par, fn, lower=-Inf, upper=Inf, bdmsk=NULL, control = list(trace=0), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hjn_+3A_par">par</code></td>
<td>
<p>A numeric vector of starting estimates.</p>
</td></tr>
<tr><td><code id="hjn_+3A_fn">fn</code></td>
<td>
<p>A function that returns the value of the objective at the
supplied set of parameters <code>par</code> using auxiliary data in ....
The first argument of <code>fn</code> must be <code>par</code>. </p>
</td></tr>
<tr><td><code id="hjn_+3A_lower">lower</code></td>
<td>
<p>A vector of lower bounds on the parameters.</p>
</td></tr>
<tr><td><code id="hjn_+3A_upper">upper</code></td>
<td>
<p>A vector of upper bounds on the parameters.</p>
</td></tr>
<tr><td><code id="hjn_+3A_bdmsk">bdmsk</code></td>
<td>
<p>An indicator vector, having 1 for each parameter that is &quot;free&quot; or
unconstrained, and 0 for any parameter that is fixed or MASKED for the
duration of the optimization.</p>
</td></tr>
<tr><td><code id="hjn_+3A_control">control</code></td>
<td>

<p>An optional list of control settings.  
</p>
</td></tr>
<tr><td><code id="hjn_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to <code>fn</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Functions <code>fn</code> must return a numeric value.
</p>
<p>The <code>control</code> argument is a list.
</p>

<dl>
<dt>maxfeval</dt><dd><p>A limit on the number of function evaluations used in the search.</p>
</dd>
<dt>trace</dt><dd><p>Set 0 (default) for no output, &gt;0 for trace output
(larger values imply more output).</p>
</dd>
<dt>eps</dt><dd><p>Tolerance used to calculate numerical gradients. Default is 1.0E-7. See 
source code for <code>hjn</code> for details of application.</p>
</dd>
<dt><code>dowarn</code></dt><dd><p>= TRUE if we want warnings generated by optimx. Default is 
TRUE.</p>
</dd>
<dt><code>tol</code></dt><dd><p>Tolerance used in testing the size of the pattern search step.</p>
</dd>
</dl>

<p>Note that the control <code>maximize</code> should NOT be used. 
</p>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>par</code></td>
<td>
<p>The best set of parameters found.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>The value of the objective at the best set of parameters found.</p>
</td></tr>
<tr><td><code>counts</code></td>
<td>
<p>A two-element integer vector giving the number of calls to
'fn' and 'gr' respectively. This excludes those calls needed
to compute the Hessian, if requested, and any calls to 'fn'
to compute a finite-difference approximation to the gradient.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>An integer code. 
'0' indicates successful convergence.
'1' indicates that the function evaluation count 'maxfeval' was reached.</p>
</td></tr>
<tr><td><code>message</code></td>
<td>
<p>A character string giving any additional information returned
by the optimizer, or 'NULL'.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Nash JC (1979). Compact Numerical Methods for Computers: Linear 
Algebra and Function Minimisation. Adam Hilger, Bristol. Second 
Edition, 1990, Bristol: Institute of Physics Publications.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+optim">optim</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>#####################
## Rosenbrock Banana function
fr &lt;- function(x) {
    x1 &lt;- x[1]
    x2 &lt;- x[2]
    100 * (x2 - x1 * x1)^2 + (1 - x1)^2
}

ansrosenbrock0 &lt;- hjn(fn=fr, par=c(1,2), control=list(maxfeval=2000, trace=0))
print(ansrosenbrock0) # use print to allow copy to separate file that 

#    can be called using source()
#####################
genrose.f&lt;- function(x, gs=NULL){ # objective function
## One generalization of the Rosenbrock banana valley function (n parameters)
	n &lt;- length(x)
        if(is.null(gs)) { gs=100.0 }
	fval&lt;-1.0 + sum (gs*(x[1:(n-1)]^2 - x[2:n])^2 + (x[2:n] - 1)^2)
        return(fval)
}

xx&lt;-rep(pi,10)
lower&lt;-NULL
upper&lt;-NULL
bdmsk&lt;-NULL

cat("timings B vs U\n")
lo&lt;-rep(-100,10)
up&lt;-rep(100,10)
bdmsk&lt;-rep(1,10)
tb&lt;-system.time(ab&lt;-hjn(xx,genrose.f, lower=lo, upper=up,
          bdmsk=bdmsk, control=list(trace=0, maxfeval=2000)))[1]
tu&lt;-system.time(au&lt;-hjn(xx,genrose.f, control=list(maxfeval=2000, trace=0)))[1]
cat("times U=",tu,"   B=",tb,"\n")
cat("solution hjnu\n")
print(au)
cat("solution hjnb\n")
print(ab)
cat("diff fu-fb=",au$value-ab$value,"\n")
cat("max abs parameter diff = ", max(abs(au$par-ab$par)),"\n")

######### One dimension test
sqtst&lt;-function(xx) {
   res&lt;-sum((xx-2)*(xx-2))
}

nn&lt;-1
startx&lt;-rep(0,nn)
onepar&lt;-hjn(startx,sqtst,control=list(trace=1)) 
print(onepar)
</code></pre>

<hr>
<h2 id='kktchk'>Check Kuhn Karush Tucker conditions for a supposed function minimum</h2><span id='topic+kktchk'></span>

<h3>Description</h3>

<p>Provide a check on Kuhn-Karush-Tucker conditions based on quantities
already computed. Some of these used only for reporting.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>     kktchk(par, fn, gr, hess=NULL, upper=NULL, lower=NULL, 
                 maximize=FALSE, control=list(dowarn=TRUE), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kktchk_+3A_par">par</code></td>
<td>
<p>A vector of values for the parameters which are supposedly optimal.</p>
</td></tr>
<tr><td><code id="kktchk_+3A_fn">fn</code></td>
<td>
<p>The objective function</p>
</td></tr>
<tr><td><code id="kktchk_+3A_gr">gr</code></td>
<td>
<p>The gradient function</p>
</td></tr>
<tr><td><code id="kktchk_+3A_hess">hess</code></td>
<td>
<p>The Hessian function</p>
</td></tr>
<tr><td><code id="kktchk_+3A_upper">upper</code></td>
<td>
<p>Upper bounds on the parameters</p>
</td></tr>
<tr><td><code id="kktchk_+3A_lower">lower</code></td>
<td>
<p>Lower bounds on the parameters</p>
</td></tr>
<tr><td><code id="kktchk_+3A_maximize">maximize</code></td>
<td>
<p>Logical TRUE if function is being maximized. Default FALSE.</p>
</td></tr>
<tr><td><code id="kktchk_+3A_control">control</code></td>
<td>
<p>A list of controls for the function</p>
</td></tr>
<tr><td><code id="kktchk_+3A_...">...</code></td>
<td>
<p>The dot arguments needed for evaluating the function and gradient and hessian</p>
</td></tr>
</table>


<h3>Details</h3>

<p>kktchk computes the gradient and Hessian measures for BOTH unconstrained and 
bounds (and masks) constrained parameters, but the kkt measures are evaluated
only for the constrained case.
</p>
<p>Note that evaluated Hessians are often not symmetric, and many, possibly most,
examples will fail the <code>is.Symmetric()</code> function. In such cases, the 
check on the Hessian uses the mean of the Hessian and its transpose.
</p>


<h3>Value</h3>

<p>The output is a list consisting of 
</p>
<table>
<tr><td><code>gmax</code></td>
<td>
<p>The absolute value of the largest gradient component in magnitude.</p>
</td></tr>
<tr><td><code>evratio</code></td>
<td>
<p>The ratio of the smallest to largest Hessian eigenvalue. Note that this
may be negative.</p>
</td></tr>
<tr><td><code>kkt1</code></td>
<td>
<p>A logical value that is TRUE if we consider the first (i.e., gradient) 
KKT condition to be satisfied. WARNING: The decision is dependent on tolerances and
scaling that may be inappropriate for some problems.</p>
</td></tr>
<tr><td><code>kkt2</code></td>
<td>
<p>A logical value that is TRUE if we consider the second (i.e., positive
definite Hessian) KKT condition to be satisfied. WARNING: The decision is dependent 
on tolerances and scaling that may be inappropriate for some problems.</p>
</td></tr>
<tr><td><code>hev</code></td>
<td>
<p>The calculated hessian eigenvalues, sorted largest to smallest. 
Sorting is a property of the <code>eigen()</code> function.</p>
</td></tr>
<tr><td><code>ngatend</code></td>
<td>
<p>The computed (unconstrained) gradient at the solution parameters.</p>
</td></tr>
<tr><td><code>nnatend</code></td>
<td>
<p>The computed (unconstrained) hessian at the solution parameters.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+optim">optim</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cat("Show how kktc works\n")

# require(optimx)

jones&lt;-function(xx){
  x&lt;-xx[1]
  y&lt;-xx[2]
  ff&lt;-sin(x*x/2 - y*y/4)*cos(2*x-exp(y))
  ff&lt;- -ff
}

jonesg &lt;- function(xx) {
  x&lt;-xx[1]
  y&lt;-xx[2]
  gx &lt;-  cos(x * x/2 - y * y/4) * ((x + x)/2) * cos(2 * x - exp(y)) - 
    sin(x * x/2 - y * y/4) * (sin(2 * x - exp(y)) * 2)
  gy &lt;- sin(x * x/2 - y * y/4) * (sin(2 * x - exp(y)) * exp(y)) - cos(x * 
             x/2 - y * y/4) * ((y + y)/4) * cos(2 * x - exp(y))
  gg &lt;- - c(gx, gy)
}

ans &lt;- list() # to ensure structure available
# If optimx package available, the following can be run.
# xx&lt;-0.5*c(pi,pi)
# ans &lt;- optimr(xx, jones, jonesg, method="Rvmmin")
# ans

ans$par &lt;- c(3.154083, -3.689620)

# 2023-8-23 need dowarn specified or get error
# Note: may want to set control=list(dowarn=TRUE)
kkans &lt;- kktchk(ans$par, jones, jonesg)
kkans



</code></pre>

<hr>
<h2 id='multistart'>General-purpose optimization - multiple starts</h2><span id='topic+multistart'></span>

<h3>Description</h3>

<p>Multiple initial parameter wrapper function that calls other
R tools for optimization, including the existing optimr() function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multistart(parmat, fn, gr=NULL, lower=-Inf, upper=Inf, 
            method=NULL, hessian=FALSE,
            control=list(),
             ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="multistart_+3A_parmat">parmat</code></td>
<td>
<p>a matrix of which each row is a set of initial values 
for the parameters 
for which optimal values are to be found. Names on the elements
of this vector are preserved and used in the results data frame.</p>
</td></tr>  
<tr><td><code id="multistart_+3A_fn">fn</code></td>
<td>
<p>A function to be minimized (or maximized), with first
argument the vector of parameters over which minimization is to take
place.  It should return a scalar result.</p>
</td></tr>
<tr><td><code id="multistart_+3A_gr">gr</code></td>
<td>
<p>A function to return (as a vector) the gradient for those methods that 
can use this information.
</p>
<p>If 'gr' is <code>NULL</code>, a finite-difference approximation will be used.
An open question concerns whether the SAME approximation code used for all methods, 
or whether there are differences that could/should be examined? </p>
</td></tr>
<tr><td><code id="multistart_+3A_lower">lower</code>, <code id="multistart_+3A_upper">upper</code></td>
<td>
<p>Bounds on the variables for methods such as <code>"L-BFGS-B"</code> that can
handle box (or bounds) constraints.</p>
</td></tr>
<tr><td><code id="multistart_+3A_method">method</code></td>
<td>
<p>A character string giving the name of the optimization method to be
applied. See the list <code>allmeth</code> in file 
<code>ctrldefault.R</code> which is part of this package.</p>
</td></tr>
<tr><td><code id="multistart_+3A_hessian">hessian</code></td>
<td>
<p>A logical control that if TRUE forces the computation of an approximation 
to the Hessian at the final set of parameters. If FALSE (default), the hessian is
calculated if needed to provide the KKT optimality tests (see <code>kkt</code> in
&lsquo;Details&rsquo; for the <code>control</code> list).
This setting is provided primarily for compatibility with optim().</p>
</td></tr>
<tr><td><code id="multistart_+3A_control">control</code></td>
<td>
<p>A list of control parameters. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="multistart_+3A_...">...</code></td>
<td>
<p>For <code>optimx</code> further arguments to be passed to <code>fn</code> 
and <code>gr</code>; otherwise, further arguments are not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that arguments after <code>...</code> must be matched exactly.
</p>
<p>See <code>optimr()</code> for other details.
</p>


<h3>Value</h3>

<p>An array with one row per set of starting parameters. Each row contains:
</p>
<table>
<tr><td><code>par</code></td>
<td>
<p>The best set of parameters found.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>The value of fn corresponding to par.</p>
</td></tr>
<tr><td><code>counts</code></td>
<td>
<p> A two-element integer vector giving the number of calls to
fn and gr respectively. This excludes those calls needed
to compute the Hessian, if requested, and any calls to fn
to compute a finite-difference approximation to the gradient.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p> An integer code. 0 indicates successful completion</p>
</td></tr>
<tr><td><code>message</code></td>
<td>
<p> A character string giving any additional information returned
by the optimizer, or NULL.</p>
</td></tr>
<tr><td><code>hessian</code></td>
<td>
<p> Always NULL for this routine.</p>
</td></tr>
</table>


<h3>Source</h3>

<p>See the manual pages for <code>optim()</code> and the packages the DESCRIPTION <code>suggests</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>fnR &lt;- function (x, gs=100.0) 
{
    n &lt;- length(x)
    x1 &lt;- x[2:n]
    x2 &lt;- x[1:(n - 1)]
    sum(gs * (x1 - x2^2)^2 + (1 - x2)^2)
}
grR &lt;- function (x, gs=100.0) 
{
    n &lt;- length(x)
    g &lt;- rep(NA, n)
    g[1] &lt;- 2 * (x[1] - 1) + 4*gs * x[1] * (x[1]^2 - x[2])
    if (n &gt; 2) {
        ii &lt;- 2:(n - 1)
        g[ii] &lt;- 2 * (x[ii] - 1) + 4 * gs * x[ii] * (x[ii]^2 - x[ii + 
            1]) + 2 * gs * (x[ii] - x[ii - 1]^2)
    }
    g[n] &lt;- 2 * gs * (x[n] - x[n - 1]^2)
    g
}

pm &lt;- rbind(rep(1,4), rep(pi, 4), rep(-2,4), rep(0,4), rep(20,4))
pm &lt;- as.matrix(pm)
cat("multistart matrix:\n")
print(pm)

ans &lt;- multistart(pm, fnR, grR, method="Rvmmin", control=list(trace=0))
ans

</code></pre>

<hr>
<h2 id='opm'>General-purpose optimization</h2><span id='topic+opm'></span>

<h3>Description</h3>

<p>General-purpose optimization wrapper function that calls multiple other
R tools for optimization, including the existing optim() function tools.
</p>
<p>Because SANN does not return a meaningful convergence code
(conv), <code>opm()</code> does not call the SANN method, but it can be invoked 
in <code>optimr()</code>.
</p>
<p>There is a pseudo-method &quot;ALL&quot; that runs all available methods. Note that
this is upper-case. This function is a replacement for optimx() from the
optimx package.  <code>opm()</code> calls the <code>optimr()</code> function for each 
solver in the <code>method</code> list.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>opm(par, fn, gr=NULL, hess=NULL, lower=-Inf, upper=Inf, 
            method=c("Nelder-Mead","BFGS"), hessian=FALSE,
            control=list(),
             ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="opm_+3A_par">par</code></td>
<td>
<p>a vector of initial values for the parameters 
for which optimal values are to be found. Names on the elements
of this vector are preserved and used in the results data frame.</p>
</td></tr>  
<tr><td><code id="opm_+3A_fn">fn</code></td>
<td>
<p>A function to be minimized (or maximized), with a first
argument the vector of parameters over which minimization is to take
place.  It should return a scalar result.</p>
</td></tr>
<tr><td><code id="opm_+3A_gr">gr</code></td>
<td>
<p>A function to return (as a vector) the gradient for those methods that 
can use this information.
</p>
<p>If 'gr' is <code>NULL</code>, whatever default actions are supplied by the methods
specified will be used. However, some methods REQUIRE a gradient function, so 
will fail in this case. <code>opm()</code> will generally return with <code>convergence</code>
set to 9998 for such methods.
</p>
<p>If 'gr' is a character string, this character string will be taken to be the name
of an available gradient approximation function. Examples are &quot;grfwd&quot;, &quot;grback&quot;, 
&quot;grcentral&quot; and &quot;grnd&quot;, with the last name referring to the default method of 
package <code>numDeriv</code>.</p>
</td></tr>
<tr><td><code id="opm_+3A_hess">hess</code></td>
<td>
<p>A function to return (as a symmetric matrix) the Hessian of the objective 
function for those methods that can use this information.</p>
</td></tr>
<tr><td><code id="opm_+3A_lower">lower</code>, <code id="opm_+3A_upper">upper</code></td>
<td>
<p>Bounds on the variables for methods such as <code>"L-BFGS-B"</code> that can
handle box (or bounds) constraints. These are vectors.</p>
</td></tr>
<tr><td><code id="opm_+3A_method">method</code></td>
<td>
<p>A vector of the methods to be used, each as a character string.
Note that this is an important change from optim() that allows
just one method to be specified. See &lsquo;Details&rsquo;. If <code>method</code>
has just one element, <code>"ALL"</code> (capitalized), all available and 
appropriate methods will be tried.</p>
</td></tr>
<tr><td><code id="opm_+3A_hessian">hessian</code></td>
<td>
<p>A logical control that if TRUE forces the computation of an approximation 
to the Hessian at the final set of parameters. If FALSE (default), the hessian is
calculated if needed to provide the KKT optimality tests (see <code>kkt</code> in
&lsquo;Details&rsquo; for the <code>control</code> list).
This setting is provided primarily for compatibility with optim().</p>
</td></tr>
<tr><td><code id="opm_+3A_control">control</code></td>
<td>
<p>A list of control parameters. See &lsquo;Details&rsquo;. There is
a spreadsheet <code>/inst/doc/optcontrol.xls</code> that is an ongoing attempt to
document the different controls used in the various methods.</p>
</td></tr>
<tr><td><code id="opm_+3A_...">...</code></td>
<td>
<p>For <code>optimx</code> further arguments to be passed to <code>fn</code> 
and <code>gr</code>; otherwise, further arguments are not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that arguments after <code>...</code> must be matched exactly.
</p>
<p>For details of how <code>opm()</code> calls the methods, see the documentation
and code for <code>optimr()</code>. The documentation and code for individual
methods may also be useful. Note that some simplification of the calls
may have been necessary, for example, to provide reasonable default values
for method controls that are consistent across several methods, though this
is not always possible to guarantee. The documentation for <code>optimr</code> and the source
code of the quite simple routine <code>ctrldefault.R</code> may be useful.
</p>
<p>Some of the commonly useful elements of the <code>control</code> list are: 
</p>

<dl>
<dt><code>trace</code></dt><dd><p>Non-negative integer. If positive,
tracing information on the
progress of the optimization is produced. Higher values may
produce more tracing information: for method <code>"L-BFGS-B"</code>
there are six levels of tracing. trace = 0 gives no output 
(To understand exactly what these do see the source code: higher 
levels give more detail.)</p>
</dd>
<dt><code>maxfeval</code></dt><dd><p>For methods that can use this control, a limit
on the number of function evaluations. This control is simply passed
through. It is not checked by <code>opm</code>.</p>
</dd>
<dt><code>maxit</code></dt><dd><p>For methods that can use this control, a limit
on the number of gradient evaluations or major iterations.</p>
</dd>
<dt><code>fnscale</code></dt><dd><p>An overall scaling to be applied to the value
of <code>fn</code> and <code>gr</code> during optimization. If negative,
turns the problem into a maximization problem. Optimization is
performed on <code>fn(par)/fnscale</code>. For methods from the set in
<code>optim()</code>. Note potential conflicts with the control <code>maximize</code>.</p>
</dd>
<dt><code>parscale</code></dt><dd><p>A vector of scaling values for the parameters.
Optimization is performed on <code>par/parscale</code> and these should be
comparable in the sense that a unit change in any element produces
about a unit change in the scaled value.For <code>optim</code>.</p>
</dd>
<dt><code>save.failures</code></dt><dd><p> = TRUE (default) if we wish to keep &quot;answers&quot; from runs 
where the method does not return convergence==0. FALSE otherwise.</p>
</dd>
<dt><code>maximize</code></dt><dd><p> = TRUE if we want to maximize rather than minimize 
a function. (Default FALSE). Methods nlm, nlminb, ucminf cannot maximize a
function, so the user must explicitly minimize and carry out the adjustment
externally. However, there is a check to avoid
usage of these codes when maximize is TRUE. See <code>fnscale</code> below for 
the method used in <code>optim</code> that we deprecate.</p>
</dd>
<dt><code>all.methods</code></dt><dd><p>= TRUE if we want to use all available (and suitable)
methods.  This is equivalent to setting <code>method="ALL"</code></p>
</dd>
<dt><code>kkt</code></dt><dd><p>=FALSE if we do NOT want to test the Kuhn, Karush, Tucker
optimality conditions. The default is generally TRUE. However, because the Hessian
computation may be very slow, we set <code>kkt</code> to be FALSE if there are 
more than than 50 parameters when the gradient function <code>gr</code> is not 
provided, and more than 500
parameters when such a function is specified. We return logical values <code>KKT1</code>
and <code>KKT2</code> TRUE if first and second order conditions are satisfied approximately.
Note, however, that the tests are sensitive to scaling, and users may need
to perform additional verification. If <code>hessian</code> is TRUE, this overrides 
control <code>kkt</code>.</p>
</dd>
<dt><code>all.methods</code></dt><dd><p>= TRUE if we want to use all available (and suitable)
methods.</p>
</dd>
<dt><code>kkttol</code></dt><dd><p>= value to use to check for small gradient and negative
Hessian eigenvalues. Default = .Machine$double.eps^(1/3) </p>
</dd>
<dt><code>kkt2tol</code></dt><dd><p>= Tolerance for eigenvalue ratio in KKT test of positive 
definite Hessian. Default same as for kkttol </p>
</dd>
<dt><code>dowarn</code></dt><dd><p>= FALSE if we want to suppress warnings generated by <code>opm()</code> or
<code>optimr()</code>. Default is TRUE.</p>
</dd>
<dt><code>badval</code></dt><dd><p>= The value to set for the function value when try(fn()) fails.
The value is then a signal of failure when execution continues with other methods.
It may also, in non-standard usage, be helpful in heuristic search methods like 
&quot;Nelder-Mead&quot; to avoid parameter
regions that are unwanted or inadmissible. It is inappropriate for gradient methods.
Default is (0.5)*.Machine$double.xmax </p>
</dd>
</dl>

<p>There may be <code>control</code> elements that apply only to some of the methods. Using these
may or may not &quot;work&quot; with <code>opm()</code>, and errors may occur with methods for which 
the controls have no meaning. 
However, it should be possible to call the underlying <code>optimr()</code> function with 
these method-specific controls.
</p>
<p>Any names given to <code>par</code> will be copied to the vectors passed to
<code>fn</code> and <code>gr</code>.  Note that no other attributes of <code>par</code>
are copied over. (We have not verified this as at 2009-07-29.)
</p>


<h3>Value</h3>

<p>If there are <code>npar</code> parameters, then the result is a dataframe having one row
for each method for which results are reported, using the method as the row name,
with columns
</p>
<p><code>par_1, .., par_npar, value, fevals, gevals, niter, convergence, kkt1, kkt2, xtimes</code>
</p>
<p>where
</p>

<dl>
<dt>par_1</dt><dd><p> .. </p>
</dd>
<dt>par_npar</dt><dd><p>The best set of parameters found.</p>
</dd>
<dt>value</dt><dd><p>The value of <code>fn</code> corresponding to <code>par</code>.</p>
</dd>
<dt>fevals</dt><dd><p>The number of calls to <code>fn</code>. NOT reported for method <code>lbfgs</code>.</p>
</dd>
<dt>gevals</dt><dd><p>The number of calls to <code>gr</code>. This excludes those calls needed
to compute the Hessian, if requested, and any calls to <code>fn</code> to
compute a finite-difference approximation to the gradient.  NOT reported for method <code>lbfgs</code>.</p>
</dd>
<dt>convergence</dt><dd><p>An integer code. <code>0</code> indicates successful
convergence. Various methods may or may not return sufficient information
to allow all the codes to be specified. An incomplete list of codes includes
</p>

<dl>
<dt><code>1</code></dt><dd><p>indicates that the iteration limit <code>maxit</code>
had been reached.</p>
</dd>
<dt><code>20</code></dt><dd><p>indicates that the initial set of parameters is inadmissible, that is,
that the function cannot be computed or returns an infinite, NULL, or NA value.</p>
</dd>
<dt><code>21</code></dt><dd><p>indicates that an intermediate set of parameters is inadmissible.</p>
</dd>
<dt><code>10</code></dt><dd><p>indicates degeneracy of the Nelder&ndash;Mead simplex.</p>
</dd>
<dt><code>51</code></dt><dd><p>indicates a warning from the <code>"L-BFGS-B"</code>
method; see component <code>message</code> for further details.</p>
</dd>
<dt><code>52</code></dt><dd><p>indicates an error from the <code>"L-BFGS-B"</code>
method; see component <code>message</code> for further details.</p>
</dd>
<dt><code>9998</code></dt><dd><p>indicates that the method has been called with a NULL 'gr'
function, and the method requires that such a function be supplied.</p>
</dd>
<dt><code>9999</code></dt><dd><p>indicates the method has failed.</p>
</dd>
</dl>

</dd>
<dt>kkt1</dt><dd><p>A logical value returned TRUE if the solution reported has a &ldquo;small&rdquo; gradient.</p>
</dd>
<dt>kkt2</dt><dd><p>A logical value returned TRUE if the solution reported appears to have a 
positive-definite Hessian.</p>
</dd>
<dt>xtimes</dt><dd><p>The reported execution time of the calculations for the particular method.</p>
</dd>
</dl>

<p>The attribute &quot;details&quot; to the returned answer object contains information,
if computed, on the gradient (<code>ngatend</code>) and Hessian matrix (<code>nhatend</code>) 
at the supposed optimum, along with the eigenvalues of the Hessian (<code>hev</code>), 
as well as the <code>message</code>, if any, returned by the computation for each <code>method</code>,
which is included for each row of the <code>details</code>. 
If the returned object from optimx() is <code>ans</code>, this is accessed 
via the construct
<code>attr(ans, "details")</code>
</p>
<p>This object is a  matrix based on a list so that if ans is the output of optimx
then attr(ans, &quot;details&quot;)[1, ] gives the first row and 
attr(ans,&quot;details&quot;)[&quot;Nelder-Mead&quot;, ] gives the Nelder-Mead row. There is 
one row for each method that has been successful 
or that has been forcibly saved by save.failures=TRUE. 
</p>
<p>There are also attributes
</p>

<dl>
<dt>maximize</dt><dd><p>to indicate we have been maximizing the objective</p>
</dd>
<dt>npar</dt><dd><p>to provide the number of parameters, thereby facilitating easy
extraction of the parameters from the results data frame</p>
</dd>
<dt>follow.on</dt><dd><p>to indicate that the results have been computed sequentially,
using the order provided by the user, with the best parameters from one
method used to start the next. There is an example (<code>ans9</code>) in 
the script <code>ox.R</code> in the demo directory of the package.</p>
</dd>
</dl>



<h3>Note</h3>

<p>Most methods in <code>optimx</code> will work with one-dimensional <code>par</code>s, but such
use is NOT recommended. Use <code><a href="stats.html#topic+optimize">optimize</a></code> or other one-dimensional methods instead.
</p>
<p>There are a series of demos available. Once the package is loaded (via <code>require(optimx)</code> or
<code>library(optimx)</code>, you may see available demos via 
</p>
<p>demo(package=&quot;optimx&quot;)
</p>
<p>The demo 'brown_test' may be run with the command
demo(brown_test, package=&quot;optimx&quot;)
</p>
<p>The package source contains several functions that are not exported in the
NAMESPACE. These are 
</p>

<dl>
<dt><code>optimx.setup()</code></dt><dd><p> which establishes the controls for a given run;</p>
</dd>
<dt><code>optimx.check()</code></dt><dd><p> which performs bounds and gradient checks on
the supplied parameters and functions;</p>
</dd>
<dt><code>optimx.run()</code></dt><dd><p>which actually performs the optimization and post-solution
computations;</p>
</dd>
<dt><code>scalechk()</code></dt><dd><p> which actually carries out a check on the relative scaling
of the input parameters.</p>
</dd>
</dl>

<p>Knowledgeable users may take advantage of these functions if they are carrying
out production calculations where the setup and checks could be run once.
</p>


<h3>Source</h3>

<p>See the manual pages for <code>optim()</code> and the packages the DESCRIPTION <code>suggests</code>.
</p>


<h3>References</h3>

<p>See the manual pages for <code>optim()</code> and the packages the DESCRIPTION <code>suggests</code>.
</p>
<p>Nash JC, and Varadhan R (2011). Unifying Optimization Algorithms to Aid Software System Users: 
<b>optimx</b> for R., <em>Journal of Statistical Software</em>, 43(9), 1-14.,  
URL http://www.jstatsoft.org/v43/i09/.
</p>
<p>Nash JC (2014). On Best Practice Optimization Methods in R., 
<em>Journal of Statistical Software</em>, 60(2), 1-14.,
URL http://www.jstatsoft.org/v60/i02/.
</p>


<h3>See Also</h3>

<p><code><a href="BB.html#topic+spg">spg</a></code>, <code><a href="stats.html#topic+nlm">nlm</a></code>, <code><a href="stats.html#topic+nlminb">nlminb</a></code>,
<code><a href="minqa.html#topic+bobyqa">bobyqa</a></code>, 
<code><a href="ucminf.html#topic+ucminf">ucminf</a></code>, 
<code><a href="dfoptim.html#topic+nmkb">nmkb</a></code>,
<code><a href="dfoptim.html#topic+hjkb">hjkb</a></code>.
<code><a href="stats.html#topic+optimize">optimize</a></code> for one-dimensional minimization;
<code><a href="stats.html#topic+constrOptim">constrOptim</a></code> or <code><a href="BB.html#topic+spg">spg</a></code> for linearly constrained optimization.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)
cat("Note possible demo(ox) for extended examples\n")


## Show multiple outputs of optimx using all.methods
# genrose function code
genrose.f&lt;- function(x, gs=NULL){ # objective function
## One generalization of the Rosenbrock banana valley function (n parameters)
	n &lt;- length(x)
        if(is.null(gs)) { gs=100.0 }
	fval&lt;-1.0 + sum (gs*(x[1:(n-1)]^2 - x[2:n])^2 + (x[2:n] - 1)^2)
        return(fval)
}

genrose.g &lt;- function(x, gs=NULL){
# vectorized gradient for genrose.f
# Ravi Varadhan 2009-04-03
	n &lt;- length(x)
        if(is.null(gs)) { gs=100.0 }
	gg &lt;- as.vector(rep(0, n))
	tn &lt;- 2:n
	tn1 &lt;- tn - 1
	z1 &lt;- x[tn] - x[tn1]^2
	z2 &lt;- 1 - x[tn]
	gg[tn] &lt;- 2 * (gs * z1 - z2)
	gg[tn1] &lt;- gg[tn1] - 4 * gs * x[tn1] * z1
	return(gg)
}

genrose.h &lt;- function(x, gs=NULL) { ## compute Hessian
   if(is.null(gs)) { gs=100.0 }
	n &lt;- length(x)
	hh&lt;-matrix(rep(0, n*n),n,n)
	for (i in 2:n) {
		z1&lt;-x[i]-x[i-1]*x[i-1]
		z2&lt;-1.0-x[i]
                hh[i,i]&lt;-hh[i,i]+2.0*(gs+1.0)
                hh[i-1,i-1]&lt;-hh[i-1,i-1]-4.0*gs*z1-4.0*gs*x[i-1]*(-2.0*x[i-1])
                hh[i,i-1]&lt;-hh[i,i-1]-4.0*gs*x[i-1]
                hh[i-1,i]&lt;-hh[i-1,i]-4.0*gs*x[i-1]
	}
        return(hh)
}

startx&lt;-4*seq(1:10)/3.
ans8&lt;-opm(startx,fn=genrose.f,gr=genrose.g, hess=genrose.h,
   method="ALL", control=list(save.failures=TRUE, trace=0), gs=10)
# Set trace=1 for output of individual solvers
ans8
ans8[, "gevals"]
ans8["spg", ]
summary(ans8, par.select = 1:3)
summary(ans8, order = value)[1, ] # show best value
head(summary(ans8, order = value)) # best few
## head(summary(ans8, order = "value")) # best few -- alternative syntax

## order by value.  Within those values the same to 3 decimals order by fevals.
## summary(ans8, order = list(round(value, 3), fevals), par.select = FALSE)
summary(ans8, order = "list(round(value, 3), fevals)", par.select = FALSE)

## summary(ans8, order = rownames, par.select = FALSE) # order by method name
summary(ans8, order = "rownames", par.select = FALSE) # same

summary(ans8, order = NULL, par.select = FALSE) # use input order
## summary(ans8, par.select = FALSE) # same

</code></pre>

<hr>
<h2 id='optchk'>General-purpose optimization</h2><span id='topic+optchk'></span>

<h3>Description</h3>

<p>A wrapper function that attempts to check the objective function,
and optionally the gradient and hessian functions, supplied by the
user for optimization. It also tries to check the scale of the
parameters and bounds to see if they are reasonable. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optchk(par, fn, gr=NULL, hess=NULL, lower=-Inf, upper=Inf, 
            control=list(), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optchk_+3A_par">par</code></td>
<td>
<p>a vector of initial values for the parameters 
for which optimal values are to be found. Names on the elements
of this vector are preserved and used in the results data frame.</p>
</td></tr>  
<tr><td><code id="optchk_+3A_fn">fn</code></td>
<td>
<p>A function to be minimized (or maximized), with first
argument the vector of parameters over which minimization is to take
place.  It should return a scalar result.</p>
</td></tr>
<tr><td><code id="optchk_+3A_gr">gr</code></td>
<td>
<p>A function to return (as a vector) the gradient for those methods that 
can use this information.</p>
</td></tr>
<tr><td><code id="optchk_+3A_hess">hess</code></td>
<td>
<p>A function to return (as a symmetric matrix) the Hessian of the objective 
function for those methods that can use this information.</p>
</td></tr>
<tr><td><code id="optchk_+3A_lower">lower</code>, <code id="optchk_+3A_upper">upper</code></td>
<td>
<p>Bounds on the variables for methods such as <code>"L-BFGS-B"</code> that can
handle box (or bounds) constraints.</p>
</td></tr>
<tr><td><code id="optchk_+3A_control">control</code></td>
<td>
<p>A list of control parameters. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="optchk_+3A_...">...</code></td>
<td>
<p>For <code>optimx</code> further arguments to be passed to <code>fn</code> 
and <code>gr</code>; otherwise, further arguments are not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that arguments after <code>...</code> must be matched exactly.
</p>
<p>While it can be envisaged that a user would have an analytic hessian but not an analytic
gradient, we do NOT permit the user to test the hessian in this situation.
</p>
<p>Any names given to <code>par</code> will be copied to the vectors passed to
<code>fn</code> and <code>gr</code>.  Note that no other attributes of <code>par</code>
are copied over. (We have not verified this as at 2009-07-29.)
</p>


<h3>Value</h3>

<p>A list of the following items:
</p>

<dl>
<dt>grOK</dt><dd><p>TRUE if the analytic gradient and a numerical approximation via <code>numDeriv</code>
agree within the <code>control$grtesttol</code> as per the <code>R</code> code in function 
<code>grchk</code>. <code>NULL</code> if no analytic gradient function is provided. </p>
</dd> 
<dt>hessOK</dt><dd><p>TRUE if the analytic hessian and a numerical approximation via <code>numDeriv::jacobian</code>
agree within the <code>control$hesstesttol</code> as per the <code>R</code> code in function 
<code>hesschk</code>. NULL if no analytic hessian or no analytic gradient is provided. Note
that since an analytic gradient must be available for this test, we use the Jacobian of the 
gradient to compute the Hessian to avoid one level of differencing, though the <code>hesschk</code>
function can work without the gradient.</p>
</dd>
<dt>scalebad</dt><dd><p>TRUE if the larger of the <code>scaleratios</code> exceeds <code>control$scaletol</code></p>
</dd>
<dt>scaleratios</dt><dd><p>A vector of the parameter and bounds scale ratios. See the function code
of <code>scalechk</code> for the computation of these values.</p>
</dd>
</dl>



<h3>References</h3>

<p>See the manual pages for <code>optim()</code> and the packages the DESCRIPTION <code>suggests</code>.
</p>
<p>Nash JC, and Varadhan R (2011). Unifying Optimization Algorithms to Aid Software System Users: 
<b>optimx</b> for R., <em>Journal of Statistical Software</em>, 43(9), 1-14.,  
URL http://www.jstatsoft.org/v43/i09/.
</p>
<p>Nash JC (2014). On Best Practice Optimization Methods in R., 
<em>Journal of Statistical Software</em>, 60(2), 1-14.,
URL http://www.jstatsoft.org/v60/i02/.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>fr &lt;- function(x) {   ## Rosenbrock Banana function
    x1 &lt;- x[1]
    x2 &lt;- x[2]
    100 * (x2 - x1 * x1)^2 + (1 - x1)^2
}
grr &lt;- function(x) { ## Gradient of 'fr'
    x1 &lt;- x[1]
    x2 &lt;- x[2]
    c(-400 * x1 * (x2 - x1 * x1) - 2 * (1 - x1),
       200 *      (x2 - x1 * x1))
}

myctrl&lt;- ctrldefault(2)
myctrl$trace &lt;- 3
mychk &lt;- optchk(par=c(-1.2,1), fr, grr, lower=rep(-10,2), upper=rep(10,2), control=myctrl)
cat("result of optchk\n")
print(mychk)

</code></pre>

<hr>
<h2 id='optimr'>General-purpose optimization</h2><span id='topic+optimr'></span>

<h3>Description</h3>

<p>General-purpose optimization wrapper function that calls other
R tools for optimization, including the existing optim() function.
<code>optimr</code> also tries to unify the calling sequence to allow
a number of tools to use the same front-end, in fact using the
calling sequence of the R function <code>optim()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optimr(par, fn, gr=NULL, hess=NULL, method=NULL, lower=-Inf, upper=Inf, 
          hessian=FALSE, control=list(), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optimr_+3A_par">par</code></td>
<td>
<p>a vector of initial values for the parameters 
for which optimal values are to be found. Names on the elements
of this vector are preserved and used in the results data frame.</p>
</td></tr>  
<tr><td><code id="optimr_+3A_fn">fn</code></td>
<td>
<p>A function to be minimized (or maximized), with first
argument the vector of parameters over which minimization is to take
place.  It should return a scalar result.</p>
</td></tr>
<tr><td><code id="optimr_+3A_gr">gr</code></td>
<td>
<p>A function to return (as a vector) the gradient for those methods that 
can use this information.
</p>
<p>If <code>gr</code> is <code>NULL</code>, then this is passed forward and 
whatever default action is specified for the 
chosen method for the case of a null gradient code
is used. For many methods, this is a finite-difference approximation, 
but some methods require user input for the gradient and will fail
otherwise. In such cases, we try to return <code>convergence</code> of 9998. 
</p>
<p>If <code>gr</code> is a character string, then that string is taken as the name of 
a gradient approximation function, for example, &quot;grfwd&quot;, &quot;grback&quot; and
&quot;grcentral&quot; for standard forward, backward and central approximations.
Method &quot;grnd&quot; uses the <code>grad()</code> function from package <code>numDeriv</code>.
</p>
</td></tr>   
<tr><td><code id="optimr_+3A_hess">hess</code></td>
<td>
<p>A function to return (as a matrix) the hessian for those methods that
can use this information.
</p>
<p>If <code>hess</code> is the character string &quot;approx&quot;, then ??
</p>
<p>If <code>hess</code> is <code>NULL</code>, then this is passed forward and 
whatever default action is specified for the 
chosen method for the case of a null gradient code is used. 
</p>
</td></tr>
<tr><td><code id="optimr_+3A_lower">lower</code>, <code id="optimr_+3A_upper">upper</code></td>
<td>
<p>Bounds on the variables for methods such as <code>"L-BFGS-B"</code> that can
handle box (or bounds) constraints. A small set of methods can handle masks, that is,
fixed parameters, and these can be specified by making the lower and upper bounds
equal to the starting value. (It is possible that the starting value could be different
from the lower/upper bounds set,
but this behaviour has NOT yet been defined and users are cautioned.)</p>
</td></tr>
<tr><td><code id="optimr_+3A_method">method</code></td>
<td>
<p>A character string giving the name of the optimization method to be
applied. See the list <code>allmeth</code> in file 
<code>ctrldefault.R</code> which is part of this package.</p>
</td></tr>
<tr><td><code id="optimr_+3A_hessian">hessian</code></td>
<td>
<p>A logical control that if TRUE forces the computation of an approximation 
to the Hessian at the final set of parameters. Note that this will NOT necessarily
use the same approximation as may be provided by the method called. Instead, 
the function <code>hessian()</code> from package <code>numDeriv</code> is used if no gradient
<code>gr</code> is supplied, else the function <code>jacobian()</code> from <code>numDeriv</code>
is applied to the gradient function <code>gr</code>.</p>
</td></tr>
<tr><td><code id="optimr_+3A_control">control</code></td>
<td>
<p>A list of control parameters. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="optimr_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to <code>fn</code> 
and <code>gr</code> if needed for computation of these quantities; otherwise, further 
arguments are not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that arguments after <code>...</code> should be matched exactly.
</p>
<p>By default <code>optimr</code> performs minimization, but it will maximize
if <code>control$maximize</code> is TRUE. The original optim() function allows
<code>control$fnscale</code> to be set negative to accomplish this, and this
control can be used with <code>optimr</code> but is deprecated. Moreover, 
if <code>control$maximize</code> is set, it will
take precedence over <code>control$fnscale</code>. Generally it is a BAD IDEA
to use both mechanisms simultaneously. 
</p>
<p>Possible method choices are specified by the list <code>allmeth</code> in the file
<code>ctrldefault.R</code> which is part of this package. 
</p>
<p>If no method is specified, the method specified by <code>defmethod</code> in file 
<code>ctrldefault.R</code> (which is part of this package) will be attempted. 
</p>
<p>Function <code>fn</code> must return a finite scalar value at the initial set
of parameters. Some methods can handle a returned value <code>NA</code> or <code>Inf</code> 
if the function cannot be evaluated at the supplied value. However, other 
methods, of which <code>"L-BFGS-B"</code> is known to be a case, require that 
the values returned should always be finite. It is recommended that user functions
ALWAYS return a usable value. Note that the control <code>badval</code> in 
<code>ctrldefault.R</code> give a possible number that could be returned.
</p>
<p>For details of methods, please consult the documentation of the individual methods.
(The NAMESPACE file lists the packages from which functions are imported.)
Note that method <code>"hjn"</code> is a conservative implementation of a Hooke and 
Jeeves (1961) and is part of this package. It is provided as a simple example of
a very crude optimization method; it is NOT intended as a production method, but
may be useful for didactic purposes.
</p>
<p>The <code>control</code> argument is a list that can supply any of the
components in the file <code>ctrldefault.R</code> which is part of this 
package. It may supply controls that are
useful or required for particular methods, but users are warned to be careful to
ensure that extraneous or incorrect components and values are not passed.
Some <code>control</code> elements apply only to some methods. 
See individual packages for details. <code>optimr</code> does not support all the
possible controls for all methods.
</p>
<p>A particular case is the method &quot;bobyqa&quot;, where the control <code>rhobeg=0</code> 
gives a set of controls that depend on the bounds supplied. This choice is
only in the current package. Unspecified or negative control <code>rhobeg=0</code>
gives the minqa defaults. Positive value of this control (and optionally 
control <code>rhoend</code>) supply those values. 
See inst/doc/examples/specctrlhobbs.R.
</p>
<p>Any names given to <code>par</code> will be copied to the vectors passed to
<code>fn</code> and <code>gr</code>.  Apparently no other attributes of <code>par</code>
are copied over, but this may need to be verified, especially if parameters
are passed to non-R routines.
</p>
<p>CAUTION: because there is a seldom-used parameter <code>hess</code>, you should NOT
make a call like 
</p>
<p>ans &lt;- optimr(start, myf, myg, lower, upper)
</p>
<p>or you will likely get wrong results. Instead use
</p>
<p>ans &lt;-  optimr(start, myf, myg, lower=lower, upper=upper)
</p>


<h3>Value</h3>

<p>A list with components:
</p>

<dl>
<dt>par</dt><dd><p>The best set of parameters found.</p>
</dd>
<dt>value</dt><dd><p>The value of fn corresponding to par.</p>
</dd>
<dt>counts</dt><dd><p> A two-element integer vector giving the number of calls to
fn and gr respectively. This excludes those calls needed
to compute the Hessian, if requested, and any calls to fn
to compute a finite-difference approximation to the gradient.
NOT available to be reported for some methods, e.g.,  <code>lbfgs</code>.</p>
</dd>
<dt>convergence</dt><dd><p> An integer code. 0 indicates successful completion. The
documentation for function <code>opm()</code> gives some other possible values and
their meaning. </p>
</dd>
<dt> message</dt><dd><p> A character string giving any additional information returned
by the optimizer, or NULL.</p>
</dd>
<dt>hessian</dt><dd><p>If requested, an approximation to the hessian of fn
at the final parameters.</p>
</dd>
</dl>



<h3>References</h3>

<p>See the manual pages for <code>optim()</code>.
</p>
<p>Hooke R. and Jeeves, TA (1961). Direct search solution of numerical and statistical problems. 
Journal of the Association for Computing Machinery (ACM). 8 (2): 212229.
</p>
<p>Nash JC, and Varadhan R (2011). Unifying Optimization Algorithms to Aid Software System Users: 
<b>optimx</b> for R., <em>Journal of Statistical Software</em>, 43(9), 1-14.,  
URL http://www.jstatsoft.org/v43/i09/.
</p>
<p>Nocedal J, and Wright SJ (1999). Numerical optimization. New York: Springer. 2nd Edition 2006.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> # Simple Test Function 1:
simfun.f = function(x) {
     fun &lt;- sum(x^2 )
## if (trace) ... to be fixed
	print(c(x = x, fun = fun))
     fun
}
simfun.g = function(x) {
     grad&lt;-2.0*x
     grad
}
simfun.h = function(x) {
     n&lt;-length(x)
     t&lt;-rep(2.0,n)
     hess&lt;-diag(t)
}

strt &lt;- c(1,2,3)
ansfgh &lt;- optimr(strt, simfun.f, simfun.g, simfun.h, method="nlm",
     hessian=TRUE, control=list(trace=2))
proptimr(ansfgh) # compact output of result


</code></pre>

<hr>
<h2 id='optimx'>General-purpose optimization</h2><span id='topic+optimx'></span><span id='topic++5B.optimx'></span><span id='topic+as.data.frame.optimx'></span>

<h3>Description</h3>

<p>General-purpose optimization wrapper function that calls other
R tools for optimization, including the existing optim() function.
<code>optimx</code> also tries to unify the calling sequence to allow
a number of tools to use the same front-end. These include 
<code>spg</code> from the BB package, <code>ucminf</code>, <code>nlm</code>, and 
<code>nlminb</code>. Note that 
optim() itself allows Nelder&ndash;Mead, quasi-Newton and 
conjugate-gradient algorithms as well as box-constrained optimization 
via L-BFGS-B. Because SANN does not return a meaningful convergence code
(conv), optimx() does not call the SANN method.
</p>
<p>Note that package <code>optimr</code> allows solvers to be called individually
by the <code>optim()</code> syntax, with the <code>parscale</code> 
control to scale parameters applicable to all methods. However, 
running multiple methods, or using the <code>follow.on</code> capability
has been moved to separate routines in the <code>optimr</code> package.
</p>
<p>Cautions: 
</p>
<p>1) Using some control list options with different or multiple methods 
may give unexpected results. 
</p>
<p>2) Testing the KKT conditions can take much longer than solving the
optimization problem, especially when the number of parameters is large
and/or analytic gradients are not available. Note that the default for
the control <code>kkt</code> is TRUE.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optimx(par, fn, gr=NULL, hess=NULL, lower=-Inf, upper=Inf, 
            method=c("Nelder-Mead","BFGS"), itnmax=NULL, hessian=FALSE,
            control=list(),
             ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optimx_+3A_par">par</code></td>
<td>
<p>a vector of initial values for the parameters 
for which optimal values are to be found. Names on the elements
of this vector are preserved and used in the results data frame.</p>
</td></tr>  
<tr><td><code id="optimx_+3A_fn">fn</code></td>
<td>
<p>A function to be minimized (or maximized), with first
argument the vector of parameters over which minimization is to take
place.  It should return a scalar result.</p>
</td></tr>
<tr><td><code id="optimx_+3A_gr">gr</code></td>
<td>
<p>A function to return (as a vector) the gradient for those methods that 
can use this information.
</p>
<p>If 'gr' is <code>NULL</code>, a finite-difference approximation will be used.
An open question concerns whether the SAME approximation code used for all methods, 
or whether there are differences that could/should be examined? </p>
</td></tr>
<tr><td><code id="optimx_+3A_hess">hess</code></td>
<td>
<p>A function to return (as a symmetric matrix) the Hessian of the objective 
function for those methods that can use this information.</p>
</td></tr>
<tr><td><code id="optimx_+3A_lower">lower</code>, <code id="optimx_+3A_upper">upper</code></td>
<td>
<p>Bounds on the variables for methods such as <code>"L-BFGS-B"</code> that can
handle box (or bounds) constraints.</p>
</td></tr>
<tr><td><code id="optimx_+3A_method">method</code></td>
<td>
<p>A list of the methods to be used. 
Note that this is an important change from optim() that allows
just one method to be specified. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="optimx_+3A_itnmax">itnmax</code></td>
<td>
<p>If provided as a vector of the same length as the list of methods <code>method</code>, 
gives the maximum number of iterations or function values for the corresponding 
method. If a single number is provided, this will be used for all methods. Note that
there may be control list elements with similar functions, but this should be the
preferred approach when using <code>optimx</code>.</p>
</td></tr>
<tr><td><code id="optimx_+3A_hessian">hessian</code></td>
<td>
<p>A logical control that if TRUE forces the computation of an approximation 
to the Hessian at the final set of parameters. If FALSE (default), the hessian is
calculated if needed to provide the KKT optimality tests (see <code>kkt</code> in
&lsquo;Details&rsquo; for the <code>control</code> list).
This setting is provided primarily for compatibility with optim().</p>
</td></tr>
<tr><td><code id="optimx_+3A_control">control</code></td>
<td>
<p>A list of control parameters. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="optimx_+3A_...">...</code></td>
<td>
<p>For <code>optimx</code> further arguments to be passed to <code>fn</code> 
and <code>gr</code>; otherwise, further arguments are not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that arguments after <code>...</code> must be matched exactly.
</p>
<p>By default this function performs minimization, but it will maximize
if <code>control$maximize</code> is TRUE. The original optim() function allows
<code>control$fnscale</code> to be set negative to accomplish this. DO NOT
use both methods. 
</p>
<p>Possible method codes at the time of writing are 'Nelder-Mead', 'BFGS',
'CG', 'L-BFGS-B', 'nlm', 'nlminb', 'spg', 'ucminf', 'newuoa', 'bobyqa',
'nmkb', 'hjkb', 'Rcgmin', or 'Rvmmin'.
</p>
<p>The default methods for unconstrained problems (no <code>lower</code> or
<code>upper</code> specified) are an implementation of the Nelder and Mead
(1965) and a Variable Metric method based on the ideas of Fletcher
(1970) as modified by him in conversation with Nash (1979). Nelder-Mead
uses only function values and is robust but relatively slow.  It will 
work reasonably well for non-differentiable functions. The Variable
Metric method, <code>"BFGS"</code> updates an approximation to the inverse
Hessian using the BFGS update formulas, along with an acceptable point
line search strategy. This method appears to work best with analytic
gradients. (<code>"Rvmmmin"</code> provides a box-constrained version of this
algorithm.
</p>
<p>If no <code>method</code> is given, and there are bounds constraints provided,
the method is set to <code>"L-BFGS-B"</code>.
</p>
<p>Method <code>"CG"</code> is a conjugate gradients method based on that by
Fletcher and Reeves (1964) (but with the option of Polak&ndash;Ribiere or
Beale&ndash;Sorenson updates). The particular implementation is now dated,
and improved yet simpler codes are being implemented (as at June 2009),
and furthermore a version with box constraints is being tested.
Conjugate gradient methods will generally be more fragile than the 
BFGS method, but as they do not store a matrix they may be successful 
in much larger optimization problems.
</p>
<p>Method <code>"L-BFGS-B"</code> is that of Byrd <em>et. al.</em> (1995) which
allows <em>box constraints</em>, that is each variable can be given a lower
and/or upper bound. The initial value must satisfy the constraints.
This uses a limited-memory modification of the BFGS quasi-Newton
method. If non-trivial bounds are supplied, this method will be
selected, with a warning.
</p>
<p>Nocedal and Wright (1999) is a comprehensive reference for the
previous three methods.
</p>
<p>Function <code>fn</code> can return <code>NA</code> or <code>Inf</code> if the function
cannot be evaluated at the supplied value, but the initial value must
have a computable finite value of <code>fn</code>. However, some methods, of
which <code>"L-BFGS-B"</code> is known to be a case, require that the values
returned should always be finite.
</p>
<p>While <code>optim</code> can be used recursively, and for a single parameter
as well as many, this may not be true for <code>optimx</code>. <code>optim</code>
also accepts a zero-length <code>par</code>, and just evaluates the function 
with that argument.
</p>
<p>Method <code>"nlm"</code> is from the package of the same name that implements
ideas of Dennis and Schnabel (1983) and Schnabel et al. (1985). See nlm()
for more details.
</p>
<p>Method <code>"nlminb"</code> is the package of the same name that uses the
minimization tools of the PORT library.  The PORT documentation is at 
&lt;URL: http://netlib.bell-labs.com/cm/cs/cstr/153.pdf&gt;. See nlminb()
for details. (Though there is very little information about the methods.)
</p>
<p>Method <code>"spg"</code> is from package BB implementing a spectral projected 
gradient method for large-scale optimization with simple constraints due
R adaptation, with significant modifications, by Ravi Varadhan,
Johns Hopkins University (Varadhan and Gilbert, 2009), from the original
FORTRAN code of Birgin, Martinez, and Raydan (2001). 
</p>
<p>Method <code>"Rcgmin"</code> is from the package of that name. It implements a
conjugate gradient algorithm with the Dai and Yuan (2001) update and also 
allows bounds constraints on the parameters. (Rcgmin also allows mask 
constraints &ndash; fixing individual parameters.) 
</p>
<p>Methods <code>"bobyqa"</code>, <code>"uobyqa"</code> and <code>"newuoa"</code> are from the 
package <code>"minqa"</code> which implement optimization by quadratic approximation
routines of the similar names due to M J D Powell (2009). See package minqa 
for details. Note that <code>"uobyqa"</code> and <code>"newuoa"</code> are for 
unconstrained minimization, while <code>"bobyqa"</code> is for box constrained
problems. While <code>"uobyqa"</code> may be specified, it is NOT part of the 
<code>all.methods = TRUE</code> set.
</p>
<p>The <code>control</code> argument is a list that can supply any of the
following components:
</p>

<dl>
<dt><code>trace</code></dt><dd><p>Non-negative integer. If positive,
tracing information on the
progress of the optimization is produced. Higher values may
produce more tracing information: for method <code>"L-BFGS-B"</code>
there are six levels of tracing. trace = 0 gives no output 
(To understand exactly what these do see the source code: higher 
levels give more detail.)</p>
</dd>
<dt><code>follow.on </code></dt><dd><p> = TRUE or FALSE. If TRUE, and there are multiple 
methods, then the last set of 
parameters from one method is used as the starting set for the next. </p>
</dd>
<dt><code>save.failures</code></dt><dd><p> = TRUE if we wish to keep &quot;answers&quot; from runs 
where the method does not return convcode==0. FALSE otherwise (default).</p>
</dd>
<dt><code>maximize</code></dt><dd><p> = TRUE if we want to maximize rather than minimize 
a function. (Default FALSE). Methods nlm, nlminb, ucminf cannot maximize a
function, so the user must explicitly minimize and carry out the adjustment
externally. However, there is a check to avoid
usage of these codes when maximize is TRUE. See <code>fnscale</code> below for 
the method used in <code>optim</code> that we deprecate.</p>
</dd>
<dt><code>all.methods</code></dt><dd><p>= TRUE if we want to use all available (and suitable)
methods.</p>
</dd>
<dt><code>kkt</code></dt><dd><p>=FALSE if we do NOT want to test the Kuhn, Karush, Tucker
optimality conditions. The default is TRUE. However, because the Hessian
computation may be very slow, we set <code>kkt</code> to be FALSE if there are 
more than than 50 parameters when the gradient function <code>gr</code> is not 
provided, and more than 500
parameters when such a function is specified. We return logical values <code>KKT1</code>
and <code>KKT2</code> TRUE if first and second order conditions are satisfied approximately.
Note, however, that the tests are sensitive to scaling, and users may need
to perform additional verification. If <code>kkt</code> is FALSE but <code>hessian</code>
is TRUE, then <code>KKT1</code> is generated, but <code>KKT2</code> is not.</p>
</dd>
<dt><code>all.methods</code></dt><dd><p>= TRUE if we want to use all available (and suitable)
methods.</p>
</dd>
<dt><code>kkttol</code></dt><dd><p>= value to use to check for small gradient and negative
Hessian eigenvalues. Default = .Machine$double.eps^(1/3) </p>
</dd>
<dt><code>kkt2tol</code></dt><dd><p>= Tolerance for eigenvalue ratio in KKT test of positive 
definite Hessian. Default same as for kkttol </p>
</dd>
<dt><code>starttests</code></dt><dd><p>= TRUE if we want to run tests of the function and 
parameters: feasibility relative to bounds, analytic vs numerical gradient, 
scaling tests, before we try optimization methods. Default is TRUE.</p>
</dd>
<dt><code>dowarn</code></dt><dd><p>= TRUE if we want warnings generated by optimx. Default is 
TRUE.</p>
</dd>
<dt><code>badval</code></dt><dd><p>= The value to set for the function value when try(fn()) fails.
Default is (0.5)*.Machine$double.xmax </p>
</dd>
<dt><code>usenumDeriv</code></dt><dd><p>= TRUE if the <code>numDeriv</code> function <code>grad()</code> is
to be used to compute gradients when the argument <code>gr</code> is NULL or not supplied.</p>
</dd>
</dl>

<p>The following <code>control</code> elements apply only to some of the methods. The list
may be incomplete. See individual packages for details. 
</p>

<dl>
<dt><code>fnscale</code></dt><dd><p>An overall scaling to be applied to the value
of <code>fn</code> and <code>gr</code> during optimization. If negative,
turns the problem into a maximization problem. Optimization is
performed on <code>fn(par)/fnscale</code>. For methods from the set in
<code>optim()</code>. Note potential conflicts with the control <code>maximize</code>.</p>
</dd>
<dt><code>parscale</code></dt><dd><p>A vector of scaling values for the parameters.
Optimization is performed on <code>par/parscale</code> and these should be
comparable in the sense that a unit change in any element produces
about a unit change in the scaled value.For <code>optim</code>.</p>
</dd>
<dt><code>ndeps</code></dt><dd><p>A vector of step sizes for the finite-difference
approximation to the gradient, on <code>par/parscale</code>
scale. Defaults to <code>1e-3</code>. For <code>optim</code>.</p>
</dd>
<dt><code>maxit</code></dt><dd><p>The maximum number of iterations. Defaults to
<code>100</code> for the derivative-based methods, and
<code>500</code> for <code>"Nelder-Mead"</code>.</p>
</dd>
<dt><code>abstol</code></dt><dd><p>The absolute convergence tolerance. Only
useful for non-negative functions, as a tolerance for reaching zero.</p>
</dd>
<dt><code>reltol</code></dt><dd><p>Relative convergence tolerance.  The algorithm
stops if it is unable to reduce the value by a factor of
<code>reltol * (abs(val) + reltol)</code> at a step.  Defaults to
<code>sqrt(.Machine$double.eps)</code>, typically about <code>1e-8</code>. For <code>optim</code>.</p>
</dd>
<dt><code>alpha</code>, <code>beta</code>, <code>gamma</code></dt><dd><p>Scaling parameters
for the <code>"Nelder-Mead"</code> method. <code>alpha</code> is the reflection
factor (default 1.0), <code>beta</code> the contraction factor (0.5) and
<code>gamma</code> the expansion factor (2.0).</p>
</dd>
<dt><code>REPORT</code></dt><dd><p>The frequency of reports for the <code>"BFGS"</code> and
<code>"L-BFGS-B"</code> methods if <code>control$trace</code>
is positive. Defaults to every 10 iterations for <code>"BFGS"</code> and
<code>"L-BFGS-B"</code>.</p>
</dd>
<dt><code>type</code></dt><dd><p>for the conjugate-gradients method. Takes value
<code>1</code> for the Fletcher&ndash;Reeves update, <code>2</code> for
Polak&ndash;Ribiere and <code>3</code> for Beale&ndash;Sorenson.</p>
</dd>
<dt><code>lmm</code></dt><dd><p>is an integer giving the number of BFGS updates
retained in the <code>"L-BFGS-B"</code> method, It defaults to <code>5</code>.</p>
</dd>
<dt><code>factr</code></dt><dd><p>controls the convergence of the <code>"L-BFGS-B"</code>
method. Convergence occurs when the reduction in the objective is
within this factor of the machine tolerance. Default is <code>1e7</code>,
that is a tolerance of about <code>1e-8</code>.</p>
</dd>
<dt><code>pgtol</code></dt><dd><p>helps control the convergence of the <code>"L-BFGS-B"</code>
method. It is a tolerance on the projected gradient in the current
search direction. This defaults to zero, when the check is
suppressed.</p>
</dd>
</dl>

<p>Any names given to <code>par</code> will be copied to the vectors passed to
<code>fn</code> and <code>gr</code>.  Note that no other attributes of <code>par</code>
are copied over. (We have not verified this as at 2009-07-29.)
</p>
<p>There are <code>[.optimx</code>, <code>as.data.frame.optimx</code>, <code><a href="#topic+coef.optimx">coef.optimx</a></code> 
and <code><a href="#topic+summary.optimx">summary.optimx</a></code> methods available.
</p>
<p>Note: Package <code>optimr</code> is a derivative of this package. It was developed
initially to overcome maintenance difficulties with the current package 
related to avoiding confusion if some multiple options were specified together,
and to allow the <code>optim()</code> function syntax to be used consistently, 
including the <code>parscale</code> control. However, this package does perform
well, and is called by a number of popular other packages.
</p>


<h3>Value</h3>

<p>If there are <code>npar</code> parameters, then the result is a dataframe having one row
for each method for which results are reported, using the method as the row name,
with columns
</p>
<p><code>par_1, .., par_npar, value, fevals, gevals, niter, convcode, kkt1, kkt2, xtimes</code>
</p>
<p>where
</p>

<dl>
<dt>par_1</dt><dd><p> .. </p>
</dd>
<dt>par_npar</dt><dd><p>The best set of parameters found.</p>
</dd>
<dt>value</dt><dd><p>The value of <code>fn</code> corresponding to <code>par</code>.</p>
</dd>
<dt>fevals</dt><dd><p>The number of calls to <code>fn</code>.</p>
</dd>
<dt>gevals</dt><dd><p>The number of calls to <code>gr</code>. This excludes those calls needed
to compute the Hessian, if requested, and any calls to <code>fn</code> to
compute a finite-difference approximation to the gradient.</p>
</dd>
<dt>niter</dt><dd><p>For those methods where it is reported, the number of &ldquo;iterations&rdquo;. See
the documentation or code for particular methods for the meaning of such counts.</p>
</dd>
<dt>convcode</dt><dd><p>An integer code. <code>0</code> indicates successful
convergence. Various methods may or may not return sufficient information
to allow all the codes to be specified. An incomplete list of codes includes
</p>

<dl>
<dt><code>1</code></dt><dd><p>indicates that the iteration limit <code>maxit</code>
had been reached.</p>
</dd>
<dt><code>20</code></dt><dd><p>indicates that the initial set of parameters is inadmissible, that is,
that the function cannot be computed or returns an infinite, NULL, or NA value.</p>
</dd>
<dt><code>21</code></dt><dd><p>indicates that an intermediate set of parameters is inadmissible.</p>
</dd>
<dt><code>10</code></dt><dd><p>indicates degeneracy of the Nelder&ndash;Mead simplex.</p>
</dd>
<dt><code>51</code></dt><dd><p>indicates a warning from the <code>"L-BFGS-B"</code>
method; see component <code>message</code> for further details.</p>
</dd>
<dt><code>52</code></dt><dd><p>indicates an error from the <code>"L-BFGS-B"</code>
method; see component <code>message</code> for further details.</p>
</dd>
</dl>

</dd>
<dt>kkt1</dt><dd><p>A logical value returned TRUE if the solution reported has a &ldquo;small&rdquo; gradient.</p>
</dd>
<dt>kkt2</dt><dd><p>A logical value returned TRUE if the solution reported appears to have a 
positive-definite Hessian.</p>
</dd>
<dt>xtimes</dt><dd><p>The reported execution time of the calculations for the particular method.</p>
</dd>
</dl>

<p>The attribute &quot;details&quot; to the returned answer object contains information,
if computed, on the gradient (<code>ngatend</code>) and Hessian matrix (<code>nhatend</code>) 
at the supposed optimum, along with the eigenvalues of the Hessian (<code>hev</code>), 
as well as the <code>message</code>, if any, returned by the computation for each <code>method</code>,
which is included for each row of the <code>details</code>. 
If the returned object from optimx() is <code>ans</code>, this is accessed 
via the construct
<code>attr(ans, "details")</code>
</p>
<p>This object is a  matrix based on a list so that if ans is the output of optimx
then attr(ans, &quot;details&quot;)[1, ] gives the first row and 
attr(ans,&quot;details&quot;)[&quot;Nelder-Mead&quot;, ] gives the Nelder-Mead row. There is 
one row for each method that has been successful 
or that has been forcibly saved by save.failures=TRUE. 
</p>
<p>There are also attributes
</p>

<dl>
<dt>maximize</dt><dd><p>to indicate we have been maximizing the objective</p>
</dd>
<dt>npar</dt><dd><p>to provide the number of parameters, thereby facilitating easy
extraction of the parameters from the results data frame</p>
</dd>
<dt>follow.on</dt><dd><p>to indicate that the results have been computed sequentially,
using the order provided by the user, with the best parameters from one
method used to start the next. There is an example (<code>ans9</code>) in 
the script <code>ox.R</code> in the demo directory of the package.</p>
</dd>
</dl>



<h3>Note</h3>

<p>Most methods in <code>optimx</code> will work with one-dimensional <code>par</code>s, but such
use is NOT recommended. Use <code><a href="stats.html#topic+optimize">optimize</a></code> or other one-dimensional methods instead.
</p>
<p>There are a series of demos available. Once the package is loaded (via <code>require(optimx)</code> or
<code>library(optimx)</code>, you may see available demos via 
</p>
<p>demo(package=&quot;optimx&quot;)
</p>
<p>The demo 'brown_test' may be run with the command
demo(brown_test, package=&quot;optimx&quot;)
</p>
<p>The package source contains several functions that are not exported in the
NAMESPACE. These are 
</p>

<dl>
<dt><code>optimx.setup()</code></dt><dd><p> which establishes the controls for a given run;</p>
</dd>
<dt><code>optimx.check()</code></dt><dd><p> which performs bounds and gradient checks on
the supplied parameters and functions;</p>
</dd>
<dt><code>optimx.run()</code></dt><dd><p>which actually performs the optimization and post-solution
computations;</p>
</dd>
<dt><code>scalecheck()</code></dt><dd><p> which actually carries out a check on the relative scaling
of the input parameters.</p>
</dd>
</dl>

<p>Knowledgeable users may take advantage of these functions if they are carrying
out production calculations where the setup and checks could be run once.
</p>


<h3>Source</h3>

<p>See the manual pages for <code>optim()</code> and the packages the DESCRIPTION <code>suggests</code>.
</p>


<h3>References</h3>

<p>See also  the manual pages for <code>optim()</code> and the packages the DESCRIPTION <code>suggests</code>.
</p>
<p>Byrd RH, Lu P, Nocedal J (1995) A Limited Memory Algorithm for Bound Constrained Optimization,
SIAM Journal on Scientific Computing, 16 (5), 1190&ndash;1208.
</p>
<p>Y. H. Dai and Y. Yuan, (2001) An Efficient Hybrid Conjugate Gradient Method for Unconstrained
Optimization, Annals of Operations Research, 103, pp 33&ndash;47, 
URL http://dx.doi.org/10.1023/A:1012930416777.
</p>
<p>Dennis JE and Schnabel RB (1983) Numerical Methods for Unconstrained Optimization and Nonlinear Equations,
Englewood Cliffs NJ: Prentice-Hall.
</p>
<p>Fletcher R (1970) A New Approach to Variable Metric Algorithms, Computer Journal,
13 (3), 317-322.
</p>
<p>Nash JC, and Varadhan R (2011). Unifying Optimization Algorithms to Aid Software System Users: 
<b>optimx</b> for R., <em>Journal of Statistical Software</em>, 43(9), 1-14.,  
URL http://www.jstatsoft.org/v43/i09/.
</p>
<p>Nash JC (2014). On Best Practice Optimization Methods in R., 
<em>Journal of Statistical Software</em>, 60(2), 1-14.,
URL http://www.jstatsoft.org/v60/i02/.
</p>
<p>Nelder JA and Mead R (1965) A Simplex Method for Function Minimization,
Computer Journal, 7 (4), 308&ndash;313.
</p>
<p>Powell MJD (2009) The BOBYQA algorithm for bound constrained optimization 
without derivatives, http://www.damtp.cam.ac.uk/user/na/NA_papers/NA2009_06.pdf      
</p>


<h3>See Also</h3>

<p><code><a href="BB.html#topic+spg">spg</a></code>, <code><a href="stats.html#topic+nlm">nlm</a></code>, <code><a href="stats.html#topic+nlminb">nlminb</a></code>,
<code><a href="minqa.html#topic+bobyqa">bobyqa</a></code>, 
<code><a href="ucminf.html#topic+ucminf">ucminf</a></code>, 
<code><a href="dfoptim.html#topic+nmkb">nmkb</a></code>,
<code><a href="dfoptim.html#topic+hjkb">hjkb</a></code>.
<code><a href="stats.html#topic+optimize">optimize</a></code> for one-dimensional minimization;
<code><a href="stats.html#topic+constrOptim">constrOptim</a></code> or <code><a href="BB.html#topic+spg">spg</a></code> for linearly constrained optimization.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)
cat("Note demo(ox) for extended examples\n")


## Show multiple outputs of optimx using all.methods
# genrose function code
genrose.f&lt;- function(x, gs=NULL){ # objective function
## One generalization of the Rosenbrock banana valley function (n parameters)
	n &lt;- length(x)
        if(is.null(gs)) { gs=100.0 }
	fval&lt;-1.0 + sum (gs*(x[1:(n-1)]^2 - x[2:n])^2 + (x[2:n] - 1)^2)
        return(fval)
}

genrose.g &lt;- function(x, gs=NULL){
# vectorized gradient for genrose.f
# Ravi Varadhan 2009-04-03
	n &lt;- length(x)
        if(is.null(gs)) { gs=100.0 }
	gg &lt;- as.vector(rep(0, n))
	tn &lt;- 2:n
	tn1 &lt;- tn - 1
	z1 &lt;- x[tn] - x[tn1]^2
	z2 &lt;- 1 - x[tn]
	gg[tn] &lt;- 2 * (gs * z1 - z2)
	gg[tn1] &lt;- gg[tn1] - 4 * gs * x[tn1] * z1
	return(gg)
}

genrose.h &lt;- function(x, gs=NULL) { ## compute Hessian
   if(is.null(gs)) { gs=100.0 }
	n &lt;- length(x)
	hh&lt;-matrix(rep(0, n*n),n,n)
	for (i in 2:n) {
		z1&lt;-x[i]-x[i-1]*x[i-1]
		z2&lt;-1.0-x[i]
                hh[i,i]&lt;-hh[i,i]+2.0*(gs+1.0)
                hh[i-1,i-1]&lt;-hh[i-1,i-1]-4.0*gs*z1-4.0*gs*x[i-1]*(-2.0*x[i-1])
                hh[i,i-1]&lt;-hh[i,i-1]-4.0*gs*x[i-1]
                hh[i-1,i]&lt;-hh[i-1,i]-4.0*gs*x[i-1]
	}
        return(hh)
}

startx&lt;-4*seq(1:10)/3.
ans8&lt;-optimx(startx,fn=genrose.f,gr=genrose.g, hess=genrose.h, 
   control=list(all.methods=TRUE, save.failures=TRUE, trace=0), gs=10)
ans8
ans8[, "gevals"]
ans8["spg", ]
summary(ans8, par.select = 1:3)
summary(ans8, order = value)[1, ] # show best value
head(summary(ans8, order = value)) # best few
## head(summary(ans8, order = "value")) # best few -- alternative syntax

## order by value.  Within those values the same to 3 decimals order by fevals.
## summary(ans8, order = list(round(value, 3), fevals), par.select = FALSE)
summary(ans8, order = "list(round(value, 3), fevals)", par.select = FALSE)

## summary(ans8, order = rownames, par.select = FALSE) # order by method name
summary(ans8, order = "rownames", par.select = FALSE) # same

summary(ans8, order = NULL, par.select = FALSE) # use input order
## summary(ans8, par.select = FALSE) # same

</code></pre>

<hr>
<h2 id='polyopt'>General-purpose optimization - sequential application of methods</h2><span id='topic+polyopt'></span>

<h3>Description</h3>

<p>Multiple minimization methods are applied in sequence to a single problem,
with the output parameters of one method being used to start the next.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>polyopt(par, fn, gr=NULL, lower=-Inf, upper=Inf, 
            methcontrol=NULL, hessian=FALSE,
            control=list(),
             ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="polyopt_+3A_par">par</code></td>
<td>
<p>a vector of initial values for the parameters 
for which optimal values are to be found. Names on the elements
of this vector are preserved and used in the results data frame.</p>
</td></tr>  
<tr><td><code id="polyopt_+3A_fn">fn</code></td>
<td>
<p>A function to be minimized (or maximized), with first
argument the vector of parameters over which minimization is to take
place.  It should return a scalar result.</p>
</td></tr>
<tr><td><code id="polyopt_+3A_gr">gr</code></td>
<td>
<p>A function to return (as a vector) the gradient for those methods that 
can use this information.
</p>
<p>If 'gr' is <code>NULL</code>, a finite-difference approximation will be used.
An open question concerns whether the SAME approximation code used for all methods, 
or whether there are differences that could/should be examined? </p>
</td></tr>
<tr><td><code id="polyopt_+3A_lower">lower</code>, <code id="polyopt_+3A_upper">upper</code></td>
<td>
<p>Bounds on the variables for methods such as <code>"L-BFGS-B"</code> that can
handle box (or bounds) constraints.</p>
</td></tr>
<tr><td><code id="polyopt_+3A_methcontrol">methcontrol</code></td>
<td>
<p>An data frame of which each row gives an optimization method, a maximum
number of iterations and a maximum number of function evaluations allowed for that
method. Each method will be executed in turn until either the maximum iterations
or function evaluations are completed, whichever is first. The next method is then
executed starting with the best parameters found so far, else the function exits.</p>
</td></tr>
<tr><td><code id="polyopt_+3A_hessian">hessian</code></td>
<td>
<p>A logical control that if TRUE forces the computation of an approximation 
to the Hessian at the final set of parameters. If FALSE (default), the hessian is
calculated if needed to provide the KKT optimality tests (see <code>kkt</code> in
&lsquo;Details&rsquo; for the <code>control</code> list).
This setting is provided primarily for compatibility with optim().</p>
</td></tr>
<tr><td><code id="polyopt_+3A_control">control</code></td>
<td>
<p>A list of control parameters. See &lsquo;Details&rsquo;.</p>
</td></tr>
<tr><td><code id="polyopt_+3A_...">...</code></td>
<td>
<p>For <code>optimx</code> further arguments to be passed to <code>fn</code> 
and <code>gr</code>; otherwise, further arguments are not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that arguments after <code>...</code> must be matched exactly.
</p>
<p>See <code>optimr()</code> for other details.
</p>
<p>Note that this function does not (yet?) make use of a hess function
to compute the hessian.
</p>


<h3>Value</h3>

<p>An array with one row per method. Each row contains:
</p>
<table>
<tr><td><code>par</code></td>
<td>
<p>The best set of parameters found for the method in question.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>The value of fn corresponding to par.</p>
</td></tr>
<tr><td><code>counts</code></td>
<td>
<p> A two-element integer vector giving the number of calls to
fn and gr respectively. This excludes those calls needed
to compute the Hessian, if requested, and any calls to fn
to compute a finite-difference approximation to the gradient.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p> An integer code. 0 indicates successful completion</p>
</td></tr>
<tr><td><code>message</code></td>
<td>
<p> A character string giving any additional information returned
by the optimizer, or NULL.</p>
</td></tr>
<tr><td><code>hessian</code></td>
<td>
<p> Always NULL for this routine.</p>
</td></tr>
</table>


<h3>Source</h3>

<p>See the manual pages for <code>optim()</code> and the packages the DESCRIPTION <code>suggests</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>fnR &lt;- function (x, gs=100.0) 
{
    n &lt;- length(x)
    x1 &lt;- x[2:n]
    x2 &lt;- x[1:(n - 1)]
    sum(gs * (x1 - x2^2)^2 + (1 - x2)^2)
}
grR &lt;- function (x, gs=100.0) 
{
    n &lt;- length(x)
    g &lt;- rep(NA, n)
    g[1] &lt;- 2 * (x[1] - 1) + 4*gs * x[1] * (x[1]^2 - x[2])
    if (n &gt; 2) {
        ii &lt;- 2:(n - 1)
        g[ii] &lt;- 2 * (x[ii] - 1) + 4 * gs * x[ii] * (x[ii]^2 - x[ii + 
            1]) + 2 * gs * (x[ii] - x[ii - 1]^2)
    }
    g[n] &lt;- 2 * gs * (x[n] - x[n - 1]^2)
    g
}

x0 &lt;- rep(pi, 4)
mc &lt;- data.frame(method=c("Nelder-Mead","Rvmmin"), maxit=c(1000, 100), maxfeval= c(1000, 1000))

ans &lt;- polyopt(x0, fnR, grR, methcontrol=mc, control=list(trace=0))
ans
mc &lt;- data.frame(method=c("Nelder-Mead","Rvmmin"), maxit=c(100, 100), maxfeval= c(100, 1000))

ans &lt;- polyopt(x0, fnR, grR, methcontrol=mc, control=list(trace=0))
ans

mc &lt;- data.frame(method=c("Nelder-Mead","Rvmmin"), maxit=c(10, 100), maxfeval= c(10, 1000))

ans &lt;- polyopt(x0, fnR, grR, methcontrol=mc, control=list(trace=0))
ans



</code></pre>

<hr>
<h2 id='proptimr'>Compact display of an <code>optimr()</code> result object</h2><span id='topic+proptimr'></span>

<h3>Description</h3>

<p><code>proptimr</code> displays the contents of a result computed by <code>optimr()</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   proptimr(opres)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="proptimr_+3A_opres">opres</code></td>
<td>

<p>the object returned by function <code>optimr()</code>
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function is intended for output only.
</p>


<h3>Author(s)</h3>

<p>John C. Nash
</p>

<hr>
<h2 id='Rcgmin'>An R implementation of a nonlinear conjugate gradient algorithm with the Dai / Yuan
update and restart. Based on Nash (1979) Algorithm 22 for its main structure.</h2><span id='topic+Rcgmin'></span><span id='topic+ncg'></span><span id='topic+ncgqs'></span>

<h3>Description</h3>

<p>The purpose of <code>Rcgmin</code> is to minimize an unconstrained or bounds
(box) and mask constrained function 
of many parameters by a nonlinear conjugate gradients method. This code is
entirely in R to allow users to explore and understand the method. It also
allows bounds (or box) constraints and masks (equality constraints) to be
imposed on parameters. 
</p>
<p><code>Rcgmin</code> is a wrapper that calls <code>Rcgminu</code> for unconstrained 
problems, else <code>Rcgminb</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   Rcgmin(par, fn, gr, lower, upper, bdmsk, control = list(), ...)

   ncg(par, fn, gr, bds, control = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Rcgmin_+3A_par">par</code></td>
<td>
<p>A numeric vector of starting estimates.</p>
</td></tr>
<tr><td><code id="Rcgmin_+3A_fn">fn</code></td>
<td>
<p>A function that returns the value of the objective at the
supplied set of parameters <code>par</code> using auxiliary data in ....
The first argument of <code>fn</code> must be <code>par</code>. </p>
</td></tr>
<tr><td><code id="Rcgmin_+3A_gr">gr</code></td>
<td>
<p>A function that returns the gradient of the objective at the
supplied set of parameters <code>par</code> using auxiliary data in ....
The first argument of <code>fn</code> must be <code>par</code>. This function 
returns the gradient as a numeric vector.
</p>
<p>If <code>gr</code> is not provided or is NULL, then the simple forward 
gradient code <code>grfwd</code> is used. However,
we recommend
carefully coded and checked analytic derivatives for Rcgmin.
</p>
<p>The use of numerical gradients for Rcgmin is discouraged.
First, the termination
test uses a size measure on the gradient, and numerical gradient 
approximations can sometimes give results that are too large. Second,
if there are bounds constraints, the step(s) taken to calculate the
approximation to the derivative are NOT checked to see if they are
out of bounds, and the function may be undefined at the evaluation
point. 
</p>
<p>There is also the option of using the routines <code>grfwd</code>, <code>grback</code>, 
<code>grcentral</code> or <code>grnd</code>. The last
of these calls the <code>grad()</code> function from package numDeriv. These 
are called by putting the name of the (numerical) gradient function in 
quotation marks, e.g.,
</p>
<p>gr=&quot;grfwd&quot;
</p>
<p>to use the standard forward difference numerical approximation.
</p>
<p>Note that all but the <code>grnd</code> routine use a stepsize parameter that
can be redefined in a special scratchpad storage variable <code>deps</code>.
The default is <code>deps = 1e-07</code>.
However, redefining this is discouraged unless you understand what
you are doing. 
</p>
</td></tr>
<tr><td><code id="Rcgmin_+3A_lower">lower</code></td>
<td>
<p>A vector of lower bounds on the parameters.</p>
</td></tr>
<tr><td><code id="Rcgmin_+3A_upper">upper</code></td>
<td>
<p>A vector of upper bounds on the parameters.</p>
</td></tr>
<tr><td><code id="Rcgmin_+3A_bdmsk">bdmsk</code></td>
<td>
<p>An indicator vector, having 1 for each parameter that is &quot;free&quot; or
unconstrained, and 0 for any parameter that is fixed or MASKED for the
duration of the optimization.</p>
</td></tr>
<tr><td><code id="Rcgmin_+3A_bds">bds</code></td>
<td>
<p>A list of information resulting from function <code>bmchk</code> giving
information on the status of the parameters and bounds and masks.</p>
</td></tr>     
<tr><td><code id="Rcgmin_+3A_control">control</code></td>
<td>

<p>An optional list of control settings.  
</p>
</td></tr>
<tr><td><code id="Rcgmin_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to <code>fn</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Functions <code>fn</code> must return a numeric value.
</p>
<p>Note that <code>ncg</code> is to be called from <code>optimr</code> and does 
NOT allow dot arguments. It is intended to use the internal functions 
<code>efn</code> and <code>egr</code> generated inside <code>optimr()</code> along with 
bounds information from <code>bmchk()</code> available there. 
</p>
<p>The <code>control</code> argument is a list.
</p>

<dl>
<dt>maxit</dt><dd><p>A limit on the number of iterations (default 500). Note that this is 
used to compute a quantity <code>maxfeval</code>&lt;-round(sqrt(n+1)*maxit) where n is the
number of parameters to be minimized.</p>
</dd>
<dt>trace</dt><dd><p>Set 0 (default) for no output, &gt;0 for trace output
(larger values imply more output).</p>
</dd>
<dt>eps</dt><dd><p>Tolerance used to calculate numerical gradients. Default is 1.0E-7. See 
source code for <code>Rcgmin</code> for details of application.</p>
</dd>
<dt><code>dowarn</code></dt><dd><p>= TRUE if we want warnings generated by optimx. Default is 
TRUE.</p>
</dd>
<dt><code>tol</code></dt><dd><p>Tolerance used in testing the size of the square of the gradient.
Default is 0 on input, which uses a value of tolgr = npar*npar*.Machine$double.eps
in testing if crossprod(g) &lt;= tolgr * (abs(fmin) + reltest). If the user supplies
a value for <code>tol</code> that is non-zero, then that value is used for tolgr.
</p>
<p>reltest=100 is only alterable by changing the code. fmin is the current best 
value found for the function minimum value. 
</p>
<p>Note that the scale of the gradient means that tests for a small gradient can
easily be mismatched to a given problem. The defaults in Rcgmin are a &quot;best 
guess&quot;.</p>
</dd>
<dt><code>checkgrad</code></dt><dd><p>= TRUE if we want gradient function checked against 
numerical approximations. Default is FALSE.</p>
</dd>
<dt><code>checkbounds</code></dt><dd><p>= TRUE if we want bounds verified. Default is 
TRUE.</p>
</dd>
</dl>

<p>As of 2011-11-21 the following controls have been REMOVED
</p>

<dl>
<dt>usenumDeriv</dt><dd><p>There is now a choice of numerical gradient routines. See argument 
<code>gr</code>.</p>
</dd>
<dt>maximize</dt><dd><p>To maximize user_function, supply a function that computes (-1)*user_function.
An alternative is to call Rcgmin via the package optimx, where the MAXIMIZE field
of the OPCON structure in package <code>optfntools</code> is used.</p>
</dd>
</dl>



<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>par</code></td>
<td>
<p>The best set of parameters found.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>The value of the objective at the best set of parameters found.</p>
</td></tr>
<tr><td><code>counts</code></td>
<td>
<p>A two-element integer vector giving the number of calls to
'fn' and 'gr' respectively. This excludes those calls needed
to compute the Hessian, if requested, and any calls to 'fn'
to compute a finite-difference approximation to the gradient.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>An integer code. 
'0' indicates successful convergence.
'1' indicates that the function evaluation count 'maxfeval' was reached.
'2' indicates initial point is infeasible.</p>
</td></tr>
<tr><td><code>message</code></td>
<td>
<p>A character string giving any additional information returned
by the optimizer, or 'NULL'.</p>
</td></tr>
<tr><td><code>bdmsk</code></td>
<td>
<p>Returned index describing the status of bounds and masks at the
proposed solution. Parameters for which bdmsk are 1 are unconstrained
or &quot;free&quot;, those with bdmsk 0 are masked i.e., fixed. For historical
reasons, we indicate a parameter is at a lower bound using -3 
or upper bound using -1.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Dai, Y. H. and Y. Yuan (2001). An efficient hybrid conjugate 
gradient method for unconstrained optimization. Annals of 
Operations Research 103 (1-4), 3347.
</p>
<p>Nash JC (1979). Compact Numerical Methods for Computers: Linear 
Algebra and Function Minimisation. Adam Hilger, Bristol. Second 
Edition, 1990, Bristol: Institute of Physics Publications.
</p>
<p>Nash, J. C. and M. Walker-Smith (1987). Nonlinear Parameter 
Estimation: An Integrated System in BASIC. New York: Marcel Dekker. 
See https://www.nashinfo.com/nlpe.htm for a downloadable version 
of this plus some extras.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+optim">optim</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>#####################
require(numDeriv)
## Rosenbrock Banana function
fr &lt;- function(x) {
    x1 &lt;- x[1]
    x2 &lt;- x[2]
    100 * (x2 - x1 * x1)^2 + (1 - x1)^2
}

grr &lt;- function(x) { ## Gradient of 'fr'
    x1 &lt;- x[1]
    x2 &lt;- x[2]
    c(-400 * x1 * (x2 - x1 * x1) - 2 * (1 - x1),
       200 *      (x2 - x1 * x1))
}

grn&lt;-function(x){
    gg&lt;-grad(fr, x)
}  


ansrosenbrock0 &lt;- Rcgmin(fn=fr,gr=grn, par=c(1,2))
print(ansrosenbrock0) # use print to allow copy to separate file that 
#    can be called using source()
#####################
# Simple bounds and masks test
bt.f&lt;-function(x){
 sum(x*x)
}

bt.g&lt;-function(x){
  gg&lt;-2.0*x
}

n&lt;-10
xx&lt;-rep(0,n)
lower&lt;-rep(0,n)
upper&lt;-lower # to get arrays set
bdmsk&lt;-rep(1,n)
bdmsk[(trunc(n/2)+1)]&lt;-0
for (i in 1:n) { 
   lower[i]&lt;-1.0*(i-1)*(n-1)/n
   upper[i]&lt;-1.0*i*(n+1)/n
}
xx&lt;-0.5*(lower+upper)
ansbt&lt;-Rcgmin(xx, bt.f, bt.g, lower, upper, bdmsk, control=list(trace=1))

print(ansbt)

#####################
genrose.f&lt;- function(x, gs=NULL){ # objective function
## One generalization of the Rosenbrock banana valley function (n parameters)
	n &lt;- length(x)
        if(is.null(gs)) { gs=100.0 }
	fval&lt;-1.0 + sum (gs*(x[1:(n-1)]^2 - x[2:n])^2 + (x[2:n] - 1)^2)
        return(fval)
}
genrose.g &lt;- function(x, gs=NULL){
# vectorized gradient for genrose.f
# Ravi Varadhan 2009-04-03
	n &lt;- length(x)
        if(is.null(gs)) { gs=100.0 }
	gg &lt;- as.vector(rep(0, n))
	tn &lt;- 2:n
	tn1 &lt;- tn - 1
	z1 &lt;- x[tn] - x[tn1]^2
	z2 &lt;- 1 - x[tn]
	gg[tn] &lt;- 2 * (gs * z1 - z2)
	gg[tn1] &lt;- gg[tn1] - 4 * gs * x[tn1] * z1
	gg
}

# analytic gradient test
xx&lt;-rep(pi,10)
lower&lt;-NULL
upper&lt;-NULL
bdmsk&lt;-NULL
genrosea&lt;-Rcgmin(xx,genrose.f, genrose.g, gs=10)
genrosen&lt;-optimr(xx, genrose.f, "grfwd", method="Rcgmin", gs=10)
genrosenn&lt;-try(Rcgmin(xx,genrose.f, gs=10)) # use local numerical gradient
cat("genrosea uses analytic gradient\n")
print(genrosea)
cat("genrosen uses default gradient approximation\n")
print(genrosen)

cat("timings B vs U\n")
lo&lt;-rep(-100,10)
up&lt;-rep(100,10)
bdmsk&lt;-rep(1,10)
tb&lt;-system.time(ab&lt;-Rcgminb(xx,genrose.f, genrose.g, lower=lo, upper=up, bdmsk=bdmsk))[1]
tu&lt;-system.time(au&lt;-Rcgminu(xx,genrose.f, genrose.g))[1]
cat("times U=",tu,"   B=",tb,"\n")
cat("solution Rcgminu\n")
print(au)
cat("solution Rcgminb\n")
print(ab)
cat("diff fu-fb=",au$value-ab$value,"\n")
cat("max abs parameter diff = ", max(abs(au$par-ab$par)),"\n")

maxfn&lt;-function(x) {
      	n&lt;-length(x)
	ss&lt;-seq(1,n)
	f&lt;-10-(crossprod(x-ss))^2
	f&lt;-as.numeric(f)
	return(f)
}

gmaxfn&lt;-function(x) {
     gg&lt;-grad(maxfn, x) 
}


negmaxfn&lt;-function(x) {
	f&lt;-(-1)*maxfn(x)
	return(f)
}



cat("test that maximize=TRUE works correctly\n")

n&lt;-6
xx&lt;-rep(1,n)
ansmax&lt;-Rcgmin(xx,maxfn, gmaxfn, control=list(maximize=TRUE,trace=1))
print(ansmax)

cat("using the negmax function should give same parameters\n")
ansnegmaxn&lt;-optimr(xx,negmaxfn, "grfwd", method="Rcgmin", control=list(trace=1))
print(ansnegmaxn)


#####################  From Rvmmin.Rd
cat("test bounds and masks\n")
nn&lt;-4
startx&lt;-rep(pi,nn)
lo&lt;-rep(2,nn)
up&lt;-rep(10,nn)
grbds1&lt;-Rcgmin(startx,genrose.f, gr=genrose.g,lower=lo,upper=up) 
print(grbds1)

cat("test lower bound only\n")
nn&lt;-4
startx&lt;-rep(pi,nn)
lo&lt;-rep(2,nn)
grbds2&lt;-Rcgmin(startx,genrose.f, gr=genrose.g,lower=lo) 
print(grbds2)

cat("test lower bound single value only\n")
nn&lt;-4
startx&lt;-rep(pi,nn)
lo&lt;-2
up&lt;-rep(10,nn)
grbds3&lt;-Rcgmin(startx,genrose.f, gr=genrose.g,lower=lo) 
print(grbds3)

cat("test upper bound only\n")
nn&lt;-4
startx&lt;-rep(pi,nn)
lo&lt;-rep(2,nn)
up&lt;-rep(10,nn)
grbds4&lt;-Rcgmin(startx,genrose.f, gr=genrose.g,upper=up) 
print(grbds4)

cat("test upper bound single value only\n")
nn&lt;-4
startx&lt;-rep(pi,nn)
grbds5&lt;-Rcgmin(startx,genrose.f, gr=genrose.g,upper=10) 
print(grbds5)



cat("test masks only\n")
nn&lt;-6
bd&lt;-c(1,1,0,0,1,1)
startx&lt;-rep(pi,nn)
grbds6&lt;-Rcgmin(startx,genrose.f, gr=genrose.g,bdmsk=bd) 
print(grbds6)

cat("test upper bound on first two elements only\n")
nn&lt;-4
startx&lt;-rep(pi,nn)
upper&lt;-c(10,8, Inf, Inf)
grbds7&lt;-Rcgmin(startx,genrose.f, gr=genrose.g,upper=upper) 
print(grbds7)


cat("test lower bound on first two elements only\n")
nn&lt;-4
startx&lt;-rep(0,nn)
lower&lt;-c(0,1.1, -Inf, -Inf)
grbds8&lt;-Rcgmin(startx,genrose.f,genrose.g,lower=lower, control=list(maxit=2000)) 
print(grbds8)

cat("test n=1 problem using simple squares of parameter\n")

sqtst&lt;-function(xx) {
   res&lt;-sum((xx-2)*(xx-2))
}

gsqtst&lt;-function(xx) {
    gg&lt;-2*(xx-2)
}

######### One dimension test
nn&lt;-1
startx&lt;-rep(0,nn)
onepar&lt;-Rcgmin(startx,sqtst,  gr=gsqtst,control=list(trace=1)) 
print(onepar)

cat("Suppress warnings\n")
oneparnw&lt;-Rcgmin(startx,sqtst,  gr=gsqtst,control=list(dowarn=FALSE,trace=1)) 
print(oneparnw)

</code></pre>

<hr>
<h2 id='Rcgminb'>An R implementation of a bounded nonlinear conjugate gradient algorithm 
with the Dai / Yuan update and restart. Based on Nash (1979) Algorithm 22 
for its main structure. CALL THIS VIA <code>Rcgmin</code> AND DO NOT USE DIRECTLY.</h2><span id='topic+Rcgminb'></span>

<h3>Description</h3>

<p>The purpose of <code>Rcgminb</code> is to minimize a bounds (box) and mask 
constrained function 
of many parameters by a nonlinear conjugate gradients method. This code is
entirely in R to allow users to explore and understand the method. It 
allows bounds (or box) constraints and masks (equality constraints) to be
imposed on parameters. 
</p>
<p>This code should be called through <code>Rcgmin</code> which selects <code>Rcgminb</code>
or <code>Rcgminu</code> according to the presence of bounds and masks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   Rcgminb(par, fn, gr, lower, upper, bdmsk, control = list(), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Rcgminb_+3A_par">par</code></td>
<td>
<p>A numeric vector of starting estimates.</p>
</td></tr>
<tr><td><code id="Rcgminb_+3A_fn">fn</code></td>
<td>
<p>A function that returns the value of the objective at the
supplied set of parameters <code>par</code> using auxiliary data in ....
The first argument of <code>fn</code> must be <code>par</code>. </p>
</td></tr>
<tr><td><code id="Rcgminb_+3A_gr">gr</code></td>
<td>
<p>A function that returns the gradient of the objective at the
supplied set of parameters <code>par</code> using auxiliary data in ....
The first argument of <code>fn</code> must be <code>par</code>. This function 
returns the gradient as a numeric vector.
</p>
<p>The use of numerical gradients for Rcgminb is STRONGLY discouraged.</p>
</td></tr>
<tr><td><code id="Rcgminb_+3A_lower">lower</code></td>
<td>
<p>A vector of lower bounds on the parameters.</p>
</td></tr>
<tr><td><code id="Rcgminb_+3A_upper">upper</code></td>
<td>
<p>A vector of upper bounds on the parameters.</p>
</td></tr>
<tr><td><code id="Rcgminb_+3A_bdmsk">bdmsk</code></td>
<td>
<p>An indicator vector, having 1 for each parameter that is &quot;free&quot; or
unconstrained, and 0 for any parameter that is fixed or MASKED for the
duration of the optimization.</p>
</td></tr>
<tr><td><code id="Rcgminb_+3A_control">control</code></td>
<td>

<p>An optional list of control settings.  
</p>
</td></tr>
<tr><td><code id="Rcgminb_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to <code>fn</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Functions <code>fn</code> must return a numeric value.
</p>
<p>The <code>control</code> argument is a list.
</p>

<dl>
<dt>maxit</dt><dd><p>A limit on the number of iterations (default 500). Note that this is 
used to compute a quantity <code>maxfeval</code>&lt;-round(sqrt(n+1)*maxit) where n is the
number of parameters to be minimized.</p>
</dd>
<dt>trace</dt><dd><p>Set 0 (default) for no output, &gt;0 for trace output
(larger values imply more output).</p>
</dd>
<dt>eps</dt><dd><p>Tolerance used to calculate numerical gradients. Default is 1.0E-7. See 
source code for <code>Rcgminb</code> for details of application.</p>
</dd>
<dt><code>dowarn</code></dt><dd><p>= TRUE if we want warnings generated by optimx. Default is 
TRUE.</p>
</dd>
<dt>Note:</dt><dd><p>The source code <code>Rcgminb</code> for R is likely to remain a work in progress for some time, 
so users should watch the console output.</p>
</dd>
</dl>

<p>As of 2011-11-21 the following controls have been REMOVED
</p>

<dl>
<dt>usenumDeriv</dt><dd><p>There is now a choice of numerical gradient routines. See argument 
<code>gr</code>.</p>
</dd>
<dt>maximize</dt><dd><p>To maximize user_function, supply a function that computes (-1)*user_function.
An alternative is to call Rcgmin via the package optimx.</p>
</dd>
</dl>



<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>par</code></td>
<td>
<p>The best set of parameters found.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>The value of the objective at the best set of parameters found.</p>
</td></tr>
<tr><td><code>counts</code></td>
<td>
<p>A two-element integer vector giving the number of calls to
'fn' and 'gr' respectively. This excludes those calls needed
to compute the Hessian, if requested, and any calls to 'fn'
to compute a finite-difference approximation to the gradient.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>An integer code. 
'0' indicates successful convergence.
'1' indicates that the function evaluation count 'maxfeval' was reached.
'2' indicates initial point is infeasible.</p>
</td></tr>
<tr><td><code>message</code></td>
<td>
<p>A character string giving any additional information returned
by the optimizer, or 'NULL'.</p>
</td></tr>
<tr><td><code>bdmsk</code></td>
<td>
<p>Returned index describing the status of bounds and masks at the
proposed solution. Parameters for which bdmsk are 1 are unconstrained
or &quot;free&quot;, those with bdmsk 0 are masked i.e., fixed. For historical
reasons, we indicate a parameter is at a lower bound using -3 
or upper bound using -1.</p>
</td></tr>
</table>


<h3>References</h3>

<p>See <code>Rcgmin</code> documentation. Note that bounds and masks were adapted 
from the work by Nash and Walker-Smith(1987).
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+optim">optim</a></code></p>

<hr>
<h2 id='Rcgminu'>An R implementation of an unconstrained nonlinear conjugate gradient algorithm 
with the Dai / Yuan update and restart. Based on Nash (1979) Algorithm 22 
for its main structure.  CALL THIS VIA <code>Rcgmin</code> AND DO NOT USE DIRECTLY.</h2><span id='topic+Rcgminu'></span>

<h3>Description</h3>

<p>The purpose of <code>Rcgminu</code> is to minimize an unconstrained function 
of many parameters by a nonlinear conjugate gradients method. This code is
entirely in R to allow users to explore and understand the method. 
</p>
<p>This code should be called through <code>Rcgmin</code> which selects <code>Rcgminb</code>
or <code>Rcgminu</code> according to the presence of bounds and masks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   Rcgminu(par, fn, gr, control = list(), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Rcgminu_+3A_par">par</code></td>
<td>
<p>A numeric vector of starting estimates.</p>
</td></tr>
<tr><td><code id="Rcgminu_+3A_fn">fn</code></td>
<td>
<p>A function that returns the value of the objective at the
supplied set of parameters <code>par</code> using auxiliary data in ....
The first argument of <code>fn</code> must be <code>par</code>. </p>
</td></tr>
<tr><td><code id="Rcgminu_+3A_gr">gr</code></td>
<td>
<p>A function that returns the gradient of the objective at the
supplied set of parameters <code>par</code> using auxiliary data in ....
The first argument of <code>fn</code> must be <code>par</code>. This function 
returns the gradient as a numeric vector.
</p>
<p>The use of numerical gradients for Rcgminu is STRONGLY discouraged.</p>
</td></tr>
<tr><td><code id="Rcgminu_+3A_control">control</code></td>
<td>

<p>An optional list of control settings.  
</p>
</td></tr>
<tr><td><code id="Rcgminu_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to <code>fn</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Functions <code>fn</code> must return a numeric value.
</p>
<p>The <code>control</code> argument is a list.
</p>

<dl>
<dt>maxit</dt><dd><p>A limit on the number of iterations (default 500). Note that this is 
used to compute a quantity <code>maxfeval</code>&lt;-round(sqrt(n+1)*maxit) where n is the
number of parameters to be minimized.</p>
</dd>
<dt>trace</dt><dd><p>Set 0 (default) for no output, &gt;0 for trace output
(larger values imply more output).</p>
</dd>
<dt>eps</dt><dd><p>Tolerance used to calculate numerical gradients. Default is 1.0E-7. See 
source code for <code>Rcgminu</code> for details of application.</p>
</dd>
<dt><code>dowarn</code></dt><dd><p>= TRUE if we want warnings generated by optimx. Default is 
TRUE.</p>
</dd>
<dt>Note:</dt><dd><p>The source code <code>Rcgminu</code> for R is likely to remain a work in progress for some time, 
so users should watch the console output.</p>
</dd>
</dl>

<p>As of 2011-11-21 the following controls have been REMOVED
</p>

<dl>
<dt>usenumDeriv</dt><dd><p>There is now a choice of numerical gradient routines. See argument 
<code>gr</code>.</p>
</dd>
<dt>maximize</dt><dd><p>To maximize user_function, supply a function that computes (-1)*user_function.
An alternative is to call Rcgmin via the package optimx.</p>
</dd>
</dl>



<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>par</code></td>
<td>
<p>The best set of parameters found.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>The value of the objective at the best set of parameters found.</p>
</td></tr>
<tr><td><code>counts</code></td>
<td>
<p>A two-element integer vector giving the number of calls to
'fn' and 'gr' respectively. This excludes those calls needed
to compute the Hessian, if requested, and any calls to 'fn'
to compute a finite-difference approximation to the gradient.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>An integer code. 
'0' indicates successful convergence.
'1' indicates that the function evaluation count 'maxfeval' was reached.
'2' indicates initial point is infeasible.</p>
</td></tr>
<tr><td><code>message</code></td>
<td>
<p>A character string giving any additional information returned
by the optimizer, or 'NULL'.</p>
</td></tr>
<tr><td><code>bdmsk</code></td>
<td>
<p>Returned index describing the status of bounds and masks at the
proposed solution. Parameters for which bdmsk are 1 are unconstrained
or &quot;free&quot;, those with bdmsk 0 are masked i.e., fixed. For historical
reasons, we indicate a parameter is at a lower bound using -3 
or upper bound using -1.</p>
</td></tr>
</table>


<h3>References</h3>

<p>See <code>Rcgmin</code> documentation.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+optim">optim</a></code></p>

<hr>
<h2 id='Rvmmin'>Variable metric nonlinear function minimization, driver.</h2><span id='topic+Rvmmin'></span><span id='topic+nvm'></span>

<h3>Description</h3>

<p>A driver to call the unconstrained and bounds constrained versions of an 
R implementation of a variable metric method for minimization
of nonlinear functions, possibly subject to bounds (box) constraints and masks 
(fixed parameters). The algorithm is based on Nash (1979) Algorithm 21 for main structure,
which is itself drawn from Fletcher's (1970) variable metric code. This is also the basis
of optim() method 'BFGS' which, however, does not deal with bounds or masks. In the 
present method, an approximation to the inverse Hessian (B) is used to generate a 
search direction t = - B %*% g, a simple backtracking line search is used until an
acceptable point is found, and the matrix B is updated using a BFGS formula. If no
acceptable point can be found, we reset B to the identity i.e., the search direction
becomes the negative gradient. If the search along the negative gradient is unsuccessful,
the method terminates. 
</p>
<p>This set of codes is
entirely in R to allow users to explore and understand the method. It also
allows bounds (or box) constraints and masks (equality constraints) to be
imposed on parameters. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   Rvmmin(par, fn, gr, lower, upper, bdmsk, control = list(), ...)

   nvm(par, fn, gr, bds, control = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Rvmmin_+3A_par">par</code></td>
<td>
<p>A numeric vector of starting estimates.</p>
</td></tr>
<tr><td><code id="Rvmmin_+3A_fn">fn</code></td>
<td>
<p>A function that returns the value of the objective at the
supplied set of parameters <code>par</code> using auxiliary data in ....
The first argument of <code>fn</code> must be <code>par</code>. </p>
</td></tr>
<tr><td><code id="Rvmmin_+3A_gr">gr</code></td>
<td>
<p>A function that returns the gradient of the objective at the
supplied set of parameters <code>par</code> using auxiliary data in ....
The first argument of <code>fn</code> must be <code>par</code>. This function 
returns the gradient as a numeric vector.
</p>
<p>Note that a gradient function must generally be provided. However, 
to ensure compatibility with other optimizers, if <code>gr</code> is NULL,
the forward gradient approximation from routine <code>grfwd</code> will
be used.
</p>
<p>The use of numerical gradients for Rvmmin is discouraged.
First, the termination
test uses a size measure on the gradient, and numerical gradient 
approximations can sometimes give results that are too large. Second,
if there are bounds constraints, the step(s) taken to calculate the
approximation to the derivative are NOT checked to see if they are
out of bounds, and the function may be undefined at the evaluation
point. 
</p>
<p>There is also the option of using the routines <code>grfwd</code>, <code>grback</code>, 
<code>grcentral</code> or <code>grnd</code>. The last 
of these calls the <code>grad()</code> function from package numDeriv. These 
are called by putting the name of the (numerical) gradient function in 
quotation marks, e.g.,
</p>
<p>gr=&quot;grfwd&quot;
</p>
<p>to use the standard forward difference numerical approximation.
</p>
<p>Note that all but the <code>grnd</code> routine use a stepsize parameter that
can be redefined in a special scratchpad storage variable <code>deps</code>.
The default is <code>deps = 1e-07</code>. 
However, redefining this is discouraged unless you understand what
you are doing. 
</p>
</td></tr>
<tr><td><code id="Rvmmin_+3A_bds">bds</code></td>
<td>
<p>A list of information resulting from function <code>bmchk</code> giving
information on the status of the parameters and bounds and masks.</p>
</td></tr>     
<tr><td><code id="Rvmmin_+3A_lower">lower</code></td>
<td>
<p>A vector of lower bounds on the parameters.</p>
</td></tr>
<tr><td><code id="Rvmmin_+3A_upper">upper</code></td>
<td>
<p>A vector of upper bounds on the parameters.</p>
</td></tr>
<tr><td><code id="Rvmmin_+3A_bdmsk">bdmsk</code></td>
<td>
<p>An indicator vector, having 1 for each parameter that is &quot;free&quot; 
or unconstrained, and 0 for any parameter that is fixed or MASKED for 
the duration of the optimization.</p>
</td></tr>
<tr><td><code id="Rvmmin_+3A_control">control</code></td>
<td>

<p>An optional list of control settings.
</p>
</td></tr>
<tr><td><code id="Rvmmin_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to <code>fn</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that <code>nvm</code> is to be called from <code>optimr</code> and does 
NOT allow dot arguments. It is intended to use the internal functions 
<code>efn</code> and <code>egr</code> generated inside <code>optimr()</code> along with 
bounds information from <code>bmchk()</code> available there. 
</p>
<p>The source codes <code>Rvmmin</code> and <code>nvm</code> for R are still a work 
in progress, so users should watch the console output. The routine
<code>nvm</code> attempts to use minimal checking and works only with a
bounds constrained version of the algorithm, which may work as fast
as a specific routine for unconstrained problems. This is an open 
question, and the author welcomes feedback.    
</p>
<p>Function <code>fn</code> must return a numeric value.
</p>
<p>The <code>control</code> argument is a list.
</p>
<p>The <code>control</code> argument is a list.
</p>

<dl>
<dt>maxit</dt><dd><p>A limit on the number of iterations (default 500 + 2*n where n is
the number of parameters). This is the maximum number of gradient evaluations 
allowed.</p>
</dd>
<dt>maxfevals</dt><dd><p>A limit on the number of function evaluations allowed 
(default 3000 + 10*n).</p>
</dd>
<dt>trace</dt><dd><p>Set 0 (default) for no output, &gt; 0 for diagnostic output
(larger values imply more output).</p>
</dd>
<dt>dowarn</dt><dd><p>= TRUE if we want warnings generated by optimx. Default is 
TRUE.</p>
</dd>
<dt>checkgrad</dt><dd><p>= TRUE if we wish analytic gradient code checked against the
approximations computed by <code>numDeriv</code>. Default is FALSE.</p>
</dd>
<dt>checkbounds</dt><dd><p>= TRUE if we wish parameters and bounds to be checked for an 
admissible and feasible start. Default is TRUE.</p>
</dd>
<dt>keepinputpar</dt><dd><p>= TRUE if we want bounds check to stop program when parameters
are out of bounds. Else when FALSE, moves parameter values to nearest bound. Default is 
FALSE.</p>
</dd>
<dt>maximize</dt><dd><p>To maximize user_function, supply a function that computes (-1)*user_function.
An alternative is to call Rvmmin via the package optimx.</p>
</dd>
<dt>eps</dt><dd><p> a tolerance used for judging small gradient norm (default = 1e-07).
a gradient norm smaller than (1 + abs(fmin))*eps*eps is considered small 
enough that a local optimum has been found, where fmin is the current 
estimate of the minimal function value. </p>
</dd>
<dt>acctol</dt><dd><p>To adjust the acceptable point tolerance (default 0.0001) in the test
( f &lt;= fmin + gradproj * steplength * acctol ). This test is used to ensure progress
is made at each iteration. </p>
</dd>
<dt>stepredn</dt><dd><p>Step reduction factor for backtrack line search (default 0.2)</p>
</dd>
<dt>reltest</dt><dd><p>Additive shift for equality test (default 100.0)</p>
</dd>
<dt>stopbadupdate</dt><dd><p>A logical flag that if set TRUE will halt the optimization
if the Hessian inverse cannot be updated after a steepest descent
search. This indicates an ill-conditioned Hessian. A settign of
FALSE causes Rvmmin methods to be aggressive in trying to
optimize the function, but may waste effort. Default TRUE.</p>
</dd>
</dl>

<p>As of 2011-11-21 the following controls have been REMOVED
</p>

<dl>
<dt>usenumDeriv</dt><dd><p>There is now a choice of numerical gradient routines.
See argument <code>gr</code>.</p>
</dd>
</dl>



<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>par</code></td>
<td>
<p>The best set of parameters found.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>The value of the objective at the best set of parameters found.</p>
</td></tr>
<tr><td><code>counts</code></td>
<td>
<p>A vector of two integers giving the number of function and gradient evaluations.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>An integer indicating the situation on termination of the function. <code>0</code>
indicates that the method believes it has succeeded. Other values:
</p>

<dl>
<dt><code>0</code></dt><dd><p>indicates successful termination to an acceptable solution</p>
</dd>
<dt><code>1</code></dt><dd><p>indicates that the iteration limit <code>maxit</code>
had been reached.</p>
</dd>
<dt><code>2</code></dt><dd><p>indicates that a point with a small gradient norm has been
found, which is likely a solution.</p>
</dd>
<dt><code>20</code></dt><dd><p>indicates that the initial set of parameters is inadmissible, that is,
that the function cannot be computed or returns an infinite, NULL, or NA value.</p>
</dd>
<dt><code>21</code></dt><dd><p>indicates that an intermediate set of parameters is inadmissible.</p>
</dd>
</dl>

</td></tr>
<tr><td><code>message</code></td>
<td>
<p>A description of the situation on termination of the function.</p>
</td></tr>
<tr><td><code>bdmsk</code></td>
<td>
<p>Returned index describing the status of bounds and masks at the
proposed solution. Parameters for which bdmsk are 1 are unconstrained
or &quot;free&quot;, those with bdmsk 0 are masked i.e., fixed. For historical
reasons, we indicate a parameter is at a lower bound using -3 
or upper bound using -1.</p>
</td></tr>
</table>


<h3>References</h3>

 
<p>Fletcher, R (1970) A New Approach to Variable Metric Algorithms,
Computer Journal, 13(3), pp. 317-322.
</p>
<p>Nash, J C (1979, 1990) Compact Numerical Methods for Computers: Linear
Algebra and Function Minimisation, Bristol: Adam Hilger. Second
Edition, Bristol: Institute of Physics Publications.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+optim">optim</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>#####################
## All examples for the Rvmmin package are in this .Rd file
##

## Rosenbrock Banana function
fr &lt;- function(x) {
  x1 &lt;- x[1]
  x2 &lt;- x[2]
  100 * (x2 - x1 * x1)^2 + (1 - x1)^2
}

ansrosenbrock &lt;- Rvmmin(fn=fr,gr="grfwd", par=c(1,2))
print(ansrosenbrock) 
cat("\n")
cat("No gr specified as a test\n")
ansrosenbrock0 &lt;- Rvmmin(fn=fr, par=c(1,2))
print(ansrosenbrock0) 
# use print to allow copy to separate file that can be called using source()

#####################
# Simple bounds and masks test
#
# The function is a sum of squares, but we impose the 
# constraints so that there are lower and upper bounds
# away from zero, and parameter 6 is fixed at the initial
# value

bt.f&lt;-function(x){
  sum(x*x)
}

bt.g&lt;-function(x){
  gg&lt;-2.0*x
}

n&lt;-10
xx&lt;-rep(0,n)
lower&lt;-rep(0,n)
upper&lt;-lower # to get arrays set
bdmsk&lt;-rep(1,n)
bdmsk[(trunc(n/2)+1)]&lt;-0
for (i in 1:n) { 
  lower[i]&lt;-1.0*(i-1)*(n-1)/n
  upper[i]&lt;-1.0*i*(n+1)/n
}
xx&lt;-0.5*(lower+upper)
cat("Initial parameters:")
print(xx)
cat("Lower bounds:")
print(lower)
cat("upper bounds:")
print(upper)
cat("Masked (fixed) parameters:")
print(which(bdmsk == 0))

ansbt&lt;-Rvmmin(xx, bt.f, bt.g, lower, upper, bdmsk, control=list(trace=1))

print(ansbt)

#####################
# A version of a generalized Rosenbrock problem
genrose.f&lt;- function(x, gs=NULL){ # objective function
  ## One generalization of the Rosenbrock banana valley function (n parameters)
  n &lt;- length(x)
  if(is.null(gs)) { gs=100.0 }
  fval&lt;-1.0 + sum (gs*(x[1:(n-1)]^2 - x[2:n])^2 + (x[2:n] - 1)^2)
  return(fval)
}
genrose.g &lt;- function(x, gs=NULL){
  # vectorized gradient for genrose.f
  # Ravi Varadhan 2009-04-03
  n &lt;- length(x)
  if(is.null(gs)) { gs=100.0 }
  gg &lt;- as.vector(rep(0, n))
  tn &lt;- 2:n
  tn1 &lt;- tn - 1
  z1 &lt;- x[tn] - x[tn1]^2
  z2 &lt;- 1 - x[tn]
  gg[tn] &lt;- 2 * (gs * z1 - z2)
  gg[tn1] &lt;- gg[tn1] - 4 * gs * x[tn1] * z1
  gg
}

# analytic gradient test
xx&lt;-rep(pi,10)
lower&lt;-NULL
upper&lt;-NULL
bdmsk&lt;-NULL
genrosea&lt;-Rvmmin(xx,genrose.f, genrose.g, gs=10)
genrosenf&lt;-Rvmmin(xx,genrose.f, gr="grfwd", gs=10) # use local numerical gradient
genrosenullgr&lt;-Rvmmin(xx,genrose.f, gs=10) # no gradient specified
cat("genrosea uses analytic gradient\n")
print(genrosea)
cat("genrosenf uses grfwd standard numerical gradient\n")
print(genrosenf)
cat("genrosenullgr has no gradient specified\n")
print(genrosenullgr)
cat("Other numerical gradients can be used.\n")

cat("timings B vs U\n")
lo&lt;-rep(-100,10)
up&lt;-rep(100,10)
bdmsk&lt;-rep(1,10)
tb&lt;-system.time(ab&lt;-Rvmminb(xx,genrose.f, genrose.g, lower=lo, upper=up, bdmsk=bdmsk))[1]
tu&lt;-system.time(au&lt;-Rvmminu(xx,genrose.f, genrose.g))[1]
cat("times U=",tu,"   B=",tb,"\n")
cat("solution Rvmminu\n")
print(au)
cat("solution Rvmminb\n")
print(ab)
cat("diff fu-fb=",au$value-ab$value,"\n")
cat("max abs parameter diff = ", max(abs(au$par-ab$par)),"\n")

# Test that Rvmmin will maximize as well as minimize

maxfn&lt;-function(x) {
  n&lt;-length(x)
  ss&lt;-seq(1,n)
  f&lt;-10-(crossprod(x-ss))^2
  f&lt;-as.numeric(f)
  return(f)
}


negmaxfn&lt;-function(x) {
  f&lt;-(-1)*maxfn(x)
  return(f)
}

cat("test that maximize=TRUE works correctly\n")

n&lt;-6
xx&lt;-rep(1,n)
ansmax&lt;-Rvmmin(xx,maxfn, gr="grfwd", control=list(maximize=TRUE,trace=1))
print(ansmax)

cat("using the negmax function should give same parameters\n")
ansnegmax&lt;-Rvmmin(xx,negmaxfn, gr="grfwd", control=list(trace=1))
print(ansnegmax)


#####################
cat("test bounds and masks\n")
nn&lt;-4
startx&lt;-rep(pi,nn)
lo&lt;-rep(2,nn)
up&lt;-rep(10,nn)
grbds1&lt;-Rvmmin(startx,genrose.f, genrose.g, lower=lo,upper=up) 
print(grbds1)

cat("test lower bound only\n")
nn&lt;-4
startx&lt;-rep(pi,nn)
lo&lt;-rep(2,nn)
grbds2&lt;-Rvmmin(startx,genrose.f, genrose.g, lower=lo) 
print(grbds2)

cat("test lower bound single value only\n")
nn&lt;-4
startx&lt;-rep(pi,nn)
lo&lt;-2
up&lt;-rep(10,nn)
grbds3&lt;-Rvmmin(startx,genrose.f, genrose.g, lower=lo) 
print(grbds3)

cat("test upper bound only\n")
nn&lt;-4
startx&lt;-rep(pi,nn)
lo&lt;-rep(2,nn)
up&lt;-rep(10,nn)
grbds4&lt;-Rvmmin(startx,genrose.f, genrose.g, upper=up) 
print(grbds4)

cat("test upper bound single value only\n")
nn&lt;-4
startx&lt;-rep(pi,nn)
grbds5&lt;-Rvmmin(startx,genrose.f, genrose.g, upper=10) 
print(grbds5)



cat("test masks only\n")
nn&lt;-6
bd&lt;-c(1,1,0,0,1,1)
startx&lt;-rep(pi,nn)
grbds6&lt;-Rvmmin(startx,genrose.f, genrose.g, bdmsk=bd) 
print(grbds6)

cat("test upper bound on first two elements only\n")
nn&lt;-4
startx&lt;-rep(pi,nn)
upper&lt;-c(10,8, Inf, Inf)
grbds7&lt;-Rvmmin(startx,genrose.f, genrose.g, upper=upper) 
print(grbds7)


cat("test lower bound on first two elements only\n")
nn&lt;-4
startx&lt;-rep(0,nn)
lower&lt;-c(0,1.1, -Inf, -Inf)
grbds8&lt;-Rvmmin(startx,genrose.f,genrose.g,lower=lower, control=list(maxit=2000)) 
print(grbds8)

cat("test n=1 problem using simple squares of parameter\n")

sqtst&lt;-function(xx) {
  res&lt;-sum((xx-2)*(xx-2))
}

nn&lt;-1
startx&lt;-rep(0,nn)
onepar&lt;-Rvmmin(startx,sqtst, gr="grfwd", control=list(trace=1)) 
print(onepar)

cat("Suppress warnings\n")
oneparnw&lt;-Rvmmin(startx,sqtst, gr="grfwd", control=list(dowarn=FALSE,trace=1)) 
print(oneparnw)

</code></pre>

<hr>
<h2 id='Rvmminb'>Variable metric nonlinear function minimization with bounds constraints</h2><span id='topic+Rvmminb'></span>

<h3>Description</h3>

<p>A bounds-constarined R implementation of a variable metric method for minimization
of nonlinear functions subject to bounds (box) constraints and masks 
(fixed parameters). 
</p>
<p>See manual Rvmmin.Rd for more details and examples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   Rvmminb(par, fn, gr, lower, upper, bdmsk, control = list(), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Rvmminb_+3A_par">par</code></td>
<td>
<p>A numeric vector of starting estimates.</p>
</td></tr>
<tr><td><code id="Rvmminb_+3A_fn">fn</code></td>
<td>
<p>A function that returns the value of the objective at the
supplied set of parameters <code>par</code> using auxiliary data in ....
The first argument of <code>fn</code> must be <code>par</code>. </p>
</td></tr>
<tr><td><code id="Rvmminb_+3A_gr">gr</code></td>
<td>
<p>A function that returns the gradient of the objective at the
supplied set of parameters <code>par</code> using auxiliary data in ....
The first argument of <code>fn</code> must be <code>par</code>. This function 
returns the gradient as a numeric vector.
</p>
<p>Note that a gradient function MUST be provided. See the manual for
<code>Rvmmin</code>, which is the usual way <code>Rvmminb</code> is called. The
user must take responsibility for errors if <code>Rvmminb</code> is called
directly. 
</p>
</td></tr>
<tr><td><code id="Rvmminb_+3A_lower">lower</code></td>
<td>
<p>A vector of lower bounds on the parameters.</p>
</td></tr>
<tr><td><code id="Rvmminb_+3A_upper">upper</code></td>
<td>
<p>A vector of upper bounds on the parameters.</p>
</td></tr>
<tr><td><code id="Rvmminb_+3A_bdmsk">bdmsk</code></td>
<td>
<p>An indicator vector, having 1 for each parameter that is &quot;free&quot; or
unconstrained, and 0 for any parameter that is fixed or MASKED for the
duration of the optimization.</p>
</td></tr>
<tr><td><code id="Rvmminb_+3A_control">control</code></td>
<td>

<p>An optional list of control settings. See the manual Rvmmin.Rd for 
details. 
</p>
</td></tr>
<tr><td><code id="Rvmminb_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to <code>fn</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This routine is intended to be called from <code>Rvmmin</code>, which will, if
necessary, supply a gradient approximation. However, some users will want
to avoid the extra overhead, in which case it is important to provide an
appropriate and high-accuracy gradient routine.
</p>
<p>Note that bounds checking, if it is carried out, is done by <code>Rvmmin</code>.
</p>
<p>Functions <code>fn</code> must return a numeric value.
</p>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>par</code></td>
<td>
<p>The best set of parameters found.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>The value of the objective at the best set of parameters found.</p>
</td></tr>
<tr><td><code>counts</code></td>
<td>
<p>A vector of two integers giving the number of function and gradient evaluations.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>An integer indicating the situation on termination of the function. <code>0</code>
indicates that the method believes it has succeeded. Other values:
</p>

<dl>
<dt><code>1</code></dt><dd><p>indicates that the iteration limit <code>maxit</code>
had been reached.</p>
</dd>
<dt><code>20</code></dt><dd><p>indicates that the initial set of parameters is inadmissible, that is,
that the function cannot be computed or returns an infinite, NULL, or NA value.</p>
</dd>
<dt><code>21</code></dt><dd><p>indicates that an intermediate set of parameters is inadmissible.</p>
</dd>
</dl>

</td></tr>
<tr><td><code>message</code></td>
<td>
<p>A description of the situation on termination of the function.</p>
</td></tr>
<tr><td><code>bdmsk</code></td>
<td>
<p>Returned index describing the status of bounds and masks at the
proposed solution. Parameters for which bdmsk are 1 are unconstrained
or &quot;free&quot;, those with bdmsk 0 are masked i.e., fixed. For historical
reasons, we indicate a parameter is at a lower bound using -3 
or upper bound using -1.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+optim">optim</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## See Rvmmin.Rd

</code></pre>

<hr>
<h2 id='Rvmminu'>Variable metric nonlinear function minimization, unconstrained</h2><span id='topic+Rvmminu'></span>

<h3>Description</h3>

<p>An R implementation of a variable metric method for minimization
of unconstrained nonlinear functions.
</p>
<p>See the manual Rvmmin.Rd for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   Rvmminu(par, fn, gr, control = list(), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Rvmminu_+3A_par">par</code></td>
<td>
<p>A numeric vector of starting estimates.</p>
</td></tr>
<tr><td><code id="Rvmminu_+3A_fn">fn</code></td>
<td>
<p>A function that returns the value of the objective at the
supplied set of parameters <code>par</code> using auxiliary data in ....
The first argument of <code>fn</code> must be <code>par</code>. </p>
</td></tr>
<tr><td><code id="Rvmminu_+3A_gr">gr</code></td>
<td>
<p>A function that returns the gradient of the objective at the
supplied set of parameters <code>par</code> using auxiliary data in ....
The first argument of <code>fn</code> must be <code>par</code>. This function 
returns the gradient as a numeric vector.
</p>
<p>Note that a gradient function MUST be provided. See the manual for
<code>Rvmmin</code>, which is the usual way <code>Rvmminu</code> is called. The
user must take responsibility for errors if <code>Rvmminu</code> is called
directly. 
</p>
</td></tr>
<tr><td><code id="Rvmminu_+3A_control">control</code></td>
<td>

<p>An optional list of control settings. See the manual Rvmmin.Rd for 
details. Some control elements apply only when parameters are bounds
constrained and are not used in this function.
</p>
</td></tr>
<tr><td><code id="Rvmminu_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to <code>fn</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This routine is intended to be called from <code>Rvmmin</code>, which will, if
necessary, supply a gradient approximation. However, some users will want
to avoid the extra overhead, in which case it is important to provide an
appropriate and high-accuracy gradient routine.
</p>
<p>Functions <code>fn</code> must return a numeric value.
</p>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>par</code></td>
<td>
<p>The best set of parameters found.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>The value of the objective at the best set of parameters found.</p>
</td></tr>
<tr><td><code>counts</code></td>
<td>
<p>A vector of two integers giving the number of function and gradient evaluations.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>An integer indicating the situation on termination of the function. <code>0</code>
indicates that the method believes it has succeeded. Other values:
</p>

<dl>
<dt><code>1</code></dt><dd><p>indicates that the iteration limit <code>maxit</code>
had been reached.</p>
</dd>
<dt><code>20</code></dt><dd><p>indicates that the initial set of parameters is inadmissible, that is,
that the function cannot be computed or returns an infinite, NULL, or NA value.</p>
</dd>
<dt><code>21</code></dt><dd><p>indicates that an intermediate set of parameters is inadmissible.</p>
</dd>
</dl>

</td></tr>
<tr><td><code>message</code></td>
<td>
<p>A description of the situation on termination of the function.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+optim">optim</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>####in Rvmmin.Rd ####
</code></pre>

<hr>
<h2 id='scalechk'>Check the scale of the initial parameters and bounds input to an optimization code
used in nonlinear optimization</h2><span id='topic+scalechk'></span>

<h3>Description</h3>

<p>Nonlinear optimization problems often have different scale for different
parameters. This function is intended to explore the differences in scale. It is, however,
an imperfect and heuristic tool, and could be improved.
</p>
<p>At this time scalechk ignores parameters and bounds for fixed (masked) parameters
for calculations of scaling measures. The rationale for this is that such parameters
are outside the optimization process.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   scalechk(par, lower = lower, upper = upper, bdmsk=NULL, dowarn = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scalechk_+3A_par">par</code></td>
<td>
<p>A numeric vector of starting values of the optimization function parameters.</p>
</td></tr>
<tr><td><code id="scalechk_+3A_lower">lower</code></td>
<td>
<p>A vector of lower bounds on the parameters.</p>
</td></tr>
<tr><td><code id="scalechk_+3A_upper">upper</code></td>
<td>
<p>A vector of upper bounds on the parameters.</p>
</td></tr>
<tr><td><code id="scalechk_+3A_bdmsk">bdmsk</code></td>
<td>
<p>An indicator vector, having 1 for each parameter that is &quot;free&quot; or
unconstrained, and 0 for any parameter that is fixed or MASKED for the
duration of the optimization. May be NULL.</p>
</td></tr>
<tr><td><code id="scalechk_+3A_dowarn">dowarn</code></td>
<td>
<p>Set TRUE to issue warnings. Othwerwise this is a silent routine.
Default TRUE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The scalechk function will check that the bounds exist and are admissible, 
that is, that there are no lower bounds that exceed upper bounds. 
</p>
<p>NOTE: Free paramters outside bounds are adjusted to the nearest bound.
We then set parchanged = TRUE which implies the original parameters
were infeasible.
</p>


<h3>Value</h3>

<p>A list with components:
</p>
<p># Returns:
#   list(lpratio, lbratio) &ndash; the log of the ratio of largest to smallest parameters
#      and bounds intervals (upper-lower) in absolute value (ignoring Inf, NULL, NA)
</p>
<table>
<tr><td><code>lpratio</code></td>
<td>
<p>The log of the ratio of largest to smallest parameters
in absolute value (ignoring Inf, NULL, NA)</p>
</td></tr>
<tr><td><code>lbratio</code></td>
<td>
<p>The log of the ratio of largest to smallest bounds intervals 
(upper-lower) in absolute value (ignoring Inf, NULL, NA)</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>#####################
  par &lt;- c(-1.2, 1)
  lower &lt;- c(-2, 0)
  upper &lt;- c(100000, 10)
  srat&lt;-scalechk(par, lower, upper,dowarn=TRUE)
  print(srat)
  sratv&lt;-c(srat$lpratio, srat$lbratio)
  if (max(sratv,na.rm=TRUE) &gt; 3) { # scaletol from ctrldefault in optimx
     warnstr&lt;-"Parameters or bounds appear to have different scalings.\n
     This can cause poor performance in optimization. \n
     It is important for derivative free methods like BOBYQA, UOBYQA, NEWUOA."
     cat(warnstr,"\n")
  }

</code></pre>

<hr>
<h2 id='snewton'>Safeguarded Newton methods for function minimization using R functions.</h2><span id='topic+snewton'></span><span id='topic+snewtm'></span>

<h3>Description</h3>

<p>These versions of the safeguarded Newton solve the Newton equations with
the R function solve(). In <code>snewton</code> a backtracking line search is used,
while in <code>snewtonm</code> we rely on a Marquardt stabilization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   snewton(par, fn, gr, hess, control = list(trace=0, maxit=500), ...)

   snewtm(par, fn, gr, hess, bds, control = list(trace=0, maxit=500))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="snewton_+3A_par">par</code></td>
<td>
<p>A numeric vector of starting estimates.</p>
</td></tr>
<tr><td><code id="snewton_+3A_fn">fn</code></td>
<td>
<p>A function that returns the value of the objective at the
supplied set of parameters <code>par</code> using auxiliary data in ....
The first argument of <code>fn</code> must be <code>par</code>. </p>
</td></tr>
<tr><td><code id="snewton_+3A_gr">gr</code></td>
<td>
<p>A function that returns the gradient of the objective at the
supplied set of parameters <code>par</code> using auxiliary data in ....
The first argument of <code>fn</code> must be <code>par</code>. This function 
returns the gradient as a numeric vector.</p>
</td></tr>
<tr><td><code id="snewton_+3A_hess">hess</code></td>
<td>
<p>A function to compute the Hessian matrix. This should be provided as a square,
symmetric matrix.</p>
</td></tr>
<tr><td><code id="snewton_+3A_bds">bds</code></td>
<td>
<p>Result of <code>bmchk()</code> for the current problem. Contains lower and upper etc.</p>
</td></tr>
<tr><td><code id="snewton_+3A_control">control</code></td>
<td>

<p>An optional list of control settings.
</p>
</td></tr>
<tr><td><code id="snewton_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to <code>fn</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>snewtm</code> is intended to be called from <code>optimr()</code>.
</p>
<p>Functions <code>fn</code> must return a numeric value. <code>gr</code> must return a vector.
<code>hess</code> must return a matrix. 
The <code>control</code> argument is a list. See the code for <code>snewton.R</code> for completeness.
Some of the values that may be important for users are:
</p>

<dl>
<dt>trace</dt><dd><p>Set 0 (default) for no output, &gt; 0 for diagnostic output
(larger values imply more output).</p>
</dd>
<dt>watch</dt><dd><p>Set TRUE if the routine is to stop for user input (e.g., Enter)
after each iteration. Default is FALSE.</p>
</dd>
<dt>maxit</dt><dd><p>A limit on the number of iterations (default 500 + 2*n where n is
the number of parameters). This is the maximum number of gradient evaluations 
allowed.</p>
</dd>
<dt>maxfeval</dt><dd><p>A limit on the number of function evaluations allowed 
(default 3000 + 10*n).</p>
</dd>
<dt>eps</dt><dd><p> a tolerance used for judging small gradient norm (default = 1e-07).
a gradient norm smaller than (1 + abs(fmin))*eps*eps is considered small 
enough that a local optimum has been found, where fmin is the current 
estimate of the minimal function value. </p>
</dd>
<dt>acctol</dt><dd><p>To adjust the acceptable point tolerance (default 0.0001) in the test
( f &lt;= fmin + gradproj * steplength * acctol ). This test is used to ensure progress
is made at each iteration. </p>
</dd>
<dt>stepdec</dt><dd><p>Step reduction factor for backtrack line search (default 0.2)</p>
</dd>
<dt>defstep</dt><dd><p>Initial stepsize default (default 1)</p>
</dd>
<dt>reltest</dt><dd><p>Additive shift for equality test (default 100.0)</p>
</dd>
</dl>

<p>The (unconstrained) solver <code>snewtonmu</code> proved to be slower than the bounded solver
called without bounds, so has been withdrawn.
</p>
<p>The <code>snewton</code> safeguarded Newton uses a simple line search but no linear solution
stabilization and has demonstrated POOR performance and reliability. NOT recommended.
</p>


<h3>Value</h3>

<p>A list with components:
</p>

<dl>
<dt>par</dt><dd><p>The best set of parameters found.</p>
</dd>
<dt>value</dt><dd><p>The value of the objective at the best set of parameters found.</p>
</dd>
<dt>grad</dt><dd><p>The value of the gradient at the best set of parameters found. A vector.</p>
</dd>
<dt>hessian</dt><dd><p>The value of the Hessian at the best set of parameters found. A matrix.</p>
</dd>
<dt>counts</dt><dd><p>A vector of 4 integers giving number of Newton equation solutions, the number of function
evaluations, the number of gradient evaluations and the number of hessian evaluations.</p>
</dd>
<dt>message</dt><dd><p>A message giving some information on the status of the solution.</p>
</dd>
</dl>



<h3>References</h3>

 
<p>Nash, J C (1979, 1990) Compact Numerical Methods for Computers: Linear
Algebra and Function Minimisation, Bristol: Adam Hilger. Second
Edition, Bristol: Institute of Physics Publications.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+optim">optim</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>#Rosenbrock banana valley function
f &lt;- function(x){
return(100*(x[2] - x[1]*x[1])^2 + (1-x[1])^2)
}
#gradient
gr &lt;- function(x){
return(c(-400*x[1]*(x[2] - x[1]*x[1]) - 2*(1-x[1]), 200*(x[2] - x[1]*x[1])))
}
#Hessian
h &lt;- function(x) {
a11 &lt;- 2 - 400*x[2] + 1200*x[1]*x[1]; a21 &lt;- -400*x[1]
return(matrix(c(a11, a21, a21, 200), 2, 2))
}

fg &lt;- function(x){ #function and gradient
  val &lt;- f(x)
  attr(val,"gradient") &lt;- gr(x)
  val
}
fgh &lt;- function(x){ #function and gradient
  val &lt;- f(x)
  attr(val,"gradient") &lt;- gr(x)
  attr(val,"hessian") &lt;- h(x)
  val
}

x0 &lt;- c(-1.2, 1)

sr &lt;- snewton(x0, fn=f, gr=gr, hess=h, control=list(trace=1))
print(sr)
# Call through optimr to get correct calling sequence, esp. with bounds
srm &lt;- optimr(x0, fn=f, gr=gr, hess=h, control=list(trace=1))
print(srm)

# bounds constrained example

lo &lt;- rep((min(x0)-0.1), 2)
up &lt;- rep((max(x0)+0.1), 2)
# Call through optimr to get correct calling sequence, esp. with bounds
srmb &lt;- optimr(x0, fn=f, gr=gr, hess=h, lower=lo, upper=up, control=list(trace=1))
proptimr(srmb)


#Example 2: Wood function
#
wood.f &lt;- function(x){
  res &lt;- 100*(x[1]^2-x[2])^2+(1-x[1])^2+90*(x[3]^2-x[4])^2+(1-x[3])^2+
    10.1*((1-x[2])^2+(1-x[4])^2)+19.8*(1-x[2])*(1-x[4])
  return(res)
}
#gradient:
wood.g &lt;- function(x){
  g1 &lt;- 400*x[1]^3-400*x[1]*x[2]+2*x[1]-2
  g2 &lt;- -200*x[1]^2+220.2*x[2]+19.8*x[4]-40
  g3 &lt;- 360*x[3]^3-360*x[3]*x[4]+2*x[3]-2
  g4 &lt;- -180*x[3]^2+200.2*x[4]+19.8*x[2]-40
  return(c(g1,g2,g3,g4))
}
#hessian:
wood.h &lt;- function(x){
  h11 &lt;- 1200*x[1]^2-400*x[2]+2;    h12 &lt;- -400*x[1]; h13 &lt;- h14 &lt;- 0
  h22 &lt;- 220.2; h23 &lt;- 0;    h24 &lt;- 19.8
  h33 &lt;- 1080*x[3]^2-360*x[4]+2;    h34 &lt;- -360*x[3]
  h44 &lt;- 200.2
  H &lt;- matrix(c(h11,h12,h13,h14,h12,h22,h23,h24,
                h13,h23,h33,h34,h14,h24,h34,h44),ncol=4)
  return(H)
}
#################################################
w0 &lt;- c(-3, -1, -3, -1)

wd &lt;- snewton(w0, fn=wood.f, gr=wood.g, hess=wood.h, control=list(trace=1))
print(wd)

# Call through optimr to get correct calling sequence, esp. with bounds
wdm &lt;- optimr(w0, fn=wood.f, gr=wood.g, hess=wood.h, control=list(trace=1))
print(wdm)

</code></pre>

<hr>
<h2 id='summary.optimx'>Summarize optimx object</h2><span id='topic+summary.optimx'></span>

<h3>Description</h3>

<p>Summarize an <code>"optimx"</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  ## S3 method for class 'optimx'
summary(object, order = NULL, par.select = TRUE, ...) 

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.optimx_+3A_object">object</code></td>
<td>
<p>Object returned by <code>optimx</code>.</p>
</td></tr>
<tr><td><code id="summary.optimx_+3A_order">order</code></td>
<td>
<p>A column name, character vector of columns names, R 
expression in terms of column names or a list of R expressions in terms of
column names.   <code>NULL</code>, the default, means no re-ordering.
<code>rownames</code> can be used to 
alphabetic ordering by method name.
<code>NULL</code>, the default, causes
it not to be reordered.  Note that if 
<code>follow.on</code> is TRUE re-ordering likely makes no sense. 
The result is ordered by the <code>order</code>
specification, each specified column in ascending order (except for 
<code>value</code> which is in descending order if the optimization problem is a 
maximization problem).</p>
</td></tr>
<tr><td><code id="summary.optimx_+3A_par.select">par.select</code></td>
<td>
<p>a numeric, character or logical vector selecting
those <code>par</code> values to display. For example, <code>par=1:5</code> means
display only the first 5 parameters.  Recycled so <code>par.select=FALSE</code>
selects no parameters.</p>
</td></tr>
<tr><td><code id="summary.optimx_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to the function. Currently not
used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>order</code> is specified then the result is reordered by the specified
columns, each in ascending order (except possibly for the
<code>value</code> column which is re-ordered in descending order for
maximization problems).</p>


<h3>Value</h3>

<p><code>summary.optimx</code> returns <code>object</code> with the rows ordered according
to <code>order</code> and with those parameters selected by <code>par.select</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ans &lt;- optimx(fn = function(x) sum(x*x), par = 1:2)

# order by method name.
summary(ans, order = rownames)

# order by objective value. Do not show parameter values.
summary(ans, order = value, par.select = FALSE)

# order by objective value and then number of function evaluations
# such that objectives that are the same to 3 decimals are 
# considered the same.  Show only first parameter.
summary(ans, order = list(round(value, 3), fevals), par.select = 1)
</code></pre>

<hr>
<h2 id='tn'>Truncated Newton minimization of an unconstrained function.</h2><span id='topic+tn'></span>

<h3>Description</h3>

<p>An R implementation of the Truncated Newton method
of Stephen Nash for  driver to call the unconstrained function
minimization. The algorithm is based on Nash (1979) 
</p>
<p>This set of codes is entirely in R to allow users to explore and
understand the method. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   tn(x, fgfun, trace, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tn_+3A_x">x</code></td>
<td>
<p>A numeric vector of starting estimates.</p>
</td></tr>
<tr><td><code id="tn_+3A_fgfun">fgfun</code></td>
<td>
<p>A function that returns the value of the objective at
the supplied set of parameters <code>par</code> using auxiliary data in
.... The gradient is returned as attribute &quot;gradient&quot;. 
The first argument of <code>fgfun</code> must be <code>par</code>. </p>
</td></tr>
<tr><td><code id="tn_+3A_trace">trace</code></td>
<td>
<p> &gt; 0 if progress output is to be presented. </p>
</td></tr>
<tr><td><code id="tn_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to <code>fn</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function <code>fgfun</code> must return a numeric value in list item <code>f</code>
and a numeric vector in list item <code>g</code>.
</p>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>xstar</code></td>
<td>
<p>The best set of parameters found.</p>
</td></tr>
<tr><td><code>f</code></td>
<td>
<p>The value of the objective at the best set of parameters found.</p>
</td></tr>
<tr><td><code>g</code></td>
<td>
<p>The gradient of the objective at the best set of parameters found.</p>
</td></tr>
<tr><td><code>ierror</code></td>
<td>
<p>An integer indicating the situation on termination. <code>0</code>
indicates that the method believes it has succeeded; <code>2</code> that
more than <code>maxfun</code> (default 150*n, where there are n parameters);
<code>3</code> if the line search appears to have failed (which may not be serious);
and <code>-1</code> if there appears to be an error in the input parameters.</p>
</td></tr>
<tr><td><code>nfngr</code></td>
<td>
<p>A number giving a measure of how many conjugate gradient solutions
were used during the minimization process.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Stephen G. Nash (1984) &quot;Newton-type minimization via the Lanczos method&quot;,
SIAM J Numerical Analysis, vol. 21, no. 4, pages 770-788.
</p>
<p>For Matlab code, see http://www.netlib.org/opt/tn
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+optim">optim</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>#####################
## All examples are in this .Rd file
##
## Rosenbrock Banana function
fr &lt;- function(x) {
    x1 &lt;- x[1]
    x2 &lt;- x[2]
    100 * (x2 - x1 * x1)^2 + (1 - x1)^2
}
gr &lt;- function(x) {
    x1 &lt;- x[1]
    x2 &lt;- x[2]
    g1 &lt;- -400 * (x2 - x1*x1) * x1 - 2*(1-x1)
    g2 &lt;- 200*(x2 - x1*x1) 
    gg&lt;-c(g1, g2)
}

rosefg&lt;-function(x){
   f&lt;-fr(x)
   g&lt;-gr(x)
   attr(f, "gradient") &lt;- g
   f
}

x&lt;-c(-1.2, 1)

ansrosenbrock &lt;- tn(x, rosefg)
print(ansrosenbrock) # use print to allow copy to separate file that 
cat("Compare to optim\n")
ansoptrose &lt;- optim(x, fr, gr)
print(ansoptrose)


genrose.f&lt;- function(x, gs=NULL){ # objective function
## One generalization of the Rosenbrock banana valley function (n parameters)
	n &lt;- length(x)
        if(is.null(gs)) { gs=100.0 }
	fval&lt;-1.0 + sum (gs*(x[1:(n-1)]^2 - x[2:n])^2 + (x[2:n] - 1)^2)
        return(fval)
}
genrose.g &lt;- function(x, gs=NULL){
# vectorized gradient for genrose.f
# Ravi Varadhan 2009-04-03
	n &lt;- length(x)
        if(is.null(gs)) { gs=100.0 }
	gg &lt;- as.vector(rep(0, n))
	tn &lt;- 2:n
	tn1 &lt;- tn - 1
	z1 &lt;- x[tn] - x[tn1]^2
	z2 &lt;- 1 - x[tn]
	gg[tn] &lt;- 2 * (gs * z1 - z2)
	gg[tn1] &lt;- gg[tn1] - 4 * gs * x[tn1] * z1
	gg
}

grosefg&lt;-function(x, gs=100.0) {
    f&lt;-genrose.f(x, gs)
    g&lt;-genrose.g(x, gs)
    attr(f, "gradient") &lt;- g
    f
}

n &lt;- 100
x &lt;- (1:100)/20
groseu&lt;-tn(x, grosefg, gs=10)
print(groseu)

groseuo &lt;- optim(x, fn=genrose.f, gr=genrose.g, method="BFGS",
      control=list(maxit=1000), gs=10)
cat("compare optim BFGS\n")
print(groseuo)


lower&lt;-1+(1:n)/100
upper&lt;-5-(1:n)/100
xmid&lt;-0.5*(lower+upper)

grosec&lt;-tnbc(xmid, grosefg, lower, upper)
print(grosec)

cat("compare L-BFGS-B\n")
grosecl &lt;- optim(par=xmid, fn=genrose.f, gr=genrose.g, 
     lower=lower, upper=upper, method="L-BFGS-B")
print(grosecl)


</code></pre>

<hr>
<h2 id='tnbc'>Truncated Newton function minimization with bounds constraints</h2><span id='topic+tnbc'></span>

<h3>Description</h3>

<p>A bounds-constarined R implementation of a truncated Newton method
for minimization of nonlinear functions subject to bounds (box) constraints. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   tnbc(x, fgfun, lower, upper, trace=0, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tnbc_+3A_x">x</code></td>
<td>
<p>A numeric vector of starting estimates.</p>
</td></tr>
<tr><td><code id="tnbc_+3A_fgfun">fgfun</code></td>
<td>
<p>A function that returns the value of the objective at
the supplied set of parameters <code>par</code> using auxiliary data in
.... The gradient is returned as attribute &quot;gradient&quot;. 
The first argument of <code>fgfun</code> must be <code>par</code>. </p>
</td></tr>
<tr><td><code id="tnbc_+3A_lower">lower</code></td>
<td>
<p>A vector of lower bounds on the parameters.</p>
</td></tr>
<tr><td><code id="tnbc_+3A_upper">upper</code></td>
<td>
<p>A vector of upper bounds on the parameters.</p>
</td></tr>
<tr><td><code id="tnbc_+3A_trace">trace</code></td>
<td>
<p>Set &gt;0 to cause intermediate output to allow progress
to be followed.</p>
</td></tr>
<tr><td><code id="tnbc_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to <code>fn</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function <code>fgfun</code> must return a numeric value in list item <code>f</code>
and a numeric vector in list item <code>g</code>.
</p>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>xstar</code></td>
<td>
<p>The best set of parameters found.</p>
</td></tr>
<tr><td><code>f</code></td>
<td>
<p>The value of the objective at the best set of parameters found.</p>
</td></tr>
<tr><td><code>g</code></td>
<td>
<p>The gradient of the objective at the best set of parameters found.</p>
</td></tr>
<tr><td><code>ierror</code></td>
<td>
<p>An integer indicating the situation on termination. <code>0</code>
indicates that the method believes it has succeeded; <code>2</code> that
more than <code>maxfun</code> (default 150*n, where there are n parameters);
<code>3</code> if the line search appears to have failed (which may not be serious);
and <code>-1</code> if there appears to be an error in the input parameters.</p>
</td></tr>
<tr><td><code>nfngr</code></td>
<td>
<p>A number giving a measure of how many conjugate gradient solutions
were used during the minimization process.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Stephen G. Nash (1984) &quot;Newton-type minimization via the Lanczos method&quot;,
SIAM J Numerical Analysis, vol. 21, no. 4, pages 770-788.
</p>
<p>For Matlab code, see http://www.netlib.org/opt/tn
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+optim">optim</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## See tn.Rd

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
