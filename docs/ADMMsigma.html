<!DOCTYPE html><html><head><title>Help for package ADMMsigma</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ADMMsigma}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ADMMc'><p>Penalized precision matrix estimation via ADMM (c++)</p></a></li>
<li><a href='#ADMMsigma'><p>Penalized precision matrix estimation via ADMM</p></a></li>
<li><a href='#CV_ADMMc'><p>CV ADMM penalized precision matrix estimation (c++)</p></a></li>
<li><a href='#CV_RIDGEc'><p>CV ridge penalized precision matrix estimation (c++)</p></a></li>
<li><a href='#CVP_ADMM'><p>Parallel CV (uses CV_ADMMc)</p></a></li>
<li><a href='#CVP_ADMMc'><p>CV (no folds) ADMM penalized precision matrix estimation (c++)</p></a></li>
<li><a href='#CVP_RIDGE'><p>Parallel Ridge CV (uses CVP_RIDGEc)</p></a></li>
<li><a href='#CVP_RIDGEc'><p>CV (no folds) RIDGE penalized precision matrix estimation (c++)</p></a></li>
<li><a href='#plot.ADMM'><p>Plot ADMM object</p></a></li>
<li><a href='#plot.RIDGE'><p>Plot RIDGE object</p></a></li>
<li><a href='#print.ADMM'><p>Print ADMM object</p></a></li>
<li><a href='#print.RIDGE'><p>Print RIDGE object</p></a></li>
<li><a href='#RIDGEc'><p>Ridge-penalized precision matrix estimation (c++)</p></a></li>
<li><a href='#RIDGEsigma'><p>Ridge penalized precision matrix estimation</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Penalized Precision Matrix Estimation via ADMM</td>
</tr>
<tr>
<td>Version:</td>
<td>2.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2018-07-31</td>
</tr>
<tr>
<td>Description:</td>
<td>Estimates a penalized precision matrix via the alternating direction method of multipliers (ADMM) algorithm. It currently supports a general elastic-net penalty that allows for both ridge and lasso-type penalties as special cases. This package is an alternative to the 'glasso' package.
    See Boyd et al (2010) &lt;<a href="https://doi.org/10.1561%2F2200000016">doi:10.1561/2200000016</a>&gt; for details regarding the estimation method.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/MGallow/ADMMsigma">https://github.com/MGallow/ADMMsigma</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/MGallow/ADMMsigma/issues">https://github.com/MGallow/ADMMsigma/issues</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>TRUE</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.0.1</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, parallel, foreach, ggplot2, dplyr</td>
</tr>
<tr>
<td>Depends:</td>
<td>Rcpp (&ge; 0.12.10), RcppProgress (&ge; 0.1), doParallel</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo, RcppProgress</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, knitr, rmarkdown, microbenchmark, pkgdown</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>GNU make</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2018-08-02 00:40:10 UTC; Matt</td>
</tr>
<tr>
<td>Author:</td>
<td>Matt Galloway [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Matt Galloway &lt;gall0441@umn.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2018-08-02 17:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='ADMMc'>Penalized precision matrix estimation via ADMM (c++)</h2><span id='topic+ADMMc'></span>

<h3>Description</h3>

<p>Penalized precision matrix estimation using the ADMM algorithm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ADMMc(S, initOmega, initZ, initY, lam, alpha = 1, diagonal = FALSE,
  rho = 2, mu = 10, tau_inc = 2, tau_dec = 2, crit = "ADMM",
  tol_abs = 1e-04, tol_rel = 1e-04, maxit = 10000L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ADMMc_+3A_s">S</code></td>
<td>
<p>pxp sample covariance matrix (denominator n).</p>
</td></tr>
<tr><td><code id="ADMMc_+3A_initomega">initOmega</code></td>
<td>
<p>initialization matrix for Omega</p>
</td></tr>
<tr><td><code id="ADMMc_+3A_initz">initZ</code></td>
<td>
<p>initialization matrix for Z</p>
</td></tr>
<tr><td><code id="ADMMc_+3A_inity">initY</code></td>
<td>
<p>initialization matrix for Y</p>
</td></tr>
<tr><td><code id="ADMMc_+3A_lam">lam</code></td>
<td>
<p>postive tuning parameter for elastic net penalty.</p>
</td></tr>
<tr><td><code id="ADMMc_+3A_alpha">alpha</code></td>
<td>
<p>elastic net mixing parameter contained in [0, 1]. <code>0 = ridge, 1 = lasso</code>. Defaults to alpha = 1.</p>
</td></tr>
<tr><td><code id="ADMMc_+3A_diagonal">diagonal</code></td>
<td>
<p>option to penalize the diagonal elements of the estimated precision matrix (<code class="reqn">\Omega</code>). Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="ADMMc_+3A_rho">rho</code></td>
<td>
<p>initial step size for ADMM algorithm.</p>
</td></tr>
<tr><td><code id="ADMMc_+3A_mu">mu</code></td>
<td>
<p>factor for primal and residual norms in the ADMM algorithm. This will be used to adjust the step size <code>rho</code> after each iteration.</p>
</td></tr>
<tr><td><code id="ADMMc_+3A_tau_inc">tau_inc</code></td>
<td>
<p>factor in which to increase step size <code>rho</code>.</p>
</td></tr>
<tr><td><code id="ADMMc_+3A_tau_dec">tau_dec</code></td>
<td>
<p>factor in which to decrease step size <code>rho</code>.</p>
</td></tr>
<tr><td><code id="ADMMc_+3A_crit">crit</code></td>
<td>
<p>criterion for convergence (<code>ADMM</code> or <code>loglik</code>). If <code>crit = loglik</code> then iterations will stop when the relative change in log-likelihood is less than <code>tol.abs</code>. Default is <code>ADMM</code> and follows the procedure outlined in Boyd, et al.</p>
</td></tr>
<tr><td><code id="ADMMc_+3A_tol_abs">tol_abs</code></td>
<td>
<p>absolute convergence tolerance. Defaults to 1e-4.</p>
</td></tr>
<tr><td><code id="ADMMc_+3A_tol_rel">tol_rel</code></td>
<td>
<p>relative convergence tolerance. Defaults to 1e-4.</p>
</td></tr>
<tr><td><code id="ADMMc_+3A_maxit">maxit</code></td>
<td>
<p>maximum number of iterations. Defaults to 1e4.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For details on the implementation of 'ADMMsigma', see the vignette
<a href="https://mgallow.github.io/ADMMsigma/">https://mgallow.github.io/ADMMsigma/</a>.
</p>


<h3>Value</h3>

<p>returns list of returns which includes:
</p>
<table>
<tr><td><code>Iterations</code></td>
<td>
<p>number of iterations.</p>
</td></tr>
<tr><td><code>lam</code></td>
<td>
<p>optimal tuning parameters.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>optimal tuning parameter.</p>
</td></tr>
<tr><td><code>Omega</code></td>
<td>
<p>estimated penalized precision matrix.</p>
</td></tr>
<tr><td><code>Z2</code></td>
<td>
<p>estimated Z matrix.</p>
</td></tr>
<tr><td><code>Y</code></td>
<td>
<p>estimated Y matrix.</p>
</td></tr>
<tr><td><code>rho</code></td>
<td>
<p>estimated rho.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Matt Galloway <a href="mailto:gall0441@umn.edu">gall0441@umn.edu</a>
</p>


<h3>References</h3>


<ul>
<li><p> Boyd, Stephen, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, and others. 2011. 'Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers.' <em>Foundations and Trends in Machine Learning</em> 3 (1). Now Publishers, Inc.: 1-122. <a href="https://web.stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf">https://web.stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf</a>
</p>
</li>
<li><p> Hu, Yue, Chi, Eric C, amd Allen, Genevera I. 2016. 'ADMM Algorithmic Regularization Paths for Sparse Statistical Machine Learning.' <em>Splitting Methods in Communication, Imaging, Science, and Engineering</em>. Springer: 433-459.
</p>
</li>
<li><p> Zou, Hui and Hastie, Trevor. 2005. &quot;Regularization and Variable Selection via the Elastic Net.&quot; <em>Journal of the Royal Statistial Society: Series B (Statistical Methodology)</em> 67 (2). Wiley Online Library: 301-320.
</p>
</li>
<li><p> Rothman, Adam. 2017. 'STAT 8931 notes on an algorithm to compute the Lasso-penalized Gaussian likelihood precision matrix estimator.'
</p>
</li></ul>


<hr>
<h2 id='ADMMsigma'>Penalized precision matrix estimation via ADMM</h2><span id='topic+ADMMsigma'></span>

<h3>Description</h3>

<p>Penalized precision matrix estimation using the ADMM algorithm.
Consider the case where <code class="reqn">X_{1}, ..., X_{n}</code> are iid <code class="reqn">N_{p}(\mu,
\Sigma)</code> and we are tasked with estimating the precision matrix,
denoted <code class="reqn">\Omega \equiv \Sigma^{-1}</code>. This function solves the
following optimization problem:
</p>

<dl>
<dt>Objective:</dt><dd>
<p><code class="reqn">\hat{\Omega}_{\lambda} = \arg\min_{\Omega \in S_{+}^{p}}
\left\{ Tr\left(S\Omega\right) - \log \det\left(\Omega \right) +
\lambda\left[\frac{1 - \alpha}{2}\left\| \Omega \right|_{F}^{2} +
\alpha\left\| \Omega \right\|_{1} \right] \right\}</code></p>
</dd>
</dl>

<p>where <code class="reqn">0 \leq \alpha \leq 1</code>, <code class="reqn">\lambda &gt; 0</code>,
<code class="reqn">\left\|\cdot \right\|_{F}^{2}</code> is the Frobenius norm and we define
<code class="reqn">\left\|A \right\|_{1} = \sum_{i, j} \left| A_{ij} \right|</code>.
This elastic net penalty is identical to the penalty used in the popular penalized
regression package <code>glmnet</code>. Clearly, when <code class="reqn">\alpha = 0</code> the elastic-net
reduces to a ridge-type penalty and when <code class="reqn">\alpha = 1</code> it reduces to a
lasso-type penalty.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ADMMsigma(X = NULL, S = NULL, nlam = 10, lam.min.ratio = 0.01,
  lam = NULL, alpha = seq(0, 1, 0.2), diagonal = FALSE, path = FALSE,
  rho = 2, mu = 10, tau.inc = 2, tau.dec = 2, crit = c("ADMM",
  "loglik"), tol.abs = 1e-04, tol.rel = 1e-04, maxit = 10000,
  adjmaxit = NULL, K = 5, crit.cv = c("loglik", "penloglik", "AIC",
  "BIC"), start = c("warm", "cold"), cores = 1, trace = c("progress",
  "print", "none"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ADMMsigma_+3A_x">X</code></td>
<td>
<p>option to provide a nxp data matrix. Each row corresponds to a single observation and each column contains n observations of a single feature/variable.</p>
</td></tr>
<tr><td><code id="ADMMsigma_+3A_s">S</code></td>
<td>
<p>option to provide a pxp sample covariance matrix (denominator n). If argument is <code>NULL</code> and <code>X</code> is provided instead then <code>S</code> will be computed automatically.</p>
</td></tr>
<tr><td><code id="ADMMsigma_+3A_nlam">nlam</code></td>
<td>
<p>number of <code>lam</code> tuning parameters for penalty term generated from <code>lam.min.ratio</code> and <code>lam.max</code> (automatically generated). Defaults to 10.</p>
</td></tr>
<tr><td><code id="ADMMsigma_+3A_lam.min.ratio">lam.min.ratio</code></td>
<td>
<p>smallest <code>lam</code> value provided as a fraction of <code>lam.max</code>. The function will automatically generate <code>nlam</code> tuning parameters from <code>lam.min.ratio*lam.max</code> to <code>lam.max</code> in log10 scale. <code>lam.max</code> is calculated to be the smallest <code>lam</code> such that all off-diagonal entries in <code>Omega</code> are equal to zero (<code>alpha</code> = 1). Defaults to 1e-2.</p>
</td></tr>
<tr><td><code id="ADMMsigma_+3A_lam">lam</code></td>
<td>
<p>option to provide positive tuning parameters for penalty term. This will cause <code>nlam</code> and <code>lam.min.ratio</code> to be disregarded. If a vector of parameters is provided, they should be in increasing order. Defaults to NULL.</p>
</td></tr>
<tr><td><code id="ADMMsigma_+3A_alpha">alpha</code></td>
<td>
<p>elastic net mixing parameter contained in [0, 1]. <code>0 = ridge, 1 = lasso</code>. If a vector of parameters is provided, they should be in increasing order. Defaults to grid of values <code>seq(0, 1, 0.2)</code>.</p>
</td></tr>
<tr><td><code id="ADMMsigma_+3A_diagonal">diagonal</code></td>
<td>
<p>option to penalize the diagonal elements of the estimated precision matrix (<code class="reqn">\Omega</code>). Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="ADMMsigma_+3A_path">path</code></td>
<td>
<p>option to return the regularization path. This option should be used with extreme care if the dimension is large. If set to TRUE, cores must be set to 1 and errors and optimal tuning parameters will based on the full sample. Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="ADMMsigma_+3A_rho">rho</code></td>
<td>
<p>initial step size for ADMM algorithm.</p>
</td></tr>
<tr><td><code id="ADMMsigma_+3A_mu">mu</code></td>
<td>
<p>factor for primal and residual norms in the ADMM algorithm. This will be used to adjust the step size <code>rho</code> after each iteration.</p>
</td></tr>
<tr><td><code id="ADMMsigma_+3A_tau.inc">tau.inc</code></td>
<td>
<p>factor in which to increase step size <code>rho</code></p>
</td></tr>
<tr><td><code id="ADMMsigma_+3A_tau.dec">tau.dec</code></td>
<td>
<p>factor in which to decrease step size <code>rho</code></p>
</td></tr>
<tr><td><code id="ADMMsigma_+3A_crit">crit</code></td>
<td>
<p>criterion for convergence (<code>ADMM</code> or <code>loglik</code>). If <code>crit = loglik</code> then iterations will stop when the relative change in log-likelihood is less than <code>tol.abs</code>. Default is <code>ADMM</code> and follows the procedure outlined in Boyd, et al.</p>
</td></tr>
<tr><td><code id="ADMMsigma_+3A_tol.abs">tol.abs</code></td>
<td>
<p>absolute convergence tolerance. Defaults to 1e-4.</p>
</td></tr>
<tr><td><code id="ADMMsigma_+3A_tol.rel">tol.rel</code></td>
<td>
<p>relative convergence tolerance. Defaults to 1e-4.</p>
</td></tr>
<tr><td><code id="ADMMsigma_+3A_maxit">maxit</code></td>
<td>
<p>maximum number of iterations. Defaults to 1e4.</p>
</td></tr>
<tr><td><code id="ADMMsigma_+3A_adjmaxit">adjmaxit</code></td>
<td>
<p>adjusted maximum number of iterations. During cross validation this option allows the user to adjust the maximum number of iterations after the first <code>lam</code> tuning parameter has converged (for each <code>alpha</code>). This option is intended to be paired with <code>warm</code> starts and allows for 'one-step' estimators. Defaults to NULL.</p>
</td></tr>
<tr><td><code id="ADMMsigma_+3A_k">K</code></td>
<td>
<p>specify the number of folds for cross validation.</p>
</td></tr>
<tr><td><code id="ADMMsigma_+3A_crit.cv">crit.cv</code></td>
<td>
<p>cross validation criterion (<code>loglik</code>, <code>penloglik</code>, <code>AIC</code>, or <code>BIC</code>). Defaults to <code>loglik</code>.</p>
</td></tr>
<tr><td><code id="ADMMsigma_+3A_start">start</code></td>
<td>
<p>specify <code>warm</code> or <code>cold</code> start for cross validation. Default is <code>warm</code>.</p>
</td></tr>
<tr><td><code id="ADMMsigma_+3A_cores">cores</code></td>
<td>
<p>option to run CV in parallel. Defaults to <code>cores = 1</code>.</p>
</td></tr>
<tr><td><code id="ADMMsigma_+3A_trace">trace</code></td>
<td>
<p>option to display progress of CV. Choose one of <code>progress</code> to print a progress bar, <code>print</code> to print completed tuning parameters, or <code>none</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For details on the implementation of 'ADMMsigma', see the website
<a href="https://mgallow.github.io/ADMMsigma/articles/Details.html">https://mgallow.github.io/ADMMsigma/articles/Details.html</a>.
</p>


<h3>Value</h3>

<p>returns class object <code>ADMMsigma</code> which includes:
</p>
<table>
<tr><td><code>Call</code></td>
<td>
<p>function call.</p>
</td></tr>
<tr><td><code>Iterations</code></td>
<td>
<p>number of iterations.</p>
</td></tr>
<tr><td><code>Tuning</code></td>
<td>
<p>optimal tuning parameters (lam and alpha).</p>
</td></tr>
<tr><td><code>Lambdas</code></td>
<td>
<p>grid of lambda values for CV.</p>
</td></tr>
<tr><td><code>Alphas</code></td>
<td>
<p>grid of alpha values for CV.</p>
</td></tr>
<tr><td><code>maxit</code></td>
<td>
<p>maximum number of iterations.</p>
</td></tr>
<tr><td><code>Omega</code></td>
<td>
<p>estimated penalized precision matrix.</p>
</td></tr>
<tr><td><code>Sigma</code></td>
<td>
<p>estimated covariance matrix from the penalized precision matrix (inverse of Omega).</p>
</td></tr>
<tr><td><code>Path</code></td>
<td>
<p>array containing the solution path. Solutions will be ordered in ascending alpha values for each lambda.</p>
</td></tr>
<tr><td><code>Z</code></td>
<td>
<p>final sparse update of estimated penalized precision matrix.</p>
</td></tr>
<tr><td><code>Y</code></td>
<td>
<p>final dual update.</p>
</td></tr>
<tr><td><code>rho</code></td>
<td>
<p>final step size.</p>
</td></tr>
<tr><td><code>Loglik</code></td>
<td>
<p>penalized log-likelihood for Omega</p>
</td></tr>
<tr><td><code>MIN.error</code></td>
<td>
<p>minimum average cross validation error (cv.crit) for optimal parameters.</p>
</td></tr>
<tr><td><code>AVG.error</code></td>
<td>
<p>average cross validation error (cv.crit) across all folds.</p>
</td></tr>
<tr><td><code>CV.error</code></td>
<td>
<p>cross validation errors (cv.crit).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Matt Galloway <a href="mailto:gall0441@umn.edu">gall0441@umn.edu</a>
</p>


<h3>References</h3>


<ul>
<li><p> Boyd, Stephen, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, and others. 2011. 'Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers.' <em>Foundations and Trends in Machine Learning</em> 3 (1). Now Publishers, Inc.: 1-122. <a href="https://web.stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf">https://web.stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf</a>
</p>
</li>
<li><p> Hu, Yue, Chi, Eric C, amd Allen, Genevera I. 2016. 'ADMM Algorithmic Regularization Paths for Sparse Statistical Machine Learning.' <em>Splitting Methods in Communication, Imaging, Science, and Engineering</em>. Springer: 433-459.
</p>
</li>
<li><p> Zou, Hui and Hastie, Trevor. 2005. 'Regularization and Variable Selection via the Elastic Net.' <em>Journal of the Royal Statistial Society: Series B (Statistical Methodology)</em> 67 (2). Wiley Online Library: 301-320.
</p>
</li>
<li><p> Rothman, Adam. 2017. 'STAT 8931 notes on an algorithm to compute the Lasso-penalized Gaussian likelihood precision matrix estimator.'
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+plot.ADMM">plot.ADMM</a></code>, <code><a href="#topic+RIDGEsigma">RIDGEsigma</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># generate data from a sparse matrix
# first compute covariance matrix
S = matrix(0.7, nrow = 5, ncol = 5)
for (i in 1:5){
 for (j in 1:5){
   S[i, j] = S[i, j]^abs(i - j)
 }
 }

# generate 100 x 5 matrix with rows drawn from iid N_p(0, S)
set.seed(123)
Z = matrix(rnorm(100*5), nrow = 100, ncol = 5)
out = eigen(S, symmetric = TRUE)
S.sqrt = out$vectors %*% diag(out$values^0.5)
S.sqrt = S.sqrt %*% t(out$vectors)
X = Z %*% S.sqrt

# elastic-net type penalty (use CV for optimal lambda and alpha)
ADMMsigma(X)

# ridge penalty (use CV for optimal lambda)
ADMMsigma(X, alpha = 0)

# lasso penalty (lam = 0.1)
ADMMsigma(X, lam = 0.1, alpha = 1)
</code></pre>

<hr>
<h2 id='CV_ADMMc'>CV ADMM penalized precision matrix estimation (c++)</h2><span id='topic+CV_ADMMc'></span>

<h3>Description</h3>

<p>Cross validation function for ADMMsigma.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CV_ADMMc(X, S, lam, alpha, diagonal = FALSE, path = FALSE, rho = 2,
  mu = 10, tau_inc = 2, tau_dec = 2, crit = "ADMM", tol_abs = 1e-04,
  tol_rel = 1e-04, maxit = 10000L, adjmaxit = 10000L, K = 5L,
  crit_cv = "loglik", start = "warm", trace = "progress")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CV_ADMMc_+3A_x">X</code></td>
<td>
<p>option to provide a nxp matrix. Each row corresponds to a single observation and each column contains n observations of a single feature/variable.</p>
</td></tr>
<tr><td><code id="CV_ADMMc_+3A_s">S</code></td>
<td>
<p>option to provide a pxp sample covariance matrix (denominator n). If argument is <code>NULL</code> and <code>X</code> is provided instead then <code>S</code> will be computed automatically.</p>
</td></tr>
<tr><td><code id="CV_ADMMc_+3A_lam">lam</code></td>
<td>
<p>positive tuning parameters for elastic net penalty. If a vector of parameters is provided, they should be in increasing order.</p>
</td></tr>
<tr><td><code id="CV_ADMMc_+3A_alpha">alpha</code></td>
<td>
<p>elastic net mixing parameter contained in [0, 1]. <code>0 = ridge, 1 = lasso</code>. If a vector of parameters is provided, they should be in increasing order.</p>
</td></tr>
<tr><td><code id="CV_ADMMc_+3A_diagonal">diagonal</code></td>
<td>
<p>option to penalize the diagonal elements of the estimated precision matrix (<code class="reqn">\Omega</code>). Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="CV_ADMMc_+3A_path">path</code></td>
<td>
<p>option to return the regularization path. This option should be used with extreme care if the dimension is large. If set to TRUE, cores will be set to 1 and errors and optimal tuning parameters will based on the full sample. Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="CV_ADMMc_+3A_rho">rho</code></td>
<td>
<p>initial step size for ADMM algorithm.</p>
</td></tr>
<tr><td><code id="CV_ADMMc_+3A_mu">mu</code></td>
<td>
<p>factor for primal and residual norms in the ADMM algorithm. This will be used to adjust the step size <code>rho</code> after each iteration.</p>
</td></tr>
<tr><td><code id="CV_ADMMc_+3A_tau_inc">tau_inc</code></td>
<td>
<p>factor in which to increase step size <code>rho</code></p>
</td></tr>
<tr><td><code id="CV_ADMMc_+3A_tau_dec">tau_dec</code></td>
<td>
<p>factor in which to decrease step size <code>rho</code></p>
</td></tr>
<tr><td><code id="CV_ADMMc_+3A_crit">crit</code></td>
<td>
<p>criterion for convergence (<code>ADMM</code> or <code>loglik</code>). If <code>crit = loglik</code> then iterations will stop when the relative change in log-likelihood is less than <code>tol.abs</code>. Default is <code>ADMM</code> and follows the procedure outlined in Boyd, et al.</p>
</td></tr>
<tr><td><code id="CV_ADMMc_+3A_tol_rel">tol_rel</code></td>
<td>
<p>relative convergence tolerance. Defaults to 1e-4.</p>
</td></tr>
<tr><td><code id="CV_ADMMc_+3A_maxit">maxit</code></td>
<td>
<p>maximum number of iterations. Defaults to 1e4.</p>
</td></tr>
<tr><td><code id="CV_ADMMc_+3A_adjmaxit">adjmaxit</code></td>
<td>
<p>adjusted maximum number of iterations. During cross validation this option allows the user to adjust the maximum number of iterations after the first <code>lam</code> tuning parameter has converged (for each <code>alpha</code>). This option is intended to be paired with <code>warm</code> starts and allows for &quot;one-step&quot; estimators. Defaults to 1e4.</p>
</td></tr>
<tr><td><code id="CV_ADMMc_+3A_k">K</code></td>
<td>
<p>specify the number of folds for cross validation.</p>
</td></tr>
<tr><td><code id="CV_ADMMc_+3A_crit_cv">crit_cv</code></td>
<td>
<p>cross validation criterion (<code>loglik</code> <code>penloglik</code>, <code>AIC</code>, or <code>BIC</code>). Defaults to <code>loglik</code>.</p>
</td></tr>
<tr><td><code id="CV_ADMMc_+3A_start">start</code></td>
<td>
<p>specify <code>warm</code> or <code>cold</code> start for cross validation. Default is <code>warm</code>.</p>
</td></tr>
<tr><td><code id="CV_ADMMc_+3A_trace">trace</code></td>
<td>
<p>option to display progress of CV. Choose one of <code>progress</code> to print a progress bar, <code>print</code> to print completed tuning parameters, or <code>none</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of returns includes:
</p>
<table>
<tr><td><code>lam</code></td>
<td>
<p>optimal tuning parameter.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>optimal tuning parameter.</p>
</td></tr>
<tr><td><code>path</code></td>
<td>
<p>array containing the solution path. Solutions will be ordered in ascending alpha values for each lambda.</p>
</td></tr>
<tr><td><code>min.error</code></td>
<td>
<p>minimum average cross validation error (cv_crit) for optimal parameters.</p>
</td></tr>
<tr><td><code>avg.error</code></td>
<td>
<p>average cross validation error (cv_crit) across all folds.</p>
</td></tr>
<tr><td><code>cv.error</code></td>
<td>
<p>cross validation errors (cv_crit).</p>
</td></tr>
</table>

<hr>
<h2 id='CV_RIDGEc'>CV ridge penalized precision matrix estimation (c++)</h2><span id='topic+CV_RIDGEc'></span>

<h3>Description</h3>

<p>Cross validation function for RIDGEsigma.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CV_RIDGEc(X, S, lam, path = FALSE, K = 3L, trace = "none")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CV_RIDGEc_+3A_x">X</code></td>
<td>
<p>option to provide a nxp matrix. Each row corresponds to a single observation and each column contains n observations of a single feature/variable.</p>
</td></tr>
<tr><td><code id="CV_RIDGEc_+3A_s">S</code></td>
<td>
<p>option to provide a pxp sample covariance matrix (denominator n). If argument is <code>NULL</code> and <code>X</code> is provided instead then <code>S</code> will be computed automatically.</p>
</td></tr>
<tr><td><code id="CV_RIDGEc_+3A_lam">lam</code></td>
<td>
<p>positive tuning parameters for ridge penalty. If a vector of parameters is provided, they should be in increasing order. Defaults to grid of values <code>10^seq(-5, 5, 0.5)</code>.</p>
</td></tr>
<tr><td><code id="CV_RIDGEc_+3A_path">path</code></td>
<td>
<p>option to return the regularization path. This option should be used with extreme care if the dimension is large. If set to TRUE, cores will be set to 1 and errors and optimal tuning parameters will based on the full sample. Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="CV_RIDGEc_+3A_k">K</code></td>
<td>
<p>specify the number of folds for cross validation.</p>
</td></tr>
<tr><td><code id="CV_RIDGEc_+3A_trace">trace</code></td>
<td>
<p>option to display progress of CV. Choose one of <code>progress</code> to print a progress bar, <code>print</code> to print completed tuning parameters, or <code>none</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of returns includes:
</p>
<table>
<tr><td><code>lam</code></td>
<td>
<p>optimal tuning parameter.</p>
</td></tr>
<tr><td><code>path</code></td>
<td>
<p>array containing the solution path. Solutions are ordered dense to sparse.</p>
</td></tr>
<tr><td><code>min.error</code></td>
<td>
<p>minimum average cross validation error for optimal parameters.</p>
</td></tr>
<tr><td><code>avg.error</code></td>
<td>
<p>average cross validation error across all folds.</p>
</td></tr>
<tr><td><code>cv.error</code></td>
<td>
<p>cross validation errors (negative validation likelihood).</p>
</td></tr>
</table>

<hr>
<h2 id='CVP_ADMM'>Parallel CV (uses CV_ADMMc)</h2><span id='topic+CVP_ADMM'></span>

<h3>Description</h3>

<p>Parallel implementation of cross validation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CVP_ADMM(X = NULL, lam = 10^seq(-2, 2, 0.2), alpha = seq(0, 1, 0.2),
  diagonal = FALSE, rho = 2, mu = 10, tau.inc = 2, tau.dec = 2,
  crit = c("ADMM", "loglik"), tol.abs = 1e-04, tol.rel = 1e-04,
  maxit = 1000, adjmaxit = NULL, K = 5, crit.cv = c("loglik",
  "penloglik", "AIC", "BIC"), start = c("warm", "cold"), cores = 1,
  trace = c("progress", "print", "none"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CVP_ADMM_+3A_x">X</code></td>
<td>
<p>nxp data matrix. Each row corresponds to a single observation and each column contains n observations of a single feature/variable.</p>
</td></tr>
<tr><td><code id="CVP_ADMM_+3A_lam">lam</code></td>
<td>
<p>positive tuning parameters for elastic net penalty. If a vector of parameters is provided, they should be in increasing order. Defaults to grid of values <code>10^seq(-2, 2, 0.2)</code>.</p>
</td></tr>
<tr><td><code id="CVP_ADMM_+3A_alpha">alpha</code></td>
<td>
<p>elastic net mixing parameter contained in [0, 1]. <code>0 = ridge, 1 = lasso</code>. If a vector of parameters is provided, they should be in increasing order. Defaults to grid of values <code>seq(-1, 1, 0.2)</code>.</p>
</td></tr>
<tr><td><code id="CVP_ADMM_+3A_diagonal">diagonal</code></td>
<td>
<p>option to penalize the diagonal elements of the estimated precision matrix (<code class="reqn">\Omega</code>). Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="CVP_ADMM_+3A_rho">rho</code></td>
<td>
<p>initial step size for ADMM algorithm.</p>
</td></tr>
<tr><td><code id="CVP_ADMM_+3A_mu">mu</code></td>
<td>
<p>factor for primal and residual norms in the ADMM algorithm. This will be used to adjust the step size <code>rho</code> after each iteration.</p>
</td></tr>
<tr><td><code id="CVP_ADMM_+3A_tau.inc">tau.inc</code></td>
<td>
<p>factor in which to increase step size <code>rho</code></p>
</td></tr>
<tr><td><code id="CVP_ADMM_+3A_tau.dec">tau.dec</code></td>
<td>
<p>factor in which to decrease step size <code>rho</code></p>
</td></tr>
<tr><td><code id="CVP_ADMM_+3A_crit">crit</code></td>
<td>
<p>criterion for convergence (<code>ADMM</code> or <code>loglik</code>). If <code>crit = loglik</code> then iterations will stop when the relative change in log-likelihood is less than <code>tol.abs</code>. Default is <code>ADMM</code> and follows the procedure outlined in Boyd, et al.</p>
</td></tr>
<tr><td><code id="CVP_ADMM_+3A_tol.abs">tol.abs</code></td>
<td>
<p>absolute convergence tolerance. Defaults to 1e-4.</p>
</td></tr>
<tr><td><code id="CVP_ADMM_+3A_tol.rel">tol.rel</code></td>
<td>
<p>relative convergence tolerance. Defaults to 1e-4.</p>
</td></tr>
<tr><td><code id="CVP_ADMM_+3A_maxit">maxit</code></td>
<td>
<p>maximum number of iterations. Defaults to 1e3.</p>
</td></tr>
<tr><td><code id="CVP_ADMM_+3A_adjmaxit">adjmaxit</code></td>
<td>
<p>adjusted maximum number of iterations. During cross validation this option allows the user to adjust the maximum number of iterations after the first <code>lam</code> tuning parameter has converged (for each <code>alpha</code>). This option is intended to be paired with <code>warm</code> starts and allows for 'one-step' estimators. Defaults to NULL.</p>
</td></tr>
<tr><td><code id="CVP_ADMM_+3A_k">K</code></td>
<td>
<p>specify the number of folds for cross validation.</p>
</td></tr>
<tr><td><code id="CVP_ADMM_+3A_crit.cv">crit.cv</code></td>
<td>
<p>cross validation criterion (<code>loglik</code>, <code>penloglik</code>, <code>AIC</code>, or <code>BIC</code>). Defaults to <code>loglik</code>.</p>
</td></tr>
<tr><td><code id="CVP_ADMM_+3A_start">start</code></td>
<td>
<p>specify <code>warm</code> or <code>cold</code> start for cross validation. Default is <code>warm</code>.</p>
</td></tr>
<tr><td><code id="CVP_ADMM_+3A_cores">cores</code></td>
<td>
<p>option to run CV in parallel. Defaults to <code>cores = 1</code>.</p>
</td></tr>
<tr><td><code id="CVP_ADMM_+3A_trace">trace</code></td>
<td>
<p>option to display progress of CV. Choose one of <code>progress</code> to print a progress bar, <code>print</code> to print completed tuning parameters, or <code>none</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns list of returns which includes:
</p>
<table>
<tr><td><code>lam</code></td>
<td>
<p>optimal tuning parameter.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>optimal tuning parameter.</p>
</td></tr>
<tr><td><code>min.error</code></td>
<td>
<p>minimum average cross validation error (cv.crit) for optimal parameters.</p>
</td></tr>
<tr><td><code>avg.error</code></td>
<td>
<p>average cross validation error (cv.crit) across all folds.</p>
</td></tr>
<tr><td><code>cv.error</code></td>
<td>
<p>cross validation errors (cv.crit).</p>
</td></tr>
</table>

<hr>
<h2 id='CVP_ADMMc'>CV (no folds) ADMM penalized precision matrix estimation (c++)</h2><span id='topic+CVP_ADMMc'></span>

<h3>Description</h3>

<p>Cross validation (no folds) function for ADMMsigma. This function is to be used with CVP_ADMM.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CVP_ADMMc(n, S_train, S_valid, lam, alpha, diagonal = FALSE, rho = 2,
  mu = 10, tau_inc = 2, tau_dec = 2, crit = "ADMM", tol_abs = 1e-04,
  tol_rel = 1e-04, maxit = 10000L, adjmaxit = 10000L,
  crit_cv = "loglik", start = "warm", trace = "progress")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CVP_ADMMc_+3A_n">n</code></td>
<td>
<p>sample size for X_valid (used to calculate crit_cv)</p>
</td></tr>
<tr><td><code id="CVP_ADMMc_+3A_s_train">S_train</code></td>
<td>
<p>pxp sample covariance matrix for training data (denominator n).</p>
</td></tr>
<tr><td><code id="CVP_ADMMc_+3A_s_valid">S_valid</code></td>
<td>
<p>pxp sample covariance matrix for validation data (denominator n).</p>
</td></tr>
<tr><td><code id="CVP_ADMMc_+3A_lam">lam</code></td>
<td>
<p>positive tuning parameters for elastic net penalty. If a vector of parameters is provided, they should be in increasing order.</p>
</td></tr>
<tr><td><code id="CVP_ADMMc_+3A_alpha">alpha</code></td>
<td>
<p>elastic net mixing parameter contained in [0, 1]. <code>0 = ridge, 1 = lasso</code>. If a vector of parameters is provided, they should be in increasing order.</p>
</td></tr>
<tr><td><code id="CVP_ADMMc_+3A_diagonal">diagonal</code></td>
<td>
<p>option to penalize the diagonal elements of the estimated precision matrix (<code class="reqn">\Omega</code>). Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="CVP_ADMMc_+3A_rho">rho</code></td>
<td>
<p>initial step size for ADMM algorithm.</p>
</td></tr>
<tr><td><code id="CVP_ADMMc_+3A_mu">mu</code></td>
<td>
<p>factor for primal and residual norms in the ADMM algorithm. This will be used to adjust the step size <code>rho</code> after each iteration.</p>
</td></tr>
<tr><td><code id="CVP_ADMMc_+3A_tau_inc">tau_inc</code></td>
<td>
<p>factor in which to increase step size <code>rho</code></p>
</td></tr>
<tr><td><code id="CVP_ADMMc_+3A_tau_dec">tau_dec</code></td>
<td>
<p>factor in which to decrease step size <code>rho</code></p>
</td></tr>
<tr><td><code id="CVP_ADMMc_+3A_crit">crit</code></td>
<td>
<p>criterion for convergence (<code>ADMM</code> or <code>loglik</code>). If <code>crit = loglik</code> then iterations will stop when the relative change in log-likelihood is less than <code>tol.abs</code>. Default is <code>ADMM</code> and follows the procedure outlined in Boyd, et al.</p>
</td></tr>
<tr><td><code id="CVP_ADMMc_+3A_tol_abs">tol_abs</code></td>
<td>
<p>absolute convergence tolerance. Defaults to 1e-4.</p>
</td></tr>
<tr><td><code id="CVP_ADMMc_+3A_tol_rel">tol_rel</code></td>
<td>
<p>relative convergence tolerance. Defaults to 1e-4.</p>
</td></tr>
<tr><td><code id="CVP_ADMMc_+3A_maxit">maxit</code></td>
<td>
<p>maximum number of iterations. Defaults to 1e4.</p>
</td></tr>
<tr><td><code id="CVP_ADMMc_+3A_adjmaxit">adjmaxit</code></td>
<td>
<p>adjusted maximum number of iterations. During cross validation this option allows the user to adjust the maximum number of iterations after the first <code>lam</code> tuning parameter has converged (for each <code>alpha</code>). This option is intended to be paired with <code>warm</code> starts and allows for &quot;one-step&quot; estimators. Defaults to 1e4.</p>
</td></tr>
<tr><td><code id="CVP_ADMMc_+3A_crit_cv">crit_cv</code></td>
<td>
<p>cross validation criterion (<code>loglik</code>, <code>penloglik</code>, <code>AIC</code>, or <code>BIC</code>). Defaults to <code>loglik</code>.</p>
</td></tr>
<tr><td><code id="CVP_ADMMc_+3A_start">start</code></td>
<td>
<p>specify <code>warm</code> or <code>cold</code> start for cross validation. Default is <code>warm</code>.</p>
</td></tr>
<tr><td><code id="CVP_ADMMc_+3A_trace">trace</code></td>
<td>
<p>option to display progress of CV. Choose one of <code>progress</code> to print a progress bar, <code>print</code> to print completed tuning parameters, or <code>none</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>cross validation errors (cv_crit)
</p>

<hr>
<h2 id='CVP_RIDGE'>Parallel Ridge CV (uses CVP_RIDGEc)</h2><span id='topic+CVP_RIDGE'></span>

<h3>Description</h3>

<p>Parallel implementation of cross validation for RIDGEsigma.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CVP_RIDGE(X = NULL, lam = 10^seq(-2, 2, 0.1), K = 5, cores = 1,
  trace = c("none", "progress", "print"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CVP_RIDGE_+3A_x">X</code></td>
<td>
<p>nxp data matrix. Each row corresponds to a single observation and each column contains n observations of a single feature/variable.</p>
</td></tr>
<tr><td><code id="CVP_RIDGE_+3A_lam">lam</code></td>
<td>
<p>positive tuning parameters for ridge penalty. If a vector of parameters is provided, they should be in increasing order. Defaults to grid of values <code>10^seq(-2, 2, 0.1)</code>.</p>
</td></tr>
<tr><td><code id="CVP_RIDGE_+3A_k">K</code></td>
<td>
<p>specify the number of folds for cross validation.</p>
</td></tr>
<tr><td><code id="CVP_RIDGE_+3A_cores">cores</code></td>
<td>
<p>option to run CV in parallel. Defaults to <code>cores = 1</code>.</p>
</td></tr>
<tr><td><code id="CVP_RIDGE_+3A_trace">trace</code></td>
<td>
<p>option to display progress of CV. Choose one of <code>progress</code> to print a progress bar, <code>print</code> to print completed tuning parameters, or <code>none</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns list of returns which includes:
</p>
<table>
<tr><td><code>lam</code></td>
<td>
<p>optimal tuning parameter.</p>
</td></tr>
<tr><td><code>min.error</code></td>
<td>
<p>minimum average cross validation error for optimal parameters.</p>
</td></tr>
<tr><td><code>avg.error</code></td>
<td>
<p>average cross validation error across all folds.</p>
</td></tr>
<tr><td><code>cv.error</code></td>
<td>
<p>cross validation errors (negative validation likelihood).</p>
</td></tr>
</table>

<hr>
<h2 id='CVP_RIDGEc'>CV (no folds) RIDGE penalized precision matrix estimation (c++)</h2><span id='topic+CVP_RIDGEc'></span>

<h3>Description</h3>

<p>Cross validation (no folds) function for RIDGEsigma. This function is to be used with CVP_RIDGE.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CVP_RIDGEc(n, S_train, S_valid, lam, trace = "none")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CVP_RIDGEc_+3A_n">n</code></td>
<td>
<p>sample size for X_valid (used to calculate CV_error)</p>
</td></tr>
<tr><td><code id="CVP_RIDGEc_+3A_s_train">S_train</code></td>
<td>
<p>pxp sample covariance matrix for training data (denominator n).</p>
</td></tr>
<tr><td><code id="CVP_RIDGEc_+3A_s_valid">S_valid</code></td>
<td>
<p>pxp sample covariance matrix for validation data (denominator n).</p>
</td></tr>
<tr><td><code id="CVP_RIDGEc_+3A_lam">lam</code></td>
<td>
<p>positive tuning parameters for ridge penalty. If a vector of parameters is provided, they should be in increasing order.</p>
</td></tr>
<tr><td><code id="CVP_RIDGEc_+3A_trace">trace</code></td>
<td>
<p>option to display progress of CV. Choose one of <code>progress</code> to print a progress bar, <code>print</code> to print completed tuning parameters, or <code>none</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>cross validation errors (negative validation likelihood)
</p>

<hr>
<h2 id='plot.ADMM'>Plot ADMM object</h2><span id='topic+plot.ADMM'></span>

<h3>Description</h3>

<p>Produces a plot for the cross validation errors, if available.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ADMM'
plot(x, type = c("line", "heatmap"), footnote = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.ADMM_+3A_x">x</code></td>
<td>
<p>class object ADMM.</p>
</td></tr>
<tr><td><code id="plot.ADMM_+3A_type">type</code></td>
<td>
<p>produce either 'heatmap' or 'line' graph</p>
</td></tr>
<tr><td><code id="plot.ADMM_+3A_footnote">footnote</code></td>
<td>
<p>option to print footnote of optimal values. Defaults to TRUE.</p>
</td></tr>
<tr><td><code id="plot.ADMM_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># generate data from a sparse matrix
# first compute covariance matrix
S = matrix(0.7, nrow = 5, ncol = 5)
for (i in 1:5){
 for (j in 1:5){
   S[i, j] = S[i, j]^abs(i - j)
 }
 }

# generate 100 x 5 matrix with rows drawn from iid N_p(0, S)
set.seed(123)
Z = matrix(rnorm(100*5), nrow = 100, ncol = 5)
out = eigen(S, symmetric = TRUE)
S.sqrt = out$vectors %*% diag(out$values^0.5)
S.sqrt = S.sqrt %*% t(out$vectors)
X = Z %*% S.sqrt

# produce line graph for ADMMsigma
plot(ADMMsigma(X), type = 'line')

# produce CV heat map for ADMMsigma
plot(ADMMsigma(X), type = 'heatmap')
</code></pre>

<hr>
<h2 id='plot.RIDGE'>Plot RIDGE object</h2><span id='topic+plot.RIDGE'></span>

<h3>Description</h3>

<p>Produces a heat plot for the cross validation errors, if available.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'RIDGE'
plot(x, type = c("heatmap", "line"), footnote = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.RIDGE_+3A_x">x</code></td>
<td>
<p>class object RIDGE</p>
</td></tr>
<tr><td><code id="plot.RIDGE_+3A_type">type</code></td>
<td>
<p>produce either 'heatmap' or 'line' graph</p>
</td></tr>
<tr><td><code id="plot.RIDGE_+3A_footnote">footnote</code></td>
<td>
<p>option to print footnote of optimal values. Defaults to TRUE.</p>
</td></tr>
<tr><td><code id="plot.RIDGE_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># generate data from a sparse matrix
# first compute covariance matrix
S = matrix(0.7, nrow = 5, ncol = 5)
for (i in 1:5){
 for (j in 1:5){
   S[i, j] = S[i, j]^abs(i - j)
 }
 }

# generate 100 x 5 matrix with rows drawn from iid N_p(0, S)
set.seed(123)
Z = matrix(rnorm(100*5), nrow = 100, ncol = 5)
out = eigen(S, symmetric = TRUE)
S.sqrt = out$vectors %*% diag(out$values^0.5)
S.sqrt = S.sqrt %*% t(out$vectors)
X = Z %*% S.sqrt

# produce CV heat map for RIDGEsigma
plot(RIDGEsigma(X, lam = 10^seq(-5, 5, 0.5)))

# produce line graph for RIDGEsigma
plot(RIDGEsigma(X), type = 'line')
</code></pre>

<hr>
<h2 id='print.ADMM'>Print ADMM object</h2><span id='topic+print.ADMM'></span>

<h3>Description</h3>

<p>Prints ADMM object and suppresses output if needed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ADMM'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.ADMM_+3A_x">x</code></td>
<td>
<p>class object ADMM</p>
</td></tr>
<tr><td><code id="print.ADMM_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>

<hr>
<h2 id='print.RIDGE'>Print RIDGE object</h2><span id='topic+print.RIDGE'></span>

<h3>Description</h3>

<p>Prints RIDGE object and suppresses output if needed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'RIDGE'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.RIDGE_+3A_x">x</code></td>
<td>
<p>class object RIDGE.</p>
</td></tr>
<tr><td><code id="print.RIDGE_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>

<hr>
<h2 id='RIDGEc'>Ridge-penalized precision matrix estimation (c++)</h2><span id='topic+RIDGEc'></span>

<h3>Description</h3>

<p>Ridge penalized matrix estimation via closed-form solution. Augmented from Adam Rothman's STAT 8931 code.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RIDGEc(S, lam)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RIDGEc_+3A_s">S</code></td>
<td>
<p>sample covariance matrix (denominator n).</p>
</td></tr>
<tr><td><code id="RIDGEc_+3A_lam">lam</code></td>
<td>
<p>tuning parameter for ridge penalty.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>estimated Omega
</p>

<hr>
<h2 id='RIDGEsigma'>Ridge penalized precision matrix estimation</h2><span id='topic+RIDGEsigma'></span>

<h3>Description</h3>

<p>Ridge penalized matrix estimation via closed-form solution. If one is only interested in the ridge penalty, this function will be faster and provide a more precise estimate than using <code>ADMMsigma</code>. <br />
Consider the case where
<code class="reqn">X_{1}, ..., X_{n}</code> are iid <code class="reqn">N_{p}(\mu, \Sigma)</code>
and we are tasked with estimating the precision matrix,
denoted <code class="reqn">\Omega \equiv \Sigma^{-1}</code>. This function solves the
following optimization problem:
</p>

<dl>
<dt>Objective:</dt><dd>
<p><code class="reqn">\hat{\Omega}_{\lambda} = \arg\min_{\Omega \in S_{+}^{p}}
\left\{ Tr\left(S\Omega\right) - \log \det\left(\Omega \right) +
\frac{\lambda}{2}\left\| \Omega \right\|_{F}^{2} \right\}</code></p>
</dd>
</dl>

<p>where <code class="reqn">\lambda &gt; 0</code> and <code class="reqn">\left\|\cdot \right\|_{F}^{2}</code> is the Frobenius
norm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RIDGEsigma(X = NULL, S = NULL, lam = 10^seq(-2, 2, 0.1), path = FALSE,
  K = 5, cores = 1, trace = c("none", "progress", "print"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RIDGEsigma_+3A_x">X</code></td>
<td>
<p>option to provide a nxp data matrix. Each row corresponds to a single observation and each column contains n observations of a single feature/variable.</p>
</td></tr>
<tr><td><code id="RIDGEsigma_+3A_s">S</code></td>
<td>
<p>option to provide a pxp sample covariance matrix (denominator n). If argument is <code>NULL</code> and <code>X</code> is provided instead then <code>S</code> will be computed automatically.</p>
</td></tr>
<tr><td><code id="RIDGEsigma_+3A_lam">lam</code></td>
<td>
<p>positive tuning parameters for ridge penalty. If a vector of parameters is provided, they should be in increasing order. Defaults to grid of values <code>10^seq(-2, 2, 0.1)</code>.</p>
</td></tr>
<tr><td><code id="RIDGEsigma_+3A_path">path</code></td>
<td>
<p>option to return the regularization path. This option should be used with extreme care if the dimension is large. If set to TRUE, cores will be set to 1 and errors and optimal tuning parameters will based on the full sample. Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="RIDGEsigma_+3A_k">K</code></td>
<td>
<p>specify the number of folds for cross validation.</p>
</td></tr>
<tr><td><code id="RIDGEsigma_+3A_cores">cores</code></td>
<td>
<p>option to run CV in parallel. Defaults to <code>cores = 1</code>.</p>
</td></tr>
<tr><td><code id="RIDGEsigma_+3A_trace">trace</code></td>
<td>
<p>option to display progress of CV. Choose one of <code>progress</code> to print a progress bar, <code>print</code> to print completed tuning parameters, or <code>none</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns class object <code>RIDGEsigma</code> which includes:
</p>
<table>
<tr><td><code>Lambda</code></td>
<td>
<p>optimal tuning parameter.</p>
</td></tr>
<tr><td><code>Lambdas</code></td>
<td>
<p>grid of lambda values for CV.</p>
</td></tr>
<tr><td><code>Omega</code></td>
<td>
<p>estimated penalized precision matrix.</p>
</td></tr>
<tr><td><code>Sigma</code></td>
<td>
<p>estimated covariance matrix from the penalized precision matrix (inverse of Omega).</p>
</td></tr>
<tr><td><code>Path</code></td>
<td>
<p>array containing the solution path. Solutions are ordered dense to sparse.</p>
</td></tr>
<tr><td><code>Gradient</code></td>
<td>
<p>gradient of optimization function (penalized gaussian likelihood).</p>
</td></tr>
<tr><td><code>MIN.error</code></td>
<td>
<p>minimum average cross validation error (cv.crit) for optimal parameters.</p>
</td></tr>
<tr><td><code>AVG.error</code></td>
<td>
<p>average cross validation error (cv.crit) across all folds.</p>
</td></tr>
<tr><td><code>CV.error</code></td>
<td>
<p>cross validation errors (cv.crit).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Matt Galloway <a href="mailto:gall0441@umn.edu">gall0441@umn.edu</a>
</p>


<h3>References</h3>


<ul>
<li><p> Rothman, Adam. 2017. 'STAT 8931 notes on an algorithm to compute the Lasso-penalized Gaussian likelihood precision matrix estimator.'
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+plot.RIDGE">plot.RIDGE</a></code>, <code><a href="#topic+ADMMsigma">ADMMsigma</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># generate data from a sparse matrix
# first compute covariance matrix
S = matrix(0.7, nrow = 5, ncol = 5)
for (i in 1:5){
 for (j in 1:5){
   S[i, j] = S[i, j]^abs(i - j)
 }
 }

# generate 100 x 5 matrix with rows drawn from iid N_p(0, S)
set.seed(123)
Z = matrix(rnorm(100*5), nrow = 100, ncol = 5)
out = eigen(S, symmetric = TRUE)
S.sqrt = out$vectors %*% diag(out$values^0.5)
S.sqrt = S.sqrt %*% t(out$vectors)
X = Z %*% S.sqrt

# ridge penalty no ADMM
RIDGEsigma(X, lam = 10^seq(-5, 5, 0.5))
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
