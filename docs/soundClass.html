<!DOCTYPE html><html lang="en"><head><title>Help for package soundClass</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {soundClass}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#+25+26gt+3B+25'><p>Pipe operator</p></a></li>
<li><a href='#app_label'><p>Shiny app to label recordings</p></a></li>
<li><a href='#app_model'><p>Shiny app to fit a model or run a fitted model</p></a></li>
<li><a href='#auto_id'><p>Automatic classification of sound events in recordings</p></a></li>
<li><a href='#butter_filter'><p>Apply a butterworth filter to sound samples</p></a></li>
<li><a href='#create_db'><p>Create a sqlite3 database</p></a></li>
<li><a href='#find_noise'><p>Detect energy peaks in recordings with non-relevant events</p></a></li>
<li><a href='#import_audio'><p>Import a recording</p></a></li>
<li><a href='#ms2samples'><p>Convert between time and number of samples in sound files</p></a></li>
<li><a href='#spectro_calls'><p>Generate spectrograms from labels</p></a></li>
<li><a href='#train_metadata'><p>Obtain train metadata to run a fitted model</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Sound Classification Using Convolutional Neural Networks</td>
</tr>
<tr>
<td>Version:</td>
<td>0.0.9.2</td>
</tr>
<tr>
<td>Author:</td>
<td>Bruno Silva [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Bruno Silva &lt;bmsasilva@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides an all-in-one solution for automatic classification of 
    sound events using convolutional neural networks (CNN). The main purpose 
    is to provide a sound classification workflow, from annotating sound events
    in recordings to training and automating model usage in real-life
    situations. Using the package requires a pre-compiled collection of 
    recordings with sound events of interest and it can be employed for: 
    1) Annotation: create a database of annotated recordings, 
    2) Training: prepare train data from annotated recordings and fit CNN models, 
    3) Classification: automate the use of the fitted model for classifying 
    new recordings. By using automatic feature selection and a user-friendly GUI
    for managing data and training/deploying models, this package is intended 
    to be used by a broad audience as it does not require specific expertise in 
    statistics, programming or sound analysis. Please refer to the vignette for
    further information.
    Gibb, R., et al. (2019) &lt;<a href="https://doi.org/10.1111%2F2041-210X.13101">doi:10.1111/2041-210X.13101</a>&gt;
    Mac Aodha, O., et al. (2018) &lt;<a href="https://doi.org/10.1371%2Fjournal.pcbi.1005995">doi:10.1371/journal.pcbi.1005995</a>&gt;
    Stowell, D., et al. (2019) &lt;<a href="https://doi.org/10.1111%2F2041-210X.13103">doi:10.1111/2041-210X.13103</a>&gt;
    LeCun, Y., et al. (2012) &lt;<a href="https://doi.org/10.1007%2F978-3-642-35289-8_3">doi:10.1007/978-3-642-35289-8_3</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/bmsasilva/soundClass/issues">https://github.com/bmsasilva/soundClass/issues</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>seewave, DBI, dplyr, dbplyr, RSQLite, signal, tuneR, zoo,
magrittr, shinyFiles, shiny, utils, graphics, generics, keras,
shinyjs</td>
</tr>
<tr>
<td>Depends:</td>
<td>shinyBS, htmltools</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-05-29 13:38:00 UTC; bruno</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-05-29 22:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='+25+26gt+3B+25'>Pipe operator</h2><span id='topic++25+3E+25'></span>

<h3>Description</h3>

<p>See documentation of package magrittr for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lhs %&gt;% rhs
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="+2B25+2B26gt+2B3B+2B25_+3A_lhs">lhs</code></td>
<td>
<p>A value or the magrittr placeholder.</p>
</td></tr>
<tr><td><code id="+2B25+2B26gt+2B3B+2B25_+3A_rhs">rhs</code></td>
<td>
<p>A function call using the magrittr semantics</p>
</td></tr>
</table>

<hr>
<h2 id='app_label'>Shiny app to label recordings</h2><span id='topic+app_label'></span>

<h3>Description</h3>

<p>Shiny app to label recordings. Use this app to visualize your
training recordings, create annotations and store them in a sqlite database.
The app has a sidebar panel with the following buttons/boxes to input
required user data:
</p>

<ol>
<li><p> Create database &ndash; if no database exists to store the annotations,
use this button to create one
</p>
</li>
<li><p> Choose database &ndash; choose the database to store the annotations
</p>
</li>
<li><p> Butterworth filter &ndash; check box to apply filter and
indicate low and high frequencies in kHz to filter the recordings
</p>
</li>
<li><p> Time expanded &ndash; only used in recorders specifically intended for
bat recordings. Can take any numeric value. If the recording is not time
expanded the value must be set to 1. If it is time expanded the numeric
value corresponding to the time expansion should be indicated
</p>
</li>
<li><p> Choose folder &ndash; choose the folder containing the training recordings
</p>
</li></ol>

<p>After the spectrogram is ploted:
</p>

<ol>
<li><p> Select events by clicking in the spectrogram on the middle of the
event of interest (bat call, bird song, etc)
</p>
</li>
<li><p> Insert the correct label in the &quot;Label&quot; box and add any additional
notes in the &quot;Observations&quot; box
</p>
</li>
<li><p> Press 'Set labels' button to add labels to database
</p>
</li>
<li><p> Repeat above steps if more than one set of events is present
in the recording
</p>
</li>
<li><p> Press 'Next' button to advance to next recording or pick another
recording from the dropdown list
</p>
</li></ol>

<p>The spectrogram can be zoomed by pressing mouse button and dragging to select
an area and then double click on it. To unzoom simply double clicking on the
spectrogram without an area selected. To adjust visualization settings,
in the top right, the tab &quot;Spectrogram options&quot; can be used to:
</p>

<ul>
<li><p> Threshold &ndash; minimum intensity values to show
in the spectrogram. A value of 100 will typically be adequate for the
majority of the recorders
</p>
</li>
<li><p> Window length &ndash; moving window length in ms. Smaller windows best
suited for short calls
</p>
</li>
<li><p> Overlap &ndash; overlap between consecutive windows, higher values give best
visualization but lower performance
</p>
</li>
<li><p> Resolution &ndash; frequency resolution of the spectrogram
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>app_label()
</code></pre>


<h3>Value</h3>

<p>Starts the shiny app, no return value.
</p>


<h3>Author(s)</h3>

<p>Bruno Silva
</p>

<hr>
<h2 id='app_model'>Shiny app to fit a model or run a fitted model</h2><span id='topic+app_model'></span>

<h3>Description</h3>

<p>Shiny app to fit a model from training recordings or to run a
fitted model to classify new recordings. This app consists of three GUIs,
i.e. three main panels, accessible by the tabs at the top:
</p>

<ol>
<li><p> Create train data &ndash; create train data from recordings and their
respective annotations database
</p>
</li>
<li><p> Fit model &ndash; fit a model from training data
</p>
</li>
<li><p> Run model &ndash; run a fitted model to classify new recordings
</p>
</li></ol>



<h4>1. Create train data</h4>

<p>This panel is used to create train data from recordings and their
respective annotations database. The sidebar panel has the following
buttons/boxes to input required user data:
</p>

<ul>
<li><p> Choose folder &ndash; choose the folder containing the training recordings
</p>
</li>
<li><p> Choose database &ndash; choose the database with the annotations for the
training recordings
</p>
</li>
<li><p> Time expanded &ndash; choose the correct time expansion factor, normally
only used in recorders specifically intended for bat recordings. Can take
the values &quot;auto&quot;, 1 or 10. If the recording is in real time the value
must be 1. If it's time expanded, the value 10 or &quot;auto&quot; can be selected.
If &quot;auto&quot; is selected it is assumed that sampling rates &lt; 50kHz
corresponds to a value of 10 and sampling rates &gt; 50kHz to corresponds to a
value of 1
</p>
</li>
<li><p> Spectrogram parameters &ndash; different typologies of sound events
require different parameters for computing the spectrograms.
The more relevant are: size (in ms), which should be large
enough to encompass the duration of the largest sound event in
analysis (not only in the training data but also in novel recordings
where the classifiers are to be applied) and moving window (in ms),
that should be smaller for shorter sound events (to capture the quick
changes in time) and larger for longer sound events (to avoid redundant
information). The other parameters are more generalist and the same
values can be used for different sound events, as they only change
the definition of the images created. Please refer to
<code><a href="#topic+spectro_calls">spectro_calls</a></code> documentation for further details
</p>
</li></ul>

<p>After entering the required information press the button &quot;Create training
data from labels&quot; to generate the training data that will be used for
fitting a model. This object is saved in the folder containing the
training recordings with the name &quot;train_data.RDATA&quot;.
</p>



<h4>2. Fit model</h4>

<p>This panel is used to fit a model from training data. The sidebar panel has
the following buttons/boxes to input required user data:
</p>

<ul>
<li><p> Choose train data &ndash; the file &quot;train_data.RDATA&quot; created in
the previous panel
</p>
</li>
<li><p> Choose model &ndash; a blank model to be fitted. A custom model is provided
but must be copied to an external folder if it is to be used. The model path
can be obtained by running the following line at the R console:
system.file(&quot;model_architectures&quot;, &quot;model_vgg_sequential.R&quot;, package=&quot;soundClass&quot;)
and should be manually copied to a an external folder
</p>
</li>
<li><p> Model parameters &ndash; the train percentage indicates the percentage of
data that is used to fit the model while the remaining are used for
validation, batch size indicates the number
of samples per gradient update, the learning rate indicates the degree of the
gradient update, early stop indicates the maximum number of epochs without
improvement allowed before training stops and epochs indicate the maximum
number of epochs to train. Further information can be found in keras
documentation <a href="https://keras.io/api/">https://keras.io/api/</a>
</p>
</li></ul>

<p>The model is evaluated during fitting using the validation data. After
completion, by reaching the maximum epochs or the early stopping parameters,
the fitted model, the fitting log and the model metadata are saved to the
folder containing the train data with file names: &quot;fitted_model.hdf5&quot;,
&quot;fitted_model_log.csv&quot; and &quot;fitted_model_metadata.RDATA&quot; respectively.
</p>



<h4>3. Run model</h4>

<p>This panel is used to run a fitted model to classify new recordings. The
sidebar panel has the following buttons/boxes to input required user data:
</p>

<ul>
<li><p> Choose folder &ndash; choose the folder containing the recordings to be
classified
</p>
</li>
<li><p> Choose model &ndash; a fitted model to be used for classification
</p>
</li>
<li><p> Choose metadata &ndash; the file containing the fitted model metadata
</p>
</li>
<li><p> Time expanded &ndash; choose the correct time expansion factor, normally
only used in recorders specifically intended for bat recordings. Can take
the values &quot;auto&quot;, 1 or 10. If the recording is not time expanded the value
must be 1. If it's time expanded, the value 10 or &quot;auto&quot; can be selected.
If &quot;auto&quot; is selected it is assumed that sampling rates &lt; 50kHz
corresponds to a value of 10 and sampling rates &gt; 50kHz to corresponds to a
value of 1
</p>
</li>
<li><p> Output file &ndash; the name of the files to store the results of the
classification
</p>
</li>
<li><p> Irrelevant &ndash; does the fitted model includes an irrelevant class?
</p>
</li>
<li><p> Export plots &ndash; should a spectrogram of the classified recordings be
saved to disk?
</p>
</li></ul>

<p>The classification results are stored in a folder called &quot;output&quot;,
created inside the folder containing the recordings. They are stored in a
database in sqlite3 format with
all the relevant events detected and the respective probability of belonging
to a given class. Additionally a file in the csv format is saved to disk,
containing summary statistics per recording, i.e. the class with most events
detected in each particular recording and the average frequency of maximum
energy of the events detected.
</p>



<h3>Usage</h3>

<pre><code class='language-R'>app_model()
</code></pre>


<h3>Value</h3>

<p>Starts the shiny app, no return value.
</p>


<h3>Author(s)</h3>

<p>Bruno Silva
</p>

<hr>
<h2 id='auto_id'>Automatic classification of sound events in recordings</h2><span id='topic+auto_id'></span>

<h3>Description</h3>

<p>Run automatic classification of sound events on a set of
recordings using a fitted model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>auto_id(model_path, update_progress = NA, metadata,
file_path, out_file, out_dir, save_png = TRUE, win_size = 50,
plot2console = FALSE, remove_noise = TRUE, recursive = FALSE, tx = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="auto_id_+3A_model_path">model_path</code></td>
<td>
<p>Character. Path to the fitted model.</p>
</td></tr>
<tr><td><code id="auto_id_+3A_update_progress">update_progress</code></td>
<td>
<p>Progress bar only to be used inside shiny.</p>
</td></tr>
<tr><td><code id="auto_id_+3A_metadata">metadata</code></td>
<td>
<p>The object created with the function train_metadata()
containing the parameters used to fit the model, or the path to the saved
RDATA file.</p>
</td></tr>
<tr><td><code id="auto_id_+3A_file_path">file_path</code></td>
<td>
<p>Character. Path to the folder containing recordings to be
classified by the fitted model.</p>
</td></tr>
<tr><td><code id="auto_id_+3A_out_file">out_file</code></td>
<td>
<p>Character. Name of the output file to save the results.
Will be used to name the csv file and the sqlite database.</p>
</td></tr>
<tr><td><code id="auto_id_+3A_out_dir">out_dir</code></td>
<td>
<p>Character. Path to the folder where the output results will
be stored. Will be created if it doesn't exist already.</p>
</td></tr>
<tr><td><code id="auto_id_+3A_save_png">save_png</code></td>
<td>
<p>Logical. Should a spectrogram of the classified recordings
with the identified event(s) and respective classification(s) be saved
as png file?</p>
</td></tr>
<tr><td><code id="auto_id_+3A_win_size">win_size</code></td>
<td>
<p>Integer. Window size in ms to split recordings in chunks
for classification. One peak per chunk is obtained and classified.</p>
</td></tr>
<tr><td><code id="auto_id_+3A_plot2console">plot2console</code></td>
<td>
<p>Logical. Should a spectrogram of the classified
recordings with the identified event(s) and respective classification(s)
be plotted in the console while the analysis is running?</p>
</td></tr>
<tr><td><code id="auto_id_+3A_remove_noise">remove_noise</code></td>
<td>
<p>Logical. TRUE indicates that the model was fitted
with a non-relevant class which will be deleted from the final output.</p>
</td></tr>
<tr><td><code id="auto_id_+3A_recursive">recursive</code></td>
<td>
<p>Logical. FALSE indicates that the recordings are in
a single folder and TRUE indicates that there are recordings
inside subfolders.</p>
</td></tr>
<tr><td><code id="auto_id_+3A_tx">tx</code></td>
<td>
<p>Only used in recorders specifically intended for
bat recordings. Can take the values &quot;auto&quot; or any numeric value. If the
recording is not time expanded tx must be set to 1 (the default). If it's
time expanded the numeric value corresponding to the time expansion should
be indicated or &quot;auto&quot; should be selected. If tx = &quot;auto&quot; the function
assumes that sampling rates &lt; 50kHz corresponds to
tx = 10 and &gt; 50kHz to tx = 1.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Runs a classification task on the recordings of a specified folder
and saves the results of the analysis.
</p>


<h3>Value</h3>

<p>Nothing.
</p>


<h3>Author(s)</h3>

<p>Bruno Silva
</p>

<hr>
<h2 id='butter_filter'>Apply a butterworth filter to sound samples</h2><span id='topic+butter_filter'></span>

<h3>Description</h3>

<p>Apply a butterworth filter, high pass or/and low pass,
to sound samples. Based on the function <a href="signal.html#topic+butter">butter</a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>butter_filter(sound_samples, low = NA, high = NA, fs, tx, order = 10)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="butter_filter_+3A_sound_samples">sound_samples</code></td>
<td>
<p>Numeric vector with the sound samples to filter</p>
</td></tr>
<tr><td><code id="butter_filter_+3A_low">low</code></td>
<td>
<p>Numeric. Minimum frequency in kHz for the butterworth filter</p>
</td></tr>
<tr><td><code id="butter_filter_+3A_high">high</code></td>
<td>
<p>Numeric. Maximum frequency in kHz for the butterworth filter</p>
</td></tr>
<tr><td><code id="butter_filter_+3A_fs">fs</code></td>
<td>
<p>Integer with the sampling frequency of the recording</p>
</td></tr>
<tr><td><code id="butter_filter_+3A_tx">tx</code></td>
<td>
<p>Integer indicating the expanded time factor of the recording</p>
</td></tr>
<tr><td><code id="butter_filter_+3A_order">order</code></td>
<td>
<p>Integer indicating the filter order to apply to the recording</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Butterworth filter
</p>


<h3>Value</h3>

<p>A vector with the filtered sound samples
</p>


<h3>Author(s)</h3>

<p>Bruno Silva
</p>

<hr>
<h2 id='create_db'>Create a sqlite3 database</h2><span id='topic+create_db'></span>

<h3>Description</h3>

<p>Create a sqlite3 database (if a database with the specified
name doesn't exist already) with predefined tables. Two types of
databases are possible, one to store recordings annotations and another
to store the output of the classification.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_db(path, db_name = NA, table_name = "labels",
type = "reference")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_db_+3A_path">path</code></td>
<td>
<p>Character. Path to the folder where the database will be created.</p>
</td></tr>
<tr><td><code id="create_db_+3A_db_name">db_name</code></td>
<td>
<p>Character. Name of the database to be created.</p>
</td></tr>
<tr><td><code id="create_db_+3A_table_name">table_name</code></td>
<td>
<p>Character. Name of the table to be created in the
database. It is mandatory to use the default table name &quot;labels&quot;
if the database is intended to be used in conjunction with other
functions of this package.</p>
</td></tr>
<tr><td><code id="create_db_+3A_type">type</code></td>
<td>
<p>Character indicating the type of database to create. Possible
options are: &quot;reference&quot; which creates a database to be used to store
recordings annotations for training purposes, and &quot;id&quot; which
creates a database to output the results of the automatic classification.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Nothing
</p>


<h3>Author(s)</h3>

<p>Bruno Silva
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
dir_path &lt;- tempdir()
create_db(dir_path,
db_name = "test",
table_name = "labels",
type = "reference")
file.remove(file.path(dir_path, "test.sqlite3"))

## End(Not run)
</code></pre>

<hr>
<h2 id='find_noise'>Detect energy peaks in recordings with non-relevant events</h2><span id='topic+find_noise'></span>

<h3>Description</h3>

<p>Detects the temporal position of the desired number of
energy peaks in a recording exclusively with non-relevant events.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>find_noise(recording, nmax = 1, plot = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="find_noise_+3A_recording">recording</code></td>
<td>
<p>Object of class &quot;rc&quot;.</p>
</td></tr>
<tr><td><code id="find_noise_+3A_nmax">nmax</code></td>
<td>
<p>Integer indicating the maximum number of peaks to detect in
the recording.</p>
</td></tr>
<tr><td><code id="find_noise_+3A_plot">plot</code></td>
<td>
<p>Logical. If TRUE a plot showing the peak(s) is returned.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector with the temporal position of the
identified peak(s), in samples.
</p>


<h3>Author(s)</h3>

<p>Bruno Silva
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create a sample wav file in a temporary directory
recording &lt;- tuneR::noise(duration = 44100)
temp_dir &lt;- tempdir()
rec_path &lt;- file.path(temp_dir, "recording.wav")
tuneR::writeWave(recording, filename = rec_path)
# Import the sample wav file
new_rec &lt;- import_audio(rec_path, butt = FALSE, tx = 1)
find_noise(new_rec, nmax = 1, plot = FALSE)
file.remove(rec_path)
</code></pre>

<hr>
<h2 id='import_audio'>Import a recording</h2><span id='topic+import_audio'></span>

<h3>Description</h3>

<p>Import a &quot;wav&quot; recording. If the recording is stereo it is
converted to mono by keeping the channel with overall higher amplitude
</p>


<h3>Usage</h3>

<pre><code class='language-R'>import_audio(path, butt = TRUE, low, high, tx = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="import_audio_+3A_path">path</code></td>
<td>
<p>Character. Full path to the recording</p>
</td></tr>
<tr><td><code id="import_audio_+3A_butt">butt</code></td>
<td>
<p>Logical. If TRUE filters the recording with a 12th order
filter. The filter is applied twice to better cleaning of the recording</p>
</td></tr>
<tr><td><code id="import_audio_+3A_low">low</code></td>
<td>
<p>Minimum frequency in kHz for the butterworth filter</p>
</td></tr>
<tr><td><code id="import_audio_+3A_high">high</code></td>
<td>
<p>Maximum frequency in kHz for the butterworth filter</p>
</td></tr>
<tr><td><code id="import_audio_+3A_tx">tx</code></td>
<td>
<p>Time expanded. Only used in recorders specifically intended for
bat recordings. Can take the values &quot;auto&quot; or any numeric value. If the
recording is not time expanded tx must be set to 1 (the default). If it's
time expanded the numeric value corresponding to the time expansion should
be indicated or &quot;auto&quot; should be selected. If tx = &quot;auto&quot; the function
assumes that sampling rates &lt; 50kHz corresponds to
tx = 10 and &gt; 50kHz to tx = 1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class &quot;rc&quot;. This object is a list
with the following components:
</p>

<ul>
<li><p> sound_samples &ndash; sound samples of the recording
</p>
</li>
<li><p> file_name &ndash; name of the recording
</p>
</li>
<li><p> file_time &ndash; time of modification of the file (indicated for
Pettersson Elektronic detectors, for other manufactures creation time should
be preferable but it's not implemented yet)
</p>
</li>
<li><p> fs &ndash; sample frequency
</p>
</li>
<li><p> tx &ndash; expanded time factor
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Bruno Silva
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create a sample wav file in a temporary directory
recording &lt;- tuneR::sine(440)
temp_dir &lt;- tempdir()
rec_path &lt;- file.path(temp_dir, "recording.wav")
tuneR::writeWave(recording, filename = rec_path)
# Import the sample wav file
new_rec &lt;- import_audio(rec_path, low = 1, high = 20, tx = 1)
new_rec
file.remove(rec_path)
</code></pre>

<hr>
<h2 id='ms2samples'>Convert between time and number of samples in sound files</h2><span id='topic+ms2samples'></span>

<h3>Description</h3>

<p>Convert time to number of samples or vice versa
in sound files.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ms2samples(value, fs = 300000, tx = 1, inv = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ms2samples_+3A_value">value</code></td>
<td>
<p>Integer. Number of samples or time in ms.</p>
</td></tr>
<tr><td><code id="ms2samples_+3A_fs">fs</code></td>
<td>
<p>Integer. The sampling frequency in samples per second.</p>
</td></tr>
<tr><td><code id="ms2samples_+3A_tx">tx</code></td>
<td>
<p>Integer. Indicating the time expansion factor. If the
recording is not time expanded tx must be set to 1 (the default).</p>
</td></tr>
<tr><td><code id="ms2samples_+3A_inv">inv</code></td>
<td>
<p>Logical. If TRUE converts time to number of samples, if FALSE
number of samples to time.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Integer. If inv = TRUE returns number of samples, if inv = FALSE
returns time in ms.
</p>


<h3>Author(s)</h3>

<p>Bruno Silva
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ms2samples(150000, fs = 300000, tx = 1, inv = FALSE)
ms2samples(100, fs = 300000, tx = 1, inv = TRUE)
</code></pre>

<hr>
<h2 id='spectro_calls'>Generate spectrograms from labels</h2><span id='topic+spectro_calls'></span>

<h3>Description</h3>

<p>Generate spectrograms from recording labels for
classification purposes. The spectrogram parameters are user defined
and should be selected depending on the type of sound event to classify.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spectro_calls(files_path, update_progress = NA,
db_path, spec_size = NA, window_length = NA,
frequency_resolution = 1, overlap = NA,
dynamic_range = NA, freq_range = NA, tx = 1, seed = 1002)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spectro_calls_+3A_files_path">files_path</code></td>
<td>
<p>Character. Path for the folder containing sound recordings.</p>
</td></tr>
<tr><td><code id="spectro_calls_+3A_update_progress">update_progress</code></td>
<td>
<p>Progress bar only to be used inside shiny.</p>
</td></tr>
<tr><td><code id="spectro_calls_+3A_db_path">db_path</code></td>
<td>
<p>Character. Path for the database of recording labels created
with the shinny app provided in the package.</p>
</td></tr>
<tr><td><code id="spectro_calls_+3A_spec_size">spec_size</code></td>
<td>
<p>Integer. Spectrogram size in ms.</p>
</td></tr>
<tr><td><code id="spectro_calls_+3A_window_length">window_length</code></td>
<td>
<p>Numeric. Moving window length in ms.</p>
</td></tr>
<tr><td><code id="spectro_calls_+3A_frequency_resolution">frequency_resolution</code></td>
<td>
<p>Integer. Spectrogram frequency resolution with
higher values meaning better resolution. Specifically, for any integer X
provided, 1/X the analysis bandwidth (as determined by the number of samples
in the analysis window) will be used. Not implemented yet, always uses 1 as
input value.</p>
</td></tr>
<tr><td><code id="spectro_calls_+3A_overlap">overlap</code></td>
<td>
<p>Percentage of overlap between moving windows. Accepts values
between 0.5 and 0.75.</p>
</td></tr>
<tr><td><code id="spectro_calls_+3A_dynamic_range">dynamic_range</code></td>
<td>
<p>Threshold of minimum intensity values to show
in the spectrogram. A value of 100 will typically be adequate for the
majority of the recorders. If this is set to NULL, no threshold is applied.</p>
</td></tr>
<tr><td><code id="spectro_calls_+3A_freq_range">freq_range</code></td>
<td>
<p>Frequency range of the spectrogram. Vector with two values,
referring to the minimum and maximum frequency to show in the spectrogram.</p>
</td></tr>
<tr><td><code id="spectro_calls_+3A_tx">tx</code></td>
<td>
<p>Time expanded. Only used in recorders specifically intended for
bat recordings. Can take the values &quot;auto&quot; or any numeric value. If the
recording is not time expanded tx must be set to 1 (the default). If it's
time expanded the numeric value corresponding to the time expansion should
be indicated or &quot;auto&quot; should be selected. If tx = &quot;auto&quot; the function
assumes that sampling rates &lt; 50kHz corresponds to
tx = 10 and &gt; 50kHz to tx = 1.</p>
</td></tr>
<tr><td><code id="spectro_calls_+3A_seed">seed</code></td>
<td>
<p>Integer. Define a custom seed for randomizing data.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following components:
</p>

<ul>
<li><p> data_x &ndash; an array with the spectrogram matrices
</p>
</li>
<li><p> data_y &ndash; the labels for each matrix in one-hot-encoded format
</p>
</li>
<li><p> parameters &ndash; the parameters used to create the matrices
</p>
</li>
<li><p> labels_df &ndash; the labels with their respective numeric index
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Bruno Silva
</p>

<hr>
<h2 id='train_metadata'>Obtain train metadata to run a fitted model</h2><span id='topic+train_metadata'></span>

<h3>Description</h3>

<p>Obtain train metadata from
the output of function <a href="#topic+spectro_calls">spectro_calls</a>. Needed to run a fitted model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>train_metadata(train_data)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="train_metadata_+3A_train_data">train_data</code></td>
<td>
<p>Output of function <a href="#topic+spectro_calls">spectro_calls</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following components:
</p>

<ul>
<li><p> parameters &ndash; parameters of the spectrograms
</p>
</li>
<li><p> classes &ndash; class names and respective codes
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Bruno Silva
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
