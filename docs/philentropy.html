<!DOCTYPE html><html><head><title>Help for package philentropy</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {philentropy}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#additive_symm_chi_sq'><p>Additive symmetric chi-squared distance (lowlevel function)</p></a></li>
<li><a href='#avg'><p>AVG distance (lowlevel function)</p></a></li>
<li><a href='#bhattacharyya'><p>Bhattacharyya distance (lowlevel function)</p></a></li>
<li><a href='#binned.kernel.est'><p>Kernel Density Estimation</p></a></li>
<li><a href='#canberra'><p>Canberra distance (lowlevel function)</p></a></li>
<li><a href='#CE'><p>Shannon's Conditional-Entropy <code class="reqn">H(X | Y)</code></p></a></li>
<li><a href='#chebyshev'><p>Chebyshev distance (lowlevel function)</p></a></li>
<li><a href='#clark_sq'><p>Clark squared distance (lowlevel function)</p></a></li>
<li><a href='#cosine_dist'><p>Cosine distance (lowlevel function)</p></a></li>
<li><a href='#czekanowski'><p>Czekanowski distance (lowlevel function)</p></a></li>
<li><a href='#dice_dist'><p>Dice distance (lowlevel function)</p></a></li>
<li><a href='#dist_many_many'><p>Distances and Similarities between Many Probability Density Functions</p></a></li>
<li><a href='#dist_one_many'><p>Distances and Similarities between One and Many Probability Density Functions</p></a></li>
<li><a href='#dist_one_one'><p>Distances and Similarities between Two Probability Density Functions</p></a></li>
<li><a href='#dist.diversity'><p>Distance Diversity between Probability Density Functions</p></a></li>
<li><a href='#distance'><p>Distances and Similarities between Probability Density Functions</p></a></li>
<li><a href='#divergence_sq'><p>Divergence squared distance (lowlevel function)</p></a></li>
<li><a href='#estimate.probability'><p>Estimate Probability Vectors From Count Vectors</p></a></li>
<li><a href='#euclidean'><p>Euclidean distance (lowlevel function)</p></a></li>
<li><a href='#fidelity'><p>Fidelity distance (lowlevel function)</p></a></li>
<li><a href='#getDistMethods'><p>Get method names for <code>distance</code></p></a></li>
<li><a href='#gJSD'><p>Generalized Jensen-Shannon Divergence</p></a></li>
<li><a href='#gower'><p>Gower distance (lowlevel function)</p></a></li>
<li><a href='#H'><p>Shannon's Entropy <code class="reqn">H(X)</code></p></a></li>
<li><a href='#harmonic_mean_dist'><p>Harmonic mean distance (lowlevel function)</p></a></li>
<li><a href='#hellinger'><p>Hellinger distance (lowlevel function)</p></a></li>
<li><a href='#inner_product'><p>Inner product distance (lowlevel function)</p></a></li>
<li><a href='#intersection_dist'><p>Intersection distance (lowlevel function)</p></a></li>
<li><a href='#jaccard'><p>Jaccard distance (lowlevel function)</p></a></li>
<li><a href='#JE'><p>Shannon's Joint-Entropy <code class="reqn">H(X,Y)</code></p></a></li>
<li><a href='#jeffreys'><p>Jeffreys distance (lowlevel function)</p></a></li>
<li><a href='#jensen_difference'><p>Jensen difference (lowlevel function)</p></a></li>
<li><a href='#jensen_shannon'><p>Jensen-Shannon distance (lowlevel function)</p></a></li>
<li><a href='#JSD'><p>Jensen-Shannon Divergence</p></a></li>
<li><a href='#k_divergence'><p>K-Divergence (lowlevel function)</p></a></li>
<li><a href='#KL'><p>Kullback-Leibler Divergence</p></a></li>
<li><a href='#kulczynski_d'><p>Kulczynski_d distance (lowlevel function)</p></a></li>
<li><a href='#kullback_leibler_distance'><p>kullback-Leibler distance (lowlevel function)</p></a></li>
<li><a href='#kumar_hassebrook'><p>Kumar hassebrook distance (lowlevel function)</p></a></li>
<li><a href='#kumar_johnson'><p>Kumar-Johnson distance (lowlevel function)</p></a></li>
<li><a href='#lin.cor'><p>Linear Correlation</p></a></li>
<li><a href='#lorentzian'><p>Lorentzian distance (lowlevel function)</p></a></li>
<li><a href='#manhattan'><p>Manhattan distance (lowlevel function)</p></a></li>
<li><a href='#matusita'><p>Matusita distance (lowlevel function)</p></a></li>
<li><a href='#MI'><p>Shannon's Mutual Information <code class="reqn">I(X,Y)</code></p></a></li>
<li><a href='#minkowski'><p>Minkowski distance (lowlevel function)</p></a></li>
<li><a href='#motyka'><p>Motyka distance (lowlevel function)</p></a></li>
<li><a href='#neyman_chi_sq'><p>Neyman chi-squared distance (lowlevel function)</p></a></li>
<li><a href='#pearson_chi_sq'><p>Pearson chi-squared distance (lowlevel function)</p></a></li>
<li><a href='#prob_symm_chi_sq'><p>Probability symmetric chi-squared distance (lowlevel function)</p></a></li>
<li><a href='#ruzicka'><p>Ruzicka distance (lowlevel function)</p></a></li>
<li><a href='#soergel'><p>Soergel distance (lowlevel function)</p></a></li>
<li><a href='#sorensen'><p>Sorensen distance (lowlevel function)</p></a></li>
<li><a href='#squared_chi_sq'><p>Squared chi-squared distance (lowlevel function)</p></a></li>
<li><a href='#squared_chord'><p>Squared chord distance (lowlevel function)</p></a></li>
<li><a href='#squared_euclidean'><p>Squared euclidean distance (lowlevel function)</p></a></li>
<li><a href='#taneja'><p>Taneja difference (lowlevel function)</p></a></li>
<li><a href='#tanimoto'><p>Tanimoto distance (lowlevel function)</p></a></li>
<li><a href='#topsoe'><p>Topsoe distance (lowlevel function)</p></a></li>
<li><a href='#wave_hedges'><p>Wave hedges distance (lowlevel function)</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Similarity and Distance Quantification Between Probability
Functions</td>
</tr>
<tr>
<td>Version:</td>
<td>0.8.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-12-02</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Hajk-Georg Drost &lt;hajk-georg.drost@tuebingen.mpg.de&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Computes 46 optimized distance and similarity measures for comparing probability functions (Drost (2018) &lt;<a href="https://doi.org/10.21105%2Fjoss.00765">doi:10.21105/joss.00765</a>&gt;). These comparisons between probability functions have their foundations in a broad range of scientific disciplines from mathematics to ecology. The aim of this package is to provide a core framework for clustering, classification, statistical inference, goodness-of-fit, non-parametric statistics, information theory, and machine learning tasks that are based on comparing univariate or multivariate probability functions.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.1.2)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp, KernSmooth, poorman</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/drostlab/philentropy">https://github.com/drostlab/philentropy</a></td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, knitr, rmarkdown, microbenchmark</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/drostlab/philentropy/issues">https://github.com/drostlab/philentropy/issues</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-12-02 16:04:45 UTC; hdrost</td>
</tr>
<tr>
<td>Author:</td>
<td>Hajk-Georg Drost <a href="https://orcid.org/0000-0002-1567-306X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Jakub Nowosad <a href="https://orcid.org/0000-0002-1057-3721"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-12-02 17:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='additive_symm_chi_sq'>Additive symmetric chi-squared distance (lowlevel function)</h2><span id='topic+additive_symm_chi_sq'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the additive_symm_chi_sq distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>additive_symm_chi_sq(P, Q, testNA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="additive_symm_chi_sq_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="additive_symm_chi_sq_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="additive_symm_chi_sq_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>additive_symm_chi_sq(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE)
</code></pre>

<hr>
<h2 id='avg'>AVG distance (lowlevel function)</h2><span id='topic+avg'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the avg distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>avg(P, Q, testNA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="avg_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="avg_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="avg_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>avg(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE)
</code></pre>

<hr>
<h2 id='bhattacharyya'>Bhattacharyya distance (lowlevel function)</h2><span id='topic+bhattacharyya'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the bhattacharyya distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bhattacharyya(P, Q, testNA, unit, epsilon)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bhattacharyya_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="bhattacharyya_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="bhattacharyya_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
<tr><td><code id="bhattacharyya_+3A_unit">unit</code></td>
<td>
<p>type of <code>log</code> function. Option are</p>
</td></tr>
<tr><td><code id="bhattacharyya_+3A_epsilon">epsilon</code></td>
<td>
<p>epsilon a small value to address cases in the distance computation where division by zero occurs. In
these cases, x / 0 or 0 / 0 will be replaced by <code>epsilon</code>. The default is <code>epsilon = 0.00001</code>.
However, we recommend to choose a custom <code>epsilon</code> value depending on the size of the input vectors,
the expected similarity between compared probability density functions and 
whether or not many 0 values are present within the compared vectors.
As a rough rule of thumb we suggest that when dealing with very large 
input vectors which are very similar and contain many <code>0</code> values,
the <code>epsilon</code> value should be set even smaller (e.g. <code>epsilon = 0.000000001</code>),
whereas when vector sizes are small or distributions very divergent then
higher <code>epsilon</code> values may also be appropriate (e.g. <code>epsilon = 0.01</code>).
Addressing this <code>epsilon</code> issue is important to avoid cases where distance metrics
return negative values which are not defined and only occur due to the
technical issues of computing x / 0 or 0 / 0 cases.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>bhattacharyya(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE,
 unit = "log2", epsilon = 0.00001)
</code></pre>

<hr>
<h2 id='binned.kernel.est'>Kernel Density Estimation</h2><span id='topic+binned.kernel.est'></span>

<h3>Description</h3>

<p>This function implements an interface to the kernel density estimation functions provided by the <span class="pkg">KernSmooth</span> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>binned.kernel.est(
  data,
  kernel = "normal",
  bandwidth = NULL,
  canonical = FALSE,
  scalest = "minim",
  level = 2L,
  gridsize = 401L,
  range.data = range(data),
  truncate = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="binned.kernel.est_+3A_data">data</code></td>
<td>
<p>a numeric vector containing the sample on which the kernel density estimate is to be constructed.</p>
</td></tr>
<tr><td><code id="binned.kernel.est_+3A_kernel">kernel</code></td>
<td>
<p>character string specifying the smoothing kernel</p>
</td></tr>
<tr><td><code id="binned.kernel.est_+3A_bandwidth">bandwidth</code></td>
<td>
<p>the kernel bandwidth smoothing parameter.</p>
</td></tr>
<tr><td><code id="binned.kernel.est_+3A_canonical">canonical</code></td>
<td>
<p>a logical value indicating whether canonically scaled kernels should be used</p>
</td></tr>
<tr><td><code id="binned.kernel.est_+3A_scalest">scalest</code></td>
<td>
<p>estimate of scale. 
</p>

<ul>
<li> <p><code>"stdev"</code> - standard deviation is used.
</p>
</li>
<li> <p><code>"iqr"</code> - inter-quartile range divided by 1.349 is used.
</p>
</li>
<li> <p><code>"minim"</code> - minimum of <code>"stdev"</code> and <code>"iqr"</code> is used.
</p>
</li></ul>
</td></tr>
<tr><td><code id="binned.kernel.est_+3A_level">level</code></td>
<td>
<p>number of levels of functional estimation used in the plug-in rule.</p>
</td></tr>
<tr><td><code id="binned.kernel.est_+3A_gridsize">gridsize</code></td>
<td>
<p>the number of equally-spaced points over which binning is performed to obtain kernel functional approximation.</p>
</td></tr>
<tr><td><code id="binned.kernel.est_+3A_range.data">range.data</code></td>
<td>
<p>vector containing the minimum and maximum values of <code>data</code> at which to compute the estimate. The default is the minimum and maximum data values.</p>
</td></tr>
<tr><td><code id="binned.kernel.est_+3A_truncate">truncate</code></td>
<td>
<p>logical value indicating whether data with x values outside the range specified by <code>range.data</code> should be ignored.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>References</h3>

<p>Matt Wand (2015). KernSmooth: Functions for Kernel Smoothing Supporting Wand &amp; Jones (1995). R package version 2.23-14.
</p>
<p>Henry Deng and Hadley Wickham (2011). Density estimation in R. <a href="http://vita.had.co.nz/papers/density-estimation.pdf">http://vita.had.co.nz/papers/density-estimation.pdf</a>.
</p>

<hr>
<h2 id='canberra'>Canberra distance (lowlevel function)</h2><span id='topic+canberra'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the canberra distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>canberra(P, Q, testNA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="canberra_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="canberra_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="canberra_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>canberra(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE)
</code></pre>

<hr>
<h2 id='CE'>Shannon's Conditional-Entropy <code class="reqn">H(X | Y)</code></h2><span id='topic+CE'></span>

<h3>Description</h3>

<p>Compute Shannon's Conditional-Entropy based on the chain rule <code class="reqn">H(X | Y)
= H(X,Y) - H(Y)</code> based on a given joint-probability vector <code class="reqn">P(X,Y)</code> and
probability vector <code class="reqn">P(Y)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CE(xy, y, unit = "log2")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CE_+3A_xy">xy</code></td>
<td>
<p>a numeric joint-probability vector <code class="reqn">P(X,Y)</code>
for which Shannon's Joint-Entropy <code class="reqn">H(X,Y)</code> shall be computed.</p>
</td></tr>
<tr><td><code id="CE_+3A_y">y</code></td>
<td>
<p>a numeric probability vector <code class="reqn">P(Y)</code> for which
Shannon's Entropy <code class="reqn">H(Y)</code> (as part of the chain rule) shall be computed.
It is important to note that this probability vector must be the probability
distribution of random variable Y ( P(Y) for which H(Y) is computed).</p>
</td></tr>
<tr><td><code id="CE_+3A_unit">unit</code></td>
<td>
<p>a character string specifying the logarithm unit that shall be used to compute distances that depend on log computations.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function might be useful to fastly compute Shannon's
Conditional-Entropy for any given joint-probability vector and probability
vector.
</p>


<h3>Value</h3>

<p>Shannon's Conditional-Entropy in bit.
</p>


<h3>Note</h3>

<p>Note that the probability vector P(Y) must be the probability
distribution of random variable Y ( P(Y) for which H(Y) is computed ) and
furthermore used for the chain rule computation of <code class="reqn">H(X | Y) = H(X,Y) -
H(Y)</code>.
</p>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>References</h3>

<p>Shannon, Claude E. 1948. &quot;A Mathematical Theory of
Communication&quot;. <em>Bell System Technical Journal</em> <b>27</b> (3): 379-423.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+H">H</a></code>, <code><a href="#topic+JE">JE</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
 CE(1:10/sum(1:10),1:10/sum(1:10))

</code></pre>

<hr>
<h2 id='chebyshev'>Chebyshev distance (lowlevel function)</h2><span id='topic+chebyshev'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the chebyshev distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chebyshev(P, Q, testNA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chebyshev_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="chebyshev_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="chebyshev_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>chebyshev(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE)
</code></pre>

<hr>
<h2 id='clark_sq'>Clark squared distance (lowlevel function)</h2><span id='topic+clark_sq'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the clark_sq distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clark_sq(P, Q, testNA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clark_sq_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="clark_sq_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="clark_sq_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>clark_sq(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE)
</code></pre>

<hr>
<h2 id='cosine_dist'>Cosine distance (lowlevel function)</h2><span id='topic+cosine_dist'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the cosine_dist distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cosine_dist(P, Q, testNA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cosine_dist_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="cosine_dist_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="cosine_dist_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cosine_dist(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE)
</code></pre>

<hr>
<h2 id='czekanowski'>Czekanowski distance (lowlevel function)</h2><span id='topic+czekanowski'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the czekanowski distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>czekanowski(P, Q, testNA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="czekanowski_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="czekanowski_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="czekanowski_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>czekanowski(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE)
</code></pre>

<hr>
<h2 id='dice_dist'>Dice distance (lowlevel function)</h2><span id='topic+dice_dist'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the dice_dist distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dice_dist(P, Q, testNA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dice_dist_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="dice_dist_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="dice_dist_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dice_dist(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE)
</code></pre>

<hr>
<h2 id='dist_many_many'>Distances and Similarities between Many Probability Density Functions</h2><span id='topic+dist_many_many'></span>

<h3>Description</h3>

<p>This functions computes the distance/dissimilarity between two sets of probability density functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dist_many_many(
  dists1,
  dists2,
  method,
  p = NA_real_,
  testNA = TRUE,
  unit = "log",
  epsilon = 1e-05
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist_many_many_+3A_dists1">dists1</code></td>
<td>
<p>a numeric matrix storing distributions in its rows.</p>
</td></tr>
<tr><td><code id="dist_many_many_+3A_dists2">dists2</code></td>
<td>
<p>a numeric matrix storing distributions in its rows.</p>
</td></tr>
<tr><td><code id="dist_many_many_+3A_method">method</code></td>
<td>
<p>a character string indicating whether the distance measure that should be computed.</p>
</td></tr>
<tr><td><code id="dist_many_many_+3A_p">p</code></td>
<td>
<p>power of the Minkowski distance.</p>
</td></tr>
<tr><td><code id="dist_many_many_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
<tr><td><code id="dist_many_many_+3A_unit">unit</code></td>
<td>
<p>type of <code>log</code> function. Option are 
</p>

<ul>
<li> <p><code>unit = "log"</code>
</p>
</li>
<li> <p><code>unit = "log2"</code>
</p>
</li>
<li> <p><code>unit = "log10"</code>   
</p>
</li></ul>
</td></tr>
<tr><td><code id="dist_many_many_+3A_epsilon">epsilon</code></td>
<td>
<p>epsilon a small value to address cases in the distance computation where division by zero occurs. In
these cases, x / 0 or 0 / 0 will be replaced by <code>epsilon</code>. The default is <code>epsilon = 0.00001</code>.
However, we recommend to choose a custom <code>epsilon</code> value depending on the size of the input vectors,
the expected similarity between compared probability density functions and 
whether or not many 0 values are present within the compared vectors.
As a rough rule of thumb we suggest that when dealing with very large 
input vectors which are very similar and contain many <code>0</code> values,
the <code>epsilon</code> value should be set even smaller (e.g. <code>epsilon = 0.000000001</code>),
whereas when vector sizes are small or distributions very divergent then
higher <code>epsilon</code> values may also be appropriate (e.g. <code>epsilon = 0.01</code>).
Addressing this <code>epsilon</code> issue is important to avoid cases where distance metrics
return negative values which are not defined and only occur due to the
technical issues of computing x / 0 or 0 / 0 cases.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix of distance values
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  set.seed(2020-08-20)
  M1 &lt;- t(replicate(10, sample(1:10, size = 10) / 55))
  M2 &lt;- t(replicate(10, sample(1:10, size = 10) / 55))
  result &lt;- dist_many_many(M1, M2, method = "euclidean", testNA = FALSE)
</code></pre>

<hr>
<h2 id='dist_one_many'>Distances and Similarities between One and Many Probability Density Functions</h2><span id='topic+dist_one_many'></span>

<h3>Description</h3>

<p>This functions computes the distance/dissimilarity between one probability density functions and a set of probability density functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dist_one_many(
  P,
  dists,
  method,
  p = NA_real_,
  testNA = TRUE,
  unit = "log",
  epsilon = 1e-05
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist_one_many_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="dist_one_many_+3A_dists">dists</code></td>
<td>
<p>a numeric matrix storing distributions in its rows.</p>
</td></tr>
<tr><td><code id="dist_one_many_+3A_method">method</code></td>
<td>
<p>a character string indicating whether the distance measure that should be computed.</p>
</td></tr>
<tr><td><code id="dist_one_many_+3A_p">p</code></td>
<td>
<p>power of the Minkowski distance.</p>
</td></tr>
<tr><td><code id="dist_one_many_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
<tr><td><code id="dist_one_many_+3A_unit">unit</code></td>
<td>
<p>type of <code>log</code> function. Option are 
</p>

<ul>
<li> <p><code>unit = "log"</code>
</p>
</li>
<li> <p><code>unit = "log2"</code>
</p>
</li>
<li> <p><code>unit = "log10"</code>   
</p>
</li></ul>
</td></tr>
<tr><td><code id="dist_one_many_+3A_epsilon">epsilon</code></td>
<td>
<p>epsilon a small value to address cases in the distance computation where division by zero occurs. In
these cases, x / 0 or 0 / 0 will be replaced by <code>epsilon</code>. The default is <code>epsilon = 0.00001</code>.
However, we recommend to choose a custom <code>epsilon</code> value depending on the size of the input vectors,
the expected similarity between compared probability density functions and 
whether or not many 0 values are present within the compared vectors.
As a rough rule of thumb we suggest that when dealing with very large 
input vectors which are very similar and contain many <code>0</code> values,
the <code>epsilon</code> value should be set even smaller (e.g. <code>epsilon = 0.000000001</code>),
whereas when vector sizes are small or distributions very divergent then
higher <code>epsilon</code> values may also be appropriate (e.g. <code>epsilon = 0.01</code>).
Addressing this <code>epsilon</code> issue is important to avoid cases where distance metrics
return negative values which are not defined and only occur due to the
technical issues of computing x / 0 or 0 / 0 cases.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of distance values
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(2020-08-20)
P &lt;- 1:10 / sum(1:10)
M &lt;- t(replicate(100, sample(1:10, size = 10) / 55))
dist_one_many(P, M, method = "euclidean", testNA = FALSE)
</code></pre>

<hr>
<h2 id='dist_one_one'>Distances and Similarities between Two Probability Density Functions</h2><span id='topic+dist_one_one'></span>

<h3>Description</h3>

<p>This functions computes the distance/dissimilarity between two probability density functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dist_one_one(
  P,
  Q,
  method,
  p = NA_real_,
  testNA = TRUE,
  unit = "log",
  epsilon = 1e-05
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist_one_one_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="dist_one_one_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="dist_one_one_+3A_method">method</code></td>
<td>
<p>a character string indicating whether the distance measure that should be computed.</p>
</td></tr>
<tr><td><code id="dist_one_one_+3A_p">p</code></td>
<td>
<p>power of the Minkowski distance.</p>
</td></tr>
<tr><td><code id="dist_one_one_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
<tr><td><code id="dist_one_one_+3A_unit">unit</code></td>
<td>
<p>type of <code>log</code> function. Option are 
</p>

<ul>
<li> <p><code>unit = "log"</code>
</p>
</li>
<li> <p><code>unit = "log2"</code>
</p>
</li>
<li> <p><code>unit = "log10"</code>   
</p>
</li></ul>
</td></tr>
<tr><td><code id="dist_one_one_+3A_epsilon">epsilon</code></td>
<td>
<p>epsilon a small value to address cases in the distance computation where division by zero occurs. In
these cases, x / 0 or 0 / 0 will be replaced by <code>epsilon</code>. The default is <code>epsilon = 0.00001</code>.
However, we recommend to choose a custom <code>epsilon</code> value depending on the size of the input vectors,
the expected similarity between compared probability density functions and 
whether or not many 0 values are present within the compared vectors.
As a rough rule of thumb we suggest that when dealing with very large 
input vectors which are very similar and contain many <code>0</code> values,
the <code>epsilon</code> value should be set even smaller (e.g. <code>epsilon = 0.000000001</code>),
whereas when vector sizes are small or distributions very divergent then
higher <code>epsilon</code> values may also be appropriate (e.g. <code>epsilon = 0.01</code>).
Addressing this <code>epsilon</code> issue is important to avoid cases where distance metrics
return negative values which are not defined and only occur due to the
technical issues of computing x / 0 or 0 / 0 cases.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A single distance value
</p>


<h3>Examples</h3>

<pre><code class='language-R'>P &lt;- 1:10 / sum(1:10)
Q &lt;- 20:29 / sum(20:29)
dist_one_one(P, Q, method = "euclidean", testNA = FALSE)
</code></pre>

<hr>
<h2 id='dist.diversity'>Distance Diversity between Probability Density Functions</h2><span id='topic+dist.diversity'></span>

<h3>Description</h3>

<p>This function computes all distance values between two probability density functions that are available in <code><a href="#topic+getDistMethods">getDistMethods</a></code>
and returns a vector storing the corresponding distance measures. This vector is <em>named distance diversity vector</em>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dist.diversity(x, p, test.na = FALSE, unit = "log2")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dist.diversity_+3A_x">x</code></td>
<td>
<p>a numeric <code>data.frame</code> or <code>matrix</code> (storing probability vectors) or a numeric <code>data.frame</code> or <code>matrix</code> storing counts (if <code>est.prob</code> is specified).</p>
</td></tr>
<tr><td><code id="dist.diversity_+3A_p">p</code></td>
<td>
<p>power of the Minkowski distance.</p>
</td></tr>
<tr><td><code id="dist.diversity_+3A_test.na">test.na</code></td>
<td>
<p>a boolean value indicating whether input vectors should be tested for NA values. Faster computations if <code>test.na = FALSE</code>.</p>
</td></tr>
<tr><td><code id="dist.diversity_+3A_unit">unit</code></td>
<td>
<p>a character string specifying the logarithm unit that should be used to compute distances that depend on log computations. Options are:
</p>

<ul>
<li> <p><code>unit = "log"</code>
</p>
</li>
<li> <p><code>unit = "log2"</code>
</p>
</li>
<li> <p><code>unit = "log10"</code>
</p>
</li></ul>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
dist.diversity(rbind(1:10/sum(1:10), 20:29/sum(20:29)), p = 2, unit = "log2")

</code></pre>

<hr>
<h2 id='distance'>Distances and Similarities between Probability Density Functions</h2><span id='topic+distance'></span>

<h3>Description</h3>

<p>This functions computes the distance/dissimilarity between two probability density functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>distance(
  x,
  method = "euclidean",
  p = NULL,
  test.na = TRUE,
  unit = "log",
  epsilon = 1e-05,
  est.prob = NULL,
  use.row.names = FALSE,
  as.dist.obj = FALSE,
  diag = FALSE,
  upper = FALSE,
  mute.message = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="distance_+3A_x">x</code></td>
<td>
<p>a numeric <code>data.frame</code> or <code>matrix</code> (storing probability vectors) or a numeric <code>data.frame</code> or <code>matrix</code> storing counts (if <code>est.prob</code> is specified).</p>
</td></tr>
<tr><td><code id="distance_+3A_method">method</code></td>
<td>
<p>a character string indicating whether the distance measure that should be computed.</p>
</td></tr>
<tr><td><code id="distance_+3A_p">p</code></td>
<td>
<p>power of the Minkowski distance.</p>
</td></tr>
<tr><td><code id="distance_+3A_test.na">test.na</code></td>
<td>
<p>a boolean value indicating whether input vectors should be tested for <code>NA</code> values. Faster computations if <code>test.na = FALSE</code>.</p>
</td></tr>
<tr><td><code id="distance_+3A_unit">unit</code></td>
<td>
<p>a character string specifying the logarithm unit that should be used to compute distances that depend on log computations.</p>
</td></tr>
<tr><td><code id="distance_+3A_epsilon">epsilon</code></td>
<td>
<p>a small value to address cases in the distance computation where division by zero occurs. In
these cases, x / 0 or 0 / 0 will be replaced by <code>epsilon</code>. The default is <code>epsilon = 0.00001</code>.
However, we recommend to choose a custom <code>epsilon</code> value depending on the size of the input vectors,
the expected similarity between compared probability density functions and 
whether or not many 0 values are present within the compared vectors.
As a rough rule of thumb we suggest that when dealing with very large 
input vectors which are very similar and contain many <code>0</code> values,
the <code>epsilon</code> value should be set even smaller (e.g. <code>epsilon = 0.000000001</code>),
whereas when vector sizes are small or distributions very divergent then
higher <code>epsilon</code> values may also be appropriate (e.g. <code>epsilon = 0.01</code>).
Addressing this <code>epsilon</code> issue is important to avoid cases where distance metrics
return negative values which are not defined and only occur due to the
technical issues of computing x / 0 or 0 / 0 cases.</p>
</td></tr>
<tr><td><code id="distance_+3A_est.prob">est.prob</code></td>
<td>
<p>method to estimate probabilities from input count vectors such as non-probability vectors. Default: <code>est.prob = NULL</code>. Options are:
</p>

<ul>
<li> <p><code>est.prob = "empirical"</code>: The relative frequencies of each vector are computed internally. For example an input matrix <code>rbind(1:10, 11:20)</code> will be transformed to a probability vector <code>rbind(1:10 / sum(1:10), 11:20 / sum(11:20))</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="distance_+3A_use.row.names">use.row.names</code></td>
<td>
<p>a logical value indicating whether or not row names from
the input matrix shall be used as rownames and colnames of the output distance matrix. Default value is <code>use.row.names = FALSE</code>.</p>
</td></tr>
<tr><td><code id="distance_+3A_as.dist.obj">as.dist.obj</code></td>
<td>
<p>shall the return value or matrix be an object of class <code>link[stats]{dist}</code>? Default is <code>as.dist.obj = FALSE</code>.</p>
</td></tr>
<tr><td><code id="distance_+3A_diag">diag</code></td>
<td>
<p>if <code>as.dist.obj = TRUE</code>, then this value indicates whether the diagonal of the distance matrix should be printed. Default</p>
</td></tr>
<tr><td><code id="distance_+3A_upper">upper</code></td>
<td>
<p>if <code>as.dist.obj = TRUE</code>, then this value indicates whether the upper triangle of the distance matrix should be printed.</p>
</td></tr>
<tr><td><code id="distance_+3A_mute.message">mute.message</code></td>
<td>
<p>a logical value indicating whether or not messages printed by <code>distance</code> shall be muted. Default is <code>mute.message = FALSE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Here a distance is defined as a quantitative degree of how far two mathematical objects are apart from eachother (Cha, 2007).
</p>
<p>This function implements the following distance/similarity measures to quantify the distance between probability density functions:
</p>

<ul>
<li><p> L_p Minkowski family
</p>

<ul>
<li><p> Euclidean : <code class="reqn">d = sqrt( \sum | P_i - Q_i |^2)</code>
</p>
</li>
<li><p> Manhattan : <code class="reqn">d = \sum | P_i - Q_i |</code>
</p>
</li>
<li><p> Minkowski : <code class="reqn">d = ( \sum | P_i - Q_i |^p)^1/p</code>
</p>
</li>
<li><p> Chebyshev : <code class="reqn">d = max | P_i - Q_i |</code>
</p>
</li></ul>

</li>
<li><p> L_1 family
</p>

<ul>
<li><p> Sorensen : <code class="reqn">d = \sum | P_i - Q_i | / \sum (P_i + Q_i)</code>
</p>
</li>
<li><p> Gower : <code class="reqn">d = 1/d * \sum | P_i - Q_i |</code>
</p>
</li>
<li><p> Soergel : <code class="reqn">d = \sum | P_i - Q_i | / \sum max(P_i , Q_i)</code>
</p>
</li>
<li><p> Kulczynski d : <code class="reqn">d = \sum | P_i - Q_i | / \sum min(P_i , Q_i)</code>
</p>
</li>
<li><p> Canberra : <code class="reqn">d = \sum | P_i - Q_i | / (P_i + Q_i)</code>
</p>
</li>
<li><p> Lorentzian : <code class="reqn">d = \sum ln(1 + | P_i - Q_i |)</code>
</p>
</li></ul>

</li>
<li><p> Intersection family
</p>

<ul>
<li><p> Intersection : <code class="reqn">s = \sum min(P_i , Q_i)</code>
</p>
</li>
<li><p> Non-Intersection : <code class="reqn">d = 1 - \sum min(P_i , Q_i)</code>
</p>
</li>
<li><p> Wave Hedges : <code class="reqn">d = \sum | P_i - Q_i | / max(P_i , Q_i)</code>
</p>
</li>
<li><p> Czekanowski : <code class="reqn">d = \sum | P_i - Q_i | / \sum | P_i + Q_i |</code>
</p>
</li>
<li><p> Motyka : <code class="reqn">d = \sum min(P_i , Q_i) / (P_i + Q_i)</code>
</p>
</li>
<li><p> Kulczynski s : <code class="reqn">d = 1 / \sum | P_i - Q_i | / \sum min(P_i , Q_i)</code>
</p>
</li>
<li><p> Tanimoto : <code class="reqn">d = \sum (max(P_i , Q_i) - min(P_i , Q_i)) / \sum max(P_i , Q_i)</code> ; equivalent to Soergel
</p>
</li>
<li><p> Ruzicka : <code class="reqn">s = \sum min(P_i , Q_i) / \sum max(P_i , Q_i)</code> ; equivalent to 1 - Tanimoto = 1 - Soergel
</p>
</li></ul>

</li>
<li><p> Inner Product family
</p>

<ul>
<li><p> Inner Product : <code class="reqn">s = \sum P_i * Q_i</code>
</p>
</li>
<li><p> Harmonic mean : <code class="reqn">s = 2 * \sum (P_i * Q_i) / (P_i + Q_i)</code>
</p>
</li>
<li><p> Cosine : <code class="reqn">s = \sum (P_i * Q_i) / sqrt(\sum P_i^2) * sqrt(\sum Q_i^2)</code>
</p>
</li>
<li><p> Kumar-Hassebrook (PCE) : <code class="reqn">s = \sum (P_i * Q_i) / (\sum P_i^2 + \sum Q_i^2 - \sum (P_i * Q_i))</code>
</p>
</li>
<li><p> Jaccard : <code class="reqn">d = 1 - \sum (P_i * Q_i) / (\sum P_i^2 + \sum Q_i^2 - \sum (P_i * Q_i))</code> ; equivalent to 1 - Kumar-Hassebrook
</p>
</li>
<li><p> Dice : <code class="reqn">d = \sum (P_i - Q_i)^2 / (\sum P_i^2 + \sum Q_i^2)</code>
</p>
</li></ul>

</li>
<li><p> Squared-chord family
</p>

<ul>
<li><p> Fidelity : <code class="reqn">s = \sum sqrt(P_i * Q_i)</code>
</p>
</li>
<li><p> Bhattacharyya : <code class="reqn">d = - ln \sum sqrt(P_i * Q_i)</code>
</p>
</li>
<li><p> Hellinger : <code class="reqn">d = 2 * sqrt( 1 - \sum sqrt(P_i * Q_i))</code>
</p>
</li>
<li><p> Matusita : <code class="reqn">d = sqrt( 2 - 2 * \sum sqrt(P_i * Q_i))</code>
</p>
</li>
<li><p> Squared-chord : <code class="reqn">d = \sum ( sqrt(P_i) - sqrt(Q_i) )^2</code>
</p>
</li></ul>

</li>
<li><p> Squared L_2 family (<code class="reqn">X</code>^2 squared family)
</p>

<ul>
<li><p> Squared Euclidean : <code class="reqn">d = \sum ( P_i - Q_i )^2</code>
</p>
</li>
<li><p> Pearson <code class="reqn">X</code>^2 : <code class="reqn">d = \sum ( (P_i - Q_i )^2 / Q_i )</code>
</p>
</li>
<li><p> Neyman <code class="reqn">X</code>^2 : <code class="reqn">d = \sum ( (P_i - Q_i )^2 / P_i )</code>
</p>
</li>
<li><p> Squared <code class="reqn">X</code>^2 : <code class="reqn">d = \sum ( (P_i - Q_i )^2 / (P_i + Q_i) )</code>
</p>
</li>
<li><p> Probabilistic Symmetric <code class="reqn">X</code>^2 : <code class="reqn">d = 2 *  \sum ( (P_i - Q_i )^2 / (P_i + Q_i) )</code>
</p>
</li>
<li><p> Divergence : <code class="reqn">X</code>^2 : <code class="reqn">d = 2 *  \sum ( (P_i - Q_i )^2 / (P_i + Q_i)^2 )</code>
</p>
</li>
<li><p> Clark : <code class="reqn">d = sqrt ( \sum (| P_i - Q_i | / (P_i + Q_i))^2 )</code>
</p>
</li>
<li><p> Additive Symmetric <code class="reqn">X</code>^2 : <code class="reqn">d = \sum ( ((P_i - Q_i)^2 * (P_i + Q_i)) / (P_i * Q_i) ) </code>
</p>
</li></ul>

</li>
<li><p> Shannon's entropy family
</p>

<ul>
<li><p> Kullback-Leibler : <code class="reqn">d = \sum P_i * log(P_i / Q_i)</code>
</p>
</li>
<li><p> Jeffreys : <code class="reqn">d = \sum (P_i - Q_i) * log(P_i / Q_i)</code>
</p>
</li>
<li><p> K divergence : <code class="reqn">d = \sum P_i * log(2 * P_i / P_i + Q_i)</code>
</p>
</li>
<li><p> Topsoe : <code class="reqn">d = \sum ( P_i * log(2 * P_i / P_i + Q_i) ) + ( Q_i * log(2 * Q_i / P_i + Q_i) )</code>
</p>
</li>
<li><p> Jensen-Shannon :  <code class="reqn">d = 0.5 * ( \sum P_i * log(2 * P_i / P_i + Q_i) + \sum Q_i * log(2 * Q_i / P_i + Q_i))</code>
</p>
</li>
<li><p> Jensen difference : <code class="reqn">d = \sum ( (P_i * log(P_i) + Q_i * log(Q_i) / 2) - (P_i + Q_i / 2) * log(P_i + Q_i / 2) )</code>
</p>
</li></ul>

</li>
<li><p> Combinations
</p>

<ul>
<li><p> Taneja : <code class="reqn">d = \sum ( P_i + Q_i / 2) * log( P_i + Q_i / ( 2 * sqrt( P_i * Q_i)) )</code>
</p>
</li>
<li><p> Kumar-Johnson : <code class="reqn">d = \sum (P_i^2 - Q_i^2)^2 / 2 * (P_i * Q_i)^1.5</code>
</p>
</li>
<li><p> Avg(L_1, L_n) : <code class="reqn">d = \sum | P_i - Q_i| + max{ | P_i - Q_i |} / 2</code>
</p>
</li></ul>

<p>In cases where <code>x</code> specifies a count matrix, the argument <code>est.prob</code> can be selected to first estimate probability vectors
from input count vectors and second compute the corresponding distance measure based on the estimated probability vectors.
</p>
<p>The following probability estimation methods are implemented in this function:
</p>

<ul>
<li> <p><code>est.prob = "empirical"</code> : relative frequencies of counts.
</p>
</li></ul>

</li></ul>



<h3>Value</h3>

<p>The following results are returned depending on the dimension of <code>x</code>:
</p>

<ul>
<li><p> in case <code>nrow(x)</code> = 2 : a single distance value.
</p>
</li>
<li><p> in case <code>nrow(x)</code> &gt; 2 : a distance <code>matrix</code> storing distance values for all pairwise probability vector comparisons.
</p>
</li></ul>



<h3>Note</h3>

<p>According to the reference in some distance measure computations invalid computations can
occur when dealing with 0 probabilities.
</p>
<p>In these cases the convention is treated as follows:
</p>

<ul>
<li><p> division by zero - case <code>0/0</code>: when the divisor and dividend become zero, <code>0/0</code> is treated as <code>0</code>.
</p>
</li>
<li><p> division by zero - case <code>n/0</code>: when only the divisor becomes <code>0</code>, the corresponsning <code>0</code> is replaced by a small <code class="reqn">\epsilon = 0.00001</code>.
</p>
</li>
<li><p> log of zero - case <code>0 * log(0)</code>: is treated as <code>0</code>.
</p>
</li>
<li><p> log of zero - case <code>log(0)</code>: zero is replaced by a small <code class="reqn">\epsilon = 0.00001</code>.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>References</h3>

<p>Sung-Hyuk Cha. (2007). <em>Comprehensive Survey on Distance/Similarity Measures between Probability Density Functions</em>. International Journal of Mathematical Models and Methods in Applied Sciences 4: 1.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+getDistMethods">getDistMethods</a></code>, <code><a href="#topic+estimate.probability">estimate.probability</a></code>, <code><a href="#topic+dist.diversity">dist.diversity</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Simple Examples

# receive a list of implemented probability distance measures
getDistMethods()

## compute the euclidean distance between two probability vectors
distance(rbind(1:10/sum(1:10), 20:29/sum(20:29)), method = "euclidean")

## compute the euclidean distance between all pairwise comparisons of probability vectors
ProbMatrix &lt;- rbind(1:10/sum(1:10), 20:29/sum(20:29),30:39/sum(30:39))
distance(ProbMatrix, method = "euclidean")

# compute distance matrix without testing for NA values in the input matrix
distance(ProbMatrix, method = "euclidean", test.na = FALSE)

# alternatively use the colnames of the input data for the rownames and colnames
# of the output distance matrix
ProbMatrix &lt;- rbind(1:10/sum(1:10), 20:29/sum(20:29),30:39/sum(30:39))
rownames(ProbMatrix) &lt;- paste0("Example", 1:3)
distance(ProbMatrix, method = "euclidean", use.row.names = TRUE)

# Specialized Examples

CountMatrix &lt;- rbind(1:10, 20:29, 30:39)

## estimate probabilities from a count matrix
distance(CountMatrix, method = "euclidean", est.prob = "empirical")

## compute the euclidean distance for count data
## NOTE: some distance measures are only defined for probability values,
distance(CountMatrix, method = "euclidean")

## compute the Kullback-Leibler Divergence with different logarithm bases:
### case: unit = log (Default)
distance(ProbMatrix, method = "kullback-leibler", unit = "log")

### case: unit = log2
distance(ProbMatrix, method = "kullback-leibler", unit = "log2")

### case: unit = log10
distance(ProbMatrix, method = "kullback-leibler", unit = "log10")

</code></pre>

<hr>
<h2 id='divergence_sq'>Divergence squared distance (lowlevel function)</h2><span id='topic+divergence_sq'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the divergence_sq distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>divergence_sq(P, Q, testNA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="divergence_sq_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="divergence_sq_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="divergence_sq_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>divergence_sq(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE)
</code></pre>

<hr>
<h2 id='estimate.probability'>Estimate Probability Vectors From Count Vectors</h2><span id='topic+estimate.probability'></span>

<h3>Description</h3>

<p>This function takes a numeric count vector and returns estimated
probabilities of the corresponding counts.
</p>
<p>The following probability estimation methods are implemented in this function:
</p>

<ul>
<li> <p><code>method = "empirical"</code> : generates the relative frequency of the data <code>x/sum(x)</code>.
</p>
</li>
<li>
</li>
<li>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>estimate.probability(x, method = "empirical")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimate.probability_+3A_x">x</code></td>
<td>
<p>a numeric vector storing count values.</p>
</td></tr>
<tr><td><code id="estimate.probability_+3A_method">method</code></td>
<td>
<p>a character string specifying the estimation method tht should be used to estimate probabilities from input counts.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a numeric probability vector.
</p>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# generate a count vector
x &lt;- runif(100)

# generate a probability vector from corresponding counts
# method = "empirical"
x.prob &lt;- estimate.probability(x, method = "empirical")

</code></pre>

<hr>
<h2 id='euclidean'>Euclidean distance (lowlevel function)</h2><span id='topic+euclidean'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the euclidean distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>euclidean(P, Q, testNA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="euclidean_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="euclidean_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="euclidean_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>euclidean(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE)
</code></pre>

<hr>
<h2 id='fidelity'>Fidelity distance (lowlevel function)</h2><span id='topic+fidelity'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the fidelity distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fidelity(P, Q, testNA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fidelity_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="fidelity_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="fidelity_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>fidelity(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE)
</code></pre>

<hr>
<h2 id='getDistMethods'>Get method names for <code>distance</code></h2><span id='topic+getDistMethods'></span>

<h3>Description</h3>

<p>This function returns the names of the methods that can be applied
to compute distances between probability density functions using the <code><a href="#topic+distance">distance</a></code> 
function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getDistMethods()
</code></pre>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
getDistMethods()

</code></pre>

<hr>
<h2 id='gJSD'>Generalized Jensen-Shannon Divergence</h2><span id='topic+gJSD'></span>

<h3>Description</h3>

<p>This function computes the Generalized Jensen-Shannon Divergence of a probability matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gJSD(x, unit = "log2", weights = NULL, est.prob = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gJSD_+3A_x">x</code></td>
<td>
<p>a probability matrix.</p>
</td></tr>
<tr><td><code id="gJSD_+3A_unit">unit</code></td>
<td>
<p>a character string specifying the logarithm unit that shall be used to compute distances that depend on log computations.</p>
</td></tr>
<tr><td><code id="gJSD_+3A_weights">weights</code></td>
<td>
<p>a numeric vector specifying the weights for each distribution in <code>x</code>.
Default: <code>weights</code> = <code>NULL</code>; in this case all distributions are weighted equally (= uniform distribution of weights).
In case users wish to specify non-uniform weights for e.g. 3 distributions, they
can specify the argument <code>weights = c(0.5, 0.25, 0.25)</code>. This notation
denotes that <code>vec1</code> is weighted by <code>0.5</code>, <code>vec2</code> is weighted by <code>0.25</code>, and <code>vec3</code> is weighted by <code>0.25</code> as well.</p>
</td></tr>
<tr><td><code id="gJSD_+3A_est.prob">est.prob</code></td>
<td>
<p>method to estimate probabilities from input count vectors such as non-probability vectors. Default: <code>est.prob = NULL</code>. Options are:
</p>

<ul>
<li> <p><code>est.prob = "empirical"</code>: The relative frequencies of each vector are computed internally. For example an input matrix <code>rbind(1:10, 11:20)</code> will be transformed to a probability vector <code>rbind(1:10 / sum(1:10), 11:20 / sum(11:20))</code>
</p>
</li></ul>
</td></tr>
</table>


<h3>Details</h3>

<p>Function to compute the Generalized Jensen-Shannon Divergence
</p>
<p><code class="reqn">JSD_{\pi_1,...,\pi_n}(P_1, ..., P_n) = H(\sum_{i = 1}^n \pi_i * P_i) - \sum_{i = 1}^n \pi_i*H(P_i)</code>
</p>
<p>where <code class="reqn">\pi_1,...,\pi_n</code> denote the weights selected for the probability vectors <code>P_1,...,P_n</code> and <code>H(P_i)</code> denotes the Shannon Entropy of probability vector <code>P_i</code>.
</p>


<h3>Value</h3>

<p>The Jensen-Shannon divergence between all possible combinations of comparisons.
</p>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>See Also</h3>

<p><code><a href="#topic+KL">KL</a></code>, <code><a href="#topic+H">H</a></code>, <code><a href="#topic+JSD">JSD</a></code>,
<code><a href="#topic+CE">CE</a></code>, <code><a href="#topic+JE">JE</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># define input probability matrix
Prob &lt;- rbind(1:10/sum(1:10), 20:29/sum(20:29), 30:39/sum(30:39))

# compute the Generalized JSD comparing the PS probability matrix
gJSD(Prob)

# Generalized Jensen-Shannon Divergence between three vectors using different log bases
gJSD(Prob, unit = "log2") # Default
gJSD(Prob, unit = "log")
gJSD(Prob, unit = "log10")

# Jensen-Shannon Divergence Divergence between count vectors P.count and Q.count
P.count &lt;- 1:10
Q.count &lt;- 20:29
R.count &lt;- 30:39
x.count &lt;- rbind(P.count, Q.count, R.count)
gJSD(x.count, est.prob = "empirical")

</code></pre>

<hr>
<h2 id='gower'>Gower distance (lowlevel function)</h2><span id='topic+gower'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the gower distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gower(P, Q, testNA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gower_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="gower_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="gower_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>gower(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE)
</code></pre>

<hr>
<h2 id='H'>Shannon's Entropy <code class="reqn">H(X)</code></h2><span id='topic+H'></span>

<h3>Description</h3>

<p>Compute the Shannon's Entropy <code class="reqn">H(X) = - \sum P(X) * log2(P(X))</code> based on a
given probability vector <code class="reqn">P(X)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>H(x, unit = "log2")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="H_+3A_x">x</code></td>
<td>
<p>a numeric probability vector <code class="reqn">P(X)</code> for which
Shannon's Entropy <code class="reqn">H(X)</code> shall be computed.</p>
</td></tr>
<tr><td><code id="H_+3A_unit">unit</code></td>
<td>
<p>a character string specifying the logarithm unit that shall be used to compute distances that depend on log computations.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function might be useful to fastly compute Shannon's Entropy for any
given probability vector.
</p>


<h3>Value</h3>

<p>a numeric value representing Shannon's Entropy in bit.
</p>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>References</h3>

<p>Shannon, Claude E. 1948. &quot;A Mathematical Theory of
Communication&quot;. <em>Bell System Technical Journal</em> <b>27</b> (3): 379-423.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+JE">JE</a></code>, <code><a href="#topic+CE">CE</a></code>, <code><a href="#topic+KL">KL</a></code>, <code><a href="#topic+JSD">JSD</a></code>, <code><a href="#topic+gJSD">gJSD</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
H(1:10/sum(1:10))

</code></pre>

<hr>
<h2 id='harmonic_mean_dist'>Harmonic mean distance (lowlevel function)</h2><span id='topic+harmonic_mean_dist'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the harmonic_mean_dist distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>harmonic_mean_dist(P, Q, testNA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="harmonic_mean_dist_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="harmonic_mean_dist_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="harmonic_mean_dist_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>harmonic_mean_dist(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE)
</code></pre>

<hr>
<h2 id='hellinger'>Hellinger distance (lowlevel function)</h2><span id='topic+hellinger'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the hellinger distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hellinger(P, Q, testNA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hellinger_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="hellinger_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="hellinger_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>hellinger(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE)
</code></pre>

<hr>
<h2 id='inner_product'>Inner product distance (lowlevel function)</h2><span id='topic+inner_product'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the inner_product distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>inner_product(P, Q, testNA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="inner_product_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="inner_product_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="inner_product_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>inner_product(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE)
</code></pre>

<hr>
<h2 id='intersection_dist'>Intersection distance (lowlevel function)</h2><span id='topic+intersection_dist'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the intersection_dist distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>intersection_dist(P, Q, testNA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="intersection_dist_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="intersection_dist_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="intersection_dist_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>intersection_dist(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE)
</code></pre>

<hr>
<h2 id='jaccard'>Jaccard distance (lowlevel function)</h2><span id='topic+jaccard'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the jaccard distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jaccard(P, Q, testNA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="jaccard_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="jaccard_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="jaccard_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>jaccard(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE)
</code></pre>

<hr>
<h2 id='JE'>Shannon's Joint-Entropy <code class="reqn">H(X,Y)</code></h2><span id='topic+JE'></span>

<h3>Description</h3>

<p>This funciton computes Shannon's Joint-Entropy <code class="reqn">H(X,Y) = - \sum \sum P(X,Y) *
log2(P(X,Y))</code> based on a given joint-probability vector <code class="reqn">P(X,Y)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>JE(x, unit = "log2")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="JE_+3A_x">x</code></td>
<td>
<p>a numeric joint-probability vector <code class="reqn">P(X,Y)</code> for
which Shannon's Joint-Entropy <code class="reqn">H(X,Y)</code> shall be computed.</p>
</td></tr>
<tr><td><code id="JE_+3A_unit">unit</code></td>
<td>
<p>a character string specifying the logarithm unit that shall be used to compute distances that depend on log computations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a numeric value representing Shannon's Joint-Entropy in bit.
</p>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>References</h3>

<p>Shannon, Claude E. 1948. &quot;A Mathematical Theory of
Communication&quot;. <em>Bell System Technical Journal</em> <b>27</b> (3): 379-423.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+H">H</a></code>, <code><a href="#topic+CE">CE</a></code>, <code><a href="#topic+KL">KL</a></code>, <code><a href="#topic+JSD">JSD</a></code>, <code><a href="#topic+gJSD">gJSD</a></code>, <code><a href="#topic+distance">distance</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
JE(1:100/sum(1:100))

</code></pre>

<hr>
<h2 id='jeffreys'>Jeffreys distance (lowlevel function)</h2><span id='topic+jeffreys'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the jeffreys distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jeffreys(P, Q, testNA, unit, epsilon)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="jeffreys_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="jeffreys_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="jeffreys_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
<tr><td><code id="jeffreys_+3A_unit">unit</code></td>
<td>
<p>type of <code>log</code> function. Option are 
</p>

<ul>
<li> <p><code>unit = "log"</code>
</p>
</li>
<li> <p><code>unit = "log2"</code>
</p>
</li>
<li> <p><code>unit = "log10"</code>   
</p>
</li></ul>
</td></tr>
<tr><td><code id="jeffreys_+3A_epsilon">epsilon</code></td>
<td>
<p>epsilon a small value to address cases in the distance computation where division by zero occurs. In
these cases, x / 0 or 0 / 0 will be replaced by <code>epsilon</code>. The default is <code>epsilon = 0.00001</code>.
However, we recommend to choose a custom <code>epsilon</code> value depending on the size of the input vectors,
the expected similarity between compared probability density functions and 
whether or not many 0 values are present within the compared vectors.
As a rough rule of thumb we suggest that when dealing with very large 
input vectors which are very similar and contain many <code>0</code> values,
the <code>epsilon</code> value should be set even smaller (e.g. <code>epsilon = 0.000000001</code>),
whereas when vector sizes are small or distributions very divergent then
higher <code>epsilon</code> values may also be appropriate (e.g. <code>epsilon = 0.01</code>).
Addressing this <code>epsilon</code> issue is important to avoid cases where distance metrics
return negative values which are not defined and only occur due to the
technical issues of computing x / 0 or 0 / 0 cases.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>jeffreys(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE,
 unit = "log2", epsilon = 0.00001)
</code></pre>

<hr>
<h2 id='jensen_difference'>Jensen difference (lowlevel function)</h2><span id='topic+jensen_difference'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the jensen_difference distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jensen_difference(P, Q, testNA, unit)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="jensen_difference_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="jensen_difference_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="jensen_difference_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
<tr><td><code id="jensen_difference_+3A_unit">unit</code></td>
<td>
<p>type of <code>log</code> function. Option are 
</p>

<ul>
<li> <p><code>unit = "log"</code>
</p>
</li>
<li> <p><code>unit = "log2"</code>
</p>
</li>
<li> <p><code>unit = "log10"</code>   
</p>
</li></ul>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>jensen_difference(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE, unit = "log2")
</code></pre>

<hr>
<h2 id='jensen_shannon'>Jensen-Shannon distance (lowlevel function)</h2><span id='topic+jensen_shannon'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the jensen_shannon distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jensen_shannon(P, Q, testNA, unit)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="jensen_shannon_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="jensen_shannon_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="jensen_shannon_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
<tr><td><code id="jensen_shannon_+3A_unit">unit</code></td>
<td>
<p>type of <code>log</code> function. Option are 
</p>

<ul>
<li> <p><code>unit = "log"</code>
</p>
</li>
<li> <p><code>unit = "log2"</code>
</p>
</li>
<li> <p><code>unit = "log10"</code>   
</p>
</li></ul>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>jensen_shannon(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE, unit = "log2")
</code></pre>

<hr>
<h2 id='JSD'>Jensen-Shannon Divergence</h2><span id='topic+JSD'></span>

<h3>Description</h3>

<p>This function computes a distance matrix or distance value based on the Jensen-Shannon Divergence with equal weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>JSD(x, test.na = TRUE, unit = "log2", est.prob = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="JSD_+3A_x">x</code></td>
<td>
<p>a numeric <code>data.frame</code> or <code>matrix</code> (storing probability vectors) or a numeric <code>data.frame</code> or <code>matrix</code> storing counts (if <code>est.prob = TRUE</code>). See <code><a href="#topic+distance">distance</a></code> for details.</p>
</td></tr>
<tr><td><code id="JSD_+3A_test.na">test.na</code></td>
<td>
<p>a boolean value specifying whether input vectors shall be tested for NA values.</p>
</td></tr>
<tr><td><code id="JSD_+3A_unit">unit</code></td>
<td>
<p>a character string specifying the logarithm unit that shall be used to compute distances that depend on log computations.</p>
</td></tr>
<tr><td><code id="JSD_+3A_est.prob">est.prob</code></td>
<td>
<p>method to estimate probabilities from input count vectors such as non-probability vectors. Default: <code>est.prob = NULL</code>. Options are:
</p>

<ul>
<li> <p><code>est.prob = "empirical"</code>: The relative frequencies of each vector are computed internally. For example an input matrix <code>rbind(1:10, 11:20)</code> will be transformed to a probability vector <code>rbind(1:10 / sum(1:10), 11:20 / sum(11:20))</code>
</p>
</li></ul>
</td></tr>
</table>


<h3>Details</h3>

<p>Function to compute the Jensen-Shannon Divergence JSD(P || Q) between two
probability distributions P and Q with equal weights <code class="reqn">\pi_1</code> =
<code class="reqn">\pi_2</code> = <code class="reqn">1/2</code>.
</p>
<p>The Jensen-Shannon Divergence JSD(P || Q) between two probability
distributions P and Q is defined as:
</p>
<p style="text-align: center;"><code class="reqn">JSD(P || Q) = 0.5 * (KL(P || R) + KL(Q || R))</code>
</p>

<p>where <code class="reqn">R = 0.5 * (P + Q)</code> denotes the mid-point of the probability
vectors P and Q, and KL(P || R), KL(Q || R) denote the Kullback-Leibler
Divergence of P and R, as well as Q and R.
</p>
<p><strong>General properties of the Jensen-Shannon Divergence:</strong>
</p>

<ul>
<li> <p><code>1)</code> JSD is non-negative.
</p>
</li>
<li> <p><code>2)</code> JSD is a symmetric measure JSD(P || Q) = JSD(Q || P).
</p>
</li>
<li> <p><code>3)</code> JSD = 0, if and only if P = Q.
</p>
</li></ul>



<h3>Value</h3>

<p>a distance value or matrix based on JSD computations.
</p>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>References</h3>

<p>Lin J. 1991. &quot;Divergence Measures Based on the Shannon Entropy&quot;.
IEEE Transactions on Information Theory. (33) 1: 145-151.
</p>
<p>Endres M. and Schindelin J. E. 2003. &quot;A new metric for probability
distributions&quot;. IEEE Trans. on Info. Thy. (49) 3: 1858-1860.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+KL">KL</a></code>, <code><a href="#topic+H">H</a></code>, <code><a href="#topic+CE">CE</a></code>, <code><a href="#topic+gJSD">gJSD</a></code>, <code><a href="#topic+distance">distance</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Jensen-Shannon Divergence between P and Q
P &lt;- 1:10/sum(1:10)
Q &lt;- 20:29/sum(20:29)
x &lt;- rbind(P,Q)
JSD(x)

# Jensen-Shannon Divergence between P and Q using different log bases
JSD(x, unit = "log2") # Default
JSD(x, unit = "log")
JSD(x, unit = "log10")

# Jensen-Shannon Divergence Divergence between count vectors P.count and Q.count
P.count &lt;- 1:10
Q.count &lt;- 20:29
x.count &lt;- rbind(P.count,Q.count)
JSD(x.count, est.prob = "empirical")

# Example: Distance Matrix using JSD-Distance

Prob &lt;- rbind(1:10/sum(1:10), 20:29/sum(20:29), 30:39/sum(30:39))

# compute the KL matrix of a given probability matrix
JSDMatrix &lt;- JSD(Prob)

# plot a heatmap of the corresponding JSD matrix
heatmap(JSDMatrix)

</code></pre>

<hr>
<h2 id='k_divergence'>K-Divergence (lowlevel function)</h2><span id='topic+k_divergence'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the k_divergence distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>k_divergence(P, Q, testNA, unit)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="k_divergence_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="k_divergence_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="k_divergence_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
<tr><td><code id="k_divergence_+3A_unit">unit</code></td>
<td>
<p>type of <code>log</code> function. Option are 
</p>

<ul>
<li> <p><code>unit = "log"</code>
</p>
</li>
<li> <p><code>unit = "log2"</code>
</p>
</li>
<li> <p><code>unit = "log10"</code>   
</p>
</li></ul>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>k_divergence(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE, unit = "log2")
</code></pre>

<hr>
<h2 id='KL'>Kullback-Leibler Divergence</h2><span id='topic+KL'></span>

<h3>Description</h3>

<p>This function computes the Kullback-Leibler divergence of two probability
distributions P and Q.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KL(x, test.na = TRUE, unit = "log2", est.prob = NULL, epsilon = 1e-05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KL_+3A_x">x</code></td>
<td>
<p>a numeric <code>data.frame</code> or <code>matrix</code> (storing probability vectors) or a numeric <code>data.frame</code> or <code>matrix</code> storing counts (if <code>est.prob = TRUE</code>). See <code><a href="#topic+distance">distance</a></code> for details.</p>
</td></tr>
<tr><td><code id="KL_+3A_test.na">test.na</code></td>
<td>
<p>a boolean value indicating whether input vectors should be tested for NA values.</p>
</td></tr>
<tr><td><code id="KL_+3A_unit">unit</code></td>
<td>
<p>a character string specifying the logarithm unit that shall be used to compute distances that depend on log computations.</p>
</td></tr>
<tr><td><code id="KL_+3A_est.prob">est.prob</code></td>
<td>
<p>method to estimate probabilities from a count vector. Default: est.prob = NULL.</p>
</td></tr>
<tr><td><code id="KL_+3A_epsilon">epsilon</code></td>
<td>
<p>a small value to address cases in the KL computation where division by zero occurs. In
these cases, x / 0 or 0 / 0 will be replaced by <code>epsilon</code>. The default is <code>epsilon = 0.00001</code>.
However, we recommend to choose a custom <code>epsilon</code> value depending on the size of the input vectors,
the expected similarity between compared probability density functions and 
whether or not many 0 values are present within the compared vectors.
As a rough rule of thumb we suggest that when dealing with very large 
input vectors which are very similar and contain many <code>0</code> values,
the <code>epsilon</code> value should be set even smaller (e.g. <code>epsilon = 0.000000001</code>),
whereas when vector sizes are small or distributions very divergent then
higher <code>epsilon</code> values may also be appropriate (e.g. <code>epsilon = 0.01</code>).
Addressing this <code>epsilon</code> issue is important to avoid cases where distance metrics
return negative values which are not defined and only occur due to the
technical issues of computing x / 0 or 0 / 0 cases.</p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">KL(P||Q) = \sum P(P) * log2(P(P) / P(Q)) = H(P,Q) - H(P)</code>
</p>

<p>where H(P,Q) denotes the joint entropy of the probability
distributions P and Q and H(P) denotes the entropy of
probability distribution P. In case P = Q then KL(P,Q) = 0 and in case P !=
Q then KL(P,Q) &gt; 0.
</p>
<p>The KL divergence is a non-symmetric measure of the directed divergence
between two probability distributions P and Q. It only fulfills the
<em>positivity</em> property of a <em>distance metric</em>.
</p>
<p>Because of the relation KL(P||Q) = H(P,Q) - H(P), the Kullback-Leibler
divergence of two probability distributions P and Q is also named
<em>Cross Entropy</em> of two probability distributions P and Q.
</p>


<h3>Value</h3>

<p>The Kullback-Leibler divergence of probability vectors.
</p>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>References</h3>

<p>Cover Thomas M. and Thomas Joy A. 2006. Elements of Information
Theory. <em>John Wiley &amp; Sons</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+H">H</a></code>, <code><a href="#topic+CE">CE</a></code>, <code><a href="#topic+JSD">JSD</a></code>, <code><a href="#topic+gJSD">gJSD</a></code>, <code><a href="#topic+distance">distance</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Kulback-Leibler Divergence between P and Q
P &lt;- 1:10/sum(1:10)
Q &lt;- 20:29/sum(20:29)
x &lt;- rbind(P,Q)
KL(x)

# Kulback-Leibler Divergence between P and Q using different log bases
KL(x, unit = "log2") # Default
KL(x, unit = "log")
KL(x, unit = "log10")

# Kulback-Leibler Divergence between count vectors P.count and Q.count
P.count &lt;- 1:10
Q.count &lt;- 20:29
x.count &lt;- rbind(P.count,Q.count)
KL(x, est.prob = "empirical")

# Example: Distance Matrix using KL-Distance

Prob &lt;- rbind(1:10/sum(1:10), 20:29/sum(20:29), 30:39/sum(30:39))

# compute the KL matrix of a given probability matrix
KLMatrix &lt;- KL(Prob)

# plot a heatmap of the corresponding KL matrix
heatmap(KLMatrix)


</code></pre>

<hr>
<h2 id='kulczynski_d'>Kulczynski_d distance (lowlevel function)</h2><span id='topic+kulczynski_d'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the kulczynski_d distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kulczynski_d(P, Q, testNA, epsilon)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kulczynski_d_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="kulczynski_d_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="kulczynski_d_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
<tr><td><code id="kulczynski_d_+3A_epsilon">epsilon</code></td>
<td>
<p>epsilon a small value to address cases in the distance computation where division by zero occurs. In
these cases, x / 0 or 0 / 0 will be replaced by <code>epsilon</code>. The default is <code>epsilon = 0.00001</code>.
However, we recommend to choose a custom <code>epsilon</code> value depending on the size of the input vectors,
the expected similarity between compared probability density functions and 
whether or not many 0 values are present within the compared vectors.
As a rough rule of thumb we suggest that when dealing with very large 
input vectors which are very similar and contain many <code>0</code> values,
the <code>epsilon</code> value should be set even smaller (e.g. <code>epsilon = 0.000000001</code>),
whereas when vector sizes are small or distributions very divergent then
higher <code>epsilon</code> values may also be appropriate (e.g. <code>epsilon = 0.01</code>).
Addressing this <code>epsilon</code> issue is important to avoid cases where distance metrics
return negative values which are not defined and only occur due to the
technical issues of computing x / 0 or 0 / 0 cases.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>kulczynski_d(P = 1:10/sum(1:10), Q = 20:29/sum(20:29),
    testNA = FALSE, epsilon = 0.00001)
</code></pre>

<hr>
<h2 id='kullback_leibler_distance'>kullback-Leibler distance (lowlevel function)</h2><span id='topic+kullback_leibler_distance'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the kullback_leibler_distance distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kullback_leibler_distance(P, Q, testNA, unit, epsilon)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kullback_leibler_distance_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="kullback_leibler_distance_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="kullback_leibler_distance_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
<tr><td><code id="kullback_leibler_distance_+3A_unit">unit</code></td>
<td>
<p>type of <code>log</code> function. Option are 
</p>

<ul>
<li> <p><code>unit = "log"</code>
</p>
</li>
<li> <p><code>unit = "log2"</code>
</p>
</li>
<li> <p><code>unit = "log10"</code>   
</p>
</li></ul>
</td></tr>
<tr><td><code id="kullback_leibler_distance_+3A_epsilon">epsilon</code></td>
<td>
<p>epsilon a small value to address cases in the distance computation where division by zero occurs. In
these cases, x / 0 or 0 / 0 will be replaced by <code>epsilon</code>. The default is <code>epsilon = 0.00001</code>.
However, we recommend to choose a custom <code>epsilon</code> value depending on the size of the input vectors,
the expected similarity between compared probability density functions and 
whether or not many 0 values are present within the compared vectors.
As a rough rule of thumb we suggest that when dealing with very large 
input vectors which are very similar and contain many <code>0</code> values,
the <code>epsilon</code> value should be set even smaller (e.g. <code>epsilon = 0.000000001</code>),
whereas when vector sizes are small or distributions very divergent then
higher <code>epsilon</code> values may also be appropriate (e.g. <code>epsilon = 0.01</code>).
Addressing this <code>epsilon</code> issue is important to avoid cases where distance metrics
return negative values which are not defined and only occur due to the
technical issues of computing x / 0 or 0 / 0 cases.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>kullback_leibler_distance(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE,
 unit = "log2", epsilon = 0.00001)
</code></pre>

<hr>
<h2 id='kumar_hassebrook'>Kumar hassebrook distance (lowlevel function)</h2><span id='topic+kumar_hassebrook'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the kumar_hassebrook distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kumar_hassebrook(P, Q, testNA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kumar_hassebrook_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="kumar_hassebrook_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="kumar_hassebrook_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>kumar_hassebrook(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE)
</code></pre>

<hr>
<h2 id='kumar_johnson'>Kumar-Johnson distance (lowlevel function)</h2><span id='topic+kumar_johnson'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the kumar_johnson distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kumar_johnson(P, Q, testNA, epsilon)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kumar_johnson_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="kumar_johnson_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="kumar_johnson_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
<tr><td><code id="kumar_johnson_+3A_epsilon">epsilon</code></td>
<td>
<p>epsilon a small value to address cases in the distance computation where division by zero occurs. In
these cases, x / 0 or 0 / 0 will be replaced by <code>epsilon</code>. The default is <code>epsilon = 0.00001</code>.
However, we recommend to choose a custom <code>epsilon</code> value depending on the size of the input vectors,
the expected similarity between compared probability density functions and 
whether or not many 0 values are present within the compared vectors.
As a rough rule of thumb we suggest that when dealing with very large 
input vectors which are very similar and contain many <code>0</code> values,
the <code>epsilon</code> value should be set even smaller (e.g. <code>epsilon = 0.000000001</code>),
whereas when vector sizes are small or distributions very divergent then
higher <code>epsilon</code> values may also be appropriate (e.g. <code>epsilon = 0.01</code>).
Addressing this <code>epsilon</code> issue is important to avoid cases where distance metrics
return negative values which are not defined and only occur due to the
technical issues of computing x / 0 or 0 / 0 cases.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>kumar_johnson(P = 1:10/sum(1:10), Q = 20:29/sum(20:29),
 testNA = FALSE, epsilon = 0.00001)
</code></pre>

<hr>
<h2 id='lin.cor'>Linear Correlation</h2><span id='topic+lin.cor'></span>

<h3>Description</h3>

<p>This function computed the linear correlation between two vectors or a correlation matrix for an input matrix.
</p>
<p>The following methods to compute linear correlations are implemented in this function:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lin.cor(x, y = NULL, method = "pearson", test.na = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lin.cor_+3A_x">x</code></td>
<td>
<p>a numeric <code>vector</code>, <code>matrix</code>, or <code>data.frame</code>.</p>
</td></tr>
<tr><td><code id="lin.cor_+3A_y">y</code></td>
<td>
<p>a numeric <code>vector</code> that should be correlated with <code>x</code>.</p>
</td></tr>
<tr><td><code id="lin.cor_+3A_method">method</code></td>
<td>
<p>the method to compute the linear correlation between <code>x</code> and <code>y</code>.</p>
</td></tr>
<tr><td><code id="lin.cor_+3A_test.na">test.na</code></td>
<td>
<p>a boolean value indicating whether input data should be checked for <code>NA</code> values.</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li> <p><code>method = "pearson"</code> : Pearson's correlation coefficient (centred).
</p>
</li>
<li> <p><code>method = "pearson2"</code> : Pearson's uncentred correlation coefficient.
</p>
</li>
<li> <p><code>method = "sq_pearson"</code> . Squared Pearson's correlation coefficient.
</p>
</li>
<li> <p><code>method = "kendall"</code> : Kendall's correlation coefficient.
</p>
</li>
<li> <p><code>method = "spearman"</code> : Spearman's correlation coefficient.
</p>
</li></ul>
 
<p>Further Details:
</p>

<ul>
<li> <p><em>Pearson's correlation coefficient (centred)</em> : 
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>

<hr>
<h2 id='lorentzian'>Lorentzian distance (lowlevel function)</h2><span id='topic+lorentzian'></span>

<h3>Description</h3>

<p>The low-level function for computing the lorentzian distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lorentzian(P, Q, testNA, unit)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lorentzian_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="lorentzian_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="lorentzian_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
<tr><td><code id="lorentzian_+3A_unit">unit</code></td>
<td>
<p>type of <code>log</code> function. Option are 
</p>

<ul>
<li> <p><code>unit = "log"</code>
</p>
</li>
<li> <p><code>unit = "log2"</code>
</p>
</li>
<li> <p><code>unit = "log10"</code>   
</p>
</li></ul>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>lorentzian(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE, unit = "log2")
</code></pre>

<hr>
<h2 id='manhattan'>Manhattan distance (lowlevel function)</h2><span id='topic+manhattan'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the manhattan distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>manhattan(P, Q, testNA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="manhattan_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="manhattan_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="manhattan_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>manhattan(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE)
</code></pre>

<hr>
<h2 id='matusita'>Matusita distance (lowlevel function)</h2><span id='topic+matusita'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the matusita distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>matusita(P, Q, testNA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="matusita_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="matusita_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="matusita_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>matusita(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE)
</code></pre>

<hr>
<h2 id='MI'>Shannon's Mutual Information <code class="reqn">I(X,Y)</code></h2><span id='topic+MI'></span>

<h3>Description</h3>

<p>Compute Shannon's Mutual Information based on the identity <code class="reqn">I(X,Y) =
H(X) + H(Y) - H(X,Y)</code> based on a given joint-probability vector <code class="reqn">P(X,Y)</code>
and probability vectors <code class="reqn">P(X)</code> and <code class="reqn">P(Y)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MI(x, y, xy, unit = "log2")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MI_+3A_x">x</code></td>
<td>
<p>a numeric probability vector <code class="reqn">P(X)</code>.</p>
</td></tr>
<tr><td><code id="MI_+3A_y">y</code></td>
<td>
<p>a numeric probability vector <code class="reqn">P(Y)</code>.</p>
</td></tr>
<tr><td><code id="MI_+3A_xy">xy</code></td>
<td>
<p>a numeric joint-probability vector <code class="reqn">P(X,Y)</code>.</p>
</td></tr>
<tr><td><code id="MI_+3A_unit">unit</code></td>
<td>
<p>a character string specifying the logarithm unit that shall be used to compute distances that depend on log computations.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function might be useful to fastly compute Shannon's Mutual Information
for any given joint-probability vector and probability vectors.
</p>


<h3>Value</h3>

<p>Shannon's Mutual Information in bit.
</p>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>References</h3>

<p>Shannon, Claude E. 1948. &quot;A Mathematical Theory of
Communication&quot;. <em>Bell System Technical Journal</em> <b>27</b> (3): 379-423.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+H">H</a></code>, <code><a href="#topic+JE">JE</a></code>, <code><a href="#topic+CE">CE</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
MI( x = 1:10/sum(1:10), y = 20:29/sum(20:29), xy = 1:10/sum(1:10) )

</code></pre>

<hr>
<h2 id='minkowski'>Minkowski distance (lowlevel function)</h2><span id='topic+minkowski'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the minkowski distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>minkowski(P, Q, n, testNA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="minkowski_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="minkowski_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="minkowski_+3A_n">n</code></td>
<td>
<p>index for the minkowski exponent.</p>
</td></tr>
<tr><td><code id="minkowski_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>minkowski(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), n = 2, testNA = FALSE)
</code></pre>

<hr>
<h2 id='motyka'>Motyka distance (lowlevel function)</h2><span id='topic+motyka'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the motyka distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>motyka(P, Q, testNA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="motyka_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="motyka_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="motyka_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>motyka(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE)
</code></pre>

<hr>
<h2 id='neyman_chi_sq'>Neyman chi-squared distance (lowlevel function)</h2><span id='topic+neyman_chi_sq'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the neyman_chi_sq distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>neyman_chi_sq(P, Q, testNA, epsilon)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="neyman_chi_sq_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="neyman_chi_sq_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="neyman_chi_sq_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
<tr><td><code id="neyman_chi_sq_+3A_epsilon">epsilon</code></td>
<td>
<p>epsilon a small value to address cases in the distance computation where division by zero occurs. In
these cases, x / 0 or 0 / 0 will be replaced by <code>epsilon</code>. The default is <code>epsilon = 0.00001</code>.
However, we recommend to choose a custom <code>epsilon</code> value depending on the size of the input vectors,
the expected similarity between compared probability density functions and 
whether or not many 0 values are present within the compared vectors.
As a rough rule of thumb we suggest that when dealing with very large 
input vectors which are very similar and contain many <code>0</code> values,
the <code>epsilon</code> value should be set even smaller (e.g. <code>epsilon = 0.000000001</code>),
whereas when vector sizes are small or distributions very divergent then
higher <code>epsilon</code> values may also be appropriate (e.g. <code>epsilon = 0.01</code>).
Addressing this <code>epsilon</code> issue is important to avoid cases where distance metrics
return negative values which are not defined and only occur due to the
technical issues of computing x / 0 or 0 / 0 cases.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>neyman_chi_sq(P = 1:10/sum(1:10), Q = 20:29/sum(20:29),
 testNA = FALSE, epsilon = 0.00001)
</code></pre>

<hr>
<h2 id='pearson_chi_sq'>Pearson chi-squared distance (lowlevel function)</h2><span id='topic+pearson_chi_sq'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the pearson_chi_sq distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pearson_chi_sq(P, Q, testNA, epsilon)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pearson_chi_sq_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="pearson_chi_sq_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="pearson_chi_sq_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
<tr><td><code id="pearson_chi_sq_+3A_epsilon">epsilon</code></td>
<td>
<p>epsilon a small value to address cases in the distance computation where division by zero occurs. In
these cases, x / 0 or 0 / 0 will be replaced by <code>epsilon</code>. The default is <code>epsilon = 0.00001</code>.
However, we recommend to choose a custom <code>epsilon</code> value depending on the size of the input vectors,
the expected similarity between compared probability density functions and 
whether or not many 0 values are present within the compared vectors.
As a rough rule of thumb we suggest that when dealing with very large 
input vectors which are very similar and contain many <code>0</code> values,
the <code>epsilon</code> value should be set even smaller (e.g. <code>epsilon = 0.000000001</code>),
whereas when vector sizes are small or distributions very divergent then
higher <code>epsilon</code> values may also be appropriate (e.g. <code>epsilon = 0.01</code>).
Addressing this <code>epsilon</code> issue is important to avoid cases where distance metrics
return negative values which are not defined and only occur due to the
technical issues of computing x / 0 or 0 / 0 cases.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>pearson_chi_sq(P = 1:10/sum(1:10), Q = 20:29/sum(20:29),
 testNA = FALSE, epsilon = 0.00001)
</code></pre>

<hr>
<h2 id='prob_symm_chi_sq'>Probability symmetric chi-squared distance (lowlevel function)</h2><span id='topic+prob_symm_chi_sq'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the prob_symm_chi_sq distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prob_symm_chi_sq(P, Q, testNA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prob_symm_chi_sq_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="prob_symm_chi_sq_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="prob_symm_chi_sq_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>prob_symm_chi_sq(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE)
</code></pre>

<hr>
<h2 id='ruzicka'>Ruzicka distance (lowlevel function)</h2><span id='topic+ruzicka'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the ruzicka distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ruzicka(P, Q, testNA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ruzicka_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="ruzicka_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="ruzicka_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ruzicka(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE)
</code></pre>

<hr>
<h2 id='soergel'>Soergel distance (lowlevel function)</h2><span id='topic+soergel'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the soergel distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>soergel(P, Q, testNA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="soergel_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="soergel_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="soergel_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>soergel(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE)
</code></pre>

<hr>
<h2 id='sorensen'>Sorensen distance (lowlevel function)</h2><span id='topic+sorensen'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the sorensen distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sorensen(P, Q, testNA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sorensen_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="sorensen_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="sorensen_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>sorensen(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE)
</code></pre>

<hr>
<h2 id='squared_chi_sq'>Squared chi-squared distance (lowlevel function)</h2><span id='topic+squared_chi_sq'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the squared_chi_sq distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>squared_chi_sq(P, Q, testNA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="squared_chi_sq_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="squared_chi_sq_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="squared_chi_sq_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>squared_chi_sq(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE)
</code></pre>

<hr>
<h2 id='squared_chord'>Squared chord distance (lowlevel function)</h2><span id='topic+squared_chord'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the squared_chord distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>squared_chord(P, Q, testNA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="squared_chord_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="squared_chord_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="squared_chord_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>squared_chord(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE)
</code></pre>

<hr>
<h2 id='squared_euclidean'>Squared euclidean distance (lowlevel function)</h2><span id='topic+squared_euclidean'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the squared_euclidean distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>squared_euclidean(P, Q, testNA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="squared_euclidean_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="squared_euclidean_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="squared_euclidean_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>squared_euclidean(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE)
</code></pre>

<hr>
<h2 id='taneja'>Taneja difference (lowlevel function)</h2><span id='topic+taneja'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the taneja distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>taneja(P, Q, testNA, unit, epsilon)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="taneja_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="taneja_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="taneja_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
<tr><td><code id="taneja_+3A_unit">unit</code></td>
<td>
<p>type of <code>log</code> function. Option are 
</p>

<ul>
<li> <p><code>unit = "log"</code>
</p>
</li>
<li> <p><code>unit = "log2"</code>
</p>
</li>
<li> <p><code>unit = "log10"</code>   
</p>
</li></ul>
</td></tr>
<tr><td><code id="taneja_+3A_epsilon">epsilon</code></td>
<td>
<p>epsilon a small value to address cases in the distance computation where division by zero occurs. In
these cases, x / 0 or 0 / 0 will be replaced by <code>epsilon</code>. The default is <code>epsilon = 0.00001</code>.
However, we recommend to choose a custom <code>epsilon</code> value depending on the size of the input vectors,
the expected similarity between compared probability density functions and 
whether or not many 0 values are present within the compared vectors.
As a rough rule of thumb we suggest that when dealing with very large 
input vectors which are very similar and contain many <code>0</code> values,
the <code>epsilon</code> value should be set even smaller (e.g. <code>epsilon = 0.000000001</code>),
whereas when vector sizes are small or distributions very divergent then
higher <code>epsilon</code> values may also be appropriate (e.g. <code>epsilon = 0.01</code>).
Addressing this <code>epsilon</code> issue is important to avoid cases where distance metrics
return negative values which are not defined and only occur due to the
technical issues of computing x / 0 or 0 / 0 cases.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>taneja(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE,
 unit = "log2", epsilon = 0.00001)
</code></pre>

<hr>
<h2 id='tanimoto'>Tanimoto distance (lowlevel function)</h2><span id='topic+tanimoto'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the tanimoto distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tanimoto(P, Q, testNA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tanimoto_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="tanimoto_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="tanimoto_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tanimoto(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE)
</code></pre>

<hr>
<h2 id='topsoe'>Topsoe distance (lowlevel function)</h2><span id='topic+topsoe'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the topsoe distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>topsoe(P, Q, testNA, unit)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="topsoe_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="topsoe_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="topsoe_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
<tr><td><code id="topsoe_+3A_unit">unit</code></td>
<td>
<p>type of <code>log</code> function. Option are 
</p>

<ul>
<li> <p><code>unit = "log"</code>
</p>
</li>
<li> <p><code>unit = "log2"</code>
</p>
</li>
<li> <p><code>unit = "log10"</code>   
</p>
</li></ul>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>topsoe(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE, unit = "log2")
</code></pre>

<hr>
<h2 id='wave_hedges'>Wave hedges distance (lowlevel function)</h2><span id='topic+wave_hedges'></span>

<h3>Description</h3>

<p>The lowlevel function for computing the wave_hedges distance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wave_hedges(P, Q, testNA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wave_hedges_+3A_p">P</code></td>
<td>
<p>a numeric vector storing the first distribution.</p>
</td></tr>
<tr><td><code id="wave_hedges_+3A_q">Q</code></td>
<td>
<p>a numeric vector storing the second distribution.</p>
</td></tr>
<tr><td><code id="wave_hedges_+3A_testna">testNA</code></td>
<td>
<p>a logical value indicating whether or not distributions shall be checked for <code>NA</code> values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hajk-Georg Drost
</p>


<h3>Examples</h3>

<pre><code class='language-R'>wave_hedges(P = 1:10/sum(1:10), Q = 20:29/sum(20:29), testNA = FALSE)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
