<!DOCTYPE html><html lang="en"><head><title>Help for package Rmpi</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {Rmpi}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#lamhosts'><p>Hosts Information</p></a></li>
<li><a href='#mpi.abort'><p>MPI_Abort API</p></a></li>
<li><a href='#mpi.any.source'><p>MPI Constants</p></a></li>
<li><a href='#mpi.apply'><p>Scatter an array to slaves and then apply a FUN</p></a></li>
<li><a href='#mpi.applyLB'><p>(Load balancing) parallel apply</p></a></li>
<li><a href='#mpi.barrier'><p>MPI_Barrier API</p></a></li>
<li><a href='#mpi.bcast'><p>MPI_Bcast API</p></a></li>
<li><a href='#mpi.bcast.cmd'><p>Extension of MPI_Bcast API</p></a></li>
<li><a href='#mpi.bcast.Robj'><p>Extensions of MPI_Bcast API</p></a></li>
<li><a href='#mpi.cart.coords'><p>MPI_Cart_coords</p></a></li>
<li><a href='#mpi.cart.create'><p>MPI_Cart_create</p></a></li>
<li><a href='#mpi.cart.get'><p>MPI_Cart_get</p></a></li>
<li><a href='#mpi.cart.rank'><p>MPI_Cart_rank</p></a></li>
<li><a href='#mpi.cart.shift'><p>MPI_Cart_shift</p></a></li>
<li><a href='#mpi.cartdim.get'><p>MPI_Cartdim_get</p></a></li>
<li><a href='#mpi.comm.disconnect'><p>MPI_Comm_disconnect API</p></a></li>
<li><a href='#mpi.comm.free'><p>MPI_Comm_free API</p></a></li>
<li><a href='#mpi.comm.get.parent'><p>MPI_Comm_get_parent, MPI_Comm_remote_size, MPI_Comm_test_inter</p>
APIs</a></li>
<li><a href='#mpi.comm.set.errhandler'><p>MPI_Comm_set_errhandler API</p></a></li>
<li><a href='#mpi.comm.size'><p>MPI_Comm_c2f, MPI_Comm_dup, MPI_Comm_rank, and MPI_Comm_size APIs</p></a></li>
<li><a href='#mpi.comm.spawn'><p>MPI_Comm_spawn API</p></a></li>
<li><a href='#mpi.dims.create'><p>MPI_Dims_create</p></a></li>
<li><a href='#mpi.exit'><p>Exit MPI Environment</p></a></li>
<li><a href='#mpi.finalize'><p>MPI_Finalize API</p></a></li>
<li><a href='#mpi.gather'><p>MPI_Gather, MPI_Gatherv, MPI_Allgather, and MPI_Allgatherv APIs</p></a></li>
<li><a href='#mpi.gather.Robj'><p>Extentions of MPI_Gather and MPI_Allgather APIs</p></a></li>
<li><a href='#mpi.get.count'><p>MPI_Get_count API</p></a></li>
<li><a href='#mpi.get.processor.name'><p>MPI_Get_processor_name API</p></a></li>
<li><a href='#mpi.get.sourcetag'><p>Utility for finding the source and tag of a received message</p></a></li>
<li><a href='#mpi.iapplyLB'><p>(Load balancing) parallel apply with nonblocking features</p></a></li>
<li><a href='#mpi.info.create'><p>MPI_Info_create, MPI_Info_free, MPI_Info_get, MPI_Info_set APIs</p></a></li>
<li><a href='#mpi.intercomm.merge'><p>MPI_Intercomm_merge API</p></a></li>
<li><a href='#mpi.parSim'><p>Parallel Monte Carlo Simulation</p></a></li>
<li><a href='#mpi.probe'><p>MPI_Probe and MPI_Iprobe APIs</p></a></li>
<li><a href='#mpi.realloc'><p>Find and increase the lengthes of MPI opaques comm, request, and</p>
status</a></li>
<li><a href='#mpi.reduce'><p>MPI_Reduce and MPI_Allreduce APIs</p></a></li>
<li><a href='#mpi.remote.exec'><p>Remote Executions on R slaves</p></a></li>
<li><a href='#mpi.scatter'><p>MPI_Scatter and MPI_Scatterv APIs</p></a></li>
<li><a href='#mpi.scatter.Robj'><p>Extensions of MPI_ SCATTER and MPI_SCATTERV</p></a></li>
<li><a href='#mpi.send'><p>MPI_Send, MPI_Isend, MPI_Recv, and MPI_Irecv APIs</p></a></li>
<li><a href='#mpi.send.Robj'><p>Extensions of MPI_Send and MPI_Recv APIs</p></a></li>
<li><a href='#mpi.sendrecv'><p>MPI_Sendrecv and MPI_Sendrecv_replace APIs</p></a></li>
<li><a href='#mpi.setup.rngstream'><p>Setup parallel RNG on all slaves</p></a></li>
<li><a href='#mpi.spawn.Rslaves'><p>Spawn and Close R Slaves</p></a></li>
<li><a href='#mpi.universe.size'><p>MPI_Universe_size API</p></a></li>
<li><a href='#mpi.wait'><p>Nonblocking completion operations</p></a></li>
<li><a href='#string'><p>Internal functions</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Version:</td>
<td>0.7-3.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2025-01-13</td>
</tr>
<tr>
<td>Title:</td>
<td>Interface (Wrapper) to MPI (Message-Passing Interface)</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.15.1)</td>
</tr>
<tr>
<td>Imports:</td>
<td>parallel</td>
</tr>
<tr>
<td>Description:</td>
<td>An interface (wrapper) to MPI. It also 
	     provides interactive R manager and worker environment.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://fisher.stats.uwo.ca/faculty/yu/Rmpi/">https://fisher.stats.uwo.ca/faculty/yu/Rmpi/</a></td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Hao Yu &lt;hyu@stats.uwo.ca&gt;</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-01-13 15:26:37 UTC; hyuadm</td>
</tr>
<tr>
<td>Author:</td>
<td>Hao Yu [aut, cre]</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-01-13 16:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='lamhosts'>Hosts Information</h2><span id='topic+mpi.is.master'></span><span id='topic+lamhosts'></span><span id='topic+mpi.hostinfo'></span><span id='topic+slave.hostinfo'></span>

<h3>Description</h3>

<p><code>lamhosts</code> finds the host name associated with its node number. Can be used 
by <code><a href="#topic+mpi.spawn.Rslaves">mpi.spawn.Rslaves</a></code> to spawn R slaves on selected hosts. This is 
a LAM-MPI specific function.
</p>
<p><code>mpi.is.master</code> checks if it is running on master or slaves.
</p>
<p><code>mpi.hostinfo</code> finds an individual host information including rank and 
size in a comm.
</p>
<p><code>slave.hostinfo</code> is executed only by master and find all master and slaves 
host information in a comm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lamhosts()
mpi.is.master()
mpi.hostinfo(comm = 1)
slave.hostinfo(comm = 1, short=TRUE) 
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lamhosts_+3A_comm">comm</code></td>
<td>
<p>a communicator number</p>
</td></tr>
<tr><td><code id="lamhosts_+3A_short">short</code></td>
<td>
<p>if true, a short form is printed</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>lamhosts</code> returns CPUs nodes numbers with their host names.
</p>
<p><code>mpi.is.master</code> returns TRUE if it is on master and FALSE otherwise.
</p>
<p><code>mpi.hostinfo</code> sends to stdio a host name, rank, size and comm.
</p>
<p><code>slave.hostname</code> sends to stdio a list of host, rank, size, and comm 
information for all master and slaves. With short=TRUE and 8 slaves or more, 
the first 3 and last 2 slaves are shown. </p>


<h3>Author(s)</h3>

<p>Hao Yu
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.spawn.Rslaves">mpi.spawn.Rslaves</a></code>
</p>

<hr>
<h2 id='mpi.abort'>MPI_Abort API</h2><span id='topic+mpi.abort'></span>

<h3>Description</h3>

<p><code>mpi.abort</code> makes a &ldquo;best attempt&quot; to abort all tasks in a comm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  mpi.abort(comm = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.abort_+3A_comm">comm</code></td>
<td>
<p>a communicator number</p>
</td></tr>
</table>


<h3>Value</h3>

<p>1 if success. Otherwise 0. 
</p>


<h3>Author(s)</h3>

<p>Hao Yu
</p>


<h3>References</h3>

<p><a href="https://www.open-mpi.org/">https://www.open-mpi.org/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.finalize">mpi.finalize</a></code>
</p>

<hr>
<h2 id='mpi.any.source'>MPI Constants</h2><span id='topic+mpi.any.source'></span><span id='topic+mpi.any.tag'></span><span id='topic+mpi.proc.null'></span>

<h3>Description</h3>

<p>Find MPI constants: MPI_ANY_SOURCE, MPI_ANY_TAG, or MPI_PROC_NULL
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mpi.any.source()
mpi.any.tag()
mpi.proc.null()
</code></pre>


<h3>Arguments</h3>

<p>None
</p>


<h3>Details</h3>

<p>These constants are mainly used by 
<code><a href="#topic+mpi.send">mpi.send</a></code>, <code><a href="#topic+mpi.recv">mpi.recv</a></code>, and 
<code><a href="#topic+mpi.probe">mpi.probe</a></code>. 
Different implementation of MPI may use different 
integers for MPI_ANY_SOURCE, MPI_ANY_TAG, and MPI_PROC_NULL. Hence one 
should use these functions instead  real integers for MPI communications.
</p>


<h3>Value</h3>

<p>Each function returns an integer value.
</p>


<h3>References</h3>

<p><a href="https://www.open-mpi.org/">https://www.open-mpi.org/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.send">mpi.send</a></code>, <code><a href="#topic+mpi.recv">mpi.recv</a></code>.
</p>

<hr>
<h2 id='mpi.apply'>Scatter an array to slaves and then apply a FUN</h2><span id='topic+mpi.apply'></span><span id='topic+mpi.iapply'></span>

<h3>Description</h3>

<p>An array (length &lt;= total number of slaves) is scattered to slaves so that the first 
slave calls <code>FUN</code> with arguments <code>x[[1]]</code> and <code>...</code>, the second one 
calls with arguments <code>x[[2]]</code> and <code>...</code>, and so on. <code>mpi.iapply</code> is a 
nonblocking version of <code>mpi.apply</code> so that it will not consume CPU on master node.</p>


<h3>Usage</h3>

<pre><code class='language-R'>mpi.apply(X, FUN, ..., comm=1)  
mpi.iapply(X, FUN, ..., comm=1, sleep=0.01)  
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.apply_+3A_x">X</code></td>
<td>
<p>an array</p>
</td></tr>
<tr><td><code id="mpi.apply_+3A_fun">FUN</code></td>
<td>
<p>a function</p>
</td></tr>
<tr><td><code id="mpi.apply_+3A_...">...</code></td>
<td>
<p>optional arguments to <code>FUN</code></p>
</td></tr>
<tr><td><code id="mpi.apply_+3A_comm">comm</code></td>
<td>
<p>a communicator number</p>
</td></tr>
<tr><td><code id="mpi.apply_+3A_sleep">sleep</code></td>
<td>
<p>a sleep interval on master node (in sec)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of the results is returned. Its length is the same as that of <code>x</code>. In 
case the call <code>FUN</code> with arguments <code>x[[i]]</code> and <code>...</code> fails on ith 
slave, corresponding error message will be returned in the returning list.  </p>


<h3>Author(s)</h3>

<p>Hao Yu
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#Assume that there are at least 5 slaves running
#Otherwise run mpi.spawn.Rslaves(nslaves=5)
#x=c(10,20)
#mpi.apply(x,runif)
#meanx=1:5
#mpi.apply(meanx,rnorm,n=2,sd=4)

</code></pre>

<hr>
<h2 id='mpi.applyLB'>(Load balancing) parallel apply</h2><span id='topic+mpi.applyLB'></span><span id='topic+mpi.parApply'></span><span id='topic+mpi.parLapply'></span><span id='topic+mpi.parSapply'></span><span id='topic+mpi.parRapply'></span><span id='topic+mpi.parCapply'></span><span id='topic+mpi.parReplicate'></span><span id='topic+mpi.parMM'></span>

<h3>Description</h3>

<p>(Load balancing) parallel<code>lapply</code> and related functions. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mpi.applyLB(X, FUN, ..., apply.seq=NULL, comm=1)
mpi.parApply(X, MARGIN, FUN, ..., job.num = mpi.comm.size(comm)-1,
                    apply.seq=NULL, comm=1)
mpi.parLapply(X, FUN, ..., job.num=mpi.comm.size(comm)-1, apply.seq=NULL, 
		comm=1)  
mpi.parSapply(X, FUN, ..., job.num=mpi.comm.size(comm)-1, apply.seq=NULL, 
		simplify=TRUE, USE.NAMES = TRUE, comm=1)  
mpi.parRapply(X, FUN, ..., job.num=mpi.comm.size(comm)-1, apply.seq=NULL, 
		comm=1)  
mpi.parCapply(X, FUN, ..., job.num=mpi.comm.size(comm)-1, apply.seq=NULL, 
		comm=1)  
mpi.parReplicate(n, expr, job.num=mpi.comm.size(comm)-1, apply.seq=NULL, 
		simplify = "array", comm=1)
mpi.parMM (A, B, job.num=mpi.comm.size(comm)-1, comm=1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.applyLB_+3A_x">X</code></td>
<td>
<p>an array or matrix.</p>
</td></tr>
<tr><td><code id="mpi.applyLB_+3A_margin">MARGIN</code></td>
<td>
<p>vector specifying the dimensions to use.</p>
</td></tr>
<tr><td><code id="mpi.applyLB_+3A_fun">FUN</code></td>
<td>
<p>a function.</p>
</td></tr>
<tr><td><code id="mpi.applyLB_+3A_simplify">simplify</code></td>
<td>
<p>logical or character string; should the result be simplified
to a vector, matrix or higher dimensional array if possible?</p>
</td></tr>
<tr><td><code id="mpi.applyLB_+3A_use.names">USE.NAMES</code></td>
<td>
<p>logical; if <code>TRUE</code> and if <code>X</code> is character, use <code>X</code> as
<code>names</code> for the result unless it had names already.</p>
</td></tr>
<tr><td><code id="mpi.applyLB_+3A_n">n</code></td>
<td>
<p>number of replications.</p>
</td></tr>
<tr><td><code id="mpi.applyLB_+3A_a">A</code></td>
<td>
<p>a matrix</p>
</td></tr>
<tr><td><code id="mpi.applyLB_+3A_b">B</code></td>
<td>
<p>a matrix</p>
</td></tr>
<tr><td><code id="mpi.applyLB_+3A_expr">expr</code></td>
<td>
<p>expression to evaluate repeatedly.</p>
</td></tr>
<tr><td><code id="mpi.applyLB_+3A_job.num">job.num</code></td>
<td>
<p>Total job numbers. If job numbers is bigger than total slave numbers (default 
value), a load balancing approach is used.</p>
</td></tr>
<tr><td><code id="mpi.applyLB_+3A_apply.seq">apply.seq</code></td>
<td>
<p>if reproducing the same computation (simulation) is desirable, set it 
to the integer vector .mpi.applyLB generated in previous computation (simulation).</p>
</td></tr>
<tr><td><code id="mpi.applyLB_+3A_...">...</code></td>
<td>
<p>optional arguments to <code>FUN</code></p>
</td></tr>
<tr><td><code id="mpi.applyLB_+3A_comm">comm</code></td>
<td>
<p>a communicator number</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Unless length of <code>X</code> is no more than total slave numbers (slave.num) and in this case 
<code>mpi.applyLB</code> is the same as <code>mpi.apply</code>, <code>mpi.applyLB</code> sends a next job to a 
slave who just delivered a finished job. The sequence of slaves who deliver results to master are 
saved into <code>.mpi.applyLB</code>. It keeps track which part of results done by which slaves. 
<code>.mpi.applyLB</code> can be used to reproduce the same simulation result if the same seed is 
used and the argument <code>apply.seq</code> is equal to <code>.mpi.applyLB</code>.
</p>
<p>With the default value of argument <code>job.num</code> which is slave.num, <code>mpi.parApply</code>, 
<code>mpi.parLapply</code>, <code>mpi.parSapply</code>, <code>mpi.parRapply</code>, <code>mpi.parCapply</code>, 
<code>mpi.parSapply</code>, and <code>mpi.parMM</code> are clones of <span class="pkg">snow</span>'s parApply, parLappy, 
parSapply, parRapply, parCapply, parSapply, and parMM, respectively. When <code>job.num</code> is 
bigger than slave.num, a load balancing approach is used.
</p>


<h3>Warning</h3>

<p>When using the argument <code>apply.seq</code> with <code>.mpi.applyLB</code>, be sure all settings are the same 
as before, i.e., the same data, job.num, slave.num, and seed. Otherwise a deadlock could occur. 
Notice that <code>apply.seq</code> is useful only if <code>job.num</code> is bigger than slave.num. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.apply">mpi.apply</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#Assume that there are some slaves running

#mpi.applyLB
#x=1:7
#mpi.applyLB(x,rnorm,mean=2,sd=4)

#get the same simulation 
#mpi.remote.exec(set.seed(111))
#mpi.applyLB(x,rnorm,mean=2,sd=4)
#mpi.remote.exec(set.seed(111))
#mpi.applyLB(x,rnorm,mean=2,sd=4,apply.seq=.mpi.applyLB)

#mpi.parApply
#x=1:24
#dim(x)=c(2,3,4)
#mpi.parApply(x, MARGIN=c(1,2), FUN=mean,job.num = 5)

#mpi.parLapply
#mdat &lt;- matrix(c(1,2,3, 7,8,9), nrow = 2, ncol=3, byrow=TRUE,
#                    dimnames = list(c("R.1", "R.2"), c("C.1", "C.2", "C.3")))
#mpi.parLapply(mdat, rnorm) 

#mpi.parSapply
#mpi.parSapply(mdat, rnorm) 

#mpi.parMM
#A=matrix(1:1000^2,ncol=1000)
#mpi.parMM(A,A)

</code></pre>

<hr>
<h2 id='mpi.barrier'>MPI_Barrier API</h2><span id='topic+mpi.barrier'></span>

<h3>Description</h3>

<p><code>mpi.barrier</code> blocks the caller until all members have called it.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  mpi.barrier(comm = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.barrier_+3A_comm">comm</code></td>
<td>
<p>a communicator number</p>
</td></tr>
</table>


<h3>Value</h3>

<p>1 if success. Otherwise 0. 
</p>


<h3>Author(s)</h3>

<p>Hao Yu
</p>


<h3>References</h3>

<p><a href="https://www.open-mpi.org/">https://www.open-mpi.org/</a>
</p>

<hr>
<h2 id='mpi.bcast'>MPI_Bcast API</h2><span id='topic+mpi.bcast'></span>

<h3>Description</h3>

<p><code>mpi.bcast</code> is a collective call among all members in a comm. It 
broadcasts a message from the specified rank to all members. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mpi.bcast(x, type, rank = 0, comm = 1, buffunit=100)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.bcast_+3A_x">x</code></td>
<td>
<p>data to be sent or received. Must be the same 
type among all members.</p>
</td></tr>
<tr><td><code id="mpi.bcast_+3A_type">type</code></td>
<td>
<p>1 for integer, 2 for double, and 3 for 
character. Others are not supported.</p>
</td></tr>
<tr><td><code id="mpi.bcast_+3A_rank">rank</code></td>
<td>
<p>the sender.</p>
</td></tr>
<tr><td><code id="mpi.bcast_+3A_comm">comm</code></td>
<td>
<p>a communicator number.</p>
</td></tr>
<tr><td><code id="mpi.bcast_+3A_buffunit">buffunit</code></td>
<td>
<p>a buffer unit number.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>mpi.bcast</code> is a blocking call among all members in a comm, i.e, 
all members have to wait until everyone calls it. All members have to 
prepare the same type of messages (buffers). Hence it is relatively 
difficult to use in R environment since the receivers may not know what 
types of data to receive, not mention the length of data. Users should 
use various extensions of <code>mpi.bcast</code> in R. They are 
<code><a href="#topic+mpi.bcast.Robj">mpi.bcast.Robj</a></code>, <code><a href="#topic+mpi.bcast.cmd">mpi.bcast.cmd</a></code>, and 
<code><a href="#topic+mpi.bcast.Robj2slave">mpi.bcast.Robj2slave</a></code>.
</p>
<p>When type=5, MPI continuous datatype (double) is defined with unit given by 
<code>buffunit</code>. It is used to transfer huge data where a double vector or matrix
is divided into many chunks with unit <code>buffunit</code>. Total 
ceiling(length(obj)/buffunit) units are transferred. Due to MPI specification, both
<code>buffunit</code> and total units transferred cannot be over 2^31-1. Notice that the last 
chunk may not have full length of data due to rounding. Special care is needed.
</p>


<h3>Value</h3>

<p><code>mpi.bcast</code> returns the message broadcasted by the sender 
(specified by the rank).
</p>


<h3>References</h3>

<p><a href="https://www.open-mpi.org/">https://www.open-mpi.org/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.bcast.Robj">mpi.bcast.Robj</a></code>,
<code><a href="#topic+mpi.bcast.cmd">mpi.bcast.cmd</a></code>,
<code><a href="#topic+mpi.bcast.Robj2slave">mpi.bcast.Robj2slave</a></code>.
</p>

<hr>
<h2 id='mpi.bcast.cmd'>Extension of MPI_Bcast API</h2><span id='topic+mpi.bcast.cmd'></span>

<h3>Description</h3>

<p><code>mpi.bcast.cmd</code> is an extension of <code><a href="#topic+mpi.bcast">mpi.bcast</a></code>. 
It is mainly used to transmit a command from master to all R slaves 
spawned by using slavedaemon.R script.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mpi.bcast.cmd(cmd=NULL, ..., rank = 0, comm = 1, nonblock=FALSE, sleep=0.1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.bcast.cmd_+3A_cmd">cmd</code></td>
<td>
<p>a command to be sent from master.</p>
</td></tr>
<tr><td><code id="mpi.bcast.cmd_+3A_...">...</code></td>
<td>
<p>used as arguments to cmd (function command) for passing their 
(master) values to R slaves, i.e., if &lsquo;myfun(x)&rsquo; will be executed on R slaves 
with &lsquo;x&rsquo; as master variable, use mpi.bcast.cmd(cmd=myfun, x=x).</p>
</td></tr>
<tr><td><code id="mpi.bcast.cmd_+3A_rank">rank</code></td>
<td>
<p>the sender</p>
</td></tr>
<tr><td><code id="mpi.bcast.cmd_+3A_comm">comm</code></td>
<td>
<p>a communicator number</p>
</td></tr>
<tr><td><code id="mpi.bcast.cmd_+3A_nonblock">nonblock</code></td>
<td>
<p>logical. If TRUE, a nonblock procedure is used on all receivers so that
they will consume none or little CPUs while waiting.</p>
</td></tr>
<tr><td><code id="mpi.bcast.cmd_+3A_sleep">sleep</code></td>
<td>
<p>a sleep interval, used when nonblock=TRUE. Smaller sleep is, more response receivers are, 
more CPUs consume</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>mpi.bcast.cmd</code> is a collective call. This means all members in a communicator must 
execute it at the same time. If slaves are spawned (created) by using slavedaemon.R (Rprofile script), 
then they are running <code>mpi.bcast.cmd</code> in infinite loop (idle state). Hence master can execute 
<code>mpi.bcast.cmd</code> alone to start computation. On the master, <code>cmd</code> and <code>...</code> are put together 
as a list which is then broadcasted (after serialization) to all slaves (using for loop with mpi.send 
and mpi.recv pair). All slaves will return an expression which will be evaluated by either slavedaemon.R, 
or by whatever an R script based on slavedaemon.R.
</p>
<p>If nonblock=TRUE, then on receiving side, a nonblock procedure is used to check if 
there is a message. If not, it will sleep for the specied amount and repeat itself.
</p>
<p>Please use <code><a href="#topic+mpi.remote.exec">mpi.remote.exec</a></code> if you want the executed results returned from R 
slaves.
</p>


<h3>Value</h3>

<p><code>mpi.bcast.cmd</code> returns no value for the sender and an expression of the transmitted command for others.
</p>


<h3>Warning</h3>

<p>Be caution to use <code>mpi.bcast.cmd</code> alone by master in the middle of comptuation. Only all slaves in idle 
states (waiting instructions from master) can it be used. Othewise it may result miscommunication
with other MPI calls.
</p>


<h3>Author(s)</h3>

<p>Hao Yu
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.remote.exec">mpi.remote.exec</a></code>
</p>

<hr>
<h2 id='mpi.bcast.Robj'>Extensions of MPI_Bcast API</h2><span id='topic+mpi.bcast.Robj'></span><span id='topic+mpi.bcast.Robj2slave'></span><span id='topic+mpi.bcast.Rfun2slave'></span><span id='topic+mpi.bcast.data2slave'></span>

<h3>Description</h3>

<p><code>mpi.bcast.Robj</code> and <code>mpi.bcast.Robj2slave</code> are used to move 
a general R object around among master and all slaves. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mpi.bcast.Robj(obj = NULL, rank = 0, comm = 1)
mpi.bcast.Robj2slave(obj, comm = 1, all = FALSE)
mpi.bcast.Rfun2slave(comm = 1)
mpi.bcast.data2slave(obj, comm = 1, buffunit = 100)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.bcast.Robj_+3A_obj">obj</code></td>
<td>
<p>an R object to be transmitted from the sender</p>
</td></tr>
<tr><td><code id="mpi.bcast.Robj_+3A_rank">rank</code></td>
<td>
<p>the sender.</p>
</td></tr>
<tr><td><code id="mpi.bcast.Robj_+3A_comm">comm</code></td>
<td>
<p>a communicator number.</p>
</td></tr>
<tr><td><code id="mpi.bcast.Robj_+3A_all">all</code></td>
<td>
<p>a logical. If TRUE, all R objects on master are transmitted to slaves.</p>
</td></tr>
<tr><td><code id="mpi.bcast.Robj_+3A_buffunit">buffunit</code></td>
<td>
<p>a buffer unit number.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>mpi.bcast.Robj</code> is an extension of <code><a href="#topic+mpi.bcast">mpi.bcast</a></code> for 
moving a general R object around from a sender to everyone. 
<code>mpi.bcast.Robj2slave</code> does an R object transmission from 
master to all slaves unless all=TRUE in which case, all master's objects with 
the global enviroment are transmitted to all slavers.  
</p>
<p><code>mpi.bcast.data2slave</code> transfers data (a double vector or a matrix) 
natively without (un)serilization. It should be used with a huge vector or matrix.
It results less memory usage and faster transmission. Notice that data with 
missing values (NA) are allowed.
</p>


<h3>Value</h3>

<p><code>mpi.bcast.Robj</code> returns no value for the sender and the 
transmitted one for others. <code>mpi.bcast.Robj2slave</code> returns no value for the master 
and the transmitted R object along its name on slaves. <code>mpi.bcast.Rfun2slave</code> 
transmits all master's functions to slaves and returns no value. <code>mpi.bcast.data2slave</code>
transmits a double vector or a matrix to slaves and returns no value.
</p>


<h3>Author(s)</h3>

<p>Hao Yu
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.send.Robj">mpi.send.Robj</a></code>,
<code><a href="#topic+mpi.recv.Robj">mpi.recv.Robj</a></code>,
</p>

<hr>
<h2 id='mpi.cart.coords'>MPI_Cart_coords</h2><span id='topic+mpi.cart.coords'></span>

<h3>Description</h3>

<p><code>mpi.cart.coords</code> translates a rank to its Cartesian topology coordinate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mpi.cart.coords(comm=3, rank, maxdims)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.cart.coords_+3A_comm">comm</code></td>
<td>
<p>Communicator with Cartesian structure</p>
</td></tr>
<tr><td><code id="mpi.cart.coords_+3A_rank">rank</code></td>
<td>
<p>rank of a process within group</p>
</td></tr>
<tr><td><code id="mpi.cart.coords_+3A_maxdims">maxdims</code></td>
<td>
<p>length of vector coord in the calling program</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is the rank-to-coordinates translator.  It is the inverse map of
<code>mpi.cart.rank</code>.  maxdims is at least as big as ndims as
returned by <code>mpi.cartdim.get</code>.
</p>


<h3>Value</h3>

<p><code>mpi.cart.coords</code> returns an integer array containing the Cartesian
coordinates of specified process. 
</p>


<h3>Author(s)</h3>

<p>Alek Hunchak and Hao Yu
</p>


<h3>References</h3>

<p><a href="https://www.open-mpi.org/">https://www.open-mpi.org/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.cart.rank">mpi.cart.rank</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#Need at least 9 slaves
#mpi.bcast.cmd(mpi.cart.create(1,c(3,3),c(F,T)))
#mpi.cart.create(1,c(3,3),c(F,T))
#mpi.cart.coords(3,4,2)


</code></pre>

<hr>
<h2 id='mpi.cart.create'>MPI_Cart_create</h2><span id='topic+mpi.cart.create'></span>

<h3>Description</h3>

<p><code>mpi.cart.create</code> creates a Cartesian structure of arbitrary dimension.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> mpi.cart.create(commold=1, dims, periods, reorder=FALSE, commcart=3)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.cart.create_+3A_commold">commold</code></td>
<td>
<p>Input communicator</p>
</td></tr>
<tr><td><code id="mpi.cart.create_+3A_dims">dims</code></td>
<td>
<p>Integery array of size ndims specifying the number of processes in 
each dimension</p>
</td></tr>
<tr><td><code id="mpi.cart.create_+3A_periods">periods</code></td>
<td>
<p>Logical array of size ndims specifying whether the grid is periodic
or not in each dimension</p>
</td></tr>
<tr><td><code id="mpi.cart.create_+3A_reorder">reorder</code></td>
<td>
<p>ranks may be reordered or not</p>
</td></tr>
<tr><td><code id="mpi.cart.create_+3A_commcart">commcart</code></td>
<td>
<p>The new communicator to which the Cartesian topology information is attached</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If reorder = false, then the rank of each process in the new group is the same
as its rank in the old group.  If the total size of the Cartesian grid is smaller
than the size of the group of commold, then some processes are returned 
mpi.comm.null.  The call is erroneous if it specifies a grid that is larger than
the group size.
</p>


<h3>Value</h3>

<p><code>mpi.cart.create</code> returns 1 if success and 0 otherwise.
</p>


<h3>Author(s)</h3>

<p>Alek Hunchak and Hao Yu
</p>


<h3>References</h3>

<p><a href="https://www.open-mpi.org/">https://www.open-mpi.org/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#Need at least 9 slaves
#mpi.bcast.cmd(mpi.cart.create(1,c(3,3),c(F,T)))
#mpi.cart.create(1,c(3,3),c(F,T))

</code></pre>

<hr>
<h2 id='mpi.cart.get'>MPI_Cart_get</h2><span id='topic+mpi.cart.get'></span>

<h3>Description</h3>

<p><code>mpi.cart.get</code> provides the user with information on the Cartesian topology
associated with a comm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> mpi.cart.get(comm=3, maxdims)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.cart.get_+3A_comm">comm</code></td>
<td>
<p>Communicator with Cartesian structure</p>
</td></tr>
<tr><td><code id="mpi.cart.get_+3A_maxdims">maxdims</code></td>
<td>
<p>length of vectors dims, periods, and coords in the calling program</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The coords are as given for the rank of the calling process as shown.
</p>


<h3>Value</h3>

<p><code>mpi.cart.get</code> returns a vector containing information on the Cartesian topology 
associated with comm.  maxdims must be at least ndims as returned by <code>mpi.cartdim.get</code>.  
</p>


<h3>Author(s)</h3>

<p>Alek Hunchak and Hao Yu
</p>


<h3>References</h3>

<p><a href="https://www.open-mpi.org/">https://www.open-mpi.org/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.cart.create">mpi.cart.create</a>,<a href="#topic+mpi.cartdim.get">mpi.cartdim.get</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#Need at least 9 slaves
#mpi.bcast.cmd(mpi.cart.create(1,c(3,3),c(F,T)))
#mpi.cart.create(1,c(3,3),c(F,T))
#mpi.remote.exec(mpi.cart.get(3,2))

</code></pre>

<hr>
<h2 id='mpi.cart.rank'>MPI_Cart_rank</h2><span id='topic+mpi.cart.rank'></span>

<h3>Description</h3>

<p><code>mpi.cart.rank</code> translates a Cartesian topology coordinate to its rank.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> mpi.cart.rank(comm=3, coords)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.cart.rank_+3A_comm">comm</code></td>
<td>
<p>Communicator with Cartesian structure</p>
</td></tr>
<tr><td><code id="mpi.cart.rank_+3A_coords">coords</code></td>
<td>
<p>Specifies the Cartesian coordinates of a process</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For a process group with a Cartesian topology, this function translates the logical
process coordinates to process ranks as they are used by the point-to-point routines.
It is the inverse map of <code>mpi.cart.coords</code>. </p>


<h3>Value</h3>

<p><code>mpi.cart.rank</code> returns the rank of the specified process.
</p>


<h3>Author(s)</h3>

<p>Alek Hunchak and Hao Yu
</p>


<h3>References</h3>

<p><a href="https://www.open-mpi.org/">https://www.open-mpi.org/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.cart.coords">mpi.cart.coords</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#Need at least 9 slaves
#mpi.bcast.cmd(mpi.cart.create(1,c(3,3),c(F,T)))
#mpi.cart.create(1,c(3,3),c(F,T))
#mpi.cart.rank(3,c(1,0))

</code></pre>

<hr>
<h2 id='mpi.cart.shift'>MPI_Cart_shift</h2><span id='topic+mpi.cart.shift'></span>

<h3>Description</h3>

<p><code>mpi.cart.shift</code> shifts the Cartesian topology in both manners, displacement
and direction.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> mpi.cart.shift(comm=3, direction, disp)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.cart.shift_+3A_comm">comm</code></td>
<td>
<p>Communicator with Cartesian structure</p>
</td></tr>
<tr><td><code id="mpi.cart.shift_+3A_direction">direction</code></td>
<td>
<p>Coordinate dimension of the shift</p>
</td></tr>
<tr><td><code id="mpi.cart.shift_+3A_disp">disp</code></td>
<td>
<p>displacement (&gt;0 for upwards or left shift, &lt;0 for downwards or right shift)</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>mpi.cart.shift</code> provides neighbor ranks from given direction and displacement.
The direction argument indicates the dimension of the shift. direction=1 means the first dim, 
direction=2 means the second dim, etc. disp=1 or -1 provides immediate neighbor ranks and disp=2 
or -2 provides neighbor's neighbor ranks. Negative ranks mean out of boundary. They correspond to 
<code>mpi.proc.null</code>.  
</p>


<h3>Value</h3>

<p><code>mpi.cart.shift</code> returns a vector containing information regarding the 
rank of the source process and rank of the destination process.
</p>


<h3>Author(s)</h3>

<p>Alek Hunchak and Hao Yu
</p>


<h3>References</h3>

<p><a href="https://www.open-mpi.org/">https://www.open-mpi.org/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.cart.create">mpi.cart.create</a></code>,<code><a href="#topic+mpi.proc.null">mpi.proc.null</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#Need at least 9 slaves
#mpi.bcast.cmd(mpi.cart.create(1,c(3,3),c(F,T)))
#mpi.cart.create(1,c(3,3),c(F,T))
#mpi.remote.exec(mpi.cart.shift(3,2,1))#get neighbor ranks
#mpi.remote.exec(mpi.cart.shift(3,1,1))

</code></pre>

<hr>
<h2 id='mpi.cartdim.get'>MPI_Cartdim_get</h2><span id='topic+mpi.cartdim.get'></span>

<h3>Description</h3>

<p><code>mpi.cartdim.get</code> gets dim information about a Cartesian topology. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'> mpi.cartdim.get(comm=3)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.cartdim.get_+3A_comm">comm</code></td>
<td>
<p>Communicator with Cartesian structure</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Can be used to provide other functions with the correct size of arrays.
</p>


<h3>Value</h3>

<p><code>mpi.cartdim.get</code> returns the number of dimensions of the Cartesian structure
</p>


<h3>Author(s)</h3>

<p>Alek Hunchak and Hao Yu
</p>


<h3>References</h3>

<p><a href="https://www.open-mpi.org/">https://www.open-mpi.org/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.cart.get">mpi.cart.get</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#Need at least 9 slaves
#mpi.bcast.cmd(mpi.cart.create(1,c(3,3),c(F,T)))
#mpi.cart.create(1,c(3,3),c(F,T))
#mpi.cartdim.get(comm=3)

</code></pre>

<hr>
<h2 id='mpi.comm.disconnect'>MPI_Comm_disconnect API</h2><span id='topic+mpi.comm.disconnect'></span>

<h3>Description</h3>

<p><code>mpi.comm.disconnect</code> disconnects itself from a communicator and then 
deallocates the communicator so it points to MPI_COMM_NULL.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mpi.comm.disconnect(comm=1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.comm.disconnect_+3A_comm">comm</code></td>
<td>
<p>a communicator number</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When members associated with a communicator finish jobs or exit, they have to 
call <code>mpi.comm.disconnect</code> to release resource if the communicator was 
created from an intercommunicator by <code><a href="#topic+mpi.intercomm.merge">mpi.intercomm.merge</a></code>. If 
<code><a href="#topic+mpi.comm.free">mpi.comm.free</a></code> is used instead, <code><a href="#topic+mpi.finalize">mpi.finalize</a></code> called 
by slaves may cause undefined impacts on master who wishes to stay.
</p>


<h3>Value</h3>

<p>1 if success. Otherwise 0. 
</p>


<h3>Author(s)</h3>

<p>Hao Yu
</p>


<h3>References</h3>

<p><a href="https://www.open-mpi.org/">https://www.open-mpi.org/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.comm.free">mpi.comm.free</a></code>
</p>

<hr>
<h2 id='mpi.comm.free'>MPI_Comm_free API</h2><span id='topic+mpi.comm.free'></span>

<h3>Description</h3>

<p><code>mpi.comm.free</code>  deallocates a communicator so it 
points to MPI_COMM_NULL.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  mpi.comm.free(comm=1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.comm.free_+3A_comm">comm</code></td>
<td>
<p>a communicator number</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When members associated with a communicator finish jobs or exit, they have to 
call <code>mpi.comm.free</code> to release resource so <code><a href="#topic+mpi.comm.size">mpi.comm.size</a></code> 
will return 0. If the comm  was created from an intercommunicator by 
<code><a href="#topic+mpi.intercomm.merge">mpi.intercomm.merge</a></code>, use <code><a href="#topic+mpi.comm.disconnect">mpi.comm.disconnect</a></code> instead.
</p>


<h3>Value</h3>

<p>1 if success. Otherwise 0. 
</p>


<h3>Author(s)</h3>

<p>Hao Yu
</p>


<h3>References</h3>

<p><a href="https://www.open-mpi.org/">https://www.open-mpi.org/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.comm.disconnect">mpi.comm.disconnect</a></code>
</p>

<hr>
<h2 id='mpi.comm.get.parent'>MPI_Comm_get_parent, MPI_Comm_remote_size, MPI_Comm_test_inter 
APIs</h2><span id='topic+mpi.comm.get.parent'></span><span id='topic+mpi.comm.remote.size'></span><span id='topic+mpi.comm.test.inter'></span>

<h3>Description</h3>

<p><code>mpi.comm.get.parent</code> is mainly used by slaves to find the 
intercommunicator or the parent who spawns them. The intercommunicator is saved 
in the specified comm number.
</p>
<p><code>mpi.comm.remote.size</code> is mainly used by master to find the total number of 
slaves spawned.
</p>
<p><code>mpi.comm.test.inter</code> tests if a comm is an intercomm or not.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  mpi.comm.get.parent(comm = 2)
  mpi.comm.remote.size(comm = 2)
  mpi.comm.test.inter(comm = 2)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.comm.get.parent_+3A_comm">comm</code></td>
<td>
<p>an intercommunicator number.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>mpi.comm.get.parent</code> and <code>mpi.comm.test.inter</code> return  1 if success 
and 0 otherwise. 
</p>
<p><code>mpi.comm.remote.size</code> returns the total number of members in the remote 
group in an intercomm. 
</p>


<h3>Author(s)</h3>

<p>Hao Yu
</p>


<h3>References</h3>

<p><a href="https://www.open-mpi.org/">https://www.open-mpi.org/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.intercomm.merge">mpi.intercomm.merge</a></code>
</p>

<hr>
<h2 id='mpi.comm.set.errhandler'>MPI_Comm_set_errhandler API</h2><span id='topic+mpi.comm.set.errhandler'></span>

<h3>Description</h3>

<p><code>mpi.comm.set.errhandler</code> sets a communicator to MPI_ERRORS_RETURN 
instead of  
MPI_ERRORS_ARE_FATAL (default) which crashes R on any type of MPI errors.  
Almost all MPI API calls return errcodes which can map to specific MPI error 
messages. All MPI related error messages come from predefined 
MPI_Error_string. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mpi.comm.set.errhandler(comm = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.comm.set.errhandler_+3A_comm">comm</code></td>
<td>
<p>a communicator number</p>
</td></tr>
</table>


<h3>Value</h3>

<p>1 if success. Otherwise 0. 
</p>


<h3>Author(s)</h3>

<p>Hao Yu
</p>


<h3>References</h3>

<p><a href="https://www.open-mpi.org/">https://www.open-mpi.org/</a>
</p>

<hr>
<h2 id='mpi.comm.size'>MPI_Comm_c2f, MPI_Comm_dup, MPI_Comm_rank, and MPI_Comm_size APIs</h2><span id='topic+mpi.comm.c2f'></span><span id='topic+mpi.comm.dup'></span><span id='topic+mpi.comm.rank'></span><span id='topic+mpi.comm.size'></span>

<h3>Description</h3>

<p><code>mpi.comm.c2f</code> converts the comm (a C communicator) and returns an integer that can be
used as the communicator in external FORTRAN code. <code>mpi.comm.dup</code> duplicates 
(copies) a comm to a new comm. <code>mpi.comm.rank</code> 
returns its rank in a comm. <code>mpi.comm.size</code> returns 
the total number of members in a comm. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  mpi.comm.c2f(comm=1)
  mpi.comm.dup(comm, newcomm)
  mpi.comm.rank(comm = 1)
  mpi.comm.size(comm = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.comm.size_+3A_comm">comm</code></td>
<td>
<p>a communicator number</p>
</td></tr>
<tr><td><code id="mpi.comm.size_+3A_newcomm">newcomm</code></td>
<td>
<p>a new communicator number</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hao Yu
</p>


<h3>References</h3>

<p><a href="https://www.open-mpi.org/">https://www.open-mpi.org/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#Assume that there are some slaves running
#mpi.comm.size(comm=1)
#mpi.comm.size(comm=0)

#mpi.remote.exec(mpi.comm.rank(comm=1))
#mpi.remote.exec(mpi.comm.rank(comm=0))

#mpi.remote.exec(mpi.comm.size(comm=1))
#mpi.remote.exec(mpi.comm.size(comm=0))

#mpi.bcast.cmd(mpi.comm.dup(comm=1,newcomm=5))
#mpi.comm.dup(comm=1,newcomm=5)

</code></pre>

<hr>
<h2 id='mpi.comm.spawn'>MPI_Comm_spawn API </h2><span id='topic+mpi.comm.spawn'></span>

<h3>Description</h3>

<p><code>mpi.comm.spawn</code> tries to start <code>nslaves</code> identical copies of 
<code>slaves</code>, establishing communication with them and returning an 
intercommunicator. The spawned slaves are referred to as children, and the 
process that spawned them is called the parent (master). The children have 
their own MPI_COMM_WORLD represented by comm 0. To make communication 
possible among master and slaves, all slaves should use 
<code><a href="#topic+mpi.comm.get.parent">mpi.comm.get.parent</a></code> to find their parent and use 
<code><a href="#topic+mpi.intercomm.merge">mpi.intercomm.merge</a></code> to merger an intercomm to a comm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> mpi.comm.spawn(slave, slavearg = character(0),
                nslaves = mpi.universe.size(), info = 0,
                root = 0, intercomm = 2, quiet = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.comm.spawn_+3A_slave">slave</code></td>
<td>
<p>a file name to an executable program.</p>
</td></tr>
<tr><td><code id="mpi.comm.spawn_+3A_slavearg">slavearg</code></td>
<td>
<p>an argument list (a char vector) to slave.</p>
</td></tr>
<tr><td><code id="mpi.comm.spawn_+3A_nslaves">nslaves</code></td>
<td>
<p>number of slaves to be spawned.</p>
</td></tr>
<tr><td><code id="mpi.comm.spawn_+3A_info">info</code></td>
<td>
<p>an info number.</p>
</td></tr>
<tr><td><code id="mpi.comm.spawn_+3A_root">root</code></td>
<td>
<p>the root member who spawns slaves.</p>
</td></tr>
<tr><td><code id="mpi.comm.spawn_+3A_intercomm">intercomm</code></td>
<td>
<p>an intercomm number.</p>
</td></tr>
<tr><td><code id="mpi.comm.spawn_+3A_quiet">quiet</code></td>
<td>
<p>a logical. If TRUE, do not print anything unless an error occurs.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Unless <code>quiet = TRUE</code>, a message is printed to indicate how many slaves are successfully 
spawned and how many failed.
</p>


<h3>Author(s)</h3>

<p>Hao Yu
</p>


<h3>References</h3>

<p><a href="https://www.open-mpi.org/">https://www.open-mpi.org/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.comm.get.parent">mpi.comm.get.parent</a></code>,
<code><a href="#topic+mpi.intercomm.merge">mpi.intercomm.merge</a></code>.
</p>

<hr>
<h2 id='mpi.dims.create'>MPI_Dims_create</h2><span id='topic+mpi.dims.create'></span>

<h3>Description</h3>

<p><code>mpi.dims.create</code> Create a Cartesian dimension used by <code>mpi.cart.create</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> mpi.dims.create(nnodes, ndims, dims=integer(ndims))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.dims.create_+3A_nnodes">nnodes</code></td>
<td>
<p>Number of nodes in a cluster</p>
</td></tr>
<tr><td><code id="mpi.dims.create_+3A_ndims">ndims</code></td>
<td>
<p>Number of dimension in a Cartesian topology</p>
</td></tr>
<tr><td><code id="mpi.dims.create_+3A_dims">dims</code></td>
<td>
<p>Initial dimension numbers</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The entries in the return value are set to describe a Cartesian grid with 
<code>ndims</code> dimensions and a total of <code>nnodes</code> nodes. The dimensions are set 
to be as close to each other as possible, using an appropriate divisibility 
algorithm. The return value can be constrained by specifying positive number(s) in 
<code>dims</code>. Only those 0 values in <code>dims</code> are modified by 
<code>mpi.dims.create</code>.</p>


<h3>Value</h3>

<p><code>mpi.dims.create</code> returns the dimension vector used by 
that in <code>mpi.cart.create</code>.
</p>


<h3>Author(s)</h3>

<p>Hao Yu
</p>


<h3>References</h3>

<p><a href="https://www.open-mpi.org/">https://www.open-mpi.org/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.cart.create">mpi.cart.create</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#What is the dim numbers of 2 dim Cartersian topology under a grid of 36 nodes
#mpi.dims.create(36,2)	#return c(6,6)

#Constrained dim numbers
#mpi.dims.create(12,2,c(0,4)) #return c(9,4)

</code></pre>

<hr>
<h2 id='mpi.exit'>Exit MPI Environment </h2><span id='topic+mpi.exit'></span><span id='topic+mpi.quit'></span>

<h3>Description</h3>

<p><code>mpi.exit</code> terminates MPI execution environment and detaches the 
library Rmpi. After that, you can still work on R.
</p>
<p><code>mpi.quit</code> terminates MPI execution environment and quits R.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mpi.exit()
mpi.quit(save = "no")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.exit_+3A_save">save</code></td>
<td>
<p>the same argument as <code>quit</code> but default to &quot;no&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Normally, <code><a href="#topic+mpi.finalize">mpi.finalize</a></code> is used to clean all MPI states. 
However, it will not detach the library Rmpi. To be more safe leaving MPI,  
<code>mpi.exit</code> not only calls <code>mpi.finalize</code> but also detaches the 
library Rmpi. This will make reload the library Rmpi impossible.
</p>
<p>If leaving MPI and R altogether, one simply uses <code>mpi.quit</code>. 
</p>


<h3>Value</h3>

<p><code>mpi.exit</code> always returns 1
</p>


<h3>Author(s)</h3>

<p>Hao Yu
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.finalize">mpi.finalize</a></code>
</p>

<hr>
<h2 id='mpi.finalize'>MPI_Finalize API</h2><span id='topic+mpi.finalize'></span>

<h3>Description</h3>

<p>Terminates MPI execution environment. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  mpi.finalize()
</code></pre>


<h3>Arguments</h3>

<p>None
</p>


<h3>Details</h3>

<p>This routines must be called by each slave (master) before it exits. This 
call cleans all MPI state. Once <code>mpi.finalize</code> has been called, no MPI 
routine may be called. To be more safe leaving MPI, please use 
<code><a href="#topic+mpi.exit">mpi.exit</a></code> which not only calls <code>mpi.finalize</code> but also 
detaches the library Rmpi. This will make reload the library Rmpi impossible. 
</p>


<h3>Value</h3>

<p>Always return 1
</p>


<h3>Author(s)</h3>

<p>Hao Yu
</p>


<h3>References</h3>

<p><a href="https://www.open-mpi.org/">https://www.open-mpi.org/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.exit">mpi.exit</a></code>
</p>

<hr>
<h2 id='mpi.gather'>MPI_Gather, MPI_Gatherv, MPI_Allgather, and MPI_Allgatherv APIs</h2><span id='topic+mpi.gather'></span><span id='topic+mpi.gatherv'></span><span id='topic+mpi.allgather'></span><span id='topic+mpi.allgatherv'></span>

<h3>Description</h3>

<p><code>mpi.gather</code> and <code>mpi.gatherv</code> (vector variant) gather each 
member's message to the member specified by the argument <code>root</code>.
The root member receives the messages and stores them in rank 
order. <code>mpi.allgather</code> and <code>mpi.allgatherv</code> are the same as
<code>mpi.gather</code> and <code>mpi.gatherv</code> except that all members receive
the result instead of just the root.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mpi.gather(x, type, rdata, root = 0, comm = 1) 
mpi.gatherv(x, type, rdata, rcounts, root = 0, comm = 1) 

mpi.allgather(x, type, rdata, comm = 1) 
mpi.allgatherv(x, type, rdata, rcounts, comm = 1) 
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.gather_+3A_x">x</code></td>
<td>
<p>data to be gathered. Must be the same type.</p>
</td></tr>
<tr><td><code id="mpi.gather_+3A_type">type</code></td>
<td>
<p>1 for integer, 2 for double, and 3 for character. Others are not 
supported.</p>
</td></tr>
<tr><td><code id="mpi.gather_+3A_rdata">rdata</code></td>
<td>
<p>the receive buffer. Must be the same type as the sender and big 
enough to include all message gathered.</p>
</td></tr>
<tr><td><code id="mpi.gather_+3A_rcounts">rcounts</code></td>
<td>
<p>int vector specifying the length of each message.</p>
</td></tr>
<tr><td><code id="mpi.gather_+3A_root">root</code></td>
<td>
<p>rank of the receiver</p>
</td></tr>
<tr><td><code id="mpi.gather_+3A_comm">comm</code></td>
<td>
<p>a communicator number</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code>mpi.gather</code> and <code>mpi.allgather</code>, the message to be gathered 
must be the same dim and the same type. The receive buffer can be prepared as 
either integer(size * dim) or double(size * dim), where size is the total 
number of members in a comm. For <code>mpi.gatherv</code> and <code>mpi.allgatherv</code>, 
the message to be gathered can have different dims but must be the same type. 
The argument <code>rcounts</code> records these different dims into an integer vector 
in rank order. Then the receive buffer can be prepared as either 
integer(sum(rcounts)) or double(sum(rcounts)).  
</p>


<h3>Value</h3>

<p>For <code>mpi.gather</code> or <code>mpi.gatherv</code>, it returns the gathered 
message for the root member. For other members, it returns what is in rdata, 
i.e., rdata (or rcounts) is ignored. For <code>mpi.allgather</code> or 
<code>mpi.allgatherv</code>, it returns the gathered message for all members. 
</p>


<h3>Author(s)</h3>

<p>Hao Yu
</p>


<h3>References</h3>

<p><a href="https://www.open-mpi.org/">https://www.open-mpi.org/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.scatter">mpi.scatter</a></code>, <code><a href="#topic+mpi.scatterv">mpi.scatterv</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#Need 3 slaves to run properly
#Or use mpi.spawn.Rslaves(nslaves=3)
#mpi.bcast.cmd(id &lt;-mpi.comm.rank(.comm), comm=1)
#mpi.bcast.cmd(mpi.gather(letters[id],type=3,rdata=string(1)))

#mpi.gather(letters[10],type=3,rdata=string(4))

# mpi.bcast.cmd(x&lt;-rnorm(id))
# mpi.bcast.cmd(mpi.gatherv(x,type=2,rdata=double(1),rcounts=1))
# mpi.gatherv(double(1),type=2,rdata=double(sum(1:3)+1),rcounts=c(1,1:3))

#mpi.bcast.cmd(out1&lt;-mpi.allgatherv(x,type=2,rdata=double(sum(1:3)+1),
#		rcounts=c(1,1:3)))
#mpi.allgatherv(double(1),type=2,rdata=double(sum(1:3)+1),rcounts=c(1,1:3))

</code></pre>

<hr>
<h2 id='mpi.gather.Robj'>Extentions of MPI_Gather and MPI_Allgather APIs</h2><span id='topic+mpi.gather.Robj'></span><span id='topic+mpi.allgather.Robj'></span>

<h3>Description</h3>

<p><code>mpi.gather.Robj</code> gathers each member's object to the member 
specified by the argument <code>root</code>.
The root member receives the objects as a list.
<code>mpi.allgather.Robj</code> is the same as <code>mpi.gather.Robj</code>
except that all members receive the result instead of just the root.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mpi.gather.Robj(obj=NULL, root = 0, comm = 1, ...)

mpi.allgather.Robj(obj=NULL, comm = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.gather.Robj_+3A_obj">obj</code></td>
<td>
<p>data to be gathered. Could be different type.</p>
</td></tr>
<tr><td><code id="mpi.gather.Robj_+3A_root">root</code></td>
<td>
<p>rank of the gather</p>
</td></tr>
<tr><td><code id="mpi.gather.Robj_+3A_comm">comm</code></td>
<td>
<p>a communicator number</p>
</td></tr>
<tr><td><code id="mpi.gather.Robj_+3A_...">...</code></td>
<td>
<p>optional arugments to <code>sapply</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Since sapply is used to gather all results, its default option 
&quot;simplify=TRUE&quot; is to simplify outputs. In some situations, this option 
is not desirable. Using &quot;simplify=FALSE&quot; as in the place of ... will tell
sapply not to simplify and a list of outputs will be returned.
</p>


<h3>Value</h3>

<p>For <code>mpi.gather.Robj</code>, it returns a list, the gathered message 
for the root member. For
<code>mpi.allgatherv.Robj</code>, it returns a list, the gathered message 
for all members.
</p>


<h3>Author(s)</h3>

<p>Hao Yu and Wei Xia
</p>


<h3>References</h3>

<p><a href="https://www.open-mpi.org/">https://www.open-mpi.org/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.gather">mpi.gather</a></code>, <code><a href="#topic+mpi.allgatherv">mpi.allgatherv</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#Assume that there are some slaves running
#mpi.bcast.cmd(id&lt;-mpi.comm.rank())
#mpi.bcast.cmd(x&lt;-rnorm(id))
#mpi.bcast.cmd(mpi.gather.Robj(x))
#x&lt;-"test mpi.gather.Robj"
#mpi.gather.Robj(x)

#mpi.bcast.cmd(obj&lt;-rnorm(id+10))
#mpi.bcast.cmd(nn&lt;-mpi.allgather.Robj(obj))
#obj&lt;-rnorm(5)
#mpi.allgather.Robj(obj)
#mpi.remote.exec(nn)

</code></pre>

<hr>
<h2 id='mpi.get.count'>MPI_Get_count API</h2><span id='topic+mpi.get.count'></span>

<h3>Description</h3>

<p><code>mpi.get.count</code> finds the length of a received message.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mpi.get.count(type, status = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.get.count_+3A_type">type</code></td>
<td>
<p>1 for integer, 2 for double, 3 for char.</p>
</td></tr>
<tr><td><code id="mpi.get.count_+3A_status">status</code></td>
<td>
<p>a status number</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code><a href="#topic+mpi.recv">mpi.recv</a></code> is used to receive a message, the receiver 
buffer can be set to be bigger than the incoming message. To find the 
exact length of the received message, <code>mpi.get.count</code> is used to 
find its exact length. <code>mpi.get.count</code> must be called 
immediately after calling <code>mpi.recv</code> otherwise the status may be 
changed.
</p>


<h3>Value</h3>

<p>the length of a received message.
</p>


<h3>Author(s)</h3>

<p>Hao Yu
</p>


<h3>References</h3>

<p><a href="https://www.open-mpi.org/">https://www.open-mpi.org/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.send">mpi.send</a></code>, <code><a href="#topic+mpi.recv">mpi.recv</a></code>,
<code><a href="#topic+mpi.get.sourcetag">mpi.get.sourcetag</a></code>, <code><a href="#topic+mpi.probe">mpi.probe</a></code>.
</p>

<hr>
<h2 id='mpi.get.processor.name'>MPI_Get_processor_name API</h2><span id='topic+mpi.get.processor.name'></span>

<h3>Description</h3>

<p><code>mpi.get.processor.name</code> returns the host name (a string) where 
it is executed. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  mpi.get.processor.name(short = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.get.processor.name_+3A_short">short</code></td>
<td>
<p>a logical.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a base host name if <code>short = TRUE</code> and a full host name otherwise.
</p>


<h3>Author(s)</h3>

<p>Hao Yu
</p>


<h3>References</h3>

<p><a href="https://www.open-mpi.org/">https://www.open-mpi.org/</a>
</p>

<hr>
<h2 id='mpi.get.sourcetag'>Utility for finding the source and tag of a received message</h2><span id='topic+mpi.get.sourcetag'></span>

<h3>Description</h3>

<p><code>mpi.get.sourcetag</code> finds the source and tag of a received message.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  mpi.get.sourcetag(status = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.get.sourcetag_+3A_status">status</code></td>
<td>
<p>a status number</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code><a href="#topic+mpi.any.source">mpi.any.source</a></code> and/or <code><a href="#topic+mpi.any.tag">mpi.any.tag</a></code> are 
used by <code><a href="#topic+mpi.recv">mpi.recv</a></code> or <code><a href="#topic+mpi.probe">mpi.probe</a></code>, one can use 
<code>mpi.get.sourcetag</code> 
to find who sends the message or with what a tag number. 
<code><a href="#topic+mpi.get.sourcetag">mpi.get.sourcetag</a></code> must be called immediately after calling 
<code>mpi.recv</code> or <code>mpi.probe</code> otherwise the obtained information may not 
be right.
</p>


<h3>Value</h3>

<p>2 dim int vector. The first integer is the source and the second is the 
tag.
</p>


<h3>Author(s)</h3>

<p>Hao Yu
</p>


<h3>References</h3>

<p><a href="https://www.open-mpi.org/">https://www.open-mpi.org/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.send">mpi.send</a></code>, <code><a href="#topic+mpi.recv">mpi.recv</a></code>, <code><a href="#topic+mpi.probe">mpi.probe</a></code>,
<code><a href="#topic+mpi.get.count">mpi.get.count</a></code>
</p>

<hr>
<h2 id='mpi.iapplyLB'>(Load balancing) parallel apply with nonblocking features</h2><span id='topic+mpi.iapplyLB'></span><span id='topic+mpi.iparApply'></span><span id='topic+mpi.iparLapply'></span><span id='topic+mpi.iparSapply'></span><span id='topic+mpi.iparRapply'></span><span id='topic+mpi.iparCapply'></span><span id='topic+mpi.iparReplicate'></span><span id='topic+mpi.iparMM'></span>

<h3>Description</h3>

<p>(Load balancing) parallel<code>lapply</code> and related functions. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mpi.iapplyLB(X, FUN, ..., apply.seq=NULL, comm=1, sleep=0.01)
mpi.iparApply(X, MARGIN, FUN, ..., job.num = mpi.comm.size(comm)-1,
                    apply.seq=NULL, comm=1, sleep=0.01)
mpi.iparLapply(X, FUN, ..., job.num=mpi.comm.size(comm)-1, apply.seq=NULL, 
		    comm=1,sleep=0.01)  
mpi.iparSapply(X, FUN, ..., job.num=mpi.comm.size(comm)-1, apply.seq=NULL, 
		simplify=TRUE, USE.NAMES = TRUE, comm=1, sleep=0.01)  
mpi.iparRapply(X, FUN, ..., job.num=mpi.comm.size(comm)-1, apply.seq=NULL, 
		comm=1, sleep=0.01)  
mpi.iparCapply(X, FUN, ..., job.num=mpi.comm.size(comm)-1, apply.seq=NULL, 
		comm=1,sleep=0.01)  
mpi.iparReplicate(n, expr, job.num=mpi.comm.size(comm)-1, apply.seq=NULL, 
		simplify = TRUE, comm=1,sleep=0.01)
mpi.iparMM(A, B, comm=1, sleep=0.01)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.iapplyLB_+3A_x">X</code></td>
<td>
<p>an array or matrix.</p>
</td></tr>
<tr><td><code id="mpi.iapplyLB_+3A_margin">MARGIN</code></td>
<td>
<p>vector specifying the dimensions to use.</p>
</td></tr>
<tr><td><code id="mpi.iapplyLB_+3A_fun">FUN</code></td>
<td>
<p>a function.</p>
</td></tr>
<tr><td><code id="mpi.iapplyLB_+3A_simplify">simplify</code></td>
<td>
<p>logical; should the result be simplified to a vector or
matrix if possible?</p>
</td></tr>
<tr><td><code id="mpi.iapplyLB_+3A_use.names">USE.NAMES</code></td>
<td>
<p>logical; if <code>TRUE</code> and if <code>X</code> is character, use <code>X</code> as
<code>names</code> for the result unless it had names already.</p>
</td></tr>
<tr><td><code id="mpi.iapplyLB_+3A_n">n</code></td>
<td>
<p>number of replications.</p>
</td></tr>
<tr><td><code id="mpi.iapplyLB_+3A_a">A</code></td>
<td>
<p>a matrix</p>
</td></tr>
<tr><td><code id="mpi.iapplyLB_+3A_b">B</code></td>
<td>
<p>a matrix</p>
</td></tr>
<tr><td><code id="mpi.iapplyLB_+3A_expr">expr</code></td>
<td>
<p>expression to evaluate repeatedly.</p>
</td></tr>
<tr><td><code id="mpi.iapplyLB_+3A_job.num">job.num</code></td>
<td>
<p>Total job numbers. If job numbers is bigger than total slave numbers (default 
value), a load balancing approach is used.</p>
</td></tr>
<tr><td><code id="mpi.iapplyLB_+3A_apply.seq">apply.seq</code></td>
<td>
<p>if reproducing the same computation (simulation) is desirable, set it 
to the integer vector .mpi.applyLB generated in previous computation (simulation).</p>
</td></tr>
<tr><td><code id="mpi.iapplyLB_+3A_...">...</code></td>
<td>
<p>optional arguments to <code>Fun</code></p>
</td></tr>
<tr><td><code id="mpi.iapplyLB_+3A_comm">comm</code></td>
<td>
<p>a communicator number</p>
</td></tr>
<tr><td><code id="mpi.iapplyLB_+3A_sleep">sleep</code></td>
<td>
<p>a sleep interval on master node (in sec)</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>mpi.iparApply</code>, <code>mpi.iparLapply</code>, <code>mpi.iparSapply</code>, <code>mpi.iparRapply</code>, 
<code>mpi.iparCapply</code>, <code>mpi.iparSapply</code>, <code>mi.iparReplicate</code>, and <code>mpi.iparMM</code>
are nonblocking versions of <code>mpi.parApply</code>, <code>mpi.parLapply</code>, <code>mpi.parSapply</code>, 
<code>mpi.parRapply</code>, <code>mpi.parCapply</code>, <code>mpi.parSapply</code>, <code>mpi.parReplicate</code>, 
and <code>mpi.parMM</code> respectively. The main difference is that <code>mpi.iprobe</code> and 
<code>Sys.sleep</code> are used so that master node consumes almost no CPU cycles while waiting for 
slaves results. However, due to frequent wake/sleep cycles on master, those functions are not 
suitable for running small jobs on slave nodes. If anticipated computing time for each job is 
relatively long, e.g., minutes or hours, setting sleep to be 1 second or longer will further 
reduce load on master (only slightly).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.iapply">mpi.iapply</a></code>
</p>

<hr>
<h2 id='mpi.info.create'>MPI_Info_create, MPI_Info_free, MPI_Info_get, MPI_Info_set APIs</h2><span id='topic+mpi.info.create'></span><span id='topic+mpi.info.free'></span><span id='topic+mpi.info.get'></span><span id='topic+mpi.info.set'></span>

<h3>Description</h3>

<p>Many MPI APIs take an info argument for additional information passing. An info 
is an object which consists of many (key,value) pairs. Rmpi uses an internal 
memory to store an info object.
</p>
<p><code>mpi.info.create</code> creates a new info object.
</p>
<p><code>mpi.info.free</code> frees an info object and sets it to MPI_INFO_NULL.
</p>
<p><code>mpi.info.get</code>  retrieves the value associated with key in an info.
</p>
<p><code>mpi.info.set</code> adds the key and value pair to info.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  mpi.info.create(info = 0)
  mpi.info.free(info = 0)
  mpi.info.get(info = 0, key, valuelen)
  mpi.info.set(info = 0, key, value)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.info.create_+3A_info">info</code></td>
<td>
<p>an info number.</p>
</td></tr>
<tr><td><code id="mpi.info.create_+3A_key">key</code></td>
<td>
<p>a char (length 1).</p>
</td></tr>
<tr><td><code id="mpi.info.create_+3A_valuelen">valuelen</code></td>
<td>
<p>the length (nchar) of a key</p>
</td></tr>
<tr><td><code id="mpi.info.create_+3A_value">value</code></td>
<td>
<p>a char (length 1).</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>mpi.info.create</code>, <code>mpi.info.free</code>, and <code>mpi.info.set</code>  return 
1 if success and 0 otherwise.
</p>
<p><code>mpi.info.get</code> returns the value (a char) for a given info and valuelen.
</p>


<h3>Author(s)</h3>

<p>Hao Yu
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.spawn.Rslaves">mpi.spawn.Rslaves</a></code>
</p>

<hr>
<h2 id='mpi.intercomm.merge'>MPI_Intercomm_merge API</h2><span id='topic+mpi.intercomm.merge'></span>

<h3>Description</h3>

<p>Creates an intracommunicator from an intercommunicator 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mpi.intercomm.merge(intercomm=2, high=0, comm=1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.intercomm.merge_+3A_intercomm">intercomm</code></td>
<td>
<p>an intercommunicator number</p>
</td></tr>
<tr><td><code id="mpi.intercomm.merge_+3A_high">high</code></td>
<td>
<p>Used to order the groups of the two intracommunicators within comm 
when creating the new communicator</p>
</td></tr>
<tr><td><code id="mpi.intercomm.merge_+3A_comm">comm</code></td>
<td>
<p>a (intra)communicator number</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When master spawns slaves, an intercommunicator is created. To make 
communications (point-to-point or groupwise) among master and slaves, an 
intracommunicator must be created. <code>mpi.intercomm.merge</code> is used for 
that purpose. This is a collective call so all master and slaves call 
together. R slaves spawned by <code><a href="#topic+mpi.spawn.Rslaves">mpi.spawn.Rslaves</a></code> should use
<code><a href="#topic+mpi.comm.get.parent">mpi.comm.get.parent</a></code> to get (set) an intercomm to a number followed 
by merging antercomm to an intracomm. One 
can use <code><a href="#topic+mpi.comm.test.inter">mpi.comm.test.inter</a></code> to test if a 
communicator is an intercommunicator or not.
</p>


<h3>Value</h3>

<p>1 if success. Otherwise 0. 
</p>


<h3>Author(s)</h3>

<p>Hao Yu
</p>


<h3>References</h3>

<p><a href="https://www.open-mpi.org/">https://www.open-mpi.org/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.comm.test.inter">mpi.comm.test.inter</a></code>
</p>

<hr>
<h2 id='mpi.parSim'>Parallel Monte Carlo Simulation</h2><span id='topic+mpi.parSim'></span>

<h3>Description</h3>

<p>Carry out parallel Monte Carlo simulation on R slaves spawned by 
using slavedaemon.R script and all executed results are returned back to 
master. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mpi.parSim(n=100, rand.gen=rnorm, rand.arg=NULL,statistic, 
nsim=100, run=1, slaveinfo=FALSE, sim.seq=NULL, simplify=TRUE, comm=1, ...)  
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.parSim_+3A_n">n</code></td>
<td>
<p>sample size.</p>
</td></tr>
<tr><td><code id="mpi.parSim_+3A_rand.gen">rand.gen</code></td>
<td>
<p>the random data generating function. See the details 
section</p>
</td></tr>
<tr><td><code id="mpi.parSim_+3A_rand.arg">rand.arg</code></td>
<td>
<p>additional argument list to <code>rand.gen</code>.</p>
</td></tr>
<tr><td><code id="mpi.parSim_+3A_statistic">statistic</code></td>
<td>
<p>the statistic function to be simulated. See the 
details section</p>
</td></tr>
<tr><td><code id="mpi.parSim_+3A_nsim">nsim</code></td>
<td>
<p>the number of simulation carried on a slave which is 
counted as one slave job.</p>
</td></tr>
<tr><td><code id="mpi.parSim_+3A_run">run</code></td>
<td>
<p>the number of looping. See the details section.</p>
</td></tr>
<tr><td><code id="mpi.parSim_+3A_slaveinfo">slaveinfo</code></td>
<td>
<p>if TRUE, the numbers of jobs finished by slaves 
will be displayed.</p>
</td></tr>
<tr><td><code id="mpi.parSim_+3A_sim.seq">sim.seq</code></td>
<td>
<p>if reproducing the same simulation is desirable, set it 
to the integer vector .mpi.parSim generated in previous simulation.</p>
</td></tr>
<tr><td><code id="mpi.parSim_+3A_simplify">simplify</code></td>
<td>
<p>logical; should the result be simplified to a vector or
matrix if possible?</p>
</td></tr>
<tr><td><code id="mpi.parSim_+3A_comm">comm</code></td>
<td>
<p>a communicator number</p>
</td></tr>
<tr><td><code id="mpi.parSim_+3A_...">...</code></td>
<td>
<p>optional arguments to <code>statistic</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>It is assumed that one simulation is carried out as 
<code>statistic(rand.gen(n))</code>, where <code>rand.gen(n)</code> can return any 
values as long as <code>statistic</code> can take them. Additional arguments can 
be passed to <code>rand.gen</code> by <code>rand.arg</code> as a list. Optional 
arguments can also be passed to <code>statistic</code> by the argument 
<code>...</code>. 
</p>
<p>Each slave job consists of <code>replicate(nsim,statistic(rand.gen(n)))</code>, 
i.e., each job runs <code>nsim</code> number of simulation. The returned values 
are transported from slaves to master.
</p>
<p>The total number of simulation (TNS) is calculated as follows. Let 
slave.num be the total number of slaves in a <code>comm</code> and it is 
<code>mpi.comm.size(comm)-1</code>. Then TNS=slave.num*nsim*run and the total
number of slave jobs is slave.num*run, where <code>run</code> is the number of 
looping from master perspective. If run=1, each slave will run one slave 
job. If run=2, each slave will run two slaves jobs on average, and so on. 
</p>
<p>The purpose of using <code>run</code> has two folds. It allows a tuneup 
of slave job size and total number of slave jobs to deal with two 
different cluster environments. On a cluster of slaves with equal CPU 
power, <code>run=1</code> is often enough. But if <code>nsim</code> is too big, one
can set <code>run=2</code> and the slave jog size to be <code>nsim/2</code> so that  
TNS=slave.num*(nsim/2)*(2*run). This may improve R computation 
efficiency slightly. On a cluster of slaves with different CPU power, one 
can choose a big value of <code>run</code> and a small value of <code>nsim</code> 
so that master can dispatch more jobs to slaves who run faster than 
others. This will keep all slaves busy so that load balancing is 
achieved. 
</p>
<p>The sequence of slaves who deliver results to master are saved into 
<code>.mpi.parSim</code>. It keeps track which part of results done by which slaves. 
<code>.mpi.parSim</code> can be used to reproduce the same simulation result if the same
seed is used and the argument <code>sim.seq</code> is equal to <code>.mpi.parSim</code>.
</p>
<p>See the warning section before you use <code>mpi.parSim</code>.
</p>


<h3>Value</h3>

<p>The returned values depend on values returned by <code><a href="base.html#topic+replicate">replicate</a></code>
of <code>statistic(rand.gen(n))</code> and the total number of simulation 
(TNS). If <code>statistic</code> returns a single value, then the result is a 
vector of length TNS. If <code>statistic</code> returns a vector (list) of 
length <code>nrow</code>, then the result is a matrix of dimension 
<code>c(nrow, TNS)</code>.   
</p>


<h3>Warning</h3>

<p>It is assumed that a parallel RNG is used on all slaves. Run
<code>mpi.setup.rngstream</code> on the master to set up a parallel RNG. Though <code>mpi.parSim</code> 
works without a parallel RNG, the quality of simulation is not guarantied. 
</p>
<p><code>mpi.parSim</code> will automatically transfer <code>rand.gen</code> 
and <code>statistic</code> to slaves. However, any functions that 
<code>rand.gen</code> and <code>statistic</code> reply on but are not on slaves 
must be transfered to slaves before using <code>mpi.parSim</code>. You 
can use <code><a href="#topic+mpi.bcast.Robj2slave">mpi.bcast.Robj2slave</a></code> for that purpose. The same is 
applied to required packages or C/Fortran codes. You can use either 
<code><a href="#topic+mpi.bcast.cmd">mpi.bcast.cmd</a></code> or put <code>required(package)</code> and/or 
<code>dyn.load(so.lib)</code> into <code>rand.gen</code> and <code>statistic</code>.
</p>
<p>If <code>simplify</code> is TRUE, sapply style simplication is applied. Otherwise a list of length 
slave.num*run is returned.</p>


<h3>Author(s)</h3>

<p>Hao Yu
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.setup.rngstream">mpi.setup.rngstream</a></code>
<code><a href="#topic+mpi.bcast.cmd">mpi.bcast.cmd</a></code>
<code><a href="#topic+mpi.bcast.Robj2slave">mpi.bcast.Robj2slave</a></code>
</p>

<hr>
<h2 id='mpi.probe'>MPI_Probe and MPI_Iprobe APIs</h2><span id='topic+mpi.probe'></span><span id='topic+mpi.iprobe'></span>

<h3>Description</h3>

<p><code>mpi.probe</code> uses the source and tag of incoming message to set a 
status. <code>mpi.iprobe</code> does the same except it is a nonblocking call, 
i.e., returns immediately.</p>


<h3>Usage</h3>

<pre><code class='language-R'>mpi.probe(source, tag, comm = 1, status = 0)
mpi.iprobe(source, tag, comm = 1, status = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.probe_+3A_source">source</code></td>
<td>
<p>the source of incoming message or mpi.any.source() for any 
source.</p>
</td></tr>
<tr><td><code id="mpi.probe_+3A_tag">tag</code></td>
<td>
<p>a tag number or mpi.any.tag() for any tag.</p>
</td></tr>
<tr><td><code id="mpi.probe_+3A_comm">comm</code></td>
<td>
<p>a communicator number</p>
</td></tr>
<tr><td><code id="mpi.probe_+3A_status">status</code></td>
<td>
<p>a status number</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code><a href="#topic+mpi.send">mpi.send</a></code> or other nonblocking sends are used to send 
a message, the receiver may not know the exact length before receiving 
it. <code>mpi.probe</code> is used to probe the incoming message and put some 
information into a status. Then the exact length can be found by using 
<code><a href="#topic+mpi.get.count">mpi.get.count</a></code> to such a status. If the wild card 
<code>mpi.any.source</code> or <code>mpi.any.tag</code> are used, then one 
can use <code><a href="#topic+mpi.get.sourcetag">mpi.get.sourcetag</a></code> to find the exact source or tag of 
a sender. </p>


<h3>Value</h3>

<p><code>mpi.probe</code> returns 1 only after a matching message has been found.
</p>
<p><code>mpi.iproble</code> returns TRUE if there is a message that can be 
received; FALSE otherwise.
</p>


<h3>Author(s)</h3>

<p>Hao Yu
</p>


<h3>References</h3>

<p><a href="https://www.open-mpi.org/">https://www.open-mpi.org/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.send">mpi.send</a></code>, <code><a href="#topic+mpi.recv">mpi.recv</a></code>,
<code><a href="#topic+mpi.get.count">mpi.get.count</a></code>
</p>

<hr>
<h2 id='mpi.realloc'>Find and increase the lengthes of MPI opaques comm, request, and 
status</h2><span id='topic+mpi.comm.maxsize'></span><span id='topic+mpi.request.maxsize'></span><span id='topic+mpi.status.maxsize'></span><span id='topic+mpi.realloc.comm'></span><span id='topic+mpi.realloc.request'></span><span id='topic+mpi.realloc.status'></span>

<h3>Description</h3>

<p><code>mpi.comm.maxsize</code>, <code>mpi.request.maxsize</code>, and 
<code>mpi.status.maxsize</code> find the lengthes of comm, request, and status 
arrayes respectively.
</p>
<p><code>mpi.realloc.comm</code>, <code>mpi.realloc.request</code> and 
<code>mpi.realloc.status</code> increase the lengthes of comm, request and 
status arrayes to <code>newmaxsize</code> respectively if <code>newmaxsize</code> is 
bigger than the original maximum size. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mpi.realloc.comm(newmaxsize)
mpi.realloc.request(newmaxsize)
mpi.realloc.status(newmaxsize)
mpi.comm.maxsize()
mpi.request.maxsize()
mpi.status.maxsize()
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.realloc_+3A_newmaxsize">newmaxsize</code></td>
<td>
<p>an integer.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <span class="pkg">Rmpi</span> is loaded, Rmpi allocs comm array with size 10, request 
array with 10,000 and status array with 5,000. They should be enough in 
most cases. They use less than 150KB system memory. In rare case, one can 
use <code>mpi.realloc.comm</code>, <code>mpi.realloc.request</code> and
<code>mpi.realloc.status</code> to increase them to bigger arrays.</p>


<h3>Author(s)</h3>

<p>Hao Yu
</p>


<h3>References</h3>

<p><a href="https://www.open-mpi.org/">https://www.open-mpi.org/</a>
</p>

<hr>
<h2 id='mpi.reduce'>MPI_Reduce and MPI_Allreduce APIs</h2><span id='topic+mpi.allreduce'></span><span id='topic+mpi.reduce'></span>

<h3>Description</h3>

<p><code>mpi.reduce</code> and <code>mpi.allreduce</code> are global reduction operations. 
<code>mpi.reduce</code> combines each member's result, using the operation 
<code>op</code>, and returns the combined value(s) to the member specified by 
the argument <code>dest</code>. <code>mpi.allreduce</code> is the same as 
<code>mpi.reduce</code> except that all members receive the combined value(s).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mpi.reduce(x, type=2, op=c("sum","prod","max","min","maxloc","minloc"), 
	dest = 0, comm = 1) 

mpi.allreduce(x, type=2, op=c("sum","prod","max","min","maxloc","minloc"), 
	comm = 1) 
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.reduce_+3A_x">x</code></td>
<td>
<p>data to be reduced. Must be the same dim and the same type for all 
members.</p>
</td></tr>
<tr><td><code id="mpi.reduce_+3A_type">type</code></td>
<td>
<p>1 for integer and 2 for double. Others are not supported.</p>
</td></tr>
<tr><td><code id="mpi.reduce_+3A_op">op</code></td>
<td>
<p>one of &quot;sum&quot;, &quot;prod&quot;, &quot;max&quot;, &quot;min&quot;, &quot;maxloc&quot;, or &quot;minloc&quot;.</p>
</td></tr> 
<tr><td><code id="mpi.reduce_+3A_dest">dest</code></td>
<td>
<p>rank of destination</p>
</td></tr>
<tr><td><code id="mpi.reduce_+3A_comm">comm</code></td>
<td>
<p>a communicator number</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It is important that all members in a comm call either all <code>mpi.reduce</code> 
or all <code>mpi.allreduce</code> even though the master may not be in 
computation. They must provide exactly the same type and dim 
vectors to be reduced. If the operation &quot;maxloc&quot; or &quot;minloc&quot; is used, 
the combined vector is twice as long as the original one since the 
maximum or minimum ranks are included. 
</p>


<h3>Value</h3>

<p><code>mpi.reduce</code> returns the combined value(s) to the member specified  
by <code>dest</code>. <code>mpi.allreduce</code> returns the combined values(s) to 
every member in a comm. The combined value(s) may be the summation, 
production, maximum, or minimum specified by the argument <code>op</code>. If 
the <code>op</code> is either &quot;maxloc&quot; or &quot;minloc&quot;, then the maximum (minimum) 
value(s) along the maximum (minimum) rank(s) will be returned.
</p>


<h3>Author(s)</h3>

<p>Hao Yu
</p>


<h3>References</h3>

<p><a href="https://www.open-mpi.org/">https://www.open-mpi.org/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.gather">mpi.gather</a></code>.
</p>

<hr>
<h2 id='mpi.remote.exec'>Remote Executions on R slaves</h2><span id='topic+mpi.remote.exec'></span>

<h3>Description</h3>

<p>Remotely execute a command on R slaves spawned by using slavedaemon.R script 
and return all executed results back to master. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mpi.remote.exec(cmd, ..., simplify = TRUE, comm = 1, ret = TRUE)  
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.remote.exec_+3A_cmd">cmd</code></td>
<td>
<p>the command to be executed on R slaves</p>
</td></tr>
<tr><td><code id="mpi.remote.exec_+3A_...">...</code></td>
<td>
<p>used as arguments to cmd (function command) for passing their 
(master) values to R slaves, i.e., if &lsquo;myfun(x)&rsquo; will be executed on R slaves 
with &lsquo;x&rsquo; as master variable, use mpi.remote.exec(cmd=myfun, x).</p>
</td></tr>
<tr><td><code id="mpi.remote.exec_+3A_simplify">simplify</code></td>
<td>
<p>logical; should the result be simplified to a data.frame if possible?</p>
</td></tr>
<tr><td><code id="mpi.remote.exec_+3A_comm">comm</code></td>
<td>
<p>a communicator number.</p>
</td></tr>
<tr><td><code id="mpi.remote.exec_+3A_ret">ret</code></td>
<td>
<p>return executed results from R slaves if TRUE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Once R slaves are spawned by <code><a href="#topic+mpi.spawn.Rslaves">mpi.spawn.Rslaves</a></code> with the 
slavedaemon.R script, they are waiting for instructions from master. One can 
use <code><a href="#topic+mpi.bcast.cmd">mpi.bcast.cmd</a></code> to send a command to R slaves. However it 
will not return executed results. Hence <code>mpi.remote.exec</code> can be 
considered an extension to <code><a href="#topic+mpi.bcast.cmd">mpi.bcast.cmd</a></code>.    
</p>


<h3>Value</h3>

<p>return executed results from R slaves if the argument <code>ret</code> is 
set to be TRUE. The value could be a data.frame if values 
(integer or double) from each slave have the same dimension. 
Otherwise a list is returned.</p>


<h3>Warning</h3>

<p><code>mpi.remote.exec</code> may have difficult guessing invisible results 
on R slaves. Use <code>ret = FALSE</code> instead. 
</p>


<h3>Author(s)</h3>

<p>Hao Yu
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.spawn.Rslaves">mpi.spawn.Rslaves</a></code>,
<code><a href="#topic+mpi.bcast.cmd">mpi.bcast.cmd</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#mpi.remote.exec(mpi.comm.rank())
# x=5
#mpi.remote.exec(rnorm,x)

</code></pre>

<hr>
<h2 id='mpi.scatter'>MPI_Scatter and MPI_Scatterv APIs</h2><span id='topic+mpi.scatter'></span><span id='topic+mpi.scatterv'></span>

<h3>Description</h3>

<p><code>mpi.scatter</code> and <code>mpi.scatterv</code> are the inverse operations of 
<code><a href="#topic+mpi.gather">mpi.gather</a></code> and <code><a href="#topic+mpi.gatherv">mpi.gatherv</a></code> respectively.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mpi.scatter(x, type, rdata, root = 0,  comm = 1) 
mpi.scatterv(x, scounts, type, rdata, root = 0, comm = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.scatter_+3A_x">x</code></td>
<td>
<p>data to be scattered.</p>
</td></tr>
<tr><td><code id="mpi.scatter_+3A_type">type</code></td>
<td>
<p>1 for integer, 2 for double, and 3 for character. Others are not 
supported.</p>
</td></tr>
<tr><td><code id="mpi.scatter_+3A_rdata">rdata</code></td>
<td>
<p>the receive buffer. Must be the same type as the sender</p>
</td></tr> 
<tr><td><code id="mpi.scatter_+3A_scounts">scounts</code></td>
<td>
<p>int vector specifying the block length inside a message to be 
scattered to other members.</p>
</td></tr>
<tr><td><code id="mpi.scatter_+3A_root">root</code></td>
<td>
<p>rank of the receiver</p>
</td></tr>
<tr><td><code id="mpi.scatter_+3A_comm">comm</code></td>
<td>
<p>a communicator number</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>mpi.scatter</code> scatters the message x to all members. Each member receives 
a portion of x with dim as length(x)/size in rank order, where size is the 
total number of members in a comm. So the receive buffer can be prepared as 
either integer(length(x)/size) or double(length(x)/size). For 
<code>mpi.scatterv</code>, scounts counts the portions (different dims) of x sent to 
each member. Each member needs to prepare the receive buffer as either 
integer(scounts[i]) or double(scounts[i]). 
</p>


<h3>Value</h3>

<p>For non-root members, <code>mpi.scatter</code> or <code>scatterv</code> returns the
scattered message and ignores whatever is in x (or scounts). For the root 
member, it returns the portion belonging to itself.
</p>


<h3>Author(s)</h3>

<p>Hao Yu
</p>


<h3>References</h3>

<p><a href="https://www.open-mpi.org/">https://www.open-mpi.org/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.gather">mpi.gather</a></code>, <code><a href="#topic+mpi.gatherv">mpi.gatherv</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#Need 3 slaves to run properly
#Or run  mpi.spawn.Rslaves(nslaves=3)
#  num="123456789abcd"
#  scounts&lt;-c(2,3,1,7)
#  mpi.bcast.cmd(strnum&lt;-mpi.scatter(integer(1),type=1,rdata=integer(1),root=0))
#  strnum&lt;-mpi.scatter(scounts,type=1,rdata=integer(1),root=0)
#  mpi.bcast.cmd(ans &lt;- mpi.scatterv(string(1),scounts=0,type=3,rdata=string(strnum),
#					root=0))
#  mpi.scatterv(as.character(num),scounts=scounts,type=3,rdata=string(strnum),root=0)
#  mpi.remote.exec(ans)

</code></pre>

<hr>
<h2 id='mpi.scatter.Robj'>Extensions of MPI_ SCATTER and MPI_SCATTERV </h2><span id='topic+mpi.scatter.Robj'></span><span id='topic+mpi.scatter.Robj2slave'></span>

<h3>Description</h3>

<p><code>mpi.scatter.Robj</code> and <code>mpi.scatter.Robj2slave</code> are used to scatter a list 
to all members. They are more efficient than using any parallel apply functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mpi.scatter.Robj(obj = NULL, root = 0, comm = 1)
mpi.scatter.Robj2slave(obj, comm = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.scatter.Robj_+3A_obj">obj</code></td>
<td>
<p>a list object to be scattered from the root or master</p>
</td></tr>
<tr><td><code id="mpi.scatter.Robj_+3A_root">root</code></td>
<td>
<p>rank of the scatter.</p>
</td></tr>
<tr><td><code id="mpi.scatter.Robj_+3A_comm">comm</code></td>
<td>
<p>a communicator number.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>mpi.scatter.Robj</code> is an extension of <code><a href="#topic+mpi.scatter">mpi.scatter</a></code> for
scattering a list object from a sender (root) to everyone. <code>mpi.scatter.Robj2slave</code>
scatters a list to all slaves.
</p>


<h3>Value</h3>

<p><code>mpi.scatter.Robj</code> for non-root members,  returns the
scattered R object. For the root  member, it returns the
portion belonging to itself. <code>mpi.scatter.Robj2slave</code> returns no value
for the master and all slaves get their corresponding components in the list,
i.e., the first slave gets the first component in the list. 
</p>


<h3>Author(s)</h3>

<p>Hao Yu  and   Wei Xia
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.scatter">mpi.scatter</a></code>,
<code><a href="#topic+mpi.gather.Robj">mpi.gather.Robj</a></code>,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#assume that there are three slaves running
#mpi.bcast.cmd(x&lt;-mpi.scatter.Robj())

#xx &lt;- list("master",rnorm(3),letters[2],1:10)
#mpi.scatter.Robj(obj=xx)

#mpi.remote.exec(x)

#scatter a matrix to slaves
#dat=matrix(1:24,ncol=3)
#splitmatrix = function(x, ncl) lapply(.splitIndices(nrow(x), ncl), function(i) x[i,])
#dat2=splitmatrix(dat,3)
#mpi.scatter.Robj2slave(dat2)
#mpi.remote.exec(dat2)

</code></pre>

<hr>
<h2 id='mpi.send'>MPI_Send, MPI_Isend, MPI_Recv, and MPI_Irecv APIs</h2><span id='topic+mpi.send'></span><span id='topic+mpi.isend'></span><span id='topic+mpi.recv'></span><span id='topic+mpi.irecv'></span>

<h3>Description</h3>

<p>The pair <code>mpi.send</code> and <code>mpi.recv</code> are two most used blocking 
calls for point-to-point communications. An int, double or char vector 
can be transmitted from any source to any destination.  
</p>
<p>The pair <code>mpi.isend</code> and <code>mpi.irecv</code> are the same except that 
they are nonblocking calls.
</p>
<p>Blocking and nonblocking calls are interchangeable, e.g., nonblocking 
sends can be matched with blocking receives, and vice-versa.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mpi.send(x, type, dest, tag,  comm = 1)
mpi.isend(x, type, dest, tag,  comm = 1, request=0)
mpi.recv(x, type, source, tag,  comm = 1, status = 0)
mpi.irecv(x, type, source, tag,  comm = 1, request = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.send_+3A_x">x</code></td>
<td>
<p>data to be sent or received. Must be the same 
type for source and destination. The receive buffer must be as large as 
the send buffer.</p>
</td></tr>
<tr><td><code id="mpi.send_+3A_type">type</code></td>
<td>
<p>1 for integer, 2 for double, and 3 for 
character. Others are not supported.</p>
</td></tr>
<tr><td><code id="mpi.send_+3A_dest">dest</code></td>
<td>
<p>the destination rank. Use <code>mpi.proc.null</code> for a 
fake destination.</p>
</td></tr>
<tr><td><code id="mpi.send_+3A_source">source</code></td>
<td>
<p>the source rank. Use <code>mpi.any.source</code> for any source. 
Use <code>mpi.proc.null</code> for a fake source. </p>
</td></tr>
<tr><td><code id="mpi.send_+3A_tag">tag</code></td>
<td>
<p>non-negative integer. Use <code>mpi.any.tag</code> for any tag flag.</p>
</td></tr>
<tr><td><code id="mpi.send_+3A_comm">comm</code></td>
<td>
<p>a communicator number.</p>
</td></tr>
<tr><td><code id="mpi.send_+3A_request">request</code></td>
<td>
<p>a request number.</p>
</td></tr>
<tr><td><code id="mpi.send_+3A_status">status</code></td>
<td>
<p>a status number.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The pair <code>mpi.send</code> (or <code>mpi.isend</code>) and <code>mpi.recv</code> 
(or <code>mpi.irecv</code>) must be used together, i.e., if there is a sender, 
then there must be a receiver. Any mismatch will result a deadlock 
situation, i.e., programs stop responding. The receive buffer must be 
large enough to contain an incoming message otherwise programs will be 
crashed. One can use <code><a href="#topic+mpi.probe">mpi.probe</a></code> (or <a href="#topic+mpi.iprobe">mpi.iprobe</a>) and 
<code><a href="#topic+mpi.get.count">mpi.get.count</a></code> to find the length of an incoming message 
before calling <code>mpi.recv</code>. If <code><a href="#topic+mpi.any.source">mpi.any.source</a></code> or  
<code><a href="#topic+mpi.any.tag">mpi.any.tag</a></code> is used in <code>mpi.recv</code>, one can use  
<code><a href="#topic+mpi.get.sourcetag">mpi.get.sourcetag</a></code> to find out the source or tag of the  
received message. To send/receive an R object rather than an int, double  
or char vector, please use the pair <code><a href="#topic+mpi.send.Robj">mpi.send.Robj</a></code> and  
<code><a href="#topic+mpi.recv.Robj">mpi.recv.Robj</a></code>.
</p>
<p>Since <code>mpi.irecv</code> is a nonblocking call, <code>x</code> with enough buffer 
must be created before using it. Then use nonblocking completion calls 
such as <code><a href="#topic+mpi.wait">mpi.wait</a></code> or <code><a href="#topic+mpi.test">mpi.test</a></code> to test if 
<code>x</code> contains data from sender.
</p>
<p>If multiple nonblocking sends or receives are used, please use request 
number consecutively from 0. For example, to receive two messages from two 
slaves, try 
mpi.irecv(x,1,source=1,tag=0,comm=1,request=0)
mpi.irecv(y,1,source=2,tag=0,comm=1,request=1)
Then <code>mpi.waitany</code>, <code>mpi.waitsome</code> or <code>mpi.waitall</code> can be 
used to complete the operations.
</p>


<h3>Value</h3>

<p><code>mpi.send</code> and <code>mpi.isend</code> return no value. <code>mpi.recv</code> 
returns the int, double or char vector sent from <code>source</code>. However, 
<code>mpi.irecv</code> returns no value. See details for explanation.
</p>


<h3>Author(s)</h3>

<p>Hao Yu
</p>


<h3>References</h3>

<p><a href="https://www.open-mpi.org/">https://www.open-mpi.org/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.send.Robj">mpi.send.Robj</a></code>,
<code><a href="#topic+mpi.recv.Robj">mpi.recv.Robj</a></code>,
<code><a href="#topic+mpi.probe">mpi.probe</a></code>,  
<code><a href="#topic+mpi.wait">mpi.wait</a></code>,  
<code><a href="#topic+mpi.get.count">mpi.get.count</a></code>, 
<code><a href="#topic+mpi.get.sourcetag">mpi.get.sourcetag</a></code>. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 

#on a slave
#mpi.send(1:10,1,0,0)

#on master
#x &lt;- integer(10)
#mpi.irecv(x,1,1,0)
#x	
#mpi.wait()
#x

</code></pre>

<hr>
<h2 id='mpi.send.Robj'>Extensions of MPI_Send and MPI_Recv APIs</h2><span id='topic+mpi.send.Robj'></span><span id='topic+mpi.isend.Robj'></span><span id='topic+mpi.recv.Robj'></span>

<h3>Description</h3>

<p><code>mpi.send.Robj</code> and <code>mpi.recv.Robj</code> are two 
extensions of <code>mpi.send</code> and <code>mpi.recv</code>. They are used to 
transmit a general R object from any source to any destination.  
</p>
<p><code>mpi.isend.Robj</code> is a nonblocking version of <code>mpi.send.Robj</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mpi.send.Robj(obj, dest, tag, comm = 1)
mpi.isend.Robj(obj, dest, tag, comm = 1, request=0)
mpi.recv.Robj(source, tag, comm = 1, status = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.send.Robj_+3A_obj">obj</code></td>
<td>
<p>an R object. Can be any R object. </p>
</td></tr>
<tr><td><code id="mpi.send.Robj_+3A_dest">dest</code></td>
<td>
<p>the destination rank.</p>
</td></tr>
<tr><td><code id="mpi.send.Robj_+3A_source">source</code></td>
<td>
<p>the source rank or mpi.any.source() for any source.</p>
</td></tr> 
<tr><td><code id="mpi.send.Robj_+3A_tag">tag</code></td>
<td>
<p>non-negative integer or mpi.any.tag() for any tag.</p>
</td></tr>
<tr><td><code id="mpi.send.Robj_+3A_comm">comm</code></td>
<td>
<p>a communicator number.</p>
</td></tr>
<tr><td><code id="mpi.send.Robj_+3A_request">request</code></td>
<td>
<p>a request number.</p>
</td></tr>
<tr><td><code id="mpi.send.Robj_+3A_status">status</code></td>
<td>
<p>a status number.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>mpi.send.Robj</code> and <code>mpi.isend.Robj</code> use 
<code>serialize</code> to encode  an R object into a binary 
char vector. It sends the message to the destination. The receiver 
decode the message back into an R object by using  
<code>unserialize</code>. 
</p>
<p>If <code>mpi.isend.Robj</code> is used, <code>mpi.wait</code> or <code>mpi.test</code> must 
be used to check the object has been sent.
</p>


<h3>Value</h3>

<p><code>mpi.send.Robj</code> or <code>mpi.isend.Robj</code> return no value. 
<code>mpi.recv.Robj</code> returns the the transmitted R object. 
</p>


<h3>Author(s)</h3>

<p>Hao Yu
</p>


<h3>References</h3>

<p><a href="https://www.open-mpi.org/">https://www.open-mpi.org/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.send">mpi.send</a></code>,
<code><a href="#topic+mpi.recv">mpi.recv</a></code>,
<code><a href="#topic+mpi.wait">mpi.wait</a></code>,
<code><a href="base.html#topic+serialize">serialize</a></code>,
<code><a href="base.html#topic+unserialize">unserialize</a></code>, 
</p>

<hr>
<h2 id='mpi.sendrecv'>MPI_Sendrecv and MPI_Sendrecv_replace APIs</h2><span id='topic+mpi.sendrecv'></span><span id='topic+mpi.sendrecv.replace'></span>

<h3>Description</h3>

<p><code>mpi.sendrecv</code> and <code>mpi.sendrecv.replace</code> execute blocking send 
and receive operations. Both of them combine the sending of one message to 
a destination and the receiving of another message from a source in one 
call. The source and destination are possibly the same. The send buffer and 
receive buffer are disjoint for <code>mpi.sendrecv</code>, while the buffers are 
not disjoint for <code>mpi.sendrecv.replace</code>.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mpi.sendrecv(senddata, sendtype, dest, sendtag, recvdata, recvtype, 
source, recvtag, comm = 1, status = 0)

mpi.sendrecv.replace(x, type, dest, sendtag, source, recvtag, 
comm = 1, status = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.sendrecv_+3A_x">x</code></td>
<td>
<p>data to be sent or recieved. Must be the same 
type for source and destination.</p>
</td></tr>
<tr><td><code id="mpi.sendrecv_+3A_senddata">senddata</code></td>
<td>
<p>data to be sent. May have different datatypes and 
lengths</p>
</td></tr> 
<tr><td><code id="mpi.sendrecv_+3A_recvdata">recvdata</code></td>
<td>
<p>data to be recieved. May have different datatypes and 
lengths</p>
</td></tr> 
<tr><td><code id="mpi.sendrecv_+3A_type">type</code></td>
<td>
<p>type of the data to be sent or recieved. 
1 for integer, 2 for double, and 3 for character. Others are not supported.</p>
</td></tr>
<tr><td><code id="mpi.sendrecv_+3A_sendtype">sendtype</code></td>
<td>
<p>type of the data to be sent. 
1 for integer, 2 for double, and 3 for character. Others are not supported.</p>
</td></tr>
<tr><td><code id="mpi.sendrecv_+3A_recvtype">recvtype</code></td>
<td>
<p>type of the data to be recieved. 
1 for integer, 2 for double, and 3 for character. Others are not supported.</p>
</td></tr>
<tr><td><code id="mpi.sendrecv_+3A_dest">dest</code></td>
<td>
<p>the destination rank. Use <code>mpi.proc.null</code> for a 
fake destination.</p>
</td></tr>
<tr><td><code id="mpi.sendrecv_+3A_source">source</code></td>
<td>
<p>the source rank. Use <code>mpi.any.source</code> for any source. 
Use <code>mpi.proc.null</code> for a fake source. </p>
</td></tr>
<tr><td><code id="mpi.sendrecv_+3A_sendtag">sendtag</code></td>
<td>
<p>non-negative integer. Use <code>mpi.any.tag</code> 
for any tag flag.</p>
</td></tr>
<tr><td><code id="mpi.sendrecv_+3A_recvtag">recvtag</code></td>
<td>
<p>non-negative integer. Use <code>mpi.any.tag</code> 
for any tag flag.</p>
</td></tr>
<tr><td><code id="mpi.sendrecv_+3A_comm">comm</code></td>
<td>
<p>a communicator number.</p>
</td></tr>
<tr><td><code id="mpi.sendrecv_+3A_status">status</code></td>
<td>
<p>a status number.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The receive buffer must be large enough to contain an incoming message 
otherwise programs will be crashed. There is compatibility between 
send-receive and normal sends and receives. A message sent by a 
send-receive can be received  by a regular receive and a send-receive 
can receive a message sent by a regular send.
</p>


<h3>Value</h3>

<p>Returns the int, double or char vector sent from the send buffers. 
</p>


<h3>Author(s)</h3>

<p>Kris Chen
</p>


<h3>References</h3>

<p><a href="https://www.open-mpi.org/">https://www.open-mpi.org/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.send.Robj">mpi.send.Robj</a></code>,
<code><a href="#topic+mpi.recv.Robj">mpi.recv.Robj</a></code>,
<code><a href="#topic+mpi.probe">mpi.probe</a></code>.  
<code><a href="#topic+mpi.get.sourcetag">mpi.get.sourcetag</a></code>. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#mpi.sendrecv(as.integer(11:20),1,0,33,integer(10),1,0,33,comm=0)
#mpi.sendrecv.replace(seq(1,2,by=0.1),2,0,99,0,99,comm=0)
 
</code></pre>

<hr>
<h2 id='mpi.setup.rngstream'>Setup parallel RNG on all slaves</h2><span id='topic+mpi.setup.rngstream'></span>

<h3>Description</h3>

<p><code>mpi.setup.rngstream</code> setups RNGstream on all slaves. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mpi.setup.rngstream(iseed=NULL, comm = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.setup.rngstream_+3A_iseed">iseed</code></td>
<td>
<p>An integer to be supplied to <code>set.seed</code>, or NULL not to set reproducible seeds.</p>
</td></tr> 
<tr><td><code id="mpi.setup.rngstream_+3A_comm">comm</code></td>
<td>
<p>A comm number.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>mpi.setup.rngstream</code> can be run only on master node. It can be run later on with the same or
different iseed.
</p>


<h3>Value</h3>

<p>No value returned.
</p>


<h3>Author(s)</h3>

<p>Hao Yu
</p>

<hr>
<h2 id='mpi.spawn.Rslaves'>Spawn and Close R Slaves</h2><span id='topic+mpi.spawn.Rslaves'></span><span id='topic+mpi.close.Rslaves'></span><span id='topic+tailslave.log'></span>

<h3>Description</h3>

<p><code>mpi.spawn.Rslaves</code> spawns R slaves to those hosts automatically 
chosen by MPI or specific hosts assigned by the argument <code>hosts</code>.
Those R slaves are running in R BATCH mode with a specific Rscript file. 
The default Rscript file &quot;slavedaemon.R&quot; provides interactive R slave 
environments.
</p>
<p><code>mpi.close.Rslaves</code> shuts down R slaves spawned by 
<code>mpi.spawn.Rslaves</code>.
</p>
<p><code>tailslave.log</code> view (from tail) R slave log files (assuming they are all 
in one working directory).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mpi.spawn.Rslaves(Rscript=system.file("slavedaemon.R", package="Rmpi"),
        nslaves=mpi.universe.size(), root = 0, intercomm = 2,
        comm = 1, hosts = NULL, needlog = TRUE, mapdrive=TRUE, quiet = FALSE, 
		nonblock=TRUE, sleep=0.1)

mpi.close.Rslaves(dellog = TRUE, comm = 1)
tailslave.log(nlines = 3, comm = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.spawn.Rslaves_+3A_rscript">Rscript</code></td>
<td>
<p>an R script file used to run R in BATCH mode.</p>
</td></tr>
<tr><td><code id="mpi.spawn.Rslaves_+3A_nslaves">nslaves</code></td>
<td>
<p>number of slaves to be spawned.</p>
</td></tr>
<tr><td><code id="mpi.spawn.Rslaves_+3A_root">root</code></td>
<td>
<p>the rank number of the member who spawns R slaves.</p>
</td></tr>
<tr><td><code id="mpi.spawn.Rslaves_+3A_intercomm">intercomm</code></td>
<td>
<p>an intercommunicator number</p>
</td></tr>
<tr><td><code id="mpi.spawn.Rslaves_+3A_comm">comm</code></td>
<td>
<p>a communicator number merged from an intercomm.</p>
</td></tr>
<tr><td><code id="mpi.spawn.Rslaves_+3A_hosts">hosts</code></td>
<td>
<p>NULL or LAM node numbers to specify where R slaves to be 
spawned.</p>
</td></tr>
<tr><td><code id="mpi.spawn.Rslaves_+3A_needlog">needlog</code></td>
<td>
<p>a logical. If TRUE, R BATCH outputs will be saved in log files.  
If FALSE, the outputs will send to /dev/null.</p>
</td></tr>
<tr><td><code id="mpi.spawn.Rslaves_+3A_mapdrive">mapdrive</code></td>
<td>
<p>a logical. If TRUE and master's working dir is on a network, mapping network drive 
is attemped on remote nodes under windows platform.</p>
</td></tr>
<tr><td><code id="mpi.spawn.Rslaves_+3A_quiet">quiet</code></td>
<td>
<p>a logical. If TRUE, do not print anything unless an error occurs.</p>
</td></tr>
<tr><td><code id="mpi.spawn.Rslaves_+3A_nonblock">nonblock</code></td>
<td>
<p>a logical. If TRUE, a nonblock procedure is used on all slaves so that
they will consume none or little CPUs while waiting.</p>
</td></tr>
<tr><td><code id="mpi.spawn.Rslaves_+3A_sleep">sleep</code></td>
<td>
<p>a sleep interval, used when nonblock=TRUE. Smaller sleep is, more response slaves are, 
more CPUs consume.</p>
</td></tr>
<tr><td><code id="mpi.spawn.Rslaves_+3A_dellog">dellog</code></td>
<td>
<p>a logical specifying if R slave's log files are deleted or 
not.</p>
</td></tr>
<tr><td><code id="mpi.spawn.Rslaves_+3A_nlines">nlines</code></td>
<td>
<p>number of lines to view from tail in R slave's log files.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The R slaves that <code>mpi.spawn.Rslaves</code> spawns are really running a shell 
program which can be found in <code>system.file("Rslaves.sh",package="Rmpi")</code> 
which takes a Rscript file as one of its arguments. Other arguments are 
used to see if a log file (R output) is needed and how to name it. The master 
process id and the comm number, along with host names where R slaves are running  are 
used to name these log files. 
</p>
<p>Once R slaves are successfully spawned, the 
mergers from an intercomm (default &lsquo;intercomm = 2&rsquo;) to a comm (default 
&lsquo;comm = 1&rsquo;) are automatically done on master and slaves (should be done 
if the default Rscript is replaced). If additional sets of R slaves are 
needed, please use &lsquo;comm = 3&rsquo;, &lsquo;comm = 4&rsquo;, etc to spawn them. At most a 
comm number up to 10 can be used. Notice that the default comm number for 
R slaves (using slavedaemon.R) is always 1 which is saved as .comm.  
</p>
<p>To spawn R slaves to specific hosts, please use the argument <code>hosts</code> 
with a list of those node numbers (an integer vector). Total node numbers 
along their host names can be found by using <code><a href="#topic+lamhosts">lamhosts</a></code>.
Notice that this is LAM-MPI specific. 
</p>


<h3>Value</h3>

<p>Unless <code>quiet = TRUE</code>, <code>mpi.spawn.Rslaves</code> prints to stdio how many 
slaves are successfully spawned and where they are running.
</p>
<p><code>mpi.close.Rslaves</code> return 1 if success and 0 otherwise.
</p>
<p><code>tailslave.log</code> returns last lines of R slave's log files.
</p>


<h3>Author(s)</h3>

<p>Hao Yu
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.comm.spawn">mpi.comm.spawn</a></code>,
<code><a href="#topic+lamhosts">lamhosts</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#mpi.spawn.Rslaves(nslaves=2)
#tailslave.log()
#mpi.remote.exec(rnorm(10))
#mpi.close.Rslaves()

</code></pre>

<hr>
<h2 id='mpi.universe.size'>MPI_Universe_size API</h2><span id='topic+mpi.universe.size'></span>

<h3>Description</h3>

<p><code>mpi.universe.size</code> returns the total number of CPUs available in a 
cluster. Some MPI implements may not have this MPI call available.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  mpi.universe.size()
</code></pre>


<h3>Arguments</h3>

<p>None.
</p>


<h3>Author(s)</h3>

<p>Hao Yu
</p>


<h3>References</h3>

<p><a href="https://www.open-mpi.org/">https://www.open-mpi.org/</a>
</p>

<hr>
<h2 id='mpi.wait'>Nonblocking completion operations</h2><span id='topic+mpi.cancel'></span><span id='topic+mpi.test.cancelled'></span><span id='topic+mpi.test'></span><span id='topic+mpi.testall'></span><span id='topic+mpi.testany'></span><span id='topic+mpi.testsome'></span><span id='topic+mpi.wait'></span><span id='topic+mpi.waitall'></span><span id='topic+mpi.waitany'></span><span id='topic+mpi.waitsome'></span>

<h3>Description</h3>

<p><code>mpi.cancel</code> cancels a nonblocking send or receive request.
</p>
<p><code>mpi.test.cancelled</code> tests if <code>mpi.cancel</code> cancels or not.
</p>
<p><code>wait</code>, <code>waitall</code>, <code>waitany</code>, and <code>waitsome</code> are used 
to complete nonblocking send or receive requests. They are not local.
</p>
<p><code>test</code>, <code>testall</code>, <code>testany</code>, and <code>testsome</code> are used 
to complete nonblocking send and receive requests. They are local.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mpi.cancel(request)
mpi.test.cancelled(status=0)
mpi.test(request, status=0)
mpi.testall(count)
mpi.testany(count, status=0)
mpi.testsome(count)
mpi.wait(request, status=0)
mpi.waitall(count)
mpi.waitany(count, status=0)
mpi.waitsome(count)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mpi.wait_+3A_count">count</code></td>
<td>
<p>total number of nonblocking operations.</p>
</td></tr>
<tr><td><code id="mpi.wait_+3A_request">request</code></td>
<td>
<p>a request number.</p>
</td></tr>
<tr><td><code id="mpi.wait_+3A_status">status</code></td>
<td>
<p>a status number.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>mpi.wait</code> and <code>mpi.test</code> are used to complete a nonblocking 
send and receive request: use the same request number by <code>mpi.isend</code> 
or <code>mpi.irecv</code>. Once completed, the associated request is set to 
MPI_REQUEST_NULL and status contains information such as source, tag, 
and length of message.
</p>
<p>If multiple nonblocking sends or receives are initiated, the following 
calls are more efficient. Make sure that request numbers are used 
consecutively as request=0, request=1, request=2, etc. In this way, the 
following calls can find request information in system memory.
</p>
<p><code>mpi.waitany</code> and <code>mpi.testany</code> are used to complete one out of 
several requests. 
</p>
<p><code>mpi.waitall</code> and <code>mpi.testall</code> are used to complete all 
requests. 
</p>
<p><code>mpi.waitsome</code> and <code>mpi.testsome</code> are used to complete all 
enabled requests. 
</p>


<h3>Value</h3>

<p><code>mpi.cancel</code> returns no value.
</p>
<p><code>mpi.test.cancelled</code> returns TRUE if a nonblocking call is cancelled; 
FALSE otherwise.
</p>
<p><code>mpi.wait</code> returns no value. Instead status contains information that 
can be retrieved by <code>mpi.get.count</code> and <code>mpi.get.sourcetag</code>.
</p>
<p><code>mpi.test</code> returns TRUE if a request is complete; FALSE otherwise. If 
TRUE, it is the same as <code>mpi.wait</code>.
</p>
<p><code>mpi.waitany</code> returns which request (index) has been completed. In
addition, status contains information that can be retrieved by
<code>mpi.get.count</code> and <code>mpi.get.sourcetag</code>. 
</p>
<p><code>mpi.testany</code> returns a list: index&mdash; request index; flag&mdash;TRUE if 
a request is complete; FALSE otherwise (index is no use in this case). 
If flag is TRUE, it is the same as <code>mpi.waitany</code>.
</p>
<p><code>mpi.waitall</code> returns no value. Instead statuses 0, 1, ..., count-1 
contain corresponding information that can be retrieved by 
<code>mpi.get.count</code> and <code>mpi.get.sourcetag</code>.
</p>
<p><code>mpi.testall</code> returns TRUE if all requests are complete; FALSE 
otherwise. If TRUE, it is the same as <code>mpi.waitall</code>.
</p>
<p><code>mpi.waitsome</code> returns a list: count&mdash; number of requests that have
been completed; indices&mdash;an integer vector of size count of those 
completed request numbers (in  0, 1 ,..., count-1). In addition, statuses 
0, 1, ..., count-1 contain corresponding information that can be 
retrieved by <code>mpi.get.count</code> and <code>mpi.get.sourcetag</code>.  
</p>
<p><code>mpi.testsome</code> is the same as <code>mpi.waitsome</code> except that count 
may be 0 and in this case indices is no use.
</p>


<h3>Author(s)</h3>

<p>Hao Yu
</p>


<h3>References</h3>

<p><a href="https://www.open-mpi.org/">https://www.open-mpi.org/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.isend">mpi.isend</a></code>,
<code><a href="#topic+mpi.irecv">mpi.irecv</a></code>,
<code><a href="#topic+mpi.get.count">mpi.get.count</a></code>,
<code><a href="#topic+mpi.get.sourcetag">mpi.get.sourcetag</a></code>.
</p>

<hr>
<h2 id='string'>Internal functions</h2><span id='topic+string'></span><span id='topic+mpi.comm.is.null'></span><span id='topic+.docall'></span><span id='topic+.mpi.worker.apply'></span><span id='topic+.mpi.worker.applyLB'></span><span id='topic+.mpi.worker.exec'></span><span id='topic+.mpi.worker.sim'></span><span id='topic+.typeindex'></span><span id='topic+.simplify'></span><span id='topic+.splitIndices'></span><span id='topic+.onUnload'></span><span id='topic+.mpi.undefined'></span><span id='topic+.force.type'></span>

<h3>Description</h3>

<p> Internal and hidden functions used by other MPI functions.
</p>
<p><code>mpi.comm.is.null</code> is used to test if a comm is MPI_COMM_NULL (empty
members).
</p>
<p><code>string</code> create a string (empty space character) buffer.
</p>
<p><code>.docall</code> a wrap to docall function.
</p>
<p><code>.mpi.worker.apply</code> apply like function used by workers.
</p>
<p><code>.mpi.worker.applyLB</code> apply like function used by workers (load balancing).
</p>
<p><code>.mpi.worker.exec</code> real execution by workers when using <a href="#topic+mpi.remote.exec">mpi.remote.exec</a>.
</p>
<p><code>.mpi.worker.sim</code> real simulation by workers when using <a href="#topic+mpi.parSim">mpi.parSim</a>.
</p>
<p><code>.type.index</code> identify input data type: integer, numeric, raw, or others.
</p>
<p><code>.simplify</code> simplify internal objects.
</p>
<p><code>.splitIndices</code> split parall apply jobs evenly.
</p>
<p><code>.onUnload</code> clean MPI when Rmpi is unloaded.
</p>
<p><code>.mpi.undefined</code> undefined mpi object.
</p>
<p><code>.force.type</code> force input data type object specified by type.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mpi.comm.is.null(comm)
string(length)
.docall(fun, args)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="string_+3A_comm">comm</code></td>
<td>
<p>a communicator number.</p>
</td></tr>
<tr><td><code id="string_+3A_length">length</code></td>
<td>
<p>length of a string.</p>
</td></tr>
<tr><td><code id="string_+3A_fun">fun</code></td>
<td>
<p> a function object.</p>
</td></tr>
<tr><td><code id="string_+3A_args">args</code></td>
<td>
<p>arguments to function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>string</code> returns an empty character string.
</p>


<h3>Author(s)</h3>

<p>Hao Yu
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mpi.spawn.Rslaves">mpi.spawn.Rslaves</a></code>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
