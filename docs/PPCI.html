<!DOCTYPE html><html lang="en"><head><title>Help for package PPCI</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {PPCI}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#PPCI-package'>
<p>Projection Pursuit for Cluster Identification</p></a></li>
<li><a href='#add_subtree'><p>Add Nodes To a Plot of a Hierarchical Clustering Model</p></a></li>
<li><a href='#breastcancer'><p>Discrimination of Cancerous and Non-Cancerous Breast Masses</p></a></li>
<li><a href='#cluster_performance'><p>External Cluster Validity Metrics</p></a></li>
<li><a href='#dermatology'><p>Eryhemato-Squamous Disease Identification</p></a></li>
<li><a href='#df_mc'><p>Gradient of the Variance Ratio Clusterability Across a Hyperplane</p></a></li>
<li><a href='#df_md'><p>Gradient of the Integrated Density on a Hyperplane</p></a></li>
<li><a href='#df_ncut'><p>Gradient of the Normalised Cut Across a Hyperplane</p></a></li>
<li><a href='#dkde'><p>Gradient of the Penalised Density at a Point</p></a></li>
<li><a href='#f_mc'><p>Variance Ratio Clusterability Across a Hyperplane</p></a></li>
<li><a href='#f_md'><p>Integrated Density on a Hyperplane</p></a></li>
<li><a href='#f_ncut'><p>Normalised Cut Across a Hyperplane</p></a></li>
<li><a href='#hp_plot'><p>Visualise a Hyperplane Separator for Clustering</p></a></li>
<li><a href='#is_minim'><p>Check if the Current Solution is a Valid Minimum Density Hyperplane</p></a></li>
<li><a href='#mc_b'><p>Location of Optimal Variance Ratio Hyperplane</p></a></li>
<li><a href='#mcdc'><p>Divisive Clustering Using Maximum Clusterability</p></a></li>
<li><a href='#mcdr'><p>Maximum Clusterability Dimension Reduction</p></a></li>
<li><a href='#mch'><p>Maximum Clusteriability Hyperplane</p></a></li>
<li><a href='#mcpp'><p>Maximum Clusterability Projection Pursuit</p></a></li>
<li><a href='#md_b'><p>Location of Minimum Density Hyperplane</p></a></li>
<li><a href='#md_reldepth'><p>Relative Depth of a Hyperplane</p></a></li>
<li><a href='#mddc'><p>Divisive Clustering Using Minimum Density Hyperplanes</p></a></li>
<li><a href='#mddr'><p>Minimum Density Dimension Reduction</p></a></li>
<li><a href='#mdh'><p>Minimum Density Hyperplane</p></a></li>
<li><a href='#mdpp'><p>Minimum Density Projection Pursuit</p></a></li>
<li><a href='#ncut_b'><p>Location of Minimum Normalised Cut Hyperplane</p></a></li>
<li><a href='#ncutdc'><p>Divisive Clustering Using Minimum Normalised Cut Hyperplanes</p></a></li>
<li><a href='#ncutdr'><p>Minimum Normalised Cut Dimension Reduction</p></a></li>
<li><a href='#ncuth'><p>Minimum Normalised Cut Hyperplane</p></a></li>
<li><a href='#ncutpp'><p>Minimum Normalised Cut Projection Pursuit</p></a></li>
<li><a href='#node_plot'><p>Visualise a Node in a Hierarchical Clustering Model</p></a></li>
<li><a href='#norm_vec'><p>Euclidean Norm of a Vector</p></a></li>
<li><a href='#optidigits'><p>Optical Recognition of Handwritten Digits</p></a></li>
<li><a href='#optidigits_mean_images'><p>Visualise Cluster Means from optidigits data set</p></a></li>
<li><a href='#pendigits'><p>Pen-based Recognition of Handwritten Digits</p></a></li>
<li><a href='#plot.ppci_cluster_solution'><p>Visualise a Hierarchical Clustering Model, or a Node Within a Hierarchical Clustering Model</p></a></li>
<li><a href='#plot.ppci_hyperplane_solution'><p>Visualise a Hyperplane Separator for Clustering</p></a></li>
<li><a href='#plot.ppci_projection_solution'><p>Visualise a Data Set Projected from Projection Pursuit</p></a></li>
<li><a href='#ppclust.optim'><p>Optimisation Call for Projection Pursuit Algorithms</p></a></li>
<li><a href='#subtree_width'><p>Determine the Largest Number of Nodes at Any Depth in a Clustering Hierarchy</p></a></li>
<li><a href='#success_ratio'><p>Evaluate External Valifity os a Binary Partition</p></a></li>
<li><a href='#tree_plot'><p>Visualise a Hierarchical Clustering Model</p></a></li>
<li><a href='#tree_prune'><p>Prune a Hierarchical Clustering Model</p></a></li>
<li><a href='#tree_split'><p>Split a Leaf in a Hierarchical Clustering Model</p></a></li>
<li><a href='#yale'><p>Face Recognition</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Projection Pursuit for Cluster Identification</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.5</td>
</tr>
<tr>
<td>Author:</td>
<td>David Hofmeyr &lt;dhofmeyr@sun.ac.za&gt; [aut, cre]
	Nicos Pavlidis &lt;n.pavlidis@lancaster.ac.uk&gt; [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>David Hofmeyr &lt;dhofmeyr@sun.ac.za&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements recently developed projection 
    pursuit algorithms for finding optimal linear cluster
    separators. The clustering algorithms use optimal
    hyperplane separators based on minimum density, Pavlidis et. al (2016) <a href="http://jmlr.org/papers/volume17/15-307/15-307.pdf">http://jmlr.org/papers/volume17/15-307/15-307.pdf</a>;
    minimum normalised cut, Hofmeyr (2017) &lt;<a href="https://doi.org/10.1109%2FTPAMI.2016.2609929">doi:10.1109/TPAMI.2016.2609929</a>&gt;;
    and maximum variance ratio clusterability, Hofmeyr and Pavlidis (2015) &lt;<a href="https://doi.org/10.1109%2FSSCI.2015.116">doi:10.1109/SSCI.2015.116</a>&gt;.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10.0), rARPACK</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>yes</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.0.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-03-06 09:20:06 UTC; dhofmeyr</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-03-06 10:10:01 UTC</td>
</tr>
</table>
<hr>
<h2 id='PPCI-package'>
Projection Pursuit for Cluster Identification
</h2><span id='topic+PPCI-package'></span><span id='topic+PPCI'></span>

<h3>Description</h3>

<p>This package provides implementations of three recently developed projection pursuit methods for clustering. These methods optimise measures of clusterability of the univariate (projected) dataset that are motivated by three well established
approaches to clustering; namely density clustering, centroid based clustering and clustering by graph cuts. The resulting partitions are formed by hyperplanes orthogonal to the optimal projection vectors, and multiple such partitioning hyperplanes are combined in a hierarchical model to generate complete clustering solutions. Model visualisations through low dimensional projections of the data/clusters are provided through multiple plotting functions, which facilitate model validation. Simple model modification functions then allow for pseudo-interactive clustering.
</p>
<p>The three main clustering algorithms are implemented in the functions mddc, mcdc and ncutdc. Each takes two mandatory arguments, the data matrix (X) and the number of clusters (K). Numerous optional arguments allow the user to modify the specifics of optimisation, etc. If the correct number of clusters is not known an approximate number can be used, and the resulting solutions visualised using the functions tree_plot (provides a visualisation of the entire model) and node_plot (provides a more detailed visualisation of a single node in the hierarchical model). Both of these cfunctions can be accessed using the base plot function applied to the output of one of mddc, mcdc or ncutdc. Nodes can then be removed using the function tree_prune, or added using the function tree_split, depending on the apparent validity of the existing clustering model.
</p>


<h3>Details</h3>

<p>Package:  PPCI
</p>
<p>Type: Package
</p>
<p>Title:  Projection Pursuit for Cluster Identification
</p>
<p>Version:  0.1.4
</p>
<p>Depends:  R (&gt;= 2.10.0), rARPACK
</p>
<p>License:  GPL-3
</p>
<p>LazyData: yes
</p>


<h3>Author(s)</h3>

<p>David Hofmeyr[aut, cre] and Nicos Pavlidis[aut]
</p>
<p>Maintainer: David Hofmeyr &lt;dhofmeyr@sun.ac.za&gt;
</p>


<h3>References</h3>

<p>Pavlidis N.G., Hofmeyr D.P., Tasoulis S.K. (2016) Minimum Density Hyperplanes. <em>Journal of
Machine Learning Research</em>, <b>17</b>(156), 1&ndash;33.
</p>
<p>Hofmeyr, D., Pavlidis, N. (2015) Maximum Clusterability Divisive Clustering. <em>Computational Intelligence, 2015 IEEE Symposium Series on</em>, pp. 780&ndash;786.
</p>
<p>Hofmeyr, D. (2017) Clustering by Minimum Cut Hyperplanes. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, <b>39</b>(8), 1547 &ndash; 1560.
</p>


<h3>See Also</h3>

<p>A <b>MATLAB</b> toolbox with similar functionality to this package is available at <a href="https://github.com/nicospavlidis/opc">https://github.com/nicospavlidis/opc</a>. Outputs may differ slightly due to differences between <b>R</b> and <b>MATLAB</b>'s base optimisation software.</p>

<hr>
<h2 id='add_subtree'>Add Nodes To a Plot of a Hierarchical Clustering Model</h2><span id='topic+add_subtree'></span>

<h3>Description</h3>

<p>Used to add nodes to a plot region created for visualising hierarchical clustering models arising from functions mcdc, mddc and ncutdc. Not intended for use outside function tree_plot.
</p>

<hr>
<h2 id='breastcancer'>Discrimination of Cancerous and Non-Cancerous Breast Masses</h2><span id='topic+breastcancer'></span>

<h3>Description</h3>

<p>This data set contains case information from 699 patients from Wisconsin General Hospital who received examination for mammographic masses, which were classified as either benign or malignant.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>breastcancer</code></pre>


<h3>Format</h3>

<p>A list with entries $x (a 699x9 matrix with each row corresponding to an individual case) and $c (a vector of labels indicating whether the mass was diagnosed as benign (2) or malignant (4)).</p>


<h3>Source</h3>

<p>UCI Machine Learning Repository.</p>


<h3>References</h3>

<p>Lichman, M. (2013) UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science. [https://archive.ics.uci.edu/ml] 
</p>

<hr>
<h2 id='cluster_performance'>External Cluster Validity Metrics</h2><span id='topic+cluster_performance'></span>

<h3>Description</h3>

<p>Computes four popular external cluster validity metrics (adjusted Rand index, purity, V-measure and Normalised Mutual Information) through comparison of cluster assignments and true class labels. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cluster_performance(assigned, labels, beta)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cluster_performance_+3A_assigned">assigned</code></td>
<td>
<p>a vector of cluster assignments made by a clustering algorithm.</p>
</td></tr>
<tr><td><code id="cluster_performance_+3A_labels">labels</code></td>
<td>
<p>a vector of true class labels to be compared with assigned.</p>
</td></tr>
<tr><td><code id="cluster_performance_+3A_beta">beta</code></td>
<td>
<p>(optional) positive numeric, used in the computation of V-measure. larger values apply higher weight to homogeneity over completeness measures. if omitted then beta = 1 (equal weight applied to both measures).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector containing the four evaluation metrics listed in the description.
</p>


<h3>References</h3>

<p>Zhao Y., Karypis G. (2004) Empirical and Theoretical Comparisons of Selected Criterion
Functions for Document Clustering. <em>Machine Learning</em>, <b>55</b>(3), 311&ndash;331.
</p>
<p>Strehl A., Ghosh J. (2002) Cluster ensembles—a knowledge reuse framework for combining
multiple partitions. <em>Journal of Machine Learning Research</em>, <b>3</b>, 583&ndash;617.
</p>
<p>Rosenberg A., Hirschberg J. (2007) V-Measure: A Conditional Entropy-Based External
Cluster Evaluation Measure. <em>EMNLP-CoNLL</em>, <b>7</b>, 410&ndash;420. Citeseer.
</p>
<p>Hubert, L., Arabie, P. (1985) Comparing Partitions. <em>Journal of Classification</em>, <b>2</b>(1), 193&ndash;218.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load dermatology dataset
data(dermatology)

## obtain clustering solution using MCDC
sol &lt;- mcdc(dermatology$x, 6)

## evaluate solution using external cluster validity measures
cluster_performance(sol$cluster, dermatology$c)
</code></pre>

<hr>
<h2 id='dermatology'>Eryhemato-Squamous Disease Identification</h2><span id='topic+dermatology'></span>

<h3>Description</h3>

<p>This data set contains clinical and histopathological information from 366 cases of 6 different skin disorders/diseases for the purpous of differential diagnosis of eryhemato-squamous disease.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dermatology</code></pre>


<h3>Format</h3>

<p>A list with entries $x (a 366x34 matrix with each row corresponding to an individual case) and $c (a vector of labels indicating the skin condition).</p>


<h3>Source</h3>

<p>UCI Machine Learning Repository.</p>


<h3>References</h3>

<p>Lichman, M. (2013) UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science. <a href="https://archive.ics.uci.edu/ml">https://archive.ics.uci.edu/ml</a>
</p>

<hr>
<h2 id='df_mc'>Gradient of the Variance Ratio Clusterability Across a Hyperplane</h2><span id='topic+df_mc'></span>

<h3>Description</h3>

<p>Finds the gradient of the variance ratio across the best hyperplane orthogonal to a given projection vector. Used to obtain maximum clusterability hyperplanes using gradient based optimisation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>df_mc(v, X, P)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="df_mc_+3A_v">v</code></td>
<td>
<p>a numeric vector of length ncol(X)</p>
</td></tr>
<tr><td><code id="df_mc_+3A_x">X</code></td>
<td>
<p>a numeric matrix (num_data x num_dimensions) to be projected on v</p>
</td></tr>
<tr><td><code id="df_mc_+3A_p">P</code></td>
<td>
<p>a list of parameters including (at least) $nmin (positive integer minimum cluster size).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the (vector) gradient of the variance across the best hyperplane orthogonal to v.
</p>

<hr>
<h2 id='df_md'>Gradient of the Integrated Density on a Hyperplane</h2><span id='topic+df_md'></span>

<h3>Description</h3>

<p>Finds the gradient of the integrated density of the best hyperplanes orthogonal to a given projection vector (assumes the data have zero mean vector). Used to obtain minimum density hyperplanes using gradient based optimisation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>df_md(v, X, P)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="df_md_+3A_v">v</code></td>
<td>
<p>a numeric vector of length ncol(X)</p>
</td></tr>
<tr><td><code id="df_md_+3A_x">X</code></td>
<td>
<p>a numeric matrix (num_data x num_dimensions) to be projected on v</p>
</td></tr>
<tr><td><code id="df_md_+3A_p">P</code></td>
<td>
<p>a list of parameters including (at least) $h (positive numeric bandwidth value), $alpha (positive numeric constraint width), $C (positive numeric affecting the slope of the penalty), $COV (covariance matrix of X)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the (vector) gradient of the integrated density of the best hyperplane orthogonal to v.
</p>

<hr>
<h2 id='df_ncut'>Gradient of the Normalised Cut Across a Hyperplane</h2><span id='topic+df_ncut'></span>

<h3>Description</h3>

<p>Finds the gradient of the normalised cut across the best hyperplane orthogonal to a given projection vector. Used to obtain minimum normalised cut hyperplanes using gradient based optimisation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>df_ncut(v, X, P)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="df_ncut_+3A_v">v</code></td>
<td>
<p>a numeric vector of length ncol(X)</p>
</td></tr>
<tr><td><code id="df_ncut_+3A_x">X</code></td>
<td>
<p>a numeric matrix (num_data x num_dimensions) to be projected on v</p>
</td></tr>
<tr><td><code id="df_ncut_+3A_p">P</code></td>
<td>
<p>a list of parameters including (at least) $s (positive numeric scaling parameter), $nmin (positive integer minimum cluster size).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the (vector) gradient of the normalised cut across the best hyperplane orthogonal to v.
</p>

<hr>
<h2 id='dkde'>Gradient of the Penalised Density at a Point</h2><span id='topic+dkde'></span>

<h3>Description</h3>

<p>Used within bisection algorithm to find the location of the best hyperplane orthogonal to a given projection vector. Not intended for use outside of functions f_md and df_md
</p>

<hr>
<h2 id='f_mc'>Variance Ratio Clusterability Across a Hyperplane</h2><span id='topic+f_mc'></span>

<h3>Description</h3>

<p>Finds the variance ratio clusterability measured across the best hyperplane orthogonal to a given projection vector. Used as projection index for projection pursuit to find maximum clusterability hyperplanes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>f_mc(v, X, P)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="f_mc_+3A_v">v</code></td>
<td>
<p>a numeric vector of length ncol(X)</p>
</td></tr>
<tr><td><code id="f_mc_+3A_x">X</code></td>
<td>
<p>a numeric matrix (num_data x num_dimensions) to be projected on v</p>
</td></tr>
<tr><td><code id="f_mc_+3A_p">P</code></td>
<td>
<p>a list of parameters including (at least) $nmin (positive integer minimum cluster size)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the value of the variance ratio across the best hyperplane orthogonal to v
</p>

<hr>
<h2 id='f_md'>Integrated Density on a Hyperplane</h2><span id='topic+f_md'></span>

<h3>Description</h3>

<p>Finds the integrated density of the best hyperplanes orthogonal to a given projection vector (assumes the data have zero mean vector). Used as projection index for projection pursuit to find minimum density hyperplanes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>f_md(v, X, P)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="f_md_+3A_v">v</code></td>
<td>
<p>a numeric vector of length ncol(X)</p>
</td></tr>
<tr><td><code id="f_md_+3A_x">X</code></td>
<td>
<p>a numeric matrix (num_data x num_dimensions) to be projected on v</p>
</td></tr>
<tr><td><code id="f_md_+3A_p">P</code></td>
<td>
<p>a list of parameters including (at least) $h (positive numeric bandwidth value), $alpha (positive numeric constraint width), $C (positive numeric affecting the slope of the penalty)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the value of the integrated density of the best hyperplane orthogonal to v
</p>

<hr>
<h2 id='f_ncut'>Normalised Cut Across a Hyperplane</h2><span id='topic+f_ncut'></span>

<h3>Description</h3>

<p>Finds the normalised cut across the best hyperplane orthogonal to a given projection vector. Used as projection index for projection pursuit to find minimum normalised cut hyperplanes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>f_ncut(v, X, P)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="f_ncut_+3A_v">v</code></td>
<td>
<p>a numeric vector of length ncol(X)</p>
</td></tr>
<tr><td><code id="f_ncut_+3A_x">X</code></td>
<td>
<p>a numeric matrix (num_data x num_dimensions) to be projected on v</p>
</td></tr>
<tr><td><code id="f_ncut_+3A_p">P</code></td>
<td>
<p>a list of parameters including (at least) $s (positive numeric scaling parameter), $nmin (positive integer minimum cluster size)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the value of the normalised cut across the best hyperplane orthogonal to v
</p>

<hr>
<h2 id='hp_plot'>Visualise a Hyperplane Separator for Clustering</h2><span id='topic+hp_plot'></span>

<h3>Description</h3>

<p>Provides a visualisation of the partition induced by a hyperplane separator generated by one of mch, mdh and ncuth.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hp_plot(sol, labels, transparency)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hp_plot_+3A_sol">sol</code></td>
<td>
<p>a solution arising from one of the functions mch, mdh and ncuth.</p>
</td></tr>
<tr><td><code id="hp_plot_+3A_labels">labels</code></td>
<td>
<p>(optional) a vector of class labels. if provided then points in different classes are plotted in different colours. the external validity of the partition is also indicated when this can be computed using true class labels.</p>
</td></tr>
<tr><td><code id="hp_plot_+3A_transparency">transparency</code></td>
<td>
<p>(optional) if ommitted then points in the scatterplot are
shown as solid. If set to a value in (0, 1) then points are shown with transparency. Lower values correspond with a greater degree of transparency.</p>
</td></tr>
</table>

<hr>
<h2 id='is_minim'>Check if the Current Solution is a Valid Minimum Density Hyperplane</h2><span id='topic+is_minim'></span>

<h3>Description</h3>

<p>Used within minimum density projection pursuit. Not intended for independent use outside of other functions.
</p>

<hr>
<h2 id='mc_b'>Location of Optimal Variance Ratio Hyperplane</h2><span id='topic+mc_b'></span>

<h3>Description</h3>

<p>The value of b which maximises the variance ratio across the hyperplane H(v, b) = [x : v'x = b]. If v is a locally optimal solution for the projection index f_mc then H(v, b) is a locally optimal hyperplane.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mc_b(v, X, P)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mc_b_+3A_v">v</code></td>
<td>
<p>a numeric vector of length ncol(X)</p>
</td></tr>
<tr><td><code id="mc_b_+3A_x">X</code></td>
<td>
<p>a numeric matrix (num_data x num_dimensions) to be projected on v</p>
</td></tr>
<tr><td><code id="mc_b_+3A_p">P</code></td>
<td>
<p>a list of parameters including (at least) $nmin (positive integer minimum cluster size).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the value of b given in the description.
</p>

<hr>
<h2 id='mcdc'>Divisive Clustering Using Maximum Clusterability</h2><span id='topic+mcdc'></span>

<h3>Description</h3>

<p>Generates a binary partitioning tree by recursively partitioning a dataset using a hierarchical collection of hyperplanes with
high variance ratio custerability across them.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcdc(X, K, v0, split.index, minsize, verb, labels, maxit, ftol)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mcdc_+3A_x">X</code></td>
<td>
<p>a numeric matrix (num_data x num_dimensions); the dataset to be clustered.</p>
</td></tr>
<tr><td><code id="mcdc_+3A_k">K</code></td>
<td>
<p>the number of clusters to extract.</p>
</td></tr>
<tr><td><code id="mcdc_+3A_split.index">split.index</code></td>
<td>
<p>(optional) determines the order in which clusters are split (in decreasing order of split indices). can be a numeric valued function(v, X, P) of projection vector v, data matrix X and list of parameters P. can also be one of &quot;size&quot; (split the largest cluster), &quot;fval&quot; (split the cluster with the maximum variance ratio) or &quot;Fdist&quot; (split indices determined by the non-central F-distribution. See SSCI paper for details. slight difference from the paper is that
when the data size is above 2000 cluster size is used instead. This is because the naive
estimation of the model degrees of freedom has been found to be unreliable when the number
of data is large). if omitted then &quot;Fdist&quot; is used.</p>
</td></tr>
<tr><td><code id="mcdc_+3A_v0">v0</code></td>
<td>
<p>(optional) initial projection direction(s). a function(X) of the data being split, which returns a matrix with ncol(X) rows. each column of the output of v0(X) is used as an initialisation for projection pursuit. the solution with the maximum variance ratio is used within the final model. initialisations are determined separately for each cluster being split. if omitted then a single initialisation is used; the vector joining the cluster means of a 2-means solution.</p>
</td></tr>
<tr><td><code id="mcdc_+3A_minsize">minsize</code></td>
<td>
<p>(optional) the minimum cluster size allowable. if omitted then minsize = 1.</p>
</td></tr>
<tr><td><code id="mcdc_+3A_verb">verb</code></td>
<td>
<p>(optional) verbosity level of optimisation procedure. verb==0 produces no output. verb==1 produces plots illustrating the progress of projection pursuit via plots of the projected data. verb==2 adds to these plots additional information about the progress. verb==3 creates a folder in working directory and stores all plots for verb==2. if omitted then verb==0.</p>
</td></tr>
<tr><td><code id="mcdc_+3A_labels">labels</code></td>
<td>
<p>(optional) vector of class labels. not used in the actual clustering procedure. only used for illustrative purposes for values of verb&gt;0.</p>
</td></tr>
<tr><td><code id="mcdc_+3A_maxit">maxit</code></td>
<td>
<p>(optional) maximum number of iterations in optimisation. if omitted then maxit=50.</p>
</td></tr>
<tr><td><code id="mcdc_+3A_ftol">ftol</code></td>
<td>
<p>(optional) tolerance level for convergence of optimisation, based on relative function value improvements. if omitted then ftol = 1e-8.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list with class ppci_cluster_solution containing
</p>
<table role = "presentation">
<tr><td><code>$cluster</code></td>
<td>
<p>cluster assignment vector.</p>
</td></tr>
<tr><td><code>$model</code></td>
<td>
<p>matrix containing the would-be location of each node (depth and position at depth) within a complete tree of appropriate depth.</p>
</td></tr>
<tr><td><code>$nodes</code></td>
<td>
<p>unnamed list each element of which is a named list containing details of the binary partitions at each node in the model.</p>
</td></tr>
<tr><td><code>$data</code></td>
<td>
<p>the data matrix being clustered.</p>
</td></tr>
<tr><td><code>$method</code></td>
<td>
<p>==&quot;MCDC&quot;. used in plotting and model modification functions.</p>
</td></tr>
<tr><td><code>$args</code></td>
<td>
<p>named list of arguments passed to mcdc.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Hofmeyr, D., Pavlidis, N. (2015) Maximum Clusterability Divisive Clustering. <em>Computational Intelligence, 2015 IEEE Symposium Series on</em>, pp. 780&ndash;786.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load the dermatology dataset
data(dermatology)

## obtain a clustering solution using MCDC
sol &lt;- mcdc(dermatology$x, 6)

## evaluate the performance of the solution using external cluster validity metrics
cluster_performance(sol$cluster, dermatology$c)

</code></pre>

<hr>
<h2 id='mcdr'>Maximum Clusterability Dimension Reduction</h2><span id='topic+mcdr'></span>

<h3>Description</h3>

<p>Finds a linear projection of a data set using projection pursuit to maximise the variance ratio clusterability measured in each dimension separately.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcdr(X, p, minsize, v0, verb, labels, maxit, ftol)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mcdr_+3A_x">X</code></td>
<td>
<p>a numeric matrix (num_data x num_dimensions); the dataset.</p>
</td></tr>
<tr><td><code id="mcdr_+3A_p">p</code></td>
<td>
<p>an integer; the number of dimensions in the projection.</p>
</td></tr>
<tr><td><code id="mcdr_+3A_v0">v0</code></td>
<td>
<p>(optional) initial projection direction(s). a function(X) of the data, which returns a matrix with ncol(X) rows. each column of the output of v0(X) is used as an initialisation for projection pursuit. the solution with the minimum normalised cut is used within the final model. if omitted then a single initialisation is used for each column of the projection matrix; the first principal component within the null space of the other columns.</p>
</td></tr>
<tr><td><code id="mcdr_+3A_verb">verb</code></td>
<td>
<p>(optional) verbosity level of optimisation procedure. verb==0 produces no output. verb==1 produces plots illustrating the progress of projection pursuit via plots of the projected data. verb==2 adds to these plots additional information about the progress. verb==3 creates a folder in working directory and stores all plots for verb==2. if omitted then verb==0.</p>
</td></tr>
<tr><td><code id="mcdr_+3A_labels">labels</code></td>
<td>
<p>(optional) vector of class labels. not used in the actual projection pursuit. only used for illustrative purposes for values of verb&gt;0.</p>
</td></tr>
<tr><td><code id="mcdr_+3A_maxit">maxit</code></td>
<td>
<p>(optional) maximum number of iterations in optimisation for each value of alpha. if omitted then maxit=15.</p>
</td></tr>
<tr><td><code id="mcdr_+3A_ftol">ftol</code></td>
<td>
<p>(optional) tolerance level for convergence of optimisation, based on relative function value improvements. if omitted then ftol = 1e-5.</p>
</td></tr>
<tr><td><code id="mcdr_+3A_minsize">minsize</code></td>
<td>
<p>(optional) the minimum number of data on each side of a hyperplane. if omitted then minsize = 1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list with class ppci_projection_solution with the following components
</p>
<table role = "presentation">
<tr><td><code>$projection</code></td>
<td>
<p>the num_dimensions x p projection matrix.</p>
</td></tr>
<tr><td><code>$fitted</code></td>
<td>
<p>the num_data x p projected data set.</p>
</td></tr>
<tr><td><code>$data</code></td>
<td>
<p>the input data matrix.</p>
</td></tr>
<tr><td><code>$method</code></td>
<td>
<p>==&quot;MCDC&quot;.</p>
</td></tr>
<tr><td><code>$params</code></td>
<td>
<p>list of parameters used to find $projection.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Hofmeyr, D., Pavlidis, N. (2015) Maximum Clusterability Divisive Clustering. <em>Computational Intelligence, 2015 IEEE Symposium Series on</em>, pp. 780&ndash;786.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### not run
run = FALSE
if(run){
  ## load optidigits dataset
  data(optidigits)

  ## find nine dimensional projection (one fewer than
  ## the number of clusters, as is common in clustering)
  sol &lt;- mcdr(optidigits$x, 9)

  ## visualise the solution via the first 3 pairs of dimensions
  plot(sol, pairs = 3, labels = optidigits$c)

  ## compare with PCA projection
  pairs(optidigits$x%*%eigen(cov(optidigits$x))$vectors[,1:3], col = optidigits$c)
  }
</code></pre>

<hr>
<h2 id='mch'>Maximum Clusteriability Hyperplane</h2><span id='topic+mch'></span>

<h3>Description</h3>

<p>Finds maximum clusterability hyperplane(s) for clustering.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mch(X, v0, minsize, verb, labels, maxit, ftol)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mch_+3A_x">X</code></td>
<td>
<p>a numeric matrix (num_data x num_dimensions); the dataset to be clustered.</p>
</td></tr>
<tr><td><code id="mch_+3A_v0">v0</code></td>
<td>
<p>(optional) initial projection direction(s). a matrix with ncol(X) rows. each column of v0 is used as an initialisation for projection pursuit. if omitted then a single initialisation is used; the vector joining the cluster means from a 2-means solution.</p>
</td></tr>
<tr><td><code id="mch_+3A_minsize">minsize</code></td>
<td>
<p>(optional) the minimum cluster size allowable. if omitted then minsize = 1.</p>
</td></tr>
<tr><td><code id="mch_+3A_verb">verb</code></td>
<td>
<p>(optional) verbosity level of optimisation procedure. verb==0 produces no output. verb==1 produces plots illustrating the progress of projection pursuit via plots of the projected data. verb==2 adds to these plots additional information about the progress. verb==3 creates a folder in working directory and stores all plots for verb==2. if omitted then verb==0.</p>
</td></tr>
<tr><td><code id="mch_+3A_labels">labels</code></td>
<td>
<p>(optional) vector of class labels. not used in the actual clustering procedure. only used for illustrative purposes for values of verb&gt;0.</p>
</td></tr>
<tr><td><code id="mch_+3A_maxit">maxit</code></td>
<td>
<p>(optional) maximum number of iterations in optimisation. if omitted then maxit=50.</p>
</td></tr>
<tr><td><code id="mch_+3A_ftol">ftol</code></td>
<td>
<p>(optional) tolerance level for convergence of optimisation, based on relative function value improvements. if omitted then ftol = 1e-8.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list with class ppci_hyperplane_solution with the following components
</p>
<table role = "presentation">
<tr><td><code>$cluster</code></td>
<td>
<p>cluster assignment vector.</p>
</td></tr>
<tr><td><code>$v</code></td>
<td>
<p>the optimal projection vector.</p>
</td></tr>
<tr><td><code>$b</code></td>
<td>
<p>the value of b making H(v, b) the minimum normalised cut hyperplane.</p>
</td></tr>
<tr><td><code>$fitted</code></td>
<td>
<p>data projected into two dimensional subspace defined by $v and the principal component in the null space of $v.</p>
</td></tr>
<tr><td><code>$data</code></td>
<td>
<p>the input data matrix.</p>
</td></tr>
<tr><td><code>$fval</code></td>
<td>
<p>the variance ratio clusterability across H(v, b).</p>
</td></tr>
<tr><td><code>$method</code></td>
<td>
<p>==&quot;MCDC&quot;.</p>
</td></tr>
<tr><td><code>$params</code></td>
<td>
<p>list of parameters used to find H(v, b).</p>
</td></tr>
<tr><td><code>$alternatives</code></td>
<td>
<p>an unnamed list. If more than one initilisation is considered, the alternatives to the best are stored in this field.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Hofmeyr, D., Pavlidis, N. (2015) Maximum Clusterability Divisive Clustering. <em>Computational Intelligence, 2015 IEEE Symposium Series on</em>, pp. 780&ndash;786.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate dataset with elongated clusters for which variance ratio in
## both dimensions is misleading for clustering
set.seed(1)
S &lt;- matrix(c(1, .7, .7, 1), 2, 2)
X &lt;- matrix(rnorm(2000), ncol = 2)%*%S
X[,1] &lt;- X[,1] + rep(c(.8, -.8), each = 500)
X[,2] &lt;- X[,2] + rep(c(-.8, .8), each = 500)

## find the optimal variance ratio hyperplane solution
sol &lt;- mch(X)

## visualise the solution
plot(X, col = sol$cluster)
</code></pre>

<hr>
<h2 id='mcpp'>Maximum Clusterability Projection Pursuit</h2><span id='topic+mcpp'></span>

<h3>Description</h3>

<p>A gateway function between functions mch and ppclust.optim, the latter calls R's base optimisation function. Not intended for independent use.
</p>

<hr>
<h2 id='md_b'>Location of Minimum Density Hyperplane</h2><span id='topic+md_b'></span>

<h3>Description</h3>

<p>The value of b which minimises the penalised integrated density on the hyperplane H(v, b) = [x : v'x = b] (assumes the data have zero mean vector). If v is a locally optimal solution for the projection index f_md then H(v, b) is a locally optimal hyperplane.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>md_b(v, X, P)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="md_b_+3A_v">v</code></td>
<td>
<p>a numeric vector of length ncol(X)</p>
</td></tr>
<tr><td><code id="md_b_+3A_x">X</code></td>
<td>
<p>a numeric matrix (num_data x num_dimensions) to be projected on v</p>
</td></tr>
<tr><td><code id="md_b_+3A_p">P</code></td>
<td>
<p>a list of parameters including (at least) $h (positive numeric bandwidth value), $alpha (positive numeric constraint width), $C (positive numeric affecting the slope of the penalty)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the value of b given in the description.
</p>

<hr>
<h2 id='md_reldepth'>Relative Depth of a Hyperplane</h2><span id='topic+md_reldepth'></span>

<h3>Description</h3>

<p>Finds the relative depth, min((M1-m)/m, (M2-m)/m), where M1 is the maximum density below the optimal hyperplane orthogonal to v and M2 the maximum above, while m is the integrated density on the optimal hyperplane orthogonal to v.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>md_reldepth(v, X, P)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="md_reldepth_+3A_v">v</code></td>
<td>
<p>a numeric vector of length ncol(X)</p>
</td></tr>
<tr><td><code id="md_reldepth_+3A_x">X</code></td>
<td>
<p>a numeric matrix (num_data x num_dimensions) to be projected on v</p>
</td></tr>
<tr><td><code id="md_reldepth_+3A_p">P</code></td>
<td>
<p>a list of parameters including (at least) $h (positive numeric bandwidth value), $alpha (positive numeric constraint width), $C (positive numeric affecting the slope of the penalty)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the value of the relative depth of the best hyperplane orthogonal to v.
</p>

<hr>
<h2 id='mddc'>Divisive Clustering Using Minimum Density Hyperplanes</h2><span id='topic+mddc'></span>

<h3>Description</h3>

<p>Generates a binary partitioning tree by recursively partitioning a dataset using a hierarchical collection of hyperplanes with
low empirical density integral.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mddc(X, K, minsize, split.index, v0, bandwidth,
      alphamin, alphamax, verb, labels, maxit, ftol)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mddc_+3A_x">X</code></td>
<td>
<p>a numeric matrix (num_data x num_dimensions); the dataset to be clustered.</p>
</td></tr>
<tr><td><code id="mddc_+3A_k">K</code></td>
<td>
<p>the number of clusters to extract.</p>
</td></tr>
<tr><td><code id="mddc_+3A_minsize">minsize</code></td>
<td>
<p>(optional) minimum cluster size. if omitted then minsize = 1.</p>
</td></tr>
<tr><td><code id="mddc_+3A_split.index">split.index</code></td>
<td>
<p>(optional) determines the order in which clusters are split (in decreasing order of split indices). can be a numeric valued function(v, X, P) of projection vector v, data matrix X and list of parameters P. can also be one of &quot;size&quot; (split the largest cluster), &quot;fval&quot; (split the cluster with the minimum density hyperplane) or &quot;rdepth&quot; (split the cluster with the maximum relative depth). if omitted then &quot;size&quot; is used.</p>
</td></tr>
<tr><td><code id="mddc_+3A_v0">v0</code></td>
<td>
<p>(optional) initial projection direction(s). a function(X) of the data being split, which returns a matrix with ncol(X) rows. each column of the output of v0(X) is used as an initialisation for projection pursuit. the solution with the greatest relative depthis used within the final model. initialisations are determined separately for each cluster being split. if omitted then a single initialisation is used; the first principal component.</p>
</td></tr>
<tr><td><code id="mddc_+3A_bandwidth">bandwidth</code></td>
<td>
<p>(optional) used to compute the bandwidth parameter (h) for the kernel density estimator. a numeric valued function(X) of the cluster being split. if omitted then bandwidth(X) = 0.9*eigen(cov(X))$values[1]^.5*nrow(X)^(-0.2).</p>
</td></tr>
<tr><td><code id="mddc_+3A_alphamin">alphamin</code></td>
<td>
<p>(optional) initial (scaled) bound on the distance of the optimal hyperplane from the mean of the data (or subset being split). if omitted then alphamin = 0.</p>
</td></tr>
<tr><td><code id="mddc_+3A_alphamax">alphamax</code></td>
<td>
<p>(optional) maximum (scaled) distance of the optimal hyperplane from the mean of the data (or subset being split). if omitted then alphamax = 1.</p>
</td></tr>
<tr><td><code id="mddc_+3A_verb">verb</code></td>
<td>
<p>(optional) verbosity level of optimisation procedure. verb==0 produces no output. verb==1 produces plots illustrating the progress of projection pursuit via plots of the projected data. verb==2 adds to these plots additional information about the progress. verb==3 creates a folder in working directory and stores all plots for verb==2. if omitted then verb==0.</p>
</td></tr>
<tr><td><code id="mddc_+3A_labels">labels</code></td>
<td>
<p>(optional) vector of class labels. not used in the actual clustering procedure. only used for illustrative purposes for values of verb&gt;0.</p>
</td></tr>
<tr><td><code id="mddc_+3A_maxit">maxit</code></td>
<td>
<p>(optional) maximum number of iterations in optimisation for each value of alpha. if omitted then maxit=15.</p>
</td></tr>
<tr><td><code id="mddc_+3A_ftol">ftol</code></td>
<td>
<p>(optional) tolerance level for convergence of optimisation, based on relative function value improvements. if omitted then ftol = 1e-5.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list with class ppci_cluster_solution containing
</p>
<table role = "presentation">
<tr><td><code>$cluster</code></td>
<td>
<p>cluster assignment vector.</p>
</td></tr>
<tr><td><code>$model</code></td>
<td>
<p>matrix containing the would-be location of each node (depth and position at depth) within a complete tree of appropriate depth.</p>
</td></tr>
<tr><td><code>$nodes</code></td>
<td>
<p>unnamed list each element of which is a named list containing details of the binary partitions at each node in the model.</p>
</td></tr>
<tr><td><code>$data</code></td>
<td>
<p>the data matrix being clustered.</p>
</td></tr>
<tr><td><code>$method</code></td>
<td>
<p>==&quot;MDH&quot;. used in plotting and model modification functions.</p>
</td></tr>
<tr><td><code>$args</code></td>
<td>
<p>named list of arguments passed to mddc.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Pavlidis N.G., Hofmeyr D.P., Tasoulis S.K. (2016) Minimum Density Hyperplanes. <em>Journal of
Machine Learning Research</em>, <b>17</b>(156), 1&ndash;33.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load dermatology dataset
data(dermatology)

## obtain a clustering solution using minimum density hyperplanes
sol &lt;- mddc(dermatology$x, 6)

## evaluate the solution using external cluster validity metrics
cluster_performance(sol$cluster, dermatology$c)
</code></pre>

<hr>
<h2 id='mddr'>Minimum Density Dimension Reduction</h2><span id='topic+mddr'></span>

<h3>Description</h3>

<p>Finds a linear projection of a data set using projection pursuit to find the vector(s) orthogonal to minimum density hyperplanes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mddr(X, p, minsize, v0, bandwidth,
      alphamin, alphamax, verb, labels, maxit, ftol)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mddr_+3A_x">X</code></td>
<td>
<p>a numeric matrix (num_data x num_dimensions); the dataset.</p>
</td></tr>
<tr><td><code id="mddr_+3A_p">p</code></td>
<td>
<p>an integer; the number of dimensions in the projection.</p>
</td></tr>
<tr><td><code id="mddr_+3A_v0">v0</code></td>
<td>
<p>(optional) initial projection direction(s). a function(X) of the data, which returns a matrix with ncol(X) rows. each column of the output of v0(X) is used as an initialisation for projection pursuit. the solution with the minimum normalised cut is used within the final model. if omitted then a single initialisation is used for each column of the projection matrix; the first principal component within the null space of the other columns.</p>
</td></tr>
<tr><td><code id="mddr_+3A_bandwidth">bandwidth</code></td>
<td>
<p>(optional) used to compute the bandwidth parameter (h) for the kernel density estimator. a numeric valued function(X) of the cluster being split. if omitted then bandwidth(X) = 0.9*eigen(cov(X))$values[1]^.5*nrow(X)^(-0.2).</p>
</td></tr>
<tr><td><code id="mddr_+3A_alphamin">alphamin</code></td>
<td>
<p>(optional) initial (scaled) bound on the distance of the optimal hyperplane from the mean of the data. if omitted then alphamin = 0.</p>
</td></tr>
<tr><td><code id="mddr_+3A_alphamax">alphamax</code></td>
<td>
<p>(optional) maximum (scaled) distance of the optimal hyperplane from the mean of the data. if omitted then alphamax = 1.</p>
</td></tr>
<tr><td><code id="mddr_+3A_verb">verb</code></td>
<td>
<p>(optional) verbosity level of optimisation procedure. verb==0 produces no output. verb==1 produces plots illustrating the progress of projection pursuit via plots of the projected data. verb==2 adds to these plots additional information about the progress. verb==3 creates a folder in working directory and stores all plots for verb==2. if omitted then verb==0.</p>
</td></tr>
<tr><td><code id="mddr_+3A_labels">labels</code></td>
<td>
<p>(optional) vector of class labels. not used in the actual projection pursuit. only used for illustrative purposes for values of verb&gt;0.</p>
</td></tr>
<tr><td><code id="mddr_+3A_maxit">maxit</code></td>
<td>
<p>(optional) maximum number of iterations in optimisation for each value of alpha. if omitted then maxit=15.</p>
</td></tr>
<tr><td><code id="mddr_+3A_ftol">ftol</code></td>
<td>
<p>(optional) tolerance level for convergence of optimisation, based on relative function value improvements. if omitted then ftol = 1e-5.</p>
</td></tr>
<tr><td><code id="mddr_+3A_minsize">minsize</code></td>
<td>
<p>(optional) the minimum number of data on each side of a hyperplane. if omitted then minsize = 1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list with class ppci_projection_solution with the following components
</p>
<table role = "presentation">
<tr><td><code>$projection</code></td>
<td>
<p>the num_dimensions x p projection matrix.</p>
</td></tr>
<tr><td><code>$fitted</code></td>
<td>
<p>the num_data x p projected data set.</p>
</td></tr>
<tr><td><code>$data</code></td>
<td>
<p>the input data matrix.</p>
</td></tr>
<tr><td><code>$method</code></td>
<td>
<p>==&quot;MDH&quot;.</p>
</td></tr>
<tr><td><code>$params</code></td>
<td>
<p>list of parameters used to find $projection.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Pavlidis N.G., Hofmeyr D.P., Tasoulis S.K. (2016) Minimum Density Hyperplanes. <em>Journal of
Machine Learning Research</em>, <b>17</b>(156), 1&ndash;33.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### not run
run = FALSE
if(run){
  ## load optidigits dataset
  data(optidigits)

  ## find nine dimensional projection (one fewer than
  ## the number of clusters, as is common in clustering)
  sol &lt;- mddr(optidigits$x, 9)

  ## visualise the solution via the first 3 pairs of dimensions
  plot(sol, pairs = 3, labels = optidigits$c)

  ## compare with PCA projection
  pairs(optidigits$x%*%eigen(cov(optidigits$x))$vectors[,1:3], col = optidigits$c)
  }
</code></pre>

<hr>
<h2 id='mdh'>Minimum Density Hyperplane</h2><span id='topic+mdh'></span>

<h3>Description</h3>

<p>Finds minimum density hyperplane(s) for clustering.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mdh(X, v0, minsize, bandwidth, alphamin, alphamax, verb, labels, maxit, ftol)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mdh_+3A_x">X</code></td>
<td>
<p>a numeric matrix (num_data x num_dimensions); the dataset to be clustered.</p>
</td></tr>
<tr><td><code id="mdh_+3A_v0">v0</code></td>
<td>
<p>(optional) initial projection direction(s). a matrix with ncol(X) rows. each column of v0 is used as an initialisation for projection pursuit. if omitted then a single initialisation is used; the first principal component.</p>
</td></tr>
<tr><td><code id="mdh_+3A_minsize">minsize</code></td>
<td>
<p>(optional) minimum cluster size. if omitted then minsize = 1.</p>
</td></tr>
<tr><td><code id="mdh_+3A_bandwidth">bandwidth</code></td>
<td>
<p>(optional) positive numeric bandwidth parameter (h) for the kernel density estimator. if omitted then bandwidth = 0.9*eigen(cov(X))$values[1]^.5*nrow(X)^(-0.2).</p>
</td></tr>
<tr><td><code id="mdh_+3A_alphamin">alphamin</code></td>
<td>
<p>(optional) initial (scaled) bound on the distance of the optimal hyperplane from the mean of the data. if omitted then alphamin = 0.</p>
</td></tr>
<tr><td><code id="mdh_+3A_alphamax">alphamax</code></td>
<td>
<p>(optional) maximum/final (scaled) distance of the optimal hyperplane from the mean of the data. if omitted then alphamax = 1.</p>
</td></tr>
<tr><td><code id="mdh_+3A_verb">verb</code></td>
<td>
<p>(optional) verbosity level of optimisation procedure. verb==0 produces no output. verb==1 produces plots illustrating the progress of projection pursuit via plots of the projected data. verb==2 adds to these plots additional information about the progress. verb==3 creates a folder in working directory and stores all plots for verb==2. if omitted then verb==0.</p>
</td></tr>
<tr><td><code id="mdh_+3A_labels">labels</code></td>
<td>
<p>(optional) vector of class labels. not used in the actual clustering procedure. only used for illustrative purposes for values of verb&gt;0.</p>
</td></tr>
<tr><td><code id="mdh_+3A_maxit">maxit</code></td>
<td>
<p>(optional) maximum number of iterations in optimisation for each value of alpha. if omitted then maxit=15.</p>
</td></tr>
<tr><td><code id="mdh_+3A_ftol">ftol</code></td>
<td>
<p>(optional) tolerance level for convergence of optimisation, based on relative function value improvements. if omitted then ftol = 1e-5.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list with class ppci_hyperplane_solution with the following components
</p>
<table role = "presentation">
<tr><td><code>$cluster</code></td>
<td>
<p>cluster assignment vector.</p>
</td></tr>
<tr><td><code>$v</code></td>
<td>
<p>the optimal projection vector.</p>
</td></tr>
<tr><td><code>$b</code></td>
<td>
<p>the value of b making H(v, b) the minimum density hyperplane.</p>
</td></tr>
<tr><td><code>$fitted</code></td>
<td>
<p>data projected into two dimensional subspace defined by $v and the principal component in the null space of $v.</p>
</td></tr>
<tr><td><code>$data</code></td>
<td>
<p>the input data matrix.</p>
</td></tr>
<tr><td><code>$rel.dep</code></td>
<td>
<p>the relative depth of H(v, b).</p>
</td></tr>
<tr><td><code>$fval</code></td>
<td>
<p>the integrated dentsity on H(v, b).</p>
</td></tr>
<tr><td><code>$method</code></td>
<td>
<p>==&quot;MDH&quot;.</p>
</td></tr>
<tr><td><code>$params</code></td>
<td>
<p>list of parameters used to find H(v, b).</p>
</td></tr>
<tr><td><code>$alternatives</code></td>
<td>
<p>an unnamed list. If more than one initilisation is considered, the alternatives to the best are stored in this field.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Pavlidis N.G., Hofmeyr D.P., Tasoulis S.K. (2016) Minimum Density Hyperplanes. <em>Journal of
Machine Learning Research</em>, <b>17</b>(156), 1&ndash;33.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load breast cancer dataset
data(breastcancer)

## find minimum density hyperplane
sol &lt;- mdh(breastcancer$x)

## visualise the solution
plot(sol)

## evaluate the quality of the partition
success_ratio(sol$cluster, breastcancer$c)
</code></pre>

<hr>
<h2 id='mdpp'>Minimum Density Projection Pursuit</h2><span id='topic+mdpp'></span>

<h3>Description</h3>

<p>A gateway function between functions mdh and ppclust.optim, the latter calls R's base optimisation function. Not intended for independent use.
</p>

<hr>
<h2 id='ncut_b'>Location of Minimum Normalised Cut Hyperplane</h2><span id='topic+ncut_b'></span>

<h3>Description</h3>

<p>The value of b which minimises the normalised cut across the hyperplane H(v, b) = [x : v'x = b]. If v is a locally optimal solution for the projection index f_ncut then H(v, b) is a locally optimal hyperplane.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ncut_b(v, X, P)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ncut_b_+3A_v">v</code></td>
<td>
<p>a numeric vector of length ncol(X)</p>
</td></tr>
<tr><td><code id="ncut_b_+3A_x">X</code></td>
<td>
<p>a numeric matrix (num_data x num_dimensions) to be projected on v</p>
</td></tr>
<tr><td><code id="ncut_b_+3A_p">P</code></td>
<td>
<p>a list of parameters including (at least) $s (positive numeric scaling parameter), $nmin (positive integer minimum cluster size).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the value of b given in the description.
</p>

<hr>
<h2 id='ncutdc'>Divisive Clustering Using Minimum Normalised Cut Hyperplanes</h2><span id='topic+ncutdc'></span>

<h3>Description</h3>

<p>Generates a binary partitioning tree by recursively partitioning a dataset using a hierarchical collection of hyperplanes with
low normalised cut measured across them.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ncutdc(X, K, split.index, v0, s, minsize, verb, labels, maxit, ftol)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ncutdc_+3A_x">X</code></td>
<td>
<p>a numeric matrix (num_data x num_dimensions); the dataset to be clustered.</p>
</td></tr>
<tr><td><code id="ncutdc_+3A_k">K</code></td>
<td>
<p>the number of clusters to extract.</p>
</td></tr>
<tr><td><code id="ncutdc_+3A_split.index">split.index</code></td>
<td>
<p>(optional) determines the order in which clusters are split (in decreasing order of split indices). can be a numeric valued function(v, X, P) of projection vector v, data matrix X and list of parameters P. can also be one of &quot;size&quot; (split the largest cluster) or &quot;fval&quot; (split the cluster with the minimum density hyperplane). if omitted then &quot;fval&quot; is used.</p>
</td></tr>
<tr><td><code id="ncutdc_+3A_v0">v0</code></td>
<td>
<p>(optional) initial projection direction(s). a function(X) of the data being split, which returns a matrix with ncol(X) rows. each column of the output of v0(X) is used as an initialisation for projection pursuit. the solution with the minimum normalised cut is used within the final model. initialisations are determined separately for each cluster being split. if omitted then a single initialisation is used; the first principal component.</p>
</td></tr>
<tr><td><code id="ncutdc_+3A_s">s</code></td>
<td>
<p>(optional) used to compute the scaling parameter (sigma) for pairwise similarities. a numeric valued function(X) of the cluster being split. if omitted then s(X) = 100*eigen(cov(X))$values[1]^.5*nrow(X)^(-0.2).</p>
</td></tr>
<tr><td><code id="ncutdc_+3A_minsize">minsize</code></td>
<td>
<p>(optional) the minimum cluster size allowable. if omitted then minsize = 1.</p>
</td></tr>
<tr><td><code id="ncutdc_+3A_verb">verb</code></td>
<td>
<p>(optional) verbosity level of optimisation procedure. verb==0 produces no output. verb==1 produces plots illustrating the progress of projection pursuit via plots of the projected data. verb==2 adds to these plots additional information about the progress. verb==3 creates a folder in working directory and stores all plots for verb==2. if omitted then verb==0.</p>
</td></tr>
<tr><td><code id="ncutdc_+3A_labels">labels</code></td>
<td>
<p>(optional) vector of class labels. not used in the actual clustering procedure. only used for illustrative purposes for values of verb&gt;0.</p>
</td></tr>
<tr><td><code id="ncutdc_+3A_maxit">maxit</code></td>
<td>
<p>(optional) maximum number of iterations in optimisation. if omitted then maxit=50.</p>
</td></tr>
<tr><td><code id="ncutdc_+3A_ftol">ftol</code></td>
<td>
<p>(optional) tolerance level for convergence of optimisation, based on relative function value improvements. if omitted then ftol = 1e-8.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list with class ppci_cluster_solution containing
</p>
<table role = "presentation">
<tr><td><code>$cluster</code></td>
<td>
<p>cluster assignment vector.</p>
</td></tr>
<tr><td><code>$model</code></td>
<td>
<p>matrix containing the would-be location of each node (depth and position at depth) within a complete tree of appropriate depth.</p>
</td></tr>
<tr><td><code>$nodes</code></td>
<td>
<p>unnamed list each element of which is a named list containing details of the binary partitions at each node in the model.</p>
</td></tr>
<tr><td><code>$data</code></td>
<td>
<p>the data matrix being clustered.</p>
</td></tr>
<tr><td><code>$method</code></td>
<td>
<p>==&quot;NCutH&quot;. used in plotting and model modification functions.</p>
</td></tr>
<tr><td><code>$args</code></td>
<td>
<p>named list of arguments passed to ncutdc.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Hofmeyr, D. (2016) Clustering by Minimum Cut Hyperplanes. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, <b>39</b>(8), 1547 &ndash; 1560.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load dermatology dataset
data(dermatology)

## obtain clustering solution
sol &lt;- ncutdc(dermatology$x, 6)

## evaluate solution using external cluster validity metrics
cluster_performance(sol$cluster, dermatology$c)
</code></pre>

<hr>
<h2 id='ncutdr'>Minimum Normalised Cut Dimension Reduction</h2><span id='topic+ncutdr'></span>

<h3>Description</h3>

<p>Finds a linear projection of a data set using projection pursuit based on minimising the normalised cut measured in each dimension separately.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ncutdr(X, p, v0, s, minsize, verb, labels, maxit, ftol)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ncutdr_+3A_x">X</code></td>
<td>
<p>a numeric matrix (num_data x num_dimensions); the dataset.</p>
</td></tr>
<tr><td><code id="ncutdr_+3A_p">p</code></td>
<td>
<p>an integer; the number of dimensions in the projection.</p>
</td></tr>
<tr><td><code id="ncutdr_+3A_v0">v0</code></td>
<td>
<p>(optional) initial projection direction(s). a function(X) of the data, which returns a matrix with ncol(X) rows. each column of the output of v0(X) is used as an initialisation for projection pursuit. the solution with the minimum normalised cut is used within the final model. if omitted then a single initialisation is used for each column of the projection matrix; the first principal component within the null space of the other columns.</p>
</td></tr>
<tr><td><code id="ncutdr_+3A_s">s</code></td>
<td>
<p>(optional) used to compute the scaling parameter (sigma) for pairwise similarities. a numeric valued function(X) of the data. if omitted then s(X) = 100*eigen(cov(X))$values[1]^.5*nrow(X)^(-0.2).</p>
</td></tr>
<tr><td><code id="ncutdr_+3A_minsize">minsize</code></td>
<td>
<p>(optional) the minimum cluster size allowable when computing the normalised cut. if omitted then minsize = 1.</p>
</td></tr>
<tr><td><code id="ncutdr_+3A_verb">verb</code></td>
<td>
<p>(optional) verbosity level of optimisation procedure. verb==0 produces no output. verb==1 produces plots illustrating the progress of projection pursuit via plots of the projected data. verb==2 adds to these plots additional information about the progress. verb==3 creates a folder in working directory and stores all plots for verb==2. if omitted then verb==0.</p>
</td></tr>
<tr><td><code id="ncutdr_+3A_labels">labels</code></td>
<td>
<p>(optional) vector of class labels. not used in the actual projection pursuit. only used for illustrative purposes for values of verb&gt;0.</p>
</td></tr>
<tr><td><code id="ncutdr_+3A_maxit">maxit</code></td>
<td>
<p>(optional) maximum number of iterations in optimisation. if omitted then maxit=50.</p>
</td></tr>
<tr><td><code id="ncutdr_+3A_ftol">ftol</code></td>
<td>
<p>(optional) tolerance level for convergence of optimisation, based on relative function value improvements. if omitted then ftol = 1e-8.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list with class ppci_projection_solution with the following components
</p>
<table role = "presentation">
<tr><td><code>$projection</code></td>
<td>
<p>the num_dimensions x p projection matrix.</p>
</td></tr>
<tr><td><code>$fitted</code></td>
<td>
<p>the num_data x p projected data set.</p>
</td></tr>
<tr><td><code>$data</code></td>
<td>
<p>the input data matrix.</p>
</td></tr>
<tr><td><code>$method</code></td>
<td>
<p>==&quot;NCutH&quot;.</p>
</td></tr>
<tr><td><code>$params</code></td>
<td>
<p>list of parameters used to find $projection.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Hofmeyr, D. (2016) Clustering by Minimum Cut Hyperplanes. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, <b>39</b>(8), 1547 &ndash; 1560.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### not run
run = FALSE
if(run){
  ## load optidigits dataset
  data(optidigits)

  ## find nine dimensional projection (one fewer than
  ## the number of clusters, as is common in clustering)
  sol &lt;- ncutdr(optidigits$x, 9)

  ## visualise the solution via the first 3 pairs of dimensions
  plot(sol, pairs = 3, labels = optidigits$c)

  ## compare with PCA projection
  pairs(optidigits$x%*%eigen(cov(optidigits$x))$vectors[,1:3], col = optidigits$c)
  }
</code></pre>

<hr>
<h2 id='ncuth'>Minimum Normalised Cut Hyperplane</h2><span id='topic+ncuth'></span>

<h3>Description</h3>

<p>Finds minimum normalised cut hyperplane(s) for clustering.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ncuth(X, v0, s, minsize, verb, labels, maxit, ftol)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ncuth_+3A_x">X</code></td>
<td>
<p>a numeric matrix (num_data x num_dimensions); the dataset to be clustered.</p>
</td></tr>
<tr><td><code id="ncuth_+3A_v0">v0</code></td>
<td>
<p>(optional) initial projection direction(s). a matrix with ncol(X) rows. each column of v0 is used as an initialisation for projection pursuit. if omitted then a single initialisation is used; the first principal component.</p>
</td></tr>
<tr><td><code id="ncuth_+3A_s">s</code></td>
<td>
<p>(optional) positive numeric scaling parameter (sigma). if omitted then s = 100*eigen(cov(X))$values[1]^.5*nrow(X)^(-0.2).</p>
</td></tr>
<tr><td><code id="ncuth_+3A_minsize">minsize</code></td>
<td>
<p>(optional) the minimum cluster size allowable. if omitted then minsize = 1.</p>
</td></tr>
<tr><td><code id="ncuth_+3A_verb">verb</code></td>
<td>
<p>(optional) verbosity level of optimisation procedure. verb==0 produces no output. verb==1 produces plots illustrating the progress of projection pursuit via plots of the projected data. verb==2 adds to these plots additional information about the progress. verb==3 creates a folder in working directory and stores all plots for verb==2. if omitted then verb==0.</p>
</td></tr>
<tr><td><code id="ncuth_+3A_labels">labels</code></td>
<td>
<p>(optional) vector of class labels. not used in the actual clustering procedure. only used for illustrative purposes for values of verb&gt;0.</p>
</td></tr>
<tr><td><code id="ncuth_+3A_maxit">maxit</code></td>
<td>
<p>(optional) maximum number of iterations in optimisation. if omitted then maxit=50.</p>
</td></tr>
<tr><td><code id="ncuth_+3A_ftol">ftol</code></td>
<td>
<p>(optional) tolerance level for convergence of optimisation, based on relative function value improvements. if omitted then ftol = 1e-8.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named list with class ppci_hyperplane_solution with the following components
</p>
<table role = "presentation">
<tr><td><code>$cluster</code></td>
<td>
<p>cluster assignment vector.</p>
</td></tr>
<tr><td><code>$v</code></td>
<td>
<p>the optimal projection vector.</p>
</td></tr>
<tr><td><code>$b</code></td>
<td>
<p>the value of b making H(v, b) the minimum normalised cut hyperplane.</p>
</td></tr>
<tr><td><code>$fitted</code></td>
<td>
<p>data projected into two dimensional subspace defined by $v and the principal component in the null space of $v.</p>
</td></tr>
<tr><td><code>$data</code></td>
<td>
<p>the input data matrix.</p>
</td></tr>
<tr><td><code>$fval</code></td>
<td>
<p>the normalised cut across H(v, b).</p>
</td></tr>
<tr><td><code>$method</code></td>
<td>
<p>==&quot;NCutH&quot;.</p>
</td></tr>
<tr><td><code>$params</code></td>
<td>
<p>list of parameters used to find H(v, b).</p>
</td></tr>
<tr><td><code>$alternatives</code></td>
<td>
<p>an unnamed list. If more than one initilisation is considered, the alternatives to the best are stored in this field.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Hofmeyr, D. (2016) Clustering by Minimum Cut Hyperplanes. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, <b>39</b>(8), 1547 &ndash; 1560.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load breast cancer dataset
data(breastcancer)

## find minimum normalised cut hyperplane
sol &lt;- ncuth(breastcancer$x)

## visualise the solution
plot(sol)

## evaluate the performance of the solution
success_ratio(sol$cluster, breastcancer$c)
</code></pre>

<hr>
<h2 id='ncutpp'>Minimum Normalised Cut Projection Pursuit</h2><span id='topic+ncutpp'></span>

<h3>Description</h3>

<p>A gateway function between functions ncuth and ppclust.optim, the latter calls R's base optimisation function. Not intended for independent use.
</p>

<hr>
<h2 id='node_plot'>Visualise a Node in a Hierarchical Clustering Model</h2><span id='topic+node_plot'></span>

<h3>Description</h3>

<p>Provides a visualisation of the partition at an internal node (or of the potential partition at a leaf node) via a two-dimensional projection of the data assigned to the node.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>node_plot(sol, node, labels, transparency)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="node_plot_+3A_sol">sol</code></td>
<td>
<p>a clustering solution arising from one of the functions mcdc, mddc and ncutdc.</p>
</td></tr>
<tr><td><code id="node_plot_+3A_node">node</code></td>
<td>
<p>the node to be visualised. can be either an integer specifying the node number in sol$Nodes or a vector of length two specifying c(depth, position at depth) of the node.</p>
</td></tr>
<tr><td><code id="node_plot_+3A_labels">labels</code></td>
<td>
<p>(optional) a vector of class labels. if provided then points in different classes are plotted in different colours. the external validity of each partition is also indicated when this can be computed using true class labels.</p>
</td></tr>
<tr><td><code id="node_plot_+3A_transparency">transparency</code></td>
<td>
<p>(optional) if ommitted then points in the scatterplot are
shown as solid. If set to a value in (0, 1) then points are shown with transparency. Lower values correspond with a greater degree of transparency.</p>
</td></tr>
</table>

<hr>
<h2 id='norm_vec'>Euclidean Norm of a Vector</h2><span id='topic+norm_vec'></span>

<h3>Description</h3>

<p>Computes the Euclidean (L2) norm of a vector
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  norm_vec(v)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="norm_vec_+3A_v">v</code></td>
<td>
<p>a numeric vector</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the Euclidean norm of v.
</p>

<hr>
<h2 id='optidigits'>Optical Recognition of Handwritten Digits</h2><span id='topic+optidigits'></span>

<h3>Description</h3>

<p>This data set contains vectorised images of handwritten digits (0&ndash;9), compressed to 8x8 pixels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optidigits</code></pre>


<h3>Format</h3>

<p>A list with entries $x (a 5620x64 matrix with each row corresponding to an image) and $c (a vector of labels indicating the written digit).</p>


<h3>Source</h3>

<p>UCI Machine Learning Repository.</p>


<h3>References</h3>

<p>Lichman, M. (2013) UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science. <a href="https://archive.ics.uci.edu/ml">https://archive.ics.uci.edu/ml</a>
</p>

<hr>
<h2 id='optidigits_mean_images'>Visualise Cluster Means from optidigits data set</h2><span id='topic+optidigits_mean_images'></span>

<h3>Description</h3>

<p>Provides a visualisation of the cluster means computed from the optidigits data set, recast as images. Cluster labels are aligned with the true labels using simulated annealing to maximise the trace of the confusion matrix (or subset if
number of clusters != number of classes.)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optidigits_mean_images(clusters)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="optidigits_mean_images_+3A_clusters">clusters</code></td>
<td>
<p>a vector of cluster assignments. Must take values in 1:k, where k is the number of clusters.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Lichman, M. (2013) UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science. <a href="https://archive.ics.uci.edu/ml">https://archive.ics.uci.edu/ml</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### not run
run = FALSE
if(run){
  ## load optidigits dataset
  data(optidigits)

  ## obtain a clustering solution using normalised cut hyperplanes
  sol &lt;- ncutdc(optidigits$x, 10)

  ## visualise the cluster means as images
  optidigits_mean_images(sol$cluster)
  }
</code></pre>

<hr>
<h2 id='pendigits'>Pen-based Recognition of Handwritten Digits</h2><span id='topic+pendigits'></span>

<h3>Description</h3>

<p>This data set contains features derived from pen trajectories arising from handwritten digits (0&ndash;9) from 44 subjects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optidigits</code></pre>


<h3>Format</h3>

<p>A list with entries $x (a 10992x16 matrix with each row corresponding to a pen trajectory) and $c (a vector of labels indicating the written digit).</p>


<h3>Source</h3>

<p>UCI Machine Learning Repository.</p>


<h3>References</h3>

<p>Lichman, M. (2013) UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science. <a href="https://archive.ics.uci.edu/ml">https://archive.ics.uci.edu/ml</a>
</p>

<hr>
<h2 id='plot.ppci_cluster_solution'>Visualise a Hierarchical Clustering Model, or a Node Within a Hierarchical Clustering Model</h2><span id='topic+plot.ppci_cluster_solution'></span>

<h3>Description</h3>

<p>Provides a visualisation of a hierarchical clustering model via two-dimensional projections of the data assigned to each node. Alternatively a more detailed visualisation of a single node in a hierarchical model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ppci_cluster_solution'
plot(x, node = NULL, labels = NULL, node.numbers = NULL, transparency = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.ppci_cluster_solution_+3A_x">x</code></td>
<td>
<p>a clustering solution arising from one of the functions mcdc, mddc and ncutdc.</p>
</td></tr>
<tr><td><code id="plot.ppci_cluster_solution_+3A_node">node</code></td>
<td>
<p>(optional) integer corresponding to a node number (see node.numbers below). If provided then a detailed plot of the specified node is provided, instead of the entire cluster hierarchy.</p>
</td></tr>
<tr><td><code id="plot.ppci_cluster_solution_+3A_labels">labels</code></td>
<td>
<p>(optional) a vector of class labels. if provided then points in different classes are plotted in different colours.</p>
</td></tr>
<tr><td><code id="plot.ppci_cluster_solution_+3A_node.numbers">node.numbers</code></td>
<td>
<p>(optional) logical. if TRUE then numbers are added to the plot to indicate the order in which nodes were added to the model. if omitted then node.numbers = TRUE.</p>
</td></tr>
<tr><td><code id="plot.ppci_cluster_solution_+3A_transparency">transparency</code></td>
<td>
<p>(optional) if ommitted then points in scatterplots are
shown as solid. If set to a value in (0, 1) then points are shown with transparency. Lower values correspond with a greater degree of transparency.</p>
</td></tr>
<tr><td><code id="plot.ppci_cluster_solution_+3A_...">...</code></td>
<td>
<p>additional graphical parameters. Currently none implemented.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None</p>


<h3>See Also</h3>

<p>mcdc, mddc, ncutdc, tree_plot, node_plot</p>

<hr>
<h2 id='plot.ppci_hyperplane_solution'>Visualise a Hyperplane Separator for Clustering</h2><span id='topic+plot.ppci_hyperplane_solution'></span>

<h3>Description</h3>

<p>Provides a visualisation of the partition induced by a hyperplane separator generated by one of mch, mdh and ncuth.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ppci_hyperplane_solution'
plot(x, labels = NULL, transparency = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.ppci_hyperplane_solution_+3A_x">x</code></td>
<td>
<p>a solution arising from one of the functions mch, mdh and ncuth.</p>
</td></tr>
<tr><td><code id="plot.ppci_hyperplane_solution_+3A_labels">labels</code></td>
<td>
<p>(optional) a vector of class labels. if provided then points in different classes are plotted in different colours.</p>
</td></tr>
<tr><td><code id="plot.ppci_hyperplane_solution_+3A_transparency">transparency</code></td>
<td>
<p>(optional) if ommitted then points in the scatterplot are
shown as solid. If set to a value in (0, 1) then points are shown with transparency. Lower values correspond with a greater degree of transparency.</p>
</td></tr>
<tr><td><code id="plot.ppci_hyperplane_solution_+3A_...">...</code></td>
<td>
<p>additional graphical parameters. Currently none implemented.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None</p>


<h3>See Also</h3>

<p>mch, mdh, ncuth, hp_plot</p>

<hr>
<h2 id='plot.ppci_projection_solution'>Visualise a Data Set Projected from Projection Pursuit</h2><span id='topic+plot.ppci_projection_solution'></span>

<h3>Description</h3>

<p>Provides a visualisation of a data set using a low dimensional projection obtained using projection pursuit based on cluster based projection indices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ppci_projection_solution'
plot(x, labels = NULL, pairs = NULL, PCA = NULL, transparency = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.ppci_projection_solution_+3A_x">x</code></td>
<td>
<p>a solution arising from one of the functions mcdr, mddr and ncutdr.</p>
</td></tr>
<tr><td><code id="plot.ppci_projection_solution_+3A_labels">labels</code></td>
<td>
<p>(optional) a vector of class labels. if provided then points in different classes are plotted in different colours.</p>
</td></tr>
<tr><td><code id="plot.ppci_projection_solution_+3A_pairs">pairs</code></td>
<td>
<p>(optional) if omitted then the first two dimensions are shown. If an integer &gt; 1 then all pairs from the first 1:pairs dimensions are shown.</p>
</td></tr>
<tr><td><code id="plot.ppci_projection_solution_+3A_pca">PCA</code></td>
<td>
<p>(optional) logical. If TRUE then an additional principal component rotation is applied. This can make more clusters visible within a low dimensional projection.</p>
</td></tr>
<tr><td><code id="plot.ppci_projection_solution_+3A_transparency">transparency</code></td>
<td>
<p>(optional) if ommitted then points in scatterplots are
shown as solid. If set to a value in (0, 1) then points are shown with transparency. Lower values correspond with a greater degree of transparency.</p>
</td></tr>
<tr><td><code id="plot.ppci_projection_solution_+3A_...">...</code></td>
<td>
<p>additional graphical parameters. Currently none implemented.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None</p>


<h3>See Also</h3>

<p>mcdr, mddr, ncutdr</p>

<hr>
<h2 id='ppclust.optim'>Optimisation Call for Projection Pursuit Algorithms</h2><span id='topic+ppclust.optim'></span>

<h3>Description</h3>

<p>Provides a gateway to R's base optimisation function optim, with additional options as required by projection pursuit algorithms within the package. Not intended for independent use.
</p>

<hr>
<h2 id='subtree_width'>Determine the Largest Number of Nodes at Any Depth in a Clustering Hierarchy</h2><span id='topic+subtree_width'></span>

<h3>Description</h3>

<p>Used to specify the geometry of the plot region in the function tree_plot. Not intended for independent use.
</p>

<hr>
<h2 id='success_ratio'>Evaluate External Valifity os a Binary Partition</h2><span id='topic+success_ratio'></span>

<h3>Description</h3>

<p>Computes the success ratio of a binary partition by comparing the solution with true class labels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>success_ratio(assigned, labels)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="success_ratio_+3A_assigned">assigned</code></td>
<td>
<p>a vector of cluster assignments made by a clustering algorithm.</p>
</td></tr>
<tr><td><code id="success_ratio_+3A_labels">labels</code></td>
<td>
<p>a vector of true class labels to be compared with assigned.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the success ratio of the cluster assignment solution.
</p>


<h3>References</h3>

<p>Hofmeyr, D. (2016) Clustering by Minimum Cut Hyperplanes. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load optidigits dataset
data(optidigits)

## generate a binary partition using minimum normalised cut hyperplane
sol &lt;- ncuth(optidigits$x)

## evaluate using success ratio
success_ratio(sol$cluster, optidigits$c)
</code></pre>

<hr>
<h2 id='tree_plot'>Visualise a Hierarchical Clustering Model</h2><span id='topic+tree_plot'></span>

<h3>Description</h3>

<p>Provides a visualisation of a hierarchical clustering model via two-dimensional projections of the data assigned to each node.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tree_plot(sol, labels, node.numbers, transparency)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tree_plot_+3A_sol">sol</code></td>
<td>
<p>a clustering solution arising from one of the functions mcdc, mddc and ncutdc.</p>
</td></tr>
<tr><td><code id="tree_plot_+3A_labels">labels</code></td>
<td>
<p>(optional) a vector of class labels. if provided then points in different classes are plotted in different colours.</p>
</td></tr>
<tr><td><code id="tree_plot_+3A_node.numbers">node.numbers</code></td>
<td>
<p>(optional) logical. if TRUE then numbers are added to the plot to indicate the order in which nodes were added to the model. if omitted then node.numbers = TRUE.</p>
</td></tr>
<tr><td><code id="tree_plot_+3A_transparency">transparency</code></td>
<td>
<p>(optional) if ommitted then points in scatterplots are
shown as solid. If set to a value in (0, 1) then points are shown with transparency. Lower values correspond with a greater degree of transparency.</p>
</td></tr>
</table>

<hr>
<h2 id='tree_prune'>Prune a Hierarchical Clustering Model</h2><span id='topic+tree_prune'></span>

<h3>Description</h3>

<p>Removes the subtree rooted at the specified node from a hierarchical clustering model generated by one of mcdc, mddc and ncutdc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tree_prune(sol, node)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tree_prune_+3A_sol">sol</code></td>
<td>
<p>a clustering solution arising from one of the functions mcdc, mddc and ncutdc.</p>
</td></tr>
<tr><td><code id="tree_prune_+3A_node">node</code></td>
<td>
<p>the node at which to prune the hierarchy. can be either an integer specifying the node number in sol$nodes or a vector of length two specifying c(depth, position at depth) of the node.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with the same components as sol.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load the optidigits dataset
data(optidigits)

## cluster using minimum normalised cut hyperplanes,
## assuming no domain knowledge begin with 12 clusters
sol &lt;- ncutdc(optidigits$x, 12)

## the node numbered 11 has been split,
## yet it appears there may not be multiple clusters present.
## inspect this node more closely
plot(sol, node = 11)

## remove this node from the model
sol_new &lt;- tree_prune(sol, 11)

## compare the solutions using external cluster validity metrics

cluster_performance(sol$cluster, optidigits$c)

cluster_performance(sol_new$cluster, optidigits$c)
</code></pre>

<hr>
<h2 id='tree_split'>Split a Leaf in a Hierarchical Clustering Model</h2><span id='topic+tree_split'></span>

<h3>Description</h3>

<p>Adds an additional binary partition to an existing hierarchical clustering model produced by one of mcdc, mddc and ncutdc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tree_split(sol, node, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tree_split_+3A_sol">sol</code></td>
<td>
<p>a clustering solution arising from one of the functions mcdc, mddc and ncutdc.</p>
</td></tr>
<tr><td><code id="tree_split_+3A_node">node</code></td>
<td>
<p>the node to be further partitioned. can be either an integer specifying the node number in sol$nodes or a vector of length two specifying c(depth, position at depth) of the node.</p>
</td></tr>
<tr><td><code id="tree_split_+3A_...">...</code></td>
<td>
<p>any modifications to parameters used in optimisation. these should have the same names and types as the corresponding arguments for the method used to construct sol.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with the same components as sol. the $args field will reflect any changes included in ... above.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load the optidigits dataset
data(optidigits)

## cluster using minimum normalised cut hyperplanes,
## assuming no domain knowledge begin with 8 clusters
sol &lt;- ncutdc(optidigits$x, 8)

## visualise solution
plot(sol)

## node 13 shows evidence of multiple clusters. Inspect this node more closely
plot(sol, node = 13)

## split node 13
sol_new &lt;- tree_split(sol, 13)

## compare the solutions using external cluster validity metrics
cluster_performance(sol$cluster, optidigits$c)

cluster_performance(sol_new$cluster, optidigits$c)
</code></pre>

<hr>
<h2 id='yale'>Face Recognition</h2><span id='topic+yale'></span>

<h3>Description</h3>

<p>This data set contains vectorised images of the faces of 10 different human subjects with different poses and lighting conditions. The images were compressed to 30x20 pixels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>yale</code></pre>


<h3>Format</h3>

<p>A list with entries $x (a 2000x600 matrix with each row corresponding to an image) and $y (a vector of labels indicating the human subject).</p>


<h3>Source</h3>

<p>Yale Faces Database B. Compressed images (30x40) available from [https://cervisia.org/machine_learning_data.php/]. Further compression was performed by the package developers. In addition only the first 200 images of each subject are included.</p>


<h3>References</h3>

<p>Georghiades, A.S. and Belhumeur, P.N. and Kriegman, D.J. (2001) From Few to Many: Illumination Cone Models for Face Recognition under
Variable Lighting and Pose. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, <b>23</b>(6) pp. 643&ndash;660.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
