<!DOCTYPE html><html><head><title>Help for package plsdof</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {plsdof}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#benchmark.pls'><p>Comparison of model selection criteria for Partial Least Squares Regression.</p></a></li>
<li><a href='#benchmark.regression'><p>Comparison of Partial Least Squares Regression, Principal Components</p>
Regression and Ridge Regression.</a></li>
<li><a href='#coef.plsdof'><p>Regression coefficients</p></a></li>
<li><a href='#compute.lower.bound'><p>Lower bound for the Degrees of Freedom</p></a></li>
<li><a href='#dA'><p>Derivative of normalization function</p></a></li>
<li><a href='#dnormalize'><p>Derivative of normalization function</p></a></li>
<li><a href='#dvvtz'><p>First derivative of the projection operator</p></a></li>
<li><a href='#first.local.minimum'><p>Index of the first local minimum.</p></a></li>
<li><a href='#information.criteria'><p>Information criteria</p></a></li>
<li><a href='#kernel.pls.fit'><p>Kernel Partial Least Squares Fit</p></a></li>
<li><a href='#krylov'><p>Krylov sequence</p></a></li>
<li><a href='#linear.pls.fit'><p>Linear Partial Least Squares Fit</p></a></li>
<li><a href='#normalize'><p>Normalization of vectors</p></a></li>
<li><a href='#pcr'><p>Principal Components Regression</p></a></li>
<li><a href='#pcr.cv'><p>Model selection for Princinpal Components regression based on</p>
cross-validation</a></li>
<li><a href='#pls.cv'><p>Model selection for Partial Least Squares based on cross-validation</p></a></li>
<li><a href='#pls.dof'><p>Computation of the Degrees of Freedom</p></a></li>
<li><a href='#pls.ic'><p>Model selection for Partial Least Squares based on information criteria</p></a></li>
<li><a href='#pls.model'><p>Partial Least Squares</p></a></li>
<li><a href='#plsdof-package'><p>Degrees of Freedom and Statistical Inference for Partial Least Squares</p>
Regression</a></li>
<li><a href='#ridge.cv'><p>Ridge Regression.</p></a></li>
<li><a href='#tr'><p>Trace of a matrix</p></a></li>
<li><a href='#vcov.plsdof'><p>Variance-covariance matrix</p></a></li>
<li><a href='#vvtz'><p>Projectin operator</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Degrees of Freedom and Statistical Inference for Partial Least
Squares Regression</td>
</tr>
<tr>
<td>Depends:</td>
<td>MASS</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3-2</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-11-29</td>
</tr>
<tr>
<td>Author:</td>
<td>Nicole Kraemer, Mikio L. Braun</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Frederic Bertrand &lt;frederic.bertrand@utt.fr&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>The plsdof package provides Degrees of Freedom estimates
        for Partial Least Squares (PLS) Regression. Model selection for
        PLS is based on various information criteria (aic, bic, gmdl)
        or on cross-validation. Estimates for the mean and covariance
        of the PLS regression coefficients are available. They allow
        the construction of approximate confidence intervals and the
        application of test procedures (Kramer and Sugiyama 
        2012 &lt;<a href="https://doi.org/10.1198%2Fjasa.2011.tm10107">doi:10.1198/jasa.2011.tm10107</a>&gt;).
        Further, cross-validation procedures for Ridge Regression and 
        Principal Components Regression are available.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.1</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/fbertran/plsdof/">https://github.com/fbertran/plsdof/</a>,
<a href="https://fbertran.github.io/plsdof/">https://fbertran.github.io/plsdof/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/fbertran/plsdof/issues/">https://github.com/fbertran/plsdof/issues/</a></td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-11-29 14:31:14 UTC; fbertran</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-11-30 08:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='benchmark.pls'>Comparison of model selection criteria for Partial Least Squares Regression.</h2><span id='topic+benchmark.pls'></span>

<h3>Description</h3>

<p>This function computes the test error over several runs for different model
selection strategies.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>benchmark.pls(
  X,
  y,
  m = ncol(X),
  R = 20,
  ratio = 0.8,
  verbose = TRUE,
  k = 10,
  ratio.samples = 1,
  use.kernel = FALSE,
  criterion = "bic",
  true.coefficients = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="benchmark.pls_+3A_x">X</code></td>
<td>
<p>matrix of predictor observations.</p>
</td></tr>
<tr><td><code id="benchmark.pls_+3A_y">y</code></td>
<td>
<p>vector of response observations. The length of <code>y</code> is the same
as the number of rows of <code>X</code>.</p>
</td></tr>
<tr><td><code id="benchmark.pls_+3A_m">m</code></td>
<td>
<p>maximal number of Partial Least Squares components. Default is
<code>m=ncol(X)</code>.</p>
</td></tr>
<tr><td><code id="benchmark.pls_+3A_r">R</code></td>
<td>
<p>number of runs. Default is 20.</p>
</td></tr>
<tr><td><code id="benchmark.pls_+3A_ratio">ratio</code></td>
<td>
<p>ratio no of training examples/(no of training examples + no of
test examples). Default is 0.8</p>
</td></tr>
<tr><td><code id="benchmark.pls_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code>, the functions plots the progress of the
function. Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="benchmark.pls_+3A_k">k</code></td>
<td>
<p>number of cross-validation splits. Default is 10.</p>
</td></tr>
<tr><td><code id="benchmark.pls_+3A_ratio.samples">ratio.samples</code></td>
<td>
<p>Ratio of (no of training examples + no of test
examples)/<code>nrow(X)</code>. Default is 1.</p>
</td></tr>
<tr><td><code id="benchmark.pls_+3A_use.kernel">use.kernel</code></td>
<td>
<p>Use kernel representation? Default is
<code>use.kernel=FALSE</code>.</p>
</td></tr>
<tr><td><code id="benchmark.pls_+3A_criterion">criterion</code></td>
<td>
<p>Choice of the model selection criterion. One of the three
options aic, bic, gmdl. Default is &quot;bic&quot;.</p>
</td></tr>
<tr><td><code id="benchmark.pls_+3A_true.coefficients">true.coefficients</code></td>
<td>
<p>The vector of true regression coefficients (without
intercept), if available. Default is <code>NULL</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function estimates the optimal number of PLS components based on four
different criteria: (1) cross-validation, (2) information criteria with the
naive Degrees of Freedom DoF(m)=m+1, (3) information criteri with the
Degrees of Freedom computed via a Lanczos represenation of PLS and (4)
information criteri with the Degrees of Freedom computed via a Krylov
represenation of PLS. Note that the latter two options only differ with
respect to the estimation of the model error.
</p>
<p>In addition, the function computes the test error of the &quot;zero model&quot;, i.e.
<code>mean(y)</code> on the training data is used for prediction.
</p>
<p>If <code>true.coefficients</code> are available, the function also computes the
model error for the different methods, i.e. the sum of squared differences
between the true and the estimated regression coefficients.
</p>


<h3>Value</h3>

<table>
<tr><td><code>MSE</code></td>
<td>
<p>data frame of size R x 5. It contains the test error for
the five different methods for each of the R runs.</p>
</td></tr> <tr><td><code>M</code></td>
<td>
<p>data frame of
size R x 5. It contains the optimal number of components for the five
different methods for each of the R runs.</p>
</td></tr> <tr><td><code>DoF</code></td>
<td>
<p>data frame of size R x
5. It contains the Degrees of Freedom (corresponding to <code>M</code>) for the
five different methods for each of the R runs.</p>
</td></tr> <tr><td><code>TIME</code></td>
<td>
<p>data frame of
size R x 4. It contains the runtime for all methods (apart from the zero
model) for each of the R runs.</p>
</td></tr> <tr><td><code>M.CRASH</code></td>
<td>
<p>data frame of size R x 2. It
contains the number of components for which the Krylov representation and
the Lanczos representation return negative Degrees of Freedom, hereby
indicating numerical problems.</p>
</td></tr> <tr><td><code>ME</code></td>
<td>
<p>if <code>true.coefficients</code> are
available, this is a data frame of size R x 5. It contains the model error
for the five different methods for each of the R runs.</p>
</td></tr> <tr><td><code>SIGMAHAT</code></td>
<td>
<p>data
frame of size R x 5. It contains the estimation of the noise level provided
by the five different methods for each of the R runs.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nicole Kraemer
</p>


<h3>References</h3>

<p>Kraemer, N., Sugiyama M. (2011). &quot;The Degrees of Freedom of Partial Least
Squares Regression&quot;. Journal of the American Statistical Association 106
(494) <a href="https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107">https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pls.ic">pls.ic</a></code>, <code><a href="#topic+pls.cv">pls.cv</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# generate artificial data
n&lt;-50 # number of examples
p&lt;-5 # number of variables
X&lt;-matrix(rnorm(n*p),ncol=p)
true.coefficients&lt;-runif(p,1,3)
y&lt;-X%*%true.coefficients + rnorm(n,0,5)
my.benchmark&lt;-benchmark.pls(X,y,R=10,true.coefficients=true.coefficients)

</code></pre>

<hr>
<h2 id='benchmark.regression'>Comparison of Partial Least Squares Regression, Principal Components
Regression and Ridge Regression.</h2><span id='topic+benchmark.regression'></span>

<h3>Description</h3>

<p>This function computes the test error over several runs for (a) PLS, (b) PCR
(c) Ridge Regression and (d) the null model, that is the mean of <code>y</code>.
In the first three cases, the optimal model is selected via
cross-validation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>benchmark.regression(
  X,
  y,
  m = ncol(X),
  R = 20,
  ratio = 0.8,
  verbose = TRUE,
  k = 10,
  nsamples = nrow(X),
  use.kernel = FALSE,
  supervised = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="benchmark.regression_+3A_x">X</code></td>
<td>
<p>matrix of predictor observations.</p>
</td></tr>
<tr><td><code id="benchmark.regression_+3A_y">y</code></td>
<td>
<p>vector of response observations. The length of <code>y</code> is the same
as the number of rows of <code>X</code>.</p>
</td></tr>
<tr><td><code id="benchmark.regression_+3A_m">m</code></td>
<td>
<p>maximal number of components for PLS. Default is <code>m=ncol(X)</code>.</p>
</td></tr>
<tr><td><code id="benchmark.regression_+3A_r">R</code></td>
<td>
<p>number of runs. Default is 20.</p>
</td></tr>
<tr><td><code id="benchmark.regression_+3A_ratio">ratio</code></td>
<td>
<p>ratio no of training examples/(no of training examples + no of
test examples). Default is 0.8</p>
</td></tr>
<tr><td><code id="benchmark.regression_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code>, the functions plots the progress of the
function. Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="benchmark.regression_+3A_k">k</code></td>
<td>
<p>number of cross-validation splits. Default is 10.</p>
</td></tr>
<tr><td><code id="benchmark.regression_+3A_nsamples">nsamples</code></td>
<td>
<p>number of data points. Default is <code>nrow(X)</code>.</p>
</td></tr>
<tr><td><code id="benchmark.regression_+3A_use.kernel">use.kernel</code></td>
<td>
<p>Use kernel representation for PLS? Default is
<code>use.kernel=FALSE</code>.</p>
</td></tr>
<tr><td><code id="benchmark.regression_+3A_supervised">supervised</code></td>
<td>
<p>Should the principal components be sorted by decreasing
squared correlation to the response? Default is FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function computes the test error, the cross-validation-optimal model
parameters, their corresponding Degrees of Freedom, and the
sum-of-squared-residuals (SSR) for PLS and PCR.
</p>


<h3>Value</h3>

<table>
<tr><td><code>MSE</code></td>
<td>
<p>data frame of size R x 4. It contains the test error for
the four different methods for each of the R runs.</p>
</td></tr> <tr><td><code>M</code></td>
<td>
<p>data frame of
size R x 4. It contains the optimal model parameters for the four different
methods for each of the R runs.</p>
</td></tr> <tr><td><code>DoF</code></td>
<td>
<p>data frame of size R x 4. It
contains the Degrees of Freedom (corresponding to <code>M</code>) for the four
different methods for each of the R runs.</p>
</td></tr> <tr><td><code>res.pls</code></td>
<td>
<p>matrix of size R x
(ncol(X+1)). It contains the SSR for PLS for each of the R runs.</p>
</td></tr>
<tr><td><code>res.pcr</code></td>
<td>
<p>matrix of size R x (ncol(X+1)). It contains the SSR for PCR
for each of the R runs.</p>
</td></tr> <tr><td><code>DoF.all</code></td>
<td>
<p>matrix of size R x (ncol(X+1)). It
contains the Degrees of Freedom for PLS for all components for each of the R
runs.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nicole Kraemer
</p>


<h3>References</h3>

<p>Kraemer, N., Sugiyama M. (2011). &quot;The Degrees of Freedom of Partial Least
Squares Regression&quot;. Journal of the American Statistical Association 106
(494) <a href="https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107">https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pls.cv">pls.cv</a></code>, <code><a href="#topic+pcr.cv">pcr.cv</a></code>,
<code><a href="#topic+benchmark.pls">benchmark.pls</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# Boston Housing data
library(MASS)
data(Boston)
X&lt;-as.matrix(Boston[,1:4]) # select the first 3 columns as predictor variables
y&lt;-as.vector(Boston[,14])

my.benchmark&lt;-benchmark.regression(X,y,ratio=0.5,R=10,k=5)

# boxplot of the mean squared error

boxplot(my.benchmark$MSE,outline=FALSE)

# boxplot of the degrees of freedom, without the null model

boxplot(my.benchmark$DoF[,-4])


</code></pre>

<hr>
<h2 id='coef.plsdof'>Regression coefficients</h2><span id='topic+coef.plsdof'></span>

<h3>Description</h3>

<p>This function returns the regression coefficients of a plsdof-object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'plsdof'
coef(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef.plsdof_+3A_object">object</code></td>
<td>
<p>an object of class &quot;plsdof&quot; that is returned by the functions
<code>pls.ic</code> and <code>pls.cv</code>.</p>
</td></tr>
<tr><td><code id="coef.plsdof_+3A_...">...</code></td>
<td>
<p>additional parameters</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function returns the regression coefficients (without intercept) for the
optimal number of components.
</p>


<h3>Value</h3>

<p>regression coefficients.
</p>


<h3>Author(s)</h3>

<p>Nicole Kraemer
</p>


<h3>References</h3>

<p>Kraemer, N., Sugiyama M. (2011). &quot;The Degrees of Freedom of Partial Least
Squares Regression&quot;. Journal of the American Statistical Association 106
(494) <a href="https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107">https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107</a>
</p>
<p>Kraemer, N., Braun, M.L. (2007) &quot;Kernelizing PLS, Degrees of Freedom, and
Efficient Model Selection&quot;, Proceedings of the 24th International Conference
on Machine Learning, Omni Press, 441 - 448
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vcov.plsdof">vcov.plsdof</a></code>, <code><a href="#topic+pls.model">pls.model</a></code>,
<code><a href="#topic+pls.ic">pls.ic</a></code>, <code><a href="#topic+pls.cv">pls.cv</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

n&lt;-50 # number of observations
p&lt;-5 # number of variables
X&lt;-matrix(rnorm(n*p),ncol=p)
y&lt;-rnorm(n)


pls.object&lt;-pls.ic(X,y,criterion="bic")
mycoef&lt;-coef(pls.object)

</code></pre>

<hr>
<h2 id='compute.lower.bound'>Lower bound for the Degrees of Freedom</h2><span id='topic+compute.lower.bound'></span>

<h3>Description</h3>

<p>This function computes the lower bound for the the Degrees of Freedom of PLS
with 1 component.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute.lower.bound(X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute.lower.bound_+3A_x">X</code></td>
<td>
<p>matrix of predictor observations.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the decay of the eigenvalues of <code>cor(X)</code> is not too fast, we can
lower-bound the Degrees of Freedom of PLS with 1 component. Note that we
implicitly assume that we use scaled predictor variables to compute the PLS
solution.
</p>


<h3>Value</h3>

<table>
<tr><td><code>bound</code></td>
<td>
<p>logical. bound is <code>TRUE</code> if the decay of the
eigenvalues is slow enough</p>
</td></tr> <tr><td><code>lower.bound</code></td>
<td>
<p>if bound is TRUE, this is the
lower bound, otherwise, it is set to -1</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nicole Kraemer
</p>


<h3>References</h3>

<p>Kraemer, N., Sugiyama M. (2011). &quot;The Degrees of Freedom of Partial Least
Squares Regression&quot;. Journal of the American Statistical Association 106
(494) <a href="https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107">https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pls.model">pls.model</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Boston Housing data
library(MASS)
data(Boston)
X&lt;-Boston[,-14]
my.lower&lt;-compute.lower.bound(X)

</code></pre>

<hr>
<h2 id='dA'>Derivative of normalization function</h2><span id='topic+dA'></span>

<h3>Description</h3>

<p>This function computes the derivative of the function </p>
<p style="text-align: center;"><code class="reqn">v\mapsto
\frac{w}{\|w\|_A}</code>
</p>
<p> with respect to y.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dA(w, A, dw)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dA_+3A_w">w</code></td>
<td>
<p>vector of length n.</p>
</td></tr>
<tr><td><code id="dA_+3A_a">A</code></td>
<td>
<p>square matrix that defines the norm</p>
</td></tr>
<tr><td><code id="dA_+3A_dw">dw</code></td>
<td>
<p>derivative of w with respect to y. As y is a vector of length n,
the derivative is a matrix of size nxn.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The first derivative of the normalization operator is
</p>
<p style="text-align: center;"><code class="reqn">\frac{\partial}{\partial y}\left(w\mapsto
\frac{w}{\|w\|_A}\right)=\frac{1}{\|w\|}\left(I_n - \frac{w w^ \top
A}{w^\top w}\right) \frac{\partial w}{\partial y}</code>
</p>



<h3>Value</h3>

<p>the Jacobian matrix of the normalization function. This is a matrix
of size nxn.
</p>


<h3>Author(s)</h3>

<p>Nicole Kraemer
</p>


<h3>References</h3>

<p>Kraemer, N., Sugiyama M. (2011). &quot;The Degrees of Freedom of
Partial Least Squares Regression&quot;. Journal of the American Statistical
Association 106 (494)
<a href="https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107">https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107</a>
</p>
<p>Kraemer, N., Braun, M.L. (2007) &quot;Kernelizing PLS, Degrees of Freedom, and
Efficient Model Selection&quot;, Proceedings of the 24th International Conference
on Machine Learning, Omni Press, 441 - 448
</p>


<h3>See Also</h3>

<p><code><a href="#topic+normalize">normalize</a></code>, <code><a href="#topic+dnormalize">dnormalize</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
w&lt;-rnorm(15)
dw&lt;-diag(15)
A&lt;-diag(1:15)
d.object&lt;-dA(w,A,dw)

</code></pre>

<hr>
<h2 id='dnormalize'>Derivative of normalization function</h2><span id='topic+dnormalize'></span>

<h3>Description</h3>

<p>This function computes the derivative of the function </p>
<p style="text-align: center;"><code class="reqn">v\mapsto
\frac{v}{\|v\|}</code>
</p>
<p> with respect to y.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dnormalize(v, dv)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dnormalize_+3A_v">v</code></td>
<td>
<p>vector of length n.</p>
</td></tr>
<tr><td><code id="dnormalize_+3A_dv">dv</code></td>
<td>
<p>derivative of v with respect to y. As y is a vector of length n,
the derivative is a matrix of size nxn.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The first derivative of the normalization operator is
</p>
<p style="text-align: center;"><code class="reqn">\frac{\partial}{\partial y}\left(v\mapsto
\frac{v}{\|v\|}\right)=\frac{1}{\|v\|}\left(I_n - \frac{v v^ \top}{v^\top
v}\right) \frac{\partial v}{\partial y}</code>
</p>



<h3>Value</h3>

<p>the Jacobian matrix of the normalization function. This is a matrix
of size nxn.
</p>


<h3>Author(s)</h3>

<p>Nicole Kraemer, Mikio L. Braun
</p>


<h3>References</h3>

<p>Kraemer, N., Sugiyama M. (2011). &quot;The Degrees of Freedom of
Partial Least Squares Regression&quot;. Journal of the American Statistical
Association 106 (494)
<a href="https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107">https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107</a>
</p>
<p>Kraemer, N., Braun, M.L. (2007) &quot;Kernelizing PLS, Degrees of Freedom, and
Efficient Model Selection&quot;, Proceedings of the 24th International Conference
on Machine Learning, Omni Press, 441 - 448
</p>


<h3>See Also</h3>

<p><code><a href="#topic+normalize">normalize</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
v&lt;-rnorm(15)
dv&lt;-diag(15)
d.object&lt;-dnormalize(v,dv)

</code></pre>

<hr>
<h2 id='dvvtz'>First derivative of the projection operator</h2><span id='topic+dvvtz'></span>

<h3>Description</h3>

<p>This function computes the first derivative of the projection operator
</p>
<p style="text-align: center;"><code class="reqn">P_V z= V V^\top z</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>dvvtz(v, z, dv, dz)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dvvtz_+3A_v">v</code></td>
<td>
<p>orthonormal basis of the space on which <code>z</code> is projected.
<code>v</code> is either a matrix or a vector.</p>
</td></tr>
<tr><td><code id="dvvtz_+3A_z">z</code></td>
<td>
<p>vector that is projected onto the columns of <code>v</code></p>
</td></tr>
<tr><td><code id="dvvtz_+3A_dv">dv</code></td>
<td>
<p>first derivative of the the columns of <code>v</code> with respect to a
vector y. If <code>v</code> is a matrix, <code>dv</code> is an array of dimension
<code>ncol(v)</code>x<code>nrow(v)</code>x<code>length(y)</code>. If <code>v</code> is a vector,
<code>dv</code> is a matrix of dimension <code>nrow(v)</code>x<code>length(y)</code>.</p>
</td></tr>
<tr><td><code id="dvvtz_+3A_dz">dz</code></td>
<td>
<p>first derivative of <code>z</code> with respect to a vector y. This is a
matrix of dimension <code>nrow(v)</code>x<code>length(y)</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For the computation of the first derivative, we assume that the columns of
<code>v</code> are normalized and mutually orthogonal. (Note that the function
will not return an error message if these assumptionsa are not fulfilled. If
we denote the columns of <code>v</code> by <code class="reqn">v_1,\ldots,v_l</code>, the first
derivative of the projection operator is </p>
<p style="text-align: center;"><code class="reqn"> \frac{\partial P}{\partial
y}=\sum_{j=1} ^ l \left[ \left(v_j z^ \top + v_j^ \top z I_n
\right)\frac{\partial v_j}{\partial y} + v_j v_j ^ \top \frac{\partial
z}{\partial y}\right] </code>
</p>
<p> Here, n denotes the length of the vectors <code class="reqn">v_j</code>.
</p>


<h3>Value</h3>

<p>The first derivative of the projection operator with respect to y.
This is a matrix of dimension <code>nrow(v)</code>x<code>length(y)</code>.
</p>


<h3>Note</h3>

<p>This is an internal function.
</p>


<h3>Author(s)</h3>

<p>Nicole Kraemer, Mikio L. Braun
</p>


<h3>References</h3>

<p>Kraemer, N., Sugiyama M. (2011). &quot;The Degrees of Freedom of
Partial Least Squares Regression&quot;. Journal of the American Statistical
Association. 106 (494)
<a href="https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107">https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107</a>
</p>
<p>Kraemer, N., Braun, M.L. (2007) &quot;Kernelizing PLS, Degrees of Freedom, and
Efficient Model Selection&quot;, Proceedings of the 24th International Conference
on Machine Learning, Omni Press, 441 - 448
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vvtz">vvtz</a></code>
</p>

<hr>
<h2 id='first.local.minimum'>Index of the first local minimum.</h2><span id='topic+first.local.minimum'></span>

<h3>Description</h3>

<p>This function computes the index of the first local minimum.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>first.local.minimum(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="first.local.minimum_+3A_x">x</code></td>
<td>
<p>vector.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the index of the first local minimum of <code>x</code>.
</p>


<h3>Author(s)</h3>

<p>Nicole Kraemer
</p>


<h3>References</h3>

<p>Kraemer, N., Sugiyama M. (2011). &quot;The Degrees of Freedom of
Partial Least Squares Regression&quot;. Journal of the American Statistical
Association. ahead of print 106 (494)
<a href="https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107">https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
v&lt;-rnorm(30)
out&lt;-first.local.minimum(v)

</code></pre>

<hr>
<h2 id='information.criteria'>Information criteria</h2><span id='topic+information.criteria'></span>

<h3>Description</h3>

<p>This function computes the optimal model parameters using three different
model selection criteria (aic, bic, gmdl).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>information.criteria(RSS, DoF, yhat = NULL, sigmahat, n, criterion = "bic")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="information.criteria_+3A_rss">RSS</code></td>
<td>
<p>vector of residual sum of squares.</p>
</td></tr>
<tr><td><code id="information.criteria_+3A_dof">DoF</code></td>
<td>
<p>vector of Degrees of Freedom. The length of <code>DoF</code> is the
same as the length of <code>RSS</code>.</p>
</td></tr>
<tr><td><code id="information.criteria_+3A_yhat">yhat</code></td>
<td>
<p>vector of squared norm of yhat. The length of <code>yhat</code> is the
same as the length of <code>RSS</code>. It is only needed for gmdl. Default value
is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="information.criteria_+3A_sigmahat">sigmahat</code></td>
<td>
<p>Estimated model error. The length of <code>sigmahat</code> is the
same as the length of <code>RSS</code>.</p>
</td></tr>
<tr><td><code id="information.criteria_+3A_n">n</code></td>
<td>
<p>number of observations.</p>
</td></tr>
<tr><td><code id="information.criteria_+3A_criterion">criterion</code></td>
<td>
<p>one of the options &quot;aic&quot;, &quot;bic&quot; and &quot;gmdl&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Akaike information criterion (aic) is defined as </p>
<p style="text-align: center;"><code class="reqn">{aic}=
\frac{{RSS}}{n} + 2\frac{{DoF}}{n} \sigma^ 2\,.</code>
</p>
<p> The Bayesian information
criterion (bic) is defined as </p>
<p style="text-align: center;"><code class="reqn">{bic}= \frac{{RSS}}{n} +
log(n)\frac{{DoF}}{n} \sigma^ 2\,.</code>
</p>
<p> The generalized minimum description
length (gmdl) is defined as
</p>
<p style="text-align: center;"><code class="reqn">gmdl=\frac{n}{2}log(S)+\frac{DoF}{2}log(F)+\frac{1}{2}log(n)</code>
</p>
<p> with
</p>
<p style="text-align: center;"><code class="reqn">S=\hat \sigma ^2</code>
</p>
<p> Note that it is also possible to use the function
<code>information.criteria</code> for other regression methods than Partial Least
Squares.
</p>


<h3>Value</h3>

<table>
<tr><td><code>DoF</code></td>
<td>
<p>degrees of freedom</p>
</td></tr> <tr><td><code>score</code></td>
<td>
<p>vector of the model
selection criterion</p>
</td></tr> <tr><td><code>par</code></td>
<td>
<p>index of the first local minimum of
<code>score</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nicole Kraemer, Mikio Braun
</p>


<h3>References</h3>

<p>Akaikie, H. (1973) &quot;Information Theory and an Extension of the
Maximum Likelihood Principle&quot;. Second International Symposium on Information
Theory, 267 - 281.
</p>
<p>Hansen, M., Yu, B. (2001). &quot;Model Selection and Minimum Descripion Length
Principle&quot;. Journal of the American Statistical Association, 96, 746 - 774
</p>
<p>Kraemer, N., Sugiyama M. (2011). &quot;The Degrees of Freedom of Partial Least
Squares Regression&quot;. Journal of the American Statistical Association 106
(494) <a href="https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107">https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107</a>
</p>
<p>Kraemer, N., Braun, M.L. (2007) &quot;Kernelizing PLS, Degrees of Freedom, and
Efficient Model Selection&quot;, Proceedings of the 24th International Conference
on Machine Learning, Omni Press, 441 - 448
</p>
<p>Schwartz, G. (1979) &quot;Estimating the Dimension of a Model&quot; Annals of
Statistics 26(5), 1651 - 1686.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pls.ic">pls.ic</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## This is an internal function called by pls.ic


</code></pre>

<hr>
<h2 id='kernel.pls.fit'>Kernel Partial Least Squares Fit</h2><span id='topic+kernel.pls.fit'></span>

<h3>Description</h3>

<p>This function computes the Partial Least Squares fit. This algorithm scales
mainly in the number of observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kernel.pls.fit(
  X,
  y,
  m = ncol(X),
  compute.jacobian = FALSE,
  DoF.max = min(ncol(X) + 1, nrow(X) - 1)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kernel.pls.fit_+3A_x">X</code></td>
<td>
<p>matrix of predictor observations.</p>
</td></tr>
<tr><td><code id="kernel.pls.fit_+3A_y">y</code></td>
<td>
<p>vector of response observations. The length of <code>y</code> is the same
as the number of rows of <code>X</code>.</p>
</td></tr>
<tr><td><code id="kernel.pls.fit_+3A_m">m</code></td>
<td>
<p>maximal number of Partial Least Squares components. Default is
<code>m</code>=ncol(X).</p>
</td></tr>
<tr><td><code id="kernel.pls.fit_+3A_compute.jacobian">compute.jacobian</code></td>
<td>
<p>Should the first derivative of the regression
coefficients be computed as well? Default is <code>FALSE</code></p>
</td></tr>
<tr><td><code id="kernel.pls.fit_+3A_dof.max">DoF.max</code></td>
<td>
<p>upper bound on the Degrees of Freedom. Default is
<code>min(ncol(X)+1,nrow(X)-1)</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>We first standardize <code>X</code> to zero mean and unit variance.
</p>


<h3>Value</h3>

<table>
<tr><td><code>coefficients</code></td>
<td>
<p>matrix of regression coefficients</p>
</td></tr>
<tr><td><code>intercept</code></td>
<td>
<p>vector of regression intercepts</p>
</td></tr> <tr><td><code>DoF</code></td>
<td>
<p>Degrees of
Freedom</p>
</td></tr> <tr><td><code>sigmahat</code></td>
<td>
<p>vector of estimated model error</p>
</td></tr> <tr><td><code>Yhat</code></td>
<td>
<p>matrix
of fitted values</p>
</td></tr> <tr><td><code>yhat</code></td>
<td>
<p>vector of squared length of fitted values</p>
</td></tr>
<tr><td><code>RSS</code></td>
<td>
<p>vector of residual sum of error</p>
</td></tr> <tr><td><code>covariance</code></td>
<td>
<p><code>NULL</code>
object.</p>
</td></tr> <tr><td><code>TT</code></td>
<td>
<p>matrix of normalized PLS components</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nicole Kraemer, Mikio L. Braun
</p>


<h3>References</h3>

<p>Kraemer, N., Sugiyama M. (2011). &quot;The Degrees of Freedom of
Partial Least Squares Regression&quot;. Journal of the American Statistical
Association 106 (494)
<a href="https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107">https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107</a>
</p>
<p>Kraemer, N., Braun, M.L. (2007) &quot;Kernelizing PLS, Degrees of Freedom, and
Efficient Model Selection&quot;, Proceedings of the 24th International Conference
on Machine Learning, Omni Press, 441 - 448
</p>


<h3>See Also</h3>

<p><code><a href="#topic+linear.pls.fit">linear.pls.fit</a></code>,
<code><a href="#topic+pls.cv">pls.cv</a></code>,<code><a href="#topic+pls.model">pls.model</a></code>, <code><a href="#topic+pls.ic">pls.ic</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n&lt;-50 # number of observations
p&lt;-5 # number of variables
X&lt;-matrix(rnorm(n*p),ncol=p)
y&lt;-rnorm(n)


pls.object&lt;-kernel.pls.fit(X,y,m=5,compute.jacobian=TRUE)



</code></pre>

<hr>
<h2 id='krylov'>Krylov sequence</h2><span id='topic+krylov'></span>

<h3>Description</h3>

<p>This function computes the Krylov sequence of a matrix and a vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>krylov(A, b, m)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="krylov_+3A_a">A</code></td>
<td>
<p>square matrix of dimension p x p.</p>
</td></tr>
<tr><td><code id="krylov_+3A_b">b</code></td>
<td>
<p>vector of length p</p>
</td></tr>
<tr><td><code id="krylov_+3A_m">m</code></td>
<td>
<p>length of the Krylov sequence</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix of size p x m containing the sequence b,Ab,..., A^(m-1)b.
</p>


<h3>Author(s)</h3>

<p>Nicole Kraemer
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
A&lt;-matrix(rnorm(8*8),ncol=8)
b&lt;-rnorm(8)
K&lt;-krylov(A,b,4)

</code></pre>

<hr>
<h2 id='linear.pls.fit'>Linear Partial Least Squares Fit</h2><span id='topic+linear.pls.fit'></span>

<h3>Description</h3>

<p>This function computes the Partial Least Squares solution and the first
derivative of the regression coefficients. This implementation scales mostly
in the number of variables
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linear.pls.fit(
  X,
  y,
  m = ncol(X),
  compute.jacobian = FALSE,
  DoF.max = min(ncol(X) + 1, nrow(X) - 1)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="linear.pls.fit_+3A_x">X</code></td>
<td>
<p>matrix of predictor observations.</p>
</td></tr>
<tr><td><code id="linear.pls.fit_+3A_y">y</code></td>
<td>
<p>vector of response observations. The length of <code>y</code> is the same
as the number of rows of <code>X</code>.</p>
</td></tr>
<tr><td><code id="linear.pls.fit_+3A_m">m</code></td>
<td>
<p>maximal number of Partial Least Squares components. Default is
<code>m</code>=ncol(X).</p>
</td></tr>
<tr><td><code id="linear.pls.fit_+3A_compute.jacobian">compute.jacobian</code></td>
<td>
<p>Should the first derivative of the regression
coefficients be computed as well? Default is <code>FALSE</code></p>
</td></tr>
<tr><td><code id="linear.pls.fit_+3A_dof.max">DoF.max</code></td>
<td>
<p>upper bound on the Degrees of Freedom. Default is
<code>min(ncol(X)+1,nrow(X)-1)</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>We first standardize <code>X</code> to zero mean and unit variance.
</p>


<h3>Value</h3>

<table>
<tr><td><code>coefficients</code></td>
<td>
<p>matrix of regression coefficients</p>
</td></tr>
<tr><td><code>intercept</code></td>
<td>
<p>vector of regression intercepts</p>
</td></tr> <tr><td><code>DoF</code></td>
<td>
<p>Degrees of
Freedom</p>
</td></tr> <tr><td><code>sigmahat</code></td>
<td>
<p>vector of estimated model error</p>
</td></tr> <tr><td><code>Yhat</code></td>
<td>
<p>matrix
of fitted values</p>
</td></tr> <tr><td><code>yhat</code></td>
<td>
<p>vector of squared length of fitted values</p>
</td></tr>
<tr><td><code>RSS</code></td>
<td>
<p>vector of residual sum of error</p>
</td></tr> </table>
<p><code>covariance</code>if
<code>compute.jacobian</code> is <code>TRUE</code>, the function returns the array of
covariance matrices for the PLS regression coefficients. </p>
<table>
<tr><td><code>TT</code></td>
<td>
<p>matrix of
normalized PLS components</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nicole Kraemer
</p>


<h3>References</h3>

<p>Kraemer, N., Sugiyama M. (2011). &quot;The Degrees of Freedom of
Partial Least Squares Regression&quot;. Journal of the American Statistical
Association 106 (494)
<a href="https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107">https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kernel.pls.fit">kernel.pls.fit</a></code>,
<code><a href="#topic+pls.cv">pls.cv</a></code>,<code><a href="#topic+pls.model">pls.model</a></code>, <code><a href="#topic+pls.ic">pls.ic</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n&lt;-50 # number of observations
p&lt;-5 # number of variables
X&lt;-matrix(rnorm(n*p),ncol=p)
y&lt;-rnorm(n)

pls.object&lt;-linear.pls.fit(X,y,m=5,compute.jacobian=TRUE)

</code></pre>

<hr>
<h2 id='normalize'>Normalization of vectors</h2><span id='topic+normalize'></span>

<h3>Description</h3>

<p>Normalization of vectors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>normalize(v, w = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="normalize_+3A_v">v</code></td>
<td>
<p>vector</p>
</td></tr>
<tr><td><code id="normalize_+3A_w">w</code></td>
<td>
<p>optional vector</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The vector <code>v</code> is normalized to length 1. If <code>w</code> is given, it is
normalized by the length of <code>v</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>v</code></td>
<td>
<p>normalized <code>v</code></p>
</td></tr> <tr><td><code>w</code></td>
<td>
<p>normalized <code>w</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nicole Kraemer, Mikio L. Braun
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
v&lt;-rnorm(5)
w&lt;-rnorm(10)
dummy&lt;-normalize(v,w)

</code></pre>

<hr>
<h2 id='pcr'>Principal Components Regression</h2><span id='topic+pcr'></span>

<h3>Description</h3>

<p>This function computes the Principal Components Regression (PCR) fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pcr(
  X,
  y,
  scale = TRUE,
  m = min(ncol(X), nrow(X) - 1),
  eps = 1e-06,
  supervised = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pcr_+3A_x">X</code></td>
<td>
<p>matrix of predictor observations.</p>
</td></tr>
<tr><td><code id="pcr_+3A_y">y</code></td>
<td>
<p>vector of response observations. The length of <code>y</code> is the same
as the number of rows of <code>X</code>.</p>
</td></tr>
<tr><td><code id="pcr_+3A_scale">scale</code></td>
<td>
<p>Should the predictor variables be scaled to unit variance?
Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="pcr_+3A_m">m</code></td>
<td>
<p>maximal number of principal components. Default is
<code>m=min(ncol(X),nrow(X)-1)</code>.</p>
</td></tr>
<tr><td><code id="pcr_+3A_eps">eps</code></td>
<td>
<p>precision. Eigenvalues of the correlation matrix of <code>X</code> that
are smaller than <code>eps</code> are set to 0. The default value is
<code>eps=10^{-6}.</code></p>
</td></tr>
<tr><td><code id="pcr_+3A_supervised">supervised</code></td>
<td>
<p>Should the principal components be sorted by decreasing
squared correlation to the response? Default is FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function first scales all predictor variables to unit variance, and then
computes the PCR fit for all components. Is <code>supervised=TRUE</code>, we sort
the principal correlation according to the squared correlation to the
response.
</p>


<h3>Value</h3>

<table>
<tr><td><code>coefficients</code></td>
<td>
<p>matrix of regression coefficients, including the
coefficients of the null model, i.e. the constant model <code>mean(y)</code>. </p>
</td></tr>
<tr><td><code>intercept</code></td>
<td>
<p>vector of intercepts, including the intercept of the null
model, i.e. the constant model <code>mean(y)</code>. </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nicole Kraemer
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pcr.cv">pcr.cv</a></code>, <code><a href="#topic+pls.cv">pls.cv</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n&lt;-50 # number of observations
p&lt;-15 # number of variables
X&lt;-matrix(rnorm(n*p),ncol=p)
y&lt;-rnorm(n)

my.pcr&lt;-pcr(X,y,m=10)


</code></pre>

<hr>
<h2 id='pcr.cv'>Model selection for Princinpal Components regression based on
cross-validation</h2><span id='topic+pcr.cv'></span>

<h3>Description</h3>

<p>This function computes the optimal model parameter using cross-validation.
Mdel selection is based on mean squared error and correlation to the
response, respectively.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pcr.cv(
  X,
  y,
  k = 10,
  m = min(ncol(X), nrow(X) - 1),
  groups = NULL,
  scale = TRUE,
  eps = 1e-06,
  plot.it = FALSE,
  compute.jackknife = TRUE,
  method.cor = "pearson",
  supervised = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pcr.cv_+3A_x">X</code></td>
<td>
<p>matrix of predictor observations.</p>
</td></tr>
<tr><td><code id="pcr.cv_+3A_y">y</code></td>
<td>
<p>vector of response observations. The length of <code>y</code> is the same
as the number of rows of <code>X</code>.</p>
</td></tr>
<tr><td><code id="pcr.cv_+3A_k">k</code></td>
<td>
<p>number of cross-validation splits. Default is 10.</p>
</td></tr>
<tr><td><code id="pcr.cv_+3A_m">m</code></td>
<td>
<p>maximal number of principal components. Default is
<code>m=min(ncol(X),nrow(X)-1)</code>.</p>
</td></tr>
<tr><td><code id="pcr.cv_+3A_groups">groups</code></td>
<td>
<p>an optional vector with the same length as <code>y</code>. It
encodes a partitioning of the data into distinct subgroups. If <code>groups</code>
is provided, <code>k=10</code> is ignored and instead, cross-validation is
performed based on the partioning. Default is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="pcr.cv_+3A_scale">scale</code></td>
<td>
<p>Should the predictor variables be scaled to unit variance?
Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="pcr.cv_+3A_eps">eps</code></td>
<td>
<p>precision. Eigenvalues of the correlation matrix of <code>X</code> that
are smaller than <code>eps</code> are set to 0. The default value is
<code>eps=10^{-6}.</code></p>
</td></tr>
<tr><td><code id="pcr.cv_+3A_plot.it">plot.it</code></td>
<td>
<p>Logical. If <code>TRUE</code>, the function plots the
cross-validation-error as a function of the number of components. Default is
<code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="pcr.cv_+3A_compute.jackknife">compute.jackknife</code></td>
<td>
<p>Logical. If <code>TRUE</code>, the regression
coefficients on each of the cross-validation splits is stored. Default is
<code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="pcr.cv_+3A_method.cor">method.cor</code></td>
<td>
<p>How should the correlation to the response be computed?
Default is &rdquo;pearson&rdquo;.</p>
</td></tr>
<tr><td><code id="pcr.cv_+3A_supervised">supervised</code></td>
<td>
<p>Should the principal components be sorted by decreasing
squared correlation to the response? Default is FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function computes the principal components on the scaled predictors.
Based on the regression coefficients <code>coefficients.jackknife</code> computed
on the cross-validation splits, we can estimate their mean and their
variance using the jackknife. We remark that under a fixed design and the
assumption of normally distributed <code>y</code>-values, we can also derive the
true distribution of the regression coefficients.
</p>


<h3>Value</h3>

<table>
<tr><td><code>cv.error.matrix</code></td>
<td>
<p>matrix of cross-validated errors based on
mean squared error. A row corresponds to one cross-validation split.</p>
</td></tr>
<tr><td><code>cv.error</code></td>
<td>
<p>vector of cross-validated errors based on mean squared
error</p>
</td></tr> <tr><td><code>m.opt</code></td>
<td>
<p>optimal number of components based on mean squared
error</p>
</td></tr> <tr><td><code>intercept</code></td>
<td>
<p>intercept of the optimal model, based on mean
squared error</p>
</td></tr> <tr><td><code>coefficients</code></td>
<td>
<p>vector of regression coefficients of the
optimal model, based on mean squared error</p>
</td></tr> <tr><td><code>cor.error.matrix</code></td>
<td>
<p>matrix
of cross-validated errors based on correlation. A row corresponds to one
cross-validation split.</p>
</td></tr> <tr><td><code>cor.error</code></td>
<td>
<p>vector of cross-validated errors
based on correlation</p>
</td></tr> <tr><td><code>m.opt.cor</code></td>
<td>
<p>optimal number of components based on
correlation</p>
</td></tr> <tr><td><code>intercept.cor</code></td>
<td>
<p>intercept of the optimal model, based on
correlation</p>
</td></tr> <tr><td><code>coefficients.cor</code></td>
<td>
<p>vector of regression coefficients of
the optimal model, based on correlation</p>
</td></tr>
<tr><td><code>coefficients.jackknife</code></td>
<td>
<p>Array of the regression coefficients on each
of the cross-validation splits, if <code>compute.jackknife=TRUE</code>. In this
case, the dimension is <code>ncol(X) x (m+1) x k</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nicole Kraemer, Mikio L. Braun
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pls.model">pls.model</a></code>, <code><a href="#topic+pls.ic">pls.ic</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n&lt;-500 # number of observations
p&lt;-5 # number of variables
X&lt;-matrix(rnorm(n*p),ncol=p)
y&lt;-rnorm(n)

# compute PCR 
pcr.object&lt;-pcr.cv(X,y,scale=FALSE,m=3)
pcr.object1&lt;-pcr.cv(X,y,groups=sample(c(1,2,3),n,replace=TRUE),m=3)

</code></pre>

<hr>
<h2 id='pls.cv'>Model selection for Partial Least Squares based on cross-validation</h2><span id='topic+pls.cv'></span>

<h3>Description</h3>

<p>This function computes the optimal model parameter using cross-validation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pls.cv(
  X,
  y,
  k = 10,
  groups = NULL,
  m = ncol(X),
  use.kernel = FALSE,
  compute.covariance = FALSE,
  method.cor = "pearson"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pls.cv_+3A_x">X</code></td>
<td>
<p>matrix of predictor observations.</p>
</td></tr>
<tr><td><code id="pls.cv_+3A_y">y</code></td>
<td>
<p>vector of response observations. The length of <code>y</code> is the same
as the number of rows of <code>X</code>.</p>
</td></tr>
<tr><td><code id="pls.cv_+3A_k">k</code></td>
<td>
<p>number of cross-validation splits. Default is 10.</p>
</td></tr>
<tr><td><code id="pls.cv_+3A_groups">groups</code></td>
<td>
<p>an optional vector with the same length as <code>y</code>. It
encodes a partitioning of the data into distinct subgroups. If <code>groups</code>
is provided, <code>k=10</code> is ignored and instead, cross-validation is
performed based on the partioning. Default is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="pls.cv_+3A_m">m</code></td>
<td>
<p>maximal number of Partial Least Squares components. Default is
<code>m=ncol(X)</code>.</p>
</td></tr>
<tr><td><code id="pls.cv_+3A_use.kernel">use.kernel</code></td>
<td>
<p>Use kernel representation? Default is
<code>use.kernel=FALSE</code>.</p>
</td></tr>
<tr><td><code id="pls.cv_+3A_compute.covariance">compute.covariance</code></td>
<td>
<p>If <code>TRUE</code>, the function computes the
covariance for the cv-optimal regression coefficients.</p>
</td></tr>
<tr><td><code id="pls.cv_+3A_method.cor">method.cor</code></td>
<td>
<p>How should the correlation to the response be computed?
Default is &rdquo;pearson&rdquo;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The data are centered and scaled to unit variance prior to the PLS
algorithm. It is possible to estimate the covariance matrix of the
cv-optimal regression coefficients (<code>compute.covariance=TRUE</code>).
Currently, this is only implemented if <code>use.kernel=FALSE</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>cv.error.matrix</code></td>
<td>
<p>matrix of cross-validated errors based on
mean squared error. A row corresponds to one cross-validation split.</p>
</td></tr>
<tr><td><code>cv.error</code></td>
<td>
<p>vector of cross-validated errors based on mean squared
error</p>
</td></tr> <tr><td><code>m.opt</code></td>
<td>
<p>optimal number of components based on mean squared
error</p>
</td></tr> <tr><td><code>intercept</code></td>
<td>
<p>intercept of the optimal model, based on mean
squared error</p>
</td></tr> <tr><td><code>coefficients</code></td>
<td>
<p>vector of regression coefficients of the
optimal model, based on mean squared error</p>
</td></tr> <tr><td><code>cor.error.matrix</code></td>
<td>
<p>matrix
of cross-validated errors based on correlation. A row corresponds to one
cross-validation split.</p>
</td></tr> <tr><td><code>cor.error</code></td>
<td>
<p>vector of cross-validated errors
based on correlation</p>
</td></tr> <tr><td><code>m.opt.cor</code></td>
<td>
<p>optimal number of components based on
correlation</p>
</td></tr> <tr><td><code>intercept.cor</code></td>
<td>
<p>intercept of the optimal model, based on
correlation</p>
</td></tr> <tr><td><code>coefficients.cor</code></td>
<td>
<p>vector of regression coefficients of
the optimal model, based on mean squared error</p>
</td></tr> <tr><td><code>covariance</code></td>
<td>
<p>If
<code>TRUE</code> and <code>use.kernel=FALSE</code>, the covariance of the cv-optimal
regression coefficients (based on mean squared error) is returned.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nicole Kraemer, Mikio L. Braun
</p>


<h3>References</h3>

<p>Kraemer, N., Sugiyama M. (2011). &quot;The Degrees of Freedom of Partial Least
Squares Regression&quot;. Journal of the American Statistical Association 106
(494) <a href="https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107">https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107</a>
</p>
<p>Kraemer, N., Braun, M.L. (2007) &quot;Kernelizing PLS, Degrees of Freedom, and
Efficient Model Selection&quot;, Proceedings of the 24th International Conference
on Machine Learning, Omni Press, 441 - 448
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pls.model">pls.model</a></code>, <code><a href="#topic+pls.ic">pls.ic</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n&lt;-50 # number of observations
p&lt;-5 # number of variables
X&lt;-matrix(rnorm(n*p),ncol=p)
y&lt;-rnorm(n)

# compute linear PLS
pls.object&lt;-pls.cv(X,y,m=ncol(X))

# define random partioning
groups&lt;-sample(c("a","b","c"),n,replace=TRUE)
pls.object1&lt;-pls.cv(X,y,groups=groups)

</code></pre>

<hr>
<h2 id='pls.dof'>Computation of the Degrees of Freedom</h2><span id='topic+pls.dof'></span>

<h3>Description</h3>

<p>This function computes the Degrees of Freedom using the Krylov
representation of PLS.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pls.dof(pls.object, n, y, K, m, DoF.max)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pls.dof_+3A_pls.object">pls.object</code></td>
<td>
<p>object returned by <code>linear.pls.fit</code> or by
<code>kernel.pls.fit</code></p>
</td></tr>
<tr><td><code id="pls.dof_+3A_n">n</code></td>
<td>
<p>number of observations</p>
</td></tr>
<tr><td><code id="pls.dof_+3A_y">y</code></td>
<td>
<p>vector of response observations.</p>
</td></tr>
<tr><td><code id="pls.dof_+3A_k">K</code></td>
<td>
<p>kernel matrix X X^t.</p>
</td></tr>
<tr><td><code id="pls.dof_+3A_m">m</code></td>
<td>
<p>number of components</p>
</td></tr>
<tr><td><code id="pls.dof_+3A_dof.max">DoF.max</code></td>
<td>
<p>upper bound on the Degrees of Freedom.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This computation of the Degrees of Freedom is based on the equivalence of
PLS regression and the projection of the response vector <code>y</code> onto the
Krylov space spanned by </p>
<p style="text-align: center;"><code class="reqn">Ky,K^2 y,...,K^m y.</code>
</p>
<p> Details can be found in
Kraemer and Sugiyama (2011).
</p>


<h3>Value</h3>

<table>
<tr><td><code>coefficients</code></td>
<td>
<p>matrix of regression coefficients</p>
</td></tr>
<tr><td><code>intercept</code></td>
<td>
<p>vector of regression intercepts</p>
</td></tr> <tr><td><code>DoF</code></td>
<td>
<p>Degrees of
Freedom</p>
</td></tr> <tr><td><code>sigmahat</code></td>
<td>
<p>vector of estimated model error</p>
</td></tr> <tr><td><code>Yhat</code></td>
<td>
<p>matrix
of fitted values</p>
</td></tr> <tr><td><code>yhat</code></td>
<td>
<p>vector of squared length of fitted values</p>
</td></tr>
<tr><td><code>RSS</code></td>
<td>
<p>vector of residual sum of error</p>
</td></tr> <tr><td><code>TT</code></td>
<td>
<p>matrix of normalized
PLS components</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nicole Kraemer, Mikio L. Braun
</p>


<h3>References</h3>

<p>Kraemer, N., Sugiyama M. (2011). &quot;The Degrees of Freedom of Partial Least
Squares Regression&quot;. Journal of the American Statistical Association 106
(494) <a href="https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107">https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107</a>
</p>
<p>Kraemer, N., Sugiyama M., Braun, M.L. (2009) &quot;Lanczos Approximations for the
Speedup of Kernel Partial Least Squares Regression.&quot; Proceedings of the
Twelfth International Conference on Artificial Intelligence and Statistics
(AISTATS), p. 272-279
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pls.model">pls.model</a></code>, <code><a href="#topic+pls.ic">pls.ic</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# this is an internal function

</code></pre>

<hr>
<h2 id='pls.ic'>Model selection for Partial Least Squares based on information criteria</h2><span id='topic+pls.ic'></span>

<h3>Description</h3>

<p>This function computes the optimal model parameters using one of three
different model selection criteria (aic, bic, gmdl) and based on two
different Degrees of Freedom estimates for PLS.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pls.ic(
  X,
  y,
  m = min(ncol(X), nrow(X) - 1),
  criterion = "bic",
  naive = FALSE,
  use.kernel = FALSE,
  compute.jacobian = FALSE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pls.ic_+3A_x">X</code></td>
<td>
<p>matrix of predictor observations.</p>
</td></tr>
<tr><td><code id="pls.ic_+3A_y">y</code></td>
<td>
<p>vector of response observations. The length of <code>y</code> is the same
as the number of rows of <code>X</code>.</p>
</td></tr>
<tr><td><code id="pls.ic_+3A_m">m</code></td>
<td>
<p>maximal number of Partial Least Squares components. Default is
<code>m=ncol(X)</code>.</p>
</td></tr>
<tr><td><code id="pls.ic_+3A_criterion">criterion</code></td>
<td>
<p>Choice of the model selection criterion. One of the three
options aic, bic, gmdl.</p>
</td></tr>
<tr><td><code id="pls.ic_+3A_naive">naive</code></td>
<td>
<p>Use the naive estimate for the Degrees of Freedom? Default is
<code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="pls.ic_+3A_use.kernel">use.kernel</code></td>
<td>
<p>Use kernel representation? Default is
<code>use.kernel=FALSE</code>.</p>
</td></tr>
<tr><td><code id="pls.ic_+3A_compute.jacobian">compute.jacobian</code></td>
<td>
<p>Should the first derivative of the regression
coefficients be computed as well? Default is <code>FALSE</code></p>
</td></tr>
<tr><td><code id="pls.ic_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code>, the function prints a warning if the
algorithms produce negative Degrees of Freedom. Default is <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are two options to estimate the Degrees of Freedom of PLS:
<code>naive=TRUE</code> defines the Degrees of Freedom as the number of components
+1, and <code>naive=FALSE</code> uses the generalized notion of Degrees of
Freedom. If <code>compute.jacobian=TRUE</code>, the function uses the Lanczos
decomposition to derive the Degrees of Freedom, otherwise, it uses the
Krylov representation. (See Kraemer and Sugiyama (2011) for details.) The
latter two methods only differ with respect to the estimation of the noise
level.
</p>


<h3>Value</h3>

<p>The function returns an object of class &quot;plsdof&quot;. </p>
<table>
<tr><td><code>DoF</code></td>
<td>
<p>Degrees
of Freedom</p>
</td></tr> <tr><td><code>m.opt</code></td>
<td>
<p>optimal number of components</p>
</td></tr>
<tr><td><code>sigmahat</code></td>
<td>
<p>vector of estimated model errors</p>
</td></tr>
<tr><td><code>intercept</code></td>
<td>
<p>intercept</p>
</td></tr> <tr><td><code>coefficients</code></td>
<td>
<p>vector of regression
coefficients</p>
</td></tr> <tr><td><code>covariance</code></td>
<td>
<p>if <code>compute.jacobian=TRUE</code> and
<code>use.kernel=FALSE</code>, the function returns the covariance matrix of the
optimal regression coefficients.</p>
</td></tr> <tr><td><code>m.crash</code></td>
<td>
<p>the number of components
for which the algorithm returns negative Degrees of Freedom</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nicole Kraemer, Mikio L. Braun
</p>


<h3>References</h3>

<p>Akaikie, H. (1973) &quot;Information Theory and an Extension of the
Maximum Likelihood Principle&quot;. Second International Symposium on Information
Theory, 267 - 281.
</p>
<p>Hansen, M., Yu, B. (2001). &quot;Model Selection and Minimum Descripion Length
Principle&quot;. Journal of the American Statistical Association, 96, 746 - 774
</p>
<p>Kraemer, N., Sugiyama M. (2011). &quot;The Degrees of Freedom of Partial Least
Squares Regression&quot;. Journal of the American Statistical Association 106
(494) <a href="https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107">https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107</a>
</p>
<p>Kraemer, N., Braun, M.L. (2007) &quot;Kernelizing PLS, Degrees of Freedom, and
Efficient Model Selection&quot;, Proceedings of the 24th International Conference
on Machine Learning, Omni Press, 441 - 448
</p>
<p>Schwartz, G. (1979) &quot;Estimating the Dimension of a Model&quot; Annals of
Statistics 26(5), 1651 - 1686.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pls.model">pls.model</a></code>, <code><a href="#topic+pls.cv">pls.cv</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n&lt;-50 # number of observations
p&lt;-5 # number of variables
X&lt;-matrix(rnorm(n*p),ncol=p)
y&lt;-rnorm(n)

# compute linear PLS
pls.object&lt;-pls.ic(X,y,m=ncol(X))

</code></pre>

<hr>
<h2 id='pls.model'>Partial Least Squares</h2><span id='topic+pls.model'></span>

<h3>Description</h3>

<p>This function computes the Partial Least Squares fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pls.model(
  X,
  y,
  m = ncol(X),
  Xtest = NULL,
  ytest = NULL,
  compute.DoF = FALSE,
  compute.jacobian = FALSE,
  use.kernel = FALSE,
  method.cor = "pearson"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pls.model_+3A_x">X</code></td>
<td>
<p>matrix of predictor observations.</p>
</td></tr>
<tr><td><code id="pls.model_+3A_y">y</code></td>
<td>
<p>vector of response observations. The length of <code>y</code> is the same
as the number of rows of <code>X</code>.</p>
</td></tr>
<tr><td><code id="pls.model_+3A_m">m</code></td>
<td>
<p>maximal number of Partial Least Squares components. Default is
<code>m=min(ncol(X),nrow(X)-1)</code>.</p>
</td></tr>
<tr><td><code id="pls.model_+3A_xtest">Xtest</code></td>
<td>
<p>optional matrix of test observations. Default is
<code>Xtest=NULL</code>.</p>
</td></tr>
<tr><td><code id="pls.model_+3A_ytest">ytest</code></td>
<td>
<p>optional vector of test observations. Default is
<code>ytest=NULL</code>.</p>
</td></tr>
<tr><td><code id="pls.model_+3A_compute.dof">compute.DoF</code></td>
<td>
<p>Logical variable. If <code>compute.DoF=TRUE</code>, the Degrees
of Freedom of Partial Least Squares are computed. Default is
<code>compute.DoF=FALSE</code>.</p>
</td></tr>
<tr><td><code id="pls.model_+3A_compute.jacobian">compute.jacobian</code></td>
<td>
<p>Should the first derivative of the regression
coefficients be computed as well? Default is <code>FALSE</code></p>
</td></tr>
<tr><td><code id="pls.model_+3A_use.kernel">use.kernel</code></td>
<td>
<p>Should the kernel representation be used to compute the
solution. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="pls.model_+3A_method.cor">method.cor</code></td>
<td>
<p>How should the correlation to the response be computed?
Default is &rdquo;pearson&rdquo;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes the Partial Least Squares fit and its Degrees of
Freedom. Further, it returns the regression coefficients and various
quantities that are needed for model selection in combination with
<code>information.criteria</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>coefficients</code></td>
<td>
<p>matrix of regression coefficients</p>
</td></tr>
<tr><td><code>intercept</code></td>
<td>
<p>vector of intercepts</p>
</td></tr> <tr><td><code>DoF</code></td>
<td>
<p>vector of Degrees of
Freedom</p>
</td></tr> <tr><td><code>RSS</code></td>
<td>
<p>vector of residual sum of error</p>
</td></tr> <tr><td><code>sigmahat</code></td>
<td>
<p>vector
of estimated model error</p>
</td></tr> <tr><td><code>Yhat</code></td>
<td>
<p>matrix of fitted values</p>
</td></tr>
<tr><td><code>yhat</code></td>
<td>
<p>vector of squared length of fitted values</p>
</td></tr> <tr><td><code>covariance</code></td>
<td>
<p>if
<code>compute.jacobian</code> is <code>TRUE</code>, the function returns the array of
covariance matrices for the PLS regression coefficients.</p>
</td></tr>
</table>
<p><code>prediction</code>if <code>Xtest</code> is provided, the predicted y-values for
<code>Xtest</code>. <code>mse</code>if <code>Xtest</code> and <code>ytest</code> are provided, the
mean squared error on the test data.  <code>cor</code>if <code>Xtest</code> and
<code>ytest</code> are provided, the correlation to the response on the test data.
</p>


<h3>Author(s)</h3>

<p>Nicole Kraemer, Mikio L. Braun
</p>


<h3>References</h3>

<p>Kraemer, N., Sugiyama M. (2011). &quot;The Degrees of Freedom of
Partial Least Squares Regression&quot;. Journal of the American Statistical
Association 106 (494)
<a href="https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107">https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107</a>
</p>
<p>Kraemer, N., Sugiyama, M., Braun, M.L. (2009) &quot;Lanczos Approximations for
the Speedup of Partial Least Squares Regression&quot;, Proceedings of the 12th
International Conference on Artificial Intelligence and Stastistics, 272 -
279
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pls.ic">pls.ic</a></code>, <code><a href="#topic+pls.cv">pls.cv</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n&lt;-50 # number of observations
p&lt;-15 # number of variables
X&lt;-matrix(rnorm(n*p),ncol=p)
y&lt;-rnorm(n)

ntest&lt;-200 #
Xtest&lt;-matrix(rnorm(ntest*p),ncol=p) # test data
ytest&lt;-rnorm(ntest) # test data

# compute PLS + degrees of freedom + prediction on Xtest
first.object&lt;-pls.model(X,y,compute.DoF=TRUE,Xtest=Xtest,ytest=NULL)

# compute PLS + test error
second.object=pls.model(X,y,m=10,Xtest=Xtest,ytest=ytest)

</code></pre>

<hr>
<h2 id='plsdof-package'>Degrees of Freedom and Statistical Inference for Partial Least Squares
Regression</h2><span id='topic+plsdof-package'></span><span id='topic+plsdof'></span>

<h3>Description</h3>

<p>The plsdof package provides Degrees of Freedom estimates for Partial Least
Squares (PLS) Regression.
</p>


<h3>Details</h3>

<p>Model selection for PLS is based on various information criteria (aic, bic,
gmdl) or on cross-validation. Estimates for the mean and covariance of the
PLS regression coefficients are available. They allow the construction of
approximate confidence intervals and the application of test procedures.
</p>
<p>Further, cross-validation procedures for Ridge Regression and Principal
Components Regression are available.
</p>

<table>
<tr>
 <td style="text-align: left;"> Package: </td><td style="text-align: left;"> plsdof</td>
</tr>
<tr>
 <td style="text-align: left;"> Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;"> Version: </td><td style="text-align: left;">
0.2-9</td>
</tr>
<tr>
 <td style="text-align: left;"> Date: </td><td style="text-align: left;"> 2019-31-01</td>
</tr>
<tr>
 <td style="text-align: left;"> License: </td><td style="text-align: left;"> GPL (&gt;=2)</td>
</tr>
<tr>
 <td style="text-align: left;"> LazyLoad: </td><td style="text-align: left;">
yes</td>
</tr>
<tr>
 <td style="text-align: left;"> </td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Nicole Kraemer, Mikio L. Braun
</p>
<p>Maintainer: Frederic Bertrand &lt;frederic.bertrand@utt.fr.fr&gt;
</p>


<h3>References</h3>

<p>Kraemer, N., Sugiyama M. (2011). &quot;The Degrees of Freedom of Partial Least
Squares Regression&quot;. Journal of the American Statistical Association 106
(494) <a href="https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107">https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107</a>
</p>
<p>Kraemer, N., Braun, M.L. (2007) &quot;Kernelizing PLS, Degrees of Freedom, and
Efficient Model Selection&quot;, Proceedings of the 24th International Conference
on Machine Learning, Omni Press, 441 - 448
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pls.model">pls.model</a></code>, <code><a href="#topic+pls.cv">pls.cv</a></code>, <code><a href="#topic+pls.ic">pls.ic</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Boston Housing data
data(Boston)
X&lt;-as.matrix(Boston[,-14])
y&lt;-as.vector(Boston[,14])

# compute PLS coefficients for the first 5 components and plot Degrees of Freedom

my.pls1&lt;-pls.model(X,y,m=5,compute.DoF=TRUE)

plot(0:5,my.pls1$DoF,pch="*",cex=3,xlab="components",ylab="DoF",ylim=c(0,14))

# add naive estimate
lines(0:5,1:6,lwd=3)

# model selection with the Bayesian Information criterion

mypls2&lt;-pls.ic(X,y,criterion="bic")

# model selection based on cross-validation. 
# returns the estimated covariance matrix of the regression coefficients

mypls3&lt;-pls.cv(X,y,compute.covariance=TRUE)
my.vcov&lt;-vcov(mypls3)
my.sd&lt;-sqrt(diag(my.vcov)) # standard deviation of the regression coefficients


</code></pre>

<hr>
<h2 id='ridge.cv'>Ridge Regression.</h2><span id='topic+ridge.cv'></span>

<h3>Description</h3>

<p>This function computes the optimal ridge regression model based on
cross-validation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ridge.cv(
  X,
  y,
  lambda = NULL,
  scale = TRUE,
  k = 10,
  plot.it = FALSE,
  groups = NULL,
  method.cor = "pearson",
  compute.jackknife = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ridge.cv_+3A_x">X</code></td>
<td>
<p>matrix of input observations. The rows of <code>X</code> contain the
samples, the columns of <code>X</code> contain the observed variables</p>
</td></tr>
<tr><td><code id="ridge.cv_+3A_y">y</code></td>
<td>
<p>vector of responses. The length of y must equal the number of rows
of X</p>
</td></tr>
<tr><td><code id="ridge.cv_+3A_lambda">lambda</code></td>
<td>
<p>Vector of penalty terms.</p>
</td></tr>
<tr><td><code id="ridge.cv_+3A_scale">scale</code></td>
<td>
<p>Scale the columns of X? Default is scale=TRUE.</p>
</td></tr>
<tr><td><code id="ridge.cv_+3A_k">k</code></td>
<td>
<p>Number of splits in <code>k</code>-fold cross-validation. Default value
is <code>k</code>=10.</p>
</td></tr>
<tr><td><code id="ridge.cv_+3A_plot.it">plot.it</code></td>
<td>
<p>Plot the cross-validation error as a function of
<code>lambda</code>? Default is FALSE.</p>
</td></tr>
<tr><td><code id="ridge.cv_+3A_groups">groups</code></td>
<td>
<p>an optional vector with the same length as <code>y</code>. It
encodes a partitioning of the data into distinct subgroups. If <code>groups</code>
is provided, <code>k=10</code> is ignored and instead, cross-validation is
performed based on the partioning. Default is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="ridge.cv_+3A_method.cor">method.cor</code></td>
<td>
<p>How should the correlation to the response be computed?
Default is &rdquo;pearson&rdquo;.</p>
</td></tr>
<tr><td><code id="ridge.cv_+3A_compute.jackknife">compute.jackknife</code></td>
<td>
<p>Logical. If <code>TRUE</code>, the regression
coefficients on each of the cross-validation splits is stored. Default is
<code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Based on the regression coefficients <code>coefficients.jackknife</code> computed
on the cross-validation splits, we can estimate their mean and their
variance using the jackknife. We remark that under a fixed design and the
assumption of normally distributed <code>y</code>-values, we can also derive the
true distribution of the regression coefficients.
</p>


<h3>Value</h3>

<table>
<tr><td><code>cv.error.matrix</code></td>
<td>
<p>matrix of cross-validated errors based on
mean squared error. A row corresponds to one cross-validation split.</p>
</td></tr>
<tr><td><code>cv.error</code></td>
<td>
<p>vector of cross-validated errors based on mean squared
error</p>
</td></tr> <tr><td><code>lambda.opt</code></td>
<td>
<p>optimal value of <code>lambda</code>, based on mean
squared error</p>
</td></tr> <tr><td><code>intercept</code></td>
<td>
<p>intercept of the optimal model, based on
mean squared error</p>
</td></tr> <tr><td><code>coefficients</code></td>
<td>
<p>vector of regression coefficients of
the optimal model, based on mean squared error</p>
</td></tr>
<tr><td><code>cor.error.matrix</code></td>
<td>
<p>matrix of cross-validated errors based on
correlation. A row corresponds to one cross-validation split.</p>
</td></tr>
<tr><td><code>cor.error</code></td>
<td>
<p>vector of cross-validated errors based on correlation</p>
</td></tr>
<tr><td><code>lambda.opt.cor</code></td>
<td>
<p>optimal value of <code>lambda</code>, based on correlation</p>
</td></tr>
<tr><td><code>intercept.cor</code></td>
<td>
<p>intercept of the optimal model, based on correlation</p>
</td></tr>
<tr><td><code>coefficients.cor</code></td>
<td>
<p>vector of regression coefficients of the optimal
model, based on mean squared error</p>
</td></tr> <tr><td><code>coefficients.jackknife</code></td>
<td>
<p>Array of
the regression coefficients on each of the cross-validation splits. The
dimension is <code>ncol(X) x length(lambda) x k</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nicole Kraemer
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pls.cv">pls.cv</a></code>, <code><a href="#topic+pcr.cv">pcr.cv</a></code>,
<code><a href="#topic+benchmark.regression">benchmark.regression</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n&lt;-100 # number of observations
p&lt;-60 # number of variables
X&lt;-matrix(rnorm(n*p),ncol=p) 
y&lt;-rnorm(n)
ridge.object&lt;-ridge.cv(X,y)

</code></pre>

<hr>
<h2 id='tr'>Trace of a matrix</h2><span id='topic+tr'></span>

<h3>Description</h3>

<p>This function computes the trace of a matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tr(M)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tr_+3A_m">M</code></td>
<td>
<p>square matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The trace of the matrix M.
</p>


<h3>Author(s)</h3>

<p>Nicole Kraemer
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
M&lt;-matrix(rnorm(8*8),ncol=8)
tr.M&lt;-tr(M)

</code></pre>

<hr>
<h2 id='vcov.plsdof'>Variance-covariance matrix</h2><span id='topic+vcov.plsdof'></span>

<h3>Description</h3>

<p>This function returns the variance-covariance matrix of a plsdof-object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'plsdof'
vcov(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vcov.plsdof_+3A_object">object</code></td>
<td>
<p>an object of class &quot;plsdof&quot; that is returned by the function
<code>linear.pls</code></p>
</td></tr>
<tr><td><code id="vcov.plsdof_+3A_...">...</code></td>
<td>
<p>additional parameters</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function returns the variance-covariance matrix for the optimal number
of components. It can be applied to objects returned by <code>pls.ic</code> and
<code>pls.cv</code>.
</p>


<h3>Value</h3>

<p>variance-covariance matrix
</p>


<h3>Author(s)</h3>

<p>Nicole Kraemer
</p>


<h3>References</h3>

<p>Kraemer, N., Sugiyama M. (2011). &quot;The Degrees of Freedom of
Partial Least Squares Regression&quot;. Journal of the American Statistical
Association 106 (494)
<a href="https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107">https://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10107</a>
</p>
<p>Kraemer, N., Sugiyama M., Braun, M.L. (2009) &quot;Lanczos Approximations for the
Speedup of Kernel Partial Least Squares Regression.&quot; Proceedings of the
Twelfth International Conference on Artificial Intelligence and Statistics
(AISTATS), p. 272-279
</p>


<h3>See Also</h3>

<p><code><a href="#topic+coef.plsdof">coef.plsdof</a></code>, <code><a href="#topic+pls.ic">pls.ic</a></code>,
<code><a href="#topic+pls.cv">pls.cv</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

n&lt;-50 # number of observations
p&lt;-5 # number of variables
X&lt;-matrix(rnorm(n*p),ncol=p)
y&lt;-rnorm(n)


pls.object&lt;-pls.ic(X,y,m=5,criterion="bic")
my.vcov&lt;-vcov(pls.object)
my.sd&lt;-sqrt(diag(my.vcov)) # standard deviation of regression coefficients

</code></pre>

<hr>
<h2 id='vvtz'>Projectin operator</h2><span id='topic+vvtz'></span>

<h3>Description</h3>

<p>This function computes the projection operator </p>
<p style="text-align: center;"><code class="reqn">P_V z= V V^\top z</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>vvtz(v, z)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vvtz_+3A_v">v</code></td>
<td>
<p>orthonormal basis of the space on which <code>z</code> is projected.
<code>v</code> is either a matrix or a vector.</p>
</td></tr>
<tr><td><code id="vvtz_+3A_z">z</code></td>
<td>
<p>vector that is projected onto the columns of <code>v</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The above formula is only valid if the columns of <code>v</code> are normalized
and mutually orthogonal.
</p>


<h3>Value</h3>

<p>value of the projection operator
</p>


<h3>Author(s)</h3>

<p>Nicole Kraemer
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dvvtz">dvvtz</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# generate random orthogonal vectors
X&lt;-matrix(rnorm(10*100),ncol=10) 	# random data
S&lt;-cor(X) 				# correlation matrix of data
v&lt;-eigen(S)$vectors[,1:3]		# first three eigenvectors of correlation matrix
z&lt;-rnorm(10)				# random vector z
projection.z&lt;-vvtz(v,z)


</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
