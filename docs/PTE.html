<!DOCTYPE html><html><head><title>Help for package PTE</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {PTE}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#PTE'><p>Personalized Medicine Inference</p></a></li>
<li><a href='#continuous_example'><p>Mock RCT data with a continuous endpoint.</p></a></li>
<li><a href='#plot.PTE_bootstrap_results'><p>Plots a summary of the bootstrap samples.</p></a></li>
<li><a href='#print.PTE_bootstrap_results'><p>Prints a summary of the model to the console</p></a></li>
<li><a href='#PTE_bootstrap_inference'><p>Bootstrap inference for a prespecified personalization / recommendation model</p></a></li>
<li><a href='#summary.PTE_bootstrap_results'><p>Prints a summary of the model to the console</p></a></li>
<li><a href='#survival_example'><p>Mock RCT data with a survival endpoint.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Personalized Treatment Evaluator</td>
</tr>
<tr>
<td>Version:</td>
<td>1.7</td>
</tr>
<tr>
<td>Date:</td>
<td>2019-01-24</td>
</tr>
<tr>
<td>Author:</td>
<td>Adam Kapelner, Alina Levine &amp; Justin Bleich</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Adam Kapelner &lt;kapelner@qc.cuny.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>We provide inference for personalized medicine models. Namely, we answer the questions: (1) how much better does a purported personalized recommendation engine for treatments do over a business-as-usual approach and (2) is that difference statistically significant?</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>foreach, parallel, doParallel, graphics, grDevices, stats,
survival</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.0.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-01-30 19:39:46 UTC; Kapelner</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-01-31 00:03:19 UTC</td>
</tr>
</table>
<hr>
<h2 id='PTE'>Personalized Medicine Inference</h2><span id='topic+PTE'></span><span id='topic+PTE-package'></span>

<h3>Description</h3>

<p>Personalized Medicine...
</p>


<h3>Author(s)</h3>

<p>Adam Kapelner <a href="mailto:kapelner@qc.cuny.edu">kapelner@qc.cuny.edu</a>, Alina Levine and Justin Bleich
</p>


<h3>References</h3>

<p>Kapelner, A, Bleich, J, Cohen, ZD, DeRubeis, RJ and Berk, R (2014) Inference for Treatment Regime Models in Personalized Medicine, arXiv
</p>

<hr>
<h2 id='continuous_example'>Mock RCT data with a continuous endpoint.</h2><span id='topic+continuous_example'></span>

<h3>Description</h3>

<p>A list with two objects (a) <code>X</code>, a dataframe with n rows representing clinical subjects and columns: 
treatment, x1, x2, x3, x4 and x5 where treatment is binary indicating the two arms of the clinical trial
and x1, ..., x5 are covariates that were collected about each subject and (b) <code>y</code>, a length n vector storing 
the continuous response values where, in this mock dataset, larger values indicate &quot;better&quot; outcomes for the 
subjects.
</p>


<h3>Author(s)</h3>

<p>My Name <a href="mailto:kapelner@qc.cuny.edu">kapelner@qc.cuny.edu</a>
</p>

<hr>
<h2 id='plot.PTE_bootstrap_results'>Plots a summary of the bootstrap samples.</h2><span id='topic+plot.PTE_bootstrap_results'></span>

<h3>Description</h3>

<p>Plots a summary of the bootstrap samples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'PTE_bootstrap_results'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.PTE_bootstrap_results_+3A_x">x</code></td>
<td>
<p>A <code>PTE_bootstrap_results</code> model object built via
running the <code>PTE_bootstrap_inference</code> function.</p>
</td></tr>
<tr><td><code id="plot.PTE_bootstrap_results_+3A_...">...</code></td>
<td>
<p>Other methods passed to plot</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Adam Kapelner
</p>

<hr>
<h2 id='print.PTE_bootstrap_results'>Prints a summary of the model to the console</h2><span id='topic+print.PTE_bootstrap_results'></span>

<h3>Description</h3>

<p>Prints a summary of the model to the console
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'PTE_bootstrap_results'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.PTE_bootstrap_results_+3A_x">x</code></td>
<td>
<p>A <code>PTE_bootstrap_results</code> model object built via
running the <code>PTE_bootstrap_inference</code> function.</p>
</td></tr>
<tr><td><code id="print.PTE_bootstrap_results_+3A_...">...</code></td>
<td>
<p>Other methods passed to print</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Adam Kapelner
</p>

<hr>
<h2 id='PTE_bootstrap_inference'>Bootstrap inference for a prespecified personalization / recommendation model</h2><span id='topic+PTE_bootstrap_inference'></span>

<h3>Description</h3>

<p>Runs B bootstrap samples using a prespecified model then computes the two I estimates based on cross validation. 
p values of the two I estimates are computed for a given <code class="reqn">H_0: \mu_{I_0} = \mu_0</code> and 
confidence intervals are provided using the basic, percentile methods by default and the BCa method as well 
if desired.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PTE_bootstrap_inference(X, y, regression_type = "continuous",
  incidence_metric = "odds_ratio", personalized_model_build_function = NULL,
  censored = NULL, predict_function = function(mod, Xyleftout) {    
  predict(mod, Xyleftout) }, difference_function = NULL,
  cleanup_mod_function = NULL, y_higher_is_better = TRUE, verbose = FALSE,
  full_verbose = FALSE, H_0_mu_equals = NULL, pct_leave_out = 0.1,
  m_prop = 1, B = 3000, alpha = 0.05, run_bca_bootstrap = FALSE,
  display_adversarial_score = FALSE, num_cores = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PTE_bootstrap_inference_+3A_x">X</code></td>
<td>
<p>A <code class="reqn">n \times p</code> dataframe of covariates where one column is labeled &quot;treatment&quot; and it
is a binary vector of treatment allocations in the study.</p>
</td></tr>
<tr><td><code id="PTE_bootstrap_inference_+3A_y">y</code></td>
<td>
<p>An <code class="reqn">n</code>-length numeric vector which is the response</p>
</td></tr>
<tr><td><code id="PTE_bootstrap_inference_+3A_regression_type">regression_type</code></td>
<td>
<p>A string indicating the regression problem. Legal values are &quot;continous&quot; (the response <code>y</code> is
a real number with no missing data, the default), &quot;incidence&quot; (the reponse <code>y</code> is
either 0 or 1) and &quot;survival&quot;. If the type is &quot;survival&quot;, the user must also supply additional data via the 
parameter <code>censored</code>.</p>
</td></tr>
<tr><td><code id="PTE_bootstrap_inference_+3A_incidence_metric">incidence_metric</code></td>
<td>
<p>Ignored unless the <code>regression_type</code> is &quot;incidence&quot; and <code>difference_function</code> is set to <code>NULL</code> (in 
the latter case, you have specified a more custom metric). Then, this parameter allows the user to select which 
of the three standard metrics to use for comparison: &quot;probability_difference&quot;, &quot;risk_ratio&quot;, &quot;odds_ratio&quot; where 
the default is &quot;odds_ratio&quot;.</p>
</td></tr>
<tr><td><code id="PTE_bootstrap_inference_+3A_personalized_model_build_function">personalized_model_build_function</code></td>
<td>
<p>An R function that will be evaluated to construct the personalized medicine / recommendation 
model. In the formula for the model, the response is &quot;y&quot;, the treatment vector is 
&quot;treatment&quot; and the data is &quot;Xytrain&quot;. This function must return some type of object
that can be used for prediction later via <code>predict_function</code>. Here are the defaults
for each <code>regression_type</code>. They are linear models with first order interactions:
</p>
<p>personalized_model_build_function = switch(regression_type,
continuous = function(Xytrain) #defalt is OLS regression
lm(y ~ . * treatment, 
data = Xytrain)
,
incidence = function(Xytrain) #default is logistic regression
glm(y ~ . * treatment, 
data = Xytrain, 
family = &quot;binomial&quot;)
,
survival = function(Xytrain) #default is Weibull regression
survreg(Surv(Xytrain$y, Xytrain$censored) ~ (. - censored) * treatment, 
data = Xytrain, 
dist = &quot;weibull&quot;)

)</p>
</td></tr>
<tr><td><code id="PTE_bootstrap_inference_+3A_censored">censored</code></td>
<td>
<p>Only required if the <code>regression_type</code> is &quot;survival&quot;. In this case, this vector is of length <code class="reqn">n</code> and is binary 
where 0 indicates censored and 1 indicates uncensored. In a clinical trial, someone who is still alive 
at the end of the study or was lost to follow up will receive a censor value of 0, while someone who died during the study 
will receive a censor value of 1. 
<code class="reqn">n</code> and is binary where 0 indicates censorship (e.g. the patient died).</p>
</td></tr>
<tr><td><code id="PTE_bootstrap_inference_+3A_predict_function">predict_function</code></td>
<td>
<p>An R function that will be evaluated on left out data after the model is built with the training data. This function
uses the object &quot;mod&quot; that is the result of the <code>personalized_model_build_function</code> and it must make use of
&quot;Xyleftout&quot;, a subset of observations from <code>X</code>. This function must return a 
scalar numeric quantity for comparison. The default function is <code>predict(mod, obs_left_out)</code> e.g. the default looks like:
</p>
<p>function(mod, Xyleftout)
predict(mod, Xyleftout)
</p>
</td></tr>
<tr><td><code id="PTE_bootstrap_inference_+3A_difference_function">difference_function</code></td>
<td>
<p>A function which takes the result of one out of sample experiment (boostrap or not) of all n samples and converts it into a difference that
will be used as a score in a score distribution to determine if the personalization model is statistically significantly able to distinguish
subjects. The function looks as follows:
</p>
<p>function(results, indices_1_1, indices_0_0, indices_0_1, indices_1_0)
...
c(rec_vs_non_rec_diff_score, rec_vs_all_score, rec_vs_best_score)
 
</p>
<p>where <code>results</code> is a matrix consisting of columns of the estimated response of the treatment administered,
the estimated response of the counterfactual treatment, the administered treatment, the recommended treatment based on the personalization
model, the real response, and if this subject was censored (0 if so). Here are a couple of example entries:
</p>
<p>est_true     est_counterfactual     given_tx     rec_tx     real_y     censored
166.8       152.2                1           1        324     1
1679.1         2072.0                1           0        160     0
</p>
<p>The arguments <code>indices_1_1, indices_0_0, indices_0_1, indices_1_0</code> give the indices of the subjects whose treatment was administered 
as 1 and whose optimal was 1, whose treatment was administered 0 and whose optimal was 0, etc.
</p>
<p>This function should return three numeric scores: the recommend vs. the non-recommended (adversarial), the recommended 
vs. all (all) and the recommended vs. the best average treatment (best) as a 3-dimensional vector as illustrated above.
</p>
<p>By default, this parameter is <code>NULL</code> which means for continuous and incidence the average difference is used and
for survival, the median Kaplan-Meier survival is used.</p>
</td></tr>
<tr><td><code id="PTE_bootstrap_inference_+3A_cleanup_mod_function">cleanup_mod_function</code></td>
<td>
<p>A function that is called at the end of a cross validation iteration to cleanup the model 
in some way. This is used for instance if you would like to release the memory your model is using but generally does not apply.
The default is <code>NA</code> for &quot;no function.&quot;</p>
</td></tr>
<tr><td><code id="PTE_bootstrap_inference_+3A_y_higher_is_better">y_higher_is_better</code></td>
<td>
<p>True if a response value being higher is clinically &quot;better&quot; than one that is lower (e.g. cognitive ability in a drug trial for the 
mentally ill). False if the response value being lower is clinically &quot;better&quot; than one that is higher (e.g. amount of weight lost 
in a weight-loss trial). Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="PTE_bootstrap_inference_+3A_verbose">verbose</code></td>
<td>
<p>Prints out a dot for each bootstrap sample. This only works on some platforms.</p>
</td></tr>
<tr><td><code id="PTE_bootstrap_inference_+3A_full_verbose">full_verbose</code></td>
<td>
<p>Prints out full information for each cross validation model for each bootstrap sample. This only works on some platforms.</p>
</td></tr>
<tr><td><code id="PTE_bootstrap_inference_+3A_h_0_mu_equals">H_0_mu_equals</code></td>
<td>
<p>The <code class="reqn">\mu_{I_0}</code> value in <code class="reqn">H_0</code>. Default is <code>NULL</code> which specifies 0 for regression types continuous,
survival and incidence (with incidence metric &quot;probability_difference&quot;) or 1 if the regression type is incidence and the incidence
metric is &quot;risk_ratio&quot; or &quot;odds_ratio&quot;. These defaults essentially answer the question: does my allocation procedure do better 
than the business-as-usual / naive allocation procedure?</p>
</td></tr>
<tr><td><code id="PTE_bootstrap_inference_+3A_pct_leave_out">pct_leave_out</code></td>
<td>
<p>In the cross-validation, the proportion of the original dataset left out to estimate out-of-sample metrics. The default is 0.1
which corresponds to 10-fold cross validation.</p>
</td></tr>
<tr><td><code id="PTE_bootstrap_inference_+3A_m_prop">m_prop</code></td>
<td>
<p>Within each bootstrap sample, the proportion of the total number of rows of <code>X</code> to sample without replacement. <code>m_prop &lt; 1</code> ensures
the number of rows sampled is less than <code>n</code> which fixes the consistency of the bootstrap estimator of a non-smooth functional. The default 
is 1 since non-smoothness may not be a common issue.</p>
</td></tr>
<tr><td><code id="PTE_bootstrap_inference_+3A_b">B</code></td>
<td>
<p>The number of bootstrap samples to take. We recommend making this as high as you can tolerate given speed considerations.
The default is 3000.</p>
</td></tr>
<tr><td><code id="PTE_bootstrap_inference_+3A_alpha">alpha</code></td>
<td>
<p>Defines the confidence interval size (1 - alpha). Defaults to 0.05.</p>
</td></tr>
<tr><td><code id="PTE_bootstrap_inference_+3A_run_bca_bootstrap">run_bca_bootstrap</code></td>
<td>
<p>Do the BCA bootstrap as well. This takes double the time. It defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="PTE_bootstrap_inference_+3A_display_adversarial_score">display_adversarial_score</code></td>
<td>
<p>The adversarial score records the personalization metric versus the deliberate opposite of the personalization. This does not correspond
to any practical situation but it is useful for debugging. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="PTE_bootstrap_inference_+3A_num_cores">num_cores</code></td>
<td>
<p>The number of cores to use in parallel to run the bootstrap samples more rapidly. 
Defaults to <code>NULL</code> which automatically sets it to one if there is one available processor or
if there are multiple available processors, the number of available processors save one.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A results object of type &quot;PTE_bootstrap_results&quot; that contains much information about the observed results
and the bootstrap runs, including hypothesis testing and confidence intervals.
</p>


<h3>Author(s)</h3>

<p>Adam Kapelner
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
	library(PTE)
	B = 1000 #lower this for quicker demos

	##response: continuous
	data(continuous_example)
	X = continuous_example$X
 y = continuous_example$y
	pte_results = PTE_bootstrap_inference(X, y, regression_type = "continuous", B = B)
	pte_results

	##response: incidence
	data(continuous_example)
	X = continuous_example$X
 y = continuous_example$y
	y = ifelse(y &gt; quantile(y, 0.75), 1, 0) #force incidence and pretend y came to you this way
#there are three ways to assess incidence effects below: 
	#	odds ratio, risk ratio and probability difference 
	pte_results = PTE_bootstrap_inference(X, y, regression_type = "incidence", B = B)
	pte_results
	pte_results = PTE_bootstrap_inference(X, y, regression_type = "incidence", B = B, 
                                      incidence_metric = "risk_ratio")
	pte_results
	pte_results = PTE_bootstrap_inference(X, y, regression_type = "incidence", B = B, 
	                                      incidence_metric = "probability_difference")
	pte_results

	##response: survival
	data(survival_example)
	X = survival_example$X
	y = survival_example$y
 censored = survival_example$censored
	pte_results = PTE_bootstrap_inference(X, y, censored = censored, 
    	regression_type = "survival", 
        B = 1000)
	pte_results

## End(Not run)
</code></pre>

<hr>
<h2 id='summary.PTE_bootstrap_results'>Prints a summary of the model to the console</h2><span id='topic+summary.PTE_bootstrap_results'></span>

<h3>Description</h3>

<p>Prints a summary of the model to the console
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'PTE_bootstrap_results'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.PTE_bootstrap_results_+3A_object">object</code></td>
<td>
<p>A <code>PTE_bootstrap_results</code> model object built via
running the <code>PTE_bootstrap_inference</code> function.</p>
</td></tr>
<tr><td><code id="summary.PTE_bootstrap_results_+3A_...">...</code></td>
<td>
<p>Other methods passed to summary</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Adam Kapelner
</p>

<hr>
<h2 id='survival_example'>Mock RCT data with a survival endpoint.</h2><span id='topic+survival_example'></span>

<h3>Description</h3>

<p>A list with three objects (a) <code>X</code>, a dataframe with n rows representing clinical subjects and columns: 
treatment, x1, x2, x3 and x4 where treatment is binary indicating the two arms of the clinical trial
and x1, ..., x4 are covariates that were collected about each subject (b) <code>y</code>, a length n vector storing 
the survival response values (a time measurement) where, in this mock dataset, smaller values indicate &quot;better&quot; 
survival outcomes for the subjects and (c) <code>censored</code>, a length n vector storing the censor dummies where 
c_16 = 1 means the response y_16 was censored and thus the truth value of y_16 is unknown and y_16 only represents
the moment it was censored (and c_16 = 0 means it was uncensored and y_16 is the true response value).
</p>


<h3>Author(s)</h3>

<p>My Name <a href="mailto:kapelner@qc.cuny.edu">kapelner@qc.cuny.edu</a>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
