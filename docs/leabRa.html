<!DOCTYPE html><html><head><title>Help for package leabRa</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {leabRa}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#animals'><p>Small example data set for self-organized artificial neural networks</p></a></li>
<li><a href='#layer'><p>Leabra layer class</p></a></li>
<li><a href='#leabRa'><p>leabRa: A package for biologically realistic neural networks based on Leabra</p></a></li>
<li><a href='#network'><p>Leabra network class</p></a></li>
<li><a href='#unit'><p>Leabra unit (neuron) class</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>The Artificial Neural Networks Algorithm Leabra</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.0</td>
</tr>
<tr>
<td>Description:</td>
<td>The algorithm Leabra (local error driven and associative
    biologically realistic algorithm) allows for the construction of artificial
    neural networks that are biologically realistic and balance supervised and
    unsupervised learning within a single framework. This package is based on
    the 'MATLAB' version by Sergio Verduzco-Flores, which in turn was based on
    the description of the algorithm by Randall O'Reilly (1996)
    <a href="ftp://grey.colorado.edu/pub/oreilly/thesis/oreilly_thesis.all.pdf">ftp://grey.colorado.edu/pub/oreilly/thesis/oreilly_thesis.all.pdf</a>. For
    more general (not 'R' specific) information on the algorithm Leabra see
    <a href="https://grey.colorado.edu/emergent/index.php/Leabra">https://grey.colorado.edu/emergent/index.php/Leabra</a>.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Imports:</td>
<td>plyr (&ge; 1.8.4), R6 (&ge; 2.2.1)</td>
</tr>
<tr>
<td>Collate:</td>
<td>'data.R' 'misc.R' 'unit.R' 'layer.R' 'network.R' 'leabRa.R'</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.0.1</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/johannes-titz/leabRa">https://github.com/johannes-titz/leabRa</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/johannes-titz/leabRa/issues">https://github.com/johannes-titz/leabRa/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2017-09-22 09:45:35 UTC; jt</td>
</tr>
<tr>
<td>Author:</td>
<td>Johannes Titz [aut, cre, cph],
  Sergio Verduczo-Flores [cph],
  Randall O'Reilly [cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Johannes Titz &lt;johannes.titz@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2017-09-22 11:07:21 UTC</td>
</tr>
</table>
<hr>
<h2 id='animals'>Small example data set for self-organized artificial neural networks</h2><span id='topic+animals'></span>

<h3>Description</h3>

<p>A small dataset describing 10 animals represented by 6 features that are
either present (1) or absent (0) for demonstrating self-organized learning in
artificial neural networks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>animals
</code></pre>


<h3>Format</h3>

<p>A data frame with 10 rows and 6 variables describing 10 different
animals with 6 feature vectors that are either present (1) or absent (0).</p>


<h3>Source</h3>

<p>Knight, K. (1990). Connectionist ideas and algorithms.
<em>Communications of the ACM</em>, <em>33</em>(11), 59â€“74.
</p>

<hr>
<h2 id='layer'>Leabra layer class</h2><span id='topic+layer'></span>

<h3>Description</h3>

<p>This class simulates a biologically realistic layer of neurons in the
Leabra framework. It consists of several <code><a href="#topic+unit">unit</a></code> objects
in the variable (field) <code>units</code> and some layer-specific
variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layer
</code></pre>


<h3>Format</h3>

<p><code><a href="R6.html#topic+R6Class">R6Class</a></code> object</p>


<h3>Value</h3>

<p>Object of <code><a href="R6.html#topic+R6Class">R6Class</a></code> with methods for calculating changes
of activation in a layer of neurons.
</p>


<h3>Fields</h3>


<dl>
<dt><code>units</code></dt><dd><p>A list with all <code><a href="#topic+unit">unit</a></code> objects of the layer.</p>
</dd>
<dt><code>avg_act</code></dt><dd><p>The average activation of all units in the layer
(this is an active binding).</p>
</dd>
<dt><code>n</code></dt><dd><p>Number of units in layer.</p>
</dd>
<dt><code>weights</code></dt><dd><p>A receiving x sending weight matrix, where the receiving units
(rows) has the current weight values for the sending units (columns). The
weights will be set by the <code><a href="#topic+network">network</a></code> object, because they
depend on the connection to other layers.</p>
</dd>
<dt><code>ce_weights</code></dt><dd><p>Sigmoidal contrast-enhanced version of the weight matrix
<code>weights</code>. These weights will be set by the <code><a href="#topic+network">network</a></code>
object.</p>
</dd>
<dt><code>layer_number</code></dt><dd><p>Layer number in network (this is 1 if you create
a layer on your own, without the network class).</p>
</dd>
</dl>


<h3>Methods</h3>


<dl>
<dt><code>new(dim, g_i_gain = 2)</code></dt><dd><p>Creates an object of this class with
default parameters.
</p>

<dl>
<dt><code>dim</code></dt><dd><p>A pair of numbers giving the dimensions (rows and
columns) of the layer.</p>
</dd>
<dt><code>g_i_gain</code></dt><dd><p>Gain factor for inhibitory conductance, if you
want less activation in a layer, set this higher.</p>
</dd>
</dl>

</dd>
<dt><code>get_unit_acts()</code></dt><dd><p>Returns a vector with the activations of all
units of a layer.
</p>
</dd>
<dt><code>get_unit_scaled_acts()</code></dt><dd><p>Returns a vector with the scaled
activations of all units of a layer. Scaling is done with
<code>recip_avg_act_n</code>, a reciprocal function of the number of active
units.
</p>
</dd>
<dt><code>cycle(intern_input, ext_input)</code></dt><dd><p>Iterates one time step with
layer object.
</p>

<dl>
<dt><code>intern_input</code></dt><dd><p>Vector with inputs from all other layers.
Each input has already been scaled by a reciprocal function of the
number of active units (<code>recip_avg_act_n</code>) of the sending layer
and by the connection strength between the receiving and sending
layer. The weight matrix <code>ce_weights</code> is multiplied with this
input vector to get the excitatory conductance for each unit in the
layer.
</p>
</dd>
<dt><code>ext_input</code></dt><dd><p>Vector with inputs not coming from another
layer, with length equal to the number of units in this layer. If
empty (<code>NULL</code>), no external inputs are processed. If the external
inputs are not clamped, this is actually an excitatory conductance
value, which is added to the conductance produced by the internal
input and weight matrix.
</p>
</dd>
</dl>

</dd>
<dt><code>clamp_cycle(activations)</code></dt><dd><p>Iterates one time step with layer
object with clamped activations, meaning that activations are
instantaneously set without time integration.
</p>

<dl>
<dt><code>activations</code></dt><dd><p>Activations you want to clamp to the units in
the layer.
</p>
</dd>
</dl>

</dd>
<dt><code>get_unit_act_avgs()</code></dt><dd><p>Returns a list with the short, medium and
long term activation averages of all units in the layer as vectors. The
super short term average is not returned, and the long term average is not
updated before being returned (this is done in the function <code>chg_wt()</code>
with the method<code>updt_unit_avg_l</code>). These averages are used by the
network class to calculate weight changes.
</p>
</dd>
<dt><code>updt_unit_avg_l()</code></dt><dd><p>Updates the long-term average
(<code>avg_l</code>) of all units in the layer, usually done after a plus phase.
</p>
</dd>
<dt><code>updt_recip_avg_act_n()</code></dt><dd><p>Updates the <code>avg_act_inert</code> and
<code>recip_avg_act_n</code> variables, these variables update before the weights
are changed instead of cycle by cycle. This version of the function assumes
full connectivity between layers.
</p>
</dd>
<dt><code>reset(random = FALSE)</code></dt><dd><p>Sets the activation and activation
averages of all units to 0. Used to begin trials from a stationary point.
</p>

<dl>
<dt><code>random</code></dt><dd><p>Logical variable, if TRUE the activations are set
randomly between .05 and .95 for every unit instead of 0.
</p>
</dd>
</dl>

</dd>
<dt><code>set_ce_weights()</code></dt><dd><p>Sets contrast enhanced weight values.
</p>
</dd>
<dt><code>get_unit_vars(show_dynamics = TRUE, show_constants =
  FALSE)</code></dt><dd><p>Returns a data frame with the current state of all unit variables
in the layer. Every row is a unit. You can choose whether you want dynamic
values and / or constant values. This might be useful if you want to
analyze what happens in units of a layer, which would otherwise not be
possible, because most of the variables (fields) are private in the unit
class.
</p>

<dl>
<dt><code>show_dynamics</code></dt><dd><p>Should dynamic values be shown? Default is
TRUE.
</p>
</dd>
<dt><code>show_constants</code></dt><dd><p>Should constant values be shown? Default
is FALSE.
</p>
</dd>
</dl>

</dd>
<dt><code>get_layer_vars(show_dynamics = TRUE, show_constants =
  FALSE)</code></dt><dd><p>Returns a data frame with 1 row with the current state of the
variables in the layer. You can choose whether you want dynamic values and
/ or constant values. This might be useful if you want to analyze what
happens in a layer, which would otherwise not be possible, because some of
the variables (fields) are private in the layer class.
</p>

<dl>
<dt><code>show_dynamics</code></dt><dd><p>Should dynamic values be shown? Default is
TRUE.
</p>
</dd>
<dt><code>show_constants</code></dt><dd><p>Should constant values be shown? Default
is FALSE.
</p>
</dd>
</dl>

</dd>
</dl>



<h3>References</h3>

<p>O'Reilly, R. C., Munakata, Y., Frank, M. J., Hazy, T. E., and
Contributors (2016). Computational Cognitive Neuroscience. Wiki Book, 3rd
(partial) Edition. URL: <a href="http://ccnbook.colorado.edu">http://ccnbook.colorado.edu</a>
</p>
<p>Have also a look at
<a href="https://grey.colorado.edu/emergent/index.php/Leabra">https://grey.colorado.edu/emergent/index.php/Leabra</a> (especially the
link to the 'MATLAB' code) and <a href="https://en.wikipedia.org/wiki/Leabra">https://en.wikipedia.org/wiki/Leabra</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>l &lt;- layer$new(c(5, 5)) # create a 5 x 5 layer with default leabra values

l$g_e_avg # private values cannot be accessed
# if you want to see alle variables, you need to use the function
l$get_layer_vars(show_dynamics = TRUE, show_constants = TRUE)
# if you want to see a summary of all units without constant values
l$get_unit_vars(show_dynamics = TRUE, show_constants = FALSE)

# let us clamp the activation of the 25 units to some random values between
# 0.05 and 0.95
l &lt;- layer$new(c(5, 5))
activations &lt;- runif(25, 0.05, .95)
l$avg_act
l$clamp_cycle(activations)
l$avg_act
# what happened to the unit activations?
l$get_unit_acts()
# compare with activations
activations
# scaled activations are scaled by the average activation of the layer and
# should be smaller
l$get_unit_scaled_acts()

</code></pre>

<hr>
<h2 id='leabRa'>leabRa: A package for biologically realistic neural networks based on Leabra</h2><span id='topic+leabRa'></span><span id='topic+leabRa-package'></span>

<h3>Description</h3>

<p>The Leabra package provides three classes to construct artificial neural
networks: <a href="#topic+unit">unit</a>, <a href="#topic+layer">layer</a> and <a href="#topic+network">network</a>.
</p>


<h3>Details</h3>

<p>Note that the classes in this package are <code><a href="R6.html#topic+R6Class">R6Class</a></code> classes.
</p>
<p>For further information check out the vignette with vignette(&quot;leabRa&quot;).
</p>

<hr>
<h2 id='network'>Leabra network class</h2><span id='topic+network'></span>

<h3>Description</h3>

<p>Class to simulate a biologically realistic network of neurons
(<code><a href="#topic+unit">unit</a>s</code>) organized in <code><a href="#topic+layer">layer</a>s</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>network
</code></pre>


<h3>Format</h3>

<p><code><a href="R6.html#topic+R6Class">R6Class</a></code> object.</p>


<h3>Details</h3>

<p>This class simulates a biologically realistic artificial neuronal network in
the Leabra framework (e.g. O'Reilly et al., 2016). It consists of several
<code><a href="#topic+layer">layer</a></code> objects in the variable (field) <code>layers</code> and some
network-specific variables.
</p>


<h3>Value</h3>

<p>Object of <code><a href="R6.html#topic+R6Class">R6Class</a></code> with methods for calculating changes
of activation in a network of neurons organized in <code><a href="#topic+layer">layer</a></code>s.
</p>


<h3>Fields</h3>


<dl>
<dt><code>layers</code></dt><dd><p>A list of <code><a href="#topic+layer">layer</a></code> objects.</p>
</dd>
<dt><code>lrate</code></dt><dd><p>Learning rate, gain factor for how much the connection weights
should change when the method <code>chg_wt()</code> is called.</p>
</dd>
</dl>


<h3>Methods</h3>


<dl>
<dt><code>new(dim_lays, cxn, g_i_gain = rep(2, length(dim_lays)),
  w_init_fun = function(x) runif(x, 0.3, 0.7), w_init = NULL)</code></dt><dd><p>Creates an
object of this class with default parameters.
</p>

<dl>
<dt><code>dim_lays</code></dt><dd><p>List of number pairs for rows and columns of the
layers, e.g. <code>list(c(5, 5), c(10, 10), c(5, 5))</code> for a 25 x 100 x
25 network.
</p>
</dd>
<dt><code>cxn</code></dt><dd><p>Matrix specifying connection strength between layers,
if layer j sends projections to layer i, then <code>cxn[i, j] =
      strength &gt; 0</code> and 0 otherwise. Strength specifies the relative strength
of that connection with respect to the other projections to layer i.
</p>
</dd>
<dt><code>g_i_gain</code></dt><dd><p>Vector of inhibitory conductance gain values for
every layer. This comes in handy to control overall level of inhibition
of specific layers. Default is 2 for every layer.
</p>
</dd>
<dt><code>w_init_fun</code></dt><dd><p>Function that specifies how random weights
should be created, default value is to generate weights between 0.3 and
0.7 from a uniform distribution. It is close to 0.5 because the weights
are contrast enhanced internally, so will actually be in a wider range.
</p>
</dd>
<dt><code>w_init</code></dt><dd><p>Matrix of initial weight matrices (like a cell
array in 'MATLAB'), this is analogous to <code>cxn</code>, i.e.
<code>w_init[i, j]</code> contains the initial weight matrix for the
connection from layer j to i.  If you specify a <code>w_init</code>,
<code>w_init_fun</code> is ignored. You can use this if you want to have full
control over the weight matrix.
</p>
</dd>
</dl>

</dd>
<dt><code>cycle(ext_inputs, clamp_inp)</code></dt><dd><p>Iterates one time step
with the network object with external inputs.
</p>

<dl>
<dt><code>ext_inputs</code></dt><dd><p>A list of matrices; ext_inputs[[i]] is a
matrix that for layer i specifies the external input to each of its
units. An empty matrix (<code>NULL</code>) denotes no input to that layer.
You can also use a vector instead of a matrix, because the matrix is
vectorized anyway.
</p>
</dd>
<dt><code>clamp_inp</code></dt><dd><p>Logical variable; TRUE: external inputs are
clamped to the activities of the units in the layers, FALSE: external
inputs are summed to excitatory conductance values (note: not to the
activation) of the units in the layers.
</p>
</dd>
</dl>

</dd>
<dt><code>chg_wt()</code></dt><dd><p>Changes the weights of the entire network with the
XCAL learning equation.
</p>
</dd>
<dt><code>reset(random = F)</code></dt><dd><p>Sets the activation of all units in all
layers to 0, and sets all activation time averages to that value. Used to
begin trials from a random stationary point. The activation values may also
be set to a random value.
</p>

<dl>
<dt><code>random</code></dt><dd><p>Logical variable, if TRUE set activation randomly
between .05 and .95, if FALSE set activation to 0, which is the
default.
</p>
</dd>
</dl>

</dd>
<dt><code>create_inputs(which_layers, n_inputs, prop_active =
  0.3)</code></dt><dd><p>Returns a list of length <code>n_inputs</code> with random input patterns
(either 0.05, or. 0.95) for the layers specified in <code>which_layers</code>.
All other layers will have an input of NULL.
</p>

<dl>
<dt><code>which_layers</code></dt><dd><p>Vector of layer numbers, for which you want
to create random inputs.
</p>
</dd>
<dt><code>n_inputs</code></dt><dd><p>Single numeric value, how many inputs should be
created.
</p>
</dd>
<dt><code>prop_active</code></dt><dd><p>Average proportion of active units in the
input patterns, default is 0.3.
</p>
</dd>
</dl>

</dd>
<dt><code>learn_error_driven(inputs_minus, inputs_plus, lrate = 0.1,
  n_cycles_minus = 50, n_cycles_plus = 25, random_order = FALSE,
  show_progress = TRUE)</code></dt><dd><p>Learns to
associate specific inputs with specific outputs in an error-driven fashion.
</p>

<dl>
<dt><code>inputs_minus</code></dt><dd><p>Inputs for the minus phase (the to be
learned output is not presented).
</p>
</dd>
<dt><code>inputs_plus</code></dt><dd><p>Inputs for the plus phase (the to be learned
output is presented).
</p>
</dd>
<dt><code>lrate</code></dt><dd><p>Learning rate, default is 0.1.
</p>
</dd>
<dt><code>n_cycles_minus</code></dt><dd><p>How many cycles to run in the minus phase,
default is 50.
</p>
</dd>
<dt><code>n_cycles_plus</code></dt><dd><p>How many cycles to run in the plus phase,
default is 25.
</p>
</dd>
<dt><code>random_order</code></dt><dd><p>Should the order of stimulus presentation be
randomized? Default is FALSE.
</p>
</dd>
<dt><code>show_progress</code></dt><dd><p>Whether progress of learning should be
shown. Default is TRUE.
</p>
</dd>
</dl>

</dd>
<dt><code>learn_self_organized(inputs, lrate = 0.1, n_cycles = 50,
 random_order = FALSE, show_progress = TRUE)</code></dt><dd><p>Learns to categorize inputs in
a self-organized fashion.
</p>

<dl>
<dt><code>inputs</code></dt><dd><p>Inputs for cycling.
</p>
</dd>
<dt><code>lrate</code></dt><dd><p>Learning rate, default is 0.1.
</p>
</dd>
<dt><code>n_cycles</code></dt><dd><p>How many cycles to run, default is 50.
</p>
</dd>
<dt><code>random_order</code></dt><dd><p>Should the order of stimulus presentation be
randomized? Default is FALSE.
</p>
</dd>
<dt><code>show_progress</code></dt><dd><p>Whether progress of learning should be
shown. Default is TRUE.
</p>
</dd>
</dl>

</dd>
<dt><code>test_inputs = function(inputs, n_cycles = 50, show_progress =
   FALSE)</code></dt><dd><p>Tests inputs without changing the weights (without learning).
This is usually done after several learning runs.
</p>

<dl>
<dt><code>inputs</code></dt><dd><p>Inputs for cycling.
</p>
</dd>
<dt><code>n_cycles</code></dt><dd><p>How many cycles to run, default is 50.
</p>
</dd>
<dt><code>show_progress</code></dt><dd><p>Whether progress of learning should be
shown. Default is FALSE.
</p>
</dd>
</dl>

</dd>
<dt><code>mad_per_epoch(outs_per_epoch, inputs_plus,
  layer)</code></dt><dd><p>Calculates mean absolute distance for two lists of activations
for a specific layer. This can be used to compare whether the network has
learned what it was supposed to learn.
</p>

<dl>
<dt><code>outs_per_epoch</code></dt><dd><p>Output activations for entire network for
each trial for every epoch. This is what the network produced on its
own.</p>
</dd>
<dt><code>inputs_plus</code></dt><dd><p>Original inputs for the plus phase. This is
what the network was supposed to learn.
</p>
</dd>
<dt><code>layer</code></dt><dd><p>Single numeric, for which layer to calculate the
mean absolute distance. Usually, this is the &quot;output&quot; layer.
</p>
</dd>
</dl>

</dd>
<dt><code>set_weights(weights)</code></dt><dd><p>Sets new weights for entire network,
useful to load networks that have already learned and thus very specific
weights.
</p>

<dl>
<dt><code>weights</code></dt><dd><p>Matrix of matrices (like a cell array in
'MATLAB') with new weight values.
</p>
</dd>
</dl>

</dd>
<dt><code>get_weights()</code></dt><dd><p>Returns the complete weight matrix, <code>w[i,
  j]</code> contains the weight matrix for the projections from layer j to layer i.
Note that this is a matrix of matrices (equivalent to a 'MATLAB' cell
array).
</p>
</dd>
<dt><code>get_layer_and_unit_vars(show_dynamics = T, show_constants =
  F)</code></dt><dd><p>Returns a data frame with the current state of all layer and unit
variables. Every row is a unit. You can choose whether you want dynamic
values and / or constant values. This might be useful if you want to
analyze what happens in the network overall, which would otherwise not be
possible, because most of the variables (fields) are private in the layer
and unit class.
</p>

<dl>
<dt><code>show_dynamics</code></dt><dd><p>Should dynamic values be shown? Default is
TRUE.
</p>
</dd>
<dt><code>show_constants</code></dt><dd><p>Should constant values be shown? Default
is FALSE.
</p>
</dd>
</dl>

</dd>
<dt><code>get_network_vars(show_dynamics = T, show_constants =
  F)</code></dt><dd><p>Returns a data frame with 1 row with the current state of the
variables in the network. You can choose whether you want dynamic values
and / or constant values. This might be useful if you want to analyze what
happens in a network, which would otherwise not be possible, because some
of the variables (fields) are private in the network class. There are some
additional variables in the network class that cannot be extracted this way
because they are matrices; if it is necessary to extract them, look at the
source code.
</p>

<dl>
<dt><code>show_dynamics</code></dt><dd><p>Should dynamic values be shown? Default is
TRUE.
</p>
</dd>
<dt><code>show_constants</code></dt><dd><p>Should constant values be shown? Default
is FALSE.
</p>
</dd>
</dl>

</dd>
</dl>



<h3>References</h3>

<p>O'Reilly, R. C., Munakata, Y., Frank, M. J., Hazy, T. E., and
Contributors (2016). Computational Cognitive Neuroscience. Wiki Book, 3rd
(partial) Edition. URL: <a href="http://ccnbook.colorado.edu">http://ccnbook.colorado.edu</a>
</p>
<p>Have also a look at
<a href="https://grey.colorado.edu/emergent/index.php/Leabra">https://grey.colorado.edu/emergent/index.php/Leabra</a> (especially the
link to the 'MATLAB' code) and <a href="https://en.wikipedia.org/wiki/Leabra">https://en.wikipedia.org/wiki/Leabra</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># create a small network with 3 layers
dim_lays &lt;- list(c(2, 5), c(2, 10), c(2, 5))
cxn &lt;- matrix(c(0, 0, 0,
                1, 0, 0.2,
                0, 1, 0), nrow = 3, byrow = TRUE)
net &lt;- network$new(dim_lays, cxn)

net$m_in_s # private values cannot be accessed
# if you want to see alle variables, you need to use the function
net$get_network_vars(show_dynamics = TRUE, show_constants = TRUE)
# if you want to see a summary of all units (with layer information) without
# constant values
net$get_layer_and_unit_vars(show_dynamics = TRUE, show_constants = FALSE)

# let us create 10 random inputs for layer 1 and 3
inputs &lt;- net$create_inputs(c(1, 3), 10)
inputs # a list of lists

# the input in layer 1 should be associated with the output in layer 3; we
# can use error driven learning to achieve this

# first we will need the input for the minus phase (where no correct output
# is presented; layer 3 is NULL)
inputs_minus &lt;- lapply(inputs, function(x) replace(x, 3, list(NULL)))
inputs_minus # layer 3 is indeed NULL
# now we can learn with default parameters; we will run 10 epochs,
# inputs_plus is equivalent to inputs; the output will be activations after
# each trial for the wohle network; this might take a while depending on your
# system
n_epochs &lt;- 10
## Not run: 
output &lt;- lapply(seq(n_epochs),
                 function(x) net$learn_error_driven(inputs_minus,
                                                    inputs,
                                                    lrate = 0.5))
# let's compare the actual output with what should have been learned we can
# use the method mad_per_epoch for this; it will calculate the mean absolute
# distance for each epoch; we are interested in layer 3
mad &lt;- net$mad_per_epoch(output, inputs, 3)
# the error should decrease with increasing epoch number
plot(mad)
## End(Not run)

</code></pre>

<hr>
<h2 id='unit'>Leabra unit (neuron) class</h2><span id='topic+unit'></span>

<h3>Description</h3>

<p>This class simulates a biologically realistic neuron (also called unit) in
the Leabra framework. When you use the layer class, you will see that a
<a href="#topic+layer">layer</a> object has a variable (field) <code>units</code>, which is a list of
unit objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unit
</code></pre>


<h3>Format</h3>

<p><code><a href="R6.html#topic+R6Class">R6Class</a></code> object.</p>


<h3>Value</h3>

<p>Object of <code><a href="R6.html#topic+R6Class">R6Class</a></code> with methods for calculating neuron
activation changes.
</p>


<h3>Fields</h3>


<dl>
<dt><code>activation</code></dt><dd><p>Percentage activation (&quot;firing rate&quot;) of the unit, which is
sent to other units, think of it as a percentage of how many neurons are
active in a microcolumn of 100 neurons.</p>
</dd>
<dt><code>avg_s</code></dt><dd><p>Short-term running average activation, integrates over avg_ss (a
private variable, which integrates over activation), represents plus phase
learning signal.</p>
</dd>
<dt><code>avg_m</code></dt><dd><p>Medium-term running average activation, integrates over avg_s,
represents minus phase learning signal.</p>
</dd>
<dt><code>avg_l</code></dt><dd><p>Long-term running average activation, integrates over avg_m,
drives long-term floating average for self-organized learning.</p>
</dd>
<dt><code>unit_number</code></dt><dd><p>Number of unit in layer, if the unit is not created within
a layer, this value will be 1.</p>
</dd>
</dl>


<h3>Methods</h3>


<dl>
<dt><code>new()</code></dt><dd><p>Creates an object of this class with default
parameters.</p>
</dd>
<dt><code>cycle(g_e_raw, g_i)</code></dt><dd><p>Cycles 1 ms with given excitatory
conductance <code>g_e_raw</code> and inhibitory conductance <code>g_i</code>.
Excitatory conductance depends on the connection weights to other units and
the activity of those other units. Inhibitory conductance depends on
feedforward and feedback inhibition. See <a href="#topic+layer">layer</a> cycle method.
</p>

<dl>
<dt><code>g_e_raw</code></dt><dd><p>Raw excitatory conductance. The actual excitatory
conductance will incrementally approach this value with every cycle.</p>
</dd>
<dt><code>g_i</code></dt><dd><p>Inhibitory conductance.</p>
</dd>
</dl>

</dd>
<dt><code>clamp_cycle(activation)</code></dt><dd><p>Clamps the value of <code>activation</code>
to the <code>activation</code> variable of the unit without any time integration.
Then updates averages (<code>avg_ss</code>, <code>avg_s</code>, <code>avg_m</code>). This is
usually done when presenting external input.
</p>

<dl>
<dt><code>activation</code></dt><dd><p>Activation to clamp.</p>
</dd>
</dl>

</dd>
<dt><code>updt_avg_l()</code></dt><dd><p>Updates the variable <code>avg_l</code>. This usually
happens before the weights are changed in the network (after the plus
phase), and not every cycle.</p>
</dd>
<dt><code>get_vars(show_dynamics = TRUE, show_constants =
  FALSE)</code></dt><dd><p>Returns a data frame with 1 row with the current state of all the
variables of the unit. You can choose whether you want dynamic values and /
or constant values. This might be useful if you want to analyze what
happens in a unit, which would otherwise not be possible, because most of
the variables (fields) are private in this class.
</p>

<dl>
<dt><code>show_dynamics</code></dt><dd><p>Should dynamic values be shown? Default is
TRUE
</p>
</dd>
<dt><code>show_constants</code></dt><dd><p>Should constant values be shown? Default
is FALSE
</p>
</dd>
</dl>

</dd>
</dl>



<h3>References</h3>

<p>O'Reilly, R. C., Munakata, Y., Frank, M. J., Hazy, T. E., and
Contributors (2016). Computational Cognitive Neuroscience. Wiki Book, 3rd
(partial) Edition. URL: <a href="http://ccnbook.colorado.edu">http://ccnbook.colorado.edu</a>
</p>
<p>Have also a look at
<a href="https://grey.colorado.edu/emergent/index.php/Leabra">https://grey.colorado.edu/emergent/index.php/Leabra</a> (especially the
link to the 'MATLAB' code) and <a href="https://en.wikipedia.org/wiki/Leabra">https://en.wikipedia.org/wiki/Leabra</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>u &lt;- unit$new() # creates a new unit with default leabra values

print(u) # a lot of private values
u$v # private values cannot be accessed
# if you want to see alle variables, you need to use the function
u$get_vars(show_dynamics = TRUE, show_constants = TRUE)

# let us clamp the activation to 0.7
u$activation
u$clamp_cycle(0.7)
c(u$activation, u$avg_s, u$avg_m, u$avg_l)
# activation is indeed 0.7, but avg_l was not updated, this only happens
# before the weights are changed, let us update it now
u$updt_avg_l()
c(u$activation, u$avg_s, u$avg_m, u$avg_l)
# seems to work

# let us run 10 cycles with unclamped activation and output the activation
# produced because of changes in conductance
u &lt;- unit$new()
cycle_number &lt;- 1:10
result &lt;- lapply(cycle_number, function(x)
                 u$cycle(g_e_raw = 0.5, g_i = 0.5)$get_vars())
# make a data frame out of the list
result &lt;- plyr::ldply(result)
# plot activation
plot(result$activation, type = "b", xlab = "cycle", ylab = "activation")
# add conductance g_e to plot, should approach g_e_raw
lines(result$g_e, type = "b", col = "blue")

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
