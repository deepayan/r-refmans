<!DOCTYPE html><html><head><title>Help for package Twitmo</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {Twitmo}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#%&gt;%'><p>Pipe operator</p></a></li>
<li><a href='#cluster_tweets'><p>Cluster tweets on an interactive map</p></a></li>
<li><a href='#filter_tweets'><p>Filter tweets</p></a></li>
<li><a href='#find_lda'><p>Find best LDA model</p></a></li>
<li><a href='#find_stm'><p>Find best STM/CTM</p></a></li>
<li><a href='#fit_ctm'><p>Fit CTM (Correlated topic model)</p></a></li>
<li><a href='#fit_lda'><p>Fit LDA Topic Model</p></a></li>
<li><a href='#fit_stm'><p>Fit STM (Structural topic model)</p></a></li>
<li><a href='#get_tweets'><p>Sample tweets by streaming or searching</p></a></li>
<li><a href='#lda_distribution'><p>View distribution of fitted LDA Models</p></a></li>
<li><a href='#lda_hashtags'><p>View Documents (hashtags) heavily associated with topics</p></a></li>
<li><a href='#lda_terms'><p>View Terms heavily associated with each topic</p></a></li>
<li><a href='#load_tweets'><p>Converts Twitter stream data (JSON file) into parsed data frame</p></a></li>
<li><a href='#plot_hashtag'><p>Plot tweets containing certain hashtag</p></a></li>
<li><a href='#plot_tweets'><p>Plot tweets on a static map</p></a></li>
<li><a href='#pool_tweets'><p>Prepare Tweets for topic modeling by pooling</p></a></li>
<li><a href='#predict_lda'><p>Predict topics of tweets using fitted LDA model</p></a></li>
<li><a href='#to_ldavis'><p>Create interactive visualization with LDAvis</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Twitter Topic Modeling and Visualization for R</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.2</td>
</tr>
<tr>
<td>Description:</td>
<td>Tailored for topic modeling with tweets and fit for visualization tasks in R.
    Collect, pre-process and analyze the contents of tweets using 
    LDA and structural topic models (STM). Comes with visualizing capabilities like tweet and hashtag maps 
    and built-in support for 'LDAvis'.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/abuchmueller/Twitmo">https://github.com/abuchmueller/Twitmo</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/abuchmueller/Twitmo/issues">https://github.com/abuchmueller/Twitmo/issues</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>jsonlite, stats, plyr, stopwords, stringr, dplyr, readr,
magrittr, rtweet, quanteda, quanteda.textstats, topicmodels,
stm, tidyr, rlang, maps, LDAvis, leaflet, ldatuning, stringi,
tm</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>Suggests:</td>
<td>rmarkdown, knitr, tidytext, modeltools, servr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-12-06 15:07:03 UTC; abuchmueller</td>
</tr>
<tr>
<td>Author:</td>
<td>Andreas Buchmueller [aut, cre] (github.com/abuchmueller),
  Gillian Kant <a href="https://orcid.org/0000-0003-2346-2841"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, ths],
  Christoph Weisser <a href="https://orcid.org/0000-0003-0616-1027"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, ths],
  Benjamin Saefken <a href="https://orcid.org/0000-0003-4702-3333"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, ths],
  Thomas Kneib <a href="https://orcid.org/0000-0003-3390-0972"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [rev, ths, dgs],
  Krisztina Kis-Katos
    <a href="https://orcid.org/0000-0003-2459-1274"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [rev]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Andreas Buchmueller &lt;a.buchmueller@stud.uni-goettingen.de&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-12-06 15:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='+25+26gt+3B+25'>Pipe operator</h2><span id='topic++25+3E+25'></span>

<h3>Description</h3>

<p>See <code>magrittr::<a href="magrittr.html#topic+pipe">%&gt;%</a></code> for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lhs %&gt;% rhs
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="+2B25+2B26gt+2B3B+2B25_+3A_lhs">lhs</code></td>
<td>
<p>A value or the magrittr placeholder.</p>
</td></tr>
<tr><td><code id="+2B25+2B26gt+2B3B+2B25_+3A_rhs">rhs</code></td>
<td>
<p>A function call using the magrittr semantics.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The result of calling 'rhs(lhs)'.
</p>

<hr>
<h2 id='cluster_tweets'>Cluster tweets on an interactive map</h2><span id='topic+cluster_tweets'></span>

<h3>Description</h3>

<p>Plot into clusters on an interactive map
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cluster_tweets(data, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cluster_tweets_+3A_data">data</code></td>
<td>
<p>A data frame of tweets parsed by <a href="#topic+load_tweets">load_tweets</a> or returned by <a href="#topic+pool_tweets">pool_tweets</a>.</p>
</td></tr>
<tr><td><code id="cluster_tweets_+3A_...">...</code></td>
<td>
<p>Extra arguments passed to <a href="leaflet.html#topic+markerClusterOptions">markerClusterOptions</a></p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function can be used to create interactive maps on OpenStreetView.
</p>


<h3>Value</h3>

<p>Interactive leaflet map
</p>


<h3>See Also</h3>

<p><a href="leaflet.html#topic+tileOptions">tileOptions</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

library(Twitmo)

# load tweets (included in package)
mytweets &lt;- load_tweets(system.file("extdata", "tweets_20191027-141233.json", package = "Twitmo"))

pool &lt;- pool_tweets(mytweets)
cluster_tweets(mytweets)

# OR
cluster_tweets(pool$data)

## End(Not run)

</code></pre>

<hr>
<h2 id='filter_tweets'>Filter tweets</h2><span id='topic+filter_tweets'></span>

<h3>Description</h3>

<p>Filter tweets by keywords.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>filter_tweets(data, keywords, include = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="filter_tweets_+3A_data">data</code></td>
<td>
<p>Data frame of parsed tweets. Obtained either by using <code><a href="#topic+load_tweets">load_tweets</a></code>  or
<code><a href="jsonlite.html#topic+stream_in">stream_in</a></code> in conjunction with <code><a href="rtweet.html#topic+tweets_with_users">tweets_with_users</a></code>.</p>
</td></tr>
<tr><td><code id="filter_tweets_+3A_keywords">keywords</code></td>
<td>
<p>Character string of keywords for black- or whitelisting provided via a comma separated character string.</p>
</td></tr>
<tr><td><code id="filter_tweets_+3A_include">include</code></td>
<td>
<p>Logical. Indicate where to perform exclusive or inclusive filtering.
Inclusive filtering is akin to whitelisting keywords. Exclusive filtering is blacklisting certain keywords.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Use this function if you want your Tweets to contain certain keywords.
This can be used for iterative filtering to create more coherent topic models.
Keyword filtering is always case insensitive (lowercase).
</p>


<h3>Value</h3>

<p>Data frame of Tweets containing specified keywords
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

library(Twitmo)


# load tweets (included in package)
mytweets &lt;- load_tweets(system.file("extdata", "tweets_20191027-141233.json", package = "Twitmo"))

# Exclude Tweets that mention "football" and/or "mood"
keyword_dict &lt;- "football,mood"
mytweets_reduced &lt;- filter_tweets(mytweets, keywords = keyword_dict, include = FALSE)

## End(Not run)
</code></pre>

<hr>
<h2 id='find_lda'>Find best LDA model</h2><span id='topic+find_lda'></span>

<h3>Description</h3>

<p>Find the optimal hyperparameter k for your LDA model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>find_lda(pooled_dfm, search_space = seq(1, 10, 2), method = "Gibbs", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="find_lda_+3A_pooled_dfm">pooled_dfm</code></td>
<td>
<p>object of class dfm (see <a href="quanteda.html#topic+dfm">dfm</a>) containing (pooled) tweets</p>
</td></tr>
<tr><td><code id="find_lda_+3A_search_space">search_space</code></td>
<td>
<p>Vector with number of topics to compare different models.</p>
</td></tr>
<tr><td><code id="find_lda_+3A_method">method</code></td>
<td>
<p>The method to be used for fitting.
Currently method = &quot;VEM&quot; or method = &quot;Gibbs&quot; are supported.</p>
</td></tr>
<tr><td><code id="find_lda_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <a href="ldatuning.html#topic+FindTopicsNumber">FindTopicsNumber</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Plot with different metrics compared.
</p>


<h3>See Also</h3>

<p><a href="ldatuning.html#topic+FindTopicsNumber">FindTopicsNumber</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

library(Twitmo)

# load tweets (included in package)
mytweets &lt;- load_tweets(system.file("extdata", "tweets_20191027-141233.json", package = "Twitmo"))

# Pool tweets into longer pseudo-documents
pool &lt;- pool_tweets(data = mytweets)
pooled_dfm &lt;- pool$document_term_matrix

# use the ldatuner to compare different K
find_lda(pooled_dfm, search_space = seq(1, 10, 1),  method = "Gibbs")

## End(Not run)
</code></pre>

<hr>
<h2 id='find_stm'>Find best STM/CTM</h2><span id='topic+find_stm'></span>

<h3>Description</h3>

<p>Gridsearch for optimal K for your STM/CTM
</p>


<h3>Usage</h3>

<pre><code class='language-R'>find_stm(data, search_space = seq(4, 20, by = 2), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="find_stm_+3A_data">data</code></td>
<td>
<p>Either a pooled dfm object returned by <a href="#topic+pool_tweets">pool_tweets</a> or
a named list of pre-processed tweets for stm modeling returned by <code><a href="#topic+fit_stm">fit_stm</a></code>.</p>
</td></tr>
<tr><td><code id="find_stm_+3A_search_space">search_space</code></td>
<td>
<p>Vector with number of topics to compare different models.</p>
</td></tr>
<tr><td><code id="find_stm_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to <a href="stm.html#topic+searchK">searchK</a></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Wrapper function around <code><a href="stm.html#topic+searchK">searchK</a></code> for pooled dfm objects returned by
<a href="#topic+pool_tweets">pool_tweets</a> and prepped stm documents returned by <code><a href="#topic+fit_stm">fit_stm</a></code>.
</p>


<h3>Value</h3>

<p>Plot with different metrics compared.
</p>


<h3>See Also</h3>

<p><a href="stm.html#topic+searchK">searchK</a>
</p>
<p><a href="stm.html#topic+searchK">searchK</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

library(Twitmo)

# load tweets (included in package)
mytweets &lt;- load_tweets(system.file("extdata", "tweets_20191027-141233.json", package = "Twitmo"))

# Pool tweets into longer pseudo-documents
pool &lt;- pool_tweets(data = mytweets)
pooled_dfm &lt;- pool$document_term_matrix

# compare different K for CTM
find_stm(pooled_dfm, search_space = seq(1, 10, 1))

# OR

# compare different K for STM
prepped_stm &lt;- stm_model$prep
find_stm(prepped_stm, search_space = seq(4, 16, by = 2))

## End(Not run)
</code></pre>

<hr>
<h2 id='fit_ctm'>Fit CTM (Correlated topic model)</h2><span id='topic+fit_ctm'></span>

<h3>Description</h3>

<p>Estimate a CTM topic model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit_ctm(pooled_dfm, n_topics = 2L, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fit_ctm_+3A_pooled_dfm">pooled_dfm</code></td>
<td>
<p>Object of class dfm (see <a href="quanteda.html#topic+dfm">dfm</a>) containing (pooled) Tweets.</p>
</td></tr>
<tr><td><code id="fit_ctm_+3A_n_topics">n_topics</code></td>
<td>
<p>Integer with number of topics</p>
</td></tr>
<tr><td><code id="fit_ctm_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <a href="stm.html#topic+stm">stm</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class <a href="stm.html#topic+stm">stm</a>
</p>


<h3>See Also</h3>

<p><a href="stm.html#topic+stm">stm</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

library(Twitmo)

# load tweets (included in package)
mytweets &lt;- load_tweets(system.file("extdata", "tweets_20191027-141233.json", package = "Twitmo"))

# Pool tweets into longer pseudo-documents
pool &lt;- pool_tweets(data = mytweets)
pooled_dfm &lt;- pool$document_term_matrix

# fit your CTM with 7 topics
ctm_model &lt;- fit_ctm(pooled_dfm, n_topics =7)

## End(Not run)
</code></pre>

<hr>
<h2 id='fit_lda'>Fit LDA Topic Model</h2><span id='topic+fit_lda'></span>

<h3>Description</h3>

<p>Estimate a LDA topic model using VEM or Gibbs Sampling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit_lda(pooled_dfm, n_topics, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fit_lda_+3A_pooled_dfm">pooled_dfm</code></td>
<td>
<p>Object of class dfm (see <a href="quanteda.html#topic+dfm">dfm</a>) containing (pooled) tweets.</p>
</td></tr>
<tr><td><code id="fit_lda_+3A_n_topics">n_topics</code></td>
<td>
<p>Integer with number of topics.</p>
</td></tr>
<tr><td><code id="fit_lda_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <a href="topicmodels.html#topic+LDA">LDA</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class <a href="topicmodels.html#topic+TopicModel-class">LDA</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

library(Twitmo)

# load tweets (included in package)
mytweets &lt;- load_tweets(system.file("extdata", "tweets_20191027-141233.json", package = "Twitmo"))

# Pool tweets into longer pseudo-documents
pool &lt;- pool_tweets(data = mytweets)
pooled_dfm &lt;- pool$document_term_matrix

# fit your LDA model with 7 topics
model &lt;- fit_lda(pooled_dfm, n_topics = 7, method = "Gibbs")

## End(Not run)
</code></pre>

<hr>
<h2 id='fit_stm'>Fit STM (Structural topic model)</h2><span id='topic+fit_stm'></span>

<h3>Description</h3>

<p>Estimate a structural topic model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit_stm(
  data,
  n_topics = 2L,
  xcov,
  remove_punct = TRUE,
  stem = TRUE,
  remove_url = TRUE,
  remove_emojis = TRUE,
  stopwords = "en",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fit_stm_+3A_data">data</code></td>
<td>
<p>Data frame of parsed tweets. Obtained either by using <code><a href="#topic+load_tweets">load_tweets</a></code>  or
<code><a href="jsonlite.html#topic+stream_in">stream_in</a></code> in conjunction with <code><a href="rtweet.html#topic+tweets_with_users">tweets_with_users</a></code>.</p>
</td></tr>
<tr><td><code id="fit_stm_+3A_n_topics">n_topics</code></td>
<td>
<p>Integer with number of topics.</p>
</td></tr>
<tr><td><code id="fit_stm_+3A_xcov">xcov</code></td>
<td>
<p>Either a \[stats]formula with an empty left-hand side specifying external covariates
(meta data) to use.e.g. <code>~favourites_count + retweet_count</code>
or a character vector (<code>c("favourites_count", "retweet_count")</code>)
or comma seperated character string (<code>"favourites_count,retweet_count"</code>)
with column names implying which metadata to use as external covariates.</p>
</td></tr>
<tr><td><code id="fit_stm_+3A_remove_punct">remove_punct</code></td>
<td>
<p>Logical. Indicates whether punctuation (includes Twitter hashtags and usernames)
should be removed. Defaults to TRUE.</p>
</td></tr>
<tr><td><code id="fit_stm_+3A_stem">stem</code></td>
<td>
<p>Logical. If <code>TRUE</code> turn on word stemming for terms.</p>
</td></tr>
<tr><td><code id="fit_stm_+3A_remove_url">remove_url</code></td>
<td>
<p>Logical. If <code>TRUE</code> find and eliminate URLs beginning with http(s).</p>
</td></tr>
<tr><td><code id="fit_stm_+3A_remove_emojis">remove_emojis</code></td>
<td>
<p>Logical. If <code>TRUE</code> all emojis will be removed from tweets.</p>
</td></tr>
<tr><td><code id="fit_stm_+3A_stopwords">stopwords</code></td>
<td>
<p>a character vector, list of character vectors, <a href="quanteda.html#topic+dictionary">dictionary</a>
or collocations object. See <a href="quanteda.html#topic+pattern">pattern</a> for details.
Defaults to <a href="stopwords.html#topic+stopwords">stopwords(&quot;english&quot;)</a>.</p>
</td></tr>
<tr><td><code id="fit_stm_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <a href="stm.html#topic+stm">stm</a>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Use this to function estimate a STM from a data frame of parsed Tweets.
Works with unpooled Tweets only. Pre-processing and fitting is done in one run.
</p>


<h3>Value</h3>

<p>Object of class <a href="stm.html#topic+stm">stm</a>. Additionally, pre-processed documents are appended into a named list called &quot;prep&quot;.
</p>


<h3>See Also</h3>

<p><a href="stm.html#topic+stm">stm</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

library(Twitmo)

# load tweets (included in package)
mytweets &lt;- load_tweets(system.file("extdata", "tweets_20191027-141233.json", package = "Twitmo"))

# fit STM with tweets
stm_model &lt;- fit_stm(mytweets, n_topics = 7,
                     xcov = ~ retweet_count + followers_count + reply_count +
                     quote_count + favorite_count,
                     remove_punct = TRUE,
                     remove_url = TRUE,
                     remove_emojis = TRUE,
                     stem = TRUE,
                     stopwords = "en")

## End(Not run)
</code></pre>

<hr>
<h2 id='get_tweets'>Sample tweets by streaming or searching</h2><span id='topic+get_tweets'></span>

<h3>Description</h3>

<p>Collect Tweets via streaming or searching.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_tweets(
  method = "stream",
  location = c(-180, -90, 180, 90),
  timeout = Inf,
  keywords = "",
  n_max = 100L,
  file_name = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_tweets_+3A_method">method</code></td>
<td>
<p>Character string. Supported methods are streaming and searching.
The default method is streaming <code>method = 'stream'</code>. This is the recommended method as it allows
to collect larger volumes of data over time.
Use <code>method = 'search'</code> if you want to collect Tweets from the past 9 days.</p>
</td></tr>
<tr><td><code id="get_tweets_+3A_location">location</code></td>
<td>
<p>Character string of location to sample from. Can be a three letter country code i.e. &quot;USA&quot; or a city name like &quot;berlin&quot;.
Use <code>Twitmo:::bbox_country</code> for all supported country locations or <code>rtweet:::citycoords</code> for a list of supported cities.
Alternatively, use a vector of doubles with four latitude/longitude bounding box points provided via a vector of length 4, in the following format c(sw.long, sw.lat, ne.long, ne.lat) e.g., c(-125, 26, -65, 49).</p>
</td></tr>
<tr><td><code id="get_tweets_+3A_timeout">timeout</code></td>
<td>
<p>Integer. Limit streaming time in seconds. By default will stream indefinitely until user interrupts by pressing [ctrl + c].</p>
</td></tr>
<tr><td><code id="get_tweets_+3A_keywords">keywords</code></td>
<td>
<p>Character string of keywords provided via a comma separated character string.
Only for searching Tweets.If you want to stream Tweets for a certain location AND filter by keywords use the location parameter and after sampling use the <a href="#topic+filter_tweets">filter_tweets</a> function.
If you are using the search method instead of streaming keywords WILL work together with a location but will yield only a very limited number of Tweets.</p>
</td></tr>
<tr><td><code id="get_tweets_+3A_n_max">n_max</code></td>
<td>
<p>Integer value. Only applies to the <code>search</code> method. Limit how many Tweets are collected.</p>
</td></tr>
<tr><td><code id="get_tweets_+3A_file_name">file_name</code></td>
<td>
<p>Character string of desired file path and file name where Tweets will be saved.
If not specified, will write to stream_tweets.json in the current working directory.</p>
</td></tr>
<tr><td><code id="get_tweets_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <a href="rtweet.html#topic+stream_tweets">stream_tweets</a> or <a href="rtweet.html#topic+search_tweets">search_tweets</a>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A function that calls on <a href="rtweet.html#topic+stream_tweets">stream_tweets</a> and <a href="rtweet.html#topic+search_tweets">search_tweets</a>
(depending on the specified method) and is specifically tailored for sampling geo-tagged data.
This function provides supports additional arguments like location for convenient
sampling of geo-tagged Tweets. Tweets can be searched up to 9 days into the past.
</p>


<h3>Value</h3>

<p>Either a json file in the specified directory.
</p>


<h3>References</h3>

<p><a href="https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/api-reference/get-search-tweets">https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/api-reference/get-search-tweets</a>
<a href="https://developer.twitter.com/en/docs/twitter-api/v1/tweets/sample-realtime/api-reference/get-statuses-sample">https://developer.twitter.com/en/docs/twitter-api/v1/tweets/sample-realtime/api-reference/get-statuses-sample</a>
</p>


<h3>See Also</h3>

<p><a href="rtweet.html#topic+stream_tweets">stream_tweets</a>, <a href="rtweet.html#topic+search_tweets">search_tweets</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

# live stream tweets from Germany for 60 seconds and save to current working directory
get_tweets(method = "stream",
           location = "DEU",
           timeout = 60,
           file_name = "german_tweets.json")

# OR
# live stream tweets from berlin for an hour
get_tweets(method = "stream",
           location = "berlin",
           timeout = 3600,
           file_name = "berlin_tweets.json")

# OR
# use your own bounding box coordinates to strean tweets indefinitely (interrupt to stop)
get_tweets(method = 'stream',
           location = c(-125, 26, -65, 49),
           timeout = Inf)


## End(Not run)
</code></pre>

<hr>
<h2 id='lda_distribution'>View distribution of fitted LDA Models</h2><span id='topic+lda_distribution'></span>

<h3>Description</h3>

<p>View the distribution of your fitted LDA model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lda_distribution(lda_model, param = "gamma", tidy = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lda_distribution_+3A_lda_model">lda_model</code></td>
<td>
<p>Object of class <a href="topicmodels.html#topic+TopicModel-class">LDA</a>).</p>
</td></tr>
<tr><td><code id="lda_distribution_+3A_param">param</code></td>
<td>
<p>String. Specify either &quot;beta&quot; to return the term distribution
over topics (term per document) or &quot;gamma&quot; for the document distribution over.
topics (i.e. hashtag pool per topic probability).</p>
</td></tr>
<tr><td><code id="lda_distribution_+3A_tidy">tidy</code></td>
<td>
<p>Logical. Specify <code>TRUE</code> for return distribution in tidy format (tbl).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Data frame or tbl of Term (beta) or document (gamma) distribution over topics.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

library(Twitmo)

# load tweets (included in package)
mytweets &lt;- load_tweets(system.file("extdata", "tweets_20191027-141233.json", package = "Twitmo"))

# Pool tweets into longer pseudo-documents
pool &lt;- pool_tweets(data = mytweets)
pooled_dfm &lt;- pool$document_term_matrix

# fit your LDA model with 7 topics
model &lt;- fit_lda(pooled_dfm, n_topics = 7, method = "Gibbs")

# Choose either "beta" to return the term distribution
# over topics (term per document) or "gamma" for the document distribution over
# topics (hashtag pool per topic probability)
lda_distribution(model, param = "gamma")

## End(Not run)
</code></pre>

<hr>
<h2 id='lda_hashtags'>View Documents (hashtags) heavily associated with topics</h2><span id='topic+lda_hashtags'></span>

<h3>Description</h3>

<p>Convenience Function to extract the most likely topics for each hashtag.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lda_hashtags(lda_model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lda_hashtags_+3A_lda_model">lda_model</code></td>
<td>
<p>Fitted LDA Model. Object of class <a href="topicmodels.html#topic+TopicModel-class">LDA</a>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Data frame with most likely topic for each hashtag.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

library(Twitmo)

# load tweets (included in package)
mytweets &lt;- load_tweets(system.file("extdata", "tweets_20191027-141233.json", package = "Twitmo"))

# Pool tweets into longer pseudo-documents
pool &lt;- pool_tweets(data = mytweets)
pooled_dfm &lt;- pool$document_term_matrix

# fit your LDA model with 7 topics
model &lt;- fit_lda(pooled_dfm, n_topics = 7, method = "Gibbs")

lda_hashtags(model)

## End(Not run)
</code></pre>

<hr>
<h2 id='lda_terms'>View Terms heavily associated with each topic</h2><span id='topic+lda_terms'></span>

<h3>Description</h3>

<p>Convenience Function to extract the most likely terms for each topic.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lda_terms(lda_model, n_terms = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lda_terms_+3A_lda_model">lda_model</code></td>
<td>
<p>Fitted LDA Model. Object of class <a href="topicmodels.html#topic+TopicModel-class">LDA</a>).</p>
</td></tr>
<tr><td><code id="lda_terms_+3A_n_terms">n_terms</code></td>
<td>
<p>Integer number of terms to return.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Data frame with top n terms for each topic.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

library(Twitmo)

# load tweets (included in package)
mytweets &lt;- load_tweets(system.file("extdata", "tweets_20191027-141233.json", package = "Twitmo"))

# Pool tweets into longer pseudo-documents
pool &lt;- pool_tweets(data = mytweets)
pooled_dfm &lt;- pool$document_term_matrix

# fit your LDA model with 7 topics
model &lt;- fit_lda(pooled_dfm, n_topics = 7, method = "Gibbs")


## End(Not run)
</code></pre>

<hr>
<h2 id='load_tweets'>Converts Twitter stream data (JSON file) into parsed data frame</h2><span id='topic+load_tweets'></span>

<h3>Description</h3>

<p>Parse JSON files of collected Tweets
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load_tweets(file_name)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="load_tweets_+3A_file_name">file_name</code></td>
<td>
<p>Character string. Name of JSON file with data collected by
<a href="rtweet.html#topic+stream_tweets">stream_tweets</a> or <code>get_tweets()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function replaces <a href="rtweet.html#topic+parse_stream">parse_stream</a> which has been
deprecated in rtweet 0.7 but is included here to ensure backwards compatibility
for data streamed with older versions of <code>rtweet</code>.
Alternatively <a href="jsonlite.html#topic+stream_in">stream_in</a> in conjunction with <a href="rtweet.html#topic+tweets_with_users">tweets_with_users</a>
and <a href="rtweet.html#topic+lat_lng">lat_lng</a> can be used if data has been collected with rtweet 0.7 or newer.
</p>


<h3>Value</h3>

<p>A data frame of tweets data with additional meta data
</p>


<h3>See Also</h3>

<p><a href="rtweet.html#topic+parse_stream">parse_stream</a>, <a href="jsonlite.html#topic+stream_in">stream_in</a>, <a href="rtweet.html#topic+tweets_with_users">tweets_with_users</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

library(Twitmo)

# load tweets (included in package)
raw_path &lt;- system.file("extdata", "tweets_20191027-141233.json", package = "Twitmo")
mytweets &lt;- load_tweets(raw_path)

## End(Not run)
</code></pre>

<hr>
<h2 id='plot_hashtag'>Plot tweets containing certain hashtag</h2><span id='topic+plot_hashtag'></span>

<h3>Description</h3>

<p>Plot the locations of certain hashtag on a static map with base plot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_hashtag(
  data,
  region = ".",
  alpha = 0.01,
  hashtag = "",
  ignore_case = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_hashtag_+3A_data">data</code></td>
<td>
<p>A data frame of tweets parsed by <a href="#topic+load_tweets">load_tweets</a> or returned by <a href="#topic+pool_tweets">pool_tweets</a>.</p>
</td></tr>
<tr><td><code id="plot_hashtag_+3A_region">region</code></td>
<td>
<p>Character vector specifying region. Returns a world <a href="maps.html#topic+map">map</a> by default.
For higher resolutions specify a region.</p>
</td></tr>
<tr><td><code id="plot_hashtag_+3A_alpha">alpha</code></td>
<td>
<p>A double between 0 and 1 specifying the opacity of plotted points.
See <a href="maps.html#topic+iso3166">iso3166</a> for country codes.</p>
</td></tr>
<tr><td><code id="plot_hashtag_+3A_hashtag">hashtag</code></td>
<td>
<p>Character vector of the hashtag you want to plot.</p>
</td></tr>
<tr><td><code id="plot_hashtag_+3A_ignore_case">ignore_case</code></td>
<td>
<p>Logical, if TRUE will ignore case of hashtag.</p>
</td></tr>
<tr><td><code id="plot_hashtag_+3A_...">...</code></td>
<td>
<p>Extra arguments passed to <a href="graphics.html#topic+polygon">polygon</a> or <a href="graphics.html#topic+lines">lines</a>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function can be used to generate high resolution spatial plots of hashtags
Works with data frames of tweets returned by <a href="#topic+pool_tweets">pool_tweets</a> as well as data frames
read in by <a href="#topic+load_tweets">load_tweets</a> and then augmented by lat/lng coordinates with <a href="rtweet.html#topic+lat_lng">lat_lng</a>.
For larger view resize the plot window then call <code>plot_tweets</code> again.
</p>


<h3>Value</h3>

<p>Maps where each dot represents a tweet.
</p>


<h3>See Also</h3>

<p><a href="maps.html#topic+map">map</a>, <a href="maps.html#topic+iso3166">iso3166</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
library(Twitmo)

# load tweets (included in package)
mytweets &lt;- load_tweets(system.file("extdata", "tweets_20191027-141233.json", package = "Twitmo"))

# Plot tweets on mainland USA region
plot_hashtag(mytweets,
             region = "USA(?!:Alaska|:Hawaii)",
             hashtag = "breakfast",
             ignore_case=TRUE,
             alpha=1)

# Add title
title("My hashtags on a map")

## End(Not run)

</code></pre>

<hr>
<h2 id='plot_tweets'>Plot tweets on a static map</h2><span id='topic+plot_tweets'></span>

<h3>Description</h3>

<p>Plot tweets on a static map with base plot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_tweets(data, region = ".", alpha = 0.01, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_tweets_+3A_data">data</code></td>
<td>
<p>A data frame of tweets parsed by <a href="#topic+load_tweets">load_tweets</a> or returned by <a href="#topic+pool_tweets">pool_tweets</a>.</p>
</td></tr>
<tr><td><code id="plot_tweets_+3A_region">region</code></td>
<td>
<p>Character vector specifying region. Returns a world <a href="maps.html#topic+map">map</a> by default.
For higher resolutions specify a region.</p>
</td></tr>
<tr><td><code id="plot_tweets_+3A_alpha">alpha</code></td>
<td>
<p>A double between 0 and 1 specifying the opacity of plotted points.
See <a href="maps.html#topic+iso3166">iso3166</a> for country codes.</p>
</td></tr>
<tr><td><code id="plot_tweets_+3A_...">...</code></td>
<td>
<p>Extra arguments passed to <a href="graphics.html#topic+polygon">polygon</a> or <a href="graphics.html#topic+lines">lines</a>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function can be used to generate high resolution spatial plots of tweets.
Works with data frames of tweets returned by <a href="#topic+pool_tweets">pool_tweets</a> as well as data frames
read in by <a href="#topic+load_tweets">load_tweets</a> and then augmented by lat/lng coordinates with <a href="rtweet.html#topic+lat_lng">lat_lng</a>.
For larger view resize the plot window then call <code>plot_tweets</code> again.
</p>


<h3>Value</h3>

<p>Maps where each dot represents a tweet.
</p>


<h3>See Also</h3>

<p><a href="maps.html#topic+map">map</a>, <a href="maps.html#topic+iso3166">iso3166</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

library(Twitmo)

# Plot tweets on mainland USA
mytweets &lt;- load_tweets(system.file("extdata", "tweets_20191027-141233.json", package = "Twitmo"))

plot_tweets(mytweets, region = "USA(?!:Alaska|:Hawaii)", alpha=1)
# Add title
title("My tweets on a map")

## End(Not run)

</code></pre>

<hr>
<h2 id='pool_tweets'>Prepare Tweets for topic modeling by pooling</h2><span id='topic+pool_tweets'></span>

<h3>Description</h3>

<p>This function pools a data frame of parsed tweets into document pools.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pool_tweets(
  data,
  remove_numbers = TRUE,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_url = TRUE,
  remove_emojis = TRUE,
  remove_users = TRUE,
  remove_hashtags = TRUE,
  cosine_threshold = 0.9,
  stopwords = "en",
  n_grams = 1L
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pool_tweets_+3A_data">data</code></td>
<td>
<p>Data frame of parsed tweets. Obtained either by using <code><a href="#topic+load_tweets">load_tweets</a></code>  or
<code><a href="jsonlite.html#topic+stream_in">stream_in</a></code> in conjunction with <code><a href="rtweet.html#topic+tweets_with_users">tweets_with_users</a></code>.</p>
</td></tr>
<tr><td><code id="pool_tweets_+3A_remove_numbers">remove_numbers</code></td>
<td>
<p>Logical. If <code>TRUE</code> remove tokens that consist only of numbers,
but not words that start with digits, e.g. 2day. See <a href="quanteda.html#topic+tokens">tokens</a>.</p>
</td></tr>
<tr><td><code id="pool_tweets_+3A_remove_punct">remove_punct</code></td>
<td>
<p>Logical. If <code>TRUE</code> remove all characters in the Unicode
&quot;Punctuation&quot; [P] class, with exceptions for those used as prefixes for valid social media tags if
<code>preserve_tags = TRUE</code>. See <a href="quanteda.html#topic+tokens">tokens</a></p>
</td></tr>
<tr><td><code id="pool_tweets_+3A_remove_symbols">remove_symbols</code></td>
<td>
<p>Logical. If <code>TRUE</code> remove all characters in the Unicode &quot;Symbol&quot; [S] class.</p>
</td></tr>
<tr><td><code id="pool_tweets_+3A_remove_url">remove_url</code></td>
<td>
<p>Logical. If <code>TRUE</code> find and eliminate URLs beginning with http(s).</p>
</td></tr>
<tr><td><code id="pool_tweets_+3A_remove_emojis">remove_emojis</code></td>
<td>
<p>Logical. If <code>TRUE</code> all emojis will be removed from tweets.</p>
</td></tr>
<tr><td><code id="pool_tweets_+3A_remove_users">remove_users</code></td>
<td>
<p>Logical. If <code>TRUE</code> will remove all mentions of user names from documents.</p>
</td></tr>
<tr><td><code id="pool_tweets_+3A_remove_hashtags">remove_hashtags</code></td>
<td>
<p>Logical. If <code>TRUE</code> will remove hashtags (not only the symbol but the hashtagged word itself) from documents.</p>
</td></tr>
<tr><td><code id="pool_tweets_+3A_cosine_threshold">cosine_threshold</code></td>
<td>
<p>Double. Value between 0 and 1 specifying the cosine similarity threshold to be used
for document pooling. Tweets without a hashtag will be assigned to document (hashtag) pools
based upon this metric. Low thresholds will reduce topic coherence by including
a large number of tweets without a hashtag into the document pools. Higher thresholds will lead
to more coherent topics but will reduce document sizes.</p>
</td></tr>
<tr><td><code id="pool_tweets_+3A_stopwords">stopwords</code></td>
<td>
<p>a character vector, list of character vectors, <a href="quanteda.html#topic+dictionary">dictionary</a>
or collocations object. See <a href="quanteda.html#topic+pattern">pattern</a> for details.
Defaults to <a href="stopwords.html#topic+stopwords">stopwords(&quot;english&quot;)</a>.</p>
</td></tr>
<tr><td><code id="pool_tweets_+3A_n_grams">n_grams</code></td>
<td>
<p>Integer vector specifying the number of elements to be concatenated in each n-gram.
Each element of this vector will define a n in the n-gram(s) that are produced. See <a href="quanteda.html#topic+tokens_ngrams">tokens_ngrams</a></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Pools tweets by hashtags using cosine similarity to create
longer pseudo-documents for better LDA estimation and creates n-gram tokens.
The method applies an implementation of the pooling algorithm from Mehrotra et al. 2013.
</p>


<h3>Value</h3>

<p>List with <a href="quanteda.html#topic+corpus">corpus</a> object and <a href="quanteda.html#topic+dfm">dfm</a> object of pooled tweets.
</p>


<h3>References</h3>

<p>Mehrotra, Rishabh &amp; Sanner, Scott &amp; Buntine, Wray &amp; Xie, Lexing. (2013).
Improving LDA Topic Models for Microblogs via Tweet Pooling and Automatic Labeling.
889-892. 10.1145/2484028.2484166.
</p>


<h3>See Also</h3>

<p><a href="quanteda.html#topic+tokens">tokens</a>, <a href="quanteda.html#topic+dfm">dfm</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

library(Twitmo)

# load tweets (included in package)
mytweets &lt;- load_tweets(system.file("extdata", "tweets_20191027-141233.json", package = "Twitmo"))

pool &lt;- pool_tweets(data = mytweets,
                    remove_numbers = TRUE,
                    remove_punct = TRUE,
                    remove_symbols = TRUE,
                    remove_url = TRUE,
                    remove_users = TRUE,
                    remove_hashtags = TRUE,
                    remove_emojis = TRUE,
                    cosine_threshold = 0.9,
                    stopwords = "en",
                    n_grams = 1)

## End(Not run)
</code></pre>

<hr>
<h2 id='predict_lda'>Predict topics of tweets using fitted LDA model</h2><span id='topic+predict_lda'></span>

<h3>Description</h3>

<p>Predict topics of tweets using fitted LDA model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_lda(
  data,
  lda_model,
  response = "max",
  remove_numbers = TRUE,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_url = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_lda_+3A_data">data</code></td>
<td>
<p>Data frame of parsed tweets. Obtained either by using <code><a href="#topic+load_tweets">load_tweets</a></code>  or
<code><a href="jsonlite.html#topic+stream_in">stream_in</a></code> in conjunction with <code><a href="rtweet.html#topic+tweets_with_users">tweets_with_users</a></code>.</p>
</td></tr>
<tr><td><code id="predict_lda_+3A_lda_model">lda_model</code></td>
<td>
<p>Fitted LDA Model. Object of class <a href="topicmodels.html#topic+TopicModel-class">LDA</a>.</p>
</td></tr>
<tr><td><code id="predict_lda_+3A_response">response</code></td>
<td>
<p>Type of response. Either &quot;prob&quot; for probabilities or &quot;max&quot; one topic (default).</p>
</td></tr>
<tr><td><code id="predict_lda_+3A_remove_numbers">remove_numbers</code></td>
<td>
<p>Logical. If <code>TRUE</code> remove tokens that consist only of numbers,
but not words that start with digits, e.g. 2day. See <a href="quanteda.html#topic+tokens">tokens</a>.</p>
</td></tr>
<tr><td><code id="predict_lda_+3A_remove_punct">remove_punct</code></td>
<td>
<p>Logical. If <code>TRUE</code> remove all characters in the Unicode
&quot;Punctuation&quot; [P] class, with exceptions for those used as prefixes for valid social media tags if
<code>preserve_tags = TRUE</code>. See <a href="quanteda.html#topic+tokens">tokens</a></p>
</td></tr>
<tr><td><code id="predict_lda_+3A_remove_symbols">remove_symbols</code></td>
<td>
<p>Logical. If <code>TRUE</code> remove all characters in the Unicode &quot;Symbol&quot; [S] class.</p>
</td></tr>
<tr><td><code id="predict_lda_+3A_remove_url">remove_url</code></td>
<td>
<p>Logical. If <code>TRUE</code> find and eliminate URLs beginning with http(s).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Data frame of topic predictions or predicted probabilities per topic (see response).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

library(Twitmo)

# load tweets (included in package)
mytweets &lt;- load_tweets(system.file("extdata", "tweets_20191027-141233.json", package = "Twitmo"))

# Pool tweets into longer pseudo-documents
pool &lt;- pool_tweets(data = mytweets)
pooled_dfm &lt;- pool$document_term_matrix

# fit your LDA model with 7 topics
model &lt;- fit_lda(pooled_dfm, n_topics = 7, method = "Gibbs")

# Predict topics of tweets using your fitted LDA model
predict_lda(mytweets, model, response = "prob")

## End(Not run)
</code></pre>

<hr>
<h2 id='to_ldavis'>Create interactive visualization with LDAvis</h2><span id='topic+to_ldavis'></span>

<h3>Description</h3>

<p>Converts <a href="topicmodels.html#topic+TopicModel-class">LDA</a> topic model to LDAvis compatible json string and starts server.
May require <code>servr</code> Package to run properly. For conversion of <a href="stm.html#topic+stm">STM</a> topic models use <a href="stm.html#topic+toLDAvis">toLDAvis</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>to_ldavis(fitted, corpus, doc_term)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="to_ldavis_+3A_fitted">fitted</code></td>
<td>
<p>Fitted LDA Model. Object of class <a href="topicmodels.html#topic+TopicModel-class">LDA</a>)</p>
</td></tr>
<tr><td><code id="to_ldavis_+3A_corpus">corpus</code></td>
<td>
<p>Document corpus. Object of class <a href="quanteda.html#topic+corpus">corpus</a>)</p>
</td></tr>
<tr><td><code id="to_ldavis_+3A_doc_term">doc_term</code></td>
<td>
<p>document term matrix (dtm).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Beware that <code>to_ldavis</code> might fail if the corpus contains documents that consist ONLY of numbers,
emojis or punctuation e.g. do not contain a single character string. This is due to a limitation in the <code>topicmodels</code> package
used for model fitting that does not consider such terms as words and omits them causing the posterior to differ in length from the corpus.
If you encounter such an error, redo your pre-processing and exclude emojis, punctuation and numbers.
When using <code><a href="#topic+pool_tweets">pool_tweets</a></code> you can remove emojis by specifying <code>remove_emojis = TRUE</code>.
</p>


<h3>Value</h3>

<p>Invisible Object (see <a href="LDAvis.html#topic+serVis">serVis</a>)).
</p>


<h3>See Also</h3>

<p><a href="stm.html#topic+toLDAvis">toLDAvis</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

library(Twitmo)

# load tweets (included in package)
mytweets &lt;- load_tweets(system.file("extdata", "tweets_20191027-141233.json", package = "Twitmo"))

# Pool tweets into longer pseudo-documents
pool &lt;- pool_tweets(data = mytweets)
pooled_dfm &lt;- pool$document_term_matrix
pooled_corp &lt;- pool$corpus

# fit your LDA model with 7 topics
model &lt;- fit_lda(pooled_dfm, n_topics = 7, method = "Gibbs")

# Explore your topics with LDAvis
to_ldavis(model, pooled_corp, pooled_dfm)

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
