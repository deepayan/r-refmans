<!DOCTYPE html><html><head><title>Help for package monomvn</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {monomvn}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#monomvn-package'><p>Estimation for Multivariate Normal and Student-t Data with Monotone Missingness</p></a></li>
<li><a href='#blasso'><p> Bayesian Lasso/NG, Horseshoe, and Ridge Regression</p></a></li>
<li><a href='#blasso.s3'><p> Summarizing Bayesian Lasso Output</p></a></li>
<li><a href='#bmonomvn'><p>Bayesian Estimation for Multivariate Normal Data with</p>
Monotone Missingness</a></li>
<li><a href='#cement'><p>Hald's Cement Data</p></a></li>
<li><a href='#default.QP'><p> Generating a default Quadratic Program for bmonomvn</p></a></li>
<li><a href='#metrics'><p> RMSE, Expected Log Likelihood and KL Divergence Between</p>
Two Multivariate Normal Distributions</a></li>
<li><a href='#monomvn'><p> Maximum Likelihood Estimation for Multivariate Normal</p>
Data with Monotone Missingness</a></li>
<li><a href='#monomvn-internal'><p>Internal Monomvn Functions</p></a></li>
<li><a href='#monomvn.s3'><p> Summarizing monomvn output</p></a></li>
<li><a href='#monomvn.solve.QP'><p> Solve a Quadratic Program</p></a></li>
<li><a href='#plot.monomvn'><p> Plotting bmonomvn output</p></a></li>
<li><a href='#randmvn'><p> Randomly Generate a Multivariate Normal Distribution</p></a></li>
<li><a href='#regress'><p> Switch function for least squares and parsimonious monomvn regressions</p></a></li>
<li><a href='#returns'><p>Financial Returns data from NYSE and AMEX</p></a></li>
<li><a href='#rmono'><p> Randomly Impose a Monotone Missingness Pattern</p></a></li>
<li><a href='#rwish'><p>Draw from the Wishart Distribution</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Estimation for MVN and Student-t Data with Monotone Missingness</td>
</tr>
<tr>
<td>Version:</td>
<td>1.9-20</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-01-11</td>
</tr>
<tr>
<td>Author:</td>
<td>Robert B. Gramacy &lt;rbg@vt.edu&gt;, with Fortran contributions from Cleve Moler (dpotri/LINPACK) as updated by Berwin A. Turlach (qpgen2/quadprog)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Robert B. Gramacy &lt;rbg@vt.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Estimation of multivariate normal (MVN) and student-t data of 
 arbitrary dimension where the pattern of missing data is monotone.
 See Pantaleo and Gramacy (2010) &lt;<a href="https://doi.org/10.48550/arXiv.0907.2135">doi:10.48550/arXiv.0907.2135</a>&gt;.
 Through the use of parsimonious/shrinkage regressions 
 (plsr, pcr, lasso, ridge,  etc.), where standard regressions fail, 
 the package can handle a nearly arbitrary amount of missing data. 
 The current version supports maximum likelihood inference and 
 a full Bayesian approach employing scale-mixtures for Gibbs sampling.
 Monotone data augmentation extends this Bayesian approach to arbitrary 
 missingness patterns.  A fully functional standalone interface to the 
 Bayesian lasso (from Park &amp; Casella), Normal-Gamma (from Griffin &amp; Brown),
 Horseshoe (from Carvalho, Polson, &amp; Scott), and ridge regression 
 with model selection via Reversible Jump, and student-t errors 
 (from Geweke) is also provided.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.14.0), pls, lars, MASS</td>
</tr>
<tr>
<td>Imports:</td>
<td>quadprog, mvtnorm</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/LGPL-2">LGPL-2</a> | <a href="https://www.r-project.org/Licenses/LGPL-2.1">LGPL-2.1</a> | <a href="https://www.r-project.org/Licenses/LGPL-3">LGPL-3</a> [expanded from: LGPL]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://bobby.gramacy.com/r_packages/monomvn/">https://bobby.gramacy.com/r_packages/monomvn/</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-01-11 13:22:50 UTC; bobby</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-01-11 14:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='monomvn-package'>Estimation for Multivariate Normal and Student-t Data with Monotone Missingness</h2><span id='topic+monomvn-package'></span>

<h3>Description</h3>

<p>Estimation of multivariate normal and student-t data of
arbitrary dimension where the pattern of missing data is monotone.
Through the use of parsimonious/shrinkage regressions
(plsr, pcr, lasso, ridge,  etc.), where standard regressions fail, 
the package can handle a nearly arbitrary amount of missing data.
The current version supports maximum likelihood inference and
a full Bayesian approach employing scale-mixtures for Gibbs sampling.
Monotone data augmentation extends this Bayesian approach to arbitrary
missingness patterns.  A fully functional standalone interface to the 
Bayesian lasso (from Park &amp; Casella), the Normal-Gamma (from Griffin
&amp; Brown), Horseshoe (from Carvalho, Polson, &amp; Scott), and ridge regression with model 
selection via Reversible Jump, and student-t errors (from Geweke) is 
also provided</p>


<h3>Details</h3>

<p>For a fuller overview including a complete list of functions, demos and
vignettes, please use <code>help(package="monomvn")</code>.
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>
<p>Maintainer: 
Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>


<h3>References</h3>

<p>Robert B. Gramacy, Joo Hee Lee and Ricardo Silva (2008).
<em>On estimating covariances between many assets with histories 
of highly variable length</em>. <br /> Preprint available on arXiv:0710.5837:
<a href="https://arxiv.org/abs/0710.5837">https://arxiv.org/abs/0710.5837</a>
</p>
<p><a href="https://bobby.gramacy.com/r_packages/monomvn/">https://bobby.gramacy.com/r_packages/monomvn/</a>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+monomvn">monomvn</a></code>, the now defunct <code>norm</code> package, <span class="pkg">mvnmle</span> </p>

<hr>
<h2 id='blasso'> Bayesian Lasso/NG, Horseshoe, and Ridge Regression </h2><span id='topic+blasso'></span><span id='topic+bhs'></span><span id='topic+bridge'></span>

<h3>Description</h3>

<p>Inference for ordinary least squares, lasso/NG, horseshoe and ridge 
regression models by (Gibbs) sampling from the Bayesian posterior 
distribution, augmented with Reversible Jump for model selection
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bhs(X, y, T=1000, thin=NULL, RJ=TRUE, M=NULL, beta=NULL,
         lambda2=1, s2=var(y-mean(y)), mprior=0, ab=NULL,
         theta=0, rao.s2=TRUE, icept=TRUE, normalize=TRUE, verb=1)
bridge(X, y, T = 1000, thin = NULL, RJ = TRUE, M = NULL,
       beta = NULL, lambda2 = 1, s2 = var(y-mean(y)), mprior = 0,
       rd = NULL, ab = NULL, theta=0, rao.s2 = TRUE, icept = TRUE,
       normalize = TRUE, verb = 1)
blasso(X, y, T = 1000, thin = NULL, RJ = TRUE, M = NULL,
       beta = NULL, lambda2 = 1, s2 = var(y-mean(y)),
       case = c("default", "ridge", "hs", "ng"), mprior = 0, rd = NULL,
       ab = NULL, theta = 0, rao.s2 = TRUE, icept = TRUE, 
       normalize = TRUE, verb = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="blasso_+3A_x">X</code></td>
<td>
<p><code>data.frame</code>, <code>matrix</code>, or vector of inputs <code>X</code> </p>
</td></tr>
<tr><td><code id="blasso_+3A_y">y</code></td>
<td>
<p> vector of output responses <code>y</code> of length equal to the
leading dimension (rows) of <code>X</code>, i.e., <code>length(y) == nrow(X)</code></p>
</td></tr>
<tr><td><code id="blasso_+3A_t">T</code></td>
<td>
<p> total number of MCMC samples to be collected </p>
</td></tr>
<tr><td><code id="blasso_+3A_thin">thin</code></td>
<td>
<p> number of MCMC samples to skip before a sample is
collected (via thinning).  If <code>NULL</code> (default), then
<code>thin</code> is determined based on the regression model implied
by <code>RJ</code>, <code>lambda2</code>, and <code>ncol(X)</code>; and also
on the errors model implied by <code>theta</code> and <code>nrow(X)</code> </p>
</td></tr>
<tr><td><code id="blasso_+3A_rj">RJ</code></td>
<td>
<p> if <code>TRUE</code> then model selection on the columns of the
design matrix (and thus the parameter <code>beta</code> in the model) is
performed by Reversible Jump (RJ) MCMC.  The initial model is
specified by the <code>beta</code> input, described below, and the maximal
number of covariates in the model is specified by <code>M</code> </p>
</td></tr>
<tr><td><code id="blasso_+3A_m">M</code></td>
<td>
<p> the maximal number of allowed covariates (columns of
<code>X</code>) in the model.  If input <code>lambda2 &gt; 0</code> then any
<code>M &lt;= ncol(X)</code> is allowed.  Otherwise it must be that
<code>M &lt;= min(ncol(X), length(y)-1)</code>, which is default value
when a <code>NULL</code> argument is given </p>
</td></tr>
<tr><td><code id="blasso_+3A_beta">beta</code></td>
<td>
<p> initial setting of the regression coefficients.  Any
zero-components will imply that the corresponding covariate (column
of <code>X</code>) is not in the initial model.  When input <code>RJ =
      FALSE</code> (no RJ) and <code>lambda2 &gt; 0</code> (use lasso) then no
components are allowed to be exactly zero.  The default setting is
therefore contextual; see below for details </p>
</td></tr>
<tr><td><code id="blasso_+3A_lambda2">lambda2</code></td>
<td>
<p> square of the initial lasso penalty parameter.  If
zero, then least squares regressions are used </p>
</td></tr>
<tr><td><code id="blasso_+3A_s2">s2</code></td>
<td>
<p> initial variance parameter </p>
</td></tr>
<tr><td><code id="blasso_+3A_case">case</code></td>
<td>
<p> specifies if ridge regression, the
Normal-Gamma, or the horseshoe prior should be done instead
of the lasso; only meaningful when <code>lambda2 &gt; 0</code> </p>
</td></tr>
<tr><td><code id="blasso_+3A_mprior">mprior</code></td>
<td>
<p> prior on the number of non-zero regression coefficients
(and therefore covariates) <code>m</code> in the model. The default
(<code>mprior = 0</code>) encodes the uniform prior on <code>0 &lt;= m &lt;= M</code>.
A scalar value <code>0 &lt; mprior &lt; 1</code> implies a Binomial prior
<code>Bin(m|n=M,p=mprior)</code>. A 2-vector <code>mprior=c(g,h)</code>
of positive values <code>g</code> and <code>h</code> represents
gives <code>Bin(m|n=M,p)</code> prior where <code>p~Beta(g,h)</code> </p>
</td></tr>
<tr><td><code id="blasso_+3A_rd">rd</code></td>
<td>
 <p><code>=c(r, delta)</code>, the alpha (shape) parameter and
<code class="reqn">\beta</code> (rate) parameter to the gamma distribution prior
<code>G(r,delta)</code> for the <code class="reqn">\lambda^2</code> parameter under
the lasso model; or, the <code class="reqn">\alpha</code> (shape) parameter and
<code class="reqn">\beta</code> (scale) parameter to the
inverse-gamma distribution <code>IG(r/2, delta/2)</code> prior for
the <code class="reqn">\lambda^2</code>
parameter under the ridge regression model. A default of <code>NULL</code>
generates appropriate non-informative values depending on the
nature of the regression.  Specifying <code>rd=FALSE</code> causes
<code>lambda2</code> values to be fixed at their starting value, i.e., not
sampled.  See the details below for information
on the special settings for ridge regression </p>
</td></tr>
<tr><td><code id="blasso_+3A_ab">ab</code></td>
<td>
 <p><code>=c(a, b)</code>, the <code class="reqn">\alpha</code> (shape)
parameter and the <code class="reqn">\beta</code> (scale) parameter for the
inverse-gamma distribution prior <code>IG(a,b)</code> for the variance
parameter <code>s2</code>.  A default of <code>NULL</code> generates appropriate
non-informative values depending on the nature of the regression </p>
</td></tr>
<tr><td><code id="blasso_+3A_theta">theta</code></td>
<td>
<p> the rate parameter (<code>&gt; 0</code>) to the exponential prior
on the degrees of freedom paramter <code>nu</code> under a model with
Student-t errors implemented by a scale-mixture prior.
The default setting of <code>theta = 0</code> turns off this prior,
defaulting to a normal errors prior </p>
</td></tr>
<tr><td><code id="blasso_+3A_rao.s2">rao.s2</code></td>
<td>
<p>indicates whether Rao-Blackwellized samples for
<code class="reqn">\sigma^2</code> should be used (default <code>TRUE</code>); see
below for more details </p>
</td></tr>
<tr><td><code id="blasso_+3A_icept">icept</code></td>
<td>
<p> if <code>TRUE</code>, an implicit intercept term is fit
in the model, otherwise the the intercept is zero; default is
<code>TRUE</code> </p>
</td></tr>
<tr><td><code id="blasso_+3A_normalize">normalize</code></td>
<td>
<p> if <code>TRUE</code>, each variable is standardized
to have unit L2-norm, otherwise it is left alone; default is
<code>TRUE</code> </p>
</td></tr>
<tr><td><code id="blasso_+3A_verb">verb</code></td>
<td>
<p> verbosity level; currently only <code>verb = 0</code> and
<code>verb = 1</code> are supported </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Bayesian lasso model and Gibbs Sampling algorithm is described
in detail in Park &amp; Casella (2008).  The algorithm implemented
by this function is identical to that described therein, with
the exception of an added &ldquo;option&rdquo; to use a Rao-Blackwellized
sample of <code class="reqn">\sigma^2</code> (with <code class="reqn">\beta</code> integrated out)
for improved mixing, and the model selections by RJ described below.
When input argument <code>lambda2 = 0</code> is
supplied, the model is a simple hierarchical linear model where
<code class="reqn">(\beta,\sigma^2)</code> is given a Jeffrey's prior
</p>
<p>Specifying <code>RJ = TRUE</code> causes Bayesian model selection and
averaging to commence for choosing which of the columns of the
design matrix <code>X</code> (and thus parameters <code>beta</code>) should be
included in the model.  The zero-components of the <code>beta</code> input
specify which columns are in the initial model, and
<code>M</code> specifies the maximal number of columns.
</p>
<p>The RJ mechanism implemented here for the Bayesian lasso model
selection differs from the one described by Hans (2009),
which is based on an idea from Geweke (1996).
Those methods require departing from the Park &amp; Casella
(2008) latent-variable model and requires sampling from each conditional
<code class="reqn">\beta_i | \beta_{(-i)}, \dots</code> for all
<code class="reqn">i</code>, since a mixture prior with a point-mass at zero is
placed on each <code class="reqn">\beta_i</code>.  Out implementation
here requires no such special prior and retains the joint sampling
from the full <code class="reqn">\beta</code> vector of non-zero entries, which
we believe yields better mixing in the Markov chain.  RJ
proposals to increase/decrease the number of non-zero entries
does proceed component-wise, but the acceptance rates are high due
due to marginalized between-model moves (Troughton &amp; Godsill, 1997).
</p>
<p>When the lasso prior or RJ is used, the automatic thinning level
(unless <code>thin != NULL</code>) is determined by the number of columns
of <code>X</code> since this many latent variables are introduced
</p>
<p>Bayesian ridge regression is implemented as a special case via the
<code>bridge</code> function.  This essentially calls <code>blasso</code>
with <code>case = "ridge"</code>. A default setting of <code>rd = c(0,0)</code> is
implied by <code>rd = NULL</code>, giving the Jeffery's prior for the
penalty parameter <code class="reqn">\lambda^2</code> unless <code>ncol(X) &gt;=
    length(y)</code> in which case the proper specification of <code>rd =
    c(5,10)</code> is used instead.
</p>
<p>The Normal&ndash;Gamma prior (Griffin &amp; Brown, 2009) is implemented as
an extension to the Bayesian lasso with <code>case = "ng"</code>.  Many
thanks to James Scott for providing the code needed to extend the
method(s) to use the horseshoe prior (Carvalho, Polson, Scott, 2010).
</p>
<p>When <code>theta &gt; 0</code> then the Student-t errors via scale mixtures
(and thereby extra latent variables <code>omega2</code>) of Geweke (1993)
is applied as an extension to the Bayesian lasso/ridge model.
If Student-t errors are used the automatic thinning level
is augmented (unless <code>thin != NULL</code>) by the number of rows
in <code>X</code> since this many latent variables are introduced
</p>


<h3>Value</h3>

<p><code>blasso</code> returns an object of class <code>"blasso"</code>, which is a
<code>list</code> containing a copy of all of the input arguments as well as
of the components listed below.
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>a copy of the function call as used</p>
</td></tr>
<tr><td><code>mu</code></td>
<td>
<p> a vector of <code>T</code> samples of the (un-penalized)
&ldquo;intercept&rdquo; parameter </p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p> a <code>T*ncol(X)</code> <code>matrix</code> of <code>T</code> samples from
the (penalized) regression coefficients</p>
</td></tr>
<tr><td><code>m</code></td>
<td>
<p> the number of non-zero entries in each vector of <code>T</code>
samples of <code>beta</code></p>
</td></tr>
<tr><td><code>s2</code></td>
<td>
<p> a vector of <code>T</code> samples of the variance parameter</p>
</td></tr>
<tr><td><code>lambda2</code></td>
<td>
<p> a vector of <code>T</code> samples of the penalty
parameter</p>
</td></tr>
<tr><td><code>gamma</code></td>
<td>
<p> a vector of <code>T</code> with the gamma parameter
when <code>case = "ng"</code> </p>
</td></tr>
<tr><td><code>tau2i</code></td>
<td>
<p> a <code>T*ncol(X)</code> <code>matrix</code> of <code>T</code> samples from
the (latent) inverse diagonal of the prior covariance matrix for
<code>beta</code>, obtained for Lasso regressions </p>
</td></tr>
<tr><td><code>omega2</code></td>
<td>
<p> a <code>T*nrow(X)</code> <code>matrix</code> of <code>T</code> samples
from the (latent) diagonal of the covariance matrix of the response
providing a scale-mixture implementation of Student-t errors with
degrees of freedom <code>nu</code> when active (input <code>theta &gt; 0</code>) </p>
</td></tr>
<tr><td><code>nu</code></td>
<td>
<p> a vector of <code>T</code> samples of the degrees of freedom
parameter to the Student-t errors mode when active
(input <code>theta &gt; 0</code>) </p>
</td></tr>
<tr><td><code>pi</code></td>
<td>
<p> a vector of <code>T</code> samples of the Binomial proportion
<code>p</code> that was given a Beta prior, as described above for the
2-vector version of the <code>mprior</code> input</p>
</td></tr>
<tr><td><code>lpost</code></td>
<td>
<p> the log posterior probability of each (saved) sample of the
joint parameters </p>
</td></tr>
<tr><td><code>llik</code></td>
<td>
<p> the log likelihood of each (saved) sample of the
parameters </p>
</td></tr>
<tr><td><code>llik.norm</code></td>
<td>
<p> the log likelihood of each (saved) sample of the
parameters under the Normal errors model when sampling under the
Student-t model; i.e., it is not present
unless <code>theta &gt; 0</code> </p>
</td></tr>
</table>


<h3>Note</h3>

<p>Whenever <code>ncol(X) &gt;= nrow(X)</code> it must be that either <code>RJ = TRUE</code>
with <code>M &lt;= nrow(X)-1</code> (the default) or that the lasso is turned
on with <code>lambda2 &gt; 0</code>.  Otherwise the regression problem is ill-posed.
</p>
<p>Since the starting values are considered to be first sample (of
<code>T</code>), the total number of (new) samples obtained by Gibbs
Sampling will be <code>T-1</code>
</p>


<h3>Author(s)</h3>

<p> Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a> </p>


<h3>References</h3>

<p>Park, T., Casella, G. (2008). <em>The Bayesian Lasso.</em><br />
Journal of the American Statistical Association, 103(482),
June 2008, pp. 681-686<br />
<a href="https://doi.org/10.1198/016214508000000337">doi:10.1198/016214508000000337</a>
</p>
<p>Griffin, J.E. and Brown, P.J. (2009).
<em>Inference with Normal-Gamma prior distributions in
regression problems.</em> Bayesian Analysis, 5, pp. 171-188.<br />
<a href="https://doi.org/10.1214/10-BA507">doi:10.1214/10-BA507</a>
</p>
<p>Hans, C. (2009). <em>Bayesian Lasso regression.</em>
Biometrika 96, pp. 835-845.<br />
<a href="https://doi.org/10.1093/biomet/asp047">doi:10.1093/biomet/asp047</a>
</p>
<p>Carvalho, C.M., Polson, N.G., and Scott, J.G. (2010) <em>The
horseshoe estimator for sparse signals.</em> Biometrika 97(2):
pp. 465-480.<br />
<a href="https://faculty.chicagobooth.edu/nicholas.polson/research/papers/Horse.pdf">https://faculty.chicagobooth.edu/nicholas.polson/research/papers/Horse.pdf</a>
</p>
<p>Geweke, J. (1996). <em>Variable selection and model comparison
in regression.</em> In Bayesian Statistics 5.  Editors: J.M. Bernardo,
J.O. Berger, A.P. Dawid and A.F.M. Smith, 609-620. Oxford Press.
</p>
<p>Paul T. Troughton and Simon J. Godsill (1997).
<em>A reversible jump sampler for autoregressive time series,
employing full conditionals to achieve efficient model space moves.</em>
Technical Report CUED/F-INFENG/TR.304, Cambridge University
Engineering Department.
</p>
<p>Geweke, J. (1993) <em>Bayesian treatment of the independent
Student-t linear model.</em> Journal of Applied Econometrics, Vol. 8,
S19-S40
</p>
<p><a href="https://bobby.gramacy.com/r_packages/monomvn/">https://bobby.gramacy.com/r_packages/monomvn/</a>
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+lm">lm</a></code> ,
<code><a href="lars.html#topic+lars">lars</a></code> in the <span class="pkg">lars</span> package,
<code><a href="#topic+regress">regress</a></code>,
<code><a href="MASS.html#topic+lm.ridge">lm.ridge</a></code> in the <span class="pkg">MASS</span> package
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## following the lars diabetes example
data(diabetes)
attach(diabetes)

## Ordinary Least Squares regression
reg.ols &lt;- regress(x, y)

## Lasso regression
reg.las &lt;- regress(x, y, method="lasso")

## Bayesian Lasso regression
reg.blas &lt;- blasso(x, y)

## summarize the beta (regression coefficients) estimates
plot(reg.blas, burnin=200)
points(drop(reg.las$b), col=2, pch=20)
points(drop(reg.ols$b), col=3, pch=18)
legend("topleft", c("blasso-map", "lasso", "lsr"),
       col=c(2,2,3), pch=c(21,20,18))

## plot the size of different models visited
plot(reg.blas, burnin=200, which="m")

## get the summary
s &lt;- summary(reg.blas, burnin=200)

## calculate the probability that each beta coef != zero
s$bn0

## summarize s2
plot(reg.blas, burnin=200, which="s2")
s$s2

## summarize lambda2
plot(reg.blas, burnin=200, which="lambda2")
s$lambda2


## Not run: 
## fit with Student-t errors
## (~400-times slower due to automatic thinning level)
regt.blas &lt;- blasso(x, y, theta=0.1)

## plotting some information about nu, and quantiles
plot(regt.blas, "nu", burnin=200)
quantile(regt.blas$nu[-(1:200)], c(0.05, 0.95))

## Bayes Factor shows strong evidence for Student-t model
mean(exp(regt.blas$llik[-(1:200)] - regt.blas$llik.norm[-(1:200)]))

## End(Not run)

## clean up
detach(diabetes)
</code></pre>

<hr>
<h2 id='blasso.s3'> Summarizing Bayesian Lasso Output </h2><span id='topic+print.blasso'></span><span id='topic+plot.blasso'></span><span id='topic+summary.blasso'></span><span id='topic+print.summary.blasso'></span>

<h3>Description</h3>

<p>Summarizing, printing, and plotting the contents of a
<code>"blasso"</code>-class object containing samples from
the posterior distribution of a Bayesian lasso model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'blasso'
print(x, ...)
## S3 method for class 'blasso'
summary(object, burnin = 0, ...)
## S3 method for class 'blasso'
plot(x, which=c("coef", "s2", "lambda2", "gamma",
    "tau2i","omega2", "nu", "m", "pi"), subset = NULL, burnin = 0,
    ... )
## S3 method for class 'summary.blasso'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="blasso.s3_+3A_object">object</code></td>
<td>
<p> a <code>"blasso"</code>-class object that must be named
<code>object</code> for the generic methods <code><a href="#topic+summary.blasso">summary.blasso</a></code> </p>
</td></tr>
<tr><td><code id="blasso.s3_+3A_x">x</code></td>
<td>
<p> a <code>"blasso"</code>-class object that must be named <code>x</code>
for the generic printing and plotting methods 
<code><a href="#topic+print.summary.blasso">print.summary.blasso</a></code> and
<code><a href="#topic+plot.blasso">plot.blasso</a></code> </p>
</td></tr>
<tr><td><code id="blasso.s3_+3A_subset">subset</code></td>
<td>
<p> a vector of indicies that can be used to specify
the a subset of the columns of <code>tau2i</code> or <code>omega2</code> that
are plotted as boxplots in order to reduce clutter </p>
</td></tr>
<tr><td><code id="blasso.s3_+3A_burnin">burnin</code></td>
<td>
<p> number of burn-in rounds to discard before
reporting summaries and making plots.  Must be non-negative
and less than <code>x$T</code></p>
</td></tr>
<tr><td><code id="blasso.s3_+3A_which">which</code></td>
<td>
<p> indicates the parameter whose characteristics
should be plotted; does not apply to the <code><a href="base.html#topic+summary">summary</a></code> </p>
</td></tr>
<tr><td><code id="blasso.s3_+3A_...">...</code></td>
<td>
<p> passed to <code><a href="#topic+print.blasso">print.blasso</a></code>, or
<code><a href="graphics.html#topic+plot.default">plot.default</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p><code><a href="#topic+print.blasso">print.blasso</a></code> prints the <code>call</code> followed by a
brief summary of the MCMC run and a suggestion to try
the summary and plot commands.  
</p>
<p><code><a href="#topic+plot.blasso">plot.blasso</a></code> uses an appropriate
<code><a href="base.html#topic+plot">plot</a></code> command on the <code>list</code> entries of the
<code>"blasso"</code>-class object thus
visually summarizing the samples from the posterior distribution of
each parameter in the model depending on the <code>which</code>
argument supplied.
</p>
<p><code><a href="#topic+summary.blasso">summary.blasso</a></code> uses the <code><a href="base.html#topic+summary">summary</a></code> command
on the list entries of the <code>"blasso"</code>-class object thus
summarizing the samples from the posterior distribution of each
parameter in the model.
</p>
<p><code><a href="#topic+print.summary.monomvn">print.summary.monomvn</a></code> calls <code><a href="#topic+print.blasso">print.blasso</a></code>
on the <code>object</code> and then prints the result of
<code><a href="#topic+summary.blasso">summary.blasso</a></code>
</p>


<h3>Value</h3>

<p><code><a href="#topic+summary.blasso">summary.blasso</a></code> returns a <code>"summary.blasso"</code>-class
object, which is a <code>list</code> containing (a subset of) the items below.
The other functions do not return values.
</p>
<table>
<tr><td><code>B</code></td>
<td>
<p> a copy of the input argument <code>thin</code> </p>
</td></tr>
<tr><td><code>T</code></td>
<td>
<p> total number of MCMC samples to be collected from <code>x$T</code>
</p>
</td></tr>
<tr><td><code>thin</code></td>
<td>
<p> number of MCMC samples to skip before a sample is
collected (via thinning) from <code>x$T</code> </p>
</td></tr>
<tr><td><code>coef</code></td>
<td>
<p> a joint <code>summary</code> of <code>x$mu</code> and
the columns of <code>x$beta</code>, the regression coefficients </p>
</td></tr>
<tr><td><code>s2</code></td>
<td>
<p> a <code>summary</code> of <code>x$s2</code>, the variance parameter </p>
</td></tr>
<tr><td><code>lambda2</code></td>
<td>
<p> a <code>summary</code> of <code>x$lambda2</code>, the penalty
parameter, when lasso or ridge regression is active </p>
</td></tr>
<tr><td><code>lambda2</code></td>
<td>
<p> a <code>summary</code> of <code>x$gamma</code>,
when the NG extensions to the lasso are used </p>
</td></tr>
<tr><td><code>tau2i</code></td>
<td>
<p> a <code>summary</code> of the columns of the latent
<code>x$tau2i</code> parameters when lasso is active </p>
</td></tr>
<tr><td><code>omega2</code></td>
<td>
<p> a <code>summary</code> of the columns of the latent
<code>x$omega2</code> parameters when Student-t errors are active </p>
</td></tr>
<tr><td><code>nu</code></td>
<td>
<p> a <code>summary</code> of <code>x$nu</code>, the degrees of freedom
parameter, when the Student-t model is active </p>
</td></tr>
<tr><td><code>bn0</code></td>
<td>
<p> the estimated posterior probability that the individual
components of the regression coefficients <code>beta</code> is 
nonzero  </p>
</td></tr>
<tr><td><code>m</code></td>
<td>
<p> a <code>summary</code> the model order <code>x$m</code>: the
number of non-zero regression coefficients <code>beta</code> </p>
</td></tr>
<tr><td><code>pi</code></td>
<td>
<p> the estimated Binomial proportion in the prior for
the model order when 2-vector input is provided for
<code>mprior</code>
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a> </p>


<h3>References</h3>

<p><a href="https://bobby.gramacy.com/r_packages/monomvn/">https://bobby.gramacy.com/r_packages/monomvn/</a>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+blasso">blasso</a></code> </p>

<hr>
<h2 id='bmonomvn'>Bayesian Estimation for Multivariate Normal Data with
Monotone Missingness</h2><span id='topic+bmonomvn'></span>

<h3>Description</h3>

<p>Bayesian estimation via sampling from the posterior distribution of the
of the mean and covariance matrix of multivariate normal (MVN)
distributed data with a monotone missingness pattern, via Gibbs Sampling.
Through the use of parsimonious/shrinkage regressions (lasso/NG &amp;
ridge),
where standard regressions fail, this function can handle an (almost)
arbitrary amount of missing data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bmonomvn(y, pre = TRUE, p = 0.9, B = 100, T = 200, thin = 1,
         economy = FALSE, method = c("lasso", "ridge", "lsr", "factor",
         "hs", "ng"), RJ = c("p", "bpsn", "none"), capm = TRUE,
         start = NULL, mprior = 0, rd = NULL, theta = 0, rao.s2 = TRUE,
         QP = NULL, verb = 1, trace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bmonomvn_+3A_y">y</code></td>
<td>
<p> data <code>matrix</code>  were each row is interpreted as a
random sample from a MVN distribution with missing
values indicated by <code>NA</code></p>
</td></tr>
<tr><td><code id="bmonomvn_+3A_pre">pre</code></td>
<td>
<p> logical indicating whether pre-processing of the
<code>y</code> is to be performed.  This sorts the columns so that the
number of <code>NA</code>s is non-decreasing with the column index </p>
</td></tr>
<tr><td><code id="bmonomvn_+3A_p">p</code></td>
<td>
<p> when performing regressions, <code>p</code> is the proportion of the
number of columns to rows in the design matrix before an
alternative regression (lasso, ridge, or RJ) is performed as if
least-squares regression has &ldquo;failed&rdquo;.
Least-squares regression is
known to fail when the number of columns equals the number of rows,
hence a default of <code>p = 0.9 &lt;= 1</code>. Alternatively, setting
<code>p = 0</code> forces a parsimonious method to be used for
<em>every</em> regression. Intermediate settings of <code>p</code> allow
the user to control when least-squares regressions stop and the
parsimonious ones start;  When <code>method = "factor"</code> the <code>p</code>
argument represents an integer (positive) number of initial columns
of <code>y</code> to treat as known factors</p>
</td></tr>
<tr><td><code id="bmonomvn_+3A_b">B</code></td>
<td>
<p> number of Burn-In MCMC sampling rounds,
during which samples are discarded </p>
</td></tr>
<tr><td><code id="bmonomvn_+3A_t">T</code></td>
<td>
<p> total number of MCMC sampling rounds to take
place after burn-in, during which samples are saved </p>
</td></tr>
<tr><td><code id="bmonomvn_+3A_thin">thin</code></td>
<td>
<p> multiplicative thinning in the MCMC.  Each Bayesian
(lasso) regression will discard <code>thin*M</code> MCMC rounds,
where <code>M</code> is the number of columns in its design matrix,
before a sample is saved as a draw from the posterior distribution;
Likewise if <code>theta != 0</code> a further <code>thin*N</code>, for
<code>N</code> responses will be discarded </p>
</td></tr>
<tr><td><code id="bmonomvn_+3A_economy">economy</code></td>
<td>
<p> indicates whether memory should be economized at
the expense of speed.  When <code>TRUE</code> the individual Bayesian
(lasso) regressions are cleaned between uses so that only one
of them has a large footprint at any time during sampling from
the Markov chain.  When <code>FALSE</code> (default) all regressions
are pre-allocated and the full memory footprint is realized at
the outset, saving dynamic allocations</p>
</td></tr>
<tr><td><code id="bmonomvn_+3A_method">method</code></td>
<td>
<p> indicates the Bayesian parsimonious regression
specification to be used, choosing between the lasso (default)
of Park &amp; Casella, the NG extension, the horseshoe,
a ridge regression special case, and least-squares.
The <code>"factor"</code> method treats the first
<code>p</code> columns of <code>y</code> as known factors</p>
</td></tr>
<tr><td><code id="bmonomvn_+3A_rj">RJ</code></td>
<td>
<p> indicates the Reversible Jump strategy to be employed.
The default argument of <code>"p"</code> method uses RJ whenever a
parsimonious regression is used;  <code>"bpsn"</code> only uses
RJ for regressions with <code>p &gt;= n</code>, and <code>"none"</code> never
uses RJ</p>
</td></tr>
<tr><td><code id="bmonomvn_+3A_capm">capm</code></td>
<td>
<p> when <code>TRUE</code> this argument indicates that the
number of components of <code class="reqn">\beta</code> should not exceed
<code class="reqn">n</code>, the number of response variables in a particular
regression</p>
</td></tr>
<tr><td><code id="bmonomvn_+3A_start">start</code></td>
<td>
<p> a list depicting starting values for the parameters
that are use to initialize the Markov chain.  Usually this will be
a <code>"monomvn"</code>-class object depicting maximum likelihood
estimates output from the <code><a href="#topic+monomvn">monomvn</a></code> function.
The relevant fields are the mean vector <code>$mu</code>, covariance
matrix <code>$S</code>, monotone ordering <code>$o</code> (for sanity checking
with input <code>y</code>), component vector <code>$ncomp</code> and
penalty parameter vector <code>$lambda</code>; see note below </p>
</td></tr>
<tr><td><code id="bmonomvn_+3A_mprior">mprior</code></td>
<td>
<p> prior on the number of non-zero regression coefficients
(and therefore covariates) <code>m</code> in the model. The default
(<code>mprior = 0</code>) encodes the uniform prior on <code>0 &lt; m &lt; M</code>.
A scalar value <code>0 &lt;= mprior &lt;= 1</code> implies a Binomial prior
<code>Bin(m|n=M,p=mprior)</code>.  A 2-vector <code>mprior=c(g,h)</code>
of positive values <code>g</code> and <code>h</code> represents
gives <code>Bin(m|n=M,p)</code> prior where <code>p~Beta(g,h)</code></p>
</td></tr>
<tr><td><code id="bmonomvn_+3A_rd">rd</code></td>
<td>
 <p><code>=c(r,delta)</code>; a 2-vector of prior parameters for
<code class="reqn">\lambda^2</code>
which depends on the regression method.  When <code>method =
      "lasso"</code> then the components are the <code class="reqn">\alpha</code>
(shape) and <code class="reqn">\beta</code> (rate) parameters to the a
gamma distribution <code>G(r,delta)</code>;
when <code>method = "ridge"</code> the components are the
<code class="reqn">\alpha</code> (shape) and <code class="reqn">\beta</code> (scale) parameters
to an inverse-gamma distribution <code>IG(r/2,delta/2)</code> </p>
</td></tr>
<tr><td><code id="bmonomvn_+3A_theta">theta</code></td>
<td>
<p> the rate parameter (<code>&gt; 0</code>) to the exponential prior
on the degrees of freedom paramter <code>nu</code> for each regression
model implementing Student-t errors (for each column of
<code>Y</code> marginally) by a scale-mixture prior.  See
<code><a href="#topic+blasso">blasso</a></code> for more details.
The default setting of <code>theta = 0</code> turns off this prior,
defaulting to a normal errors prior.  A negative setting
triggers a pooling of the degrees of freedom parameter
across all columns of <code>Y</code>.  I.e., <code>Y</code> is modeled as
multivariate-t.  In this case <code>abs{theta}</code> is used as the
prior parameterization </p>
</td></tr>
<tr><td><code id="bmonomvn_+3A_rao.s2">rao.s2</code></td>
<td>
<p>indicates whether to  Rao-Blackwellized samples for
<code class="reqn">\sigma^2</code> should be used (default <code>TRUE</code>); see
the details section of <code><a href="#topic+blasso">blasso</a></code> for more information </p>
</td></tr>
<tr><td><code id="bmonomvn_+3A_qp">QP</code></td>
<td>
<p> if non-<code>NULL</code> this argument should either be
<code>TRUE</code>, a positive integer, or contain a <code>list</code>
specifying a Quadratic Program to solve as a function of the
samples of <code>mu = dvec</code> and
<code>Sigma = Dmat</code> in the notation of <code><a href="quadprog.html#topic+solve.QP">solve.QP</a></code>;
see <code><a href="#topic+default.QP">default.QP</a></code> for a default specification that
is used when <code>QP = TRUE</code> or a positive integer is is given;
more details are below </p>
</td></tr>
<tr><td><code id="bmonomvn_+3A_verb">verb</code></td>
<td>
<p> verbosity level; currently only <code>verb = 0</code> and
<code>verb = 1</code> are supported </p>
</td></tr>
<tr><td><code id="bmonomvn_+3A_trace">trace</code></td>
<td>
<p> if <code>TRUE</code> then samples from all parameters are
saved to files in the CWD, and then read back into the
<code>"monomvn"</code>-class object upon return </p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>pre = TRUE</code> then <code>bmonomvn</code> first re-arranges the columns
of <code>y</code> into nondecreasing order with respect to the number of
missing (<code>NA</code>)  entries. Then (at least) the first column should
be completely observed.
</p>
<p>Samples from the posterior distribution of the MVN mean vector and
covariance matrix are obtained sampling
from the posterior distribution of Bayesian regression models.
The methodology for converting these to samples from the mean vector
and covariance matrix is outlined in the <code><a href="#topic+monomvn">monomvn</a></code>
documentation, detailing a similarly structured maximum likelihood
approach.  Also see the references below.
</p>
<p>Whenever the regression model is ill&ndash;posed (i.e., when there are
more covariates than  responses, or a
&ldquo;big <code>p</code> small <code>n</code>&rdquo; problem) then
Bayesian lasso or ridge regressions &ndash; possibly augmented with Reversible
Jump (RJ) for model selection &ndash; are used instead.
See the Park &amp; Casella reference below, and the <code><a href="#topic+blasso">blasso</a></code>
documentation.  To guarantee each regression is well posed the
combination setting of <code>method="lsr"</code> and <code>RJ="none"</code>
is not allowed.
As in <code><a href="#topic+monomvn">monomvn</a></code> the <code>p</code> argument can be used to
turn on lasso or ridge regressions (possibly with RJ) at other times.
The exception is the <code>"factor"</code> method which always involves
an OLS regression on (a subset of) the first <code>p</code>
columns of <code>y</code>.
</p>
<p>Samples from a function of samples of <code>mu</code> and <code>Sigma</code>
can be obtained by specifying a Quadratic program via the
argument <code>QP</code>.  The idea is to allow for the calculation of
the distribution of minimum variance and mean&ndash;variance portfolios,
although the interface is quite general.  See <code><a href="#topic+default.QP">default.QP</a></code>
for more details, as <code><a href="#topic+default.QP">default.QP</a>(ncol(y))</code> is used
when the argument <code>QP = TRUE</code> is given.  When a positive integer
is given, then the first <code>QP</code> columns of <code>y</code> are treated
as factors by using
</p>
<p><code><a href="#topic+default.QP">default.QP</a>(ncol(y) - QP)</code>
</p>
<p>instead. The result is that the corresponding components of (samples of)
<code>mu</code> and rows/cols of <code>S</code> are not factored into the
specification of the resulting Quadratic Program
</p>


<h3>Value</h3>

<p><code>bmonomvn</code> returns an object of class <code>"monomvn"</code>,
which is a <code>list</code> containing the inputs above and a
subset of the components below.
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>a copy of the function call as used</p>
</td></tr>
<tr><td><code>mu</code></td>
<td>
<p>estimated mean vector with columns corresponding to the
columns of <code>y</code> </p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>estimated covariance matrix with rows and columns
corresponding to the columns of <code>y</code> </p>
</td></tr>
<tr><td><code>mu.var</code></td>
<td>
<p>estimated variance of the mean vector with columns
corresponding to the columns of <code>y</code> </p>
</td></tr>
<tr><td><code>mu.cov</code></td>
<td>
<p>estimated covariance matrix of the mean vector
with columns corresponding to the columns of <code>y</code> </p>
</td></tr>
<tr><td><code>S.var</code></td>
<td>
<p>estimated variance of the individual components of the
covariance matrix with columns and rows corresponding to the columns
of <code>y</code></p>
</td></tr>
<tr><td><code>mu.map</code></td>
<td>
<p>estimated maximum <em>a' posteriori</em> (MAP) of the
mean vector with columns corresponding to the columns of <code>y</code> </p>
</td></tr>
<tr><td><code>S.map</code></td>
<td>
<p>estimated MAP of the individual
components of the covariance matrix with columns and rows
corresponding to the columns of <code>y</code></p>
</td></tr>
<tr><td><code>S.nz</code></td>
<td>
<p> posterior probability that the individual entries of
the covariance matrix are non&ndash;zero </p>
</td></tr>
<tr><td><code>Si.nz</code></td>
<td>
<p> posterior probability that the individual entries of
the inverse of the covariance matrix are non&ndash;zero </p>
</td></tr>
<tr><td><code>nu</code></td>
<td>
<p> when <code>theta &lt; 0</code> this field provides a trace of
the pooled <code>nu</code> parameter to the multivariate-t distribution</p>
</td></tr>
<tr><td><code>lpost.map</code></td>
<td>
<p> log posterior probability of the MAP estimate</p>
</td></tr>
<tr><td><code>which.map</code></td>
<td>
<p> gives the time index of the sample corresponding to
the MAP estimate</p>
</td></tr>
<tr><td><code>llik</code></td>
<td>
<p> a trace of the log likelihood of the data </p>
</td></tr>
<tr><td><code>llik.norm</code></td>
<td>
<p> a trace of the log likelihood 
under the Normal errors model when sampling under the
Student-t model; i.e., it is not present unless <code>theta &gt; 0</code>.
Used for calculating Bayes Factors</p>
</td></tr>
<tr><td><code>na</code></td>
<td>
<p> when <code>pre = TRUE</code> this is a vector containing number of
<code>NA</code> entries in each column of <code>y</code></p>
</td></tr>
<tr><td><code>o</code></td>
<td>
<p> when <code>pre = TRUE</code> this is a vector containing the
index of each column in the sorting of the columns of <code>y</code>
obtained by <code>o &lt;- <a href="base.html#topic+order">order</a>(na)</code></p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>method of regression used on each column, or
<code>"bcomplete"</code> indicating that no regression was used</p>
</td></tr>
<tr><td><code>thin</code></td>
<td>
<p> the (actual) number of thinning rounds used for the
regression (<code>method</code>) in each column</p>
</td></tr>
<tr><td><code>lambda2</code></td>
<td>
<p> records the mean <code class="reqn">\lambda^2</code> value
found in the trace of the Bayesian Lasso regressions.  Zero-values
result when the column corresponds to a complete
case or an ordinary least squares regression (these would be
<code>NA</code> entries from <code><a href="#topic+monomvn">monomvn</a></code>) </p>
</td></tr>
<tr><td><code>ncomp</code></td>
<td>
<p> records the mean number of components
(columns of the design matrix) used in the regression model for
each column of <code>y</code>. If input <code>RJ = FALSE</code> then this simply
corresponds to the monotone ordering (these would correspond to
the <code>NA</code> entries from <code><a href="#topic+monomvn">monomvn</a></code>).
When <code>RJ = TRUE</code>
the monotone ordering is an upper bound (on each entry)</p>
</td></tr>
<tr><td><code>trace</code></td>
<td>
<p> if input <code>trace = TRUE</code> then this field contains
traces of the samples of <code class="reqn">\mu</code> in the field <code>$mu</code> and
of <code class="reqn">\Sigma</code> in the field <code>$S</code>, and of all regression
parameters for each of the <code>m = length(mu)</code> columns in the field
<code>$reg</code>. This <code>$reg</code> field is a stripped-down
<code>"blasso"</code>-class object so that the methods of that object
may be used for analysis.  If data augmentation is required to
complete the monotone missingness pattern, then samples from these
entries of <code>Y</code> are contained in <code>$DA</code> where the column
names indicate the <code>i</code>-<code>j</code> entry of <code>Y</code> sampled;
see the <code>R</code> output below </p>
</td></tr>
<tr><td><code>R</code></td>
<td>
<p> gives a <code>matrix</code> version of the missingness pattern
used: <code>0</code>-entries mean observed; <code>1</code>-entries indicate
missing values conforming to a monotone pattern; <code>2</code>-entries
indicate missing values that require data augmentation to complete
a monotone missingness pattern </p>
</td></tr>
<tr><td><code>B</code></td>
<td>
<p> from inputs: number of Burn-In MCMC sampling rounds,
during which samples are discarded </p>
</td></tr>
<tr><td><code>T</code></td>
<td>
<p> from inputs: total number of MCMC sampling rounds to take
place after burn-in, during which samples are saved </p>
</td></tr>
<tr><td><code>r</code></td>
<td>
<p> from inputs: alpha (shape) parameter to the gamma
distribution prior for the lasso parameter lambda </p>
</td></tr>
<tr><td><code>delta</code></td>
<td>
<p> from inputs: beta (rate) parameter to the gamma
distribution prior for the lasso parameter lambda </p>
</td></tr>
<tr><td><code>QP</code></td>
<td>
<p> if a valid (non&ndash;<code>FALSE</code> or <code>NULL</code>) <code>QP</code>
argument is given, then this field contains the specification of
a Quadratic Program in the form of a list with entries including
<code>$dvec</code>, <code>$Amat</code>, <code>$b0</code>, and <code>$meq</code>, similar
to the usage in <code><a href="quadprog.html#topic+solve.QP">solve.QP</a></code>, and some
others; see <code><a href="#topic+default.QP">default.QP</a></code> for more details</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p> when input <code>QP = TRUE</code> is given, then this field
contains a <code>T*ncol(y)</code> matrix of samples from the posterior
distribution of the solution to the Quadratic Program, which can
be visualized via <code><a href="#topic+plot.monomvn">plot.monomvn</a></code> using the argument
<code>which = "QP"</code> </p>
</td></tr>
</table>


<h3>Note</h3>

<p>Whenever the <code>bmonomvn</code> algorithm requires a regression
where <code>p &gt;= n</code>, i.e., if any of the columns in the <code>y</code>
matrix have fewer non&ndash;<code>NA</code> elements than the number of
columns with more non&ndash;<code>NA</code> elements, then it is helpful
to employ both lasso/ridge and the RJ method.
</p>
<p>It is important that any starting values provided in the
<code>start</code> be compatible with the regression model
specified by inputs <code>RJ</code> and <code>method</code>.  Any
incompatibilities will result with a warning that
(alternative) default action was taken and may result in
an undesired (possibly inferior) model being fit
</p>


<h3>Author(s)</h3>

<p> Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a> </p>


<h3>References</h3>

<p>R.B. Gramacy and E. Pantaleo (2010).
<em>Shrinkage regression for multivariate inference with missing
data, and an application to portfolio balancing.</em>  Bayesian Analysis.
5(1), 237-262.  <a href="https://doi.org/10.1214/10-BA602">doi:10.1214/10-BA602</a>
Preprint available on arXiv:0710.5837 <a href="https://arxiv.org/abs/0907.2135">https://arxiv.org/abs/0907.2135</a>
</p>
<p>Roderick J.A. Little and Donald B. Rubin (2002).
<em>Statistical Analysis with Missing Data</em>, Second Edition.
Wilely.
</p>
<p><a href="https://bobby.gramacy.com/r_packages/monomvn/">https://bobby.gramacy.com/r_packages/monomvn/</a>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+blasso">blasso</a></code>, <code><a href="#topic+monomvn">monomvn</a></code>,
<code><a href="#topic+default.QP">default.QP</a></code>, <code>em.norm</code> in the now defunct
<code>norm</code> and <code>mvnmle</code> packages, and <code><a href="#topic+returns">returns</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## standard usage, duplicating the results in
## Little and Rubin, section 7.4.3
data(cement.miss)
out &lt;- bmonomvn(cement.miss)
out
out$mu
out$S

##
## A bigger example, comparing the various
## parsimonious methods
##

## generate N=100 samples from a 10-d random MVN
xmuS &lt;- randmvn(100, 20)

## randomly impose monotone missingness
xmiss &lt;- rmono(xmuS$x)

## using least squares only when necessary,
obl &lt;- bmonomvn(xmiss)
obl

## look at the posterior variability
par(mfrow=c(1,2))
plot(obl)
plot(obl, "S")

## compare to maximum likelihood
Ellik.norm(obl$mu, obl$S, xmuS$mu, xmuS$S)
oml &lt;- monomvn(xmiss, method="lasso")
Ellik.norm(oml$mu, oml$S, xmuS$mu, xmuS$S)


##
## a min-variance portfolio allocation example
##

## get the returns data, and use 20 random cols
data(returns)
train &lt;- returns[,sample(1:ncol(returns), 20)]

## missingness pattern requires DA; also gather
## samples from the solution to a QP
obl.da &lt;- bmonomvn(train, p=0, QP=TRUE)

## plot the QP weights distribution
plot(obl.da, "QP", xaxis="index")

## get ML solution: will warn about monotone violations
suppressWarnings(oml.da &lt;- monomvn(train, method="lasso"))

## add mean and MLE comparison, requires the
## quadprog library for the solve.QP function
add.pe.QP(obl.da, oml.da)

## now consider adding in the market as a factor
data(market)
mtrain &lt;- cbind(market, train)

## fit the model using only factor regressions
obl.daf &lt;- bmonomvn(mtrain, method="factor", p=1, QP=1)
plot(obl.daf, "QP", xaxis="index", main="using only factors")
suppressWarnings(oml.daf &lt;- monomvn(mtrain, method="factor"))
add.pe.QP(obl.daf, oml.daf)


##
## a Bayes/MLE comparison using least squares sparingly
##

## fit Bayesian and classical lasso
p &lt;- 0.25
obls &lt;- bmonomvn(xmiss, p=p)
Ellik.norm(obls$mu, obls$S, xmuS$mu, xmuS$S)
omls &lt;- monomvn(xmiss, p=p, method="lasso")
Ellik.norm(omls$mu, omls$S, xmuS$mu, xmuS$S)

## compare to ridge regression
obrs &lt;- bmonomvn(xmiss, p=p, method="ridge")
Ellik.norm(obrs$mu, obrs$S, xmuS$mu, xmuS$S)
omrs &lt;- monomvn(xmiss, p=p, method="ridge")
Ellik.norm(omrs$mu, omrs$S, xmuS$mu, xmuS$S)
</code></pre>

<hr>
<h2 id='cement'>Hald's Cement Data</h2><span id='topic+cement'></span><span id='topic+cement.miss'></span>

<h3>Description</h3>

<p>Heat evolved in setting of cement, as a function of its chemical
composition.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(cement)
data(cement.miss)
</code></pre>


<h3>Format</h3>

<p>A <code>data.frame</code> with 13 observations on the following 5 variables.
</p>

<dl>
<dt>x1 </dt><dd><p>percentage weight in clinkers of 3CaO.Al2O3</p>
</dd>
<dt>x2 </dt><dd><p>percentage weight in clinkers of 3CaO.SiO2</p>
</dd>
<dt>x3 </dt><dd><p>percentage weight in clinkers of 4CaO.Al2O3.Fe2O3</p>
</dd>
<dt>x4 </dt><dd><p>percentage weight in clinkers of 2CaO.SiO2</p>
</dd>
<dt>y </dt><dd><p>heat evolved (calories/gram)</p>
</dd>
</dl>



<h3>Details</h3>

<p><code>cement.miss</code> is taken from an example in Little &amp; Rubin's book
on <em>Statistical Analysis with Missing Data</em> (2002), pp.~154, for
demonstrating estimation of multivariate means and variances when
the missing data pattern is monotone.  These are indicated by
<code>NA</code> in <code>cement.miss</code>.  See the examples section of
<code><a href="#topic+monomvn">monomvn</a></code> for a re-working of the example from the textbook
</p>


<h3>Source</h3>

<p>Woods, H., Steinour, H. H. and Starke, H. R. (1932) 
Effect of composition of Portland cement on heat evolved during hardening.
<em>Industrial Engineering and Chemistry</em>, <b>24</b>, 1207&ndash;1214.
</p>


<h3>References</h3>

<p>Davison, A. C. (2003)  <em>Statistical Models</em>.  Cambridge University Press.
Page 355.
</p>
<p>Draper, N.R. and Smith, H. (1998) <em>Applied Regression
Analysis</em>. Wiley. Page 630.
</p>
<p>Roderick J.A. Little and Donald B. Rubin (2002).
<em>Statistical Analysis with Missing Data</em>, Second Edition.
Wilely.  Page 154.
</p>
<p><a href="https://bobby.gramacy.com/r_packages/monomvn/">https://bobby.gramacy.com/r_packages/monomvn/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+monomvn">monomvn</a></code> &ndash;  Several other <span class="rlang"><b>R</b></span> packages also include
this data set
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(cement)
lm(y~x1+x2+x3+x4,data=cement)
</code></pre>

<hr>
<h2 id='default.QP'> Generating a default Quadratic Program for bmonomvn </h2><span id='topic+default.QP'></span>

<h3>Description</h3>

<p>This function generates a default &ldquo;minimum variance&rdquo;
Quadratic Program in order to obtain samples of the solution
under the posterior for parameters <code class="reqn">\mu</code> and <code class="reqn">\Sigma</code>
obtained via <code>bmonomvn</code>.  The list generated as output
has entries similar to the inputs of <code><a href="quadprog.html#topic+solve.QP">solve.QP</a></code>
from the <span class="pkg">quadprog</span> package
</p>


<h3>Usage</h3>

<pre><code class='language-R'>default.QP(m, dmu = FALSE, mu.constr = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="default.QP_+3A_m">m</code></td>
<td>
<p> the dimension of the solution space; usually
<code>ncol(y)</code> or equivalently <code>length(mu)</code>, <code>ncol(S)</code>
and <code>nrow(S)</code> in the usage of <code><a href="#topic+bmonomvn">bmonomvn</a></code> </p>
</td></tr>
<tr><td><code id="default.QP_+3A_dmu">dmu</code></td>
<td>
<p> a logical indicating whether <code>dvec</code> should
be replaced with samples of <code class="reqn">\mu</code>; see details below </p>
</td></tr>
<tr><td><code id="default.QP_+3A_mu.constr">mu.constr</code></td>
<td>
<p> a vector indicating linear constraints on the
samples of <code class="reqn">\mu</code> to be included in the default constraint
set. See details below; the default of <code>NULL</code> indicates none </p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code><a href="#topic+bmonomvn">bmonomvn</a>(y, QP=TRUE)</code> is called, this
function is used to generate a default Quadratic Program that
samples from the argument <code class="reqn">w</code> such that
</p>
<p style="text-align: center;"><code class="reqn">\min_w w^\top \Sigma w,</code>
</p>

<p>subject to the constraints that all
<code class="reqn">0\leq w_i \leq 1</code>, for
<code class="reqn">i=1,\dots,m</code>,
</p>
<p style="text-align: center;"><code class="reqn">\sum_{i=1}^m w_i = 1,</code>
</p>

<p>and where <code class="reqn">\Sigma</code> is sampled from its posterior distribution
conditional on the data <code class="reqn">y</code>.
Alternatively, this function
can be used as a skeleton to for adaptation to more general 
Quadratic Programs by adjusting the <code>list</code> that is returned,
as described in the &ldquo;value&rdquo; section below.
</p>
<p>Non-default settings of the arguments <code>dmu</code> and <code>mu.constr</code>
augment the default Quadratic Program, described above, in two standard ways.
Specifying <code>dvec = TRUE</code> causes the program objective to change
to </p>
<p style="text-align: center;"><code class="reqn">\min_w - w^\top \mu + \frac{1}{2} w^\top \Sigma w,</code>
</p>

<p>with the same constraints as above.  Setting <code>mu.constr = 1</code>,
say, would augment the constraints to include
</p>
<p style="text-align: center;"><code class="reqn">\mu^\top w \geq 1,</code>
</p>
<p> for samples of <code class="reqn">\mu</code>
from the posterior.  Setting <code>mu.constr = c(1,2)</code> would
augment the constraints still further with
</p>
<p style="text-align: center;"><code class="reqn">-\mu^\top w \geq -2,</code>
</p>
<p> i.e., with
alternating sign on the linear part, so that each sample of
<code class="reqn">\mu^\top w</code> must lie in the interval [1,2].
So whereas <code>dmu = TRUE</code> allows the <code>mu</code> samples to
enter the objective in a standard way, <code>mu.constr</code>
(<code>!= NULL</code>) allows it to enter the constraints.
</p>
<p>The accompanying function <code><a href="#topic+monomvn.solve.QP">monomvn.solve.QP</a></code> can
act as an interface between the constructed (default) <code>QP</code>
object, and estimates of the covariance matrix <code class="reqn">\Sigma</code> and
mean vector <code class="reqn">\mu</code>, that is identical to the one used on
the posterior-sample version implemented in <code><a href="#topic+bmonomvn">bmonomvn</a></code>.
The example below, and those in the documentation for
<code><a href="#topic+bmonomvn">bmonomvn</a></code>, illustrate how this feature may be used
to extract mean and MLE solutions to the constructed
Quadratic Program
</p>


<h3>Value</h3>

<p>This function returns a <code>list</code> that can be interpreted as
specifying the following arguments to the
<code><a href="quadprog.html#topic+solve.QP">solve.QP</a></code> function in the <span class="pkg">quadprog</span>
package.  See <code><a href="quadprog.html#topic+solve.QP">solve.QP</a></code> for more information
of the general specification of these arguments.  In what follows
we simply document the defaults provided by <code>default.QP</code>.
Note that the <code>Dmat</code> argument is not, specified as
<code><a href="#topic+bmonomvn">bmonomvn</a></code> will use samples from <code>S</code> (from the
posterior) instead
</p>
<table>
<tr><td><code>m</code></td>
<td>
 <p><code>length(dvec)</code>, etc.</p>
</td></tr>
<tr><td><code>dvec</code></td>
<td>
<p> a zero-vector <code>rep(0, m)</code>, or a one-vector
<code>rep(1, m)</code> when <code>dmu = TRUE</code> as the real <code>dvec</code>
that will be used by <code><a href="quadprog.html#topic+solve.QP">solve.QP</a></code> will
then be <code>dvec * mu</code> </p>
</td></tr>
<tr><td><code>dmu</code></td>
<td>
<p> a copy of the <code>dmu</code> input argument </p>
</td></tr>
<tr><td><code>Amat</code></td>
<td>
<p> a <code>matrix</code> describing a linear transformation
which, together with <code>b0</code>  and <code>meq</code>, describe the
constraint that the components of the sampled solution(s),
<code>w</code>, must be positive and sum to one </p>
</td></tr>
<tr><td><code>b0</code></td>
<td>
<p> a vector containing the (RHS of) in/equalities described by
the these constraints </p>
</td></tr>
<tr><td><code>meq</code></td>
<td>
<p> an integer scalar indicating that the first <code>meq</code>
constraints described by <code>Amat</code> and <code>b0</code> are equality
constraints; the rest are <code>&gt;=</code> </p>
</td></tr>
<tr><td><code>mu.constr</code></td>
<td>
<p> a vector whose length is one greater than the
input argument of the same name, providing <code>bmonomvn</code> with
the number 
</p>
<p><code>mu.constr[1] = length(mu.constr[-1])</code>
</p>
<p>and location <code>mu.constr[-1]</code> of the columns of <code>Amat</code>
which require multiplication by samples of <code>mu</code> </p>
</td></tr>
</table>
<p>The <code>$QP</code> object that is returned from <code><a href="#topic+bmonomvn">bmonomvn</a></code>
will have the following additional field
</p>
<table>
<tr><td><code>o</code></td>
<td>
<p> an integer vector of length <code>m</code> indicating the ordering
of the rows of <code>$Amat</code>, and thus the rows of solutions
<code>$W</code> that was used in the monotone factorization of the
likelihood.  This field appears only after <code><a href="#topic+bmonomvn">bmonomvn</a></code>
returns a <code>QP</code> object checked by the internal function
<code>check.QP</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a> </p>


<h3>See Also</h3>

 <p><code><a href="#topic+bmonomvn">bmonomvn</a></code> and <code><a href="quadprog.html#topic+solve.QP">solve.QP</a></code>
in the <span class="pkg">quadprog</span> package, <code><a href="#topic+monomvn.solve.QP">monomvn.solve.QP</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate N=100 samples from a 10-d random MVN
## and randomly impose monotone missingness
xmuS &lt;- randmvn(100, 20)
xmiss &lt;- rmono(xmuS$x)

## set up the minimum-variance (default) Quadratic Program
## and sample from the posterior of the solution space
qp1 &lt;- default.QP(ncol(xmiss))
obl1 &lt;- bmonomvn(xmiss, QP=qp1)
bm1 &lt;- monomvn.solve.QP(obl1$S, qp1) ## calculate mean
bm1er &lt;- monomvn.solve.QP(obl1$S + obl1$mu.cov, qp1) ## use estimation risk
oml1 &lt;- monomvn(xmiss)
mm1 &lt;- monomvn.solve.QP(oml1$S, qp1) ## calculate MLE

## now obtain samples from the solution space of the
## mean-variance QP
qp2 &lt;- default.QP(ncol(xmiss), dmu=TRUE)
obl2 &lt;- bmonomvn(xmiss, QP=qp2)
bm2 &lt;- monomvn.solve.QP(obl2$S, qp2, obl2$mu) ## calculate mean
bm2er &lt;- monomvn.solve.QP(obl2$S + obl2$mu.cov, qp2, obl2$mu) ## use estimation risk
oml2 &lt;- monomvn(xmiss)
mm2 &lt;- monomvn.solve.QP(oml2$S, qp2, oml2$mu) ## calculate MLE

## now obtain samples from minimum variance solutions
## where the mean weighted (samples) are constrained to be
## greater one
qp3 &lt;- default.QP(ncol(xmiss), mu.constr=1)
obl3 &lt;- bmonomvn(xmiss, QP=qp3)
bm3 &lt;- monomvn.solve.QP(obl3$S, qp3, obl3$mu) ## calculate mean
bm3er &lt;- monomvn.solve.QP(obl3$S + obl3$mu.cov, qp3, obl3$mu) ## use estimation risk
oml3 &lt;- monomvn(xmiss)
mm3 &lt;- monomvn.solve.QP(oml3$S, qp3, oml2$mu) ## calculate MLE

## plot a comparison
par(mfrow=c(3,1))
plot(obl1, which="QP", xaxis="index", main="Minimum Variance")
points(bm1er, col=4, pch=17, cex=1.5) ## add estimation risk
points(bm1, col=3, pch=18, cex=1.5) ## add mean
points(mm1, col=5, pch=16, cex=1.5) ## add MLE
legend("topleft", c("MAP", "posterior mean", "ER", "MLE"), col=2:5,
       pch=c(21,18,17,16), cex=1.5)
plot(obl2, which="QP", xaxis="index", main="Mean Variance")
points(bm2er, col=4, pch=17, cex=1.5) ## add estimation risk
points(bm2, col=3, pch=18, cex=1.5) ## add mean
points(mm2, col=5, pch=16, cex=1.5) ## add MLE
plot(obl3, which="QP", xaxis="index", main="Minimum Variance, mean &gt;= 1")
points(bm3er, col=4, pch=17, cex=1.5) ## add estimation risk
points(bm3, col=3, pch=18, cex=1.5) ## add mean
points(mm3, col=5, pch=16, cex=1.5) ## add MLE

## for a further comparison of samples of the QP solution
## w under Bayesian and non-Bayesian monomvn, see the
## examples in the bmonomvn help file
</code></pre>

<hr>
<h2 id='metrics'> RMSE, Expected Log Likelihood and KL Divergence Between
Two Multivariate Normal Distributions </h2><span id='topic+kl.norm'></span><span id='topic+Ellik.norm'></span><span id='topic+rmse.muS'></span>

<h3>Description</h3>

<p>These functions calculate the root-mean-squared-error,
the expected log likelihood, and Kullback-Leibler (KL) divergence
(a.k.a. distance), between two multivariate normal (MVN)
distributions described by their mean vector and covariance matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmse.muS(mu1, S1, mu2, S2)
Ellik.norm(mu1, S1, mu2, S2, quiet=FALSE)
kl.norm(mu1, S1, mu2, S2, quiet=FALSE, symm=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="metrics_+3A_mu1">mu1</code></td>
<td>
<p> mean vector of first (estimated) MVN </p>
</td></tr>
<tr><td><code id="metrics_+3A_s1">S1</code></td>
<td>
<p> covariance matrix of first (estimated) MVN </p>
</td></tr>
<tr><td><code id="metrics_+3A_mu2">mu2</code></td>
<td>
<p> mean vector of second (true, baseline, or comparator) MVN </p>
</td></tr>
<tr><td><code id="metrics_+3A_s2">S2</code></td>
<td>
<p> covariance matrix of second (true, baseline, or comparator) MVN </p>
</td></tr>
<tr><td><code id="metrics_+3A_quiet">quiet</code></td>
<td>
<p> when <code>FALSE</code> (default) </p>
</td></tr>
<tr><td><code id="metrics_+3A_symm">symm</code></td>
<td>
<p> when <code>TRUE</code> a symmetrized version of the
KL divergence is used; see the note below </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The root-mean-squared-error is calculated between the entries of
the mean vectors, and the upper-triangular part of the covariance
matrices (including the diagonal).
</p>
<p>The KL divergence is given by the formula:
</p>
<p style="text-align: center;"><code class="reqn">D_{\mbox{\tiny KL}}(N_1 \| N_2) = \frac{1}{2}
  \left( \log \left( \frac{|\Sigma_1|}{|\Sigma_2|} \right)
    + \mbox{tr} \left( \Sigma_1^{-1} \Sigma_2 \right) +
    \left( \mu_1 - \mu_2\right)^\top \Sigma_1^{-1}
    ( \mu_1 - \mu_2 ) - N \right)
    </code>
</p>

<p>where <code class="reqn">N</code> is <code>length(mu1)</code>, and must agree with
the dimensions of the other parameters.  Note that the parameterization
used involves swapped arguments compared to some other references,
e.g., as provided by Wikipedia.  See note below.
</p>
<p>The expected log likelihood can be formulated in terms of the
KL divergence.  That is, the expected log likelihood of data
simulated from the normal distribution with parameters <code>mu2</code>
and <code>S2</code> under the estimated normal with parameters
<code>mu1</code> and <code>S1</code> is given by
</p>
<p style="text-align: center;"><code class="reqn"> -\frac{1}{2} \ln \{(2\pi e)^N |\Sigma_2|\} -
    D_{\mbox{\tiny KL}}(N_1 \| N_2).
  </code>
</p>



<h3>Value</h3>

<p>In the case of the expected log likelihood the result is
a real number.  The RMSE is a positive real number.
The KL divergence method returns a positive
real number depicting the <em>distance</em> between the
two normal distributions
</p>


<h3>Note</h3>

<p>The KL-divergence is not symmetric.  Therefore
</p>
<p><code>kl.norm(mu1,S1,mu2,S2) != kl.norm(mu2,S2,mu1,S1).</code>
</p>
<p>But a symmetric metric can be constructed from
</p>
<p><code>0.5 * (kl.norm(mu1,S1,mu2,S2) + kl.norm(mu2,S2,mu1,S1))</code>
</p>
<p>or by using <code>symm = TRUE</code>.  The arguments are reversed
compared to some other references, like Wikipedia.  To match
those versions use <code>kl.norm(mu2, S2, mu1, s1)</code>
</p>


<h3>Author(s)</h3>

<p> Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a> </p>


<h3>References</h3>

<p><a href="https://bobby.gramacy.com/r_packages/monomvn/">https://bobby.gramacy.com/r_packages/monomvn/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>mu1 &lt;- rnorm(5)
s1 &lt;- matrix(rnorm(100), ncol=5)
S1 &lt;- t(s1) %*% s1

mu2 &lt;- rnorm(5)
s2 &lt;- matrix(rnorm(100), ncol=5)
S2 &lt;- t(s2) %*% s2

## RMSE
rmse.muS(mu1, S1, mu2, S2)

## expected log likelihood
Ellik.norm(mu1, S1, mu2, S2)

## KL is not symmetric
kl.norm(mu1, S1, mu2, S2)
kl.norm(mu2, S2, mu1, S1)

## symmetric version
kl.norm(mu2, S2, mu1, S1, symm=TRUE)
</code></pre>

<hr>
<h2 id='monomvn'> Maximum Likelihood Estimation for Multivariate Normal
Data with Monotone Missingness </h2><span id='topic+monomvn'></span>

<h3>Description</h3>

<p>Maximum likelihood estimation of the mean and covariance matrix of
multivariate normal (MVN) distributed data with a monotone missingness pattern.
Through the use of parsimonious/shrinkage regressions (e.g., plsr, pcr,
ridge, lasso, etc.), where standard regressions fail,
this function can handle an (almost) arbitrary amount of missing data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>monomvn(y, pre = TRUE, method = c("plsr", "pcr", "lasso", "lar",
        "forward.stagewise", "stepwise", "ridge", "factor"), p = 0.9,
        ncomp.max = Inf, batch = TRUE, validation = c("CV", "LOO", "Cp"),
        obs = FALSE, verb = 0, quiet = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="monomvn_+3A_y">y</code></td>
<td>
<p> data <code>matrix</code>  were each row is interpreted as a
random sample from a MVN distribution with missing
values indicated by <code>NA</code></p>
</td></tr>
<tr><td><code id="monomvn_+3A_pre">pre</code></td>
<td>
<p> logical indicating whether pre-processing of the
<code>y</code> is to be performed.  This sorts the columns so that the
number of <code>NA</code>s is non-decreasing with the column index </p>
</td></tr>
<tr><td><code id="monomvn_+3A_method">method</code></td>
<td>
<p> describes the type of <em>parsimonious</em>
(or <em>shrinkage</em>) regression to
be performed when standard least squares regression fails.
From the <span class="pkg">pls</span> package we have <code>"plsr"</code>
(<a href="pls.html#topic+plsr">plsr</a>, the default) for  partial least squares and
<code>"pcr"</code> (<a href="pls.html#topic+pcr">pcr</a>) for standard principal
component regression.  From the <span class="pkg">lars</span> package (see the
<code>"type"</code> argument to <a href="lars.html#topic+lars">lars</a>)
we have <code>"lasso"</code> for L1-constrained regression, <code>"lar"</code>
for least angle regression, <code>"forward.stagewise"</code> and
<code>"stepwise"</code> for fast implementations of classical forward
selection of covariates.  From the <span class="pkg">MASS</span> package we have
<code>"ridge"</code> as implemented by the <code><a href="MASS.html#topic+lm.ridge">lm.ridge</a></code>
function.  The <code>"factor"</code> method treats the first <code>p</code>
columns of <code>y</code> as known factors </p>
</td></tr>
<tr><td><code id="monomvn_+3A_p">p</code></td>
<td>
<p> when performing regressions, <code>p</code> is the proportion of the
number of columns to rows in the design matrix before an
alternative regression <code>method</code> (those above) is performed as if
least-squares regression has &ldquo;failed&rdquo;.  Least-squares regression is
known to fail when the number of columns equals the number of rows,
hence a default of <code>p = 0.9 &lt;= 1</code>. Alternatively, setting
<code>p = 0</code> forces <code>method</code> to be used for <em>every</em> regression.
Intermediate settings of <code>p</code> allow the user to control when
least-squares regressions stop and the <code>method</code> ones start.
When <code>method = "factor"</code> the <code>p</code> argument represents an
integer (positive) number of initial columns of <code>y</code> to treat
as known factors </p>
</td></tr>
<tr><td><code id="monomvn_+3A_ncomp.max">ncomp.max</code></td>
<td>
<p> maximal number of (principal) components to include
in a <code>method</code>&mdash;only meaningful for the <code>"plsr"</code> or
<code>"pcr"</code> methods.  Large settings can cause the execution to be
slow as it drastically increases the cross-validation (CV) time</p>
</td></tr>
<tr><td><code id="monomvn_+3A_batch">batch</code></td>
<td>
<p> indicates whether the columns with equal missingness
should be processed together using a multi-response regression.
This is more efficient if many OLS regressions are used, but can
lead to slightly poorer, even unstable, fits when parsimonious
regressions are used</p>
</td></tr>
<tr><td><code id="monomvn_+3A_validation">validation</code></td>
<td>
<p> method for cross validation when applying 
a <em>parsimonious</em> regression method.  The default setting
of <code>"CV"</code> (randomized 10-fold cross-validation) is the faster
method, but does not yield a deterministic result and does not apply
for regressions on less than ten responses.
<code>"LOO"</code> (leave-one-out cross-validation)
is deterministic, always applicable, and applied automatically whenever 
<code>"CV"</code> cannot be used.  When standard least squares is
appropriate, the methods implemented the
<span class="pkg">lars</span> package (e.g. lasso) support model choice via the
<code>"Cp"</code> statistic, which defaults to the <code>"CV"</code> method
when least squares fails.  This argument is ignored for the
<code>"ridge"</code> method; see details below</p>
</td></tr>
<tr><td><code id="monomvn_+3A_obs">obs</code></td>
<td>
<p> logical indicating whether or not to (additionally)
compute a mean vector and covariance matrix based only on the observed
data, without regressions.  I.e., means are calculated as averages
of each non-<code>NA</code> entry in the columns of <code>y</code>, and entries
<code>(a,b)</code> of the
covariance matrix are calculated by applying <code><a href="stats.html#topic+cov">cov</a>(ya,yb)</code>
to the jointly non-<code>NA</code> entries of columns <code>a</code> and <code>b</code>
of <code>y</code></p>
</td></tr>
<tr><td><code id="monomvn_+3A_verb">verb</code></td>
<td>
<p> whether or not to print progress indicators.  The default
(<code>verb = 0</code>) keeps quiet, while any positive number causes brief
statement about dimensions of each regression to print to
the screen as it happens.  <code>verb = 2</code> causes each of the ML
regression estimators to be printed along with the corresponding
new entries of the mean and columns of the covariance matrix.
<code>verb = 3</code> requires that the RETURN key be pressed between
each print statement</p>
</td></tr>
<tr><td><code id="monomvn_+3A_quiet">quiet</code></td>
<td>
<p> causes <code><a href="base.html#topic+warning">warning</a></code>s about regressions to be silenced
when <code>TRUE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>pre = TRUE</code> then <code>monomvn</code> first re-arranges the columns
of <code>y</code> into nondecreasing order with respect to the number of
missing (<code>NA</code>)  entries. Then (at least) the first column should
be completely observed.  The mean components and covariances between
the first set of complete columns are obtained through the standard
<code><a href="base.html#topic+mean">mean</a></code> and <code><a href="stats.html#topic+cov">cov</a></code> routines.
</p>
<p>Next each successive group of columns with the same missingness pattern
is processed in sequence (assuming <code>batch = TRUE</code>).
Suppose a total of <code>j</code> columns have
been processed this way already.  Let <code>y2</code> represent the non-missing
contingent of the next group of <code>k</code> columns of <code>y</code>
with and identical missingness pattern, and let <code>y1</code> be the
previously processed <code>j-1</code> columns of <code>y</code>
containing only the rows
corresponding to each non-<code>NA</code> entry in <code>y2</code>.  I.e.,
<code>nrow(y1) = nrow(y2)</code>.  Note that <code>y1</code> contains no
<code>NA</code> entries since the missing data pattern is monotone.
The <code>k</code> next entries (indices <code>j:(j+k)</code>) of the mean vector,
and the <code>j:(j+k)</code> rows and columns of the covariance matrix are
obtained by multivariate regression of <code>y2</code> on <code>y1</code>.
The regression method used (except in the case of <code>method =
    "factor"</code> depends on the number of rows and columns
in <code>y1</code> and on the <code>p</code> parameter.  Whenever <code>ncol(y1)
    &lt; p*nrow(y1)</code> least-squares regression is used, otherwise
<code>method = c("pcr", "plsr")</code>.  If ever a least-squares regression
fails due to co-linearity then one of the other <code>method</code>s is
tried.  The <code>"factor"</code> method always involves an OLS regression
on (a subset of) the first <code>p</code> columns of <code>y</code>.
</p>
<p>All <code>method</code>s require a scheme for estimating the amount of
variability explained by increasing the numbers of coefficients
(or principal components) in the model.
Towards this end, the <span class="pkg">pls</span> and <span class="pkg">lars</span> packages support
10-fold cross validation (CV) or leave-one-out (LOO) CV estimates of
root mean squared error.  See <span class="pkg">pls</span> and <span class="pkg">lars</span> for
more details.  <code>monomvn</code> uses
CV in all cases except when <code>nrow(y1) &lt;= 10</code>, in which case CV fails and
LOO is used.  Whenever <code>nrow(y1) &lt;= 3</code> <code><a href="pls.html#topic+pcr">pcr</a></code>
fails,  so <code><a href="pls.html#topic+plsr">plsr</a></code> is used instead.
If <code>quiet = FALSE</code> then a <code><a href="base.html#topic+warning">warning</a></code>
is given whenever the first choice for a regression fails.
</p>
<p>For <span class="pkg">pls</span> methods, RMSEs are calculated for a number of
components in <code>1:ncomp.max</code> where
a <code>NULL</code> value for <code>ncomp.max</code> it is replaced with
</p>
<p><code>ncomp.max &lt;- min(ncomp.max, ncol(y2), nrow(y1)-1)</code>
</p>
<p>which is the max allowed by the <span class="pkg">pls</span> package.
</p>
<p>Simple heuristics are used to select a small number of components
(<code>ncomp</code> for <span class="pkg">pls</span>), or number of coefficients (for
<span class="pkg">lars</span>), which explains a large amount of the variability (RMSE).
The <span class="pkg">lars</span> methods use a &ldquo;one-standard error rule&rdquo; outlined
in Section 7.10, page 216 of HTF below.  The
<span class="pkg">pls</span> package does not currently support the calculation of
standard errors for CV estimates of RMSE, so a simple linear penalty
for increasing <code>ncomp</code> is used instead.  The ridge constant
(lambda) for <code><a href="MASS.html#topic+lm.ridge">lm.ridge</a></code> is set using the
<code><a href="stats.html#topic+optimize">optimize</a></code> function on the <code>GCV</code> output.
</p>
<p>Based on the ML <code>ncol(y1)+1</code> regression coefficients (including
intercept) obtained for each of the
columns of <code>y2</code>, and on the corresponding <code>matrix</code> of
residual sum of squares, and on the previous <code>j-1</code> means
and rows/cols of the covariance matrix, the <code>j:(j+k)</code> entries and
rows/cols can be filled in as described by Little and Rubin, section 7.4.3.
</p>
<p>Once every column has been processed, the entries of the mean vector, and
rows/cols of the covariance matrix are re-arranged into their original
order.
</p>


<h3>Value</h3>

<p><code>monomvn</code> returns an object of class <code>"monomvn"</code>, which is a
<code>list</code> containing a subset of the components below.
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>a copy of the function call as used</p>
</td></tr>
<tr><td><code>mu</code></td>
<td>
<p>estimated mean vector with columns corresponding to the
columns of <code>y</code> </p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>estimated covariance matrix with rows and columns
corresponding to the columns of <code>y</code> </p>
</td></tr>
<tr><td><code>na</code></td>
<td>
<p> when <code>pre = TRUE</code> this is a vector containing number of
<code>NA</code> entries in each column of <code>y</code></p>
</td></tr>
<tr><td><code>o</code></td>
<td>
<p> when <code>pre = TRUE</code> this is a vector containing the
index of each column in the sorting of the columns of <code>y</code>
obtained by <code>o &lt;- <a href="base.html#topic+order">order</a>(na)</code></p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>method of regression used on each column, or
<code>"complete"</code> indicating that no regression was necessary</p>
</td></tr>
<tr><td><code>ncomp</code></td>
<td>
<p>number of components in a <code><a href="pls.html#topic+plsr">plsr</a></code> or
<code><a href="pls.html#topic+pcr">pcr</a></code> regression, or <code>NA</code> if such a method was
not used.  This field is used to record <code class="reqn">\lambda</code>
when <code><a href="MASS.html#topic+lm.ridge">lm.ridge</a></code> is used </p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>if <code>method</code> is one of <code>c("lasso",
      "forward.stagewise", "ridge")</code>, then this field records the
<code class="reqn">\lambda</code> penalty parameters used</p>
</td></tr>
<tr><td><code>mu.obs</code></td>
<td>
<p>when <code>obs = TRUE</code> this is the &ldquo;observed&rdquo;
mean vector</p>
</td></tr>
<tr><td><code>S.obs</code></td>
<td>
<p>when <code>obs = TRUE</code> this is the &ldquo;observed&rdquo;
covariance matrix, as described above.  Note that <code>S.obs</code> is
usually not positive definite </p>
</td></tr>
</table>


<h3>Note</h3>

<p>The CV in <span class="pkg">plsr</span> and <span class="pkg">lars</span> are random in nature, and so
can be dependent on the random seed.  Use <code>validation=LOO</code> for
deterministic (but slower) result.
</p>
<p>When using <code>method = "factor"</code> in the current version of
the package, the factors in the first <code>p</code>
columns of <code>y</code> must also obey the monotone pattern, and,
have no more <code>NA</code> entries than the other columns of <code>y</code>.
</p>
<p>Be warned that the <span class="pkg">lars</span> implementation of
<code>"forward.stagewise"</code> can sometimes get stuck in
(what seems like) an infinite loop.
This is not a bug in the <code>monomvn</code> package;
the bug has been reported to the authors of <span class="pkg">lars</span>
</p>


<h3>Author(s)</h3>

<p> Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a></p>


<h3>References</h3>

<p>Robert B. Gramacy, Joo Hee Lee, and Ricardo Silva (2007).
<em>On estimating covariances between many assets with histories 
of highly variable length</em>. <br /> Preprint available on arXiv:0710.5837:
<a href="https://arxiv.org/abs/0710.5837">https://arxiv.org/abs/0710.5837</a>
</p>
<p>Roderick J.A. Little and Donald B. Rubin (2002).
<em>Statistical Analysis with Missing Data</em>, Second Edition.
Wilely.
</p>
<p>Bjorn-Helge Mevik and Ron Wehrens (2007).
<em>The <span class="pkg">pls</span> Package: Principal Component and Partial
Least Squares Regression in R.</em> 
Journal of Statistical Software <b>18</b>(2)
</p>
<p>Bradley Efron, Trevor Hastie, Ian Johnstone and Robert Tibshirani
(2003).
<em>Least Angle Regression (with discussion).</em>
Annals of Statistics <b>32</b>(2); see also<br />
<a href="https://hastie.su.domains/Papers/LARS/LeastAngle_2002.pdf">https://hastie.su.domains/Papers/LARS/LeastAngle_2002.pdf</a>
</p>
<p>Trevor Hastie, Robert Tibshirani and Jerome Friedman (2002).
<em>Elements of Statistical Learning.</em> Springer, NY. [HTF]
</p>
<p>Some of the code for <code>monomvn</code>, and its subroutines, was inspired
by code found on the world wide web, written by Daniel Heitjan.
Search for &ldquo;fcn.q&rdquo;
</p>
<p><a href="https://bobby.gramacy.com/r_packages/monomvn/">https://bobby.gramacy.com/r_packages/monomvn/</a>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+bmonomvn">bmonomvn</a></code>, <code>em.norm</code>
in the now defunct <code>norm</code> and <code>mvnmle</code> packages</p>


<h3>Examples</h3>

<pre><code class='language-R'>## standard usage, duplicating the results in
## Little and Rubin, section 7.4.3 -- try adding 
## verb=3 argument for a step-by-step breakdown
data(cement.miss)
out &lt;- monomvn(cement.miss)
out
out$mu
out$S

##
## A bigger example, comparing the various methods
##

## generate N=100 samples from a 10-d random MVN
xmuS &lt;- randmvn(100, 20)

## randomly impose monotone missingness
xmiss &lt;- rmono(xmuS$x)

## plsr
oplsr &lt;- monomvn(xmiss, obs=TRUE)
oplsr
Ellik.norm(oplsr$mu, oplsr$S, xmuS$mu, xmuS$S)

## calculate the complete and observed RMSEs
n &lt;- nrow(xmiss) - max(oplsr$na)
x.c &lt;- xmiss[1:n,]
mu.c &lt;- apply(x.c, 2, mean)
S.c &lt;- cov(x.c)*(n-1)/n
Ellik.norm(mu.c, S.c, xmuS$mu, xmuS$S)
Ellik.norm(oplsr$mu.obs, oplsr$S.obs, xmuS$mu, xmuS$S)

## plcr
opcr &lt;- monomvn(xmiss, method="pcr")
Ellik.norm(opcr$mu, opcr$S, xmuS$mu, xmuS$S)

## ridge regression
oridge &lt;- monomvn(xmiss, method="ridge")
Ellik.norm(oridge$mu, oridge$S, xmuS$mu, xmuS$S)

## lasso
olasso &lt;- monomvn(xmiss, method="lasso")
Ellik.norm(olasso$mu, olasso$S, xmuS$mu, xmuS$S)

## lar
olar &lt;- monomvn(xmiss, method="lar")
Ellik.norm(olar$mu, olar$S, xmuS$mu, xmuS$S)

## forward.stagewise
ofs &lt;- monomvn(xmiss, method="forward.stagewise")
Ellik.norm(ofs$mu, ofs$S, xmuS$mu, xmuS$S)

## stepwise
ostep &lt;- monomvn(xmiss, method="stepwise")
Ellik.norm(ostep$mu, ostep$S, xmuS$mu, xmuS$S)
</code></pre>

<hr>
<h2 id='monomvn-internal'>Internal Monomvn Functions</h2><span id='topic+Igamma.inv'></span><span id='topic+get.lambda'></span><span id='topic+get.regress'></span><span id='topic+get.regress.C'></span><span id='topic+adjust.elist.C'></span><span id='topic+mvnpdf.C'></span><span id='topic+rgig.C'></span><span id='topic+bmonomvn.read.traces'></span><span id='topic+read.DA.trace'></span><span id='topic+read.muS.trace'></span><span id='topic+addy'></span><span id='topic+regress.ls'></span><span id='topic+regress.lars'></span><span id='topic+regress.pls'></span><span id='topic+regress.ridge'></span><span id='topic+regress.fact'></span><span id='topic+opt.ridge'></span><span id='topic+blasso.cleanup'></span><span id='topic+bmonomvn.cleanup'></span><span id='topic+check.start'></span><span id='topic+check.QP'></span><span id='topic+postprocess.QP'></span><span id='topic+table2blasso'></span><span id='topic+da.perm'></span><span id='topic+add.pe.QP'></span>

<h3>Description</h3>

<p>Internal monomvn functions
</p>


<h3>Details</h3>

<p>These are not to be called by the user (or in some cases are just
waiting for proper documentation to be written :))
</p>

<hr>
<h2 id='monomvn.s3'> Summarizing monomvn output </h2><span id='topic+print.monomvn'></span><span id='topic+summary.monomvn'></span><span id='topic+print.summary.monomvn'></span><span id='topic+plot.summary.monomvn'></span>

<h3>Description</h3>

<p>Summarizing, printing, and plotting the contents of a
<code>"monomvn"</code>-class object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'monomvn'
summary(object, Si = FALSE, ...)
## S3 method for class 'summary.monomvn'
print(x, ...)
## S3 method for class 'summary.monomvn'
plot(x, gt0 = FALSE, main = NULL,
     xlab = "number of zeros", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="monomvn.s3_+3A_object">object</code></td>
<td>
<p> a <code>"monomvn"</code>-class object that must be named
<code>object</code> for the generic methods <code><a href="#topic+summary.monomvn">summary.monomvn</a></code> </p>
</td></tr>
<tr><td><code id="monomvn.s3_+3A_x">x</code></td>
<td>
<p> a <code>"monomvn"</code>-class object that must be named <code>x</code>
for generic printing and plotting via
<code><a href="#topic+print.summary.monomvn">print.summary.monomvn</a></code> and
<code><a href="#topic+plot.summary.monomvn">plot.summary.monomvn</a></code> </p>
</td></tr>
<tr><td><code id="monomvn.s3_+3A_si">Si</code></td>
<td>
<p> boolean indicating whether <code>object$S</code> should be
inverted and inspected for zeros within
<code><a href="#topic+summary.monomvn">summary.monomvn</a></code>, indicating pairwise independence;
default is <code>FALSE</code> </p>
</td></tr>
<tr><td><code id="monomvn.s3_+3A_gt0">gt0</code></td>
<td>
<p> boolean indicating whether the histograms in
<code><a href="#topic+plot.summary.monomvn">plot.summary.monomvn</a></code> should exclude columns
of <code>object$S</code> or <code>Si</code> without any zero entries </p>
</td></tr>
<tr><td><code id="monomvn.s3_+3A_main">main</code></td>
<td>
<p> optional text to be added to the main title of the histograms
produced by the generic <code><a href="#topic+plot.summary.monomvn">plot.summary.monomvn</a></code> </p>
</td></tr>
<tr><td><code id="monomvn.s3_+3A_xlab">xlab</code></td>
<td>
<p>label for the x-axes of the histograms produced by
<code><a href="#topic+plot.summary.monomvn">plot.summary.monomvn</a></code>; otherwise default
automatically-generated text is used </p>
</td></tr>
<tr><td><code id="monomvn.s3_+3A_...">...</code></td>
<td>
<p> passed to <code><a href="#topic+print.monomvn">print.monomvn</a></code>, or
<code><a href="graphics.html#topic+plot.default">plot.default</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions work on the output from both <code><a href="#topic+monomvn">monomvn</a></code>
and <code><a href="#topic+bmonomvn">bmonomvn</a></code>.
</p>
<p><code><a href="#topic+print.monomvn">print.monomvn</a></code> prints the <code>call</code> followed by a
summary of the regression  method used at each iteration of the
algorithm.  It also indicates how many completely observed features
(columns) there were in the data.
For non-least-squares regressions (i.e., <span class="pkg">plsr</span>, <span class="pkg">lars</span>
and <code><a href="MASS.html#topic+lm.ridge">lm.ridge</a></code> methods)
and indication of the method used for selecting the
number of components (i.e., <code>CV</code>, <code>LOO</code>, etc., or
<code>none</code>) is provided
</p>
<p><code><a href="#topic+summary.monomvn">summary.monomvn</a></code> summarizes information about the
number of zeros in the estimated covariance matrix <code>object$S</code>
and its inverse
</p>
<p><code><a href="#topic+print.summary.monomvn">print.summary.monomvn</a></code> calls <code><a href="#topic+print.monomvn">print.monomvn</a></code>
on the <code>object</code> and then prints the result of
<code><a href="#topic+summary.monomvn">summary.monomvn</a></code>
</p>
<p><code><a href="#topic+plot.summary.monomvn">plot.summary.monomvn</a></code> makes histograms of the number of
zeros in the columns of <code>object$S</code> and its inverse
</p>


<h3>Value</h3>

<p><code><a href="#topic+summary.monomvn">summary.monomvn</a></code> returns a
<code>"summary.monomvn"</code>-class object, which is a <code>list</code>
containing (a subset of) the items below.  The other
functions do not return values.
</p>
<table>
<tr><td><code>obj</code></td>
<td>
<p> the <code>"monomvn"</code>-class <code>object</code> </p>
</td></tr>
<tr><td><code>marg</code></td>
<td>
<p> the proportion of zeros in <code>object$S</code> </p>
</td></tr>
<tr><td><code>S0</code></td>
<td>
<p> a vector containing the number of zeros in each column
of <code>object$S</code> </p>
</td></tr>
<tr><td><code>cond</code></td>
<td>
<p> if input <code>Si = TRUE</code> this field contains the
proportion of zeros in the inverse of <code>object$S</code> </p>
</td></tr>
<tr><td><code>Si0</code></td>
<td>
<p> if input <code>Si = TRUE</code> this field contains a
vector with the number of zeros in each column of the inverse
of <code>object$S</code> </p>
</td></tr>
</table>


<h3>Note</h3>

<p>There is one further S3 function for <code>"monomvn"</code>-class
objects that has its own help file: <code><a href="#topic+plot.monomvn">plot.monomvn</a></code>
</p>


<h3>Author(s)</h3>

<p> Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a> </p>


<h3>References</h3>

<p><a href="https://bobby.gramacy.com/r_packages/monomvn/">https://bobby.gramacy.com/r_packages/monomvn/</a>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+bmonomvn">bmonomvn</a></code>, <code><a href="#topic+monomvn">monomvn</a></code>,
<code><a href="#topic+plot.monomvn">plot.monomvn</a></code> </p>

<hr>
<h2 id='monomvn.solve.QP'> Solve a Quadratic Program  </h2><span id='topic+monomvn.solve.QP'></span>

<h3>Description</h3>

<p>Solve a Quadratic Program specified by a <code>QP</code> object
using the covariance matrix and mean vector specified
</p>


<h3>Usage</h3>

<pre><code class='language-R'>monomvn.solve.QP(S, QP, mu = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="monomvn.solve.QP_+3A_s">S</code></td>
<td>
<p> a positive-definite covariance <code>matrix</code> whose
dimensions agree with the Quadratic Program, e.g.,
<code>nrow(QP$Amat)</code> </p>
</td></tr>
<tr><td><code id="monomvn.solve.QP_+3A_qp">QP</code></td>
<td>
<p> a Quadratic Programming object like one that can
be generated automatically by <code>default.QP</code> </p>
</td></tr>
<tr><td><code id="monomvn.solve.QP_+3A_mu">mu</code></td>
<td>
<p> an mean vector with
<code>length(mu) = nrow(QP$Amat)</code> that is required
if <code>QP$dmu == TRUE</code> or <code>QP$mu.constr[1] != 0</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The protocol executed by this function is identical to
the one used on samples of <code class="reqn">\Sigma</code> and <code class="reqn">\mu</code>
obtained in <code><a href="#topic+bmonomvn">bmonomvn</a></code> when a Quadratic Program
is specified through the <code>QP</code> argument.  For more details
on the specification of the Quadratic Program implied by a
<code>QP</code> object, please see <code><a href="#topic+default.QP">default.QP</a></code> and
the examples therein
</p>


<h3>Value</h3>

<p>The output is a vector whose length agrees with
the dimension of <code>S</code>, describing the solution to the
Quadratic Program given
</p>


<h3>Author(s)</h3>

<p> Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a> </p>


<h3>See Also</h3>

 <p><code><a href="#topic+default.QP">default.QP</a></code>, <code><a href="#topic+bmonomvn">bmonomvn</a></code>,
and <code><a href="quadprog.html#topic+solve.QP">solve.QP</a></code> in the <span class="pkg">quadprog</span> package
</p>

<hr>
<h2 id='plot.monomvn'> Plotting bmonomvn output </h2><span id='topic+plot.monomvn'></span>

<h3>Description</h3>

<p>Functions for visualizing the output from <code><a href="#topic+bmonomvn">bmonomvn</a></code>,
particularly the posterior standard deviation estimates of the mean
vector and covariance matrix, and samples from the solution to a
Quadratic Program
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'monomvn'
plot(x, which=c("mu", "S", "Snz", "Sinz", "QP"),
     xaxis=c("numna", "index"), main=NULL, uselog=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.monomvn_+3A_x">x</code></td>
<td>
<p> a <code>"monomvn"</code>-class object that must be named <code>x</code>
for generic plotting </p>
</td></tr>
<tr><td><code id="plot.monomvn_+3A_which">which</code></td>
<td>
<p> determines the parameter whose standard deviation
to be visualized: the mean vector (<code>"mu"</code> for <code>sqrt($mu.var)</code>);
the covariance <code>matrix</code> (<code>"S"</code> for <code>sqrt($S.var)</code>),
or <code>"S{i}nz"</code> for <code>sqrt($S{i}.nz)</code>, which both result in
an <code><a href="graphics.html#topic+image">image</a></code> plot; or the distribution of solutions
<code>$W</code> to a Quadratic Program that may be obtained by
supplying <code>QP = TRUE</code> as input to <code><a href="#topic+bmonomvn">bmonomvn</a></code> </p>
</td></tr>
<tr><td><code id="plot.monomvn_+3A_xaxis">xaxis</code></td>
<td>
<p> indicates how x-axis (or x- and y-axis in the case
of <code>which = "S" || "S{i}nz"</code>) should be displayed.  The default option
<code>"numna"</code> shows the (ordered) number of missing entries
(<code>NA</code>s) in the corresponding column, whereas <code>"index"</code>
simply uses the column index; see details below </p>
</td></tr>
<tr><td><code id="plot.monomvn_+3A_main">main</code></td>
<td>
<p> optional text to be added to the main title of the plots;
the default of <code>NULL</code> causes the automatic generation of a
title </p>
</td></tr>
<tr><td><code id="plot.monomvn_+3A_uselog">uselog</code></td>
<td>
<p> a logical which, when <code>TRUE</code>, causes the log
of the standard deviation to be plotted instead </p>
</td></tr>
<tr><td><code id="plot.monomvn_+3A_...">...</code></td>
<td>
<p> passed to <code><a href="graphics.html#topic+plot.default">plot.default</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Currently, this function only provides a visualization of the
posterior standard deviation estimates of the parameters, and
the distributions of samples from the posterior of the solution
to a specified Quadratic Program.  Therefore
it only works on the output from <code><a href="#topic+bmonomvn">bmonomvn</a></code>
</p>
<p>All types of visualization (specified by <code>which</code>) are presented
in the order of the number of missing entries in the columns of the
data passed as input to  <code><a href="#topic+bmonomvn">bmonomvn</a></code>.
In the case of <code>which = "mu"</code>
this means that y-values are presented in the order <code>x$o</code>, where
the x-axis is either <code>1:length(x$o)</code> in the case of
<code>xaxis = "index"</code>, or <code>x$na[x$o]</code> in the case of <code>xaxis
    = "numna"</code>.  When <code>which = "S"</code> is given the resulting
<code><a href="graphics.html#topic+image">image</a></code> plot is likewise ordered by <code>x$o</code> where the
x- and y-axis are as above, except that in the case
where <code>xaxis = "numna"</code> the repeated counts of <code>NA</code>s are
are adjusted by small increments so that x and y arguments to
<code><a href="graphics.html#topic+image">image</a></code> are distinct.  Since a <code><a href="graphics.html#topic+boxplot">boxplot</a></code> is
used when <code>which = "QP"</code> it may be that <code>xaxis = "index"</code>
is preferred
</p>


<h3>Value</h3>

<p>The only output of this function is beautiful plots
</p>


<h3>Author(s)</h3>

<p> Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a> </p>


<h3>References</h3>

<p><a href="https://bobby.gramacy.com/r_packages/monomvn/">https://bobby.gramacy.com/r_packages/monomvn/</a>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+bmonomvn">bmonomvn</a></code>, <code><a href="#topic+print.monomvn">print.monomvn</a></code>,
<code><a href="#topic+summary.monomvn">summary.monomvn</a></code> </p>

<hr>
<h2 id='randmvn'> Randomly Generate a Multivariate Normal Distribution </h2><span id='topic+randmvn'></span>

<h3>Description</h3>

<p>Randomly generate a mean vector and covariance matrix describing
a multivariate normal (MVN) distribution, and then sample from it
</p>


<h3>Usage</h3>

<pre><code class='language-R'>randmvn(N, d, method = c("normwish", "parsimonious"),
        mup=list(mu = 0, s2 = 1), s2p=list(a = 0.5, b = 1),
        pnz=0.1, nu=Inf)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="randmvn_+3A_n">N</code></td>
<td>
<p> number of samples to draw </p>
</td></tr>
<tr><td><code id="randmvn_+3A_d">d</code></td>
<td>
<p> dimension of the  MVN, i.e., the length of the mean vector
and the number of rows/cols of the covariance matrix </p>
</td></tr>
<tr><td><code id="randmvn_+3A_method">method</code></td>
<td>
<p> the default generation method is <code>"norwish"</code>
uses the direct method described in the details section below,
whereas the <code>"parsimonious"</code> method builds up the random mean
vector and covariance via regression coefficients, intercepts,
and variances. See below for more details.  Here, a random number of
regression coefficients for each regression are set to zero </p>
</td></tr>
<tr><td><code id="randmvn_+3A_mup">mup</code></td>
<td>
<p> a <code>list</code> with entries <code>$mu</code> and <code>$s2</code>:
<code>$mu</code> is the prior mean for the independent components
of the normally distributed mean vector; <code>$s2</code> is the prior
variance</p>
</td></tr>
<tr><td><code id="randmvn_+3A_s2p">s2p</code></td>
<td>
<p> a <code>list</code> with entries <code>$a</code> and <code>$b</code> 
only valid for <code>method = "parsimonious"</code>:
<code>$a &gt; 0</code> is the baseline inverse gamma prior scale parameter
for the regression variances (the actual parameter used for
each column <code>i in 1:d</code> of the covariance matrix is
<code>a + i - 1</code>); <code>$b &gt;= 0</code> is the rate parameter</p>
</td></tr>
<tr><td><code id="randmvn_+3A_pnz">pnz</code></td>
<td>
<p> a scalar <code>0 &lt;= pnz &lt;= 1</code>, only valid for
<code>method = "parsimonious"</code>: determines the binomial
proportion of non-zero regression coefficients in the sequential
build-up of <code>mu</code> and <code>S</code>, thereby indirectly determining
the number of non-zero entries in <code>S</code></p>
</td></tr>
<tr><td><code id="randmvn_+3A_nu">nu</code></td>
<td>
<p> a scalar <code>&gt;= 1</code> indicating the degrees of freedom
of a Student-t distribution to be used instead of an MVN
when not infinite </p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the direct method (<code>"normwish"</code>) the components of the
mean vector <code>mu</code> are iid from a standard normal distribution,
and the covariance matrix <code>S</code> is
drawn from an inverse&ndash;Wishart distribution with degrees of freedom
<code>d + 2</code> and mean (centering matrix) <code>diag(d)</code>
</p>
<p>In the <code>"parsimonious"</code> method <code>mu</code> and <code>S</code> are
built up sequentially by randomly sampling intercepts, regression
coefficients (of length <code>i-1</code> for <code>i in 1:d</code>) and variances
by applying the <code>monomvn</code> equations.  A unique prior results
when a random number of the regression coefficients are set to zero.
When none are set to zero the direct method results
</p>


<h3>Value</h3>

<p>The return value is a <code>list</code> with the following components:
</p>
<table>
<tr><td><code>mu</code></td>
<td>
<p>randomly generated mean vector of length <code>d</code> </p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>randomly generated covariance <code>matrix</code> with <code>d</code>
rows and <code>d</code> columns </p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>if <code>N &gt; 0</code> then <code>x</code> is an <code>N*d</code>
<code>matrix</code> of <code>N</code> samples from the MVN with mean vector
<code>mu</code> and covariance <code>matrix</code> <code>S</code>; otherwise when
<code>N = 0</code> this component is not included</p>
</td></tr>
</table>


<h3>Note</h3>

<p> requires the <code><a href="mvtnorm.html#topic+rmvnorm">rmvnorm</a></code> function of the
<span class="pkg">mvtnorm</span> package </p>


<h3>Author(s)</h3>

<p> Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a> </p>


<h3>See Also</h3>

 <p><code><a href="#topic+rwish">rwish</a></code>, <code><a href="mvtnorm.html#topic+rmvnorm">rmvnorm</a></code>,
<code><a href="#topic+rmono">rmono</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>randmvn(5, 3)
</code></pre>

<hr>
<h2 id='regress'> Switch function for least squares and parsimonious monomvn regressions </h2><span id='topic+regress'></span>

<h3>Description</h3>

<p>This function fits the specified ordinary least squares or
parsimonious regression (plsr, pcr, ridge, and lars methods)
depending on the arguments provided, and returns estimates of
coefficients and (co-)variances in a <code>monomvn</code> friendly
format
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regress(X, y, method = c("lsr", "plsr", "pcr", "lasso", "lar",
     "forward.stagewise", "stepwise", "ridge", "factor"), p = 0,
     ncomp.max = Inf, validation = c("CV", "LOO", "Cp"),
     verb = 0, quiet = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="regress_+3A_x">X</code></td>
<td>
<p><code>data.frame</code>, <code>matrix</code>, or vector of inputs <code>X</code> </p>
</td></tr>
<tr><td><code id="regress_+3A_y">y</code></td>
<td>
<p> matrix of responses <code>y</code> of row-length equal to the
leading dimension (rows) of <code>X</code>, i.e., <code>nrow(y) ==
      nrow(X)</code>; if <code>y</code> is a vector, then <code>nrow</code> may be
interpreted as <code>length</code> </p>
</td></tr>
<tr><td><code id="regress_+3A_method">method</code></td>
<td>
<p> describes the type of <em>parsimonious</em>
(or <em>shrinkage</em>) regression, or ordinary least squares.
From the <span class="pkg">pls</span> package we have <code>"plsr"</code>
(<a href="pls.html#topic+plsr">plsr</a>, the default) for  partial least squares and
<code>"pcr"</code> (<a href="pls.html#topic+pcr">pcr</a>) for standard principal
component regression.  From the <span class="pkg">lars</span> package (see the
<code>"type"</code> argument to <a href="lars.html#topic+lars">lars</a>)
we have <code>"lasso"</code> for L1-constrained regression, <code>"lar"</code>
for least angle regression, <code>"forward.stagewise"</code> and
<code>"stepwise"</code> for fast implementations of classical forward
selection of covariates.  From the <span class="pkg">MASS</span> package we have
<code>"ridge"</code> as implemented by the <code><a href="MASS.html#topic+lm.ridge">lm.ridge</a></code>
function.  The <code>"factor"</code> method treats the first <code>p</code>
columns of <code>y</code> as known factors</p>
</td></tr>
<tr><td><code id="regress_+3A_p">p</code></td>
<td>
<p> when performing regressions, <code>0 &lt;= p &lt;= 1</code>
is the proportion of the
number of columns to rows in the design matrix before an
alternative regression <code>method</code> (except <code>"lsr"</code>)
is performed as if  least-squares regression &ldquo;failed&rdquo;.
Least-squares regression is
known to fail when the number of columns is greater than or
equal to the number of rows.
The default setting, <code>p = 0</code>, forces the specified
<code>method</code> to be used for <em>every</em> regression unless
<code>method = "lsr"</code> is specified but is unstable.
Intermediate settings of <code>p</code> allow the user
to specify that least squares regressions are preferred only
when there are sufficiently more rows in the design matrix
(<code>X</code>) than columns. When <code>method = "factor"</code> the <code>p</code>
argument represents an integer (positive) number of initial columns
of <code>y</code> to treat as known factors</p>
</td></tr>
<tr><td><code id="regress_+3A_ncomp.max">ncomp.max</code></td>
<td>
<p> maximal number of (principal) components to consider
in a <code>method</code>&mdash;only meaningful for the <code>"plsr"</code> or
<code>"pcr"</code> methods.  Large settings can cause the execution to be
slow as they drastically increase the cross-validation (CV) time</p>
</td></tr>
<tr><td><code id="regress_+3A_validation">validation</code></td>
<td>
<p> method for cross validation when applying 
a <em>parsimonious</em> regression method.  The default setting
of <code>"CV"</code> (randomized 10-fold cross-validation) is the faster method, 
but does not yield a deterministic result and does not apply for
regressions on less than ten responses. <code>"LOO"</code>
(leave-one-out cross-validation)
is deterministic, always applicable, and applied automatically whenever 
<code>"CV"</code> cannot be used.  When standard least squares is
appropriate, the methods implemented the
<span class="pkg">lars</span> package (e.g. lasso) support model choice via the
<code>"Cp"</code> statistic, which defaults to the <code>"CV"</code> method
when least squares fails.  This argument is ignored for the
<code>"ridge"</code> method; see details below</p>
</td></tr>
<tr><td><code id="regress_+3A_verb">verb</code></td>
<td>
<p> whether or not to print progress indicators.  The default
(<code>verb = 0</code>) keeps quiet.  This argument is provided for
<code><a href="#topic+monomvn">monomvn</a></code> and is not intended to be set by the user
via this interface </p>
</td></tr>
<tr><td><code id="regress_+3A_quiet">quiet</code></td>
<td>
<p> causes <code><a href="base.html#topic+warning">warning</a></code>s about regressions to be silenced
when <code>TRUE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>All <code>method</code>s (except <code>"lsr"</code>) require a scheme for
estimating the amount of variability explained by increasing numbers
of non-zero coefficients (or principal components) in the model.
Towards this end, the <span class="pkg">pls</span> and <span class="pkg">lars</span> packages support
10-fold cross validation (CV) or leave-one-out (LOO) CV estimates of
root mean squared error.  See <span class="pkg">pls</span> and <span class="pkg">lars</span> for
more details.  The <code>regress</code> function uses CV in all cases
except when <code>nrow(X) &lt;= 10</code>, in which case CV fails and
LOO is used.  Whenever <code>nrow(X) &lt;= 3</code> <code><a href="pls.html#topic+pcr">pcr</a></code>
fails,  so <code><a href="pls.html#topic+plsr">plsr</a></code> is used instead.
If <code>quiet = FALSE</code> then a <code><a href="base.html#topic+warning">warning</a></code>
is given whenever the first choice for a regression fails.
</p>
<p>For <span class="pkg">pls</span> methods, RMSEs
are calculated for a number of components in <code>1:ncomp.max</code> where
a <code>NULL</code> value for <code>ncomp.max</code> it is replaced with
</p>
<p><code>ncomp.max &lt;- min(ncomp.max, ncol(y), nrow(X)-1)</code>
</p>
<p>which is the max allowed by the <span class="pkg">pls</span> package.
</p>
<p>Simple heuristics are used to select a small number of components
(<code>ncomp</code> for <span class="pkg">pls</span>), or number of coefficients (for
<span class="pkg">lars</span>) which explains a large amount of the variability (RMSE).
The <span class="pkg">lars</span> methods use a &ldquo;one-standard error rule&rdquo; outlined
in Section 7.10, page 216 of HTF below.  The
<span class="pkg">pls</span> package does not currently support the calculation of
standard errors for CV estimates of RMSE, so a simple linear penalty
for increasing <code>ncomp</code> is used instead.  The ridge constant
(lambda) for <code><a href="MASS.html#topic+lm.ridge">lm.ridge</a></code> is set using the <code><a href="stats.html#topic+optimize">optimize</a></code>
function on the <code>GCV</code> output.
</p>


<h3>Value</h3>

<p><code>regress</code> returns a <code>list</code> containing
the components listed below.
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>a copy of the function call as used</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>a copy of the <code>method</code> input argument</p>
</td></tr>
<tr><td><code>ncomp</code></td>
<td>
<p>depends on the <code>method</code> used: is <code>NA</code> when
<code>method = "lsr"</code>; is the number of principal
components for <code>method = "pcr"</code> and <code>method = "plsr"</code>;
is the number of non-zero components in the coefficient vector
(<code>$b</code>, not counting the intercept) for any of the
<code><a href="lars.html#topic+lars">lars</a></code> methods; and gives the chosen
<code class="reqn">\lambda</code> penalty parameter for <code>method = "ridge"</code></p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>if <code>method</code> is one of <code>c("lasso",
      "forward.stagewise", "ridge")</code>, then this field records the
<code class="reqn">\lambda</code> penalty parameter used</p>
</td></tr>
<tr><td><code>b</code></td>
<td>
<p>matrix containing the estimated regression coefficients,
with <code>ncol(b) = ncol(y)</code> and the intercept
in the first row</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>(biased corrected) maximum likelihood estimate of residual
covariance matrix</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The CV in <span class="pkg">plsr</span> and <span class="pkg">lars</span> are random in nature, and so
can be dependent on the random seed.  Use <code>validation="LOO"</code> for
deterministic (but slower) result
</p>
<p>Be warned that the <span class="pkg">lars</span> implementation of
<code>"forward.stagewise"</code> can sometimes get stuck in
(what seems like) an infinite loop.
This is not a bug in the <code>regress</code> function;
the bug has been reported to the authors of <span class="pkg">lars</span>
</p>


<h3>Author(s)</h3>

<p> Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a> </p>


<h3>References</h3>

<p>Bjorn-Helge Mevik and Ron Wehrens (2007).
<em>The <span class="pkg">pls</span> Package: Principal Component and Partial
Least Squares Regression in R.</em> 
Journal of Statistical Software <b>18</b>(2)
</p>
<p>Bradley Efron, Trevor Hastie, Ian Johnstone and Robert Tibshirani
(2003).
<em>Least Angle Regression (with discussion).</em>
Annals of Statistics <b>32</b>(2); see also <br />
<a href="https://hastie.su.domains/Papers/LARS/LeastAngle_2002.pdf">https://hastie.su.domains/Papers/LARS/LeastAngle_2002.pdf</a>
</p>
<p><a href="https://bobby.gramacy.com/r_packages/monomvn/">https://bobby.gramacy.com/r_packages/monomvn/</a>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+monomvn">monomvn</a></code>, <code><a href="#topic+blasso">blasso</a></code>,
<code><a href="lars.html#topic+lars">lars</a></code> in the <span class="pkg">lars</span> library,
<code><a href="MASS.html#topic+lm.ridge">lm.ridge</a></code> in the <span class="pkg">MASS</span> library,
<code><a href="pls.html#topic+plsr">plsr</a></code> and <code><a href="pls.html#topic+pcr">pcr</a></code> in the
<span class="pkg">pls</span> library
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## following the lars diabetes example
data(diabetes)
attach(diabetes)

## Ordinary Least Squares regression
reg.ols &lt;- regress(x, y)

## Lasso regression
reg.lasso &lt;- regress(x, y, method="lasso")

## partial least squares regression
reg.plsr &lt;- regress(x, y, method="plsr")

## ridge regression
reg.ridge &lt;- regress(x, y, method="ridge")

## compare the coefs
data.frame(ols=reg.ols$b, lasso=reg.lasso$b,
           plsr=reg.plsr$b, ridge=reg.ridge$b)

## summarize the posterior distribution of lambda2 and s2
detach(diabetes)
</code></pre>

<hr>
<h2 id='returns'>Financial Returns data from NYSE and AMEX</h2><span id='topic+returns'></span><span id='topic+returns.test'></span><span id='topic+market'></span><span id='topic+market.test'></span>

<h3>Description</h3>

<p>Monthly returns of common domestic stocks traded on the NYSE and the
AMEX from April 1968 until 1998; also contains the return to the market
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(returns)
data(returns.test)
data(market)
data(market.test)
</code></pre>


<h3>Format</h3>

<p>The returns provided are collected in a <code>data.frame</code> with
1168 columns, and 360 rows in the case of <code>returns</code> and 12
rows for <code>returns.test</code>. The columns are uniquely coded to
identify the stock traded on NYSE or AMEX.  The market return
is in two vectors <code>market</code> and <code>market.test</code>
of length 360 and 12, respectively
</p>


<h3>Details</h3>

<p>The columns contain monthly returns of common domestic stocks traded
on the NYSE and the AMEX from April 1968 until 1998. <code>returns</code>
contains returns up until 1997, whereas <code>returns.test</code> has the
returns for 1997.  Both data sets have been cleaned in the following
way.  All stocks have a share price greater than $5 and a market
capitalization greater than 20% based on the size distribution of
NYSE firms.  Stocks without completely observed return
series in 1997 were also discarded.
</p>
<p>The market returns provided are essentially the monthly return on the
S&amp;P500 during the same period, which is highly correlated with the
raw monthly returns weighted by their market capitalization
</p>


<h3>Source</h3>

<p>This data is a subset of that originally used by Chan, Karceski,
and Lakonishok (1999), and subsequently by several others;
see the references below.  We use it as part of the <span class="pkg">monomvn</span>
package as an example of a real world data set following a
nearly monotone missingness pattern
</p>


<h3>References</h3>

<p>Louis K. Chan, Jason Karceski, and Josef Lakonishok (1999).
<em>On Portfolio Optimization: Forecasting Covariances and
Choosing the Risk Model</em>. The Review of Financial Studies.
<b>12</b>(5), 937-974
</p>
<p>Ravi Jagannathan and Tongshu Ma (2003).
<em>Risk Reduction in Large Portfolios: Why Imposing the
Wrong Constraints Helps</em>. Journal of Finance, American Finance
Association. <b>58</b>(4), 1641-1684
</p>
<p>Robert B. Gramacy, Joo Hee Lee, and Ricardo Silva (2008).
<em>On estimating covariances between many assets with histories 
of highly variable length</em>. <br /> Preprint available on arXiv:0710.5837:
<a href="https://arxiv.org/abs/0710.5837">https://arxiv.org/abs/0710.5837</a>
</p>
<p><a href="https://bobby.gramacy.com/r_packages/monomvn/">https://bobby.gramacy.com/r_packages/monomvn/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+monomvn">monomvn</a></code>, <code><a href="#topic+bmonomvn">bmonomvn</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(returns)

## investigate the monotone missingness pattern
returns.na &lt;- is.na(returns)
image(1:ncol(returns), 1:nrow(returns), t(returns.na))

## for a portfolio balancing exercise, see
## the example in the bmonomvn help file
</code></pre>

<hr>
<h2 id='rmono'> Randomly Impose a Monotone Missingness Pattern </h2><span id='topic+rmono'></span>

<h3>Description</h3>

<p>Randomly impose a monotone missingness pattern by replacing the ends
of each column of the input matrix by a random number of <code>NA</code>s
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmono(x, m = 7, ab = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rmono_+3A_x">x</code></td>
<td>
<p> data <code>matrix</code> </p>
</td></tr>
<tr><td><code id="rmono_+3A_m">m</code></td>
<td>
<p> minimum number of non-<code>NA</code> entries in each column </p>
</td></tr>
<tr><td><code id="rmono_+3A_ab">ab</code></td>
<td>
<p> a two-vector of <code class="reqn">\alpha</code> (<code>ab[1]</code>) and
<code class="reqn">\beta</code> (<code>ab[2]</code>) parameters to a
Beta<code class="reqn">(\alpha, \beta)</code> distribution
describing the proportion of <code>NA</code> entries in each column.
The default setting <code>ab = NULL</code> yields a uniform distribution </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The returned <code>x</code> always has one (randomly selected)
complete column, and no column has fewer than <code>m</code>
non-missing entries.  Otherwise, the proportion of missing entries
in each column can be uniform, or it can have a beta
distribution with parameters <code class="reqn">\alpha</code> (<code>ab[1]</code>) and
<code class="reqn">\beta</code> (<code>ab[2]</code>)
</p>


<h3>Value</h3>

<p>returns a <code>matrix</code> with the same dimensions as the input <code>x</code>
</p>


<h3>Author(s)</h3>

<p> Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a></p>


<h3>References</h3>

<p><a href="https://bobby.gramacy.com/r_packages/monomvn/">https://bobby.gramacy.com/r_packages/monomvn/</a>
</p>


<h3>See Also</h3>

 <p><code>randmvn</code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>out &lt;- randmvn(10, 3)
rmono(out$x)
</code></pre>

<hr>
<h2 id='rwish'>Draw from the Wishart Distribution</h2><span id='topic+rwish'></span><span id='topic+Wishart'></span>

<h3>Description</h3>

<p>Random generation from the Wishart distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   rwish(v, S)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rwish_+3A_v">v</code></td>
<td>
<p>degrees of freedom (scalar)</p>
</td></tr>
<tr><td><code id="rwish_+3A_s">S</code></td>
<td>
<p>inverse scale matrix <code class="reqn">(p \times p)</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The mean of a Wishart random variable with <code>v</code> degrees of freedom
and inverse scale matrix <code>S</code> is <code class="reqn">vS</code>.
</p>


<h3>Value</h3>

<p>Returns generates one random draw from the distribution which is a
<code>matrix</code> with the same dimensions as <code>S</code>
</p>


<h3>References</h3>

<p>This was copied from the <span class="pkg">MCMCpack</span> package
</p>


<h3>Examples</h3>

<pre><code class='language-R'>draw &lt;- rwish(3, matrix(c(1,.3,.3,1),2,2))
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
