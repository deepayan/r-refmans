<!DOCTYPE html><html lang="en"><head><title>Help for package FNN</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {FNN}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#crossentropy'><p>Cross Entropy</p></a></li>
<li><a href='#entropy'><p>Shannon Entropy</p></a></li>
<li><a href='#get.knn'><p>Search Nearest Neighbors</p></a></li>
<li><a href='#KL.dist'><p>Kullback-Leibler Divergence</p></a></li>
<li><a href='#KL.divergence'><p>Kullback-Leibler Divergence</p></a></li>
<li><a href='#knn'>
<p>k-Nearest Neighbour Classification</p></a></li>
<li><a href='#knn.cv'><p>k-Nearest Neighbour Classification Cross-Validation</p></a></li>
<li><a href='#knn.dist'><p>k Nearest Neighbor Distances</p></a></li>
<li><a href='#knn.index'><p>Search Nearest Neighbors</p></a></li>
<li><a href='#knn.reg'><p>k Nearest Neighbor Regression</p></a></li>
<li><a href='#mutinfo'><p>Mutual Information</p></a></li>
<li><a href='#ownn'>
<p>Optimal Weighted Nearest Neighbor Classification</p></a></li>
<li><a href='#print.knnReg'><p>Print Method for KNN Regression</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Version:</td>
<td>1.1.4.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-12-31</td>
</tr>
<tr>
<td>Title:</td>
<td>Fast Nearest Neighbor Search Algorithms and Applications</td>
</tr>
<tr>
<td>Copyright:</td>
<td>ANN Copyright (c) 1997-2010 University of Maryland and Sunil
Arya and David Mount. All Rights Reserved.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0.0)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>chemometrics, mvtnorm</td>
</tr>
<tr>
<td>Description:</td>
<td>Cover-tree and kd-tree fast k-nearest neighbor search algorithms and related applications
        including KNN classification, regression and information measures are implemented.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-09-22 07:57:30 UTC; hornik</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-09-22 08:24:48 UTC</td>
</tr>
<tr>
<td>Author:</td>
<td>Alina Beygelzimer [aut] (cover tree library),
  Sham Kakadet [aut] (cover tree library),
  John Langford [aut] (cover tree library),
  Sunil Arya [aut] (ANN library 1.1.2 for the kd-tree approach),
  David Mount [aut] (ANN library 1.1.2 for the kd-tree approach),
  Shengqiao Li [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Shengqiao Li &lt;lishengqiao@yahoo.com&gt;</td>
</tr>
</table>
<hr>
<h2 id='crossentropy'>Cross Entropy</h2><span id='topic+crossentropy'></span>

<h3>Description</h3>

<p>KNN Cross Entropy Estimators.</p>


<h3>Usage</h3>

<pre><code class='language-R'>  crossentropy(X, Y, k=10, algorithm=c("kd_tree", "cover_tree", "brute"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="crossentropy_+3A_x">X</code></td>
<td>
<p>an input data matrix.</p>
</td></tr>
<tr><td><code id="crossentropy_+3A_y">Y</code></td>
<td>
<p>an input data matrix.</p>
</td></tr>
<tr><td><code id="crossentropy_+3A_k">k</code></td>
<td>
<p>the maximum number of nearest neighbors to search. The default value is set to 10.</p>
</td></tr>
<tr><td><code id="crossentropy_+3A_algorithm">algorithm</code></td>
<td>
<p>nearest neighbor search algorithm.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>p(x)</code> and <code>q(x)</code> are two continuous probability density functions,
then the cross-entropy of <code>p</code> and <code>q</code> is defined as
<code class="reqn">H(p;q) = E_p[-\log q(x)]</code>.
</p>


<h3>Value</h3>

<p>a vector of length <code>k</code> for crossentropy estimates using <code>1:k</code> nearest neighbors, respectively.
</p>


<h3>Author(s)</h3>

<p>Shengqiao Li. To report any bugs or suggestions please email: <a href="mailto:lishengqiao@yahoo.com">lishengqiao@yahoo.com</a></p>


<h3>References</h3>

<p>S. Boltz, E. Debreuve and M. Barlaud (2007).
&ldquo;kNN-based high-dimensional Kullback-Leibler distance for tracking&rdquo;.
<em>Image Analysis for Multimedia Interactive Services, 2007. WIAMIS '07. Eighth International Workshop on</em>.
</p>

<hr>
<h2 id='entropy'>Shannon Entropy</h2><span id='topic+entropy'></span>

<h3>Description</h3>

<p>KNN Shannon Entropy Estimators.</p>


<h3>Usage</h3>

<pre><code class='language-R'>  entropy(X, k = 10, algorithm = c("kd_tree", "brute"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="entropy_+3A_x">X</code></td>
<td>
<p>an input data matrix.</p>
</td></tr>  
<tr><td><code id="entropy_+3A_k">k</code></td>
<td>
<p>the maximum number of nearest neighbors to search. The default value is set to 10.</p>
</td></tr>
<tr><td><code id="entropy_+3A_algorithm">algorithm</code></td>
<td>
<p>nearest neighbor search algorithm.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector of length <code>k</code> for entropy estimates using <code>1:k</code> nearest neighbors, respectively.
</p>


<h3>Author(s)</h3>

<p>Shengqiao Li. To report any bugs or suggestions please email: <a href="mailto:lishengqiao@yahoo.com">lishengqiao@yahoo.com</a></p>


<h3>References</h3>

<p>H. Singh, N. Misra, V. Hnizdo, A. Fedorowicz and E. Demchuk (2003). &ldquo;Nearest neighbor
estimates of entropy&rdquo;. <em>American Journal of Mathematical and Management Sciences</em>, <b>23</b>, 301-321.
</p>
<p>M.N. Goria, N.N.Leonenko, V.V. Mergel and P.L. Novi Inverardi (2005). 
&ldquo;A new class of random vector entropy estimators and its applications in testing statistical hypotheses&rdquo;.
<em>Journal of Nonparametric Statistics</em>, <b>17</b>:3, 277&ndash;297.
</p>
<p>R.M. Mnatsakanov, N. Misra, S. Li and E.J. Harner (2008). &ldquo;K_n-nearest neighbor estimators of entropy&rdquo;.
<em>Mathematical Methods of Statistics</em>, <b>17</b>:3, 261-277.
</p>

<hr>
<h2 id='get.knn'>Search Nearest Neighbors</h2><span id='topic+get.knn'></span><span id='topic+get.knnx'></span>

<h3>Description</h3>

<p>Fast k-nearest neighbor searching algorithms including a kd-tree, cover-tree
and the algorithm implemented in class package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  get.knn(data, k=10, algorithm=c("kd_tree", "cover_tree", "CR", "brute"))
  get.knnx(data, query, k=10, algorithm=c("kd_tree", "cover_tree",
	  "CR", "brute"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get.knn_+3A_data">data</code></td>
<td>
<p>an input data matrix.</p>
</td></tr>
<tr><td><code id="get.knn_+3A_query">query</code></td>
<td>
<p>a query data matrix.</p>
</td></tr>
<tr><td><code id="get.knn_+3A_algorithm">algorithm</code></td>
<td>
<p>nearest neighbor searching algorithm.</p>
</td></tr>
<tr><td><code id="get.knn_+3A_k">k</code></td>
<td>
<p>the maximum number of nearest neighbors to search. The default value
is set to 10.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <em>cover tree</em> is O(n) space data structure which allows us to answer queries
in the same O(log(n)) time as <em>kd tree</em> given a fixed intrinsic dimensionality.
Templated code from <a href="https://hunch.net/~jl/projects/cover_tree/cover_tree.html">https://hunch.net/~jl/projects/cover_tree/cover_tree.html</a> is used.
</p>
<p>The <em>kd tree</em> algorithm is implemented in the Approximate Near Neighbor (ANN) C++ library (see <a href="http://www.cs.umd.edu/~mount/ANN/">http://www.cs.umd.edu/~mount/ANN/</a>).
The exact nearest neighbors are searched in this package.
</p>
<p>The <em>CR</em> algorithm is the <em>VR</em> using distance <em>1-x'y</em> assuming <code>x</code> and <code>y</code> are unit vectors.
The <em>brute</em> algorithm searches linearly. It is a naive method.
</p>


<h3>Value</h3>

<p>a list contains:
</p>
<table role = "presentation">
<tr><td><code>nn.index</code></td>
<td>
<p>an n x k matrix for the nearest neighbor indice.</p>
</td></tr>
<tr><td><code>nn.dist</code></td>
<td>
<p>an n x k matrix for the nearest neighbor Euclidean distances.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Shengqiao Li. To report any bugs or suggestions please email: <a href="mailto:lishengqiao@yahoo.com">lishengqiao@yahoo.com</a></p>


<h3>References</h3>

<p>Bentley J.L. (1975), &ldquo;Multidimensional binary search trees used for associative
search,&rdquo; <em>Communication ACM</em>, <b>18</b>, 309-517.
</p>
<p>Arya S. and Mount D.M. (1993),
&ldquo;Approximate nearest neighbor searching,&rdquo;
<em>Proc. 4th Ann. ACM-SIAM Symposium on Discrete Algorithms (SODA'93)</em>, 271-280.
</p>
<p>Arya S., Mount D.M., Netanyahu N.S., Silverman R. and Wu A.Y. (1998),
&ldquo;An optimal algorithm for approximate nearest neighbor searching,&rdquo;
<em>Journal of the ACM</em>, <b>45</b>, 891-923.
</p>
<p>Beygelzimer A., Kakade S. and Langford J. (2006),
&ldquo;Cover trees for nearest neighbor,&rdquo;
<em>ACM Proc. 23rd international conference on Machine learning</em>, <b>148</b>, 97-104.
</p>


<h3>See Also</h3>

<p><code>nn2</code> in <span class="pkg">RANN</span>, <code>ann</code> in <span class="pkg">yaImpute</span> and <code><a href="class.html#topic+knn">knn</a></code> in <span class="pkg">class</span>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data&lt;- query&lt;- cbind(1:10, 1:10)

  get.knn(data, k=5)
  get.knnx(data, query, k=5)
  get.knnx(data, query, k=5, algo="kd_tree")

  th&lt;- runif(10, min=0, max=2*pi)
  data2&lt;-  cbind(cos(th), sin(th))
  get.knn(data2, k=5, algo="CR")

</code></pre>

<hr>
<h2 id='KL.dist'>Kullback-Leibler Divergence</h2><span id='topic+KL.dist'></span><span id='topic+KLx.dist'></span>

<h3>Description</h3>

<p>Compute Kullback-Leibler symmetric distance.</p>


<h3>Usage</h3>

<pre><code class='language-R'>  KL.dist(X, Y, k = 10, algorithm=c("kd_tree", "cover_tree", "brute"))
  KLx.dist(X, Y, k = 10, algorithm="kd_tree")    
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="KL.dist_+3A_x">X</code></td>
<td>
<p>An input data matrix.</p>
</td></tr>
<tr><td><code id="KL.dist_+3A_y">Y</code></td>
<td>
<p>An input data matrix.</p>
</td></tr>  
<tr><td><code id="KL.dist_+3A_k">k</code></td>
<td>
<p>The maximum number of nearest neighbors to search. The default value 
is set to 10.</p>
</td></tr>
<tr><td><code id="KL.dist_+3A_algorithm">algorithm</code></td>
<td>
<p>nearest neighbor search algorithm.</p>
</td></tr>
</table>


<h3>Details</h3>

 
<p>Kullback-Leibler distance is the sum of divergence <code>q(x)</code> from <code>p(x)</code>  and <code>p(x)</code> from <code>q(x)</code> .
</p>
<p><code>KL.*</code> versions return distances from <code>C</code> code to <code>R</code> but <code>KLx.*</code> do not.
</p>


<h3>Value</h3>

<p>Return the Kullback-Leibler distance between <code>X</code> and <code>Y</code>.
</p>


<h3>Author(s)</h3>

<p>Shengqiao Li. To report any bugs or suggestions please email: <a href="mailto:lishengqiao@yahoo.com">lishengqiao@yahoo.com</a></p>


<h3>References</h3>

<p>S. Boltz, E. Debreuve and M. Barlaud (2007).
&ldquo;kNN-based high-dimensional Kullback-Leibler distance for tracking&rdquo;.
<em>Image Analysis for Multimedia Interactive Services, 2007. WIAMIS '07. Eighth International Workshop on</em>. 
</p>
<p>S. Boltz, E. Debreuve and M. Barlaud (2009).
&ldquo;High-dimensional statistical measure for region-of-interest tracking&rdquo;.
<em>Trans. Img. Proc.</em>, <b>18</b>:6, 1266&ndash;1283.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+KL.divergence">KL.divergence</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>    set.seed(1000)
    X&lt;- rexp(10000, rate=0.2)
    Y&lt;- rexp(10000, rate=0.4)
    
    KL.dist(X, Y, k=5)                 
    KLx.dist(X, Y, k=5) 
    #thoretical distance = (0.2-0.4)^2/(0.2*0.4) = 0.5
    
</code></pre>

<hr>
<h2 id='KL.divergence'>Kullback-Leibler Divergence</h2><span id='topic+KL.divergence'></span><span id='topic+KLx.divergence'></span>

<h3>Description</h3>

<p>Compute Kullback-Leibler divergence.</p>


<h3>Usage</h3>

<pre><code class='language-R'>  KL.divergence(X, Y, k = 10, algorithm=c("kd_tree", "cover_tree", "brute"))
  KLx.divergence(X, Y, k = 10, algorithm="kd_tree")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="KL.divergence_+3A_x">X</code></td>
<td>
<p>An input data matrix.</p>
</td></tr>
<tr><td><code id="KL.divergence_+3A_y">Y</code></td>
<td>
<p>An input data matrix.</p>
</td></tr>
<tr><td><code id="KL.divergence_+3A_k">k</code></td>
<td>
<p>The maximum number of nearest neighbors to search. The default value
is set to 10.</p>
</td></tr>
<tr><td><code id="KL.divergence_+3A_algorithm">algorithm</code></td>
<td>
<p>nearest neighbor search algorithm.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>p(x)</code> and <code>q(x)</code> are two continuous probability density functions,
then the Kullback-Leibler divergence of <code>q</code> from <code>p</code> is defined as
<code class="reqn">E_p[\log \frac{p(x)}{q(x)}]</code>.
</p>
<p><code>KL.*</code> versions return divergences from <code>C</code> code to <code>R</code> but <code>KLx.*</code> do not.
</p>


<h3>Value</h3>

<p>Return the Kullback-Leibler divergence from <code>X</code> to <code>Y</code>.
</p>


<h3>Author(s)</h3>

<p>Shengqiao Li. To report any bugs or suggestions please email: <a href="mailto:lishengqiao@yahoo.com">lishengqiao@yahoo.com</a></p>


<h3>References</h3>

<p>S. Boltz, E. Debreuve and M. Barlaud (2007).
&ldquo;kNN-based high-dimensional Kullback-Leibler distance for tracking&rdquo;.
<em>Image Analysis for Multimedia Interactive Services, 2007. WIAMIS '07. Eighth International Workshop on</em>.
</p>
<p>S. Boltz, E. Debreuve and M. Barlaud (2009).
&ldquo;High-dimensional statistical measure for region-of-interest tracking&rdquo;.
<em>Trans. Img. Proc.</em>, <b>18</b>:6, 1266&ndash;1283.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+KL.dist">KL.dist</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>    set.seed(1000)
    X&lt;- rexp(10000, rate=0.2)
    Y&lt;- rexp(10000, rate=0.4)

    KL.divergence(X, Y, k=5)
    #theoretical divergence = log(0.2/0.4)+(0.4/0.2)-1 = 1-log(2) = 0.307
</code></pre>

<hr>
<h2 id='knn'>
k-Nearest Neighbour Classification
</h2><span id='topic+knn'></span>

<h3>Description</h3>

<p>k-nearest neighbour classification for test set from training set. For
each row of the test set, the <code>k</code> nearest (in Euclidean distance)
training set vectors are found, and the classification is decided by
majority vote, with ties broken at random. If there are ties for the
<code>k</code>th nearest vector, all candidates are included in the vote.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  knn(train, test, cl, k = 1, prob = FALSE, algorithm=c("kd_tree", 
      "cover_tree", "brute"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="knn_+3A_train">train</code></td>
<td>
<p>matrix or data frame of training set cases.</p>
</td></tr>
<tr><td><code id="knn_+3A_test">test</code></td>
<td>
<p>matrix or data frame of test set cases. A vector will be interpreted
as a row vector for a single case.</p>
</td></tr>
<tr><td><code id="knn_+3A_cl">cl</code></td>
<td>
<p>factor of true classifications of training set.</p>
</td></tr>
<tr><td><code id="knn_+3A_k">k</code></td>
<td>
<p>number of neighbours considered.</p>
</td></tr>
<tr><td><code id="knn_+3A_prob">prob</code></td>
<td>
<p>if this is true, the proportion of the votes for the winning class
are returned as attribute <code>prob</code>.</p>
</td></tr>
<tr><td><code id="knn_+3A_algorithm">algorithm</code></td>
<td>
<p>nearest neighbor search algorithm.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>factor of classifications of test set. <code>doubt</code> will be returned as <code>NA</code>.
</p>


<h3>Author(s)</h3>

<p>Shengqiao Li. To report any bugs or suggestions please email: <a href="mailto:lishengqiao@yahoo.com">lishengqiao@yahoo.com</a></p>


<h3>References</h3>

<p>B.D. Ripley (1996). <em>Pattern Recognition and Neural Networks.</em> Cambridge.
</p>
<p>M.N. Venables and B.D. Ripley (2002).
<em>Modern Applied Statistics with S.</em> Fourth edition.  Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ownn">ownn</a></code>, <code><a href="#topic+knn.cv">knn.cv</a></code> and <code><a href="class.html#topic+knn">knn</a></code> in <span class="pkg">class</span>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>    data(iris3)
    train &lt;- rbind(iris3[1:25,,1], iris3[1:25,,2], iris3[1:25,,3])
    test &lt;- rbind(iris3[26:50,,1], iris3[26:50,,2], iris3[26:50,,3])
    cl &lt;- factor(c(rep("s",25), rep("c",25), rep("v",25)))
    knn(train, test, cl, k = 3, prob=TRUE)
    attributes(.Last.value)
</code></pre>

<hr>
<h2 id='knn.cv'>k-Nearest Neighbour Classification Cross-Validation</h2><span id='topic+knn.cv'></span>

<h3>Description</h3>

<p>k-nearest neighbour classification cross-validation from training set.</p>


<h3>Usage</h3>

<pre><code class='language-R'>knn.cv(train, cl, k = 1, prob = FALSE, algorithm=c("kd_tree",
       "cover_tree", "brute"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="knn.cv_+3A_train">train</code></td>
<td>
<p>matrix or data frame of training set cases.</p>
</td></tr>
<tr><td><code id="knn.cv_+3A_cl">cl</code></td>
<td>
<p>factor of true classifications of training set</p>
</td></tr>
<tr><td><code id="knn.cv_+3A_k">k</code></td>
<td>
<p>number of neighbours considered.</p>
</td></tr>
<tr><td><code id="knn.cv_+3A_prob">prob</code></td>
<td>
<p>if this is true, the proportion of the votes for the winning class
are returned as attribute <code>prob</code>.
</p>
</td></tr>
<tr><td><code id="knn.cv_+3A_algorithm">algorithm</code></td>
<td>
<p>nearest neighbor search algorithm.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This uses leave-one-out cross validation.
For each row of the training set <code>train</code>, the <code>k</code> nearest
(in Euclidean distance) other training set vectors are found, and the classification 
is decided by majority vote, with ties broken at random. If there are ties for the
<code>k</code>th nearest vector, all candidates are included in the vote.
</p>


<h3>Value</h3>

<p>factor of classifications of training set. <code>doubt</code> will be returned as <code>NA</code>.
distances and indice of k nearest neighbors are also returned as attributes.
</p>


<h3>Author(s)</h3>

<p>Shengqiao Li. To report any bugs or suggestions please email: <a href="mailto:lishengqiao@yahoo.com">lishengqiao@yahoo.com</a></p>


<h3>References</h3>

<p>Ripley, B. D. (1996)
<em>Pattern Recognition and Neural Networks.</em> Cambridge.
</p>
<p>Venables, W. N. and Ripley, B. D. (2002)
<em>Modern Applied Statistics with S.</em> Fourth edition.  Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+knn">knn</a></code> and <code><a href="class.html#topic+knn.cv">knn.cv</a></code> in <span class="pkg">class</span>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(iris3)
  train &lt;- rbind(iris3[,,1], iris3[,,2], iris3[,,3])
  cl &lt;- factor(c(rep("s",50), rep("c",50), rep("v",50)))
  knn.cv(train, cl, k = 3, prob = TRUE)
  attributes(.Last.value)
</code></pre>

<hr>
<h2 id='knn.dist'>k Nearest Neighbor Distances</h2><span id='topic+knn.dist'></span><span id='topic+knnx.dist'></span>

<h3>Description</h3>

<p>Fast k-nearest neighbor distance searching algorithms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  knn.dist(data, k=10, algorithm=c("kd_tree", "cover_tree", "CR", "brute"))
  knnx.dist(data, query, k=10, algorithm=c("kd_tree", "cover_tree",
           "CR", "brute"))

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="knn.dist_+3A_data">data</code></td>
<td>
<p>an input data matrix.</p>
</td></tr>
<tr><td><code id="knn.dist_+3A_query">query</code></td>
<td>
<p>a query data matrix.</p>
</td></tr>
<tr><td><code id="knn.dist_+3A_algorithm">algorithm</code></td>
<td>
<p>nearest neighbor searching algorithm.</p>
</td></tr>
<tr><td><code id="knn.dist_+3A_k">k</code></td>
<td>
<p>the maximum number of nearest neighbors to search. The default value
is set to 10.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>return the Euclidiean distances of k nearest neighbors.
</p>


<h3>Author(s)</h3>

<p>Shengqiao Li. To report any bugs or suggestions please email: <a href="mailto:lishengqiao@yahoo.com">lishengqiao@yahoo.com</a></p>


<h3>References</h3>

<p>Bentley J.L. (1975), &ldquo;Multidimensional binary search trees used for associative
search,&rdquo; <em>Communication ACM</em>, <b>18</b>, 309-517.
</p>
<p>Arya S. and Mount D.M. (1993),
&ldquo;Approximate nearest neighbor searching,&rdquo;
<em>Proc. 4th Ann. ACM-SIAM Symposium on Discrete Algorithms (SODA'93)</em>, 271-280.
</p>
<p>Arya S., Mount D.M., Netanyahu N.S., Silverman R. and Wu A.Y. (1998),
&ldquo;An optimal algorithm for approximate nearest neighbor searching,&rdquo;
<em>Journal of the ACM</em>, <b>45</b>, 891-923.
</p>
<p>Beygelzimer A., Kakade S. and Langford J. (2006),
&ldquo;Cover trees for nearest neighbor,&rdquo;
<em>ACM Proc. 23rd international conference on Machine learning</em>, <b>148</b>, 97-104.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+get.knn">get.knn</a></code> and <code><a href="#topic+knn.index">knn.index</a></code> .
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  if(require(mvtnorm))
  {
    sigma&lt;- function(v, r, p)
    {
      	V&lt;- matrix(r^2, ncol=p, nrow=p)
    	  diag(V)&lt;- 1
        V*v
    }

    X&lt;- rmvnorm(1000, mean=rep(0, 20), sigma(1, .5, 20))
    print(system.time(knn.dist(X)) )
    print(system.time(knn.dist(X, algorithm = "kd_tree")))

  }
</code></pre>

<hr>
<h2 id='knn.index'>Search Nearest Neighbors</h2><span id='topic+knn.index'></span><span id='topic+knnx.index'></span>

<h3>Description</h3>

<p>Fast k-nearest neighbor searching algorithms including a kd-tree, cover-tree
and the algorithm implemented in class package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  knn.index(data, k=10, algorithm=c("kd_tree", "cover_tree", "CR", "brute"))
  knnx.index(data, query, k=10, algorithm=c("kd_tree", "cover_tree", 
             "CR", "brute"))

</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="knn.index_+3A_data">data</code></td>
<td>
<p>an input data matrix.</p>
</td></tr>
<tr><td><code id="knn.index_+3A_query">query</code></td>
<td>
<p>a query data matrix.</p>
</td></tr>
<tr><td><code id="knn.index_+3A_algorithm">algorithm</code></td>
<td>
<p>nearest neighbor searching algorithm.</p>
</td></tr>
<tr><td><code id="knn.index_+3A_k">k</code></td>
<td>
<p>the maximum number of nearest neighbors to search. The default value
is set to 10.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>return the indice of k nearest neighbors.
</p>


<h3>Author(s)</h3>

<p>Shengqiao Li. To report any bugs or suggestions please email: <a href="mailto:lishengqiao@yahoo.com">lishengqiao@yahoo.com</a></p>


<h3>References</h3>

<p>Bentley J.L. (1975), &ldquo;Multidimensional binary search trees used for associative
search,&rdquo; <em>Communication ACM</em>, <b>18</b>, 309-517.
</p>
<p>Arya S. and Mount D.M. (1993),
&ldquo;Approximate nearest neighbor searching,&rdquo;
<em>Proc. 4th Ann. ACM-SIAM Symposium on Discrete Algorithms (SODA'93)</em>, 271-280.
</p>
<p>Arya S., Mount D.M., Netanyahu N.S., Silverman R. and Wu A.Y. (1998),
&ldquo;An optimal algorithm for approximate nearest neighbor searching,&rdquo;
<em>Journal of the ACM</em>, <b>45</b>, 891-923.
</p>
<p>Beygelzimer A., Kakade S. and Langford J. (2006),
&ldquo;Cover trees for nearest neighbor,&rdquo;
<em>ACM Proc. 23rd international conference on Machine learning</em>, <b>148</b>, 97-104.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+knn.dist">knn.dist</a></code> and <code><a href="#topic+get.knn">get.knn</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data&lt;- query&lt;- cbind(1:10, 1:10)

  knn.index(data, k=5)
  knnx.index(data, query, k=5)
  knnx.index(data, query, k=5, algo="kd_tree")

</code></pre>

<hr>
<h2 id='knn.reg'>k Nearest Neighbor Regression</h2><span id='topic+knn.reg'></span>

<h3>Description</h3>

<p>k-nearest neighbor regression
</p>


<h3>Usage</h3>

<pre><code class='language-R'>knn.reg(train, test = NULL, y, k = 3, algorithm=c("kd_tree", 
        "cover_tree", "brute"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="knn.reg_+3A_train">train</code></td>
<td>

<p>matrix or data frame of training set cases.
</p>
</td></tr>
<tr><td><code id="knn.reg_+3A_test">test</code></td>
<td>

<p>matrix or data frame of test set cases. A vector will be interpreted
as a row vector for a single case. If not supplied, cross-validataion will be done.
</p>
</td></tr>
<tr><td><code id="knn.reg_+3A_y">y</code></td>
<td>
<p>reponse of each observation in the training set.</p>
</td></tr>
<tr><td><code id="knn.reg_+3A_k">k</code></td>
<td>
<p> number of neighbours considered.</p>
</td></tr>
<tr><td><code id="knn.reg_+3A_algorithm">algorithm</code></td>
<td>
<p>nearest neighbor search algorithm.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If test is not supplied, Leave one out cross-validation is performed and <em>R-square</em> is the predicted R-square.
</p>


<h3>Value</h3>

<p><code>knn.reg</code> returns an object of <code>class</code> <code>"knnReg"</code> or <code>"knnRegCV"</code> 
if <code>test</code> data is not supplied.
</p>
<p>The returnedobject is a list containing at least the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the match call.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>number of neighbours considered.</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>number of predicted values, either equals test size or train size.</p>
</td></tr>
<tr><td><code>pred</code></td>
<td>
<p>a vector of predicted values.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>predicted residuals. <code>NULL</code> if <code>test</code> is supplied.</p>
</td></tr>
<tr><td><code>PRESS</code></td>
<td>
<p>the sums of squares of the predicted residuals. <code>NULL</code> if <code>test</code> is supplied.</p>
</td></tr>
<tr><td><code>R2Pred</code></td>
<td>
<p>predicted R-square. <code>NULL</code> if <code>test</code> is supplied.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The code for &ldquo;VR&rdquo; nearest neighbor searching is taken from <code>class</code> source
</p>


<h3>Author(s)</h3>

<p>Shengqiao Li. To report any bugs or suggestions please email: <a href="mailto:lishengqiao@yahoo.com">lishengqiao@yahoo.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+knn">knn</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  if(require(chemometrics)){
    data(PAC);
    pac.knn&lt;- knn.reg(PAC$X, y=PAC$y, k=3);
    
    plot(PAC$y, pac.knn$pred, xlab="y", ylab=expression(hat(y)))
  } 
</code></pre>

<hr>
<h2 id='mutinfo'>Mutual Information</h2><span id='topic+mutinfo'></span>

<h3>Description</h3>

<p>KNN Mutual Information Estimators.</p>


<h3>Usage</h3>

<pre><code class='language-R'>  mutinfo(X, Y, k=10, direct=TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mutinfo_+3A_x">X</code></td>
<td>
<p>an input data matrix.</p>
</td></tr>
<tr><td><code id="mutinfo_+3A_y">Y</code></td>
<td>
<p>an input data matrix.</p>
</td></tr>
<tr><td><code id="mutinfo_+3A_k">k</code></td>
<td>
<p>the maximum number of nearest neighbors to search. The default value is set to 10.</p>
</td></tr>
<tr><td><code id="mutinfo_+3A_direct">direct</code></td>
<td>
<p>Directly compute or via entropies.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The direct computation is based on the first estimator of A. Kraskov, H. Stogbauer and P.Grassberger (2004) and 
the indirect computation is done via entropy estimates, i.e., I(X, Y) = H (X) + H(Y) - H(X, Y).
The direct method has smaller bias and variance but the indirect method is faster, see Evans (2008).
</p>


<h3>Value</h3>

<p>For direct method,  one mutual information estimate;
For indirect method,a vector of length <code>k</code> for mutual information estimates using <code>1:k</code> nearest neighbors, respectively.
</p>


<h3>Author(s)</h3>

<p>Shengqiao Li. To report any bugs or suggestions please email: <a href="mailto:lishengqiao@yahoo.com">lishengqiao@yahoo.com</a></p>


<h3>References</h3>

<p>A. Kraskov, H. Stogbauer and P.Grassberger (2004).
&ldquo;Estimating mutual information&rdquo;.
<em>Physical Review E</em>, <b>69</b>:066138, 1&ndash;16.
</p>
<p>D. Evans (2008).
&ldquo;A Computationally efficient estimator for mutual information&rdquo;.
<em>Proc. R. Soc. A</em>, <b>464</b>, 1203&ndash;1215.
</p>

<hr>
<h2 id='ownn'>
Optimal Weighted Nearest Neighbor Classification
</h2><span id='topic+ownn'></span>

<h3>Description</h3>

<p>This function implements Samworth's optimal weighting scheme for k nearest neighbor classification. The performance improvement is greatest when the dimension is 4 as reported in the reference.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  ownn(train, test, cl, testcl=NULL, k = NULL, prob = FALSE,
      algorithm=c("kd_tree", "cover_tree", "brute"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ownn_+3A_train">train</code></td>
<td>
<p>matrix or data frame of training set cases.</p>
</td></tr>
<tr><td><code id="ownn_+3A_test">test</code></td>
<td>
<p>matrix or data frame of test set cases. A vector will be interpreted
as a row vector for a single case.</p>
</td></tr>
<tr><td><code id="ownn_+3A_cl">cl</code></td>
<td>
<p>factor of true classifications of training set.</p>
</td></tr>
<tr><td><code id="ownn_+3A_testcl">testcl</code></td>
<td>
<p>factor of true classifications of testing set for error rate calculation.</p>
</td></tr>
<tr><td><code id="ownn_+3A_k">k</code></td>
<td>
<p>number of neighbours considered, chosen by 5-fold cross-validation if not supplied.</p>
</td></tr>
<tr><td><code id="ownn_+3A_prob">prob</code></td>
<td>
<p>if this is true, the proportion of the weights for the winning class
are returned as attribute <code>prob</code>.</p>
</td></tr>
<tr><td><code id="ownn_+3A_algorithm">algorithm</code></td>
<td>
<p>nearest neighbor search algorithm.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list includes k, predictions by ordinary knn, optimal weighted knn and bagged knn, and accuracies if class labels of test data set are given. 
</p>


<h3>Author(s)</h3>

<p>Shengqiao Li. To report any bugs or suggestions please email: <a href="mailto:lishengqiao@yahoo.com">lishengqiao@yahoo.com</a></p>


<h3>References</h3>

<p>Richard J. Samworth (2012),
&ldquo;Optimal Weighted Nearest Neighbor Classifiers,&rdquo; <em>Annals of Statistics</em>,  <b>40:5</b>, 2733-2763.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+knn">knn</a></code> and <code><a href="class.html#topic+knn">knn</a></code> in <span class="pkg">class</span>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>    data(iris3)
    train &lt;- rbind(iris3[1:25,,1], iris3[1:25,,2], iris3[1:25,,3])
    test &lt;- rbind(iris3[26:50,,1], iris3[26:50,,2], iris3[26:50,,3])
    cl &lt;- factor(c(rep("s",25), rep("c",25), rep("v",25)))
    testcl &lt;- factor(c(rep("s",25), rep("c",25), rep("v",25)))
    out &lt;- ownn(train, test, cl, testcl)
    out
</code></pre>

<hr>
<h2 id='print.knnReg'>Print Method for KNN Regression</h2><span id='topic+print.knnRegCV'></span><span id='topic+print.knnReg'></span>

<h3>Description</h3>

<p>Print method for KNN regression.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'knnReg'
print(x, ...)
## S3 method for class 'knnRegCV'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.knnReg_+3A_x">x</code></td>
<td>
<p>a <code>knnReg</code> or <code>knnRegCV</code> object.</p>
</td></tr>
<tr><td><code id="print.knnReg_+3A_...">...</code></td>
<td>
<p>Additonal <code>print</code> arguments.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Shengqiao Li. To report any bugs or suggestions please email: <a href="mailto:lishengqiao@yahoo.com">lishengqiao@yahoo.com</a></p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
