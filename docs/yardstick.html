<!DOCTYPE html><html><head><title>Help for package yardstick</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {yardstick}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#yardstick-package'><p>yardstick: Tidy Characterizations of Model Performance</p></a></li>
<li><a href='#accuracy'><p>Accuracy</p></a></li>
<li><a href='#average_precision'><p>Area under the precision recall curve</p></a></li>
<li><a href='#bal_accuracy'><p>Balanced accuracy</p></a></li>
<li><a href='#brier_class'><p>Brier score for classification models</p></a></li>
<li><a href='#brier_survival'><p>Time-Dependent Brier score for right censored data</p></a></li>
<li><a href='#brier_survival_integrated'><p>Integrated Brier score for right censored data</p></a></li>
<li><a href='#ccc'><p>Concordance correlation coefficient</p></a></li>
<li><a href='#check_metric'><p>Developer function for checking inputs in new metrics</p></a></li>
<li><a href='#classification_cost'><p>Costs function for poor classification</p></a></li>
<li><a href='#concordance_survival'><p>Concordance index for right-censored data</p></a></li>
<li><a href='#conf_mat'><p>Confusion Matrix for Categorical Data</p></a></li>
<li><a href='#demographic_parity'><p>Demographic parity</p></a></li>
<li><a href='#detection_prevalence'><p>Detection prevalence</p></a></li>
<li><a href='#developer-helpers'><p>Developer helpers</p></a></li>
<li><a href='#equal_opportunity'><p>Equal opportunity</p></a></li>
<li><a href='#equalized_odds'><p>Equalized odds</p></a></li>
<li><a href='#f_meas'><p>F Measure</p></a></li>
<li><a href='#gain_capture'><p>Gain capture</p></a></li>
<li><a href='#gain_curve'><p>Gain curve</p></a></li>
<li><a href='#hpc_cv'><p>Multiclass Probability Predictions</p></a></li>
<li><a href='#huber_loss'><p>Huber loss</p></a></li>
<li><a href='#huber_loss_pseudo'><p>Psuedo-Huber Loss</p></a></li>
<li><a href='#iic'><p>Index of ideality of correlation</p></a></li>
<li><a href='#j_index'><p>J-index</p></a></li>
<li><a href='#kap'><p>Kappa</p></a></li>
<li><a href='#lift_curve'><p>Lift curve</p></a></li>
<li><a href='#lung_surv'><p>Survival Analysis Results</p></a></li>
<li><a href='#mae'><p>Mean absolute error</p></a></li>
<li><a href='#mape'><p>Mean absolute percent error</p></a></li>
<li><a href='#mase'><p>Mean absolute scaled error</p></a></li>
<li><a href='#mcc'><p>Matthews correlation coefficient</p></a></li>
<li><a href='#metric_set'><p>Combine metric functions</p></a></li>
<li><a href='#metric_summarizer'><p>Developer function for summarizing new metrics</p></a></li>
<li><a href='#metric_tweak'><p>Tweak a metric function</p></a></li>
<li><a href='#metric_vec_template'><p>Developer function for calling new metrics</p></a></li>
<li><a href='#metric-summarizers'><p>Developer function for summarizing new metrics</p></a></li>
<li><a href='#metrics'><p>General Function to Estimate Performance</p></a></li>
<li><a href='#mn_log_loss'><p>Mean log loss for multinomial data</p></a></li>
<li><a href='#mpe'><p>Mean percentage error</p></a></li>
<li><a href='#msd'><p>Mean signed deviation</p></a></li>
<li><a href='#new_groupwise_metric'><p>Create groupwise metrics</p></a></li>
<li><a href='#new-metric'><p>Construct a new metric function</p></a></li>
<li><a href='#npv'><p>Negative predictive value</p></a></li>
<li><a href='#pathology'><p>Liver Pathology Data</p></a></li>
<li><a href='#poisson_log_loss'><p>Mean log loss for Poisson data</p></a></li>
<li><a href='#ppv'><p>Positive predictive value</p></a></li>
<li><a href='#pr_auc'><p>Area under the precision recall curve</p></a></li>
<li><a href='#pr_curve'><p>Precision recall curve</p></a></li>
<li><a href='#precision'><p>Precision</p></a></li>
<li><a href='#recall'><p>Recall</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#rmse'><p>Root mean squared error</p></a></li>
<li><a href='#roc_auc'><p>Area under the receiver operator curve</p></a></li>
<li><a href='#roc_auc_survival'><p>Time-Dependent ROC AUC for Censored Data</p></a></li>
<li><a href='#roc_aunp'><p>Area under the ROC curve of each class against the rest, using the a priori</p>
class distribution</a></li>
<li><a href='#roc_aunu'><p>Area under the ROC curve of each class against the rest, using the uniform</p>
class distribution</a></li>
<li><a href='#roc_curve'><p>Receiver operator curve</p></a></li>
<li><a href='#roc_curve_survival'><p>Time-Dependent ROC surve for Censored Data</p></a></li>
<li><a href='#rpd'><p>Ratio of performance to deviation</p></a></li>
<li><a href='#rpiq'><p>Ratio of performance to inter-quartile</p></a></li>
<li><a href='#rsq'><p>R squared</p></a></li>
<li><a href='#rsq_trad'><p>R squared - traditional</p></a></li>
<li><a href='#sens'><p>Sensitivity</p></a></li>
<li><a href='#smape'><p>Symmetric mean absolute percentage error</p></a></li>
<li><a href='#solubility_test'><p>Solubility Predictions from MARS Model</p></a></li>
<li><a href='#spec'><p>Specificity</p></a></li>
<li><a href='#summary.conf_mat'><p>Summary Statistics for Confusion Matrices</p></a></li>
<li><a href='#two_class_example'><p>Two Class Predictions</p></a></li>
<li><a href='#yardstick_remove_missing'><p>Developer function for handling missing values in new metrics</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Tidy Characterizations of Model Performance</td>
</tr>
<tr>
<td>Version:</td>
<td>1.3.1</td>
</tr>
<tr>
<td>Description:</td>
<td>Tidy tools for quantifying how well model fits to a data set
    such as confusion matrices, class probability curve summaries, and
    regression metrics (e.g., RMSE).</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/tidymodels/yardstick">https://github.com/tidymodels/yardstick</a>,
<a href="https://yardstick.tidymodels.org">https://yardstick.tidymodels.org</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/tidymodels/yardstick/issues">https://github.com/tidymodels/yardstick/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>cli, dplyr (&ge; 1.1.0), generics (&ge; 0.1.2), hardhat (&ge;
1.3.0), lifecycle (&ge; 1.0.3), rlang (&ge; 1.1.0), tibble,
tidyselect (&ge; 1.2.0), utils, vctrs (&ge; 0.5.0), withr</td>
</tr>
<tr>
<td>Suggests:</td>
<td>covr, crayon, ggplot2, knitr, probably (&ge; 1.0.0), rmarkdown,
survival (&ge; 3.5-0), testthat (&ge; 3.0.0), tidyr</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Config/Needs/website:</td>
<td>tidyverse/tidytemplate</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.0</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-21 21:01:38 UTC; emilhvitfeldt</td>
</tr>
<tr>
<td>Author:</td>
<td>Max Kuhn [aut],
  Davis Vaughan [aut],
  Emil Hvitfeldt <a href="https://orcid.org/0000-0002-0679-1945"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Posit Software, PBC [cph, fnd]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Emil Hvitfeldt &lt;emil.hvitfeldt@posit.co&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-21 21:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='yardstick-package'>yardstick: Tidy Characterizations of Model Performance</h2><span id='topic+yardstick'></span><span id='topic+yardstick-package'></span>

<h3>Description</h3>

<p><img src="../help/figures/logo.png" style='float: right' alt='logo' width='120' />
</p>
<p>Tidy tools for quantifying how well model fits to a data set such as confusion matrices, class probability curve summaries, and regression metrics (e.g., RMSE).
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Emil Hvitfeldt <a href="mailto:emil.hvitfeldt@posit.co">emil.hvitfeldt@posit.co</a> (<a href="https://orcid.org/0000-0002-0679-1945">ORCID</a>)
</p>
<p>Authors:
</p>

<ul>
<li><p> Max Kuhn <a href="mailto:max@posit.co">max@posit.co</a>
</p>
</li>
<li><p> Davis Vaughan <a href="mailto:davis@posit.co">davis@posit.co</a>
</p>
</li></ul>

<p>Other contributors:
</p>

<ul>
<li><p> Posit Software, PBC [copyright holder, funder]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/tidymodels/yardstick">https://github.com/tidymodels/yardstick</a>
</p>
</li>
<li> <p><a href="https://yardstick.tidymodels.org">https://yardstick.tidymodels.org</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/tidymodels/yardstick/issues">https://github.com/tidymodels/yardstick/issues</a>
</p>
</li></ul>


<hr>
<h2 id='accuracy'>Accuracy</h2><span id='topic+accuracy'></span><span id='topic+accuracy.data.frame'></span><span id='topic+accuracy_vec'></span>

<h3>Description</h3>

<p>Accuracy is the proportion of the data that are predicted correctly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>accuracy(data, ...)

## S3 method for class 'data.frame'
accuracy(data, truth, estimate, na_rm = TRUE, case_weights = NULL, ...)

accuracy_vec(truth, estimate, na_rm = TRUE, case_weights = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="accuracy_+3A_data">data</code></td>
<td>
<p>Either a <code>data.frame</code> containing the columns specified by the
<code>truth</code> and <code>estimate</code> arguments, or a <code>table</code>/<code>matrix</code> where the true
class results should be in the columns of the table.</p>
</td></tr>
<tr><td><code id="accuracy_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="accuracy_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a <code>factor</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="accuracy_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predicted class
results (that is also <code>factor</code>). As with <code>truth</code> this can be
specified different ways but the primary method is to use an
unquoted variable name. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="accuracy_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="accuracy_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights.
This should be an unquoted column name that evaluates to a numeric column
in <code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>accuracy_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Multiclass</h3>

<p>Accuracy extends naturally to multiclass scenarios. Because
of this, macro and micro averaging are not implemented.
</p>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>See Also</h3>

<p>Other class metrics: 
<code><a href="#topic+bal_accuracy">bal_accuracy</a>()</code>,
<code><a href="#topic+detection_prevalence">detection_prevalence</a>()</code>,
<code><a href="#topic+f_meas">f_meas</a>()</code>,
<code><a href="#topic+j_index">j_index</a>()</code>,
<code><a href="#topic+kap">kap</a>()</code>,
<code><a href="#topic+mcc">mcc</a>()</code>,
<code><a href="#topic+npv">npv</a>()</code>,
<code><a href="#topic+ppv">ppv</a>()</code>,
<code><a href="#topic+precision">precision</a>()</code>,
<code><a href="#topic+recall">recall</a>()</code>,
<code><a href="#topic+sens">sens</a>()</code>,
<code><a href="#topic+spec">spec</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)
data("two_class_example")
data("hpc_cv")

# Two class
accuracy(two_class_example, truth, predicted)

# Multiclass
# accuracy() has a natural multiclass extension
hpc_cv %&gt;%
  filter(Resample == "Fold01") %&gt;%
  accuracy(obs, pred)

# Groups are respected
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  accuracy(obs, pred)
</code></pre>

<hr>
<h2 id='average_precision'>Area under the precision recall curve</h2><span id='topic+average_precision'></span><span id='topic+average_precision.data.frame'></span><span id='topic+average_precision_vec'></span>

<h3>Description</h3>

<p><code>average_precision()</code> is an alternative to <code>pr_auc()</code> that avoids any
ambiguity about what the value of <code>precision</code> should be when <code>recall == 0</code>
and there are not yet any false positive values (some say it should be <code>0</code>,
others say <code>1</code>, others say undefined).
</p>
<p>It computes a weighted average of the precision values returned from
<code><a href="#topic+pr_curve">pr_curve()</a></code>, where the weights are the increase in recall from the previous
threshold. See <code><a href="#topic+pr_curve">pr_curve()</a></code> for the full curve.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>average_precision(data, ...)

## S3 method for class 'data.frame'
average_precision(
  data,
  truth,
  ...,
  estimator = NULL,
  na_rm = TRUE,
  event_level = yardstick_event_level(),
  case_weights = NULL
)

average_precision_vec(
  truth,
  estimate,
  estimator = NULL,
  na_rm = TRUE,
  event_level = yardstick_event_level(),
  case_weights = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="average_precision_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by <code>truth</code> and
<code>...</code>.</p>
</td></tr>
<tr><td><code id="average_precision_+3A_...">...</code></td>
<td>
<p>A set of unquoted column names or one or more
<code>dplyr</code> selector functions to choose which variables contain the
class probabilities. If <code>truth</code> is binary, only 1 column should be selected,
and it should correspond to the value of <code>event_level</code>. Otherwise, there
should be as many columns as factor levels of <code>truth</code> and the ordering of
the columns should be the same as the factor levels of <code>truth</code>.</p>
</td></tr>
<tr><td><code id="average_precision_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a <code>factor</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="average_precision_+3A_estimator">estimator</code></td>
<td>
<p>One of <code>"binary"</code>, <code>"macro"</code>, or <code>"macro_weighted"</code> to
specify the type of averaging to be done. <code>"binary"</code> is only relevant for
the two class case. The other two are general methods for calculating
multiclass metrics. The default will automatically choose <code>"binary"</code> or
<code>"macro"</code> based on <code>truth</code>.</p>
</td></tr>
<tr><td><code id="average_precision_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="average_precision_+3A_event_level">event_level</code></td>
<td>
<p>A single string. Either <code>"first"</code> or <code>"second"</code> to specify
which level of <code>truth</code> to consider as the &quot;event&quot;. This argument is only
applicable when <code>estimator = "binary"</code>. The default uses an internal helper
that defaults to <code>"first"</code>.</p>
</td></tr>
<tr><td><code id="average_precision_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights.
This should be an unquoted column name that evaluates to a numeric column
in <code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
<tr><td><code id="average_precision_+3A_estimate">estimate</code></td>
<td>
<p>If <code>truth</code> is binary, a numeric vector of class probabilities
corresponding to the &quot;relevant&quot; class. Otherwise, a matrix with as many
columns as factor levels of <code>truth</code>. <em>It is assumed that these are in the
same order as the levels of <code>truth</code>.</em></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The computation for average precision is a weighted average of the precision
values. Assuming you have <code>n</code> rows returned from <code><a href="#topic+pr_curve">pr_curve()</a></code>, it is a sum
from <code>2</code> to <code>n</code>, multiplying the precision value <code>p_i</code> by the increase in
recall over the previous threshold, <code>r_i - r_(i-1)</code>.
</p>
<p style="text-align: center;"><code class="reqn">AP = \sum (r_{i} - r_{i-1}) * p_i</code>
</p>

<p>By summing from <code>2</code> to <code>n</code>, the precision value <code>p_1</code> is never used. While
<code><a href="#topic+pr_curve">pr_curve()</a></code> returns a value for <code>p_1</code>, it is technically undefined as
<code>tp / (tp + fp)</code> with <code>tp = 0</code> and <code>fp = 0</code>. A common convention is to use
<code>1</code> for <code>p_1</code>, but this metric has the nice property of avoiding the
ambiguity. On the other hand, <code>r_1</code> is well defined as long as there are
some events (<code>p</code>), and it is <code>tp / p</code> with <code>tp = 0</code>, so <code>r_1 = 0</code>.
</p>
<p>When <code>p_1</code> is defined as <code>1</code>, the <code>average_precision()</code> and <code>roc_auc()</code>
values are often very close to one another.
</p>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>average_precision_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Multiclass</h3>

<p>Macro and macro-weighted averaging is available for this metric.
The default is to select macro averaging if a <code>truth</code> factor with more
than 2 levels is provided. Otherwise, a standard binary calculation is done.
See <code>vignette("multiclass", "yardstick")</code> for more information.
</p>


<h3>Relevant Level</h3>

<p>There is no common convention on which factor level should
automatically be considered the &quot;event&quot; or &quot;positive&quot; result
when computing binary classification metrics. In <code>yardstick</code>, the default
is to use the <em>first</em> level. To alter this, change the argument
<code>event_level</code> to <code>"second"</code> to consider the <em>last</em> level of the factor the
level of interest. For multiclass extensions involving one-vs-all
comparisons (such as macro averaging), this option is ignored and
the &quot;one&quot; level is always the relevant result.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pr_curve">pr_curve()</a></code> for computing the full precision recall curve.
</p>
<p><code><a href="#topic+pr_auc">pr_auc()</a></code> for computing the area under the precision recall curve using
the trapezoidal rule.
</p>
<p>Other class probability metrics: 
<code><a href="#topic+brier_class">brier_class</a>()</code>,
<code><a href="#topic+classification_cost">classification_cost</a>()</code>,
<code><a href="#topic+gain_capture">gain_capture</a>()</code>,
<code><a href="#topic+mn_log_loss">mn_log_loss</a>()</code>,
<code><a href="#topic+pr_auc">pr_auc</a>()</code>,
<code><a href="#topic+roc_auc">roc_auc</a>()</code>,
<code><a href="#topic+roc_aunp">roc_aunp</a>()</code>,
<code><a href="#topic+roc_aunu">roc_aunu</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># ---------------------------------------------------------------------------
# Two class example

# `truth` is a 2 level factor. The first level is `"Class1"`, which is the
# "event of interest" by default in yardstick. See the Relevant Level
# section above.
data(two_class_example)

# Binary metrics using class probabilities take a factor `truth` column,
# and a single class probability column containing the probabilities of
# the event of interest. Here, since `"Class1"` is the first level of
# `"truth"`, it is the event of interest and we pass in probabilities for it.
average_precision(two_class_example, truth, Class1)

# ---------------------------------------------------------------------------
# Multiclass example

# `obs` is a 4 level factor. The first level is `"VF"`, which is the
# "event of interest" by default in yardstick. See the Relevant Level
# section above.
data(hpc_cv)

# You can use the col1:colN tidyselect syntax
library(dplyr)
hpc_cv %&gt;%
  filter(Resample == "Fold01") %&gt;%
  average_precision(obs, VF:L)

# Change the first level of `obs` from `"VF"` to `"M"` to alter the
# event of interest. The class probability columns should be supplied
# in the same order as the levels.
hpc_cv %&gt;%
  filter(Resample == "Fold01") %&gt;%
  mutate(obs = relevel(obs, "M")) %&gt;%
  average_precision(obs, M, VF:L)

# Groups are respected
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  average_precision(obs, VF:L)

# Weighted macro averaging
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  average_precision(obs, VF:L, estimator = "macro_weighted")

# Vector version
# Supply a matrix of class probabilities
fold1 &lt;- hpc_cv %&gt;%
  filter(Resample == "Fold01")

average_precision_vec(
   truth = fold1$obs,
   matrix(
     c(fold1$VF, fold1$F, fold1$M, fold1$L),
     ncol = 4
   )
)

</code></pre>

<hr>
<h2 id='bal_accuracy'>Balanced accuracy</h2><span id='topic+bal_accuracy'></span><span id='topic+bal_accuracy.data.frame'></span><span id='topic+bal_accuracy_vec'></span>

<h3>Description</h3>

<p>Balanced accuracy is computed here as the average of <code><a href="#topic+sens">sens()</a></code> and <code><a href="#topic+spec">spec()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bal_accuracy(data, ...)

## S3 method for class 'data.frame'
bal_accuracy(
  data,
  truth,
  estimate,
  estimator = NULL,
  na_rm = TRUE,
  case_weights = NULL,
  event_level = yardstick_event_level(),
  ...
)

bal_accuracy_vec(
  truth,
  estimate,
  estimator = NULL,
  na_rm = TRUE,
  case_weights = NULL,
  event_level = yardstick_event_level(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bal_accuracy_+3A_data">data</code></td>
<td>
<p>Either a <code>data.frame</code> containing the columns specified by the
<code>truth</code> and <code>estimate</code> arguments, or a <code>table</code>/<code>matrix</code> where the true
class results should be in the columns of the table.</p>
</td></tr>
<tr><td><code id="bal_accuracy_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="bal_accuracy_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a <code>factor</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="bal_accuracy_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predicted class
results (that is also <code>factor</code>). As with <code>truth</code> this can be
specified different ways but the primary method is to use an
unquoted variable name. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="bal_accuracy_+3A_estimator">estimator</code></td>
<td>
<p>One of: <code>"binary"</code>, <code>"macro"</code>, <code>"macro_weighted"</code>,
or <code>"micro"</code> to specify the type of averaging to be done. <code>"binary"</code> is
only relevant for the two class case. The other three are general methods
for calculating multiclass metrics. The default will automatically choose
<code>"binary"</code> or <code>"macro"</code> based on <code>estimate</code>.</p>
</td></tr>
<tr><td><code id="bal_accuracy_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="bal_accuracy_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights.
This should be an unquoted column name that evaluates to a numeric column
in <code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
<tr><td><code id="bal_accuracy_+3A_event_level">event_level</code></td>
<td>
<p>A single string. Either <code>"first"</code> or <code>"second"</code> to specify
which level of <code>truth</code> to consider as the &quot;event&quot;. This argument is only
applicable when <code>estimator = "binary"</code>. The default uses an internal helper
that defaults to <code>"first"</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>bal_accuracy_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Relevant Level</h3>

<p>There is no common convention on which factor level should
automatically be considered the &quot;event&quot; or &quot;positive&quot; result
when computing binary classification metrics. In <code>yardstick</code>, the default
is to use the <em>first</em> level. To alter this, change the argument
<code>event_level</code> to <code>"second"</code> to consider the <em>last</em> level of the factor the
level of interest. For multiclass extensions involving one-vs-all
comparisons (such as macro averaging), this option is ignored and
the &quot;one&quot; level is always the relevant result.
</p>


<h3>Multiclass</h3>

<p>Macro, micro, and macro-weighted averaging is available for this metric.
The default is to select macro averaging if a <code>truth</code> factor with more
than 2 levels is provided. Otherwise, a standard binary calculation is done.
See <code>vignette("multiclass", "yardstick")</code> for more information.
</p>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>See Also</h3>

<p>Other class metrics: 
<code><a href="#topic+accuracy">accuracy</a>()</code>,
<code><a href="#topic+detection_prevalence">detection_prevalence</a>()</code>,
<code><a href="#topic+f_meas">f_meas</a>()</code>,
<code><a href="#topic+j_index">j_index</a>()</code>,
<code><a href="#topic+kap">kap</a>()</code>,
<code><a href="#topic+mcc">mcc</a>()</code>,
<code><a href="#topic+npv">npv</a>()</code>,
<code><a href="#topic+ppv">ppv</a>()</code>,
<code><a href="#topic+precision">precision</a>()</code>,
<code><a href="#topic+recall">recall</a>()</code>,
<code><a href="#topic+sens">sens</a>()</code>,
<code><a href="#topic+spec">spec</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Two class
data("two_class_example")
bal_accuracy(two_class_example, truth, predicted)

# Multiclass
library(dplyr)
data(hpc_cv)

hpc_cv %&gt;%
  filter(Resample == "Fold01") %&gt;%
  bal_accuracy(obs, pred)

# Groups are respected
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  bal_accuracy(obs, pred)

# Weighted macro averaging
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  bal_accuracy(obs, pred, estimator = "macro_weighted")

# Vector version
bal_accuracy_vec(
  two_class_example$truth,
  two_class_example$predicted
)

# Making Class2 the "relevant" level
bal_accuracy_vec(
  two_class_example$truth,
  two_class_example$predicted,
  event_level = "second"
)
</code></pre>

<hr>
<h2 id='brier_class'>Brier score for classification models</h2><span id='topic+brier_class'></span><span id='topic+brier_class.data.frame'></span><span id='topic+brier_class_vec'></span>

<h3>Description</h3>

<p>Compute the Brier score for a classification model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>brier_class(data, ...)

## S3 method for class 'data.frame'
brier_class(data, truth, ..., na_rm = TRUE, case_weights = NULL)

brier_class_vec(truth, estimate, na_rm = TRUE, case_weights = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="brier_class_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by <code>truth</code> and
<code>...</code>.</p>
</td></tr>
<tr><td><code id="brier_class_+3A_...">...</code></td>
<td>
<p>A set of unquoted column names or one or more
<code>dplyr</code> selector functions to choose which variables contain the
class probabilities. If <code>truth</code> is binary, only 1 column should be selected,
and it should correspond to the value of <code>event_level</code>. Otherwise, there
should be as many columns as factor levels of <code>truth</code> and the ordering of
the columns should be the same as the factor levels of <code>truth</code>.</p>
</td></tr>
<tr><td><code id="brier_class_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a <code>factor</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="brier_class_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="brier_class_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights.
This should be an unquoted column name that evaluates to a numeric column
in <code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
<tr><td><code id="brier_class_+3A_estimate">estimate</code></td>
<td>
<p>If <code>truth</code> is binary, a numeric vector of class probabilities
corresponding to the &quot;relevant&quot; class. Otherwise, a matrix with as many
columns as factor levels of <code>truth</code>. <em>It is assumed that these are in the
same order as the levels of <code>truth</code>.</em></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Brier score is analogous to the mean squared error in regression models.
The difference between a binary indicator for a class and its corresponding
class probability are squared and averaged.
</p>
<p>This function uses the convention in Kruppa <em>et al</em> (2014) and divides the
result by two.
</p>
<p>Smaller values of the score are associated with better model performance.
</p>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>brier_class_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Multiclass</h3>

<p>Brier scores can be computed in the same way for any number of classes.
Because of this, no averaging types are supported.
</p>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>References</h3>

<p>Kruppa, J., Liu, Y., Diener, H.-C., Holste, T., Weimar, C.,
Koonig, I. R., and Ziegler, A. (2014) Probability estimation with machine
learning methods for dichotomous and multicategory outcome: Applications.
Biometrical Journal, 56 (4): 564-583.
</p>


<h3>See Also</h3>

<p>Other class probability metrics: 
<code><a href="#topic+average_precision">average_precision</a>()</code>,
<code><a href="#topic+classification_cost">classification_cost</a>()</code>,
<code><a href="#topic+gain_capture">gain_capture</a>()</code>,
<code><a href="#topic+mn_log_loss">mn_log_loss</a>()</code>,
<code><a href="#topic+pr_auc">pr_auc</a>()</code>,
<code><a href="#topic+roc_auc">roc_auc</a>()</code>,
<code><a href="#topic+roc_aunp">roc_aunp</a>()</code>,
<code><a href="#topic+roc_aunu">roc_aunu</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Two class
data("two_class_example")
brier_class(two_class_example, truth, Class1)

# Multiclass
library(dplyr)
data(hpc_cv)

# You can use the col1:colN tidyselect syntax
hpc_cv %&gt;%
  filter(Resample == "Fold01") %&gt;%
  brier_class(obs, VF:L)

# Groups are respected
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  brier_class(obs, VF:L)

</code></pre>

<hr>
<h2 id='brier_survival'>Time-Dependent Brier score for right censored data</h2><span id='topic+brier_survival'></span><span id='topic+brier_survival.data.frame'></span><span id='topic+brier_survival_vec'></span>

<h3>Description</h3>

<p>Compute the time-dependent Brier score for right censored data, which is the
mean squared error at time point <code>.eval_time</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>brier_survival(data, ...)

## S3 method for class 'data.frame'
brier_survival(data, truth, ..., na_rm = TRUE, case_weights = NULL)

brier_survival_vec(truth, estimate, na_rm = TRUE, case_weights = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="brier_survival_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by <code>truth</code> and
<code>...</code>.</p>
</td></tr>
<tr><td><code id="brier_survival_+3A_...">...</code></td>
<td>
<p>The column identifier for the survival probabilities this
should be a list column of data.frames corresponding to the output given when
predicting with <a href="https://censored.tidymodels.org/">censored</a> model. This
should be an unquoted column name although this argument is passed by
expression and supports <a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can
unquote column names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, the dots are not used.</p>
</td></tr>
<tr><td><code id="brier_survival_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true survival result (that
is created using <code><a href="survival.html#topic+Surv">survival::Surv()</a></code>.). This should be an unquoted column name
although this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column names). For
<code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, an <code><a href="survival.html#topic+Surv">survival::Surv()</a></code> object.</p>
</td></tr>
<tr><td><code id="brier_survival_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="brier_survival_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights.
This should be an unquoted column name that evaluates to a numeric column
in <code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
<tr><td><code id="brier_survival_+3A_estimate">estimate</code></td>
<td>
<p>A list column of data.frames corresponding to the output
given when predicting with <a href="https://censored.tidymodels.org/">censored</a>
model. See the details for more information regarding format.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This formulation takes survival probability predictions at one or more
specific <em>evaluation times</em> and, for each time, computes the Brier score. To
account for censoring, inverse probability of censoring weights (IPCW) are
used in the calculations.
</p>
<p>The column passed to <code>...</code> should be a list column with one element per
independent experiential unit (e.g. patient). The list column should contain
data frames with several columns:
</p>

<ul>
<li> <p><code>.eval_time</code>: The time that the prediction is made.
</p>
</li>
<li> <p><code>.pred_survival</code>: The predicted probability of survival up to <code>.eval_time</code>
</p>
</li>
<li> <p><code>.weight_censored</code>: The case weight for the inverse probability of censoring.
</p>
</li></ul>

<p>The last column can be produced using <code><a href="parsnip.html#topic+censoring_weights">parsnip::.censoring_weights_graf()</a></code>.
This corresponds to the weighting scheme of  Graf <em>et al</em> (1999). The
internal data set <code>lung_surv</code> shows an example of the format.
</p>
<p>This method automatically groups by the <code>.eval_time</code> argument.
</p>
<p>Smaller values of the score are associated with better model performance.
</p>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>, and <code>.estimate</code>.
</p>
<p>For an ungrouped data frame, the result has one row of values. For a grouped data frame,
the number of rows returned is the same as the number of groups.
</p>
<p>For <code>brier_survival_vec()</code>, a <code>numeric</code> vector same length as the input argument
<code>eval_time</code>. (or <code>NA</code>).
</p>


<h3>Author(s)</h3>

<p>Emil Hvitfeldt
</p>


<h3>References</h3>

<p>E. Graf, C. Schmoor, W. Sauerbrei, and M. Schumacher, “Assessment and
comparison of prognostic classification schemes for survival data,”
<em>Statistics in Medicine</em>, vol. 18, no. 17-18, pp. 2529–2545, 1999.
</p>


<h3>See Also</h3>

<p>Other dynamic survival metrics: 
<code><a href="#topic+brier_survival_integrated">brier_survival_integrated</a>()</code>,
<code><a href="#topic+roc_auc_survival">roc_auc_survival</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)

lung_surv %&gt;%
  brier_survival(
    truth = surv_obj,
    .pred
  )
</code></pre>

<hr>
<h2 id='brier_survival_integrated'>Integrated Brier score for right censored data</h2><span id='topic+brier_survival_integrated'></span><span id='topic+brier_survival_integrated.data.frame'></span><span id='topic+brier_survival_integrated_vec'></span>

<h3>Description</h3>

<p>Compute the integrated Brier score for right censored data, which is an
overall calculation of model performance for all values of <code>.eval_time</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>brier_survival_integrated(data, ...)

## S3 method for class 'data.frame'
brier_survival_integrated(data, truth, ..., na_rm = TRUE, case_weights = NULL)

brier_survival_integrated_vec(
  truth,
  estimate,
  na_rm = TRUE,
  case_weights = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="brier_survival_integrated_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by <code>truth</code> and
<code>...</code>.</p>
</td></tr>
<tr><td><code id="brier_survival_integrated_+3A_...">...</code></td>
<td>
<p>The column identifier for the survival probabilities this
should be a list column of data.frames corresponding to the output given when
predicting with <a href="https://censored.tidymodels.org/">censored</a> model. This
should be an unquoted column name although this argument is passed by
expression and supports <a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can
unquote column names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, the dots are not used.</p>
</td></tr>
<tr><td><code id="brier_survival_integrated_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true survival result (that
is created using <code><a href="survival.html#topic+Surv">survival::Surv()</a></code>.). This should be an unquoted column name
although this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column names). For
<code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, an <code><a href="survival.html#topic+Surv">survival::Surv()</a></code> object.</p>
</td></tr>
<tr><td><code id="brier_survival_integrated_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="brier_survival_integrated_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights.
This should be an unquoted column name that evaluates to a numeric column
in <code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
<tr><td><code id="brier_survival_integrated_+3A_estimate">estimate</code></td>
<td>
<p>A list column of data.frames corresponding to the output
given when predicting with <a href="https://censored.tidymodels.org/">censored</a>
model. See the details for more information regarding format.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The integrated time-dependent brier score is calculated in an &quot;area under the
curve&quot; fashion. The brier score is calculated for each value of <code>.eval_time</code>.
The area is calculated via the trapezoidal rule. The area is divided by the
largest value of <code>.eval_time</code> to bring it into the same scale as the
traditional brier score.
</p>
<p>Smaller values of the score are associated with better model performance.
</p>
<p>This formulation takes survival probability predictions at one or more
specific <em>evaluation times</em> and, for each time, computes the Brier score.
To account for censoring, inverse probability of censoring weights
(IPCW) are used in the calculations.
</p>
<p>The column passed to <code>...</code> should be a list column with one element per
independent experiential unit (e.g. patient). The list column should contain
data frames with several columns:
</p>

<ul>
<li> <p><code>.eval_time</code>: The time that the prediction is made.
</p>
</li>
<li> <p><code>.pred_survival</code>: The predicted probability of survival up to <code>.eval_time</code>
</p>
</li>
<li> <p><code>.weight_censored</code>: The case weight for the inverse probability of censoring.
</p>
</li></ul>

<p>The last column can be produced using <code><a href="parsnip.html#topic+censoring_weights">parsnip::.censoring_weights_graf()</a></code>.
This corresponds to the weighting scheme of  Graf <em>et al</em> (1999). The
internal data set <code>lung_surv</code> shows an example of the format.
</p>
<p>This method automatically groups by the <code>.eval_time</code> argument.
</p>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>, and <code>.estimate</code>.
</p>
<p>For an ungrouped data frame, the result has one row of values. For a grouped data frame,
the number of rows returned is the same as the number of groups.
</p>
<p>For <code>brier_survival_integrated_vec()</code>, a <code>numeric</code> vector same length as the input argument
<code>eval_time</code>. (or <code>NA</code>).
</p>


<h3>Author(s)</h3>

<p>Emil Hvitfeldt
</p>


<h3>References</h3>

<p>E. Graf, C. Schmoor, W. Sauerbrei, and M. Schumacher, “Assessment
and comparison of prognostic classification schemes for survival data,”
Statistics in Medicine, vol. 18, no. 17-18, pp. 2529–2545, 1999.
</p>


<h3>See Also</h3>

<p>Other dynamic survival metrics: 
<code><a href="#topic+brier_survival">brier_survival</a>()</code>,
<code><a href="#topic+roc_auc_survival">roc_auc_survival</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)

lung_surv %&gt;%
  brier_survival_integrated(
    truth = surv_obj,
    .pred
  )
</code></pre>

<hr>
<h2 id='ccc'>Concordance correlation coefficient</h2><span id='topic+ccc'></span><span id='topic+ccc.data.frame'></span><span id='topic+ccc_vec'></span>

<h3>Description</h3>

<p>Calculate the concordance correlation coefficient.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ccc(data, ...)

## S3 method for class 'data.frame'
ccc(
  data,
  truth,
  estimate,
  bias = FALSE,
  na_rm = TRUE,
  case_weights = NULL,
  ...
)

ccc_vec(truth, estimate, bias = FALSE, na_rm = TRUE, case_weights = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ccc_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by the <code>truth</code>
and <code>estimate</code> arguments.</p>
</td></tr>
<tr><td><code id="ccc_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="ccc_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true results
(that is <code>numeric</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>numeric</code> vector.</p>
</td></tr>
<tr><td><code id="ccc_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predicted
results (that is also <code>numeric</code>). As with <code>truth</code> this can be
specified different ways but the primary method is to use an
unquoted variable name. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>numeric</code> vector.</p>
</td></tr>
<tr><td><code id="ccc_+3A_bias">bias</code></td>
<td>
<p>A <code>logical</code>; should the biased estimate of variance
be used (as is Lin (1989))?</p>
</td></tr>
<tr><td><code id="ccc_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="ccc_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights. This
should be an unquoted column name that evaluates to a numeric column in
<code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code><a href="#topic+ccc">ccc()</a></code> is a metric of both consistency/correlation and accuracy,
while metrics such as <code><a href="#topic+rmse">rmse()</a></code> are strictly for accuracy and metrics
such as <code><a href="#topic+rsq">rsq()</a></code> are strictly for consistency/correlation
</p>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>ccc_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>References</h3>

<p>Lin, L. (1989). A concordance correlation
coefficient to evaluate reproducibility. <em>Biometrics</em>, 45 (1),
255-268.
</p>
<p>Nickerson, C. (1997). A note on &quot;A concordance correlation
coefficient to evaluate reproducibility&quot;. <em>Biometrics</em>, 53(4),
1503-1507.
</p>


<h3>See Also</h3>

<p>Other numeric metrics: 
<code><a href="#topic+huber_loss_pseudo">huber_loss_pseudo</a>()</code>,
<code><a href="#topic+huber_loss">huber_loss</a>()</code>,
<code><a href="#topic+iic">iic</a>()</code>,
<code><a href="#topic+mae">mae</a>()</code>,
<code><a href="#topic+mape">mape</a>()</code>,
<code><a href="#topic+mase">mase</a>()</code>,
<code><a href="#topic+mpe">mpe</a>()</code>,
<code><a href="#topic+msd">msd</a>()</code>,
<code><a href="#topic+poisson_log_loss">poisson_log_loss</a>()</code>,
<code><a href="#topic+rmse">rmse</a>()</code>,
<code><a href="#topic+rpd">rpd</a>()</code>,
<code><a href="#topic+rpiq">rpiq</a>()</code>,
<code><a href="#topic+rsq_trad">rsq_trad</a>()</code>,
<code><a href="#topic+rsq">rsq</a>()</code>,
<code><a href="#topic+smape">smape</a>()</code>
</p>
<p>Other consistency metrics: 
<code><a href="#topic+rpd">rpd</a>()</code>,
<code><a href="#topic+rpiq">rpiq</a>()</code>,
<code><a href="#topic+rsq_trad">rsq_trad</a>()</code>,
<code><a href="#topic+rsq">rsq</a>()</code>
</p>
<p>Other accuracy metrics: 
<code><a href="#topic+huber_loss_pseudo">huber_loss_pseudo</a>()</code>,
<code><a href="#topic+huber_loss">huber_loss</a>()</code>,
<code><a href="#topic+iic">iic</a>()</code>,
<code><a href="#topic+mae">mae</a>()</code>,
<code><a href="#topic+mape">mape</a>()</code>,
<code><a href="#topic+mase">mase</a>()</code>,
<code><a href="#topic+mpe">mpe</a>()</code>,
<code><a href="#topic+msd">msd</a>()</code>,
<code><a href="#topic+poisson_log_loss">poisson_log_loss</a>()</code>,
<code><a href="#topic+rmse">rmse</a>()</code>,
<code><a href="#topic+smape">smape</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Supply truth and predictions as bare column names
ccc(solubility_test, solubility, prediction)

library(dplyr)

set.seed(1234)
size &lt;- 100
times &lt;- 10

# create 10 resamples
solubility_resampled &lt;- bind_rows(
  replicate(
    n = times,
    expr = sample_n(solubility_test, size, replace = TRUE),
    simplify = FALSE
  ),
  .id = "resample"
)

# Compute the metric by group
metric_results &lt;- solubility_resampled %&gt;%
  group_by(resample) %&gt;%
  ccc(solubility, prediction)

metric_results

# Resampled mean estimate
metric_results %&gt;%
  summarise(avg_estimate = mean(.estimate))
</code></pre>

<hr>
<h2 id='check_metric'>Developer function for checking inputs in new metrics</h2><span id='topic+check_metric'></span><span id='topic+check_numeric_metric'></span><span id='topic+check_class_metric'></span><span id='topic+check_prob_metric'></span><span id='topic+check_dynamic_survival_metric'></span><span id='topic+check_static_survival_metric'></span>

<h3>Description</h3>

<p><code>check_numeric_metric()</code>, <code>check_class_metric()</code>, and <code>check_prob_metric()</code>
are useful alongside <a href="#topic+metric-summarizers">metric-summarizers</a> for implementing new custom
metrics. <a href="#topic+metric-summarizers">metric-summarizers</a> call the metric function inside
<code>dplyr::summarise()</code>. These functions perform checks on the inputs in
accordance with the type of metric that is used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_numeric_metric(truth, estimate, case_weights, call = caller_env())

check_class_metric(
  truth,
  estimate,
  case_weights,
  estimator,
  call = caller_env()
)

check_prob_metric(
  truth,
  estimate,
  case_weights,
  estimator,
  call = caller_env()
)

check_dynamic_survival_metric(
  truth,
  estimate,
  case_weights,
  call = caller_env()
)

check_static_survival_metric(
  truth,
  estimate,
  case_weights,
  call = caller_env()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_metric_+3A_truth">truth</code></td>
<td>
<p>The realized vector of <code>truth</code>.
</p>

<ul>
<li><p> For <code>check_numeric_metric()</code>, a numeric vector.
</p>
</li>
<li><p> For <code>check_class_metric()</code>, a factor.
</p>
</li>
<li><p> For <code>check_prob_metric()</code>, a factor.
</p>
</li>
<li><p> For <code>check_dynamic_survival_metric()</code>, a Surv object.
</p>
</li>
<li><p> For <code>check_static_survival_metric()</code>, a Surv object.
</p>
</li></ul>
</td></tr>
<tr><td><code id="check_metric_+3A_estimate">estimate</code></td>
<td>
<p>The realized <code>estimate</code> result.
</p>

<ul>
<li><p> For <code>check_numeric_metric()</code>, a numeric vector.
</p>
</li>
<li><p> For <code>check_class_metric()</code>, a factor.
</p>
</li>
<li><p> For <code>check_prob_metric()</code>, a numeric vector for binary <code>truth</code>,
a numeric matrix for multic-class <code>truth</code>.
</p>
</li>
<li><p> For <code>check_dynamic_survival_metric()</code>, list-column of data.frames.
</p>
</li>
<li><p> For <code>check_static_survival_metric()</code>, a numeric vector.
</p>
</li></ul>
</td></tr>
<tr><td><code id="check_metric_+3A_case_weights">case_weights</code></td>
<td>
<p>The realized case weights, as a numeric vector. This must
be the same length as <code>truth</code>.</p>
</td></tr>
<tr><td><code id="check_metric_+3A_call">call</code></td>
<td>
<p>The execution environment of a currently
running function, e.g. <code>caller_env()</code>. The function will be
mentioned in error messages as the source of the error. See the
<code>call</code> argument of <code><a href="rlang.html#topic+abort">abort()</a></code> for more information.</p>
</td></tr>
<tr><td><code id="check_metric_+3A_estimator">estimator</code></td>
<td>
<p>This can either be <code>NULL</code> for the default auto-selection of
averaging (<code>"binary"</code> or <code>"macro"</code>), or a single character to pass along to
the metric implementation describing the kind of averaging to use.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><a href="#topic+metric-summarizers">metric-summarizers</a>
</p>

<hr>
<h2 id='classification_cost'>Costs function for poor classification</h2><span id='topic+classification_cost'></span><span id='topic+classification_cost.data.frame'></span><span id='topic+classification_cost_vec'></span>

<h3>Description</h3>

<p><code>classification_cost()</code> calculates the cost of a poor prediction based on
user-defined costs. The costs are multiplied by the estimated class
probabilities and the mean cost is returned.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>classification_cost(data, ...)

## S3 method for class 'data.frame'
classification_cost(
  data,
  truth,
  ...,
  costs = NULL,
  na_rm = TRUE,
  event_level = yardstick_event_level(),
  case_weights = NULL
)

classification_cost_vec(
  truth,
  estimate,
  costs = NULL,
  na_rm = TRUE,
  event_level = yardstick_event_level(),
  case_weights = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="classification_cost_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by <code>truth</code> and
<code>...</code>.</p>
</td></tr>
<tr><td><code id="classification_cost_+3A_...">...</code></td>
<td>
<p>A set of unquoted column names or one or more
<code>dplyr</code> selector functions to choose which variables contain the
class probabilities. If <code>truth</code> is binary, only 1 column should be selected,
and it should correspond to the value of <code>event_level</code>. Otherwise, there
should be as many columns as factor levels of <code>truth</code> and the ordering of
the columns should be the same as the factor levels of <code>truth</code>.</p>
</td></tr>
<tr><td><code id="classification_cost_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a <code>factor</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="classification_cost_+3A_costs">costs</code></td>
<td>
<p>A data frame with columns <code>"truth"</code>, <code>"estimate"</code>, and <code>"cost"</code>.
</p>
<p><code>"truth"</code> and <code>"estimate"</code> should be character columns containing unique
combinations of the levels of the <code>truth</code> factor.
</p>
<p><code>"costs"</code> should be a numeric column representing the cost that should
be applied when the <code>"estimate"</code> is predicted, but the true result is
<code>"truth"</code>.
</p>
<p>It is often the case that when <code>"truth" == "estimate"</code>, the cost is zero
(no penalty for correct predictions).
</p>
<p>If any combinations of the levels of <code>truth</code> are missing, their costs are
assumed to be zero.
</p>
<p>If <code>NULL</code>, equal costs are used, applying a cost of <code>0</code> to correct
predictions, and a cost of <code>1</code> to incorrect predictions.</p>
</td></tr>
<tr><td><code id="classification_cost_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="classification_cost_+3A_event_level">event_level</code></td>
<td>
<p>A single string. Either <code>"first"</code> or <code>"second"</code> to specify
which level of <code>truth</code> to consider as the &quot;event&quot;. This argument is only
applicable when <code>estimator = "binary"</code>. The default uses an internal helper
that defaults to <code>"first"</code>.</p>
</td></tr>
<tr><td><code id="classification_cost_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights.
This should be an unquoted column name that evaluates to a numeric column
in <code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
<tr><td><code id="classification_cost_+3A_estimate">estimate</code></td>
<td>
<p>If <code>truth</code> is binary, a numeric vector of class probabilities
corresponding to the &quot;relevant&quot; class. Otherwise, a matrix with as many
columns as factor levels of <code>truth</code>. <em>It is assumed that these are in the
same order as the levels of <code>truth</code>.</em></p>
</td></tr>
</table>


<h3>Details</h3>

<p>As an example, suppose that there are three classes: <code>"A"</code>, <code>"B"</code>, and <code>"C"</code>.
Suppose there is a truly <code>"A"</code> observation with class probabilities <code>A = 0.3 / B = 0.3 / C = 0.4</code>. Suppose that, when the true result is class <code>"A"</code>, the
costs for each class were <code>A = 0 / B = 5 / C = 10</code>, penalizing the
probability of incorrectly predicting <code>"C"</code> more than predicting <code>"B"</code>. The
cost for this prediction would be <code>0.3 * 0 + 0.3 * 5 + 0.4 * 10</code>. This
calculation is done for each sample and the individual costs are averaged.
</p>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>class_cost_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>See Also</h3>

<p>Other class probability metrics: 
<code><a href="#topic+average_precision">average_precision</a>()</code>,
<code><a href="#topic+brier_class">brier_class</a>()</code>,
<code><a href="#topic+gain_capture">gain_capture</a>()</code>,
<code><a href="#topic+mn_log_loss">mn_log_loss</a>()</code>,
<code><a href="#topic+pr_auc">pr_auc</a>()</code>,
<code><a href="#topic+roc_auc">roc_auc</a>()</code>,
<code><a href="#topic+roc_aunp">roc_aunp</a>()</code>,
<code><a href="#topic+roc_aunu">roc_aunu</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)

# ---------------------------------------------------------------------------
# Two class example
data(two_class_example)

# Assuming `Class1` is our "event", this penalizes false positives heavily
costs1 &lt;- tribble(
  ~truth,   ~estimate, ~cost,
  "Class1", "Class2",  1,
  "Class2", "Class1",  2
)

# Assuming `Class1` is our "event", this penalizes false negatives heavily
costs2 &lt;- tribble(
  ~truth,   ~estimate, ~cost,
  "Class1", "Class2",  2,
  "Class2", "Class1",  1
)

classification_cost(two_class_example, truth, Class1, costs = costs1)

classification_cost(two_class_example, truth, Class1, costs = costs2)

# ---------------------------------------------------------------------------
# Multiclass
data(hpc_cv)

# Define cost matrix from Kuhn and Johnson (2013)
hpc_costs &lt;- tribble(
  ~estimate, ~truth, ~cost,
  "VF",      "VF",    0,
  "VF",      "F",     1,
  "VF",      "M",     5,
  "VF",      "L",    10,
  "F",       "VF",    1,
  "F",       "F",     0,
  "F",       "M",     5,
  "F",       "L",     5,
  "M",       "VF",    1,
  "M",       "F",     1,
  "M",       "M",     0,
  "M",       "L",     1,
  "L",       "VF",    1,
  "L",       "F",     1,
  "L",       "M",     1,
  "L",       "L",     0
)

# You can use the col1:colN tidyselect syntax
hpc_cv %&gt;%
  filter(Resample == "Fold01") %&gt;%
  classification_cost(obs, VF:L, costs = hpc_costs)

# Groups are respected
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  classification_cost(obs, VF:L, costs = hpc_costs)
</code></pre>

<hr>
<h2 id='concordance_survival'>Concordance index for right-censored data</h2><span id='topic+concordance_survival'></span><span id='topic+concordance_survival.data.frame'></span><span id='topic+concordance_survival_vec'></span>

<h3>Description</h3>

<p>Compute the Concordance index for right-censored data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>concordance_survival(data, ...)

## S3 method for class 'data.frame'
concordance_survival(
  data,
  truth,
  estimate,
  na_rm = TRUE,
  case_weights = NULL,
  ...
)

concordance_survival_vec(
  truth,
  estimate,
  na_rm = TRUE,
  case_weights = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="concordance_survival_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by <code>truth</code> and
<code>...</code>.</p>
</td></tr>
<tr><td><code id="concordance_survival_+3A_...">...</code></td>
<td>
<p>Currently not used.</p>
</td></tr>
<tr><td><code id="concordance_survival_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true survival result (that
is created using <code><a href="survival.html#topic+Surv">survival::Surv()</a></code>.). This should be an unquoted column name
although this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column names). For
<code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, an <code><a href="survival.html#topic+Surv">survival::Surv()</a></code> object.</p>
</td></tr>
<tr><td><code id="concordance_survival_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predicted time, this should be
a numeric variables. This should be an unquoted column name although this
argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column names). For
<code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector.</p>
</td></tr>
<tr><td><code id="concordance_survival_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="concordance_survival_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights.
This should be an unquoted column name that evaluates to a numeric column
in <code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The concordance index is defined as the proportion of all comparable pairs in
which the predictions and outcomes are concordant.
</p>
<p>Two observations are comparable if:
</p>

<ol>
<li><p> both of the observations experienced an event (at different times), or
</p>
</li>
<li><p> the observation with the shorter observed survival time experienced an
event, in which case the event-free subject “outlived” the other.
</p>
</li></ol>

<p>A pair is not comparable if they experienced events at the same time.
</p>
<p>Concordance intuitively means that two samples were ordered correctly by the
model. More specifically, two samples are concordant, if the one with a
higher estimated risk score has a shorter actual survival time.
</p>
<p>Larger values of the score are associated with better model performance.
</p>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>concordance_survival_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Author(s)</h3>

<p>Emil Hvitfeldt
</p>


<h3>References</h3>

<p>Harrell, F.E., Califf, R.M., Pryor, D.B., Lee, K.L., Rosati, R.A,
“Multivariable prognostic models: issues in developing models, evaluating
assumptions and adequacy, and measuring and reducing errors”, Statistics in
Medicine, 15(4), 361-87, 1996.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>concordance_survival(
  data = lung_surv,
  truth = surv_obj,
  estimate = .pred_time
)
</code></pre>

<hr>
<h2 id='conf_mat'>Confusion Matrix for Categorical Data</h2><span id='topic+conf_mat'></span><span id='topic+conf_mat.table'></span><span id='topic+conf_mat.default'></span><span id='topic+conf_mat.data.frame'></span><span id='topic+tidy.conf_mat'></span>

<h3>Description</h3>

<p>Calculates a cross-tabulation of observed and predicted classes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>conf_mat(data, ...)

## S3 method for class 'data.frame'
conf_mat(
  data,
  truth,
  estimate,
  dnn = c("Prediction", "Truth"),
  case_weights = NULL,
  ...
)

## S3 method for class 'conf_mat'
tidy(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="conf_mat_+3A_data">data</code></td>
<td>
<p>A data frame or a <code><a href="base.html#topic+table">base::table()</a></code>.</p>
</td></tr>
<tr><td><code id="conf_mat_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
<tr><td><code id="conf_mat_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a <code>factor</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="conf_mat_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predicted class
results (that is also <code>factor</code>). As with <code>truth</code> this can be
specified different ways but the primary method is to use an
unquoted variable name. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="conf_mat_+3A_dnn">dnn</code></td>
<td>
<p>A character vector of dimnames for the table.</p>
</td></tr>
<tr><td><code id="conf_mat_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights.
This should be an unquoted column name that evaluates to a numeric column
in <code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
<tr><td><code id="conf_mat_+3A_x">x</code></td>
<td>
<p>A <code>conf_mat</code> object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code><a href="#topic+conf_mat">conf_mat()</a></code> objects, a <code>broom</code> <code>tidy()</code> method has been created
that collapses the cell counts by cell into a data frame for
easy manipulation.
</p>
<p>There is also a <code>summary()</code> method that computes various classification
metrics at once. See <code><a href="#topic+summary.conf_mat">summary.conf_mat()</a></code>
</p>
<p>There is a <code><a href="ggplot2.html#topic+autoplot">ggplot2::autoplot()</a></code>
method for quickly visualizing the matrix. Both a heatmap and mosaic type
is implemented.
</p>
<p>The function requires that the factors have exactly the same levels.
</p>


<h3>Value</h3>

<p><code>conf_mat()</code> produces an object with class <code>conf_mat</code>. This contains the
table and other objects. <code>tidy.conf_mat()</code> generates a tibble with columns
<code>name</code> (the cell identifier) and <code>value</code> (the cell count).
</p>
<p>When used on a grouped data frame, <code>conf_mat()</code> returns a tibble containing
columns for the groups along with <code>conf_mat</code>, a list-column
where each element is a <code>conf_mat</code> object.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+summary.conf_mat">summary.conf_mat()</a></code> for computing a large number of metrics from one
confusion matrix.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)
data("hpc_cv")

# The confusion matrix from a single assessment set (i.e. fold)
cm &lt;- hpc_cv %&gt;%
  filter(Resample == "Fold01") %&gt;%
  conf_mat(obs, pred)
cm

# Now compute the average confusion matrix across all folds in
# terms of the proportion of the data contained in each cell.
# First get the raw cell counts per fold using the `tidy` method
library(tidyr)

cells_per_resample &lt;- hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  conf_mat(obs, pred) %&gt;%
  mutate(tidied = lapply(conf_mat, tidy)) %&gt;%
  unnest(tidied)

# Get the totals per resample
counts_per_resample &lt;- hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  summarize(total = n()) %&gt;%
  left_join(cells_per_resample, by = "Resample") %&gt;%
  # Compute the proportions
  mutate(prop = value / total) %&gt;%
  group_by(name) %&gt;%
  # Average
  summarize(prop = mean(prop))

counts_per_resample

# Now reshape these into a matrix
mean_cmat &lt;- matrix(counts_per_resample$prop, byrow = TRUE, ncol = 4)
rownames(mean_cmat) &lt;- levels(hpc_cv$obs)
colnames(mean_cmat) &lt;- levels(hpc_cv$obs)

round(mean_cmat, 3)

# The confusion matrix can quickly be visualized using autoplot()
library(ggplot2)

autoplot(cm, type = "mosaic")
autoplot(cm, type = "heatmap")
</code></pre>

<hr>
<h2 id='demographic_parity'>Demographic parity</h2><span id='topic+demographic_parity'></span>

<h3>Description</h3>

<p>Demographic parity is satisfied when a model's predictions have the
same predicted positive rate across groups. A value of 0 indicates parity
across groups. Note that this definition does not depend on the true
outcome; the <code>truth</code> argument is included in outputted metrics
for consistency.
</p>
<p><code>demographic_parity()</code> is calculated as the difference between the largest
and smallest value of <code><a href="#topic+detection_prevalence">detection_prevalence()</a></code> across groups.
</p>
<p>Demographic parity is sometimes referred to as group fairness,
disparate impact, or statistical parity.
</p>
<p>See the &quot;Measuring Disparity&quot; section for details on implementation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>demographic_parity(by)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="demographic_parity_+3A_by">by</code></td>
<td>
<p>The column identifier for the sensitive feature. This should be an
unquoted column name referring to a column in the un-preprocessed data.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function outputs a yardstick <em>fairness metric</em> function. Given a
grouping variable <code>by</code>, <code>demographic_parity()</code> will return a yardstick metric
function that is associated with the data-variable grouping <code>by</code> and a
post-processor. The outputted function will first generate a set
of detection_prevalence metric values by group before summarizing across
groups using the post-processing function.
</p>
<p>The outputted function only has a data frame method and is intended to
be used as part of a metric set.
</p>


<h3>Measuring Disparity</h3>

<p>By default, this function takes the difference in range of detection_prevalence
<code>.estimate</code>s across groups. That is, the maximum pair-wise disparity between
groups is the return value of <code>demographic_parity()</code>'s <code>.estimate</code>.
</p>
<p>For finer control of group treatment, construct a context-aware fairness
metric with the <code><a href="#topic+new_groupwise_metric">new_groupwise_metric()</a></code> function by passing a custom <code>aggregate</code>
function:
</p>
<div class="sourceCode"><pre># the actual default `aggregate` is:
diff_range &lt;- function(x, ...) {diff(range(x$.estimate))}

demographic_parity_2 &lt;-
  new_groupwise_metric(
    fn = detection_prevalence,
    name = "demographic_parity_2",
    aggregate = diff_range
  )
</pre></div>
<p>In <code>aggregate()</code>, <code>x</code> is the <code>metric_set()</code> output with detection_prevalence values
for each group, and <code>...</code> gives additional arguments (such as a grouping
level to refer to as the &quot;baseline&quot;) to pass to the function outputted
by <code>demographic_parity_2()</code> for context.
</p>


<h3>References</h3>

<p>Agarwal, A., Beygelzimer, A., Dudik, M., Langford, J., &amp; Wallach, H. (2018).
&quot;A Reductions Approach to Fair Classification.&quot; Proceedings of the 35th
International Conference on Machine Learning, in Proceedings of Machine
Learning Research. 80:60-69.
</p>
<p>Verma, S., &amp; Rubin, J. (2018). &quot;Fairness definitions explained&quot;. In
Proceedings of the international workshop on software fairness (pp. 1-7).
</p>
<p>Bird, S., Dudík, M., Edgar, R., Horn, B., Lutz, R., Milan, V., ... &amp; Walker,
K. (2020). &quot;Fairlearn: A toolkit for assessing and improving fairness in AI&quot;.
Microsoft, Tech. Rep. MSR-TR-2020-32.
</p>


<h3>See Also</h3>

<p>Other fairness metrics: 
<code><a href="#topic+equal_opportunity">equal_opportunity</a>()</code>,
<code><a href="#topic+equalized_odds">equalized_odds</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)

data(hpc_cv)

head(hpc_cv)

# evaluate `demographic_parity()` by Resample
m_set &lt;- metric_set(demographic_parity(Resample))

# use output like any other metric set
hpc_cv %&gt;%
  m_set(truth = obs, estimate = pred)

# can mix fairness metrics and regular metrics
m_set_2 &lt;- metric_set(sens, demographic_parity(Resample))

hpc_cv %&gt;%
  m_set_2(truth = obs, estimate = pred)
</code></pre>

<hr>
<h2 id='detection_prevalence'>Detection prevalence</h2><span id='topic+detection_prevalence'></span><span id='topic+detection_prevalence.data.frame'></span><span id='topic+detection_prevalence_vec'></span>

<h3>Description</h3>

<p>Detection prevalence is defined as the number of <em>predicted</em> positive events (both
true positive and false positive) divided by the total number of predictions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>detection_prevalence(data, ...)

## S3 method for class 'data.frame'
detection_prevalence(
  data,
  truth,
  estimate,
  estimator = NULL,
  na_rm = TRUE,
  case_weights = NULL,
  event_level = yardstick_event_level(),
  ...
)

detection_prevalence_vec(
  truth,
  estimate,
  estimator = NULL,
  na_rm = TRUE,
  case_weights = NULL,
  event_level = yardstick_event_level(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="detection_prevalence_+3A_data">data</code></td>
<td>
<p>Either a <code>data.frame</code> containing the columns specified by the
<code>truth</code> and <code>estimate</code> arguments, or a <code>table</code>/<code>matrix</code> where the true
class results should be in the columns of the table.</p>
</td></tr>
<tr><td><code id="detection_prevalence_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="detection_prevalence_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a <code>factor</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="detection_prevalence_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predicted class
results (that is also <code>factor</code>). As with <code>truth</code> this can be
specified different ways but the primary method is to use an
unquoted variable name. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="detection_prevalence_+3A_estimator">estimator</code></td>
<td>
<p>One of: <code>"binary"</code>, <code>"macro"</code>, <code>"macro_weighted"</code>,
or <code>"micro"</code> to specify the type of averaging to be done. <code>"binary"</code> is
only relevant for the two class case. The other three are general methods
for calculating multiclass metrics. The default will automatically choose
<code>"binary"</code> or <code>"macro"</code> based on <code>estimate</code>.</p>
</td></tr>
<tr><td><code id="detection_prevalence_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="detection_prevalence_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights.
This should be an unquoted column name that evaluates to a numeric column
in <code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
<tr><td><code id="detection_prevalence_+3A_event_level">event_level</code></td>
<td>
<p>A single string. Either <code>"first"</code> or <code>"second"</code> to specify
which level of <code>truth</code> to consider as the &quot;event&quot;. This argument is only
applicable when <code>estimator = "binary"</code>. The default uses an internal helper
that defaults to <code>"first"</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>detection_prevalence_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Relevant Level</h3>

<p>There is no common convention on which factor level should
automatically be considered the &quot;event&quot; or &quot;positive&quot; result
when computing binary classification metrics. In <code>yardstick</code>, the default
is to use the <em>first</em> level. To alter this, change the argument
<code>event_level</code> to <code>"second"</code> to consider the <em>last</em> level of the factor the
level of interest. For multiclass extensions involving one-vs-all
comparisons (such as macro averaging), this option is ignored and
the &quot;one&quot; level is always the relevant result.
</p>


<h3>Multiclass</h3>

<p>Macro, micro, and macro-weighted averaging is available for this metric.
The default is to select macro averaging if a <code>truth</code> factor with more
than 2 levels is provided. Otherwise, a standard binary calculation is done.
See <code>vignette("multiclass", "yardstick")</code> for more information.
</p>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>See Also</h3>

<p>Other class metrics: 
<code><a href="#topic+accuracy">accuracy</a>()</code>,
<code><a href="#topic+bal_accuracy">bal_accuracy</a>()</code>,
<code><a href="#topic+f_meas">f_meas</a>()</code>,
<code><a href="#topic+j_index">j_index</a>()</code>,
<code><a href="#topic+kap">kap</a>()</code>,
<code><a href="#topic+mcc">mcc</a>()</code>,
<code><a href="#topic+npv">npv</a>()</code>,
<code><a href="#topic+ppv">ppv</a>()</code>,
<code><a href="#topic+precision">precision</a>()</code>,
<code><a href="#topic+recall">recall</a>()</code>,
<code><a href="#topic+sens">sens</a>()</code>,
<code><a href="#topic+spec">spec</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Two class
data("two_class_example")
detection_prevalence(two_class_example, truth, predicted)

# Multiclass
library(dplyr)
data(hpc_cv)

hpc_cv %&gt;%
  filter(Resample == "Fold01") %&gt;%
  detection_prevalence(obs, pred)

# Groups are respected
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  detection_prevalence(obs, pred)

# Weighted macro averaging
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  detection_prevalence(obs, pred, estimator = "macro_weighted")

# Vector version
detection_prevalence_vec(
  two_class_example$truth,
  two_class_example$predicted
)

# Making Class2 the "relevant" level
detection_prevalence_vec(
  two_class_example$truth,
  two_class_example$predicted,
  event_level = "second"
)
</code></pre>

<hr>
<h2 id='developer-helpers'>Developer helpers</h2><span id='topic+developer-helpers'></span><span id='topic+dots_to_estimate'></span><span id='topic+get_weights'></span><span id='topic+finalize_estimator'></span><span id='topic+finalize_estimator_internal'></span><span id='topic+validate_estimator'></span>

<h3>Description</h3>

<p>Helpers to be used alongside <a href="#topic+check_metric">check_metric</a>, <a href="#topic+yardstick_remove_missing">yardstick_remove_missing</a> and
<a href="#topic+class_metric_summarizer">metric summarizers</a> when creating new metrics.
See <a href="https://www.tidymodels.org/learn/develop/metrics/">Custom performance metrics</a> for more
information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dots_to_estimate(data, ...)

get_weights(data, estimator)

finalize_estimator(
  x,
  estimator = NULL,
  metric_class = "default",
  call = caller_env()
)

finalize_estimator_internal(
  metric_dispatcher,
  x,
  estimator,
  call = caller_env()
)

validate_estimator(estimator, estimator_override = NULL, call = caller_env())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="developer-helpers_+3A_data">data</code></td>
<td>
<p>A table with truth values as columns and predicted values
as rows.</p>
</td></tr>
<tr><td><code id="developer-helpers_+3A_...">...</code></td>
<td>
<p>A set of unquoted column names or one or more
<code>dplyr</code> selector functions to choose which variables contain the
class probabilities. If <code>truth</code> is binary, only 1 column should be selected,
and it should correspond to the value of <code>event_level</code>. Otherwise, there
should be as many columns as factor levels of <code>truth</code> and the ordering of
the columns should be the same as the factor levels of <code>truth</code>.</p>
</td></tr>
<tr><td><code id="developer-helpers_+3A_estimator">estimator</code></td>
<td>
<p>Either <code>NULL</code> for auto-selection, or a single character
for the type of estimator to use.</p>
</td></tr>
<tr><td><code id="developer-helpers_+3A_x">x</code></td>
<td>
<p>The column used to autoselect the estimator. This is generally
the <code>truth</code> column, but can also be a table if your metric has table methods.</p>
</td></tr>
<tr><td><code id="developer-helpers_+3A_metric_class">metric_class</code></td>
<td>
<p>A single character of the name of the metric to autoselect
the estimator for. This should match the method name created for
<code>finalize_estimator_internal()</code>.</p>
</td></tr>
<tr><td><code id="developer-helpers_+3A_call">call</code></td>
<td>
<p>The execution environment of a currently
running function, e.g. <code>caller_env()</code>. The function will be
mentioned in error messages as the source of the error. See the
<code>call</code> argument of <code><a href="rlang.html#topic+abort">abort()</a></code> for more information.</p>
</td></tr>
<tr><td><code id="developer-helpers_+3A_metric_dispatcher">metric_dispatcher</code></td>
<td>
<p>A simple dummy object with the class provided to
<code>metric_class</code>. This is created and passed along for you.</p>
</td></tr>
<tr><td><code id="developer-helpers_+3A_estimator_override">estimator_override</code></td>
<td>
<p>A character vector overriding the default allowed
estimator list of
<code>c("binary", "macro", "micro", "macro_weighted")</code>. Set
this if your classification estimator does not support all of these methods.</p>
</td></tr>
</table>


<h3>Dots -&gt; Estimate</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#deprecated"><img src="../help/figures/lifecycle-deprecated.svg" alt='[Deprecated]' /></a>
</p>
<p><code>dots_to_estimate()</code> is useful with class probability metrics that take
<code>...</code> rather than <code>estimate</code> as an argument. It constructs either a single
name if 1 input is provided to <code>...</code> or it constructs a quosure where the
expression constructs a matrix of as many columns as are provided to <code>...</code>.
These are eventually evaluated in the <code>summarise()</code> call in
<a href="#topic+metric-summarizers">metric-summarizers</a> and evaluate to either a vector or a matrix for
further use in the underlying vector functions.
</p>


<h3>Weight Calculation</h3>

<p><code>get_weights()</code> accepts a confusion matrix and an <code>estimator</code> of type
<code>"macro"</code>, <code>"micro"</code>, or <code>"macro_weighted"</code> and returns the correct weights.
It is useful when creating multiclass metrics.
</p>


<h3>Estimator Selection</h3>

<p><code>finalize_estimator()</code> is the engine for auto-selection of <code>estimator</code> based
on the type of <code>x</code>. Generally <code>x</code> is the <code>truth</code> column. This function
is called from the vector method of your metric.
</p>
<p><code>finalize_estimator_internal()</code> is an S3 generic that you should extend for
your metric if it does not implement <em>only</em> the following estimator types:
<code>"binary"</code>, <code>"macro"</code>, <code>"micro"</code>, and <code>"macro_weighted"</code>.
If your metric does support all of these, the default version of
<code>finalize_estimator_internal()</code> will autoselect <code>estimator</code> appropriately.
If you need to create a method, it should take the form:
<code>finalize_estimator_internal.metric_name</code>. Your method for
<code>finalize_estimator_internal()</code> should do two things:
</p>

<ol>
<li><p> If <code>estimator</code> is <code>NULL</code>, autoselect the <code>estimator</code> based on the
type of <code>x</code> and return a single character for the <code>estimator</code>.
</p>
</li>
<li><p> If <code>estimator</code> is not <code>NULL</code>, validate that it is an allowed <code>estimator</code>
for your metric and return it.
</p>
</li></ol>

<p>If you are using the default for <code>finalize_estimator_internal()</code>, the
<code>estimator</code> is selected using the following heuristics:
</p>

<ol>
<li><p> If <code>estimator</code> is not <code>NULL</code>, it is validated and returned immediately
as no auto-selection is needed.
</p>
</li>
<li><p> If <code>x</code> is a:
</p>

<ul>
<li> <p><code>factor</code> - Then <code>"binary"</code> is returned if it has 2 levels, otherwise
<code>"macro"</code> is returned.
</p>
</li>
<li> <p><code>numeric</code> - Then <code>"binary"</code> is returned.
</p>
</li>
<li> <p><code>table</code> - Then <code>"binary"</code> is returned if it has 2 columns, otherwise
<code>"macro"</code> is returned. This is useful if you have <code>table</code> methods.
</p>
</li>
<li> <p><code>matrix</code> - Then <code>"macro"</code> is returned.
</p>
</li></ul>

</li></ol>



<h3>Estimator Validation</h3>

<p><code>validate_estimator()</code> is called from your metric specific method of
<code>finalize_estimator_internal()</code> and ensures that a user provided estimator
is of the right format and is one of the allowed values.
</p>


<h3>See Also</h3>

<p><a href="#topic+metric-summarizers">metric-summarizers</a> <a href="#topic+check_metric">check_metric</a> <a href="#topic+yardstick_remove_missing">yardstick_remove_missing</a>
</p>

<hr>
<h2 id='equal_opportunity'>Equal opportunity</h2><span id='topic+equal_opportunity'></span>

<h3>Description</h3>

<p>Equal opportunity is satisfied when a model's predictions have the same
true positive and false negative rates across protected groups. A value of
0 indicates parity across groups.
</p>
<p><code>equal_opportunity()</code> is calculated as the difference between the largest
and smallest value of <code><a href="#topic+sens">sens()</a></code> across groups.
</p>
<p>Equal opportunity is sometimes referred to as equality of opportunity.
</p>
<p>See the &quot;Measuring Disparity&quot; section for details on implementation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>equal_opportunity(by)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="equal_opportunity_+3A_by">by</code></td>
<td>
<p>The column identifier for the sensitive feature. This should be an
unquoted column name referring to a column in the un-preprocessed data.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function outputs a yardstick <em>fairness metric</em> function. Given a
grouping variable <code>by</code>, <code>equal_opportunity()</code> will return a yardstick metric
function that is associated with the data-variable grouping <code>by</code> and a
post-processor. The outputted function will first generate a set
of sens metric values by group before summarizing across
groups using the post-processing function.
</p>
<p>The outputted function only has a data frame method and is intended to
be used as part of a metric set.
</p>


<h3>Measuring Disparity</h3>

<p>By default, this function takes the difference in range of sens
<code>.estimate</code>s across groups. That is, the maximum pair-wise disparity between
groups is the return value of <code>equal_opportunity()</code>'s <code>.estimate</code>.
</p>
<p>For finer control of group treatment, construct a context-aware fairness
metric with the <code><a href="#topic+new_groupwise_metric">new_groupwise_metric()</a></code> function by passing a custom <code>aggregate</code>
function:
</p>
<div class="sourceCode"><pre># the actual default `aggregate` is:
diff_range &lt;- function(x, ...) {diff(range(x$.estimate))}

equal_opportunity_2 &lt;-
  new_groupwise_metric(
    fn = sens,
    name = "equal_opportunity_2",
    aggregate = diff_range
  )
</pre></div>
<p>In <code>aggregate()</code>, <code>x</code> is the <code>metric_set()</code> output with sens values
for each group, and <code>...</code> gives additional arguments (such as a grouping
level to refer to as the &quot;baseline&quot;) to pass to the function outputted
by <code>equal_opportunity_2()</code> for context.
</p>


<h3>References</h3>

<p>Hardt, M., Price, E., &amp; Srebro, N. (2016). &quot;Equality of opportunity in
supervised learning&quot;. Advances in neural information processing systems, 29.
</p>
<p>Verma, S., &amp; Rubin, J. (2018). &quot;Fairness definitions explained&quot;. In
Proceedings of the international workshop on software fairness (pp. 1-7).
</p>
<p>Bird, S., Dudík, M., Edgar, R., Horn, B., Lutz, R., Milan, V., ... &amp; Walker,
K. (2020). &quot;Fairlearn: A toolkit for assessing and improving fairness in AI&quot;.
Microsoft, Tech. Rep. MSR-TR-2020-32.
</p>


<h3>See Also</h3>

<p>Other fairness metrics: 
<code><a href="#topic+demographic_parity">demographic_parity</a>()</code>,
<code><a href="#topic+equalized_odds">equalized_odds</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)

data(hpc_cv)

head(hpc_cv)

# evaluate `equal_opportunity()` by Resample
m_set &lt;- metric_set(equal_opportunity(Resample))

# use output like any other metric set
hpc_cv %&gt;%
  m_set(truth = obs, estimate = pred)

# can mix fairness metrics and regular metrics
m_set_2 &lt;- metric_set(sens, equal_opportunity(Resample))

hpc_cv %&gt;%
  m_set_2(truth = obs, estimate = pred)
</code></pre>

<hr>
<h2 id='equalized_odds'>Equalized odds</h2><span id='topic+equalized_odds'></span>

<h3>Description</h3>

<p>Equalized odds is satisfied when a model's predictions have the same false
positive, true positive, false negative, and true negative rates across
protected groups. A value of 0 indicates parity across groups.
</p>
<p>By default, this function takes the maximum difference in range of <code><a href="#topic+sens">sens()</a></code>
and <code><a href="#topic+spec">spec()</a></code> <code>.estimate</code>s across groups. That is, the maximum pair-wise
disparity in <code><a href="#topic+sens">sens()</a></code> or <code><a href="#topic+spec">spec()</a></code> between groups is the return value of
<code>equalized_odds()</code>'s <code>.estimate</code>.
</p>
<p>Equalized odds is sometimes referred to as conditional procedure accuracy
equality or disparate mistreatment.
</p>
<p>See the &quot;Measuring disparity&quot; section for details on implementation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>equalized_odds(by)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="equalized_odds_+3A_by">by</code></td>
<td>
<p>The column identifier for the sensitive feature. This should be an
unquoted column name referring to a column in the un-preprocessed data.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function outputs a yardstick <em>fairness metric</em> function. Given a
grouping variable <code>by</code>, <code>equalized_odds()</code> will return a yardstick metric
function that is associated with the data-variable grouping <code>by</code> and a
post-processor. The outputted function will first generate a set
of <code><a href="#topic+sens">sens()</a></code> and <code><a href="#topic+spec">spec()</a></code> metric values by group before summarizing across
groups using the post-processing function.
</p>
<p>The outputted function only has a data frame method and is intended to
be used as part of a metric set.
</p>


<h3>Measuring Disparity</h3>

<p>For finer control of group treatment, construct a context-aware fairness
metric with the <code><a href="#topic+new_groupwise_metric">new_groupwise_metric()</a></code> function by passing a custom <code>aggregate</code>
function:
</p>
<div class="sourceCode"><pre># see yardstick:::max_positive_rate_diff for the actual `aggregate()`
diff_range &lt;- function(x, ...) {diff(range(x$.estimate))}

equalized_odds_2 &lt;-
  new_groupwise_metric(
    fn = metric_set(sens, spec),
    name = "equalized_odds_2",
    aggregate = diff_range
  )
</pre></div>
<p>In <code>aggregate()</code>, <code>x</code> is the <code><a href="#topic+metric_set">metric_set()</a></code> output with <code><a href="#topic+sens">sens()</a></code> and <code><a href="#topic+spec">spec()</a></code>
values for each group, and <code>...</code> gives additional arguments (such as a grouping
level to refer to as the &quot;baseline&quot;) to pass to the function outputted
by <code>equalized_odds_2()</code> for context.
</p>


<h3>References</h3>

<p>Agarwal, A., Beygelzimer, A., Dudik, M., Langford, J., &amp; Wallach, H. (2018).
&quot;A Reductions Approach to Fair Classification.&quot; Proceedings of the 35th
International Conference on Machine Learning, in Proceedings of Machine
Learning Research. 80:60-69.
</p>
<p>Verma, S., &amp; Rubin, J. (2018). &quot;Fairness definitions explained&quot;. In
Proceedings of the international workshop on software fairness (pp. 1-7).
</p>
<p>Bird, S., Dudík, M., Edgar, R., Horn, B., Lutz, R., Milan, V., ... &amp; Walker,
K. (2020). &quot;Fairlearn: A toolkit for assessing and improving fairness in AI&quot;.
Microsoft, Tech. Rep. MSR-TR-2020-32.
</p>


<h3>See Also</h3>

<p>Other fairness metrics: 
<code><a href="#topic+demographic_parity">demographic_parity</a>()</code>,
<code><a href="#topic+equal_opportunity">equal_opportunity</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)

data(hpc_cv)

head(hpc_cv)

# evaluate `equalized_odds()` by Resample
m_set &lt;- metric_set(equalized_odds(Resample))

# use output like any other metric set
hpc_cv %&gt;%
  m_set(truth = obs, estimate = pred)

# can mix fairness metrics and regular metrics
m_set_2 &lt;- metric_set(sens, equalized_odds(Resample))

hpc_cv %&gt;%
  m_set_2(truth = obs, estimate = pred)
</code></pre>

<hr>
<h2 id='f_meas'>F Measure</h2><span id='topic+f_meas'></span><span id='topic+f_meas.data.frame'></span><span id='topic+f_meas_vec'></span>

<h3>Description</h3>

<p>These functions calculate the <code><a href="#topic+f_meas">f_meas()</a></code> of a measurement system for
finding relevant documents compared to reference results
(the truth regarding relevance). Highly related functions are <code><a href="#topic+recall">recall()</a></code>
and <code><a href="#topic+precision">precision()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>f_meas(data, ...)

## S3 method for class 'data.frame'
f_meas(
  data,
  truth,
  estimate,
  beta = 1,
  estimator = NULL,
  na_rm = TRUE,
  case_weights = NULL,
  event_level = yardstick_event_level(),
  ...
)

f_meas_vec(
  truth,
  estimate,
  beta = 1,
  estimator = NULL,
  na_rm = TRUE,
  case_weights = NULL,
  event_level = yardstick_event_level(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="f_meas_+3A_data">data</code></td>
<td>
<p>Either a <code>data.frame</code> containing the columns specified by the
<code>truth</code> and <code>estimate</code> arguments, or a <code>table</code>/<code>matrix</code> where the true
class results should be in the columns of the table.</p>
</td></tr>
<tr><td><code id="f_meas_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="f_meas_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a <code>factor</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="f_meas_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predicted class
results (that is also <code>factor</code>). As with <code>truth</code> this can be
specified different ways but the primary method is to use an
unquoted variable name. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="f_meas_+3A_beta">beta</code></td>
<td>
<p>A numeric value used to weight precision and
recall. A value of 1 is traditionally used and corresponds to
the harmonic mean of the two values but other values weight
recall beta times more important than precision.</p>
</td></tr>
<tr><td><code id="f_meas_+3A_estimator">estimator</code></td>
<td>
<p>One of: <code>"binary"</code>, <code>"macro"</code>, <code>"macro_weighted"</code>,
or <code>"micro"</code> to specify the type of averaging to be done. <code>"binary"</code> is
only relevant for the two class case. The other three are general methods
for calculating multiclass metrics. The default will automatically choose
<code>"binary"</code> or <code>"macro"</code> based on <code>estimate</code>.</p>
</td></tr>
<tr><td><code id="f_meas_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="f_meas_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights.
This should be an unquoted column name that evaluates to a numeric column
in <code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
<tr><td><code id="f_meas_+3A_event_level">event_level</code></td>
<td>
<p>A single string. Either <code>"first"</code> or <code>"second"</code> to specify
which level of <code>truth</code> to consider as the &quot;event&quot;. This argument is only
applicable when <code>estimator = "binary"</code>. The default uses an internal helper
that defaults to <code>"first"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The measure &quot;F&quot; is a combination of precision and recall (see below).
</p>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>f_meas_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Relevant Level</h3>

<p>There is no common convention on which factor level should
automatically be considered the &quot;event&quot; or &quot;positive&quot; result
when computing binary classification metrics. In <code>yardstick</code>, the default
is to use the <em>first</em> level. To alter this, change the argument
<code>event_level</code> to <code>"second"</code> to consider the <em>last</em> level of the factor the
level of interest. For multiclass extensions involving one-vs-all
comparisons (such as macro averaging), this option is ignored and
the &quot;one&quot; level is always the relevant result.
</p>


<h3>Multiclass</h3>

<p>Macro, micro, and macro-weighted averaging is available for this metric.
The default is to select macro averaging if a <code>truth</code> factor with more
than 2 levels is provided. Otherwise, a standard binary calculation is done.
See <code>vignette("multiclass", "yardstick")</code> for more information.
</p>


<h3>Implementation</h3>

<p>Suppose a 2x2 table with notation:
</p>

<table>
<tr>
 <td style="text-align: right;"> </td><td style="text-align: center;"> Reference </td><td style="text-align: center;"> </td>
</tr>
<tr>
 <td style="text-align: right;"> Predicted </td><td style="text-align: center;"> Relevant </td><td style="text-align: center;">
Irrelevant </td>
</tr>
<tr>
 <td style="text-align: right;"> Relevant </td><td style="text-align: center;"> A </td><td style="text-align: center;"> B </td>
</tr>
<tr>
 <td style="text-align: right;"> Irrelevant </td><td style="text-align: center;"> C </td><td style="text-align: center;"> D </td>
</tr>
<tr>
 <td style="text-align: right;"> </td>
</tr>

</table>

<p>The formulas used here are:
</p>
<p style="text-align: center;"><code class="reqn">recall = A/(A+C)</code>
</p>

<p style="text-align: center;"><code class="reqn">precision = A/(A+B)</code>
</p>

<p style="text-align: center;"><code class="reqn">F_{meas} = (1+\beta^2) * precision * recall/((\beta^2 * precision)+recall)</code>
</p>

<p>See the references for discussions of the statistics.
</p>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>References</h3>

<p>Buckland, M., &amp; Gey, F. (1994). The relationship
between Recall and Precision. <em>Journal of the American Society
for Information Science</em>, 45(1), 12-19.
</p>
<p>Powers, D. (2007). Evaluation: From Precision, Recall and F
Factor to ROC, Informedness, Markedness and Correlation.
Technical Report SIE-07-001, Flinders University
</p>


<h3>See Also</h3>

<p>Other class metrics: 
<code><a href="#topic+accuracy">accuracy</a>()</code>,
<code><a href="#topic+bal_accuracy">bal_accuracy</a>()</code>,
<code><a href="#topic+detection_prevalence">detection_prevalence</a>()</code>,
<code><a href="#topic+j_index">j_index</a>()</code>,
<code><a href="#topic+kap">kap</a>()</code>,
<code><a href="#topic+mcc">mcc</a>()</code>,
<code><a href="#topic+npv">npv</a>()</code>,
<code><a href="#topic+ppv">ppv</a>()</code>,
<code><a href="#topic+precision">precision</a>()</code>,
<code><a href="#topic+recall">recall</a>()</code>,
<code><a href="#topic+sens">sens</a>()</code>,
<code><a href="#topic+spec">spec</a>()</code>
</p>
<p>Other relevance metrics: 
<code><a href="#topic+precision">precision</a>()</code>,
<code><a href="#topic+recall">recall</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Two class
data("two_class_example")
f_meas(two_class_example, truth, predicted)

# Multiclass
library(dplyr)
data(hpc_cv)

hpc_cv %&gt;%
  filter(Resample == "Fold01") %&gt;%
  f_meas(obs, pred)

# Groups are respected
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  f_meas(obs, pred)

# Weighted macro averaging
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  f_meas(obs, pred, estimator = "macro_weighted")

# Vector version
f_meas_vec(
  two_class_example$truth,
  two_class_example$predicted
)

# Making Class2 the "relevant" level
f_meas_vec(
  two_class_example$truth,
  two_class_example$predicted,
  event_level = "second"
)
</code></pre>

<hr>
<h2 id='gain_capture'>Gain capture</h2><span id='topic+gain_capture'></span><span id='topic+gain_capture.data.frame'></span><span id='topic+gain_capture_vec'></span>

<h3>Description</h3>

<p><code>gain_capture()</code> is a measure of performance similar to an AUC calculation,
but applied to a gain curve.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gain_capture(data, ...)

## S3 method for class 'data.frame'
gain_capture(
  data,
  truth,
  ...,
  estimator = NULL,
  na_rm = TRUE,
  event_level = yardstick_event_level(),
  case_weights = NULL
)

gain_capture_vec(
  truth,
  estimate,
  estimator = NULL,
  na_rm = TRUE,
  event_level = yardstick_event_level(),
  case_weights = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gain_capture_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by <code>truth</code> and
<code>...</code>.</p>
</td></tr>
<tr><td><code id="gain_capture_+3A_...">...</code></td>
<td>
<p>A set of unquoted column names or one or more
<code>dplyr</code> selector functions to choose which variables contain the
class probabilities. If <code>truth</code> is binary, only 1 column should be selected,
and it should correspond to the value of <code>event_level</code>. Otherwise, there
should be as many columns as factor levels of <code>truth</code> and the ordering of
the columns should be the same as the factor levels of <code>truth</code>.</p>
</td></tr>
<tr><td><code id="gain_capture_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a <code>factor</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="gain_capture_+3A_estimator">estimator</code></td>
<td>
<p>One of <code>"binary"</code>, <code>"macro"</code>, or <code>"macro_weighted"</code> to
specify the type of averaging to be done. <code>"binary"</code> is only relevant for
the two class case. The other two are general methods for calculating
multiclass metrics. The default will automatically choose <code>"binary"</code> or
<code>"macro"</code> based on <code>truth</code>.</p>
</td></tr>
<tr><td><code id="gain_capture_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="gain_capture_+3A_event_level">event_level</code></td>
<td>
<p>A single string. Either <code>"first"</code> or <code>"second"</code> to specify
which level of <code>truth</code> to consider as the &quot;event&quot;. This argument is only
applicable when <code>estimator = "binary"</code>. The default uses an internal helper
that defaults to <code>"first"</code>.</p>
</td></tr>
<tr><td><code id="gain_capture_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights.
This should be an unquoted column name that evaluates to a numeric column
in <code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
<tr><td><code id="gain_capture_+3A_estimate">estimate</code></td>
<td>
<p>If <code>truth</code> is binary, a numeric vector of class probabilities
corresponding to the &quot;relevant&quot; class. Otherwise, a matrix with as many
columns as factor levels of <code>truth</code>. <em>It is assumed that these are in the
same order as the levels of <code>truth</code>.</em></p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>gain_capture()</code> calculates the area <em>under</em> the gain curve, but <em>above</em>
the baseline, and then divides that by the area <em>under</em> a perfect gain curve,
but <em>above</em> the baseline. It is meant to represent the amount of potential
gain &quot;captured&quot; by the model.
</p>
<p>The <code>gain_capture()</code> metric is identical to the <em>accuracy ratio (AR)</em>, which
is also sometimes called the <em>gini coefficient</em>. These two are generally
calculated on a cumulative accuracy profile curve, but this is the same as
a gain curve. See the Engelmann reference for more information.
</p>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>gain_capture_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Relevant Level</h3>

<p>There is no common convention on which factor level should
automatically be considered the &quot;event&quot; or &quot;positive&quot; result
when computing binary classification metrics. In <code>yardstick</code>, the default
is to use the <em>first</em> level. To alter this, change the argument
<code>event_level</code> to <code>"second"</code> to consider the <em>last</em> level of the factor the
level of interest. For multiclass extensions involving one-vs-all
comparisons (such as macro averaging), this option is ignored and
the &quot;one&quot; level is always the relevant result.
</p>


<h3>Multiclass</h3>

<p>Macro and macro-weighted averaging is available for this metric.
The default is to select macro averaging if a <code>truth</code> factor with more
than 2 levels is provided. Otherwise, a standard binary calculation is done.
See <code>vignette("multiclass", "yardstick")</code> for more information.
</p>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>References</h3>

<p>Engelmann, Bernd &amp; Hayden, Evelyn &amp; Tasche, Dirk (2003).
&quot;Measuring the Discriminative Power of Rating Systems,&quot;
Discussion Paper Series 2: Banking and Financial Studies 2003,01,
Deutsche Bundesbank.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gain_curve">gain_curve()</a></code> to compute the full gain curve.
</p>
<p>Other class probability metrics: 
<code><a href="#topic+average_precision">average_precision</a>()</code>,
<code><a href="#topic+brier_class">brier_class</a>()</code>,
<code><a href="#topic+classification_cost">classification_cost</a>()</code>,
<code><a href="#topic+mn_log_loss">mn_log_loss</a>()</code>,
<code><a href="#topic+pr_auc">pr_auc</a>()</code>,
<code><a href="#topic+roc_auc">roc_auc</a>()</code>,
<code><a href="#topic+roc_aunp">roc_aunp</a>()</code>,
<code><a href="#topic+roc_aunu">roc_aunu</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># ---------------------------------------------------------------------------
# Two class example

# `truth` is a 2 level factor. The first level is `"Class1"`, which is the
# "event of interest" by default in yardstick. See the Relevant Level
# section above.
data(two_class_example)

# Binary metrics using class probabilities take a factor `truth` column,
# and a single class probability column containing the probabilities of
# the event of interest. Here, since `"Class1"` is the first level of
# `"truth"`, it is the event of interest and we pass in probabilities for it.
gain_capture(two_class_example, truth, Class1)

# ---------------------------------------------------------------------------
# Multiclass example

# `obs` is a 4 level factor. The first level is `"VF"`, which is the
# "event of interest" by default in yardstick. See the Relevant Level
# section above.
data(hpc_cv)

# You can use the col1:colN tidyselect syntax
library(dplyr)
hpc_cv %&gt;%
  filter(Resample == "Fold01") %&gt;%
  gain_capture(obs, VF:L)

# Change the first level of `obs` from `"VF"` to `"M"` to alter the
# event of interest. The class probability columns should be supplied
# in the same order as the levels.
hpc_cv %&gt;%
  filter(Resample == "Fold01") %&gt;%
  mutate(obs = relevel(obs, "M")) %&gt;%
  gain_capture(obs, M, VF:L)

# Groups are respected
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  gain_capture(obs, VF:L)

# Weighted macro averaging
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  gain_capture(obs, VF:L, estimator = "macro_weighted")

# Vector version
# Supply a matrix of class probabilities
fold1 &lt;- hpc_cv %&gt;%
  filter(Resample == "Fold01")

gain_capture_vec(
   truth = fold1$obs,
   matrix(
     c(fold1$VF, fold1$F, fold1$M, fold1$L),
     ncol = 4
   )
)

# ---------------------------------------------------------------------------
# Visualize gain_capture()

# Visually, this represents the area under the black curve, but above the
# 45 degree line, divided by the area of the shaded triangle.
library(ggplot2)
autoplot(gain_curve(two_class_example, truth, Class1))

</code></pre>

<hr>
<h2 id='gain_curve'>Gain curve</h2><span id='topic+gain_curve'></span><span id='topic+gain_curve.data.frame'></span>

<h3>Description</h3>

<p><code>gain_curve()</code> constructs the full gain curve and returns a tibble. See
<code><a href="#topic+gain_capture">gain_capture()</a></code> for the relevant area under the gain curve. Also see
<code><a href="#topic+lift_curve">lift_curve()</a></code> for a closely related concept.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gain_curve(data, ...)

## S3 method for class 'data.frame'
gain_curve(
  data,
  truth,
  ...,
  na_rm = TRUE,
  event_level = yardstick_event_level(),
  case_weights = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gain_curve_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by <code>truth</code> and
<code>...</code>.</p>
</td></tr>
<tr><td><code id="gain_curve_+3A_...">...</code></td>
<td>
<p>A set of unquoted column names or one or more
<code>dplyr</code> selector functions to choose which variables contain the
class probabilities. If <code>truth</code> is binary, only 1 column should be selected,
and it should correspond to the value of <code>event_level</code>. Otherwise, there
should be as many columns as factor levels of <code>truth</code> and the ordering of
the columns should be the same as the factor levels of <code>truth</code>.</p>
</td></tr>
<tr><td><code id="gain_curve_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a <code>factor</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="gain_curve_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="gain_curve_+3A_event_level">event_level</code></td>
<td>
<p>A single string. Either <code>"first"</code> or <code>"second"</code> to specify
which level of <code>truth</code> to consider as the &quot;event&quot;. This argument is only
applicable when <code>estimator = "binary"</code>. The default uses an internal helper
that defaults to <code>"first"</code>.</p>
</td></tr>
<tr><td><code id="gain_curve_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights.
This should be an unquoted column name that evaluates to a numeric column
in <code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There is a <code><a href="ggplot2.html#topic+autoplot">ggplot2::autoplot()</a></code> method for quickly visualizing the curve.
This works for binary and multiclass output, and also works with grouped data
(i.e. from resamples). See the examples.
</p>
<p>The greater the area between the gain curve and the baseline, the better the
model.
</p>
<p>Gain curves are identical to CAP curves (cumulative accuracy profile). See
the Engelmann reference for more information on CAP curves.
</p>


<h3>Value</h3>

<p>A tibble with class <code>gain_df</code> or <code>gain_grouped_df</code> having columns:
</p>

<ul>
<li> <p><code>.n</code> The index of the current sample.
</p>
</li>
<li> <p><code>.n_events</code> The index of the current <em>unique</em> sample. Values with repeated
<code>estimate</code> values are given identical indices in this column.
</p>
</li>
<li> <p><code>.percent_tested</code> The cumulative percentage of values tested.
</p>
</li>
<li> <p><code>.percent_found</code> The cumulative percentage of true results relative to the
total number of true results.
</p>
</li></ul>

<p>If using the <code>case_weights</code> argument, all of the above columns will be
weighted. This makes the most sense with frequency weights, which are integer
weights representing the number of times a particular observation should be
repeated.
</p>


<h3>Gain and Lift Curves</h3>

<p>The motivation behind cumulative gain and lift charts is as a visual method to
determine the effectiveness of a model when compared to the results one
might expect without a model. As an example, without a model, if you were
to advertise to a random 10% of your customer base, then you might expect
to capture 10% of the of the total number of positive responses had you
advertised to your entire customer base. Given a model that predicts
which customers are more likely to respond, the hope is that you can more
accurately target 10% of your customer base and capture
<code>&gt;</code>10% of the total number of positive responses.
</p>
<p>The calculation to construct gain curves is as follows:
</p>

<ol>
<li> <p><code>truth</code> and <code>estimate</code> are placed in descending order by the <code>estimate</code>
values (<code>estimate</code> here is a single column supplied in <code>...</code>).
</p>
</li>
<li><p> The cumulative number of samples with true results relative to the
entire number of true results are found. This is the y-axis in a gain chart.
</p>
</li></ol>



<h3>Multiclass</h3>

<p>If a multiclass <code>truth</code> column is provided, a one-vs-all
approach will be taken to calculate multiple curves, one per level.
In this case, there will be an additional column, <code>.level</code>,
identifying the &quot;one&quot; column in the one-vs-all calculation.
</p>


<h3>Relevant Level</h3>

<p>There is no common convention on which factor level should
automatically be considered the &quot;event&quot; or &quot;positive&quot; result
when computing binary classification metrics. In <code>yardstick</code>, the default
is to use the <em>first</em> level. To alter this, change the argument
<code>event_level</code> to <code>"second"</code> to consider the <em>last</em> level of the factor the
level of interest. For multiclass extensions involving one-vs-all
comparisons (such as macro averaging), this option is ignored and
the &quot;one&quot; level is always the relevant result.
</p>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>References</h3>

<p>Engelmann, Bernd &amp; Hayden, Evelyn &amp; Tasche, Dirk (2003).
&quot;Measuring the Discriminative Power of Rating Systems,&quot;
Discussion Paper Series 2: Banking and Financial Studies 2003,01,
Deutsche Bundesbank.
</p>


<h3>See Also</h3>

<p>Compute the relevant area under the gain curve with <code><a href="#topic+gain_capture">gain_capture()</a></code>.
</p>
<p>Other curve metrics: 
<code><a href="#topic+lift_curve">lift_curve</a>()</code>,
<code><a href="#topic+pr_curve">pr_curve</a>()</code>,
<code><a href="#topic+roc_curve">roc_curve</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># ---------------------------------------------------------------------------
# Two class example

# `truth` is a 2 level factor. The first level is `"Class1"`, which is the
# "event of interest" by default in yardstick. See the Relevant Level
# section above.
data(two_class_example)

# Binary metrics using class probabilities take a factor `truth` column,
# and a single class probability column containing the probabilities of
# the event of interest. Here, since `"Class1"` is the first level of
# `"truth"`, it is the event of interest and we pass in probabilities for it.
gain_curve(two_class_example, truth, Class1)

# ---------------------------------------------------------------------------
# `autoplot()`

library(ggplot2)
library(dplyr)

# Use autoplot to visualize
# The top left hand corner of the grey triangle is a "perfect" gain curve
autoplot(gain_curve(two_class_example, truth, Class1))

# Multiclass one-vs-all approach
# One curve per level
hpc_cv %&gt;%
  filter(Resample == "Fold01") %&gt;%
  gain_curve(obs, VF:L) %&gt;%
  autoplot()

# Same as above, but will all of the resamples
# The resample with the minimum (farthest to the left) "perfect" value is
# used to draw the shaded region
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  gain_curve(obs, VF:L) %&gt;%
  autoplot()

</code></pre>

<hr>
<h2 id='hpc_cv'>Multiclass Probability Predictions</h2><span id='topic+hpc_cv'></span>

<h3>Description</h3>

<p>Multiclass Probability Predictions
</p>


<h3>Details</h3>

<p>This data frame contains the predicted classes and
class probabilities for a linear discriminant analysis model fit
to the HPC data set from Kuhn and Johnson (2013). These data are
the assessment sets from a 10-fold cross-validation scheme. The
data column columns for the true class (<code>obs</code>), the class
prediction (<code>pred</code>) and columns for each class probability
(columns <code>VF</code>, <code>F</code>, <code>M</code>, and <code>L</code>). Additionally, a column for
the resample indicator is included.
</p>


<h3>Value</h3>

<table>
<tr><td><code>hpc_cv</code></td>
<td>
<p>a data frame</p>
</td></tr>
</table>


<h3>Source</h3>

<p>Kuhn, M., Johnson, K. (2013) <em>Applied Predictive
Modeling</em>, Springer
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(hpc_cv)
str(hpc_cv)

# `obs` is a 4 level factor. The first level is `"VF"`, which is the
# "event of interest" by default in yardstick. See the Relevant Level
# section in any classification function (such as `?pr_auc`) to see how
# to change this.
levels(hpc_cv$obs)
</code></pre>

<hr>
<h2 id='huber_loss'>Huber loss</h2><span id='topic+huber_loss'></span><span id='topic+huber_loss.data.frame'></span><span id='topic+huber_loss_vec'></span>

<h3>Description</h3>

<p>Calculate the Huber loss, a loss function used in robust regression. This
loss function is less sensitive to outliers than <code><a href="#topic+rmse">rmse()</a></code>. This function is
quadratic for small residual values and linear for large residual values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>huber_loss(data, ...)

## S3 method for class 'data.frame'
huber_loss(
  data,
  truth,
  estimate,
  delta = 1,
  na_rm = TRUE,
  case_weights = NULL,
  ...
)

huber_loss_vec(
  truth,
  estimate,
  delta = 1,
  na_rm = TRUE,
  case_weights = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="huber_loss_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by the <code>truth</code>
and <code>estimate</code> arguments.</p>
</td></tr>
<tr><td><code id="huber_loss_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="huber_loss_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true results
(that is <code>numeric</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>numeric</code> vector.</p>
</td></tr>
<tr><td><code id="huber_loss_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predicted
results (that is also <code>numeric</code>). As with <code>truth</code> this can be
specified different ways but the primary method is to use an
unquoted variable name. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>numeric</code> vector.</p>
</td></tr>
<tr><td><code id="huber_loss_+3A_delta">delta</code></td>
<td>
<p>A single <code>numeric</code> value. Defines the boundary where the loss function
transitions from quadratic to linear. Defaults to 1.</p>
</td></tr>
<tr><td><code id="huber_loss_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="huber_loss_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights. This
should be an unquoted column name that evaluates to a numeric column in
<code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>huber_loss_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Author(s)</h3>

<p>James Blair
</p>


<h3>References</h3>

<p>Huber, P. (1964). Robust Estimation of a Location Parameter.
<em>Annals of Statistics</em>, 53 (1), 73-101.
</p>


<h3>See Also</h3>

<p>Other numeric metrics: 
<code><a href="#topic+ccc">ccc</a>()</code>,
<code><a href="#topic+huber_loss_pseudo">huber_loss_pseudo</a>()</code>,
<code><a href="#topic+iic">iic</a>()</code>,
<code><a href="#topic+mae">mae</a>()</code>,
<code><a href="#topic+mape">mape</a>()</code>,
<code><a href="#topic+mase">mase</a>()</code>,
<code><a href="#topic+mpe">mpe</a>()</code>,
<code><a href="#topic+msd">msd</a>()</code>,
<code><a href="#topic+poisson_log_loss">poisson_log_loss</a>()</code>,
<code><a href="#topic+rmse">rmse</a>()</code>,
<code><a href="#topic+rpd">rpd</a>()</code>,
<code><a href="#topic+rpiq">rpiq</a>()</code>,
<code><a href="#topic+rsq_trad">rsq_trad</a>()</code>,
<code><a href="#topic+rsq">rsq</a>()</code>,
<code><a href="#topic+smape">smape</a>()</code>
</p>
<p>Other accuracy metrics: 
<code><a href="#topic+ccc">ccc</a>()</code>,
<code><a href="#topic+huber_loss_pseudo">huber_loss_pseudo</a>()</code>,
<code><a href="#topic+iic">iic</a>()</code>,
<code><a href="#topic+mae">mae</a>()</code>,
<code><a href="#topic+mape">mape</a>()</code>,
<code><a href="#topic+mase">mase</a>()</code>,
<code><a href="#topic+mpe">mpe</a>()</code>,
<code><a href="#topic+msd">msd</a>()</code>,
<code><a href="#topic+poisson_log_loss">poisson_log_loss</a>()</code>,
<code><a href="#topic+rmse">rmse</a>()</code>,
<code><a href="#topic+smape">smape</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Supply truth and predictions as bare column names
huber_loss(solubility_test, solubility, prediction)

library(dplyr)

set.seed(1234)
size &lt;- 100
times &lt;- 10

# create 10 resamples
solubility_resampled &lt;- bind_rows(
  replicate(
    n = times,
    expr = sample_n(solubility_test, size, replace = TRUE),
    simplify = FALSE
  ),
  .id = "resample"
)

# Compute the metric by group
metric_results &lt;- solubility_resampled %&gt;%
  group_by(resample) %&gt;%
  huber_loss(solubility, prediction)

metric_results

# Resampled mean estimate
metric_results %&gt;%
  summarise(avg_estimate = mean(.estimate))
</code></pre>

<hr>
<h2 id='huber_loss_pseudo'>Psuedo-Huber Loss</h2><span id='topic+huber_loss_pseudo'></span><span id='topic+huber_loss_pseudo.data.frame'></span><span id='topic+huber_loss_pseudo_vec'></span>

<h3>Description</h3>

<p>Calculate the Pseudo-Huber Loss, a smooth approximation of <code><a href="#topic+huber_loss">huber_loss()</a></code>.
Like <code><a href="#topic+huber_loss">huber_loss()</a></code>, this is less sensitive to outliers than <code><a href="#topic+rmse">rmse()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>huber_loss_pseudo(data, ...)

## S3 method for class 'data.frame'
huber_loss_pseudo(
  data,
  truth,
  estimate,
  delta = 1,
  na_rm = TRUE,
  case_weights = NULL,
  ...
)

huber_loss_pseudo_vec(
  truth,
  estimate,
  delta = 1,
  na_rm = TRUE,
  case_weights = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="huber_loss_pseudo_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by the <code>truth</code>
and <code>estimate</code> arguments.</p>
</td></tr>
<tr><td><code id="huber_loss_pseudo_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="huber_loss_pseudo_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true results
(that is <code>numeric</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>numeric</code> vector.</p>
</td></tr>
<tr><td><code id="huber_loss_pseudo_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predicted
results (that is also <code>numeric</code>). As with <code>truth</code> this can be
specified different ways but the primary method is to use an
unquoted variable name. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>numeric</code> vector.</p>
</td></tr>
<tr><td><code id="huber_loss_pseudo_+3A_delta">delta</code></td>
<td>
<p>A single <code>numeric</code> value. Defines the boundary where the loss function
transitions from quadratic to linear. Defaults to 1.</p>
</td></tr>
<tr><td><code id="huber_loss_pseudo_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="huber_loss_pseudo_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights. This
should be an unquoted column name that evaluates to a numeric column in
<code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>huber_loss_pseudo_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Author(s)</h3>

<p>James Blair
</p>


<h3>References</h3>

<p>Huber, P. (1964). Robust Estimation of a Location Parameter.
<em>Annals of Statistics</em>, 53 (1), 73-101.
</p>
<p>Hartley, Richard (2004). Multiple View Geometry in Computer Vision.
(Second Edition). Page 619.
</p>


<h3>See Also</h3>

<p>Other numeric metrics: 
<code><a href="#topic+ccc">ccc</a>()</code>,
<code><a href="#topic+huber_loss">huber_loss</a>()</code>,
<code><a href="#topic+iic">iic</a>()</code>,
<code><a href="#topic+mae">mae</a>()</code>,
<code><a href="#topic+mape">mape</a>()</code>,
<code><a href="#topic+mase">mase</a>()</code>,
<code><a href="#topic+mpe">mpe</a>()</code>,
<code><a href="#topic+msd">msd</a>()</code>,
<code><a href="#topic+poisson_log_loss">poisson_log_loss</a>()</code>,
<code><a href="#topic+rmse">rmse</a>()</code>,
<code><a href="#topic+rpd">rpd</a>()</code>,
<code><a href="#topic+rpiq">rpiq</a>()</code>,
<code><a href="#topic+rsq_trad">rsq_trad</a>()</code>,
<code><a href="#topic+rsq">rsq</a>()</code>,
<code><a href="#topic+smape">smape</a>()</code>
</p>
<p>Other accuracy metrics: 
<code><a href="#topic+ccc">ccc</a>()</code>,
<code><a href="#topic+huber_loss">huber_loss</a>()</code>,
<code><a href="#topic+iic">iic</a>()</code>,
<code><a href="#topic+mae">mae</a>()</code>,
<code><a href="#topic+mape">mape</a>()</code>,
<code><a href="#topic+mase">mase</a>()</code>,
<code><a href="#topic+mpe">mpe</a>()</code>,
<code><a href="#topic+msd">msd</a>()</code>,
<code><a href="#topic+poisson_log_loss">poisson_log_loss</a>()</code>,
<code><a href="#topic+rmse">rmse</a>()</code>,
<code><a href="#topic+smape">smape</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Supply truth and predictions as bare column names
huber_loss_pseudo(solubility_test, solubility, prediction)

library(dplyr)

set.seed(1234)
size &lt;- 100
times &lt;- 10

# create 10 resamples
solubility_resampled &lt;- bind_rows(
  replicate(
    n = times,
    expr = sample_n(solubility_test, size, replace = TRUE),
    simplify = FALSE
  ),
  .id = "resample"
)

# Compute the metric by group
metric_results &lt;- solubility_resampled %&gt;%
  group_by(resample) %&gt;%
  huber_loss_pseudo(solubility, prediction)

metric_results

# Resampled mean estimate
metric_results %&gt;%
  summarise(avg_estimate = mean(.estimate))
</code></pre>

<hr>
<h2 id='iic'>Index of ideality of correlation</h2><span id='topic+iic'></span><span id='topic+iic.data.frame'></span><span id='topic+iic_vec'></span>

<h3>Description</h3>

<p>Calculate the index of ideality of correlation. This metric has been
studied in QSPR/QSAR models as a good criterion for the predictive
potential of these models. It is highly dependent on the correlation
coefficient as well as the mean absolute error.
</p>
<p>Note the application of IIC is useless under two conditions:
</p>

<ul>
<li><p> When the negative mean absolute error and positive mean absolute
error are both zero.
</p>
</li>
<li><p> When the outliers are symmetric. Since outliers are context
dependent, please use your own checks to validate whether this
restriction holds and whether the resulting IIC has
interpretative value.
</p>
</li></ul>

<p>The IIC is seen as an alternative to the traditional correlation
coefficient and is in the same units as the original data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>iic(data, ...)

## S3 method for class 'data.frame'
iic(data, truth, estimate, na_rm = TRUE, case_weights = NULL, ...)

iic_vec(truth, estimate, na_rm = TRUE, case_weights = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="iic_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by the <code>truth</code>
and <code>estimate</code> arguments.</p>
</td></tr>
<tr><td><code id="iic_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="iic_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true results
(that is <code>numeric</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>numeric</code> vector.</p>
</td></tr>
<tr><td><code id="iic_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predicted
results (that is also <code>numeric</code>). As with <code>truth</code> this can be
specified different ways but the primary method is to use an
unquoted variable name. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>numeric</code> vector.</p>
</td></tr>
<tr><td><code id="iic_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="iic_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights. This
should be an unquoted column name that evaluates to a numeric column in
<code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>iic_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Author(s)</h3>

<p>Joyce Cahoon
</p>


<h3>References</h3>

<p>Toropova, A. and Toropov, A. (2017). &quot;The index of ideality
of correlation. A criterion of predictability of QSAR models for skin
permeability?&quot; <em>Science of the Total Environment</em>. 586: 466-472.
</p>


<h3>See Also</h3>

<p>Other numeric metrics: 
<code><a href="#topic+ccc">ccc</a>()</code>,
<code><a href="#topic+huber_loss_pseudo">huber_loss_pseudo</a>()</code>,
<code><a href="#topic+huber_loss">huber_loss</a>()</code>,
<code><a href="#topic+mae">mae</a>()</code>,
<code><a href="#topic+mape">mape</a>()</code>,
<code><a href="#topic+mase">mase</a>()</code>,
<code><a href="#topic+mpe">mpe</a>()</code>,
<code><a href="#topic+msd">msd</a>()</code>,
<code><a href="#topic+poisson_log_loss">poisson_log_loss</a>()</code>,
<code><a href="#topic+rmse">rmse</a>()</code>,
<code><a href="#topic+rpd">rpd</a>()</code>,
<code><a href="#topic+rpiq">rpiq</a>()</code>,
<code><a href="#topic+rsq_trad">rsq_trad</a>()</code>,
<code><a href="#topic+rsq">rsq</a>()</code>,
<code><a href="#topic+smape">smape</a>()</code>
</p>
<p>Other accuracy metrics: 
<code><a href="#topic+ccc">ccc</a>()</code>,
<code><a href="#topic+huber_loss_pseudo">huber_loss_pseudo</a>()</code>,
<code><a href="#topic+huber_loss">huber_loss</a>()</code>,
<code><a href="#topic+mae">mae</a>()</code>,
<code><a href="#topic+mape">mape</a>()</code>,
<code><a href="#topic+mase">mase</a>()</code>,
<code><a href="#topic+mpe">mpe</a>()</code>,
<code><a href="#topic+msd">msd</a>()</code>,
<code><a href="#topic+poisson_log_loss">poisson_log_loss</a>()</code>,
<code><a href="#topic+rmse">rmse</a>()</code>,
<code><a href="#topic+smape">smape</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Supply truth and predictions as bare column names
iic(solubility_test, solubility, prediction)

library(dplyr)

set.seed(1234)
size &lt;- 100
times &lt;- 10

# create 10 resamples
solubility_resampled &lt;- bind_rows(
  replicate(
    n = times,
    expr = sample_n(solubility_test, size, replace = TRUE),
    simplify = FALSE
  ),
  .id = "resample"
)

# Compute the metric by group
metric_results &lt;- solubility_resampled %&gt;%
  group_by(resample) %&gt;%
  iic(solubility, prediction)

metric_results

# Resampled mean estimate
metric_results %&gt;%
  summarise(avg_estimate = mean(.estimate))
</code></pre>

<hr>
<h2 id='j_index'>J-index</h2><span id='topic+j_index'></span><span id='topic+j_index.data.frame'></span><span id='topic+j_index_vec'></span>

<h3>Description</h3>

<p>Youden's J statistic is defined as:
</p>
<p><code><a href="#topic+sens">sens()</a></code> + <code><a href="#topic+spec">spec()</a></code> - 1
</p>
<p>A related metric is Informedness, see the Details section for the relationship.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>j_index(data, ...)

## S3 method for class 'data.frame'
j_index(
  data,
  truth,
  estimate,
  estimator = NULL,
  na_rm = TRUE,
  case_weights = NULL,
  event_level = yardstick_event_level(),
  ...
)

j_index_vec(
  truth,
  estimate,
  estimator = NULL,
  na_rm = TRUE,
  case_weights = NULL,
  event_level = yardstick_event_level(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="j_index_+3A_data">data</code></td>
<td>
<p>Either a <code>data.frame</code> containing the columns specified by the
<code>truth</code> and <code>estimate</code> arguments, or a <code>table</code>/<code>matrix</code> where the true
class results should be in the columns of the table.</p>
</td></tr>
<tr><td><code id="j_index_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="j_index_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a <code>factor</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="j_index_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predicted class
results (that is also <code>factor</code>). As with <code>truth</code> this can be
specified different ways but the primary method is to use an
unquoted variable name. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="j_index_+3A_estimator">estimator</code></td>
<td>
<p>One of: <code>"binary"</code>, <code>"macro"</code>, <code>"macro_weighted"</code>,
or <code>"micro"</code> to specify the type of averaging to be done. <code>"binary"</code> is
only relevant for the two class case. The other three are general methods
for calculating multiclass metrics. The default will automatically choose
<code>"binary"</code> or <code>"macro"</code> based on <code>estimate</code>.</p>
</td></tr>
<tr><td><code id="j_index_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="j_index_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights.
This should be an unquoted column name that evaluates to a numeric column
in <code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
<tr><td><code id="j_index_+3A_event_level">event_level</code></td>
<td>
<p>A single string. Either <code>"first"</code> or <code>"second"</code> to specify
which level of <code>truth</code> to consider as the &quot;event&quot;. This argument is only
applicable when <code>estimator = "binary"</code>. The default uses an internal helper
that defaults to <code>"first"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The value of the J-index ranges from [0, 1] and is <code>1</code> when there are
no false positives and no false negatives.
</p>
<p>The binary version of J-index is equivalent to the binary concept of
Informedness. Macro-weighted J-index is equivalent to multiclass informedness
as defined in Powers, David M W (2011), equation (42).
</p>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>j_index_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Relevant Level</h3>

<p>There is no common convention on which factor level should
automatically be considered the &quot;event&quot; or &quot;positive&quot; result
when computing binary classification metrics. In <code>yardstick</code>, the default
is to use the <em>first</em> level. To alter this, change the argument
<code>event_level</code> to <code>"second"</code> to consider the <em>last</em> level of the factor the
level of interest. For multiclass extensions involving one-vs-all
comparisons (such as macro averaging), this option is ignored and
the &quot;one&quot; level is always the relevant result.
</p>


<h3>Multiclass</h3>

<p>Macro, micro, and macro-weighted averaging is available for this metric.
The default is to select macro averaging if a <code>truth</code> factor with more
than 2 levels is provided. Otherwise, a standard binary calculation is done.
See <code>vignette("multiclass", "yardstick")</code> for more information.
</p>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>References</h3>

<p>Youden, W.J. (1950). &quot;Index for rating diagnostic tests&quot;. Cancer. 3: 32-35.
</p>
<p>Powers, David M W (2011). &quot;Evaluation: From Precision, Recall and F-Score to
ROC, Informedness, Markedness and Correlation&quot;. Journal of Machine Learning
Technologies. 2 (1): 37-63.
</p>


<h3>See Also</h3>

<p>Other class metrics: 
<code><a href="#topic+accuracy">accuracy</a>()</code>,
<code><a href="#topic+bal_accuracy">bal_accuracy</a>()</code>,
<code><a href="#topic+detection_prevalence">detection_prevalence</a>()</code>,
<code><a href="#topic+f_meas">f_meas</a>()</code>,
<code><a href="#topic+kap">kap</a>()</code>,
<code><a href="#topic+mcc">mcc</a>()</code>,
<code><a href="#topic+npv">npv</a>()</code>,
<code><a href="#topic+ppv">ppv</a>()</code>,
<code><a href="#topic+precision">precision</a>()</code>,
<code><a href="#topic+recall">recall</a>()</code>,
<code><a href="#topic+sens">sens</a>()</code>,
<code><a href="#topic+spec">spec</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Two class
data("two_class_example")
j_index(two_class_example, truth, predicted)

# Multiclass
library(dplyr)
data(hpc_cv)

hpc_cv %&gt;%
  filter(Resample == "Fold01") %&gt;%
  j_index(obs, pred)

# Groups are respected
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  j_index(obs, pred)

# Weighted macro averaging
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  j_index(obs, pred, estimator = "macro_weighted")

# Vector version
j_index_vec(
  two_class_example$truth,
  two_class_example$predicted
)

# Making Class2 the "relevant" level
j_index_vec(
  two_class_example$truth,
  two_class_example$predicted,
  event_level = "second"
)
</code></pre>

<hr>
<h2 id='kap'>Kappa</h2><span id='topic+kap'></span><span id='topic+kap.data.frame'></span><span id='topic+kap_vec'></span>

<h3>Description</h3>

<p>Kappa is a similar measure to <code><a href="#topic+accuracy">accuracy()</a></code>, but is normalized by
the accuracy that would be expected by chance alone and is very useful
when one or more classes have large frequency distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kap(data, ...)

## S3 method for class 'data.frame'
kap(
  data,
  truth,
  estimate,
  weighting = "none",
  na_rm = TRUE,
  case_weights = NULL,
  ...
)

kap_vec(
  truth,
  estimate,
  weighting = "none",
  na_rm = TRUE,
  case_weights = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kap_+3A_data">data</code></td>
<td>
<p>Either a <code>data.frame</code> containing the columns specified by the
<code>truth</code> and <code>estimate</code> arguments, or a <code>table</code>/<code>matrix</code> where the true
class results should be in the columns of the table.</p>
</td></tr>
<tr><td><code id="kap_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="kap_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a <code>factor</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="kap_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predicted class
results (that is also <code>factor</code>). As with <code>truth</code> this can be
specified different ways but the primary method is to use an
unquoted variable name. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="kap_+3A_weighting">weighting</code></td>
<td>
<p>A weighting to apply when computing the scores. One of:
<code>"none"</code>, <code>"linear"</code>, or <code>"quadratic"</code>. Linear and quadratic weighting
penalizes mis-predictions that are &quot;far away&quot; from the true value. Note
that distance is judged based on the ordering of the levels in <code>truth</code> and
<code>estimate</code>. It is recommended to provide ordered factors for <code>truth</code> and
<code>estimate</code> to explicitly code the ordering, but this is not required.
</p>
<p>In the binary case, all 3 weightings produce the same value, since it is
only ever possible to be 1 unit away from the true value.</p>
</td></tr>
<tr><td><code id="kap_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="kap_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights.
This should be an unquoted column name that evaluates to a numeric column
in <code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>kap_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Multiclass</h3>

<p>Kappa extends naturally to multiclass scenarios. Because
of this, macro and micro averaging are not implemented.
</p>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>
<p>Jon Harmon
</p>


<h3>References</h3>

<p>Cohen, J. (1960). &quot;A coefficient of agreement for nominal
scales&quot;. <em>Educational and Psychological Measurement</em>. 20 (1): 37-46.
</p>
<p>Cohen, J. (1968). &quot;Weighted kappa: Nominal scale agreement provision for
scaled disagreement or partial credit&quot;. <em>Psychological
Bulletin</em>. 70 (4): 213-220.
</p>


<h3>See Also</h3>

<p>Other class metrics: 
<code><a href="#topic+accuracy">accuracy</a>()</code>,
<code><a href="#topic+bal_accuracy">bal_accuracy</a>()</code>,
<code><a href="#topic+detection_prevalence">detection_prevalence</a>()</code>,
<code><a href="#topic+f_meas">f_meas</a>()</code>,
<code><a href="#topic+j_index">j_index</a>()</code>,
<code><a href="#topic+mcc">mcc</a>()</code>,
<code><a href="#topic+npv">npv</a>()</code>,
<code><a href="#topic+ppv">ppv</a>()</code>,
<code><a href="#topic+precision">precision</a>()</code>,
<code><a href="#topic+recall">recall</a>()</code>,
<code><a href="#topic+sens">sens</a>()</code>,
<code><a href="#topic+spec">spec</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)
data("two_class_example")
data("hpc_cv")

# Two class
kap(two_class_example, truth, predicted)

# Multiclass
# kap() has a natural multiclass extension
hpc_cv %&gt;%
  filter(Resample == "Fold01") %&gt;%
  kap(obs, pred)

# Groups are respected
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  kap(obs, pred)
</code></pre>

<hr>
<h2 id='lift_curve'>Lift curve</h2><span id='topic+lift_curve'></span><span id='topic+lift_curve.data.frame'></span>

<h3>Description</h3>

<p><code>lift_curve()</code> constructs the full lift curve and returns a tibble. See
<code><a href="#topic+gain_curve">gain_curve()</a></code> for a closely related concept.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lift_curve(data, ...)

## S3 method for class 'data.frame'
lift_curve(
  data,
  truth,
  ...,
  na_rm = TRUE,
  event_level = yardstick_event_level(),
  case_weights = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lift_curve_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by <code>truth</code> and
<code>...</code>.</p>
</td></tr>
<tr><td><code id="lift_curve_+3A_...">...</code></td>
<td>
<p>A set of unquoted column names or one or more
<code>dplyr</code> selector functions to choose which variables contain the
class probabilities. If <code>truth</code> is binary, only 1 column should be selected,
and it should correspond to the value of <code>event_level</code>. Otherwise, there
should be as many columns as factor levels of <code>truth</code> and the ordering of
the columns should be the same as the factor levels of <code>truth</code>.</p>
</td></tr>
<tr><td><code id="lift_curve_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a <code>factor</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="lift_curve_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="lift_curve_+3A_event_level">event_level</code></td>
<td>
<p>A single string. Either <code>"first"</code> or <code>"second"</code> to specify
which level of <code>truth</code> to consider as the &quot;event&quot;. This argument is only
applicable when <code>estimator = "binary"</code>. The default uses an internal helper
that defaults to <code>"first"</code>.</p>
</td></tr>
<tr><td><code id="lift_curve_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights.
This should be an unquoted column name that evaluates to a numeric column
in <code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There is a <code><a href="ggplot2.html#topic+autoplot">ggplot2::autoplot()</a></code> method for quickly visualizing the curve.
This works for binary and multiclass output, and also works with grouped data
(i.e. from resamples). See the examples.
</p>


<h3>Value</h3>

<p>A tibble with class <code>lift_df</code> or <code>lift_grouped_df</code> having
columns:
</p>

<ul>
<li> <p><code>.n</code> The index of the current sample.
</p>
</li>
<li> <p><code>.n_events</code> The index of the current <em>unique</em> sample. Values with repeated
<code>estimate</code> values are given identical indices in this column.
</p>
</li>
<li> <p><code>.percent_tested</code> The cumulative percentage of values tested.
</p>
</li>
<li> <p><code>.lift</code> First calculate the cumulative percentage of true results relative
to the total number of true results. Then divide that by <code>.percent_tested</code>.
</p>
</li></ul>

<p>If using the <code>case_weights</code> argument, all of the above columns will be
weighted. This makes the most sense with frequency weights, which are integer
weights representing the number of times a particular observation should be
repeated.
</p>


<h3>Gain and Lift Curves</h3>

<p>The motivation behind cumulative gain and lift charts is as a visual method
to determine the effectiveness of a model when compared to the results one
might expect without a model. As an example, without a model, if you were to
advertise to a random 10% of your customer base, then you might expect to
capture 10% of the of the total number of positive responses had you
advertised to your entire customer base. Given a model that predicts which
customers are more likely to respond, the hope is that you can more
accurately target 10% of your customer base and capture <code>&gt;</code>10% of the total
number of positive responses.
</p>
<p>The calculation to construct lift curves is as follows:
</p>

<ol>
<li> <p><code>truth</code> and <code>estimate</code> are placed in descending order by the <code>estimate</code>
values (<code>estimate</code> here is a single column supplied in <code>...</code>).
</p>
</li>
<li><p> The cumulative number of samples with true results relative to the
entire number of true results are found.
</p>
</li>
<li><p> The cumulative <code style="white-space: pre;">&#8288;%&#8288;</code> found is divided by the cumulative <code style="white-space: pre;">&#8288;%&#8288;</code> tested
to construct the lift value. This ratio represents the factor of improvement
over an uninformed model. Values <code>&gt;</code>1 represent a valuable model. This is the
y-axis of the lift chart.
</p>
</li></ol>



<h3>Multiclass</h3>

<p>If a multiclass <code>truth</code> column is provided, a one-vs-all
approach will be taken to calculate multiple curves, one per level.
In this case, there will be an additional column, <code>.level</code>,
identifying the &quot;one&quot; column in the one-vs-all calculation.
</p>


<h3>Relevant Level</h3>

<p>There is no common convention on which factor level should
automatically be considered the &quot;event&quot; or &quot;positive&quot; result
when computing binary classification metrics. In <code>yardstick</code>, the default
is to use the <em>first</em> level. To alter this, change the argument
<code>event_level</code> to <code>"second"</code> to consider the <em>last</em> level of the factor the
level of interest. For multiclass extensions involving one-vs-all
comparisons (such as macro averaging), this option is ignored and
the &quot;one&quot; level is always the relevant result.
</p>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>See Also</h3>

<p>Other curve metrics: 
<code><a href="#topic+gain_curve">gain_curve</a>()</code>,
<code><a href="#topic+pr_curve">pr_curve</a>()</code>,
<code><a href="#topic+roc_curve">roc_curve</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># ---------------------------------------------------------------------------
# Two class example

# `truth` is a 2 level factor. The first level is `"Class1"`, which is the
# "event of interest" by default in yardstick. See the Relevant Level
# section above.
data(two_class_example)

# Binary metrics using class probabilities take a factor `truth` column,
# and a single class probability column containing the probabilities of
# the event of interest. Here, since `"Class1"` is the first level of
# `"truth"`, it is the event of interest and we pass in probabilities for it.
lift_curve(two_class_example, truth, Class1)

# ---------------------------------------------------------------------------
# `autoplot()`

library(ggplot2)
library(dplyr)

# Use autoplot to visualize
autoplot(lift_curve(two_class_example, truth, Class1))

# Multiclass one-vs-all approach
# One curve per level
hpc_cv %&gt;%
  filter(Resample == "Fold01") %&gt;%
  lift_curve(obs, VF:L) %&gt;%
  autoplot()

# Same as above, but will all of the resamples
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  lift_curve(obs, VF:L) %&gt;%
  autoplot()

</code></pre>

<hr>
<h2 id='lung_surv'>Survival Analysis Results</h2><span id='topic+lung_surv'></span>

<h3>Description</h3>

<p>Survival Analysis Results
</p>


<h3>Details</h3>

<p>These data contain plausible results from applying predictive
survival models to the <a href="survival.html#topic+lung">lung</a> data set using the censored package.
</p>


<h3>Value</h3>

<table>
<tr><td><code>lung_surv</code></td>
<td>
<p>a data frame</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(lung_surv)
str(lung_surv)

# `surv_obj` is a `Surv()` object
</code></pre>

<hr>
<h2 id='mae'>Mean absolute error</h2><span id='topic+mae'></span><span id='topic+mae.data.frame'></span><span id='topic+mae_vec'></span>

<h3>Description</h3>

<p>Calculate the mean absolute error. This metric is in the same units as the
original data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mae(data, ...)

## S3 method for class 'data.frame'
mae(data, truth, estimate, na_rm = TRUE, case_weights = NULL, ...)

mae_vec(truth, estimate, na_rm = TRUE, case_weights = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mae_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by the <code>truth</code>
and <code>estimate</code> arguments.</p>
</td></tr>
<tr><td><code id="mae_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="mae_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true results
(that is <code>numeric</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>numeric</code> vector.</p>
</td></tr>
<tr><td><code id="mae_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predicted
results (that is also <code>numeric</code>). As with <code>truth</code> this can be
specified different ways but the primary method is to use an
unquoted variable name. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>numeric</code> vector.</p>
</td></tr>
<tr><td><code id="mae_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="mae_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights. This
should be an unquoted column name that evaluates to a numeric column in
<code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>mae_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>See Also</h3>

<p>Other numeric metrics: 
<code><a href="#topic+ccc">ccc</a>()</code>,
<code><a href="#topic+huber_loss_pseudo">huber_loss_pseudo</a>()</code>,
<code><a href="#topic+huber_loss">huber_loss</a>()</code>,
<code><a href="#topic+iic">iic</a>()</code>,
<code><a href="#topic+mape">mape</a>()</code>,
<code><a href="#topic+mase">mase</a>()</code>,
<code><a href="#topic+mpe">mpe</a>()</code>,
<code><a href="#topic+msd">msd</a>()</code>,
<code><a href="#topic+poisson_log_loss">poisson_log_loss</a>()</code>,
<code><a href="#topic+rmse">rmse</a>()</code>,
<code><a href="#topic+rpd">rpd</a>()</code>,
<code><a href="#topic+rpiq">rpiq</a>()</code>,
<code><a href="#topic+rsq_trad">rsq_trad</a>()</code>,
<code><a href="#topic+rsq">rsq</a>()</code>,
<code><a href="#topic+smape">smape</a>()</code>
</p>
<p>Other accuracy metrics: 
<code><a href="#topic+ccc">ccc</a>()</code>,
<code><a href="#topic+huber_loss_pseudo">huber_loss_pseudo</a>()</code>,
<code><a href="#topic+huber_loss">huber_loss</a>()</code>,
<code><a href="#topic+iic">iic</a>()</code>,
<code><a href="#topic+mape">mape</a>()</code>,
<code><a href="#topic+mase">mase</a>()</code>,
<code><a href="#topic+mpe">mpe</a>()</code>,
<code><a href="#topic+msd">msd</a>()</code>,
<code><a href="#topic+poisson_log_loss">poisson_log_loss</a>()</code>,
<code><a href="#topic+rmse">rmse</a>()</code>,
<code><a href="#topic+smape">smape</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Supply truth and predictions as bare column names
mae(solubility_test, solubility, prediction)

library(dplyr)

set.seed(1234)
size &lt;- 100
times &lt;- 10

# create 10 resamples
solubility_resampled &lt;- bind_rows(
  replicate(
    n = times,
    expr = sample_n(solubility_test, size, replace = TRUE),
    simplify = FALSE
  ),
  .id = "resample"
)

# Compute the metric by group
metric_results &lt;- solubility_resampled %&gt;%
  group_by(resample) %&gt;%
  mae(solubility, prediction)

metric_results

# Resampled mean estimate
metric_results %&gt;%
  summarise(avg_estimate = mean(.estimate))
</code></pre>

<hr>
<h2 id='mape'>Mean absolute percent error</h2><span id='topic+mape'></span><span id='topic+mape.data.frame'></span><span id='topic+mape_vec'></span>

<h3>Description</h3>

<p>Calculate the mean absolute percentage error. This metric is in <em>relative
units</em>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mape(data, ...)

## S3 method for class 'data.frame'
mape(data, truth, estimate, na_rm = TRUE, case_weights = NULL, ...)

mape_vec(truth, estimate, na_rm = TRUE, case_weights = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mape_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by the <code>truth</code>
and <code>estimate</code> arguments.</p>
</td></tr>
<tr><td><code id="mape_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="mape_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true results
(that is <code>numeric</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>numeric</code> vector.</p>
</td></tr>
<tr><td><code id="mape_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predicted
results (that is also <code>numeric</code>). As with <code>truth</code> this can be
specified different ways but the primary method is to use an
unquoted variable name. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>numeric</code> vector.</p>
</td></tr>
<tr><td><code id="mape_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="mape_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights. This
should be an unquoted column name that evaluates to a numeric column in
<code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that a value of <code>Inf</code> is returned for <code>mape()</code> when the
observed value is negative.
</p>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>mape_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>See Also</h3>

<p>Other numeric metrics: 
<code><a href="#topic+ccc">ccc</a>()</code>,
<code><a href="#topic+huber_loss_pseudo">huber_loss_pseudo</a>()</code>,
<code><a href="#topic+huber_loss">huber_loss</a>()</code>,
<code><a href="#topic+iic">iic</a>()</code>,
<code><a href="#topic+mae">mae</a>()</code>,
<code><a href="#topic+mase">mase</a>()</code>,
<code><a href="#topic+mpe">mpe</a>()</code>,
<code><a href="#topic+msd">msd</a>()</code>,
<code><a href="#topic+poisson_log_loss">poisson_log_loss</a>()</code>,
<code><a href="#topic+rmse">rmse</a>()</code>,
<code><a href="#topic+rpd">rpd</a>()</code>,
<code><a href="#topic+rpiq">rpiq</a>()</code>,
<code><a href="#topic+rsq_trad">rsq_trad</a>()</code>,
<code><a href="#topic+rsq">rsq</a>()</code>,
<code><a href="#topic+smape">smape</a>()</code>
</p>
<p>Other accuracy metrics: 
<code><a href="#topic+ccc">ccc</a>()</code>,
<code><a href="#topic+huber_loss_pseudo">huber_loss_pseudo</a>()</code>,
<code><a href="#topic+huber_loss">huber_loss</a>()</code>,
<code><a href="#topic+iic">iic</a>()</code>,
<code><a href="#topic+mae">mae</a>()</code>,
<code><a href="#topic+mase">mase</a>()</code>,
<code><a href="#topic+mpe">mpe</a>()</code>,
<code><a href="#topic+msd">msd</a>()</code>,
<code><a href="#topic+poisson_log_loss">poisson_log_loss</a>()</code>,
<code><a href="#topic+rmse">rmse</a>()</code>,
<code><a href="#topic+smape">smape</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Supply truth and predictions as bare column names
mape(solubility_test, solubility, prediction)

library(dplyr)

set.seed(1234)
size &lt;- 100
times &lt;- 10

# create 10 resamples
solubility_resampled &lt;- bind_rows(
  replicate(
    n = times,
    expr = sample_n(solubility_test, size, replace = TRUE),
    simplify = FALSE
  ),
  .id = "resample"
)

# Compute the metric by group
metric_results &lt;- solubility_resampled %&gt;%
  group_by(resample) %&gt;%
  mape(solubility, prediction)

metric_results

# Resampled mean estimate
metric_results %&gt;%
  summarise(avg_estimate = mean(.estimate))
</code></pre>

<hr>
<h2 id='mase'>Mean absolute scaled error</h2><span id='topic+mase'></span><span id='topic+mase.data.frame'></span><span id='topic+mase_vec'></span>

<h3>Description</h3>

<p>Calculate the mean absolute scaled error. This metric is <em>scale independent</em>
and <em>symmetric</em>. It is generally used for comparing forecast error in
time series settings. Due to the time series nature of this metric, it
is necessary to order observations in ascending order by time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mase(data, ...)

## S3 method for class 'data.frame'
mase(
  data,
  truth,
  estimate,
  m = 1L,
  mae_train = NULL,
  na_rm = TRUE,
  case_weights = NULL,
  ...
)

mase_vec(
  truth,
  estimate,
  m = 1L,
  mae_train = NULL,
  na_rm = TRUE,
  case_weights = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mase_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by the <code>truth</code>
and <code>estimate</code> arguments.</p>
</td></tr>
<tr><td><code id="mase_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="mase_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true results
(that is <code>numeric</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>numeric</code> vector.</p>
</td></tr>
<tr><td><code id="mase_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predicted
results (that is also <code>numeric</code>). As with <code>truth</code> this can be
specified different ways but the primary method is to use an
unquoted variable name. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>numeric</code> vector.</p>
</td></tr>
<tr><td><code id="mase_+3A_m">m</code></td>
<td>
<p>An integer value of the number of lags used to calculate the
in-sample seasonal naive error. The default is used for non-seasonal time
series. If each observation was at the daily level and the data showed weekly
seasonality, then <code>m = 7L</code> would be a reasonable choice for a 7-day seasonal
naive calculation.</p>
</td></tr>
<tr><td><code id="mase_+3A_mae_train">mae_train</code></td>
<td>
<p>A numeric value which allows the user to provide the
in-sample seasonal naive mean absolute error. If this value is not provided,
then the out-of-sample seasonal naive mean absolute error will be calculated
from <code>truth</code> and will be used instead.</p>
</td></tr>
<tr><td><code id="mase_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="mase_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights. This
should be an unquoted column name that evaluates to a numeric column in
<code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>mase()</code> is different from most numeric metrics. The original implementation
of <code>mase()</code> calls for using the <em>in-sample</em> naive mean absolute error to
compute scaled errors with. It uses this instead of the out-of-sample error
because there is a chance that the out-of-sample error cannot be computed
when forecasting a very short horizon (i.e. the out of sample size is only
1 or 2). However, <code>yardstick</code> only knows about the out-of-sample <code>truth</code> and
<code>estimate</code> values. Because of this, the out-of-sample error is used in the
computation by default. If the in-sample naive mean absolute error is
required and known, it can be passed through in the <code>mae_train</code> argument
and it will be used instead. If the in-sample data is available, the
naive mean absolute error can easily be computed with
<code>mae(data, truth, lagged_truth)</code>.
</p>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>mase_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Author(s)</h3>

<p>Alex Hallam
</p>


<h3>References</h3>

<p>Rob J. Hyndman (2006). ANOTHER LOOK AT FORECAST-ACCURACY METRICS FOR
INTERMITTENT DEMAND. <em>Foresight</em>, 4, 46.
</p>


<h3>See Also</h3>

<p>Other numeric metrics: 
<code><a href="#topic+ccc">ccc</a>()</code>,
<code><a href="#topic+huber_loss_pseudo">huber_loss_pseudo</a>()</code>,
<code><a href="#topic+huber_loss">huber_loss</a>()</code>,
<code><a href="#topic+iic">iic</a>()</code>,
<code><a href="#topic+mae">mae</a>()</code>,
<code><a href="#topic+mape">mape</a>()</code>,
<code><a href="#topic+mpe">mpe</a>()</code>,
<code><a href="#topic+msd">msd</a>()</code>,
<code><a href="#topic+poisson_log_loss">poisson_log_loss</a>()</code>,
<code><a href="#topic+rmse">rmse</a>()</code>,
<code><a href="#topic+rpd">rpd</a>()</code>,
<code><a href="#topic+rpiq">rpiq</a>()</code>,
<code><a href="#topic+rsq_trad">rsq_trad</a>()</code>,
<code><a href="#topic+rsq">rsq</a>()</code>,
<code><a href="#topic+smape">smape</a>()</code>
</p>
<p>Other accuracy metrics: 
<code><a href="#topic+ccc">ccc</a>()</code>,
<code><a href="#topic+huber_loss_pseudo">huber_loss_pseudo</a>()</code>,
<code><a href="#topic+huber_loss">huber_loss</a>()</code>,
<code><a href="#topic+iic">iic</a>()</code>,
<code><a href="#topic+mae">mae</a>()</code>,
<code><a href="#topic+mape">mape</a>()</code>,
<code><a href="#topic+mpe">mpe</a>()</code>,
<code><a href="#topic+msd">msd</a>()</code>,
<code><a href="#topic+poisson_log_loss">poisson_log_loss</a>()</code>,
<code><a href="#topic+rmse">rmse</a>()</code>,
<code><a href="#topic+smape">smape</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Supply truth and predictions as bare column names
mase(solubility_test, solubility, prediction)

library(dplyr)

set.seed(1234)
size &lt;- 100
times &lt;- 10

# create 10 resamples
solubility_resampled &lt;- bind_rows(
  replicate(
    n = times,
    expr = sample_n(solubility_test, size, replace = TRUE),
    simplify = FALSE
  ),
  .id = "resample"
)

# Compute the metric by group
metric_results &lt;- solubility_resampled %&gt;%
  group_by(resample) %&gt;%
  mase(solubility, prediction)

metric_results

# Resampled mean estimate
metric_results %&gt;%
  summarise(avg_estimate = mean(.estimate))
</code></pre>

<hr>
<h2 id='mcc'>Matthews correlation coefficient</h2><span id='topic+mcc'></span><span id='topic+mcc.data.frame'></span><span id='topic+mcc_vec'></span>

<h3>Description</h3>

<p>Matthews correlation coefficient
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcc(data, ...)

## S3 method for class 'data.frame'
mcc(data, truth, estimate, na_rm = TRUE, case_weights = NULL, ...)

mcc_vec(truth, estimate, na_rm = TRUE, case_weights = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcc_+3A_data">data</code></td>
<td>
<p>Either a <code>data.frame</code> containing the columns specified by the
<code>truth</code> and <code>estimate</code> arguments, or a <code>table</code>/<code>matrix</code> where the true
class results should be in the columns of the table.</p>
</td></tr>
<tr><td><code id="mcc_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="mcc_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a <code>factor</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="mcc_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predicted class
results (that is also <code>factor</code>). As with <code>truth</code> this can be
specified different ways but the primary method is to use an
unquoted variable name. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="mcc_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="mcc_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights.
This should be an unquoted column name that evaluates to a numeric column
in <code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>mcc_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Relevant Level</h3>

<p>There is no common convention on which factor level should
automatically be considered the &quot;event&quot; or &quot;positive&quot; result
when computing binary classification metrics. In <code>yardstick</code>, the default
is to use the <em>first</em> level. To alter this, change the argument
<code>event_level</code> to <code>"second"</code> to consider the <em>last</em> level of the factor the
level of interest. For multiclass extensions involving one-vs-all
comparisons (such as macro averaging), this option is ignored and
the &quot;one&quot; level is always the relevant result.
</p>


<h3>Multiclass</h3>

<p><code>mcc()</code> has a known multiclass generalization and that is computed
automatically if a factor with more than 2 levels is provided. Because
of this, no averaging methods are provided.
</p>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>References</h3>

<p>Giuseppe, J. (2012). &quot;A Comparison of MCC and CEN Error
Measures in Multi-Class Prediction&quot;. <em>PLOS ONE</em>. Vol 7, Iss 8, e41882.
</p>


<h3>See Also</h3>

<p>Other class metrics: 
<code><a href="#topic+accuracy">accuracy</a>()</code>,
<code><a href="#topic+bal_accuracy">bal_accuracy</a>()</code>,
<code><a href="#topic+detection_prevalence">detection_prevalence</a>()</code>,
<code><a href="#topic+f_meas">f_meas</a>()</code>,
<code><a href="#topic+j_index">j_index</a>()</code>,
<code><a href="#topic+kap">kap</a>()</code>,
<code><a href="#topic+npv">npv</a>()</code>,
<code><a href="#topic+ppv">ppv</a>()</code>,
<code><a href="#topic+precision">precision</a>()</code>,
<code><a href="#topic+recall">recall</a>()</code>,
<code><a href="#topic+sens">sens</a>()</code>,
<code><a href="#topic+spec">spec</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)
data("two_class_example")
data("hpc_cv")

# Two class
mcc(two_class_example, truth, predicted)

# Multiclass
# mcc() has a natural multiclass extension
hpc_cv %&gt;%
  filter(Resample == "Fold01") %&gt;%
  mcc(obs, pred)

# Groups are respected
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  mcc(obs, pred)
</code></pre>

<hr>
<h2 id='metric_set'>Combine metric functions</h2><span id='topic+metric_set'></span>

<h3>Description</h3>

<p><code>metric_set()</code> allows you to combine multiple metric functions together
into a new function that calculates all of them at once.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>metric_set(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="metric_set_+3A_...">...</code></td>
<td>
<p>The bare names of the functions to be included in the metric set.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>All functions must be either:
</p>

<ul>
<li><p> Only numeric metrics
</p>
</li>
<li><p> A mix of class metrics or class prob metrics
</p>
</li>
<li><p> A mix of dynamic, integrated, and static survival metrics
</p>
</li></ul>

<p>For instance, <code>rmse()</code> can be used with <code>mae()</code> because they
are numeric metrics, but not with <code>accuracy()</code> because it is a classification
metric. But <code>accuracy()</code> can be used with <code>roc_auc()</code>.
</p>
<p>The returned metric function will have a different argument list
depending on whether numeric metrics or a mix of class/prob metrics were
passed in.
</p>
<div class="sourceCode"><pre># Numeric metric set signature:
fn(
  data,
  truth,
  estimate,
  na_rm = TRUE,
  case_weights = NULL,
  ...
)

# Class / prob metric set signature:
fn(
  data,
  truth,
  ...,
  estimate,
  estimator = NULL,
  na_rm = TRUE,
  event_level = yardstick_event_level(),
  case_weights = NULL
)

# Dynamic / integrated / static survival metric set signature:
fn(
  data,
  truth,
  ...,
  estimate,
  na_rm = TRUE,
  case_weights = NULL
)
</pre></div>
<p>When mixing class and class prob metrics, pass in the hard predictions
(the factor column) as the named argument <code>estimate</code>, and the soft
predictions (the class probability columns) as bare column names or
<code>tidyselect</code> selectors to <code>...</code>.
</p>
<p>When mixing dynamic, integrated, and static survival metrics, pass in the
time predictions as the named argument <code>estimate</code>, and the survival
predictions as bare column names or <code>tidyselect</code> selectors to <code>...</code>.
</p>
<p>If <code>metric_tweak()</code> has been used to &quot;tweak&quot; one of these arguments, like
<code>estimator</code> or <code>event_level</code>, then the tweaked version wins. This allows you
to set the estimator on a metric by metric basis and still use it in a
<code>metric_set()</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+metrics">metrics()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)

# Multiple regression metrics
multi_metric &lt;- metric_set(rmse, rsq, ccc)

# The returned function has arguments:
# fn(data, truth, estimate, na_rm = TRUE, ...)
multi_metric(solubility_test, truth = solubility, estimate = prediction)

# Groups are respected on the new metric function
class_metrics &lt;- metric_set(accuracy, kap)

hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  class_metrics(obs, estimate = pred)

# ---------------------------------------------------------------------------

# If you need to set options for certain metrics,
# do so by wrapping the metric and setting the options inside the wrapper,
# passing along truth and estimate as quoted arguments.
# Then add on the function class of the underlying wrapped function,
# and the direction of optimization.
ccc_with_bias &lt;- function(data, truth, estimate, na_rm = TRUE, ...) {
  ccc(
    data = data,
    truth = !!rlang::enquo(truth),
    estimate = !!rlang::enquo(estimate),
    # set bias = TRUE
    bias = TRUE,
    na_rm = na_rm,
    ...
  )
}

# Use `new_numeric_metric()` to formalize this new metric function
ccc_with_bias &lt;- new_numeric_metric(ccc_with_bias, "maximize")

multi_metric2 &lt;- metric_set(rmse, rsq, ccc_with_bias)

multi_metric2(solubility_test, truth = solubility, estimate = prediction)

# ---------------------------------------------------------------------------
# A class probability example:

# Note that, when given class or class prob functions,
# metric_set() returns a function with signature:
# fn(data, truth, ..., estimate)
# to be able to mix class and class prob metrics.

# You must provide the `estimate` column by explicitly naming
# the argument

class_and_probs_metrics &lt;- metric_set(roc_auc, pr_auc, accuracy)

hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  class_and_probs_metrics(obs, VF:L, estimate = pred)

</code></pre>

<hr>
<h2 id='metric_summarizer'>Developer function for summarizing new metrics</h2><span id='topic+metric_summarizer'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#deprecated"><img src="../help/figures/lifecycle-deprecated.svg" alt='[Deprecated]' /></a>
</p>
<p><code>metric_summarizer()</code> has been soft-deprecated as of yardstick 1.2.0. Please
switch to use <code><a href="#topic+class_metric_summarizer">class_metric_summarizer()</a></code>, <code><a href="#topic+numeric_metric_summarizer">numeric_metric_summarizer()</a></code>,
<code><a href="#topic+prob_metric_summarizer">prob_metric_summarizer()</a></code>, or <code><a href="#topic+curve_metric_summarizer">curve_metric_summarizer()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>metric_summarizer(
  metric_nm,
  metric_fn,
  data,
  truth,
  estimate,
  estimator = NULL,
  na_rm = TRUE,
  event_level = NULL,
  case_weights = NULL,
  ...,
  metric_fn_options = list()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="metric_summarizer_+3A_metric_nm">metric_nm</code></td>
<td>
<p>A single character representing the name of the metric to
use in the <code>tibble</code> output. This will be modified to include the type
of averaging if appropriate.</p>
</td></tr>
<tr><td><code id="metric_summarizer_+3A_metric_fn">metric_fn</code></td>
<td>
<p>The vector version of your custom metric function. It
generally takes <code>truth</code>, <code>estimate</code>, <code>na_rm</code>, and any other extra arguments
needed to calculate the metric.</p>
</td></tr>
<tr><td><code id="metric_summarizer_+3A_data">data</code></td>
<td>
<p>The data frame with <code>truth</code> and <code>estimate</code> columns passed
in from the data frame version of your metric function that called
<code>metric_summarizer()</code>.</p>
</td></tr>
<tr><td><code id="metric_summarizer_+3A_truth">truth</code></td>
<td>
<p>The unquoted column name corresponding to the <code>truth</code> column.</p>
</td></tr>
<tr><td><code id="metric_summarizer_+3A_estimate">estimate</code></td>
<td>
<p>Generally, the unquoted column name corresponding to
the <code>estimate</code> column. For metrics that take multiple columns through <code>...</code>
like class probability metrics, this is a result of <code><a href="#topic+dots_to_estimate">dots_to_estimate()</a></code>.</p>
</td></tr>
<tr><td><code id="metric_summarizer_+3A_estimator">estimator</code></td>
<td>
<p>For numeric metrics, this is left as <code>NULL</code> so averaging
is not passed on to the metric function implementation. For classification
metrics, this can either be <code>NULL</code> for the default auto-selection of
averaging (<code>"binary"</code> or <code>"macro"</code>), or a single character to pass along
to the metric implementation describing the kind of averaging to use.</p>
</td></tr>
<tr><td><code id="metric_summarizer_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code> values should be
stripped before the computation proceeds. The removal is executed in
<code>metric_vec_template()</code>.</p>
</td></tr>
<tr><td><code id="metric_summarizer_+3A_event_level">event_level</code></td>
<td>
<p>For numeric metrics, this is left as <code>NULL</code> to prevent
it from being passed on to the metric function implementation. For
classification metrics, this can either be <code>NULL</code> to use the default
<code>event_level</code> value of the <code>metric_fn</code> or a single string of either
<code>"first"</code> or <code>"second"</code> to pass along describing which level should be
considered the &quot;event&quot;.</p>
</td></tr>
<tr><td><code id="metric_summarizer_+3A_case_weights">case_weights</code></td>
<td>
<p>For metrics supporting case weights, an unquoted
column name corresponding to case weights can be passed here. If not <code>NULL</code>,
the case weights will be passed on to <code>metric_fn</code> as the named argument
<code>case_weights</code>.</p>
</td></tr>
<tr><td><code id="metric_summarizer_+3A_...">...</code></td>
<td>
<p>Currently not used. Metric specific options are passed in
through <code>metric_fn_options</code>.</p>
</td></tr>
<tr><td><code id="metric_summarizer_+3A_metric_fn_options">metric_fn_options</code></td>
<td>
<p>A named list of metric specific options. These
are spliced into the metric function call using <code style="white-space: pre;">&#8288;!!!&#8288;</code> from <code>rlang</code>. The
default results in nothing being spliced into the call.</p>
</td></tr>
</table>

<hr>
<h2 id='metric_tweak'>Tweak a metric function</h2><span id='topic+metric_tweak'></span>

<h3>Description</h3>

<p><code>metric_tweak()</code> allows you to tweak an existing metric <code>.fn</code>, giving it a
new <code>.name</code> and setting new optional argument defaults through <code>...</code>. It
is similar to <code>purrr::partial()</code>, but is designed specifically for yardstick
metrics.
</p>
<p><code>metric_tweak()</code> is especially useful when constructing a <code><a href="#topic+metric_set">metric_set()</a></code> for
tuning with the tune package. After the metric set has been constructed,
there is no way to adjust the value of any optional arguments (such as
<code>beta</code> in <code><a href="#topic+f_meas">f_meas()</a></code>). Using <code>metric_tweak()</code>, you can set optional arguments
to custom values ahead of time, before they go into the metric set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>metric_tweak(.name, .fn, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="metric_tweak_+3A_.name">.name</code></td>
<td>
<p>A single string giving the name of the new metric. This will be
used in the <code>".metric"</code> column of the output.</p>
</td></tr>
<tr><td><code id="metric_tweak_+3A_.fn">.fn</code></td>
<td>
<p>An existing yardstick metric function to tweak.</p>
</td></tr>
<tr><td><code id="metric_tweak_+3A_...">...</code></td>
<td>
<p>Name-value pairs specifying which optional arguments to override
and the values to replace them with.
</p>
<p>Arguments <code>data</code>, <code>truth</code>, and <code>estimate</code> are considered <em>protected</em>,
and cannot be overridden, but all other optional arguments can be
altered.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function returned from <code>metric_tweak()</code> only takes <code>...</code> as arguments,
which are passed through to the original <code>.fn</code>. Passing <code>data</code>, <code>truth</code>,
and <code>estimate</code> through by position should generally be safe, but it is
recommended to pass any other optional arguments through by name to ensure
that they are evaluated correctly.
</p>


<h3>Value</h3>

<p>A tweaked version of <code>.fn</code>, updated to use new defaults supplied in <code>...</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>mase12 &lt;- metric_tweak("mase12", mase, m = 12)

# Defaults to `m = 1`
mase(solubility_test, solubility, prediction)

# Updated to use `m = 12`. `mase12()` has this set already.
mase(solubility_test, solubility, prediction, m = 12)
mase12(solubility_test, solubility, prediction)

# This is most useful to set optional argument values ahead of time when
# using a metric set
mase10 &lt;- metric_tweak("mase10", mase, m = 10)
metrics &lt;- metric_set(mase, mase10, mase12)
metrics(solubility_test, solubility, prediction)
</code></pre>

<hr>
<h2 id='metric_vec_template'>Developer function for calling new metrics</h2><span id='topic+metric_vec_template'></span>

<h3>Description</h3>

<p><a href="https://lifecycle.r-lib.org/articles/stages.html#deprecated"><img src="../help/figures/lifecycle-deprecated.svg" alt='[Deprecated]' /></a>
</p>
<p><code>metric_vec_template()</code> has been soft-deprecated as of yardstick 1.2.0.
Please switch to use <a href="#topic+check_metric">check_metric</a> and <a href="#topic+yardstick_remove_missing">yardstick_remove_missing</a> functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>metric_vec_template(
  metric_impl,
  truth,
  estimate,
  na_rm = TRUE,
  cls = "numeric",
  estimator = NULL,
  case_weights = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="metric_vec_template_+3A_metric_impl">metric_impl</code></td>
<td>
<p>The core implementation function of your custom metric.
This core implementation function is generally defined inside the vector
method of your metric function.</p>
</td></tr>
<tr><td><code id="metric_vec_template_+3A_truth">truth</code></td>
<td>
<p>The realized vector of <code>truth</code>. This is either a factor
or a numeric.</p>
</td></tr>
<tr><td><code id="metric_vec_template_+3A_estimate">estimate</code></td>
<td>
<p>The realized <code>estimate</code> result. This is either a numeric
vector, a factor vector, or a numeric matrix (in the case of multiple
class probability columns) depending on your metric function.</p>
</td></tr>
<tr><td><code id="metric_vec_template_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code> values should be
stripped before the computation proceeds. <code>NA</code> values are removed
before getting to your core implementation function so you do not have to
worry about handling them yourself. If <code>na_rm=FALSE</code> and any <code>NA</code> values
exist, then <code>NA</code> is automatically returned.</p>
</td></tr>
<tr><td><code id="metric_vec_template_+3A_cls">cls</code></td>
<td>
<p>A character vector of length 1 or 2 corresponding to the
class that <code>truth</code> and <code>estimate</code> should be, respectively. If <code>truth</code> and
<code>estimate</code> are of the same class, just supply a vector of length 1. If
they are different, supply a vector of length 2. For matrices, it is best
to supply <code>"numeric"</code> as the class to check here.</p>
</td></tr>
<tr><td><code id="metric_vec_template_+3A_estimator">estimator</code></td>
<td>
<p>The type of averaging to use. By this point, the averaging
type should be finalized, so this should be a character vector of length 1\.
By default, this character value is required to be one of: <code>"binary"</code>,
<code>"macro"</code>, <code>"micro"</code>, or <code>"macro_weighted"</code>. If your metric allows more
or less averaging methods, override this with <code>averaging_override</code>.</p>
</td></tr>
<tr><td><code id="metric_vec_template_+3A_case_weights">case_weights</code></td>
<td>
<p>Optionally, the realized case weights, as a numeric
vector. This must be the same length as <code>truth</code>, and will be considered in
the <code>na_rm</code> checks. If supplied, this will be passed on to <code>metric_impl</code> as
the named argument <code>case_weights</code>.</p>
</td></tr>
<tr><td><code id="metric_vec_template_+3A_...">...</code></td>
<td>
<p>Extra arguments to your core metric function, <code>metric_impl</code>, can
technically be passed here, but generally the extra args are added through
R's scoping rules because the core metric function is created on the fly
when the vector method is called.</p>
</td></tr>
</table>

<hr>
<h2 id='metric-summarizers'>Developer function for summarizing new metrics</h2><span id='topic+metric-summarizers'></span><span id='topic+numeric_metric_summarizer'></span><span id='topic+class_metric_summarizer'></span><span id='topic+prob_metric_summarizer'></span><span id='topic+curve_metric_summarizer'></span><span id='topic+dynamic_survival_metric_summarizer'></span><span id='topic+static_survival_metric_summarizer'></span><span id='topic+curve_survival_metric_summarizer'></span>

<h3>Description</h3>

<p><code>numeric_metric_summarizer()</code>, <code>class_metric_summarizer()</code>,
<code>prob_metric_summarizer()</code>, <code>curve_metric_summarizer()</code>,
<code>dynamic_survival_metric_summarizer()</code>, and
<code>static_survival_metric_summarizer()</code> are useful alongside <a href="#topic+check_metric">check_metric</a> and
<a href="#topic+yardstick_remove_missing">yardstick_remove_missing</a> for implementing new custom metrics. These
functions call the metric function inside <code>dplyr::summarise()</code> or
<code>dplyr::reframe()</code> for <code>curve_metric_summarizer()</code>. See <a href="https://www.tidymodels.org/learn/develop/metrics/">Custom performance metrics</a> for more
information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>numeric_metric_summarizer(
  name,
  fn,
  data,
  truth,
  estimate,
  ...,
  na_rm = TRUE,
  case_weights = NULL,
  fn_options = list(),
  error_call = caller_env()
)

class_metric_summarizer(
  name,
  fn,
  data,
  truth,
  estimate,
  ...,
  estimator = NULL,
  na_rm = TRUE,
  event_level = NULL,
  case_weights = NULL,
  fn_options = list(),
  error_call = caller_env()
)

prob_metric_summarizer(
  name,
  fn,
  data,
  truth,
  ...,
  estimator = NULL,
  na_rm = TRUE,
  event_level = NULL,
  case_weights = NULL,
  fn_options = list(),
  error_call = caller_env()
)

curve_metric_summarizer(
  name,
  fn,
  data,
  truth,
  ...,
  estimator = NULL,
  na_rm = TRUE,
  event_level = NULL,
  case_weights = NULL,
  fn_options = list(),
  error_call = caller_env()
)

dynamic_survival_metric_summarizer(
  name,
  fn,
  data,
  truth,
  ...,
  na_rm = TRUE,
  case_weights = NULL,
  fn_options = list(),
  error_call = caller_env()
)

static_survival_metric_summarizer(
  name,
  fn,
  data,
  truth,
  estimate,
  ...,
  na_rm = TRUE,
  case_weights = NULL,
  fn_options = list(),
  error_call = caller_env()
)

curve_survival_metric_summarizer(
  name,
  fn,
  data,
  truth,
  ...,
  na_rm = TRUE,
  case_weights = NULL,
  fn_options = list(),
  error_call = caller_env()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="metric-summarizers_+3A_name">name</code></td>
<td>
<p>A single character representing the name of the metric to
use in the <code>tibble</code> output. This will be modified to include the type
of averaging if appropriate.</p>
</td></tr>
<tr><td><code id="metric-summarizers_+3A_fn">fn</code></td>
<td>
<p>The vector version of your custom metric function. It
generally takes <code>truth</code>, <code>estimate</code>, <code>na_rm</code>, and any other extra arguments
needed to calculate the metric.</p>
</td></tr>
<tr><td><code id="metric-summarizers_+3A_data">data</code></td>
<td>
<p>The data frame with <code>truth</code> and <code>estimate</code> columns passed in from
the data frame version of your metric function that called
<code>numeric_metric_summarizer()</code>, <code>class_metric_summarizer()</code>,
<code>prob_metric_summarizer()</code>, <code>curve_metric_summarizer()</code>,
<code>dynamic_survival_metric_summarizer()</code>, or
<code>static_survival_metric_summarizer()</code>.</p>
</td></tr>
<tr><td><code id="metric-summarizers_+3A_truth">truth</code></td>
<td>
<p>The unquoted column name corresponding to the <code>truth</code> column.</p>
</td></tr>
<tr><td><code id="metric-summarizers_+3A_estimate">estimate</code></td>
<td>
<p>Generally, the unquoted column name corresponding to
the <code>estimate</code> column. For metrics that take multiple columns through <code>...</code>
like class probability metrics, this is a result of <code><a href="#topic+dots_to_estimate">dots_to_estimate()</a></code>.</p>
</td></tr>
<tr><td><code id="metric-summarizers_+3A_...">...</code></td>
<td>
<p>These dots are for future extensions and must be empty.</p>
</td></tr>
<tr><td><code id="metric-summarizers_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code> values should be
stripped before the computation proceeds. The removal is executed in
<code><a href="#topic+yardstick_remove_missing">yardstick_remove_missing()</a></code>.</p>
</td></tr>
<tr><td><code id="metric-summarizers_+3A_case_weights">case_weights</code></td>
<td>
<p>For metrics supporting case weights, an unquoted
column name corresponding to case weights can be passed here. If not <code>NULL</code>,
the case weights will be passed on to <code>fn</code> as the named argument
<code>case_weights</code>.</p>
</td></tr>
<tr><td><code id="metric-summarizers_+3A_fn_options">fn_options</code></td>
<td>
<p>A named list of metric specific options. These
are spliced into the metric function call using <code style="white-space: pre;">&#8288;!!!&#8288;</code> from <code>rlang</code>. The
default results in nothing being spliced into the call.</p>
</td></tr>
<tr><td><code id="metric-summarizers_+3A_error_call">error_call</code></td>
<td>
<p>The execution environment of a currently
running function, e.g. <code>caller_env()</code>. The function will be
mentioned in error messages as the source of the error. See the
<code>call</code> argument of <code><a href="rlang.html#topic+abort">abort()</a></code> for more information.</p>
</td></tr>
<tr><td><code id="metric-summarizers_+3A_estimator">estimator</code></td>
<td>
<p>This can either be <code>NULL</code> for the default auto-selection of
averaging (<code>"binary"</code> or <code>"macro"</code>), or a single character to pass along to
the metric implementation describing the kind of averaging to use.</p>
</td></tr>
<tr><td><code id="metric-summarizers_+3A_event_level">event_level</code></td>
<td>
<p>This can either be <code>NULL</code> to use the default <code>event_level</code>
value of the <code>fn</code> or a single string of either <code>"first"</code> or <code>"second"</code>
to pass along describing which level should be considered the &quot;event&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>numeric_metric_summarizer()</code>, <code>class_metric_summarizer()</code>,
<code>prob_metric_summarizer()</code>, <code>curve_metric_summarizer()</code>,
<code>dynamic_survival_metric_summarizer()</code>, and
<code>dynamic_survival_metric_summarizer()</code> are generally called from the data
frame version of your metric function. It knows how to call your metric over
grouped data frames and returns a <code>tibble</code> consistent with other metrics.
</p>


<h3>See Also</h3>

<p><a href="#topic+check_metric">check_metric</a> <a href="#topic+yardstick_remove_missing">yardstick_remove_missing</a> <code><a href="#topic+finalize_estimator">finalize_estimator()</a></code> <code><a href="#topic+dots_to_estimate">dots_to_estimate()</a></code>
</p>

<hr>
<h2 id='metrics'>General Function to Estimate Performance</h2><span id='topic+metrics'></span><span id='topic+metrics.data.frame'></span>

<h3>Description</h3>

<p>This function estimates one or more common performance estimates depending
on the class of <code>truth</code> (see <strong>Value</strong> below) and returns them in a three
column tibble. If you wish to modify the metrics used or how they are used
see <code><a href="#topic+metric_set">metric_set()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>metrics(data, ...)

## S3 method for class 'data.frame'
metrics(data, truth, estimate, ..., na_rm = TRUE, options = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="metrics_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by <code>truth</code>,
<code>estimate</code>, and <code>...</code>.</p>
</td></tr>
<tr><td><code id="metrics_+3A_...">...</code></td>
<td>
<p>A set of unquoted column names or one or more
<code>dplyr</code> selector functions to choose which variables contain the
class probabilities. If <code>truth</code> is binary, only 1 column should be selected,
and it should correspond to the value of <code>event_level</code>. Otherwise, there
should be as many columns as factor levels of <code>truth</code> and the ordering of
the columns should be the same as the factor levels of <code>truth</code>.</p>
</td></tr>
<tr><td><code id="metrics_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true results (that
is <code>numeric</code> or <code>factor</code>). This should be an unquoted column name
although this argument is passed by expression and support
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names).</p>
</td></tr>
<tr><td><code id="metrics_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predicted results
(that is also <code>numeric</code> or <code>factor</code>). As with <code>truth</code> this can be
specified different ways but the primary method is to use an
unquoted variable name.</p>
</td></tr>
<tr><td><code id="metrics_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="metrics_+3A_options">options</code></td>
<td>
<p><code style="white-space: pre;">&#8288;[deprecated]&#8288;</code>
</p>
<p>No longer supported as of yardstick 1.0.0. If you pass something here it
will be ignored with a warning.
</p>
<p>Previously, these were options passed on to <code>pROC::roc()</code>. If you need
support for this, use the pROC package directly.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A three column tibble.
</p>

<ul>
<li><p> When <code>truth</code> is a factor, there are rows for <code><a href="#topic+accuracy">accuracy()</a></code> and the
Kappa statistic (<code><a href="#topic+kap">kap()</a></code>).
</p>
</li>
<li><p> When <code>truth</code> has two levels and 1 column of class probabilities is
passed to <code>...</code>, there are rows for the two class versions of
<code><a href="#topic+mn_log_loss">mn_log_loss()</a></code> and <code><a href="#topic+roc_auc">roc_auc()</a></code>.
</p>
</li>
<li><p> When <code>truth</code> has more than two levels and a full set of class probabilities
are passed to <code>...</code>, there are rows for the multiclass version of
<code><a href="#topic+mn_log_loss">mn_log_loss()</a></code> and the Hand Till generalization of <code><a href="#topic+roc_auc">roc_auc()</a></code>.
</p>
</li>
<li><p> When <code>truth</code> is numeric, there are rows for <code><a href="#topic+rmse">rmse()</a></code>, <code><a href="#topic+rsq">rsq()</a></code>,
and <code><a href="#topic+mae">mae()</a></code>.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+metric_set">metric_set()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Accuracy and kappa
metrics(two_class_example, truth, predicted)

# Add on multinomal log loss and ROC AUC by specifying class prob columns
metrics(two_class_example, truth, predicted, Class1)

# Regression metrics
metrics(solubility_test, truth = solubility, estimate = prediction)

# Multiclass metrics work, but you cannot specify any averaging
# for roc_auc() besides the default, hand_till. Use the specific function
# if you need more customization
library(dplyr)

hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  metrics(obs, pred, VF:L) %&gt;%
  print(n = 40)

</code></pre>

<hr>
<h2 id='mn_log_loss'>Mean log loss for multinomial data</h2><span id='topic+mn_log_loss'></span><span id='topic+mn_log_loss.data.frame'></span><span id='topic+mn_log_loss_vec'></span>

<h3>Description</h3>

<p>Compute the logarithmic loss of a classification model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mn_log_loss(data, ...)

## S3 method for class 'data.frame'
mn_log_loss(
  data,
  truth,
  ...,
  na_rm = TRUE,
  sum = FALSE,
  event_level = yardstick_event_level(),
  case_weights = NULL
)

mn_log_loss_vec(
  truth,
  estimate,
  na_rm = TRUE,
  sum = FALSE,
  event_level = yardstick_event_level(),
  case_weights = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mn_log_loss_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by <code>truth</code> and
<code>...</code>.</p>
</td></tr>
<tr><td><code id="mn_log_loss_+3A_...">...</code></td>
<td>
<p>A set of unquoted column names or one or more
<code>dplyr</code> selector functions to choose which variables contain the
class probabilities. If <code>truth</code> is binary, only 1 column should be selected,
and it should correspond to the value of <code>event_level</code>. Otherwise, there
should be as many columns as factor levels of <code>truth</code> and the ordering of
the columns should be the same as the factor levels of <code>truth</code>.</p>
</td></tr>
<tr><td><code id="mn_log_loss_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a <code>factor</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="mn_log_loss_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="mn_log_loss_+3A_sum">sum</code></td>
<td>
<p>A <code>logical</code>. Should the sum of the likelihood contributions be
returned (instead of the mean value)?</p>
</td></tr>
<tr><td><code id="mn_log_loss_+3A_event_level">event_level</code></td>
<td>
<p>A single string. Either <code>"first"</code> or <code>"second"</code> to specify
which level of <code>truth</code> to consider as the &quot;event&quot;. This argument is only
applicable when <code>estimator = "binary"</code>. The default uses an internal helper
that defaults to <code>"first"</code>.</p>
</td></tr>
<tr><td><code id="mn_log_loss_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights.
This should be an unquoted column name that evaluates to a numeric column
in <code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
<tr><td><code id="mn_log_loss_+3A_estimate">estimate</code></td>
<td>
<p>If <code>truth</code> is binary, a numeric vector of class probabilities
corresponding to the &quot;relevant&quot; class. Otherwise, a matrix with as many
columns as factor levels of <code>truth</code>. <em>It is assumed that these are in the
same order as the levels of <code>truth</code>.</em></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Log loss is a measure of the performance of a classification model. A
perfect model has a log loss of <code>0</code>.
</p>
<p>Compared with <code><a href="#topic+accuracy">accuracy()</a></code>, log loss
takes into account the uncertainty in the prediction and gives a more
detailed view into the actual performance. For example, given two input
probabilities of <code>.6</code> and <code>.9</code> where both are classified as predicting
a positive value, say, <code>"Yes"</code>, the accuracy metric would interpret them
as having the same value. If the true output is <code>"Yes"</code>, log loss penalizes
<code>.6</code> because it is &quot;less sure&quot; of it's result compared to the probability
of <code>.9</code>.
</p>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>mn_log_loss_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Multiclass</h3>

<p>Log loss has a known multiclass extension, and is simply the sum of the
log loss values for each class prediction. Because of this, no averaging
types are supported.
</p>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>See Also</h3>

<p>Other class probability metrics: 
<code><a href="#topic+average_precision">average_precision</a>()</code>,
<code><a href="#topic+brier_class">brier_class</a>()</code>,
<code><a href="#topic+classification_cost">classification_cost</a>()</code>,
<code><a href="#topic+gain_capture">gain_capture</a>()</code>,
<code><a href="#topic+pr_auc">pr_auc</a>()</code>,
<code><a href="#topic+roc_auc">roc_auc</a>()</code>,
<code><a href="#topic+roc_aunp">roc_aunp</a>()</code>,
<code><a href="#topic+roc_aunu">roc_aunu</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Two class
data("two_class_example")
mn_log_loss(two_class_example, truth, Class1)

# Multiclass
library(dplyr)
data(hpc_cv)

# You can use the col1:colN tidyselect syntax
hpc_cv %&gt;%
  filter(Resample == "Fold01") %&gt;%
  mn_log_loss(obs, VF:L)

# Groups are respected
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  mn_log_loss(obs, VF:L)


# Vector version
# Supply a matrix of class probabilities
fold1 &lt;- hpc_cv %&gt;%
  filter(Resample == "Fold01")

mn_log_loss_vec(
  truth = fold1$obs,
  matrix(
    c(fold1$VF, fold1$F, fold1$M, fold1$L),
    ncol = 4
  )
)

# Supply `...` with quasiquotation
prob_cols &lt;- levels(two_class_example$truth)
mn_log_loss(two_class_example, truth, Class1)
mn_log_loss(two_class_example, truth, !!prob_cols[1])

</code></pre>

<hr>
<h2 id='mpe'>Mean percentage error</h2><span id='topic+mpe'></span><span id='topic+mpe.data.frame'></span><span id='topic+mpe_vec'></span>

<h3>Description</h3>

<p>Calculate the mean percentage error. This metric is in <em>relative
units</em>. It can be used as a measure of the <code>estimate</code>'s bias.
</p>
<p>Note that if <em>any</em> <code>truth</code> values are <code>0</code>, a value of:
<code>-Inf</code> (<code>estimate &gt; 0</code>), <code>Inf</code> (<code>estimate &lt; 0</code>), or <code>NaN</code> (<code>estimate == 0</code>)
is returned for <code>mpe()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mpe(data, ...)

## S3 method for class 'data.frame'
mpe(data, truth, estimate, na_rm = TRUE, case_weights = NULL, ...)

mpe_vec(truth, estimate, na_rm = TRUE, case_weights = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mpe_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by the <code>truth</code>
and <code>estimate</code> arguments.</p>
</td></tr>
<tr><td><code id="mpe_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="mpe_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true results
(that is <code>numeric</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>numeric</code> vector.</p>
</td></tr>
<tr><td><code id="mpe_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predicted
results (that is also <code>numeric</code>). As with <code>truth</code> this can be
specified different ways but the primary method is to use an
unquoted variable name. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>numeric</code> vector.</p>
</td></tr>
<tr><td><code id="mpe_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="mpe_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights. This
should be an unquoted column name that evaluates to a numeric column in
<code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>mpe_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Author(s)</h3>

<p>Thomas Bierhance
</p>


<h3>See Also</h3>

<p>Other numeric metrics: 
<code><a href="#topic+ccc">ccc</a>()</code>,
<code><a href="#topic+huber_loss_pseudo">huber_loss_pseudo</a>()</code>,
<code><a href="#topic+huber_loss">huber_loss</a>()</code>,
<code><a href="#topic+iic">iic</a>()</code>,
<code><a href="#topic+mae">mae</a>()</code>,
<code><a href="#topic+mape">mape</a>()</code>,
<code><a href="#topic+mase">mase</a>()</code>,
<code><a href="#topic+msd">msd</a>()</code>,
<code><a href="#topic+poisson_log_loss">poisson_log_loss</a>()</code>,
<code><a href="#topic+rmse">rmse</a>()</code>,
<code><a href="#topic+rpd">rpd</a>()</code>,
<code><a href="#topic+rpiq">rpiq</a>()</code>,
<code><a href="#topic+rsq_trad">rsq_trad</a>()</code>,
<code><a href="#topic+rsq">rsq</a>()</code>,
<code><a href="#topic+smape">smape</a>()</code>
</p>
<p>Other accuracy metrics: 
<code><a href="#topic+ccc">ccc</a>()</code>,
<code><a href="#topic+huber_loss_pseudo">huber_loss_pseudo</a>()</code>,
<code><a href="#topic+huber_loss">huber_loss</a>()</code>,
<code><a href="#topic+iic">iic</a>()</code>,
<code><a href="#topic+mae">mae</a>()</code>,
<code><a href="#topic+mape">mape</a>()</code>,
<code><a href="#topic+mase">mase</a>()</code>,
<code><a href="#topic+msd">msd</a>()</code>,
<code><a href="#topic+poisson_log_loss">poisson_log_loss</a>()</code>,
<code><a href="#topic+rmse">rmse</a>()</code>,
<code><a href="#topic+smape">smape</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># `solubility_test$solubility` has zero values with corresponding
# `$prediction` values that are negative. By definition, this causes `Inf`
# to be returned from `mpe()`.
solubility_test[solubility_test$solubility == 0, ]

mpe(solubility_test, solubility, prediction)

# We'll remove the zero values for demonstration
solubility_test &lt;- solubility_test[solubility_test$solubility != 0, ]

# Supply truth and predictions as bare column names
mpe(solubility_test, solubility, prediction)

library(dplyr)

set.seed(1234)
size &lt;- 100
times &lt;- 10

# create 10 resamples
solubility_resampled &lt;- bind_rows(
  replicate(
    n = times,
    expr = sample_n(solubility_test, size, replace = TRUE),
    simplify = FALSE
  ),
  .id = "resample"
)

# Compute the metric by group
metric_results &lt;- solubility_resampled %&gt;%
  group_by(resample) %&gt;%
  mpe(solubility, prediction)

metric_results

# Resampled mean estimate
metric_results %&gt;%
  summarise(avg_estimate = mean(.estimate))
</code></pre>

<hr>
<h2 id='msd'>Mean signed deviation</h2><span id='topic+msd'></span><span id='topic+msd.data.frame'></span><span id='topic+msd_vec'></span>

<h3>Description</h3>

<p>Mean signed deviation (also known as mean signed difference, or mean signed
error) computes the average differences between <code>truth</code> and <code>estimate</code>. A
related metric is the mean absolute error (<code><a href="#topic+mae">mae()</a></code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>msd(data, ...)

## S3 method for class 'data.frame'
msd(data, truth, estimate, na_rm = TRUE, case_weights = NULL, ...)

msd_vec(truth, estimate, na_rm = TRUE, case_weights = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="msd_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by the <code>truth</code>
and <code>estimate</code> arguments.</p>
</td></tr>
<tr><td><code id="msd_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="msd_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true results
(that is <code>numeric</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>numeric</code> vector.</p>
</td></tr>
<tr><td><code id="msd_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predicted
results (that is also <code>numeric</code>). As with <code>truth</code> this can be
specified different ways but the primary method is to use an
unquoted variable name. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>numeric</code> vector.</p>
</td></tr>
<tr><td><code id="msd_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="msd_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights. This
should be an unquoted column name that evaluates to a numeric column in
<code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mean signed deviation is rarely used, since positive and negative errors
cancel each other out. For example, <code>msd_vec(c(100, -100), c(0, 0))</code> would
return a seemingly &quot;perfect&quot; value of <code>0</code>, even though <code>estimate</code> is wildly
different from <code>truth</code>. <code><a href="#topic+mae">mae()</a></code> attempts to remedy this by taking the
absolute value of the differences before computing the mean.
</p>
<p>This metric is computed as <code>mean(truth - estimate)</code>, following the convention
that an &quot;error&quot; is computed as <code>observed - predicted</code>. If you expected this
metric to be computed as <code>mean(estimate - truth)</code>, reverse the sign of the
result.
</p>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>msd_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Author(s)</h3>

<p>Thomas Bierhance
</p>


<h3>See Also</h3>

<p>Other numeric metrics: 
<code><a href="#topic+ccc">ccc</a>()</code>,
<code><a href="#topic+huber_loss_pseudo">huber_loss_pseudo</a>()</code>,
<code><a href="#topic+huber_loss">huber_loss</a>()</code>,
<code><a href="#topic+iic">iic</a>()</code>,
<code><a href="#topic+mae">mae</a>()</code>,
<code><a href="#topic+mape">mape</a>()</code>,
<code><a href="#topic+mase">mase</a>()</code>,
<code><a href="#topic+mpe">mpe</a>()</code>,
<code><a href="#topic+poisson_log_loss">poisson_log_loss</a>()</code>,
<code><a href="#topic+rmse">rmse</a>()</code>,
<code><a href="#topic+rpd">rpd</a>()</code>,
<code><a href="#topic+rpiq">rpiq</a>()</code>,
<code><a href="#topic+rsq_trad">rsq_trad</a>()</code>,
<code><a href="#topic+rsq">rsq</a>()</code>,
<code><a href="#topic+smape">smape</a>()</code>
</p>
<p>Other accuracy metrics: 
<code><a href="#topic+ccc">ccc</a>()</code>,
<code><a href="#topic+huber_loss_pseudo">huber_loss_pseudo</a>()</code>,
<code><a href="#topic+huber_loss">huber_loss</a>()</code>,
<code><a href="#topic+iic">iic</a>()</code>,
<code><a href="#topic+mae">mae</a>()</code>,
<code><a href="#topic+mape">mape</a>()</code>,
<code><a href="#topic+mase">mase</a>()</code>,
<code><a href="#topic+mpe">mpe</a>()</code>,
<code><a href="#topic+poisson_log_loss">poisson_log_loss</a>()</code>,
<code><a href="#topic+rmse">rmse</a>()</code>,
<code><a href="#topic+smape">smape</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Supply truth and predictions as bare column names
msd(solubility_test, solubility, prediction)

library(dplyr)

set.seed(1234)
size &lt;- 100
times &lt;- 10

# create 10 resamples
solubility_resampled &lt;- bind_rows(
  replicate(
    n = times,
    expr = sample_n(solubility_test, size, replace = TRUE),
    simplify = FALSE
  ),
  .id = "resample"
)

# Compute the metric by group
metric_results &lt;- solubility_resampled %&gt;%
  group_by(resample) %&gt;%
  msd(solubility, prediction)

metric_results

# Resampled mean estimate
metric_results %&gt;%
  summarise(avg_estimate = mean(.estimate))
</code></pre>

<hr>
<h2 id='new_groupwise_metric'>Create groupwise metrics</h2><span id='topic+new_groupwise_metric'></span>

<h3>Description</h3>

<p>Groupwise metrics quantify the disparity in value of a metric across a
number of groups. Groupwise metrics with a value of zero indicate that the
underlying metric is equal across groups. yardstick defines
several common fairness metrics using this function, such as
<code><a href="#topic+demographic_parity">demographic_parity()</a></code>, <code><a href="#topic+equal_opportunity">equal_opportunity()</a></code>, and <code><a href="#topic+equalized_odds">equalized_odds()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>new_groupwise_metric(fn, name, aggregate, direction = "minimize")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="new_groupwise_metric_+3A_fn">fn</code></td>
<td>
<p>A yardstick metric function or metric set.</p>
</td></tr>
<tr><td><code id="new_groupwise_metric_+3A_name">name</code></td>
<td>
<p>The name of the metric to place in the <code>.metric</code> column
of the output.</p>
</td></tr>
<tr><td><code id="new_groupwise_metric_+3A_aggregate">aggregate</code></td>
<td>
<p>A function to summarize the generated metric set results.
The function takes metric set results as the first argument and returns
a single numeric giving the <code>.estimate</code> value as output. See the Value and
Examples sections for example uses.</p>
</td></tr>
<tr><td><code id="new_groupwise_metric_+3A_direction">direction</code></td>
<td>
<p>A string. One of:
</p>

<ul>
<li> <p><code>"maximize"</code>
</p>
</li>
<li> <p><code>"minimize"</code>
</p>
</li>
<li> <p><code>"zero"</code>
</p>
</li></ul>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that <em>all</em> yardstick metrics are group-aware in that, when passed
grouped data, they will return metric values calculated for each group.
When passed grouped data, groupwise metrics also return metric values
for each group, but those metric values are calculated by first additionally
grouping by the variable passed to <code>by</code> and then summarizing the per-group
metric estimates across groups using the function passed as the
<code>aggregate</code> argument. Learn more about grouping behavior in yardstick using
<code>vignette("grouping", "yardstick")</code>.
</p>


<h3>Value</h3>

<p>This function is a
<a href="https://adv-r.hadley.nz/function-factories.html">function factory</a>; it's
output is itself a function. Further, the functions that this function
outputs are also function factories. More explicitly, this looks like:
</p>
<div class="sourceCode"><pre># a function with similar implementation to `demographic_parity()`:
diff_range &lt;- function(x) {diff(range(x$.estimate))}

dem_parity &lt;-
  new_groupwise_metric(
    fn = detection_prevalence,
    name = "dem_parity",
    aggregate = diff_range
  )
</pre></div>
<p>The outputted <code>dem_parity</code> is a function that takes one argument, <code>by</code>,
indicating the data-masked variable giving the sensitive feature.
</p>
<p>When called with a <code>by</code> argument, <code>dem_parity</code> will return a yardstick
metric function like any other:
</p>
<div class="sourceCode"><pre>dem_parity_by_gender &lt;- dem_parity(gender)
</pre></div>
<p>Note that <code>dem_parity</code> doesn't take any arguments other than <code>by</code>, and thus
knows nothing about the data it will be applied to other than that it ought
to have a column with name <code>"gender"</code> in it.
</p>
<p>The output <code>dem_parity_by_gender</code> is a metric function that takes the
same arguments as the function supplied as <code>fn</code>, in this case
<code>detection_prevalence</code>. It will thus interface like any other yardstick
function except that it will look for a <code>"gender"</code> column in
the data it's supplied.
</p>
<p>In addition to the examples below, see the documentation on the
return value of fairness metrics like <code><a href="#topic+demographic_parity">demographic_parity()</a></code>,
<code><a href="#topic+equal_opportunity">equal_opportunity()</a></code>, or <code><a href="#topic+equalized_odds">equalized_odds()</a></code> to learn more about how the
output of this function can be used.
</p>


<h3>Relevant Group Level</h3>

<p>Additional arguments can be passed to the function outputted by
the function that this function outputs. That is:
</p>
<div class="sourceCode"><pre>res_fairness &lt;- new_groupwise_metric(...)
res_by &lt;- res_fairness(by)
res_by(..., additional_arguments_to_aggregate = TRUE)
</pre></div>
<p>For finer control of how groups in <code>by</code> are treated, use the
<code>aggregate</code> argument.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(hpc_cv)

# `demographic_parity`, among other fairness metrics,
# is generated with `new_groupwise_metric()`:
diff_range &lt;- function(x) {diff(range(x$.estimate))}
demographic_parity_ &lt;-
  new_groupwise_metric(
    fn = detection_prevalence,
    name = "demographic_parity",
    aggregate = diff_range
  )

m_set &lt;- metric_set(demographic_parity_(Resample))

m_set(hpc_cv, truth = obs, estimate = pred)

# the `post` argument can be used to accommodate a wide
# variety of parameterizations. to encode demographic
# parity as a ratio inside of a difference, for example:
ratio_range &lt;- function(x, ...) {
  range &lt;- range(x$.estimate)
  range[1] / range[2]
}

demographic_parity_ratio &lt;-
  new_groupwise_metric(
    fn = detection_prevalence,
    name = "demographic_parity_ratio",
    aggregate = ratio_range
  )

</code></pre>

<hr>
<h2 id='new-metric'>Construct a new metric function</h2><span id='topic+new-metric'></span><span id='topic+new_class_metric'></span><span id='topic+new_prob_metric'></span><span id='topic+new_numeric_metric'></span><span id='topic+new_dynamic_survival_metric'></span><span id='topic+new_integrated_survival_metric'></span><span id='topic+new_static_survival_metric'></span>

<h3>Description</h3>

<p>These functions provide convenient wrappers to create the three types of
metric functions in yardstick: numeric metrics, class metrics, and
class probability metrics. They add a metric-specific class to <code>fn</code> and
attach a <code>direction</code> attribute. These features are used by <code><a href="#topic+metric_set">metric_set()</a></code>
and by <a href="https://tune.tidymodels.org/">tune</a> when model tuning.
</p>
<p>See <a href="https://www.tidymodels.org/learn/develop/metrics/">Custom performance metrics</a> for more
information about creating custom metrics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>new_class_metric(fn, direction)

new_prob_metric(fn, direction)

new_numeric_metric(fn, direction)

new_dynamic_survival_metric(fn, direction)

new_integrated_survival_metric(fn, direction)

new_static_survival_metric(fn, direction)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="new-metric_+3A_fn">fn</code></td>
<td>
<p>A function. The metric function to attach a metric-specific class
and <code>direction</code> attribute to.</p>
</td></tr>
<tr><td><code id="new-metric_+3A_direction">direction</code></td>
<td>
<p>A string. One of:
</p>

<ul>
<li> <p><code>"maximize"</code>
</p>
</li>
<li> <p><code>"minimize"</code>
</p>
</li>
<li> <p><code>"zero"</code>
</p>
</li></ul>
</td></tr>
</table>

<hr>
<h2 id='npv'>Negative predictive value</h2><span id='topic+npv'></span><span id='topic+npv.data.frame'></span><span id='topic+npv_vec'></span>

<h3>Description</h3>

<p>These functions calculate the <code><a href="#topic+npv">npv()</a></code> (negative predictive value) of a
measurement system compared to a reference result (the &quot;truth&quot; or gold standard).
Highly related functions are <code><a href="#topic+spec">spec()</a></code>, <code><a href="#topic+sens">sens()</a></code>, and <code><a href="#topic+ppv">ppv()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npv(data, ...)

## S3 method for class 'data.frame'
npv(
  data,
  truth,
  estimate,
  prevalence = NULL,
  estimator = NULL,
  na_rm = TRUE,
  case_weights = NULL,
  event_level = yardstick_event_level(),
  ...
)

npv_vec(
  truth,
  estimate,
  prevalence = NULL,
  estimator = NULL,
  na_rm = TRUE,
  case_weights = NULL,
  event_level = yardstick_event_level(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npv_+3A_data">data</code></td>
<td>
<p>Either a <code>data.frame</code> containing the columns specified by the
<code>truth</code> and <code>estimate</code> arguments, or a <code>table</code>/<code>matrix</code> where the true
class results should be in the columns of the table.</p>
</td></tr>
<tr><td><code id="npv_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="npv_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a <code>factor</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="npv_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predicted class
results (that is also <code>factor</code>). As with <code>truth</code> this can be
specified different ways but the primary method is to use an
unquoted variable name. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="npv_+3A_prevalence">prevalence</code></td>
<td>
<p>A numeric value for the rate of the
&quot;positive&quot; class of the data.</p>
</td></tr>
<tr><td><code id="npv_+3A_estimator">estimator</code></td>
<td>
<p>One of: <code>"binary"</code>, <code>"macro"</code>, <code>"macro_weighted"</code>,
or <code>"micro"</code> to specify the type of averaging to be done. <code>"binary"</code> is
only relevant for the two class case. The other three are general methods
for calculating multiclass metrics. The default will automatically choose
<code>"binary"</code> or <code>"macro"</code> based on <code>estimate</code>.</p>
</td></tr>
<tr><td><code id="npv_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="npv_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights.
This should be an unquoted column name that evaluates to a numeric column
in <code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
<tr><td><code id="npv_+3A_event_level">event_level</code></td>
<td>
<p>A single string. Either <code>"first"</code> or <code>"second"</code> to specify
which level of <code>truth</code> to consider as the &quot;event&quot;. This argument is only
applicable when <code>estimator = "binary"</code>. The default uses an internal helper
that defaults to <code>"first"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The positive predictive value (<code><a href="#topic+ppv">ppv()</a></code>) is defined as the percent of
predicted positives that are actually positive while the
negative predictive value (<code><a href="#topic+npv">npv()</a></code>) is defined as the percent of negative
positives that are actually negative.
</p>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>npv_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Relevant Level</h3>

<p>There is no common convention on which factor level should
automatically be considered the &quot;event&quot; or &quot;positive&quot; result
when computing binary classification metrics. In <code>yardstick</code>, the default
is to use the <em>first</em> level. To alter this, change the argument
<code>event_level</code> to <code>"second"</code> to consider the <em>last</em> level of the factor the
level of interest. For multiclass extensions involving one-vs-all
comparisons (such as macro averaging), this option is ignored and
the &quot;one&quot; level is always the relevant result.
</p>


<h3>Multiclass</h3>

<p>Macro, micro, and macro-weighted averaging is available for this metric.
The default is to select macro averaging if a <code>truth</code> factor with more
than 2 levels is provided. Otherwise, a standard binary calculation is done.
See <code>vignette("multiclass", "yardstick")</code> for more information.
</p>


<h3>Implementation</h3>

<p>Suppose a 2x2 table with notation:
</p>

<table>
<tr>
 <td style="text-align: right;"> </td><td style="text-align: center;"> Reference </td><td style="text-align: center;"> </td>
</tr>
<tr>
 <td style="text-align: right;"> Predicted </td><td style="text-align: center;"> Positive </td><td style="text-align: center;"> Negative
</td>
</tr>
<tr>
 <td style="text-align: right;"> Positive </td><td style="text-align: center;"> A </td><td style="text-align: center;"> B </td>
</tr>
<tr>
 <td style="text-align: right;"> Negative </td><td style="text-align: center;"> C </td><td style="text-align: center;"> D </td>
</tr>
<tr>
 <td style="text-align: right;"> </td>
</tr>

</table>

<p>The formulas used here are:
</p>
<p style="text-align: center;"><code class="reqn">Sensitivity = A/(A+C)</code>
</p>

<p style="text-align: center;"><code class="reqn">Specificity = D/(B+D)</code>
</p>

<p style="text-align: center;"><code class="reqn">Prevalence = (A+C)/(A+B+C+D)</code>
</p>

<p style="text-align: center;"><code class="reqn">PPV = (Sensitivity * Prevalence) / ((Sensitivity * Prevalence) + ((1-Specificity) * (1-Prevalence)))</code>
</p>

<p style="text-align: center;"><code class="reqn">NPV = (Specificity * (1-Prevalence)) / (((1-Sensitivity) * Prevalence) + ((Specificity) * (1-Prevalence)))</code>
</p>

<p>See the references for discussions of the statistics.
</p>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>References</h3>

<p>Altman, D.G., Bland, J.M. (1994) &ldquo;Diagnostic tests 2:
predictive values,&rdquo; <em>British Medical Journal</em>, vol 309,
102.
</p>


<h3>See Also</h3>

<p>Other class metrics: 
<code><a href="#topic+accuracy">accuracy</a>()</code>,
<code><a href="#topic+bal_accuracy">bal_accuracy</a>()</code>,
<code><a href="#topic+detection_prevalence">detection_prevalence</a>()</code>,
<code><a href="#topic+f_meas">f_meas</a>()</code>,
<code><a href="#topic+j_index">j_index</a>()</code>,
<code><a href="#topic+kap">kap</a>()</code>,
<code><a href="#topic+mcc">mcc</a>()</code>,
<code><a href="#topic+ppv">ppv</a>()</code>,
<code><a href="#topic+precision">precision</a>()</code>,
<code><a href="#topic+recall">recall</a>()</code>,
<code><a href="#topic+sens">sens</a>()</code>,
<code><a href="#topic+spec">spec</a>()</code>
</p>
<p>Other sensitivity metrics: 
<code><a href="#topic+ppv">ppv</a>()</code>,
<code><a href="#topic+sens">sens</a>()</code>,
<code><a href="#topic+spec">spec</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Two class
data("two_class_example")
npv(two_class_example, truth, predicted)

# Multiclass
library(dplyr)
data(hpc_cv)

hpc_cv %&gt;%
  filter(Resample == "Fold01") %&gt;%
  npv(obs, pred)

# Groups are respected
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  npv(obs, pred)

# Weighted macro averaging
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  npv(obs, pred, estimator = "macro_weighted")

# Vector version
npv_vec(
  two_class_example$truth,
  two_class_example$predicted
)

# Making Class2 the "relevant" level
npv_vec(
  two_class_example$truth,
  two_class_example$predicted,
  event_level = "second"
)
</code></pre>

<hr>
<h2 id='pathology'>Liver Pathology Data</h2><span id='topic+pathology'></span>

<h3>Description</h3>

<p>Liver Pathology Data
</p>


<h3>Details</h3>

<p>These data have the results of a <em>x</em>-ray examination
to determine whether liver is abnormal or not (in the <code>scan</code>
column) versus the more extensive pathology results that
approximate the truth (in <code>pathology</code>).
</p>


<h3>Value</h3>

<table>
<tr><td><code>pathology</code></td>
<td>
<p>a data frame</p>
</td></tr>
</table>


<h3>Source</h3>

<p>Altman, D.G., Bland, J.M. (1994) &ldquo;Diagnostic tests 1:
sensitivity and specificity,&rdquo; <em>British Medical Journal</em>,
vol 308, 1552.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(pathology)
str(pathology)
</code></pre>

<hr>
<h2 id='poisson_log_loss'>Mean log loss for Poisson data</h2><span id='topic+poisson_log_loss'></span><span id='topic+poisson_log_loss.data.frame'></span><span id='topic+poisson_log_loss_vec'></span>

<h3>Description</h3>

<p>Calculate the loss function for the Poisson distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>poisson_log_loss(data, ...)

## S3 method for class 'data.frame'
poisson_log_loss(data, truth, estimate, na_rm = TRUE, case_weights = NULL, ...)

poisson_log_loss_vec(truth, estimate, na_rm = TRUE, case_weights = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="poisson_log_loss_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by the <code>truth</code>
and <code>estimate</code> arguments.</p>
</td></tr>
<tr><td><code id="poisson_log_loss_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="poisson_log_loss_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true counts (that is <code>integer</code>).
This should be an unquoted column name although this argument is passed by
expression and supports <a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can
unquote column names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, an <code>integer</code> vector.</p>
</td></tr>
<tr><td><code id="poisson_log_loss_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predicted
results (that is also <code>numeric</code>). As with <code>truth</code> this can be
specified different ways but the primary method is to use an
unquoted variable name. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>numeric</code> vector.</p>
</td></tr>
<tr><td><code id="poisson_log_loss_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="poisson_log_loss_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights. This
should be an unquoted column name that evaluates to a numeric column in
<code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>poisson_log_loss_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>See Also</h3>

<p>Other numeric metrics: 
<code><a href="#topic+ccc">ccc</a>()</code>,
<code><a href="#topic+huber_loss_pseudo">huber_loss_pseudo</a>()</code>,
<code><a href="#topic+huber_loss">huber_loss</a>()</code>,
<code><a href="#topic+iic">iic</a>()</code>,
<code><a href="#topic+mae">mae</a>()</code>,
<code><a href="#topic+mape">mape</a>()</code>,
<code><a href="#topic+mase">mase</a>()</code>,
<code><a href="#topic+mpe">mpe</a>()</code>,
<code><a href="#topic+msd">msd</a>()</code>,
<code><a href="#topic+rmse">rmse</a>()</code>,
<code><a href="#topic+rpd">rpd</a>()</code>,
<code><a href="#topic+rpiq">rpiq</a>()</code>,
<code><a href="#topic+rsq_trad">rsq_trad</a>()</code>,
<code><a href="#topic+rsq">rsq</a>()</code>,
<code><a href="#topic+smape">smape</a>()</code>
</p>
<p>Other accuracy metrics: 
<code><a href="#topic+ccc">ccc</a>()</code>,
<code><a href="#topic+huber_loss_pseudo">huber_loss_pseudo</a>()</code>,
<code><a href="#topic+huber_loss">huber_loss</a>()</code>,
<code><a href="#topic+iic">iic</a>()</code>,
<code><a href="#topic+mae">mae</a>()</code>,
<code><a href="#topic+mape">mape</a>()</code>,
<code><a href="#topic+mase">mase</a>()</code>,
<code><a href="#topic+mpe">mpe</a>()</code>,
<code><a href="#topic+msd">msd</a>()</code>,
<code><a href="#topic+rmse">rmse</a>()</code>,
<code><a href="#topic+smape">smape</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>count_truth &lt;- c(2L,   7L,   1L,   1L,   0L,  3L)
count_pred  &lt;- c(2.14, 5.35, 1.65, 1.56, 1.3, 2.71)
count_results &lt;- dplyr::tibble(count = count_truth, pred = count_pred)

# Supply truth and predictions as bare column names
poisson_log_loss(count_results, count, pred)

</code></pre>

<hr>
<h2 id='ppv'>Positive predictive value</h2><span id='topic+ppv'></span><span id='topic+ppv.data.frame'></span><span id='topic+ppv_vec'></span>

<h3>Description</h3>

<p>These functions calculate the <code><a href="#topic+ppv">ppv()</a></code> (positive predictive value) of a
measurement system compared to a reference result (the &quot;truth&quot; or gold standard).
Highly related functions are <code><a href="#topic+spec">spec()</a></code>, <code><a href="#topic+sens">sens()</a></code>, and <code><a href="#topic+npv">npv()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ppv(data, ...)

## S3 method for class 'data.frame'
ppv(
  data,
  truth,
  estimate,
  prevalence = NULL,
  estimator = NULL,
  na_rm = TRUE,
  case_weights = NULL,
  event_level = yardstick_event_level(),
  ...
)

ppv_vec(
  truth,
  estimate,
  prevalence = NULL,
  estimator = NULL,
  na_rm = TRUE,
  case_weights = NULL,
  event_level = yardstick_event_level(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ppv_+3A_data">data</code></td>
<td>
<p>Either a <code>data.frame</code> containing the columns specified by the
<code>truth</code> and <code>estimate</code> arguments, or a <code>table</code>/<code>matrix</code> where the true
class results should be in the columns of the table.</p>
</td></tr>
<tr><td><code id="ppv_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="ppv_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a <code>factor</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="ppv_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predicted class
results (that is also <code>factor</code>). As with <code>truth</code> this can be
specified different ways but the primary method is to use an
unquoted variable name. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="ppv_+3A_prevalence">prevalence</code></td>
<td>
<p>A numeric value for the rate of the
&quot;positive&quot; class of the data.</p>
</td></tr>
<tr><td><code id="ppv_+3A_estimator">estimator</code></td>
<td>
<p>One of: <code>"binary"</code>, <code>"macro"</code>, <code>"macro_weighted"</code>,
or <code>"micro"</code> to specify the type of averaging to be done. <code>"binary"</code> is
only relevant for the two class case. The other three are general methods
for calculating multiclass metrics. The default will automatically choose
<code>"binary"</code> or <code>"macro"</code> based on <code>estimate</code>.</p>
</td></tr>
<tr><td><code id="ppv_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="ppv_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights.
This should be an unquoted column name that evaluates to a numeric column
in <code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
<tr><td><code id="ppv_+3A_event_level">event_level</code></td>
<td>
<p>A single string. Either <code>"first"</code> or <code>"second"</code> to specify
which level of <code>truth</code> to consider as the &quot;event&quot;. This argument is only
applicable when <code>estimator = "binary"</code>. The default uses an internal helper
that defaults to <code>"first"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The positive predictive value (<code><a href="#topic+ppv">ppv()</a></code>) is defined as the percent of
predicted positives that are actually positive while the
negative predictive value (<code><a href="#topic+npv">npv()</a></code>) is defined as the percent of negative
positives that are actually negative.
</p>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>ppv_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Relevant Level</h3>

<p>There is no common convention on which factor level should
automatically be considered the &quot;event&quot; or &quot;positive&quot; result
when computing binary classification metrics. In <code>yardstick</code>, the default
is to use the <em>first</em> level. To alter this, change the argument
<code>event_level</code> to <code>"second"</code> to consider the <em>last</em> level of the factor the
level of interest. For multiclass extensions involving one-vs-all
comparisons (such as macro averaging), this option is ignored and
the &quot;one&quot; level is always the relevant result.
</p>


<h3>Multiclass</h3>

<p>Macro, micro, and macro-weighted averaging is available for this metric.
The default is to select macro averaging if a <code>truth</code> factor with more
than 2 levels is provided. Otherwise, a standard binary calculation is done.
See <code>vignette("multiclass", "yardstick")</code> for more information.
</p>


<h3>Implementation</h3>

<p>Suppose a 2x2 table with notation:
</p>

<table>
<tr>
 <td style="text-align: right;"> </td><td style="text-align: center;"> Reference </td><td style="text-align: center;"> </td>
</tr>
<tr>
 <td style="text-align: right;"> Predicted </td><td style="text-align: center;"> Positive </td><td style="text-align: center;"> Negative
</td>
</tr>
<tr>
 <td style="text-align: right;"> Positive </td><td style="text-align: center;"> A </td><td style="text-align: center;"> B </td>
</tr>
<tr>
 <td style="text-align: right;"> Negative </td><td style="text-align: center;"> C </td><td style="text-align: center;"> D </td>
</tr>
<tr>
 <td style="text-align: right;"> </td>
</tr>

</table>

<p>The formulas used here are:
</p>
<p style="text-align: center;"><code class="reqn">Sensitivity = A/(A+C)</code>
</p>

<p style="text-align: center;"><code class="reqn">Specificity = D/(B+D)</code>
</p>

<p style="text-align: center;"><code class="reqn">Prevalence = (A+C)/(A+B+C+D)</code>
</p>

<p style="text-align: center;"><code class="reqn">PPV = (Sensitivity * Prevalence) / ((Sensitivity * Prevalence) + ((1-Specificity) * (1-Prevalence)))</code>
</p>

<p style="text-align: center;"><code class="reqn">NPV = (Specificity * (1-Prevalence)) / (((1-Sensitivity) * Prevalence) + ((Specificity) * (1-Prevalence)))</code>
</p>

<p>See the references for discussions of the statistics.
</p>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>References</h3>

<p>Altman, D.G., Bland, J.M. (1994) &ldquo;Diagnostic tests 2:
predictive values,&rdquo; <em>British Medical Journal</em>, vol 309,
102.
</p>


<h3>See Also</h3>

<p>Other class metrics: 
<code><a href="#topic+accuracy">accuracy</a>()</code>,
<code><a href="#topic+bal_accuracy">bal_accuracy</a>()</code>,
<code><a href="#topic+detection_prevalence">detection_prevalence</a>()</code>,
<code><a href="#topic+f_meas">f_meas</a>()</code>,
<code><a href="#topic+j_index">j_index</a>()</code>,
<code><a href="#topic+kap">kap</a>()</code>,
<code><a href="#topic+mcc">mcc</a>()</code>,
<code><a href="#topic+npv">npv</a>()</code>,
<code><a href="#topic+precision">precision</a>()</code>,
<code><a href="#topic+recall">recall</a>()</code>,
<code><a href="#topic+sens">sens</a>()</code>,
<code><a href="#topic+spec">spec</a>()</code>
</p>
<p>Other sensitivity metrics: 
<code><a href="#topic+npv">npv</a>()</code>,
<code><a href="#topic+sens">sens</a>()</code>,
<code><a href="#topic+spec">spec</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Two class
data("two_class_example")
ppv(two_class_example, truth, predicted)

# Multiclass
library(dplyr)
data(hpc_cv)

hpc_cv %&gt;%
  filter(Resample == "Fold01") %&gt;%
  ppv(obs, pred)

# Groups are respected
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  ppv(obs, pred)

# Weighted macro averaging
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  ppv(obs, pred, estimator = "macro_weighted")

# Vector version
ppv_vec(
  two_class_example$truth,
  two_class_example$predicted
)

# Making Class2 the "relevant" level
ppv_vec(
  two_class_example$truth,
  two_class_example$predicted,
  event_level = "second"
)
# But what if we think that Class 1 only occurs 40% of the time?
ppv(two_class_example, truth, predicted, prevalence = 0.40)

</code></pre>

<hr>
<h2 id='pr_auc'>Area under the precision recall curve</h2><span id='topic+pr_auc'></span><span id='topic+pr_auc.data.frame'></span><span id='topic+pr_auc_vec'></span>

<h3>Description</h3>

<p><code>pr_auc()</code> is a metric that computes the area under the precision
recall curve. See <code><a href="#topic+pr_curve">pr_curve()</a></code> for the full curve.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pr_auc(data, ...)

## S3 method for class 'data.frame'
pr_auc(
  data,
  truth,
  ...,
  estimator = NULL,
  na_rm = TRUE,
  event_level = yardstick_event_level(),
  case_weights = NULL
)

pr_auc_vec(
  truth,
  estimate,
  estimator = NULL,
  na_rm = TRUE,
  event_level = yardstick_event_level(),
  case_weights = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pr_auc_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by <code>truth</code> and
<code>...</code>.</p>
</td></tr>
<tr><td><code id="pr_auc_+3A_...">...</code></td>
<td>
<p>A set of unquoted column names or one or more
<code>dplyr</code> selector functions to choose which variables contain the
class probabilities. If <code>truth</code> is binary, only 1 column should be selected,
and it should correspond to the value of <code>event_level</code>. Otherwise, there
should be as many columns as factor levels of <code>truth</code> and the ordering of
the columns should be the same as the factor levels of <code>truth</code>.</p>
</td></tr>
<tr><td><code id="pr_auc_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a <code>factor</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="pr_auc_+3A_estimator">estimator</code></td>
<td>
<p>One of <code>"binary"</code>, <code>"macro"</code>, or <code>"macro_weighted"</code> to
specify the type of averaging to be done. <code>"binary"</code> is only relevant for
the two class case. The other two are general methods for calculating
multiclass metrics. The default will automatically choose <code>"binary"</code> or
<code>"macro"</code> based on <code>truth</code>.</p>
</td></tr>
<tr><td><code id="pr_auc_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="pr_auc_+3A_event_level">event_level</code></td>
<td>
<p>A single string. Either <code>"first"</code> or <code>"second"</code> to specify
which level of <code>truth</code> to consider as the &quot;event&quot;. This argument is only
applicable when <code>estimator = "binary"</code>. The default uses an internal helper
that defaults to <code>"first"</code>.</p>
</td></tr>
<tr><td><code id="pr_auc_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights.
This should be an unquoted column name that evaluates to a numeric column
in <code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
<tr><td><code id="pr_auc_+3A_estimate">estimate</code></td>
<td>
<p>If <code>truth</code> is binary, a numeric vector of class probabilities
corresponding to the &quot;relevant&quot; class. Otherwise, a matrix with as many
columns as factor levels of <code>truth</code>. <em>It is assumed that these are in the
same order as the levels of <code>truth</code>.</em></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>pr_auc_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Multiclass</h3>

<p>Macro and macro-weighted averaging is available for this metric.
The default is to select macro averaging if a <code>truth</code> factor with more
than 2 levels is provided. Otherwise, a standard binary calculation is done.
See <code>vignette("multiclass", "yardstick")</code> for more information.
</p>


<h3>Relevant Level</h3>

<p>There is no common convention on which factor level should
automatically be considered the &quot;event&quot; or &quot;positive&quot; result
when computing binary classification metrics. In <code>yardstick</code>, the default
is to use the <em>first</em> level. To alter this, change the argument
<code>event_level</code> to <code>"second"</code> to consider the <em>last</em> level of the factor the
level of interest. For multiclass extensions involving one-vs-all
comparisons (such as macro averaging), this option is ignored and
the &quot;one&quot; level is always the relevant result.
</p>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pr_curve">pr_curve()</a></code> for computing the full precision recall curve.
</p>
<p>Other class probability metrics: 
<code><a href="#topic+average_precision">average_precision</a>()</code>,
<code><a href="#topic+brier_class">brier_class</a>()</code>,
<code><a href="#topic+classification_cost">classification_cost</a>()</code>,
<code><a href="#topic+gain_capture">gain_capture</a>()</code>,
<code><a href="#topic+mn_log_loss">mn_log_loss</a>()</code>,
<code><a href="#topic+roc_auc">roc_auc</a>()</code>,
<code><a href="#topic+roc_aunp">roc_aunp</a>()</code>,
<code><a href="#topic+roc_aunu">roc_aunu</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># ---------------------------------------------------------------------------
# Two class example

# `truth` is a 2 level factor. The first level is `"Class1"`, which is the
# "event of interest" by default in yardstick. See the Relevant Level
# section above.
data(two_class_example)

# Binary metrics using class probabilities take a factor `truth` column,
# and a single class probability column containing the probabilities of
# the event of interest. Here, since `"Class1"` is the first level of
# `"truth"`, it is the event of interest and we pass in probabilities for it.
pr_auc(two_class_example, truth, Class1)

# ---------------------------------------------------------------------------
# Multiclass example

# `obs` is a 4 level factor. The first level is `"VF"`, which is the
# "event of interest" by default in yardstick. See the Relevant Level
# section above.
data(hpc_cv)

# You can use the col1:colN tidyselect syntax
library(dplyr)
hpc_cv %&gt;%
  filter(Resample == "Fold01") %&gt;%
  pr_auc(obs, VF:L)

# Change the first level of `obs` from `"VF"` to `"M"` to alter the
# event of interest. The class probability columns should be supplied
# in the same order as the levels.
hpc_cv %&gt;%
  filter(Resample == "Fold01") %&gt;%
  mutate(obs = relevel(obs, "M")) %&gt;%
  pr_auc(obs, M, VF:L)

# Groups are respected
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  pr_auc(obs, VF:L)

# Weighted macro averaging
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  pr_auc(obs, VF:L, estimator = "macro_weighted")

# Vector version
# Supply a matrix of class probabilities
fold1 &lt;- hpc_cv %&gt;%
  filter(Resample == "Fold01")

pr_auc_vec(
   truth = fold1$obs,
   matrix(
     c(fold1$VF, fold1$F, fold1$M, fold1$L),
     ncol = 4
   )
)

</code></pre>

<hr>
<h2 id='pr_curve'>Precision recall curve</h2><span id='topic+pr_curve'></span><span id='topic+pr_curve.data.frame'></span>

<h3>Description</h3>

<p><code>pr_curve()</code> constructs the full precision recall curve and returns a
tibble. See <code><a href="#topic+pr_auc">pr_auc()</a></code> for the area under the precision recall curve.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pr_curve(data, ...)

## S3 method for class 'data.frame'
pr_curve(
  data,
  truth,
  ...,
  na_rm = TRUE,
  event_level = yardstick_event_level(),
  case_weights = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pr_curve_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by <code>truth</code> and
<code>...</code>.</p>
</td></tr>
<tr><td><code id="pr_curve_+3A_...">...</code></td>
<td>
<p>A set of unquoted column names or one or more
<code>dplyr</code> selector functions to choose which variables contain the
class probabilities. If <code>truth</code> is binary, only 1 column should be selected,
and it should correspond to the value of <code>event_level</code>. Otherwise, there
should be as many columns as factor levels of <code>truth</code> and the ordering of
the columns should be the same as the factor levels of <code>truth</code>.</p>
</td></tr>
<tr><td><code id="pr_curve_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a <code>factor</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="pr_curve_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="pr_curve_+3A_event_level">event_level</code></td>
<td>
<p>A single string. Either <code>"first"</code> or <code>"second"</code> to specify
which level of <code>truth</code> to consider as the &quot;event&quot;. This argument is only
applicable when <code>estimator = "binary"</code>. The default uses an internal helper
that defaults to <code>"first"</code>.</p>
</td></tr>
<tr><td><code id="pr_curve_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights.
This should be an unquoted column name that evaluates to a numeric column
in <code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>pr_curve()</code> computes the precision at every unique value of the
probability column (in addition to infinity).
</p>
<p>There is a <code><a href="ggplot2.html#topic+autoplot">ggplot2::autoplot()</a></code>
method for quickly visualizing the curve. This works for
binary and multiclass output, and also works with grouped data (i.e. from
resamples). See the examples.
</p>


<h3>Value</h3>

<p>A tibble with class <code>pr_df</code> or <code>pr_grouped_df</code> having
columns <code>.threshold</code>, <code>recall</code>, and <code>precision</code>.
</p>


<h3>Multiclass</h3>

<p>If a multiclass <code>truth</code> column is provided, a one-vs-all
approach will be taken to calculate multiple curves, one per level.
In this case, there will be an additional column, <code>.level</code>,
identifying the &quot;one&quot; column in the one-vs-all calculation.
</p>


<h3>Relevant Level</h3>

<p>There is no common convention on which factor level should
automatically be considered the &quot;event&quot; or &quot;positive&quot; result
when computing binary classification metrics. In <code>yardstick</code>, the default
is to use the <em>first</em> level. To alter this, change the argument
<code>event_level</code> to <code>"second"</code> to consider the <em>last</em> level of the factor the
level of interest. For multiclass extensions involving one-vs-all
comparisons (such as macro averaging), this option is ignored and
the &quot;one&quot; level is always the relevant result.
</p>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>See Also</h3>

<p>Compute the area under the precision recall curve with <code><a href="#topic+pr_auc">pr_auc()</a></code>.
</p>
<p>Other curve metrics: 
<code><a href="#topic+gain_curve">gain_curve</a>()</code>,
<code><a href="#topic+lift_curve">lift_curve</a>()</code>,
<code><a href="#topic+roc_curve">roc_curve</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># ---------------------------------------------------------------------------
# Two class example

# `truth` is a 2 level factor. The first level is `"Class1"`, which is the
# "event of interest" by default in yardstick. See the Relevant Level
# section above.
data(two_class_example)

# Binary metrics using class probabilities take a factor `truth` column,
# and a single class probability column containing the probabilities of
# the event of interest. Here, since `"Class1"` is the first level of
# `"truth"`, it is the event of interest and we pass in probabilities for it.
pr_curve(two_class_example, truth, Class1)

# ---------------------------------------------------------------------------
# `autoplot()`

# Visualize the curve using ggplot2 manually
library(ggplot2)
library(dplyr)
pr_curve(two_class_example, truth, Class1) %&gt;%
  ggplot(aes(x = recall, y = precision)) +
  geom_path() +
  coord_equal() +
  theme_bw()

# Or use autoplot
autoplot(pr_curve(two_class_example, truth, Class1))

# Multiclass one-vs-all approach
# One curve per level
hpc_cv %&gt;%
  filter(Resample == "Fold01") %&gt;%
  pr_curve(obs, VF:L) %&gt;%
  autoplot()

# Same as above, but will all of the resamples
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  pr_curve(obs, VF:L) %&gt;%
  autoplot()

</code></pre>

<hr>
<h2 id='precision'>Precision</h2><span id='topic+precision'></span><span id='topic+precision.data.frame'></span><span id='topic+precision_vec'></span>

<h3>Description</h3>

<p>These functions calculate the <code><a href="#topic+precision">precision()</a></code> of a measurement system for
finding relevant documents compared to reference results
(the truth regarding relevance). Highly related functions are <code><a href="#topic+recall">recall()</a></code>
and <code><a href="#topic+f_meas">f_meas()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>precision(data, ...)

## S3 method for class 'data.frame'
precision(
  data,
  truth,
  estimate,
  estimator = NULL,
  na_rm = TRUE,
  case_weights = NULL,
  event_level = yardstick_event_level(),
  ...
)

precision_vec(
  truth,
  estimate,
  estimator = NULL,
  na_rm = TRUE,
  case_weights = NULL,
  event_level = yardstick_event_level(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="precision_+3A_data">data</code></td>
<td>
<p>Either a <code>data.frame</code> containing the columns specified by the
<code>truth</code> and <code>estimate</code> arguments, or a <code>table</code>/<code>matrix</code> where the true
class results should be in the columns of the table.</p>
</td></tr>
<tr><td><code id="precision_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="precision_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a <code>factor</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="precision_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predicted class
results (that is also <code>factor</code>). As with <code>truth</code> this can be
specified different ways but the primary method is to use an
unquoted variable name. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="precision_+3A_estimator">estimator</code></td>
<td>
<p>One of: <code>"binary"</code>, <code>"macro"</code>, <code>"macro_weighted"</code>,
or <code>"micro"</code> to specify the type of averaging to be done. <code>"binary"</code> is
only relevant for the two class case. The other three are general methods
for calculating multiclass metrics. The default will automatically choose
<code>"binary"</code> or <code>"macro"</code> based on <code>estimate</code>.</p>
</td></tr>
<tr><td><code id="precision_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="precision_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights.
This should be an unquoted column name that evaluates to a numeric column
in <code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
<tr><td><code id="precision_+3A_event_level">event_level</code></td>
<td>
<p>A single string. Either <code>"first"</code> or <code>"second"</code> to specify
which level of <code>truth</code> to consider as the &quot;event&quot;. This argument is only
applicable when <code>estimator = "binary"</code>. The default uses an internal helper
that defaults to <code>"first"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The precision is the percentage of predicted truly relevant results
of the total number of predicted relevant results and
characterizes the &quot;purity in retrieval performance&quot; (Buckland
and Gey, 1994).
</p>
<p>When the denominator of the calculation is <code>0</code>, precision is undefined. This
happens when both <code style="white-space: pre;">&#8288;# true_positive = 0&#8288;</code> and <code style="white-space: pre;">&#8288;# false_positive = 0&#8288;</code> are true,
which mean that there were no predicted events. When computing binary
precision, a <code>NA</code> value will be returned with a warning. When computing
multiclass precision, the individual <code>NA</code> values will be removed, and the
computation will procede, with a warning.
</p>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>precision_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Relevant Level</h3>

<p>There is no common convention on which factor level should
automatically be considered the &quot;event&quot; or &quot;positive&quot; result
when computing binary classification metrics. In <code>yardstick</code>, the default
is to use the <em>first</em> level. To alter this, change the argument
<code>event_level</code> to <code>"second"</code> to consider the <em>last</em> level of the factor the
level of interest. For multiclass extensions involving one-vs-all
comparisons (such as macro averaging), this option is ignored and
the &quot;one&quot; level is always the relevant result.
</p>


<h3>Multiclass</h3>

<p>Macro, micro, and macro-weighted averaging is available for this metric.
The default is to select macro averaging if a <code>truth</code> factor with more
than 2 levels is provided. Otherwise, a standard binary calculation is done.
See <code>vignette("multiclass", "yardstick")</code> for more information.
</p>


<h3>Implementation</h3>

<p>Suppose a 2x2 table with notation:
</p>

<table>
<tr>
 <td style="text-align: right;"> </td><td style="text-align: center;"> Reference </td><td style="text-align: center;"> </td>
</tr>
<tr>
 <td style="text-align: right;"> Predicted </td><td style="text-align: center;"> Relevant </td><td style="text-align: center;">
Irrelevant </td>
</tr>
<tr>
 <td style="text-align: right;"> Relevant </td><td style="text-align: center;"> A </td><td style="text-align: center;"> B </td>
</tr>
<tr>
 <td style="text-align: right;"> Irrelevant </td><td style="text-align: center;"> C </td><td style="text-align: center;"> D </td>
</tr>
<tr>
 <td style="text-align: right;"> </td>
</tr>

</table>

<p>The formulas used here are:
</p>
<p style="text-align: center;"><code class="reqn">recall = A/(A+C)</code>
</p>

<p style="text-align: center;"><code class="reqn">precision = A/(A+B)</code>
</p>

<p style="text-align: center;"><code class="reqn">F_{meas} = (1+\beta^2) * precision * recall/((\beta^2 * precision)+recall)</code>
</p>

<p>See the references for discussions of the statistics.
</p>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>References</h3>

<p>Buckland, M., &amp; Gey, F. (1994). The relationship
between Recall and Precision. <em>Journal of the American Society
for Information Science</em>, 45(1), 12-19.
</p>
<p>Powers, D. (2007). Evaluation: From Precision, Recall and F
Factor to ROC, Informedness, Markedness and Correlation.
Technical Report SIE-07-001, Flinders University
</p>


<h3>See Also</h3>

<p>Other class metrics: 
<code><a href="#topic+accuracy">accuracy</a>()</code>,
<code><a href="#topic+bal_accuracy">bal_accuracy</a>()</code>,
<code><a href="#topic+detection_prevalence">detection_prevalence</a>()</code>,
<code><a href="#topic+f_meas">f_meas</a>()</code>,
<code><a href="#topic+j_index">j_index</a>()</code>,
<code><a href="#topic+kap">kap</a>()</code>,
<code><a href="#topic+mcc">mcc</a>()</code>,
<code><a href="#topic+npv">npv</a>()</code>,
<code><a href="#topic+ppv">ppv</a>()</code>,
<code><a href="#topic+recall">recall</a>()</code>,
<code><a href="#topic+sens">sens</a>()</code>,
<code><a href="#topic+spec">spec</a>()</code>
</p>
<p>Other relevance metrics: 
<code><a href="#topic+f_meas">f_meas</a>()</code>,
<code><a href="#topic+recall">recall</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Two class
data("two_class_example")
precision(two_class_example, truth, predicted)

# Multiclass
library(dplyr)
data(hpc_cv)

hpc_cv %&gt;%
  filter(Resample == "Fold01") %&gt;%
  precision(obs, pred)

# Groups are respected
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  precision(obs, pred)

# Weighted macro averaging
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  precision(obs, pred, estimator = "macro_weighted")

# Vector version
precision_vec(
  two_class_example$truth,
  two_class_example$predicted
)

# Making Class2 the "relevant" level
precision_vec(
  two_class_example$truth,
  two_class_example$predicted,
  event_level = "second"
)
</code></pre>

<hr>
<h2 id='recall'>Recall</h2><span id='topic+recall'></span><span id='topic+recall.data.frame'></span><span id='topic+recall_vec'></span>

<h3>Description</h3>

<p>These functions calculate the <code><a href="#topic+recall">recall()</a></code> of a measurement system for
finding relevant documents compared to reference results
(the truth regarding relevance). Highly related functions are <code><a href="#topic+precision">precision()</a></code>
and <code><a href="#topic+f_meas">f_meas()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>recall(data, ...)

## S3 method for class 'data.frame'
recall(
  data,
  truth,
  estimate,
  estimator = NULL,
  na_rm = TRUE,
  case_weights = NULL,
  event_level = yardstick_event_level(),
  ...
)

recall_vec(
  truth,
  estimate,
  estimator = NULL,
  na_rm = TRUE,
  case_weights = NULL,
  event_level = yardstick_event_level(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="recall_+3A_data">data</code></td>
<td>
<p>Either a <code>data.frame</code> containing the columns specified by the
<code>truth</code> and <code>estimate</code> arguments, or a <code>table</code>/<code>matrix</code> where the true
class results should be in the columns of the table.</p>
</td></tr>
<tr><td><code id="recall_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="recall_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a <code>factor</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="recall_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predicted class
results (that is also <code>factor</code>). As with <code>truth</code> this can be
specified different ways but the primary method is to use an
unquoted variable name. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="recall_+3A_estimator">estimator</code></td>
<td>
<p>One of: <code>"binary"</code>, <code>"macro"</code>, <code>"macro_weighted"</code>,
or <code>"micro"</code> to specify the type of averaging to be done. <code>"binary"</code> is
only relevant for the two class case. The other three are general methods
for calculating multiclass metrics. The default will automatically choose
<code>"binary"</code> or <code>"macro"</code> based on <code>estimate</code>.</p>
</td></tr>
<tr><td><code id="recall_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="recall_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights.
This should be an unquoted column name that evaluates to a numeric column
in <code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
<tr><td><code id="recall_+3A_event_level">event_level</code></td>
<td>
<p>A single string. Either <code>"first"</code> or <code>"second"</code> to specify
which level of <code>truth</code> to consider as the &quot;event&quot;. This argument is only
applicable when <code>estimator = "binary"</code>. The default uses an internal helper
that defaults to <code>"first"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The recall (aka sensitivity) is defined as the proportion of
relevant results out of the number of samples which were
actually relevant. When there are no relevant results, recall is
not defined and a value of <code>NA</code> is returned.
</p>
<p>When the denominator of the calculation is <code>0</code>, recall is undefined. This
happens when both <code style="white-space: pre;">&#8288;# true_positive = 0&#8288;</code> and <code style="white-space: pre;">&#8288;# false_negative = 0&#8288;</code> are true,
which mean that there were no true events. When computing binary
recall, a <code>NA</code> value will be returned with a warning. When computing
multiclass recall, the individual <code>NA</code> values will be removed, and the
computation will procede, with a warning.
</p>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>recall_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Relevant Level</h3>

<p>There is no common convention on which factor level should
automatically be considered the &quot;event&quot; or &quot;positive&quot; result
when computing binary classification metrics. In <code>yardstick</code>, the default
is to use the <em>first</em> level. To alter this, change the argument
<code>event_level</code> to <code>"second"</code> to consider the <em>last</em> level of the factor the
level of interest. For multiclass extensions involving one-vs-all
comparisons (such as macro averaging), this option is ignored and
the &quot;one&quot; level is always the relevant result.
</p>


<h3>Multiclass</h3>

<p>Macro, micro, and macro-weighted averaging is available for this metric.
The default is to select macro averaging if a <code>truth</code> factor with more
than 2 levels is provided. Otherwise, a standard binary calculation is done.
See <code>vignette("multiclass", "yardstick")</code> for more information.
</p>


<h3>Implementation</h3>

<p>Suppose a 2x2 table with notation:
</p>

<table>
<tr>
 <td style="text-align: right;"> </td><td style="text-align: center;"> Reference </td><td style="text-align: center;"> </td>
</tr>
<tr>
 <td style="text-align: right;"> Predicted </td><td style="text-align: center;"> Relevant </td><td style="text-align: center;">
Irrelevant </td>
</tr>
<tr>
 <td style="text-align: right;"> Relevant </td><td style="text-align: center;"> A </td><td style="text-align: center;"> B </td>
</tr>
<tr>
 <td style="text-align: right;"> Irrelevant </td><td style="text-align: center;"> C </td><td style="text-align: center;"> D </td>
</tr>
<tr>
 <td style="text-align: right;"> </td>
</tr>

</table>

<p>The formulas used here are:
</p>
<p style="text-align: center;"><code class="reqn">recall = A/(A+C)</code>
</p>

<p style="text-align: center;"><code class="reqn">precision = A/(A+B)</code>
</p>

<p style="text-align: center;"><code class="reqn">F_{meas} = (1+\beta^2) * precision * recall/((\beta^2 * precision)+recall)</code>
</p>

<p>See the references for discussions of the statistics.
</p>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>References</h3>

<p>Buckland, M., &amp; Gey, F. (1994). The relationship
between Recall and Precision. <em>Journal of the American Society
for Information Science</em>, 45(1), 12-19.
</p>
<p>Powers, D. (2007). Evaluation: From Precision, Recall and F
Factor to ROC, Informedness, Markedness and Correlation.
Technical Report SIE-07-001, Flinders University
</p>


<h3>See Also</h3>

<p>Other class metrics: 
<code><a href="#topic+accuracy">accuracy</a>()</code>,
<code><a href="#topic+bal_accuracy">bal_accuracy</a>()</code>,
<code><a href="#topic+detection_prevalence">detection_prevalence</a>()</code>,
<code><a href="#topic+f_meas">f_meas</a>()</code>,
<code><a href="#topic+j_index">j_index</a>()</code>,
<code><a href="#topic+kap">kap</a>()</code>,
<code><a href="#topic+mcc">mcc</a>()</code>,
<code><a href="#topic+npv">npv</a>()</code>,
<code><a href="#topic+ppv">ppv</a>()</code>,
<code><a href="#topic+precision">precision</a>()</code>,
<code><a href="#topic+sens">sens</a>()</code>,
<code><a href="#topic+spec">spec</a>()</code>
</p>
<p>Other relevance metrics: 
<code><a href="#topic+f_meas">f_meas</a>()</code>,
<code><a href="#topic+precision">precision</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Two class
data("two_class_example")
recall(two_class_example, truth, predicted)

# Multiclass
library(dplyr)
data(hpc_cv)

hpc_cv %&gt;%
  filter(Resample == "Fold01") %&gt;%
  recall(obs, pred)

# Groups are respected
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  recall(obs, pred)

# Weighted macro averaging
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  recall(obs, pred, estimator = "macro_weighted")

# Vector version
recall_vec(
  two_class_example$truth,
  two_class_example$predicted
)

# Making Class2 the "relevant" level
recall_vec(
  two_class_example$truth,
  two_class_example$predicted,
  event_level = "second"
)
</code></pre>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic+tidy'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>generics</dt><dd><p><code><a href="generics.html#topic+tidy">tidy</a></code></p>
</dd>
</dl>

<hr>
<h2 id='rmse'>Root mean squared error</h2><span id='topic+rmse'></span><span id='topic+rmse.data.frame'></span><span id='topic+rmse_vec'></span>

<h3>Description</h3>

<p>Calculate the root mean squared error. <code>rmse()</code> is a metric that is in
the same units as the original data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmse(data, ...)

## S3 method for class 'data.frame'
rmse(data, truth, estimate, na_rm = TRUE, case_weights = NULL, ...)

rmse_vec(truth, estimate, na_rm = TRUE, case_weights = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rmse_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by the <code>truth</code>
and <code>estimate</code> arguments.</p>
</td></tr>
<tr><td><code id="rmse_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="rmse_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true results
(that is <code>numeric</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>numeric</code> vector.</p>
</td></tr>
<tr><td><code id="rmse_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predicted
results (that is also <code>numeric</code>). As with <code>truth</code> this can be
specified different ways but the primary method is to use an
unquoted variable name. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>numeric</code> vector.</p>
</td></tr>
<tr><td><code id="rmse_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="rmse_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights. This
should be an unquoted column name that evaluates to a numeric column in
<code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>rmse_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>See Also</h3>

<p>Other numeric metrics: 
<code><a href="#topic+ccc">ccc</a>()</code>,
<code><a href="#topic+huber_loss_pseudo">huber_loss_pseudo</a>()</code>,
<code><a href="#topic+huber_loss">huber_loss</a>()</code>,
<code><a href="#topic+iic">iic</a>()</code>,
<code><a href="#topic+mae">mae</a>()</code>,
<code><a href="#topic+mape">mape</a>()</code>,
<code><a href="#topic+mase">mase</a>()</code>,
<code><a href="#topic+mpe">mpe</a>()</code>,
<code><a href="#topic+msd">msd</a>()</code>,
<code><a href="#topic+poisson_log_loss">poisson_log_loss</a>()</code>,
<code><a href="#topic+rpd">rpd</a>()</code>,
<code><a href="#topic+rpiq">rpiq</a>()</code>,
<code><a href="#topic+rsq_trad">rsq_trad</a>()</code>,
<code><a href="#topic+rsq">rsq</a>()</code>,
<code><a href="#topic+smape">smape</a>()</code>
</p>
<p>Other accuracy metrics: 
<code><a href="#topic+ccc">ccc</a>()</code>,
<code><a href="#topic+huber_loss_pseudo">huber_loss_pseudo</a>()</code>,
<code><a href="#topic+huber_loss">huber_loss</a>()</code>,
<code><a href="#topic+iic">iic</a>()</code>,
<code><a href="#topic+mae">mae</a>()</code>,
<code><a href="#topic+mape">mape</a>()</code>,
<code><a href="#topic+mase">mase</a>()</code>,
<code><a href="#topic+mpe">mpe</a>()</code>,
<code><a href="#topic+msd">msd</a>()</code>,
<code><a href="#topic+poisson_log_loss">poisson_log_loss</a>()</code>,
<code><a href="#topic+smape">smape</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Supply truth and predictions as bare column names
rmse(solubility_test, solubility, prediction)

library(dplyr)

set.seed(1234)
size &lt;- 100
times &lt;- 10

# create 10 resamples
solubility_resampled &lt;- bind_rows(
  replicate(
    n = times,
    expr = sample_n(solubility_test, size, replace = TRUE),
    simplify = FALSE
  ),
  .id = "resample"
)

# Compute the metric by group
metric_results &lt;- solubility_resampled %&gt;%
  group_by(resample) %&gt;%
  rmse(solubility, prediction)

metric_results

# Resampled mean estimate
metric_results %&gt;%
  summarise(avg_estimate = mean(.estimate))
</code></pre>

<hr>
<h2 id='roc_auc'>Area under the receiver operator curve</h2><span id='topic+roc_auc'></span><span id='topic+roc_auc.data.frame'></span><span id='topic+roc_auc_vec'></span>

<h3>Description</h3>

<p><code>roc_auc()</code> is a metric that computes the area under the ROC curve. See
<code><a href="#topic+roc_curve">roc_curve()</a></code> for the full curve.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>roc_auc(data, ...)

## S3 method for class 'data.frame'
roc_auc(
  data,
  truth,
  ...,
  estimator = NULL,
  na_rm = TRUE,
  event_level = yardstick_event_level(),
  case_weights = NULL,
  options = list()
)

roc_auc_vec(
  truth,
  estimate,
  estimator = NULL,
  na_rm = TRUE,
  event_level = yardstick_event_level(),
  case_weights = NULL,
  options = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="roc_auc_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by <code>truth</code> and
<code>...</code>.</p>
</td></tr>
<tr><td><code id="roc_auc_+3A_...">...</code></td>
<td>
<p>A set of unquoted column names or one or more
<code>dplyr</code> selector functions to choose which variables contain the
class probabilities. If <code>truth</code> is binary, only 1 column should be selected,
and it should correspond to the value of <code>event_level</code>. Otherwise, there
should be as many columns as factor levels of <code>truth</code> and the ordering of
the columns should be the same as the factor levels of <code>truth</code>.</p>
</td></tr>
<tr><td><code id="roc_auc_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a <code>factor</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="roc_auc_+3A_estimator">estimator</code></td>
<td>
<p>One of <code>"binary"</code>, <code>"hand_till"</code>, <code>"macro"</code>, or
<code>"macro_weighted"</code> to specify the type of averaging to be done. <code>"binary"</code>
is only relevant for the two class case. The others are general methods for
calculating multiclass metrics. The default will automatically choose
<code>"binary"</code> if <code>truth</code> is binary, <code>"hand_till"</code> if <code>truth</code> has &gt;2 levels and
<code>case_weights</code> isn't specified, or <code>"macro"</code> if <code>truth</code> has &gt;2 levels and
<code>case_weights</code> is specified (in which case <code>"hand_till"</code> isn't
well-defined).</p>
</td></tr>
<tr><td><code id="roc_auc_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="roc_auc_+3A_event_level">event_level</code></td>
<td>
<p>A single string. Either <code>"first"</code> or <code>"second"</code> to specify
which level of <code>truth</code> to consider as the &quot;event&quot;. This argument is only
applicable when <code>estimator = "binary"</code>. The default uses an internal helper
that defaults to <code>"first"</code>.</p>
</td></tr>
<tr><td><code id="roc_auc_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights.
This should be an unquoted column name that evaluates to a numeric column
in <code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
<tr><td><code id="roc_auc_+3A_options">options</code></td>
<td>
<p><code style="white-space: pre;">&#8288;[deprecated]&#8288;</code>
</p>
<p>No longer supported as of yardstick 1.0.0. If you pass something here it
will be ignored with a warning.
</p>
<p>Previously, these were options passed on to <code>pROC::roc()</code>. If you need
support for this, use the pROC package directly.</p>
</td></tr>
<tr><td><code id="roc_auc_+3A_estimate">estimate</code></td>
<td>
<p>If <code>truth</code> is binary, a numeric vector of class probabilities
corresponding to the &quot;relevant&quot; class. Otherwise, a matrix with as many
columns as factor levels of <code>truth</code>. <em>It is assumed that these are in the
same order as the levels of <code>truth</code>.</em></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Generally, an ROC AUC value is between <code>0.5</code> and <code>1</code>, with <code>1</code> being a
perfect prediction model. If your value is between <code>0</code> and <code>0.5</code>, then
this implies that you have meaningful information in your model, but it
is being applied incorrectly because doing the opposite of what the model
predicts would result in an AUC <code style="white-space: pre;">&#8288;&gt;0.5&#8288;</code>.
</p>
<p>Note that you can't combine <code>estimator = "hand_till"</code> with <code>case_weights</code>.
</p>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>roc_auc_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Relevant Level</h3>

<p>There is no common convention on which factor level should
automatically be considered the &quot;event&quot; or &quot;positive&quot; result
when computing binary classification metrics. In <code>yardstick</code>, the default
is to use the <em>first</em> level. To alter this, change the argument
<code>event_level</code> to <code>"second"</code> to consider the <em>last</em> level of the factor the
level of interest. For multiclass extensions involving one-vs-all
comparisons (such as macro averaging), this option is ignored and
the &quot;one&quot; level is always the relevant result.
</p>


<h3>Multiclass</h3>

<p>The default multiclass method for computing <code>roc_auc()</code> is to use the
method from Hand, Till, (2001). Unlike macro-averaging, this method is
insensitive to class distributions like the binary ROC AUC case.
Additionally, while other multiclass techniques will return <code>NA</code> if any
levels in <code>truth</code> occur zero times in the actual data, the Hand-Till method
will simply ignore those levels in the averaging calculation, with a warning.
</p>
<p>Macro and macro-weighted averaging are still provided, even though they are
not the default. In fact, macro-weighted averaging corresponds to the same
definition of multiclass AUC given by Provost and Domingos (2001).
</p>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>References</h3>

<p>Hand, Till (2001). &quot;A Simple Generalisation of the Area Under the
ROC Curve for Multiple Class Classification Problems&quot;. <em>Machine Learning</em>.
Vol 45, Iss 2, pp 171-186.
</p>
<p>Fawcett (2005). &quot;An introduction to ROC analysis&quot;. <em>Pattern Recognition
Letters</em>. 27 (2006), pp 861-874.
</p>
<p>Provost, F., Domingos, P., 2001. &quot;Well-trained PETs: Improving probability
estimation trees&quot;, CeDER Working Paper #IS-00-04, Stern School of Business,
New York University, NY, NY 10012.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+roc_curve">roc_curve()</a></code> for computing the full ROC curve.
</p>
<p>Other class probability metrics: 
<code><a href="#topic+average_precision">average_precision</a>()</code>,
<code><a href="#topic+brier_class">brier_class</a>()</code>,
<code><a href="#topic+classification_cost">classification_cost</a>()</code>,
<code><a href="#topic+gain_capture">gain_capture</a>()</code>,
<code><a href="#topic+mn_log_loss">mn_log_loss</a>()</code>,
<code><a href="#topic+pr_auc">pr_auc</a>()</code>,
<code><a href="#topic+roc_aunp">roc_aunp</a>()</code>,
<code><a href="#topic+roc_aunu">roc_aunu</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># ---------------------------------------------------------------------------
# Two class example

# `truth` is a 2 level factor. The first level is `"Class1"`, which is the
# "event of interest" by default in yardstick. See the Relevant Level
# section above.
data(two_class_example)

# Binary metrics using class probabilities take a factor `truth` column,
# and a single class probability column containing the probabilities of
# the event of interest. Here, since `"Class1"` is the first level of
# `"truth"`, it is the event of interest and we pass in probabilities for it.
roc_auc(two_class_example, truth, Class1)

# ---------------------------------------------------------------------------
# Multiclass example

# `obs` is a 4 level factor. The first level is `"VF"`, which is the
# "event of interest" by default in yardstick. See the Relevant Level
# section above.
data(hpc_cv)

# You can use the col1:colN tidyselect syntax
library(dplyr)
hpc_cv %&gt;%
  filter(Resample == "Fold01") %&gt;%
  roc_auc(obs, VF:L)

# Change the first level of `obs` from `"VF"` to `"M"` to alter the
# event of interest. The class probability columns should be supplied
# in the same order as the levels.
hpc_cv %&gt;%
  filter(Resample == "Fold01") %&gt;%
  mutate(obs = relevel(obs, "M")) %&gt;%
  roc_auc(obs, M, VF:L)

# Groups are respected
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  roc_auc(obs, VF:L)

# Weighted macro averaging
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  roc_auc(obs, VF:L, estimator = "macro_weighted")

# Vector version
# Supply a matrix of class probabilities
fold1 &lt;- hpc_cv %&gt;%
  filter(Resample == "Fold01")

roc_auc_vec(
   truth = fold1$obs,
   matrix(
     c(fold1$VF, fold1$F, fold1$M, fold1$L),
     ncol = 4
   )
)

</code></pre>

<hr>
<h2 id='roc_auc_survival'>Time-Dependent ROC AUC for Censored Data</h2><span id='topic+roc_auc_survival'></span><span id='topic+roc_auc_survival.data.frame'></span><span id='topic+roc_auc_survival_vec'></span>

<h3>Description</h3>

<p>Compute the area under the ROC survival curve using predicted survival
probabilities that corresponds to different time points.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>roc_auc_survival(data, ...)

## S3 method for class 'data.frame'
roc_auc_survival(data, truth, ..., na_rm = TRUE, case_weights = NULL)

roc_auc_survival_vec(truth, estimate, na_rm = TRUE, case_weights = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="roc_auc_survival_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by <code>truth</code> and
<code>...</code>.</p>
</td></tr>
<tr><td><code id="roc_auc_survival_+3A_...">...</code></td>
<td>
<p>The column identifier for the survival probabilities this
should be a list column of data.frames corresponding to the output given when
predicting with <a href="https://censored.tidymodels.org/">censored</a> model. This
should be an unquoted column name although this argument is passed by
expression and supports <a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can
unquote column names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, the dots are not used.</p>
</td></tr>
<tr><td><code id="roc_auc_survival_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true survival result (that
is created using <code><a href="survival.html#topic+Surv">survival::Surv()</a></code>.). This should be an unquoted column name
although this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column names). For
<code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, an <code><a href="survival.html#topic+Surv">survival::Surv()</a></code> object.</p>
</td></tr>
<tr><td><code id="roc_auc_survival_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="roc_auc_survival_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights.
This should be an unquoted column name that evaluates to a numeric column
in <code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
<tr><td><code id="roc_auc_survival_+3A_estimate">estimate</code></td>
<td>
<p>A list column of data.frames corresponding to the output
given when predicting with <a href="https://censored.tidymodels.org/">censored</a>
model. See the details for more information regarding format.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This formulation takes survival probability predictions at one or more
specific <em>evaluation times</em> and, for each time, computes the area under the
ROC curve. To account for censoring, inverse probability of censoring weights
(IPCW) are used in the calculations. See equation 7 of section 4.3 in
Blanche <em>at al</em> (2013) for the details.
</p>
<p>The column passed to <code>...</code> should be a list column with one element per
independent experiential unit (e.g. patient). The list column should contain
data frames with several columns:
</p>

<ul>
<li> <p><code>.eval_time</code>: The time that the prediction is made.
</p>
</li>
<li> <p><code>.pred_survival</code>: The predicted probability of survival up to <code>.eval_time</code>
</p>
</li>
<li> <p><code>.weight_censored</code>: The case weight for the inverse probability of censoring.
</p>
</li></ul>

<p>The last column can be produced using <code><a href="parsnip.html#topic+censoring_weights">parsnip::.censoring_weights_graf()</a></code>.
This corresponds to the weighting scheme of  Graf <em>et al</em> (1999). The
internal data set <code>lung_surv</code> shows an example of the format.
</p>
<p>This method automatically groups by the <code>.eval_time</code> argument.
</p>
<p>Larger values of the score are associated with better model performance.
</p>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>, and <code>.estimate</code>.
</p>
<p>For an ungrouped data frame, the result has one row of values. For a grouped data frame,
the number of rows returned is the same as the number of groups.
</p>
<p>For <code>roc_auc_survival_vec()</code>, a <code>numeric</code> vector same length as the input argument
<code>eval_time</code>. (or <code>NA</code>).
</p>


<h3>Author(s)</h3>

<p>Emil Hvitfeldt
</p>


<h3>References</h3>

<p>Blanche, P., Dartigues, J.-F. and Jacqmin-Gadda, H. (2013), Review and
comparison of ROC curve estimators for a time-dependent outcome with
marker-dependent censoring. <em>Biom. J.</em>, 55: 687-704.
</p>
<p>Graf, E., Schmoor, C., Sauerbrei, W. and Schumacher, M. (1999), Assessment
and comparison of prognostic classification schemes for survival data.
<em>Statist. Med.</em>, 18: 2529-2545.
</p>


<h3>See Also</h3>

<p>Compute the ROC survival curve with <code><a href="#topic+roc_curve_survival">roc_curve_survival()</a></code>.
</p>
<p>Other dynamic survival metrics: 
<code><a href="#topic+brier_survival_integrated">brier_survival_integrated</a>()</code>,
<code><a href="#topic+brier_survival">brier_survival</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)

lung_surv %&gt;%
  roc_auc_survival(
    truth = surv_obj,
    .pred
  )
</code></pre>

<hr>
<h2 id='roc_aunp'>Area under the ROC curve of each class against the rest, using the a priori
class distribution</h2><span id='topic+roc_aunp'></span><span id='topic+roc_aunp.data.frame'></span><span id='topic+roc_aunp_vec'></span>

<h3>Description</h3>

<p><code>roc_aunp()</code> is a multiclass metric that computes the area under the ROC
curve of each class against the rest, using the a priori class distribution.
This is equivalent to <code>roc_auc(estimator = "macro_weighted")</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>roc_aunp(data, ...)

## S3 method for class 'data.frame'
roc_aunp(data, truth, ..., na_rm = TRUE, case_weights = NULL, options = list())

roc_aunp_vec(
  truth,
  estimate,
  na_rm = TRUE,
  case_weights = NULL,
  options = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="roc_aunp_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by <code>truth</code> and
<code>...</code>.</p>
</td></tr>
<tr><td><code id="roc_aunp_+3A_...">...</code></td>
<td>
<p>A set of unquoted column names or one or more <code>dplyr</code> selector
functions to choose which variables contain the class probabilities. There
should be as many columns as factor levels of <code>truth</code>.</p>
</td></tr>
<tr><td><code id="roc_aunp_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a <code>factor</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="roc_aunp_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="roc_aunp_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights.
This should be an unquoted column name that evaluates to a numeric column
in <code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
<tr><td><code id="roc_aunp_+3A_options">options</code></td>
<td>
<p><code style="white-space: pre;">&#8288;[deprecated]&#8288;</code>
</p>
<p>No longer supported as of yardstick 1.0.0. If you pass something here it
will be ignored with a warning.
</p>
<p>Previously, these were options passed on to <code>pROC::roc()</code>. If you need
support for this, use the pROC package directly.</p>
</td></tr>
<tr><td><code id="roc_aunp_+3A_estimate">estimate</code></td>
<td>
<p>A matrix with as many
columns as factor levels of <code>truth</code>. <em>It is assumed that these are in the
same order as the levels of <code>truth</code>.</em></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>roc_aunp_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Relevant Level</h3>

<p>There is no common convention on which factor level should
automatically be considered the &quot;event&quot; or &quot;positive&quot; result
when computing binary classification metrics. In <code>yardstick</code>, the default
is to use the <em>first</em> level. To alter this, change the argument
<code>event_level</code> to <code>"second"</code> to consider the <em>last</em> level of the factor the
level of interest. For multiclass extensions involving one-vs-all
comparisons (such as macro averaging), this option is ignored and
the &quot;one&quot; level is always the relevant result.
</p>


<h3>Multiclass</h3>

<p>This multiclass method for computing the area under the ROC curve uses the
a priori class distribution and is equivalent to
<code>roc_auc(estimator = "macro_weighted")</code>.
</p>


<h3>Author(s)</h3>

<p>Julia Silge
</p>


<h3>References</h3>

<p>Ferri, C., Hernández-Orallo, J., &amp; Modroiu, R. (2009). &quot;An experimental
comparison of performance measures for classification&quot;. <em>Pattern Recognition
Letters</em>. 30 (1), pp 27-38.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+roc_aunu">roc_aunu()</a></code> for computing the area under the ROC curve of each class against
the rest, using the uniform class distribution.
</p>
<p>Other class probability metrics: 
<code><a href="#topic+average_precision">average_precision</a>()</code>,
<code><a href="#topic+brier_class">brier_class</a>()</code>,
<code><a href="#topic+classification_cost">classification_cost</a>()</code>,
<code><a href="#topic+gain_capture">gain_capture</a>()</code>,
<code><a href="#topic+mn_log_loss">mn_log_loss</a>()</code>,
<code><a href="#topic+pr_auc">pr_auc</a>()</code>,
<code><a href="#topic+roc_auc">roc_auc</a>()</code>,
<code><a href="#topic+roc_aunu">roc_aunu</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Multiclass example

# `obs` is a 4 level factor. The first level is `"VF"`, which is the
# "event of interest" by default in yardstick. See the Relevant Level
# section above.
data(hpc_cv)

# You can use the col1:colN tidyselect syntax
library(dplyr)
hpc_cv %&gt;%
  filter(Resample == "Fold01") %&gt;%
  roc_aunp(obs, VF:L)

# Change the first level of `obs` from `"VF"` to `"M"` to alter the
# event of interest. The class probability columns should be supplied
# in the same order as the levels.
hpc_cv %&gt;%
  filter(Resample == "Fold01") %&gt;%
  mutate(obs = relevel(obs, "M")) %&gt;%
  roc_aunp(obs, M, VF:L)

# Groups are respected
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  roc_aunp(obs, VF:L)

# Vector version
# Supply a matrix of class probabilities
fold1 &lt;- hpc_cv %&gt;%
  filter(Resample == "Fold01")

roc_aunp_vec(
  truth = fold1$obs,
  matrix(
    c(fold1$VF, fold1$F, fold1$M, fold1$L),
    ncol = 4
  )
)
</code></pre>

<hr>
<h2 id='roc_aunu'>Area under the ROC curve of each class against the rest, using the uniform
class distribution</h2><span id='topic+roc_aunu'></span><span id='topic+roc_aunu.data.frame'></span><span id='topic+roc_aunu_vec'></span>

<h3>Description</h3>

<p><code>roc_aunu()</code> is a multiclass metric that computes the area under the ROC
curve of each class against the rest, using the uniform class distribution.
This is equivalent to <code>roc_auc(estimator = "macro")</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>roc_aunu(data, ...)

## S3 method for class 'data.frame'
roc_aunu(data, truth, ..., na_rm = TRUE, case_weights = NULL, options = list())

roc_aunu_vec(
  truth,
  estimate,
  na_rm = TRUE,
  case_weights = NULL,
  options = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="roc_aunu_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by <code>truth</code> and
<code>...</code>.</p>
</td></tr>
<tr><td><code id="roc_aunu_+3A_...">...</code></td>
<td>
<p>A set of unquoted column names or one or more <code>dplyr</code> selector
functions to choose which variables contain the class probabilities. There
should be as many columns as factor levels of <code>truth</code>.</p>
</td></tr>
<tr><td><code id="roc_aunu_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a <code>factor</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="roc_aunu_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="roc_aunu_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights.
This should be an unquoted column name that evaluates to a numeric column
in <code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
<tr><td><code id="roc_aunu_+3A_options">options</code></td>
<td>
<p><code style="white-space: pre;">&#8288;[deprecated]&#8288;</code>
</p>
<p>No longer supported as of yardstick 1.0.0. If you pass something here it
will be ignored with a warning.
</p>
<p>Previously, these were options passed on to <code>pROC::roc()</code>. If you need
support for this, use the pROC package directly.</p>
</td></tr>
<tr><td><code id="roc_aunu_+3A_estimate">estimate</code></td>
<td>
<p>A matrix with as many
columns as factor levels of <code>truth</code>. <em>It is assumed that these are in the
same order as the levels of <code>truth</code>.</em></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>roc_aunu_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Relevant Level</h3>

<p>There is no common convention on which factor level should
automatically be considered the &quot;event&quot; or &quot;positive&quot; result
when computing binary classification metrics. In <code>yardstick</code>, the default
is to use the <em>first</em> level. To alter this, change the argument
<code>event_level</code> to <code>"second"</code> to consider the <em>last</em> level of the factor the
level of interest. For multiclass extensions involving one-vs-all
comparisons (such as macro averaging), this option is ignored and
the &quot;one&quot; level is always the relevant result.
</p>


<h3>Multiclass</h3>

<p>This multiclass method for computing the area under the ROC curve uses the
uniform class distribution and is equivalent to
<code>roc_auc(estimator = "macro")</code>.
</p>


<h3>Author(s)</h3>

<p>Julia Silge
</p>


<h3>References</h3>

<p>Ferri, C., Hernández-Orallo, J., &amp; Modroiu, R. (2009). &quot;An experimental
comparison of performance measures for classification&quot;. <em>Pattern Recognition
Letters</em>. 30 (1), pp 27-38.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+roc_aunp">roc_aunp()</a></code> for computing the area under the ROC curve of each class against
the rest, using the a priori class distribution.
</p>
<p>Other class probability metrics: 
<code><a href="#topic+average_precision">average_precision</a>()</code>,
<code><a href="#topic+brier_class">brier_class</a>()</code>,
<code><a href="#topic+classification_cost">classification_cost</a>()</code>,
<code><a href="#topic+gain_capture">gain_capture</a>()</code>,
<code><a href="#topic+mn_log_loss">mn_log_loss</a>()</code>,
<code><a href="#topic+pr_auc">pr_auc</a>()</code>,
<code><a href="#topic+roc_auc">roc_auc</a>()</code>,
<code><a href="#topic+roc_aunp">roc_aunp</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Multiclass example

# `obs` is a 4 level factor. The first level is `"VF"`, which is the
# "event of interest" by default in yardstick. See the Relevant Level
# section above.
data(hpc_cv)

# You can use the col1:colN tidyselect syntax
library(dplyr)
hpc_cv %&gt;%
  filter(Resample == "Fold01") %&gt;%
  roc_aunu(obs, VF:L)

# Change the first level of `obs` from `"VF"` to `"M"` to alter the
# event of interest. The class probability columns should be supplied
# in the same order as the levels.
hpc_cv %&gt;%
  filter(Resample == "Fold01") %&gt;%
  mutate(obs = relevel(obs, "M")) %&gt;%
  roc_aunu(obs, M, VF:L)

# Groups are respected
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  roc_aunu(obs, VF:L)

# Vector version
# Supply a matrix of class probabilities
fold1 &lt;- hpc_cv %&gt;%
  filter(Resample == "Fold01")

roc_aunu_vec(
  truth = fold1$obs,
  matrix(
    c(fold1$VF, fold1$F, fold1$M, fold1$L),
    ncol = 4
  )
)
</code></pre>

<hr>
<h2 id='roc_curve'>Receiver operator curve</h2><span id='topic+roc_curve'></span><span id='topic+roc_curve.data.frame'></span>

<h3>Description</h3>

<p><code>roc_curve()</code> constructs the full ROC curve and returns a
tibble. See <code><a href="#topic+roc_auc">roc_auc()</a></code> for the area under the ROC curve.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>roc_curve(data, ...)

## S3 method for class 'data.frame'
roc_curve(
  data,
  truth,
  ...,
  na_rm = TRUE,
  event_level = yardstick_event_level(),
  case_weights = NULL,
  options = list()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="roc_curve_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by <code>truth</code> and
<code>...</code>.</p>
</td></tr>
<tr><td><code id="roc_curve_+3A_...">...</code></td>
<td>
<p>A set of unquoted column names or one or more
<code>dplyr</code> selector functions to choose which variables contain the
class probabilities. If <code>truth</code> is binary, only 1 column should be selected,
and it should correspond to the value of <code>event_level</code>. Otherwise, there
should be as many columns as factor levels of <code>truth</code> and the ordering of
the columns should be the same as the factor levels of <code>truth</code>.</p>
</td></tr>
<tr><td><code id="roc_curve_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a <code>factor</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="roc_curve_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="roc_curve_+3A_event_level">event_level</code></td>
<td>
<p>A single string. Either <code>"first"</code> or <code>"second"</code> to specify
which level of <code>truth</code> to consider as the &quot;event&quot;. This argument is only
applicable when <code>estimator = "binary"</code>. The default uses an internal helper
that defaults to <code>"first"</code>.</p>
</td></tr>
<tr><td><code id="roc_curve_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights.
This should be an unquoted column name that evaluates to a numeric column
in <code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
<tr><td><code id="roc_curve_+3A_options">options</code></td>
<td>
<p><code style="white-space: pre;">&#8288;[deprecated]&#8288;</code>
</p>
<p>No longer supported as of yardstick 1.0.0. If you pass something here it
will be ignored with a warning.
</p>
<p>Previously, these were options passed on to <code>pROC::roc()</code>. If you need
support for this, use the pROC package directly.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>roc_curve()</code> computes the sensitivity at every unique
value of the probability column (in addition to infinity and
minus infinity).
</p>
<p>There is a <code><a href="ggplot2.html#topic+autoplot">ggplot2::autoplot()</a></code> method for quickly visualizing the curve.
This works for binary and multiclass output, and also works with grouped
data (i.e. from resamples). See the examples.
</p>


<h3>Value</h3>

<p>A tibble with class <code>roc_df</code> or <code>roc_grouped_df</code> having
columns <code>.threshold</code>, <code>specificity</code>, and <code>sensitivity</code>.
</p>


<h3>Multiclass</h3>

<p>If a multiclass <code>truth</code> column is provided, a one-vs-all
approach will be taken to calculate multiple curves, one per level.
In this case, there will be an additional column, <code>.level</code>,
identifying the &quot;one&quot; column in the one-vs-all calculation.
</p>


<h3>Relevant Level</h3>

<p>There is no common convention on which factor level should
automatically be considered the &quot;event&quot; or &quot;positive&quot; result
when computing binary classification metrics. In <code>yardstick</code>, the default
is to use the <em>first</em> level. To alter this, change the argument
<code>event_level</code> to <code>"second"</code> to consider the <em>last</em> level of the factor the
level of interest. For multiclass extensions involving one-vs-all
comparisons (such as macro averaging), this option is ignored and
the &quot;one&quot; level is always the relevant result.
</p>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>See Also</h3>

<p>Compute the area under the ROC curve with <code><a href="#topic+roc_auc">roc_auc()</a></code>.
</p>
<p>Other curve metrics: 
<code><a href="#topic+gain_curve">gain_curve</a>()</code>,
<code><a href="#topic+lift_curve">lift_curve</a>()</code>,
<code><a href="#topic+pr_curve">pr_curve</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># ---------------------------------------------------------------------------
# Two class example

# `truth` is a 2 level factor. The first level is `"Class1"`, which is the
# "event of interest" by default in yardstick. See the Relevant Level
# section above.
data(two_class_example)

# Binary metrics using class probabilities take a factor `truth` column,
# and a single class probability column containing the probabilities of
# the event of interest. Here, since `"Class1"` is the first level of
# `"truth"`, it is the event of interest and we pass in probabilities for it.
roc_curve(two_class_example, truth, Class1)

# ---------------------------------------------------------------------------
# `autoplot()`

# Visualize the curve using ggplot2 manually
library(ggplot2)
library(dplyr)
roc_curve(two_class_example, truth, Class1) %&gt;%
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  theme_bw()

# Or use autoplot
autoplot(roc_curve(two_class_example, truth, Class1))

## Not run: 

# Multiclass one-vs-all approach
# One curve per level
hpc_cv %&gt;%
  filter(Resample == "Fold01") %&gt;%
  roc_curve(obs, VF:L) %&gt;%
  autoplot()

# Same as above, but will all of the resamples
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  roc_curve(obs, VF:L) %&gt;%
  autoplot()

## End(Not run)

</code></pre>

<hr>
<h2 id='roc_curve_survival'>Time-Dependent ROC surve for Censored Data</h2><span id='topic+roc_curve_survival'></span><span id='topic+roc_curve_survival.data.frame'></span>

<h3>Description</h3>

<p>Compute the ROC survival curve using predicted survival probabilities that
corresponds to different time points.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>roc_curve_survival(data, ...)

## S3 method for class 'data.frame'
roc_curve_survival(data, truth, ..., na_rm = TRUE, case_weights = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="roc_curve_survival_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by <code>truth</code> and
<code>...</code>.</p>
</td></tr>
<tr><td><code id="roc_curve_survival_+3A_...">...</code></td>
<td>
<p>The column identifier for the survival probabilities this
should be a list column of data.frames corresponding to the output given when
predicting with <a href="https://censored.tidymodels.org/">censored</a> model. This
should be an unquoted column name although this argument is passed by
expression and supports <a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can
unquote column names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, the dots are not used.</p>
</td></tr>
<tr><td><code id="roc_curve_survival_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true survival result (that
is created using <code><a href="survival.html#topic+Surv">survival::Surv()</a></code>.). This should be an unquoted column name
although this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column names). For
<code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, an <code><a href="survival.html#topic+Surv">survival::Surv()</a></code> object.</p>
</td></tr>
<tr><td><code id="roc_curve_survival_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="roc_curve_survival_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights.
This should be an unquoted column name that evaluates to a numeric column
in <code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This formulation takes survival probability predictions at one or more
specific <em>evaluation times</em> and, for each time, computes the ROC curve. To
account for censoring, inverse probability of censoring weights (IPCW) are
used in the calculations. See equation 7 of section 4.3 in Blanche <em>at al</em>
(2013) for the details.
</p>
<p>The column passed to <code>...</code> should be a list column with one element per
independent experiential unit (e.g. patient). The list column should contain
data frames with several columns:
</p>

<ul>
<li> <p><code>.eval_time</code>: The time that the prediction is made.
</p>
</li>
<li> <p><code>.pred_survival</code>: The predicted probability of survival up to <code>.eval_time</code>
</p>
</li>
<li> <p><code>.weight_censored</code>: The case weight for the inverse probability of censoring.
</p>
</li></ul>

<p>The last column can be produced using <code><a href="parsnip.html#topic+censoring_weights">parsnip::.censoring_weights_graf()</a></code>.
This corresponds to the weighting scheme of  Graf <em>et al</em> (1999). The
internal data set <code>lung_surv</code> shows an example of the format.
</p>
<p>This method automatically groups by the <code>.eval_time</code> argument.
</p>


<h3>Value</h3>

<p>A tibble with class <code>roc_survival_df</code>, <code>grouped_roc_survival_df</code> having
columns <code>.threshold</code>, <code>sensitivity</code>, <code>specificity</code>, and <code>.eval_time</code>.
</p>


<h3>Author(s)</h3>

<p>Emil Hvitfeldt
</p>


<h3>References</h3>

<p>Blanche, P., Dartigues, J.-F. and Jacqmin-Gadda, H. (2013), Review and
comparison of ROC curve estimators for a time-dependent outcome with
marker-dependent censoring. <em>Biom. J.</em>, 55: 687-704.
</p>
<p>Graf, E., Schmoor, C., Sauerbrei, W. and Schumacher, M. (1999), Assessment
and comparison of prognostic classification schemes for survival data.
<em>Statist. Med.</em>, 18: 2529-2545.
</p>


<h3>See Also</h3>

<p>Compute the area under the ROC survival curve with <code><a href="#topic+roc_auc_survival">roc_auc_survival()</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>result &lt;- roc_curve_survival(
  lung_surv,
  truth = surv_obj,
  .pred
)
result

# ---------------------------------------------------------------------------
# `autoplot()`

# Visualize the curve using ggplot2 manually
library(ggplot2)
library(dplyr)
result %&gt;%
  mutate(.eval_time = format(.eval_time)) %&gt;%
  ggplot(aes(
    x = 1 - specificity, y = sensitivity,
    group = .eval_time, col = .eval_time
  )) +
  geom_step(direction = "hv") +
  geom_abline(lty = 3) +
  coord_equal() +
  theme_bw()

# Or use autoplot
autoplot(result)
</code></pre>

<hr>
<h2 id='rpd'>Ratio of performance to deviation</h2><span id='topic+rpd'></span><span id='topic+rpd.data.frame'></span><span id='topic+rpd_vec'></span>

<h3>Description</h3>

<p>These functions are appropriate for cases where the model outcome is a
numeric. The ratio of performance to deviation
(<code><a href="#topic+rpd">rpd()</a></code>) and the ratio of performance to inter-quartile (<code><a href="#topic+rpiq">rpiq()</a></code>)
are both measures of consistency/correlation between observed
and predicted values (and not of accuracy).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rpd(data, ...)

## S3 method for class 'data.frame'
rpd(data, truth, estimate, na_rm = TRUE, case_weights = NULL, ...)

rpd_vec(truth, estimate, na_rm = TRUE, case_weights = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rpd_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by the <code>truth</code>
and <code>estimate</code> arguments.</p>
</td></tr>
<tr><td><code id="rpd_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="rpd_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true results
(that is <code>numeric</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>numeric</code> vector.</p>
</td></tr>
<tr><td><code id="rpd_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predicted
results (that is also <code>numeric</code>). As with <code>truth</code> this can be
specified different ways but the primary method is to use an
unquoted variable name. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>numeric</code> vector.</p>
</td></tr>
<tr><td><code id="rpd_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="rpd_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights. This
should be an unquoted column name that evaluates to a numeric column in
<code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the field of spectroscopy in particular, the ratio
of performance to deviation (RPD) has been used as the standard
way to report the quality of a model. It is the ratio between
the standard deviation of a variable and the standard error of
prediction of that variable by a given model. However, its
systematic use has been criticized by several authors, since
using the standard deviation to represent the spread of a
variable can be misleading on skewed dataset. The ratio of
performance to inter-quartile has been introduced by
Bellon-Maurel et al. (2010) to address some of these issues, and
generalise the RPD to non-normally distributed variables.
</p>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>rpd_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Author(s)</h3>

<p>Pierre Roudier
</p>


<h3>References</h3>

<p>Williams, P.C. (1987) Variables affecting near-infrared
reflectance spectroscopic analysis. In: Near Infrared Technology
in the Agriculture and Food Industries. 1st Ed. P.Williams and
K.Norris, Eds. Am. Cereal Assoc. Cereal Chem., St. Paul, MN.
</p>
<p>Bellon-Maurel, V., Fernandez-Ahumada, E., Palagos, B., Roger,
J.M. and McBratney, A., (2010). Critical review of chemometric
indicators commonly used for assessing the quality of the
prediction of soil attributes by NIR spectroscopy. TrAC Trends
in Analytical Chemistry, 29(9), pp.1073-1081.
</p>


<h3>See Also</h3>

<p>The closely related inter-quartile metric: <code><a href="#topic+rpiq">rpiq()</a></code>
</p>
<p>Other numeric metrics: 
<code><a href="#topic+ccc">ccc</a>()</code>,
<code><a href="#topic+huber_loss_pseudo">huber_loss_pseudo</a>()</code>,
<code><a href="#topic+huber_loss">huber_loss</a>()</code>,
<code><a href="#topic+iic">iic</a>()</code>,
<code><a href="#topic+mae">mae</a>()</code>,
<code><a href="#topic+mape">mape</a>()</code>,
<code><a href="#topic+mase">mase</a>()</code>,
<code><a href="#topic+mpe">mpe</a>()</code>,
<code><a href="#topic+msd">msd</a>()</code>,
<code><a href="#topic+poisson_log_loss">poisson_log_loss</a>()</code>,
<code><a href="#topic+rmse">rmse</a>()</code>,
<code><a href="#topic+rpiq">rpiq</a>()</code>,
<code><a href="#topic+rsq_trad">rsq_trad</a>()</code>,
<code><a href="#topic+rsq">rsq</a>()</code>,
<code><a href="#topic+smape">smape</a>()</code>
</p>
<p>Other consistency metrics: 
<code><a href="#topic+ccc">ccc</a>()</code>,
<code><a href="#topic+rpiq">rpiq</a>()</code>,
<code><a href="#topic+rsq_trad">rsq_trad</a>()</code>,
<code><a href="#topic+rsq">rsq</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Supply truth and predictions as bare column names
rpd(solubility_test, solubility, prediction)

library(dplyr)

set.seed(1234)
size &lt;- 100
times &lt;- 10

# create 10 resamples
solubility_resampled &lt;- bind_rows(
  replicate(
    n = times,
    expr = sample_n(solubility_test, size, replace = TRUE),
    simplify = FALSE
  ),
  .id = "resample"
)

# Compute the metric by group
metric_results &lt;- solubility_resampled %&gt;%
  group_by(resample) %&gt;%
  rpd(solubility, prediction)

metric_results

# Resampled mean estimate
metric_results %&gt;%
  summarise(avg_estimate = mean(.estimate))
</code></pre>

<hr>
<h2 id='rpiq'>Ratio of performance to inter-quartile</h2><span id='topic+rpiq'></span><span id='topic+rpiq.data.frame'></span><span id='topic+rpiq_vec'></span>

<h3>Description</h3>

<p>These functions are appropriate for cases where the model outcome is a
numeric. The ratio of performance to deviation
(<code><a href="#topic+rpd">rpd()</a></code>) and the ratio of performance to inter-quartile (<code><a href="#topic+rpiq">rpiq()</a></code>)
are both measures of consistency/correlation between observed
and predicted values (and not of accuracy).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rpiq(data, ...)

## S3 method for class 'data.frame'
rpiq(data, truth, estimate, na_rm = TRUE, case_weights = NULL, ...)

rpiq_vec(truth, estimate, na_rm = TRUE, case_weights = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rpiq_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by the <code>truth</code>
and <code>estimate</code> arguments.</p>
</td></tr>
<tr><td><code id="rpiq_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="rpiq_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true results
(that is <code>numeric</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>numeric</code> vector.</p>
</td></tr>
<tr><td><code id="rpiq_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predicted
results (that is also <code>numeric</code>). As with <code>truth</code> this can be
specified different ways but the primary method is to use an
unquoted variable name. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>numeric</code> vector.</p>
</td></tr>
<tr><td><code id="rpiq_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="rpiq_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights. This
should be an unquoted column name that evaluates to a numeric column in
<code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the field of spectroscopy in particular, the ratio
of performance to deviation (RPD) has been used as the standard
way to report the quality of a model. It is the ratio between
the standard deviation of a variable and the standard error of
prediction of that variable by a given model. However, its
systematic use has been criticized by several authors, since
using the standard deviation to represent the spread of a
variable can be misleading on skewed dataset. The ratio of
performance to inter-quartile has been introduced by
Bellon-Maurel et al. (2010) to address some of these issues, and
generalise the RPD to non-normally distributed variables.
</p>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>rpd_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Author(s)</h3>

<p>Pierre Roudier
</p>


<h3>References</h3>

<p>Williams, P.C. (1987) Variables affecting near-infrared
reflectance spectroscopic analysis. In: Near Infrared Technology
in the Agriculture and Food Industries. 1st Ed. P.Williams and
K.Norris, Eds. Am. Cereal Assoc. Cereal Chem., St. Paul, MN.
</p>
<p>Bellon-Maurel, V., Fernandez-Ahumada, E., Palagos, B., Roger,
J.M. and McBratney, A., (2010). Critical review of chemometric
indicators commonly used for assessing the quality of the
prediction of soil attributes by NIR spectroscopy. TrAC Trends
in Analytical Chemistry, 29(9), pp.1073-1081.
</p>


<h3>See Also</h3>

<p>The closely related deviation metric: <code><a href="#topic+rpd">rpd()</a></code>
</p>
<p>Other numeric metrics: 
<code><a href="#topic+ccc">ccc</a>()</code>,
<code><a href="#topic+huber_loss_pseudo">huber_loss_pseudo</a>()</code>,
<code><a href="#topic+huber_loss">huber_loss</a>()</code>,
<code><a href="#topic+iic">iic</a>()</code>,
<code><a href="#topic+mae">mae</a>()</code>,
<code><a href="#topic+mape">mape</a>()</code>,
<code><a href="#topic+mase">mase</a>()</code>,
<code><a href="#topic+mpe">mpe</a>()</code>,
<code><a href="#topic+msd">msd</a>()</code>,
<code><a href="#topic+poisson_log_loss">poisson_log_loss</a>()</code>,
<code><a href="#topic+rmse">rmse</a>()</code>,
<code><a href="#topic+rpd">rpd</a>()</code>,
<code><a href="#topic+rsq_trad">rsq_trad</a>()</code>,
<code><a href="#topic+rsq">rsq</a>()</code>,
<code><a href="#topic+smape">smape</a>()</code>
</p>
<p>Other consistency metrics: 
<code><a href="#topic+ccc">ccc</a>()</code>,
<code><a href="#topic+rpd">rpd</a>()</code>,
<code><a href="#topic+rsq_trad">rsq_trad</a>()</code>,
<code><a href="#topic+rsq">rsq</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Supply truth and predictions as bare column names
rpd(solubility_test, solubility, prediction)

library(dplyr)

set.seed(1234)
size &lt;- 100
times &lt;- 10

# create 10 resamples
solubility_resampled &lt;- bind_rows(
  replicate(
    n = times,
    expr = sample_n(solubility_test, size, replace = TRUE),
    simplify = FALSE
  ),
  .id = "resample"
)

# Compute the metric by group
metric_results &lt;- solubility_resampled %&gt;%
  group_by(resample) %&gt;%
  rpd(solubility, prediction)

metric_results

# Resampled mean estimate
metric_results %&gt;%
  summarise(avg_estimate = mean(.estimate))
</code></pre>

<hr>
<h2 id='rsq'>R squared</h2><span id='topic+rsq'></span><span id='topic+rsq.data.frame'></span><span id='topic+rsq_vec'></span>

<h3>Description</h3>

<p>Calculate the coefficient of determination using correlation. For the
traditional measure of R squared, see <code><a href="#topic+rsq_trad">rsq_trad()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rsq(data, ...)

## S3 method for class 'data.frame'
rsq(data, truth, estimate, na_rm = TRUE, case_weights = NULL, ...)

rsq_vec(truth, estimate, na_rm = TRUE, case_weights = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rsq_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by the <code>truth</code>
and <code>estimate</code> arguments.</p>
</td></tr>
<tr><td><code id="rsq_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="rsq_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true results
(that is <code>numeric</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>numeric</code> vector.</p>
</td></tr>
<tr><td><code id="rsq_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predicted
results (that is also <code>numeric</code>). As with <code>truth</code> this can be
specified different ways but the primary method is to use an
unquoted variable name. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>numeric</code> vector.</p>
</td></tr>
<tr><td><code id="rsq_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="rsq_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights. This
should be an unquoted column name that evaluates to a numeric column in
<code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The two estimates for the
coefficient of determination, <code><a href="#topic+rsq">rsq()</a></code> and <code><a href="#topic+rsq_trad">rsq_trad()</a></code>, differ by
their formula. The former guarantees a value on (0, 1) while the
latter can generate inaccurate values when the model is
non-informative (see the examples). Both are measures of
consistency/correlation and not of accuracy.
</p>
<p><code>rsq()</code> is simply the squared correlation between <code>truth</code> and <code>estimate</code>.
</p>
<p>Because <code>rsq()</code> internally computes a correlation, if either <code>truth</code> or
<code>estimate</code> are constant it can result in a divide by zero error. In these
cases, a warning is thrown and <code>NA</code> is returned. This can occur when a model
predicts a single value for all samples. For example, a regularized model
that eliminates all predictors except for the intercept would do this.
Another example would be a CART model that contains no splits.
</p>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>rsq_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>References</h3>

<p>Kvalseth. Cautionary note about <code class="reqn">R^2</code>.
American Statistician (1985) vol. 39 (4) pp. 279-285.
</p>


<h3>See Also</h3>

<p>Other numeric metrics: 
<code><a href="#topic+ccc">ccc</a>()</code>,
<code><a href="#topic+huber_loss_pseudo">huber_loss_pseudo</a>()</code>,
<code><a href="#topic+huber_loss">huber_loss</a>()</code>,
<code><a href="#topic+iic">iic</a>()</code>,
<code><a href="#topic+mae">mae</a>()</code>,
<code><a href="#topic+mape">mape</a>()</code>,
<code><a href="#topic+mase">mase</a>()</code>,
<code><a href="#topic+mpe">mpe</a>()</code>,
<code><a href="#topic+msd">msd</a>()</code>,
<code><a href="#topic+poisson_log_loss">poisson_log_loss</a>()</code>,
<code><a href="#topic+rmse">rmse</a>()</code>,
<code><a href="#topic+rpd">rpd</a>()</code>,
<code><a href="#topic+rpiq">rpiq</a>()</code>,
<code><a href="#topic+rsq_trad">rsq_trad</a>()</code>,
<code><a href="#topic+smape">smape</a>()</code>
</p>
<p>Other consistency metrics: 
<code><a href="#topic+ccc">ccc</a>()</code>,
<code><a href="#topic+rpd">rpd</a>()</code>,
<code><a href="#topic+rpiq">rpiq</a>()</code>,
<code><a href="#topic+rsq_trad">rsq_trad</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Supply truth and predictions as bare column names
rsq(solubility_test, solubility, prediction)

library(dplyr)

set.seed(1234)
size &lt;- 100
times &lt;- 10

# create 10 resamples
solubility_resampled &lt;- bind_rows(
  replicate(
    n = times,
    expr = sample_n(solubility_test, size, replace = TRUE),
    simplify = FALSE
  ),
  .id = "resample"
)

# Compute the metric by group
metric_results &lt;- solubility_resampled %&gt;%
  group_by(resample) %&gt;%
  rsq(solubility, prediction)

metric_results

# Resampled mean estimate
metric_results %&gt;%
  summarise(avg_estimate = mean(.estimate))
# With uninformitive data, the traditional version of R^2 can return
# negative values.
set.seed(2291)
solubility_test$randomized &lt;- sample(solubility_test$prediction)
rsq(solubility_test, solubility, randomized)
rsq_trad(solubility_test, solubility, randomized)

# A constant `truth` or `estimate` vector results in a warning from
# a divide by zero error in the correlation calculation.
# `NA` will be returned in these cases.
truth &lt;- c(1, 2)
estimate &lt;- c(1, 1)
rsq_vec(truth, estimate)
</code></pre>

<hr>
<h2 id='rsq_trad'>R squared - traditional</h2><span id='topic+rsq_trad'></span><span id='topic+rsq_trad.data.frame'></span><span id='topic+rsq_trad_vec'></span>

<h3>Description</h3>

<p>Calculate the coefficient of determination using the traditional definition
of R squared using sum of squares. For a measure of R squared that is
strictly between (0, 1), see <code><a href="#topic+rsq">rsq()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rsq_trad(data, ...)

## S3 method for class 'data.frame'
rsq_trad(data, truth, estimate, na_rm = TRUE, case_weights = NULL, ...)

rsq_trad_vec(truth, estimate, na_rm = TRUE, case_weights = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rsq_trad_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by the <code>truth</code>
and <code>estimate</code> arguments.</p>
</td></tr>
<tr><td><code id="rsq_trad_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="rsq_trad_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true results
(that is <code>numeric</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>numeric</code> vector.</p>
</td></tr>
<tr><td><code id="rsq_trad_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predicted
results (that is also <code>numeric</code>). As with <code>truth</code> this can be
specified different ways but the primary method is to use an
unquoted variable name. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>numeric</code> vector.</p>
</td></tr>
<tr><td><code id="rsq_trad_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="rsq_trad_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights. This
should be an unquoted column name that evaluates to a numeric column in
<code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The two estimates for the
coefficient of determination, <code><a href="#topic+rsq">rsq()</a></code> and <code><a href="#topic+rsq_trad">rsq_trad()</a></code>, differ by
their formula. The former guarantees a value on (0, 1) while the
latter can generate inaccurate values when the model is
non-informative (see the examples). Both are measures of
consistency/correlation and not of accuracy.
</p>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>rsq_trad_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>References</h3>

<p>Kvalseth. Cautionary note about <code class="reqn">R^2</code>.
American Statistician (1985) vol. 39 (4) pp. 279-285.
</p>


<h3>See Also</h3>

<p>Other numeric metrics: 
<code><a href="#topic+ccc">ccc</a>()</code>,
<code><a href="#topic+huber_loss_pseudo">huber_loss_pseudo</a>()</code>,
<code><a href="#topic+huber_loss">huber_loss</a>()</code>,
<code><a href="#topic+iic">iic</a>()</code>,
<code><a href="#topic+mae">mae</a>()</code>,
<code><a href="#topic+mape">mape</a>()</code>,
<code><a href="#topic+mase">mase</a>()</code>,
<code><a href="#topic+mpe">mpe</a>()</code>,
<code><a href="#topic+msd">msd</a>()</code>,
<code><a href="#topic+poisson_log_loss">poisson_log_loss</a>()</code>,
<code><a href="#topic+rmse">rmse</a>()</code>,
<code><a href="#topic+rpd">rpd</a>()</code>,
<code><a href="#topic+rpiq">rpiq</a>()</code>,
<code><a href="#topic+rsq">rsq</a>()</code>,
<code><a href="#topic+smape">smape</a>()</code>
</p>
<p>Other consistency metrics: 
<code><a href="#topic+ccc">ccc</a>()</code>,
<code><a href="#topic+rpd">rpd</a>()</code>,
<code><a href="#topic+rpiq">rpiq</a>()</code>,
<code><a href="#topic+rsq">rsq</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Supply truth and predictions as bare column names
rsq_trad(solubility_test, solubility, prediction)

library(dplyr)

set.seed(1234)
size &lt;- 100
times &lt;- 10

# create 10 resamples
solubility_resampled &lt;- bind_rows(
  replicate(
    n = times,
    expr = sample_n(solubility_test, size, replace = TRUE),
    simplify = FALSE
  ),
  .id = "resample"
)

# Compute the metric by group
metric_results &lt;- solubility_resampled %&gt;%
  group_by(resample) %&gt;%
  rsq_trad(solubility, prediction)

metric_results

# Resampled mean estimate
metric_results %&gt;%
  summarise(avg_estimate = mean(.estimate))
# With uninformitive data, the traditional version of R^2 can return
# negative values.
set.seed(2291)
solubility_test$randomized &lt;- sample(solubility_test$prediction)
rsq(solubility_test, solubility, randomized)
rsq_trad(solubility_test, solubility, randomized)

</code></pre>

<hr>
<h2 id='sens'>Sensitivity</h2><span id='topic+sens'></span><span id='topic+sens.data.frame'></span><span id='topic+sens_vec'></span><span id='topic+sensitivity'></span><span id='topic+sensitivity.data.frame'></span><span id='topic+sensitivity_vec'></span>

<h3>Description</h3>

<p>These functions calculate the <code><a href="#topic+sens">sens()</a></code> (sensitivity) of a measurement system
compared to a reference result (the &quot;truth&quot; or gold standard).
Highly related functions are <code><a href="#topic+spec">spec()</a></code>, <code><a href="#topic+ppv">ppv()</a></code>, and <code><a href="#topic+npv">npv()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sens(data, ...)

## S3 method for class 'data.frame'
sens(
  data,
  truth,
  estimate,
  estimator = NULL,
  na_rm = TRUE,
  case_weights = NULL,
  event_level = yardstick_event_level(),
  ...
)

sens_vec(
  truth,
  estimate,
  estimator = NULL,
  na_rm = TRUE,
  case_weights = NULL,
  event_level = yardstick_event_level(),
  ...
)

sensitivity(data, ...)

## S3 method for class 'data.frame'
sensitivity(
  data,
  truth,
  estimate,
  estimator = NULL,
  na_rm = TRUE,
  case_weights = NULL,
  event_level = yardstick_event_level(),
  ...
)

sensitivity_vec(
  truth,
  estimate,
  estimator = NULL,
  na_rm = TRUE,
  case_weights = NULL,
  event_level = yardstick_event_level(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sens_+3A_data">data</code></td>
<td>
<p>Either a <code>data.frame</code> containing the columns specified by the
<code>truth</code> and <code>estimate</code> arguments, or a <code>table</code>/<code>matrix</code> where the true
class results should be in the columns of the table.</p>
</td></tr>
<tr><td><code id="sens_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="sens_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a <code>factor</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="sens_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predicted class
results (that is also <code>factor</code>). As with <code>truth</code> this can be
specified different ways but the primary method is to use an
unquoted variable name. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="sens_+3A_estimator">estimator</code></td>
<td>
<p>One of: <code>"binary"</code>, <code>"macro"</code>, <code>"macro_weighted"</code>,
or <code>"micro"</code> to specify the type of averaging to be done. <code>"binary"</code> is
only relevant for the two class case. The other three are general methods
for calculating multiclass metrics. The default will automatically choose
<code>"binary"</code> or <code>"macro"</code> based on <code>estimate</code>.</p>
</td></tr>
<tr><td><code id="sens_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="sens_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights.
This should be an unquoted column name that evaluates to a numeric column
in <code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
<tr><td><code id="sens_+3A_event_level">event_level</code></td>
<td>
<p>A single string. Either <code>"first"</code> or <code>"second"</code> to specify
which level of <code>truth</code> to consider as the &quot;event&quot;. This argument is only
applicable when <code>estimator = "binary"</code>. The default uses an internal helper
that defaults to <code>"first"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The sensitivity (<code>sens()</code>) is defined as the proportion of positive
results out of the number of samples which were actually
positive.
</p>
<p>When the denominator of the calculation is <code>0</code>, sensitivity is undefined.
This happens when both <code style="white-space: pre;">&#8288;# true_positive = 0&#8288;</code> and <code style="white-space: pre;">&#8288;# false_negative = 0&#8288;</code>
are true, which mean that there were no true events. When computing binary
sensitivity, a <code>NA</code> value will be returned with a warning. When computing
multiclass sensitivity, the individual <code>NA</code> values will be removed, and the
computation will procede, with a warning.
</p>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>sens_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Relevant Level</h3>

<p>There is no common convention on which factor level should
automatically be considered the &quot;event&quot; or &quot;positive&quot; result
when computing binary classification metrics. In <code>yardstick</code>, the default
is to use the <em>first</em> level. To alter this, change the argument
<code>event_level</code> to <code>"second"</code> to consider the <em>last</em> level of the factor the
level of interest. For multiclass extensions involving one-vs-all
comparisons (such as macro averaging), this option is ignored and
the &quot;one&quot; level is always the relevant result.
</p>


<h3>Multiclass</h3>

<p>Macro, micro, and macro-weighted averaging is available for this metric.
The default is to select macro averaging if a <code>truth</code> factor with more
than 2 levels is provided. Otherwise, a standard binary calculation is done.
See <code>vignette("multiclass", "yardstick")</code> for more information.
</p>


<h3>Implementation</h3>

<p>Suppose a 2x2 table with notation:
</p>

<table>
<tr>
 <td style="text-align: right;"> </td><td style="text-align: center;"> Reference </td><td style="text-align: center;"> </td>
</tr>
<tr>
 <td style="text-align: right;"> Predicted </td><td style="text-align: center;"> Positive </td><td style="text-align: center;"> Negative
</td>
</tr>
<tr>
 <td style="text-align: right;"> Positive </td><td style="text-align: center;"> A </td><td style="text-align: center;"> B </td>
</tr>
<tr>
 <td style="text-align: right;"> Negative </td><td style="text-align: center;"> C </td><td style="text-align: center;"> D </td>
</tr>
<tr>
 <td style="text-align: right;"> </td>
</tr>

</table>

<p>The formulas used here are:
</p>
<p style="text-align: center;"><code class="reqn">Sensitivity = A/(A+C)</code>
</p>

<p style="text-align: center;"><code class="reqn">Specificity = D/(B+D)</code>
</p>

<p style="text-align: center;"><code class="reqn">Prevalence = (A+C)/(A+B+C+D)</code>
</p>

<p style="text-align: center;"><code class="reqn">PPV = (Sensitivity * Prevalence) / ((Sensitivity * Prevalence) + ((1-Specificity) * (1-Prevalence)))</code>
</p>

<p style="text-align: center;"><code class="reqn">NPV = (Specificity * (1-Prevalence)) / (((1-Sensitivity) * Prevalence) + ((Specificity) * (1-Prevalence)))</code>
</p>

<p>See the references for discussions of the statistics.
</p>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>References</h3>

<p>Altman, D.G., Bland, J.M. (1994) &ldquo;Diagnostic tests 1:
sensitivity and specificity,&rdquo; <em>British Medical Journal</em>,
vol 308, 1552.
</p>


<h3>See Also</h3>

<p>Other class metrics: 
<code><a href="#topic+accuracy">accuracy</a>()</code>,
<code><a href="#topic+bal_accuracy">bal_accuracy</a>()</code>,
<code><a href="#topic+detection_prevalence">detection_prevalence</a>()</code>,
<code><a href="#topic+f_meas">f_meas</a>()</code>,
<code><a href="#topic+j_index">j_index</a>()</code>,
<code><a href="#topic+kap">kap</a>()</code>,
<code><a href="#topic+mcc">mcc</a>()</code>,
<code><a href="#topic+npv">npv</a>()</code>,
<code><a href="#topic+ppv">ppv</a>()</code>,
<code><a href="#topic+precision">precision</a>()</code>,
<code><a href="#topic+recall">recall</a>()</code>,
<code><a href="#topic+spec">spec</a>()</code>
</p>
<p>Other sensitivity metrics: 
<code><a href="#topic+npv">npv</a>()</code>,
<code><a href="#topic+ppv">ppv</a>()</code>,
<code><a href="#topic+spec">spec</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Two class
data("two_class_example")
sens(two_class_example, truth, predicted)

# Multiclass
library(dplyr)
data(hpc_cv)

hpc_cv %&gt;%
  filter(Resample == "Fold01") %&gt;%
  sens(obs, pred)

# Groups are respected
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  sens(obs, pred)

# Weighted macro averaging
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  sens(obs, pred, estimator = "macro_weighted")

# Vector version
sens_vec(
  two_class_example$truth,
  two_class_example$predicted
)

# Making Class2 the "relevant" level
sens_vec(
  two_class_example$truth,
  two_class_example$predicted,
  event_level = "second"
)
</code></pre>

<hr>
<h2 id='smape'>Symmetric mean absolute percentage error</h2><span id='topic+smape'></span><span id='topic+smape.data.frame'></span><span id='topic+smape_vec'></span>

<h3>Description</h3>

<p>Calculate the symmetric mean absolute percentage error. This metric is in
<em>relative units</em>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smape(data, ...)

## S3 method for class 'data.frame'
smape(data, truth, estimate, na_rm = TRUE, case_weights = NULL, ...)

smape_vec(truth, estimate, na_rm = TRUE, case_weights = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="smape_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> containing the columns specified by the <code>truth</code>
and <code>estimate</code> arguments.</p>
</td></tr>
<tr><td><code id="smape_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="smape_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true results
(that is <code>numeric</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>numeric</code> vector.</p>
</td></tr>
<tr><td><code id="smape_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predicted
results (that is also <code>numeric</code>). As with <code>truth</code> this can be
specified different ways but the primary method is to use an
unquoted variable name. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>numeric</code> vector.</p>
</td></tr>
<tr><td><code id="smape_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="smape_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights. This
should be an unquoted column name that evaluates to a numeric column in
<code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This implementation of <code>smape()</code> is the &quot;usual definition&quot; where the
denominator is divided by two.
</p>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>smape_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Author(s)</h3>

<p>Max Kuhn, Riaz Hedayati
</p>


<h3>See Also</h3>

<p>Other numeric metrics: 
<code><a href="#topic+ccc">ccc</a>()</code>,
<code><a href="#topic+huber_loss_pseudo">huber_loss_pseudo</a>()</code>,
<code><a href="#topic+huber_loss">huber_loss</a>()</code>,
<code><a href="#topic+iic">iic</a>()</code>,
<code><a href="#topic+mae">mae</a>()</code>,
<code><a href="#topic+mape">mape</a>()</code>,
<code><a href="#topic+mase">mase</a>()</code>,
<code><a href="#topic+mpe">mpe</a>()</code>,
<code><a href="#topic+msd">msd</a>()</code>,
<code><a href="#topic+poisson_log_loss">poisson_log_loss</a>()</code>,
<code><a href="#topic+rmse">rmse</a>()</code>,
<code><a href="#topic+rpd">rpd</a>()</code>,
<code><a href="#topic+rpiq">rpiq</a>()</code>,
<code><a href="#topic+rsq_trad">rsq_trad</a>()</code>,
<code><a href="#topic+rsq">rsq</a>()</code>
</p>
<p>Other accuracy metrics: 
<code><a href="#topic+ccc">ccc</a>()</code>,
<code><a href="#topic+huber_loss_pseudo">huber_loss_pseudo</a>()</code>,
<code><a href="#topic+huber_loss">huber_loss</a>()</code>,
<code><a href="#topic+iic">iic</a>()</code>,
<code><a href="#topic+mae">mae</a>()</code>,
<code><a href="#topic+mape">mape</a>()</code>,
<code><a href="#topic+mase">mase</a>()</code>,
<code><a href="#topic+mpe">mpe</a>()</code>,
<code><a href="#topic+msd">msd</a>()</code>,
<code><a href="#topic+poisson_log_loss">poisson_log_loss</a>()</code>,
<code><a href="#topic+rmse">rmse</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Supply truth and predictions as bare column names
smape(solubility_test, solubility, prediction)

library(dplyr)

set.seed(1234)
size &lt;- 100
times &lt;- 10

# create 10 resamples
solubility_resampled &lt;- bind_rows(
  replicate(
    n = times,
    expr = sample_n(solubility_test, size, replace = TRUE),
    simplify = FALSE
  ),
  .id = "resample"
)

# Compute the metric by group
metric_results &lt;- solubility_resampled %&gt;%
  group_by(resample) %&gt;%
  smape(solubility, prediction)

metric_results

# Resampled mean estimate
metric_results %&gt;%
  summarise(avg_estimate = mean(.estimate))
</code></pre>

<hr>
<h2 id='solubility_test'>Solubility Predictions from MARS Model</h2><span id='topic+solubility_test'></span>

<h3>Description</h3>

<p>Solubility Predictions from MARS Model
</p>


<h3>Details</h3>

<p>For the solubility data in Kuhn and Johnson (2013),
these data are the test set results for the MARS model. The
observed solubility (in column <code>solubility</code>) and the model
results (<code>prediction</code>) are contained in the data.
</p>


<h3>Value</h3>

<table>
<tr><td><code>solubility_test</code></td>
<td>
<p>a data frame</p>
</td></tr>
</table>


<h3>Source</h3>

<p>Kuhn, M., Johnson, K. (2013) <em>Applied Predictive
Modeling</em>, Springer
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(solubility_test)
str(solubility_test)
</code></pre>

<hr>
<h2 id='spec'>Specificity</h2><span id='topic+spec'></span><span id='topic+spec.data.frame'></span><span id='topic+spec_vec'></span><span id='topic+specificity'></span><span id='topic+specificity.data.frame'></span><span id='topic+specificity_vec'></span>

<h3>Description</h3>

<p>These functions calculate the <code><a href="#topic+spec">spec()</a></code> (specificity) of a measurement system
compared to a reference result (the &quot;truth&quot; or gold standard).
Highly related functions are <code><a href="#topic+sens">sens()</a></code>, <code><a href="#topic+ppv">ppv()</a></code>, and <code><a href="#topic+npv">npv()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spec(data, ...)

## S3 method for class 'data.frame'
spec(
  data,
  truth,
  estimate,
  estimator = NULL,
  na_rm = TRUE,
  case_weights = NULL,
  event_level = yardstick_event_level(),
  ...
)

spec_vec(
  truth,
  estimate,
  estimator = NULL,
  na_rm = TRUE,
  case_weights = NULL,
  event_level = yardstick_event_level(),
  ...
)

specificity(data, ...)

## S3 method for class 'data.frame'
specificity(
  data,
  truth,
  estimate,
  estimator = NULL,
  na_rm = TRUE,
  case_weights = NULL,
  event_level = yardstick_event_level(),
  ...
)

specificity_vec(
  truth,
  estimate,
  estimator = NULL,
  na_rm = TRUE,
  case_weights = NULL,
  event_level = yardstick_event_level(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spec_+3A_data">data</code></td>
<td>
<p>Either a <code>data.frame</code> containing the columns specified by the
<code>truth</code> and <code>estimate</code> arguments, or a <code>table</code>/<code>matrix</code> where the true
class results should be in the columns of the table.</p>
</td></tr>
<tr><td><code id="spec_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
<tr><td><code id="spec_+3A_truth">truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a <code>factor</code>). This should be an unquoted column name although
this argument is passed by expression and supports
<a href="rlang.html#topic+topic-inject">quasiquotation</a> (you can unquote column
names). For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="spec_+3A_estimate">estimate</code></td>
<td>
<p>The column identifier for the predicted class
results (that is also <code>factor</code>). As with <code>truth</code> this can be
specified different ways but the primary method is to use an
unquoted variable name. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a <code>factor</code> vector.</p>
</td></tr>
<tr><td><code id="spec_+3A_estimator">estimator</code></td>
<td>
<p>One of: <code>"binary"</code>, <code>"macro"</code>, <code>"macro_weighted"</code>,
or <code>"micro"</code> to specify the type of averaging to be done. <code>"binary"</code> is
only relevant for the two class case. The other three are general methods
for calculating multiclass metrics. The default will automatically choose
<code>"binary"</code> or <code>"macro"</code> based on <code>estimate</code>.</p>
</td></tr>
<tr><td><code id="spec_+3A_na_rm">na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td></tr>
<tr><td><code id="spec_+3A_case_weights">case_weights</code></td>
<td>
<p>The optional column identifier for case weights.
This should be an unquoted column name that evaluates to a numeric column
in <code>data</code>. For <code style="white-space: pre;">&#8288;_vec()&#8288;</code> functions, a numeric vector,
<code><a href="hardhat.html#topic+importance_weights">hardhat::importance_weights()</a></code>, or <code><a href="hardhat.html#topic+frequency_weights">hardhat::frequency_weights()</a></code>.</p>
</td></tr>
<tr><td><code id="spec_+3A_event_level">event_level</code></td>
<td>
<p>A single string. Either <code>"first"</code> or <code>"second"</code> to specify
which level of <code>truth</code> to consider as the &quot;event&quot;. This argument is only
applicable when <code>estimator = "binary"</code>. The default uses an internal helper
that defaults to <code>"first"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The specificity measures the proportion of negatives that are correctly
identified as negatives.
</p>
<p>When the denominator of the calculation is <code>0</code>, specificity is undefined.
This happens when both <code style="white-space: pre;">&#8288;# true_negative = 0&#8288;</code> and <code style="white-space: pre;">&#8288;# false_positive = 0&#8288;</code>
are true, which mean that there were no true negatives. When computing binary
specificity, a <code>NA</code> value will be returned with a warning. When computing
multiclass specificity, the individual <code>NA</code> values will be removed, and the
computation will procede, with a warning.
</p>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>spec_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Relevant Level</h3>

<p>There is no common convention on which factor level should
automatically be considered the &quot;event&quot; or &quot;positive&quot; result
when computing binary classification metrics. In <code>yardstick</code>, the default
is to use the <em>first</em> level. To alter this, change the argument
<code>event_level</code> to <code>"second"</code> to consider the <em>last</em> level of the factor the
level of interest. For multiclass extensions involving one-vs-all
comparisons (such as macro averaging), this option is ignored and
the &quot;one&quot; level is always the relevant result.
</p>


<h3>Multiclass</h3>

<p>Macro, micro, and macro-weighted averaging is available for this metric.
The default is to select macro averaging if a <code>truth</code> factor with more
than 2 levels is provided. Otherwise, a standard binary calculation is done.
See <code>vignette("multiclass", "yardstick")</code> for more information.
</p>


<h3>Implementation</h3>

<p>Suppose a 2x2 table with notation:
</p>

<table>
<tr>
 <td style="text-align: right;"> </td><td style="text-align: center;"> Reference </td><td style="text-align: center;"> </td>
</tr>
<tr>
 <td style="text-align: right;"> Predicted </td><td style="text-align: center;"> Positive </td><td style="text-align: center;"> Negative
</td>
</tr>
<tr>
 <td style="text-align: right;"> Positive </td><td style="text-align: center;"> A </td><td style="text-align: center;"> B </td>
</tr>
<tr>
 <td style="text-align: right;"> Negative </td><td style="text-align: center;"> C </td><td style="text-align: center;"> D </td>
</tr>
<tr>
 <td style="text-align: right;"> </td>
</tr>

</table>

<p>The formulas used here are:
</p>
<p style="text-align: center;"><code class="reqn">Sensitivity = A/(A+C)</code>
</p>

<p style="text-align: center;"><code class="reqn">Specificity = D/(B+D)</code>
</p>

<p style="text-align: center;"><code class="reqn">Prevalence = (A+C)/(A+B+C+D)</code>
</p>

<p style="text-align: center;"><code class="reqn">PPV = (Sensitivity * Prevalence) / ((Sensitivity * Prevalence) + ((1-Specificity) * (1-Prevalence)))</code>
</p>

<p style="text-align: center;"><code class="reqn">NPV = (Specificity * (1-Prevalence)) / (((1-Sensitivity) * Prevalence) + ((Specificity) * (1-Prevalence)))</code>
</p>

<p>See the references for discussions of the statistics.
</p>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>References</h3>

<p>Altman, D.G., Bland, J.M. (1994) &ldquo;Diagnostic tests 1:
sensitivity and specificity,&rdquo; <em>British Medical Journal</em>,
vol 308, 1552.
</p>


<h3>See Also</h3>

<p>Other class metrics: 
<code><a href="#topic+accuracy">accuracy</a>()</code>,
<code><a href="#topic+bal_accuracy">bal_accuracy</a>()</code>,
<code><a href="#topic+detection_prevalence">detection_prevalence</a>()</code>,
<code><a href="#topic+f_meas">f_meas</a>()</code>,
<code><a href="#topic+j_index">j_index</a>()</code>,
<code><a href="#topic+kap">kap</a>()</code>,
<code><a href="#topic+mcc">mcc</a>()</code>,
<code><a href="#topic+npv">npv</a>()</code>,
<code><a href="#topic+ppv">ppv</a>()</code>,
<code><a href="#topic+precision">precision</a>()</code>,
<code><a href="#topic+recall">recall</a>()</code>,
<code><a href="#topic+sens">sens</a>()</code>
</p>
<p>Other sensitivity metrics: 
<code><a href="#topic+npv">npv</a>()</code>,
<code><a href="#topic+ppv">ppv</a>()</code>,
<code><a href="#topic+sens">sens</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Two class
data("two_class_example")
spec(two_class_example, truth, predicted)

# Multiclass
library(dplyr)
data(hpc_cv)

hpc_cv %&gt;%
  filter(Resample == "Fold01") %&gt;%
  spec(obs, pred)

# Groups are respected
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  spec(obs, pred)

# Weighted macro averaging
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  spec(obs, pred, estimator = "macro_weighted")

# Vector version
spec_vec(
  two_class_example$truth,
  two_class_example$predicted
)

# Making Class2 the "relevant" level
spec_vec(
  two_class_example$truth,
  two_class_example$predicted,
  event_level = "second"
)
</code></pre>

<hr>
<h2 id='summary.conf_mat'>Summary Statistics for Confusion Matrices</h2><span id='topic+summary.conf_mat'></span>

<h3>Description</h3>

<p>Various statistical summaries of confusion matrices are
produced and returned in a tibble. These include those shown in the help
pages for <code><a href="#topic+sens">sens()</a></code>, <code><a href="#topic+recall">recall()</a></code>, and <code><a href="#topic+accuracy">accuracy()</a></code>, among others.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'conf_mat'
summary(
  object,
  prevalence = NULL,
  beta = 1,
  estimator = NULL,
  event_level = yardstick_event_level(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.conf_mat_+3A_object">object</code></td>
<td>
<p>An object of class <code><a href="#topic+conf_mat">conf_mat()</a></code>.</p>
</td></tr>
<tr><td><code id="summary.conf_mat_+3A_prevalence">prevalence</code></td>
<td>
<p>A number in <code style="white-space: pre;">&#8288;(0, 1)&#8288;</code> for the prevalence (i.e.
prior) of the event. If left to the default, the data are used
to derive this value.</p>
</td></tr>
<tr><td><code id="summary.conf_mat_+3A_beta">beta</code></td>
<td>
<p>A numeric value used to weight precision and
recall for <code><a href="#topic+f_meas">f_meas()</a></code>.</p>
</td></tr>
<tr><td><code id="summary.conf_mat_+3A_estimator">estimator</code></td>
<td>
<p>One of: <code>"binary"</code>, <code>"macro"</code>, <code>"macro_weighted"</code>,
or <code>"micro"</code> to specify the type of averaging to be done. <code>"binary"</code> is
only relevant for the two class case. The other three are general methods
for calculating multiclass metrics. The default will automatically choose
<code>"binary"</code> or <code>"macro"</code> based on <code>estimate</code>.</p>
</td></tr>
<tr><td><code id="summary.conf_mat_+3A_event_level">event_level</code></td>
<td>
<p>A single string. Either <code>"first"</code> or <code>"second"</code> to specify
which level of <code>truth</code> to consider as the &quot;event&quot;. This argument is only
applicable when <code>estimator = "binary"</code>. The default uses an internal helper
that defaults to <code>"first"</code>.</p>
</td></tr>
<tr><td><code id="summary.conf_mat_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble containing various classification metrics.
</p>


<h3>Relevant Level</h3>

<p>There is no common convention on which factor level should
automatically be considered the &quot;event&quot; or &quot;positive&quot; result
when computing binary classification metrics. In <code>yardstick</code>, the default
is to use the <em>first</em> level. To alter this, change the argument
<code>event_level</code> to <code>"second"</code> to consider the <em>last</em> level of the factor the
level of interest. For multiclass extensions involving one-vs-all
comparisons (such as macro averaging), this option is ignored and
the &quot;one&quot; level is always the relevant result.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+conf_mat">conf_mat()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("two_class_example")

cmat &lt;- conf_mat(two_class_example, truth = "truth", estimate = "predicted")
summary(cmat)
summary(cmat, prevalence = 0.70)

library(dplyr)
library(tidyr)
data("hpc_cv")

# Compute statistics per resample then summarize
all_metrics &lt;- hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  conf_mat(obs, pred) %&gt;%
  mutate(summary_tbl = lapply(conf_mat, summary)) %&gt;%
  unnest(summary_tbl)

all_metrics %&gt;%
  group_by(.metric) %&gt;%
  summarise(
    mean = mean(.estimate, na.rm = TRUE),
    sd = sd(.estimate, na.rm = TRUE)
  )
</code></pre>

<hr>
<h2 id='two_class_example'>Two Class Predictions</h2><span id='topic+two_class_example'></span>

<h3>Description</h3>

<p>Two Class Predictions
</p>


<h3>Details</h3>

<p>These data are a test set form a model built for two
classes (&quot;Class1&quot; and &quot;Class2&quot;). There are columns for the true
and predicted classes and column for the probabilities for each
class.
</p>


<h3>Value</h3>

<table>
<tr><td><code>two_class_example</code></td>
<td>
<p>a data frame</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(two_class_example)
str(two_class_example)

# `truth` is a 2 level factor. The first level is `"Class1"`, which is the
# "event of interest" by default in yardstick. See the Relevant Level
# section in any classification function (such as `?pr_auc`) to see how
# to change this.
levels(hpc_cv$obs)
</code></pre>

<hr>
<h2 id='yardstick_remove_missing'>Developer function for handling missing values in new metrics</h2><span id='topic+yardstick_remove_missing'></span><span id='topic+yardstick_any_missing'></span>

<h3>Description</h3>

<p><code>yardstick_remove_missing()</code>,  and <code>yardstick_any_missing()</code> are useful
alongside the <a href="#topic+metric-summarizers">metric-summarizers</a> functions for implementing new custom
metrics. <code>yardstick_remove_missing()</code> removes any observations that contains
missing values across, <code>truth</code>, <code>estimate</code> and <code>case_weights</code>.
<code>yardstick_any_missing()</code> returns <code>FALSE</code> if there is any missing values in
the inputs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>yardstick_remove_missing(truth, estimate, case_weights)

yardstick_any_missing(truth, estimate, case_weights)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="yardstick_remove_missing_+3A_truth">truth</code>, <code id="yardstick_remove_missing_+3A_estimate">estimate</code></td>
<td>
<p>Vectors of the same length.</p>
</td></tr>
<tr><td><code id="yardstick_remove_missing_+3A_case_weights">case_weights</code></td>
<td>
<p>A vector of the same length as <code>truth</code> and <code>estimate</code>, or
<code>NULL</code> if case weights are not being used.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><a href="#topic+metric-summarizers">metric-summarizers</a>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
