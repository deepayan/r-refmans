<!DOCTYPE html><html lang="en-US"><head><title>Help for package RLescalation</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {RLescalation}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#clean_python_settings'><p>Clean the Python Virtual Environment</p></a></li>
<li><a href='#compute_rl_scenarios'><p>Compute DLT Probability Scenarios for Reinforcement Learning</p></a></li>
<li><a href='#EscalationRule'><p>EscalationRule Class</p></a></li>
<li><a href='#learn_escalation_rule'><p>Build an Optimal Dose Escalation Rule using Reinforcement Learning</p></a></li>
<li><a href='#rl_config_set'><p>Configuration of Reinforcement Learning</p></a></li>
<li><a href='#rl_dnn_config'><p>DNN Configuration for Reinforcement Learning</p></a></li>
<li><a href='#setup_python'><p>Setting up a Python Virtual Environment</p></a></li>
<li><a href='#simulate_one_trial'><p>Simulate One Trial Using an Obtained Optimal Dose Escalation Rule</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Optimal Dose Escalation Using Deep Reinforcement Learning</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.2</td>
</tr>
<tr>
<td>Description:</td>
<td>An implementation to compute an optimal dose escalation rule
    using deep reinforcement learning in phase I oncology trials
    (Matsuura et al. (2023) &lt;<a href="https://doi.org/10.1080%2F10543406.2023.2170402">doi:10.1080/10543406.2023.2170402</a>&gt;).
    The dose escalation rule can directly optimize the percentages of correct
    selection (PCS) of the maximum tolerated dose (MTD).</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/MatsuuraKentaro/RLescalation">https://github.com/MatsuuraKentaro/RLescalation</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/MatsuuraKentaro/RLescalation/issues">https://github.com/MatsuuraKentaro/RLescalation/issues</a></td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Imports:</td>
<td>glue, R6, nleqslv, reticulate, stats, utils</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>Collate:</td>
<td>'timer.R' 'train_algo.R' 'utils.R' 'escalation_rule.R'
'rl_dnn_config.R' 'rl_config_set.R' 'compute_rl_scenarios.R'
'learn_escalation_rule.R' 'setup_python.R' 'zzz.R'
'simulate_one_trial.R'</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-02-08 01:17:03 UTC; kmatsuu</td>
</tr>
<tr>
<td>Author:</td>
<td>Kentaro Matsuura <a href="https://orcid.org/0000-0001-5262-055X"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre, cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Kentaro Matsuura &lt;matsuurakentaro55@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-02-08 03:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='clean_python_settings'>Clean the Python Virtual Environment</h2><span id='topic+clean_python_settings'></span>

<h3>Description</h3>

<p>Clean the Python Virtual Environment
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clean_python_settings(envname = "r-RLescalation")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="clean_python_settings_+3A_envname">envname</code></td>
<td>
<p>Python virtual environment name.</p>
</td></tr>
</table>

<hr>
<h2 id='compute_rl_scenarios'>Compute DLT Probability Scenarios for Reinforcement Learning</h2><span id='topic+compute_rl_scenarios'></span>

<h3>Description</h3>

<p>Compute the scenarios described in Sect. 2.2 of the original paper.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_rl_scenarios(J, target, epsilon, delta, lower = 0.1, upper = 0.8)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="compute_rl_scenarios_+3A_j">J</code></td>
<td>
<p>A positive integer value. The number of doses.</p>
</td></tr>
<tr><td><code id="compute_rl_scenarios_+3A_target">target</code></td>
<td>
<p>A positive numeric value. The target DLT probability.</p>
</td></tr>
<tr><td><code id="compute_rl_scenarios_+3A_epsilon">epsilon</code></td>
<td>
<p>A positive numeric value. The acceptable range of target DLT
probabilities is defined as [<code>target</code> - <code>epsilon</code>, <code>target</code> + <code>epsilon</code>].</p>
</td></tr>
<tr><td><code id="compute_rl_scenarios_+3A_delta">delta</code></td>
<td>
<p>A positive numeric value. The unacceptable ranges of target DLT
probabilities are defined as [0, <code>target</code> - <code>delta</code>] and
[<code>target</code> + <code>delta</code>, 1].</p>
</td></tr>
<tr><td><code id="compute_rl_scenarios_+3A_lower">lower</code></td>
<td>
<p>A positive numeric value. Values lower than <code>lower</code> are clipped.
Default is 0.1, which is modified from Sect. 2.2 of the original paper.</p>
</td></tr>
<tr><td><code id="compute_rl_scenarios_+3A_upper">upper</code></td>
<td>
<p>A positive numeric value. Values higher than <code>upper</code> are clipped.
Default is 0.8.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list of three elements:
- prob: a list of DLT probability scenarios
- MTD: a list of true MTD indices (Note that <code>-1</code> means &quot;no MTD&quot;)
- weight: a vector of weights for each scenario
</p>


<h3>Examples</h3>

<pre><code class='language-R'>scenarios &lt;- compute_rl_scenarios(J = 6, target = 0.25, epsilon = 0.04, delta = 0.1)
print(scenarios)

</code></pre>

<hr>
<h2 id='EscalationRule'>EscalationRule Class</h2><span id='topic+EscalationRule'></span>

<h3>Description</h3>

<p>This class represents an escalation rule that generates a next escalation.
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>policy</code></dt><dd><p>The RLlib policy that is a Python object.</p>
</dd>
<dt><code>dir</code></dt><dd><p>Directory path of the escalation rule (policy).</p>
</dd>
<dt><code>dirpath</code></dt><dd><p>Full path to the directory of the escalation rule.</p>
</dd>
<dt><code>created_at</code></dt><dd><p>Created time of this object.</p>
</dd>
<dt><code>info</code></dt><dd><p>Information when learning the escalation rule.</p>
</dd>
<dt><code>input</code></dt><dd><p>Inputs for learning the escalation rule.</p>
</dd>
<dt><code>log</code></dt><dd><p>The log of scores during the learning of the escalation rule.</p>
</dd>
<dt><code>checkpoints</code></dt><dd><p>The integer vector of iteration counts for checkpoints.</p>
</dd>
<dt><code>checkpoints_paths</code></dt><dd><p>The paths to the directories where each checkpoint is stored.</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-EscalationRule-new"><code>EscalationRule$new()</code></a>
</p>
</li>
<li> <p><a href="#method-EscalationRule-opt_action"><code>EscalationRule$opt_action()</code></a>
</p>
</li>
<li> <p><a href="#method-EscalationRule-resume_learning"><code>EscalationRule$resume_learning()</code></a>
</p>
</li>
<li> <p><a href="#method-EscalationRule-set_info"><code>EscalationRule$set_info()</code></a>
</p>
</li>
<li> <p><a href="#method-EscalationRule-print"><code>EscalationRule$print()</code></a>
</p>
</li>
<li> <p><a href="#method-EscalationRule-clone"><code>EscalationRule$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-EscalationRule-new"></a>



<h4>Method <code>new()</code></h4>

<p>Create a new EscalationRule object.
</p>


<h5>Usage</h5>

<div class="r"><pre>EscalationRule$new(dir = "latest", base_dir = "escalation_rules")</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>dir</code></dt><dd><p>A character value. A directory name or path where an
escalation rule is outputted. By default, the latest escalation
rule is searched in 'base_dir'.</p>
</dd>
<dt><code>base_dir</code></dt><dd><p>A character value. A directory path that is used as the
parent directory if the 'dir' argument is a directory name and is
not used otherwise.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-EscalationRule-opt_action"></a>



<h4>Method <code>opt_action()</code></h4>

<p>Compute optimal action probabilities using the obtained escalation rule
for data of N and DLT.
</p>


<h5>Usage</h5>

<div class="r"><pre>EscalationRule$opt_action(current_dose, data_Ns, data_DLTs)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>current_dose</code></dt><dd><p>An integer value. This is the current dose index,
which is within <code>1:J</code>.</p>
</dd>
<dt><code>data_Ns</code></dt><dd><p>A numeric vector. The cumulative number of patients
assigned to each dose in your clinical trial.</p>
</dd>
<dt><code>data_DLTs</code></dt><dd><p>A numeric vector. The cumulative number of DLTs
corresponding to each dose for the 'data_Ns' argument.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>A character that represents the optimal action. One of the followings:
down, stay, up, MTD_1, ..., MTD_J, no_MTD
</p>


<hr>
<a id="method-EscalationRule-resume_learning"></a>



<h4>Method <code>resume_learning()</code></h4>

<p>Resume learning the escalation rule. This function updates the original
EscalationRule object.
</p>


<h5>Usage</h5>

<div class="r"><pre>EscalationRule$resume_learning(iter)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>iter</code></dt><dd><p>A number of additional iterations.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>An updated <a href="#topic+EscalationRule">EscalationRule</a> object.
</p>


<hr>
<a id="method-EscalationRule-set_info"></a>



<h4>Method <code>set_info()</code></h4>

<p>Set information when learning the escalation rule.
</p>


<h5>Usage</h5>

<div class="r"><pre>EscalationRule$set_info(info, input, log, checkpoints)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>info</code></dt><dd><p>Information when learning the escalation rule.</p>
</dd>
<dt><code>input</code></dt><dd><p>Inputs for learning the escalation rule.</p>
</dd>
<dt><code>log</code></dt><dd><p>The log of scores during the learning of the escalation rule.</p>
</dd>
<dt><code>checkpoints</code></dt><dd><p>The paths to the directories where each checkpoint is stored.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-EscalationRule-print"></a>



<h4>Method <code>print()</code></h4>

<p>Print function for EscalationRule object
</p>


<h5>Usage</h5>

<div class="r"><pre>EscalationRule$print()</pre></div>


<hr>
<a id="method-EscalationRule-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>EscalationRule$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='learn_escalation_rule'>Build an Optimal Dose Escalation Rule using Reinforcement Learning</h2><span id='topic+learn_escalation_rule'></span>

<h3>Description</h3>

<p>Build an Optimal Dose Escalation Rule using Reinforcement Learning
</p>


<h3>Usage</h3>

<pre><code class='language-R'>learn_escalation_rule(
  J,
  target,
  epsilon,
  delta,
  N_total,
  N_cohort,
  seed = NULL,
  rl_config = rl_config_set(),
  rl_scenarios = NULL,
  output_dir = format(Sys.time(), "%Y%m%d_%H%M%S"),
  output_base_dir = "escalation_rules",
  checkpoint_dir = "checkpoints"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="learn_escalation_rule_+3A_j">J</code></td>
<td>
<p>A positive integer value. The number of doses.</p>
</td></tr>
<tr><td><code id="learn_escalation_rule_+3A_target">target</code></td>
<td>
<p>A positive numeric value. The target DLT probability.</p>
</td></tr>
<tr><td><code id="learn_escalation_rule_+3A_epsilon">epsilon</code></td>
<td>
<p>A positive numeric value. The acceptable range of target DLT
probabilities is defined as [<code>target</code> - <code>epsilon</code>, <code>target</code> + <code>epsilon</code>].</p>
</td></tr>
<tr><td><code id="learn_escalation_rule_+3A_delta">delta</code></td>
<td>
<p>A positive numeric value. The unacceptable ranges of target DLT
probabilities are defined as [0, <code>target</code> - <code>delta</code>] and
[<code>target</code> + <code>delta</code>, 1].</p>
</td></tr>
<tr><td><code id="learn_escalation_rule_+3A_n_total">N_total</code></td>
<td>
<p>A positive integer value. The total number of patients.</p>
</td></tr>
<tr><td><code id="learn_escalation_rule_+3A_n_cohort">N_cohort</code></td>
<td>
<p>A positive integer value. The number of patients for each cohort.</p>
</td></tr>
<tr><td><code id="learn_escalation_rule_+3A_seed">seed</code></td>
<td>
<p>An integer value. Random seed for reinforcement learning.</p>
</td></tr>
<tr><td><code id="learn_escalation_rule_+3A_rl_config">rl_config</code></td>
<td>
<p>A list. Other settings for reinforcement learning. See
<a href="#topic+rl_config_set">rl_config_set</a> for details.</p>
</td></tr>
<tr><td><code id="learn_escalation_rule_+3A_rl_scenarios">rl_scenarios</code></td>
<td>
<p>A list. Scenarios used for reinforcement learning.
Default is <code>NULL</code> (use scenarios in the Sect. 2.2 of the original paper).
See <a href="#topic+compute_rl_scenarios">compute_rl_scenarios</a> for details.</p>
</td></tr>
<tr><td><code id="learn_escalation_rule_+3A_output_dir">output_dir</code></td>
<td>
<p>A character value. Directory name or path to store the
built escalation rule. Default is the current datetime.</p>
</td></tr>
<tr><td><code id="learn_escalation_rule_+3A_output_base_dir">output_base_dir</code></td>
<td>
<p>A character value. Parent directory path where the
built escalation rule will be stored. Valid only if 'output_dir' does
not contain '/'. Default is &quot;escalation_rules&quot;.</p>
</td></tr>
<tr><td><code id="learn_escalation_rule_+3A_checkpoint_dir">checkpoint_dir</code></td>
<td>
<p>A character value. Parent directory path to save
checkpoints. It enables you to resume learning from that point onwards.
Default is &quot;checkpoints&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <a href="#topic+EscalationRule">EscalationRule</a> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(RLescalation)

# We obtain an optimal dose escalation rule by executing `learn_escalation_rule()`.
## Not run: 
escalation_rule &lt;- learn_escalation_rule(
  J = 6, target = 0.25, epsilon = 0.04, delta = 0.1,
  N_total = 36, N_cohort = 3, seed = 123,
  rl_config = rl_config_set(iter = 1000)
)
## End(Not run)

</code></pre>

<hr>
<h2 id='rl_config_set'>Configuration of Reinforcement Learning</h2><span id='topic+rl_config_set'></span>

<h3>Description</h3>

<p>Mainly settings for the arguments of the training() function.
Not compatible with the new API stack introduced in Ray 2.10.0.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rl_config_set(
  iter = 1000L,
  save_start_iter = NULL,
  save_every_iter = NULL,
  cores = 4L,
  gamma = 1,
  lr = 5e-05,
  train_batch_size = 10000L,
  model = rl_dnn_config(),
  sgd_minibatch_size = 200L,
  num_sgd_iter = 20L,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rl_config_set_+3A_iter">iter</code></td>
<td>
<p>A positive integer value. Number of iterations.</p>
</td></tr>
<tr><td><code id="rl_config_set_+3A_save_start_iter">save_start_iter</code>, <code id="rl_config_set_+3A_save_every_iter">save_every_iter</code></td>
<td>
<p>An integer value. Save checkpoints every
'save_every_iter' iterations starting from 'save_start_iter' or later.</p>
</td></tr>
<tr><td><code id="rl_config_set_+3A_cores">cores</code></td>
<td>
<p>A positive integer value. Number of CPU cores used for learning.</p>
</td></tr>
<tr><td><code id="rl_config_set_+3A_gamma">gamma</code></td>
<td>
<p>A positive numeric value. Discount factor of the Markov decision
process. Default is 1.0 (not discount).</p>
</td></tr>
<tr><td><code id="rl_config_set_+3A_lr">lr</code></td>
<td>
<p>A positive numeric value. Learning rate (default 5e-5). You can set
a learning schedule instead of a learning rate.</p>
</td></tr>
<tr><td><code id="rl_config_set_+3A_train_batch_size">train_batch_size</code></td>
<td>
<p>A positive integer value. Training batch size.
Deprecated on the new API stack.</p>
</td></tr>
<tr><td><code id="rl_config_set_+3A_model">model</code></td>
<td>
<p>A list. Arguments passed into the policy model. See
<a href="#topic+rl_dnn_config">rl_dnn_config</a> for details.</p>
</td></tr>
<tr><td><code id="rl_config_set_+3A_sgd_minibatch_size">sgd_minibatch_size</code></td>
<td>
<p>A positive integer value. Total SGD batch size
across all devices for SGD. Deprecated on the new API stack.</p>
</td></tr>
<tr><td><code id="rl_config_set_+3A_num_sgd_iter">num_sgd_iter</code></td>
<td>
<p>A positive integer value. Number of SGD iterations in
each outer loop.</p>
</td></tr>
<tr><td><code id="rl_config_set_+3A_...">...</code></td>
<td>
<p>Other settings for training(). See the arguments of the training()
function in the source code of RLlib.
https://github.com/ray-project/ray/blob/master/rllib/algorithms/algorithm_config.py
https://github.com/ray-project/ray/blob/master/rllib/algorithms/ppo/ppo.py</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of reinforcement learning configuration parameters
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
escalation_rule &lt;- learn_escalation_rule(
  J = 6, target = 0.25, epsilon = 0.04, delta = 0.1,
  N_total = 36, N_cohort = 3, seed = 123,
  # We change `iter` to 200 and `cores` for reinforcement learning to 2
  rl_config = rl_config_set(iter = 200, cores = 2)
)
## End(Not run) 

</code></pre>

<hr>
<h2 id='rl_dnn_config'>DNN Configuration for Reinforcement Learning</h2><span id='topic+rl_dnn_config'></span>

<h3>Description</h3>

<p>DNN (deep neural network) configuration for reinforcement learning.
For detail, see Section 3.1 of the original paper.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rl_dnn_config(
  fcnet_hiddens = c(256L, 256L),
  fcnet_activation = c("relu", "tanh", "swish", "silu", "linear"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rl_dnn_config_+3A_fcnet_hiddens">fcnet_hiddens</code></td>
<td>
<p>A positive integer vector. Numbers of units of the
intermediate layers.</p>
</td></tr>
<tr><td><code id="rl_dnn_config_+3A_fcnet_activation">fcnet_activation</code></td>
<td>
<p>A character value specifying the activation function.
Possible values are &quot;ReLU&quot; (default), &quot;tanh&quot;, &quot;Swish&quot; (or &quot;SiLU&quot;), or
&quot;linear&quot;.</p>
</td></tr>
<tr><td><code id="rl_dnn_config_+3A_...">...</code></td>
<td>
<p>Other configurations. See source code of RLlib.
https://github.com/ray-project/ray/blob/master/rllib/models/catalog.py</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of DNN configuration parameters
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
escalation_rule &lt;- learn_escalation_rule(
  J = 6, target = 0.25, epsilon = 0.04, delta = 0.1,
  N_total = 36, N_cohort = 3, seed = 123,
  rl_config = rl_config_set(
    iter = 1000, 
    # We change the DNN model
    model = rl_dnn_config(fcnet_hiddens = c(512L, 512L), fcnet_activation = "tanh")
  )
)
## End(Not run) 

</code></pre>

<hr>
<h2 id='setup_python'>Setting up a Python Virtual Environment</h2><span id='topic+setup_python'></span>

<h3>Description</h3>

<p>Setting up a Python virtual environment for the Ray package, which includes
the RLlib library for reinforcement learning.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setup_python(envname = "r-RLescalation")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="setup_python_+3A_envname">envname</code></td>
<td>
<p>Python virtual environment name.</p>
</td></tr>
</table>

<hr>
<h2 id='simulate_one_trial'>Simulate One Trial Using an Obtained Optimal Dose Escalation Rule</h2><span id='topic+simulate_one_trial'></span>

<h3>Description</h3>

<p>Simulate One Trial Using an Obtained Optimal Dose Escalation Rule
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simulate_one_trial(escalation_rule, prob_true, seed = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="simulate_one_trial_+3A_escalation_rule">escalation_rule</code></td>
<td>
<p>An object of class <a href="#topic+EscalationRule">EscalationRule</a>
specifying an obtained optimal dose escalation rule.</p>
</td></tr>
<tr><td><code id="simulate_one_trial_+3A_prob_true">prob_true</code></td>
<td>
<p>A numeric vector specifying the true DLT probabilities.</p>
</td></tr>
<tr><td><code id="simulate_one_trial_+3A_seed">seed</code></td>
<td>
<p>An integer value. Random seed for data generation in this trial.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame which contains the cohort ID, the assigned dose,
the number of assigned patients, the number of DLTs, and the recommended
action including down, stay, up, MTD_1, ..., MTD_J, no_MTD,
and fail to determine MTD.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(RLescalation)

## Not run: 
escalation_rule &lt;- learn_escalation_rule(
  J = 6, target = 0.25, epsilon = 0.04, delta = 0.1,
  N_total = 36, N_cohort = 3, seed = 123,
  rl_config = rl_config_set(iter = 1000)
)
## End(Not run)

prob_true &lt;- c(0.03, 0.13, 0.17, 0.19, 0.26, 0.31)

# Simulate one trial using the obtained `escalation_rule`
## Not run: 
sim_one &lt;- simulate_one_trial(escalation_rule, prob_true, seed = 123)
## End(Not run)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
