<!DOCTYPE html><html><head><title>Help for package causalweight</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {causalweight}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#attrlateweight'><p>Local average treatment effect estimation in multiple follow-up periods with outcome attrition based on inverse probability weighting</p></a></li>
<li><a href='#coffeeleaflet'><p>Information leaflet on coffee production and environmental awareness of high school / university students in Bulgaria</p></a></li>
<li><a href='#coupon'><p>Data on daily spending and coupon receipt (selective subsample)</p>
This data set is a selective subsample of the data set &quot;couponsretailer&quot; which was constructed for illustrative purposes.</a></li>
<li><a href='#couponsretailer'><p>Data on daily spending and coupon receipt</p>
A dataset containing information on the purchasing behavior of 1582 retail store customers across 32 coupon campaigns.</a></li>
<li><a href='#didweight'><p>Difference-in-differences based on inverse probability weighting</p></a></li>
<li><a href='#dyntreatDML'><p>Dynamic treatment effect evaluation with double machine learning</p></a></li>
<li><a href='#games'><p>Sales of video games</p></a></li>
<li><a href='#identificationDML'><p>Testing identification with double machine learning</p></a></li>
<li><a href='#india'><p>India's National Health Insurance Program (RSBY)</p></a></li>
<li><a href='#ivnr'><p>Instrument-based treatment evaluation under endogeneity and non-response bias</p></a></li>
<li><a href='#JC'><p>Job Corps data</p></a></li>
<li><a href='#lateweight'><p>Local average treatment effect estimation based on inverse probability weighting</p></a></li>
<li><a href='#medDML'><p>Causal mediation analysis with double machine learning</p></a></li>
<li><a href='#medlateweight'><p>Causal mediation analysis with instruments for treatment and mediator based on weighting</p></a></li>
<li><a href='#medweight'><p>Causal mediation analysis based on inverse probability weighting with optional sample selection correction.</p></a></li>
<li><a href='#medweightcont'><p>Causal mediation analysis with a continuous treatment based on weighting by the inverse of generalized propensity scores</p></a></li>
<li><a href='#RDDcovar'><p>Sharp regression discontinuity design conditional on covariates</p></a></li>
<li><a href='#rkd'><p>Swedish municipalities</p></a></li>
<li><a href='#swissexper'><p>Correspondence test in Swiss apprenticeship market</p></a></li>
<li><a href='#treatDML'><p>Binary or multiple  discrete treatment effect evaluation with double machine learning</p></a></li>
<li><a href='#treatselDML'><p>Binary or multiple  treatment effect evaluation with double machine learning under sample selection/outcome attrition</p></a></li>
<li><a href='#treatweight'><p>Treatment evaluation based on inverse probability weighting with optional sample selection correction.</p></a></li>
<li><a href='#ubduration'><p>Austrian unemployment duration data</p></a></li>
<li><a href='#wexpect'><p>Wage expectations of students in Switzerland</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Estimation Methods for Causal Inference Based on Inverse
Probability Weighting</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Hugo Bodory &lt;hugo.bodory@unisg.ch&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Various estimators of causal effects based on inverse probability weighting, doubly robust estimation, and double machine learning. Specifically, the package includes methods for estimating average treatment effects, direct and indirect effects in causal mediation analysis, and dynamic treatment effects. The models refer to studies of Froelich (2007) &lt;<a href="https://doi.org/10.1016%2Fj.jeconom.2006.06.004">doi:10.1016/j.jeconom.2006.06.004</a>&gt;, Huber (2012) &lt;<a href="https://doi.org/10.3102%2F1076998611411917">doi:10.3102/1076998611411917</a>&gt;, Huber (2014) &lt;<a href="https://doi.org/10.1080%2F07474938.2013.806197">doi:10.1080/07474938.2013.806197</a>&gt;, Huber (2014) &lt;<a href="https://doi.org/10.1002%2Fjae.2341">doi:10.1002/jae.2341</a>&gt;, Froelich and Huber (2017) &lt;<a href="https://doi.org/10.1111%2Frssb.12232">doi:10.1111/rssb.12232</a>&gt;, Hsu, Huber, Lee, and Lettry (2020)  &lt;<a href="https://doi.org/10.1002%2Fjae.2765">doi:10.1002/jae.2765</a>&gt;, and others.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0), ranger</td>
</tr>
<tr>
<td>Imports:</td>
<td>mvtnorm, np, LARF, hdm, SuperLearner, glmnet, xgboost, e1071,
fastDummies, grf, checkmate</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-01-24 14:02:52 UTC; HBodory</td>
</tr>
<tr>
<td>Author:</td>
<td>Hugo Bodory <a href="https://orcid.org/0000-0002-3645-1204"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre],
  Martin Huber <a href="https://orcid.org/0000-0002-8590-9402"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Jannis Kueck <a href="https://orcid.org/0000-0003-4367-0285"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-01-24 14:32:55 UTC</td>
</tr>
</table>
<hr>
<h2 id='attrlateweight'>Local average treatment effect estimation in multiple follow-up periods with outcome attrition based on inverse probability weighting</h2><span id='topic+attrlateweight'></span>

<h3>Description</h3>

<p>Instrumental variable-based evaluation of local average treatment effects using weighting by the inverse of the instrument propensity score.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>attrlateweight(
  y1,
  y2,
  s1,
  s2,
  d,
  z,
  x0,
  x1,
  weightmax = 0.1,
  boot = 1999,
  cluster = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="attrlateweight_+3A_y1">y1</code></td>
<td>
<p>Outcome variable in the first outcome period.</p>
</td></tr>
<tr><td><code id="attrlateweight_+3A_y2">y2</code></td>
<td>
<p>Outcome variable in the second outcome period.</p>
</td></tr>
<tr><td><code id="attrlateweight_+3A_s1">s1</code></td>
<td>
<p>Selection indicator for first outcome period. Must be one if <code>y1</code> is observed (non-missing) and zero if <code>y1</code> is not observed (missing).</p>
</td></tr>
<tr><td><code id="attrlateweight_+3A_s2">s2</code></td>
<td>
<p>Selection indicator for second outcome period. Must be one if <code>y1</code> is observed (non-missing) and zero if <code>y1</code> is not observed (missing).</p>
</td></tr>
<tr><td><code id="attrlateweight_+3A_d">d</code></td>
<td>
<p>Treatment, must be binary (either 1 or 0), must not contain missings.</p>
</td></tr>
<tr><td><code id="attrlateweight_+3A_z">z</code></td>
<td>
<p>Instrument for the endogenous treatment, must be binary (either 1 or 0), must not contain missings.</p>
</td></tr>
<tr><td><code id="attrlateweight_+3A_x0">x0</code></td>
<td>
<p>Baseline (pre-instrument) confounders of the instrument and outcome, must not contain missings.</p>
</td></tr>
<tr><td><code id="attrlateweight_+3A_x1">x1</code></td>
<td>
<p>Confounders in outcome period 1 (may include outcomes of period 1 <code>y1</code>)</p>
</td></tr>
<tr><td><code id="attrlateweight_+3A_weightmax">weightmax</code></td>
<td>
<p>Trimming rule based on the maximum relative weight a single observation may obtain in estimation - observations with higher weights are discarded. Default is 0.1 (no observation can be assigned more than 10 percent of weights)</p>
</td></tr>
<tr><td><code id="attrlateweight_+3A_boot">boot</code></td>
<td>
<p>Number of bootstrap replications for estimating standard errors. Default is 1999.</p>
</td></tr>
<tr><td><code id="attrlateweight_+3A_cluster">cluster</code></td>
<td>
<p>A cluster ID for block or cluster bootstrapping when units are clustered rather than iid. Must be numerical. Default is NULL (standard bootstrap without clustering).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Estimation of local average treatment effects of a binary endogenous treatment on outcomes in two follow up periods that are prone to attrition. Treatment endogeneity is tackled by a binary instrument that is assumed to be conditionally valid given observed baseline confounders <code>x0</code>. Outcome attrition is tackled by either assuming that it is missing at random (MAR), i.e. selection w.r.t. observed variables  <code>d</code>, <code>z</code>, <code>x0</code>, <code>x1</code> (in the case of <code>y2</code>), and <code>s1</code> (in the case of <code>y2</code>); or by assuming latent ignorability (LI), i.e. selection w.r.t. the treatment compliance type as well as <code>z</code>, <code>x0</code>, <code>x1</code> (in the case of <code>y2</code>), and <code>s1</code> (in the case of <code>y2</code>). Units are weighted by the inverse of their conditional instrument and selection propensities, which are estimated by probit regression. Standard errors are obtained by bootstrapping the effect.
</p>


<h3>Value</h3>

<p>An attrlateweight object contains one component <code>results</code>:
</p>
<p><code>results</code>: a 4X4 matrix containing the effect estimates in the first row (&quot;effects&quot;), standard errors in the second row (&quot;se&quot;), p-values in the third row (&quot;p-value&quot;), and the number of trimmed observations due to too large weights in the fourth row (&quot;trimmed obs&quot;). The first column provides the local average treatment effect (LATE) on <code>y1</code> among compliers under missingness at random (MAR). The second column provides the local average treatment effect (LATE) on <code>y2</code>  under missingness at random (MAR). The third column provides the local average treatment effect (LATE) on <code>y1</code>  under latent ignorability (LI). The forth column provides the local average treatment effect (LATE) on <code>y2</code> under latent ignorability (LI).
</p>


<h3>References</h3>

<p>Fr√∂lich, M., Huber, M. (2014): &quot;Treatment Evaluation With Multiple Outcome Periods Under Endogeneity and Attrition&quot;, Journal of the American Statistical Association, 109, 1697-1711.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># A little example with simulated data (4000 observations)
## Not run: 
n=4000
e=(rmvnorm(n,rep(0,3), matrix(c(1,0.3,0.3,  0.3,1,0.3,  0.3,0.3,1),3,3) ))
x0=runif(n,0,1)
z=(0.25*x0+rnorm(n)&gt;0)*1
d=(1.2*z-0.25*x0+e[,1]&gt;0.5)*1
y1_star=0.5*x0+0.5*d+e[,2]
s1=(0.25*x0+0.25*d+rnorm(n)&gt;-0.5)*1
y1=s1*y1_star
x1=(0.5*x0+0.5*rnorm(n))
y2_star=0.5*x0+x1+d+e[,3]
s2=s1*((0.25*x0+0.25*x1+0.25*d+rnorm(n)&gt;-0.5)*1)
y2=s2*y2_star
# The true LATEs on y1 and y2 are equal to 0.5 and 1, respectively.
output=attrlateweight(y1=y1,y2=y2,s1=s1,s2=s2,d=d,z=z,x0=x0,x1=x1,boot=19)
round(output$results,3)
## End(Not run)
</code></pre>

<hr>
<h2 id='coffeeleaflet'>Information leaflet on coffee production and environmental awareness of high school / university students in Bulgaria</h2><span id='topic+coffeeleaflet'></span>

<h3>Description</h3>

<p>A dataset on the impact of an information leaflet about coffee production on students' awareness about environmental issues
collected at Bulgarian highschools and universities in the year 2015.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>coffeeleaflet
</code></pre>


<h3>Format</h3>

<p>A data frame with 522 rows and 48 variables:
</p>

<dl>
<dt>grade</dt><dd><p>school grade</p>
</dd>
<dt>sex</dt><dd><p>1=male, 0=female</p>
</dd>
<dt>age</dt><dd><p>age in years</p>
</dd>
<dt>mob</dt><dd><p>month of birth</p>
</dd>
<dt>bulgnationality</dt><dd><p>dummy for Bulgarian nationality</p>
</dd>
<dt>langbulg</dt><dd><p>dummy for Bulgarian mother tongue</p>
</dd>
<dt>mumage</dt><dd><p>mother's age in years</p>
</dd>
<dt>mumedu</dt><dd><p>mother's education (1=lower secondary or less, 2=upper secondary, 3=higher)</p>
</dd>
<dt>mumprof</dt><dd><p>mother's profession (1=manager, 2=specialist, 3=worker, 4=self-employed,5=not working,6=retired,7=other)</p>
</dd>
<dt>dadage</dt><dd><p>father's age in years</p>
</dd>
<dt>dadedu</dt><dd><p>father's education (1=lower secondary or less, 2=upper secondary, 3=higher)</p>
</dd>
<dt>dadprof</dt><dd><p>father's profession (1=manager, 2=specialist, 3=worker, 4=self-employed,5=not working,6=retired,7=other)</p>
</dd>
<dt>material</dt><dd><p>material situation of the family (1=very bad,...,5=very good)</p>
</dd>
<dt>withbothpar</dt><dd><p>dummy for living with both parents</p>
</dd>
<dt>withmum</dt><dd><p>dummy for living with mother only</p>
</dd>
<dt>withdad</dt><dd><p>dummy for living with father only</p>
</dd>
<dt>withneither</dt><dd><p>dummy for living with neither mother nor father</p>
</dd>
<dt>oldsiblings</dt><dd><p>number of older siblings</p>
</dd>
<dt>youngsiblings</dt><dd><p>numer of younger siblings</p>
</dd>
<dt>schoolmaths</dt><dd><p>school dummy (for highschool with maths specialization)</p>
</dd>
<dt>schoolrakdelsve</dt><dd><p>school dummy</p>
</dd>
<dt>schoolvazov</dt><dd><p>school dummy</p>
</dd>
<dt>schoolfinance</dt><dd><p>school dummy</p>
</dd>
<dt>schoolvarna</dt><dd><p>school dummy (for highschool in city of Varna)</p>
</dd>
<dt>schoolspanish</dt><dd><p>school dummy (for Spanish highschool)</p>
</dd>
<dt>schooltechuni</dt><dd><p>school dummy (for technical university)</p>
</dd>
<dt>schoolvidin</dt><dd><p>school dummy (for highschool in city of Vidin)</p>
</dd>
<dt>schooluni</dt><dd><p>school dummy (for university)</p>
</dd>
<dt>citysofia</dt><dd><p>dummy for the capital city of Sofia</p>
</dd>
<dt>cityvarna</dt><dd><p>dummy for the city of Varna</p>
</dd>
<dt>cityvidin</dt><dd><p>dummy for the city of Vidin</p>
</dd>
<dt>treatment</dt><dd><p>treatment (1=leaflet on environmental impact of coffee growing, 0=control group)</p>
</dd>
<dt>drinkcoffee</dt><dd><p>drinks coffee (1=never, 2=not more than 1 time per week,3=several times per week, 4=1 time per day, 5=several times per day)</p>
</dd>
<dt>cupsest</dt><dd><p>outcome: guess how many cups of coffee per capita are consumed in Bulgaria per year</p>
</dd>
<dt>devi_cupsest</dt><dd><p>outcome: deviation of guess from true coffee consumption per capita and year in Bulgaria</p>
</dd>
<dt>impworldecon</dt><dd><p>outcome: assess the importance of coffee for world economy (1=not at all important,..., 5=very important)</p>
</dd>
<dt>impincome</dt><dd><p>assess the importance of coffee as a source of income for people in Africa and Latin America (1=not at all important,..., 5=very important)</p>
</dd>
<dt>awarewaste</dt><dd><p>outcome: awareness of waste production due to coffee production (1=not aware,..., 5=fully aware)</p>
</dd>
<dt>awarepesticide</dt><dd><p>outcome: awareness of pesticide use due to coffee production (1=not aware,..., 5=fully aware)</p>
</dd>
<dt>awaredeforestation</dt><dd><p>outcome: awareness of deforestation due to coffee production (1=not aware,..., 5=fully aware)</p>
</dd>
<dt>awarewastewater</dt><dd><p>outcome: awareness of waste water due to coffee production (1=not aware,..., 5=fully aware)</p>
</dd>
<dt>awarebiodiversityloss</dt><dd><p>outcome: awareness of biodiversity loss due to coffee production (1=not aware,..., 5=fully aware)</p>
</dd>
<dt>awareunfairworking</dt><dd><p>outcome: awareness of unfair working conditions due to coffee production (1=not aware,..., 5=fully aware)</p>
</dd>
<dt>reusepurposeful</dt><dd><p>outcome: can coffee waste be reused purposefully (1=no, 2=maybe, 3=yes)</p>
</dd>
<dt>reusesoil</dt><dd><p>outcome: can coffee waste be reused as soil (1=no, 2=maybe, 3=yes)</p>
</dd>
<dt>choiceprice</dt><dd><p>importance of price when buying coffee (1=not important at all,..., 5=very important, 6=I don't drink coffee)</p>
</dd>
<dt>choicetastepleasure</dt><dd><p>importance of pleasure or taste when buying coffee (1=not important at all,..., 5=very important, 6=I don't drink coffee)</p>
</dd>
<dt>choiceenvironsocial</dt><dd><p>importance of environmental or social impact when buying coffee (1=not important at all,..., 5=very important, 6=I don't drink coffee)</p>
</dd>
</dl>



<h3>References</h3>

<p>Faldzhiyskiy, S. (Ecosystem Europe, Bulgaria) and Huber, M. (University of Fribourg): &quot;The impact of an information leaflet about coffee production on students' awareness about environmental issues&quot;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data(coffeeleaflet)
attach(coffeeleaflet)
data=na.omit(cbind(awarewaste,treatment,grade,sex,age))
# effect of information leaflet (treatment) on awareness of waste production
treatweight(y=data[,1],d=data[,2],x=data[,3:5],boot=199)
## End(Not run)
</code></pre>

<hr>
<h2 id='coupon'>Data on daily spending and coupon receipt (selective subsample)
This data set is a selective subsample of the data set &quot;couponsretailer&quot; which was constructed for illustrative purposes.</h2><span id='topic+coupon'></span>

<h3>Description</h3>

<p>Data on daily spending and coupon receipt (selective subsample)
This data set is a selective subsample of the data set &quot;couponsretailer&quot; which was constructed for illustrative purposes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>coupon
</code></pre>


<h3>Format</h3>

<p>A data frame with 1293 rows and 9 variables:
</p>

<dl>
<dt>dailyspending</dt><dd><p>outcome: customer's daily spending at the retailer in a specific period</p>
</dd>
<dt>coupons</dt><dd><p>treatment: 1 = customer received at least one coupon in that period; 0 = customer did not receive any coupon</p>
</dd>
<dt>coupons_preperiod</dt><dd><p>coupon reception in previous period: 1 = customer received at least one coupon; 0 = customer did not receive any coupon</p>
</dd>
<dt>dailyspending_preperiod</dt><dd><p>daily spending at the retailer in previous period</p>
</dd>
<dt>income_bracket</dt><dd><p>income group: 1 = lowest to 12 = highest</p>
</dd>
<dt>age_range</dt><dd><p>age of customer: 1 = 18-25; 2 = 26-35; 3 = 36-45; 4 = 46-55; 5 = 56-70; 6 = 71 plus</p>
</dd>
<dt>married</dt><dd><p>marital status: 1 = married; 0 = unmarried</p>
</dd>
<dt>rented</dt><dd><p>dwelling type: 1 = rented; 0 = owned</p>
</dd>
<dt>family_size</dt><dd><p>number of family members: 1 = 1; 2 = 2; 3 = 3; 4 = 4; 5 = 5 plus</p>
</dd>
</dl>


<hr>
<h2 id='couponsretailer'>Data on daily spending and coupon receipt
A dataset containing information on the purchasing behavior of 1582 retail store customers across 32 coupon campaigns.</h2><span id='topic+couponsretailer'></span>

<h3>Description</h3>

<p>Data on daily spending and coupon receipt
A dataset containing information on the purchasing behavior of 1582 retail store customers across 32 coupon campaigns.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>couponsretailer
</code></pre>


<h3>Format</h3>

<p>A data frame with 50,624 rows and 27 variables:
</p>

<dl>
<dt>customer_id</dt><dd><p>customer identifier</p>
</dd>
<dt>period</dt><dd><p>period of observation: 1 = 1st period to 32 = last period</p>
</dd>
<dt>age_range</dt><dd><p>age of customer: 1 = 18-25; 2 = 26-35; 3 = 36-45; 4 = 46-55; 5 = 56-70; 6 = 71 plus</p>
</dd>
<dt>married</dt><dd><p>marital status: 1 = married; 0 = unmarried</p>
</dd>
<dt>rented</dt><dd><p>dwelling type: 1 = rented; 0 = owned</p>
</dd>
<dt>family_size</dt><dd><p>number of family members: 1 = 1; 2 = 2; 3 = 3; 4 = 4; 5 = 5 plus</p>
</dd>
<dt>income_bracket</dt><dd><p>income group: 1 = lowest to 12 = highest</p>
</dd>
<dt>dailyspending_preperiod</dt><dd><p>customer's daily spending at the retailer in previous period</p>
</dd>
<dt>purchase_ReadyEatFood_preperiod</dt><dd><p>purchases of ready-to-eat food in previous period: 1 = yes, 0 = no</p>
</dd>
<dt>purchase_MeatSeafood_preperiod</dt><dd><p>purchases of meat and seafood products in previous period: 1 = yes, 0 = no</p>
</dd>
<dt>purchase_OtherFood_preperiod</dt><dd><p>purchases of other food products in previous period: 1 = yes, 0 = no</p>
</dd>
<dt>purchase_Drugstore_preperiod</dt><dd><p>purchases of drugstore products in previous period: 1 = yes, 0 = no</p>
</dd>
<dt>purchase_OtherNonfood_preperiod</dt><dd><p>purchases of other non-food products in previous period: 1 = yes, 0 = no</p>
</dd>
<dt>coupons_Any_preperiod</dt><dd><p>coupon reception in previous period: 1 = customer received at least one coupon; 0 = customer did not receive any coupon</p>
</dd>
<dt>coupons_ReadyEatFood_preperiod</dt><dd><p>coupon reception in previous period: 1 = customer received at least one ready-to-eat food coupon; 0 = customer did not receive any ready-to-eat food coupon</p>
</dd>
<dt>coupons_MeatSeafood_preperiod</dt><dd><p>coupon reception in previous period: 1 = customer received at least one meat/seafood coupon; 0 = customer did not receive any meat/seafood coupon</p>
</dd>
<dt>coupons_OtherFood_preperiod</dt><dd><p>coupon reception in previous period: 1 = customer received at least one coupon applicable to other food items; 0 = customer did not receive any coupon applicable to other food items</p>
</dd>
<dt>coupons_Drugstore_preperiod</dt><dd><p>coupon reception in previous period: 1 = customer received at least one drugstore coupon; 0 = customer did not receive any drugstore coupon</p>
</dd>
<dt>coupons_OtherNonfood_preperiod</dt><dd><p>coupon reception in previous period: 1 = customer received at least one coupon applicable to other non-food items; 0 = customer did not receive any coupon applicable to other non-food items</p>
</dd>
<dt>coupons_Any_redeemed_preperiod</dt><dd><p>coupon redemption in previous period: 1 = customer redeemed at least one coupon; 0 = customer did not redeem any coupon</p>
</dd>
<dt>coupons_Any</dt><dd><p>treatment: 1 = customer received at least one coupon in current period; 0 = customer did not receive any coupon</p>
</dd>
<dt>coupons_ReadyEatFood</dt><dd><p>treatment: 1 = customer received at least one ready-to-eat food coupon; 0 = customer did not receive any ready-to-eat food coupon</p>
</dd>
<dt>coupons_MeatSeafood</dt><dd><p>treatment: 1 = customer received at least one meat/seafood coupon; 0 = customer did not receive any meat/seafood coupon</p>
</dd>
<dt>coupons_OtherFood</dt><dd><p>treatment: 1 = customer received at least one coupon applicable to other food items; 0 = customer did not receive any coupon applicable to other food items</p>
</dd>
<dt>coupons_Drugstore</dt><dd><p>treatment: 1 = customer received at least one drugstore coupon; 0 = customer did not receive any drugstore coupon</p>
</dd>
<dt>coupons_OtherNonfood</dt><dd><p>treatment: 1 = customer received at least one coupon applicable to other non-food items; 0 = customer did not receive any coupon applicable to other non-food items</p>
</dd>
<dt>dailyspending</dt><dd><p>outcome: customer's daily spending at the retailer in current period</p>
</dd>
</dl>



<h3>References</h3>

<p>Langen, Henrika, and Huber, Martin (2023): &quot;How causal machine learning can leverage marketing strategies: Assessing and improving the performance of a coupon campaign.&quot; PLoS ONE, 18 (1): e0278937.
</p>

<hr>
<h2 id='didweight'>Difference-in-differences based on inverse probability weighting</h2><span id='topic+didweight'></span>

<h3>Description</h3>

<p>Difference-in-differences-based estimation of the average treatment effect on the treated in the post-treatment period, given a binary treatment with one pre- and one post-treatment period. Permits controlling for differences in observed covariates across treatment groups and/or time periods based on inverse probability weighting.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>didweight(y, d, t, x = NULL, boot = 1999, trim = 0.05, cluster = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="didweight_+3A_y">y</code></td>
<td>
<p>Dependent variable, must not contain missings.</p>
</td></tr>
<tr><td><code id="didweight_+3A_d">d</code></td>
<td>
<p>Treatment, must be binary (either 1 or 0), must not contain missings.</p>
</td></tr>
<tr><td><code id="didweight_+3A_t">t</code></td>
<td>
<p>Time period, must be binary, 0 for pre-treatment and 1 for post-treatment, must not contain missings.</p>
</td></tr>
<tr><td><code id="didweight_+3A_x">x</code></td>
<td>
<p>Covariates to be controlled for by inverse probability weighting. Default is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="didweight_+3A_boot">boot</code></td>
<td>
<p>Number of bootstrap replications for estimating standard errors. Default is 1999.</p>
</td></tr>
<tr><td><code id="didweight_+3A_trim">trim</code></td>
<td>
<p>Trimming rule for discarding observations with extreme propensity scores in the 3 reweighting steps, which reweight (1) treated in the pre-treatment period, (2) non-treated in the post-treatment period, and (3) non-treated in the pre-treatment period according to the covariate distribution of the treated in the post-treatment period. Default is 0.05, implying that observations with a probability lower than 5 percent of not being treated in some weighting step are discarded.</p>
</td></tr>
<tr><td><code id="didweight_+3A_cluster">cluster</code></td>
<td>
<p>A cluster ID for block or cluster bootstrapping when units are clustered rather than iid. Must be numerical. Default is <code>NULL</code> (standard bootstrap without clustering).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Estimation of the average treatment effect on the treated in the post-treatment period based Difference-in-differences. Inverse probability weighting is used to control for differences in covariates across treatment groups and/or over time. That is, (1) treated observations in the pre-treatment period, (2) non-treated observations in the post-treatment period, and (3) non-treated observations in the pre-treatment period are reweighted according to the covariate distribution of the treated observations in the post-treatment period. The respective propensity scores are obtained by probit regressions.
</p>


<h3>Value</h3>

<p>A didweight object contains 4 components, <code>eff</code>, <code>se</code>, <code>pvalue</code>, and <code>ntrimmed</code>.
</p>
<p><code>eff</code>: estimate of the average treatment effect on the treated in the post-treatment period.
</p>
<p><code>se</code>: standard error obtained by bootstrapping the effect.
</p>
<p><code>pvalue</code>: p-value based on the t-statistic.
</p>
<p><code>ntrimmed</code>: total number of discarded (trimmed) observations in any of the 3 reweighting steps due to extreme propensity score values.
</p>


<h3>References</h3>

<p>Abadie, A. (2005): &quot;Semiparametric Difference-in-Differences Estimators&quot;, The Review of Economic Studies, 72, 1-19.
</p>
<p>Lechner, M. (2011): &quot;The Estimation of Causal Effects by Difference-in-Difference Methods&quot;, Foundations and Trends in Econometrics, 4, 165-224.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># A little example with simulated data (4000 observations)
## Not run: 
n=4000                            # sample size
t=1*(rnorm(n)&gt;0)                  # time period
u=rnorm(n)                        # time constant unobservable
x=0.5*t+rnorm(n)                  # time varying covariate
d=1*(x+u+rnorm(n)&gt;0)              # treatment
y=d*t+d+t+x+u                     # outcome
# The true effect equals 1
didweight(y=y,d=d,t=t,x=x, boot=199)
## End(Not run)
</code></pre>

<hr>
<h2 id='dyntreatDML'>Dynamic treatment effect evaluation with double machine learning</h2><span id='topic+dyntreatDML'></span>

<h3>Description</h3>

<p>Dynamic treatment effect estimation for assessing the average effects of sequences of treatments (consisting of two sequential treatments). Combines estimation based on (doubly robust) efficient score functions with double machine learning to control for confounders in a data-driven way.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dyntreatDML(
  y2,
  d1,
  d2,
  x0,
  x1,
  s = NULL,
  d1treat = 1,
  d2treat = 1,
  d1control = 0,
  d2control = 0,
  trim = 0.01,
  MLmethod = "lasso",
  fewsplits = FALSE,
  normalized = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dyntreatDML_+3A_y2">y2</code></td>
<td>
<p>Dependent variable in the second period (=outcome period), must not contain missings.</p>
</td></tr>
<tr><td><code id="dyntreatDML_+3A_d1">d1</code></td>
<td>
<p>Treatment in the first period, must be discrete, must not contain missings.</p>
</td></tr>
<tr><td><code id="dyntreatDML_+3A_d2">d2</code></td>
<td>
<p>Treatment in the second period, must be discrete, must not contain missings.</p>
</td></tr>
<tr><td><code id="dyntreatDML_+3A_x0">x0</code></td>
<td>
<p>Covariates in the baseline period (prior to the treatment in the first period), must not contain missings.</p>
</td></tr>
<tr><td><code id="dyntreatDML_+3A_x1">x1</code></td>
<td>
<p>Covariates in the first period (prior to the treatment in the second period), must not contain missings.</p>
</td></tr>
<tr><td><code id="dyntreatDML_+3A_s">s</code></td>
<td>
<p>Indicator function for defining a subpopulation for whom the treatment effect is estimated as a function of the subpopulation's distribution of <code>x0</code>. Default is <code>NULL</code> (estimation of the treatment effect in the total population).</p>
</td></tr>
<tr><td><code id="dyntreatDML_+3A_d1treat">d1treat</code></td>
<td>
<p>Value of the first treatment in the treatment sequence. Default is 1.</p>
</td></tr>
<tr><td><code id="dyntreatDML_+3A_d2treat">d2treat</code></td>
<td>
<p>Value of the second treatment in the treatment sequence. Default is 1.</p>
</td></tr>
<tr><td><code id="dyntreatDML_+3A_d1control">d1control</code></td>
<td>
<p>Value of the first treatment in the control sequence. Default is 0.</p>
</td></tr>
<tr><td><code id="dyntreatDML_+3A_d2control">d2control</code></td>
<td>
<p>Value of the second treatment in the control sequence. Default is 0.</p>
</td></tr>
<tr><td><code id="dyntreatDML_+3A_trim">trim</code></td>
<td>
<p>Trimming rule for discarding observations with products of treatment propensity scores in the first and second period that are smaller than <code>trim</code> (to avoid too small denominators in weighting by the inverse of the propensity scores). Default is 0.01.</p>
</td></tr>
<tr><td><code id="dyntreatDML_+3A_mlmethod">MLmethod</code></td>
<td>
<p>Machine learning method for estimating the nuisance parameters based on the <code>SuperLearner</code> package. Must be either  <code>"lasso"</code> (default) for lasso estimation,  <code>"randomforest"</code> for random forests, <code>"xgboost"</code> for xg boosting,  <code>"svm"</code> for support vector machines, <code>"ensemble"</code> for using an ensemble algorithm based on all previously mentioned machine learners, or <code>"parametric"</code> for linear or logit regression.</p>
</td></tr>
<tr><td><code id="dyntreatDML_+3A_fewsplits">fewsplits</code></td>
<td>
<p>If set to <code>TRUE</code>, the same training data are used for estimating a nested model of conditional mean outcomes, namely <code>E[E[y2|d1,d2,x0,x1]|d1,x0]</code>. If <code>fewsplits</code> is <code>FALSE</code>, the training data are split for the sequential estimation of the nested model. Default of <code>fewsplits</code> is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="dyntreatDML_+3A_normalized">normalized</code></td>
<td>
<p>If set to <code>TRUE</code>, then the inverse probability-based weights are normalized such that they add up to 1 within treatment groups. Default is <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Estimation of the causal effects of sequences of two treatments under sequential conditional independence, assuming that all confounders of the treatment in either period and the outcome of interest are observed. Estimation is based on the (doubly robust) efficient score functions for potential outcomes, see e.g. Bodory, Huber, and Laffers (2020), in combination with double machine learning with cross-fitting, see Chernozhukov et al (2018). To this end, one part of the data is used for estimating the model parameters of the treatment and outcome equations based machine learning. The other part of the data is used for predicting the efficient score functions. The roles of the data parts are swapped (using 3-fold cross-fitting) and the average dynamic treatment effect is estimated based on averaging the predicted efficient score functions in the total sample.
Standard errors are based on asymptotic approximations using the estimated variance of the (estimated) efficient score functions.
</p>


<h3>Value</h3>

<p>A <code>dyntreatDML</code> object contains ten components, <code>effect</code>, <code>se</code>, <code>pval</code>, <code>ntrimmed</code>, <code>meantreat</code>, <code>meancontrol</code>, <code>psd1treat</code>, <code>psd2treat</code>, <code>psd1control</code>, and <code>psd2control</code> :
</p>
<p><code>effect</code>: estimate of the average effect of the treatment sequence.
</p>
<p><code>se</code>: standard error of the effect estimate.
</p>
<p><code>pval</code>: p-value of the effect estimate.
</p>
<p><code>ntrimmed</code>: number of discarded (trimmed) observations due to low products of propensity scores.
</p>
<p><code>meantreat</code>: Estimate of the mean potential outcome under the treatment sequence.
</p>
<p><code>meancontrol</code>: Estimate of the mean potential outcome under the control sequence.
</p>
<p><code>psd1treat</code>: P-score estimates for first treatment in treatment sequence.
</p>
<p><code>psd2treat</code>: P-score estimates for second treatment in treatment sequence.
</p>
<p><code>psd1control</code>: P-score estimates for first treatment in control sequence.
</p>
<p><code>psd2control</code>: P-score estimates for second treatment in control sequence.
</p>


<h3>References</h3>

<p>Bodory, H., Huber, M., Laffers, L. (2020): &quot;Evaluating (weighted) dynamic treatment effects by double machine learning&quot;, working paper, arXiv preprint arXiv:2012.00370.
</p>
<p>Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., Robins, J. (2018): &quot;Double/debiased machine learning for treatment and structural parameters&quot;, The Econometrics Journal, 21, C1-C68.
</p>
<p>van der Laan, M., Polley, E., Hubbard, A. (2007): &quot;Super Learner&quot;, Statistical Applications in Genetics and Molecular Biology, 6.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># A little example with simulated data (2000 observations)
## Not run: 
n=2000
# sample size
p0=10
# number of covariates at baseline
s0=5
# number of covariates that are confounders at baseline
p1=10
# number of additional covariates in period 1
s1=5
# number of additional covariates that are confounders in period 1
x0=matrix(rnorm(n*p0),ncol=p0)
# covariate matrix at baseline
beta0=c(rep(0.25,s0), rep(0,p0-s0))
# coefficients determining degree of confounding for baseline covariates
d1=(x0%*%beta0+rnorm(n)&gt;0)*1
# equation of first treatment in period 1
x1=matrix(rnorm(n*p1),ncol=p1)+matrix(0.1 * d1, nrow = n, ncol = p1)
# covariate matrix for covariates of period 1 (affected by 1st treatment d1)
beta1=c(rep(0.25,s1), rep(0,p1-s1))
# coefficients determining degree of confounding for covariates of period 1
d2=(x0%*%beta0+x1%*%beta1+0.5*d1+rnorm(n)&gt;0)*1
# equation of second treatment in period 2
y2=x0%*%beta0+x1%*%beta1+1*d1+0.5*d2+rnorm(n)
# outcome equation in period 2
output=dyntreatDML(y2=y2,d1=d1,d2=d2,x0=x0,x1=x1,
       d1treat=1,d2treat=1,d1control=0,d2control=0)
cat("dynamic ATE: ",round(c(output$effect),3),", standard error: ",
    round(c(output$se),3), ", p-value: ",round(c(output$pval),3))
output$ntrimmed
# The true effect of the treatment sequence is 1.5
## End(Not run)
</code></pre>

<hr>
<h2 id='games'>Sales of video games</h2><span id='topic+games'></span>

<h3>Description</h3>

<p>A dataset containing information on 3956 video games, including sales as well as expert and user ratings.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>games
</code></pre>


<h3>Format</h3>

<p>A data frame with 3956 rows and 9 variables:
</p>

<dl>
<dt>name</dt><dd><p>factor variable providing the name of the video game</p>
</dd>
<dt>genre</dt><dd><p>factor variable indicating the genre of the game (e.g. Action, Sports...)</p>
</dd>
<dt>platform</dt><dd><p>factor variable indicating the hardware platform of the game (e.g. PC,...)</p>
</dd>
<dt>esrbrating</dt><dd><p>factor variable indicating the age recommendation for the game(E is age 6+, T is 13+, M is 17+)</p>
</dd>
<dt>publisher</dt><dd><p>factor variable indicating the publisher of the game</p>
</dd>
<dt>year</dt><dd><p>numeric variable indicating the year the video game was released</p>
</dd>
<dt>metascore</dt><dd><p>numeric variable providing a weighted average rating of the game by professional critics</p>
</dd>
<dt>userscore</dt><dd><p>numeric variable providing the average user rating of the game</p>
</dd>
<dt>sales</dt><dd><p>numeric variable indicating the total global sales (in millions) of the game up to the year 2018</p>
</dd>
</dl>



<h3>References</h3>

<p>Wittwer, J. (2020): &quot;Der Erfolg von Videospielen - eine empirische Untersuchung moeglicher Erfolgsfaktoren&quot;, BA thesis, University of Fribourg.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#load data
data(games)
#select non-missing observations
games_nomis=na.omit(games)
#turn year into a factor variable
games_nomis$year=factor(games_nomis$year)
#attach data
attach(games_nomis)
#load library for generating dummies
library(fastDummies)
#generate dummies for genre
dummies=dummy_cols(genre, remove_most_frequent_dummy = TRUE)
#drop original variable
genredummies=dummies[,2:ncol(dummies)]
#make dummies numeric
genredummies=apply(genredummies, 2, function(genredummies) as.numeric(genredummies))
#generate dummies for year
dummies=dummy_cols(year, remove_most_frequent_dummy = TRUE)
#drop original variable
yeardummies=dummies[,2:ncol(dummies)]
#make dummies numeric
yeardummies=apply(yeardummies, 2, function(yeardummies) as.numeric(yeardummies))
# mediation analysis with metascore as treatment, userscore as mediator, sales as outcome
x=cbind(genredummies,yeardummies)
output=medweightcont(y=sales,d=metascore, d0=60, d1=80, m=userscore, x=x, boot=199)
round(output$results,3)
output$ntrimmed
## End(Not run)
</code></pre>

<hr>
<h2 id='identificationDML'>Testing identification with double machine learning</h2><span id='topic+identificationDML'></span>

<h3>Description</h3>

<p>Testing identification with double machine learning
</p>


<h3>Usage</h3>

<pre><code class='language-R'>identificationDML(
  y,
  d,
  x,
  z,
  score = "DR",
  bootstrap = FALSE,
  ztreat = 1,
  zcontrol = 0,
  seed = 123,
  MLmethod = "lasso",
  k = 3,
  DR_parameters = list(s = NULL, normalized = TRUE, trim = 0.01),
  squared_parameters = list(zeta_sigma = min(0.5, 500/dim(y)[1])),
  bootstrap_parameters = list(B = 2000, importance = 0.95, alpha = 0.1, share = 0.5)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="identificationDML_+3A_y">y</code></td>
<td>
<p>Dependent variable, must not contain missings.</p>
</td></tr>
<tr><td><code id="identificationDML_+3A_d">d</code></td>
<td>
<p>Treatment variable, must be discrete, must not contain missings.</p>
</td></tr>
<tr><td><code id="identificationDML_+3A_x">x</code></td>
<td>
<p>Covariates, must not contain missings.</p>
</td></tr>
<tr><td><code id="identificationDML_+3A_z">z</code></td>
<td>
<p>Instrument, must not contain missings.</p>
</td></tr>
<tr><td><code id="identificationDML_+3A_score">score</code></td>
<td>
<p>Orthogonal score used for testing identification, either <code>"DR"</code> for using the average of the doubly robust (DR) score function (see Section 6 of Huber and Kueck, 2022) for testing, or <code>"squared"</code> for using squared differences in the conditional means outcomes (see Section 7 of Huber and Kueck, 2022). Default is <code>"DR"</code>. Note that this argument is ignored if <code>bootstrap=TRUE</code>.</p>
</td></tr>
<tr><td><code id="identificationDML_+3A_bootstrap">bootstrap</code></td>
<td>
<p>If set to <code>TRUE</code>, testing identification is based on the DR score function within data-driven partitioning of the data (using a random forest with 200 trees) as described at the end of Sections 6 and 8 in Huber and Kueck (2022). Default is <code>FALSE</code>. Note that the argument <code>score</code> is ignored if <code>bootstrap=TRUE</code>.</p>
</td></tr>
<tr><td><code id="identificationDML_+3A_ztreat">ztreat</code></td>
<td>
<p>Value of the instrument in the &quot;treatment&quot; group. Default is 1.</p>
</td></tr>
<tr><td><code id="identificationDML_+3A_zcontrol">zcontrol</code></td>
<td>
<p>Value of the instrument in the &quot;control&quot; group. Default is 0.</p>
</td></tr>
<tr><td><code id="identificationDML_+3A_seed">seed</code></td>
<td>
<p>Default is 123.</p>
</td></tr>
<tr><td><code id="identificationDML_+3A_mlmethod">MLmethod</code></td>
<td>
<p>Machine learning method for estimating the nuisance parameters based on the <code>SuperLearner</code> package. Must be either  <code>"lasso"</code> (default) for lasso estimation,  <code>"randomforest"</code> for random forests, <code>"xgboost"</code> for xg boosting,  <code>"svm"</code> for support vector machines, <code>"ensemble"</code> for using an ensemble algorithm based on all previously mentioned machine learners, or <code>"parametric"</code> for linear or logit regression.</p>
</td></tr>
<tr><td><code id="identificationDML_+3A_k">k</code></td>
<td>
<p>Number of folds in k-fold cross-fitting. Default is 3.</p>
</td></tr>
<tr><td><code id="identificationDML_+3A_dr_parameters">DR_parameters</code></td>
<td>
<p>List of input parameters to test identification using the doubly robust score:
s: Indicator function for defining a subpopulation for which the treatment effect is estimated as a function of the subpopulation's distribution of <code>x</code>. Default is <code>NULL</code> (estimation of the average treatment effect in the total population).
normalized: If set to <code>TRUE</code>, then the inverse probability-based weights are normalized such that they add up to 1 within treatment groups. Default is <code>TRUE</code>
trim: Trimming rule for discarding observations with treatment propensity scores that are smaller than <code>trim</code> or larger than <code>1-trim</code> (to avoid too small denominators in weighting by the inverse of the propensity scores). Default is 0.01.</p>
</td></tr>
<tr><td><code id="identificationDML_+3A_squared_parameters">squared_parameters</code></td>
<td>
<p>List of input parameters to test identification using the squared deviation:
zeta_sigma: standard deviation of the normal distributed errors to avoid degenerated limit distribution. Default is min(0.05,500/n).</p>
</td></tr>
<tr><td><code id="identificationDML_+3A_bootstrap_parameters">bootstrap_parameters</code></td>
<td>
<p>List of input parameters to test identification using the DR score and sample splitting to detect heterogeneity (if <code>bootstrap=TRUE</code>):
B: number of bootstrap samples to be used in the multiplier bootstrap. Default is 2000.
importance: upper quantile of covariates in terms of their predictive importance for heterogeneity in the DR score function according to a random forest (with 200 trees). The data are split into subsets based on the median values of these predictive covariates (entering the upper quantile). Default is 0.95.
alpha: level of the statistical test. Default is 0.1.
share: share of observations used to detect heterogeneity in the DR score function by the random forest (while the remaining observations are used for hypothesis testing). Default is 0.5.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Testing the identification of causal effects of a treatment <code>d</code> on an outcome <code>y</code> in observational data using a supposed instrument <code>z</code> and controlling for observed covariates <code>x</code>.
</p>


<h3>Value</h3>

<p>An <code>identificationDML</code> object contains different parameters, at least the two following:
</p>
<p><code>effect</code>: estimate of the target parameter(s).
</p>
<p><code>pval</code>: p-value(s) of the identification test.
</p>


<h3>References</h3>

<p>Huber, M., &amp; Kueck, J. (2022): Testing the identification of causal effects in observational data. arXiv:2203.15890.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Two examples with simulated data
## Not run: 
set.seed(777)
n &lt;- 20000  # sample size
p &lt;- 50    # number of covariates
s &lt;- 5  # sparsity (relevant covariates)
alpha &lt;- 0.1    # level

control violation of identification
delta &lt;- 2    # effect of unobservable in outcome on index of treatment - either 0 or 2
gamma &lt;- 0   # direct effect of the instrument on outcome  - either 0 or 0.1

DGP - general
xcorr &lt;- 1   # if 1, then non-zero covariance between regressors
if (xcorr == 0) {
 sigmax &lt;- diag(1,p)}       # covariate matrix at baseline
if (xcorr != 0){
 sigmax = matrix(NA,p,p)
for (i in 1:p){
   for (j in 1:p){
     sigmax[i,j] = 0.5^(abs(i-j))
   }
 }}
sparse = FALSE # if FALSE, an approximate sparse setting is considered
beta = rep(0,p)
if (sparse == TRUE){
 for (j in 1:s){ beta[j] &lt;- 1} }
if (sparse != TRUE){
 for (j in 1:p) beta[j] &lt;- (1/j)}
noise_U &lt;- 0.1 # control signal-to-noise
noise_V &lt;- 0.1
noise_W &lt;- 0.25
x &lt;- (rmvnorm(n,rep(0,p),sigmax))
w &lt;- rnorm(n,0,sd=noise_W)
z &lt;- 1*(rnorm(n)&gt;0)
d &lt;- (x%*%beta+z+w+rnorm(n,0,sd=noise_V)&gt;0)*1         # treatment equation

DGP 1 - effect homogeneity

y &lt;- x%*%beta+d+gamma*z+delta*w+rnorm(n,0,sd=noise_U)

output1 &lt;- identificationDML(y = y, d=d, x=x, z=z, score = "DR", bootstrap = FALSE,
ztreat = 1, zcontrol = 0 , seed = 123, MLmethod ="lasso", k = 3,
DR_parameters = list(s = NULL , normalized = TRUE, trim = 0.01))
output1$pval
output2 &lt;- identificationDML(y=y, d=d, x=x, z=z, score = "squared", bootstrap = FALSE,
ztreat = 1, zcontrol =0 , seed = 123, MLmethod ="lasso", k = 3)
output2$pval
output3 &lt;- identificationDML(y=y, d=d, x=x, z=z, score = "squared", bootstrap = TRUE,
ztreat = 1, zcontrol =0 , seed = 123, MLmethod ="lasso", k = 3,
DR_parameters = list(s = NULL , normalized = TRUE, trim = 0.005),
bootstrap_parameters = list(B = 2000, importance = 0.95, alpha = 0.1, share = 0.5))
output3$pval

DGP 2 - effect heterogeneity

y = x%*%beta+d+gamma*z*x[,1]+gamma*z*x[,2]+delta*w*x[,1]+delta*w*x[,2]+rnorm(n/2,0,sd=noise_U)

output1 &lt;- identificationDML(y = y, d=d, x=x, z=z, score = "DR", bootstrap = FALSE,
ztreat = 1, zcontrol = 0 , seed = 123, MLmethod ="lasso", k = 3,
DR_parameters = list(s = NULL , normalized = TRUE, trim = 0.01))
output1$pval
output2 &lt;- identificationDML(y=y, d=d, x=x, z=z, score = "squared", bootstrap = FALSE,
ztreat = 1, zcontrol =0 , seed = 123, MLmethod ="lasso", k = 3)
output2$pval
output3 &lt;- identificationDML(y=y, d=d, x=x, z=z, score = "DR", bootstrap = TRUE,
ztreat = 1, zcontrol =0 , seed = 123, MLmethod ="lasso", k = 3,
DR_parameters = list(s = NULL , normalized = TRUE, trim = 0.005),
bootstrap_parameters = list(B = 2000, importance = 0.95, alpha = 0.1, share = 0.5))
output3$pval

## End(Not run)
</code></pre>

<hr>
<h2 id='india'>India's National Health Insurance Program (RSBY)</h2><span id='topic+india'></span>

<h3>Description</h3>

<p>A dataset which is a subset of the data from the randomized evaluation of the India's National
Health Insurance Program (RSBY).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>india
</code></pre>


<h3>Format</h3>

<p>A data frame with 11'089 rows and 7 variables:
</p>

<dl>
<dt>X</dt><dd><p>individual identification</p>
</dd>
<dt>village_id</dt><dd><p>village identification</p>
</dd>
<dt>DistrictId</dt><dd><p>district identification</p>
</dd>
<dt>treat</dt><dd><p>treatment status (insurance)</p>
</dd>
<dt>mech</dt><dd><p>treatment assignment mechanism</p>
</dd>
<dt>enrolled</dt><dd><p>enrolled</p>
</dd>
<dt>EXPhosp_1</dt><dd><p>hospital expenditure</p>
</dd>
</dl>



<h3>References</h3>

<p>Imai, Kosuke; Jiang, Zhichao; Malani, Anup, 2020, &quot;Replication Data for: Causal Inference with Interference and Noncompliance in Two-Stage Randomized Experiments.&quot;, https://doi.org/10.7910/DVN/N7D9LS, Harvard Dataverse, V1
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
require(devtools)                        # load devtools package
install_github("szonszein/interference") # install interference package
library(interference)                    # load interference package
data(india)                              # load data
attach(india)                            # attach data
india=na.omit(india)                     # drop observations with missings
group=india$village_id                   # cluster id
group_tr=india$mech                      # indicator high treatment proportion
indiv_tr=india$treat                     # individual treatment (insurance)
obs_outcome=india$EXPhosp_1              # outcome  (hospital expenditure)
dat=data.frame(group,group_tr,indiv_tr,obs_outcome) # generate data frame
estimates_hierarchical(dat)
## End(Not run)             # run estimation
</code></pre>

<hr>
<h2 id='ivnr'>Instrument-based treatment evaluation under endogeneity and non-response bias</h2><span id='topic+ivnr'></span>

<h3>Description</h3>

<p>Non- and semiparaemtric treatment effect estimation under treatment endogeneity and selective non-response in the outcome based on a binary instrument for the treatment and a continous instrument for response.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ivnr(
  y,
  d,
  r,
  z1,
  z2,
  x = NULL,
  xpar = NULL,
  ruleofthumb = 1,
  wgtfct = 2,
  rtype = "ll",
  numresprob = 20,
  boot = 499,
  estlate = TRUE,
  trim = 0.01
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ivnr_+3A_y">y</code></td>
<td>
<p>Dependent variable.</p>
</td></tr>
<tr><td><code id="ivnr_+3A_d">d</code></td>
<td>
<p>Treatment, must be binary and must not contain missings.</p>
</td></tr>
<tr><td><code id="ivnr_+3A_r">r</code></td>
<td>
<p>Response, must be a binary indicator for whether the outcome is observed.</p>
</td></tr>
<tr><td><code id="ivnr_+3A_z1">z1</code></td>
<td>
<p>Binary instrument for the treatment, must not contain missings.</p>
</td></tr>
<tr><td><code id="ivnr_+3A_z2">z2</code></td>
<td>
<p>Continuous instrument for response, must not contain missings.</p>
</td></tr>
<tr><td><code id="ivnr_+3A_x">x</code></td>
<td>
<p>A data frame of covariates to be included in the nonparametric estimation, must not contain missings. Factors and ordered varaibles must be appropriately defined as such by <code>factor()</code> and <code>ordered()</code>. Default is <code>NULL</code> (no covariates included). Covariates are only considered if both <code>x</code> and <code>xpar</code> are not <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="ivnr_+3A_xpar">xpar</code></td>
<td>
<p>Covariates  to be included in the semiparametric estimation, must not contain missings. Default is <code>NULL</code> (no covariates included). Covariates are only considered if both <code>x</code> and <code>xpar</code> are not <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="ivnr_+3A_ruleofthumb">ruleofthumb</code></td>
<td>
<p>If 1, bandwidth selection in any kernel function is based on the Silverman (1986) rule of thumb. Otherwise, least squares cross-validation is used. Default is 1.</p>
</td></tr>
<tr><td><code id="ivnr_+3A_wgtfct">wgtfct</code></td>
<td>
<p>Weighting function to be used in effect estimation. If set to 1, equation (18) in Fricke et al (2020) is used as weight. If set to 2, equation (19) in Fricke et al (2020) is used as weight. If set to 3, the median of LATEs across values of response probabilities <code>numresprob</code> is used. Default is 2.</p>
</td></tr>
<tr><td><code id="ivnr_+3A_rtype">rtype</code></td>
<td>
<p>Regression type used for continuous outcomes in the kernel regressions. Either <code>"ll"</code> for local linear or <code>"lc"</code> for local constant regression. Default is <code>"ll"</code>.</p>
</td></tr>
<tr><td><code id="ivnr_+3A_numresprob">numresprob</code></td>
<td>
<p>number of response probabilities at which the effects are evaluated. An equidistant grid is constructed based on the number provided. Default is 20.</p>
</td></tr>
<tr><td><code id="ivnr_+3A_boot">boot</code></td>
<td>
<p>Number of bootstrap replications for estimating standard errors of the effects. Default is 499.</p>
</td></tr>
<tr><td><code id="ivnr_+3A_estlate">estlate</code></td>
<td>
<p>If set to <code>TRUE</code> the local average treatment effect on compliers (LATE) is estimated, otherwise the average treatment effect (ATE) is estimated. Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="ivnr_+3A_trim">trim</code></td>
<td>
<p>Trimming rule for too extreme denominators in the weighting functions or inverses of products of conditional treatment probabilities. Values below <code>trim</code> are set to <code>trim</code> to avoid values that are too close to zero in any denominator. Default is 0.01.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Non- and semiparametric treatment effect estimation under treatment endogeneity and selective non-response in the outcome based on a binary instrument for the treatment and a continuous instrument for response. The effects are estimated both semi-parametrically (using probit and OLS for the estimation of plug-in parameters like conditional probabilities and outcomes) and fully non-parametrically (based on kernel regression for any conditional probability/mean). Besides the instrument-based estimates, results are also presented under a missing-at-random assumption (MAR) when not using the instrument <code>z2</code> for response (but only <code>z1</code> for the treatment). See Fricke et al. (2020) for further details.
</p>


<h3>Value</h3>

<p>A <code>ivnr</code> object contains one output component:
</p>
<p><code>output</code>: The first row provides the effect estimates under non- and semi-parametric estimation using both instruments, see <code>"nonpara (L)ATE IV"</code> and <code>"semipara (L)ATE IV"</code> as well as under a missing-at-random assumption for response when using only the first instrument for the treatment, see <code>"nonpara (L)ATE MAR"</code> and <code>"semipara (L)ATE MAR"</code>. The second row provides the standard errors based on bootstrapping the effects. The third row provides the p-values based on the t-statistics.
</p>


<h3>References</h3>

<p>Fricke, H., Fr√∂lich, M., Huber, M., Lechner, M. (2020): &quot;Endogeneity and non-response bias in treatment evaluation - nonparametric identification of causal effects by instruments&quot;, Journal of Applied Econometrics, forthcoming.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># A little example with simulated data (1000 observations)
## Not run: 
n=1000          # sample size
e&lt;-(rmvnorm(n,rep(0,3), matrix(c(1,0.5,0.5,  0.5,1,0.5,  0.5,0.5,1),3,3)))
# correlated error term of treatment, response, and outcome equation
x=runif(n,-0.5,0.5)           # observed confounder
z1&lt;-(-0.25*x+rnorm(n)&gt;0)*1    # binary instrument for treatment
z2&lt;- -0.25*x+rnorm(n)         # continuous instrument for selection
d&lt;-(z1-0.25*x+e[,1]&gt;0)*1      # treatment equation
 y_star&lt;- -0.25*x+d+e[,2]     # latent outcome
 r&lt;-(-0.25*x+z2+d+e[,3]&gt;0)*1  # response equation
 y=y_star                     # observed outcome
 y[r==0]=0                    # nonobserved outcomes are set to zero
 # The true treatment effect is 1
 ivnr(y=y,d=d,r=r,z1=z1,z2=z2,x=x,xpar=x,numresprob=4,boot=39)
## End(Not run)
</code></pre>

<hr>
<h2 id='JC'>Job Corps data</h2><span id='topic+JC'></span>

<h3>Description</h3>

<p>A dataset from the U.S. Job Corps experimental study with information on the participation of disadvantaged youths in (academic and vocational) training in the first and second year after program assignment.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>JC
</code></pre>


<h3>Format</h3>

<p>A data frame with 9240 rows and 46 variables:
</p>

<dl>
<dt>assignment</dt><dd><p>1=randomly assigned to Job Corps, 0=randomized out of Job Corps</p>
</dd>
<dt>female</dt><dd><p>1=female, 0=male</p>
</dd>
<dt>age</dt><dd><p>age in years at assignment</p>
</dd>
<dt>white</dt><dd><p>1=white, 0=non-white</p>
</dd>
<dt>black</dt><dd><p>1=black, 0=non-black</p>
</dd>
<dt>hispanic</dt><dd><p>1=hispanic, 0=non-hispanic</p>
</dd>
<dt>educ</dt><dd><p>years of education at assignment</p>
</dd>
<dt>educmis</dt><dd><p>1=education missing at assignment</p>
</dd>
<dt>geddegree</dt><dd><p>1=has a GED degree at assignment</p>
</dd>
<dt>hsdegree</dt><dd><p>1=has a high school degree at assignment</p>
</dd>
<dt>english</dt><dd><p>1=English mother tongue</p>
</dd>
<dt>cohabmarried</dt><dd><p>1=cohabiting or married at assignment</p>
</dd>
<dt>haschild</dt><dd><p>1=has at least one child, 0=no children at assignment</p>
</dd>
<dt>everwkd</dt><dd><p>1=has ever worked at assignment, 0=has never worked at assignment</p>
</dd>
<dt>mwearn</dt><dd><p>average weekly gross earnings at assignment</p>
</dd>
<dt>hhsize</dt><dd><p>household size at assignment</p>
</dd>
<dt>hhsizemis</dt><dd><p>1=household size missing</p>
</dd>
<dt>educmum</dt><dd><p>mother's years of education at assignment</p>
</dd>
<dt>educmummis</dt><dd><p>1=mother's years of education missing</p>
</dd>
<dt>educdad</dt><dd><p>father's years of education at assignment</p>
</dd>
<dt>educdadmis</dt><dd><p>1=father's years of education missing</p>
</dd>
<dt>welfarechild</dt><dd><p>welfare receipt during childhood in categories from 1 to 4 (measured at assignment)</p>
</dd>
<dt>welfarechildmis</dt><dd><p>1=missing welfare receipt during childhood</p>
</dd>
<dt>health</dt><dd><p>general health at assignment from 1 (excellent) to 4 (poor)</p>
</dd>
<dt>healthmis</dt><dd><p>1=missing health at assignment</p>
</dd>
<dt>smoke</dt><dd><p>extent of smoking at assignment in categories from 1 to 4</p>
</dd>
<dt>smokemis</dt><dd><p>1=extent of smoking missing</p>
</dd>
<dt>alcohol</dt><dd><p>extent of alcohol consumption at assignment in categories from 1 to 4</p>
</dd>
<dt>alcoholmis</dt><dd><p>1=extent of alcohol consumption missing</p>
</dd>
<dt>everwkdy1</dt><dd><p>1=has ever worked one year after assignment, 0=has never worked one year after assignment</p>
</dd>
<dt>earnq4</dt><dd><p>weekly earnings in fourth quarter after assignment</p>
</dd>
<dt>earnq4mis</dt><dd><p>1=missing weekly earnings in fourth quarter after assignment</p>
</dd>
<dt>pworky1</dt><dd><p>proportion of weeks employed in first year after assignment</p>
</dd>
<dt>pworky1mis</dt><dd><p>1=missing proportion of weeks employed in first year after assignment</p>
</dd>
<dt>health12</dt><dd><p>general health 12 months after assignment from 1 (excellent) to 4 (poor)</p>
</dd>
<dt>health12mis</dt><dd><p>1=missing general health 12 months after assignment</p>
</dd>
<dt>trainy1</dt><dd><p>1=enrolled in education and/or vocational training in the first year after assignment, 0=no education or training in the first year after assignment</p>
</dd>
<dt>trainy2</dt><dd><p>1=enrolled in education and/or vocational training in the second year after assignment, 0=no education or training in the second year after assignment</p>
</dd>
<dt>pworky2</dt><dd><p>proportion of weeks employed in second year after assignment</p>
</dd>
<dt>pworky3</dt><dd><p>proportion of weeks employed in third year after assignment</p>
</dd>
<dt>pworky4</dt><dd><p>proportion of weeks employed in fourth year after assignment</p>
</dd>
<dt>earny2</dt><dd><p>weekly earnings in second year after assignment</p>
</dd>
<dt>earny3</dt><dd><p>weekly earnings in third year after assignment</p>
</dd>
<dt>earny4</dt><dd><p>weekly earnings in fourth year after assignment</p>
</dd>
<dt>health30</dt><dd><p>general health 30 months after assignment from 1 (excellent) to 4 (poor)</p>
</dd>
<dt>health48</dt><dd><p>general health 48 months after assignment from 1 (excellent) to 4 (poor)</p>
</dd>
</dl>



<h3>References</h3>

<p>Schochet, P. Z., Burghardt, J., Glazerman, S. (2001): &quot;National Job Corps study: The impacts of Job Corps on participants' employment and related outcomes&quot;, Mathematica Policy Research, Washington, DC.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data(JC)
# Dynamic treatment effect evaluation of training in 1st and 2nd year
# define covariates at assignment (x0) and after one year (x1)
x0=JC[,2:29]; x1=JC[,30:36]
# define treatment (training) in first year (d1) and second year (d2)
d1=JC[,37]; d2=JC[,38]
# define outcome (weekly earnings in fourth year after assignment)
y2=JC[,44]
# assess dynamic treatment effects (training in 1st+2nd year vs. no training)
output=dyntreatDML(y2=y2, d1=d1, d2=d2, x0=x0, x1=x1)
cat("dynamic ATE: ",round(c(output$effect),3),", standard error: ",
    round(c(output$se),3), ", p-value: ",round(c(output$pval),3))
## End(Not run)
</code></pre>

<hr>
<h2 id='lateweight'>Local average treatment effect estimation based on inverse probability weighting</h2><span id='topic+lateweight'></span>

<h3>Description</h3>

<p>Instrumental variable-based evaluation of local average treatment effects using weighting by the inverse of the instrument propensity score.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lateweight(
  y,
  d,
  z,
  x,
  LATT = FALSE,
  trim = 0.05,
  logit = FALSE,
  boot = 1999,
  cluster = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lateweight_+3A_y">y</code></td>
<td>
<p>Dependent variable, must not contain missings.</p>
</td></tr>
<tr><td><code id="lateweight_+3A_d">d</code></td>
<td>
<p>Treatment, must be binary (either 1 or 0), must not contain missings.</p>
</td></tr>
<tr><td><code id="lateweight_+3A_z">z</code></td>
<td>
<p>Instrument for the endogenous treatment, must be binary (either 1 or 0), must not contain missings.</p>
</td></tr>
<tr><td><code id="lateweight_+3A_x">x</code></td>
<td>
<p>Confounders of the instrument and outcome, must not contain missings.</p>
</td></tr>
<tr><td><code id="lateweight_+3A_latt">LATT</code></td>
<td>
<p>If FALSE, the local average treatment effect (LATE) among compliers (whose treatment reacts to the instrument) is estimated. If TRUE, the local average treatment effect on the treated compliers (LATT)  is estimated. Default is FALSE.</p>
</td></tr>
<tr><td><code id="lateweight_+3A_trim">trim</code></td>
<td>
<p>Trimming rule for discarding observations with extreme propensity scores. If <code>LATT=FALSE</code>, observations with Pr(Z=1|X)&lt;<code>trim</code> or Pr(Z=1|X)&gt;(1-<code>trim</code>) are dropped.
If <code>LATT=TRUE</code>, observations with Pr(Z=1|X)&gt;(1-<code>trim</code>) are dropped. Default is 0.05.</p>
</td></tr>
<tr><td><code id="lateweight_+3A_logit">logit</code></td>
<td>
<p>If FALSE, probit regression is used for propensity score estimation. If TRUE, logit regression is used. Default is FALSE.</p>
</td></tr>
<tr><td><code id="lateweight_+3A_boot">boot</code></td>
<td>
<p>Number of bootstrap replications for estimating standard errors. Default is 1999.</p>
</td></tr>
<tr><td><code id="lateweight_+3A_cluster">cluster</code></td>
<td>
<p>A cluster ID for block or cluster bootstrapping when units are clustered rather than iid. Must be numerical. Default is NULL (standard bootstrap without clustering).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Estimation of local average treatment effects of a binary endogenous treatment based on a binary instrument that is conditionally valid, implying that all confounders of the instrument and the outcome are observed. Units are weighted by the inverse of their conditional instrument propensities given the observed confounders, which are estimated by probit or logit regression. Standard errors are obtained by bootstrapping the effect.
</p>


<h3>Value</h3>

<p>A lateweight object contains 10 components, <code>effect</code>, <code>se.effect</code>, <code>pval.effect</code>, <code>first</code>, <code>se.first</code>, <code>pval.first</code>, <code>ITT</code>, <code>se.ITT</code>, <code>pval.ITT</code>, and <code>ntrimmed</code>:
</p>
<p><code>effect</code>: local average treatment effect (LATE) among compliers if <code>LATT=FALSE</code> or the local average treatment effect on treated compliers (LATT) if <code>LATT=TRUE</code>.
</p>
<p><code>se.effect</code>: bootstrap-based standard error of the effect.
</p>
<p><code>pval.effect</code>: p-value of the effect.
</p>
<p><code>first</code>: first stage estimate of the complier share if <code>LATT=FALSE</code> or the first stage estimate among treated if <code>LATT=TRUE</code>.
</p>
<p><code>se.first</code>: bootstrap-based standard error of the first stage effect.
</p>
<p><code>pval.first</code>: p-value of the first stage effect.
</p>
<p><code>ITT</code>: intention to treat effect (ITT) of <code>z</code> on <code>y</code> if <code>LATT=FALSE</code> or the ITT among treated if <code>LATT=TRUE</code>.
</p>
<p><code>se.ITT</code>: bootstrap-based standard error of the ITT.
</p>
<p><code>pval.ITT</code>: p-value of the ITT.
</p>
<p><code>ntrimmed</code>: number of discarded (trimmed) observations due to extreme propensity score values.
</p>


<h3>References</h3>

<p>Fr√∂lich, M. (2007): &quot;Nonparametric IV estimation of local average treatment effects with covariates&quot;, Journal of Econometrics, 139, 35-75.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># A little example with simulated data (10000 observations)
## Not run: 
n=10000
u=rnorm(n)
x=rnorm(n)
z=(0.25*x+rnorm(n)&gt;0)*1
d=(z+0.25*x+0.25*u+rnorm(n)&gt;0.5)*1
y=0.5*d+0.25*x+u
# The true LATE is equal to 0.5
output=lateweight(y=y,d=d,z=z,x=x,trim=0.05,LATT=FALSE,logit=TRUE,boot=19)
cat("LATE: ",round(c(output$effect),3),", standard error: ",
             round(c(output$se.effect),3), ", p-value: ",
             round(c(output$pval.effect),3))
output$ntrimmed
## End(Not run)
</code></pre>

<hr>
<h2 id='medDML'>Causal mediation analysis with double machine learning</h2><span id='topic+medDML'></span>

<h3>Description</h3>

<p>Causal mediation analysis (evaluation of natural direct and indirect effects) for a binary treatment and one or several mediators using double machine learning to control for confounders based on (doubly robust) efficient score functions for potential outcomes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>medDML(
  y,
  d,
  m,
  x,
  k = 3,
  trim = 0.05,
  order = 1,
  multmed = TRUE,
  fewsplits = FALSE,
  normalized = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="medDML_+3A_y">y</code></td>
<td>
<p>Dependent variable, must not contain missings.</p>
</td></tr>
<tr><td><code id="medDML_+3A_d">d</code></td>
<td>
<p>Treatment, must be binary (either 1 or 0), must not contain missings.</p>
</td></tr>
<tr><td><code id="medDML_+3A_m">m</code></td>
<td>
<p>Mediator, must not contain missings. May be a scalar or a vector of binary, categorical, or continuous variables if <code>multmed</code> is <code>TRUE</code>. Must be a binary scalar if  <code>multmed</code> is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="medDML_+3A_x">x</code></td>
<td>
<p>(Potential) pre-treatment confounders of the treatment, mediator, and/or outcome, must not contain missings.</p>
</td></tr>
<tr><td><code id="medDML_+3A_k">k</code></td>
<td>
<p>Number of folds in k-fold cross-fitting if <code>multmed</code> is <code>FALSE</code>. <code>k</code>-1 folds are used for estimating the model parameters of the treatment, mediator, and outcome equations and one fold is used for predicting the efficient score functions. The roles of the folds are swapped. Default for <code>k</code> is 3. If <code>multmed</code> is <code>TRUE</code>, then 3-fold cross-valdiation is used, irrespective of the number provided in <code>k</code> (i.e. <code>k</code> is ignored if <code>multmed</code> is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="medDML_+3A_trim">trim</code></td>
<td>
<p>Trimming rule for discarding observations with extreme conditional treatment or mediator probabilities (or products thereof). Observations with (products of) conditional probabilities that are smaller than <code>trim</code> in any denominator of the potential outcomes are dropped. Default is 0.05.</p>
</td></tr>
<tr><td><code id="medDML_+3A_order">order</code></td>
<td>
<p>If set to an integer larger than 1, then polynomials of that order and interactions (using the power series) rather than the original control variables are used in the estimation of any conditional probability or conditional mean outcome. Polynomials/interactions are created using the <code>Generate.Powers</code> command of the <code>LARF</code> package.</p>
</td></tr>
<tr><td><code id="medDML_+3A_multmed">multmed</code></td>
<td>
<p>If set to <code>TRUE</code>, a representation of direct and indirect effects that avoids conditional mediator densities/probabilities is used, see Farbmacher, Huber, Langen, and Spindler (2019). This method can incorporate multiple and/or continuous mediators. If <code>multmed</code> is <code>FALSE</code>, the representation of Tchetgen Tchetgen and Shpitser (2012) is used, which involves mediator densities. In this case, the mediator must be a binary scalar. Default of <code>multimed</code> is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="medDML_+3A_fewsplits">fewsplits</code></td>
<td>
<p>If set to <code>TRUE</code>, the same training data are used for estimating nested models of nuisance parameters, i.e. <code>E[Y|D=d,M,X]</code> and <code>E[E[Y|D=d,M,X]|D=1-d,X]</code>. If <code>fewsplits</code> is <code>FALSE</code>, the training data are split for the sequential estimation of nested models <code>E[Y|D=d,M,X]</code> and <code>E[E[Y|D=d,M,X]|D=1-d,X]</code>. This parameter is only relevant if <code>multmed</code> is <code>TRUE</code>. Default of <code>fewsplits</code> is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="medDML_+3A_normalized">normalized</code></td>
<td>
<p>If set to <code>TRUE</code>, then the inverse probability-based weights are normalized such that they add up to 1 within treatment groups. Default is <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Estimation of causal mechanisms (natural direct and indirect effects) of a treatment under selection on observables, assuming that all confounders of the binary treatment and the mediator, the treatment and the outcome, or the mediator and the outcome are observed and not affected by the treatment. Estimation is based on the (doubly robust) efficient score functions for potential outcomes, see Tchetgen Tchetgen and Shpitser (2012) and Farbmacher, Huber, Langen, and Spindler (2019),
as well as on double machine learning with cross-fitting, see Chernozhukov et al (2018). To this end, one part of the data is used for estimating the model parameters of the treatment, mediator, and outcome equations based on post-lasso regression, using the <code>rlasso</code> and <code>rlassologit</code> functions (for conditional means and probabilities, respectively) of the <code>hdm</code> package with default settings. The other part of the data is used for predicting the efficient score functions. The roles of the data parts are swapped and the direct and indirect effects are estimated based on averaging the predicted efficient score functions in the total sample.
Standard errors are based on asymptotic approximations using the estimated variance of the (estimated) efficient score functions.
</p>


<h3>Value</h3>

<p>A medDML object contains two components, <code>results</code> and <code>ntrimmed</code>:
</p>
<p><code>results</code>: a 3X6 matrix containing the effect estimates in the first row (&quot;effects&quot;), standard errors in the second row (&quot;se&quot;), and p-values in the third row (&quot;p-value&quot;).
The first column provides the total effect, namely the average treatment effect (ATE). The second and third columns provide the direct effects under treatment and control, respectively (&quot;dir.treat&quot;, &quot;dir.control&quot;). The fourth and fifth columns provide the indirect effects under treatment and control, respectively (&quot;indir.treat&quot;, &quot;indir.control&quot;). The sixth column provides the estimated mean under non-treatment (&quot;Y(0,M(0))&quot;).
</p>
<p><code>ntrimmed</code>: number of discarded (trimmed) observations due to extreme conditional probabilities.
</p>


<h3>References</h3>

<p>Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., Robins, J. (2018): &quot;Double/debiased machine learning for treatment and structural parameters&quot;, The Econometrics Journal, 21, C1-C68.
</p>
<p>Farbmacher, H., Huber, M., Laffers, L., Langen, H., and Spindler, M. (2019): &quot;Causal mediation analysis with double machine learning&quot;, working paper, University of Fribourg.
</p>
<p>Tchetgen Tchetgen, E. J., and Shpitser, I. (2012): &quot;Semiparametric theory for causal mediation analysis: efficiency bounds, multiple robustness, and sensitivity analysis&quot;, The Annals of Statistics, 40, 1816-1845.
</p>
<p>Tibshirani, R. (1996): &quot;Regression shrinkage and selection via the lasso&quot;, Journal of the Royal Statistical Society: Series B, 58, 267-288.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># A little example with simulated data (10000 observations)
## Not run: 
n=10000                           # sample size
p=100                             # number of covariates
s=2                               # number of covariates that are confounders
x=matrix(rnorm(n*p),ncol=p)       # covariate matrix
beta=c(rep(0.25,s), rep(0,p-s))   # coefficients determining degree of confounding
d=(x%*%beta+rnorm(n)&gt;0)*1         # treatment equation
m=(x%*%beta+0.5*d+rnorm(n)&gt;0)*1   # mediator equation
y=x%*%beta+0.5*d+m+rnorm(n)       # outcome equation
# The true direct effects are equal to 0.5, the indirect effects equal to 0.19
output=medDML(y=y,d=d,m=m,x=x)
round(output$results,3)
output$ntrimmed
## End(Not run)
</code></pre>

<hr>
<h2 id='medlateweight'>Causal mediation analysis with instruments for treatment and mediator based on weighting</h2><span id='topic+medlateweight'></span>

<h3>Description</h3>

<p>Causal mediation analysis (evaluation of natural direct and indirect effects) with instruments for a binary treatment and a continuous mediator based on weighting as suggested in Fr√∂lich and Huber (2017),  Theorem 1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>medlateweight(
  y,
  d,
  m,
  zd,
  zm,
  x,
  trim = 0.1,
  csquared = FALSE,
  boot = 1999,
  cminobs = 40,
  bwreg = NULL,
  bwm = NULL,
  logit = FALSE,
  cluster = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="medlateweight_+3A_y">y</code></td>
<td>
<p>Dependent variable, must not contain missings.</p>
</td></tr>
<tr><td><code id="medlateweight_+3A_d">d</code></td>
<td>
<p>Treatment, must be binary (either 1 or 0), must not contain missings.</p>
</td></tr>
<tr><td><code id="medlateweight_+3A_m">m</code></td>
<td>
<p>Mediator(s),must be a continuous scalar, must not contain missings.</p>
</td></tr>
<tr><td><code id="medlateweight_+3A_zd">zd</code></td>
<td>
<p>Instrument for the treatment,  must be binary (either 1 or 0), must not contain missings.</p>
</td></tr>
<tr><td><code id="medlateweight_+3A_zm">zm</code></td>
<td>
<p>Instrument for the mediator, must contain at least one continuous element, may be a scalar or a vector, must not contain missings. If no user-specified bandwidth is provided for the regressors when estimating the conditional cumulative distribution function F(M|Z2,X), i.e. if <code>bwreg=NULL</code>, then <code>zm</code> must be exclusively numeric.</p>
</td></tr>
<tr><td><code id="medlateweight_+3A_x">x</code></td>
<td>
<p>Pre-treatment confounders, may be a scalar or a vector, must not contain missings. If no user-specified bandwidth is provided for the regressors when estimating the conditional cumulative distribution function F(M|Z2,X), i.e. if <code>bwreg=NULL</code>, then <code>x</code> must be exclusively numeric.</p>
</td></tr>
<tr><td><code id="medlateweight_+3A_trim">trim</code></td>
<td>
<p>Trimming rule for discarding observations with extreme weights. Discards observations whose relative weight would exceed the value in <code>trim</code> in the estimation of any of the potential outcomes. Default is 0.1 (i.e. a maximum weight of 10 percent per observation).</p>
</td></tr>
<tr><td><code id="medlateweight_+3A_csquared">csquared</code></td>
<td>
<p>If TRUE, then not only the control function C, but also its square is used as regressor in any estimated function that conditions on C. Default is FALSE.</p>
</td></tr>
<tr><td><code id="medlateweight_+3A_boot">boot</code></td>
<td>
<p>Number of bootstrap replications for estimating standard errors. Default is 1999.</p>
</td></tr>
<tr><td><code id="medlateweight_+3A_cminobs">cminobs</code></td>
<td>
<p>Minimum number of observations to compute the control function C, see the numerator of equation (7) in Fr√∂lich and Huber (2017). A larger value increases boundary bias when estimating the control function for lower values of M, but reduces the variance. Default is 40, but should be adapted to sample size and the number of variables in Z2 and X.</p>
</td></tr>
<tr><td><code id="medlateweight_+3A_bwreg">bwreg</code></td>
<td>
<p>Bandwidths for <code>zm</code> and <code>x</code> in the estimation of the conditional cumulative distribution function F(M|Z2,X) based on the np package by Hayfield and Racine (2008). The length of the numeric vector must correspond to the joint number of elements in <code>zm</code> and <code>x</code> and will be used both in the original sample for effect estimation and in bootstrap samples to compute standard errors. If set to <code>NULL</code>, then the rule of thumb is used for bandwidth calculation, see the np package for details. In the latter case, all elements in the regressors must be numeric. Default is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="medlateweight_+3A_bwm">bwm</code></td>
<td>
<p>Bandwidth for <code>m</code> in the estimation of the conditional cumulative distribution function F(M|Z2,X) based on the np package by Hayfield and Racine (2008). Must be scalar and will be used both in the original sample for effect estimation and in bootstrap samples to compute standard errors. If set to <code>NULL</code>, then the rule of thumb is used for bandwidth calculation, see the np package for details. Default is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="medlateweight_+3A_logit">logit</code></td>
<td>
<p>If FALSE, probit regression is used for any propensity score estimation. If TRUE, logit regression is used. Default is FALSE.</p>
</td></tr>
<tr><td><code id="medlateweight_+3A_cluster">cluster</code></td>
<td>
<p>A cluster ID for block or cluster bootstrapping when units are clustered rather than iid. Must be numerical. Default is NULL (standard bootstrap without clustering).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Estimation of causal mechanisms (natural direct and indirect effects) of a binary treatment among treatment compliers based on distinct instruments for the treatment and the mediator. The treatment and its instrument are assumed to be binary, while the mediator and its instrument are assumed to be continuous, see Theorem 1 in Fr√∂lich and Huber (2017). The instruments are assumed to be conditionally valid given a set of observed confounders. A control function is used to tackle mediator endogeneity. Standard errors are obtained by bootstrapping the effects.
</p>


<h3>Value</h3>

<p>A medlateweight object contains two components, <code>results</code> and <code>ntrimmed</code>:
</p>
<p><code>results</code>: a 3x7 matrix containing the effect estimates in the first row (&quot;effects&quot;), standard errors in the second row (&quot;se&quot;), and p-values in the third row (&quot;p-value&quot;).
The first column provides the total effect, namely the local average treatment effect (LATE) on the compliers.
The second and third columns provide the direct effects under treatment and control, respectively (&quot;dir.treat&quot;, &quot;dir.control&quot;).
The fourth and fifth columns provide the indirect effects under treatment and control, respectively (&quot;indir.treat&quot;, &quot;indir.control&quot;).
The sixth and seventh columns provide the parametric direct and indirect effect estimates (&quot;dir.para&quot;, &quot;indir.para&quot;) without intercation terms, respectively. For the parametric estimates, probit or logit specifications are used for the treatment model and OLS specifications for the mediator and outcome models.
</p>
<p><code>ntrimmed</code>: number of discarded (trimmed) observations due to large weights.
</p>


<h3>References</h3>

<p>Fr√∂lich, M. and Huber, M. (2017): &quot;Direct and indirect treatment effects: Causal chains and mediation analysis with instrumental variables&quot;, Journal of the Royal Statistical Society Series B, 79, 1645‚Äì1666.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># A little example with simulated data (3000 observations)
## Not run: 
n=3000; sigma=matrix(c(1,0.5,0.5,0.5,1,0.5,0.5,0.5,1),3,3)
e=(rmvnorm(n,rep(0,3),sigma))
x=rnorm(n)
zd=(0.5*x+rnorm(n)&gt;0)*1
d=(-1+0.5*x+2*zd+e[,3]&gt;0)
zm=0.5*x+rnorm(n)
m=(0.5*x+2*zm+0.5*d+e[,2])
y=0.5*x+d+m+e[,1]
# The true direct and indirect effects on compliers are equal to 1 and 0.5, respectively
medlateweight(y,d,m,zd,zm,x,trim=0.1,csquared=FALSE,boot=19,cminobs=40,
bwreg=NULL,bwm=NULL,logit=FALSE)
## End(Not run)
</code></pre>

<hr>
<h2 id='medweight'>Causal mediation analysis based on inverse probability weighting with optional sample selection correction.</h2><span id='topic+medweight'></span>

<h3>Description</h3>

<p>Causal mediation analysis (evaluation of natural direct and indirect effects) based on weighting by the inverse of treatment propensity scores as suggested in Huber (2014) and Huber and Solovyeva (2018).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>medweight(
  y,
  d,
  m,
  x,
  w = NULL,
  s = NULL,
  z = NULL,
  selpop = FALSE,
  ATET = FALSE,
  trim = 0.05,
  logit = FALSE,
  boot = 1999,
  cluster = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="medweight_+3A_y">y</code></td>
<td>
<p>Dependent variable, must not contain missings.</p>
</td></tr>
<tr><td><code id="medweight_+3A_d">d</code></td>
<td>
<p>Treatment, must be binary (either 1 or 0), must not contain missings.</p>
</td></tr>
<tr><td><code id="medweight_+3A_m">m</code></td>
<td>
<p>Mediator(s), may be a scalar or a vector, must not contain missings.</p>
</td></tr>
<tr><td><code id="medweight_+3A_x">x</code></td>
<td>
<p>Pre-treatment confounders of the treatment, mediator, and/or outcome, must not contain missings.</p>
</td></tr>
<tr><td><code id="medweight_+3A_w">w</code></td>
<td>
<p>Post-treatment confounders of the mediator and the outcome. Default is <code>NULL</code>. Must not contain missings.</p>
</td></tr>
<tr><td><code id="medweight_+3A_s">s</code></td>
<td>
<p>Optional selection indicator. Must be one if <code>y</code> is observed (non-missing) and zero if <code>y</code> is not observed (missing). Default is <code>NULL</code>, implying that <code>y</code> does not contain any missings. Is ignored if <code>w</code> is not <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="medweight_+3A_z">z</code></td>
<td>
<p>Optional instrumental variable(s) for selection <code>s</code>. If <code>NULL</code>, outcome selection based on observables (<code>x</code>,<code>d</code>,<code>m</code>) - known as &quot;missing at random&quot; - is assumed.</p>
</td></tr>
<tr><td><code id="medweight_+3A_selpop">selpop</code></td>
<td>
<p>Only to be used if both <code>s</code> and <code>z</code> are defined. If <code>TRUE</code>, the effects are estimated for the selected subpopulation with <code>s</code>=1 only. If <code>FALSE</code>, the effects are estimated for the total population.</p>
</td></tr>
<tr><td><code id="medweight_+3A_atet">ATET</code></td>
<td>
<p>If FALSE, the average treatment effect (ATE) and the corresponding direct and indirect effects are estimated. If TRUE, the average treatment effect on the treated (ATET)  and the corresponding direct and indirect effects are estimated. Default is FALSE.</p>
</td></tr>
<tr><td><code id="medweight_+3A_trim">trim</code></td>
<td>
<p>Trimming rule for discarding observations with extreme propensity scores. In the absence of post-treatment confounders (w=NULL), observations with Pr(D=1|M,X)&lt;<code>trim</code> or Pr(D=1|M,X)&gt;(1-<code>trim</code>) are dropped.
In the presence of post-treatment confounders (<code>w</code> is defined), observations with Pr(D=1|M,W,X)&lt;<code>trim</code> or Pr(D=1|M,W,X)&gt;(1-<code>trim</code>) are dropped. Default is 0.05. If <code>s</code> is defined (only considered if <code>w</code> is <code>NULL</code>!) and <code>z</code> is <code>NULL</code>, observations with low selection propensity scores, Pr(S=1|D,M,X)&lt;<code>trim</code>, are discarded, too. If <code>s</code> and <code>z</code> are defined, the treatment propensity scores to be trimmed change to Pr(D=1|M,X,Pr(S=1|D,X,Z)).</p>
</td></tr>
<tr><td><code id="medweight_+3A_logit">logit</code></td>
<td>
<p>If FALSE, probit regression is used for propensity score estimation. If TRUE, logit regression is used. Default is FALSE.</p>
</td></tr>
<tr><td><code id="medweight_+3A_boot">boot</code></td>
<td>
<p>Number of bootstrap replications for estimating standard errors. Default is 1999.</p>
</td></tr>
<tr><td><code id="medweight_+3A_cluster">cluster</code></td>
<td>
<p>A cluster ID for block or cluster bootstrapping when units are clustered rather than iid. Must be numerical. Default is NULL (standard bootstrap without clustering).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Estimation of causal mechanisms (natural direct and indirect effects) of a binary treatment under a selection on observables assumption assuming that all confounders of the treatment and the mediator, the treatment and the outcome, or the mediator and the outcome are observed. Units are weighted by the inverse of their conditional treatment propensities given the mediator and/or observed confounders, which are estimated by probit or logit regression.
The form of weighting depends on whether the observed confounders are exclusively pre-treatment (<code>x</code>), or also contain post-treatment confounders of the mediator and the outcome (<code>w</code>). In the latter case, only partial indirect effects (from  <code>d</code> to <code>m</code> to <code>y</code>) can be estimated that exclude any causal paths from <code>d</code> to <code>w</code> to <code>m</code> to <code>y</code>, see the discussion in Huber (2014). Standard errors are obtained by bootstrapping the effects.
In the absence of post-treatment confounders (such that <code>w</code> is <code>NULL</code>), defining <code>s</code> allows correcting for sample selection due to missing outcomes based on the inverse of the conditional selection probability. The latter might either be related to observables, which implies a missing at random assumption, or in addition also to unobservables, if an instrument for sample selection is available. Effects are then estimated for the total population, see Huber and Solovyeva (2018) for further details.
</p>


<h3>Value</h3>

<p>A medweight object contains two components, <code>results</code> and <code>ntrimmed</code>:
</p>
<p><code>results</code>: a 3X5 matrix containing the effect estimates in the first row (&quot;effects&quot;), standard errors in the second row (&quot;se&quot;), and p-values in the third row (&quot;p-value&quot;).
The first column provides the total effect, namely the average treatment effect (ATE) if <code>ATET=FALSE</code> or the average treatment effect on the treated (ATET) if <code>ATET=TRUE</code>.
The second and third columns provide the direct effects under treatment and control, respectively (&quot;dir.treat&quot;, &quot;dir.control&quot;). See equation (6) if <code>w=NULL</code> (no post-treatment confounders) and equation (13) if <code>w</code> is defined, respectively, in Huber (2014). If <code>w=NULL</code>, the fourth and fifth columns provide the indirect effects under treatment and control, respectively (&quot;indir.treat&quot;, &quot;indir.control&quot;), see equation (7) in Huber (2014).
If <code>w</code> is defined, the fourth and fifth columns provide the partial indirect effects under treatment and control, respectively (&quot;par.in.treat&quot;, &quot;par.in.control&quot;), see equation (14) in Huber (2014).
</p>
<p><code>ntrimmed</code>: number of discarded (trimmed) observations due to extreme propensity score values.
</p>


<h3>References</h3>

<p>Huber, M. (2014): &quot;Identifying causal mechanisms (primarily) based on inverse probability weighting&quot;,  Journal of Applied Econometrics, 29, 920-943.
</p>
<p>Huber, M. and Solovyeva, A. (2018): &quot;Direct and indirect effects under sample selection and outcome attrition &quot;,  SES working paper 496, University of Fribourg.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># A little example with simulated data (10000 observations)
## Not run: 
n=10000
x=rnorm(n)
d=(0.25*x+rnorm(n)&gt;0)*1
w=0.2*d+0.25*x+rnorm(n)
m=0.5*w+0.5*d+0.25*x+rnorm(n)
y=0.5*d+m+w+0.25*x+rnorm(n)
# The true direct and partial indirect effects are all equal to 0.5
output=medweight(y=y,d=d,m=m,x=x,w=w,trim=0.05,ATET=FALSE,logit=TRUE,boot=19)
round(output$results,3)
output$ntrimmed
## End(Not run)
</code></pre>

<hr>
<h2 id='medweightcont'>Causal mediation analysis with a continuous treatment based on weighting by the inverse of generalized propensity scores</h2><span id='topic+medweightcont'></span>

<h3>Description</h3>

<p>Causal mediation analysis (evaluation of natural direct and indirect effects) of a continuous treatment based on weighting by the inverse of generalized propensity scores as suggested in Hsu, Huber, Lee, and Lettry (2020).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>medweightcont(
  y,
  d,
  m,
  x,
  d0,
  d1,
  ATET = FALSE,
  trim = 0.1,
  lognorm = FALSE,
  bw = NULL,
  boot = 1999,
  cluster = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="medweightcont_+3A_y">y</code></td>
<td>
<p>Dependent variable, must not contain missings.</p>
</td></tr>
<tr><td><code id="medweightcont_+3A_d">d</code></td>
<td>
<p>Continuous treatment, must not contain missings.</p>
</td></tr>
<tr><td><code id="medweightcont_+3A_m">m</code></td>
<td>
<p>Mediator(s), may be a scalar or a vector, must not contain missings.</p>
</td></tr>
<tr><td><code id="medweightcont_+3A_x">x</code></td>
<td>
<p>Pre-treatment confounders of the treatment, mediator, and/or outcome, must not contain missings.</p>
</td></tr>
<tr><td><code id="medweightcont_+3A_d0">d0</code></td>
<td>
<p>Value of <code>d</code> under non-treatment. Effects are based on pairwise comparisons, i.e. differences in potential outcomes evaluated at <code>d1</code> and <code>d0</code>.</p>
</td></tr>
<tr><td><code id="medweightcont_+3A_d1">d1</code></td>
<td>
<p>Value of <code>d</code> under treatment. Effects are based on pairwise comparisons, i.e. differences in potential outcomes evaluated at <code>d1</code> and <code>d0</code>.</p>
</td></tr>
<tr><td><code id="medweightcont_+3A_atet">ATET</code></td>
<td>
<p>If FALSE, the average treatment effect (ATE) and the corresponding direct and indirect effects are estimated. If TRUE, the average treatment effect on the treated (ATET)  and the corresponding direct and indirect effects are estimated. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="medweightcont_+3A_trim">trim</code></td>
<td>
<p>Trimming rule for discarding observations with too large weights in the estimation of any mean potential outcome. That is, observations with a weight&gt;<code>trim</code> are dropped from the sample. Default is a maximum weight of 0.1 (or 10 percent) per observation.</p>
</td></tr>
<tr><td><code id="medweightcont_+3A_lognorm">lognorm</code></td>
<td>
<p>If FALSE, a linear model with normally distributed errors is assumed for generalized propensity score estimation. If TRUE, a lognormal model is assumed. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="medweightcont_+3A_bw">bw</code></td>
<td>
<p>Bandwith for the second order Epanechnikov kernel functions of the treatment. If set to NULL, bandwidth computation is based on the rule of thumb for Epanechnikov kernels, determining the bandwidth as the standard deviation of the treatment times 2.34/(<code>n</code>^0.25), where <code>n</code> is the sample size. Default is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="medweightcont_+3A_boot">boot</code></td>
<td>
<p>Number of bootstrap replications for estimating standard errors. Default is 1999.</p>
</td></tr>
<tr><td><code id="medweightcont_+3A_cluster">cluster</code></td>
<td>
<p>A cluster ID for block or cluster bootstrapping when units are clustered rather than iid. Must be numerical. Default is NULL (standard bootstrap without clustering).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Estimation of causal mechanisms (natural direct and indirect effects) of a continuous treatment under a selection on observables assumption assuming that all confounders of the treatment and the mediator, the treatment and the outcome, or the mediator and the outcome are observed. Units are weighted by the inverse of their conditional treatment densities (known as generalized propensity scores) given the mediator and/or observed confounders, which are estimated by linear or loglinear regression.
Standard errors are obtained by bootstrapping the effects.
</p>


<h3>Value</h3>

<p>A medweightcont object contains two components, <code>results</code> and <code>ntrimmed</code>:
</p>
<p><code>results</code>: a 3X5 matrix containing the effect estimates in the first row (&quot;effects&quot;), standard errors in the second row (&quot;se&quot;), and p-values in the third row (&quot;p-value&quot;).
The first column provides the total effect, namely the average treatment effect (ATE) if <code>ATET=FALSE</code> or the average treatment effect on the treated (ATET), i.e. those with D=<code>d1</code>, if <code>ATET=TRUE</code>.
The second and third columns provide the direct effects under treatment and control, respectively (&quot;dir.treat&quot;, &quot;dir.control&quot;). The fourth and fifth columns provide the indirect effects under treatment and control, respectively (&quot;indir.treat&quot;, &quot;indir.control&quot;).
</p>
<p><code>ntrimmed</code>: number of discarded (trimmed) observations due to extreme propensity score values.
</p>


<h3>References</h3>

<p>Hsu, Y.-C., Huber, M., Lee, Y.-Y., Lettry, L. (2020): &quot;Direct and indirect effects of continuous treatments based on generalized propensity score weighting&quot;, Journal of Applied Econometrics, forthcoming.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># A little example with simulated data (10000 observations)
## Not run: 
n=10000
x=runif(n=n,min=-1,max=1)
d=0.25*x+runif(n=n,min=-2,max=2)
d=d-min(d)
m=0.5*d+0.25*x+runif(n=n,min=-2,max=2)
y=0.5*d+m+0.25*x+runif(n=n,min=-2,max=2)
# The true direct and indirect effects are all equal to 0.5
output=medweightcont(y,d,m,x,d0=2,d1=3,ATET=FALSE,trim=0.1,
       lognorm=FALSE,bw=NULL,boot=19)
round(output$results,3)
output$ntrimmed
## End(Not run)
</code></pre>

<hr>
<h2 id='RDDcovar'>Sharp regression discontinuity design conditional on covariates</h2><span id='topic+RDDcovar'></span>

<h3>Description</h3>

<p>Nonparametric (kernel regression-based) sharp regression discontinuity controlling for covariates that are permitted to jointly affect the treatment assignment and the outcome at the threshold of the running variable, see Fr√∂lich and Huber (2019).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RDDcovar(
  y,
  z,
  x,
  boot = 1999,
  bw0 = NULL,
  bw1 = NULL,
  regtype = "ll",
  bwz = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RDDcovar_+3A_y">y</code></td>
<td>
<p>Dependent variable, must not contain missings.</p>
</td></tr>
<tr><td><code id="RDDcovar_+3A_z">z</code></td>
<td>
<p>Running variable. Must be coded such that the treatment is zero for <code>z</code> being smaller than zero and one for <code>z</code> being larger than or equal to zero. Must not contain missings.</p>
</td></tr>
<tr><td><code id="RDDcovar_+3A_x">x</code></td>
<td>
<p>Covariates, must not contain missings.</p>
</td></tr>
<tr><td><code id="RDDcovar_+3A_boot">boot</code></td>
<td>
<p>Number of bootstrap replications for estimating standard errors. Default is 1999.</p>
</td></tr>
<tr><td><code id="RDDcovar_+3A_bw0">bw0</code></td>
<td>
<p>Bandwidth for a kernel regression of <code>y</code> on <code>z</code> and <code>x</code> below the threshold (for treatment equal to zero), using the Epanechnikov kernel. Default is <code>NULL</code>, implying that the bandwidth is estimated by least squares cross-validation.</p>
</td></tr>
<tr><td><code id="RDDcovar_+3A_bw1">bw1</code></td>
<td>
<p>Bandwidth for a kernel regression of <code>y</code> on <code>z</code> and <code>x</code> above the threshold (for treatment equal to one), using the Epanechnikov kernel. Default is <code>NULL</code>, implying that the bandwidth is estimated by least squares cross-validation.</p>
</td></tr>
<tr><td><code id="RDDcovar_+3A_regtype">regtype</code></td>
<td>
<p>Defines the type of the kernel regressions of <code>y</code> on <code>z</code> and <code>x</code> below and above the threshold.  Must either be set to <code>"ll"</code> for local linear regression or to <code>"ll"</code> for local constant regression. Default is <code>"ll"</code>.</p>
</td></tr>
<tr><td><code id="RDDcovar_+3A_bwz">bwz</code></td>
<td>
<p>Bandwidth for the (Epanechnikov) kernel function on <code>z</code>. Default is <code>NULL</code>, implying that the bandwidth is estimated by least squares cross-validation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Sharp regression discontinuity design conditional on covariates to control for observed confounders jointly affecting the treatment assignment and outcome at the threshold of the running variable as discussed in Fr√∂lich and Huber (2019). This is implemented by running kernel regressions of the outcome on the running variable and the covariates separately above and below the threshold and by applying a kernel smoother to the running variable around the threshold. The procedure permits choosing kernel bandwidths by cross-validation, even though this does in general not yield the optimal bandwidths for treatment effect estimation (checking the robustness of the results by varying the bandwidths is therefore highly recommended). Standard errors are based on bootstrapping.
</p>


<h3>Value</h3>

<p><code>effect</code>: Estimated treatment effect at the threshold.
</p>
<p><code>se</code>: Bootstrap-based standard error of the effect estimate.
</p>
<p><code>pvalue</code>: P-value based on the t-statistic.
</p>
<p><code>bw0</code>: Bandwidth for kernel regression of <code>y</code> on <code>z</code> and <code>x</code> below the threshold (for treatment equal to zero).
</p>
<p><code>bw1</code>: Bandwidth for kernel regression of <code>y</code> on <code>z</code> and <code>x</code> above the threshold (for treatment equal to one).
</p>
<p><code>bwz</code>: Bandwidth for the kernel function on <code>z</code>.
</p>


<h3>References</h3>

<p>Fr√∂lich, M. and Huber, M. (2019): &quot;Including covariates in the regression discontinuity design&quot;, Journal of Business &amp; Economic Statistics, 37, 736-748.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# load unemployment duration data
data(ubduration)
# run sharp RDD conditional on covariates with user-defined bandwidths
output=RDDcovar(y=ubduration[,1],z=ubduration[,2],x=ubduration[,c(-1,-2)],
 bw0=c(0.17, 1, 0.01, 0.05, 0.54, 70000, 0.12, 0.91, 100000),
 bw1=c(0.59, 0.65, 0.30, 0.06, 0.81, 0.04, 0.12, 0.76, 1.03),bwz=0.2,boot=19)
cat("RDD effect estimate: ",round(c(output$effect),3),", standard error: ",
 round(c(output$se),3), ", p-value: ", round(c(output$pvalue),3))
## End(Not run)
</code></pre>

<hr>
<h2 id='rkd'>Swedish municipalities</h2><span id='topic+rkd'></span>

<h3>Description</h3>

<p>A dataset containing information on Swedish municipalities in the years 1996-2004
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rkd
</code></pre>


<h3>Format</h3>

<p>A data frame with 2511 rows and 53 variables:
</p>

<dl>
<dt>code</dt><dd><p>Municipality code</p>
</dd>
<dt>year</dt><dd><p>Year</p>
</dd>
<dt>municipality</dt><dd><p>Minicipality name</p>
</dd>
<dt>pop_1</dt><dd><p>Population, lagged 1 year</p>
</dd>
<dt>pop</dt><dd><p>Population</p>
</dd>
<dt>partpop06</dt><dd><p>Share of population aged 0-6</p>
</dd>
<dt>partpop7_15</dt><dd><p>Share of population aged 7-15</p>
</dd>
<dt>partpop80_</dt><dd><p>Share of population aged 80+</p>
</dd>
<dt>partforeign</dt><dd><p>Share of foreign born population</p>
</dd>
<dt>costequalgrants</dt><dd><p>Cost-equalizing grants</p>
</dd>
<dt>popchange_10y</dt><dd><p>10 year out-migration, lagged 2 years</p>
</dd>
<dt>pers_admin</dt><dd><p>Personnel, administration</p>
</dd>
<dt>pers_child</dt><dd><p>Personnel, child care</p>
</dd>
<dt>pers_school</dt><dd><p>Personnel, schools</p>
</dd>
<dt>pers_elder</dt><dd><p>Personnel, elderly care</p>
</dd>
<dt>pers_total</dt><dd><p>Personnel, total (full time equivalents pers 1,000 capita)</p>
</dd>
<dt>pers_social</dt><dd><p>Personnel, social welfare</p>
</dd>
<dt>pers_tech</dt><dd><p>Personnel, technical services</p>
</dd>
<dt>expenditures_total</dt><dd><p>Total per capita public expenditures</p>
</dd>
<dt>wage_admin</dt><dd><p>Average montly wage, administration</p>
</dd>
<dt>wage_child</dt><dd><p>Average monthly wage, child care</p>
</dd>
<dt>wage_school</dt><dd><p>Average monthly wage, schools</p>
</dd>
<dt>wage_elder</dt><dd><p>Average monthly wage, elderly care</p>
</dd>
<dt>wage_total</dt><dd><p>Average monthly wage, total</p>
</dd>
<dt>wage_social</dt><dd><p>Average monthly wage, social welfare</p>
</dd>
<dt>wage_tech</dt><dd><p>Average monthly wage, technical services</p>
</dd>
<dt>pers_officials</dt><dd><p>Personnel, high administrative officials</p>
</dd>
<dt>pers_assistants</dt><dd><p>Personnel, administrative assistants</p>
</dd>
<dt>pers_priv_school</dt><dd><p>Outsourced personnel, schools</p>
</dd>
<dt>pers_priv_elder</dt><dd><p>Outsourced personnel, elderly care</p>
</dd>
<dt>pers_priv_social</dt><dd><p>Outsourced personnel, social welfare</p>
</dd>
<dt>pers_priv_child</dt><dd><p>Outsourced personnel, child care</p>
</dd>
<dt>migrationgrant</dt><dd><p>round(abs((popchange_10y+2)\*100)) if popchange_10y &lt;= -2, 0 otherwise</p>
</dd>
<dt>migpop</dt><dd><p>migrationgrant\*pop_1</p>
</dd>
<dt>summigpop</dt><dd><p>sum(migpop) by year</p>
</dd>
<dt>sumpop</dt><dd><p>sum(pop_1) by year</p>
</dd>
<dt>migrationmean</dt><dd><p>summigpop/sumpop</p>
</dd>
<dt>exp_total</dt><dd><p>Annual expenditures on personnel in 100SEK/capita</p>
</dd>
<dt>exp_admin</dt><dd><p>Annual expenditures on personnel in 100SEK/capita</p>
</dd>
<dt>exp_child</dt><dd><p>Annual expenditures on personnel in 100SEK/capita</p>
</dd>
<dt>exp_school</dt><dd><p>Annual expenditures on personnel in 100SEK/capita</p>
</dd>
<dt>exp_elder</dt><dd><p>Annual expenditures on personnel in 100SEK/capita</p>
</dd>
<dt>exp_social</dt><dd><p>Annual expenditures on personnel in 100SEK/capita</p>
</dd>
<dt>exp_tech</dt><dd><p>Annual expenditures on personnel in 100SEK/capita</p>
</dd>
<dt>expshare_total</dt><dd><p>exp_total\*100/expenditures_total</p>
</dd>
<dt>expshare_admin</dt><dd><p>exp_admin\*100/expenditures_total</p>
</dd>
<dt>expshare_child</dt><dd><p>exp_child\*100/expenditures_total</p>
</dd>
<dt>expshare_school</dt><dd><p>exp_school\*100/expenditures_total</p>
</dd>
<dt>expshare_elder</dt><dd><p>exp_elder\*100/expenditures_total</p>
</dd>
<dt>expshare_social</dt><dd><p>exp_social\*100/expenditures_total</p>
</dd>
<dt>expshare_tech</dt><dd><p>exp_tech\*100/expenditures_total</p>
</dd>
<dt>outmigration</dt><dd><p>-popchange_10y</p>
</dd>
<dt>forcing</dt><dd><p>outmigration-2</p>
</dd>
</dl>



<h3>References</h3>

<p>Lundqvist, Hel√©ne, Dahlberg, Matz and M√∂rk, Eva (2014): &quot;Stimulating Local Public Employment: Do General Grants Work?&quot; American Economic Journal: Economic Policy, 6 (1): 167-92.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
require(rdrobust)                             # load rdrobust package
require(causalweight)                         # load causalweight package
data(rkd)                                     # load rkd data
attach(rkd)                                   # attach rkd data
Y=pers_total                                  # define outcome (total personnel)
R=forcing                                     # define running variable
D=costequalgrants                             # define treatment (grants)
results=rdrobust(y=Y, x=R, fuzzy=D, deriv=1)  # run fuzzy RKD
summary(results)
## End(Not run)                             # show results
</code></pre>

<hr>
<h2 id='swissexper'>Correspondence test in Swiss apprenticeship market</h2><span id='topic+swissexper'></span>

<h3>Description</h3>

<p>A dataset related to a field experiment (correspondence test) in the Swiss
apprenticeship market 2018/2019. The experiment investigated the effects of
applicant gender and parental occupation in applications to apprenticeships
on callback rates (invitations to interviews, assessment centers, or trial apprenticeships)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>swissexper
</code></pre>


<h3>Format</h3>

<p>A data frame with 2928 rows and 18 variables:
</p>

<dl>
<dt>city</dt><dd><p>agglomeration of apprenticeship: 1=Bern,2=Zurich,3=Basel,6=Lausanne</p>
</dd>
<dt>foundatdate</dt><dd><p>date when job add was found</p>
</dd>
<dt>employees</dt><dd><p>(estimated) number of employees: 1=1-20; 2=21-50; 3=51-100; 4=101-250; 5=251-500; 6=501-1000; 7=1001+</p>
</dd>
<dt>sector</dt><dd><p>1=public sector; 2=trade/wholesale; 3=manufacturing/goods; 4=services</p>
</dd>
<dt>uniqueID</dt><dd><p>ID of application</p>
</dd>
<dt>sendatdate</dt><dd><p>date when application was sent</p>
</dd>
<dt>job_father</dt><dd><p>treatment: father's occupation: 1=professor; 2=unskilled worker; 3=intermediate commercial; 4=intermediate technical</p>
</dd>
<dt>job_mother</dt><dd><p>treatment: mother's occupation: 1= primary school teacher; 2=homemaker</p>
</dd>
<dt>tier</dt><dd><p>skill tier of apprenticeship: 1=lower; 2=intermediate; 3=upper</p>
</dd>
<dt>hasmoved</dt><dd><p>applicant moved from different city: 1=yes; 0=no</p>
</dd>
<dt>contgender</dt><dd><p>gender of contact person in company: 0=unknown; 1=female; 2=male</p>
</dd>
<dt>letterback</dt><dd><p>1: letters sent from company to applicant were returned; 0: no issues with returned letters</p>
</dd>
<dt>outcome_invite</dt><dd><p>outcome: invitation to interview, assessment center, or trial apprenticeship: 1=yes; 0=no</p>
</dd>
<dt>female_appl</dt><dd><p>treatment: 1=female applicant; 0=male applicant</p>
</dd>
<dt>antidiscrpolicy</dt><dd><p>1=explicit antidiscrimination policy on company's website; 0=no explicit antidiscrimination policy</p>
</dd>
<dt>outcome_interest</dt><dd><p>outcome: either invitation, or asking further questions, or keeping application for further consideration</p>
</dd>
<dt>gender_neutrality</dt><dd><p>0=gender neutral job type; 1=female dominated job type; 2=male dominated type</p>
</dd>
<dt>company_activity</dt><dd><p>scope of company's activity: 0=local; 1=national; 2=international</p>
</dd>
</dl>



<h3>References</h3>

<p>Fernandes, A., Huber, M., and Plaza, C. (2019): &quot;The Effects of Gender and Parental Occupation in the Apprenticeship Market: An Experimental Evaluation&quot;, SES working paper 506, University of Fribourg.
</p>

<hr>
<h2 id='treatDML'>Binary or multiple  discrete treatment effect evaluation with double machine learning</h2><span id='topic+treatDML'></span>

<h3>Description</h3>

<p>Treatment effect estimation for assessing the average effects of discrete (multiple or binary) treatments. Combines estimation based on (doubly robust) efficient score functions with double machine learning to control for confounders in a data-driven way.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>treatDML(
  y,
  d,
  x,
  s = NULL,
  dtreat = 1,
  dcontrol = 0,
  trim = 0.01,
  MLmethod = "lasso",
  k = 3,
  normalized = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="treatDML_+3A_y">y</code></td>
<td>
<p>Dependent variable, must not contain missings.</p>
</td></tr>
<tr><td><code id="treatDML_+3A_d">d</code></td>
<td>
<p>Treatment variable, must be discrete, must not contain missings.</p>
</td></tr>
<tr><td><code id="treatDML_+3A_x">x</code></td>
<td>
<p>Covariates, must not contain missings.</p>
</td></tr>
<tr><td><code id="treatDML_+3A_s">s</code></td>
<td>
<p>Indicator function for defining a subpopulation for whom the treatment effect is estimated as a function of the subpopulation's distribution of <code>x</code>. Default is <code>NULL</code> (estimation of the average treatment effect in the total population).</p>
</td></tr>
<tr><td><code id="treatDML_+3A_dtreat">dtreat</code></td>
<td>
<p>Value of the treatment in the treatment group. Default is 1.</p>
</td></tr>
<tr><td><code id="treatDML_+3A_dcontrol">dcontrol</code></td>
<td>
<p>Value of the treatment in the control group. Default is 0.</p>
</td></tr>
<tr><td><code id="treatDML_+3A_trim">trim</code></td>
<td>
<p>Trimming rule for discarding observations with treatment propensity scores that are smaller than <code>trim</code> or larger than <code>1-trim</code> (to avoid too small denominators in weighting by the inverse of the propensity scores). Default is 0.01.</p>
</td></tr>
<tr><td><code id="treatDML_+3A_mlmethod">MLmethod</code></td>
<td>
<p>Machine learning method for estimating the nuisance parameters based on the <code>SuperLearner</code> package. Must be either  <code>"lasso"</code> (default) for lasso estimation,  <code>"randomforest"</code> for random forests, <code>"xgboost"</code> for xg boosting,  <code>"svm"</code> for support vector machines, <code>"ensemble"</code> for using an ensemble algorithm based on all previously mentioned machine learners, or <code>"parametric"</code> for linear or logit regression.</p>
</td></tr>
<tr><td><code id="treatDML_+3A_k">k</code></td>
<td>
<p>Number of folds in k-fold cross-fitting. Default is 3.</p>
</td></tr>
<tr><td><code id="treatDML_+3A_normalized">normalized</code></td>
<td>
<p>If set to <code>TRUE</code>, then the inverse probability-based weights are normalized such that they add up to 1 within treatment groups. Default is <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Estimation of the causal effects of binary or multiple discrete treatments under conditional independence, assuming that confounders jointly affecting the treatment and the outcome can be controlled for by observed covariates. Estimation is based on the (doubly robust) efficient score functions for potential outcomes in combination with double machine learning with cross-fitting, see Chernozhukov et al (2018). To this end, one part of the data is used for estimating the model parameters of the treatment and outcome equations based machine learning. The other part of the data is used for predicting the efficient score functions. The roles of the data parts are swapped (using k-fold cross-fitting) and the average treatment effect is estimated based on averaging the predicted efficient score functions in the total sample.
Standard errors are based on asymptotic approximations using the estimated variance of the (estimated) efficient score functions.
</p>


<h3>Value</h3>

<p>A <code>treatDML</code> object contains eight components, <code>effect</code>, <code>se</code>, <code>pval</code>, <code>ntrimmed</code>, <code>meantreat</code>, <code>meancontrol</code>, <code>pstreat</code>, and <code>pscontrol</code>:
</p>
<p><code>effect</code>: estimate of the average treatment effect.
</p>
<p><code>se</code>: standard error of the effect.
</p>
<p><code>pval</code>: p-value of the effect estimate.
</p>
<p><code>ntrimmed</code>: number of discarded (trimmed) observations due to extreme propensity scores.
</p>
<p><code>meantreat</code>: Estimate of the mean potential outcome under treatment.
</p>
<p><code>meancontrol</code>: Estimate of the mean potential outcome under control.
</p>
<p><code>pstreat</code>: P-score estimates for treatment in treatment group.
</p>
<p><code>pscontrol</code>: P-score estimates for treatment in control group.
</p>


<h3>References</h3>

<p>Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., Robins, J. (2018): &quot;Double/debiased machine learning for treatment and structural parameters&quot;, The Econometrics Journal, 21, C1-C68.
</p>
<p>van der Laan, M., Polley, E., Hubbard, A. (2007): &quot;Super Learner&quot;, Statistical Applications in Genetics and Molecular Biology, 6.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># A little example with simulated data (2000 observations)
## Not run: 
n=2000                            # sample size
p=100                             # number of covariates
s=2                               # number of covariates that are confounders
x=matrix(rnorm(n*p),ncol=p)       # covariate matrix
beta=c(rep(0.25,s), rep(0,p-s))   # coefficients determining degree of confounding
d=(x%*%beta+rnorm(n)&gt;0)*1         # treatment equation
y=x%*%beta+0.5*d+rnorm(n)       # outcome equation
# The true ATE is equal to 0.5
output=treatDML(y,d,x)
cat("ATE: ",round(c(output$effect),3),", standard error: ",
    round(c(output$se),3), ", p-value: ",round(c(output$pval),3))
output$ntrimmed
## End(Not run)
</code></pre>

<hr>
<h2 id='treatselDML'>Binary or multiple  treatment effect evaluation with double machine learning under sample selection/outcome attrition</h2><span id='topic+treatselDML'></span>

<h3>Description</h3>

<p>Average treatment effect (ATE) estimation for assessing the average effects of discrete (multiple or binary) treatments under sample selection/outcome attrition. Combines estimation based on Neyman-orthogonal score functions with double machine learning to control for confounders in a data-driven way.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>treatselDML(
  y,
  d,
  x,
  s,
  z = NULL,
  selected = 0,
  dtreat = 1,
  dcontrol = 0,
  trim = 0.01,
  MLmethod = "lasso",
  k = 3,
  normalized = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="treatselDML_+3A_y">y</code></td>
<td>
<p>Dependent variable, may contain missings.</p>
</td></tr>
<tr><td><code id="treatselDML_+3A_d">d</code></td>
<td>
<p>Treatment variable, must be discrete, must not contain missings.</p>
</td></tr>
<tr><td><code id="treatselDML_+3A_x">x</code></td>
<td>
<p>Covariates, must not contain missings.</p>
</td></tr>
<tr><td><code id="treatselDML_+3A_s">s</code></td>
<td>
<p>Selection indicator. Must be 1 if <code>y</code> is observed (non-missing) and 0 if <code>y</code> is not observed (missing).</p>
</td></tr>
<tr><td><code id="treatselDML_+3A_z">z</code></td>
<td>
<p>Optional instrumental variable(s) for selection <code>s</code>. If <code>NULL</code>, outcome selection based on observables (<code>x</code>,<code>d</code>) - known as &quot;missing at random&quot; - is assumed. If <code>z</code> is defined, outcome selection based on unobservables - known as &quot;non-ignorable missingness&quot; - is assumed. Default is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="treatselDML_+3A_selected">selected</code></td>
<td>
<p>Must be 1 if ATE is to be estimated for the selected population without missing outcomes. Must be 0 if the ATE is to be estimated for the total population. Default is 0 (ATE for total population). This parameter is ignored if <code>z</code> is <code>NULL</code> (under MAR, the ATE in the total population is estimated).</p>
</td></tr>
<tr><td><code id="treatselDML_+3A_dtreat">dtreat</code></td>
<td>
<p>Value of the treatment in the treatment group. Default is 1.</p>
</td></tr>
<tr><td><code id="treatselDML_+3A_dcontrol">dcontrol</code></td>
<td>
<p>Value of the treatment in the control group. Default is 0.</p>
</td></tr>
<tr><td><code id="treatselDML_+3A_trim">trim</code></td>
<td>
<p>Trimming rule for discarding observations with (products of) propensity scores that are smaller than <code>trim</code> (to avoid too small denominators in weighting by the inverse of the propensity scores). If <code>selected</code> is 0 (ATE estimation for the total population), observations with products of the treatment and selection propensity scores that are smaller than <code>trim</code> are discarded.  If <code>selected</code> is 1 (ATE estimation for the subpopulation with observed outcomes), observations with treatment propensity scores smaller than <code>trim</code> are discarded. Default for <code>trim</code> is 0.01.</p>
</td></tr>
<tr><td><code id="treatselDML_+3A_mlmethod">MLmethod</code></td>
<td>
<p>Machine learning method for estimating the nuisance parameters based on the <code>SuperLearner</code> package. Must be either  <code>"lasso"</code> (default) for lasso estimation,  <code>"randomforest"</code> for random forests, <code>"xgboost"</code> for xg boosting,  <code>"svm"</code> for support vector machines, <code>"ensemble"</code> for using an ensemble algorithm based on all previously mentioned machine learners, or <code>"parametric"</code> for linear or logit regression.</p>
</td></tr>
<tr><td><code id="treatselDML_+3A_k">k</code></td>
<td>
<p>Number of folds in k-fold cross-fitting. Default is 3.</p>
</td></tr>
<tr><td><code id="treatselDML_+3A_normalized">normalized</code></td>
<td>
<p>If set to <code>TRUE</code>, then the inverse probability-based weights are normalized such that they add up to 1 within treatment groups. Default is <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Estimation of the causal effects of binary or multiple discrete treatments under conditional independence, assuming that confounders jointly affecting the treatment and the outcome can be controlled for by observed covariates, and sample selection/outcome attrition. The latter might either be related to observables, which implies a missing at random assumption, or in addition also to unobservables, if an instrument for sample selection is available. Estimation is based on Neyman-orthogonal score functions for potential outcomes in combination with double machine learning with cross-fitting, see Chernozhukov et al (2018). To this end, one part of the data is used for estimating the model parameters of the treatment and outcome equations based machine learning. The other part of the data is used for predicting the efficient score functions. The roles of the data parts are swapped (using k-fold cross-fitting) and the average treatment effect is estimated based on averaging the predicted efficient score functions in the total sample.  Standard errors are based on asymptotic approximations using the estimated variance of the (estimated) efficient score functions.
</p>


<h3>Value</h3>

<p>A <code>treatDML</code> object contains eight components, <code>effect</code>, <code>se</code>, <code>pval</code>, <code>ntrimmed</code>, <code>meantreat</code>, <code>meancontrol</code>, <code>pstreat</code>, and <code>pscontrol</code>:
</p>
<p><code>effect</code>: estimate of the average treatment effect.
</p>
<p><code>se</code>: standard error of the effect.
</p>
<p><code>pval</code>: p-value of the effect estimate.
</p>
<p><code>ntrimmed</code>: number of discarded (trimmed) observations due to extreme propensity scores.
</p>
<p><code>meantreat</code>: Estimate of the mean potential outcome under treatment.
</p>
<p><code>meancontrol</code>: Estimate of the mean potential outcome under control.
</p>
<p><code>pstreat</code>: P-score estimates for treatment in treatment group.
</p>
<p><code>pscontrol</code>: P-score estimates for treatment in control group.
</p>


<h3>References</h3>

<p>Bia, M., Huber, M., Laffers, L. (2020): &quot;Double machine learning for sample selection models&quot;, working paper, University of Fribourg.
</p>
<p>Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., Robins, J. (2018): &quot;Double/debiased machine learning for treatment and structural parameters&quot;, The Econometrics Journal, 21, C1-C68.
</p>
<p>van der Laan, M., Polley, E., Hubbard, A. (2007): &quot;Super Learner&quot;, Statistical Applications in Genetics and Molecular Biology, 6.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># A little example with simulated data (2000 observations)
## Not run: 
n=2000                            # sample size
p=100                             # number of covariates
s=2                               # number of covariates that are confounders
sigma=matrix(c(1,0.5,0.5,1),2,2)
e=(2*rmvnorm(n,rep(0,2),sigma))
x=matrix(rnorm(n*p),ncol=p)       # covariate matrix
beta=c(rep(0.25,s), rep(0,p-s))   # coefficients determining degree of confounding
d=(x%*%beta+rnorm(n)&gt;0)*1         # treatment equation
z=rnorm(n)
s=(x%*%beta+0.25*d+z+e[,1]&gt;0)*1   # selection equation
y=x%*%beta+0.5*d+e[,2]            # outcome equation
 y[s==0]=0
# The true ATE is equal to 0.5
output=treatselDML(y,d,x,s,z)
cat("ATE: ",round(c(output$effect),3),", standard error: ",
    round(c(output$se),3), ", p-value: ",round(c(output$pval),3))
output$ntrimmed
## End(Not run)
</code></pre>

<hr>
<h2 id='treatweight'>Treatment evaluation based on inverse probability weighting with optional sample selection correction.</h2><span id='topic+treatweight'></span>

<h3>Description</h3>

<p>Treatment evaluation based on inverse probability weighting with optional sample selection correction.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>treatweight(
  y,
  d,
  x,
  s = NULL,
  z = NULL,
  selpop = FALSE,
  ATET = FALSE,
  trim = 0.05,
  logit = FALSE,
  boot = 1999,
  cluster = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="treatweight_+3A_y">y</code></td>
<td>
<p>Dependent variable.</p>
</td></tr>
<tr><td><code id="treatweight_+3A_d">d</code></td>
<td>
<p>Treatment, must be binary (either 1 or 0), must not contain missings.</p>
</td></tr>
<tr><td><code id="treatweight_+3A_x">x</code></td>
<td>
<p>Confounders of the treatment and outcome, must not contain missings.</p>
</td></tr>
<tr><td><code id="treatweight_+3A_s">s</code></td>
<td>
<p>Selection indicator. Must be one if <code>y</code> is observed (non-missing) and zero if <code>y</code> is not observed (missing). Default is <code>NULL</code>, implying that <code>y</code> does not contain any missings.</p>
</td></tr>
<tr><td><code id="treatweight_+3A_z">z</code></td>
<td>
<p>Optional instrumental variable(s) for selection <code>s</code>. If <code>NULL</code>, outcome selection based on observables (<code>x</code>,<code>d</code>) - known as &quot;missing at random&quot; - is assumed.
If <code>z</code> is defined, outcome selection based on unobservables - known as &quot;non-ignorable missingness&quot; - is assumed. Default is <code>NULL</code>. If <code>s</code> is <code>NULL</code>, <code>z</code> is ignored.</p>
</td></tr>
<tr><td><code id="treatweight_+3A_selpop">selpop</code></td>
<td>
<p>Only to be used if both <code>s</code> and <code>z</code> are defined. If <code>TRUE</code>, the effect is estimated for the selected subpopulation with <code>s</code>=1 only. If <code>FALSE</code>, the effect is estimated for the total population.
(note that this relies on somewhat stronger statistical assumptions). Default is <code>FALSE</code>. If <code>s</code> or  <code>z</code> is <code>NULL</code>, <code>selpop</code> is ignored.</p>
</td></tr>
<tr><td><code id="treatweight_+3A_atet">ATET</code></td>
<td>
<p>If <code>FALSE</code>, the average treatment effect (ATE) is estimated. If <code>TRUE</code>, the average treatment effect on the treated (ATET)  is estimated. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="treatweight_+3A_trim">trim</code></td>
<td>
<p>Trimming rule for discarding observations with extreme propensity scores. If <code>ATET=FALSE</code>, observations with Pr(D=1|X)&lt;<code>trim</code> or Pr(D=1|X)&gt;(1-<code>trim</code>) are dropped.
If <code>ATET=TRUE</code>, observations with Pr(D=1|X)&gt;(1-<code>trim</code>) are dropped. If <code>s</code> is defined and <code>z</code> is <code>NULL</code>, observations with extremely low selection propensity scores, Pr(S=1|D,X)&lt;<code>trim</code>, are discarded, too. If <code>s</code> and <code>z</code> are defined, the treatment propensity scores to be trimmed change to Pr(D=1|X,Pr(S=1|D,X,Z)). If in addition <code>selpop</code> is <code>FALSE</code>, observation with Pr(S=1|D,X,Z)&lt;<code>trim</code> are discarded, too. Default for <code>trim</code> is 0.05.</p>
</td></tr>
<tr><td><code id="treatweight_+3A_logit">logit</code></td>
<td>
<p>If <code>FALSE</code>, probit regression is used for propensity score estimation. If <code>TRUE</code>, logit regression is used. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="treatweight_+3A_boot">boot</code></td>
<td>
<p>Number of bootstrap replications for estimating standard errors. Default is 1999.</p>
</td></tr>
<tr><td><code id="treatweight_+3A_cluster">cluster</code></td>
<td>
<p>A cluster ID for block or cluster bootstrapping when units are clustered rather than iid. Must be numerical. Default is NULL (standard bootstrap without clustering).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Estimation of treatment effects of a binary treatment under a selection on observables assumption assuming that all confounders of the treatment and the outcome are observed. Units are weighted by the inverse of their conditional treatment propensities given the observed confounders, which are estimated by probit or logit regression. Standard errors are obtained by bootstrapping the effect.
If <code>s</code> is defined, the procedure allows correcting for sample selection due to missing outcomes based on the inverse of the conditional selection probability. The latter might either be related to observables, which implies a missing at random assumption, or in addition also to unobservables, if an instrument for sample selection is available. See Huber (2012, 2014) for further details.
</p>


<h3>Value</h3>

<p>A treatweight object contains six components: <code>effect</code>, <code>se</code>, <code>pval</code>, <code>y1</code>, <code>y0</code>, and <code>ntrimmed</code>.
</p>
<p><code>effect</code>: average treatment effect (ATE) if <code>ATET=FALSE</code> or the average treatment effect on the treated (ATET) if <code>ATET=TRUE</code>.
</p>
<p><code>se</code>: bootstrap-based standard error of the effect.
</p>
<p><code>pval</code>: p-value of the effect.
</p>
<p><code>y1</code>: mean potential outcome under treatment.
</p>
<p><code>y0</code>: mean potential outcome under control.
</p>
<p><code>ntrimmed</code>: number of discarded (trimmed) observations due to extreme propensity score values.
</p>


<h3>References</h3>

<p>Horvitz, D. G., and Thompson, D. J. (1952): &quot;A generalization of sampling without replacement from a finite universe&quot;, Journal of the American Statistical Association, 47, 663‚Äì685.
</p>
<p>Huber, M. (2012): &quot;Identification of average treatment effects in social experiments under alternative forms of attrition&quot;, Journal of Educational and Behavioral Statistics, 37, 443-474.
</p>
<p>Huber, M. (2014): &quot;Treatment evaluation in the presence of sample selection&quot;, Econometric Reviews, 33, 869-905.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># A little example with simulated data (10000 observations)
## Not run: 
n=10000
x=rnorm(n); d=(0.25*x+rnorm(n)&gt;0)*1
y=0.5*d+0.25*x+rnorm(n)
# The true ATE is equal to 0.5
output=treatweight(y=y,d=d,x=x,trim=0.05,ATET=FALSE,logit=TRUE,boot=19)
cat("ATE: ",round(c(output$effect),3),", standard error: ",
    round(c(output$se),3), ", p-value: ",round(c(output$pval),3))
output$ntrimmed
## End(Not run)
# An example with non-random outcome selection and an instrument for selection
## Not run: 
n=10000
sigma=matrix(c(1,0.6,0.6,1),2,2)
e=(2*rmvnorm(n,rep(0,2),sigma))
x=rnorm(n)
d=(0.5*x+rnorm(n)&gt;0)*1
z=rnorm(n)
s=(0.25*x+0.25*d+0.5*z+e[,1]&gt;0)*1
y=d+x+e[,2]; y[s==0]=0
# The true ATE is equal to 1
output=treatweight(y=y,d=d,x=x,s=s,z=z,selpop=FALSE,trim=0.05,ATET=FALSE,
       logit=TRUE,boot=19)
cat("ATE: ",round(c(output$effect),3),", standard error: ",
    round(c(output$se),3), ", p-value: ",round(c(output$pval),3))
output$ntrimmed
## End(Not run)
</code></pre>

<hr>
<h2 id='ubduration'>Austrian unemployment duration data</h2><span id='topic+ubduration'></span>

<h3>Description</h3>

<p>A dataset containing unemployed females between 46 and 53 years old living in an Austrian region where an extension of the maximum duration of unemployment benefits (from 30 to 209 weeks under particular conditions) for job seekers aged 50 or older was introduced.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ubduration
</code></pre>


<h3>Format</h3>

<p>A data frame with 5659 rows and 10 variables:
</p>

<dl>
<dt>y</dt><dd><p>Outcome variable: unemployment duration of the jobseeker in weeks (registered at the unemployment office). Variable is numeric.</p>
</dd>
<dt>z</dt><dd><p>Running variable: distance to the age threshold of 50 (implying an extended duration of unemployment benefits), measured in months divided by 12. Variable is numeric.</p>
</dd>
<dt>marrstatus</dt><dd><p>Marital status: 0=other, 1=married, 2=single. Variable is a factor.</p>
</dd>
<dt>education</dt><dd><p>Eductation: 0=low education, 1=medium education, 2=high education. Variable is ordered.</p>
</dd>
<dt>foreign</dt><dd><p>Migrant status: 1=foreigner, 0=Austrian. Variable is a factor.</p>
</dd>
<dt>rr</dt><dd><p>Replacement rate (of previous earnings by unemployment benefits). Variable is numeric.</p>
</dd>
<dt>lwageljob</dt><dd><p>Log wage in last job. Variable is numeric.</p>
</dd>
<dt>experience</dt><dd><p>Ratio of actual to potential work experience. Variable is numeric.</p>
</dd>
<dt>whitecollar</dt><dd><p>1=white collar worker, 0=blue collar worker. Variable is a factor.</p>
</dd>
<dt>industry</dt><dd><p>Industry: 0=other, 1=agriculture, 2=utilities, 3=food, 4=textiles, 5=wood, 6=machines, 7=other manufacturing, 8=construction, 9=tourism, 10=traffic, 11=services. Variable is a factor.</p>
</dd>
</dl>



<h3>References</h3>

<p>Lalive, R. (2008): &quot;How Do Extended Benefits Affect Unemployment Duration? A Regression Discontinuity Approach&quot;, Journal of Econometrics, 142, 785‚Äì806.
</p>
<p>Fr√∂lich, M. and Huber, M. (2019): &quot;Including covariates in the regression discontinuity design&quot;, Journal of Business &amp; Economic Statistics, 37, 736-748.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# load unemployment duration data
data(ubduration)
# run sharp RDD conditional on covariates with user-defined bandwidths
output=RDDcovar(y=ubduration[,1],z=ubduration[,2],x=ubduration[,c(-1,-2)],
 bw0=c(0.17, 1, 0.01, 0.05, 0.54, 70000, 0.12, 0.91, 100000),
 bw1=c(0.59, 0.65, 0.30, 0.06, 0.81, 0.04, 0.12, 0.76, 1.03),bwz=0.2,boot=19)
cat("RDD effect estimate: ",round(c(output$effect),3),", standard error: ",
 round(c(output$se),3), ", p-value: ", round(c(output$pvalue),3))
## End(Not run)
</code></pre>

<hr>
<h2 id='wexpect'>Wage expectations of students in Switzerland</h2><span id='topic+wexpect'></span>

<h3>Description</h3>

<p>A dataset containing information on wage expectations of 804 students
at the University of Fribourg and the University of Applied Sciences in Bern in the year 2017.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wexpect
</code></pre>


<h3>Format</h3>

<p>A data frame with 804 rows and 39 variables:
</p>

<dl>
<dt>wexpect1</dt><dd><p>wage expectations after finishing studies: 0=less than 3500 CHF gross per month; 1=3500-4000 CHF; 2=4000-4500 CHF;...; 15=10500-11000 CHF; 16=more than 11000 CHF</p>
</dd>
<dt>wexpect2</dt><dd><p>wage expectations 3 years after studying: 0=less than 3500 CHF gross per month; 1=3500-4000 CHF; 2=4000-4500 CHF;...; 15=10500-11000 CHF; 16=more than 11000 CHF</p>
</dd>
<dt>wexpect1othersex</dt><dd><p>expected wage of other sex after finishing studies in percent of own expected wage</p>
</dd>
<dt>wexpect2othersex</dt><dd><p>expected wage of other sex 3 years after studying in percent of own expected wage</p>
</dd>
<dt>male</dt><dd><p>1=male; 0=female</p>
</dd>
<dt>business</dt><dd><p>1=BA in business</p>
</dd>
<dt>econ</dt><dd><p>1=BA in economics</p>
</dd>
<dt>communi</dt><dd><p>1=BA in communication</p>
</dd>
<dt>businform</dt><dd><p>1=BA in business informatics</p>
</dd>
<dt>plansfull</dt><dd><p>1=plans working fulltime after studies</p>
</dd>
<dt>planseduc</dt><dd><p>1=plans obtaining further education (e.g. MA) after studies</p>
</dd>
<dt>sectorcons</dt><dd><p>1=planned sector: construction</p>
</dd>
<dt>sectortradesales</dt><dd><p>1=planned sector: trade and sales</p>
</dd>
<dt>sectortransware</dt><dd><p>1=planned sector: transport and warehousing</p>
</dd>
<dt>sectorhosprest</dt><dd><p>1=planned sector: hospitality and restaurant</p>
</dd>
<dt>sectorinfocom</dt><dd><p>1=planned sector: information and communication</p>
</dd>
<dt>sectorfininsur</dt><dd><p>1=planned sector: finance and insurance</p>
</dd>
<dt>sectorconsult</dt><dd><p>1=planned sector: consulting</p>
</dd>
<dt>sectoreduscience</dt><dd><p>1=planned sector: education and science</p>
</dd>
<dt>sectorhealthsocial</dt><dd><p>1=planned sector: health and social services</p>
</dd>
<dt>typegenstratman</dt><dd><p>1=planned job type: general or strategic management</p>
</dd>
<dt>typemarketing</dt><dd><p>1=planned job type: marketing</p>
</dd>
<dt>typecontrol</dt><dd><p>1=planned job type: controlling</p>
</dd>
<dt>typefinance</dt><dd><p>1=planned job type: finance</p>
</dd>
<dt>typesales</dt><dd><p>1=planned job type: sales</p>
</dd>
<dt>typetechengin</dt><dd><p>1=planned job type: technical/engineering</p>
</dd>
<dt>typehumanres</dt><dd><p>1=planned job type: human resources</p>
</dd>
<dt>posmanager</dt><dd><p>1=planned position: manager</p>
</dd>
<dt>age</dt><dd><p>age in years</p>
</dd>
<dt>swiss</dt><dd><p>1=Swiss nationality</p>
</dd>
<dt>hassiblings</dt><dd><p>1=has one or more siblings</p>
</dd>
<dt>motherhighedu</dt><dd><p>1=mother has higher education</p>
</dd>
<dt>fatherhighedu</dt><dd><p>1=father has higher education</p>
</dd>
<dt>motherworkedfull</dt><dd><p>1=mother worked fulltime at respondent's age 4-6</p>
</dd>
<dt>motherworkedpart</dt><dd><p>1=mother worked parttime at respondent's age 4-6</p>
</dd>
<dt>matwellbeing</dt><dd><p>self-assessed material wellbeing compared to average Swiss: 1=much worse; 2=worse; 3=as average Swiss; 4=better; 5=much better</p>
</dd>
<dt>homeowner</dt><dd><p>1=home ownership</p>
</dd>
<dt>treatmentinformation</dt><dd><p>1=if information on median wages in Switzerland was provided (randomized treatment)</p>
</dd>
<dt>treatmentorder</dt><dd><p>1=if order of questions on professional plans and personal information in survey has been reversed (randomized treatment), meaning that personal questions are asked first and professional ones later</p>
</dd>
</dl>



<h3>References</h3>

<p>Fernandes, A., Huber, M., and Vaccaro, G. (2020): &quot;Gender Differences in Wage Expectations&quot;, arXiv preprint arXiv:2003.11496.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wexpect)
attach(wexpect)
# effect of randomized wage information (treatment) on wage expectations 3 years after
# studying (outcome)
treatweight(y=wexpect2,d=treatmentinformation,x=cbind(male,business,econ,communi,
businform,age,swiss,motherhighedu,fatherhighedu),boot=199)
# direct effect of gender (treatment) and indirect effect through choice of field of
# studies (mediator) on wage expectations (outcome)
medweight(y=wexpect2,d=male,m=cbind(business,econ,communi,businform),
x=cbind(treatmentinformation,age,swiss,motherhighedu,fatherhighedu),boot=199)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
