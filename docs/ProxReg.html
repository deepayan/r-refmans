<!DOCTYPE html><html lang="en-US"><head><title>Help for package ProxReg</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ProxReg}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#delete_rect'><p>rectangular hole in image</p></a></li>
<li><a href='#inpainting'><p>image recovery using Lasso regression</p></a></li>
<li><a href='#k_fold_cross'><p>k_fold_cross</p></a></li>
<li><a href='#l_CV'><p>K-Fold Cross validation for L1/L2 regression</p></a></li>
<li><a href='#lasso_fista'><p>Lasso regression with fixed step with FISTA algorithm</p></a></li>
<li><a href='#lasso_fista_back'><p>Lasso regression with backtraking line research with FISTA algorithm</p></a></li>
<li><a href='#lasso_ista'><p>Lasso regression with fixed step with ISTA algorithm</p></a></li>
<li><a href='#lasso_ista_back'><p>Lasso regression with backtraking line research</p></a></li>
<li><a href='#lasso_multi'><p>Lasso logistic regression for multinomial response variable with fixed step</p></a></li>
<li><a href='#lasso_multi_back'><p>Lasso regression with backtraking line research for multinomial response variable</p></a></li>
<li><a href='#ols'><p>Ordinary Least Square regression</p></a></li>
<li><a href='#ols_KCV'><p>K-Fold Cross Validation for OLS</p></a></li>
<li><a href='#ridge'><p>Ridge regression</p></a></li>
<li><a href='#softmax'><p>Softmax function for multinomial response variable</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Linear Models for Prediction and Classification using Proximal
Operators</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2025-02-27</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>YingHong Chen &lt;yinghongchen1402@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements optimization techniques for Lasso regression, R.Tibshirani(1996)&lt;<a href="https://doi.org/10.1111%2Fj.2517-6161.1996.tb02080.x">doi:10.1111/j.2517-6161.1996.tb02080.x</a>&gt; using Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) and Iterative Shrinkage-Thresholding Algorithm (ISTA) based on proximal operators, A.Beck(2009)&lt;<a href="https://doi.org/10.1137%2F080716542">doi:10.1137/080716542</a>&gt;. The package is useful for high-dimensional regression problems and includes cross-validation procedures to select optimal penalty parameters.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown,</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Imports:</td>
<td>dplyr, EBImage, glmnet</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-03-13 15:48:55 UTC; yingh</td>
</tr>
<tr>
<td>Author:</td>
<td>YingHong Chen [aut, cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-03-15 17:10:12 UTC</td>
</tr>
</table>
<hr>
<h2 id='delete_rect'>rectangular hole in image</h2><span id='topic+delete_rect'></span>

<h3>Description</h3>

<p>creates a rectangular hole in the image with the specified dimensions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>delete_rect(image,i,j,width,height)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="delete_rect_+3A_image">image</code></td>
<td>
<p>image to be modified, it has to be a 3D array proceed with readImage function from EBImage package</p>
</td></tr>
<tr><td><code id="delete_rect_+3A_i">i</code></td>
<td>
<p>row index of the upper left corner of the rectangle</p>
</td></tr>
<tr><td><code id="delete_rect_+3A_j">j</code></td>
<td>
<p>column index of the upper left corner of the rectangle</p>
</td></tr>
<tr><td><code id="delete_rect_+3A_width">width</code></td>
<td>
<p>width of the rectangle</p>
</td></tr>
<tr><td><code id="delete_rect_+3A_height">height</code></td>
<td>
<p>height of the rectangle</p>
</td></tr>
</table>


<h3>Details</h3>

<p>delete_rect
</p>


<h3>Value</h3>

<p>a 3D array with pixels in the hole set to -100 and the rest of the image pixels unchanged
</p>


<h3>Examples</h3>

<pre><code class='language-R'>image&lt;-EBImage::readImage(system.file("extdata", "bird.jpg", package = "ProxReg"))
image_noise&lt;-delete_rect(image,160,160,20,20)
image_noise&lt;-EBImage::Image(image_noise,colormode = "Color")
EBImage::display(image_noise)
</code></pre>

<hr>
<h2 id='inpainting'>image recovery using Lasso regression</h2><span id='topic+inpainting'></span>

<h3>Description</h3>

<p>predicts the missing pixels in an image using Lasso regression and fills the hole in the image
</p>


<h3>Usage</h3>

<pre><code class='language-R'>inpainting(image,h,stride,i,j,width,height,lambda=0.1,max_iter=50000,
fista=TRUE, verbose=TRUE,ini=0,glmnet=TRUE,noise=TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="inpainting_+3A_image">image</code></td>
<td>
<p>image to be modified, it has to be a 3D array proceed with readImage function from EBImage package</p>
</td></tr>
<tr><td><code id="inpainting_+3A_h">h</code></td>
<td>
<p>size of the patch</p>
</td></tr>
<tr><td><code id="inpainting_+3A_stride">stride</code></td>
<td>
<p>stride for the patch</p>
</td></tr>
<tr><td><code id="inpainting_+3A_i">i</code></td>
<td>
<p>row index of the upper left corner of the rectangle</p>
</td></tr>
<tr><td><code id="inpainting_+3A_j">j</code></td>
<td>
<p>column index of the upper left corner of the rectangle</p>
</td></tr>
<tr><td><code id="inpainting_+3A_width">width</code></td>
<td>
<p>width of the rectangle</p>
</td></tr>
<tr><td><code id="inpainting_+3A_height">height</code></td>
<td>
<p>height of the rectangle</p>
</td></tr>
<tr><td><code id="inpainting_+3A_lambda">lambda</code></td>
<td>
<p>a penalized parameter for the Lasso regression, it is 0.1 by default</p>
</td></tr>
<tr><td><code id="inpainting_+3A_max_iter">max_iter</code></td>
<td>
<p>maximum number of iterations, it is 50000 by default</p>
</td></tr>
<tr><td><code id="inpainting_+3A_fista">fista</code></td>
<td>
<p>fista=TRUE: use FISTA algortihm for the pixel prediction</p>
</td></tr>
<tr><td><code id="inpainting_+3A_verbose">verbose</code></td>
<td>
<p>print the iteration number and the size of the boundary</p>
</td></tr>
<tr><td><code id="inpainting_+3A_ini">ini</code></td>
<td>
<p>initial value for the coefficients, default is 0</p>
</td></tr>
<tr><td><code id="inpainting_+3A_glmnet">glmnet</code></td>
<td>
<p>use glmnet package for the Lasso regression</p>
</td></tr>
<tr><td><code id="inpainting_+3A_noise">noise</code></td>
<td>
<p>display the image with the hole, it is TRUE by default</p>
</td></tr>
</table>


<h3>Details</h3>

<p>inpainting
</p>


<h3>Value</h3>

<p>a 3D array with the hole filled by pixels predicted by Lasso regression
</p>


<h3>Examples</h3>

<pre><code class='language-R'>test_img &lt;- EBImage::readImage(system.file("extdata", "bird.jpg", package = "ProxReg"))
image_repaired &lt;- inpainting(
  test_img, h = 10, stride = 6, i = 160, j = 160, width = 20, height = 20,
  lambda = 0.001, max_iter = 1000, verbose = TRUE, glmnet = TRUE,noise=TRUE)
RGB_repaired&lt;-EBImage::Image(image_repaired,colormode = "Color")
</code></pre>

<hr>
<h2 id='k_fold_cross'>k_fold_cross</h2><span id='topic+k_fold_cross'></span>

<h3>Description</h3>

<p>k_fold_cross splits the dataset into k parts, and uses k-1 parts to train the model and the remaining part to test the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>k_fold_cross(data,k)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="k_fold_cross_+3A_data">data</code></td>
<td>
<p>dataset which will be used for K-Fols Cross Validation</p>
</td></tr>
<tr><td><code id="k_fold_cross_+3A_k">k</code></td>
<td>
<p>integer</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with two sublists: training set  and test set
</p>


<h3>Examples</h3>

<pre><code class='language-R'>df = data.frame("hours"=c(1, 2, 4, 5, 5, 6, 6, 7, 8, 10, 11, 11, 12, 12, 14),
"score"=c(64, 66, 76, 73, 74, 81, 83, 82, 80, 88, 84, 82, 91, 93, 89))
k_fold_cross(df,k=2)
</code></pre>

<hr>
<h2 id='l_CV'>K-Fold Cross validation for L1/L2 regression</h2><span id='topic+l_CV'></span>

<h3>Description</h3>

<p>the function realizes K-Fold Cross validation for ridge/Lasso regression
to help to choose the lambda that minimise the RSS
</p>


<h3>Usage</h3>

<pre><code class='language-R'>l_CV(data,y,x,lambda,k,mode=2,binary=FALSE,step=1000,bound=0.5,fista=TRUE,tol=10^-7)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="l_CV_+3A_data">data</code></td>
<td>
<p>name of the dataset</p>
</td></tr>
<tr><td><code id="l_CV_+3A_y">y</code></td>
<td>
<p>name of the dependent variables</p>
</td></tr>
<tr><td><code id="l_CV_+3A_x">x</code></td>
<td>
<p>name of the independent variable</p>
</td></tr>
<tr><td><code id="l_CV_+3A_lambda">lambda</code></td>
<td>
<p>a number or a vector of lambda-value to be evaluated in the regression</p>
</td></tr>
<tr><td><code id="l_CV_+3A_k">k</code></td>
<td>
<p>integer, which indicates how many training and test set will be splited from the dataset</p>
</td></tr>
<tr><td><code id="l_CV_+3A_mode">mode</code></td>
<td>
<p>1: ridge regression; 2: lasso regression</p>
</td></tr>
<tr><td><code id="l_CV_+3A_binary">binary</code></td>
<td>
<p>logical, if TRUE, the dependent variable is binary</p>
</td></tr>
<tr><td><code id="l_CV_+3A_step">step</code></td>
<td>
<p>maximum number of steps</p>
</td></tr>
<tr><td><code id="l_CV_+3A_bound">bound</code></td>
<td>
<p>threshold for binary dependent variable</p>
</td></tr>
<tr><td><code id="l_CV_+3A_fista">fista</code></td>
<td>
<p>logical, if TRUE, the FISTA algorithm is used</p>
</td></tr>
<tr><td><code id="l_CV_+3A_tol">tol</code></td>
<td>
<p>tolerance for convergence, it is 10^-7 by default</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the lambda values that minimize the MSE
</p>


<h3>Examples</h3>

<pre><code class='language-R'>l_CV(mtcars,"hp",c("mpg","qsec","disp"),c(0.01,0.1),k=5,mode=2)
</code></pre>

<hr>
<h2 id='lasso_fista'>Lasso regression with fixed step with FISTA algorithm</h2><span id='topic+lasso_fista'></span>

<h3>Description</h3>

<p>the function carries out the Lasso regression using fixed step using FISTA algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lasso_fista(data,y,x,lambda,max_step=10000,type="Gaussian",image=TRUE,ini=0.5,tol=10^-7)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lasso_fista_+3A_data">data</code></td>
<td>
<p>name of the dataset</p>
</td></tr>
<tr><td><code id="lasso_fista_+3A_y">y</code></td>
<td>
<p>name of the dependent variables</p>
</td></tr>
<tr><td><code id="lasso_fista_+3A_x">x</code></td>
<td>
<p>name of the independent variable</p>
</td></tr>
<tr><td><code id="lasso_fista_+3A_lambda">lambda</code></td>
<td>
<p>a vector of lambda-value to be evaluated in the regression</p>
</td></tr>
<tr><td><code id="lasso_fista_+3A_max_step">max_step</code></td>
<td>
<p>maximum number of steps</p>
</td></tr>
<tr><td><code id="lasso_fista_+3A_type">type</code></td>
<td>
<p>type of response variable, by default, it is 'Gaussian' for continuos response and can be modified as 'Binomial' for binary response</p>
</td></tr>
<tr><td><code id="lasso_fista_+3A_image">image</code></td>
<td>
<p>logical, if TRUE, the evolution of errors in term of lambda values will be plotted</p>
</td></tr>
<tr><td><code id="lasso_fista_+3A_ini">ini</code></td>
<td>
<p>initial value for the coefficients</p>
</td></tr>
<tr><td><code id="lasso_fista_+3A_tol">tol</code></td>
<td>
<p>tolerance for convergence, it is 10^-7 by default</p>
</td></tr>
</table>


<h3>Details</h3>

<p>lasso_fista
</p>


<h3>Value</h3>

<p>A list containing:
</p>

<ul>
<li><p><code>coefficients</code>: A matrix where each column represents the estimated regression coefficients for a different lambda value.
</p>
</li>
<li><p><code>error_evolution</code>: A numeric vector tracking the error at certain step.
</p>
</li>
<li><p><code>num_steps</code>: An integer vector indicating the number of steps in which errors are calculated.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>library("glmnet")
data("QuickStartExample")
test&lt;-as.data.frame(cbind(QuickStartExample$y,QuickStartExample$x))
lasso_fista(test,"V1",colnames(test)[2:21],lambda=0.1,image=TRUE,max_step=1000)
</code></pre>

<hr>
<h2 id='lasso_fista_back'>Lasso regression with backtraking line research with FISTA algorithm</h2><span id='topic+lasso_fista_back'></span>

<h3>Description</h3>

<p>the function carries out the Lasso regression using backtraking line research and FISTA algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lasso_fista_back(data,y,x,lambda,max_step=10000,tol=10^-7,
type="Gaussian",ini=0.5,image=TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lasso_fista_back_+3A_data">data</code></td>
<td>
<p>name of the dataset</p>
</td></tr>
<tr><td><code id="lasso_fista_back_+3A_y">y</code></td>
<td>
<p>name of the dependent variables</p>
</td></tr>
<tr><td><code id="lasso_fista_back_+3A_x">x</code></td>
<td>
<p>name of the independent variable</p>
</td></tr>
<tr><td><code id="lasso_fista_back_+3A_lambda">lambda</code></td>
<td>
<p>a vector of lambda-value to be evaluated in the regression</p>
</td></tr>
<tr><td><code id="lasso_fista_back_+3A_max_step">max_step</code></td>
<td>
<p>maximum number of steps</p>
</td></tr>
<tr><td><code id="lasso_fista_back_+3A_tol">tol</code></td>
<td>
<p>tolerance for convergence, it is 10^-7 by default</p>
</td></tr>
<tr><td><code id="lasso_fista_back_+3A_type">type</code></td>
<td>
<p>type of response variable, by default, it is 'Gaussian' for continuos response and can be modified as 'Binomial' for binary response</p>
</td></tr>
<tr><td><code id="lasso_fista_back_+3A_ini">ini</code></td>
<td>
<p>initial value for the coefficients, default is 0.5</p>
</td></tr>
<tr><td><code id="lasso_fista_back_+3A_image">image</code></td>
<td>
<p>plots the evolution of errors in term of lambda values</p>
</td></tr>
</table>


<h3>Details</h3>

<p>lasso_fista_back
</p>


<h3>Value</h3>

<p>A list containing:
</p>

<ul>
<li><p><code>coefficients</code>: A matrix where each column represents the estimated regression coefficients for a different lambda value.
</p>
</li>
<li><p><code>error_evolution</code>: A numeric vector tracking the error at certain step.
</p>
</li>
<li><p><code>num_steps</code>: An integer vector indicating the number of steps in which errors are calculated.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>library("glmnet")
data("QuickStartExample")
test&lt;-as.data.frame(cbind(QuickStartExample$y,QuickStartExample$x))
lasso_fista_back(test,"V1",colnames(test)[2:21],lambda=0.1,image=TRUE,type='Gaussian',max_step=1000)
</code></pre>

<hr>
<h2 id='lasso_ista'>Lasso regression with fixed step with ISTA algorithm</h2><span id='topic+lasso_ista'></span>

<h3>Description</h3>

<p>the function carries out the Lasso regression using fixed step using ISTA algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lasso_ista(data,y,x,lambda,max_step=10000,type="Gaussian",image=TRUE,tol=10^-7,ini=0.5)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lasso_ista_+3A_data">data</code></td>
<td>
<p>name of the dataset</p>
</td></tr>
<tr><td><code id="lasso_ista_+3A_y">y</code></td>
<td>
<p>name of the dependent variables</p>
</td></tr>
<tr><td><code id="lasso_ista_+3A_x">x</code></td>
<td>
<p>name of the independent variable</p>
</td></tr>
<tr><td><code id="lasso_ista_+3A_lambda">lambda</code></td>
<td>
<p>a vector of lambda-value to be evaluated in the regression</p>
</td></tr>
<tr><td><code id="lasso_ista_+3A_max_step">max_step</code></td>
<td>
<p>maximum number of steps</p>
</td></tr>
<tr><td><code id="lasso_ista_+3A_type">type</code></td>
<td>
<p>type of response variable, by default, it is 'Gaussian' for continuos response and can be modified as 'Binomial' for binary response</p>
</td></tr>
<tr><td><code id="lasso_ista_+3A_image">image</code></td>
<td>
<p>logical, if TRUE, the evolution of errors in term of lambda values will be plotted</p>
</td></tr>
<tr><td><code id="lasso_ista_+3A_tol">tol</code></td>
<td>
<p>tolerance for convergence, it is 10^-7 by default</p>
</td></tr>
<tr><td><code id="lasso_ista_+3A_ini">ini</code></td>
<td>
<p>initial value for the coefficients</p>
</td></tr>
</table>


<h3>Details</h3>

<p>lasso_ista
</p>


<h3>Value</h3>

<p>A list containing:
</p>

<ul>
<li><p><code>coefficients</code>: A matrix where each column represents the estimated regression coefficients for a different lambda value.
</p>
</li>
<li><p><code>error_evolution</code>: A numeric vector tracking the error at certain step.
</p>
</li>
<li><p><code>num_steps</code>: An integer vector indicating the number of steps in which errors are calculated.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>library("glmnet")
data("QuickStartExample")
test&lt;-as.data.frame(cbind(QuickStartExample$y,QuickStartExample$x))
lasso_ista(test,"V1",colnames(test)[2:21],lambda=0.1,image=TRUE,max_step=1000)
</code></pre>

<hr>
<h2 id='lasso_ista_back'>Lasso regression with backtraking line research</h2><span id='topic+lasso_ista_back'></span>

<h3>Description</h3>

<p>the function carries out the Lasso regression using backtraking line research and ISTA algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lasso_ista_back(data,y,x,lambda,max_step=10000,tol=10^-7,
type="Gaussian",ini=0.5,image=TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lasso_ista_back_+3A_data">data</code></td>
<td>
<p>name of the dataset</p>
</td></tr>
<tr><td><code id="lasso_ista_back_+3A_y">y</code></td>
<td>
<p>name of the dependent variables</p>
</td></tr>
<tr><td><code id="lasso_ista_back_+3A_x">x</code></td>
<td>
<p>name of the independent variable</p>
</td></tr>
<tr><td><code id="lasso_ista_back_+3A_lambda">lambda</code></td>
<td>
<p>a vector of lambda-value to be evaluated in the regression</p>
</td></tr>
<tr><td><code id="lasso_ista_back_+3A_max_step">max_step</code></td>
<td>
<p>maximum number of steps</p>
</td></tr>
<tr><td><code id="lasso_ista_back_+3A_tol">tol</code></td>
<td>
<p>tolerance for convergence, it is 10^-7 by default</p>
</td></tr>
<tr><td><code id="lasso_ista_back_+3A_type">type</code></td>
<td>
<p>type of response variable, by default, it is 'Gaussian' for continuos response and can be modified as 'Binomial' for binary response</p>
</td></tr>
<tr><td><code id="lasso_ista_back_+3A_ini">ini</code></td>
<td>
<p>initial value for the coefficients,dafault is 0.5</p>
</td></tr>
<tr><td><code id="lasso_ista_back_+3A_image">image</code></td>
<td>
<p>plots the evolution of errors in term of lambda values</p>
</td></tr>
</table>


<h3>Details</h3>

<p>lasso_ista_back
</p>


<h3>Value</h3>

<p>A list containing:
</p>

<ul>
<li><p><code>coefficients</code>: A matrix where each column represents the estimated regression coefficients for a different lambda value.
</p>
</li>
<li><p><code>error_evolution</code>: A numeric vector tracking the error at certain step.
</p>
</li>
<li><p><code>num_steps</code>: An integer vector indicating the number of steps in which errors are calculated.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>library("glmnet")
data("QuickStartExample")
test&lt;-as.data.frame(cbind(QuickStartExample$y,QuickStartExample$x))
lasso_ista_back(test,"V1",colnames(test)[2:21],lambda=0.1,image=TRUE,type='Gaussian',max_step=100)
</code></pre>

<hr>
<h2 id='lasso_multi'>Lasso logistic regression for multinomial response variable with fixed step</h2><span id='topic+lasso_multi'></span>

<h3>Description</h3>

<p>the function realizes L1-regularized classification for multinomial response variable using ISTA / FISTA algorithm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lasso_multi(data,y,x,lambda,max_step=10000,image=FALSE,fista=TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lasso_multi_+3A_data">data</code></td>
<td>
<p>name of the dataset</p>
</td></tr>
<tr><td><code id="lasso_multi_+3A_y">y</code></td>
<td>
<p>name of the dependent variables</p>
</td></tr>
<tr><td><code id="lasso_multi_+3A_x">x</code></td>
<td>
<p>name of the independent variable</p>
</td></tr>
<tr><td><code id="lasso_multi_+3A_lambda">lambda</code></td>
<td>
<p>a number or a vector of lambda-value to be evaluated in the regression</p>
</td></tr>
<tr><td><code id="lasso_multi_+3A_max_step">max_step</code></td>
<td>
<p>maximum number of steps</p>
</td></tr>
<tr><td><code id="lasso_multi_+3A_image">image</code></td>
<td>
<p>plots the evolution of errors in term of lambda values</p>
</td></tr>
<tr><td><code id="lasso_multi_+3A_fista">fista</code></td>
<td>
<p>fista=TRUE: use FISTA algortihm for the multiclass logistic regression; fista=FALSE: use ISTA algortihm</p>
</td></tr>
</table>


<h3>Details</h3>

<p>lasso_multi
</p>


<h3>Value</h3>

<p>A list containing:
</p>

<ul>
<li><p><code>coefficients</code>: A matrix where each column represents the estimated regression coefficients for a different lambda value.
</p>
</li>
<li><p><code>error_evolution</code>: A numeric vector tracking the error at certain step.
</p>
</li>
<li><p><code>num_steps</code>: An integer vector indicating the number of steps in which errors are calculated.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>library(glmnet)
data("MultinomialExample")
x&lt;-MultinomialExample$x
y&lt;-MultinomialExample$y
mult&lt;-as.data.frame(cbind(x,y))
lasso_multi(mult,y="y",x=colnames(mult)[-31],max_step = 1000,lambda=0.01,image=TRUE,fista=TRUE)
</code></pre>

<hr>
<h2 id='lasso_multi_back'>Lasso regression with backtraking line research for multinomial response variable</h2><span id='topic+lasso_multi_back'></span>

<h3>Description</h3>

<p>the function carries out the Lasso regression for multinomial response using backtraking line research and FISTA/ISTA algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lasso_multi_back(data,y,x,lambda,max_step=10000,image=FALSE,fista=TRUE,tol=10^-7,ini=0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lasso_multi_back_+3A_data">data</code></td>
<td>
<p>name of the dataset</p>
</td></tr>
<tr><td><code id="lasso_multi_back_+3A_y">y</code></td>
<td>
<p>name of the dependent variables</p>
</td></tr>
<tr><td><code id="lasso_multi_back_+3A_x">x</code></td>
<td>
<p>name of the independent variable</p>
</td></tr>
<tr><td><code id="lasso_multi_back_+3A_lambda">lambda</code></td>
<td>
<p>a vector of lambda-value to be evaluated in the regression</p>
</td></tr>
<tr><td><code id="lasso_multi_back_+3A_max_step">max_step</code></td>
<td>
<p>maximum number of steps</p>
</td></tr>
<tr><td><code id="lasso_multi_back_+3A_image">image</code></td>
<td>
<p>plots the evolution of errors in term of lambda values</p>
</td></tr>
<tr><td><code id="lasso_multi_back_+3A_fista">fista</code></td>
<td>
<p>fista=TRUE: use FISTA algortihm for the multiclass logistic regression; fista=FALSE: use ISTA algortihm</p>
</td></tr>
<tr><td><code id="lasso_multi_back_+3A_tol">tol</code></td>
<td>
<p>tolerance for the convergence</p>
</td></tr>
<tr><td><code id="lasso_multi_back_+3A_ini">ini</code></td>
<td>
<p>initial value for the coefficients, default is 0
#'@examples
library(glmnet)
data(&quot;MultinomialExample&quot;)
x&lt;-MultinomialExample$x
y&lt;-MultinomialExample$y
mult&lt;-as.data.frame(cbind(x,y))
lasso_multi_back(mult,y=&quot;y&quot;,x=colnames(mult)[-31],max_step = 1000,lambda=0.01,image=TRUE,fista=TRUE,ini=0)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>lasso_multi_back
</p>


<h3>Value</h3>

<p>A list containing:
</p>

<ul>
<li><p><code>coefficients</code>: A matrix where each column represents the estimated regression coefficients for a different lambda value.
</p>
</li>
<li><p><code>error_evolution</code>: A numeric vector tracking the error at certain step.
</p>
</li>
<li><p><code>num_steps</code>: An integer vector indicating the number of steps in which errors are calculated.
</p>
</li></ul>


<hr>
<h2 id='ols'>Ordinary Least Square regression</h2><span id='topic+ols'></span>

<h3>Description</h3>

<p>This is a function that estimates coefficients for a linear model using Ordinary Least Squares (OLS) regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols(data,y,x,alpha=0.025,verbose=TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ols_+3A_data">data</code></td>
<td>
<p>Dataset used to estimated the coefficients</p>
</td></tr>
<tr><td><code id="ols_+3A_y">y</code></td>
<td>
<p>name of the dependent variable</p>
</td></tr>
<tr><td><code id="ols_+3A_x">x</code></td>
<td>
<p>name or a vector of names of the independent variables</p>
</td></tr>
<tr><td><code id="ols_+3A_alpha">alpha</code></td>
<td>
<p>confedence level</p>
</td></tr>
<tr><td><code id="ols_+3A_verbose">verbose</code></td>
<td>
<p>logical, if TRUE, the table will be printed</p>
</td></tr>
</table>


<h3>Value</h3>

<p>coefficients of the linear model, or a table with the coefficients, standard errors, t-values, p-values and confidence intervals
</p>


<h3>Examples</h3>

<pre><code class='language-R'>df = data.frame("hours"=c(1, 2, 4, 5, 5, 6, 6, 7, 8, 10, 11, 11, 12, 12, 14),
"score"=c(64, 66, 76, 73, 74, 81, 83, 82, 80, 88, 84, 82, 91, 93, 89))
ols(df,"score","hours")
</code></pre>

<hr>
<h2 id='ols_KCV'>K-Fold Cross Validation for OLS</h2><span id='topic+ols_KCV'></span>

<h3>Description</h3>

<p>ols_KCV makes the K-Fold Cross Validation for ordinary least squared regression
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ols_KCV(data,k,y,x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ols_KCV_+3A_data">data</code></td>
<td>
<p>full dataset which will be used for KCV</p>
</td></tr>
<tr><td><code id="ols_KCV_+3A_k">k</code></td>
<td>
<p>integer, which indicates how many training and test set will be splited from the dataset</p>
</td></tr>
<tr><td><code id="ols_KCV_+3A_y">y</code></td>
<td>
<p>dependent variable</p>
</td></tr>
<tr><td><code id="ols_KCV_+3A_x">x</code></td>
<td>
<p>independent variables</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the root mean square error after K-Fold Cross Validation on training set
</p>


<h3>Examples</h3>

<pre><code class='language-R'>df&lt;-mtcars
ols_KCV(mtcars,5,"hp",c("mpg","qsec","disp"))
</code></pre>

<hr>
<h2 id='ridge'>Ridge regression</h2><span id='topic+ridge'></span>

<h3>Description</h3>

<p>ridge function estimates the coefficients for a linear model using Ridge regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ridge(data,y,x,lambda)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ridge_+3A_data">data</code></td>
<td>
<p>name of the dataset</p>
</td></tr>
<tr><td><code id="ridge_+3A_y">y</code></td>
<td>
<p>name of dependent variables</p>
</td></tr>
<tr><td><code id="ridge_+3A_x">x</code></td>
<td>
<p>name of independent variable</p>
</td></tr>
<tr><td><code id="ridge_+3A_lambda">lambda</code></td>
<td>
<p>a numeric value or a numeric vector to penalize the squared residual</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix with the coefficients for each lambda
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ridge(mtcars,"hp",c("mpg","qsec","disp"),c(0.01,0.1))
</code></pre>

<hr>
<h2 id='softmax'>Softmax function for multinomial response variable</h2><span id='topic+softmax'></span>

<h3>Description</h3>

<p>the function calculates the softmax function for the multinomial response variable
</p>


<h3>Usage</h3>

<pre><code class='language-R'>softmax(num)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="softmax_+3A_num">num</code></td>
<td>
<p>A numeric matrix or vector</p>
</td></tr>
</table>


<h3>Details</h3>

<p>softmax
</p>


<h3>Value</h3>

<p>A numeric matrix or vector of the same shape as num, where each element represents a probability value between 0 and 1. The values sum to 1 across each row or the entire vector.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
