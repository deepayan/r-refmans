<!DOCTYPE html><html lang="en"><head><title>Help for package bnclassify</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {bnclassify}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bnclassify'><p>Learn discrete Bayesian network classifiers from data.</p></a></li>
<li><a href='#accuracy'><p>Compute predictive accuracy.</p></a></li>
<li><a href='#aode'><p>Learn an AODE ensemble.</p></a></li>
<li><a href='#are_factors'><p>Checks if all columns in a data frame are factors.</p></a></li>
<li><a href='#are_pdists'><p>Returns <code>TRUE</code> is <code>x</code> is a valid probability distribution.</p></a></li>
<li><a href='#as_mlr'><p>Convert to <code>mlr</code>.</p></a></li>
<li><a href='#augment_kdb'><p>Arcs that do not invalidate the k-DB structure</p></a></li>
<li><a href='#augment_kdb_arcs'><p>Returns augmenting arcs that do not invalidate the k-DB.</p></a></li>
<li><a href='#augment_ode'><p>Arcs that do not invalidate the tree-like structure</p></a></li>
<li><a href='#augment_ode_arcs'><p>Returns augmenting arcs that do not invalidate the ODE.</p></a></li>
<li><a href='#bnc'><p>Learn network structure and parameters.</p></a></li>
<li><a href='#bnc_aode'><p>Returns a <code>c("bnc_aode", "bnc")</code> object.</p></a></li>
<li><a href='#bnc_aode_bns'><p>Fits an AODE model.</p></a></li>
<li><a href='#bnc_bn'><p>Bayesian network classifier with structure and parameters.</p></a></li>
<li><a href='#bnc_dag'><p>Bayesian network classifier structure.</p></a></li>
<li><a href='#bootstrap_ss'><p>Return a bootstrap sub-sample.</p></a></li>
<li><a href='#car'><p>Car Evaluation Data Set.</p></a></li>
<li><a href='#check_mlr_attached'><p>Checks if mlr attached.</p></a></li>
<li><a href='#cmi'><p>Compute the (conditional) mutual information between two variables.</p></a></li>
<li><a href='#cmi_table'><p>Returns the conditional mutual information three variables.</p></a></li>
<li><a href='#complete_graph'><p>Returns a complete unweighted graph with the given nodes.</p></a></li>
<li><a href='#compute_cll'><p>Computes the conditional log-likelihood of the model on the provided data.</p></a></li>
<li><a href='#compute_ll'><p>Computes log-likelihood of the model on the provided data.</p></a></li>
<li><a href='#compute_wanbia_weights'><p>Compute WANBIA weights.</p>
</p>
<p>Computes feature weights by optimizing conditional log-likelihood.</p>
Weights are bounded to [0, 1]. Implementation based on the original paper
and the code provided at <a href="https://sourceforge.net/projects/rawnaivebayes">https://sourceforge.net/projects/rawnaivebayes</a>.</a></li>
<li><a href='#cpt_vars_values'><p>Get just form first dimension in their own cpt, not checking for consistency</p>
in others.</a></li>
<li><a href='#cv'><p>Estimate predictive accuracy with stratified cross validation.</p></a></li>
<li><a href='#dag'><p>Get underlying graph. This should be exported.</p></a></li>
<li><a href='#direct_forest'><p>Direct an undirected graph.</p></a></li>
<li><a href='#direct_tree'><p>Direct an undirected graph.</p></a></li>
<li><a href='#extract_ctgt'><p>Returns a contingency table over the variables.</p></a></li>
<li><a href='#fast_equal'><p>Compares all elements in a to b</p></a></li>
<li><a href='#forget'><p>Forget a memoized function.</p></a></li>
<li><a href='#get_ancestors'><p>Based on gRbase::ancestors()</p></a></li>
<li><a href='#get_but_last'><p>Return all but last element of x.</p></a></li>
<li><a href='#get_last'><p>Return last element of x.</p></a></li>
<li><a href='#get_log_leaf_entries'><p>Assuming that the cpt is a leaf, returns 1 instead of a CPT entry when value missing</p></a></li>
<li><a href='#get_null_safe'><p>Get i-th element of x.</p></a></li>
<li><a href='#grain_and_graph'><p>Convert to igraph and gRain.</p></a></li>
<li><a href='#graph_add_edges'><p>Add edges</p>
Does not allow edges among adjacent nodes</a></li>
<li><a href='#graph_connected_components'><p>connected_components</p></a></li>
<li><a href='#graph_get_adjacent'><p>Finds adjacent nodes. Has not been tested much</p></a></li>
<li><a href='#graph_is_adjacent'><p>Checks whether nodes are adjacent</p></a></li>
<li><a href='#graph_named_edge_matrix'><p>Returns an edge matrix with node names (instead of node indices).</p></a></li>
<li><a href='#graph_subgraph'><p>Subgraph.</p>
Only for a directed graph?</a></li>
<li><a href='#graph_union'><p>Merges multiple disjoint graphs into a single one.</p></a></li>
<li><a href='#greedy_wrapper'><p>Learn Bayesian network classifiers in a a greedy wrapper fashion.</p></a></li>
<li><a href='#identify_all_testing_depths'><p>Identifies all depths at which the features of a classification tree are</p>
tested.</a></li>
<li><a href='#identify_min_testing_depths'><p>Identifies the lowest (closest to root) depths at which the features of a</p>
classification tree are tested.</a></li>
<li><a href='#inspect_bnc_bn'><p>Inspect a Bayesian network classifier (with structure and parameters).</p></a></li>
<li><a href='#inspect_bnc_dag'><p>Inspect a Bayesian network classifier structure.</p></a></li>
<li><a href='#is_aode'><p>Is it en AODE?</p></a></li>
<li><a href='#is.memoised'><p>Is it memoized?</p></a></li>
<li><a href='#learn_params'><p>Learn the parameters of a Bayesian network structure.</p></a></li>
<li><a href='#learn_unprunned_tree'><p>Learns a unpruned <code>rpart</code> recursive partition.</p></a></li>
<li><a href='#local_ode_score_contrib'><p>Returns pairwise component of ODE (penalized) log-likelihood scores.</p>
In natural logarithms.</a></li>
<li><a href='#log_normalize'><p>Normalize log probabilities.</p></a></li>
<li><a href='#loglik'><p>Compute (penalized) log-likelihood.</p></a></li>
<li><a href='#make_cll'><p>Returns a function to compute negative conditional log-likelihood given feature weights</p></a></li>
<li><a href='#make_cll_gradient'><p>Returns a function to compute the gradient of negative conditional log-likelihood with respect to feature weights</p></a></li>
<li><a href='#makeRLearner.bnc'><p>makeRLearner. Auxiliary mlr function.</p></a></li>
<li><a href='#map'><p>Assigns instances to the most likely class.</p></a></li>
<li><a href='#max_weight_forest'><p>Returns the undirected augmenting forest.</p></a></li>
<li><a href='#memoise_char'><p>Memoise a function.</p></a></li>
<li><a href='#nb'><p>Learn a naive Bayes network structure.</p></a></li>
<li><a href='#nb_dag'><p>Returns a naive Bayes structure</p></a></li>
<li><a href='#new_cache'><p>Make a new cache.</p></a></li>
<li><a href='#order_acyclic'><p>Provide an acyclic ordering (i.e., a topological sort).</p></a></li>
<li><a href='#plot.bnc_dag'><p>Plot the structure.</p></a></li>
<li><a href='#predict.bnc_fit'><p>Predicts class labels or class posterior probability distributions.</p></a></li>
<li><a href='#predictLearner.bnc'><p>predictLearner. Auxiliary mlr function.</p></a></li>
<li><a href='#print.bnc_base'><p>Print basic information about a classifier.</p></a></li>
<li><a href='#skip_assert'><p>Whether to do checks or not. Set TRUE to speed up debugging or building.</p></a></li>
<li><a href='#skip_testing'><p>Skip while testing to isolate errors</p></a></li>
<li><a href='#spode'><p>Returns a Superparent one-dependence estimator.</p></a></li>
<li><a href='#subset_by_colnames'><p>Subset a 2D structure by a vector of column names.</p></a></li>
<li><a href='#superparent_children'><p>Return nodes which can be superparents along with their possible children.</p></a></li>
<li><a href='#tan_chowliu'><p>Learns a one-dependence estimator using Chow-Liu's algorithm.</p></a></li>
<li><a href='#trainLearner.bnc'><p>trainLearner. Auxiliary mlr function.</p></a></li>
<li><a href='#voting'><p>Congress Voting Data Set.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Learning Discrete Bayesian Network Classifiers from Data</td>
</tr>
<tr>
<td>Description:</td>
<td>State-of-the art algorithms for learning discrete Bayesian network classifiers from data, including a number of those described in Bielza &amp; Larranaga (2014) &lt;<a href="https://doi.org/10.1145%2F2576868">doi:10.1145/2576868</a>&gt;, with functions for prediction, model evaluation and inspection.</td>
</tr>
<tr>
<td>Version:</td>
<td>0.4.8</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/bmihaljevic/bnclassify">https://github.com/bmihaljevic/bnclassify</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/bmihaljevic/bnclassify/issues">https://github.com/bmihaljevic/bnclassify/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.2.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>assertthat (&ge; 0.1), entropy(&ge; 1.2.0), matrixStats(&ge;
0.14.0), rpart(&ge; 4.1-8), Rcpp,</td>
</tr>
<tr>
<td>Suggests:</td>
<td>igraph, gRain(&ge; 1.2-3), gRbase(&ge; 1.7-0.1), mlr(&ge; 2.2),
testthat(&ge; 0.8.1), knitr(&ge; 1.10.5), ParamHelpers(&ge; 1.5),
rmarkdown(&ge; 0.7), mlbench, covr</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Mihaljevic Bojan &lt;boki.mihaljevic@gmail.com&gt;</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, BH</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-13 10:57:12 UTC; bmihaljevic</td>
</tr>
<tr>
<td>Author:</td>
<td>Mihaljevic Bojan [aut, cre, cph],
  Bielza Concha [aut],
  Larranaga Pedro [aut],
  Wickham Hadley [ctb] (some code extracted from memoise package)</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-13 12:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='bnclassify'>Learn discrete Bayesian network classifiers from data.</h2><span id='topic+bnclassify-package'></span><span id='topic+bnclassify'></span>

<h3>Description</h3>

<p>State-of-the-art algorithms for learning discrete Bayesian network 
classifiers from data, with functions prediction, model evaluation and inspection.
</p>


<h3>Details</h3>

<p>The learn more about the package, start with the vignettes:
<code>browseVignettes(package = "bnclassify")</code>. The following is a list of available
functionalities: 
</p>
<p>Structure learning algorithms:
</p>

<ul>
<li> <p><code><a href="#topic+nb">nb</a></code>: Naive Bayes (Minsky, 1961)
</p>
</li>
<li> <p><code><a href="#topic+tan_cl">tan_cl</a></code>: Chow-Liu's algorithm for one-dependence estimators (CL-ODE)  (Friedman et al., 1997)
</p>
</li>
<li> <p><code><a href="#topic+fssj">fssj</a></code>: Forward sequential selection and joining (FSSJ) (Pazzani, 1996)
</p>
</li>
<li> <p><code><a href="#topic+bsej">bsej</a></code>: Backward sequential elimination and joining (BSEJ)  (Pazzani, 1996)
</p>
</li>
<li> <p><code><a href="#topic+tan_hc">tan_hc</a></code>: Hill-climbing tree augmented naive Bayes (TAN-HC)  (Keogh and Pazzani, 2002)
</p>
</li>
<li> <p><code><a href="#topic+tan_hcsp">tan_hcsp</a></code>: Hill-climbing super-parent tree augmented naive Bayes (TAN-HCSP) (Keogh and Pazzani, 2002)
</p>
</li>
<li> <p><code><a href="#topic+aode">aode</a></code>: Averaged one-dependence estimators (AODE) (Webb et al., 2005)
</p>
</li></ul>

<p>Parameter learning methods (<code><a href="#topic+lp">lp</a></code>):
</p>

<ul>
<li><p> Bayesian and maximum likelihood estimation
</p>
</li>
<li><p> Weighting attributes to alleviate naive bayes' independence assumption (WANBIA) (Zaidi et al., 2013)
</p>
</li>
<li><p> Attribute-weighted naive Bayes (AWNB)  (Hall, 2007)
</p>
</li>
<li><p> Model averaged naive Bayes (MANB) (Dash and Cooper, 2002)
</p>
</li></ul>

<p>Model evaluating:
</p>

<ul>
<li> <p><code><a href="#topic+cv">cv</a></code>: Cross-validated estimate of accuracy 
</p>
</li>
<li> <p><code><a href="#topic+logLik.bnc_bn">logLik</a></code>: Log-likelihood
</p>
</li>
<li> <p><code><a href="#topic+AIC.bnc_bn">AIC</a></code>: Akaike's information criterion (AIC) 
</p>
</li>
<li> <p><code><a href="#topic+BIC.bnc_bn">BIC</a></code>: Bayesian information criterion (BIC) 
</p>
</li></ul>

<p>Predicting: 
</p>

<ul>
<li> <p><code><a href="#topic+predict.bnc_fit">predict</a></code>: Inference for complete and/or incomplete data (the latter through <code>gRain</code>)</p>
</li></ul>

<p>Inspecting models:
</p>
 
<ul>
<li> <p><code><a href="#topic+plot.bnc_dag">plot</a></code>: Structure plotting (through <code>igraph</code>)
</p>
</li>
<li> <p><code><a href="#topic+print.bnc_base">print</a></code>: Summary 
</p>
</li>
<li> <p><code><a href="#topic+params">params</a></code>: Access conditional probability tables 
</p>
</li>
<li> <p><code><a href="#topic+nparams">nparams</a></code>: Number of free parameters 
</p>
</li>
<li><p> and more. See <code><a href="#topic+inspect_bnc_dag">inspect_bnc_dag</a></code> and <code><a href="#topic+inspect_bnc_bn">inspect_bnc_bn</a></code>.
</p>
</li></ul>



<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Mihaljevic Bojan <a href="mailto:boki.mihaljevic@gmail.com">boki.mihaljevic@gmail.com</a> [copyright holder]
</p>
<p>Authors:
</p>

<ul>
<li><p> Bielza Concha <a href="mailto:mcbielza@fi.upm.es">mcbielza@fi.upm.es</a>
</p>
</li>
<li><p> Larranaga Pedro <a href="mailto:pedro.larranaga@fi.upm.es">pedro.larranaga@fi.upm.es</a>
</p>
</li></ul>

<p>Other contributors:
</p>

<ul>
<li><p> Wickham Hadley (some code extracted from memoise package) [contributor]
</p>
</li></ul>



<h3>References</h3>

<p>Bielza C and Larranaga P (2014), Discrete Bayesian network 
classifiers: A survey. <em>ACM Computing Surveys</em>, <b>47</b>(1), Article 
5.
</p>
<p>Dash D and Cooper GF (2002). Exact model averaging with naive Bayesian 
classifiers. <em>19th International Conference on Machine Learning 
(ICML-2002)</em>, 91-98.
</p>
<p>Friedman N, Geiger D and Goldszmidt M (1997). Bayesian network classifiers.
<em>Machine Learning</em>, <b>29</b>, pp. 131&ndash;163.
</p>
<p>Zaidi NA, Cerquides J, Carman MJ, and Webb GI (2013) Alleviating naive Bayes 
attribute independence assumption by attribute weighting.
<em>Journal of Machine Learning Research</em>, <b>14</b> pp. 1947&ndash;1988.  
</p>
<p>GI. Webb, JR Boughton, and Z Wang (2005) Not so naive bayes: Aggregating one-dependence 
estimators. <em>Machine Learning</em>, <b>58</b>(1) pp. 5&ndash;24.  
</p>
<p>Hall M (2007). A decision tree-based attribute weighting filter for naive 
Bayes. <em>Knowledge-Based Systems</em>, <b>20</b>(2), pp. 120-126.
</p>
<p>Koegh E and Pazzani M (2002).Learning the structure of augmented Bayesian 
classifiers. In <em>International Journal on Artificial Intelligence 
Tools</em>, <b>11</b>(4), pp. 587-601.
</p>
<p>Koller D, Friedman N (2009). Probabilistic Graphical Models: Principles and
Techniques. MIT Press.
</p>
<p>Pazzani M (1996). Constructive induction of Cartesian product attributes. 
In <em>Proceedings of the Information, Statistics and Induction in 
Science Conference (ISIS-1996)</em>, pp. 66-77
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/bmihaljevic/bnclassify">https://github.com/bmihaljevic/bnclassify</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/bmihaljevic/bnclassify/issues">https://github.com/bmihaljevic/bnclassify/issues</a>
</p>
</li></ul>


<hr>
<h2 id='accuracy'>Compute predictive accuracy.</h2><span id='topic+accuracy'></span>

<h3>Description</h3>

<p>Compute predictive accuracy.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>accuracy(x, y)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="accuracy_+3A_x">x</code></td>
<td>
<p>A vector of predicted labels.</p>
</td></tr>
<tr><td><code id="accuracy_+3A_y">y</code></td>
<td>
<p>A vector of true labels.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'> 
data(car)
nb &lt;- bnc('nb', 'class', car, smooth = 1)
p &lt;- predict(nb, car)
accuracy(p, car$class)
</code></pre>

<hr>
<h2 id='aode'>Learn an AODE ensemble.</h2><span id='topic+aode'></span>

<h3>Description</h3>

<p>If there is a single predictor then returns a naive Bayes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aode(class, dataset, features = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="aode_+3A_class">class</code></td>
<td>
<p>A character. Name of the class variable.</p>
</td></tr>
<tr><td><code id="aode_+3A_dataset">dataset</code></td>
<td>
<p>The data frame from which to learn the classifier.</p>
</td></tr>
<tr><td><code id="aode_+3A_features">features</code></td>
<td>
<p>A character vector. The names of the features. This argument
is ignored if <code>dataset</code> is provided.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>bnc_aode</code> or a <code>bnc_dag</code> (if returning a naive Bayes)
</p>

<hr>
<h2 id='are_factors'>Checks if all columns in a data frame are factors.</h2><span id='topic+are_factors'></span>

<h3>Description</h3>

<p>Checks if all columns in a data frame are factors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>are_factors(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="are_factors_+3A_x">x</code></td>
<td>
<p>a <code>data.frame</code></p>
</td></tr>
</table>

<hr>
<h2 id='are_pdists'>Returns <code>TRUE</code> is <code>x</code> is a valid probability distribution.</h2><span id='topic+are_pdists'></span>

<h3>Description</h3>

<p>Returns <code>TRUE</code> is <code>x</code> is a valid probability distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>are_pdists(x)
</code></pre>

<hr>
<h2 id='as_mlr'>Convert to <code>mlr</code>.</h2><span id='topic+as_mlr'></span>

<h3>Description</h3>

<p>Convert a <code><a href="#topic+bnc_bn">bnc_bn</a></code> to a <code><a href="mlr.html#topic+Learner">Learner</a></code>
object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_mlr(x, dag, id = "1")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="as_mlr_+3A_x">x</code></td>
<td>
<p>A <code><a href="#topic+bnc_bn">bnc_bn</a></code> object.</p>
</td></tr>
<tr><td><code id="as_mlr_+3A_dag">dag</code></td>
<td>
<p>A logical. Whether to learn structure on each training subsample. 
Parameters are always learned.</p>
</td></tr>
<tr><td><code id="as_mlr_+3A_id">id</code></td>
<td>
<p>A character.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(car)
nb &lt;- bnc('nb', 'class', car, smooth = 1)
## Not run: library(mlr)
## Not run: nb_mlr &lt;- as_mlr(nb, dag = FALSE, id = "ode_cl_aic")
## Not run: nb_mlr
</code></pre>

<hr>
<h2 id='augment_kdb'>Arcs that do not invalidate the k-DB structure</h2><span id='topic+augment_kdb'></span>

<h3>Description</h3>

<p>Arcs that do not invalidate the k-DB structure
</p>


<h3>Usage</h3>

<pre><code class='language-R'>augment_kdb(kdbk)
</code></pre>

<hr>
<h2 id='augment_kdb_arcs'>Returns augmenting arcs that do not invalidate the k-DB.</h2><span id='topic+augment_kdb_arcs'></span>

<h3>Description</h3>

<p>Returns augmenting arcs that do not invalidate the k-DB.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>augment_kdb_arcs(bnc_dag, k)
</code></pre>


<h3>Value</h3>

<p>a character matrix. NULL if no arcs can be added.
</p>

<hr>
<h2 id='augment_ode'>Arcs that do not invalidate the tree-like structure</h2><span id='topic+augment_ode'></span>

<h3>Description</h3>

<p>Arcs that do not invalidate the tree-like structure
</p>


<h3>Usage</h3>

<pre><code class='language-R'>augment_ode(bnc_dag, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="augment_ode_+3A_...">...</code></td>
<td>
<p>Ignored.</p>
</td></tr>
</table>

<hr>
<h2 id='augment_ode_arcs'>Returns augmenting arcs that do not invalidate the ODE.</h2><span id='topic+augment_ode_arcs'></span>

<h3>Description</h3>

<p>Returns augmenting arcs that do not invalidate the ODE.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>augment_ode_arcs(bnc_dag)
</code></pre>


<h3>Value</h3>

<p>a character matrix. NULL if no arcs can be added.
</p>

<hr>
<h2 id='bnc'>Learn network structure and parameters.</h2><span id='topic+bnc'></span>

<h3>Description</h3>

<p>A convenience function to learn the structure and parameters in a single 
call. Must provide the name of the structure learning algorithm function;
see <code><a href="#topic+bnclassify">bnclassify</a></code> for the list.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bnc(
  dag_learner,
  class,
  dataset,
  smooth,
  dag_args = NULL,
  awnb_trees = NULL,
  awnb_bootstrap = NULL,
  manb_prior = NULL,
  wanbia = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bnc_+3A_dag_learner">dag_learner</code></td>
<td>
<p>A character. Name of the structure learning function.</p>
</td></tr>
<tr><td><code id="bnc_+3A_class">class</code></td>
<td>
<p>A character. Name of the class variable.</p>
</td></tr>
<tr><td><code id="bnc_+3A_dataset">dataset</code></td>
<td>
<p>The data frame from which to learn network structure and 
parameters.</p>
</td></tr>
<tr><td><code id="bnc_+3A_smooth">smooth</code></td>
<td>
<p>A numeric. The smoothing value (<code class="reqn">\alpha</code>) for Bayesian 
parameter estimation. Nonnegative.</p>
</td></tr>
<tr><td><code id="bnc_+3A_dag_args">dag_args</code></td>
<td>
<p>A list. Optional additional arguments to <code>dag_learner</code>.</p>
</td></tr>
<tr><td><code id="bnc_+3A_awnb_trees">awnb_trees</code></td>
<td>
<p>An integer. The number (<code class="reqn">M</code>) of bootstrap samples to 
generate.</p>
</td></tr>
<tr><td><code id="bnc_+3A_awnb_bootstrap">awnb_bootstrap</code></td>
<td>
<p>A numeric. The size of the bootstrap subsample, 
relative to the size of <code>dataset</code> (given in [0,1]).</p>
</td></tr>
<tr><td><code id="bnc_+3A_manb_prior">manb_prior</code></td>
<td>
<p>A numeric. The prior probability for an arc between the 
class and any feature.</p>
</td></tr>
<tr><td><code id="bnc_+3A_wanbia">wanbia</code></td>
<td>
<p>A logical. If <code>TRUE</code>, WANBIA feature weighting is
performed.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(car)
nb &lt;- bnc('nb', 'class', car, smooth = 1)
nb_manb &lt;- bnc('nb', 'class', car, smooth = 1, manb_prior = 0.3)
ode_cl_aic &lt;- bnc('tan_cl', 'class', car, smooth = 1, dag_args = list(score = 'aic'))
</code></pre>

<hr>
<h2 id='bnc_aode'>Returns a <code>c("bnc_aode", "bnc")</code> object.</h2><span id='topic+bnc_aode'></span>

<h3>Description</h3>

<p>Returns a <code>c("bnc_aode", "bnc")</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bnc_aode(models, class_var, features)
</code></pre>

<hr>
<h2 id='bnc_aode_bns'>Fits an AODE model.</h2><span id='topic+bnc_aode_bns'></span>

<h3>Description</h3>

<p>Fits an AODE model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bnc_aode_bns(x, fit_models)
</code></pre>

<hr>
<h2 id='bnc_bn'>Bayesian network classifier with structure and parameters.</h2><span id='topic+bnc_bn'></span>

<h3>Description</h3>

<p>A Bayesian network classifier with structure and parameters. Returned by 
<code><a href="#topic+lp">lp</a></code> and <code><a href="#topic+bnc">bnc</a></code> functions. You can use it to classify
data (with <code><a href="#topic+predict.bnc_fit">predict</a></code>). Can estimate its
predictive accuracy with <code><a href="#topic+cv">cv</a></code>, plot its structure (with 
<code><a href="#topic+plot.bnc_dag">plot</a></code>), print a summary to console 
(<code><a href="#topic+print.bnc_base">print</a></code>), inspect it with functions documented 
in <code><a href="#topic+inspect_bnc_bn">inspect_bnc_bn</a></code> and <code><a href="#topic+inspect_bnc_dag">inspect_bnc_dag</a></code>, and
convert it to mlr, grain, and graph objects &ndash;see <code><a href="#topic+as_mlr">as_mlr</a></code> and 
<code><a href="#topic+grain_and_graph">grain_and_graph</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(car)
tan &lt;- bnc('tan_cl', 'class', car, smooth = 1)   
tan
p &lt;- predict(tan, car)
head(p)
## Not run: plot(tan)
nparams(tan)
</code></pre>

<hr>
<h2 id='bnc_dag'>Bayesian network classifier structure.</h2><span id='topic+bnc_dag'></span>

<h3>Description</h3>

<p>A Bayesian network classifier structure, returned by functions such as 
<code><a href="#topic+nb">nb</a></code> and <code><a href="#topic+tan_cl">tan_cl</a></code>. You can plot its structure (with 
<code><a href="#topic+plot.bnc_dag">plot</a></code>), print a summary to console 
(<code><a href="#topic+print.bnc_base">print</a></code>), inspect it with functions documented
in <code><a href="#topic+inspect_bnc_dag">inspect_bnc_dag</a></code>, and convert it to a graph object with 
<code><a href="#topic+grain_and_graph">grain_and_graph</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(car)
nb &lt;- tan_cl('class', car)   
nb
## Not run: plot(nb)
narcs(nb)
</code></pre>

<hr>
<h2 id='bootstrap_ss'>Return a bootstrap sub-sample.</h2><span id='topic+bootstrap_ss'></span>

<h3>Description</h3>

<p>Return a bootstrap sub-sample.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bootstrap_ss(dataset, proportion)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bootstrap_ss_+3A_dataset">dataset</code></td>
<td>
<p>a <code>data.frame</code></p>
</td></tr>
<tr><td><code id="bootstrap_ss_+3A_proportion">proportion</code></td>
<td>
<p>numeric given as fraction of <code>dataset</code> size</p>
</td></tr>
</table>

<hr>
<h2 id='car'>Car Evaluation Data Set.</h2><span id='topic+car'></span>

<h3>Description</h3>

<p>Data set from the UCI repository:
<a href="https://archive.ics.uci.edu/ml/datasets/Car+Evaluation">https://archive.ics.uci.edu/ml/datasets/Car+Evaluation</a>.
</p>


<h3>Format</h3>

<p>A <code>data.frame</code> with 7 columns and 1728 rows.
</p>


<h3>Source</h3>

<p><a href="https://goo.gl/GTXrCz">https://goo.gl/GTXrCz</a>
</p>

<hr>
<h2 id='check_mlr_attached'>Checks if mlr attached.</h2><span id='topic+check_mlr_attached'></span>

<h3>Description</h3>

<p>mlr must be attached because otherwise  'getMlrOptions()' in 'makeLearner' will not be found.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_mlr_attached()
</code></pre>

<hr>
<h2 id='cmi'>Compute the (conditional) mutual information between two variables.</h2><span id='topic+cmi'></span>

<h3>Description</h3>

<p>Computes the (conditional) mutual information between two variables. If 
<code>z</code> is not <code>NULL</code> then returns the conditional mutual information,
<code class="reqn">I(X;Y|Z)</code>. Otherwise, returns mutual information, <code class="reqn">I(X;Y)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cmi(x, y, dataset, z = NULL, unit = "log")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cmi_+3A_x">x</code></td>
<td>
<p>A length one character.</p>
</td></tr>
<tr><td><code id="cmi_+3A_y">y</code></td>
<td>
<p>A length one character.</p>
</td></tr>
<tr><td><code id="cmi_+3A_dataset">dataset</code></td>
<td>
<p>A data frame. Must contain x, y and, optionally, z columns.</p>
</td></tr>
<tr><td><code id="cmi_+3A_z">z</code></td>
<td>
<p>A character vector.</p>
</td></tr>
<tr><td><code id="cmi_+3A_unit">unit</code></td>
<td>
<p>A character. Logarithm base. See <code>entropy</code> package.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code class="reqn">I(X;Y|Z) = H(X|Z) + H(Y|Z) - H(X,Y,Z) - H(Z)</code>, where <code class="reqn">H()</code> is 
Shannon's entropy.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(car)
cmi('maint', 'class', car)
</code></pre>

<hr>
<h2 id='cmi_table'>Returns the conditional mutual information three variables.</h2><span id='topic+cmi_table'></span>

<h3>Description</h3>

<p>Returns the conditional mutual information three variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cmi_table(xyz_freqs, unit = "log")
</code></pre>

<hr>
<h2 id='complete_graph'>Returns a complete unweighted graph with the given nodes.</h2><span id='topic+complete_graph'></span>

<h3>Description</h3>

<p>Returns a complete unweighted graph with the given nodes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>complete_graph(nodes)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="complete_graph_+3A_nodes">nodes</code></td>
<td>
<p>A character vector.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>graphNEL</code> object.
</p>

<hr>
<h2 id='compute_cll'>Computes the conditional log-likelihood of the model on the provided data.</h2><span id='topic+compute_cll'></span>

<h3>Description</h3>

<p>Computes the conditional log-likelihood of the model on the provided data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_cll(x, dataset)
</code></pre>

<hr>
<h2 id='compute_ll'>Computes log-likelihood of the model on the provided data.</h2><span id='topic+compute_ll'></span>

<h3>Description</h3>

<p>Computes log-likelihood of the model on the provided data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_ll(x, dataset)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="compute_ll_+3A_x">x</code></td>
<td>
<p>A <code><a href="#topic+bnc_bn">bnc_bn</a></code> object.</p>
</td></tr>
<tr><td><code id="compute_ll_+3A_dataset">dataset</code></td>
<td>
<p>A data frame.</p>
</td></tr>
</table>

<hr>
<h2 id='compute_wanbia_weights'>Compute WANBIA weights.
Computes feature weights by optimizing conditional log-likelihood.  
Weights are bounded to [0, 1]. Implementation based on the original paper 
and the code provided at <a href="https://sourceforge.net/projects/rawnaivebayes">https://sourceforge.net/projects/rawnaivebayes</a>.</h2><span id='topic+compute_wanbia_weights'></span>

<h3>Description</h3>

<p>Compute WANBIA weights.
</p>
<p>Computes feature weights by optimizing conditional log-likelihood.  
Weights are bounded to [0, 1]. Implementation based on the original paper 
and the code provided at <a href="https://sourceforge.net/projects/rawnaivebayes">https://sourceforge.net/projects/rawnaivebayes</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_wanbia_weights(class, dataset, return_optim_object = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="compute_wanbia_weights_+3A_class">class</code></td>
<td>
<p>A character. Name of the class variable.</p>
</td></tr>
<tr><td><code id="compute_wanbia_weights_+3A_dataset">dataset</code></td>
<td>
<p>The data frame from which to learn feature weights</p>
</td></tr>
<tr><td><code id="compute_wanbia_weights_+3A_return_optim_object">return_optim_object</code></td>
<td>
<p>Return full output of 'optim'</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named numeric vector
</p>

<hr>
<h2 id='cpt_vars_values'>Get just form first dimension in their own cpt, not checking for consistency
in others.</h2><span id='topic+cpt_vars_values'></span>

<h3>Description</h3>

<p>Get just form first dimension in their own cpt, not checking for consistency
in others.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cpt_vars_values(cpts)
</code></pre>

<hr>
<h2 id='cv'>Estimate predictive accuracy with stratified cross validation.</h2><span id='topic+cv'></span>

<h3>Description</h3>

<p>Estimate predictive accuracy of a classifier with stratified cross 
validation. It learns the models from the training subsamples by repeating 
the learning procedures used to obtain <code>x</code>. It can keep the network 
structure fixed and re-learn only the parameters, or re-learn both structure 
and parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv(x, dataset, k, dag = TRUE, mean = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cv_+3A_x">x</code></td>
<td>
<p>List of <code><a href="#topic+bnc_bn">bnc_bn</a></code> or a single 
<code><a href="#topic+bnc_bn">bnc_bn</a></code>. The classifiers to evaluate.</p>
</td></tr>
<tr><td><code id="cv_+3A_dataset">dataset</code></td>
<td>
<p>The data frame on which to evaluate the classifiers.</p>
</td></tr>
<tr><td><code id="cv_+3A_k">k</code></td>
<td>
<p>An integer. The number of folds.</p>
</td></tr>
<tr><td><code id="cv_+3A_dag">dag</code></td>
<td>
<p>A logical. Whether to learn structure on each training subsample. 
Parameters are always learned.</p>
</td></tr>
<tr><td><code id="cv_+3A_mean">mean</code></td>
<td>
<p>A logical. Whether to return mean accuracy for each classifier or
to return a k-row matrix with accuracies per fold.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector of same length as <code>x</code>, giving the predictive
accuracy of each classifier. If <code>mean = FALSE</code> then a matrix with k
rows and a column per each classifier in <code>x</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(car)
nb &lt;- bnc('nb', 'class', car, smooth = 1) 
# CV a single classifier
cv(nb, car, k = 10) 
nb_manb &lt;- bnc('nb', 'class', car, smooth = 1, manb_prior = 0.5) 
cv(list(nb=nb, manb=nb_manb), car, k = 10)
# Get accuracies on each fold
cv(list(nb=nb, manb=nb_manb), car, k = 10, mean = FALSE)
ode &lt;- bnc('tan_cl', 'class', car, smooth = 1, dag_args = list(score = 'aic')) 
# keep structure fixed accross training subsamples
cv(ode, car, k = 10, dag = FALSE)
</code></pre>

<hr>
<h2 id='dag'>Get underlying graph. This should be exported.</h2><span id='topic+dag'></span>

<h3>Description</h3>

<p>Get underlying graph. This should be exported.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dag(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dag_+3A_x">x</code></td>
<td>
<p>the bnc object</p>
</td></tr>
</table>

<hr>
<h2 id='direct_forest'>Direct an undirected graph.</h2><span id='topic+direct_forest'></span>

<h3>Description</h3>

<p>Starting from a <code>root</code> not, directs all arcs away from it and applies 
the same, recursively to its children and descendants. Produces a directed
forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>direct_forest(g, root = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="direct_forest_+3A_g">g</code></td>
<td>
<p>An undirected graph.</p>
</td></tr>
<tr><td><code id="direct_forest_+3A_root">root</code></td>
<td>
<p>A character. Optional tree root.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A directed graph
</p>

<hr>
<h2 id='direct_tree'>Direct an undirected graph.</h2><span id='topic+direct_tree'></span>

<h3>Description</h3>

<p>The graph must be connected and the function produces a directed tree.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>direct_tree(g, root = NULL)
</code></pre>


<h3>Value</h3>

<p>A graph. The directed tree.
</p>

<hr>
<h2 id='extract_ctgt'>Returns a contingency table over the variables.</h2><span id='topic+extract_ctgt'></span>

<h3>Description</h3>

<p>Each variable may be a character vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_ctgt(cols, dataset)
</code></pre>


<h3>Details</h3>

<p>Any rows with incomplete observations of the variables are ignored.
</p>

<hr>
<h2 id='fast_equal'>Compares all elements in a to b</h2><span id='topic+fast_equal'></span>

<h3>Description</h3>

<p>Compares all elements in a to b
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fast_equal(a, b)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fast_equal_+3A_b">b</code></td>
<td>
<p>numeric. Must be length one but no check is performed.</p>
</td></tr>
</table>

<hr>
<h2 id='forget'>Forget a memoized function.</h2><span id='topic+forget'></span>

<h3>Description</h3>

<p>Forget a memoized function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>forget(f)
</code></pre>


<h3>Author(s)</h3>

<p>Hadley Wickham
</p>

<hr>
<h2 id='get_ancestors'>Based on gRbase::ancestors()</h2><span id='topic+get_ancestors'></span>

<h3>Description</h3>

<p>Based on gRbase::ancestors()
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_ancestors(node, families)
</code></pre>

<hr>
<h2 id='get_but_last'>Return all but last element of x.</h2><span id='topic+get_but_last'></span>

<h3>Description</h3>

<p>If x is NULL returns NA not NULL
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_but_last(x)
</code></pre>

<hr>
<h2 id='get_last'>Return last element of x.</h2><span id='topic+get_last'></span>

<h3>Description</h3>

<p>If x is NULL returns NA not NULL
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_last(x)
</code></pre>

<hr>
<h2 id='get_log_leaf_entries'>Assuming that the cpt is a leaf, returns 1 instead of a CPT entry when value missing</h2><span id='topic+get_log_leaf_entries'></span>

<h3>Description</h3>

<p>Assuming that the cpt is a leaf, returns 1 instead of a CPT entry when value missing
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_log_leaf_entries(cpt, x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_log_leaf_entries_+3A_x">x</code></td>
<td>
<p>a vector of values</p>
</td></tr>
</table>

<hr>
<h2 id='get_null_safe'>Get i-th element of x.</h2><span id='topic+get_null_safe'></span>

<h3>Description</h3>

<p>If x is NULL returns NA not NULL
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_null_safe(x, i)
</code></pre>

<hr>
<h2 id='grain_and_graph'>Convert to igraph and gRain.</h2><span id='topic+grain_and_graph'></span><span id='topic+as_igraph'></span><span id='topic+as_grain'></span>

<h3>Description</h3>

<p>Convert a <code><a href="#topic+bnc_dag">bnc_dag</a></code> to <code>igraph</code> and
<code><a href="gRain.html#topic+grain">grain</a></code> objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_igraph(x)

as_grain(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="grain_and_graph_+3A_x">x</code></td>
<td>
<p>The <code><a href="#topic+bnc_bn">bnc_bn</a></code> object. The Bayesian network classifier.</p>
</td></tr>
</table>


<h3>Functions</h3>


<ul>
<li> <p><code>as_igraph()</code>: Convert to a graphNEL.
</p>
</li>
<li> <p><code>as_grain()</code>: Convert to a grain.
</p>
</li></ul>


<h3>Examples</h3>

<pre><code class='language-R'>data(car)
nb &lt;- bnc('nb', 'class', car, smooth = 1)
# Requires the grain and igraph packages installed
## Not run: g &lt;- as_grain(nb)
## Not run: gRain::querygrain.grain(g)$buying
</code></pre>

<hr>
<h2 id='graph_add_edges'>Add edges
Does not allow edges among adjacent nodes</h2><span id='topic+graph_add_edges'></span>

<h3>Description</h3>

<p>Add edges
Does not allow edges among adjacent nodes
</p>


<h3>Usage</h3>

<pre><code class='language-R'>graph_add_edges(from, to, g)
</code></pre>

<hr>
<h2 id='graph_connected_components'>connected_components</h2><span id='topic+graph_connected_components'></span>

<h3>Description</h3>

<p>connected_components
</p>


<h3>Usage</h3>

<pre><code class='language-R'>graph_connected_components(g)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="graph_connected_components_+3A_g">g</code></td>
<td>
<p>graph_internal.</p>
</td></tr>
</table>

<hr>
<h2 id='graph_get_adjacent'>Finds adjacent nodes. Has not been tested much</h2><span id='topic+graph_get_adjacent'></span>

<h3>Description</h3>

<p>Finds adjacent nodes. Has not been tested much
</p>


<h3>Usage</h3>

<pre><code class='language-R'>graph_get_adjacent(node, g)
</code></pre>

<hr>
<h2 id='graph_is_adjacent'>Checks whether nodes are adjacent</h2><span id='topic+graph_is_adjacent'></span>

<h3>Description</h3>

<p>Checks whether nodes are adjacent
</p>


<h3>Usage</h3>

<pre><code class='language-R'>graph_is_adjacent(from, to, g)
</code></pre>

<hr>
<h2 id='graph_named_edge_matrix'>Returns an edge matrix with node names (instead of node indices).</h2><span id='topic+graph_named_edge_matrix'></span>

<h3>Description</h3>

<p>Returns an edge matrix with node names (instead of node indices).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>graph_named_edge_matrix(x)
</code></pre>


<h3>Value</h3>

<p>A character matrix.
</p>

<hr>
<h2 id='graph_subgraph'>Subgraph.  
Only for a directed graph?</h2><span id='topic+graph_subgraph'></span>

<h3>Description</h3>

<p>Subgraph.  
Only for a directed graph?
</p>


<h3>Usage</h3>

<pre><code class='language-R'>graph_subgraph(nodes, g)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="graph_subgraph_+3A_nodes">nodes</code></td>
<td>
<p>character</p>
</td></tr>
<tr><td><code id="graph_subgraph_+3A_g">g</code></td>
<td>
<p>graph_internal.</p>
</td></tr>
</table>

<hr>
<h2 id='graph_union'>Merges multiple disjoint graphs into a single one.</h2><span id='topic+graph_union'></span>

<h3>Description</h3>

<p>Merges multiple disjoint graphs into a single one.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>graph_union(g)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="graph_union_+3A_g">g</code></td>
<td>
<p>A graph</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A graph
</p>

<hr>
<h2 id='greedy_wrapper'>Learn Bayesian network classifiers in a a greedy wrapper fashion.</h2><span id='topic+greedy_wrapper'></span><span id='topic+fssj'></span><span id='topic+bsej'></span><span id='topic+tan_hc'></span><span id='topic+kdb'></span><span id='topic+tan_hcsp'></span>

<h3>Description</h3>

<p>Greedy wrapper algorithms for learning Bayesian network classifiers. All 
algorithms use cross-validated estimate of predictive accuracy to evaluate 
candidate structures.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fssj(class, dataset, k, epsilon = 0.01, smooth = 0, cache_reset = NULL)

bsej(class, dataset, k, epsilon = 0.01, smooth = 0, cache_reset = NULL)

tan_hc(class, dataset, k, epsilon = 0.01, smooth = 0, cache_reset = NULL)

kdb(
  class,
  dataset,
  k,
  kdbk = 2,
  epsilon = 0.01,
  smooth = 0,
  cache_reset = NULL
)

tan_hcsp(class, dataset, k, epsilon = 0.01, smooth = 0, cache_reset = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="greedy_wrapper_+3A_class">class</code></td>
<td>
<p>A character. Name of the class variable.</p>
</td></tr>
<tr><td><code id="greedy_wrapper_+3A_dataset">dataset</code></td>
<td>
<p>The data frame from which to learn the classifier.</p>
</td></tr>
<tr><td><code id="greedy_wrapper_+3A_k">k</code></td>
<td>
<p>An integer. The number of folds.</p>
</td></tr>
<tr><td><code id="greedy_wrapper_+3A_epsilon">epsilon</code></td>
<td>
<p>A numeric. Minimum absolute improvement in accuracy required 
to keep searching.</p>
</td></tr>
<tr><td><code id="greedy_wrapper_+3A_smooth">smooth</code></td>
<td>
<p>A numeric. The smoothing value (<code class="reqn">\alpha</code>) for Bayesian 
parameter estimation. Nonnegative.</p>
</td></tr>
<tr><td><code id="greedy_wrapper_+3A_cache_reset">cache_reset</code></td>
<td>
<p>A numeric. Number of iterations after which to reset the 
cache of conditional probability tables. A small number reduces the amount
of memory used. <code>NULL</code> means the cache is never reset (the default).</p>
</td></tr>
<tr><td><code id="greedy_wrapper_+3A_kdbk">kdbk</code></td>
<td>
<p>An integer. The maximum number of feature parents per feature.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="#topic+bnc_dag">bnc_dag</a></code> object.
</p>


<h3>References</h3>

<p>Pazzani M (1996). Constructive induction of Cartesian product 
attributes. In <em>Proceedings of the Information, Statistics and 
Induction in Science Conference (ISIS-1996)</em>, pp. 66-77
</p>
<p>Koegh E and Pazzani M (2002).Learning the structure of augmented Bayesian 
classifiers. In <em>International Journal on Artificial Intelligence 
Tools</em>, <b>11</b>(4), pp. 587-601.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(car)
tanhc &lt;- tan_hc('class', car, k = 5, epsilon = 0)  
## Not run: plot(tanhc)
  
</code></pre>

<hr>
<h2 id='identify_all_testing_depths'>Identifies all depths at which the features of a classification tree are
tested.</h2><span id='topic+identify_all_testing_depths'></span>

<h3>Description</h3>

<p>Identifies all depths at which the features of a classification tree are
tested.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>identify_all_testing_depths(tree)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="identify_all_testing_depths_+3A_tree">tree</code></td>
<td>
<p>an <code>rpart</code> object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a numeric vector. The names are the names of the variables.
</p>

<hr>
<h2 id='identify_min_testing_depths'>Identifies the lowest (closest to root) depths at which the features of a 
classification tree are tested.</h2><span id='topic+identify_min_testing_depths'></span>

<h3>Description</h3>

<p>Identifies the lowest (closest to root) depths at which the features of a 
classification tree are tested.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>identify_min_testing_depths(tree)
</code></pre>

<hr>
<h2 id='inspect_bnc_bn'>Inspect a Bayesian network classifier (with structure and parameters).</h2><span id='topic+inspect_bnc_bn'></span><span id='topic+nparams'></span><span id='topic+manb_arc_posterior'></span><span id='topic+awnb_weights'></span><span id='topic+params'></span><span id='topic+values'></span><span id='topic+classes'></span>

<h3>Description</h3>

<p>Functions for inspecting a <code><a href="#topic+bnc_bn">bnc_bn</a></code> object. In addition, you can 
query this object with the functions documented in 
<code><a href="#topic+inspect_bnc_dag">inspect_bnc_dag</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nparams(x)

manb_arc_posterior(x)

awnb_weights(x)

params(x)

values(x)

classes(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="inspect_bnc_bn_+3A_x">x</code></td>
<td>
<p>The <code><a href="#topic+bnc_bn">bnc_bn</a></code> object. The Bayesian network classifier.</p>
</td></tr>
</table>


<h3>Functions</h3>


<ul>
<li> <p><code>nparams()</code>: Returns the number of free parameters in the model.
</p>
</li>
<li> <p><code>manb_arc_posterior()</code>: Returns the posterior of each arc from the class
according to the MANB method.
</p>
</li>
<li> <p><code>awnb_weights()</code>: Returns the AWNB feature weights.
</p>
</li>
<li> <p><code>params()</code>: Returns the list of CPTs, in the same order as <code><a href="#topic+vars">vars</a></code>.
</p>
</li>
<li> <p><code>values()</code>: Returns the possible values of each variable, in the same order as <code><a href="#topic+vars">vars</a></code>.
</p>
</li>
<li> <p><code>classes()</code>: Returns the possible values of the class variable.
</p>
</li></ul>


<h3>Examples</h3>

<pre><code class='language-R'> 
data(car)
nb &lt;- bnc('nb', 'class', car, smooth = 1)
nparams(nb)
nb &lt;- bnc('nb', 'class', car, smooth = 1, manb_prior = 0.5)
manb_arc_posterior(nb)
nb &lt;- bnc('nb', 'class', car, smooth = 1, awnb_bootstrap = 0.5)
awnb_weights(nb)
</code></pre>

<hr>
<h2 id='inspect_bnc_dag'>Inspect a Bayesian network classifier structure.</h2><span id='topic+inspect_bnc_dag'></span><span id='topic+class_var'></span><span id='topic+features'></span><span id='topic+vars'></span><span id='topic+families'></span><span id='topic+modelstring'></span><span id='topic+feature_families'></span><span id='topic+narcs'></span><span id='topic+is_semi_naive'></span><span id='topic+is_anb'></span><span id='topic+is_nb'></span><span id='topic+is_ode'></span>

<h3>Description</h3>

<p>Functions for inspecting a <code><a href="#topic+bnc_dag">bnc_dag</a></code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>class_var(x)

features(x)

vars(x)

families(x)

modelstring(x)

feature_families(x)

narcs(x)

is_semi_naive(x)

is_anb(x)

is_nb(x)

is_ode(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="inspect_bnc_dag_+3A_x">x</code></td>
<td>
<p>The <code><a href="#topic+bnc_dag">bnc_dag</a></code> object. The Bayesian network classifier
structure.</p>
</td></tr>
</table>


<h3>Functions</h3>


<ul>
<li> <p><code>class_var()</code>: Returns the class variable.
</p>
</li>
<li> <p><code>features()</code>: Returns the features.
</p>
</li>
<li> <p><code>vars()</code>: Returns all variables (i.e., features + class).
</p>
</li>
<li> <p><code>families()</code>: Returns the family of each variable.
</p>
</li>
<li> <p><code>modelstring()</code>: Returns the model string of the network in bnlearn format (adding a space in between two families).
</p>
</li>
<li> <p><code>feature_families()</code>: Returns the family of each feature.
</p>
</li>
<li> <p><code>narcs()</code>: Returns the number of arcs.
</p>
</li>
<li> <p><code>is_semi_naive()</code>: Returns TRUE if <code>x</code> is a semi-naive Bayes.
</p>
</li>
<li> <p><code>is_anb()</code>: Returns TRUE if <code>x</code> is an augmented naive Bayes.
</p>
</li>
<li> <p><code>is_nb()</code>: Returns TRUE if <code>x</code> is a naive Bayes.
</p>
</li>
<li> <p><code>is_ode()</code>: Returns TRUE if <code>x</code> is a one-dependence estimator.
</p>
</li></ul>


<h3>Examples</h3>

<pre><code class='language-R'>data(car)
nb &lt;- bnc('nb', 'class', car, smooth = 1)
narcs(nb)
is_ode(nb)
</code></pre>

<hr>
<h2 id='is_aode'>Is it en AODE?</h2><span id='topic+is_aode'></span>

<h3>Description</h3>

<p>Is it en AODE?
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_aode(x)
</code></pre>

<hr>
<h2 id='is.memoised'>Is it memoized?</h2><span id='topic+is.memoised'></span>

<h3>Description</h3>

<p>Is it memoized?
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.memoised(f)
</code></pre>


<h3>Author(s)</h3>

<p>Hadley Wickham
</p>

<hr>
<h2 id='learn_params'>Learn the parameters of a Bayesian network structure.</h2><span id='topic+learn_params'></span><span id='topic+lp'></span>

<h3>Description</h3>

<p>Learn parameters with maximum likelihood or Bayesian estimation, the 
weighting attributes to alleviate naive bayes' independence assumption (WANBIA), 
attribute weighted naive Bayes (AWNB), or the model averaged naive Bayes 
(MANB) methods. Returns a <code><a href="#topic+bnc_bn">bnc_bn</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lp(
  x,
  dataset,
  smooth,
  awnb_trees = NULL,
  awnb_bootstrap = NULL,
  manb_prior = NULL,
  wanbia = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="learn_params_+3A_x">x</code></td>
<td>
<p>The <code><a href="#topic+bnc_dag">bnc_dag</a></code> object. The Bayesian network classifier
structure.</p>
</td></tr>
<tr><td><code id="learn_params_+3A_dataset">dataset</code></td>
<td>
<p>The data frame from which to learn network parameters.</p>
</td></tr>
<tr><td><code id="learn_params_+3A_smooth">smooth</code></td>
<td>
<p>A numeric. The smoothing value (<code class="reqn">\alpha</code>) for Bayesian 
parameter estimation. Nonnegative.</p>
</td></tr>
<tr><td><code id="learn_params_+3A_awnb_trees">awnb_trees</code></td>
<td>
<p>An integer. The number (<code class="reqn">M</code>) of bootstrap samples to 
generate.</p>
</td></tr>
<tr><td><code id="learn_params_+3A_awnb_bootstrap">awnb_bootstrap</code></td>
<td>
<p>A numeric. The size of the bootstrap subsample, 
relative to the size of <code>dataset</code> (given in [0,1]).</p>
</td></tr>
<tr><td><code id="learn_params_+3A_manb_prior">manb_prior</code></td>
<td>
<p>A numeric. The prior probability for an arc between the 
class and any feature.</p>
</td></tr>
<tr><td><code id="learn_params_+3A_wanbia">wanbia</code></td>
<td>
<p>A logical. If <code>TRUE</code>, WANBIA feature weighting is
performed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>lp</code> learns the parameters of each local distribution <code class="reqn">\theta_{ijk} 
= P(X_i = k \mid \mathbf{Pa}(X_i) = j)</code> as </p>
<p style="text-align: center;"><code class="reqn">\theta_{ijk} = \frac{N_{ijk} + \alpha}{N_{ ij \cdot } + r_i 
\alpha},</code>
</p>
<p> where
<code class="reqn">N_{ijk}</code> is the number of instances in <code>dataset</code> in which 
<code class="reqn">X_i = k</code> and <code class="reqn">\mathbf{Pa}(X_i) = j</code>, 
<code class="reqn">N_{ ij \cdot} = \sum_{k=1}^{r_i} N_{ijk}</code>, <code class="reqn">r_i</code> is the cardinality of <code class="reqn">X_i</code>, and all 
hyperparameters of the Dirichlet prior equal to <code class="reqn">\alpha</code>. <code class="reqn">\alpha = 
0</code> corresponds to maximum likelihood estimation. Returns a uniform 
distribution when <code class="reqn">N_{ i j \cdot } + r_i \alpha = 0</code>. With partially observed data, the above amounts to 
<em>available case analysis</em>.
</p>
<p>WANBIA learns a unique exponent 'weight' per feature. They are 
computed by optimizing conditional log-likelihood, and are bounded with
all <code class="reqn">w_i \in [0, 1]</code>. For WANBIA estimates, set <code>wanbia</code> to <code>TRUE</code>.
</p>
<p>In order to get the AWNB parameter estimate, provide either the 
<code>awnb_bootstrap</code> and/or the <code>awnb_trees</code> argument. The estimate is:
</p>
<p style="text-align: center;"><code class="reqn">\theta_{ijk}^{AWNB} = \frac{\theta_{ijk}^{w_i}}{\sum_{k=1}^{r_i} 
\theta_{ijk}^{w_i}},</code>
</p>
<p> while the weights <code class="reqn">w_i</code> are
computed as </p>
<p style="text-align: center;"><code class="reqn">w_i = \frac{1}{M}\sum_{t=1}^M \sqrt{\frac{1}{d_{ti}}},</code>
</p>
<p> where <code class="reqn">M</code> is the number of 
bootstrap samples from <code>dataset</code> and <code class="reqn">d_{ti}</code> the minimum 
testing depth of <code class="reqn">X_i</code> in an unpruned classification tree learned 
from the <code class="reqn">t</code>-th subsample (<code class="reqn">d_{ti} = 0</code> if <code class="reqn">X_i</code> 
is omitted from <code class="reqn">t</code>-th tree).
</p>
<p>The MANB parameters correspond to Bayesian model averaging over the naive 
Bayes models obtained from all <code class="reqn">2^n</code> subsets over the <code class="reqn">n</code> 
features. To get MANB parameters, provide the <code>manb_prior</code> argument.
</p>


<h3>Value</h3>

<p>A <code><a href="#topic+bnc_bn">bnc_bn</a></code> object.
</p>


<h3>References</h3>

<p>Hall M (2004). A decision tree-based attribute weighting filter 
for naive Bayes. <em>Knowledge-based Systems</em>, <b>20</b>(2), 120-126.
</p>
<p>Dash D and Cooper GF (2002). Exact model averaging with naive Bayesian 
classifiers. <em>19th International Conference on Machine Learning 
(ICML-2002)</em>, 91-98.
</p>
<p>Pigott T D (2001) A review of methods for missing data. <em>Educational 
research and evaluation</em>, <b>7</b>(4), 353-383.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(car)
nb &lt;- nb('class', car)
# Maximum likelihood estimation
mle &lt;- lp(nb, car, smooth = 0)
# Bayesian estimaion
bayes &lt;- lp(nb, car, smooth = 0.5)
# MANB
manb &lt;- lp(nb, car, smooth = 0.5, manb_prior = 0.5)
# AWNB
awnb &lt;- lp(nb, car, smooth = 0.5, awnb_trees = 10)
</code></pre>

<hr>
<h2 id='learn_unprunned_tree'>Learns a unpruned <code>rpart</code> recursive partition.</h2><span id='topic+learn_unprunned_tree'></span>

<h3>Description</h3>

<p>Learns a unpruned <code>rpart</code> recursive partition.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>learn_unprunned_tree(dataset, class)
</code></pre>

<hr>
<h2 id='local_ode_score_contrib'>Returns pairwise component of ODE (penalized) log-likelihood scores. 
In natural logarithms.</h2><span id='topic+local_ode_score_contrib'></span>

<h3>Description</h3>

<p>Returns pairwise component of ODE (penalized) log-likelihood scores. 
In natural logarithms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>local_ode_score_contrib(x, y, class, dataset)
</code></pre>

<hr>
<h2 id='log_normalize'>Normalize log probabilities.</h2><span id='topic+log_normalize'></span>

<h3>Description</h3>

<p>Uses the log-sum-exp trick.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>log_normalize(lp)
</code></pre>


<h3>References</h3>

<p>Murphy KP (2012). <em>Machine learning: a probabilistic
perspective</em>. The MIT Press. pp. 86-87.
</p>

<hr>
<h2 id='loglik'>Compute (penalized) log-likelihood.</h2><span id='topic+loglik'></span><span id='topic+AIC.bnc_bn'></span><span id='topic+BIC.bnc_bn'></span><span id='topic+logLik.bnc_bn'></span><span id='topic+cLogLik'></span>

<h3>Description</h3>

<p>Compute (penalized) log-likelihood and conditional log-likelihood score of a <code><a href="#topic+bnc_bn">bnc_bn</a></code> object on
a data set. Requires a data frame argument in addition to <code>object</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bnc_bn'
AIC(object, ...)

## S3 method for class 'bnc_bn'
BIC(object, ...)

## S3 method for class 'bnc_bn'
logLik(object, ...)

cLogLik(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="loglik_+3A_object">object</code></td>
<td>
<p>A <code><a href="#topic+bnc_bn">bnc_bn</a></code> object.</p>
</td></tr>
<tr><td><code id="loglik_+3A_...">...</code></td>
<td>
<p>A data frame (<code class="reqn">\mathcal{D}</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>log-likelihood =  <code class="reqn">log P(\mathcal{D} \mid \theta)</code>,
</p>
<p>Akaike's information criterion (AIC) = <code class="reqn">log P(\mathcal{D} \mid \theta) - 
\frac{1}{2} |\theta|</code>,
</p>
<p>The Bayesian information criterion (BIC) score: = <code class="reqn">log P(\mathcal{D} \mid
\theta) - \frac{\log N}{2} |\theta|</code>,
</p>
<p>where <code class="reqn">|\theta|</code> is the number of free parameters in <code>object</code>, 
<code class="reqn">\mathcal{D}</code> is the data set and N is the number of instances in 
<code class="reqn">\mathcal{D}</code>.
</p>
<p><code>cLogLik</code> computes the conditional log-likelihood of the model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(car)
nb &lt;- bnc('nb', 'class', car, smooth = 1)
logLik(nb, car)   
AIC(nb, car)
BIC(nb, car)
cLogLik(nb, car)   
</code></pre>

<hr>
<h2 id='make_cll'>Returns a function to compute negative conditional log-likelihood given feature weights</h2><span id='topic+make_cll'></span>

<h3>Description</h3>

<p>Returns a function to compute negative conditional log-likelihood given feature weights
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make_cll(class_var, dataset)
</code></pre>

<hr>
<h2 id='make_cll_gradient'>Returns a function to compute the gradient of negative conditional log-likelihood with respect to feature weights</h2><span id='topic+make_cll_gradient'></span>

<h3>Description</h3>

<p>Returns a function to compute the gradient of negative conditional log-likelihood with respect to feature weights
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make_cll_gradient(class_var, dataset)
</code></pre>

<hr>
<h2 id='makeRLearner.bnc'>makeRLearner. Auxiliary mlr function.</h2><span id='topic+makeRLearner.bnc'></span>

<h3>Description</h3>

<p>makeRLearner. Auxiliary mlr function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>makeRLearner.bnc()
</code></pre>

<hr>
<h2 id='map'>Assigns instances to the most likely class.</h2><span id='topic+map'></span>

<h3>Description</h3>

<p>Ties are resolved randomly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>map(pred)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="map_+3A_pred">pred</code></td>
<td>
<p>A numeric matrix. Each row corresponds to class posterior 
probabilities for an instance.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a factor with the same levels as the class variable.
</p>

<hr>
<h2 id='max_weight_forest'>Returns the undirected augmenting forest.</h2><span id='topic+max_weight_forest'></span>

<h3>Description</h3>

<p>Uses Kruskal's algorithm to find the augmenting forest that maximizes the sum
of pairwise weights. When the weights are class-conditional mutual
information this forest maximizes the likelihood of the tree-augmented naive
Bayes network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>max_weight_forest(g)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="max_weight_forest_+3A_g">g</code></td>
<td>
<p>A graph. The undirected graph with pairwise 
weights.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>g</code> is not connected than this will return a forest; otherwise it is 
a tree.
</p>


<h3>Value</h3>

<p>A graph. The maximum spanning forest.
</p>


<h3>References</h3>

<p>Friedman N, Geiger D and Goldszmidt M (1997). Bayesian network 
classifiers. <em>Machine Learning</em>, <b>29</b>, pp. 131&ndash;163.
</p>
<p>Murphy KP (2012). <em>Machine learning: a probabilistic perspective</em>. The
MIT Press. pp. 912-914.
</p>

<hr>
<h2 id='memoise_char'>Memoise a function.</h2><span id='topic+memoise_char'></span>

<h3>Description</h3>

<p>Based on Hadley Wickham's memoise package. Assumes that argument to f is a
character vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>memoise_char(f)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="memoise_char_+3A_f">f</code></td>
<td>
<p>a function</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is a slightly modified version of
<code>memoise</code> to avoid the use of digest. The rest functions
copied as is from memoise.
</p>


<h3>Author(s)</h3>

<p>Hadley Wickham, Bojan Mihaljevic
</p>

<hr>
<h2 id='nb'>Learn a naive Bayes network structure.</h2><span id='topic+nb'></span>

<h3>Description</h3>

<p>Learn a naive Bayes network structure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nb(class, dataset = NULL, features = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nb_+3A_class">class</code></td>
<td>
<p>A character. Name of the class variable.</p>
</td></tr>
<tr><td><code id="nb_+3A_dataset">dataset</code></td>
<td>
<p>The data frame from which to learn the classifier.</p>
</td></tr>
<tr><td><code id="nb_+3A_features">features</code></td>
<td>
<p>A character vector. The names of the features. This argument
is ignored if <code>dataset</code> is provided.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="#topic+bnc_dag">bnc_dag</a></code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
data(car)
nb &lt;- nb('class', car)   
nb2 &lt;- nb('class', features = letters[1:10])
## Not run: plot(nb2)
</code></pre>

<hr>
<h2 id='nb_dag'>Returns a naive Bayes structure</h2><span id='topic+nb_dag'></span>

<h3>Description</h3>

<p>Returns a naive Bayes structure
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nb_dag(class, features)
</code></pre>

<hr>
<h2 id='new_cache'>Make a new cache.</h2><span id='topic+new_cache'></span>

<h3>Description</h3>

<p>Make a new cache.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>new_cache()
</code></pre>


<h3>Author(s)</h3>

<p>Hadley Wickham
</p>

<hr>
<h2 id='order_acyclic'>Provide an acyclic ordering (i.e., a topological sort).</h2><span id='topic+order_acyclic'></span>

<h3>Description</h3>

<p>Provide an acyclic ordering (i.e., a topological sort).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>order_acyclic(families)
</code></pre>


<h3>References</h3>

<p>Beng-Jensen and Gutin, 2007, page 14.
</p>

<hr>
<h2 id='plot.bnc_dag'>Plot the structure.</h2><span id='topic+plot.bnc_dag'></span>

<h3>Description</h3>

<p>If node labels are to small to be viewed properly, you may fix label fontsize
with argument fontsize. Also, you may try multiple different layouts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bnc_dag'
plot(x, y, layoutType = "dot", fontsize = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.bnc_dag_+3A_x">x</code></td>
<td>
<p>The <code><a href="#topic+bnc_dag">bnc_dag</a></code> object. The Bayesian network classifier
structure.</p>
</td></tr>
<tr><td><code id="plot.bnc_dag_+3A_y">y</code></td>
<td>
<p>Not used</p>
</td></tr>
<tr><td><code id="plot.bnc_dag_+3A_layouttype">layoutType</code></td>
<td>
<p>a character. Optional.</p>
</td></tr>
<tr><td><code id="plot.bnc_dag_+3A_fontsize">fontsize</code></td>
<td>
<p>integer Font size for node labels. Optional.</p>
</td></tr>
<tr><td><code id="plot.bnc_dag_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'> 

# Requires the igraph package to be installed.
data(car)
nb &lt;- nb('class', car)
nb &lt;- nb('class', car)
## Not run: plot(nb)
## Not run: plot(nb, fontsize = 20)
## Not run: plot(nb, layoutType = 'circo')
## Not run: plot(nb, layoutType = 'fdp')
## Not run: plot(nb, layoutType = 'osage')
## Not run: plot(nb, layoutType = 'twopi')
## Not run: plot(nb, layoutType = 'neato')
</code></pre>

<hr>
<h2 id='predict.bnc_fit'>Predicts class labels or class posterior probability distributions.</h2><span id='topic+predict.bnc_fit'></span>

<h3>Description</h3>

<p>Predicts class labels or class posterior probability distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bnc_fit'
predict(object, newdata, prob = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.bnc_fit_+3A_object">object</code></td>
<td>
<p>A <code><a href="#topic+bnc_bn">bnc_bn</a></code> object.</p>
</td></tr>
<tr><td><code id="predict.bnc_fit_+3A_newdata">newdata</code></td>
<td>
<p>A data frame containing observations whose class has to be 
predicted.</p>
</td></tr>
<tr><td><code id="predict.bnc_fit_+3A_prob">prob</code></td>
<td>
<p>A logical. Whether class posterior probability should be returned.</p>
</td></tr>
<tr><td><code id="predict.bnc_fit_+3A_...">...</code></td>
<td>
<p>Ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Ties are resolved randomly. Inference is much slower if
<code>newdata</code> contains <code>NA</code>s.
</p>


<h3>Value</h3>

<p>If <code>prob=FALSE</code>, then returns a length-<code class="reqn">N</code> factor with the 
same levels as the class variable in <code>x</code>, where <code class="reqn">N</code> is the number 
of rows in <code>newdata</code>. Each element is the most likely 
class for the corresponding row in <code>newdata</code>. If <code>prob=TRUE</code>, 
returns a <code class="reqn">N</code> by <code class="reqn">C</code> numeric matrix, where <code class="reqn">C</code> is the number of 
classes; each row corresponds to the class posterior of the instance.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(car)
nb &lt;- bnc('nb', 'class', car, smooth = 1)
p &lt;- predict(nb, car)
head(p)
p &lt;- predict(nb, car, prob = TRUE)
head(p)
</code></pre>

<hr>
<h2 id='predictLearner.bnc'>predictLearner. Auxiliary mlr function.</h2><span id='topic+predictLearner.bnc'></span>

<h3>Description</h3>

<p>predictLearner. Auxiliary mlr function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predictLearner.bnc(.learner, .model, .newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predictLearner.bnc_+3A_.learner">.learner</code>, <code id="predictLearner.bnc_+3A_.model">.model</code>, <code id="predictLearner.bnc_+3A_.newdata">.newdata</code></td>
<td>
<p>Internal.</p>
</td></tr>
<tr><td><code id="predictLearner.bnc_+3A_...">...</code></td>
<td>
<p>Internal.</p>
</td></tr>
</table>

<hr>
<h2 id='print.bnc_base'>Print basic information about a classifier.</h2><span id='topic+print.bnc_base'></span>

<h3>Description</h3>

<p>Print basic information about a classifier.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bnc_base'
print(x, ...)
</code></pre>

<hr>
<h2 id='skip_assert'>Whether to do checks or not. Set TRUE to speed up debugging or building.</h2><span id='topic+skip_assert'></span>

<h3>Description</h3>

<p>Whether to do checks or not. Set TRUE to speed up debugging or building.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>skip_assert()
</code></pre>

<hr>
<h2 id='skip_testing'>Skip while testing to isolate errors</h2><span id='topic+skip_testing'></span>

<h3>Description</h3>

<p>Skip while testing to isolate errors
</p>


<h3>Usage</h3>

<pre><code class='language-R'>skip_testing()
</code></pre>

<hr>
<h2 id='spode'>Returns a Superparent one-dependence estimator.</h2><span id='topic+spode'></span>

<h3>Description</h3>

<p>Returns a Superparent one-dependence estimator.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spode(sp, features, class)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="spode_+3A_sp">sp</code></td>
<td>
<p>character The superparent.</p>
</td></tr>
</table>

<hr>
<h2 id='subset_by_colnames'>Subset a 2D structure by a vector of column names.</h2><span id='topic+subset_by_colnames'></span>

<h3>Description</h3>

<p>Not all colnames are necessarily in the columns of data; in that case this
returns NA.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>subset_by_colnames(colnames, data)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="subset_by_colnames_+3A_colnames">colnames</code></td>
<td>
<p>a character vector</p>
</td></tr>
<tr><td><code id="subset_by_colnames_+3A_data">data</code></td>
<td>
<p>a matrix or data frame</p>
</td></tr>
</table>

<hr>
<h2 id='superparent_children'>Return nodes which can be superparents along with their possible children.</h2><span id='topic+superparent_children'></span>

<h3>Description</h3>

<p>Return nodes which can be superparents along with their possible children.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>superparent_children(bnc_dag)
</code></pre>


<h3>Value</h3>

<p>list of <code>search_state</code>. NULL if no orphans
</p>

<hr>
<h2 id='tan_chowliu'>Learns a one-dependence estimator using Chow-Liu's algorithm.</h2><span id='topic+tan_chowliu'></span><span id='topic+tan_cl'></span>

<h3>Description</h3>

<p>Learns a one-dependence Bayesian classifier using Chow-Liu's algorithm, by 
maximizing either log-likelihood, the AIC or BIC scores; maximizing 
log-likelihood corresponds to the well-known tree augmented naive Bayes 
(Friedman et al., 1997). When maximizing AIC or BIC the output might be a
forest-augmented rather than a tree-augmented naive Bayes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tan_cl(class, dataset, score = "loglik", root = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tan_chowliu_+3A_class">class</code></td>
<td>
<p>A character. Name of the class variable.</p>
</td></tr>
<tr><td><code id="tan_chowliu_+3A_dataset">dataset</code></td>
<td>
<p>The data frame from which to learn the classifier.</p>
</td></tr>
<tr><td><code id="tan_chowliu_+3A_score">score</code></td>
<td>
<p>A character. The score to be maximized. <code>'loglik'</code>, 
<code>'bic'</code>, and <code>'aic'</code> return the maximum likelihood, maximum BIC 
and maximum AIC tree/forest, respectively.</p>
</td></tr>
<tr><td><code id="tan_chowliu_+3A_root">root</code></td>
<td>
<p>A character. The feature to be used as root of the augmenting 
tree. Only one feature can be supplied, even in case of an augmenting 
forest. This argument is optional.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="#topic+bnc_dag">bnc_dag</a></code> object.
</p>


<h3>References</h3>

<p>Friedman N, Geiger D and Goldszmidt M (1997). Bayesian network 
classifiers. <em>Machine Learning</em>, <b>29</b>, pp. 131&ndash;163.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(car)
ll &lt;- tan_cl('class', car, score = 'loglik')   
## Not run: plot(ll)
ll &lt;- tan_cl('class', car, score = 'loglik', root = 'maint')   
## Not run: plot(ll)
aic &lt;- tan_cl('class', car, score = 'aic')   
bic &lt;- tan_cl('class', car, score = 'bic')   
</code></pre>

<hr>
<h2 id='trainLearner.bnc'>trainLearner. Auxiliary mlr function.</h2><span id='topic+trainLearner.bnc'></span>

<h3>Description</h3>

<p>trainLearner. Auxiliary mlr function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trainLearner.bnc(.learner, .task, .subset, .weights, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="trainLearner.bnc_+3A_.learner">.learner</code>, <code id="trainLearner.bnc_+3A_.task">.task</code>, <code id="trainLearner.bnc_+3A_.subset">.subset</code>, <code id="trainLearner.bnc_+3A_.weights">.weights</code></td>
<td>
<p>Internal.</p>
</td></tr>
<tr><td><code id="trainLearner.bnc_+3A_...">...</code></td>
<td>
<p>Internal.</p>
</td></tr>
</table>

<hr>
<h2 id='voting'>Congress Voting Data Set.</h2><span id='topic+voting'></span>

<h3>Description</h3>

<p>Data set from the UCI repository
<a href="https://archive.ics.uci.edu/ml/datasets/Congressional+Voting+Records">https://archive.ics.uci.edu/ml/datasets/Congressional+Voting+Records</a>.
</p>


<h3>Format</h3>

<p>A <code>data.frame</code> with 17 columns and 435 rows.
</p>


<h3>Source</h3>

<p><a href="https://goo.gl/GTXrCz">https://goo.gl/GTXrCz</a>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
