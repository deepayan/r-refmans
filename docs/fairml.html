<!DOCTYPE html><html><head><title>Help for package fairml</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {fairml}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#fairml-package'><p>Fair models in machine learning</p></a></li>
<li><a href='#adult'><p>Census Income</p></a></li>
<li><a href='#bank'><p>Bank Marketing</p></a></li>
<li><a href='#communities.and.crime'><p>Communities and Crime Data Set</p></a></li>
<li><a href='#compas'><p>Criminal Offenders Screened in Florida</p></a></li>
<li><a href='#drug.consumption'><p>Drug Consumption</p></a></li>
<li><a href='#fairml.cv'><p>Cross-Validation for Fair Models</p></a></li>
<li><a href='#fairness.profile.plot'><p>Profile Fair Models with Respect to Tuning Parameters</p></a></li>
<li><a href='#flchain'><p>Obesity Levels</p></a></li>
<li><a href='#frrm'><p>Fair Ridge Regression Model</p></a></li>
<li><a href='#german.credit'><p>German Credit Data</p></a></li>
<li><a href='#health.retirement'><p>Health and Retirement Survey</p></a></li>
<li><a href='#law.school.admissions'><p>Law School Admission Council data</p></a></li>
<li><a href='#methods for fair.model objects'><p>Extract information from fair.model objects</p></a></li>
<li><a href='#national.longitudinal.survey'><p>Income and Labour Market Activities</p></a></li>
<li><a href='#nclm'><p>Nonconvex Optimization for Regression with Fairness Constraints</p></a></li>
<li><a href='#obesity.levels'><p>Obesity Levels</p></a></li>
<li><a href='#synthetic data sets'><p>Synthetic data set to test fair models</p></a></li>
<li><a href='#zlm'><p>Zafar's Linear and Logistic Regressions</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Fair Models in Machine Learning</td>
</tr>
<tr>
<td>Version:</td>
<td>0.8</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-05-13</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods, glmnet</td>
</tr>
<tr>
<td>Suggests:</td>
<td>lattice, gridExtra, parallel, cccp, CVXR, survival</td>
</tr>
<tr>
<td>Author:</td>
<td>Marco Scutari [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Marco Scutari &lt;scutari@bnlearn.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Fair machine learning regression models which take sensitive attributes into account in
  model estimation. Currently implementing Komiyama et al. (2018) 
  <a href="http://proceedings.mlr.press/v80/komiyama18a/komiyama18a.pdf">http://proceedings.mlr.press/v80/komiyama18a/komiyama18a.pdf</a>, Zafar et al.
  (2019) <a href="https://www.jmlr.org/papers/volume20/18-262/18-262.pdf">https://www.jmlr.org/papers/volume20/18-262/18-262.pdf</a> and my own
  approach from Scutari, Panero and Proissl (2022)
  <a href="https://link.springer.com/content/pdf/10.1007/s11222-022-10143-w.pdf">https://link.springer.com/content/pdf/10.1007/s11222-022-10143-w.pdf</a>
  that uses ridge regression to enforce fairness.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>LazyData:</td>
<td>yes</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-05-13 20:59:37 UTC; fizban</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-05-13 22:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='fairml-package'>Fair models in machine learning</h2><span id='topic+fairml-package'></span><span id='topic+fairml'></span>

<h3>Description</h3>

<p>Fair machine learning models: estimation, tuning and prediction.
</p>


<h3>Details</h3>

<p><span class="pkg">fairml</span> implements key algorithms for learning machine learning models
while enforcing fairness with respect to a set of observed sensitive (or
protected) attributes.
</p>
<p>Currently <span class="pkg">fairml</span> implements the following algorithms (references below):
</p>

<ul>
<li> <p><code>nclm()</code>: the non-convex formulation of fair linear regression
model from Komiyama et al. (2018).
</p>
</li>
<li> <p><code>frrm()</code>: the fair (linear) ridge regression model from Scutari,
Panero and Proissl (2022).
</p>
</li>
<li> <p><code>fgrrm()</code>: thefair generalized (linear) ridge regression model
from Scutari, Panero and Proissl (2022), supporting the Gaussian,
binomial, Poisson, multinomial and Cox (proportional hazards) families.
</p>
</li>
<li> <p><code>zlrm()</code>: the fair logistic regression with covariance
constraints from Zafar et al. (2019).
</p>
</li>
<li> <p><code>zlrm()</code>: a fair linear regression with covariance
constraints following Zafar et al. (2019).
</p>
</li></ul>

<p>Furthermore, different fairness definitions can be used in <code>frrm()</code>
and <code>fgrrm()</code>:
</p>

<ul>
<li> <p><code>"sp-komiyama"</code>: the statistical parity fairness constraint from
Komiyama et al. (2018);
</p>
</li>
<li> <p><code>"eo-komiyama"</code>: the analogous equality of opportunity constraint
built on the proportion of variance (or deviance) explained by sensitive
attributes;
</p>
</li>
<li> <p><code>"if-berk"</code>: the individual fairness constraint from Berk et al.
(2017) adapted in Scutari, Panero and Proissl (2022);
</p>
</li>
<li><p> user-provided functions for custom definitions.
</p>
</li></ul>

<p>In addition, <span class="pkg">fairml</span> implements diagnostic plots, cross-validation,
prediction and methods for most of the generics made available for linear
models from <code>lm()</code> and <code>glm()</code>. Profile plots to trace key model
and goodness-of-fit indicators at varying levels of fairness are available
from <br /> <code>fairness.profile.plot()</code>.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari<br />
Istituto Dalle Molle di Studi sull'Intelligenza Artificiale (IDSIA)<br />
</p>
<p>Maintainer: Marco Scutari <a href="mailto:scutari@bnlearn.com">scutari@bnlearn.com</a>
</p>


<h3>References</h3>

<p>Berk R, Heidari H, Jabbari S, Joseph M, Kearns M, Morgenstern J, Neel S,
Roth A (2017). &quot;A Convex Framework for Fair Regression&quot;. FATML. <br />
<code>https://www.fatml.org/media/documents/convex_framework_for_fair_regression.pdf</code>
</p>
<p>Komiyama J, Takeda A, Honda J, Shimao H (2018). &quot;Nonconvex Optimization for
Regression with Fairness Constraints&quot;. Proceedings of the 35th International
Conference on Machine Learning (ICML), PMLR <strong>80</strong>:2737&ndash;2746. <br />
<code>http://proceedings.mlr.press/v80/komiyama18a/komiyama18a.pdf</code>
</p>
<p>Scutari M, Panero F, Proissl M (2022). &quot;Achieving Fairness with a Simple Ridge
Penalty&quot;. Statistics and Computing, <strong>32</strong>, 77. <br />
<code>https://link.springer.com/content/pdf/10.1007/s11222-022-10143-w.pdf</code>
</p>
<p>Zafar BJ, Valera I, Gomez-Rodriguez M, Gummadi KP (2019). &quot;Fairness
Constraints: a Flexible Approach for Fair Classification&quot;. Journal of
Machine Learning Research, 30:1&ndash;42. <br />
<code>https://www.jmlr.org/papers/volume20/18-262/18-262.pdf</code>
</p>

<hr>
<h2 id='adult'>Census Income</h2><span id='topic+adult'></span>

<h3>Description</h3>

<p>Predict whether income exceeds $50K per year using the U.S. 1994 Census data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(adult)
</code></pre>


<h3>Format</h3>

<p>The data contains 30162 observations and 14 variables. See the UCI Machine
Learning Repository for details.
</p>


<h3>Note</h3>

<p>The data set has been pre-processed as in Zafar et al. (2019), with the
following exceptions:
</p>

<ul>
<li><p> the data do not include the test sample from the UCI repository;
</p>
</li>
<li><p> the variables <code>"capital_gain"</code> and <code>"capital_loss"</code> have
been scaled by <code>1/1000</code>.
</p>
</li></ul>

<p>In that paper, <code>income</code> is the response variable, <code>sex</code> and
<code>race</code> are the sensitive attributes and the remaining variables are
used as predictors.
</p>
<p>The data contain the following variables:
</p>

<ul>
<li> <p><code>age</code> as a numeric variable;
</p>
</li>
<li> <p><code>workclass</code>, a factor with 8 levels encoding the type of
employment (<code>"Private"</code>, <code>"Self-emp-not-inc"</code>,
<code>"Federal-gov"</code>, etc.);
</p>
</li>
<li> <p><code>education</code>, a factor with 10 levels from <code>"Preschool"</code> to
<code>"Doctorate"</code>;
</p>
</li>
<li> <p><code>education-num</code>, the number of years in education;
</p>
</li>
<li> <p><code>marital-status</code>, a factor with 7 levels from
<code>"Married-civ-spouse"</code> to <code>"Divorced"</code> and
<code>"Never-married"</code>;
</p>
</li>
<li> <p><code>occupation</code>, a factor with 14 levels encoding the field of
employment (<code>"Tech-support"</code>, <code>"Craft-repair"</code>, etc.);
</p>
</li>
<li> <p><code>relationship</code> a factor with 6 levels (<code>"Wife"</code>,
<code>"Own-child"</code>, etc.);
</p>
</li>
<li> <p><code>race</code>, a factor with levels <code>"White"</code>,
<code>"Asian-Pac-Islander"</code>, <code>"Amer-Indian-Eskimo"</code>, <code>"Other"</code>
and <code>"Black"</code>;
</p>
</li>
<li> <p><code>sex</code>, a factor with levels <code>"Female"</code> and <code>"Male"</code>;
</p>
</li>
<li> <p><code>capital-gain</code> as a numeric variable;
</p>
</li>
<li> <p><code>capital-loss</code> as a numeric variable;
</p>
</li>
<li> <p><code>native-country</code> as a factor with two levels
<code>"United-States"</code> and <code>"Non-United-States"</code>;
</p>
</li>
<li> <p><code>hours-per-week</code> as a numeric variable.
</p>
</li></ul>



<h3>References</h3>

<p>UCI Machine Learning Repository. <br />
<code>https://archive.ics.uci.edu/ml/datasets/adult</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(adult)

# short-hand variable names.
r = adult[, "income"]
s = adult[, c("sex", "race")]
p = adult[, setdiff(names(adult), c("income", "sex", "race"))]

## Not run: 
m = zlrm(response = r, sensitive = s, predictors = p, unfairness = 0.05)
summary(m)

## End(Not run)</code></pre>

<hr>
<h2 id='bank'>Bank Marketing</h2><span id='topic+bank'></span>

<h3>Description</h3>

<p>Direct marketing campaigns (phone calls) of a Portuguese banking institution
to make clients subscribe a term deposit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(bank)
</code></pre>


<h3>Format</h3>

<p>The data contains 41188 observations and 19 variables. See the UCI Machine
Learning Repository for details.
</p>


<h3>Note</h3>

<p>The data set has been pre-processed as in Zafar et al. (2019), with the
following exceptions:
</p>

<ul>
<li><p> the variable <code>duration</code> has been dropped in order to learn as
realistic predictive model;
</p>
</li>
<li><p> the variable <code>pdays</code> has been dropped because it is not defined
for the vast majority of samples;
</p>
</li>
<li><p> observations where <code>loan</code> is <code>"unknown"</code> have been dropped
because the corresponding regression coefficient estimated by <code>glm()</code>
is <code>NA</code>;
</p>
</li>
<li><p> the three observations where <code>default</code> is <code>"yes"</code> have been
dropped to avoid errors in cross-validation (if all those three
observations are in the test fold it is impossible to compute predictions
from them).
</p>
</li></ul>

<p>In that paper, <code>subscribed</code> is the response variable, <code>age</code> is the
sensitive attribute and the remaining variables are used as predictors.
</p>
<p>The data contains the following variables:
</p>

<ul>
<li> <p><code>age</code> as a numeric variable;
</p>
</li>
<li> <p><code>job</code>, a factor with 12 levels ranging from <code>"blue-collar"</code>
to <code>"services"</code>;
</p>
</li>
<li> <p><code>marital</code>, a factor with levels <code>"divorced"</code>,
<code>"married"</code>, <code>"single"</code> and <code>"unknown"</code>;
</p>
</li>
<li> <p><code>education</code>, a factor with 8 levels ranging from
<code>"basic.4y"</code> to <code>"university.degree"</code>;
</p>
</li>
<li> <p><code>default</code>, a factor with levels <code>"no"</code> and <code>"unknown"</code>;
</p>
</li>
<li> <p><code>housing</code>, a factor with levels <code>"yes"</code> and <code>"no"</code>;
</p>
</li>
<li> <p><code>loan</code>, a factor with levels <code>"yes"</code> and <code>"no"</code>;
</p>
</li>
<li> <p><code>contact</code>, a factor with levels <code>"cellular"</code> and
<code>"telephone"</code>;
</p>
</li>
<li> <p><code>month</code>, a factor with 12 levels for the months of the year;
</p>
</li>
<li> <p><code>day_of_week</code>, a factor with 7 levels for the days of the week;
</p>
</li>
<li> <p><code>campaign</code>, the number of contacts performed during this
campaign;
</p>
</li>
<li> <p><code>previous</code>, the number of contacts performed before this
campaign;
</p>
</li>
<li> <p><code>poutcome</code>, a factor with levels <code>"failure"</code>,
<code>"nonexistent"</code> and <code>"success"</code>;
</p>
</li>
<li> <p><code>emp_var_rate</code>, the (numeric) quarterly employment variation
rate;
</p>
</li>
<li> <p><code>cons_price_idx</code>, the (numeric) monthly consumer price index;
</p>
</li>
<li> <p><code>cons_conf_idx</code>, the (numeric) monthly consumer confidence index;
</p>
</li>
<li> <p><code>euribor3m</code>, the (numeric) euribor 3-month rate;
</p>
</li>
<li> <p><code>nr_employed</code>, a numeric variable with the number of employees
in the company in that quarter;
</p>
</li>
<li> <p><code>subscribed</code>, a factor with levels <code>"yes"</code> and <code>"no"</code>.
</p>
</li></ul>



<h3>References</h3>

<p>UCI Machine Learning Repository. <br />
<code>https://archive.ics.uci.edu/ml/datasets/bank+marketing</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(bank)

# remove loans with unknown status, the corresponding coefficient is NA in glm().
bank = bank[bank$loan != "unknown", ]

# short-hand variable names.
r = bank[, "subscribed"]
s = bank[, c("age")]
p = bank[, setdiff(names(bank), c("subscribed", "age"))]

m = zlrm(response = r, sensitive = s, predictors = p, unfairness = 0.05)
summary(m)
</code></pre>

<hr>
<h2 id='communities.and.crime'>Communities and Crime Data Set</h2><span id='topic+communities.and.crime'></span>

<h3>Description</h3>

<p>Combined socio-economic data from the 1990 Census, law enforcement data
from the 1990 LEMAS survey, and crime data from the 1995 FBI UCR for various
communities in the United States.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(communities.and.crime)
</code></pre>


<h3>Format</h3>

<p>The data contains 1969 observations and 104 variables. See the UCI Machine
Learning Repository for details.
</p>


<h3>Note</h3>

<p>The data set has been pre-processed as in Komiyama et al. (2018), with the
following exceptions:
</p>

<ul>
<li><p> the variable <code>community</code> has been dropped, as it is
non-predictive and contains a sizeable number of missing values;
</p>
</li>
<li><p> the variables <code>LemasSwornFT</code>, <code>LemasSwFTPerPop</code>,
<code>LemasSwFTFieldOps</code>, <code>LemasSwFTFieldPerPop</code>,
<code>LemasTotalReq</code>, <code>LemasTotReqPerPop</code>, <code>PolicReqPerOffic</code>,
<code>PolicPerPop</code>, <code>RacialMatchCommPol</code>, <code>PctPolicWhite</code>,
<code>PctPolicBlack</code>, <code>PctPolicHisp</code>, <code>PctPolicAsian</code>,
<code>PctPolicMinor</code>, <code>OfficAssgnDrugUnits</code>,
<code>NumKindsDrugsSeiz</code>, <code>PolicAveOTWorked</code>, <code>PolicCars</code>,
<code>PolicOperBudg</code>, <code>LemasPctPolicOnPatr</code>,
<code>LemasGangUnitDeploy</code> and <code>PolicBudgPerPop</code> have been dropped
because they have more than 80% missing values.
</p>
</li></ul>

<p>In that paper, <code>ViolentCrimesPerPop</code> is the response variable,
<code>racepctblack</code> and <code>PctForeignBorn</code> are the sensitive attributes and
the remaining variables are used as predictors.
</p>
<p>The data contain too many variable to list them here: we refer the reader to
the documentation on the UCI Machine Learning Repository.
</p>


<h3>References</h3>

<p>UCI Machine Learning Repository: <br />
<code>http://archive.ics.uci.edu/ml/datasets/communities+and+crime</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(communities.and.crime)

# short-hand variable names.
cc = communities.and.crime[complete.cases(communities.and.crime), ]
r = cc[, "ViolentCrimesPerPop"]
s = cc[, c("racepctblack", "PctForeignBorn")]
p = cc[, setdiff(names(cc), c("ViolentCrimesPerPop", names(s)))]

m = nclm(response = r, sensitive = s, predictors = p, unfairness = 0.05)
summary(m)

m = frrm(response = r, sensitive = s, predictors = p, unfairness = 0.05)
summary(m)
</code></pre>

<hr>
<h2 id='compas'>Criminal Offenders Screened in Florida</h2><span id='topic+compas'></span>

<h3>Description</h3>

<p>A collection of criminal offenders screened in Florida (US) during 2013-14.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(compas)
</code></pre>


<h3>Format</h3>

<p>The data contains 5855 observations and the following variables:
</p>

<ul>
<li> <p><code>age</code>, a continuous variable containing the age (in years) of the
person;
</p>
</li>
<li> <p><code>juv_fel_count</code>, a continuous variable containing the number of
juvenile felonies;
</p>
</li>
<li> <p><code>decile_score</code>, a continuous variable, the decile of the COMPAS
score;
</p>
</li>
<li> <p><code>juv_misd_count</code>, a continuous variable containing the number of
juvenile misdemeanors;
</p>
</li>
<li> <p><code>juv_other_count</code>, a continuous variable containing the number
of prior juvenile convictions that are not considered either felonies or
misdemeanors;
</p>
</li>
<li> <p><code>v_decile_score</code>, a continuous variable containing the predicted
decile of the COMPAS score;
</p>
</li>
<li> <p><code>priors_count</code>, a continuous variable containing the number of
prior crimes committed;
</p>
</li>
<li> <p><code>sex</code>, a factor with levels <code>"Female"</code> and <code>"Male"</code>;
</p>
</li>
<li> <p><code>two_year_recid</code>, a factor with two levels <code>"Yes"</code> and
<code>"No"</code> (if the person has recidivated within two years);
</p>
</li>
<li> <p><code>race</code>, a factor encoding the race of the person;
</p>
</li>
<li> <p><code>c_jail_in</code>, a numeric variable containing the date in which the
person entered jail (normalized between 0 and 1);
</p>
</li>
<li> <p><code>c_jail_out</code>, a numeric variable containing the date in which the
person was released from jail (normalized between 0 and 1);
</p>
</li>
<li> <p><code>c_offense_date</code>, a numeric variable containing the date the
offense was committed;
</p>
</li>
<li> <p><code>screening_date</code>, a numeric variable containing the date in which
the person was screened (normalized between 0 and 1);
</p>
</li>
<li> <p><code>in_custody</code>, a numeric variable containing the date in which the
person was placed in custody (normalized between 0 and 1);
</p>
</li>
<li> <p><code>out_custody</code>, a numeric variable containing the date in which
the person was released from custody (normalized between 0 and 1);
</p>
</li></ul>



<h3>Note</h3>

<p>The data set has been pre-processed as in Komiyama et al. (2018), with the
following exceptions:
</p>

<ul>
<li><p> the <code>race</code> variable has not been reduced to a binary factor with
levels <code>"African-American"</code> and <code>"not African-American"</code>;
</p>
</li>
<li><p> the variables <code>type_of_assessment</code>, <code>v_type_of_assessment</code>
have been dropped from the analysis because they take the same value for
all observations;
</p>
</li>
<li><p> variables like <code>c_jail_in</code> and <code>c_jail_out</code> that encode
dates have been jointly rescaled to preserve the temporal ordering of
events.
</p>
</li></ul>

<p>In that paper, <code>two_year_recid</code> is the response variable, <code>sex</code> and
<code>race</code> are the sensitive attributes and the remaining variables are
used as predictors.
</p>


<h3>References</h3>

<p>Angwin J, Larson J, Mattu S, Kirchner L (2016). &quot;Machine Bias: Theres Software
Used Around the Country to Predict Future Criminals.&quot; <br />
<code>https://www.propublica.org</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(compas)

# convert the response back to a numeric variable.
compas$two_year_recid = as.numeric(compas$two_year_recid) - 1

# short-hand variable names.
r = compas[, "two_year_recid"]
s = compas[, c("sex", "race")]
p = compas[, setdiff(names(compas), c("two_year_recid", "sex", "race"))]

m = nclm(response = r, sensitive = s, predictors = p, unfairness = 0.05)
summary(m)

m = frrm(response = r, sensitive = s, predictors = p, unfairness = 0.05)
summary(m)
</code></pre>

<hr>
<h2 id='drug.consumption'>Drug Consumption</h2><span id='topic+drug.consumption'></span>

<h3>Description</h3>

<p>Predict drug consumption based on psychological scores and demographics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(drug.consumption)
</code></pre>


<h3>Format</h3>

<p>The data contains 1885 observations and 31 variables. See the UCI Machine
Learning Repository for details.
</p>


<h3>Note</h3>

<p>The data set has been minimally pre-processed following the instructions on
the UCI Machine Learning Repository to re-encode the variables. Categorical
variables are stored as factors and the psychological scores are stored as
numeric variables on their original scales.
</p>
<p>Any of the drug use variables can be used as the response variable
(with 7 different levels); <code>Age</code>, <code>Gender</code> and <code>Race</code> are the
sensitive attributes. The remaining variables are used as predictors.
</p>
<p>The data contain the following variables:
</p>

<ul>
<li> <p><code>Age</code>, a factor with 6 10-years age brackets;
</p>
</li>
<li> <p><code>Gender</code>, as a factor;
</p>
</li>
<li> <p><code>Education</code>, a factor with 9 levels from <code>"Left school
      before 16"</code> to <code>"Doctorate degree"</code>;
</p>
</li>
<li> <p><code>Country</code>, a factor with 7 different levels for <code>"USA"</code>,
<code>"New Zealand"</code>, <code>"Other"</code>, <code>"Australia"</code>,
<code>"Republic of Ireland" "Canada"</code> and <code>"UK"</code>;
</p>
</li>
<li> <p><code>Race</code> a factor with 7 levels comprising mixed backgrounds as
well;
</p>
</li>
<li> <p><code>Nscore</code>, <code>Escore</code>, <code>Oscore</code>, <code>Ascore</code>,
<code>Cscore</code>, numeric scores from the five-factor model for
personality traits;
</p>
</li>
<li> <p><code>Impulsive</code>, a numeric score for impulsivity;
</p>
</li>
<li> <p><code>SS</code>, a numeric score for sensation seeking;
</p>
</li>
<li> <p><code>Alcohol</code>, <code>Amphet</code>, <code>Amyl</code>, <code>Benzos</code>,
<code>Caff</code>, <code>Cannabis</code>, <code>Choc</code>, <code>Coke</code>, <code>Crack</code>,
<code>Ecstasy</code>, <code>Heroin</code>, <code>Ketamine</code>, <code>Legalh</code>,
<code>LSD</code>, <code>Meth</code>, <code>Mushrooms</code>, <code>Nicotine</code>, <code>Semer</code>
and <code>VSA</code>: factors with 7 levels ranging from <code>"Never Used"</code> to
<code>"Used in Last Day"</code>.
</p>
</li></ul>



<h3>References</h3>

<p>UCI Machine Learning Repository. <br />
<code>https://archive-beta.ics.uci.edu/dataset/373/</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(drug.consumption)

# short-hand variable names.
r = drug.consumption[, "Meth"]
s = drug.consumption[, c("Age", "Gender", "Race")]
p = drug.consumption[, c("Education", "Nscore", "Escore", "Oscore", "Ascore",
                         "Cscore", "Impulsive", "SS")]

# collapse levels with low observed frequencies.
levels(p$Education) =
  c("at.most.18y", "at.most.18y", "at.most.18y", "at.most.18y", "university",
    "diploma", "bachelor", "master", "phd")

## Not run: 
m = fgrrm(response = r, sensitive = s, predictors = p, ,
      family = "multinomial", unfairness = 0.05)
summary(m)

HH = drug.consumption$Heroin
levels(HH) = c("Never Used", "Used", "Used", "Used", "Used Recently",
               "Used Recently", "Used Recently")

m = fgrrm(response = HH, sensitive = s, predictors = p, ,
      family = "multinomial", unfairness = 0.05)
summary(m)

## End(Not run)</code></pre>

<hr>
<h2 id='fairml.cv'>Cross-Validation for Fair Models</h2><span id='topic+fairml.cv'></span><span id='topic+cv.loss'></span><span id='topic+cv.unfairness'></span><span id='topic+cv.folds'></span>

<h3>Description</h3>

<p>Cross-validation for the models in the <span class="pkg">fairml</span> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fairml.cv(response, predictors, sensitive, method = "k-fold", ..., unfairness,
  model, model.args = list(), cluster)

cv.loss(x)
cv.unfairness(x)
cv.folds(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fairml.cv_+3A_response">response</code></td>
<td>
<p>a numeric vector, the response variable.</p>
</td></tr>
<tr><td><code id="fairml.cv_+3A_predictors">predictors</code></td>
<td>
<p>a numeric matrix or a data frame containing numeric and
factor columns; the predictors.</p>
</td></tr>
<tr><td><code id="fairml.cv_+3A_sensitive">sensitive</code></td>
<td>
<p>a numeric matrix or a data frame containing numeric and
factor columns; the sensitive attributes.</p>
</td></tr>
<tr><td><code id="fairml.cv_+3A_method">method</code></td>
<td>
<p>a character string, either <code>k-fold</code>, <code>custom-folds</code>
or <code>hold-out</code>. See below for details.</p>
</td></tr>
<tr><td><code id="fairml.cv_+3A_...">...</code></td>
<td>
<p>additional arguments for the cross-validation <code>method</code>.</p>
</td></tr>
<tr><td><code id="fairml.cv_+3A_unfairness">unfairness</code></td>
<td>
<p>a positive number in [0, 1], the proportion of the explained
variance that can be attributed to the sensitive attributes.</p>
</td></tr>
<tr><td><code id="fairml.cv_+3A_model">model</code></td>
<td>
<p>a character string, the label of the model. Currently
<code>"nclm"</code>, <code>"frrm"</code>, <code>"fgrrm"</code>, <code>"zlm"</code> and <code>"zlrm"</code>
are available.</p>
</td></tr>
<tr><td><code id="fairml.cv_+3A_model.args">model.args</code></td>
<td>
<p>additional arguments passed to model estimation.</p>
</td></tr>
<tr><td><code id="fairml.cv_+3A_cluster">cluster</code></td>
<td>
<p>an optional cluster object from package <span class="pkg">parallel</span>, to
process folds or subsamples in parallel.</p>
</td></tr>
<tr><td><code id="fairml.cv_+3A_x">x</code></td>
<td>
<p>an object of class <code>fair.kcv</code> or <code>fair.kcv.list</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The following cross-validation methods are implemented:
</p>

<ul>
<li> <p><em>k-fold</em>: the data are split in <code>k</code> subsets of equal size.
For each subset in turn, <code>model</code> is fitted on the other <code>k - 1</code>
subsets and the loss function is then computed using that subset. Loss
estimates for each of the <code>k</code> subsets are then combined to give an
overall loss for data.
</p>
</li>
<li> <p><em>custom-folds</em>: the data are manually partitioned by the user
into subsets, which are then used as in k-fold cross-validation. Subsets
are not constrained to have the same size, and every observation must be
assigned to one subset.
</p>
</li>
<li> <p><em>hold-out</em>: <code>k</code> subsamples of size <code>m</code> are sampled
independently without replacement from the data. For each subsample,
<code>model</code> is fitted on the remaining <code>m - length(response)</code>
samples and the loss function is computed on the <code>m</code> observations in
the subsample. The overall loss estimate is the average of the <code>k</code>
loss estimates from the subsamples.
</p>
</li></ul>

<p>Cross-validation methods accept the following optional arguments:
</p>

<ul>
<li> <p><code>k</code>: a positive integer number, the number of groups into which
the data will be split (in k-fold cross-validation) or the number of times
the data will be split in training and test samples (in hold-out
cross-validation).
</p>
</li>
<li> <p><code>m</code>: a positive integer number, the size of the test set in
hold-out cross-validation.
</p>
</li>
<li> <p><code>runs</code>: a positive integer number, the number of times
k-fold or hold-out cross-validation will be run.
</p>
</li>
<li> <p><code>folds</code>: a list in which element corresponds to one fold and
contains the indices for the observations that are included to that fold;
or a list with an element for each run, in which each element is itself a
list of the folds to be used for that run.
</p>
</li></ul>

<p>If cross-validation is used with multiple <code>runs</code>, the overall loss is the
average of the loss estimates from the different runs.
</p>
<p>The predictive performance of the models is measured using the mean square
error as the loss function.
</p>


<h3>Value</h3>

<p><code>fairml.cv()</code> returns an object of class <code>fair.kcv.list</code> if
<code>runs</code> is at least 2, an object of class <code>fair.kcv</code> if <code>runs</code>
is equal to 1.
</p>
<p><code>cv.loss()</code> returns a numeric vector or a numeric matrix containing the
values of the loss function computed for each run of cross-validation.
</p>
<p><code>cv.unfairness()</code> returns a numeric vectors containing the values of the
unfairness criterion computed on the validation folds for each run of
cross-validation.
</p>
<p><code>cv.folds()</code> returns a list containing the indexes of the observations in
each of the cross-validation folds. In the case of k-fold cross-validation,
if <code>runs</code> is larger than <code>1</code>, each element of the list is itself a
list with the indexes for the observations in each fold in each run.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>Examples</h3>

<pre><code class='language-R'>kcv = fairml.cv(response = vu.test$gaussian, predictors = vu.test$X,
        sensitive = vu.test$S, unfairness = 0.10, model = "nclm",
        method = "k-fold", k = 10, runs = 10)
kcv
cv.loss(kcv)
cv.unfairness(kcv)

# run a second cross-validation with the same folds.
fairml.cv(response = vu.test$gaussian, predictors = vu.test$X,
        sensitive = vu.test$S, unfairness = 0.10, model = "nclm",
        method = "custom-folds", folds = cv.folds(kcv))

# run cross-validation in parallel.
## Not run: 
library(parallel)
cl = makeCluster(2)
fairml.cv(response = vu.test$gaussian, predictors = vu.test$X,
  sensitive = vu.test$S, unfairness = 0.10, model = "nclm",
  method = "k-fold", k = 10, runs = 10, cluster = cl)
stopCluster(cl)

## End(Not run)</code></pre>

<hr>
<h2 id='fairness.profile.plot'>Profile Fair Models with Respect to Tuning Parameters</h2><span id='topic+fairness.profile.plot'></span>

<h3>Description</h3>

<p>Visually explore various aspect of a model over the range of possible values
of the tuning parameters that control its fairness.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fairness.profile.plot(response, predictors, sensitive, unfairness,
  legend = FALSE, type = "coefficients", model, model.args = list(), cluster)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fairness.profile.plot_+3A_response">response</code></td>
<td>
<p>a numeric vector, the response variable.</p>
</td></tr>
<tr><td><code id="fairness.profile.plot_+3A_predictors">predictors</code></td>
<td>
<p>a numeric matrix or a data frame containing numeric and
factor columns; the predictors.</p>
</td></tr>
<tr><td><code id="fairness.profile.plot_+3A_sensitive">sensitive</code></td>
<td>
<p>a numeric matrix or a data frame containing numeric and
factor columns; the sensitive attributes.</p>
</td></tr>
<tr><td><code id="fairness.profile.plot_+3A_unfairness">unfairness</code></td>
<td>
<p>a vector of positive numbers in [0, 1], how unfair is the
model allowed to be. The default value is <code>seq(from = 0.00, to = 1,
    by = 0.02)</code>.</p>
</td></tr>
<tr><td><code id="fairness.profile.plot_+3A_legend">legend</code></td>
<td>
<p>a logical value, whether to add a legend to the plot.</p>
</td></tr>
<tr><td><code id="fairness.profile.plot_+3A_type">type</code></td>
<td>
<p>a character string, either <code>"coefficients"</code> (the default),
<code>"constraints"</code>,  <code>"precision-recall"</code> or <code>"rmse"</code>.</p>
</td></tr>
<tr><td><code id="fairness.profile.plot_+3A_model">model</code></td>
<td>
<p>a character string, the label of the model. Currently
<code>"nclm"</code>, <code>"frrm"</code>, <code>"fgrrm"</code>, <code>"zlm"</code> and <code>"zlrm"</code>
are available.</p>
</td></tr>
<tr><td><code id="fairness.profile.plot_+3A_model.args">model.args</code></td>
<td>
<p>additional arguments passed to model estimation.</p>
</td></tr>
<tr><td><code id="fairness.profile.plot_+3A_cluster">cluster</code></td>
<td>
<p>an optional cluster object from package <span class="pkg">parallel</span>, to
fit models in parallel.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>fairness.profile.plot()</code> fits the model for all the values of the
argument <code>unfairness</code>, and produces a profile plot of the regression
coefficients or the proportion of explained variance.
</p>
<p>If <code>type = "coefficients"</code>, the coefficients of the model are plotted
against the values of <code>unfairness</code>.
</p>
<p>If <code>type = "constraints"</code>, the following quantities are plotted against
the values of <code>unfairness</code>:
</p>

<ol>
<li><p> For model <code>"nclm"</code>, and model <code>"frrm"</code> with
<code>definition = "sp-komiyama"</code>:
</p>

<ol>
<li><p> the proportion of variance explained by the sensitive attributes
(with respect to the response);
</p>
</li>
<li><p> the proportion of variance explained by the predictors (with respect
to the response);
</p>
</li>
<li><p> the proportion of variance explained by the sensitive attributes
(with respect to the combined sensitive attributes and predictors).
</p>
</li></ol>

</li>
<li><p> For model <code>"frrm"</code> with <code>definition = "eo-komiyama"</code>:
</p>

<ol>
<li><p> the proportion of variance explained by the sensitive attributes
(with respect to the fitted values);
</p>
</li>
<li><p> the proportion of variance explained by the response (with respect
to the fitted values);
</p>
</li>
<li><p> the proportion of variance explained by the sensitive attributes
(with respect to the combined sensitive attributes and response).
</p>
</li></ol>

</li>
<li><p> For model <code>"frrm"</code> with <code>definition = "if-berk"</code>, the ratio
between the individual fairness loss computed for a given values of the
constraint and that of the unrestricted model with <code>unfairness = 1</code>.
</p>
</li>
<li><p> For model <code>"fgrrm"</code>: same as for <code>"frrm"</code> for each
<code>definition</code>.
</p>
</li>
<li><p> For models <code>"zlm"</code> and <code>"zlrm"</code>: the correlations between
the fitted values (from <code>fitted()</code> with <code>type = "link"</code>) and the
sensitive attributes.
</p>
</li></ol>

<p>If <code>type = "precision-recall"</code> and the <code>model</code> is a classifier, the
precision, recall and F1 measures are plotted against the values of
<code>unfairness</code>.
</p>
<p>If <code>type = "rmse"</code> and the <code>model</code> is a linear regression, the
residuals mean square error are plotted against the values of
<code>unfairness</code>.
</p>


<h3>Value</h3>

<p>A <code>trellis</code> object containing a <span class="pkg">lattice</span> plot.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(vu.test)
fairness.profile.plot(response = vu.test$gaussian, predictors = vu.test$X,
  sensitive = vu.test$S, type = "coefficients", model = "nclm", legend = TRUE)
fairness.profile.plot(response = vu.test$gaussian, predictors = vu.test$X,
  sensitive = vu.test$S, type = "constraints", model = "nclm", legend = TRUE)
fairness.profile.plot(response = vu.test$gaussian, predictors = vu.test$X,
  sensitive = vu.test$S, type = "rmse", model = "nclm", legend = TRUE)

# profile plots fitting models in parallel.
## Not run: 
library(parallel)
cl = makeCluster(2)
fairness.profile.plot(response = vu.test$gaussian, predictors = vu.test$X,
  sensitive = vu.test$S, model = "nclm", cluster = cl)
stopCluster(cl)

## End(Not run)</code></pre>

<hr>
<h2 id='flchain'>Obesity Levels</h2><span id='topic+flchain'></span>

<h3>Description</h3>

<p>A re-analysis of the <code>flchain</code> data set in the <span class="pkg">survival</span> package.
</p>


<h3>References</h3>

<p>Keya KN, Islam R, Pan S, Stockwell I, Foulds J (2020). Equitable Allocation of
Healthcare Resources with Fair Cox Models. Proceedings of the 2021 SIAM
International Conference on Data Mining (SDM), 190&ndash;198. <br />
<code>https://epubs.siam.org/doi/pdf/10.1137/1.9781611976700.22</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(survival)
data(flchain)

# complete data analysis.
flchain = flchain[complete.cases(flchain), ]
# short-hand variable names.
r = cbind(time = flchain$futime + 1, status = flchain$death)
s = flchain[, c("age", "sex")]
p = flchain[, c("sample.yr", "kappa", "lambda", "flc.grp", "creatinine", "mgus",
                "chapter")]

## Not run: 
m = fgrrm(response = r, sensitive = s, predictors = p, family = "cox",
          unfairness = 0.05)
summary(m)

## End(Not run)</code></pre>

<hr>
<h2 id='frrm'>Fair Ridge Regression Model</h2><span id='topic+frrm'></span><span id='topic+fgrrm'></span>

<h3>Description</h3>

<p>A regression model enforcing fairness with a ridge penalty.
</p>


<h3>Usage</h3>

<pre><code class='language-R'># a fair ridge regression model.
frrm(response, predictors, sensitive, unfairness,
  definition = "sp-komiyama", lambda = 0, save.auxiliary = FALSE)
# a fair generalized ridge regression model.
fgrrm(response, predictors, sensitive, unfairness,
  definition = "sp-komiyama", family = "binomial", lambda = 0,
  save.auxiliary = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="frrm_+3A_response">response</code></td>
<td>
<p>a numeric vector, the response variable.</p>
</td></tr>
<tr><td><code id="frrm_+3A_predictors">predictors</code></td>
<td>
<p>a numeric matrix or a data frame containing numeric and
factor columns; the predictors.</p>
</td></tr>
<tr><td><code id="frrm_+3A_sensitive">sensitive</code></td>
<td>
<p>a numeric matrix or a data frame containing numeric and
factor columns; the sensitive attributes.</p>
</td></tr>
<tr><td><code id="frrm_+3A_unfairness">unfairness</code></td>
<td>
<p>a positive number in [0, 1], how unfair is the model allowed
to be. A value of <code>0</code> means the model is completely fair, while a value
of <code>1</code> means the model is not constrained to be fair at all.</p>
</td></tr>
<tr><td><code id="frrm_+3A_definition">definition</code></td>
<td>
<p>a character string, the label of the definition of fairness
used in fitting the model. Currently either <code>"sp-komiyama"</code>,
<code>"eo-komiyama"</code> or <code>"if-berk"</code>. It may also be a function: see
below for details.</p>
</td></tr>
<tr><td><code id="frrm_+3A_family">family</code></td>
<td>
<p>a character string, either <code>"gaussian"</code> to fit a linear
regression, <code>"binomial"</code> to fit a logistic regression, <code>"poisson"</code>
to fit a log-linear regression, <code>"cox"</code> to fit a Cox proportional
hazards regression of <code>"multinomial"</code> to fit a multinomial logistic
regression.</p>
</td></tr>
<tr><td><code id="frrm_+3A_lambda">lambda</code></td>
<td>
<p>a non-negative number, a ridge-regression penalty coefficient.
It defaults to zero.</p>
</td></tr>
<tr><td><code id="frrm_+3A_save.auxiliary">save.auxiliary</code></td>
<td>
<p>a logical value, whether to save the fitted values and
the residuals of the auxiliary model that constructs the decorrelated
predictors. The default value is <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>frrm()</code> and <code>fgrrm()</code> can accommodate different definitions of
fairness, which can be selected via the <code>definition</code> argument. The labels
for the built-in definitions are:
</p>

<ul>
<li> <p><code>"sp-komiyama"</code> for the same definition of fairness as
<code>nclm()</code>: the model bounds the proportion of the variance that is
explained by the sensitive attributes over the total explained variance.
This falls within the definition of statistical parity.
</p>
</li>
<li> <p><code>"eo-komiyama"</code> enforces equality of opportunity in a similar
way: it regresses the fitted values against the sensitive attributes and
the response, and it bounds the proportion of the variance explained by
the sensitive attributes over the total explained variance in that model.
</p>
</li>
<li> <p><code>"if-berk"</code> enforces individual fairness by penalizing the model
for each pair of observations with different values of the sensitive
attributes and different responses.
</p>
</li></ul>

<p>Users may also pass a function via the <code>definition</code> argument to plug
custom fairness definitions. This function should have signature
<code>function(model, y, S, U, family)</code> and return an array with an element
called <code>"value"</code> (optionally along with others). The arguments will
contain the model fitted for the current level of fairness (<code>model</code>),
the sanitized response variable (<code>y</code>), the design matrix for the
sanitized sensitive attributes (<code>S</code>), the design matrix for the
sanitized decorrelated predictors (<code>U</code>) and the character string
identifying the family the model belongs to (<code>family</code>).
</p>
<p>The algorithm works like this:
</p>

<ol>
<li><p> regresses the predictors against the sensitive attributes;
</p>
</li>
<li><p> constructs a new set of predictors that are decorrelated from the
sensitive attributes using the residuals of this regression;
</p>
</li>
<li><p> regresses the response against the decorrelated predictors and the
sensitive attributes; while
</p>
</li>
<li><p> using a ridge penalty to control the proportion of variance the
sensitive attributes can explain with respect to the overall explained
variance of the model.
</p>
</li></ol>

<p>Both <code>sensitive</code> and <code>predictors</code> are standardized internally before
estimating the regression coefficients, which are then rescaled back to match
the original scales of the variables.
</p>
<p><code>fgrrm()</code> is the extension of <code>frrm()</code> to generalized linear models,
currently implementing linear (<code>family = "gaussian"</code>) and logistic
(<code>family = "binomial"</code>) regressions. <code>fgrrm()</code> is equivalent to
<code>frrm()</code> with <code>family = "gaussian"</code>. The definition of fairness are
identical between <code>frrm()</code> and <code>fgrrm()</code>.
</p>


<h3>Value</h3>

<p><code>frrm()</code> returns an object of class <code>c("frrm", "fair.model")</code>.
<code>fgrrm()</code> returns an object of class <code>c("fgrrm", "fair.model")</code>.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>References</h3>

<p>Scutari M, Panero F, Proissl M (2022). &quot;Achieving Fairness with a Simple Ridge
Penalty&quot;. Statistics and Computing, <strong>32</strong>, 77. <br />
<code>https://link.springer.com/content/pdf/10.1007/s11222-022-10143-w.pdf</code>
</p>


<h3>See Also</h3>

<p><a href="#topic+nclm">nclm</a>, <a href="#topic+zlm">zlm</a>, <a href="#topic+zlrm">zlrm</a></p>

<hr>
<h2 id='german.credit'>German Credit Data</h2><span id='topic+german.credit'></span>

<h3>Description</h3>

<p>A credit scoring data set that can be used to predict defaults on consumer
loans in the German market.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(german.credit)
</code></pre>


<h3>Format</h3>

<p>The data contains 1000 observations (700 good loans, 300 bad loans) and the
following variables:
</p>

<ul>
<li> <p><code>Account_status</code>: a factor with four levels representing the
amount of money in the account or <code>"no chcking account"</code>.
</p>
</li>
<li> <p><code>Duration</code>: a continuous variable, the duration in months.
</p>
</li>
<li> <p><code>Credit_history</code>: a factor with five levels representing possible
credit history backgrounds.
</p>
</li>
<li> <p><code>Purpose</code>: a factor with ten levels representing possible reasons
for taking out a loan.
</p>
</li>
<li> <p><code>Credit_amount</code>: a continuous variable.
</p>
</li>
<li> <p><code>Savings_bonds</code>: a factor with five levels representing amount of
money available in savings and bonds or
<code>"unknown / no savings account"</code>.
</p>
</li>
<li> <p><code>Present_employment_since</code>: a factor with five levels representing
the length of tenure in the current employment or <code>"unemployed"</code>.
</p>
</li>
<li> <p><code>Installment_rate</code>: a continuous variable, the installment rate in
percentage of disposable income.
</p>
</li>
<li> <p><code>Other_debtors_guarantors</code>: a factor with levels <code>"none"</code>,
<code>"co-applicant"</code> and <code>"guarantor"</code>.
</p>
</li>
<li> <p><code>Resident_since</code>: a continuous variable, number of years in the
current residence.
</p>
</li>
<li> <p><code>Property</code>: a factor with four levels describing the type of
property to be bought or <code>"unknown / no property"</code>.
</p>
</li>
<li> <p><code>Age</code>: a continuous variable, the age in years.
</p>
</li>
<li> <p><code>Other_installment_plans</code>: a factor with levels <code>"bank"</code>,
<code>"none"</code> and <code>"stores"</code>.
</p>
</li>
<li> <p><code>Housing</code>: a factor with levels <code>"rent"</code>, <code>"own"</code> and
<code>"for free"</code>.
</p>
</li>
<li> <p><code>Existing_credits</code>: a continuous variable, the number of existing
credit lines at this bank.
</p>
</li>
<li> <p><code>Job</code>: a factor with four levels for different job descriptions.
</p>
</li>
<li> <p><code>People_maintenance_for</code>: a continuous variable, the number of
people being liable to provide maintenance for.
</p>
</li>
<li> <p><code>Telephone</code>: a factor with levels <code>"none"</code> and <code>"yes"</code>.
</p>
</li>
<li> <p><code>Foreign_worker</code>: a factor with levels <code>"no"</code> and
<code>"yes"</code>.
</p>
</li>
<li> <p><code>Credit_risk</code>: a factor with levels <code>"BAD"</code> and
<code>"GOOD"</code>.
</p>
</li>
<li> <p><code>Gender</code>: a factor with levels <code>"Male"</code> and
<code>"Female"</code>.
</p>
</li></ul>



<h3>Note</h3>

<p>The variable &quot;Personal status and sex&quot; in the original data has been
transformed into <code>Gender</code> by dropping the personal status information.
</p>


<h3>References</h3>

<p>UCI Machine Learning Repository: <br />
<code>https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)</code>
</p>

<hr>
<h2 id='health.retirement'>Health and Retirement Survey</h2><span id='topic+health.retirement'></span>

<h3>Description</h3>

<p>The University of Michigan Health and Retirement Study (HRS) longitudinal
dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(health.retirement)
</code></pre>


<h3>Format</h3>

<p>The data contains 38653 observations and 27 variables.
</p>


<h3>Note</h3>

<p>The data set has been minimally pre-processed: the redundant variables
<code>HISPANIC</code> and <code>BITHYR</code> were removed, along with the patient ID
<code>PID</code>. A single patient was recorded twice: the duplicate has been
removed. However, incomplete observations have been left in the data set.
</p>
<p>The number of dependencies in daily activities <code>score</code> is the response
(count) variable and <code>marriage</code>, <code>gender</code>, <code>race</code>,
<code>race.ethnicity</code> and <code>age</code> are the sensitive attributes. The
remaining variables are used as predictors.
</p>
<p>The data contain the following variables:
</p>

<ul>
<li> <p><code>year</code>, the year of retirement as a numeric variable;
</p>
</li>
<li> <p><code>age</code>, the age as a numeric variable;
</p>
</li>
<li> <p><code>educa</code>, the number of years in education as a numeric variable;
</p>
</li>
<li> <p><code>networth</code>, household net worth as a numeric variable;
</p>
</li>
<li> <p><code>cognition_catnew</code> cognistion assessment as a numeric variable;
</p>
</li>
<li> <p><code>bmi</code> as a numeric variable;
</p>
</li>
<li> <p><code>hlthrte</code>, a numeric health rating;
</p>
</li>
<li> <p><code>bloodp</code>, blood pressure diagnosis as a numeric variable;
</p>
</li>
<li> <p><code>diabetes</code>, diabetes diagnosis as a numeric variable;
</p>
</li>
<li> <p><code>cancer</code>, cancer diagnosis as a numeric variable;
</p>
</li>
<li> <p><code>lung</code>, lung disease diagnosis as a numeric variable;
</p>
</li>
<li> <p><code>heart</code>, heart condition diagnosis as a numeric variable;
</p>
</li>
<li> <p><code>stroke</code>, stroke diagnosis as a numeric variable;
</p>
</li>
<li> <p><code>pchiat</code>, psychiatric condition diagnosis as a numeric variable;
</p>
</li>
<li> <p><code>arthrit</code>, arthritis diagnosis as a numeric variable;
</p>
</li>
<li> <p><code>fall</code>, recently falling as a numeric variable;
</p>
</li>
<li> <p><code>pain</code>, pain conditions as a numeric variable;
</p>
</li>
<li> <p><code>A1c_adj</code>, biomarker for hemoglobin A1C;
</p>
</li>
<li> <p><code>CRP_adj</code>, biomarker for C-reactive protein;
</p>
</li>
<li> <p><code>CYSC_adj</code>, biomarker for Cystatin C;
</p>
</li>
<li> <p><code>HDL_adj</code>, biomarker for HDL cholesterol;
</p>
</li>
<li> <p><code>TC_adj</code>, biomarker for total cholesterol;
</p>
</li>
<li> <p><code>score</code>, another numeric health rating;
</p>
</li>
<li> <p><code>gender</code>, a factor with levels <code>"Female"</code> and <code>"Male"</code>;
</p>
</li>
<li> <p><code>marriage</code>, a factor with levels <code>"Married/Partner"</code> and
<code>"Not Married"</code>;
</p>
</li>
<li> <p><code>race</code>, a factor withe levels <code>"Black"</code>, <code>"Other"</code>
and <code>"White"</code>;
</p>
</li>
<li> <p><code>race.ethnicity</code>, a factor withe levels <code>"Hispanic"</code>,
<code>"NHB"</code>, <code>"NHW"</code> and <code>"Other"</code>.
</p>
</li></ul>



<h3>References</h3>

<p><code>https://hrs.isr.umich.edu/about</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(health.retirement)

# complete data analysis.
health.retirement = health.retirement[complete.cases(health.retirement), ]
# short-hand variable names.
r = health.retirement[, "score"]
s = health.retirement[, c("marriage", "gender", "race", "age")]
p = health.retirement[, setdiff(names(health.retirement), c(names(r), names(s)))]
# drop the second race variable.
p = p[, colnames(p) != "race.ethnicity"]

## Not run: 
# the lambda = 0.1 is very helpful in making model estimation succeed.
m = fgrrm(response = r, sensitive = s, predictors = p, ,
      family = "poisson", unfairness = 0.05, lambda = 0.1)
summary(m)

## End(Not run)</code></pre>

<hr>
<h2 id='law.school.admissions'>Law School Admission Council data</h2><span id='topic+law.school.admissions'></span>

<h3>Description</h3>

<p>Survey among students attending law school in the U.S. in 1991.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(law.school.admissions)
</code></pre>


<h3>Format</h3>

<p>The data contains 20800 observations and the following variables:
</p>

<ul>
<li> <p><code>age</code>, a continuous variable containing the student's age in
years;
</p>
</li>
<li> <p><code>decile1</code>, a continuous variable containing the student's decile
in the school given his grades in Year 1;
</p>
</li>
<li> <p><code>decile3</code>, a continuous variable containing the student's decile
in the school given his grades in Year 3;
</p>
</li>
<li> <p><code>fam_inc</code>, a continuous variable containing student's family
income bracket (from <code>1</code> to <code>5</code>);
</p>
</li>
<li> <p><code>lsat</code>, a continuous variable containing the student's LSAT
score;
</p>
</li>
<li> <p><code>ugpa</code>, a continuous variable containing the student's
undergraduate GPA;
</p>
</li>
<li> <p><code>gender</code>, a factor with levels <code>"female"</code> and <code>"male"</code>;
</p>
</li>
<li> <p><code>race1</code>, a factor with levels <code>"asian"</code>, <code>"black"</code>,
<code>"hisp"</code>, <code>"other"</code> and <code>"white"</code>;
</p>
</li>
<li> <p><code>cluster</code>, a factor with levels <code>"1"</code>, <code>"2"</code>,
<code>"3"</code>, <code>"4"</code>, <code>"5"</code> and <code>"6"</code> encoding the tiers of
law school prestige;
</p>
</li>
<li> <p><code>fulltime</code>, a factor with levels <code>"FALSE"</code> and
<code>"TRUE"</code>, whether the student will work full-time or part-time;
</p>
</li>
<li> <p><code>bar</code>, a factor with levels <code>"FALSE"</code> and <code>"TRUE"</code>,
whether the student passed the bar exam on the first try.
</p>
</li></ul>



<h3>Note</h3>

<p>The data set has been pre-processed as in Komiyama et al. (2018), with the
following exceptions:
</p>

<ul>
<li> <p><code>DOB_yr</code>, the year of birth, has been dropped because it is
(nearly) perfectly collinear with <code>age</code>, and thus it is redundant;
</p>
</li>
<li> <p><code>decile1b</code> has been dropped because it is (nearly) perfectly
collinear with <code>decile1</code>, and thus it is redundant.
</p>
</li></ul>

<p>In that paper, <code>ugpa</code> is the response variable, <code>age</code> and
<code>race1</code> are the sensitive attributes and the remaining variables are
used as predictors.
</p>


<h3>References</h3>

<p>Sander RH (2004). &quot;A Systemic Analysis of Affirmative Action in American Law
Schools&quot;. Stanford Law Review, 57:367&ndash;483.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(law.school.admissions)

# short-hand variable names.
ll = law.school.admissions
r = ll[, "ugpa"]
s = ll[, c("age", "race1")]
p = ll[, setdiff(names(ll), c("ugpa", "age", "race1"))]

m = nclm(response = r, sensitive = s, predictors = p, unfairness = 0.05)
summary(m)

m = frrm(response = r, sensitive = s, predictors = p, unfairness = 0.05)
summary(m)
</code></pre>

<hr>
<h2 id='methods+20for+20fair.model+20objects'>Extract information from fair.model objects</h2><span id='topic+methods+20for+20fair.model+20objects'></span><span id='topic+coef.fair.model'></span><span id='topic+residuals.fair.model'></span><span id='topic+fitted.fair.model'></span><span id='topic+sigma.fair.model'></span><span id='topic+deviance.fair.model'></span><span id='topic+logLik.fair.model'></span><span id='topic+nobs.fair.model'></span><span id='topic+print.fair.model'></span><span id='topic+summary.fair.model'></span><span id='topic+all.equal.fair.model'></span><span id='topic+plot.fair.model'></span><span id='topic+predict.nclm'></span><span id='topic+predict.zlm'></span><span id='topic+predict.zlrm'></span><span id='topic+predict.frrm'></span><span id='topic+predict.fgrrm'></span>

<h3>Description</h3>

<p>Extract various quantities of interest from an object of class
<code>fair.model</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'># methods for all fair.model objects.
## S3 method for class 'fair.model'
coef(object, ...)
## S3 method for class 'fair.model'
residuals(object, ...)
## S3 method for class 'fair.model'
fitted(object, type = "response", ...)
## S3 method for class 'fair.model'
sigma(object, ...)
## S3 method for class 'fair.model'
deviance(object, ...)
## S3 method for class 'fair.model'
logLik(object, ...)
## S3 method for class 'fair.model'
nobs(object, ...)
## S3 method for class 'fair.model'
print(x, digits, ...)
## S3 method for class 'fair.model'
summary(object, ...)
## S3 method for class 'fair.model'
all.equal(target, current, ...)
## S3 method for class 'fair.model'
plot(x, support = FALSE, regression = FALSE, ncol = 2, ...)

# predict() methods.
## S3 method for class 'nclm'
predict(object, new.predictors, new.sensitive, type = "response", ...)
## S3 method for class 'zlm'
predict(object, new.predictors, type = "response", ...)
## S3 method for class 'zlrm'
predict(object, new.predictors, type = "response", ...)
## S3 method for class 'frrm'
predict(object, new.predictors, new.sensitive, type = "response", ...)
## S3 method for class 'fgrrm'
predict(object, new.predictors, new.sensitive, type = "response", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="methods+2B20for+2B20fair.model+2B20objects_+3A_object">object</code>, <code id="methods+2B20for+2B20fair.model+2B20objects_+3A_x">x</code>, <code id="methods+2B20for+2B20fair.model+2B20objects_+3A_target">target</code>, <code id="methods+2B20for+2B20fair.model+2B20objects_+3A_current">current</code></td>
<td>
<p>an object of class <code>fair.model</code> or
<code>nclm</code>.</p>
</td></tr>
<tr><td><code id="methods+2B20for+2B20fair.model+2B20objects_+3A_type">type</code></td>
<td>
<p>a character string, the type of fitted value. If
<code>"response"</code>, <code>fitted()</code> and <code>predict()</code> will return the
fitted values (if the response in the model is continuous) or the
classification probabilities (if it was discrete). If <code>"class"</code> and
<code>object</code> is a classifier, <code>fitted()</code> and <code>predict()</code> will
return the class labels as a factor. If <code>"link"</code> and <code>object</code> is a
classifier, <code>fitted()</code> and <code>predict()</code> will return the linear
component of the fitted or predicted value, on the scale of the link
function.</p>
</td></tr>
<tr><td><code id="methods+2B20for+2B20fair.model+2B20objects_+3A_digits">digits</code></td>
<td>
<p>a non-negative integer, the number of significant digits.</p>
</td></tr>
<tr><td><code id="methods+2B20for+2B20fair.model+2B20objects_+3A_new.predictors">new.predictors</code></td>
<td>
<p>a numeric matrix or a data frame containing numeric and
factor columns; the predictors for the new observations.</p>
</td></tr>
<tr><td><code id="methods+2B20for+2B20fair.model+2B20objects_+3A_new.sensitive">new.sensitive</code></td>
<td>
<p>a numeric matrix or a data frame containing numeric and
factor columns; the sensitive attributes for the new observations.</p>
</td></tr>
<tr><td><code id="methods+2B20for+2B20fair.model+2B20objects_+3A_support">support</code></td>
<td>
<p>a logical value, whether to draw support lines (diagonal of the
first quadrant, horizontal line at zero, etc.) in <code>plot()</code>.</p>
</td></tr>
<tr><td><code id="methods+2B20for+2B20fair.model+2B20objects_+3A_regression">regression</code></td>
<td>
<p>a logical value, whether to draw the regression line of the
observed values on the fitted values from the model in <code>plot()</code>.</p>
</td></tr>
<tr><td><code id="methods+2B20for+2B20fair.model+2B20objects_+3A_ncol">ncol</code></td>
<td>
<p>a positive integer, the number of columns the plots will be
arranged into.</p>
</td></tr>
<tr><td><code id="methods+2B20for+2B20fair.model+2B20objects_+3A_...">...</code></td>
<td>
<p>additional arguments, currently ignored.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Marco Scutari</p>

<hr>
<h2 id='national.longitudinal.survey'>Income and Labour Market Activities</h2><span id='topic+national.longitudinal.survey'></span>

<h3>Description</h3>

<p>Survey results from the U.S. Bureau of Labor Statistics to gather information
on the labour market activities and other life events of several groups.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(national.longitudinal.survey)
</code></pre>


<h3>Format</h3>

<p>The data contains 4908 observations and the following variables:
</p>

<ul>
<li> <p><code>age</code>, a numeric variable containing the interviewee's age in
years;
</p>
</li>
<li> <p><code>race</code>, a factor with 20 levels denoting various racial/ethnic
origins;
</p>
</li>
<li> <p><code>gender</code>, a factor with levels <code>"Male"</code> and <code>"Female"</code>.
</p>
</li>
<li> <p><code>grade90</code>, a factor containing the highest completed school
grade from &quot;3RD GRADE&quot; to &quot;8TH YR COL OR MORE&quot;, with 18 levels;
</p>
</li>
<li> <p><code>income06</code>, a numeric variable, income in 2006 in 10000-USD
units;
</p>
</li>
<li> <p><code>income96</code>, a numeric variable, income in 1996 in 10000-USD
units;
</p>
</li>
<li> <p><code>income90</code>, a numeric variable, income in 1990 in 10000-USD
units;
</p>
</li>
<li> <p><code>partner</code>, a factor encoding whether the interviewee has a
partner, with levels <code>"No"</code> and <code>"Yes"</code>;
</p>
</li>
<li> <p><code>height</code>, a numeric variable, the height of the interviewee;
</p>
</li>
<li> <p><code>weight</code>, a numeric variable, the weight of the interviewee;
</p>
</li>
<li> <p><code>famsize</code>, a numeric variable, the number of family members;
</p>
</li>
<li> <p><code>genhealth</code>, a factor with levels <code>"Excellent"</code>,
<code>"Very Good"</code>, <code>"Good"</code>, <code>"Fair"</code>, <code>"Poor"</code> encoding
the general health status of the interviewee;
</p>
</li>
<li> <p><code>illegalact</code>, a numeric variable containing the number of illegal
acts committed by the interviewee;
</p>
</li>
<li> <p><code>charged</code>, a numeric variable containing the number of illegal
acts for which the interviewee has been charged;
</p>
</li>
<li> <p><code>jobsnum90</code>, a numeric value, the number of different jobs ever
reported;
</p>
</li>
<li> <p><code>afqt89</code>, a numeric value, the percentile score of the &quot;Profiles,
Armed Forces Qualification Test&quot; (AFQT);
</p>
</li>
<li> <p><code>typejob90</code>, a factor with 13 levels encoding different job
types;
</p>
</li>
<li> <p><code>jobtrain90</code>, a factor with levels <code>"No"</code> and <code>"Yes"</code>
encoding whether the job was classified as training.
</p>
</li></ul>



<h3>Note</h3>

<p>The data set has been pre-processed differently from Komiyama et al. (2018).
In particular:
</p>

<ul>
<li><p> the variables <code>income96</code> and <code>income06</code> have been retained
as alternative responses;
</p>
</li>
<li><p> the variables <code>height</code>, <code>weight</code>, <code>race</code>,
<code>partner</code> and <code>famsize</code> have been retained;
</p>
</li>
<li><p> the variables <code>grade90</code> and <code>genhealth</code> are coded as ordered
factors because they do not make sense on a numeric scale.
</p>
</li></ul>

<p>In that paper, <code>income90</code> is the response variable, <code>gender</code> and
<code>age</code> are the sensitive attributes.
</p>


<h3>References</h3>

<p>U.S. Bureau of Labor Statistics. <br />
<code>https://www.bls.gov/nls/</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(national.longitudinal.survey)

# short-hand variable names.
nn = national.longitudinal.survey
# remove alternative response variables.
nn = nn[, setdiff(names(nn), c("income96", "income06"))]
# short-hand variable names.
r = nn[, "income90"]
s = nn[, c("gender", "age")]
p = nn[, setdiff(names(nn), c("income90", "gender", "age"))]

m = nclm(response = r, sensitive = s, predictors = p, unfairness = 0.05)
summary(m)

m = frrm(response = r, sensitive = s, predictors = p, unfairness = 0.05)
summary(m)
</code></pre>

<hr>
<h2 id='nclm'>Nonconvex Optimization for Regression with Fairness Constraints</h2><span id='topic+nclm'></span>

<h3>Description</h3>

<p>Fair regression model based on nonconvex optimization from Komiyama
et al. (2018).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nclm(response, predictors, sensitive, unfairness, covfun, lambda = 0,
  save.auxiliary = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nclm_+3A_response">response</code></td>
<td>
<p>a numeric vector, the response variable.</p>
</td></tr>
<tr><td><code id="nclm_+3A_predictors">predictors</code></td>
<td>
<p>a numeric matrix or a data frame containing numeric and
factor columns; the predictors.</p>
</td></tr>
<tr><td><code id="nclm_+3A_sensitive">sensitive</code></td>
<td>
<p>a numeric matrix or a data frame containing numeric and
factor columns; the sensitive attributes.</p>
</td></tr>
<tr><td><code id="nclm_+3A_unfairness">unfairness</code></td>
<td>
<p>a positive number in [0, 1], how unfair is the model allowed
to be. A value of <code>0</code> means the model is completely fair, while a value
of <code>1</code> means the model is not constrained to be fair at all.</p>
</td></tr>
<tr><td><code id="nclm_+3A_covfun">covfun</code></td>
<td>
<p>a function computing covariance matrices. It defaults to the
<code>cov()</code> function from the <span class="pkg">stats</span> package.</p>
</td></tr>
<tr><td><code id="nclm_+3A_lambda">lambda</code></td>
<td>
<p>a non-negative number, a ridge-regression penalty coefficient.
It defaults to zero.</p>
</td></tr>
<tr><td><code id="nclm_+3A_save.auxiliary">save.auxiliary</code></td>
<td>
<p>a logical value, whether to save the fitted values and
the residuals of the auxiliary model that constructs the decorrelated
predictors. The default value is <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>nclm()</code> defines fairness as statistical parity. The model bounds the
proportion of the variance that is explained by the sensitive attributes over
the total explained variance.
</p>
<p>The algorithm proposed by Komiyama et al. (2018) works like this:
</p>

<ol>
<li><p> regresses the predictors against the sensitive attributes;
</p>
</li>
<li><p> constructs a new set of predictors that are decorrelated from the
sensitive attributes using the residuals of this regression;
</p>
</li>
<li><p> regresses the response against the decorrelated predictors and the
sensitive attributes, while
</p>
</li>
<li><p> bounding the proportion of variance the sensitive attributes can
explain with respect to the overall explained variance of the model.
</p>
</li></ol>

<p>Both <code>sensitive</code> and <code>predictors</code> are standardized internally before
estimating the regression coefficients, which are then rescaled back to match
the original scales of the variables. <code>response</code> is only standardized if
it has a variance smaller than <code>1</code>, as that seems to improve the
stability of the solutions provided by the optimizer (as far as the data
included in <span class="pkg">fairml</span> are concerned).
</p>
<p>The <code>covfun</code> argument makes it possible to specify a custom function to
compute the covariance matrices used in the constrained optimization. Some
examples are the kernel estimators described in Komiyama et al. (2018) and
the shrinkage estimators in the <span class="pkg">corpcor</span> package.
</p>


<h3>Value</h3>

<p><code>nclm()</code> returns an object of class <code>c("nclm", "fair.model")</code>.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>References</h3>

<p>Komiyama J, Takeda A, Honda J, Shimao H (2018). &quot;Nonconvex Optimization for
Regression with Fairness Constraints&quot;. Proceedints of the 35th International
Conference on Machine Learning (ICML), PMLR <strong>80</strong>:2737&ndash;2746. <br />
<code>http://proceedings.mlr.press/v80/komiyama18a/komiyama18a.pdf</code>
</p>


<h3>See Also</h3>

<p><a href="#topic+frrm">frrm</a>, <a href="#topic+zlm">zlm</a></p>

<hr>
<h2 id='obesity.levels'>Obesity Levels</h2><span id='topic+obesity.levels'></span>

<h3>Description</h3>

<p>Predict obesity levels based on eating habits and physical condition.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(obesity.levels)
</code></pre>


<h3>Format</h3>

<p>The data contains 2111 observations and 17 variables. See the UCI Machine
Learning Repository for details.
</p>


<h3>Note</h3>

<p>The data set has been minimally pre-processed: the only change is that the
only observation for which the <code>CALC</code> variable was equal to
<code>"Always"</code> has been changed to <code>"Frequently"</code> to merge the two
levels.
</p>
<p>The obesity level <code>NObeyesdad</code> is the response variable (with 7 different
levels) and <code>Age</code> and <code>Gender</code> are the sensitive attributes. The
remaining variables are used as predictors.
</p>
<p>The data contain the following variables:
</p>

<ul>
<li> <p><code>Gender</code>;
</p>
</li>
<li> <p><code>Age</code>;
</p>
</li>
<li> <p><code>Height</code>;
</p>
</li>
<li> <p><code>Weight</code>;
</p>
</li>
<li> <p><code>family_history_with_overweight</code>;
</p>
</li>
<li> <p><code>FAVC</code>, frequent consumption of high caloric food as a factor
with levels <code>"no"</code> and <code>"yes"</code>;
</p>
</li>
<li> <p><code>FCVC</code>, frequency of consumption of vegetables as a numeric
variable;
</p>
</li>
<li> <p><code>NCP</code>, number of main meals;
</p>
</li>
<li> <p><code>CAEC</code>, consumption of food between meals as a factor with levels
<code>"no"</code>, <code>"Sometimes"</code>, <code>"Frequently"</code> and <code>"Always"</code>;
</p>
</li>
<li> <p><code>SMOKE</code>, smoking status as a factor with levels <code>"no"</code> and
<code>"yes"</code>;
</p>
</li>
<li> <p><code>CH2O</code>, consumption of water daily as a numeric variable;
</p>
</li>
<li> <p><code>SCC</code>, calories consumption monitoring as a factor with level
<code>"no"</code> and <code>"yes"</code>;
</p>
</li>
<li> <p><code>FAF</code>, physical activity frequency as a numeric variable;
</p>
</li>
<li> <p><code>TUE</code>, time using technology devices as a numeric variable;
</p>
</li>
<li> <p><code>CALC</code>, consumption of alcohol as a dfactor with levels
<code>"no"</code>, <code>"Sometimes"</code>, <code>"Frequently"</code> and <code>"Always"</code>;
</p>
</li>
<li> <p><code>MTRANS</code>, transportation used as a factor with levels
<code>"Automobile"</code>, <code>"Bike"</code>, <code>"Motorbike"</code>,
<code>"Public_Transportation"</code> and <code>"Walking"</code>;
</p>
</li>
<li> <p><code>NObeyesdad</code>, the obesity level as a factor with levels
<code>"Insufficient_Weight"</code>, <code>"Normal_Weight"</code>,
<code>"Overweight_Level_I"</code>, <code>"Overweight_Level_II"</code>,
<code>"Obesity_Type_I"</code>, <code>"Obesity_Type_II"</code>,
<code>"Obesity_Type_III"</code>.
</p>
</li></ul>



<h3>References</h3>

<p>UCI Machine Learning Repository. <br />
<code>https://archive-beta.ics.uci.edu/dataset/544</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(obesity.levels)

# short-hand variable names.
r = obesity.levels[, "NObeyesdad"]
s = obesity.levels[, c("Gender", "Age")]
p = obesity.levels[, setdiff(names(obesity.levels), c("NObeyesdad", "Gender", "Age"))]

## Not run: 
# the lambda = 0.1 is very helpful in making model estimation succeed.
m = fgrrm(response = r, sensitive = s, predictors = p, ,
      family = "multinomial", unfairness = 0.05, lambda = 0.1)
summary(m)

## End(Not run)</code></pre>

<hr>
<h2 id='synthetic+20data+20sets'>Synthetic data set to test fair models</h2><span id='topic+vu.test'></span>

<h3>Description</h3>

<p>Synthetic data set used as test cases in the <span class="pkg">fairml</span> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(vu.test)
</code></pre>


<h3>Format</h3>

<p>The data are stored a list with following three elements:
</p>

<ul>
<li> <p><code>gaussian</code>, <code>binomial</code>, <code>poisson</code>, <code>coxph</code> and
<code>multinomial</code> are response variables for the different families;
</p>
</li>
<li> <p><code>X</code>, a numeric matrix containing 3 predictors called <code>X1</code>,
<code>X2</code> and <code>X3</code>;
</p>
</li>
<li> <p><code>S</code>, a numeric matrix containing 3 sensitive attributes called
<code>S1</code>, <code>S2</code> and <code>S3</code>.
</p>
</li></ul>



<h3>Note</h3>

<p>This data set is called <code>vu.test</code> because it is generated from
<em>v</em>ery <em>u</em>nfair models in which sensitive attributes explain the
lion's share of the overall explained variance or deviance.
</p>
<p>The code used to generate the predictors and the sensitive attributes is as
follows.
</p>
<pre>
library(mvtnorm)
sigma = matrix(0.3, nrow = 6, ncol = 6)
diag(sigma) = 1
n = 1000
X = rmvnorm(n, mean = rep(0, 6), sigma = sigma)
S = X[, 4:6]
X = X[, 1:3]
colnames(X) = c("X1", "X2", "X3")
colnames(S) = c("S1", "S2", "S3")
</pre>
<p>The continuous response in <code>gaussian</code> is produced as follows.
</p>
<pre>
gaussian = 2 + 2 * X[, 1] + 3 * X[, 2] + 4 * X[, 3] + 5 * S[, 1] +
               6 * S[, 2] + 7 * S[, 3] + rnorm(n, sd = 10)
</pre>
<p>The discrete response in <code>binomial</code> is produced as follows.
</p>
<pre>
nu = 1 + 0.5 * X[, 1] + 0.6 * X[, 2] + 0.7 * X[, 3] + 0.8 * S[, 1] +
         0.9 * S[, 2] + 1.0 * S[, 3]
binomial = rbinom(n = nrow(X), size = 1, prob = exp(nu) / (1 + exp(nu)))
binomial = as.factor(binomial)
</pre>
<p>The log-linear response in <code>poisson</code> is produced as follows.
</p>
<pre>
nu = 1 + 0.5 * X[, 1] + 0.6 * X[, 2] + 0.7 * X[, 3] + 0.8 * S[, 1] +
         0.9 * S[, 2] + 1.0 * S[, 3]
poisson = rpois(n = nrow(X), lambda = exp(nu))
</pre>
<p>The response for the Cox proportional hazards <code>coxph</code> is
produced as follows.
</p>
<pre>
fx = 1 + 0.5 * X[, 1] + 0.6 * X[, 2] + 0.7 * X[, 3] + 0.8 * S[, 1] +
         0.9 * S[, 2] + 1.0 * S[, 3]
hx = exp(fx)
ty = rexp(length(fx), hx)
tcens = rbinom(n = length(fx), prob = 0.3, size = 1)
coxph = cbind(time = ty, status = 1 - tcens)
</pre>
<p>The discrete response in <code>multinomial</code> is produced as follows.
</p>
<pre>
nu1 = 1 + 0.5 * X[, 1] + 0.6 * X[, 2] + 0.7 * X[, 3] + 0.8 * S[, 1] +
          0.9 * S[, 2] + 1.0 * S[, 3]
nu2 = 1 + 0.2 * X[, 1] + 0.2 * X[, 2] + 0.2 * X[, 3] + 0.6 * S[, 1] +
          0.6 * S[, 2] + 0.6 * S[, 3]
nu3 = 1 + 0.7 * X[, 1] + 0.6 * X[, 2] + 0.5 * X[, 3] + 0.1 * S[, 1] +
          0.1 * S[, 2] + 0.1 * S[, 3]
nu4 = 1 + 0.4 * X[, 1] + 0.4 * X[, 2] + 0.4 * X[, 3] + 0.4 * S[, 1] +
          0.4 * S[, 2] + 0.4 * S[, 3]
norm = exp(nu1) + exp(nu2) + exp(nu3) + exp(nu4)
probs = matrix(c(exp(nu1) / norm, exp(nu2) / norm,
                 exp(nu3) / norm, exp(nu4) / norm),
        ncol = 4, byrow = FALSE)
multinomial = apply(probs, MARGIN = 1,
                function(x) sample(letters[1:4], size = 1, prob = x))
multinomial = factor(multinomial, labels = letters[1:4])
</pre>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>Examples</h3>

<pre><code class='language-R'>summary(fgrrm(response = vu.test$gaussian, predictors = vu.test$X,
  sensitive = vu.test$S, unfairness = 1, family = "gaussian"))
summary(fgrrm(response = vu.test$binomial, predictors = vu.test$X,
  sensitive = vu.test$S, unfairness = 1, family = "binomial"))
summary(fgrrm(response = vu.test$poisson, predictors = vu.test$X,
  sensitive = vu.test$S, unfairness = 1, family = "poisson"))
summary(fgrrm(response = vu.test$coxph, predictors = vu.test$X,
  sensitive = vu.test$S, unfairness = 1, family = "cox"))
summary(fgrrm(response = vu.test$multinomial, predictors = vu.test$X,
  sensitive = vu.test$S, unfairness = 1, family = "multinomial"))
</code></pre>

<hr>
<h2 id='zlm'>Zafar's Linear and Logistic Regressions</h2><span id='topic+zlm'></span><span id='topic+zlm.orig'></span><span id='topic+zlrm'></span><span id='topic+zlrm.orig'></span>

<h3>Description</h3>

<p>Linear and logistic regression models enforcing fairness by bounding the
covariance between sensitive attributes and predictors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'># a fair linear regression model.
zlm(response, predictors, sensitive, unfairness)
zlm.orig(response, predictors, sensitive, max.abs.cov)
# a fair logistic regression model.
zlrm(response, predictors, sensitive, unfairness)
zlrm.orig(response, predictors, sensitive, max.abs.cov)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="zlm_+3A_response">response</code></td>
<td>
<p>a numeric vector, the response variable.</p>
</td></tr>
<tr><td><code id="zlm_+3A_predictors">predictors</code></td>
<td>
<p>a numeric matrix or a data frame containing numeric and
factor columns; the predictors.</p>
</td></tr>
<tr><td><code id="zlm_+3A_sensitive">sensitive</code></td>
<td>
<p>a numeric matrix or a data frame containing numeric and
factor columns; the sensitive attributes.</p>
</td></tr>
<tr><td><code id="zlm_+3A_unfairness">unfairness</code></td>
<td>
<p>a positive number in [0, 1], how unfair is the model allowed
to be. A value of <code>0</code> means the model is completely fair, while a value
of <code>1</code> means the model is not constrained to be fair at all.</p>
</td></tr>
<tr><td><code id="zlm_+3A_max.abs.cov">max.abs.cov</code></td>
<td>
<p>a non-negative number, the original bound on the maximum
absolute covariance from Zafar et al. (2019).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>zlm()</code> and <code>zlrm()</code> define fairness as statistical parity.
</p>
<p>Estimation minimizes the log-likelihood of the regression models under the
constraint that the correlation between each sensitive attribute and the
fitted values (on the linear predictor scale, in the case of logistic
regression) is smaller than <code>unfairness</code> in absolute value. Both models
include <code>predictors</code> as explanatory variables; the variables
<code>sensitive</code> only appear in the constraints.
</p>
<p>The only difference between <code>zlm()</code> and <code>zlm.orig()</code>, and between
<code>zlrm()</code> and <code>zlrm.orig()</code>, is that the latter uses the original
constraint on the covariances of the individual sensitive attributes from
Zafar et al. (2019).
</p>


<h3>Value</h3>

<p><code>zlm()</code> and <code>zlm.orig()</code> return an object of class
<code>c("zlm", "fair.model")</code>.
<code>zlrm()</code> and <code>zlrm.orig()</code> return an object of class
<code>c("zlrm", "fair.model")</code>.
</p>


<h3>Author(s)</h3>

<p>Marco Scutari</p>


<h3>References</h3>

<p>Zafar BJ, Valera I, Gomez-Rodriguez M, Gummadi KP (2019). &quot;Fairness
Constraints: a Flexible Approach for Fair Classification&quot;. Journal of
Machine Learning Research, 30:1&ndash;42. <br />
<code>https://www.jmlr.org/papers/volume20/18-262/18-262.pdf</code>
</p>


<h3>See Also</h3>

<p><a href="#topic+nclm">nclm</a>, <a href="#topic+frrm">frrm</a>, <a href="#topic+fgrrm">fgrrm</a></p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
