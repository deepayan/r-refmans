<!DOCTYPE html><html><head><title>Help for package BCT</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {BCT}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#BCT'><p>Bayesian Context Trees (BCT) algorithm</p></a></li>
<li><a href='#calculate_exact_changepoint_posterior'><p>Calculates the exact posterior for a sequence with a single change-point.</p></a></li>
<li><a href='#compute_counts'><p>Compute empirical frequencies of all contexts</p></a></li>
<li><a href='#CTW'><p>Context Tree Weighting (CTW) algorithm</p></a></li>
<li><a href='#draw_models'><p>Plot the results of the BCT and kBCT functions</p></a></li>
<li><a href='#el_nino'><p>El Nino</p></a></li>
<li><a href='#enterophage'><p>Enterobacteria_phage_lambda</p></a></li>
<li><a href='#gene_s'><p>SARS-CoV-2 gene S</p></a></li>
<li><a href='#generate_data'><p>Sequence generator</p></a></li>
<li><a href='#infer_fixed_changepoints'><p>Inferring the change-points locations when the number of change-points is fixed.</p></a></li>
<li><a href='#infer_unknown_changepoints'><p>Inferring the number of change-points and their locations.</p></a></li>
<li><a href='#kBCT'><p>k-Bayesian Context Trees (kBCT) algorithm</p></a></li>
<li><a href='#log_loss'><p>Calculating the log-loss incurred in prediction</p></a></li>
<li><a href='#MAP_parameters'><p>Parameters of the MAP model</p></a></li>
<li><a href='#ML'><p>Maximum Likelihood</p></a></li>
<li><a href='#pewee'><p>Pewee birdsong</p></a></li>
<li><a href='#plot_changepoint_posterior'><p>Plot the empirical posterior distribution of the change-points.</p></a></li>
<li><a href='#plot_individual_changepoint_posterior'><p>Plot empirical conditional posterior of the number of change-points.</p></a></li>
<li><a href='#prediction'><p>Prediction</p></a></li>
<li><a href='#sars_cov_2'><p>SARS-CoV-2 genome</p></a></li>
<li><a href='#show_tree'><p>Plot tree with given contexts</p></a></li>
<li><a href='#simian_40'><p>simian_40</p></a></li>
<li><a href='#SP500'><p>Daily changes in the S&amp;P 500 index</p></a></li>
<li><a href='#three_changes'><p>three_changes</p></a></li>
<li><a href='#zero_one_loss'><p>Calculating the 0-1 loss incurred in prediction</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Bayesian Context Trees for Discrete Time Series</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-11-05</td>
</tr>
<tr>
<td>Author:</td>
<td>Ioannis Papageorgiou, Valentinian Mihai Lungu, Ioannis Kontoyiannis</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Valentinian Mihai Lungu &lt;valentinian.mihai@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>An implementation of a collection of tools for exact Bayesian inference with discrete times series. This package contains functions that can be used for prediction, model selection, estimation, segmentation/change-point detection and other statistical tasks. Specifically, the functions provided can be used for the exact computation of the prior predictive likelihood of the data, for the identification of the a posteriori most likely (MAP) variable-memory Markov models, for calculating the exact posterior probabilities and the AIC and BIC scores of these models, for prediction with respect to log-loss and 0-1 loss and segmentation/change-point detection. Example data sets from finance, genetics, animal communication and meteorology are also provided. Detailed descriptions of the underlying theory and algorithms can be found in [Kontoyiannis et al. 'Bayesian Context Trees: Modelling and exact inference for discrete time series.' Journal of the Royal Statistical Society: Series B (Statistical Methodology), April 2022. Available at: &lt;<a href="https://doi.org/10.48550/arXiv.2007.14900">doi:10.48550/arXiv.2007.14900</a>&gt; [stat.ME], July 2020] and [Lungu et al. 'Change-point Detection and Segmentation of Discrete Data using Bayesian Context Trees' &lt;<a href="https://doi.org/10.48550/arXiv.2203.04341">doi:10.48550/arXiv.2203.04341</a>&gt; [stat.ME], March 2022]. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>C++11</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 1.0.5), stringr, igraph, grDevices, graphics</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0)</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-05-12 13:30:53 UTC; Valentinian</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-05-12 14:00:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='BCT'>Bayesian Context Trees (BCT) algorithm</h2><span id='topic+BCT'></span>

<h3>Description</h3>

<p>Finds the maximum a posteriori probability (MAP) tree model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BCT(input_data, depth, beta = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BCT_+3A_input_data">input_data</code></td>
<td>
<p>the sequence to be analysed. 
The sequence needs to be a &quot;character&quot; object. See the examples section on how to transform any dataset to a &quot;character&quot; object.</p>
</td></tr>
<tr><td><code id="BCT_+3A_depth">depth</code></td>
<td>
<p>maximum memory length.</p>
</td></tr>
<tr><td><code id="BCT_+3A_beta">beta</code></td>
<td>
<p>hyper-parameter of the model prior. 
Takes values between 0 and 1. If not initialised in the call function, the default value is 1-2<sup>-m+1</sup>, 
where m is the size of the alphabet; for more information see <a href="https://arxiv.org/pdf/2007.14900.pdf">Kontoyiannis et al. (2020)</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a list object which includes:
</p>
<table>
<tr><td><code>Contexts</code></td>
<td>
<p>MAP model given as a list object containing the contexts of its leaves.</p>
</td></tr>
<tr><td><code>Results</code></td>
<td>
<p>a dataframe with the following columns: prior probability, log(prior probability), posterior probability, log(posterior probability), number of leaves, maximum depth, BIC score, AIC score and maximum log-likelihood. </p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+kBCT">kBCT</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Finding the MAP model with maximum depth &lt;= 10 
# for the SP500 dataset (with default value beta):

BCT(SP500, 10)  

# For custom beta (e.g. 0.7):

BCT(SP500, 10, 0.7)  

# The type of the input dataset is "character"
# If the dataset is contained within a vector:

q &lt;- c(1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0)

# Convert a vector to a "character" object:
s &lt;- paste(q, collapse = "")

BCT(s, 2)

# Reading a file using the readChar function 

# Example 1: The dataset is stored in a .txt file

# fileName &lt;- '~/example_data.txt' # fileName stores the path to the dataset

# s&lt;-readChar(fileName, file.info(fileName)$size)

# Make sure that s does not contain any "\n" at the end of the string
# To remove last entry:
# s&lt;-gsub('.$', '', s)

# To remove any unwanted characters (e.g. "\n"):
# s&lt;-gsub('\n', '', s)

# Example 2: The dataset is stored in a .csv file

# fileName &lt;- '~/example_data.csv' # fileName stores the path to the dataset

# s&lt;-readChar(fileName, file.info(fileName)$size)

# Depending on the running environment, 
# s might contain unwanted characters such as: "\n" or "\r\n".
# Remove any unwanted characters (e.g. "\r\n"):
# s&lt;-gsub('\r\n', '', s)

# Always make sure that s does not contain any unwanted characters
</code></pre>

<hr>
<h2 id='calculate_exact_changepoint_posterior'>Calculates the exact posterior for a sequence with a single change-point.</h2><span id='topic+calculate_exact_changepoint_posterior'></span>

<h3>Description</h3>

<p>This function calculates the exact posterior for a sequence with a single change-point.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calculate_exact_changepoint_posterior(input_data, depth, alphabet)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calculate_exact_changepoint_posterior_+3A_input_data">input_data</code></td>
<td>
<p>the sequence to be analysed.</p>
</td></tr>
<tr><td><code id="calculate_exact_changepoint_posterior_+3A_depth">depth</code></td>
<td>
<p>maximum memory length.</p>
</td></tr>
<tr><td><code id="calculate_exact_changepoint_posterior_+3A_alphabet">alphabet</code></td>
<td>
<p>symbols appearing in the sequence.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>empirical posterior of the change-points locations.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+infer_unknown_changepoints">infer_unknown_changepoints</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Use the first 300 samples of the simian_40 dataset.
# Run the function with 1 change-point, a maximum depth of 2 and the ["a", "c", "g", "t"] alphabet.

res &lt;- calculate_exact_changepoint_posterior(substr(simian_40, 1, 300), 2, c("acgt"))
</code></pre>

<hr>
<h2 id='compute_counts'>Compute empirical frequencies of all contexts</h2><span id='topic+compute_counts'></span>

<h3>Description</h3>

<p>Computes the count vectors of all contexts up to a certain length (D) for a given dataset. The first D characters are used to construct the initial context and the counting is performed on the remaining characters.
These counts are needed for intermediate computations in BCT and kBCT, 
and can also be viewed as maximum likelihood estimates of associated parameters; see <a href="https://arxiv.org/pdf/2007.14900.pdf">Kontoyiannis et al. (2020)</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_counts(input_data, depth)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute_counts_+3A_input_data">input_data</code></td>
<td>
<p>the sequence to be analysed. 
The sequence needs to be a &quot;character&quot; object. See the examples section of the BCT/kBCT functions on how to transform any dataset to a &quot;character&quot; object.</p>
</td></tr>
<tr><td><code id="compute_counts_+3A_depth">depth</code></td>
<td>
<p>maximum memory length.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing the counts of all contexts of length &le; depth. 
If a context with a smaller length than the maximum depth is not contained in the output, its associated count vector is 0. 'Root' indicates the empty context.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BCT">BCT</a></code>, <code><a href="#topic+generate_data">generate_data</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># For the pewee dataset:
compute_counts(pewee, 3)
</code></pre>

<hr>
<h2 id='CTW'>Context Tree Weighting (CTW) algorithm</h2><span id='topic+CTW'></span>

<h3>Description</h3>

<p>Computes the prior predictive likelihood of the data given a specific alphabet. This function is used in for change-point point/segmentation problems
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CTW(input_data, depth, desired_alphabet = NULL, beta = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CTW_+3A_input_data">input_data</code></td>
<td>
<p>the sequence to be analysed. 
The sequence needs to be a &quot;character&quot; object. See the examples section of the BCT/kBCT functions on how to transform any dataset to a &quot;character&quot; object.</p>
</td></tr>
<tr><td><code id="CTW_+3A_depth">depth</code></td>
<td>
<p>maximum memory length.</p>
</td></tr>
<tr><td><code id="CTW_+3A_desired_alphabet">desired_alphabet</code></td>
<td>
<p>set containing the symbols of the process. If not initialised, the default set contains all the unique symbols which appear in the sequence. 
This parameter is needed for the segmentation problem where short segments might not contain all the symbols in the alphabet.</p>
</td></tr>
<tr><td><code id="CTW_+3A_beta">beta</code></td>
<td>
<p>hyper-parameter of the model prior. 
Takes values between 0 and 1. If not initialised in the call function, the default value is 1-2<sup>-m+1</sup>, 
where m is the size of the alphabet; for more information see <a href="https://arxiv.org/pdf/2007.14900.pdf">Kontoyiannis et al. (2020)</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns the natural logarithm of the prior predictive likelihood of the data.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BCT">BCT</a></code>, <code><a href="#topic+kBCT">kBCT</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># For the gene_s dataset with a maximum depth of 10 (with dafault value of beta):
CTW(gene_s, 10)

# With the ["0", "1", "2", "3"] alphabet
CTW(gene_s, 10, "0123")

# For custom beta (e.g. 0.8):
CTW(gene_s, 10, ,0.8)
</code></pre>

<hr>
<h2 id='draw_models'>Plot the results of the BCT and kBCT functions</h2><span id='topic+draw_models'></span>

<h3>Description</h3>

<p>This function plots the models produced by the BCT and kBCT functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>draw_models(lst)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="draw_models_+3A_lst">lst</code></td>
<td>
<p>output of the BCT/kBCT function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>plots of the BCT/kBCT output models.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+show_tree">show_tree</a></code>, <code><a href="#topic+BCT">BCT</a></code>, <code><a href="#topic+kBCT">kBCT</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Use the pewee dataset as an example:
q &lt;- BCT(pewee, 5) # maximum depth of 5

draw_models(q)
r &lt;- kBCT(pewee, 5, 3) 

# maximum depth of 5, and k = 3 (top 3 a posteriori most likely models)
draw_models(r)
</code></pre>

<hr>
<h2 id='el_nino'>El Nino</h2><span id='topic+el_nino'></span>

<h3>Description</h3>

<p>This dataset consists of 495 annual observations between 1525 to 2020, with 0 representing the absence of an El Nino event and 1 indicating its presence.
El Nino is one of the most influential natural climate patterns on earth. It impacts ocean temperatures, the strength of ocean currents, and the local weather in South America.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>el_nino
</code></pre>


<h3>Format</h3>

<p>An object of class <code>"character"</code>.
</p>


<h3>References</h3>

<p>W.H. Quinn, V.T. Neal, and S.E. Antunez De Mayolo. El Nino occurrences over the past four and a half centuries. Journal of Geophysical Research: Oceans, 92(C13):14449–14461, 1987.
(<a href="https://agupubs.onlinelibrary.wiley.com/doi/10.1029/JC092iC13p14449">Quinn</a>)
</p>

<p>(<a href="https://origin.cpc.ncep.noaa.gov/products/analysis_monitoring/ensostuff/ONI_v5.php">el_nino</a>)s
</p>

<hr>
<h2 id='enterophage'>Enterobacteria_phage_lambda</h2><span id='topic+enterophage'></span>

<h3>Description</h3>

<p>This dataset contains the 48502 base-pair-long genome of the bacteriophage lambda virus.
The bacteriophage lambda is a parasite of the intestinal bacterium Escherichia coli.
This virus is a benchmark sequence for the comparison of segmentation algorithms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>enterophage
</code></pre>


<h3>Format</h3>

<p>An object of class <code>"character"</code>.
</p>


<h3>References</h3>

<p>Enterobacteria phage lambda, complete genome.
(<a href="https://www.ncbi.nlm.nih.gov/nuccore/9626243">Enterobacteria_phage_lambda</a>)
</p>

<hr>
<h2 id='gene_s'>SARS-CoV-2 gene S</h2><span id='topic+gene_s'></span>

<h3>Description</h3>

<p>This dataset contains the spike (S) gene, in positions 21,563–25,384 of the SARS-CoV-2 genome. 
The importance of this gene is that it codes for the surface glycoprotein whose function was identified in Yan et al. (2020) and Lan et al. (2020) as critical, 
in that it binds onto the Angiotensin Converting Enzyme 2 (ACE2) receptor on human epithelial cells, 
giving the virus access to the cell and thus facilitating the COVID-19 disease. The gene sequence is mapped to the alphabet 0,1,2,3 via the obvious map A-&gt;0, C-&gt;1, G-&gt;2, T-&gt;3.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gene_s
</code></pre>


<h3>Format</h3>

<p>An object of class <code>"character"</code>.
</p>


<h3>References</h3>

<p>Wu, F., S. Zhao, B. Yu, et al. (2020). A new coronavirus associated with human respiratory disease in China. Nature 579(7798), 265–269.
(<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7094943/">PMC</a>)
</p>
<p>Yan, R., Y. Zhang, Y. Li, L. Xia, Y. Guo, and Q. Zhou (2020). Structural basis for the recognition of SARS-CoV-2 by full-length human ACE2. Science 367(6485), 1444–1448.
(<a href="https://www.science.org/doi/10.1126/science.abb2762">aaas</a>)
</p>
<p>Lan, J., J. Ge, J. Yu, et al. (2020). Structure of the SARS-CoV-2 spike receptor-binding domain bound to the ACE2 receptor. Nature 581, 215–220.
(<a href="https://www.nature.com/articles/s41586-020-2180-5">nature</a>)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>BCT(gene_s, 5)
</code></pre>

<hr>
<h2 id='generate_data'>Sequence generator</h2><span id='topic+generate_data'></span>

<h3>Description</h3>

<p>Generates a simulated sequence of data according to a given model and associated parameters. 
An initial context of length equal to the maximum depth of the model is first generated uniformly and independently, and it is deleted after the desired  
number of samples has been generated.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate_data(ct_theta, N)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generate_data_+3A_ct_theta">ct_theta</code></td>
<td>
<p>a list containing the contexts that specify a model, and also a parameter vector for each context.</p>
</td></tr>
<tr><td><code id="generate_data_+3A_n">N</code></td>
<td>
<p>length of the sequence to be generated.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a simulated sequence as a &quot;character&quot; object
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BCT">BCT</a></code>, <code><a href="#topic+kBCT">kBCT</a></code>, <code><a href="#topic+MAP_parameters">MAP_parameters</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create a list containing contexts and associated parameters.
d1 &lt;- list("0" = c(0.2, 0.8), "10" = c(0.9, 0.1), "11" = c(0,1))

# The contexts need to correspond to the leaves of a proper tree. 
# The key of each vector is the context. 
# For example:
# For context "0": P(x_{i+1} = 0 | x_{i} = 0) = 0.2
# and P(x_{i+1} = 1 | x_{i} = 0) = 0.8

# If a dataset containing only letters is desired:
d2 &lt;- list("ab" = c(0.3, 0.7), "b" = c(0.8, 0.2), "aa" = c(0.5,0.5))

# Generate data from d2
gd &lt;- generate_data(d2, 10000) 

# Use the BCT function to find the MAP model
BCT(gd, 10) # maximum depth of 10

# or the kBCT function can be used:
kBCT(gd, 10, 5) # maximum depth of 10 and top 5 models
</code></pre>

<hr>
<h2 id='infer_fixed_changepoints'>Inferring the change-points locations when the number of change-points is fixed.</h2><span id='topic+infer_fixed_changepoints'></span>

<h3>Description</h3>

<p>This function implements the Metropolis-Hastings sampling algorithm for inferring the locations of the change-points.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>infer_fixed_changepoints(
  input_data,
  l,
  depth,
  alphabet,
  iters,
  fileName = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="infer_fixed_changepoints_+3A_input_data">input_data</code></td>
<td>
<p>the sequence to be analysed.</p>
</td></tr>
<tr><td><code id="infer_fixed_changepoints_+3A_l">l</code></td>
<td>
<p>number of change-points.</p>
</td></tr>
<tr><td><code id="infer_fixed_changepoints_+3A_depth">depth</code></td>
<td>
<p>maximum memory length.</p>
</td></tr>
<tr><td><code id="infer_fixed_changepoints_+3A_alphabet">alphabet</code></td>
<td>
<p>symbols appearing in the sequence.</p>
</td></tr>
<tr><td><code id="infer_fixed_changepoints_+3A_iters">iters</code></td>
<td>
<p>number of iterations; for more information see <a href="https://arxiv.org/pdf/2203.04341.pdf">Lungu et al. (2022)</a>.</p>
</td></tr>
<tr><td><code id="infer_fixed_changepoints_+3A_filename">fileName</code></td>
<td>
<p>file path for storing the results.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>return a list object which includes:
</p>
<table>
<tr><td><code>positions</code></td>
<td>
<p>the sampled locations of the change-points.</p>
</td></tr>
<tr><td><code>acceptance_prob</code></td>
<td>
<p>the empirical acceptance ratio.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+infer_unknown_changepoints">infer_unknown_changepoints</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Use as an example the three_changes dataset.
# Run the function with 3 change-points, a maximum depth of 5 and the [0,1,2] alphabet.
# The sampler is run for 100 iterations
output &lt;- infer_fixed_changepoints(three_changes, 3, 5, c("012"), 100, fileName = NULL)

# If the fileName is not set to NULL, 
# the output file will contain on each line the sampled locations of the change-points.
</code></pre>

<hr>
<h2 id='infer_unknown_changepoints'>Inferring the number of change-points and their locations.</h2><span id='topic+infer_unknown_changepoints'></span>

<h3>Description</h3>

<p>This function implements the Metropolis-Hastings sampling algorithm for inferring the number of change-points and their locations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>infer_unknown_changepoints(
  input_data,
  l_max,
  depth,
  alphabet,
  iters,
  fileName = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="infer_unknown_changepoints_+3A_input_data">input_data</code></td>
<td>
<p>the sequence to be analysed.</p>
</td></tr>
<tr><td><code id="infer_unknown_changepoints_+3A_l_max">l_max</code></td>
<td>
<p>maximum number of change-points.</p>
</td></tr>
<tr><td><code id="infer_unknown_changepoints_+3A_depth">depth</code></td>
<td>
<p>maximum memory length.</p>
</td></tr>
<tr><td><code id="infer_unknown_changepoints_+3A_alphabet">alphabet</code></td>
<td>
<p>symbols appearing in the sequence.</p>
</td></tr>
<tr><td><code id="infer_unknown_changepoints_+3A_iters">iters</code></td>
<td>
<p>number of iterations; for more information see <a href="https://arxiv.org/pdf/2203.04341.pdf">Lungu et al. (2022)</a>.</p>
</td></tr>
<tr><td><code id="infer_unknown_changepoints_+3A_filename">fileName</code></td>
<td>
<p>file path for storing the results.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>return a list object which includes:
</p>
<table>
<tr><td><code>number_changes</code></td>
<td>
<p>sampled number of change-points.</p>
</td></tr>
<tr><td><code>positions</code></td>
<td>
<p>sampled locations of the change-points.</p>
</td></tr>
<tr><td><code>acceptance_prob</code></td>
<td>
<p>the empirical acceptance ratio.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+infer_fixed_changepoints">infer_fixed_changepoints</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Use as an example the three_changes dataset.
# Run the function with 5 change-points, a maximum depth of 5 and the [0,1,2] alphabet.
# The sampler is run for 100 iterations
output &lt;- infer_unknown_changepoints(three_changes, 5, 5, c("012"), 100, fileName = NULL)

# If the fileName is not set to NULL, 
# the output file will contain on each line the sampled number of change-points 
# and the associated sampled locations of the change-points.
</code></pre>

<hr>
<h2 id='kBCT'>k-Bayesian Context Trees (kBCT) algorithm</h2><span id='topic+kBCT'></span>

<h3>Description</h3>

<p>Finds the top k a posteriori most likely tree models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kBCT(input_data, depth, k, beta = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kBCT_+3A_input_data">input_data</code></td>
<td>
<p>the sequence to be analysed. 
The sequence needs to be a &quot;character&quot; object. See the examples section on how to transform any dataset to a &quot;character&quot; object.</p>
</td></tr>
<tr><td><code id="kBCT_+3A_depth">depth</code></td>
<td>
<p>maximum memory length.</p>
</td></tr>
<tr><td><code id="kBCT_+3A_k">k</code></td>
<td>
<p>number of the a posteriori most likely tree models to be identified.</p>
</td></tr>
<tr><td><code id="kBCT_+3A_beta">beta</code></td>
<td>
<p>hyper-parameter of the model prior. 
Takes values between 0 and 1. If not initialised in the call function, the default value is 1-2<sup>-m+1</sup>, 
where m is the size of the alphabet; for more information see: <a href="https://arxiv.org/pdf/2007.14900.pdf">Kontoyiannis et al. (2020)</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list object which includes:
</p>
<table>
<tr><td><code>Contexts</code></td>
<td>
<p>top k a posteriori most likely models. Each model given as a list object containing the contexts of its leaves.</p>
</td></tr>
<tr><td><code>Results</code></td>
<td>
<p>a dataframe with the following columns: prior probability, log(prior probability), posterior probability, log(posterior probability), posterior odds, number of leaves, maximum depth, BIC score, AIC score and maximum log-likelihood. </p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+BCT">BCT</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Finding the first 5 a posteriori most likely models with maximum depth &lt;= 5 
# for the SP500 dataset (with default value beta):

kBCT(SP500, 5, 2)  

# For custom beta (e.g. 0.8):

kBCT(SP500, 5, 2, 0.8)  

# The type of the input dataset is "character"
# If the dataset is contained within a vector:

q &lt;- c(1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0)

# Convert a vector to a "character" object:
s &lt;- paste(q, collapse = "")

kBCT(s, 2, 2)

# Reading a file using the readChar function 

# Example 1: The dataset is stored in a .txt file

# fileName &lt;- '~/example_data.txt' # fileName stores the path to the dataset

# s&lt;-readChar(fileName, file.info(fileName)$size)

# Make sure that s does not contain any "\n" at the end of the string
# To remove last entry:
# s&lt;-gsub('.$', '', s)

# To remove any unwanted characters (e.g. "\n"):
# s&lt;-gsub('\n', '', s)

# Example 2: The dataset is stored in a .csv file

# fileName &lt;- '~/example_data.csv' # fileName stores the path to the dataset

# s&lt;-readChar(fileName, file.info(fileName)$size)

# Depending on the running environment, 
# s might contain unwanted characters such as: "\n" or "\r\n".
# Remove any unwanted characters (e.g. "\r\n"):
# s&lt;-gsub('\r\n', '', s)

# Always make sure that s does not contain any unwanted characters

</code></pre>

<hr>
<h2 id='log_loss'>Calculating the log-loss incurred in prediction</h2><span id='topic+log_loss'></span>

<h3>Description</h3>

<p>Compute the log-loss incurred in BCT prediction with memory length D. Given an initial context
(x<sub>-D+1</sub>, ..., x<sub>0</sub>) and training data (x<sub>1</sub>, ..., x<sub>n</sub>), the log-loss is computed in sequentially predicting
the test data (x<sub>n+1</sub>, ..., x<sub>n+T</sub>). The function outputs the cummulative, normalized (per-sample) log-loss, at each prediction step; for more information see <a href="https://arxiv.org/pdf/2007.14900.pdf">Kontoyiannis et al.(2020)</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>log_loss(input_data, depth, train_size, beta = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="log_loss_+3A_input_data">input_data</code></td>
<td>
<p>the sequence to be analysed. 
The sequence needs to be a &quot;character&quot; object. See the examples section of BCT/kBCT functions on how to transform any dataset to a &quot;character&quot; object.</p>
</td></tr>
<tr><td><code id="log_loss_+3A_depth">depth</code></td>
<td>
<p>maximum memory length.</p>
</td></tr>
<tr><td><code id="log_loss_+3A_train_size">train_size</code></td>
<td>
<p>number of samples used in the training set. The training set size should be at least equal to the depth.</p>
</td></tr>
<tr><td><code id="log_loss_+3A_beta">beta</code></td>
<td>
<p>hyper-parameter of the model prior. 
Takes values between 0 and 1. If not initialised in the call function, the default value is 1-2<sup>-m+1</sup>, 
where m is the size of the alphabet; for more information see <a href="https://arxiv.org/pdf/2007.14900.pdf">Kontoyiannis et al. (2020)</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a vector containing the averaged log-loss incurred in the sequential prediction at each time-step.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+prediction">prediction</a></code>, <code><a href="#topic+zero_one_loss">zero_one_loss</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Compute the log-loss in the prediction of the last 10 elements 
# of a dataset. 
log_loss(pewee, 5, nchar(pewee) - 10)

# For custom beta (e.g. 0.7):
log_loss(pewee, 5, nchar(pewee) - 10, 0.7)
</code></pre>

<hr>
<h2 id='MAP_parameters'>Parameters of the MAP model</h2><span id='topic+MAP_parameters'></span>

<h3>Description</h3>

<p>Returns the parameters of each leaf contained in the MAP model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MAP_parameters(input_data, depth, beta = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MAP_parameters_+3A_input_data">input_data</code></td>
<td>
<p>the sequence to be analysed. 
The sequence needs to be a &quot;character&quot; object. See the examples section of BCT/kBCT functions on how to transform any dataset to a &quot;character&quot; object.</p>
</td></tr>
<tr><td><code id="MAP_parameters_+3A_depth">depth</code></td>
<td>
<p>maximum memory length.</p>
</td></tr>
<tr><td><code id="MAP_parameters_+3A_beta">beta</code></td>
<td>
<p>hyper-parameter of the model prior. 
Takes values between 0 and 1. If not initialised in the call function, the default value is 1-2<sup>-m+1</sup>, 
where m is the size of the alphabet; for more information see <a href="https://arxiv.org/pdf/2007.14900.pdf">Kontoyiannis et al. (2020)</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of parameters for each of the context within the MAP model.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BCT">BCT</a></code>, <code><a href="#topic+kBCT">kBCT</a></code>, <code><a href="#topic+generate_data">generate_data</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Use the gene_s dataset:
q &lt;- BCT(gene_s, 10)
expected_contexts &lt;- q[['Contexts']]
expected_contexts
# [1] "3"  "1"  "0"  "23" "20" "21" "22"

# For default beta:
v &lt;- MAP_parameters(gene_s, 10)

# For custom beta (e.g. 0.8):
MAP_parameters(gene_s, 10, 0.8)

# generate a sequence of data using the generate_data function
s &lt;- generate_data(v, 20000)

# Use BCT:
r &lt;- BCT(s, 10)

# Check the resulting contexts:
r[['Contexts']]
# [1] "3"  "0"  "1"  "20" "22" "23" "21"

# The resulting contexts are as expected 

</code></pre>

<hr>
<h2 id='ML'>Maximum Likelihood</h2><span id='topic+ML'></span>

<h3>Description</h3>

<p>Computes the logarithm of the likelihood of the observations, maximised over all models and parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ML(input_data, depth)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ML_+3A_input_data">input_data</code></td>
<td>
<p>the sequence to be analysed. 
The sequence needs to be a &quot;character&quot; object. See the examples section of the BCT/kBCT functions on how to transform any dataset to a &quot;character&quot; object.</p>
</td></tr>
<tr><td><code id="ML_+3A_depth">depth</code></td>
<td>
<p>maximum memory length.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns the natural logarithm of the maximum likelihood.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BCT">BCT</a></code>, <code><a href="#topic+kBCT">kBCT</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Computing the maximum likelihood of the gene_s dataset 
# with a maximum depth of 5:
ML(gene_s, 5)
</code></pre>

<hr>
<h2 id='pewee'>Pewee birdsong</h2><span id='topic+pewee'></span>

<h3>Description</h3>

<p>The twilight song of the wood pewee bird can be described as a sequence consisting of an arrangement of musical phrases taken 
from an alphabet of three specific, distinct phrases.
The dataset consists of a single continuous song by a wood pewee, 
of length n = 1327 phrases.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pewee
</code></pre>


<h3>Format</h3>

<p>An object of class <code>"character"</code>.
</p>


<h3>References</h3>

<p>W. Craig. The song of the wood pewee (Myiochanes virens Linnaeus): A study of bird music. New York State Museum Bulletin No. 334. University of the State of New York, Albany, NY, 1943.
(<a href="https://www.worldcat.org/title/song-of-the-wood-pewee-myiochanes-virens-linnaeus-a-study-of-bird-music/oclc/2133859">craig</a>)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>BCT(pewee, 5)
</code></pre>

<hr>
<h2 id='plot_changepoint_posterior'>Plot the empirical posterior distribution of the change-points.</h2><span id='topic+plot_changepoint_posterior'></span>

<h3>Description</h3>

<p>This function plots the empirical posterior distribution of the change-points.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_changepoint_posterior(res, burn)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_changepoint_posterior_+3A_res">res</code></td>
<td>
<p>the output obtained from the Metropolis-Hastings algorithms.</p>
</td></tr>
<tr><td><code id="plot_changepoint_posterior_+3A_burn">burn</code></td>
<td>
<p>the proportion of the samples discarded as burn-in.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns plot of the empirical posterior of the number of change-points (if the results from the infer_unknown_changepoints function were used).
</p>
<p>returns plot of the empirical posterior of the change-points.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+infer_unknown_changepoints">infer_unknown_changepoints</a></code>, <code><a href="#topic+infer_fixed_changepoints">infer_fixed_changepoints</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Use as an example the el_nino dataset.
# Run the function with l_max = 3 change-points, a maximum depth of 5 and the [0, 1] alphabet.
# The sampler is run for 100 iterations

res_unknown &lt;- infer_unknown_changepoints(el_nino, 3, 5, c("01"), 100, fileName = NULL)

# Plot the posterior distribution of the locations and the posterior of the number of change-points.

plot_changepoint_posterior(res_unknown, 0.2)

# This function can be also used with the infer_fixed_changepoints.
# Assume l = 2.

res_fixed &lt;- infer_fixed_changepoints(el_nino, 2, 5, c("01"), 100, fileName = NULL)

# Now, the function will only output the posterior distribution of the change-points 
# (the number is fixed).

plot_changepoint_posterior(res_fixed, 0.2)
</code></pre>

<hr>
<h2 id='plot_individual_changepoint_posterior'>Plot empirical conditional posterior of the number of change-points.</h2><span id='topic+plot_individual_changepoint_posterior'></span>

<h3>Description</h3>

<p>This function plots the conditional posterior distribution of the change-points locations given a specific number of change-points.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_individual_changepoint_posterior(res, burn, pm, l = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_individual_changepoint_posterior_+3A_res">res</code></td>
<td>
<p>the output obtained from the Metropolis-Hastings algorithms (either from infer_fixed_changepoints or infer_unknown_changepoints).</p>
</td></tr>
<tr><td><code id="plot_individual_changepoint_posterior_+3A_burn">burn</code></td>
<td>
<p>the proportion of the samples discarded as burn-in.</p>
</td></tr>
<tr><td><code id="plot_individual_changepoint_posterior_+3A_pm">pm</code></td>
<td>
<p>the desired range around the MAP location for each change-point location.</p>
</td></tr>
<tr><td><code id="plot_individual_changepoint_posterior_+3A_l">l</code></td>
<td>
<p>condition on the number of change-points. If not initialised, the function expects as input the results obtained from the infer_fixed_changepoints function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>plots of the empirical posterior distributions of the change-points given a specific number of change-points.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+infer_fixed_changepoints">infer_fixed_changepoints</a></code>, <code><a href="#topic+infer_unknown_changepoints">infer_unknown_changepoints</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Use as an example the el_nino dataset.
# Run the function with l_max = 3 change-points, a maximum depth of 5 and the [0, 1] alphabet.
# The sampler is run for 10000 iterations.

res_unknown &lt;- infer_unknown_changepoints(el_nino, 3, 5, c("01"), 100, fileName = NULL)

# Because l_max = 3 , there can be 0, 1, 2 or 3 changes.
# Let's see the posterior distribution on the number of changes

 plot_changepoint_posterior(res_unknown, 0.2)

# The MAP l is 2. Let's see the distribution of changes given l = 2.

plot_individual_changepoint_posterior(res_unknown, 0.2, 20, 2)

# One can also see the distribution of changes given l = 1. 

plot_individual_changepoint_posterior(res_unknown, 0.2, 500, 1)

# This function can be also used with the infer_fixed_changepoints
# Assume l = 2.

res_fixed &lt;- infer_fixed_changepoints(el_nino, 2, 5, c("01"), 100, fileName = NULL)

# The function is now called without l = 2 as the number of changes is fixed 
# (all sampled vectors have 2 values). 

plot_individual_changepoint_posterior(res_fixed, 0.2, 20)
</code></pre>

<hr>
<h2 id='prediction'>Prediction</h2><span id='topic+prediction'></span>

<h3>Description</h3>

<p>Computes the posterior predictive distribution at each time step, and predicts the next symbol as its most likely value. Given an initial context
(x<sub>-D+1</sub>, ..., x<sub>0</sub>) and training data (x<sub>1</sub>, ..., x<sub>n</sub>), the posterior predictive distribution is computed sequentially for the test data (x<sub>n+1</sub>, ..., x<sub>n+T</sub>). The function outputs the predicted distribution at each time step, along with the most likely symbol; for more information see <a href="https://arxiv.org/pdf/2007.14900.pdf">Kontoyiannis et al.(2020)</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prediction(input_data, depth, train_size, beta = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prediction_+3A_input_data">input_data</code></td>
<td>
<p>the sequence to be analysed. 
The sequence needs to be a &quot;character&quot; object. See the examples section on how to transform any dataset to a &quot;character&quot; object.</p>
</td></tr>
<tr><td><code id="prediction_+3A_depth">depth</code></td>
<td>
<p>maximum memory length.</p>
</td></tr>
<tr><td><code id="prediction_+3A_train_size">train_size</code></td>
<td>
<p>number of samples used for training.</p>
</td></tr>
<tr><td><code id="prediction_+3A_beta">beta</code></td>
<td>
<p>hyper-parameter of the model prior. 
Takes values between 0 and 1. If not initialised in the call function, the default value is 1-2<sup>-m+1</sup>, 
where m is the size of the alphabet; for more information see: <a href="https://arxiv.org/pdf/2007.14900.pdf">Kontoyiannis et al. (2020)</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a &quot;list&quot; containing the posterior predictive distribution at each time step. The last entry in the list, named &quot;Prediction&quot;, contains the most likely character 
at each time step according to the posterior predictive distribution.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+log_loss">log_loss</a></code>, <code><a href="#topic+zero_one_loss">zero_one_loss</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Predicting the 2 last characters of a dataset using a model with a maximum depth of 5
# The training size is the total number of characters within the dataset minus 2: nchar(pewee) - 2

q &lt;- prediction(pewee, 5, nchar(pewee) - 2)

q
# [[1]]
# [1] 0.56300039 0.05899728 0.37800233

# [[2]]
# [1] 0.08150306 0.76293065 0.15556628

# $Prediction
# [1] "0" "1"

# To access the "Prediction" from result list q:
q[["Prediction"]]

# For custom beta (e.g. 0.8):
prediction(pewee, 5, nchar(pewee) - 10, 0.8)
</code></pre>

<hr>
<h2 id='sars_cov_2'>SARS-CoV-2 genome</h2><span id='topic+sars_cov_2'></span>

<h3>Description</h3>

<p>The severe acute respiratory syndrome coronavirus, SARS-CoV-2, is the novel coronavirus responsible for the Covid-19 global pandemic in 2019-20. This dataset contains the SARS-CoV-2 genome, 
available in the GenBank database as the sequence MN908947.3.
It consists of n = 29903 base pairs. The gene sequence is mapped to the alphabet 0,1,2,3 via the obvious map A-&gt;0, C-&gt;1, G-&gt;2, T-&gt;3.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sars_cov_2
</code></pre>


<h3>Format</h3>

<p>An object of class <code>"character"</code>.
</p>


<h3>References</h3>

<p>K. Clark, I. Karsch-Mizrachi, D.J. Lipman, J. Ostell, and E.W. Sayers. GenBank. Nucleic Acids Research, 44(D1): D67–D72, January 2016.
(<a href="https://www.ncbi.nlm.nih.gov">ncbi</a>)
</p>
<p>F. Wu, S. Zhao, B. Yu, et al. A new coronavirus associated with human respiratory disease in China. Nature, 579(7798):265–269, Februrary 2020.
(<a href="https://pubmed.ncbi.nlm.nih.gov/32296181/">PubMed</a>)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>BCT(sars_cov_2, 5)
</code></pre>

<hr>
<h2 id='show_tree'>Plot tree with given contexts</h2><span id='topic+show_tree'></span>

<h3>Description</h3>

<p>Plots a tree depicting a model with the given set of contexts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>show_tree(s)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="show_tree_+3A_s">s</code></td>
<td>
<p>vector containing the contexts of the leaves of the desired tree.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>plot of the desired tree model.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BCT">BCT</a></code>, <code><a href="#topic+draw_models">draw_models</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Construct an example vector:
r &lt;- c("a", "ab", "aab", "b", "ba")

show_tree(r)

# If the input contains digits:
q &lt;- c(11,1,0)

show_tree(q)
</code></pre>

<hr>
<h2 id='simian_40'>simian_40</h2><span id='topic+simian_40'></span>

<h3>Description</h3>

<p>This dataset contains the 5243 base-pair-long genome of the simian vacuolating virus 40 (SV40).
SV40 is a polyomavirus that is found in both monkeys and humans. The expression of SV40 genes is regulated by two major transcripts (early and late), suggesting the presence of a single major change-point in the entire genome.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simian_40
</code></pre>


<h3>Format</h3>

<p>An object of class <code>"character"</code>.
</p>


<h3>References</h3>


<p>(<a href="https://www.ncbi.nlm.nih.gov/nuccore/J02400">SV40</a>)
</p>

<hr>
<h2 id='SP500'>Daily changes in the S&amp;P 500 index</h2><span id='topic+SP500'></span>

<h3>Description</h3>

<p>This dataset contains the quantised daily changes x<sub>i</sub> in the Standard &amp; Poor’s index price, 
from January 2, 1928 until October 7, 2016. The price changes are quantised to 7 values as follows: 
If the change between two successive trading days (i - 1) and i is smaller than 
-3&percnt;, x<sub>i</sub> is set equal to 0; if the change is between -3&percnt; and -2&percnt;, 
x<sub>i</sub>&equals;1; for 
changes in the intervals (-2&percnt;, -1&percnt;], (-1&percnt;, 1&percnt;], 
(1&percnt;, 2&percnt;], and (2&percnt;, 3&percnt;]
x<sub>i</sub> is set 
equal to 2,3,4 and 5, respectively; and for changes greater than 3&percnt;, x<sub>i</sub> = 6.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SP500
</code></pre>


<h3>Format</h3>

<p>An object of class <code>"character"</code>.
</p>


<h3>References</h3>

<p>Yahoo! finance.
(<a href="https://finance.yahoo.com">yahoo_finance</a>)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>BCT(SP500, 10)

</code></pre>

<hr>
<h2 id='three_changes'>three_changes</h2><span id='topic+three_changes'></span>

<h3>Description</h3>

<p>This dataset contains synthetic-generated data. The sequence has three change-points located at 2500, 3500 and 4000.
The tree models and associated parameters are chosen to be quite similar, so that the segmentation problem is nontrivial.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>three_changes
</code></pre>


<h3>Format</h3>

<p>An object of class <code>"character"</code>.
</p>


<h3>References</h3>

<p>V. Lungu, I. Papageorgiou and I. Kontoyiannis. Change-point Detection and Segmentation of Discrete Data using Bayesian Context Trees. arxiv.2203.04341
(<a href="https://arxiv.org/abs/2203.04341">Segmentation</a>)
</p>

<hr>
<h2 id='zero_one_loss'>Calculating the 0-1 loss incurred in prediction</h2><span id='topic+zero_one_loss'></span>

<h3>Description</h3>

<p>Compute the 0-1 loss, i.e., the proportion of incorrectly predicted values,
incurred in BCT prediction with memory length D. Given an initial context
(x<sub>-D+1</sub>, ..., x<sub>0</sub>)  and training data (x<sub>1</sub>, ..., x<sub>n</sub>), the 0-1 loss is computed in sequentially predicting
the test data (x<sub>n+1</sub>, ..., x<sub>n+T</sub>). The function outputs the cummulative, normalized (per-sample) 0-1 loss, at each prediction step; for more information see <a href="https://arxiv.org/pdf/2007.14900.pdf">Kontoyiannis et al. (2020)</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>zero_one_loss(input_data, depth, train_size, beta = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="zero_one_loss_+3A_input_data">input_data</code></td>
<td>
<p>the sequence to be analysed. 
The sequence needs to be a &quot;character&quot; object. See the examples section of kBCT/BCT functions on how to transform any dataset to a &quot;character&quot; object.</p>
</td></tr>
<tr><td><code id="zero_one_loss_+3A_depth">depth</code></td>
<td>
<p>maximum memory length.</p>
</td></tr>
<tr><td><code id="zero_one_loss_+3A_train_size">train_size</code></td>
<td>
<p>number of samples used in the training set. The training set size should be at least equal to the depth.</p>
</td></tr>
<tr><td><code id="zero_one_loss_+3A_beta">beta</code></td>
<td>
<p>hyper-parameter of the model prior. 
Takes values between 0 and 1. If not initialised in the call function, the default value is 1-2<sup>-m+1</sup>, 
where m is the size of the alphabet; for more information see <a href="https://arxiv.org/pdf/2007.14900.pdf">Kontoyiannis et al. (2020)</a></p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a vector containing the averaged number of errors at each timestep.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+log_loss">log_loss</a></code>, <code><a href="#topic+prediction">prediction</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Use the pewee dataset and look at the last 8 elements:
  substring(pewee, nchar(pewee)-7, nchar(pewee)) 
# [1] "10001001"

# Predict last 8 elements using the prediction function
pred &lt;- prediction(pewee, 10, nchar(pewee)-8)[["Prediction"]] 
# Taking only the "Prediction" vector:

pred
# [1] "1" "0" "0" "1" "1" "0" "0" "1"

# To transform the result of the prediction function into a "character" object:
paste(pred, collapse = "")
# [1] "10011001"

# As observed, there is only 1 error (the sixth predicted element is 1 instead of a 0). 
# Thus, up to the 4th place, the averaged error is 0 
# and the sixth averaged error is expected to be 1/4. 
# Indeed, the zero_one_loss function yields the expected answer: 

zero_one_loss(pewee, 10, nchar(pewee)-8) 
# [1] 0.0000000 0.0000000 0.0000000 0.2500000 0.2000000 0.1666667 0.1428571 0.1250000

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
