<!DOCTYPE html><html lang="en"><head><title>Help for package bench</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {bench}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bench-package'><p>bench: High Precision Timing of R Expressions</p></a></li>
<li><a href='#as_bench_mark'><p>Coerce to a bench mark object Bench mark objects</p></a></li>
<li><a href='#as_bench_time'><p>Human readable times</p></a></li>
<li><a href='#autoplot.bench_mark'><p>Autoplot method for bench_mark objects</p></a></li>
<li><a href='#bench_bytes'><p>Human readable memory sizes</p></a></li>
<li><a href='#bench_bytes_trans'><p>Benchmark time transformation</p></a></li>
<li><a href='#bench_load_average'><p>Get system load averages</p></a></li>
<li><a href='#bench_memory'><p>Measure memory that an expression used.</p></a></li>
<li><a href='#bench_process_memory'><p>Retrieve the current and maximum memory from the R process</p></a></li>
<li><a href='#bench_time'><p>Measure Process CPU and real time that an expression used.</p></a></li>
<li><a href='#bench_time_trans'><p>Benchmark time transformation</p></a></li>
<li><a href='#hires_time'><p>Return the current high-resolution real time.</p></a></li>
<li><a href='#knit_print.bench_mark'><p>Custom printing function for <code>bench_mark</code> objects in knitr documents</p></a></li>
<li><a href='#mark'><p>Benchmark a series of functions</p></a></li>
<li><a href='#press'><p>Run setup code and benchmarks across a grid of parameters</p></a></li>
<li><a href='#scale_bench_expr'><p>Position and color scales for bench_expr data</p></a></li>
<li><a href='#scale_bench_time'><p>Position scales for bench_time data</p></a></li>
<li><a href='#summary.bench_mark'><p>Summarize mark results.</p></a></li>
<li><a href='#workout'><p>Workout a group of expressions individually</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>High Precision Timing of R Expressions</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.4</td>
</tr>
<tr>
<td>Description:</td>
<td>Tools to accurately benchmark and analyze execution times for
    R expressions.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://bench.r-lib.org/">https://bench.r-lib.org/</a>, <a href="https://github.com/r-lib/bench">https://github.com/r-lib/bench</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/r-lib/bench/issues">https://github.com/r-lib/bench/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>glue (&ge; 1.8.0), methods, pillar (&ge; 1.10.1), profmem (&ge;
0.6.0), rlang (&ge; 1.1.4), stats, tibble (&ge; 3.2.1), utils</td>
</tr>
<tr>
<td>Suggests:</td>
<td>covr, dplyr, forcats, ggbeeswarm, ggplot2 (&ge; 3.5.1),
ggridges, parallel, scales, testthat (&ge; 3.2.3), tidyr (&ge;
1.3.1), vctrs (&ge; 0.6.5), withr</td>
</tr>
<tr>
<td>Config/Needs/website:</td>
<td>tidyverse/tidytemplate</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Config/usethis/last-upkeep:</td>
<td>2025-01-16</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-01-16 22:07:42 UTC; davis</td>
</tr>
<tr>
<td>Author:</td>
<td>Jim Hester [aut],
  Davis Vaughan [aut, cre],
  Drew Schmidt [ctb] (read_proc_file implementation),
  Posit Software, PBC [cph, fnd]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Davis Vaughan &lt;davis@posit.co&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-01-16 22:40:07 UTC</td>
</tr>
</table>
<hr>
<h2 id='bench-package'>bench: High Precision Timing of R Expressions</h2><span id='topic+bench'></span><span id='topic+bench-package'></span>

<h3>Description</h3>

<p>Tools to accurately benchmark and analyze execution times for R expressions.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Davis Vaughan <a href="mailto:davis@posit.co">davis@posit.co</a>
</p>
<p>Authors:
</p>

<ul>
<li><p> Jim Hester
</p>
</li></ul>

<p>Other contributors:
</p>

<ul>
<li><p> Drew Schmidt (read_proc_file implementation) [contributor]
</p>
</li>
<li><p> Posit Software, PBC [copyright holder, funder]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://bench.r-lib.org/">https://bench.r-lib.org/</a>
</p>
</li>
<li> <p><a href="https://github.com/r-lib/bench">https://github.com/r-lib/bench</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/r-lib/bench/issues">https://github.com/r-lib/bench/issues</a>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- data.frame(x = runif(10000, 1, 1000), y=runif(10000, 1, 1000))

# `bench::mark()` implicitly calls summary() automatically
results &lt;- bench::mark(
  dat[dat$x &gt; 500, ],
  dat[which(dat$x &gt; 500), ],
  subset(dat, x &gt; 500))

# However you can also do so explicitly to filter gc differently.
summary(results, filter_gc = FALSE)

# Or output relative times
summary(results, relative = TRUE)
</code></pre>

<hr>
<h2 id='as_bench_mark'>Coerce to a bench mark object Bench mark objects</h2><span id='topic+as_bench_mark'></span>

<h3>Description</h3>

<p>This is typically needed only if you are performing additional manipulations
after calling <code><a href="#topic+mark">mark()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_bench_mark(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="as_bench_mark_+3A_x">x</code></td>
<td>
<p>Object to be coerced</p>
</td></tr>
</table>

<hr>
<h2 id='as_bench_time'>Human readable times</h2><span id='topic+as_bench_time'></span>

<h3>Description</h3>

<p>Construct, manipulate and display vectors of elapsed times in seconds. These
are numeric vectors, so you can compare them numerically, but they can also
be compared to human readable values such as '10ms'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_bench_time(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="as_bench_time_+3A_x">x</code></td>
<td>
<p>A numeric or character vector. Character representations can use
shorthand sizes (see examples).</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>as_bench_time("1ns")
as_bench_time("1")
as_bench_time("1us")
as_bench_time("1ms")
as_bench_time("1s")

as_bench_time("100ns") &lt; "1ms"

sum(as_bench_time(c("1MB", "5MB", "500KB")))
</code></pre>

<hr>
<h2 id='autoplot.bench_mark'>Autoplot method for bench_mark objects</h2><span id='topic+autoplot.bench_mark'></span><span id='topic+plot.bench_mark'></span>

<h3>Description</h3>

<p>Autoplot method for bench_mark objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>autoplot.bench_mark(
  object,
  type = c("beeswarm", "jitter", "ridge", "boxplot", "violin"),
  ...
)

## S3 method for class 'bench_mark'
plot(x, ..., type = c("beeswarm", "jitter", "ridge", "boxplot", "violin"), y)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="autoplot.bench_mark_+3A_object">object</code></td>
<td>
<p>A <code>bench_mark</code> object.</p>
</td></tr>
<tr><td><code id="autoplot.bench_mark_+3A_type">type</code></td>
<td>
<p>The type of plot. Plotting geoms used for each type are
</p>

<ul>
<li><p> beeswarm - <code><a href="ggbeeswarm.html#topic+geom_quasirandom">ggbeeswarm::geom_quasirandom()</a></code>
</p>
</li>
<li><p> jitter - <code><a href="ggplot2.html#topic+geom_jitter">ggplot2::geom_jitter()</a></code>
</p>
</li>
<li><p> ridge - <code><a href="ggridges.html#topic+geom_density_ridges">ggridges::geom_density_ridges()</a></code>
</p>
</li>
<li><p> boxplot - <code><a href="ggplot2.html#topic+geom_boxplot">ggplot2::geom_boxplot()</a></code>
</p>
</li>
<li><p> violin - <code><a href="ggplot2.html#topic+geom_violin">ggplot2::geom_violin()</a></code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="autoplot.bench_mark_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to the plotting geom.</p>
</td></tr>
<tr><td><code id="autoplot.bench_mark_+3A_x">x</code></td>
<td>
<p>A <code>bench_mark</code> object.</p>
</td></tr>
<tr><td><code id="autoplot.bench_mark_+3A_y">y</code></td>
<td>
<p>Ignored, required for compatibility with the <code>plot()</code> generic.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function requires some optional dependencies. <a href="ggplot2.html#topic+ggplot2-package">ggplot2</a>,
<a href="tidyr.html#topic+tidyr-package">tidyr</a>, and depending on the plot type
<a href="ggbeeswarm.html#topic+ggbeeswarm">ggbeeswarm</a>, <a href="ggridges.html#topic+ggridges-package">ggridges</a>.
</p>
<p>For <code>type</code> of <code>beeswarm</code> and <code>jitter</code> the points are colored by the highest
level garbage collection performed during each iteration.
</p>
<p>For plots with 2 parameters <code>ggplot2::facet_grid()</code> is used to construct a
2d facet. For other numbers of parameters <code>ggplot2::facet_wrap()</code> is used
instead.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- data.frame(x = runif(10000, 1, 1000), y=runif(10000, 1, 1000))

res &lt;- bench::mark(
  dat[dat$x &gt; 500, ],
  dat[which(dat$x &gt; 500), ],
  subset(dat, x &gt; 500))

if (require(ggplot2) &amp;&amp; require(tidyr) &amp;&amp; require(ggbeeswarm)) {

  # Beeswarm plot
  autoplot(res)

  # ridge (joyplot)
  autoplot(res, "ridge")

  # If you want to have the plots ordered by execution time you can do so by
  # ordering factor levels in the expressions.
  if (require(dplyr) &amp;&amp; require(forcats)) {

    res %&gt;%
      mutate(expression = forcats::fct_reorder(as.character(expression), min, .desc = TRUE)) %&gt;%
      as_bench_mark() %&gt;%
      autoplot("violin")
  }
}
</code></pre>

<hr>
<h2 id='bench_bytes'>Human readable memory sizes</h2><span id='topic+bench_bytes'></span><span id='topic+as_bench_bytes'></span>

<h3>Description</h3>

<p>Construct, manipulate and display vectors of byte sizes. These are numeric
vectors, so you can compare them numerically, but they can also be compared
to human readable values such as '10MB'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_bench_bytes(x)

bench_bytes(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bench_bytes_+3A_x">x</code></td>
<td>
<p>A numeric or character vector. Character representations can use
shorthand sizes (see examples).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These memory sizes are always assumed to be base 1024, rather than 1000.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>bench_bytes("1")
bench_bytes("1K")
bench_bytes("1Kb")
bench_bytes("1KiB")
bench_bytes("1MB")

bench_bytes("1KB") &lt; "1MB"

sum(bench_bytes(c("1MB", "5MB", "500KB")))
</code></pre>

<hr>
<h2 id='bench_bytes_trans'>Benchmark time transformation</h2><span id='topic+bench_bytes_trans'></span>

<h3>Description</h3>

<p>This both log transforms the times and formats the labels as a <code>bench_time</code>
object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bench_bytes_trans(base = 2)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bench_bytes_trans_+3A_base">base</code></td>
<td>
<p>base of logarithm</p>
</td></tr>
</table>

<hr>
<h2 id='bench_load_average'>Get system load averages</h2><span id='topic+bench_load_average'></span>

<h3>Description</h3>

<p>Uses OS system APIs to return the load average for the past 1, 5 and 15 minutes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bench_load_average()
</code></pre>

<hr>
<h2 id='bench_memory'>Measure memory that an expression used.</h2><span id='topic+bench_memory'></span>

<h3>Description</h3>

<p>Measure memory that an expression used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bench_memory(expr)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bench_memory_+3A_expr">expr</code></td>
<td>
<p>A expression to be measured.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble with two columns
</p>

<ul>
<li><p> The total amount of memory allocated
</p>
</li>
<li><p> The raw memory allocations as parsed by <code><a href="profmem.html#topic+readRprofmem">profmem::readRprofmem()</a></code>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (capabilities("profmem")) {
  bench_memory(1 + 1:10000)
}
</code></pre>

<hr>
<h2 id='bench_process_memory'>Retrieve the current and maximum memory from the R process</h2><span id='topic+bench_process_memory'></span>

<h3>Description</h3>

<p>The memory reported here will likely differ from that reported by <code>gc()</code>, as
this includes all memory from the R process, including any child processes
and memory allocated outside R's garbage collector heap.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bench_process_memory()
</code></pre>


<h3>Details</h3>

<p>The OS APIs used are as follows
</p>


<h4>Windows</h4>


<ul>
<li> <p><a href="https://learn.microsoft.com/en-us/windows/win32/api/psapi/ns-psapi-process_memory_counters">PROCESS_MEMORY_COUNTERS.WorkingSetSize</a>
</p>
</li>
<li> <p><a href="https://learn.microsoft.com/en-us/windows/win32/api/psapi/ns-psapi-process_memory_counters">PROCESS_MEMORY_COUNTERS.PeakWorkingSetSize</a>
</p>
</li></ul>




<h4>macOS</h4>


<ul>
<li> <p><a href="https://developer.apple.com/documentation/kernel/1537934-task_info?language=occ">task_info(TASK_BASIC_INFO)</a>
</p>
</li>
<li> <p><a href="https://developer.apple.com/library/archive/documentation/System/Conceptual/ManPages_iPhoneOS/man2/getrusage.2.html">rusage.ru_maxrss</a>
</p>
</li></ul>




<h4>linux</h4>


<ul>
<li> <p><a href="https://man7.org/linux/man-pages/man5/proc.5.html">/proc/pid/status VmSize</a>
</p>
</li>
<li> <p><a href="https://man7.org/linux/man-pages/man5/proc.5.html">/proc/pid/status VmPeak</a>
</p>
</li></ul>



<hr>
<h2 id='bench_time'>Measure Process CPU and real time that an expression used.</h2><span id='topic+bench_time'></span><span id='topic+system_time'></span>

<h3>Description</h3>

<p>Measure Process CPU and real time that an expression used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bench_time(expr)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bench_time_+3A_expr">expr</code></td>
<td>
<p>A expression to be timed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>On some systems (such as macOS) the process clock has lower
precision than the realtime clock, as a result there may be cases where the
process time is larger than the real time for fast expressions.
</p>


<h3>Value</h3>

<p>A <code>bench_time</code> object with two values.
</p>

<ul>
<li> <p><code>process</code> - The process CPU usage of the expression evaluation.
</p>
</li>
<li> <p><code>real</code> - The wallclock time of the expression evaluation.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+bench_memory">bench_memory()</a></code> To measure memory allocations for a given expression.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># This will use ~.5 seconds of real time, but very little process time.
bench_time(Sys.sleep(.5))
</code></pre>

<hr>
<h2 id='bench_time_trans'>Benchmark time transformation</h2><span id='topic+bench_time_trans'></span>

<h3>Description</h3>

<p>This both log transforms the times and formats the labels as a <code>bench_time</code>
object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bench_time_trans(base = 10)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bench_time_trans_+3A_base">base</code></td>
<td>
<p>base of logarithm</p>
</td></tr>
</table>

<hr>
<h2 id='hires_time'>Return the current high-resolution real time.</h2><span id='topic+hires_time'></span>

<h3>Description</h3>

<p>Time is expressed as seconds since some arbitrary time in the past; it
is not correlated in any way to the time of day, and thus is not subject to
resetting or drifting. The hi-res
timer is ideally suited to performance measurement tasks, where cheap,
accurate interval timing is required.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hires_time()
</code></pre>


<h3>Examples</h3>

<pre><code class='language-R'>hires_time()

# R rounds doubles to 7 digits by default, see greater precision by setting
# the digits argument when printing
print(hires_time(), digits = 20)

# Generally used by recording two times and then subtracting them
start &lt;- hires_time()
end &lt;- hires_time()
elapsed &lt;- end - start
elapsed
</code></pre>

<hr>
<h2 id='knit_print.bench_mark'>Custom printing function for <code>bench_mark</code> objects in knitr documents</h2><span id='topic+knit_print.bench_mark'></span>

<h3>Description</h3>

<p>By default, data columns (<code>result</code>, <code>memory</code>, <code>time</code>, <code>gc</code>) are omitted when
printing in knitr. If you would like to include these columns, set the knitr
chunk option <code>bench.all_columns = TRUE</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>knit_print.bench_mark(x, ..., options)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="knit_print.bench_mark_+3A_x">x</code></td>
<td>
<p>An R object to be printed</p>
</td></tr>
<tr><td><code id="knit_print.bench_mark_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to the S3 method. Currently ignored,
except two optional arguments <code>options</code> and <code>inline</code>; see
the references below.</p>
</td></tr>
<tr><td><code id="knit_print.bench_mark_+3A_options">options</code></td>
<td>
<p>A list of knitr chunk options set in the currently evaluated
chunk.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>You can set <code>bench.all_columns = TRUE</code> to show all columns of the bench mark
object.
</p>
<div class="sourceCode"><pre>```{r, bench.all_columns = TRUE}
bench::mark(
  subset(mtcars, cyl == 3),
  mtcars[mtcars$cyl == 3, ]
)
```
</pre></div>

<hr>
<h2 id='mark'>Benchmark a series of functions</h2><span id='topic+mark'></span><span id='topic+bench_mark'></span>

<h3>Description</h3>

<p>Benchmark a list of quoted expressions. Each expression will always run at
least twice, once to measure the memory allocation and store results and one
or more times to measure timing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mark(
  ...,
  min_time = 0.5,
  iterations = NULL,
  min_iterations = 1,
  max_iterations = 10000,
  check = TRUE,
  memory = capabilities("profmem"),
  filter_gc = TRUE,
  relative = FALSE,
  time_unit = NULL,
  exprs = NULL,
  env = parent.frame()
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mark_+3A_...">...</code></td>
<td>
<p>Expressions to benchmark, if named the <code>expression</code> column will
be the name, otherwise it will be the deparsed expression.</p>
</td></tr>
<tr><td><code id="mark_+3A_min_time">min_time</code></td>
<td>
<p>The minimum number of seconds to run each expression, set to
<code>Inf</code> to always run <code>max_iterations</code> times instead.</p>
</td></tr>
<tr><td><code id="mark_+3A_iterations">iterations</code></td>
<td>
<p>If not <code>NULL</code>, the default, run each expression for
exactly this number of iterations. This overrides both <code>min_iterations</code>
and <code>max_iterations</code>.</p>
</td></tr>
<tr><td><code id="mark_+3A_min_iterations">min_iterations</code></td>
<td>
<p>Each expression will be evaluated a minimum of <code>min_iterations</code> times.</p>
</td></tr>
<tr><td><code id="mark_+3A_max_iterations">max_iterations</code></td>
<td>
<p>Each expression will be evaluated a maximum of <code>max_iterations</code> times.</p>
</td></tr>
<tr><td><code id="mark_+3A_check">check</code></td>
<td>
<p>Check if results are consistent. If <code>TRUE</code>, checking is done
with <code><a href="base.html#topic+all.equal">all.equal()</a></code>, if <code>FALSE</code> checking is disabled and results are not
stored. If <code>check</code> is a function that function will be called with each
pair of results to determine consistency.</p>
</td></tr>
<tr><td><code id="mark_+3A_memory">memory</code></td>
<td>
<p>If <code>TRUE</code> (the default when R is compiled with memory
profiling), track memory allocations using <code><a href="utils.html#topic+Rprofmem">utils::Rprofmem()</a></code>. If <code>FALSE</code>
disable memory tracking.</p>
</td></tr>
<tr><td><code id="mark_+3A_filter_gc">filter_gc</code></td>
<td>
<p>If <code>TRUE</code> remove iterations that contained at least one
garbage collection before summarizing. If <code>TRUE</code> but an expression had
a garbage collection in every iteration, filtering is disabled, with a warning.</p>
</td></tr>
<tr><td><code id="mark_+3A_relative">relative</code></td>
<td>
<p>If <code>TRUE</code> all summaries are computed relative to the minimum
execution time rather than absolute time.</p>
</td></tr>
<tr><td><code id="mark_+3A_time_unit">time_unit</code></td>
<td>
<p>If <code>NULL</code> the times are reported in a human readable
fashion depending on each value. If one of 'ns', 'us', 'ms', 's', 'm', 'h',
'd', 'w' the time units are instead expressed as nanoseconds, microseconds,
milliseconds, seconds, hours, minutes, days or weeks respectively.</p>
</td></tr>
<tr><td><code id="mark_+3A_exprs">exprs</code></td>
<td>
<p>A list of quoted expressions. If supplied overrides expressions
defined in <code>...</code>.</p>
</td></tr>
<tr><td><code id="mark_+3A_env">env</code></td>
<td>
<p>The environment which to evaluate the expressions</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="tibble.html#topic+tibble">tibble</a> with the additional summary columns.
The following summary columns are computed
</p>

<ul>
<li> <p><code>expression</code> - <code>bench_expr</code> The deparsed expression that was evaluated
(or its name if one was provided).
</p>
</li>
<li> <p><code>min</code> - <code>bench_time</code> The minimum execution time.
</p>
</li>
<li> <p><code>median</code> - <code>bench_time</code> The sample median of execution time.
</p>
</li>
<li> <p><code>itr/sec</code> - <code>double</code> The estimated number of executions performed per
second.
</p>
</li>
<li> <p><code>mem_alloc</code> - <code>bench_bytes</code> Total amount of memory allocated by R while
running the expression. Memory allocated <em>outside</em> the R heap, e.g. by
<code>malloc()</code> or <code>new</code> directly is <em>not</em> tracked, take care to avoid
misinterpreting the results if running code that may do this.
</p>
</li>
<li> <p><code>gc/sec</code> - <code>double</code> The number of garbage collections per second.
</p>
</li>
<li> <p><code>n_itr</code> - <code>integer</code> Total number of iterations after filtering
garbage collections (if <code>filter_gc == TRUE</code>).
</p>
</li>
<li> <p><code>n_gc</code> - <code>double</code> Total number of garbage collections performed over all
iterations. This is a psudo-measure of the pressure on the garbage collector, if
it varies greatly between to alternatives generally the one with fewer
collections will cause fewer allocation in real usage.
</p>
</li>
<li> <p><code>total_time</code> - <code>bench_time</code> The total time to perform the benchmarks.
</p>
</li>
<li> <p><code>result</code> - <code>list</code> A list column of the object(s) returned by the
evaluated expression(s).
</p>
</li>
<li> <p><code>memory</code> - <code>list</code> A list column with results from <code><a href="utils.html#topic+Rprofmem">Rprofmem()</a></code>.
</p>
</li>
<li> <p><code>time</code> - <code>list</code> A list column of <code>bench_time</code> vectors for each evaluated
expression.
</p>
</li>
<li> <p><code>gc</code> - <code>list</code> A list column with tibbles containing the level of
garbage collection (0-2, columns) for each iteration (rows).
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+press">press()</a></code> to run benchmarks across a grid of parameters.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- data.frame(x = runif(100, 1, 1000), y=runif(10, 1, 1000))
mark(
  min_time = .1,

  dat[dat$x &gt; 500, ],
  dat[which(dat$x &gt; 500), ],
  subset(dat, x &gt; 500))
</code></pre>

<hr>
<h2 id='press'>Run setup code and benchmarks across a grid of parameters</h2><span id='topic+press'></span>

<h3>Description</h3>

<p><code>press()</code> is used to run <code><a href="#topic+mark">mark()</a></code> across a grid of parameters and
then <em>press</em> the results together.
</p>
<p>The parameters you want to set are given as named arguments and a grid of
all possible combinations is automatically created.
</p>
<p>The code to setup and benchmark is given by one unnamed expression (often
delimited by <code style="white-space: pre;">&#8288;\{&#8288;</code>).
</p>
<p>If replicates are desired a dummy variable can be used, e.g. <code>rep = 1:5</code> for
replicates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>press(..., .grid = NULL, .quiet = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="press_+3A_...">...</code></td>
<td>
<p>If named, parameters to define, if unnamed the expression to run.
Only one unnamed expression is permitted.</p>
</td></tr>
<tr><td><code id="press_+3A_.grid">.grid</code></td>
<td>
<p>A pre-built grid of values to use, typically a <code><a href="base.html#topic+data.frame">data.frame()</a></code> or
<code><a href="tibble.html#topic+tibble">tibble::tibble()</a></code>. This is useful if you only want to benchmark a subset
of all possible combinations.</p>
</td></tr>
<tr><td><code id="press_+3A_.quiet">.quiet</code></td>
<td>
<p>If <code>TRUE</code>, progress messages will not be emitted.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># Helper function to create a simple data.frame of the specified dimensions
create_df &lt;- function(rows, cols) {
  as.data.frame(setNames(
    replicate(cols, runif(rows, 1, 1000), simplify = FALSE),
    rep_len(c("x", letters), cols)))
}

# Run 4 data sizes across 3 samples with 2 replicates (24 total benchmarks)
press(
  rows = c(1000, 10000),
  cols = c(10, 100),
  rep = 1:2,
  {
    dat &lt;- create_df(rows, cols)
    bench::mark(
      min_time = .05,
      bracket = dat[dat$x &gt; 500, ],
      which = dat[which(dat$x &gt; 500), ],
      subset = subset(dat, x &gt; 500)
    )
  }
)
</code></pre>

<hr>
<h2 id='scale_bench_expr'>Position and color scales for bench_expr data</h2><span id='topic+scale_bench_expr'></span><span id='topic+scale_x_bench_expr'></span><span id='topic+scale_y_bench_expr'></span><span id='topic+scale_colour_bench_expr'></span><span id='topic+scale_color_bench_expr'></span>

<h3>Description</h3>

<p>Default scales for the <code>bench_expr</code> class, these are added to plots using
<code>bench_expr</code> objects automatically.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scale_x_bench_expr(...)

scale_y_bench_expr(...)

scale_colour_bench_expr(
  palette = scales::hue_pal(...),
  ...,
  aesthetics = "colour"
)

scale_color_bench_expr(
  palette = scales::hue_pal(...),
  ...,
  aesthetics = "colour"
)
</code></pre>

<hr>
<h2 id='scale_bench_time'>Position scales for bench_time data</h2><span id='topic+scale_bench_time'></span><span id='topic+scale_x_bench_bytes'></span><span id='topic+scale_y_bench_bytes'></span><span id='topic+scale_x_bench_time'></span><span id='topic+scale_y_bench_time'></span>

<h3>Description</h3>

<p>Default scales for the <code>bench_time</code> class, these are added to plots using
<code>bench_time</code> objects automatically.
</p>
<p>Default scales for the <code>bench_time</code> class, these are added to plots using
<code>bench_time</code> objects automatically.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scale_x_bench_bytes(base = 10, ...)

scale_y_bench_bytes(base = 10, ...)

scale_x_bench_time(base = 10, ...)

scale_y_bench_time(base = 10, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="scale_bench_time_+3A_base">base</code></td>
<td>
<p>The base of the logarithm, if <code>NULL</code> instead use a
non-logarithmic scale.</p>
</td></tr>
</table>

<hr>
<h2 id='summary.bench_mark'>Summarize <a href="#topic+mark">mark</a> results.</h2><span id='topic+summary.bench_mark'></span>

<h3>Description</h3>

<p>Summarize <a href="#topic+mark">mark</a> results.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bench_mark'
summary(object, filter_gc = TRUE, relative = FALSE, time_unit = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="summary.bench_mark_+3A_object">object</code></td>
<td>
<p><a href="#topic+bench_mark">bench_mark</a> object to summarize.</p>
</td></tr>
<tr><td><code id="summary.bench_mark_+3A_filter_gc">filter_gc</code></td>
<td>
<p>If <code>TRUE</code> remove iterations that contained at least one
garbage collection before summarizing. If <code>TRUE</code> but an expression had
a garbage collection in every iteration, filtering is disabled, with a warning.</p>
</td></tr>
<tr><td><code id="summary.bench_mark_+3A_relative">relative</code></td>
<td>
<p>If <code>TRUE</code> all summaries are computed relative to the minimum
execution time rather than absolute time.</p>
</td></tr>
<tr><td><code id="summary.bench_mark_+3A_time_unit">time_unit</code></td>
<td>
<p>If <code>NULL</code> the times are reported in a human readable
fashion depending on each value. If one of 'ns', 'us', 'ms', 's', 'm', 'h',
'd', 'w' the time units are instead expressed as nanoseconds, microseconds,
milliseconds, seconds, hours, minutes, days or weeks respectively.</p>
</td></tr>
<tr><td><code id="summary.bench_mark_+3A_...">...</code></td>
<td>
<p>Additional arguments ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>filter_gc == TRUE</code> (the default) runs that contain a garbage
collection will be removed before summarizing. This is most useful for fast
expressions when the majority of runs do not contain a gc. Call
<code>summary(filter_gc = FALSE)</code> if you would like to compute summaries <em>with</em>
these times, such as expressions with lots of allocations when all or most
runs contain a gc.
</p>


<h3>Value</h3>

<p>A <a href="tibble.html#topic+tibble">tibble</a> with the additional summary columns.
The following summary columns are computed
</p>

<ul>
<li> <p><code>expression</code> - <code>bench_expr</code> The deparsed expression that was evaluated
(or its name if one was provided).
</p>
</li>
<li> <p><code>min</code> - <code>bench_time</code> The minimum execution time.
</p>
</li>
<li> <p><code>median</code> - <code>bench_time</code> The sample median of execution time.
</p>
</li>
<li> <p><code>itr/sec</code> - <code>double</code> The estimated number of executions performed per
second.
</p>
</li>
<li> <p><code>mem_alloc</code> - <code>bench_bytes</code> Total amount of memory allocated by R while
running the expression. Memory allocated <em>outside</em> the R heap, e.g. by
<code>malloc()</code> or <code>new</code> directly is <em>not</em> tracked, take care to avoid
misinterpreting the results if running code that may do this.
</p>
</li>
<li> <p><code>gc/sec</code> - <code>double</code> The number of garbage collections per second.
</p>
</li>
<li> <p><code>n_itr</code> - <code>integer</code> Total number of iterations after filtering
garbage collections (if <code>filter_gc == TRUE</code>).
</p>
</li>
<li> <p><code>n_gc</code> - <code>double</code> Total number of garbage collections performed over all
iterations. This is a psudo-measure of the pressure on the garbage collector, if
it varies greatly between to alternatives generally the one with fewer
collections will cause fewer allocation in real usage.
</p>
</li>
<li> <p><code>total_time</code> - <code>bench_time</code> The total time to perform the benchmarks.
</p>
</li>
<li> <p><code>result</code> - <code>list</code> A list column of the object(s) returned by the
evaluated expression(s).
</p>
</li>
<li> <p><code>memory</code> - <code>list</code> A list column with results from <code><a href="utils.html#topic+Rprofmem">Rprofmem()</a></code>.
</p>
</li>
<li> <p><code>time</code> - <code>list</code> A list column of <code>bench_time</code> vectors for each evaluated
expression.
</p>
</li>
<li> <p><code>gc</code> - <code>list</code> A list column with tibbles containing the level of
garbage collection (0-2, columns) for each iteration (rows).
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- data.frame(x = runif(10000, 1, 1000), y=runif(10000, 1, 1000))

# `bench::mark()` implicitly calls summary() automatically
results &lt;- bench::mark(
  dat[dat$x &gt; 500, ],
  dat[which(dat$x &gt; 500), ],
  subset(dat, x &gt; 500))

# However you can also do so explicitly to filter gc differently.
summary(results, filter_gc = FALSE)

# Or output relative times
summary(results, relative = TRUE)
</code></pre>

<hr>
<h2 id='workout'>Workout a group of expressions individually</h2><span id='topic+workout'></span><span id='topic+workout_expressions'></span>

<h3>Description</h3>

<p>Given an block of expressions in <code>{}</code> <code><a href="#topic+workout">workout()</a></code> individually times each
expression in the group. <code><a href="#topic+workout_expressions">workout_expressions()</a></code> is a lower level function most
useful when reading lists of calls from a file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>workout(expr, description = NULL)

workout_expressions(exprs, env = parent.frame(), description = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="workout_+3A_expr">expr</code></td>
<td>
<p>one or more expressions to workout, use <code>{}</code> to pass multiple
expressions.</p>
</td></tr>
<tr><td><code id="workout_+3A_description">description</code></td>
<td>
<p>A name to label each expression, if not supplied the
deparsed expression will be used.</p>
</td></tr>
<tr><td><code id="workout_+3A_exprs">exprs</code></td>
<td>
<p>A list of calls to measure.</p>
</td></tr>
<tr><td><code id="workout_+3A_env">env</code></td>
<td>
<p>The environment in which the expressions should be evaluated.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>workout({
  x &lt;- 1:1000
  evens &lt;- x %% 2 == 0
  y &lt;- x[evens]
  length(y)
  length(which(evens))
  sum(evens)
})

# The equivalent to the above, reading the code from a file
workout_expressions(as.list(parse(system.file("examples/exprs.R", package = "bench"))))
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
