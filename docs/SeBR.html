<!DOCTYPE html><html lang="en"><head><title>Help for package SeBR</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {SeBR}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bgp_bc'><p>Bayesian Gaussian processes with a Box-Cox transformation</p></a></li>
<li><a href='#blm_bc'><p>Bayesian linear model with a Box-Cox transformation</p></a></li>
<li><a href='#bqr'><p>Bayesian quantile regression</p></a></li>
<li><a href='#bsm_bc'><p>Bayesian spline model with a Box-Cox transformation</p></a></li>
<li><a href='#computeTimeRemaining'><p>Estimate the remaining time in the MCMC based on previous samples</p></a></li>
<li><a href='#contract_grid'><p>Grid contraction</p></a></li>
<li><a href='#Fz_fun'><p>Compute the latent data CDF</p></a></li>
<li><a href='#g_bc'><p>Box-Cox transformation</p></a></li>
<li><a href='#g_fun'><p>Compute the transformation</p></a></li>
<li><a href='#g_inv_approx'><p>Approximate inverse transformation</p></a></li>
<li><a href='#g_inv_bc'><p>Inverse Box-Cox transformation</p></a></li>
<li><a href='#plot_pptest'><p>Plot point and interval predictions on testing data</p></a></li>
<li><a href='#rank_approx'><p>Rank-based estimation of the linear regression coefficients</p></a></li>
<li><a href='#sbgp'><p>Semiparametric Bayesian Gaussian processes</p></a></li>
<li><a href='#sblm'><p>Semiparametric Bayesian linear model</p></a></li>
<li><a href='#sbqr'><p>Semiparametric Bayesian quantile regression</p></a></li>
<li><a href='#sbsm'><p>Semiparametric Bayesian spline model</p></a></li>
<li><a href='#simulate_tlm'><p>Simulate a transformed linear model</p></a></li>
<li><a href='#sir_adjust'><p>Post-processing with importance sampling</p></a></li>
<li><a href='#uni.slice'><p>Univariate Slice Sampler from Neal (2008)</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Semiparametric Bayesian Regression Analysis</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Monte Carlo and MCMC sampling algorithms for semiparametric
    Bayesian regression analysis. These models feature a nonparametric
    (unknown) transformation of the data paired with widely-used
    regression models including linear regression, spline regression,
    quantile regression, and Gaussian processes. The transformation
    enables broader applicability of these key models, including for
    real-valued, positive, and compactly-supported data with challenging
    distributional features. The samplers prioritize computational
    scalability and, for most cases, Monte Carlo (not MCMC) sampling for
    greater efficiency. Details of the methods and algorithms are provided
    in Kowal and Wu (2023) &lt;<a href="https://doi.org/10.48550/arXiv.2306.05498">doi:10.48550/arXiv.2306.05498</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/drkowal/SeBR">https://github.com/drkowal/SeBR</a>, <a href="https://drkowal.github.io/SeBR/">https://drkowal.github.io/SeBR/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/drkowal/SeBR/issues">https://github.com/drkowal/SeBR/issues</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>fields, GpGp, MASS, quantreg, spikeSlabGAM, statmod</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-06-30 20:11:03 UTC; danielkowal</td>
</tr>
<tr>
<td>Author:</td>
<td>Dan Kowal <a href="https://orcid.org/0000-0003-0917-3007"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre, cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Dan Kowal &lt;daniel.r.kowal@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-07-03 16:30:10 UTC</td>
</tr>
</table>
<hr>
<h2 id='bgp_bc'>Bayesian Gaussian processes with a Box-Cox transformation</h2><span id='topic+bgp_bc'></span>

<h3>Description</h3>

<p>MCMC sampling for Bayesian Gaussian process regression with a
(known or unknown) Box-Cox transformation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bgp_bc(
  y,
  locs,
  X = NULL,
  covfun_name = "matern_isotropic",
  locs_test = locs,
  X_test = NULL,
  nn = 30,
  emp_bayes = TRUE,
  lambda = NULL,
  sample_lambda = TRUE,
  nsave = 1000,
  nburn = 1000,
  nskip = 0
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bgp_bc_+3A_y">y</code></td>
<td>
<p><code>n x 1</code> response vector</p>
</td></tr>
<tr><td><code id="bgp_bc_+3A_locs">locs</code></td>
<td>
<p><code>n x d</code> matrix of locations</p>
</td></tr>
<tr><td><code id="bgp_bc_+3A_x">X</code></td>
<td>
<p><code>n x p</code> design matrix; if unspecified, use intercept only</p>
</td></tr>
<tr><td><code id="bgp_bc_+3A_covfun_name">covfun_name</code></td>
<td>
<p>string name of a covariance function; see ?GpGp</p>
</td></tr>
<tr><td><code id="bgp_bc_+3A_locs_test">locs_test</code></td>
<td>
<p><code>n_test x d</code> matrix of locations
at which predictions are needed; default is <code>locs</code></p>
</td></tr>
<tr><td><code id="bgp_bc_+3A_x_test">X_test</code></td>
<td>
<p><code>n_test x p</code> design matrix for test data;
default is <code>X</code></p>
</td></tr>
<tr><td><code id="bgp_bc_+3A_nn">nn</code></td>
<td>
<p>number of nearest neighbors to use; default is 30
(larger values improve the approximation but increase computing cost)</p>
</td></tr>
<tr><td><code id="bgp_bc_+3A_emp_bayes">emp_bayes</code></td>
<td>
<p>logical; if TRUE, use a (faster!) empirical Bayes
approach for estimating the mean function</p>
</td></tr>
<tr><td><code id="bgp_bc_+3A_lambda">lambda</code></td>
<td>
<p>Box-Cox transformation; if NULL, estimate this parameter</p>
</td></tr>
<tr><td><code id="bgp_bc_+3A_sample_lambda">sample_lambda</code></td>
<td>
<p>logical; if TRUE, sample lambda, otherwise
use the fixed value of lambda above or the MLE (if lambda unspecified)</p>
</td></tr>
<tr><td><code id="bgp_bc_+3A_nsave">nsave</code></td>
<td>
<p>number of MCMC iterations to save</p>
</td></tr>
<tr><td><code id="bgp_bc_+3A_nburn">nburn</code></td>
<td>
<p>number of MCMC iterations to discard</p>
</td></tr>
<tr><td><code id="bgp_bc_+3A_nskip">nskip</code></td>
<td>
<p>number of MCMC iterations to skip between saving iterations,
i.e., save every (nskip + 1)th draw</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function provides Bayesian inference for
transformed Gaussian processes. The transformation is
parametric from the Box-Cox family, which has one parameter <code>lambda</code>.
That parameter may be fixed in advanced or learned from the data.
For computational efficiency, the Gaussian process parameters are
fixed at point estimates, and the latent Gaussian process is only sampled
when <code>emp_bayes</code> = FALSE.
</p>


<h3>Value</h3>

<p>a list with the following elements:
</p>

<ul>
<li> <p><code>coefficients</code> the posterior mean of the regression coefficients
</p>
</li>
<li> <p><code>fitted.values</code> the posterior predictive mean at the test points <code>locs_test</code>
</p>
</li>
<li> <p><code>fit_gp</code> the fitted <code>GpGp_fit</code> object, which includes
covariance parameter estimates and other model information
</p>
</li>
<li> <p><code>post_ypred</code>: <code>nsave x n_test</code> samples
from the posterior predictive distribution at <code>locs_test</code>
</p>
</li>
<li> <p><code>post_g</code>: <code>nsave</code> posterior samples of the transformation
evaluated at the unique <code>y</code> values
</p>
</li>
<li> <p><code>post_lambda</code> <code>nsave</code> posterior samples of lambda
</p>
</li>
<li> <p><code>model</code>: the model fit (here, <code>bgp_bc</code>)
</p>
</li></ul>

<p>as well as the arguments passed in.
</p>


<h3>Note</h3>

<p>Box-Cox transformations may be useful in some cases, but
in general we recommend the nonparametric transformation (with
Monte Carlo, not MCMC sampling) in <code><a href="#topic+sbgp">sbgp</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Simulate some data:
n = 200 # sample size
x = seq(0, 1, length = n) # observation points

# Transform a noisy, periodic function:
y = g_inv_bc(
  sin(2*pi*x) + sin(4*pi*x) + rnorm(n, sd = .5),
             lambda = .5) # Signed square-root transformation

# Fit a Bayesian Gaussian process with Box-Cox transformation:
fit = bgp_bc(y = y, locs = x)
names(fit) # what is returned
coef(fit) # estimated regression coefficients (here, just an intercept)
class(fit$fit_gp) # the GpGp object is also returned
round(quantile(fit$post_lambda), 3) # summary of unknown Box-Cox parameter

# Plot the model predictions (point and interval estimates):
pi_y = t(apply(fit$post_ypred, 2, quantile, c(0.05, .95))) # 90% PI
plot(x, y, type='n', ylim = range(pi_y,y),
     xlab = 'x', ylab = 'y', main = paste('Fitted values and prediction intervals'))
polygon(c(x, rev(x)),c(pi_y[,2], rev(pi_y[,1])),col='gray', border=NA)
lines(x, y, type='p')
lines(x, fitted(fit), lwd = 3)


</code></pre>

<hr>
<h2 id='blm_bc'>Bayesian linear model with a Box-Cox transformation</h2><span id='topic+blm_bc'></span>

<h3>Description</h3>

<p>MCMC sampling for Bayesian linear regression with a
(known or unknown) Box-Cox transformation. A g-prior is assumed
for the regression coefficients.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>blm_bc(
  y,
  X,
  X_test = X,
  psi = length(y),
  lambda = NULL,
  sample_lambda = TRUE,
  nsave = 1000,
  nburn = 1000,
  nskip = 0,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="blm_bc_+3A_y">y</code></td>
<td>
<p><code>n x 1</code> vector of observed counts</p>
</td></tr>
<tr><td><code id="blm_bc_+3A_x">X</code></td>
<td>
<p><code>n x p</code> matrix of predictors</p>
</td></tr>
<tr><td><code id="blm_bc_+3A_x_test">X_test</code></td>
<td>
<p><code>n_test x p</code> matrix of predictors for test data;
default is the observed covariates <code>X</code></p>
</td></tr>
<tr><td><code id="blm_bc_+3A_psi">psi</code></td>
<td>
<p>prior variance (g-prior)</p>
</td></tr>
<tr><td><code id="blm_bc_+3A_lambda">lambda</code></td>
<td>
<p>Box-Cox transformation; if NULL, estimate this parameter</p>
</td></tr>
<tr><td><code id="blm_bc_+3A_sample_lambda">sample_lambda</code></td>
<td>
<p>logical; if TRUE, sample lambda, otherwise
use the fixed value of lambda above or the MLE (if lambda unspecified)</p>
</td></tr>
<tr><td><code id="blm_bc_+3A_nsave">nsave</code></td>
<td>
<p>number of MCMC iterations to save</p>
</td></tr>
<tr><td><code id="blm_bc_+3A_nburn">nburn</code></td>
<td>
<p>number of MCMC iterations to discard</p>
</td></tr>
<tr><td><code id="blm_bc_+3A_nskip">nskip</code></td>
<td>
<p>number of MCMC iterations to skip between saving iterations,
i.e., save every (nskip + 1)th draw</p>
</td></tr>
<tr><td><code id="blm_bc_+3A_verbose">verbose</code></td>
<td>
<p>logical; if TRUE, print time remaining</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function provides fully Bayesian inference for a
transformed linear model via MCMC sampling. The transformation is
parametric from the Box-Cox family, which has one parameter <code>lambda</code>.
That parameter may be fixed in advanced or learned from the data.
</p>


<h3>Value</h3>

<p>a list with the following elements:
</p>

<ul>
<li> <p><code>coefficients</code> the posterior mean of the regression coefficients
</p>
</li>
<li> <p><code>fitted.values</code> the posterior predictive mean at the test points <code>X_test</code>
</p>
</li>
<li> <p><code>post_theta</code>: <code>nsave x p</code> samples from the posterior distribution
of the regression coefficients
</p>
</li>
<li> <p><code>post_ypred</code>: <code>nsave x n_test</code> samples
from the posterior predictive distribution at test points <code>X_test</code>
</p>
</li>
<li> <p><code>post_g</code>: <code>nsave</code> posterior samples of the transformation
evaluated at the unique <code>y</code> values
</p>
</li>
<li> <p><code>post_lambda</code> <code>nsave</code> posterior samples of lambda
</p>
</li>
<li> <p><code>post_sigma</code> <code>nsave</code> posterior samples of sigma
</p>
</li>
<li> <p><code>model</code>: the model fit (here, <code>blm_bc</code>)
</p>
</li></ul>

<p>as well as the arguments passed in.
</p>


<h3>Note</h3>

<p>Box-Cox transformations may be useful in some cases, but
in general we recommend the nonparametric transformation (with
Monte Carlo, not MCMC sampling) in <code><a href="#topic+sblm">sblm</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Simulate some data:
dat = simulate_tlm(n = 100, p = 5, g_type = 'step')
y = dat$y; X = dat$X # training data
y_test = dat$y_test; X_test = dat$X_test # testing data

hist(y, breaks = 25) # marginal distribution

# Fit the Bayesian linear model with a Box-Cox transformation:
fit = blm_bc(y = y, X = X, X_test = X_test)
names(fit) # what is returned
round(quantile(fit$post_lambda), 3) # summary of unknown Box-Cox parameter

</code></pre>

<hr>
<h2 id='bqr'>Bayesian quantile regression</h2><span id='topic+bqr'></span>

<h3>Description</h3>

<p>MCMC sampling for Bayesian quantile regression.
An asymmetric Laplace distribution is assumed for the errors,
so the regression models targets the specified quantile.
A g-prior is assumed for the regression coefficients.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bqr(
  y,
  X,
  tau = 0.5,
  X_test = X,
  psi = length(y),
  nsave = 1000,
  nburn = 1000,
  nskip = 0,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bqr_+3A_y">y</code></td>
<td>
<p><code>n x 1</code> vector of observed counts</p>
</td></tr>
<tr><td><code id="bqr_+3A_x">X</code></td>
<td>
<p><code>n x p</code> matrix of predictors</p>
</td></tr>
<tr><td><code id="bqr_+3A_tau">tau</code></td>
<td>
<p>the target quantile (between zero and one)</p>
</td></tr>
<tr><td><code id="bqr_+3A_x_test">X_test</code></td>
<td>
<p><code>n_test x p</code> matrix of predictors for test data;
default is the observed covariates <code>X</code></p>
</td></tr>
<tr><td><code id="bqr_+3A_psi">psi</code></td>
<td>
<p>prior variance (g-prior)</p>
</td></tr>
<tr><td><code id="bqr_+3A_nsave">nsave</code></td>
<td>
<p>number of MCMC iterations to save</p>
</td></tr>
<tr><td><code id="bqr_+3A_nburn">nburn</code></td>
<td>
<p>number of MCMC iterations to discard</p>
</td></tr>
<tr><td><code id="bqr_+3A_nskip">nskip</code></td>
<td>
<p>number of MCMC iterations to skip between saving iterations,
i.e., save every (nskip + 1)th draw</p>
</td></tr>
<tr><td><code id="bqr_+3A_verbose">verbose</code></td>
<td>
<p>logical; if TRUE, print time remaining</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with the following elements:
</p>

<ul>
<li> <p><code>coefficients</code> the posterior mean of the regression coefficients
</p>
</li>
<li> <p><code>fitted.values</code> the estimated <code>tau</code>th quantile at test points <code>X_test</code>
</p>
</li>
<li> <p><code>post_theta</code>: <code>nsave x p</code> samples from the posterior distribution
of the regression coefficients
</p>
</li>
<li> <p><code>post_ypred</code>: <code>nsave x n_test</code> samples
from the posterior predictive distribution at test points <code>X_test</code>
</p>
</li>
<li> <p><code>model</code>: the model fit (here, <code>bqr</code>)
</p>
</li></ul>

<p>as well as the arguments passed
</p>


<h3>Note</h3>

<p>The asymmetric Laplace distribution is advantageous because
it links the regression model (<code>X%*%theta</code>) to a pre-specified
quantile (<code>tau</code>). However, it is often a poor model for
observed data, and the semiparametric version <code><a href="#topic+sbqr">sbqr</a></code> is
recommended in general.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Simulate some heteroskedastic data (no transformation):
dat = simulate_tlm(n = 100, p = 5, g_type = 'box-cox', heterosked = TRUE, lambda = 1)
y = dat$y; X = dat$X # training data
y_test = dat$y_test; X_test = dat$X_test # testing data

# Target this quantile:
tau = 0.05

# Fit the Bayesian quantile regression model:
fit = bqr(y = y, X = X, tau = tau, X_test = X_test)
names(fit) # what is returned

# Posterior predictive checks on testing data: empirical CDF
y0 = sort(unique(y_test))
plot(y0, y0, type='n', ylim = c(0,1),
     xlab='y', ylab='F_y', main = 'Posterior predictive ECDF')
temp = sapply(1:nrow(fit$post_ypred), function(s)
  lines(y0, ecdf(fit$post_ypred[s,])(y0), # ECDF of posterior predictive draws
        col='gray', type ='s'))
lines(y0, ecdf(y_test)(y0),  # ECDF of testing data
     col='black', type = 's', lwd = 3)

# The posterior predictive checks usually do not pass!
# try ?sbqr instead...

</code></pre>

<hr>
<h2 id='bsm_bc'>Bayesian spline model with a Box-Cox transformation</h2><span id='topic+bsm_bc'></span>

<h3>Description</h3>

<p>MCMC sampling for Bayesian spline regression with a
(known or unknown) Box-Cox transformation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bsm_bc(
  y,
  x = NULL,
  x_test = NULL,
  psi = NULL,
  lambda = NULL,
  sample_lambda = TRUE,
  nsave = 1000,
  nburn = 1000,
  nskip = 0,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bsm_bc_+3A_y">y</code></td>
<td>
<p><code>n x 1</code> vector of observed counts</p>
</td></tr>
<tr><td><code id="bsm_bc_+3A_x">x</code></td>
<td>
<p><code>n x 1</code> vector of observation points; if NULL, assume equally-spaced on [0,1]</p>
</td></tr>
<tr><td><code id="bsm_bc_+3A_x_test">x_test</code></td>
<td>
<p><code>n_test x 1</code> vector of testing points; if NULL, assume equal to <code>x</code></p>
</td></tr>
<tr><td><code id="bsm_bc_+3A_psi">psi</code></td>
<td>
<p>prior variance (inverse smoothing parameter); if NULL,
sample this parameter</p>
</td></tr>
<tr><td><code id="bsm_bc_+3A_lambda">lambda</code></td>
<td>
<p>Box-Cox transformation; if NULL, estimate this parameter</p>
</td></tr>
<tr><td><code id="bsm_bc_+3A_sample_lambda">sample_lambda</code></td>
<td>
<p>logical; if TRUE, sample lambda, otherwise
use the fixed value of lambda above or the MLE (if lambda unspecified)</p>
</td></tr>
<tr><td><code id="bsm_bc_+3A_nsave">nsave</code></td>
<td>
<p>number of MCMC iterations to save</p>
</td></tr>
<tr><td><code id="bsm_bc_+3A_nburn">nburn</code></td>
<td>
<p>number of MCMC iterations to discard</p>
</td></tr>
<tr><td><code id="bsm_bc_+3A_nskip">nskip</code></td>
<td>
<p>number of MCMC iterations to skip between saving iterations,
i.e., save every (nskip + 1)th draw</p>
</td></tr>
<tr><td><code id="bsm_bc_+3A_verbose">verbose</code></td>
<td>
<p>logical; if TRUE, print time remaining</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function provides fully Bayesian inference for a
transformed spline model via MCMC sampling. The transformation is
parametric from the Box-Cox family, which has one parameter <code>lambda</code>.
That parameter may be fixed in advanced or learned from the data.
</p>


<h3>Value</h3>

<p>a list with the following elements:
</p>

<ul>
<li> <p><code>coefficients</code> the posterior mean of the regression coefficients
</p>
</li>
<li> <p><code>fitted.values</code> the posterior predictive mean at the test points <code>x_test</code>
</p>
</li>
<li> <p><code>post_theta</code>: <code>nsave x p</code> samples from the posterior distribution
of the regression coefficients
</p>
</li>
<li> <p><code>post_ypred</code>: <code>nsave x n_test</code> samples
from the posterior predictive distribution at <code>x_test</code>
</p>
</li>
<li> <p><code>post_g</code>: <code>nsave</code> posterior samples of the transformation
evaluated at the unique <code>y</code> values
</p>
</li>
<li> <p><code>post_lambda</code> <code>nsave</code> posterior samples of lambda
</p>
</li>
<li> <p><code>model</code>: the model fit (here, <code>sbsm_bc</code>)
</p>
</li></ul>

<p>as well as the arguments passed in.
</p>


<h3>Note</h3>

<p>Box-Cox transformations may be useful in some cases, but
in general we recommend the nonparametric transformation (with
Monte Carlo, not MCMC sampling) in <code><a href="#topic+sbsm">sbsm</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Simulate some data:
n = 100 # sample size
x = sort(runif(n)) # observation points

# Transform a noisy, periodic function:
y = g_inv_bc(
  sin(2*pi*x) + sin(4*pi*x) + rnorm(n, sd = .5),
             lambda = .5) # Signed square-root transformation

# Fit the Bayesian spline model with a Box-Cox transformation:
fit = bsm_bc(y = y, x = x)
names(fit) # what is returned
round(quantile(fit$post_lambda), 3) # summary of unknown Box-Cox parameter

# Plot the model predictions (point and interval estimates):
pi_y = t(apply(fit$post_ypred, 2, quantile, c(0.05, .95))) # 90% PI
plot(x, y, type='n', ylim = range(pi_y,y),
     xlab = 'x', ylab = 'y', main = paste('Fitted values and prediction intervals'))
polygon(c(x, rev(x)),c(pi_y[,2], rev(pi_y[,1])),col='gray', border=NA)
lines(x, y, type='p')
lines(x, fitted(fit), lwd = 3)

</code></pre>

<hr>
<h2 id='computeTimeRemaining'>Estimate the remaining time in the MCMC based on previous samples</h2><span id='topic+computeTimeRemaining'></span>

<h3>Description</h3>

<p>Estimate the remaining time in the MCMC based on previous samples
</p>


<h3>Usage</h3>

<pre><code class='language-R'>computeTimeRemaining(nsi, timer0, nsims, nrep = 1000)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="computeTimeRemaining_+3A_nsi">nsi</code></td>
<td>
<p>Current iteration</p>
</td></tr>
<tr><td><code id="computeTimeRemaining_+3A_timer0">timer0</code></td>
<td>
<p>Initial timer value, returned from <code>proc.time()[3]</code></p>
</td></tr>
<tr><td><code id="computeTimeRemaining_+3A_nsims">nsims</code></td>
<td>
<p>Total number of simulations</p>
</td></tr>
<tr><td><code id="computeTimeRemaining_+3A_nrep">nrep</code></td>
<td>
<p>Print the estimated time remaining every <code>nrep</code> iterations</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Table of summary statistics using the function <code>summary</code>
</p>

<hr>
<h2 id='contract_grid'>Grid contraction</h2><span id='topic+contract_grid'></span>

<h3>Description</h3>

<p>Contract the grid if the evaluation points exceed some threshold.
This removes the corresponding z values.
We can add points back to achieve the same (approximate) length.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>contract_grid(z, Fz, lower, upper, add_back = TRUE, monotone = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="contract_grid_+3A_z">z</code></td>
<td>
<p>grid points (ordered)</p>
</td></tr>
<tr><td><code id="contract_grid_+3A_fz">Fz</code></td>
<td>
<p>function evaluated at those grid points</p>
</td></tr>
<tr><td><code id="contract_grid_+3A_lower">lower</code></td>
<td>
<p>lower threshold at which to check Fz</p>
</td></tr>
<tr><td><code id="contract_grid_+3A_upper">upper</code></td>
<td>
<p>upper threshold at which to check Fz</p>
</td></tr>
<tr><td><code id="contract_grid_+3A_add_back">add_back</code></td>
<td>
<p>logical; if true, expand the grid to
(about) the original size</p>
</td></tr>
<tr><td><code id="contract_grid_+3A_monotone">monotone</code></td>
<td>
<p>logical; if true, enforce monotonicity
on the expanded grid</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing the grid points <code>z</code> and the (interpolated) function
<code>Fz</code> at those points
</p>

<hr>
<h2 id='Fz_fun'>Compute the latent data CDF</h2><span id='topic+Fz_fun'></span>

<h3>Description</h3>

<p>Assuming a Gaussian latent data distribution (given x),
compute the CDF on a grid of points
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Fz_fun(z, weights = NULL, mean_vec = NULL, sd_vec)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Fz_fun_+3A_z">z</code></td>
<td>
<p>vector of points at which the CDF of z is evaluated</p>
</td></tr>
<tr><td><code id="Fz_fun_+3A_weights">weights</code></td>
<td>
<p><code>n</code>-dimensional vector of weights; if NULL,
assume 1/n</p>
</td></tr>
<tr><td><code id="Fz_fun_+3A_mean_vec">mean_vec</code></td>
<td>
<p><code>n</code>-dimensional vector of means; if NULL,
assume mean zero</p>
</td></tr>
<tr><td><code id="Fz_fun_+3A_sd_vec">sd_vec</code></td>
<td>
<p><code>n</code>-dimensional vector of standard deviations</p>
</td></tr>
</table>


<h3>Value</h3>

<p>CDF of z evaluated at <code>z</code>
</p>

<hr>
<h2 id='g_bc'>Box-Cox transformation</h2><span id='topic+g_bc'></span>

<h3>Description</h3>

<p>Evaluate the Box-Cox transformation, which is a scaled power transformation
to preserve continuity in the index <code>lambda</code> at zero. Negative values are
permitted.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>g_bc(t, lambda)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="g_bc_+3A_t">t</code></td>
<td>
<p>argument(s) at which to evaluate the function</p>
</td></tr>
<tr><td><code id="g_bc_+3A_lambda">lambda</code></td>
<td>
<p>Box-Cox parameter</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The evaluation(s) of the Box-Cox function at the given input(s) <code>t</code>.
</p>


<h3>Note</h3>

<p>Special cases include
the identity transformation (<code>lambda = 1</code>),
the square-root transformation (<code>lambda = 1/2</code>),
and the log transformation (<code>lambda = 0</code>).
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Log-transformation:
g_bc(1:5, lambda = 0); log(1:5)

# Square-root transformation: note the shift and scaling
g_bc(1:5, lambda = 1/2); sqrt(1:5)

</code></pre>

<hr>
<h2 id='g_fun'>Compute the transformation</h2><span id='topic+g_fun'></span>

<h3>Description</h3>

<p>Given the CDFs of z and y, compute a smoothed function
to evaluate the transformation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>g_fun(y, Fy_eval, z, Fz_eval)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="g_fun_+3A_y">y</code></td>
<td>
<p>vector of points at which the CDF of y is evaluated</p>
</td></tr>
<tr><td><code id="g_fun_+3A_fy_eval">Fy_eval</code></td>
<td>
<p>CDF of y evaluated at <code>y</code></p>
</td></tr>
<tr><td><code id="g_fun_+3A_z">z</code></td>
<td>
<p>vector of points at which the CDF of z is evaluated</p>
</td></tr>
<tr><td><code id="g_fun_+3A_fz_eval">Fz_eval</code></td>
<td>
<p>CDF of z evaluated at <code>z</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A smooth monotone function which can be used for evaluations of the transformation.
</p>

<hr>
<h2 id='g_inv_approx'>Approximate inverse transformation</h2><span id='topic+g_inv_approx'></span>

<h3>Description</h3>

<p>Compute the inverse function of a transformation <code>g</code> based on a grid search.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>g_inv_approx(g, t_grid)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="g_inv_approx_+3A_g">g</code></td>
<td>
<p>the transformation function</p>
</td></tr>
<tr><td><code id="g_inv_approx_+3A_t_grid">t_grid</code></td>
<td>
<p>grid of arguments at which to evaluate the transformation function</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A function which can be used for evaluations of the
(approximate) inverse transformation function.
</p>

<hr>
<h2 id='g_inv_bc'>Inverse Box-Cox transformation</h2><span id='topic+g_inv_bc'></span>

<h3>Description</h3>

<p>Evaluate the inverse Box-Cox transformation. Negative values are permitted.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>g_inv_bc(s, lambda)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="g_inv_bc_+3A_s">s</code></td>
<td>
<p>argument(s) at which to evaluate the function</p>
</td></tr>
<tr><td><code id="g_inv_bc_+3A_lambda">lambda</code></td>
<td>
<p>Box-Cox parameter</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The evaluation(s) of the inverse Box-Cox function at the given input(s) <code>s</code>.
</p>


<h3>Note</h3>

<p>Special cases include
the identity transformation (<code>lambda = 1</code>),
the square-root transformation (<code>lambda = 1/2</code>),
and the log transformation (<code>lambda = 0</code>).
</p>
<p>#' @examples
# (Inverse) log-transformation:
g_inv_bc(1:5, lambda = 0); exp(1:5)
</p>
<p># (Inverse) square-root transformation: note the shift and scaling
g_inv_bc(1:5, lambda = 1/2); (1:5)^2
</p>

<hr>
<h2 id='plot_pptest'>Plot point and interval predictions on testing data</h2><span id='topic+plot_pptest'></span>

<h3>Description</h3>

<p>Given posterior predictive samples at <code>X_test</code>,
plot the point and interval estimates and compare
to the actual testing data <code>y_test</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_pptest(post_ypred, y_test, alpha_level = 0.1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot_pptest_+3A_post_ypred">post_ypred</code></td>
<td>
<p><code>nsave x n_test</code> samples
from the posterior predictive distribution at test points <code>X_test</code></p>
</td></tr>
<tr><td><code id="plot_pptest_+3A_y_test">y_test</code></td>
<td>
<p><code>n_test</code> testing points</p>
</td></tr>
<tr><td><code id="plot_pptest_+3A_alpha_level">alpha_level</code></td>
<td>
<p>alpha-level for prediction intervals</p>
</td></tr>
</table>


<h3>Value</h3>

<p>plot of the testing data, point and interval predictions,
and a summary of the empirical coverage
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Simulate some data:
dat = simulate_tlm(n = 100, p = 5, g_type = 'step')

# Fit a semiparametric Bayesian linear model:
fit = sblm(y = dat$y, X = dat$X, X_test = dat$X_test)

# Evaluate posterior predictive means and intervals on the testing data:
plot_pptest(fit$post_ypred, dat$y_test,
            alpha_level = 0.10) # coverage should be about 90%

</code></pre>

<hr>
<h2 id='rank_approx'>Rank-based estimation of the linear regression coefficients</h2><span id='topic+rank_approx'></span>

<h3>Description</h3>

<p>For a transformed Gaussian linear model, compute point estimates
of the regression coefficients. This  approach uses the ranks of the
data and does not require the transformation, but must expand the
sample size to <code>n^2</code> and thus can be slow.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rank_approx(y, X)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rank_approx_+3A_y">y</code></td>
<td>
<p><code>n x 1</code> response vector</p>
</td></tr>
<tr><td><code id="rank_approx_+3A_x">X</code></td>
<td>
<p><code>n x p</code> matrix of predictors (should not include an intercept!)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the estimated linear coefficients
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Simulate some data:
dat = simulate_tlm(n = 200, p = 10, g_type = 'step')

# Point estimates for the linear coefficients:
theta_hat = suppressWarnings(
  rank_approx(y = dat$y,
              X = dat$X[,-1]) # remove intercept
) # warnings occur from glm.fit (fitted probabilities 0 or 1)

# Check: correlation with true coefficients
cor(dat$beta_true[-1], # excluding the intercept
    theta_hat)

</code></pre>

<hr>
<h2 id='sbgp'>Semiparametric Bayesian Gaussian processes</h2><span id='topic+sbgp'></span>

<h3>Description</h3>

<p>Monte Carlo sampling for Bayesian Gaussian process regression with an
unknown (nonparametric) transformation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sbgp(
  y,
  locs,
  X = NULL,
  covfun_name = "matern_isotropic",
  locs_test = locs,
  X_test = NULL,
  nn = 30,
  emp_bayes = TRUE,
  approx_g = FALSE,
  nsave = 1000,
  ngrid = 100
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sbgp_+3A_y">y</code></td>
<td>
<p><code>n x 1</code> response vector</p>
</td></tr>
<tr><td><code id="sbgp_+3A_locs">locs</code></td>
<td>
<p><code>n x d</code> matrix of locations</p>
</td></tr>
<tr><td><code id="sbgp_+3A_x">X</code></td>
<td>
<p><code>n x p</code> design matrix; if unspecified, use intercept only</p>
</td></tr>
<tr><td><code id="sbgp_+3A_covfun_name">covfun_name</code></td>
<td>
<p>string name of a covariance function; see ?GpGp</p>
</td></tr>
<tr><td><code id="sbgp_+3A_locs_test">locs_test</code></td>
<td>
<p><code>n_test x d</code> matrix of locations
at which predictions are needed; default is <code>locs</code></p>
</td></tr>
<tr><td><code id="sbgp_+3A_x_test">X_test</code></td>
<td>
<p><code>n_test x p</code> design matrix for test data;
default is <code>X</code></p>
</td></tr>
<tr><td><code id="sbgp_+3A_nn">nn</code></td>
<td>
<p>number of nearest neighbors to use; default is 30
(larger values improve the approximation but increase computing cost)</p>
</td></tr>
<tr><td><code id="sbgp_+3A_emp_bayes">emp_bayes</code></td>
<td>
<p>logical; if TRUE, use a (faster!) empirical Bayes
approach for estimating the mean function</p>
</td></tr>
<tr><td><code id="sbgp_+3A_approx_g">approx_g</code></td>
<td>
<p>logical; if TRUE, apply large-sample
approximation for the transformation</p>
</td></tr>
<tr><td><code id="sbgp_+3A_nsave">nsave</code></td>
<td>
<p>number of Monte Carlo simulations</p>
</td></tr>
<tr><td><code id="sbgp_+3A_ngrid">ngrid</code></td>
<td>
<p>number of grid points for inverse approximations</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function provides Bayesian inference for a
transformed Gaussian process model using Monte Carlo (not MCMC) sampling.
The transformation is modeled as unknown and learned jointly
with the regression function (unless <code>approx_g</code> = TRUE, which then uses
a point approximation). This model applies for real-valued data, positive data, and
compactly-supported data (the support is automatically deduced from the observed <code>y</code> values).
The results are typically unchanged whether <code>laplace_approx</code> is TRUE/FALSE;
setting it to TRUE may reduce sensitivity to the prior, while setting it to FALSE
may speed up computations for very large datasets. For computational efficiency,
the Gaussian process parameters are fixed at point estimates, and the latent Gaussian
process is only sampled when <code>emp_bayes</code> = FALSE. However, the uncertainty
from this term is often negligible compared to the observation errors, and the
transformation serves as an additional layer of robustness.
</p>


<h3>Value</h3>

<p>a list with the following elements:
</p>

<ul>
<li> <p><code>coefficients</code> the estimated regression coefficients
</p>
</li>
<li> <p><code>fitted.values</code> the posterior predictive mean at the test points <code>locs_test</code>
</p>
</li>
<li> <p><code>fit_gp</code> the fitted <code>GpGp_fit</code> object, which includes
covariance parameter estimates and other model information
</p>
</li>
<li> <p><code>post_ypred</code>: <code>nsave x ntest</code> samples
from the posterior predictive distribution at <code>locs_test</code>
</p>
</li>
<li> <p><code>post_g</code>: <code>nsave</code> posterior samples of the transformation
evaluated at the unique <code>y</code> values
</p>
</li>
<li> <p><code>model</code>: the model fit (here, <code>sbgp</code>)
</p>
</li></ul>

<p>as well as the arguments passed in.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Simulate some data:
n = 200 # sample size
x = seq(0, 1, length = n) # observation points

# Transform a noisy, periodic function:
y = g_inv_bc(
  sin(2*pi*x) + sin(4*pi*x) + rnorm(n, sd = .5),
             lambda = .5) # Signed square-root transformation

# Fit the semiparametric Bayesian Gaussian process:
fit = sbgp(y = y, locs = x)
names(fit) # what is returned
coef(fit) # estimated regression coefficients (here, just an intercept)
class(fit$fit_gp) # the GpGp object is also returned

# Plot the model predictions (point and interval estimates):
pi_y = t(apply(fit$post_ypred, 2, quantile, c(0.05, .95))) # 90% PI
plot(x, y, type='n', ylim = range(pi_y,y),
     xlab = 'x', ylab = 'y', main = paste('Fitted values and prediction intervals'))
polygon(c(x, rev(x)),c(pi_y[,2], rev(pi_y[,1])),col='gray', border=NA)
lines(x, y, type='p')
lines(x, fitted(fit), lwd = 3)

</code></pre>

<hr>
<h2 id='sblm'>Semiparametric Bayesian linear model</h2><span id='topic+sblm'></span>

<h3>Description</h3>

<p>Monte Carlo sampling for Bayesian linear regression with an
unknown (nonparametric) transformation. A g-prior is assumed
for the regression coefficients.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sblm(
  y,
  X,
  X_test = X,
  psi = length(y),
  laplace_approx = TRUE,
  approx_g = FALSE,
  nsave = 1000,
  ngrid = 100,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sblm_+3A_y">y</code></td>
<td>
<p><code>n x 1</code> response vector</p>
</td></tr>
<tr><td><code id="sblm_+3A_x">X</code></td>
<td>
<p><code>n x p</code> matrix of predictors</p>
</td></tr>
<tr><td><code id="sblm_+3A_x_test">X_test</code></td>
<td>
<p><code>n_test x p</code> matrix of predictors for test data;
default is the observed covariates <code>X</code></p>
</td></tr>
<tr><td><code id="sblm_+3A_psi">psi</code></td>
<td>
<p>prior variance (g-prior)</p>
</td></tr>
<tr><td><code id="sblm_+3A_laplace_approx">laplace_approx</code></td>
<td>
<p>logical; if TRUE, use a normal approximation
to the posterior in the definition of the transformation;
otherwise the prior is used</p>
</td></tr>
<tr><td><code id="sblm_+3A_approx_g">approx_g</code></td>
<td>
<p>logical; if TRUE, apply large-sample
approximation for the transformation</p>
</td></tr>
<tr><td><code id="sblm_+3A_nsave">nsave</code></td>
<td>
<p>number of Monte Carlo simulations</p>
</td></tr>
<tr><td><code id="sblm_+3A_ngrid">ngrid</code></td>
<td>
<p>number of grid points for inverse approximations</p>
</td></tr>
<tr><td><code id="sblm_+3A_verbose">verbose</code></td>
<td>
<p>logical; if TRUE, print time remaining</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function provides fully Bayesian inference for a
transformed linear model using Monte Carlo (not MCMC) sampling.
The transformation is modeled as unknown and learned jointly
with the regression coefficients (unless <code>approx_g</code> = TRUE, which then uses
a point approximation). This model applies for real-valued data, positive data, and
compactly-supported data (the support is automatically deduced from the observed <code>y</code> values).
The results are typically unchanged whether <code>laplace_approx</code> is TRUE/FALSE;
setting it to TRUE may reduce sensitivity to the prior, while setting it to FALSE
may speed up computations for very large datasets.
</p>


<h3>Value</h3>

<p>a list with the following elements:
</p>

<ul>
<li> <p><code>coefficients</code> the posterior mean of the regression coefficients
</p>
</li>
<li> <p><code>fitted.values</code> the posterior predictive mean at the test points <code>X_test</code>
</p>
</li>
<li> <p><code>post_theta</code>: <code>nsave x p</code> samples from the posterior distribution
of the regression coefficients
</p>
</li>
<li> <p><code>post_ypred</code>: <code>nsave x n_test</code> samples
from the posterior predictive distribution at test points <code>X_test</code>
</p>
</li>
<li> <p><code>post_g</code>: <code>nsave</code> posterior samples of the transformation
evaluated at the unique <code>y</code> values
</p>
</li>
<li> <p><code>model</code>: the model fit (here, <code>sblm</code>)
</p>
</li></ul>

<p>as well as the arguments passed in.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Simulate some data:
dat = simulate_tlm(n = 100, p = 5, g_type = 'step')
y = dat$y; X = dat$X # training data
y_test = dat$y_test; X_test = dat$X_test # testing data

hist(y, breaks = 25) # marginal distribution

# Fit the semiparametric Bayesian linear model:
fit = sblm(y = y, X = X, X_test = X_test)
names(fit) # what is returned

# Note: this is Monte Carlo sampling, so no need for MCMC diagnostics!

# Evaluate posterior predictive means and intervals on the testing data:
plot_pptest(fit$post_ypred, y_test,
            alpha_level = 0.10) # coverage should be about 90%

# Check: correlation with true coefficients
cor(dat$beta_true[-1],
    coef(fit)[-1]) # excluding the intercept

# Summarize the transformation:
y0 = sort(unique(y)) # posterior draws of g are evaluated at the unique y observations
plot(y0, fit$post_g[1,], type='n', ylim = range(fit$post_g),
     xlab = 'y', ylab = 'g(y)', main = "Posterior draws of the transformation")
temp = sapply(1:nrow(fit$post_g), function(s)
  lines(y0, fit$post_g[s,], col='gray')) # posterior draws
lines(y0, colMeans(fit$post_g), lwd = 3) # posterior mean

# Add the true transformation, rescaled for easier comparisons:
lines(y,
      scale(dat$g_true)*sd(colMeans(fit$post_g)) + mean(colMeans(fit$post_g)), type='p', pch=2)
legend('bottomright', c('Truth'), pch = 2) # annotate the true transformation

# Posterior predictive checks on testing data: empirical CDF
y0 = sort(unique(y_test))
plot(y0, y0, type='n', ylim = c(0,1),
     xlab='y', ylab='F_y', main = 'Posterior predictive ECDF')
temp = sapply(1:nrow(fit$post_ypred), function(s)
  lines(y0, ecdf(fit$post_ypred[s,])(y0), # ECDF of posterior predictive draws
        col='gray', type ='s'))
lines(y0, ecdf(y_test)(y0),  # ECDF of testing data
     col='black', type = 's', lwd = 3)

</code></pre>

<hr>
<h2 id='sbqr'>Semiparametric Bayesian quantile regression</h2><span id='topic+sbqr'></span>

<h3>Description</h3>

<p>MCMC sampling for Bayesian quantile regression with an
unknown (nonparametric) transformation. Like in traditional Bayesian
quantile regression, an asymmetric Laplace distribution is assumed
for the errors, so the regression models targets the specified quantile.
However, these models are often woefully inadequate for describing
observed data. We introduce a nonparametric transformation to
improve model adequacy while still providing inference for the
regression coefficients and the specified quantile. A g-prior is assumed
for the regression coefficients.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sbqr(
  y,
  X,
  tau = 0.5,
  X_test = X,
  psi = length(y),
  laplace_approx = TRUE,
  approx_g = FALSE,
  nsave = 1000,
  nburn = 100,
  ngrid = 100,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sbqr_+3A_y">y</code></td>
<td>
<p><code>n x 1</code> response vector</p>
</td></tr>
<tr><td><code id="sbqr_+3A_x">X</code></td>
<td>
<p><code>n x p</code> matrix of predictors</p>
</td></tr>
<tr><td><code id="sbqr_+3A_tau">tau</code></td>
<td>
<p>the target quantile (between zero and one)</p>
</td></tr>
<tr><td><code id="sbqr_+3A_x_test">X_test</code></td>
<td>
<p><code>n_test x p</code> matrix of predictors for test data;
default is the observed covariates <code>X</code></p>
</td></tr>
<tr><td><code id="sbqr_+3A_psi">psi</code></td>
<td>
<p>prior variance (g-prior)</p>
</td></tr>
<tr><td><code id="sbqr_+3A_laplace_approx">laplace_approx</code></td>
<td>
<p>logical; if TRUE, use a normal approximation
to the posterior in the definition of the transformation;
otherwise the prior is used</p>
</td></tr>
<tr><td><code id="sbqr_+3A_approx_g">approx_g</code></td>
<td>
<p>logical; if TRUE, apply large-sample
approximation for the transformation</p>
</td></tr>
<tr><td><code id="sbqr_+3A_nsave">nsave</code></td>
<td>
<p>number of MCMC iterations to save</p>
</td></tr>
<tr><td><code id="sbqr_+3A_nburn">nburn</code></td>
<td>
<p>number of MCMC iterations to discard</p>
</td></tr>
<tr><td><code id="sbqr_+3A_ngrid">ngrid</code></td>
<td>
<p>number of grid points for inverse approximations</p>
</td></tr>
<tr><td><code id="sbqr_+3A_verbose">verbose</code></td>
<td>
<p>logical; if TRUE, print time remaining</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function provides fully Bayesian inference for a
transformed quantile linear model.
The transformation is modeled as unknown and learned jointly
with the regression coefficients (unless <code>approx_g</code> = TRUE, which then uses
a point approximation). This model applies for real-valued data, positive data, and
compactly-supported data (the support is automatically deduced from the observed <code>y</code> values).
The results are typically unchanged whether <code>laplace_approx</code> is TRUE/FALSE;
setting it to TRUE may reduce sensitivity to the prior, while setting it to FALSE
may speed up computations for very large datasets.
</p>


<h3>Value</h3>

<p>a list with the following elements:
</p>

<ul>
<li> <p><code>coefficients</code> the posterior mean of the regression coefficients
</p>
</li>
<li> <p><code>fitted.values</code> the estimated <code>tau</code>th quantile at test points <code>X_test</code>
</p>
</li>
<li> <p><code>post_theta</code>: <code>nsave x p</code> samples from the posterior distribution
of the regression coefficients
</p>
</li>
<li> <p><code>post_ypred</code>: <code>nsave x n_test</code> samples
from the posterior predictive distribution at test points <code>X_test</code>
</p>
</li>
<li> <p><code>post_qtau</code>: <code>nsave x n_test</code> samples of the <code>tau</code>th conditional quantile at test points <code>X_test</code>
</p>
</li>
<li> <p><code>post_g</code>: <code>nsave</code> posterior samples of the transformation
evaluated at the unique <code>y</code> values
</p>
</li>
<li> <p><code>model</code>: the model fit (here, <code>sbqr</code>)
</p>
</li></ul>

<p>as well as the arguments passed in.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Simulate some heteroskedastic data (no transformation):
dat = simulate_tlm(n = 200, p = 10, g_type = 'box-cox', heterosked = TRUE, lambda = 1)
y = dat$y; X = dat$X # training data
y_test = dat$y_test; X_test = dat$X_test # testing data

# Target this quantile:
tau = 0.05

# Fit the semiparametric Bayesian quantile regression model:
fit = sbqr(y = y, X = X, tau = tau, X_test = X_test)
names(fit) # what is returned

# Posterior predictive checks on testing data: empirical CDF
y0 = sort(unique(y_test))
plot(y0, y0, type='n', ylim = c(0,1),
     xlab='y', ylab='F_y', main = 'Posterior predictive ECDF')
temp = sapply(1:nrow(fit$post_ypred), function(s)
  lines(y0, ecdf(fit$post_ypred[s,])(y0), # ECDF of posterior predictive draws
        col='gray', type ='s'))
lines(y0, ecdf(y_test)(y0),  # ECDF of testing data
     col='black', type = 's', lwd = 3)


</code></pre>

<hr>
<h2 id='sbsm'>Semiparametric Bayesian spline model</h2><span id='topic+sbsm'></span>

<h3>Description</h3>

<p>Monte Carlo sampling for Bayesian spline regression with an
unknown (nonparametric) transformation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sbsm(
  y,
  x = NULL,
  x_test = NULL,
  psi = NULL,
  laplace_approx = TRUE,
  approx_g = FALSE,
  nsave = 1000,
  ngrid = 100,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sbsm_+3A_y">y</code></td>
<td>
<p><code>n x 1</code> response vector</p>
</td></tr>
<tr><td><code id="sbsm_+3A_x">x</code></td>
<td>
<p><code>n x 1</code> vector of observation points; if NULL, assume equally-spaced on [0,1]</p>
</td></tr>
<tr><td><code id="sbsm_+3A_x_test">x_test</code></td>
<td>
<p><code>n_test x 1</code> vector of testing points; if NULL, assume equal to <code>x</code></p>
</td></tr>
<tr><td><code id="sbsm_+3A_psi">psi</code></td>
<td>
<p>prior variance (inverse smoothing parameter); if NULL,
sample this parameter</p>
</td></tr>
<tr><td><code id="sbsm_+3A_laplace_approx">laplace_approx</code></td>
<td>
<p>logical; if TRUE, use a normal approximation
to the posterior in the definition of the transformation;
otherwise the prior is used</p>
</td></tr>
<tr><td><code id="sbsm_+3A_approx_g">approx_g</code></td>
<td>
<p>logical; if TRUE, apply large-sample
approximation for the transformation</p>
</td></tr>
<tr><td><code id="sbsm_+3A_nsave">nsave</code></td>
<td>
<p>number of Monte Carlo simulations</p>
</td></tr>
<tr><td><code id="sbsm_+3A_ngrid">ngrid</code></td>
<td>
<p>number of grid points for inverse approximations</p>
</td></tr>
<tr><td><code id="sbsm_+3A_verbose">verbose</code></td>
<td>
<p>logical; if TRUE, print time remaining</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function provides fully Bayesian inference for a
transformed spline regression model using Monte Carlo (not MCMC) sampling.
The transformation is modeled as unknown and learned jointly
with the regression function (unless <code>approx_g</code> = TRUE, which then uses
a point approximation). This model applies for real-valued data, positive data, and
compactly-supported data (the support is automatically deduced from the observed <code>y</code> values).
The results are typically unchanged whether <code>laplace_approx</code> is TRUE/FALSE;
setting it to TRUE may reduce sensitivity to the prior, while setting it to FALSE
may speed up computations for very large datasets.
</p>


<h3>Value</h3>

<p>a list with the following elements:
</p>

<ul>
<li> <p><code>coefficients</code> the posterior mean of the regression coefficients
</p>
</li>
<li> <p><code>fitted.values</code> the posterior predictive mean at the test points <code>x_test</code>
</p>
</li>
<li> <p><code>post_theta</code>: <code>nsave x p</code> samples from the posterior distribution
of the regression coefficients
</p>
</li>
<li> <p><code>post_ypred</code>: <code>nsave x n_test</code> samples
from the posterior predictive distribution at <code>x_test</code>
</p>
</li>
<li> <p><code>post_g</code>: <code>nsave</code> posterior samples of the transformation
evaluated at the unique <code>y</code> values
</p>
</li>
<li> <p><code>model</code>: the model fit (here, <code>sbsm</code>)
</p>
</li></ul>

<p>as well as the arguments passed in.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Simulate some data:
n = 100 # sample size
x = sort(runif(n)) # observation points

# Transform a noisy, periodic function:
y = g_inv_bc(
  sin(2*pi*x) + sin(4*pi*x) + rnorm(n, sd = .5),
             lambda = .5) # Signed square-root transformation

# Fit the semiparametric Bayesian spline model:
fit = sbsm(y = y, x = x)
names(fit) # what is returned

# Note: this is Monte Carlo sampling, so no need for MCMC diagnostics!

# Plot the model predictions (point and interval estimates):
pi_y = t(apply(fit$post_ypred, 2, quantile, c(0.05, .95))) # 90% PI
plot(x, y, type='n', ylim = range(pi_y,y),
     xlab = 'x', ylab = 'y', main = paste('Fitted values and prediction intervals'))
polygon(c(x, rev(x)),c(pi_y[,2], rev(pi_y[,1])),col='gray', border=NA)
lines(x, y, type='p')
lines(x, fitted(fit), lwd = 3)

</code></pre>

<hr>
<h2 id='simulate_tlm'>Simulate a transformed linear model</h2><span id='topic+simulate_tlm'></span>

<h3>Description</h3>

<p>Generate training data (X, y) and testing data (X_test, y_test)
for a transformed linear model. The covariates are correlated
Gaussian variables. Half of the true regression coefficients
are zero and the other half are one. There are multiple options
for the transformation, which define the support of the data (see below).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simulate_tlm(
  n,
  p,
  g_type = "beta",
  n_test = 1000,
  heterosked = FALSE,
  lambda = 1
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="simulate_tlm_+3A_n">n</code></td>
<td>
<p>number of observations in the training data</p>
</td></tr>
<tr><td><code id="simulate_tlm_+3A_p">p</code></td>
<td>
<p>number of covariates</p>
</td></tr>
<tr><td><code id="simulate_tlm_+3A_g_type">g_type</code></td>
<td>
<p>type of transformation; must be one of
<code>beta</code>, <code>step</code>, or <code>box-cox</code></p>
</td></tr>
<tr><td><code id="simulate_tlm_+3A_n_test">n_test</code></td>
<td>
<p>number of observations in the testing data</p>
</td></tr>
<tr><td><code id="simulate_tlm_+3A_heterosked">heterosked</code></td>
<td>
<p>logical; if TRUE, simulate the latent data with heteroskedasticity</p>
</td></tr>
<tr><td><code id="simulate_tlm_+3A_lambda">lambda</code></td>
<td>
<p>Box-Cox parameter (only applies for <code>g_type = 'box-cox'</code>)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The transformations vary in complexity and support
for the observed data, and include the following options:
<code>beta</code> yields marginally Beta(0.1, 0.5) data
supported on [0,1]; <code>step</code> generates a locally-linear
inverse transformation and produces positive data; and <code>box-cox</code>
refers to the signed Box-Cox family indexed by <code>lambda</code>,
which generates real-valued data with examples including identity,
square-root, and log transformations.
</p>


<h3>Value</h3>

<p>a list with the following elements:
</p>

<ul>
<li> <p><code>y</code>: the response variable in the training data
</p>
</li>
<li> <p><code>X</code>: the covariates in the training data
</p>
</li>
<li> <p><code>y_test</code>: the response variable in the testing data
</p>
</li>
<li> <p><code>X_test</code>: the covariates in the testing data
</p>
</li>
<li> <p><code>beta_true</code>: the true regression coefficients
</p>
</li>
<li> <p><code>g_true</code>: the true transformation, evaluated at y
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Simulate data:
dat = simulate_tlm(n = 100, p = 5, g_type = 'beta')
names(dat) # what is returned
hist(dat$y, breaks = 25) # marginal distribution

</code></pre>

<hr>
<h2 id='sir_adjust'>Post-processing with importance sampling</h2><span id='topic+sir_adjust'></span>

<h3>Description</h3>

<p>Given Monte Carlo draws from the surrogate posterior,
apply sampling importance reweighting (SIR) to
correct for the true model likelihood.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sir_adjust(fit, sir_frac = 0.3, nsims_prior = 100, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sir_adjust_+3A_fit">fit</code></td>
<td>
<p>a fitted model object that includes
</p>

<ul>
<li> <p><code>coefficients</code> the posterior mean of the regression coefficients
</p>
</li>
<li> <p><code>post_theta</code>: <code>nsave x p</code> samples from the posterior distribution
of the regression coefficients
</p>
</li>
<li> <p><code>post_ypred</code>: <code>nsave x n_test</code> samples
from the posterior predictive distribution at test points <code>X_test</code>
</p>
</li>
<li> <p><code>post_g</code>: <code>nsave</code> posterior samples of the transformation
evaluated at the unique <code>y</code> values
</p>
</li>
<li> <p><code>model</code>: the model fit (<code>sblm</code> or <code>sbsm</code>)
</p>
</li></ul>
</td></tr>
<tr><td><code id="sir_adjust_+3A_sir_frac">sir_frac</code></td>
<td>
<p>fraction of draws to sample for SIR</p>
</td></tr>
<tr><td><code id="sir_adjust_+3A_nsims_prior">nsims_prior</code></td>
<td>
<p>number of draws from the prior</p>
</td></tr>
<tr><td><code id="sir_adjust_+3A_verbose">verbose</code></td>
<td>
<p>logical; if TRUE, print time remaining</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Monte Carlo sampling for <code><a href="#topic+sblm">sblm</a></code> and
<code><a href="#topic+sbsm">sbsm</a></code> uses a surrogate likelihood for posterior inference,
which enables much faster and easier computing. SIR provides a correction for
the actual (specified) likelihood. However, this correction
step is quite slow and typically does not produce any noticeable
discrepancies, even for small sample sizes.
</p>


<h3>Value</h3>

<p>the fitted model object with the posterior draws subsampled
based on the SIR adjustment
</p>


<h3>Note</h3>

<p>SIR sampling is done WITHOUT replacement, so <code>sir_frac</code>
is typically between 0.1 and 0.5. The <code>nsims_priors</code> draws
are used to approximate a prior expectation, but larger values
can significantly slow down this function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Simulate some data:
dat = simulate_tlm(n = 50, p = 5, g_type = 'step')
y = dat$y; X = dat$X # training data
y_test = dat$y_test; X_test = dat$X_test # testing data

hist(y, breaks = 10) # marginal distribution

# Fit the semiparametric Bayesian linear model:
fit = sblm(y = y, X = X, X_test = X_test)
names(fit) # what is returned

# Update with SIR:
fit_sir = sir_adjust(fit)

# Prediction: unadjusted vs. adjusted?

# Point estimates:
y_hat = fitted(fit)
y_hat_sir = fitted(fit_sir)
cor(y_hat, y_hat_sir) # similar

# Interval estimates:
pi_y = t(apply(fit$post_ypred, 2, quantile, c(0.05, .95))) # 90% PI
pi_y_sir = t(apply(fit_sir$post_ypred, 2, quantile, c(0.05, .95))) # 90% PI

# PI overlap (%):
overlaps = 100*sapply(1:length(y_test), function(i){
  # innermost part
  (min(pi_y[i,2], pi_y_sir[i,2]) - max(pi_y[i,1], pi_y_sir[i,1]))/
    # outermost part
    (max(pi_y[i,2], pi_y_sir[i,2]) - min(pi_y[i,1], pi_y_sir[i,1]))
})
summary(overlaps) # mostly close to 100%

# Coverage of PIs on testing data (should be ~ 90%)
mean((pi_y[,1] &lt;= y_test)*(pi_y[,2] &gt;= y_test)) # unadjusted
mean((pi_y_sir[,1] &lt;= y_test)*(pi_y_sir[,2] &gt;= y_test)) # adjusted

# Plot together with testing data:
plot(y_test, y_test, type='n', ylim = range(pi_y, pi_y_sir, y_test),
     xlab = 'y_test', ylab = 'y_hat', main = paste('Prediction intervals: testing data'))
abline(0,1) # reference line
suppressWarnings(
  arrows(y_test, pi_y[,1], y_test, pi_y[,2],
         length=0.15, angle=90, code=3, col='gray', lwd=2)
) # plot the PIs (unadjusted)
suppressWarnings(
  arrows(y_test, pi_y_sir[,1], y_test, pi_y_sir[,2],
         length=0.15, angle=90, code=3, col='darkgray', lwd=2)
) # plot the PIs (adjusted)
lines(y_test, y_hat, type='p', pch=2) # plot the means (unadjusted)
lines(y_test, y_hat_sir, type='p', pch=3) # plot the means (adjusted)

</code></pre>

<hr>
<h2 id='uni.slice'>Univariate Slice Sampler from Neal (2008)</h2><span id='topic+uni.slice'></span>

<h3>Description</h3>

<p>Compute a draw from a univariate distribution using the code provided by
Radford M. Neal. The documentation below is also reproduced from Neal (2008).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>uni.slice(x0, g, w = 1, m = Inf, lower = -Inf, upper = +Inf, gx0 = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="uni.slice_+3A_x0">x0</code></td>
<td>
<p>Initial point</p>
</td></tr>
<tr><td><code id="uni.slice_+3A_g">g</code></td>
<td>
<p>Function returning the log of the probability density (plus constant)</p>
</td></tr>
<tr><td><code id="uni.slice_+3A_w">w</code></td>
<td>
<p>Size of the steps for creating interval (default 1)</p>
</td></tr>
<tr><td><code id="uni.slice_+3A_m">m</code></td>
<td>
<p>Limit on steps (default infinite)</p>
</td></tr>
<tr><td><code id="uni.slice_+3A_lower">lower</code></td>
<td>
<p>Lower bound on support of the distribution (default -Inf)</p>
</td></tr>
<tr><td><code id="uni.slice_+3A_upper">upper</code></td>
<td>
<p>Upper bound on support of the distribution (default +Inf)</p>
</td></tr>
<tr><td><code id="uni.slice_+3A_gx0">gx0</code></td>
<td>
<p>Value of g(x0), if known (default is not known)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The point sampled, with its log density attached as an attribute.
</p>


<h3>Note</h3>

<p>The log density function may return -Inf for points outside the support
of the distribution.  If a lower and/or upper bound is specified for the
support, the log density function will not be called outside such limits.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
