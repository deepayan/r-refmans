<!DOCTYPE html><html lang="en"><head><title>Help for package bigSurvSGD</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {bigSurvSGD}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bigSurvSGD'><p>Big survival data analysis using stochastic gradient descent</p></a></li>
<li><a href='#lambdaMaxC'><p>Calculates the maximum penalty coefficient lambda for which all</p>
coefficients become zero</a></li>
<li><a href='#oneChunkC'><p>Updates the coefficients based on one pass of data</p></a></li>
<li><a href='#oneObsPlugingC'><p>Calculates the gradient and Hessian corresponding to one individual</p></a></li>
<li><a href='#sparseSurvData'><p>Simulated sparse survival dataset</p></a></li>
<li><a href='#survData'><p>Simulated survival dataset</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Big Survival Analysis Using Stochastic Gradient Descent</td>
</tr>
<tr>
<td>Version:</td>
<td>0.0.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2020-09-11</td>
</tr>
<tr>
<td>Description:</td>
<td>Fits Cox model via stochastic gradient descent. This implementation avoids computational instability of the standard Cox Model when dealing large datasets. Furthermore, it scales up with large datasets that do not fit the memory. It also handles large sparse datasets using proximal stochastic gradient descent algorithm. For more details about the method, please see Aliasghar Tarkhan and Noah Simon (2020) &lt;<a href="https://doi.org/10.48550/arXiv.2003.00116">doi:10.48550/arXiv.2003.00116</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 1.0.4), bigmemory, doParallel, survival</td>
</tr>
<tr>
<td>Depends:</td>
<td>foreach, parallel, R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.0</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/atarkhan/bigSurvSGD/issues">https://github.com/atarkhan/bigSurvSGD/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-09-28 16:59:42 UTC; arash</td>
</tr>
<tr>
<td>Author:</td>
<td>Aliasghar Tarkhan [aut, cre],
  Noah Simon [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Aliasghar Tarkhan &lt;atarkhan@uw.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-10-01 08:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='bigSurvSGD'>Big survival data analysis using stochastic gradient descent</h2><span id='topic+bigSurvSGD'></span><span id='topic+print.bigSurvSGD'></span><span id='topic+plot.bigSurvSGD'></span>

<h3>Description</h3>

<p>Fits Cox model via stochastic gradient descent (SGD). This implementation avoids computational 
instability of the standard Cox Model when datasets are large. Furthermore, it scales up with 
very large datasets that do not fit the memory. It also handles large sparse datasets using the 
proximal stochastic gradient descent algorithm. For more details about the method, please see 
Aliasghar Tarkhan and Noah Simon (2020) &lt;arXiv:2003.00116v2&gt;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bigSurvSGD(
  formula = Surv(time = time, status = status) ~ .,
  data,
  norm.method = "standardize",
  features.mean = NULL,
  features.sd = NULL,
  opt.method = "AMSGrad",
  beta.init = NULL,
  beta.type = "averaged",
  lr.const = 0.12,
  lr.tau = 0.5,
  strata.size = 20,
  batch.size = 1,
  num.epoch = 100,
  b1 = 0.9,
  b2 = 0.99,
  eps = 1e-08,
  inference.method = "plugin",
  num.boot = 1000,
  num.epoch.boot = 100,
  boot.method = "SGD",
  lr.const.boot = 0.12,
  lr.tau.boot = 0.5,
  num.sample.strata = 1000,
  sig.level = 0.05,
  beta0 = 0,
  alpha = NULL,
  lambda = NULL,
  nlambda = 100,
  num.strata.lambda = 10,
  lambda.scale = 1,
  parallel.flag = FALSE,
  num.cores = NULL,
  bigmemory.flag = FALSE,
  num.rows.chunk = 1e+06,
  col.names = NULL
)

## S3 method for class 'bigSurvSGD'
print(x, ...)

## S3 method for class 'bigSurvSGD'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bigSurvSGD_+3A_formula">formula</code></td>
<td>
<p>a formula in format of Surv(time=time, status=status)~feature1+feature2+... 
describing time-to-event variable, status variable, and features to be 
included in model. Default is &quot;Surv(time, status)~.&quot; that regresses
on all the features included in the dataset.</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_data">data</code></td>
<td>
<p>survival dataset. It can be in form of data.frame or a path to a .csv file if
we aim not to read data off the memory. If we aim to read data off the memory, it must be 
a path to a .csv data.</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_norm.method">norm.method</code></td>
<td>
<p>normalization method before starting the analysis.
&quot;center&quot; only centers the features by subtracting the mean, &quot;scale&quot; only
scales the features by dividing features to their standard deviation, &quot;normalization&quot;
does both centering and scaling, and &quot;none&quot; does not perform any pre-processing. The default
is &quot;normalization&quot;.</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_features.mean">features.mean</code></td>
<td>
<p>mean vector of features used for normalization.
The default is NULL where our alorithm calculates it.</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_features.sd">features.sd</code></td>
<td>
<p>standard deviation vector of features used for normalization.
The default is NULL where our alorithm calculates it.</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_opt.method">opt.method</code></td>
<td>
<p>optimization algorithm: &quot;SGD&quot; estimates the coefficients
using the standard stochastic gradient descent; &quot;ADAM&quot; estimates the coefficients
using ADAM optimizer; &quot;AMSGrad&quot; estimates the coefficients using AMSGrad optimizer.
The default is &quot;AMSGrad&quot;.</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_beta.init">beta.init</code></td>
<td>
<p>initialization for coefficient. The default is NULL where our algorithm
starts with an all-zero vector.</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_beta.type">beta.type</code></td>
<td>
<p>type of coefficient to be returned. If specified as &quot;single&quot;, the last
updated coefficient is returned. If specified as &quot;averaged&quot;, the Polyak-Ruppert
(i.e., average over iterates) is returned. The default is &quot;averaged&quot;.</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_lr.const">lr.const</code></td>
<td>
<p>proportional constant for the learning rate. The higher values
give faster but noisier estimates and vice versa. The default is 0.12 for
&quot;AMSGrad&quot; optimizer.</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_lr.tau">lr.tau</code></td>
<td>
<p>the power of iteration index in the learning rate. The bigger
value represents the faster decay in the lerning rate and vice versa. The
default is 0.5.</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_strata.size">strata.size</code></td>
<td>
<p>strata size. The default is 20 patients per stratum.</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_batch.size">batch.size</code></td>
<td>
<p>batch size. The default is 1 stratum per batch.</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_num.epoch">num.epoch</code></td>
<td>
<p>Number of epochs for the SGD-based algorithms. The default is 100.</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_b1">b1</code></td>
<td>
<p>hyper parameter for &quot;AMSGrad&quot; and &quot;ADAM&quot;. The default is 0.9.
See <a href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</a> for &quot;ADMA&quot; and
<a href="https://arxiv.org/abs/1904.03590">https://arxiv.org/abs/1904.03590</a> for &quot;AMSGrad&quot;.</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_b2">b2</code></td>
<td>
<p>hyper parameter for &quot;AMSGrad&quot; and &quot;ADAM&quot;. The default is 0.99.</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_eps">eps</code></td>
<td>
<p>hyper parameter for &quot;AMSGrad&quot; and &quot;ADAM&quot;. The default is 1e-8.</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_inference.method">inference.method</code></td>
<td>
<p>method for inference, i.e., constructing confidence
interval (CI): &quot;bootstrap&quot; constructs CI usin non-parametric bootstrap;
&quot;plugin&quot;: constructs CI using asymptotic properties of U-statistics;
The default is &quot;plugin&quot; which returns estimates, confidence intervals,
test statistics, and p-values.</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_num.boot">num.boot</code></td>
<td>
<p>number of boostrap resamples. Default is 1000.</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_num.epoch.boot">num.epoch.boot</code></td>
<td>
<p>number of epochs for each boorstrap resamples.
Default is 100.</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_boot.method">boot.method</code></td>
<td>
<p>optimization method for bootstrap. Default is &quot;SGD&quot;.</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_lr.const.boot">lr.const.boot</code></td>
<td>
<p>proportional constant for the learning rate for bootstrap
resamples. Defauls is &quot;0.12&quot;</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_lr.tau.boot">lr.tau.boot</code></td>
<td>
<p>power of iteration index in the learning rate for bootstrap resamples. 
Defauls is &quot;0.5&quot;</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_num.sample.strata">num.sample.strata</code></td>
<td>
<p>number of sample strata per observation to estimate
standard error using plugin method. Default value is 1000.</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_sig.level">sig.level</code></td>
<td>
<p>significance level for constructing (1-sig.level) confidence interval.
Default is 0.05.</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_beta0">beta0</code></td>
<td>
<p>null vector of coefficients for calculating p-value using plugin method.
Default is zero vector.</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_alpha">alpha</code></td>
<td>
<p>penalty coeficient between 0 and 1. alpha=0 only considers
the ridge penlaty and alpha=1 only considers the lasso penalty. Otherwise,
it considers a convex combination of these two penalties. Defualt is NULL, i.e.,
no penalty.</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_lambda">lambda</code></td>
<td>
<p>coeficient for the elastic net penalty.
There are three possible scenarios: (1) If alpha is defined NULL, no penalty
(ridge or lasso) is considered regardless of values of lambda; (2)
If alpha is not NULL but lambda is NULL, it first calculates
the largest value of lambda (lambda.max) for which all coefficients become zero.
Then it considers an exponentially decreasing sequence of lambda starting from
lambda.max ges toward lambda.min (lambda.min=0.01*lambda.max if p&gt;n, otherewise
lambda.min=0.0001*lambda.max) and return their corresponding coefficients.
(3) If a value for lambda is specified, our algorithm returns
coefficients for specified pair of (lambda, alpha). The default is NULL.</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_nlambda">nlambda</code></td>
<td>
<p>number of elements to be considered for scenario (2) above.
Default is 100.</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_num.strata.lambda">num.strata.lambda</code></td>
<td>
<p>number of sample strata to estimate maximum lambda (lambda.max) 
when alpha is not NULL and lambda=NULL (see lambda).</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_lambda.scale">lambda.scale</code></td>
<td>
<p>we scale lambda.max to make sure we start with a lambda
for which we get all coefficients equal to 0. Default is 1.</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_parallel.flag">parallel.flag</code></td>
<td>
<p>to specify if we want to use parallel computing for
inference. Default is &quot;F&quot;, i.e., no parallel computing.</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_num.cores">num.cores</code></td>
<td>
<p>number of cores for parallel computing. The default is &quot;NULL&quot;
for which if parallel.flag=T, it uses all available cores on your system.</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_bigmemory.flag">bigmemory.flag</code></td>
<td>
<p>determins if data needs to be read off the memory in case
data does not fit memory. Default is F, not to use bigmemoty package.</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_num.rows.chunk">num.rows.chunk</code></td>
<td>
<p>maximum number of rows per chunk to be read off the memory.
This is crucial for the large datasets that do not fit memory. 
Use fewer number of rows for the large number of features, especially 
if you receive an error due to lack of memory. The default value is 1e6 rows.</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_col.names">col.names</code></td>
<td>
<p>a vector of characters for column names of data.
If NULL, the column names of dataset &quot;data&quot; will be selected. The default
is NULL (i.e., reads columns of given dataset).</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_x">x</code></td>
<td>
<p>a 'bigSurvSGD' object</p>
</td></tr>
<tr><td><code id="bigSurvSGD_+3A_...">...</code></td>
<td>
<p>additional argument used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>coef: Log of hazards ratio. If no inference is used, it returns a vector for estimated
coefficients: If inference is used, it returns a matrix including estimates and
confidence intervals of coefficients. In case of penalization, it resturns a
matrix with columns corresponding to lambdas.
</p>
<p>coef.exp: Exponentiated version of coef (hazards ratio).
</p>
<p>lambda: Returns lambda(s) used for penalizarion.
</p>
<p>alpha: Returns alpha used for penalizarion.
</p>
<p>features.mean: Returns means of features, if given or calculated
</p>
<p>features.sd: Returns standard deviations of features, if given or calculated.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Simulated survival data - just estimation and no confidence interval
data(survData) # a dataset with 1000 observations (rows) and 10 features (columns)
resultsBig &lt;- bigSurvSGD(formula=Surv(time, status)~.,data=survData, inference.method="none",
parallel.flag=TRUE, num.cores=2)
resultsBig



# Simulated survival data
data(survData) # a dataset with 1000 observations (rows) and 10 features (columns)
resultsBig &lt;- bigSurvSGD(formula=Surv(time, status)~.,data=survData, inference="none",
parallel.flag=TRUE, num.cores=2)
resultsBig
 



# Simulated survival data to be read off the memory
data(survData) # a dataset with 1000 observations (rows) and 10 features (columns)
# Save dataset survSGD as bigSurvSGD to be read chunk-by-chunk off the memory 
write.csv(survData, file.path(tempdir(), "bigSurvData.csv"), row.names = FALSE) 
dataPath &lt;- file.path(tempdir(), "bigSurvData.csv") # path to where data is
resultsBigOffMemory &lt;- bigSurvSGD(formula=Surv(time, status)~., data=dataPath, 
bigmemory.flag=TRUE, parallel.flag=TRUE, num.cores=2)
resultsBigOffMemory




# Simulated sparse survival data
data(sparseSurvData) # a sparse data with 100 observations (rows) and 150 features (columns)
resultsBigSparse &lt;- bigSurvSGD(formula=Surv(time, status)~.,data=sparseSurvData, 
alpha=0.9, lambda=0.1)
resultsBigSparse


</code></pre>

<hr>
<h2 id='lambdaMaxC'>Calculates the maximum penalty coefficient lambda for which all 
coefficients become zero</h2><span id='topic+lambdaMaxC'></span>

<h3>Description</h3>

<p>Calculates the maximum penalty coefficient lambda for which all 
coefficients become zero
</p>

<hr>
<h2 id='oneChunkC'>Updates the coefficients based on one pass of data</h2><span id='topic+oneChunkC'></span>

<h3>Description</h3>

<p>Updates the coefficients based on one pass of data
</p>

<hr>
<h2 id='oneObsPlugingC'>Calculates the gradient and Hessian corresponding to one individual</h2><span id='topic+oneObsPlugingC'></span>

<h3>Description</h3>

<p>Calculates the gradient and Hessian corresponding to one individual
</p>

<hr>
<h2 id='sparseSurvData'>Simulated sparse survival dataset</h2><span id='topic+sparseSurvData'></span>

<h3>Description</h3>

<p>Simulated sparse survival dataset
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(sparseSurvData)
</code></pre>


<h3>Format</h3>

<p>An object of class data.frame including 100 observations (rows)
and 150 features (columns)
</p>


<h3>References</h3>

<p>Ralf Bender, Thomas Augustin, and Maria Blettner
(<a href="https://onlinelibrary.wiley.com/doi/10.1002/sim.2059">Generating survival times to simulate Cox proportional hazards models</a>)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(sparseSurvData)

</code></pre>

<hr>
<h2 id='survData'>Simulated survival dataset</h2><span id='topic+survData'></span>

<h3>Description</h3>

<p>Simulated survival dataset
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(survData)
</code></pre>


<h3>Format</h3>

<p>An object of class data.frame including 1000 observations (rows)
and 10 features (columns)
</p>


<h3>References</h3>

<p>Ralf Bender, Thomas Augustin, and Maria Blettner
(<a href="https://onlinelibrary.wiley.com/doi/10.1002/sim.2059">Generating survival times to simulate Cox proportional hazards models</a>)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(survData)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
