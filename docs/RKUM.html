<!DOCTYPE html><html><head><title>Help for package RKUM</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {RKUM}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#gkm'>
<p>Kernel Matrix Using Guasian Kernel</p></a></li>
<li><a href='#gm3edc'>
<p>A helper function</p></a></li>
<li><a href='#gmedc'>
<p>A helper function</p></a></li>
<li><a href='#gmi'>
<p>A helper function</p></a></li>
<li><a href='#hadr'>
<p>Hampel's psi function</p>
</p></a></li>
<li><a href='#halfun'>
<p>A Hampel loss function</p></a></li>
<li><a href='#halofun'>
<p>Objective function</p></a></li>
<li><a href='#hudr'>
<p>Huber's psi function</p></a></li>
<li><a href='#hulfun'>
<p>A Huber loss function</p></a></li>
<li><a href='#hulofun'>
<p>Objective function</p></a></li>
<li><a href='#ibskm'>
<p>Kernel Matrix Using Identity-by-state Kernel</p></a></li>
<li><a href='#ifcca'>
<p>Influence Funciton of Canonical Correlation Analysis</p></a></li>
<li><a href='#ifmkcca'>
<p>Influence Function of Multiple Kernel Canonical Analysis</p></a></li>
<li><a href='#ifrkcca'>
<p>Influence Function of Robust Kernel Canonical Analysis</p></a></li>
<li><a href='#lcv'>
<p>A helper function</p></a></li>
<li><a href='#lkm'>
<p>Kernel Matrix Using Linear  Kernel</p></a></li>
<li><a href='#mdbw'>
<p>Bandwidth  of the Gaussian kernel</p></a></li>
<li><a href='#medc'>
<p>A helper function</p></a></li>
<li><a href='#mvnod'>
<p>A helper function</p></a></li>
<li><a href='#ranuf'>
<p>A helper function</p></a></li>
<li><a href='#rkcca'>
<p>Robust kernel canonical correlation analysis</p></a></li>
<li><a href='#rkcco'>
<p>Robust kernel cross-covariance opetator</p></a></li>
<li><a href='#rkcm'>
<p>Robsut Kernel Center Matrix</p></a></li>
<li><a href='#rlogit'>
<p>A helper fuction</p></a></li>
<li><a href='#snpfmridata'>
<p>An example of imaging genetics data to calcualte influential  observations from two view data</p></a></li>
<li><a href='#snpfmrimth3D'>
<p>An example of imaging genetics and epi-genetics data  to  calcualte influential  observations from three view data</p></a></li>
<li><a href='#udtd'>
<p>A helper function</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Robust Kernel Unsupervised Methods</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.1.1</td>
</tr>
<tr>
<td>Author:</td>
<td>Md Ashad Alam</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Md Ashad Alam &lt;malam@tulane.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Robust  kernel center matrix, robust  kernel cross-covariance operator for kernel unsupervised methods, kernel canonical correlation analysis, 
 influence function of identifying significant outliers or atypical objects from multimodal datasets. Alam, M. A,  Fukumizu, K., Wang  Y.-P. (2018) &lt;<a href="https://doi.org/10.1016%2Fj.neucom.2018.04.008">doi:10.1016/j.neucom.2018.04.008</a>&gt;.
   Alam, M. A,  Calhoun, C. D.,  Wang  Y.-P. (2018) &lt;<a href="https://doi.org/10.1016%2Fj.csda.2018.03.013">doi:10.1016/j.csda.2018.03.013</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, graphics</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-06-22 04:48:39 UTC; hornik</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-06-22 04:50:17 UTC</td>
</tr>
</table>
<hr>
<h2 id='gkm'>
Kernel Matrix Using Guasian Kernel
</h2><span id='topic+gkm'></span>

<h3>Description</h3>

<p>Many radial basis function kernels, such as the Gaussian kernel, map X into a infinte dimensional space.  While the Gaussian kernel has a free parameter (bandwidth), it still follows a number of theoretical properties such as boundedness, consistence, universality, robustness etc. It is the most applicable kernel of the  positive definite kernel based methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gkm(X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gkm_+3A_x">X</code></td>
<td>

<p>a data matrix.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Many radial basis function kernels, such as the Gaussian kernel, map
input sapce into a infinite dimensional space. The Gaussian kernel has a  a number of theoretical properties such as boundedness, consistence,
universality and robustness, etc.
</p>


<h3>Value</h3>

<table>
<tr><td><code>K</code></td>
<td>
<p>a  Gram/ kernel  matrix</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Md Ashad Alam
&lt;malam@tulane.edu&gt;
</p>


<h3>References</h3>

<p>Md. Ashad Alam, Hui-Yi Lin, HOng-Wen Deng, Vince Calhour Yu-Ping Wang (2018),
A kernel machine method for detecting higher order interactions in
multimodal datasets: Application to schizophrenia,
Journal of Neuroscience Methods, Vol. 309, 161-174.
</p>
<p>Md Ashad Alam, Kenji Fukumizu and  Yu-Ping Wang (2018),
Influence Function and Robust Variant of Kernel Canonical Correlation Analysis,
Neurocomputing, Vol. 304 (2018) 12-29.
</p>
<p>M. Romanazzi (1992),
 Influence in canonical correlation analysis,
 Psychometrika
vol 57(2) (1992) 237-259.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##Dummy data:
X&lt;-matrix(rnorm(1000),100)
gkm(X)
</code></pre>

<hr>
<h2 id='gm3edc'>
A helper function
</h2><span id='topic+gm3edc'></span>

<h3>Description</h3>

<p>#An matrices dicomposition function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gm3edc(Amat, Bmat, Cmat)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gm3edc_+3A_amat">Amat</code></td>
<td>

<p>a square  matrix
</p>
</td></tr>
<tr><td><code id="gm3edc_+3A_bmat">Bmat</code></td>
<td>

<p>a square  matrix
</p>
</td></tr>
<tr><td><code id="gm3edc_+3A_cmat">Cmat</code></td>
<td>

<p>a square  matrix
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Md Ashad Alam
&lt;malam@tulane.edu&gt;
</p>

<hr>
<h2 id='gmedc'>
A helper function
</h2><span id='topic+gmedc'></span>

<h3>Description</h3>

<p>#An matrices dicomposition function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gmedc(A, B = diag(nrow(A)))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gmedc_+3A_a">A</code></td>
<td>

<p>a square  matrix
</p>
</td></tr>
<tr><td><code id="gmedc_+3A_b">B</code></td>
<td>

<p>a diagonal  matrix
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Md Ashad Alam
&lt;malam@tulane.edu&gt;
</p>

<hr>
<h2 id='gmi'>
A helper function
</h2><span id='topic+gmi'></span>

<h3>Description</h3>

<p>###An  function to adjust
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gmi(X, tol = sqrt(.Machine$double.eps))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gmi_+3A_x">X</code></td>
<td>

<p>a square  matrix
</p>
</td></tr>
<tr><td><code id="gmi_+3A_tol">tol</code></td>
<td>

<p>a real value
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Md Ashad Alam
&lt;malam@tulane.edu&gt;
</p>

<hr>
<h2 id='hadr'>
Hampel's psi function
</h2><span id='topic+hadr'></span>

<h3>Description</h3>

<p>##The ratio of  the first derivative of the Hampel loss fuction to  the argument. Tuning constants are fixed in different quintiles.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hadr(u)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hadr_+3A_u">u</code></td>
<td>

<p>vector  values
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a real value
</p>


<h3>Author(s)</h3>

<p>Md Ashad Alam
&lt;malam@tulane.edu&gt;
</p>


<h3>References</h3>

<p>Md Ashad Alam, Kenji Fukumizu and  Yu-Ping Wang (2018),
Influence Function and Robust Variant of Kernel Canonical Correlation Analysis,
Neurocomputing, Vol. 304 (2018) 12-29.
</p>
<p>M. Romanazzi (1992),
 Influence in canonical correlation analysis,
 Psychometrika
vol 57(2) (1992) 237-259.
</p>


<h3>See Also</h3>

<p>#See Also as <code><a href="#topic+gkm">gkm</a></code>, <code><a href="#topic+hudr">hudr</a></code>
</p>

<hr>
<h2 id='halfun'>
A Hampel loss function
</h2><span id='topic+halfun'></span>

<h3>Description</h3>

<p>#Tuning constants  of the  Hampel loss fuction are fixed in different quintiles of the arguments.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>halfun(u)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="halfun_+3A_u">u</code></td>
<td>
<p>vector of values.
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>comp1</code></td>
<td>
<p>a real number</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Md Ashad Alam
&lt;malam@tulane.edu&gt;
</p>


<h3>References</h3>

<p>Md Ashad Alam, Kenji Fukumizu and  Yu-Ping Wang (2018),
Influence Function and Robust Variant of Kernel Canonical Correlation Analysis,
Neurocomputing, Vol. 304 (2018) 12-29.
</p>
<p>M. Romanazzi (1992),
 Influence in canonical correlation analysis,
 Psychometrika
vol 57(2) (1992) 237-259.
</p>


<h3>See Also</h3>

<p>See Also as <code><a href="#topic+hulfun">hulfun</a></code>, <code><a href="#topic+hadr">hadr</a></code>, <code><a href="#topic+hudr">hudr</a></code>
</p>

<hr>
<h2 id='halofun'>
Objective function
</h2><span id='topic+halofun'></span>

<h3>Description</h3>

<p>Objective function of Hampel's loss fucntion
</p>


<h3>Usage</h3>

<pre><code class='language-R'>halofun(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="halofun_+3A_x">x</code></td>
<td>

<p>vector values
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a real value
</p>


<h3>Author(s)</h3>

<p>Md Ashad Alam
&lt;malam@tulane.edu&gt;
</p>


<h3>References</h3>

<p>Md Ashad Alam, Kenji Fukumizu and  Yu-Ping Wang (2018),
Influence Function and Robust Variant of Kernel Canonical Correlation Analysis,
Neurocomputing, Vol. 304 (2018) 12-29.
</p>
<p>M. Romanazzi (1992),
 Influence in canonical correlation analysis,
 Psychometrika
vol 57(2) (1992) 237-259.
</p>


<h3>See Also</h3>

<p>See also as <code><a href="#topic+hulofun">hulofun</a></code>
</p>

<hr>
<h2 id='hudr'>
Huber's psi function
</h2><span id='topic+hudr'></span>

<h3>Description</h3>

<p>The ratio of  the first derivative of the Huber loss fuction to  the argument. Tuning constants is fixed as a meadian vlue.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hudr(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hudr_+3A_x">x</code></td>
<td>

<p>vector  values
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>y</code></td>
<td>
<p>a real value</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Md Ashad Alam
&lt;malam@tulane.edu&gt;
</p>


<h3>References</h3>

<p>Md Ashad Alam, Kenji Fukumizu and  Yu-Ping Wang (2018),
Influence Function and Robust Variant of Kernel Canonical Correlation Analysis,
Neurocomputing, Vol. 304 (2018) 12-29.
</p>
<p>M. Romanazzi (1992),
 Influence in canonical correlation analysis,
 Psychometrika
vol 57(2) (1992) 237-259.
</p>


<h3>See Also</h3>

<p>See also as <code><a href="#topic+hadr">hadr</a></code>
</p>

<hr>
<h2 id='hulfun'>
A Huber loss function
</h2><span id='topic+hulfun'></span>

<h3>Description</h3>

<p>Tuning constants  of the  Huber loss fuction are fixed in different quintiles of the arguments.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hulfun(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hulfun_+3A_x">x</code></td>
<td>

<p>a vector values
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Tuning constants  of the  Huber  fuction is fixed as a   median.
</p>


<h3>Value</h3>

<p>a real number
</p>


<h3>Author(s)</h3>

<p>Md Ashad Alam
&lt;malam@tulane.edu&gt;
</p>


<h3>References</h3>

<p>Md Ashad Alam, Kenji Fukumizu and  Yu-Ping Wang (2018),
Influence Function and Robust Variant of Kernel Canonical Correlation Analysis,
Neurocomputing, Vol. 304 (2018) 12-29.
</p>
<p>M. Romanazzi (1992),
 Influence in canonical correlation analysis,
 Psychometrika
vol 57(2) (1992) 237-259.
</p>


<h3>See Also</h3>

<p>See also as <code><a href="#topic+halfun">halfun</a></code>
</p>

<hr>
<h2 id='hulofun'>
Objective function
</h2><span id='topic+hulofun'></span>

<h3>Description</h3>

<p>Objective function of Huber's loss fucntion
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hulofun(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hulofun_+3A_x">x</code></td>
<td>
<p>            vector values
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a real value
</p>


<h3>Author(s)</h3>

<p>Md Ashad Alam
&lt;malam@tulane.edu&gt;
</p>


<h3>References</h3>

<p>Md Ashad Alam, Kenji Fukumizu and  Yu-Ping Wang (2018),
Influence Function and Robust Variant of Kernel Canonical Correlation Analysis,
Neurocomputing, Vol. 304 (2018) 12-29.
</p>
<p>M. Romanazzi (1992),
 Influence in canonical correlation analysis,
 Psychometrika
vol 57(2) (1992) 237-259.
</p>


<h3>See Also</h3>

<p>See Also as <code><a href="#topic+halofun">halofun</a></code>, ~~~
</p>

<hr>
<h2 id='ibskm'>
Kernel Matrix Using Identity-by-state Kernel
</h2><span id='topic+ibskm'></span>

<h3>Description</h3>

<p>For GWASs, a kernel captures the pairwise similarity across a number of SNPs in each gene.
Kernel projects the genotype data from original high dimensional space to a feature space. One of
the more popular kernels used for genomics similarity is the identity-by-state (IBS) kernel (non-
parametric function of the genotypes)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ibskm(Z)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ibskm_+3A_z">Z</code></td>
<td>

<p>a data matrix
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For genome-wide association study, a kernel captures the
pairwise similarity across a number of SNPs in each gene. Kernel projects
the genotype data from original high dimensional space to a feature
space. One popular kernel used for genomics similarity is the
identity-by-state (IBS) kernel, The IBS kernel does not need any assumption on the type of genetic interactions.
</p>


<h3>Value</h3>

<table>
<tr><td><code>K</code></td>
<td>
<p>a  Gram/ kernel  matrix</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Md Ashad Alam
&lt;malam@tulane.edu&gt;
</p>


<h3>References</h3>

<p>Md. Ashad Alam, Hui-Yi Lin, HOng-Wen Deng, Vince Calhour Yu-Ping Wang (2018),
A kernel machine method for detecting higher order interactions in
multimodal datasets: Application to schizophrenia,
Journal of Neuroscience Methods, Vol. 309, 161-174.
</p>
<p>Md Ashad Alam, Kenji Fukumizu and  Yu-Ping Wang (2018),
Influence Function and Robust Variant of Kernel Canonical Correlation Analysis,
Neurocomputing, Vol. 304 (2018) 12-29.
</p>
<p>M. Romanazzi (1992),
 Influence in canonical correlation analysis,
 Psychometrika
vol 57(2) (1992) 237-259.
</p>


<h3>See Also</h3>

<p>See also as <code><a href="#topic+gkm">gkm</a></code>, <code><a href="#topic+lkm">lkm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##Dummy data:
X &lt;- matrix(rnorm(200),50)
ibskm(X)
</code></pre>

<hr>
<h2 id='ifcca'>
Influence Funciton of Canonical Correlation Analysis
</h2><span id='topic+ifcca'></span>

<h3>Description</h3>

<p>##To define the robustness in statistics, different approaches have been pro-
posed, for example, the minimax approach, the sensitivity curve, the  influence
function (IF) and the finite sample breakdown point.  Due to its simplic-
ity, the IF is the most useful approach in statistical machine  learning
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ifcca(X, Y, gamma = 1e-05, ncomps = 2, jth = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ifcca_+3A_x">X</code></td>
<td>

<p>a  data matrix  index by row
</p>
</td></tr>
<tr><td><code id="ifcca_+3A_y">Y</code></td>
<td>

<p>a  data matrix  index by row
</p>
</td></tr>
<tr><td><code id="ifcca_+3A_gamma">gamma</code></td>
<td>

<p>the  hyper-parameters
</p>
</td></tr>
<tr><td><code id="ifcca_+3A_ncomps">ncomps</code></td>
<td>

<p>the number of canonical vectors
</p>
</td></tr>
<tr><td><code id="ifcca_+3A_jth">jth</code></td>
<td>
<p> the influence function of  the jth canonical vector
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>iflccor</code></td>
<td>
<p>Influence value of the data by linear canonical correalation</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Md Ashad Alam
&lt;malam@tulane.edu&gt;
</p>


<h3>References</h3>

<p>Md Ashad Alam, Kenji Fukumizu and  Yu-Ping Wang (2018),
Influence Function and Robust Variant of Kernel Canonical Correlation Analysis,
Neurocomputing, Vol. 304 (2018) 12-29.
</p>
<p>M. Romanazzi (1992),
 Influence in canonical correlation analysis,
 Psychometrika
vol 57(2) (1992) 237-259.
</p>


<h3>See Also</h3>

<p>See also as <code><a href="#topic+rkcca">rkcca</a></code>, <code><a href="#topic+ifrkcca">ifrkcca</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##Dummy data:

X &lt;- matrix(rnorm(500),100); Y &lt;- matrix(rnorm(500),100)

ifcca(X,Y,  1e-05,  2, 2)
</code></pre>

<hr>
<h2 id='ifmkcca'>
Influence Function of Multiple Kernel Canonical Analysis
</h2><span id='topic+ifmkcca'></span>

<h3>Description</h3>

<p>## To define the robustness in statistics, different approaches have been pro-
posed, for example, the minimax approach, the sensitivity curve, the  influence
function (IF) and the finite sample breakdown point.  Due to its simplic-
ity, the IF is the most useful approach in statistical machine  learning.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ifmkcca(xx, yy, zz, kernel = "rbfdot", gamma = 1e-05, ncomps = 1, jth=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ifmkcca_+3A_xx">xx</code></td>
<td>

<p>a  data matrix  index by row
</p>
</td></tr>
<tr><td><code id="ifmkcca_+3A_yy">yy</code></td>
<td>

<p>a  data matrix  index by row
</p>
</td></tr>
<tr><td><code id="ifmkcca_+3A_zz">zz</code></td>
<td>

<p>a  data matrix  index by row
</p>
</td></tr>
<tr><td><code id="ifmkcca_+3A_kernel">kernel</code></td>
<td>

<p>a positive definite kernel
</p>
</td></tr>
<tr><td><code id="ifmkcca_+3A_ncomps">ncomps</code></td>
<td>

<p>the number of canonical vectors
</p>
</td></tr>
<tr><td><code id="ifmkcca_+3A_gamma">gamma</code></td>
<td>

<p>the  hyper-parameters.
</p>
</td></tr>
<tr><td><code id="ifmkcca_+3A_jth">jth</code></td>
<td>
<p> the influence function of  the jth canonical vector
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>iflccor</code></td>
<td>
<p>Influence value of the data by multiple kernel canonical correalation</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Md Ashad Alam
&lt;malam@tulane.edu&gt;
</p>


<h3>References</h3>

<p>Md Ashad Alam, Kenji Fukumizu and  Yu-Ping Wang (2018),
Influence Function and Robust Variant of Kernel Canonical Correlation Analysis,
Neurocomputing, Vol. 304 (2018) 12-29.
</p>
<p>M. Romanazzi (1992),
 Influence in canonical correlation analysis,
 Psychometrika
vol 57(2) (1992) 237-259.
</p>


<h3>See Also</h3>

<p>See also as <code><a href="#topic+ifcca">ifcca</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##Dummy data:

X &lt;- matrix(rnorm(500),100); Y &lt;- matrix(rnorm(500),100); Z &lt;- matrix(rnorm(500),100)

ifmkcca(X,Y, Z, "rbfdot",  1e-05,  2, 1)
</code></pre>

<hr>
<h2 id='ifrkcca'>
Influence Function of Robust Kernel Canonical Analysis
</h2><span id='topic+ifrkcca'></span>

<h3>Description</h3>

<p>##To define the robustness in statistics, different approaches have been pro-
posed, for example, the minimax approach, the sensitivity curve, the  influence
function (IF) and the finite sample breakdown point.  Due to its simplic-
ity, the IF is the most useful approach in statistical machine  learning.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ifrkcca(X, Y, lossfu = "Huber", kernel = "rbfdot", gamma = 0.00001, ncomps = 10, jth = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ifrkcca_+3A_x">X</code></td>
<td>

<p>a  data matrix  index by row
</p>
</td></tr>
<tr><td><code id="ifrkcca_+3A_y">Y</code></td>
<td>

<p>a  data matrix  index by row
</p>
</td></tr>
<tr><td><code id="ifrkcca_+3A_lossfu">lossfu</code></td>
<td>

<p>a loss function: square,  Hampel's or Huber's loss
</p>
</td></tr>
<tr><td><code id="ifrkcca_+3A_kernel">kernel</code></td>
<td>

<p>a positive definite kernel
</p>
</td></tr>
<tr><td><code id="ifrkcca_+3A_gamma">gamma</code></td>
<td>

<p>the  hyper-parameters
</p>
</td></tr>
<tr><td><code id="ifrkcca_+3A_ncomps">ncomps</code></td>
<td>

<p>the number of canonical vectors
</p>
</td></tr>
<tr><td><code id="ifrkcca_+3A_jth">jth</code></td>
<td>

<p>the influence function of  the jth canonical vector
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>ifrkcor</code></td>
<td>
<p>Influence value of the data by robust kernel canonical correalation</p>
</td></tr>
<tr><td><code>ifrkxcv</code></td>
<td>
<p>Influence value of  cnonical vector of X dataset</p>
</td></tr>
<tr><td><code>ifrkycv</code></td>
<td>
<p>Influence value of  cnonical vector of Y dataset</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Md Ashad Alam
&lt;malam@tulane.edu&gt;
</p>


<h3>References</h3>

<p>Md Ashad Alam, Kenji Fukumizu and  Yu-Ping Wang (2018),
Influence Function and Robust Variant of Kernel Canonical Correlation Analysis,
Neurocomputing, Vol. 304 (2018) 12-29.
</p>
<p>M. Romanazzi (1992),
 Influence in canonical correlation analysis,
 Psychometrika
vol 57(2) (1992) 237-259.
</p>


<h3>See Also</h3>

<p>See also as <code><a href="#topic+rkcca">rkcca</a></code>, <code><a href="#topic+ifrkcca">ifrkcca</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##Dummy data:

X &lt;- matrix(rnorm(500),100); Y &lt;- matrix(rnorm(500),100)

ifrkcca(X,Y, lossfu = "Huber", kernel = "rbfdot", gamma = 0.00001, ncomps = 10, jth = 2)
</code></pre>

<hr>
<h2 id='lcv'>
A helper function
</h2><span id='topic+lcv'></span>

<h3>Description</h3>

<p>#A function ..............
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lcv(X, Y, res)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lcv_+3A_x">X</code></td>
<td>

<p>a  matrix
</p>
</td></tr>
<tr><td><code id="lcv_+3A_y">Y</code></td>
<td>

<p>a   matrix
</p>
</td></tr>
<tr><td><code id="lcv_+3A_res">res</code></td>
<td>

<p>a real value
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Md Ashad Alam
&lt;malam@tulane.edu&gt;
</p>

<hr>
<h2 id='lkm'>
Kernel Matrix Using Linear  Kernel
</h2><span id='topic+lkm'></span>

<h3>Description</h3>

<p>The linear kernel is  used  by  the underlying Euclidean space to define the similarity measure.
Whenever the dimensionality is high, it may allow for more complexity in the function class than what we could measure and assess otherwise
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lkm(X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lkm_+3A_x">X</code></td>
<td>

<p>a data matrix
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The linear kernel is used by the underlying Euclidean space to define
the similarity measure.  Whenever the dimensionality of the data is high, it
may allow for more complexity in the function class than what we could
measure and assess otherwise.
</p>


<h3>Value</h3>

<table>
<tr><td><code>K</code></td>
<td>
<p>a kernel matrix.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Md Ashad Alam
&lt;malam@tulane.edu&gt;
</p>


<h3>References</h3>

<p>Md. Ashad Alam, Hui-Yi Lin, HOng-Wen Deng, Vince Calhour Yu-Ping Wang (2018),
A kernel machine method for detecting higher order interactions in
multimodal datasets: Application to schizophrenia,
Journal of Neuroscience Methods, Vol. 309, 161-174.
</p>
<p>Md Ashad Alam, Kenji Fukumizu and  Yu-Ping Wang (2018),
Influence Function and Robust Variant of Kernel Canonical Correlation Analysis,
Neurocomputing, Vol. 304 (2018) 12-29.
</p>
<p>Md Ashad Alam, Vince D. Calhoun and Yu-Ping Wang (2018),
 Identifying outliers using multiple kernel canonical
correlation analysis with application to imaging genetics,
Computational Statistics and Data Analysis, Vol. 125, 70- 85
</p>


<h3>See Also</h3>

<p>See also as <code><a href="#topic+gkm">gkm</a></code>, <code><a href="#topic+ibskm">ibskm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##Dummy data:

X &lt;- matrix(rnorm(500),100)
lkm(X)
</code></pre>

<hr>
<h2 id='mdbw'>
Bandwidth  of the Gaussian kernel
</h2><span id='topic+mdbw'></span>

<h3>Description</h3>

<p>A  median of the pairwise distance of the  data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mdbw(X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mdbw_+3A_x">X</code></td>
<td>

<p>a data matrix
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>While the Gaussian kernel has a free parameter (bandwidth), it still
follows a number of theoretical properties such as boundedness, consistenc, universality, robustness, etc. It is the most applicable one. In a Gaussian RBF kernel, we need to select an appropriate a bandwidth. It is well known that the parameter has a strong influence on the result of kernel methods. For the Gaussian kernel, we can use the median of the pairwise distance as a bandwidth.
</p>


<h3>Value</h3>

<table>
<tr><td><code>s</code></td>
<td>
<p> a  median of the pairwise distance of the   X dataset</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Md Ashad Alam
&lt;malam@tulane.edu&gt;
</p>


<h3>References</h3>

<p>Md. Ashad Alam, Hui-Yi Lin, HOng-Wen Deng, Vince Calhour Yu-Ping Wang (2018),
A kernel machine method for detecting higher order interactions in
multimodal datasets: Application to schizophrenia,
Journal of Neuroscience Methods, Vol. 309, 161-174.
</p>
<p>Md. Ashad Alam, Kenji Fukumizu and  Yu-Ping Wang (2018),
Influence Function and Robust Variant of Kernel Canonical Correlation Analysis,
Neurocomputing, Vol. 304 (2018) 12-29.
</p>
<p>Md. Ashad Alam and Kenji Fukumizu (2015),
Higher-order regularized kernel canonical correlation analysis,
International Journal of Pattern Recognition and Artificial Intelligence, Vol. 29(4) 1551005.
</p>
<p>Arthu Gretton, Kenji. Fukumizu, C. H. Teo, L. Song, B. Scholkopf and A. Smola (2008),
 A Kernel
statistical test of independence,
 in Advances in Neural Information Processing Systems,
Vol. 20  585â€“592.
</p>


<h3>See Also</h3>

<p>See also as  <code><a href="#topic+lkm">lkm</a></code>, <code><a href="#topic+gkm">gkm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##Dummy data:

X &lt;- matrix(rnorm(1000),100)

mdbw(X)
</code></pre>

<hr>
<h2 id='medc'>
A helper function
</h2><span id='topic+medc'></span>

<h3>Description</h3>

<p># A function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>medc(A, fn = sqrt)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="medc_+3A_a">A</code></td>
<td>

<p>a  matrix
</p>
</td></tr>
<tr><td><code id="medc_+3A_fn">fn</code></td>
<td>

<p>a  funciton
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Md Ashad Alam
&lt;malam@tulane.edu&gt;
</p>

<hr>
<h2 id='mvnod'>
A helper function
</h2><span id='topic+mvnod'></span>

<h3>Description</h3>

<p>## A function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mvnod(n = 1, mu, Sigma, tol = 1e-06, empirical = FALSE, EISPACK = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mvnod_+3A_n">n</code></td>
<td>

<p>an  integer number
</p>
</td></tr>
<tr><td><code id="mvnod_+3A_mu">mu</code></td>
<td>

<p>a real value
</p>
</td></tr>
<tr><td><code id="mvnod_+3A_sigma">Sigma</code></td>
<td>

<p>a real value
</p>
</td></tr>
<tr><td><code id="mvnod_+3A_tol">tol</code></td>
<td>

<p>a curection factor
</p>
</td></tr>
<tr><td><code id="mvnod_+3A_empirical">empirical</code></td>
<td>

<p>a logical value
</p>
</td></tr>
<tr><td><code id="mvnod_+3A_eispack">EISPACK</code></td>
<td>

<p>a logical value. TRUE for a complex values.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Md Ashad Alam
&lt;malam@tulane.edu&gt;
</p>

<hr>
<h2 id='ranuf'>
A helper function
</h2><span id='topic+ranuf'></span>

<h3>Description</h3>

<p>A function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ranuf(p)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ranuf_+3A_p">p</code></td>
<td>

<p>a real value
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Md Ashad Alam
&lt;malam@tulane.edu&gt;
</p>

<hr>
<h2 id='rkcca'>
Robust kernel canonical correlation analysis
</h2><span id='topic+rkcca'></span>

<h3>Description</h3>

<p>#A robust correlation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rkcca(X, Y, lossfu = "Huber", kernel = "rbfdot", gamma = 1e-05, ncomps = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rkcca_+3A_x">X</code></td>
<td>

<p>a  data matrix  index by row
</p>
</td></tr>
<tr><td><code id="rkcca_+3A_y">Y</code></td>
<td>

<p>a  data matrix  index by row
</p>
</td></tr>
<tr><td><code id="rkcca_+3A_lossfu">lossfu</code></td>
<td>

<p>a loss function: square,  Hampel's or Huber's loss
</p>
</td></tr>
<tr><td><code id="rkcca_+3A_kernel">kernel</code></td>
<td>

<p>a positive definite kernel
</p>
</td></tr>
<tr><td><code id="rkcca_+3A_gamma">gamma</code></td>
<td>

<p>the  hyper-parameters
</p>
</td></tr>
<tr><td><code id="rkcca_+3A_ncomps">ncomps</code></td>
<td>

<p>the number of canonical vectors
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An S3 object containing the following slots:
</p>
<table>
<tr><td><code>rkcor</code></td>
<td>
<p>Robsut kernel canonical correlation</p>
</td></tr>
<tr><td><code>rxcoef</code></td>
<td>
<p>Robsut  kernel canonical  coficient of X dataset</p>
</td></tr>
<tr><td><code>rycoef</code></td>
<td>
<p>Robsut   kernel canonical  coficient of Y dataset</p>
</td></tr>
<tr><td><code>rxcv</code></td>
<td>
<p>Robsut   kernel canonical  vector of X dataset</p>
</td></tr>
<tr><td><code>rycv</code></td>
<td>
<p>Robsut  kernel  canonical  vector of Y dataset</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Md Ashad Alam
&lt;malam@tulane.edu&gt;
</p>


<h3>References</h3>

<p>Md Ashad Alam, Kenji Fukumizu and  Yu-Ping Wang (2018),
Influence Function and Robust Variant of Kernel Canonical Correlation Analysis,
Neurocomputing, Vol. 304 (2018) 12-29.
</p>
<p>M. Romanazzi (1992),
 Influence in canonical correlation analysis,
 Psychometrika
vol 57(2) (1992) 237-259.
</p>


<h3>See Also</h3>

<p>See also as  <code><a href="#topic+ifcca">ifcca</a></code>, <code><a href="#topic+rkcca">rkcca</a></code>, <code><a href="#topic+ifrkcca">ifrkcca</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##Dummy data:

X &lt;- matrix(rnorm(1000),100); Y &lt;- matrix(rnorm(1000),100)

rkcca(X,Y, "Huber",  "rbfdot", 1e-05,  10)
</code></pre>

<hr>
<h2 id='rkcco'>
Robust kernel cross-covariance opetator
</h2><span id='topic+rkcco'></span>

<h3>Description</h3>

<p># A function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rkcco(X, Y, lossfu = "Huber", kernel = "rbfdot", gamma = 1e-05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rkcco_+3A_x">X</code></td>
<td>

<p>a  data matrix  index by row
</p>
</td></tr>
<tr><td><code id="rkcco_+3A_y">Y</code></td>
<td>

<p>a  data matrix  index by row
</p>
</td></tr>
<tr><td><code id="rkcco_+3A_lossfu">lossfu</code></td>
<td>

<p>a loss function: square,  Hampel's or Huber's loss
</p>
</td></tr>
<tr><td><code id="rkcco_+3A_kernel">kernel</code></td>
<td>

<p>a positive definite kernel
</p>
</td></tr>
<tr><td><code id="rkcco_+3A_gamma">gamma</code></td>
<td>

<p>the  hyper-parameters
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>rkcmx</code></td>
<td>
<p> Robust kernel center matrix of  X dataset</p>
</td></tr>
<tr><td><code>rkcmy</code></td>
<td>
<p> Robust kernel center matrix of  Y dataset</p>
</td></tr>
<tr><td><code>rkcmx</code></td>
<td>
<p> Robust kernel covariacne operator of  X dataset</p>
</td></tr>
<tr><td><code>rkcmy</code></td>
<td>
<p> Robust kernel covariacne operator of  Y dataset</p>
</td></tr>
<tr><td><code>rkcmx</code></td>
<td>
<p> Robust kernel cross-covariacne operator of  X  and Y datasets</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Md Ashad Alam
&lt;malam@tulane.edu&gt;
</p>


<h3>References</h3>

<p>Md Ashad Alam, Kenji Fukumizu and  Yu-Ping Wang (2018),
Influence Function and Robust Variant of Kernel Canonical Correlation Analysis,
Neurocomputing, Vol. 304 (2018) 12-29.
</p>
<p>M. Romanazzi (1992),
 Influence in canonical correlation analysis,
 Psychometrika
vol 57(2) (1992) 237-259.
</p>


<h3>See Also</h3>

<p>See also as  <code><a href="#topic+rkcca">rkcca</a></code> <code><a href="#topic+snpfmridata">snpfmridata</a></code>, <code><a href="#topic+ifrkcca">ifrkcca</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##Dummy data:

X &lt;- matrix(rnorm(2000),200); Y &lt;- matrix(rnorm(2000),200)

rkcco(X,Y, "Huber","rbfdot", 1e-05)
</code></pre>

<hr>
<h2 id='rkcm'>
Robsut Kernel Center Matrix
</h2><span id='topic+rkcm'></span>

<h3>Description</h3>

<p># A functioin
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rkcm(X, lossfu = "Huber", kernel = "rbfdot")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rkcm_+3A_x">X</code></td>
<td>

<p>a  data matrix  index by row
</p>
</td></tr>
<tr><td><code id="rkcm_+3A_lossfu">lossfu</code></td>
<td>

<p>a loss function: square,  Hampel's or Huber's loss
</p>
</td></tr>
<tr><td><code id="rkcm_+3A_kernel">kernel</code></td>
<td>

<p>a positive definite kernel
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>rkcm</code></td>
<td>
<p> a square robust kernel center matrix</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Md Ashad Alam
&lt;malam@tulane.edu&gt;
</p>


<h3>References</h3>

<p>Md Ashad Alam, Kenji Fukumizu and  Yu-Ping Wang (2018),
Influence Function and Robust Variant of Kernel Canonical Correlation Analysis,
Neurocomputing, Vol. 304 (2018) 12-29.
</p>
<p>Md Ashad Alam, Vince D. Calhoun and Yu-Ping Wang (2018),
 Identifying outliers using multiple kernel canonical
correlation analysis with application to imaging genetics,
Computational Statistics and Data Analysis, Vol. 125, 70- 85
</p>


<h3>See Also</h3>

<p>See also as  <code><a href="#topic+ifcca">ifcca</a></code>, <code><a href="#topic+rkcca">rkcca</a></code>, <code><a href="#topic+ifrkcca">ifrkcca</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##Dummy data:

X &lt;- matrix(rnorm(2000),200); Y &lt;- matrix(rnorm(2000),200)

rkcm(X, "Huber","rbfdot")
</code></pre>

<hr>
<h2 id='rlogit'>
A helper fuction
</h2><span id='topic+rlogit'></span>

<h3>Description</h3>

<p>#A function to calcualte generalized logit function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rlogit(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rlogit_+3A_x">x</code></td>
<td>

<p>a   real value to be tranformed
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Md Ashad Alam
&lt;malam@tulane.edu&gt;
</p>

<hr>
<h2 id='snpfmridata'>
An example of imaging genetics data to calcualte influential  observations from two view data
</h2><span id='topic+snpfmridata'></span>

<h3>Description</h3>

<p>#A function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>snpfmridata(n = 300, gamma=0.00001, ncomps = 2, jth = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="snpfmridata_+3A_n">n</code></td>
<td>

<p>the sample size
</p>
</td></tr>
<tr><td><code id="snpfmridata_+3A_gamma">gamma</code></td>
<td>

<p>the  hyper-parameters
</p>
</td></tr>
<tr><td><code id="snpfmridata_+3A_ncomps">ncomps</code></td>
<td>

<p>the number of canonical vectors
</p>
</td></tr>
<tr><td><code id="snpfmridata_+3A_jth">jth</code></td>
<td>

<p>the influence function of  the jth canonical vector
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>IFCCAID</code></td>
<td>
<p> Influence value of canonical correlation analysis for the  ideal data </p>
</td></tr>
<tr><td><code>IFCCACD</code></td>
<td>
<p> Influence value of canonical correlation analysis for the  contaminated data </p>
</td></tr>
<tr><td><code>IFKCCAID</code></td>
<td>
<p> Influence value of kernel canonical correlation analysis for the  ideal data </p>
</td></tr>
<tr><td><code>IFKCCACD</code></td>
<td>
<p> Influence value of  kernel canonical correlation analysis for the  contaminated data </p>
</td></tr>
<tr><td><code>IFHACCAID</code></td>
<td>
<p> Influence value of  robsut (Hampel's loss) canonical correlation analysis for the  ideal data </p>
</td></tr>
<tr><td><code>IFHACCACD</code></td>
<td>
<p> Influence value of robsut (Hampel's loss) canonical correlation analysis for the  contaminated data </p>
</td></tr>
<tr><td><code>IFHUCCAID</code></td>
<td>
<p> Influence value of  robsut (Huber's loss) canonical correlation analysis for the  ideal data </p>
</td></tr>
<tr><td><code>IFHUCCACD</code></td>
<td>
<p> Influence value of robsut (Huber's loss) canonical correlation analysis for the  contaminated data </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Md Ashad Alam
&lt;malam@tulane.edu&gt;
</p>


<h3>References</h3>

<p>Md Ashad Alam, Kenji Fukumizu and  Yu-Ping Wang (2018),
Influence Function and Robust Variant of Kernel Canonical Correlation Analysis,
Neurocomputing, Vol. 304 (2018) 12-29.
</p>
<p>Md Ashad Alam, Vince D. Calhoun and Yu-Ping Wang (2018),
 Identifying outliers using multiple kernel canonical
correlation analysis with application to imaging genetics,
Computational Statistics and Data Analysis, Vol. 125, 70- 85
</p>


<h3>See Also</h3>

<p>See also as  <code><a href="#topic+rkcca">rkcca</a></code>, <code><a href="#topic+ifrkcca">ifrkcca</a></code>, <code><a href="#topic+snpfmrimth3D">snpfmrimth3D</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##Dummy data:

n&lt;-100

snpfmridata(n, 0.00001,  10, jth = 1)
</code></pre>

<hr>
<h2 id='snpfmrimth3D'>
An example of imaging genetics and epi-genetics data  to  calcualte influential  observations from three view data
</h2><span id='topic+snpfmrimth3D'></span>

<h3>Description</h3>

<p>#A function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>snpfmrimth3D(n = 500, gamma = 1e-05, ncomps = 1, jth=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="snpfmrimth3D_+3A_n">n</code></td>
<td>

<p>the sample size
</p>
</td></tr>
<tr><td><code id="snpfmrimth3D_+3A_gamma">gamma</code></td>
<td>

<p>the  hyper-parameters
</p>
</td></tr>
<tr><td><code id="snpfmrimth3D_+3A_ncomps">ncomps</code></td>
<td>

<p>the number of canonical vectors
</p>
</td></tr>
<tr><td><code id="snpfmrimth3D_+3A_jth">jth</code></td>
<td>

<p>the influence function of  the jth canonical vector
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>IFim</code></td>
<td>
<p> Influence value of multiple kernel canonical correlation analysis for the  ideal data </p>
</td></tr>
<tr><td><code>IFcm</code></td>
<td>
<p> Influence value of  multiple kernel canonical correlation analysis for the  contaminated data </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Md Ashad Alam
&lt;malam@tulane.edu&gt;
</p>


<h3>References</h3>

<p>Md Ashad Alam, Kenji Fukumizu and  Yu-Ping Wang (2018),
Influence Function and Robust Variant of Kernel Canonical Correlation Analysis,
Neurocomputing, Vol. 304 (2018) 12-29.
</p>
<p>Md Ashad Alam, Vince D. Calhoun and Yu-Ping Wang (2018),
 Identifying outliers using multiple kernel canonical
correlation analysis with application to imaging genetics,
Computational Statistics and Data Analysis, Vol. 125, 70- 85
</p>


<h3>See Also</h3>

<p>See also as <code><a href="#topic+rkcca">rkcca</a></code>, <code><a href="#topic+snpfmridata">snpfmridata</a></code>, <code><a href="#topic+ifrkcca">ifrkcca</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##Dummy data:

n&lt;-100

snpfmrimth3D(n, 0.00001,  10, 1)
</code></pre>

<hr>
<h2 id='udtd'>
A helper function
</h2><span id='topic+udtd'></span>

<h3>Description</h3>

<p>### A  function to  a measure of a system's real  point computing power
</p>


<h3>Usage</h3>

<pre><code class='language-R'> udtd(x)
 </code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="udtd_+3A_x">x</code></td>
<td>

<p>a  real value
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Md Ashad Alam
&lt;malam@tulane.edu&gt;
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
