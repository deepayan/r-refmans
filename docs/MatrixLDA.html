<!DOCTYPE html><html lang="en"><head><title>Help for package MatrixLDA</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {MatrixLDA}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#MatLDA'><p>Fits the <code class="reqn">J</code>-class penalized matrix-normal model for a single pair of tuning parameters.</p></a></li>
<li><a href='#MatLDA_Grid'><p>Fits the <code class="reqn">J</code>-class penalized matrix-normal model for a two-dimensional grid of tuning parameters. Used for tuning parameter selection.</p></a></li>
<li><a href='#MN_MLE'><p>Matrix-normal maximum likelihood estimator</p></a></li>
<li><a href='#PredictMN'><p>Classify matrix-valued data based on a matrix-normal linear discriminant; an object of class &quot;MN&quot;.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Penalized Matrix-Normal Linear Discriminant Analysis</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2</td>
</tr>
<tr>
<td>Date:</td>
<td>2018-08-02</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Aaron J. Molstad &lt;amolstad@fredhutch.org&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Fits the penalized matrix-normal model to be used for linear discriminant analysis with matrix-valued predictors. For a description of the method, see Molstad and Rothman (2018) &lt;<a href="https://doi.org/10.1080%2F10618600.2018.1476249">doi:10.1080/10618600.2018.1476249</a>&gt;. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>URL:</td>
<td>ajmolstad@github.io</td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.12.1), plyr, glasso</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>5.0.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2018-08-02 21:45:36 UTC; aaronmolstad</td>
</tr>
<tr>
<td>Author:</td>
<td>Aaron J. Molstad [aut, cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2018-08-06 21:00:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='MatLDA'>Fits the <code class="reqn">J</code>-class penalized matrix-normal model for a single pair of tuning parameters. </h2><span id='topic+MatLDA'></span>

<h3>Description</h3>

<p>A function for fitting the <code class="reqn">J</code>-class penalized matrix-normal model based on a single set of tuning parameters <code class="reqn">(\lambda_1, \lambda_2)</code>. Returns an object of class &quot;MN&quot;, which can be used for prediction using the <code>PredictMN</code> function. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MatLDA(X, class, lambda1, lambda2, quiet = TRUE, Xval = NULL,
  classval = NULL, k.iter = 100, cov.tol = 1e-05, m.tol = 1e-05,
  full.tol = 1e-06)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="MatLDA_+3A_x">X</code></td>
<td>
<p>An <code class="reqn">r \times c \times N</code> array of training set predictors.</p>
</td></tr>
<tr><td><code id="MatLDA_+3A_class">class</code></td>
<td>
<p><code class="reqn">N</code>-vector of training set class labels; should be numeric from <code class="reqn">\left\{1,...,J\right\}</code>.</p>
</td></tr>
<tr><td><code id="MatLDA_+3A_lambda1">lambda1</code></td>
<td>
<p>A non-negative tuning parameter for the mean penalty.</p>
</td></tr>
<tr><td><code id="MatLDA_+3A_lambda2">lambda2</code></td>
<td>
<p>A non-negative tuning parameter for the Kronecker penalty.</p>
</td></tr>
<tr><td><code id="MatLDA_+3A_quiet">quiet</code></td>
<td>
<p>Logical. Should the objective function value be printed at each update? Default is <code>TRUE</code>. Note that <code>quiet=FALSE</code> will increase computational time.</p>
</td></tr>
<tr><td><code id="MatLDA_+3A_xval">Xval</code></td>
<td>
<p>An <code class="reqn">r \times c \times N_{\rm val}</code> array of validation set predictors. Default is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="MatLDA_+3A_classval">classval</code></td>
<td>
<p><code class="reqn">N_{\rm val}</code>-vector of validation set class labels; should be numeric from <code class="reqn">\left\{1,...,J\right\}</code>. Default is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="MatLDA_+3A_k.iter">k.iter</code></td>
<td>
<p>Maximum number of iterations for full blockwise coordinate descent algorithm.</p>
</td></tr>
<tr><td><code id="MatLDA_+3A_cov.tol">cov.tol</code></td>
<td>
<p>Convergence tolerance for graphical lasso sub-algorithms; passed to <code>glasso</code>. Default is <code class="reqn">1e^{-5}</code>.</p>
</td></tr>
<tr><td><code id="MatLDA_+3A_m.tol">m.tol</code></td>
<td>
<p>Convergence tolerance for mean update alternating minimization algorithm. Default is <code class="reqn">1e^{-5}</code>. It is recommended to track the objective function value using <code>quiet = FALSE</code> and adjust if necessary.</p>
</td></tr>
<tr><td><code id="MatLDA_+3A_full.tol">full.tol</code></td>
<td>
<p>Convergence tolerance for full blockwise coordinate descent algorithm; based on decrease in objective function value. Default is <code class="reqn">1e^{-6}</code>. It is recommended to track the objective function value using <code>quiet = FALSE</code> and adjust if necessary. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns of list of class &quot;MN&quot;, which contains the following elements: 
</p>
<table role = "presentation">
<tr><td><code>Val</code></td>
<td>
<p>The misclassification rate on the validation set, if provided.</p>
</td></tr>
<tr><td><code>Mean</code></td>
<td>
<p><code class="reqn">\hat{M}</code>; an <code class="reqn">r \times c \times J</code> array of estimated class means. </p>
</td></tr>
<tr><td><code>U</code></td>
<td>
<p><code class="reqn">\hat{U}</code>; the <code class="reqn">r \times r</code> estimated precision matrix for the row variables.</p>
</td></tr>
<tr><td><code>V</code></td>
<td>
<p><code class="reqn">\hat{V}</code>; the <code class="reqn">c \times c</code> estimated precision matrix for the column variables.</p>
</td></tr>
<tr><td><code>pi.list</code></td>
<td>
<p><code class="reqn">\hat{\pi}</code>; <code class="reqn">J</code>-vector with marginal class probabilities from training set.</p>
</td></tr>
</table>


<h3>References</h3>


<ul>
<li><p> Molstad, A. J., and Rothman, A. J. (2018). A penalized likelihood method for classification with matrix-valued predictors. <em>Journal of Computational and Graphical Statistics</em>.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## Generate realizations of matrix-normal random variables
## set sample size, dimensionality, number of classes, 
## and marginal class probabilities

N = 75
N.test = 150
N.val = 75

N.total = N + N.test + N.val

r = 16
p = 16
C = 3

pi.list = rep(1/C, C)

## create class means
M.array = array(0, dim=c(r, p, C))
M.array[3:4, 3:4, 1] = 1
M.array[5:6, 5:6, 2] = .5
M.array[3:4, 3:4, 3] = -2
M.array[5:6, 5:6, 3] = -.5


## create covariance matrices U and V
Uinv = matrix(0, nrow=r, ncol=r)
for (i in 1:r) {
	for (j in 1:r) {
		Uinv[i,j] = .5^abs(i-j)
	}
}

eoU = eigen(Uinv)
Uinv.sqrt = tcrossprod(tcrossprod(eoU$vec, 
diag(eoU$val^(1/2))),eoU$vec)

Vinv = matrix(.5, nrow=p, ncol=p)
diag(Vinv) = 1 
eoV = eigen(Vinv)
Vinv.sqrt = tcrossprod(tcrossprod(eoV$vec, 
diag(eoV$val^(1/2))),eoV$vec)

## generate N.total realizations of matrix-variate normal data
set.seed(10)
dat.array = array(0, dim=c(r,p,N.total))
	class.total = numeric(length=N.total)
	for(jj in 1:N.total){
		class.total[jj] = sample(1:C, 1, prob=pi.list)
		dat.array[,,jj] = tcrossprod(crossprod(Uinv.sqrt, 
		matrix(rnorm(r*p), nrow=r)),
		Vinv.sqrt) + M.array[,,class.total[jj]]
	}

## store generated data 
X = dat.array[,,1:N]
X.val = dat.array[,,(N+1):(N+N.val)]
X.test = dat.array[,,(N+N.val+1):N.total]

class = class.total[1:N]
class.val = class.total[(N+1):(N+N.val)]
class.test = class.total[(N+N.val+1):N.total]

## fit two-dimensional grid of tuning parameters; 
## measure classification accuracy on validation set
lambda1 = c(2^seq(-5, 0, by=1))
lambda2 = c(2^seq(-8, -4, by=1))
fit.grid = MatLDA_Grid(X=X, class=class, lambda1=lambda1, 
	lambda2=lambda2, quiet=TRUE,
	Xval=X.val, classval= class.val,
	k.iter = 100, cov.tol=1e-5, m.tol=1e-5, full.tol=1e-6)

## identify minimum misclassification proportion; 
## select tuning parameters corresponding to 
## smallest model at minimum misclassification proportion
CV.mat = fit.grid$Val.mat
G.mat = fit.grid$G.mat*(CV.mat==min(CV.mat))
ind1 = (which(G.mat==max(G.mat), arr.ind=TRUE))[,2]
ind2 = (which(G.mat==max(G.mat), arr.ind=TRUE))[,1]
out = unique(ind2[which(ind2==max(ind2))])
lambda1.cv = lambda1[out]
out2 = unique(max(ind1[ind2==out]))
lambda2.cv = lambda2[out2]

## refit model with sinlge tuning parameter pair
out = MatLDA(X=X, class=class, lambda1=lambda1.cv, 
	lambda2=lambda2.cv, quiet=FALSE,
	Xval=X.test, classval= class.test,
	k.iter = 100, cov.tol=1e-5, m.tol=1e-5, full.tol=1e-6)

## print misclassification proportion on test set 
out$Val

## print images of estimated mean differences
dev.new(width=10, height=3)
par(mfrow=c(1,3))
image(t(abs(out$M[,,1] - out$M[,,2]))[,r:1], 
main=expression(paste("|", hat(mu)[1], "-", hat(mu)[2], "|")),
 col = grey(seq(1, 0, length = 100)))
image(t(abs(out$M[,,1] - out$M[,,3]))[,r:1], 
main=expression(paste("|", hat(mu)[1], "-", hat(mu)[3], "|")),
 col = grey(seq(1, 0, length = 100)))
image(t(abs(out$M[,,2] - out$M[,,3]))[,r:1], 
main=expression(paste("|", hat(mu)[2], "-", hat(mu)[3], "|")),
 col = grey(seq(1, 0, length = 100)))

</code></pre>

<hr>
<h2 id='MatLDA_Grid'>Fits the <code class="reqn">J</code>-class penalized matrix-normal model for a two-dimensional grid of tuning parameters. Used for tuning parameter selection. </h2><span id='topic+MatLDA_Grid'></span>

<h3>Description</h3>

<p>A function for fitting the penalized matrix normal model for a two-dimensional grid of tuning parameters. Meant to be used with a validation set to select tuning parameters. Can also be used inside a <code class="reqn">k</code>-fold cross-validation function where the training set is the data outside the <code class="reqn">k</code>th fold and the validation set is comprised of the <code class="reqn">k</code>th fold sample data. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MatLDA_Grid(X, class, lambda1, lambda2, quiet = TRUE, Xval = NULL,
  classval = NULL, k.iter = 100, cov.tol = 1e-05, m.tol = 1e-05,
  full.tol = 1e-06)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="MatLDA_Grid_+3A_x">X</code></td>
<td>
<p>An <code class="reqn">r \times c \times N</code> array of training set predictors.</p>
</td></tr>
<tr><td><code id="MatLDA_Grid_+3A_class">class</code></td>
<td>
<p><code class="reqn">N</code>-vector of training set class labels; should be numeric from <code class="reqn">\left\{1,...,J\right\}</code>.</p>
</td></tr>
<tr><td><code id="MatLDA_Grid_+3A_lambda1">lambda1</code></td>
<td>
<p>A vector of non-negative candidate tuning parameters for the mean penalty.</p>
</td></tr>
<tr><td><code id="MatLDA_Grid_+3A_lambda2">lambda2</code></td>
<td>
<p>A vector of non-negative candidate tuning parameters for the Kronecker penalty.</p>
</td></tr>
<tr><td><code id="MatLDA_Grid_+3A_quiet">quiet</code></td>
<td>
<p>Logical. Should the objective function value be printed at each update? Default is <code>TRUE</code>. Note that <code>quiet=FALSE</code> will increase computational time.</p>
</td></tr>
<tr><td><code id="MatLDA_Grid_+3A_xval">Xval</code></td>
<td>
<p>An <code class="reqn">r \times c \times N_{\rm val}</code> array of validation set predictors. Default is <code>NULL</code>. </p>
</td></tr>
<tr><td><code id="MatLDA_Grid_+3A_classval">classval</code></td>
<td>
<p><code class="reqn">N_{\rm val}</code>-vector of validation set class labels; should be numeric from <code class="reqn">\left\{1,...,J\right\}</code>.Default is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="MatLDA_Grid_+3A_k.iter">k.iter</code></td>
<td>
<p>Maximum number of iterations for full blockwise coordinate descent algorithm. Default is 100.</p>
</td></tr>
<tr><td><code id="MatLDA_Grid_+3A_cov.tol">cov.tol</code></td>
<td>
<p>Convergence tolerance for graphical lasso subalgorithms; passed to <code>glasso</code>. Default is <code class="reqn">1e^{-5}</code>.</p>
</td></tr>
<tr><td><code id="MatLDA_Grid_+3A_m.tol">m.tol</code></td>
<td>
<p>Convergence tolerance for mean update alternating minimization algorithm. Default is <code class="reqn">1e^{-5}</code>. It is recommended to track the objective function value using <code>quiet = FALSE</code> and adjust tolerance if necessary.</p>
</td></tr>
<tr><td><code id="MatLDA_Grid_+3A_full.tol">full.tol</code></td>
<td>
<p>Convergence tolerance for full blockwise coordinate descent algorithm; based on decrease in objective function value. Default is <code class="reqn">1e^{-6}</code>. It is recommended to track the objective function value using <code>quiet = FALSE</code> and adjust tolerance if necessary. </p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>Val.mat</code></td>
<td>
<p>A matrix of dimension <code>length</code>(lambda1) <code class="reqn">\times</code><code>length</code>(lambda2) with validation set misclassification propotions.</p>
</td></tr>
<tr><td><code>G.mat</code></td>
<td>
<p>A matrix of dimension <code>length</code>(lambda1) <code class="reqn">\times</code><code>length</code>(lambda2) with the number of pairwise mean differences that are zero, i.e., a larger entry corresponds to a more sparse model.</p>
</td></tr>
<tr><td><code>U.mat</code></td>
<td>
<p>A matrix of dimension <code>length</code>(lambda1) <code class="reqn">\times</code><code>length</code>(lambda2) with the number of zeros in <code class="reqn">\hat{U}</code>.</p>
</td></tr>
<tr><td><code>V.mat</code></td>
<td>
<p>A matrix of dimension <code>length</code>(lambda1) <code class="reqn">\times</code><code>length</code>(lambda2) with the number of zeros in <code class="reqn">\hat{V}</code>.</p>
</td></tr>
</table>


<h3>References</h3>


<ul>
<li><p> Molstad, A. J., and Rothman, A. J. (2018). A penalized likelihood method for classification with matrix-valued predictors. <em>Journal of Computational and Graphical Statistics</em>.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
## Generate realizations of matrix-normal random variables
## set sample size, dimensionality, number of classes, 
## and marginal class probabilities

N = 75
N.test = 150
N.val = 75

N.total = N + N.test + N.val

r = 16
p = 16
C = 3

pi.list = rep(1/C, C)

## create class means
M.array = array(0, dim=c(r, p, C))
M.array[3:4, 3:4, 1] = 1
M.array[5:6, 5:6, 2] = .5
M.array[3:4, 3:4, 3] = -2
M.array[5:6, 5:6, 3] = -.5


## create covariance matrices U and V
Uinv = matrix(0, nrow=r, ncol=r)
for (i in 1:r) {
	for (j in 1:r) {
		Uinv[i,j] = .5^abs(i-j)
	}
}

eoU = eigen(Uinv)
Uinv.sqrt = tcrossprod(tcrossprod(eoU$vec, 
diag(eoU$val^(1/2))),eoU$vec)

Vinv = matrix(.5, nrow=p, ncol=p)
diag(Vinv) = 1 
eoV = eigen(Vinv)
Vinv.sqrt = tcrossprod(tcrossprod(eoV$vec, 
diag(eoV$val^(1/2))),eoV$vec)

## generate N.total realizations of matrix-variate normal data
set.seed(10)
dat.array = array(0, dim=c(r,p,N.total))
class.total = numeric(length=N.total)
for(jj in 1:N.total){
	class.total[jj] = sample(1:C, 1, prob=pi.list)
	dat.array[,,jj] = tcrossprod(crossprod(Uinv.sqrt, 
	matrix(rnorm(r*p), nrow=r)),
	Vinv.sqrt) + M.array[,,class.total[jj]]
}

## store generated data 
X = dat.array[,,1:N]
X.val = dat.array[,,(N+1):(N+N.val)]
X.test = dat.array[,,(N+N.val+1):N.total]

class = class.total[1:N]
class.val = class.total[(N+1):(N+N.val)]
class.test = class.total[(N+N.val+1):N.total]

## fit two-dimensional grid of tuning parameters; 
## measure classification accuracy on validation set
lambda1 = c(2^seq(-5, 0, by=1))
lambda2 = c(2^seq(-8, -4, by=1))
fit.grid = MatLDA_Grid(X=X, class=class, lambda1=lambda1, 
	lambda2=lambda2, quiet=TRUE,
	Xval=X.val, classval= class.val,
	k.iter = 100, cov.tol=1e-5, m.tol=1e-5, full.tol=1e-6)

## identify minimum misclassification proportion; 
## select tuning parameters corresponding to 
## smallest model at minimum misclassification proportion
CV.mat = fit.grid$Val.mat
G.mat = fit.grid$G.mat*(CV.mat==min(CV.mat))
ind1 = (which(G.mat==max(G.mat), arr.ind=TRUE))[,2]
ind2 = (which(G.mat==max(G.mat), arr.ind=TRUE))[,1]
out = unique(ind2[which(ind2==max(ind2))])
lambda1.cv = lambda1[out]
out2 = unique(max(ind1[ind2==out]))
lambda2.cv = lambda2[out2]

## refit model with sinlge tuning parameter pair
out = MatLDA(X=X, class=class, lambda1=lambda1.cv, 
	lambda2=lambda2.cv, quiet=FALSE,
	Xval=X.test, classval= class.test,
	k.iter = 100, cov.tol=1e-5, m.tol=1e-5, full.tol=1e-6)

## print misclassification proportion on test set 
out$Val

## print images of estimated mean differences
dev.new(width=10, height=3)
par(mfrow=c(1,3))
image(t(abs(out$M[,,1] - out$M[,,2]))[,r:1], 
main=expression(paste("|", hat(mu)[1], "-", hat(mu)[2], "|")), 
col = grey(seq(1, 0, length = 100)))
image(t(abs(out$M[,,1] - out$M[,,3]))[,r:1], 
main=expression(paste("|", hat(mu)[1], "-", hat(mu)[3], "|")), 
col = grey(seq(1, 0, length = 100)))
image(t(abs(out$M[,,2] - out$M[,,3]))[,r:1], 
main=expression(paste("|", hat(mu)[2], "-", hat(mu)[3], "|")), 
col = grey(seq(1, 0, length = 100)))

</code></pre>

<hr>
<h2 id='MN_MLE'>Matrix-normal maximum likelihood estimator</h2><span id='topic+MN_MLE'></span>

<h3>Description</h3>

<p>A function for fitting the <code class="reqn">J</code>-class matrix-normal model using maximum likelihood. Uses the so-called &ldquo;flip-flop&rdquo; algorithm after initializing at <code class="reqn">U = I_r</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MN_MLE(X, class, max.iter = 1000, tol = 1e-06, quiet = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="MN_MLE_+3A_x">X</code></td>
<td>
<p>An <code class="reqn">r \times c \times N</code> array of training set predictors.</p>
</td></tr>
<tr><td><code id="MN_MLE_+3A_class">class</code></td>
<td>
<p><code class="reqn">N</code>-vector of training set class labels; should be numeric from <code class="reqn">\left\{1,...,J\right\}</code>.</p>
</td></tr>
<tr><td><code id="MN_MLE_+3A_max.iter">max.iter</code></td>
<td>
<p>Maximum number of iterations for &ldquo;flip-flop&rdquo; algorithm.</p>
</td></tr>
<tr><td><code id="MN_MLE_+3A_tol">tol</code></td>
<td>
<p>Convergence tolerance for the &ldquo;flip flop&rdquo; algorithm; based on decrease in negative log-likelihood.</p>
</td></tr>
<tr><td><code id="MN_MLE_+3A_quiet">quiet</code></td>
<td>
<p>Logical. Should the objective function value be printed at each update? Default is <code>TRUE</code>. Note that <code>quiet=FALSE</code> will increase computational time.</p>
</td></tr></table>


<h3>Value</h3>

<p>Returns of list of class &quot;MN&quot;, which contains the following elements: 
</p>
<table role = "presentation">
<tr><td><code>Mean</code></td>
<td>
<p><code class="reqn">\bar{X}</code>; An <code class="reqn">r \times c \times C</code> array of sample class means.</p>
</td></tr>
<tr><td><code>U</code></td>
<td>
<p><code class="reqn">\hat{U}^{\rm MLE}</code>;  the <code class="reqn">r \times r</code> estimated precision matrix for the row variables.</p>
</td></tr>
<tr><td><code>V</code></td>
<td>
<p><code class="reqn">\hat{V}^{\rm MLE}</code>;  the <code class="reqn">c \times c</code> estimated precision matrix for the column variables.</p>
</td></tr>
<tr><td><code>pi.list</code></td>
<td>
<p><code class="reqn">\hat{\pi}</code>; <code class="reqn">J</code>-vector with marginal class probabilities from training set.</p>
</td></tr>
</table>


<h3>References</h3>


<ul>
<li><p> Molstad, A. J., and Rothman, A. J. (2018). A penalized likelihood method for classification with matrix-valued predictors. <em>Journal of Computational and Graphical Statistics</em>.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## Generate realizations of matrix-normal random variables
## set sample size, dimensionality, number of classes, 
## and marginal class probabilities

N = 75
N.test = 150

N.total = N + N.test

r = 16
p = 16
C = 3

pi.list = rep(1/C, C)

## create class means
M.array = array(0, dim=c(r, p, C))
M.array[3:4, 3:4, 1] = 1
M.array[5:6, 5:6, 2] = .5
M.array[3:4, 3:4, 3] = -2
M.array[5:6, 5:6, 3] = -.5


## create covariance matrices U and V
Uinv = matrix(0, nrow=r, ncol=r)
for (i in 1:r) {
	for (j in 1:r) {
		Uinv[i,j] = .5^abs(i-j)
	}
}

eoU = eigen(Uinv)
Uinv.sqrt = tcrossprod(tcrossprod(eoU$vec, 
diag(eoU$val^(1/2))),eoU$vec)

Vinv = matrix(.5, nrow=p, ncol=p)
diag(Vinv) = 1 
eoV = eigen(Vinv)
Vinv.sqrt = tcrossprod(tcrossprod(eoV$vec, 
diag(eoV$val^(1/2))),eoV$vec)

## generate N.total realizations of matrix-variate normal data
set.seed(1)
X = array(0, dim=c(r,p,N.total))
	class = numeric(length=N.total)
	for(jj in 1:N.total){
		class[jj] = sample(1:C, 1, prob=pi.list)
		X[,,jj] = tcrossprod(crossprod(Uinv.sqrt, 
		matrix(rnorm(r*p), nrow=r)),
		Vinv.sqrt) + M.array[,,class[jj]]
	}

## fit matrix-normal model using maximum likelihood
out = MN_MLE(X = X, class = class)
</code></pre>

<hr>
<h2 id='PredictMN'>Classify matrix-valued data based on a matrix-normal linear discriminant; an object of class &quot;MN&quot;. </h2><span id='topic+PredictMN'></span>

<h3>Description</h3>

<p>A function for prediction based on an object of class &quot;MN&quot;; models fit by <code>MatLDA</code> or <code>MN_MLE</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PredictMN(object, newdata, newclass = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="PredictMN_+3A_object">object</code></td>
<td>
<p>An object of class &quot;MN&quot;; output from <code>MatLDA</code> or <code>MN_MLE</code>. </p>
</td></tr>
<tr><td><code id="PredictMN_+3A_newdata">newdata</code></td>
<td>
<p>New data to be classified; an <code class="reqn">r \times c \times N_{\rm test}</code> array.</p>
</td></tr>
<tr><td><code id="PredictMN_+3A_newclass">newclass</code></td>
<td>
<p>Class labels for new data; should be <code class="reqn">in \left\{1, \dots, J\right\}</code>. Default is <code>NULL</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>pred.class</code></td>
<td>
<p>An <code class="reqn">N_{\rm test}</code>-vector of predicted class membership based on the inputed object.</p>
</td></tr>
<tr><td><code>misclass</code></td>
<td>
<p>If <code>newclass</code> is non-<code>NULL</code>, returns the misclassification proportion on the test data set.</p>
</td></tr>
<tr><td><code>prob.mat</code></td>
<td>
<p>A <code class="reqn">N_{\rm test} \times J</code> matrix with the value of discriminant evaluated at each test data point.</p>
</td></tr>
</table>


<h3>References</h3>


<ul>
<li><p> Molstad, A. J., and Rothman, A. J. (2018). A penalized likelihood method for classification with matrix-valued predictors. <em>Journal of Computational and Graphical Statistics</em>.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## Generate realizations of matrix-normal random variables
## set sample size, dimensionality, number of classes, 
## and marginal class probabilities

N = 75
N.test = 150

N.total = N + N.test

r = 16
p = 16
C = 3

pi.list = rep(1/C, C)

## create class means
M.array = array(0, dim=c(r, p, C))
M.array[3:4, 3:4, 1] = 1
M.array[5:6, 5:6, 2] = .5
M.array[3:4, 3:4, 3] = -2
M.array[5:6, 5:6, 3] = -.5


## create covariance matrices U and V
Uinv = matrix(0, nrow=r, ncol=r)
for (i in 1:r) {
	for (j in 1:r) {
		Uinv[i,j] = .5^abs(i-j)
	}
}


eoU = eigen(Uinv)
Uinv.sqrt = tcrossprod(tcrossprod(eoU$vec, diag(eoU$val^(1/2))),eoU$vec)

Vinv = matrix(.5, nrow=p, ncol=p)
diag(Vinv) = 1 
eoV = eigen(Vinv)
Vinv.sqrt = tcrossprod(tcrossprod(eoV$vec, diag(eoV$val^(1/2))),eoV$vec)

## generate N.total realizations of matrix-variate normal data
set.seed(1)
dat.array = array(0, dim=c(r,p,N.total))
class.total = numeric(length=N.total)
for(jj in 1:N.total){
	class.total[jj] = sample(1:C, 1, prob=pi.list)
	dat.array[,,jj] = tcrossprod(crossprod(Uinv.sqrt, matrix(rnorm(r*p), nrow=r)),
	Vinv.sqrt) + M.array[,,class.total[jj]]
}

## store generated data 
X = dat.array[,,1:N]
X.test = dat.array[,,(N+1):N.total]

class = class.total[1:N]
class.test = class.total[(N+1):N.total]

## fit matrix-normal model using maximum likelihood
out = MN_MLE(X = X, class = class)

## use output to classify test set
check = PredictMN(out, newdata = X.test, newclass = class.test)

## print misclassification proportion
check$misclass
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
