<!DOCTYPE html><html lang="en"><head><title>Help for package stepgbm</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {stepgbm}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#cran-comments'>
<p>Note on notes</p></a></li>
<li><a href='#stepgbm'><p>Select predictive variables for generalized boosted regression modeling (gbm)</p>
by various variable influence methods and predictive accuracy in a stepwise algorithm</a></li>
<li><a href='#stepgbmRVI'><p>Select predictive variables for generalized boosted regression modeling (gbm)</p>
by relative variable influence (rvi) and accuracy in a stepwise algorithm</a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Stepwise Variable Selection for Generalized Boosted Regression
Modeling</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-04-03</td>
</tr>
<tr>
<td>Description:</td>
<td>An introduction to a couple of novel predictive variable selection methods for generalised boosted regression modeling (gbm). They are based on various variable influence methods (i.e., relative variable influence (RVI) and knowledge informed RVI (i.e., KIRVI, and KIRVI2)) that adopted similar ideas as AVI, KIAVI and KIAVI2 in the 'steprf' package, and also based on predictive accuracy in stepwise algorithms. For details of the variable selection methods, please see: Li, J., Siwabessy, J., Huang, Z. and Nichol, S. (2019) &lt;<a href="https://doi.org/10.3390%2Fgeosciences9040180">doi:10.3390/geosciences9040180</a>&gt;. Li, J., Alvarez, B., Siwabessy, J., Tran, M., Huang, Z., Przeslawski, R., Radke, L., Howard, F., Nichol, S. (2017). &lt;<a href="https://doi.org/10.13140%2FRG.2.2.27686.22085">doi:10.13140/RG.2.2.27686.22085</a>&gt;.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>spm, steprf</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, reshape2, lattice</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-04-03 06:02:06 UTC; Jin</td>
</tr>
<tr>
<td>Author:</td>
<td>Jin Li [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jin Li &lt;jinli68@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-04-04 08:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='cran-comments'>
Note on notes
</h2><span id='topic+cran-comments'></span>

<h3>Description</h3>

<p>This is an updated and extended version of &lsquo;spm' package. The change in package name from 'spm' to 'spm2' is due to the change in Author&rsquo;s support from Geoscience Australia to Data2Action Australia.
</p>
<p>## R CMD check results
0 errors | 0 warnings | 0 notes
</p>


<h3>Author(s)</h3>

<p>Jin Li
</p>

<hr>
<h2 id='stepgbm'>Select predictive variables for generalized boosted regression modeling (gbm)
by various variable influence methods and predictive accuracy in a stepwise algorithm</h2><span id='topic+stepgbm'></span>

<h3>Description</h3>

<p>This function is to select predictive variables for
generalized boosted regression modeling (gbm) based on various variable influence
methods (i.e., relative variable influence (RVI) and knowledge informed RVI
(i.e., KIRVI, and KIRVI2)) and predictive accuracy. It is implemented via the functions
'stepgbmRVI' and 'steprf::steprfAVIPredictors'. It should be noted that this function
can be slow, may taking several minutes, hours or even days depending on the
number of the predictive variables to be selected and the specifications of
relevant arguments.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stepgbm(
  trainx,
  trainy,
  method = "KIRVI",
  var.monotone = rep(0, ncol(trainx)),
  family = "gaussian",
  n.trees = 3000,
  learning.rate = 0.001,
  interaction.depth = 2,
  bag.fraction = 0.5,
  train.fraction = 1,
  n.minobsinnode = 10,
  cv.fold = 10,
  weights = rep(1, nrow(trainx)),
  keep.data = FALSE,
  verbose = TRUE,
  n.cores = 6,
  rpt = 2,
  predacc = "VEcv",
  min.n.var = 2,
  delta.predacc = 0.001,
  rseed = 1234,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="stepgbm_+3A_trainx">trainx</code></td>
<td>
<p>a dataframe or matrix contains columns of predictive variables.</p>
</td></tr>
<tr><td><code id="stepgbm_+3A_trainy">trainy</code></td>
<td>
<p>a vector of response, must have length equal to the number of
rows in trainx.</p>
</td></tr>
<tr><td><code id="stepgbm_+3A_method">method</code></td>
<td>
<p>a variable selection method for 'GBM'; can be: &quot;RVI&quot;, &quot;KIRVI&quot;
and &quot;KIRVI2&quot;. If &quot;RVI&quot; is used, it would produce the same results as
'stepgbmRVI'. By default, &quot;KIRVI&quot; is used.</p>
</td></tr>
<tr><td><code id="stepgbm_+3A_var.monotone">var.monotone</code></td>
<td>
<p>an optional vector, the same length as the number of
predictors, indicating which variables have a monotone increasing (+1),
decreasing (-1), or arbitrary (0) relationship with the outcome. By default,
a vector of 0 is used.</p>
</td></tr>
<tr><td><code id="stepgbm_+3A_family">family</code></td>
<td>
<p>either a character string specifying the name of the distribution to
use or a list with a component name specifying the distribution and any
additional parameters needed. See gbm for details. By default, &quot;gaussian&quot; is used.</p>
</td></tr>
<tr><td><code id="stepgbm_+3A_n.trees">n.trees</code></td>
<td>
<p>the total number of trees to fit. This is equivalent to the
number of iterations and the number of basis functions in the additive
expansion. By default, 3000 is used.</p>
</td></tr>
<tr><td><code id="stepgbm_+3A_learning.rate">learning.rate</code></td>
<td>
<p>a shrinkage parameter applied to each tree in the
expansion. Also known as step-size reduction. By default, 0.001 is used.</p>
</td></tr>
<tr><td><code id="stepgbm_+3A_interaction.depth">interaction.depth</code></td>
<td>
<p>the maximum depth of variable interactions.
1 implies an additive model, 2 implies a model with up to 2-way
interactions, etc. By default, 2 is used.</p>
</td></tr>
<tr><td><code id="stepgbm_+3A_bag.fraction">bag.fraction</code></td>
<td>
<p>the fraction of the training set observations randomly
selected to propose the next tree in the expansion. By default, 0.5 is used.</p>
</td></tr>
<tr><td><code id="stepgbm_+3A_train.fraction">train.fraction</code></td>
<td>
<p>The first train.fraction * nrows(data) observations
are used to fit the gbm and the remainder are used for computing
out-of-sample estimates of the loss function.</p>
</td></tr>
<tr><td><code id="stepgbm_+3A_n.minobsinnode">n.minobsinnode</code></td>
<td>
<p>minimum number of observations in the trees terminal
nodes. Note that this is the actual number of observations not the total
weight. By default, 10 is used.</p>
</td></tr>
<tr><td><code id="stepgbm_+3A_cv.fold">cv.fold</code></td>
<td>
<p>integer; number of cross-validation folds to perform within
gbm. if &gt; 1, then apply n-fold cross validation; the default is 10, i.e.,
10-fold cross validation that is recommended.</p>
</td></tr>
<tr><td><code id="stepgbm_+3A_weights">weights</code></td>
<td>
<p>an optional vector of weights to be used in the fitting
process. Must be positive but do not need to be normalized.
If keep.data = FALSE in the initial call to gbm then it is the user's
responsibility to resupply the weights to gbm.more. By default, a vector of
1 is used.</p>
</td></tr>
<tr><td><code id="stepgbm_+3A_keep.data">keep.data</code></td>
<td>
<p>a logical variable indicating whether to keep the data and
an index of the data stored with the object. Keeping the data and index
makes subsequent calls to gbm.more faster at the cost of storing an extra
copy of the dataset. By default, 'FALSE' is used.</p>
</td></tr>
<tr><td><code id="stepgbm_+3A_verbose">verbose</code></td>
<td>
<p>If TRUE, gbm will print out progress and performance
indicators. By default, 'TRUE' is used.</p>
</td></tr>
<tr><td><code id="stepgbm_+3A_n.cores">n.cores</code></td>
<td>
<p>The number of CPU cores to use. See gbm for details. By
default, 6 is used.</p>
</td></tr>
<tr><td><code id="stepgbm_+3A_rpt">rpt</code></td>
<td>
<p>iteration of cross validation.</p>
</td></tr>
<tr><td><code id="stepgbm_+3A_predacc">predacc</code></td>
<td>
<p>&quot;VEcv&quot; for vecv in function pred.acc.</p>
</td></tr>
<tr><td><code id="stepgbm_+3A_min.n.var">min.n.var</code></td>
<td>
<p>minimum number of predictive variables remained in the final
predictive model the default is 1.</p>
</td></tr>
<tr><td><code id="stepgbm_+3A_delta.predacc">delta.predacc</code></td>
<td>
<p>minimum changes between the accuracy of two consecutive
predictive models. By default, 0.01 is used.</p>
</td></tr>
<tr><td><code id="stepgbm_+3A_rseed">rseed</code></td>
<td>
<p>random seed. By default, 1234 is used.</p>
</td></tr>
<tr><td><code id="stepgbm_+3A_...">...</code></td>
<td>
<p>other arguments passed on to gbm.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following components: 1) stepgbmPredictorsFinal: the
variables selected for the last GBM model, whether it is of the highest
predictive accuracy need to be confirmed using 'max.predictive.accuracy'
that is listed next; 2) max.predictive.accuracy: the predictive accuracy
of the most accurate GBM model for each run of 'stepgbmRVI', which can be used
to confirm the model with the highest accuracy, 3) numberruns: number of runs
of 'stepgbmRVI'; 4) laststepRVI: the outpouts of last run of 'stepgbmRVI'; 5)
stepgbmRVIOutputsAll: the outpouts of all 'stepgbmRVI' produced during the
variable selection process; 6) stepgbmPredictorsAll: the outpouts of
'stepgbmRVIPredictors' for all 'stepgbmRVI' produced during the variable
selection process; 7) KIRVIPredictorsAll: predictors used for all
'stepgbmRVI' produced during the variable selection process; for a method
&quot;RVI&quot;, if the variables are different from those in the traning dataset,
it suggests that these variables should be tested if the predictive
accuracy can be further improved.
</p>


<h3>Author(s)</h3>

<p>Jin Li
</p>


<h3>References</h3>

<p>Li, J. (2022). Spatial Predictive Modeling with R. Boca Raton,
Chapman and Hall/CRC.
</p>
<p>Li, J., Siwabessy, J., Huang, Z., Nichol, S. (2019). &quot;Developing
an optimal spatial predictive model for seabed sand content using machine
learning, geostatistics and their hybrid methods.&quot; Geosciences 9 (4):180.
</p>
<p>Li, J., Alvarez, B., Siwabessy, J., Tran, M., Huang, Z., Przeslawski, R.,
Radke, L., Howard, F., Nichol, S. (2017). &quot;Application of random forest,
generalised linear model and their hybrid methods with geostatistical
techniques to count data: Predicting sponge species richness.&quot;
Environmental Modelling &amp; Software 97: 112-129.
</p>
<p>Li, J., Alvarez, B., Siwabessy, J., Tran, M., Huang, Z., Przeslawski, R.,
Radke, L., Howard, F., Nichol, S. (2017). Selecting predictors to form the
most accurate predictive model for count data. International Congress on
Modelling and Simulation (MODSIM) 2017, Hobart.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(spm)

data(petrel)

stepgbm1 &lt;- stepgbm(trainx = petrel[, c(1,2, 6:9)], trainy =
log(petrel[, 5] + 1), method = "KIRVI", family = "gaussian", rpt = 2,
predacc = "VEcv", cv.fold = 5,  min.n.var = 2, n.cores = 2,
delta.predacc = 0.01, rseed = 1234)
names(stepgbm1)
stepgbm1$stepgbmPredictorsFinal$variables.most.accurate
stepgbm1$max.predictive.accuracy
stepgbm1$stepgbmPredictorsAll[[1]]

# The variables selected can be derived with
stepgbm1$stepgbmPredictorsAll[[1]]$variables.most.accurate

data(sponge)
stepgbm2 &lt;- stepgbm(trainx = sponge[, -3], trainy = sponge[, 3], method = "KIRVI",
family = "poisson", rpt = 2, cv.fold = 5, predacc = "VEcv", min.n.var = 2,
n.cores = 2, delta.predacc = 0.01, rseed = 1234)
stepgbm2
stepgbm2$max.predictive.accuracy

# The variables selected can be derived with
stepgbm2$stepgbmPredictorsAll[[1]]$variables.most.accurate


</code></pre>

<hr>
<h2 id='stepgbmRVI'>Select predictive variables for generalized boosted regression modeling (gbm)
by relative variable influence (rvi) and accuracy in a stepwise algorithm</h2><span id='topic+stepgbmRVI'></span>

<h3>Description</h3>

<p>This function is to select predictive variables for generalized
boosted regression modeling (gbm) by their relative variable influence that is
calculated for each model after excluding the least influence variable, and
corresponding predictive accuracy. It is also developed for 'stepgbm' function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stepgbmRVI(
  trainx,
  trainy,
  var.monotone = rep(0, ncol(trainx)),
  family = "gaussian",
  n.trees = 3000,
  learning.rate = 0.001,
  interaction.depth = 2,
  bag.fraction = 0.5,
  train.fraction = 1,
  n.minobsinnode = 10,
  cv.fold = 10,
  weights = rep(1, nrow(trainx)),
  keep.data = FALSE,
  verbose = TRUE,
  n.cores = 6,
  rpt = 2,
  predacc = "VEcv",
  min.n.var = 2,
  rseed = 1234,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="stepgbmRVI_+3A_trainx">trainx</code></td>
<td>
<p>a dataframe or matrix contains columns of predictive variables.</p>
</td></tr>
<tr><td><code id="stepgbmRVI_+3A_trainy">trainy</code></td>
<td>
<p>a vector of response, must have length equal to the number of
rows in trainx.</p>
</td></tr>
<tr><td><code id="stepgbmRVI_+3A_var.monotone">var.monotone</code></td>
<td>
<p>an optional vector, the same length as the number of
predictors, indicating which variables have a monotone increasing (+1),
decreasing (-1), or arbitrary (0) relationship with the outcome. By default,
a vector of 0 is used.</p>
</td></tr>
<tr><td><code id="stepgbmRVI_+3A_family">family</code></td>
<td>
<p>either a character string specifying the name of the distribution to
use or a list with a component name specifying the distribution and any
additional parameters needed. See gbm for details. By default, &quot;gaussian&quot; is used.</p>
</td></tr>
<tr><td><code id="stepgbmRVI_+3A_n.trees">n.trees</code></td>
<td>
<p>the total number of trees to fit. This is equivalent to the
number of iterations and the number of basis functions in the additive
expansion. By default, 3000 is used.</p>
</td></tr>
<tr><td><code id="stepgbmRVI_+3A_learning.rate">learning.rate</code></td>
<td>
<p>a shrinkage parameter applied to each tree in the
expansion. Also known as step-size reduction.</p>
</td></tr>
<tr><td><code id="stepgbmRVI_+3A_interaction.depth">interaction.depth</code></td>
<td>
<p>the maximum depth of variable interactions.
1 implies an additive model, 2 implies a model with up to 2-way
interactions, etc. By default, 2 is used.</p>
</td></tr>
<tr><td><code id="stepgbmRVI_+3A_bag.fraction">bag.fraction</code></td>
<td>
<p>the fraction of the training set observations randomly
selected to propose the next tree in the expansion. By default, 0.5 is used.</p>
</td></tr>
<tr><td><code id="stepgbmRVI_+3A_train.fraction">train.fraction</code></td>
<td>
<p>The first train.fraction * nrows(data) observations
are used to fit the gbm and the remainder are used for computing
out-of-sample estimates of the loss function.</p>
</td></tr>
<tr><td><code id="stepgbmRVI_+3A_n.minobsinnode">n.minobsinnode</code></td>
<td>
<p>minimum number of observations in the trees terminal
nodes. Note that this is the actual number of observations not the total
weight. By default, 10 is used.</p>
</td></tr>
<tr><td><code id="stepgbmRVI_+3A_cv.fold">cv.fold</code></td>
<td>
<p>integer; number of cross-validation folds to perform within
gbm. if &gt; 1, then apply n-fold cross validation; the default is 10, i.e.,
10-fold cross validation that is recommended.</p>
</td></tr>
<tr><td><code id="stepgbmRVI_+3A_weights">weights</code></td>
<td>
<p>an optional vector of weights to be used in the fitting
process. Must be positive but do not need to be normalized.
If keep.data = FALSE in the initial call to gbm then it is the user's
responsibility to resupply the weights to gbm.more. By default, a vector of
1 is used.</p>
</td></tr>
<tr><td><code id="stepgbmRVI_+3A_keep.data">keep.data</code></td>
<td>
<p>a logical variable indicating whether to keep the data and
an index of the data stored with the object. Keeping the data and index
makes subsequent calls to gbm.more faster at the cost of storing an extra
copy of the dataset. By default, 'FALSE' is used.</p>
</td></tr>
<tr><td><code id="stepgbmRVI_+3A_verbose">verbose</code></td>
<td>
<p>If TRUE, gbm will print out progress and performance
indicators. By default, 'TRUE' is used.</p>
</td></tr>
<tr><td><code id="stepgbmRVI_+3A_n.cores">n.cores</code></td>
<td>
<p>The number of CPU cores to use. See gbm for details. By
default, 6 is used.</p>
</td></tr>
<tr><td><code id="stepgbmRVI_+3A_rpt">rpt</code></td>
<td>
<p>iteration of cross validation.</p>
</td></tr>
<tr><td><code id="stepgbmRVI_+3A_predacc">predacc</code></td>
<td>
<p>&quot;VEcv&quot; for vecv in function pred.acc.</p>
</td></tr>
<tr><td><code id="stepgbmRVI_+3A_min.n.var">min.n.var</code></td>
<td>
<p>minimum number of predictive variables remained in the final
predictive model the default is 1.</p>
</td></tr>
<tr><td><code id="stepgbmRVI_+3A_rseed">rseed</code></td>
<td>
<p>random seed. By default, 1234 is used.</p>
</td></tr>
<tr><td><code id="stepgbmRVI_+3A_...">...</code></td>
<td>
<p>other arguments passed on to gbm.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following components:
variable removed based on avi (variable.removed), averaged predictive accuracy
of the model after excluding variable.removed (predictive.accuracy),
contribution to accuracy by each variable.removed (delta.accuracy), and
predictive accuracy matrix of the model after excluding variable.removed for
each iteration (predictive.accuracy2)
</p>


<h3>Author(s)</h3>

<p>Jin Li
</p>


<h3>References</h3>

<p>Li, J., Siwabessy, J., Huang, Z., Nichol, S. (2019). &quot;Developing
an optimal spatial predictive model for seabed sand content using machine
learning, geostatistics and their hybrid methods.&quot; Geosciences 9 (4):180.
</p>
<p>Li, J., Alvarez, B., Siwabessy, J., Tran, M., Huang, Z., Przeslawski, R.,
Radke, L., Howard, F., Nichol, S. (2017). &quot;Application of random forest,
generalised linear model and their hybrid methods with geostatistical
techniques to count data: Predicting sponge species richness.&quot;
Environmental Modelling &amp; Software 97: 112-129.
</p>
<p>Li, J., Alvarez, B., Siwabessy, J., Tran, M., Huang, Z., Przeslawski, R.,
Radke, L., Howard, F., Nichol, S. (2017). Selecting predictors to form the
most accurate predictive model for count data. International Congress on
Modelling and Simulation (MODSIM) 2017, Hobart.
</p>
<p>Chang, W. 2021. Cookbook for R. http://www.cookbook-r.com/.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(spm)
data(petrel)
stepgbm1 &lt;- stepgbmRVI(trainx = petrel[, c(1,2, 6:9)], trainy = log(petrel[, 5] + 1),
 cv.fold = 5, min.n.var = 2, n.cores = 2, rseed = 1234)
stepgbm1

#plot stepgbm1 results
library(reshape2)
pa1 &lt;- as.data.frame(stepgbm1$predictive.accuracy2)
names(pa1) &lt;- stepgbm1$variable.removed
pa2 &lt;- melt(pa1, id = NULL)
names(pa2) &lt;- c("Variable","VEcv")
library(lattice)
with(pa2, boxplot(VEcv~Variable, ylab="VEcv (%)", xlab="Predictive variable removed"))

barplot(stepgbm1$delta.accuracy, col = (1:length(stepgbm1$variable.removed)),
names.arg = stepgbm1$variable.removed, main = "Predictive accuracy vs variable removed",
font.main = 4, cex.names=1, font=2, ylab="Increase rate in VEcv (%)")

# Extract names of the selected predictive variables by stepgbm
library(steprf)
steprfAVIPredictors(stepgbm1, trainx = petrel[, c(1,2, 6:9)])

data(sponge)
set.seed(1234)
stepgbm2 &lt;- stepgbmRVI(trainx = sponge[, -3], trainy = sponge[, 3],
family = "poisson", cv.fold = 5, min.n.var = 2, n.cores = 2)
stepgbm2

#plot stepgbm2 results
library(reshape2)
pa1 &lt;- as.data.frame(stepgbm2$predictive.accuracy2)
names(pa1) &lt;- stepgbm2$variable.removed
pa2 &lt;- melt(pa1, id = NULL)
names(pa2) &lt;- c("Variable","VEcv")
library(lattice)
with(pa2, boxplot(VEcv~Variable, ylab="VEcv (%)", xlab="Predictive variable removed"))

barplot(stepgbm2$delta.accuracy, col = (1:length(stepgbm2$variable.removed)),
names.arg = stepgbm2$variable.removed, main = "Predictive accuracy vs variable removed",
font.main = 4, cex.names=1, font=2, ylab="Increase rate in VEcv (%)")

# Extract names of the selected predictive variables by stepgbm
steprfAVIPredictors(stepgbm2, trainx =  sponge[, -3])


</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
