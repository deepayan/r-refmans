<!DOCTYPE html><html><head><title>Help for package innsight</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {innsight}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#[,innsight_ggplot2,ANY,ANY,ANY-method'><p>Indexing plots of <code>innsight_ggplot2</code></p></a></li>
<li><a href='#[,innsight_plotly,ANY,ANY,ANY-method'><p>Indexing plots of <code>innsight_plotly</code></p></a></li>
<li><a href='#+,innsight_ggplot2,ANY-method'><p>Generic add function for <code>innsight_ggplot2</code></p></a></li>
<li><a href='#AgnosticWrapper'><p>Super class for model-agnostic interpretability methods</p></a></li>
<li><a href='#ConnectionWeights'><p>Connection weights method</p></a></li>
<li><a href='#ConvertedModel'><p>Converted torch-based model</p></a></li>
<li><a href='#Converter'><p>Converter of an artificial neural network</p></a></li>
<li><a href='#DeepLift'><p>Deep learning important features (DeepLift)</p></a></li>
<li><a href='#DeepSHAP'><p>Deep Shapley additive explanations (DeepSHAP)</p></a></li>
<li><a href='#ExpectedGradient'><p>Expected Gradients</p></a></li>
<li><a href='#get_result'><p>Get the result of an interpretation method</p></a></li>
<li><a href='#Gradient'><p>Vanilla Gradient and Gradient<code class="reqn">\times</code>Input</p></a></li>
<li><a href='#GradientBased'><p>Super class for gradient-based interpretation methods</p></a></li>
<li><a href='#innsight_ggplot2'><p>S4 class for ggplot2-based plots</p></a></li>
<li><a href='#innsight_plotly'><p>S4 class for plotly-based plots</p></a></li>
<li><a href='#innsight_sugar'><p>Syntactic sugar for object construction</p></a></li>
<li><a href='#innsight-package'><p>Get the insight of your neural network</p></a></li>
<li><a href='#IntegratedGradient'><p>Integrated Gradients</p></a></li>
<li><a href='#InterpretingMethod'><p>Super class for interpreting methods</p></a></li>
<li><a href='#LIME'><p>Local interpretable model-agnostic explanations (LIME)</p></a></li>
<li><a href='#LRP'><p>Layer-wise relevance propagation (LRP)</p></a></li>
<li><a href='#plot_global'><p>Get the result of an interpretation method</p></a></li>
<li><a href='#print,innsight_ggplot2-method'><p>Generic print, plot and show for <code>innsight_ggplot2</code></p></a></li>
<li><a href='#print,innsight_plotly-method'><p>Generic print, plot and show for <code>innsight_plotly</code></p></a></li>
<li><a href='#SHAP'><p>Shapley values</p></a></li>
<li><a href='#SmoothGrad'><p>SmoothGrad and SmoothGrad<code class="reqn">\times</code>Input</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Get the Insights of Your Neural Network</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Interpretation methods for analyzing the behavior and individual
    predictions of modern neural networks in a three-step procedure: Converting 
    the model, running the interpretation method, and visualizing the results. 
    Implemented methods are, e.g., 'Connection Weights' described by Olden et al. (2004)
    &lt;<a href="https://doi.org/10.1016%2Fj.ecolmodel.2004.03.013">doi:10.1016/j.ecolmodel.2004.03.013</a>&gt;, layer-wise relevance
    propagation ('LRP') described by Bach et al. (2015)
    &lt;<a href="https://doi.org/10.1371%2Fjournal.pone.0130140">doi:10.1371/journal.pone.0130140</a>&gt;, deep learning important features
    ('DeepLIFT') described by Shrikumar et al.  (2017) &lt;<a href="https://arxiv.org/abs/1704.02685">arXiv:1704.02685</a>&gt;
    and gradient-based methods like 'SmoothGrad' described by Smilkov et
    al. (2017) &lt;<a href="https://arxiv.org/abs/1706.03825">arXiv:1706.03825</a>&gt;, 'Gradient x Input' described by
    Baehrens et al. (2009) &lt;<a href="https://arxiv.org/abs/0912.1128">arXiv:0912.1128</a>&gt; or 'Vanilla Gradient'.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://bips-hb.github.io/innsight/">https://bips-hb.github.io/innsight/</a>,
<a href="https://github.com/bips-hb/innsight/">https://github.com/bips-hb/innsight/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/bips-hb/innsight/issues/">https://github.com/bips-hb/innsight/issues/</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>checkmate, cli, ggplot2, methods, R6, torch</td>
</tr>
<tr>
<td>Suggests:</td>
<td>covr, fastshap, GGally, grid, gridExtra, gtable, keras,
knitr, lime, luz, neuralnet, palmerpenguins, plotly, rmarkdown,
ranger, spelling, tensorflow, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Collate:</td>
<td>'AgnosticMethods.R' 'AgnosticWrapper.R' 'ConnectionWeights.R'
'Convert_keras.R' 'Convert_neuralnet.R' 'Convert_torch.R'
'ConvertedModel.R' 'Converter.R' 'DeepLift.R' 'GradienBased.R'
'InterpretingLayer.R' 'InterpretingMethod.R' 'LRP.R'
'Layer_conv1d.R' 'Layer_conv2d.R' 'Layer_dense.R'
'Layer_normalization.R' 'Layer_other.R' 'Layer_pooling.R'
'innsight.R' 'utils.R' 'utils_ggplot.R' 'utils_plotly.R'
'innsight_sugar.R' 'innsight_ggplot2.R' 'innsight_plotly.R'</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-12-21 15:33:01 UTC; niklas</td>
</tr>
<tr>
<td>Author:</td>
<td>Niklas Koenen <a href="https://orcid.org/0000-0002-4623-8271"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Raphael Baudeu [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Niklas Koenen &lt;niklas.koenen@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-12-21 16:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='+5B+2Cinnsight_ggplot2+2CANY+2CANY+2CANY-method'>Indexing plots of <code>innsight_ggplot2</code></h2><span id='topic++5B+2Cinnsight_ggplot2+2CANY+2CANY+2CANY-method'></span><span id='topic++5B+2Cinnsight_ggplot2-method'></span><span id='topic++5B.innsight_ggplot2'></span><span id='topic++5B+5B+2Cinnsight_ggplot2-method'></span><span id='topic++5B+5B.innsight_ggplot2'></span><span id='topic++5B+3C-+2Cinnsight_ggplot2+2CANY+2CANY+2CANY-method'></span><span id='topic++5B+3C-+2Cinnsight_ggplot2-method'></span><span id='topic++5B+3C-.innsight_ggplot2'></span><span id='topic++5B+5B+3C-+2Cinnsight_ggplot2-method'></span><span id='topic++5B+5B+3C-.innsight_ggplot2'></span>

<h3>Description</h3>

<p>The S4 class <code><a href="#topic+innsight_ggplot2">innsight_ggplot2</a></code> visualizes the results in the form of
a matrix, with the output nodes (and also the input layers) in the columns
and the selected data points in the rows. With these basic generic indexing
functions, the plots of individual rows and columns can be accessed,
modified and the overall plot can be adjusted accordingly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'innsight_ggplot2,ANY,ANY,ANY'
x[i, j, ..., restyle = TRUE, drop = TRUE]

## S4 method for signature 'innsight_ggplot2'
x[[i, j, ..., restyle = TRUE]]

## S4 replacement method for signature 'innsight_ggplot2,ANY,ANY,ANY'
x[i, j, ...] &lt;- value

## S4 replacement method for signature 'innsight_ggplot2'
x[[i, j, ...]] &lt;- value
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="+2B5B+2B2Cinnsight_ggplot2+2B2CANY+2B2CANY+2B2CANY-method_+3A_x">x</code></td>
<td>
<p>An instance of the S4 class <code><a href="#topic+innsight_ggplot2">innsight_ggplot2</a></code>.</p>
</td></tr>
<tr><td><code id="+2B5B+2B2Cinnsight_ggplot2+2B2CANY+2B2CANY+2B2CANY-method_+3A_i">i</code></td>
<td>
<p>The numeric (or missing) index for the rows.</p>
</td></tr>
<tr><td><code id="+2B5B+2B2Cinnsight_ggplot2+2B2CANY+2B2CANY+2B2CANY-method_+3A_j">j</code></td>
<td>
<p>The numeric (or missing) index for the columns.</p>
</td></tr>
<tr><td><code id="+2B5B+2B2Cinnsight_ggplot2+2B2CANY+2B2CANY+2B2CANY-method_+3A_...">...</code></td>
<td>
<p>other unused arguments</p>
</td></tr>
<tr><td><code id="+2B5B+2B2Cinnsight_ggplot2+2B2CANY+2B2CANY+2B2CANY-method_+3A_restyle">restyle</code></td>
<td>
<p>This logical value determines whether the labels and facet
stripes remain as they were in the original plot or are adjusted to the
subplot accordingly. However, this argument is only used if the <code>innsight_ggplot2</code>
instance is a multiplot, i.e., <code>x@multiplot</code> is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="+2B5B+2B2Cinnsight_ggplot2+2B2CANY+2B2CANY+2B2CANY-method_+3A_drop">drop</code></td>
<td>
<p>unused argument</p>
</td></tr>
<tr><td><code id="+2B5B+2B2Cinnsight_ggplot2+2B2CANY+2B2CANY+2B2CANY-method_+3A_value">value</code></td>
<td>
<p>Another instance of the S4 class <code>innsight_ggplot2</code> but of
shape <code>i</code> x <code>j</code>.</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li> <p><code style="white-space: pre;">&#8288;[.innsight_ggplot2&#8288;</code>: Selects only the plots from the <code>i</code>-th rows and
<code>j</code>-th columns and returns them as a new instance of <code><a href="#topic+innsight_ggplot2">innsight_ggplot2</a></code>.
If <code>restyle = TRUE</code> the facet stripes and axis labels of the original
plot are transferred to the subplot, otherwise they are returned as they are.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;[[.innsight_ggplot2&#8288;</code>: Selects only the subplot in row <code>i</code> and column
<code>j</code> and returns it as a <a href="ggplot2.html#topic+ggplot">ggplot2::ggplot</a> object. If <code>restyle = TRUE</code> the
facet stripes and axis labels of the original plot are transferred to
the subplot, otherwise they are returned as they are.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;[&lt;-.innsight_ggplot2&#8288;</code>: Replaces the plots in the rows <code>i</code> and columns <code>j</code>
with those from <code>value</code> and returns the modified instance of
<code><a href="#topic+innsight_ggplot2">innsight_ggplot2</a></code>.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;[[&lt;-.innsight_ggplot2&#8288;</code>: Replaces the plot from the <code>i</code>-th row and <code>j</code>-th
column with the plot from <code>value</code> and returns the modified instance of
<code><a href="#topic+innsight_ggplot2">innsight_ggplot2</a></code>.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+innsight_ggplot2">innsight_ggplot2</a></code>, <code><a href="#topic+print.innsight_ggplot2">print.innsight_ggplot2</a></code>,
<code><a href="#topic++2B.innsight_ggplot2">+.innsight_ggplot2</a></code>
</p>

<hr>
<h2 id='+5B+2Cinnsight_plotly+2CANY+2CANY+2CANY-method'>Indexing plots of <code>innsight_plotly</code></h2><span id='topic++5B+2Cinnsight_plotly+2CANY+2CANY+2CANY-method'></span><span id='topic++5B+2Cinnsight_plotly-method'></span><span id='topic++5B.innsight_plotly'></span><span id='topic++5B+5B+2Cinnsight_plotly-method'></span><span id='topic++5B+5B.innsight_plotly'></span>

<h3>Description</h3>

<p>The S4 class <code><a href="#topic+innsight_plotly">innsight_plotly</a></code> visualizes the results as a matrix of
plots based on <code><a href="plotly.html#topic+plot_ly">plotly::plot_ly</a></code>. The output nodes (and also input layers)
are displayed in the columns and the selected data points in the rows. With
these basic generic indexing functions, the plots of individual rows and
columns can be accessed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'innsight_plotly,ANY,ANY,ANY'
x[i, j, ..., drop = TRUE]

## S4 method for signature 'innsight_plotly'
x[[i, j, ..., drop]]
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="+2B5B+2B2Cinnsight_plotly+2B2CANY+2B2CANY+2B2CANY-method_+3A_x">x</code></td>
<td>
<p>An instance of the S4 class <code><a href="#topic+innsight_plotly">innsight_plotly</a></code>.</p>
</td></tr>
<tr><td><code id="+2B5B+2B2Cinnsight_plotly+2B2CANY+2B2CANY+2B2CANY-method_+3A_i">i</code></td>
<td>
<p>The numeric (or missing) index for the rows.</p>
</td></tr>
<tr><td><code id="+2B5B+2B2Cinnsight_plotly+2B2CANY+2B2CANY+2B2CANY-method_+3A_j">j</code></td>
<td>
<p>The numeric (or missing) index for the columns.</p>
</td></tr>
<tr><td><code id="+2B5B+2B2Cinnsight_plotly+2B2CANY+2B2CANY+2B2CANY-method_+3A_...">...</code></td>
<td>
<p>other unused arguments</p>
</td></tr>
<tr><td><code id="+2B5B+2B2Cinnsight_plotly+2B2CANY+2B2CANY+2B2CANY-method_+3A_drop">drop</code></td>
<td>
<p>unused argument</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li> <p><code style="white-space: pre;">&#8288;[.innsight_plotly&#8288;</code>: Selects the plots from the i-th rows and j-th
columns and returns them as a new instance of <code>innsight_plotly</code>.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;[[.innisght_plotly&#8288;</code>: Selects only the single plot in the i-th row and
j-th column and returns it as a plotly object.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+innsight_plotly">innsight_plotly</a></code>, <code><a href="#topic+print.innsight_plotly">print.innsight_plotly</a></code>,
<code><a href="#topic+plot.innsight_plotly">plot.innsight_plotly</a></code>, <code><a href="#topic+show.innsight_plotly">show.innsight_plotly</a></code>
</p>

<hr>
<h2 id='+2B+2Cinnsight_ggplot2+2CANY-method'>Generic add function for <code>innsight_ggplot2</code></h2><span id='topic++2B+2Cinnsight_ggplot2+2CANY-method'></span><span id='topic++2B.innsight_ggplot2'></span>

<h3>Description</h3>

<p>This generic add function allows to treat an instance of <code><a href="#topic+innsight_ggplot2">innsight_ggplot2</a></code>
as an ordinary plot object of <code><a href="ggplot2.html#topic+ggplot2">ggplot2</a></code>. For example geoms, themes and
scales can be added as usual (see <code><a href="ggplot2.html#topic+gg-add">ggplot2::+.gg</a></code> for more information).<br /> <br />
<strong>Note:</strong> If <code>e1</code> represents a multiplot (i.e., <code>e1@mulitplot = TRUE</code>),
<code>e2</code> is added to each individual plot. If only specific plots need to be
changed, the generic assignment function should be used (see
<a href="#topic+innsight_ggplot2">innsight_ggplot2</a> for details).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'innsight_ggplot2,ANY'
e1 + e2
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="+2B2B+2B2Cinnsight_ggplot2+2B2CANY-method_+3A_e1">e1</code></td>
<td>
<p>An instance of the S4 class <code><a href="#topic+innsight_ggplot2">innsight_ggplot2</a></code>.</p>
</td></tr>
<tr><td><code id="+2B2B+2B2Cinnsight_ggplot2+2B2CANY-method_+3A_e2">e2</code></td>
<td>
<p>An object of class <code><a href="ggplot2.html#topic+ggplot">ggplot2::ggplot</a></code> or a <code><a href="ggplot2.html#topic+theme">ggplot2::theme</a></code>.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+innsight_ggplot2">innsight_ggplot2</a></code>,
<code><a href="#topic+print.innsight_ggplot2">print.innsight_ggplot2</a></code>,
<code><a href="#topic++5B.innsight_ggplot2">[.innsight_ggplot2</a></code>,
<code><a href="#topic++5B+5B.innsight_ggplot2">[[.innsight_ggplot2</a></code>,
<code><a href="#topic++5B+3C-.innsight_ggplot2">[&lt;-.innsight_ggplot2</a></code>,
<code><a href="#topic++5B+5B+3C-.innsight_ggplot2">[[&lt;-.innsight_ggplot2</a></code>
</p>

<hr>
<h2 id='AgnosticWrapper'>Super class for model-agnostic interpretability methods</h2><span id='topic+AgnosticWrapper'></span>

<h3>Description</h3>

<p>This is a super class for all implemented model-agnostic
interpretability methods and inherits from the <code><a href="#topic+InterpretingMethod">InterpretingMethod</a></code>
class. Instead of just an object of the <code><a href="#topic+Converter">Converter</a></code> class, any model
can now be passed. In contrast to the other model-specific methods in this
package, only the prediction function of the model is required, and not
the internal details of the model. The following model-agnostic methods
are available (all are wrapped by other packages):
</p>

<ul>
<li> <p><em>Shapley values</em> (<code><a href="#topic+SHAP">SHAP</a></code>) based on <code><a href="fastshap.html#topic+explain">fastshap::explain</a></code>
</p>
</li>
<li> <p><em>Local interpretable model-agnostic explanations</em>  (<code><a href="#topic+LIME">LIME</a></code>) based on
<code><a href="lime.html#topic+lime">lime::lime</a></code>
</p>
</li></ul>



<h3>Super class</h3>

<p><code><a href="#topic+InterpretingMethod">innsight::InterpretingMethod</a></code> -&gt; <code>AgnosticWrapper</code>
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>data_orig</code></dt><dd><p>The individual instances to be explained by the method
(unprocessed!).</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-AgnosticWrapper-new"><code>AgnosticWrapper$new()</code></a>
</p>
</li>
<li> <p><a href="#method-AgnosticWrapper-clone"><code>AgnosticWrapper$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="get_result"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-get_result'><code>innsight::InterpretingMethod$get_result()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="plot"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-plot'><code>innsight::InterpretingMethod$plot()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="plot_global"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-plot_global'><code>innsight::InterpretingMethod$plot_global()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="print"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-print'><code>innsight::InterpretingMethod$print()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-AgnosticWrapper-new"></a>



<h4>Method <code>new()</code></h4>

<p>Create a new instance of the <code>AgnosticWrapper</code> R6 class.
</p>


<h5>Usage</h5>

<div class="r"><pre>AgnosticWrapper$new(
  model,
  data,
  data_ref,
  output_type = NULL,
  pred_fun = NULL,
  output_idx = NULL,
  output_label = NULL,
  channels_first = TRUE,
  input_dim = NULL,
  input_names = NULL,
  output_names = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>model</code></dt><dd><p>(any prediction model)<br />
A fitted model for a classification or regression task that
is intended to be interpreted. A <code><a href="#topic+Converter">Converter</a></code> object can also be
passed. In order for the package to know how to make predictions
with the given model, a prediction function must also be passed with
the argument <code>pred_fun</code>. However, for models created by
<code><a href="torch.html#topic+nn_sequential">nn_sequential</a></code>, <code><a href="keras.html#topic+keras_model">keras_model</a></code>,
<code><a href="neuralnet.html#topic+neuralnet">neuralnet</a></code> or <code><a href="#topic+Converter">Converter</a></code>,
these have already been pre-implemented and do not need to be
specified.<br /></p>
</dd>
<dt><code>data</code></dt><dd><p>(<code>array</code>, <code>data.frame</code> or <code>torch_tensor</code>)<br />
The individual instances to be explained by the method.
These must have the same format as the input data of the passed model
and has to be either <code><a href="base.html#topic+matrix">matrix</a></code>, an <code><a href="base.html#topic+array">array</a></code>, a <code><a href="base.html#topic+data.frame">data.frame</a></code> or a
<code><a href="torch.html#topic+torch_tensor">torch_tensor</a></code>. If no value is specified, all instances in the
dataset <code>data</code> will be explained.<br />
<strong>Note:</strong> For the model-agnostic methods, only models with a single
input and output layer is allowed!<br /></p>
</dd>
<dt><code>data_ref</code></dt><dd><p>(<code><a href="base.html#topic+array">array</a></code>, <code><a href="base.html#topic+data.frame">data.frame</a></code> or <code><a href="torch.html#topic+torch_tensor">torch_tensor</a></code>)<br />
The dataset to which the method is to be applied. These must
have the same format as the input data of the passed model and has to
be either <code><a href="base.html#topic+matrix">matrix</a></code>, an <code><a href="base.html#topic+array">array</a></code>, a <code><a href="base.html#topic+data.frame">data.frame</a></code> or a
<code><a href="torch.html#topic+torch_tensor">torch_tensor</a></code>.<br />
<strong>Note:</strong> For the model-agnostic methods, only models with a single
input and output layer is allowed!<br /></p>
</dd>
<dt><code>output_type</code></dt><dd><p>(<code>character(1)</code>)<br />
Type of the model output, i.e., either
<code>"classification"</code> or <code>"regression"</code>.<br /></p>
</dd>
<dt><code>pred_fun</code></dt><dd><p>(<code>function</code>)<br />
Prediction function for the model. This argument is only
needed if <code>model</code> is not a model created by
<code><a href="torch.html#topic+nn_sequential">nn_sequential</a></code>, <code><a href="keras.html#topic+keras_model">keras_model</a></code>,
<code><a href="neuralnet.html#topic+neuralnet">neuralnet</a></code> or <code><a href="#topic+Converter">Converter</a></code>. The first argument of
<code>pred_fun</code> has to be <code>newdata</code>, e.g.,
</p>
<div class="sourceCode"><pre>function(newdata, ...) model(newdata)
</pre></div></dd>
<dt><code>output_idx</code></dt><dd><p>(<code>integer</code>, <code>list</code> or <code>NULL</code>)<br />
These indices specify the output nodes for which
the method is to be applied. In order to allow models with multiple
output layers, there are the following possibilities to select
the indices of the output nodes in the individual output layers:
</p>

<ul>
<li><p> An <code>integer</code> vector of indices: If the model has only one output
layer, the values correspond to the indices of the output nodes, e.g.,
<code>c(1,3,4)</code> for the first, third and fourth output node. If there are
multiple output layers, the indices of the output nodes from the first
output layer are considered.
</p>
</li>
<li><p> A <code>list</code> of <code>integer</code> vectors of indices: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired indices of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of indices, e.g., <code>list(NULL, c(1,3))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br />
</p>
</li></ul>
</dd>
<dt><code>output_label</code></dt><dd><p>(<code>character</code>, <code>factor</code>, <code>list</code> or <code>NULL</code>)<br />
These values specify the output nodes for which
the method is to be applied. Only values that were previously passed with
the argument <code>output_names</code> in the <code>converter</code> can be used. In order to
allow models with multiple
output layers, there are the following possibilities to select
the names of the output nodes in the individual output layers:
</p>

<ul>
<li><p> A <code>character</code> vector or <code>factor</code> of labels: If the model has only one output
layer, the values correspond to the labels of the output nodes named in the
passed <code>Converter</code> object, e.g.,
<code>c("a", "c", "d")</code> for the first, third and fourth output node if the
output names are <code>c("a", "b", "c", "d")</code>. If there are
multiple output layers, the names of the output nodes from the first
output layer are considered.
</p>
</li>
<li><p> A <code>list</code> of <code>charactor</code>/<code>factor</code> vectors of labels: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired labels of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of labels, e.g., <code>list(NULL, c("a", "c"))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br />
</p>
</li></ul>
</dd>
<dt><code>channels_first</code></dt><dd><p>(<code>logical(1)</code>)<br />
The channel position of the given data (argument
<code>data</code>). If <code>TRUE</code>, the channel axis is placed at the second position
between the batch size and the rest of the input axes, e.g.,
<code>c(10,3,32,32)</code> for a batch of ten images with three channels and a
height and width of 32 pixels. Otherwise (<code>FALSE</code>), the channel axis
is at the last position, i.e., <code>c(10,32,32,3)</code>. If the data
has no channel axis, use the default value <code>TRUE</code>.<br /></p>
</dd>
<dt><code>input_dim</code></dt><dd><p>(<code>integer</code>)<br />
The model input dimension excluding the batch
dimension. It can be specified as vector of integers, but has to be in
the format &quot;channels first&quot;.<br /></p>
</dd>
<dt><code>input_names</code></dt><dd><p>(<code>character</code>, <code>factor</code> or <code>list</code>)<br />
The input names of the model excluding the batch dimension. For a model
with a single input layer and input axis (e.g., for tabular data), the
input names can be specified as a character vector or factor, e.g.,
for a dense layer with 3 input features use <code>c("X1", "X2", "X3")</code>. If
the model input consists of multiple axes (e.g., for signal and
image data), use a list of character vectors or factors for each axis
in the format &quot;channels first&quot;, e.g., use
<code>list(c("C1", "C2"), c("L1","L2","L3","L4","L5"))</code> for a 1D
convolutional input layer with signal length 4 and 2 channels.<br />
<em>Note:</em> This argument is optional and otherwise the names are
generated automatically. But if this argument is set, all found
input names in the passed model will be disregarded.<br /></p>
</dd>
<dt><code>output_names</code></dt><dd><p>(<code>character</code>, <code>factor</code> )<br />
A character vector with the names for the output dimensions
excluding the batch dimension, e.g., for a model with 3 output nodes use
<code>c("Y1", "Y2", "Y3")</code>. Instead of a character
vector you can also use a factor to set an order for the plots.<br />
<em>Note:</em> This argument is optional and otherwise the names are
generated automatically. But if this argument is set, all found
output names in the passed model will be disregarded.<br /></p>
</dd>
</dl>

</div>


<hr>
<a id="method-AgnosticWrapper-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>AgnosticWrapper$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='ConnectionWeights'>Connection weights method</h2><span id='topic+ConnectionWeights'></span>

<h3>Description</h3>

<p>This class implements the <em>Connection weights</em> method investigated by
Olden et al. (2004), which results in a relevance score for each input
variable. The basic idea is to multiply all path weights for each
possible connection between an input feature and the output node and then
calculate the sum over them. Besides, it is originally a global
interpretation method and independent of the input data. For a neural
network with <code class="reqn">3</code> hidden layers with weight matrices <code class="reqn">W_1</code>,
<code class="reqn">W_2</code> and <code class="reqn">W_3</code>, this method results in a simple matrix
multiplication independent of the activation functions in between:
</p>
<p style="text-align: center;"><code class="reqn">W_1 * W_2 * W_3.</code>
</p>

<p>In this package, we extended this method to a local method inspired by the
method <em>Gradient<code class="reqn">\times</code>Input</em> (see <code><a href="#topic+Gradient">Gradient</a></code>). Hence, the local variant is
simply the point-wise product of the global <em>Connection weights</em> method and
the input data. You can use this variant by setting the <code>times_input</code>
argument to <code>TRUE</code> and providing input data.
</p>
<p>The R6 class can also be initialized using the <code><a href="#topic+run_cw">run_cw</a></code> function
as a helper function so that no prior knowledge of R6 classes is required.
</p>


<h3>Super class</h3>

<p><code><a href="#topic+InterpretingMethod">innsight::InterpretingMethod</a></code> -&gt; <code>ConnectionWeights</code>
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>times_input</code></dt><dd><p>(<code>logical(1)</code>)<br />
This logical value indicates whether the results from
the <em>Connection weights</em> method were multiplied by the provided input
data or not. Thus, this value specifies whether the original global
variant of the method or the local one was applied. If the value is
<code>TRUE</code>, then data is provided in the field <code>data</code>.</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-ConnectionWeights-new"><code>ConnectionWeights$new()</code></a>
</p>
</li>
<li> <p><a href="#method-ConnectionWeights-clone"><code>ConnectionWeights$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="get_result"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-get_result'><code>innsight::InterpretingMethod$get_result()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="plot"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-plot'><code>innsight::InterpretingMethod$plot()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="plot_global"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-plot_global'><code>innsight::InterpretingMethod$plot_global()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="print"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-print'><code>innsight::InterpretingMethod$print()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-ConnectionWeights-new"></a>



<h4>Method <code>new()</code></h4>

<p>Create a new instance of the class <code>ConnectionWeights</code>. When
initialized, the method is applied and the results
are stored in the field <code>result</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>ConnectionWeights$new(
  converter,
  data = NULL,
  output_idx = NULL,
  output_label = NULL,
  channels_first = TRUE,
  times_input = FALSE,
  verbose = interactive(),
  dtype = "float"
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>converter</code></dt><dd><p>(<code><a href="#topic+Converter">Converter</a></code>)<br />
An instance of the <code>Converter</code> class that includes the
torch-converted model and some other model-specific attributes. See
<code><a href="#topic+Converter">Converter</a></code> for details.<br /></p>
</dd>
<dt><code>data</code></dt><dd><p>(<code><a href="base.html#topic+array">array</a></code>, <code><a href="base.html#topic+data.frame">data.frame</a></code>, <code><a href="torch.html#topic+torch_tensor">torch_tensor</a></code> or <code>list</code>)<br />
The data to which the method is to be applied. These must
have the same format as the input data of the passed model to the
converter object. This means either
</p>

<ul>
<li><p> an <code>array</code>, <code>data.frame</code>, <code>torch_tensor</code> or array-like format of
size <em>(batch_size, dim_in)</em>, if e.g.the model has only one input layer, or
</p>
</li>
<li><p> a <code>list</code> with the corresponding input data (according to the
upper point) for each of the input layers.
</p>
</li></ul>

<p>This argument is only relevant if
<code>times_input</code> is <code>TRUE</code>, otherwise it will be ignored because it is a
locale (i.e. explanation for each data point individually) method only
in this case.<br /></p>
</dd>
<dt><code>output_idx</code></dt><dd><p>(<code>integer</code>, <code>list</code> or <code>NULL</code>)<br />
These indices specify the output nodes for which
the method is to be applied. In order to allow models with multiple
output layers, there are the following possibilities to select
the indices of the output nodes in the individual output layers:
</p>

<ul>
<li><p> An <code>integer</code> vector of indices: If the model has only one output
layer, the values correspond to the indices of the output nodes, e.g.,
<code>c(1,3,4)</code> for the first, third and fourth output node. If there are
multiple output layers, the indices of the output nodes from the first
output layer are considered.
</p>
</li>
<li><p> A <code>list</code> of <code>integer</code> vectors of indices: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired indices of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of indices, e.g., <code>list(NULL, c(1,3))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br />
</p>
</li></ul>
</dd>
<dt><code>output_label</code></dt><dd><p>(<code>character</code>, <code>factor</code>, <code>list</code> or <code>NULL</code>)<br />
These values specify the output nodes for which
the method is to be applied. Only values that were previously passed with
the argument <code>output_names</code> in the <code>converter</code> can be used. In order to
allow models with multiple
output layers, there are the following possibilities to select
the names of the output nodes in the individual output layers:
</p>

<ul>
<li><p> A <code>character</code> vector or <code>factor</code> of labels: If the model has only one output
layer, the values correspond to the labels of the output nodes named in the
passed <code>Converter</code> object, e.g.,
<code>c("a", "c", "d")</code> for the first, third and fourth output node if the
output names are <code>c("a", "b", "c", "d")</code>. If there are
multiple output layers, the names of the output nodes from the first
output layer are considered.
</p>
</li>
<li><p> A <code>list</code> of <code>charactor</code>/<code>factor</code> vectors of labels: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired labels of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of labels, e.g., <code>list(NULL, c("a", "c"))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br />
</p>
</li></ul>
</dd>
<dt><code>channels_first</code></dt><dd><p>(<code>logical(1)</code>)<br />
The channel position of the given data (argument
<code>data</code>). If <code>TRUE</code>, the channel axis is placed at the second position
between the batch size and the rest of the input axes, e.g.,
<code>c(10,3,32,32)</code> for a batch of ten images with three channels and a
height and width of 32 pixels. Otherwise (<code>FALSE</code>), the channel axis
is at the last position, i.e., <code>c(10,32,32,3)</code>. If the data
has no channel axis, use the default value <code>TRUE</code>.<br /></p>
</dd>
<dt><code>times_input</code></dt><dd><p>(<code>logical(1)</code>)<br />
Multiplies the results with the input features.
This variant turns the global <em>Connection weights</em> method into a local
one. Default: <code>FALSE</code>.<br /></p>
</dd>
<dt><code>verbose</code></dt><dd><p>(<code>logical(1)</code>)<br />
This logical argument determines whether a progress bar is
displayed for the calculation of the method or not. The default value is
the output of the primitive R function <code><a href="base.html#topic+interactive">interactive()</a></code>.<br /></p>
</dd>
<dt><code>dtype</code></dt><dd><p>(<code>character(1)</code>)<br />
The data type for the calculations. Use
either <code>'float'</code> for <a href="torch.html#topic+torch_float">torch_float</a> or <code>'double'</code> for
<a href="torch.html#topic+torch_double">torch_double</a>.<br /></p>
</dd>
</dl>

</div>


<hr>
<a id="method-ConnectionWeights-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>ConnectionWeights$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>References</h3>


<ul>
<li><p> J. D. Olden et al. (2004) <em>An accurate comparison of methods for
quantifying variable importance in artificial neural networks using
simulated data.</em> Ecological Modelling 178, p. 389â€“397
</p>
</li></ul>



<h3>See Also</h3>

<p>Other methods: 
<code><a href="#topic+DeepLift">DeepLift</a></code>,
<code><a href="#topic+DeepSHAP">DeepSHAP</a></code>,
<code><a href="#topic+ExpectedGradient">ExpectedGradient</a></code>,
<code><a href="#topic+Gradient">Gradient</a></code>,
<code><a href="#topic+IntegratedGradient">IntegratedGradient</a></code>,
<code><a href="#topic+LIME">LIME</a></code>,
<code><a href="#topic+LRP">LRP</a></code>,
<code><a href="#topic+SHAP">SHAP</a></code>,
<code><a href="#topic+SmoothGrad">SmoothGrad</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#----------------------- Example 1: Torch ----------------------------------
library(torch)

# Create nn_sequential model
model &lt;- nn_sequential(
  nn_linear(5, 12),
  nn_relu(),
  nn_linear(12, 1),
  nn_sigmoid()
)

# Create Converter with input names
converter &lt;- Converter$new(model,
  input_dim = c(5),
  input_names = list(c("Car", "Cat", "Dog", "Plane", "Horse"))
)

# You can also use the helper function for the initialization part
converter &lt;- convert(model,
  input_dim = c(5),
  input_names = list(c("Car", "Cat", "Dog", "Plane", "Horse"))
)

# Apply method Connection Weights
cw &lt;- ConnectionWeights$new(converter)

# Again, you can use a helper function `run_cw()` for initializing
cw &lt;- run_cw(converter)

# Print the head of the result as a data.frame
head(get_result(cw, "data.frame"), 5)

# Plot the result
plot(cw)

#----------------------- Example 2: Neuralnet ------------------------------
if (require("neuralnet")) {
  library(neuralnet)
  data(iris)

  # Train a Neural Network
  nn &lt;- neuralnet((Species == "setosa") ~ Petal.Length + Petal.Width,
    iris,
    linear.output = FALSE,
    hidden = c(3, 2), act.fct = "tanh", rep = 1
  )

  # Convert the trained model
  converter &lt;- convert(nn)

  # Apply the Connection Weights method
  cw &lt;- run_cw(converter)

  # Get the result as a torch tensor
  get_result(cw, type = "torch.tensor")

  # Plot the result
  plot(cw)
}


# ------------------------- Example 3: Keras -------------------------------
if (require("keras") &amp; keras::is_keras_available()) {
  library(keras)

  # Make sure keras is installed properly
  is_keras_available()

  data &lt;- array(rnorm(10 * 32 * 32 * 3), dim = c(10, 32, 32, 3))

  model &lt;- keras_model_sequential()
  model %&gt;%
    layer_conv_2d(
      input_shape = c(32, 32, 3), kernel_size = 8, filters = 8,
      activation = "softplus", padding = "valid") %&gt;%
    layer_conv_2d(
      kernel_size = 8, filters = 4, activation = "tanh",
      padding = "same") %&gt;%
    layer_conv_2d(
      kernel_size = 4, filters = 2, activation = "relu",
      padding = "valid") %&gt;%
    layer_flatten() %&gt;%
    layer_dense(units = 64, activation = "relu") %&gt;%
    layer_dense(units = 16, activation = "relu") %&gt;%
    layer_dense(units = 2, activation = "softmax")

  # Convert the model
  converter &lt;- convert(model)

  # Apply the Connection Weights method
  cw &lt;- run_cw(converter)

  # Get the head of the result as a data.frame
  head(get_result(cw, type = "data.frame"), 5)

  # Plot the result for all classes
  plot(cw, output_idx = 1:2)
}


#------------------------- Plotly plots ------------------------------------
if (require("plotly")) {
  # You can also create an interactive plot with plotly.
  # This is a suggested package, so make sure that it is installed
  library(plotly)
  plot(cw, as_plotly = TRUE)
}

</code></pre>

<hr>
<h2 id='ConvertedModel'>Converted torch-based model</h2><span id='topic+ConvertedModel'></span>

<h3>Description</h3>

<p>This class stores all layers converted to <code>torch</code> in a module which can be
used like the original model (but <code>torch</code>-based). In addition, it provides
other functions that are useful for interpreting individual predictions or
explaining the entire model. This model is part of the class <code><a href="#topic+Converter">Converter</a></code>
and is the core for all the necessary calculations in the methods provided
in this package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ConvertedModel(modules_list, graph, input_nodes, output_nodes, dtype = "float")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ConvertedModel_+3A_modules_list">modules_list</code></td>
<td>
<p>(<code>list</code>)<br />
A list of all accepted layers created by the <code><a href="#topic+Converter">Converter</a></code>
class during initialization.<br /></p>
</td></tr>
<tr><td><code id="ConvertedModel_+3A_graph">graph</code></td>
<td>
<p>(<code>list</code>)<br />
The <code>graph</code> argument gives a way to pass an input through
the model, which is especially relevant for non-sequential architectures.
It can be seen as a list of steps in which order the layers from
<code>modules_list</code> must be applied. The list contains the following elements:
</p>

<ul>
<li> <p><code style="white-space: pre;">&#8288;$current_nodes&#8288;</code><br />
This list describes the current position and the number
of the respective intermediate values when passing through the model.
For example, <code>list(1,3,3)</code> means that in this step one output from the
first layer and two from the third layer (the numbers correspond to the
list indices from the <code>modules_list</code> argument) are available for
the calculation of the current layer with index <code>used_node</code>.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$used_node&#8288;</code><br />
The index of the layer from the <code>modules_list</code> argument
which will be applied in this step.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$used_idx&#8288;</code><br />
The indices of the outputs from <code>current_nodes</code>, which are
used as inputs of the current layer (<code>used_node</code>).
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$times&#8288;</code><br />
The frequency of the output value, i.e., is the output used
more than once as an input for subsequent layers?<br />
</p>
</li></ul>
</td></tr>
<tr><td><code id="ConvertedModel_+3A_input_nodes">input_nodes</code></td>
<td>
<p>(<code>numeric</code>)<br />
A vector of layer indices describing the input layers,
i.e., they are used as the starting point for the calculations.<br /></p>
</td></tr>
<tr><td><code id="ConvertedModel_+3A_output_nodes">output_nodes</code></td>
<td>
<p>(<code>numeric</code>)<br />
A vector of layer indices describing the indices
of the output layers.<br /></p>
</td></tr>
<tr><td><code id="ConvertedModel_+3A_dtype">dtype</code></td>
<td>
<p>(<code>character(1)</code>)<br />
The data type for all the calculations and defined tensors. Use
either <code>'float'</code> for <code><a href="torch.html#topic+torch_dtype">torch::torch_float</a></code> or <code>'double'</code> for
<code><a href="torch.html#topic+torch_dtype">torch::torch_double</a></code>.<br /></p>
</td></tr>
</table>


<h3>Method <code>forward()</code></h3>

<p>The forward method of the whole model, i.e., it calculates the output
<code class="reqn">y=f(x)</code> of a given input <code class="reqn">x</code>. In doing so, all intermediate
values are stored in the individual torch modules from <code>modules_list</code>.
</p>


<h4>Usage</h4>

<div class="sourceCode"><pre>self(x,
     channels_first = TRUE,
     save_input = FALSE,
     save_preactivation = FALSE,
     save_output = FAlSE,
     save_last_layer = FALSE)
</pre></div>



<h4>Arguments</h4>


<dl>
<dt><code>x</code></dt><dd><p>The input torch tensor for this model.</p>
</dd>
<dt><code>channels_first</code></dt><dd><p>If the input tensor <code>x</code> is given in the format
'channels first', use <code>TRUE</code>. Otherwise, if the channels are last,
use <code>FALSE</code> and the input will be transformed into the format 'channels
first'. Default: <code>TRUE</code>.</p>
</dd>
<dt><code>save_input</code></dt><dd><p>Logical value whether the inputs from each layer
are to be saved or not. Default: <code>FALSE</code>.</p>
</dd>
<dt><code>save_preactivation</code></dt><dd><p>Logical value whether the preactivations
from each layer are to be saved or not. Default: <code>FALSE</code>.</p>
</dd>
<dt><code>save_output</code></dt><dd><p>Logical value whether the outputs from each layer
are to be saved or not. Default: <code>FALSE</code>.</p>
</dd>
<dt><code>save_last_layer</code></dt><dd><p>Logical value whether the inputs,
preactivations and outputs from the last layer are to be saved or not.
Default: <code>FALSE</code>.</p>
</dd>
</dl>




<h4>Return</h4>

<p>Returns a list of the output values of the model with respect to the
given inputs.
</p>



<h3>Method <code>update_ref()</code></h3>

<p>This method updates the intermediate values in each module from the
list <code>modules_list</code> for the reference input <code>x_ref</code> and returns the
output from it in the same way as in the <code>forward</code> method.
</p>


<h4>Usage</h4>

<div class="sourceCode"><pre>self$update_ref(x_ref,
                channels_first = TRUE,
                save_input = FALSE,
                save_preactivation = FALSE,
                save_output = FAlSE,
                save_last_layer = FALSE)
</pre></div>



<h4>Arguments</h4>


<dl>
<dt><code>x_ref</code></dt><dd><p>Reference input of the model.</p>
</dd>
<dt><code>channels_first</code></dt><dd><p>If the tensor <code>x_ref</code> is given in the format
'channels first' use <code>TRUE</code>. Otherwise, if the channels are last,
use <code>FALSE</code> and the input will be transformed into the format 'channels
first'. Default: <code>TRUE</code>.</p>
</dd>
<dt><code>save_input</code></dt><dd><p>Logical value whether the inputs from each layer
are to be saved or not. Default: <code>FALSE</code>.</p>
</dd>
<dt><code>save_preactivation</code></dt><dd><p>Logical value whether the preactivations
from each layer are to be saved or not. Default: <code>FALSE</code>.</p>
</dd>
<dt><code>save_output</code></dt><dd><p>Logical value whether the outputs from each layer
are to be saved or not. Default: <code>FALSE</code>.</p>
</dd>
<dt><code>save_last_layer</code></dt><dd><p>Logical value whether the inputs,
preactivations and outputs from the last layer are to be saved or not.
Default: <code>FALSE</code>.</p>
</dd>
</dl>




<h4>Return</h4>

<p>Returns a list of the output values of the model with respect to the
given reference input.
</p>



<h3>Method <code>set_dtype()</code></h3>

<p>This method changes the data type for all the layers in <code>modules_list</code>.
Use either <code>'float'</code> for <a href="torch.html#topic+torch_dtype">torch::torch_float</a> or <code>'double'</code> for
<a href="torch.html#topic+torch_dtype">torch::torch_double</a>.
</p>


<h4>Usage</h4>

<p><code>self$set_dtype(dtype)</code>
</p>



<h4>Arguments</h4>


<dl>
<dt><code>dtype</code></dt><dd><p>The data type for all the calculations and defined
tensors.</p>
</dd>
</dl>



<hr>
<h2 id='Converter'>Converter of an artificial neural network</h2><span id='topic+Converter'></span>

<h3>Description</h3>

<p>This class analyzes a passed neural network and stores its internal
structure and the individual layers by converting the entire network into an
<code><a href="torch.html#topic+nn_module">nn_module</a></code>. With the help of this converter, many
methods for interpreting the behavior of neural networks are provided, which
give a better understanding of the whole model or individual predictions.
You can use models from the following libraries:
</p>

<ul>
<li> <p><code>torch</code> (<code><a href="torch.html#topic+nn_sequential">nn_sequential</a></code>)
</p>
</li>
<li> <p><code><a href="keras.html#topic+keras">keras</a></code> (<code><a href="keras.html#topic+keras_model">keras_model</a></code>,
<code><a href="keras.html#topic+keras_model_sequential">keras_model_sequential</a></code>),
</p>
</li>
<li> <p><code><a href="neuralnet.html#topic+neuralnet">neuralnet</a></code>
</p>
</li></ul>

<p>Furthermore, a model can be passed as a list (see
<code>vignette("detailed_overview", package = "innsight")</code> or the
<a href="https://bips-hb.github.io/innsight/articles/detailed_overview.html#model-as-named-list">website</a>).
</p>
<p>The R6 class can also be initialized using the <code><a href="#topic+convert">convert</a></code> function
as a helper function so that no prior knowledge of R6 classes is required.
</p>


<h3>Details</h3>

<p>In order to better understand and analyze the prediction of a neural
network, the preactivation or other information of the individual layers,
which are not stored in an ordinary forward pass, are often required. For
this reason, a given neural network is converted into a torch-based neural
network, which provides all the necessary information for an interpretation.
The converted torch model is stored in the field <code>model</code> and is an instance
of <code><a href="#topic+ConvertedModel">ConvertedModel</a></code>.
However, before the torch model is created, all relevant details of the
passed model are extracted into a named list. This list can be saved
in complete form in the <code>model_as_list</code> field with the argument
<code>save_model_as_list</code>, but this may consume a lot of memory for large
networks and is not done by default. Also, this named list can again be
used as a passed model for the class <code>Converter</code>, which will be described
in more detail in the section 'Implemented Libraries'.
</p>


<h4>Implemented methods</h4>

<p>An object of the Converter class can be applied to the
following methods:
</p>

<ul>
<li> <p><em>Layerwise Relevance Propagation</em> (<a href="#topic+LRP">LRP</a>), Bach et al. (2015)
</p>
</li>
<li> <p><em>Deep Learning Important Features</em> (<a href="#topic+DeepLift">DeepLift</a>), Shrikumar et al. (2017)
</p>
</li>
<li> <p><em><a href="#topic+DeepSHAP">DeepSHAP</a></em>, Lundberg et al. (2017)
</p>
</li>
<li> <p><em><a href="#topic+SmoothGrad">SmoothGrad</a></em> including <em>SmoothGrad<code class="reqn">\times</code>Input</em>, Smilkov et al. (2017)
</p>
</li>
<li> <p><em>Vanilla <a href="#topic+Gradient">Gradient</a></em> including <em>Gradient<code class="reqn">\times</code>Input</em>
</p>
</li>
<li> <p><em>Integrated gradients</em> (<a href="#topic+IntegratedGradient">IntegratedGradient</a>), Sundararajan et al. (2017)
</p>
</li>
<li> <p><em>Expected gradients</em> (<a href="#topic+ExpectedGradient">ExpectedGradient</a>), Erion et al. (2021)
</p>
</li>
<li> <p><em><a href="#topic+ConnectionWeights">ConnectionWeights</a></em>, Olden et al. (2004)
</p>
</li>
<li> <p><em>Local interpretable model-agnostic explanation (<a href="#topic+LIME">LIME</a>)</em>, Ribeiro et al. (2016)
</p>
</li>
<li> <p><em>Shapley values (<a href="#topic+SHAP">SHAP</a>)</em>, Lundberg et al. (2017)
</p>
</li></ul>




<h4>Implemented libraries</h4>

<p>The converter is implemented for models from the libraries
<code><a href="torch.html#topic+nn_sequential">nn_sequential</a></code>,
<code><a href="neuralnet.html#topic+neuralnet">neuralnet</a></code> and <code><a href="keras.html#topic+keras">keras</a></code>. But you
can also write a wrapper for other libraries because a model can be passed
as a named list which is described in detail in the vignette &quot;In-depth
explanation&quot; <br />
(see <code>vignette("detailed_overview", package = "innsight")</code> or the
<a href="https://bips-hb.github.io/innsight/articles/detailed_overview.html#model-as-named-list">website</a>).
</p>



<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>model</code></dt><dd><p>(<code><a href="#topic+ConvertedModel">ConvertedModel</a></code>)<br />
The converted neural network based on the torch module <a href="#topic+ConvertedModel">ConvertedModel</a>.<br /></p>
</dd>
<dt><code>input_dim</code></dt><dd><p>(<code>list</code>)<br />
A list of the input dimensions of each input layer. Since
internally the &quot;channels first&quot; format is used for all calculations, the
input shapes are already in this format. In addition, the batch
dimension isn't included, e.g., for an input layer of shape <code style="white-space: pre;">&#8288;c(*,32,32,3)&#8288;</code>
with channels in the last axis you get <code>list(c(3,32,32))</code>.<br /></p>
</dd>
<dt><code>input_names</code></dt><dd><p>(<code>list</code>)<br />
A list with the names as factors for each input
dimension of the shape as stored in the field <code>input_dim</code>.<br /></p>
</dd>
<dt><code>output_dim</code></dt><dd><p>(<code>list</code>)<br />
A list of the output dimensions of each output layer.<br /></p>
</dd>
<dt><code>output_names</code></dt><dd><p>(<code>list</code>)<br /> A list with the names as factors for each
output dimension of shape as stored in the field <code>output_dim</code>.<br /></p>
</dd>
<dt><code>model_as_list</code></dt><dd><p>(<code>list</code>)<br />
The model stored in a named list (see details for more
information). By default, the entry <code>model_as_list$layers</code> is deleted
because it may require a lot of memory for large networks. However, with
the argument <code>save_model_as_list</code> this can be saved anyway.<br /></p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-Converter-new"><code>Converter$new()</code></a>
</p>
</li>
<li> <p><a href="#method-Converter-print"><code>Converter$print()</code></a>
</p>
</li>
<li> <p><a href="#method-Converter-clone"><code>Converter$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-Converter-new"></a>



<h4>Method <code>new()</code></h4>

<p>Create a new <a href="#topic+Converter">Converter</a> object for a given neural network. When initialized,
the model is inspected, converted as a list and then the a
torch-converted model (<a href="#topic+ConvertedModel">ConvertedModel</a>) is created and stored in
the field <code>model</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>Converter$new(
  model,
  input_dim = NULL,
  input_names = NULL,
  output_names = NULL,
  dtype = "float",
  save_model_as_list = FALSE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>model</code></dt><dd><p>(<code><a href="torch.html#topic+nn_sequential">nn_sequential</a></code>, <code><a href="keras.html#topic+keras_model">keras_model</a></code>,
<code><a href="neuralnet.html#topic+neuralnet">neuralnet</a></code> or <code>list</code>)<br />
A trained neural network for classification or regression
tasks to be interpreted. Only models from the following types or
packages are allowed: <code><a href="torch.html#topic+nn_sequential">nn_sequential</a></code>,
<code><a href="keras.html#topic+keras_model">keras_model</a></code>,
<code><a href="keras.html#topic+keras_model_sequential">keras_model_sequential</a></code>,
<code><a href="neuralnet.html#topic+neuralnet">neuralnet</a></code> or a named list (see details).</p>
</dd>
<dt><code>input_dim</code></dt><dd><p>(<code>integer</code> or <code>list</code>)<br />
The model input dimension excluding the batch
dimension. If there is only one input layer it can be specified as
a vector, otherwise use a list of the shapes of the
individual input layers.<br />
<em>Note:</em> This argument is only necessary for <code>torch::nn_sequential</code>,
for all others it is automatically extracted from the passed model
and used for internal checks. In addition, the input dimension
<code>input_dim</code> has to be in the format &quot;channels first&quot;.<br /></p>
</dd>
<dt><code>input_names</code></dt><dd><p>(<code>character</code>, <code>factor</code> or <code>list</code>)<br />
The input names of the model excluding the batch dimension. For a model
with a single input layer and input axis (e.g., for tabular data), the
input names can be specified as a character vector or factor, e.g.,
for a dense layer with 3 input features use <code>c("X1", "X2", "X3")</code>. If
the model input consists of multiple axes (e.g., for signal and
image data), use a list of character vectors or factors for each axis
in the format &quot;channels first&quot;, e.g., use
<code>list(c("C1", "C2"), c("L1","L2","L3","L4","L5"))</code> for a 1D
convolutional input layer with signal length 4 and 2 channels. For
models with multiple input layers, use a list of the upper ones for each
layer.<br />
<em>Note:</em> This argument is optional and otherwise the names are
generated automatically. But if this argument is set, all found
input names in the passed model will be disregarded.<br /></p>
</dd>
<dt><code>output_names</code></dt><dd><p>(<code>character</code>, <code>factor</code> or <code>list</code>)<br />
A character vector with the names for the output dimensions
excluding the batch dimension, e.g., for a model with 3 output nodes use
<code>c("Y1", "Y2", "Y3")</code>. Instead of a character
vector you can also use a factor to set an order for the plots. If the
model has multiple output layers, use a list of the upper ones.<br />
<em>Note:</em> This argument is optional and otherwise the names are
generated automatically. But if this argument is set, all found
output names in the passed model will be disregarded.<br /></p>
</dd>
<dt><code>dtype</code></dt><dd><p>(<code>character(1)</code>)<br />
The data type for the calculations. Use
either <code>'float'</code> for <a href="torch.html#topic+torch_dtype">torch::torch_float</a> or <code>'double'</code> for
<a href="torch.html#topic+torch_dtype">torch::torch_double</a>.<br /></p>
</dd>
<dt><code>save_model_as_list</code></dt><dd><p>(<code>logical(1)</code>)<br />
This logical value specifies whether the
passed model should be stored as a list. This list can take
a lot of memory for large networks, so by default the model is not
stored as a list (<code>FALSE</code>).<br /></p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>A new instance of the R6 class <code>Converter</code>.
</p>


<hr>
<a id="method-Converter-print"></a>



<h4>Method <code>print()</code></h4>

<p>Print a summary of the <code>Converter</code> object. This summary contains the
individual fields and in particular the torch-converted model
(<a href="#topic+ConvertedModel">ConvertedModel</a>) with the layers.
</p>


<h5>Usage</h5>

<div class="r"><pre>Converter$print()</pre></div>



<h5>Returns</h5>

<p>Returns the <code>Converter</code> object invisibly via <code><a href="base.html#topic+invisible">base::invisible</a></code>.
</p>


<hr>
<a id="method-Converter-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>Converter$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>References</h3>


<ul>
<li><p> J. D. Olden et al. (2004) <em>An accurate comparison of methods for
quantifying variable importance in artificial neural networks using
simulated data.</em> Ecological Modelling 178, p. 389â€“397
</p>
</li>
<li><p> S. Bach et al. (2015) <em>On pixel-wise explanations for non-linear
classifier decisions by layer-wise relevance propagation.</em> PLoS ONE 10,
p. 1-46
</p>
</li>
<li><p> M. T. Ribeiro et al. (2016) <em>&quot;Why should I trust you?&quot;: Explaining
the predictions of any classifier.</em> KDD 2016, p. 1135-1144
</p>
</li>
<li><p> A. Shrikumar et al. (2017) <em>Learning important features through
propagating activation differences.</em>  ICML 2017, p. 4844-4866
</p>
</li>
<li><p> D. Smilkov et al. (2017) <em>SmoothGrad: removing noise by adding noise.</em>
CoRR, abs/1706.03825
M. Sundararajan et al. (2017) <em>Axiomatic attribution for deep networks.</em>
ICML 2017, p.3319-3328
</p>
</li>
<li><p> S. Lundberg et al. (2017) <em>A unified approach to interpreting model
predictions.</em> NIPS 2017, p. 4768-4777
</p>
</li>
<li><p> G. Erion et al. (2021) <em>Improving performance of deep learning models
with axiomatic attribution priors and expected gradients.</em> Nature Machine
Intelligence 3, p. 620-631
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
#----------------------- Example 1: Torch ----------------------------------
library(torch)

model &lt;- nn_sequential(
  nn_linear(5, 10),
  nn_relu(),
  nn_linear(10, 2, bias = FALSE),
  nn_softmax(dim = 2)
)
data &lt;- torch_randn(25, 5)

# Convert the model (for torch models is 'input_dim' required!)
converter &lt;- Converter$new(model, input_dim = c(5))

# You can also use the helper function `convert()` for initializing a
# Converter object
converter &lt;- convert(model, input_dim = c(5))

# Get the converted model stored in the field 'model'
converted_model &lt;- converter$model

# Test it with the original model
mean(abs(converted_model(data)[[1]] - model(data)))


#----------------------- Example 2: Neuralnet ------------------------------
if (require("neuralnet")) {
  library(neuralnet)
  data(iris)

  # Train a neural network
  nn &lt;- neuralnet((Species == "setosa") ~ Petal.Length + Petal.Width,
    iris,
    linear.output = FALSE,
    hidden = c(3, 2), act.fct = "tanh", rep = 1
  )

  # Convert the model
  converter &lt;- convert(nn)

  # Print all the layers
  converter$model$modules_list
}


#----------------------- Example 3: Keras ----------------------------------
if (require("keras") &amp; keras::is_keras_available()) {
  library(keras)

  # Make sure keras is installed properly
  is_keras_available()

  # Define a keras model
  model &lt;- keras_model_sequential() %&gt;%
    layer_conv_2d(
      input_shape = c(32, 32, 3), kernel_size = 8, filters = 8,
      activation = "relu", padding = "same") %&gt;%
    layer_conv_2d(
      kernel_size = 8, filters = 4,
      activation = "tanh", padding = "same") %&gt;%
    layer_conv_2d(
      kernel_size = 4, filters = 2,
      activation = "relu", padding = "same") %&gt;%
    layer_flatten() %&gt;%
    layer_dense(units = 64, activation = "relu") %&gt;%
    layer_dense(units = 1, activation = "sigmoid")

  # Convert this model and save model as list
  converter &lt;- convert(model, save_model_as_list = TRUE)

  # Print the converted model as a named list
  str(converter$model_as_list, max.level = 1)
}


#----------------------- Example 4: List  ----------------------------------

# Define a model

model &lt;- list()
model$input_dim &lt;- 5
model$input_names &lt;- list(c("Feat1", "Feat2", "Feat3", "Feat4", "Feat5"))
model$input_nodes &lt;- c(1)
model$output_dim &lt;- 2
model$output_names &lt;- list(c("Cat", "no-Cat"))
model$output_nodes &lt;- c(2)
model$layers$Layer_1 &lt;-
  list(
    type = "Dense",
    weight = matrix(rnorm(5 * 20), 20, 5),
    bias = rnorm(20),
    activation_name = "tanh",
    dim_in = 5,
    dim_out = 20,
    input_layers = 0, # '0' means model input layer
    output_layers = 2
  )
model$layers$Layer_2 &lt;-
  list(
    type = "Dense",
    weight = matrix(rnorm(20 * 2), 2, 20),
    bias = rnorm(2),
    activation_name = "softmax",
    input_layers = 1,
    output_layers = -1 # '-1' means model output layer
    #dim_in = 20, # These values are optional, but
    #dim_out = 2  # useful for internal checks
  )

# Convert the model
converter &lt;- convert(model)

# Get the model as a torch::nn_module
torch_model &lt;- converter$model

# You can use it as a normal torch model
x &lt;- torch::torch_randn(3, 5)
torch_model(x)

</code></pre>

<hr>
<h2 id='DeepLift'>Deep learning important features (DeepLift)</h2><span id='topic+DeepLift'></span>

<h3>Description</h3>

<p>This is an implementation of the <em>deep learning important features
(DeepLift)</em> algorithm introduced by Shrikumar et al. (2017). It's a local
method for interpreting a single element <code class="reqn">x</code> of the dataset concerning
a reference value <code class="reqn">x'</code> and returns the contribution of each input
feature from the difference of the output (<code class="reqn">y=f(x)</code>) and reference
output (<code class="reqn">y'=f(x')</code>) prediction. The basic idea of this method is to
decompose the difference-from-reference prediction with respect to the
input features, i.e.,
</p>
<p style="text-align: center;"><code class="reqn">\Delta y = y - y'  = \sum_i C(x_i).</code>
</p>

<p>Compared to <em>Layer-wise relevance propagation</em> (see <a href="#topic+LRP">LRP</a>), the
DeepLift method is an exact decomposition and not an approximation, so we
get real contributions of the input features to the
difference-from-reference prediction. There are two ways to handle
activation functions: the <em>Rescale</em> rule (<code>'rescale'</code>) and
<em>RevealCancel</em> rule (<code>'reveal_cancel'</code>).
</p>
<p>The R6 class can also be initialized using the <code><a href="#topic+run_deeplift">run_deeplift</a></code> function
as a helper function so that no prior knowledge of R6 classes is required.
</p>


<h3>Super class</h3>

<p><code><a href="#topic+InterpretingMethod">innsight::InterpretingMethod</a></code> -&gt; <code>DeepLift</code>
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>x_ref</code></dt><dd><p>(<code>list</code>)<br />
The reference input for the DeepLift method. This value is stored as a list
of <code>torch_tensor</code>s of shape <em>(1, dim_in)</em> for each input layer.<br /></p>
</dd>
<dt><code>rule_name</code></dt><dd><p>(<code>character(1)</code>)<br />
Name of the applied rule to calculate the contributions.
Either <code>'rescale'</code> or <code>'reveal_cancel'</code>.<br /></p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-DeepLift-new"><code>DeepLift$new()</code></a>
</p>
</li>
<li> <p><a href="#method-DeepLift-clone"><code>DeepLift$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="get_result"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-get_result'><code>innsight::InterpretingMethod$get_result()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="plot"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-plot'><code>innsight::InterpretingMethod$plot()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="plot_global"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-plot_global'><code>innsight::InterpretingMethod$plot_global()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="print"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-print'><code>innsight::InterpretingMethod$print()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-DeepLift-new"></a>



<h4>Method <code>new()</code></h4>

<p>Create a new instance of the <code>DeepLift</code> R6 class. When initialized,
the method <em>DeepLift</em> is applied to the given data and the results are stored in
the field <code>result</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>DeepLift$new(
  converter,
  data,
  channels_first = TRUE,
  output_idx = NULL,
  output_label = NULL,
  ignore_last_act = TRUE,
  rule_name = "rescale",
  x_ref = NULL,
  winner_takes_all = TRUE,
  verbose = interactive(),
  dtype = "float"
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>converter</code></dt><dd><p>(<code><a href="#topic+Converter">Converter</a></code>)<br />
An instance of the <code>Converter</code> class that includes the
torch-converted model and some other model-specific attributes. See
<code><a href="#topic+Converter">Converter</a></code> for details.<br /></p>
</dd>
<dt><code>data</code></dt><dd><p>(<code><a href="base.html#topic+array">array</a></code>, <code><a href="base.html#topic+data.frame">data.frame</a></code>, <code><a href="torch.html#topic+torch_tensor">torch_tensor</a></code> or <code>list</code>)<br />
The data to which the method is to be applied. These must
have the same format as the input data of the passed model to the
converter object. This means either
</p>

<ul>
<li><p> an <code>array</code>, <code>data.frame</code>, <code>torch_tensor</code> or array-like format of
size <em>(batch_size, dim_in)</em>, if e.g., the model has only one input layer, or
</p>
</li>
<li><p> a <code>list</code> with the corresponding input data (according to the
upper point) for each of the input layers.<br />
</p>
</li></ul>
</dd>
<dt><code>channels_first</code></dt><dd><p>(<code>logical(1)</code>)<br />
The channel position of the given data (argument
<code>data</code>). If <code>TRUE</code>, the channel axis is placed at the second position
between the batch size and the rest of the input axes, e.g.,
<code>c(10,3,32,32)</code> for a batch of ten images with three channels and a
height and width of 32 pixels. Otherwise (<code>FALSE</code>), the channel axis
is at the last position, i.e., <code>c(10,32,32,3)</code>. If the data
has no channel axis, use the default value <code>TRUE</code>.<br /></p>
</dd>
<dt><code>output_idx</code></dt><dd><p>(<code>integer</code>, <code>list</code> or <code>NULL</code>)<br />
These indices specify the output nodes for which
the method is to be applied. In order to allow models with multiple
output layers, there are the following possibilities to select
the indices of the output nodes in the individual output layers:
</p>

<ul>
<li><p> An <code>integer</code> vector of indices: If the model has only one output
layer, the values correspond to the indices of the output nodes, e.g.,
<code>c(1,3,4)</code> for the first, third and fourth output node. If there are
multiple output layers, the indices of the output nodes from the first
output layer are considered.
</p>
</li>
<li><p> A <code>list</code> of <code>integer</code> vectors of indices: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired indices of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of indices, e.g., <code>list(NULL, c(1,3))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br />
</p>
</li></ul>
</dd>
<dt><code>output_label</code></dt><dd><p>(<code>character</code>, <code>factor</code>, <code>list</code> or <code>NULL</code>)<br />
These values specify the output nodes for which
the method is to be applied. Only values that were previously passed with
the argument <code>output_names</code> in the <code>converter</code> can be used. In order to
allow models with multiple
output layers, there are the following possibilities to select
the names of the output nodes in the individual output layers:
</p>

<ul>
<li><p> A <code>character</code> vector or <code>factor</code> of labels: If the model has only one output
layer, the values correspond to the labels of the output nodes named in the
passed <code>Converter</code> object, e.g.,
<code>c("a", "c", "d")</code> for the first, third and fourth output node if the
output names are <code>c("a", "b", "c", "d")</code>. If there are
multiple output layers, the names of the output nodes from the first
output layer are considered.
</p>
</li>
<li><p> A <code>list</code> of <code>charactor</code>/<code>factor</code> vectors of labels: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired labels of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of labels, e.g., <code>list(NULL, c("a", "c"))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br />
</p>
</li></ul>
</dd>
<dt><code>ignore_last_act</code></dt><dd><p>(<code>logical(1)</code>)<br />
Set this logical value to include the last
activation functions for each output layer, or not (default: <code>TRUE</code>).
In practice, the last activation (especially for softmax activation) is
often omitted.<br /></p>
</dd>
<dt><code>rule_name</code></dt><dd><p>(<code>character(1)</code>)<br />
Name of the applied rule to calculate the
contributions. Use either <code>'rescale'</code> or <code>'reveal_cancel'</code>. <br /></p>
</dd>
<dt><code>x_ref</code></dt><dd><p>(<code><a href="base.html#topic+array">array</a></code>, <code><a href="base.html#topic+data.frame">data.frame</a></code>, <code><a href="torch.html#topic+torch_tensor">torch_tensor</a></code> or <code>list</code>)<br />
The reference input for the DeepLift method. This value
must have the same format as the input data of the passed model to the
converter object. This means either
</p>

<ul>
<li><p> an <code>array</code>, <code>data.frame</code>, <code>torch_tensor</code> or array-like format of
size <em>(1, dim_in)</em>, if e.g., the model has only one input layer, or
</p>
</li>
<li><p> a <code>list</code> with the corresponding input data (according to the upper point)
for each of the input layers.
</p>
</li>
<li><p> It is also possible to use the default value <code>NULL</code> to take only
zeros as reference input.<br />
</p>
</li></ul>
</dd>
<dt><code>winner_takes_all</code></dt><dd><p>(<code>logical(1)</code>)<br />
This logical argument is only relevant for MaxPooling
layers and is otherwise ignored. With this layer type, it is possible that
the position of the maximum values in the pooling kernel of the normal input
<code class="reqn">x</code> and the reference input <code class="reqn">x'</code> may not match, which leads to a
violation of the summation-to-delta property. To overcome this problem,
another variant is implemented, which treats a MaxPooling layer as an
AveragePooling layer in the backward pass only, leading to an uniform
distribution of the upper-layer contribution to the lower layer.<br /></p>
</dd>
<dt><code>verbose</code></dt><dd><p>(<code>logical(1)</code>)<br />
This logical argument determines whether a progress bar is
displayed for the calculation of the method or not. The default value is
the output of the primitive R function <code><a href="base.html#topic+interactive">interactive()</a></code>.<br /></p>
</dd>
<dt><code>dtype</code></dt><dd><p>(<code>character(1)</code>)<br />
The data type for the calculations. Use
either <code>'float'</code> for <a href="torch.html#topic+torch_float">torch_float</a> or <code>'double'</code> for
<a href="torch.html#topic+torch_double">torch_double</a>.<br /></p>
</dd>
</dl>

</div>


<hr>
<a id="method-DeepLift-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>DeepLift$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>References</h3>

<p>A. Shrikumar et al. (2017) <em>Learning important features through
propagating activation differences.</em>  ICML 2017, p. 4844-4866
</p>


<h3>See Also</h3>

<p>Other methods: 
<code><a href="#topic+ConnectionWeights">ConnectionWeights</a></code>,
<code><a href="#topic+DeepSHAP">DeepSHAP</a></code>,
<code><a href="#topic+ExpectedGradient">ExpectedGradient</a></code>,
<code><a href="#topic+Gradient">Gradient</a></code>,
<code><a href="#topic+IntegratedGradient">IntegratedGradient</a></code>,
<code><a href="#topic+LIME">LIME</a></code>,
<code><a href="#topic+LRP">LRP</a></code>,
<code><a href="#topic+SHAP">SHAP</a></code>,
<code><a href="#topic+SmoothGrad">SmoothGrad</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#----------------------- Example 1: Torch ----------------------------------
library(torch)

# Create nn_sequential model and data
model &lt;- nn_sequential(
  nn_linear(5, 12),
  nn_relu(),
  nn_linear(12, 2),
  nn_softmax(dim = 2)
)
data &lt;- torch_randn(25, 5)
ref &lt;- torch_randn(1, 5)

# Create Converter using the helper function `convert`
converter &lt;- convert(model, input_dim = c(5))

# Apply method DeepLift
deeplift &lt;- DeepLift$new(converter, data, x_ref = ref)

# You can also use the helper function `run_deeplift` for initializing
# an R6 DeepLift object
deeplift &lt;- run_deeplift(converter, data, x_ref = ref)

# Print the result as a torch tensor for first two data points
get_result(deeplift, "torch.tensor")[1:2]

# Plot the result for both classes
plot(deeplift, output_idx = 1:2)

# Plot the boxplot of all datapoints and for both classes
boxplot(deeplift, output_idx = 1:2)

# ------------------------- Example 2: Neuralnet ---------------------------
if (require("neuralnet")) {
  library(neuralnet)
  data(iris)

  # Train a neural network
  nn &lt;- neuralnet((Species == "setosa") ~ Petal.Length + Petal.Width,
    iris,
    linear.output = FALSE,
    hidden = c(3, 2), act.fct = "tanh", rep = 1
  )

  # Convert the model
  converter &lt;- convert(nn)

  # Apply DeepLift with rescale-rule and a reference input of the feature
  # means
  x_ref &lt;- matrix(colMeans(iris[, c(3, 4)]), nrow = 1)
  deeplift_rescale &lt;- run_deeplift(converter, iris[, c(3, 4)], x_ref = x_ref)

  # Get the result as a dataframe and show first 5 rows
  get_result(deeplift_rescale, type = "data.frame")[1:5, ]

  # Plot the result for the first datapoint in the data
  plot(deeplift_rescale, data_idx = 1)

  # Plot the result as boxplots
  boxplot(deeplift_rescale)
}


# ------------------------- Example 3: Keras -------------------------------
if (require("keras") &amp; keras::is_keras_available()) {
  library(keras)

  # Make sure keras is installed properly
  is_keras_available()

  data &lt;- array(rnorm(10 * 32 * 32 * 3), dim = c(10, 32, 32, 3))

  model &lt;- keras_model_sequential()
  model %&gt;%
    layer_conv_2d(
      input_shape = c(32, 32, 3), kernel_size = 8, filters = 8,
      activation = "softplus", padding = "valid") %&gt;%
    layer_conv_2d(
      kernel_size = 8, filters = 4, activation = "tanh",
      padding = "same") %&gt;%
    layer_conv_2d(
      kernel_size = 4, filters = 2, activation = "relu",
      padding = "valid") %&gt;%
    layer_flatten() %&gt;%
    layer_dense(units = 64, activation = "relu") %&gt;%
    layer_dense(units = 16, activation = "relu") %&gt;%
    layer_dense(units = 2, activation = "softmax")

  # Convert the model
  converter &lt;- convert(model)

  # Apply the DeepLift method with reveal-cancel rule
  deeplift_revcancel &lt;- run_deeplift(converter, data,
    channels_first = FALSE,
    rule_name = "reveal_cancel"
  )

  # Plot the result for the first image and both classes
  plot(deeplift_revcancel, output_idx = 1:2)

  # Plot the pixel-wise median reelvance image
  plot_global(deeplift_revcancel, output_idx = 1)
}


#------------------------- Plotly plots ------------------------------------
if (require("plotly")) {
  # You can also create an interactive plot with plotly.
  # This is a suggested package, so make sure that it is installed
  library(plotly)
  boxplot(deeplift, as_plotly = TRUE)
}

</code></pre>

<hr>
<h2 id='DeepSHAP'>Deep Shapley additive explanations (DeepSHAP)</h2><span id='topic+DeepSHAP'></span>

<h3>Description</h3>

<p>The <em>DeepSHAP</em> method extends the <code><a href="#topic+DeepLift">DeepLift</a></code> technique by not only
considering a single reference value but by calculating the average
from several, ideally representative reference values at each layer. The
obtained feature-wise results are approximate Shapley values for the
chosen output, where the conditional expectation is computed using these
different reference values, i.e., the <em>DeepSHAP</em> method decompose the
difference from the prediction and the mean prediction <code class="reqn">f(x) - E[f(\tilde{x})]</code>
in feature-wise effects. The reference values can be passed by the argument
<code>data_ref</code>.
</p>
<p>The R6 class can also be initialized using the <code><a href="#topic+run_deepshap">run_deepshap</a></code> function
as a helper function so that no prior knowledge of R6 classes is required.
</p>


<h3>Super class</h3>

<p><code><a href="#topic+InterpretingMethod">innsight::InterpretingMethod</a></code> -&gt; <code>DeepSHAP</code>
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>rule_name</code></dt><dd><p>(<code>character(1)</code>)<br />
Name of the applied rule to calculate the contributions.
Either <code>'rescale'</code> or <code>'reveal_cancel'</code>.<br /></p>
</dd>
<dt><code>data_ref</code></dt><dd><p>(<code>list</code>)<br />
The passed reference dataset for estimating the conditional expectation
as a <code>list</code> of <code>torch_tensors</code> in the selected
data format (field <code>dtype</code>) matching the corresponding shapes of the
individual input layers. Besides, the channel axis is moved to the
second position after the batch size because internally only the
format <em>channels first</em> is used.<br /></p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-DeepSHAP-new"><code>DeepSHAP$new()</code></a>
</p>
</li>
<li> <p><a href="#method-DeepSHAP-clone"><code>DeepSHAP$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="get_result"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-get_result'><code>innsight::InterpretingMethod$get_result()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="plot"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-plot'><code>innsight::InterpretingMethod$plot()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="plot_global"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-plot_global'><code>innsight::InterpretingMethod$plot_global()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="print"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-print'><code>innsight::InterpretingMethod$print()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-DeepSHAP-new"></a>



<h4>Method <code>new()</code></h4>

<p>Create a new instance of the <code>DeepSHAP</code> R6 class. When initialized,
the method <em>DeepSHAP</em> is applied to the given data and the results are
stored in the field <code>result</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>DeepSHAP$new(
  converter,
  data,
  channels_first = TRUE,
  output_idx = NULL,
  output_label = NULL,
  ignore_last_act = TRUE,
  rule_name = "rescale",
  data_ref = NULL,
  limit_ref = 100,
  winner_takes_all = TRUE,
  verbose = interactive(),
  dtype = "float"
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>converter</code></dt><dd><p>(<code><a href="#topic+Converter">Converter</a></code>)<br />
An instance of the <code>Converter</code> class that includes the
torch-converted model and some other model-specific attributes. See
<code><a href="#topic+Converter">Converter</a></code> for details.<br /></p>
</dd>
<dt><code>data</code></dt><dd><p>(<code><a href="base.html#topic+array">array</a></code>, <code><a href="base.html#topic+data.frame">data.frame</a></code>, <code><a href="torch.html#topic+torch_tensor">torch_tensor</a></code> or <code>list</code>)<br />
The data to which the method is to be applied. These must
have the same format as the input data of the passed model to the
converter object. This means either
</p>

<ul>
<li><p> an <code>array</code>, <code>data.frame</code>, <code>torch_tensor</code> or array-like format of
size <em>(batch_size, dim_in)</em>, if e.g., the model has only one input layer, or
</p>
</li>
<li><p> a <code>list</code> with the corresponding input data (according to the
upper point) for each of the input layers.<br />
</p>
</li></ul>
</dd>
<dt><code>channels_first</code></dt><dd><p>(<code>logical(1)</code>)<br />
The channel position of the given data (argument
<code>data</code>). If <code>TRUE</code>, the channel axis is placed at the second position
between the batch size and the rest of the input axes, e.g.,
<code>c(10,3,32,32)</code> for a batch of ten images with three channels and a
height and width of 32 pixels. Otherwise (<code>FALSE</code>), the channel axis
is at the last position, i.e., <code>c(10,32,32,3)</code>. If the data
has no channel axis, use the default value <code>TRUE</code>.<br /></p>
</dd>
<dt><code>output_idx</code></dt><dd><p>(<code>integer</code>, <code>list</code> or <code>NULL</code>)<br />
These indices specify the output nodes for which
the method is to be applied. In order to allow models with multiple
output layers, there are the following possibilities to select
the indices of the output nodes in the individual output layers:
</p>

<ul>
<li><p> An <code>integer</code> vector of indices: If the model has only one output
layer, the values correspond to the indices of the output nodes, e.g.,
<code>c(1,3,4)</code> for the first, third and fourth output node. If there are
multiple output layers, the indices of the output nodes from the first
output layer are considered.
</p>
</li>
<li><p> A <code>list</code> of <code>integer</code> vectors of indices: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired indices of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of indices, e.g., <code>list(NULL, c(1,3))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br />
</p>
</li></ul>
</dd>
<dt><code>output_label</code></dt><dd><p>(<code>character</code>, <code>factor</code>, <code>list</code> or <code>NULL</code>)<br />
These values specify the output nodes for which
the method is to be applied. Only values that were previously passed with
the argument <code>output_names</code> in the <code>converter</code> can be used. In order to
allow models with multiple
output layers, there are the following possibilities to select
the names of the output nodes in the individual output layers:
</p>

<ul>
<li><p> A <code>character</code> vector or <code>factor</code> of labels: If the model has only one output
layer, the values correspond to the labels of the output nodes named in the
passed <code>Converter</code> object, e.g.,
<code>c("a", "c", "d")</code> for the first, third and fourth output node if the
output names are <code>c("a", "b", "c", "d")</code>. If there are
multiple output layers, the names of the output nodes from the first
output layer are considered.
</p>
</li>
<li><p> A <code>list</code> of <code>charactor</code>/<code>factor</code> vectors of labels: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired labels of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of labels, e.g., <code>list(NULL, c("a", "c"))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br />
</p>
</li></ul>
</dd>
<dt><code>ignore_last_act</code></dt><dd><p>(<code>logical(1)</code>)<br />
Set this logical value to include the last
activation functions for each output layer, or not (default: <code>TRUE</code>).
In practice, the last activation (especially for softmax activation) is
often omitted.<br /></p>
</dd>
<dt><code>rule_name</code></dt><dd><p>(<code>character(1)</code>)<br />
Name of the applied rule to calculate the
contributions. Use either <code>'rescale'</code> or <code>'reveal_cancel'</code>. <br /></p>
</dd>
<dt><code>data_ref</code></dt><dd><p>(<code><a href="base.html#topic+array">array</a></code>, <code><a href="base.html#topic+data.frame">data.frame</a></code>, <code><a href="torch.html#topic+torch_tensor">torch_tensor</a></code> or <code>list</code>)<br />
The reference data which is used to estimate the conditional expectation.
These must have the same format as the input data of the passed model to
the converter object. This means either
</p>

<ul>
<li><p> an <code>array</code>, <code>data.frame</code>, <code>torch_tensor</code> or array-like format of
size <em>(batch_size, dim_in)</em>, if e.g., the model has only one input layer, or
</p>
</li>
<li><p> a <code>list</code> with the corresponding input data (according to the
upper point) for each of the input layers.
</p>
</li>
<li><p> or <code>NULL</code> (default) to use only a zero baseline for the estimation.<br />
</p>
</li></ul>
</dd>
<dt><code>limit_ref</code></dt><dd><p>(<code>integer(1)</code>)<br />
This argument limits the number of instances taken from the reference
dataset <code>data_ref</code> so that only random <code>limit_ref</code> elements and not
the entire dataset are used to estimate the conditional expectation.
A too-large number can significantly increase the computation time.<br /></p>
</dd>
<dt><code>winner_takes_all</code></dt><dd><p>(<code>logical(1)</code>)<br />
This logical argument is only relevant for MaxPooling
layers and is otherwise ignored. With this layer type, it is possible that
the position of the maximum values in the pooling kernel of the normal input
<code class="reqn">x</code> and the reference input <code class="reqn">x'</code> may not match, which leads to a
violation of the summation-to-delta property. To overcome this problem,
another variant is implemented, which treats a MaxPooling layer as an
AveragePooling layer in the backward pass only, leading to an uniform
distribution of the upper-layer contribution to the lower layer.<br /></p>
</dd>
<dt><code>verbose</code></dt><dd><p>(<code>logical(1)</code>)<br />
This logical argument determines whether a progress bar is
displayed for the calculation of the method or not. The default value is
the output of the primitive R function <code><a href="base.html#topic+interactive">interactive()</a></code>.<br /></p>
</dd>
<dt><code>dtype</code></dt><dd><p>(<code>character(1)</code>)<br />
The data type for the calculations. Use
either <code>'float'</code> for <a href="torch.html#topic+torch_float">torch_float</a> or <code>'double'</code> for
<a href="torch.html#topic+torch_double">torch_double</a>.<br /></p>
</dd>
</dl>

</div>


<hr>
<a id="method-DeepSHAP-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>DeepSHAP$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>References</h3>

<p>S. Lundberg &amp; S. Lee (2017) <em>A unified approach to interpreting model
predictions.</em>  NIPS 2017, p. 4768â€“4777
</p>


<h3>See Also</h3>

<p>Other methods: 
<code><a href="#topic+ConnectionWeights">ConnectionWeights</a></code>,
<code><a href="#topic+DeepLift">DeepLift</a></code>,
<code><a href="#topic+ExpectedGradient">ExpectedGradient</a></code>,
<code><a href="#topic+Gradient">Gradient</a></code>,
<code><a href="#topic+IntegratedGradient">IntegratedGradient</a></code>,
<code><a href="#topic+LIME">LIME</a></code>,
<code><a href="#topic+LRP">LRP</a></code>,
<code><a href="#topic+SHAP">SHAP</a></code>,
<code><a href="#topic+SmoothGrad">SmoothGrad</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#----------------------- Example 1: Torch ----------------------------------
library(torch)

# Create nn_sequential model and data
model &lt;- nn_sequential(
  nn_linear(5, 12),
  nn_relu(),
  nn_linear(12, 2),
  nn_softmax(dim = 2)
)
data &lt;- torch_randn(25, 5)

# Create a reference dataset for the estimation of the conditional
# expectation
ref &lt;- torch_randn(5, 5)

# Create Converter
converter &lt;- convert(model, input_dim = c(5))

# Apply method DeepSHAP
deepshap &lt;- DeepSHAP$new(converter, data, data_ref = ref)

# You can also use the helper function `run_deepshap` for initializing
# an R6 DeepSHAP object
deepshap &lt;- run_deepshap(converter, data, data_ref = ref)

# Print the result as a torch tensor for first two data points
get_result(deepshap, "torch.tensor")[1:2]

# Plot the result for both classes
plot(deepshap, output_idx = 1:2)

# Plot the boxplot of all datapoints and for both classes
boxplot(deepshap, output_idx = 1:2)

# ------------------------- Example 2: Neuralnet ---------------------------
if (require("neuralnet")) {
  library(neuralnet)
  data(iris)

  # Train a neural network
  nn &lt;- neuralnet((Species == "setosa") ~ Petal.Length + Petal.Width,
    iris,
    linear.output = FALSE,
    hidden = c(3, 2), act.fct = "tanh", rep = 1
  )

  # Convert the model
  converter &lt;- convert(nn)

  # Apply DeepSHAP with rescale-rule and a 100 (default of `limit_ref`)
  # instances as the reference dataset
  deepshap &lt;- run_deepshap(converter, iris[, c(3, 4)],
                           data_ref = iris[, c(3, 4)])

  # Get the result as a dataframe and show first 5 rows
  get_result(deepshap, type = "data.frame")[1:5, ]

  # Plot the result for the first datapoint in the data
  plot(deepshap, data_idx = 1)

  # Plot the result as boxplots
  boxplot(deepshap)
}


# ------------------------- Example 3: Keras -------------------------------
if (require("keras") &amp; keras::is_keras_available()) {
  library(keras)

  # Make sure keras is installed properly
  is_keras_available()

  data &lt;- array(rnorm(10 * 32 * 32 * 3), dim = c(10, 32, 32, 3))

  model &lt;- keras_model_sequential()
  model %&gt;%
    layer_conv_2d(
      input_shape = c(32, 32, 3), kernel_size = 8, filters = 8,
      activation = "softplus", padding = "valid") %&gt;%
    layer_conv_2d(
      kernel_size = 8, filters = 4, activation = "tanh",
      padding = "same") %&gt;%
    layer_conv_2d(
      kernel_size = 4, filters = 2, activation = "relu",
      padding = "valid") %&gt;%
    layer_flatten() %&gt;%
    layer_dense(units = 64, activation = "relu") %&gt;%
    layer_dense(units = 16, activation = "relu") %&gt;%
    layer_dense(units = 2, activation = "softmax")

  # Convert the model
  converter &lt;- convert(model)

  # Apply the DeepSHAP method with zero baseline (wich is equivalent to
  # DeepLift with zero baseline)
  deepshap &lt;- run_deepshap(converter, data, channels_first = FALSE)

  # Plot the result for the first image and both classes
  plot(deepshap, output_idx = 1:2)

  # Plot the pixel-wise median of the results
  plot_global(deepshap, output_idx = 1)
}


#------------------------- Plotly plots ------------------------------------
if (require("plotly")) {
  # You can also create an interactive plot with plotly.
  # This is a suggested package, so make sure that it is installed
  library(plotly)
  boxplot(deepshap, as_plotly = TRUE)
}

</code></pre>

<hr>
<h2 id='ExpectedGradient'>Expected Gradients</h2><span id='topic+ExpectedGradient'></span>

<h3>Description</h3>

<p>The <em>Expected Gradients</em> method (Erion et al., 2021), also known as
<em>GradSHAP</em>, is a local feature attribution technique which extends the
<code><a href="#topic+IntegratedGradient">IntegratedGradient</a></code> method and provides approximate Shapley values. In
contrast to IntegratedGradient, it considers not only a single reference
value <code class="reqn">x'</code> but the whole distribution of reference values
<code class="reqn">X' \sim x'</code> and averages the IntegratedGradient values over this
distribution. Mathematically, the method can be described as follows:
</p>
<p style="text-align: center;"><code class="reqn">
E_{x'\sim X', \alpha \sim U(0,1)}[(x - x') \times \frac{\partial f(x' + \alpha (x - x'))}{\partial x}]
</code>
</p>

<p>The distribution of the reference values is specified with the argument
<code>data_ref</code>, of which <code>n</code> samples are taken at random for each instance
during the estimation.
</p>
<p>The R6 class can also be initialized using the <code><a href="#topic+run_expgrad">run_expgrad</a></code> function
as a helper function so that no prior knowledge of R6 classes is required.
</p>


<h3>Super classes</h3>

<p><code><a href="#topic+InterpretingMethod">innsight::InterpretingMethod</a></code> -&gt; <code><a href="#topic+GradientBased">innsight::GradientBased</a></code> -&gt; <code>ExpectedGradient</code>
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>n</code></dt><dd><p>(<code>integer(1)</code>)<br />
Number of samples from the distribution of reference values and number
of samples for the approximation of the integration path along
<code class="reqn">\alpha</code> (default: <code class="reqn">50</code>).<br /></p>
</dd>
<dt><code>data_ref</code></dt><dd><p>(<code>list</code>)<br />
The reference input for the ExpectedGradient method. This value is
stored as a list of <code>torch_tensor</code>s of shape <em>( , dim_in)</em> for each
input layer.<br /></p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-ExpectedGradient-new"><code>ExpectedGradient$new()</code></a>
</p>
</li>
<li> <p><a href="#method-ExpectedGradient-clone"><code>ExpectedGradient$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="get_result"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-get_result'><code>innsight::InterpretingMethod$get_result()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="plot"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-plot'><code>innsight::InterpretingMethod$plot()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="plot_global"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-plot_global'><code>innsight::InterpretingMethod$plot_global()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="print"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-print'><code>innsight::InterpretingMethod$print()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-ExpectedGradient-new"></a>



<h4>Method <code>new()</code></h4>

<p>Create a new instance of the <code>ExpectedGradient</code> R6 class. When
initialized, the method <em>Expected Gradient</em> is applied to the given
data and baseline values and the results are stored in the field <code>result</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>ExpectedGradient$new(
  converter,
  data,
  data_ref = NULL,
  n = 50,
  channels_first = TRUE,
  output_idx = NULL,
  output_label = NULL,
  ignore_last_act = TRUE,
  verbose = interactive(),
  dtype = "float"
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>converter</code></dt><dd><p>(<code><a href="#topic+Converter">Converter</a></code>)<br />
An instance of the <code>Converter</code> class that includes the
torch-converted model and some other model-specific attributes. See
<code><a href="#topic+Converter">Converter</a></code> for details.<br /></p>
</dd>
<dt><code>data</code></dt><dd><p>(<code><a href="base.html#topic+array">array</a></code>, <code><a href="base.html#topic+data.frame">data.frame</a></code>, <code><a href="torch.html#topic+torch_tensor">torch_tensor</a></code> or <code>list</code>)<br />
The data to which the method is to be applied. These must
have the same format as the input data of the passed model to the
converter object. This means either
</p>

<ul>
<li><p> an <code>array</code>, <code>data.frame</code>, <code>torch_tensor</code> or array-like format of
size <em>(batch_size, dim_in)</em>, if e.g., the model has only one input layer, or
</p>
</li>
<li><p> a <code>list</code> with the corresponding input data (according to the
upper point) for each of the input layers.<br />
</p>
</li></ul>
</dd>
<dt><code>data_ref</code></dt><dd><p>(<code><a href="base.html#topic+array">array</a></code>, <code><a href="base.html#topic+data.frame">data.frame</a></code>, <code><a href="torch.html#topic+torch_tensor">torch_tensor</a></code> or <code>list</code>)<br />
The reference inputs for the ExpectedGradient method. This value
must have the same format as the input data of the passed model to the
converter object. This means either
</p>

<ul>
<li><p> an <code>array</code>, <code>data.frame</code>, <code>torch_tensor</code> or array-like format of
size <em>( , dim_in)</em>, if e.g., the model has only one input layer, or
</p>
</li>
<li><p> a <code>list</code> with the corresponding input data (according to the upper point)
for each of the input layers.
</p>
</li>
<li><p> It is also possible to use the default value <code>NULL</code> to take only
zeros as reference input.<br />
</p>
</li></ul>
</dd>
<dt><code>n</code></dt><dd><p>(<code>integer(1)</code>)<br />
Number of samples from the distribution of reference values and number
of samples for the approximation of the integration path along
<code class="reqn">\alpha</code> (default: <code class="reqn">50</code>).<br /></p>
</dd>
<dt><code>channels_first</code></dt><dd><p>(<code>logical(1)</code>)<br />
The channel position of the given data (argument
<code>data</code>). If <code>TRUE</code>, the channel axis is placed at the second position
between the batch size and the rest of the input axes, e.g.,
<code>c(10,3,32,32)</code> for a batch of ten images with three channels and a
height and width of 32 pixels. Otherwise (<code>FALSE</code>), the channel axis
is at the last position, i.e., <code>c(10,32,32,3)</code>. If the data
has no channel axis, use the default value <code>TRUE</code>.<br /></p>
</dd>
<dt><code>output_idx</code></dt><dd><p>(<code>integer</code>, <code>list</code> or <code>NULL</code>)<br />
These indices specify the output nodes for which
the method is to be applied. In order to allow models with multiple
output layers, there are the following possibilities to select
the indices of the output nodes in the individual output layers:
</p>

<ul>
<li><p> An <code>integer</code> vector of indices: If the model has only one output
layer, the values correspond to the indices of the output nodes, e.g.,
<code>c(1,3,4)</code> for the first, third and fourth output node. If there are
multiple output layers, the indices of the output nodes from the first
output layer are considered.
</p>
</li>
<li><p> A <code>list</code> of <code>integer</code> vectors of indices: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired indices of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of indices, e.g., <code>list(NULL, c(1,3))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br />
</p>
</li></ul>
</dd>
<dt><code>output_label</code></dt><dd><p>(<code>character</code>, <code>factor</code>, <code>list</code> or <code>NULL</code>)<br />
These values specify the output nodes for which
the method is to be applied. Only values that were previously passed with
the argument <code>output_names</code> in the <code>converter</code> can be used. In order to
allow models with multiple
output layers, there are the following possibilities to select
the names of the output nodes in the individual output layers:
</p>

<ul>
<li><p> A <code>character</code> vector or <code>factor</code> of labels: If the model has only one output
layer, the values correspond to the labels of the output nodes named in the
passed <code>Converter</code> object, e.g.,
<code>c("a", "c", "d")</code> for the first, third and fourth output node if the
output names are <code>c("a", "b", "c", "d")</code>. If there are
multiple output layers, the names of the output nodes from the first
output layer are considered.
</p>
</li>
<li><p> A <code>list</code> of <code>charactor</code>/<code>factor</code> vectors of labels: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired labels of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of labels, e.g., <code>list(NULL, c("a", "c"))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br />
</p>
</li></ul>
</dd>
<dt><code>ignore_last_act</code></dt><dd><p>(<code>logical(1)</code>)<br />
Set this logical value to include the last
activation functions for each output layer, or not (default: <code>TRUE</code>).
In practice, the last activation (especially for softmax activation) is
often omitted.<br /></p>
</dd>
<dt><code>verbose</code></dt><dd><p>(<code>logical(1)</code>)<br />
This logical argument determines whether a progress bar is
displayed for the calculation of the method or not. The default value is
the output of the primitive R function <code><a href="base.html#topic+interactive">interactive()</a></code>.<br /></p>
</dd>
<dt><code>dtype</code></dt><dd><p>(<code>character(1)</code>)<br />
The data type for the calculations. Use
either <code>'float'</code> for <a href="torch.html#topic+torch_float">torch_float</a> or <code>'double'</code> for
<a href="torch.html#topic+torch_double">torch_double</a>.<br /></p>
</dd>
</dl>

</div>


<hr>
<a id="method-ExpectedGradient-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>ExpectedGradient$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>References</h3>

<p>G. Erion et al. (2021) *Improving performance of deep learning models with *
<em>axiomatic attribution priors and expected gradients.</em> Nature Machine
Intelligence 3, pp. 620-631.
</p>


<h3>See Also</h3>

<p>Other methods: 
<code><a href="#topic+ConnectionWeights">ConnectionWeights</a></code>,
<code><a href="#topic+DeepLift">DeepLift</a></code>,
<code><a href="#topic+DeepSHAP">DeepSHAP</a></code>,
<code><a href="#topic+Gradient">Gradient</a></code>,
<code><a href="#topic+IntegratedGradient">IntegratedGradient</a></code>,
<code><a href="#topic+LIME">LIME</a></code>,
<code><a href="#topic+LRP">LRP</a></code>,
<code><a href="#topic+SHAP">SHAP</a></code>,
<code><a href="#topic+SmoothGrad">SmoothGrad</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#----------------------- Example 1: Torch ----------------------------------
library(torch)

# Create nn_sequential model and data
model &lt;- nn_sequential(
  nn_linear(5, 12),
  nn_relu(),
  nn_linear(12, 2),
  nn_softmax(dim = 2)
)
data &lt;- torch_randn(25, 5)
ref &lt;- torch_randn(1, 5)

# Create Converter
converter &lt;- convert(model, input_dim = c(5))

# Apply method IntegratedGradient
int_grad &lt;- IntegratedGradient$new(converter, data, x_ref = ref)

# You can also use the helper function `run_intgrad` for initializing
# an R6 IntegratedGradient object
int_grad &lt;- run_intgrad(converter, data, x_ref = ref)

# Print the result as a torch tensor for first two data points
get_result(int_grad, "torch.tensor")[1:2]

# Plot the result for both classes
plot(int_grad, output_idx = 1:2)

# Plot the boxplot of all datapoints and for both classes
boxplot(int_grad, output_idx = 1:2)

# ------------------------- Example 2: Neuralnet ---------------------------
if (require("neuralnet")) {
  library(neuralnet)
  data(iris)

  # Train a neural network
  nn &lt;- neuralnet((Species == "setosa") ~ Petal.Length + Petal.Width,
    iris,
    linear.output = FALSE,
    hidden = c(3, 2), act.fct = "tanh", rep = 1
  )

  # Convert the model
  converter &lt;- convert(nn)

  # Apply IntegratedGradient with a reference input of the feature means
  x_ref &lt;- matrix(colMeans(iris[, c(3, 4)]), nrow = 1)
  int_grad &lt;- run_intgrad(converter, iris[, c(3, 4)], x_ref = x_ref)

  # Get the result as a dataframe and show first 5 rows
  get_result(int_grad, type = "data.frame")[1:5, ]

  # Plot the result for the first datapoint in the data
  plot(int_grad, data_idx = 1)

  # Plot the result as boxplots
  boxplot(int_grad)
}


# ------------------------- Example 3: Keras -------------------------------
if (require("keras") &amp; keras::is_keras_available()) {
  library(keras)

  # Make sure keras is installed properly
  is_keras_available()

  data &lt;- array(rnorm(10 * 32 * 32 * 3), dim = c(10, 32, 32, 3))

  model &lt;- keras_model_sequential()
  model %&gt;%
    layer_conv_2d(
      input_shape = c(32, 32, 3), kernel_size = 8, filters = 8,
      activation = "softplus", padding = "valid") %&gt;%
    layer_conv_2d(
      kernel_size = 8, filters = 4, activation = "tanh",
      padding = "same") %&gt;%
    layer_conv_2d(
      kernel_size = 4, filters = 2, activation = "relu",
      padding = "valid") %&gt;%
    layer_flatten() %&gt;%
    layer_dense(units = 64, activation = "relu") %&gt;%
    layer_dense(units = 2, activation = "softmax")

  # Convert the model
  converter &lt;- convert(model)

  # Apply the IntegratedGradient method with a zero baseline and n = 20
  # iteration steps
  int_grad &lt;- run_intgrad(converter, data,
    channels_first = FALSE,
    n = 20
  )

  # Plot the result for the first image and both classes
  plot(int_grad, output_idx = 1:2)

  # Plot the pixel-wise median of the results
  plot_global(int_grad, output_idx = 1)
}


#------------------------- Plotly plots ------------------------------------
if (require("plotly")) {
  # You can also create an interactive plot with plotly.
  # This is a suggested package, so make sure that it is installed
  library(plotly)
  boxplot(int_grad, as_plotly = TRUE)
}

</code></pre>

<hr>
<h2 id='get_result'>Get the result of an interpretation method</h2><span id='topic+get_result'></span>

<h3>Description</h3>

<p>This is a generic S3 method for the R6 method
<code>InterpretingMethod$get_result()</code>. See the respective method described in
<code><a href="#topic+InterpretingMethod">InterpretingMethod</a></code> for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_result(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_result_+3A_x">x</code></td>
<td>
<p>An object of the class <code><a href="#topic+InterpretingMethod">InterpretingMethod</a></code> including the
subclasses <code><a href="#topic+Gradient">Gradient</a></code>, <code><a href="#topic+SmoothGrad">SmoothGrad</a></code>, <code><a href="#topic+LRP">LRP</a></code>, <code><a href="#topic+DeepLift">DeepLift</a></code>,
<code><a href="#topic+DeepSHAP">DeepSHAP</a></code>, <code><a href="#topic+IntegratedGradient">IntegratedGradient</a></code>, <code><a href="#topic+ExpectedGradient">ExpectedGradient</a></code> and
<code><a href="#topic+ConnectionWeights">ConnectionWeights</a></code>.</p>
</td></tr>
<tr><td><code id="get_result_+3A_...">...</code></td>
<td>
<p>Other arguments specified in the R6 method
<code>InterpretingMethod$get_result()</code>. See <code><a href="#topic+InterpretingMethod">InterpretingMethod</a></code> for details.</p>
</td></tr>
</table>

<hr>
<h2 id='Gradient'>Vanilla Gradient and Gradient<code class="reqn">\times</code>Input</h2><span id='topic+Gradient'></span>

<h3>Description</h3>

<p>This method computes the gradients (also known as <em>Vanilla Gradients</em>) of
the outputs with respect to the input variables, i.e., for all input
variable <code class="reqn">i</code> and output class <code class="reqn">j</code>
</p>
<p style="text-align: center;"><code class="reqn">d f(x)_j / d x_i.</code>
</p>

<p>If the argument <code>times_input</code> is <code>TRUE</code>, the gradients are multiplied by
the respective input value (<em>Gradient<code class="reqn">\times</code>Input</em>), i.e.,
</p>
<p style="text-align: center;"><code class="reqn">x_i * d f(x)_j / d x_i.</code>
</p>

<p>While the vanilla gradients emphasize prediction-sensitive features,
Gradient<code class="reqn">\times</code>Input is a decomposition of the output into feature-wise
effects based on the first-order Taylor decomposition.
</p>
<p>The R6 class can also be initialized using the <code><a href="#topic+run_grad">run_grad</a></code> function as a
helper function so that no prior knowledge of R6 classes is required.
</p>


<h3>Super classes</h3>

<p><code><a href="#topic+InterpretingMethod">innsight::InterpretingMethod</a></code> -&gt; <code><a href="#topic+GradientBased">innsight::GradientBased</a></code> -&gt; <code>Gradient</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-Gradient-new"><code>Gradient$new()</code></a>
</p>
</li>
<li> <p><a href="#method-Gradient-clone"><code>Gradient$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="get_result"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-get_result'><code>innsight::InterpretingMethod$get_result()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="plot"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-plot'><code>innsight::InterpretingMethod$plot()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="plot_global"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-plot_global'><code>innsight::InterpretingMethod$plot_global()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="print"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-print'><code>innsight::InterpretingMethod$print()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-Gradient-new"></a>



<h4>Method <code>new()</code></h4>

<p>Create a new instance of the <code>Gradient</code> R6 class. When initialized,
the method <em>Gradient</em> or <em>Gradient<code class="reqn">\times</code>Input</em> is applied to the
given data and the results are stored in the field <code>result</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>Gradient$new(
  converter,
  data,
  channels_first = TRUE,
  output_idx = NULL,
  output_label = NULL,
  ignore_last_act = TRUE,
  times_input = FALSE,
  verbose = interactive(),
  dtype = "float"
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>converter</code></dt><dd><p>(<code><a href="#topic+Converter">Converter</a></code>)<br />
An instance of the <code>Converter</code> class that includes the
torch-converted model and some other model-specific attributes. See
<code><a href="#topic+Converter">Converter</a></code> for details.<br /></p>
</dd>
<dt><code>data</code></dt><dd><p>(<code><a href="base.html#topic+array">array</a></code>, <code><a href="base.html#topic+data.frame">data.frame</a></code>, <code><a href="torch.html#topic+torch_tensor">torch_tensor</a></code> or <code>list</code>)<br />
The data to which the method is to be applied. These must
have the same format as the input data of the passed model to the
converter object. This means either
</p>

<ul>
<li><p> an <code>array</code>, <code>data.frame</code>, <code>torch_tensor</code> or array-like format of
size <em>(batch_size, dim_in)</em>, if e.g., the model has only one input layer, or
</p>
</li>
<li><p> a <code>list</code> with the corresponding input data (according to the
upper point) for each of the input layers.<br />
</p>
</li></ul>
</dd>
<dt><code>channels_first</code></dt><dd><p>(<code>logical(1)</code>)<br />
The channel position of the given data (argument
<code>data</code>). If <code>TRUE</code>, the channel axis is placed at the second position
between the batch size and the rest of the input axes, e.g.,
<code>c(10,3,32,32)</code> for a batch of ten images with three channels and a
height and width of 32 pixels. Otherwise (<code>FALSE</code>), the channel axis
is at the last position, i.e., <code>c(10,32,32,3)</code>. If the data
has no channel axis, use the default value <code>TRUE</code>.<br /></p>
</dd>
<dt><code>output_idx</code></dt><dd><p>(<code>integer</code>, <code>list</code> or <code>NULL</code>)<br />
These indices specify the output nodes for which
the method is to be applied. In order to allow models with multiple
output layers, there are the following possibilities to select
the indices of the output nodes in the individual output layers:
</p>

<ul>
<li><p> An <code>integer</code> vector of indices: If the model has only one output
layer, the values correspond to the indices of the output nodes, e.g.,
<code>c(1,3,4)</code> for the first, third and fourth output node. If there are
multiple output layers, the indices of the output nodes from the first
output layer are considered.
</p>
</li>
<li><p> A <code>list</code> of <code>integer</code> vectors of indices: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired indices of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of indices, e.g., <code>list(NULL, c(1,3))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br />
</p>
</li></ul>
</dd>
<dt><code>output_label</code></dt><dd><p>(<code>character</code>, <code>factor</code>, <code>list</code> or <code>NULL</code>)<br />
These values specify the output nodes for which
the method is to be applied. Only values that were previously passed with
the argument <code>output_names</code> in the <code>converter</code> can be used. In order to
allow models with multiple
output layers, there are the following possibilities to select
the names of the output nodes in the individual output layers:
</p>

<ul>
<li><p> A <code>character</code> vector or <code>factor</code> of labels: If the model has only one output
layer, the values correspond to the labels of the output nodes named in the
passed <code>Converter</code> object, e.g.,
<code>c("a", "c", "d")</code> for the first, third and fourth output node if the
output names are <code>c("a", "b", "c", "d")</code>. If there are
multiple output layers, the names of the output nodes from the first
output layer are considered.
</p>
</li>
<li><p> A <code>list</code> of <code>charactor</code>/<code>factor</code> vectors of labels: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired labels of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of labels, e.g., <code>list(NULL, c("a", "c"))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br />
</p>
</li></ul>
</dd>
<dt><code>ignore_last_act</code></dt><dd><p>(<code>logical(1)</code>)<br />
Set this logical value to include the last
activation functions for each output layer, or not (default: <code>TRUE</code>).
In practice, the last activation (especially for softmax activation) is
often omitted.<br /></p>
</dd>
<dt><code>times_input</code></dt><dd><p>(<code style="white-space: pre;">&#8288;logical(1&#8288;</code>))<br />
Multiplies the gradients with the input features.
This method is called <em>Gradient<code class="reqn">\times</code>Input</em>.<br /></p>
</dd>
<dt><code>verbose</code></dt><dd><p>(<code>logical(1)</code>)<br />
This logical argument determines whether a progress bar is
displayed for the calculation of the method or not. The default value is
the output of the primitive R function <code><a href="base.html#topic+interactive">interactive()</a></code>.<br /></p>
</dd>
<dt><code>dtype</code></dt><dd><p>(<code>character(1)</code>)<br />
The data type for the calculations. Use
either <code>'float'</code> for <a href="torch.html#topic+torch_float">torch_float</a> or <code>'double'</code> for
<a href="torch.html#topic+torch_double">torch_double</a>.<br /></p>
</dd>
</dl>

</div>


<hr>
<a id="method-Gradient-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>Gradient$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other methods: 
<code><a href="#topic+ConnectionWeights">ConnectionWeights</a></code>,
<code><a href="#topic+DeepLift">DeepLift</a></code>,
<code><a href="#topic+DeepSHAP">DeepSHAP</a></code>,
<code><a href="#topic+ExpectedGradient">ExpectedGradient</a></code>,
<code><a href="#topic+IntegratedGradient">IntegratedGradient</a></code>,
<code><a href="#topic+LIME">LIME</a></code>,
<code><a href="#topic+LRP">LRP</a></code>,
<code><a href="#topic+SHAP">SHAP</a></code>,
<code><a href="#topic+SmoothGrad">SmoothGrad</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#----------------------- Example 1: Torch ----------------------------------
library(torch)

# Create nn_sequential model and data
model &lt;- nn_sequential(
  nn_linear(5, 12),
  nn_relu(),
  nn_linear(12, 2),
  nn_softmax(dim = 2)
)
data &lt;- torch_randn(25, 5)

# Create Converter with input and output names
converter &lt;- convert(model,
  input_dim = c(5),
  input_names = list(c("Car", "Cat", "Dog", "Plane", "Horse")),
  output_names = list(c("Buy it!", "Don't buy it!"))
)

# Calculate the Gradients
grad &lt;- Gradient$new(converter, data)

# You can also use the helper function `run_grad` for initializing
# an R6 Gradient object
grad &lt;- run_grad(converter, data)

# Print the result as a data.frame for first 5 rows
get_result(grad, "data.frame")[1:5,]

# Plot the result for both classes
plot(grad, output_idx = 1:2)

# Plot the boxplot of all datapoints
boxplot(grad, output_idx = 1:2)

# ------------------------- Example 2: Neuralnet ---------------------------
if (require("neuralnet")) {
  library(neuralnet)
  data(iris)

  # Train a neural network
  nn &lt;- neuralnet(Species ~ ., iris,
    linear.output = FALSE,
    hidden = c(10, 5),
    act.fct = "logistic",
    rep = 1
  )

  # Convert the trained model
  converter &lt;- convert(nn)

  # Calculate the gradients
  gradient &lt;- run_grad(converter, iris[, -5])

  # Plot the result for the first and 60th data point and all classes
  plot(gradient, data_idx = c(1, 60), output_idx = 1:3)

  # Calculate Gradients x Input and do not ignore the last activation
  gradient &lt;- run_grad(converter, iris[, -5],
                       ignore_last_act = FALSE,
                       times_input = TRUE)

  # Plot the result again
  plot(gradient, data_idx = c(1, 60), output_idx = 1:3)
}


# ------------------------- Example 3: Keras -------------------------------
if (require("keras") &amp; keras::is_keras_available()) {
  library(keras)

  # Make sure keras is installed properly
  is_keras_available()

  data &lt;- array(rnorm(64 * 60 * 3), dim = c(64, 60, 3))

  model &lt;- keras_model_sequential()
  model %&gt;%
    layer_conv_1d(
      input_shape = c(60, 3), kernel_size = 8, filters = 8,
      activation = "softplus", padding = "valid") %&gt;%
    layer_conv_1d(
      kernel_size = 8, filters = 4, activation = "tanh",
      padding = "same") %&gt;%
    layer_conv_1d(
      kernel_size = 4, filters = 2, activation = "relu",
      padding = "valid") %&gt;%
    layer_flatten() %&gt;%
    layer_dense(units = 64, activation = "relu") %&gt;%
    layer_dense(units = 16, activation = "relu") %&gt;%
    layer_dense(units = 3, activation = "softmax")

  # Convert the model
  converter &lt;- convert(model)

  # Apply the Gradient method
  gradient &lt;- run_grad(converter, data, channels_first = FALSE)

  # Plot the result for the first datapoint and all classes
  plot(gradient, output_idx = 1:3)

  # Plot the result as boxplots for first two classes
  boxplot(gradient, output_idx = 1:2)
}


#------------------------- Plotly plots ------------------------------------
if (require("plotly")) {
  # You can also create an interactive plot with plotly.
  # This is a suggested package, so make sure that it is installed
  library(plotly)

  # Result as boxplots
  boxplot(gradient, as_plotly = TRUE)

  # Result of the second data point
  plot(gradient, data_idx = 2, as_plotly = TRUE)
}

</code></pre>

<hr>
<h2 id='GradientBased'>Super class for gradient-based interpretation methods</h2><span id='topic+GradientBased'></span>

<h3>Description</h3>

<p>Super class for gradient-based interpretation methods. This
class inherits from <code><a href="#topic+InterpretingMethod">InterpretingMethod</a></code>. It summarizes all implemented
gradient-based methods and provides a private function to calculate the
gradients w.r.t. to the input for given data. Implemented are:
</p>

<ul>
<li> <p><em>Vanilla Gradients</em> and <em>Gradient<code class="reqn">\times</code>Input</em> (<code><a href="#topic+Gradient">Gradient</a></code>)
</p>
</li>
<li> <p><em>Integrated Gradients</em> (<code><a href="#topic+IntegratedGradient">IntegratedGradient</a></code>)
</p>
</li>
<li> <p><em>SmoothGrad</em> and <em>SmoothGrad<code class="reqn">\times</code>Input</em> (<code><a href="#topic+SmoothGrad">SmoothGrad</a></code>)
</p>
</li>
<li> <p><em>ExpectedGradients</em> (<code><a href="#topic+ExpectedGradient">ExpectedGradient</a></code>)
</p>
</li></ul>



<h3>Super class</h3>

<p><code><a href="#topic+InterpretingMethod">innsight::InterpretingMethod</a></code> -&gt; <code>GradientBased</code>
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>times_input</code></dt><dd><p>(<code style="white-space: pre;">&#8288;logical(1&#8288;</code>))<br />
This logical value indicates whether the results
were multiplied by the provided input data or not.<br /></p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-GradientBased-new"><code>GradientBased$new()</code></a>
</p>
</li>
<li> <p><a href="#method-GradientBased-clone"><code>GradientBased$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="get_result"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-get_result'><code>innsight::InterpretingMethod$get_result()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="plot"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-plot'><code>innsight::InterpretingMethod$plot()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="plot_global"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-plot_global'><code>innsight::InterpretingMethod$plot_global()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="print"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-print'><code>innsight::InterpretingMethod$print()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-GradientBased-new"></a>



<h4>Method <code>new()</code></h4>

<p>Create a new instance of this class. When initialized,
the method is applied to the given data and the results are stored in
the field <code>result</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>GradientBased$new(
  converter,
  data,
  channels_first = TRUE,
  output_idx = NULL,
  output_label = NULL,
  ignore_last_act = TRUE,
  times_input = TRUE,
  verbose = interactive(),
  dtype = "float"
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>converter</code></dt><dd><p>(<code><a href="#topic+Converter">Converter</a></code>)<br />
An instance of the <code>Converter</code> class that includes the
torch-converted model and some other model-specific attributes. See
<code><a href="#topic+Converter">Converter</a></code> for details.<br /></p>
</dd>
<dt><code>data</code></dt><dd><p>(<code><a href="base.html#topic+array">array</a></code>, <code><a href="base.html#topic+data.frame">data.frame</a></code>, <code><a href="torch.html#topic+torch_tensor">torch_tensor</a></code> or <code>list</code>)<br />
The data to which the method is to be applied. These must
have the same format as the input data of the passed model to the
converter object. This means either
</p>

<ul>
<li><p> an <code>array</code>, <code>data.frame</code>, <code>torch_tensor</code> or array-like format of
size <em>(batch_size, dim_in)</em>, if e.g., the model has only one input layer, or
</p>
</li>
<li><p> a <code>list</code> with the corresponding input data (according to the
upper point) for each of the input layers.<br />
</p>
</li></ul>
</dd>
<dt><code>channels_first</code></dt><dd><p>(<code>logical(1)</code>)<br />
The channel position of the given data (argument
<code>data</code>). If <code>TRUE</code>, the channel axis is placed at the second position
between the batch size and the rest of the input axes, e.g.,
<code>c(10,3,32,32)</code> for a batch of ten images with three channels and a
height and width of 32 pixels. Otherwise (<code>FALSE</code>), the channel axis
is at the last position, i.e., <code>c(10,32,32,3)</code>. If the data
has no channel axis, use the default value <code>TRUE</code>.<br /></p>
</dd>
<dt><code>output_idx</code></dt><dd><p>(<code>integer</code>, <code>list</code> or <code>NULL</code>)<br />
These indices specify the output nodes for which
the method is to be applied. In order to allow models with multiple
output layers, there are the following possibilities to select
the indices of the output nodes in the individual output layers:
</p>

<ul>
<li><p> An <code>integer</code> vector of indices: If the model has only one output
layer, the values correspond to the indices of the output nodes, e.g.,
<code>c(1,3,4)</code> for the first, third and fourth output node. If there are
multiple output layers, the indices of the output nodes from the first
output layer are considered.
</p>
</li>
<li><p> A <code>list</code> of <code>integer</code> vectors of indices: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired indices of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of indices, e.g., <code>list(NULL, c(1,3))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br />
</p>
</li></ul>
</dd>
<dt><code>output_label</code></dt><dd><p>(<code>character</code>, <code>factor</code>, <code>list</code> or <code>NULL</code>)<br />
These values specify the output nodes for which
the method is to be applied. Only values that were previously passed with
the argument <code>output_names</code> in the <code>converter</code> can be used. In order to
allow models with multiple
output layers, there are the following possibilities to select
the names of the output nodes in the individual output layers:
</p>

<ul>
<li><p> A <code>character</code> vector or <code>factor</code> of labels: If the model has only one output
layer, the values correspond to the labels of the output nodes named in the
passed <code>Converter</code> object, e.g.,
<code>c("a", "c", "d")</code> for the first, third and fourth output node if the
output names are <code>c("a", "b", "c", "d")</code>. If there are
multiple output layers, the names of the output nodes from the first
output layer are considered.
</p>
</li>
<li><p> A <code>list</code> of <code>charactor</code>/<code>factor</code> vectors of labels: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired labels of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of labels, e.g., <code>list(NULL, c("a", "c"))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br />
</p>
</li></ul>
</dd>
<dt><code>ignore_last_act</code></dt><dd><p>(<code>logical(1)</code>)<br />
Set this logical value to include the last
activation functions for each output layer, or not (default: <code>TRUE</code>).
In practice, the last activation (especially for softmax activation) is
often omitted.<br /></p>
</dd>
<dt><code>times_input</code></dt><dd><p>(<code style="white-space: pre;">&#8288;logical(1&#8288;</code>)<br />
Multiplies the gradients with the input features.
This method is called <em>Gradient<code class="reqn">\times</code>Input</em>.<br /></p>
</dd>
<dt><code>verbose</code></dt><dd><p>(<code>logical(1)</code>)<br />
This logical argument determines whether a progress bar is
displayed for the calculation of the method or not. The default value is
the output of the primitive R function <code><a href="base.html#topic+interactive">interactive()</a></code>.<br /></p>
</dd>
<dt><code>dtype</code></dt><dd><p>(<code>character(1)</code>)<br />
The data type for the calculations. Use
either <code>'float'</code> for <a href="torch.html#topic+torch_float">torch_float</a> or <code>'double'</code> for
<a href="torch.html#topic+torch_double">torch_double</a>.<br /></p>
</dd>
</dl>

</div>


<hr>
<a id="method-GradientBased-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>GradientBased$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='innsight_ggplot2'>S4 class for ggplot2-based plots</h2><span id='topic+innsight_ggplot2'></span>

<h3>Description</h3>

<p>The S4 class <code>innsight_ggplot2</code> visualizes the results of the methods
provided from the package <code>innsight</code> using <a href="ggplot2.html#topic+ggplot2">ggplot2</a>. In addition, it
allows easier analysis of the results and modification of the
visualization by basic generic functions. The individual slots are for
internal use only and should not be modified.
</p>


<h3>Details</h3>

<p>This S4 class is a simple extension of a <a href="ggplot2.html#topic+ggplot2">ggplot2</a> object that enables
a more detailed analysis of the results and a way to visualize the results
of models with multiple input layers (e.g., images and tabular data).
The distinction between one and multiple input layers decides the behavior
of this class, and this information is stored in the slot <code>multiplot</code>.
</p>


<h4>One input layer (<code>multiplot = FALSE</code>)</h4>

<p>If the model passed to a method from the innsight package has only one
input layer, the S4 class <code>innsight_ggplot2</code> is just a wrapper of a
single ggplot2 object. This object is stored as a 1x1 matrix in
the slot <code>grobs</code> and the slots <code>output_strips</code> and <code>col_dims</code> contain
only empty lists because no second line of stripes describing the input
layer is needed.
Although it is an object of the class <code>innsight_ggplot2</code>,
the generic function <a href="#topic++2B.innsight_ggplot2">+.innsight_ggplot2</a> provides a ggplot2-typical usage
to modify the representation. The graphical objects are simply forwarded to
the ggplot2 object in <code>grobs</code> and added using <a href="ggplot2.html#topic+gg-add">ggplot2::+.gg</a>. In addition,
some generic functions are implemented to visualize or examine
individual aspects of the overall plot in more detail. All available
generic functions are listed below:
</p>

<ul>
<li> <p><code><a href="#topic++2B.innsight_ggplot2">+</a></code>
</p>
</li>
<li> <p><code><a href="#topic+plot.innsight_ggplot2">plot</a></code>,
<code><a href="#topic+print.innsight_ggplot2">print</a></code> and
<code><a href="#topic+show.innsight_ggplot2">show</a></code>
(all behave the same)
</p>
</li>
<li> <p><code><a href="#topic++5B.innsight_ggplot2">[</a></code>
</p>
</li>
<li> <p><code><a href="#topic++5B+5B.innsight_ggplot2">[[</a></code>
</p>
</li></ul>

<p><em>Note:</em> In this case, the generic function <code style="white-space: pre;">&#8288;[&lt;-&#8288;</code> is not implemented
because there is only one ggplot2 object and not multiple ones.
</p>



<h4>Multiple input layers (<code>multiplot = TRUE</code>)</h4>

<p>If the passed model has multiple input layers, a ggplot2 object is
created for each data point, input layer and output node and then stored
as a matrix in the slot <code>grobs</code>. During visualization, these are combined
using the function <code><a href="gridExtra.html#topic+arrangeGrob">gridExtra::arrangeGrob</a></code> and corresponding strips for
the output layer/node names are added at the top. The labels, column
indices and theme for the extra row of strips are stored in the slots
<code>output_strips</code> and <code>col_dims</code>. The strips for the input
layer and the data points (if not boxplot) are created using
<a href="ggplot2.html#topic+facet_grid">ggplot2::facet_grid</a> in the individual ggplot2 objects of the grob matrix.
An example structure is shown below:
</p>
<div class="sourceCode"><pre>|      Output 1: Node 1      |      Output 1: Node 3      |
|   Input 1   |   Input 2    |   Input 1   |   Input 2    |
|---------------------------------------------------------|-------------
|             |              |             |              |
| grobs[1,1]  |  grobs[1,2]  | grobs[1,3]  | grobs[1,4]   | data point 1
|             |              |             |              |
|---------------------------------------------------------|-------------
|             |              |             |              |
| grobs[2,1]  |  grobs[2,2]  | grobs[2,3]  | grobs[2,4]   | data point 2
|             |              |             |              |
</pre></div>
<p>Similar to the other case, generic functions are implemented to add
graphical objects from ggplot2, create the whole plot or select only
specific rows/columns. The difference, however, is that each entry in
each row and column is a separate ggplot2 object and can be modified
individually. For example, adds <code>+ ggplot2::xlab("X")</code> the x-axis label
&quot;X&quot; to all objects and not only to those in the last row. The generic
function <code><a href="#topic++5B+3C-.innsight_ggplot2">[&lt;-</a></code> allows you to replace
a selection of objects in <code>grobs</code> and thus, for example, to change
the x-axis title only in the bottom row. All available
generic functions are listed below:
</p>

<ul>
<li> <p><code><a href="#topic++2B.innsight_ggplot2">+</a></code>
</p>
</li>
<li> <p><code><a href="#topic+plot.innsight_ggplot2">plot</a></code>,
<code><a href="#topic+print.innsight_ggplot2">print</a></code> and
<code><a href="#topic+show.innsight_ggplot2">show</a></code>
(all behave the same)
</p>
</li>
<li> <p><code><a href="#topic++5B.innsight_ggplot2">[</a></code>
</p>
</li>
<li> <p><code><a href="#topic++5B+5B.innsight_ggplot2">[[</a></code>
</p>
</li>
<li> <p><code><a href="#topic++5B+3C-.innsight_ggplot2">[&lt;-</a></code>
</p>
</li>
<li> <p><code><a href="#topic++5B+3C-.innsight_ggplot2">[[&lt;-</a></code>
</p>
</li></ul>

<p><em>Note:</em> Since this is not a standard visualization, the suggested packages
<code>'grid'</code>, <code>'gridExtra'</code> and <code>'gtable'</code> must be installed.
</p>



<h3>Slots</h3>


<dl>
<dt><code>grobs</code></dt><dd><p>The individual ggplot2 objects arranged as a matrix (see
details for more information)</p>
</dd>
<dt><code>multiplot</code></dt><dd><p>A logical value indicating whether there are multiple
input layers and therefore correspondingly individual ggplot2 objects
instead of one single object.</p>
</dd>
<dt><code>output_strips</code></dt><dd><p>A list containing the labels and themes of the strips
for the output nodes. This slot is only relevant if <code>multiplot</code> is <code>TRUE</code>.</p>
</dd>
<dt><code>col_dims</code></dt><dd><p>A list of the length of <code>output_strips</code> assigning to
each strip the column index of <code>grobs</code> of the associated strip.</p>
</dd>
<dt><code>boxplot</code></dt><dd><p>A logical value indicating whether the result of individual
data points or a boxplot over multiple instances is displayed.</p>
</dd>
</dl>

<hr>
<h2 id='innsight_plotly'>S4 class for plotly-based plots</h2><span id='topic+innsight_plotly'></span>

<h3>Description</h3>

<p>The S4 class <code>innsight_plotly</code> visualizes the results of the methods
provided from the package <code>innsight</code> using <a href="https://plotly.com/r/">plotly</a>.
In addition, it allows easier analysis of the results and modification of
the visualization by basic generic functions. The individual slots are for
internal use only and should not be modified.
</p>


<h3>Details</h3>

<p>This S4 class is a simple extension of a plotly object that enables
a more detailed analysis of the results and a way to visualize the results
of models with multiple input layers (e.g., images and tabular data).
</p>
<p>The overall plot is created in the following order:
</p>

<ol>
<li><p> The corresponding shapes and annotations of the slots <code>annotations</code>
and <code>shapes</code> are added to each plot in <code>plots</code>. This also adds the strips
at the top for the output node (or input layer) and, if necessary, on the
right side for the data point.
</p>
</li>
<li><p> Subsequently, all individual plots are combined into one plot with
the help of the function <a href="plotly.html#topic+subplot">plotly::subplot</a>.
</p>
</li>
<li><p> Lastly, the global elements from the <code>layout</code> slot are added and if there
are multiple input layers (<code>multiplot = TRUE</code>), another output strip is
added for the columns.
</p>
</li></ol>

<p>An example structure of the plot with multiple input layers is shown below:
</p>
<div class="sourceCode"><pre>|      Output 1: Node 1      |      Output 1: Node 3      |
|   Input 1   |   Input 2    |   Input 1   |   Input 2    |
|---------------------------------------------------------|-------------
|             |              |             |              |
| plots[1,1]  |  plots[1,2]  | plots[1,3]  | plots[1,4]   | data point 1
|             |              |             |              |
|---------------------------------------------------------|-------------
|             |              |             |              |
| plots[2,1]  |  plots[2,2]  | plots[2,3]  | plots[2,4]   | data point 2
|             |              |             |              |
</pre></div>
<p>Additionally, some generic functions are implemented to visualize individual
aspects of the overall plot or to examine them in more detail. All available
generic functions are listed below:
</p>

<ul>
<li> <p><code><a href="#topic+plot.innsight_plotly">plot</a></code>,
<code><a href="#topic+print.innsight_plotly">print</a></code> and
<code><a href="#topic+show.innsight_plotly">show</a></code>
(all behave the same)
</p>
</li>
<li> <p><code><a href="#topic++5B.innsight_plotly">[</a></code>
</p>
</li>
<li> <p><code><a href="#topic++5B+5B.innsight_plotly">[[</a></code>
</p>
</li></ul>



<h3>Slots</h3>


<dl>
<dt><code>plots</code></dt><dd><p>The individual plotly objects arranged as a matrix (see
details for more information).</p>
</dd>
<dt><code>shapes</code></dt><dd><p>A list of two lists with the names <code>shapes_strips</code> and
<code>shapes_other</code>. The list <code>shapes_strips</code> contains the shapes for the
strips and may not be manipulated. The other list <code>shapes_other</code> contains
a matrix of the same size as <code>plots</code> and each entry contains the shapes
of the corresponding plot.</p>
</dd>
<dt><code>annotations</code></dt><dd><p>A list of two lists with the names <code>annotations_strips</code>
and <code>annotations_other</code>. The list <code>annotations_strips</code> contains the
annotations for the strips and may not be manipulated. The other list
<code>annotations_other</code> contains a matrix of the same size as <code>plots</code> and
each entry contains the annotations of the corresponding plot.</p>
</dd>
<dt><code>multiplot</code></dt><dd><p>A logical value indicating whether there are multiple
input layers and therefore correspondingly individual ggplot2 objects
instead of one single object.</p>
</dd>
<dt><code>layout</code></dt><dd><p>This list contains all global layout options, e.g. update
buttons, sliders, margins etc. (see <a href="plotly.html#topic+layout">plotly::layout</a> for more details).</p>
</dd>
<dt><code>col_dims</code></dt><dd><p>A list to assign a label to the columns for the output
strips.</p>
</dd>
</dl>

<hr>
<h2 id='innsight_sugar'>Syntactic sugar for object construction</h2><span id='topic+innsight_sugar'></span><span id='topic+convert'></span><span id='topic+run_grad'></span><span id='topic+run_smoothgrad'></span><span id='topic+run_intgrad'></span><span id='topic+run_expgrad'></span><span id='topic+run_lrp'></span><span id='topic+run_deeplift'></span><span id='topic+run_deepshap'></span><span id='topic+run_cw'></span><span id='topic+run_lime'></span><span id='topic+run_shap'></span>

<h3>Description</h3>

<p>Since all methods and the preceding conversion step in the <code>innsight</code>
package were implemented using R6 classes and these always require a call
to <code>classname$new()</code> for initialization, the following functions are
defined to shorten the construction of the corresponding R6 objects:
</p>

<ul>
<li> <p><code>convert()</code> for <code><a href="#topic+Converter">Converter</a></code>
</p>
</li>
<li> <p><code>run_grad()</code> for <code><a href="#topic+Gradient">Gradient</a></code>
</p>
</li>
<li> <p><code>run_smoothgrad()</code> for <code><a href="#topic+SmoothGrad">SmoothGrad</a></code>
</p>
</li>
<li> <p><code>run_intgrad()</code> for <code><a href="#topic+IntegratedGradient">IntegratedGradient</a></code>
</p>
</li>
<li> <p><code>run_expgrad()</code> for <code><a href="#topic+ExpectedGradient">ExpectedGradient</a></code>
</p>
</li>
<li> <p><code>run_lrp()</code> for <code><a href="#topic+LRP">LRP</a></code>
</p>
</li>
<li> <p><code>run_deeplift()</code> for <code><a href="#topic+DeepLift">DeepLift</a></code>
</p>
</li>
<li> <p><code>run_deepshap</code> for <code><a href="#topic+DeepSHAP">DeepSHAP</a></code>
</p>
</li>
<li> <p><code>run_cw</code> for <code><a href="#topic+ConnectionWeights">ConnectionWeights</a></code>
</p>
</li>
<li> <p><code>run_lime</code> for <code><a href="#topic+LIME">LIME</a></code>
</p>
</li>
<li> <p><code>run_shap</code> for <code><a href="#topic+SHAP">SHAP</a></code>
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'># Create a new `Converter` object of the given `model`
convert(model, ...)

# Apply the `Gradient` method to the passed `data` to be explained
run_grad(converter, data, ...)

# Apply the `SmoothGrad` method to the passed `data` to be explained
run_smoothgrad(converter, data, ...)

# Apply the `IntegratedGradient` method to the passed `data` to be explained
run_intgrad(converter, data, ...)

# Apply the `ExpectedGradient` method to the passed `data` to be explained
run_expgrad(converter, data, ...)

# Apply the `LRP` method to the passed `data` to be explained
run_lrp(converter, data, ...)

# Apply the `DeepLift` method to the passed `data` to be explained
run_deeplift(converter, data, ...)

# Apply the `DeepSHAP` method to the passed `data` to be explained
run_deepshap(converter, data, ...)

# Apply the `ConnectionWeights` method (argument `data` is not always required)
run_cw(converter, ...)

# Apply the `LIME` method to explain `data` by using the dataset `data_ref`
run_lime(model, data, data_ref, ...)

# Apply the `SHAP` method to explain `data` by using the dataset `data_ref`
run_shap(model, data, data_ref, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="innsight_sugar_+3A_model">model</code></td>
<td>
<p>(<code><a href="torch.html#topic+nn_sequential">nn_sequential</a></code>, <code><a href="keras.html#topic+keras_model">keras_model</a></code>,
<code><a href="neuralnet.html#topic+neuralnet">neuralnet</a></code> or <code>list</code>)<br />
A trained neural network for classification or regression
tasks to be interpreted. Only models from the following types or
packages are allowed: <code><a href="torch.html#topic+nn_sequential">nn_sequential</a></code>,
<code><a href="keras.html#topic+keras_model">keras_model</a></code>,
<code><a href="keras.html#topic+keras_model_sequential">keras_model_sequential</a></code>,
<code><a href="neuralnet.html#topic+neuralnet">neuralnet</a></code> or a named list (see details).<br />
<strong>Note:</strong> For the model-agnostic methods, an arbitrary fitted model for a
classification or regression task can be passed. A <code><a href="#topic+Converter">Converter</a></code> object can
also be passed. In order for the package to know how to make predictions
with the given model, a prediction function must also be passed with
the argument <code>pred_fun</code>. However, for models created by
<code><a href="torch.html#topic+nn_sequential">nn_sequential</a></code>, <code><a href="keras.html#topic+keras_model">keras_model</a></code>,
<code><a href="neuralnet.html#topic+neuralnet">neuralnet</a></code> or <code><a href="#topic+Converter">Converter</a></code>,
these have already been pre-implemented and do not need to be
specified.<br /></p>
</td></tr>
<tr><td><code id="innsight_sugar_+3A_...">...</code></td>
<td>
<p>Other arguments passed to the individual constructor functions
of the methods R6 classes.</p>
</td></tr>
<tr><td><code id="innsight_sugar_+3A_converter">converter</code></td>
<td>
<p>(<code><a href="#topic+Converter">Converter</a></code>)<br />
An instance of the <code>Converter</code> class that includes the
torch-converted model and some other model-specific attributes. See
<code><a href="#topic+Converter">Converter</a></code> for details.<br /></p>
</td></tr>
<tr><td><code id="innsight_sugar_+3A_data">data</code></td>
<td>
<p>(<code><a href="base.html#topic+array">array</a></code>, <code><a href="base.html#topic+data.frame">data.frame</a></code>, <code><a href="torch.html#topic+torch_tensor">torch_tensor</a></code> or <code>list</code>)<br />
The data to which the method is to be applied. These must
have the same format as the input data of the passed model to the
converter object. This means either
</p>

<ul>
<li><p> an <code>array</code>, <code>data.frame</code>, <code>torch_tensor</code> or array-like format of
size <em>(batch_size, dim_in)</em>, if e.g., the model has only one input layer, or
</p>
</li>
<li><p> a <code>list</code> with the corresponding input data (according to the
upper point) for each of the input layers.
</p>
</li></ul>

<p><strong>Note:</strong> For the model-agnostic methods, only models with a single
input and output layer is allowed!<br /></p>
</td></tr>
<tr><td><code id="innsight_sugar_+3A_data_ref">data_ref</code></td>
<td>
<p>(<code><a href="base.html#topic+array">array</a></code>, <code><a href="base.html#topic+data.frame">data.frame</a></code> or <code><a href="torch.html#topic+torch_tensor">torch_tensor</a></code>)<br />
The dataset to which the method is to be applied. These must
have the same format as the input data of the passed model and has to
be either <code><a href="base.html#topic+matrix">matrix</a></code>, an <code><a href="base.html#topic+array">array</a></code>, a <code><a href="base.html#topic+data.frame">data.frame</a></code> or a
<code><a href="torch.html#topic+torch_tensor">torch_tensor</a></code>.<br />
<strong>Note:</strong> For the model-agnostic methods, only models with a single
input and output layer is allowed!<br /></p>
</td></tr>
</table>


<h3>Value</h3>

<p><a href="R6.html#topic+R6Class">R6::R6Class</a> object of the respective type.
</p>

<hr>
<h2 id='innsight-package'>Get the insight of your neural network</h2><span id='topic+innsight'></span><span id='topic+innsight-package'></span>

<h3>Description</h3>

<p><code>innsight</code> is an R package that interprets the behavior and explains
individual predictions of modern neural networks. Many methods for
explaining individual predictions already exist, but hardly any of them
are implemented or available in R. Most of these so-called
<em>feature attribution</em> methods are only implemented in Python and,
thus, difficult to access or use for the R community. In this sense,
the package <code>innsight</code> provides a common interface for various methods
for the interpretability of neural networks and can therefore be considered
as an R analogue to 'iNNvestigate' or 'Captum' for Python.
</p>


<h3>Details</h3>

<p>This package implements several model-specific interpretability
(feature attribution) methods based on neural networks in R, e.g.,
</p>

<ul>
<li> <p><em>Layer-wise relevance propagation (<a href="#topic+LRP">LRP</a>)</em>
</p>

<ul>
<li><p> Including propagation rules: <code class="reqn">\epsilon</code>-rule and
<code class="reqn">\alpha</code>-<code class="reqn">\beta</code>-rule
</p>
</li></ul>

</li>
<li> <p><em>Deep learning important features (<a href="#topic+DeepLift">DeepLift</a>)</em>
</p>

<ul>
<li><p> Including propagation rules for non-linearities: <em>Rescale</em> rule and
<em>RevealCancel</em> rule
</p>
</li></ul>

</li>
<li> <p><a href="#topic+DeepSHAP">DeepSHAP</a>
</p>
</li>
<li><p> Gradient-based methods:
</p>

<ul>
<li> <p><em>Vanilla <a href="#topic+Gradient">Gradient</a></em>, including <em>Gradient<code class="reqn">\times</code>Input</em>
</p>
</li>
<li><p> Smoothed gradients <em>(<a href="#topic+SmoothGrad">SmoothGrad</a>)</em>, including <em>SmoothGrad<code class="reqn">\times</code>Input</em>
</p>
</li>
<li> <p><em>Integrated gradients</em> (<a href="#topic+IntegratedGradient">IntegratedGradient</a>)
</p>
</li>
<li> <p><em>Expected gradients</em> (<a href="#topic+ExpectedGradient">ExpectedGradient</a>)
</p>
</li></ul>

</li>
<li> <p><em><a href="#topic+ConnectionWeights">ConnectionWeights</a></em>
</p>
</li>
<li><p> Model-agnostic methods:
</p>

<ul>
<li> <p><em>Local interpretable model-agnostic explanation (<a href="#topic+LIME">LIME</a>)</em>
</p>
</li>
<li> <p><em>Shapley values</em> (<a href="#topic+SHAP">SHAP</a>)
</p>
</li></ul>

</li></ul>

<p>The package <code>innsight</code> aims to be as flexible as possible and independent
of a specific deep learning package in which the passed network has been
learned. Basically, a neural network of the libraries
<code><a href="torch.html#topic+nn_sequential">torch::nn_sequential</a></code>, <code><a href="keras.html#topic+keras_model_sequential">keras::keras_model_sequential</a></code>,
<code><a href="keras.html#topic+keras_model">keras::keras_model</a></code> and <code><a href="neuralnet.html#topic+neuralnet">neuralnet::neuralnet</a></code> can be passed to the
main building block <code><a href="#topic+Converter">Converter</a></code>,
which converts and stores the passed model as a torch model
(<code><a href="#topic+ConvertedModel">ConvertedModel</a></code>) with special insights needed for interpretation.
It is also possible to pass an arbitrary net in form of a named list
(see details in <code><a href="#topic+Converter">Converter</a></code>).
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Niklas Koenen <a href="mailto:niklas.koenen@gmail.com">niklas.koenen@gmail.com</a> (<a href="https://orcid.org/0000-0002-4623-8271">ORCID</a>)
</p>
<p>Other contributors:
</p>

<ul>
<li><p> Raphael Baudeu <a href="mailto:raphael.baudeu@gmail.com">raphael.baudeu@gmail.com</a> [contributor]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://bips-hb.github.io/innsight/">https://bips-hb.github.io/innsight/</a>
</p>
</li>
<li> <p><a href="https://github.com/bips-hb/innsight/">https://github.com/bips-hb/innsight/</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/bips-hb/innsight/issues/">https://github.com/bips-hb/innsight/issues/</a>
</p>
</li></ul>


<hr>
<h2 id='IntegratedGradient'>Integrated Gradients</h2><span id='topic+IntegratedGradient'></span>

<h3>Description</h3>

<p>The <code>IntegratedGradient</code> class implements the method Integrated Gradients
(Sundararajan et al., 2017), which incorporates a reference value <code class="reqn">x'</code>
(also known as baseline value) analogous to the <code><a href="#topic+DeepLift">DeepLift</a></code> method.
Integrated Gradients helps to uncover the relative importance of input
features in the predictions <code class="reqn">y = f(x)</code> made by a model compared to the
prediction of the reference value <code class="reqn">y' = f(x')</code>. This is achieved through
the following formula:
</p>
<p style="text-align: center;"><code class="reqn">
(x - x') \times \int_{\alpha=0}^{1} \frac{\partial f(x' + \alpha (x - x'))}{\partial x} d\alpha
</code>
</p>

<p>In simpler terms, it calculates how much each feature contributes to a
model's output by tracing a path from a baseline input <code class="reqn">x'</code> to the actual
input <code class="reqn">x</code> and measuring the average gradients along that path.
</p>
<p>Similar to the other gradient-based methods, by default the integrated
gradient is multiplied by the input to get an approximate decomposition
of <code class="reqn">y - y'</code>. However, with the parameter <code>times_input</code> only the gradient
describing the output sensitivity can be returned.
</p>
<p>The R6 class can also be initialized using the <code><a href="#topic+run_intgrad">run_intgrad</a></code> function
as a helper function so that no prior knowledge of R6 classes is required.
</p>


<h3>Super classes</h3>

<p><code><a href="#topic+InterpretingMethod">innsight::InterpretingMethod</a></code> -&gt; <code><a href="#topic+GradientBased">innsight::GradientBased</a></code> -&gt; <code>IntegratedGradient</code>
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>n</code></dt><dd><p>(<code>integer(1)</code>)<br />
Number of steps for the approximation of the integration path along
<code class="reqn">\alpha</code> (default: <code class="reqn">50</code>).<br /></p>
</dd>
<dt><code>x_ref</code></dt><dd><p>(<code>list</code>)<br />
The reference input for the IntegratedGradient method. This value is
stored as a list of <code>torch_tensor</code>s of shape <em>(1, dim_in)</em> for each
input layer.<br /></p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-IntegratedGradient-new"><code>IntegratedGradient$new()</code></a>
</p>
</li>
<li> <p><a href="#method-IntegratedGradient-clone"><code>IntegratedGradient$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="get_result"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-get_result'><code>innsight::InterpretingMethod$get_result()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="plot"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-plot'><code>innsight::InterpretingMethod$plot()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="plot_global"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-plot_global'><code>innsight::InterpretingMethod$plot_global()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="print"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-print'><code>innsight::InterpretingMethod$print()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-IntegratedGradient-new"></a>



<h4>Method <code>new()</code></h4>

<p>Create a new instance of the <code>IntegratedGradient</code> R6 class. When
initialized, the method <em>Integrated Gradient</em> is applied to the given
data and baseline value and the results are stored in the field <code>result</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>IntegratedGradient$new(
  converter,
  data,
  x_ref = NULL,
  n = 50,
  times_input = TRUE,
  channels_first = TRUE,
  output_idx = NULL,
  output_label = NULL,
  ignore_last_act = TRUE,
  verbose = interactive(),
  dtype = "float"
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>converter</code></dt><dd><p>(<code><a href="#topic+Converter">Converter</a></code>)<br />
An instance of the <code>Converter</code> class that includes the
torch-converted model and some other model-specific attributes. See
<code><a href="#topic+Converter">Converter</a></code> for details.<br /></p>
</dd>
<dt><code>data</code></dt><dd><p>(<code><a href="base.html#topic+array">array</a></code>, <code><a href="base.html#topic+data.frame">data.frame</a></code>, <code><a href="torch.html#topic+torch_tensor">torch_tensor</a></code> or <code>list</code>)<br />
The data to which the method is to be applied. These must
have the same format as the input data of the passed model to the
converter object. This means either
</p>

<ul>
<li><p> an <code>array</code>, <code>data.frame</code>, <code>torch_tensor</code> or array-like format of
size <em>(batch_size, dim_in)</em>, if e.g., the model has only one input layer, or
</p>
</li>
<li><p> a <code>list</code> with the corresponding input data (according to the
upper point) for each of the input layers.<br />
</p>
</li></ul>
</dd>
<dt><code>x_ref</code></dt><dd><p>(<code><a href="base.html#topic+array">array</a></code>, <code><a href="base.html#topic+data.frame">data.frame</a></code>, <code><a href="torch.html#topic+torch_tensor">torch_tensor</a></code> or <code>list</code>)<br />
The reference input for the IntegratedGradient method. This value
must have the same format as the input data of the passed model to the
converter object. This means either
</p>

<ul>
<li><p> an <code>array</code>, <code>data.frame</code>, <code>torch_tensor</code> or array-like format of
size <em>(1, dim_in)</em>, if e.g., the model has only one input layer, or
</p>
</li>
<li><p> a <code>list</code> with the corresponding input data (according to the upper point)
for each of the input layers.
</p>
</li>
<li><p> It is also possible to use the default value <code>NULL</code> to take only
zeros as reference input.<br />
</p>
</li></ul>
</dd>
<dt><code>n</code></dt><dd><p>(<code>integer(1)</code>)<br />
Number of steps for the approximation of the integration path along
<code class="reqn">\alpha</code> (default: <code class="reqn">50</code>).<br /></p>
</dd>
<dt><code>times_input</code></dt><dd><p>(<code style="white-space: pre;">&#8288;logical(1&#8288;</code>)<br />
Multiplies the integrated gradients with the difference of the input
features and the baseline values. By default, the original definition of
IntegratedGradient is applied. However, by setting <code>times_input = FALSE</code>
only an approximation of the integral is calculated, which describes the
sensitivity of the features to the output.<br /></p>
</dd>
<dt><code>channels_first</code></dt><dd><p>(<code>logical(1)</code>)<br />
The channel position of the given data (argument
<code>data</code>). If <code>TRUE</code>, the channel axis is placed at the second position
between the batch size and the rest of the input axes, e.g.,
<code>c(10,3,32,32)</code> for a batch of ten images with three channels and a
height and width of 32 pixels. Otherwise (<code>FALSE</code>), the channel axis
is at the last position, i.e., <code>c(10,32,32,3)</code>. If the data
has no channel axis, use the default value <code>TRUE</code>.<br /></p>
</dd>
<dt><code>output_idx</code></dt><dd><p>(<code>integer</code>, <code>list</code> or <code>NULL</code>)<br />
These indices specify the output nodes for which
the method is to be applied. In order to allow models with multiple
output layers, there are the following possibilities to select
the indices of the output nodes in the individual output layers:
</p>

<ul>
<li><p> An <code>integer</code> vector of indices: If the model has only one output
layer, the values correspond to the indices of the output nodes, e.g.,
<code>c(1,3,4)</code> for the first, third and fourth output node. If there are
multiple output layers, the indices of the output nodes from the first
output layer are considered.
</p>
</li>
<li><p> A <code>list</code> of <code>integer</code> vectors of indices: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired indices of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of indices, e.g., <code>list(NULL, c(1,3))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br />
</p>
</li></ul>
</dd>
<dt><code>output_label</code></dt><dd><p>(<code>character</code>, <code>factor</code>, <code>list</code> or <code>NULL</code>)<br />
These values specify the output nodes for which
the method is to be applied. Only values that were previously passed with
the argument <code>output_names</code> in the <code>converter</code> can be used. In order to
allow models with multiple
output layers, there are the following possibilities to select
the names of the output nodes in the individual output layers:
</p>

<ul>
<li><p> A <code>character</code> vector or <code>factor</code> of labels: If the model has only one output
layer, the values correspond to the labels of the output nodes named in the
passed <code>Converter</code> object, e.g.,
<code>c("a", "c", "d")</code> for the first, third and fourth output node if the
output names are <code>c("a", "b", "c", "d")</code>. If there are
multiple output layers, the names of the output nodes from the first
output layer are considered.
</p>
</li>
<li><p> A <code>list</code> of <code>charactor</code>/<code>factor</code> vectors of labels: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired labels of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of labels, e.g., <code>list(NULL, c("a", "c"))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br />
</p>
</li></ul>
</dd>
<dt><code>ignore_last_act</code></dt><dd><p>(<code>logical(1)</code>)<br />
Set this logical value to include the last
activation functions for each output layer, or not (default: <code>TRUE</code>).
In practice, the last activation (especially for softmax activation) is
often omitted.<br /></p>
</dd>
<dt><code>verbose</code></dt><dd><p>(<code>logical(1)</code>)<br />
This logical argument determines whether a progress bar is
displayed for the calculation of the method or not. The default value is
the output of the primitive R function <code><a href="base.html#topic+interactive">interactive()</a></code>.<br /></p>
</dd>
<dt><code>dtype</code></dt><dd><p>(<code>character(1)</code>)<br />
The data type for the calculations. Use
either <code>'float'</code> for <a href="torch.html#topic+torch_float">torch_float</a> or <code>'double'</code> for
<a href="torch.html#topic+torch_double">torch_double</a>.<br /></p>
</dd>
</dl>

</div>


<hr>
<a id="method-IntegratedGradient-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>IntegratedGradient$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>References</h3>

<p>M. Sundararajan et al. (2017) <em>Axiomatic attribution for deep networks.</em> ICML
2017, PMLR 70, pp. 3319-3328.
</p>


<h3>See Also</h3>

<p>Other methods: 
<code><a href="#topic+ConnectionWeights">ConnectionWeights</a></code>,
<code><a href="#topic+DeepLift">DeepLift</a></code>,
<code><a href="#topic+DeepSHAP">DeepSHAP</a></code>,
<code><a href="#topic+ExpectedGradient">ExpectedGradient</a></code>,
<code><a href="#topic+Gradient">Gradient</a></code>,
<code><a href="#topic+LIME">LIME</a></code>,
<code><a href="#topic+LRP">LRP</a></code>,
<code><a href="#topic+SHAP">SHAP</a></code>,
<code><a href="#topic+SmoothGrad">SmoothGrad</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#----------------------- Example 1: Torch ----------------------------------
library(torch)

# Create nn_sequential model and data
model &lt;- nn_sequential(
  nn_linear(5, 12),
  nn_relu(),
  nn_linear(12, 2),
  nn_softmax(dim = 2)
)
data &lt;- torch_randn(25, 5)
ref &lt;- torch_randn(1, 5)

# Create Converter
converter &lt;- convert(model, input_dim = c(5))

# Apply method IntegratedGradient
int_grad &lt;- IntegratedGradient$new(converter, data, x_ref = ref)

# You can also use the helper function `run_intgrad` for initializing
# an R6 IntegratedGradient object
int_grad &lt;- run_intgrad(converter, data, x_ref = ref)

# Print the result as a torch tensor for first two data points
get_result(int_grad, "torch.tensor")[1:2]

# Plot the result for both classes
plot(int_grad, output_idx = 1:2)

# Plot the boxplot of all datapoints and for both classes
boxplot(int_grad, output_idx = 1:2)

# ------------------------- Example 2: Neuralnet ---------------------------
if (require("neuralnet")) {
  library(neuralnet)
  data(iris)

  # Train a neural network
  nn &lt;- neuralnet((Species == "setosa") ~ Petal.Length + Petal.Width,
    iris,
    linear.output = FALSE,
    hidden = c(3, 2), act.fct = "tanh", rep = 1
  )

  # Convert the model
  converter &lt;- convert(nn)

  # Apply IntegratedGradient with a reference input of the feature means
  x_ref &lt;- matrix(colMeans(iris[, c(3, 4)]), nrow = 1)
  int_grad &lt;- run_intgrad(converter, iris[, c(3, 4)], x_ref = x_ref)

  # Get the result as a dataframe and show first 5 rows
  get_result(int_grad, type = "data.frame")[1:5, ]

  # Plot the result for the first datapoint in the data
  plot(int_grad, data_idx = 1)

  # Plot the result as boxplots
  boxplot(int_grad)
}


# ------------------------- Example 3: Keras -------------------------------
if (require("keras") &amp; keras::is_keras_available()) {
  library(keras)

  # Make sure keras is installed properly
  is_keras_available()

  data &lt;- array(rnorm(10 * 32 * 32 * 3), dim = c(10, 32, 32, 3))

  model &lt;- keras_model_sequential()
  model %&gt;%
    layer_conv_2d(
      input_shape = c(32, 32, 3), kernel_size = 8, filters = 8,
      activation = "softplus", padding = "valid") %&gt;%
    layer_conv_2d(
      kernel_size = 8, filters = 4, activation = "tanh",
      padding = "same") %&gt;%
    layer_conv_2d(
      kernel_size = 4, filters = 2, activation = "relu",
      padding = "valid") %&gt;%
    layer_flatten() %&gt;%
    layer_dense(units = 64, activation = "relu") %&gt;%
    layer_dense(units = 2, activation = "softmax")

  # Convert the model
  converter &lt;- convert(model)

  # Apply the IntegratedGradient method with a zero baseline and n = 20
  # iteration steps
  int_grad &lt;- run_intgrad(converter, data,
    channels_first = FALSE,
    n = 20
  )

  # Plot the result for the first image and both classes
  plot(int_grad, output_idx = 1:2)

  # Plot the pixel-wise median of the results
  plot_global(int_grad, output_idx = 1)
}


#------------------------- Plotly plots ------------------------------------
if (require("plotly")) {
  # You can also create an interactive plot with plotly.
  # This is a suggested package, so make sure that it is installed
  library(plotly)
  boxplot(int_grad, as_plotly = TRUE)
}

</code></pre>

<hr>
<h2 id='InterpretingMethod'>Super class for interpreting methods</h2><span id='topic+InterpretingMethod'></span>

<h3>Description</h3>

<p>This is a super class for all interpreting methods in the
<code>innsight</code> package. Implemented are the following methods:
</p>

<ul>
<li> <p><em>Deep Learning Important Features</em> (<code><a href="#topic+DeepLift">DeepLift</a></code>)
</p>
</li>
<li> <p><em>Deep Shapley additive explanations</em> (<code><a href="#topic+DeepSHAP">DeepSHAP</a></code>)
</p>
</li>
<li> <p><em>Layer-wise Relevance Propagation</em> (<code><a href="#topic+LRP">LRP</a></code>)
</p>
</li>
<li><p> Gradient-based methods:
</p>

<ul>
<li> <p><em>Vanilla gradients</em> including <em>Gradient<code class="reqn">\times</code>Input</em> (<code><a href="#topic+Gradient">Gradient</a></code>)
</p>
</li>
<li><p> Smoothed gradients including <em>SmoothGrad<code class="reqn">\times</code>Input</em> (<code><a href="#topic+SmoothGrad">SmoothGrad</a></code>)
</p>
</li>
<li> <p><em>Integrated gradients</em> (<code><a href="#topic+IntegratedGradient">IntegratedGradient</a></code>)
</p>
</li>
<li> <p><em>Expected gradients</em> (<code><a href="#topic+ExpectedGradient">ExpectedGradient</a></code>)
</p>
</li></ul>

</li>
<li> <p><em>Connection Weights</em> (global and local) (<code><a href="#topic+ConnectionWeights">ConnectionWeights</a></code>)
</p>
</li>
<li><p> Also some model-agnostic approaches:
</p>

<ul>
<li> <p><em>Local interpretable model-agnostic explanations</em> (<code><a href="#topic+LIME">LIME</a></code>)
</p>
</li>
<li> <p><em>Shapley values</em> (<code><a href="#topic+SHAP">SHAP</a></code>)
</p>
</li></ul>

</li></ul>



<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>data</code></dt><dd><p>(<code>list</code>)<br />
The passed data as a <code>list</code> of <code>torch_tensors</code> in the selected
data format (field <code>dtype</code>) matching the corresponding shapes of the
individual input layers. Besides, the channel axis is moved to the
second position after the batch size because internally only the
format <em>channels first</em> is used.<br /></p>
</dd>
<dt><code>converter</code></dt><dd><p>(<code><a href="#topic+Converter">Converter</a></code>)<br />
An instance of the <code>Converter</code> class that includes the
torch-converted model and some other model-specific attributes. See
<code><a href="#topic+Converter">Converter</a></code> for details.<br /></p>
</dd>
<dt><code>channels_first</code></dt><dd><p>(<code>logical(1)</code>)<br />
The channel position of the given data. If <code>TRUE</code>, the
channel axis is placed at the second position between the batch size and
the rest of the input axes, e.g., <code>c(10,3,32,32)</code> for a batch of ten images
with three channels and a height and width of 32 pixels. Otherwise (<code>FALSE</code>),
the channel axis is at the last position, i.e., <code>c(10,32,32,3)</code>. This is
especially important for layers like flatten, where the order is crucial
and therefore the channels have to be moved from the internal
format &quot;channels first&quot; back to the original format before the layer
is calculated.<br /></p>
</dd>
<dt><code>dtype</code></dt><dd><p>(<code>character(1)</code>)<br />
The data type for the calculations. Either <code>'float'</code> for
<a href="torch.html#topic+torch_float">torch_float</a> or <code>'double'</code> for <a href="torch.html#topic+torch_double">torch_double</a>.<br /></p>
</dd>
<dt><code>ignore_last_act</code></dt><dd><p>(<code>logical(1)</code>)<br />
A logical value to include the last activation
functions into all the calculations, or not.<br /></p>
</dd>
<dt><code>result</code></dt><dd><p>(<code>list</code>)<br />
The results of the method on the passed data. A unified
list structure is used regardless of the complexity of the model: The outer
list contains the individual output layers and the inner list the input
layers. The results for the respective output and input layer are then
stored there as torch tensors in the given data format (field <code>dtype</code>).
In addition, the channel axis is moved to its original place and the last
axis contains the selected output nodes for the individual output layers
(see <code>output_idx</code>).<br />
For example, the structure of the result for two output
layers (output node 1 for the first and 2 and 4 for the second) and two
input layers with <code>channels_first = FALSE</code> looks like this:
</p>
<div class="sourceCode"><pre>List of 2 # both output layers
  $ :List of 2 # both input layers
    ..$ : torch_tensor [batch_size, dim_in_1, channel_axis, 1]
    ..$ : torch_tensor [batch_size, dim_in_2, channel_axis, 1]
 $ :List of 2 # both input layers
    ..$ : torch_tensor [batch_size, dim_in_1, channel_axis, 2]
    ..$ : torch_tensor [batch_size, dim_in_2, channel_axis, 2]
</pre></div></dd>
<dt><code>output_idx</code></dt><dd><p>(<code>list</code>)<br />
This list of indices specifies the output nodes to which
the method is to be applied. In the order of the output layers, the list
contains the respective output nodes indices and unwanted output layers
have the entry <code>NULL</code> instead of a vector of indices,
e.g., <code>list(NULL, c(1,3))</code> for the first and third output node in the
second output layer.<br /></p>
</dd>
<dt><code>output_label</code></dt><dd><p>(<code>list</code>)<br />
This list of <code>factors</code> specifies the output nodes to which
the method is to be applied. In the order of the output layers, the list
contains the respective output nodes labels and unwanted output layers
have the entry <code>NULL</code> instead of a vector of labels,
e.g., <code>list(NULL, c("a", "c"))</code> for the first and third output node in the
second output layer.<br /></p>
</dd>
<dt><code>verbose</code></dt><dd><p>(<code>logical(1)</code>)<br />
This logical value determines whether a progress bar is
displayed for the calculation of the method or not. The default value is
the output of the primitive R function <code><a href="base.html#topic+interactive">interactive()</a></code>.<br /></p>
</dd>
<dt><code>winner_takes_all</code></dt><dd><p>(<code>logical(1)</code>)<br />
This logical value is only relevant for
models with a MaxPooling layer. Since many zeros are produced during
the backward pass due to the selection of the maximum value in the
pooling kernel, another variant is implemented, which treats a
MaxPooling as an AveragePooling layer in the backward pass to overcome
the problem of too many zero relevances. With the default value <code>TRUE</code>,
the whole upper-layer relevance is passed to the maximum value in each
pooling window. Otherwise, if <code>FALSE</code>, the relevance is distributed equally
among all nodes in a pooling window.<br /></p>
</dd>
<dt><code>preds</code></dt><dd><p>(<code>list</code>)<br />
In this field, all calculated predictions are stored as a list of
<code>torch_tensor</code>s. Each output layer has its own list entry and contains
the respective predicted values.<br /></p>
</dd>
<dt><code>decomp_goal</code></dt><dd><p>(<code>list</code>)<br />
In this field, the method-specific decomposition objectives are stored as
a list of <code>torch_tensor</code>s for each output layer. For example,
GradientxInput and LRP attempt to decompose the prediction into
feature-wise additive effects. DeepLift and IntegratedGradient decompose
the difference between <code class="reqn">f(x)</code> and <code class="reqn">f(x')</code>. On the other hand,
DeepSHAP and ExpectedGradient aim to decompose <code class="reqn">f(x)</code> minus the
averaged prediction across the reference values.<br /></p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-InterpretingMethod-new"><code>InterpretingMethod$new()</code></a>
</p>
</li>
<li> <p><a href="#method-InterpretingMethod-get_result"><code>InterpretingMethod$get_result()</code></a>
</p>
</li>
<li> <p><a href="#method-InterpretingMethod-plot"><code>InterpretingMethod$plot()</code></a>
</p>
</li>
<li> <p><a href="#method-InterpretingMethod-plot_global"><code>InterpretingMethod$plot_global()</code></a>
</p>
</li>
<li> <p><a href="#method-InterpretingMethod-print"><code>InterpretingMethod$print()</code></a>
</p>
</li>
<li> <p><a href="#method-InterpretingMethod-clone"><code>InterpretingMethod$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-InterpretingMethod-new"></a>



<h4>Method <code>new()</code></h4>

<p>Create a new instance of this super class.
</p>


<h5>Usage</h5>

<div class="r"><pre>InterpretingMethod$new(
  converter,
  data,
  channels_first = TRUE,
  output_idx = NULL,
  output_label = NULL,
  ignore_last_act = TRUE,
  winner_takes_all = TRUE,
  verbose = interactive(),
  dtype = "float"
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>converter</code></dt><dd><p>(<code><a href="#topic+Converter">Converter</a></code>)<br />
An instance of the <code>Converter</code> class that includes the
torch-converted model and some other model-specific attributes. See
<code><a href="#topic+Converter">Converter</a></code> for details.<br /></p>
</dd>
<dt><code>data</code></dt><dd><p>(<code><a href="base.html#topic+array">array</a></code>, <code><a href="base.html#topic+data.frame">data.frame</a></code>, <code><a href="torch.html#topic+torch_tensor">torch_tensor</a></code> or <code>list</code>)<br />
The data to which the method is to be applied. These must
have the same format as the input data of the passed model to the
converter object. This means either
</p>

<ul>
<li><p> an <code>array</code>, <code>data.frame</code>, <code>torch_tensor</code> or array-like format of
size <em>(batch_size, dim_in)</em>, if e.g., the model has only one input layer, or
</p>
</li>
<li><p> a <code>list</code> with the corresponding input data (according to the
upper point) for each of the input layers.<br />
</p>
</li></ul>
</dd>
<dt><code>channels_first</code></dt><dd><p>(<code>logical(1)</code>)<br />
The channel position of the given data (argument
<code>data</code>). If <code>TRUE</code>, the channel axis is placed at the second position
between the batch size and the rest of the input axes, e.g.,
<code>c(10,3,32,32)</code> for a batch of ten images with three channels and a
height and width of 32 pixels. Otherwise (<code>FALSE</code>), the channel axis
is at the last position, i.e., <code>c(10,32,32,3)</code>. If the data
has no channel axis, use the default value <code>TRUE</code>.<br /></p>
</dd>
<dt><code>output_idx</code></dt><dd><p>(<code>integer</code>, <code>list</code> or <code>NULL</code>)<br />
These indices specify the output nodes for which the method is to be
applied. In order to allow models with multiple output layers, there are
the following possibilities to select the indices of the output
nodes in the individual output layers:
</p>

<ul>
<li><p> An <code>integer</code> vector of indices: If the model has only one output
layer, the values correspond to the indices of the output nodes, e.g.
<code>c(1,3,4)</code> for the first, third and fourth output node. If there are
multiple output layers, the indices of the output nodes from the first
output layer are considered.
</p>
</li>
<li><p> A <code>list</code> of <code>integer</code> vectors of indices: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired indices of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead
of a vector of indices, e.g. <code>list(NULL, c(1,3))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes
in the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more
output nodes.<br />
</p>
</li></ul>
</dd>
<dt><code>output_label</code></dt><dd><p>(<code>character</code>, <code>factor</code>, <code>list</code> or <code>NULL</code>)<br />
These values specify the output nodes for which
the method is to be applied. Only values that were previously passed with
the argument <code>output_names</code> in the <code>converter</code> can be used. In order to
allow models with multiple
output layers, there are the following possibilities to select
the names of the output nodes in the individual output layers:
</p>

<ul>
<li><p> A <code>character</code> vector or <code>factor</code> of labels: If the model has only one output
layer, the values correspond to the labels of the output nodes named in the
passed <code>Converter</code> object, e.g.,
<code>c("a", "c", "d")</code> for the first, third and fourth output node if the
output names are <code>c("a", "b", "c", "d")</code>. If there are
multiple output layers, the names of the output nodes from the first
output layer are considered.
</p>
</li>
<li><p> A <code>list</code> of <code>charactor</code>/<code>factor</code> vectors of labels: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired labels of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of labels, e.g., <code>list(NULL, c("a", "c"))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br />
</p>
</li></ul>
</dd>
<dt><code>ignore_last_act</code></dt><dd><p>(<code>logical(1)</code>)<br />
Set this logical value to include the last
activation functions for each output layer, or not (default: <code>TRUE</code>).
In practice, the last activation (especially for softmax activation) is
often omitted.<br /></p>
</dd>
<dt><code>winner_takes_all</code></dt><dd><p>(<code>logical(1)</code>)<br />
This logical argument is only relevant for
models with a MaxPooling layer. Since many zeros are produced during
the backward pass due to the selection of the maximum value in the
pooling kernel, another variant is implemented, which treats a
MaxPooling as an AveragePooling layer in the backward pass to overcome
the problem of too many zero relevances. With the default value <code>TRUE</code>,
the whole upper-layer relevance is passed to the maximum value in each
pooling window. Otherwise, if <code>FALSE</code>, the relevance is distributed equally
among all nodes in a pooling window.<br /></p>
</dd>
<dt><code>verbose</code></dt><dd><p>(<code>logical(1)</code>)<br />
This logical argument determines whether a progress bar is
displayed for the calculation of the method or not. The default value is
the output of the primitive R function <code><a href="base.html#topic+interactive">interactive()</a></code>.<br /></p>
</dd>
<dt><code>dtype</code></dt><dd><p>(<code>character(1)</code>)<br />
The data type for the calculations. Use
either <code>'float'</code> for <a href="torch.html#topic+torch_float">torch_float</a> or <code>'double'</code> for
<a href="torch.html#topic+torch_double">torch_double</a>.<br /></p>
</dd>
</dl>

</div>


<hr>
<a id="method-InterpretingMethod-get_result"></a>



<h4>Method <code>get_result()</code></h4>

<p>This function returns the result of this method for the given data
either as an array (<code>'array'</code>), a torch tensor (<code>'torch.tensor'</code>,
or <code>'torch_tensor'</code>) of size <em>(batch_size, dim_in, dim_out)</em> or as a
data.frame (<code>'data.frame'</code>). This method is also implemented as a
generic S3 function <code><a href="#topic+get_result">get_result</a></code>. For a detailed description, we refer
to our in-depth vignette (<code>vignette("detailed_overview", package = "innsight")</code>)
or our <a href="https://bips-hb.github.io/innsight/articles/detailed_overview.html#get-results">website</a>.
</p>


<h5>Usage</h5>

<div class="r"><pre>InterpretingMethod$get_result(type = "array")</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>type</code></dt><dd><p>(<code>character(1)</code>)<br />
The data type of the result. Use one of <code>'array'</code>,
<code>'torch.tensor'</code>, <code>'torch_tensor'</code> or <code>'data.frame'</code>
(default: <code>'array'</code>).<br /></p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>The result of this method for the given data in the chosen
type.
</p>


<hr>
<a id="method-InterpretingMethod-plot"></a>



<h4>Method <code>plot()</code></h4>

<p>This method visualizes the result of the selected
method and enables a visual in-depth investigation with the help
of the S4 classes <code><a href="#topic+innsight_ggplot2">innsight_ggplot2</a></code> and <code><a href="#topic+innsight_plotly">innsight_plotly</a></code>.<br />
You can use the argument <code>data_idx</code> to select the data points in the
given data for the plot. In addition, the individual output nodes for
the plot can be selected with the argument <code>output_idx</code>. The different
results for the selected data points and outputs are visualized using
the ggplot2-based S4 class <code>innsight_ggplot2</code>. You can also use the
<code>as_plotly</code> argument to generate an interactive plot with
<code>innsight_plotly</code> based on the plot function <a href="plotly.html#topic+plot_ly">plotly::plot_ly</a>. For
more information and the whole bunch of possibilities,
see <code><a href="#topic+innsight_ggplot2">innsight_ggplot2</a></code> and <code><a href="#topic+innsight_plotly">innsight_plotly</a></code>.<br />
<br />
<strong>Notes:</strong>
</p>

<ol>
<li><p> For the interactive plotly-based plots, the suggested package
<code>plotly</code> is required.
</p>
</li>
<li><p> The ggplot2-based plots for models with multiple input layers are
a bit more complex, therefore the suggested packages <code>'grid'</code>,
<code>'gridExtra'</code> and <code>'gtable'</code> must be installed in your R session.
</p>
</li>
<li><p> If the global <em>Connection Weights</em> method was applied, the
unnecessary argument <code>data_idx</code> will be ignored.
</p>
</li>
<li><p> The predictions, the sum of relevances, and, if available, the
decomposition target are displayed by default in a box within the plot.
Currently, these are not generated for <code>plotly</code> plots.
</p>
</li></ol>



<h5>Usage</h5>

<div class="r"><pre>InterpretingMethod$plot(
  data_idx = 1,
  output_idx = NULL,
  output_label = NULL,
  aggr_channels = "sum",
  as_plotly = FALSE,
  same_scale = FALSE,
  show_preds = TRUE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>data_idx</code></dt><dd><p>(<code>integer</code>)<br />
An integer vector containing the numbers of the data
points whose result is to be plotted, e.g., <code>c(1,3)</code> for the first
and third data point in the given data. Default: <code>1</code>. This argument
will be ignored for the global <em>Connection Weights</em> method.<br /></p>
</dd>
<dt><code>output_idx</code></dt><dd><p>(<code>integer</code>, <code>list</code> or <code>NULL</code>)<br />
The indices of the output nodes for which the results
is to be plotted. This can be either a <code>integer</code> vector of indices or a
<code>list</code> of <code>integer</code> vectors of indices but must be a subset of the indices for
which the results were calculated, i.e., a subset of <code>output_idx</code> from the
initialization <code>new()</code> (see argument <code>output_idx</code> in method <code>new()</code> of
this R6 class for details). By default (<code>NULL</code>), the smallest index
of all calculated output nodes and output layers is used.<br /></p>
</dd>
<dt><code>output_label</code></dt><dd><p>(<code>character</code>, <code>factor</code>, <code>list</code> or <code>NULL</code>)<br />
These values specify the output nodes for which
the method is to be applied. Only values that were previously passed with
the argument <code>output_names</code> in the <code>converter</code> can be used. In order to
allow models with multiple
output layers, there are the following possibilities to select
the names of the output nodes in the individual output layers:
</p>

<ul>
<li><p> A <code>character</code> vector or <code>factor</code> of labels: If the model has only one output
layer, the values correspond to the labels of the output nodes named in the
passed <code>Converter</code> object, e.g.,
<code>c("a", "c", "d")</code> for the first, third and fourth output node if the
output names are <code>c("a", "b", "c", "d")</code>. If there are
multiple output layers, the names of the output nodes from the first
output layer are considered.
</p>
</li>
<li><p> A <code>list</code> of <code>charactor</code>/<code>factor</code> vectors of labels: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired labels of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of labels, e.g., <code>list(NULL, c("a", "c"))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br />
</p>
</li></ul>
</dd>
<dt><code>aggr_channels</code></dt><dd><p>(<code>character(1)</code> or <code><a href="base.html#topic+function">function</a></code>)<br />
Pass one of <code>'norm'</code>, <code>'sum'</code>, <code>'mean'</code> or a
custom function to aggregate the channels, e.g., the maximum
(<a href="base.html#topic+Extremes">base::max</a>) or minimum (<a href="base.html#topic+Extremes">base::min</a>) over the channels or only
individual channels with <code>function(x) x[1]</code>. By default (<code>'sum'</code>),
the sum of all channels is used.<br />
<em>Note:</em> This argument is used only for 2D and 3D input data.<br /></p>
</dd>
<dt><code>as_plotly</code></dt><dd><p>(<code>logical(1)</code>)<br />
This logical value (default: <code>FALSE</code>) can be used to
create an interactive plot based on the library <code>plotly</code>
(see <code><a href="#topic+innsight_plotly">innsight_plotly</a></code> for details).<br />
<em>Note:</em> Make sure that the suggested package <code>plotly</code> is installed
in your R session.<br /></p>
</dd>
<dt><code>same_scale</code></dt><dd><p>(<code>logical</code>)<br />
A logical value that specifies whether the individual plots have the
same fill scale across multiple input layers or whether each is
scaled individually. This argument is only used if more than one input
layer results are plotted.<br /></p>
</dd>
<dt><code>show_preds</code></dt><dd><p>(<code>logical</code>)<br />
This logical value indicates whether the plots display the prediction,
the sum of calculated relevances, and, if available, the targeted
decomposition value. For example, in the case of GradientxInput, the
goal is to obtain a decomposition of the predicted value, while for
DeepLift and IntegratedGradient, the goal is the difference between
the prediction and the reference value, i.e., <code class="reqn">f(x) - f(x')</code>.<br /></p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Returns either an <code><a href="#topic+innsight_ggplot2">innsight_ggplot2</a></code> (<code>as_plotly = FALSE</code>) or an
<code><a href="#topic+innsight_plotly">innsight_plotly</a></code> (<code>as_plotly = TRUE</code>) object with the plotted
individual results.
</p>


<hr>
<a id="method-InterpretingMethod-plot_global"></a>



<h4>Method <code>plot_global()</code></h4>

<p>This method visualizes the results of the selected method summarized as
boxplots/median image and enables a visual in-depth investigation of the global
behavior with the help of the S4 classes <code><a href="#topic+innsight_ggplot2">innsight_ggplot2</a></code> and
<code><a href="#topic+innsight_plotly">innsight_plotly</a></code>.<br />
You can use the argument <code>output_idx</code> to select the individual output
nodes for the plot. For tabular and 1D data, boxplots are created in
which a reference value can be selected from the data using the
<code>ref_data_idx</code> argument. For images, only the pixel-wise median is
visualized due to the complexity. The plot is generated using the
ggplot2-based S4 class <code>innsight_ggplot2</code>. You can also use the
<code>as_plotly</code> argument to generate an interactive plot with
<code>innsight_plotly</code> based on the plot function <a href="plotly.html#topic+plot_ly">plotly::plot_ly</a>. For
more information and the whole bunch of possibilities, see
<code><a href="#topic+innsight_ggplot2">innsight_ggplot2</a></code> and <code><a href="#topic+innsight_plotly">innsight_plotly</a></code>.<br /> <br />
<strong>Notes:</strong>
</p>

<ol>
<li><p> This method can only be used for the local <em>Connection Weights</em>
method, i.e., if <code>times_input</code> is <code>TRUE</code> and <code>data</code> is provided.
</p>
</li>
<li><p> For the interactive plotly-based plots, the suggested package
<code>plotly</code> is required.
</p>
</li>
<li><p> The ggplot2-based plots for models with multiple input layers are
a bit more complex, therefore the suggested packages <code>'grid'</code>,
<code>'gridExtra'</code> and <code>'gtable'</code> must be installed in your R session.
</p>
</li></ol>



<h5>Usage</h5>

<div class="r"><pre>InterpretingMethod$plot_global(
  output_idx = NULL,
  output_label = NULL,
  data_idx = "all",
  ref_data_idx = NULL,
  aggr_channels = "sum",
  preprocess_FUN = abs,
  as_plotly = FALSE,
  individual_data_idx = NULL,
  individual_max = 20
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>output_idx</code></dt><dd><p>(<code>integer</code>, <code>list</code> or <code>NULL</code>)<br />
The indices of the output nodes for which the
results is to be plotted. This can be either a <code>vector</code> of indices or
a <code>list</code> of vectors of indices but must be a subset of the indices for
which the results were calculated, i.e., a subset of <code>output_idx</code> from
the initialization <code>new()</code> (see argument <code>output_idx</code> in method <code>new()</code>
of this R6 class for details). By default (<code>NULL</code>), the smallest index
of all calculated output nodes and output layers is used.<br /></p>
</dd>
<dt><code>output_label</code></dt><dd><p>(<code>character</code>, <code>factor</code>, <code>list</code> or <code>NULL</code>)<br />
These values specify the output nodes for which
the method is to be applied. Only values that were previously passed with
the argument <code>output_names</code> in the <code>converter</code> can be used. In order to
allow models with multiple
output layers, there are the following possibilities to select
the names of the output nodes in the individual output layers:
</p>

<ul>
<li><p> A <code>character</code> vector or <code>factor</code> of labels: If the model has only one output
layer, the values correspond to the labels of the output nodes named in the
passed <code>Converter</code> object, e.g.,
<code>c("a", "c", "d")</code> for the first, third and fourth output node if the
output names are <code>c("a", "b", "c", "d")</code>. If there are
multiple output layers, the names of the output nodes from the first
output layer are considered.
</p>
</li>
<li><p> A <code>list</code> of <code>charactor</code>/<code>factor</code> vectors of labels: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired labels of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of labels, e.g., <code>list(NULL, c("a", "c"))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br />
</p>
</li></ul>
</dd>
<dt><code>data_idx</code></dt><dd><p>(<code>integer</code>)<br />
By default, all available data points are used
to calculate the boxplot information. However, this parameter can be
used to select a subset of them by passing the indices. For example, with
<code>c(1:10, 25, 26)</code> only the first 10 data points and
the 25th and 26th are used to calculate the boxplots.<br /></p>
</dd>
<dt><code>ref_data_idx</code></dt><dd><p>(<code>integer(1)</code> or <code>NULL</code>)<br />
This integer number determines the index for the
reference data point. In addition to the boxplots, it is displayed in
red color and is used to compare an individual result with the summary
statistics provided by the boxplot. With the default value (<code>NULL</code>),
no individual data point is plotted. This index can be chosen with
respect to all available data, even if only a subset is selected with
argument <code>data_idx</code>.<br />
<em>Note:</em> Because of the complexity of 2D inputs, this argument is used
only for tabular and 1D inputs and disregarded for 2D inputs.<br /></p>
</dd>
<dt><code>aggr_channels</code></dt><dd><p>(<code>character(1)</code> or <code><a href="base.html#topic+function">function</a></code>)<br />
Pass one of <code>'norm'</code>, <code>'sum'</code>, <code>'mean'</code> or a
custom function to aggregate the channels, e.g., the maximum
(<a href="base.html#topic+Extremes">base::max</a>) or minimum (<a href="base.html#topic+Extremes">base::min</a>) over the channels or only
individual channels with <code>function(x) x[1]</code>. By default (<code>'sum'</code>),
the sum of all channels is used.<br />
<em>Note:</em> This argument is used only for 2D and 3D input data.<br /></p>
</dd>
<dt><code>preprocess_FUN</code></dt><dd><p>(<code>function</code>)<br />
This function is applied to the method's result
before calculating the boxplots or medians. Since positive and negative values
often cancel each other out, the absolute value (<code>abs</code>) is used by
default. But you can also use the raw results (<code>identity</code>) to see the
results' orientation, the squared data (<code>function(x) x^2</code>) to weight
the outliers higher or any other function.<br /></p>
</dd>
<dt><code>as_plotly</code></dt><dd><p>(<code>logical(1)</code>)<br />
This logical value (default: <code>FALSE</code>) can be used to
create an interactive plot based on the library <code>plotly</code>
(see <code><a href="#topic+innsight_plotly">innsight_plotly</a></code> for details).<br />
<em>Note:</em> Make sure that the suggested package <code>plotly</code> is installed
in your R session.<br /></p>
</dd>
<dt><code>individual_data_idx</code></dt><dd><p>(<code>integer</code> or <code>NULL</code>)<br />
Only relevant for a <code>plotly</code> plot with tabular
or 1D inputs! This integer vector of data indices determines
the available data points in a dropdown menu, which are drawn
individually analogous to <code>ref_data_idx</code> only for more data points.
With the default value <code>NULL</code>, the first <code>individual_max</code> data points
are used.<br />
<em>Note:</em> If <code>ref_data_idx</code> is specified, this data point will be
added to those from <code>individual_data_idx</code> in the dropdown menu.<br /></p>
</dd>
<dt><code>individual_max</code></dt><dd><p>(<code>integer(1)</code>)<br />
Only relevant for a <code>plotly</code> plot with tabular or
1D inputs! This integer determines the maximum number of
individual data points in the dropdown menu without counting
<code>ref_data_idx</code>. This means that if <code>individual_data_idx</code> has more
than <code>individual_max</code> indices, only the first <code>individual_max</code> will
be used. A too high number can significantly increase the runtime.<br /></p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Returns either an <code><a href="#topic+innsight_ggplot2">innsight_ggplot2</a></code> (<code>as_plotly = FALSE</code>) or an
<code><a href="#topic+innsight_plotly">innsight_plotly</a></code> (<code>as_plotly = TRUE</code>) object with the plotted
summarized results.
</p>


<hr>
<a id="method-InterpretingMethod-print"></a>



<h4>Method <code>print()</code></h4>

<p>Print a summary of the method object. This summary contains the
individual fields and in particular the results of the applied method.
</p>


<h5>Usage</h5>

<div class="r"><pre>InterpretingMethod$print()</pre></div>



<h5>Returns</h5>

<p>Returns the method object invisibly via <code><a href="base.html#topic+invisible">base::invisible</a></code>.
</p>


<hr>
<a id="method-InterpretingMethod-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>InterpretingMethod$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='LIME'>Local interpretable model-agnostic explanations (LIME)</h2><span id='topic+LIME'></span>

<h3>Description</h3>

<p>The R6 class <code>LIME</code> calculates the feature weights of a linear surrogate of
the prediction model for a instance to be explained, namely the
<em>local interpretable model-agnostic explanations (LIME)</em>. It is a
model-agnostic method that can be applied to any predictive model.
This means, in particular, that
<code>LIME</code> can be applied not only to objects of the <code><a href="#topic+Converter">Converter</a></code> class but
also to any other model. The only requirement is the argument <code>pred_fun</code>,
which generates predictions with the model for given data. However, this
function is pre-implemented for models created with
<code><a href="torch.html#topic+nn_sequential">nn_sequential</a></code>, <code><a href="keras.html#topic+keras_model">keras_model</a></code>,
<code><a href="neuralnet.html#topic+neuralnet">neuralnet</a></code> or <code><a href="#topic+Converter">Converter</a></code>. Internally, the
suggested package <code>lime</code> is utilized and applied to <code>data.frame</code>.
</p>
<p>The R6 class can also be initialized using the <code><a href="#topic+run_lime">run_lime</a></code> function
as a helper function so that no prior knowledge of R6 classes is required.
</p>
<p><strong>Note:</strong> Even signal and image data are initially transformed into a
<code>data.frame</code> using <code>as.data.frame()</code> and then <code><a href="lime.html#topic+lime">lime::lime</a></code> and
<code><a href="lime.html#topic+explain">lime::explain</a></code> are
applied. In other words, a custom <code>pred_fun</code> may need to convert the
<code>data.frame</code> back into an <code>array</code> as necessary.
</p>


<h3>Super classes</h3>

<p><code><a href="#topic+InterpretingMethod">innsight::InterpretingMethod</a></code> -&gt; <code><a href="#topic+AgnosticWrapper">innsight::AgnosticWrapper</a></code> -&gt; <code>LIME</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-LIME-new"><code>LIME$new()</code></a>
</p>
</li>
<li> <p><a href="#method-LIME-clone"><code>LIME$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="get_result"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-get_result'><code>innsight::InterpretingMethod$get_result()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="plot"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-plot'><code>innsight::InterpretingMethod$plot()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="plot_global"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-plot_global'><code>innsight::InterpretingMethod$plot_global()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="print"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-print'><code>innsight::InterpretingMethod$print()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-LIME-new"></a>



<h4>Method <code>new()</code></h4>

<p>Create a new instance of the <code>LIME</code> R6 class. When initialized,
the method <em>LIME</em> is applied to the given data and the results are
stored in the field <code>result</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>LIME$new(
  model,
  data,
  data_ref,
  output_type = NULL,
  pred_fun = NULL,
  output_idx = NULL,
  output_label = NULL,
  channels_first = TRUE,
  input_dim = NULL,
  input_names = NULL,
  output_names = NULL,
  ...
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>model</code></dt><dd><p>(any prediction model)<br />
A fitted model for a classification or regression task that
is intended to be interpreted. A <code><a href="#topic+Converter">Converter</a></code> object can also be
passed. In order for the package to know how to make predictions
with the given model, a prediction function must also be passed with
the argument <code>pred_fun</code>. However, for models created by
<code><a href="torch.html#topic+nn_sequential">nn_sequential</a></code>, <code><a href="keras.html#topic+keras_model">keras_model</a></code>,
<code><a href="neuralnet.html#topic+neuralnet">neuralnet</a></code> or <code><a href="#topic+Converter">Converter</a></code>,
these have already been pre-implemented and do not need to be
specified.<br /></p>
</dd>
<dt><code>data</code></dt><dd><p>(<code>array</code>, <code>data.frame</code> or <code>torch_tensor</code>)<br />
The individual instances to be explained by the method.
These must have the same format as the input data of the passed model
and has to be either <code><a href="base.html#topic+matrix">matrix</a></code>, an <code><a href="base.html#topic+array">array</a></code>, a <code><a href="base.html#topic+data.frame">data.frame</a></code> or a
<code><a href="torch.html#topic+torch_tensor">torch_tensor</a></code>. If no value is specified, all instances in the
dataset <code>data</code> will be explained.<br />
<strong>Note:</strong> For the model-agnostic methods, only models with a single
input and output layer is allowed!<br /></p>
</dd>
<dt><code>data_ref</code></dt><dd><p>(<code><a href="base.html#topic+array">array</a></code>, <code><a href="base.html#topic+data.frame">data.frame</a></code> or <code><a href="torch.html#topic+torch_tensor">torch_tensor</a></code>)<br />
The dataset to which the method is to be applied. These must
have the same format as the input data of the passed model and has to
be either <code><a href="base.html#topic+matrix">matrix</a></code>, an <code><a href="base.html#topic+array">array</a></code>, a <code><a href="base.html#topic+data.frame">data.frame</a></code> or a
<code><a href="torch.html#topic+torch_tensor">torch_tensor</a></code>.<br />
<strong>Note:</strong> For the model-agnostic methods, only models with a single
input and output layer is allowed!<br /></p>
</dd>
<dt><code>output_type</code></dt><dd><p>(<code>character(1)</code>)<br />
Type of the model output, i.e., either
<code>"classification"</code> or <code>"regression"</code>.<br /></p>
</dd>
<dt><code>pred_fun</code></dt><dd><p>(<code>function</code>)<br />
Prediction function for the model. This argument is only
needed if <code>model</code> is not a model created by
<code><a href="torch.html#topic+nn_sequential">nn_sequential</a></code>, <code><a href="keras.html#topic+keras_model">keras_model</a></code>,
<code><a href="neuralnet.html#topic+neuralnet">neuralnet</a></code> or <code><a href="#topic+Converter">Converter</a></code>. The first argument of
<code>pred_fun</code> has to be <code>newdata</code>, e.g.,
</p>
<div class="sourceCode"><pre>function(newdata, ...) model(newdata)
</pre></div></dd>
<dt><code>output_idx</code></dt><dd><p>(<code>integer</code>, <code>list</code> or <code>NULL</code>)<br />
These indices specify the output nodes for which
the method is to be applied. In order to allow models with multiple
output layers, there are the following possibilities to select
the indices of the output nodes in the individual output layers:
</p>

<ul>
<li><p> An <code>integer</code> vector of indices: If the model has only one output
layer, the values correspond to the indices of the output nodes, e.g.,
<code>c(1,3,4)</code> for the first, third and fourth output node. If there are
multiple output layers, the indices of the output nodes from the first
output layer are considered.
</p>
</li>
<li><p> A <code>list</code> of <code>integer</code> vectors of indices: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired indices of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of indices, e.g., <code>list(NULL, c(1,3))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br />
</p>
</li></ul>
</dd>
<dt><code>output_label</code></dt><dd><p>(<code>character</code>, <code>factor</code>, <code>list</code> or <code>NULL</code>)<br />
These values specify the output nodes for which
the method is to be applied. Only values that were previously passed with
the argument <code>output_names</code> in the <code>converter</code> can be used. In order to
allow models with multiple
output layers, there are the following possibilities to select
the names of the output nodes in the individual output layers:
</p>

<ul>
<li><p> A <code>character</code> vector or <code>factor</code> of labels: If the model has only one output
layer, the values correspond to the labels of the output nodes named in the
passed <code>Converter</code> object, e.g.,
<code>c("a", "c", "d")</code> for the first, third and fourth output node if the
output names are <code>c("a", "b", "c", "d")</code>. If there are
multiple output layers, the names of the output nodes from the first
output layer are considered.
</p>
</li>
<li><p> A <code>list</code> of <code>charactor</code>/<code>factor</code> vectors of labels: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired labels of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of labels, e.g., <code>list(NULL, c("a", "c"))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br />
</p>
</li></ul>
</dd>
<dt><code>channels_first</code></dt><dd><p>(<code>logical(1)</code>)<br />
The channel position of the given data (argument
<code>data</code>). If <code>TRUE</code>, the channel axis is placed at the second position
between the batch size and the rest of the input axes, e.g.,
<code>c(10,3,32,32)</code> for a batch of ten images with three channels and a
height and width of 32 pixels. Otherwise (<code>FALSE</code>), the channel axis
is at the last position, i.e., <code>c(10,32,32,3)</code>. If the data
has no channel axis, use the default value <code>TRUE</code>.<br /></p>
</dd>
<dt><code>input_dim</code></dt><dd><p>(<code>integer</code>)<br />
The model input dimension excluding the batch
dimension. It can be specified as vector of integers, but has to be in
the format &quot;channels first&quot;.<br /></p>
</dd>
<dt><code>input_names</code></dt><dd><p>(<code>character</code>, <code>factor</code> or <code>list</code>)<br />
The input names of the model excluding the batch dimension. For a model
with a single input layer and input axis (e.g., for tabular data), the
input names can be specified as a character vector or factor, e.g.,
for a dense layer with 3 input features use <code>c("X1", "X2", "X3")</code>. If
the model input consists of multiple axes (e.g., for signal and
image data), use a list of character vectors or factors for each axis
in the format &quot;channels first&quot;, e.g., use
<code>list(c("C1", "C2"), c("L1","L2","L3","L4","L5"))</code> for a 1D
convolutional input layer with signal length 4 and 2 channels.<br />
<em>Note:</em> This argument is optional and otherwise the names are
generated automatically. But if this argument is set, all found
input names in the passed model will be disregarded.<br /></p>
</dd>
<dt><code>output_names</code></dt><dd><p>(<code>character</code>, <code>factor</code> )<br />
A character vector with the names for the output dimensions
excluding the batch dimension, e.g., for a model with 3 output nodes use
<code>c("Y1", "Y2", "Y3")</code>. Instead of a character
vector you can also use a factor to set an order for the plots.<br />
<em>Note:</em> This argument is optional and otherwise the names are
generated automatically. But if this argument is set, all found
output names in the passed model will be disregarded.<br /></p>
</dd>
<dt><code>...</code></dt><dd><p>other arguments forwarded to <code><a href="lime.html#topic+explain">lime::explain</a></code>.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-LIME-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>LIME$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other methods: 
<code><a href="#topic+ConnectionWeights">ConnectionWeights</a></code>,
<code><a href="#topic+DeepLift">DeepLift</a></code>,
<code><a href="#topic+DeepSHAP">DeepSHAP</a></code>,
<code><a href="#topic+ExpectedGradient">ExpectedGradient</a></code>,
<code><a href="#topic+Gradient">Gradient</a></code>,
<code><a href="#topic+IntegratedGradient">IntegratedGradient</a></code>,
<code><a href="#topic+LRP">LRP</a></code>,
<code><a href="#topic+SHAP">SHAP</a></code>,
<code><a href="#topic+SmoothGrad">SmoothGrad</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#----------------------- Example 1: Torch -----------------------------------
library(torch)

# Create nn_sequential model and data
model &lt;- nn_sequential(
  nn_linear(5, 12),
  nn_relu(),
  nn_linear(12, 2),
  nn_softmax(dim = 2)
  )
data &lt;- torch_randn(25, 5)

# Calculate LIME for the first 10 instances and set the
# feature and outcome names
lime &lt;- LIME$new(model, data[1:10, ], data_ref = data,
                 input_names = c("Car", "Cat", "Dog", "Plane", "Horse"),
                 output_names = c("Buy it!", "Don't buy it!"))

# You can also use the helper function `run_lime` for initializing
# an R6 LIME object
lime &lt;- run_lime(model, data[1:10, ], data_ref = data,
                 input_names = c("Car", "Cat", "Dog", "Plane", "Horse"),
                 output_names = c("Buy it!", "Don't buy it!"))

# Get the result as an array for the first two instances
get_result(lime)[1:2,, ]

# Plot the result for both classes
plot(lime, output_idx = c(1, 2))

# Show the boxplot over all 10 instances
boxplot(lime, output_idx = c(1, 2))

# We can also forward some arguments to lime::explain, e.g. n_permutatuins
# to get more accurate values
lime &lt;- run_lime(model, data[1:10, ], data_ref = data,
                 input_names = c("Car", "Cat", "Dog", "Plane", "Horse"),
                 output_names = c("Buy it!", "Don't buy it!"),
                 n_perturbations = 200)

# Plot the boxplots again
boxplot(lime, output_idx = c(1, 2))

#----------------------- Example 2: Converter object --------------------------
# We can do the same with an Converter object (all feature and outcome names
# will be extracted by the LIME method!)
conv &lt;- convert(model,
                input_dim = c(5),
                input_names = c("Car", "Cat", "Dog", "Plane", "Horse"),
                output_names = c("Buy it!", "Don't buy it!"))

# Calculate LIME for the first 10 instances
lime &lt;- run_lime(conv, data[1:10], data_ref = data, n_perturbations = 300)

# Plot the result for both classes
plot(lime, output_idx = c(1, 2))

#----------------------- Example 3: Other model -------------------------------
if (require("neuralnet") &amp; require("ranger")) {
  library(neuralnet)
  library(ranger)
  data(iris)

  # Fit a random forest unsing the ranger package
  model &lt;- ranger(Species ~ ., data = iris, probability = TRUE)

  # There is no pre-implemented predict function for ranger models, i.e.,
  # we have to define it ourselves.
  pred_fun &lt;- function(newdata, ...) {
    predict(model, newdata, ...)$predictions
  }

  # Calculate LIME for the instances of index 1 and 111 and add
  # the outcome labels (for LIME, the output_type is required!)
  lime &lt;- run_lime(model, iris[c(1, 111), -5],
                   data_ref = iris[, -5],
                   pred_fun = pred_fun,
                   output_type = "classification",
                   output_names = levels(iris$Species),
                   n_perturbations = 300)

  # Plot the result for the first two classes and all selected instances
  plot(lime, data_idx = 1:2, output_idx = 1:2)

  # Get the result as a torch_tensor
  get_result(lime, "torch_tensor")
}

</code></pre>

<hr>
<h2 id='LRP'>Layer-wise relevance propagation (LRP)</h2><span id='topic+LRP'></span>

<h3>Description</h3>

<p>This is an implementation of the <em>layer-wise relevance propagation
(LRP)</em> algorithm introduced by Bach et al. (2015). It's a local method for
interpreting a single element of the dataset and calculates the relevance
scores for each input feature to the model output. The basic idea of this
method is to decompose the prediction score of the model with respect to
the input features, i.e.,
</p>
<p style="text-align: center;"><code class="reqn">f(x) = \sum_i R(x_i).</code>
</p>

<p>Because of the bias vector that absorbs some relevance, this decomposition
is generally an approximation. There exist several propagation rules to
determine the relevance scores. In this package are implemented: simple
rule (&quot;simple&quot;), <code class="reqn">\varepsilon</code>-rule (&quot;epsilon&quot;) and
<code class="reqn">\alpha</code>-<code class="reqn">\beta</code>-rule (&quot;alpha_beta&quot;).
</p>
<p>The R6 class can also be initialized using the <code><a href="#topic+run_lrp">run_lrp</a></code> function
as a helper function so that no prior knowledge of R6 classes is required.
</p>


<h3>Super class</h3>

<p><code><a href="#topic+InterpretingMethod">innsight::InterpretingMethod</a></code> -&gt; <code>LRP</code>
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>rule_name</code></dt><dd><p>(<code>character(1)</code> or <code>list</code>)<br />
The name of the rule with which the relevance scores
are calculated. Implemented are <code>"simple"</code>, <code>"epsilon"</code>, <code>"alpha_beta"</code>
(and <code>"pass"</code> but only for 'BatchNorm_Layer'). However, this value
can also be a named list that assigns one of these three rules to each
implemented layer type separately, e.g.,
<code>list(Dense_Layer = "simple", Conv2D_Layer = "alpha_beta")</code>.
Layers not specified in this list then use the default value <code>"simple"</code>.
The implemented layer types are:
</p>

<table>
<tr>
 <td style="text-align: left;">
<code class="reqn">\cdot</code> 'Dense_Layer' </td><td style="text-align: left;"> <code class="reqn">\cdot</code> 'Conv1D_Layer' </td><td style="text-align: left;"> <code class="reqn">\cdot</code> 'Conv2D_Layer'</td>
</tr>
<tr>
 <td style="text-align: left;">
<code class="reqn">\cdot</code> 'BatchNorm_Layer' </td><td style="text-align: left;"> <code class="reqn">\cdot</code> 'AvgPool1D_Layer' </td><td style="text-align: left;"> <code class="reqn">\cdot</code> 'AvgPool2D_Layer'</td>
</tr>
<tr>
 <td style="text-align: left;">
<code class="reqn">\cdot</code> 'MaxPool1D_Layer' </td><td style="text-align: left;"> <code class="reqn">\cdot</code> 'MaxPool2D_Layer' </td><td style="text-align: left;">
</td>
</tr>

</table>
</dd>
<dt><code>rule_param</code></dt><dd><p>(<code>numeric</code> or <code>list</code>)<br />
The parameter of the selected rule. Similar to the
argument <code>rule_name</code>, this can also be a named list that assigns a
rule parameter to each layer type.<br /></p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-LRP-new"><code>LRP$new()</code></a>
</p>
</li>
<li> <p><a href="#method-LRP-clone"><code>LRP$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="get_result"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-get_result'><code>innsight::InterpretingMethod$get_result()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="plot"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-plot'><code>innsight::InterpretingMethod$plot()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="plot_global"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-plot_global'><code>innsight::InterpretingMethod$plot_global()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="print"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-print'><code>innsight::InterpretingMethod$print()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-LRP-new"></a>



<h4>Method <code>new()</code></h4>

<p>Create a new instance of the <code>LRP</code> R6 class. When initialized,
the method <em>LRP</em> is applied to the given data and the results are stored in
the field <code>result</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>LRP$new(
  converter,
  data,
  channels_first = TRUE,
  output_idx = NULL,
  output_label = NULL,
  ignore_last_act = TRUE,
  rule_name = "simple",
  rule_param = NULL,
  winner_takes_all = TRUE,
  verbose = interactive(),
  dtype = "float"
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>converter</code></dt><dd><p>(<code><a href="#topic+Converter">Converter</a></code>)<br />
An instance of the <code>Converter</code> class that includes the
torch-converted model and some other model-specific attributes. See
<code><a href="#topic+Converter">Converter</a></code> for details.<br /></p>
</dd>
<dt><code>data</code></dt><dd><p>(<code><a href="base.html#topic+array">array</a></code>, <code><a href="base.html#topic+data.frame">data.frame</a></code>, <code><a href="torch.html#topic+torch_tensor">torch_tensor</a></code> or <code>list</code>)<br />
The data to which the method is to be applied. These must
have the same format as the input data of the passed model to the
converter object. This means either
</p>

<ul>
<li><p> an <code>array</code>, <code>data.frame</code>, <code>torch_tensor</code> or array-like format of
size <em>(batch_size, dim_in)</em>, if e.g., the model has only one input layer, or
</p>
</li>
<li><p> a <code>list</code> with the corresponding input data (according to the
upper point) for each of the input layers.<br />
</p>
</li></ul>
</dd>
<dt><code>channels_first</code></dt><dd><p>(<code>logical(1)</code>)<br />
The channel position of the given data (argument
<code>data</code>). If <code>TRUE</code>, the channel axis is placed at the second position
between the batch size and the rest of the input axes, e.g.,
<code>c(10,3,32,32)</code> for a batch of ten images with three channels and a
height and width of 32 pixels. Otherwise (<code>FALSE</code>), the channel axis
is at the last position, i.e., <code>c(10,32,32,3)</code>. If the data
has no channel axis, use the default value <code>TRUE</code>.<br /></p>
</dd>
<dt><code>output_idx</code></dt><dd><p>(<code>integer</code>, <code>list</code> or <code>NULL</code>)<br />
These indices specify the output nodes for which
the method is to be applied. In order to allow models with multiple
output layers, there are the following possibilities to select
the indices of the output nodes in the individual output layers:
</p>

<ul>
<li><p> An <code>integer</code> vector of indices: If the model has only one output
layer, the values correspond to the indices of the output nodes, e.g.,
<code>c(1,3,4)</code> for the first, third and fourth output node. If there are
multiple output layers, the indices of the output nodes from the first
output layer are considered.
</p>
</li>
<li><p> A <code>list</code> of <code>integer</code> vectors of indices: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired indices of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of indices, e.g., <code>list(NULL, c(1,3))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br />
</p>
</li></ul>
</dd>
<dt><code>output_label</code></dt><dd><p>(<code>character</code>, <code>factor</code>, <code>list</code> or <code>NULL</code>)<br />
These values specify the output nodes for which
the method is to be applied. Only values that were previously passed with
the argument <code>output_names</code> in the <code>converter</code> can be used. In order to
allow models with multiple
output layers, there are the following possibilities to select
the names of the output nodes in the individual output layers:
</p>

<ul>
<li><p> A <code>character</code> vector or <code>factor</code> of labels: If the model has only one output
layer, the values correspond to the labels of the output nodes named in the
passed <code>Converter</code> object, e.g.,
<code>c("a", "c", "d")</code> for the first, third and fourth output node if the
output names are <code>c("a", "b", "c", "d")</code>. If there are
multiple output layers, the names of the output nodes from the first
output layer are considered.
</p>
</li>
<li><p> A <code>list</code> of <code>charactor</code>/<code>factor</code> vectors of labels: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired labels of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of labels, e.g., <code>list(NULL, c("a", "c"))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br />
</p>
</li></ul>
</dd>
<dt><code>ignore_last_act</code></dt><dd><p>(<code>logical(1)</code>)<br />
Set this logical value to include the last
activation functions for each output layer, or not (default: <code>TRUE</code>).
In practice, the last activation (especially for softmax activation) is
often omitted.<br /></p>
</dd>
<dt><code>rule_name</code></dt><dd><p>(<code>character(1)</code> or <code>list</code>)<br />
The name of the rule with which the relevance scores
are calculated. Implemented are <code>"simple"</code>, <code>"epsilon"</code>, <code>"alpha_beta"</code>.
You can pass one of the above characters to apply this rule to all
possible layers. However, this value can also be a named list that
assigns one of these three rules to each
implemented layer type separately, e.g.,
<code>list(Dense_Layer = "simple", Conv2D_Layer = "alpha_beta")</code>.
Layers not specified in this list then use the default value <code>"simple"</code>.
The implemented layer types are:<br />
</p>

<table>
<tr>
 <td style="text-align: left;">
<code class="reqn">\cdot</code> 'Dense_Layer' </td><td style="text-align: left;"> <code class="reqn">\cdot</code> 'Conv1D_Layer' </td><td style="text-align: left;"> <code class="reqn">\cdot</code> 'Conv2D_Layer'</td>
</tr>
<tr>
 <td style="text-align: left;">
<code class="reqn">\cdot</code> 'BatchNorm_Layer' </td><td style="text-align: left;"> <code class="reqn">\cdot</code> 'AvgPool1D_Layer' </td><td style="text-align: left;"> <code class="reqn">\cdot</code> 'AvgPool2D_Layer'</td>
</tr>
<tr>
 <td style="text-align: left;">
<code class="reqn">\cdot</code> 'MaxPool1D_Layer' </td><td style="text-align: left;"> <code class="reqn">\cdot</code> 'MaxPool2D_Layer' </td><td style="text-align: left;">
</td>
</tr>

</table>

<p><em>Note:</em> For normalization layers like 'BatchNorm_Layer', the rule
<code>"pass"</code> is implemented as well, which ignores such layers in the
backward pass.<br /></p>
</dd>
<dt><code>rule_param</code></dt><dd><p>(<code>numeric(1)</code> or <code>list</code>)<br />
The parameter of the selected rule. Note: Only the
rules <code>"epsilon"</code> and <code>"alpha_beta"</code> take use of the
parameter. Use the default value <code>NULL</code> for the default parameters
(&quot;epsilon&quot; : <code class="reqn">0.01</code>, &quot;alpha_beta&quot; : <code class="reqn">0.5</code>). Similar to the
argument <code>rule_name</code>, this can also be a named list that assigns a
rule parameter to each layer type. If the layer type is not specified
in the named list, the default parameters will be used.<br /></p>
</dd>
<dt><code>winner_takes_all</code></dt><dd><p>(<code>logical(1)</code>)<br />
This logical argument is only relevant for
models with a MaxPooling layer. Since many zeros are produced during
the backward pass due to the selection of the maximum value in the
pooling kernel, another variant is implemented, which treats a
MaxPooling as an AveragePooling layer in the backward pass to overcome
the problem of too many zero relevances. With the default value <code>TRUE</code>,
the whole upper-layer relevance is passed to the maximum value in each
pooling window. Otherwise, if <code>FALSE</code>, the relevance is distributed equally
among all nodes in a pooling window.<br /></p>
</dd>
<dt><code>verbose</code></dt><dd><p>(<code>logical(1)</code>)<br />
This logical argument determines whether a progress bar is
displayed for the calculation of the method or not. The default value is
the output of the primitive R function <code><a href="base.html#topic+interactive">interactive()</a></code>.<br /></p>
</dd>
<dt><code>dtype</code></dt><dd><p>(<code>character(1)</code>)<br />
The data type for the calculations. Use
either <code>'float'</code> for <a href="torch.html#topic+torch_float">torch_float</a> or <code>'double'</code> for
<a href="torch.html#topic+torch_double">torch_double</a>.<br /></p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>A new instance of the R6 class <code>LRP</code>.
</p>


<hr>
<a id="method-LRP-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>LRP$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>References</h3>

<p>S. Bach et al. (2015) <em>On pixel-wise explanations for non-linear
classifier decisions by layer-wise relevance propagation.</em> PLoS ONE 10,
p. 1-46
</p>


<h3>See Also</h3>

<p>Other methods: 
<code><a href="#topic+ConnectionWeights">ConnectionWeights</a></code>,
<code><a href="#topic+DeepLift">DeepLift</a></code>,
<code><a href="#topic+DeepSHAP">DeepSHAP</a></code>,
<code><a href="#topic+ExpectedGradient">ExpectedGradient</a></code>,
<code><a href="#topic+Gradient">Gradient</a></code>,
<code><a href="#topic+IntegratedGradient">IntegratedGradient</a></code>,
<code><a href="#topic+LIME">LIME</a></code>,
<code><a href="#topic+SHAP">SHAP</a></code>,
<code><a href="#topic+SmoothGrad">SmoothGrad</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 #----------------------- Example 1: Torch ----------------------------------
library(torch)

# Create nn_sequential model and data
model &lt;- nn_sequential(
  nn_linear(5, 12),
  nn_relu(),
  nn_linear(12, 2),
  nn_softmax(dim = 2)
)
data &lt;- torch_randn(25, 5)

# Create Converter
converter &lt;- convert(model, input_dim = c(5))

# Apply method LRP with simple rule (default)
lrp &lt;- LRP$new(converter, data)

# You can also use the helper function `run_lrp` for initializing
# an R6 LRP object
lrp &lt;- run_lrp(converter, data)

# Print the result as an array for data point one and two
get_result(lrp)[1:2,,]

# Plot the result for both classes
plot(lrp, output_idx = 1:2)

# Plot the boxplot of all datapoints without a preprocess function
boxplot(lrp, output_idx = 1:2, preprocess_FUN = identity)

# ------------------------- Example 2: Neuralnet ---------------------------
if (require("neuralnet")) {
  library(neuralnet)
  data(iris)
  nn &lt;- neuralnet(Species ~ .,
    iris,
    linear.output = FALSE,
    hidden = c(10, 8), act.fct = "tanh", rep = 1, threshold = 0.5
  )

  # Create an converter for this model
  converter &lt;- convert(nn)

  # Create new instance of 'LRP'
  lrp &lt;- run_lrp(converter, iris[, -5], rule_name = "simple")

  # Get the result as an array for data point one and two
  get_result(lrp)[1:2,,]

  # Get the result as a torch tensor for data point one and two
  get_result(lrp, type = "torch.tensor")[1:2]

  # Use the alpha-beta rule with alpha = 2
  lrp &lt;- run_lrp(converter, iris[, -5],
    rule_name = "alpha_beta",
    rule_param = 2
  )

  # Include the last activation into the calculation
  lrp &lt;- run_lrp(converter, iris[, -5],
    rule_name = "alpha_beta",
    rule_param = 2,
    ignore_last_act = FALSE
  )

  # Plot the result for all classes
  plot(lrp, output_idx = 1:3)
}


# ------------------------- Example 3: Keras -------------------------------
if (require("keras") &amp; keras::is_keras_available()) {
  library(keras)

  # Make sure keras is installed properly
  is_keras_available()

  data &lt;- array(rnorm(10 * 60 * 3), dim = c(10, 60, 3))

  model &lt;- keras_model_sequential()
  model %&gt;%
    layer_conv_1d(
      input_shape = c(60, 3), kernel_size = 8, filters = 8,
      activation = "softplus", padding = "valid") %&gt;%
    layer_conv_1d(
      kernel_size = 8, filters = 4, activation = "tanh",
      padding = "same") %&gt;%
    layer_conv_1d(
      kernel_size = 4, filters = 2, activation = "relu",
      padding = "valid") %&gt;%
    layer_flatten() %&gt;%
    layer_dense(units = 64, activation = "relu") %&gt;%
    layer_dense(units = 16, activation = "relu") %&gt;%
    layer_dense(units = 3, activation = "softmax")

  # Convert the model
  converter &lt;- convert(model)

  # Apply the LRP method with the epsilon rule for the dense layers and
  # the alpha-beta rule for the convolutional layers
  lrp_comp &lt;- run_lrp(converter, data,
    channels_first = FALSE,
    rule_name = list(Dense_Layer = "epsilon", Conv1D_Layer = "alpha_beta"),
    rule_param = list(Dense_Layer = 0.1, Conv1D_Layer = 1)
  )

  # Plot the result for the first datapoint and all classes
  plot(lrp_comp, output_idx = 1:3)

  # Plot the result as boxplots for first two classes
  boxplot(lrp_comp, output_idx = 1:2)
}


#------------------------- Plotly plots ------------------------------------
if (require("plotly")) {
  # You can also create an interactive plot with plotly.
  # This is a suggested package, so make sure that it is installed
  library(plotly)

  # Result as boxplots
  boxplot(lrp, as_plotly = TRUE)

  # Result of the second data point
  plot(lrp, data_idx = 2, as_plotly = TRUE)
}

</code></pre>

<hr>
<h2 id='plot_global'>Get the result of an interpretation method</h2><span id='topic+plot_global'></span>

<h3>Description</h3>

<p>This is a generic S3 method for the R6 method
<code>InterpretingMethod$plot_global()</code>. See the respective method described in
<code><a href="#topic+InterpretingMethod">InterpretingMethod</a></code> for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_global(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_global_+3A_x">x</code></td>
<td>
<p>An object of the class <code><a href="#topic+InterpretingMethod">InterpretingMethod</a></code> including the
subclasses <code><a href="#topic+Gradient">Gradient</a></code>, <code><a href="#topic+SmoothGrad">SmoothGrad</a></code>, <code><a href="#topic+LRP">LRP</a></code>, <code><a href="#topic+DeepLift">DeepLift</a></code>,
<code><a href="#topic+DeepSHAP">DeepSHAP</a></code>, <code><a href="#topic+IntegratedGradient">IntegratedGradient</a></code>, <code><a href="#topic+ExpectedGradient">ExpectedGradient</a></code> and
<code><a href="#topic+ConnectionWeights">ConnectionWeights</a></code>.</p>
</td></tr>
<tr><td><code id="plot_global_+3A_...">...</code></td>
<td>
<p>Other arguments specified in the R6 method
<code>InterpretingMethod$plot_global()</code>. See <code><a href="#topic+InterpretingMethod">InterpretingMethod</a></code> for details.</p>
</td></tr>
</table>

<hr>
<h2 id='print+2Cinnsight_ggplot2-method'>Generic print, plot and show for <code>innsight_ggplot2</code></h2><span id='topic+print+2Cinnsight_ggplot2-method'></span><span id='topic+print.innsight_ggplot2'></span><span id='topic+show+2Cinnsight_ggplot2-method'></span><span id='topic+show.innsight_ggplot2'></span><span id='topic+plot+2Cinnsight_ggplot2-method'></span><span id='topic+plot.innsight_ggplot2'></span>

<h3>Description</h3>

<p>The class <a href="#topic+innsight_ggplot2">innsight_ggplot2</a> provides the generic visualization functions
<code><a href="base.html#topic+print">print</a></code>, <code><a href="graphics.html#topic+plot">plot</a></code> and <code><a href="methods.html#topic+show">show</a></code>, which all
behave the same in this case. They create the plot of the results
(see <a href="#topic+innsight_ggplot2">innsight_ggplot2</a> for details) and return it invisibly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'innsight_ggplot2'
print(x, ...)

## S4 method for signature 'innsight_ggplot2'
show(object)

## S4 method for signature 'innsight_ggplot2'
plot(x, y, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print+2B2Cinnsight_ggplot2-method_+3A_x">x</code></td>
<td>
<p>An instance of the S4 class <a href="#topic+innsight_ggplot2">innsight_ggplot2</a>.</p>
</td></tr>
<tr><td><code id="print+2B2Cinnsight_ggplot2-method_+3A_...">...</code></td>
<td>
<p>Further arguments passed to the base function <code>print</code> if
<code>x@multiplot</code> is <code>FALSE</code>. Otherwise, if <code>x@multiplot</code> is <code>TRUE</code>, the
arguments are passed to <a href="gridExtra.html#topic+arrangeGrob">gridExtra::arrangeGrob</a>.</p>
</td></tr>
<tr><td><code id="print+2B2Cinnsight_ggplot2-method_+3A_object">object</code></td>
<td>
<p>An instance of the S4 class <a href="#topic+innsight_ggplot2">innsight_ggplot2</a>.</p>
</td></tr>
<tr><td><code id="print+2B2Cinnsight_ggplot2-method_+3A_y">y</code></td>
<td>
<p>unused argument</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For multiple plots (<code>x@multiplot = TRUE</code>), a <a href="gtable.html#topic+gtable">gtable::gtable</a> and
otherwise a <a href="ggplot2.html#topic+ggplot">ggplot2::ggplot</a> object is returned invisibly.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+innsight_ggplot2">innsight_ggplot2</a></code>,
<code><a href="#topic++2B.innsight_ggplot2">+.innsight_ggplot2</a></code>,
<code><a href="#topic++5B.innsight_ggplot2">[.innsight_ggplot2</a></code>,
<code><a href="#topic++5B+5B.innsight_ggplot2">[[.innsight_ggplot2</a></code>,
<code><a href="#topic++5B+3C-.innsight_ggplot2">[&lt;-.innsight_ggplot2</a></code>, <code><a href="#topic++5B+5B+3C-.innsight_ggplot2">[[&lt;-.innsight_ggplot2</a></code>
</p>

<hr>
<h2 id='print+2Cinnsight_plotly-method'>Generic print, plot and show for <code>innsight_plotly</code></h2><span id='topic+print+2Cinnsight_plotly-method'></span><span id='topic+print.innsight_plotly'></span><span id='topic+show+2Cinnsight_plotly-method'></span><span id='topic+show.innsight_plotly'></span><span id='topic+plot+2Cinnsight_plotly-method'></span><span id='topic+plot.innsight_plotly'></span>

<h3>Description</h3>

<p>The class <code><a href="#topic+innsight_plotly">innsight_plotly</a></code> provides the generic visualization functions
<code><a href="base.html#topic+print">print</a></code>, <code><a href="graphics.html#topic+plot">plot</a></code> and <code><a href="methods.html#topic+show">show</a></code>, which all
behave the same in this case. They create a plot of the results using
<code><a href="plotly.html#topic+subplot">plotly::subplot</a></code> (see <code><a href="#topic+innsight_plotly">innsight_plotly</a></code> for details) and return it
invisibly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'innsight_plotly'
print(x, shareX = TRUE, ...)

## S4 method for signature 'innsight_plotly'
show(object)

## S4 method for signature 'innsight_plotly'
plot(x, y, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print+2B2Cinnsight_plotly-method_+3A_x">x</code></td>
<td>
<p>An instance of the S4 class <code><a href="#topic+innsight_plotly">innsight_plotly</a></code>.</p>
</td></tr>
<tr><td><code id="print+2B2Cinnsight_plotly-method_+3A_sharex">shareX</code></td>
<td>
<p>A logical value whether the x-axis should be shared among
the subplots.</p>
</td></tr>
<tr><td><code id="print+2B2Cinnsight_plotly-method_+3A_...">...</code></td>
<td>
<p>Further arguments passed to <code><a href="plotly.html#topic+subplot">plotly::subplot</a></code>.</p>
</td></tr>
<tr><td><code id="print+2B2Cinnsight_plotly-method_+3A_object">object</code></td>
<td>
<p>An instance of the S4 class <code><a href="#topic+innsight_plotly">innsight_plotly</a></code>.</p>
</td></tr>
<tr><td><code id="print+2B2Cinnsight_plotly-method_+3A_y">y</code></td>
<td>
<p>unused argument</p>
</td></tr>
</table>

<hr>
<h2 id='SHAP'>Shapley values</h2><span id='topic+SHAP'></span>

<h3>Description</h3>

<p>The R6 class <code>SHAP</code> calculates the famous Shapley values based on game
theory for an instance to be explained. It is a model-agnostic method
that can be applied to any predictive model. This means, in particular, that
<code>SHAP</code> can be applied not only to objects of the <code><a href="#topic+Converter">Converter</a></code> class but
also to any other model. The only requirement is the argument <code>pred_fun</code>,
which generates predictions with the model for given data. However, this
function is pre-implemented for models created with
<code><a href="torch.html#topic+nn_sequential">nn_sequential</a></code>, <code><a href="keras.html#topic+keras_model">keras_model</a></code>,
<code><a href="neuralnet.html#topic+neuralnet">neuralnet</a></code> or <code><a href="#topic+Converter">Converter</a></code>. Internally, the
suggested package <code>fastshap</code> is utilized and applied to <code>data.frame</code>.
</p>
<p>The R6 class can also be initialized using the <code><a href="#topic+run_shap">run_shap</a></code> function
as a helper function so that no prior knowledge of R6 classes is required.
</p>
<p><strong>Note:</strong> Even signal and image data are initially transformed into a
<code>data.frame</code> using <code>as.data.frame()</code> and then <code><a href="fastshap.html#topic+explain">fastshap::explain</a></code> is
applied. In other words, a custom <code>pred_fun</code> may need to convert the
<code>data.frame</code> back into an <code>array</code> as necessary.
</p>


<h3>Super classes</h3>

<p><code><a href="#topic+InterpretingMethod">innsight::InterpretingMethod</a></code> -&gt; <code><a href="#topic+AgnosticWrapper">innsight::AgnosticWrapper</a></code> -&gt; <code>SHAP</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-SHAP-new"><code>SHAP$new()</code></a>
</p>
</li>
<li> <p><a href="#method-SHAP-clone"><code>SHAP$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="get_result"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-get_result'><code>innsight::InterpretingMethod$get_result()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="plot"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-plot'><code>innsight::InterpretingMethod$plot()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="plot_global"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-plot_global'><code>innsight::InterpretingMethod$plot_global()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="print"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-print'><code>innsight::InterpretingMethod$print()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-SHAP-new"></a>



<h4>Method <code>new()</code></h4>

<p>Create a new instance of the <code>SHAP</code> R6 class. When initialized,
the method <em>SHAP</em> is applied to the given data and the results are
stored in the field <code>result</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>SHAP$new(
  model,
  data,
  data_ref,
  pred_fun = NULL,
  output_idx = NULL,
  output_label = NULL,
  channels_first = TRUE,
  input_dim = NULL,
  input_names = NULL,
  output_names = NULL,
  ...
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>model</code></dt><dd><p>(any prediction model)<br />
A fitted model for a classification or regression task that
is intended to be interpreted. A <code><a href="#topic+Converter">Converter</a></code> object can also be
passed. In order for the package to know how to make predictions
with the given model, a prediction function must also be passed with
the argument <code>pred_fun</code>. However, for models created by
<code><a href="torch.html#topic+nn_sequential">nn_sequential</a></code>, <code><a href="keras.html#topic+keras_model">keras_model</a></code>,
<code><a href="neuralnet.html#topic+neuralnet">neuralnet</a></code> or <code><a href="#topic+Converter">Converter</a></code>,
these have already been pre-implemented and do not need to be
specified.<br /></p>
</dd>
<dt><code>data</code></dt><dd><p>(<code>array</code>, <code>data.frame</code> or <code>torch_tensor</code>)<br />
The individual instances to be explained by the method.
These must have the same format as the input data of the passed model
and has to be either <code><a href="base.html#topic+matrix">matrix</a></code>, an <code><a href="base.html#topic+array">array</a></code>, a <code><a href="base.html#topic+data.frame">data.frame</a></code> or a
<code><a href="torch.html#topic+torch_tensor">torch_tensor</a></code>. If no value is specified, all instances in the
dataset <code>data</code> will be explained.<br />
<strong>Note:</strong> For the model-agnostic methods, only models with a single
input and output layer is allowed!<br /></p>
</dd>
<dt><code>data_ref</code></dt><dd><p>(<code><a href="base.html#topic+array">array</a></code>, <code><a href="base.html#topic+data.frame">data.frame</a></code> or <code><a href="torch.html#topic+torch_tensor">torch_tensor</a></code>)<br />
The dataset to which the method is to be applied. These must
have the same format as the input data of the passed model and has to
be either <code><a href="base.html#topic+matrix">matrix</a></code>, an <code><a href="base.html#topic+array">array</a></code>, a <code><a href="base.html#topic+data.frame">data.frame</a></code> or a
<code><a href="torch.html#topic+torch_tensor">torch_tensor</a></code>.<br />
<strong>Note:</strong> For the model-agnostic methods, only models with a single
input and output layer is allowed!<br /></p>
</dd>
<dt><code>pred_fun</code></dt><dd><p>(<code>function</code>)<br />
Prediction function for the model. This argument is only
needed if <code>model</code> is not a model created by
<code><a href="torch.html#topic+nn_sequential">nn_sequential</a></code>, <code><a href="keras.html#topic+keras_model">keras_model</a></code>,
<code><a href="neuralnet.html#topic+neuralnet">neuralnet</a></code> or <code><a href="#topic+Converter">Converter</a></code>. The first argument of
<code>pred_fun</code> has to be <code>newdata</code>, e.g.,
</p>
<div class="sourceCode"><pre>function(newdata, ...) model(newdata)
</pre></div></dd>
<dt><code>output_idx</code></dt><dd><p>(<code>integer</code>, <code>list</code> or <code>NULL</code>)<br />
These indices specify the output nodes for which
the method is to be applied. In order to allow models with multiple
output layers, there are the following possibilities to select
the indices of the output nodes in the individual output layers:
</p>

<ul>
<li><p> An <code>integer</code> vector of indices: If the model has only one output
layer, the values correspond to the indices of the output nodes, e.g.,
<code>c(1,3,4)</code> for the first, third and fourth output node. If there are
multiple output layers, the indices of the output nodes from the first
output layer are considered.
</p>
</li>
<li><p> A <code>list</code> of <code>integer</code> vectors of indices: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired indices of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of indices, e.g., <code>list(NULL, c(1,3))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br />
</p>
</li></ul>
</dd>
<dt><code>output_label</code></dt><dd><p>(<code>character</code>, <code>factor</code>, <code>list</code> or <code>NULL</code>)<br />
These values specify the output nodes for which
the method is to be applied. Only values that were previously passed with
the argument <code>output_names</code> in the <code>converter</code> can be used. In order to
allow models with multiple
output layers, there are the following possibilities to select
the names of the output nodes in the individual output layers:
</p>

<ul>
<li><p> A <code>character</code> vector or <code>factor</code> of labels: If the model has only one output
layer, the values correspond to the labels of the output nodes named in the
passed <code>Converter</code> object, e.g.,
<code>c("a", "c", "d")</code> for the first, third and fourth output node if the
output names are <code>c("a", "b", "c", "d")</code>. If there are
multiple output layers, the names of the output nodes from the first
output layer are considered.
</p>
</li>
<li><p> A <code>list</code> of <code>charactor</code>/<code>factor</code> vectors of labels: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired labels of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of labels, e.g., <code>list(NULL, c("a", "c"))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br />
</p>
</li></ul>
</dd>
<dt><code>channels_first</code></dt><dd><p>(<code>logical(1)</code>)<br />
The channel position of the given data (argument
<code>data</code>). If <code>TRUE</code>, the channel axis is placed at the second position
between the batch size and the rest of the input axes, e.g.,
<code>c(10,3,32,32)</code> for a batch of ten images with three channels and a
height and width of 32 pixels. Otherwise (<code>FALSE</code>), the channel axis
is at the last position, i.e., <code>c(10,32,32,3)</code>. If the data
has no channel axis, use the default value <code>TRUE</code>.<br /></p>
</dd>
<dt><code>input_dim</code></dt><dd><p>(<code>integer</code>)<br />
The model input dimension excluding the batch
dimension. It can be specified as vector of integers, but has to be in
the format &quot;channels first&quot;.<br /></p>
</dd>
<dt><code>input_names</code></dt><dd><p>(<code>character</code>, <code>factor</code> or <code>list</code>)<br />
The input names of the model excluding the batch dimension. For a model
with a single input layer and input axis (e.g., for tabular data), the
input names can be specified as a character vector or factor, e.g.,
for a dense layer with 3 input features use <code>c("X1", "X2", "X3")</code>. If
the model input consists of multiple axes (e.g., for signal and
image data), use a list of character vectors or factors for each axis
in the format &quot;channels first&quot;, e.g., use
<code>list(c("C1", "C2"), c("L1","L2","L3","L4","L5"))</code> for a 1D
convolutional input layer with signal length 4 and 2 channels.<br />
<em>Note:</em> This argument is optional and otherwise the names are
generated automatically. But if this argument is set, all found
input names in the passed model will be disregarded.<br /></p>
</dd>
<dt><code>output_names</code></dt><dd><p>(<code>character</code>, <code>factor</code> )<br />
A character vector with the names for the output dimensions
excluding the batch dimension, e.g., for a model with 3 output nodes use
<code>c("Y1", "Y2", "Y3")</code>. Instead of a character
vector you can also use a factor to set an order for the plots.<br />
<em>Note:</em> This argument is optional and otherwise the names are
generated automatically. But if this argument is set, all found
output names in the passed model will be disregarded.<br /></p>
</dd>
<dt><code>...</code></dt><dd><p>other arguments forwarded to <code><a href="fastshap.html#topic+explain">fastshap::explain</a></code>.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-SHAP-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>SHAP$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other methods: 
<code><a href="#topic+ConnectionWeights">ConnectionWeights</a></code>,
<code><a href="#topic+DeepLift">DeepLift</a></code>,
<code><a href="#topic+DeepSHAP">DeepSHAP</a></code>,
<code><a href="#topic+ExpectedGradient">ExpectedGradient</a></code>,
<code><a href="#topic+Gradient">Gradient</a></code>,
<code><a href="#topic+IntegratedGradient">IntegratedGradient</a></code>,
<code><a href="#topic+LIME">LIME</a></code>,
<code><a href="#topic+LRP">LRP</a></code>,
<code><a href="#topic+SmoothGrad">SmoothGrad</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#----------------------- Example 1: Torch -----------------------------------
library(torch)

# Create nn_sequential model and data
model &lt;- nn_sequential(
  nn_linear(5, 12),
  nn_relu(),
  nn_linear(12, 2),
  nn_softmax(dim = 2)
  )
data &lt;- torch_randn(25, 5)

# Calculate Shapley values for the first 10 instances and set the
# feature and outcome names
shap &lt;- SHAP$new(model, data[1:10, ], data_ref = data,
                 input_names = c("Car", "Cat", "Dog", "Plane", "Horse"),
                 output_names = c("Buy it!", "Don't buy it!"))

# You can also use the helper function `run_shap` for initializing
# an R6 SHAP object
shap &lt;- run_shap(model, data[1:10, ], data_ref = data,
                 input_names = c("Car", "Cat", "Dog", "Plane", "Horse"),
                 output_names = c("Buy it!", "Don't buy it!"))

# Get the result as an array for the first two instances
get_result(shap)[1:2,, ]

# Plot the result for both classes
plot(shap, output_idx = c(1, 2))

# Show the boxplot over all 10 instances
boxplot(shap, output_idx = c(1, 2))

# We can also forward some arguments to fastshap::explain, e.g. nsim to
# get more accurate values
shap &lt;- run_shap(model, data[1:10, ], data_ref = data,
                 input_names = c("Car", "Cat", "Dog", "Plane", "Horse"),
                 output_names = c("Buy it!", "Don't buy it!"),
                 nsim = 10)

# Plot the boxplots again
boxplot(shap, output_idx = c(1, 2))

#----------------------- Example 2: Converter object --------------------------
# We can do the same with an Converter object (all feature and outcome names
# will be extracted by the SHAP method!)
conv &lt;- convert(model,
                input_dim = c(5),
                input_names = c("Car", "Cat", "Dog", "Plane", "Horse"),
                output_names = c("Buy it!", "Don't buy it!"))

# Calculate Shapley values for the first 10 instances
shap &lt;- run_shap(conv, data[1:10], data_ref = data)

# Plot the result for both classes
plot(shap, output_idx = c(1, 2))

#----------------------- Example 3: Other model -------------------------------
if (require("neuralnet") &amp; require("ranger")) {
  library(neuralnet)
  library(ranger)
  data(iris)

  # Fit a random forest unsing the ranger package
  model &lt;- ranger(Species ~ ., data = iris, probability = TRUE)

  # There is no pre-implemented predict function for ranger models, i.e.,
  # we have to define it ourselves.
  pred_fun &lt;- function(newdata, ...) {
    predict(model, newdata, ...)$predictions
  }

  # Calculate Shapley values for the instances of index 1 and 111 and add
  # the outcome labels
  shap &lt;- run_shap(model, iris[c(1, 111), -5], data_ref = iris[, -5],
                   pred_fun = pred_fun,
                   output_names = levels(iris$Species),
                   nsim = 10)

  # Plot the result for the first two classes and all selected instances
  plot(shap, data_idx = 1:2, output_idx = 1:2)

  # Get the result as a torch_tensor
  get_result(shap, "torch_tensor")
}

</code></pre>

<hr>
<h2 id='SmoothGrad'>SmoothGrad and SmoothGrad<code class="reqn">\times</code>Input</h2><span id='topic+SmoothGrad'></span>

<h3>Description</h3>

<p><em>SmoothGrad</em> was introduced by D. Smilkov et al. (2017) and is an extension
to the classical <em>Vanilla <a href="#topic+Gradient">Gradient</a></em> method. It takes the mean of the
gradients for <code>n</code> perturbations of each data point, i.e., with
<code class="reqn">\epsilon \sim N(0,\sigma)</code>
</p>
<p style="text-align: center;"><code class="reqn">1/n \sum_n d f(x+ \epsilon)_j / d x_j.</code>
</p>

<p>Analogous to the <em>Gradient<code class="reqn">\times</code>Input</em> method, you can also use the argument
<code>times_input</code> to multiply the gradients by the inputs before taking the
average (<em>SmoothGrad<code class="reqn">\times</code>Input</em>).
</p>
<p>The R6 class can also be initialized using the <code><a href="#topic+run_smoothgrad">run_smoothgrad</a></code> function
as a helper function so that no prior knowledge of R6 classes is required.
</p>


<h3>Super classes</h3>

<p><code><a href="#topic+InterpretingMethod">innsight::InterpretingMethod</a></code> -&gt; <code><a href="#topic+GradientBased">innsight::GradientBased</a></code> -&gt; <code>SmoothGrad</code>
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>n</code></dt><dd><p>(<code>integer(1)</code>)<br />
Number of perturbations of the input data (default: <code class="reqn">50</code>).<br /></p>
</dd>
<dt><code>noise_level</code></dt><dd><p>(<code>numeric(1)</code>)<br />
The standard deviation of the Gaussian
perturbation, i.e., <code class="reqn">\sigma = (max(x) - min(x)) *</code> <code>noise_level</code>.<br /></p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-SmoothGrad-new"><code>SmoothGrad$new()</code></a>
</p>
</li>
<li> <p><a href="#method-SmoothGrad-clone"><code>SmoothGrad$clone()</code></a>
</p>
</li></ul>



<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="get_result"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-get_result'><code>innsight::InterpretingMethod$get_result()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="plot"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-plot'><code>innsight::InterpretingMethod$plot()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="plot_global"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-plot_global'><code>innsight::InterpretingMethod$plot_global()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="print"><a href='../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-print'><code>innsight::InterpretingMethod$print()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-SmoothGrad-new"></a>



<h4>Method <code>new()</code></h4>

<p>Create a new instance of the <code>SmoothGrad</code> R6 class. When initialized,
the method <em>SmoothGrad</em> or <em>SmoothGrad<code class="reqn">\times</code>Input</em> is applied to
the given data and the results are stored in the field <code>result</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>SmoothGrad$new(
  converter,
  data,
  channels_first = TRUE,
  output_idx = NULL,
  output_label = NULL,
  ignore_last_act = TRUE,
  times_input = FALSE,
  n = 50,
  noise_level = 0.1,
  verbose = interactive(),
  dtype = "float"
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>converter</code></dt><dd><p>(<code><a href="#topic+Converter">Converter</a></code>)<br />
An instance of the <code>Converter</code> class that includes the
torch-converted model and some other model-specific attributes. See
<code><a href="#topic+Converter">Converter</a></code> for details.<br /></p>
</dd>
<dt><code>data</code></dt><dd><p>(<code><a href="base.html#topic+array">array</a></code>, <code><a href="base.html#topic+data.frame">data.frame</a></code>, <code><a href="torch.html#topic+torch_tensor">torch_tensor</a></code> or <code>list</code>)<br />
The data to which the method is to be applied. These must
have the same format as the input data of the passed model to the
converter object. This means either
</p>

<ul>
<li><p> an <code>array</code>, <code>data.frame</code>, <code>torch_tensor</code> or array-like format of
size <em>(batch_size, dim_in)</em>, if e.g., the model has only one input layer, or
</p>
</li>
<li><p> a <code>list</code> with the corresponding input data (according to the
upper point) for each of the input layers.<br />
</p>
</li></ul>
</dd>
<dt><code>channels_first</code></dt><dd><p>(<code>logical(1)</code>)<br />
The channel position of the given data (argument
<code>data</code>). If <code>TRUE</code>, the channel axis is placed at the second position
between the batch size and the rest of the input axes, e.g.,
<code>c(10,3,32,32)</code> for a batch of ten images with three channels and a
height and width of 32 pixels. Otherwise (<code>FALSE</code>), the channel axis
is at the last position, i.e., <code>c(10,32,32,3)</code>. If the data
has no channel axis, use the default value <code>TRUE</code>.<br /></p>
</dd>
<dt><code>output_idx</code></dt><dd><p>(<code>integer</code>, <code>list</code> or <code>NULL</code>)<br />
These indices specify the output nodes for which
the method is to be applied. In order to allow models with multiple
output layers, there are the following possibilities to select
the indices of the output nodes in the individual output layers:
</p>

<ul>
<li><p> An <code>integer</code> vector of indices: If the model has only one output
layer, the values correspond to the indices of the output nodes, e.g.,
<code>c(1,3,4)</code> for the first, third and fourth output node. If there are
multiple output layers, the indices of the output nodes from the first
output layer are considered.
</p>
</li>
<li><p> A <code>list</code> of <code>integer</code> vectors of indices: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired indices of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of indices, e.g., <code>list(NULL, c(1,3))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br />
</p>
</li></ul>
</dd>
<dt><code>output_label</code></dt><dd><p>(<code>character</code>, <code>factor</code>, <code>list</code> or <code>NULL</code>)<br />
These values specify the output nodes for which
the method is to be applied. Only values that were previously passed with
the argument <code>output_names</code> in the <code>converter</code> can be used. In order to
allow models with multiple
output layers, there are the following possibilities to select
the names of the output nodes in the individual output layers:
</p>

<ul>
<li><p> A <code>character</code> vector or <code>factor</code> of labels: If the model has only one output
layer, the values correspond to the labels of the output nodes named in the
passed <code>Converter</code> object, e.g.,
<code>c("a", "c", "d")</code> for the first, third and fourth output node if the
output names are <code>c("a", "b", "c", "d")</code>. If there are
multiple output layers, the names of the output nodes from the first
output layer are considered.
</p>
</li>
<li><p> A <code>list</code> of <code>charactor</code>/<code>factor</code> vectors of labels: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired labels of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of labels, e.g., <code>list(NULL, c("a", "c"))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br />
</p>
</li></ul>
</dd>
<dt><code>ignore_last_act</code></dt><dd><p>(<code>logical(1)</code>)<br />
Set this logical value to include the last
activation functions for each output layer, or not (default: <code>TRUE</code>).
In practice, the last activation (especially for softmax activation) is
often omitted.<br /></p>
</dd>
<dt><code>times_input</code></dt><dd><p>(<code style="white-space: pre;">&#8288;logical(1&#8288;</code>)<br />
Multiplies the gradients with the input features.
This method is called <em>SmoothGrad<code class="reqn">\times</code>Input</em>.<br /></p>
</dd>
<dt><code>n</code></dt><dd><p>(<code>integer(1)</code>)<br />
Number of perturbations of the input data (default: <code class="reqn">50</code>).<br /></p>
</dd>
<dt><code>noise_level</code></dt><dd><p>(<code>numeric(1)</code>)<br />
Determines the standard deviation of the Gaussian
perturbation, i.e., <code class="reqn">\sigma = (max(x) - min(x)) *</code> <code>noise_level</code>.<br /></p>
</dd>
<dt><code>verbose</code></dt><dd><p>(<code>logical(1)</code>)<br />
This logical argument determines whether a progress bar is
displayed for the calculation of the method or not. The default value is
the output of the primitive R function <code><a href="base.html#topic+interactive">interactive()</a></code>.<br /></p>
</dd>
<dt><code>dtype</code></dt><dd><p>(<code>character(1)</code>)<br />
The data type for the calculations. Use
either <code>'float'</code> for <a href="torch.html#topic+torch_float">torch_float</a> or <code>'double'</code> for
<a href="torch.html#topic+torch_double">torch_double</a>.<br /></p>
</dd>
</dl>

</div>


<hr>
<a id="method-SmoothGrad-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>SmoothGrad$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>References</h3>

<p>D. Smilkov et al. (2017) <em>SmoothGrad: removing noise by adding noise.</em>
CoRR, abs/1706.03825
</p>


<h3>See Also</h3>

<p>Other methods: 
<code><a href="#topic+ConnectionWeights">ConnectionWeights</a></code>,
<code><a href="#topic+DeepLift">DeepLift</a></code>,
<code><a href="#topic+DeepSHAP">DeepSHAP</a></code>,
<code><a href="#topic+ExpectedGradient">ExpectedGradient</a></code>,
<code><a href="#topic+Gradient">Gradient</a></code>,
<code><a href="#topic+IntegratedGradient">IntegratedGradient</a></code>,
<code><a href="#topic+LIME">LIME</a></code>,
<code><a href="#topic+LRP">LRP</a></code>,
<code><a href="#topic+SHAP">SHAP</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# ------------------------- Example 1: Torch -------------------------------
library(torch)

# Create nn_sequential model and data
model &lt;- nn_sequential(
  nn_linear(5, 10),
  nn_relu(),
  nn_linear(10, 2),
  nn_sigmoid()
)
data &lt;- torch_randn(25, 5)

# Create Converter
converter &lt;- convert(model, input_dim = c(5))

# Calculate the smoothed Gradients
smoothgrad &lt;- SmoothGrad$new(converter, data)

# You can also use the helper function `run_smoothgrad` for initializing
# an R6 SmoothGrad object
smoothgrad &lt;- run_smoothgrad(converter, data)

# Print the result as a data.frame for first 5 rows
head(get_result(smoothgrad, "data.frame"), 5)

# Plot the result for both classes
plot(smoothgrad, output_idx = 1:2)

# Plot the boxplot of all datapoints
boxplot(smoothgrad, output_idx = 1:2)

# ------------------------- Example 2: Neuralnet ---------------------------
if (require("neuralnet")) {
  library(neuralnet)
  data(iris)

  # Train a neural network
  nn &lt;- neuralnet(Species ~ ., iris,
    linear.output = FALSE,
    hidden = c(10, 5),
    act.fct = "logistic",
    rep = 1
  )

  # Convert the trained model
  converter &lt;- convert(nn)

  # Calculate the smoothed gradients
  smoothgrad &lt;- run_smoothgrad(converter, iris[, -5], times_input = FALSE)

  # Plot the result for the first and 60th data point and all classes
  plot(smoothgrad, data_idx = c(1, 60), output_idx = 1:3)

  # Calculate SmoothGrad x Input and do not ignore the last activation
  smoothgrad &lt;- run_smoothgrad(converter, iris[, -5], ignore_last_act = FALSE)

  # Plot the result again
  plot(smoothgrad, data_idx = c(1, 60), output_idx = 1:3)
}


# ------------------------- Example 3: Keras -------------------------------
if (require("keras") &amp; keras::is_keras_available()) {
  library(keras)

  # Make sure keras is installed properly
  is_keras_available()

  data &lt;- array(rnorm(64 * 60 * 3), dim = c(64, 60, 3))

  model &lt;- keras_model_sequential()
  model %&gt;%
    layer_conv_1d(
      input_shape = c(60, 3), kernel_size = 8, filters = 8,
      activation = "softplus", padding = "valid") %&gt;%
    layer_conv_1d(
      kernel_size = 8, filters = 4, activation = "tanh",
      padding = "same") %&gt;%
    layer_conv_1d(
      kernel_size = 4, filters = 2, activation = "relu",
      padding = "valid") %&gt;%
    layer_flatten() %&gt;%
    layer_dense(units = 64, activation = "relu") %&gt;%
    layer_dense(units = 16, activation = "relu") %&gt;%
    layer_dense(units = 3, activation = "softmax")

  # Convert the model
  converter &lt;- convert(model)

  # Apply the SmoothGrad method
  smoothgrad &lt;- run_smoothgrad(converter, data, channels_first = FALSE)

  # Plot the result for the first datapoint and all classes
  plot(smoothgrad, output_idx = 1:3)

  # Plot the result as boxplots for first two classes
  boxplot(smoothgrad, output_idx = 1:2)
}


#------------------------- Plotly plots ------------------------------------
if (require("plotly")) {
  # You can also create an interactive plot with plotly.
  # This is a suggested package, so make sure that it is installed
  library(plotly)

  # Result as boxplots
  boxplot(smoothgrad, as_plotly = TRUE)

  # Result of the second data point
  plot(smoothgrad, data_idx = 2, as_plotly = TRUE)
}

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
